<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-04-29</h1>
<h3>Title: Critical Challenges and Guidelines in Evaluating Synthetic Tabular Data: A Systematic Review</h3>
<ul>
<li><strong>Authors: </strong>Nazia Nafis, Inaki Esnaola, Alvaro Martinez-Perez, Maria-Cruz Villa-Uriol, Venet Osmani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18544">https://arxiv.org/abs/2504.18544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18544">https://arxiv.org/pdf/2504.18544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18544]] Critical Challenges and Guidelines in Evaluating Synthetic Tabular Data: A Systematic Review(https://arxiv.org/abs/2504.18544)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating synthetic tabular data can be challenging, however evaluation of their quality is just as challenging, if not more. This systematic review sheds light on the critical importance of rigorous evaluation of synthetic health data to ensure reliability, relevance, and their appropriate use. Based on screening of 1766 papers and a detailed review of 101 papers we identified key challenges, including lack of consensus on evaluation methods, improper use of evaluation metrics, limited input from domain experts, inadequate reporting of dataset characteristics, and limited reproducibility of results. In response, we provide several guidelines on the generation and evaluation of synthetic data, to allow the community to unlock and fully harness the transformative potential of synthetic data and accelerate innovation.</li>
<li><strong>摘要：</strong>生成合成的表格数据可能具有挑战性，但是对其质量的评估同样具有挑战性，甚至更多。该系统评价阐明了对合成健康数据进行严格评估的重要重要性，以确保可靠性，相关性及其适当使用。根据对1766篇论文的筛查和对101篇论文的详细评论，我们确定了关键挑战，包括缺乏评估方法的共识，不当使用评估指标，域专家的有限投入，数据集特征的报告不足以及结果有限的结果可重复性。作为回应，我们提供了几种有关合成数据的生成和评估的准则，以使社区能够解锁和充分利用合成数据和加速创新的变革潜力。</li>
</ul>

<h3>Title: PARD: Accelerating LLM Inference with Low-Cost PARallel Draft Model Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Zihao An, Huajun Bai, Ziqiong Liu, Dong Li, Emad Barsoum</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18583">https://arxiv.org/abs/2504.18583</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18583">https://arxiv.org/pdf/2504.18583</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18583]] PARD: Accelerating LLM Inference with Low-Cost PARallel Draft Model Adaptation(https://arxiv.org/abs/2504.18583)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The autoregressive nature of large language models (LLMs) limits inference speed. Each forward pass generates only a single token and is often bottlenecked by memory bandwidth. Speculative decoding alleviates this issue using a draft-then-verify approach to accelerate token generation. However, the overhead introduced during the draft phase and the training cost of the draft model limit the efficiency and adaptability of speculative decoding. In this work, we introduce PARallel Draft (PARD), a novel speculative decoding method that enables low-cost adaptation of autoregressive draft models into parallel draft models. PARD enhances inference efficiency by predicting multiple future tokens in a single forward pass of the draft phase, and incorporates a conditional drop token method to accelerate training. Its target-independence property allows a single draft model to be applied to an entire family of different models, minimizing the adaptation cost. Our proposed conditional drop token method can improves draft model training efficiency by 3x. On our optimized inference framework, PARD accelerates LLaMA3.1-8B inference by 4.08x, achieving 311.5 tokens per second.</li>
<li><strong>摘要：</strong>大语言模型（LLMS）的自回归性质限制了推理速度。每个正向通行证仅生成一个令牌，通常会被内存带宽所瓶颈。投机性解码使用草稿进行加速代币产生的方法来减轻此问题。但是，在草案阶段引入的间接费用以及草案模型的培训成本限制了投机解码的效率和适应性。在这项工作中，我们介绍了平行草稿（PARD），这是一种新颖的投机解码方法，可以将自回归草稿模型的低成本改编为并行草稿模型。 PARD通过在草稿阶段的单个前向传球中预测多个未来令牌，并结合了有条件的下降令牌方法来加速训练，从而提高了推理效率。它的目标独立属性允许将单个草稿模型应用于整个不同模型的家庭，从而最大程度地减少适应性成本。我们提出的条件下降令牌方法可以将模型训练效率提高3倍。在我们优化的推理框架上，PARD将Llama3.1-8b推断加速4.08倍，每秒可实现311.5令牌。</li>
</ul>

<h3>Title: Geometry aware inference of steady state PDEs using Equivariant Neural Fields representations</h3>
<ul>
<li><strong>Authors: </strong>Giovanni Catalani, Michael Bauerheim, Frédéric Tost, Xavier Bertrand, Joseph Morlier</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18591">https://arxiv.org/abs/2504.18591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18591">https://arxiv.org/pdf/2504.18591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18591]] Geometry aware inference of steady state PDEs using Equivariant Neural Fields representations(https://arxiv.org/abs/2504.18591)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Recent advances in Neural Fields have enabled powerful, discretization-invariant methods for learning neural operators that approximate solutions of Partial Differential Equations (PDEs) on general geometries. Building on these developments, we introduce enf2enf, an encoder--decoder methodology for predicting steady-state Partial Differential Equations with non-parameterized geometric variability, based on recently proposed Equivariant Neural Field architectures. In enf2enf, input geometries are encoded into latent point cloud embeddings that inherently preserve geometric grounding and capture local phenomena. The resulting representations are then combined with global parameters and directly decoded into continuous output fields, thus efficiently modeling the coupling between geometry and physics. By leveraging the inductive biases of locality and translation invariance, our approach is able to capture fine-scale physical features as well as complex shape variations, thereby enhancing generalization and physical compliance. Extensive experiments on a high-fidelity aerodynamic dataset, a hyper-elastic material benchmark, and multi-element airfoil geometries, demonstrate that the proposed model achieves superior or competitive performance compared to state-of-the-art graph based, operator learning, and neural field methods. Notably, our method supports real time inference and zero-shot super-resolution, enabling efficient training on low-resolution meshes while maintaining high accuracy on full-scale discretizations.</li>
<li><strong>摘要：</strong>神经领域的最新进展已实现了学习神经操作员的强大，离散不变的方法，该方法近似于一般几何形状的部分微分方程（PDE）的解决方案。在这些发展的基础上，我们介绍了ENF2ENF，这是一种基于最近提出的eproivariant神经场体系结构，用于预测具有非参数几何变异性的稳态部分微分方程。在ENF2ENF中，输入几何形状被编码为潜在点云嵌入，这些嵌入固有地保留了几何接地并捕获局部现象。然后将所得的表示与全局参数结合，并直接解码为连续的输出字段，从而有效地对几何和物理之间的耦合进行建模。通过利用局部和翻译不变性的归纳偏见，我们的方法能够捕获精细的物理特征以及复杂的形状变化，从而增强了概括和物理合规性。对高保真空气动力学数据集的广泛实验，超弹性材料基准和多元素翼型的几何形状表明，与基于最先进的图形，操作员学习和神经领域的方法相比，所提出的模型可实现优于或竞争性的性能。值得注意的是，我们的方法支持实时推理和零击的超分辨率，从而在低分辨率网格上有效训练，同时在全尺度离散化方面保持高精度。</li>
</ul>

<h3>Title: Multimodal graph representation learning for website generation based on visual sketch</h3>
<ul>
<li><strong>Authors: </strong>Tung D. Vu, Chung Hoang, Truong-Son Hy</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18729">https://arxiv.org/abs/2504.18729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18729">https://arxiv.org/pdf/2504.18729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18729]] Multimodal graph representation learning for website generation based on visual sketch(https://arxiv.org/abs/2504.18729)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The Design2Code problem, which involves converting digital designs into functional source code, is a significant challenge in software development due to its complexity and time-consuming nature. Traditional approaches often struggle with accurately interpreting the intricate visual details and structural relationships inherent in webpage designs, leading to limitations in automation and efficiency. In this paper, we propose a novel method that leverages multimodal graph representation learning to address these challenges. By integrating both visual and structural information from design sketches, our approach enhances the accuracy and efficiency of code generation, particularly in producing semantically correct and structurally sound HTML code. We present a comprehensive evaluation of our method, demonstrating significant improvements in both accuracy and efficiency compared to existing techniques. Extensive evaluation demonstrates significant improvements of multimodal graph learning over existing techniques, highlighting the potential of our method to revolutionize design-to-code automation. Code available at this https URL</li>
<li><strong>摘要：</strong>Design2Code问题涉及将数字设计转换为功能源代码，由于其复杂性和耗时的性质，在软件开发中是一个重大挑战。传统方法通常在准确解释网页设计中固有的复杂视觉细节和结构关系方面遇到困难，从而导致自动化和效率的限制。在本文中，我们提出了一种利用多模式图表示学习来应对这些挑战的新方法。通过从设计草图中集成视觉和结构信息，我们的方法提高了代码生成的准确性和效率，尤其是在生成语义上正确和结构上声音的HTML代码方面。我们对我们的方法进行了全面的评估，与现有技术相比，准确性和效率都有显着提高。广泛的评估表明，多模式图学习对现有技术的显着改善，强调了我们方法彻底改变设计对代码自动化的潜力。此https URL可用代码</li>
</ul>

<h3>Title: Dream-Box: Object-wise Outlier Generation for Out-of-Distribution Detection</h3>
<ul>
<li><strong>Authors: </strong>Brian K. S. Isaac-Medina, Toby P. Breckon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18746">https://arxiv.org/abs/2504.18746</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18746">https://arxiv.org/pdf/2504.18746</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18746]] Dream-Box: Object-wise Outlier Generation for Out-of-Distribution Detection(https://arxiv.org/abs/2504.18746)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Deep neural networks have demonstrated great generalization capabilities for tasks whose training and test sets are drawn from the same distribution. Nevertheless, out-of-distribution (OOD) detection remains a challenging task that has received significant attention in recent years. Specifically, OOD detection refers to the detection of instances that do not belong to the training distribution, while still having good performance on the in-distribution task (e.g., classification or object detection). Recent work has focused on generating synthetic outliers and using them to train an outlier detector, generally achieving improved OOD detection than traditional OOD methods. In this regard, outliers can be generated either in feature or pixel space. Feature space driven methods have shown strong performance on both the classification and object detection tasks, at the expense that the visualization of training outliers remains unknown, making further analysis on OOD failure modes challenging. On the other hand, pixel space outlier generation techniques enabled by diffusion models have been used for image classification using, providing improved OOD detection performance and outlier visualization, although their adaption to the object detection task is as yet unexplored. We therefore introduce Dream-Box, a method that provides a link to object-wise outlier generation in the pixel space for OOD detection. Specifically, we use diffusion models to generate object-wise outliers that are used to train an object detector for an in-distribution task and OOD detection. Our method achieves comparable performance to previous traditional methods while being the first technique to provide concrete visualization of generated OOD objects.</li>
<li><strong>摘要：</strong>深层神经网络已显示出从相同分布中绘制的训练和测试集的任务的出色概括能力。然而，近年来一直受到重大关注的挑战性的任务（OOD）仍然是一项具有挑战性的任务。具体而言，OOD检测是指未属于培训分布的实例，同时仍然在分布任务（例如分类或对象检测）上具有良好的性能。最近的工作集中在产生合成异常值并使用它们来训练异常值检测器，通常比传统的OOD方法可以改善OOD检测。在这方面，可以在功能或像素空间中生成异常值。特征空间驱动的方法在分类和对象检测任务上都表现出强烈的性能，以牺牲训练异常值的可视化仍然未知，从而进一步分析了OOD故障模式具有挑战性。另一方面，通过扩散模型启用的像素空间超出相关性的生成技术已用于使用图像分类，提供了改进的OOD检测性能和较高的可视化，尽管它们对对象检测任务的适应性尚未探索。因此，我们介绍了Dream-Box，该方法提供了指向OOD检测的像素空间中对象异常生成的链接。具体而言，我们使用扩散模型来生成针对对象的异常值，这些异常值用于训练对象检测器进行分配任务和OOD检测。我们的方法可以实现与以前的传统方法相当的性能，同时是第一种提供生成的OOD对象​​具体可视化的技术。</li>
</ul>

<h3>Title: Performance of Machine Learning Classifiers for Anomaly Detection in Cyber Security Applications</h3>
<ul>
<li><strong>Authors: </strong>Markus Haug, Gissel Velarde</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18771">https://arxiv.org/abs/2504.18771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18771">https://arxiv.org/pdf/2504.18771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18771]] Performance of Machine Learning Classifiers for Anomaly Detection in Cyber Security Applications(https://arxiv.org/abs/2504.18771)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This work empirically evaluates machine learning models on two imbalanced public datasets (KDDCUP99 and Credit Card Fraud 2013). The method includes data preparation, model training, and evaluation, using an 80/20 (train/test) split. Models tested include eXtreme Gradient Boosting (XGB), Multi Layer Perceptron (MLP), Generative Adversarial Network (GAN), Variational Autoencoder (VAE), and Multiple-Objective Generative Adversarial Active Learning (MO-GAAL), with XGB and MLP further combined with Random-Over-Sampling (ROS) and Self-Paced-Ensemble (SPE). Evaluation involves 5-fold cross-validation and imputation techniques (mean, median, and IterativeImputer) with 10, 20, 30, and 50 % missing data. Findings show XGB and MLP outperform generative models. IterativeImputer results are comparable to mean and median, but not recommended for large datasets due to increased complexity and execution time. The code used is publicly available on GitHub (this http URL).</li>
<li><strong>摘要：</strong>这项工作通过经验评估了两个不平衡的公共数据集（KDDCUP99和信用卡欺诈2013）上的机器学习模型。该方法包括使用80/20（火车/测试）拆分的数据准备，模型培训和评估。测试的模型包括极端梯度提升（XGB），多层感知器（MLP），生成对抗网络（GAN），变异自动编码器（VAE）和多物体生成的对抗性活性学习（MO-GAAL），与XGB和MLP与XGB和MLP的进一步结合，并与随机的（ros-over-over-over-over-over-selfsemence）结合在一起。评估涉及10％，20％，30％和50％丢失的数据，涉及5倍的交叉验证和插补技术（平均值，中值和迭代Imputer）。发现显示XGB和MLP优于生成模型。 IterativeImputer结果与平均值和中位数相当，但由于增加的复杂性和执行时间，不建议大型数据集使用。所使用的代码在GitHub（此HTTP URL）上公开可用。</li>
</ul>

<h3>Title: Audio-Driven Talking Face Video Generation with Joint Uncertainty Learning</h3>
<ul>
<li><strong>Authors: </strong>Yifan Xie, Fei Ma, Yi Bin, Ying He, Fei Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18810">https://arxiv.org/abs/2504.18810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18810">https://arxiv.org/pdf/2504.18810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18810]] Audio-Driven Talking Face Video Generation with Joint Uncertainty Learning(https://arxiv.org/abs/2504.18810)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Talking face video generation with arbitrary speech audio is a significant challenge within the realm of digital human technology. The previous studies have emphasized the significance of audio-lip synchronization and visual quality. Currently, limited attention has been given to the learning of visual uncertainty, which creates several issues in existing systems, including inconsistent visual quality and unreliable performance across different input conditions. To address the problem, we propose a Joint Uncertainty Learning Network (JULNet) for high-quality talking face video generation, which incorporates a representation of uncertainty that is directly related to visual error. Specifically, we first design an uncertainty module to individually predict the error map and uncertainty map after obtaining the generated image. The error map represents the difference between the generated image and the ground truth image, while the uncertainty map is used to predict the probability of incorrect estimates. Furthermore, to match the uncertainty distribution with the error distribution through a KL divergence term, we introduce a histogram technique to approximate the distributions. By jointly optimizing error and uncertainty, the performance and robustness of our model can be enhanced. Extensive experiments demonstrate that our method achieves superior high-fidelity and audio-lip synchronization in talking face video generation compared to previous methods.</li>
<li><strong>摘要：</strong>用任意语音音频的说话面对视频发电是数字人类技术领域的重大挑战。先前的研究强调了音频同步和视觉质量的重要性。当前，对视觉不确定性的学习有限，这在现有系统中造成了几个问题，包括视觉质量不一致和在不同输入条件下的性能不可靠。为了解决这个问题，我们提出了一个联合不确定性学习网络（JULNET），以进行高质量的说话面部视频生成，该网络结合了与视觉错误直接相关的不确定性的表示。具体而言，我们首先设计一个不确定性模块，以在获得生成的图像后单独预测误差图和不确定性图。误差图表示生成的图像和地面真相图像之间的差异，而不确定性图则用于预测错误估计的概率。此外，为了使不确定性分布与误差分布通过KL差异项匹配，我们引入了一种直方图技术来近似分布。通过共同优化误差和不确定性，可以增强我们模型的性能和鲁棒性。广泛的实验表明，与以前的方法相比，我们的方法在说话面部视频的产生中实现了卓越的高保真性和音频同步。</li>
</ul>

<h3>Title: Frequency-Integrated Transformer for Arbitrary-Scale Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Xufei Wang, Fei Ge, Jinchen Zhu, Mingjian Zhang, Qi Wu, Jifeng Ren Shizhuang Weng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18818">https://arxiv.org/abs/2504.18818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18818">https://arxiv.org/pdf/2504.18818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18818]] Frequency-Integrated Transformer for Arbitrary-Scale Super-Resolution(https://arxiv.org/abs/2504.18818)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Methods based on implicit neural representation have demonstrated remarkable capabilities in arbitrary-scale super-resolution (ASSR) tasks, but they neglect the potential value of the frequency domain, leading to sub-optimal performance. We proposes a novel network called Frequency-Integrated Transformer (FIT) to incorporate and utilize frequency information to enhance ASSR performance. FIT employs Frequency Incorporation Module (FIM) to introduce frequency information in a lossless manner and Frequency Utilization Self-Attention module (FUSAM) to efficiently leverage frequency information by exploiting spatial-frequency interrelationship and global nature of frequency. FIM enriches detail characterization by incorporating frequency information through a combination of Fast Fourier Transform (FFT) with real-imaginary mapping. In FUSAM, Interaction Implicit Self-Attention (IISA) achieves cross-domain information synergy by interacting spatial and frequency information in subspace, while Frequency Correlation Self-attention (FCSA) captures the global context by computing correlation in frequency. Experimental results demonstrate FIT yields superior performance compared to existing methods across multiple benchmark datasets. Visual feature map proves the superiority of FIM in enriching detail characterization. Frequency error map validates IISA productively improve the frequency fidelity. Local attribution map validates FCSA effectively captures global context.</li>
<li><strong>摘要：</strong>基于隐式神经表示的方法表明，在任意规模的超分辨率（ASSR）任务中具有显着的功能，但它们忽略了频域的潜在价值，从而导致了次优性能。我们提出了一个称为频率集成变压器（FIT）的新型网络，以合并和利用频率信息来增强ASSR性能。 FIT采用频率合并模块（FIM）以无损的方式和频率利用自我发项模块（FUSAM）引入频率信息，以通过利用空间频率相互关系和频率的全球性质来有效利用频率信息。 FIM通过将快速傅立叶变换（FFT）与现实疑问映射组合结合频率信息来丰富详细的表征。在FUSAM中，相互作用隐式自我注意力（IISA）通过在子空间中的空间和频率信息进行交互，而频率相关自我注意（FCSA）通过计算频率计算相关性来捕获全局环境，从而实现了跨域信息协同作用。实验结果表明，与多个基准数据集的现有方法相比，拟合度的表现较高。视觉特征图证明了FIM在丰富细节表征中的优越性。频率错误图验证了IISA有效地提高频率保真度。本地归因图验证FCSA有效地捕获了全局上下文。</li>
</ul>

<h3>Title: Improved Molecular Generation through Attribute-Driven Integrative Embeddings and GAN Selectivity</h3>
<ul>
<li><strong>Authors: </strong>Nandan Joshi, Erhan Guven</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19040">https://arxiv.org/abs/2504.19040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19040">https://arxiv.org/pdf/2504.19040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19040]] Improved Molecular Generation through Attribute-Driven Integrative Embeddings and GAN Selectivity(https://arxiv.org/abs/2504.19040)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The growing demand for molecules with tailored properties in fields such as drug discovery and chemical engineering has driven advancements in computational methods for molecular design. Machine learning-based approaches for de-novo molecular generation have recently garnered significant attention. This paper introduces a transformer-based vector embedding generator combined with a modified Generative Adversarial Network (GAN) to generate molecules with desired properties. The embedding generator utilizes a novel molecular descriptor, integrating Morgan fingerprints with global molecular attributes, enabling the transformer to capture local functional groups and broader molecular characteristics. Modifying the GAN generator loss function ensures the generation of molecules with specific desired properties. The transformer achieves a reconversion accuracy of 94% while translating molecular descriptors back to SMILES strings, validating the utility of the proposed embeddings for generative tasks. The approach is validated by generating novel odorant molecules using a labeled dataset of odorant and non-odorant compounds. With the modified range-loss function, the GAN exclusively generates odorant molecules. This work underscores the potential of combining novel vector embeddings with transformers and modified GAN architectures to accelerate the discovery of tailored molecules, offering a robust tool for diverse molecular design applications.</li>
<li><strong>摘要：</strong>在药物发现和化学工程等领域，对具有量身定制特性的分子的需求不断增长，从而推动了分子设计计算方法的进步。 De-Novo分子产生的基于机器学习的方法最近引起了极大的关注。本文介绍了基于变压器的矢量嵌入发生器，并结合了修改的生成对抗网络（GAN），以生成具有所需属性的分子。嵌入发生器利用了一种新颖的分子描述符，将摩根指纹与全局分子属性整合在一起，从而使变压器能够捕获局部官能团和更广泛的分子特性。修改GAN发生器损耗函数可确保具有特定所需特性的分子产生。变压器在将分子描述符转换回微笑字符串的同时，达到了94％的重新交流准确性，从而验证了提出的嵌入式用于生成任务的效用。该方法通过使用有气味和非塑料化合物的标记数据集生成新型气味分子来验证。通过修改的范围函数，GAN专门生成气味分子。这项工作强调了将新型矢量嵌入与变压器和改进的GAN架构相结合的潜力，以加速发现量身定制的分子，为各种分子设计应用提供强大的工具。</li>
</ul>

<h3>Title: Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Mahdi Abootorabi, Omid Ghahroodi, Pardis Sadat Zahraei, Hossein Behzadasl, Alireza Mirrokni, Mobina Salimipanah, Arash Rasouli, Bahar Behzadipour, Sara Azarnoush, Benyamin Maleki, Erfan Sadraiye, Kiarash Kiani Feriz, Mahdi Teymouri Nahad, Ali Moghadasi, Abolfazl Eshagh Abianeh, Nizi Nazar, Hamid R. Rabiee, Mahdieh Soleymani Baghshah, Meisam Ahmadi, Ehsaneddin Asgari</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19056">https://arxiv.org/abs/2504.19056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19056">https://arxiv.org/pdf/2504.19056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19056]] Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions(https://arxiv.org/abs/2504.19056)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative AI is reshaping art, gaming, and most notably animation. Recent breakthroughs in foundation and diffusion models have reduced the time and cost of producing animated content. Characters are central animation components, involving motion, emotions, gestures, and facial expressions. The pace and breadth of advances in recent months make it difficult to maintain a coherent view of the field, motivating the need for an integrative review. Unlike earlier overviews that treat avatars, gestures, or facial animation in isolation, this survey offers a single, comprehensive perspective on all the main generative AI applications for character animation. We begin by examining the state-of-the-art in facial animation, expression rendering, image synthesis, avatar creation, gesture modeling, motion synthesis, object generation, and texture synthesis. We highlight leading research, practical deployments, commonly used datasets, and emerging trends for each area. To support newcomers, we also provide a comprehensive background section that introduces foundational models and evaluation metrics, equipping readers with the knowledge needed to enter the field. We discuss open challenges and map future research directions, providing a roadmap to advance AI-driven character-animation technologies. This survey is intended as a resource for researchers and developers entering the field of generative AI animation or adjacent fields. Resources are available at: this https URL.</li>
<li><strong>摘要：</strong>生成的AI正在重塑艺术，游戏，最著名的是动画。基础和扩散模型的最新突破减少了生产动画内容的时间和成本。角色是中心动画组成部分，涉及运动，情感，手势和面部表情。近几个月来，进步的步伐和广度使得很难保持对该领域的连贯观点，从而激发了对综合审查的需求。与较早的概述孤立地处理化身，手势或面部动画的概述不同，该调查为所有主要生成AI应用程序提供了一个单一的，全面的视角，用于角色动画。我们首先检查面部动画，表达渲染，图像合成，化身创建，手势建模，运动合成，对象产生和纹理综合中的最先进。我们强调了领先的研究，实际部署，常用的数据集以及每个领域的新兴趋势。为了支持新移民，我们还提供了一个全面的背景部分，该部分介绍了基础模型和评估指标，为读者提供了进入该领域所需的知识。我们讨论开放挑战并绘制未来的研究方向，提供了推进AI驱动角色动画技术的路线图。该调查旨在作为研究人员和开发人员进入生成AI动画或相邻领域的资源。资源可用：此HTTPS URL。</li>
</ul>

<h3>Title: Harmonizing Generalization and Personalization in Ring-topology Decentralized Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Shunxin Guo, Jiaqi Lv, Xin Geng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19103">https://arxiv.org/abs/2504.19103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19103">https://arxiv.org/pdf/2504.19103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19103]] Harmonizing Generalization and Personalization in Ring-topology Decentralized Federated Learning(https://arxiv.org/abs/2504.19103)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce Ring-topology Decentralized Federated Learning (RDFL) for distributed model training, aiming to avoid the inherent risks of centralized failure in server-based FL. However, RDFL faces the challenge of low information-sharing efficiency due to the point-to-point communication manner when handling inherent data heterogeneity. Existing studies to mitigate data heterogeneity focus on personalized optimization of models, ignoring that the lack of shared information constraints can lead to large differences among models, weakening the benefits of collaborative learning. To tackle these challenges, we propose a Divide-and-conquer RDFL framework (DRDFL) that uses a feature generation model to extract personalized information and invariant shared knowledge from the underlying data distribution, ensuring both effective personalization and strong generalization. Specifically, we design a \textit{PersonaNet} module that encourages class-specific feature representations to follow a Gaussian mixture distribution, facilitating the learning of discriminative latent representations tailored to local data distributions. Meanwhile, the \textit{Learngene} module is introduced to encapsulate shared knowledge through an adversarial classifier to align latent representations and extract globally invariant information. Extensive experiments demonstrate that DRDFL outperforms state-of-the-art methods in various data heterogeneity settings.</li>
<li><strong>摘要：</strong>我们介绍了环境分散的联合学习（RDFL）进行分布式模型培训，旨在避免基于服务器的FL中集中式失败的固有风险。但是，在处理固有的数据异质性时，由于点对点通信方式，RDFL面临低信息共享效率的挑战。现有的减轻数据异质性的研究集中在模型的个性化优化上，而忽略缺乏共享信息限制会导致模型之间的巨大差异，从而削弱了协作学习的好处。为了应对这些挑战，我们提出了一个分裂和争议的RDFL框架（DRDFL），该框架使用功能生成模型从基础数据分布中提取个性化信息和不变的共享知识，从而确保有效的个性化和强有力的概括。具体来说，我们设计了一个\ textIt {PersonAnet}模块，该模块鼓励特定于类的功能表示遵循高斯混合物分布，从而促进了针对本地数据分布量身定制的判别潜在表示。同时，引入了\ textIt {LearnGene}模块，以通过对抗分类器来封装共享知识，以使潜在表示并提取全球不变的信息。广泛的实验表明，在各种数据异质性设置中，DRDFL优于最先进的方法。</li>
</ul>

<h3>Title: Hierarchical Attention Generates Better Proofs</h3>
<ul>
<li><strong>Authors: </strong>Jianlong Chen, Chao Li, Yang Yuan, Andrew C Yao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19188">https://arxiv.org/abs/2504.19188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19188">https://arxiv.org/pdf/2504.19188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19188]] Hierarchical Attention Generates Better Proofs(https://arxiv.org/abs/2504.19188)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown promise in formal theorem proving, but their token-level processing often fails to capture the inherent hierarchical nature of mathematical proofs. We introduce \textbf{Hierarchical Attention}, a regularization method that aligns LLMs' attention mechanisms with mathematical reasoning structures. Our approach establishes a five-level hierarchy from foundational elements to high-level concepts, ensuring structured information flow in proof generation. Experiments demonstrate that our method improves proof success rates by 2.05\% on miniF2F and 1.69\% on ProofNet while reducing proof complexity by 23.81\% and 16.50\% respectively. The code is available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在正式定理中表现出了希望，但是它们的令牌级处理通常无法捕获数学证明的固有层次结构。我们介绍了\ textbf {分层注意}，这是一种正则化方法，将LLMS的注意机制与数学推理结构保持一致。我们的方法建立了从基础元素到高级概念的五级层次结构，从而确保了证明生成中的结构化信息流。实验表明，我们的方法在minif2f上提高了2.05 \％的证明成功率和验证网络上的1.69 \％，同时将证明复杂性降低了23.81 \％和16.50 \％。该代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Rendering Anywhere You See: Renderability Field-guided Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Xiaofeng Jin, Yan Fang, Matteo Frosi, Jianfei Ge, Jiangjian Xiao, Matteo Matteucci</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19261">https://arxiv.org/abs/2504.19261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19261">https://arxiv.org/pdf/2504.19261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19261]] Rendering Anywhere You See: Renderability Field-guided Gaussian Splatting(https://arxiv.org/abs/2504.19261)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Scene view synthesis, which generates novel views from limited perspectives, is increasingly vital for applications like virtual reality, augmented reality, and robotics. Unlike object-based tasks, such as generating 360° views of a car, scene view synthesis handles entire environments where non-uniform observations pose unique challenges for stable rendering quality. To address this issue, we propose a novel approach: renderability field-guided gaussian splatting (RF-GS). This method quantifies input inhomogeneity through a renderability field, guiding pseudo-view sampling to enhanced visual consistency. To ensure the quality of wide-baseline pseudo-views, we train an image restoration model to map point projections to visible-light styles. Additionally, our validated hybrid data optimization strategy effectively fuses information of pseudo-view angles and source view textures. Comparative experiments on simulated and real-world data show that our method outperforms existing approaches in rendering stability.</li>
<li><strong>摘要：</strong>场景视图合成从有限的角度产生新颖的观点，对于虚拟现实，增强现实和机器人技术等应用来说越来越重要。与基于对象的任务（例如生成360°的汽车视图）不同，场景视图合成处理了整个环境，在这些环境中，不均匀观测值对稳定的渲染质量构成了独特的挑战。为了解决这个问题，我们提出了一种新颖的方法：可渲染性场引导的高斯分裂（RF-GS）。此方法通过渲染性字段量化输入不均匀性，将伪视图采样引导以增强视觉一致性。为了确保宽基线伪视图的质量，我们训练图像恢复模型，以将点投影映射到可见光的样式。此外，我们经过验证的混合数据优化策略有效地融合了伪视角和源查看纹理的信息。对模拟和现实世界数据的比较实验表明，我们的方法在呈现稳定性方面的现有方法优于现有方法。</li>
</ul>

<h3>Title: TeleSparse: Practical Privacy-Preserving Verification of Deep Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Mohammad M Maheri, Hamed Haddadi, Alex Davidson</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19274">https://arxiv.org/abs/2504.19274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19274">https://arxiv.org/pdf/2504.19274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19274]] TeleSparse: Practical Privacy-Preserving Verification of Deep Neural Networks(https://arxiv.org/abs/2504.19274)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Verification of the integrity of deep learning inference is crucial for understanding whether a model is being applied correctly. However, such verification typically requires access to model weights and (potentially sensitive or private) training data. So-called Zero-knowledge Succinct Non-Interactive Arguments of Knowledge (ZK-SNARKs) would appear to provide the capability to verify model inference without access to such sensitive data. However, applying ZK-SNARKs to modern neural networks, such as transformers and large vision models, introduces significant computational overhead. We present TeleSparse, a ZK-friendly post-processing mechanisms to produce practical solutions to this problem. TeleSparse tackles two fundamental challenges inherent in applying ZK-SNARKs to modern neural networks: (1) Reducing circuit constraints: Over-parameterized models result in numerous constraints for ZK-SNARK verification, driving up memory and proof generation costs. We address this by applying sparsification to neural network models, enhancing proof efficiency without compromising accuracy or security. (2) Minimizing the size of lookup tables required for non-linear functions, by optimizing activation ranges through neural teleportation, a novel adaptation for narrowing activation functions' range. TeleSparse reduces prover memory usage by 67% and proof generation time by 46% on the same model, with an accuracy trade-off of approximately 1%. We implement our framework using the Halo2 proving system and demonstrate its effectiveness across multiple architectures (Vision-transformer, ResNet, MobileNet) and datasets (ImageNet,CIFAR-10,CIFAR-100). This work opens new directions for ZK-friendly model design, moving toward scalable, resource-efficient verifiable deep learning.</li>
<li><strong>摘要：</strong>验证深度学习推断的完整性对于理解是否正确应用模型至关重要。但是，这种验证通常需要访问模型权重和（潜在敏感或私人）培训数据。所谓的零知识简洁的知识非相互作用论证（ZK-SNARKS）似乎可以提供验证模型推断而无需访问此类敏感数据的能力。但是，将ZK-SNARK应用于现代神经网络，例如变形金刚和大型视觉模型，都引入了重要的计算开销。我们提出了TeleSparse，这是一种ZK友好的后处理机制，可为此问题提供实用的解决方案。 Telesparse应对将ZK-SNARKS应用于现代神经网络固有的两个基本挑战：（1）减少电路限制：过度参数化的模型导致许多限制ZK-SNARK验证，推动记忆力和证明发电成本。我们通过将稀疏性应用于神经网络模型，提高证明效率而不损害准确性或安全性来解决这一问题。 （2）最小化非线性函数所需的查找表的大小，通过通过神经传送来优化激活范围，这是一种新颖的适应激活函数范围的新型适应。 Telesparse在同一模型上将供者的记忆使用量减少了67％，并且证明生成时间的精度约为1％。我们使用HALO2证明系统实施了框架，并在多个架构（视觉变换器，Resnet，Mobilenet）和数据集（Imagenet，Cifar-10，Cifar-100）中展示了其有效性。这项工作为ZK友好型模型设计打开了新的方向，朝着可扩展的，资源有效的可验证深度学习方向发展。</li>
</ul>

<h3>Title: Anyprefer: An Agentic Framework for Preference Data Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Yiyang Zhou, Zhaoyang Wang, Tianle Wang, Shangyu Xing, Peng Xia, Bo Li, Kaiyuan Zheng, Zijian Zhang, Zhaorun Chen, Wenhao Zheng, Xuchao Zhang, Chetan Bansal, Weitong Zhang, Ying Wei, Mohit Bansal, Huaxiu Yao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19276">https://arxiv.org/abs/2504.19276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19276">https://arxiv.org/pdf/2504.19276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19276]] Anyprefer: An Agentic Framework for Preference Data Synthesis(https://arxiv.org/abs/2504.19276)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>High-quality preference data is essential for aligning foundation models with human values through preference learning. However, manual annotation of such data is often time-consuming and costly. Recent methods often adopt a self-rewarding approach, where the target model generates and annotates its own preference data, but this can lead to inaccuracies since the reward model shares weights with the target model, thereby amplifying inherent biases. To address these issues, we propose Anyprefer, a framework designed to synthesize high-quality preference data for aligning the target model. Anyprefer frames the data synthesis process as a cooperative two-player Markov Game, where the target model and the judge model collaborate together. Here, a series of external tools are introduced to assist the judge model in accurately rewarding the target model's responses, mitigating biases in the rewarding process. In addition, a feedback mechanism is introduced to optimize prompts for both models, enhancing collaboration and improving data quality. The synthesized data is compiled into a new preference dataset, Anyprefer-V1, consisting of 58K high-quality preference pairs. Extensive experiments show that Anyprefer significantly improves model alignment performance across four main applications, covering 21 datasets, achieving average improvements of 18.55% in five natural language generation datasets, 3.66% in nine vision-language understanding datasets, 30.05% in three medical image analysis datasets, and 16.00% in four visuo-motor control tasks.</li>
<li><strong>摘要：</strong>高质量的偏好数据对于通过偏好学习使基础模型与人类价值观保持一致至关重要。但是，此类数据的手动注释通常是耗时且昂贵的。最近的方法通常采用一种自我奖励的方法，在该方法中，目标模型会生成和注释其自身的偏好数据，但这可能导致不准确，因为奖励模型与目标模型共享权重，从而扩大了固有的偏见。为了解决这些问题，我们提出了AnyPrefer，该框架旨在综合用于对齐目标模型的高质量偏好数据。 AnyPrefer将数据综合过程描绘成合作的两人马尔可夫游戏，目标模型和法官模型共同合作。在这里，引入了一系列外部工具，以协助法官模型准确奖励目标模型的响应，从而减轻奖励过程中的偏见。此外，还引入了反馈机制，以优化两个模型的提示，增强协作并提高数据质量。合成的数据被编译到新的偏好数据集AnyPrefer-V1中，该数据集由58K高质量的首选项对组成。广泛的实验表明，AnyPrefer显着提高了四个主要应用程序的模型对齐性能，涵盖了21个数据集，在五个自然语言生成数据集中达到了18.55％的平均改善，在九个Vision-Language Geeldment Geeldment数据集中3.66％，三个医学图像分析数据集中的30.05％，以及四个Visuo-Motor-Motor-Motor-Motor-Motor-Motor-Motor中的16.00％。</li>
</ul>

<h3>Title: Marine Snow Removal Using Internally Generated Pseudo Ground Truth</h3>
<ul>
<li><strong>Authors: </strong>Alexandra Malyugina, Guoxi Huang, Eduardo Ruiz, Benjamin Leslie, Nantheera Anantrasirichai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19289">https://arxiv.org/abs/2504.19289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19289">https://arxiv.org/pdf/2504.19289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19289]] Marine Snow Removal Using Internally Generated Pseudo Ground Truth(https://arxiv.org/abs/2504.19289)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Underwater videos often suffer from degraded quality due to light absorption, scattering, and various noise sources. Among these, marine snow, which is suspended organic particles appearing as bright spots or noise, significantly impacts machine vision tasks, particularly those involving feature matching. Existing methods for removing marine snow are ineffective due to the lack of paired training data. To address this challenge, this paper proposes a novel enhancement framework that introduces a new approach for generating paired datasets from raw underwater videos. The resulting dataset consists of paired images of generated snowy and snow, free underwater videos, enabling supervised training for video enhancement. We describe the dataset creation process, highlight its key characteristics, and demonstrate its effectiveness in enhancing underwater image restoration in the absence of ground truth.</li>
<li><strong>摘要：</strong>水下视频通常由于吸收光，散射和各种噪声来源而遭受质量下降的质量。其中，悬挂的有机颗粒的海洋雪是明亮的斑点或噪声，会显着影响机器视觉任务，尤其是涉及特征匹配的任务。由于缺乏配对的训练数据，现有的去除海洋雪的方法无效。为了应对这一挑战，本文提出了一个新颖的增强框架，该框架引入了一种新方法，用于从原始水下视频中生成配对的数据集。由此产生的数据集由生成的雪和雪，免费的水下视频的配对图像组成，从而为视频增强提供了监督的培训。我们描述了数据集创建过程，强调其关键特征，并在没有地面真理的情况下证明了其在增强水下图像恢复方面的有效性。</li>
</ul>

<h3>Title: Flow Along the K-Amplitude for Generative Modeling</h3>
<ul>
<li><strong>Authors: </strong>Weitao Du, Shuning Chang, Jiasheng Tang, Yu Rong, Fan Wang, Shengchao Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19353">https://arxiv.org/abs/2504.19353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19353">https://arxiv.org/pdf/2504.19353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19353]] Flow Along the K-Amplitude for Generative Modeling(https://arxiv.org/abs/2504.19353)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In this work, we propose a novel generative learning paradigm, K-Flow, an algorithm that flows along the $K$-amplitude. Here, $k$ is a scaling parameter that organizes frequency bands (or projected coefficients), and amplitude describes the norm of such projected coefficients. By incorporating the $K$-amplitude decomposition, K-Flow enables flow matching across the scaling parameter as time. We discuss three venues and six properties of K-Flow, from theoretical foundations, energy and temporal dynamics, and practical applications, respectively. Specifically, from the practical usage perspective, K-Flow allows steerable generation by controlling the information at different scales. To demonstrate the effectiveness of K-Flow, we conduct experiments on unconditional image generation, class-conditional image generation, and molecule assembly generation. Additionally, we conduct three ablation studies to demonstrate how K-Flow steers scaling parameter to effectively control the resolution of image generation.</li>
<li><strong>摘要：</strong>在这项工作中，我们提出了一种新颖的生成学习范式，k-flow，这是一种沿着$ k $ aplitude流动的算法。在这里，$ k $是一个缩放参数，该参数组织频带（或投影系数），幅度描述了此类预测系数的规范。通过合并$ k $  - 振幅的分解，k-flow可以作为时间在缩放参数上匹配流量。我们分别从理论基础，能量和时间动力学以及实际应用中讨论了K-Flow的三个场所和六个特性。具体而言，从实际用法的角度来看，K-Flow可以通过在不同尺度上控制信息来传达生成。为了证明k-flow的有效性，我们对无条件图像产生，阶级图像产生和分子组装产生进行实验。此外，我们进行了三项消融研究，以证明k-flow转向缩放参数如何有效地控制图像产生的分辨率。</li>
</ul>

<h3>Title: Boosting 3D Liver Shape Datasets with Diffusion Models and Implicit Neural Representations</h3>
<ul>
<li><strong>Authors: </strong>Khoa Tuan Nguyen, Francesca Tozzi, Wouter Willaert, Joris Vankerschaver, Nikdokht Rashidian, Wesley De Neve</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19402">https://arxiv.org/abs/2504.19402</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19402">https://arxiv.org/pdf/2504.19402</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19402]] Boosting 3D Liver Shape Datasets with Diffusion Models and Implicit Neural Representations(https://arxiv.org/abs/2504.19402)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>While the availability of open 3D medical shape datasets is increasing, offering substantial benefits to the research community, we have found that many of these datasets are, unfortunately, disorganized and contain artifacts. These issues limit the development and training of robust models, particularly for accurate 3D reconstruction tasks. In this paper, we examine the current state of available 3D liver shape datasets and propose a solution using diffusion models combined with implicit neural representations (INRs) to augment and expand existing datasets. Our approach utilizes the generative capabilities of diffusion models to create realistic, diverse 3D liver shapes, capturing a wide range of anatomical variations and addressing the problem of data scarcity. Experimental results indicate that our method enhances dataset diversity, providing a scalable solution to improve the accuracy and reliability of 3D liver reconstruction and generation in medical applications. Finally, we suggest that diffusion models can also be applied to other downstream tasks in 3D medical imaging.</li>
<li><strong>摘要：</strong>虽然开放3D医学形状数据集的可用性正在增加，这为研究界带来了可观的好处，但我们发现其中许多数据集杂乱无章并包含伪像。这些问题限制了健壮模型的开发和培训，特别是对于准确的3D重建任务。在本文中，我们检查了可用的3D肝脏形状数据集的当前状态，并使用与隐式神经表示（INR）相结合的扩散模型提出了一个解决方案，以增强和扩展现有数据集。我们的方法利用扩散模型的生成能力来创建现实的，多样化的3D肝形状，捕获各种解剖学变化并解决数据稀缺问题。实验结果表明，我们的方法增强了数据集多样性，提供了可扩展的解决方案，以提高3D肝脏重建和医疗应用中产生的准确性和可靠性。最后，我们建议扩散模型也可以应用于3D医学成像中的其他下游任务。</li>
</ul>

<h3>Title: EarthMapper: Visual Autoregressive Models for Controllable Bidirectional Satellite-Map Translation</h3>
<ul>
<li><strong>Authors: </strong>Zhe Dong, Yuzhe Sun, Tianzhu Liu, Wangmeng Zuo, Yanfeng Gu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19432">https://arxiv.org/abs/2504.19432</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19432">https://arxiv.org/pdf/2504.19432</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19432]] EarthMapper: Visual Autoregressive Models for Controllable Bidirectional Satellite-Map Translation(https://arxiv.org/abs/2504.19432)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Satellite imagery and maps, as two fundamental data modalities in remote sensing, offer direct observations of the Earth's surface and human-interpretable geographic abstractions, respectively. The task of bidirectional translation between satellite images and maps (BSMT) holds significant potential for applications in urban planning and disaster response. However, this task presents two major challenges: first, the absence of precise pixel-wise alignment between the two modalities substantially complicates the translation process; second, it requires achieving both high-level abstraction of geographic features and high-quality visual synthesis, which further elevates the technical complexity. To address these limitations, we introduce EarthMapper, a novel autoregressive framework for controllable bidirectional satellite-map translation. EarthMapper employs geographic coordinate embeddings to anchor generation, ensuring region-specific adaptability, and leverages multi-scale feature alignment within a geo-conditioned joint scale autoregression (GJSA) process to unify bidirectional translation in a single training cycle. A semantic infusion (SI) mechanism is introduced to enhance feature-level consistency, while a key point adaptive guidance (KPAG) mechanism is proposed to dynamically balance diversity and precision during inference. We further contribute CNSatMap, a large-scale dataset comprising 302,132 precisely aligned satellite-map pairs across 38 Chinese cities, enabling robust benchmarking. Extensive experiments on CNSatMap and the New York dataset demonstrate EarthMapper's superior performance, achieving significant improvements in visual realism, semantic consistency, and structural fidelity over state-of-the-art methods. Additionally, EarthMapper excels in zero-shot tasks like in-painting, out-painting and coordinate-conditional generation, underscoring its versatility.</li>
<li><strong>摘要：</strong>卫星图像和地图作为遥感中的两个基本数据方式，分别提供了对地球表面和人类解动地理抽象的直接观察。卫星图像和地图（BSMT）之间双向翻译的任务具有在城市规划和灾难响应中应用的巨大潜力。但是，这项任务提出了两个主要挑战：首先，两种方式之间缺乏精确的像素对齐，这实质上使翻译过程变得复杂。其次，它需要实现地理特征的高级抽象和高质量的视觉合成，这进一步提高了技术复杂性。为了解决这些局限性，我们介绍了EarthMapper，这是一种新型自回归框架，用于可控的双向卫星图翻译。 EarthMapper使用地理坐标嵌入来锚定生成，确保特定区域的适应性，并利用地理条件的联合尺度自动进度（GJSA）过程中的多尺度特征对齐过程，以在单个训练周期内统一双向翻译。引入了语义输注（SI）机制以增强特征级的一致性，而关键点自适应引导（KPAG）机制提出了在推理过程中动态平衡多样性和精度。我们进一步贡献了CNSATMAP，这是一个大规模数据集，包括302,132个精确比对的卫星映射对，跨38个中国城市，实现了强大的基准测试。关于CNSATMAP和纽约数据集的广泛实验证明了EarthMapper的出色表现，从而在视觉现实主义，语义一致性和结构性忠诚度上取得了显着改善。此外，EarthMapper在零拍的任务中出色，例如镶嵌，外观和坐标条件生成，强调其多功能性。</li>
</ul>

<h3>Title: Masked Language Prompting for Generative Data Augmentation in Few-shot Fashion Style Recognition</h3>
<ul>
<li><strong>Authors: </strong>Yuki Hirakawa, Ryotaro Shimizu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19455">https://arxiv.org/abs/2504.19455</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19455">https://arxiv.org/pdf/2504.19455</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19455]] Masked Language Prompting for Generative Data Augmentation in Few-shot Fashion Style Recognition(https://arxiv.org/abs/2504.19455)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Constructing dataset for fashion style recognition is challenging due to the inherent subjectivity and ambiguity of style concepts. Recent advances in text-to-image models have facilitated generative data augmentation by synthesizing images from labeled data, yet existing methods based solely on class names or reference captions often fail to balance visual diversity and style consistency. In this work, we propose \textbf{Masked Language Prompting (MLP)}, a novel prompting strategy that masks selected words in a reference caption and leverages large language models to generate diverse yet semantically coherent completions. This approach preserves the structural semantics of the original caption while introducing attribute-level variations aligned with the intended style, enabling style-consistent and diverse image generation without fine-tuning. Experimental results on the FashionStyle14 dataset demonstrate that our MLP-based augmentation consistently outperforms class-name and caption-based baselines, validating its effectiveness for fashion style recognition under limited supervision.</li>
<li><strong>摘要：</strong>由于风格概念的固有主观性和模棱两可，为时尚风格识别构建数据集具有挑战性。文本到图像模型的最新进展通过从标记的数据中综合图像来促进了生成数据的增强，但是现有的方法仅基于班级名称或参考字幕通常无法平衡视觉多样性和样式一致性。在这项工作中，我们提出\ textbf {蒙版语言提示（MLP）}，这是一种新颖的提示策略，在参考文章中掩盖了选定的单词，并利用大型语言模型来生成多样化但具有语义上的连贯的完成。这种方法保留了原始字幕的结构语义，同时引入了与预期样式相一致的属性级别的变化，从而实现了风格符合风格的和多样化的图像生成而无需微调。 FashionStyle14数据集的实验结果表明，我们基于MLP的增强始终优于基于类名称和字幕的基线，从而在有限的监督下验证了其对时尚风格识别的有效性。</li>
</ul>

<h3>Title: CasaGPT: Cuboid Arrangement and Scene Assembly for Interior Design</h3>
<ul>
<li><strong>Authors: </strong>Weitao Feng, Hang Zhou, Jing Liao, Li Cheng, Wenbo Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19478">https://arxiv.org/abs/2504.19478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19478">https://arxiv.org/pdf/2504.19478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19478]] CasaGPT: Cuboid Arrangement and Scene Assembly for Interior Design(https://arxiv.org/abs/2504.19478)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present a novel approach for indoor scene synthesis, which learns to arrange decomposed cuboid primitives to represent 3D objects within a scene. Unlike conventional methods that use bounding boxes to determine the placement and scale of 3D objects, our approach leverages cuboids as a straightforward yet highly effective alternative for modeling objects. This allows for compact scene generation while minimizing object intersections. Our approach, coined CasaGPT for Cuboid Arrangement and Scene Assembly, employs an autoregressive model to sequentially arrange cuboids, producing physically plausible scenes. By applying rejection sampling during the fine-tuning stage to filter out scenes with object collisions, our model further reduces intersections and enhances scene quality. Additionally, we introduce a refined dataset, 3DFRONT-NC, which eliminates significant noise presented in the original dataset, 3D-FRONT. Extensive experiments on the 3D-FRONT dataset as well as our dataset demonstrate that our approach consistently outperforms the state-of-the-art methods, enhancing the realism of generated scenes, and providing a promising direction for 3D scene synthesis.</li>
<li><strong>摘要：</strong>我们为室内场景综合提供了一种新颖的方法，该方法学会了安排分解的立方体原语，以在场景中表示3D对象。与使用边界框来确定3D对象的放置和比例的传统方法不同，我们的方法利用Cuboid作为建模对象的直接但高效的替代方案。这允许在最小化对象交叉点的同时生成紧凑的场景。我们的方法是用于立方排列和场景组件的Casagpt，采用自回归模型来顺序排列立方体，从而产生物理上合理的场景。通过在微调阶段应用拒绝采样以通过对象碰撞过滤场景，我们的模型进一步降低了交叉点并增强了场景质量。此外，我们引入了一个精制的数据集3DFront-NC，该数据集消除了原始数据集中3D-Front中显示的明显噪声。在3D-Front数据集以及我们的数据集上进行的广泛实验表明，我们的方法始终优于最先进的方法，增强了生成的场景的现实性，并为3D场景合成提供了有希望的方向。</li>
</ul>

<h3>Title: An Automated Reinforcement Learning Reward Design Framework with Large Language Model for Cooperative Platoon Coordination</h3>
<ul>
<li><strong>Authors: </strong>Dixiao Wei, Peng Yi, Jinlong Lei, Yiguang Hong, Yuchuan Du</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19480">https://arxiv.org/abs/2504.19480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19480">https://arxiv.org/pdf/2504.19480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19480]] An Automated Reinforcement Learning Reward Design Framework with Large Language Model for Cooperative Platoon Coordination(https://arxiv.org/abs/2504.19480)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) has demonstrated excellent decision-making potential in platoon coordination problems. However, due to the variability of coordination goals, the complexity of the decision problem, and the time-consumption of trial-and-error in manual design, finding a well performance reward function to guide RL training to solve complex platoon coordination problems remains challenging. In this paper, we formally define the Platoon Coordination Reward Design Problem (PCRDP), extending the RL-based cooperative platoon coordination problem to incorporate automated reward function generation. To address PCRDP, we propose a Large Language Model (LLM)-based Platoon coordination Reward Design (PCRD) framework, which systematically automates reward function discovery through LLM-driven initialization and iterative optimization. In this method, LLM first initializes reward functions based on environment code and task requirements with an Analysis and Initial Reward (AIR) module, and then iteratively optimizes them based on training feedback with an evolutionary module. The AIR module guides LLM to deepen their understanding of code and tasks through a chain of thought, effectively mitigating hallucination risks in code generation. The evolutionary module fine-tunes and reconstructs the reward function, achieving a balance between exploration diversity and convergence stability for training. To validate our approach, we establish six challenging coordination scenarios with varying complexity levels within the Yangtze River Delta transportation network simulation. Comparative experimental results demonstrate that RL agents utilizing PCRD-generated reward functions consistently outperform human-engineered reward functions, achieving an average of 10\% higher performance metrics in all scenarios.</li>
<li><strong>摘要：</strong>强化学习（RL）在排协调问题中表现出了极好的决策潜力。但是，由于协调目标的差异，决策问题的复杂性以及手动设计中反复试验的时间消耗，因此找到良好的性能奖励功能来指导RL培训以解决复杂的排协调问题仍然具有挑战性。在本文中，我们正式定义了排协调奖励设计问题（PCRDP），从而扩展了基于RL的合作排协调问题，以结合自动奖励功能的生成。为了解决PCRDP，我们提出了一个基于大型语言模型（LLM）基于排的协调奖励设计（PCRD）框架，该框架通过LLM驱动的初始化和迭代优化系统地自动化奖励功能发现。在此方法中，LLM首先根据环境代码和任务要求初始化奖励功能，并通过分析和初始奖励（AIR）模块初始化奖励功能，然后根据训练反馈使用进化模块对它们进行迭代优化它们。空气模块指导LLM通过一系列思想来加深他们对代码和任务的理解，从而有效地减轻代码生成中的幻觉风险。进化模块微调和重建奖励功能，在训练多样性和融合稳定性之间取得了平衡。为了验证我们的方法，我们建立了六个具有挑战性的协调方案，在长江三角洲运输网络模拟中具有不同的复杂性水平。比较实验结果表明，利用PCRD生成的奖励功能的RL药物始终超过人工设计的奖励功能，在所有情况下平均达到10 \％的性能指标。</li>
</ul>

<h3>Title: SynergyAmodal: Deocclude Anything with Text Control</h3>
<ul>
<li><strong>Authors: </strong>Xinyang Li, Chengjie Yi, Jiawei Lai, Mingbao Lin, Yansong Qu, Shengchuan Zhang, Liujuan Cao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19506">https://arxiv.org/abs/2504.19506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19506">https://arxiv.org/pdf/2504.19506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19506]] SynergyAmodal: Deocclude Anything with Text Control(https://arxiv.org/abs/2504.19506)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Image deocclusion (or amodal completion) aims to recover the invisible regions (\ie, shape and appearance) of occluded instances in images. Despite recent advances, the scarcity of high-quality data that balances diversity, plausibility, and fidelity remains a major obstacle. To address this challenge, we identify three critical elements: leveraging in-the-wild image data for diversity, incorporating human expertise for plausibility, and utilizing generative priors for fidelity. We propose SynergyAmodal, a novel framework for co-synthesizing in-the-wild amodal datasets with comprehensive shape and appearance annotations, which integrates these elements through a tripartite data-human-model collaboration. First, we design an occlusion-grounded self-supervised learning algorithm to harness the diversity of in-the-wild image data, fine-tuning an inpainting diffusion model into a partial completion diffusion model. Second, we establish a co-synthesis pipeline to iteratively filter, refine, select, and annotate the initial deocclusion results of the partial completion diffusion model, ensuring plausibility and fidelity through human expert guidance and prior model constraints. This pipeline generates a high-quality paired amodal dataset with extensive category and scale diversity, comprising approximately 16K pairs. Finally, we train a full completion diffusion model on the synthesized dataset, incorporating text prompts as conditioning signals. Extensive experiments demonstrate the effectiveness of our framework in achieving zero-shot generalization and textual controllability. Our code, dataset, and models will be made publicly available at this https URL.</li>
<li><strong>摘要：</strong>图像除含义（或Amodal完成）旨在恢复图像中遮挡实例的无形区域（\ ie，形状和外观）。尽管有最近的进步，但平衡多样性，合理性和忠诚度的高质量数据的稀缺仍然是一个主要障碍。为了应对这一挑战，我们确定了三个关键要素：利用野外图像数据以获取多样性，将人类的专业知识纳入合理性，并利用生成的先验来实现保真度。我们提出了Synergyamodal，这是一个具有全面形状和外观注释的野外野外野外数据集的新型框架，该数据集通过三方数据 - 人类模型协作来整合这些元素。首先，我们设计了一种遮挡的自我监督学习算法，以利用野外图像数据的多样性，将镶嵌扩散模型微调为部分完成扩散模型。其次，我们建立了一个共同合成管道，以迭代过滤，完善，选择和注释部分完成扩散模型的初始去概括结果，从而通过人类专家指导和先前的模型约束来确保合理性和忠诚度。该管道生成具有广泛类别和比例多样性的高质量配对的Amodal数据集，其中约为16K对。最后，我们在合成数据集上训练完整的完成扩散模型，并将文本提示作为条件信号。广泛的实验证明了我们的框架在实现零弹性概括和文本可控性方面的有效性。我们的代码，数据集和模型将在此HTTPS URL上公开可用。</li>
</ul>

<h3>Title: Point2Quad: Generating Quad Meshes from Point Clouds via Face Prediction</h3>
<ul>
<li><strong>Authors: </strong>Zezeng Li, Zhihui Qi, Weimin Wang, Ziliang Wang, Junyi Duan, Na Lei</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19545">https://arxiv.org/abs/2504.19545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19545">https://arxiv.org/pdf/2504.19545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19545]] Point2Quad: Generating Quad Meshes from Point Clouds via Face Prediction(https://arxiv.org/abs/2504.19545)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Quad meshes are essential in geometric modeling and computational mechanics. Although learning-based methods for triangle mesh demonstrate considerable advancements, quad mesh generation remains less explored due to the challenge of ensuring coplanarity, convexity, and quad-only meshes. In this paper, we present Point2Quad, the first learning-based method for quad-only mesh generation from point clouds. The key idea is learning to identify quad mesh with fused pointwise and facewise features. Specifically, Point2Quad begins with a k-NN-based candidate generation considering the coplanarity and squareness. Then, two encoders are followed to extract geometric and topological features that address the challenge of quad-related constraints, especially by combining in-depth quadrilaterals-specific characteristics. Subsequently, the extracted features are fused to train the classifier with a designed compound loss. The final results are derived after the refinement by a quad-specific post-processing. Extensive experiments on both clear and noise data demonstrate the effectiveness and superiority of Point2Quad, compared to baseline methods under comprehensive metrics.</li>
<li><strong>摘要：</strong>四方网格在几何建模和计算力学中至关重要。尽管基于学习的三角形网格的方法表现出很大的进步，但由于确保共同性，凸度和仅Quad的网格的挑战，Quad网格的生成仍然较少。在本文中，我们介绍了Point2Quad，这是第一个基于学习的基于学习的方法，用于从点云中生成四倍的网格。关键的想法是学会识别四分球网格，并呈融合，面向和面向特征。具体而言，Point2Quad始于基于K-NN的候选人的一代，考虑到Coplanarity和Squarentes。然后，遵循两个编码器来提取几何和拓扑特征，以解决与四边形相关约束的挑战，尤其是通过结合深度四边形特异性特征的特征。随后，提取的功能融合在一起以训练分类器的设计复合损失。最终结果是在精炼之后通过四足特定后处理后得出的。与综合指标下的基线方法相比，对清晰和噪声数据的广泛实验证明了Point2quad的有效性和优势。</li>
</ul>

<h3>Title: Image Generation Method Based on Heat Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Pengfei Zhang, Shouqing Jia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19600">https://arxiv.org/abs/2504.19600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19600">https://arxiv.org/pdf/2504.19600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19600]] Image Generation Method Based on Heat Diffusion Models(https://arxiv.org/abs/2504.19600)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Denoising Diffusion Probabilistic Models (DDPMs) achieve high-quality image generation without adversarial training, but they process images as a whole. Since adjacent pixels are highly likely to belong to the same object, we propose the Heat Diffusion Model (HDM) to further preserve image details and generate more realistic images. HDM is a model that incorporates pixel-level operations while maintaining the same training process as DDPM. In HDM, the discrete form of the two-dimensional heat equation is integrated into the diffusion and generation formulas of DDPM, enabling the model to compute relationships between neighboring pixels during image processing. Our experiments demonstrate that HDM can generate higher-quality samples compared to models such as DDPM, Consistency Diffusion Models (CDM), Latent Diffusion Models (LDM), and Vector Quantized Generative Adversarial Networks (VQGAN).</li>
<li><strong>摘要：</strong>将扩散概率模型（DDPM）降解，无需对抗训练就可以生成高质量的图像，但它们整体上处理图像。由于相邻的像素很可能属于同一对象，因此我们提出了热扩散模型（HDM），以进一步保留图像细节并生成更真实的图像。 HDM是一种结合像素级操作的模型，同时保持与DDPM相同的训练过程。在HDM中，将二维热方程的离散形式集成到DDPM的扩散和生成公式中，从而使模型能够在图像处理过程中计算相邻像素之间的关系。我们的实验表明，与DDPM，一致性扩散模型（CDM），潜扩散模型（LDM）（LDM）（LDM）和Vector量化量化的生成对抗网络（VQGAN）相比，HDM可以生成更高质量的样品。</li>
</ul>

<h3>Title: DiVE: Efficient Multi-View Driving Scenes Generation Based on Video Diffusion Transformer</h3>
<ul>
<li><strong>Authors: </strong>Junpeng Jiang, Gangyi Hong, Miao Zhang, Hengtong Hu, Kun Zhan, Rui Shao, Liqiang Nie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19614">https://arxiv.org/abs/2504.19614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19614">https://arxiv.org/pdf/2504.19614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19614]] DiVE: Efficient Multi-View Driving Scenes Generation Based on Video Diffusion Transformer(https://arxiv.org/abs/2504.19614)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Collecting multi-view driving scenario videos to enhance the performance of 3D visual perception tasks presents significant challenges and incurs substantial costs, making generative models for realistic data an appealing alternative. Yet, the videos generated by recent works suffer from poor quality and spatiotemporal consistency, undermining their utility in advancing perception tasks under driving scenarios. To address this gap, we propose DiVE, a diffusion transformer-based generative framework meticulously engineered to produce high-fidelity, temporally coherent, and cross-view consistent multi-view videos, aligning seamlessly with bird's-eye view layouts and textual descriptions. DiVE leverages a unified cross-attention and a SketchFormer to exert precise control over multimodal data, while incorporating a view-inflated attention mechanism that adds no extra parameters, thereby guaranteeing consistency across views. Despite these advancements, synthesizing high-resolution videos under multimodal constraints introduces dual challenges: investigating the optimal classifier-free guidance coniguration under intricate multi-condition inputs and mitigating excessive computational latency in high-resolution rendering--both of which remain underexplored in prior researches. To resolve these limitations, we introduce two innovations: Multi-Control Auxiliary Branch Distillation, which streamlines multi-condition CFG selection while circumventing high computational overhead, and Resolution Progressive Sampling, a training-free acceleration strategy that staggers resolution scaling to reduce high latency due to high resolution. These innovations collectively achieve a 2.62x speedup with minimal quality degradation. Evaluated on the nuScenes dataset, DiVE achieves SOTA performance in multi-view video generation, yielding photorealistic outputs with exceptional temporal and cross-view coherence.</li>
<li><strong>摘要：</strong>收集多视图驱动方案视频以增强3D视觉感知任务的性能提出了重大挑战并带来了大量成本，从而使现实数据的生成模型成为有吸引力的选择。然而，最近作品产生的视频质量较差和时空的一致性却破坏了它们在驾驶场景下推进感知任务的效用。为了解决这一差距，我们提出了Dive，这是一种基于扩散变压器的生成框架，经过精心设计，可以产生高保真性，时间连贯和跨视图一致的多视频视频，与Bird's-s-eye视图布局和文本描述无缝地对齐。潜水利用统一的跨注意事项和素描图表对多模式数据进行精确控制，同时结合了一个视图上膨胀的注意机制，该机制没有增加额外的参数，从而确保了跨视图的一致性。尽管取得了这些进步，但在多模式约束下的合成高分辨率视频引入了双重挑战：调查在复杂的多条件输入下进行的无最佳分类器指导共同处理，并减轻先前研究中的高分辨率构成构成中的过度计算潜伏期。为了解决这些局限性，我们介绍了两项创新：多控制辅助分支蒸馏，它简化了多条件CFG的选择，同时避免了高计算开销，而分辨率进行了无训练的加速策略，这是一种由于高分辨率而降低了高延迟的分辨率。这些创新共同实现了2.62倍的速度，质量降低最少。在Nuscenes数据集上进行了评估，潜水在多视频视频生成中实现了SOTA性能，从而产生了具有特殊时间和跨视图相干性的影片输出。</li>
</ul>

<h3>Title: ClearVision: Leveraging CycleGAN and SigLIP-2 for Robust All-Weather Classification in Traffic Camera Imagery</h3>
<ul>
<li><strong>Authors: </strong>Anush Lakshman Sivaraman, Kojo Adu-Gyamfi, Ibne Farabi Shihab, Anuj Sharma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19684">https://arxiv.org/abs/2504.19684</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19684">https://arxiv.org/pdf/2504.19684</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19684]] ClearVision: Leveraging CycleGAN and SigLIP-2 for Robust All-Weather Classification in Traffic Camera Imagery(https://arxiv.org/abs/2504.19684)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Accurate weather classification from low-quality traffic camera imagery remains a challenging task, particularly under adverse nighttime conditions. In this study, we propose a scalable framework that combines generative domain adaptation with efficient contrastive learning to enhance classification performance. Using CycleGAN-based domain translation, we improve the quality of nighttime images, enabling better feature extraction by downstream models. While the baseline EVA-02 model employing CLIP-based contrastive loss achieves an overall accuracy of 96.55\%, it exhibits a significant performance gap between daytime (97.21\%) and nighttime conditions (63.40\%). Replacing CLIP with the lightweight SigLIP-2 (Sigmoid contrastive loss) achieves a competitive overall accuracy of 94.00\%, with substantial improvements in nighttime performance (85.90\% accuracy). The combination of Vision-SigLIP-2, Text-SigLIP-2, CycleGAN, and contrastive training achieves the best nighttime accuracy (85.90\%) among all models tested, while EVA-02 with CycleGAN maintains the highest overall accuracy (97.01\%) and per-class accuracies. These findings demonstrate the potential of combining domain adaptation and efficient contrastive learning to build practical, resource-efficient weather classification systems for intelligent transportation infrastructure.</li>
<li><strong>摘要：</strong>低质量交通摄像头图像的准确天气分类仍然是一项具有挑战性的任务，尤其是在不良夜间条件下。在这项研究中，我们提出了一个可扩展的框架，该框架结合了生成域的适应性与有效的对比度学习以增强分类性能。使用基于Cyclegan的域翻译，我们提高了夜间图像的质量，从而可以通过下游模型更好地提取特征。尽管采用基于夹子的对比损失的基线EVA-02模型达到了96.55 \％的总体精度，但它在白天（97.21 \％）和夜间条件（63.40 \％）之间表现出显着的性能差距。用轻量级siglip-2（Sigmoid对比度损失）代替夹子的总体准确度为94.00％\％，夜间性能（85.90 \％的精度）有了很大的提高。在所有测试的模型中，Vision-Sigiglip-2，Text-Siglip-2，Cyclegan和对比度训练的组合达到了最佳的夜间准确性（85.90 \％），而具有Cyclegan的EVA-02保持了最高的总体准确性（97.01 \％）和每类精度。这些发现证明了结合域适应性和有效的对比度学习的潜力，以建立实用，资源有效的天气分类系统，以用于智能运输基础设施。</li>
</ul>

<h3>Title: RepText: Rendering Visual Text via Replicating</h3>
<ul>
<li><strong>Authors: </strong>Haofan Wang, Yujia Xu, Yimeng Li, Junchen Li, Chaowei Zhang, Jing Wang, Kejia Yang, Zhibo Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19724">https://arxiv.org/abs/2504.19724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19724">https://arxiv.org/pdf/2504.19724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19724]] RepText: Rendering Visual Text via Replicating(https://arxiv.org/abs/2504.19724)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Although contemporary text-to-image generation models have achieved remarkable breakthroughs in producing visually appealing images, their capacity to generate precise and flexible typographic elements, especially non-Latin alphabets, remains constrained. To address these limitations, we start from an naive assumption that text understanding is only a sufficient condition for text rendering, but not a necessary condition. Based on this, we present RepText, which aims to empower pre-trained monolingual text-to-image generation models with the ability to accurately render, or more precisely, replicate, multilingual visual text in user-specified fonts, without the need to really understand them. Specifically, we adopt the setting from ControlNet and additionally integrate language agnostic glyph and position of rendered text to enable generating harmonized visual text, allowing users to customize text content, font and position on their needs. To improve accuracy, a text perceptual loss is employed along with the diffusion loss. Furthermore, to stabilize rendering process, at the inference phase, we directly initialize with noisy glyph latent instead of random initialization, and adopt region masks to restrict the feature injection to only the text region to avoid distortion of the background. We conducted extensive experiments to verify the effectiveness of our RepText relative to existing works, our approach outperforms existing open-source methods and achieves comparable results to native multi-language closed-source models. To be more fair, we also exhaustively discuss its limitations in the end.</li>
<li><strong>摘要：</strong>尽管当代文本到图像生成模型在产生视觉上吸引人的图像方面取得了显着突破，但它们产生精确和灵活的印刷元素（尤其是非拉丁蛋白字母）的能力仍然受到限制。为了解决这些限制，我们从一个天真的假设开始，即文本理解只是文本渲染的足够条件，但不是必要的条件。基于此，我们介绍了reptext，该reptext旨在使预训练的单语文本对图像生成模型具有准确渲染或更精确的能力，或者更精确地复制，以用户指定的字体复制，多语言的视觉文本，而无需真正理解它们。具体来说，我们采用了ControlNet的设置，并集成了语言不可知论的字形和渲染文本的位置以启用生成统一的视觉文本，从而使用户可以自定义文本内容，字体和位置，并满足其需求。为了提高准确性，与扩散损失一起使用文本感知损失。此外，为了稳定渲染过程，在推理阶段，我们直接以嘈杂的字形潜在而不是随机初始化初始化，并采用区域掩码以将特征注入限制为仅到文本区域以避免背景变形。我们进行了广泛的实验，以验证相对于现有作品的reptext的有效性，我们的方法的表现优于现有的开源方法，并且取得了与本机多语言封闭源模型的可比结果。更公平，我们还详尽地讨论了它的局限性。</li>
</ul>

<h3>Title: Learning Brenier Potentials with Convex Generative Adversarial Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Claudia Drygala, Hanno Gottschalk, Thomas Kruse, Ségolène Martin, Annika Mütze</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19779">https://arxiv.org/abs/2504.19779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19779">https://arxiv.org/pdf/2504.19779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19779]] Learning Brenier Potentials with Convex Generative Adversarial Neural Networks(https://arxiv.org/abs/2504.19779)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Brenier proved that under certain conditions on a source and a target probability measure there exists a strictly convex function such that its gradient is a transport map from the source to the target distribution. This function is called the Brenier potential. Furthermore, detailed information on the Hölder regularity of the Brenier potential is available. In this work we develop the statistical learning theory of generative adversarial neural networks that learn the Brenier potential. As by the transformation of densities formula, the density of the generated measure depends on the second derivative of the Brenier potential, we develop the universal approximation theory of ReCU networks with cubic activation $\mathtt{ReCU}(x)=\max\{0,x\}^3$ that combines the favorable approximation properties of Hölder functions with a Lipschitz continuous density. In order to assure the convexity of such general networks, we introduce an adversarial training procedure for a potential function represented by the ReCU networks that combines the classical discriminator cross entropy loss with a penalty term that enforces (strict) convexity. We give a detailed decomposition of learning errors and show that for a suitable high penalty parameter all networks chosen in the adversarial min-max optimization problem are strictly convex. This is further exploited to prove the consistency of the learning procedure for (slowly) expanding network capacity. We also implement the described learning algorithm and apply it to a number of standard test cases from Gaussian mixture to image data as target distributions. As predicted in theory, we observe that the convexity loss becomes inactive during the training process and the potentials represented by the neural networks have learned convexity.</li>
<li><strong>摘要：</strong>Brenier证明，在某些条件下，在某些条件下，存在严格的凸函数，因此其梯度是从源到目标分布的传输图。此功能称为Brenier电位。此外，还提供有关Brenier潜力的Hölder规律性的详细信息。在这项工作中，我们开发了学习Brenier潜力的生成对抗神经网络的统计学习理论。 As by the transformation of densities formula, the density of the generated measure depends on the second derivative of the Brenier potential, we develop the universal approximation theory of ReCU networks with cubic activation $\mathtt{ReCU}(x)=\max\{0,x\}^3$ that combines the favorable approximation properties of Hölder functions with a Lipschitz continuous density.为了确保此类通用网络的凸度，我们为RECU网络所代表的潜在功能引入了对抗性训练程序，该程序将经典歧视者交叉熵损失与强制执行（严格）凸度的惩罚项相结合。我们给出了学习错误的详细分解，并表明，对于适当的高惩罚参数，在“对抗性最小值 - 最大优化问题”中选择的所有网络都严格凸出。进一步利用这是为了证明（缓慢）扩大网络容量的学习过程的一致性。我们还实施了所描述的学习算法，并将其应用于从高斯混合物到图像数据作为目标分布的许多标准测试用例。正如从理论上预测的那样，我们观察到在训练过程中凸度损失变得不活跃，而神经网络所代表的潜力已经学到了凸度。</li>
</ul>

<h3>Title: Heterophily-informed Message Passing</h3>
<ul>
<li><strong>Authors: </strong>Haishan Wang, Arno Solin, Vikas Garg</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19785">https://arxiv.org/abs/2504.19785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19785">https://arxiv.org/pdf/2504.19785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19785]] Heterophily-informed Message Passing(https://arxiv.org/abs/2504.19785)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Graph neural networks (GNNs) are known to be vulnerable to oversmoothing due to their implicit homophily assumption. We mitigate this problem with a novel scheme that regulates the aggregation of messages, modulating the type and extent of message passing locally thereby preserving both the low and high-frequency components of information. Our approach relies solely on learnt embeddings, obviating the need for auxiliary labels, thus extending the benefits of heterophily-aware embeddings to broader applications, e.g., generative modelling. Our experiments, conducted across various data sets and GNN architectures, demonstrate performance enhancements and reveal heterophily patterns across standard classification benchmarks. Furthermore, application to molecular generation showcases notable performance improvements on chemoinformatics benchmarks.</li>
<li><strong>摘要：</strong>图形神经网络（GNN）由于其隐式同质假设而容易受到过度厚度的影响。我们通过一种新的方案来缓解这个问题，该方案调节消息的聚集，调节消息传递的类型和程度，从而保留信息的低频和高频组成部分。我们的方法仅依赖于学习的嵌入，从而消除了对辅助标签的需求，从而将异性含有的嵌入的益处扩展到更广泛的应用，例如生成建模。我们的实验在各种数据集和GNN架构上进行，展示了性能提高，并揭示了跨标准分类基准的异性模式。此外，应用于分子生成，展示了化学信息信息基准的显着性能改善。</li>
</ul>

<h3>Title: Contextures: The Mechanism of Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Runtian Zhai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19792">https://arxiv.org/abs/2504.19792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19792">https://arxiv.org/pdf/2504.19792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19792]] Contextures: The Mechanism of Representation Learning(https://arxiv.org/abs/2504.19792)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This dissertation establishes the contexture theory to mathematically characterize the mechanism of representation learning, or pretraining. Despite the remarkable empirical success of foundation models, it is not very clear what representations they learn, and why these representations are useful for various downstream tasks. A scientific understanding of representation learning is critical, especially at this point when scaling up the model size is producing diminishing returns, and designing new pretraining methods is imperative for further progress. Prior work treated different representation learning methods quite differently, whereas the contexture theory provides a unified framework for analyzing these methods. The central argument is that a representation is learned from the association between the input X and a context variable A. We prove that if an encoder captures the maximum information of this association, in which case we say that the encoder learns the contexture, then it will be optimal on the class of tasks that are compatible with the context. We also show that a context is the most useful when the association between X and A is neither too strong nor too weak. The important implication of the contexture theory is that increasing the model size alone will achieve diminishing returns, and further advancements require better contexts. We demonstrate that many pretraining objectives can learn the contexture, including supervised learning, self-supervised learning, generative models, etc. Then, we introduce two general objectives -- SVME and KISE, for learning the contexture. We also show how to mix multiple contexts together, an effortless way to create better contexts from existing ones. Then, we prove statistical learning bounds for representation learning. Finally, we discuss the effect of the data distribution shift from pretraining to the downstream task.</li>
<li><strong>摘要：</strong>本论文建立了上下文理论，以数学表征表示或训练的机制。尽管基金会模型取得了显着的经验成功，但尚不清楚它们所学的表示形式，以及为什么这些表示对各种下游任务有用。对代表学习的科学理解至关重要，尤其是在扩大模型大小的情况下的回报减少时，设计新的预审进方法对于进一步的进步至关重要。先前的工作对不同的表示学习方法的处理方式完全不同，而情境理论为分析这些方法提供了一个统一的框架。中心论点是，从输入X与上下文变量A之间的关联中学到了表示形式。我们证明，如果编码器捕获了该关联的最大信息，在这种情况下，我们说编码器会学习上下文，那么它将在与上下文兼容的任务类别上是最佳的。我们还表明，当x和a之间的关联既不太强也不太弱时，上下文是最有用的。上下文理论的重要含义是，仅增加模型大小将获得减少的回报，进一步的进步需要更好的背景。我们证明，许多预处理的目标都可以学习上下文，包括监督学习，自学学习，生成模型等。然后，我们介绍了两个一般目标 -  SVME和KISE，以学习上下文。我们还展示了如何将多个上下文混合在一起，这是一种从现有上下文中创建更好上下文的轻松方法。然后，我们证明了代表学习的统计学习界限。最后，我们讨论了从审奥训练到下游任务的数据分布的效果。</li>
</ul>

<h3>Title: Mjölnir: A Deep Learning Parametrization Framework for Global Lightning Flash Density</h3>
<ul>
<li><strong>Authors: </strong>Minjong Cheon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, physics.ao-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19822">https://arxiv.org/abs/2504.19822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19822">https://arxiv.org/pdf/2504.19822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19822]] Mjölnir: A Deep Learning Parametrization Framework for Global Lightning Flash Density(https://arxiv.org/abs/2504.19822)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in AI-based weather forecasting models, such as FourCastNet, Pangu-Weather, and GraphCast, have demonstrated the remarkable ability of deep learning to emulate complex atmospheric dynamics. Building on this momentum, we propose Mjölnir, a novel deep learning-based framework for global lightning flash density parameterization. Trained on ERA5 atmospheric predictors and World Wide Lightning Location Network (WWLLN) observations at a daily temporal resolution and 1 degree spatial resolution, Mjölnir captures the nonlinear mapping between large-scale environmental conditions and lightning activity. The model architecture is based on the InceptionNeXt backbone with SENet, and a multi-task learning strategy to simultaneously predict lightning occurrence and magnitude. Extensive evaluations yield that Mollnir accurately reproduces the global distribution, seasonal variability, and regional characteristics of lightning activity, achieving a global Pearson correlation coefficient of 0.96 for annual mean fields. These results suggest that Mjölnir serves not only as an effective data-driven global lightning parameterization but also as a promising AI-based scheme for next-generation Earth system models (AI-ESMs).</li>
<li><strong>摘要：</strong>基于AI的天气预报模型（例如Fourcastnet，Pangu-Weather和Graphcast）的最新进展证明了深度学习模仿复杂大气动力学的非凡能力。在这种势头的基础上，我们提出了Mjölnir，这是一种基于全球闪电闪存密度参数化的新型深度学习框架。 Mjölnir在每日时间分辨率和1度空间分辨率的情况下对ERA5大气预测变量和全球闪电位置网络（WWLLN）观察进行了训练，Mjölnir捕获了大规模环境条件和闪电活动之间的非线性映射。该模型架构基于带有SENET的InceptionNext主链，以及同时预测闪电发生和幅度的多任务学习策略。广泛的评估产生了Mollnir准确地重现闪电活动的全球分布，季节性变异性和区域特征，对于年平均田地的全球Pearson相关系数为0.96。这些结果表明，Mjölnir不仅可以用作有效的数据驱动的全球闪电参数化，而且还可以作为下一代地球系统模型（AI-ESMS）的有希望的基于AI的有希望的方案。</li>
</ul>

<h3>Title: AnimateAnywhere: Rouse the Background in Human Image Animation</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Liu, Mingshuai Yao, Yabo Zhang, Xianhui Lin, Peiran Ren, Xiaoming Li, Ming Liu, Wangmeng Zuo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19834">https://arxiv.org/abs/2504.19834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19834">https://arxiv.org/pdf/2504.19834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19834]] AnimateAnywhere: Rouse the Background in Human Image Animation(https://arxiv.org/abs/2504.19834)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Human image animation aims to generate human videos of given characters and backgrounds that adhere to the desired pose sequence. However, existing methods focus more on human actions while neglecting the generation of background, which typically leads to static results or inharmonious movements. The community has explored camera pose-guided animation tasks, yet preparing the camera trajectory is impractical for most entertainment applications and ordinary users. As a remedy, we present an AnimateAnywhere framework, rousing the background in human image animation without requirements on camera trajectories. In particular, based on our key insight that the movement of the human body often reflects the motion of the background, we introduce a background motion learner (BML) to learn background motions from human pose sequences. To encourage the model to learn more accurate cross-frame correspondences, we further deploy an epipolar constraint on the 3D attention map. Specifically, the mask used to suppress geometrically unreasonable attention is carefully constructed by combining an epipolar mask and the current 3D attention map. Extensive experiments demonstrate that our AnimateAnywhere effectively learns the background motion from human pose sequences, achieving state-of-the-art performance in generating human animation results with vivid and realistic backgrounds. The source code and model will be available at this https URL.</li>
<li><strong>摘要：</strong>人类图像动画旨在生成遵循所需姿势序列的给定角色和背景的人类视频。但是，现有方法更多地集中在人类行动的同时，同时忽略了背景的产生，这通常会导致静态结果或inharmonious运动。社区探索了相机姿势指导的动画任务，但是对于大多数娱乐应用程序和普通用户来说，准备相机轨迹是不切实际的。作为一种补救措施，我们提出了一个AnimateAny Where框架，引起了人类图像动画的背景，而无需对摄像机轨迹进行要求。特别是，基于我们的关键见解，即人体的运动通常反映了背景的运动，我们引入了背景运动学习者（BML），以从人类姿势序列中学习背景运动。为了鼓励该模型学习更准确的跨框对应，我们进一步在3D注意图上部署了外观约束。具体而言，通过结合胶面膜和当前的3D注意图，仔细构建了用于抑制几何不合理注意的掩模。广泛的实验表明，我们的AnimateAny Where有效地从人类姿势序列中学习了背景运动，从而在以生动和逼真的背景生成人类动画结果中实现了最新的性能。源代码和模型将在此HTTPS URL上可用。</li>
</ul>

<h3>Title: CoherenDream: Boosting Holistic Text Coherence in 3D Generation via Multimodal Large Language Models Feedback</h3>
<ul>
<li><strong>Authors: </strong>Chenhan Jiang, Yihan Zeng, Hang Xu, Dit-Yan Yeung</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19860">https://arxiv.org/abs/2504.19860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19860">https://arxiv.org/pdf/2504.19860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19860]] CoherenDream: Boosting Holistic Text Coherence in 3D Generation via Multimodal Large Language Models Feedback(https://arxiv.org/abs/2504.19860)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Score Distillation Sampling (SDS) has achieved remarkable success in text-to-3D content generation. However, SDS-based methods struggle to maintain semantic fidelity for user prompts, particularly when involving multiple objects with intricate interactions. While existing approaches often address 3D consistency through multiview diffusion model fine-tuning on 3D datasets, this strategy inadvertently exacerbates text-3D alignment degradation. The limitation stems from SDS's inherent accumulation of view-independent biases during optimization, which progressively diverges from the ideal text alignment direction. To alleviate this limitation, we propose a novel SDS objective, dubbed as Textual Coherent Score Distillation (TCSD), which integrates alignment feedback from multimodal large language models (MLLMs). Our TCSD leverages cross-modal understanding capabilities of MLLMs to assess and guide the text-3D correspondence during the optimization. We further develop 3DLLaVA-CRITIC - a fine-tuned MLLM specialized for evaluating multiview text alignment in 3D generations. Additionally, we introduce an LLM-layout initialization that significantly accelerates optimization convergence through semantic-aware spatial configuration. Comprehensive evaluations demonstrate that our framework, CoherenDream, establishes state-of-the-art performance in text-aligned 3D generation across multiple benchmarks, including T$^3$Bench and TIFA subset. Qualitative results showcase the superior performance of CoherenDream in preserving textual consistency and semantic interactions. As the first study to incorporate MLLMs into SDS optimization, we also conduct extensive ablation studies to explore optimal MLLM adaptations for 3D generation tasks.</li>
<li><strong>摘要：</strong>得分蒸馏采样（SDS）在文本到3D内容生成方面取得了巨大的成功。但是，基于SDS的方法难以维持用户提示的语义保真度，尤其是在涉及多个具有复杂互动的对象时。尽管现有方法通常通过3D数据集上的多视频扩散模型微调来解决3D一致性，但该策略无意间加剧了Text-3D对齐降级。限制源于SDS在优化过程中固有的偏见的固有积累，这逐渐与理想的文本对齐方向逐渐不同。为了减轻这一限制，我们提出了一个新型的SDS目标，称为文本相干得分蒸馏（TCSD），该目标集成了多模式大语言模型（MLLMS）的对齐反馈。我们的TCSD利用了MLLM的跨模式理解能力来评估和指导优化过程中的文本3D对应关系。我们进一步开发了3dllava-Critic-专门评估3D代的多视文本对齐方式的微调MLLM。此外，我们引入了LLM-Layout初始化，该初始化通过语义感知的空间配置可以显着加速优化收敛。全面的评估表明，我们的框架CoherendReam在多个基准中，包括T $^3 $ bench和Tifa子集，在文本一致性的3D一代中建立了最先进的性能。定性结果展示了Coherendream在保留文本一致性和语义相互作用方面的出色表现。作为将MLLM纳入SDS优化的首次研究，我们还进行了广泛的消融研究，以探索3D生成任务的最佳MLLM适应性。</li>
</ul>

<h3>Title: DeeCLIP: A Robust and Generalizable Transformer-Based Framework for Detecting AI-Generated Images</h3>
<ul>
<li><strong>Authors: </strong>Mamadou Keita, Wassim Hamidouche, Hessen Bougueffa Eutamene, Abdelmalik Taleb-Ahmed, Abdenour Hadid</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19876">https://arxiv.org/abs/2504.19876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19876">https://arxiv.org/pdf/2504.19876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19876]] DeeCLIP: A Robust and Generalizable Transformer-Based Framework for Detecting AI-Generated Images(https://arxiv.org/abs/2504.19876)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper introduces DeeCLIP, a novel framework for detecting AI-generated images using CLIP-ViT and fusion learning. Despite significant advancements in generative models capable of creating highly photorealistic images, existing detection methods often struggle to generalize across different models and are highly sensitive to minor perturbations. To address these challenges, DeeCLIP incorporates DeeFuser, a fusion module that combines high-level and low-level features, improving robustness against degradations such as compression and blurring. Additionally, we apply triplet loss to refine the embedding space, enhancing the model's ability to distinguish between real and synthetic content. To further enable lightweight adaptation while preserving pre-trained knowledge, we adopt parameter-efficient fine-tuning using low-rank adaptation (LoRA) within the CLIP-ViT backbone. This approach supports effective zero-shot learning without sacrificing generalization. Trained exclusively on 4-class ProGAN data, DeeCLIP achieves an average accuracy of 89.00% on 19 test subsets composed of generative adversarial network (GAN) and diffusion models. Despite having fewer trainable parameters, DeeCLIP outperforms existing methods, demonstrating superior robustness against various generative models and real-world distortions. The code is publicly available at this https URL for research purposes.</li>
<li><strong>摘要：</strong>本文介绍了Deeclip，这是一个新型的框架，用于使用剪辑和融合学习来检测AI生成的图像。尽管能够创建高度逼真的图像的生成模型中取得了重大进步，但现有的检测方法通常很难跨越不同的模型，并且对较小的扰动非常敏感。为了应对这些挑战，Deeclip结合了Deefuser，这是一种结合高级和低水平特征的融合模块，改善了针对压缩和模糊等降解的稳健性。此外，我们应用三重态损失以完善嵌入空间，增强模型区分真实和合成内容的能力。为了在保留预训练的知识的同时进一步启用轻质适应性，我们在夹子式主链中使用低级适应性（LORA）采用参数有效的微调。这种方法支持有效的零拍学习，而无需牺牲概括。 Deeclip仅对4级Progan数据进行了培训，在由生成对抗网络（GAN）和扩散模型组成的19个测试子集上的平均准确度为89.00％。尽管可训练的参数较少，但Deeclip比现有的方法优于现有的方法，证明了针对各种生成模型和现实世界变形的出色鲁棒性。该代码可在此HTTPS URL上公开可用，以进行研究。</li>
</ul>

<h3>Title: CineVerse: Consistent Keyframe Synthesis for Cinematic Scene Composition</h3>
<ul>
<li><strong>Authors: </strong>Quynh Phung, Long Mai, Fabian David Caba Heilbron, Feng Liu, Jia-Bin Huang, Cusuh Ham</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19894">https://arxiv.org/abs/2504.19894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19894">https://arxiv.org/pdf/2504.19894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19894]] CineVerse: Consistent Keyframe Synthesis for Cinematic Scene Composition(https://arxiv.org/abs/2504.19894)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present CineVerse, a novel framework for the task of cinematic scene composition. Similar to traditional multi-shot generation, our task emphasizes the need for consistency and continuity across frames. However, our task also focuses on addressing challenges inherent to filmmaking, such as multiple characters, complex interactions, and visual cinematic effects. In order to learn to generate such content, we first create the CineVerse dataset. We use this dataset to train our proposed two-stage approach. First, we prompt a large language model (LLM) with task-specific instructions to take in a high-level scene description and generate a detailed plan for the overall setting and characters, as well as the individual shots. Then, we fine-tune a text-to-image generation model to synthesize high-quality visual keyframes. Experimental results demonstrate that CineVerse yields promising improvements in generating visually coherent and contextually rich movie scenes, paving the way for further exploration in cinematic video synthesis.</li>
<li><strong>摘要：</strong>我们提出了Cineverse，这是电影场景构图的新型框架。与传统的多发相似，我们的任务强调了跨帧的一致性和连续性的需求。但是，我们的任务还重点是解决电影制作固有的挑战，例如多个字符，复杂的互动和视觉电影效果。为了学习生成此类内容，我们首先创建Cineverse数据集。我们使用此数据集训练我们提出的两阶段方法。首先，我们提示了一个大型语言模型（LLM），其中包含特定于任务的说明，以进行高级场景描述，并为整体设置和角色以及单个镜头生成详细的计划。然后，我们微调一个文本对图像生成模型，以合成高质量的视觉钥匙帧。实验结果表明，在电影视频综合中，为进一步的探索铺平了道路，从而产生视觉上和上下文丰富的电影场景，从而产生有希望的改善。</li>
</ul>

<h3>Title: Enhancing Surgical Documentation through Multimodal Visual-Temporal Transformers and Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Hugo Georgenthum, Cristian Cosentino, Fabrizio Marozzo, Pietro Liò</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19918">https://arxiv.org/abs/2504.19918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19918">https://arxiv.org/pdf/2504.19918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19918]] Enhancing Surgical Documentation through Multimodal Visual-Temporal Transformers and Generative AI(https://arxiv.org/abs/2504.19918)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The automatic summarization of surgical videos is essential for enhancing procedural documentation, supporting surgical training, and facilitating post-operative analysis. This paper presents a novel method at the intersection of artificial intelligence and medicine, aiming to develop machine learning models with direct real-world applications in surgical contexts. We propose a multi-modal framework that leverages recent advancements in computer vision and large language models to generate comprehensive video summaries. % The approach is structured in three key stages. First, surgical videos are divided into clips, and visual features are extracted at the frame level using visual transformers. This step focuses on detecting tools, tissues, organs, and surgical actions. Second, the extracted features are transformed into frame-level captions via large language models. These are then combined with temporal features, captured using a ViViT-based encoder, to produce clip-level summaries that reflect the broader context of each video segment. Finally, the clip-level descriptions are aggregated into a full surgical report using a dedicated LLM tailored for the summarization task. % We evaluate our method on the CholecT50 dataset, using instrument and action annotations from 50 laparoscopic videos. The results show strong performance, achieving 96\% precision in tool detection and a BERT score of 0.74 for temporal context summarization. This work contributes to the advancement of AI-assisted tools for surgical reporting, offering a step toward more intelligent and reliable clinical documentation.</li>
<li><strong>摘要：</strong>手术视频的自动汇总对于增强程序文档，支持手术培训和促进术后分析至关重要。本文在人工智能和医学的交汇处介绍了一种新颖的方法，旨在开发在手术环境下直接使用现实世界应用的机器学习模型。我们提出了一个多模式框架，该框架利用计算机视觉和大型语言模型的最新进步来生成全面的视频摘要。 ％该方法在三个关键阶段结构。首先，将手术视频分为剪辑，并使用Visual Transformers在框架级别提取视觉特征。此步骤着重于检测工具，组织，器官和手术作用。其次，提取的功能通过大语言模型转换为帧级字幕。然后将它们与使用基于Vivit的编码器捕获的时间功能结合使用，以产生剪贴级摘要，以反映每个视频段的更广泛上下文。最后，使用用于摘要任务的专用LLM汇总剪辑级描述将其汇总到完整的外科报告中。 ％我们使用50个腹腔镜视频的仪器和动作注释在Cholect50数据集上评估我们的方法。结果显示出强劲的性能，在刀具检测中达到96 \％精度，时间上下文摘要的BERT得分为0.74。这项工作有助于进步用于手术报告的AI辅助工具，从而为更聪明和可靠的临床文档提供了一步。</li>
</ul>

<h3>Title: Accurate and Diverse LLM Mathematical Reasoning via Automated PRM-Guided GFlowNets</h3>
<ul>
<li><strong>Authors: </strong>Adam Younsi, Abdalgader Abubaker, Mohamed El Amine Seddik, Hakim Hacid, Salem Lahlou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.19981">https://arxiv.org/abs/2504.19981</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.19981">https://arxiv.org/pdf/2504.19981</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.19981]] Accurate and Diverse LLM Mathematical Reasoning via Automated PRM-Guided GFlowNets(https://arxiv.org/abs/2504.19981)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Achieving both accuracy and diverse reasoning remains challenging for Large Language Models (LLMs) in complex domains like mathematics. A key bottleneck is evaluating intermediate reasoning steps to guide generation without costly human annotations. To address this, we first introduce a novel Process Reward Model (PRM) trained automatically using Monte Carlo Tree Search coupled with a similarity-based data augmentation technique, effectively capturing step-level reasoning quality. Leveraging this PRM, we then adapt Generative Flow Networks (GFlowNets) to operate at the reasoning step level. Unlike traditional reinforcement learning focused on maximizing a single reward, GFlowNets naturally sample diverse, high-quality solutions proportional to their rewards, as measured by our PRM. Empirical evaluation shows strong improvements in both accuracy and solution diversity on challenging mathematical benchmarks (e.g., +2.59% absolute accuracy on MATH Level 5 for Llama3.2-3B), with effective generalization to unseen datasets (+9.4% absolute on SAT MATH). Our work demonstrates the potential of PRM-guided, step-level GFlowNets for developing more robust and versatile mathematical reasoning in LLMs.</li>
<li><strong>摘要：</strong>对于数学等复杂领域中的大型语言模型（LLM）来说，实现准确性和各种推理仍然具有挑战性。一个关键的瓶颈是评估中间推理步骤，以指导发电而无需昂贵的人类注释。为了解决这个问题，我们首先引入了一种新型的过程奖励模型（PRM），该模型（PRM）使用Monte Carlo Tree搜索自动培训，并与基于相似性的数据增强技术相结合，有效地捕获了阶梯级别的推理质量。利用此PRM，我们将生成流网络（GFLOWNETS）调整为在推理步骤级别上运行。与我们的PRM衡量的传统增强学习的重点是最大化单一的奖励，Gflownets自然可以品尝多样化的解决方案，与他们的奖励成正比。经验评估表明，在挑战性数学基准测试方面的准确性和解决方案多样性都有很大的提高（例如，Llama3.2-3B的数学级别5 +2.59％的绝对精度为 +2.59％），具有有效的概括性，可有效地概括（ +9.4％的SAT数学绝对绝对值9.4％）。我们的工作展示了PRM引导的，步进式GFLOWNET的潜力，以在LLMS中开发更健壮和多功能的数学推理。</li>
</ul>

<h3>Title: Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xin Wang, Haoyang Li, Zeyang Zhang, Haibo Chen, Wenwu Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20020">https://arxiv.org/abs/2504.20020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20020">https://arxiv.org/pdf/2504.20020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20020]] Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models(https://arxiv.org/abs/2504.20020)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have dramatically advanced machine learning research including natural language processing, computer vision, data mining, etc., yet they still exhibit critical limitations in reasoning, factual consistency, and interpretability. In this paper, we introduce a novel learning paradigm -- Modular Machine Learning (MML) -- as an essential approach toward new-generation LLMs. MML decomposes the complex structure of LLMs into three interdependent components: modular representation, modular model, and modular reasoning, aiming to enhance LLMs' capability of counterfactual reasoning, mitigating hallucinations, as well as promoting fairness, safety, and transparency. Specifically, the proposed MML paradigm can: i) clarify the internal working mechanism of LLMs through the disentanglement of semantic components; ii) allow for flexible and task-adaptive model design; iii) enable interpretable and logic-driven decision-making process. We present a feasible implementation of MML-based LLMs via leveraging advanced techniques such as disentangled representation learning, neural architecture search and neuro-symbolic learning. We critically identify key challenges, such as the integration of continuous neural and discrete symbolic processes, joint optimization, and computational scalability, present promising future research directions that deserve further exploration. Ultimately, the integration of the MML paradigm with LLMs has the potential to bridge the gap between statistical (deep) learning and formal (logical) reasoning, thereby paving the way for robust, adaptable, and trustworthy AI systems across a wide range of real-world applications.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）具有急剧高级的机器学习研究，包括自然语言处理，计算机视觉，数据挖掘等，但它们仍然在推理，事实一致性和解释性方面表现出关键的局限性。在本文中，我们介绍了一种新颖的学习范式 - 模块化机器学习（MML），作为新一代LLM的重要方法。 MML将LLM的复杂结构分解为三个相互依存的组件：模块化表示，模块化模型和模块化推理，旨在增强LLMS反事实推理的能力，减轻幻觉，并促进公平，安全性，安全性和透明度。具体而言，提出的MML范式可以：i）通过分离语义成分阐明LLM的内部工作机制； ii）允许灵活和任务自适应模型设计； iii）启用可解释和逻辑驱动的决策过程。我们通过利用先进的技术（例如分离的表示学习，神经体系结构搜索和神经符号学习）来提供基于MML的LLM的可行实现。我们严格确定了关键挑战，例如连续神经和离散符号过程的整合，关节优化和计算可扩展性，提出了有希望的未来研究方向，应进一步探索。最终，MML范式与LLM的集成有可能弥合统计（深度）学习和正式（逻辑）推理之间的差距，从而为跨多种现实世界应用的可靠，适应性和可信赖的AI系统铺平了道路。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
