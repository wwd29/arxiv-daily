<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-02-10</h1>
<h3>Title: Where Not to Learn: Prior-Aligned Training with Subset-based Attribution Constraints for Reliable Decision-Making</h3>
<ul>
<li><strong>Authors: </strong>Ruoyu Chen, Shangquan Sun, Xiaoqing Guo, Sanyi Zhang, Kangwei Liu, Shiming Liu, Zhangcheng Wang, Qunli Zhang, Hua Zhang, Xiaochun Cao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07008">https://arxiv.org/abs/2602.07008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07008">https://arxiv.org/pdf/2602.07008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07008]] Where Not to Learn: Prior-Aligned Training with Subset-based Attribution Constraints for Reliable Decision-Making(https://arxiv.org/abs/2602.07008)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reliable models should not only predict correctly, but also justify decisions with acceptable evidence. Yet conventional supervised learning typically provides only class-level labels, allowing models to achieve high accuracy through shortcut correlations rather than the intended evidence. Human priors can help constrain such behavior, but aligning models to these priors remains challenging because learned representations often diverge from human perception. To address this challenge, we propose an attribution-based human prior alignment method. We encode human priors as input regions that the model is expected to rely on (e.g., bounding boxes), and leverage a highly faithful subset-selection-based attribution approach to expose the model's decision evidence during training. When the attribution region deviates substantially from the prior regions, we penalize reliance on off-prior evidence, encouraging the model to shift its attribution toward the intended regions. This is achieved through a training objective that imposes attribution constraints induced by the human prior. We validate our method on both image classification and click decision tasks in MLLM-based GUI agent models. Across conventional classification and autoregressive generation settings, human prior alignment consistently improves task accuracy while also enhancing the model's decision reasonability.</li>
<li><strong>摘要：</strong>可靠的模型不仅应该正确预测，还应该用可接受的证据来证明决策的合理性。然而，传统的监督学习通常只提供类级别的标签，允许模型通过快捷相关性而不是预期的证据来实现高精度。人类先验可以帮助限制这种行为，但将模型与这些先验保持一致仍然具有挑战性，因为学习到的表示通常与人类的感知有所不同。为了应对这一挑战，我们提出了一种基于归因的人类先验对齐方法。我们将人类先验编码为模型预期依赖的输入区域（例如边界框），并利用高度可靠的基于子集选择的归因方法在训练期间公开模型的决策证据。当归因区域严重偏离先前区域时，我们会惩罚对非先前证据的依赖，鼓励模型将其归因转移到预期区域。这是通过施加由人类先验引起的归因约束的训练目标来实现的。我们在基于 MLLM 的 GUI 代理模型中的图像分类和点击决策任务上验证了我们的方法。在传统的分类和自回归生成设置中，人类先验对齐不断提高任务准确性，同时也增强模型决策的合理性。</li>
</ul>

<h3>Title: A General Model for Retinal Segmentation and Quantification</h3>
<ul>
<li><strong>Authors: </strong>Zhonghua Wang, Lie Ju, Sijia Li, Wei Feng, Sijin Zhou, Ming Hu, Jianhao Xiong, Xiaoying Tang, Yifan Peng, Mingquan Lin, Yaodong Ding, Yong Zeng, Wenbin Wei, Li Dong, Zongyuan Ge</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07012">https://arxiv.org/abs/2602.07012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07012">https://arxiv.org/pdf/2602.07012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07012]] A General Model for Retinal Segmentation and Quantification(https://arxiv.org/abs/2602.07012)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Retinal imaging is fast, non-invasive, and widely available, offering quantifiable structural and vascular signals for ophthalmic and systemic health assessment. This accessibility creates an opportunity to study how quantitative retinal phenotypes relate to ocular and systemic diseases. However, such analyses remain difficult at scale due to the limited availability of public multi-label datasets and the lack of a unified segmentation-to-quantification pipeline. We present RetSAM, a general retinal segmentation and quantification framework for fundus imaging. It delivers robust multi-target segmentation and standardized biomarker extraction, supporting downstream ophthalmologic studies and oculomics correlation analyses. Trained on over 200,000 fundus images, RetSAM supports three task categories and segments five anatomical structures, four retinal phenotypic patterns, and more than 20 distinct lesion types. It converts these segmentation results into over 30 standardized biomarkers that capture structural morphology, vascular geometry, and degenerative changes. Trained with a multi-stage strategy using both private and public fundus data, RetSAM achieves superior segmentation performance on 17 public datasets. It improves on prior best methods by 3.9 percentage points in DSC on average, with up to 15 percentage points on challenging multi-task benchmarks, and generalizes well across diverse populations, imaging devices, and clinical settings. The resulting biomarkers enable systematic correlation analyses across major ophthalmic diseases, including diabetic retinopathy, age-related macular degeneration, glaucoma, and pathologic myopia. Together, RetSAM transforms fundus images into standardized, interpretable quantitative phenotypes, enabling large-scale ophthalmic research and translation.</li>
<li><strong>摘要：</strong>视网膜成像快速、非侵入性且广泛应用，为眼科和全身健康评估提供可量化的结构和血管信号。这种可及性为研究定量视网膜表型与眼部和全身疾病的关系创造了机会。然而，由于公共多标签数据集的可用性有限并且缺乏统一的分割到量化管道，此类分析在规模上仍然很困难。我们提出了 RetSAM，一种用于眼底成像的通用视网膜分割和量化框架。它提供强大的多目标分割和标准化生物标志物提取，支持下游眼科研究和眼组学相关分析。 RetSAM 经过超过 200,000 张眼底图像的训练，支持三个任务类别并​​分割五种解剖结构、四种视网膜表型模式和 20 多种不同的病变类型。它将这些分割结果转换为 30 多个标准化生物标志物，捕获结构形态、血管几何形状和退行性变化。 RetSAM 通过使用私有和公共眼底数据的多阶段策略进行训练，在 17 个公共数据集上实现了卓越的分割性能。它在 DSC 中比之前的最佳方法平均提高了 3.9 个百分点，在具有挑战性的多任务基准上提高了高达 15 个百分点，并且在不同人群、成像设备和临床环境中具有良好的泛化性。由此产生的生物标志物能够对主要眼科疾病进行系统的相关性分析，包括糖尿病视网膜病变、年龄相关性黄斑变性、青光眼和病理性近视。 RetSAM 将眼底图像转化为标准化、可解释的定量表型，从而实现大规模眼科研究和转化。</li>
</ul>

<h3>Title: Vectra: A New Metric, Dataset, and Model for Visual Quality Assessment in E-Commerce In-Image Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Qingyu Wu, Yuxuan Han, Haijun Li, Zhao Xu, Jianshan Zhao, Xu Jin, Longyue Wang, Weihua Luo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07014">https://arxiv.org/abs/2602.07014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07014">https://arxiv.org/pdf/2602.07014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07014]] Vectra: A New Metric, Dataset, and Model for Visual Quality Assessment in E-Commerce In-Image Machine Translation(https://arxiv.org/abs/2602.07014)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>In-Image Machine Translation (IIMT) powers cross-border e-commerce product listings; existing research focuses on machine translation evaluation, while visual rendering quality is critical for user engagement. When facing context-dense product imagery and multimodal defects, current reference-based methods (e.g., SSIM, FID) lack explainability, while model-as-judge approaches lack domain-grounded, fine-grained reward signals. To bridge this gap, we introduce Vectra, to the best of our knowledge, the first reference-free, MLLM-driven visual quality assessment framework for e-commerce IIMT. Vectra comprises three components: (1) Vectra Score, a multidimensional quality metric system that decomposes visual quality into 14 interpretable dimensions, with spatially-aware Defect Area Ratio (DAR) quantification to reduce annotation ambiguity; (2) Vectra Dataset, constructed from 1.1M real-world product images via diversity-aware sampling, comprising a 2K benchmark for system evaluation, 30K reasoning-based annotations for instruction tuning, and 3.5K expert-labeled preferences for alignment and evaluation; and (3) Vectra Model, a 4B-parameter MLLM that generates both quantitative scores and diagnostic reasoning. Experiments demonstrate that Vectra achieves state-of-the-art correlation with human rankings, and our model outperforms leading MLLMs, including GPT-5 and Gemini-3, in scoring performance. The dataset and model will be released upon acceptance.</li>
<li><strong>摘要：</strong>图像内机器翻译（IIMT）为跨境电商产品列表提供支持；现有的研究重点是机器翻译评估，而视觉渲染质量对于用户参与度至关重要。当面对上下文密集的产品图像和多模态缺陷时，当前基于参考的方法（例如，SSIM、FID）缺乏可解释性，而模型即判断方法缺乏基于领域的细粒度奖励信号。为了弥补这一差距，我们引入了 Vectra，据我们所知，这是第一个用于电子商务 IIMT 的无参考、MLLM 驱动的视觉质量评估框架。 Vectra 包含三个组成部分：(1) Vectra Score，一个多维质量度量系统，将视觉质量分解为 14 个可解释的维度，并通过空间感知缺陷面积比 (DAR) 量化来减少注释歧义； (2) Vectra 数据集，通过多样性感知采样从 110 万个真实世界产品图像构建而成，包括用于系统评估的 2K 基准、用于指令调整的 30K 基于推理的注释以及用于对齐和评估的 3500 个专家标记的偏好； (3) Vectra 模型，一种 4B 参数 MLLM，可生成定量分数和诊断推理。实验表明，Vectra 与人类排名实现了最先进的相关性，并且我们的模型在评分性能方面优于领先的 MLLM，包括 GPT-5 和 Gemini-3。数据集和模型将在接受后发布。</li>
</ul>

<h3>Title: Neural Sabermetrics with World Model: Play-by-play Predictive Modeling with Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Young Jin Ahn, Yiyang Du, Zheyuan Zhang, Haisen Kang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07030">https://arxiv.org/abs/2602.07030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07030">https://arxiv.org/pdf/2602.07030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07030]] Neural Sabermetrics with World Model: Play-by-play Predictive Modeling with Large Language Model(https://arxiv.org/abs/2602.07030)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Classical sabermetrics has profoundly shaped baseball analytics by summarizing long histories of play into compact statistics. While these metrics are invaluable for valuation and retrospective analysis, they do not define a generative model of how baseball games unfold pitch by pitch, leaving most existing approaches limited to single-step prediction or post-hoc analysis. In this work, we present Neural Sabermetrics with World Model, a Large Language Model (LLM) based play-by-play world model for baseball. We cast baseball games as long auto-regressive sequences of events and continuously pretrain a single LLM on more than ten years of Major League Baseball (MLB) tracking data, comprising over seven million pitch sequences and approximately three billion tokens. The resulting model is capable of predicting multiple aspects of game evolution within a unified framework. We evaluate our model on both in-distribution regular-season data and out-of-distribution postseason games and compare against strong neural baselines from prior work. Despite using a single backbone model, our approach outperforms the performance of existing baselines, (1) correctly predicting approximately 64% of next pitches within a plate appearance and (2) 78% of batter swing decisions, suggesting that LLMs can serve as effective world models for sports.</li>
<li><strong>摘要：</strong>经典的军刀计量学通过将悠久的比赛历史总结为紧凑的统计数据，深刻地塑造了棒球分析。虽然这些指标对于评估和回顾性分析非常有价值，但它们没有定义棒球比赛如何逐场展开的生成模型，使得大多数现有方法仅限于单步预测或事后分析。在这项工作中，我们提出了带有世界模型的神经 Sabermetrics，这是一种基于大型语言模型 (LLM) 的棒球逐场比赛世界模型。我们将棒球比赛视为长的自回归事件序列，并根据美国职业棒球大联盟 (MLB) 十多年的跟踪数据（包括超过 700 万个投球序列和大约 30 亿个令牌）持续对单个 LLM 进行预训练。由此产生的模型能够在统一框架内预测游戏演变的多个方面。我们根据分布内常规赛数据和分布外季后赛数据评估我们的模型，并与之前工作中的强大神经基线进行比较。尽管使用单一骨干模型，我们的方法仍优于现有基线的性能，（1）正确预测了本垒板外观中大约 64% 的下一个投球，（2）正确预测了 78% 的击球手挥杆决策，这表明法学硕士可以作为有效的体育世界模型。</li>
</ul>

<h3>Title: TransConv-DDPM: Enhanced Diffusion Model for Generating Time-Series Data in Healthcare</h3>
<ul>
<li><strong>Authors: </strong>Md Shahriar Kabir, Sana Alamgeer, Minakshi Debnath, Anne H. H. Ngu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07033">https://arxiv.org/abs/2602.07033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07033">https://arxiv.org/pdf/2602.07033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07033]] TransConv-DDPM: Enhanced Diffusion Model for Generating Time-Series Data in Healthcare(https://arxiv.org/abs/2602.07033)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The lack of real-world data in clinical fields poses a major obstacle in training effective AI models for diagnostic and preventive tools in medicine. Generative AI has shown promise in increasing data volume and enhancing model training, particularly in computer vision and natural language processing (NLP) domains. However, generating physiological time-series data, a common type in medical AI applications, presents unique challenges due to its inherent complexity and variability. This paper introduces TransConv-DDPM, an enhanced generative AI method for biomechanical and physiological time-series data generation. The model employs a denoising diffusion probabilistic model (DDPM) with U-Net, multi-scale convolution modules, and a transformer layer to capture both global and local temporal dependencies. We evaluated TransConv-DDPM on three diverse datasets, generating both long and short-sequence time-series data. Quantitative comparisons against state-of-the-art methods, TimeGAN and Diffusion-TS, using four performance metrics, demonstrated promising results, particularly on the SmartFallMM and EEG datasets, where it effectively captured the more gradual temporal change patterns between data points. Additionally, a utility test on the SmartFallMM dataset revealed that adding synthetic fall data generated by TransConv-DDPM improved predictive model performance, showing a 13.64% improvement in F1-score and a 14.93% increase in overall accuracy compared to the baseline model trained solely on fall data from the SmartFallMM dataset. These findings highlight the potential of TransConv-DDPM to generate high-quality synthetic data for real-world applications.</li>
<li><strong>摘要：</strong>临床领域缺乏真实数据，给训练有效的人工智能模型用于医学诊断和预防工具带来了主要障碍。生成式人工智能在增加数据量和增强模型训练方面表现出了希望，特别是在计算机视觉和自然语言处理 (NLP) 领域。然而，生成生理时间序列数据（医疗人工智能应用中的常见类型）由于其固有的复杂性和可变性而带来了独特的挑战。本文介绍了 TransConv-DDPM，这是一种用于生物力学和生理时间序列数据生成的增强型生成人工智能方法。该模型采用带有 U-Net 的去噪扩散概率模型 (DDPM)、多尺度卷积模块和转换器层来捕获全局和局部时间依赖性。我们在三个不同的数据集上评估了 TransConv-DDPM，生成长序列和短序列时间序列数据。使用四个性能指标与最先进的方法 TimeGAN 和 Diffusion-TS 进行定量比较，显示出有希望的结果，特别是在 SmartFallMM 和 EEG 数据集上，它有效地捕获了数据点之间更渐进的时间变化模式。此外，对 SmartFallMM 数据集的实用测试表明，添加 TransConv-DDPM 生成的合成跌倒数据可以提高预测模型的性能，与仅根据 SmartFallMM 数据集的跌倒数据训练的基线模型相比，F1 分数提高了 13.64%，总体准确度提高了 14.93%。这些发现凸显了 TransConv-DDPM 为实际应用生成高质量合成数据的潜力。</li>
</ul>

<h3>Title: RECITYGEN -- Interactive and Generative Participatory Urban Design Tool with Latent Diffusion and Segment Anything</h3>
<ul>
<li><strong>Authors: </strong>Di Mo, Mingyang Sun, Chengxiu Yin, Runjia Tian, Yanhong Wu, Liyan Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07057">https://arxiv.org/abs/2602.07057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07057">https://arxiv.org/pdf/2602.07057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07057]] RECITYGEN -- Interactive and Generative Participatory Urban Design Tool with Latent Diffusion and Segment Anything(https://arxiv.org/abs/2602.07057)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Urban design profoundly impacts public spaces and community engagement. Traditional top-down methods often overlook public input, creating a gap in design aspirations and reality. Recent advancements in digital tools, like City Information Modelling and augmented reality, have enabled a more participatory process involving more stakeholders in urban design. Further, deep learning and latent diffusion models have lowered barriers for design generation, providing even more opportunities for participatory urban design. Combining state-of-the-art latent diffusion models with interactive semantic segmentation, we propose RECITYGEN, a novel tool that allows users to interactively create variational street view images of urban environments using text prompts. In a pilot project in Beijing, users employed RECITYGEN to suggest improvements for an ongoing Urban Regeneration project. Despite some limitations, RECITYGEN has shown significant potential in aligning with public preferences, indicating a shift towards more dynamic and inclusive urban planning methods. The source code for the project can be found at RECITYGEN GitHub.</li>
<li><strong>摘要：</strong>城市设计深刻影响公共空间和社区参与。传统的自上而下的方法常常忽视公众的意见，造成设计愿望与现实之间的差距。城市信息模型和增强现实等数字工具的最新进展使得城市设计中的更多利益相关者参与度更高。此外，深度学习和潜在扩散模型降低了设计生成的障碍，为参与式城市设计提供了更多机会。我们将最先进的潜在扩散模型与交互式语义分割相结合，提出了 RECITYGEN，这是一种新颖的工具，允许用户使用文本提示交互式地创建城市环境的变化街景图像。在北京的一个试点项目中，用户使用 RECITYGEN 为正在进行的城市更新项目提出改进建议。尽管存在一些局限性，RECITYGEN 在符合公众偏好方面表现出了巨大的潜力，表明城市规划方法正在向更具活力和包容性的方向转变。该项目的源代码可以在 RECITYGEN GitHub 上找到。</li>
</ul>

<h3>Title: FADE: Selective Forgetting via Sparse LoRA and Self-Distillation</h3>
<ul>
<li><strong>Authors: </strong>Carolina R. Kelsch, Leonardo S. B. Pereira, Natnael Mola, Luis H. Arribas, Juan C. S. M. Avedillo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07058">https://arxiv.org/abs/2602.07058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07058">https://arxiv.org/pdf/2602.07058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07058]] FADE: Selective Forgetting via Sparse LoRA and Self-Distillation(https://arxiv.org/abs/2602.07058)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Machine Unlearning aims to remove the influence of specific data or concepts from trained models while preserving overall performance, a capability increasingly required by data protection regulations and responsible AI practices. Despite recent progress, unlearning in text-to-image diffusion models remains challenging due to high computational costs and the difficulty of balancing effective forgetting with retention of unrelated concepts. We introduce FADE (Fast Adapter for Data Erasure), a two-stage unlearning method for image generation that combines parameter localization with self-distillation. FADE first identifies parameters most responsible for the forget set using gradient-based saliency and constrains updates through sparse LoRA adapters, ensuring lightweight, localized modifications. In a second stage, FADE applies a self-distillation objective that overwrites the forgotten concept with a user-defined surrogate while preserving behavior on retained data. The resulting adapters are memory-efficient, reversible, and can be merged or removed at runtime, enabling flexible deployment in production systems. We evaluated FADE on the UnlearnCanvas benchmark and conducted ablation studies on Imagenette, Labeled Faces in the Wild, AtharvaTaras Dog Breeds Dataset, and SUN Attributes datasets, demonstrating State-of-the-Art unlearning performance with fine-grained control over the forgetting-retention trade-off. Our results demonstrate that FADE achieves strong concept erasure and high retainability across various domains, making it a suitable solution for selective unlearning in diffusion-based image generation models.</li>
<li><strong>摘要：</strong>机器学习旨在消除经过训练的模型中特定数据或概念的影响，同时保持整体性能，这是数据保护法规和负责任的人工智能实践日益需要的功能。尽管最近取得了进展，但由于高计算成本以及平衡有效遗忘与保留不相关概念的困难，文本到图像扩散模型的遗忘仍然具有挑战性。我们引入了 FADE（数据擦除快速适配器），这是一种将参数本地化与自蒸馏相结合的两阶段图像生成方法。 FADE 首先使用基于梯度的显着性识别对遗忘集最有影响的参数，并通过稀疏 LoRA 适配器约束更新，确保轻量级、局部修改。在第二阶段，FADE 应用自蒸馏目标，用用户定义的替代项覆盖被遗忘的概念，同时保留保留数据的行为。由此产生的适配器具有内存高效、可逆的特点，并且可以在运行时合并或删除，从而能够在生产系统中灵活部署。我们在 UnlearnCanvas 基准上评估了 FADE，并对 Imagenette、Labeled Faces in the Wild、AtharvaTaras Dog Breeds Dataset 和 SUN Attributes 数据集进行了消融研究，展示了最先进的遗忘性能以及对遗忘-保留权衡的细粒度控制。我们的结果表明，FADE 在各个领域实现了强大的概念擦除和高保留性，使其成为基于扩散的图像生成模型中选择性遗忘的合适解决方案。</li>
</ul>

<h3>Title: Video-based Music Generation</h3>
<ul>
<li><strong>Authors: </strong>Serkan Sulun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.MM, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07063">https://arxiv.org/abs/2602.07063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07063">https://arxiv.org/pdf/2602.07063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07063]] Video-based Music Generation(https://arxiv.org/abs/2602.07063)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>As the volume of video content on the internet grows rapidly, finding a suitable soundtrack remains a significant challenge. This thesis presents EMSYNC (EMotion and SYNChronization), a fast, free, and automatic solution that generates music tailored to the input video, enabling content creators to enhance their productions without composing or licensing music. Our model creates music that is emotionally and rhythmically synchronized with the video. A core component of EMSYNC is a novel video emotion classifier. By leveraging pretrained deep neural networks for feature extraction and keeping them frozen while training only fusion layers, we reduce computational complexity while improving accuracy. We show the generalization abilities of our method by obtaining state-of-the-art results on Ekman-6 and MovieNet. Another key contribution is a large-scale, emotion-labeled MIDI dataset for affective music generation. We then present an emotion-based MIDI generator, the first to condition on continuous emotional values rather than discrete categories, enabling nuanced music generation aligned with complex emotional content. To enhance temporal synchronization, we introduce a novel temporal boundary conditioning method, called "boundary offset encodings," aligning musical chords with scene changes. Combining video emotion classification, emotion-based music generation, and temporal boundary conditioning, EMSYNC emerges as a fully automatic video-based music generator. User studies show that it consistently outperforms existing methods in terms of music richness, emotional alignment, temporal synchronization, and overall preference, setting a new state-of-the-art in video-based music generation.</li>
<li><strong>摘要：</strong>随着互联网上视频内容量的快速增长，找到合适的配乐仍然是一个重大挑战。本论文介绍了 EMSYNC（EMotion 和 SYNChronization），这是一种快速、免费、自动的解决方案，可生成针对输入视频定制的音乐，使内容创作者能够在无需创作或许可音乐的情况下增强其作品。我们的模型创建的音乐在情感和节奏上与视频同步。 EMSYNC 的核心组件是一种新颖的视频情感分类器。通过利用预训练的深度神经网络进行特征提取并在仅训练融合层时保持它们冻结，我们降低了计算复杂性，同时提高了准确性。我们通过在 Ekman-6 和 MovieNet 上获得最先进的结果来展示我们方法的泛化能力。另一个关键贡献是用于情感音乐生成的大规模、带有情感标签的 MIDI 数据集。然后，我们提出了一个基于情感的 MIDI 生成器，它是第一个以连续情感值而不是离散类别为条件的生成器，从而能够生成与复杂情感内容一致的细致入微的音乐。为了增强时间同步，我们引入了一种新颖的时间边界调节方法，称为“边界偏移编码”，使音乐和弦与场景变化对齐。 EMSYNC 结合了视频情感分类、基于情感的音乐生成和时间边界调节，成为一款全自动的基于视频的音乐生成器。用户研究表明，它在音乐丰富度、情感一致性、时间同步和整体偏好方面始终优于现有方法，在基于视频的音乐生成中树立了新的最先进技术。</li>
</ul>

<h3>Title: Exploring Physical Intelligence Emergence via Omni-Modal Architecture and Physical Data Engine</h3>
<ul>
<li><strong>Authors: </strong>Minghao Han, Dingkang Yang, Yue Jiang, Yizhou Liu, Lihua Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07064">https://arxiv.org/abs/2602.07064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07064">https://arxiv.org/pdf/2602.07064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07064]] Exploring Physical Intelligence Emergence via Omni-Modal Architecture and Physical Data Engine(https://arxiv.org/abs/2602.07064)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Physical understanding remains brittle in omni-modal models because key physical attributes are visually ambiguous and sparsely represented in web-scale data. We present OmniFysics, a compact omni-modal model that unifies understanding across images, audio, video, and text, with integrated speech and image generation. To inject explicit physical knowledge, we build a physical data engine with two components. FysicsAny produces physics-grounded instruction--image supervision by mapping salient objects to verified physical attributes through hierarchical retrieval over a curated prototype database, followed by physics-law--constrained verification and caption rewriting. FysicsOmniCap distills web videos via audio--visual consistency filtering to generate high-fidelity video--instruction pairs emphasizing cross-modal physical cues. We train OmniFysics with staged multimodal alignment and instruction tuning, adopt latent-space flow matching for text-to-image generation, and use an intent router to activate generation only when needed. Experiments show competitive performance on standard multimodal benchmarks and improved results on physics-oriented evaluations.</li>
<li><strong>摘要：</strong>在全模态模型中，物理理解仍然脆弱，因为关键物理属性在视觉上不明确，并且在网络规模的数据中稀疏表示。我们推出了 OmniFysics，这是一种紧凑的全模态模型，它统一了对图像、音频、视频和文本的理解，以及集成的语音和图像生成。为了注入显式的物理知识，我们构建了一个包含两个组件的物理数据引擎。 FysicsAny 通过对策划的原型数据库进行分层检索，将显着对象映射到经过验证的物理属性，然后进行物理定律约束验证和标题重写，从而产生基于物理的指令 - 图像监督。 FysicsOmniCap 通过音频视觉一致性过滤来提取网络视频，以生成高保真视频指令对，强调跨模式物理线索。我们通过分阶段多模态对齐和指令调整来训练 OmniFysics，采用潜在空间流匹配来生成文本到图像，并使用意图路由器仅在需要时激活生成。实验显示了标准多模式基准的竞争性能以及面向物理的评估的改进结果。</li>
</ul>

<h3>Title: Bidirectional Reward-Guided Diffusion for Real-World Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Zihao Fan, Xin Lu, Yidi Liu, Jie Huang, Dong Li, Xueyang Fu, Zheng-Jun Zha</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07069">https://arxiv.org/abs/2602.07069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07069">https://arxiv.org/pdf/2602.07069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07069]] Bidirectional Reward-Guided Diffusion for Real-World Image Super-Resolution(https://arxiv.org/abs/2602.07069)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Diffusion-based super-resolution can synthesize rich details, but models trained on synthetic paired data often fail on real-world LR images due to distribution shifts. We propose Bird-SR, a bidirectional reward-guided diffusion framework that formulates super-resolution as trajectory-level preference optimization via reward feedback learning (ReFL), jointly leveraging synthetic LR-HR pairs and real-world LR images. For structural fidelity easily affected in ReFL, the model is directly optimized on synthetic pairs at early diffusion steps, which also facilitates structure preservation for real-world inputs under smaller distribution gap in structure levels. For perceptual enhancement, quality-guided rewards are applied at later sampling steps to both synthetic and real LR images. To mitigate reward hacking, the rewards for synthetic results are formulated in a relative advantage space bounded by their clean counterparts, while real-world optimization is regularized via a semantic alignment constraint. Furthermore, to balance structural and perceptual learning, we adopt a dynamic fidelity-perception weighting strategy that emphasizes structure preservation at early stages and progressively shifts focus toward perceptual optimization at later diffusion steps. Extensive experiments on real-world SR benchmarks demonstrate that Bird-SR consistently outperforms state-of-the-art methods in perceptual quality while preserving structural consistency, validating its effectiveness for real-world super-resolution.</li>
<li><strong>摘要：</strong>基于扩散的超分辨率可以合成丰富的细节，但由于分布变化，在合成配对数据上训练的模型通常在现实世界的 LR 图像上失败。我们提出 Bird-SR，一种双向奖励引导扩散框架，通过奖励反馈学习 (ReFL) 将超分辨率制定为轨迹级偏好优化，联合利用合成的 LR-HR 对和真实世界的 LR 图像。对于 ReFL 中结构保真度容易受到影响的情况，该模型在早期扩散步骤中直接对合成对进行优化，这也有利于在结构水平上分布间隙较小的情况下真实世界输入的结构保存。为了增强感知，在稍后的采样步骤中，对合成图像和真实 LR 图像应用质量引导奖励。为了减轻奖励黑客行为，合成结果的奖励是在由其干净对应物界定的相对优势空间中制定的，而现实世界的优化则通过语义对齐约束进行正则化。此外，为了平衡结构和感知学习，我们采用动态保真度感知加权策略，强调早期阶段的结构保留，并在后期扩散步骤中逐步将重点转向感知优化。对现实世界 SR 基准的大量实验表明，Bird-SR 在感知质量方面始终优于最先进的方法，同时保持结构一致性，验证了其在现实世界超分辨率方面的有效性。</li>
</ul>

<h3>Title: TLC-Plan: A Two-Level Codebook Based Network for End-to-End Vector Floorplan Generation</h3>
<ul>
<li><strong>Authors: </strong>Biao Xiong, Zhen Peng, Ping Wang, Qiegen Liu, Xian Zhong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07100">https://arxiv.org/abs/2602.07100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07100">https://arxiv.org/pdf/2602.07100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07100]] TLC-Plan: A Two-Level Codebook Based Network for End-to-End Vector Floorplan Generation(https://arxiv.org/abs/2602.07100)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Automated floorplan generation aims to improve design quality, architectural efficiency, and sustainability by jointly modeling global spatial organization and precise geometric detail. However, existing approaches operate in raster space and rely on post hoc vectorization, which introduces structural inconsistencies and hinders end-to-end learning. Motivated by compositional spatial reasoning, we propose TLC-Plan, a hierarchical generative model that directly synthesizes vector floorplans from input boundaries, aligning with human architectural workflows based on modular and reusable patterns. TLC-Plan employs a two-level VQ-VAE to encode global layouts as semantically labeled room bounding boxes and to refine local geometries using polygon-level codes. This hierarchy is unified in a CodeTree representation, while an autoregressive transformer samples codes conditioned on the boundary to generate diverse and topologically valid designs, without requiring explicit room topology or dimensional priors. Extensive experiments show state-of-the-art performance on RPLAN dataset (FID = 1.84, MSE = 2.06) and leading results on LIFULL dataset. The proposed framework advances constraint-aware and scalable vector floorplan generation for real-world architectural applications. Source code and trained models are released at this https URL.</li>
<li><strong>摘要：</strong>自动生成平面图旨在通过对全球空间组织和精确的几何细节进行联合建模来提高设计质量、建筑效率和可持续性。然而，现有的方法在栅格空间中运行并依赖于事后矢量化，这会引入结构不一致并阻碍端到端学习。在组合空间推理的推动下，我们提出了 TLC-Plan，这是一种分层生成模型，可以直接从输入边界合成矢量平面图，与基于模块化和可重用模式的人类建筑工作流程保持一致。 TLC-Plan 采用两级 VQ-VAE 将全局布局编码为语义标记的房间边界框，并使用多边形级代码细化局部几何形状。该层次结构在 CodeTree 表示中统一，而自回归变换器对边界条件下的代码进行采样，以生成多样化且拓扑有效的设计，而不需要显式的房间拓扑或维度先验。大量实验显示了 RPLAN 数据集（FID = 1.84，MSE = 2.06）上最先进的性能以及 LIFULL 数据集上的领先结果。所提出的框架为现实世界的建筑应用程序改进了约束感知和可扩展的矢量平面图生成。源代码和经过训练的模型在此 https URL 发布。</li>
</ul>

<h3>Title: Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Zhang, Zhipeng Li, Yiwen Guo, Tianshu Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07106">https://arxiv.org/abs/2602.07106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07106">https://arxiv.org/pdf/2602.07106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07106]] Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models(https://arxiv.org/abs/2602.07106)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Omni-modal large language models (OLLMs) aim to unify multimodal understanding and generation, yet incorporating speech with 3D facial animation remains largely unexplored despite its importance for natural interaction. A key challenge arises from the representation mismatch between discrete, token-level semantic reasoning in LLMs and the dense, fine-grained temporal dynamics required for 3D facial motion, which makes direct modeling difficult to optimize under limited data. We propose Expressive Omni (Ex-Omni), an open-source omni-modal framework that augments OLLMs with speech-accompanied 3D facial animation. Ex-Omni reduces learning difficulty by decoupling semantic reasoning from temporal generation, leveraging speech units as temporal scaffolding and a unified token-as-query gated fusion (TQGF) mechanism for controlled semantic injection. We further introduce InstructEx, a dataset aims to facilitate augment OLLMs with speech-accompanied 3D facial animation. Extensive experiments demonstrate that Ex-Omni performs competitively against existing open-source OLLMs while enabling stable aligned speech and facial animation generation.</li>
<li><strong>摘要：</strong>全模态大语言模型 (OLLM) 旨在统一多模态理解和生成，但尽管语音对于自然交互很重要，但将语音与 3D 面部动画相结合在很大程度上仍未得到探索。一个关键的挑战来自于法学硕士中离散、标记级语义推理与 3D 面部运动所需的密集、细粒度时间动态之间的表示不匹配，这使得直接建模在有限数据下难以优化。我们提出了 Expressive Omni (Ex-Omni)，这是一种开源全模式框架，可通过语音伴随的 3D 面部动画增强 OLLM。 Ex-Omni 通过将语义推理与时间生成分离、利用语音单元作为时间支架以及用于受控语义注入的统一令牌即查询门控融合 (TQGF) 机制来降低学习难度。我们进一步介绍 InstructEx，这是一个旨在通过语音伴随的 3D 面部动画来增强 OLLM 的数据集。大量实验表明，Ex-Omni 的性能与现有开源 OLLM 相比具有竞争力，同时能够生成稳定对齐的语音和面部动画。</li>
</ul>

<h3>Title: Finding Connections: Membership Inference Attacks for the Multi-Table Synthetic Data Setting</h3>
<ul>
<li><strong>Authors: </strong>Joshua Ward, Chi-Hua Wang, Guang Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07126">https://arxiv.org/abs/2602.07126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07126">https://arxiv.org/pdf/2602.07126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07126]] Finding Connections: Membership Inference Attacks for the Multi-Table Synthetic Data Setting(https://arxiv.org/abs/2602.07126)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Synthetic tabular data has gained attention for enabling privacy-preserving data sharing. While substantial progress has been made in single-table synthetic generation where data are modeled at the row or item level, most real-world data exists in relational databases where a user's information spans items across multiple interconnected tables. Recent advances in synthetic relational data generation have emerged to address this complexity, yet release of these data introduce unique privacy challenges as information can be leaked not only from individual items but also through the relationships that comprise a complete user entity. To address this, we propose a novel Membership Inference Attack (MIA) setting to audit the empirical user-level privacy of synthetic relational data and show that single-table MIAs that audit at an item level underestimate user-level privacy leakage. We then propose Multi-Table Membership Inference Attack (MT-MIA), a novel adversarial attack under a No-Box threat model that targets learned representations of user entities via Heterogeneous Graph Neural Networks. By incorporating all connected items for a user, MT-MIA better targets user-level vulnerabilities induced by inter-tabular relationships than existing attacks. We evaluate MT-MIA on a range of real-world multi-table datasets and demonstrate that this vulnerability exists in state-of-the-art relational synthetic data generators, employing MT-MIA to additionally study where this leakage occurs.</li>
<li><strong>摘要：</strong>合成表格数据因实现隐私保护数据共享而受到关注。虽然在单表合成生成方面取得了实质性进展，其中数据在行或项目级别建模，但大多数现实世界的数据存在于关系数据库中，其中用户的信息跨越多个互连表中的项目。合成关系数据生成方面的最新进展已经出现，以解决这种复杂性，但这些数据的发布带来了独特的隐私挑战，因为信息不仅可以从单个项目中泄露，还可以通过构成完整用户实体的关系泄露。为了解决这个问题，我们提出了一种新颖的成员推理攻击（MIA）设置来审核合成关系数据的经验用户级隐私，并表明在项目级别审核的单表 MIA 低估了用户级隐私泄漏。然后，我们提出了多表成员推理攻击（MT-MIA），这是一种 No-Box 威胁模型下的新型对抗攻击，其目标是通过异构图神经网络学习用户实体的表示。通过合并用户的所有连接项，MT-MIA 比现有攻击更好地针对由表间关系引起的用户级漏洞。我们在一系列现实世界的多表数据集上评估 MT-MIA，并证明该漏洞存在于最先进的关系合成数据生成器中，并利用 MT-MIA 进一步研究发生泄漏的位置。</li>
</ul>

<h3>Title: Privacy in Image Datasets: A Case Study on Pregnancy Ultrasounds</h3>
<ul>
<li><strong>Authors: </strong>Rawisara Lohanimit, Yankun Wu, Amelia Katirai, Yuta Nakashima, Noa Garcia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07149">https://arxiv.org/abs/2602.07149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07149">https://arxiv.org/pdf/2602.07149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07149]] Privacy in Image Datasets: A Case Study on Pregnancy Ultrasounds(https://arxiv.org/abs/2602.07149)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rise of generative models has led to increased use of large-scale datasets collected from the internet, often with minimal or no data curation. This raises concerns about the inclusion of sensitive or private information. In this work, we explore the presence of pregnancy ultrasound images, which contain sensitive personal information and are often shared online. Through a systematic examination of LAION-400M dataset using CLIP embedding similarity, we retrieve images containing pregnancy ultrasound and detect thousands of entities of private information such as names and locations. Our findings reveal that multiple images have high-risk information that could enable re-identification or impersonation. We conclude with recommended practices for dataset curation, data privacy, and ethical use of public image datasets.</li>
<li><strong>摘要：</strong>生成模型的兴起导致人们越来越多地使用从互联网收集的大规模数据集，而这些数据集通常很少或根本没有数据管理。这引起了人们对包含敏感或私人信息的担忧。在这项工作中，我们探讨了妊娠超声图像的存在，这些图像包含敏感的个人信息，并且经常在网上共享。通过使用 CLIP 嵌入相似性对 LAION-400M 数据集进行系统检查，我们检索包含妊娠超声的图像并检测数千个私人信息实体，例如姓名和位置。我们的研究结果表明，多张图像包含高风险信息，可能导致重新识别或冒充。最后，我们提出了数据集管理、数据隐私和公共图像数据集道德使用的推荐实践。</li>
</ul>

<h3>Title: Condition Matters in Full-head 3D GANs</h3>
<ul>
<li><strong>Authors: </strong>Heyuan Li, Huimin Zhang, Yuda Qiu, Zhengwentai Sun, Keru Zheng, Lingteng Qiu, Peihao Li, Qi Zuo, Ce Chen, Yujian Zheng, Yuming Gu, Zilong Dong, Xiaoguang Han</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07198">https://arxiv.org/abs/2602.07198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07198">https://arxiv.org/pdf/2602.07198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07198]] Condition Matters in Full-head 3D GANs(https://arxiv.org/abs/2602.07198)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Conditioning is crucial for stable training of full-head 3D GANs. Without any conditioning signal, the model suffers from severe mode collapse, making it impractical to training. However, a series of previous full-head 3D GANs conventionally choose the view angle as the conditioning input, which leads to a bias in the learned 3D full-head space along the conditional view direction. This is evident in the significant differences in generation quality and diversity between the conditional view and non-conditional views of the generated 3D heads, resulting in global incoherence across different head regions. In this work, we propose to use view-invariant semantic feature as the conditioning input, thereby decoupling the generative capability of 3D heads from the viewing direction. To construct a view-invariant semantic condition for each training image, we create a novel synthesized head image dataset. We leverage FLUX.1 Kontext to extend existing high-quality frontal face datasets to a wide range of view angles. The image clip feature extracted from the frontal view is then used as a shared semantic condition across all views in the extended images, ensuring semantic alignment while eliminating directional bias. This also allows supervision from different views of the same subject to be consolidated under a shared semantic condition, which accelerates training and enhances the global coherence of the generated 3D heads. Moreover, as GANs often experience slower improvements in diversity once the generator learns a few modes that successfully fool the discriminator, our semantic conditioning encourages the generator to follow the true semantic distribution, thereby promoting continuous learning and diverse generation. Extensive experiments on full-head synthesis and single-view GAN inversion demonstrate that our method achieves significantly higher fidelity, diversity, and generalizability.</li>
<li><strong>摘要：</strong>调节对于全头 3D GAN 的稳定训练至关重要。在没有任何调节信号的情况下，模型会出现严重的模式崩溃，使其无法进行训练。然而，之前的一系列全头 3D GAN 通常选择视角作为条件输入，这导致学习的 3D 全头空间沿条件视图方向存在偏差。生成的 3D 头部的条件视图和非条件视图之间的生成质量和多样性存在显着差异，导致不同头部区域之间的全局不连贯性，这一点很明显。在这项工作中，我们建议使用视图不变语义特征作为条件输入，从而将 3D 头部的生成能力与观看方向解耦。为了为每个训练图像构建视图不变的语义条件，我们创建了一个新颖的合成头部图像数据集。我们利用 FLUX.1 Kontext 将现有的高质量正面数据集扩展到广泛的视角。然后将从正面视图中提取的图像剪辑特征用作扩展图像中所有视图的共享语义条件，确保语义对齐，同时消除方向偏差。这还允许在共享语义条件下整合来自同一主题的不同视图的监督，从而加速训练并增强生成的 3D 头部的全局一致性。此外，一旦生成器学习了一些成功欺骗鉴别器的模式，GAN 的多样性改进通常会变慢，因此我们的语义条件鼓励生成器遵循真实的语义分布，从而促进持续学习和多样化生成。全头合成和单视图 GAN 反演的大量实验表明，我们的方法实现了显着更高的保真度、多样性和泛化性。</li>
</ul>

<h3>Title: Adaptive Retrieval helps Reasoning in LLMs -- but mostly if it's not used</h3>
<ul>
<li><strong>Authors: </strong>Srijan Shakya, Anamaria-Roberta Hartl, Sepp Hochreiter, Korbinian Pöppel</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07213">https://arxiv.org/abs/2602.07213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07213">https://arxiv.org/pdf/2602.07213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07213]] Adaptive Retrieval helps Reasoning in LLMs -- but mostly if it's not used(https://arxiv.org/abs/2602.07213)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) often falter in complex reasoning tasks due to their static, parametric knowledge, leading to hallucinations and poor performance in specialized domains like mathematics. This work explores a fundamental principle for enhancing generative models: treating retrieval as a form of dynamic in-context learning. We test an adaptive retrieval-augmented architecture where an LLM agent actively decides when to query an external knowledge base during its reasoning process. We compare this adaptive strategy against a standard Chain-of-Thought (CoT) baseline and a static retrieval approach on the GSM8K and MATH-500 benchmarks. Although our experiments show that static retrieval is inferior to CoT, the adaptive retrieval shows interesting behavior: While traces including retrieved results show slightly worse performance compared to CoT, traces that do not include retrieval actually perform better compared to CoT. This suggests that: (a) retrieval only rarely helps reasoning (we show a few counterexamples, e.g. using useful theorems) and (b) actively not using retrieval is indicative of good model performance. Furthermore, we find that the model scales its retrieval frequency with the difficulty of the problem, reinforcing that the decision to retrieve is a crucial metacognitive signal. The agent's ability to self-assess its knowledge and selectively engage with external information represents a key principle for building more robust and reliable generative models.</li>
<li><strong>摘要：</strong>由于其静态的参数化知识，大型语言模型 (LLM) 在复杂的推理任务中常常表现不佳，导致在数学等专业领域产生幻觉和表现不佳。这项工作探索了增强生成模型的基本原则：将检索视为动态上下文学习的一种形式。我们测试了一种自适应检索增强架构，其中 LLM 代理在推理过程中主动决定何时查询外部知识库。我们将这种自适应策略与标准思想链 (CoT) 基线以及 GSM8K 和 MATH-500 基准上的静态检索方法进行比较。尽管我们的实验表明静态检索不如 CoT，但自适应检索显示出有趣的行为：虽然包含检索结果的跟踪显示的性能稍差于 CoT，但不包含检索的跟踪实际上比 CoT 表现更好。这表明：（a）检索很少有助于推理（我们展示了一些反例，例如使用有用的定理）和（b）主动不使用检索表明模型性能良好。此外，我们发现该模型会根据问题的难度来调整检索频率，这强化了检索决策是一个至关重要的元认知信号。智能体自我评估其知识并有选择地参与外部信息的能力代表了构建更强大和更可靠的生成模型的关键原则。</li>
</ul>

<h3>Title: The Double-Edged Sword of Data-Driven Super-Resolution: Adversarial Super-Resolution Models</h3>
<ul>
<li><strong>Authors: </strong>Haley Duba-Sullivan, Steven R. Young, Emma J. Reid</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07251">https://arxiv.org/abs/2602.07251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07251">https://arxiv.org/pdf/2602.07251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07251]] The Double-Edged Sword of Data-Driven Super-Resolution: Adversarial Super-Resolution Models(https://arxiv.org/abs/2602.07251)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Data-driven super-resolution (SR) methods are often integrated into imaging pipelines as preprocessing steps to improve downstream tasks such as classification and detection. However, these SR models introduce a previously unexplored attack surface into imaging pipelines. In this paper, we present AdvSR, a framework demonstrating that adversarial behavior can be embedded directly into SR model weights during training, requiring no access to inputs at inference time. Unlike prior attacks that perturb inputs or rely on backdoor triggers, AdvSR operates entirely at the model level. By jointly optimizing for reconstruction quality and targeted adversarial outcomes, AdvSR produces models that appear benign under standard image quality metrics while inducing downstream misclassification. We evaluate AdvSR on three SR architectures (SRCNN, EDSR, SwinIR) paired with a YOLOv11 classifier and demonstrate that AdvSR models can achieve high attack success rates with minimal quality degradation. These findings highlight a new model-level threat for imaging pipelines, with implications for how practitioners source and validate models in safety-critical applications.</li>
<li><strong>摘要：</strong>数据驱动的超分辨率（SR）方法通常作为预处理步骤集成到成像管道中，以改进分类和检测等下游任务。然而，这些 SR 模型将以前未探索过的攻击面引入到成像管道中。在本文中，我们提出了 AdvSR，这是一个框架，证明对抗行为可以在训练期间直接嵌入到 SR 模型权重中，无需在推理时访问输入。与之前扰乱输入或依赖后门触发器的攻击不同，AdvSR 完全在模型级别运行。通过联合优化重建质量和有针对性的对抗结果，AdvSR 生成的模型在标准图像质量指标下显得良性，同时引发下游错误分类。我们在与 YOLOv11 分类器配对的三种 SR 架构（SRCNN、EDSR、SwinIR）上评估 AdvSR，并证明 AdvSR 模型可以在质量下降最小的情况下实现高攻击成功率。这些发现凸显了成像管道的新模型级威胁，对从业者如何在安全关键型应用中获取和验证模型具有影响。</li>
</ul>

<h3>Title: VideoNeuMat: Neural Material Extraction from Generative Video Models</h3>
<ul>
<li><strong>Authors: </strong>Bowen Xue, Saeed Hadadan, Zheng Zeng, Fabrice Rousselle, Zahra Montazeri, Milos Hasan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07272">https://arxiv.org/abs/2602.07272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07272">https://arxiv.org/pdf/2602.07272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07272]] VideoNeuMat: Neural Material Extraction from Generative Video Models(https://arxiv.org/abs/2602.07272)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Creating photorealistic materials for 3D rendering requires exceptional artistic skill. Generative models for materials could help, but are currently limited by the lack of high-quality training data. While recent video generative models effortlessly produce realistic material appearances, this knowledge remains entangled with geometry and lighting. We present VideoNeuMat, a two-stage pipeline that extracts reusable neural material assets from video diffusion models. First, we finetune a large video model (Wan 2.1 14B) to generate material sample videos under controlled camera and lighting trajectories, effectively creating a "virtual gonioreflectometer" that preserves the model's material realism while learning a structured measurement pattern. Second, we reconstruct compact neural materials from these videos through a Large Reconstruction Model (LRM) finetuned from a smaller Wan 1.3B video backbone. From 17 generated video frames, our LRM performs single-pass inference to predict neural material parameters that generalize to novel viewing and lighting conditions. The resulting materials exhibit realism and diversity far exceeding the limited synthetic training data, demonstrating that material knowledge can be successfully transferred from internet-scale video models into standalone, reusable neural 3D assets.</li>
<li><strong>摘要：</strong>创建用于 3D 渲染的逼真材质需要卓越的艺术技巧。材料的生成模型可能会有所帮助，但目前由于缺乏高质量的培训数据而受到限制。虽然最近的视频生成模型可以毫不费力地产生逼真的材质外观，但这些知识仍然与几何和照明纠缠在一起。我们提出了 VideoNeuMat，这是一个两阶段管道，可从视频扩散模型中提取可重用的神经材料资产。首先，我们对大型视频模型（Wan 2.1 14B）进行微调，以在受控相机和照明轨迹下生成材质样本视频，有效地创建一个“虚拟测角反射仪”，在学习结构化测量模式的同时保留模型的材质真实性。其次，我们通过从较小的 Wan 1.3B 视频主干微调的大型重建模型 (LRM) 从这些视频中重建紧凑的神经材料。我们的 LRM 从 17 个生成的视频帧中执行单遍推理来预测神经材料参数，这些参数可推广到新的观看和照明条件。由此产生的材料表现出的真实性和多样性远远超过了有限的合成训练数据，表明材料知识可以成功地从互联网规模的视频模型转移到独立的、可重复使用的神经 3D 资产中。</li>
</ul>

<h3>Title: Optimization of Precipitate Segmentation Through Linear Genetic Programming of Image Processing</h3>
<ul>
<li><strong>Authors: </strong>Kyle Williams, Andrew Seltzman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07310">https://arxiv.org/abs/2602.07310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07310">https://arxiv.org/pdf/2602.07310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07310]] Optimization of Precipitate Segmentation Through Linear Genetic Programming of Image Processing(https://arxiv.org/abs/2602.07310)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Current analysis of additive manufactured niobium-based copper alloys relies on hand annotation due to varying contrast, noise, and image artifacts present in micrographs, slowing iteration speed in alloy development. We present a filtering and segmentation algorithm for detecting precipitates in FIB cross-section micrographs, optimized using linear genetic programming (LGP), which accounts for the various artifacts. To this end, the optimization environment uses a domain-specific language for image processing to iterate on solutions. Programs in this language are a list of image-filtering blocks with tunable parameters that sequentially process an input image, allowing for reliable generation and mutation by a genetic algorithm. Our environment produces optimized human-interpretable MATLAB code representing an image filtering pipeline. Under ideal conditions--a population size of 60 and a maximum program length of 5 blocks--our system was able to find a near-human accuracy solution with an average evaluation error of 1.8% when comparing segmentations pixel-by-pixel to a human baseline using an XOR error evaluation. Our automation work enabled faster iteration cycles and furthered exploration of the material composition and processing space: our optimized pipeline algorithm processes a 3.6 megapixel image in about 2 seconds on average. This ultimately enables convergence on strong, low-activation, precipitation hardened copper alloys for additive manufactured fusion reactor parts.</li>
<li><strong>摘要：</strong>目前对增材制造的铌基铜合金的分析依赖于手动注释，因为显微照片中存在不同的对比度、噪声和图像伪影，从而减慢了合金开发的迭代速度。我们提出了一种过滤和分割算法，用于检测 FIB 横截面显微照片中的沉淀物，并使用线性遗传编程 (LGP) 进行优化，该算法考虑了各种伪影。为此，优化环境使用特定于领域的图像处理语言来迭代解决方案。这种语言的程序是具有可调参数的图像过滤块的列表，这些参数顺序处理输入图像，从而允许通过遗传算法进行可靠的生成和变异。我们的环境生成优化的人类可解释的 MATLAB 代码，表示图像过滤管道。在理想条件下（群体规模为 60，最大程序长度为 5 个块），我们的系统能够找到接近人类精度的解决方案，当使用 XOR 误差评估将分割逐像素与人类基线进行比较时，平均评估误差为 1.8%。我们的自动化工作实现了更快的迭代周期，并进一步探索了材料成分和处理空间：我们优化的管道算法平均在大约 2 秒内处理 3.6 兆像素的图像。这最终能够融合用于增材制造聚变反应堆零件的高强度、低活化、沉淀硬化铜合金。</li>
</ul>

<h3>Title: Optimizing Few-Step Generation with Adaptive Matching Distillation</h3>
<ul>
<li><strong>Authors: </strong>Lichen Bai, Zikai Zhou, Shitong Shao, Wenliang Zhong, Shuo Yang, Shuo Chen, Bojun Chen, Zeke Xie</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07345">https://arxiv.org/abs/2602.07345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07345">https://arxiv.org/pdf/2602.07345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07345]] Optimizing Few-Step Generation with Adaptive Matching Distillation(https://arxiv.org/abs/2602.07345)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Distribution Matching Distillation (DMD) is a powerful acceleration paradigm, yet its stability is often compromised in Forbidden Zone, regions where the real teacher provides unreliable guidance while the fake teacher exerts insufficient repulsive force. In this work, we propose a unified optimization framework that reinterprets prior art as implicit strategies to avoid these corrupted regions. Based on this insight, we introduce Adaptive Matching Distillation (AMD), a self-correcting mechanism that utilizes reward proxies to explicitly detect and escape Forbidden Zones. AMD dynamically prioritizes corrective gradients via structural signal decomposition and introduces Repulsive Landscape Sharpening to enforce steep energy barriers against failure mode collapse. Extensive experiments across image and video generation tasks (e.g., SDXL, Wan2.1) and rigorous benchmarks (e.g., VBench, GenEval) demonstrate that AMD significantly enhances sample fidelity and training robustness. For instance, AMD improves the HPSv2 score on SDXL from 30.64 to 31.25, outperforming state-of-the-art baselines. These findings validate that explicitly rectifying optimization trajectories within Forbidden Zones is essential for pushing the performance ceiling of few-step generative models.</li>
<li><strong>摘要：</strong>分布匹配蒸馏（DMD）是一种强大的加速范式，但其稳定性在禁区中常常受到损害，在这些禁区中，真老师提供的指导不可靠，而假老师施加的排斥力不足。在这项工作中，我们提出了一个统一的优化框架，将现有技术重新解释为避免这些损坏区域的隐式策略。基于这一见解，我们引入了自适应匹配蒸馏（AMD），这是一种利用奖励代理来明确检测和逃离禁区的自我纠正机制。 AMD 通过结构信号分解动态地确定校正梯度的优先级，并引入排斥景观锐化来加强陡峭的能量屏障，防止故障模式崩溃。跨图像和视频生成任务（例如 SDXL、Wan2.1）和严格基准测试（例如 VBench、GenEval）的大量实验表明，AMD 显着增强了样本保真度和训练鲁棒性。例如，AMD 将 SDXL 上的 HPSv2 分数从 30.64 提高到 31.25，超越了最先进的基准。这些发现证实，明确纠正禁区内的优化轨迹对于推动少步生成模型的性能上限至关重要。</li>
</ul>

<h3>Title: PTB-XL-Image-17K: A Large-Scale Synthetic ECG Image Dataset with Comprehensive Ground Truth for Deep Learning-Based Digitization</h3>
<ul>
<li><strong>Authors: </strong>Naqcho Ali Mehdi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07446">https://arxiv.org/abs/2602.07446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07446">https://arxiv.org/pdf/2602.07446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07446]] PTB-XL-Image-17K: A Large-Scale Synthetic ECG Image Dataset with Comprehensive Ground Truth for Deep Learning-Based Digitization(https://arxiv.org/abs/2602.07446)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Electrocardiogram (ECG) digitization-converting paper-based or scanned ECG images back into time-series signals-is critical for leveraging decades of legacy clinical data in modern deep learning applications. However, progress has been hindered by the lack of large-scale datasets providing both ECG images and their corresponding ground truth signals with comprehensive annotations. We introduce PTB-XL-Image-17K, a complete synthetic ECG image dataset comprising 17,271 high-quality 12-lead ECG images generated from the PTB-XL signal database. Our dataset uniquely provides five complementary data types per sample: (1) realistic ECG images with authentic grid patterns and annotations (50% with visible grid, 50% without), (2) pixel-level segmentation masks, (3) ground truth time-series signals, (4) bounding box annotations in YOLO format for both lead regions and lead name labels, and (5) comprehensive metadata including visual parameters and patient information. We present an open-source Python framework enabling customizable dataset generation with controllable parameters including paper speed (25/50 mm/s), voltage scale (5/10 mm/mV), sampling rate (500 Hz), grid appearance (4 colors), and waveform characteristics. The dataset achieves 100% generation success rate with an average processing time of 1.35 seconds per sample. PTB-XL-Image-17K addresses critical gaps in ECG digitization research by providing the first large-scale resource supporting the complete pipeline: lead detection, waveform segmentation, and signal extraction with full ground truth for rigorous evaluation. The dataset, generation framework, and documentation are publicly available at this https URL and this https URL.</li>
<li><strong>摘要：</strong>心电图 (ECG) 数字化（将纸质或扫描的心电图图像转换回时间序列信号）对于在现代深度学习应用中利用数十年的遗留临床数据至关重要。然而，由于缺乏提供心电图图像及其相应的地面实况信号和全面注释的大规模数据集，进展受到阻碍。我们推出 PTB-XL-Image-17K，这是一个完整的合成心电图图像数据集，包含从 PTB-XL 信号数据库生成的 17,271 张高质量 12 导联心电图图像。我们的数据集为每个样本提供了五种补充数据类型：(1) 具有真实网格图案和注释的真实心电图图像（50% 有可见网格，50% 无），(2) 像素级分割掩模，(3) 地面实况时间序列信号，(4) YOLO 格式的导联区域和导联名称标签的边界框注释，以及 (5) 全面的元数据，包括视觉参数和患者信息。我们提出了一个开源 Python 框架，支持生成可定制的数据集，并具有可控参数，包括走纸速度 (25/50 mm/s)、电压刻度 (5/10 mm/mV)、采样率 (500 Hz)、网格外观（4 种颜色）和波形特征。该数据集实现了 100% 的生成成功率，每个样本的平均处理时间为 1.35 秒。 PTB-XL-Image-17K 通过提供第一个支持完整流程的大规模资源来解决心电图数字化研究中的关键差距：导联检测、波形分割和信号提取，并具有完整的基本事实以进行严格评估。数据集、生成框架和文档可通过此 https URL 和此 https URL 公开获取。</li>
</ul>

<h3>Title: SoulX-FlashHead: Oracle-guided Generation of Infinite Real-time Streaming Talking Heads</h3>
<ul>
<li><strong>Authors: </strong>Tan Yu, Qian Qiao, Le Shen, Ke Zhou, Jincheng Hu, Dian Sheng, Bo Hu, Haoming Qin, Jun Gao, Changhai Zhou, Shunshun Yin, Siyuan Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07449">https://arxiv.org/abs/2602.07449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07449">https://arxiv.org/pdf/2602.07449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07449]] SoulX-FlashHead: Oracle-guided Generation of Infinite Real-time Streaming Talking Heads(https://arxiv.org/abs/2602.07449)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Achieving a balance between high-fidelity visual quality and low-latency streaming remains a formidable challenge in audio-driven portrait generation. Existing large-scale models often suffer from prohibitive computational costs, while lightweight alternatives typically compromise on holistic facial representations and temporal stability. In this paper, we propose SoulX-FlashHead, a unified 1.3B-parameter framework designed for real-time, infinite-length, and high-fidelity streaming video generation. To address the instability of audio features in streaming scenarios, we introduce Streaming-Aware Spatiotemporal Pre-training equipped with a Temporal Audio Context Cache mechanism, which ensures robust feature extraction from short audio fragments. Furthermore, to mitigate the error accumulation and identity drift inherent in long-sequence autoregressive generation, we propose Oracle-Guided Bidirectional Distillation, leveraging ground-truth motion priors to provide precise physical guidance. We also present VividHead, a large-scale, high-quality dataset containing 782 hours of strictly aligned footage to support robust training. Extensive experiments demonstrate that SoulX-FlashHead achieves state-of-the-art performance on HDTF and VFHQ benchmarks. Notably, our Lite variant achieves an inference speed of 96 FPS on a single NVIDIA RTX 4090, facilitating ultra-fast interaction without sacrificing visual coherence.</li>
<li><strong>摘要：</strong>在音频驱动的肖像生成中，实现高保真视觉质量和低延迟流媒体之间的平衡仍然是一个巨大的挑战。现有的大型模型通常会受到计算成本过高的影响，而轻量级模型通常会在整体面部表征和时间稳定性方面做出妥协。在本文中，我们提出了 SoulX-FlashHead，这是一个统一的 1.3B 参数框架，专为实时、无限长度和高保真流视频生成而设计。为了解决流媒体场景中音频特征的不稳定问题，我们引入了配备时间音频上下文缓存机制的流媒体感知时空预训练，确保从短音频片段中可靠地提取特征。此外，为了减轻长序列自回归生成中固有的误差积累和身份漂移，我们提出了Oracle引导的双向蒸馏，利用地面真实运动先验来提供精确的物理指导。我们还推出了 VividHead，这是一个大规模、高质量的数据集，包含 782 小时严格对齐的镜头，以支持稳健的训练。大量实验表明 SoulX-FlashHead 在 HDTF 和 VFHQ 基准测试中实现了最先进的性能。值得注意的是，我们的 Lite 变体在单个 NVIDIA RTX 4090 上实现了 96 FPS 的推理速度，在不牺牲视觉一致性的情况下促进超快速交互。</li>
</ul>

<h3>Title: GlobalWasteData: A Large-Scale, Integrated Dataset for Robust Waste Classification and Environmental Monitoring</h3>
<ul>
<li><strong>Authors: </strong>Misbah Ijaz, Saif Ur Rehman Khan, Abd Ur Rehman, Tayyaba Asif, Sebastian Vollmer, Andreas Dengel, Muhammad Nabeel Asim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07463">https://arxiv.org/abs/2602.07463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07463">https://arxiv.org/pdf/2602.07463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07463]] GlobalWasteData: A Large-Scale, Integrated Dataset for Robust Waste Classification and Environmental Monitoring(https://arxiv.org/abs/2602.07463)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The growing amount of waste is a problem for the environment that requires efficient sorting techniques for various kinds of waste. An automated waste classification system is used for this purpose. The effectiveness of these Artificial Intelligence (AI) models depends on the quality and accessibility of publicly available datasets, which provide the basis for training and analyzing classification algorithms. Although several public waste classification datasets exist, they remain fragmented, inconsistent, and biased toward specific environments. Differences in class names, annotation formats, image conditions, and class distributions make it difficult to combine these datasets or train models that generalize well to real world scenarios. To address these issues, we introduce the GlobalWasteData (GWD) archive, a large scale dataset of 89,807 images across 14 main categories, annotated with 68 distinct subclasses. We compile this novel integrated GWD archive by merging multiple publicly available datasets into a single, unified resource. This GWD archive offers consistent labeling, improved domain diversity, and more balanced class representation, enabling the development of robust and generalizable waste recognition models. Additional preprocessing steps such as quality filtering, duplicate removal, and metadata generation further improve dataset reliability. Overall, this dataset offers a strong foundation for Machine Learning (ML) applications in environmental monitoring, recycling automation, and waste identification, and is publicly available to promote future research and reproducibility.</li>
<li><strong>摘要：</strong>废物量的不断增加给环境带来了问题，需要对各种废物进行有效的分类技术。为此使用了自动废物分类系统。这些人工智能 (AI) 模型的有效性取决于公开数据集的质量和可访问性，这为训练和分析分类算法提供了基础。尽管存在多个公共废物分类数据集，但它们仍然分散、不一致且偏向特定环境。类名称、注释格式、图像条件和类分布的差异使得组合这些数据集或训练能够很好地推广到现实世界场景的模型变得困难。为了解决这些问题，我们引入了 GlobalWasteData (GWD) 档案，这是一个包含 14 个主要类别的 89,807 张图像的大型数据集，并用 68 个不同的子类进行注释。我们通过将多个公开可用的数据集合并到一个统一的资源中来编译这个新颖的集成 GWD 存档。该 GWD 档案提供一致的标签、改进的领域多样性和更平衡的类别表示，从而能够开发强大且通用的废物识别模型。其他预处理步骤（例如质量过滤、重复删除和元数据生成）进一步提高了数据集的可靠性。总体而言，该数据集为环境监测、回收自动化和废物识别方面的机器学习 (ML) 应用奠定了坚实的基础，并且可公开用于促进未来的研究和可重复性。</li>
</ul>

<h3>Title: CoMI-IRL: Contrastive Multi-Intention Inverse Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Antonio Mone, Frans A. Oliehoek, Luciano Cavalcante Siebert</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07496">https://arxiv.org/abs/2602.07496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07496">https://arxiv.org/pdf/2602.07496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07496]] CoMI-IRL: Contrastive Multi-Intention Inverse Reinforcement Learning(https://arxiv.org/abs/2602.07496)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Inverse Reinforcement Learning (IRL) seeks to infer reward functions from expert demonstrations. When demonstrations originate from multiple experts with different intentions, the problem is known as Multi-Intention IRL (MI-IRL). Recent deep generative MI-IRL approaches couple behavior clustering and reward learning, but typically require prior knowledge of the number of true behavioral modes $K^*$. This reliance on expert knowledge limits their adaptability to new behaviors, and only enables analysis related to the learned rewards, and not across the behavior modes used to train them. We propose Contrastive Multi-Intention IRL (CoMI-IRL), a transformer-based unsupervised framework that decouples behavior representation and clustering from downstream reward learning. Our experiments show that CoMI-IRL outperforms existing approaches without a priori knowledge of $K^*$ or labels, while allowing for visual interpretation of behavior relationships and adaptation to unseen behavior without full retraining.</li>
<li><strong>摘要：</strong>逆强化学习（IRL）旨在从专家演示中推断奖励函数。当演示来自具有不同意图的多个专家时，该问题称为多意图 IRL (MI-IRL)。最近的深度生成 MI-IRL 方法将行为聚类和奖励学习结合起来，但通常需要先验了解真实行为模式的数量 $K^*$。这种对专家知识的依赖限制了它们对新行为的适应能力，并且只能进行与学到的奖励相关的分析，而不能跨越用于训练它们的行为模式。我们提出了对比多意图 IRL (CoMI-IRL)，这是一种基于 Transformer 的无监督框架，可将行为表示和聚类与下游奖励学习解耦。我们的实验表明，CoMI-IRL 在没有 $K^*$ 或标签先验知识的情况下优于现有方法，同时允许对行为关系进行视觉解释并适应看不见的行为，而无需完全再训练。</li>
</ul>

<h3>Title: IM-Animation: An Implicit Motion Representation for Identity-decoupled Character Animation</h3>
<ul>
<li><strong>Authors: </strong>Zhufeng Xu, Xuan Gao, Feng-Lin Liu, Haoxian Zhang, Zhixue Fang, Yu-Kun Lai, Xiaoqiang Liu, Pengfei Wan, Lin Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07498">https://arxiv.org/abs/2602.07498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07498">https://arxiv.org/pdf/2602.07498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07498]] IM-Animation: An Implicit Motion Representation for Identity-decoupled Character Animation(https://arxiv.org/abs/2602.07498)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent progress in video diffusion models has markedly advanced character animation, which synthesizes motioned videos by animating a static identity image according to a driving video. Explicit methods represent motion using skeleton, DWPose or other explicit structured signals, but struggle to handle spatial mismatches and varying body scales. %proportions. Implicit methods, on the other hand, capture high-level implicit motion semantics directly from the driving video, but suffer from identity leakage and entanglement between motion and appearance. To address the above challenges, we propose a novel implicit motion representation that compresses per-frame motion into compact 1D motion tokens. This design relaxes strict spatial constraints inherent in 2D representations and effectively prevents identity information leakage from the motion video. Furthermore, we design a temporally consistent mask token-based retargeting module that enforces a temporal training bottleneck, mitigating interference from the source images' motion and improving retargeting consistency. Our methodology employs a three-stage training strategy to enhance the training efficiency and ensure high fidelity. Extensive experiments demonstrate that our implicit motion representation and the propose IM-Animation's generative capabilities are achieve superior or competitive performance compared with state-of-the-art methods.</li>
<li><strong>摘要：</strong>视频扩散模型的最新进展显着改进了角色动画，它通过根据驾驶视频对静态身份图像进行动画处理来合成运动视频。显式方法使用骨骼、DWPose 或其他显式结构化信号来表示运动，但难以处理空间不匹配和变化的身体尺度。 %比例。另一方面，隐式方法直接从驾驶视频中捕获高级隐式运动语义，但会遭受身份泄漏以及运动和外观之间的纠缠。为了解决上述挑战，我们提出了一种新颖的隐式运动表示，将每帧运动压缩为紧凑的一维运动标记。这种设计放松了二维表示中固有的严格空间限制，并有效防止运动视频中的身份信息泄漏。此外，我们设计了一个基于时间一致掩模标记的重定向模块，该模块强制执行时间训练瓶颈，减轻源图像运动的干扰并提高重定向一致性。我们的方法采用三阶段训练策略来提高训练效率并确保高保真度。大量的实验表明，与最先进的方法相比，我们的隐式运动表示和提出的 IM 动画的生成能力实现了卓越或有竞争力的性能。</li>
</ul>

<h3>Title: MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution</h3>
<ul>
<li><strong>Authors: </strong>Jianwen Chen, Xinyu Yang, Peng Xia, Arian Azarang, Yueh Z Lee, Gang Li, Hongtu Zhu, Yun Li, Beidi Chen, Huaxiu Yao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07529">https://arxiv.org/abs/2602.07529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07529">https://arxiv.org/pdf/2602.07529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07529]] MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution(https://arxiv.org/abs/2602.07529)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated strong performance and rapid progress in a wide range of medical reasoning tasks. However, their sequential autoregressive decoding forces inherently parallel clinical reasoning, such as differential diagnosis, into a single linear reasoning path, limiting both efficiency and reliability for complex medical problems. To address this, we propose MedVerse, a reasoning framework for complex medical inference that reformulates medical reasoning as a parallelizable directed acyclic graph (DAG) process based on Petri net theory. The framework adopts a full-stack design across data, model architecture, and system execution. For data creation, we introduce the MedVerse Curator, an automated pipeline that synthesizes knowledge-grounded medical reasoning paths and transforms them into Petri net-structured representations. At the architectural level, we propose a topology-aware attention mechanism with adaptive position indices that supports parallel reasoning while preserving logical consistency. Systematically, we develop a customized inference engine that supports parallel execution without additional overhead. Empirical evaluations show that MedVerse improves strong general-purpose LLMs by up to 8.9%. Compared to specialized medical LLMs, MedVerse achieves comparable performance while delivering a 1.3x reduction in inference latency and a 1.7x increase in generation throughput, enabled by its parallel decoding capability.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在广泛的医学推理任务中表现出了强大的性能和快速的进展。然而，它们的顺序自回归解码迫使本质上并行的临床推理（例如鉴别诊断）进入单一线性推理路径，限制了复杂医疗问题的效率和可靠性。为了解决这个问题，我们提出了 MedVerse，这是一种复杂医学推理的推理框架，它将医学推理重新表述为基于 Petri 网理论的可并行有向无环图 (DAG) 过程。该框架采用跨数据、模型架构、系统执行的全栈设计。对于数据创建，我们引入了 MedVerse Curator，这是一个自动化管道，可以综合基于知识的医学推理路径并将其转换为 Petri 网络结构表示。在架构层面，我们提出了一种具有自适应位置索引的拓扑感知注意机制，支持并行推理，同时保持逻辑一致性。我们系统地开发了一个定制的推理引擎，支持并行执行而无需额外的开销。实证评估表明，MedVerse 将通用型法学硕士的成绩提高了 8.9%。与专业医学法学硕士相比，MedVerse 实现了相当的性能，同时通过其并行解码功能将推理延迟减少了 1.3 倍，并将生成吞吐量提高了 1.7 倍。</li>
</ul>

<h3>Title: FlexID: Training-Free Flexible Identity Injection via Intent-Aware Modulation for Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Guandong Li, Yijun Ding</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07554">https://arxiv.org/abs/2602.07554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07554">https://arxiv.org/pdf/2602.07554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07554]] FlexID: Training-Free Flexible Identity Injection via Intent-Aware Modulation for Text-to-Image Generation(https://arxiv.org/abs/2602.07554)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Personalized text-to-image generation aims to seamlessly integrate specific identities into textual descriptions. However, existing training-free methods often rely on rigid visual feature injection, creating a conflict between identity fidelity and textual adaptability. To address this, we propose FlexID, a novel training-free framework utilizing intent-aware modulation. FlexID orthogonally decouples identity into two dimensions: a Semantic Identity Projector (SIP) that injects high-level priors into the language space, and a Visual Feature Anchor (VFA) that ensures structural fidelity within the latent space. Crucially, we introduce a Context-Aware Adaptive Gating (CAG) mechanism that dynamically modulates the weights of these streams based on editing intent and diffusion timesteps. By automatically relaxing rigid visual constraints when strong editing intent is detected, CAG achieves synergy between identity preservation and semantic variation. Extensive experiments on IBench demonstrate that FlexID achieves a state-of-the-art balance between identity consistency and text adherence, offering an efficient solution for complex narrative generation.</li>
<li><strong>摘要：</strong>个性化文本到图像的生成旨在将特定身份无缝集成到文本描述中。然而，现有的免训练方法通常依赖于严格的视觉特征注入，在身份保真度和文本适应性之间造成冲突。为了解决这个问题，我们提出了 FlexID，这是一种利用意图感知调制的新型免训练框架。 FlexID 将身份正交解耦为两个维度：将高级先验注入语言空间的语义身份投影器 (SIP)，以及确保潜在空间内的结构保真度的视觉特征锚点 (VFA)。至关重要的是，我们引入了上下文感知自适应门控（CAG）机制，该机制根据编辑意图和扩散时间步长动态调节这些流的权重。通过在检测到强烈的编辑意图时自动放松严格的视觉约束，CAG 实现了身份保留和语义变化之间的协同作用。 IBench 上的大量实验表明，FlexID 在身份一致性和文本依从性之间实现了最先进的平衡，为复杂的叙事生成提供了有效的解决方案。</li>
</ul>

<h3>Title: SIGMA: Selective-Interleaved Generation with Multi-Attribute Tokens</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyan Zhang, Zechen Bai, Haofan Wang, Yiren Song</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07564">https://arxiv.org/abs/2602.07564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07564">https://arxiv.org/pdf/2602.07564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07564]] SIGMA: Selective-Interleaved Generation with Multi-Attribute Tokens(https://arxiv.org/abs/2602.07564)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent unified models such as Bagel demonstrate that paired image-edit data can effectively align multiple visual tasks within a single diffusion transformer. However, these models remain limited to single-condition inputs and lack the flexibility needed to synthesize results from multiple heterogeneous sources. We present SIGMA (Selective-Interleaved Generation with Multi-Attribute Tokens), a unified post-training framework that enables interleaved multi-condition generation within diffusion transformers. SIGMA introduces selective multi-attribute tokens, including style, content, subject, and identity tokens, which allow the model to interpret and compose multiple visual conditions in an interleaved text-image sequence. Through post-training on the Bagel unified backbone with 700K interleaved examples, SIGMA supports compositional editing, selective attribute transfer, and fine-grained multimodal alignment. Extensive experiments show that SIGMA improves controllability, cross-condition consistency, and visual quality across diverse editing and generation tasks, with substantial gains over Bagel on compositional tasks.</li>
<li><strong>摘要：</strong>最近的统一模型（例如 Bagel）证明，配对图像编辑数据可以在单个扩散转换器内有效地对齐多个视觉任务。然而，这些模型仍然仅限于单条件输入，并且缺乏合成多个异构源结果所需的灵活性。我们提出了 SIGMA（具有多属性标记的选择性交错生成），这是一个统一的后训练框架，可在扩散变压器内实现交错多条件生成。 SIGMA 引入了选择性多属性标记，包括样式、内容、主题和身份标记，这使得模型能够解释和组合交错的文本图像序列中的多个视觉条件。通过在 Bagel 统一主干上使用 700K 交错示例进行后训练，SIGMA 支持组合编辑、选择性属性传输和细粒度多模态对齐。大量实验表明，SIGMA 提高了各种编辑和生成任务的可控性、跨条件一致性和视觉质量，在合成任务上比 Bagel 有了显着的进步。</li>
</ul>

<h3>Title: Cross-Camera Cow Identification via Disentangled Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Runcheng Wang, Yaru Chen, Guiguo Zhang, Honghua Jiang, Yongliang Qiao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07566">https://arxiv.org/abs/2602.07566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07566">https://arxiv.org/pdf/2602.07566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07566]] Cross-Camera Cow Identification via Disentangled Representation Learning(https://arxiv.org/abs/2602.07566)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Precise identification of individual cows is a fundamental prerequisite for comprehensive digital management in smart livestock farming. While existing animal identification methods excel in controlled, single-camera settings, they face severe challenges regarding cross-camera generalization. When models trained on source cameras are deployed to new monitoring nodes characterized by divergent illumination, backgrounds, viewpoints, and heterogeneous imaging properties, recognition performance often degrades dramatically. This limits the large-scale application of non-contact technologies in dynamic, real-world farming environments. To address this challenge, this study proposes a cross-camera cow identification framework based on disentangled representation learning. This framework leverages the Subspace Identifiability Guarantee (SIG) theory in the context of bovine visual recognition. By modeling the underlying physical data generation process, we designed a principle-driven feature disentanglement module that decomposes observed images into multiple orthogonal latent subspaces. This mechanism effectively isolates stable, identity-related biometric features that remain invariant across cameras, thereby substantially improving generalization to unseen cameras. We constructed a high-quality dataset spanning five distinct camera nodes, covering heterogeneous acquisition devices and complex variations in lighting and angles. Extensive experiments across seven cross-camera tasks demonstrate that the proposed method achieves an average accuracy of 86.0%, significantly outperforming the Source-only Baseline (51.9%) and the strongest cross-camera baseline method (79.8%). This work establishes a subspace-theoretic feature disentanglement framework for collaborative cross-camera cow identification, offering a new paradigm for precise animal monitoring in uncontrolled smart farming environments.</li>
<li><strong>摘要：</strong>对奶牛个体的精确识别是智慧畜牧业全面数字化管理的基本前提。虽然现有的动物识别方法在受控的单摄像头设置中表现出色，但它们在跨摄像头泛化方面面临着严峻的挑战。当在源摄像机上训练的模型被部署到以发散照明、背景、视点和异构成像特性为特征的新监控节点时，识别性能通常会急剧下降。这限制了非接触技术在动态的现实农业环境中的大规模应用。为了应对这一挑战，本研究提出了一种基于解缠表示学习的跨摄像头奶牛识别框架。该框架在牛视觉识别的背景下利用了子空间可识别性保证（SIG）理论。通过对底层物理数据生成过程进行建模，我们设计了一个原理驱动的特征解缠模块，将观察到的图像分解为多个正交潜在子空间。这种机制有效地隔离了稳定的、与身份相关的生物识别特征，这些特征在摄像机之间保持不变，从而大大提高了对看不见的摄像机的泛化能力。我们构建了一个跨越五个不同相机节点的高质量数据集，涵盖异构采集设备以及照明和角度的复杂变化。七个跨相机任务的大量实验表明，所提出的方法的平均准确率达到 86.0%，显着优于仅源基线（51.9%）和最强的跨相机基线方法（79.8%）。这项工作建立了一个用于协作跨摄像头奶牛识别的子空间理论特征解缠框架，为不受控制的智能农业环境中的精确动物监测提供了新的范例。</li>
</ul>

<h3>Title: Unified Biomolecular Trajectory Generation via Pretrained Variational Bridge</h3>
<ul>
<li><strong>Authors: </strong>Ziyang Yu, Wenbing Huang, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07588">https://arxiv.org/abs/2602.07588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07588">https://arxiv.org/pdf/2602.07588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07588]] Unified Biomolecular Trajectory Generation via Pretrained Variational Bridge(https://arxiv.org/abs/2602.07588)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Molecular Dynamics (MD) simulations provide a fundamental tool for characterizing molecular behavior at full atomic resolution, but their applicability is severely constrained by the computational cost. To address this, a surge of deep generative models has recently emerged to learn dynamics at coarsened timesteps for efficient trajectory generation, yet they either generalize poorly across systems or, due to limited molecular diversity of trajectory data, fail to fully exploit structural information to improve generative fidelity. Here, we present the Pretrained Variational Bridge (PVB) in an encoder-decoder fashion, which maps the initial structure into a noised latent space and transports it toward stage-specific targets through augmented bridge matching. This unifies training on both single-structure and paired trajectory data, enabling consistent use of cross-domain structural knowledge across training stages. Moreover, for protein-ligand complexes, we further introduce a reinforcement learning-based optimization via adjoint matching that speeds progression toward the holo state, which supports efficient post-optimization of docking poses. Experiments on proteins and protein-ligand complexes demonstrate that PVB faithfully reproduces thermodynamic and kinetic observables from MD while delivering stable and efficient generative dynamics.</li>
<li><strong>摘要：</strong>分子动力学 (MD) 模拟提供了在全原子分辨率下表征分子行为的基本工具，但其适用性受到计算成本的严重限制。为了解决这个问题，最近出现了一批深度生成模型，以粗化时间步长学习动力学，以实现有效的轨迹生成，但它们要么在系统中泛化性较差，要么由于轨迹数据的分子多样性有限，无法充分利用结构信息来提高生成保真度。在这里，我们以编码器-解码器的方式呈现预训练变分桥（PVB），它将初始结构映射到噪声潜在空间，并通过增强桥匹配将其传输到特定阶段的目标。这统一了对单结构和配对轨迹数据的训练，从而能够在训练阶段一致地使用跨域结构知识。此外，对于蛋白质-配体复合物，我们通过伴随匹配进一步引入基于强化学习的优化，加速向全息状态的进展，这支持对接姿势的有效后优化。对蛋白质和蛋白质-配体复合物的实验表明，PVB 忠实地再现了 MD 的热力学和动力学观测值，同时提供稳定且高效的生成动力学。</li>
</ul>

<h3>Title: Automated rock joint trace mapping using a supervised learning model trained on synthetic data generated by parametric modelling</h3>
<ul>
<li><strong>Authors: </strong>Jessica Ka Yi Chiu, Tom Frode Hansen, Eivind Magnus Paulsen, Ole Jakob Mengshoel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07590">https://arxiv.org/abs/2602.07590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07590">https://arxiv.org/pdf/2602.07590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07590]] Automated rock joint trace mapping using a supervised learning model trained on synthetic data generated by parametric modelling(https://arxiv.org/abs/2602.07590)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This paper presents a geology-driven machine learning method for automated rock joint trace mapping from images. The approach combines geological modelling, synthetic data generation, and supervised image segmentation to address limited real data and class imbalance. First, discrete fracture network models are used to generate synthetic jointed rock images at field-relevant scales via parametric modelling, preserving joint persistence, connectivity, and node-type distributions. Second, segmentation models are trained using mixed training and pretraining followed by fine-tuning on real images. The method is tested in box and slope domains using several real datasets. The results show that synthetic data can support supervised joint trace detection when real data are scarce. Mixed training performs well when real labels are consistent (e.g. box-domain), while fine-tuning is more robust when labels are noisy (e.g. slope-domain where labels can be biased, incomplete, and inconsistent). Fully zero-shot prediction from synthetic model remains limited, but useful generalisation is achieved by fine-tuning with a small number of real data. Qualitative analysis shows clearer and more geologically meaningful joint traces than indicated by quantitative metrics alone. The proposed method supports reliable joint mapping and provides a basis for further work on domain adaptation and evaluation.</li>
<li><strong>摘要：</strong>本文提出了一种地质驱动的机器学习方法，用于从图像中自动绘制岩石节理轨迹。该方法结合了地质建模、合成数据生成和监督图像分割，以解决有限的真实数据和类别不平衡问题。首先，离散裂缝网络模型用于通过参数建模生成现场相关尺度的合成节理岩石图像，保留节理持久性、连通性和节点类型分布。其次，使用混合训练和预训练来训练分割模型，然后对真实图像进行微调。该方法使用多个真实数据集在箱域和斜率域中进行了测试。结果表明，当真实数据稀缺时，合成数据可以支持有监督的联合痕迹检测。当真实标签一致时（例如框域），混合训练表现良好，而当标签有噪声时（例如标签可能有偏差、不完整和不一致的斜率域），微调会更加稳健。合成模型的完全零样本预测仍然有限，但通过使用少量实际数据进行微调可以实现有用的泛化。与单独的定量指标相比，定性分析显示出更清晰、更具地质意义的节理痕迹。所提出的方法支持可靠的联合映射，并为域适应和评估的进一步工作提供了基础。</li>
</ul>

<h3>Title: TeleBoost: A Systematic Alignment Framework for High-Fidelity, Controllable, and Robust Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Yuanzhi Liang, Xuan'er Wu, Yirui Liu, Yijie Fang, Yizhen Fan, Ke Hao, Rui Li, Ruiying Liu, Ziqi Ni, Peng Yu, Yanbo Wang, Haibin Huang, Qizhen Weng, Chi Zhang, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07595">https://arxiv.org/abs/2602.07595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07595">https://arxiv.org/pdf/2602.07595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07595]] TeleBoost: A Systematic Alignment Framework for High-Fidelity, Controllable, and Robust Video Generation(https://arxiv.org/abs/2602.07595)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Post-training is the decisive step for converting a pretrained video generator into a production-oriented model that is instruction-following, controllable, and robust over long temporal horizons. This report presents a systematical post-training framework that organizes supervised policy shaping, reward-driven reinforcement learning, and preference-based refinement into a single stability-constrained optimization stack. The framework is designed around practical video-generation constraints, including high rollout cost, temporally compounding failure modes, and feedback that is heterogeneous, uncertain, and often weakly discriminative. By treating optimization as a staged, diagnostic-driven process rather than a collection of isolated tricks, the report summarizes a cohesive recipe for improving perceptual fidelity, temporal coherence, and prompt adherence while preserving the controllability established at initialization. The resulting framework provides a clear blueprint for building scalable post-training pipelines that remain stable, extensible, and effective in real-world deployment settings.</li>
<li><strong>摘要：</strong>后训练是将预训练视频生成器转换为面向生产的模型的决定性步骤，该模型在长时间范围内遵循指令、可控且鲁棒。本报告提出了一个系统的训练后框架，它将监督策略制定、奖励驱动的强化学习和基于偏好的细化组织到单个稳定性约束的优化堆栈中。该框架是围绕实际的视频生成限制而设计的，包括高部署成本、暂时复合的故障模式以及异构、不确定且通常辨别力较弱的反馈。通过将优化视为一个分阶段的、诊断驱动的过程而不是孤立技巧的集合，该报告总结了一个有凝聚力的方法，用于提高感知保真度、时间连贯性和即时依从性，同时保留初始化时建立的可控性。由此产生的框架为构建可扩展的训练后管道提供了清晰的蓝图，这些管道在现实部署环境中保持稳定、可扩展和有效。</li>
</ul>

<h3>Title: Surprisal-Guided Selection: Compute-Optimal Test-Time Strategies for Execution-Grounded Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Jarrod Barnes</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07670">https://arxiv.org/abs/2602.07670</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07670">https://arxiv.org/pdf/2602.07670</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07670]] Surprisal-Guided Selection: Compute-Optimal Test-Time Strategies for Execution-Grounded Code Generation(https://arxiv.org/abs/2602.07670)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Test-time training (TTT) adapts language models through gradient-based updates at inference. But is adaptation the right strategy? We study compute-optimal test-time strategies for verifiable execution-grounded (VEG) tasks, domains like GPU kernel optimization where a deterministic evaluator provides dense, continuous reward signals. Using KernelBench as our testbed and a 120B-parameter model (GPT-OSS-120B with LoRA adaptation), we find that search outperforms minimal adaptation (1-5 gradient steps): Best-of-N sampling achieves 90% task success (18/20 tasks) at K=64 across the full KernelBench L1 eval set while TTT's best checkpoint reaches only 30.6% (3-seed mean), with TTT's "equivalent K" falling below 1, worse than single-sample inference. The failure mode is over-sharpening: gradient updates collapse diversity toward mediocre solutions rather than discovering optimal ones. Our main contribution is surprisal-guided selection: selecting the highest-surprisal (lowest-confidence) correct sample yields 80% success vs. 50% for most-confident selection, a 30% improvement. Extending to surprisal-guided-top3 matches oracle performance at 100%. This zero-cost strategy, validated through length-controlled analysis, recovers oracle performance. For dense-reward VEG tasks, compute should be allocated to sample diversity and intelligent selection rather than gradient adaptation. The surprisal-guided selection principle may generalize to other execution-grounded domains where optimal solutions occupy the distribution tail.</li>
<li><strong>摘要：</strong>测试时训练 (TTT) 通过推理时基于梯度的更新来调整语言模型。但适应是正确的策略吗？我们研究可验证执行基础 (VEG) 任务的计算最优测试时间策略，例如 GPU 内核优化等领域，其中确定性评估器提供密集、连续的奖励信号。使用 KernelBench 作为我们的测试平台和 120B 参数模型（具有 LoRA 适应的 GPT-OSS-120B），我们发现搜索优于最小适应（1-5 个梯度步骤）：在整个 KernelBench L1 评估集中，Best-of-N 采样在 K=64 时实现了 90% 的任务成功率（18/20 个任务），而 TTT 的最佳检查点仅达到 30.6%（3 种子）平均值），TTT 的“等效 K”低于 1，比单样本推断更糟糕。失败模式是过度锐化：梯度更新会破坏多样性，导致平庸的解决方案，而不是发现最佳的解决方案。我们的主要贡献是意外引导选择：选择最意外（最低置信度）的正确样本的成功率为 80%，而最置信度选择的成功率为 50%，提高了 30%。扩展到 surrisal-guided-top3 匹配预言机性能为 100%。这种零成本策略通过长度控制分析进行验证，可恢复预言机性能。对于密集奖励 VEG 任务，计算应分配给样本多样性和智能选择，而不是梯度适应。意外引导选择原则可以推广到其他以执行为基础的领域，其中最优解决方案占据分布尾部。</li>
</ul>

<h3>Title: Dense Feature Learning via Linear Structure Preservation in Medical Data</h3>
<ul>
<li><strong>Authors: </strong>Yuanyun Zhang, Mingxuan Zhang, Siyuan Li, Zihan Wang, Haoran Chen, Wenbo Zhou, Shi Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07706">https://arxiv.org/abs/2602.07706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07706">https://arxiv.org/pdf/2602.07706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07706]] Dense Feature Learning via Linear Structure Preservation in Medical Data(https://arxiv.org/abs/2602.07706)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep learning models for medical data are typically trained using task specific objectives that encourage representations to collapse onto a small number of discriminative directions. While effective for individual prediction problems, this paradigm underutilizes the rich structure of clinical data and limits the transferability, stability, and interpretability of learned features. In this work, we propose dense feature learning, a representation centric framework that explicitly shapes the linear structure of medical embeddings. Our approach operates directly on embedding matrices, encouraging spectral balance, subspace consistency, and feature orthogonality through objectives defined entirely in terms of linear algebraic properties. Without relying on labels or generative reconstruction, dense feature learning produces representations with higher effective rank, improved conditioning, and greater stability across time. Empirical evaluations across longitudinal EHR data, clinical text, and multimodal patient representations demonstrate consistent improvements in downstream linear performance, robustness, and subspace alignment compared to supervised and self supervised baselines. These results suggest that learning to span clinical variation may be as important as learning to predict clinical outcomes, and position representation geometry as a first class objective in medical AI.</li>
<li><strong>摘要：</strong>医疗数据的深度学习模型通常使用特定于任务的目标进行训练，这些目标鼓励表征折叠到少量的判别方向上。虽然对于个体预测问题有效，但这种范式没有充分利用临床数据的丰富结构，并限制了学习特征的可转移性、稳定性和可解释性。在这项工作中，我们提出了密集特征学习，这是一种以表示为中心的框架，可以明确地塑造医学嵌入的线性结构。我们的方法直接对嵌入矩阵进行操作，通过完全根据线性代数性质定义的目标来促进谱平衡、子空间一致性和特征正交性。在不依赖标签或生成重建的情况下，密集特征学习可以产生具有更高有效等级、改进条件和更大时间稳定性的表示。对纵向 EHR 数据、临床文本和多模式患者表征的实证评估表明，与监督和自我监督基线相比，下游线性性能、鲁棒性和子空间对齐得到了一致改善。这些结果表明，学习跨越临床变异可能与学习预测临床结果一样重要，而位置表示几何学是医学人工智能的首要目标。</li>
</ul>

<h3>Title: Towards Robust Scaling Laws for Optimizers</h3>
<ul>
<li><strong>Authors: </strong>Alexandra Volkova, Mher Safaryan, Christoph H. Lampert, Dan Alistarh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07712">https://arxiv.org/abs/2602.07712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07712">https://arxiv.org/pdf/2602.07712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07712]] Towards Robust Scaling Laws for Optimizers(https://arxiv.org/abs/2602.07712)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The quality of Large Language Model (LLM) pretraining depends on multiple factors, including the compute budget and the choice of optimization algorithm. Empirical scaling laws are widely used to predict loss as model size and training data grow, however, almost all existing studies fix the optimizer (typically AdamW). At the same time, a new generation of optimizers (e.g., Muon, Shampoo, SOAP) promises faster and more stable convergence, but their relationship with model and data scaling is not yet well understood. In this work, we study scaling laws across different optimizers. Empirically, we show that 1) separate Chinchilla-style scaling laws for each optimizer are ill-conditioned and have highly correlated parameters. Instead, 2) we propose a more robust law with shared power-law exponents and optimizer-specific rescaling factors, which enable direct comparison between optimizers. Finally, 3) we provide a theoretical analysis of gradient-based methods for the proxy task of a convex quadratic objective, demonstrating that Chinchilla-style scaling laws emerge naturally as a result of loss decomposition into irreducible, approximation, and optimization errors.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 预训练的质量取决于多种因素，包括计算预算和优化算法的选择。随着模型大小和训练数据的增长，经验缩放定律被广泛用于预测损失，但是，几乎所有现有研究都修复了优化器（通常是 AdamW）。与此同时，新一代优化器（例如 Muon、Shampoo、SOAP）承诺更快、更稳定的收敛，但它们与模型和数据扩展的关系尚未得到很好的理解。在这项工作中，我们研究了不同优化器的缩放法则。根据经验，我们表明 1）每个优化器的单独 Chinchilla 式缩放法则是病态的并且具有高度相关的参数。相反，2）我们提出了一个更稳健的定律，具有共享的幂律指数和优化器特定的缩放因子，这使得优化器之间能够直接比较。最后，3）我们为凸二次目标的代理任务提供了基于梯度的方法的理论分析，证明了 Chinchilla 式缩放定律作为损失分解为不可约误差、近似误差和优化误差的结果而自然出现。</li>
</ul>

<h3>Title: ParisKV: Fast and Drift-Robust KV-Cache Retrieval for Long-Context LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yanlin Qi, Xinhang Chen, Huiqiang Jiang, Qitong Wang, Botao Peng, Themis Palpanas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07721">https://arxiv.org/abs/2602.07721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07721">https://arxiv.org/pdf/2602.07721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07721]] ParisKV: Fast and Drift-Robust KV-Cache Retrieval for Long-Context LLMs(https://arxiv.org/abs/2602.07721)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>KV-cache retrieval is essential for long-context LLM inference, yet existing methods struggle with distribution drift and high latency at scale. We introduce ParisKV, a drift-robust, GPU-native KV-cache retrieval framework based on collision-based candidate selection, followed by a quantized inner-product reranking estimator. For million-token contexts, ParisKV supports CPU-offloaded KV caches via Unified Virtual Addressing (UVA), enabling on-demand top-$k$ fetching with minimal overhead. ParisKV matches or outperforms full attention quality on long-input and long-generation benchmarks. It achieves state-of-the-art long-context decoding efficiency: it matches or exceeds full attention speed even at batch size 1 for long contexts, delivers up to 2.8$\times$ higher throughput within full attention's runnable range, and scales to million-token contexts where full attention runs out of memory. At million-token scale, ParisKV reduces decode latency by 17$\times$ and 44$\times$ compared to MagicPIG and PQCache, respectively, two state-of-the-art KV-cache Top-$k$ retrieval baselines.</li>
<li><strong>摘要：</strong>KV 缓存检索对于长上下文 LLM 推理至关重要，但现有方法在大规模分布漂移和高延迟方面遇到了困难。我们介绍了 ParisKV，这是一种抗漂移、GPU 原生的 KV 缓存检索框架，基于基于碰撞的候选选择，然后是量化内积重排序估计器。对于百万个令牌上下文，ParisKV 通过统一虚拟寻址 (UVA) 支持 CPU 卸载的 KV 缓存，从而以最小的开销实现按需的 top-$k$ 获取。 ParisKV 在长输入和长生成基准上匹配或优于完全注意力质量。它实现了最先进的长上下文解码效率：即使在长上下文的批量大小为 1 的情况下，它也能匹配或超过全注意力速度，在全注意力的可运行范围内提供高达 2.8$\times$ 的吞吐量，并可扩展到全注意力耗尽内存的百万令牌上下文。在百万令牌规模上，与 MagicPIG 和 PQCache（两个最先进的 KV 缓存 Top-$k$ 检索基线）相比，ParisKV 分别将解码延迟减少了 17$\times$ 和 44$\times$。</li>
</ul>

<h3>Title: TerraBind: Fast and Accurate Binding Affinity Prediction through Coarse Structural Representations</h3>
<ul>
<li><strong>Authors: </strong>Matteo Rossi, Ryan Pederson, Miles Wang-Henderson, Ben Kaufman, Edward C. Williams, Carl Underkoffler, Owen Lewis Howell, Adrian Layer, Stephan Thaler, Narbe Mardirossian, John Anthony Parkhill</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07735">https://arxiv.org/abs/2602.07735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07735">https://arxiv.org/pdf/2602.07735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07735]] TerraBind: Fast and Accurate Binding Affinity Prediction through Coarse Structural Representations(https://arxiv.org/abs/2602.07735)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present TerraBind, a foundation model for protein-ligand structure and binding affinity prediction that achieves 26-fold faster inference than state-of-the-art methods while improving affinity prediction accuracy by $\sim$20\%. Current deep learning approaches to structure-based drug design rely on expensive all-atom diffusion to generate 3D coordinates, creating inference bottlenecks that render large-scale compound screening computationally intractable. We challenge this paradigm with a critical hypothesis: full all-atom resolution is unnecessary for accurate small molecule pose and binding affinity prediction. TerraBind tests this hypothesis through a coarse pocket-level representation (protein C$_\beta$ atoms and ligand heavy atoms only) within a multimodal architecture combining COATI-3 molecular encodings and ESM-2 protein embeddings that learns rich structural representations, which are used in a diffusion-free optimization module for pose generation and a binding affinity likelihood prediction module. On structure prediction benchmarks (FoldBench, PoseBusters, Runs N' Poses), TerraBind matches diffusion-based baselines in ligand pose accuracy. Crucially, TerraBind outperforms Boltz-2 by $\sim$20\% in Pearson correlation for binding affinity prediction on both a public benchmark (CASP16) and a diverse proprietary dataset (18 biochemical/cell assays). We show that the affinity prediction module also provides well-calibrated affinity uncertainty estimates, addressing a critical gap in reliable compound prioritization for drug discovery. Furthermore, this module enables a continual learning framework and a hedged batch selection strategy that, in simulated drug discovery cycles, achieves 6$\times$ greater affinity improvement of selected molecules over greedy-based approaches.</li>
<li><strong>摘要：</strong>我们推出了 TerraBind，这是一种用于蛋白质配体结构和结合亲和力预测的基础模型，其推理速度比最先进的方法快 26 倍，同时将亲和力预测精度提高 $\sim$20\%。当前基于结构的药物设计的深度学习方法依赖于昂贵的全原子扩散来生成 3D 坐标，从而产生了推理瓶颈，使得大规模化合物筛选在计算上变得困难。我们用一个关键假设挑战这一范式：对于准确的小分子位姿和结合亲和力预测来说，完全的全原子分辨率是不必要的。 TerraBind 通过结合 COATI-3 分子编码和 ESM-2 蛋白质嵌入的多模式架构中的粗口袋级表示（仅蛋白质 C$_\​​beta$ 原子和配体重原子）来测试这一假设，该架构可学习丰富的结构表示，这些结构表示用于姿势生成的无扩散优化模块和结合亲和力可能性预测模块。在结构预测基准（FoldBench、PoseBusters、Runs N' Poses）上，TerraBind 在配体姿势精度方面匹配基于扩散的基线。至关重要的是，在公共基准 (CASP16) 和多种专有数据集（18 种生化/细胞测定）上的结合亲和力预测的 Pearson 相关性中，TerraBind 的性能优于 Boltz-2 $\sim$20\%。我们表明，亲和力预测模块还提供了经过良好校准的亲和力不确定性估计，解决了药物发现的可靠化合物优先顺序中的关键差距。此外，该模块支持持续学习框架和对冲批次选择策略，在模拟药物发现周期中，与基于贪婪的方法相比，所选分子的亲和力提高了 6 倍。</li>
</ul>

<h3>Title: Learnable Chernoff Baselines for Inference-Time Alignment</h3>
<ul>
<li><strong>Authors: </strong>Sunil Madhow, Yuchen Liang, Ness Shroff, Yingbin Liang, Yu-Xiang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07738">https://arxiv.org/abs/2602.07738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07738">https://arxiv.org/pdf/2602.07738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07738]] Learnable Chernoff Baselines for Inference-Time Alignment(https://arxiv.org/abs/2602.07738)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We study inference-time reward-guided alignment for generative models. Existing methods often rely on either architecture-specific adaptations or computationally costly inference procedures. We introduce Learnable Chernoff Baselines (LCBs) as a method for efficiently and approximately sampling from the exponentially tilted kernels that arise from KL-regularized reward alignment. Using only black-box sampling access to the pretrained model, LCBs implement a form of rejection sampling with adaptively selected acceptance probabilities, which allows fine-grained control over inference-compute scaling. We establish total-variation guarantees to the ideal aligned model, and demonstrate in both continuous and discrete diffusion settings that LCB sampling closely matches ideal rejection sampling while using substantially fewer queries to the pretrained model.</li>
<li><strong>摘要：</strong>我们研究生成模型的推理时间奖励引导对齐。现有方法通常依赖于特定于体系结构的适应或计算成本高昂的推理过程。我们引入可学习切尔诺夫基线（LCB）作为一种从 KL 正则化奖励对齐产生的指数倾斜核中高效、近似采样的方法。 LCB 仅使用对预训练模型的黑盒采样访问，实现了一种具有自适应选择接受概率的拒绝采样形式，从而允许对推理计算扩展进行细粒度控制。我们为理想对齐模型建立了总变异保证，并在连续和离散扩散设置中证明，LCB 采样与理想拒绝采样非常匹配，同时对预训练模型使用的查询要少得多。</li>
</ul>

<h3>Title: Riemannian MeanFlow</h3>
<ul>
<li><strong>Authors: </strong>Dongyeop Woo, Marta Skreta, Seonghyun Park, Sungsoo Ahn, Kirill Neklyudov</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07744">https://arxiv.org/abs/2602.07744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07744">https://arxiv.org/pdf/2602.07744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07744]] Riemannian MeanFlow(https://arxiv.org/abs/2602.07744)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion and flow models have become the dominant paradigm for generative modeling on Riemannian manifolds, with successful applications in protein backbone generation and DNA sequence design. However, these methods require tens to hundreds of neural network evaluations at inference time, which can become a computational bottleneck in large-scale scientific sampling workflows. We introduce Riemannian MeanFlow~(RMF), a framework for learning flow maps directly on manifolds, enabling high-quality generations with as few as one forward pass. We derive three equivalent characterizations of the manifold average velocity (Eulerian, Lagrangian, and semigroup identities), and analyze parameterizations and stabilization techniques to improve training on high-dimensional manifolds. In promoter DNA design and protein backbone generation settings, RMF achieves comparable sample quality to prior methods while requiring up to 10$\times$ fewer function evaluations. Finally, we show that few-step flow maps enable efficient reward-guided design through reward look-ahead, where terminal states can be predicted from intermediate steps at minimal additional cost.</li>
<li><strong>摘要：</strong>扩散和流动模型已成为黎曼流形生成模型的主导范式，并在蛋白质主干生成和 DNA 序列设计中得到成功应用。然而，这些方法在推理时需要数十到数百次神经网络评估，这可能成为大规模科学采样工作流程中的计算瓶颈。我们引入了 Riemannian MeanFlow~(RMF)，这是一种直接在流形上学习流图的框架，只需一次前向传递即可实现高质量生成。我们推导了流形平均速度的三个等效特征（欧拉、拉格朗日和半群恒等式），并分析参数化和稳定技术以改进高维流形的训练。在启动子 DNA 设计和蛋白质骨架生成设置中，RMF 实现了与之前方法相当的样品质量，同时需要的功能评估减少了 10 倍。最后，我们表明，少步流程图可以通过奖励前瞻实现有效的奖励引导设计，其中可以以最小的额外成本从中间步骤预测最终状态。</li>
</ul>

<h3>Title: Fairness Aware Reward Optimization</h3>
<ul>
<li><strong>Authors: </strong>Ching Lam Choi, Vighnesh Subramaniam, Phillip Isola, Antonio Torralba, Stefanie Jegelka</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07799">https://arxiv.org/abs/2602.07799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07799">https://arxiv.org/pdf/2602.07799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07799]] Fairness Aware Reward Optimization(https://arxiv.org/abs/2602.07799)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Demographic skews in human preference data propagate systematic unfairness through reward models into aligned LLMs. We introduce Fairness Aware Reward Optimization (Faro), an in-processing framework that trains reward models under demographic parity, equalized odds, or counterfactual fairness constraints. We provide the first theoretical analysis of reward-level fairness in LLM alignment, establishing: (i) provable fairness certificates for Faro-trained rewards with controllable slack; a (ii) formal characterization of the accuracy-fairness trade-off induced by KL-regularized fine-tuning, proving fairness transfers from reward to policy; and the (iii) existence of a non-empty Pareto frontier. Unlike pre- and post-processing methods, Faro ensures reward models are simultaneously ordinal (ranking correctly), cardinal (calibrated), and fair. Across multiple LLMs and benchmarks, Faro significantly reduces bias and harmful generations while maintaining or improving model quality.</li>
<li><strong>摘要：</strong>人类偏好数据中的人口统计偏差通过奖励模型将系统性不公平传播到一致的法学硕士中。我们引入公平意识奖励优化（Faro），这是一个处理中的框架，可在人口平等、均等赔率或反事实公平约束下训练奖励模型。我们提供了 LLM 调整中奖励水平公平性的第一个理论分析，建立：（i）具有可控松弛的 Faro 训练奖励的可证明公平性证书； a (ii) KL 正则化微调引起的准确性与公平性权衡的正式表征，证明从奖励到政策的公平性转移； (iii) 存在非空帕累托边界。与预处理和后处理方法不同，Faro 确保奖励模型同时是序数（正确排名）、基数（校准）和公平的。在多个法学硕士和基准测试中，Faro 显着减少了偏差和有害生成，同时保持或提高了模型质量。</li>
</ul>

<h3>Title: Back to Physics: Operator-Guided Generative Paths for SMS MRI Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Zhibo Chen, Yu Guan, Yajuan Huang, Chaoqi Chen, XiangJi, Qiuyun Fan, Dong Liang, Qiegen Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07820">https://arxiv.org/abs/2602.07820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07820">https://arxiv.org/pdf/2602.07820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07820]] Back to Physics: Operator-Guided Generative Paths for SMS MRI Reconstruction(https://arxiv.org/abs/2602.07820)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Simultaneous multi-slice (SMS) imaging with in-plane undersampling enables highly accelerated MRI but yields a strongly coupled inverse problem with deterministic inter-slice interference and missing k-space data. Most diffusion-based reconstructions are formulated around Gaussian-noise corruption and rely on additional consistency steps to incorporate SMS physics, which can be mismatched to the operator-governed degradations in SMS acquisition. We propose an operator-guided framework that models the degradation trajectory using known acquisition operators and inverts this process via deterministic updates. Within this framework, we introduce an operator-conditional dual-stream interaction network (OCDI-Net) that explicitly disentangles target-slice content from inter-slice interference and predicts structured degradations for operator-aligned inversion, and we instantiate reconstruction as a two-stage chained inference procedure that performs SMS slice separation followed by in-plane completion. Experiments on fastMRI brain data and prospectively acquired in vivo diffusion MRI data demonstrate improved fidelity and reduced slice leakage over conventional and learning-based SMS reconstructions.</li>
<li><strong>摘要：</strong>具有面内欠采样的同步多切片 (SMS) 成像可实现高度加速的 MRI，但会产生强耦合逆问题，导致确定性切片间干扰和丢失 k 空间数据。大多数基于扩散的重建都是围绕高斯噪声损坏制定的，并依赖于额外的一致性步骤来合并 SMS 物理，这可能与 SMS 采集中操作员控制的降级不匹配。我们提出了一个算子引导的框架，该框架使用已知的采集算子对退化轨迹进行建模，并通过确定性更新反转该过程。在此框架内，我们引入了一个算子条件双流交互网络（OCDI-Net），它明确地将目标切片内容与切片间干扰分开，并预测算子对齐反演的结构化退化，并且我们将重建实例化为两阶段链式推理过程，该过程执行 SMS 切片分离，然后进行平面内完成。对快速 MRI 大脑数据和前瞻性采集的体内扩散 MRI 数据的实验表明，与传统和基于学习的 SMS 重建相比，保真度得到了提高，切片泄漏也减少了。</li>
</ul>

<h3>Title: Efficient Representations are Controllable Representations</h3>
<ul>
<li><strong>Authors: </strong>Charles Ye, Jasmine Cui</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07828">https://arxiv.org/abs/2602.07828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07828">https://arxiv.org/pdf/2602.07828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07828]] Efficient Representations are Controllable Representations(https://arxiv.org/abs/2602.07828)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>What is the most brute-force way to install interpretable, controllable features into a model's activations? Controlling how LLMs internally represent concepts typically requires sophisticated methods to first identify, then intervene on the model's existing feature geometry. We bypass all of this. We finetune an LLM with a simple auxiliary loss, training 16 of its 3072 residual stream dimensions to be inert interpretability flags that simply indicate what concepts are required for generation. The model reorganizes around them anyway, learning to rely on these flags during actual generation tasks. As a result, these inert flags become genuine internal features: interpretable control switches that allow us to steer generation at inference time. Why does this work? When a feature is reliably supplied at a fixed location, gradient descent gradually eliminates redundant encodings elsewhere, and the model erodes its own alternative representations. A model's efficiency pressure is a lever - exploitable to induce interpretable, controllable representations.</li>
<li><strong>摘要：</strong>将可解释、可控制的特征安装到模型的激活中的最强力的方法是什么？控制法学硕士内部表示概念的方式通常需要复杂的方法来首先识别，然后干预模型的现有特征几何。我们绕过这一切。我们使用简单的辅助损失对 LLM 进行微调，将其 3072 个残差流维度中的 16 个训练为惰性可解释性标志，仅指示生成所需的概念。无论如何，模型都会围绕它们进行重组，学习在实际生成任务期间依赖这些标志。因此，这些惰性标志成为真正的内部功能：可解释的控制开关，使我们能够在推理时引导生成。为什么这有效？当在固定位置可靠地提供特征时，梯度下降会逐渐消除其他地方的冗余编码，并且模型会侵蚀其自身的替代表示。模型的效率压力是一个杠杆——可利用它来产生可解释、可控的表示。</li>
</ul>

<h3>Title: VFace: A Training-Free Approach for Diffusion-Based Video Face Swapping</h3>
<ul>
<li><strong>Authors: </strong>Sanoojan Baliah, Yohan Abeysinghe, Rusiru Thushara, Khan Muhammad, Abhinav Dhall, Karthik Nandakumar, Muhammad Haris Khan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07835">https://arxiv.org/abs/2602.07835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07835">https://arxiv.org/pdf/2602.07835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07835]] VFace: A Training-Free Approach for Diffusion-Based Video Face Swapping(https://arxiv.org/abs/2602.07835)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present a training-free, plug-and-play method, namely VFace, for high-quality face swapping in videos. It can be seamlessly integrated with image-based face swapping approaches built on diffusion models. First, we introduce a Frequency Spectrum Attention Interpolation technique to facilitate generation and intact key identity characteristics. Second, we achieve Target Structure Guidance via plug-and-play attention injection to better align the structural features from the target frame to the generation. Third, we present a Flow-Guided Attention Temporal Smoothening mechanism that enforces spatiotemporal coherence without modifying the underlying diffusion model to reduce temporal inconsistencies typically encountered in frame-wise generation. Our method requires no additional training or video-specific fine-tuning. Extensive experiments show that our method significantly enhances temporal consistency and visual fidelity, offering a practical and modular solution for video-based face swapping. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>我们提出了一种免训练、即插即用的方法，即 VFace，用于视频中的高质量人脸交换。它可以与基于扩散模型的基于图像的面部交换方法无缝集成。首先，我们引入频谱注意力插值技术来促进生成和完整的关键身份特征。其次，我们通过即插即用的注意力注入实现目标结构指导，以更好地将目标框架到生成的结构特征对齐。第三，我们提出了一种流引导注意力时间平滑机制，该机制在不修改底层扩散模型的情况下增强时空一致性，以减少逐帧生成中通常遇到的时间不一致。我们的方法不需要额外的培训或视频特定的微调。大量实验表明，我们的方法显着增强了时间一致性和视觉保真度，为基于视频的人脸交换提供了实用且模块化的解决方案。我们的代码可以在这个 https URL 上找到。</li>
</ul>

<h3>Title: MARTI-MARS$^2$: Scaling Multi-Agent Self-Search via Reinforcement Learning for Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Shijie Wang, Pengfei Li, Yikun Fu, Kaifeng Liu, Fangyuan Li, Yang Liu, Xiaowei Sun, Zonglin Li, Siyao Zhao, Jian Zhao, Kai Tian, Dong Li, Junqi Gao, Yutong Zhang, Yiqun Chen, Yuqiang Li, Zoe Li, Weinan Zhang, Peng Ye, Shuyue Hu, Lei Bai, Bowen Zhou, Kaiyan Zhang, Biqing Qi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07848">https://arxiv.org/abs/2602.07848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07848">https://arxiv.org/pdf/2602.07848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07848]] MARTI-MARS$^2$: Scaling Multi-Agent Self-Search via Reinforcement Learning for Code Generation(https://arxiv.org/abs/2602.07848)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While the complex reasoning capability of Large Language Models (LLMs) has attracted significant attention, single-agent systems often encounter inherent performance ceilings in complex tasks such as code generation. Multi-agent collaboration offers a promising avenue to transcend these boundaries. However, existing frameworks typically rely on prompt-based test-time interactions or multi-role configurations trained with homogeneous parameters, limiting error correction capabilities and strategic diversity. In this paper, we propose a Multi-Agent Reinforced Training and Inference Framework with Self-Search Scaling (MARTI-MARS2), which integrates policy learning with multi-agent tree search by formulating the multi-agent collaborative exploration process as a dynamic and learnable environment. By allowing agents to iteratively explore and refine within the environment, the framework facilitates evolution from parameter-sharing homogeneous multi-role training to heterogeneous multi-agent training, breaking through single-agent capability limits. We also introduce an efficient inference strategy MARTI-MARS2-T+ to fully exploit the scaling potential of multi-agent collaboration at test time. We conduct extensive experiments across varied model scales (8B, 14B, and 32B) on challenging code generation benchmarks. Utilizing two collaborating 32B models, MARTI-MARS2 achieves 77.7%, outperforming strong baselines like GPT-5.1. Furthermore, MARTI-MARS2 reveals a novel scaling law: shifting from single-agent to homogeneous multi-role and ultimately to heterogeneous multi-agent paradigms progressively yields higher RL performance ceilings, robust TTS capabilities, and greater policy diversity, suggesting that policy diversity is critical for scaling intelligence via multi-agent reinforcement learning.</li>
<li><strong>摘要：</strong>虽然大型语言模型（LLM）的复杂推理能力引起了人们的广泛关注，但单代理系统在代码生成等复杂任务中经常遇到固有的性能上限。多智能体协作为超越这些界限提供了一条有前途的途径。然而，现有框架通常依赖于基于提示的测试时交互或多角色配置，并使用同质参数进行训练，从而限制了纠错能力和策略多样性。在本文中，我们提出了一种具有自搜索扩展的多智能体强化训练和推理框架（MARTI-MARS2），该框架通过将多智能体协作探索过程制定为动态且可学习的环境，将策略学习与多智能体树搜索相结合。通过允许智能体在环境中迭代探索和完善，该框架促进了从参数共享的同质多角色训练向异构多智能体训练的演进，突破了单智能体的能力限制。我们还引入了一种高效的推理策略 MARTI-MARS2-T+，以在测试时充分利用多智能体协作的扩展潜力。我们在具有挑战性的代码生成基准上跨不同模型规模（8B、14B 和 32B）进行了广泛的实验。利用两个协作的 32B 模型，MARTI-MARS2 达到了 77.7%，优于 GPT-5.1 等强大的基线。此外，MARTI-MARS2 揭示了一种新颖的扩展法则：从单智能体转变为同质多角色，最终转向异构多智能体范式，逐渐产生更高的 RL 性能上限、强大的 TTS 功能和更大的策略多样性，这表明策略多样性对于通过多智能体强化学习扩展智能至关重要。</li>
</ul>

<h3>Title: Harpoon: Generalised Manifold Guidance for Conditional Tabular Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Aditya Shankar, Yuandou Wang, Rihan Hai, Lydia Y. Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07875">https://arxiv.org/abs/2602.07875</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07875">https://arxiv.org/pdf/2602.07875</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07875]] Harpoon: Generalised Manifold Guidance for Conditional Tabular Diffusion(https://arxiv.org/abs/2602.07875)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generating tabular data under conditions is critical to applications requiring precise control over the generative process. Existing methods rely on training-time strategies that do not generalise to unseen constraints during inference, and struggle to handle conditional tasks beyond tabular imputation. While manifold theory offers a principled way to guide generation, current formulations are tied to specific inference-time objectives and are limited to continuous domains. We extend manifold theory to tabular data and expand its scope to handle diverse inference-time objectives. On this foundation, we introduce HARPOON, a tabular diffusion method that guides unconstrained samples along the manifold geometry to satisfy diverse tabular conditions at inference. We validate our theoretical contributions empirically on tasks such as imputation and enforcing inequality constraints, demonstrating HARPOON'S strong performance across diverse datasets and the practical benefits of manifold-aware guidance for tabular data. Code URL: this https URL</li>
<li><strong>摘要：</strong>在一定条件下生成表格数据对于需要精确控制生成过程的应用程序至关重要。现有方法依赖于训练时策略，这些策略不能推广到推理过程中看不见的约束，并且很难处理表格​​插补之外的条件任务。虽然流形理论提供了指导生成的原则性方法，但当前的公式与特定的推理时间目标相关，并且仅限于连续域。我们将流形理论扩展到表格数据，并扩展其范围以处理不同的推理时间目标。在此基础上，我们引入了 HARPOON，一种表格扩散方法，可沿着流形几何形状引导无约束样本，以满足推理时的各种表格条件。我们在插补和执行不平等约束等任务上实证验证了我们的理论贡献，证明了 HARPOON 在不同数据集上的强大性能以及表格数据的流形感知指导的实际好处。代码URL：这个https URL</li>
</ul>

<h3>Title: A Kinetic-Energy Perspective of Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Ziyun Li, Huancheng Hu, Soon Hoe Lim, Xuyu Li, Fei Gao, Enmao Diao, Zezhen Ding, Michalis Vazirgiannis, Henrik Bostrom</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07928">https://arxiv.org/abs/2602.07928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07928">https://arxiv.org/pdf/2602.07928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07928]] A Kinetic-Energy Perspective of Flow Matching(https://arxiv.org/abs/2602.07928)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Flow-based generative models can be viewed through a physics lens: sampling transports a particle from noise to data by integrating a time-varying velocity field, and each sample corresponds to a trajectory with its own dynamical effort. Motivated by classical mechanics, we introduce Kinetic Path Energy (KPE), an action-like, per-sample diagnostic that measures the accumulated kinetic effort along an Ordinary Differential Equation (ODE) trajectory. Empirically, KPE exhibits two robust correspondences: (i) higher KPE predicts stronger semantic fidelity; (ii) high-KPE trajectories terminate on low-density manifold frontiers. We further provide theoretical guarantees linking trajectory energy to data density. Paradoxically, this correlation is non-monotonic. At sufficiently high energy, generation can degenerate into memorization. Leveraging the closed-form of empirical flow matching, we show that extreme energies drive trajectories toward near-copies of training examples. This yields a Goldilocks principle and motivates Kinetic Trajectory Shaping (KTS), a training-free two-phase inference strategy that boosts early motion and enforces a late-time soft landing, reducing memorization and improving generation quality across benchmark tasks.</li>
<li><strong>摘要：</strong>基于流的生成模型可以通过物理透镜来看待：采样通过积分时变速度场将粒子从噪声传输到数据，并且每个样本对应于具有其自身动态作用的轨迹。受经典力学的启发，我们引入了动路径能量 (KPE)，这是一种类似动作的按样本诊断，可测量沿常微分方程 (ODE) 轨迹累积的动能。根据经验，KPE 表现出两种稳健的对应关系：（i）较高的 KPE 预示着较强的语义保真度； (ii) 高 KPE 轨迹终止于低密度流形边界。我们进一步提供了将轨迹能量与数据密度联系起来的理论保证。矛盾的是，这种相关性是非单调的。在足够高的能量下，生成可以退化为记忆。利用经验流匹配的封闭形式，我们表明极端能量将轨迹驱动到接近训练样本的副本。这产生了金发姑娘原理并激发了动力学轨迹塑造（KTS），这是一种免训练的两阶段推理策略，可促进早期运动并强制后期软着陆，减少记忆并提高基准任务的生成质量。</li>
</ul>

<h3>Title: EasyTune: Efficient Step-Aware Fine-Tuning for Diffusion-Based Motion Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiaofeng Tan, Wanjiang Weng, Haodong Lei, Hongsong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07967">https://arxiv.org/abs/2602.07967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07967">https://arxiv.org/pdf/2602.07967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07967]] EasyTune: Efficient Step-Aware Fine-Tuning for Diffusion-Based Motion Generation(https://arxiv.org/abs/2602.07967)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In recent years, motion generative models have undergone significant advancement, yet pose challenges in aligning with downstream objectives. Recent studies have shown that using differentiable rewards to directly align the preference of diffusion models yields promising results. However, these methods suffer from (1) inefficient and coarse-grained optimization with (2) high memory consumption. In this work, we first theoretically and empirically identify the key reason of these limitations: the recursive dependence between different steps in the denoising trajectory. Inspired by this insight, we propose EasyTune, which fine-tunes diffusion at each denoising step rather than over the entire trajectory. This decouples the recursive dependence, allowing us to perform (1) a dense and fine-grained, and (2) memory-efficient optimization. Furthermore, the scarcity of preference motion pairs restricts the availability of motion reward model training. To this end, we further introduce a Self-refinement Preference Learning (SPL) mechanism that dynamically identifies preference pairs and conducts preference learning. Extensive experiments demonstrate that EasyTune outperforms DRaFT-50 by 8.2% in alignment (MM-Dist) improvement while requiring only 31.16% of its additional memory overhead and achieving a 7.3x training speedup. The project page is available at this link {this https URL}.</li>
<li><strong>摘要：</strong>近年来，运动生成模型取得了显着进步，但在与下游目标保持一致方面提出了挑战。最近的研究表明，使用可微分奖励直接调整扩散模型的偏好会产生有希望的结果。然而，这些方法存在 (1) 低效且粗粒度的优化以及 (2) 高内存消耗的问题。在这项工作中，我们首先从理论上和经验上确定了这些限制的关键原因：去噪轨迹中不同步骤之间的递归依赖性。受这一见解的启发，我们提出了 EasyTune，它在每个去噪步骤而不是整个轨迹上微调扩散。这解耦了递归依赖性，使我们能够执行（1）密集且细粒度的优化，以及（2）内存高效的优化。此外，偏好运动对的稀缺限制了运动奖励模型训练的可用性。为此，我们进一步引入自我细化偏好学习（SPL）机制，动态识别偏好对并进行偏好学习。大量实验表明，EasyTune 在对齐 (MM-Dist) 改进方面比 DRaFT-50 提高了 8.2%，同时仅需要 31.16% 的额外内存开销，并实现了 7.3 倍的训练加速。项目页面可通过此链接 {this https URL} 获取。</li>
</ul>

<h3>Title: FSP-Diff: Full-Spectrum Prior-Enhanced DualDomain Latent Diffusion for Ultra-Low-Dose Spectral CT Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Peng Peng, Xinrui Zhang, Junlin Wang, Lei Li, Shaoyu Wang, Qiegen Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07979">https://arxiv.org/abs/2602.07979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07979">https://arxiv.org/pdf/2602.07979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07979]] FSP-Diff: Full-Spectrum Prior-Enhanced DualDomain Latent Diffusion for Ultra-Low-Dose Spectral CT Reconstruction(https://arxiv.org/abs/2602.07979)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Spectral computed tomography (CT) with photon-counting detectors holds immense potential for material discrimination and tissue characterization. However, under ultra-low-dose conditions, the sharply degraded signal-to-noise ratio (SNR) in energy-specific projections poses a significant challenge, leading to severe artifacts and loss of structural details in reconstructed images. To address this, we propose FSP-Diff, a full-spectrum prior-enhanced dual-domain latent diffusion framework for ultra-low-dose spectral CT reconstruction. Our framework integrates three core strategies: 1) Complementary Feature Construction: We integrate direct image reconstructions with projection-domain denoised results. While the former preserves latent textural nuances amidst heavy noise, the latter provides a stable structural scaffold to balance detail fidelity and noise suppression. 2) Full-Spectrum Prior Integration: By fusing multi-energy projections into a high-SNR full-spectrum image, we establish a unified structural reference that guides the reconstruction across all energy bins. 3) Efficient Latent Diffusion Synthesis: To alleviate the high computational burden of high-dimensional spectral data, multi-path features are embedded into a compact latent space. This allows the diffusion process to facilitate interactive feature fusion in a lower-dimensional manifold, achieving accelerated reconstruction while maintaining fine-grained detail restoration. Extensive experiments on simulated and real-world datasets demonstrate that FSP-Diff significantly outperforms state-of-the-art methods in both image quality and computational efficiency, underscoring its potential for clinically viable ultra-low-dose spectral CT imaging.</li>
<li><strong>摘要：</strong>带有光子计数探测器的光谱计算机断层扫描 (CT) 在材料辨别和组织表征方面具有巨大的潜力。然而，在超低剂量条件下，特定能量投影中信噪比（SNR）急剧下降带来了重大挑战，导致重建图像中出现严重的伪影和结构细节丢失。为了解决这个问题，我们提出了 FSP-Diff，一种用于超低剂量能谱 CT 重建的全谱先验增强双域潜在扩散框架。我们的框架集成了三个核心策略：1）互补特征构建：我们将直接图像重建与投影域去噪结果集成。前者在强噪音中保留了潜在的纹理细微差别，而后者则提供了稳定的结构支架来平衡细节保真度和噪音抑制。 2）全谱先验积分：通过将多能量投影融合到高信噪比全谱图像中，我们建立了一个统一的结构参考来指导所有能量箱的重建。 3）高效的潜在扩散合成：为了减轻高维光谱数据的高计算负担，多路径特征被嵌入到紧凑的潜在空间中。这使得扩散过程能够促进低维流形中的交互式特征融合，实现加速重建，同时保持细粒度的细节恢复。对模拟和真实数据集的大量实验表明，FSP-Diff 在图像质量和计算效率方面均显着优于最先进的方法，强调了其在临床上可行的超低剂量能谱 CT 成像的潜力。</li>
</ul>

<h3>Title: Deepfake Synthesis vs. Detection: An Uneven Contest</h3>
<ul>
<li><strong>Authors: </strong>Md. Tarek Hasan, Sanjay Saha, Shaojing Fan, Swakkhar Shatabda, Terence Sim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07986">https://arxiv.org/abs/2602.07986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07986">https://arxiv.org/pdf/2602.07986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07986]] Deepfake Synthesis vs. Detection: An Uneven Contest(https://arxiv.org/abs/2602.07986)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of deepfake technology has significantly elevated the realism and accessibility of synthetic media. Emerging techniques, such as diffusion-based models and Neural Radiance Fields (NeRF), alongside enhancements in traditional Generative Adversarial Networks (GANs), have contributed to the sophisticated generation of deepfake videos. Concurrently, deepfake detection methods have seen notable progress, driven by innovations in Transformer architectures, contrastive learning, and other machine learning approaches. In this study, we conduct a comprehensive empirical analysis of state-of-the-art deepfake detection techniques, including human evaluation experiments against cutting-edge synthesis methods. Our findings highlight a concerning trend: many state-of-the-art detection models exhibit markedly poor performance when challenged with deepfakes produced by modern synthesis techniques, including poor performance by human participants against the best quality deepfakes. Through extensive experimentation, we provide evidence that underscores the urgent need for continued refinement of detection models to keep pace with the evolving capabilities of deepfake generation technologies. This research emphasizes the critical gap between current detection methodologies and the sophistication of new generation techniques, calling for intensified efforts in this crucial area of study.</li>
<li><strong>摘要：</strong>Deepfake 技术的快速进步显着提高了合成媒体的真实性和可访问性。基于扩散的模型和神经辐射场 (NeRF) 等新兴技术，以及传统生成对抗网络 (GAN) 的增强，为深度伪造视频的复杂生成做出了贡献。与此同时，在 Transformer 架构、对比学习和其他机器学习方法创新的推动下，深度伪造检测方法取得了显着进展。在这项研究中，我们对最先进的深度伪造检测技术进行了全面的实证分析，包括针对尖端合成方法的人体评估实验。我们的研究结果强调了一个令人担忧的趋势：许多最先进的检测模型在面对现代合成技术产生的深度赝品的挑战时表现出明显较差的性能，包括人类参与者在对抗最优质的深度赝品时表现不佳。通过广泛的实验，我们提供的证据强调了持续改进检测模型的迫切需要，以跟上 Deepfake 生成技术不断发展的能力。这项研究强调了当前检测方法与新一代技术的复杂性之间的关键差距，呼吁在这一关键研究领域加大力度。</li>
</ul>

<h3>Title: A Unified Density Operator View of Flow Control and Merging</h3>
<ul>
<li><strong>Authors: </strong>Riccardo De Santi, Malte Franke, Ya-Ping Hsieh, Andreas Krause</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08012">https://arxiv.org/abs/2602.08012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08012">https://arxiv.org/pdf/2602.08012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08012]] A Unified Density Operator View of Flow Control and Merging(https://arxiv.org/abs/2602.08012)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent progress in large-scale flow and diffusion models raised two fundamental algorithmic challenges: (i) control-based reward adaptation of pre-trained flows, and (ii) integration of multiple models, i.e., flow merging. While current approaches address them separately, we introduce a unifying probability-space framework that subsumes both as limit cases, and enables reward-guided flow merging, allowing principled, task-aware combination of multiple pre-trained flows (e.g., merging priors while maximizing drug-discovery utilities). Our formulation renders possible to express a rich family of operators over generative models densities, including intersection (e.g., to enforce safety), union (e.g., to compose diverse models), interpolation (e.g., for discovery), their reward-guided counterparts, as well as complex logical expressions via generative circuits. Next, we introduce Reward-Guided Flow Merging (RFM), a mirror-descent scheme that reduces reward-guided flow merging to a sequence of standard fine-tuning problems. Then, we provide first-of-their-kind theoretical guarantees for reward-guided and pure flow merging via RFM. Ultimately, we showcase the capabilities of the proposed method on illustrative settings providing visually interpretable insights, and apply our method to high-dimensional de-novo molecular design and low-energy conformer generation.</li>
<li><strong>摘要：</strong>大规模流和扩散模型的最新进展提出了两个基本的算法挑战：（i）预训练流的基于控制的奖励适应，以及（ii）多个模型的集成，即流合并。虽然当前的方法分别解决它们，但我们引入了一个统一的概率空间框架，将两者都纳入极限情况，并实现奖励引导的流程合并，允许多个预先训练的流程有原则的、任务感知的组合（例如，合并先验，同时最大化药物发现效用）。我们的公式使得在生成模型密度上表达丰富的运算符系列成为可能，包括交集（例如，为了加强安全性）、并集（例如，为了组成不同的模型）、插值（例如，为了发现）、它们的奖励引导对应项，以及通过生成电路的复杂逻辑表达式。接下来，我们介绍奖励引导流合并（RFM），这是一种镜像下降方案，可将奖励引导流合并减少为一系列标准微调问题。然后，我们通过 RFM 为奖励引导和纯流合并提供首创的理论保证。最终，我们展示了所提出的方法在说明性设置上的能力，提供了视觉上可解释的见解，并将我们的方法应用于高维从头分子设计和低能量构象异构体生成。</li>
</ul>

<h3>Title: Horizon Imagination: Efficient On-Policy Training in Diffusion World Models</h3>
<ul>
<li><strong>Authors: </strong>Lior Cohen, Ofir Nabati, Kaixin Wang, Navdeep Kumar, Shie Mannor</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08032">https://arxiv.org/abs/2602.08032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08032">https://arxiv.org/pdf/2602.08032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08032]] Horizon Imagination: Efficient On-Policy Training in Diffusion World Models(https://arxiv.org/abs/2602.08032)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We study diffusion-based world models for reinforcement learning, which offer high generative fidelity but face critical efficiency challenges in control. Current methods either require heavyweight models at inference or rely on highly sequential imagination, both of which impose prohibitive computational costs. We propose Horizon Imagination (HI), an on-policy imagination process for discrete stochastic policies that denoises multiple future observations in parallel. HI incorporates a stabilization mechanism and a novel sampling schedule that decouples the denoising budget from the effective horizon over which denoising is applied while also supporting sub-frame budgets. Experiments on Atari 100K and Craftium show that our approach maintains control performance with a sub-frame budget of half the denoising steps and achieves superior generation quality under varied schedules. Code is available at this https URL.</li>
<li><strong>摘要：</strong>我们研究基于扩散的强化学习世界模型，该模型提供高生成保真度，但在控制方面面临关键的效率挑战。当前的方法要么需要重量级的推理模型，要么依赖高度连续的想象，这两种方法都会带来高昂的计算成本。我们提出了地平线想象（HI），这是一种针对离散随机政策的政策想象过程，可以并行地对多个未来观察进行去噪。 HI 结合了稳定机制和新颖的采样计划，将去噪预算与应用去噪的有效范围分离，同时还支持子帧预算。 Atari 100K 和 Craftium 上的实验表明，我们的方法以一半的去噪步骤的子帧预算保持了控制性能，并在不同的时间表下实现了卓越的生成质量。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Enhanced Mixture 3D CGAN for Completion and Generation of 3D Objects</h3>
<ul>
<li><strong>Authors: </strong>Yahia Hamdi, Nicolas Andrialovanirina, Kélig Mahé, Emilie Poisson Caillault</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08046">https://arxiv.org/abs/2602.08046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08046">https://arxiv.org/pdf/2602.08046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08046]] Enhanced Mixture 3D CGAN for Completion and Generation of 3D Objects(https://arxiv.org/abs/2602.08046)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The generation and completion of 3D objects represent a transformative challenge in computer vision. Generative Adversarial Networks (GANs) have recently demonstrated strong potential in synthesizing realistic visual data. However, they often struggle to capture complex and diverse data distributions, particularly in scenarios involving incomplete inputs or significant missing regions. These challenges arise mainly from the high computational requirements and the difficulty of modeling heterogeneous and structurally intricate data, which restrict their applicability in real-world settings. Mixture of Experts (MoE) models have emerged as a promising solution to these limitations. By dynamically selecting and activating the most relevant expert sub-networks for a given input, MoEs improve both performance and efficiency. In this paper, we investigate the integration of Deep 3D Convolutional GANs (CGANs) with a MoE framework to generate high-quality 3D models and reconstruct incomplete or damaged objects. The proposed architecture incorporates multiple generators, each specialized to capture distinct modalities within the dataset. Furthermore, an auxiliary loss-free dynamic capacity constraint (DCC) mechanism is introduced to guide the selection of categorical generators, ensuring a balance between specialization, training stability, and computational efficiency, which is critical for 3D voxel processing. We evaluated the model's ability to generate and complete shapes with missing regions of varying sizes and compared its performance with state-of-the-art approaches. Both quantitative and qualitative results confirm the effectiveness of the proposed MoE-DCGAN in handling complex 3D data.</li>
<li><strong>摘要：</strong>3D 对象的生成和完成代表了计算机视觉领域的变革性挑战。生成对抗网络（GAN）最近在合成真实视觉数据方面表现出了强大的潜力。然而，他们经常难以捕获复杂且多样化的数据分布，特别是在涉及不完整输入或严重缺失区域的情况下。这些挑战主要源于高计算要求以及对异构和结构复杂的数据进行建模的困难，这限制了它们在现实世界中的适用性。专家混合 (MoE) 模型已成为解决这些限制的有希望的解决方案。通过针对给定输入动态选择和激活最相关的专家子网络，MoE 可以提高性能和效率。在本文中，我们研究了深度 3D 卷积 GAN (CGAN) 与 MoE 框架的集成，以生成高质量的 3D 模型并重建不完整或损坏的对象。所提出的架构包含多个生成器，每个生成器专门用于捕获数据集中的不同模式。此外，引入了辅助无损动态容量约束（DCC）机制来指导分类生成器的选择，确保专业化、训练稳定性和计算效率之间的平衡，这对于 3D 体素处理至关重要。我们评估了该模型生成和完成具有不同大小的缺失区域的形状的能力，并将其性能与最先进的方法进行了比较。定量和定性结果都证实了所提出的 MoE-DCGAN 在处理复杂 3D 数据方面的有效性。</li>
</ul>

<h3>Title: Epigraph-Guided Flow Matching for Safe and Performant Offline Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Manan Tayal, Mumuksh Tayal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08054">https://arxiv.org/abs/2602.08054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08054">https://arxiv.org/pdf/2602.08054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08054]] Epigraph-Guided Flow Matching for Safe and Performant Offline Reinforcement Learning(https://arxiv.org/abs/2602.08054)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Offline reinforcement learning (RL) provides a compelling paradigm for training autonomous systems without the risks of online exploration, particularly in safety-critical domains. However, jointly achieving strong safety and performance from fixed datasets remains challenging. Existing safe offline RL methods often rely on soft constraints that allow violations, introduce excessive conservatism, or struggle to balance safety, reward optimization, and adherence to the data distribution. To address this, we propose Epigraph-Guided Flow Matching (EpiFlow), a framework that formulates safe offline RL as a state-constrained optimal control problem to co-optimize safety and performance. We learn a feasibility value function derived from an epigraph reformulation of the optimal control problem, thereby avoiding the decoupled objectives or post-hoc filtering common in prior work. Policies are synthesized by reweighting the behavior distribution based on this epigraph value function and fitting a generative policy via flow matching, enabling efficient, distribution-consistent sampling. Across various safety-critical tasks, including Safety-Gymnasium benchmarks, EpiFlow achieves competitive returns with near-zero empirical safety violations, demonstrating the effectiveness of epigraph-guided policy synthesis.</li>
<li><strong>摘要：</strong>离线强化学习（RL）为训练自主系统提供了一个令人信服的范例，而无需承担在线探索的风险，特别是在安全关键领域。然而，从固定数据集共同实现强大的安全性和性能仍然具有挑战性。现有的安全离线强化学习方法通​​常依赖于允许违规、引入过度保守主义或难以平衡安全性、奖励优化和遵守数据分布的软约束。为了解决这个问题，我们提出了 Epigraph-Guided Flow Matching (EpiFlow)，这是一个框架，它将安全离线 RL 制定为状态约束的最优控制问题，以共同优化安全性和性能。我们学习从最优控制问题的铭文重新表述中得出的可行性价值函数，从而避免了先前工作中常见的解耦目标或事后过滤。通过根据铭文价值函数重新加权行为分布并通过流匹配拟合生成策略来合成策略，从而实现高效、分布一致的采样。在各种安全关键任务中，包括 Safety-Gymnasium 基准，EpiFlow 实现了具有竞争力的回报，经验安全违规率接近于零，证明了铭文引导的政策综合的有效性。</li>
</ul>

<h3>Title: Compiler-Assisted Speculative Sampling for Accelerated LLM Inference on Heterogeneous Edge Devices</h3>
<ul>
<li><strong>Authors: </strong>Alejandro Ruiz y Mesa, Guilherme Korol, Moritz Riesteter, João Paulo Cardoso de Lima, Jeronimo Castrillon</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08060">https://arxiv.org/abs/2602.08060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08060">https://arxiv.org/pdf/2602.08060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08060]] Compiler-Assisted Speculative Sampling for Accelerated LLM Inference on Heterogeneous Edge Devices(https://arxiv.org/abs/2602.08060)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>LLM deployment on resource-constrained edge devices faces severe latency constraints, particularly in real-time applications where delayed responses can compromise safety or usability. Among many approaches to mitigate the inefficiencies of sequential token-by-token generation, Speculative Decoding (SD) has emerged as a promising technique. However, SD at the edge is hindered by two major challenges: (1) integrating SD into a compiler-based workflow without sacrificing performance or programmability, and (2) exploiting the heterogeneous compute resources of modern SoCs through carefully designed partitioning strategies. This work addresses these challenges by using an analytical cost model that explores heterogeneous hardware configurations and guides coarse-grained partitioning of LLM subgraphs, particularly with edge-typical short input sequence lengths. The cost model predicts when speculative sampling and heterogeneous execution are jointly beneficial and is validated on an edge device featuring a hexacore Cortex-A CPU and a Mali GPU, revealing up to 1.68$\times$ speedup for translation tasks, closely matching analytic expectations.</li>
<li><strong>摘要：</strong>在资源受限的边缘设备上部署 LLM 面临着严重的延迟限制，特别是在实时应用程序中，延迟响应可能会损害安全性或可用性。在缓解顺序逐个令牌生成的低效率问题的许多方法中，推测解码 (SD) 已成为一项有前途的技术。然而，边缘的 SD 受到两个主要挑战的阻碍：(1) 在不牺牲性能或可编程性的情况下将 SD 集成到基于编译器的工作流程中，以及 (2) 通过精心设计的分区策略来利用现代 SoC 的异构计算资源。这项工作通过使用分析成本模型来解决这些挑战，该模型探索异构硬件配置并指导 LLM 子图的粗粒度分区，特别是对于边缘典型的短输入序列长度。该成本模型可以预测投机采样和异构执行何时共同受益，并在配备六核 Cortex-A CPU 和 Mali GPU 的边缘设备上进行了验证，显示翻译任务的加速高达 1.68$\times$，与分析预期非常接近。</li>
</ul>

<h3>Title: Efficient and Adaptable Detection of Malicious LLM Prompts via Bootstrap Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Shayan Ali Hassan, Tao Ni, Zafar Ayyub Qazi, Marco Canini</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08062">https://arxiv.org/abs/2602.08062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08062">https://arxiv.org/pdf/2602.08062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08062]] Efficient and Adaptable Detection of Malicious LLM Prompts via Bootstrap Aggregation(https://arxiv.org/abs/2602.08062)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, reasoning, and generation. However, these systems remain susceptible to malicious prompts that induce unsafe or policy-violating behavior through harmful requests, jailbreak techniques, and prompt injection attacks. Existing defenses face fundamental limitations: black-box moderation APIs offer limited transparency and adapt poorly to evolving threats, while white-box approaches using large LLM judges impose prohibitive computational costs and require expensive retraining for new attacks. Current systems force designers to choose between performance, efficiency, and adaptability. To address these challenges, we present BAGEL (Bootstrap AGgregated Ensemble Layer), a modular, lightweight, and incrementally updatable framework for malicious prompt detection. BAGEL employs a bootstrap aggregation and mixture of expert inspired ensemble of fine-tuned models, each specialized on a different attack dataset. At inference, BAGEL uses a random forest router to identify the most suitable ensemble member, then applies stochastic selection to sample additional members for prediction aggregation. When new attacks emerge, BAGEL updates incrementally by fine-tuning a small prompt-safety classifier (86M parameters) and adding the resulting model to the ensemble. BAGEL achieves an F1 score of 0.92 by selecting just 5 ensemble members (430M parameters), outperforming OpenAI Moderation API and ShieldGemma which require billions of parameters. Performance remains robust after nine incremental updates, and BAGEL provides interpretability through its router's structural features. Our results show ensembles of small finetuned classifiers can match or exceed billion-parameter guardrails while offering the adaptability and efficiency required for production systems.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在自然语言理解、推理和生成方面表现出了卓越的能力。然而，这些系统仍然容易受到恶意提示的影响，这些提示会通过有害请求、越狱技术和提示注入攻击来诱发不安全或违反策略的行为。现有的防御措施面临着根本性的限制：黑盒审核 API 的透明度有限，并且很难适应不断变化的威胁，而使用大型 LLM 法官的白盒方法则带来了高昂的计算成本，并且需要针对新攻击进行昂贵的再培训。当前的系统迫使设计人员在性能、效率和适应性之间做出选择。为了应对这些挑战，我们提出了 BAGEL（Bootstrap AGgreated Ensemble Layer），这是一个模块化、轻量级且可增量更新的框架，用于恶意提示检测。 BAGEL 采用引导聚合和受专家启发的微调模型集合的混合，每个模型专门针对不同的攻击数据集。在推理时，BAGEL 使用随机森林路由器来识别最合适的集成成员，然后应用随机选择来采样其他成员以进行预测聚合。当新的攻击出现时，BAGEL 通过微调小型提示安全分类器（86M 参数）并将生成的模型添加到集成中来增量更新。 BAGEL 仅选择 5 个集成成员（4.3 亿个参数），就获得了 0.92 的 F1 分数，优于需要数十亿个参数的 OpenAI Moderation API 和 ShieldGemma。经过九次增量更新后，性能仍然保持强劲，并且 BAGEL 通过其路由器的结构功能提供可解释性。我们的结果表明，小型微调分类器的集合可以匹配或超过十亿参数的护栏，同时提供生产系统所需的适应性和效率。</li>
</ul>

<h3>Title: ReRoPE: Repurposing RoPE for Relative Camera Control</h3>
<ul>
<li><strong>Authors: </strong>Chunyang Li, Yuanbo Yang, Jiahao Shao, Hongyu Zhou, Katja Schwarz, Yiyi Liao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08068">https://arxiv.org/abs/2602.08068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08068">https://arxiv.org/pdf/2602.08068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08068]] ReRoPE: Repurposing RoPE for Relative Camera Control(https://arxiv.org/abs/2602.08068)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Video generation with controllable camera viewpoints is essential for applications such as interactive content creation, gaming, and simulation. Existing methods typically adapt pre-trained video models using camera poses relative to a fixed reference, e.g., the first frame. However, these encodings lack shift-invariance, often leading to poor generalization and accumulated drift. While relative camera pose embeddings defined between arbitrary view pairs offer a more robust alternative, integrating them into pre-trained video diffusion models without prohibitive training costs or architectural changes remains challenging. We introduce ReRoPE, a plug-and-play framework that incorporates relative camera information into pre-trained video diffusion models without compromising their generation capability. Our approach is based on the insight that Rotary Positional Embeddings (RoPE) in existing models underutilize their full spectral bandwidth, particularly in the low-frequency components. By seamlessly injecting relative camera pose information into these underutilized bands, ReRoPE achieves precise control while preserving strong pre-trained generative priors. We evaluate our method on both image-to-video (I2V) and video-to-video (V2V) tasks in terms of camera control accuracy and visual fidelity. Our results demonstrate that ReRoPE offers a training-efficient path toward controllable, high-fidelity video generation. See project page for more results: this https URL</li>
<li><strong>摘要：</strong>具有可控摄像机视点的视频生成对于交互式内容创建、游戏和模拟等应用至关重要。现有方法通常使用相对于固定参考（例如第一帧）的相机姿势来调整预先训练的视频模型。然而，这些编码缺乏平移不变性，通常导致泛化能力差和累积漂移。虽然在任意视图对之间定义的相对相机姿势嵌入提供了更强大的替代方案，但将它们集成到预先训练的视频扩散模型中而不需要高昂的训练成本或架构变化仍然具有挑战性。我们引入了 ReRoPE，这是一个即插即用的框架，它将相关摄像机信息合并到预先训练的视频扩散模型中，而不影响其生成能力。我们的方法基于这样的见解：现有模型中的旋转位置嵌入 (RoPE) 未充分利用其完整频谱带宽，特别是在低频分量中。通过将相对相机姿态信息无缝注入这些未充分利用的频段，ReRoPE 实现了精确控制，同时保留了强大的预先训练的生成先验。我们在图像到视频（I2V）和视频到视频（V2V）任务上从相机控制精度和视觉保真度方面评估了我们的方法。我们的结果表明，ReRoPE 提供了一条实现可控、高保真视频生成的高效训练路径。请参阅项目页面以获取更多结果：此 https URL</li>
</ul>

<h3>Title: ViT-5: Vision Transformers for The Mid-2020s</h3>
<ul>
<li><strong>Authors: </strong>Feng Wang, Sucheng Ren, Tiezheng Zhang, Predrag Neskovic, Anand Bhattad, Cihang Xie, Alan Yuille</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08071">https://arxiv.org/abs/2602.08071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08071">https://arxiv.org/pdf/2602.08071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08071]] ViT-5: Vision Transformers for The Mid-2020s(https://arxiv.org/abs/2602.08071)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>This work presents a systematic investigation into modernizing Vision Transformer backbones by leveraging architectural advancements from the past five years. While preserving the canonical Attention-FFN structure, we conduct a component-wise refinement involving normalization, activation functions, positional encoding, gating mechanisms, and learnable tokens. These updates form a new generation of Vision Transformers, which we call ViT-5. Extensive experiments demonstrate that ViT-5 consistently outperforms state-of-the-art plain Vision Transformers across both understanding and generation benchmarks. On ImageNet-1k classification, ViT-5-Base reaches 84.2\% top-1 accuracy under comparable compute, exceeding DeiT-III-Base at 83.8\%. ViT-5 also serves as a stronger backbone for generative modeling: when plugged into an SiT diffusion framework, it achieves 1.84 FID versus 2.06 with a vanilla ViT backbone. Beyond headline metrics, ViT-5 exhibits improved representation learning and favorable spatial reasoning behavior, and transfers reliably across tasks. With a design aligned with contemporary foundation-model practices, ViT-5 offers a simple drop-in upgrade over vanilla ViT for mid-2020s vision backbones.</li>
<li><strong>摘要：</strong>这项工作通过利用过去五年的架构进步，对 Vision Transformer 主干网的现代化进行了系统的调查。在保留规范的 Attention-FFN 结构的同时，我们进行了组件级的细化，涉及标准化、激活函数、位置编码、门控机制和可学习标记。这些更新形成了新一代视觉变形金刚，我们称之为 ViT-5。大量实验表明，ViT-5 在理解和生成基准方面始终优于最先进的普通视觉 Transformer。在 ImageNet-1k 分类上，ViT-5-Base 在可比计算下达到 84.2% top-1 准确率，超过 DeiT-III-Base 的 83.8%。 ViT-5 还可以作为生成建模的更强大的骨干网：当插入 SiT 扩散框架时，它的 FID 达到 1.84，而普通 ViT 骨干网的 FID 为 2.06。除了标题指标之外，ViT-5 还表现出改进的表示学习和有利的空间推理行为，并且能够可靠地跨任务传输。 ViT-5 的设计与当代基础模型实践相一致，为 2020 年代中期的视觉主干提供了对普通 ViT 的简单直接升级。</li>
</ul>

<h3>Title: VidVec: Unlocking Video MLLM Embeddings for Video-Text Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Issar Tzachor, Dvir Samuel, Rami Ben-Ari</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08099">https://arxiv.org/abs/2602.08099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08099">https://arxiv.org/pdf/2602.08099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08099]] VidVec: Unlocking Video MLLM Embeddings for Video-Text Retrieval(https://arxiv.org/abs/2602.08099)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into embedding extractors for vision tasks, typically through fine-tuning to produce universal representations. However, their performance on video remains inferior to Video Foundation Models (VFMs). In this paper, we focus on leveraging MLLMs for video-text embedding and retrieval. We first conduct a systematic layer-wise analysis, showing that intermediate (pre-trained) MLLM layers already encode substantial task-relevant information. Leveraging this insight, we demonstrate that combining intermediate-layer embeddings with a calibrated MLLM head yields strong zero-shot retrieval performance without any training. Building on these findings, we introduce a lightweight text-based alignment strategy which maps dense video captions to short summaries and enables task-related video-text embedding learning without visual supervision. Remarkably, without any fine-tuning beyond text, our method outperforms current methods, often by a substantial margin, achieving state-of-the-art results across common video retrieval benchmarks.</li>
<li><strong>摘要：</strong>最近的研究已将生成式多模态大语言模型（MLLM）改编为视觉任务的嵌入提取器，通常通过微调来产生通用表示。然而，它们在视频上的性能仍然不如视频基础模型（VFM）。在本文中，我们重点关注利用 MLLM 进行视频文本嵌入和检索。我们首先进行系统的分层分析，表明中间（预训练的）MLLM 层已经编码了大量与任务相关的信息。利用这一见解，我们证明了将中间层嵌入与校准的 MLLM 头相结合可以在无需任何训练的情况下产生强大的零样本检索性能。基于这些发现，我们引入了一种轻量级的基于文本的对齐策略，该策略将密集的视频字幕映射到简短的摘要，并在没有视觉监督的情况下实现与任务相关的视频文本嵌入学习。值得注意的是，在没有任何文本之外的微调的情况下，我们的方法通常远远超过当前的方法，在常见的视频检索基准中实现了最先进的结果。</li>
</ul>

<h3>Title: Reliable and Responsible Foundation Models: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Yang, Junlin Han, Rishi Bommasani, Jinqi Luo, Wenjie Qu, Wangchunshu Zhou, Adel Bibi, Xiyao Wang, Jaehong Yoon, Elias Stengel-Eskin, Shengbang Tong, Lingfeng Shen, Rafael Rafailov, Runjia Li, Zhaoyang Wang, Yiyang Zhou, Chenhang Cui, Yu Wang, Wenhao Zheng, Huichi Zhou, Jindong Gu, Zhaorun Chen, Peng Xia, Tony Lee, Thomas Zollo, Vikash Sehwag, Jixuan Leng, Jiuhai Chen, Yuxin Wen, Huan Zhang, Zhun Deng, Linjun Zhang, Pavel Izmailov, Pang Wei Koh, Yulia Tsvetkov, Andrew Wilson, Jiaheng Zhang, James Zou, Cihang Xie, Hao Wang, Philip Torr, Julian McAuley, David Alvarez-Melis, Florian Tramèr, Kaidi Xu, Suman Jana, Chris Callison-Burch, Rene Vidal, Filippos Kokkinos, Mohit Bansal, Beidi Chen, Huaxiu Yao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08145">https://arxiv.org/abs/2602.08145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08145">https://arxiv.org/pdf/2602.08145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08145]] Reliable and Responsible Foundation Models: A Comprehensive Survey(https://arxiv.org/abs/2602.08145)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Foundation models, including Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), Image Generative Models (i.e, Text-to-Image Models and Image-Editing Models), and Video Generative Models, have become essential tools with broad applications across various domains such as law, medicine, education, finance, science, and beyond. As these models see increasing real-world deployment, ensuring their reliability and responsibility has become critical for academia, industry, and government. This survey addresses the reliable and responsible development of foundation models. We explore critical issues, including bias and fairness, security and privacy, uncertainty, explainability, and distribution shift. Our research also covers model limitations, such as hallucinations, as well as methods like alignment and Artificial Intelligence-Generated Content (AIGC) detection. For each area, we review the current state of the field and outline concrete future research directions. Additionally, we discuss the intersections between these areas, highlighting their connections and shared challenges. We hope our survey fosters the development of foundation models that are not only powerful but also ethical, trustworthy, reliable, and socially responsible.</li>
<li><strong>摘要：</strong>基础模型，包括大语言模型（LLM）、多模态大语言模型（MLLM）、图像生成模型（即文本到图像模型和图像编辑模型）和视频生成模型，已成为在法律、医学、教育、金融、科学等各个领域广泛应用的重要工具。随着这些模型在现实世界中的部署不断增加，确保其可靠性和责任感对于学术界、工业界和政府来说变得至关重要。这项调查探讨了基础模型的可靠和负责任的开发。我们探讨关键问题，包括偏见和公平、安全和隐私、不确定性、可解释性和分布转移。我们的研究还涵盖了模型的局限性，例如幻觉，以及对齐和人工智能生成内容（AIGC）检测等方法。对于每个领域，我们回顾了该领域的现状并概述了未来的具体研究方向。此外，我们还讨论了这些领域之间的交叉点，强调它们的联系和共同的挑战。我们希望我们的调查能够促进基金会模型的发展，这些模型不仅强大，而且道德、值得信赖、可靠且具有社会责任感。</li>
</ul>

<h3>Title: Spherical Steering: Geometry-Aware Activation Rotation for Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zejia You, Chunyuan Deng, Hanjie Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08169">https://arxiv.org/abs/2602.08169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08169">https://arxiv.org/pdf/2602.08169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08169]] Spherical Steering: Geometry-Aware Activation Rotation for Language Models(https://arxiv.org/abs/2602.08169)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Inference-time steering has emerged as a promising paradigm for controlling language models (LMs) without the cost of retraining. However, standard approaches typically rely on activation addition, a geometric operation that inevitably alters the magnitude of hidden representations. This raises concerns about representation collapse and degradation of open-ended generation capabilities. In this work, we explore Spherical Steering, a training-free primitive that resolves this trade-off through activation rotation. Rather than shifting activations with a fixed vector, our method rotates them along a geodesic toward a target direction, guiding the activation toward the target concept while preserving the integrity of the signal. To further enhance adaptivity, we incorporate a confidence gate that dynamically modulates steering strength based on input uncertainty. Extensive experiments across multiple-choice benchmarks demonstrate that Spherical Steering significantly outperforms addition-based baselines (notably by +10% on TruthfulQA, COPA, and Storycloze), while simultaneously maintaining the model's general open-ended generation quality. This work highlights the value of geometric consistency, suggesting that norm-preserving rotation is a robust and effective primitive for precise inference-time control.</li>
<li><strong>摘要：</strong>推理时间引导已成为控制语言模型 (LM) 且无需再训练成本的有前途的范例。然而，标准方法通常依赖于激活加法，这是一种不可避免地改变隐藏表示大小的几何运算。这引起了人们对代表性崩溃和开放式生成能力退化的担忧。在这项工作中，我们探索了球形转向，这是一种无需训练的原语，可以通过激活旋转来解决这种权衡。我们的方法不是使用固定向量来移动激活，而是沿着测地线向目标方向旋转它们，引导激活朝向目标概念，同时保持信号的完整性。为了进一步增强适应性，我们采用了一个置信门，可以根据输入的不确定性动态调节转向强度。跨多项选择基准的大量实验表明，球形转向的性能显着优于基于加法的基线（特别是在 TruthfulQA、COPA 和 Storycloze 上提高了 10%），同时保持了模型的总体开放式生成质量。这项工作强调了几何一致性的价值，表明保模旋转是精确推理时间控制的稳健且有效的原语。</li>
</ul>

<h3>Title: Nansde-net: A neural sde framework for generating time series with memory</h3>
<ul>
<li><strong>Authors: </strong>Hiromu Ozai, Kei Nakagawa</a></li>
<li><strong>Subjects: </strong>cs.LG, q-fin.CP, q-fin.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08182">https://arxiv.org/abs/2602.08182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08182">https://arxiv.org/pdf/2602.08182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08182]] Nansde-net: A neural sde framework for generating time series with memory(https://arxiv.org/abs/2602.08182)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Modeling time series with long- or short-memory characteristics is a fundamental challenge in many scientific and engineering domains. While fractional Brownian motion has been widely used as a noise source to capture such memory effects, its incompatibility with Itô calculus limits its applicability in neural stochastic differential equation~(SDE) frameworks. In this paper, we propose a novel class of noise, termed Neural Network-kernel ARMA-type noise~(NA-noise), which is an Itô-process-based alternative capable of capturing both long- and short-memory behaviors. The kernel function defining the noise structure is parameterized via neural networks and decomposed into a product form to preserve the Markov property. Based on this noise process, we develop NANSDE-Net, a generative model that extends Neural SDEs by incorporating NA-noise. We prove the theoretical existence and uniqueness of the solution under mild conditions and derive an efficient backpropagation scheme for training. Empirical results on both synthetic and real-world datasets demonstrate that NANSDE-Net matches or outperforms existing models, including fractional SDE-Net, in reproducing long- and short-memory features of the data, while maintaining computational tractability within the Itô calculus framework.</li>
<li><strong>摘要：</strong>对具有长记忆或短记忆特征的时间序列进行建模是许多科学和工程领域的一项基本挑战。虽然分数布朗运动已被广泛用作捕获此类记忆效应的噪声源，但其与伊藤微积分的不兼容性限制了其在神经随机微分方程（SDE）框架中的适用性。在本文中，我们提出了一类新型噪声，称为神经网络内核 ARMA 型噪声~（NA 噪声），它是一种基于 Itô 过程的替代方案，能够捕获长记忆和短记忆行为。定义噪声结构的核函数通过神经网络进行参数化，并分解为乘积形式以保留马尔可夫特性。基于此噪声过程，我们开发了 NANSDE-Net，这是一种通过合并 NA 噪声来扩展神经 SDE 的生成模型。我们证明了该解在温和条件下的理论存在性和唯一性，并推导了一种有效的反向传播训练方案。合成数据集和真实世界数据集的实证结果表明，NANSDE-Net 在再现数据的长记忆和短记忆特征方面匹配或优于现有模型（包括分数 SDE-Net），同时保持 Itô 微积分框架内的计算易处理性。</li>
</ul>

<h3>Title: PEGAsus: 3D Personalization of Geometry and Appearance</h3>
<ul>
<li><strong>Authors: </strong>Jingyu Hu, Bin Hu, Ka-Hei Hui, Haipeng Li, Zhengzhe Liu, Daniel Cohen-Or, Chi-Wing Fu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08198">https://arxiv.org/abs/2602.08198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08198">https://arxiv.org/pdf/2602.08198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08198]] PEGAsus: 3D Personalization of Geometry and Appearance(https://arxiv.org/abs/2602.08198)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present PEGAsus, a new framework capable of generating Personalized 3D shapes by learning shape concepts at both Geometry and Appearance levels. First, we formulate 3D shape personalization as extracting reusable, category-agnostic geometric and appearance attributes from reference shapes, and composing these attributes with text to generate novel shapes. Second, we design a progressive optimization strategy to learn shape concepts at both the geometry and appearance levels, decoupling the shape concept learning process. Third, we extend our approach to region-wise concept learning, enabling flexible concept extraction, with context-aware and context-free losses. Extensive experimental results show that PEGAsus is able to effectively extract attributes from a wide range of reference shapes and then flexibly compose these concepts with text to synthesize new shapes. This enables fine-grained control over shape generation and supports the creation of diverse, personalized results, even in challenging cross-category scenarios. Both quantitative and qualitative experiments demonstrate that our approach outperforms existing state-of-the-art solutions.</li>
<li><strong>摘要：</strong>我们推出了 PEGAsus，这是一个新框架，能够通过学习几何和外观级别的形状概念来生成个性化 3D 形状。首先，我们将 3D 形状个性化制定为从参考形状中提取可重用、与类别无关的几何和外观属性，并将这些属性与文本组合以生成新颖的形状。其次，我们设计了一种渐进式优化策略来学习几何和外观级别的形状概念，从而解耦形状概念学习过程。第三，我们将我们的方法扩展到区域概念学习，从而实现灵活的概念提取，并具有上下文感知和上下文无关的损失。大量的实验结果表明，PEGAsus 能够有效地从各种参考形状中提取属性，然后灵活地将这些概念与文本组合成新的形状。这使得能够对形状生成进行细粒度控制，并支持创建多样化的个性化结果，即使在具有挑战性的跨类别场景中也是如此。定量和定性实验都表明我们的方法优于现有的最先进的解决方案。</li>
</ul>

<h3>Title: Generative Regression for Left Ventricular Ejection Fraction Estimation from Echocardiography Video</h3>
<ul>
<li><strong>Authors: </strong>Jinrong Lv, Xun Gong, Zhaohuan Li, Weili Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08202">https://arxiv.org/abs/2602.08202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08202">https://arxiv.org/pdf/2602.08202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08202]] Generative Regression for Left Ventricular Ejection Fraction Estimation from Echocardiography Video(https://arxiv.org/abs/2602.08202)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Estimating Left Ventricular Ejection Fraction (LVEF) from echocardiograms constitutes an ill-posed inverse problem. Inherent noise, artifacts, and limited viewing angles introduce ambiguity, where a single video sequence may map not to a unique ground truth, but rather to a distribution of plausible physiological values. Prevailing deep learning approaches typically formulate this task as a standard regression problem that minimizes the Mean Squared Error (MSE). However, this paradigm compels the model to learn the conditional expectation, which may yield misleading predictions when the underlying posterior distribution is multimodal or heavy-tailed -- a common phenomenon in pathological scenarios. In this paper, we investigate the paradigm shift from deterministic regression toward generative regression. We propose the Multimodal Conditional Score-based Diffusion model for Regression (MCSDR), a probabilistic framework designed to model the continuous posterior distribution of LVEF conditioned on echocardiogram videos and patient demographic attribute priors. Extensive experiments conducted on the EchoNet-Dynamic, EchoNet-Pediatric, and CAMUS datasets demonstrate that MCSDR achieves state-of-the-art performance. Notably, qualitative analysis reveals that the generation trajectories of our model exhibit distinct behaviors in cases characterized by high noise or significant physiological variability, thereby offering a novel layer of interpretability for AI-aided diagnosis.</li>
<li><strong>摘要：</strong>根据超声心动图估计左心室射血分数 (LVEF) 构成了一个不适定的逆问题。固有的噪声、伪影和有限的视角会带来歧义，其中单个视频序列可能不会映射到唯一的地面事实，而是映射到合理的生理值的分布。流行的深度学习方法通​​常将此任务表述为最小化均方误差 (MSE) 的标准回归问题。然而，这种范式迫使模型学习条件期望，当潜在的后验分布是多模态或重尾分布时（这是病理场景中的常见现象），这可能会产生误导性的预测。在本文中，我们研究了从确定性回归到生成性回归的范式转变。我们提出了基于多模态条件评分的扩散回归模型 (MCSDR)，这是一种概率框架，旨在对以超声心动图视频和患者人口统计属性先验为条件的 LVEF 的连续后验分布进行建模。在 EchoNet-Dynamic、EchoNet-Pediatric 和 CAMUS 数据集上进行的大量实验表明，MCSDR 实现了最先进的性能。值得注意的是，定性分析表明，我们模型的生成轨迹在以高噪声或显着生理变异为特征的情况下表现出不同的行为，从而为人工智能辅助诊断提供了新的可解释性。</li>
</ul>

<h3>Title: DrugR: Optimizing Molecular Drugs through LLM-based Explicit Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Haoran Liu, Zheni Zeng, Yukun Yan, Yuxuan Chen, Yunduo Xiao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08213">https://arxiv.org/abs/2602.08213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08213">https://arxiv.org/pdf/2602.08213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08213]] DrugR: Optimizing Molecular Drugs through LLM-based Explicit Reasoning(https://arxiv.org/abs/2602.08213)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Molecule generation and optimization is a fundamental task in chemical domain. The rapid development of intelligent tools, especially large language models (LLMs) with powerful knowledge reserves and interactive capabilities, has provided new paradigms for it. Nevertheless, the intrinsic challenge for LLMs lies in the complex implicit relationship between molecular structure and pharmacological properties and the lack of corresponding labeled data. To bridge this gap, we propose DrugR, an LLM-based method that introduces explicit, step-by-step pharmacological reasoning into the optimization process. Our approach integrates domain-specific continual pretraining, supervised fine-tuning via reverse data engineering, and self-balanced multi-granular reinforcement learning. This framework enables DrugR to effectively improve key ADMET properties while preserving the original molecule's core efficacy. Experimental results demonstrate that DrugR achieves comprehensive enhancement across multiple properties without compromising structural similarity or target binding affinity. Importantly, its explicit reasoning process provides clear, interpretable rationales for each optimization step, yielding actionable design insights and advancing toward automated, knowledge-driven scientific discovery. Our code and model checkpoints are open-sourced to foster future research.</li>
<li><strong>摘要：</strong>分子生成和优化是化学领域的一项基本任务。智能工具特别是具有强大知识储备和交互能力的大型语言模型（LLM）的快速发展为其提供了新的范式。然而，法学硕士面临的内在挑战在于分子结构和药理特性之间复杂的隐含关系以及缺乏相应的标记数据。为了弥补这一差距，我们提出了 DrugR，这是一种基于法学硕士的方法，它将明确的、逐步的药理学推理引入优化过程。我们的方法集成了特定领域的持续预训练、通过逆向数据工程进行监督微调以及自平衡多粒度强化学习。该框架使 DrugR 能够有效改善关键的 ADMET 特性，同时保留原始分子的核心功效。实验结果表明，DrugR 在不影响结构相似性或靶点结合亲和力的情况下实现了多种特性的全面增强。重要的是，其明确的推理过程为每个优化步骤提供了清晰、可解释的基本原理，产生可操作的设计见解，并推动自动化、知识驱动的科学发现。我们的代码和模型检查点是开源的，以促进未来的研究。</li>
</ul>

<h3>Title: Constraint-Aware Generative Auto-bidding via Pareto-Prioritized Regret Optimization</h3>
<ul>
<li><strong>Authors: </strong>Binglin Wu, Yingyi Zhang, Xianneng Li, Ruyue Deng, Chuan Yue, Weiru Zhang, Xiaoyi Zeng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08261">https://arxiv.org/abs/2602.08261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08261">https://arxiv.org/pdf/2602.08261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08261]] Constraint-Aware Generative Auto-bidding via Pareto-Prioritized Regret Optimization(https://arxiv.org/abs/2602.08261)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Auto-bidding systems aim to maximize marketing value while satisfying strict efficiency constraints such as Target Cost-Per-Action (CPA). Although Decision Transformers provide powerful sequence modeling capabilities, applying them to this constrained setting encounters two challenges: 1) standard Return-to-Go conditioning causes state aliasing by neglecting the cost dimension, preventing precise resource pacing; and 2) standard regression forces the policy to mimic average historical behaviors, thereby limiting the capacity to optimize performance toward the constraint boundary. To address these challenges, we propose PRO-Bid, a constraint-aware generative auto-bidding framework based on two synergistic mechanisms: 1) Constraint-Decoupled Pareto Representation (CDPR) decomposes global constraints into recursive cost and value contexts to restore resource perception, while reweighting trajectories based on the Pareto frontier to focus on high-efficiency data; and 2) Counterfactual Regret Optimization (CRO) facilitates active improvement by utilizing a global outcome predictor to identify superior counterfactual actions. By treating these high-utility outcomes as weighted regression targets, the model transcends historical averages to approach the optimal constraint boundary. Extensive experiments on two public benchmarks and online A/B tests demonstrate that PRO-Bid achieves superior constraint satisfaction and value acquisition compared to state-of-the-art baselines.</li>
<li><strong>摘要：</strong>自动出价系统旨在最大限度地提高营销价值，同时满足严格的效率约束，例如目标每次操作费用 (CPA)。尽管决策转换器提供了强大的序列建模功能，但将它们应用到这种受限环境中会遇到两个挑战：1）标准的 Return-to-Go 条件会忽略成本维度，从而导致状态混叠，从而妨碍精确的资源调配； 2）标准回归迫使策略模仿平均历史行为，从而限制了向约束边界优化性能的能力。为了应对这些挑战，我们提出了 PRO-Bid，一种基于两种协同机制的约束感知生成自动投标框架：1）约束解耦帕累托表示（CDPR）将全局约束分解为递归成本和价值上下文以恢复资源感知，同时根据帕累托前沿重新加权轨迹以专注于高效数据； 2）反事实后悔优化（CRO）通过利用全局结果预测器来识别更好的反事实行动，从而促进主动改进。通过将这些高效用结果视为加权回归目标，该模型超越历史平均值以接近最佳约束边界。对两个公共基准和在线 A/B 测试的大量实验表明，与最先进的基准相比，PRO-Bid 实现了卓越的约束满足和价值获取。</li>
</ul>

<h3>Title: PISCO: Precise Video Instance Insertion with Sparse Control</h3>
<ul>
<li><strong>Authors: </strong>Xiangbo Gao, Renjie Li, Xinghao Chen, Yuheng Wu, Suofei Feng, Qing Yin, Zhengzhong Tu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08277">https://arxiv.org/abs/2602.08277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08277">https://arxiv.org/pdf/2602.08277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08277]] PISCO: Precise Video Instance Insertion with Sparse Control(https://arxiv.org/abs/2602.08277)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The landscape of AI video generation is undergoing a pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and "cherry-picking" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, it is crucial to perform precise, targeted modifications. A cornerstone of this transition is video instance insertion, which requires inserting a specific instance into existing footage while maintaining scene integrity. Unlike traditional video editing, this task demands several requirements: precise spatial-temporal placement, physically consistent scene interaction, and the faithful preservation of original dynamics - all achieved under minimal user effort. In this paper, we propose PISCO, a video diffusion model for precise video instance insertion with arbitrary sparse keyframe control. PISCO allows users to specify a single keyframe, start-and-end keyframes, or sparse keyframes at arbitrary timestamps, and automatically propagates object appearance, motion, and interaction. To address the severe distribution shift induced by sparse conditioning in pretrained video diffusion models, we introduce Variable-Information Guidance for robust conditioning and Distribution-Preserving Temporal Masking to stabilize temporal generation, together with geometry-aware conditioning for realistic scene adaptation. We further construct PISCO-Bench, a benchmark with verified instance annotations and paired clean background videos, and evaluate performance using both reference-based and reference-free perceptual metrics. Experiments demonstrate that PISCO consistently outperforms strong inpainting and video editing baselines under sparse control, and exhibits clear, monotonic performance improvements as additional control signals are provided. Project page: this http URL.</li>
<li><strong>摘要：</strong>人工智能视频生成的格局正在经历关键转变：超越依赖于详尽的即时工程和“樱桃采摘”的一般生成，转向细粒度、可控生成和高保真后处理。在专业的人工智能辅助电影制作中，进行精准、有针对性的修改至关重要。此过渡的基石是视频实例插入，这需要将特定实例插入到现有素材中，同时保持场景完整性。与传统的视频编辑不同，此任务需要几个要求：精确的时空放置、物理上一致的场景交互以及忠实保留原始动态 - 所有这些都在用户最小的努力下实现。在本文中，我们提出了 PISCO，一种视频扩散模型，用于具有任意稀疏关键帧控制的精确视频实例插入。 PISCO 允许用户在任意时间戳指定单个关键帧、开始和结束关键帧或稀疏关键帧，并自动传播对象外观、运动和交互。为了解决预训练视频扩散模型中稀疏条件引起的严重分布变化，我们引入了用于鲁棒条件的可变信息指导和用于稳定时间生成的分布保持时间掩蔽，以及用于现实场景适应的几何感知条件。我们进一步构建了 PISCO-Bench，这是一个具有经过验证的实例注释和配对的干净背景视频的基准，并使用基于参考和无参考的感知指标来评估性能。实验表明，PISCO 在稀疏控制下始终优于强大的修复和视频编辑基线，并且随着提供额外的控制信号，表现出清晰、单调的性能改进。项目页面：这个http URL。</li>
</ul>

<h3>Title: Language-Guided Transformer Tokenizer for Human Motion Generation</h3>
<ul>
<li><strong>Authors: </strong>Sheng Yan, Yong Wang, Xin Du, Junsong Yuan, Mengyuan Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08337">https://arxiv.org/abs/2602.08337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08337">https://arxiv.org/pdf/2602.08337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08337]] Language-Guided Transformer Tokenizer for Human Motion Generation(https://arxiv.org/abs/2602.08337)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In this paper, we focus on motion discrete tokenization, which converts raw motion into compact discrete tokens--a process proven crucial for efficient motion generation. In this paradigm, increasing the number of tokens is a common approach to improving motion reconstruction quality, but more tokens make it more difficult for generative models to learn. To maintain high reconstruction quality while reducing generation complexity, we propose leveraging language to achieve efficient motion tokenization, which we term Language-Guided Tokenization (LG-Tok). LG-Tok aligns natural language with motion at the tokenization stage, yielding compact, high-level semantic representations. This approach not only strengthens both tokenization and detokenization but also simplifies the learning of generative models. Furthermore, existing tokenizers predominantly adopt convolutional architectures, whose local receptive fields struggle to support global language guidance. To this end, we propose a Transformer-based Tokenizer that leverages attention mechanisms to enable effective alignment between language and motion. Additionally, we design a language-drop scheme, in which language conditions are randomly removed during training, enabling the detokenizer to support language-free guidance during generation. On the HumanML3D and Motion-X generation benchmarks, LG-Tok achieves Top-1 scores of 0.542 and 0.582, outperforming state-of-the-art methods (MARDM: 0.500 and 0.528), and with FID scores of 0.057 and 0.088, respectively, versus 0.114 and 0.147. LG-Tok-mini uses only half the tokens while maintaining competitive performance (Top-1: 0.521/0.588, FID: 0.085/0.071), validating the efficiency of our semantic representations.</li>
<li><strong>摘要：</strong>在本文中，我们重点关注运动离散标记化，它将原始运动转换为紧凑的离散标记——事实证明，这一过程对于高效运动生成至关重要。在这种范式中，增加标记的数量是提高运动重建质量的常见方法，但更多的标记使生成模型的学习变得更加困难。为了保持高重建质量，同时降低生成复杂性，我们建议利用语言来实现高效的运动标记化，我们将其称为语言引导标记化（LG-Tok）。 LG-Tok 在标记化阶段将自然语言与动作结合起来，产生紧凑的高级语义表示。这种方法不仅加强了标记化和去标记化，还简化了生成模型的学习。此外，现有的分词器主要采用卷积架构，其局部感受野很难支持全局语言指导。为此，我们提出了一种基于 Transformer 的 Tokenizer，它利用注意力机制来实现语言和动作之间的有效对齐。此外，我们设计了一种语言删除方案，在训练过程中随机删除语言条件，使去标记器能够在生成过程中支持无语言指导。在 HumanML3D 和 Motion-X 一代基准测试中，LG-Tok 获得了 0.542 和 0.582 的 Top-1 分数，优于最先进的方法（MARDM：0.500 和 0.528），并且 FID 分数分别为 0.057 和 0.088，而 FID 分数分别为 0.114 和 0.147。 LG-Tok-mini 仅使用一半的令牌，同时保持竞争性能（Top-1：0.521/0.588，FID：0.085/0.071），验证了我们语义表示的效率。</li>
</ul>

<h3>Title: Reinforcement Learning with Backtracking Feedback</h3>
<ul>
<li><strong>Authors: </strong>Bilgehan Sel, Vaishakh Keshava, Phillip Wallis, Lukas Rutishauser, Ming Jin, Dingcheng Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08377">https://arxiv.org/abs/2602.08377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08377">https://arxiv.org/pdf/2602.08377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08377]] Reinforcement Learning with Backtracking Feedback(https://arxiv.org/abs/2602.08377)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Addressing the critical need for robust safety in Large Language Models (LLMs), particularly against adversarial attacks and in-distribution errors, we introduce Reinforcement Learning with Backtracking Feedback (RLBF). This framework advances upon prior methods, such as BSAFE, by primarily leveraging a Reinforcement Learning (RL) stage where models learn to dynamically correct their own generation errors. Through RL with critic feedback on the model's live outputs, LLMs are trained to identify and recover from their actual, emergent safety violations by emitting an efficient "backtrack by x tokens" signal, then continuing generation autoregressively. This RL process is crucial for instilling resilience against sophisticated adversarial strategies, including middle filling, Greedy Coordinate Gradient (GCG) attacks, and decoding parameter manipulations. To further support the acquisition of this backtracking capability, we also propose an enhanced Supervised Fine-Tuning (SFT) data generation strategy (BSAFE+). This method improves upon previous data creation techniques by injecting violations into coherent, originally safe text, providing more effective initial training for the backtracking mechanism. Comprehensive empirical evaluations demonstrate that RLBF significantly reduces attack success rates across diverse benchmarks and model scales, achieving superior safety outcomes while critically preserving foundational model utility.</li>
<li><strong>摘要：</strong>为了满足大型语言模型（LLM）中强大安全性的关键需求，特别是针对对抗性攻击和分布错误，我们引入了带有回溯反馈的强化学习（RLBF）。该框架在 BSAFE 等先前方法的基础上进行了改进，主要利用强化学习 (RL) 阶段，模型学习动态纠正自己的生成错误。通过强化学习以及对模型实时输出的评论家反馈，法学硕士经过训练，可以通过发出有效的“按 x 令牌回溯”信号，然后继续自回归生成来识别实际的、紧急的安全违规行为并从中恢复。这个强化学习过程对于增强抵御复杂对抗策略（包括中间填充、贪婪坐标梯度（GCG）攻击和解码参数操作）的弹性至关重要。为了进一步支持这种回溯能力的获取，我们还提出了增强的监督微调（SFT）数据生成策略（BSAFE+）。该方法通过将违规注入连贯的、原本安全的文本中，改进了以前的数据创建技术，为回溯机制提供了更有效的初始训练。全面的实证评估表明，RLBF 显着降低了不同基准和模型规模的攻击成功率，实现了卓越的安全结果，同时严格保留了基础模型的效用。</li>
</ul>

<h3>Title: Geometric Image Editing via Effects-Sensitive In-Context Inpainting with Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Shuo Zhang, Wenzhuo Wu, Huayu Zhang, Jiarong Cheng, Xianghao Zang, Chao Ban, Hao Sun, Zhongjiang He, Tianwei Cao, Kongming Liang, Zhanyu Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08388">https://arxiv.org/abs/2602.08388</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08388">https://arxiv.org/pdf/2602.08388</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08388]] Geometric Image Editing via Effects-Sensitive In-Context Inpainting with Diffusion Transformers(https://arxiv.org/abs/2602.08388)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion models have significantly improved image editing. However, challenges persist in handling geometric transformations, such as translation, rotation, and scaling, particularly in complex scenes. Existing approaches suffer from two main limitations: (1) difficulty in achieving accurate geometric editing of object translation, rotation, and scaling; (2) inadequate modeling of intricate lighting and shadow effects, leading to unrealistic results. To address these issues, we propose GeoEdit, a framework that leverages in-context generation through a diffusion transformer module, which integrates geometric transformations for precise object edits. Moreover, we introduce Effects-Sensitive Attention, which enhances the modeling of intricate lighting and shadow effects for improved realism. To further support training, we construct RS-Objects, a large-scale geometric editing dataset containing over 120,000 high-quality image pairs, enabling the model to learn precise geometric editing while generating realistic lighting and shadows. Extensive experiments on public benchmarks demonstrate that GeoEdit consistently outperforms state-of-the-art methods in terms of visual quality, geometric accuracy, and realism.</li>
<li><strong>摘要：</strong>扩散模型的最新进展显着改进了图像编辑。然而，处理几何变换（例如平移、旋转和缩放）仍然存在挑战，特别是在复杂场景中。现有方法存在两个主要局限性：（1）难以实现对象平移、旋转和缩放的精确几何编辑； (2)复杂的光影效果建模不充分，导致结果不切实际。为了解决这些问题，我们提出了 GeoEdit，这是一个通过扩散转换器模块利用上下文生成的框架，该模块集成了几何变换以实现精确的对象编辑。此外，我们引入了效果敏感注意力，它增强了复杂照明和阴影效果的建模，以提高真实感。为了进一步支持训练，我们构建了 RS-Objects，这是一个包含超过 120,000 个高质量图像对的大规模几何编辑数据集，使模型能够学习精确的几何编辑，同时生成逼真的光照和阴影。对公共基准的大量实验表明，GeoEdit 在视觉质量、几何精度和真实感方面始终优于最先进的方法。</li>
</ul>

<h3>Title: D$^2$-VR: Degradation-Robust and Distilled Video Restoration with Synergistic Optimization Strategy</h3>
<ul>
<li><strong>Authors: </strong>Jianfeng Liang, Shaocheng Shen, Botao Xu, Qiang Hu, Xiaoyun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08395">https://arxiv.org/abs/2602.08395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08395">https://arxiv.org/pdf/2602.08395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08395]] D$^2$-VR: Degradation-Robust and Distilled Video Restoration with Synergistic Optimization Strategy(https://arxiv.org/abs/2602.08395)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>The integration of diffusion priors with temporal alignment has emerged as a transformative paradigm for video restoration, delivering fantastic perceptual quality, yet the practical deployment of such frameworks is severely constrained by prohibitive inference latency and temporal instability when confronted with complex real-world degradations. To address these limitations, we propose \textbf{D$^2$-VR}, a single-image diffusion-based video-restoration framework with low-step inference. To obtain precise temporal guidance under severe degradation, we first design a Degradation-Robust Flow Alignment (DRFA) module that leverages confidence-aware attention to filter unreliable motion cues. We then incorporate an adversarial distillation paradigm to compress the diffusion sampling trajectory into a rapid few-step regime. Finally, a synergistic optimization strategy is devised to harmonize perceptual quality with rigorous temporal consistency. Extensive experiments demonstrate that D$^2$-VR achieves state-of-the-art performance while accelerating the sampling process by \textbf{12$\times$}</li>
<li><strong>摘要：</strong>扩散先验与时间对齐的集成已成为视频恢复的变革范例，提供了出色的感知质量，但在面对复杂的现实世界退化时，此类框架的实际部署受到令人望而却步的推理延迟和时间不稳定性的严重限制。为了解决这些限制，我们提出了 \textbf{D$^2$-VR}，一种具有低步推理的基于单图像扩散的视频恢复框架。为了在严重退化的情况下获得精确的时间指导，我们首先设计了一个退化鲁棒流对齐（DRFA）模块，该模块利用置信感知注意力来过滤不可靠的运动线索。然后，我们采用对抗性蒸馏范例，将扩散采样轨迹压缩为快速的几步状态。最后，设计了一种协同优化策略来协调感知质量与严格的时间一致性。大量实验表明，D$^2$-VR 实现了最先进的性能，同时将采样过程加速了 \textbf{12$\times$}</li>
</ul>

<h3>Title: TriC-Motion: Tri-Domain Causal Modeling Grounded Text-to-Motion Generation</h3>
<ul>
<li><strong>Authors: </strong>Yiyang Cao, Yunze Deng, Ziyu Lin, Bin Feng, Xinggang Wang, Wenyu Liu, Dandan Zheng, Jingdong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08462">https://arxiv.org/abs/2602.08462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08462">https://arxiv.org/pdf/2602.08462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08462]] TriC-Motion: Tri-Domain Causal Modeling Grounded Text-to-Motion Generation(https://arxiv.org/abs/2602.08462)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-motion generation, a rapidly evolving field in computer vision, aims to produce realistic and text-aligned motion sequences. Current methods primarily focus on spatial-temporal modeling or independent frequency domain analysis, lacking a unified framework for joint optimization across spatial, temporal, and frequency domains. This limitation hinders the model's ability to leverage information from all domains simultaneously, leading to suboptimal generation quality. Additionally, in motion generation frameworks, motion-irrelevant cues caused by noise are often entangled with features that contribute positively to generation, thereby leading to motion distortion. To address these issues, we propose Tri-Domain Causal Text-to-Motion Generation (TriC-Motion), a novel diffusion-based framework integrating spatial-temporal-frequency-domain modeling with causal intervention. TriC-Motion includes three core modeling modules for domain-specific modeling, namely Temporal Motion Encoding, Spatial Topology Modeling, and Hybrid Frequency Analysis. After comprehensive modeling, a Score-guided Tri-domain Fusion module integrates valuable information from the triple domains, simultaneously ensuring temporal consistency, spatial topology, motion trends, and dynamics. Moreover, the Causality-based Counterfactual Motion Disentangler is meticulously designed to expose motion-irrelevant cues to eliminate noise, disentangling the real modeling contributions of each domain for superior generation. Extensive experimental results validate that TriC-Motion achieves superior performance compared to state-of-the-art methods, attaining an outstanding R@1 of 0.612 on the HumanML3D dataset. These results demonstrate its capability to generate high-fidelity, coherent, diverse, and text-aligned motion sequences. Code is available at: this https URL.</li>
<li><strong>摘要：</strong>文本到运动生成是计算机视觉中一个快速发展的领域，旨在生成逼真且文本对齐的运动序列。目前的方法主要集中于时空建模或独立频域分析，缺乏跨时空频域联合优化的统一框架。这种限制阻碍了模型同时利用来自所有领域的信息的能力，导致生成质量不佳。此外，在运动生成框架中，由噪声引起的与运动无关的线索通常与对生成有积极贡献的特征纠缠在一起，从而导致运动失真。为了解决这些问题，我们提出了三域因果文本到运动生成（TriC-Motion），这是一种基于扩散的新型框架，将时空频域建模与因果干预相结合。 TriC-Motion 包括用于特定领域建模的三个核心建模模块，即时间运动编码、空间拓扑建模和混合频率分析。经过全面建模后，分数引导的三域融合模块集成了来自三域的有价值的信息，同时确保时间一致性、空间拓扑、运动趋势和动态。此外，基于因果关系的反事实运动解缠器经过精心设计，可暴露与运动无关的线索以消除噪声，解开每个域的真实建模贡献，以实现卓越的生成。大量实验结果证明，与最先进的方法相比，TriC-Motion 具有卓越的性能，在 HumanML3D 数据集上获得了 0.612 的出色 R@1。这些结果证明了其生成高保真、连贯、多样化和文本对齐的运动序列的能力。代码可在以下位置获取：此 https URL。</li>
</ul>

<h3>Title: Causal Schrödinger Bridges: Constrained Optimal Transport on Structural Manifolds</h3>
<ul>
<li><strong>Authors: </strong>Rui Wu, Li YongJun</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08535">https://arxiv.org/abs/2602.08535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08535">https://arxiv.org/pdf/2602.08535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08535]] Causal Schrödinger Bridges: Constrained Optimal Transport on Structural Manifolds(https://arxiv.org/abs/2602.08535)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative modeling typically seeks the path of least action via deterministic flows (ODE). While effective for in-distribution tasks, we argue that these deterministic paths become brittle under causal interventions, which often require transporting probability mass across low-density regions ("off-manifold") where the vector field is ill-defined. This leads to numerical instability and spurious correlations. In this work, we introduce the Causal Schrödinger Bridge (CSB), a framework that reformulates counterfactual inference as Entropic Optimal Transport. Unlike deterministic approaches that require strict invertibility, CSB leverages diffusion processes (SDEs) to robustly "tunnel" through support mismatches while strictly enforcing structural admissibility constraints. We prove the Structural Decomposition Theorem, showing that the global high-dimensional bridge factorizes into local, robust transitions. Empirical validation on high-dimensional interventions (Morpho-MNIST) demonstrates that CSB significantly outperforms deterministic baselines in structural consistency, particularly in regimes of strong, out-of-distribution treatments.</li>
<li><strong>摘要：</strong>生成建模通常通过确定性流 (ODE) 寻找最少作用的路径。虽然对于分布任务有效，但我们认为这些确定性路径在因果干预下变得脆弱，这通常需要在矢量场定义不明确的低密度区域（“流形外”）传输概率质量。这导致数值不稳定和虚假相关性。在这项工作中，我们介绍了因果薛定谔桥（CSB），这是一个将反事实推理重新表述为熵最优传输的框架。与需要严格可逆性的确定性方法不同，CSB 利用扩散过程 (SDE) 通过支持不匹配稳健地“隧道”，同时严格执行结构可接受性约束。我们证明了结构分解定理，表明全局高维桥分解为局部的、鲁棒的转换。对高维干预措施 (Morpho-MNIST) 的实证验证表明，CSB 在结构一致性方面显着优于确定性基线，特别是在强分布外治疗方案中。</li>
</ul>

<h3>Title: Modeling Score Approximation Errors in Diffusion Models via Forward SPDEs</h3>
<ul>
<li><strong>Authors: </strong>Junsu Seo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08579">https://arxiv.org/abs/2602.08579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08579">https://arxiv.org/pdf/2602.08579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08579]] Modeling Score Approximation Errors in Diffusion Models via Forward SPDEs(https://arxiv.org/abs/2602.08579)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This study investigates the dynamics of Score-based Generative Models (SGMs) by treating the score estimation error as a stochastic source driving the Fokker-Planck equation. Departing from particle-centric SDE analyses, we employ an SPDE framework to model the evolution of the probability density field under stochastic drift perturbations. Under a simplified setting, we utilize this framework to interpret the robustness of generative models through the lens of geometric stability and displacement convexity. Furthermore, we introduce a candidate evaluation metric derived from the quadratic variation of the SPDE solution projected onto a radial test function. Preliminary observations suggest that this metric remains effective using only the initial 10% of the sampling trajectory, indicating a potential for computational efficiency.</li>
<li><strong>摘要：</strong>本研究通过将分数估计误差视为驱动 Fokker-Planck 方程的随机源，研究了基于分数的生成模型 (SGM) 的动态。与以粒子为中心的 SDE 分析不同，我们采用 SPDE 框架来模拟随机漂移扰动下概率密度场的演化。在简化的设置下，我们利用该框架通过几何稳定性和位移凸性的角度来解释生成模型的鲁棒性。此外，我们引入了一种候选评估指标，该指标源自投影到径向测试函数上的 SPDE 解的二次变分。初步观察表明，仅使用采样轨迹的初始 10%，该指标仍然有效，表明计算效率的潜力。</li>
</ul>

<h3>Title: Overview and Comparison of AVS Point Cloud Compression Standard</h3>
<ul>
<li><strong>Authors: </strong>Wei Gao, Wenxu Gao, Xingming Mu, Changhao Peng, Ge Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08613">https://arxiv.org/abs/2602.08613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08613">https://arxiv.org/pdf/2602.08613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08613]] Overview and Comparison of AVS Point Cloud Compression Standard(https://arxiv.org/abs/2602.08613)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Point cloud is a prevalent 3D data representation format with significant application values in immersive media, autonomous driving, digital heritage protection, etc. However, the large data size of point clouds poses challenges to transmission and storage, which influences the wide deployments. Therefore, point cloud compression plays a crucial role in practical applications for both human and machine perception optimization. To this end, the Moving Picture Experts Group (MPEG) has established two standards for point cloud compression, including Geometry-based Point Cloud Compression (G-PCC) and Video-based Point Cloud Compression (V-PCC). In the meantime, the Audio Video coding Standard (AVS) Workgroup of China also have launched and completed the development for its first generation point cloud compression standard, namely AVS PCC. This new standardization effort has adopted many new coding tools and techniques, which are different from the other counterpart standards. This paper reviews the AVS PCC standard from two perspectives, i.e., the related technologies and performance comparisons.</li>
<li><strong>摘要：</strong>点云是一种流行的3D数据表示格式，在沉浸式媒体、自动驾驶、数字遗产保护等方面具有重要的应用价值。然而，点云的大数据量给传输和存储带来了挑战，影响了广泛的部署。因此，点云压缩在人类和机器感知优化的实际应用中起着至关重要的作用。为此，运动图像专家组（MPEG）建立了两个点云压缩标准，包括基于几何的点云压缩（G-PCC）和基于视频的点云压缩（V-PCC）。与此同时，中国音视频编码标准（AVS）工作组也启动并完成了第一代点云压缩标准AVS PCC的开发。这项新的标准化工作采用了许多新的编码工具和技术，与其他对应标准不同。本文从相关技术和性能比较两个角度对AVS PCC标准进行了回顾。</li>
</ul>

<h3>Title: Inspiration Seeds: Learning Non-Literal Visual Combinations for Generative Exploration</h3>
<ul>
<li><strong>Authors: </strong>Kfir Goldberg, Elad Richardson, Yael Vinker</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08615">https://arxiv.org/abs/2602.08615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08615">https://arxiv.org/pdf/2602.08615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08615]] Inspiration Seeds: Learning Non-Literal Visual Combinations for Generative Exploration(https://arxiv.org/abs/2602.08615)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>While generative models have become powerful tools for image synthesis, they are typically optimized for executing carefully crafted textual prompts, offering limited support for the open-ended visual exploration that often precedes idea formation. In contrast, designers frequently draw inspiration from loosely connected visual references, seeking emergent connections that spark new ideas. We propose Inspiration Seeds, a generative framework that shifts image generation from final execution to exploratory ideation. Given two input images, our model produces diverse, visually coherent compositions that reveal latent relationships between inputs, without relying on user-specified text prompts. Our approach is feed-forward, trained on synthetic triplets of decomposed visual aspects derived entirely through visual means: we use CLIP Sparse Autoencoders to extract editing directions in CLIP latent space and isolate concept pairs. By removing the reliance on language and enabling fast, intuitive recombination, our method supports visual ideation at the early and ambiguous stages of creative work.</li>
<li><strong>摘要：</strong>虽然生成模型已成为图像合成的强大工具，但它们通常针对执行精心设计的文本提示进行了优化，为通常在想法形成之前的开放式视觉探索提供了有限的支持。相比之下，设计师经常从松散连接的视觉参考中汲取灵感，寻求激发新想法的新兴联系。我们提出了 Inspiration Seeds，这是一种生成框架，可将图像生成从最终执行转变为探索性构思。给定两个输入图像，我们的模型会产生多样化的、视觉上连贯的组合，揭示输入之间的潜在关系，而不依赖于用户指定的文本提示。我们的方法是前馈的，对完全通过视觉手段导出的分解视觉方面的合成三元组进行训练：我们使用 CLIP 稀疏自动编码器来提取 CLIP 潜在空间中的编辑方向并隔离概念对。通过消除对语言的依赖并实现快速、直观的重组，我们的方法支持创意工作的早期和模糊阶段的视觉构思。</li>
</ul>

<h3>Title: Improving Reconstruction of Representation Autoencoder</h3>
<ul>
<li><strong>Authors: </strong>Siyu Liu, Chujie Qin, Hubery Yin, Qixin Yan, Zheng-Peng Duan, Chen Li, Jing Lyu, Chun-Le Guo, Chongyi Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08620">https://arxiv.org/abs/2602.08620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08620">https://arxiv.org/pdf/2602.08620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08620]] Improving Reconstruction of Representation Autoencoder(https://arxiv.org/abs/2602.08620)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent work leverages Vision Foundation Models as image encoders to boost the generative performance of latent diffusion models (LDMs), as their semantic feature distributions are easy to learn. However, such semantic features often lack low-level information (\eg, color and texture), leading to degraded reconstruction fidelity, which has emerged as a primary bottleneck in further scaling LDMs. To address this limitation, we propose LV-RAE, a representation autoencoder that augments semantic features with missing low-level information, enabling high-fidelity reconstruction while remaining highly aligned with the semantic distribution. We further observe that the resulting high-dimensional, information-rich latent make decoders sensitive to latent perturbations, causing severe artifacts when decoding generated latent and consequently degrading generation quality. Our analysis suggests that this sensitivity primarily stems from excessive decoder responses along directions off the data manifold. Building on these insights, we propose fine-tuning the decoder to increase its robustness and smoothing the generated latent via controlled noise injection, thereby enhancing generation quality. Experiments demonstrate that LV-RAE significantly improves reconstruction fidelity while preserving the semantic abstraction and achieving strong generative quality. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>最近的工作利用视觉基础模型作为图像编码器来提高潜在扩散模型（LDM）的生成性能，因为它们的语义特征分布很容易学习。然而，此类语义特征通常缺乏低级信息（例如颜色和纹理），导致重建保真度下降，这已成为进一步扩展 LDM 的主要瓶颈。为了解决这个限制，我们提出了 LV-RAE，一种表示自动编码器，它通过丢失的低级信息来增强语义特征，从而实现高保真重建，同时保持与语义分布高度一致。我们进一步观察到，由此产生的高维、信息丰富的潜在数据使解码器对潜在扰动敏感，在解码生成潜在数据时导致严重的伪影，从而降低生成质量。我们的分析表明，这种敏感性主要源于沿数据流形方向的过度解码器响应。基于这些见解，我们建议对解码器进行微调，以提高其鲁棒性，并通过受控噪声注入平滑生成的潜伏，从而提高生成质量。实验表明，LV-RAE 显着提高了重建保真度，同时保留了语义抽象并实现了强大的生成质量。我们的代码可以在这个 https URL 上找到。</li>
</ul>

<h3>Title: Projected Gradient Ascent for Efficient Reward-Guided Updates with One-Step Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Jisung Hwang, Minhyuk Sung</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08646">https://arxiv.org/abs/2602.08646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08646">https://arxiv.org/pdf/2602.08646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08646]] Projected Gradient Ascent for Efficient Reward-Guided Updates with One-Step Generative Models(https://arxiv.org/abs/2602.08646)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We propose a constrained latent optimization method for reward-guided generation that preserves white Gaussian noise characteristics with negligible overhead. Test-time latent optimization can unlock substantially better reward-guided generations from pretrained generative models, but it is prone to reward hacking that degrades quality and also too slow for practical use. In this work, we make test-time optimization both efficient and reliable by replacing soft regularization with hard white Gaussian noise constraints enforced via projected gradient ascent. Our method applies a closed-form projection after each update to keep the latent vector explicitly noise-like throughout optimization, preventing the drift that leads to unrealistic artifacts. This enforcement adds minimal cost: the projection matches the $O(N \log N)$ complexity of standard algorithms such as sorting or FFT and does not practically increase wall-clock time. In experiments, our approach reaches a comparable Aesthetic Score using only 30% of the wall-clock time required by the SOTA regularization-based method, while preventing reward hacking.</li>
<li><strong>摘要：</strong>我们提出了一种用于奖励引导生成的约束潜在优化方法，该方法以可忽略的开销保留白高斯噪声特征。测试时的潜在优化可以从预训练的生成模型中解锁更好的奖励引导生成，但它很容易出现奖励黑客行为，从而降低质量，而且对于实际使用来说速度太慢。在这项工作中，我们通过用投影梯度上升强制执行的硬白高斯噪声约束替换软正则化，使测试时优化既高效又可靠。我们的方法在每次更新后应用封闭式投影，以在整个优化过程中保持潜在向量显式地类似于噪声，从而防止导致不切实际的伪影的漂移。这种实施增加了最小的成本：投影与排序或 FFT 等标准算法的 $O(N \log N)$ 复杂度相匹配，并且实际上不会增加​​挂钟时间。在实验中，我们的方法仅使用基于 SOTA 正则化的方法所需的 30% 的挂钟时间即可达到相当的美学分数，同时防止奖励黑客攻击。</li>
</ul>

<h3>Title: Equalized Generative Treatment: Matching f-divergences for Fairness in Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Alexandre Verine, Rafael Pinot, Florian Le Bronnec</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08660">https://arxiv.org/abs/2602.08660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08660">https://arxiv.org/pdf/2602.08660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08660]] Equalized Generative Treatment: Matching f-divergences for Fairness in Generative Models(https://arxiv.org/abs/2602.08660)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Fairness is a crucial concern for generative models, which not only reflect but can also amplify societal and cultural biases. Existing fairness notions for generative models are largely adapted from classification and focus on balancing the probability of generating samples from each sensitive group. We show that such criteria are brittle, as they can be met even when different sensitive groups are modeled with widely varying quality. To address this limitation, we introduce a new fairness definition for generative models, termed as equalized generative treatment (EGT), which requires comparable generation quality across all sensitive groups, with quality measured via a reference f-divergence. We further analyze the trade-offs induced by EGT, demonstrating that enforcing fairness constraints necessarily couples the overall model quality to that of the most challenging group to approximate. This indicates that a simple yet efficient min-max fine-tuning method should be able to balance f-divergences across sensitive groups to satisfy EGT. We validate this theoretical insight through a set of experiments on both image and text generation tasks. We demonstrate that min-max methods consistently achieve fairer outcomes compared to other approaches from the literature, while maintaining competitive overall performance for both tasks.</li>
<li><strong>摘要：</strong>公平性是生成模型的一个关键问题，它不仅反映而且还可能放大社会和文化偏见。生成模型的现有公平概念很大程度上改编自分类，并侧重于平衡从每个敏感组生成样本的概率。我们表明，这样的标准是脆弱的，因为即使不同的敏感群体的建模质量差异很大，它们也可以得到满足。为了解决这一限制，我们为生成模型引入了一种新的公平性定义，称为均衡生成处理（EGT），它要求所有敏感群体的生成质量具有可比性，并通过参考 f 散度来衡量质量。我们进一步分析了 EGT 引起的权衡，证明强制执行公平性约束必然会将整体模型质量与最具挑战性的近似组的质量结合起来。这表明简单而有效的最小-最大微调方法应该能够平衡敏感组之间的 f 散度以满足 EGT。我们通过一系列图像和文本生成任务的实验验证了这一理论见解。我们证明，与文献中的其他方法相比，最小-最大方法始终能够实现更公平的结果，同时保持两项任务的有竞争力的整体性能。</li>
</ul>

<h3>Title: LLaDA2.1: Speeding Up Text Diffusion via Token Editing</h3>
<ul>
<li><strong>Authors: </strong>Tiwei Bie, Maosong Cao, Xiang Cao, Bingsen Chen, Fuyuan Chen, Kun Chen, Lun Du, Daozhuo Feng, Haibo Feng, Mingliang Gong, Zhuocheng Gong, Yanmei Gu, Jian Guan, Kaiyuan Guan, Hongliang He, Zenan Huang, Juyong Jiang, Zhonghui Jiang, Zhenzhong Lan, Chengxi Li, Jianguo Li, Zehuan Li, Huabin Liu, Lin Liu, Guoshan Lu, Yuan Lu, Yuxin Ma, Xingyu Mou, Zhenxuan Pan, Kaida Qiu, Yuji Ren, Jianfeng Tan, Yiding Tian, Zian Wang, Lanning Wei, Tao Wu, Yipeng Xing, Wentao Ye, Liangyu Zha, Tianze Zhang, Xiaolu Zhang, Junbo Zhao, Da Zheng, Hao Zhong, Wanli Zhong, Jun Zhou, Junlin Zhou, Liwang Zhu, Muzhi Zhu, Yihong Zhuang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08676">https://arxiv.org/abs/2602.08676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08676">https://arxiv.org/pdf/2602.08676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08676]] LLaDA2.1: Speeding Up Text Diffusion via Token Editing(https://arxiv.org/abs/2602.08676)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.</li>
<li><strong>摘要：</strong>虽然 LLaDA2.0 展示了 100B 级块扩散模型的扩展潜力及其固有的并行性，但解码速度和生成质量之间的微妙平衡仍然是一个难以捉摸的前沿。今天，我们推出了 LLaDA2.1，这是一种旨在超越这种权衡的范式转变。通过将令牌到令牌（T2T）编辑无缝地编织到传统的掩模到令牌（M2T）方案中，我们引入了一种联合的、可配置的阈值解码方案。这种结构创新催生了两种截然不同的角色：快速模式（S模式），大胆降低M2T门槛，绕过传统约束，依靠T2T精细化产出；质量模式（Q 模式），它倾向于保守的阈值，以确保卓越的基准性能和可管理的效率下降。在广阔的上下文窗口的支持下，进一步推动这一发展，我们实现了第一个专门为 dLLM 定制的大规模强化学习 (RL) 框架，以稳定梯度估计的专门技术为基础。这种对齐不仅提高了推理精度，还提高了指令遵循的保真度，弥合了扩散动力学和复杂的人类意图之间的鸿沟。我们通过发布 LLaDA2.1-Mini (16B) 和 LLaDA2.1-Flash (100B) 来完成这项工作。在 33 个严格的基准测试中，LLaDA2.1 提供了强大的任务性能和闪电般的解码速度。尽管其容量为 100B，但在编码任务上，它在 HumanEval+ 上达到了惊人的 892 TPS，在 BigCodeBench 上达到了 801 TPS，在 LiveCodeBench 上达到了 663 TPS。</li>
</ul>

<h3>Title: Dashed Line Defense: Plug-And-Play Defense Against Adaptive Score-Based Query Attacks</h3>
<ul>
<li><strong>Authors: </strong>Yanzhang Fu, Zizheng Guo, Jizhou Luo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08679">https://arxiv.org/abs/2602.08679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08679">https://arxiv.org/pdf/2602.08679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08679]] Dashed Line Defense: Plug-And-Play Defense Against Adaptive Score-Based Query Attacks(https://arxiv.org/abs/2602.08679)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Score-based query attacks pose a serious threat to deep learning models by crafting adversarial examples (AEs) using only black-box access to model output scores, iteratively optimizing inputs based on observed loss values. While recent runtime defenses attempt to disrupt this process via output perturbation, most either require access to model parameters or fail when attackers adapt their tactics. In this paper, we first reveal that even the state-of-the-art plug-and-play defense can be bypassed by adaptive attacks, exposing a critical limitation of existing runtime defenses. We then propose Dashed Line Defense (DLD), a plug-and-play post-processing method specifically designed to withstand adaptive query strategies. By introducing ambiguity in how the observed loss reflects the true adversarial strength of candidate examples, DLD prevents attackers from reliably analyzing and adapting their queries, effectively disrupting the AE generation process. We provide theoretical guarantees of DLD's defense capability and validate its effectiveness through experiments on ImageNet, demonstrating that DLD consistently outperforms prior defenses--even under worst-case adaptive attacks--while preserving the model's predicted labels.</li>
<li><strong>摘要：</strong>基于分数的查询攻击仅使用对模型输出分数的黑盒访问来制作对抗性示例 (AE)，并根据观察到的损失值迭代优化输入，从而对深度学习模型构成严重威胁。虽然最近的运行时防御试图通过输出扰动来破坏这个过程，但大多数要么需要访问模型参数，要么在攻击者调整策略时失败。在本文中，我们首先揭示即使是最先进的即插即用防御也可以通过自适应攻击绕过，从而暴露出现有运行时防御的关键限制。然后，我们提出了虚线防御（DLD），这是一种即插即用的后处理方法，专门设计用于承受自适应查询策略。通过在观察到的损失如何反映候选示例的真实对抗强度方面引入模糊性，DLD 可以防止攻击者可靠地分析和调整其查询，从而有效地破坏 AE 生成过程。我们为 DLD 的防御能力提供理论保证，并通过 ImageNet 上的实验验证其有效性，证明 DLD 始终优于先前的防御（即使在最坏情况的自适应攻击下），同时保留模型的预测标签。</li>
</ul>

<h3>Title: ALIVE: Animate Your World with Lifelike Audio-Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Ying Guo, Qijun Gan, Yifu Zhang, Jinlai Liu, Yifei Hu, Pan Xie, Dongjun Qian, Yu Zhang, Ruiqi Li, Yuqi Zhang, Ruibiao Lu, Xiaofeng Mei, Bo Han, Xiang Yin, Bingyue Peng, Zehuan Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08682">https://arxiv.org/abs/2602.08682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08682">https://arxiv.org/pdf/2602.08682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08682]] ALIVE: Animate Your World with Lifelike Audio-Video Generation(https://arxiv.org/abs/2602.08682)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Video generation is rapidly evolving towards unified audio-video generation. In this paper, we present ALIVE, a generation model that adapts a pretrained Text-to-Video (T2V) model to Sora-style audio-video generation and animation. In particular, the model unlocks the Text-to-Video&Audio (T2VA) and Reference-to-Video&Audio (animation) capabilities compared to the T2V foundation models. To support the audio-visual synchronization and reference animation, we augment the popular MMDiT architecture with a joint audio-video branch which includes TA-CrossAttn for temporally-aligned cross-modal fusion and UniTemp-RoPE for precise audio-visual alignment. Meanwhile, a comprehensive data pipeline consisting of audio-video captioning, quality control, etc., is carefully designed to collect high-quality finetuning data. Additionally, we introduce a new benchmark to perform a comprehensive model test and comparison. After continue pretraining and finetuning on million-level high-quality data, ALIVE demonstrates outstanding performance, consistently outperforming open-source models and matching or surpassing state-of-the-art commercial solutions. With detailed recipes and benchmarks, we hope ALIVE helps the community develop audio-video generation models more efficiently. Official page: this https URL.</li>
<li><strong>摘要：</strong>视频生成正在快速发展为统一的音视频生成。在本文中，我们提出了 ALIVE，一种生成模型，它将预训练的文本到视频 (T2V) 模型应用于 Sora 风格的音频视频生成和动画。特别是，与 T2V 基础模型相比，该模型解锁了文本到视频和音频 (T2VA) 以及参考到视频和音频（动画）功能。为了支持视听同步和参考动画，我们通过联合音视频分支增强了流行的 MMDiT 架构，其中包括用于时间对齐跨模态融合的 TA-CrossAttn 和用于精确视听对齐的 UniTemp-RoPE。同时，精心设计了由音视频字幕、质量控制等组成的综合数据管道，以收集高质量的微调数据。此外，我们引入了一个新的基准来执行全面的模型测试和比较。经过百万级高质量数据的持续预训练和微调，ALIVE表现出了出色的性能，持续优于开源模型，并匹配或超越最先进的商业解决方案。通过详细的配方和基准，我们希望 ALIVE 能够帮助社区更有效地开发音视频生成模型。官方页面：此 https URL。</li>
</ul>

<h3>Title: OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Feilong Tang, Xiang An, Yunyao Yan, Yin Xie, Bin Qin, Kaicheng Yang, Yifei Shen, Yuanhan Zhang, Chunyuan Li, Shikun Feng, Changrui Chen, Huajie Tan, Ming Hu, Manyuan Zhang, Bo Li, Ziyong Feng, Ziwei Liu, Zongyuan Ge, Jiankang Deng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08683">https://arxiv.org/abs/2602.08683</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08683">https://arxiv.org/pdf/2602.08683</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08683]] OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence(https://arxiv.org/abs/2602.08683)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Hypothesis. Artificial general intelligence is, at its core, a compression problem. Effective compression demands resonance: deep learning scales best when its architecture aligns with the fundamental structure of the data. These are the fundamental principles. Yet, modern vision architectures have strayed from these truths: visual signals are highly redundant, while discriminative information, the surprise, is sparse. Current models process dense pixel grids uniformly, wasting vast compute on static background rather than focusing on the predictive residuals that define motion and meaning. We argue that to solve visual understanding, we must align our architectures with the information-theoretic principles of video, i.e., Codecs. Method. OneVision-Encoder encodes video by compressing predictive visual structure into semantic meaning. By adopting Codec Patchification, OV-Encoder abandons uniform computation to focus exclusively on the 3.1%-25% of regions rich in signal entropy. To unify spatial and temporal reasoning under irregular token layouts, OneVision-Encoder employs a shared 3D RoPE and is trained with a large-scale cluster discrimination objective over more than one million semantic concepts, jointly capturing object permanence and motion dynamics. Evidence. The results validate our core hypothesis: efficiency and accuracy are not a trade-off; they are positively correlated. When integrated into LLM, it consistently outperforms strong vision backbones such as Qwen3-ViT and SigLIP2 across 16 image, video, and document understanding benchmarks, despite using substantially fewer visual tokens and pretraining data. Notably, on video understanding tasks, OV-Encoder achieves an average improvement of 4.1% over Qwen3-ViT. Codec-aligned, patch-level sparsity is a foundational principle, enabling OV-Encoder as a scalable engine for next-generation visual generalists.</li>
<li><strong>摘要：</strong>假设。通用人工智能的核心是一个压缩问题。有效的压缩需要共鸣：当深度学习的架构与数据的基本结构保持一致时，深度学习的扩展效果最佳。这些是基本原则。然而，现代视觉架构已经偏离了这些事实：视觉信号高度冗余，而令人惊讶的是，辨别信息却很少。当前的模型统一处理密集的像素网格，在静态背景上浪费大量计算，而不是专注于定义运动和含义的预测残差。我们认为，为了解决视觉理解问题，我们必须使我们的架构与视频的信息理论原则（即编解码器）保持一致。方法。 OneVision-Encoder 通过将预测视觉结构压缩为语义来对视频进行编码。通过采用Codec Patchification，OV-Encoder放弃了统一计算，只关注信号熵丰富的3.1%-25%区域。为了统一不规则标记布局下的空间和时间推理，OneVision-Encoder 采用共享 3D RoPE，并使用超过一百万个语义概念的大规模集群判别目标进行训练，共同捕获对象持久性和运动动态。证据。结果验证了我们的核心假设：效率和准确性不是一个权衡；它们是正相关的。当集成到 LLM 中时，尽管使用的视觉标记和预训练数据少得多，但它在 16 个图像、视频和文档理解基准中始终优于 Qwen3-ViT 和 SigLIP2 等强大的视觉主干。值得注意的是，在视频理解任务上，OV-Encoder 比 Qwen3-ViT 平均提高了 4.1%。编解码器对齐、补丁级稀疏性是一项基本原则，使 OV-Encoder 成为下一代视觉通才的可扩展引擎。</li>
</ul>

<h3>Title: Shifting the Breaking Point of Flow Matching for Multi-Instance Editing</h3>
<ul>
<li><strong>Authors: </strong>Carmine Zaccagnino, Fabio Quattrini, Enis Simsar, Marta Tintoré Gazulla, Rita Cucchiara, Alessio Tonioni, Silvia Cascianelli</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08749">https://arxiv.org/abs/2602.08749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08749">https://arxiv.org/pdf/2602.08749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08749]] Shifting the Breaking Point of Flow Matching for Multi-Instance Editing(https://arxiv.org/abs/2602.08749)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Flow matching models have recently emerged as an efficient alternative to diffusion, especially for text-guided image generation and editing, offering faster inference through continuous-time dynamics. However, existing flow-based editors predominantly support global or single-instruction edits and struggle with multi-instance scenarios, where multiple parts of a reference input must be edited independently without semantic interference. We identify this limitation as a consequence of globally conditioned velocity fields and joint attention mechanisms, which entangle concurrent edits. To address this issue, we introduce Instance-Disentangled Attention, a mechanism that partitions joint attention operations, enforcing binding between instance-specific textual instructions and spatial regions during velocity field estimation. We evaluate our approach on both natural image editing and a newly introduced benchmark of text-dense infographics with region-level editing instructions. Experimental results demonstrate that our approach promotes edit disentanglement and locality while preserving global output coherence, enabling single-pass, instance-level editing.</li>
<li><strong>摘要：</strong>流匹配模型最近已成为扩散的有效替代方案，特别是对于文本引导的图像生成和编辑，通过连续时间动态提供更快的推理。然而，现有的基于流的编辑器主要支持全局或单指令编辑，并且难以应对多实例场景，其中参考输入的多个部分必须在没有语义干扰的情况下独立编辑。我们认为这种限制是全局条件速度场和联合注意机制的结果，这会纠缠并发编辑。为了解决这个问题，我们引入了实例解缠注意力（Instance-Disentangled Attention），这是一种划分联合注意力操作的机制，在速度场估计期间强制特定于实例的文本指令和空间区域之间的绑定。我们评估了自然图像编辑和新引入的带有区域级编辑指令的文本密集信息图表基准的方法。实验结果表明，我们的方法促进了编辑解缠结和局部性，同时保持全局输出一致性，从而实现单通道实例级编辑。</li>
</ul>

<h3>Title: MVAnimate: Enhancing Character Animation with Multi-View Optimization</h3>
<ul>
<li><strong>Authors: </strong>Tianyu Sun, Zhoujie Fu, Bang Zhang, Guosheng Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08753">https://arxiv.org/abs/2602.08753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08753">https://arxiv.org/pdf/2602.08753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08753]] MVAnimate: Enhancing Character Animation with Multi-View Optimization(https://arxiv.org/abs/2602.08753)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The demand for realistic and versatile character animation has surged, driven by its wide-ranging applications in various domains. However, the animation generation algorithms modeling human pose with 2D or 3D structures all face various problems, including low-quality output content and training data deficiency, preventing the related algorithms from generating high-quality animation videos. Therefore, we introduce MVAnimate, a novel framework that synthesizes both 2D and 3D information of dynamic figures based on multi-view prior information, to enhance the generated video quality. Our approach leverages multi-view prior information to produce temporally consistent and spatially coherent animation outputs, demonstrating improvements over existing animation methods. Our MVAnimate also optimizes the multi-view videos of the target character, enhancing the video quality from different views. Experimental results on diverse datasets highlight the robustness of our method in handling various motion patterns and appearances.</li>
<li><strong>摘要：</strong>由于其在各个领域的广泛应用，对逼真且多功能的角色动画的需求激增。然而，利用2D或3D结构建模人体姿势的动画生成算法都面临着各种问题，包括输出内容质量低和训练数据缺乏，阻碍了相关算法生成高质量的动画视频。因此，我们引入了MVAnimate，这是一种新颖的框架，它基于多视图先验信息合成动态人物的2D和3D信息，以提高生成的视频质量。我们的方法利用多视图先验信息来生成时间一致和空间一致的动画输出，展示了对现有动画方法的改进。我们的MVAnimate还优化了目标角色的多视图视频，提高了不同视图的视频质量。不同数据集上的实验结果凸显了我们的方法在处理各种运动模式和外观方面的稳健性。</li>
</ul>

<h3>Title: VedicTHG: Symbolic Vedic Computation for Low-Resource Talking-Head Generation in Educational Avatars</h3>
<ul>
<li><strong>Authors: </strong>Vineet Kumar Rakesh, Ahana Bhattacharjee, Soumya Mazumdar, Tapas Samanta, Hemendra Kumar Pandey, Amitabha Das, Sarbajit Pal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08775">https://arxiv.org/abs/2602.08775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08775">https://arxiv.org/pdf/2602.08775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08775]] VedicTHG: Symbolic Vedic Computation for Low-Resource Talking-Head Generation in Educational Avatars(https://arxiv.org/abs/2602.08775)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Talking-head avatars are increasingly adopted in educational technology to deliver content with social presence and improved engagement. However, many recent talking-head generation (THG) methods rely on GPU-centric neural rendering, large training sets, or high-capacity diffusion models, which limits deployment in offline or resource-constrained learning environments. A deterministic and CPU-oriented THG framework is described, termed Symbolic Vedic Computation, that converts speech to a time-aligned phoneme stream, maps phonemes to a compact viseme inventory, and produces smooth viseme trajectories through symbolic coarticulation inspired by Vedic sutra Urdhva Tiryakbhyam. A lightweight 2D renderer performs region-of-interest (ROI) warping and mouth compositing with stabilization to support real-time synthesis on commodity CPUs. Experiments report synchronization accuracy, temporal stability, and identity consistency under CPU-only execution, alongside benchmarking against representative CPU-feasible baselines. Results indicate that acceptable lip-sync quality can be achieved while substantially reducing computational load and latency, supporting practical educational avatars on low-end hardware. GitHub: this https URL</li>
<li><strong>摘要：</strong>教育技术中越来越多地采用头像头像来传递具有社交存在感和提高参与度的内容。然而，许多最近的头部说话生成（THG）方法依赖于以 GPU 为中心的神经渲染、大型训练集或高容量扩散模型，这限制了在离线或资源受限的学习环境中的部署。描述了一种确定性和面向 CPU 的 THG 框架，称为符号吠陀计算，它将语音转换为时间对齐的音素流，将音素映射到紧凑的视位库存，并通过受吠陀经 Urdhva Tiryakbhyam 启发的符号协同发音产生平滑的视位轨迹。轻量级 2D 渲染器执行感兴趣区域 (ROI) 变形和稳定的嘴部合成，以支持商用 CPU 上的实时合成。实验报告了仅 CPU 执行下的同步准确性、时间稳定性和身份一致性，以及针对代表性 CPU 可行基线的基准测试。结果表明，可以实现可接受的口型同步质量，同时大幅减少计算负载和延迟，支持低端硬件上的实用教育化身。 GitHub：此 https URL</li>
</ul>

<h3>Title: Multimodal Learning for Arcing Detection in Pantograph-Catenary Systems</h3>
<ul>
<li><strong>Authors: </strong>Hao Dong, Eleni Chatzi, Olga Fink</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08792">https://arxiv.org/abs/2602.08792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08792">https://arxiv.org/pdf/2602.08792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08792]] Multimodal Learning for Arcing Detection in Pantograph-Catenary Systems(https://arxiv.org/abs/2602.08792)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The pantograph-catenary interface is essential for ensuring uninterrupted and reliable power delivery in electrified rail systems. However, electrical arcing at this interface poses serious risks, including accelerated wear of contact components, degraded system performance, and potential service disruptions. Detecting arcing events at the pantograph-catenary interface is challenging due to their transient nature, noisy operating environment, data scarcity, and the difficulty of distinguishing arcs from other similar transient phenomena. To address these challenges, we propose a novel multimodal framework that combines high-resolution image data with force measurements to more accurately and robustly detect arcing events. First, we construct two arcing detection datasets comprising synchronized visual and force measurements. One dataset is built from data provided by the Swiss Federal Railways (SBB), and the other is derived from publicly available videos of arcing events in different railway systems and synthetic force data that mimic the characteristics observed in the real dataset. Leveraging these datasets, we propose MultiDeepSAD, an extension of the DeepSAD algorithm for multiple modalities with a new loss formulation. Additionally, we introduce tailored pseudo-anomaly generation techniques specific to each data type, such as synthetic arc-like artifacts in images and simulated force irregularities, to augment training data and improve the discriminative ability of the model. Through extensive experiments and ablation studies, we demonstrate that our framework significantly outperforms baseline approaches, exhibiting enhanced sensitivity to real arcing events even under domain shifts and limited availability of real arcing observations.</li>
<li><strong>摘要：</strong>受电弓-接触网接口对于确保电气化铁路系统不间断且可靠的电力传输至关重要。然而，该接口处的电弧会带来严重的风险，包括接触部件的加速磨损、系统性能下降以及潜在的服务中断。由于其瞬态性质、嘈杂的操作环境、数据稀缺以及将电弧与其他类似瞬态现象区分开来的困难，检测受电弓-接触网界面处的电弧事件具有挑战性。为了应对这些挑战，我们提出了一种新颖的多模式框架，它将高分辨率图像数据与力测量相结合，以更准确、更稳健地检测电弧事件。首先，我们构建两个电弧检测数据集，包括同步的视觉和力测量。一个数据集是根据瑞士联邦铁路 (SBB) 提供的数据构建的，另一个数据集是根据不同铁路系统中电弧事件的公开视频以及模拟真实数据集中观察到的特征的合成力数据得出的。利用这些数据集，我们提出了 MultiDeepSAD，它是 DeepSAD 算法的扩展，具有新的损失公式，适用于多种模式。此外，我们还引入了针对每种数据类型的定制伪异常生成技术，例如图像中的合成弧状伪影和模拟力不规则性，以增强训练数据并提高模型的判别能力。通过广泛的实验和消融研究，我们证明我们的框架显着优于基线方法，即使在域转移和真实电弧观察的可用性有限的情况下，也表现出对真实电弧事件的更高敏感性。</li>
</ul>

<h3>Title: MOVA: Towards Scalable and Synchronized Video-Audio Generation</h3>
<ul>
<li><strong>Authors: </strong>SII-OpenMOSS Team: Donghua Yu, Mingshu Chen, Qi Chen, Qi Luo, Qianyi Wu, Qinyuan Cheng, Ruixiao Li, Tianyi Liang, Wenbo Zhang, Wenming Tu, Xiangyu Peng, Yang Gao, Yanru Huo, Ying Zhu, Yinze Luo, Yiyang Zhang, Yuerong Song, Zhe Xu, Zhiyu Zhang, Chenchen Yang, Cheng Chang, Chushu Zhou, Hanfu Chen, Hongnan Ma, Jiaxi Li, Jingqi Tong, Junxi Liu, Ke Chen, Shimin Li, Songlin Wang, Wei Jiang, Zhaoye Fei, Zhiyuan Ning, Chunguo Li, Chenhui Li, Ziwei He, Zengfeng Huang, Xie Chen, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08794">https://arxiv.org/abs/2602.08794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08794">https://arxiv.org/pdf/2602.08794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08794]] MOVA: Towards Scalable and Synchronized Video-Audio Generation(https://arxiv.org/abs/2602.08794)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.</li>
<li><strong>摘要：</strong>音频对于现实世界的视频是不可或缺的，但一代模型在很大程度上忽略了音频组件。当前制作视听内容的方法通常依赖于级联管道，这会增加成本、累积错误并降低整体质量。虽然 Veo 3 和 Sora 2 等系统强调同时生成的价值，但联合多模态建模在架构、数据和训练方面带来了独特的挑战。此外，现有系统的闭源性质限制了该领域的进展。在这项工作中，我们介绍了 MOVA（MOSS 视频和音频），这是一种开源模型，能够生成高质量、同步的视听内容，包括逼真的口型同步语音、环境感知音效和内容一致的音乐。 MOVA采用混合专家（MoE）架构，共有32B个参数，其中18B在推理过程中处于活动状态。它支持IT2VA（图像文本到视频音频）生成任务。通过发布模型权重和代码，我们的目标是推进研究并培育一个充满活力的创作者社区。发布的代码库全面支持高效推理、LoRA 微调和即时增强。</li>
</ul>

<h3>Title: How2Everything: Mining the Web for How-To Procedures to Evaluate and Improve LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yapei Chang, Kyle Lo, Mohit Iyyer, Luca Soldaini</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08808">https://arxiv.org/abs/2602.08808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08808">https://arxiv.org/pdf/2602.08808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08808]] How2Everything: Mining the Web for How-To Procedures to Evaluate and Improve LLMs(https://arxiv.org/abs/2602.08808)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating step-by-step "how-to" procedures is a key LLM capability: how-to advice is commonly requested in chatbots, and step-by-step planning is critical for reasoning over complex tasks. Yet, measuring and improving procedural validity at scale on real-world tasks remains challenging and understudied. To address this, we introduce How2Everything, a scalable framework to evaluate and improve goal-conditioned procedure generation. Our framework includes How2Mine, which mines 351K procedures from 980K web pages across 14 topics and readily scales to larger corpora. From this pool we build How2Bench, a 7K-example evaluation set balanced across topics. To reliably score model outputs, we develop How2Score, an evaluation protocol that uses an LLM judge to detect whether a generation contains any critical failure that would prevent achieving the goal. For low-cost, reproducible evaluation, we distill a frontier model into an open 8B model, achieving 80.5% agreement with human annotators. How2Bench reveals clear scaling trends across model sizes and training stages, providing signal early in pretraining. Finally, RL using How2Score as a reward improves performance on How2Bench by >10 points across three models without systematic regressions on standard benchmarks, with gains robust to superficial source-document memorization or format compliance. Taken together, How2Everything shows how pretraining web data can support a closed loop of capability evaluation and improvement at scale.</li>
<li><strong>摘要：</strong>生成分步“操作方法”程序是法学硕士的一项关键能力：聊天机器人通常需要操作方法建议，而分步规划对于复杂任务的推理至关重要。然而，大规模测量和提高现实世界任务的程序有效性仍然具有挑战性且研究不足。为了解决这个问题，我们引入了 How2Everything，一个可扩展的框架，用于评估和改进目标条件过程的生成。我们的框架包括 How2Mine，它从 14 个主题的 980K 网页中挖掘 351K 程序，并可以轻松扩展到更大的语料库。我们从这个池中构建了 How2Bench，这是一个跨主题平衡的 7K 示例评估集。为了可靠地对模型输出进行评分，我们开发了 How2Score，这是一种评估协议，它使用 LLM 判断来检测一代是否包含任何妨碍实现目标的严重故障。为了实现低成本、可重复的评估，我们将前沿模型提炼为开放的 8B 模型，与人类注释者达到 80.5% 的一致性。 How2Bench 揭示了模型大小和训练阶段的清晰扩展趋势，在预训练的早期提供信号。最后，使用 How2Score 作为奖励的强化学习将三个模型在 How2Bench 上的性能提高了 10 分以上，而无需对标准基准进行系统回归，并且对表面源文档记忆或格式合规性具有鲁棒性。总而言之，How2Everything 展示了预训练 Web 数据如何支持大规模能力评估和改进的闭环。</li>
</ul>

<h3>Title: Efficient Deep Learning for Biometrics: Overview, Challenges and Trends in Ear of Frugal AI</h3>
<ul>
<li><strong>Authors: </strong>Karim Haroun, Aya Zitouni, Aicha Zenakhri, Meriem Amel Guessoum, Larbi Boubchir</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08809">https://arxiv.org/abs/2602.08809</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08809">https://arxiv.org/pdf/2602.08809</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08809]] Efficient Deep Learning for Biometrics: Overview, Challenges and Trends in Ear of Frugal AI(https://arxiv.org/abs/2602.08809)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in deep learning, whether on discriminative or generative tasks have been beneficial for various applications, among which security and defense. However, their increasing computational demands during training and deployment translates directly into high energy consumption. As a consequence, this induces a heavy carbon footprint which hinders their widespread use and scalability, but also a limitation when deployed on resource-constrained edge devices for real-time use. In this paper, we briefly survey efficient deep learning methods for biometric applications. Specifically, we tackle the challenges one might incur when training and deploying deep learning approaches, and provide a taxonomy of the various efficient deep learning families. Additionally, we discuss complementary metrics for evaluating the efficiency of these models such as memory, computation, latency, throughput, and advocate for universal and reproducible metrics for better comparison. Last, we give future research directions to consider.</li>
<li><strong>摘要：</strong>深度学习的最新进展，无论是在判别性任务还是生成性任务上，都有益于各种应用，其中包括安全和防御。然而，它们在训练和部署过程中不断增加的计算需求直接转化为高能耗。因此，这会产生大量的碳足迹，阻碍了它们的广泛使用和可扩展性，而且在部署在资源有限的边缘设备上进行实时使用时也受到限制。在本文中，我们简要调查了生物识别应用的有效深度学习方法。具体来说，我们解决了训练和部署深度学习方法时可能遇到的挑战，并提供了各种高效深度学习系列的分类。此外，我们讨论了用于评估这些模型效率的补充指标，例如内存、计算、延迟、吞吐量，并提倡通用和可重复的指标以进行更好的比较。最后，我们给出了未来需要考虑的研究方向。</li>
</ul>

<h3>Title: Omni-Video 2: Scaling MLLM-Conditioned Diffusion for Unified Video Generation and Editing</h3>
<ul>
<li><strong>Authors: </strong>Hao Yang, Zhiyu Tan, Jia Gong, Luozheng Qin, Hesen Chen, Xiaomeng Yang, Yuqing Sun, Yuetan Lin, Mengping Yang, Hao Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08820">https://arxiv.org/abs/2602.08820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08820">https://arxiv.org/pdf/2602.08820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08820]] Omni-Video 2: Scaling MLLM-Conditioned Diffusion for Unified Video Generation and Editing(https://arxiv.org/abs/2602.08820)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We present Omni-Video 2, a scalable and computationally efficient model that connects pretrained multimodal large-language models (MLLMs) with video diffusion models for unified video generation and editing. Our key idea is to exploit the understanding and reasoning capabilities of MLLMs to produce explicit target captions to interpret user instructions. In this way, the rich contextual representations from the understanding model are directly used to guide the generative process, thereby improving performance on complex and compositional editing. Moreover, a lightweight adapter is developed to inject multimodal conditional tokens into pretrained text-to-video diffusion models, allowing maximum reuse of their powerful generative priors in a parameter-efficient manner. Benefiting from these designs, we scale up Omni-Video 2 to a 14B video diffusion model on meticulously curated training data with quality, supporting high quality text-to-video generation and various video editing tasks such as object removal, addition, background change, complex motion editing, \emph{etc.} We evaluate the performance of Omni-Video 2 on the FiVE benchmark for fine-grained video editing and the VBench benchmark for text-to-video generation. The results demonstrate its superior ability to follow complex compositional instructions in video editing, while also achieving competitive or superior quality in video generation tasks.</li>
<li><strong>摘要：</strong>我们推出了 Omni-Video 2，这是一种可扩展且计算高效的模型，它将预训练的多模态大语言模型 (MLLM) 与视频扩散模型连接起来，以实现统一的视频生成和编辑。我们的关键想法是利用 MLLM 的理解和推理能力来生成明确的目标标题来解释用户指令。通过这种方式，理解模型中丰富的上下文表示可以直接用于指导生成过程，从而提高复杂和组合编辑的性能。此外，还开发了一个轻量级适配器，将多模态条件标记注入到预训练的文本到视频扩散模型中，从而以参数有效的方式最大程度地重用其强大的生成先验。受益于这些设计，我们在精心策划的高质量训练数据上将 Omni-Video 2 扩展到 14B 视频扩散模型，支持高质量的文本到视频生成和各种视频编辑任务，例如对象删除、添加、背景更改、复杂运动编辑、\emph{等}。我们在细粒度视频编辑的 FiVE 基准和文本到视频生成的 VBench 基准上评估 Omni-Video 2 的性能。结果证明了其在视频编辑中遵循复杂构图指令的卓越能力，同时在视频生成任务中也实现了具有竞争力或卓越的质量。</li>
</ul>

<h3>Title: VideoVeritas: AI-Generated Video Detection via Perception Pretext Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Hao Tan, Jun Lan, Senyuan Shi, Zichang Tan, Zijian Yu, Huijia Zhu, Weiqiang Wang, Jun Wan, Zhen Lei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08828">https://arxiv.org/abs/2602.08828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08828">https://arxiv.org/pdf/2602.08828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08828]] VideoVeritas: AI-Generated Video Detection via Perception Pretext Reinforcement Learning(https://arxiv.org/abs/2602.08828)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The growing capability of video generation poses escalating security risks, making reliable detection increasingly essential. In this paper, we introduce VideoVeritas, a framework that integrates fine-grained perception and fact-based reasoning. We observe that while current multi-modal large language models (MLLMs) exhibit strong reasoning capacity, their granular perception ability remains limited. To mitigate this, we introduce Joint Preference Alignment and Perception Pretext Reinforcement Learning (PPRL). Specifically, rather than directly optimizing for detection task, we adopt general spatiotemporal grounding and self-supervised object counting in the RL stage, enhancing detection performance with simple perception pretext tasks. To facilitate robust evaluation, we further introduce MintVid, a light yet high-quality dataset containing 3K videos from 9 state-of-the-art generators, along with a real-world collected subset that has factual errors in content. Experimental results demonstrate that existing methods tend to bias towards either superficial reasoning or mechanical analysis, while VideoVeritas achieves more balanced performance across diverse benchmarks.</li>
<li><strong>摘要：</strong>视频生成能力的不断增强带来了不断升级的安全风险，因此可靠的检测变得越来越重要。在本文中，我们介绍了VideoVeritas，一个集成了细粒度感知和基于事实的推理的框架。我们观察到，虽然当前的多模态大语言模型（MLLM）表现出强大的推理能力，但它们的粒度感知能力仍然有限。为了缓解这个问题，我们引入了联合偏好对齐和感知借口强化学习（PPRL）。具体来说，我们没有直接针对检测任务进行优化，而是在强化学习阶段采用通用时空基础和自监督对象计数，通过简单的感知借口任务来增强检测性能。为了促进稳健的评估，我们进一步引入了 MintVid，这是一个轻量但高质量的数据集，包含来自 9 个最先进的生成器的 3K 视频，以及现实世界中收集的内容存在事实错误的子集。实验结果表明，现有方法往往偏向于肤浅推理或机械分析，而 VideoVeritas 在不同基准测试中实现了更平衡的性能。</li>
</ul>

<h3>Title: Rethinking Graph Generalization through the Lens of Sharpness-Aware Minimization</h3>
<ul>
<li><strong>Authors: </strong>Yang Qiu, Yixiong Zou, Jun Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08855">https://arxiv.org/abs/2602.08855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08855">https://arxiv.org/pdf/2602.08855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08855]] Rethinking Graph Generalization through the Lens of Sharpness-Aware Minimization(https://arxiv.org/abs/2602.08855)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have achieved remarkable success across various graph-based tasks but remain highly sensitive to distribution shifts. In this work, we focus on a prevalent yet under-explored phenomenon in graph generalization, Minimal Shift Flip (MSF),where test samples that slightly deviate from the training distribution are abruptly misclassified. To interpret this phenomenon, we revisit MSF through the lens of Sharpness-Aware Minimization (SAM), which characterizes the local stability and sharpness of the loss landscape while providing a theoretical foundation for modeling generalization error. To quantify loss sharpness, we introduce the concept of Local Robust Radius, measuring the smallest perturbation required to flip a prediction and establishing a theoretical link between local stability and generalization. Building on this perspective, we further observe a continual decrease in the robust radius during training, indicating weakened local stability and an increasingly sharp loss landscape that gives rise to MSF. To jointly solve the MSF phenomenon and the intractability of radius, we develop an energy-based formulation that is theoretically proven to be monotonically correlated with the robust radius, offering a tractable and principled objective for modeling flatness and stability. Building on these insights, we propose an energy-driven generative augmentation framework (E2A) that leverages energy-guided latent perturbations to generate pseudo-OOD samples and enhance model generalization. Extensive experiments across multiple benchmarks demonstrate that E2A consistently improves graph OOD generalization, outperforming state-of-the-art baselines.</li>
<li><strong>摘要：</strong>图神经网络（GNN）在各种基于图的任务中取得了显着的成功，但对分布变化仍然高度敏感。在这项工作中，我们关注图泛化中普遍存在但尚未充分探索的现象，即最小移位翻转（MSF），其中稍微偏离训练分布的测试样本突然被错误分类。为了解释这一现象，我们通过锐度感知最小化（SAM）的镜头重新审视 MSF，它表征了损失景观的局部稳定性和锐度，同时为泛化误差建模提供了理论基础。为了量化损失锐度，我们引入了局部鲁棒半径的概念，测量翻转预测所需的最小扰动，并在局部稳定性和泛化之间建立理论联系。基于这个观点，我们进一步观察到训练过程中鲁棒半径不断减小，表明局部稳定性减弱，损失景观日益尖锐，从而产生了 MSF。为了共同解决 MSF 现象和半径的棘手问题，我们开发了一种基于能量的公式，该公式在理论上被证明与鲁棒半径单调相关，为平坦度和稳定性建模提供了易于处理且有原则的目标。基于这些见解，我们提出了一种能量驱动的生成增强框架（E2A），该框架利用能量引导的潜在扰动来生成伪 OOD 样本并增强模型泛化。跨多个基准的大量实验表明，E2A 持续改进图 OOD 泛化能力，优于最先进的基线。</li>
</ul>

<h3>Title: Magnitude Distance: A Geometric Measure of Dataset Similarity</h3>
<ul>
<li><strong>Authors: </strong>Sahel Torkamani, Henry Gouk, Rik Sarkar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08859">https://arxiv.org/abs/2602.08859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08859">https://arxiv.org/pdf/2602.08859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08859]] Magnitude Distance: A Geometric Measure of Dataset Similarity(https://arxiv.org/abs/2602.08859)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Quantifying the distance between datasets is a fundamental question in mathematics and machine learning. We propose \textit{magnitude distance}, a novel distance metric defined on finite datasets using the notion of the \emph{magnitude} of a metric space. The proposed distance incorporates a tunable scaling parameter, $t$, that controls the sensitivity to global structure (small $t$) and finer details (large $t$). We prove several theoretical properties of magnitude distance, including its limiting behavior across scales and conditions under which it satisfies key metric properties. In contrast to classical distances, we show that magnitude distance remains discriminative in high-dimensional settings when the scale is appropriately tuned. We further demonstrate how magnitude distance can be used as a training objective for push-forward generative models. Our experimental results support our theoretical analysis and demonstrate that magnitude distance provides meaningful signals, comparable to established distance-based generative approaches.</li>
<li><strong>摘要：</strong>量化数据集之间的距离是数学和机器学习中的一个基本问题。我们提出 \textit{magnitude distance}，这是一种使用度量空间的 \emph{magnitude} 概念在有限数据集上定义的新颖距离度量。所提出的距离包含一个可调缩放参数 $t$，该参数控制对全局结构（小 $t$）和更精细细节（大 $t$）的敏感度。我们证明了星等距离的几个理论属性，包括它在尺度上的限制行为以及它满足关键度量属性的条件。与经典距离相比，我们表明，当尺度适当调整时，星等距离在高维设置中仍然具有辨别力。我们进一步演示了如何将幅度距离用作前推生成模型的训练目标。我们的实验结果支持我们的理论分析，并证明震级距离提供了有意义的信号，与现有的基于距离的生成方法相当。</li>
</ul>

<h3>Title: Discrete Bridges for Mutual Information Estimation</h3>
<ul>
<li><strong>Authors: </strong>Iryna Zabarianska, Sergei Kholkin, Grigoriy Ksenofontov, Ivan Butakov, Alexander Korotin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08894">https://arxiv.org/abs/2602.08894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08894">https://arxiv.org/pdf/2602.08894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08894]] Discrete Bridges for Mutual Information Estimation(https://arxiv.org/abs/2602.08894)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion bridge models in both continuous and discrete state spaces have recently become powerful tools in the field of generative modeling. In this work, we leverage the discrete state space formulation of bridge matching models to address another important problem in machine learning and information theory: the estimation of the mutual information (MI) between discrete random variables. By neatly framing MI estimation as a domain transfer problem, we construct a Discrete Bridge Mutual Information (DBMI) estimator suitable for discrete data, which poses difficulties for conventional MI estimators. We showcase the performance of our estimator on two MI estimation settings: low-dimensional and image-based.</li>
<li><strong>摘要：</strong>连续和离散状态空间中的扩散桥模型最近已成为生成建模领域的强大工具。在这项工作中，我们利用桥匹配模型的离散状态空间公式来解决机器学习和信息论中的另一个重要问题：离散随机变量之间的互信息（MI）的估计。通过巧妙地将 MI 估计构建为域转移问题，我们构建了适合离散数据的离散桥互信息 (DBMI) 估计器，这给传统 MI 估计器带来了困难。我们展示了估计器在两种 MI 估计设置上的性能：低维和基于图像。</li>
</ul>

<h3>Title: WorldArena: A Unified Benchmark for Evaluating Perception and Functional Utility of Embodied World Models</h3>
<ul>
<li><strong>Authors: </strong>Yu Shang, Zhuohang Li, Yiding Ma, Weikang Su, Xin Jin, Ziyou Wang, Xin Zhang, Yinzhou Tang, Chen Gao, Wei Wu, Xihui Liu, Dhruv Shah, Zhaoxiang Zhang, Zhibo Chen, Jun Zhu, Yonghong Tian, Tat-Seng Chua, Wenwu Zhu, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08971">https://arxiv.org/abs/2602.08971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08971">https://arxiv.org/pdf/2602.08971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08971]] WorldArena: A Unified Benchmark for Evaluating Perception and Functional Utility of Embodied World Models(https://arxiv.org/abs/2602.08971)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While world models have emerged as a cornerstone of embodied intelligence by enabling agents to reason about environmental dynamics through action-conditioned prediction, their evaluation remains fragmented. Current evaluation of embodied world models has largely focused on perceptual fidelity (e.g., video generation quality), overlooking the functional utility of these models in downstream decision-making tasks. In this work, we introduce WorldArena, a unified benchmark designed to systematically evaluate embodied world models across both perceptual and functional dimensions. WorldArena assesses models through three dimensions: video perception quality, measured with 16 metrics across six sub-dimensions; embodied task functionality, which evaluates world models as data engines, policy evaluators, and action planners integrating with subjective human evaluation. Furthermore, we propose EWMScore, a holistic metric integrating multi-dimensional performance into a single interpretable index. Through extensive experiments on 14 representative models, we reveal a significant perception-functionality gap, showing that high visual quality does not necessarily translate into strong embodied task capability. WorldArena benchmark with the public leaderboard is released at this https URL, providing a framework for tracking progress toward truly functional world models in embodied AI.</li>
<li><strong>摘要：</strong>虽然世界模型已成为体现智能的基石，使智能体能够通过行动条件预测来推理环境动态，但它们的评估仍然支离破碎。目前对具体世界模型的评估主要集中在感知保真度（例如视频生成质量），而忽视了这些模型在下游决策任务中的功能效用。在这项工作中，我们介绍了 WorldArena，这是一个统一的基准，旨在跨感知和功能维度系统地评估具体世界模型。 WorldArena 通过三个维度评估模型：视频感知质量，通过 6 个子维度的 16 个指标进行衡量；体现任务功能，将世界模型评估为数据引擎、政策评估者和与主观人类评估相结合的行动规划者。此外，我们提出了 EWMScore，这是一种将多维性能集成到单个可解释指数中的整体指标。通过对 14 个代表性模型的广泛实验，我们揭示了显着的感知功能差距，表明高视觉质量并不一定转化为强大的具体任务能力。 WorldArena 基准测试和公共排行榜在此 https URL 发布，提供了一个框架，用于跟踪具体人工智能中真正功能性世界模型的进展。</li>
</ul>

<h3>Title: Distributionally Robust Optimization via Generative Ambiguity Modeling</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Wen, Jianyi Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08976">https://arxiv.org/abs/2602.08976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08976">https://arxiv.org/pdf/2602.08976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08976]] Distributionally Robust Optimization via Generative Ambiguity Modeling(https://arxiv.org/abs/2602.08976)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper studies Distributionally Robust Optimization (DRO), a fundamental framework for enhancing the robustness and generalization of statistical learning and optimization. An effective ambiguity set for DRO must involve distributions that remain consistent to the nominal distribution while being diverse enough to account for a variety of potential scenarios. Moreover, it should lead to tractable DRO solutions. To this end, we propose generative model-based ambiguity sets that capture various adversarial distributions beyond the nominal support space while maintaining consistency with the nominal distribution. Building on this generative ambiguity modeling, we propose DRO with Generative Ambiguity Set (GAS-DRO), a tractable DRO algorithm that solves the inner maximization over the parameterized generative model space. We formally establish the stationary convergence performance of GAS-DRO. We implement GAS-DRO with a diffusion model and empirically demonstrate its superior Out-of-Distribution (OOD) generalization performance in ML tasks.</li>
<li><strong>摘要：</strong>本文研究分布式鲁棒优化（DRO），这是一个增强统计学习和优化的鲁棒性和泛化性的基本框架。 DRO 的有效模糊度集必须涉及与名义分布保持一致的分布，同时足够多样化以考虑各种潜在场景。此外，它应该会带来易于处理的 DRO 解决方案。为此，我们提出了基于生成模型的模糊集，该集捕获超出名义支持空间的各种对抗分布，同时保持与名义分布的一致性。基于这种生成歧义建模，我们提出了带有生成歧义集的 DRO（GAS-DRO），这是一种易于处理的 DRO 算法，可以解决参数化生成模型空间上的内部最大化问题。我们正式建立了 GAS-DRO 的平稳收敛性能。我们使用扩散模型实现 GAS-DRO，并凭经验证明其在 ML 任务中卓越的分布外 (OOD) 泛化性能。</li>
</ul>

<h3>Title: Generalizing Sports Feedback Generation by Watching Competitions and Reading Books: A Rock Climbing Case Study</h3>
<ul>
<li><strong>Authors: </strong>Arushi Rai, Adriana Kovashka</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08996">https://arxiv.org/abs/2602.08996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08996">https://arxiv.org/pdf/2602.08996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08996]] Generalizing Sports Feedback Generation by Watching Competitions and Reading Books: A Rock Climbing Case Study(https://arxiv.org/abs/2602.08996)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While there is rapid progress in video-LLMs with advanced reasoning capabilities, prior work shows that these models struggle on the challenging task of sports feedback generation and require expensive and difficult-to-collect finetuning feedback data for each sport. This limitation is evident from the poor generalization to sports unseen during finetuning. Furthermore, traditional text generation evaluation metrics (e.g., BLEU-4, METEOR, ROUGE-L, BERTScore), originally developed for machine translation and summarization, fail to capture the unique aspects of sports feedback quality. To address the first problem, using rock climbing as our case study, we propose using auxiliary freely-available web data from the target domain, such as competition videos and coaching manuals, in addition to existing sports feedback from a disjoint, source domain to improve sports feedback generation performance on the target domain. To improve evaluation, we propose two evaluation metrics: (1) specificity and (2) actionability. Together, our approach enables more meaningful and practical generation of sports feedback under limited annotations.</li>
<li><strong>摘要：</strong>虽然具有高级推理能力的视频法学硕士取得了快速进展，但先前的工作表明，这些模型在运动反馈生成这一具有挑战性的任务上举步维艰，并且需要昂贵且难以收集的每项运动的微调反馈数据。这种限制从微调过程中看不见的运动的较差泛化中显而易见。此外，最初为机器翻译和摘要而开发的传统文本生成评估指标（例如 BLEU-4、METEOR、ROUGE-L、BERTScore）无法捕捉运动反馈质量的独特方面。为了解决第一个问题，以攀岩作为我们的案例研究，除了来自不相交的源域的现有运动反馈之外，我们建议使用来自目标域的辅助免费网络数据，例如比赛视频和教练手册，以提高目标域上的运动反馈生成性能。为了改进评估，我们提出了两个评估指标：（1）特异性和（2）可操作性。总之，我们的方法可以在有限的注释下生成更有意义、更实用的运动反馈。</li>
</ul>

<h3>Title: Next-Gen CAPTCHAs: Leveraging the Cognitive Gap for Scalable and Diverse GUI-Agent Defense</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Liu, Yaxin Luo, Jiacheng Cui, Xinyi Shang, Xiaohan Zhao, Zhiqiang Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.09012">https://arxiv.org/abs/2602.09012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.09012">https://arxiv.org/pdf/2602.09012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.09012]] Next-Gen CAPTCHAs: Leveraging the Cognitive Gap for Scalable and Diverse GUI-Agent Defense(https://arxiv.org/abs/2602.09012)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The rapid evolution of GUI-enabled agents has rendered traditional CAPTCHAs obsolete. While previous benchmarks like OpenCaptchaWorld established a baseline for evaluating multimodal agents, recent advancements in reasoning-heavy models, such as Gemini3-Pro-High and GPT-5.2-Xhigh have effectively collapsed this security barrier, achieving pass rates as high as 90% on complex logic puzzles like "Bingo". In response, we introduce Next-Gen CAPTCHAs, a scalable defense framework designed to secure the next-generation web against the advanced agents. Unlike static datasets, our benchmark is built upon a robust data generation pipeline, allowing for large-scale and easily scalable evaluations, notably, for backend-supported types, our system is capable of generating effectively unbounded CAPTCHA instances. We exploit the persistent human-agent "Cognitive Gap" in interactive perception, memory, decision-making, and action. By engineering dynamic tasks that require adaptive intuition rather than granular planning, we re-establish a robust distinction between biological users and artificial agents, offering a scalable and diverse defense mechanism for the agentic era.</li>
<li><strong>摘要：</strong>支持 GUI 的代理的快速发展已经使传统的验证码变得过时。虽然 OpenCaptchaWorld 等之前的基准测试为评估多模式代理建立了基准，但 Gemini3-Pro-High 和 GPT-5.2-Xhigh 等推理型模型的最新进展有效地打破了这一安全屏障，在“Bingo”等复杂逻辑谜题上实现了高达 90% 的通过率。作为回应，我们推出了下一代验证码，这是一个可扩展的防御框架，旨在保护下一代网络免受高级代理的攻击。与静态数据集不同，我们的基准测试建立在强大的数据生成管道之上，允许大规模且易于扩展的评估，特别是对于后端支持的类型，我们的系统能够有效生成无限制的验证码实例。我们利用交互感知、记忆、决策和行动中持续存在的人类代理“认知差距”。通过设计需要自适应直觉而不是细粒度规划的动态任务，我们重新建立了生物用户和人工代理之间的牢固区别，为代理时代提供了可扩展且多样化的防御机制。</li>
</ul>

<h3>Title: ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation</h3>
<ul>
<li><strong>Authors: </strong>Zihan Yang (1), Shuyuan Tu (1), Licheng Zhang (1), Qi Dai (2), Yu-Gang Jiang (1), Zuxuan Wu (1) ((1) Fudan University, (2) Microsoft Research Asia)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.09014">https://arxiv.org/abs/2602.09014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.09014">https://arxiv.org/pdf/2602.09014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.09014]] ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation(https://arxiv.org/abs/2602.09014)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively.</li>
<li><strong>摘要：</strong>扩散模型已经实现了卓越的生成质量，但由于依赖多个连续的去噪步骤，它们的推理成本很高，这促使人们最近努力将这种推理过程提炼为几个步骤。然而，现有的蒸馏方法通常使用线性捷径来近似教师轨迹，这使得随着速度跨时间步长的变化而难以匹配其不断变化的切线方向，从而导致质量下降。为了解决这个限制，我们提出了 ArcFlow，这是一种分步蒸馏框架，它明确地采用非线性流动轨迹来近似预先训练的教师轨迹。具体来说，ArcFlow 将推理轨迹下的速度场参数化为连续动量过程的混合。这使得 ArcFlow 能够捕获速度演化并推断相干速度，以在每个去噪步骤内形成连续的非线性轨迹。重要的是，这种参数化允许对非线性轨迹进行分析积分，从而避免数值离散误差并导致教师轨迹的高精度近似。为了将这种参数化训练成几步生成器，我们使用轻量级适配器在预先训练的教师模型上通过轨迹蒸馏来实现 ArcFlow。该策略确保快速、稳定的收敛，同时保持生成多样性和质量。 ArcFlow 基于大型模型（Qwen-Image-20B 和 FLUX.1-dev）构建，仅对不到 5% 的原始参数进行微调，与原始多步教师相比，通过 2 个 NFE 实现了 40 倍的加速，而质量没有显着下降。基准实验从定性和定量两个方面证明了 ArcFlow 的有效性。</li>
</ul>

<h3>Title: Raster2Seq: Polygon Sequence Generation for Floorplan Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Hao Phung, Hadar Averbuch-Elor</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.09016">https://arxiv.org/abs/2602.09016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.09016">https://arxiv.org/pdf/2602.09016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.09016]] Raster2Seq: Polygon Sequence Generation for Floorplan Reconstruction(https://arxiv.org/abs/2602.09016)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reconstructing a structured vector-graphics representation from a rasterized floorplan image is typically an important prerequisite for computational tasks involving floorplans such as automated understanding or CAD workflows. However, existing techniques struggle in faithfully generating the structure and semantics conveyed by complex floorplans that depict large indoor spaces with many rooms and a varying numbers of polygon corners. To this end, we propose Raster2Seq, framing floorplan reconstruction as a sequence-to-sequence task in which floorplan elements--such as rooms, windows, and doors--are represented as labeled polygon sequences that jointly encode geometry and semantics. Our approach introduces an autoregressive decoder that learns to predict the next corner conditioned on image features and previously generated corners using guidance from learnable anchors. These anchors represent spatial coordinates in image space, hence allowing for effectively directing the attention mechanism to focus on informative image regions. By embracing the autoregressive mechanism, our method offers flexibility in the output format, enabling for efficiently handling complex floorplans with numerous rooms and diverse polygon structures. Our method achieves state-of-the-art performance on standard benchmarks such as Structure3D, CubiCasa5K, and Raster2Graph, while also demonstrating strong generalization to more challenging datasets like WAFFLE, which contain diverse room structures and complex geometric variations.</li>
<li><strong>摘要：</strong>从光栅化平面图图像重建结构化矢量图形表示通常是涉及平面图的计算任务（例如自动理解或 CAD 工作流程）的重要先决条件。然而，现有技术很难忠实地生成复杂平面图所传达的结构和语义，这些平面图描绘了具有许多房间和不同数量的多边形角的大型室内空间。为此，我们提出了 Raster2Seq，将平面图重建框架作为序列到序列的任务，其中平面图元素（例如房间、窗户和门）被表示为联合编码几何和语义的标记多边形序列。我们的方法引入了一种自回归解码器，它学习使用可学习锚点的指导来根据图像特征和先前生成的角点来预测下一个角点。这些锚点代表图像空间中的空间坐标，因此可以有效地引导注意力机制关注信息丰富的图像区域。通过采用自回归机制，我们的方法提供了输出格式的灵活性，能够有效地处理具有大量房间和不同多边形结构的复杂平面图。我们的方法在 Structure3D、CubiCasa5K 和 Raster2Graph 等标准基准上实现了最先进的性能，同时还展示了对 WAFFLE 等更具挑战性的数据集的强大泛化能力，这些数据集包含不同的房间结构和复杂的几何变化。</li>
</ul>

<h3>Title: WorldCompass: Reinforcement Learning for Long-Horizon World Models</h3>
<ul>
<li><strong>Authors: </strong>Zehan Wang, Tengfei Wang, Haiyu Zhang, Xuhui Zuo, Junta Wu, Haoyuan Wang, Wenqiang Sun, Zhenwei Wang, Chenjie Cao, Hengshuang Zhao, Chunchao Guo, Zhou Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.09022">https://arxiv.org/abs/2602.09022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.09022">https://arxiv.org/pdf/2602.09022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.09022]] WorldCompass: Reinforcement Learning for Long-Horizon World Models(https://arxiv.org/abs/2602.09022)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively "steer" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.</li>
<li><strong>摘要：</strong>这项工作提出了 WorldCompass，这是一种新颖的强化学习 (RL) 后训练框架，适用于长视野、基于交互式视频的世界模型，使他们能够根据交互信号更准确、一致地探索世界。为了有效地“引导”世界模型的探索，我们引入了针对自回归视频生成范式量身定制的三项核心创新：1）剪辑级推出策略：我们在单个目标剪辑上生成并评估多个样本，这显着提高了推出效率并提供细粒度的奖励信号。 2）补充奖励函数：我们设计了针对交互跟踪准确性和视觉质量的奖励函数，提供直接监督并有效抑制奖励黑客行为。 3）高效的强化学习算法：我们采用负感知微调策略结合各种效率优化来高效且有效地增强模型容量。对SoTA开源世界模型WorldPlay的评估表明，WorldCompass显着提高了各种场景下的交互准确性和视觉保真度。</li>
</ul>

<h3>Title: Autoregressive Image Generation with Masked Bit Modeling</h3>
<ul>
<li><strong>Authors: </strong>Qihang Yu, Qihao Liu, Ju He, Xinyang Zhang, Yang Liu, Liang-Chieh Chen, Xi Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.09024">https://arxiv.org/abs/2602.09024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.09024">https://arxiv.org/pdf/2602.09024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.09024]] Autoregressive Image Generation with Masked Bit Modeling(https://arxiv.org/abs/2602.09024)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This paper challenges the dominance of continuous pipelines in visual generation. We systematically investigate the performance gap between discrete and continuous methods. Contrary to the belief that discrete tokenizers are intrinsically inferior, we demonstrate that the disparity arises primarily from the total number of bits allocated in the latent space (i.e., the compression ratio). We show that scaling up the codebook size effectively bridges this gap, allowing discrete tokenizers to match or surpass their continuous counterparts. However, existing discrete generation methods struggle to capitalize on this insight, suffering from performance degradation or prohibitive training costs with scaled codebook. To address this, we propose masked Bit AutoRegressive modeling (BAR), a scalable framework that supports arbitrary codebook sizes. By equipping an autoregressive transformer with a masked bit modeling head, BAR predicts discrete tokens through progressively generating their constituent bits. BAR achieves a new state-of-the-art gFID of 0.99 on ImageNet-256, outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches. Project page is available at this https URL</li>
<li><strong>摘要：</strong>本文挑战了连续管道在视觉生成中的主导地位。我们系统地研究离散方法和连续方法之间的性能差距。与离散分词器本质上较差的观点相反，我们证明这种差异主要源于潜在空间中分配的位数（即压缩比）。我们证明，扩大码本大小可以有效地弥补这一差距，使离散分词器能够匹配或超越其连续分词器。然而，现有的离散生成方法很难利用这种洞察力，因为性能下降或缩放码本的训练成本过高。为了解决这个问题，我们提出了屏蔽位自回归建模（BAR），这是一种支持任意码本大小的可扩展框架。通过为自回归变压器配备掩码位建模头，BAR 通过逐步生成离散标记的组成位来预测离散标记。 BAR 在 ImageNet-256 上实现了最先进的 gFID 0.99，在连续和离散范式上都优于领先方法，同时显着降低了采样成本，并且比之前的连续方法收敛得更快。项目页面可在此 https URL 获取</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
