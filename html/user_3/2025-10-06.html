<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-06</h1>
<h3>Title: Extreme value forecasting using relevance-based data augmentation with deep learning models</h3>
<ul>
<li><strong>Authors: </strong>Junru Hua, Rahul Ahluwalia, Rohitash Chandra</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02407">https://arxiv.org/abs/2510.02407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02407">https://arxiv.org/pdf/2510.02407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02407]] Extreme value forecasting using relevance-based data augmentation with deep learning models(https://arxiv.org/abs/2510.02407)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Data augmentation with generative adversarial networks (GANs) has been popular for class imbalance problems, mainly for pattern classification and computer vision-related applications. Extreme value forecasting is a challenging field that has various applications from finance to climate change problems. In this study, we present a data augmentation framework for extreme value forecasting. In this framework, our focus is on forecasting extreme values using deep learning models in combination with data augmentation models such as GANs and synthetic minority oversampling technique (SMOTE). We use deep learning models such as convolutional long short-term memory (Conv-LSTM) and bidirectional long short-term memory (BD-LSTM) networks for multistep ahead prediction featuring extremes. We investigate which data augmentation models are the most suitable, taking into account the prediction accuracy overall and at extreme regions, along with computational efficiency. We also present novel strategies for incorporating data augmentation, considering extreme values based on a relevance function. Our results indicate that the SMOTE-based strategy consistently demonstrated superior adaptability, leading to improved performance across both short- and long-horizon forecasts. Conv-LSTM and BD-LSTM exhibit complementary strengths: the former excels in periodic, stable datasets, while the latter performs better in chaotic or non-stationary sequences.</li>
<li><strong>摘要：</strong>具有生成对抗网络（GAN）的数据增强在类不平衡问题上很受欢迎，主要用于模式分类和与计算机视觉相关的应用程序。极端价值预测是一个具有挑战性的领域，具有从金融到气候变化问题的各种应用。在这项研究中，我们提出了一个数据增强框架，以预测极端价值。在此框架中，我们的重点是使用深度学习模型与数据增强模型（例如gan和合成少数族裔过度采样技术（SMOTE））结合使用深度学习模型进行预测。我们使用深度学习模型，例如卷积长的短期记忆（CORV-LSTM）和双向长期记忆（BD-LSTM）网络，以实现多步骤的预测，以极端的方式进行预测。我们研究了哪些数据增强模型最合适，考虑到总体和极端区域的预测准确性以及计算效率。我们还提出了基于相关函数的极端价值，以纳入数据增强。我们的结果表明，基于SMOTE的策略始终显示出卓越的适应性，从而改善了短期和长期预测的性能。 CORV-LSTM和BD-LSTM具有互补的优势：前者在周期性稳定的数据集中表现出色，而后者在混乱或非平稳序列中表现更好。</li>
</ul>

<h3>Title: Graph Generation with Spectral Geodesic Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Xikun Huang, Tianyu Ruan, Chihao Zhang, Shihua Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02520">https://arxiv.org/abs/2510.02520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02520">https://arxiv.org/pdf/2510.02520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02520]] Graph Generation with Spectral Geodesic Flow Matching(https://arxiv.org/abs/2510.02520)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Graph generation is a fundamental task with wide applications in modeling complex systems. Although existing methods align the spectrum or degree profile of the target graph, they often ignore the geometry induced by eigenvectors and the global structure of the graph. In this work, we propose Spectral Geodesic Flow Matching (SFMG), a novel framework that uses spectral eigenmaps to embed both input and target graphs into continuous Riemannian manifolds. We then define geodesic flows between embeddings and match distributions along these flows to generate output graphs. Our method yields several advantages: (i) captures geometric structure beyond eigenvalues, (ii) supports flexible generation of diverse graphs, and (iii) scales efficiently. Empirically, SFMG matches the performance of state-of-the-art approaches on graphlet, degree, and spectral metrics across diverse benchmarks. In particular, it achieves up to 30$\times$ speedup over diffusion-based models, offering a substantial advantage in scalability and training efficiency. We also demonstrate its ability to generalize to unseen graph scales. Overall, SFMG provides a new approach to graph synthesis by integrating spectral geometry with flow matching.</li>
<li><strong>摘要：</strong>图是一项基本任务，在建模复杂系统中进行了广泛的应用。尽管现有方法对齐目标图的频谱或程度曲线，但它们通常会忽略特征向量引起的几何形状和图形的整体结构。在这项工作中，我们提出了光谱测量流匹配（SFMG），这是一个新颖的框架，该框架使用光谱本特征图将输入和目标图嵌入连续的riemannian歧管中。然后，我们沿着这些流量定义了嵌入和匹配分布之间的测量流以生成输出图。我们的方法产生了几个优点：（i）捕获特征值以外的几何结构，（ii）支持柔性生成各种图形，并且（iii）尺度有效。从经验上讲，SFMG与各种基准测试的图形，学位和光谱指标上最先进的方法的性能相匹配。特别是，它在基于扩散的模型上最多可实现30美元$ \ times $加速，从而在可伸缩性和训练效率方面具有很大的优势。我们还展示了其推广到看不见的图形量表的能力。总体而言，SFMG通过将光谱几何形状与流量匹配整合在一起，提供了一种新的图形合成方法。</li>
</ul>

<h3>Title: Exploring OCR-augmented Generation for Bilingual VQA</h3>
<ul>
<li><strong>Authors: </strong>JoonHo Lee, Sunho Park</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02543">https://arxiv.org/abs/2510.02543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02543">https://arxiv.org/pdf/2510.02543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02543]] Exploring OCR-augmented Generation for Bilingual VQA(https://arxiv.org/abs/2510.02543)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We investigate OCR-augmented generation with Vision Language Models (VLMs), exploring tasks in Korean and English toward multilingualism. To support research in this domain, we train and release KLOCR, a strong bilingual OCR baseline trained on 100M instances to augment VLMs with OCR ability. To complement existing VQA benchmarks, we curate KOCRBench for Korean VQA, and analyze different prompting methods. Extensive experiments show that OCR-extracted text significantly boosts performance across open source and commercial models. Our work offers new insights into OCR-augmented generation for bilingual VQA. Model, code, and data are available at this https URL.</li>
<li><strong>摘要：</strong>我们通过视觉语言模型（VLM）调查了OCR的一代，探索了韩语和英语的任务，以实现多种语言。为了支持该领域的研究，我们训练和释放KLOCR，这是一种强大的双语OCR基线，对100m实例进行了训练，以增强OCR能力的VLM。为了补充现有的VQA基准测试，我们为韩国VQA策划了Kocrbench，并分析了不同的提示方法。广泛的实验表明，OCR提取的文本可显着提高开源和商业模型的性能。我们的作品为双语VQA的OCR演说一代提供了新的见解。模型，代码和数据可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: How Confident are Video Models? Empowering Video Models to Express their Uncertainty</h3>
<ul>
<li><strong>Authors: </strong>Zhiting Mei, Ola Shorinwa, Anirudha Majumdar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02571">https://arxiv.org/abs/2510.02571</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02571">https://arxiv.org/pdf/2510.02571</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02571]] How Confident are Video Models? Empowering Video Models to Express their Uncertainty(https://arxiv.org/abs/2510.02571)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative video models demonstrate impressive text-to-video capabilities, spurring widespread adoption in many real-world applications. However, like large language models (LLMs), video generation models tend to hallucinate, producing plausible videos even when they are factually wrong. Although uncertainty quantification (UQ) of LLMs has been extensively studied in prior work, no UQ method for video models exists, raising critical safety concerns. To our knowledge, this paper represents the first work towards quantifying the uncertainty of video models. We present a framework for uncertainty quantification of generative video models, consisting of: (i) a metric for evaluating the calibration of video models based on robust rank correlation estimation with no stringent modeling assumptions; (ii) a black-box UQ method for video models (termed S-QUBED), which leverages latent modeling to rigorously decompose predictive uncertainty into its aleatoric and epistemic components; and (iii) a UQ dataset to facilitate benchmarking calibration in video models. By conditioning the generation task in the latent space, we disentangle uncertainty arising due to vague task specifications from that arising from lack of knowledge. Through extensive experiments on benchmark video datasets, we demonstrate that S-QUBED computes calibrated total uncertainty estimates that are negatively correlated with the task accuracy and effectively computes the aleatoric and epistemic constituents.</li>
<li><strong>摘要：</strong>生成视频模型展示了令人印象深刻的文本对视频功能，在许多现实世界中都广泛采用了广泛的采用。但是，像大型语言模型（LLMS）一样，视频生成模型倾向于幻觉，即使实际上是错误的，也会产生合理的视频。尽管LLM的不确定性量化（UQ）在先前的工作中已经进行了广泛的研究，但不存在视频模型的UQ方法，从而引发了关键的安全问题。据我们所知，本文代表了量化视频模型不确定性的第一项工作。我们提出了一个用于生成视频模型的不确定性量化的框架，该框架由：（i）用于评估基于强大的秩相关估计的视频模型校准的度量，而没有严格的建模假设； （ii）一种用于视频模型（称为S QueD）的黑盒UQ方法，它利用潜在的建模将预测性不确定性严格分解为其质地和认知成分； （iii）一个UQ数据集，以促进视频模型中的基准测试。通过调节潜在空间中的发电任务，我们将由于缺乏知识而引起的含糊任务规范引起的不确定性删除。通过基准视频数据集的广泛实验，我们证明了S Qubed Compuct校准了与任务准确性负相关的总体不确定性估计值，并有效地计算出了核心和认识的成分。</li>
</ul>

<h3>Title: Input-Aware Sparse Attention for Real-Time Co-Speech Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Beijia Lu, Ziyi Chen, Jing Xiao, Jun-Yan Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02617">https://arxiv.org/abs/2510.02617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02617">https://arxiv.org/pdf/2510.02617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02617]] Input-Aware Sparse Attention for Real-Time Co-Speech Video Generation(https://arxiv.org/abs/2510.02617)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models can synthesize realistic co-speech video from audio for various applications, such as video creation and virtual agents. However, existing diffusion-based methods are slow due to numerous denoising steps and costly attention mechanisms, preventing real-time deployment. In this work, we distill a many-step diffusion video model into a few-step student model. Unfortunately, directly applying recent diffusion distillation methods degrades video quality and falls short of real-time performance. To address these issues, our new video distillation method leverages input human pose conditioning for both attention and loss functions. We first propose using accurate correspondence between input human pose keypoints to guide attention to relevant regions, such as the speaker's face, hands, and upper body. This input-aware sparse attention reduces redundant computations and strengthens temporal correspondences of body parts, improving inference efficiency and motion coherence. To further enhance visual quality, we introduce an input-aware distillation loss that improves lip synchronization and hand motion realism. By integrating our input-aware sparse attention and distillation loss, our method achieves real-time performance with improved visual quality compared to recent audio-driven and input-driven methods. We also conduct extensive experiments showing the effectiveness of our algorithmic design choices.</li>
<li><strong>摘要：</strong>扩散模型可以为各种应用程序（例如视频创建和虚拟代理）从音频中综合现实的共同语音视频。但是，由于众多降解步骤和昂贵的注意机制，因此现有的基于扩散的方法速度很慢，从而阻止了实时部署。在这项工作中，我们将多步扩散视频模型提炼为几步学生模型。不幸的是，直接采用最近的扩散蒸馏方法降低了视频质量，并且缺乏实时性能。为了解决这些问题，我们的新视频蒸馏方法利用了注重和损失功能的人姿势调节。我们首先建议使用输入人姿势关键点之间的准确对应关系，以指导注意相关区域，例如说话者的脸，手和上半身。这种输入意识到的稀疏注意力减少了冗余计算，并加强了身体部位的时间对应关系，从而提高了推理效率和运动相干性。为了进一步提高视觉质量，我们引入了一种输入感知的蒸馏损失，可改善唇部同步和手运动现实主义。通过整合我们的输入意识到的稀疏注意力和蒸馏损失，与最近的音频驱动和输入驱动的方法相比，我们的方法可以通过提高视觉质量实现实时性能。我们还进行了广泛的实验，显示了算法设计选择的有效性。</li>
</ul>

<h3>Title: TabImpute: Accurate and Fast Zero-Shot Missing-Data Imputation with a Pre-Trained Transformer</h3>
<ul>
<li><strong>Authors: </strong>Jacob Feitelberg, Dwaipayan Saha, Kyuseong Choi, Zaid Ahmad, Anish Agarwal, Raaz Dwivedi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02625">https://arxiv.org/abs/2510.02625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02625">https://arxiv.org/pdf/2510.02625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02625]] TabImpute: Accurate and Fast Zero-Shot Missing-Data Imputation with a Pre-Trained Transformer(https://arxiv.org/abs/2510.02625)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Missing data is a pervasive problem in tabular settings. Existing solutions range from simple averaging to complex generative adversarial networks. However, due to huge variance in performance across real-world domains and time-consuming hyperparameter tuning, no default imputation method exists. Building on TabPFN, a recent tabular foundation model for supervised learning, we propose TabImpute, a pre-trained transformer that delivers accurate and fast zero-shot imputations requiring no fitting or hyperparameter tuning at inference-time. To train and evaluate TabImpute, we introduce (i) an entry-wise featurization for tabular settings, which enables a $100\times$ speedup over the previous TabPFN imputation method, (ii) a synthetic training data generation pipeline incorporating realistic missingness patterns, which boosts test-time performance, and (iii) MissBench, a comprehensive benchmark for evaluation of imputation methods with $42$ OpenML datasets and $13$ missingness patterns. MissBench spans domains such as medicine, finance, and engineering, showcasing TabImpute's robust performance compared to $11$ established imputation methods.</li>
<li><strong>摘要：</strong>缺少数据是表格设置中的普遍问题。现有解决方案范围从简单的平均到复杂的生成对抗网络。但是，由于现实世界中的性能差异很大，并且不存在默认的超参数调整，因此不存在默认的插补方法。在TABPFN的基础上，我们提出了一种预先训练的变压器TABPFN，这是一种用于监督学习的标准基础模型，它提供了准确且快速的零射击归档，在推理时间内不需要拟合或超参数调整。要训​​练和评估tabimpute，我们介绍了（i）为表格设置的入门特征，该功能可以在先前的TABPFN归因方法上进行100美元的加速$速度，（ii）合成培训数据生成管道，结合了现实的失踪性模式，并促进了测试时间和$ 13的bench $ $ bench $ $ bench $ $ $ $的$ 42，$ 42缺失模式。 Missbench跨越医学，金融和工程等领域，展示Tabimptute的出色表现，而$ 11 $已建立的插图方法。</li>
</ul>

<h3>Title: Deep Generative Continual Learning using Functional LoRA: FunLoRA</h3>
<ul>
<li><strong>Authors: </strong>Victor Enescu, Hichem Sahbi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02631">https://arxiv.org/abs/2510.02631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02631">https://arxiv.org/pdf/2510.02631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02631]] Deep Generative Continual Learning using Functional LoRA: FunLoRA(https://arxiv.org/abs/2510.02631)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Continual adaptation of deep generative models holds tremendous potential and critical importance, given their rapid and expanding usage in text and vision based applications. Incremental training, however, remains highly challenging due to catastrophic forgetting phenomenon, which makes it difficult for neural networks to effectively incorporate new knowledge. A common strategy consists in retraining the generative model on its own synthetic data in order to mitigate forgetting. Yet, such an approach faces two major limitations: (i) the continually increasing training time eventually becomes intractable, and (ii) reliance on synthetic data inevitably leads to long-term performance degradation, since synthetic samples lack the richness of real training data. In this paper, we attenuate these issues by designing a novel and more expressive conditioning mechanism for generative models based on low rank adaptation (LoRA), that exclusively employs rank 1 matrices, whose reparametrized matrix rank is functionally increased using carefully selected functions -- and dubbed functional LoRA: FunLoRA. Using this dynamic conditioning, the generative model is guaranteed to avoid catastrophic forgetting and needs only to be trained on data from the current task. Extensive experiments using flow-matching based models trained from scratch, showcase that our proposed parameter-efficient fine-tuning (PEFT) method surpasses prior state-of-the-art results based on diffusion models, reaching higher classification accuracy scores, while only requiring a fraction of the memory cost and sampling time.</li>
<li><strong>摘要：</strong>鉴于它们在基于文本和视觉的应用中的迅速使用和不断扩大的使用情况下，深层生成模型的持续适应具有巨大的潜力和至关重要。然而，由于灾难性的遗忘现象，增量训练仍然具有高度挑战性，这使得神经网络难以有效地合并新知识。一种共同的策略在于在其自身的合成数据上重新培训生成模型以减轻遗忘。然而，这种方法面临两个主要局限性：（i）不断增加的训练时间最终变得棘手，并且（ii）依赖合成数据不可避免地导致长期绩效降低，因为合成样本缺乏真实培训数据的丰富性。在本文中，我们通过设计一种基于低级适应性（Lora）的生成模型的新颖，更具表现力的调理机制来减轻这些问题，该机制仅使用等级1矩阵，其重新配置的矩阵等级在功能上使用精心选择的功能在功能上增加了功能 - 并配音功能性Lora：Funlora：Funlora。使用这种动态调节，可以保证生成模型避免灾难性的遗忘，并且需要对当前任务的数据进行培训。使用基于流量匹配的模型进行了从头开始训练的大量实验，展示了我们提出的参数有效的微调（PEFT）方法超过了基于扩散模型的先前最新结果，达到了更高的分类精度得分，而仅需要一部分存储器成本和采样时间。</li>
</ul>

<h3>Title: Smart-GRPO: Smartly Sampling Noise for Efficient RL of Flow-Matching Models</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Yu, Jackie Liu, Justin Cui</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02654">https://arxiv.org/abs/2510.02654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02654">https://arxiv.org/pdf/2510.02654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02654]] Smart-GRPO: Smartly Sampling Noise for Efficient RL of Flow-Matching Models(https://arxiv.org/abs/2510.02654)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advancements in flow-matching have enabled high-quality text-to-image generation. However, the deterministic nature of flow-matching models makes them poorly suited for reinforcement learning, a key tool for improving image quality and human alignment. Prior work has introduced stochasticity by perturbing latents with random noise, but such perturbations are inefficient and unstable. We propose Smart-GRPO, the first method to optimize noise perturbations for reinforcement learning in flow-matching models. Smart-GRPO employs an iterative search strategy that decodes candidate perturbations, evaluates them with a reward function, and refines the noise distribution toward higher-reward regions. Experiments demonstrate that Smart-GRPO improves both reward optimization and visual quality compared to baseline methods. Our results suggest a practical path toward reinforcement learning in flow-matching frameworks, bridging the gap between efficient training and human-aligned generation.</li>
<li><strong>摘要：</strong>流量匹配的最新进展已使高质量的文本对图像生成。但是，匹配模型的确定性性质使它们适合增强学习，这是改善图像质量和人类对齐的关键工具。先前的工作通过与随机噪声扰动潜在的潜在潜在的潜在性，但是这种扰动效率低下且不稳定。我们提出了Smart-Grpo，这是优化流动匹配模型增强噪声扰动的第一种方法。 Smart-Grpo采用了一种迭代搜索策略，该策略可以解码候选人的扰动，通过奖励功能对其进行评估，并将噪声分布提高到更高奖励区域。实验表明，与基线方法相比，Smart-GRPO可以改善奖励优化和视觉质量。我们的结果表明，在流动匹配框架中进行加固学习的实用途径，弥合了有效的训练和人类对准的一代之间的差距。</li>
</ul>

<h3>Title: TutorBench: A Benchmark To Assess Tutoring Capabilities Of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Rakshith S Srinivasa, Zora Che, Chen Bo Calvin Zhang, Diego Mares, Ernesto Hernandez, Jayeon Park, Dean Lee, Guillermo Mangialardi, Charmaine Ng, Ed-Yeremai Hernandez Cardona, Anisha Gunjal, Yunzhong He, Bing Liu, Chen Xing</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02663">https://arxiv.org/abs/2510.02663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02663">https://arxiv.org/pdf/2510.02663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02663]] TutorBench: A Benchmark To Assess Tutoring Capabilities Of Large Language Models(https://arxiv.org/abs/2510.02663)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>As students increasingly adopt large language models (LLMs) as learning aids, it is crucial to build models that are adept at handling the nuances of tutoring: they need to identify the core needs of students, be adaptive, provide personalized guidance, and be accurate. To this end, we introduce TutorBench, a dataset and evaluation benchmark designed to rigorously evaluate the core tutoring skills of LLMs. The dataset comprises 1,490 samples curated by human experts, focused on high-school and AP-level curricula. The samples are drawn from three common tutoring tasks: (i) generating adaptive explanations tailored to a student's confusion, (ii) providing actionable feedback on a student's work, and (iii) promoting active learning through effective hint generation. To account for the inherent complexity of tutoring, samples are accompanied by sample-specific rubrics which are used to judge model responses during evaluation. TutorBench uses a reliable and fine-grained automatic evaluation method that uses an LLM-judge and the sample-specific rubrics. We evaluate 16 frontier LLMs on TutorBench and present a detailed analysis of their performance and behavior. Our results show that none of the frontier LLMs achieve a score of greater than $56\%$, showing a large room for improvement. We find that LLMs fall short in exhibiting the full range of tutoring skills needed to guide, diagnose, and support students effectively, with all the frontier models achieving less than a $60\%$ pass rate on rubric criteria related to these skills. We also find that different model families exhibit varied strengths and limitations: the Claude models outperform others in supporting active learning, while they lag behind in the other two use cases. By releasing TutorBench, we provide a comprehensive and unsaturated benchmark to guide the development of the next-generation of AI tutors.</li>
<li><strong>摘要：</strong>随着学生越来越多地采用大型语言模型（LLM）作为学习辅助工具，建立擅长处理辅导细微差别的模型至关重要：他们需要确定学生的核心需求，自适应，提供个性化的指导并准确。为此，我们介绍了TutorBench，这是一个数据集和评估基准测试，旨在严格评估LLM的核心辅导技能。该数据集由人类专家策划的1,490个样本，专注于高中和AP级课程。样本是从三个常见的辅导任务中得出的：（i）生成针对学生混乱的自适应解释，（ii）对学生的工作提供可行的反馈，以及（iii）通过有效的提示产生来促进积极的学习。为了说明辅导的固有复杂性，样本伴随着样本特异性的专栏，这些标题用于评估过程中用于判断模型响应。 TutorBench使用一种可靠且细粒度的自动评估方法，该方法使用LLM法官和样本特​​定的专栏。我们评估了16个Frontier LLM在辅导台上，并对其性能和行为进行了详细的分析。我们的结果表明，没有一个边境LLMS的分数超过$ 56 \％$，这显示了一个巨大的改进空间。我们发现，LLMS在有效指导，诊断和支持学生所需的全部辅导技能方面缺乏，所有边境模型都在与这些技能相关的标准上达到的$ 60 \％$ $通过率。我们还发现，不同的模型家族具有多样化的优势和局限性：克劳德模型在支持主动学习方面的表现优于其他人，而在其他两个用例中却落后。通过释放辅导员，我们提供了一个全面且不饱和的基准，以指导下一代AI导师的发展。</li>
</ul>

<h3>Title: To Compress or Not? Pushing the Frontier of Lossless GenAI Model Weights Compression with Exponent Concentration</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Yang, Tianyi Zhang, Jianwen Xie, Chuan Li, Zhaozhuo Xu, Anshumali Shrivastava</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02676">https://arxiv.org/abs/2510.02676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02676">https://arxiv.org/pdf/2510.02676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02676]] To Compress or Not? Pushing the Frontier of Lossless GenAI Model Weights Compression with Exponent Concentration(https://arxiv.org/abs/2510.02676)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The scaling of Generative AI (GenAI) models into the hundreds of billions of parameters makes low-precision computation indispensable for efficient deployment. We argue that the fundamental solution lies in developing low-precision floating-point formats, which inherently provide numerical stability, memory savings, and hardware efficiency without dequantization overhead. In this paper, we present a theoretical and empirical study of an exponent concentration phenomenon in GenAI weights: exponents consistently exhibit low entropy across architectures and modalities. We show that this arises naturally from $\alpha$-stable distributions induced by stochastic gradient descent, and we prove tight bounds on the entropy of exponents. Our analysis establishes a theoretical compression limit near FP4.67, which motivates the design of a practical FP8 format. Building on these insights, we propose Exponent-Concentrated FP8 (ECF8), a lossless compression framework with entropy-aware encoding and GPU-optimized decoding. Experiments on LLMs and DiTs up to 671B parameters demonstrate up to 26.9% memory savings and 177.1% throughput acceleration, with perfectly lossless computations, i.e., no deviation in model outputs. Our results establish exponent concentration as a statistical law of trained models and open a principled path for lossless low-precision floating-point design in the FP8 era.</li>
<li><strong>摘要：</strong>生成型AI（Genai）模型的缩放范围在数百十亿个参数中，因此对于有效部署而言，低精度计算是必不可少的。我们认为，基本解决方案在于开发低精确的浮点格式，该格式固有地提供了数值稳定性，存储器节省和硬件效率而没有悬空。在本文中，我们介绍了对Genai重量中指数浓度现象的理论和经验研究：指数始终在跨体系结构和方式上表现出较低的熵。我们表明，这自然来自由随机梯度下降引起的$ \ alpha $稳定分布，并且我们证明了指数熵的紧密界限。我们的分析建立了在FP4.67附近的理论压缩极限，这激发了实用FP8格式的设计。在这些见解的基础上，我们提出了指数浓缩的FP8（ECF8），这是一个无损压缩框架，具有熵意识到的编码和GPU优化的解码。在LLM上进行的实验并最多可达671B参数，最多可节省26.9％的存储器和177.1％的吞吐量加速度，并具有完美的无损计算，即模型输出无偏差。我们的结果将指数集中度确定为训练有素的模型的统计定律，并为FP8时代的无损低精度浮点设计打开了原则性的途径。</li>
</ul>

<h3>Title: EvoSpeak: Large Language Models for Interpretable Genetic Programming-Evolved Heuristics</h3>
<ul>
<li><strong>Authors: </strong>Meng Xu, Jiao Liu, Yew Soon Ong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02686">https://arxiv.org/abs/2510.02686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02686">https://arxiv.org/pdf/2510.02686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02686]] EvoSpeak: Large Language Models for Interpretable Genetic Programming-Evolved Heuristics(https://arxiv.org/abs/2510.02686)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Genetic programming (GP) has demonstrated strong effectiveness in evolving tree-structured heuristics for complex optimization problems. Yet, in dynamic and large-scale scenarios, the most effective heuristics are often highly complex, hindering interpretability, slowing convergence, and limiting transferability across tasks. To address these challenges, we present EvoSpeak, a novel framework that integrates GP with large language models (LLMs) to enhance the efficiency, transparency, and adaptability of heuristic evolution. EvoSpeak learns from high-quality GP heuristics, extracts knowledge, and leverages this knowledge to (i) generate warm-start populations that accelerate convergence, (ii) translate opaque GP trees into concise natural-language explanations that foster interpretability and trust, and (iii) enable knowledge transfer and preference-aware heuristic generation across related tasks. We verify the effectiveness of EvoSpeak through extensive experiments on dynamic flexible job shop scheduling (DFJSS), under both single- and multi-objective formulations. The results demonstrate that EvoSpeak produces more effective heuristics, improves evolutionary efficiency, and delivers human-readable reports that enhance usability. By coupling the symbolic reasoning power of GP with the interpretative and generative strengths of LLMs, EvoSpeak advances the development of intelligent, transparent, and user-aligned heuristics for real-world optimization problems.</li>
<li><strong>摘要：</strong>遗传编程（GP）在不断发展的树结构启发式方法方面表现出强大的有效性，以解决复杂的优化问题。然而，在动态和大规模的情况下，最有效的启发式方法通常是高度复杂的，阻碍了可解释性，减慢收敛性以及跨任务的可转移性。为了应对这些挑战，我们提出了EvoSeak，这是一个新颖的框架，将GP与大语言模型（LLMS）集成在一起，以提高启发式演化的效率，透明度和适应性。 EvoSeak从高质量的GP启发式方法中学习，提取知识，并利用这些知识为（i）产生温暖的启动人群，以加速融合，（ii）将不透明的GP树转化为简洁的自然语言解释，以促进可解释性和可信度，以及（iii）跨相关任务跨越相关任务的知识传递和preferfection-aware aware aware aware aware aware oferist。我们通过在单目标和多目标配方下对动态灵活的车间调度（DFJSS）进行广泛的实验来验证EvoSeak的有效性。结果表明，EvoSeak会产生更有效的启发式方法，提高进化效率，并提供可读取的报告，以提高可用性。通过将GP的象征性推理能力与LLMS的解释性和生成性优势结合在一起，EvoSeak促进了智能，透明和用户一致的启发式方法的发展，以解决现实世界中的优化问题。</li>
</ul>

<h3>Title: Fine-Tuning Diffusion Models via Intermediate Distribution Shaping</h3>
<ul>
<li><strong>Authors: </strong>Gautham Govind Anil, Shaan Ul Haque, Nithish Kannen, Dheeraj Nagaraj, Sanjay Shakkottai, Karthikeyan Shanmugam</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02692">https://arxiv.org/abs/2510.02692</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02692">https://arxiv.org/pdf/2510.02692</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02692]] Fine-Tuning Diffusion Models via Intermediate Distribution Shaping(https://arxiv.org/abs/2510.02692)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models are widely used for generative tasks across domains. While pre-trained diffusion models effectively capture the training data distribution, it is often desirable to shape these distributions using reward functions to align with downstream applications. Policy gradient methods, such as Proximal Policy Optimization (PPO), are widely used in the context of autoregressive generation. However, the marginal likelihoods required for such methods are intractable for diffusion models, leading to alternative proposals and relaxations. In this context, we unify variants of Rejection sAmpling based Fine-Tuning (RAFT) as GRAFT, and show that this implicitly performs PPO with reshaped rewards. We then introduce P-GRAFT to shape distributions at intermediate noise levels and demonstrate empirically that this can lead to more effective fine-tuning. We mathematically explain this via a bias-variance tradeoff. Motivated by this, we propose inverse noise correction to improve flow models without leveraging explicit rewards. We empirically evaluate our methods on text-to-image(T2I) generation, layout generation, molecule generation and unconditional image generation. Notably, our framework, applied to Stable Diffusion 2, improves over policy gradient methods on popular T2I benchmarks in terms of VQAScore and shows an $8.81\%$ relative improvement over the base model. For unconditional image generation, inverse noise correction improves FID of generated images at lower FLOPs/image.</li>
<li><strong>摘要：</strong>扩散模型被广泛用于跨域的生成任务。虽然预训练的扩散模型有效地捕获了训练数据分布，但通常希望使用奖励功能来塑造这些分布以与下游应用程序保持一致。策略梯度方法（例如近端政策优化（PPO））在自回归生成的背景下广泛使用。但是，这种方法所需的边际可能性对于扩散模型很难，导致了替代性建议和放松。在这种情况下，我们将基于拒绝抽样的微调（RAFT）的变体统一为移植物，并表明这隐含地通过重塑奖励执行PPO。然后，我们引入p-graft以在中间噪声水平上塑造分布，并从经验上证明这可以导致更有效的微调。我们通过数学上的偏见差异来解释这一点。在此激励的情况下，我们提出了反向噪声校正，以改善流量模型，而不会利用明确的奖励。我们从经验上评估了有关文本对图像（T2I）生成，布局产生，分子生成和无条件图像生成的方法。值得注意的是，我们适用于稳定扩散2的框架在VQASCORE方面改善了对流行T2I基准测试的策略梯度方法，并显示出$ 8.81 \％$ $相对改进的基础模型。对于无条件的图像产生，逆噪声校正改善了下拖上掉落/图像处生成的图像的FID。</li>
</ul>

<h3>Title: RAMAC: Multimodal Risk-Aware Offline Reinforcement Learning and the Role of Behavior Regularization</h3>
<ul>
<li><strong>Authors: </strong>Kai Fukazawa, Kunal Mundada, Iman Soltani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02695">https://arxiv.org/abs/2510.02695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02695">https://arxiv.org/pdf/2510.02695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02695]] RAMAC: Multimodal Risk-Aware Offline Reinforcement Learning and the Role of Behavior Regularization(https://arxiv.org/abs/2510.02695)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In safety-critical domains where online data collection is infeasible, offline reinforcement learning (RL) offers an attractive alternative but only if policies deliver high returns without incurring catastrophic lower-tail risk. Prior work on risk-averse offline RL achieves safety at the cost of value conservatism and restricted policy classes, whereas expressive policies are only used in risk-neutral settings. Here, we address this gap by introducing the \textbf{Risk-Aware Multimodal Actor-Critic (RAMAC)} framework, which couples an \emph{expressive generative actor} with a distributional critic. The RAMAC differentiates composite objective combining distributional risk and BC loss through the generative path, achieving risk-sensitive learning in complex multimodal scenarios. We instantiate RAMAC with diffusion and flow-matching actors and observe consistent gains in $\mathrm{CVaR}_{0.1}$ while maintaining strong returns on most Stochastic-D4RL tasks. Code: this https URL</li>
<li><strong>摘要：</strong>在在线数据收集不可行的安全 - 关键领域中，离线增强学习（RL）提供了一个有吸引力的替代方案，但前提是政策在不产生灾难性的下尾风险的情况下提供高回报。先前的规避风险离线工作RL以价值保守主义和受限制的政策类别实现安全性，而表达政策仅在风险中性环境中使用。在这里，我们通过介绍\ textbf {风险意识的多模式参与者 - 批评（RAMAC）}框架来解决这一差距，该框架将\ emph {emph {emph {emph {empherative osterative contor}与分布评论家融合在一起。 RAMAC通过生成路径区分了结合分布风险和BC损失的复合目标，在复杂的多模式方案中实现了对风险敏感的学习。我们使用扩散和流程匹配的参与者实例化RAMAC，并在$ \ Mathrm {Cvar} _ {0.1} $中观察到一致的收益，同时在大多数随机D4RL任务上保持强劲回报。代码：此HTTPS URL</li>
</ul>

<h3>Title: MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context</h3>
<ul>
<li><strong>Authors: </strong>Junyu Shi, Yong Sun, Zhiyuan Zhang, Lijiang Liu, Zhengjie Zhang, Yuxin He, Qiang Nie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02722">https://arxiv.org/abs/2510.02722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02722">https://arxiv.org/pdf/2510.02722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02722]] MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context(https://arxiv.org/abs/2510.02722)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Existing text-driven motion generation methods often treat synthesis as a bidirectional mapping between language and motion, but remain limited in capturing the causal logic of action execution and the human intentions that drive behavior. The absence of visual grounding further restricts precision and personalization, as language alone cannot specify fine-grained spatiotemporal details. We propose MoGIC, a unified framework that integrates intention modeling and visual priors into multimodal motion synthesis. By jointly optimizing multimodal-conditioned motion generation and intention prediction, MoGIC uncovers latent human goals, leverages visual priors to enhance generation, and exhibits versatile multimodal generative capability. We further introduce a mixture-of-attention mechanism with adaptive scope to enable effective local alignment between conditional tokens and motion subsequences. To support this paradigm, we curate Mo440H, a 440-hour benchmark from 21 high-quality motion datasets. Experiments show that after finetuning, MoGIC reduces FID by 38.6\% on HumanML3D and 34.6\% on Mo440H, surpasses LLM-based methods in motion captioning with a lightweight text head, and further enables intention prediction and vision-conditioned generation, advancing controllable motion synthesis and intention understanding. The code is available at this https URL</li>
<li><strong>摘要：</strong>现有的文本驱动运动生成方法通常将综合视为语言和运动之间的双向映射，但在捕获行动执行的因果逻辑和驱动行为的人类意图方面仍然有限。缺乏视觉接地进一步限制了精度和个性化，因为仅语言无法指定细粒度的时空细节。我们提出了Mogic，这是一个统一的框架，将意图建模和视觉先验整合到多模式运动合成中。通过共同优化多模式条件的运动产生和意图预测，Mogic揭示了潜在的人类目标，利用视觉先验来增强产生，并表现出多功能的多模式生成能力。我们进一步引入了具有自适应范围的注意力混合物，以实现有条件令牌和运动子序列之间的有效局部比分。为了支持此范式，我们策划了21个高质量运动数据集的MO440H，这是一个440小时的基准测试。实验表明，在填充后，Mogic在HumanML3D上将FID降低了38.6 \％，MO4440H的FID降低了34.6 \％，超过了基于LLM的方法，具有轻巧的文本字幕，并进一步启用了意图预测和远见的预测，并具有远见性的预测，可控制的动作综合和意图理解。该代码可在此HTTPS URL上找到</li>
</ul>

<h3>Title: Dale meets Langevin: A Multiplicative Denoising Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Nishanth Shetty, Madhava Prasath, Chandra Sekhar Seelamantula</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02730">https://arxiv.org/abs/2510.02730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02730">https://arxiv.org/pdf/2510.02730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02730]] Dale meets Langevin: A Multiplicative Denoising Diffusion Model(https://arxiv.org/abs/2510.02730)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Gradient descent has proven to be a powerful and effective technique for optimization in numerous machine learning applications. Recent advances in computational neuroscience have shown that learning in standard gradient descent optimization formulation is not consistent with learning in biological systems. This has opened up interesting avenues for building biologically inspired learning techniques. One such approach is inspired by Dale's law, which states that inhibitory and excitatory synapses do not swap roles during the course of learning. The resulting exponential gradient descent optimization scheme leads to log-normally distributed synaptic weights. Interestingly, the density that satisfies the Fokker-Planck equation corresponding to the stochastic differential equation (SDE) with geometric Brownian motion (GBM) is the log-normal density. Leveraging this connection, we start with the SDE governing geometric Brownian motion, and show that discretizing the corresponding reverse-time SDE yields a multiplicative update rule, which surprisingly, coincides with the sampling equivalent of the exponential gradient descent update founded on Dale's law. Furthermore, we propose a new formalism for multiplicative denoising score-matching, subsuming the loss function proposed by Hyvaerinen for non-negative data. Indeed, log-normally distributed data is positive and the proposed score-matching formalism turns out to be a natural fit. This allows for training of score-based models for image data and results in a novel multiplicative update scheme for sample generation starting from a log-normal density. Experimental results on MNIST, Fashion MNIST, and Kuzushiji datasets demonstrate generative capability of the new scheme. To the best of our knowledge, this is the first instance of a biologically inspired generative model employing multiplicative updates, founded on geometric Brownian motion.</li>
<li><strong>摘要：</strong>事实证明，梯度下降是在众多机器学习应用中优化的强大有效技术。计算神经科学的最新进展表明，在标准梯度下降优化公式中学习与生物系统中的学习不一致。这为建立生物学启发的学习技术开辟了有趣的途径。一种这样的方法是受戴尔定律启发的，该法律指出，在学习过程中，抑制性和兴奋性突触不会交换角色。产生的指数梯度下降优化方案导致对数正态分布的突触权重。有趣的是，满足与几何布朗运动（GBM）随机微分方程（SDE）相对应的Fokker-Planck方程的密度是对数正态量密度。利用这种连接，我们从执行几何布朗尼运动的SDE开始，并表明将相应的倒车SDE离散产生了乘法更新规则，令人惊讶的是，该规则与dale律法上建立的指数梯度下降更新的采样相吻合。此外，我们提出了一种新的形式主义，用于乘法deNo的得分匹配，从而包含Hyvaerinen提出的非阴性数据提出的损失函数。实际上，对数正态分布的数据是正态分布，而拟议的得分匹配形式主义原来是一种自然的拟合。这允许训练基于分数的模型以获取图像数据，并导致一种新型的乘法更新方案，用于从对数正态密度开始的样本生成。 MNIST，时尚MNIST和Kuzushiji数据集的实验结果证明了新方案的生成能力。据我们所知，这是使用基于几何布朗运动的乘法更新的生物学启发的生成模型的第一个实例。</li>
</ul>

<h3>Title: TokenFlow: Responsive LLM Text Streaming Serving under Request Burst via Preemptive Scheduling</h3>
<ul>
<li><strong>Authors: </strong>Junyi Chen, Chuheng Du, Renyuan Liu, Shuochao Yao, Dingtian Yan, Jiang Liao, Shengzhong Liu, Fan Wu, Guihai Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02758">https://arxiv.org/abs/2510.02758</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02758">https://arxiv.org/pdf/2510.02758</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02758]] TokenFlow: Responsive LLM Text Streaming Serving under Request Burst via Preemptive Scheduling(https://arxiv.org/abs/2510.02758)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Real-time LLM interactions demand streamed token generations, where text tokens are progressively generated and delivered to users while balancing two objectives: responsiveness (i.e., low time-to-first-token) and steady generation (i.e.,required time-between-tokens). Standard LLM serving systems suffer from the inflexibility caused by non-preemptive request scheduling and reactive memory management, leading to poor resource utilization and low request processing parallelism under request bursts. Therefore, we present TokenFlow, a novel LLM serving system with enhanced text streaming performance via preemptive request scheduling and proactive key-value (KV) cache management. TokenFlow dynamically prioritizes requests based on real-time token buffer occupancy and token consumption rate, while actively transferring KV cache between GPU and CPU memory in the background and overlapping I/O with computation to minimize request preemption overhead. Extensive experiments on Llama3-8B and Qwen2.5-32B across multiple GPUs (RTX 4090, A6000, H200) demonstrate that TokenFlow achieves up to 82.5% higher effective throughput (accounting for actual user consumption) while reducing P99 TTFT by up to 80.2%, without degrading overall token throughput.</li>
<li><strong>摘要：</strong>实时LLM交互要求流式令牌世代，其中文本令牌是逐步生成并传递给用户的，同时平衡了两个目标：响应能力（即低时间的时间）和稳定的生成（即所需的时间量表）。标准的LLM服务系统遭受了由非抢先要求的调度和反应性内存管理引起的僵化性，导致资源利用率差和要求较低的请求处理并行在请求中。因此，我们提出了TokenFlow，这是一种新颖的LLM服务系统，其通过先发制的请求调度和主动的键值（KV）缓存管理增强了文本流效果。 TokenFlow基于实时令牌缓冲区的占用率和令牌消耗率，将请求动态优先级，同时在后台在GPU和CPU内存之间主动传输KV缓存，并与I/O重叠I/O并与计算重叠以最大程度地减少请求的抢先设置。在多个GPU（RTX 4090，A6000，H200）上进行的Llama3-8B和Qwen2.5-32B进行了广泛的实验，这表明，TokenFlow实现了有效的吞吐量高达82.5％（核算实际用户消费），而将P99 TTFT降低了80.2％，而无需降低80.2％，而无需降低80.2％，而无需降低整个vava Gressput。</li>
</ul>

<h3>Title: Med-K2N: Flexible K-to-N Modality Translation for Medical Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Feng Yuan, Yifan Gao, Yuehua Ye, Haoyue Li, Xin Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02815">https://arxiv.org/abs/2510.02815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02815">https://arxiv.org/pdf/2510.02815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02815]] Med-K2N: Flexible K-to-N Modality Translation for Medical Image Synthesis(https://arxiv.org/abs/2510.02815)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Cross-modal medical image synthesis research focuses on reconstructing missing imaging modalities from available ones to support clinical diagnosis. Driven by clinical necessities for flexible modality reconstruction, we explore K to N medical generation, where three critical challenges emerge: How can we model the heterogeneous contributions of different modalities to various target tasks? How can we ensure fusion quality control to prevent degradation from noisy information? How can we maintain modality identity consistency in multi-output generation? Driven by these clinical necessities, and drawing inspiration from SAM2's sequential frame paradigm and clinicians' progressive workflow of incrementally adding and selectively integrating multi-modal information, we treat multi-modal medical data as sequential frames with quality-driven selection mechanisms. Our key idea is to "learn" adaptive weights for each modality-task pair and "memorize" beneficial fusion patterns through progressive enhancement. To achieve this, we design three collaborative modules: PreWeightNet for global contribution assessment, ThresholdNet for adaptive filtering, and EffiWeightNet for effective weight computation. Meanwhile, to maintain modality identity consistency, we propose the Causal Modality Identity Module (CMIM) that establishes causal constraints between generated images and target modality descriptions using vision-language modeling. Extensive experimental results demonstrate that our proposed Med-K2N outperforms state-of-the-art methods by significant margins on multiple benchmarks. Source code is available.</li>
<li><strong>摘要：</strong>跨模式医学图像合成研究的重点是重建可用的成像方式，以支持临床诊断。在灵活方式重建的临床必需品的驱动下，我们探索了K到N医学的K，在这里出现了三个关键挑战：我们如何对不同方式对各种目标任务的异质贡献进行建模？我们如何确保融合质量控制以防止嘈杂信息降解？我们如何在多输出生成中保持模态身份的一致性？在这些临床必需品的驱动下，并从SAM2的顺序框架范式和临床医生的逐步工作流程中汲取灵感，以逐步添加和选择性地整合多模式信息，我们将多模式医学数据视为顺序框架，并将其视为质量驱动的选择机制。我们的关键想法是为每个模态任务对“学习”自适应权重，并通过逐步增强来“记住”有益的融合模式。为了实现这一目标，我们设计了三个协作模块：用于全球贡献评估的preweightnet，用于自适应滤波的阈值以及用于有效重量计算的Effiweeightnet。同时，为了维持模态认同一致性，我们提出了因果模态认同模块（CMIM），该模块（CMIM）使用视觉语言建模建立了生成的图像和目标模态描述之间的因果约束。广泛的实验结果表明，我们提出的MED-K2N在多个基准上的大幅度优于最先进的方法。源代码可用。</li>
</ul>

<h3>Title: Multi-scale Autoregressive Models are Laplacian, Discrete, and Latent Diffusion Models in Disguise</h3>
<ul>
<li><strong>Authors: </strong>Steve Hong, Samuel Belkadi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02826">https://arxiv.org/abs/2510.02826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02826">https://arxiv.org/pdf/2510.02826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02826]] Multi-scale Autoregressive Models are Laplacian, Discrete, and Latent Diffusion Models in Disguise(https://arxiv.org/abs/2510.02826)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We revisit Visual Autoregressive (VAR) models through the lens of an iterative-refinement framework. Rather than viewing VAR solely as next-scale autoregression, we formalise it as a deterministic forward process that constructs a Laplacian-style latent pyramid, paired with a learned backward process that reconstructs it in a small number of coarse-to-fine steps. This view connects VAR to denoising diffusion and isolates three design choices that help explain its efficiency and fidelity: refining in a learned latent space, casting prediction as discrete classification over code indices, and partitioning the task by spatial frequency. We run controlled experiments to quantify each factor's contribution to fidelity and speed, and we outline how the same framework extends to permutation-invariant graph generation and to probabilistic, ensemble-style medium-range weather forecasting. The framework also suggests practical interfaces for VAR to leverage tools from the diffusion ecosystem while retaining few-step, scale-parallel generation.</li>
<li><strong>摘要：</strong>我们通过迭代式框架的镜头重新访问视觉自回旋（VAR）模型。我们将其形式化为一个确定性的前向过程，而不是仅将VAR视为临时自动化，而是将其构建Laplacian风格的潜在金字塔构造，并与学习的后退过程配对，以少量的粗到1个步骤重建它。该视图将VAR连接到降级扩散，并隔离了三个设计选择，这些选择有助于解释其效率和忠诚度：在学习的潜在空间中提炼，将预测作为对代码指数的离散分类，并按空间频率对任务进行分区。我们运行受控的实验，以量化每个因素对忠诚度和速度的贡献，并概述了同一框架如何扩展到置换不变的图形生成以及概率，集合式中型天气预测。该框架还提出了VAR的实用接口，以利用扩散生态系统中的工具，同时保留较少的步骤平行生成。</li>
</ul>

<h3>Title: Knowledge-Aware Modeling with Frequency Adaptive Learning for Battery Health Prognostics</h3>
<ul>
<li><strong>Authors: </strong>Vijay Babu Pamshetti, Wei Zhang, Sumei Sun, Jie Zhang, Yonggang Wen, Qingyu Yan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02839">https://arxiv.org/abs/2510.02839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02839">https://arxiv.org/pdf/2510.02839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02839]] Knowledge-Aware Modeling with Frequency Adaptive Learning for Battery Health Prognostics(https://arxiv.org/abs/2510.02839)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Battery health prognostics are critical for ensuring safety, efficiency, and sustainability in modern energy systems. However, it has been challenging to achieve accurate and robust prognostics due to complex battery degradation behaviors with nonlinearity, noise, capacity regeneration, etc. Existing data-driven models capture temporal degradation features but often lack knowledge guidance, which leads to unreliable long-term health prognostics. To overcome these limitations, we propose Karma, a knowledge-aware model with frequency-adaptive learning for battery capacity estimation and remaining useful life prediction. The model first performs signal decomposition to derive battery signals in different frequency bands. A dual-stream deep learning architecture is developed, where one stream captures long-term low-frequency degradation trends and the other models high-frequency short-term dynamics. Karma regulates the prognostics with knowledge, where battery degradation is modeled as a double exponential function based on empirical studies. Our dual-stream model is used to optimize the parameters of the knowledge with particle filters to ensure physically consistent and reliable prognostics and uncertainty quantification. Experimental study demonstrates Karma's superior performance, achieving average error reductions of 50.6% and 32.6% over state-of-the-art algorithms for battery health prediction on two mainstream datasets, respectively. These results highlight Karma's robustness, generalizability, and potential for safer and more reliable battery management across diverse applications.</li>
<li><strong>摘要：</strong>电池健康预后对确保现代能源系统的安全性，效率和可持续性至关重要。但是，由于具有非线性，噪声，容量再生等的复杂电池降解行为，实现准确，可靠的预后。现有数据驱动的模型捕获了时间降解功能，但通常缺乏知识指导，这导致了不可靠的长期健康预言。为了克服这些局限性，我们提出了Karma，这是一种具有频率自适应学习的知识吸引模型，以估算电池容量和剩余的使用寿命预测。该模型首先执行信号分解以在不同频带中得出电池信号。开发了双流深度学习体系结构，其中一个流捕获了长期的低频退化趋势和其他模型高频短期动态。业力通过知识调节预后，其中电池降解是基于经验研究的双重指数函数建模的。我们的双流模型用于使用粒子过滤器优化知识的参数，以确保身体一致，可靠的预后和不确定性定量。实验研究表明，业力的出色表现，比最先进的算法分别达到了两个主流数据集的电池健康预测的平均误差降低50.6％和32.6％。这些结果突出了业力的鲁棒性，可推广性以及在各种应用程序中更安全，更可靠的电池管理的潜力。</li>
</ul>

<h3>Title: ELMF4EggQ: Ensemble Learning with Multimodal Feature Fusion for Non-Destructive Egg Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Md Zahim Hassan, Md. Osama, Muhammad Ashad Kabir, Md. Saiful Islam, Zannatul Naim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02876">https://arxiv.org/abs/2510.02876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02876">https://arxiv.org/pdf/2510.02876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02876]] ELMF4EggQ: Ensemble Learning with Multimodal Feature Fusion for Non-Destructive Egg Quality Assessment(https://arxiv.org/abs/2510.02876)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Accurate, non-destructive assessment of egg quality is critical for ensuring food safety, maintaining product standards, and operational efficiency in commercial poultry production. This paper introduces ELMF4EggQ, an ensemble learning framework that employs multimodal feature fusion to classify egg grade and freshness using only external attributes - image, shape, and weight. A novel, publicly available dataset of 186 brown-shelled eggs was constructed, with egg grade and freshness levels determined through laboratory-based expert assessments involving internal quality measurements, such as yolk index and Haugh unit. To the best of our knowledge, this is the first study to apply machine learning methods for internal egg quality assessment using only external, non-invasive features, and the first to release a corresponding labeled dataset. The proposed framework integrates deep features extracted from external egg images with structural characteristics such as egg shape and weight, enabling a comprehensive representation of each egg. Image feature extraction is performed using top-performing pre-trained CNN models (ResNet152, DenseNet169, and ResNet152V2), followed by PCA-based dimensionality reduction, SMOTE augmentation, and classification using multiple machine learning algorithms. An ensemble voting mechanism combines predictions from the best-performing classifiers to enhance overall accuracy. Experimental results demonstrate that the multimodal approach significantly outperforms image-only and tabular (shape and weight) only baselines, with the multimodal ensemble approach achieving 86.57% accuracy in grade classification and 70.83% in freshness prediction. All code and data are publicly available at this https URL, promoting transparency, reproducibility, and further research in this domain.</li>
<li><strong>摘要：</strong>对鸡蛋质量的准确，无损评估对于确保食品安全，维持产品标准和商业家禽生产的运营效率至关重要。本文介绍了ELMF4EGGQ，这是一个合奏学习框架，该框架采用多模式融合来仅使用外部属性 - 图像，形状和重量来对鸡蛋等级和新鲜度进行分类。建造了一个新型的，公开可用的数据集，其中包括186个棕色卵子，其卵级和新鲜度通过涉及内部质量测量的基于实验室的专家评估确定，例如蛋黄指数和Haugh单元。据我们所知，这是仅使用外部，非侵入性特征应用机器学习方法进行内部鸡蛋质量评估的研究，也是第一个发布相应标记的数据集的研究。所提出的框架整合了从外部鸡蛋图像中提取的具有结构特征（例如鸡蛋形状和重量）的深度特征，从而使每个鸡蛋都具有全面的代表。使用最佳训练的CNN模型（RESNET152，DENSENET169和RESNET152V2）进行图像特征提取，然后使用多个机器学习算法进行基于PCA的尺寸降低，SMOTE增强和分类。合奏投票机制结合了表现最佳分类器的预测，以提高整体准确性。实验结果表明，多模式方法仅优于仅图像和表格（形状和重量）基准，而多模式集合方法的坡度分类的精度为86.57％，新鲜度预测的精度为70.83％。所有代码和数据均在此HTTPS URL上公开可用，可促进该领域的透明度，可重复性和进一步的研究。</li>
</ul>

<h3>Title: One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework</h3>
<ul>
<li><strong>Authors: </strong>Lorenzo Bianchi, Giacomo Pacini, Fabio Carrara, Nicola Messina, Giuseppe Amato, Fabrizio Falchi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02898">https://arxiv.org/abs/2510.02898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02898">https://arxiv.org/pdf/2510.02898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02898]] One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework(https://arxiv.org/abs/2510.02898)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Zero-shot captioners are recently proposed models that utilize common-space vision-language representations to caption images without relying on paired image-text data. To caption an image, they proceed by textually decoding a text-aligned image feature, but they limit their scope to global representations and whole-image captions. We present \frameworkName{}, a unified framework for zero-shot captioning that shifts from an image-centric to a patch-centric paradigm, enabling the captioning of arbitrary regions without the need of region-level supervision. Instead of relying on global image representations, we treat individual patches as atomic captioning units and aggregate them to describe arbitrary regions, from single patches to non-contiguous areas and entire images. We analyze the key ingredients that enable current latent captioners to work in our novel proposed framework. Experiments demonstrate that backbones producing meaningful, dense visual features, such as DINO, are key to achieving state-of-the-art performance in multiple region-based captioning tasks. Compared to other baselines and state-of-the-art competitors, our models achieve better performance on zero-shot dense, region-set, and a newly introduced trace captioning task, highlighting the effectiveness of patch-wise semantic representations for scalable caption generation. Project page at this https URL .</li>
<li><strong>摘要：</strong>最近提出的零射门字幕符是将通用空间视觉语言表示形式用于字幕图像的情况，而无需依赖配对的图像文本数据。要为图像字幕，它们通过文本解码文本一致的图像功能进行进行，但它们将其范围限制为全局表示形式和整个图像字幕。我们提出\ Frameworkname {}，这是一个统一的零击字幕框架，从以图像为中心转移到以补丁为中心的范式，启用了不需要区域级别监督的任意区域的字幕。我们不依靠全局图像表示，而是将各个贴片视为原子字幕单元，并将其汇总以描述从单个斑块到非连续区域和整个图像的任意区域。我们分析了使当前的潜在标题能够在我们的新型拟议框架中工作的关键成分。实验表明，产生有意义的，密集的视觉特征（例如Dino）的骨干是在多个基于区域的字幕任务中实现最新性能的关键。与其他基线和最先进的竞争对手相比，我们的模型在零拍，区域集和新引入的跟踪字幕任务上获得更好的性能，从而突出了贴片语义表示对可扩展字幕产生的有效性。此HTTPS URL的项目页面。</li>
</ul>

<h3>Title: DMark: Order-Agnostic Watermarking for Diffusion Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Linyu Wu, Linhao Zhong, Wenjie Qu, Yuexin Li, Yue Liu, Shengfang Zhai, Chunhua Shen, Jiaheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02902">https://arxiv.org/abs/2510.02902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02902">https://arxiv.org/pdf/2510.02902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02902]] DMark: Order-Agnostic Watermarking for Diffusion Large Language Models(https://arxiv.org/abs/2510.02902)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion large language models (dLLMs) offer faster generation than autoregressive models while maintaining comparable quality, but existing watermarking methods fail on them due to their non-sequential decoding. Unlike autoregressive models that generate tokens left-to-right, dLLMs can finalize tokens in arbitrary order, breaking the causal design underlying traditional watermarks. We present DMark, the first watermarking framework designed specifically for dLLMs. DMark introduces three complementary strategies to restore watermark detectability: predictive watermarking uses model-predicted tokens when actual context is unavailable; bidirectional watermarking exploits both forward and backward dependencies unique to diffusion decoding; and predictive-bidirectional watermarking combines both approaches to maximize detection strength. Experiments across multiple dLLMs show that DMark achieves 92.0-99.5% detection rates at 1% false positive rate while maintaining text quality, compared to only 49.6-71.2% for naive adaptations of existing methods. DMark also demonstrates robustness against text manipulations, establishing that effective watermarking is feasible for non-autoregressive language models.</li>
<li><strong>摘要：</strong>扩散大语言模型（DLLM）的生成速度比自回归模型更快，同时保持可比的质量，但是由于其非序列解码，现有的水印方法失败了。与从左到右产生代币的自回旋模型不同，dllms可以按任意顺序最终确定令牌，从而打破了传统水印的因果设计。我们提出了Dmark，这是专门为DLLM设计的第一个水印框架。 dmark引入了三种互补策略来恢复水印可检测性：当实际情况不可用时，预测水印的使用模型被预测的令牌；双向水印可利用扩散解码所特有的前后依赖关系；预测三方向的水印结合了两种方法，以最大化检测强度。多个DLLM的实验表明，Dmark以1％的假阳性率达到92.0-99.5％的检测率，同时保持文本质量，而现有方法的天真适应性仅为49.6-71.2％。 dmark还证明了针对文本操作的鲁棒性，确定有效的水印对于非自动回忆语言模型是可行的。</li>
</ul>

<h3>Title: ContextFlow: Context-Aware Flow Matching For Trajectory Inference From Spatial Omics Data</h3>
<ul>
<li><strong>Authors: </strong>Santanu Subhash Rathod, Francesco Ceccarelli, Sean B. Holden, Pietro Liò, Xiao Zhang, Jovan Tanevski</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02952">https://arxiv.org/abs/2510.02952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02952">https://arxiv.org/pdf/2510.02952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02952]] ContextFlow: Context-Aware Flow Matching For Trajectory Inference From Spatial Omics Data(https://arxiv.org/abs/2510.02952)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Inferring trajectories from longitudinal spatially-resolved omics data is fundamental to understanding the dynamics of structural and functional tissue changes in development, regeneration and repair, disease progression, and response to treatment. We propose ContextFlow, a novel context-aware flow matching framework that incorporates prior knowledge to guide the inference of structural tissue dynamics from spatially resolved omics data. Specifically, ContextFlow integrates local tissue organization and ligand-receptor communication patterns into a transition plausibility matrix that regularizes the optimal transport objective. By embedding these contextual constraints, ContextFlow generates trajectories that are not only statistically consistent but also biologically meaningful, making it a generalizable framework for modeling spatiotemporal dynamics from longitudinal, spatially resolved omics data. Evaluated on three datasets, ContextFlow consistently outperforms state-of-the-art flow matching methods across multiple quantitative and qualitative metrics of inference accuracy and biological coherence. Our code is available at: \href{this https URL}{ContextFlow}</li>
<li><strong>摘要：</strong>从纵向空间分辨的OMICS数据中推断出轨迹是了解结构和功能组织变化在发育，再生和修复，疾病进展以及对治疗反应的动态的基础。我们提出了上下文流，这是一种新型的上下文感知流量匹配框架，结合了先验知识，以指导从空间解决的OMICS数据中推断结构组织动力学的推断。具体而言，ContextFlow将局部组织组织和配体 - 受体通信模式集成到过渡合理性矩阵中，该矩阵正规化了最佳运输目标。通过嵌入这些上下文约束，上下文流生成的轨迹不仅在统计上保持一致，而且在生物学上有意义，这使其成为建模从纵向，空间解析的OMICS数据中建模时空动力学的概括框架。在三个数据集上进行评估，上下文流始终优于推理准确性和生物相干性的多个定量和定性指标，均超过了最先进的流动匹配方法。我们的代码可在：\ href {this https url} {contextFlow}</li>
</ul>

<h3>Title: TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency</h3>
<ul>
<li><strong>Authors: </strong>Juntong Wang, Huiyu Duan, Jiarui Wang, Ziheng Jia, Guangtao Zhai, Xiongkuo Min</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02987">https://arxiv.org/abs/2510.02987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02987">https://arxiv.org/pdf/2510.02987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02987]] TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency(https://arxiv.org/abs/2510.02987)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of large multimodal models (LMMs), recent text-to-image (T2I) models can generate high-quality images and demonstrate great alignment to short prompts. However, they still struggle to effectively understand and follow long and detailed prompts, displaying inconsistent generation. To address this challenge, we introduce LPG-Bench, a comprehensive benchmark for evaluating long-prompt-based text-to-image generation. LPG-Bench features 200 meticulously crafted prompts with an average length of over 250 words, approaching the input capacity of several leading commercial models. Using these prompts, we generate 2,600 images from 13 state-of-the-art models and further perform comprehensive human-ranked annotations. Based on LPG-Bench, we observe that state-of-the-art T2I alignment evaluation metrics exhibit poor consistency with human preferences on long-prompt-based image generation. To address the gap, we introduce a novel zero-shot metric based on text-to-image-to-text consistency, termed TIT, for evaluating long-prompt-generated images. The core concept of TIT is to quantify T2I alignment by directly comparing the consistency between the raw prompt and the LMM-produced description on the generated image, which includes an efficient score-based instantiation TIT-Score and a large-language-model (LLM) based instantiation TIT-Score-LLM. Extensive experiments demonstrate that our framework achieves superior alignment with human judgment compared to CLIP-score, LMM-score, etc., with TIT-Score-LLM attaining a 7.31% absolute improvement in pairwise accuracy over the strongest baseline. LPG-Bench and TIT methods together offer a deeper perspective to benchmark and foster the development of T2I models. All resources will be made publicly available.</li>
<li><strong>摘要：</strong>随着大型多模型（LMM）的快速发展，最近的文本对图像（T2I）模型可以生成高质量的图像，并证明与短提示相符。但是，他们仍然很难有效理解并遵循长期详细的提示，从而表现出不一致的一代。为了应对这一挑战，我们介绍了LPG Bench，这是评估基于长期累积的文本形象的全面基准。 LPG板凳具有200个精心制作的提示，平均长度超过250个单词，接近了几种领先的商业模型的输入容量。使用这些提示，我们从13个最先进的模型中产生了2,600张图像，并进一步执行全面的人类级注释。基于LPG基础，我们观察到，最先进的T2I一致性评估指标表现出与基于长期提出的图像产生的人类偏好的一致性。为了解决差距，我们基于文本对图像到文本的一致性（称为TIT）引入了一种新颖的零击度量，用于评估长期准备点的图像。 TIT的核心概念是通过直接比较生成图像上的原始提示和LMM生成的描述之间的一致性来量化T2I对齐，该图像包括有效的基于基于得分的实例化TIT-SCORE和基于大型的实例化模型（LLM）基于实例化的tit-tit-tit-score-llm。广泛的实验表明，与剪辑得分，LMM得分等相比，我们的框架与人类判断力相比，在TIT-SCORE-LLM上获得了7.31％的绝对成对准确性，而在最强的基线上，成对精度的绝对确定性达到了7.31％。 LPG基础和TIT方法共同提供了更深入的观点，可以基准和促进T2I模型的开发。所有资源将公开可用。</li>
</ul>

<h3>Title: Towards Scalable and Consistent 3D Editing</h3>
<ul>
<li><strong>Authors: </strong>Ruihao Xia, Yang Tang, Pan Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02994">https://arxiv.org/abs/2510.02994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02994">https://arxiv.org/pdf/2510.02994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02994]] Towards Scalable and Consistent 3D Editing(https://arxiv.org/abs/2510.02994)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>3D editing - the task of locally modifying the geometry or appearance of a 3D asset - has wide applications in immersive content creation, digital entertainment, and AR/VR. However, unlike 2D editing, it remains challenging due to the need for cross-view consistency, structural fidelity, and fine-grained controllability. Existing approaches are often slow, prone to geometric distortions, or dependent on manual and accurate 3D masks that are error-prone and impractical. To address these challenges, we advance both the data and model fronts. On the data side, we introduce 3DEditVerse, the largest paired 3D editing benchmark to date, comprising 116,309 high-quality training pairs and 1,500 curated test pairs. Built through complementary pipelines of pose-driven geometric edits and foundation model-guided appearance edits, 3DEditVerse ensures edit locality, multi-view consistency, and semantic alignment. On the model side, we propose 3DEditFormer, a 3D-structure-preserving conditional transformer. By enhancing image-to-3D generation with dual-guidance attention and time-adaptive gating, 3DEditFormer disentangles editable regions from preserved structure, enabling precise and consistent edits without requiring auxiliary 3D masks. Extensive experiments demonstrate that our framework outperforms state-of-the-art baselines both quantitatively and qualitatively, establishing a new standard for practical and scalable 3D editing. Dataset and code will be released. Project: this https URL</li>
<li><strong>摘要：</strong>3D编辑 - 在本地修改3D资产的几何形状或外观的任务 - 在沉浸式内容创建，数字娱乐和AR/VR中具有广泛的应用。但是，与2D编辑不同，由于需要跨视图一致性，结构保真度和细粒度的可控性，因此仍然具有挑战性。现有的方法通常很慢，容易出现几何变形，或者取决于容易出错且不切实际的手动和准确的3D掩码。为了应对这些挑战，我们同时推进了数据和模型方面。在数据侧，我们介绍了迄今为止最大的配对3D编辑基准，包括116,309个高质量的训练对和1,500个策划的测试对。通过姿势驱动的几何编辑和基础模型引导的外观编辑的互补管道构建，3Deditverse确保编辑局部性，多视图一致性和语义对齐。在模型侧，我们提出了3DDITFormer，这是一种3D结构的条件变压器。通过以双重施加注意力和时间自适应的门控来增强图像到3D的生成，3DIDETFORMER DISENTANGER会从保存的结构中删除可编辑的区域，从而无需辅助3D口罩即可精确且一致的编辑。广泛的实验表明，我们的框架在定量和定性上都优于最先进的基线，从而为实用和可扩展的3D编辑建立了新的标准。数据集和代码将发布。项目：此HTTPS URL</li>
</ul>

<h3>Title: PocketSR: The Super-Resolution Expert in Your Pocket Mobiles</h3>
<ul>
<li><strong>Authors: </strong>Haoze Sun, Linfeng Jiang, Fan Li, Renjing Pei, Zhixin Wang, Yong Guo, Jiaqi Xu, Haoyu Chen, Jin Han, Fenglong Song, Yujiu Yang, Wenbo Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03012">https://arxiv.org/abs/2510.03012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03012">https://arxiv.org/pdf/2510.03012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03012]] PocketSR: The Super-Resolution Expert in Your Pocket Mobiles(https://arxiv.org/abs/2510.03012)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generative</a></li>
<li><strong>Abstract: </strong>Real-world image super-resolution (RealSR) aims to enhance the visual quality of in-the-wild images, such as those captured by mobile phones. While existing methods leveraging large generative models demonstrate impressive results, the high computational cost and latency make them impractical for edge deployment. In this paper, we introduce PocketSR, an ultra-lightweight, single-step model that brings generative modeling capabilities to RealSR while maintaining high fidelity. To achieve this, we design LiteED, a highly efficient alternative to the original computationally intensive VAE in SD, reducing parameters by 97.5% while preserving high-quality encoding and decoding. Additionally, we propose online annealing pruning for the U-Net, which progressively shifts generative priors from heavy modules to lightweight counterparts, ensuring effective knowledge transfer and further optimizing efficiency. To mitigate the loss of prior knowledge during pruning, we incorporate a multi-layer feature distillation loss. Through an in-depth analysis of each design component, we provide valuable insights for future research. PocketSR, with a model size of 146M parameters, processes 4K images in just 0.8 seconds, achieving a remarkable speedup over previous methods. Notably, it delivers performance on par with state-of-the-art single-step and even multi-step RealSR models, making it a highly practical solution for edge-device applications.</li>
<li><strong>摘要：</strong>现实世界图像超分辨率（REALSR）旨在提高野外图像的视觉质量，例如手机捕获的图像。尽管利用大型生成模型的现有方法表现出令人印象深刻的结果，但高计算成本和延迟使它们对于边缘部署不切实际。在本文中，我们介绍了Pocketsr，这是一种超轻质，单步的模型，在维持高忠诚度的同时，将生成性建模功能带给了Realsr。为此，我们设计了Liteed，这是SD中原始计算密集型VAE的高效替代品，在保留高质量的编码和解码的同时，将参数降低了97.5％。此外，我们为U-NET提出了在线退火修剪，该修剪逐渐将生成的先验从重型模块转移到轻质的同行，以确保有效的知识转移并进一步优化效率。为了减轻修剪过程中的先验知识的丧失，我们结合了多层特征蒸馏损失。通过对每个设计组件的深入分析，我们为将来的研究提供了宝贵的见解。 Pocketsr的型号大小为146m参数，仅在0.8秒内处理4K图像，从而在先前的方法上实现了显着的加速。值得注意的是，它与最先进的单步甚至多步室Realsr模型相同的性能，使其成为边缘设备应用程序的高度实用解决方案。</li>
</ul>

<h3>Title: Learning Robust Diffusion Models from Imprecise Supervision</h3>
<ul>
<li><strong>Authors: </strong>Dong-Dong Wu, Jiacheng Cui, Wei Wang, Zhiqiang She, Masashi Sugiyama</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03016">https://arxiv.org/abs/2510.03016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03016">https://arxiv.org/pdf/2510.03016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03016]] Learning Robust Diffusion Models from Imprecise Supervision(https://arxiv.org/abs/2510.03016)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Conditional diffusion models have achieved remarkable success in various generative tasks recently, but their training typically relies on large-scale datasets that inevitably contain imprecise information in conditional inputs. Such supervision, often stemming from noisy, ambiguous, or incomplete labels, will cause condition mismatch and degrade generation quality. To address this challenge, we propose DMIS, a unified framework for training robust Diffusion Models from Imprecise Supervision, which is the first systematic study within diffusion models. Our framework is derived from likelihood maximization and decomposes the objective into generative and classification components: the generative component models imprecise-label distributions, while the classification component leverages a diffusion classifier to infer class-posterior probabilities, with its efficiency further improved by an optimized timestep sampling strategy. Extensive experiments on diverse forms of imprecise supervision, covering tasks of image generation, weakly supervised learning, and noisy dataset condensation demonstrate that DMIS consistently produces high-quality and class-discriminative samples.</li>
<li><strong>摘要：</strong>有条件的扩散模型最近在各种生成任务中取得了巨大的成功，但是它们的培训通常依赖于大规模数据集，这些数据集不可避免地包含条件输入中的不精确信息。这种监督通常是出于嘈杂，模棱两可或不完整的标签，会导致条件不匹配并降低发电质量。为了应对这一挑战，我们提出了DMI，这是一个统一的框架，用于从不精确的监督中训练强大的扩散模型，这是扩散模型中的首次系统研究。我们的框架来自可能性最大化，并将目标分解为生成和分类组件：生成组件模型不精确的标签分布，而分类组件则利用扩散分类器来推断类 - 形成率的概率，其效率通过优化的时间播放抽样策略进一步提高。对各种形式的不精确监督，涵盖图像产生的任务，弱监督学习以及嘈杂的数据集凝结的广泛实验表明，DMI始终产生高质量和类别的歧义样本。</li>
</ul>

<h3>Title: When and Where do Events Switch in Multi-Event Video Generation?</h3>
<ul>
<li><strong>Authors: </strong>Ruotong Liao, Guowen Huang, Qing Cheng, Thomas Seidl, Daniel Cremers, Volker Tresp</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03049">https://arxiv.org/abs/2510.03049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03049">https://arxiv.org/pdf/2510.03049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03049]] When and Where do Events Switch in Multi-Event Video Generation?(https://arxiv.org/abs/2510.03049)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-video (T2V) generation has surged in response to challenging questions, especially when a long video must depict multiple sequential events with temporal coherence and controllable content. Existing methods that extend to multi-event generation omit an inspection of the intrinsic factor in event shifting. The paper aims to answer the central question: When and where multi-event prompts control event transition during T2V generation. This work introduces MEve, a self-curated prompt suite for evaluating multi-event text-to-video (T2V) generation, and conducts a systematic study of two representative model families, i.e., OpenSora and CogVideoX. Extensive experiments demonstrate the importance of early intervention in denoising steps and block-wise model layers, revealing the essential factor for multi-event video generation and highlighting the possibilities for multi-event conditioning in future models.</li>
<li><strong>摘要：</strong>文本到视频（T2V）的一代已经响应挑战性问题，尤其是当长视频必须描绘出具有时间连贯性和可控内容的多个顺序事件时。扩展到多事件一代的现有方法忽略了事件转移中内在因素的检查。该论文旨在回答一个中心问题：多项事件何时何地促使T2V生成期间控制事件过渡。这项工作介绍了Meve，这是一个自我策划的及时套件，用于评估多项式文本对视频（T2V）的生成，并对两个代表性模型家族（即Opensora和Cogvideox）进行了系统的研究。广泛的实验表明，早期干预在降级步骤和块模型层中的重要性，揭示了多事实视频生成的基本因素，并突出了未来模型中多事实条件的可能性。</li>
</ul>

<h3>Title: Comparative Analysis of Parameterized Action Actor-Critic Reinforcement Learning Algorithms for Web Search Match Plan Generation</h3>
<ul>
<li><strong>Authors: </strong>Ubayd Bapoo, Clement N Nyirenda</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03064">https://arxiv.org/abs/2510.03064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03064">https://arxiv.org/pdf/2510.03064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03064]] Comparative Analysis of Parameterized Action Actor-Critic Reinforcement Learning Algorithms for Web Search Match Plan Generation(https://arxiv.org/abs/2510.03064)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This study evaluates the performance of Soft Actor Critic (SAC), Greedy Actor Critic (GAC), and Truncated Quantile Critics (TQC) in high-dimensional decision-making tasks using fully observable environments. The focus is on parametrized action (PA) spaces, eliminating the need for recurrent networks, with benchmarks Platform-v0 and Goal-v0 testing discrete actions linked to continuous action-parameter spaces. Hyperparameter optimization was performed with Microsoft NNI, ensuring reproducibility by modifying the codebase for GAC and TQC. Results show that Parameterized Action Greedy Actor-Critic (PAGAC) outperformed other algorithms, achieving the fastest training times and highest returns across benchmarks, completing 5,000 episodes in 41:24 for the Platform game and 24:04 for the Robot Soccer Goal game. Its speed and stability provide clear advantages in complex action spaces. Compared to PASAC and PATQC, PAGAC demonstrated superior efficiency and reliability, making it ideal for tasks requiring rapid convergence and robust performance. Future work could explore hybrid strategies combining entropy-regularization with truncation-based methods to enhance stability and expand investigations into generalizability.</li>
<li><strong>摘要：</strong>这项研究评估了使用完全可观察的环境在高维决策任务中评估软演员评论家（SAC），贪婪的演员评论家（GAC）和截断的分位数评论家（TQC）。重点是参数化动作（PA）空间，消除了对经常性网络的需求，基准测试平台-V0和目标V0测试离散操作与连续的动作参数空间相关联。使用Microsoft NNI进行了高参数优化，通过修改GAC和TQC的代码库来确保重现性。结果表明，参数化的动作贪婪参与者批评（PAGAC）优于其他算法，达到了基准的最快训练时间和最高的回报，平台游戏的41:24在41:24中完成了5,000集，并在24:04中获得了机器人足球目标游戏的24:04。它的速度和稳定性在复杂的动作空间中提供了明显的优势。与PASAC和PATQC相比，PAGAC表现出较高的效率和可靠性，使其非常适合需要快速收敛和稳健性能的任务。未来的工作可以探索将熵验证与基于截断的方法相结合的混合策略，以增强稳定性并扩大对概括性的研究。</li>
</ul>

<h3>Title: What Drives Compositional Generalization in Visual Generative Models?</h3>
<ul>
<li><strong>Authors: </strong>Karim Farid, Rajat Sahay, Yumna Ali Alnaggar, Simon Schrodi, Volker Fischer, Cordelia Schmid, Thomas Brox</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03075">https://arxiv.org/abs/2510.03075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03075">https://arxiv.org/pdf/2510.03075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03075]] What Drives Compositional Generalization in Visual Generative Models?(https://arxiv.org/abs/2510.03075)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Compositional generalization, the ability to generate novel combinations of known concepts, is a key ingredient for visual generative models. Yet, not all mechanisms that enable or inhibit it are fully understood. In this work, we conduct a systematic study of how various design choices influence compositional generalization in image and video generation in a positive or negative way. Through controlled experiments, we identify two key factors: (i) whether the training objective operates on a discrete or continuous distribution, and (ii) to what extent conditioning provides information about the constituent concepts during training. Building on these insights, we show that relaxing the MaskGIT discrete loss with an auxiliary continuous JEPA-based objective can improve compositional performance in discrete models like MaskGIT.</li>
<li><strong>摘要：</strong>组成概括是生成已知概念的新型组合的能力，是视觉生成模型的关键要素。但是，并非所有能够或抑制它的机制都被完全理解。在这项工作中，我们对各种设计选择如何以积极或负面的方式影响图像和视频生成中的组成概括。通过对照实验，我们确定了两个关键因素：（i）培训目标是在离散或连续分配上运行，以及（ii）在何种程度上提供有关培训期间成分概念的信息。在这些见解的基础上，我们表明，通过基于JEPA的辅助连续目标，放松MaskGit离散损失可以改善MaskGit等离散模型中的组成性能。</li>
</ul>

<h3>Title: Latent Diffusion Unlearning: Protecting Against Unauthorized Personalization Through Trajectory Shifted Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Naresh Kumar Devulapally, Shruti Agarwal, Tejas Gokhale, Vishnu Suresh Lokhande</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03089">https://arxiv.org/abs/2510.03089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03089">https://arxiv.org/pdf/2510.03089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03089]] Latent Diffusion Unlearning: Protecting Against Unauthorized Personalization Through Trajectory Shifted Perturbations(https://arxiv.org/abs/2510.03089)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models have demonstrated remarkable effectiveness in rapid and high-fidelity personalization, even when provided with only a few user images. However, the effectiveness of personalization techniques has lead to concerns regarding data privacy, intellectual property protection, and unauthorized usage. To mitigate such unauthorized usage and model replication, the idea of generating ``unlearnable'' training samples utilizing image poisoning techniques has emerged. Existing methods for this have limited imperceptibility as they operate in the pixel space which results in images with noise and artifacts. In this work, we propose a novel model-based perturbation strategy that operates within the latent space of diffusion models. Our method alternates between denoising and inversion while modifying the starting point of the denoising trajectory: of diffusion models. This trajectory-shifted sampling ensures that the perturbed images maintain high visual fidelity to the original inputs while being resistant to inversion and personalization by downstream generative models. This approach integrates unlearnability into the framework of Latent Diffusion Models (LDMs), enabling a practical and imperceptible defense against unauthorized model adaptation. We validate our approach on four benchmark datasets to demonstrate robustness against state-of-the-art inversion attacks. Results demonstrate that our method achieves significant improvements in imperceptibility ($\sim 8 \% -10\%$ on perceptual metrics including PSNR, SSIM, and FID) and robustness ( $\sim 10\%$ on average across five adversarial settings), highlighting its effectiveness in safeguarding sensitive data.</li>
<li><strong>摘要：</strong>文本到图像扩散模型在快速和高保真个性化中也表现出了显着的有效性，即使仅提供了一些用户图像。但是，个性化技术的有效性导致人们对数据隐私，知识产权保护和未经授权的用法引起关注。为了减轻这种未经授权的用法和模型复制，已经出现了利用图像中毒技术的``无可融合''训练样本的想法。现有的方法在像素空间中运行时具有有限的不可识别性，从而导致具有噪声和伪影的图像。在这项工作中，我们提出了一种基于模型的新型扰动策略，该策略在扩散模型的潜在空间内运行。我们的方法在修改denoising轨迹的起点时在denoising和倒置之间交替：扩散模型的起点。这种轨迹移动的采样可确保扰动的图像对原始输入保持较高的视觉保真度，同时可以通过下游生成模型抵抗反转和个性化。这种方法将无效性集成到潜在扩散模型（LDMS）的框架中，从而实现了对未经授权模型适应的实用且不可察觉的防御。我们在四个基准数据集上验证我们的方法，以证明针对最新反转攻击的鲁棒性。结果表明，我们的方法可以实现不可识别的能力（$ \ sim 8 \％-10 \％$ $ in感知指标，包括PSNR，SSIM和FID）和鲁棒性（$ \ sim 10 \％的平均$ sim 10 \％$在五个对抗性设置中），在保护敏感的数据方面具有高度强调其有效性。</li>
</ul>

<h3>Title: Distilled Protein Backbone Generation</h3>
<ul>
<li><strong>Authors: </strong>Liyang Xie, Haoran Zhang, Zhendong Wang, Wesley Tansey, Mingyuan Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03095">https://arxiv.org/abs/2510.03095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03095">https://arxiv.org/pdf/2510.03095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03095]] Distilled Protein Backbone Generation(https://arxiv.org/abs/2510.03095)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion- and flow-based generative models have recently demonstrated strong performance in protein backbone generation tasks, offering unprecedented capabilities for de novo protein design. However, while achieving notable performance in generation quality, these models are limited by their generating speed, often requiring hundreds of iterative steps in the reverse-diffusion process. This computational bottleneck limits their practical utility in large-scale protein discovery, where thousands to millions of candidate structures are needed. To address this challenge, we explore the techniques of score distillation, which has shown great success in reducing the number of sampling steps in the vision domain while maintaining high generation quality. However, a straightforward adaptation of these methods results in unacceptably low designability. Through extensive study, we have identified how to appropriately adapt Score identity Distillation (SiD), a state-of-the-art score distillation strategy, to train few-step protein backbone generators which significantly reduce sampling time, while maintaining comparable performance to their pretrained teacher model. In particular, multistep generation combined with inference time noise modulation is key to the success. We demonstrate that our distilled few-step generators achieve more than a 20-fold improvement in sampling speed, while achieving similar levels of designability, diversity, and novelty as the Proteina teacher model. This reduction in inference cost enables large-scale in silico protein design, thereby bringing diffusion-based models closer to real-world protein engineering applications.</li>
<li><strong>摘要：</strong>扩散和基于流动的生成模型最近在蛋白质主链生成任务中表现出强大的性能，为从头蛋白质设计提供了前所未有的功能。但是，尽管这些模型在发电质量方面取得了显着的性能，但这些模型受其发电速度的限制，通常需要在反向扩散过程中进行数百个迭代步骤。这种计算瓶颈限制了它们在大规模蛋白质发现中的实际实用性，其中需要数千至数百万的候选结构。为了应对这一挑战，我们探讨了得分蒸馏的技术，这在减少视觉域中的采样步骤的数量方面取得了巨大成功，同时保持高发电质量。但是，这些方法的直接适应性导致令人难以置信的低设计性。通过广泛的研究，我们已经确定了如何适当适应得分身份蒸馏（SID）（一种最先进的分数蒸馏策略）来训练少数步骤的蛋白质骨架发电机，从而显着降低采样时间，同时保持与预处理的教师模型的可比性。特别是，多步生成与推理时间噪声调制相结合是成功的关键。我们证明，蒸馏的几步发电机的采样速度取得了20倍以上的提高，同时达到了与Proteina教师模型相似的可设计性，多样性和新颖性水平。推理成本的降低使得硅蛋白设计的大规模大规模，从而使基于扩散的模型更接近现实世界的蛋白质工程应用。</li>
</ul>

<h3>Title: GeoComplete: Geometry-Aware Diffusion for Reference-Driven Image Completion</h3>
<ul>
<li><strong>Authors: </strong>Beibei Lin, Tingting Chen, Robby T. Tan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03110">https://arxiv.org/abs/2510.03110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03110">https://arxiv.org/pdf/2510.03110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03110]] GeoComplete: Geometry-Aware Diffusion for Reference-Driven Image Completion(https://arxiv.org/abs/2510.03110)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Reference-driven image completion, which restores missing regions in a target view using additional images, is particularly challenging when the target view differs significantly from the references. Existing generative methods rely solely on diffusion priors and, without geometric cues such as camera pose or depth, often produce misaligned or implausible content. We propose GeoComplete, a novel framework that incorporates explicit 3D structural guidance to enforce geometric consistency in the completed regions, setting it apart from prior image-only approaches. GeoComplete introduces two key ideas: conditioning the diffusion process on projected point clouds to infuse geometric information, and applying target-aware masking to guide the model toward relevant reference cues. The framework features a dual-branch diffusion architecture. One branch synthesizes the missing regions from the masked target, while the other extracts geometric features from the projected point cloud. Joint self-attention across branches ensures coherent and accurate completion. To address regions visible in references but absent in the target, we project the target view into each reference to detect occluded areas, which are then masked during training. This target-aware masking directs the model to focus on useful cues, enhancing performance in difficult scenarios. By integrating a geometry-aware dual-branch diffusion architecture with a target-aware masking strategy, GeoComplete offers a unified and robust solution for geometry-conditioned image completion. Experiments show that GeoComplete achieves a 17.1 PSNR improvement over state-of-the-art methods, significantly boosting geometric accuracy while maintaining high visual quality.</li>
<li><strong>摘要：</strong>参考驱动的图像完成使用其他图像恢复目标视图中缺失区域，当目标视图与参考文献显着不同时，尤其具有挑战性。现有的生成方法仅依赖于扩散先验，而没有几何提示（例如相机姿势或深度），通常会产生不一致或不可行的内容。我们提出了GeoComplete，这是一个新颖的框架，结合了明确的3D结构指导，以在完整的区域中执行几何一致性，从而将其与先前的仅图像方法区分开来。 GeoComplete介绍了两个关键思想：将投影点云上的扩散过程调节以注入几何信息，并应用目标感知的掩码以指导模型到相关的参考线索。该框架具有双分支扩散体系结构。一个分支从蒙版目标中综合了缺失区域，而另一个分支从投影点云中提取几何特征。跨分支的联合自我注意确保了连贯和准确的完成。为了解决参考文献中可见的区域，但在目标中不存在，我们将目标视图投射到每个参考文献中以检测遮挡区域，然后在训练过程中掩盖了目标。这种目标感知的掩盖指导该模型专注于有用的线索，从而在困难的情况下提高性能。通过将几何感知的双支分支扩散架构与目标感知掩盖策略集成，GeoComplete为几何形成图像完成提供了统一且坚固的解决方案。实验表明，GeoComplete对最先进的方法的改善17.1 PSNR改进，在保持高视觉质量的同时显着提高了几何精度。</li>
</ul>

<h3>Title: Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction</h3>
<ul>
<li><strong>Authors: </strong>Kaisi Guan, Xihua Wang, Zhengfeng Lai, Xin Cheng, Peng Zhang, XiaoJiang Liu, Ruihua Song, Meng Cao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03117">https://arxiv.org/abs/2510.03117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03117">https://arxiv.org/pdf/2510.03117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03117]] Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction(https://arxiv.org/abs/2510.03117)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This study focuses on a challenging yet promising task, Text-to-Sounding-Video (T2SV) generation, which aims to generate a video with synchronized audio from text conditions, meanwhile ensuring both modalities are aligned with text. Despite progress in joint audio-video training, two critical challenges still remain unaddressed: (1) a single, shared text caption where the text for video is equal to the text for audio often creates modal interference, confusing the pretrained backbones, and (2) the optimal mechanism for cross-modal feature interaction remains unclear. To address these challenges, we first propose the Hierarchical Visual-Grounded Captioning (HVGC) framework that generates pairs of disentangled captions, a video caption, and an audio caption, eliminating interference at the conditioning stage. Based on HVGC, we further introduce BridgeDiT, a novel dual-tower diffusion transformer, which employs a Dual CrossAttention (DCA) mechanism that acts as a robust ``bridge" to enable a symmetric, bidirectional exchange of information, achieving both semantic and temporal synchronization. Extensive experiments on three benchmark datasets, supported by human evaluations, demonstrate that our method achieves state-of-the-art results on most metrics. Comprehensive ablation studies further validate the effectiveness of our contributions, offering key insights for the future T2SV task. All the codes and checkpoints will be publicly released.</li>
<li><strong>摘要：</strong>这项研究重点是一项具有挑战性但有前途的任务，即听起来像是响亮的视频（T2SV）一代，旨在从文本条件中生成带有同步音频的视频，同时确保两种方式都与文本一致。尽管联合音频视频训练取得了进展，但仍未解决两个关键挑战：（1）单个共享的文本标题，其中视频的文本与音频的文本相等，通常会产生模态干扰，并使预验证的骨干混淆，并且（2）交叉模式交互的最佳机制仍然不清楚。为了应对这些挑战，我们首先提出了层次的视觉接地字幕（HVGC）框架，该框架生成成对的分离字幕，视频字幕和音频字幕，消除了调节阶段的干扰。基于HVGC，我们进一步介绍了一种新型的双重扩散变压器BridgedIt，它采用了双重交叉式（DCA）机制，该机制充当了强大的``桥''，可以启用对称性的，双向的信息交换，从而在语义和时间同步方面实现三个Beantime的方法。大多数指标的最新结果。</li>
</ul>

<h3>Title: Mask2IV: Interaction-Centric Video Generation via Mask Trajectories</h3>
<ul>
<li><strong>Authors: </strong>Gen Li, Bo Zhao, Jianfei Yang, Laura Sevilla-Lara</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03135">https://arxiv.org/abs/2510.03135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03135">https://arxiv.org/pdf/2510.03135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03135]] Mask2IV: Interaction-Centric Video Generation via Mask Trajectories(https://arxiv.org/abs/2510.03135)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating interaction-centric videos, such as those depicting humans or robots interacting with objects, is crucial for embodied intelligence, as they provide rich and diverse visual priors for robot learning, manipulation policy training, and affordance reasoning. However, existing methods often struggle to model such complex and dynamic interactions. While recent studies show that masks can serve as effective control signals and enhance generation quality, obtaining dense and precise mask annotations remains a major challenge for real-world use. To overcome this limitation, we introduce Mask2IV, a novel framework specifically designed for interaction-centric video generation. It adopts a decoupled two-stage pipeline that first predicts plausible motion trajectories for both actor and object, then generates a video conditioned on these trajectories. This design eliminates the need for dense mask inputs from users while preserving the flexibility to manipulate the interaction process. Furthermore, Mask2IV supports versatile and intuitive control, allowing users to specify the target object of interaction and guide the motion trajectory through action descriptions or spatial position cues. To support systematic training and evaluation, we curate two benchmarks covering diverse action and object categories across both human-object interaction and robotic manipulation scenarios. Extensive experiments demonstrate that our method achieves superior visual realism and controllability compared to existing baselines.</li>
<li><strong>摘要：</strong>生成以互动为中心的视频，例如描绘人类或机器人与物体相互作用的视频，对于体现的智能至关重要，因为它们为机器人学习，操纵政策培训和负担得起的推理提供了丰富而多样的视觉先验。但是，现有的方法通常很难模拟这种复杂而动态的相互作用。尽管最近的研究表明，面具可以充当有效的控制信号并提高发电质量，但获得致密和精确的面具注释仍然是现实世界中使用的主要挑战。为了克服这一限制，我们介绍了Mask2IV，这是一个专门为以相互作用为中心的视频生成而设计的新型框架。它采用了一个分离的两阶段管道，该管道首先预测演员和对象的合理运动轨迹，然后生成以这些轨迹为条件的视频。这种设计消除了用户对密集的遮罩输入的需求，同时保留了操纵交互过程的灵活性。此外，Mask2IV支持多功能和直观的控制，使用户可以指定交互的目标对象，并通过动作描述或空间位置提示指导运动轨迹。为了支持系统的培训和评估，我们策划了两个基准，涵盖了人类对象相互作用和机器人操纵方案的各种动作和对象类别。广泛的实验表明，与现有基准相比，我们的方法实现了优越的视觉现实主义和可控性。</li>
</ul>

<h3>Title: Taming Imperfect Process Verifiers: A Sampling Perspective on Backtracking</h3>
<ul>
<li><strong>Authors: </strong>Dhruv Rohatgi, Abhishek Shetty, Donya Saless, Yuchen Li, Ankur Moitra, Andrej Risteski, Dylan J. Foster</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03149">https://arxiv.org/abs/2510.03149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03149">https://arxiv.org/pdf/2510.03149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03149]] Taming Imperfect Process Verifiers: A Sampling Perspective on Backtracking(https://arxiv.org/abs/2510.03149)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Test-time algorithms that combine the generative power of language models with process verifiers that assess the quality of partial generations offer a promising lever for eliciting new reasoning capabilities, but the algorithmic design space and computational scaling properties of such approaches are still opaque, and their benefits are far from apparent when one accounts for the cost of learning a high-quality verifier. Our starting point is the observation that seemingly benign errors in a learned verifier can lead to catastrophic failures for standard decoding techniques due to error amplification during the course of generation. We then ask: can this be improved with more sophisticated decoding strategies? We introduce a new process-guided test-time sampling algorithm, VGB, which uses theoretically grounded backtracking to achieve provably better robustness to verifier errors. VGB interprets autoregressive generation as a random walk on a tree of partial generations, with transition probabilities guided by the process verifier and base model; crucially, backtracking occurs probabilistically. This process generalizes the seminal Sinclair-Jerrum random walk (Sinclair & Jerrum, 1989) from the literature on approximate counting and sampling in theoretical computer science, and a conceptual contribution of our work is to highlight parallels with this literature. Empirically, we demonstrate on both synthetic and real language modeling tasks that VGB outperforms baselines on a variety of metrics.</li>
<li><strong>摘要：</strong>将语言模型的生成力与流程验证器结合起来的测试时间算法可以评估部分世代的质量，这为获得新的推理能力提供了有希望的杠杆，但是算法设计空间和此类方法的计算缩放属性仍然是不透明的，并且在一个算法的成本远远不足以学习高级知识的成本。我们的起点是观察到，在学识渊博的验证者中看似良性的错误可能会导致由于产生过程中的错误扩增而导致标准解码技术的灾难性故障。然后我们问：可以通过更复杂的解码策略来改善这一点吗？我们引入了一种新的过程引导的测试时间采样算法VGB，该算法使用理论上接地的回溯来实现对验证误差的鲁棒性。 VGB将自回归生成解释为部分世代的随机步行，其过渡概率在过程验证器和基本模型的指导下；至关重要的是，回溯概率发生。这个过程概括了有关理论计算机科学中近似计数和抽样的文献中的开创性Sinclair-Jerrum随机步行（Sinclair＆Jerrum，1989），我们的工作概念上的贡献是突出与本文相关的。从经验上讲，我们在合成和真实语言建模任务上证明了VGB在各种指标上的表现优于基准。</li>
</ul>

<h3>Title: SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus</h3>
<ul>
<li><strong>Authors: </strong>Ming Zhao, Wenhui Dong, Yang Zhang, Xiang Zheng, Zhonghao Zhang, Zian Zhou, Yunzhi Guan, Liukun Xu, Wei Peng, Zhaoyang Gong, Zhicheng Zhang, Dachuan Li, Xiaosheng Ma, Yuli Ma, Jianing Ni, Changjiang Jiang, Lixia Tian, Qixin Chen, Kaishun Xia, Pingping Liu, Tongshun Zhang, Zhiqiang Liu, Zhongan Bi, Chenyang Si, Tiansheng Sun, Caifeng Shan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03160">https://arxiv.org/abs/2510.03160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03160">https://arxiv.org/pdf/2510.03160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03160]] SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus(https://arxiv.org/abs/2510.03160)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Spine disorders affect 619 million people globally and are a leading cause of disability, yet AI-assisted diagnosis remains limited by the lack of level-aware, multimodal datasets. Clinical decision-making for spine disorders requires sophisticated reasoning across X-ray, CT, and MRI at specific vertebral levels. However, progress has been constrained by the absence of traceable, clinically-grounded instruction data and standardized, spine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem co-designed with practicing spine surgeons. It features SpineMed-450k, the first large-scale dataset explicitly designed for vertebral-level reasoning across imaging modalities with over 450,000 instruction instances, and SpineBench, a clinically-grounded evaluation framework. SpineMed-450k is curated from diverse sources, including textbooks, guidelines, open datasets, and ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline with a two-stage LLM generation method (draft and revision) to ensure high-quality, traceable data for question-answering, multi-turn consultations, and report generation. SpineBench evaluates models on clinically salient axes, including level identification, pathology assessment, and surgical planning. Our comprehensive evaluation of several recently advanced large vision-language models (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained, level-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k demonstrates consistent and significant improvements across all tasks. Clinician assessments confirm the diagnostic clarity and practical utility of our model's outputs.</li>
<li><strong>摘要：</strong>脊柱疾病在全球影响6.19亿人，是残疾的主要原因，但是AI辅助诊断仍然受到缺乏水平了解的多模式数据集的限制。脊柱疾病的临床决策需要在特定椎骨水平上进行X射线，CT和MRI的复杂推理。但是，由于缺乏可追溯，临床基础的指导数据和标准化的，特定于脊柱特异性的基准，进展受到了限制。为了解决这个问题，我们介绍了Spinemed，这是一种与脊柱外科医生共同设计的生态系统。它具有Spinemed-450k，这是第一个大型数据集，该数据集明确设计用于具有超过450,000个说明实例的成像方式的椎骨级别推理，以及SpineBench是一个临床上扎根的评估框架。 Spinemed-450k由不同的来源策划，包括教科书，指南，开放数据集和〜1,000个去识别的医院病例，使用临床医生在循环管道中采用两阶段LLM生成方法（草稿和修订），以确保提问的高质量，可追溯的问答数据，多欧转移，多型咨询和报告咨询，并生成报告。 Spinebench评估了临床显着轴的模型，包括水平识别，病理评估和手术计划。我们对脊柱台基础上几种最近先进的大型视力模型（LVLM）的全面评估揭示了精细粒度，特定水平的推理中的系统弱点。相比之下，我们对Spinemed-450k进行微调的模型在所有任务中都表现出一致和显着的改进。临床医生评估证实了我们模型产出的诊断清晰度和实用性。</li>
</ul>

<h3>Title: UniShield: An Adaptive Multi-Agent Framework for Unified Forgery Image Detection and Localization</h3>
<ul>
<li><strong>Authors: </strong>Qing Huang, Zhipei Xu, Xuanyu Zhang, Jian Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03161">https://arxiv.org/abs/2510.03161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03161">https://arxiv.org/pdf/2510.03161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03161]] UniShield: An Adaptive Multi-Agent Framework for Unified Forgery Image Detection and Localization(https://arxiv.org/abs/2510.03161)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>With the rapid advancements in image generation, synthetic images have become increasingly realistic, posing significant societal risks, such as misinformation and fraud. Forgery Image Detection and Localization (FIDL) thus emerges as essential for maintaining information integrity and societal security. Despite impressive performances by existing domain-specific detection methods, their practical applicability remains limited, primarily due to their narrow specialization, poor cross-domain generalization, and the absence of an integrated adaptive framework. To address these issues, we propose UniShield, the novel multi-agent-based unified system capable of detecting and localizing image forgeries across diverse domains, including image manipulation, document manipulation, DeepFake, and AI-generated images. UniShield innovatively integrates a perception agent with a detection agent. The perception agent intelligently analyzes image features to dynamically select suitable detection models, while the detection agent consolidates various expert detectors into a unified framework and generates interpretable reports. Extensive experiments show that UniShield achieves state-of-the-art results, surpassing both existing unified approaches and domain-specific detectors, highlighting its superior practicality, adaptiveness, and scalability.</li>
<li><strong>摘要：</strong>随着图像产生的快速发展，合成图像已变得越来越现实，带来了重大的社会风险，例如错误信息和欺诈。因此，伪造图像检测和本地化（FIDL）对于维持信息完整性和社会安全至关重要。尽管现有域特异性检测方法的表现令人印象深刻，但其实际适用性仍然有限，这主要是由于它们的狭窄专业化，跨域概括不良以及缺乏集成的自适应框架。为了解决这些问题，我们提出了unishield，这是一种新型的基于多代理的统一系统，能够检测和本地化各种领域的图像伪造，包括图像操纵，文档操纵，深击和AI生成的图像。 Unishield创新将感知剂与检测剂整合在一起。感知代理会智能地分析图像特征，以动态选择合适的检测模型，而检测代理将各种专家检测器合并到统一的框架中并生成可解释的报告。广泛的实验表明，Unishield取得了最新的结果，超过了现有的统一方法和特定于域的探测器，突出了其优越的实用性，适应性和可扩展性。</li>
</ul>

<h3>Title: ROGR: Relightable 3D Objects using Generative Relighting</h3>
<ul>
<li><strong>Authors: </strong>Jiapeng Tang, Matthew Lavine, Dor Verbin, Stephan J. Garbin, Matthias Nießner, Ricardo Martin Brualla, Pratul P. Srinivasan, Philipp Henzler</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03163">https://arxiv.org/abs/2510.03163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03163">https://arxiv.org/pdf/2510.03163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03163]] ROGR: Relightable 3D Objects using Generative Relighting(https://arxiv.org/abs/2510.03163)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce ROGR, a novel approach that reconstructs a relightable 3D model of an object captured from multiple views, driven by a generative relighting model that simulates the effects of placing the object under novel environment illuminations. Our method samples the appearance of the object under multiple lighting environments, creating a dataset that is used to train a lighting-conditioned Neural Radiance Field (NeRF) that outputs the object's appearance under any input environmental lighting. The lighting-conditioned NeRF uses a novel dual-branch architecture to encode the general lighting effects and specularities separately. The optimized lighting-conditioned NeRF enables efficient feed-forward relighting under arbitrary environment maps without requiring per-illumination optimization or light transport simulation. We evaluate our approach on the established TensoIR and Stanford-ORB datasets, where it improves upon the state-of-the-art on most metrics, and showcase our approach on real-world object captures.</li>
<li><strong>摘要：</strong>我们介绍了Rogr，这是一种新颖的方法，该方法重建了从多个视图捕获的对象的可靠的3D模型，该模型是由生成重新定制模型驱动的，该模型模拟了将对象放置在新的环境照明下的效果。我们的方法在多个照明环境下示例对象的外观，创建一个数据集，该数据集用于训练照明条件的神经辐射场（NERF），该数据集在任何输入环境照明下输出对象的外观。照明条件的NERF使用一种新颖的双分支结构来分别编码一般的照明效果和镜面。优化的照明条件的NERF可以在任意环境地图下有效地进行进料重新确认，而无需进行截至每次刷新的优化或轻型传输模拟。我们在既定的Tensoir和Stanford-Orb数据集上评估了我们的方法，在该数据集上，它可以改善大多数指标的最新方法，并展示我们在现实世界对象捕获的方法。</li>
</ul>

<h3>Title: Dynamic Prompt Generation for Interactive 3D Medical Image Segmentation Training</h3>
<ul>
<li><strong>Authors: </strong>Tidiane Camaret Ndir, Alexander Pfefferle, Robin Tibor Schirrmeister</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03189">https://arxiv.org/abs/2510.03189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03189">https://arxiv.org/pdf/2510.03189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03189]] Dynamic Prompt Generation for Interactive 3D Medical Image Segmentation Training(https://arxiv.org/abs/2510.03189)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Interactive 3D biomedical image segmentation requires efficient models that can iteratively refine predictions based on user prompts. Current foundation models either lack volumetric awareness or suffer from limited interactive capabilities. We propose a training strategy that combines dynamic volumetric prompt generation with content-aware adaptive cropping to optimize the use of the image encoder. Our method simulates realistic user interaction patterns during training while addressing the computational challenges of learning from sequential refinement feedback on a single GPU. For efficient training, we initialize our network using the publicly available weights from the nnInteractive segmentation model. Evaluation on the \textbf{Foundation Models for Interactive 3D Biomedical Image Segmentation} competition demonstrates strong performance with an average final Dice score of 0.6385, normalized surface distance of 0.6614, and area-under-the-curve metrics of 2.4799 (Dice) and 2.5671 (NSD).</li>
<li><strong>摘要：</strong>交互式3D生物医学图像分割需要有效的模型，该模型可以根据用户提示进行迭代完善预测。当前的基础模型要么缺乏体积意识，要么具有有限的互动功能。我们提出了一种训练策略，将动态体积及时生成与内容感知的自适应裁剪相结合，以优化图像编码器的使用。我们的方法在训练过程中模拟了现实的用户交互模式，同时解决了从单个GPU上的顺序改进反馈学习的计算挑战。为了进行有效的培训，我们使用NNNInteractive分割模型的公开权重初始化我们的网络。 \ textBf {交互式3D生物医学图像分割的基础模型的评估}竞争表现出强烈的性能，平均最终骰子得分为0.6385，归一化的表面距离为0.6614，面积下方的面积为2.4799（DICE）和2.5671（NSD）。</li>
</ul>

<h3>Title: Product-Quantised Image Representation for High-Quality Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Denis Zavadski, Nikita Philip Tatsch, Carsten Rother</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03191">https://arxiv.org/abs/2510.03191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03191">https://arxiv.org/pdf/2510.03191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03191]] Product-Quantised Image Representation for High-Quality Image Synthesis(https://arxiv.org/abs/2510.03191)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Product quantisation (PQ) is a classical method for scalable vector encoding, yet it has seen limited usage for latent representations in high-fidelity image generation. In this work, we introduce PQGAN, a quantised image autoencoder that integrates PQ into the well-known vector quantisation (VQ) framework of VQGAN. PQGAN achieves a noticeable improvement over state-of-the-art methods in terms of reconstruction performance, including both quantisation methods and their continuous counterparts. We achieve a PSNR score of 37dB, where prior work achieves 27dB, and are able to reduce the FID, LPIPS, and CMMD score by up to 96%. Our key to success is a thorough analysis of the interaction between codebook size, embedding dimensionality, and subspace factorisation, with vector and scalar quantisation as special cases. We obtain novel findings, such that the performance of VQ and PQ behaves in opposite ways when scaling the embedding dimension. Furthermore, our analysis shows performance trends for PQ that help guide optimal hyperparameter selection. Finally, we demonstrate that PQGAN can be seamlessly integrated into pre-trained diffusion models. This enables either a significantly faster and more compute-efficient generation, or a doubling of the output resolution at no additional cost, positioning PQ as a strong extension for discrete latent representation in image synthesis.</li>
<li><strong>摘要：</strong>产品定量（PQ）是一种可扩展矢量编码的经典方法，但是在高保真图像生成中，潜在表示的使用有限。在这项工作中，我们引入了PQGAN，这是一种定量的图像自动编码器，将PQ集成到VQGAN的众所周知的矢量定量（VQ）框架中。 PQGAN在重建性能方面对最先进的方法取得了显着改善，包括定量方法及其连续的对应物。我们的PSNR得分为37dB，以前的工作达到27dB，并且能够将FID，LPIP和CMMD分数降低96％。我们成功的关键是对代码书大小，嵌入维度和子空间分解之间的相互作用进行彻底分析，并将矢量和标量定量作为特殊情况。我们获得了新的发现，使得VQ和PQ的性能在缩放嵌入维度时以相反的方式行为。此外，我们的分析显示了PQ的性能趋势，有助于指导最佳的超参数选择。最后，我们证明可以将PQGAN无缝整合到预训练的扩散模型中。这使得可以显着更快，更高的计算生成，或者以无需额外的成本将输出分辨率加倍，将PQ定位为图像合成中离散潜在表示的强大扩展。</li>
</ul>

<h3>Title: Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft</h3>
<ul>
<li><strong>Authors: </strong>Junchao Huang, Xinting Hu, Boyao Han, Shaoshuai Shi, Zhuotao Tian, Tianyu He, Li Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03198">https://arxiv.org/abs/2510.03198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03198">https://arxiv.org/pdf/2510.03198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03198]] Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft(https://arxiv.org/abs/2510.03198)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Autoregressive video diffusion models have proved effective for world modeling and interactive scene generation, with Minecraft gameplay as a representative application. To faithfully simulate play, a model must generate natural content while exploring new scenes and preserve spatial consistency when revisiting explored areas. Under limited computation budgets, it must compress and exploit historical cues within a finite context window, which exposes a trade-off: Temporal-only memory lacks long-term spatial consistency, whereas adding spatial memory strengthens consistency but may degrade new scene generation quality when the model over-relies on insufficient spatial context. We present Memory Forcing, a learning framework that pairs training protocols with a geometry-indexed spatial memory. Hybrid Training exposes distinct gameplay regimes, guiding the model to rely on temporal memory during exploration and incorporate spatial memory for revisits. Chained Forward Training extends autoregressive training with model rollouts, where chained predictions create larger pose variations and encourage reliance on spatial memory for maintaining consistency. Point-to-Frame Retrieval efficiently retrieves history by mapping currently visible points to their source frames, while Incremental 3D Reconstruction maintains and updates an explicit 3D cache. Extensive experiments demonstrate that Memory Forcing achieves superior long-term spatial consistency and generative quality across diverse environments, while maintaining computational efficiency for extended sequences.</li>
<li><strong>摘要：</strong>已证明自回归的视频扩散模型对世界建模和交互式场景的生成有效，而Minecraft游戏是代表性应用程序。为了忠实模拟游戏，模型必须在探索新场景的同时产生自然内容，并在重新访问探索区域时保持空间一致性。在有限的计算预算下，它必须在有限的上下文窗口中压缩和利用历史提示，该窗口暴露了权衡：仅时间的记忆缺乏长期的空间一致性，而添加空间记忆会增强一致性，但当模型过高的空间上的空间上下文时，可能会降低新的场景生成质量。我们提出内存强迫，这是一个学习框架，该框架将培训协议与几何索引的空间内​​存配对。混合训练公开了不同的游戏制度，指导模型在探索过程中依靠时间记忆，并将空间记忆纳入重访中。链式训练通过模型推出扩展了自回归训练，其中链式预测会带来更大的姿势变化，并鼓励依赖空间记忆以保持一致性。点对上的检索可以通过将当前可见点映射到其源框架上有效检索历史记录，而增量3D重建则保持并更新显式的3D缓存。广泛的实验表明，记忆力强迫在各种环境中实现了卓越的长期空间一致性和生成质量，同时维持扩展序列的计算效率。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
