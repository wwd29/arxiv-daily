<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-02-09</h1>
<h3>Title: Toward Faithful and Complete Answer Construction from a Single Document</h3>
<ul>
<li><strong>Authors: </strong>Zhaoyang Chen, Cody Fleming</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06103">https://arxiv.org/abs/2602.06103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06103">https://arxiv.org/pdf/2602.06103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06103]] Toward Faithful and Complete Answer Construction from a Single Document(https://arxiv.org/abs/2602.06103)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Modern large language models (LLMs) are powerful generators driven by statistical next-token prediction. While effective at producing fluent text, this design biases models toward high-probability continuations rather than exhaustive and faithful answers grounded in source content. As a result, directly applying LLMs lacks systematic mechanisms to ensure both completeness (avoiding omissions) and faithfulness (avoiding unsupported content), which fundamentally conflicts with core AI safety principles. To address this limitation, we present EVE, a structured framework for document-grounded reasoning. Unlike free-form prompting, EVE constrains generation to a structured, verifiable pipeline that decomposes high-rigor reasoning into extraction, validation, and enumeration. Empirically, this design enables consistent and simultaneous improvements in recall, precision, and F1-score: recall and precision increase by up to 24\% and 29\%, respectively, with a corresponding 31\% gain in F1-score. This effectively breaks the long-standing trade-off between coverage and accuracy typical of single-pass LLM generation, while also mitigating generation truncation caused by length limitations. At the same time, we emphasize that EVE exhibits performance saturation due to the inherent ambiguity of natural language, reflecting fundamental limits of language-based reasoning.</li>
<li><strong>摘要：</strong>现代大型语言模型 (LLM) 是由统计下一个标记预测驱动的强大生成器。虽然可以有效地生成流畅的文本，但这种设计使模型偏向于高概率的延续，而不是基于源内容的详尽而忠实的答案。因此，直接申请LLM缺乏系统的机制来确保完整性（避免遗漏）和忠实性（避免不支持的内容），这从根本上与人工智能的核心安全原则相冲突。为了解决这个限制，我们提出了 EVE，一个基于文档的推理的结构化框架。与自由形式的提示不同，EVE 将生成限制为结构化、可验证的管道，将高度严格的推理分解为提取、验证和枚举。根据经验，这种设计能够在召回率、精确度和 F1 分数方面实现一致且同步的改进：召回率和精确度分别提高了 24% 和 29%，相应的 F1 分数提高了 31%。这有效地打破了单通道 LLM 生成典型的覆盖率和准确性之间长期存在的权衡，同时还减轻了由于长度限制导致的生成截断。同时，我们强调，由于自然语言固有的歧义性，EVE 表现出性能饱和，反映了基于语言的推理的基本限制。</li>
</ul>

<h3>Title: From Blurry to Believable: Enhancing Low-quality Talking Heads with 3D Generative Priors</h3>
<ul>
<li><strong>Authors: </strong>Ding-Jiun Huang, Yuanhao Wang, Shao-Ji Yuan, Albert Mosella-Montoro, Francisco Vicente Carrasco, Cheng Zhang, Fernando De la Torre</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06122">https://arxiv.org/abs/2602.06122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06122">https://arxiv.org/pdf/2602.06122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06122]] From Blurry to Believable: Enhancing Low-quality Talking Heads with 3D Generative Priors(https://arxiv.org/abs/2602.06122)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generative</a></li>
<li><strong>Abstract: </strong>Creating high-fidelity, animatable 3D talking heads is crucial for immersive applications, yet often hindered by the prevalence of low-quality image or video sources, which yield poor 3D reconstructions. In this paper, we introduce SuperHead, a novel framework for enhancing low-resolution, animatable 3D head avatars. The core challenge lies in synthesizing high-quality geometry and textures, while ensuring both 3D and temporal consistency during animation and preserving subject identity. Despite recent progress in image, video and 3D-based super-resolution (SR), existing SR techniques are ill-equipped to handle dynamic 3D inputs. To address this, SuperHead leverages the rich priors from pre-trained 3D generative models via a novel dynamics-aware 3D inversion scheme. This process optimizes the latent representation of the generative model to produce a super-resolved 3D Gaussian Splatting (3DGS) head model, which is subsequently rigged to an underlying parametric head model (e.g., FLAME) for animation. The inversion is jointly supervised using a sparse collection of upscaled 2D face renderings and corresponding depth maps, captured from diverse facial expressions and camera viewpoints, to ensure realism under dynamic facial motions. Experiments demonstrate that SuperHead generates avatars with fine-grained facial details under dynamic motions, significantly outperforming baseline methods in visual quality.</li>
<li><strong>摘要：</strong>创建高保真、可动画的 3D 头部对沉浸式应用至关重要，但常常受到低质量图像或视频源的阻碍，从而产生较差的 3D 重建效果。在本文中，我们介绍了 SuperHead，这是一种用于增强低分辨率、可动画 3D 头部头像的新颖框架。核心挑战在于合成高质量的几何体和纹理，同时确保动画过程中的 3D 和时间一致性并保留主体身份。尽管最近在图像、视频和基于 3D 的超分辨率 (SR) 方面取得了进展，但现有的 SR 技术不足以处理动态 3D 输入。为了解决这个问题，SuperHead 通过新颖的动态感知 3D 反演方案，利用预先训练的 3D 生成模型的丰富先验。该过程优化了生成模型的潜在表示，以生成超分辨率的 3D 高斯溅射 (3DGS) 头部模型，随后将其装配到底层参数化头部模型（例如 FLAME）以进行动画处理。使用从不同的面部表情和相机视点捕获的稀疏的放大的 2D 面部渲染和相应的深度图集合来联合监督反演，以确保动态面部运动下的真实感。实验表明，SuperHead 在动态运动下生成具有细粒度面部细节的化身，在视觉质量方面显着优于基线方法。</li>
</ul>

<h3>Title: Flow Matching for Offline Reinforcement Learning with Discrete Actions</h3>
<ul>
<li><strong>Authors: </strong>Fairoz Nower Khan, Nabuat Zaman Nahim, Ruiquan Huang, Haibo Yang, Peizhong Ju</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06138">https://arxiv.org/abs/2602.06138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06138">https://arxiv.org/pdf/2602.06138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06138]] Flow Matching for Offline Reinforcement Learning with Discrete Actions(https://arxiv.org/abs/2602.06138)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative policies based on diffusion models and flow matching have shown strong promise for offline reinforcement learning (RL), but their applicability remains largely confined to continuous action spaces. To address a broader range of offline RL settings, we extend flow matching to a general framework that supports discrete action spaces with multiple objectives. Specifically, we replace continuous flows with continuous-time Markov chains, trained using a Q-weighted flow matching objective. We then extend our design to multi-agent settings, mitigating the exponential growth of joint action spaces via a factorized conditional path. We theoretically show that, under idealized conditions, optimizing this objective recovers the optimal policy. Extensive experiments further demonstrate that our method performs robustly in practical scenarios, including high-dimensional control, multi-modal decision-making, and dynamically changing preferences over multiple objectives. Our discrete framework can also be applied to continuous-control problems through action quantization, providing a flexible trade-off between representational complexity and performance.</li>
<li><strong>摘要：</strong>基于扩散模型和流匹配的生成策略在离线强化学习（RL）方面显示出了强大的前景，但它们的适用性仍然主要局限于连续行动空间。为了解决更广泛的离线强化学习设置，我们将流匹配扩展到支持具有多个目标的离散动作空间的通用框架。具体来说，我们用连续时间马尔可夫链替换连续流，并使用 Q 加权流匹配目标进行训练。然后，我们将设计扩展到多智能体设置，通过因子化条件路径减轻联合动作空间的指数增长。我们从理论上证明，在理想条件下，优化该目标可以恢复最优策略。大量的实验进一步证明，我们的方法在实际场景中表现稳健，包括高维控制、多模态决策以及对多个目标的动态变化偏好。我们的离散框架还可以通过动作量化应用于连续控制问题，从而在表示复杂性和性能之间提供灵活的权衡。</li>
</ul>

<h3>Title: Latent Structure Emergence in Diffusion Models via Confidence-Based Filtering</h3>
<ul>
<li><strong>Authors: </strong>Wei Wei, Yizhou Zeng, Kuntian Chen, Sophie Langer, Mariia Seleznova, Hung-Hsu Chou</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06155">https://arxiv.org/abs/2602.06155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06155">https://arxiv.org/pdf/2602.06155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06155]] Latent Structure Emergence in Diffusion Models via Confidence-Based Filtering(https://arxiv.org/abs/2602.06155)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models rely on a high-dimensional latent space of initial noise seeds, yet it remains unclear whether this space contains sufficient structure to predict properties of the generated samples, such as their classes. In this work, we investigate the emergence of latent structure through the lens of confidence scores assigned by a pre-trained classifier to generated samples. We show that while the latent space appears largely unstructured when considering all noise realizations, restricting attention to initial noise seeds that produce high-confidence samples reveals pronounced class separability. By comparing class predictability across noise subsets of varying confidence and examining the class separability of the latent space, we find evidence of class-relevant latent structure that becomes observable only under confidence-based filtering. As a practical implication, we discuss how confidence-based filtering enables conditional generation as an alternative to guidance-based methods.</li>
<li><strong>摘要：</strong>扩散模型依赖于初始噪声种子的高维潜在空间，但仍不清楚该空间是否包含足够的结构来预测生成样本的属性（例如它们的类别）。在这项工作中，我们通过预先训练的分类器为生成的样本分配的置信度分数来研究潜在结构的出现。我们表明，虽然在考虑所有噪声实现时，潜在空间在很大程度上看起来是非结构化的，但将注意力限制在产生高置信度样本的初始噪声种子上，揭示了明显的类可分离性。通过比较不同置信度的噪声子集的类可预测性并检查潜在空间的类可分离性，我们找到了仅在基于置信度的过滤下才可观察到的类相关潜在结构的证据。作为实际意义，我们讨论基于置信度的过滤如何实现条件生成，作为基于指导的方法的替代方案。</li>
</ul>

<h3>Title: Driving with DINO: Vision Foundation Features as a Unified Bridge for Sim-to-Real Generation in Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Xuyang Chen, Conglang Zhang, Chuanheng Fu, Zihao Yang, Kaixuan Zhou, Yizhi Zhang, Jianan He, Yanfeng Zhang, Mingwei Sun, Zengmao Wang, Zhen Dong, Xiaoxiao Long, Liqiu Meng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06159">https://arxiv.org/abs/2602.06159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06159">https://arxiv.org/pdf/2602.06159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06159]] Driving with DINO: Vision Foundation Features as a Unified Bridge for Sim-to-Real Generation in Autonomous Driving(https://arxiv.org/abs/2602.06159)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Driven by the emergence of Controllable Video Diffusion, existing Sim2Real methods for autonomous driving video generation typically rely on explicit intermediate representations to bridge the domain gap. However, these modalities face a fundamental Consistency-Realism Dilemma. Low-level signals (e.g., edges, blurred images) ensure precise control but compromise realism by "baking in" synthetic artifacts, whereas high-level priors (e.g., depth, semantics, HDMaps) facilitate photorealism but lack the structural detail required for consistent guidance. In this work, we present Driving with DINO (DwD), a novel framework that leverages Vision Foundation Module (VFM) features as a unified bridge between the simulation and real-world domains. We first identify that these features encode a spectrum of information, from high-level semantics to fine-grained structure. To effectively utilize this, we employ Principal Subspace Projection to discard the high-frequency elements responsible for "texture baking," while concurrently introducing Random Channel Tail Drop to mitigate the structural loss inherent in rigid dimensionality reduction, thereby reconciling realism with control consistency. Furthermore, to fully leverage DINOv3's high-resolution capabilities for enhancing control precision, we introduce a learnable Spatial Alignment Module that adapts these high-resolution features to the diffusion backbone. Finally, we propose a Causal Temporal Aggregator employing causal convolutions to explicitly preserve historical motion context when integrating frame-wise DINO features, which effectively mitigates motion blur and guarantees temporal stability. Project page: this https URL</li>
<li><strong>摘要：</strong>在可控视频扩散出现的推动下，现有的自动驾驶视频生成 Sim2Real 方法通常依赖于显式中间表示来弥合域差距。然而，这些模式面临着根本的一致性-现实主义困境。低级信号（例如边缘、模糊图像）可确保精确控制，但会通过“烘焙”合成伪影而损害真实感，而高级先验信号（例如深度、语义、高清地图）有利于照片级真实感，但缺乏一致指导所需的结构细节。在这项工作中，我们提出了 Driving with DINO (DwD)，这是一种新颖的框架，利用视觉基础模块 (VFM) 功能作为模拟和现实世界之间的统一桥梁。我们首先确定这些特征编码一系列信息，从高级语义到细粒度结构。为了有效地利用这一点，我们采用主子空间投影来丢弃负责“纹理烘焙”的高频元素，同时引入随机通道尾部下降来减轻刚性降维中固有的结构损失，从而协调现实性与控制一致性。此外，为了充分利用 DINOv3 的高分辨率功能来提高控制精度，我们引入了一个可学习的空间对齐模块，该模块可将这些高分辨率特征适应扩散主干。最后，我们提出了一种因果时间聚合器，在集成逐帧 DINO 特征时，采用因果卷积来显式保留历史运动上下文，从而有效减轻运动模糊并保证时间稳定性。项目页面：此 https URL</li>
</ul>

<h3>Title: M3: High-fidelity Text-to-Image Generation via Multi-Modal, Multi-Agent and Multi-Round Visual Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Bangji Yang, Ruihan Guo, Jiajun Fan, Chaoran Cheng, Ge Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06166">https://arxiv.org/abs/2602.06166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06166">https://arxiv.org/pdf/2602.06166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06166]] M3: High-fidelity Text-to-Image Generation via Multi-Modal, Multi-Agent and Multi-Round Visual Reasoning(https://arxiv.org/abs/2602.06166)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative models have achieved impressive fidelity in text-to-image synthesis, yet struggle with complex compositional prompts involving multiple constraints. We introduce \textbf{M3 (Multi-Modal, Multi-Agent, Multi-Round)}, a training-free framework that systematically resolves these failures through iterative inference-time refinement. M3 orchestrates off-the-shelf foundation models in a robust multi-agent loop: a Planner decomposes prompts into verifiable checklists, while specialized Checker, Refiner, and Editor agents surgically correct constraints one at a time, with a Verifier ensuring monotonic improvement. Applied to open-source models, M3 achieves remarkable results on the challenging OneIG-EN benchmark, with our Qwen-Image+M3 surpassing commercial flagship systems including Imagen4 (0.515) and Seedream 3.0 (0.530), reaching state-of-the-art performance (0.532 overall). This demonstrates that intelligent multi-agent reasoning can elevate open-source models beyond proprietary alternatives. M3 also substantially improves GenEval compositional metrics, effectively doubling spatial reasoning performance on hardened test sets. As a plug-and-play module compatible with any pre-trained T2I model, M3 establishes a new paradigm for compositional generation without costly retraining.</li>
<li><strong>摘要：</strong>生成模型在文本到图像的合成中取得了令人印象深刻的保真度，但仍难以应对涉及多种约束的复杂合成提示。我们引入了 \textbf{M3（多模式、多代理、多轮）}，这是一种免训练框架，可以通过迭代推理时间细化系统地解决这些故障。 M3 在强大的多代理循环中编排现成的基础模型：规划器将提示分解为可验证的清单，而专门的检查器、精炼器和编辑器代理一次一次精确地纠正一个约束，而验证器则确保单调改进。应用于开源模型，M3在具有挑战性的OneIG-EN基准测试中取得了显着的成绩，我们的Qwen-Image+M3超越了包括Imagen4（0.515）和Seedream 3.0（0.530）在内的商业旗舰系统，达到了最先进的性能（总体为0.532）。这表明智能多代理推理可以使开源模型超越专有替代方案。 M3 还大幅改进了 GenEval 组合指标，有效地将强化测试集上的空间推理性能提高了一倍。作为与任何预先训练的 T2I 模型兼容的即插即用模块，M3 为组合生成建立了新的范例，而无需进行昂贵的重新训练。</li>
</ul>

<h3>Title: To 2:4 Sparsity and Beyond: Neuron-level Activation Function to Accelerate LLM Pre-Training</h3>
<ul>
<li><strong>Authors: </strong>Meghana Madhyastha, Daniel Haziza, Jesse Cai, Newsha Ardalani, Zhiqi Bu, Carole-Jean Wu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06183">https://arxiv.org/abs/2602.06183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06183">https://arxiv.org/pdf/2602.06183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06183]] To 2:4 Sparsity and Beyond: Neuron-level Activation Function to Accelerate LLM Pre-Training(https://arxiv.org/abs/2602.06183)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Trainings of Large Language Models are generally bottlenecked by matrix multiplications. In the Transformer architecture, a large portion of these operations happens in the Feed Forward Network (FFN), and this portion increases for larger models, up to 50% of the total pretraining floating point operations. We show that we can leverage hardware-accelerated sparsity to accelerate all matrix multiplications in the FFN, with 2:4 sparsity for weights and v:n:m (Venom) sparsity for activations. Our recipe relies on sparse training steps to accelerate a large part of the pretraining, associated with regular dense training steps towards the end. Overall, models trained with this approach exhibit the same performance on our quality benchmarks, and can speed up training end-to-end by 1.4 to 1.7x. This approach is applicable to all NVIDIA GPUs starting with the A100 generation, and is orthogonal to common optimization techniques, such as, quantization, and can also be applied to mixture-of-experts model architectures.</li>
<li><strong>摘要：</strong>大型语言模型的训练通常受到矩阵乘法的瓶颈。在 Transformer 架构中，这些操作的很大一部分发生在前馈网络 (FFN) 中，并且对于较大的模型，该部分会增加，高达预训练浮点操作总数的 50%。我们证明，我们可以利用硬件加速的稀疏性来加速 FFN 中的所有矩阵乘法，权重稀疏性为 2:4，激活稀疏性为 v:n:m (Venom)。我们的方案依靠稀疏训练步骤来加速大部分预训练，并与最后的常规密集训练步骤相关。总体而言，使用这种方法训练的模型在我们的质量基准上表现出相同的性能，并且可以将端到端训练速度提高 1.4 至 1.7 倍。此方法适用于从 A100 一代开始的所有 NVIDIA GPU，并且与量化等常见优化技术正交，也可应用于专家混合模型架构。</li>
</ul>

<h3>Title: Learning Rate Scaling across LoRA Ranks and Transfer to Full Finetuning</h3>
<ul>
<li><strong>Authors: </strong>Nan Chen, Soledad Villar, Soufiane Hayou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06204">https://arxiv.org/abs/2602.06204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06204">https://arxiv.org/pdf/2602.06204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06204]] Learning Rate Scaling across LoRA Ranks and Transfer to Full Finetuning(https://arxiv.org/abs/2602.06204)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Low-Rank Adaptation (LoRA) is a standard tool for parameter-efficient finetuning of large models. While it induces a small memory footprint, its training dynamics can be surprisingly complex as they depend on several hyperparameters such as initialization, adapter rank, and learning rate. In particular, it is unclear how the optimal learning rate scales with adapter rank, which forces practitioners to re-tune the learning rate whenever the rank is changed. In this paper, we introduce Maximal-Update Adaptation ($\mu$A), a theoretical framework that characterizes how the "optimal" learning rate should scale with model width and adapter rank to produce stable, non-vanishing feature updates under standard configurations. $\mu$A is inspired from the Maximal-Update Parametrization ($\mu$P) in pretraining. Our analysis leverages techniques from hyperparameter transfer and reveals that the optimal learning rate exhibits different scaling patterns depending on initialization and LoRA scaling factor. Specifically, we identify two regimes: one where the optimal learning rate remains roughly invariant across ranks, and another where it scales inversely with rank. We further identify a configuration that allows learning rate transfer from LoRA to full finetuning, drastically reducing the cost of learning rate tuning for full finetuning. Experiments across language, vision, vision--language, image generation, and reinforcement learning tasks validate our scaling rules and show that learning rates tuned on LoRA transfer reliably to full finetuning.</li>
<li><strong>摘要：</strong>低秩适应 (LoRA) 是对大型模型进行参数高效微调的标准工具。虽然它占用的内存很小，但它的训练动态可能非常复杂，因为它们取决于几个超参数，例如初始化、适配器等级和学习率。特别是，目前尚不清楚最佳学习率如何随适配器等级变化，这迫使从业者在等级改变时重新调整学习率。在本文中，我们介绍了最大更新适应（$\mu$A），这是一个理论框架，描述了“最佳”学习率如何随着模型宽度和适配器等级进行扩展，以在标准配置下产生稳定的、非消失的特征更新。 $\mu$A 的灵感来自于预训练中的最大更新参数化（$\mu$P）。我们的分析利用超参数传输技术，并揭示最佳学习率根据初始化和 LoRA 缩放因子表现出不同的缩放模式。具体来说，我们确定了两种制度：一种是最佳学习率在不同等级之间大致保持不变，另一种是与等级成反比。我们进一步确定了一种配置，允许学习率从 LoRA 转移到完全微调，从而大大降低完全微调的学习率调整成本。跨语言、视觉、视觉——语言、图像生成和强化学习任务的实验验证了我们的缩放规则，并表明在 LoRA 上调整的学习率可靠地转移到完全微调。</li>
</ul>

<h3>Title: ATEX-CF: Attack-Informed Counterfactual Explanations for Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Yu Zhang, Sean Bin Yang, Arijit Khan, Cuneyt Gurcan Akcora</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06240">https://arxiv.org/abs/2602.06240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06240">https://arxiv.org/pdf/2602.06240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06240]] ATEX-CF: Attack-Informed Counterfactual Explanations for Graph Neural Networks(https://arxiv.org/abs/2602.06240)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Counterfactual explanations offer an intuitive way to interpret graph neural networks (GNNs) by identifying minimal changes that alter a model's prediction, thereby answering "what must differ for a different outcome?". In this work, we propose a novel framework, ATEX-CF that unifies adversarial attack techniques with counterfactual explanation generation-a connection made feasible by their shared goal of flipping a node's prediction, yet differing in perturbation strategy: adversarial attacks often rely on edge additions, while counterfactual methods typically use deletions. Unlike traditional approaches that treat explanation and attack separately, our method efficiently integrates both edge additions and deletions, grounded in theory, leveraging adversarial insights to explore impactful counterfactuals. In addition, by jointly optimizing fidelity, sparsity, and plausibility under a constrained perturbation budget, our method produces instance-level explanations that are both informative and realistic. Experiments on synthetic and real-world node classification benchmarks demonstrate that ATEX-CF generates faithful, concise, and plausible explanations, highlighting the effectiveness of integrating adversarial insights into counterfactual reasoning for GNNs.</li>
<li><strong>摘要：</strong>反事实解释提供了一种直观的方法来解释图神经网络（GNN），通过识别改变模型预测的最小变化，从而回答“不同结果必须有什么不同？”。在这项工作中，我们提出了一个新颖的框架，ATEX-CF，它将对抗性攻击技术与反事实解释生成相结合，这种连接通过翻转节点预测的共同目标而变得可行，但扰动策略有所不同：对抗性攻击通常依赖于边缘添加，而反事实方法通常使用删除。与分别处理解释和攻击的传统方法不同，我们的方法以理论为基础，有效地整合了边缘添加和删除，利用对抗性见解来探索有影响力的反事实。此外，通过在受约束的扰动预算下联合优化保真度、稀疏性和合理性，我们的方法产生了既丰富又现实的实例级解释。对合成和现实世界节点分类基准的实验表明，ATEX-CF 生成了忠实、简洁且合理的解释，突出了将对抗性见解整合到 GNN 反事实推理中的有效性。</li>
</ul>

<h3>Title: REBEL: Hidden Knowledge Recovery via Evolutionary-Based Evaluation Loop</h3>
<ul>
<li><strong>Authors: </strong>Patryk Rybak, Paweł Batorski, Paul Swoboda, Przemysław Spurek</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06248">https://arxiv.org/abs/2602.06248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06248">https://arxiv.org/pdf/2602.06248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06248]] REBEL: Hidden Knowledge Recovery via Evolutionary-Based Evaluation Loop(https://arxiv.org/abs/2602.06248)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Machine unlearning for LLMs aims to remove sensitive or copyrighted data from trained models. However, the true efficacy of current unlearning methods remains uncertain. Standard evaluation metrics rely on benign queries that often mistake superficial information suppression for genuine knowledge removal. Such metrics fail to detect residual knowledge that more sophisticated prompting strategies could still extract. We introduce REBEL, an evolutionary approach for adversarial prompt generation designed to probe whether unlearned data can still be recovered. Our experiments demonstrate that REBEL successfully elicits ``forgotten'' knowledge from models that seemed to be forgotten in standard unlearning benchmarks, revealing that current unlearning methods may provide only a superficial layer of protection. We validate our framework on subsets of the TOFU and WMDP benchmarks, evaluating performance across a diverse suite of unlearning algorithms. Our experiments show that REBEL consistently outperforms static baselines, recovering ``forgotten'' knowledge with Attack Success Rates (ASRs) reaching up to 60% on TOFU and 93% on WMDP. We will make all code publicly available upon acceptance. Code is available at this https URL</li>
<li><strong>摘要：</strong>法学硕士的机器取消学习旨在从训练模型中删除敏感或受版权保护的数据。然而，当前遗忘方法的真正功效仍然不确定。标准评估指标依赖于良性查询，这些查询经常将表面信息抑制误认为是真正的知识删除。这些指标无法检测到更复杂的提示策略仍然可以提取的残留知识。我们引入了 REBEL，这是一种用于生成对抗性提示的进化方法，旨在探测未学习的数据是否仍然可以恢复。我们的实验表明，REBEL 成功地从模型中引出了“被遗忘”的知识，这些知识在标准的忘却基准中似乎被遗忘了，这表明当前的忘却方法可能只提供了表面的保护层。我们在 TOFU 和 WMDP 基准的子集上验证我们的框架，评估各种不学习算法的性能。我们的实验表明，REBEL 始终优于静态基线，恢复“遗忘”的知识，攻击成功率 (ASR) 在 TOFU 上达到 60%，在 WMDP 上达到 93%。我们将在接受后公开所有代码。代码可在此 https URL 获取</li>
</ul>

<h3>Title: GRP-Obliteration: Unaligning LLMs With a Single Unlabeled Prompt</h3>
<ul>
<li><strong>Authors: </strong>Mark Russinovich, Yanan Cai, Keegan Hines, Giorgio Severi, Blake Bullwinkel, Ahmed Salem</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06258">https://arxiv.org/abs/2602.06258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06258">https://arxiv.org/pdf/2602.06258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06258]] GRP-Obliteration: Unaligning LLMs With a Single Unlabeled Prompt(https://arxiv.org/abs/2602.06258)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Safety alignment is only as robust as its weakest failure mode. Despite extensive work on safety post-training, it has been shown that models can be readily unaligned through post-deployment fine-tuning. However, these methods often require extensive data curation and degrade model utility. In this work, we extend the practical limits of unalignment by introducing GRP-Obliteration (GRP-Oblit), a method that uses Group Relative Policy Optimization (GRPO) to directly remove safety constraints from target models. We show that a single unlabeled prompt is sufficient to reliably unalign safety-aligned models while largely preserving their utility, and that GRP-Oblit achieves stronger unalignment on average than existing state-of-the-art techniques. Moreover, GRP-Oblit generalizes beyond language models and can also unalign diffusion-based image generation systems. We evaluate GRP-Oblit on six utility benchmarks and five safety benchmarks across fifteen 7-20B parameter models, spanning instruct and reasoning models, as well as dense and MoE architectures. The evaluated model families include GPT-OSS, distilled DeepSeek, Gemma, Llama, Ministral, and Qwen.</li>
<li><strong>摘要：</strong>安全对准的稳健性取决于其最弱的故障模式。尽管在安全后培训方面进行了大量工作，但事实证明，通过部署后微调，模型可以很容易地取消对齐。然而，这些方法通常需要大量的数据管理并降低模型的实用性。在这项工作中，我们通过引入 GRP-Obliteration (GRP-Oblit) 来扩展未对齐的实际限制，这是一种使用组相对策略优化 (GRPO) 直接从目标模型中消除安全约束的方法。我们证明，单个未标记的提示足以可靠地取消对齐安全对齐模型，同时在很大程度上保留其实用性，并且 GRP-Oblit 平均比现有的最先进技术实现更强的对齐。此外，GRP-Oblit 的推广超越了语言模型，还可以取消基于扩散的图像生成系统。我们根据 15 个 7-20B 参数模型的 6 个实用基准和 5 个安全基准来评估 GRP-Oblit，涵盖指令和推理模型以及密集和 MoE 架构。评估的模型系列包括 GPT-OSS、distilled DeepSeek、Gemma、Llama、Ministral 和 Qwen。</li>
</ul>

<h3>Title: SOCKET: SOft Collison Kernel EsTimator for Sparse Attention</h3>
<ul>
<li><strong>Authors: </strong>Sahil Joshi, Agniva Chowdhury, Wyatt Bellinger, Amar Kanakamedala, Ekam Singh, Hoang Anh Duy Le, Aditya Desai, Anshumali Shrivastava</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06283">https://arxiv.org/abs/2602.06283</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06283">https://arxiv.org/pdf/2602.06283</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06283]] SOCKET: SOft Collison Kernel EsTimator for Sparse Attention(https://arxiv.org/abs/2602.06283)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Exploiting sparsity during long-context inference is central to scaling large language models, as attention dominates the cost of autoregressive decoding. Sparse attention reduces this cost by restricting computation to a subset of tokens, but its effectiveness depends critically on efficient scoring and selection of relevant tokens at inference time. We revisit Locality-Sensitive Hashing (LSH) as a sparsification primitive and introduce SOCKET, a SOft Collision Kernel EsTimator that replaces hard bucket matches with probabilistic, similarity-aware aggregation. Our key insight is that hard LSH produces discrete collision signals and is therefore poorly suited for ranking. In contrast, soft LSH aggregates graded collision evidence across hash tables, preserving the stability of relative ordering among the true top-$k$ tokens. This transformation elevates LSH from a candidate-generation heuristic to a principled and mathematically grounded scoring kernel for sparse attention. Leveraging this property, SOCKET enables efficient token selection without ad-hoc voting mechanism, and matches or surpasses established sparse attention baselines across multiple long-context benchmarks using diverse set of models. With a custom CUDA kernel for scoring keys and a Flash Decode Triton backend for sparse attention, SOCKET achieves up to 1.5$\times$ higher throughput than FlashAttention, making it an effective tool for long-context inference. Code is open-sourced at this https URL.</li>
<li><strong>摘要：</strong>在长上下文推理过程中利用稀疏性对于扩展大型语言模型至关重要，因为注意力主导着自回归解码的成本。稀疏注意力通过将计算限制在令牌子集来降低这种成本，但其有效性关键取决于推理时相关令牌的有效评分和选择。我们重新审视局部敏感哈希 (LSH) 作为稀疏化原语，并引入 SOCKET，这是一种 SOft 冲突内核估算器，它用概率、相似性感知聚合取代硬桶匹配。我们的主要见解是，硬 LSH 会产生离散的冲突信号，因此不太适合排名。相比之下，软 LSH 聚合了哈希表中的分级碰撞证据，从而保持了真正的 top-$k$ 令牌之间相对排序的稳定性。这种转变将 LSH 从候选生成启发式提升为有原则且以数学为基础的稀疏注意力评分内核。利用这一特性，SOCKET 无需临时投票机制即可实现高效的代币选择，并使用不同的模型集在多个长上下文基准上匹配或超越既定的稀疏注意力基线。借助用于对键进行评分的自定义 CUDA 内核和用于稀疏注意力的 Flash Decode Triton 后端，SOCKET 的吞吐量比 FlashAttention 高出 1.5 倍，使其成为长上下文推理的有效工具。代码在此 https URL 上开源。</li>
</ul>

<h3>Title: Toward generative machine learning for boosting ensembles of climate simulations</h3>
<ul>
<li><strong>Authors: </strong>Parsa Gooya, Reinel Sospedra-Alfonso, Johannes Exenberger</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.ao-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06287">https://arxiv.org/abs/2602.06287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06287">https://arxiv.org/pdf/2602.06287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06287]] Toward generative machine learning for boosting ensembles of climate simulations(https://arxiv.org/abs/2602.06287)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Accurately quantifying uncertainty in predictions and projections arising from irreducible internal climate variability is critical for informed decision making. Such uncertainty is typically assessed using ensembles produced with physics based climate models. However, computational constraints impose a trade off between generating the large ensembles required for robust uncertainty estimation and increasing model resolution to better capture fine scale dynamics. Generative machine learning offers a promising pathway to alleviate these constraints. We develop a conditional Variational Autoencoder (cVAE) trained on a limited sample of climate simulations to generate arbitrary large ensembles. The approach is applied to output from monthly CMIP6 historical and future scenario experiments produced with the Canadian Centre for Climate Modelling and Analysis' (CCCma's) Earth system model CanESM5. We show that the cVAE model learns the underlying distribution of the data and generates physically consistent samples that reproduce realistic low and high moment statistics, including extremes. Compared with more sophisticated generative architectures, cVAEs offer a mathematically transparent, interpretable, and computationally efficient framework. Their simplicity lead to some limitations, such as overly smooth outputs, spectral bias, and underdispersion, that we discuss along with strategies to mitigate them. Specifically, we show that incorporating output noise improves the representation of climate relevant multiscale variability, and we propose a simple method to achieve this. Finally, we show that cVAE-enhanced ensembles capture realistic global teleconnection patterns, even under climate conditions absent from the training data.</li>
<li><strong>摘要：</strong>准确量化不可减少的内部气候变化引起的预测和预测的不确定性对于做出明智的决策至关重要。这种不确定性通常是使用基于物理的气候模型产生的集合来评估的。然而，计算限制在生成稳健的不确定性估计所需的大型系综和提高模型分辨率以更好地捕获精细尺度动态之间进行了权衡。生成机器学习为缓解这些限制提供了一条有前途的途径。我们开发了一种条件变分自动编码器（cVAE），在有限的气候模拟样本上进行训练，以生成任意大型集合。该方法适用于加拿大气候建模与分析中心 (CCCma) 地球系统模型 CanESM5 制作的每月 CMIP6 历史和未来情景实验的输出。我们表明，cVAE 模型可以学习数据的基本分布，并生成物理上一致的样本，这些样本可以再现真实的低矩和高矩统计数据，包括极值。与更复杂的生成架构相比，cVAE 提供了一个数学上透明、可解释且计算高效的框架。它们的简单性导致了一些限制，例如输出过于平滑、光谱偏差和色散不足，我们将讨论这些限制以及缓解这些限制的策略。具体来说，我们表明，合并输出噪声可以改善气候相关的多尺度变异性的表示，并且我们提出了一种简单的方法来实现这一目标。最后，我们表明，即使在训练数据不存在的气候条件下，cVAE 增强型集合也能捕获现实的全球遥相关模式。</li>
</ul>

<h3>Title: Uncertainty-Aware 4D Gaussian Splatting for Monocular Occluded Human Rendering</h3>
<ul>
<li><strong>Authors: </strong>Weiquan Wang, Feifei Shao, Lin Li, Zhen Wang, Jun Xiao, Long Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06343">https://arxiv.org/abs/2602.06343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06343">https://arxiv.org/pdf/2602.06343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06343]] Uncertainty-Aware 4D Gaussian Splatting for Monocular Occluded Human Rendering(https://arxiv.org/abs/2602.06343)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>High-fidelity rendering of dynamic humans from monocular videos typically degrades catastrophically under occlusions. Existing solutions incorporate external priors-either hallucinating missing content via generative models, which induces severe temporal flickering, or imposing rigid geometric heuristics that fail to capture diverse appearances. To this end, we reformulate the task as a Maximum A Posteriori estimation problem under heteroscedastic observation noise. In this paper, we propose U-4DGS, a framework integrating a Probabilistic Deformation Network and a Double Rasterization pipeline. This architecture renders pixel-aligned uncertainty maps that act as an adaptive gradient modulator, automatically attenuating artifacts from unreliable observations. Furthermore, to prevent geometric drift in regions lacking reliable visual cues, we enforce Confidence-Aware Regularizations, which leverage the learned uncertainty to selectively propagate spatial-temporal validity. Extensive experiments on ZJU-MoCap and OcMotion demonstrate that U-4DGS achieves SOTA rendering fidelity and robustness.</li>
<li><strong>摘要：</strong>单眼视频中动态人体的高保真渲染通常会在遮挡情况下发生灾难性的退化。现有的解决方案结合了外部先验——要么通过生成模型幻觉缺失的内容，这会导致严重的时间闪烁，要么强加严格的几何启发式，无法捕捉不同的外观。为此，我们将任务重新表述为异方差观测噪声下的最大后验估计问题。在本文中，我们提出了 U-4DGS，一个集成概率变形网络和双光栅化管道的框架。该架构渲染像素对齐的不确定性图，充当自适应梯度调制器，自动衰减不可靠观测带来的伪影。此外，为了防止缺乏可靠视觉线索的区域中的几何漂移，我们强制执行置信感知正则化，它利用学习到的不确定性来选择性地传播时空有效性。在 ZJU-MoCap 和 OcMotion 上进行的大量实验表明，U-4DGS 实现了 SOTA 渲染保真度和鲁棒性。</li>
</ul>

<h3>Title: FlowConsist: Make Your Flow Consistent with Real Trajectory</h3>
<ul>
<li><strong>Authors: </strong>Tianyi Zhang, Chengcheng Liu, Jinwei Chen, Chun-Le Guo, Chongyi Li, Ming-Ming Cheng, Bo Li, Peng-Tao Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06346">https://arxiv.org/abs/2602.06346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06346">https://arxiv.org/pdf/2602.06346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06346]] FlowConsist: Make Your Flow Consistent with Real Trajectory(https://arxiv.org/abs/2602.06346)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Fast flow models accelerate the iterative sampling process by learning to directly predict ODE path integrals, enabling one-step or few-step generation. However, we argue that current fast-flow training paradigms suffer from two fundamental issues. First, conditional velocities constructed from randomly paired noise-data samples introduce systematic trajectory drift, preventing models from following a consistent ODE path. Second, the model's approximation errors accumulate over time steps, leading to severe deviations across long time intervals. To address these issues, we propose FlowConsist, a training framework designed to enforce trajectory consistency in fast flows. We propose a principled alternative that replaces conditional velocities with the marginal velocities predicted by the model itself, aligning optimization with the true trajectory. To further address error accumulation over time steps, we introduce a trajectory rectification strategy that aligns the marginal distributions of generated and real samples at every time step along the trajectory. Our method establishes a new state-of-the-art on ImageNet 256$\times$256, achieving an FID of 1.52 with only 1 sampling step.</li>
<li><strong>摘要：</strong>快速流模型通过学习直接预测 ODE 路径积分来加速迭代采样过程，从而实现一步或几步生成。然而，我们认为当前的快速流程训练范式存在两个基本问题。首先，由随机配对的噪声数据样本构建的条件速度会引入系统轨迹漂移，从而阻止模型遵循一致的 ODE 路径。其次，模型的近似误差随着时间步长的增加而累积，导致长时间间隔内出现严重偏差。为了解决这些问题，我们提出了 FlowConsist，这是一个旨在增强快速流中轨迹一致性的训练框架。我们提出了一种有原则的替代方案，用模型本身预测的边际速度代替条件速度，使优化与真实轨迹保持一致。为了进一步解决随时间步长的误差累积问题，我们引入了一种轨迹校正策略，该策略在沿着轨迹的每个时间步长处对齐生成样本和真实样本的边缘分布。我们的方法在 ImageNet 256$\times$256 上建立了新的最先进技术，仅用 1 个采样步骤即可实现 1.52 的 FID。</li>
</ul>

<h3>Title: Di3PO -- Diptych Diffusion DPO for Targeted Improvements in Image</h3>
<ul>
<li><strong>Authors: </strong>Sanjana Reddy (1), Ishaan Malhi (2), Sally Ma (2), Praneet Dutta (2) ((1) Google, (2) Google DeepMind)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06355">https://arxiv.org/abs/2602.06355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06355">https://arxiv.org/pdf/2602.06355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06355]] Di3PO -- Diptych Diffusion DPO for Targeted Improvements in Image(https://arxiv.org/abs/2602.06355)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Existing methods for preference tuning of text-to-image (T2I) diffusion models often rely on computationally expensive generation steps to create positive and negative pairs of images. These approaches frequently yield training pairs that either lack meaningful differences, are expensive to sample and filter, or exhibit significant variance in irrelevant pixel regions, thereby degrading training efficiency. To address these limitations, we introduce "Di3PO", a novel method for constructing positive and negative pairs that isolates specific regions targeted for improvement during preference tuning, while keeping the surrounding context in the image stable. We demonstrate the efficacy of our approach by applying it to the challenging task of text rendering in diffusion models, showcasing improvements over baseline methods of SFT and DPO.</li>
<li><strong>摘要：</strong>现有的文本到图像 (T2I) 扩散模型偏好调整方法通常依赖于计算成本高昂的生成步骤来创建正负图像对。这些方法经常产生的训练对要么缺乏有意义的差异，采样和过滤成本昂贵，要么在不相关的像素区域中表现出显着的差异，从而降低了训练效率。为了解决这些限制，我们引入了“Di3PO”，这是一种构建正负对的新方法，可以在偏好调整期间隔离需要改进的特定区域，同时保持图像中周围环境的稳定。我们通过将我们的方法应用于扩散模型中文本渲染的挑战性任务来证明该方法的有效性，展示了相对于 SFT 和 DPO 基线方法的改进。</li>
</ul>

<h3>Title: Generating High-quality Privacy-preserving Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>David Yavo, Richard Khoury, Christophe Pere, Sadoune Ait Kaci Azzou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06390">https://arxiv.org/abs/2602.06390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06390">https://arxiv.org/pdf/2602.06390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06390]] Generating High-quality Privacy-preserving Synthetic Data(https://arxiv.org/abs/2602.06390)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Synthetic tabular data enables sharing and analysis of sensitive records, but its practical deployment requires balancing distributional fidelity, downstream utility, and privacy protection. We study a simple, model agnostic post processing framework that can be applied on top of any synthetic data generator to improve this trade off. First, a mode patching step repairs categories that are missing or severely underrepresented in the synthetic data, while largely preserving learned dependencies. Second, a k nearest neighbor filter replaces synthetic records that lie too close to real data points, enforcing a minimum distance between real and synthetic samples. We instantiate this framework for two neural generative models for tabular data, a feed forward generator and a variational autoencoder, and evaluate it on three public datasets covering credit card transactions, cardiovascular health, and census based income. We assess marginal and joint distributional similarity, the performance of models trained on synthetic data and evaluated on real data, and several empirical privacy indicators, including nearest neighbor distances and attribute inference attacks. With moderate thresholds between 0.2 and 0.35, the post processing reduces divergence between real and synthetic categorical distributions by up to 36 percent and improves a combined measure of pairwise dependence preservation by 10 to 14 percent, while keeping downstream predictive performance within about 1 percent of the unprocessed baseline. At the same time, distance based privacy indicators improve and the success rate of attribute inference attacks remains largely unchanged. These results provide practical guidance for selecting thresholds and applying post hoc repairs to improve the quality and empirical privacy of synthetic tabular data, while complementing approaches that provide formal differential privacy guarantees.</li>
<li><strong>摘要：</strong>合成表格数据可以共享和分析敏感记录，但其实际部署需要平衡分布保真度、下游效用和隐私保护。我们研究了一个简单的、与模型无关的后处理框架，该框架可以应用于任何合成数据生成器之上，以改善这种权衡。首先，模式修补步骤修复合成数据中缺失或严重不足的类别，同时在很大程度上保留学习到的依赖关系。其次，k 最近邻过滤器替换了距离真实数据点太近的合成记录，强制真实样本和合成样本之间的最小距离。我们为表格数据的两个神经生成模型（前馈生成器和变分自动编码器）实例化了该框架，并在涵盖信用卡交易、心血管健康和基于人口普查的收入的三个公共数据集上对其进行了评估。我们评估边际和联合分布相似性、在合成数据上训练并在真实数据上评估的模型的性能，以及几个经验隐私指标，包括最近邻距离和属性推断攻击。使用 0.2 至 0.35 之间的适度阈值，后处理可将真实分类分布与合成分类分布之间的差异减少高达 36%，并将成对依赖性保留的综合测量提高 10% 至 14%，同时将下游预测性能保持在未处理基线的约 1% 以内。与此同时，基于距离的隐私指标得到改善，属性推断攻击的成功率基本保持不变。这些结果为选择阈值和应用事后修复来提高合成表格数据的质量和经验隐私提供了实用指导，同时补充了提供正式差异隐私保证的方法。</li>
</ul>

<h3>Title: Adaptive Protein Tokenization</h3>
<ul>
<li><strong>Authors: </strong>Rohit Dilip, Ayush Varshney, David Van Valen</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06418">https://arxiv.org/abs/2602.06418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06418">https://arxiv.org/pdf/2602.06418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06418]] Adaptive Protein Tokenization(https://arxiv.org/abs/2602.06418)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Tokenization is a promising path to multi-modal models capable of jointly understanding protein sequences, structure, and function. Existing protein structure tokenizers create tokens by pooling information from local neighborhoods, an approach that limits their performance on generative and representation tasks. In this work, we present a method for global tokenization of protein structures in which successive tokens contribute increasing levels of detail to a global representation. This change resolves several issues with generative models based on local protein tokenization: it mitigates error accumulation, provides embeddings without sequence-reduction operations, and allows task-specific adaptation of a tokenized sequence's information content. We validate our method on reconstruction, generative, and representation tasks and demonstrate that it matches or outperforms existing models based on local protein structure tokenizers. We show how adaptive tokens enable inference criteria based on information content, which boosts designability. We validate representations generated from our tokenizer on CATH classification tasks and demonstrate that non-linear probing on our tokenized sequences outperforms equivalent probing on representations from other tokenizers. Finally, we demonstrate how our method supports zero-shot protein shrinking and affinity maturation.</li>
<li><strong>摘要：</strong>标记化是实现能够共同理解蛋白质序列、结构和功能的多模式模型的一条有前途的途径。现有的蛋白质结构标记器通过汇集来自本地社区的信息来创建标记，这种方法限制了它们在生成和表示任务上的性能。在这项工作中，我们提出了一种蛋白质结构全局标记化的方法，其中连续的标记为全局表示提供了越来越多的细节水平。这一变化解决了基于本地蛋白质标记化的生成模型的几个问题：它减少了错误积累，提供了无需序列缩减操作的嵌入，并允许对标记化序列的信息内容进行特定于任务的调整。我们在重建、生成和表示任务上验证了我们的方法，并证明它匹配或优于基于局部蛋白质结构标记器的现有模型。我们展示了自适应令牌如何基于信息内容启用推理标准，从而提高可设计性。我们在 CATH 分类任务上验证了我们的分词器生成的表示，并证明对我们的分词序列的非线性探测优于对其他分词器表示的等效探测。最后，我们展示了我们的方法如何支持零次蛋白质收缩和亲和力成熟。</li>
</ul>

<h3>Title: Learning Human Visual Attention on 3D Surfaces through Geometry-Queried Semantic Priors</h3>
<ul>
<li><strong>Authors: </strong>Soham Pahari, Sandeep C. Kumain</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06419">https://arxiv.org/abs/2602.06419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06419">https://arxiv.org/pdf/2602.06419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06419]] Learning Human Visual Attention on 3D Surfaces through Geometry-Queried Semantic Priors(https://arxiv.org/abs/2602.06419)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Human visual attention on three-dimensional objects emerges from the interplay between bottom-up geometric processing and top-down semantic recognition. Existing 3D saliency methods rely on hand-crafted geometric features or learning-based approaches that lack semantic awareness, failing to explain why humans fixate on semantically meaningful but geometrically unremarkable regions. We introduce SemGeo-AttentionNet, a dual-stream architecture that explicitly formalizes this dichotomy through asymmetric cross-modal fusion, leveraging diffusion-based semantic priors from geometry-conditioned multi-view rendering and point cloud transformers for geometric processing. Cross-attention ensures geometric features query semantic content, enabling bottom-up distinctiveness to guide top-down retrieval. We extend our framework to temporal scanpath generation through reinforcement learning, introducing the first formulation respecting 3D mesh topology with inhibition-of-return dynamics. Evaluation on SAL3D, NUS3D and 3DVA datasets demonstrates substantial improvements, validating how cognitively motivated architectures effectively model human visual attention on three-dimensional surfaces.</li>
<li><strong>摘要：</strong>人类对三维物体的视觉注意力源于自下而上的几何处理和自上而下的语义识别之间的相互作用。现有的 3D 显着性方法依赖于手工制作的几何特征或基于学习的方法，缺乏语义意识，无法解释为什么人类会关注语义上有意义但几何上不起眼的区域。我们引入了 SemGeo-AttentionNet，这是一种双流架构，它通过非对称跨模态融合明确地形式化了这种二分法，利用来自几何条件多视图渲染的基于扩散的语义先验和用于几何处理的点云变换器。交叉注意力确保几何特征查询语义内容，从而实现自下而上的独特性来指导自上而下的检索。我们通过强化学习将我们的框架扩展到时间扫描路径生成，引入第一个考虑具有返回动态抑制的 3D 网格拓扑的公式。对 SAL3D、NUS3D 和 3DVA 数据集的评估表明了显着的改进，验证了认知驱动架构如何有效地模拟三维表面上的人类视觉注意力。</li>
</ul>

<h3>Title: Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO</h3>
<ul>
<li><strong>Authors: </strong>Yunze Tong, Mushui Liu, Canyu Zhao, Wanggui He, Shiyi Zhang, Hongwei Zhang, Peng Zhang, Jinlong Liu, Ju Huang, Jiamang Wang, Hao Jiang, Pipei Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06422">https://arxiv.org/abs/2602.06422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06422">https://arxiv.org/pdf/2602.06422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06422]] Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO(https://arxiv.org/abs/2602.06422)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Deploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TP-GRPO), a GRPO framework that alleviates step-wise reward sparsity and explicitly models long-term effects within the denoising trajectory. TP-GRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing a dense, step-aware learning signal that better isolates each denoising action's "pure" effect, and (ii) it identifies turning points-steps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trend-and assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameter-free. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at this https URL.</li>
<li><strong>摘要：</strong>事实证明，在流匹配模型上部署 GRPO 对于文本到图像的生成非常有效。然而，现有的范例通常将基于结果的奖励传播到所有先前的去噪步骤，而不区分每个步骤的局部效果。此外，当前的分组排序主要比较匹配时间步长的轨迹，并忽略轨迹内依赖性，其中某些早期去噪动作可以通过延迟的隐式交互影响后来的状态。我们提出了 TurningPoint-GRPO (TP-GRPO)，这是一种 GRPO 框架，可以缓解逐步奖励稀疏性并明确模拟去噪轨迹内的长期影响。 TP-GRPO 做出了两项关键创新：（i）它用步骤级增量奖励取代了基于结果的奖励，提供了密集的、步骤感知的学习信号，可以更好地隔离每个去噪操作的“纯粹”效果；（ii）它识别转折点 - 翻转局部奖励趋势并使后续奖励演变与整体轨迹趋势一致的步骤 - 并为这些操作分配一个聚合的长期奖励以捕获其延迟影响。仅通过增量奖励的符号变化即可检测转折点，从而使 TP-GRPO 高效且无超参数。大量实验还表明，TP-GRPO 更有效地利用奖励信号并持续提高发电量。演示代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Bridging the Indoor-Outdoor Gap: Vision-Centric Instruction-Guided Embodied Navigation for the Last Meters</h3>
<ul>
<li><strong>Authors: </strong>Yuxiang Zhao, Yirong Yang, Yanqing Zhu, Yanfen Shen, Chiyu Wang, Zhining Gu, Pei Shi, Wei Guo, Mu Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06427">https://arxiv.org/abs/2602.06427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06427">https://arxiv.org/pdf/2602.06427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06427]] Bridging the Indoor-Outdoor Gap: Vision-Centric Instruction-Guided Embodied Navigation for the Last Meters(https://arxiv.org/abs/2602.06427)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Embodied navigation holds significant promise for real-world applications such as last-mile delivery. However, most existing approaches are confined to either indoor or outdoor environments and rely heavily on strong assumptions, such as access to precise coordinate systems. While current outdoor methods can guide agents to the vicinity of a target using coarse-grained localization, they fail to enable fine-grained entry through specific building entrances, critically limiting their utility in practical deployment scenarios that require seamless outdoor-to-indoor transitions. To bridge this gap, we introduce a novel task: out-to-in prior-free instruction-driven embodied navigation. This formulation explicitly eliminates reliance on accurate external priors, requiring agents to navigate solely based on egocentric visual observations guided by instructions. To tackle this task, we propose a vision-centric embodied navigation framework that leverages image-based prompts to drive decision-making. Additionally, we present the first open-source dataset for this task, featuring a pipeline that integrates trajectory-conditioned video synthesis into the data generation process. Through extensive experiments, we demonstrate that our proposed method consistently outperforms state-of-the-art baselines across key metrics including success rate and path efficiency.</li>
<li><strong>摘要：</strong>嵌入式导航对于最后一英里交付等现实应用具有重大前景。然而，大多数现有方法仅限于室内或室外环境，并且严重依赖于强有力的假设，例如访问精确的坐标系。虽然当前的室外方法可以使用粗粒度定位引导代理到达目标附近，但它们无法通过特定的建筑物入口实现细粒度的进入，这严重限制了它们在需要无缝室外到室内过渡的实际部署场景中的实用性。为了弥补这一差距，我们引入了一项新颖的任务：从外到内的无先验指令驱动的具体导航。这种表述明确消除了对准确的外部先验的依赖，要求智能体仅根据指令指导的以自我为中心的视觉观察进行导航。为了解决这一任务，我们提出了一个以视觉为中心的体现导航框架，利用基于图像的提示来推动决策。此外，我们还为此任务提供了第一个开源数据集，其特点是将轨迹条件视频合成集成到数据生成过程中的管道。通过广泛的实验，我们证明我们提出的方法在包括成功率和路径效率在内的关键指标上始终优于最先进的基线。</li>
</ul>

<h3>Title: ChatUMM: Robust Context Tracking for Conversational Interleaved Generation</h3>
<ul>
<li><strong>Authors: </strong>Wenxun Dai, Zhiyuan Zhao, Yule Zhong, Yiji Cheng, Jianwei Zhang, Linqing Wang, Shiyi Zhang, Yunlong Lin, Runze He, Fellix Song, Wayne Zhuang, Yong Liu, Haoji Zhang, Yansong Tang, Qinglin Lu, Chunyu Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06442">https://arxiv.org/abs/2602.06442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06442">https://arxiv.org/pdf/2602.06442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06442]] ChatUMM: Robust Context Tracking for Conversational Interleaved Generation(https://arxiv.org/abs/2602.06442)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Unified multimodal models (UMMs) have achieved remarkable progress yet remain constrained by a single-turn interaction paradigm, effectively functioning as solvers for independent requests rather than assistants in continuous dialogue. To bridge this gap, we present ChatUMM. As a conversational unified model, it excels at robust context tracking to sustain interleaved multimodal generation. ChatUMM derives its capabilities from two key innovations: an interleaved multi-turn training strategy that models serialized text-image streams as a continuous conversational flow, and a systematic conversational data synthesis pipeline. This pipeline transforms a diverse set of standard single-turn datasets into fluid dialogues through three progressive stages: constructing basic stateful dialogues, enforcing long-range dependency resolution via ``distractor'' turns with history-dependent query rewriting, and synthesizing naturally interleaved multimodal responses. Extensive evaluations demonstrate that ChatUMM achieves state-of-the-art performance among open-source unified models on visual understanding and instruction-guided editing benchmarks, while maintaining competitive fidelity in text-to-image generation. Notably, ChatUMM exhibits superior robustness in complex multi-turn scenarios, ensuring fluid, context-aware dialogues.</li>
<li><strong>摘要：</strong>统一多模态模型（UMM）已经取得了显着的进步，但仍然受到单轮交互范式的限制，有效地充当独立请求的解决者，而不是连续对话的助手。为了弥补这一差距，我们推出了 ChatUMM。作为一个会话统一模型，它擅长强大的上下文跟踪以维持交错的多模态生成。 ChatUMM 的功能源自两项关键创新：将序列化文本图像流建模为连续对话流的交错多轮训练策略，以及系统的对话数据合成管道。该管道通过三个渐进阶段将一组不同的标准单轮数据集转换为流畅的对话：构建基本的有状态对话，通过历史相关查询重写的“干扰器”轮次强制执行远程依赖解析，以及合成自然交错的多模式响应。广泛的评估表明，ChatUMM 在视觉理解和指令引导编辑基准方面实现了开源统一模型中最先进的性能，同时在文本到图像生成方面保持了有竞争力的保真度。值得注意的是，ChatUMM 在复杂的多轮场景中表现出卓越的鲁棒性，确保流畅的上下文感知对话。</li>
</ul>

<h3>Title: Exploring Specular Reflection Inconsistency for Generalizable Face Forgery Detection</h3>
<ul>
<li><strong>Authors: </strong>Hongyan Fei, Zexi Jia, Chuanwei Huang, Jinchao Zhang, Jie Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06452">https://arxiv.org/abs/2602.06452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06452">https://arxiv.org/pdf/2602.06452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06452]] Exploring Specular Reflection Inconsistency for Generalizable Face Forgery Detection(https://arxiv.org/abs/2602.06452)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Detecting deepfakes has become increasingly challenging as forgery faces synthesized by AI-generated methods, particularly diffusion models, achieve unprecedented quality and resolution. Existing forgery detection approaches relying on spatial and frequency features demonstrate limited efficacy against high-quality, entirely synthesized forgeries. In this paper, we propose a novel detection method grounded in the observation that facial attributes governed by complex physical laws and multiple parameters are inherently difficult to replicate. Specifically, we focus on illumination, particularly the specular reflection component in the Phong illumination model, which poses the greatest replication challenge due to its parametric complexity and nonlinear formulation. We introduce a fast and accurate face texture estimation method based on Retinex theory to enable precise specular reflection separation. Furthermore, drawing from the mathematical formulation of specular reflection, we posit that forgery evidence manifests not only in the specular reflection itself but also in its relationship with corresponding face texture and direct light. To address this issue, we design the Specular-Reflection-Inconsistency-Network (SRI-Net), incorporating a two-stage cross-attention mechanism to capture these correlations and integrate specular reflection related features with image features for robust forgery detection. Experimental results demonstrate that our method achieves superior performance on both traditional deepfake datasets and generative deepfake datasets, particularly those containing diffusion-generated forgery faces.</li>
<li><strong>摘要：</strong>随着人工智能生成的方法（特别是扩散模型）合成的伪造面孔达到了前所未有的质量和分辨率，检测深度伪造变得越来越具有挑战性。现有的基于空间和频率特征的伪造品检测方法对于高质量、完全合成的伪造品表现出有限的功效。在本文中，我们提出了一种新颖的检测方法，该方法基于以下观察：受复杂物理定律和多个参数控制的面部属性本质上难以复制。具体来说，我们关注照明，特别是 Phong 照明模型中的镜面反射分量，由于其参数复杂性和非线性公式，它带来了最大的复制挑战。我们引入了一种基于 Retinex 理论的快速准确的人脸纹理估计方法，以实现精确的镜面反射分离。此外，根据镜面反射的数学公式，我们假设伪造证据不仅体现在镜面反射本身，而且体现在它与相应的面部纹理和直射光的关系中。为了解决这个问题，我们设计了镜面反射不一致网络（SRI-Net），采用两阶段交叉注意机制来捕获这些相关性，并将镜面反射相关特征与图像特征相集成，以实现稳健的伪造检测。实验结果表明，我们的方法在传统的 Deepfake 数据集和生成 Deepfake 数据集上都取得了优异的性能，特别是那些包含扩散生成的伪造面孔的数据集。</li>
</ul>

<h3>Title: DreamHome-Pano: Design-Aware and Conflict-Free Panoramic Interior Generation</h3>
<ul>
<li><strong>Authors: </strong>Lulu Chen, Yijiang Hu, Yuanqing Liu, Yulong Li, Yue Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06494">https://arxiv.org/abs/2602.06494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06494">https://arxiv.org/pdf/2602.06494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06494]] DreamHome-Pano: Design-Aware and Conflict-Free Panoramic Interior Generation(https://arxiv.org/abs/2602.06494)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In modern interior design, the generation of personalized spaces frequently necessitates a delicate balance between rigid architectural structural constraints and specific stylistic preferences. However, existing multi-condition generative frameworks often struggle to harmonize these inputs, leading to "condition conflicts" where stylistic attributes inadvertently compromise the geometric precision of the layout. To address this challenge, we present DreamHome-Pano, a controllable panoramic generation framework designed for high-fidelity interior synthesis. Our approach introduces a Prompt-LLM that serves as a semantic bridge, effectively translating layout constraints and style references into professional descriptive prompts to achieve precise cross-modal alignment. To safeguard architectural integrity during the generative process, we develop a Conflict-Free Control architecture that incorporates structural-aware geometric priors and a multi-condition decoupling strategy, effectively suppressing stylistic interference from eroding the spatial layout. Furthermore, we establish a comprehensive panoramic interior benchmark alongside a multi-stage training pipeline, encompassing progressive Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). Experimental results demonstrate that DreamHome-Pano achieves a superior balance between aesthetic quality and structural consistency, offering a robust and professional-grade solution for panoramic interior visualization.</li>
<li><strong>摘要：</strong>在现代室内设计中，个性化空间的产生常常需要在严格的建筑结构约束和特定的风格偏好之间取得微妙的平衡。然而，现有的多条件生成框架通常难以协调这些输入，从而导致“条件冲突”，其中样式属性无意中损害了布局的几何精度。为了应对这一挑战，我们推出了 DreamHome-Pano，这是一种专为高保真室内合成而设计的可控全景生成框架。我们的方法引入了 Prompt-LLM 作为语义桥梁，有效地将布局约束和样式参考转换为专业的描述性提示，以实现精确的跨模式对齐。为了在生成过程中维护架构的完整性，我们开发了一种无冲突控制架构，该架构结合了结构感知的几何先验和多条件解耦策略，有效地抑制了风格干扰，从而侵蚀了空间布局。此外，我们建立了全面的全景室内基准以及多阶段训练管道，包括渐进式监督微调（SFT）和强化学习（RL）。实验结果表明，DreamHome-Pano 在美学质量和结构一致性之间实现了卓越的平衡，为全景室内可视化提供了强大且专业级的解决方案。</li>
</ul>

<h3>Title: Evolutionary Generation of Multi-Agent Systems</h3>
<ul>
<li><strong>Authors: </strong>Yuntong Hu, Matthew Trager, Yuting Zhang, Yi Zhang, Shuo Yang, Wei Xia, Stefano Soatto</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06511">https://arxiv.org/abs/2602.06511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06511">https://arxiv.org/pdf/2602.06511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06511]] Evolutionary Generation of Multi-Agent Systems(https://arxiv.org/abs/2602.06511)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language model (LLM)-based multi-agent systems (MAS) show strong promise for complex reasoning, planning, and tool-augmented tasks, but designing effective MAS architectures remains labor-intensive, brittle, and hard to generalize. Existing automatic MAS generation methods either rely on code generation, which often leads to executability and robustness failures, or impose rigid architectural templates that limit expressiveness and adaptability. We propose Evolutionary Generation of Multi-Agent Systems (EvoMAS), which formulates MAS generation as structured configuration generation. EvoMAS performs evolutionary generation in configuration space. Specifically, EvoMAS selects initial configurations from a pool, applies feedback-conditioned mutation and crossover guided by execution traces, and iteratively refines both the candidate pool and an experience memory. We evaluate EvoMAS on diverse benchmarks, including BBEH, SWE-Bench, and WorkBench, covering reasoning, software engineering, and tool-use tasks. EvoMAS consistently improves task performance over both human-designed MAS and prior automatic MAS generation methods, while producing generated systems with higher executability and runtime robustness. EvoMAS outperforms the agent evolution method EvoAgent by +10.5 points on BBEH reasoning and +7.1 points on WorkBench. With Claude-4.5-Sonnet, EvoMAS also reaches 79.1% on SWE-Bench-Verified, matching the top of the leaderboard.</li>
<li><strong>摘要：</strong>基于大型语言模型 (LLM) 的多智能体系统 (MAS) 在复杂推理、规划和工具增强任务方面显示出强大的前景，但设计有效的 MAS 架构仍然是劳动密集型、脆弱且难以推广。现有的自动 MAS 生成方法要么依赖于代码生成，这通常会导致可执行性和鲁棒性失败，要么强加严格的架构模板，限制表达性和适应性。我们提出多智能体系统进化生成（EvoMAS），它将 MAS 生成表述为结构化配置生成。 EvoMAS 在配置空间中执行进化生成。具体来说，EvoMAS 从池中选择初始配置，应用由执行轨迹引导的反馈条件突变和交叉，并迭代地细化候选池和经验记忆。我们根据不同的基准评估 EvoMAS，包括 BBEH、SWE-Bench 和 WorkBench，涵盖推理、软件工程和工具使用任务。与人工设计的 MAS 和先前的自动 MAS 生成方法相比，EvoMAS 不断提高任务性能，同时生成具有更高可执行性和运行时鲁棒性的生成系统。 EvoMAS 在 BBEH 推理上比智能体进化方法 EvoAgent 高出 10.5 分，在 WorkBench 上高出 7.1 分。凭借 Claude-4.5-Sonnet，EvoMAS 在 SWE-Bench-Verified 上也达到了 79.1%，与排行榜名列前茅。</li>
</ul>

<h3>Title: NECromancer: Breathing Life into Skeletons via BVH Animation</h3>
<ul>
<li><strong>Authors: </strong>Mingxi Xu, Qi Wang, Zhengyu Wen, Phong Dao Thien, Zhengyu Li, Ning Zhang, Xiaoyu He, Wei Zhao, Kehong Gong, Mingyuan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06548">https://arxiv.org/abs/2602.06548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06548">https://arxiv.org/pdf/2602.06548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06548]] NECromancer: Breathing Life into Skeletons via BVH Animation(https://arxiv.org/abs/2602.06548)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Motion tokenization is a key component of generalizable motion models, yet most existing approaches are restricted to species-specific skeletons, limiting their applicability across diverse morphologies. We propose NECromancer (NEC), a universal motion tokenizer that operates directly on arbitrary BVH skeletons. NEC consists of three components: (1) an Ontology-aware Skeletal Graph Encoder (OwO) that encodes structural priors from BVH files, including joint semantics, rest-pose offsets, and skeletal topology, into skeletal embeddings; (2) a Topology-Agnostic Tokenizer (TAT) that compresses motion sequences into a universal, topology-invariant discrete representation; and (3) the Unified BVH Universe (UvU), a large-scale dataset aggregating BVH motions across heterogeneous skeletons. Experiments show that NEC achieves high-fidelity reconstruction under substantial compression and effectively disentangles motion from skeletal structure. The resulting token space supports cross-species motion transfer, composition, denoising, generation with token-based models, and text-motion retrieval, establishing a unified framework for motion analysis and synthesis across diverse morphologies. Demo page: this https URL</li>
<li><strong>摘要：</strong>运动标记化是可推广运动模型的关键组成部分，但大多数现有方法仅限于物种特定的骨架，限制了它们在不同形态中的适用性。我们提出了 NECromancer (NEC)，这是一种直接在任意 BVH 骨架上运行的通用运动标记器。 NEC 由三个组件组成：(1) 本体感知骨骼图编码器 (OwO)，将 BVH 文件中的结构先验（包括关节语义、静止姿势偏移和骨骼拓扑）编码为骨骼嵌入； (2) 与拓扑无关的分词器 (TAT)，将运动序列压缩为通用的、拓扑不变的离散表示； (3) 统一 BVH Universe (UvU)，一个聚合跨异构骨架的 BVH 运动的大规模数据集。实验表明，NEC在大幅压缩下实现了高保真重建，并有效地将运动与骨骼结构分离。由此产生的令牌空间支持跨物种运动转移、合成、去噪、基于令牌的模型生成以及文本运动检索，为跨不同形态的运动分析和合成建立了统一的框架。演示页面：此 https URL</li>
</ul>

<h3>Title: DiTS: Multimodal Diffusion Transformers Are Time Series Forecasters</h3>
<ul>
<li><strong>Authors: </strong>Haoran Zhang, Haixuan Liu, Yong Liu, Yunzhong Qiu, Yuxuan Wang, Jianmin Wang, Mingsheng Long</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06597">https://arxiv.org/abs/2602.06597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06597">https://arxiv.org/pdf/2602.06597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06597]] DiTS: Multimodal Diffusion Transformers Are Time Series Forecasters(https://arxiv.org/abs/2602.06597)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>While generative modeling on time series facilitates more capable and flexible probabilistic forecasting, existing generative time series models do not address the multi-dimensional properties of time series data well. The prevalent architecture of Diffusion Transformers (DiT), which relies on simplistic conditioning controls and a single-stream Transformer backbone, tends to underutilize cross-variate dependencies in covariate-aware forecasting. Inspired by Multimodal Diffusion Transformers that integrate textual guidance into video generation, we propose Diffusion Transformers for Time Series (DiTS), a general-purpose architecture that frames endogenous and exogenous variates as distinct modalities. To better capture both inter-variate and intra-variate dependencies, we design a dual-stream Transformer block tailored for time-series data, comprising a Time Attention module for autoregressive modeling along the temporal dimension and a Variate Attention module for cross-variate modeling. Unlike the common approach for images, which flattens 2D token grids into 1D sequences, our design leverages the low-rank property inherent in multivariate dependencies, thereby reducing computational costs. Experiments show that DiTS achieves state-of-the-art performance across benchmarks, regardless of the presence of future exogenous variate observations, demonstrating unique generative forecasting strengths over traditional deterministic deep forecasting models.</li>
<li><strong>摘要：</strong>虽然时间序列的生成建模有助于更强大和灵活的概率预测，但现有的生成时间序列模型并不能很好地解决时间序列数据的多维属性。扩散变压器 (DiT) 的流行架构依赖于简单的条件控制和单流变压器主干，往往在协变量感知预测中未充分利用跨变量依赖关系。受到将文本指导集成到视频生成中的多模态扩散变压器的启发，我们提出了时间序列扩散变压器（DiTS），这是一种通用架构，将内源变量和外源变量构建为不同的模态。为了更好地捕获变量间和变量内的依赖关系，我们设计了一个针对时间序列数据定制的双流 Transformer 模块，包括用于沿时间维度进行自回归建模的时间注意力模块和用于跨变量建模的变量注意力模块。与将 2D 标记网格扁平化为 1D 序列的常见图像方法不同，我们的设计利用了多元依赖关系中固有的低秩属性，从而降低了计算成本。实验表明，无论未来是否存在外生变量观测值，DiTS 都能在各个基准上实现最先进的性能，与传统的确定性深度预测模型相比，展现出独特的生成预测优势。</li>
</ul>

<h3>Title: The hidden risks of temporal resampling in clinical reinforcement learning</h3>
<ul>
<li><strong>Authors: </strong>Thomas Frost, Hrisheekesh Vaidya, Steve Harris</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06603">https://arxiv.org/abs/2602.06603</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06603">https://arxiv.org/pdf/2602.06603</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06603]] The hidden risks of temporal resampling in clinical reinforcement learning(https://arxiv.org/abs/2602.06603)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Offline reinforcement learning (ORL) has shown potential for improving decision-making in healthcare. However, contemporary research typically aggregates patient data into fixed time intervals, simplifying their mapping to standard ORL frameworks. The impact of these temporal manipulations on model safety and efficacy remains poorly understood. In this work, using both a gridworld navigation task and the UVA/Padova clinical diabetes simulator, we demonstrate that temporal resampling significantly degrades the performance of offline reinforcement learning algorithms during live deployment. We propose three mechanisms that drive this failure: (i) the generation of counterfactual trajectories, (ii) the distortion of temporal expectations, and (iii) the compounding of generalisation errors. Crucially, we find that standard off-policy evaluation metrics can fail to detect these drops in performance. Our findings reveal a fundamental risk in current healthcare ORL pipelines and emphasise the need for methods that explicitly handle the irregular timing of clinical decision-making.</li>
<li><strong>摘要：</strong>离线强化学习 (ORL) 已显示出改善医疗保健决策的潜力。然而，当代研究通常将患者数据聚合到固定的时间间隔中，从而简化其与标准 ORL 框架的映射。这些时间操作对模型安全性和有效性的影响仍然知之甚少。在这项工作中，我们使用 gridworld 导航任务和 UVA/Padova 临床糖尿病模拟器，证明时间重采样会显着降低实时部署期间离线强化学习算法的性能。我们提出了导致这种失败的三种机制：（i）反事实轨迹的生成，（ii）时间期望的扭曲，以及（iii）泛化错误的复合。至关重要的是，我们发现标准的离策略评估指标可能无法检测到这些性能下降。我们的研究结果揭示了当前医疗保健 ORL 管道中的根本风险，并强调需要明确处理临床决策的不规则时间的方法。</li>
</ul>

<h3>Title: The challenge of generating and evolving real-life like synthetic test data without accessing real-world raw data -- a Systematic Review</h3>
<ul>
<li><strong>Authors: </strong>Maj-Annika Tammisto, Faiz Ali Shah, Daniel Rodriguez, Dietmar Pfahl</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06609">https://arxiv.org/abs/2602.06609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06609">https://arxiv.org/pdf/2602.06609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06609]] The challenge of generating and evolving real-life like synthetic test data without accessing real-world raw data -- a Systematic Review(https://arxiv.org/abs/2602.06609)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Background: High-level system testing of applications that use data from e-Government services as input requires test data that is real-life-like but where the privacy of personal information is guaranteed. Applications with such strong requirement include information exchange between countries, medicine, banking, etc. This review aims to synthesize the current state-of-the-practice in this domain. Objectives: The objective of this Systematic Review is to identify existing approaches for creating and evolving synthetic test data without using real-life raw data. Methods: We followed well-known methodologies for conducting systematic literature reviews, including the ones from Kitchenham as well as guidelines for analysing the limitations of our review and its threats to validity. Results: A variety of methods and tools exist for creating privacy-preserving test data. Our search found 1,013 publications in IEEE Xplore, ACM Digital Library, and SCOPUS. We extracted data from 75 of those publications and identified 37 approaches that answer our research question partly. A common prerequisite for using these methods and tools is direct access to real-life data for data anonymization or synthetic test data generation. Nine existing synthetic test data generation approaches were identified that were closest to answering our research question. Nevertheless, further work would be needed to add the ability to evolve synthetic test data to the existing approaches. Conclusions: None of the publications really covered our requirements completely, only partially. Synthetic test data evolution is a field that has not received much attention from researchers but needs to be explored in Digital Government Solutions, especially since new legal regulations are being placed in force in many countries.</li>
<li><strong>摘要：</strong>背景：使用电子政务服务数据作为输入的应用程序的高级系统测试需要类似于现实生活但保证个人信息隐私的测试数据。具有如此强烈要求的应用包括国家之间的信息交换、医学、银行等。本次综述旨在综合该领域的当前实践状况。目标：本次系统审查的目的是确定在不使用现实生活中的原始数据的情况下创建和发展合成测试数据的现有方法。方法：我们遵循众所周知的方法进行系统文献综述，包括基奇汉姆的方法以及分析我们综述的局限性及其对有效性的威胁的指南。结果：有多种方法和工具可用于创建隐私保护测试数据。我们的搜索在 IEEE Xplore、ACM Digital Library 和 SCOPUS 中找到了 1,013 篇出版物。我们从其中 75 篇出版物中提取了数据，并确定了 37 种方法，部分回答了我们的研究问题。使用这些方法和工具的一个常见先决条件是直接访问现实生活中的数据以进行数据匿名化或综合测试数据生成。确定了九种现有的综合测试数据生成方法，它们最接近回答我们的研究问题。然而，还需要进一步的工作来增加现有方法发展综合测试数据的能力。结论：没有任何出版物真正完全满足我们的要求，只是部分满足我们的要求。综合测试数据演化是一个尚未受到研究人员太多关注的领域，但需要在数字政府解决方案中进行探索，特别是因为许多国家正在实施新的法律法规。</li>
</ul>

<h3>Title: PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks</h3>
<ul>
<li><strong>Authors: </strong>Junxian Li, Kai Liu, Leyang Chen, Weida Wang, Zhixin Wang, Jiaqi Xu, Fan Li, Renjing Pei, Linghe Kong, Yulun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06663">https://arxiv.org/abs/2602.06663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06663">https://arxiv.org/pdf/2602.06663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06663]] PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks(https://arxiv.org/abs/2602.06663)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Unified multimodal models (UMMs) have shown impressive capabilities in generating natural images and supporting multimodal reasoning. However, their potential in supporting computer-use planning tasks, which are closely related to our lives, remain underexplored. Image generation and editing in computer-use tasks require capabilities like spatial reasoning and procedural understanding, and it is still unknown whether UMMs have these capabilities to finish these tasks or not. Therefore, we propose PlanViz, a new benchmark designed to evaluate image generation and editing for computer-use tasks. To achieve the goal of our evaluation, we focus on sub-tasks which frequently involve in daily life and require planning steps. Specifically, three new sub-tasks are designed: route planning, work diagramming, and web&UI displaying. We address challenges in data quality ensuring by curating human-annotated questions and reference images, and a quality control process. For challenges of comprehensive and exact evaluation, a task-adaptive score, PlanScore, is proposed. The score helps understanding the correctness, visual quality and efficiency of generated images. Through experiments, we highlight key limitations and opportunities for future research on this topic.</li>
<li><strong>摘要：</strong>统一多模态模型 (UMM) 在生成自然图像和支持多模态推理方面表现出了令人印象深刻的能力。然而，它们在支持与我们的生活密切相关的计算机使用规划任务方面的潜力仍未得到充分开发。计算机使用任务中的图像生成和编辑需要空间推理和程序理解等能力，而 UMM 是否具备这些能力来完成这些任务仍然未知。因此，我们提出了 PlanViz，这是一个新的基准，旨在评估计算机使用任务的图像生成和编辑。为了实现我们的评估目标，我们重点关注日常生活中经常涉及并需要规划步骤的子任务。具体来说，设计了三个新的子任务：路线规划、工作图绘制以及Web&UI显示。我们通过策划人工注释的问题和参考图像以及质量控制流程来解决数据质量确保方面的挑战。针对全面准确评估的挑战，提出了任务自适应评分 PlanScore。该分数有助于了解生成图像的正确性、视觉质量和效率。通过实验，我们强调了该主题未来研究的主要局限性和机遇。</li>
</ul>

<h3>Title: CytoCrowd: A Multi-Annotator Benchmark Dataset for Cytology Image Analysis</h3>
<ul>
<li><strong>Authors: </strong>Yonghao Si, Xingyuan Zeng, Zhao Chen, Libin Zheng, Caleb Chen Cao, Lei Chen, Jian Yin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06674">https://arxiv.org/abs/2602.06674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06674">https://arxiv.org/pdf/2602.06674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06674]] CytoCrowd: A Multi-Annotator Benchmark Dataset for Cytology Image Analysis(https://arxiv.org/abs/2602.06674)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>High-quality annotated datasets are crucial for advancing machine learning in medical image analysis. However, a critical gap exists: most datasets either offer a single, clean ground truth, which hides real-world expert disagreement, or they provide multiple annotations without a separate gold standard for objective evaluation. To bridge this gap, we introduce CytoCrowd, a new public benchmark for cytology analysis. The dataset features 446 high-resolution images, each with two key components: (1) raw, conflicting annotations from four independent pathologists, and (2) a separate, high-quality gold-standard ground truth established by a senior expert. This dual structure makes CytoCrowd a versatile resource. It serves as a benchmark for standard computer vision tasks, such as object detection and classification, using the ground truth. Simultaneously, it provides a realistic testbed for evaluating annotation aggregation algorithms that must resolve expert disagreements. We provide comprehensive baseline results for both tasks. Our experiments demonstrate the challenges presented by CytoCrowd and establish its value as a resource for developing the next generation of models for medical image analysis.</li>
<li><strong>摘要：</strong>高质量的带注释数据集对于推进医学图像分析中的机器学习至关重要。然而，存在一个关键差距：大多数数据集要么提供单一、干净的基本事实，隐藏了现实世界专家的分歧，要么提供多个注释，而没有单独的客观评估黄金标准。为了弥补这一差距，我们推出了 CytoCrowd，这是一种新的细胞学分析公共基准。该数据集包含 446 张高分辨率图像，每张图像都有两个关键组成部分：(1) 来自四位独立病理学家的原始、相互冲突的注释，以及 (2) 由资深专家建立的独立的高质量黄金标准地面事实。这种双重结构使 CytoCrowd 成为一种多功能资源。它可以作为标准计算机视觉任务的基准，例如使用地面事实进行物体检测和分类。同时，它为评估必须解决专家分歧的注释聚合算法提供了一个现实的测试平台。我们为这两项任务提供全面的基线结果。我们的实验证明了 CytoCrowd 带来的挑战，并确立了其作为开发下一代医学图像分析模型的资源的价值。</li>
</ul>

<h3>Title: Memory-Conditioned Flow-Matching for Stable Autoregressive PDE Rollouts</h3>
<ul>
<li><strong>Authors: </strong>Victor Armegioiu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06689">https://arxiv.org/abs/2602.06689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06689">https://arxiv.org/pdf/2602.06689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06689]] Memory-Conditioned Flow-Matching for Stable Autoregressive PDE Rollouts(https://arxiv.org/abs/2602.06689)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Autoregressive generative PDE solvers can be accurate one step ahead yet drift over long rollouts, especially in coarse-to-fine regimes where each step must regenerate unresolved fine scales. This is the regime of diffusion and flow-matching generators: although their internal dynamics are Markovian, rollout stability is governed by per-step \emph{conditional law} errors. Using the Mori--Zwanzig projection formalism, we show that eliminating unresolved variables yields an exact resolved evolution with a Markov term, a memory term, and an orthogonal forcing, exposing a structural limitation of memoryless closures. Motivated by this, we introduce memory-conditioned diffusion/flow-matching with a compact online state injected into denoising via latent features. Via disintegration, memory induces a structured conditional tail prior for unresolved scales and reduces the transport needed to populate missing frequencies. We prove Wasserstein stability of the resulting conditional kernel. We then derive discrete Grönwall rollout bounds that separate memory approximation from conditional generation error. Experiments on compressible flows with shocks and multiscale mixing show improved accuracy and markedly more stable long-horizon rollouts, with better fine-scale spectral and statistical fidelity.</li>
<li><strong>摘要：</strong>自回归生成式偏微分方程求解器可以准确地领先一步，但在长时间的部署过程中会发生漂移，特别是在从粗到细的情况下，其中每一步都必须重新生成未解析的精细尺度。这是扩散和流量匹配生成器的机制：尽管它们的内部动力学是马尔可夫的，但转出稳定性受每步 \emph{条件律} 误差的控制。使用 Mori-Zwanzig 投影形式主义，我们证明消除未解决的变量会产生具有马尔可夫项、记忆项和正交强迫的精确解决的演化，从而暴露了无记忆闭包的结构限制。受此启发，我们引入了记忆条件扩散/流匹配，并通过潜在特征将紧凑的在线状态注入到去噪中。通过分解，记忆会在未解决的尺度之前诱导出结构化的条件尾部，并减少填充缺失频率所需的传输。我们证明了所得条件内核的 Wasserstein 稳定性。然后，我们导出离散的 Grönwall rollout 边界，将内存近似与条件生成错误分开。具有冲击和多尺度混合的可压缩流实验表明，精度得到了提高，长范围滚动明显更加稳定，并且具有更好的精细尺度谱和统计保真度。</li>
</ul>

<h3>Title: SaDiT: Efficient Protein Backbone Design via Latent Structural Tokenization and Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Shentong Mo, Lanqing Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06706">https://arxiv.org/abs/2602.06706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06706">https://arxiv.org/pdf/2602.06706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06706]] SaDiT: Efficient Protein Backbone Design via Latent Structural Tokenization and Diffusion Transformers(https://arxiv.org/abs/2602.06706)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative models for de novo protein backbone design have achieved remarkable success in creating novel protein structures. However, these diffusion-based approaches remain computationally intensive and slower than desired for large-scale structural exploration. While recent efforts like Proteina have introduced flow-matching to improve sampling efficiency, the potential of tokenization for structural compression and acceleration remains largely unexplored in the protein domain. In this work, we present SaDiT, a novel framework that accelerates protein backbone generation by integrating SaProt Tokenization with a Diffusion Transformer (DiT) architecture. SaDiT leverages a discrete latent space to represent protein geometry, significantly reducing the complexity of the generation process while maintaining theoretical SE(3) equivalence. To further enhance efficiency, we introduce an IPA Token Cache mechanism that optimizes the Invariant Point Attention (IPA) layers by reusing computed token states during iterative sampling. Experimental results demonstrate that SaDiT outperforms state-of-the-art models, including RFDiffusion and Proteina, in both computational speed and structural viability. We evaluate our model across unconditional backbone generation and fold-class conditional generation tasks, where SaDiT shows superior ability to capture complex topological features with high designability.</li>
<li><strong>摘要：</strong>从头蛋白质主链设计的生成模型在创建新型蛋白质结构方面取得了显着的成功。然而，这些基于扩散的方法仍然需要大量计算，并且比大规模结构探索所需的速度慢。虽然 Proteina 等最近的努力引入了流匹配来提高采样效率，但在蛋白质领域，结构压缩和加速的标记化潜力在很大程度上仍未得到探索。在这项工作中，我们提出了 SaDiT，这是一种新颖的框架，通过将 SaProt Tokenization 与 Diffusion Transformer (DiT) 架构集成来加速蛋白质主干的生成。 SaDiT 利用离散潜在空间来表示蛋白质几何形状，显着降低生成过程的复杂性，同时保持理论 SE(3) 等效性。为了进一步提高效率，我们引入了 IPA 令牌缓存机制，该机制通过在迭代采样期间重用计算的令牌状态来优化不变点注意（IPA）层。实验结果表明，SaDiT 在计算速度和结构可行性方面均优于最先进的模型，包括 RFDiffusion 和 Proteina。我们在无条件主干生成和折叠类条件生成任务中评估我们的模型，其中 SaDiT 显示出具有高可设计性的捕获复杂拓扑特征的卓越能力。</li>
</ul>

<h3>Title: Disentanglement by means of action-induced representations</h3>
<ul>
<li><strong>Authors: </strong>Gorka Muñoz-Gil, Hendrik Poulsen Nautrup, Arunava Majumder, Paulin de Schoulepnikoff, Florian Fürrutter, Marius Krumm, Hans J. Briegel</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.data-an, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06741">https://arxiv.org/abs/2602.06741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06741">https://arxiv.org/pdf/2602.06741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06741]] Disentanglement by means of action-induced representations(https://arxiv.org/abs/2602.06741)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Learning interpretable representations with variational autoencoders (VAEs) is a major goal of representation learning. The main challenge lies in obtaining disentangled representations, where each latent dimension corresponds to a distinct generative factor. This difficulty is fundamentally tied to the inability to perform nonlinear independent component analysis. Here, we introduce the framework of action-induced representations (AIRs) which models representations of physical systems given experiments (or actions) that can be performed on them. We show that, in this framework, we can provably disentangle degrees of freedom w.r.t. their action dependence. We further introduce a variational AIR architecture (VAIR) that can extract AIRs and therefore achieve provable disentanglement where standard VAEs fail. Beyond state representation, VAIR also captures the action dependence of the underlying generative factors, directly linking experiments to the degrees of freedom they influence.</li>
<li><strong>摘要：</strong>使用变分自动编码器（VAE）学习可解释的表示是表示学习的一个主要目标。主要挑战在于获得解开的表示，其中每个潜在维度对应于一个不同的生成因素。这一困难从根本上与无法执行非线性独立分量分析有关。在这里，我们介绍了动作诱导表示（AIR）的框架，该框架根据可以在物理系统上执行的实验（或动作）对物理系统的表示进行建模。我们证明，在这个框架中，我们可以证明我们可以解开自由度。他们的行动依赖性。我们进一步引入了一种变分 AIR 架构 (VAIR)，它可以提取 AIR，从而在标准 VAE 失败的情况下实现可证明的解开。除了状态表示之外，VAIR 还捕获了潜在生成因素的动作依赖性，将实验与其影响的自由度直接联系起来。</li>
</ul>

<h3>Title: Gold Exploration using Representations from a Multispectral Autoencoder</h3>
<ul>
<li><strong>Authors: </strong>Argyro Tsandalidou, Konstantinos Dogeas, Eleftheria Tetoula Tsonga, Elisavet Parselia, Georgios Tsimiklis, George Arvanitakis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06748">https://arxiv.org/abs/2602.06748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06748">https://arxiv.org/pdf/2602.06748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06748]] Gold Exploration using Representations from a Multispectral Autoencoder(https://arxiv.org/abs/2602.06748)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Satellite imagery is employed for large-scale prospectivity mapping due to the high cost and typically limited availability of on-site mineral exploration data. In this work, we present a proof-of-concept framework that leverages generative representations learned from multispectral Sentinel-2 imagery to identify gold-bearing regions from space. An autoencoder foundation model, called Isometric, which is pretrained on the large-scale FalconSpace-S2 v1.0 dataset, produces information-dense spectral-spatial representations that serve as inputs to a lightweight XGBoost classifier. We compare this representation-based approach with a raw spectral input baseline using a dataset of 63 Sentinel-2 images from known gold and non-gold locations. The proposed method improves patch-level accuracy from 0.51 to 0.68 and image-level accuracy from 0.55 to 0.73, demonstrating that generative embeddings capture transferable mineralogical patterns even with limited labeled data. These results highlight the potential of foundation-model representations to make mineral exploration more efficient, scalable, and globally applicable.</li>
<li><strong>摘要：</strong>由于现场矿产勘探数据成本高昂且可用性通常有限，因此采用卫星图像进行大规模勘探测绘。在这项工作中，我们提出了一个概念验证框架，该框架利用从多光谱 Sentinel-2 图像中学习到的生成表示来识别太空中的含金区域。称为 Isometric 的自动编码器基础模型在大规模 FalconSpace-S2 v1.0 数据集上进行了预训练，可生成信息密集的光谱空间表示，作为轻量级 XGBoost 分类器的输入。我们使用来自已知黄金和非黄金位置的 63 张 Sentinel-2 图像的数据集，将这种基于表示的方法与原始光谱输入基线进行比较。所提出的方法将块级精度从 0.51 提高到 0.68，将图像级精度从 0.55 提高到 0.73，证明生成嵌入即使在标记数据有限的情况下也能捕获可转移的矿物学模式。这些结果凸显了基础模型表示在使矿产勘探更加高效、可扩展和全球适用方面的潜力。</li>
</ul>

<h3>Title: Calibrating Generative AI to Produce Realistic Essays for Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Edward W. Wolfe, Justin O. Barber</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06772">https://arxiv.org/abs/2602.06772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06772">https://arxiv.org/pdf/2602.06772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06772]] Calibrating Generative AI to Produce Realistic Essays for Data Augmentation(https://arxiv.org/abs/2602.06772)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Data augmentation can mitigate limited training data in machine-learning automated scoring engines for constructed response items. This study seeks to determine how well three approaches to large language model prompting produce essays that preserve the writing quality of the original essays and produce realistic text for augmenting ASE training datasets. We created simulated versions of student essays, and human raters assigned scores to them and rated the realism of the generated text. The results of the study indicate that the predict next prompting strategy produces the highest level of agreement between human raters regarding simulated essay scores, predict next and sentence strategies best preserve the rated quality of the original essay in the simulated essays, and predict next and 25 examples strategies produce the most realistic text as judged by human raters.</li>
<li><strong>摘要：</strong>数据增强可以减轻机器学习自动评分引擎中构建响应项目的有限训练数据。本研究旨在确定大型语言模型提示的三种方法在保留原始论文的写作质量和生成用于增强 ASE 训练数据集的真实文本方面的效果如何。我们创建了学生论文的模拟版本，由人工评分者为其打分，并对生成文本的真实性进行评分。研究结果表明，预测下一个提示策略在人类评分者之间就模拟论文分数产生了最高程度的一致性，预测下一个和句子策略最好地保留了模拟论文中原始论文的评分质量，预测下一个和 25 个示例策略产生了人类评分者判断的最真实的文本。</li>
</ul>

<h3>Title: Rare Event Analysis of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jake McAllister Dorman, Edward Gillman, Dominic C. Rose, Jamie F. Mair, Juan P. Garrahan</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.dis-nn, cond-mat.stat-mech</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06791">https://arxiv.org/abs/2602.06791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06791">https://arxiv.org/pdf/2602.06791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06791]] Rare Event Analysis of Large Language Models(https://arxiv.org/abs/2602.06791)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Being probabilistic models, during inference large language models (LLMs) display rare events: behaviour that is far from typical but highly significant. By definition all rare events are hard to see, but the enormous scale of LLM usage means that events completely unobserved during development are likely to become prominent in deployment. Here we present an end-to-end framework for the systematic analysis of rare events in LLMs. We provide a practical implementation spanning theory, efficient generation strategies, probability estimation and error analysis, which we illustrate with concrete examples. We outline extensions and applications to other models and contexts, highlighting the generality of the concepts and techniques presented here.</li>
<li><strong>摘要：</strong>作为概率模型，大型语言模型 (LLM) 在推理过程中会显示罕见事件：与典型但非常重要的行为相去甚远。根据定义，所有罕见事件都很难看到，但 LLM 使用的巨大规模意味着在开发过程中完全未被观察到的事件可能会在部署中变得突出。在这里，我们提出了一个用于系统分析法学硕士罕见事件的端到端框架。我们提供了实际的实现跨越理论、高效生成策略、概率估计和误差分析，并用具体例子进行说明。我们概述了对其他模型和上下文的扩展和应用，强调了此处提出的概念和技术的通用性。</li>
</ul>

<h3>Title: FlowDA: Accurate, Low-Latency Weather Data Assimilation via Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Ran Cheng, Lailai Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06800">https://arxiv.org/abs/2602.06800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06800">https://arxiv.org/pdf/2602.06800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06800]] FlowDA: Accurate, Low-Latency Weather Data Assimilation via Flow Matching(https://arxiv.org/abs/2602.06800)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Data assimilation (DA) is a fundamental component of modern weather prediction, yet it remains a major computational bottleneck in machine learning (ML)-based forecasting pipelines due to reliance on traditional variational methods. Recent generative ML-based DA methods offer a promising alternative but typically require many sampling steps and suffer from error accumulation under long-horizon auto-regressive rollouts with cycling assimilation. We propose FlowDA, a low-latency weather-scale generative DA framework based on flow matching. FlowDA conditions on observations through a SetConv-based embedding and fine-tunes the Aurora foundation model to deliver accurate, efficient, and robust analyses. Experiments across observation rates decreasing from $3.9\%$ to $0.1\%$ demonstrate superior performance of FlowDA over strong baselines with similar tunable-parameter size. FlowDA further shows robustness to observational noise and stable performance in long-horizon auto-regressive cycling DA. Overall, FlowDA points to an efficient and scalable direction for data-driven DA.</li>
<li><strong>摘要：</strong>数据同化（DA）是现代天气预报的基本组成部分，但由于依赖传统的变分方法，它仍然是基于机器学习（ML）的预测管道的主要计算瓶颈。最近的基于机器学习的生成 DA 方法提供了一种有前途的替代方案，但通常需要许多采样步骤，并且在循环同化的长范围自回归推出下会遭受误差积累。我们提出了 FlowDA，一种基于流匹配的低延迟天气尺度生成 DA 框架。 FlowDA 通过基于 SetConv 的嵌入来调节观测结果，并微调 Aurora 基础模型，以提供准确、高效和稳健的分析。观察率从 $3.9\%$ 下降到 $0.1\%$ 的实验证明了 FlowDA 的性能优于具有相似可调参数大小的强基线。 FlowDA 进一步显示了对观测噪声的鲁棒性和长视野自回归循环 DA 的稳定性能。总体而言，FlowDA 为数据驱动的 DA 指明了一个高效且可扩展的方向。</li>
</ul>

<h3>Title: RAIGen: Rare Attribute Identification in Text-to-Image Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Silpa Vadakkeeveetil Sreelatha, Dan Wang, Serge Belongie, Muhammad Awais, Anjan Dutta</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06806">https://arxiv.org/abs/2602.06806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06806">https://arxiv.org/pdf/2602.06806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06806]] RAIGen: Rare Attribute Identification in Text-to-Image Generative Models(https://arxiv.org/abs/2602.06806)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models achieve impressive generation quality but inherit and amplify training-data biases, skewing coverage of semantic attributes. Prior work addresses this in two ways. Closed-set approaches mitigate biases in predefined fairness categories (e.g., gender, race), assuming socially salient minority attributes are known a priori. Open-set approaches frame the task as bias identification, highlighting majority attributes that dominate outputs. Both overlook a complementary task: uncovering rare or minority features underrepresented in the data distribution (social, cultural, or stylistic) yet still encoded in model representations. We introduce RAIGen, the first framework, to our knowledge, for un-supervised rare-attribute discovery in diffusion models. RAIGen leverages Matryoshka Sparse Autoencoders and a novel minority metric combining neuron activation frequency with semantic distinctiveness to identify interpretable neurons whose top-activating images reveal underrepresented attributes. Experiments show RAIGen discovers attributes beyond fixed fairness categories in Stable Diffusion, scales to larger models such as SDXL, supports systematic auditing across architectures, and enables targeted amplification of rare attributes during generation.</li>
<li><strong>摘要：</strong>文本到图像的扩散模型实现了令人印象深刻的生成质量，但继承并放大了训练数据偏差，扭曲了语义属性的覆盖范围。先前的工作通过两种方式解决了这个问题。假设社会上显着的少数群体属性是先验已知的，封闭集方法可以减轻预定义公平类别（例如性别、种族）中的偏见。开放集方法将任务定义为偏差识别，突出显示主导输出的大多数属性。两者都忽略了一个互补的任务：发现数据分布（社会、文化或风格）中代表性不足但仍编码在模型表示中的罕见或少数特征。我们介绍了 RAIGen，据我们所知，第一个框架，用于扩散模型中无监督的稀有属性发现。 RAIGen 利用 Matryoshka 稀疏自动编码器和一种新颖的少数度量，将神经元激活频率与语义独特性相结合，以识别可解释的神经元，其顶部激活图像揭示了未被充分代表的属性。实验表明，RAIGen 在稳定扩散中发现了超出固定公平类别的属性，可扩展到 SDXL 等更大的模型，支持跨架构的系统审计，并能够在生成过程中有针对性地放大稀有属性。</li>
</ul>

<h3>Title: AEGPO: Adaptive Entropy-Guided Policy Optimization for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yuming Li, Qingyu Li, Chengyu Bai, Xiangyang Luo, Zeyue Xue, Wenyu Qin, Meng Wang, Yikai Wang, Shanghang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06825">https://arxiv.org/abs/2602.06825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06825">https://arxiv.org/pdf/2602.06825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06825]] AEGPO: Adaptive Entropy-Guided Policy Optimization for Diffusion Models(https://arxiv.org/abs/2602.06825)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reinforcement learning from human feedback (RLHF) shows promise for aligning diffusion and flow models, yet policy optimization methods such as GRPO suffer from inefficient and static sampling strategies. These methods treat all prompts and denoising steps uniformly, ignoring substantial variations in sample learning value as well as the dynamic nature of critical exploration moments. To address this issue, we conduct a detailed analysis of the internal attention dynamics during GRPO training and uncover a key insight: attention entropy can serve as a powerful dual-signal proxy. First, across different samples, the relative change in attention entropy ({\Delta}Entropy), which reflects the divergence between the current policy and the base policy, acts as a robust indicator of sample learning value. Second, during the denoising process, the peaks of absolute attention entropy (Entropy(t)), which quantify attention dispersion, effectively identify critical timesteps where high-value exploration occurs. Building on this observation, we propose Adaptive Entropy-Guided Policy Optimization (AEGPO), a novel dual-signal, dual-level adaptive optimization strategy. At the global level, AEGPO uses {\Delta}Entropy to dynamically allocate rollout budgets, prioritizing prompts with higher learning value. At the local level, it exploits the peaks of Entropy(t) to guide exploration selectively at critical high-dispersion timesteps rather than uniformly across all denoising steps. By focusing computation on the most informative samples and the most critical moments, AEGPO enables more efficient and effective policy optimization. Experiments on text-to-image generation tasks demonstrate that AEGPO significantly accelerates convergence and achieves superior alignment performance compared to standard GRPO variants.</li>
<li><strong>摘要：</strong>来自人类反馈的强化学习 (RLHF) 显示出调整扩散和流动模型的希望，但 GRPO 等策略优化方法却受到低效和静态采样策略的困扰。这些方法统一处理所有提示和去噪步骤，忽略样本学习值的实质性变化以及关键探索时刻的动态性质。为了解决这个问题，我们对 GRPO 训练期间的内部注意力动态进行了详细分析，并揭示了一个关键见解：注意力熵可以作为强大的双信号代理。首先，在不同的样本中，注意力熵的相对变化（{\Delta}Entropy）反映了当前策略与基础策略之间的差异，可以作为样本学习价值的稳健指标。其次，在去噪过程中，量化注意力分散的绝对注意力熵（Entropy(t)）的峰值可以有效识别发生高价值探索的关键时间步长。基于这一观察，我们提出了自适应熵引导策略优化（AEGPO），这是一种新颖的双信号、双级自适应优化策略。在全球层面，AEGPO 使用 {\Delta}Entropy 动态分配部署预算，优先考虑具有较高学习价值的提示。在局部层面，它利用熵（t）的峰值来有选择地指导关键高离散时间步长的探索，而不是在所有去噪步骤中统一进行探索。通过将计算集中在信息最丰富的样本和最关键的时刻，AEGPO 可以实现更高效、更有效的策略优化。文本到图像生成任务的实验表明，与标准 GRPO 变体相比，AEGPO 显着加速了收敛速度并实现了卓越的对齐性能。</li>
</ul>

<h3>Title: Improved Sampling Schedules for Discrete Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Alberto Foresti, Mustapha Bounoua, Giulio Franzese, Luca Ambrogioni, Pietro Michiardi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06849">https://arxiv.org/abs/2602.06849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06849">https://arxiv.org/pdf/2602.06849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06849]] Improved Sampling Schedules for Discrete Diffusion Models(https://arxiv.org/abs/2602.06849)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Discrete diffusion models have emerged as a powerful paradigm for generative modeling on sequence data; however, the information-theoretic principles governing their reverse processes remain significantly less understood than those of their continuous counterparts. In this work, we bridge this gap by analyzing the reverse process dynamics through the lens of thermodynamic entropy production. We propose the entropy production rate as a rigorous proxy for quantifying information generation, deriving as a byproduct a bound on the Wasserstein distance between intermediate states and the data distribution. Leveraging these insights, we introduce two novel sampling schedules that are uniformly spaced with respect to their corresponding physics-inspired metrics: the Entropic Discrete Schedule (EDS), which is defined by maintaining a constant rate of information gain, and the Wasserstein Discrete Schedule (WDS), which is defined by taking equal steps in terms of the Wasserstein distance. We empirically demonstrate that our proposed schedules significantly outperform state-of-the-art strategies across diverse application domains, including synthetic data, music notation, vision and language modeling, consistently achieving superior performance at a lower computational budget.</li>
<li><strong>摘要：</strong>离散扩散模型已成为序列数据生成建模的强大范例；然而，与连续过程相比，人们对控制其逆过程的信息论原理仍然知之甚少。在这项工作中，我们通过热力学熵产生的视角分析逆过程动力学来弥补这一差距。我们建议将熵产生率作为量化信息生成的严格代理，并作为副产品导出中间状态和数据分布之间的 Wasserstein 距离的界限。利用这些见解，我们引入了两种新颖的采样计划，它们根据相应的物理启发指标均匀间隔：熵离散计划（EDS），它是通过保持恒定的信息增益率来定义的，以及 Wasserstein 离散计划（WDS），它是通过根据 Wasserstein 距离采取相等的步长来定义的。我们凭经验证明，我们提出的时间表在不同的应用领域（包括合成数据、音乐符号、视觉和语言建模）显着优于最先进的策略，以较低的计算预算持续实现卓越的性能。</li>
</ul>

<h3>Title: Rethinking Multi-Condition DiTs: Eliminating Redundant Attention via Position-Alignment and Keyword-Scoping</h3>
<ul>
<li><strong>Authors: </strong>Chao Zhou, Tianyi Wei, Yiling Chen, Wenbo Zhou, Nenghai Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06850">https://arxiv.org/abs/2602.06850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06850">https://arxiv.org/pdf/2602.06850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06850]] Rethinking Multi-Condition DiTs: Eliminating Redundant Attention via Position-Alignment and Keyword-Scoping(https://arxiv.org/abs/2602.06850)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While modern text-to-image models excel at prompt-based generation, they often lack the fine-grained control necessary for specific user requirements like spatial layouts or subject appearances. Multi-condition control addresses this, yet its integration into Diffusion Transformers (DiTs) is bottlenecked by the conventional ``concatenate-and-attend'' strategy, which suffers from quadratic computational and memory overhead as the number of conditions scales. Our analysis reveals that much of this cross-modal interaction is spatially or semantically redundant. To this end, we propose Position-aligned and Keyword-scoped Attention (PKA), a highly efficient framework designed to eliminate these redundancies. Specifically, Position-Aligned Attention (PAA) linearizes spatial control by enforcing localized patch alignment, while Keyword-Scoped Attention (KSA) prunes irrelevant subject-driven interactions via semantic-aware masking. To facilitate efficient learning, we further introduce a Conditional Sensitivity-Aware Sampling (CSAS) strategy that reweights the training objective towards critical denoising phases, drastically accelerating convergence and enhancing conditional fidelity. Empirically, PKA delivers a 10.0$\times$ inference speedup and a 5.1$\times$ VRAM saving, providing a scalable and resource-friendly solution for high-fidelity multi-conditioned generation.</li>
<li><strong>摘要：</strong>虽然现代文本到图像模型擅长基于提示的生成，但它们通常缺乏特定用户需求（例如空间布局或主题外观）所需的细粒度控制。多条件控制解决了这个问题，但它与扩散变压器 (DiT) 的集成受到传统“连接并参与”策略的瓶颈，该策略随着条件数量的增加而遭受二次计算和内存开销。我们的分析表明，这种跨模式交互大部分在空间或语义上都是冗余的。为此，我们提出了位置对齐和关键字范围的注意力（PKA），这是一个旨在消除这些冗余的高效框架。具体来说，位置对齐注意力（PAA）通过强制局部补丁对齐来线性化空间控制，而关键字范围注意力（KSA）则通过语义感知屏蔽来修剪不相关的主题驱动交互。为了促进高效学习，我们进一步引入了条件敏感度感知采样（CSAS）策略，该策略将训练目标重新加权到关键的去噪阶段，从而大大加速收敛并增强条件保真度。根据经验，PKA 提供 10.0$\times$ 的推理加速和 5.1$\times$ VRAM 节省，为高保真多条件生成提供可扩展且资源友好的解决方案。</li>
</ul>

<h3>Title: RFDM: Residual Flow Diffusion Model for Efficient Causal Video Editing</h3>
<ul>
<li><strong>Authors: </strong>Mohammadreza Salehi, Mehdi Noroozi, Luca Morreale, Ruchika Chavhan, Malcolm Chadwick, Alberto Gil Ramos, Abhinav Mehrotra</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06871">https://arxiv.org/abs/2602.06871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06871">https://arxiv.org/pdf/2602.06871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06871]] RFDM: Residual Flow Diffusion Model for Efficient Causal Video Editing(https://arxiv.org/abs/2602.06871)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Instructional video editing applies edits to an input video using only text prompts, enabling intuitive natural-language control. Despite rapid progress, most methods still require fixed-length inputs and substantial compute. Meanwhile, autoregressive video generation enables efficient variable-length synthesis, yet remains under-explored for video editing. We introduce a causal, efficient video editing model that edits variable-length videos frame by frame. For efficiency, we start from a 2D image-to-image (I2I) diffusion model and adapt it to video-to-video (V2V) editing by conditioning the edit at time step t on the model's prediction at t-1. To leverage videos' temporal redundancy, we propose a new I2I diffusion forward process formulation that encourages the model to predict the residual between the target output and the previous prediction. We call this Residual Flow Diffusion Model (RFDM), which focuses the denoising process on changes between consecutive frames. Moreover, we propose a new benchmark that better ranks state-of-the-art methods for editing tasks. Trained on paired video data for global/local style transfer and object removal, RFDM surpasses I2I-based methods and competes with fully spatiotemporal (3D) V2V models, while matching the compute of image models and scaling independently of input video length. More content can be found in: this https URL</li>
<li><strong>摘要：</strong>教学视频编辑仅使用文本提示对输入视频进行编辑，从而实现直观的自然语言控制。尽管进展迅速，大多数方法仍然需要固定长度的输入和大量的计算。与此同时，自回归视频生成可以实现高效的可变长度合成，但在视频编辑方面仍有待探索。我们引入了一种因果高效的视频编辑模型，可以逐帧编辑可变长度的视频。为了提高效率，我们从 2D 图像到图像 (I2I) 扩散模型开始，并通过根据模型在 t-1 时的预测来调节时间步 t 的编辑，使其适应视频到视频 (V2V) 编辑。为了利用视频的时间冗余，我们提出了一种新的 I2I 扩散前向过程公式，鼓励模型预测目标输出和先前预测之间的残差。我们称之为残余流扩散模型（RFDM），它将去噪过程集中在连续帧之间的变化上。此外，我们提出了一个新的基准，可以更好地对编辑任务的最先进方法进行排名。 RFDM 在配对视频数据上进行全局/局部风格转移和对象移除训练，超越了基于 I2I 的方法，并与完全时空 (3D) V2V 模型竞争，同时匹配图像模型的计算并独立于输入视频长度进行缩放。更多内容可以在：这个https网址找到</li>
</ul>

<h3>Title: NanoFLUX: Distillation-Driven Compression of Large Text-to-Image Generation Models for Mobile Devices</h3>
<ul>
<li><strong>Authors: </strong>Ruchika Chavhan, Malcolm Chadwick, Alberto Gil Couto Pimentel Ramos, Luca Morreale, Mehdi Noroozi, Abhinav Mehrotra</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06879">https://arxiv.org/abs/2602.06879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06879">https://arxiv.org/pdf/2602.06879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06879]] NanoFLUX: Distillation-Driven Compression of Large Text-to-Image Generation Models for Mobile Devices(https://arxiv.org/abs/2602.06879)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While large-scale text-to-image diffusion models continue to improve in visual quality, their increasing scale has widened the gap between state-of-the-art models and on-device solutions. To address this gap, we introduce NanoFLUX, a 2.4B text-to-image flow-matching model distilled from 17B FLUX.1-Schnell using a progressive compression pipeline designed to preserve generation quality. Our contributions include: (1) A model compression strategy driven by pruning redundant components in the diffusion transformer, reducing its size from 12B to 2B; (2) A ResNet-based token downsampling mechanism that reduces latency by allowing intermediate blocks to operate on lower-resolution tokens while preserving high-resolution processing elsewhere; (3) A novel text encoder distillation approach that leverages visual signals from early layers of the denoiser during sampling. Empirically, NanoFLUX generates 512 x 512 images in approximately 2.5 seconds on mobile devices, demonstrating the feasibility of high-quality on-device text-to-image generation.</li>
<li><strong>摘要：</strong>虽然大规模文本到图像扩散模型的视觉质量不断提高，但其规模的不断扩大扩大了最先进模型和设备上解决方案之间的差距。为了解决这一差距，我们引入了 NanoFLUX，这是一种从 17B FLUX.1-Schnell 中提取的 2.4B 文本到图像流匹配模型，使用旨在保持生成质量的渐进式压缩管道。我们的贡献包括：（1）通过修剪扩散变压器中的冗余组件驱动的模型压缩策略，将其大小从 12B 减小到 2B； (2) 基于 ResNet 的令牌下采样机制，通过允许中间块对较低分辨率的令牌进行操作来减少延迟，同时在其他地方保留高分辨率处理； (3) 一种新颖的文本编码器蒸馏方法，该方法在采样过程中利用来自降噪器早期层的视觉信号。根据经验，NanoFLUX 在移动设备上大约需要 2.5 秒生成 512 x 512 图像，证明了在设备上生成高质量文本到图像的可行性。</li>
</ul>

<h3>Title: Prompt Reinjection: Alleviating Prompt Forgetting in Multimodal Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Yao, Yuxuan Chen, Hui Li, Kaihui Cheng, Qipeng Guo, Yuwei Sun, Zilong Dong, Jingdong Wang, Siyu Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06886">https://arxiv.org/abs/2602.06886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06886">https://arxiv.org/pdf/2602.06886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06886]] Prompt Reinjection: Alleviating Prompt Forgetting in Multimodal Diffusion Transformers(https://arxiv.org/abs/2602.06886)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal Diffusion Transformers (MMDiTs) for text-to-image generation maintain separate text and image branches, with bidirectional information flow between text tokens and visual latents throughout denoising. In this setting, we observe a prompt forgetting phenomenon: the semantics of the prompt representation in the text branch is progressively forgotten as depth increases. We further verify this effect on three representative MMDiTs--SD3, SD3.5, and FLUX.1 by probing linguistic attributes of the representations over the layers in the text branch. Motivated by these findings, we introduce a training-free approach, prompt reinjection, which reinjects prompt representations from early layers into later layers to alleviate this forgetting. Experiments on GenEval, DPG, and T2I-CompBench++ show consistent gains in instruction-following capability, along with improvements on metrics capturing preference, aesthetics, and overall text--image generation quality.</li>
<li><strong>摘要：</strong>用于文本到图像生成的多模态扩散变压器 (MMDiT) 维护单独的文本和图像分支，并在整个去噪过程中在文本标记和视觉潜伏之间实现双向信息流。在这种情况下，我们观察到提示遗忘现象：随着深度的增加，文本分支中提示表示的语义逐渐被遗忘。我们通过探测文本分支中各层表示的语言属性，进一步验证了对三个代表性 MMDiT（SD3、SD3.5 和 FLUX.1）的这种影响。受这些发现的启发，我们引入了一种免训练的方法，即提示重新注入，它将早期层的提示表示重新注入到后面的层中，以减轻这种遗忘。 GenEval、DPG 和 T2I-CompBench++ 上的实验表明，指令跟踪能力得到了持续提升，并且捕获偏好、美观性和整体文本图像生成质量的指标也得到了改进。</li>
</ul>

<h3>Title: Seeing Beyond Redundancy: Task Complexity's Role in Vision Token Specialization in VLLMs</h3>
<ul>
<li><strong>Authors: </strong>Darryl Hannan, John Cooper, Dylan White, Yijing Watkins</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06914">https://arxiv.org/abs/2602.06914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06914">https://arxiv.org/pdf/2602.06914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06914]] Seeing Beyond Redundancy: Task Complexity's Role in Vision Token Specialization in VLLMs(https://arxiv.org/abs/2602.06914)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision capabilities in vision large language models (VLLMs) have consistently lagged behind their linguistic capabilities. In particular, numerous benchmark studies have demonstrated that VLLMs struggle when fine-grained visual information or spatial reasoning is required. However, we do not yet understand exactly why VLLMs struggle so much with these tasks relative to others. Some works have focused on visual redundancy as an explanation, where high-level visual information is uniformly spread across numerous tokens and specific, fine-grained visual information is discarded. In this work, we investigate this premise in greater detail, seeking to better understand exactly how various types of visual information are processed by the model and what types of visual information are discarded. To do so, we introduce a simple synthetic benchmark dataset that is specifically constructed to probe various visual features, along with a set of metrics for measuring visual redundancy, allowing us to better understand the nuances of their relationship. Then, we explore fine-tuning VLLMs on a number of complex visual tasks to better understand how redundancy and compression change based upon the complexity of the data that a model is trained on. We find that there is a connection between task complexity and visual compression, implying that having a sufficient ratio of high complexity visual data is crucial for altering the way that VLLMs distribute their visual representation and consequently improving their performance on complex visual tasks. We hope that this work will provide valuable insights for training the next generation of VLLMs.</li>
<li><strong>摘要：</strong>视觉大语言模型（VLLM）中的视觉能力一直落后于其语言能力。特别是，大量基准研究表明，当需要细粒度的视觉信息或空间推理时，VLLM 会遇到困难。然而，我们还不清楚为什么 VLLM 相对于其他任务在这些任务上如此困难。一些作品侧重于视觉冗余作为解释，其中高级视觉信息均匀地分布在众多标记中，而特定的、细粒度的视觉信息被丢弃。在这项工作中，我们更详细地研究了这个前提，试图更好地理解模型如何处理各种类型的视觉信息以及丢弃哪些类型的视觉信息。为此，我们引入了一个简单的综合基准数据集，该数据集专门用于探测各种视觉特征，以及一组用于测量视觉冗余的指标，使我们能够更好地理解它们之间关系的细微差别。然后，我们探索在许多复杂的视觉任务上微调 VLLM，以更好地理解冗余和压缩如何根据模型训练数据的复杂性而变化。我们发现任务复杂性和视觉压缩之间存在联系，这意味着拥有足够比例的高复杂性视觉数据对于改变 VLLM 分配其视觉表示的方式至关重要，从而提高其在复杂视觉任务上的性能。我们希望这项工作能为训练下一代 VLLM 提供有价值的见解。</li>
</ul>

<h3>Title: Endogenous Resistance to Activation Steering in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Alex McKenzie, Keenan Pepper, Stijn Servaes, Martin Leitgab, Murat Cubuktepe, Mike Vaiana, Diogo de Lucena, Judd Rosenblatt, Michael S. A. Graziano</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06941">https://arxiv.org/abs/2602.06941</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06941">https://arxiv.org/pdf/2602.06941</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06941]] Endogenous Resistance to Activation Steering in Language Models(https://arxiv.org/abs/2602.06941)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models can resist task-misaligned activation steering during inference, sometimes recovering mid-generation to produce improved responses even when steering remains active. We term this Endogenous Steering Resistance (ESR). Using sparse autoencoder (SAE) latents to steer model activations, we find that Llama-3.3-70B shows substantial ESR, while smaller models from the Llama-3 and Gemma-2 families exhibit the phenomenon less frequently. We identify 26 SAE latents that activate differentially during off-topic content and are causally linked to ESR in Llama-3.3-70B. Zero-ablating these latents reduces the multi-attempt rate by 25%, providing causal evidence for dedicated internal consistency-checking circuits. We demonstrate that ESR can be deliberately enhanced through both prompting and training: meta-prompts instructing the model to self-monitor increase the multi-attempt rate by 4x for Llama-3.3-70B, and fine-tuning on self-correction examples successfully induces ESR-like behavior in smaller models. These findings have dual implications: ESR could protect against adversarial manipulation but might also interfere with beneficial safety interventions that rely on activation steering. Understanding and controlling these resistance mechanisms is important for developing transparent and controllable AI systems. Code is available at this http URL.</li>
<li><strong>摘要：</strong>大型语言模型可以在推理过程中抵抗任务错位的激活转向，有时即使在转向保持活动状态时也能恢复中期生成以产生改进的响应。我们将这种现象称为内源转向阻力 (ESR)。使用稀疏自动编码器 (SAE) 潜伏来引导模型激活，我们发现 Llama-3.3-70B 显示出大量 ESR，而 Llama-3 和 Gemma-2 系列的较小模型表现出这种现象的频率较低。我们确定了 26 个潜在的 SAE，它们在偏离主题的内容期间有差异地激活，并且与 Llama-3.3-70B 中的 ESR 存在因果关系。对这些潜伏进行零消除可将多次尝试率降低 25%，为专用内部一致性检查电路提供因果证据。我们证明，可以通过提示和训练来有意增强 ESR：指示模型进行自我监控的元提示将 Llama-3.3-70B 的多次尝试率提高了 4 倍，并且对自我校正示例的微调成功地在较小​​的模型中诱导了类似 ESR 的行为。这些发现具有双重意义：ESR 可以防止对抗性操纵，但也可能干扰依赖激活转向的有益安全干预措施。了解和控制这些阻力机制对于开发透明可控的人工智能系统非常重要。代码可从此 http URL 获取。</li>
</ul>

<h3>Title: CineScene: Implicit 3D as Effective Scene Representation for Cinematic Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Kaiyi Huang, Yukun Huang, Yu Li, Jianhong Bai, Xintao Wang, Zinan Lin, Xuefei Ning, Jiwen Yu, Pengfei Wan, Yu Wang, Xihui Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06959">https://arxiv.org/abs/2602.06959</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06959">https://arxiv.org/pdf/2602.06959</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06959]] CineScene: Implicit 3D as Effective Scene Representation for Cinematic Video Generation(https://arxiv.org/abs/2602.06959)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Cinematic video production requires control over scene-subject composition and camera movement, but live-action shooting remains costly due to the need for constructing physical sets. To address this, we introduce the task of cinematic video generation with decoupled scene context: given multiple images of a static environment, the goal is to synthesize high-quality videos featuring dynamic subject while preserving the underlying scene consistency and following a user-specified camera trajectory. We present CineScene, a framework that leverages implicit 3D-aware scene representation for cinematic video generation. Our key innovation is a novel context conditioning mechanism that injects 3D-aware features in an implicit way: By encoding scene images into visual representations through VGGT, CineScene injects spatial priors into a pretrained text-to-video generation model by additional context concatenation, enabling camera-controlled video synthesis with consistent scenes and dynamic subjects. To further enhance the model's robustness, we introduce a simple yet effective random-shuffling strategy for the input scene images during training. To address the lack of training data, we construct a scene-decoupled dataset with Unreal Engine 5, containing paired videos of scenes with and without dynamic subjects, panoramic images representing the underlying static scene, along with their camera trajectories. Experiments show that CineScene achieves state-of-the-art performance in scene-consistent cinematic video generation, handling large camera movements and demonstrating generalization across diverse environments.</li>
<li><strong>摘要：</strong>电影视频制作需要控制场景主体构图和摄像机移动，但由于需要构建物理布景，实景拍摄仍然成本高昂。为了解决这个问题，我们引入了具有解耦场景上下文的电影视频生成任务：给定静态环境的多个图像，目标是合成具有动态主题的高质量视频，同时保持底层场景的一致性并遵循用户指定的摄像机轨迹。我们提出了 CineScene，一个利用隐式 3D 感知场景表示来生成电影视频的框架。我们的关键创新是一种新颖的上下文调节机制，它以隐式方式注入 3D 感知功能：通过 VGGT 将场景图像编码为视觉表示，CineScene 通过额外的上下文串联将空间先验注入到预训练的文本到视频生成模型中，从而实现具有一致场景和动态主题的摄像机控制视频合成。为了进一步增强模型的鲁棒性，我们在训练期间对输入场景图像引入了一种简单而有效的随机洗牌策略。为了解决训练数据的缺乏，我们使用虚幻引擎 5 构建了一个场景解耦数据集，其中包含有和没有动态主体的场景配对视频、代表底层静态场景的全景图像及其摄像机轨迹。实验表明，CineScene 在场景一致的电影视频生成方面实现了最先进的性能，可处理大型摄像机运动并展示跨不同环境的泛化能力。</li>
</ul>

<h3>Title: Learning a Generative Meta-Model of LLM Activations</h3>
<ul>
<li><strong>Authors: </strong>Grace Luo, Jiahai Feng, Trevor Darrell, Alec Radford, Jacob Steinhardt</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06964">https://arxiv.org/abs/2602.06964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06964">https://arxiv.org/pdf/2602.06964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06964]] Learning a Generative Meta-Model of LLM Activations(https://arxiv.org/abs/2602.06964)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Existing approaches for analyzing neural network activations, such as PCA and sparse autoencoders, rely on strong structural assumptions. Generative models offer an alternative: they can uncover structure without such assumptions and act as priors that improve intervention fidelity. We explore this direction by training diffusion models on one billion residual stream activations, creating "meta-models" that learn the distribution of a network's internal states. We find that diffusion loss decreases smoothly with compute and reliably predicts downstream utility. In particular, applying the meta-model's learned prior to steering interventions improves fluency, with larger gains as loss decreases. Moreover, the meta-model's neurons increasingly isolate concepts into individual units, with sparse probing scores that scale as loss decreases. These results suggest generative meta-models offer a scalable path toward interpretability without restrictive structural assumptions. Project page: this https URL.</li>
<li><strong>摘要：</strong>分析神经网络激活的现有方法（例如 PCA 和稀疏自动编码器）依赖于强大的结构假设。生成模型提供了另一种选择：它们可以在没有此类假设的情况下揭示结构，并充当提高干预保真度的先验。我们通过在十亿个残差流激活上训练扩散模型来探索这个方向，创建学习网络内部状态分布的“元模型”。我们发现扩散损失随着计算的增加而平稳减少，并可靠地预测下游效用。特别是，在指导干预之前应用学到的元模型可以提高流畅性，随着损失的减少，收益会更大。此外，元模型的神经元越来越多地将概念隔离成单独的单元，稀疏的探测分数随着损失的减少而缩放。这些结果表明生成元模型提供了一条可扩展的可解释性路径，而无需限制性的结构假设。项目页面：此 https URL。</li>
</ul>

<h3>Title: MedMO: Grounding and Understanding Multimodal Large Language Model for Medical Images</h3>
<ul>
<li><strong>Authors: </strong>Ankan Deria, Komal Kumar, Adinath Madhavrao Dukre, Eran Segal, Salman Khan, Imran Razzak</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06965">https://arxiv.org/abs/2602.06965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06965">https://arxiv.org/pdf/2602.06965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06965]] MedMO: Grounding and Understanding Multimodal Large Language Model for Medical Images(https://arxiv.org/abs/2602.06965)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have rapidly advanced, yet their adoption in medicine remains limited by gaps in domain coverage, modality alignment, and grounded reasoning. In this work, we introduce MedMO, a medical foundation model built upon a generalized MLLM architecture and trained exclusively on large-scale, domain-specific data. MedMO follows a multi-stage training recipe: (i) cross-modal pretraining to align heterogeneous visual encoders with a medical language backbone; (ii) instruction tuning on multi-task supervision that spans captioning, VQA, report generation, retrieval, and grounded disease localization with bounding boxes; and (iii) reinforcement learning with verifiable rewards that combine factuality checks with a box-level GIoU reward to strengthen spatial grounding and step-by-step reasoning in complex clinical scenarios. MedMO consistently outperforms strong open-source medical MLLMs across multiple modalities and tasks. On VQA benchmarks, MedMO achieves an average accuracy improvement of +13.7% over the baseline and performs within 1.9% of the SOTA Fleming-VL. For text-based QA, it attains +6.9% over the baseline and +14.5% over Fleming-VL. In medical report generation, MedMO delivers significant gains in both semantic and clinical accuracy. Moreover, it exhibits strong grounding capability, achieving an IoU improvement of +40.4 over the baseline and +37.0% over Fleming-VL, underscoring its robust spatial reasoning and localization performance. Evaluations across radiology, ophthalmology, and pathology-microscopy confirm MedMO's broad cross-modality generalization. We release two versions of MedMO: 4B and 8B. Project is available at this https URL</li>
<li><strong>摘要：</strong>多模态大语言模型 (MLLM) 迅速发展，但其在医学中的采用仍然受到领域覆盖、模态对齐和基础推理方面的差距的限制。在这项工作中，我们介绍了 MedMO，这是一种基于通用 MLLM 架构构建的医学基础模型，并专门针对大规模、特定领域的数据进行训练。 MedMO 遵循多阶段训练方案：(i) 跨模式预训练，将异构视觉编码器与医学语言主干对齐； (ii) 多任务监督的指令调整，涵盖字幕、VQA、报告生成、检索和带边界框的基础疾病定位； (iii) 具有可验证奖励的强化学习，将事实检查与框级 GIoU 奖励相结合，以加强复杂临床场景中的空间基础和逐步推理。 MedMO 在多种模式和任务中始终优于强大的开源医疗 MLLM。在 VQA 基准测试中，MedMO 的平均准确度比基线提高了 13.7%，与 SOTA Fleming-VL 的差距在 1.9% 以内。对于基于文本的 QA，它比基线提高了 6.9%，比 Fleming-VL 提高了 14.5%。在医疗报告生成中，MedMO 在语义和临床准确性方面都取得了显着的进步。此外，它表现出强大的接地能力，IoU 比基线提高了 +40.4，比 Fleming-VL 提高了 +37.0%，凸显了其强大的空间推理和定位性能。放射学、眼科和病理学显微镜的评估证实了 MedMO 广泛的跨模态概括。我们发布了 MedMO 的两个版本：4B 和 8B。项目可在此 https URL 获取</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
