<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-01-05</h1>
<h3>Title: TeleWorld: Towards Dynamic Multimodal Synthesis with a 4D World Model</h3>
<ul>
<li><strong>Authors: </strong>Yabo Chen, Yuanzhi Liang, Jiepeng Wang, Tingxi Chen, Junfei Cheng, Zixiao Gu, Yuyang Huang, Zicheng Jiang, Wei Li, Tian Li, Weichen Li, Zuoxin Li, Guangce Liu, Jialun Liu, Junqi Liu, Haoyuan Wang, Qizhen Weng, Xuan'er Wu, Xunzhi Xiang, Xiaoyan Yang, Xin Zhang, Shiwen Zhang, Junyu Zhou, Chengcheng Zhou, Haibin Huang, Chi Zhang, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00051">https://arxiv.org/abs/2601.00051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00051">https://arxiv.org/pdf/2601.00051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00051]] TeleWorld: Towards Dynamic Multimodal Synthesis with a 4D World Model(https://arxiv.org/abs/2601.00051)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>World models aim to endow AI systems with the ability to represent, generate, and interact with dynamic environments in a coherent and temporally consistent manner. While recent video generation models have demonstrated impressive visual quality, they remain limited in real-time interaction, long-horizon consistency, and persistent memory of dynamic scenes, hindering their evolution into practical world models. In this report, we present TeleWorld, a real-time multimodal 4D world modeling framework that unifies video generation, dynamic scene reconstruction, and long-term world memory within a closed-loop system. TeleWorld introduces a novel generation-reconstruction-guidance paradigm, where generated video streams are continuously reconstructed into a dynamic 4D spatio-temporal representation, which in turn guides subsequent generation to maintain spatial, temporal, and physical consistency. To support long-horizon generation with low latency, we employ an autoregressive diffusion-based video model enhanced with Macro-from-Micro Planning (MMPL)--a hierarchical planning method that reduces error accumulation from frame-level to segment-level-alongside efficient Distribution Matching Distillation (DMD), enabling real-time synthesis under practical computational budgets. Our approach achieves seamless integration of dynamic object modeling and static scene representation within a unified 4D framework, advancing world models toward practical, interactive, and computationally accessible systems. Extensive experiments demonstrate that TeleWorld achieves strong performance in both static and dynamic world understanding, long-term consistency, and real-time generation efficiency, positioning it as a practical step toward interactive, memory-enabled world models for multimodal generation and embodied intelligence.</li>
<li><strong>摘要：</strong>世界模型旨在赋予人工智能系统以连贯且时间一致的方式表示、生成动态环境并与之交互的能力。虽然最近的视频生成模型表现出了令人印象深刻的视觉质量，但它们在实时交互、长视野一致性和动态场景的持久记忆方面仍然有限，阻碍了它们向实际世界模型的演进。在本报告中，我们介绍了 TeleWorld，这是一种实时多模态 4D 世界建模框架，它将视频生成、动态场景重建和长期世界记忆统一在闭环系统中。 TeleWorld 引入了一种新颖的生成-重构-指导范式，其中生成的视频流被不断重构为动态 4D 时空表示，从而指导后续生成保持空间、时间和物理一致性。为了支持低延迟的长视野生成，我们采用了基于自回归扩散的视频模型，该模型通过宏观微观规划（MMPL）进行了增强，这是一种分层规划方法，可减少从帧级到段级的误差累积，并结合高效的分布匹配蒸馏（DMD），从而在实际计算预算下实现实时合成。我们的方法在统一的 4D 框架内实现了动态对象建模和静态场景表示的无缝集成，将世界模型推进到实用、交互式和计算可访问的系统。大量实验表明，TeleWorld 在静态和动态世界理解、长期一致性和实时生成效率方面均取得了出色的性能，将其定位为迈向交互式、支持记忆的多模式生成和体现智能的世界模型的实用步骤。</li>
</ul>

<h3>Title: The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition</h3>
<ul>
<li><strong>Authors: </strong>Xiaoze Liu, Weichen Yu, Matt Fredrikson, Xiaoqian Wang, Jing Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00065">https://arxiv.org/abs/2601.00065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00065">https://arxiv.org/pdf/2601.00065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00065]] The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition(https://arxiv.org/abs/2601.00065)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The open-weight LLM ecosystem is increasingly defined by model composition techniques (such as weight merging, speculative decoding, and vocabulary expansion) that remix capabilities from diverse sources. A critical prerequisite for applying these methods across different model families is tokenizer transplant, which aligns incompatible vocabularies to a shared embedding space. We demonstrate that this essential interoperability step introduces a supply-chain vulnerability: we engineer a single "breaker token" that is functionally inert in a donor model yet reliably reconstructs into a high-salience malicious feature after transplant into a base model. By exploiting the geometry of coefficient reuse, our attack creates an asymmetric realizability gap that sabotages the base model's generation while leaving the donor's utility statistically indistinguishable from nominal behavior. We formalize this as a dual-objective optimization problem and instantiate the attack using a sparse solver. Empirically, the attack is training-free and achieves spectral mimicry to evade outlier detection, while demonstrating structural persistence against fine-tuning and weight merging, highlighting a hidden risk in the pipeline of modular AI composition. Code is available at this https URL</li>
<li><strong>摘要：</strong>开放权重 LLM 生态系统越来越多地由模型组合技术（例如权重合并、推测性解码和词汇扩展）来定义，这些技术重新混合了不同来源的功能。在不同模型系列中应用这些方法的一个关键先决条件是分词器移植，它将不兼容的词汇表与共享的嵌入空间对齐。我们证明了这一重要的互操作性步骤引入了供应链漏洞：我们设计了一个单一的“破坏者代币”，该代币在捐赠者模型中功能惰性，但在移植到基础模型后可靠地重建为高显着性恶意特征。通过利用系数重用的几何结构，我们的攻击创建了一个不对称的可实现性差距，破坏了基本模型的生成，同时使捐赠者的效用在统计上与名义行为无法区分。我们将其形式化为双目标优化问题，并使用稀疏求解器实例化攻击。根据经验，该攻击无需训练，可实现光谱模仿来逃避异常值检测，同时表现出针对微调和权重合并的结构持久性，凸显了模块化人工智能组合管道中的隐藏风险。代码可在此 https URL 获取</li>
</ul>

<h3>Title: Dynamic Bayesian Optimization Framework for Instruction Tuning in Partial Differential Equation Discovery</h3>
<ul>
<li><strong>Authors: </strong>Junqi Qu, Yan Zhang, Shangqian Gao, Shibo Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00088">https://arxiv.org/abs/2601.00088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00088">https://arxiv.org/pdf/2601.00088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00088]] Dynamic Bayesian Optimization Framework for Instruction Tuning in Partial Differential Equation Discovery(https://arxiv.org/abs/2601.00088)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) show promise for equation discovery, yet their outputs are highly sensitive to prompt phrasing, a phenomenon we term instruction brittleness. Static prompts cannot adapt to the evolving state of a multi-step generation process, causing models to plateau at suboptimal solutions. To address this, we propose NeuroSymBO, which reframes prompt engineering as a sequential decision problem. Our method maintains a discrete library of reasoning strategies and uses Bayesian Optimization to select the optimal instruction at each step based on numerical feedback. Experiments on PDE discovery benchmarks show that adaptive instruction selection significantly outperforms fixed prompts, achieving higher recovery rates with more parsimonious solutions.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）显示出方程发现的前景，但它们的输出对提示短语高度敏感，我们将这种现象称为指令脆弱性。静态提示无法适应多步骤生成过程的演变状态，导致模型在次优解决方案上停滞不前。为了解决这个问题，我们提出了 NeuroSymBO，它将提示工程重新定义为顺序决策问题。我们的方法维护一个离散的推理策略库，并使用贝叶斯优化根据数值反馈在每一步选择最佳指令。 PDE 发现基准实验表明，自适应指令选择显着优于固定提示，通过更简洁的解决方案实现了更高的恢复率。</li>
</ul>

<h3>Title: It's Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Anne Harrington, A. Sophia Koepke, Shyamgopal Karthik, Trevor Darrell, Alexei A. Efros</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00090">https://arxiv.org/abs/2601.00090</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00090">https://arxiv.org/pdf/2601.00090</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00090]] It's Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models(https://arxiv.org/abs/2601.00090)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Contemporary text-to-image models exhibit a surprising degree of mode collapse, as can be seen when sampling several images given the same text prompt. While previous work has attempted to address this issue by steering the model using guidance mechanisms, or by generating a large pool of candidates and refining them, in this work we take a different direction and aim for diversity in generations via noise optimization. Specifically, we show that a simple noise optimization objective can mitigate mode collapse while preserving the fidelity of the base model. We also analyze the frequency characteristics of the noise and show that alternative noise initializations with different frequency profiles can improve both optimization and search. Our experiments demonstrate that noise optimization yields superior results in terms of generation quality and variety.</li>
<li><strong>摘要：</strong>当代文本到图像模型表现出令人惊讶的模式崩溃程度，在给定相同文本提示的情况下对多个图像进行采样时可以看出。虽然之前的工作试图通过使用引导机制引导模型，或者通过生成大量候选对象并对其进行改进来解决这个问题，但在这项工作中，我们采取了不同的方向，旨在通过噪声优化来实现世代的多样性。具体来说，我们表明简单的噪声优化目标可以减轻模式崩溃，同时保持基本模型的保真度。我们还分析了噪声的频率特性，并表明具有不同频率分布的替代噪声初始化可以改善优化和搜索。我们的实验表明，噪声优化在发电质量和品种方面产生了优异的结果。</li>
</ul>

<h3>Title: Attention to Detail: Global-Local Attention for High-Resolution AI-Generated Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Lawrence Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00141">https://arxiv.org/abs/2601.00141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00141">https://arxiv.org/pdf/2601.00141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00141]] Attention to Detail: Global-Local Attention for High-Resolution AI-Generated Image Detection(https://arxiv.org/abs/2601.00141)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid development of generative AI has made AI-generated images increasingly realistic and high-resolution. Most AI-generated image detection architectures typically downsample images before inputting them into models, risking the loss of fine-grained details. This paper presents GLASS (Global-Local Attention with Stratified Sampling), an architecture that combines a globally resized view with multiple randomly sampled local crops. These crops are original-resolution regions efficiently selected through spatially stratified sampling and aggregated using attention-based scoring. GLASS can be integrated into vision models to leverage both global and local information in images of any size. Vision Transformer, ResNet, and ConvNeXt models are used as backbones, and experiments show that GLASS outperforms standard transfer learning by achieving higher predictive performance within feasible computational constraints.</li>
<li><strong>摘要：</strong>生成式人工智能的快速发展使得人工智能生成的图像变得越来越真实和高分辨率。大多数人工智能生成的图像检测架构通常会在将图像输入模型之前对图像进行下采样，这可能会导致细粒度细节的丢失。本文提出了 GLASS（具有分层采样的全局局部注意力），这是一种将全局调整大小的视图与多个随机采样的局部作物相结合的架构。这些作物是通过空间分层采样有效选择的原始分辨率区域，并使用基于注意力的评分进行聚合。 GLASS 可以集成到视觉模型中，以利用任何尺寸图像中的全局和局部信息。 Vision Transformer、ResNet 和 ConvNeXt 模型被用作主干，实验表明 GLASS 在可行的计算约束内实现了更高的预测性能，从而优于标准迁移学习。</li>
</ul>

<h3>Title: Reinforcement-Learned Unequal Error Protection for Quantized Semantic Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Moirangthem Tiken Singh, Adnan Arif</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00186">https://arxiv.org/abs/2601.00186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00186">https://arxiv.org/pdf/2601.00186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00186]] Reinforcement-Learned Unequal Error Protection for Quantized Semantic Embeddings(https://arxiv.org/abs/2601.00186)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This paper tackles the pressing challenge of preserving semantic meaning in communication systems constrained by limited bandwidth. We introduce a novel reinforcement learning framework that achieves per-dimension unequal error protection via adaptive repetition coding. Central to our approach is a composite semantic distortion metric that balances global embedding similarity with entity-level preservation, empowering the reinforcement learning agent to allocate protection in a context-aware manner. Experiments show statistically significant gains over uniform protection, achieving 6.8% higher chrF scores and 9.3% better entity preservation at 1 dB SNR. The key innovation of our framework is the demonstration that simple, intelligently allocated repetition coding enables fine-grained semantic protection -- an advantage unattainable with conventional codes such as LDPC or Reed-Solomon. Our findings challenge traditional channel coding paradigms by establishing that code structure must align with semantic granularity. This approach is particularly suited to edge computing and IoT scenarios, where bandwidth is scarce, but semantic fidelity is critical, providing a practical pathway for next-generation semantic-aware networks.</li>
<li><strong>摘要：</strong>本文解决了在有限带宽限制的通信系统中保留语义的紧迫挑战。我们引入了一种新颖的强化学习框架，该框架通过自适应重复编码实现每维不等错误保护。我们方法的核心是复合语义失真度量，它平衡全局嵌入相似性与实体级保留，使强化学习代理能够以上下文感知的方式分配保护。实验表明，与统一保护相比，具有统计显着性优势，在 1 dB SNR 下，chrF 分数提高了 6.8%，实体保留提高了 9.3%。我们框架的关键创新在于证明了简单、智能分配的重复编码能够实现细粒度的语义保护——这是 LDPC 或 Reed-Solomon 等传统编码无法实现的优势。我们的研究结果通过建立代码结构必须与语义粒度保持一致来挑战传统的信道编码范例。这种方法特别适合带宽稀缺但语义保真度至关重要的边缘计算和物联网场景，为下一代语义感知网络提供了实用的途径。</li>
</ul>

<h3>Title: SSI-GAN: Semi-Supervised Swin-Inspired Generative Adversarial Networks for Neuronal Spike Classification</h3>
<ul>
<li><strong>Authors: </strong>Danial Sharifrazi, Nouman Javed, Mojtaba Mohammadi, Seyede Sana Salehi, Roohallah Alizadehsani, Prasad N. Paradkar, U. Rajendra Acharya, Asim Bhatti</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00189">https://arxiv.org/abs/2601.00189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00189">https://arxiv.org/pdf/2601.00189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00189]] SSI-GAN: Semi-Supervised Swin-Inspired Generative Adversarial Networks for Neuronal Spike Classification(https://arxiv.org/abs/2601.00189)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Mosquitos are the main transmissive agents of arboviral diseases. Manual classification of their neuronal spike patterns is very labor-intensive and expensive. Most available deep learning solutions require fully labeled spike datasets and highly preprocessed neuronal signals. This reduces the feasibility of mass adoption in actual field scenarios. To address the scarcity of labeled data problems, we propose a new Generative Adversarial Network (GAN) architecture that we call the Semi-supervised Swin-Inspired GAN (SSI-GAN). The Swin-inspired, shifted-window discriminator, together with a transformer-based generator, is used to classify neuronal spike trains and, consequently, detect viral neurotropism. We use a multi-head self-attention model in a flat, window-based transformer discriminator that learns to capture sparser high-frequency spike features. Using just 1 to 3% labeled data, SSI-GAN was trained with more than 15 million spike samples collected at five-time post-infection and recording classification into Zika-infected, dengue-infected, or uninfected categories. Hyperparameters were optimized using the Bayesian Optuna framework, and performance for robustness was validated under fivefold Monte Carlo cross-validation. SSI-GAN reached 99.93% classification accuracy on the third day post-infection with only 3% labeled data. It maintained high accuracy across all stages of infection with just 1% supervision. This shows a 97-99% reduction in manual labeling effort relative to standard supervised approaches at the same performance level. The shifted-window transformer design proposed here beat all baselines by a wide margin and set new best marks in spike-based neuronal infection classification.</li>
<li><strong>摘要：</strong>蚊子是虫媒病毒性疾病的主要传播媒介。对其神经元尖峰模式进行手动分类非常耗时且昂贵。大多数可用的深度学习解决方案都需要完全标记的尖峰数据集和高度预处理的神经元信号。这降低了在实际场景中大规模采用的可行性。为了解决标记数据稀缺的问题，我们提出了一种新的生成对抗网络 (GAN) 架构，我们称之为半监督 Swin-Inspired GAN (SSI-GAN)。受 Swin 启发的移动窗口鉴别器与基于变压器的生成器一起用于对神经元尖峰序列进行分类，从而检测病毒的向神经性。我们在基于窗口的扁平变压器鉴别器中使用多头自注意力模型，该模型学习捕获稀疏的高频尖峰特征。仅使用 1% 到 3% 的标记数据，SSI-GAN 就通过在感染后五次收集的超过 1500 万个尖峰样本进行了训练，并记录了寨卡病毒感染、登革热感染或未感染类别的分类。使用贝叶斯 Optuna 框架优化超参数，并在五重蒙特卡罗交叉验证下验证鲁棒性性能。 SSI-GAN 在感染后第三天就达到了 99.93% 的分类准确率，而标记数据仅为 3%。它在感染的各个阶段都保持了高精度，仅需要 1% 的监督。这表明，在相同性能水平下，相对于标准监督方法，手动标记工作量减少了 97-99%。这里提出的移位窗口变压器设计大幅击败了所有基线，并在基于尖峰的神经元感染分类中设定了新的最佳标记。</li>
</ul>

<h3>Title: DichroGAN: Towards Restoration of in-air Colours of Seafloor from Satellite Imagery</h3>
<ul>
<li><strong>Authors: </strong>Salma Gonzalez-Sabbagh, Antonio Robles-Kelly, Shang Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00194">https://arxiv.org/abs/2601.00194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00194">https://arxiv.org/pdf/2601.00194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00194]] DichroGAN: Towards Restoration of in-air Colours of Seafloor from Satellite Imagery(https://arxiv.org/abs/2601.00194)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generative</a></li>
<li><strong>Abstract: </strong>Recovering the in-air colours of seafloor from satellite imagery is a challenging task due to the exponential attenuation of light with depth in the water column. In this study, we present DichroGAN, a conditional generative adversarial network (cGAN) designed for this purpose. DichroGAN employs a two-steps simultaneous training: first, two generators utilise a hyperspectral image cube to estimate diffuse and specular reflections, thereby obtaining atmospheric scene radiance. Next, a third generator receives as input the generated scene radiance containing the features of each spectral band, while a fourth generator estimates the underwater light transmission. These generators work together to remove the effects of light absorption and scattering, restoring the in-air colours of seafloor based on the underwater image formation equation. DichroGAN is trained on a compact dataset derived from PRISMA satellite imagery, comprising RGB images paired with their corresponding spectral bands and masks. Extensive experiments on both satellite and underwater datasets demonstrate that DichroGAN achieves competitive performance compared to state-of-the-art underwater restoration techniques.</li>
<li><strong>摘要：</strong>从卫星图像中恢复海底的空气颜色是一项具有挑战性的任务，因为光随着水柱深度的增加呈指数衰减。在这项研究中，我们提出了 DichroGAN，这是一种为此目的而设计的条件生成对抗网络（cGAN）。 DichroGAN 采用​​两步同时训练：首先，两个生成器利用高光谱图像立方体来估计漫反射和镜面反射，从而获得大气场景辐射率。接下来，第三个生成器接收包含每个光谱带特征的生成的场景辐射率作为输入，而第四个生成器估计水下光透射率。这些发生器协同工作，消除光吸收和散射的影响，根据水下图像形成方程恢复海底的空气颜色。 DichroGAN 在源自 PRISMA 卫星图像的紧凑数据集上进行训练，其中包含与其相应光谱带和掩模配对的 RGB 图像。对卫星和水下数据集的大量实验表明，与最先进的水下恢复技术相比，DichroGAN 实现了具有竞争力的性能。</li>
</ul>

<h3>Title: MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing</h3>
<ul>
<li><strong>Authors: </strong>Xiaokun Sun, Zeyu Cai, Hao Tang, Ying Tai, Jian Yang, Zhenyu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00204">https://arxiv.org/abs/2601.00204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00204">https://arxiv.org/pdf/2601.00204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00204]] MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing(https://arxiv.org/abs/2601.00204)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>3D morphing remains challenging due to the difficulty of generating semantically consistent and temporally smooth deformations, especially across categories. We present MorphAny3D, a training-free framework that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. Our key insight is that intelligently blending source and target SLAT features within the attention mechanisms of 3D generators naturally produces plausible morphing sequences. To this end, we introduce Morphing Cross-Attention (MCA), which fuses source and target information for structural coherence, and Temporal-Fused Self-Attention (TFSA), which enhances temporal consistency by incorporating features from preceding frames. An orientation correction strategy further mitigates the pose ambiguity within the morphing steps. Extensive experiments show that our method generates state-of-the-art morphing sequences, even for challenging cross-category cases. MorphAny3D further supports advanced applications such as decoupled morphing and 3D style transfer, and can be generalized to other SLAT-based generative models. Project page: this https URL.</li>
<li><strong>摘要：</strong>由于难以生成语义一致且时间平滑的变形，尤其是跨类别的变形，3D 变形仍然具有挑战性。我们推出 MorphAny3D，这是一个免训练框架，利用结构化潜在 (SLAT) 表示来实现高质量 3D 变形。我们的主要见解是，在 3D 生成器的注意力机制中智能地混合源和目标 SLAT 特征，自然会产生合理的变形序列。为此，我们引入了变形交叉注意（MCA）和时间融合自注意（TFSA），前者融合源和目标信息以实现结构一致性，后者通过合并先前帧的特征来增强时间一致性。方向校正策略进一步减轻了变形步骤中的姿势模糊性。大量的实验表明，我们的方法可以生成最先进的变形序列，即使对于具有挑战性的跨类别情况也是如此。 MorphAny3D 进一步支持解耦变形和 3D 风格迁移等高级应用，并且可以推广到其他基于 SLAT 的生成模型。项目页面：此 https URL。</li>
</ul>

<h3>Title: Unknown Aware AI-Generated Content Attribution</h3>
<ul>
<li><strong>Authors: </strong>Ellie Thieu, Jifan Zhang, Haoyue Bai</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00218">https://arxiv.org/abs/2601.00218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00218">https://arxiv.org/pdf/2601.00218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00218]] Unknown Aware AI-Generated Content Attribution(https://arxiv.org/abs/2601.00218)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of photorealistic generative models has made it increasingly important to attribute the origin of synthetic content, moving beyond binary real or fake detection toward identifying the specific model that produced a given image. We study the problem of distinguishing outputs from a target generative model (e.g., OpenAI Dalle 3) from other sources, including real images and images generated by a wide range of alternative models. Using CLIP features and a simple linear classifier, shown to be effective in prior work, we establish a strong baseline for target generator attribution using only limited labeled data from the target model and a small number of known generators. However, this baseline struggles to generalize to harder, unseen, and newly released generators. To address this limitation, we propose a constrained optimization approach that leverages unlabeled wild data, consisting of images collected from the Internet that may include real images, outputs from unknown generators, or even samples from the target model itself. The proposed method encourages wild samples to be classified as non target while explicitly constraining performance on labeled data to remain high. Experimental results show that incorporating wild data substantially improves attribution performance on challenging unseen generators, demonstrating that unlabeled data from the wild can be effectively exploited to enhance AI generated content attribution in open world settings.</li>
<li><strong>摘要：</strong>真实感生成模型的快速发展使得归因合成内容的来源变得越来越重要，超越了二进制真实或虚假检测，转向识别生成给定图像的特定模型。我们研究将目标生成模型（例如 OpenAI Dalle 3）的输出与其他来源（包括真实图像和由各种替代​​模型生成的图像）​​区分开来的问题。使用 CLIP 特征和简单的线性分类器（在之前的工作中被证明是有效的），我们仅使用来自目标模型的有限标记数据和少量已知生成器，为目标生成器归因建立了强大的基线。然而，这个基线很难推广到更难的、看不见的和新发布的生成器。为了解决这一限制，我们提出了一种约束优化方法，该方法利用未标记的野生数据，包括从互联网收集的图像，其中可能包括真实图像、未知生成器的输出，甚至来自目标模型本身的样本。所提出的方法鼓励野生样本被分类为非目标，同时明确限制标记数据的性能保持较高水平。实验结果表明，结合野生数据可以显着提高挑战看不见的生成器的归因性能，证明来自野生的未标记数据可以有效地利用来增强开放世界环境中人工智能生成的内容归因。</li>
</ul>

<h3>Title: Towards Syn-to-Real IQA: A Novel Perspective on Reshaping Synthetic Data Distributions</h3>
<ul>
<li><strong>Authors: </strong>Aobo Li, Jinjian Wu, Yongxu Liu, Leida Li, Weisheng Dong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00225">https://arxiv.org/abs/2601.00225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00225">https://arxiv.org/pdf/2601.00225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00225]] Towards Syn-to-Real IQA: A Novel Perspective on Reshaping Synthetic Data Distributions(https://arxiv.org/abs/2601.00225)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Blind Image Quality Assessment (BIQA) has advanced significantly through deep learning, but the scarcity of large-scale labeled datasets remains a challenge. While synthetic data offers a promising solution, models trained on existing synthetic datasets often show limited generalization ability. In this work, we make a key observation that representations learned from synthetic datasets often exhibit a discrete and clustered pattern that hinders regression performance: features of high-quality images cluster around reference images, while those of low-quality images cluster based on distortion types. Our analysis reveals that this issue stems from the distribution of synthetic data rather than model architecture. Consequently, we introduce a novel framework SynDR-IQA, which reshapes synthetic data distribution to enhance BIQA generalization. Based on theoretical derivations of sample diversity and redundancy's impact on generalization error, SynDR-IQA employs two strategies: distribution-aware diverse content upsampling, which enhances visual diversity while preserving content distribution, and density-aware redundant cluster downsampling, which balances samples by reducing the density of densely clustered areas. Extensive experiments across three cross-dataset settings (synthetic-to-authentic, synthetic-to-algorithmic, and synthetic-to-synthetic) demonstrate the effectiveness of our method. The code is available at this https URL.</li>
<li><strong>摘要：</strong>盲图像质量评估（BIQA）通过深度学习取得了显着进步，但大规模标记数据集的稀缺仍然是一个挑战。虽然合成数据提供了一个有前途的解决方案，但在现有合成数据集上训练的模型通常表现出有限的泛化能力。在这项工作中，我们进行了一个关键观察，即从合成数据集中学习的表示通常表现出阻碍回归性能的离散和聚类模式：高质量图像的特征聚集在参考图像周围，而低质量图像的特征则基于失真类型聚集。我们的分析表明，这个问题源于合成数据的分布而不是模型架构。因此，我们引入了一种新颖的框架 SynDR-IQA，它重塑了合成数据分布以增强 BIQA 泛化能力。基于样本多样性和冗余对泛化误差影响的理论推导，SynDR-IQA采用两种策略：分布感知的多样化内容上采样，在保留内容分布的同时增强视觉多样性；密度感知的冗余集群下采样，通过降低密集聚集区域的密度来平衡样本。跨三个跨数据集设置（合成到真实、合成到算法和合成到合成）的广泛实验证明了我们方法的有效性。该代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection</h3>
<ul>
<li><strong>Authors: </strong>Chao Yang, Haoyuan Zheng, Yue Ma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00237">https://arxiv.org/abs/2601.00237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00237">https://arxiv.org/pdf/2601.00237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00237]] Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection(https://arxiv.org/abs/2601.00237)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper addresses the critical bottleneck of infrared (IR) data scarcity in Printed Circuit Board (PCB) defect detection by proposing a cross-modal data augmentation framework integrating CycleGAN and YOLOv8. Unlike conventional methods relying on paired supervision, we leverage CycleGAN to perform unpaired image-to-image translation, mapping abundant visible-light PCB images into the infrared domain. This generative process synthesizes high-fidelity pseudo-IR samples that preserve the structural semantics of defects while accurately simulating thermal distribution patterns. Subsequently, we construct a heterogeneous training strategy that fuses generated pseudo-IR data with limited real IR samples to train a lightweight YOLOv8 detector. Experimental results demonstrate that this method effectively enhances feature learning under low-data conditions. The augmented detector significantly outperforms models trained on limited real data alone and approaches the performance benchmarks of fully supervised training, proving the efficacy of pseudo-IR synthesis as a robust augmentation strategy for industrial inspection.</li>
<li><strong>摘要：</strong>本文提出了一种集成 CycleGAN 和 YOLOv8 的跨模态数据增强框架，解决了印刷电路板 (PCB) 缺陷检测中红外 (IR) 数据稀缺的关键瓶颈。与依赖成对监督的传统方法不同，我们利用 CycleGAN 执行不成对的图像到图像转换，将丰富的可见光 PCB 图像映射到红外域。该生成过程合成高保真伪红外样本，保留缺陷的结构语义，同时准确模拟热分布模式。随后，我们构建了一种异构训练策略，将生成的伪红外数据与有限的真实红外样本融合，以训练轻量级 YOLOv8 检测器。实验结果表明，该方法有效增强了低数据条件下的特征学习。增强型探测器的性能显着优于仅在有限的真实数据上训练的模型，并接近完全监督训练的性能基准，证明了伪红外合成作为工业检测的稳健增强策略的有效性。</li>
</ul>

<h3>Title: TotalFM: An Organ-Separated Framework for 3D-CT Vision Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Kohei Yamamoto, Tomohiro Kikuchi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00260">https://arxiv.org/abs/2601.00260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00260">https://arxiv.org/pdf/2601.00260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00260]] TotalFM: An Organ-Separated Framework for 3D-CT Vision Foundation Models(https://arxiv.org/abs/2601.00260)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While foundation models in radiology are expected to be applied to various clinical tasks, computational cost constraints remain a major challenge when training on 3D-CT volumetric data. In this study, we propose TotalFM, a radiological foundation model that efficiently learns the correspondence between 3D-CT images and linguistic expressions based on the concept of organ separation, utilizing a large-scale dataset of 140,000 series. By automating the creation of organ volume and finding-sentence pairs through segmentation techniques and Large Language Model (LLM)-based radiology report processing, and by combining self-supervised pre-training via VideoMAE with contrastive learning using volume-text pairs, we aimed to balance computational efficiency and representation capability. In zero-shot organ-wise lesion classification tasks, the proposed model achieved higher F1 scores in 83% (5/6) of organs compared to CT-CLIP and 64% (9/14) of organs compared to Merlin. These results suggest that the proposed model exhibits high generalization performance in a clinical evaluation setting using actual radiology report sentences. Furthermore, in zero-shot finding-wise lesion classification tasks, our model achieved a higher AUROC in 83% (25/30) of finding categories compared to Merlin. We also confirmed performance comparable to existing Vision-Language Models (VLMs) in radiology report generation tasks. Our results demonstrate that the organ-separated learning framework can serve as a realistic and effective design guideline for the practical implementation of 3D-CT foundation models.</li>
<li><strong>摘要：</strong>虽然放射学基础模型预计将应用于各种临床任务，但在 3D-CT 体积数据训练时，计算成本限制仍然是一个主要挑战。在这项研究中，我们提出了TotalFM，一种放射学基础模型，它利用140,000系列的大规模数据集，基于器官分离的概念，有效地学习3D-CT图像和语言表达之间的对应关系。通过分割技术和基于大语言模型 (LLM) 的放射学报告处理自动创建器官体积和查找句子对，并通过 VideoMAE 进行自我监督预训练与使用体积文本对的对比学习相结合，我们的目标是平衡计算效率和表示能力。在零样本器官病变分类任务中，与 CT-CLIP 相比，所提出的模型在 83% (5/6) 的器官中获得了更高的 F1 分数，与 Merlin 相比，在 64% (9/14) 的器官中获得了更高的 F1 分数。这些结果表明，所提出的模型在使用实际放射学报告句子的临床评估环境中表现出较高的泛化性能。此外，在零样本查找明智的病变分类任务中，与 Merlin 相比，我们的模型在 83% (25/30) 的查找类别中实现了更高的 AUROC。我们还确认了在放射学报告生成任务中与现有视觉语言模型 (VLM) 相当的性能。我们的结果表明，器官分离学习框架可以作为 3D-CT 基础模型实际实施的现实有效的设计指南。</li>
</ul>

<h3>Title: ActErase: A Training-Free Paradigm for Precise Concept Erasure via Activation Patching</h3>
<ul>
<li><strong>Authors: </strong>Yi Sun, Xinhao Zhong, Hongyan Li, Yimin Zhou, Junhao Li, Bin Chen, Xuan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00267">https://arxiv.org/abs/2601.00267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00267">https://arxiv.org/pdf/2601.00267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00267]] ActErase: A Training-Free Paradigm for Precise Concept Erasure via Activation Patching(https://arxiv.org/abs/2601.00267)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in text-to-image diffusion models have demonstrated remarkable generation capabilities, yet they raise significant concerns regarding safety, copyright, and ethical implications. Existing concept erasure methods address these risks by removing sensitive concepts from pre-trained models, but most of them rely on data-intensive and computationally expensive fine-tuning, which poses a critical limitation. To overcome these challenges, inspired by the observation that the model's activations are predominantly composed of generic concepts, with only a minimal component can represent the target concept, we propose a novel training-free method (ActErase) for efficient concept erasure. Specifically, the proposed method operates by identifying activation difference regions via prompt-pair analysis, extracting target activations and dynamically replacing input activations during forward passes. Comprehensive evaluations across three critical erasure tasks (nudity, artistic style, and object removal) demonstrates that our training-free method achieves state-of-the-art (SOTA) erasure performance, while effectively preserving the model's overall generative capability. Our approach also exhibits strong robustness against adversarial attacks, establishing a new plug-and-play paradigm for lightweight yet effective concept manipulation in diffusion models.</li>
<li><strong>摘要：</strong>文本到图像扩散模型的最新进展已经展示了卓越的生成能力，但它们引起了对安全、版权和伦理影响的重大担忧。现有的概念擦除方法通过从预先训练的模型中删除敏感概念来解决这些风险，但它们大多数依赖于数据密集型和计算成本高昂的微调，这构成了关键的限制。为了克服这些挑战，受到模型的激活主要由通用概念组成，只有最小的组件可以代表目标概念的观察的启发，我们提出了一种新颖的免训练方法（ActErase）来有效地擦除概念。具体来说，所提出的方法通过提示对分析来识别激活差异区域，提取目标激活并在前向传播期间动态替换输入激活。对三个关键擦除任务（裸体、艺术风格和对象移除）的综合评估表明，我们的免训练方法实现了最先进的（SOTA）擦除性能，同时有效地保留了模型的整体生成能力。我们的方法还表现出针对对抗性攻击的强大鲁棒性，为扩散模型中轻量级但有效的概念操作建立了一种新的即插即用范例。</li>
</ul>

<h3>Title: SV-GS: Sparse View 4D Reconstruction with Skeleton-Driven Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Jun-Jee Chao, Volkan Isler</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00285">https://arxiv.org/abs/2601.00285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00285">https://arxiv.org/pdf/2601.00285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00285]] SV-GS: Sparse View 4D Reconstruction with Skeleton-Driven Gaussian Splatting(https://arxiv.org/abs/2601.00285)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Reconstructing a dynamic target moving over a large area is challenging. Standard approaches for dynamic object reconstruction require dense coverage in both the viewing space and the temporal dimension, typically relying on multi-view videos captured at each time step. However, such setups are only possible in constrained environments. In real-world scenarios, observations are often sparse over time and captured sparsely from diverse viewpoints (e.g., from security cameras), making dynamic reconstruction highly ill-posed. We present SV-GS, a framework that simultaneously estimates a deformation model and the object's motion over time under sparse observations. To initialize SV-GS, we leverage a rough skeleton graph and an initial static reconstruction as inputs to guide motion estimation. (Later, we show that this input requirement can be relaxed.) Our method optimizes a skeleton-driven deformation field composed of a coarse skeleton joint pose estimator and a module for fine-grained deformations. By making only the joint pose estimator time-dependent, our model enables smooth motion interpolation while preserving learned geometric details. Experiments on synthetic datasets show that our method outperforms existing approaches under sparse observations by up to 34% in PSNR, and achieves comparable performance to dense monocular video methods on real-world datasets despite using significantly fewer frames. Moreover, we demonstrate that the input initial static reconstruction can be replaced by a diffusion-based generative prior, making our method more practical for real-world scenarios.</li>
<li><strong>摘要：</strong>重建大范围移动的动态目标具有挑战性。动态对象重建的标准方法需要在观看空间和时间维度上进行密集覆盖，通常依赖于在每个时间步骤捕获的多视图视频。然而，这样的设置只能在受限的环境中进行。在现实场景中，随着时间的推移，观察结果往往是稀疏的，并且从不同的角度（例如，从安全摄像头）捕获的数据也稀疏，使得动态重建非常不适定。我们提出了 SV-GS，这是一个框架，可以在稀疏观测下同时估计变形模型和物体随时间的运动。为了初始化 SV-GS，我们利用粗略的骨架图和初始静态重建作为输入来指导运动估计。 （稍后，我们表明可以放宽此输入要求。）我们的方法优化了由粗略骨架关节姿态估计器和细粒度变形模块组成的骨架驱动变形场。通过仅使关节姿态估计器与时间相关，我们的模型可以实现平滑的运动插值，同时保留学习的几何细节。对合成数据集的实验表明，我们的方法在稀疏观测下的 PSNR 性能优于现有方法高达 34%，并且尽管使用的帧数明显减少，但在真实数据集上实现了与密集单目视频方法相当的性能。此外，我们证明输入初始静态重建可以用基于扩散的生成先验代替，使我们的方法对于现实场景更加实用。</li>
</ul>

<h3>Title: Joint Geometry-Appearance Human Reconstruction in a Unified Latent Space via Bridge Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Yingzhi Tang, Qijian Zhang, Junhui Hou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00328">https://arxiv.org/abs/2601.00328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00328">https://arxiv.org/pdf/2601.00328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00328]] Joint Geometry-Appearance Human Reconstruction in a Unified Latent Space via Bridge Diffusion(https://arxiv.org/abs/2601.00328)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Achieving consistent and high-fidelity geometry and appearance reconstruction of 3D digital humans from a single RGB image is inherently a challenging task. Existing studies typically resort to decoupled pipelines for geometry estimation and appearance synthesis, often hindering unified reconstruction and causing inconsistencies. This paper introduces \textbf{JGA-LBD}, a novel framework that unifies the modeling of geometry and appearance into a joint latent representation and formulates the generation process as bridge diffusion. Observing that directly integrating heterogeneous input conditions (e.g., depth maps, SMPL models) leads to substantial training difficulties, we unify all conditions into the 3D Gaussian representations, which can be further compressed into a unified latent space through a shared sparse variational autoencoder (VAE). Subsequently, the specialized form of bridge diffusion enables to start with a partial observation of the target latent code and solely focuses on inferring the missing components. Finally, a dedicated decoding module extracts the complete 3D human geometric structure and renders novel views from the inferred latent representation. Experiments demonstrate that JGA-LBD outperforms current state-of-the-art approaches in terms of both geometry fidelity and appearance quality, including challenging in-the-wild scenarios. Our code will be made publicly available at this https URL.</li>
<li><strong>摘要：</strong>从单个 RGB 图像实现 3D 数字人类一致且高保真的几何形状和外观重建本质上是一项具有挑战性的任务。现有的研究通常采用解耦的管道来进行几何估计和外观合成，这通常会阻碍统一重建并导致不一致。本文介绍了 \textbf{JGA-LBD}，这是一种新颖的框架，它将几何和外观的建模统一为联合潜在表示，并将生成过程表述为桥扩散。观察到直接集成异构输入条件（例如深度图、SMPL 模型）会导致大量的训练困难，我们将所有条件统一为 3D 高斯表示，并可以通过共享稀疏变分自动编码器（VAE）进一步压缩为统一的潜在空间。随后，桥扩散的特殊形式使得能够从对目标潜在代码的部分观察开始，并仅专注于推断缺失的组件。最后，专用解码模块提取完整的 3D 人体几何结构，并根据推断的潜在表示呈现新颖的视图。实验表明，JGA-LBD 在几何保真度和外观质量方面都优于当前最先进的方法，包括具有挑战性的野外场景。我们的代码将在此 https URL 上公开提供。</li>
</ul>

<h3>Title: OmniVaT: Single Domain Generalization for Multimodal Visual-Tactile Learning</h3>
<ul>
<li><strong>Authors: </strong>Liuxiang Qiu, Hui Da, Yuzhen Niu, Tiesong Zhao, Yang Cao, Zheng-Jun Zha</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00352">https://arxiv.org/abs/2601.00352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00352">https://arxiv.org/pdf/2601.00352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00352]] OmniVaT: Single Domain Generalization for Multimodal Visual-Tactile Learning(https://arxiv.org/abs/2601.00352)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Visual-tactile learning (VTL) enables embodied agents to perceive the physical world by integrating visual (VIS) and tactile (TAC) sensors. However, VTL still suffers from modality discrepancies between VIS and TAC images, as well as domain gaps caused by non-standardized tactile sensors and inconsistent data collection procedures. We formulate these challenges as a new task, termed single domain generalization for multimodal VTL (SDG-VTL). In this paper, we propose an OmniVaT framework that, for the first time, successfully addresses this task. On the one hand, OmniVaT integrates a multimodal fractional Fourier adapter (MFFA) to map VIS and TAC embeddings into a unified embedding-frequency space, thereby effectively mitigating the modality gap without multi-domain training data or careful cross-modal fusion strategies. On the other hand, it also incorporates a discrete tree generation (DTG) module that obtains diverse and reliable multimodal fractional representations through a hierarchical tree structure, thereby enhancing its adaptivity to fluctuating domain shifts in unseen domains. Extensive experiments demonstrate the superior cross-domain generalization performance of OmniVaT on the SDG-VTL task.</li>
<li><strong>摘要：</strong>视觉触觉学习（VTL）使实体代理能够通过集成视觉（VIS）和触觉（TAC）传感器来感知物理世界。然而，VTL 仍然存在 VIS 和 TAC 图像之间的模态差异，以及由非标准化触觉传感器和不一致的数据收集程序引起的域差距。我们将这些挑战表述为一项新任务，称为多模式 VTL 的单域泛化 (SDG-VTL)。在本文中，我们提出了一个 OmniVaT 框架，该框架首次成功解决了这一任务。一方面，OmniVaT 集成了多模态分数傅里叶适配器（MFFA），将 VIS 和 TAC 嵌入映射到统一的嵌入频率空间，从而有效地缩小模态差距，而无需多域训练数据或仔细的跨模态融合策略。另一方面，它还集成了离散树生成（DTG）模块，通过分层树结构获得多样化且可靠的多模态分数表示，从而增强其对未见域中波动域移位的适应性。大量实验证明了 OmniVaT 在 SDG-VTL 任务上具有卓越的跨域泛化性能。</li>
</ul>

<h3>Title: Mask-Conditioned Voxel Diffusion for Joint Geometry and Color Inpainting</h3>
<ul>
<li><strong>Authors: </strong>Aarya Sumuk</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00368">https://arxiv.org/abs/2601.00368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00368">https://arxiv.org/pdf/2601.00368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00368]] Mask-Conditioned Voxel Diffusion for Joint Geometry and Color Inpainting(https://arxiv.org/abs/2601.00368)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>We present a lightweight two-stage framework for joint geometry and color inpainting of damaged 3D objects, motivated by the digital restoration of cultural heritage artifacts. The pipeline separates damage localization from reconstruction. In the first stage, a 2D convolutional network predicts damage masks on RGB slices extracted from a voxelized object, and these predictions are aggregated into a volumetric mask. In the second stage, a diffusion-based 3D U-Net performs mask-conditioned inpainting directly on voxel grids, reconstructing geometry and color while preserving observed regions. The model jointly predicts occupancy and color using a composite objective that combines occupancy reconstruction with masked color reconstruction and perceptual regularization. We evaluate the approach on a curated set of textured artifacts with synthetically generated damage using standard geometric and color metrics. Compared to symmetry-based baselines, our method produces more complete geometry and more coherent color reconstructions at a fixed 32^3 resolution. Overall, the results indicate that explicit mask conditioning is a practical way to guide volumetric diffusion models for joint 3D geometry and color inpainting.</li>
<li><strong>摘要：</strong>受文化遗产文物数字修复的推动，我们提出了一个轻量级的两阶段框架，用于受损 3D 物体的关节几何和颜色修复。该管道将​​损伤定位与重建分开。在第一阶段，2D 卷积网络预测从体素化对象提取的 RGB 切片上的损坏掩模，并将这些预测聚合到体积掩模中。在第二阶段，基于扩散的 3D U-Net 直接在体素网格上执行掩模条件修复，重建几何形状和颜色，同时保留观察到的区域。该模型使用将占用重建与遮蔽颜色重建和感知正则化相结合的复合目标来联合预测占用和颜色。我们使用标准几何和颜色指标对一组经过综合生成的损坏的纹理工件进行评估。与基于对称的基线相比，我们的方法在固定的 32^3 分辨率下产生更完整的几何形状和更连贯的颜色重建。总体而言，结果表明显式掩模调节是指导联合 3D 几何和颜色修复的体积扩散模型的实用方法。</li>
</ul>

<h3>Title: NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos</h3>
<ul>
<li><strong>Authors: </strong>Yuxue Yang, Lue Fan, Ziqi Shi, Junran Peng, Feng Wang, Zhaoxiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00393">https://arxiv.org/abs/2601.00393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00393">https://arxiv.org/pdf/2601.00393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00393]] NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos(https://arxiv.org/abs/2601.00393)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at this https URL</li>
<li><strong>摘要：</strong>在本文中，我们提出了 NeoVerse，一种多功能的 4D 世界模型，能够进行 4D 重建、新轨迹视频生成和丰富的下游应用。我们首先确定当前 4D 世界建模方法中可扩展性的常见限制，该限制是由昂贵且专门的多视图 4D 数据或繁琐的训练预处理引起的。相比之下，我们的 NeoVerse 建立在一个核心理念之上，该理念使整个管道可扩展至各种野外单目视频。具体来说，NeoVerse 具有无姿态前馈 4D 重建、在线单目退化模式模拟和其他良好对齐的技术。这些设计使 NeoVerse 具有多功能性并可推广到各个领域。同时，NeoVerse 在标准重建和生成基准方面实现了最先进的性能。我们的项目页面可通过此 https URL 访问</li>
</ul>

<h3>Title: Imitation from Observations with Trajectory-Level Generative Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Yongtao Qu, Shangzhe Li, Weitong Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00452">https://arxiv.org/abs/2601.00452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00452">https://arxiv.org/pdf/2601.00452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00452]] Imitation from Observations with Trajectory-Level Generative Embeddings(https://arxiv.org/abs/2601.00452)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We consider the offline imitation learning from observations (LfO) where the expert demonstrations are scarce and the available offline suboptimal data are far from the expert behavior. Many existing distribution-matching approaches struggle in this regime because they impose strict support constraints and rely on brittle one-step models, making it hard to extract useful signal from imperfect data. To tackle this challenge, we propose TGE, a trajectory-level generative embedding for offline LfO that constructs a dense, smooth surrogate reward by estimating expert state density in the latent space of a temporal diffusion model trained on offline trajectory data. By leveraging the smooth geometry of the learned diffusion embedding, TGE captures long-horizon temporal dynamics and effectively bridges the gap between disjoint supports, ensuring a robust learning signal even when offline data is distributionally distinct from the expert. Empirically, the proposed approach consistently matches or outperforms prior offline LfO methods across a range of D4RL locomotion and manipulation benchmarks.</li>
<li><strong>摘要：</strong>我们考虑从观察中进行离线模仿学习（LfO），其中专家演示很少，并且可用的离线次优数据与专家行为相去甚远。许多现有的分布匹配方法在这种情况下举步维艰，因为它们施加了严格的支持约束并依赖于脆弱的一步模型，使得很难从不完美的数据中提取有用的信号。为了应对这一挑战，我们提出了 TGE，一种用于离线 LfO 的轨迹级生成嵌入，它通过估计在离线轨迹数据上训练的时间扩散模型的潜在空间中的专家状态密度来构建密集、平滑的代理奖励。通过利用学习的扩散嵌入的平滑几何结构，TGE 捕获长范围时间动态并有效地弥合不相交支持之间的差距，即使离线数据在分布上与专家不同时也能确保强大的学习信号。根据经验，所提出的方法在一系列 D4RL 运动和操纵基准中始终匹配或优于先前的离线 LfO 方法。</li>
</ul>

<h3>Title: When Small Models Are Right for Wrong Reasons: Process Verification for Trustworthy Agents</h3>
<ul>
<li><strong>Authors: </strong>Laksh Advani</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00513">https://arxiv.org/abs/2601.00513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00513">https://arxiv.org/pdf/2601.00513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00513]] When Small Models Are Right for Wrong Reasons: Process Verification for Trustworthy Agents(https://arxiv.org/abs/2601.00513)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Deploying small language models (7-9B parameters) as autonomous agents requires trust in their reasoning, not just their outputs. We reveal a critical reliability crisis: 50-69\% of correct answers from these models contain fundamentally flawed reasoning -- a ``Right-for-Wrong-Reasons'' phenomenon invisible to standard accuracy metrics. Through analysis of 10,734 reasoning traces across three models and diverse tasks, we introduce the Reasoning Integrity Score (RIS), a process-based metric validated with substantial inter-rater agreement ($\kappa=0.657$). Conventional practices are challenged by our findings: while retrieval-augmented generation (RAG) significantly improves reasoning integrity (Cohen's $d=0.23$--$0.93$), meta-cognitive interventions like self-critique often harm performance ($d=-0.14$ to $-0.33$) in small models on the evaluated tasks. Mechanistic analysis reveals RAG succeeds by grounding calculations in external evidence, reducing errors by 7.6\%, while meta-cognition amplifies confusion without sufficient model capacity. To enable deployment, verification capabilities are distilled into a neural classifier achieving 0.86 F1-score with 100$\times$ speedup. These results underscore the necessity of process-based verification for trustworthy agents: accuracy alone is dangerously insufficient when models can be right for entirely wrong reasons.</li>
<li><strong>摘要：</strong>将小语言模型（7-9B 参数）部署为自主代理需要信任它们的推理，而不仅仅是它们的输出。我们揭示了一个严重的可靠性危机：这些模型中 50-69% 的正确答案包含根本上有缺陷的推理——一种标准准确度指标看不到的“错误原因正确”现象。通过对三个模型和不同任务的 10,734 条推理轨迹进行分析，我们引入了推理完整性评分 (RIS)，这是一种基于流程的指标，经过大量评估者间一致性验证 ($\kappa=0.657$)。我们的研究结果对传统做法提出了挑战：虽然检索增强生成（RAG）显着提高了推理完整性（Cohen 的 $d=0.23$--$0.93$），但自我批评等元认知干预通常会损害评估任务的小模型中的表现（$d=-0.14$ 至 $-0.33$）。机制分析表明，RAG 的成功在于将计算建立在外部证据的基础上，将错误减少了 7.6%，而元认知在模型容量不足的情况下会放大混乱。为了实现部署，验证功能被提炼到神经分类器中，实现 0.86 F1 分数和 100$\times$ 加速。这些结果强调了对值得信赖的代理进行基于流程的验证的必要性：当模型因完全错误的原因而正确时，仅靠准确性是不够的，这是危险的。</li>
</ul>

<h3>Title: A Sparse-Attention Deep Learning Model Integrating Heterogeneous Multimodal Features for Parkinson's Disease Severity Profiling</h3>
<ul>
<li><strong>Authors: </strong>Dristi Datta, Tanmoy Debnath, Minh Chau, Manoranjan Paul, Gourab Adhikary, Md Geaur Rahman</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00519">https://arxiv.org/abs/2601.00519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00519">https://arxiv.org/pdf/2601.00519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00519]] A Sparse-Attention Deep Learning Model Integrating Heterogeneous Multimodal Features for Parkinson's Disease Severity Profiling(https://arxiv.org/abs/2601.00519)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Characterising the heterogeneous presentation of Parkinson's disease (PD) requires integrating biological and clinical markers within a unified predictive framework. While multimodal data provide complementary information, many existing computational models struggle with interpretability, class imbalance, or effective fusion of high-dimensional imaging and tabular clinical features. To address these limitations, we propose the Class-Weighted Sparse-Attention Fusion Network (SAFN), an interpretable deep learning framework for robust multimodal profiling. SAFN integrates MRI cortical thickness, MRI volumetric measures, clinical assessments, and demographic variables using modality-specific encoders and a symmetric cross-attention mechanism that captures nonlinear interactions between imaging and clinical representations. A sparsity-constrained attention-gating fusion layer dynamically prioritises informative modalities, while a class-balanced focal loss (beta = 0.999, gamma = 1.5) mitigates dataset imbalance without synthetic oversampling. Evaluated on 703 participants (570 PD, 133 healthy controls) from the Parkinson's Progression Markers Initiative using subject-wise five-fold cross-validation, SAFN achieves an accuracy of 0.98 plus or minus 0.02 and a PR-AUC of 1.00 plus or minus 0.00, outperforming established machine learning and deep learning baselines. Interpretability analysis shows a clinically coherent decision process, with approximately 60 percent of predictive weight assigned to clinical assessments, consistent with Movement Disorder Society diagnostic principles. SAFN provides a reproducible and transparent multimodal modelling paradigm for computational profiling of neurodegenerative disease.</li>
<li><strong>摘要：</strong>表征帕金森病 (PD) 的异质性表现需要将生物和临床标志物整合到统一的预测框架内。虽然多模态数据提供补充信息，但许多现有的计算模型在可解释性、类别不平衡或高维成像和表格临床特征的有效融合方面遇到困难。为了解决这些限制，我们提出了类加权稀疏注意力融合网络（SAFN），这是一种用于稳健多模态分析的可解释深度学习框架。 SAFN 使用特定模态编码器和捕获成像和临床表征之间非线性相互作用的对称交叉注意机制，集成了 MRI 皮质厚度、MRI 体积测量、临床评估和人口统计变量。稀疏约束的注意力门融合层动态地优先考虑信息模式，而类平衡的焦点损失（beta = 0.999，gamma = 1.5）可以减轻数据集不平衡，而无需合成过采样。使用按主题五倍交叉验证对帕金森病进展标记计划的 703 名参与者（570 名 PD、133 名健康对照）进行评估，SAFN 的准确度为 0.98 正负 0.02，PR-AUC 为 1.00 正负 0.00，优于既定的机器学习和深度学习基线。可解释性分析显示了临床连贯的决策过程，大约 60% 的预测权重分配给临床评估，与运动障碍协会的诊断原则一致。 SAFN 为神经退行性疾病的计算分析提供了可重复且透明的多模态建模范例。</li>
</ul>

<h3>Title: Federated Customization of Large Models: Approaches, Experiments, and Insights</h3>
<ul>
<li><strong>Authors: </strong>Yuchuan Ye, Ming Ding, Youjia Chen, Peng Cheng, Dusit Niyato</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00526">https://arxiv.org/abs/2601.00526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00526">https://arxiv.org/pdf/2601.00526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00526]] Federated Customization of Large Models: Approaches, Experiments, and Insights(https://arxiv.org/abs/2601.00526)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In this article, we explore federated customization of large models and highlight the key challenges it poses within the federated learning framework. We review several popular large model customization techniques, including full fine-tuning, efficient fine-tuning, prompt engineering, prefix-tuning, knowledge distillation, and retrieval-augmented generation. Then, we discuss how these techniques can be implemented within the federated learning framework. Moreover, we conduct experiments on federated prefix-tuning, which, to the best of our knowledge, is the first trial to apply prefix-tuning in the federated learning setting. The conducted experiments validate its feasibility with performance close to centralized approaches. Further comparison with three other federated customization methods demonstrated its competitive performance, satisfactory efficiency, and consistent robustness.</li>
<li><strong>摘要：</strong>在本文中，我们探讨了大型模型的联邦定制，并强调了它在联邦学习框架中带来的关键挑战。我们回顾了几种流行的大型模型定制技术，包括完全微调、高效微调、即时工程、前缀调优、知识蒸馏和检索增强生成。然后，我们讨论如何在联邦学习框架中实施这些技术。此外，我们还进行了联邦前缀调整的实验，据我们所知，这是在联邦学习环境中应用前缀调整的第一个尝试。进行的实验验证了其可行性，性能接近集中式方法。与其他三种联合定制方法的进一步比较表明了其具有竞争力的性能、令人满意的效率和一致的鲁棒性。</li>
</ul>

<h3>Title: Cloud-Native Generative AI for Automated Planogram Synthesis: A Diffusion Model Approach for Multi-Store Retail Optimization</h3>
<ul>
<li><strong>Authors: </strong>Ravi Teja Pagidoju, Shriya Agarwal</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00527">https://arxiv.org/abs/2601.00527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00527">https://arxiv.org/pdf/2601.00527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00527]] Cloud-Native Generative AI for Automated Planogram Synthesis: A Diffusion Model Approach for Multi-Store Retail Optimization(https://arxiv.org/abs/2601.00527)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Planogram creation is a significant challenge for retail, requiring an average of 30 hours per complex layout. This paper introduces a cloud-native architecture using diffusion models to automatically generate store-specific planograms. Unlike conventional optimization methods that reorganize existing layouts, our system learns from successful shelf arrangements across multiple retail locations to create new planogram configurations. The architecture combines cloud-based model training via AWS with edge deployment for real-time inference. The diffusion model integrates retail-specific constraints through a modified loss function. Simulation-based analysis demonstrates the system reduces planogram design time by 98.3% (from 30 to 0.5 hours) while achieving 94.4% constraint satisfaction. Economic analysis reveals a 97.5% reduction in creation expenses with a 4.4-month break-even period. The cloud-native architecture scales linearly, supporting up to 10,000 concurrent store requests. This work demonstrates the viability of generative AI for automated retail space optimization.</li>
<li><strong>摘要：</strong>货架图创建对于零售业来说是一项重大挑战，每个复杂的布局平均需要 30 个小时。本文介绍了一种使用扩散模型自动生成商店特定货架图的云原生架构。与重新组织现有布局的传统优化方法不同，我们的系统从多个零售地点的成功货架布置中学习，以创建新的货架图配置。该架构将通过 AWS 进行的基于云的模型训练与用于实时推理的边缘部署相结合。扩散模型通过修改后的损失函数整合了零售特定的约束。基于仿真的分析表明，该系统将货架图设计时间缩短了 98.3%（从 30 小时减少到 0.5 小时），同时实现了 94.4% 的约束满足。经济分析显示，创作费用减少了 97.5%，盈亏平衡期为 4.4 个月。云原生架构线性扩展，支持高达 10,000 个并发存储请求。这项工作展示了生成式人工智能在自动化零售空间优化方面的可行性。</li>
</ul>

<h3>Title: All-in-One Video Restoration under Smoothly Evolving Unknown Weather Degradations</h3>
<ul>
<li><strong>Authors: </strong>Wenrui Li, Hongtao Chen, Yao Xiao, Wangmeng Zuo, Jiantao Zhou, Yonghong Tian, Xiaopeng Fan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00533">https://arxiv.org/abs/2601.00533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00533">https://arxiv.org/pdf/2601.00533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00533]] All-in-One Video Restoration under Smoothly Evolving Unknown Weather Degradations(https://arxiv.org/abs/2601.00533)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generation</a></li>
<li><strong>Abstract: </strong>All-in-one image restoration aims to recover clean images from diverse unknown degradations using a single model. But extending this task to videos faces unique challenges. Existing approaches primarily focus on frame-wise degradation variation, overlooking the temporal continuity that naturally exists in real-world degradation processes. In practice, degradation types and intensities evolve smoothly over time, and multiple degradations may coexist or transition gradually. In this paper, we introduce the Smoothly Evolving Unknown Degradations (SEUD) scenario, where both the active degradation set and degradation intensity change continuously over time. To support this scenario, we design a flexible synthesis pipeline that generates temporally coherent videos with single, compound, and evolving degradations. To address the challenges in the SEUD scenario, we propose an all-in-One Recurrent Conditional and Adaptive prompting Network (ORCANet). First, a Coarse Intensity Estimation Dehazing (CIED) module estimates haze intensity using physical priors and provides coarse dehazed features as initialization. Second, a Flow Prompt Generation (FPG) module extracts degradation features. FPG generates both static prompts that capture segment-level degradation types and dynamic prompts that adapt to frame-level intensity variations. Furthermore, a label-aware supervision mechanism improves the discriminability of static prompt representations under different degradations. Extensive experiments show that ORCANet achieves superior restoration quality, temporal consistency, and robustness over image and video-based baselines. Code is available at this https URL.</li>
<li><strong>摘要：</strong>多合一图像恢复旨在使用单一模型从各种未知的退化中恢复干净的图像。但将这项任务扩展到视频面临着独特的挑战。现有方法主要关注逐帧退化变化，忽略了现实世界退化过程中自然存在的时间连续性。在实践中，退化类型和强度随着时间的推移平稳演变，多种退化可能共存或逐渐过渡。在本文中，我们介绍了平滑演化的未知退化（SEUD）场景，其中主动退化集和退化强度都随着时间的推移而不断变化。为了支持这种情况，我们设计了一个灵活的合成管道，可以生成具有单一、复合和演化退化的时间连贯视频。为了解决 SEUD 场景中的挑战，我们提出了一种一体化的循环条件和自适应提示网络（ORCANet）。首先，粗略强度估计去雾 (CIED) 模块使用物理先验估计雾度强度，并提供粗略去雾特征作为初始化。其次，流程提示生成（FPG）模块提取退化特征。 FPG 生成捕获段级退化类型的静态提示和适应帧级强度变化的动态提示。此外，标签感知监督机制提高了不同退化下静态提示表示的可辨别性。大量实验表明，与基于图像和视频的基线相比，ORCANet 实现了卓越的恢复质量、时间一致性和鲁棒性。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: A Comprehensive Dataset for Human vs. AI Generated Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Rajarshi Roy, Nasrin Imanpour, Ashhar Aziz, Shashwat Bajpai, Gurpreet Singh, Shwetangshu Biswas, Kapil Wanaskar, Parth Patwa, Subhankar Ghosh, Shreyas Dixit, Nilesh Ranjan Pal, Vipula Rawte, Ritvik Garimella, Gaytri Jena, Vasu Sharma, Vinija Jain, Aman Chadha, Aishwarya Naresh Reganti, Amitava Das</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00553">https://arxiv.org/abs/2601.00553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00553">https://arxiv.org/pdf/2601.00553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00553]] A Comprehensive Dataset for Human vs. AI Generated Image Detection(https://arxiv.org/abs/2601.00553)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Multimodal generative AI systems like Stable Diffusion, DALL-E, and MidJourney have fundamentally changed how synthetic images are created. These tools drive innovation but also enable the spread of misleading content, false information, and manipulated media. As generated images become harder to distinguish from photographs, detecting them has become an urgent priority. To combat this challenge, We release MS COCOAI, a novel dataset for AI generated image detection consisting of 96000 real and synthetic datapoints, built using the MS COCO dataset. To generate synthetic images, we use five generators: Stable Diffusion 3, Stable Diffusion 2.1, SDXL, DALL-E 3, and MidJourney v6. Based on the dataset, we propose two tasks: (1) classifying images as real or generated, and (2) identifying which model produced a given synthetic image. The dataset is available at this https URL.</li>
<li><strong>摘要：</strong>Stable Diffusion、DALL-E 和 MidJourney 等多模式生成 AI 系统从根本上改变了合成图像的创建方式。这些工具推动了创新，但也导致了误导性内容、虚假信息和受操纵媒体的传播。随着生成的图像与照片变得越来越难以区分，检测它们已成为当务之急。为了应对这一挑战，我们发布了 MS COCOAI，这是一个用于 AI 生成图像检测的新颖数据集，由 96000 个真实和合成数据点组成，使用 MS COCO 数据集构建。为了生成合成图像，我们使用五个生成器：Stable Diffusion 3、Stable Diffusion 2.1、SDXL、DALL-E 3 和 MidJourney v6。基于数据集，我们提出两个任务：（1）将图像分类为真实图像或生成图像，以及（2）识别哪个模型生成给定的合成图像。该数据集可从此 https URL 获取。</li>
</ul>

<h3>Title: AEGIS: Exploring the Limit of World Knowledge Capabilities for Unified Mulitmodal Models</h3>
<ul>
<li><strong>Authors: </strong>Jintao Lin, Bowen Dong, Weikang Shi, Chenyang Lei, Suiyun Zhang, Rui Liu, Xihui Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00561">https://arxiv.org/abs/2601.00561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00561">https://arxiv.org/pdf/2601.00561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00561]] AEGIS: Exploring the Limit of World Knowledge Capabilities for Unified Mulitmodal Models(https://arxiv.org/abs/2601.00561)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The capability of Unified Multimodal Models (UMMs) to apply world knowledge across diverse tasks remains a critical, unresolved challenge. Existing benchmarks fall short, offering only siloed, single-task evaluations with limited diagnostic power. To bridge this gap, we propose AEGIS (\emph{i.e.}, \textbf{A}ssessing \textbf{E}diting, \textbf{G}eneration, \textbf{I}nterpretation-Understanding for \textbf{S}uper-intelligence), a comprehensive multi-task benchmark covering visual understanding, generation, editing, and interleaved generation. AEGIS comprises 1,050 challenging, manually-annotated questions spanning 21 topics (including STEM, humanities, daily life, etc.) and 6 reasoning types. To concretely evaluate the performance of UMMs in world knowledge scope without ambiguous metrics, we further propose Deterministic Checklist-based Evaluation (DCE), a protocol that replaces ambiguous prompt-based scoring with atomic ``Y/N'' judgments, to enhance evaluation reliability. Our extensive experiments reveal that most UMMs exhibit severe world knowledge deficits and that performance degrades significantly with complex reasoning. Additionally, simple plug-in reasoning modules can partially mitigate these vulnerabilities, highlighting a promising direction for future research. These results highlight the importance of world-knowledge-based reasoning as a critical frontier for UMMs.</li>
<li><strong>摘要：</strong>统一多模态模型 (UMM) 在不同任务中应用世界知识的能力仍然是一个尚未解决的关键挑战。现有的基准测试存在不足，仅提供孤立的、单一任务的评估，且诊断能力有限。为了弥补这一差距，我们提出了 AEGIS（\emph{i.e}、\textbf{A}ssessing \textbf{E}diting、\textbf{G}eneration、\textbf{I}nterpretation-Understanding for \textbf{S}super-intelligence），这是一个涵盖视觉理解、生成、编辑和交错生成的综合多任务基准。 AEGIS 包含 1,050 个具有挑战性的手动注释问题，涵盖 21 个主题（包括 STEM、人文、日常生活等）和 6 种推理类型。为了在没有模糊度量的情况下具体评估 UMM 在世界知识范围内的性能，我们进一步提出了基于确定性检查表的评估（DCE），这是一种用原子“Y/N”判断取代模糊的基于提示的评分的协议，以提高评估的可靠性。我们广泛的实验表明，大多数 UMM 都表现出严重的世界知识缺陷，并且复杂推理的性能会显着下降。此外，简单的插件推理模块可以部分缓解这些漏洞，为未来研究指明了一个有希望的方向。这些结果凸显了基于世界知识的推理作为 UMM 关键前沿的重要性。</li>
</ul>

<h3>Title: GranAlign: Granularity-Aware Alignment Framework for Zero-Shot Video Moment Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Mingyu Jeon, Sunjae Yoon, Jonghee Kim, Junyeoung Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00584">https://arxiv.org/abs/2601.00584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00584">https://arxiv.org/pdf/2601.00584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00584]] GranAlign: Granularity-Aware Alignment Framework for Zero-Shot Video Moment Retrieval(https://arxiv.org/abs/2601.00584)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Zero-shot video moment retrieval (ZVMR) is the task of localizing a temporal moment within an untrimmed video using a natural language query without relying on task-specific training data. The primary challenge in this setting lies in the mismatch in semantic granularity between textual queries and visual content. Previous studies in ZVMR have attempted to achieve alignment by leveraging high-quality pre-trained knowledge that represents video and language in a joint space. However, these approaches failed to balance the semantic granularity between the pre-trained knowledge provided by each modality for a given scene. As a result, despite the high quality of each modality's representations, the mismatch in granularity led to inaccurate retrieval. In this paper, we propose a training-free framework, called Granularity-Aware Alignment (GranAlign), that bridges this gap between coarse and fine semantic representations. Our approach introduces two complementary techniques: granularity-based query rewriting to generate varied semantic granularities, and query-aware caption generation to embed query intent into video content. By pairing multi-level queries with both query-agnostic and query-aware captions, we effectively resolve semantic mismatches. As a result, our method sets a new state-of-the-art across all three major benchmarks (QVHighlights, Charades-STA, ActivityNet-Captions), with a notable 3.23% mAP@avg improvement on the challenging QVHighlights dataset.</li>
<li><strong>摘要：</strong>零镜头视频时刻检索 (ZVMR) 是使用自然语言查询在未修剪的视频中定位时间时刻的任务，而不依赖于特定于任务的训练数据。此设置中的主要挑战在于文本查询和视觉内容之间语义粒度的不匹配。之前的 ZVMR 研究试图通过利用在联合空间中表示视频和语言的高质量预训练知识来实现​​对齐。然而，这些方法未能平衡给定场景的每种模态提供的预训练知识之间的语义粒度。结果，尽管每种模态的表示质量很高，但粒度的不匹配导致检索不准确。在本文中，我们提出了一种免训练框架，称为粒度感知对齐（GranAlign），它弥补了粗略和精细语义表示之间的差距。我们的方法引入了两种互补技术：基于粒度的查询重写以生成不同的语义粒度，以及查询感知字幕生成以将查询意图嵌入到视频内容中。通过将多级查询与查询无关和查询感知的标题配对，我们有效地解决了语义不匹配问题。因此，我们的方法在所有三个主要基准（QVHighlights、Charades-STA、ActivityNet-Captions）上都创下了新的最先进水平，在具有挑战性的 QVHighlights 数据集上，mAP@avg 显着提高了 3.23%。</li>
</ul>

<h3>Title: SafeMo: Linguistically Grounded Unlearning for Trustworthy Text-to-Motion Generation</h3>
<ul>
<li><strong>Authors: </strong>Yiling Wang, Zeyu Zhang, Yiran Wang, Hao Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00590">https://arxiv.org/abs/2601.00590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00590">https://arxiv.org/pdf/2601.00590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00590]] SafeMo: Linguistically Grounded Unlearning for Trustworthy Text-to-Motion Generation(https://arxiv.org/abs/2601.00590)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Text-to-motion (T2M) generation with diffusion backbones achieves strong realism and alignment. Safety concerns in T2M methods have been raised in recent years; existing methods replace discrete VQ-VAE codebook entries to steer the model away from unsafe behaviors. However, discrete codebook replacement-based methods have two critical flaws: firstly, replacing codebook entries which are reused by benign prompts leads to drifts on everyday tasks, degrading the model's benign performance; secondly, discrete token-based methods introduce quantization and smoothness loss, resulting in artifacts and jerky transitions. Moreover, existing text-to-motion datasets naturally contain unsafe intents and corresponding motions, making them unsuitable for safety-driven machine learning. To address these challenges, we propose SafeMo, a trustworthy motion generative framework integrating Minimal Motion Unlearning (MMU), a two-stage machine unlearning strategy, enabling safe human motion generation in continuous space, preserving continuous kinematics without codebook loss and delivering strong safety-utility trade-offs compared to current baselines. Additionally, we present the first safe text-to-motion dataset SafeMoVAE-29K integrating rewritten safe text prompts and continuous refined motion for trustworthy human motion unlearning. Built upon DiP, SafeMo efficiently generates safe human motions with natural transitions. Experiments demonstrate effective unlearning performance of SafeMo by showing strengthened forgetting on unsafe prompts, reaching 2.5x and 14.4x higher forget-set FID on HumanML3D and Motion-X respectively, compared to the previous SOTA human motion unlearning method LCR, with benign performance on safe prompts being better or comparable. Code: this https URL. Website: this https URL.</li>
<li><strong>摘要：</strong>具有扩散主干的文本到运动 (T2M) 生成实现了强大的真实感和对齐。近年来，T2M 方法的安全性问题引起了人们的关注；现有方法取代了离散的 VQ-VAE 码本条目，以引导模型远离不安全行为。然而，基于离散码本替换的方法有两个严重缺陷：首先，替换良性提示重复使用的码本条目会导致日常任务发生偏差，从而降低模型的良性性能；其次，基于离散标记的方法引入了量化和平滑度损失，导致伪像和不稳定的过渡。此外，现有的文本到运动数据集自然包含不安全的意图和相应的运动，使得它们不适合安全驱动的机器学习。为了应对这些挑战，我们提出了 SafeMo，这是一种值得信赖的运动生成框架，集成了最小运动取消学习（MMU）（一种两阶段机器取消学习策略），能够在连续空间中安全地生成人体运动，在不丢失密码本的情况下保持连续运动学，并与当前基线相比提供强大的安全性与实用性权衡。此外，我们还提出了第一个安全文本到运动数据集 SafeMoVAE-29K，集成了重写的安全文本提示和持续精细的运动，以实现值得信赖的人体运动遗忘。 SafeMo 基于 DiP 构建，可有效生成具有自然过渡的安全人体动作。实验证明了 SafeMo 的有效忘却性能，与之前的 SOTA 人体运动忘却方法 LCR 相比，对不安全提示的遗忘得到了加强，在 HumanML3D 和 Motion-X 上分别达到了 2.5 倍和 14.4 倍的高遗忘集 FID，并且在安全提示上的良性表现更好或相当。代码：此 https URL。网站：此 https URL。</li>
</ul>

<h3>Title: Do Chatbot LLMs Talk Too Much? The YapBench Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Vadim Borisov, Michael Gröger, Mina Mikhael, Richard H. Schreiber</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00624">https://arxiv.org/abs/2601.00624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00624">https://arxiv.org/pdf/2601.00624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00624]] Do Chatbot LLMs Talk Too Much? The YapBench Benchmark(https://arxiv.org/abs/2601.00624)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) such as ChatGPT, Claude, and Gemini increasingly act as general-purpose copilots, yet they often respond with unnecessary length on simple requests, adding redundant explanations, hedging, or boilerplate that increases cognitive load and inflates token-based inference cost. Prior work suggests that preference-based post-training and LLM-judged evaluations can induce systematic length bias, where longer answers are rewarded even at comparable quality. We introduce YapBench, a lightweight benchmark for quantifying user-visible over-generation on brevity-ideal prompts. Each item consists of a single-turn prompt, a curated minimal-sufficient baseline answer, and a category label. Our primary metric, YapScore, measures excess response length beyond the baseline in characters, enabling comparisons across models without relying on any specific tokenizer. We summarize model performance via the YapIndex, a uniformly weighted average of category-level median YapScores. YapBench contains over three hundred English prompts spanning three common brevity-ideal settings: (A) minimal or ambiguous inputs where the ideal behavior is a short clarification, (B) closed-form factual questions with short stable answers, and (C) one-line coding tasks where a single command or snippet suffices. Evaluating 76 assistant LLMs, we observe an order-of-magnitude spread in median excess length and distinct category-specific failure modes, including vacuum-filling on ambiguous inputs and explanation or formatting overhead on one-line technical requests. We release the benchmark and maintain a live leaderboard for tracking verbosity behavior over time.</li>
<li><strong>摘要：</strong>ChatGPT、Claude 和 Gemini 等大型语言模型 (LLM) 越来越多地充当通用副驾驶，但它们经常对简单请求做出不必要的长度响应，添加冗余解释、对冲或样板文件，从而增加认知负荷并增加基于令牌的推理成本。先前的研究表明，基于偏好的培训后和法学硕士评审的评估可能会导致系统性长度偏差，即即使质量相当，较长的答案也会得到奖励。我们引入了 YapBench，这是一个轻量级基准，用于根据简洁理想的提示量化用户可见的过度生成。每个项目都包含一个单轮提示、一个精心策划的最小足够基线答案和一个类别标签。我们的主要指标 YapScore 测量超出字符基线的额外响应长度，从而无需依赖任何特定标记器即可进行模型之间的比较。我们通过 YapIndex 总结模型性能，YapIndex 是类别级别中值 YapScore 的均匀加权平均值。 YapBench 包含三百多个英语提示，涵盖三种常见的简洁理想设置：(A) 最小或模糊的输入，理想行为是简短的澄清；(B) 具有简短稳定答案的封闭式事实问题；以及 (C) 一行编码任务，其中单个命令或片段就足够了。通过评估 76 名助理法学硕士，我们观察到中位超长和不同类别特定故障模式的数量级分布，包括对不明确输入的真空填充以及对单行技术请求的解释或格式化开销。我们发布基准并维护实时排行榜，以跟踪一段时间内的冗长行为。</li>
</ul>

<h3>Title: Quality Detection of Stored Potatoes via Transfer Learning: A CNN and Vision Transformer Approach</h3>
<ul>
<li><strong>Authors: </strong>Shrikant Kapse, Priyankkumar Dhrangdhariya, Priya Kedia, Manasi Patwardhan, Shankar Kausley, Soumyadipta Maiti, Beena Rai, Shirish Karande</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00645">https://arxiv.org/abs/2601.00645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00645">https://arxiv.org/pdf/2601.00645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00645]] Quality Detection of Stored Potatoes via Transfer Learning: A CNN and Vision Transformer Approach(https://arxiv.org/abs/2601.00645)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Image-based deep learning provides a non-invasive, scalable solution for monitoring potato quality during storage, addressing key challenges such as sprout detection, weight loss estimation, and shelf-life prediction. In this study, images and corresponding weight data were collected over a 200-day period under controlled temperature and humidity conditions. Leveraging powerful pre-trained architectures of ResNet, VGG, DenseNet, and Vision Transformer (ViT), we designed two specialized models: (1) a high-precision binary classifier for sprout detection, and (2) an advanced multi-class predictor to estimate weight loss and forecast remaining shelf-life with remarkable accuracy. DenseNet achieved exceptional performance, with 98.03% accuracy in sprout detection. Shelf-life prediction models performed best with coarse class divisions (2-5 classes), achieving over 89.83% accuracy, while accuracy declined for finer divisions (6-8 classes) due to subtle visual differences and limited data per class. These findings demonstrate the feasibility of integrating image-based models into automated sorting and inventory systems, enabling early identification of sprouted potatoes and dynamic categorization based on storage stage. Practical implications include improved inventory management, differential pricing strategies, and reduced food waste across supply chains. While predicting exact shelf-life intervals remains challenging, focusing on broader class divisions ensures robust performance. Future research should aim to develop generalized models trained on diverse potato varieties and storage conditions to enhance adaptability and scalability. Overall, this approach offers a cost-effective, non-destructive method for quality assessment, supporting efficiency and sustainability in potato storage and distribution.</li>
<li><strong>摘要：</strong>基于图像的深度学习提供了一种非侵入性、可扩展的解决方案，用于监测储存期间的马铃薯质量，解决发芽检测、重量损失估计和保质期预测等关键挑战。在这项研究中，在受控温度和湿度条件下收集了 200 天的图像和相应的重量数据。利用 ResNet、VGG、DenseNet 和 Vision Transformer (ViT) 等强大的预训练架构，我们设计了两个专用模型：(1) 用于芽检测的高精度二元分类器，以及 (2) 先进的多类预测器，用于估计重量损失并以极高的准确度预测剩余保质期。 DenseNet 取得了卓越的性能，芽苗检测准确率高达 98.03%。保质期预测模型在粗分类（2-5 类）时表现最佳，准确率超过 89.83%，而由于细微的视觉差异和每类数据有限，更细分类（6-8 类）的准确度有所下降。这些发现证明了将基于图像的模型集成到自动分类和库存系统中的可行性，从而能够早期识别发芽马铃薯并根据存储阶段进行动态分类。实际影响包括改善库存管理、差别定价策略以及减少整个供应链的食物浪费。虽然预测准确的保质期间隔仍然具有挑战性，但关注更广泛的类别划分可确保稳健的性能。未来的研究应致力于开发针对不同马铃薯品种和储存条件进行训练的通用模型，以增强适应性和可扩展性。总体而言，这种方法提供了一种经济高效、无损的质量评估方法，支持马铃薯储存和分销的效率和可持续性。</li>
</ul>

<h3>Title: CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Neeraj Anand, Samyak Jha, Udbhav Bamba, Rahul Rahaman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00659">https://arxiv.org/abs/2601.00659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00659">https://arxiv.org/pdf/2601.00659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00659]] CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models(https://arxiv.org/abs/2601.00659)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite the rapid success of Large Vision-Language Models (LVLMs), a persistent challenge is their tendency to generate hallucinated content, undermining reliability in real-world use. Existing training-free methods address hallucinations but face two limitations: (i) they rely on narrow assumptions about hallucination sources, and (ii) their effectiveness declines toward the end of generation, where hallucinations are most likely to occur. A common strategy is to build hallucinated models by completely or partially removing visual tokens and contrasting them with the original model. Yet, this alone proves insufficient, since visual information still propagates into generated text. Building on this insight, we propose a novel hallucinated model that captures hallucination effects by selectively removing key text tokens. We further introduce Generalized Contrastive Decoding, which integrates multiple hallucinated models to represent diverse hallucination sources. Together, these ideas form CRoPS, a training-free hallucination mitigation framework that improves CHAIR scores by 20% and achieves consistent gains across six benchmarks and three LVLM families, outperforming state-of-the-art training-free methods.</li>
<li><strong>摘要：</strong>尽管大型视觉语言模型 (LVLM) 取得了迅速成功，但一个持续存在的挑战是它们倾向于生成幻觉内容，从而破坏了现实世界使用的可靠性。现有的免训练方法可以解决幻觉问题，但面临两个局限性：（i）它们依赖于对幻觉来源的狭隘假设，（ii）它们的有效性在一代人末期下降，而此时幻觉最有可能发生。一种常见的策略是通过完全或部分删除视觉标记并将其与原始模型进行对比来构建幻觉模型。然而，仅此一点是不够的，因为视觉信息仍然传播到生成的文本中。基于这一见解，我们提出了一种新颖的幻觉模型，通过有选择地删除关键文本标记来捕获幻觉效果。我们进一步介绍了广义对比解码，它集成了多个幻觉模型来表示不同的幻觉源。这些想法共同形成了 CRoPS，这是一种免训练的幻觉缓解框架，可将 CHAIR 分数提高 20%，并在六个基准和三个 LVLM 系列中取得一致的收益，优于最先进的免训练方法。</li>
</ul>

<h3>Title: Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation</h3>
<ul>
<li><strong>Authors: </strong>Taekyung Ki, Sangwon Jang, Jaehyeong Jo, Jaehong Yoon, Sung Ju Hwang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.HC, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00664">https://arxiv.org/abs/2601.00664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00664">https://arxiv.org/pdf/2601.00664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00664]] Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation(https://arxiv.org/abs/2601.00664)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.</li>
<li><strong>摘要：</strong>会说话的头像生成从静态肖像中创建栩栩如生的头像，用于虚拟通信和内容创建。然而，当前的模型尚未传达真正互动交流的感觉，常常产生缺乏情感参与的单向反应。我们确定了真正交互式化身的两个关键挑战：在因果约束下实时生成运动，以及在没有额外标记数据的情况下学习富有表现力的、充满活力的反应。为了应对这些挑战，我们提出了 Avatar Forcing，这是一种用于交互式头部头像生成的新框架，可通过扩散强制来模拟实时用户与头像交互。这种设计允许化身处理实时多模式输入，包括用户的音频和动作，并且具有低延迟，可以对言语和非言语线索（例如言语、点头和笑声）做出即时反应。此外，我们引入了一种直接偏好优化方法，该方法利用通过删除用户条件构建的合成丢失样本，从而实现表达交互的无标签学习。实验结果表明，我们的框架能够实现低延迟（约 500 毫秒）的实时交互，与基线相比实现了 6.8 倍的加速，并产生反应性和表现力的化身运动，与基线相比，首选率超过 80%。</li>
</ul>

<h3>Title: IRPO: Scaling the Bradley-Terry Model via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Haonan Song, Qingchen Xie, Huan Zhu, Feng Xiao, Luxi Xing, Fuzhen Li, Liu Kang, Feng Jiang, Zhiyong Zheng, Fan Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00677">https://arxiv.org/abs/2601.00677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00677">https://arxiv.org/pdf/2601.00677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00677]] IRPO: Scaling the Bradley-Terry Model via Reinforcement Learning(https://arxiv.org/abs/2601.00677)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative Reward Models (GRMs) have attracted considerable research interest in reward modeling due to their interpretability, inference-time scalability, and potential for refinement through reinforcement learning (RL). However, widely used pairwise GRMs create a computational bottleneck when integrated with RL algorithms such as Group Relative Policy Optimization (GRPO). This bottleneck arises from two factors: (i) the O(n^2) time complexity of pairwise comparisons required to obtain relative scores, and (ii) the computational overhead of repeated sampling or additional chain-of-thought (CoT) reasoning to improve performance. To address the first factor, we propose Intergroup Relative Preference Optimization (IRPO), a novel RL framework that incorporates the well-established Bradley-Terry model into GRPO. By generating a pointwise score for each response, IRPO enables efficient evaluation of arbitrarily many candidates during RL training while preserving interpretability and fine-grained reward signals. Experimental results demonstrate that IRPO achieves state-of-the-art (SOTA) performance among pointwise GRMs across multiple benchmarks, with performance comparable to that of current leading pairwise GRMs. Furthermore, we show that IRPO significantly outperforms pairwise GRMs in post-training evaluations.</li>
<li><strong>摘要：</strong>生成奖励模型 (GRM) 由于其可解释性、推理时间可扩展性以及通过强化学习 (RL) 进行细化的潜力，吸引了人们对奖励模型的广泛研究兴趣。然而，广泛使用的成对 GRM 在与组相对策略优化 (GRPO) 等 RL 算法集成时会产生计算瓶颈。该瓶颈源自两个因素：(i) 获得相对分数所需的成对比较的时间复杂度为 O(n^2)，以及 (ii) 重复采样或额外的思想链 (CoT) 推理以提高性能的计算开销。为了解决第一个因素，我们提出了群间相对偏好优化（IRPO），这是一种新颖的强化学习框架，它将完善的 Bradley-Terry 模型融入到 GRPO 中。通过为每个响应生成逐点分数，IRPO 可以在 RL 训练期间有效评估任意多个候选者，同时保留可解释性和细粒度的奖励信号。实验结果表明，IRPO 在多个基准的点式 GRM 中实现了最先进的 (SOTA) 性能，其性能与当前领先的成对 GRM 相当。此外，我们表明 IRPO 在训练后评估中显着优于成对 GRM。</li>
</ul>

<h3>Title: Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians</h3>
<ul>
<li><strong>Authors: </strong>Melonie de Almeida, Daniela Ivanova, Tong Shi, John H. Williamson, Paul Henderson</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00678">https://arxiv.org/abs/2601.00678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00678">https://arxiv.org/pdf/2601.00678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00678]] Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians(https://arxiv.org/abs/2601.00678)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Humans excel at forecasting the future dynamics of a scene given just a single image. Video generation models that can mimic this ability are an essential component for intelligent systems. Recent approaches have improved temporal coherence and 3D consistency in single-image-conditioned video generation. However, these methods often lack robust user controllability, such as modifying the camera path, limiting their applicability in real-world applications. Most existing camera-controlled image-to-video models struggle with accurately modeling camera motion, maintaining temporal consistency, and preserving geometric integrity. Leveraging explicit intermediate 3D representations offers a promising solution by enabling coherent video generation aligned with a given camera trajectory. Although these methods often use 3D point clouds to render scenes and introduce object motion in a later stage, this two-step process still falls short in achieving full temporal consistency, despite allowing precise control over camera movement. We propose a novel framework that constructs a 3D Gaussian scene representation and samples plausible object motion, given a single image in a single forward pass. This enables fast, camera-guided video generation without the need for iterative denoising to inject object motion into render frames. Extensive experiments on the KITTI, Waymo, RealEstate10K and DL3DV-10K datasets demonstrate that our method achieves state-of-the-art video quality and inference efficiency. The project page is available at this https URL.</li>
<li><strong>摘要：</strong>人类擅长仅凭一张图像来预测场景的未来动态。能够模仿这种能力的视频生成模型是智能系统的重要组成部分。最近的方法提高了单图像条件视频生成中的时间相干性和 3D 一致性。然而，这些方法通常缺乏强大的用户可控性，例如修改相机路径，限制了它们在实际应用中的适用性。大多数现有的相机控制的图像到视频模型都难以准确建模相机运动、保持时间一致性和保持几何完整性。利用显式中间 3D 表示可实现与给定摄像机轨迹对齐的连贯视频生成，从而提供了一种有前景的解决方案。尽管这些方法通常使用 3D 点云来渲染场景并在后期引入对象运动，但尽管允许精确控制相机运动，但这种两步过程仍然无法实现完全的时间一致性。我们提出了一种新颖的框架，在单次前向传递中给定单个图像的情况下，构建 3D 高斯场景表示并对合理的对象运动进行采样。这使得能够快速生成相机引导的视频，而无需迭代去噪将对象运动注入渲染帧。在 KITTI、Waymo、RealEstate10K 和 DL3DV-10K 数据集上进行的大量实验表明，我们的方法实现了最先进的视频质量和推理效率。项目页面可通过此 https URL 获取。</li>
</ul>

<h3>Title: TeleDoCTR: Domain-Specific and Contextual Troubleshooting for Telecommunications</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Trabelsi, Huseyin Uzunalioglu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00691">https://arxiv.org/abs/2601.00691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00691">https://arxiv.org/pdf/2601.00691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00691]] TeleDoCTR: Domain-Specific and Contextual Troubleshooting for Telecommunications(https://arxiv.org/abs/2601.00691)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Ticket troubleshooting refers to the process of analyzing and resolving problems that are reported through a ticketing system. In large organizations offering a wide range of services, this task is highly complex due to the diversity of submitted tickets and the need for specialized domain knowledge. In particular, troubleshooting in telecommunications (telecom) is a very time-consuming task as it requires experts to interpret ticket content, consult documentation, and search historical records to identify appropriate resolutions. This human-intensive approach not only delays issue resolution but also hinders overall operational efficiency. To enhance the effectiveness and efficiency of ticket troubleshooting in telecom, we propose TeleDoCTR, a novel telecom-related, domain-specific, and contextual troubleshooting system tailored for end-to-end ticket resolution in telecom. TeleDoCTR integrates both domain-specific ranking and generative models to automate key steps of the troubleshooting workflow which are: routing tickets to the appropriate expert team responsible for resolving the ticket (classification task), retrieving contextually and semantically similar historical tickets (retrieval task), and generating a detailed fault analysis report outlining the issue, root cause, and potential solutions (generation task). We evaluate TeleDoCTR on a real-world dataset from a telecom infrastructure and demonstrate that it achieves superior performance over existing state-of-the-art methods, significantly enhancing the accuracy and efficiency of the troubleshooting process.</li>
<li><strong>摘要：</strong>工单故障排除是指分析和解决通过工单系统报告的问题的过程。在提供广泛服务的大型组织中，由于提交的票证的多样性以及对专业领域知识的需求，这项任务非常复杂。特别是，电信领域的故障排除是一项非常耗时的任务，因为它需要专家解释故障单内容、查阅文档并搜索历史记录来确定适当的解决方案。这种人力密集型方法不仅会延迟问题的解决，还会影响整体运营效率。为了提高电信中工单故障排除的有效性和效率，我们提出了 TeleDoCTR，这是一种新颖的电信相关、特定领域和上下文故障排除系统，专为电信中的端到端工单解决而定制。 TeleDoCTR 集成了特定领域的排名和生成模型，以自动执行故障排除工作流程的关键步骤，这些步骤是：将故障单路由到负责解决故障单的相应专家团队（分类任务），检索上下文和语义上相似的历史故障单（检索任务），以及生成概述问题、根本原因和潜在解决方案的详细故障分析报告（生成任务）。我们在电信基础设施的真实数据集上评估 TeleDoCTR，并证明它比现有最先进的方法具有卓越的性能，从而显着提高了故障排除过程的准确性和效率。</li>
</ul>

<h3>Title: FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing</h3>
<ul>
<li><strong>Authors: </strong>Sunny Gupta, Amit Sethi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00785">https://arxiv.org/abs/2601.00785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00785">https://arxiv.org/pdf/2601.00785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00785]] FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing(https://arxiv.org/abs/2601.00785)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Federated data sharing promises utility without centralizing raw data, yet existing embedding-level generators struggle under non-IID client heterogeneity and provide limited formal protection against gradient leakage. We propose FedHypeVAE, a differentially private, hypernetwork-driven framework for synthesizing embedding-level data across decentralized clients. Building on a conditional VAE backbone, we replace the single global decoder and fixed latent prior with client-aware decoders and class-conditional priors generated by a shared hypernetwork from private, trainable client codes. This bi-level design personalizes the generative layerrather than the downstream modelwhile decoupling local data from communicated parameters. The shared hypernetwork is optimized under differential privacy, ensuring that only noise-perturbed, clipped gradients are aggregated across clients. A local MMD alignment between real and synthetic embeddings and a Lipschitz regularizer on hypernetwork outputs further enhance stability and distributional coherence under non-IID conditions. After training, a neutral meta-code enables domain agnostic synthesis, while mixtures of meta-codes provide controllable multi-domain coverage. FedHypeVAE unifies personalization, privacy, and distribution alignment at the generator level, establishing a principled foundation for privacy-preserving data synthesis in federated settings. Code: this http URL</li>
<li><strong>摘要：</strong>联合数据共享承诺在不集中原始数据的情况下提供实用性，但现有的嵌入级生成器在非 IID 客户端异构性下挣扎，并且提供有限的针对梯度泄漏的正式保护。我们提出了 FedHypeVAE，这是一种差分隐私、超网络驱动的框架，用于跨分散的客户端合成嵌入级数据。基于条件 VAE 主干，我们用客户端感知解码器和由共享超网络从私有、可训练客户端代码生成的类条件先验替换了单个全局解码器和固定潜在先验。这种双层设计个性化生成层而不是下游模型，同时将本地数据与通信参数解耦。共享超网络在差分隐私下进行了优化，确保只有受噪声干扰、剪切的梯度才会在客户端之间聚合。真实嵌入和合成嵌入之间的局部 MMD 对齐以及超网络输出上的 Lipschitz 正则化器进一步增强了非独立同分布条件下的稳定性和分布一致性。训练后，中性元代码可以实现与领域无关的合成，而元代码的混合则提供可控的多域覆盖。 FedHypeVAE 在生成器级别统一了个性化、隐私和分布对齐，为联邦设置中保护隐私的数据合成奠定了原则基础。代码：这个http URL</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
