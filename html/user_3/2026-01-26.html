<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-01-26</h1>
<h3>Title: GR3EN: Generative Relighting for 3D Environments</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyan Xing, Philipp Henzler, Junhwa Hur, Runze Li, Jonathan T. Barron, Pratul P. Srinivasan, Dor Verbin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16272">https://arxiv.org/abs/2601.16272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16272">https://arxiv.org/pdf/2601.16272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16272]] GR3EN: Generative Relighting for 3D Environments(https://arxiv.org/abs/2601.16272)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present a method for relighting 3D reconstructions of large room-scale environments. Existing solutions for 3D scene relighting often require solving under-determined or ill-conditioned inverse rendering problems, and are as such unable to produce high-quality results on complex real-world scenes. Though recent progress in using generative image and video diffusion models for relighting has been promising, these techniques are either limited to 2D image and video relighting or 3D relighting of individual objects. Our approach enables controllable 3D relighting of room-scale scenes by distilling the outputs of a video-to-video relighting diffusion model into a 3D reconstruction. This side-steps the need to solve a difficult inverse rendering problem, and results in a flexible system that can relight 3D reconstructions of complex real-world scenes. We validate our approach on both synthetic and real-world datasets to show that it can faithfully render novel views of scenes under new lighting conditions.</li>
<li><strong>摘要：</strong>我们提出了一种重新照亮大型房间规模环境 3D 重建的方法。现有的 3D 场景重新照明解决方案通常需要解决不确定或病态的逆渲染问题，因此无法在复杂的现实场景上产生高质量的结果。尽管最近在使用生成图像和视频扩散模型进行重新照明方面取得了很大的进展，但这些技术要么仅限于 2D 图像和视频重新照明，要么仅限于单个对象的 3D 重新照明。我们的方法通过将视频到视频重新照明扩散模型的输出提炼为 3D 重建，实现房间规模场景的可控 3D 重新照明。这回避了解决困难的逆渲染问题的需要，并产生了一个灵活的系统，可以重新照亮复杂现实世界场景的 3D 重建。我们在合成数据集和真实数据集上验证了我们的方法，以表明它可以在新的照明条件下忠实地渲染场景的新颖视图。</li>
</ul>

<h3>Title: Where is the multimodal goal post? On the Ability of Foundation Models to Recognize Contextually Important Moments</h3>
<ul>
<li><strong>Authors: </strong>Aditya K Surikuchi, Raquel Fernández, Sandro Pezzelle</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16333">https://arxiv.org/abs/2601.16333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16333">https://arxiv.org/pdf/2601.16333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16333]] Where is the multimodal goal post? On the Ability of Foundation Models to Recognize Contextually Important Moments(https://arxiv.org/abs/2601.16333)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Foundation models are used for many real-world applications involving language generation from temporally-ordered multimodal events. In this work, we study the ability of models to identify the most important sub-events in a video, which is a fundamental prerequisite for narrating or summarizing multimodal events. Specifically, we focus on football games and evaluate models on their ability to distinguish between important and non-important sub-events in a game. To this end, we construct a new dataset by leveraging human preferences for importance implicit in football game highlight reels, without any additional annotation costs. Using our dataset, which we will publicly release to the community, we compare several state-of-the-art multimodal models and show that they are not far from chance level performance. Analyses of models beyond standard evaluation metrics reveal their tendency to rely on a single dominant modality and their ineffectiveness in synthesizing necessary information from multiple sources. Our findings underline the importance of modular architectures that can handle sample-level heterogeneity in multimodal data and the need for complementary training procedures that can maximize cross-modal synergy.</li>
<li><strong>摘要：</strong>基础模型用于许多现实世界的应用程序，涉及从时间顺序的多模态事件生成语言。在这项工作中，我们研究模型识别视频中最重要的子事件的能力，这是叙述或总结多模态事件的基本先决条件。具体来说，我们专注于足球比赛，并评估模型区分比赛中重要和非重要子事件的能力。为此，我们利用人类对足球比赛精彩片段中隐含的重要性的偏好来构建一个新的数据集，而无需任何额外的注释成本。使用我们将向社区公开发布的数据集，我们比较了几种最先进的多模式模型，并表明它们距离机会水平性能不远。对标准评估指标之外的模型的分析表明，它们倾向于依赖单一的主导模式，并且在综合来自多个来源的必要信息时效率低下。我们的研究结果强调了可以处理多模态数据中样本级异质性的模块化架构的重要性，以及可以最大化跨模态协同作用的补充训练程序的需要。</li>
</ul>

<h3>Title: A Regularized Actor-Critic Algorithm for Bi-Level Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Sihan Zeng, Sujay Bhatt, Sumitra Ganesh, Alec Koppel</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16399">https://arxiv.org/abs/2601.16399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16399">https://arxiv.org/pdf/2601.16399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16399]] A Regularized Actor-Critic Algorithm for Bi-Level Reinforcement Learning(https://arxiv.org/abs/2601.16399)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We study a structured bi-level optimization problem where the upper-level objective is a smooth function and the lower-level problem is policy optimization in a Markov decision process (MDP). The upper-level decision variable parameterizes the reward of the lower-level MDP, and the upper-level objective depends on the optimal induced policy. Existing methods for bi-level optimization and RL often require second-order information, impose strong regularization at the lower level, or inefficiently use samples through nested-loop procedures. In this work, we propose a single-loop, first-order actor-critic algorithm that optimizes the bi-level objective via a penalty-based reformulation. We introduce into the lower-level RL objective an attenuating entropy regularization, which enables asymptotically unbiased upper-level hyper-gradient estimation without solving the unregularized RL problem exactly. We establish the finite-time and finite-sample convergence of the proposed algorithm to a stationary point of the original, unregularized bi-level optimization problem through a novel lower-level residual analysis under a special type of Polyak-Lojasiewicz condition. We validate the performance of our method through experiments on a GridWorld goal position problem and on happy tweet generation through reinforcement learning from human feedback (RLHF).</li>
<li><strong>摘要：</strong>我们研究结构化双层优化问题，其中上层目标是平滑函数，下层问题是马尔可夫决策过程（MDP）中的策略优化。上层决策变量参数化下层MDP的奖励，上层目标取决于最优诱导策略。现有的双层优化和强化学习方法通​​常需要二阶信息，在较低层施加强正则化，或者通过嵌套循环过程低效地使用样本。在这项工作中，我们提出了一种单循环、一阶 actor-critic 算法，该算法通过基于惩罚的重构来优化双层目标。我们在低层 RL 目标中引入了衰减熵正则化，它可以实现渐近无偏的上层超梯度估计，而无需精确解决非正则化 RL 问题。我们通过在特殊类型的 Polyak-Lojasiewicz 条件下的新颖的低级残差分析，建立了所提出的算法到原始非正则化双层优化问题的驻点的有限时间和有限样本收敛。我们通过 GridWorld 目标位置问题和通过人类反馈强化学习 (RLHF) 生成快乐推文的实验来验证我们方法的性能。</li>
</ul>

<h3>Title: A Cosine Network for Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Chunwei Tian, Chengyuan Zhang, Bob Zhang, Zhiwu Li, C. L. Philip Chen, David Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16413">https://arxiv.org/abs/2601.16413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16413">https://arxiv.org/pdf/2601.16413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16413]] A Cosine Network for Image Super-Resolution(https://arxiv.org/abs/2601.16413)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Deep convolutional neural networks can use hierarchical information to progressively extract structural information to recover high-quality images. However, preserving the effectiveness of the obtained structural information is important in image super-resolution. In this paper, we propose a cosine network for image super-resolution (CSRNet) by improving a network architecture and optimizing the training strategy. To extract complementary homologous structural information, odd and even heterogeneous blocks are designed to enlarge the architectural differences and improve the performance of image super-resolution. Combining linear and non-linear structural information can overcome the drawback of homologous information and enhance the robustness of the obtained structural information in image super-resolution. Taking into account the local minimum of gradient descent, a cosine annealing mechanism is used to optimize the training procedure by performing warm restarts and adjusting the learning rate. Experimental results illustrate that the proposed CSRNet is competitive with state-of-the-art methods in image super-resolution.</li>
<li><strong>摘要：</strong>深度卷积神经网络可以利用层次信息逐步提取结构信息来恢复高质量图像。然而，保持所获得的结构信息的有效性在图像超分辨率中非常重要。在本文中，我们通过改进网络架构和优化训练策略，提出了一种用于图像超分辨率的余弦网络（CSRNet）。为了提取互补的同源结构信息，设计了奇数和偶数异质块以放大结构差异并提高图像超分辨率的性能。将线性和非线性结构信息结合起来可以克服同源信息的缺点，增强所获得的结构信息在图像超分辨率中的鲁棒性。考虑到梯度下降的局部最小值，余弦退火机制用于通过执行热重启和调整学习率来优化训练过程。实验结果表明，所提出的 CSRNet 在图像超分辨率方面与最先进的方法具有竞争力。</li>
</ul>

<h3>Title: A Cautionary Tale of Self-Supervised Learning for Imaging Biomarkers: Alzheimer's Disease Case Study</h3>
<ul>
<li><strong>Authors: </strong>Maxwell Reynolds, Chaitanya Srinivasan, Vijay Cherupally, Michael Leone, Ke Yu, Li Sun, Tigmanshu Chaudhary, Andreas Pfenning, Kayhan Batmanghelich</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16467">https://arxiv.org/abs/2601.16467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16467">https://arxiv.org/pdf/2601.16467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16467]] A Cautionary Tale of Self-Supervised Learning for Imaging Biomarkers: Alzheimer's Disease Case Study(https://arxiv.org/abs/2601.16467)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Discovery of sensitive and biologically grounded biomarkers is essential for early detection and monitoring of Alzheimer's disease (AD). Structural MRI is widely available but typically relies on hand-crafted features such as cortical thickness or volume. We ask whether self-supervised learning (SSL) can uncover more powerful biomarkers from the same data. Existing SSL methods underperform FreeSurfer-derived features in disease classification, conversion prediction, and amyloid status prediction. We introduce Residual Noise Contrastive Estimation (R-NCE), a new SSL framework that integrates auxiliary FreeSurfer features while maximizing additional augmentation-invariant information. R-NCE outperforms traditional features and existing SSL methods across multiple benchmarks, including AD conversion prediction. To assess biological relevance, we derive Brain Age Gap (BAG) measures and perform genome-wide association studies. R-NCE-BAG shows high heritability and associations with MAPT and IRAG1, with enrichment in astrocytes and oligodendrocytes, indicating sensitivity to neurodegenerative and cerebrovascular processes.</li>
<li><strong>摘要：</strong>发现敏感且具有生物学基础的生物标志物对于阿尔茨海默病 (AD) 的早期检测和监测至关重要。结构 MRI 广泛可用，但通常依赖于手工制作的特征，例如皮质厚度或体积。我们询问自我监督学习（SSL）是否可以从相同的数据中发现更强大的生物标志物。现有的 SSL 方法在疾病分类、转化预测和淀粉样蛋白状态预测方面的性能不如 FreeSurfer 衍生的功能。我们引入了残余噪声对比估计 (R-NCE)，这是一种新的 SSL 框架，它集成了辅助 FreeSurfer 功能，同时最大化了额外的增强不变信息。 R-NCE 在多个基准测试中均优于传统功能和现有 SSL 方法，包括 AD 转换预测。为了评估生物学相关性，我们得出脑年龄差距（BAG）测量值并进行全基因组关联研究。 R-NCE-BAG 显示出高遗传力，并与 MAPT 和 IRAG1 相关，星形胶质细胞和少突胶质细胞富集，表明对神经退行性和脑血管过程敏感。</li>
</ul>

<h3>Title: Order from Chaos: Physical World Understanding from Glitchy Gameplay Videos</h3>
<ul>
<li><strong>Authors: </strong>Meng Cao, Haoran Tang, Haoze Zhao, Mingfei Han, Ruyang Liu, Qiang Sun, Xiaojun Chang, Ian Reid, Xiaodan Liang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16471">https://arxiv.org/abs/2601.16471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16471">https://arxiv.org/pdf/2601.16471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16471]] Order from Chaos: Physical World Understanding from Glitchy Gameplay Videos(https://arxiv.org/abs/2601.16471)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Understanding the physical world, including object dynamics, material properties, and causal interactions, remains a core challenge in artificial intelligence. Although recent multi-modal large language models (MLLMs) have demonstrated impressive general reasoning capabilities, they still fall short of achieving human-level understanding of physical principles. Existing datasets for physical reasoning either rely on real-world videos, which incur high annotation costs, or on synthetic simulations, which suffer from limited realism and diversity. In this paper, we propose a novel paradigm that leverages glitches in gameplay videos, referring to visual anomalies that violate predefined physical laws, as a rich and scalable supervision source for physical world understanding. We introduce PhysGame, an meta information guided instruction-tuning dataset containing 140,057 glitch-centric question-answer pairs across five physical domains and sixteen fine-grained categories. To ensure data accuracy, we design a prompting strategy that utilizes gameplay metadata such as titles and descriptions to guide high-quality QA generation. Complementing PhysGame, we construct GameBench, an expert-annotated benchmark with 880 glitch-identified gameplay videos designed to evaluate physical reasoning capabilities. Extensive experiments show that PhysGame significantly enhances both Game2Real transferability, improving the real world physical reasoning performance of Qwen2.5VL by 2.5% on PhysBench, and Game2General transferability, yielding a 1.9% gain on the MVBench benchmark. Moreover, PhysGame-tuned models achieve a 3.7% absolute improvement on GameBench, demonstrating enhanced robustness in detecting physical implausibilities. These results indicate that learning from gameplay anomalies offers a scalable and effective pathway toward advancing physical world understanding in multimodal intelligence.</li>
<li><strong>摘要：</strong>理解物理世界，包括物体动力学、材料属性和因果相互作用，仍然是人工智能的核心挑战。尽管最近的多模态大语言模型（MLLM）已经表现出令人印象深刻的一般推理能力，但它们仍然未能实现人类对物理原理的理解。现有的物理推理数据集要么依赖于现实世界的视频，这会产生高昂的注释成本，要么依赖于合成模拟，这会导致真实性和多样性有限。在本文中，我们提出了一种新颖的范例，该范例利用游戏视频中的故障，即违反预定义物理定律的视觉异常，作为物理世界理解的丰富且可扩展的监督源。我们引入了 PhysGame，这是一个元信息引导的指令调整数据集，包含 140,057 个以故障为中心的问答对，跨越 5 个物理领域和 16 个细粒度类别。为了确保数据准确性，我们设计了一种提示策略，利用标题和描述等游戏元数据来指导高质量的 QA 生成。为了补充 PhysGame，我们构建了 GameBench，这是一个专家注释的基准测试，包含 880 个故障识别的游戏视频，旨在评估物理推理能力。大量实验表明，PhysGame 显着增强了 Game2Real 的可迁移性，将 Qwen2.5VL 的现实世界物理推理性能在 PhysBench 上提高了 2.5%，以及 Game2General 的可迁移性，在 MVBench 基准上提高了 1.9%。此外，经过 PhysGame 调整的模型在 GameBench 上实现了 3.7% 的绝对改进，表明在检测物理不可信性方面增强了鲁棒性。这些结果表明，从游戏玩法异常中学习为促进多模态智能中的物理世界理解提供了一条可扩展且有效的途径。</li>
</ul>

<h3>Title: SALAD: Achieve High-Sparsity Attention via Efficient Linear Attention Tuning for Video Diffusion Transformer</h3>
<ul>
<li><strong>Authors: </strong>Tongcheng Fang, Hanling Zhang, Ruiqi Xie, Zhuo Han, Xin Tao, Tianchen Zhao, Pengfei Wan, Wenbo Ding, Wanli Ouyang, Xuefei Ning, Yu Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16515">https://arxiv.org/abs/2601.16515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16515">https://arxiv.org/pdf/2601.16515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16515]] SALAD: Achieve High-Sparsity Attention via Efficient Linear Attention Tuning for Video Diffusion Transformer(https://arxiv.org/abs/2601.16515)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion Transformers have recently demonstrated remarkable performance in video generation. However, the long input sequences result in high computational latency due to the quadratic complexity of full attention. Various sparse attention mechanisms have been proposed. Training-free sparse attention is constrained by limited sparsity and thus offers modest acceleration, whereas training-based methods can reach much higher sparsity but demand substantial data and computation for training. In this work, we propose SALAD, introducing a lightweight linear attention branch in parallel with the sparse attention. By incorporating an input-dependent gating mechanism to finely balance the two branches, our method attains 90% sparsity and 1.72x inference speedup, while maintaining generation quality comparable to the full attention baseline. Moreover, our finetuning process is highly efficient, requiring only 2,000 video samples and 1,600 training steps with a batch size of 8.</li>
<li><strong>摘要：</strong>Diffusion Transformers 最近在视频生成方面表现出了卓越的性能。然而，由于完全注意力的二次复杂度，长输入序列会导致高计算延迟。人们提出了各种稀疏注意力机制。免训练稀疏注意力受到有限稀疏性的限制，因此提供了适度的加速，而基于训练的方法可以达到更高的稀疏性，但需要大量数据和计算来进行训练。在这项工作中，我们提出了 SALAD，引入了与稀疏注意力并行的轻量级线性注意力分支。通过结合输入相关的门控机制来精细平衡两个分支，我们的方法实现了 90% 的稀疏性和 1.72 倍的推理加速，同时保持与完全注意力基线相当的生成质量。此外，我们的微调过程非常高效，仅需要 2,000 个视频样本和 1,600 个训练步骤，批量大小为 8。</li>
</ul>

<h3>Title: TangramPuzzle: Evaluating Multimodal Large Language Models with Compositional Spatial Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Daixian Liu, Jiayi Kuang, Yinghui Li, Yangning Li, Di Yin, Haoyu Cao, Xing Sun, Ying Shen, Hai-Tao Zheng, Liang Lin, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16520">https://arxiv.org/abs/2601.16520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16520">https://arxiv.org/pdf/2601.16520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16520]] TangramPuzzle: Evaluating Multimodal Large Language Models with Compositional Spatial Reasoning(https://arxiv.org/abs/2601.16520)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual recognition and semantic understanding. Nevertheless, their ability to perform precise compositional spatial reasoning remains largely unexplored. Existing benchmarks often involve relatively simple tasks and rely on semantic approximations or coarse relative positioning, while their evaluation metrics are typically limited and lack rigorous mathematical formulations. To bridge this gap, we introduce TangramPuzzle, a geometry-grounded benchmark designed to evaluate compositional spatial reasoning through the lens of the classic Tangram game. We propose the Tangram Construction Expression (TCE), a symbolic geometric framework that grounds tangram assemblies in exact, machine-verifiable coordinate specifications, to mitigate the ambiguity of visual approximation. We design two complementary tasks: Outline Prediction, which demands inferring global shapes from local components, and End-to-End Code Generation, which requires solving inverse geometric assembly problems. We conduct extensive evaluation experiments on advanced open-source and proprietary models, revealing an interesting insight: MLLMs tend to prioritize matching the target silhouette while neglecting geometric constraints, leading to distortions or deformations of the pieces.</li>
<li><strong>摘要：</strong>多模态大语言模型（MLLM）在视觉识别和语义理解方面取得了显着进展。然而，它们执行精确组合空间推理的能力在很大程度上仍未得到探索。现有的基准通常涉及相对简单的任务，并且依赖于语义近似或粗略的相对定位，而它们的评估指标通常是有限的并且缺乏严格的数学公式。为了弥补这一差距，我们引入了 TangramPuzzle，这是一个基于几何的基准测试，旨在通过经典七巧板游戏的镜头评估组合空间推理。我们提出了七巧板构造表达式（TCE），这是一种符号几何框架，它将七巧板组件建立在精确的、机器可验证的坐标规范中，以减轻视觉近似的模糊性。我们设计了两个互补的任务：轮廓预测（需要从局部组件推断全局形状）和端到端代码生成（需要解决逆几何装配问题）。我们对先进的开源和专有模型进行了广泛的评估实验，揭示了一个有趣的见解：MLLM 倾向于优先考虑匹配目标轮廓，而忽略几何约束，从而导致部件扭曲或变形。</li>
</ul>

<h3>Title: Beyond Superficial Unlearning: Sharpness-Aware Robust Erasure of Hallucinations in Multimodal LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xianya Fang, Feiyang Ren, Xiang Chen, Yu Tian, Zhen Bi, Haiyang Yu, Sheng-Jun Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16527">https://arxiv.org/abs/2601.16527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16527">https://arxiv.org/pdf/2601.16527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16527]] Beyond Superficial Unlearning: Sharpness-Aware Robust Erasure of Hallucinations in Multimodal LLMs(https://arxiv.org/abs/2601.16527)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal LLMs are powerful but prone to object hallucinations, which describe non-existent entities and harm reliability. While recent unlearning methods attempt to mitigate this, we identify a critical flaw: structural fragility. We empirically demonstrate that standard erasure achieves only superficial suppression, trapping the model in sharp minima where hallucinations catastrophically resurge after lightweight relearning. To ensure geometric stability, we propose SARE, which casts unlearning as a targeted min-max optimization problem and uses a Targeted-SAM mechanism to explicitly flatten the loss landscape around hallucinated concepts. By suppressing hallucinations under simulated worst-case parameter perturbations, our framework ensures robust removal stable against weight shifts. Extensive experiments demonstrate that SARE significantly outperforms baselines in erasure efficacy while preserving general generation quality. Crucially, it maintains persistent hallucination suppression against relearning and parameter updates, validating the effectiveness of geometric stabilization.</li>
<li><strong>摘要：</strong>多模式法学硕士很强大，但容易产生物体幻觉，描述不存在的实体并损害可靠性。虽然最近的遗忘方法试图缓解这一问题，但我们发现了一个关键缺陷：结构脆弱性。我们凭经验证明，标准擦除只能实现表面抑制，使模型陷入尖锐的极小值，在轻量级重新学习后，幻觉会灾难性地重新出现。为了确保几何稳定性，我们提出了 SARE，它将遗忘作为有针对性的最小-最大优化问题，并使用 Targeted-SAM 机制明确地平坦化幻觉概念周围的损失景观。通过抑制模拟最坏情况参数扰动下的幻觉，我们的框架确保了针对重量变化的鲁棒去除稳定。大量实验表明，SARE 在擦除效率方面显着优于基线，同时保持了总体生成质量。至关重要的是，它对重新学习和参数更新保持持续的幻觉抑制，验证几何稳定的有效性。</li>
</ul>

<h3>Title: AnchoredDream: Zero-Shot 360° Indoor Scene Generation from a Single View via Geometric Grounding</h3>
<ul>
<li><strong>Authors: </strong>Runmao Yao, Junsheng Zhou, Zhen Dong, Yu-Shen Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16532">https://arxiv.org/abs/2601.16532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16532">https://arxiv.org/pdf/2601.16532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16532]] AnchoredDream: Zero-Shot 360° Indoor Scene Generation from a Single View via Geometric Grounding(https://arxiv.org/abs/2601.16532)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Single-view indoor scene generation plays a crucial role in a range of real-world applications. However, generating a complete 360° scene from a single image remains a highly ill-posed and challenging problem. Recent approaches have made progress by leveraging diffusion models and depth estimation networks, yet they still struggle to maintain appearance consistency and geometric plausibility under large viewpoint changes, limiting their effectiveness in full-scene generation. To address this, we propose AnchoredDream, a novel zero-shot pipeline that anchors 360° scene generation on high-fidelity geometry via an appearance-geometry mutual boosting mechanism. Given a single-view image, our method first performs appearance-guided geometry generation to construct a reliable 3D scene layout. Then, we progressively generate the complete scene through a series of modules: warp-and-inpaint, warp-and-refine, post-optimization, and a novel Grouting Block, which ensures seamless transitions between the input view and generated regions. Extensive experiments demonstrate that AnchoredDream outperforms existing methods by a large margin in both appearance consistency and geometric plausibility--all in a zero-shot manner. Our results highlight the potential of geometric grounding for high-quality, zero-shot single-view scene generation.</li>
<li><strong>摘要：</strong>单视图室内场景生成在一系列现实应用中发挥着至关重要的作用。然而，从单个图像生成完整的 360° 场景仍然是一个非常不适定且具有挑战性的问题。最近的方法通过利用扩散模型和深度估计网络取得了进展，但它们仍然难以在大视点变化下保持外观一致性和几何合理性，限制了它们在全场景生成中的有效性。为了解决这个问题，我们提出了 AnchoredDream，这是一种新颖的零镜头管道，通过外观几何相互增强机制将 360° 场景生成锚定在高保真几何上。给定单视图图像，我们的方法首先执行外观引导的几何生成，以构建可靠的 3D 场景布局。然后，我们通过一系列模块逐步生成完整的场景：扭曲和修复、扭曲和细化、后优化和新颖的灌浆块，确保输入视图和生成区域之间的无缝过渡。大量实验表明，AnchoredDream 在外观一致性和几何合理性方面都大幅优于现有方法，而且全部都是零样本方式。我们的结果凸显了几何基础在高质量、零镜头单视图场景生成方面的潜力。</li>
</ul>

<h3>Title: SCHIGAND: A Synthetic Facial Generation Mode Pipeline</h3>
<ul>
<li><strong>Authors: </strong>Ananya Kadali, Sunnie Jehan-Morrison, Orasiki Wellington, Barney Evans, Precious Durojaiye, Richard Guest</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16627">https://arxiv.org/abs/2601.16627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16627">https://arxiv.org/pdf/2601.16627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16627]] SCHIGAND: A Synthetic Facial Generation Mode Pipeline(https://arxiv.org/abs/2601.16627)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The growing demand for diverse and high-quality facial datasets for training and testing biometric systems is challenged by privacy regulations, data scarcity, and ethical concerns. Synthetic facial images offer a potential solution, yet existing generative models often struggle to balance realism, diversity, and identity preservation. This paper presents SCHIGAND, a novel synthetic face generation pipeline integrating StyleCLIP, HyperStyle, InterfaceGAN, and Diffusion models to produce highly realistic and controllable facial datasets. SCHIGAND enhances identity preservation while generating realistic intra-class variations and maintaining inter-class distinctiveness, making it suitable for biometric testing. The generated datasets were evaluated using ArcFace, a leading facial verification model, to assess their effectiveness in comparison to real-world facial datasets. Experimental results demonstrate that SCHIGAND achieves a balance between image quality and diversity, addressing key limitations of prior generative models. This research highlights the potential of SCHIGAND to supplement and, in some cases, replace real data for facial biometric applications, paving the way for privacy-compliant and scalable solutions in synthetic dataset generation.</li>
<li><strong>摘要：</strong>对用于训练和测试生物识别系统的多样化和高质量面部数据集的需求不断增长，但受到隐私法规、数据稀缺和道德问题的挑战。合成面部图像提供了一种潜在的解决方案，但现有的生成模型往往难以平衡现实性、多样性和身份保存。本文提出了 SCHIGAND，一种新颖的合成面部生成管道，集成了 StyleCLIP、HyperStyle、InterfaceGAN 和 Diffusion 模型，可生成高度真实且可控的面部数据集。 SCHIGAND 增强了身份保存，同时生成真实的类内差异并保持类间独特性，使其适合生物识别测试。使用领先的面部验证模型 ArcFace 对生成的数据集进行评估，以评估其与现实世界面部数据集相比的有效性。实验结果表明，SCHIGAND 实现了图像质量和多样性之间的平衡，解决了先前生成模型的关键局限性。这项研究凸显了 SCHIGAND 补充并在某些情况下取代面部生物识别应用的真实数据的潜力，为合成数据集生成中符合隐私且可扩展的解决方案铺平了道路。</li>
</ul>

<h3>Title: Edge-Aware Image Manipulation via Diffusion Models with a Novel Structure-Preservation Loss</h3>
<ul>
<li><strong>Authors: </strong>Minsu Gong, Nuri Ryu, Jungseul Ok, Sunghyun Cho</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16645">https://arxiv.org/abs/2601.16645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16645">https://arxiv.org/pdf/2601.16645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16645]] Edge-Aware Image Manipulation via Diffusion Models with a Novel Structure-Preservation Loss(https://arxiv.org/abs/2601.16645)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in image editing leverage latent diffusion models (LDMs) for versatile, text-prompt-driven edits across diverse tasks. Yet, maintaining pixel-level edge structures-crucial for tasks such as photorealistic style transfer or image tone adjustment-remains as a challenge for latent-diffusion-based editing. To overcome this limitation, we propose a novel Structure Preservation Loss (SPL) that leverages local linear models to quantify structural differences between input and edited images. Our training-free approach integrates SPL directly into the diffusion model's generative process to ensure structural fidelity. This core mechanism is complemented by a post-processing step to mitigate LDM decoding distortions, a masking strategy for precise edit localization, and a color preservation loss to preserve hues in unedited areas. Experiments confirm SPL enhances structural fidelity, delivering state-of-the-art performance in latent-diffusion-based image editing. Our code will be publicly released at this https URL.</li>
<li><strong>摘要：</strong>图像编辑领域的最新进展利用潜在扩散模型 (LDM) 来实现跨不同任务的多功能、文本提示驱动的编辑。然而，维护像素级边缘结构（对于照片级真实风格转换或图像色调调整等任务至关重要）仍然是基于潜在扩散的编辑的挑战。为了克服这一限制，我们提出了一种新颖的结构保留损失（SPL），它利用局部线性模型来量化输入图像和编辑图像之间的结构差异。我们的免训练方法将 SPL 直接集成到扩散模型的生成过程中，以确保结构保真度。这一核心机制得到了后处理步骤的补充，以减轻 LDM 解码失真、用于精确编辑本地化的掩蔽策略，以及用于保留未编辑区域色调的颜色保留损失。实验证实 SPL 增强了结构保真度，在基于潜在扩散的图像编辑中提供了最先进的性能。我们的代码将在此 https URL 公开发布。</li>
</ul>

<h3>Title: Flow Matching for Probabilistic Monocular 3D Human Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Cuong Le, Pavló Melnyk, Bastian Wandt, Mårten Wadenbäck</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16763">https://arxiv.org/abs/2601.16763</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16763">https://arxiv.org/pdf/2601.16763</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16763]] Flow Matching for Probabilistic Monocular 3D Human Pose Estimation(https://arxiv.org/abs/2601.16763)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recovering 3D human poses from a monocular camera view is a highly ill-posed problem due to the depth ambiguity. Earlier studies on 3D human pose lifting from 2D often contain incorrect-yet-overconfident 3D estimations. To mitigate the problem, emerging probabilistic approaches treat the 3D estimations as a distribution, taking into account the uncertainty measurement of the poses. Falling in a similar category, we proposed FMPose, a probabilistic 3D human pose estimation method based on the flow matching generative approach. Conditioned on the 2D cues, the flow matching scheme learns the optimal transport from a simple source distribution to the plausible 3D human pose distribution via continuous normalizing flows. The 2D lifting condition is modeled via graph convolutional networks, leveraging the learnable connections between human body joints as the graph structure for feature aggregation. Compared to diffusion-based methods, the FMPose with optimal transport produces faster and more accurate 3D pose generations. Experimental results show major improvements of our FMPose over current state-of-the-art methods on three common benchmarks for 3D human pose estimation, namely Human3.6M, MPI-INF-3DHP and 3DPW.</li>
<li><strong>摘要：</strong>由于深度模糊，从单目相机视图恢复 3D 人体姿势是一个非常不适定的问题。早期关于从 2D 提升 3D 人体姿势的研究通常包含不正确但过于自信的 3D 估计。为了缓解这个问题，新兴的概率方法将 3D 估计视为分布，同时考虑姿势的不确定性测量。属于类似类别，我们提出了 FMPose，一种基于流匹配生成方法的概率 3D 人体姿势估计方法。以 2D 线索为条件，流匹配方案通过连续归一化流学习从简单源分布到合理的 3D 人体姿势分布的最佳传输。二维提升条件通过图卷积网络进行建模，利用人体关节之间的可学习连接作为特征聚合的图结构。与基于扩散的方法相比，具有最佳传输的 FMPose 可以生成更快、更准确的 3D 姿态生成。实验结果表明，在 3D 人体姿态估计的三个常见基准（即 Human3.6M、MPI-INF-3DHP 和 3DPW）上，我们的 FMPose 相对于当前最先进的方法有了重大改进。</li>
</ul>

<h3>Title: AutoRegressive Generation with B-rep Holistic Token Sequence Representation</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Li, Yunpeng Bai, Yongkang Dai, Hao Guo, Hongping Gan, Yilei Shi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16771">https://arxiv.org/abs/2601.16771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16771">https://arxiv.org/pdf/2601.16771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16771]] AutoRegressive Generation with B-rep Holistic Token Sequence Representation(https://arxiv.org/abs/2601.16771)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Previous representation and generation approaches for the B-rep relied on graph-based representations that disentangle geometric and topological features through decoupled computational pipelines, thereby precluding the application of sequence-based generative frameworks, such as transformer architectures that have demonstrated remarkable performance. In this paper, we propose BrepARG, the first attempt to encode B-rep's geometry and topology into a holistic token sequence representation, enabling sequence-based B-rep generation with an autoregressive architecture. Specifically, BrepARG encodes B-rep into 3 types of tokens: geometry and position tokens representing geometric features, and face index tokens representing topology. Then the holistic token sequence is constructed hierarchically, starting with constructing the geometry blocks (i.e., faces and edges) using the above tokens, followed by geometry block sequencing. Finally, we assemble the holistic sequence representation for the entire B-rep. We also construct a transformer-based autoregressive model that learns the distribution over holistic token sequences via next-token prediction, using a multi-layer decoder-only architecture with causal masking. Experiments demonstrate that BrepARG achieves state-of-the-art (SOTA) performance. BrepARG validates the feasibility of representing B-rep as holistic token sequences, opening new directions for B-rep generation.</li>
<li><strong>摘要：</strong>以前的 B-rep 表示和生成方法依赖于基于图的表示，通过解耦的计算管道来解开几何和拓扑特征，从而排除了基于序列的生成框架的应用，例如已经表现出卓越性能的 Transformer 架构。在本文中，我们提出了 BrepARG，这是将 B-rep 的几何和拓扑编码为整体令牌序列表示的首次尝试，从而实现了具有自回归架构的基于序列的 B-rep 生成。具体来说，BrepARG 将 B-rep 编码为 3 种类型的标记：表示几何特征的几何和位置标记，以及表示拓扑的面索引标记。然后，分层构建整体令牌序列，首先使用上述令牌构建几何块（即面和边），然后进行几何块排序。最后，我们组装整个 B-rep 的整体序列表示。我们还构建了一个基于变压器的自回归模型，该模型使用具有因果屏蔽的多层仅解码器架构，通过下一个令牌预测来学习整体令牌序列的分布。实验表明 BrepARG 实现了最先进的 (SOTA) 性能。 BrepARG 验证了将 B-rep 表示为整体令牌序列的可行性，为 B-rep 生成开辟了新方向。</li>
</ul>

<h3>Title: The Art of Being Difficult: Combining Human and AI Strengths to Find Adversarial Instances for Heuristics</h3>
<ul>
<li><strong>Authors: </strong>Henri Nikoleit, Ankit Anand, Anurag Murty Naredla, Heiko Röglin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16849">https://arxiv.org/abs/2601.16849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16849">https://arxiv.org/pdf/2601.16849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16849]] The Art of Being Difficult: Combining Human and AI Strengths to Find Adversarial Instances for Heuristics(https://arxiv.org/abs/2601.16849)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We demonstrate the power of human-LLM collaboration in tackling open problems in theoretical computer science. Focusing on combinatorial optimization, we refine outputs from the FunSearch algorithm [Romera-Paredes et al., Nature 2023] to derive state-of-the-art lower bounds for standard heuristics. Specifically, we target the generation of adversarial instances where these heuristics perform poorly. By iterating on FunSearch's outputs, we identify improved constructions for hierarchical $k$-median clustering, bin packing, the knapsack problem, and a generalization of Lovász's gasoline problem - some of these have not seen much improvement for over a decade, despite intermittent attention. These results illustrate how expert oversight can effectively extrapolate algorithmic insights from LLM-based evolutionary methods to break long-standing barriers. Our findings demonstrate that while LLMs provide critical initial patterns, human expertise is essential for transforming these patterns into mathematically rigorous and insightful constructions. This work highlights that LLMs are a strong collaborative tool in mathematics and computer science research.</li>
<li><strong>摘要：</strong>我们展示了人类与法学硕士合作在解决理论计算机科学中的开放问题方面的力量。专注于组合优化，我们改进了 FunSearch 算法的输出 [Romera-Paredes 等人，Nature 2023]，以导出标准启发式的最先进的下界。具体来说，我们的目标是生成这些启发式方法表现不佳的对抗性实例。通过迭代 FunSearch 的输出，我们确定了分层 $k$ 中值聚类、装箱、背包问题和 Lovász 汽油问题的推广的改进结构 - 尽管间歇性受到关注，但其中一些问题十多年来没有看到太大改进。这些结果说明了专家监督如何有效地从基于法学硕士的进化方法中推断出算法见解，以打破长期存在的障碍。我们的研究结果表明，虽然法学硕士提供了关键的初始模式，但人类的专业知识对于将这些模式转化为数学上严格且富有洞察力的结构至关重要。这项工作强调了法学硕士是数学和计算机科学研究中强大的协作工具。</li>
</ul>

<h3>Title: LoL: Longer than Longer, Scaling Video Generation to Hour</h3>
<ul>
<li><strong>Authors: </strong>Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li, Rui Wang, Andrew Bai, Yuanhao Ban, Cho-Jui Hsieh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16914">https://arxiv.org/abs/2601.16914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16914">https://arxiv.org/pdf/2601.16914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16914]] LoL: Longer than Longer, Scaling Video Generation to Hour(https://arxiv.org/abs/2601.16914)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent research in long-form video generation has shifted from bidirectional to autoregressive models, yet these methods commonly suffer from error accumulation and a loss of long-term coherence. While attention sink frames have been introduced to mitigate this performance decay, they often induce a critical failure mode we term sink-collapse: the generated content repeatedly reverts to the sink frame, resulting in abrupt scene resets and cyclic motion patterns. Our analysis reveals that sink-collapse originates from an inherent conflict between the periodic structure of Rotary Position Embedding (RoPE) and the multi-head attention mechanisms prevalent in current generative models. To address it, we propose a lightweight, training-free approach that effectively suppresses this behavior by introducing multi-head RoPE jitter that breaks inter-head attention homogenization and mitigates long-horizon collapse. Extensive experiments show that our method successfully alleviates sink-collapse while preserving generation quality. To the best of our knowledge, this work achieves the first demonstration of real-time, streaming, and infinite-length video generation with little quality decay. As an illustration of this robustness, we generate continuous videos up to 12 hours in length, which, to our knowledge, is among the longest publicly demonstrated results in streaming video generation.</li>
<li><strong>摘要：</strong>最近对长格式视频生成的研究已经从双向模型转向自回归模型，但这些方法通常会遭受错误积累和长期一致性丧失的困扰。虽然引入注意力接收器帧来减轻这种性能下降，但它们通常会引发一种严重的故障模式，我们称之为接收器崩溃：生成的内容反复恢复到接收器帧，导致突然的场景重置和循环运动模式。我们的分析表明，汇崩溃源于旋转位置嵌入（RoPE）的周期结构与当前生成模型中普遍存在的多头注意力机制之间的固有冲突。为了解决这个问题，我们提出了一种轻量级、免训练的方法，通过引入多头 RoPE 抖动来有效抑制这种行为，打破头间注意力均匀化并减轻长期崩溃。大量的实验表明，我们的方法成功地缓解了汇崩溃，同时保持了发电质量。据我们所知，这项工作首次实现了实时、流媒体和无限长度视频生成的演示，并且质量几乎没有下降。为了说明这种鲁棒性，我们生成了长达 12 小时的连续视频，据我们所知，这是流媒体视频生成中公开展示的最长的结果之一。</li>
</ul>

<h3>Title: Reward-Forcing: Autoregressive Video Generation with Reward Feedback</h3>
<ul>
<li><strong>Authors: </strong>Jingran Zhang, Ning Li, Yuanhao Ban, Andrew Bai, Justin Cui</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16933">https://arxiv.org/abs/2601.16933</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16933">https://arxiv.org/pdf/2601.16933</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16933]] Reward-Forcing: Autoregressive Video Generation with Reward Feedback(https://arxiv.org/abs/2601.16933)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While most prior work in video generation relies on bidirectional architectures, recent efforts have sought to adapt these models into autoregressive variants to support near real-time generation. However, such adaptations often depend heavily on teacher models, which can limit performance, particularly in the absence of a strong autoregressive teacher, resulting in output quality that typically lags behind their bidirectional counterparts. In this paper, we explore an alternative approach that uses reward signals to guide the generation process, enabling more efficient and scalable autoregressive generation. By using reward signals to guide the model, our method simplifies training while preserving high visual fidelity and temporal consistency. Through extensive experiments on standard benchmarks, we find that our approach performs comparably to existing autoregressive models and, in some cases, surpasses similarly sized bidirectional models by avoiding constraints imposed by teacher architectures. For example, on VBench, our method achieves a total score of 84.92, closely matching state-of-the-art autoregressive methods that score 84.31 but require significant heterogeneous distillation.</li>
<li><strong>摘要：</strong>虽然视频生成方面的大多数先前工作依赖于双向架构，但最近的努力试图将这些模型适应自回归变体以支持近实时生成。然而，这种适应通常严重依赖于教师模型，这可能会限制性能，特别是在没有强大的自回归教师的情况下，导致输出质量通常落后于双向模型。在本文中，我们探索了一种使用奖励信号来指导生成过程的替代方法，从而实现更高效和可扩展的自回归生成。通过使用奖励信号来指导模型，我们的方法简化了训练，同时保持了高视觉保真度和时间一致性。通过对标准基准的广泛实验，我们发现我们的方法的性能与现有的自回归模型相当，并且在某些情况下，通过避免教师架构施加的约束，超越了类似大小的双向模型。例如，在 VBench 上，我们的方法获得了 84.92 的总分，与得分 84.31 但需要显着异质蒸馏的最先进的自回归方法非常接近。</li>
</ul>

<h3>Title: 3D Molecule Generation from Rigid Motifs via SE(3) Flows</h3>
<ul>
<li><strong>Authors: </strong>Roman Poletukhin, Marcel Kollovieh, Eike Eberhard, Stephan Günnemann</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16955">https://arxiv.org/abs/2601.16955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16955">https://arxiv.org/pdf/2601.16955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16955]] 3D Molecule Generation from Rigid Motifs via SE(3) Flows(https://arxiv.org/abs/2601.16955)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Three-dimensional molecular structure generation is typically performed at the level of individual atoms, yet molecular graph generation techniques often consider fragments as their structural units. Building on the advances in frame-based protein structure generation, we extend these fragmentation ideas to 3D, treating general molecules as sets of rigid-body motifs. Utilising this representation, we employ SE(3)-equivariant generative modelling for de novo 3D molecule generation from rigid motifs. In our evaluations, we observe comparable or superior results to state-of-the-art across benchmarks, surpassing it in atom stability on GEOM-Drugs, while yielding a 2x to 10x reduction in generation steps and offering 3.5x compression in molecular representations compared to the standard atom-based methods.</li>
<li><strong>摘要：</strong>三维分子结构生成通常在单个原子的水平上进行，但分子图生成技术通常将片段视为其结构单元。基于基于框架的蛋白质结构生成的进展，我们将这些片段化想法扩展到 3D，将一般分子视为一组刚体图案。利用这种表示，我们采用 SE(3) 等变生成模型从刚性基序生成 3D 分子。在我们的评估中，我们观察到与基准测试中最先进的结果相当或更好的结果，在 GEOM-Drugs 上的原子稳定性方面超越了它，同时与标准的基于原子的方法相比，生成步骤减少了 2 倍到 10 倍，分子表示压缩率提高了 3.5 倍。</li>
</ul>

<h3>Title: Auto-Regressive Masked Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Mahdi Karami, Ali Ghodsi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16971">https://arxiv.org/abs/2601.16971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16971">https://arxiv.org/pdf/2601.16971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16971]] Auto-Regressive Masked Diffusion Models(https://arxiv.org/abs/2601.16971)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Masked diffusion models (MDMs) have emerged as a promising approach for language modeling, yet they face a performance gap compared to autoregressive models (ARMs) and require more training iterations. In this work, we present the Auto-Regressive Masked Diffusion (ARMD) model, an architecture designed to close this gap by unifying the training efficiency of autoregressive models with the parallel generation capabilities of diffusion-based models. Our key insight is to reframe the masked diffusion process as a block-wise causal model. This perspective allows us to design a strictly causal, permutation-equivariant architecture that computes all conditional probabilities across multiple denoising steps in a single, parallel forward pass. The resulting architecture supports efficient, autoregressive-style decoding and a progressive permutation training scheme, allowing the model to learn both canonical left-to-right and random token orderings. Leveraging this flexibility, we introduce a novel strided parallel generation strategy that accelerates inference by generating tokens in parallel streams while maintaining global coherence. Empirical results demonstrate that ARMD achieves state-of-the-art performance on standard language modeling benchmarks, outperforming established diffusion baselines while requiring significantly fewer training steps. Furthermore, it establishes a new benchmark for parallel text generation, effectively bridging the performance gap between parallel and sequential decoding.</li>
<li><strong>摘要：</strong>掩蔽扩散模型 (MDM) 已成为一种有前途的语言建模方法，但与自回归模型 (ARM) 相比，它们面临着性能差距，并且需要更多的训练迭代。在这项工作中，我们提出了自回归掩模扩散（ARMD）模型，该架构旨在通过将自回归模型的训练效率与基于扩散的模型的并行生成功能统一起来来缩小这一差距。我们的主要见解是将掩模扩散过程重新构建为块级因果模型。这种观点使我们能够设计一个严格因果、排列等变的架构，该架构可以在单个并行前向传递中计算多个去噪步骤的所有条件概率。由此产生的架构支持高效的自回归式解码和渐进排列训练方案，允许模型学习规范的从左到右和随机令牌排序。利用这种灵活性，我们引入了一种新颖的跨步并行生成策略，该策略通过在并行流中生成令牌来加速推理，同时保持全局一致性。实证结果表明，ARMD 在标准语言建模基准上实现了最先进的性能，优于既定的扩散基线，同时需要的训练步骤显着减少。此外，它为并行文本生成建立了新的基准，有效地缩小了并行解码和顺序解码之间的性能差距。</li>
</ul>

<h3>Title: Latent Diffusion for Internet of Things Attack Data Generation in Intrusion Detection</h3>
<ul>
<li><strong>Authors: </strong>Estela Sánchez-Carballo, Francisco M. Melgarejo-Meseguer, José Luis Rojo-Álvarez</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16976">https://arxiv.org/abs/2601.16976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16976">https://arxiv.org/pdf/2601.16976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16976]] Latent Diffusion for Internet of Things Attack Data Generation in Intrusion Detection(https://arxiv.org/abs/2601.16976)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Intrusion Detection Systems (IDSs) are a key component for protecting Internet of Things (IoT) environments. However, in Machine Learning-based (ML-based) IDSs, performance is often degraded by the strong class imbalance between benign and attack traffic. Although data augmentation has been widely explored to mitigate this issue, existing approaches typically rely on simple oversampling techniques or generative models that struggle to simultaneously achieve high sample fidelity, diversity, and computational efficiency. To address these limitations, we propose the use of a Latent Diffusion Model (LDM) for attack data augmentation in IoT intrusion detection and provide a comprehensive comparison against state-of-the-art baselines. Experiments were conducted on three representative IoT attack types, specifically Distributed Denial-of-Service (DDoS), Mirai, and Man-in-the-Middle, evaluating both downstream IDS performance and intrinsic generative quality using distributional, dependency-based, and diversity metrics. Results show that balancing the training data with LDM-generated samples substantially improves IDS performance, achieving F1-scores of up to 0.99 for DDoS and Mirai attacks and consistently outperforming competing methods. Additionally, quantitative and qualitative analyses demonstrate that LDMs effectively preserve feature dependencies while generating diverse samples and reduce sampling time by approximately 25\% compared to diffusion models operating directly in data space. These findings highlight latent diffusion as an effective and scalable solution for synthetic IoT attack data generation, substantially mitigating the impact of class imbalance in ML-based IDSs for IoT scenarios.</li>
<li><strong>摘要：</strong>入侵检测系统 (IDS) 是保护物联网 (IoT) 环境的关键组件。然而，在基于机器学习（基于 ML）的 IDS 中，良性流量和攻击流量之间的严重类别不平衡常常会导致性能下降。尽管数据增强已经被广泛探索来缓解这个问题，但现有的方法通常依赖于简单的过采样技术或生成模型，这些技术或生成模型很难同时实现高样本保真度、多样性和计算效率。为了解决这些限制，我们建议在物联网入侵检测中使用潜在扩散模型（LDM）来增强攻击数据，并提供与最先进基线的全面比较。对三种代表性的物联网攻击类型（特别是分布式拒绝服务 (DDoS)、Mirai 和中间人）进行了实验，使用分布式、基于依赖和多样性指标评估下游 IDS 性能和内在生成质量。结果表明，平衡训练数据与 LDM 生成的样本可显着提高 IDS 性能，针对 DDoS 和 Mirai 攻击实现高达 0.99 的 F1 分数，并且始终优于竞争方法。此外，定量和定性分析表明，与直接在数据空间中运行的扩散模型相比，LDM 在生成不同样本的同时有效地保留了特征依赖性，并将采样时间减少了约 25%。这些发现强调潜在扩散作为合成物联网攻击数据生成的有效且可扩展的解决方案，大大减轻了物联网场景中基于机器学习的 IDS 中类别不平衡的影响。</li>
</ul>

<h3>Title: SyncLight: Controllable and Consistent Multi-View Relighting</h3>
<ul>
<li><strong>Authors: </strong>David Serrano-Lozano, Anand Bhattad, Luis Herranz, Jean-François Lalonde, Javier Vazquez-Corral</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16981">https://arxiv.org/abs/2601.16981</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16981">https://arxiv.org/pdf/2601.16981</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16981]] SyncLight: Controllable and Consistent Multi-View Relighting(https://arxiv.org/abs/2601.16981)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present SyncLight, the first method to enable consistent, parametric relighting across multiple uncalibrated views of a static scene. While single-view relighting has advanced significantly, existing generative approaches struggle to maintain the rigorous lighting consistency essential for multi-camera broadcasts, stereoscopic cinema, and virtual production. SyncLight addresses this by enabling precise control over light intensity and color across a multi-view capture of a scene, conditioned on a single reference edit. Our method leverages a multi-view diffusion transformer trained using a latent bridge matching formulation, achieving high-fidelity relighting of the entire image set in a single inference step. To facilitate training, we introduce a large-scale hybrid dataset comprising diverse synthetic environments -- curated from existing sources and newly designed scenes -- alongside high-fidelity, real-world multi-view captures under calibrated illumination. Surprisingly, though trained only on image pairs, SyncLight generalizes zero-shot to an arbitrary number of viewpoints, effectively propagating lighting changes across all views, without requiring camera pose information. SyncLight enables practical relighting workflows for multi-view capture systems.</li>
<li><strong>摘要：</strong>我们提出了 SyncLight，这是第一种在静态场景的多个未校准视图中实现一致的参数重新照明的方法。虽然单视图重新照明取得了显着进步，但现有的生成方法很难保持多机位广播、立体电影和虚拟制作所必需的严格照明一致性。 SyncLight 通过在场景的多视图捕获中精确控制光强度和颜色来解决这个问题，并以单个参考编辑为条件。我们的方法利用使用潜在桥匹配公式训练的多视图扩散变换器，在单个推理步骤中实现整个图像集的高保真重新照明。为了促进训练，我们引入了一个大规模混合数据集，其中包含不同的合成环境（根据现有来源和新设计的场景精心策划）以及校准照明下的高保真、真实世界多视图捕获。令人惊讶的是，虽然仅在图像对上进行训练，但 SyncLight 将零镜头推广到任意数量的视点，有效地在所有视图中传播照明变化，而不需要相机姿势信息。 SyncLight 为多视图捕捉系统提供实用的重新照明工作流程。</li>
</ul>

<h3>Title: AnyView: Synthesizing Any Novel View in Dynamic Scenes</h3>
<ul>
<li><strong>Authors: </strong>Basile Van Hoorick, Dian Chen, Shun Iwase, Pavel Tokmakov, Muhammad Zubair Irshad, Igor Vasiljevic, Swati Gupta, Fangzhou Cheng, Sergey Zakharov, Vitor Campagnolo Guizilini</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16982">https://arxiv.org/abs/2601.16982</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16982">https://arxiv.org/pdf/2601.16982</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16982]] AnyView: Synthesizing Any Novel View in Dynamic Scenes(https://arxiv.org/abs/2601.16982)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Modern generative video models excel at producing convincing, high-quality outputs, but struggle to maintain multi-view and spatiotemporal consistency in highly dynamic real-world environments. In this work, we introduce \textbf{AnyView}, a diffusion-based video generation framework for \emph{dynamic view synthesis} with minimal inductive biases or geometric assumptions. We leverage multiple data sources with various levels of supervision, including monocular (2D), multi-view static (3D) and multi-view dynamic (4D) datasets, to train a generalist spatiotemporal implicit representation capable of producing zero-shot novel videos from arbitrary camera locations and trajectories. We evaluate AnyView on standard benchmarks, showing competitive results with the current state of the art, and propose \textbf{AnyViewBench}, a challenging new benchmark tailored towards \emph{extreme} dynamic view synthesis in diverse real-world scenarios. In this more dramatic setting, we find that most baselines drastically degrade in performance, as they require significant overlap between viewpoints, while AnyView maintains the ability to produce realistic, plausible, and spatiotemporally consistent videos when prompted from \emph{any} viewpoint. Results, data, code, and models can be viewed at: this https URL</li>
<li><strong>摘要：</strong>现代生成视频模型擅长产生令人信服的高质量输出，但难以在高度动态的现实环境中保持多视图和时空一致性。在这项工作中，我们介绍了 \textbf{AnyView}，一种基于扩散的视频生成框架，用于具有最小归纳偏差或几何假设的 \emph{动态视图合成}。我们利用具有不同监督级别的多个数据源，包括单目（2D）、多视图静态（3D）和多视图动态（4D）数据集，来训练通用时空隐式表示，能够从任意相机位置和轨迹生成零样本新颖视频。我们在标准基准上评估 AnyView，显示出与当前最先进水平具有竞争力的结果，并提出 \textbf{AnyViewBench}，这是一个具有挑战性的新基准，专为各种现实场景中的 \emph{extreme} 动态视图合成而定制。在这种更戏剧性的设置中，我们发现大多数基线的性能都会急剧下降，因为它们需要视点之间存在显着重叠，而 AnyView 在从 \emph{any} 视点提示时保持了生成真实、合理且时空一致的视频的能力。结果、数据、代码和模型可以在以下位置查看：此 https URL</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
