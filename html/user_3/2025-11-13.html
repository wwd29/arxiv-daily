<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-11-13</h1>
<h3>Title: Case Study: Transformer-Based Solution for the Automatic Digitization of Gas Plants</h3>
<ul>
<li><strong>Authors: </strong>I. Bailo, F. Buonora, G. Ciarfaglia, L. T. Consoli, A. Evangelista, M. Gabusi, M. Ghiani, C. Petracca Ciavarella, F. Picariello, F. Sarcina, F. Tuosto, V. Zullo, L. Airoldi, G. Bruno, D. D. Gobbo, S. Pezzenati, G. A. Tona</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08609">https://arxiv.org/abs/2511.08609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08609">https://arxiv.org/pdf/2511.08609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08609]] Case Study: Transformer-Based Solution for the Automatic Digitization of Gas Plants(https://arxiv.org/abs/2511.08609)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The energy transition is a key theme of the last decades to determine a future of eco-sustainability, and an area of such importance cannot disregard digitization, innovation and the new technological tools available. This is the context in which the Generative Artificial Intelligence models described in this paper are positioned, developed by Engineering Ingegneria Informatica SpA in order to automate the plant structures acquisition of SNAM energy infrastructure, a leading gas transportation company in Italy and Europe. The digitization of a gas plant consists in registering all its relevant information through the interpretation of the related documentation. The aim of this work is therefore to design an effective solution based on Artificial Intelligence techniques to automate the extraction of the information necessary for the digitization of a plant, in order to streamline the daily work of MGM users. The solution received the P&ID of the plant as input, each one in pdf format, and uses OCR, Vision LLM, Object Detection, Relational Reasoning and optimization algorithms to return an output consisting of two sets of information: a structured overview of the relevant design data and the hierarchical framework of the plant. To achieve convincing results, we extend a state-of-the-art model for Scene Graph Generation introducing a brand new Transformer architecture with the aim of deepening the analysis of the complex relations between the plant's components. The synergistic use of the listed AI-based technologies allowed to overcome many obstacles arising from the high variety of data, due to the lack of standardization. An accuracy of 91\% has been achieved in the extraction of textual information relating to design data. Regarding the plants topology, 93\% of components are correctly identified and the hierarchical structure is extracted with an accuracy around 80\%.</li>
<li><strong>摘要：</strong>能源转型是过去几十年来决定生态可持续性未来的一个关键主题，如此重要的领域不能忽视数字化、创新和可用的新技术工具。本文中描述的生成人工智能模型就是在这样的背景下进行的，该模型由 Engineering Ingegneria Informatica SpA 开发，旨在自动化 SNAM 能源基础设施（意大利和欧洲领先的天然气运输公司）的工厂结构收购。天然气工厂的数字化在于通过解释相关文件来注册其所有相关信息。因此，这项工作的目的是设计一种基于人工智能技术的有效解决方案，自动提取工厂数字化所需的信息，以简化米高梅用户的日常工作。该解决方案接收工厂的 P&ID 作为输入，每一个均为 pdf 格式，并使用 OCR、Vision LLM、对象检测、关系推理和优化算法返回包含两组信息的输出：相关设计数据的结构化概述和工厂的分层框架。为了获得令人信服的结果，我们扩展了最先进的场景图生成模型，引入了全新的 Transformer 架构，旨在加深对工厂组件之间复杂关系的分析。所列出的基于人工智能的技术的协同使用可以克服由于缺乏标准化而导致的数据种类繁多而产生的许多障碍。设计数据相关文本信息的提取准确率达到 91%。对于植物拓扑，93% 的组件被正确识别，层次结构提取的准确度约为 80%。</li>
</ul>

<h3>Title: Assessing Identity Leakage in Talking Face Generation: Metrics and Evaluation Framework</h3>
<ul>
<li><strong>Authors: </strong>Dogucan Yaman, Fevziye Irem Eyiokur, Hazım Kemal Ekenel, Alexander Waibel</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08613">https://arxiv.org/abs/2511.08613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08613">https://arxiv.org/pdf/2511.08613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08613]] Assessing Identity Leakage in Talking Face Generation: Metrics and Evaluation Framework(https://arxiv.org/abs/2511.08613)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Inpainting-based talking face generation aims to preserve video details such as pose, lighting, and gestures while modifying only lip motion, often using an identity reference image to maintain speaker consistency. However, this mechanism can introduce lip leaking, where generated lips are influenced by the reference image rather than solely by the driving audio. Such leakage is difficult to detect with standard metrics and conventional test setup. To address this, we propose a systematic evaluation methodology to analyze and quantify lip leakage. Our framework employs three complementary test setups: silent-input generation, mismatched audio-video pairing, and matched audio-video synthesis. We also introduce derived metrics including lip-sync discrepancy and silent-audio-based lip-sync scores. In addition, we study how different identity reference selections affect leakage, providing insights into reference design. The proposed methodology is model-agnostic and establishes a more reliable benchmark for future research in talking face generation.</li>
<li><strong>摘要：</strong>基于修复的说话人脸生成旨在保留姿势、灯光和手势等视频细节，同时仅修改嘴唇运动，通常使用身份参考图像来保持说话者的一致性。然而，这种机制可能会导致嘴唇泄漏，其中生成的嘴唇受到参考图像的影响，而不仅仅是受驱动音频的影响。使用标准指标和传统测试设置很难检测到这种泄漏。为了解决这个问题，我们提出了一种系统的评估方法来分析和量化唇部泄漏。我们的框架采用三种互补的测试设置：无声输入生成、不匹配的音频-视频配对和匹配的音频-视频合成。我们还引入了衍生指标，包括口型同步差异和基于无声音频的口型同步分数。此外，我们研究了不同的身份参考选择如何影响泄漏，为参考设计提供见解。所提出的方法与模型无关，并为未来的说话面孔生成研究建立了更可靠的基准。</li>
</ul>

<h3>Title: Time-to-Move: Training-Free Motion Controlled Video Generation via Dual-Clock Denoising</h3>
<ul>
<li><strong>Authors: </strong>Assaf Singer, Noam Rotstein, Amir Mann, Ron Kimmel, Or Litany</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08633">https://arxiv.org/abs/2511.08633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08633">https://arxiv.org/pdf/2511.08633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08633]] Time-to-Move: Training-Free Motion Controlled Video Generation via Dual-Clock Denoising(https://arxiv.org/abs/2511.08633)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion-based video generation can create realistic videos, yet existing image- and text-based conditioning fails to offer precise motion control. Prior methods for motion-conditioned synthesis typically require model-specific fine-tuning, which is computationally expensive and restrictive. We introduce Time-to-Move (TTM), a training-free, plug-and-play framework for motion- and appearance-controlled video generation with image-to-video (I2V) diffusion models. Our key insight is to use crude reference animations obtained through user-friendly manipulations such as cut-and-drag or depth-based reprojection. Motivated by SDEdit's use of coarse layout cues for image editing, we treat the crude animations as coarse motion cues and adapt the mechanism to the video domain. We preserve appearance with image conditioning and introduce dual-clock denoising, a region-dependent strategy that enforces strong alignment in motion-specified regions while allowing flexibility elsewhere, balancing fidelity to user intent with natural dynamics. This lightweight modification of the sampling process incurs no additional training or runtime cost and is compatible with any backbone. Extensive experiments on object and camera motion benchmarks show that TTM matches or exceeds existing training-based baselines in realism and motion control. Beyond this, TTM introduces a unique capability: precise appearance control through pixel-level conditioning, exceeding the limits of text-only prompting. Visit our project page for video examples and code: this https URL.</li>
<li><strong>摘要：</strong>基于扩散的视频生成可以创建逼真的视频，但现有的基于图像和文本的调节无法提供精确的运动控制。先前的运动条件合成方法通常需要特定于模型的微调，这在计算上是昂贵的且具有限制性。我们引入了 Time-to-Move (TTM)，这是一种免训练、即插即用的框架，用于通过图像到视频 (I2V) 扩散模型生成运动和外观控制的视频。我们的主要见解是使用通过用户友好的操作（例如剪切和拖动或基于深度的重投影）获得的粗略参考动画。受 SDEdit 使用粗略布局提示进行图像编辑的启发，我们将粗略动画视为粗略运动提示，并使该机制适应视频领域。我们通过图像调节来保留外观，并引入双时钟去噪，这是一种区域相关策略，可在运动指定区域中强制对齐，同时在其他地方允许灵活性，平衡用户意图的保真度与自然动态。这种对采样过程的轻量级修改不会产生额外的培训或运行时间成本，并且与任何骨干网兼容。对物体和相机运动基准的大量实验表明，TTM 在真实感和运动控制方面匹配或超过了现有的基于训练的基线。除此之外，TTM 还引入了一项独特的功能：通过像素级调节进行精确的外观控制，超越了纯文本提示的限制。请访问我们的项目页面以获取视频示例和代码：此 https URL。</li>
</ul>

<h3>Title: RS-Net: Context-Aware Relation Scoring for Dynamic Scene Graph Generation</h3>
<ul>
<li><strong>Authors: </strong>Hae-Won Jo, Yeong-Jun Cho</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08651">https://arxiv.org/abs/2511.08651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08651">https://arxiv.org/pdf/2511.08651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08651]] RS-Net: Context-Aware Relation Scoring for Dynamic Scene Graph Generation(https://arxiv.org/abs/2511.08651)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Dynamic Scene Graph Generation (DSGG) models how object relations evolve over time in videos. However, existing methods are trained only on annotated object pairs and lack guidance for non-related pairs, making it difficult to identify meaningful relations during inference. In this paper, we propose Relation Scoring Network (RS-Net), a modular framework that scores the contextual importance of object pairs using both spatial interactions and long-range temporal context. RS-Net consists of a spatial context encoder with learnable context tokens and a temporal encoder that aggregates video-level information. The resulting relation scores are integrated into a unified triplet scoring mechanism to enhance relation prediction. RS-Net can be easily integrated into existing DSGG models without architectural changes. Experiments on the Action Genome dataset show that RS-Net consistently improves both Recall and Precision across diverse baselines, with notable gains in mean Recall, highlighting its ability to address the long-tailed distribution of relations. Despite the increased number of parameters, RS-Net maintains competitive efficiency, achieving superior performance over state-of-the-art methods.</li>
<li><strong>摘要：</strong>动态场景图生成 (DSGG) 对视频中对象关系如何随时间演变进行建模。然而，现有的方法仅针对带注释的对象对进行训练，缺乏对非相关对象对的指导，使得在推理过程中难以识别有意义的关系。在本文中，我们提出了关系评分网络（RS-Net），这是一种模块化框架，可使用空间交互和远程时间上下文对对象对的上下文重要性进行评分。 RS-Net 由具有可学习上下文标记的空间上下文编码器和聚合视频级信息的时间编码器组成。得到的关系分数被集成到统一的三元组评分机制中，以增强关系预测。 RS-Net 可以轻松集成到现有的 DSGG 模型中，无需更改架构。 Action Genome 数据集上的实验表明，RS-Net 在不同基线上持续提高召回率和精确度，平均召回率显着提高，凸显了其解决关系长尾分布的能力。尽管参数数量增加，RS-Net 仍保持有竞争力的效率，实现了优于最先进方法的性能。</li>
</ul>

<h3>Title: TabPFN-2.5: Advancing the State of the Art in Tabular Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Léo Grinsztajn, Klemens Flöge, Oscar Key, Felix Birkel, Philipp Jund, Brendan Roof, Benjamin Jäger, Dominik Safaric, Simone Alessi, Adrian Hayler, Mihir Manium, Rosen Yu, Felix Jablonski, Shi Bin Hoo, Anurag Garg, Jake Robertson, Magnus Bühler, Vladyslav Moroshan, Lennart Purucker, Clara Cornu, Lilly Charlotte Wehrhahn, Alessandro Bonetto, Bernhard Schölkopf, Sauraj Gambhir, Noah Hollmann, Frank Hutter</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08667">https://arxiv.org/abs/2511.08667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08667">https://arxiv.org/pdf/2511.08667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08667]] TabPFN-2.5: Advancing the State of the Art in Tabular Foundation Models(https://arxiv.org/abs/2511.08667)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The first tabular foundation model, TabPFN, and its successor TabPFNv2 have impacted tabular AI substantially, with dozens of methods building on it and hundreds of applications across different use cases. This report introduces TabPFN-2.5, the next generation of our tabular foundation model, built for datasets with up to 50,000 data points and 2,000 features, a 20x increase in data cells compared to TabPFNv2. TabPFN-2.5 is now the leading method for the industry standard benchmark TabArena (which contains datasets with up to 100,000 training data points), substantially outperforming tuned tree-based models and matching the accuracy of AutoGluon 1.4, a complex four-hour tuned ensemble that even includes the previous TabPFNv2. Remarkably, default TabPFN-2.5 has a 100% win rate against default XGBoost on small to medium-sized classification datasets (<=10,000 data points, 500 features) and a 87% win rate on larger datasets up to 100K samples and 2K features (85% for regression). For production use cases, we introduce a new distillation engine that converts TabPFN-2.5 into a compact MLP or tree ensemble, preserving most of its accuracy while delivering orders-of-magnitude lower latency and plug-and-play deployment. This new release will immediately strengthen the performance of the many applications and methods already built on the TabPFN ecosystem.</li>
<li><strong>摘要：</strong>第一个表格基础模型 TabPFN 及其后继者 TabPFNv2 对表格 AI 产生了重大影响，在其上构建了数十种方法以及跨不同用例的数百个应用程序。本报告介绍了 TabPFN-2.5，它是我们的下一代表格基础模型，专为具有多达 50,000 个数据点和 2,000 个特征的数据集而构建，与 TabPFNv2 相比，数据单元数量增加了 20 倍。 TabPFN-2.5 现在是行业标准基准 TabArena（包含多达 100,000 个训练数据点的数据集）的领先方法，其性能大大优于经过调整的基于树的模型，并与 AutoGluon 1.4 的准确性相匹配，AutoGluon 1.4 是一个复杂的四小时调整集成，甚至包括之前的 TabPFNv2。值得注意的是，默认 TabPFN-2.5 在中小型分类数据集（<=10,000 个数据点、500 个特征）上相对于默认 XGBoost 的胜率是 100%，在高达 100K 样本和 2K 特征的较大数据集上胜率为 87%（回归为 85%）。对于生产用例，我们引入了一种新的蒸馏引擎，可将 TabPFN-2.5 转换为紧凑的 MLP 或树集成，保留其大部分准确性，同时提供数量级的低延迟和即插即用部署。这个新版本将立即增强 TabPFN 生态系统上已经构建的许多应用程序和方法的性能。</li>
</ul>

<h3>Title: Rethinking generative image pretraining: How far are we from scaling up next-pixel prediction?</h3>
<ul>
<li><strong>Authors: </strong>Xinchen Yan, Chen Liang, Lijun Yu, Adams Wei Yu, Yifeng Lu, Quoc V. Le</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08704">https://arxiv.org/abs/2511.08704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08704">https://arxiv.org/pdf/2511.08704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08704]] Rethinking generative image pretraining: How far are we from scaling up next-pixel prediction?(https://arxiv.org/abs/2511.08704)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>This paper investigates the scaling properties of autoregressive next-pixel prediction, a simple, end-to-end yet under-explored framework for unified vision models. Starting with images at resolutions of 32x32, we train a family of Transformers using IsoFlops profiles across compute budgets up to 7e19 FLOPs and evaluate three distinct target metrics: next-pixel prediction objective, ImageNet classification accuracy, and generation quality measured by Fr'echet Distance. First, optimal scaling strategy is critically task-dependent. At a fixed 32x32 resolution alone, the optimal scaling properties for image classification and image generation diverge, where generation optimal setup requires the data size grow three to five times faster than for the classification optimal setup. Second, as image resolution increases, the optimal scaling strategy indicates that the model size must grow much faster than data size. Surprisingly, by projecting our findings, we discover that the primary bottleneck is compute rather than the amount of training data. As compute continues to grow four to five times annually, we forecast the feasibility of pixel-by-pixel modeling of images within the next five years.</li>
<li><strong>摘要：</strong>本文研究了自回归下一像素预测的缩放特性，这是一种简单的端到端但尚未充分探索的统一视觉模型框架。从分辨率为 32x32 的图像开始，我们使用 IsoFlops 配置文件在高达 7e19 FLOP 的计算预算中训练一系列 Transformer，并评估三个不同的目标指标：下一像素预测目标、ImageNet 分类准确性以及通过 Fr'echet Distance 测量的生成质量。首先，最佳扩展策略与任务密切相关。仅在固定的 32x32 分辨率下，图像分类和图像生成的最佳缩放属性就存在差异，其中生成最佳设置要求数据大小的增长速度比分类最佳设置快三到五倍。其次，随着图像分辨率的增加，最佳缩放策略表明模型大小必须比数据大小增长得快得多。令人惊讶的是，通过预测我们的发现，我们发现主要瓶颈是计算而不是训练数据量。随着计算量每年持续增长四到五倍，我们预测未来五年内逐像素建模图像的可行性。</li>
</ul>

<h3>Title: Rethinking Graph Super-resolution: Dual Frameworks for Topological Fidelity</h3>
<ul>
<li><strong>Authors: </strong>Pragya Singh, Islem Rekik</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08853">https://arxiv.org/abs/2511.08853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08853">https://arxiv.org/pdf/2511.08853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08853]] Rethinking Graph Super-resolution: Dual Frameworks for Topological Fidelity(https://arxiv.org/abs/2511.08853)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Graph super-resolution, the task of inferring high-resolution (HR) graphs from low-resolution (LR) counterparts, is an underexplored yet crucial research direction that circumvents the need for costly data acquisition. This makes it especially desirable for resource-constrained fields such as the medical domain. While recent GNN-based approaches show promise, they suffer from two key limitations: (1) matrix-based node super-resolution that disregards graph structure and lacks permutation invariance; and (2) reliance on node representations to infer edge weights, which limits scalability and expressivity. In this work, we propose two GNN-agnostic frameworks to address these issues. First, Bi-SR introduces a bipartite graph connecting LR and HR nodes to enable structure-aware node super-resolution that preserves topology and permutation invariance. Second, DEFEND learns edge representations by mapping HR edges to nodes of a dual graph, allowing edge inference via standard node-based GNNs. We evaluate both frameworks on a real-world brain connectome dataset, where they achieve state-of-the-art performance across seven topological measures. To support generalization, we introduce twelve new simulated datasets that capture diverse topologies and LR-HR relationships. These enable comprehensive benchmarking of graph super-resolution methods.</li>
<li><strong>摘要：</strong>图超分辨率是从低分辨率（LR）图推断高分辨率（HR）图的任务，是一个尚未充分探索但至关重要的研究方向，可以避免昂贵的数据采集需求。这使得它特别适合医疗领域等资源有限的领域。虽然最近基于 GNN 的方法显示出了希望，但它们存在两个关键限制：（1）基于矩阵的节点超分辨率，它忽略图结构并且缺乏排列不变性； (2)依赖节点表示来推断边权重，这限制了可扩展性和表达能力。在这项工作中，我们提出了两个与 GNN 无关的框架来解决这些问题。首先，Bi-SR 引入了连接 LR 和 HR 节点的二部图，以实现结构感知节点超分辨率，从而保留拓扑和排列不变性。其次，DEFEND 通过将 HR 边映射到对偶图的节点来学习边表示，从而允许通过标准的基于节点的 GNN 进行边推断。我们在现实世界的大脑连接组数据集上评估这两个框架，它们在七个拓扑测量中实现了最先进的性能。为了支持泛化，我们引入了 12 个新的模拟数据集，这些数据集捕获不同的拓扑和 LR-HR 关系。这些可以对图超分辨率方法进行全面的基准测试。</li>
</ul>

<h3>Title: Covariance Scattering Transforms</h3>
<ul>
<li><strong>Authors: </strong>Andrea Cavallo, Ayushman Raghuvanshi, Sundeep Prabhakar Chepuri, Elvin Isufi</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08878">https://arxiv.org/abs/2511.08878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08878">https://arxiv.org/pdf/2511.08878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08878]] Covariance Scattering Transforms(https://arxiv.org/abs/2511.08878)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Machine learning and data processing techniques relying on covariance information are widespread as they identify meaningful patterns in unsupervised and unlabeled settings. As a prominent example, Principal Component Analysis (PCA) projects data points onto the eigenvectors of their covariance matrix, capturing the directions of maximum variance. This mapping, however, falls short in two directions: it fails to capture information in low-variance directions, relevant when, e.g., the data contains high-variance noise; and it provides unstable results in low-sample regimes, especially when covariance eigenvalues are close. CoVariance Neural Networks (VNNs), i.e., graph neural networks using the covariance matrix as a graph, show improved stability to estimation errors and learn more expressive functions in the covariance spectrum than PCA, but require training and operate in a labeled setup. To get the benefits of both worlds, we propose Covariance Scattering Transforms (CSTs), deep untrained networks that sequentially apply filters localized in the covariance spectrum to the input data and produce expressive hierarchical representations via nonlinearities. We define the filters as covariance wavelets that capture specific and detailed covariance spectral patterns. We improve CSTs' computational and memory efficiency via a pruning mechanism, and we prove that their error due to finite-sample covariance estimations is less sensitive to close covariance eigenvalues compared to PCA, improving their stability. Our experiments on age prediction from cortical thickness measurements on 4 datasets collecting patients with neurodegenerative diseases show that CSTs produce stable representations in low-data settings, as VNNs but without any training, and lead to comparable or better predictions w.r.t. more complex learning models.</li>
<li><strong>摘要：</strong>依赖协方差信息的机器学习和数据处理技术非常广泛，因为它们可以在无监督和无标签的环境中识别有意义的模式。一个突出的例子是，主成分分析 (PCA) 将数据点投影到其协方差矩阵的特征向量上，捕获最大方差的方向。然而，这种映射在两个方向上存在不足：它无法捕获低方差方向的信息，例如当数据包含高方差噪声时相关的信息；它在低样本情况下提供不稳定的结果，特别是当协方差特征值接近时。协方差神经网络 (VNN)，即使用协方差矩阵作为图形的图形神经网络，与 PCA 相比，对估计误差具有更高的稳定性，并且可以在协方差谱中学习更多的表达函数，但需要在标记设置中进行训练和操作。为了获得两全其美的好处，我们提出了协方差散射变换（CST），这是一种未经训练的深度网络，它顺序地将协方差谱中的局部滤波器应用于输入数据，并通过非线性产生富有表现力的层次表示。我们将滤波器定义为捕获特定且详细的协方差谱模式的协方差小波。我们通过剪枝机制提高了 CST 的计算和内存效率，并且证明与 PCA 相比，有限样本协方差估计引起的误差对接近的协方差特征值不太敏感，从而提高了其稳定性。我们对收集神经退行性疾病患者的 4 个数据集进行皮层厚度测量进行年龄预测的实验表明，CST 在低数据设置中产生稳定的表示，如 VNN，但未经任何训练，并导致可比较或更好的预测。更复杂的学习模型。</li>
</ul>

<h3>Title: Weaver: Kronecker Product Approximations of Spatiotemporal Attention for Traffic Network Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Christopher Cheong, Gary Davis, Seongjin Choi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08888">https://arxiv.org/abs/2511.08888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08888">https://arxiv.org/pdf/2511.08888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08888]] Weaver: Kronecker Product Approximations of Spatiotemporal Attention for Traffic Network Forecasting(https://arxiv.org/abs/2511.08888)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Spatiotemporal forecasting on transportation networks is a complex task that requires understanding how traffic nodes interact within a dynamic, evolving system dictated by traffic flow dynamics and social behavioral patterns. The importance of transportation networks and ITS for modern mobility and commerce necessitates forecasting models that are not only accurate but also interpretable, efficient, and robust under structural or temporal perturbations. Recent approaches, particularly Transformer-based architectures, have improved predictive performance but often at the cost of high computational overhead and diminished architectural interpretability. In this work, we introduce Weaver, a novel attention-based model that applies Kronecker product approximations (KPA) to decompose the PN X PN spatiotemporal attention of O(P^2N^2) complexity into local P X P temporal and N X N spatial attention maps. This Kronecker attention map enables our Parallel-Kronecker Matrix-Vector product (P2-KMV) for efficient spatiotemporal message passing with O(P^2N + N^2P) complexity. To capture real-world traffic dynamics, we address the importance of negative edges in modeling traffic behavior by introducing Valence Attention using the continuous Tanimoto coefficient (CTC), which provides properties conducive to precise latent graph generation and training stability. To fully utilize the model's learning capacity, we introduce the Traffic Phase Dictionary for self-conditioning. Evaluations on PEMS-BAY and METR-LA show that Weaver achieves competitive performance across model categories while training more efficiently.</li>
<li><strong>摘要：</strong>交通网络的时空预测是一项复杂的任务，需要了解交通节点如何在由交通流动态和社会行为模式决定的动态、不断发展的系统中相互作用。交通网络和智能交通系统对于现代交通和商业至关重要，因此预测模型不仅要准确，而且在结构或时间扰动下还要可解释、高效且稳健。最近的方法，特别是基于 Transformer 的架构，提高了预测性能，但通常以高计算开销和架构可解释性降低为代价。在这项工作中，我们介绍了 Weaver，一种基于注意力的新型模型，它应用克罗内克乘积近似 (KPA) 将 O(P^2N^2) 复杂度的 PN X PN 时空注意力分解为局部 P X P 时间和 N X N 空间注意力图。该克罗内克注意力图使我们的并行克罗内克矩阵向量产品 (P2-KMV) 能够以 O(P^2N + N^2P) 复杂度实现高效的时空消息传递。为了捕捉现实世界的交通动态，我们通过使用连续谷本系数（CTC）引入效价注意力来解决负边在交通行为建模中的重要性，该系数提供了有利于精确潜在图生成和训练稳定性的属性。为了充分利用模型的学习能力，我们引入了交通阶段词典进行自我调节。对 PEMS-BAY 和 METR-LA 的评估表明，Weaver 在跨模型类别方面取得了有竞争力的表现，同时训练效率更高。</li>
</ul>

<h3>Title: FGM-HD: Boosting Generation Diversity of Fractal Generative Models through Hausdorff Dimension Induction</h3>
<ul>
<li><strong>Authors: </strong>Haowei Zhang, Yuanpei Zhao, Jizhe Zhou, Mao Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08945">https://arxiv.org/abs/2511.08945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08945">https://arxiv.org/pdf/2511.08945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08945]] FGM-HD: Boosting Generation Diversity of Fractal Generative Models through Hausdorff Dimension Induction(https://arxiv.org/abs/2511.08945)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Improving the diversity of generated results while maintaining high visual quality remains a significant challenge in image generation tasks. Fractal Generative Models (FGMs) are efficient in generating high-quality images, but their inherent self-similarity limits the diversity of output images. To address this issue, we propose a novel approach based on the Hausdorff Dimension (HD), a widely recognized concept in fractal geometry used to quantify structural complexity, which aids in enhancing the diversity of generated outputs. To incorporate HD into FGM, we propose a learnable HD estimation method that predicts HD directly from image embeddings, addressing computational cost concerns. However, simply introducing HD into a hybrid loss is insufficient to enhance diversity in FGMs due to: 1) degradation of image quality, and 2) limited improvement in generation diversity. To this end, during training, we adopt an HD-based loss with a monotonic momentum-driven scheduling strategy to progressively optimize the hyperparameters, obtaining optimal diversity without sacrificing visual quality. Moreover, during inference, we employ HD-guided rejection sampling to select geometrically richer outputs. Extensive experiments on the ImageNet dataset demonstrate that our FGM-HD framework yields a 39\% improvement in output diversity compared to vanilla FGMs, while preserving comparable image quality. To our knowledge, this is the very first work introducing HD into FGM. Our method effectively enhances the diversity of generated outputs while offering a principled theoretical contribution to FGM development.</li>
<li><strong>摘要：</strong>在保持高视觉质量的同时提高生成结果的多样性仍然是图像生成任务中的重大挑战。分形生成模型（FGM）可以有效地生成高质量图像，但其固有的自相似性限制了输出图像的多样性。为了解决这个问题，我们提出了一种基于豪斯多夫维（HD）的新方法，这是分形几何中广泛认可的概念，用于量化结构复杂性，有助于增强生成输出的多样性。为了将 HD 纳入 FGM，我们提出了一种可学习的 HD 估计方法，该方法可以直接从图像嵌入中预测 HD，从而解决计算成本问题。然而，简单地将 HD 引入混合损失不足以增强 FGM 的多样性，因为：1）图像质量下降，2）生成多样性的改善有限。为此，在训练过程中，我们采用基于HD的损失和单调动量驱动的调度策略来逐步优化超参数，在不牺牲视觉质量的情况下获得最佳多样性。此外，在推理过程中，我们采用 HD 引导拒绝采样来选择几何上更丰富的输出。对 ImageNet 数据集的大量实验表明，与普通 FGM 相比，我们的 FGM-HD 框架在输出多样性方面提高了 39%，同时保持了相当的图像质量。据我们所知，这是第一部将HD引入FGM的作品。我们的方法有效地增强了生成输出的多样性，同时为女性生殖器切割的发展提供了原则性的理论贡献。</li>
</ul>

<h3>Title: Improving Conditional VAE with approximation using Normalizing Flows</h3>
<ul>
<li><strong>Authors: </strong>Tuhin Subhra De</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08946">https://arxiv.org/abs/2511.08946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08946">https://arxiv.org/pdf/2511.08946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08946]] Improving Conditional VAE with approximation using Normalizing Flows(https://arxiv.org/abs/2511.08946)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Variational Autoencoders and Generative Adversarial Networks remained the state-of-the-art (SOTA) generative models until 2022. Now they are superseded by diffusion based models. Efforts to improve traditional models have stagnated as a result. In old-school fashion, we explore image generation with conditional Variational Autoencoders (CVAE) to incorporate desired attributes within the images. VAEs are known to produce blurry images with less diversity, we refer a method that solve this issue by leveraging the variance of the gaussian decoder as a learnable parameter during training. Previous works on CVAEs assumed that the conditional distribution of the latent space given the labels is equal to the prior distribution, which is not the case in reality. We show that estimating it using normalizing flows results in better image generation than existing methods by reducing the FID by 5% and increasing log likelihood by 7.7% than the previous case.</li>
<li><strong>摘要：</strong>直到 2022 年，变分自动编码器和生成对抗网络仍然是最先进的 (SOTA) 生成模型。现在它们已被基于扩散的模型所取代。结果，改进传统模式的努力陷入停滞。在老式方式中，我们探索使用条件变分自动编码器（CVAE）生成图像，以将所需的属性合并到图像中。已知 VAE 会产生多样性较低的模糊图像，我们参考了一种通过利用高斯解码器的方差作为训练期间可学习参数来解决此问题的方法。之前关于 CVAE 的研究假设给定标签的潜在空间的条件分布等于先验分布，但现实情况并非如此。我们表明，使用归一化流进行估计可以比现有方法产生更好的图像，与之前的情况相比，FID 减少了 5%，对数似然增加了 7.7%。</li>
</ul>

<h3>Title: AuthSig: Safeguarding Scanned Signatures Against Unauthorized Reuse in Paperless Workflows</h3>
<ul>
<li><strong>Authors: </strong>RuiQiang Zhang, Zehua Ma, Guanjie Wang, Chang Liu, Hengyi Wang, Weiming Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08967">https://arxiv.org/abs/2511.08967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08967">https://arxiv.org/pdf/2511.08967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08967]] AuthSig: Safeguarding Scanned Signatures Against Unauthorized Reuse in Paperless Workflows(https://arxiv.org/abs/2511.08967)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>With the deepening trend of paperless workflows, signatures as a means of identity authentication are gradually shifting from traditional ink-on-paper to electronic this http URL the availability of dynamic pressure-sensitive and PKI-based digital signatures, static scanned signatures remain prevalent in practice due to their convenience. However, these static images, having almost lost their authentication attributes, cannot be reliably verified and are vulnerable to malicious copying and reuse. To address these issues, we propose AuthSig, a novel static electronic signature framework based on generative models and watermark, which binds authentication information to the signature image. Leveraging the human visual system's insensitivity to subtle style variations, AuthSig finely modulates style embeddings during generation to implicitly encode watermark bits-enforcing a One Signature, One Use this http URL overcome the scarcity of handwritten signature data and the limitations of traditional augmentation methods, we introduce a keypoint-driven data augmentation strategy that effectively enhances style diversity to support robust watermark embedding. Experimental results show that AuthSig achieves over 98% extraction accuracy under both digital-domain distortions and signature-specific degradations, and remains effective even in print-scan scenarios.</li>
<li><strong>摘要：</strong>随着无纸化工作流程的深入，签名作为身份认证的手段逐渐从传统的纸质签名转向电子化，动态压敏签名和基于PKI的数字签名、静态扫描签名因其便捷性在实践中仍然普遍存在。然而，这些静态图像几乎失去了认证属性，无法可靠地验证，并且容易被恶意复制和重用。为了解决这些问题，我们提出了 AuthSig，一种基于生成模型和水印的新型静态电子签名框架，它将身份验证信息绑定到签名图像。利用人类视觉系统对微妙风格变化的不敏感性，AuthSig在生成过程中精细地调制风格嵌入，以隐式编码水印位——强制执行一个签名，一个使用此http URL克服了手写签名数据的稀缺性和传统增强方法的局限性，我们引入了一种关键点驱动的数据增强策略，该策略有效增强了风格多样性以支持鲁棒的水印嵌入。实验结果表明，AuthSig 在数字域失真和签名特定退化的情况下都能实现超过 98% 的提取准确率，并且即使在打印扫描场景下也仍然有效。</li>
</ul>

<h3>Title: Data reuse enables cost-efficient randomized trials of medical AI models</h3>
<ul>
<li><strong>Authors: </strong>Michael Nercessian, Wenxin Zhang, Alexander Schubert, Daphne Yang, Maggie Chung, Ahmed Alaa, Adam Yala</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08986">https://arxiv.org/abs/2511.08986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08986">https://arxiv.org/pdf/2511.08986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08986]] Data reuse enables cost-efficient randomized trials of medical AI models(https://arxiv.org/abs/2511.08986)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Randomized controlled trials (RCTs) are indispensable for establishing the clinical value of medical artificial-intelligence (AI) tools, yet their high cost and long timelines hinder timely validation as new models emerge rapidly. Here, we propose BRIDGE, a data-reuse RCT design for AI-based risk models. AI risk models support a broad range of interventions, including screening, treatment selection, and clinical alerts. BRIDGE trials recycle participant-level data from completed trials of AI models when legacy and updated models make concordant predictions, thereby reducing the enrollment requirement for subsequent trials. We provide a practical checklist for investigators to assess whether reusing data from previous trials allows for valid causal inference and preserves type I error. Using real-world datasets across breast cancer, cardiovascular disease, and sepsis, we demonstrate concordance between successive AI models, with up to 64.8% overlap in top 5% high-risk cohorts. We then simulate a series of breast cancer screening studies, where our design reduced required enrollment by 46.6%--saving over US$2.8 million--while maintaining 80% power. By transforming trials into adaptive, modular studies, our proposed design makes Level I evidence generation feasible for every model iteration, thereby accelerating cost-effective translation of AI into routine care.</li>
<li><strong>摘要：</strong>随机对照试验（RCT）对于确定医疗人工智能（AI）工具的临床价值是不可或缺的，但随着新模型的迅速出现，其成本高且时间长，阻碍了及时验证。在这里，我们提出了 BRIDGE，这是一种针对基于人工智能的风险模型的数据重用 RCT 设计。人工智能风险模型支持广泛的干预措施，包括筛查、治疗选择和临床警报。当旧模型和更新模型做出一致预测时，BRIDGE 试验会从已完成的 AI 模型试验中回收参与者级别的数据，从而减少后续试验的注册要求。我们为研究人员提供了一份实用的清单，以评估重复使用先前试验的数据是否可以进行有效的因果推断并保留 I 类错误。使用乳腺癌、心血管疾病和脓毒症的真实数据集，我们证明了连续 AI 模型之间的一致性，前 5% 的高风险队列中重叠率高达 64.8%。然后，我们模拟了一系列乳腺癌筛查研究，我们的设计将所需的入组人数减少了 46.6%，节省了超过 280 万美元，同时保持了 80% 的功效。通过将试验转变为适应性、模块化研究，我们提出的设计使每次模型迭代都可以生成 I 级证据，从而加速将人工智能经济有效地转化为日常护理。</li>
</ul>

<h3>Title: PAN: A World Model for General, Interactable, and Long-Horizon World Simulation</h3>
<ul>
<li><strong>Authors: </strong>PAN Team Institute of Foundation Models: Jiannan Xiang, Yi Gu, Zihan Liu, Zeyu Feng, Qiyue Gao, Yiyan Hu, Benhao Huang, Guangyi Liu, Yichi Yang, Kun Zhou, Davit Abrahamyan, Arif Ahmad, Ganesh Bannur, Junrong Chen, Kimi Chen, Mingkai Deng, Ruobing Han, Xinqi Huang, Haoqiang Kang, Zheqi Li, Enze Ma, Hector Ren, Yashowardhan Shinde, Rohan Shingre, Ramsundar Tanikella, Kaiming Tao, Dequan Yang, Xinle Yu, Cong Zeng, Binglin Zhou, Hector Liu, Zhiting Hu, Eric P. Xing</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09057">https://arxiv.org/abs/2511.09057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09057">https://arxiv.org/pdf/2511.09057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09057]] PAN: A World Model for General, Interactable, and Long-Horizon World Simulation(https://arxiv.org/abs/2511.09057)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>A world model enables an intelligent agent to imagine, predict, and reason about how the world evolves in response to its actions, and accordingly to plan and strategize. While recent video generation models produce realistic visual sequences, they typically operate in the prompt-to-full-video manner without causal control, interactivity, or long-horizon consistency required for purposeful reasoning. Existing world modeling efforts, on the other hand, often focus on restricted domains (e.g., physical, game, or 3D-scene dynamics) with limited depth and controllability, and struggle to generalize across diverse environments and interaction formats. In this work, we introduce PAN, a general, interactable, and long-horizon world model that predicts future world states through high-quality video simulation conditioned on history and natural language actions. PAN employs the Generative Latent Prediction (GLP) architecture that combines an autoregressive latent dynamics backbone based on a large language model (LLM), which grounds simulation in extensive text-based knowledge and enables conditioning on language-specified actions, with a video diffusion decoder that reconstructs perceptually detailed and temporally coherent visual observations, to achieve a unification between latent space reasoning (imagination) and realizable world dynamics (reality). Trained on large-scale video-action pairs spanning diverse domains, PAN supports open-domain, action-conditioned simulation with coherent, long-term dynamics. Extensive experiments show that PAN achieves strong performance in action-conditioned world simulation, long-horizon forecasting, and simulative reasoning compared to other video generators and world models, taking a step towards general world models that enable predictive simulation of future world states for reasoning and acting.</li>
<li><strong>摘要：</strong>世界模型使智能代理能够想象、预测和推理世界如何响应其行为而演变，并相应地制定计划和战略。虽然最近的视频生成模型产生逼真的视觉序列，但它们通常以从提示到完整视频的方式运行，没有因果控制、交互性或有目的推理所需的长期一致性。另一方面，现有的世界建模工作通常侧重于深度和可控性有限的受限领域（例如物理、游戏或 3D 场景动态），并且很难在不同的环境和交互格式中进行泛化。在这项工作中，我们介绍了 PAN，这是一种通用的、可交互的、长视野的世界模型，它通过以历史和自然语言动作为条件的高质量视频模拟来预测未来的世界状态。 PAN 采用​​生成潜在预测 (GLP) 架构，该架构结合了基于大语言模型 (LLM) 的自回归潜在动态主干，该模型以广泛的基于文本的知识为基础进行模拟，并能够对语言指定的动作进行调节，与视频扩散解码器一起重建感知细节和时间连贯的视觉观察，以实现潜在空间推理（想象）和可实现的世界动态（现实）之间的统一。 PAN 经过跨不同领域的大规模视频动作对的训练，支持具有连贯、长期动态的开放域、动作条件模拟。大量实验表明，与其他视频生成器和世界模型相比，PAN 在动作条件世界模拟、长视野预测和模拟推理方面取得了强大的性能，向通用世界模型迈出了一步，能够对未来世界状态进行预测模拟以进行推理和行动。</li>
</ul>

<h3>Title: VietMEAgent: Culturally-Aware Few-Shot Multimodal Explanation for Vietnamese Visual Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Hai-Dang Nguyen, Minh-Anh Dang, Minh-Tan Le, Minh-Tuan Le</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09058">https://arxiv.org/abs/2511.09058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09058">https://arxiv.org/pdf/2511.09058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09058]] VietMEAgent: Culturally-Aware Few-Shot Multimodal Explanation for Vietnamese Visual Question Answering(https://arxiv.org/abs/2511.09058)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Contemporary Visual Question Answering (VQA) systems remain constrained when confronted with culturally specific content, largely because cultural knowledge is under-represented in training corpora and the reasoning process is not rendered interpretable to end users. This paper introduces VietMEAgent, a multimodal explainable framework engineered for Vietnamese cultural understanding. The method integrates a cultural object detection backbone with a structured program generation layer, yielding a pipeline in which answer prediction and explanation are tightly coupled. A curated knowledge base of Vietnamese cultural entities serves as an explicit source of background information, while a dual-modality explanation module combines attention-based visual evidence with structured, human-readable textual rationales. We further construct a Vietnamese Cultural VQA dataset sourced from public repositories and use it to demonstrate the practicality of programming-based methodologies for cultural AI. The resulting system provides transparent explanations that disclose both the computational rationale and the underlying cultural context, supporting education and cultural preservation with an emphasis on interpretability and cultural sensitivity.</li>
<li><strong>摘要：</strong>当代视觉问答（VQA）系统在面对特定于文化的内容时仍然受到限制，这主要是因为文化知识在训练语料库中代表性不足，并且推理过程无法向最终用户解释。本文介绍了 VietMEAgent，这是一个为理解越南文化而设计的多模式可解释框架。该方法将文化对象检测主干与结构化程序生成层集成，产生答案预测和解释紧密耦合的管道。越南文化实体的精选知识库作为背景信息的明确来源，而双模态解释模块将基于注意力的视觉证据与结构化的、人类可读的文本原理结合起来。我们进一步构建了一个来自公共存储库的越南文化 VQA 数据集，并用它来证明基于编程的文化人工智能方法的实用性。由此产生的系统提供了透明的解释，揭示了计算原理和潜在的文化背景，支持教育和文化保护，重点是可解释性和文化敏感性。</li>
</ul>

<h3>Title: Human-Corrected Labels Learning: Enhancing Labels Quality via Human Correction of VLMs Discrepancies</h3>
<ul>
<li><strong>Authors: </strong>Zhongnian Li, Lan Chen, Yixin Xu, Shi Xu, Xinzheng Xu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09063">https://arxiv.org/abs/2511.09063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09063">https://arxiv.org/pdf/2511.09063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09063]] Human-Corrected Labels Learning: Enhancing Labels Quality via Human Correction of VLMs Discrepancies(https://arxiv.org/abs/2511.09063)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs), with their powerful content generation capabilities, have been successfully applied to data annotation processes. However, the VLM-generated labels exhibit dual limitations: low quality (i.e., label noise) and absence of error correction mechanisms. To enhance label quality, we propose Human-Corrected Labels (HCLs), a novel setting that efficient human correction for VLM-generated noisy labels. As shown in Figure 1(b), HCL strategically deploys human correction only for instances with VLM discrepancies, achieving both higher-quality annotations and reduced labor costs. Specifically, we theoretically derive a risk-consistent estimator that incorporates both human-corrected labels and VLM predictions to train classifiers. Besides, we further propose a conditional probability method to estimate the label distribution using a combination of VLM outputs and model predictions. Extensive experiments demonstrate that our approach achieves superior classification performance and is robust to label noise, validating the effectiveness of HCL in practical weak supervision scenarios. Code this https URL</li>
<li><strong>摘要：</strong>视觉语言模型（VLM）以其强大的内容生成能力，已成功应用于数据注释过程。然而，VLM 生成的标签具有双重局限性：质量低（即标签噪声）和缺乏纠错机制。为了提高标签质量，我们提出了人工校正标签（HCL），这是一种新颖的设置，可以对 VLM 生成的噪声标签进行有效的人工校正。如图 1(b) 所示，HCL 策略性地仅针对存在 VLM 差异的实例部署人工校正，从而实现更高质量的注释并降低劳动力成本。具体来说，我们从理论上推导出一个风险一致的估计器，该估计器结合了人工校正的标签和 VLM 预测来训练分类器。此外，我们还提出了一种条件概率方法，结合 VLM 输出和模型预测来估计标签分布。大量实验表明，我们的方法实现了卓越的分类性能，并且对标签噪声具有鲁棒性，验证了 HCL 在实际弱监督场景中的有效性。编码此 https URL</li>
</ul>

<h3>Title: DKDS: A Benchmark Dataset of Degraded Kuzushiji Documents with Seals for Detection and Binarization</h3>
<ul>
<li><strong>Authors: </strong>Rui-Yang Ju, Kohei Yamashita, Hirotaka Kameko, Shinsuke Mori</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09117">https://arxiv.org/abs/2511.09117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09117">https://arxiv.org/pdf/2511.09117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09117]] DKDS: A Benchmark Dataset of Degraded Kuzushiji Documents with Seals for Detection and Binarization(https://arxiv.org/abs/2511.09117)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Kuzushiji, a pre-modern Japanese cursive script, can currently be read and understood by only a few thousand trained experts in Japan. With the rapid development of deep learning, researchers have begun applying Optical Character Recognition (OCR) techniques to transcribe Kuzushiji into modern Japanese. Although existing OCR methods perform well on clean pre-modern Japanese documents written in Kuzushiji, they often fail to consider various types of noise, such as document degradation and seals, which significantly affect recognition accuracy. To the best of our knowledge, no existing dataset specifically addresses these challenges. To address this gap, we introduce the Degraded Kuzushiji Documents with Seals (DKDS) dataset as a new benchmark for related tasks. We describe the dataset construction process, which required the assistance of a trained Kuzushiji expert, and define two benchmark tracks: (1) text and seal detection and (2) document binarization. For the text and seal detection track, we provide baseline results using multiple versions of the You Only Look Once (YOLO) models for detecting Kuzushiji characters and seals. For the document binarization track, we present baseline results from traditional binarization algorithms, traditional algorithms combined with K-means clustering, and Generative Adversarial Network (GAN)-based methods. The DKDS dataset and the implementation code for baseline methods are available at this https URL.</li>
<li><strong>摘要：</strong>草书是一种前现代的日本草书文字，目前日本只有几千名受过训练的专家可以阅读和理解。随着深度学习的快速发展，研究人员开始应用光学字符识别（OCR）技术将“Kuzushiji”转录成现代日语。尽管现有的 OCR 方法在用 Kuzushiji 编写的干净的前现代日语文档上表现良好，但它们通常无法考虑各种类型的噪声，例如文档退化和密封，这些噪声会严重影响识别准确性。据我们所知，没有现有的数据集专门解决这些挑战。为了解决这一差距，我们引入了 Degraded Kuzushiji Documents with Seals (DKDS) 数据集作为相关任务的新基准。我们描述了数据集构建过程，该过程需要训练有素的 Kuzushiji 专家的帮助，并定义了两个基准轨道：（1）文本和印章检测以及（2）文档二值化。对于文本和印章检测轨迹，我们使用多个版本的 You Only Look Once (YOLO) 模型来提供基线结果来检测 Kuzushiji 字符和印章。对于文档二值化轨道，我们提供了传统二值化算法、传统算法与 K 均值聚类相结合以及基于生成对抗网络 (GAN) 的方法的基线结果。 DKDS 数据集和基线方法的实现代码可从此 https URL 获取。</li>
</ul>

<h3>Title: PIFF: A Physics-Informed Generative Flow Model for Real-Time Flood Depth Mapping</h3>
<ul>
<li><strong>Authors: </strong>ChunLiang Wu, Tsunhua Yang, Hungying Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09130">https://arxiv.org/abs/2511.09130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09130">https://arxiv.org/pdf/2511.09130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09130]] PIFF: A Physics-Informed Generative Flow Model for Real-Time Flood Depth Mapping(https://arxiv.org/abs/2511.09130)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Flood mapping is crucial for assessing and mitigating flood impacts, yet traditional methods like numerical modeling and aerial photography face limitations in efficiency and reliability. To address these challenges, we propose PIFF, a physics-informed, flow-based generative neural network for near real-time flood depth estimation. Built on an image-to-image generative framework, it efficiently maps Digital Elevation Models (DEM) to flood depth predictions. The model is conditioned on a simplified inundation model (SPM) that embeds hydrodynamic priors into the training process. Additionally, a transformer-based rainfall encoder captures temporal dependencies in precipitation. Integrating physics-informed constraints with data-driven learning, PIFF captures the causal relationships between rainfall, topography, SPM, and flooding, replacing costly simulations with accurate, real-time flood maps. Using a 26 km study area in Tainan, Taiwan, with 182 rainfall scenarios ranging from 24 mm to 720 mm over 24 hours, our results demonstrate that PIFF offers an effective, data-driven alternative for flood prediction and response.</li>
<li><strong>摘要：</strong>洪水测绘对于评估和减轻洪水影响至关重要，但数值建模和航空摄影等传统方法在效率和可靠性方面面临限制。为了应对这些挑战，我们提出了 PIFF，一种基于物理的、基于流的生成神经网络，用于近实时洪水深度估计。它建立在图像到图像生成框架的基础上，可有效地将数字高程模型 (DEM) 映射到洪水深度预测。该模型以简化的淹没模型 (SPM) 为条件，该模型将水动力先验嵌入到训练过程中。此外，基于变压器的降雨编码器捕获降水的时间依赖性。 PIFF 将物理约束与数据驱动学习相结合，捕获降雨、地形、SPM 和洪水之间的因果关系，用准确、实时的洪水地图取代昂贵的模拟。我们使用台湾台南 26 公里的研究区域，在 24 小时内有 182 个降雨量范围从 24 毫米到 720 毫米的降雨情景，结果表明 PIFF 为洪水预测和响应提供了一种有效的、数据驱动的替代方案。</li>
</ul>

<h3>Title: Trusted Multi-view Learning for Long-tailed Classification</h3>
<ul>
<li><strong>Authors: </strong>Chuanqing Tang, Yifei Shi, Guanghao Lin, Lei Xing, Long Shi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09138">https://arxiv.org/abs/2511.09138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09138">https://arxiv.org/pdf/2511.09138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09138]] Trusted Multi-view Learning for Long-tailed Classification(https://arxiv.org/abs/2511.09138)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Class imbalance has been extensively studied in single-view scenarios; however, addressing this challenge in multi-view contexts remains an open problem, with even scarcer research focusing on trustworthy solutions. In this paper, we tackle a particularly challenging class imbalance problem in multi-view scenarios: long-tailed classification. We propose TMLC, a Trusted Multi-view Long-tailed Classification framework, which makes contributions on two critical aspects: opinion aggregation and pseudo-data generation. Specifically, inspired by Social Identity Theory, we design a group consensus opinion aggregation mechanism that guides decision making toward the direction favored by the majority of the group. In terms of pseudo-data generation, we introduce a novel distance metric to adapt SMOTE for multi-view scenarios and develop an uncertainty-guided data generation module that produces high-quality pseudo-data, effectively mitigating the adverse effects of class imbalance. Extensive experiments on long-tailed multi-view datasets demonstrate that our model is capable of achieving superior performance. The code is released at this https URL.</li>
<li><strong>摘要：</strong>类不平衡在单视图场景中得到了广泛的研究；然而，在多视角背景下应对这一挑战仍然是一个悬而未决的问题，关注可靠解决方案的研究甚至更少。在本文中，我们解决了多视图场景中一个特别具有挑战性的类不平衡问题：长尾分类。我们提出了 TMLC，一种可信多视图长尾分类框架，它在两个关键方面做出了贡献：意见聚合和伪数据生成。具体来说，受社会认同理论的启发，我们设计了一种群体共识意见聚合机制，引导决策朝着群体大多数人青睐的方向发展。在伪数据生成方面，我们引入了一种新颖的距离度量，使SMOTE适应多视图场景，并开发了一个不确定性引导的数据生成模块，可以生成高质量的伪数据，有效减轻类别不平衡的不利影响。对长尾多视图数据集的大量实验表明，我们的模型能够实现卓越的性能。代码在此 https URL 发布。</li>
</ul>

<h3>Title: MACEval: A Multi-Agent Continual Evaluation Network for Large Models</h3>
<ul>
<li><strong>Authors: </strong>Zijian Chen, Yuze Sun, Yuan Tian, Wenjun Zhang, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09139">https://arxiv.org/abs/2511.09139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09139">https://arxiv.org/pdf/2511.09139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09139]] MACEval: A Multi-Agent Continual Evaluation Network for Large Models(https://arxiv.org/abs/2511.09139)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Hundreds of benchmarks dedicated to evaluating large models from multiple perspectives have been presented over the past few years. Albeit substantial efforts, most of them remain closed-ended and are prone to overfitting due to the potential data contamination in the ever-growing training corpus of large models, thereby undermining the credibility of the evaluation. Moreover, the increasing scale and scope of current benchmarks with transient metrics, as well as the heavily human-dependent curation procedure, pose significant challenges for timely maintenance and adaptation to gauge the advancing capabilities of large models. In this paper, we introduce MACEval, a \Multi-Agent Continual Evaluation network for dynamic evaluation of large models, and define a new set of metrics to quantify performance longitudinally and sustainably. MACEval adopts an interactive and autonomous evaluation mode that employs role assignment, in-process data generation, and evaluation routing through a cascaded agent network. Extensive experiments on 9 open-ended tasks with 23 participating large models demonstrate that MACEval is (1) human-free and automatic, mitigating laborious result processing with inter-agent judgment guided; (2) efficient and economical, reducing a considerable amount of data and overhead to obtain similar results compared to related benchmarks; and (3) flexible and scalable, migrating or integrating existing benchmarks via customized evaluation topologies. We hope that MACEval can broaden future directions of large model evaluation.</li>
<li><strong>摘要：</strong>在过去的几年里，已经出现了数百个致力于从多个角度评估大型模型的基准。尽管付出了巨大的努力，但大多数方法仍然是封闭式的，并且由于大型模型不断增长的训练语料库中潜在的数据污染，很容易出现过度拟合，从而损害了评估的可信度。此外，当前瞬态指标基准的规模和范围不断扩大，以及严重依赖人类的管理程序，对及时维护和适应以衡量大型模型的先进能力提出了重大挑战。在本文中，我们介绍了 MACEval，一个用于动态评估大型模型的多智能体持续评估网络，并定义了一组新的指标来纵向和可持续地量化性能。 MACEval 采用交互式自主评估模式，通过级联代理网络采用角色分配、过程中数据生成和评估路由。对 23 个参与的大型模型的 9 项开放式任务进行的广泛实验表明，MACEval 是（1）无人且自动的，通过代理间判断引导减轻费力的结果处理； （2）高效且经济，与相关基准相比，减少大量数据和开销以获得相似结果； and (3) flexible and scalable, migrating or integrating existing benchmarks via customized evaluation topologies.我们希望 MACEval 能够拓宽大型模型评估的未来方向。</li>
</ul>

<h3>Title: Sure! Here's a short and concise title for your paper: "Contamination in Generated Text Detection Benchmarks"</h3>
<ul>
<li><strong>Authors: </strong>Philipp Dingfelder, Christian Riess</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09200">https://arxiv.org/abs/2511.09200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09200">https://arxiv.org/pdf/2511.09200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09200]] Sure! Here's a short and concise title for your paper: "Contamination in Generated Text Detection Benchmarks"(https://arxiv.org/abs/2511.09200)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models are increasingly used for many applications. To prevent illicit use, it is desirable to be able to detect AI-generated text. Training and evaluation of such detectors critically depend on suitable benchmark datasets. Several groups took on the tedious work of collecting, curating, and publishing large and diverse datasets for this task. However, it remains an open challenge to ensure high quality in all relevant aspects of such a dataset. For example, the DetectRL benchmark exhibits relatively simple patterns of AI-generation in 98.5% of the Claude-LLM data. These patterns may include introductory words such as "Sure! Here is the academic article abstract:", or instances where the LLM rejects the prompted task. In this work, we demonstrate that detectors trained on such data use such patterns as shortcuts, which facilitates spoofing attacks on the trained detectors. We consequently reprocessed the DetectRL dataset with several cleansing operations. Experiments show that such data cleansing makes direct attacks more difficult. The reprocessed dataset is publicly available.</li>
<li><strong>摘要：</strong>大型语言模型越来越多地用于许多应用程序。为了防止非法使用，希望能够检测人工智能生成的文本。此类检测器的训练和评估关键取决于合适的基准数据集。几个小组承担了为此任务收集、整理和发布大型且多样化的数据集的繁琐工作。然而，确保此类数据集所有相关方面的高质量仍然是一个公开的挑战。例如，DetectRL 基准测试在 98.5% 的 Claude-LLM 数据中展示了相对简单的 AI 生成模式。 These patterns may include introductory words such as "Sure! Here is the academic article abstract:", or instances where the LLM rejects the prompted task.在这项工作中，我们证明了接受此类数据训练的检测器使用此类模式作为快捷方式，这有助于对经过训练的检测器进行欺骗攻击。因此，我们通过多次清理操作重新处理了 DetectRL 数据集。实验表明，这种数据清理使得直接攻击变得更加困难。重新处理的数据集是公开可用的。</li>
</ul>

<h3>Title: Controllable protein design through Feynman-Kac steering</h3>
<ul>
<li><strong>Authors: </strong>Erik Hartman, Jonas Wallin, Johan Malmström, Jimmy Olsson</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09216">https://arxiv.org/abs/2511.09216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09216">https://arxiv.org/pdf/2511.09216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09216]] Controllable protein design through Feynman-Kac steering(https://arxiv.org/abs/2511.09216)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion-based models have recently enabled the generation of realistic and diverse protein structures, yet they remain limited in their ability to steer outcomes toward specific functional or biochemical objectives, such as binding affinity or sequence composition. Here we extend the Feynman-Kac (FK) steering framework, an inference-time control approach, to diffusion-based protein design. By coupling FK steering with structure generation, the method guides sampling toward desirable structural or energetic features while maintaining the diversity of the underlying diffusion process. To enable simultaneous generation of both sequence and structure properties, rewards are computed on models refined through ProteinMPNN and all-atom relaxation. Applied to binder design, FK steering consistently improves predicted interface energetics across diverse targets with minimal computational overhead. More broadly, this work demonstrates that inference-time FK control generalizes diffusion-based protein design to arbitrary, non-differentiable, and reward-agnostic objectives, providing a unified and model-independent framework for guided molecular generation.</li>
<li><strong>摘要：</strong>基于扩散的模型最近已经能够生成真实且多样化的蛋白质结构，但它们将结果导向特定功能或生化目标（例如结合亲和力或序列组成）的能力仍然有限。在这里，我们将 Feynman-Kac (FK) 控制框架（一种推理时间控制方法）扩展到基于扩散的蛋白质设计。通过将 FK 控制与结构生成相结合，该方法引导采样获得所需的结构或能量特征，同时保持底层扩散过程的多样性。为了能够同时生成序列和结构属性，奖励是在通过 ProteinMPNN 和全原子松弛精炼的模型上计算的。 FK 转向应用于粘合剂设计，以最小的计算开销持续改进不同目标的预测界面能量。更广泛地说，这项工作表明，推理时间 FK 控制将基于扩散的蛋白质设计推广到任意、不可微分和奖励不可知的目标，为引导分子生成提供了统一且独立于模型的框架。</li>
</ul>

<h3>Title: GRACE: Designing Generative Face Video Codec via Agile Hardware-Centric Workflow</h3>
<ul>
<li><strong>Authors: </strong>Rui Wan, Qi Zheng, Ruoyu Zhang, Bu Chen, Jiaming Liu, Min Li, Minge Jing, Jinjia Zhou, Yibo Fan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09272">https://arxiv.org/abs/2511.09272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09272">https://arxiv.org/pdf/2511.09272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09272]] GRACE: Designing Generative Face Video Codec via Agile Hardware-Centric Workflow(https://arxiv.org/abs/2511.09272)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The Animation-based Generative Codec (AGC) is an emerging paradigm for talking-face video compression. However, deploying its intricate decoder on resource and power-constrained edge devices presents challenges due to numerous parameters, the inflexibility to adapt to dynamically evolving algorithms, and the high power consumption induced by extensive computations and data transmission. This paper for the first time proposes a novel field programmable gate arrays (FPGAs)-oriented AGC deployment scheme for edge-computing video services. Initially, we analyze the AGC algorithm and employ network compression methods including post-training static quantization and layer fusion techniques. Subsequently, we design an overlapped accelerator utilizing the co-processor paradigm to perform computations through software-hardware co-design. The hardware processing unit comprises engines such as convolution, grid sampling, upsample, etc. Parallelization optimization strategies like double-buffered pipelines and loop unrolling are employed to fully exploit the resources of FPGA. Ultimately, we establish an AGC FPGA prototype on the PYNQ-Z1 platform using the proposed scheme, achieving \textbf{24.9$\times$} and \textbf{4.1$\times$} higher energy efficiency against commercial Central Processing Unit (CPU) and Graphic Processing Unit (GPU), respectively. Specifically, only \textbf{11.7} microjoules ($\upmu$J) are required for one pixel reconstructed by this FPGA system.</li>
<li><strong>摘要：</strong>基于动画的生成编解码器 (AGC) 是一种新兴的人脸视频压缩范例。然而，由于参数众多、适应动态演进算法的不灵活性以及大量计算和数据传输引起的高功耗，在资源和功耗受限的边缘设备上部署其复杂的解码器面临着挑战。本文首次提出了一种面向边缘计算视频服务的面向现场可编程门阵列（FPGA）的新型AGC部署方案。首先，我们分析 AGC 算法并采用网络压缩方法，包括训练后静态量化和层融合技术。随后，我们设计了一个利用协处理器范例的重叠加速器，通过软硬件协同设计来执行计算。硬件处理单元包括卷积、网格采样、上采样等引擎，采用双缓冲流水线、循环展开等并行化优化策略，充分利用FPGA资源。最终，我们使用所提出的方案在 PYNQ-Z1 平台上建立了 AGC FPGA 原型，与商用中央处理单元（CPU）和图形处理单元（GPU）相比，分别实现了 \textbf{24.9$\times$} 和 \textbf{4.1$\times$} 更高的能效。具体来说，该 FPGA 系统重建一个像素仅需要 \textbf{11.7} 微焦耳 ($\upmu$J)。</li>
</ul>

<h3>Title: DensiCrafter: Physically-Constrained Generation and Fabrication of Self-Supporting Hollow Structures</h3>
<ul>
<li><strong>Authors: </strong>Shengqi Dang, Fu Chai, Jiaxin Li, Chao Yuan, Wei Ye, Nan Cao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09298">https://arxiv.org/abs/2511.09298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09298">https://arxiv.org/pdf/2511.09298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09298]] DensiCrafter: Physically-Constrained Generation and Fabrication of Self-Supporting Hollow Structures(https://arxiv.org/abs/2511.09298)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The rise of 3D generative models has enabled automatic 3D geometry and texture synthesis from multimodal inputs (e.g., text or images). However, these methods often ignore physical constraints and manufacturability considerations. In this work, we address the challenge of producing 3D designs that are both lightweight and self-supporting. We present DensiCrafter, a framework for generating lightweight, self-supporting 3D hollow structures by optimizing the density field. Starting from coarse voxel grids produced by Trellis, we interpret these as continuous density fields to optimize and introduce three differentiable, physically constrained, and simulation-free loss terms. Additionally, a mass regularization penalizes unnecessary material, while a restricted optimization domain preserves the outer surface. Our method seamlessly integrates with pretrained Trellis-based models (e.g., Trellis, DSO) without any architectural changes. In extensive evaluations, we achieve up to 43% reduction in material mass on the text-to-3D task. Compared to state-of-the-art baselines, our method could improve the stability and maintain high geometric fidelity. Real-world 3D-printing experiments confirm that our hollow designs can be reliably fabricated and could be self-supporting.</li>
<li><strong>摘要：</strong>The rise of 3D generative models has enabled automatic 3D geometry and texture synthesis from multimodal inputs (e.g., text or images).然而，这些方法常常忽略物理约束和可制造性考虑。在这项工作中，我们解决了制作轻量且自支撑的 3D 设计的挑战。我们推出 DensiCrafter，这是一个通过优化密度场生成轻质、自支撑 3D 中空结构的框架。从 Trellis 生成的粗体素网格开始，我们将它们解释为连续密度场，以优化并引入三个可微分、物理约束和无模拟的损失项。此外，质量正则化会惩罚不必要的材料，而受限的优化域会保留外表面。我们的方法与预训练的基于 Trellis 的模型（例如 Trellis、DSO）无缝集成，无需任何架构更改。在广泛的评估中，我们在文本转 3D 任务中实现了高达 43% 的材料质量减少。与最先进的基线相比，我们的方法可以提高稳定性并保持高几何保真度。现实世界的 3D 打印实验证实，我们的中空设计可以可靠地制造并且可以自支撑。</li>
</ul>

<h3>Title: Diffusion-based Sinogram Interpolation for Limited Angle PET</h3>
<ul>
<li><strong>Authors: </strong>Rüveyda Yilmaz, Julian Thull, Johannes Stegmaier, Volkmar Schulz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09383">https://arxiv.org/abs/2511.09383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09383">https://arxiv.org/pdf/2511.09383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09383]] Diffusion-based Sinogram Interpolation for Limited Angle PET(https://arxiv.org/abs/2511.09383)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Accurate PET imaging increasingly requires methods that support unconstrained detector layouts from walk-through designs to long-axial rings where gaps and open sides lead to severely undersampled sinograms. Instead of constraining the hardware to form complete cylinders, we propose treating the missing lines-of-responses as a learnable prior. Data-driven approaches, particularly generative models, offer a promising pathway to recover this missing information. In this work, we explore the use of conditional diffusion models to interpolate sparsely sampled sinograms, paving the way for novel, cost-efficient, and patient-friendly PET geometries in real clinical settings.</li>
<li><strong>摘要：</strong>精确的 PET 成像越来越需要支持不受约束的探测器布局的方法，从漫游设计到长轴环，其中间隙和开放侧会导致正弦图严重采样不足。我们建议将缺失的响应线视为可学习的先验，而不是限制硬件形成完整的圆柱体。数据驱动的方法，特别是生成模型，为恢复这些丢失的信息提供了一条有前途的途径。在这项工作中，我们探索使用条件扩散模型来插值稀疏采样的正弦图，为真实临床环境中新颖、经济高效且患者友好的 PET 几何形状铺平道路。</li>
</ul>

<h3>Title: AutoSynth: Automated Workflow Optimization for High-Quality Synthetic Dataset Generation via Monte Carlo Tree Search</h3>
<ul>
<li><strong>Authors: </strong>Shuzhen Bi, Chang Song, Siyu Song, Jinze Lv, Jian Chen, Xinyun Wang, Aimin Zhou, Hao Hao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09488">https://arxiv.org/abs/2511.09488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09488">https://arxiv.org/pdf/2511.09488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09488]] AutoSynth: Automated Workflow Optimization for High-Quality Synthetic Dataset Generation via Monte Carlo Tree Search(https://arxiv.org/abs/2511.09488)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Supervised fine-tuning (SFT) of large language models (LLMs) for specialized tasks requires high-quality datasets, but manual curation is prohibitively expensive. Synthetic data generation offers scalability, but its effectiveness relies on complex, multi-stage workflows, integrating prompt engineering and model orchestration. Existing automated workflow methods face a cold start problem: they require labeled datasets for reward modeling, which is especially problematic for subjective, open-ended tasks with no objective ground truth. We introduce AutoSynth, a framework that automates workflow discovery and optimization without reference datasets by reframing the problem as a Monte Carlo Tree Search guided by a novel dataset-free hybrid reward. This reward enables meta-learning through two LLM-as-judge components: one evaluates sample quality using dynamically generated task-specific metrics, and another assesses workflow code and prompt quality. Experiments on subjective educational tasks show that while expert-designed workflows achieve higher human preference rates (96-99% win rates vs. AutoSynth's 40-51%), models trained on AutoSynth-generated data dramatically outperform baselines (40-51% vs. 2-5%) and match or surpass expert workflows on certain metrics, suggesting discovery of quality dimensions beyond human intuition. These results are achieved while reducing human effort from 5-7 hours to just 30 minutes (>90% reduction). AutoSynth tackles the cold start issue in data-centric AI, offering a scalable, cost-effective method for subjective LLM tasks. Code: this https URL.</li>
<li><strong>摘要：</strong>针对专门任务的大型语言模型 (LLM) 的监督微调 (SFT) 需要高质量的数据集，但手动管理成本高昂。合成数据生成提供了可扩展性，但其有效性依赖于复杂的多阶段工作流程，集成了即时工程和模型编排。现有的自动化工作流程方法面临着冷启动问题：它们需要标记数据集来进行奖励建模，这对于没有客观事实的主观、开放式任务来说尤其成问题。我们引入了 AutoSynth，这是一个框架，通过将问题重新定义为由新颖的无数据集混合奖励引导的蒙特卡罗树搜索，可以在没有参考数据集的情况下自动执行工作流发现和优化。该奖励通过两个 LLM 作为评判组件实现元学习：一个使用动态生成的特定于任务的指标评估样本质量，另一个评估工作流程代码和提示质量。主观教育任务的实验表明，虽然专家设计的工作流程实现了更高的人类偏好率（96-99% 的胜率 vs. AutoSynth 的 40-51%），但在 AutoSynth 生成的数据上训练的模型显着优于基线（40-51% vs. 2-5%），并且在某些指标上匹配或超过了专家工作流程，这表明发现了超出人类直觉的质量维度。取得这些成果的同时，将人力从 5-7 小时缩短至仅 30 分钟（减少了 90% 以上）。 AutoSynth 解决了以数据为中心的 AI 中的冷启动问题，为主观 LLM 任务提供了一种可扩展、经济高效的方法。代码：此 https URL。</li>
</ul>

<h3>Title: SiDGen: Structure-informed Diffusion for Generative modeling of Ligands for Proteins</h3>
<ul>
<li><strong>Authors: </strong>Samyak Sanghvi, Nishant Ranjan, Tarak Karmakar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09529">https://arxiv.org/abs/2511.09529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09529">https://arxiv.org/pdf/2511.09529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09529]] SiDGen: Structure-informed Diffusion for Generative modeling of Ligands for Proteins(https://arxiv.org/abs/2511.09529)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Designing ligands that are both chemically valid and structurally compatible with protein binding pockets is a key bottleneck in computational drug discovery. Existing approaches either ignore structural context or rely on expensive, memory-intensive encoding that limits throughput and scalability. We present SiDGen (Structure-informed Diffusion Generator), a protein-conditioned diffusion framework that integrates masked SMILES generation with lightweight folding-derived features for pocket awareness. To balance expressivity with efficiency, SiDGen supports two conditioning pathways: a streamlined mode that pools coarse structural signals from protein embeddings and a full mode that injects localized pairwise biases for stronger coupling. A coarse-stride folding mechanism with nearest-neighbor upsampling alleviates the quadratic memory costs of pair tensors, enabling training on realistic sequence lengths. Learning stability is maintained through in-loop chemical validity checks and an invalidity penalty, while large-scale training efficiency is restored \textit{via} selective compilation, dataloader tuning, and gradient accumulation. In automated benchmarks, SiDGen generates ligands with high validity, uniqueness, and novelty, while achieving competitive performance in docking-based evaluations and maintaining reasonable molecular properties. These results demonstrate that SiDGen can deliver scalable, pocket-aware molecular design, providing a practical route to conditional generation for high-throughput drug discovery.</li>
<li><strong>摘要：</strong>设计化学上有效且结构上与蛋白质结合袋相容的配体是计算药物发现的关键瓶颈。现有方法要么忽略结构上下文，要么依赖昂贵的内存密集型编码，这限制了吞吐量和可扩展性。我们提出了 SiDGen（结构信息扩散生成器），这是一种蛋白质条件扩散框架，它将掩蔽 SMILES 生成与轻量级折叠衍生功能集成在一起，以实现口袋感知。为了平衡表达性和效率，SiDGen 支持两种调节途径：一种是汇集来自蛋白质嵌入的粗略结构信号的简化模式，另一种是注入局部成对偏差以实现更强耦合的完整模式。具有最近邻上采样的粗步折叠机制减轻了张量对的二次内存成本，从而能够对实际序列长度进行训练。通过循环化学有效性检查和无效性惩罚来维持学习稳定性，同时通过选择性编译、数据加载器调整和梯度积累来恢复大规模训练效率。在自动化基准测试中，SiDGen 生成具有高有效性、独特性和新颖性的配体，同时在基于对接的评估中实现具有竞争力的性能并保持合理的分子特性。这些结果表明，SiDGen 可以提供可扩展、口袋感知的分子设计，为高通量药物发现的条件生成提供实用途径。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
