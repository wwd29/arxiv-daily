<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-12-29</h1>
<h3>Title: A Reinforcement Learning Approach to Synthetic Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Natalia Espinosa-Dice, Nicholas J. Jackson, Chao Yan, Aaron Lee, Bradley A. Malin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21395">https://arxiv.org/abs/2512.21395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21395">https://arxiv.org/pdf/2512.21395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21395]] A Reinforcement Learning Approach to Synthetic Data Generation(https://arxiv.org/abs/2512.21395)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Synthetic data generation (SDG) is a promising approach for enabling data sharing in biomedical studies while preserving patient privacy. Yet, state-of-the-art generative models often require large datasets and complex training procedures, limiting their applicability in small-sample settings. In this work, we reframe SDG as a reinforcement learning (RL) problem and introduce RLSyn, a novel framework that models the data generator as a stochastic policy over patient records and optimizes it using Proximal Policy Optimization with discriminator-derived rewards, yielding more stable and data-efficient training. We evaluate RLSyn on two biomedical datasets - AI-READI and MIMIC-IV- and benchmark it against state-of-the-art generative adversarial networks (GANs) and diffusion-based methods across extensive privacy, utility, and fidelity evaluations. RL-Syn performs comparably to diffusion models and outperforms GANs on MIMIC-IV, while outperforming both diffusion models and GANs on the smaller AI-READI dataset. These results demonstrate that reinforcement learning provides a principled and effective alternative for synthetic biomedical data generation, particularly in data-scarce regimes.</li>
<li><strong>摘要：</strong>合成数据生成（SDG）是一种很有前途的方法，可以在生物医学研究中实现数据共享，同时保护患者隐私。然而，最先进的生成模型通常需要大型数据集和复杂的训练过程，限制了它们在小样本环境中的适用性。在这项工作中，我们将 SDG 重新定义为强化学习 (RL) 问题，并引入 RLSyn，这是一种新颖的框架，它将数据生成器建模为患者记录的随机策略，并使用近端策略优化和判别器衍生的奖励对其进行优化，从而产生更稳定和数据效率更高的训练。我们在两个生物医学数据集 AI-READI 和 MIMIC-IV 上评估 RLSyn，并将其与最先进的生成对抗网络 (GAN) 和基于扩散的方法进行基准测试，涵盖广泛的隐私、效用和保真度评估。 RL-Syn 在 MIMIC-IV 上的表现与扩散模型相当，并且优于 GAN，同时在较小的 AI-READI 数据集上优于扩散模型和 GAN。这些结果表明，强化学习为合成生物医学数据生成提供了一种有原则且有效的替代方案，特别是在数据稀缺的情况下。</li>
</ul>

<h3>Title: A Survey of Freshness-Aware Wireless Networking with Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Alimu Alibotaiken, Suyang Wang, Oluwaseun T. Ajayi, Yu Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21412">https://arxiv.org/abs/2512.21412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21412">https://arxiv.org/pdf/2512.21412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21412]] A Survey of Freshness-Aware Wireless Networking with Reinforcement Learning(https://arxiv.org/abs/2512.21412)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The age of information (AoI) has become a central measure of data freshness in modern wireless systems, yet existing surveys either focus on classical AoI formulations or provide broad discussions of reinforcement learning (RL) in wireless networks without addressing freshness as a unified learning problem. Motivated by this gap, this survey examines RL specifically through the lens of AoI and generalized freshness optimization. We organize AoI and its variants into native, function-based, and application-oriented families, providing a clearer view of how freshness should be modeled in B5G and 6G systems. Building on this foundation, we introduce a policy-centric taxonomy that reflects the decisions most relevant to freshness, consisting of update-control RL, medium-access RL, risk-sensitive RL, and multi-agent RL. This structure provides a coherent framework for understanding how learning can support sampling, scheduling, trajectory planning, medium access, and distributed coordination. We further synthesize recent progress in RL-driven freshness control and highlight open challenges related to delayed decision processes, stochastic variability, and cross-layer design. The goal is to establish a unified foundation for learning-based freshness optimization in next-generation wireless networks.</li>
<li><strong>摘要：</strong>信息时代 (AoI) 已成为现代无线系统中数据新鲜度的核心衡量标准，但现有的调查要么侧重于经典的 AoI 公式，要么对无线网络中的强化学习 (RL) 进行广泛讨论，而没有将新鲜度作为一个统一的学习问题来解决。受这一差距的推动，本次调查通过 AoI 和广义新鲜度优化的视角专门研究了 RL。我们将 AoI 及其变体组织成原生的、基于功能的和面向应用的系列，为如何在 B5G 和 6G 系统中建模新鲜度提供了更清晰的视图。在此基础上，我们引入了一种以策略为中心的分类法，反映了与新鲜度最相关的决策，包括更新控制强化学习、中等访问强化学习、风险敏感强化学习和多智能体强化学习。这种结构提供了一个连贯的框架，用于理解学习如何支持采样、调度、轨迹规划、介质访问和分布式协调。我们进一步综合了强化学习驱动的新鲜度控制方面的最新进展，并强调了与延迟决策过程、随机变异性和跨层设计相关的开放挑战。目标是为下一代无线网络中基于学习的新鲜度优化建立统一的基础。</li>
</ul>

<h3>Title: dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Shirui Chen, Jiantao Jiao, Lillian J. Ratliff, Banghua Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21446">https://arxiv.org/abs/2512.21446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21446">https://arxiv.org/pdf/2512.21446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21446]] dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning(https://arxiv.org/abs/2512.21446)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Masked diffusion language models (MDLMs) offer the potential for parallel token generation, but most open-source MDLMs decode fewer than 5 tokens per model forward pass even with sophisticated sampling strategies. As a result, their sampling speeds are often comparable to AR + speculative decoding schemes, limiting their advantage over mainstream autoregressive approaches. Existing distillation-based accelerators (dParallel, d3LLM) finetune MDLMs on trajectories generated by a base model, which can become off-policy during finetuning and restrict performance to the quality of the base model's samples. We propose \texttt{dUltra}, an on-policy reinforcement learning framework based on Group Relative Policy Optimization (GRPO) that learns unmasking strategies for efficient parallel decoding. dUltra introduces an unmasking planner head that predicts per-token unmasking likelihoods under independent Bernoulli distributions. We jointly optimize the base diffusion LLM and the unmasking order planner using reward signals combining verifiable reward, distillation reward, and the number of unmasking steps. Across mathematical reasoning and code generation tasks, dUltra improves the accuracy--efficiency trade-off over state-of-the-art heuristic and distillation baselines, moving towards achieving ``diffusion supremacy'' over autoregressive models.</li>
<li><strong>摘要：</strong>掩码扩散语言模型 (MDLM) 提供了并行令牌生成的潜力，但大多数开源 MDLM 即使采用复杂的采样策略，每个模型前向传递解码的令牌也少于 5 个。因此，它们的采样速度通常与 AR + 推测解码方案相当，限制了它们相对于主流自回归方法的优势。现有的基于蒸馏的加速器（dParallel、d3LLM）在基本模型生成的轨迹上微调 MDLM，这可能会在微调过程中变得偏离策略，并将性能限制为基本模型样本的质量。我们提出了 \texttt{dUltra}，一个基于组相对策略优化（GRPO）的策略强化学习框架，它学习有效并行解码的揭露策略。 dUltra 引入了一个揭露规划器头，可以预测独立伯努利分布下每个令牌的揭露可能性。我们使用结合了可验证奖励、蒸馏奖励和揭露步骤数量的奖励信号来联合优化基础扩散 LLM 和揭露顺序规划器。在数学推理和代码生成任务中，dUltra 提高了最先进的启发式和蒸馏基线的准确性和效率权衡，朝着实现自回归模型的“扩散霸权”迈进。</li>
</ul>

<h3>Title: Generative Multi-Focus Image Fusion</h3>
<ul>
<li><strong>Authors: </strong>Xinzhe Xie, Buyu Guo, Bolin Li, Shuangyan He, Yanzhen Gu, Qingyan Jiang, Peiliang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21495">https://arxiv.org/abs/2512.21495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21495">https://arxiv.org/pdf/2512.21495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21495]] Generative Multi-Focus Image Fusion(https://arxiv.org/abs/2512.21495)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generative</a></li>
<li><strong>Abstract: </strong>Multi-focus image fusion aims to generate an all-in-focus image from a sequence of partially focused input images. Existing fusion algorithms generally assume that, for every spatial location in the scene, there is at least one input image in which that location is in focus. Furthermore, current fusion models often suffer from edge artifacts caused by uncertain focus estimation or hard-selection operations in complex real-world scenarios. To address these limitations, we propose a generative multi-focus image fusion framework, termed GMFF, which operates in two sequential stages. In the first stage, deterministic fusion is implemented using StackMFF V4, the latest version of the StackMFF series, and integrates the available focal plane information to produce an initial fused image. The second stage, generative restoration, is realized through IFControlNet, which leverages the generative capabilities of latent diffusion models to reconstruct content from missing focal planes, restore fine details, and eliminate edge artifacts. Each stage is independently developed and functions seamlessly in a cascaded manner. Extensive experiments demonstrate that GMFF achieves state-of-the-art fusion performance and exhibits significant potential for practical applications, particularly in scenarios involving complex multi-focal content. The implementation is publicly available at this https URL.</li>
<li><strong>摘要：</strong>多焦点图像融合旨在从一系列部分聚焦的输入图像生成全焦点图像。现有的融合算法通常假设，对于场景中的每个空间位置，至少有一个输入图像，其中该位置处于焦点位置。此外，当前的融合模型经常受到复杂现实场景中不确定的焦点估计或硬选择操作引起的边缘伪影的影响。为了解决这些限制，我们提出了一种生成多焦点图像融合框架，称为 GMFF，它分两个连续阶段运行。在第一阶段，使用StackMFF V4（StackMFF系列的最新版本）实现确定性融合，并集成可用的焦平面信息以生成初始融合图像。第二阶段是生成恢复，通过 IFControlNet 实现，它利用潜在扩散模型的生成功能从丢失的焦平面中重建内容，恢复精细细节并消除边缘伪影。每个阶段都是独立开发的，并以级联方式无缝运行。大量实验表明，GMFF 实现了最先进的融合性能，并在实际应用中展现出巨大的潜力，特别是在涉及复杂多焦点内容的场景中。该实现可通过此 https URL 公开获得。</li>
</ul>

<h3>Title: MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs for Health and Behavioral Understanding</h3>
<ul>
<li><strong>Authors: </strong>Aiwei Zhang, Arvind Pillai, Andrew Campbell, Nicholas C. Jacobson</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21506">https://arxiv.org/abs/2512.21506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21506">https://arxiv.org/pdf/2512.21506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21506]] MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs for Health and Behavioral Understanding(https://arxiv.org/abs/2512.21506)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>As wearable sensing becomes increasingly pervasive, a key challenge remains: how can we generate natural language summaries from raw physiological signals such as actigraphy - minute-level movement data collected via accelerometers? In this work, we introduce MotionTeller, a generative framework that natively integrates minute-level wearable activity data with large language models (LLMs). MotionTeller combines a pretrained actigraphy encoder with a lightweight projection module that maps behavioral embeddings into the token space of a frozen decoder-only LLM, enabling free-text, autoregressive generation of daily behavioral summaries. We construct a novel dataset of 54383 (actigraphy, text) pairs derived from real-world NHANES recordings, and train the model using cross-entropy loss with supervision only on the language tokens. MotionTeller achieves high semantic fidelity (BERTScore-F1 = 0.924) and lexical accuracy (ROUGE-1 = 0.722), outperforming prompt-based baselines by 7 percent in ROUGE-1. The average training loss converges to 0.38 by epoch 15, indicating stable optimization. Qualitative analysis confirms that MotionTeller captures circadian structure and behavioral transitions, while PCA plots reveal enhanced cluster alignment in embedding space post-training. Together, these results position MotionTeller as a scalable, interpretable system for transforming wearable sensor data into fluent, human-centered descriptions, introducing new pathways for behavioral monitoring, clinical review, and personalized health interventions.</li>
<li><strong>摘要：</strong>随着可穿戴传感变得越来越普遍，一个关键的挑战仍然存在：我们如何从原始生理信号（例如通过加速度计收集的分钟级运动数据）生成自然语言摘要？在这项工作中，我们介绍了 MotionTeller，这是一个生成框架，可以将分钟级可穿戴活动数据与大型语言模型 (LLM) 原生集成。 MotionTeller 将预训练的体动记录编码器与轻量级投影模块相结合，将行为嵌入映射到冻结的仅解码器 LLM 的令牌空间中，从而实现日常行为摘要的自由文本、自回归生成。我们构建了一个由 54383 个（活动记录、文本）对组成的新颖数据集，这些数据集源自真实世界的 NHANES 记录，并使用交叉熵损失训练模型，仅对语言标记进行监督。 MotionTeller 实现了高语义保真度 (BERTScore-F1 = 0.924) 和词汇准确性 (ROUGE-1 = 0.722)，在 ROUGE-1 中比基于提示的基线高出 7%。到 epoch 15 时，平均训练损失收敛到 0.38，表明优化稳定。定性分析证实，MotionTeller 捕获了昼夜节律结构和行为转变，而 PCA 图则揭示了训练后嵌入空间中增强的聚类对齐。总之，这些结果使 MotionTeller 成为一个可扩展、可解释的系统，用于将可穿戴传感器数据转换为流畅、以人为本的描述，为行为监测、临床审查和个性化健康干预引入新途径。</li>
</ul>

<h3>Title: SVBench: Evaluation of Video Generation Models on Social Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Wenshuo Peng, Gongxuan Wang, Tianmeng Yang, Chuanhao Li, Xiaojie Xu, Hui He, Kaipeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21507">https://arxiv.org/abs/2512.21507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21507">https://arxiv.org/pdf/2512.21507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21507]] SVBench: Evaluation of Video Generation Models on Social Reasoning(https://arxiv.org/abs/2512.21507)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent text-to-video generation models exhibit remarkable progress in visual realism, motion fidelity, and text-video alignment, yet they remain fundamentally limited in their ability to generate socially coherent behavior. Unlike humans, who effortlessly infer intentions, beliefs, emotions, and social norms from brief visual cues, current models tend to render literal scenes without capturing the underlying causal or psychological logic. To systematically evaluate this gap, we introduce the first benchmark for social reasoning in video generation. Grounded in findings from developmental and social psychology, our benchmark organizes thirty classic social cognition paradigms into seven core dimensions, including mental-state inference, goal-directed action, joint attention, social coordination, prosocial behavior, social norms, and multi-agent strategy. To operationalize these paradigms, we develop a fully training-free agent-based pipeline that (i) distills the reasoning mechanism of each experiment, (ii) synthesizes diverse video-ready scenarios, (iii) enforces conceptual neutrality and difficulty control through cue-based critique, and (iv) evaluates generated videos using a high-capacity VLM judge across five interpretable dimensions of social reasoning. Using this framework, we conduct the first large-scale study across seven state-of-the-art video generation systems. Our results reveal substantial performance gaps: while modern models excel in surface-level plausibility, they systematically fail in intention recognition, belief reasoning, joint attention, and prosocial inference.</li>
<li><strong>摘要：</strong>最近的文本到视频生成模型在视觉真实感、运动保真度和文本视频对齐方面表现出显着进步，但它们生成社会连贯行为的能力仍然受到根本限制。人类可以轻松地从简短的视觉线索中推断出意图、信仰、情感和社会规范，而当前的模型则倾向于渲染字面场景，而不捕捉潜在的因果或心理逻辑。为了系统地评估这一差距，我们引入了视频生成中社交推理的第一个基准。我们的基准以发展心理学和社会心理学的研究结果为基础，将三十种经典的社会认知范式组织成七个核心维度，包括心理状态推断、目标导向行动、共同注意力、社会协调、亲社会行为、社会规范和多主体策略。为了实施这些范式，我们开发了一个完全免训练的基于代理的管道，该管道（i）提炼每个实验的推理机制，（ii）综合不同的视频就绪场景，（iii）通过基于提示的批评强制概念中立和难度控制，以及（iv）使用高容量 VLM 法官在社会推理的五个可解释维度上评估生成的视频。使用这个框架，我们对七个最先进的视频生成系统进行了首次大规模研究。我们的结果揭示了巨大的性能差距：虽然现代模型在表面合理性方面表现出色，但它们在意图识别、信念推理、共同关注和亲社会推理方面系统性地失败了。</li>
</ul>

<h3>Title: DiverseGRPO: Mitigating Mode Collapse in Image Generation via Diversity-Aware GRPO</h3>
<ul>
<li><strong>Authors: </strong>Henglin Liu, Huijuan Huang, Jing Wang, Chang Liu, Xiu Li, Xiangyang Ji</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21514">https://arxiv.org/abs/2512.21514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21514">https://arxiv.org/pdf/2512.21514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21514]] DiverseGRPO: Mitigating Mode Collapse in Image Generation via Diversity-Aware GRPO(https://arxiv.org/abs/2512.21514)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL), particularly GRPO, improves image generation quality significantly by comparing the relative performance of images generated within the same group. However, in the later stages of training, the model tends to produce homogenized outputs, lacking creativity and visual diversity, which restricts its application scenarios. This issue can be analyzed from both reward modeling and generation dynamics perspectives. First, traditional GRPO relies on single-sample quality as the reward signal, driving the model to converge toward a few high-reward generation modes while neglecting distribution-level diversity. Second, conventional GRPO regularization neglects the dominant role of early-stage denoising in preserving diversity, causing a misaligned regularization budget that limits the achievable quality--diversity trade-off. Motivated by these insights, we revisit the diversity degradation problem from both reward modeling and generation dynamics. At the reward level, we propose a distributional creativity bonus based on semantic grouping. Specifically, we construct a distribution-level representation via spectral clustering over samples generated from the same caption, and adaptively allocate exploratory rewards according to group sizes to encourage the discovery of novel visual modes. At the generation level, we introduce a structure-aware regularization, which enforces stronger early-stage constraints to preserve diversity without compromising reward optimization efficiency. Experiments demonstrate that our method achieves a 13\%--18\% improvement in semantic diversity under matched quality scores, establishing a new Pareto frontier between image quality and diversity for GRPO-based image generation.</li>
<li><strong>摘要：</strong>强化学习 (RL)，特别是 GRPO，通过比较同一组内生成的图像的相对性能，显着提高图像生成质量。然而，在训练后期，模型往往会产生同质化的输出，缺乏创造力和视觉多样性，这限制了其应用场景。这个问题可以从奖励模型和生成动态两个角度来分析。首先，传统GRPO依靠单样本质量作为奖励信号，驱动模型向少数高奖励生成模式收敛，而忽略了分布层面的多样性。其次，传统的 GRPO 正则化忽略了早期去噪在保护多样性方面的主导作用，导致正则化预算失调，限制了可实现的质量-多样性权衡。受这些见解的激励，我们从奖励模型和生成动态方面重新审视多样性退化问题。在奖励层面，我们提出了基于语义分组的分布式创造力奖励。具体来说，我们通过对同一标题生成的样本进行谱聚类构建分布级表示，并根据组大小自适应地分配探索性奖励，以鼓励发现新颖的视觉模式。在生成层面，我们引入了结构感知正则化，它强制执行更强大的早期约束以保持多样性而不影响奖励优化效率。实验表明，我们的方法在匹配的质量分数下实现了 13%--18% 的语义多样性改进，为基于 GRPO 的图像生成在图像质量和多样性之间建立了新的帕累托前沿。</li>
</ul>

<h3>Title: Generative Actor Critic</h3>
<ul>
<li><strong>Authors: </strong>Aoyang Qin, Deqian Kong, Wei Wang, Ying Nian Wu, Song-Chun Zhu, Sirui Xie</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21527">https://arxiv.org/abs/2512.21527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21527">https://arxiv.org/pdf/2512.21527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21527]] Generative Actor Critic(https://arxiv.org/abs/2512.21527)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Conventional Reinforcement Learning (RL) algorithms, typically focused on estimating or maximizing expected returns, face challenges when refining offline pretrained models with online experiences. This paper introduces Generative Actor Critic (GAC), a novel framework that decouples sequential decision-making by reframing \textit{policy evaluation} as learning a generative model of the joint distribution over trajectories and returns, $p(\tau, y)$, and \textit{policy improvement} as performing versatile inference on this learned model. To operationalize GAC, we introduce a specific instantiation based on a latent variable model that features continuous latent plan vectors. We develop novel inference strategies for both \textit{exploitation}, by optimizing latent plans to maximize expected returns, and \textit{exploration}, by sampling latent plans conditioned on dynamically adjusted target returns. Experiments on Gym-MuJoCo and Maze2D benchmarks demonstrate GAC's strong offline performance and significantly enhanced offline-to-online improvement compared to state-of-the-art methods, even in absence of step-wise rewards.</li>
<li><strong>摘要：</strong>传统的强化学习 (RL) 算法通常专注于估计或最大化预期回报，在利用在线体验完善离线预训练模型时面临挑战。本文介绍了 Generative Actor Critic (GAC)，这是一种新颖的框架，它通过将 \textit{policyvaluation} 重新构建为学习轨迹和回报联合分布的生成模型，$p(\tau, y)$ 和 \textit{policyimprovement} 作为在此学习模型上执行通用推理的方式来解耦顺序决策。为了操作 GAC，我们引入了基于潜在变量模型的特定实例，该模型具有连续潜在计划向量。我们为 \textit{exploitation} 开发新颖的推理策略，通过优化潜在计划以最大化预期回报，并为 \textit{exploration} 开发新颖的推理策略，通过对动态调整的目标回报条件进行采样潜在计划。 Gym-MuJoCo 和 Maze2D 基准测试的实验表明，即使没有逐步奖励，GAC 也具有强大的离线性能，并且与最先进的方法相比，显着增强了离线到在线的改进。</li>
</ul>

<h3>Title: Toward Intelligent Scene Augmentation for Context-Aware Object Placement and Sponsor-Logo Integration</h3>
<ul>
<li><strong>Authors: </strong>Unnati Saraswat, Tarun Rao, Namah Gupta, Shweta Swami, Shikhar Sharma, Prateek Narang, Dhruv Kumar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21560">https://arxiv.org/abs/2512.21560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21560">https://arxiv.org/pdf/2512.21560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21560]] Toward Intelligent Scene Augmentation for Context-Aware Object Placement and Sponsor-Logo Integration(https://arxiv.org/abs/2512.21560)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Intelligent image editing increasingly relies on advances in computer vision, multimodal reasoning, and generative modeling. While vision-language models (VLMs) and diffusion models enable guided visual manipulation, existing work rarely ensures that inserted objects are \emph{contextually appropriate}. We introduce two new tasks for advertising and digital media: (1) \emph{context-aware object insertion}, which requires predicting suitable object categories, generating them, and placing them plausibly within the scene; and (2) \emph{sponsor-product logo augmentation}, which involves detecting products and inserting correct brand logos, even when items are unbranded or incorrectly branded. To support these tasks, we build two new datasets with category annotations, placement regions, and sponsor-product labels.</li>
<li><strong>摘要：</strong>智能图像编辑越来越依赖于计算机视觉、多模态推理和生成建模的进步。虽然视觉语言模型（VLM）和扩散模型可以实现引导视觉操作，但现有的工作很少确保插入的对象\emph{上下文适当}。我们为广告和数字媒体引入了两个新任务：（1）\emph{上下文感知对象插入}，这需要预测合适的对象类别，生成它们，并将它们合理地放置在场景中； (2) \emph{赞助商产品标志增强}，包括检测产品并插入正确的品牌标志，即使商品没有品牌或品牌错误。为了支持这些任务，我们构建了两个带有类别注释、放置区域和赞助商产品标签的新数据集。</li>
</ul>

<h3>Title: RefineBridge: Generative Bridge Models Improve Financial Forecasting by Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Anthony Bolton, Wuyang Zhou, Zehua Chen, Giorgos Iacovides, Danilo Mandic</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21572">https://arxiv.org/abs/2512.21572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21572">https://arxiv.org/pdf/2512.21572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21572]] RefineBridge: Generative Bridge Models Improve Financial Forecasting by Foundation Models(https://arxiv.org/abs/2512.21572)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Financial time series forecasting is particularly challenging for transformer-based time series foundation models (TSFMs) due to non-stationarity, heavy-tailed distributions, and high-frequency noise present in data. Low-rank adaptation (LoRA) has become a popular parameter-efficient method for adapting pre-trained TSFMs to downstream data domains. However, it still underperforms in financial data, as it preserves the network architecture and training objective of TSFMs rather than complementing the foundation model. To further enhance TSFMs, we propose a novel refinement module, RefineBridge, built upon a tractable Schrödinger Bridge (SB) generative framework. Given the forecasts of TSFM as generative prior and the observed ground truths as targets, RefineBridge learns context-conditioned stochastic transport maps to improve TSFM predictions, iteratively approaching the ground-truth target from even a low-quality prior. Simulations on multiple financial benchmarks demonstrate that RefineBridge consistently improves the performance of state-of-the-art TSFMs across different prediction horizons.</li>
<li><strong>摘要：</strong>由于数据中存在非平稳性、重尾分布和高频噪声，金融时间序列预测对于基于变压器的时间序列基础模型 (TSFM) 来说尤其具有挑战性。低秩自适应 (LoRA) 已成为一种流行的参数高效方法，用于使预训练的 TSFM 适应下游数据域​​。然而，它在金融数据方面仍然表现不佳，因为它保留了 TSFM 的网络架构和训练目标，而不是补充基础模型。为了进一步增强 TSFM，我们提出了一种新颖的细化模块 RefineBridge，它建立在易于处理的薛定谔桥 (SB) 生成框架之上。考虑到 TSFM 的预测作为生成先验，观察到的地面实况作为目标，RefineBridge 学习上下文条件随机传输图来改进 TSFM 预测，甚至从低质量的先验迭代地接近地面实况目标。对多个金融基准的模拟表明，RefineBridge 在不同的预测范围内持续提高最先进的 TSFM 的性能。</li>
</ul>

<h3>Title: LLM-Free Image Captioning Evaluation in Reference-Flexible Settings</h3>
<ul>
<li><strong>Authors: </strong>Shinnosuke Hirano, Yuiga Wada, Kazuki Matsuda, Seitaro Otsuki, Komei Sugiura</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21582">https://arxiv.org/abs/2512.21582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21582">https://arxiv.org/pdf/2512.21582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21582]] LLM-Free Image Captioning Evaluation in Reference-Flexible Settings(https://arxiv.org/abs/2512.21582)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We focus on the automatic evaluation of image captions in both reference-based and reference-free settings. Existing metrics based on large language models (LLMs) favor their own generations; therefore, the neutrality is in question. Most LLM-free metrics do not suffer from such an issue, whereas they do not always demonstrate high performance. To address these issues, we propose Pearl, an LLM-free supervised metric for image captioning, which is applicable to both reference-based and reference-free settings. We introduce a novel mechanism that learns the representations of image--caption and caption--caption similarities. Furthermore, we construct a human-annotated dataset for image captioning metrics, that comprises approximately 333k human judgments collected from 2,360 annotators across over 75k images. Pearl outperformed other existing LLM-free metrics on the Composite, Flickr8K-Expert, Flickr8K-CF, Nebula, and FOIL datasets in both reference-based and reference-free settings. Our project page is available at this https URL.</li>
<li><strong>摘要：</strong>我们专注于在基于参考和无参考设置中自动评估图像标题。基于大语言模型（LLM）的现有指标有利于他们自己的一代人；因此，中立性是有问题的。大多数非法学硕士指标不会遇到这样的问题，但它们并不总是表现出高性能。为了解决这些问题，我们提出了 Pearl，一种用于图像字幕的无法学硕士监督指标，它适用于基于参考和无参考的设置。我们引入了一种新颖的机制，可以学习图像-标题和标题-标题相似性的表示。此外，我们还构建了一个用于图像字幕指标的人工注释数据集，其中包含从 2,360 名注释者收集的超过 75,000 张图像的约 333,000 个人类判断。在基于参考和无参考设置的 Composite、Flickr8K-Expert、Flickr8K-CF、Nebula 和 FOIL 数据集上，Pearl 的表现均优于其他现有的无 LLM 指标。我们的项目页面可通过此 https URL 获取。</li>
</ul>

<h3>Title: TAMEing Long Contexts in Personalization: Towards Training-Free and State-Aware MLLM Personalized Assistant</h3>
<ul>
<li><strong>Authors: </strong>Rongpei Hong, Jian Lang, Ting Zhong, Yong Wang, Fan Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21616">https://arxiv.org/abs/2512.21616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21616">https://arxiv.org/pdf/2512.21616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21616]] TAMEing Long Contexts in Personalization: Towards Training-Free and State-Aware MLLM Personalized Assistant(https://arxiv.org/abs/2512.21616)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Model (MLLM) Personalization is a critical research problem that facilitates personalized dialogues with MLLMs targeting specific entities (known as personalized concepts). However, existing methods and benchmarks focus on the simple, context-agnostic visual identification and textual replacement of the personalized concept (e.g., "A yellow puppy" -> "Your puppy Mochi"), overlooking the ability to support long-context conversations. An ideal personalized MLLM assistant is capable of engaging in long-context dialogues with humans and continually improving its experience quality by learning from past dialogue histories. To bridge this gap, we propose LCMP, the first Long-Context MLLM Personalization evaluation benchmark. LCMP assesses the capability of MLLMs in perceiving variations of personalized concepts and generating contextually appropriate personalized responses that reflect these variations. As a strong baseline for LCMP, we introduce a novel training-free and state-aware framework TAME. TAME endows MLLMs with double memories to manage the temporal and persistent variations of each personalized concept in a differentiated manner. In addition, TAME incorporates a new training-free Retrieve-then-Align Augmented Generation (RA2G) paradigm. RA2G introduces an alignment step to extract the contextually fitted information from the multi-memory retrieved knowledge to the current questions, enabling better interactions for complex real-world user queries. Experiments on LCMP demonstrate that TAME achieves the best performance, showcasing remarkable and evolving interaction experiences in long-context scenarios.</li>
<li><strong>摘要：</strong>多模态大语言模型 (MLLM) 个性化是一个关键的研究问题，它有助于与针对特定实体（称为个性化概念）的 MLLM 进行个性化对话。然而，现有的方法和基准侧重于简单的、与上下文无关的视觉识别和个性化概念的文本替换（例如，“一只黄色的小狗”->“你的小狗麻糬”），忽视了支持长上下文对话的能力。理想的个性化 MLLM 助手能够与人类进行长上下文对话，并通过学习过去的对话历史来不断提高其体验质量。为了弥补这一差距，我们提出了 LCMP，第一个长上下文 MLLM 个性化评估基准。 LCMP 评估 MLLM 感知个性化概念变化并生成反映这些变化的适合上下文的个性化响应的能力。作为 LCMP 的强大基线，我们引入了一种新颖的免培训和状态感知框架 TAME。 TAME 赋予 MLLM 双重记忆，以差异化的方式管理每个个性化概念的时间和持久变化。此外，TAME 还采用了新的免训练检索然后对齐增强生成 (RA2G) 范例。 RA2G 引入了一个对齐步骤，用于从多记忆检索的知识中提取与当前问题相符的上下文信息，从而为复杂的现实世界用户查询提供更好的交互。 LCMP 上的实验表明，TAME 实现了最佳性能，在长上下文场景中展示了卓越且不断发展的交互体验。</li>
</ul>

<h3>Title: SymDrive: Realistic and Controllable Driving Simulator via Symmetric Auto-regressive Online Restoration</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Liu, Daocheng Fu, Pinlong Cai, Lening Wang, Ying Liu, Yilong Ren, Botian Shi, Jianqiang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21618">https://arxiv.org/abs/2512.21618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21618">https://arxiv.org/pdf/2512.21618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21618]] SymDrive: Realistic and Controllable Driving Simulator via Symmetric Auto-regressive Online Restoration(https://arxiv.org/abs/2512.21618)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generation</a></li>
<li><strong>Abstract: </strong>High-fidelity and controllable 3D simulation is essential for addressing the long-tail data scarcity in Autonomous Driving (AD), yet existing methods struggle to simultaneously achieve photorealistic rendering and interactive traffic editing. Current approaches often falter in large-angle novel view synthesis and suffer from geometric or lighting artifacts during asset manipulation. To address these challenges, we propose SymDrive, a unified diffusion-based framework capable of joint high-quality rendering and scene editing. We introduce a Symmetric Auto-regressive Online Restoration paradigm, which constructs paired symmetric views to recover fine-grained details via a ground-truth-guided dual-view formulation and utilizes an auto-regressive strategy for consistent lateral view generation. Furthermore, we leverage this restoration capability to enable a training-free harmonization mechanism, treating vehicle insertion as context-aware inpainting to ensure seamless lighting and shadow consistency. Extensive experiments demonstrate that SymDrive achieves state-of-the-art performance in both novel-view enhancement and realistic 3D vehicle insertion.</li>
<li><strong>摘要：</strong>高保真且可控的 3D 模拟对于解决自动驾驶 (AD) 中的长尾数据稀缺问题至关重要，但现有方法难以同时实现真实感渲染和交互式交通编辑。当前的方法通常在大角度新颖的视图合成中表现不佳，并且在资产操作过程中会受到几何或照明伪影的影响。为了应对这些挑战，我们提出了 SymDrive，这是一个基于扩散的统一框架，能够联合高质量渲染和场景编辑。我们引入了对称自回归在线恢复范例，该范例构建配对对称视图，通过地面实况引导的双视图公式来恢复细粒度细节，并利用自回归策略来生成一致的横向视图。此外，我们利用这种恢复功能来实现无需培训的协调机制，将车辆插入视为上下文感知修复，以确保无缝照明和阴影的一致性。大量实验表明，SymDrive 在新颖的视图增强和逼真的 3D 车辆插入方面均实现了最先进的性能。</li>
</ul>

<h3>Title: Training-Free Disentangled Text-Guided Image Editing via Sparse Latent Constraints</h3>
<ul>
<li><strong>Authors: </strong>Mutiara Shabrina, Nova Kurnia Putri, Jefri Satria Ferdiansyah, Sabita Khansa Dewi, Novanto Yudistira</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21637">https://arxiv.org/abs/2512.21637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21637">https://arxiv.org/pdf/2512.21637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21637]] Training-Free Disentangled Text-Guided Image Editing via Sparse Latent Constraints(https://arxiv.org/abs/2512.21637)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-driven image manipulation often suffers from attribute entanglement, where modifying a target attribute (e.g., adding bangs) unintentionally alters other semantic properties such as identity or appearance. The Predict, Prevent, and Evaluate (PPE) framework addresses this issue by leveraging pre-trained vision-language models for disentangled editing. In this work, we analyze the PPE framework, focusing on its architectural components, including BERT-based attribute prediction and StyleGAN2-based image generation on the CelebA-HQ dataset. Through empirical analysis, we identify a limitation in the original regularization strategy, where latent updates remain dense and prone to semantic leakage. To mitigate this issue, we introduce a sparsity-based constraint using L1 regularization on latent space manipulation. Experimental results demonstrate that the proposed approach enforces more focused and controlled edits, effectively reducing unintended changes in non-target attributes while preserving facial identity.</li>
<li><strong>摘要：</strong>文本驱动的图像处理通常会遭受属性纠缠，修改目标属性（例如添加刘海）会无意中改变其他语义属性，例如身份或外观。预测、预防和评估 (PPE) 框架通过利用预先训练的视觉语言模型进行解耦编辑来解决此问题。在这项工作中，我们分析了 PPE 框架，重点关注其架构组件，包括基于 BERT 的属性预测和 CelebA-HQ 数据集上基于 StyleGAN2 的图像生成。通过实证分析，我们发现了原始正则化策略的局限性，即潜在更新仍然密集并且容易出现语义泄漏。为了缓解这个问题，我们在潜在空间操作上使用 L1 正则化引入基于稀疏性的约束。实验结果表明，所提出的方法强制执行更有针对性和受控的编辑，有效减少非目标属性的意外变化，同时保留面部身份。</li>
</ul>

<h3>Title: Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding</h3>
<ul>
<li><strong>Authors: </strong>Zhiwang Zhou, Yuandong Pu, Xuming He, Yidi Liu, Yixin Chen, Junchao Gong, Xiang Zhuang, Wanghan Xu, Qinglong Cao, Shixiang Tang, Yihao Liu, Wenlong Zhang, Lei Bai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21643">https://arxiv.org/abs/2512.21643</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21643">https://arxiv.org/pdf/2512.21643</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21643]] Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding(https://arxiv.org/abs/2512.21643)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Weather modeling requires both accurate prediction and mechanistic interpretation, yet existing methods treat these goals in isolation, separating generation from understanding. To address this gap, we present Omni-Weather, the first multimodal foundation model that unifies weather generation and understanding within a single architecture. Omni-Weather integrates a radar encoder for weather generation tasks, followed by unified processing using a shared self-attention mechanism. Moreover, we construct a Chain-of-Thought dataset for causal reasoning in weather generation, enabling interpretable outputs and improved perceptual quality. Extensive experiments show Omni-Weather achieves state-of-the-art performance in both weather generation and understanding. Our findings further indicate that generative and understanding tasks in the weather domain can mutually enhance each other. Omni-Weather also demonstrates the feasibility and value of unifying weather generation and understanding.</li>
<li><strong>摘要：</strong>天气建模需要准确的预测和机械解释，但现有方法孤立地对待这些目标，将生成与理解分开。为了解决这一差距，我们推出了 Omni-Weather，这是第一个多模式基础模型，它将天气生成和理解统一在一个架构中。 Omni-Weather 集成了用于天气生成任务的雷达编码器，然后使用共享的自注意力机制进行统一处理。此外，我们构建了一个思想链数据集，用于天气生成中的因果推理，从而实现可解释的输出并提高感知质量。大量实验表明 Omni-Weather 在天气生成和理解方面均实现了最先进的性能。我们的研究结果进一步表明，天气领域的生成任务和理解任务可以相互促进。 Omni-Weather 还展示了统一天气生成和理解的可行性和价值。</li>
</ul>

<h3>Title: Causal-HM: Restoring Physical Generative Logic in Multimodal Anomaly Detection via Hierarchical Modulation</h3>
<ul>
<li><strong>Authors: </strong>Xiao Liu, Junchen Jin, Yanjie Zhao, Zhixuan Xing</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21650">https://arxiv.org/abs/2512.21650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21650">https://arxiv.org/pdf/2512.21650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21650]] Causal-HM: Restoring Physical Generative Logic in Multimodal Anomaly Detection via Hierarchical Modulation(https://arxiv.org/abs/2512.21650)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Multimodal Unsupervised Anomaly Detection (UAD) is critical for quality assurance in smart manufacturing, particularly in complex processes like robotic welding. However, existing methods often suffer from causal blindness, treating process modalities (e.g., real-time video, audio, and sensors) and result modalities (e.g., post-weld images) as equal feature sources, thereby ignoring the inherent physical generative logic. Furthermore, the heterogeneity gap between high-dimensional visual data and low-dimensional sensor signals frequently leads to critical process context being drowned out. In this paper, we propose Causal-HM, a unified multimodal UAD framework that explicitly models the physical Process to Result dependency. Specifically, our framework incorporates two key innovations: a Sensor-Guided CHM Modulation mechanism that utilizes low-dimensional sensor signals as context to guide high-dimensional audio-visual feature extraction , and a Causal-Hierarchical Architecture that enforces a unidirectional generative mapping to identify anomalies that violate physical consistency. Extensive experiments on our newly constructed Weld-4M benchmark across four modalities demonstrate that Causal-HM achieves a state-of-the-art (SOTA) I-AUROC of 90.7%. Code will be released after the paper is accepted.</li>
<li><strong>摘要：</strong>多模式无监督异常检测 (UAD) 对于智能制造的质量保证至关重要，特别是在机器人焊接等复杂工艺中。然而，现有方法常常存在因果盲目性，将过程模态（例如实时视频、音频和传感器）和结果模态（例如焊接后图像）视为相同的特征源，从而忽略了固有的物理生成逻辑。此外，高维视觉数据和低维传感器信号之间的异质性差距经常导致关键过程上下文被淹没。在本文中，我们提出了 Causal-HM，这是一个统一的多模式 UAD 框架，它显式地模拟了物理过程与结果的依赖关系。具体来说，我们的框架包含两项关键创新：传感器引导的 CHM 调制机制，利用低维传感器信号作为上下文来指导高维视听特征提取，以及因果分层架构，强制执行单向生成映射以识别违反物理一致性的异常。在我们新构建的 Weld-4M 基准上跨四种模式进行的广泛实验表明，Causal-HM 实现了 90.7% 的最先进 (SOTA) I-AUROC。论文被接受后将发布代码。</li>
</ul>

<h3>Title: UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture</h3>
<ul>
<li><strong>Authors: </strong>Shuo Cao, Jiayang Li, Xiaohui Li, Yuandong Pu, Kaiwen Zhu, Yuanting Gao, Siqi Luo, Yi Xin, Qi Qin, Yu Zhou, Xiangyu Chen, Wenlong Zhang, Bin Fu, Yu Qiao, Yihao Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21675">https://arxiv.org/abs/2512.21675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21675">https://arxiv.org/pdf/2512.21675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21675]] UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture(https://arxiv.org/abs/2512.21675)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have achieved remarkable progress in visual understanding tasks such as visual grounding, segmentation, and captioning. However, their ability to perceive perceptual-level image features remains limited. In this work, we present UniPercept-Bench, a unified framework for perceptual-level image understanding across three key domains: Aesthetics, Quality, Structure and Texture. We establish a hierarchical definition system and construct large-scale datasets to evaluate perceptual-level image understanding. Based on this foundation, we develop a strong baseline UniPercept trained via Domain-Adaptive Pre-Training and Task-Aligned RL, enabling robust generalization across both Visual Rating (VR) and Visual Question Answering (VQA) tasks. UniPercept outperforms existing MLLMs on perceptual-level image understanding and can serve as a plug-and-play reward model for text-to-image generation. This work defines Perceptual-Level Image Understanding in the era of MLLMs and, through the introduction of a comprehensive benchmark together with a strong baseline, provides a solid foundation for advancing perceptual-level multimodal image understanding.</li>
<li><strong>摘要：</strong>多模态大语言模型（MLLM）在视觉理解任务（例如视觉基础、分割和字幕）方面取得了显着进展。然而，它们感知感知级图像特征的能力仍然有限。在这项工作中，我们提出了 UniPercept-Bench，这是一个跨三个关键领域的感知级图像理解的统一框架：美学、质量、结构和纹理。我们建立了分层定义系统并构建大规模数据集来评估感知级图像理解。在此基础上，我们开发了一个强大的基线 UniPercept，通过域自适应预训练和任务对齐 RL 进行训练，从而实现了视觉评分 (VR) 和视觉问答 (VQA) 任务的稳健泛化。 UniPercept 在感知级图像理解方面优于现有的 MLLM，并且可以作为文本到图像生成的即插即用奖励模型。这项工作定义了 MLLM 时代的感知级图像理解，并通过引入全面的基准和强大的基线，为推进感知级多模态图像理解提供了坚实的基础。</li>
</ul>

<h3>Title: Dictionary-Transform Generative Adversarial Networks</h3>
<ul>
<li><strong>Authors: </strong>Angshul Majumdar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21677">https://arxiv.org/abs/2512.21677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21677">https://arxiv.org/pdf/2512.21677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21677]] Dictionary-Transform Generative Adversarial Networks(https://arxiv.org/abs/2512.21677)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative adversarial networks (GANs) are widely used for distribution learning, yet their classical formulations remain theoretically fragile, with ill-posed objectives, unstable training dynamics, and limited interpretability. In this work, we introduce \emph{Dictionary-Transform Generative Adversarial Networks} (DT-GAN), a fully model-based adversarial framework in which the generator is a sparse synthesis dictionary and the discriminator is an analysis transform acting as an energy model. By restricting both players to linear operators with explicit constraints, DT-GAN departs fundamentally from neural GAN architectures and admits rigorous theoretical analysis. We show that the DT-GAN adversarial game is well posed and admits at least one Nash equilibrium. Under a sparse generative model, equilibrium solutions are provably identifiable up to standard permutation and sign ambiguities and exhibit a precise geometric alignment between synthesis and analysis operators. We further establish finite-sample stability and consistency of empirical equilibria, demonstrating that DT-GAN training converges reliably under standard sampling assumptions and remains robust in heavy-tailed regimes. Experiments on mixture-structured synthetic data validate the theoretical predictions, showing that DT-GAN consistently recovers underlying structure and exhibits stable behavior under identical optimization budgets where a standard GAN degrades. DT-GAN is not proposed as a universal replacement for neural GANs, but as a principled adversarial alternative for data distributions that admit sparse synthesis structure. The results demonstrate that adversarial learning can be made interpretable, stable, and provably correct when grounded in classical sparse modeling.</li>
<li><strong>摘要：</strong>生成对抗网络（GAN）广泛用于分布式学习，但其经典公式在理论上仍然脆弱，目标不适当、训练动态不稳定且可解释性有限。在这项工作中，我们引入了 \emph{字典变换生成对抗网络}（DT-GAN），这是一种完全基于模型的对抗框架，其中生成器是稀疏合成字典，鉴别器是充当能量模型的分析变换。通过将两个参与者限制为具有明确约束的线性算子，DT-GAN 从根本上脱离了神经 GAN 架构，并允许严格的理论分析。我们证明 DT-GAN 对抗博弈是适定的并且承认至少一个纳什均衡。在稀疏生成模型下，平衡解在标准排列和符号模糊度范围内可被证明是可识别的，并且在综合和分析算子之间表现出精确的几何对齐。我们进一步建立了有限样本稳定性和经验平衡的一致性，证明 DT-GAN 训练在标准采样假设下可靠收敛，并在重尾机制中保持稳健。混合结构合成数据的实验验证了理论预测，表明 DT-GAN 始终如一地恢复底层结构，并在标准 GAN 退化的相同优化预算下表现出稳定的行为。 DT-GAN 并不是作为神经 GAN 的通用替代品，而是作为允许稀疏合成结构的数据分布的原则性对抗替代方案。结果表明，当基于经典稀疏模型时，对抗性学习可以变得可解释、稳定且可证明正确。</li>
</ul>

<h3>Title: Prior-AttUNet: Retinal OCT Fluid Segmentation Based on Normal Anatomical Priors and Attention Gating</h3>
<ul>
<li><strong>Authors: </strong>Li Yang, Yuting Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21693">https://arxiv.org/abs/2512.21693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21693">https://arxiv.org/pdf/2512.21693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21693]] Prior-AttUNet: Retinal OCT Fluid Segmentation Based on Normal Anatomical Priors and Attention Gating(https://arxiv.org/abs/2512.21693)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Accurate segmentation of macular edema, a hallmark pathological feature in vision-threatening conditions such as age-related macular degeneration and diabetic macular edema, is essential for clinical diagnosis and management. To overcome the challenges of segmenting fluid regions in optical coherence tomography (OCT) images-notably ambiguous boundaries and cross-device heterogeneity-this study introduces Prior-AttUNet, a segmentation model augmented with generative anatomical priors. The framework adopts a hybrid dual-path architecture that integrates a generative prior pathway with a segmentation network. A variational autoencoder supplies multi-scale normative anatomical priors, while the segmentation backbone incorporates densely connected blocks and spatial pyramid pooling modules to capture richer contextual information. Additionally, a novel triple-attention mechanism, guided by anatomical priors, dynamically modulates feature importance across decoding stages, substantially enhancing boundary delineation. Evaluated on the public RETOUCH benchmark, Prior-AttUNet achieves excellent performance across three OCT imaging devices (Cirrus, Spectralis, and Topcon), with mean Dice similarity coefficients of 93.93%, 95.18%, and 93.47%, respectively. The model maintains a low computational cost of 0.37 TFLOPs, striking an effective balance between segmentation precision and inference efficiency. These results demonstrate its potential as a reliable tool for automated clinical analysis.</li>
<li><strong>摘要：</strong>黄斑水肿是年龄相关性黄斑变性和糖尿病性黄斑水肿等视力威胁疾病的标志性病理特征，黄斑水肿的准确分割对于临床诊断和治疗至关重要。为了克服在光学相干断层扫描 (OCT) 图像中分割流体区域的挑战，尤其是模糊边界和跨设备异质性，本研究引入了 Prior-AttUNet，这是一种利用生成解剖先验增强的分割模型。该框架采用混合双路径架构，将生成先验路径与分割网络集成在一起。变分自动编码器提供多尺度规范解剖先验，而分割主干则包含密集连接的块和空间金字塔池模块以捕获更丰富的上下文信息。此外，一种新颖的三重注意力机制在解剖学先验的指导下，动态调节解码阶段的特征重要性，从而大大增强了边界描绘。根据公共 RETOUCH 基准评估，Prior-AttUNet 在三种 OCT 成像设备（Cirrus、Spectralis 和 Topcon）上实现了出色的性能，平均 Dice 相似系数分别为 93.93%、95.18% 和 93.47%。该模型保持了0.37 TFLOPs的低计算成本，在分割精度和推理效率之间取得了有效的平衡。这些结果证明了其作为自动化临床分析的可靠工具的潜力。</li>
</ul>

<h3>Title: BeHGAN: Bengali Handwritten Word Generation from Plain Text Using Generative Adversarial Networks</h3>
<ul>
<li><strong>Authors: </strong>Md. Rakibul Islam, Md. Kamrozzaman Bhuiyan, Safwan Muntasir, Arifur Rahman Jawad, Most. Sharmin Sultana Samu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21694">https://arxiv.org/abs/2512.21694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21694">https://arxiv.org/pdf/2512.21694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21694]] BeHGAN: Bengali Handwritten Word Generation from Plain Text Using Generative Adversarial Networks(https://arxiv.org/abs/2512.21694)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Handwritten Text Recognition (HTR) is a well-established research area. In contrast, Handwritten Text Generation (HTG) is an emerging field with significant potential. This task is challenging due to the variation in individual handwriting styles. A large and diverse dataset is required to generate realistic handwritten text. However, such datasets are difficult to collect and are not readily available. Bengali is the fifth most spoken language in the world. While several studies exist for languages such as English and Arabic, Bengali handwritten text generation has received little attention. To address this gap, we propose a method for generating Bengali handwritten words. We developed and used a self-collected dataset of Bengali handwriting samples. The dataset includes contributions from approximately five hundred individuals across different ages and genders. All images were pre-processed to ensure consistency and quality. Our approach demonstrates the ability to produce diverse handwritten outputs from input plain text. We believe this work contributes to the advancement of Bengali handwriting generation and can support further research in this area.</li>
<li><strong>摘要：</strong>手写文本识别（HTR）是一个成熟的研究领域。相比之下，手写文本生成（HTG）是一个具有巨大潜力的新兴领域。由于个人笔迹风格的差异，这项任务具有挑战性。生成真实的手写文本需要大量且多样化的数据集。然而，此类数据集很难收集并且不容易获得。孟加拉语是世界上第五大语言。虽然针对英语和阿拉伯语等语言进行了多项研究，但孟加拉语手写文本生成很少受到关注。为了解决这一差距，我们提出了一种生成孟加拉语手写单词的方法。我们开发并使用了自行收集的孟加拉语手写样本数据集。该数据集包括来自不同年龄和性别的大约五百个人的贡献。所有图像均经过预处理，以确保一致性和质量。我们的方法展示了从输入纯文本生成各种手写输出的能力。我们相信这项工作有助于孟加拉语手写体生成的进步，并可以支持该领域的进一步研究。</li>
</ul>

<h3>Title: FUSE: Unifying Spectral and Semantic Cues for Robust AI-Generated Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Md. Zahid Hossain, Most. Sharmin Sultana Samu, Md. Kamrozzaman Bhuiyan, Farhad Uz Zaman, Md. Rakibul Islam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21695">https://arxiv.org/abs/2512.21695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21695">https://arxiv.org/pdf/2512.21695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21695]] FUSE: Unifying Spectral and Semantic Cues for Robust AI-Generated Image Detection(https://arxiv.org/abs/2512.21695)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The fast evolution of generative models has heightened the demand for reliable detection of AI-generated images. To tackle this challenge, we introduce FUSE, a hybrid system that combines spectral features extracted through Fast Fourier Transform with semantic features obtained from the CLIP's Vision encoder. The features are fused into a joint representation and trained progressively in two stages. Evaluations on GenImage, WildFake, DiTFake, GPT-ImgEval and Chameleon datasets demonstrate strong generalization across multiple generators. Our FUSE (Stage 1) model demonstrates state-of-the-art results on the Chameleon benchmark. It also attains 91.36% mean accuracy on the GenImage dataset, 88.71% accuracy across all tested generators, and a mean Average Precision of 94.96%. Stage 2 training further improves performance for most generators. Unlike existing methods, which often perform poorly on high-fidelity images in Chameleon, our approach maintains robustness across diverse generators. These findings highlight the benefits of integrating spectral and semantic features for generalized detection of images generated by AI.</li>
<li><strong>摘要：</strong>生成模型的快速发展提高了对人工智能生成图像的可靠检测的需求。为了应对这一挑战，我们引入了 FUSE，这是一种混合系统，它将通过快速傅立叶变换提取的光谱特征与从 CLIP 的视觉编码器获得的语义特征相结合。这些特征被融合成联合表示，并分两个阶段逐步训练。对 GenImage、WildFake、DiTFake、GPT-ImgEval 和 Chameleon 数据集的评估证明了跨多个生成器的强大泛化能力。我们的 FUSE（第一阶段）模型在 Chameleon 基准测试中展示了最先进的结果。它还在 GenImage 数据集上达到了 91.36% 的平均准确度，在所有测试的生成器上达到了 88.71% 的准确度，平均精度为 94.96%。第 2 阶段训练进一步提高了大多数生成器的性能。现有的方法通常在 Chameleon 的高保真图像上表现不佳，与此不同，我们的方法在不同的生成器中保持了鲁棒性。这些发现强调了集成光谱和语义特征对于人工智能生成的图像的广义检测的好处。</li>
</ul>

<h3>Title: RAPTOR: Real-Time High-Resolution UAV Video Prediction with Efficient Video Attention</h3>
<ul>
<li><strong>Authors: </strong>Zhan Chen, Zile Guo, Enze Zhu, Peirong Zhang, Xiaoxuan Liu, Lei Wang, Yidan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21710">https://arxiv.org/abs/2512.21710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21710">https://arxiv.org/pdf/2512.21710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21710]] RAPTOR: Real-Time High-Resolution UAV Video Prediction with Efficient Video Attention(https://arxiv.org/abs/2512.21710)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Video prediction is plagued by a fundamental trilemma: achieving high-resolution and perceptual quality typically comes at the cost of real-time speed, hindering its use in latency-critical applications. This challenge is most acute for autonomous UAVs in dense urban environments, where foreseeing events from high-resolution imagery is non-negotiable for safety. Existing methods, reliant on iterative generation (diffusion, autoregressive models) or quadratic-complexity attention, fail to meet these stringent demands on edge hardware. To break this long-standing trade-off, we introduce RAPTOR, a video prediction architecture that achieves real-time, high-resolution performance. RAPTOR's single-pass design avoids the error accumulation and latency of iterative approaches. Its core innovation is Efficient Video Attention (EVA), a novel translator module that factorizes spatiotemporal modeling. Instead of processing flattened spacetime tokens with $O((ST)^2)$ or $O(ST)$ complexity, EVA alternates operations along the spatial (S) and temporal (T) axes. This factorization reduces the time complexity to $O(S + T)$ and memory complexity to $O(max(S, T))$, enabling global context modeling at $512^2$ resolution and beyond, operating directly on dense feature maps with a patch-free design. Complementing this architecture is a 3-stage training curriculum that progressively refines predictions from coarse structure to sharp, temporally coherent details. Experiments show RAPTOR is the first predictor to exceed 30 FPS on a Jetson AGX Orin for $512^2$ video, setting a new state-of-the-art on UAVid, KTH, and a custom high-resolution dataset in PSNR, SSIM, and LPIPS. Critically, RAPTOR boosts the mission success rate in a real-world UAV navigation task by 18/%, paving the way for safer and more anticipatory embodied agents.</li>
<li><strong>摘要：</strong>视频预测受到一个基本的三难困境的困扰：实现高分辨率和感知质量通常是以牺牲实时速度为代价的，这阻碍了其在延迟关键型应用中的使用。对于密集城市环境中的自主无人机来说，这一挑战最为严峻，因为在这些环境中，从高分辨率图像中预测事件对于安全来说是不容妥协的。现有的方法依赖于迭代生成（扩散、自回归模型）或二次复杂度注意力，无法满足对边缘硬件的严格要求。为了打破这种长期存在的权衡，我们引入了 RAPTOR，这是一种实现实时、高分辨率性能的视频预测架构。 RAPTOR 的单遍设计避免了迭代方法的错误累积和延迟。其核心创新是高效视频注意力（EVA），这是一种新颖的翻译模块，可分解时空建模。 EVA 不是以 $O((ST)^2)$ 或 $O(ST)$ 复杂度处理扁平化时空标记，而是沿着空间 (S) 和时间 (T) 轴交替操作。这种因式分解将时间复杂度降低至 $O(S + T)$，将内存复杂度降低至 $O(max(S, T))$，从而能够以 512^2$ 及以上的分辨率进行全局上下文建模，并通过无补丁设计直接在密集特征图上进行操作。对该架构的补充是一个三阶段的培训课程，该课程逐步将预测从粗略结构细化为清晰、时间连贯的细节。实验表明，RAPTOR 是第一个在 Jetson AGX Orin 上以 512^2 美元的视频速度超过 30 FPS 的预测器，在 UAVid、KTH 以及 PSNR、SSIM 和 LPIPS 方面的自定义高分辨率数据集上创下了新的最先进水平。至关重要的是，RAPTOR 将现实世界无人机导航任务的任务成功率提高了 18%，为更安全、更具预见性的实体代理铺平了道路。</li>
</ul>

<h3>Title: AstraNav-World: World Model for Foresight Control and Consistency</h3>
<ul>
<li><strong>Authors: </strong>Junjun Hu, Jintao Chen, Haochen Bai, Minghua Luo, Shichao Xie, Ziyi Chen, Fei Liu, Zedong Chu, Xinda Xue, Botao Ren, Xiaolong Wu, Mu Xu, Shanghang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21714">https://arxiv.org/abs/2512.21714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21714">https://arxiv.org/pdf/2512.21714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21714]] AstraNav-World: World Model for Foresight Control and Consistency(https://arxiv.org/abs/2512.21714)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Embodied navigation in open, dynamic environments demands accurate foresight of how the world will evolve and how actions will unfold over time. We propose AstraNav-World, an end-to-end world model that jointly reasons about future visual states and action sequences within a unified probabilistic framework. Our framework integrates a diffusion-based video generator with a vision-language policy, enabling synchronized rollouts where predicted scenes and planned actions are updated simultaneously. Training optimizes two complementary objectives: generating action-conditioned multi-step visual predictions and deriving trajectories conditioned on those predicted visuals. This bidirectional constraint makes visual predictions executable and keeps decisions grounded in physically consistent, task-relevant futures, mitigating cumulative errors common in decoupled "envision-then-plan" pipelines. Experiments across diverse embodied navigation benchmarks show improved trajectory accuracy and higher success rates. Ablations confirm the necessity of tight vision-action coupling and unified training, with either branch removal degrading both prediction quality and policy reliability. In real-world testing, AstraNav-World demonstrated exceptional zero-shot capabilities, adapting to previously unseen scenarios without any real-world fine-tuning. These results suggest that AstraNav-World captures transferable spatial understanding and planning-relevant navigation dynamics, rather than merely overfitting to simulation-specific data distribution. Overall, by unifying foresight vision and control within a single generative model, we move closer to reliable, interpretable, and general-purpose embodied agents that operate robustly in open-ended real-world settings.</li>
<li><strong>摘要：</strong>开放、动态环境中的具体导航需要准确预见世界将如何演变以及随着时间的推移行动将如何展开。我们提出了 AstraNav-World，这是一种端到端的世界模型，可以在统一的概率框架内共同推理未来的视觉状态和动作序列。我们的框架将基于扩散的视频生成器与视觉语言策略集成在一起，从而实现同步推出，其中预测的场景和计划的动作可以同时更新。训练优化了两个互补的目标：生成以动作为条件的多步骤视觉预测，并根据这些预测的视觉结果导出轨迹。这种双向约束使视觉预测变得可执行，并使决策基于物理上一致、与任务相关的未来，从而减少解耦的“设想然后计划”流程中常见的累积错误。跨各种具体导航基准的实验表明，轨迹精度得到了提高，成功率也得到了提高。消融证实了紧密的视觉-动作耦合和统一训练的必要性，任何一个分支的删除都会降低预测质量和策略可靠性。在现实世界的测试中，AstraNav-World 展示了卓越的零射击能力，无需任何现实世界的微调即可适应以前未见过的场景。这些结果表明，AstraNav-World 捕获了可转移的空间理解和与规划相关的导航动态，而不仅仅是过度拟合特定于模拟的数据分布。总体而言，通过在单个生成模型中统一远见和控制，我们更接近可靠、可解释和通用的具体代理，这些代理可以在开放式的现实世界环境中稳健运行。</li>
</ul>

<h3>Title: Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation</h3>
<ul>
<li><strong>Authors: </strong>Steven Xiao, XIndi Zhang, Dechao Meng, Qi Wang, Peng Zhang, Bang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21734">https://arxiv.org/abs/2512.21734</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21734">https://arxiv.org/pdf/2512.21734</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21734]] Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation(https://arxiv.org/abs/2512.21734)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Real-time portrait animation is essential for interactive applications such as virtual assistants and live avatars, requiring high visual fidelity, temporal coherence, ultra-low latency, and responsive control from dynamic inputs like reference images and driving signals. While diffusion-based models achieve strong quality, their non-causal nature hinders streaming deployment. Causal autoregressive video generation approaches enable efficient frame-by-frame generation but suffer from error accumulation, motion discontinuities at chunk boundaries, and degraded long-term consistency. In this work, we present a novel streaming framework named Knot Forcing for real-time portrait animation that addresses these challenges through three key designs: (1) a chunk-wise generation strategy with global identity preservation via cached KV states of the reference image and local temporal modeling using sliding window attention; (2) a temporal knot module that overlaps adjacent chunks and propagates spatio-temporal cues via image-to-video conditioning to smooth inter-chunk motion transitions; and (3) A "running ahead" mechanism that dynamically updates the reference frame's temporal coordinate during inference, keeping its semantic context ahead of the current rollout frame to support long-term coherence. Knot Forcing enables high-fidelity, temporally consistent, and interactive portrait animation over infinite sequences, achieving real-time performance with strong visual stability on consumer-grade GPUs.</li>
<li><strong>摘要：</strong>实时肖像动画对于虚拟助手和实时头像等交互式应用至关重要，需要高视觉保真度、时间连贯性、超低延迟以及来自参考图像和驾驶信号等动态输入的响应控制。虽然基于扩散的模型具有很高的质量，但其非因果性质阻碍了流媒体部署。因果自回归视频生成方法可以实现高效的逐帧生成，但会遭受错误累积、块边界处的运动不连续性以及长期一致性下降的问题。在这项工作中，我们提出了一种名为 Knot Forcing 的新颖流框架，用于实时肖像动画，该框架通过三个关键设计解决了这些挑战：（1）通过缓存的参考图像的 KV 状态和使用滑动窗口注意的局部时间建模来实现全局身份保存的分块生成策略； （2）时间结模块，重叠相邻块并通过图像到视频调节传播时空线索，以平滑块间运动过渡； (3)“提前运行”机制，在推理过程中动态更新参考系的时间坐标，使其语义上下文保持在当前推出帧之前，以支持长期一致性。 Knot Forcing 可在无限序列上实现高保真、时间一致的交互式肖像动画，在消费级 GPU 上实现实时性能和强大的视觉稳定性。</li>
</ul>

<h3>Title: SyncAnyone: Implicit Disentanglement via Progressive Self-Correction for Lip-Syncing in the wild</h3>
<ul>
<li><strong>Authors: </strong>Xindi Zhang, Dechao Meng, Steven Xiao, Qi Wang, Peng Zhang, Bang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21736">https://arxiv.org/abs/2512.21736</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21736">https://arxiv.org/pdf/2512.21736</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21736]] SyncAnyone: Implicit Disentanglement via Progressive Self-Correction for Lip-Syncing in the wild(https://arxiv.org/abs/2512.21736)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>High-quality AI-powered video dubbing demands precise audio-lip synchronization, high-fidelity visual generation, and faithful preservation of identity and background. Most existing methods rely on a mask-based training strategy, where the mouth region is masked in talking-head videos, and the model learns to synthesize lip movements from corrupted inputs and target audios. While this facilitates lip-sync accuracy, it disrupts spatiotemporal context, impairing performance on dynamic facial motions and causing instability in facial structure and background consistency. To overcome this limitation, we propose SyncAnyone, a novel two-stage learning framework that achieves accurate motion modeling and high visual fidelity simultaneously. In Stage 1, we train a diffusion-based video transformer for masked mouth inpainting, leveraging its strong spatiotemporal modeling to generate accurate, audio-driven lip movements. However, due to input corruption, minor artifacts may arise in the surrounding facial regions and the background. In Stage 2, we develop a mask-free tuning pipeline to address mask-induced artifacts. Specifically, on the basis of the Stage 1 model, we develop a data generation pipeline that creates pseudo-paired training samples by synthesizing lip-synced videos from the source video and random sampled audio. We further tune the stage 2 model on this synthetic data, achieving precise lip editing and better background consistency. Extensive experiments show that our method achieves state-of-the-art results in visual quality, temporal coherence, and identity preservation under in-the wild lip-syncing scenarios.</li>
<li><strong>摘要：</strong>高质量的人工智能视频配音需要精确的音唇同步、高保真视觉生成以及忠实地保留身份和背景。大多数现有方法依赖于基于掩模的训练策略，其中嘴部区域在头部说话的视频中被掩模，并且模型学习从损坏的输入和目标音频合成嘴唇运动。虽然这有助于口型同步的准确性，但它会破坏时空背景，损害动态面部运动的性能，并导致面部结构和背景一致性的不稳定。为了克服这一限制，我们提出了 SyncAnyone，这是一种新颖的两阶段学习框架，可以同时实现精确的运动建模和高视觉保真度。在第一阶段，我们训练一个基于扩散的视频转换器用于蒙面嘴巴修复，利用其强大的时空建模来生成准确的、音频驱动的嘴唇运动。然而，由于输入损坏，周围的面部区域和背景可能会出现轻微的伪影。在第二阶段，我们开发了一个无掩模调整管道来解决掩模引起的伪影。具体来说，在第一阶段模型的基础上，我们开发了一个数据生成管道，通过从源视频和随机采样音频合成口型同步视频来创建伪配对训练样本。我们根据这些合成数据进一步调整第二阶段模型，实现精确的唇部编辑和更好的背景一致性。大量的实验表明，我们的方法在野外口型同步场景下在视觉质量、时间连贯性和身份保存方面取得了最先进的结果。</li>
</ul>

<h3>Title: Inference-based GAN Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Jingbo Yang, Adrian G. Bors</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21776">https://arxiv.org/abs/2512.21776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21776">https://arxiv.org/pdf/2512.21776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21776]] Inference-based GAN Video Generation(https://arxiv.org/abs/2512.21776)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Video generation has seen remarkable progresses thanks to advancements in generative deep learning. Generated videos should not only display coherent and continuous movement but also meaningful movement in successions of scenes. Generating models such as Generative Adversarial Networks (GANs) or Variational Autoencoders (VAEs) and more recently Diffusion Networks have been used for generating short video sequences, usually of up to 16 frames. In this paper, we first propose a new type of video generator by enabling adversarial-based unconditional video generators with a variational encoder, akin to a VAE-GAN hybrid structure, in order to enable the generation process with inference capabilities. The proposed model, as in other video deep learning-based processing frameworks, incorporates two processing branches, one for content and another for movement. However, existing models struggle with the temporal scaling of the generated videos. In classical approaches when aiming to increase the generated video length, the resulting video quality degrades, particularly when considering generating significantly long sequences. To overcome this limitation, our research study extends the initially proposed VAE-GAN video generation model by employing a novel, memory-efficient approach to generate long videos composed of hundreds or thousands of frames ensuring their temporal continuity, consistency and dynamics. Our approach leverages a Markov chain framework with a recall mechanism, with each state representing a VAE-GAN short-length video generator. This setup allows for the sequential connection of generated video sub-sequences, enabling temporal dependencies, resulting in meaningful long video sequences.</li>
<li><strong>摘要：</strong>由于生成深度学习的进步，视频生成取得了显着的进步。生成的视频不仅应显示连贯且连续的运动，而且应在连续场景中显示有意义的运动。生成对抗网络 (GAN) 或变分自动编码器 (VAE) 以及最近的扩散网络等生成模型已用于生成短视频序列，通常最多为 16 帧。在本文中，我们首先提出了一种新型视频生成器，通过使用变分编码器（类似于 VAE-GAN 混合结构）启用基于对抗性的无条件视频生成器，以便使生成过程具有推理能力。与其他基于深度学习的视频处理框架一样，所提出的模型包含两个处理分支，一个用于内容，另一个用于运动。然而，现有模型难以应对生成视频的时间缩放问题。在经典方法中，当旨在增加生成的视频长度时，最终的视频质量会下降，特别是在考虑生成非常长的序列时。为了克服这一限制，我们的研究扩展了最初提出的 VAE-GAN 视频生成模型，采用一种新颖的、内存高效的方法来生成由数百或数千帧组成的长视频，确保其时间连续性、一致性和动态性。我们的方法利用带有召回机制的马尔可夫链框架，每个状态代表一个 VAE-GAN 短视频生成器。此设置允许按顺序连接生成的视频子序列，实现时间依赖性，从而产生有意义的长视频序列。</li>
</ul>

<h3>Title: InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Jinqi Xiao, Qing Yan, Liming Jiang, Zichuan Liu, Hao Kang, Shen Sang, Tiancheng Zhi, Jing Liu, Cheng Yang, Xin Lu, Bo Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21788">https://arxiv.org/abs/2512.21788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21788">https://arxiv.org/pdf/2512.21788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21788]] InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation(https://arxiv.org/abs/2512.21788)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Parameter-Efficient Fine-Tuning of Diffusion Transformers (DiTs) for diverse, multi-conditional tasks often suffers from task interference when using monolithic adapters like LoRA. The Mixture of Low-rank Experts (MoLE) architecture offers a modular solution, but its potential is usually limited by routing policies that operate at a token level. Such local routing can conflict with the global nature of user instructions, leading to artifacts like spatial fragmentation and semantic drift in complex image generation tasks. To address these limitations, we introduce InstructMoLE, a novel framework that employs an Instruction-Guided Mixture of Low-Rank Experts. Instead of per-token routing, InstructMoLE utilizes a global routing signal, Instruction-Guided Routing (IGR), derived from the user's comprehensive instruction. This ensures that a single, coherently chosen expert council is applied uniformly across all input tokens, preserving the global semantics and structural integrity of the generation process. To complement this, we introduce an output-space orthogonality loss, which promotes expert functional diversity and mitigates representational collapse. Extensive experiments demonstrate that InstructMoLE significantly outperforms existing LoRA adapters and MoLE variants across challenging multi-conditional generation benchmarks. Our work presents a robust and generalizable framework for instruction-driven fine-tuning of generative models, enabling superior compositional control and fidelity to user intent.</li>
<li><strong>摘要：</strong>在使用 LoRA 等单片适配器时，针对各种多条件任务的扩散变压器 (DiT) 的参数高效微调通常会受到任务干扰。低等级专家混合 (MoLE) 架构提供了模块化解决方案，但其潜力通常受到在令牌级别运行的路由策略的限制。这种本地路由可能与用户指令的全局性质相冲突，导致复杂图像生成任务中出现空间碎片和语义漂移等伪影。为了解决这些限制，我们引入了 InstructMoLE，这是一种采用低级专家指令引导混合的新颖框架。 InstructMoLE 不是按令牌进行路由，而是利用从用户的综合指令派生的全局路由信号、指令引导路由 (IGR)。这确保了单个、一致选择的专家委员会在所有输入令牌中统一应用，从而保留生成过程的全局语义和结构完整性。为了补充这一点，我们引入了输出空间正交性损失，它促进了专家功能多样性并减轻了表征崩溃。大量实验表明，InstructMoLE 在具有挑战性的多条件生成基准测试中显着优于现有的 LoRA 适配器和 MoLE 变体。我们的工作提供了一个强大且可概括的框架，用于指令驱动的生成模型微调，从而实现卓越的成分控制和对用户意图的保真度。</li>
</ul>

<h3>Title: Synthetic Financial Data Generation for Enhanced Financial Modelling</h3>
<ul>
<li><strong>Authors: </strong>Christophe D. Hounwanou, Yae Ulrich Gaba, Pierre Ntakirutimana</a></li>
<li><strong>Subjects: </strong>cs.LG, q-fin.CP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21791">https://arxiv.org/abs/2512.21791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21791">https://arxiv.org/pdf/2512.21791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21791]] Synthetic Financial Data Generation for Enhanced Financial Modelling(https://arxiv.org/abs/2512.21791)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Data scarcity and confidentiality in finance often impede model development and robust testing. This paper presents a unified multi-criteria evaluation framework for synthetic financial data and applies it to three representative generative paradigms: the statistical ARIMA-GARCH baseline, Variational Autoencoders (VAEs), and Time-series Generative Adversarial Networks (TimeGAN). Using historical S and P 500 daily data, we evaluate fidelity (Maximum Mean Discrepancy, MMD), temporal structure (autocorrelation and volatility clustering), and practical utility in downstream tasks, specifically mean-variance portfolio optimization and volatility forecasting. Empirical results indicate that ARIMA-GARCH captures linear trends and conditional volatility but fails to reproduce nonlinear dynamics; VAEs produce smooth trajectories that underestimate extreme events; and TimeGAN achieves the best trade-off between realism and temporal coherence (e.g., TimeGAN attained the lowest MMD: 1.84e-3, average over 5 seeds). Finally, we articulate practical guidelines for selecting generative models according to application needs and computational constraints. Our unified evaluation protocol and reproducible codebase aim to standardize benchmarking in synthetic financial data research.</li>
<li><strong>摘要：</strong>金融领域的数据稀缺和保密性往往会阻碍模型开发和稳健的测试。本文提出了一个针对合成金融数据的统一多标准评估框架，并将其应用于三种代表性的生成范式：统计 ARIMA-GARCH 基线、变分自动编码器（VAE）和时间序列生成对抗网络（TimeGAN）。使用历史 S 和 P 500 每日数据，我们评估保真度（最大均值差异，MMD）、时间结构（自相关和波动性聚类）以及下游任务中的实际效用，特别是均值方差投资组合优化和波动性预测。经验结果表明，ARIMA-GARCH 捕获了线性趋势和条件波动性，但无法重现非线性动态； VAE 产生低估极端事件的平滑轨迹； TimeGAN 在真实性和时间一致性之间实现了最佳权衡（例如，TimeGAN 达到了最低的 MMD：1.84e-3，平均超过 5 个种子）。最后，我们阐明了根据应用需求和计算限制选择生成模型的实用指南。我们统一的评估协议和可复制的代码库旨在标准化合成金融数据研究的基准测试。</li>
</ul>

<h3>Title: Diffusion Posterior Sampling for Super-Resolution under Gaussian Measurement Noise</h3>
<ul>
<li><strong>Authors: </strong>Abu Hanif Muhammad Syarubany</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21797">https://arxiv.org/abs/2512.21797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21797">https://arxiv.org/pdf/2512.21797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21797]] Diffusion Posterior Sampling for Super-Resolution under Gaussian Measurement Noise(https://arxiv.org/abs/2512.21797)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>This report studies diffusion posterior sampling (DPS) for single-image super-resolution (SISR) under a known degradation model. We implement a likelihood-guided sampling procedure that combines an unconditional diffusion prior with gradient-based conditioning to enforce measurement consistency for $4\times$ super-resolution with additive Gaussian noise. We evaluate posterior sampling (PS) conditioning across guidance scales and noise levels, using PSNR and SSIM as fidelity metrics and a combined selection score $(\mathrm{PSNR}/40)+\mathrm{SSIM}$. Our ablation shows that moderate guidance improves reconstruction quality, with the best configuration achieved at PS scale $0.95$ and noise standard deviation $\sigma=0.01$ (score $1.45231$). Qualitative results confirm that the selected PS setting restores sharper edges and more coherent facial details compared to the downsampled inputs, while alternative conditioning strategies (e.g., MCG and PS-annealed) exhibit different texture fidelity trade-offs. These findings highlight the importance of balancing diffusion priors and measurement-gradient strength to obtain stable, high-quality reconstructions without retraining the diffusion model for each operator.</li>
<li><strong>摘要：</strong>本报告研究了已知退化模型下单图像超分辨率 (SISR) 的扩散后验采样 (DPS)。我们实现了一种似然引导采样程序，将无条件扩散先验与基于梯度的调节相结合，以增强具有加性高斯噪声的 4\times$ 超分辨率的测量一致性。我们使用 PSNR 和 SSIM 作为保真度指标以及组合选择分数 $(\mathrm{PSNR}/40)+\mathrm{SSIM}$ 来评估跨引导尺度和噪声水平的后验采样 (PS) 调节。我们的消融表明，适度的指导可以提高重建质量，在 PS 规模 $0.95$ 和噪声标准偏差 $\sigma=0.01$ 下实现最佳配置（得分 $1.45231$）。定性结果证实，与下采样输入相比，所选的 PS 设置可以恢复更清晰的边缘和更连贯的面部细节，而替代调节策略（例如 MCG 和 PS 退火）则表现出不同的纹理保真度权衡。这些发现强调了平衡扩散先验和测量梯度强度以获得稳定、高质量重建而无需重新训练每个算子的扩散模型的重要性。</li>
</ul>

<h3>Title: Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mengqi He, Xinyu Tian, Xin Shen, Jinhong Ni, Shu Zou, Zhaoyuan Yang, Jing Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21815">https://arxiv.org/abs/2512.21815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21815">https://arxiv.org/pdf/2512.21815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21815]] Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models(https://arxiv.org/abs/2512.21815)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.</li>
<li><strong>摘要：</strong>视觉语言模型（VLM）取得了显着的性能，但仍然容易受到对抗性攻击。熵是模型不确定性的度量，与 VLM 的可靠性密切相关。先前基于熵的攻击最大化了所有解码步骤的不确定性，隐含地假设每个令牌对生成不稳定的贡献相同。相反，我们表明一小部分（约 20%）的高熵标记（即自回归生成中的关键决策点）不成比例地控制着输出轨迹。通过将对抗性扰动集中在这些位置上，我们在使用更少的预算的情况下实现了与全局方法相当的语义退化。更重要的是，在多个代表性的 VLM 中，这种选择性攻击将 35-49% 的良性输出转化为有害输出，从而暴露出更严重的安全风险。值得注意的是，这些脆弱的高熵分叉在架构不同的 VLM 中重复出现，从而实现了可行的可转移性（对看不见的目标的有害率为 17-26%）。受这些发现的启发，我们提出了熵库引导对抗攻击（EGA），它实现了竞争性攻击成功率（93-95%）以及高有害转换，从而揭示了当前 VLM 安全机制的新弱点。</li>
</ul>

<h3>Title: EasyOmnimatte: Taming Pretrained Inpainting Diffusion Models for End-to-End Video Layered Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Yihan Hu, Xuelin Chen, Xiaodong Cun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21865">https://arxiv.org/abs/2512.21865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21865">https://arxiv.org/pdf/2512.21865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21865]] EasyOmnimatte: Taming Pretrained Inpainting Diffusion Models for End-to-End Video Layered Decomposition(https://arxiv.org/abs/2512.21865)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Existing video omnimatte methods typically rely on slow, multi-stage, or inference-time optimization pipelines that fail to fully exploit powerful generative priors, producing suboptimal decompositions. Our key insight is that, if a video inpainting model can be finetuned to remove the foreground-associated effects, then it must be inherently capable of perceiving these effects, and hence can also be finetuned for the complementary task: foreground layer decomposition with associated effects. However, although naïvely finetuning the inpainting model with LoRA applied to all blocks can produce high-quality alpha mattes, it fails to capture associated effects. Our systematic analysis reveals this arises because effect-related cues are primarily encoded in specific DiT blocks and become suppressed when LoRA is applied across all blocks. To address this, we introduce EasyOmnimatte, the first unified, end-to-end video omnimatte method. Concretely, we finetune a pretrained video inpainting diffusion model to learn dual complementary experts while keeping its original weights intact: an Effect Expert, where LoRA is applied only to effect-sensitive DiT blocks to capture the coarse structure of the foreground and associated effects, and a fully LoRA-finetuned Quality Expert learns to refine the alpha matte. During sampling, Effect Expert is used for denoising at early, high-noise steps, while Quality Expert takes over at later, low-noise steps. This design eliminates the need for two full diffusion passes, significantly reducing computational cost without compromising output quality. Ablation studies validate the effectiveness of this Dual-Expert strategy. Experiments demonstrate that EasyOmnimatte sets a new state-of-the-art for video omnimatte and enables various downstream tasks, significantly outperforming baselines in both quality and efficiency.</li>
<li><strong>摘要：</strong>现有的视频全向方法通常依赖于缓慢的、多阶段的或推理时间的优化管道，这些管道无法充分利用强大的生成先验，从而产生次优的分解。我们的主要见解是，如果可以对视频修复模型进行微调以消除前景相关的效果，那么它本质上必须能够感知这些效果，因此也可以针对补充任务进行微调：具有相关效果的前景层分解。然而，尽管通过将 LoRA 应用于所有块来简单地微调修复模型可以产生高质量的 alpha 遮罩，但它无法捕获相关效果。我们的系统分析表明，出现这种情况是因为与效果相关的线索主要编码在特定的 DiT 块中，当 LoRA 应用于所有块时，这些线索会被抑制。为了解决这个问题，我们引入了 EasyOmnimatte，这是第一个统一的端到端视频全遮光方法。具体来说，我们微调预训练的视频修复扩散模型，以学习双重互补专家，同时保持其原始权重不变：效果专家，其中 LoRA 仅应用于效果敏感的 DiT 块以捕获前景的粗略结构和相关效果，而完全经过 LoRA 微调的质量专家学习细化 alpha 遮罩。在采样过程中，Effect Expert 用于在早期的高噪声步骤中进行降噪，而 Quality Expert 在后期的低噪声步骤中进行降噪。这种设计消除了两次完整扩散过程的需要，显着降低了计算成本，而不会影响输出质量。消融研究验证了这种双专家策略的有效性。实验表明，EasyOmnimatte 为视频全遮光设定了新的最先进水平，并支持各种下游任务，在质量和效率方面显着优于基准。</li>
</ul>

<h3>Title: DPAR: Dynamic Patchification for Efficient Autoregressive Visual Generation</h3>
<ul>
<li><strong>Authors: </strong>Divyansh Srivastava, Akshay Mehra, Pranav Maneriker, Debopam Sanyal, Vishnu Raj, Vijay Kamarshi, Fan Du, Joshua Kimball</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21867">https://arxiv.org/abs/2512.21867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21867">https://arxiv.org/pdf/2512.21867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21867]] DPAR: Dynamic Patchification for Efficient Autoregressive Visual Generation(https://arxiv.org/abs/2512.21867)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Decoder-only autoregressive image generation typically relies on fixed-length tokenization schemes whose token counts grow quadratically with resolution, substantially increasing the computational and memory demands of attention. We present DPAR, a novel decoder-only autoregressive model that dynamically aggregates image tokens into a variable number of patches for efficient image generation. Our work is the first to demonstrate that next-token prediction entropy from a lightweight and unsupervised autoregressive model provides a reliable criterion for merging tokens into larger patches based on information content. DPAR makes minimal modifications to the standard decoder architecture, ensuring compatibility with multimodal generation frameworks and allocating more compute to generation of high-information image regions. Further, we demonstrate that training with dynamically sized patches yields representations that are robust to patch boundaries, allowing DPAR to scale to larger patch sizes at inference. DPAR reduces token count by 1.81x and 2.06x on Imagenet 256 and 384 generation resolution respectively, leading to a reduction of up to 40% FLOPs in training costs. Further, our method exhibits faster convergence and improves FID by up to 27.1% relative to baseline models.</li>
<li><strong>摘要：</strong>仅解码器的自回归图像生成通常依赖于固定长度的标记化方案，其标记计数随分辨率呈二次方增长，从而大大增加了注意力的计算和内存需求。我们提出了 DPAR，一种新颖的仅解码器自回归模型，它动态地将图像标记聚合成可变数量的补丁，以实现高效的图像生成。我们的工作首次证明了轻量级无监督自回归模型的下一个令牌预测熵为根据信息内容将令牌合并到更大的补丁中提供了可靠的标准。 DPAR 对标准解码器架构进行了最小的修改，确保与多模态生成框架的兼容性，并分配更多的计算来生成高信息图像区域。此外，我们证明使用动态大小的补丁进行训练可以产生对补丁边界具有鲁棒性的表示，从而允许 DPAR 在推理时扩展到更大的补丁大小。 DPAR 在 Imagenet 256 和 384 生成分辨率上分别减少了 1.81 倍和 2.06 倍的令牌计数，从而使训练成本减少高达 40% 的 FLOP。此外，我们的方法表现出更快的收敛速度，并且相对于基线模型，FID 提高了 27.1%。</li>
</ul>

<h3>Title: CrownGen: Patient-customized Crown Generation via Point Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Juyoung Bae, Moo Hyun Son, Jiale Peng, Wanting Qu, Wener Chen, Zelin Qiu, Kaixin Li, Xiaojuan Chen, Yifan Lin, Hao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21890">https://arxiv.org/abs/2512.21890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21890">https://arxiv.org/pdf/2512.21890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21890]] CrownGen: Patient-customized Crown Generation via Point Diffusion Model(https://arxiv.org/abs/2512.21890)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generation, generative</a></li>
<li><strong>Abstract: </strong>Digital crown design remains a labor-intensive bottleneck in restorative dentistry. We present \textbf{CrownGen}, a generative framework that automates patient-customized crown design using a denoising diffusion model on a novel tooth-level point cloud representation. The system employs two core components: a boundary prediction module to establish spatial priors and a diffusion-based generative module to synthesize high-fidelity morphology for multiple teeth in a single inference pass. We validated CrownGen through a quantitative benchmark on 496 external scans and a clinical study of 26 restoration cases. Results demonstrate that CrownGen surpasses state-of-the-art models in geometric fidelity and significantly reduces active design time. Clinical assessments by trained dentists confirmed that CrownGen-assisted crowns are statistically non-inferior in quality to those produced by expert technicians using manual workflows. By automating complex prosthetic modeling, CrownGen offers a scalable solution to lower costs, shorten turnaround times, and enhance patient access to high-quality dental care.</li>
<li><strong>摘要：</strong>数字牙冠设计仍然是修复牙科中的劳动密集型瓶颈。我们提出了 \textbf{CrownGen}，这是一个生成框架，它使用新颖的牙齿级点云表示上的降噪扩散模型来自动化患者定制的牙冠设计。该系统采用两个核心组件：用于建立空间先验的边界预测模块和用于在单次推理过程中合成多个牙齿的高保真形态的基于扩散的生成模块。我们通过 496 次外部扫描的定量基准和 26 个修复病例的临床研究对 CrownGen 进行了验证。结果表明，CrownGen 在几何保真度方面超越了最先进的模型，并显着缩短了主动设计时间。经过训练有素的牙医进行的临床评估证实，从统计数据来看，CrownGen 辅助牙冠的质量不低于专家技术人员使用手动工作流程制作的牙冠。通过自动化复杂的修复体建模，CrownGen 提供了可扩展的解决方案，以降低成本、缩短周转时间并增强患者获得高质量牙科护理的机会。</li>
</ul>

<h3>Title: High-Fidelity and Long-Duration Human Image Animation with Diffusion Transformer</h3>
<ul>
<li><strong>Authors: </strong>Shen Zheng, Jiaran Cai, Yuansheng Guan, Shenneng Huang, Xingpei Ma, Junjie Cao, Hanfeng Zhao, Qiang Zhang, Shunsi Zhang, Xiao-Ping Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21905">https://arxiv.org/abs/2512.21905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21905">https://arxiv.org/pdf/2512.21905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21905]] High-Fidelity and Long-Duration Human Image Animation with Diffusion Transformer(https://arxiv.org/abs/2512.21905)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent progress in diffusion models has significantly advanced the field of human image animation. While existing methods can generate temporally consistent results for short or regular motions, significant challenges remain, particularly in generating long-duration videos. Furthermore, the synthesis of fine-grained facial and hand details remains under-explored, limiting the applicability of current approaches in real-world, high-quality applications. To address these limitations, we propose a diffusion transformer (DiT)-based framework which focuses on generating high-fidelity and long-duration human animation videos. First, we design a set of hybrid implicit guidance signals and a sharpness guidance factor, enabling our framework to additionally incorporate detailed facial and hand features as guidance. Next, we incorporate the time-aware position shift fusion module, modify the input format within the DiT backbone, and refer to this mechanism as the Position Shift Adaptive Module, which enables video generation of arbitrary length. Finally, we introduce a novel data augmentation strategy and a skeleton alignment model to reduce the impact of human shape variations across different identities. Experimental results demonstrate that our method outperforms existing state-of-the-art approaches, achieving superior performance in both high-fidelity and long-duration human image animation.</li>
<li><strong>摘要：</strong>扩散模型的最新进展极大地推进了人类图像动画领域的发展。虽然现有方法可以为短时间或规则运动生成时间一致的结果，但仍然存在重大挑战，特别是在生成长时间视频方面。此外，细粒度面部和手部细节的合成仍未得到充分探索，限制了当前方法在现实世界的高质量应用中的适用性。为了解决这些限制，我们提出了一种基于扩散变压器（DiT）的框架，该框架专注于生成高保真和长时间的人类动画视频。首先，我们设计了一组混合隐式引导信号和锐度引导因子，使我们的框架能够另外纳入详细的面部和手部特征作为指导。接下来，我们结合了时间感知位置移位融合模块，修改了 DiT 主干内的输入格式，并将该机制称为位置移位自适应模块，它可以生成任意长度的视频。最后，我们引入了一种新颖的数据增强策略和骨架对齐模型，以减少不同身份的人体形状变化的影响。实验结果表明，我们的方法优于现有的最先进方法，在高保真和长时间的人类图像动画方面实现了卓越的性能。</li>
</ul>

<h3>Title: Exploring the Heterogeneity of Tabular Data: A Diversity-aware Data Generator via LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yafeng Tang, Xiaoou Ding, Jianzhuo Du, Zishuo Yan, Zhuang Ma, Zheng Liang, Zekai Qian, Hongzhi Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21915">https://arxiv.org/abs/2512.21915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21915">https://arxiv.org/pdf/2512.21915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21915]] Exploring the Heterogeneity of Tabular Data: A Diversity-aware Data Generator via LLMs(https://arxiv.org/abs/2512.21915)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Tabular data generation has become increasingly essential for enabling robust machine learning applications, which require large-scale, high-quality data. Existing solutions leverage generative models to learn original data distributions. However, real-world data are naturally heterogeneous with diverse distributions, making it challenging to obtain a universally good model for diverse data generation. To address this limitation, we introduce Diversity-Aware Tabular data gEnerator (DATE), a framework that (i) prepares high-quality and distributionally distinct examples for in-context learning by effectively partitioning the original heterogeneous data into multiple diverse subsets; (ii) harnesses Large Language Models (LLMs) to explore the diversity of the partitioned distribution with decision tree reasoning as feedback, generating high-quality labeled data for each subset. However, the massive generated data inherently involves a trade-off between diversity and quality. To integrate this issue, existing solutions greedily select the validation-best data. However, we prove that the selection in heterogeneous settings does not possess the greedy-choice property, and design a Multi-Arm Bandit-based sampling algorithm that balances the diversity and quality of generated data. Extensive experiments on tabular classification and regression benchmarks demonstrate that DATE consistently outperforms state-of-the-art GAN-based and LLM-based methods. On average, DATE achieves a 23.75% reduction in error rate with just 100 generated data. Empirically, we demonstrate that data generated by DATE can improve the accuracy of Direct Preference Optimization (DPO) and enhance the reasoning capability of LLMs on the target data. Code is available at this https URL.</li>
<li><strong>摘要：</strong>表格数据生成对于实现强大的机器学习应用程序变得越来越重要，这些应用程序需要大规模、高质量的数据。现有的解决方案利用生成模型来学习原始数据分布。然而，现实世界的数据本质上是异构的，具有不同的分布，这使得为不同的数据生成获得普遍良好的模型具有挑战性。为了解决这一限制，我们引入了多样性感知表格数据生成器（DATE），该框架（i）通过有效地将原始异构数据划分为多个不同的子集，为上下文学习准备高质量且分布不同的示例； (ii) 利用大型语言模型 (LLM) 探索分区分布的多样性，并以决策树推理作为反馈，为每个子集生成高质量的标记数据。然而，海量生成的数据本质上涉及多样性和质量之间的权衡。为了解决这个问题，现有的解决方案贪婪地选择验证最佳数据。然而，我们证明异构设置中的选择不具有贪婪选择属性，并设计了一种基于多臂强盗的采样算法来平衡生成数据的多样性和质量。对表格分类和回归基准的大量实验表明，DATE 始终优于最先进的基于 GAN 和基于 LLM 的方法。平均而言，DATE 仅生成 100 个数据，错误率就降低了 23.75%。根据经验，我们证明了 DATE 生成的数据可以提高直接偏好优化（DPO）的准确性，并增强 LLM 对目标数据的推理能力。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: AutoPP: Towards Automated Product Poster Generation and Optimization</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Fan, Yuxin Qin, Wei Feng, Yanyin Chen, Yaoyu Li, Ao Ma, Yixiu Li, Li Zhuang, Haoyi Bian, Zheng Zhang, Jingjing Lv, Junjie Shen, Ching Law</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21921">https://arxiv.org/abs/2512.21921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21921">https://arxiv.org/pdf/2512.21921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21921]] AutoPP: Towards Automated Product Poster Generation and Optimization(https://arxiv.org/abs/2512.21921)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Product posters blend striking visuals with informative text to highlight the product and capture customer attention. However, crafting appealing posters and manually optimizing them based on online performance is laborious and resource-consuming. To address this, we introduce AutoPP, an automated pipeline for product poster generation and optimization that eliminates the need for human intervention. Specifically, the generator, relying solely on basic product information, first uses a unified design module to integrate the three key elements of a poster (background, text, and layout) into a cohesive output. Then, an element rendering module encodes these elements into condition tokens, efficiently and controllably generating the product poster. Based on the generated poster, the optimizer enhances its Click-Through Rate (CTR) by leveraging online feedback. It systematically replaces elements to gather fine-grained CTR comparisons and utilizes Isolated Direct Preference Optimization (IDPO) to attribute CTR gains to isolated elements. Our work is supported by AutoPP1M, the largest dataset specifically designed for product poster generation and optimization, which contains one million high-quality posters and feedback collected from over one million users. Experiments demonstrate that AutoPP achieves state-of-the-art results in both offline and online settings. Our code and dataset are publicly available at: this https URL</li>
<li><strong>摘要：</strong>产品海报将引人注目的视觉效果与信息丰富的文字融为一体，以突出产品并吸引客户的注意力。然而，制作吸引人的海报并根据在线表现手动优化它们是费力且消耗资源的。为了解决这个问题，我们引入了 AutoPP，这是一种用于产品海报生成和优化的自动化管道，无需人工干预。具体来说，生成器仅依靠基本的产品信息，首先使用统一的设计模块将海报的三个关键元素（背景、文本和布局）整合为一个有凝聚力的输出。然后，元素渲染模块将这些元素编码为条件标记，高效可控地生成产品海报。根据生成的海报，优化器通过利用在线反馈来提高其点击率 (CTR)。它系统地替换元素以收集细粒度的 CTR 比较，并利用孤立直接偏好优化 (IDPO) 将 CTR 增益归因于孤立的元素。我们的工作得到 AutoPP1M 的支持，这是专门为产品海报生成和优化而设计的最大数据集，其中包含 100 万张高质量海报以及从超过 100 万用户收集的反馈。实验表明，AutoPP 在离线和在线设置中均取得了最先进的结果。我们的代码和数据集可在以下位置公开获取：此 https URL</li>
</ul>

<h3>Title: Unsupervised Anomaly Detection in Brain MRI via Disentangled Anatomy Learning</h3>
<ul>
<li><strong>Authors: </strong>Tao Yang, Xiuying Wang, Hao Liu, Guanzhong Gong, Lian-Ming Wu, Yu-Ping Wang, Lisheng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21924">https://arxiv.org/abs/2512.21924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21924">https://arxiv.org/pdf/2512.21924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21924]] Unsupervised Anomaly Detection in Brain MRI via Disentangled Anatomy Learning(https://arxiv.org/abs/2512.21924)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Detection of various lesions in brain MRI is clinically critical, but challenging due to the diversity of lesions and variability in imaging conditions. Current unsupervised learning methods detect anomalies mainly through reconstructing abnormal images into pseudo-healthy images (PHIs) by normal samples learning and then analyzing differences between images. However, these unsupervised models face two significant limitations: restricted generalizability to multi-modality and multi-center MRIs due to their reliance on the specific imaging information in normal training data, and constrained performance due to abnormal residuals propagated from input images to reconstructed PHIs. To address these limitations, two novel modules are proposed, forming a new PHI reconstruction framework. Firstly, the disentangled representation module is proposed to improve generalizability by decoupling brain MRI into imaging information and essential imaging-invariant anatomical images, ensuring that the reconstruction focuses on the anatomy. Specifically, brain anatomical priors and a differentiable one-hot encoding operator are introduced to constrain the disentanglement results and enhance the disentanglement stability. Secondly, the edge-to-image restoration module is designed to reconstruct high-quality PHIs by restoring the anatomical representation from the high-frequency edge information of anatomical images, and then recoupling the disentangled imaging information. This module not only suppresses abnormal residuals in PHI by reducing abnormal pixels input through edge-only input, but also effectively reconstructs normal regions using the preserved structural details in the edges. Evaluated on nine public datasets (4,443 patients' MRIs from multiple centers), our method outperforms 17 SOTA methods, achieving absolute improvements of +18.32% in AP and +13.64% in DSC.</li>
<li><strong>摘要：</strong>脑 MRI 中各种病变的检测在临床上至关重要，但由于病变的多样性和成像条件的可变性，具有挑战性。目前的无监督学习方法主要通过正常样本学习将异常图像重建为伪健康图像（PHI），然后分析图像之间的差异来检测异常。然而，这些无监督模型面临两个显着的局限性：由于它们依赖于正常训练数据中的特定成像信息，因此对多模态和多中心 MRI 的通用性受到限制；以及由于从输入图像传播到重建 PHI 的异常残差而导致性能受到限制。为了解决这些限制，提出了两个新颖的模块，形成了新的 PHI 重建框架。首先，提出了解耦表示模块，通过将脑 MRI 解耦为成像信息和基本成像不变的解剖图像来提高泛化性，确保重建集中于解剖结构。具体来说，引入大脑解剖先验和可微的独热编码算子来约束解缠结结果并增强解缠结稳定性。其次，边缘到图像恢复模块旨在通过从解剖图像的高频边缘信息恢复解剖表示，然后重新耦合解开的成像信息来重建高质量的PHI。该模块不仅通过仅边缘输入减少异常像素输入来抑制PHI中的异常残差，而且还利用边缘中保留的结构细节有效地重建正常区域。在 9 个公共数据集（来自多个中心的 4,443 名患者的 MRI）上进行评估，我们的方法优于 17 个 SOTA 方法，在 AP 中实现 +18.32% 的绝对改进，在 DSC 中实现 +13.64% 的绝对改进。</li>
</ul>

<h3>Title: Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs</h3>
<ul>
<li><strong>Authors: </strong>Jiayu Hu, Beibei Li, Jiangwei Xia, Yanjun Qin, Bing Ji, Zhongshi He</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.21999">https://arxiv.org/abs/2512.21999</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.21999">https://arxiv.org/pdf/2512.21999</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.21999]] Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs(https://arxiv.org/abs/2512.21999)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While Vision-Language Models (VLMs) have garnered increasing attention in the AI community due to their promising practical applications, they exhibit persistent hallucination issues, generating outputs misaligned with visual inputs. Recent studies attribute these hallucinations to VLMs' over-reliance on linguistic priors and insufficient visual feature integration, proposing heuristic decoding calibration strategies to mitigate them. However, the non-trainable nature of these strategies inherently limits their optimization potential. To this end, we propose an adversarial parametric editing framework for Hallucination mitigation in VLMs, which follows an \textbf{A}ctivate-\textbf{L}ocate-\textbf{E}dit \textbf{A}dversarially paradigm. Specifically, we first construct an activation dataset that comprises grounded responses (positive samples attentively anchored in visual features) and hallucinatory responses (negative samples reflecting LLM prior bias and internal knowledge artifacts). Next, we identify critical hallucination-prone parameter clusters by analyzing differential hidden states of response pairs. Then, these clusters are fine-tuned using prompts injected with adversarial tuned prefixes that are optimized to maximize visual neglect, thereby forcing the model to prioritize visual evidence over inherent parametric biases. Evaluations on both generative and discriminative VLM tasks demonstrate the significant effectiveness of ALEAHallu in alleviating hallucinations. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>虽然视觉语言模型（VLM）因其有前景的实际应用而在人工智能社区中获得了越来越多的关注，但它们表现出持续的幻觉问题，产生与视觉输入不一致的输出。最近的研究将这些幻觉归因于 VLM 过度依赖语言先验和视觉特征集成不足，并提出启发式解码校准策略来缓解这些幻觉。然而，这些策略的不可训练性质本质上限制了它们的优化潜力。为此，我们提出了一种用于 VLM 中幻觉缓解的对抗性参数编辑框架，该框架遵循 \textbf{A}ctivate-\textbf{L}ocate-\textbf{E}dit \textbf{A}dversarially 范式。具体来说，我们首先构建一个激活数据集，其中包括接地响应（专注于视觉特征的正样本）和幻觉响应（反映 LLM 先验偏差和内部知识工件的负样本）。接下来，我们通过分析响应对的差异隐藏状态来识别关键的易产生幻觉的参数簇。然后，使用注入对抗性调整前缀的提示对这些集群进行微调，这些前缀经过优化以最大限度地提高视觉忽略率，从而迫使模型优先考虑视觉证据而不是固有的参数偏差。对生成性和判别性 VLM 任务的评估表明 ALEAHallu 在减轻幻觉方面具有显着效果。我们的代码可以在这个 https URL 上找到。</li>
</ul>

<h3>Title: Patch-Discontinuity Mining for Generalized Deepfake Detection</h3>
<ul>
<li><strong>Authors: </strong>Huanhuan Yuan, Yang Ping, Zhengqin Xu, Junyi Cao, Shuai Jia, Chao Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.22027">https://arxiv.org/abs/2512.22027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.22027">https://arxiv.org/pdf/2512.22027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.22027]] Patch-Discontinuity Mining for Generalized Deepfake Detection(https://arxiv.org/abs/2512.22027)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of generative artificial intelligence has enabled the creation of highly realistic fake facial images, posing serious threats to personal privacy and the integrity of online information. Existing deepfake detection methods often rely on handcrafted forensic cues and complex architectures, achieving strong performance in intra-domain settings but suffering significant degradation when confronted with unseen forgery patterns. In this paper, we propose GenDF, a simple yet effective framework that transfers a powerful large-scale vision model to the deepfake detection task with a compact and neat network design. GenDF incorporates deepfake-specific representation learning to capture discriminative patterns between real and fake facial images, feature space redistribution to mitigate distribution mismatch, and a classification-invariant feature augmentation strategy to enhance generalization without introducing additional trainable parameters. Extensive experiments demonstrate that GenDF achieves state-of-the-art generalization performance in cross-domain and cross-manipulation settings while requiring only 0.28M trainable parameters, validating the effectiveness and efficiency of the proposed framework.</li>
<li><strong>摘要：</strong>生成人工智能的快速发展使得能够创建高度逼真的虚假面部图像，对个人隐私和在线信息的完整性构成严重威胁。现有的深度伪造检测方法通常依赖于手工制作的取证线索和复杂的架构，在域内设置中实现了强大的性能，但在遇到看不见的伪造模式时会遭受显着的退化。在本文中，我们提出了 GenDF，这是一个简单而有效的框架，它通过紧凑而整洁的网络设计将强大的大规模视觉模型转移到深度伪造检测任务中。 GenDF 结合了 Deepfake 特定的表示学习来捕获真实和虚假面部图像之间的区分模式，特征空间重新分配以减轻分布不匹配，以及分类不变的特征增强策略来增强泛化，而无需引入额外的可训练参数。大量实验表明，GenDF 在跨域和交叉操作设置中实现了最先进的泛化性能，同时仅需要 0.28M 可训练参数，验证了所提出框架的有效性和效率。</li>
</ul>

<h3>Title: From In Silico to In Vitro: Evaluating Molecule Generative Models for Hit Generation</h3>
<ul>
<li><strong>Authors: </strong>Nagham Osman, Vittorio Lembo, Giovanni Bottegoni, Laura Toni</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.22031">https://arxiv.org/abs/2512.22031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.22031">https://arxiv.org/pdf/2512.22031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.22031]] From In Silico to In Vitro: Evaluating Molecule Generative Models for Hit Generation(https://arxiv.org/abs/2512.22031)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Hit identification is a critical yet resource-intensive step in the drug discovery pipeline, traditionally relying on high-throughput screening of large compound libraries. Despite advancements in virtual screening, these methods remain time-consuming and costly. Recent progress in deep learning has enabled the development of generative models capable of learning complex molecular representations and generating novel compounds de novo. However, using ML to replace the entire drug-discovery pipeline is highly challenging. In this work, we rather investigate whether generative models can replace one step of the pipeline: hit-like molecule generation. To the best of our knowledge, this is the first study to explicitly frame hit-like molecule generation as a standalone task and empirically test whether generative models can directly support this stage of the drug discovery pipeline. Specifically, we investigate if such models can be trained to generate hit-like molecules, enabling direct incorporation into, or even substitution of, traditional hit identification workflows. We propose an evaluation framework tailored to this task, integrating physicochemical, structural, and bioactivity-related criteria within a multi-stage filtering pipeline that defines the hit-like chemical space. Two autoregressive and one diffusion-based generative models were benchmarked across various datasets and training settings, with outputs assessed using standard metrics and target-specific docking scores. Our results show that these models can generate valid, diverse, and biologically relevant compounds across multiple targets, with a few selected GSK-3$\beta$ hits synthesized and confirmed active in vitro. We also identify key limitations in current evaluation metrics and available training data.</li>
<li><strong>摘要：</strong>命中鉴定是药物发现流程中关键但资源密集的步骤，传统上依赖于大型化合物库的高通量筛选。尽管虚拟筛选取得了进步，但这些方法仍然耗时且昂贵。深度学习的最新进展使得生成模型的开发能够学习复杂的分子表示并从头生成新的化合物。然而，使用机器学习来取代整个药物发现流程极具挑战性。在这项工作中，我们宁愿研究生成模型是否可以取代管道中的一个步骤：类命中分子生成。据我们所知，这是第一项明确将类命中分子生成作为一项独立任务的研究，并凭经验测试生成模型是否可以直接支持药物发现流程的这一阶段。具体来说，我们研究是否可以训练此类模型来生成类似命中的分子，从而能够直接纳入甚至替代传统的命中识别工作流程。我们提出了一个针对此任务量身定制的评估框架，将物理化学、结构和生物活性相关标准整合到定义命中类化学空间的多级过滤管道中。两种自回归和一种基于扩散的生成模型在各种数据集和训练设置中进行了基准测试，并使用标准指标和特定目标对接分数评估输出。我们的结果表明，这些模型可以跨多个靶点生成有效、多样化且具有生物学相关性的化合物，其中合成了一些选定的 GSK-3$\beta$ 化合物并在体外证实具有活性。我们还确定了当前评估指标和可用培训数据的主要局限性。</li>
</ul>

<h3>Title: MAI-UI Technical Report: Real-World Centric Foundation GUI Agents</h3>
<ul>
<li><strong>Authors: </strong>Hanzhang Zhou, Xu Zhang, Panrong Tong, Jianan Zhang, Liangyu Chen, Quyu Kong, Chenglin Cai, Chen Liu, Yue Wang, Jingren Zhou, Steven Hoi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.22047">https://arxiv.org/abs/2512.22047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.22047">https://arxiv.org/pdf/2512.22047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.22047]] MAI-UI Technical Report: Real-World Centric Foundation GUI Agents(https://arxiv.org/abs/2512.22047)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The development of GUI agents could revolutionize the next generation of human-computer interaction. Motivated by this vision, we present MAI-UI, a family of foundation GUI agents spanning the full spectrum of sizes, including 2B, 8B, 32B, and 235B-A22B variants. We identify four key challenges to realistic deployment: the lack of native agent-user interaction, the limits of UI-only operation, the absence of a practical deployment architecture, and brittleness in dynamic environments. MAI-UI addresses these issues with a unified methodology: a self-evolving data pipeline that expands the navigation data to include user interaction and MCP tool calls, a native device-cloud collaboration system routes execution by task state, and an online RL framework with advanced optimizations to scale parallel environments and context length. MAI-UI establishes new state-of-the-art across GUI grounding and mobile navigation. On grounding benchmarks, it reaches 73.5% on ScreenSpot-Pro, 91.3% on MMBench GUI L2, 70.9% on OSWorld-G, and 49.2% on UI-Vision, surpassing Gemini-3-Pro and Seed1.8 on ScreenSpot-Pro. On mobile GUI navigation, it sets a new SOTA of 76.7% on AndroidWorld, surpassing UI-Tars-2, Gemini-2.5-Pro and Seed1.8. On MobileWorld, MAI-UI obtains 41.7% success rate, significantly outperforming end-to-end GUI models and competitive with Gemini-3-Pro based agentic frameworks. Our online RL experiments show significant gains from scaling parallel environments from 32 to 512 (+5.2 points) and increasing environment step budget from 15 to 50 (+4.3 points). Finally, the native device-cloud collaboration system improves on-device performance by 33%, reduces cloud model calls by over 40%, and preserves user privacy.</li>
<li><strong>摘要：</strong>GUI 代理的开发可能会彻底改变下一代人机交互。受这一愿景的推动，我们推出了 MAI-UI，这是一系列涵盖各种尺寸的基础 GUI 代理，包括 2B、8B、32B 和 235B-A22B 变体。我们确定了实际部署的四个关键挑战：缺乏本机代理与用户交互、仅 UI 操作的限制、缺乏实用的部署架构以及动态环境中的脆弱性。 MAI-UI 通过统一的方法解决了这些问题：自我演化的数据管道，可扩展导航数据以包括用户交互和 MCP 工具调用，本机设备-云协作系统按任务状态路由执行，以及具有高级优化功能的在线 RL 框架，可扩展并行环境和上下文长度。 MAI-UI 在 GUI 基础和移动导航方面建立了新的最先进技术。在接地基准测试中，它在 ScreenSpot-Pro 上达到 73.5%，在 MMBench GUI L2 上达到 91.3%，在 OSWorld-G 上达到 70.9%，在 UI-Vision 上达到 49.2%，超过了 Gemini-3-Pro 和 ScreenSpot-Pro 上的 Seed1.8。在移动GUI导航方面，它在AndroidWorld上刷新了76.7%的SOTA，超越了UI-Tars-2、Gemini-2.5-Pro和Seed1.8。在 MobileWorld 上，MAI-UI 获得了 41.7% 的成功率，显着优于端到端 GUI 模型，并且与基于 Gemini-3-Pro 的代理框架具有竞争力。我们的在线 RL 实验显示，将并行环境从 32 个扩展到 512 个（+5.2 点）以及将环境步骤预算从 15 增加到 50 个（+4.3 点），可以获得显着收益。最后，原生端云协同系统将端端性能提升33%，云模型调用减少40%以上，并保护用户隐私。</li>
</ul>

<h3>Title: StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars</h3>
<ul>
<li><strong>Authors: </strong>Zhiyao Sun, Ziqiao Peng, Yifeng Ma, Yi Chen, Zhengguang Zhou, Zixiang Zhou, Guozhen Zhang, Youliang Zhang, Yuan Zhou, Qinglin Lu, Yong-Jin Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.22065">https://arxiv.org/abs/2512.22065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.22065">https://arxiv.org/pdf/2512.22065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.22065]] StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars(https://arxiv.org/abs/2512.22065)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Real-time, streaming interactive avatars represent a critical yet challenging goal in digital human research. Although diffusion-based human avatar generation methods achieve remarkable success, their non-causal architecture and high computational costs make them unsuitable for streaming. Moreover, existing interactive approaches are typically limited to head-and-shoulder region, limiting their ability to produce gestures and body motions. To address these challenges, we propose a two-stage autoregressive adaptation and acceleration framework that applies autoregressive distillation and adversarial refinement to adapt a high-fidelity human video diffusion model for real-time, interactive streaming. To ensure long-term stability and consistency, we introduce three key components: a Reference Sink, a Reference-Anchored Positional Re-encoding (RAPR) strategy, and a Consistency-Aware Discriminator. Building on this framework, we develop a one-shot, interactive, human avatar model capable of generating both natural talking and listening behaviors with coherent gestures. Extensive experiments demonstrate that our method achieves state-of-the-art performance, surpassing existing approaches in generation quality, real-time efficiency, and interaction naturalness. Project page: this https URL .</li>
<li><strong>摘要：</strong>实时、流媒体交互式化身代表了数字人类研究中一个关键但具有挑战性的目标。尽管基于扩散的人体头像生成方法取得了显着的成功，但其非因果架构和高计算成本使它们不适合流式传输。此外，现有的交互方法通常仅限于头肩区域，限制了它们产生手势和身体运动的能力。为了应对这些挑战，我们提出了一个两阶段自回归适应和加速框架，该框架应用自回归蒸馏和对抗性细化来适应实时交互式流媒体的高保真人类视频扩散模型。为了确保长期稳定性和一致性，我们引入了三个关键组件：参考接收器、参考锚定位置重编码（RAPR）策略和一致性感知鉴别器。在此框架的基础上，我们开发了一种一次性、交互式的人类化身模型，能够通过连贯的手势生成自然的交谈和倾听行为。大量的实验表明，我们的方法实现了最先进的性能，在生成质量、实时效率和交互自然度方面超越了现有方法。项目页面：此 https URL 。</li>
</ul>

<h3>Title: Yume-1.5: A Text-Controlled Interactive World Generation Model</h3>
<ul>
<li><strong>Authors: </strong>Xiaofeng Mao, Zhen Li, Chuanhao Li, Xiaojie Xu, Kaining Ying, Tong He, Jiangmiao Pang, Yu Qiao, Kaipeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.22096">https://arxiv.org/abs/2512.22096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.22096">https://arxiv.org/pdf/2512.22096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.22096]] Yume-1.5: A Text-Controlled Interactive World Generation Model(https://arxiv.org/abs/2512.22096)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt. \method achieves this through a carefully designed framework that supports keyboard-based exploration of the generated worlds. The framework comprises three core components: (1) a long-video generation framework integrating unified context compression with linear attention; (2) a real-time streaming acceleration strategy powered by bidirectional attention distillation and an enhanced text embedding scheme; (3) a text-controlled method for generating world events. We have provided the codebase in the supplementary material.</li>
<li><strong>摘要：</strong>最近的方法已经证明了使用扩散模型来生成交互式和可探索的世界的前景。然而，这些方法大多数都面临着严峻的挑战，例如参数大小过大、依赖冗长的推理步骤以及快速增长的历史背景，这些挑战严重限制了实时性能并缺乏文本控制的生成能力。为了应对这些挑战，我们提出了 \ 方法，这是一种新颖的框架，旨在从单个图像或文本提示生成现实的、交互式的和连续的世界。 \method 通过精心设计的框架来实现这一点，该框架支持基于键盘的生成世界的探索。该框架由三个核心组件组成：（1）集成了统一上下文压缩和线性注意力的长视频生成框架； （2）基于双向注意力蒸馏和增强文本嵌入方案的实时流加速策略； (3) 生成世界事件的文本控制方法。我们在补充材料中提供了代码库。</li>
</ul>

<h3>Title: A2P-Vis: an Analyzer-to-Presenter Agentic Pipeline for Visual Insights Generation and Reporting</h3>
<ul>
<li><strong>Authors: </strong>Shuyu Gan, Renxiang Wang, James Mooney, Dongyeop Kang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.22101">https://arxiv.org/abs/2512.22101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.22101">https://arxiv.org/pdf/2512.22101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.22101]] A2P-Vis: an Analyzer-to-Presenter Agentic Pipeline for Visual Insights Generation and Reporting(https://arxiv.org/abs/2512.22101)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Automating end-to-end data science pipeline with AI agents still stalls on two gaps: generating insightful, diverse visual evidence and assembling it into a coherent, professional report. We present A2P-Vis, a two-part, multi-agent pipeline that turns raw datasets into a high-quality data-visualization report. The Data Analyzer orchestrates profiling, proposes diverse visualization directions, generates and executes plotting code, filters low-quality figures with a legibility checker, and elicits candidate insights that are automatically scored for depth, correctness, specificity, depth and actionability. The Presenter then orders topics, composes chart-grounded narratives from the top-ranked insights, writes justified transitions, and revises the document for clarity and consistency, yielding a coherent, publication-ready report. Together, these agents convert raw data into curated materials (charts + vetted insights) and into a readable narrative without manual glue work. We claim that by coupling a quality-assured Analyzer with a narrative Presenter, A2P-Vis operationalizes co-analysis end-to-end, improving the real-world usefulness of automated data analysis for practitioners. For the complete dataset report, please see: this https URL.</li>
<li><strong>摘要：</strong>使用人工智能代理实现端到端数据科学管道自动化仍然存在两个差距：生成富有洞察力、多样化的视觉证据，并将其组装成连贯、专业的报告。我们提出了 A2P-Vis，这是一个由两部分组成的多代理管道，可将原始数据集转换为高质量的数据可视化报告。数据分析器协调分析，提出不同的可视化方向，生成和执行绘图代码，使用易读性检查器过滤低质量的图形，并引出候选见解，并自动对深度、正确性、特异性、深度和可操作性进行评分。然后，演示者对主题进行排序，根据排名靠前的见解撰写基于图表的叙述，编写合理的过渡，并修改文档以确保清晰度和一致性，从而生成连贯的、可发布的报告。这些代理共同将原始数据转换为精选材料（图表+经过审查的见解）并转换为可读的叙述，而无需手动粘合工作。我们声称，通过将有质量保证的分析器与叙述性演示器相结合，A2P-Vis 实现了端到端的协同分析，从而提高了自动化数据分析在现实世界中对从业者的实用性。有关完整的数据集报告，请参阅：此 https URL。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
