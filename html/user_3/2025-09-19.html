<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-19</h1>
<h3>Title: Discovering New Theorems via LLMs with In-Context Proof Learning in Lean</h3>
<ul>
<li><strong>Authors: </strong>Kazumi Kasaura, Naoto Onda, Yuta Oriike, Masaya Taniguchi, Akiyoshi Sannai, Sho Sonoda</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14274">https://arxiv.org/abs/2509.14274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14274">https://arxiv.org/pdf/2509.14274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14274]] Discovering New Theorems via LLMs with In-Context Proof Learning in Lean(https://arxiv.org/abs/2509.14274)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models have demonstrated significant promise in formal theorem proving. However, previous works mainly focus on solving existing problems. In this paper, we focus on the ability of LLMs to find novel theorems. We propose Conjecturing-Proving Loop pipeline for automatically generating mathematical conjectures and proving them in Lean 4 format. A feature of our approach is that we generate and prove further conjectures with context including previously generated theorems and their proofs, which enables the generation of more difficult proofs by in-context learning of proof strategies without changing parameters of LLMs. We demonstrated that our framework rediscovered theorems with verification, which were published in past mathematical papers and have not yet formalized. Moreover, at least one of these theorems could not be proved by the LLM without in-context learning, even in natural language, which means that in-context learning was effective for neural theorem proving. The source code is available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型在正式定理证明中表现出了巨大的希望。但是，以前的工作主要集中于解决现有问题。在本文中，我们专注于LLM找到新定理的能力。我们提出了猜想的循环管道，以自动生成数学猜想并以精益4格式证明它们。我们方法的一个特征是，我们生成并证明了与上下文有关的猜想，包括先前生成的定理及其证明，这可以通过在不更改LLMS参数的情况下通过在不更改LLM的参数的情况下通过在内，可以生成更困难的证明。我们证明了我们的框架通过验证重新发现了定理，这些定理已在过去的数学论文中发表，但尚未正式化。此外，在没有文章学习的情况下，即使在自然语言中，LLM也无法证明这些定理中的至少一个，这意味着对神经定理证明是有效的。源代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: AToken: A Unified Tokenizer for Vision</h3>
<ul>
<li><strong>Authors: </strong>Jiasen Lu, Liangchen Song, Mingze Xu, Byeongjoo Ahn, Yanjun Wang, Chen Chen, Afshin Dehghan, Yinfei Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14476">https://arxiv.org/abs/2509.14476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14476">https://arxiv.org/pdf/2509.14476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14476]] AToken: A Unified Tokenizer for Vision(https://arxiv.org/abs/2509.14476)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present AToken, the first unified visual tokenizer that achieves both high-fidelity reconstruction and semantic understanding across images, videos, and 3D assets. Unlike existing tokenizers that specialize in either reconstruction or understanding for single modalities, AToken encodes these diverse visual inputs into a shared 4D latent space, unifying both tasks and modalities in a single framework. Specifically, we introduce a pure transformer architecture with 4D rotary position embeddings to process visual inputs of arbitrary resolutions and temporal durations. To ensure stable training, we introduce an adversarial-free training objective that combines perceptual and Gram matrix losses, achieving state-of-the-art reconstruction quality. By employing a progressive training curriculum, AToken gradually expands from single images, videos, and 3D, and supports both continuous and discrete latent tokens. AToken achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01 rFVD with 32.6% MSRVTT retrieval for videos, and 28.19 PSNR with 90.9% classification accuracy for 3D. In downstream applications, AToken enables both visual generation tasks (e.g., image generation with continuous and discrete tokens, text-to-video generation, image-to-3D synthesis) and understanding tasks (e.g., multimodal LLMs), achieving competitive performance across all benchmarks. These results shed light on the next-generation multimodal AI systems built upon unified visual tokenization.</li>
<li><strong>摘要：</strong>我们介绍了Atoken，这是第一个跨图像，视频和3D资产的高保真重建和语义理解的统一视觉令牌。与专门研究单个模式的重建或理解的现有图形不同，atoken将这些不同的视觉输入编码为共享的4D潜在空间，在单个框架中统一任务和模态。具体而言，我们引入了带有4D旋转位置嵌入的纯变压器体系结构，以处理任意分辨率和时间持续时间的视觉输入。为了确保稳定的培训，我们引入了一个无对抗性的训练目标，该目标结合了感知和革兰氏矩阵损失，以实现最新的重建质量。通过采用渐进培训课程，Atoken逐渐从单个图像，视频和3D扩展，并支持连续和离散的潜在令牌。 Atoken的图像具有82.2％Imagenet精度的0.21 RFID，视频的32.6％MSRVTT检索的3.01 RFVD和28.19 PSNR，3D的分类精度为90.9％。在下游应用程序中，Atoken可以启用视觉生成任务（例如，具有连续和离散令牌的图像生成，文本到视频生成，图像到3D综合）和理解任务（例如，多模式LLMS），在所有基准标准中实现竞争性能。这些结果阐明了下一代多模式AI系统建立在统一的视觉令牌上。</li>
</ul>

<h3>Title: Edge-Aware Normalized Attention for Efficient and Detail-Preserving Single Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Penghao Rao, Tieyong Zeng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14550">https://arxiv.org/abs/2509.14550</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14550">https://arxiv.org/pdf/2509.14550</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14550]] Edge-Aware Normalized Attention for Efficient and Detail-Preserving Single Image Super-Resolution(https://arxiv.org/abs/2509.14550)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Single-image super-resolution (SISR) remains highly ill-posed because recovering structurally faithful high-frequency content from a single low-resolution observation is ambiguous. Existing edge-aware methods often attach edge priors or attention branches onto increasingly complex backbones, yet ad hoc fusion frequently introduces redundancy, unstable optimization, or limited structural gains. We address this gap with an edge-guided attention mechanism that derives an adaptive modulation map from jointly encoded edge features and intermediate feature activations, then applies it to normalize and reweight responses, selectively amplifying structurally salient regions while suppressing spurious textures. In parallel, we integrate this mechanism into a lightweight residual design trained under a composite objective combining pixel-wise, perceptual, and adversarial terms to balance fidelity, perceptual realism, and training stability. Extensive experiments on standard SISR benchmarks demonstrate consistent improvements in structural sharpness and perceptual quality over SRGAN, ESRGAN, and prior edge-attention baselines at comparable model complexity. The proposed formulation provides (i) a parameter-efficient path to inject edge priors, (ii) stabilized adversarial refinement through a tailored multiterm loss, and (iii) enhanced edge fidelity without resorting to deeper or heavily overparameterized architectures. These results highlight the effectiveness of principled edge-conditioned modulation for advancing perceptual super-resolution.</li>
<li><strong>摘要：</strong>单位图像超分辨率（SISR）仍然很不错，因为从单个低分辨率观察中恢复了结构上忠实的高频含量是模棱两可的。现有的边缘感知方法通常将边缘先验或注意力分支连接到越来越复杂的骨架上，但是临时融合经常引入冗余，不稳定的优化或有限的结构增益。我们使用边缘引导的注意机制来解决这一差距，该机制从共同编码的边缘特征和中间特征激活中得出自适应调制图，然后将其应用于正常化和重量的响应，选择性地放大结构上的良好区域，同时抑制了刺激性的纹理。同时，我们将这种机制集成到一个轻巧的残留设计中，该设计在一个复合物镜下训练，将像素，感知和对抗性术语结合在一起，以平衡忠诚度，感知现实主义和训练稳定性。对标准SISR基准测试的广泛实验表明，在可比的模型复杂性下，对Srgan，Esrgan和先前的Edge-Crespention基线的结构清晰度和感知质量的持续改善。提出的配方提供了（i）注入边缘先验的参数效率路径，（ii）通过量身定制的多标准损失稳定的对抗性精炼，（iii）增强的边缘保真度，而无需诉诸更深层或更严重的过度参数化的体系结构。这些结果突出了原则边缘条件调制的有效性，以提高感知超分辨率。</li>
</ul>

<h3>Title: DiffVL: Diffusion-Based Visual Localization on 2D Maps via BEV-Conditioned GPS Denoising</h3>
<ul>
<li><strong>Authors: </strong>Li Gao, Hongyang Sun, Liu Liu, Yunhao Li, Yang Cai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14565">https://arxiv.org/abs/2509.14565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14565">https://arxiv.org/pdf/2509.14565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14565]] DiffVL: Diffusion-Based Visual Localization on 2D Maps via BEV-Conditioned GPS Denoising(https://arxiv.org/abs/2509.14565)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Accurate visual localization is crucial for autonomous driving, yet existing methods face a fundamental dilemma: While high-definition (HD) maps provide high-precision localization references, their costly construction and maintenance hinder scalability, which drives research toward standard-definition (SD) maps like OpenStreetMap. Current SD-map-based approaches primarily focus on Bird's-Eye View (BEV) matching between images and maps, overlooking a ubiquitous signal-noisy GPS. Although GPS is readily available, it suffers from multipath errors in urban environments. We propose DiffVL, the first framework to reformulate visual localization as a GPS denoising task using diffusion models. Our key insight is that noisy GPS trajectory, when conditioned on visual BEV features and SD maps, implicitly encode the true pose distribution, which can be recovered through iterative diffusion refinement. DiffVL, unlike prior BEV-matching methods (e.g., OrienterNet) or transformer-based registration approaches, learns to reverse GPS noise perturbations by jointly modeling GPS, SD map, and visual signals, achieving sub-meter accuracy without relying on HD maps. Experiments on multiple datasets demonstrate that our method achieves state-of-the-art accuracy compared to BEV-matching baselines. Crucially, our work proves that diffusion models can enable scalable localization by treating noisy GPS as a generative prior-making a paradigm shift from traditional matching-based methods.</li>
<li><strong>摘要：</strong>准确的视觉定位对于自主驾驶至关重要，但是现有的方法面临着根本的困境：而高清（HD）地图提供了高精度的定位参考，其昂贵的结构和维护阻碍了可伸缩性，从而推动了对标准定义（SD）地图（如OpenTreetMap）的研究。当前基于SD-图的方法主要集中在图像和地图之间匹配的鸟眼视图（BEV），俯瞰着无处不在的信号噪声GP。尽管GP很容易获得，但它遭受了城市环境中的多径错误。我们提出了DIFFVL，这是使用扩散模型重新将视觉定位作为GPS deno deno任务的第一个框架。我们的关键见解是，嘈杂的GPS轨迹在视觉BEV特征和SD图的条件下，隐式编码了真实的姿势分布，可以通过迭代扩散细化来恢复。 DIFFVL与先前的BEV匹配方法（例如Orienternet）或基于变压器的注册方法不同，学会通过共同建模GPS，SD MAP和视觉信号来逆转GPS噪声扰动，从而实现了不依赖HD MAP的子计量表精度。多个数据集的实验表明，与BEV匹配基线相比，我们的方法达到了最先进的精度。至关重要的是，我们的工作证明，扩散模型可以通过将嘈杂的GP视为生成性的事先提出的​​生成性，从而从传统的基于匹配的方法进行范式转移。</li>
</ul>

<h3>Title: DICE: Diffusion Consensus Equilibrium for Sparse-view CT Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Leon Suarez-Rodriguez, Roman Jacome, Romario Gualdron-Hurtado, Ana Mantilla-Dulcey, Henry Arguello</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14566">https://arxiv.org/abs/2509.14566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14566">https://arxiv.org/pdf/2509.14566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14566]] DICE: Diffusion Consensus Equilibrium for Sparse-view CT Reconstruction(https://arxiv.org/abs/2509.14566)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Sparse-view computed tomography (CT) reconstruction is fundamentally challenging due to undersampling, leading to an ill-posed inverse problem. Traditional iterative methods incorporate handcrafted or learned priors to regularize the solution but struggle to capture the complex structures present in medical images. In contrast, diffusion models (DMs) have recently emerged as powerful generative priors that can accurately model complex image distributions. In this work, we introduce Diffusion Consensus Equilibrium (DICE), a framework that integrates a two-agent consensus equilibrium into the sampling process of a DM. DICE alternates between: (i) a data-consistency agent, implemented through a proximal operator enforcing measurement consistency, and (ii) a prior agent, realized by a DM performing a clean image estimation at each sampling step. By balancing these two complementary agents iteratively, DICE effectively combines strong generative prior capabilities with measurement consistency. Experimental results show that DICE significantly outperforms state-of-the-art baselines in reconstructing high-quality CT images under uniform and non-uniform sparse-view settings of 15, 30, and 60 views (out of a total of 180), demonstrating both its effectiveness and robustness.</li>
<li><strong>摘要：</strong>稀疏视图计算机断层扫描（CT）的重建根本上是由于采样而挑战，导致了不良的反问题。传统的迭代方法结合了手工制作或学习的先验，以使解决方案正规化，但努力捕获医学图像中存在的复杂结构。相比之下，扩散模型（DMS）最近已成为强大的生成先验，可以准确地对复杂的图像分布进行建模。在这项工作中，我们引入了扩散共识平衡（DICE），该框架将两国共识平衡集成到DM的采样过程中。骰子在以下位置交替：（i）通过近端操作员执行测量一致性实现的数据矛盾代理，以及（ii）通过在每个采​​样步骤执行干净图像估计的DM实现的先验代理。通过迭代地平衡这两种互补剂，骰子有效地将强大的先验能力与测量一致性结合在一起。实验结果表明，骰子在重建15、30和60视图的均匀和非均匀的稀疏视图设置下重建高质量的CT图像（在总共180次中），表明其有效性和鲁棒性。</li>
</ul>

<h3>Title: Attention Lattice Adapter: Visual Explanation Generation for Visual Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Shinnosuke Hirano, Yuiga Wada, Tsumugi Iida, Komei Sugiura</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14664">https://arxiv.org/abs/2509.14664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14664">https://arxiv.org/pdf/2509.14664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14664]] Attention Lattice Adapter: Visual Explanation Generation for Visual Foundation Model(https://arxiv.org/abs/2509.14664)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In this study, we consider the problem of generating visual explanations in visual foundation models. Numerous methods have been proposed for this purpose; however, they often cannot be applied to complex models due to their lack of adaptability. To overcome these limitations, we propose a novel explanation generation method in visual foundation models that is aimed at both generating explanations and partially updating model parameters to enhance interpretability. Our approach introduces two novel mechanisms: Attention Lattice Adapter (ALA) and Alternating Epoch Architect (AEA). ALA mechanism simplifies the process by eliminating the need for manual layer selection, thus enhancing the model's adaptability and interpretability. Moreover, the AEA mechanism, which updates ALA's parameters every other epoch, effectively addresses the common issue of overly small attention regions. We evaluated our method on two benchmark datasets, CUB-200-2011 and ImageNet-S. Our results showed that our method outperformed the baseline methods in terms of mean intersection over union (IoU), insertion score, deletion score, and insertion-deletion score on both the CUB-200-2011 and ImageNet-S datasets. Notably, our best model achieved a 53.2-point improvement in mean IoU on the CUB-200-2011 dataset compared with the baselines.</li>
<li><strong>摘要：</strong>在这项研究中，我们考虑了在视觉基础模型中产生视觉解释的问题。为此，已经提出了许多方法。但是，由于缺乏适应性，它们通常无法应用于复杂的模型。为了克服这些局限性，我们在视觉基础模型中提出了一种新颖的解释生成方法，该方法旨在生成解释和部分更新模型参数以增强可解释性。我们的方法引入了两种新型机制：注意晶格适配器（ALA）和交替的时代建筑师（AEA）。 ALA机制通过消除对手动层选择的需求来简化过程，从而增强模型的适应性和解释性。此外，更新ALA参数的AEA机制有效地解决了过度关注区域的常见问题。我们在两个基准数据集（CUB-200-2011和Imagenet-s）上评估了我们的方法。我们的结果表明，我们的方法在CUB-200-2011和ImageNet-S数据集的平均交点（IOU），插入得分，删除得分和插入删除得分方面优于基线方法。值得注意的是，与基准相比，我们最好的模型在CUB-200-2011数据集上的平均值提高了53.2分。</li>
</ul>

<h3>Title: DACoN: DINO for Anime Paint Bucket Colorization with Any Number of Reference Images</h3>
<ul>
<li><strong>Authors: </strong>Kazuma Nagata, Naoshi Kaneko</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14685">https://arxiv.org/abs/2509.14685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14685">https://arxiv.org/pdf/2509.14685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14685]] DACoN: DINO for Anime Paint Bucket Colorization with Any Number of Reference Images(https://arxiv.org/abs/2509.14685)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Automatic colorization of line drawings has been widely studied to reduce the labor cost of hand-drawn anime production. Deep learning approaches, including image/video generation and feature-based correspondence, have improved accuracy but struggle with occlusions, pose variations, and viewpoint changes. To address these challenges, we propose DACoN, a framework that leverages foundation models to capture part-level semantics, even in line drawings. Our method fuses low-resolution semantic features from foundation models with high-resolution spatial features from CNNs for fine-grained yet robust feature extraction. In contrast to previous methods that rely on the Multiplex Transformer and support only one or two reference images, DACoN removes this constraint, allowing any number of references. Quantitative and qualitative evaluations demonstrate the benefits of using multiple reference images, achieving superior colorization performance. Our code and model are available at this https URL.</li>
<li><strong>摘要：</strong>已广泛研究了线条图纸的自动着色，以降低手绘动漫生产的人工成本。深度学习方法，包括图像/视频生成和基于特征的对应关系，具有提高的精度，但在遮挡，姿势变化和观点变化方面挣扎。为了应对这些挑战，我们提出了DACON，即即使在线条图中，也利用基础模型来捕获零件级别的语义。我们的方法融合了来自CNN的高分辨率空间特征的基础模型的低分辨率语义特征，可用于精细元素但可靠的特征提取。与以前依赖多重变压器并仅支持一个或两个参考图像的方法相反，DACON删除了此约束，从而允许任何数量的参考。定量和定性评估证明了使用多个参考图像的好处，从而实现了出色的着色性能。我们的代码和模型可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Data Augmentation via Latent Diffusion Models for Detecting Smell-Related Objects in Historical Artworks</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Sheta, Mathias Zinnen, Aline Sindel, Andreas Maier, Vincent Christlein</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14755">https://arxiv.org/abs/2509.14755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14755">https://arxiv.org/pdf/2509.14755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14755]] Data Augmentation via Latent Diffusion Models for Detecting Smell-Related Objects in Historical Artworks(https://arxiv.org/abs/2509.14755)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Finding smell references in historic artworks is a challenging problem. Beyond artwork-specific challenges such as stylistic variations, their recognition demands exceptionally detailed annotation classes, resulting in annotation sparsity and extreme class imbalance. In this work, we explore the potential of synthetic data generation to alleviate these issues and enable accurate detection of smell-related objects. We evaluate several diffusion-based augmentation strategies and demonstrate that incorporating synthetic data into model training can improve detection performance. Our findings suggest that leveraging the large-scale pretraining of diffusion models offers a promising approach for improving detection accuracy, particularly in niche applications where annotations are scarce and costly to obtain. Furthermore, the proposed approach proves to be effective even with relatively small amounts of data, and scaling it up provides high potential for further enhancements.</li>
<li><strong>摘要：</strong>在历史艺术品中找到气味参考是一个具有挑战性的问题。除了特定于艺术品的挑战（例如风格变化）之外，它们的认可还需要非常详细的注释类别，从而导致注释稀疏性和极端的阶级失衡。在这项工作中，我们探讨了合成数据生成减轻这些问题的潜力，并能够准确检测与气味相关的物体。我们评估了几种基于扩散的增强策略，并证明将合成数据纳入模型训练可以改善检测性能。我们的发现表明，利用扩散模型的大规模预处理提供了一种有希望的方法来提高检测准确性，尤其是在稀缺和昂贵的注释的利基应用程序中。此外，即使使用少量数据，提出的方法也被证明是有效的，并且对其进行扩展为进一步增强提供了很大的潜力。</li>
</ul>

<h3>Title: Dataset Distillation for Super-Resolution without Class Labels and Pre-trained Models</h3>
<ul>
<li><strong>Authors: </strong>Sunwoo Cho, Yejin Jung, Nam Ik Cho, Jae Woong Soh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14777">https://arxiv.org/abs/2509.14777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14777">https://arxiv.org/pdf/2509.14777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14777]] Dataset Distillation for Super-Resolution without Class Labels and Pre-trained Models(https://arxiv.org/abs/2509.14777)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generative</a></li>
<li><strong>Abstract: </strong>Training deep neural networks has become increasingly demanding, requiring large datasets and significant computational resources, especially as model complexity advances. Data distillation methods, which aim to improve data efficiency, have emerged as promising solutions to this challenge. In the field of single image super-resolution (SISR), the reliance on large training datasets highlights the importance of these techniques. Recently, a generative adversarial network (GAN) inversion-based data distillation framework for SR was proposed, showing potential for better data utilization. However, the current method depends heavily on pre-trained SR networks and class-specific information, limiting its generalizability and applicability. To address these issues, we introduce a new data distillation approach for image SR that does not need class labels or pre-trained SR models. In particular, we first extract high-gradient patches and categorize images based on CLIP features, then fine-tune a diffusion model on the selected patches to learn their distribution and synthesize distilled training images. Experimental results show that our method achieves state-of-the-art performance while using significantly less training data and requiring less computational time. Specifically, when we train a baseline Transformer model for SR with only 0.68\% of the original dataset, the performance drop is just 0.3 dB. In this case, diffusion model fine-tuning takes 4 hours, and SR model training completes within 1 hour, much shorter than the 11-hour training time with the full dataset.</li>
<li><strong>摘要：</strong>培训深层神经网络已经变得越来越苛刻，需要大量数据集和大量的计算资源，尤其是随着模型复杂性的发展。旨在提高数据效率的数据蒸馏方法已成为解决这一挑战的有希望的解决方案。在单像超分辨率（SISR）领域，对大型训练数据集的依赖突出了这些技术的重要性。最近，提出了基于SR的基于基于反转的数据蒸馏框架的生成对抗网络（GAN），显示了更好的数据利用的潜力。但是，当前方法在很大程度上取决于预训练的SR网络和特定于类的信息，从而限制了其普遍性和适用性。为了解决这些问题，我们为图像SR引入了一种新的数据蒸馏方法，该方法不需要类标签或预训练的SR模型。特别是，我们首先提取高梯度贴片并根据夹子特征对图像进行分类，然后在所选贴片上微调一个扩散模型，以了解其分布并合成蒸馏训练图像。实验结果表明，我们的方法可以实现最先进的性能，同时使用较少的训练数据并需要更少的计算时间。具体而言，当我们仅使用原始数据集的0.68％训练SR的基线变压器模型时，性能下降仅为0.3 dB。在这种情况下，扩散模型微调需要4个小时，而SR模型培训在1小时内完成，比完整数据集的11小时培训时间短得多。</li>
</ul>

<h3>Title: Radiology Report Conditional 3D CT Generation with Multi Encoder Latent diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Sina Amirrajab, Zohaib Salahuddin, Sheng Kuang, Henry C. Woodruff, Philippe Lambin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14780">https://arxiv.org/abs/2509.14780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14780">https://arxiv.org/pdf/2509.14780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14780]] Radiology Report Conditional 3D CT Generation with Multi Encoder Latent diffusion Model(https://arxiv.org/abs/2509.14780)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text to image latent diffusion models have recently advanced medical image synthesis, but applications to 3D CT generation remain limited. Existing approaches rely on simplified prompts, neglecting the rich semantic detail in full radiology reports, which reduces text image alignment and clinical fidelity. We propose Report2CT, a radiology report conditional latent diffusion framework for synthesizing 3D chest CT volumes directly from free text radiology reports, incorporating both findings and impression sections using multiple text encoder. Report2CT integrates three pretrained medical text encoders (BiomedVLP CXR BERT, MedEmbed, and ClinicalBERT) to capture nuanced clinical context. Radiology reports and voxel spacing information condition a 3D latent diffusion model trained on 20000 CT volumes from the CT RATE dataset. Model performance was evaluated using Frechet Inception Distance (FID) for real synthetic distributional similarity and CLIP based metrics for semantic alignment, with additional qualitative and quantitative comparisons against GenerateCT model. Report2CT generated anatomically consistent CT volumes with excellent visual quality and text image alignment. Multi encoder conditioning improved CLIP scores, indicating stronger preservation of fine grained clinical details in the free text radiology reports. Classifier free guidance further enhanced alignment with only a minor trade off in FID. We ranked first in the VLM3D Challenge at MICCAI 2025 on Text Conditional CT Generation and achieved state of the art performance across all evaluation metrics. By leveraging complete radiology reports and multi encoder text conditioning, Report2CT advances 3D CT synthesis, producing clinically faithful and high quality synthetic data.</li>
<li><strong>摘要：</strong>对图像潜在扩散模型的文本最近具有先进的医疗图像综合，但是对3D CT生成的应用仍然有限。现有的方法依赖于简化的提示，在完整的放射学报告中忽略了丰富的语义细节，从而降低了文本图像对准和临床保真度。我们提出了Report2CT，这是一个直接从自由文本放射学报告中合成3D胸部CT量的有条件潜在扩散框架，使用多个文本编码器结合了发现和印象部分。 Report2CT整合了三个验证的医学文本编码器（BiomedVLP CXR Bert，Medembed和Clinicalbert），以捕获细微的临床环境。放射学报告和体素间距信息条件3D潜在扩散模型在CT速率数据集对20000 CT量的训练中进行了训练。使用Frechet Inception距离（FID）评估了模型性能，以实现用于语义比对的实际合成分布相似性和基于夹子的指标，并与Generatect模型进行了其他定性和定量比较。 Report2CT生成了具有出色视觉质量和文本图像对齐方式的解剖学上一致的CT量。多编码器调节改进的夹得分，表明在自由文本放射学报告中，更强地保留细粒度的临床细节。分类器免费指导进一步增强了一致性，仅在FID中进行了轻微的权衡。我们在2025年Miccai的VLM3D挑战中排名第一，在所有评估指标中都达到了条件CT的生成，并在所有评估指标中都取得了最高的状态。通过利用完整的放射学报告和多编码器文本条件，Report2CT Prospances 3D CT合成，产生临床忠实且高质量的合成数据。</li>
</ul>

<h3>Title: Diffusion-Based Scenario Tree Generation for Multivariate Time Series Prediction and Multistage Stochastic Optimization</h3>
<ul>
<li><strong>Authors: </strong>Stelios Zarifis, Ioannis Kordonis, Petros Maragos</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14832">https://arxiv.org/abs/2509.14832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14832">https://arxiv.org/pdf/2509.14832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14832]] Diffusion-Based Scenario Tree Generation for Multivariate Time Series Prediction and Multistage Stochastic Optimization(https://arxiv.org/abs/2509.14832)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Stochastic forecasting is critical for efficient decision-making in uncertain systems, such as energy markets and finance, where estimating the full distribution of future scenarios is essential. We propose Diffusion Scenario Tree (DST), a general framework for constructing scenario trees for multivariate prediction tasks using diffusion-based probabilistic forecasting models. DST recursively samples future trajectories and organizes them into a tree via clustering, ensuring non-anticipativity (decisions depending only on observed history) at each stage. We evaluate the framework on the optimization task of energy arbitrage in New York State's day-ahead electricity market. Experimental results show that our approach consistently outperforms the same optimization algorithms that use scenario trees from more conventional models and Model-Free Reinforcement Learning baselines. Furthermore, using DST for stochastic optimization yields more efficient decision policies, achieving higher performance by better handling uncertainty than deterministic and stochastic MPC variants using the same diffusion-based forecaster.</li>
<li><strong>摘要：</strong>随机预测对于不确定系统（例如能源市场和金融）的有效决策至关重要，在这种系统中，估计未来场景的完整分布至关重要。我们提出了扩散方案树（DST），这是一个使用基于扩散的概率预测模型来构造多元预测任务的场景树的一般框架。 DST递归地对未来的轨迹进行采样，并通过聚类将它们组织到树上，以确保在每个阶段的非中期性（仅取决于观察到的历史）。我们评估了纽约州日电力市场的能源套利优化任务的框架。实验结果表明，我们的方法始终优于相同的优化算法，这些优化算法使用了来自更常规模型和无模型的增强学习基线的场景树。此外，使用DST进行随机优化会产生更有效的决策策略，通过使用基于相同扩散的预测器的确定性和随机MPC变体，通过更好地处理不确定性来实现更高的性能。</li>
</ul>

<h3>Title: Not All Degradations Are Equal: A Targeted Feature Denoising Framework for Generalizable Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Hongjun Wang, Jiyuan Chen, Zhengwei Yin, Xuan Song, Yinqiang Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14841">https://arxiv.org/abs/2509.14841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14841">https://arxiv.org/pdf/2509.14841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14841]] Not All Degradations Are Equal: A Targeted Feature Denoising Framework for Generalizable Image Super-Resolution(https://arxiv.org/abs/2509.14841)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Generalizable Image Super-Resolution aims to enhance model generalization capabilities under unknown degradations. To achieve this goal, the models are expected to focus only on image content-related features instead of overfitting degradations. Recently, numerous approaches such as Dropout and Feature Alignment have been proposed to suppress models' natural tendency to overfit degradations and yield promising results. Nevertheless, these works have assumed that models overfit to all degradation types (e.g., blur, noise, JPEG), while through careful investigations in this paper, we discover that models predominantly overfit to noise, largely attributable to its distinct degradation pattern compared to other degradation types. In this paper, we propose a targeted feature denoising framework, comprising noise detection and denoising modules. Our approach presents a general solution that can be seamlessly integrated with existing super-resolution models without requiring architectural modifications. Our framework demonstrates superior performance compared to previous regularization-based methods across five traditional benchmarks and datasets, encompassing both synthetic and real-world scenarios.</li>
<li><strong>摘要：</strong>可推广的图像超分辨率旨在增强未知降解下的模型泛化能力。为了实现这一目标，预期这些模型仅关注与图像内容相关的功能，而不是过度拟合降解。最近，已经提出了许多方法，例如辍学和特征对准，以抑制模型过度降解并产生令人鼓舞的结果的自然趋势。然而，这些作品假设模型过于适合所有降解类型（例如，模糊，噪声，JPEG），而通过在本文中进行仔细的研究，我们发现模型主要与其他降解类型相比，主要归因于其独特的降级模式。在本文中，我们提出了一个有针对性的特征降级框架，包括噪声检测和降解模块。我们的方法提出了一种通用解决方案，可以与现有的超级分辨率模型无缝集成，而无需进行体系结构修改。与以前的五个传统基准和数据集相比，我们的框架表现出了卓越的性能，包括合成和现实世界的情况。</li>
</ul>

<h3>Title: GenKOL: Modular Generative AI Framework For Scalable Virtual KOL Generation</h3>
<ul>
<li><strong>Authors: </strong>Tan-Hiep To, Duy-Khang Nguyen, Tam V. Nguyen, Minh-Triet Tran, Trung-Nghia Le</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14927">https://arxiv.org/abs/2509.14927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14927">https://arxiv.org/pdf/2509.14927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14927]] GenKOL: Modular Generative AI Framework For Scalable Virtual KOL Generation(https://arxiv.org/abs/2509.14927)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Key Opinion Leader (KOL) play a crucial role in modern marketing by shaping consumer perceptions and enhancing brand credibility. However, collaborating with human KOLs often involves high costs and logistical challenges. To address this, we present GenKOL, an interactive system that empowers marketing professionals to efficiently generate high-quality virtual KOL images using generative AI. GenKOL enables users to dynamically compose promotional visuals through an intuitive interface that integrates multiple AI capabilities, including garment generation, makeup transfer, background synthesis, and hair editing. These capabilities are implemented as modular, interchangeable services that can be deployed flexibly on local machines or in the cloud. This modular architecture ensures adaptability across diverse use cases and computational environments. Our system can significantly streamline the production of branded content, lowering costs and accelerating marketing workflows through scalable virtual KOL creation.</li>
<li><strong>摘要：</strong>关键意见领导者（KOL）通过塑造消费者的看法并提高品牌信誉，在现代营销中发挥着至关重要的作用。但是，与人类KOL合作通常涉及高成本和后勤挑战。为了解决这个问题，我们提出了Genkol，这是一种交互式系统，它使营销专业人员使用生成AI有效地生成高质量的虚拟KOL图像。 Genkol使用户能够通过直观的接口动态组成促销视觉效果，该界面集成了多个AI功能，包括生成服装，化妆转移，背景合成和头发编辑。这些功能被实现为模块化的可互换服务，可以在本地机器或云中灵活部署。这种模块化体系结构可确保各种用例和计算环境之间的适应性。我们的系统可以通过可扩展的虚拟KOL创建来大大简化品牌内容的生产，降低成本并加速营销工作流程。</li>
</ul>

<h3>Title: A Comparative Analysis of Transformer Models in Social Bot Detection</h3>
<ul>
<li><strong>Authors: </strong>Rohan Veit, Michael Lones</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14936">https://arxiv.org/abs/2509.14936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14936">https://arxiv.org/pdf/2509.14936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14936]] A Comparative Analysis of Transformer Models in Social Bot Detection(https://arxiv.org/abs/2509.14936)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Social media has become a key medium of communication in today's society. This realisation has led to many parties employing artificial users (or bots) to mislead others into believing untruths or acting in a beneficial manner to such parties. Sophisticated text generation tools, such as large language models, have further exacerbated this issue. This paper aims to compare the effectiveness of bot detection models based on encoder and decoder transformers. Pipelines are developed to evaluate the performance of these classifiers, revealing that encoder-based classifiers demonstrate greater accuracy and robustness. However, decoder-based models showed greater adaptability through task-specific alignment, suggesting more potential for generalisation across different use cases in addition to superior observa. These findings contribute to the ongoing effort to prevent digital environments being manipulated while protecting the integrity of online discussion.</li>
<li><strong>摘要：</strong>社交媒体已成为当今社会中沟通的关键媒介。这种认识导致许多政党雇用人工用户（或机器人）误导他人相信对这些当事方的不真实或以有益的方式行事。复杂的文本生成工具（例如大型语言模型）进一步加剧了此问题。本文旨在比较基于编码器和解码器变压器的机器人检测模型的有效性。开发了管道来评估这些分类器的性能，揭示了基于编码器的分类器表现出更高的准确性和鲁棒性。但是，基于解码器的模型通过特定于任务的比对显示了更大的适应性，这表明除了出色的观测值外，不同用例以外的泛化潜力更大。这些发现有助于持续的努力，以防止数字环境在保护在线讨论的完整性的同时。</li>
</ul>

<h3>Title: FAWN: A MultiEncoder Fusion-Attention Wave Network for Integrated Sensing and Communication Indoor Scene Inference</h3>
<ul>
<li><strong>Authors: </strong>Carlos Barroso-Fernández, Alejandro Calvillo-Fernandez, Antonio de la Oliva, Carlos J. Bernardos</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14968">https://arxiv.org/abs/2509.14968</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14968">https://arxiv.org/pdf/2509.14968</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14968]] FAWN: A MultiEncoder Fusion-Attention Wave Network for Integrated Sensing and Communication Indoor Scene Inference(https://arxiv.org/abs/2509.14968)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The upcoming generations of wireless technologies promise an era where everything is interconnected and intelligent. As the need for intelligence grows, networks must learn to better understand the physical world. However, deploying dedicated hardware to perceive the environment is not always feasible, mainly due to costs and/or complexity. Integrated Sensing and Communication (ISAC) has made a step forward in addressing this challenge. Within ISAC, passive sensing emerges as a cost-effective solution that reuses wireless communications to sense the environment, without interfering with existing communications. Nevertheless, the majority of current solutions are limited to one technology (mostly Wi-Fi or 5G), constraining the maximum accuracy reachable. As different technologies work with different spectrums, we see a necessity in integrating more than one technology to augment the coverage area. Hence, we take the advantage of ISAC passive sensing, to present FAWN, a MultiEncoder Fusion-Attention Wave Network for ISAC indoor scene inference. FAWN is based on the original transformers architecture, to fuse information from Wi-Fi and 5G, making the network capable of understanding the physical world without interfering with the current communication. To test our solution, we have built a prototype and integrated it in a real scenario. Results show errors below 0.6 m around 84% of times.</li>
<li><strong>摘要：</strong>即将到来的几代无线技术使一切都相互联系和聪明。随着对智力的需求的增长，网络必须学会更好地了解物理世界。但是，部署专用硬件来感知环境并不总是可行的，这主要是由于成本和/或复杂性。集成的传感和通信（ISAC）在应对这一挑战方面迈出了一步。在ISAC中，被动传感是一种具有成本效益的解决方案，可重用无线通信以感知环境，而不会干扰现有的通信。然而，当前大多数解决方案仅限于一种技术（主要是Wi-Fi或5g），从而限制了最高准确性。随着不同的技术与不同的频谱一起工作，我们认为需要整合多种技术来扩大覆盖范围。因此，我们利用ISAC被动传感的优势来展示Fawn，这是ISAC室内场景推理的多重编码器融合 - 注意力机网络。 Fawn基于原始的变形金刚结构，将Wi-Fi和5G的信息融合，使网络能够理解物理世界而不会干扰当前的通信。为了测试我们的解决方案，我们已经构建了一个原型并将其集成在实际情况下。结果显示，错误低于0.6 m的84％次。</li>
</ul>

<h3>Title: EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal Ultrasound Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Chaoyin She, Ruifang Lu, Lida Chen, Wei Wang, Qinghua Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14977">https://arxiv.org/abs/2509.14977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14977">https://arxiv.org/pdf/2509.14977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14977]] EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal Ultrasound Intelligence(https://arxiv.org/abs/2509.14977)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Ultrasound imaging has become the preferred imaging modality for early cancer screening due to its advantages of non-ionizing radiation, low cost, and real-time imaging capabilities. However, conventional ultrasound diagnosis heavily relies on physician expertise, presenting challenges of high subjectivity and low diagnostic efficiency. Vision-language models (VLMs) offer promising solutions for this issue, but existing general-purpose models demonstrate limited knowledge in ultrasound medical tasks, with poor generalization in multi-organ lesion recognition and low efficiency across multi-task diagnostics. To address these limitations, we propose EchoVLM, a vision-language model specifically designed for ultrasound medical imaging. The model employs a Mixture of Experts (MoE) architecture trained on data spanning seven anatomical regions. This design enables the model to perform multiple tasks, including ultrasound report generation, diagnosis and visual question-answering (VQA). The experimental results demonstrated that EchoVLM achieved significant improvements of 10.15 and 4.77 points in BLEU-1 scores and ROUGE-1 scores respectively compared to Qwen2-VL on the ultrasound report generation task. These findings suggest that EchoVLM has substantial potential to enhance diagnostic accuracy in ultrasound imaging, thereby providing a viable technical solution for future clinical applications. Source code and model weights are available at this https URL.</li>
<li><strong>摘要：</strong>由于非电离辐射，低成本和实时成像功能的优势，超声成像已成为早期癌症筛查的首选成像方式。但是，常规的超声诊断在很大程度上取决于医师的专业知识，提出了高主观性和低诊断效率的挑战。视觉语言模型（VLMS）为此问题提供了有希望的解决方案，但是现有的通用模型表明，超声医疗任务的知识有限，多器官病变识别率较差，并且在多任务诊断方面的效率低。为了解决这些局限性，我们提出了Echovlm，这是一种专门为超声医学成像设计的视觉模型。该模型采用了专家（MOE）架构的混合物，该架构培训了跨越七个解剖区域的数据。该设计使该模型能够执行多个任务，包括超声报告生成，诊断和视觉提问（VQA）。实验结果表明，与超声报告生成任务相比，ECHOVLM与QWEN2-VL相比，BLEU-1得分和Rouge-1得分分别获得了10.15和4.77分的显着改善。这些发现表明，Echovlm具有增强超声成像诊断准确性的巨大潜力，从而为将来的临床应用提供了可行的技术解决方案。源代码和模型权重可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: SPATIALGEN: Layout-guided 3D Indoor Scene Generation</h3>
<ul>
<li><strong>Authors: </strong>Chuan Fang, Heng Li, Yixun Liang, Jia Zheng, Yongsen Mao, Yuan Liu, Rui Tang, Zihan Zhou, Ping Tan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14981">https://arxiv.org/abs/2509.14981</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14981">https://arxiv.org/pdf/2509.14981</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14981]] SPATIALGEN: Layout-guided 3D Indoor Scene Generation(https://arxiv.org/abs/2509.14981)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Creating high-fidelity 3D models of indoor environments is essential for applications in design, virtual reality, and robotics. However, manual 3D modeling remains time-consuming and labor-intensive. While recent advances in generative AI have enabled automated scene synthesis, existing methods often face challenges in balancing visual quality, diversity, semantic consistency, and user control. A major bottleneck is the lack of a large-scale, high-quality dataset tailored to this task. To address this gap, we introduce a comprehensive synthetic dataset, featuring 12,328 structured annotated scenes with 57,440 rooms, and 4.7M photorealistic 2D renderings. Leveraging this dataset, we present SpatialGen, a novel multi-view multi-modal diffusion model that generates realistic and semantically consistent 3D indoor scenes. Given a 3D layout and a reference image (derived from a text prompt), our model synthesizes appearance (color image), geometry (scene coordinate map), and semantic (semantic segmentation map) from arbitrary viewpoints, while preserving spatial consistency across modalities. SpatialGen consistently generates superior results to previous methods in our experiments. We are open-sourcing our data and models to empower the community and advance the field of indoor scene understanding and generation.</li>
<li><strong>摘要：</strong>创建室内环境的高保真3D模型对于设计，虚拟现实和机器人技术的应用至关重要。但是，手动3D建模仍然耗时和劳动力密集。尽管生成AI的最新进展已启用了自动场景综合，但现有方法通常在平衡视觉质量，多样性，语义一致性和用户控制方面面临挑战。一个主要的瓶颈是缺乏针对此任务量身定制的大型高质量数据集。为了解决这个差距，我们引入了一个全面的合成数据集，其中包含12,328个结构化注释的场景，带有57,440个房间，以及470万个逼真的2D渲染。利用此数据集，我们提出了SpatialGen，这是一种新型的多视模式扩散模型，它生成了现实且在语义上一致的3D室内场景。给定3D布局和参考图像（源自文本提示符），我们的模型从任意观点综合了外观（颜色图像），几何图（场景坐标图）和语义（语义分割图），同时保留跨模态的空间一致性。在我们的实验中，Patialgen始终如一地产生优于以前的方法的较好结果。我们正在开放式数据和模型，以增强社区的能力，并推进室内场景的理解和发电领域。</li>
</ul>

<h3>Title: Sea-ing Through Scattered Rays: Revisiting the Image Formation Model for Realistic Underwater Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Vasiliki Ismiroglou, Malte Pedersen, Stefan H. Bengtson, Andreas Aakerberg, Thomas B. Moeslund</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15011">https://arxiv.org/abs/2509.15011</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15011">https://arxiv.org/pdf/2509.15011</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15011]] Sea-ing Through Scattered Rays: Revisiting the Image Formation Model for Realistic Underwater Image Generation(https://arxiv.org/abs/2509.15011)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In recent years, the underwater image formation model has found extensive use in the generation of synthetic underwater data. Although many approaches focus on scenes primarily affected by discoloration, they often overlook the model's ability to capture the complex, distance-dependent visibility loss present in highly turbid environments. In this work, we propose an improved synthetic data generation pipeline that includes the commonly omitted forward scattering term, while also considering a nonuniform medium. Additionally, we collected the BUCKET dataset under controlled turbidity conditions to acquire real turbid footage with the corresponding reference images. Our results demonstrate qualitative improvements over the reference model, particularly under increasing turbidity, with a selection rate of 82. 5\% by survey participants. Data and code can be accessed on the project page: this http URL.</li>
<li><strong>摘要：</strong>近年来，水下图像形成模型已发现在合成水下数据的生成中广泛使用。尽管许多方法集中在主要受变色影响的场景上，但它们经常忽略模型捕获高度模糊环境中存在的复杂，距离可见性损失的能力。在这项工作中，我们提出了改进的合成数据生成管道，其中包括通常省略的前向散射项，同时也考虑了不均匀的培养基。此外，我们在受控的浊度条件下收集了存储桶数据集，以使用相应的参考图像获取真实的浑浊镜头。我们的结果表明，对参考模型的定性改进，尤其是在增加的浊度下，调查参与者的选择率为82。5\％。可以在项目页面上访问数据和代码：此HTTP URL。</li>
</ul>

<h3>Title: Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Saleh Vahdatpour, Maryam Eyvazi, Yanqing Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15076">https://arxiv.org/abs/2509.15076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15076">https://arxiv.org/pdf/2509.15076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15076]] Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models(https://arxiv.org/abs/2509.15076)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Air pollution remains a critical threat to public health and environmental sustainability, yet conventional monitoring systems are often constrained by limited spatial coverage and accessibility. This paper proposes an AI-driven agent that predicts ambient air pollution levels from sky images and synthesizes realistic visualizations of pollution scenarios using generative modeling. Our approach combines statistical texture analysis with supervised learning for pollution classification, and leverages vision-language model (VLM)-guided image generation to produce interpretable representations of air quality conditions. The generated visuals simulate varying degrees of pollution, offering a foundation for user-facing interfaces that improve transparency and support informed environmental decision-making. These outputs can be seamlessly integrated into intelligent applications aimed at enhancing situational awareness and encouraging behavioral responses based on real-time forecasts. We validate our method using a dataset of urban sky images and demonstrate its effectiveness in both pollution level estimation and semantically consistent visual synthesis. The system design further incorporates human-centered user experience principles to ensure accessibility, clarity, and public engagement in air quality forecasting. To support scalable and energy-efficient deployment, future iterations will incorporate a green CNN architecture enhanced with FPGA-based incremental learning, enabling real-time inference on edge platforms.</li>
<li><strong>摘要：</strong>空气污染仍然对公共卫生和环境可持续性构成关键威胁，但是常规监测系统通常受到有限的空间覆盖范围和可访问性的限制。本文提出了一种AI驱动的试剂，该试剂可预测天空图像的环境空气污染水平，并使用生成建模综合污染场景的现实可视化。我们的方法将统计纹理分析与监督学习进行污染分类，并利用视觉语言模型（VLM）指导的图像生成生成可解释的空气质量条件表示。生成的视觉效果模拟了不同程度的污染，为面向用户的接口提供了基础，以提高透明度并支持知情的环境决策。这些输出可以无缝集成到旨在提高情境意识和鼓励基于实时预测的行为响应的智能应用中。我们使用城市天空图像数据集验证我们的方法，并在污染水平估计和语义一致的视觉合成中证明其有效性。系统设计进一步纳入了以人为本的用户体验原则，以确保可及性，清晰度和公众参与空气质量预测。为了支持可扩展和节能的部署，未来的迭代将通过基于FPGA的增量学习来结合增强的绿色CNN体系结构，从而在边缘平台上实时推断。</li>
</ul>

<h3>Title: AIP: Subverting Retrieval-Augmented Generation via Adversarial Instructional Prompt</h3>
<ul>
<li><strong>Authors: </strong>Saket S. Chaturvedi, Gaurav Bagwe, Lan Zhang, Xiaoyong Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15159">https://arxiv.org/abs/2509.15159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15159">https://arxiv.org/pdf/2509.15159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15159]] AIP: Subverting Retrieval-Augmented Generation via Adversarial Instructional Prompt(https://arxiv.org/abs/2509.15159)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by retrieving relevant documents from external sources to improve factual accuracy and verifiability. However, this reliance introduces new attack surfaces within the retrieval pipeline, beyond the LLM itself. While prior RAG attacks have exposed such vulnerabilities, they largely rely on manipulating user queries, which is often infeasible in practice due to fixed or protected user inputs. This narrow focus overlooks a more realistic and stealthy vector: instructional prompts, which are widely reused, publicly shared, and rarely audited. Their implicit trust makes them a compelling target for adversaries to manipulate RAG behavior covertly. We introduce a novel attack for Adversarial Instructional Prompt (AIP) that exploits adversarial instructional prompts to manipulate RAG outputs by subtly altering retrieval behavior. By shifting the attack surface to the instructional prompts, AIP reveals how trusted yet seemingly benign interface components can be weaponized to degrade system integrity. The attack is crafted to achieve three goals: (1) naturalness, to evade user detection; (2) utility, to encourage use of prompts; and (3) robustness, to remain effective across diverse query variations. We propose a diverse query generation strategy that simulates realistic linguistic variation in user queries, enabling the discovery of prompts that generalize across paraphrases and rephrasings. Building on this, a genetic algorithm-based joint optimization is developed to evolve adversarial prompts by balancing attack success, clean-task utility, and stealthiness. Experimental results show that AIP achieves up to 95.23% ASR while preserving benign functionality. These findings uncover a critical and previously overlooked vulnerability in RAG systems, emphasizing the need to reassess the shared instructional prompts.</li>
<li><strong>摘要：</strong>检索增强的生成（RAG）通过从外部来源检索相关文档来提高事实准确性和可验证性，从而增强了大语言模型（LLM）。但是，这种依赖在检索管道内引入了新的攻击表面，超出了LLM本身。虽然先前的破布攻击暴露了这种漏洞，但它们主要依赖于操纵用户查询，而用户查询通常是由于固定或受保护的用户输入而在实践中不可行的。这个狭窄的焦点忽略了一个更现实，更隐秘的向量：教学提示，这些提示被广泛重复使用，公开共享且很少经过审核。他们的隐式信任使它们成为对手秘密操纵抹布行为的引人注目的目标。我们引入了针对对抗教学提示（AIP）的新型攻击，该攻击利用了对抗性教学提示，通过巧妙地改变检索行为来操纵抹布输出。通过将攻击表面转移到教学提示中，AIP揭示了可以将信任但看似良性的界面组件被武器化以降低系统完整性的方式。攻击是为了实现三个目标而制定的：（1）自然，逃避用户检测； （2）公用事业，以鼓励使用提示； （3）鲁棒性，以在各种查询变化中保持有效。我们提出了一种多样化的查询生成策略，该策略模拟了用户查询中的现实语言差异，从而发现了在跨释义和翻译之间概括的提示的发现。在此基础上，开发了基于遗传算法的关节优化，以通过平衡攻击成功，清洁任务实用程序和隐身性来发展对抗性提示。实验结果表明，AIP在保留良性功能的同时最多可达到95.23％的ASR。这些发现发现了抹布系统中的关键且以前被忽视的脆弱性，强调需要重新评估共享的教学提示。</li>
</ul>

<h3>Title: Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Yue, Zidong Wang, Yuqing Wang, Wenlong Zhang, Xihui Liu, Wanli Ouyang, Lei Bai, Luping Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15185">https://arxiv.org/abs/2509.15185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15185">https://arxiv.org/pdf/2509.15185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15185]] Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation(https://arxiv.org/abs/2509.15185)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent studies have demonstrated the importance of high-quality visual representations in image generation and have highlighted the limitations of generative models in image understanding. As a generative paradigm originally designed for natural language, autoregressive models face similar challenges. In this work, we present the first systematic investigation into the mechanisms of applying the next-token prediction paradigm to the visual domain. We identify three key properties that hinder the learning of high-level visual semantics: local and conditional dependence, inter-step semantic inconsistency, and spatial invariance deficiency. We show that these issues can be effectively addressed by introducing self-supervised objectives during training, leading to a novel training framework, Self-guided Training for AutoRegressive models (ST-AR). Without relying on pre-trained representation models, ST-AR significantly enhances the image understanding ability of autoregressive models and leads to improved generation quality. Specifically, ST-AR brings approximately 42% FID improvement for LlamaGen-L and 49% FID improvement for LlamaGen-XL, while maintaining the same sampling strategy.</li>
<li><strong>摘要：</strong>最近的研究表明，高质量的视觉表示在图像生成中的重要性，并强调了生成模型在图像理解中的局限性。作为最初为自然语言设计的生成范式，自回归模型面临类似的挑战。在这项工作中，我们介绍了对将下一个预测范式应用于视觉领域的机制进行的首次系统研究。我们确定了妨碍高级视觉语义学习的三个关键属性：局部和有条件依赖性，步进语义上的不一致和空间不变性缺陷。我们表明，可以通过在培训期间引入自我监督的目标来有效解决这些问题，从而导致新型的培训框架，自导向自回旋模型（ST-AR）的自我引入培训。 ST-AR在不依赖预训练的表示模型的情况下显着增强了自回归模型的图像理解能力，并提高了发电质量的提高。具体而言，ST-AR为Llamagen-L带来了大约42％的FID提高，而Lamagen-XL则提高了49％的FID，同时保持了相同的抽样策略。</li>
</ul>

<h3>Title: Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation</h3>
<ul>
<li><strong>Authors: </strong>Yujun Zhou, Zhenwen Liang, Haolin Liu, Wenhao Yu, Kishan Panaganti, Linfeng Song, Dian Yu, Xiangliang Zhang, Haitao Mi, Dong Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15194">https://arxiv.org/abs/2509.15194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15194">https://arxiv.org/pdf/2509.15194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15194]] Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation(https://arxiv.org/abs/2509.15194)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly trained with reinforcement learning from verifiable rewards (RLVR), yet real-world deployment demands models that can self-improve without labels or external judges. Existing label-free methods, confidence minimization, self-consistency, or majority-vote objectives, stabilize learning but steadily shrink exploration, causing an entropy collapse: generations become shorter, less diverse, and brittle. Unlike prior approaches such as Test-Time Reinforcement Learning (TTRL), which primarily adapt models to the immediate unlabeled dataset at hand, our goal is broader: to enable general improvements without sacrificing the model's inherent exploration capacity and generalization ability, i.e., evolving. We formalize this issue and propose EVolution-Oriented and Label-free Reinforcement Learning (EVOL-RL), a simple rule that couples stability with variation under a label-free setting. EVOL-RL keeps the majority-voted answer as a stable anchor (selection) while adding a novelty-aware reward that favors responses whose reasoning differs from what has already been produced (variation), measured in semantic space. Implemented with GRPO, EVOL-RL also uses asymmetric clipping to preserve strong signals and an entropy regularizer to sustain search. This majority-for-selection + novelty-for-variation design prevents collapse, maintains longer and more informative chains of thought, and improves both pass@1 and pass@n. EVOL-RL consistently outperforms the majority-only TTRL baseline; e.g., training on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks stronger generalization across domains (e.g., GPQA). Furthermore, we demonstrate that EVOL-RL also boosts performance in the RLVR setting, highlighting its broad applicability.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）越来越多地通过从可验证的奖励（RLVR）学习的加强学习中训练，但是现实世界中的部署要求模型可以在没有标签或外部法官的情况下自我爆发。现有的无标签方法，最小化的信心，自矛盾或多数票数目标，稳定学习，但稳步收缩探索，导致熵崩溃：世代变得更短，多样化和脆弱。与先前的方法（例如测试时间增强学习（TTRL））不同，该方法主要将模型适应了即时的未标记数据集，我们的目标是更广泛的：在不牺牲模型固有的探索能力和概括能力（即进化）的情况下，可以实现一般改进。我们正式化了这个问题，并提出了面向进化的和无标签的加固学习（EVOL-RL），这是一个简单的规则，它使稳定性与无标签设置下的稳定性结合在一起。 Evol-RL将大多数投票的答案作为稳定的锚（选择），同时添加了新颖的意识奖励，这种奖励有利于反应的反应，其推理与已经在语义空间中测量的已经产生的（变体）不同。 EVOL-RL通过GRPO实施，还使用了不对称的剪辑来保留强信号和熵正常器来维持搜索。这种多数派选择 +新颖性的变化设计可防止崩溃，维持更长，更有信息的思想链，并改善PASS@1和Pass@n。 EVOL-RL始终优于仅大多数TTRL基线；例如，对无标签AIME24的培训将QWEN3-4B-BASE AIME25通过@1从TTRL的4.6％到16.4％，并将@16通过从18.5％传递至37.9％。 EVOL-RL不仅可以防止多样性崩溃，而且可以解锁跨领域的更强概括（例如GPQA）。此外，我们证明了EVOL-RL还提高了RLVR设置中的性能，突出了其广泛的适用性。</li>
</ul>

<h3>Title: RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation</h3>
<ul>
<li><strong>Authors: </strong>Yuming Jiang, Siteng Huang, Shengke Xue, Yaxi Zhao, Jun Cen, Sicong Leng, Kehan Li, Jiayan Guo, Kexiang Wang, Mingxiu Chen, Fan Wang, Deli Zhao, Xin Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15212">https://arxiv.org/abs/2509.15212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15212">https://arxiv.org/pdf/2509.15212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15212]] RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation(https://arxiv.org/abs/2509.15212)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper presents RynnVLA-001, a vision-language-action(VLA) model built upon large-scale video generative pretraining from human demonstrations. We propose a novel two-stage pretraining methodology. The first stage, Ego-Centric Video Generative Pretraining, trains an Image-to-Video model on 12M ego-centric manipulation videos to predict future frames conditioned on an initial frame and a language instruction. The second stage, Human-Centric Trajectory-Aware Modeling, extends this by jointly predicting future keypoint trajectories, thereby effectively bridging visual frame prediction with action prediction. Furthermore, to enhance action representation, we propose ActionVAE, a variational autoencoder that compresses sequences of actions into compact latent embeddings, reducing the complexity of the VLA output space. When finetuned on the same downstream robotics datasets, RynnVLA-001 achieves superior performance over state-of-the-art baselines, demonstrating that the proposed pretraining strategy provides a more effective initialization for VLA models.</li>
<li><strong>摘要：</strong>本文介绍了Rynnvla-001，这是一种基于人类示范的大规模视频生成预处理建立的视觉语言操作（VLA）模型。我们提出了一种新型的两阶段预处理方法。第一阶段是以自我为中心的视频生成预处理，在12M以自我为中心的操作视频上训练图像到视频模型，以预测以初始框架和语言指令为条件的未来框架。第二阶段是以人为中心的轨迹吸引的建模，通过共同预测未来的关键轨迹来扩展这一点，从而有效地将视觉框架预测与动作预测相结合。此外，为了增强动作表示形式，我们提出了ActionVae，这是一种将作用序列压缩到紧凑的潜在嵌入中，从而降低了VLA输出空间的复杂性。当在相同的下游机器人数据集上进行填充时，Rynnvla-001在比​​最先进的基线方面实现了优越的性能，这表明拟议的预审前策略为VLA模型提供了更有效的初始化。</li>
</ul>

<h3>Title: Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Fangjinhua Wang, Qingshan Xu, Yew-Soon Ong, Marc Pollefeys</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15220">https://arxiv.org/abs/2509.15220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15220">https://arxiv.org/pdf/2509.15220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15220]] Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model(https://arxiv.org/abs/2509.15220)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>To reconstruct the 3D geometry from calibrated images, learning-based multi-view stereo (MVS) methods typically perform multi-view depth estimation and then fuse depth maps into a mesh or point cloud. To improve the computational efficiency, many methods initialize a coarse depth map and then gradually refine it in higher resolutions. Recently, diffusion models achieve great success in generation tasks. Starting from a random noise, diffusion models gradually recover the sample with an iterative denoising process. In this paper, we propose a novel MVS framework, which introduces diffusion models in MVS. Specifically, we formulate depth refinement as a conditional diffusion process. Considering the discriminative characteristic of depth estimation, we design a condition encoder to guide the diffusion process. To improve efficiency, we propose a novel diffusion network combining lightweight 2D U-Net and convolutional GRU. Moreover, we propose a novel confidence-based sampling strategy to adaptively sample depth hypotheses based on the confidence estimated by diffusion model. Based on our novel MVS framework, we propose two novel MVS methods, DiffMVS and CasDiffMVS. DiffMVS achieves competitive performance with state-of-the-art efficiency in run-time and GPU memory. CasDiffMVS achieves state-of-the-art performance on DTU, Tanks & Temples and ETH3D. Code is available at: this https URL.</li>
<li><strong>摘要：</strong>为了从校准图像重建3D几何形状，基于学习的多视图立体声（MVS）方法通常执行多视图深度估计，然后将深度映射到网格或点云中。为了提高计算效率，许多方法初始化了一个粗糙的深度图，然后在更高的分辨率中逐渐完善它。最近，扩散模型在发电任务中取得了巨大成功。从随机噪声开始，扩散模型逐渐通过迭代降解过程逐渐恢复样品。在本文中，我们提出了一个新颖的MVS框架，该框架引入了MVS中的扩散模型。具体而言，我们将深度细化作为条件扩散过程。考虑到深度估计的判别特征，我们设计了一个条件编码器来指导扩散过程。为了提高效率，我们提出了一个新型扩散网络，结合了轻巧的2D U-NET和卷积GRU。此外，我们提出了一种基于置信的新型抽样策略，以根据扩散模型估计的置信度适应性采样深度假设。根据我们的新型MVS框架，我们提出了两种新型的MVS方法，DIFFMV和CasdiffMV。 DIFFMV通过运行时和GPU内存的最先进效率实现竞争性能。 CasdiffMVS在DTU，Tanks＆Semples和Eth3d上实现了最先进的性能。代码可用：此HTTPS URL。</li>
</ul>

<h3>Title: Lost in Translation? Vocabulary Alignment for Source-Free Domain Adaptation in Open-Vocabulary Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Silvio Mazzucco, Carl Persson, Mattia Segu, Pier Luigi Dovesi, Federico Tombari, Luc Van Gool, Matteo Poggi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15225">https://arxiv.org/abs/2509.15225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15225">https://arxiv.org/pdf/2509.15225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15225]] Lost in Translation? Vocabulary Alignment for Source-Free Domain Adaptation in Open-Vocabulary Semantic Segmentation(https://arxiv.org/abs/2509.15225)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce VocAlign, a novel source-free domain adaptation framework specifically designed for VLMs in open-vocabulary semantic segmentation. Our method adopts a student-teacher paradigm enhanced with a vocabulary alignment strategy, which improves pseudo-label generation by incorporating additional class concepts. To ensure efficiency, we use Low-Rank Adaptation (LoRA) to fine-tune the model, preserving its original capabilities while minimizing computational overhead. In addition, we propose a Top-K class selection mechanism for the student model, which significantly reduces memory requirements while further improving adaptation performance. Our approach achieves a notable 6.11 mIoU improvement on the CityScapes dataset and demonstrates superior performance on zero-shot segmentation benchmarks, setting a new standard for source-free adaptation in the open-vocabulary setting.</li>
<li><strong>摘要：</strong>我们介绍了Vocalign，这是一种新型的无源域自适应框架，专为开放式语义语义分割而设计。我们的方法采用了词汇对齐策略来提高学生教师范式，从而通过结合其他类概念来改善伪标签的生成。为了确保效率，我们使用低级适应（LORA）来微调模型，并保留其原始功能，同时最大程度地减少计算开销。此外，我们为学生模型提出了一种Top-K类选择机制，该机制大大降低了记忆要求，同时进一步改善了适应性的性能。我们的方法在CityScapes数据集上取得了显着的6.11 MIOU改进，并在零射击细分基准上展示了卓越的性能，为开放式摄入量表设置设定了新的无源适应标准。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
