<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-01-29</h1>
<h3>Title: MeanCache: From Instantaneous to Average Velocity for Accelerating Flow Matching Inference</h3>
<ul>
<li><strong>Authors: </strong>Huanlin Gao, Ping Chen, Fuyuan Shi, Ruijia Wu, Li YanTao, Qiang Hui, Yuren You, Ting Lu, Chao Tan, Shaoan Zhao, Zhaoxiang Liu, Fang Zhao, Kai Wang, Shiguo Lian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19961">https://arxiv.org/abs/2601.19961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19961">https://arxiv.org/pdf/2601.19961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19961]] MeanCache: From Instantaneous to Average Velocity for Accelerating Flow Matching Inference(https://arxiv.org/abs/2601.19961)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We present MeanCache, a training-free caching framework for efficient Flow Matching inference. Existing caching methods reduce redundant computation but typically rely on instantaneous velocity information (e.g., feature caching), which often leads to severe trajectory deviations and error accumulation under high acceleration ratios. MeanCache introduces an average-velocity perspective: by leveraging cached Jacobian--vector products (JVP) to construct interval average velocities from instantaneous velocities, it effectively mitigates local error accumulation. To further improve cache timing and JVP reuse stability, we develop a trajectory-stability scheduling strategy as a practical tool, employing a Peak-Suppressed Shortest Path under budget constraints to determine the schedule. Experiments on FLUX.1, Qwen-Image, and HunyuanVideo demonstrate that MeanCache achieves 4.12X and 4.56X and 3.59X acceleration, respectively, while consistently outperforming state-of-the-art caching baselines in generation quality. We believe this simple yet effective approach provides a new perspective for Flow Matching inference and will inspire further exploration of stability-driven acceleration in commercial-scale generative models.</li>
<li><strong>摘要：</strong>我们提出了 MeanCache，一个无需训练的缓存框架，用于高效的流匹配推理。现有的缓存方法减少了冗余计算，但通常依赖于瞬时速度信息（例如特征缓存），这往往会导致高加速比下严重的轨迹偏差和误差累积。 MeanCache引入了平均速度视角：通过利用缓存的雅可比向量积（JVP）从瞬时速度构造区间平均速度，它有效地减轻了局部误差累积。为了进一步提高缓存时序和 JVP 重用稳定性，我们开发了一种轨迹稳定性调度策略作为实用工具，在预算约束下采用峰值抑制最短路径来确定调度。在 FLUX.1、Qwen-Image 和 HunyuanVideo 上的实验表明，MeanCache 分别实现了 4.12 倍、4.56 倍和 3.59 倍的加速，同时在生成质量方面始终优于最先进的缓存基准。我们相信这种简单而有效的方法为流匹配推理提供了新的视角，并将激发对商业规模生成模型中稳定性驱动加速的进一步探索。</li>
</ul>

<h3>Title: Perturbation-Induced Linearization: Constructing Unlearnable Data with Solely Linear Classifiers</h3>
<ul>
<li><strong>Authors: </strong>Jinlin Liu, Wei Chen, Xiaojin Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19967">https://arxiv.org/abs/2601.19967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19967">https://arxiv.org/pdf/2601.19967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19967]] Perturbation-Induced Linearization: Constructing Unlearnable Data with Solely Linear Classifiers(https://arxiv.org/abs/2601.19967)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Collecting web data to train deep models has become increasingly common, raising concerns about unauthorized data usage. To mitigate this issue, unlearnable examples introduce imperceptible perturbations into data, preventing models from learning effectively. However, existing methods typically rely on deep neural networks as surrogate models for perturbation generation, resulting in significant computational costs. In this work, we propose Perturbation-Induced Linearization (PIL), a computationally efficient yet effective method that generates perturbations using only linear surrogate models. PIL achieves comparable or better performance than existing surrogate-based methods while reducing computational time dramatically. We further reveal a key mechanism underlying unlearnable examples: inducing linearization to deep models, which explains why PIL can achieve competitive results in a very short time. Beyond this, we provide an analysis about the property of unlearnable examples under percentage-based partial perturbation. Our work not only provides a practical approach for data protection but also offers insights into what makes unlearnable examples effective.</li>
<li><strong>摘要：</strong>收集网络数据来训练深度模型已经变得越来越普遍，引发了对未经授权的数据使用的担忧。为了缓解这个问题，不可学习的例子会给数据带来难以察觉的扰动，从而阻止模型有效学习。然而，现有方法通常依赖深度神经网络作为扰动生成的替代模型，从而导致巨大的计算成本。在这项工作中，我们提出了扰动诱导线性化（PIL），这是一种计算高效且有效的方法，仅使用线性代理模型生成扰动。 PIL 实现了与现有基于代理的方法相当或更好的性能，同时显着减少了计算时间。我们进一步揭示了不可学习示例背后的一个关键机制：将线性化引入深度模型，这解释了为什么 PIL 可以在很短的时间内取得有竞争力的结果。除此之外，我们还对基于百分比的部分扰动下不可学习的示例的属性进行了分析。我们的工作不仅提供了一种实用的数据保护方法，还提供了关于如何使无法学习的示例变得有效的见解。</li>
</ul>

<h3>Title: CiMRAG: Cim-Aware Domain-Adaptive and Noise-Resilient Retrieval-Augmented Generation for Edge-Based LLMs</h3>
<ul>
<li><strong>Authors: </strong>Shih-Hsuan Chiu, Ming-Syan Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20041">https://arxiv.org/abs/2601.20041</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20041">https://arxiv.org/pdf/2601.20041</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20041]] CiMRAG: Cim-Aware Domain-Adaptive and Noise-Resilient Retrieval-Augmented Generation for Edge-Based LLMs(https://arxiv.org/abs/2601.20041)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Personalized virtual assistants powered by large language models (LLMs) on edge devices are attracting growing attention, with Retrieval-Augmented Generation (RAG) emerging as a key method for personalization by retrieving relevant profile data and generating tailored responses. However, deploying RAG on edge devices faces efficiency hurdles due to the rapid growth of profile data, such as user-LLM interactions and recent updates. While Computing-in-Memory (CiM) architectures mitigate this bottleneck by eliminating data movement between memory and processing units via in-situ operations, they are susceptible to environmental noise that can degrade retrieval precision. This poses a critical issue in dynamic, multi-domain edge-based scenarios (e.g., travel, medicine, and law) where both accuracy and adaptability are paramount. To address these challenges, we propose Task-Oriented Noise-resilient Embedding Learning (TONEL), a framework that improves noise robustness and domain adaptability for RAG in noisy edge environments. TONEL employs a noise-aware projection model to learn task-specific embeddings compatible with CiM hardware constraints, enabling accurate retrieval under noisy conditions. Extensive experiments conducted on personalization benchmarks demonstrate the effectiveness and practicality of our methods relative to strong baselines, especially in task-specific noisy scenarios.</li>
<li><strong>摘要：</strong>由边缘设备上的大语言模型 (LLM) 提供支持的个性化虚拟助理正吸引着越来越多的关注，检索增强生成 (RAG) 通过检索相关配置文件数据并生成定制响应，成为个性化的关键方法。然而，由于配置文件数据（例如用户与 LLM 交互和最近的更新）的快速增长，在边缘设备上部署 RAG 面临着效率障碍。虽然内存计算 (CiM) 架构通过原位操作消除内存和处理单元之间的数据移动来缓解这一瓶颈，但它们容易受到环境噪声的影响，从而降低检索精度。这在动态、多域边缘场景（例如旅行、医学和法律）中提出了一个关键问题，在这些场景中，准确性和适应性都至关重要。为了应对这些挑战，我们提出了面向任务的抗噪声嵌入学习（TONEL），这是一个可以提高 RAG 在噪声边缘环境中的噪声鲁棒性和域适应性的框架。 TONEL 采用噪声感知投影模型来学习与 CiM 硬件约束兼容的特定于任务的嵌入，从而在噪声条件下实现准确检索。在个性化基准上进行的大量实验证明了我们的方法相对于强基线的有效性和实用性，特别是在特定于任务的噪声场景中。</li>
</ul>

<h3>Title: Scaling Next-Brain-Token Prediction for MEG</h3>
<ul>
<li><strong>Authors: </strong>Richard Csaky</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20138">https://arxiv.org/abs/2601.20138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20138">https://arxiv.org/pdf/2601.20138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20138]] Scaling Next-Brain-Token Prediction for MEG(https://arxiv.org/abs/2601.20138)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present a large autoregressive model for source-space MEG that scales next-token prediction to long context across datasets and scanners: handling a corpus of over 500 hours and thousands of sessions across the three largest MEG datasets. A modified SEANet-style vector-quantizer reduces multichannel MEG into a flattened token stream on which we train a Qwen2.5-VL backbone from scratch to predict the next brain token and to recursively generate minutes of MEG from up to a minute of context. To evaluate long-horizon generation, we introduce three task-matched tests: (i) on-manifold stability via generated-only drift compared to the time-resolved distribution of real sliding windows, and (ii) conditional specificity via correct context versus prompt-swap controls using a neurophysiologically grounded metric set. We train on CamCAN and Omega and run all analyses on held-out MOUS, establishing cross-dataset generalization. Across metrics, generations remain relatively stable over long rollouts and are closer to the correct continuation than swapped controls. Code available at: this https URL.</li>
<li><strong>摘要：</strong>我们提出了一个用于源空间 MEG 的大型自回归模型，可将下一个标记预测扩展到跨数据集和扫描仪的长上下文：处理三个最大 MEG 数据集中超过 500 小时的语料库和数千个会话。修改后的 SEANet 式矢量量化器将多通道 MEG 简化为扁平化的令牌流，我们在该流上从头开始训练 Qwen2.5-VL 主干以预测下一个大脑令牌，并从长达一分钟的上下文中递归生成几分钟的 MEG。为了评估长范围生成，我们引入了三个任务匹配测试：（i）通过仅生成的漂移与真实滑动窗口的时间分辨分布相比的流形稳定性，以及（ii）通过正确的上下文与使用基于神经生理学的度量集的提示交换控制的条件特异性。我们在 CamCAN 和 Omega 上进行训练，并对保留的 MOUS 进行所有分析，建立跨数据集泛化。从指标来看，各代在长期部署过程中保持相对稳定，并且比交换的控件更接近正确的延续。代码位于：此 https URL。</li>
</ul>

<h3>Title: What's the plan? Metrics for implicit planning in LLMs and their application to rhyme generation and question answering</h3>
<ul>
<li><strong>Authors: </strong>Jim Maar, Denis Paperno, Callum Stuart McDougall, Neel Nanda</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20164">https://arxiv.org/abs/2601.20164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20164">https://arxiv.org/pdf/2601.20164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20164]] What's the plan? Metrics for implicit planning in LLMs and their application to rhyme generation and question answering(https://arxiv.org/abs/2601.20164)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Prior work suggests that language models, while trained on next token prediction, show implicit planning behavior: they may select the next token in preparation to a predicted future token, such as a likely rhyming word, as supported by a prior qualitative study of Claude 3.5 Haiku using a cross-layer transcoder. We propose much simpler techniques for assessing implicit planning in language models. With case studies on rhyme poetry generation and question answering, we demonstrate that our methodology easily scales to many models. Across models, we find that the generated rhyme (e.g. "-ight") or answer to a question ("whale") can be manipulated by steering at the end of the preceding line with a vector, affecting the generation of intermediate tokens leading up to the rhyme or answer word. We show that implicit planning is a universal mechanism, present in smaller models than previously thought, starting from 1B parameters. Our methodology offers a widely applicable direct way to study implicit planning abilities of LLMs. More broadly, understanding planning abilities of language models can inform decisions in AI safety and control.</li>
<li><strong>摘要：</strong>先前的工作表明，语言模型在接受下一个标记预测训练时，会表现出隐含的规划行为：它们可能会选择下一个标记，为预测的未来标记做好准备，例如可能的押韵词，这一点得到了先前使用跨层转码器对 Claude 3.5 Haiku 的定性研究的支持。我们提出了更简单的技术来评估语言模型中的隐式规划。通过对韵诗生成和问答的案例研究，我们证明了我们的方法可以轻松扩展到许多模型。在各个模型中，我们发现生成的韵律（例如“-ight”）或问题的答案（“whale”）可以通过在前一行的末尾使用向量进行操纵，从而影响导致韵律或答案词的中间标记的生成。我们证明隐式规划是一种通用机制，从 1B 参数开始，出现在比之前想象的更小的模型中。我们的方法提供了一种广泛适用的直接方法来研究法学硕士的隐性规划能力。更广泛地说，了解语言模型的规划能力可以为人工智能安全和控制方面的决策提供信息。</li>
</ul>

<h3>Title: Efficient Token Pruning for LLaDA-V</h3>
<ul>
<li><strong>Authors: </strong>Zhewen Wan, Tianchen Song, Chen Lin, Zhiyong Zhao, Xianpeng Lang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20168">https://arxiv.org/abs/2601.20168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20168">https://arxiv.org/pdf/2601.20168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20168]] Efficient Token Pruning for LLaDA-V(https://arxiv.org/abs/2601.20168)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion-based large multimodal models, such as LLaDA-V, have demonstrated impressive capabilities in vision-language understanding and generation. However, their bidirectional attention mechanism and diffusion-style iterative denoising paradigm introduce significant computational overhead, as visual tokens are repeatedly processed across all layers and denoising steps. In this work, we conduct an in-depth attention analysis and reveal that, unlike autoregressive decoders, LLaDA-V aggregates cross-modal information predominantly in middle-to-late layers, leading to delayed semantic alignment. Motivated by this observation, we propose a structured token pruning strategy inspired by FastV, selectively removing a proportion of visual tokens at designated layers to reduce FLOPs while preserving critical semantic information. To the best of our knowledge, this is the first work to investigate structured token pruning in diffusion-based large multimodal models. Unlike FastV, which focuses on shallow-layer pruning, our method targets the middle-to-late layers of the first denoising step to align with LLaDA-V's delayed attention aggregation to maintain output quality, and the first-step pruning strategy reduces the computation across all subsequent steps. Our framework provides an empirical basis for efficient LLaDA-V inference and highlights the potential of vision-aware pruning in diffusion-based multimodal models. Across multiple benchmarks, our best configuration reduces computational cost by up to 65% while preserving an average of 95% task performance.</li>
<li><strong>摘要：</strong>基于扩散的大型多模态模型，例如 LLaDA-V，在视觉语言理解和生成方面表现出了令人印象深刻的能力。然而，它们的双向注意力机制和扩散式迭代去噪范式引入了显着的计算开销，因为视觉标记在所有层和去噪步骤中重复处理。在这项工作中，我们进行了深入的注意力分析，并揭示了与自回归解码器不同，LLaDA-V 主要在中后期层聚合跨模态信息，导致语义对齐延迟。受这一观察的启发，我们提出了一种受 FastV 启发的结构化标记修剪策略，有选择地删除指定层的一部分视觉标记，以减少 FLOP，同时保留关键语义信息。据我们所知，这是第一个研究基于扩散的大型多模态模型中结构化令牌修剪的工作。与专注于浅层剪枝的 FastV 不同，我们的方法针对第一个去噪步骤的中后期层，以与 LLaDA-V 的延迟注意力聚合保持一致，以保持输出质量，并且第一步剪枝策略减少了所有后续步骤的计算量。我们的框架为高效的 LLaDA-V 推理提供了经验基础，并强调了基于扩散的多模态模型中视觉感知修剪的潜力。在多个基准测试中，我们的最佳配置可将计算成本降低高达 65%，同时保持平均 95% 的任务性能。</li>
</ul>

<h3>Title: DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment</h3>
<ul>
<li><strong>Authors: </strong>Haoyou Deng, Keyu Yan, Chaojie Mao, Xiang Wang, Yu Liu, Changxin Gao, Nong Sang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20218">https://arxiv.org/abs/2601.20218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20218">https://arxiv.org/pdf/2601.20218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20218]] DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment(https://arxiv.org/abs/2601.20218)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in human preference alignment for text-to-image generation. Nevertheless, they still suffer from the sparse reward problem: the terminal reward of the entire denoising trajectory is applied to all intermediate steps, resulting in a mismatch between the global feedback signals and the exact fine-grained contributions at intermediate denoising steps. To address this issue, we introduce \textbf{DenseGRPO}, a novel framework that aligns human preference with dense rewards, which evaluates the fine-grained contribution of each denoising step. Specifically, our approach includes two key components: (1) we propose to predict the step-wise reward gain as dense reward of each denoising step, which applies a reward model on the intermediate clean images via an ODE-based approach. This manner ensures an alignment between feedback signals and the contributions of individual steps, facilitating effective training; and (2) based on the estimated dense rewards, a mismatch drawback between the uniform exploration setting and the time-varying noise intensity in existing GRPO-based methods is revealed, leading to an inappropriate exploration space. Thus, we propose a reward-aware scheme to calibrate the exploration space by adaptively adjusting a timestep-specific stochasticity injection in the SDE sampler, ensuring a suitable exploration space at all timesteps. Extensive experiments on multiple standard benchmarks demonstrate the effectiveness of the proposed DenseGRPO and highlight the critical role of the valid dense rewards in flow matching model alignment.</li>
<li><strong>摘要：</strong>最近建立在流匹配模型之上的基于 GRPO 的方法在文本到图像生成的人类偏好对齐方面显示出显着的改进。尽管如此，它们仍然面临稀疏奖励问题：整个去噪轨迹的最终奖励应用于所有中间步骤，导致全局反馈信号与中间去噪步骤的精确细粒度贡献之间不匹配。为了解决这个问题，我们引入了 \textbf{DenseGRPO}，这是一种将人类偏好与密集奖励结合起来的新颖框架，它评估每个去噪步骤的细粒度贡献。具体来说，我们的方法包括两个关键组成部分：（1）我们建议将逐步奖励增益预测为每个去噪步骤的密集奖励，这通过基于 ODE 的方法在中间干净图像上应用奖励模型。这种方式确保反馈信号与各个步骤的贡献之间的一致性，从而促进有效的培训； (2)基于估计的密集奖励，揭示了现有基于GRPO的方法中均匀探索设置和时变噪声强度之间的不匹配缺陷，导致不适当的探索空间。因此，我们提出了一种奖励感知方案，通过自适应调整 SDE 采样器中特定于时间步长的随机性注入来校准探索空间，确保在所有时间步长都有合适的探索空间。对多个标准基准的广泛实验证明了所提出的 DenseGRPO 的有效性，并强调了有效密集奖励在流匹配模型对齐中的关键作用。</li>
</ul>

<h3>Title: Parametric and Generative Forecasts of Day-Ahead Market Curves for Storage Optimization</h3>
<ul>
<li><strong>Authors: </strong>Julian Gutierrez, Redouane Silvente</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20226">https://arxiv.org/abs/2601.20226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20226">https://arxiv.org/pdf/2601.20226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20226]] Parametric and Generative Forecasts of Day-Ahead Market Curves for Storage Optimization(https://arxiv.org/abs/2601.20226)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present two machine learning frameworks for forecasting aggregated curves and optimizing storage in the EPEX SPOT day-ahead market. First, a fast parametric model forecasts hourly demand and supply curves in a low-dimensional and grid-robust representation, with minimum and maximum volumes combined with a Chebyshev polynomial for the elastic segment. The model enables daily use with low error and clear interpretability. Second, for a more comprehensive analysis, though less suited to daily operation, we employ generative models that learn the joint distribution of 24-hour order-level submissions given weather and fuel variables. These models generate synthetic daily scenarios of individual buy and sell orders, which, once aggregated, yield hourly supply and demand curves. Based on these forecasts, we optimize a price-making storage strategy, quantify revenue distributions, and highlight the price-compression effect with lower peaks, higher off-peak levels, and diminishing returns as capacity expands.</li>
<li><strong>摘要：</strong>我们提出了两个机器学习框架，用于预测 EPEX SPOT 日前市场的聚合曲线和优化存储。首先，快速参数模型以低维和网格稳健的表示形式预测每小时的需求和供给曲线，其中最小和最大交易量与弹性段的切比雪夫多项式相结合。该模型可实现日常使用，误差低且可解释性清晰。其次，为了进行更全面的分析，尽管不太适合日常操作，我们采用生成模型来学习给定天气和燃料变量的 24 小时订单级别提交的联合分布。这些模型生成单个买卖订单的综合每日场景，这些订单一旦汇总，就会产生每小时的供给和需求曲线。根据这些预测，我们优化存储定价策略，量化收入分配，并突出峰值更低、非高峰水平更高以及容量扩张带来的收益递减的价格压缩效应。</li>
</ul>

<h3>Title: ProFlow: Zero-Shot Physics-Consistent Sampling via Proximal Flow Guidance</h3>
<ul>
<li><strong>Authors: </strong>Zichao Yu, Ming Li, Wenyi Zhang, Difan Zou, Weiguo Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20227">https://arxiv.org/abs/2601.20227</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20227">https://arxiv.org/pdf/2601.20227</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20227]] ProFlow: Zero-Shot Physics-Consistent Sampling via Proximal Flow Guidance(https://arxiv.org/abs/2601.20227)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Inferring physical fields from sparse observations while strictly satisfying partial differential equations (PDEs) is a fundamental challenge in computational physics. Recently, deep generative models offer powerful data-driven priors for such inverse problems, yet existing methods struggle to enforce hard physical constraints without costly retraining or disrupting the learned generative prior. Consequently, there is a critical need for a sampling mechanism that can reconcile strict physical consistency and observational fidelity with the statistical structure of the pre-trained prior. To this end, we present ProFlow, a proximal guidance framework for zero-shot physics-consistent sampling, defined as inferring solutions from sparse observations using a fixed generative prior without task-specific retraining. The algorithm employs a rigorous two-step scheme that alternates between: (\romannumeral1) a terminal optimization step, which projects the flow prediction onto the intersection of the physically and observationally consistent sets via proximal minimization; and (\romannumeral2) an interpolation step, which maps the refined state back to the generative trajectory to maintain consistency with the learned flow probability path. This procedure admits a Bayesian interpretation as a sequence of local maximum a posteriori (MAP) updates. Comprehensive benchmarks on Poisson, Helmholtz, Darcy, and viscous Burgers' equations demonstrate that ProFlow achieves superior physical and observational consistency, as well as more accurate distributional statistics, compared to state-of-the-art diffusion- and flow-based baselines.</li>
<li><strong>摘要：</strong>从稀疏观测中推断物理场，同时严格满足偏微分方程（PDE）是计算物理学中的一个基本挑战。最近，深度生成模型为此类逆问题提供了强大的数据驱动先验，但现有方法很难在不进行昂贵的再训练或破坏学习的生成先验的情况下强制实施严格的物理约束。因此，迫切需要一种采样机制，能够将严格的物理一致性和观察保真度与预训练先验的统计结构相协调。为此，我们提出了 ProFlow，一种用于零样本物理一致性采样的近端引导框架，其定义为使用固定的生成先验从稀疏观察中推断解决方案，而无需特定于任务的再训练。该算法采用严格的两步方案，交替进行：（\romannumeral1）终端优化步骤，通过近端最小化将流预测投影到物理和观察一致集的交集上；和（\romannumeral2）插值步骤，它将精炼状态映射回生成轨迹，以保持与学习的流概率路径的一致性。该过程承认贝叶斯解释为局部最大后验 (MAP) 更新的序列。泊松、亥姆霍兹、达西和粘性伯格斯方程的综合基准表明，与最先进的基于扩散和流动的基线相比，ProFlow 实现了卓越的物理和观测一致性，以及更准确的分布统计数据。</li>
</ul>

<h3>Title: BLENDER: Blended Text Embeddings and Diffusion Residuals for Intra-Class Image Synthesis in Deep Metric Learning</h3>
<ul>
<li><strong>Authors: </strong>Jan Niklas Kolf, Ozan Tezcan, Justin Theiss, Hyung Jun Kim, Wentao Bao, Bhargav Bhushanam, Khushi Gupta, Arun Kejariwal, Naser Damer, Fadi Boutros</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20246">https://arxiv.org/abs/2601.20246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20246">https://arxiv.org/pdf/2601.20246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20246]] BLENDER: Blended Text Embeddings and Diffusion Residuals for Intra-Class Image Synthesis in Deep Metric Learning(https://arxiv.org/abs/2601.20246)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The rise of Deep Generative Models (DGM) has enabled the generation of high-quality synthetic data. When used to augment authentic data in Deep Metric Learning (DML), these synthetic samples enhance intra-class diversity and improve the performance of downstream DML tasks. We introduce BLenDeR, a diffusion sampling method designed to increase intra-class diversity for DML in a controllable way by leveraging set-theory inspired union and intersection operations on denoising residuals. The union operation encourages any attribute present across multiple prompts, while the intersection extracts the common direction through a principal component surrogate. These operations enable controlled synthesis of diverse attribute combinations within each class, addressing key limitations of existing generative approaches. Experiments on standard DML benchmarks demonstrate that BLenDeR consistently outperforms state-of-the-art baselines across multiple datasets and backbones. Specifically, BLenDeR achieves 3.7% increase in Recall@1 on CUB-200 and a 1.8% increase on Cars-196, compared to state-of-the-art baselines under standard experimental settings.</li>
<li><strong>摘要：</strong>深度生成模型（DGM）的兴起使得高质量合成数据的生成成为可能。当用于增强深度度量学习 (DML) 中的真实数据时，这些合成样本可增强类内多样性并提高下游 DML 任务的性能。我们引入了 BLenDeR，这是一种扩散采样方法，旨在通过利用集合论启发的去噪残差的并集和交集运算，以可控的方式增加 DML 的类内多样性。并集操作鼓励多个提示中存在的任何属性，而交集则通过主成分代理提取共同方向。这些操作使得每个类中不同属性组合的受控合成成为可能，解决了现有生成方法的关键限制。标准 DML 基准测试表明，BLenDeR 在多个数据集和主干上始终优于最先进的基准。具体而言，与标准实验设置下的最先进基线相比，BLenDeR 在 CUB-200 上的 Recall@1 提高了 3.7%，在 Cars-196 上提高了 1.8%。</li>
</ul>

<h3>Title: Order-Optimal Sample Complexity of Rectified Flows</h3>
<ul>
<li><strong>Authors: </strong>Hari Krishna Sahoo, Mudit Gaur, Vaneet Aggarwal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IT, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20250">https://arxiv.org/abs/2601.20250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20250">https://arxiv.org/pdf/2601.20250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20250]] Order-Optimal Sample Complexity of Rectified Flows(https://arxiv.org/abs/2601.20250)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recently, flow-based generative models have shown superior efficiency compared to diffusion models. In this paper, we study rectified flow models, which constrain transport trajectories to be linear from the base distribution to the data distribution. This structural restriction greatly accelerates sampling, often enabling high-quality generation with a single Euler step. Under standard assumptions on the neural network classes used to parameterize the velocity field and data distribution, we prove that rectified flows achieve sample complexity $\tilde{O}(\varepsilon^{-2})$. This improves on the best known $O(\varepsilon^{-4})$ bounds for flow matching model and matches the optimal rate for mean estimation. Our analysis exploits the particular structure of rectified flows: because the model is trained with a squared loss along linear paths, the associated hypothesis class admits a sharply controlled localized Rademacher complexity. This yields the improved, order-optimal sample complexity and provides a theoretical explanation for the strong empirical performance of rectified flow models.</li>
<li><strong>摘要：</strong>最近，与扩散模型相比，基于流的生成模型显示出更高的效率。在本文中，我们研究了修正流模型，该模型将传输轨迹从基础分布到数据分布限制为线性。这种结构限制极大地加速了采样，通常可以通过单个欧拉步骤实现高质量生成。在用于参数化速度场和数据分布的神经网络类的标准假设下，我们证明修正流实现了样本复杂度 $\tilde{O}(\varepsilon^{-2})$。这改进了流匹配模型最著名的 $O(\varepsilon^{-4})$ 界限，并匹配平均估计的最佳速率。我们的分析利用了整流流的特殊结构：因为模型是通过沿线性路径的平方损失进行训练的，所以相关的假设类承认严格控制的局部 Rademacher 复杂度。这产生了改进的、顺序最优的样本复杂性，并为修正流模型的强大经验性能提供了理论解释。</li>
</ul>

<h3>Title: C2:Cross learning module enhanced decision transformer with Constraint-aware loss for auto-bidding</h3>
<ul>
<li><strong>Authors: </strong>Jinren Ding, Xuejian Xu, Shen Jiang, Zhitong Hao, Jinhui Yang, Peng Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20257">https://arxiv.org/abs/2601.20257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20257">https://arxiv.org/pdf/2601.20257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20257]] C2:Cross learning module enhanced decision transformer with Constraint-aware loss for auto-bidding(https://arxiv.org/abs/2601.20257)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Decision Transformer (DT) shows promise for generative auto-bidding by capturing temporal dependencies, but suffers from two critical limitations: insufficient cross-correlation modeling among state, action, and return-to-go (RTG) sequences, and indiscriminate learning of optimal/suboptimal behaviors. To address these, we propose C2, a novel framework enhancing DT with two core innovations: (1) a Cross Learning Block (CLB) via cross-attention to strengthen inter-sequence correlation modeling; (2) a Constraint-aware Loss (CL) incorporating budget and Cost-Per-Acquisition (CPA) constraints for selective learning of optimal trajectories. Extensive offline evaluations on the AuctionNet dataset demonstrate consistent performance gains (up to 3.23\% over state-of-the-art GAVE) across diverse budget settings; ablation studies verify the complementary synergy of CLB and CL, confirming C2's superiority in auto-bidding. The code for reproducing our results is available at: this https URL.</li>
<li><strong>摘要：</strong>决策转换器 (DT) 通过捕获时间依赖性显示了生成式自动投标的前景，但存在两个关键限制：状态、动作和返回 (RTG) 序列之间的互相关建模不足，以及不加区别地学习最优/次优行为。为了解决这些问题，我们提出了 C2，这是一种通过两个核心创新来增强 DT 的新颖框架：（1）通过交叉注意力来加强序列间相关性建模的交叉学习块（CLB）； (2) 约束感知损失 (CL) 结合了预算和每次获取成本 (CPA) 约束，用于选择性学习最佳轨迹。对 AuctionNet 数据集进行的广泛离线评估表明，在不同的预算设置下，性能始终保持一致（比最先进的 GAVE 提高了 3.23%）；消融研究验证了 CLB 和 CL 的互补协同作用，证实了 C2 在自动投标方面的优势。用于重现结果的代码可在以下位置找到：此 https URL。</li>
</ul>

<h3>Title: Reversible Efficient Diffusion for Image Fusion</h3>
<ul>
<li><strong>Authors: </strong>Xingxin Xu, Bing Cao, DongDong Li, Qinghua Hu, Pengfei Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20260">https://arxiv.org/abs/2601.20260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20260">https://arxiv.org/pdf/2601.20260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20260]] Reversible Efficient Diffusion for Image Fusion(https://arxiv.org/abs/2601.20260)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Multi-modal image fusion aims to consolidate complementary information from diverse source images into a unified representation. The fused image is expected to preserve fine details and maintain high visual fidelity. While diffusion models have demonstrated impressive generative capabilities in image generation, they often suffer from detail loss when applied to image fusion tasks. This issue arises from the accumulation of noise errors inherent in the Markov process, leading to inconsistency and degradation in the fused results. However, incorporating explicit supervision into end-to-end training of diffusion-based image fusion introduces challenges related to computational efficiency. To address these limitations, we propose the Reversible Efficient Diffusion (RED) model - an explicitly supervised training framework that inherits the powerful generative capability of diffusion models while avoiding the distribution estimation.</li>
<li><strong>摘要：</strong>多模态图像融合旨在将不同源图像的互补信息整合为统一的表示。融合图像有望保留精细细节并保持高视觉保真度。虽然扩散模型在图像生成方面表现出了令人印象深刻的生成能力，但在应用于图像融合任务时，它们经常会遭受细节损失。该问题是由马尔可夫过程中固有的噪声误差累积引起的，导致融合结果的不一致和退化。然而，将显式监督纳入基于扩散的图像融合的端到端训练会带来与计算效率相关的挑战。为了解决这些限制，我们提出了可逆高效扩散（RED）模型 - 一种明确监督的训练框架，继承了扩散模型强大的生成能力，同时避免了分布估计。</li>
</ul>

<h3>Title: A Learning-based Framework for Spatial Impulse Response Compensation in 3D Photoacoustic Computed Tomography</h3>
<ul>
<li><strong>Authors: </strong>Kaiyi Yang, Seonyeong Park, Gangwon Jeong, Hsuan-Kai Huang, Alexander A. Oraevsky, Umberto Villa, Mark A. Anastasio</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP, physics.med-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20291">https://arxiv.org/abs/2601.20291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20291">https://arxiv.org/pdf/2601.20291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20291]] A Learning-based Framework for Spatial Impulse Response Compensation in 3D Photoacoustic Computed Tomography(https://arxiv.org/abs/2601.20291)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Photoacoustic computed tomography (PACT) is a promising imaging modality that combines the advantages of optical contrast with ultrasound detection. Utilizing ultrasound transducers with larger surface areas can improve detection sensitivity. However, when computationally efficient analytic reconstruction methods that neglect the spatial impulse responses (SIRs) of the transducer are employed, the spatial resolution of the reconstructed images will be compromised. Although optimization-based reconstruction methods can explicitly account for SIR effects, their computational cost is generally high, particularly in three-dimensional (3D) applications. To address the need for accurate but rapid 3D PACT image reconstruction, this study presents a framework for establishing a learned SIR compensation method that operates in the data domain. The learned compensation method maps SIR-corrupted PACT measurement data to compensated data that would have been recorded by idealized point-like transducers. Subsequently, the compensated data can be used with a computationally efficient reconstruction method that neglects SIR effects. Two variants of the learned compensation model are investigated that employ a U-Net model and a specifically designed, physics-inspired model, referred to as Deconv-Net. A fast and analytical training data generation procedure is also a component of the presented framework. The framework is rigorously validated in virtual imaging studies, demonstrating resolution improvement and robustness to noise variations, object complexity, and sound speed heterogeneity. When applied to in-vivo breast imaging data, the learned compensation models revealed fine structures that had been obscured by SIR-induced artifacts. To our knowledge, this is the first demonstration of learned SIR compensation in 3D PACT imaging.</li>
<li><strong>摘要：</strong>光声计算机断层扫描（PACT）是一种很有前途的成像方式，它结合了光学对比与超声检测的优点。使用具有较大表面积的超声换能器可以提高检测灵敏度。然而，当采用忽略换能器的空间脉冲响应（SIR）的计算高效的分析重建方法时，重建图像的空间分辨率将受到损害。尽管基于优化的重建方法可以明确地考虑 SIR 效应，但其计算成本通常很高，特别是在三维 (3D) 应用中。为了满足准确而快速的 3D PACT 图像重建的需求，本研究提出了一个框架，用于建立在数据域中运行的学习 SIR 补偿方法。学习的补偿方法将 SIR 损坏的 PACT 测量数据映射到理想化点状传感器记录的补偿数据。随后，补偿后的数据可以与忽略 SIR 效应的计算有效的重建方法一起使用。研究了学习补偿模型的两种变体，它们采用 U-Net 模型和专门设计的物理启发模型（称为 Deconv-Net）。快速且分析的训练数据生成过程也是所提出框架的组成部分。该框架在虚拟成像研究中经过严格验证，证明了分辨率的提高以及对噪声变化、物体复杂性和声速异质性的鲁棒性。当应用于体内乳腺成像数据时，学习到的补偿模型揭示了被 SIR 引起的伪影所掩盖的精细结构。据我们所知，这是 3D PACT 成像中学习 SIR 补偿的首次演示。</li>
</ul>

<h3>Title: Artifact-Aware Evaluation for High-Quality Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Chen Zhu, Jiashu Zhu, Yanxun Li, Meiqi Wu, Bingze Song, Chubin Chen, Jiahong Wu, Xiangxiang Chu, Yangang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20297">https://arxiv.org/abs/2601.20297</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20297">https://arxiv.org/pdf/2601.20297</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20297]] Artifact-Aware Evaluation for High-Quality Video Generation(https://arxiv.org/abs/2601.20297)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of video generation techniques, evaluating and auditing generated videos has become increasingly crucial. Existing approaches typically offer coarse video quality scores, lacking detailed localization and categorization of specific artifacts. In this work, we introduce a comprehensive evaluation protocol focusing on three key aspects affecting human perception: Appearance, Motion, and Camera. We define these axes through a taxonomy of 10 prevalent artifact categories reflecting common generative failures observed in video generation. To enable robust artifact detection and categorization, we introduce GenVID, a large-scale dataset of 80k videos generated by various state-of-the-art video generation models, each carefully annotated for the defined artifact categories. Leveraging GenVID, we develop DVAR, a Dense Video Artifact Recognition framework for fine-grained identification and classification of generative artifacts. Extensive experiments show that our approach significantly improves artifact detection accuracy and enables effective filtering of low-quality content.</li>
<li><strong>摘要：</strong>随着视频生成技术的快速进步，评估和审核生成的视频变得越来越重要。现有方法通常提供粗略的视频质量分数，缺乏特定工件的详细定位和分类。在这项工作中，我们引入了一个综合评估协议，重点关注影响人类感知的三个关键方面：外观、运动和相机。我们通过 10 个流行的工件类别的分类来定义这些轴，反映了视频生成中观察到的常见生成失败。为了实现强大的伪影检测和分类，我们引入了 GenVID，这是一个由各种最先进的视频生成模型生成的 80k 视频的大型数据集，每个模型都针对定义的伪影类别进行了仔细注释。利用 GenVID，我们开发了 DVAR，一种密集视频伪像识别框架，用于对生成伪像进行细粒度识别和分类。大量实验表明，我们的方法显着提高了伪影检测的准确性，并能够有效过滤低质量内容。</li>
</ul>

<h3>Title: Structure-constrained Language-informed Diffusion Model for Unpaired Low-dose Computed Tomography Angiography Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Genyuan Zhang, Zihao Wang, Zhifan Gao, Lei Xu, Zhen Zhou, Haijun Yu, Jianjia Zhang, Xiujian Liu, Weiwei Zhang, Shaoyu Wang, Huazhu Fu, Fenglin Liu, Weiwen Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20304">https://arxiv.org/abs/2601.20304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20304">https://arxiv.org/pdf/2601.20304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20304]] Structure-constrained Language-informed Diffusion Model for Unpaired Low-dose Computed Tomography Angiography Reconstruction(https://arxiv.org/abs/2601.20304)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The application of iodinated contrast media (ICM) improves the sensitivity and specificity of computed tomography (CT) for a wide range of clinical indications. However, overdose of ICM can cause problems such as kidney damage and life-threatening allergic reactions. Deep learning methods can generate CT images of normal-dose ICM from low-dose ICM, reducing the required dose while maintaining diagnostic power. However, existing methods are difficult to realize accurate enhancement with incompletely paired images, mainly because of the limited ability of the model to recognize specific structures. To overcome this limitation, we propose a Structure-constrained Language-informed Diffusion Model (SLDM), a unified medical generation model that integrates structural synergy and spatial intelligence. First, the structural prior information of the image is effectively extracted to constrain the model inference process, thus ensuring structural consistency in the enhancement process. Subsequently, semantic supervision strategy with spatial intelligence is introduced, which integrates the functions of visual perception and spatial reasoning, thus prompting the model to achieve accurate enhancement. Finally, the subtraction angiography enhancement module is applied, which serves to improve the contrast of the ICM agent region to suitable interval for observation. Qualitative analysis of visual comparison and quantitative results of several metrics demonstrate the effectiveness of our method in angiographic reconstruction for low-dose contrast medium CT angiography.</li>
<li><strong>摘要：</strong>碘造影剂 (ICM) 的应用提高了计算机断层扫描 (CT) 对多种临床适应症的敏感性和特异性。然而，过量服用 ICM 可能会导致肾脏损伤和危及生命的过敏反应等问题。深度学习方法可以从低剂量 ICM 生成正常剂量 ICM 的 CT 图像，在保持诊断能力的同时减少所需剂量。然而，现有方法很难实现不完全配对图像的精确增强，这主要是因为模型识别特定结构的能力有限。为了克服这一限制，我们提出了结构约束语言信息扩散模型（SLDM），这是一种集成结构协同和空间智能的统一医学生成模型。首先，有效提取图像的结构先验信息来约束模型推理过程，从而保证增强过程中的结构一致性。随后，引入了具有空间智能的语义监督策略，将视觉感知和空间推理的功能融为一体，从而促使模型实现精确增强。最后，应用减影血管造影增强模块，其用于将ICM试剂区域的对比度提高到合适的观察间隔。视觉比较的定性分析和几个指标的定量结果证明了我们的方法在低剂量造影剂 CT 血管造影的血管造影重建中的有效性。</li>
</ul>

<h3>Title: TPGDiff: Hierarchical Triple-Prior Guided Diffusion for Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Yanjie Tu, Qingsen Yan, Axi Niu, Jiacong Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20306">https://arxiv.org/abs/2601.20306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20306">https://arxiv.org/pdf/2601.20306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20306]] TPGDiff: Hierarchical Triple-Prior Guided Diffusion for Image Restoration(https://arxiv.org/abs/2601.20306)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generation</a></li>
<li><strong>Abstract: </strong>All-in-one image restoration aims to address diverse degradation types using a single unified model. Existing methods typically rely on degradation priors to guide restoration, yet often struggle to reconstruct content in severely degraded regions. Although recent works leverage semantic information to facilitate content generation, integrating it into the shallow layers of diffusion models often disrupts spatial structures (\emph{e.g.}, blurring artifacts). To address this issue, we propose a Triple-Prior Guided Diffusion (TPGDiff) network for unified image restoration. TPGDiff incorporates degradation priors throughout the diffusion trajectory, while introducing structural priors into shallow layers and semantic priors into deep layers, enabling hierarchical and complementary prior guidance for image reconstruction. Specifically, we leverage multi-source structural cues as structural priors to capture fine-grained details and guide shallow layers representations. To complement this design, we further develop a distillation-driven semantic extractor that yields robust semantic priors, ensuring reliable high-level guidance at deep layers even under severe degradations. Furthermore, a degradation extractor is employed to learn degradation-aware priors, enabling stage-adaptive control of the diffusion process across all timesteps. Extensive experiments on both single- and multi-degradation benchmarks demonstrate that TPGDiff achieves superior performance and generalization across diverse restoration scenarios. Our project page is: this https URL.</li>
<li><strong>摘要：</strong>一体式图像恢复旨在使用单一统一模型解决各种退化类型。现有方法通常依赖于退化先验来指导恢复，但通常难以重建严重退化区域的内容。尽管最近的工作利用语义信息来促进内容生成，但将其集成到扩散模型的浅层中通常会破坏空间结构（例如，模糊伪影）。为了解决这个问题，我们提出了一种三优先引导扩散（TPGDiff）网络来进行统一图像恢复。 TPGDiff 在整个扩散轨迹中融入了退化先验，同时将结构先验引入浅层，将语义先验引入深层，从而为图像重建提供分层和互补的先验指导。具体来说，我们利用多源结构线索作为结构先验来捕获细粒度细节并指导浅层表示。为了补充这一设计，我们进一步开发了一种蒸馏驱动的语义提取器，它可以产生强大的语义先验，即使在严重退化的情况下也能确保深层可靠的高级指导。此外，采用退化提取器来学习退化感知先验，从而能够对所有时间步长的扩散过程进行阶段自适应控制。对单一和多重退化基准的大量实验表明，TPGDiff 在不同的恢复场景中实现了卓越的性能和泛化。我们的项目页面是：这个https URL。</li>
</ul>

<h3>Title: OSDEnhancer: Taming Real-World Space-Time Video Super-Resolution with One-Step Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Shuoyan Wei, Feng Li, Chen Zhou, Runmin Cong, Yao Zhao, Huihui Bai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20308">https://arxiv.org/abs/2601.20308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20308">https://arxiv.org/pdf/2601.20308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20308]] OSDEnhancer: Taming Real-World Space-Time Video Super-Resolution with One-Step Diffusion(https://arxiv.org/abs/2601.20308)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Diffusion models (DMs) have demonstrated exceptional success in video super-resolution (VSR), showcasing a powerful capacity for generating fine-grained details. However, their potential for space-time video super-resolution (STVSR), which necessitates not only recovering realistic visual content from low-resolution to high-resolution but also improving the frame rate with coherent temporal dynamics, remains largely underexplored. Moreover, existing STVSR methods predominantly address spatiotemporal upsampling under simplified degradation assumptions, which often struggle in real-world scenarios with complex unknown degradations. Such a high demand for reconstruction fidelity and temporal consistency makes the development of a robust STVSR framework particularly non-trivial. To address these challenges, we propose OSDEnhancer, a novel framework that, to the best of our knowledge, represents the first method to achieve real-world STVSR through an efficient one-step diffusion process. OSDEnhancer initializes essential spatiotemporal structures through a linear pre-interpolation strategy and pivots on training temporal refinement and spatial enhancement mixture of experts (TR-SE MoE), which allows distinct expert pathways to progressively learn robust, specialized representations for temporal coherence and spatial detail, further collaboratively reinforcing each other during inference. A bidirectional deformable variational autoencoder (VAE) decoder is further introduced to perform recurrent spatiotemporal aggregation and propagation, enhancing cross-frame reconstruction fidelity. Experiments demonstrate that the proposed method achieves state-of-the-art performance while maintaining superior generalization capability in real-world scenarios.</li>
<li><strong>摘要：</strong>扩散模型 (DM) 在视频超分辨率 (VSR) 方面取得了巨大成功，展示了生成细粒度细节的强大能力。然而，它们在时空视频超分辨率（STVSR）方面的潜力仍然很大程度上尚未得到充分开发，这不仅需要将逼真的视觉内容从低分辨率恢复到高分辨率，而且还需要通过连贯的时间动态来提高帧速率。此外，现有的 STVSR 方法主要解决简化退化假设下的时空上采样，这通常在具有复杂未知退化的现实场景中遇到困难。对重建保真度和时间一致性的如此高要求使得强大的 STVSR 框架的开发变得尤为重要。为了应对这些挑战，我们提出了 OSDEnhancer，这是一种新颖的框架，据我们所知，它代表了第一种通过高效的一步扩散过程实现现实世界 STVSR 的方法。 OSDEnhancer 通过线性预插值策略初始化基本的时空结构，并以训练专家的时间细化和空间增强混合 (TR-SE MoE) 为中心，这允许不同的专家路径逐步学习时间连贯性和空间细节的稳健、专门的表示，并在推理过程中进一步相互协作加强。进一步引入双向可变形变分自编码器（VAE）解码器来执行循环时空聚合和传播，增强跨帧重建保真度。实验表明，所提出的方法实现了最先进的性能，同时在现实场景中保持了卓越的泛化能力。</li>
</ul>

<h3>Title: Window-Diffusion: Accelerating Diffusion Language Model Inference with Windowed Token Pruning and Caching</h3>
<ul>
<li><strong>Authors: </strong>Fengrui Zuo, Zhiwei Ke, Yiming Liu, Wenqi Lou, Chao Wang, Xvehai Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20332">https://arxiv.org/abs/2601.20332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20332">https://arxiv.org/pdf/2601.20332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20332]] Window-Diffusion: Accelerating Diffusion Language Model Inference with Windowed Token Pruning and Caching(https://arxiv.org/abs/2601.20332)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion language models (DLMs) generate text through iterative denoising, but inference requires full-sequence attention at every iteration, resulting in substantial redundant computation on masked tokens. Block-wise diffusion can reduce this cost, yet it typically relies on retraining and constrained update orders, limiting its direct applicability to pretrained DLMs. Our token-level analysis reveals pronounced structural locality in DLM inference. Decoding is driven by a small set of prefix-localized active tokens; the influence of distant undecoded context diminishes rapidly, and decoded tokens exhibit stage-wise temporal stability, enabling reuse of intermediate representations except for a brief post-decode transient. Motivated by these observations, we propose \textbf{\placeholder}\footnote{The source code is available at this https URL.}, a window-based token pruning and caching method for inference. We maintain a local computation window that slides rightward as denoising progresses, and partition undecoded tokens into: (i) \textit{active tokens} that are computed online, (ii) \textit{buffer tokens} whose KV states are cached and periodically refreshed, and (iii) \textit{far-field tokens} that are pruned outside the window. Computation is restricted to active and buffer tokens within the window, while far-field tokens are omitted at each stage. Experiments on LLaDA and Dream show that, under matched compute budgets, our method achieves up to $99\times$ inference speedup while largely preserving generation performance.</li>
<li><strong>摘要：</strong>扩散语言模型（DLM）通过迭代去噪生成文本，但推理需要在每次迭代时进行全序列关注，从而导致对屏蔽标记进行大量冗余计算。分块扩散可以降低这种成本，但它通常依赖于重新训练和约束更新顺序，限制了其对预训练 DLM 的直接适用性。我们的 token 级分析揭示了 DLM 推理中明显的结构局部性。解码由一小组前缀本地化的活动令牌驱动；遥远的未解码上下文的影响迅速减弱，并且解码的令牌表现出阶段性的时间稳定性，使得除了短暂的解码后瞬态之外，能够重用中间表示。受这些观察的启发，我们提出了 \textbf{\placeholder}\footnote{源代码可在此 https URL 获取。}，一种基于窗口的令牌修剪和缓存推理方法。我们维护一个随着去噪的进行而向右滑动的本地计算窗口，并将未解码的令牌划分为：（i）在线计算的\textit{active tokens}，（ii）其KV状态被缓存并定期刷新的\textit{buffer tokens}，以及（iii）在窗口外修剪的\textit{far-field tokens}。计算仅限于窗口内的活动令牌和缓冲令牌，而远场令牌在每个阶段都被忽略。 LLaDA 和 Dream 上的实验表明，在匹配的计算预算下，我们的方法实现了高达 99 美元的推理加速，同时很大程度上保留了生成性能。</li>
</ul>

<h3>Title: Can Continuous-Time Diffusion Models Generate and Solve Globally Constrained Discrete Problems? A Study on Sudoku</h3>
<ul>
<li><strong>Authors: </strong>Mariia Drozdova</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20363">https://arxiv.org/abs/2601.20363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20363">https://arxiv.org/pdf/2601.20363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20363]] Can Continuous-Time Diffusion Models Generate and Solve Globally Constrained Discrete Problems? A Study on Sudoku(https://arxiv.org/abs/2601.20363)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Can standard continuous-time generative models represent distributions whose support is an extremely sparse, globally constrained discrete set? We study this question using completed Sudoku grids as a controlled testbed, treating them as a subset of a continuous relaxation space. We train flow-matching and score-based models along a Gaussian probability path and compare deterministic (ODE) sampling, stochastic (SDE) sampling, and DDPM-style discretizations derived from the same continuous-time training. Unconditionally, stochastic sampling substantially outperforms deterministic flows; score-based samplers are the most reliable among continuous-time methods, and DDPM-style ancestral sampling achieves the highest validity overall. We further show that the same models can be repurposed for guided generation: by repeatedly sampling completions under clamped clues and stopping when constraints are satisfied, the model acts as a probabilistic Sudoku solver. Although far less sample-efficient than classical solvers and discrete-geometry-aware diffusion methods, these experiments demonstrate that classic diffusion/flow formulations can assign non-zero probability mass to globally constrained combinatorial structures and can be used for constraint satisfaction via stochastic search.</li>
<li><strong>摘要：</strong>标准连续时间生成模型能否表示支持极其稀疏、全局约束的离散集的分布？我们使用完整的数独网格作为受控测试台来研究这个问题，将它们视为连续松弛空间的子集。我们沿着高斯概率路径训练流匹配和基于分数的模型，并比较从相同的连续时间训练中得出的确定性 (ODE) 采样、随机 (SDE) 采样和 DDPM 式离散化。无条件地，随机采样大大优于确定性流；基于分数的采样器是连续时间方法中最可靠的，而 DDPM 式祖先采样的总体有效性最高。我们进一步表明，相同的模型可以重新用于引导生成：通过在固定线索下重复采样完成情况并在满足约束时停止，该模型充当概率数独求解器。尽管样本效率远低于经典求解器和离散几何感知扩散方法，但这些实验表明，经典扩散/流动公式可以将非零概率质量分配给全局约束组合结构，并且可以通过随机搜索用于约束满足。</li>
</ul>

<h3>Title: RAW-Flow: Advancing RGB-to-RAW Image Reconstruction with Deterministic Latent Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Zhen Liu, Diedong Feng, Hai Jiang, Liaoyuan Zeng, Hao Wang, Chaoyu Feng, Lei Lei, Bing Zeng, Shuaicheng Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20364">https://arxiv.org/abs/2601.20364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20364">https://arxiv.org/pdf/2601.20364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20364]] RAW-Flow: Advancing RGB-to-RAW Image Reconstruction with Deterministic Latent Flow Matching(https://arxiv.org/abs/2601.20364)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>RGB-to-RAW reconstruction, or the reverse modeling of a camera Image Signal Processing (ISP) pipeline, aims to recover high-fidelity RAW data from RGB images. Despite notable progress, existing learning-based methods typically treat this task as a direct regression objective and struggle with detail inconsistency and color deviation, due to the ill-posed nature of inverse ISP and the inherent information loss in quantized RGB images. To address these limitations, we pioneer a generative perspective by reformulating RGB-to-RAW reconstruction as a deterministic latent transport problem and introduce a novel framework named RAW-Flow, which leverages flow matching to learn a deterministic vector field in latent space, to effectively bridge the gap between RGB and RAW representations and enable accurate reconstruction of structural details and color information. To further enhance latent transport, we introduce a cross-scale context guidance module that injects hierarchical RGB features into the flow estimation process. Moreover, we design a dual-domain latent autoencoder with a feature alignment constraint to support the proposed latent transport framework, which jointly encodes RGB and RAW inputs while promoting stable training and high-fidelity reconstruction. Extensive experiments demonstrate that RAW-Flow outperforms state-of-the-art approaches both quantitatively and visually.</li>
<li><strong>摘要：</strong>RGB 到 RAW 重建或相机图像信号处理 (ISP) 管道的逆向建模旨在从 RGB 图像中恢复高保真 RAW 数据。尽管取得了显着的进展，但现有的基于学习的方法通常将此任务视为直接回归目标，并且由于逆 ISP 的不适定性质以及量化 RGB 图像中固有的信息丢失，因此会遇到细节不一致和颜色偏差的问题。为了解决这些限制，我们开创了一种生成视角，将 RGB 到 RAW 重建重新表述为确定性潜在传输问题，并引入了一种名为 RAW-Flow 的新颖框架，该框架利用流匹配来学习潜在空间中的确定性向量场，以有效弥合 RGB 和 RAW 表示之间的差距，并实现结构细节和颜色信息的准确重建。为了进一步增强潜在传输，我们引入了跨尺度上下文引导模块，该模块将分层 RGB 特征注入流估计过程中。此外，我们设计了一个具有特征对齐约束的双域潜在自动编码器，以支持所提出的潜在传输框架，该框架联合编码 RGB 和 RAW 输入，同时促进稳定的训练和高保真重建。大量实验表明，RAW-Flow 在定量和视觉上都优于最先进的方法。</li>
</ul>

<h3>Title: HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation</h3>
<ul>
<li><strong>Authors: </strong>Mengge Liu, Yan Di, Gu Wang, Yun Qu, Dekai Zhu, Yanyan Li, Xiangyang Ji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20383">https://arxiv.org/abs/2601.20383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20383">https://arxiv.org/pdf/2601.20383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20383]] HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation(https://arxiv.org/abs/2601.20383)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-driven multi-human motion generation with complex interactions remains a challenging problem. Despite progress in performance, existing offline methods that generate fixed-length motions with a fixed number of agents, are inherently limited in handling long or variable text, and varying agent counts. These limitations naturally encourage autoregressive formulations, which predict future motions step by step conditioned on all past trajectories and current text guidance. In this work, we introduce HINT, the first autoregressive framework for multi-human motion generation with Hierarchical INTeraction modeling in diffusion. First, HINT leverages a disentangled motion representation within a canonicalized latent space, decoupling local motion semantics from inter-person interactions. This design facilitates direct adaptation to varying numbers of human participants without requiring additional refinement. Second, HINT adopts a sliding-window strategy for efficient online generation, and aggregates local within-window and global cross-window conditions to capture past human history, inter-person dependencies, and align with text guidance. This strategy not only enables fine-grained interaction modeling within each window but also preserves long-horizon coherence across all the long sequence. Extensive experiments on public benchmarks demonstrate that HINT matches the performance of strong offline models and surpasses autoregressive baselines. Notably, on InterHuman, HINT achieves an FID of 3.100, significantly improving over the previous state-of-the-art score of 5.154.</li>
<li><strong>摘要：</strong>具有复杂交互的文本驱动的多人运动生成仍然是一个具有挑战性的问题。尽管在性能方面取得了进步，但现有的离线方法（使用固定数量的代理生成固定长度的动作）在处理长文本或可变文本以及变化的代理数量方面本质上受到限制。这些限制自然会鼓励自回归公式，它根据所有过去的轨迹和当前的文本指导逐步预测未来的运动。在这项工作中，我们介绍了 HINT，这是第一个用于多人运动生成的自回归框架，在扩散中采用分层交互建模。首先，HINT 利用规范化潜在空间内的解耦运动表示，将局部运动语义与人际交互解耦。这种设计有助于直接适应不同数量的人类参与者，而不需要额外的改进。其次，HINT采用滑动窗口策略来实现高效的在线生成，并聚合本地窗口内和全局跨窗口条件，以捕获过去的人类历史、人与人之间的依赖关系，并与文本指导保持一致。这种策略不仅能够在每个窗口内进行细粒度的交互建模，而且还可以在所有长序列中保持长范围的一致性。对公共基准的大量实验表明，HINT 与强大的离线模型的性能相匹配，并超越了自回归基线。值得注意的是，在 InterHuman 上，HINT 的 FID 达到了 3.100，比之前的最佳分数 5.154 有了显着提高。</li>
</ul>

<h3>Title: Quartet of Diffusions: Structure-Aware Point Cloud Generation through Part and Symmetry Guidance</h3>
<ul>
<li><strong>Authors: </strong>Chenliang Zhou, Fangcheng Zhong, Weihao Xia, Albert Miao, Canberk Baykal, Cengiz Oztireli</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20425">https://arxiv.org/abs/2601.20425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20425">https://arxiv.org/pdf/2601.20425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20425]] Quartet of Diffusions: Structure-Aware Point Cloud Generation through Part and Symmetry Guidance(https://arxiv.org/abs/2601.20425)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We introduce the Quartet of Diffusions, a structure-aware point cloud generation framework that explicitly models part composition and symmetry. Unlike prior methods that treat shape generation as a holistic process or only support part composition, our approach leverages four coordinated diffusion models to learn distributions of global shape latents, symmetries, semantic parts, and their spatial assembly. This structured pipeline ensures guaranteed symmetry, coherent part placement, and diverse, high-quality outputs. By disentangling the generative process into interpretable components, our method supports fine-grained control over shape attributes, enabling targeted manipulation of individual parts while preserving global consistency. A central global latent further reinforces structural coherence across assembled parts. Our experiments show that the Quartet achieves state-of-the-art performance. To our best knowledge, this is the first 3D point cloud generation framework that fully integrates and enforces both symmetry and part priors throughout the generative process.</li>
<li><strong>摘要：</strong>我们引入了 Quartet of Diffusions，这是一种结构感知点云生成框架，可以显式地模拟零件的组成和对称性。与先前将形状生成视为整体过程或仅支持部分组成的方法不同，我们的方法利用四个协调扩散模型来学习全局形状潜在、对称性、语义部分及其空间组装的分布。这种结构化的管道可确保对称性、连贯的零件放置以及多样化的高质量输出。通过将生成过程分解为可解释的组件，我们的方法支持对形状属性的细粒度控制，从而能够对各个部分进行有针对性的操作，同时保持全局一致性。中心全局潜伏进一步增强了组装部件之间的结构一致性。我们的实验表明，四重奏实现了最先进的性能。据我们所知，这是第一个在整个生成过程中完全集成和强化对称性和部分先验的 3D 点云生成框架。</li>
</ul>

<h3>Title: MARE: Multimodal Alignment and Reinforcement for Explainable Deepfake Detection via Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wenbo Xu, Wei Lu, Xiangyang Luo, Jiantao Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20433">https://arxiv.org/abs/2601.20433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20433">https://arxiv.org/pdf/2601.20433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20433]] MARE: Multimodal Alignment and Reinforcement for Explainable Deepfake Detection via Vision-Language Models(https://arxiv.org/abs/2601.20433)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Deepfake detection is a widely researched topic that is crucial for combating the spread of malicious content, with existing methods mainly modeling the problem as classification or spatial localization. The rapid advancements in generative models impose new demands on Deepfake detection. In this paper, we propose multimodal alignment and reinforcement for explainable Deepfake detection via vision-language models, termed MARE, which aims to enhance the accuracy and reliability of Vision-Language Models (VLMs) in Deepfake detection and reasoning. Specifically, MARE designs comprehensive reward functions, incorporating reinforcement learning from human feedback (RLHF), to incentivize the generation of text-spatially aligned reasoning content that adheres to human preferences. Besides, MARE introduces a forgery disentanglement module to capture intrinsic forgery traces from high-level facial semantics, thereby improving its authenticity detection capability. We conduct thorough evaluations on the reasoning content generated by MARE. Both quantitative and qualitative experimental results demonstrate that MARE achieves state-of-the-art performance in terms of accuracy and reliability.</li>
<li><strong>摘要：</strong>Deepfake 检测是一个广泛研究的主题，对于打击恶意内容的传播至关重要，现有方法主要将问题建模为分类或空间定位。生成模型的快速进步对 Deepfake 检测提出了新的要求。在本文中，我们提出了通过视觉语言模型进行可解释的 Deepfake 检测的多模态对齐和强化，称为 MARE，旨在提高 Deepfake 检测和推理中视觉语言模型（VLM）的准确性和可靠性。具体来说，MARE 设计了全面的奖励函数，结合了人类反馈的强化学习 (RLHF)，以激励生成符合人类偏好的文本空间对齐推理内容。此外，MARE引入了伪造解缠模块，从高级面部语义中捕获内在的伪造痕迹，从而提高其真实性检测能力。我们对 MARE 生成的推理内容进行彻底的评估。定量和定性实验结果都表明，MARE 在准确性和可靠性方面实现了最先进的性能。</li>
</ul>

<h3>Title: Fair Recourse for All: Ensuring Individual and Group Fairness in Counterfactual Explanations</h3>
<ul>
<li><strong>Authors: </strong>Fatima Ezzeddine, Obaida Ammar, Silvia Giordano, Omran Ayoub</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20449">https://arxiv.org/abs/2601.20449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20449">https://arxiv.org/pdf/2601.20449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20449]] Fair Recourse for All: Ensuring Individual and Group Fairness in Counterfactual Explanations(https://arxiv.org/abs/2601.20449)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Explainable Artificial Intelligence (XAI) is becoming increasingly essential for enhancing the transparency of machine learning (ML) models. Among the various XAI techniques, counterfactual explanations (CFs) hold a pivotal role due to their ability to illustrate how changes in input features can alter an ML model's decision, thereby offering actionable recourse to users. Ensuring that individuals with comparable attributes and those belonging to different protected groups (e.g., demographic) receive similar and actionable recourse options is essential for trustworthy and fair decision-making. In this work, we address this challenge directly by focusing on the generation of fair CFs. Specifically, we start by defining and formulating fairness at: 1) individual fairness, ensuring that similar individuals receive similar CFs, 2) group fairness, ensuring equitable CFs across different protected groups and 3) hybrid fairness, which accounts for both individual and broader group-level fairness. We formulate the problem as an optimization task and propose a novel model-agnostic, reinforcement learning based approach to generate CFs that satisfy fairness constraints at both the individual and group levels, two objectives that are usually treated as orthogonal. As fairness metrics, we extend existing metrics commonly used for auditing ML models, such as equal choice of recourse and equal effectiveness across individuals and groups. We evaluate our approach on three benchmark datasets, showing that it effectively ensures individual and group fairness while preserving the quality of the generated CFs in terms of proximity and plausibility, and quantify the cost of fairness in the different levels separately. Our work opens a broader discussion on hybrid fairness and its role and implications for XAI and beyond CFs.</li>
<li><strong>摘要：</strong>可解释的人工智能 (XAI) 对于提高机器学习 (ML) 模型的透明度变得越来越重要。在各种 XAI 技术中，反事实解释 (CF) 发挥着关键作用，因为它们能够说明输入特征的变化如何改变 ML 模型的决策，从而为用户提供可操作的资源。确保具有相似属性的个人和属于不同受保护群体（例如人口统计）的个人获得相似且可操作的追索选项对于做出值得信赖和公平的决策至关重要。在这项工作中，我们通过关注公平 CF 的生成来直接应对这一挑战。具体来说，我们首先定义和制定以下方面的公平性：1）个人公平性，确保类似的个人获得类似的 CF；2）群体公平性，确保不同受保护群体之间的公平 CF；3）混合公平，既考虑个人层面的公平性，又考虑更广泛的群体层面的公平性。我们将问题表述为优化任务，并提出了一种与模型无关、基于强化学习的新颖方法来生成满足个人和群体级别公平约束的 CF，这两个目标通常被视为正交。作为公平性指标，我们扩展了常用于审核 ML 模型的现有指标，例如个人和群体之间的平等追索选择和平等有效性。我们在三个基准数据集上评估了我们的方法，表明它有效地确保了个人和群体的公平性，同时在接近性和合理性方面保留了生成的 CF 的质量，并分别量化了不同级别的公平成本。我们的工作开启了关于混合公平及其对 XAI 和 CF 之外的作用和影响的更广泛的讨论。</li>
</ul>

<h3>Title: An explainable framework for the relationship between dementia and glucose metabolism patterns</h3>
<ul>
<li><strong>Authors: </strong>C. Vázquez-García, F. J. Martínez-Murcia, F. Segovia Román, A. Forte, J. Ramírez, I. Illán, A. Hernández-Segura, C. Jiménez-Mesa, Juan M. Górriz</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20480">https://arxiv.org/abs/2601.20480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20480">https://arxiv.org/pdf/2601.20480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20480]] An explainable framework for the relationship between dementia and glucose metabolism patterns(https://arxiv.org/abs/2601.20480)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>High-dimensional neuroimaging data presents challenges for assessing neurodegenerative diseases due to complex non-linear relationships. Variational Autoencoders (VAEs) can encode scans into lower-dimensional latent spaces capturing disease-relevant features. We propose a semi-supervised VAE framework with a flexible similarity regularization term that aligns selected latent variables with clinical or biomarker measures of dementia progression. This allows adapting the similarity metric and supervised variables to specific goals or available data. We demonstrate the approach using PET scans from the Alzheimer's Disease Neuroimaging Initiative (ADNI), guiding the first latent dimension to align with a cognitive score. Using this supervised latent variable, we generate average reconstructions across levels of cognitive impairment. Voxel-wise GLM analysis reveals reduced metabolism in key regions, mainly the hippocampus, and within major Resting State Networks, particularly the Default Mode and Central Executive Networks. The remaining latent variables encode affine transformations and intensity variations, capturing confounds such as inter-subject variability and site effects. Our framework effectively extracts disease-related patterns aligned with established Alzheimer's biomarkers, offering an interpretable and adaptable tool for studying neurodegenerative progression.</li>
<li><strong>摘要：</strong>由于复杂的非线性关系，高维神经影像数据给评估神经退行性疾病带来了挑战。变分自动编码器 (VAE) 可以将扫描编码到低维潜在空间中，捕获疾病相关特征。我们提出了一个半监督 VAE 框架，具有灵活的相似性正则化项，可将选定的潜在变量与痴呆进展的临床或生物标志物测量相结合。这允许根据特定目标或可用数据调整相似性度量和监督变量。我们使用阿尔茨海默病神经影像计划 (ADNI) 的 PET 扫描演示了该方法，指导第一个潜在维度与认知评分保持一致。使用这个受监督的潜在变量，我们生成了不同认知障碍水平的平均重建。体素 GLM 分析揭示了关键区域（主要是海马体）和主要静息状态网络（特别是默认模式和中央执行网络）内的新陈代谢降低。其余的潜在变量编码仿射变换和强度变化，捕获诸如受试者间变异性和位点效应等混杂因素。我们的框架有效地提取与已建立的阿尔茨海默氏症生物标志物一致的疾病相关模式，为研究神经退行性进展提供可解释和适应性强的工具。</li>
</ul>

<h3>Title: Efficient Autoregressive Video Diffusion with Dummy Head</h3>
<ul>
<li><strong>Authors: </strong>Hang Guo, Zhaoyang Jia, Jiahao Li, Bin Li, Yuanhao Cai, Jiangshan Wang, Yawei Li, Yan Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20499">https://arxiv.org/abs/2601.20499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20499">https://arxiv.org/pdf/2601.20499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20499]] Efficient Autoregressive Video Diffusion with Dummy Head(https://arxiv.org/abs/2601.20499)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The autoregressive video diffusion model has recently gained considerable research interest due to its causal modeling and iterative denoising. In this work, we identify that the multi-head self-attention in these models under-utilizes historical frames: approximately 25% heads attend almost exclusively to the current frame, and discarding their KV caches incurs only minor performance degradation. Building upon this, we propose Dummy Forcing, a simple yet effective method to control context accessibility across different heads. Specifically, the proposed heterogeneous memory allocation reduces head-wise context redundancy, accompanied by dynamic head programming to adaptively classify head types. Moreover, we develop a context packing technique to achieve more aggressive cache compression. Without additional training, our Dummy Forcing delivers up to 2.0x speedup over the baseline, supporting video generation at 24.3 FPS with less than 0.5% quality drop. Project page is available at this https URL.</li>
<li><strong>摘要：</strong>自回归视频扩散模型由于其因果建模和迭代去噪，最近引起了相当大的研究兴趣。在这项工作中，我们发现这些模型中的多头自注意力机制没有充分利用历史帧：大约 25% 的头几乎只关注当前帧，并且丢弃它们的 KV 缓存只会导致轻微的性能下降。在此基础上，我们提出了虚拟强制（Dummy Forcing），这是一种简单而有效的方法来控制不同头之间的上下文可访问性。具体来说，所提出的异构内存分配减少了头方面的上下文冗余，并伴随着动态头编程来自适应地对头类型进行分类。此外，我们开发了一种上下文打包技术来实现更积极的缓存压缩。无需额外训练，我们的虚拟强制即可实现比基准高出 2.0 倍的加速，支持 24.3 FPS 的视频生成，质量下降不到 0.5%。项目页面可通过此 https URL 获取。</li>
</ul>

<h3>Title: Latent Temporal Discrepancy as Motion Prior: A Loss-Weighting Strategy for Dynamic Fidelity in T2V</h3>
<ul>
<li><strong>Authors: </strong>Meiqi Wu, Bingze Song, Ruimin Lin, Chen Zhu, Xiaokun Feng, Jiahong Wu, Xiangxiang Chu, Kaiqi Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20504">https://arxiv.org/abs/2601.20504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20504">https://arxiv.org/pdf/2601.20504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20504]] Latent Temporal Discrepancy as Motion Prior: A Loss-Weighting Strategy for Dynamic Fidelity in T2V(https://arxiv.org/abs/2601.20504)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Video generation models have achieved notable progress in static scenarios, yet their performance in motion video generation remains limited, with quality degrading under drastic dynamic changes. This is due to noise disrupting temporal coherence and increasing the difficulty of learning dynamic regions. {Unfortunately, existing diffusion models rely on static loss for all scenarios, constraining their ability to capture complex dynamics.} To address this issue, we introduce Latent Temporal Discrepancy (LTD) as a motion prior to guide loss weighting. LTD measures frame-to-frame variation in the latent space, assigning larger penalties to regions with higher discrepancy while maintaining regular optimization for stable regions. This motion-aware strategy stabilizes training and enables the model to better reconstruct high-frequency dynamics. Extensive experiments on the general benchmark VBench and the motion-focused VMBench show consistent gains, with our method outperforming strong baselines by 3.31% on VBench and 3.58% on VMBench, achieving significant improvements in motion quality.</li>
<li><strong>摘要：</strong>视频生成模型在静态场景中取得了显着的进步，但其在运动视频生成中的性能仍然有限，在剧烈的动态变化下质量会下降。这是由于噪声破坏了时间相干性并增加了学习动态区域的难度。 {不幸的是，现有的扩散模型依赖于所有场景的静态损失，限制了它们捕获复杂动态的能力。}为了解决这个问题，我们引入潜在时间差异（LTD）作为引导损失加权之前的运动。 LTD 测量潜在空间中帧与帧的变化，对差异较大的区域分配更大的惩罚，同时对稳定区域保持定期优化。这种运动感知策略可以稳定训练，并使模型能够更好地重建高频动态。在通用基准 VBench 和以运动为中心的 VMBench 上进行的大量实验显示了一致的增益，我们的方法在 VBench 上比强基线高出 3.31%，在 VMBench 上比强基线高出 3.58%，在运动质量方面实现了显着改进。</li>
</ul>

<h3>Title: Say Cheese! Detail-Preserving Portrait Collection Generation via Natural Language Edits</h3>
<ul>
<li><strong>Authors: </strong>Zelong Sun, Jiahui Wu, Ying Ba, Dong Jing, Zhiwu Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20511">https://arxiv.org/abs/2601.20511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20511">https://arxiv.org/pdf/2601.20511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20511]] Say Cheese! Detail-Preserving Portrait Collection Generation via Natural Language Edits(https://arxiv.org/abs/2601.20511)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>As social media platforms proliferate, users increasingly demand intuitive ways to create diverse, high-quality portrait collections. In this work, we introduce Portrait Collection Generation (PCG), a novel task that generates coherent portrait collections by editing a reference portrait image through natural language instructions. This task poses two unique challenges to existing methods: (1) complex multi-attribute modifications such as pose, spatial layout, and camera viewpoint; and (2) high-fidelity detail preservation including identity, clothing, and accessories. To address these challenges, we propose CHEESE, the first large-scale PCG dataset containing 24K portrait collections and 573K samples with high-quality modification text annotations, constructed through an Large Vison-Language Model-based pipeline with inversion-based verification. We further propose SCheese, a framework that combines text-guided generation with hierarchical identity and detail preservation. SCheese employs adaptive feature fusion mechanism to maintain identity consistency, and ConsistencyNet to inject fine-grained features for detail consistency. Comprehensive experiments validate the effectiveness of CHEESE in advancing PCG, with SCheese achieving state-of-the-art performance.</li>
<li><strong>摘要：</strong>随着社交媒体平台的激增，用户越来越需要直观的方式来创建多样化、高质量的肖像收藏。在这项工作中，我们介绍了肖像集合生成（PCG），这是一项新颖的任务，通过自然语言指令编辑参考肖像图像来生成连贯的肖像集合。这项任务对现有方法提出了两个独特的挑战：（1）复杂的多属性修改，例如姿态、空间布局和相机视点； (2) 高保真细节保留，包括身份、服装和配饰。为了应对这些挑战，我们提出了 CHEESE，这是第一个大规模 PCG 数据集，包含 24K 肖像集合和 573K 具有高质量修改文本注释的样本，通过基于大型视觉语言模型的管道和基于反演的验证构建。我们进一步提出了 SCheese，一个将文本引导生成与分层身份和细节保存相结合的框架。 SCheese 采用自适应特征融合机制来保持身份一致性，并使用 ConsistencyNet 注入细粒度特征以实现细节一致性。综合实验验证了 CHEESE 在推进 PCG 方面的有效性，SCheese 实现了最先进的性能。</li>
</ul>

<h3>Title: Context Tokens are Anchors: Understanding the Repetition Curse in dMLLMs from an Information Flow Perspective</h3>
<ul>
<li><strong>Authors: </strong>Qiyan Zhao, Xiaofeng Zhang, Shuochen Chang, Qianyu Chen, Xiaosong Yuan, Xuhang Chen, Luoqi Liu, Jiajun Zhang, Xu-Yao Zhang, Da-Han Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20520">https://arxiv.org/abs/2601.20520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20520">https://arxiv.org/pdf/2601.20520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20520]] Context Tokens are Anchors: Understanding the Repetition Curse in dMLLMs from an Information Flow Perspective(https://arxiv.org/abs/2601.20520)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent diffusion-based Multimodal Large Language Models (dMLLMs) suffer from high inference latency and therefore rely on caching techniques to accelerate decoding. However, the application of cache mechanisms often introduces undesirable repetitive text generation, a phenomenon we term the \textbf{Repeat Curse}. To better investigate underlying mechanism behind this issue, we analyze repetition generation through the lens of information flow. Our work reveals three key findings: (1) context tokens aggregate semantic information as anchors and guide the final predictions; (2) as information propagates across layers, the entropy of context tokens converges in deeper layers, reflecting the model's growing prediction certainty; (3) Repetition is typically linked to disruptions in the information flow of context tokens and to the inability of their entropy to converge in deeper layers. Based on these insights, we present \textbf{CoTA}, a plug-and-play method for mitigating repetition. CoTA enhances the attention of context tokens to preserve intrinsic information flow patterns, while introducing a penalty term to the confidence score during decoding to avoid outputs driven by uncertain context tokens. With extensive experiments, CoTA demonstrates significant effectiveness in alleviating repetition and achieves consistent performance improvements on general tasks. Code is available at this https URL</li>
<li><strong>摘要：</strong>最近基于扩散的多模态大型语言模型 (dMLLM) 存在高推理延迟，因此依赖缓存技术来加速解码。然而，缓存机制的应用经常会引入不需要的重复文本生成，我们将这种现象称为\textbf{重复诅咒}。为了更好地研究这个问题背后的潜在机制，我们从信息流的角度分析重复的产生。我们的工作揭示了三个关键发现：（1）上下文标记聚合语义信息作为锚并指导最终预测； （2）随着信息跨层传播，上下文标记的熵在更深的层中收敛，反映了模型预测确定性不断增长； (3) 重复通常与上下文标记信息流的中断以及它们的熵无法在更深的层中收敛有关。基于这些见解，我们提出了 \textbf{CoTA}，一种用于减少重复的即插即用方法。 CoTA 增强了上下文标记的注意力，以保留内在的信息流模式，同时在解码过程中向置信度分数引入惩罚项，以避免由不确定的上下文标记驱动的输出。通过大量实验，CoTA 在减少重复方面表现出显着的有效性，并在一般任务上实现了一致的性能改进。代码可在此 https URL 获取</li>
</ul>

<h3>Title: AnomalyVFM -- Transforming Vision Foundation Models into Zero-Shot Anomaly Detectors</h3>
<ul>
<li><strong>Authors: </strong>Matic Fučka, Vitjan Zavrtanik, Danijel Skočaj</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20524">https://arxiv.org/abs/2601.20524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20524">https://arxiv.org/pdf/2601.20524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20524]] AnomalyVFM -- Transforming Vision Foundation Models into Zero-Shot Anomaly Detectors(https://arxiv.org/abs/2601.20524)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Zero-shot anomaly detection aims to detect and localise abnormal regions in the image without access to any in-domain training images. While recent approaches leverage vision-language models (VLMs), such as CLIP, to transfer high-level concept knowledge, methods based on purely vision foundation models (VFMs), like DINOv2, have lagged behind in performance. We argue that this gap stems from two practical issues: (i) limited diversity in existing auxiliary anomaly detection datasets and (ii) overly shallow VFM adaptation strategies. To address both challenges, we propose AnomalyVFM, a general and effective framework that turns any pretrained VFM into a strong zero-shot anomaly detector. Our approach combines a robust three-stage synthetic dataset generation scheme with a parameter-efficient adaptation mechanism, utilising low-rank feature adapters and a confidence-weighted pixel loss. Together, these components enable modern VFMs to substantially outperform current state-of-the-art methods. More specifically, with RADIO as a backbone, AnomalyVFM achieves an average image-level AUROC of 94.1% across 9 diverse datasets, surpassing previous methods by significant 3.3 percentage points. Project Page: this https URL</li>
<li><strong>摘要：</strong>零样本异常检测旨在检测和定位图像中的异常区域，而无需访问任何域内训练图像。虽然最近的方法利用视觉语言模型 (VLM)（例如 CLIP）来传输高级概念知识，但基于纯视觉基础模型 (VFM)（例如 DINOv2）的方法在性能上已经落后。我们认为，这种差距源于两个实际问题：(i) 现有辅助异常检测数据集的多样性有限；(ii) VFM 适应策略过于浅薄。为了解决这两个挑战，我们提出了 AnomalyVFM，这是一个通用且有效的框架，可将任何预训练的 VFM 转变为强大的零样本异常检测器。我们的方法将强大的三阶段合成数据集生成方案与参数高效的适应机制结合起来，利用低秩特征适配器和置信加权像素损失。这些组件共同使现代 VFM 的性能远远优于当前最先进的方法。更具体地说，以 RADIO 作为骨干，AnomalyVFM 在 9 个不同的数据集上实现了 94.1% 的平均图像级 AUROC，显着超过了之前的方法 3.3 个百分点。项目页面：此 https URL</li>
</ul>

<h3>Title: Advancing Open-source World Models</h3>
<ul>
<li><strong>Authors: </strong>Robbyant Team: Zelin Gao, Qiuyu Wang, Yanhong Zeng, Jiapeng Zhu, Ka Leong Cheng, Yixuan Li, Hanlin Wang, Yinghao Xu, Shuailei Ma, Yihang Chen, Jie Liu, Yansong Cheng, Yao Yao, Jiayi Zhu, Yihao Meng, Kecheng Zheng, Qingyan Bai, Jingye Chen, Zehong Shen, Yue Yu, Xing Zhu, Yujun Shen, Hao Ouyang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20540">https://arxiv.org/abs/2601.20540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20540">https://arxiv.org/pdf/2601.20540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20540]] Advancing Open-source World Models(https://arxiv.org/abs/2601.20540)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as "long-term memory". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.</li>
<li><strong>摘要：</strong>我们推出 LingBot-World，一个源于视频生成的开源世界模拟器。 LingBot-World定位于顶级世界模型，具有以下特点。 (1) 它在广泛的环境中保持高保真度和强大的动态，包括现实主义、科学背景、卡通风格等。 （2）它能够实现分钟级的视野，同时保持一段时间内的上下文一致性，这也称为“长期记忆”。 (3) 支持实时交互，每秒生成16帧时延迟低于1秒。我们提供对代码和模型的公共访问，以努力缩小开源和闭源技术之间的鸿沟。我们相信我们的发布将为社区提供内容创建、游戏和机器人学习等领域的实际应用。</li>
</ul>

<h3>Title: DiffVC-RT: Towards Practical Real-Time Diffusion-based Perceptual Neural Video Compression</h3>
<ul>
<li><strong>Authors: </strong>Wenzhuo Ma, Zhenzhong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20564">https://arxiv.org/abs/2601.20564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20564">https://arxiv.org/pdf/2601.20564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20564]] DiffVC-RT: Towards Practical Real-Time Diffusion-based Perceptual Neural Video Compression(https://arxiv.org/abs/2601.20564)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The practical deployment of diffusion-based Neural Video Compression (NVC) faces critical challenges, including severe information loss, prohibitive inference latency, and poor temporal consistency. To bridge this gap, we propose DiffVC-RT, the first framework designed to achieve real-time diffusion-based perceptual NVC. First, we introduce an Efficient and Informative Model Architecture. Through strategic module replacements and pruning, this architecture significantly reduces computational complexity while mitigating structural information loss. Second, to address generative flickering artifacts, we propose Explicit and Implicit Consistency Modeling. We enhance temporal consistency by explicitly incorporating a zero-cost Online Temporal Shift Module within the U-Net, complemented by hybrid implicit consistency constraints. Finally, we present an Asynchronous and Parallel Decoding Pipeline incorporating Mixed Half Precision, which enables asynchronous latent decoding and parallel frame reconstruction via a Batch-dimension Temporal Shift design. Experiments show that DiffVC-RT achieves 80.1% bitrate savings in terms of LPIPS over VTM-17.0 on HEVC dataset with real-time encoding and decoding speeds of 206 / 30 fps for 720p videos on an NVIDIA H800 GPU, marking a significant milestone in diffusion-based video compression.</li>
<li><strong>摘要：</strong>基于扩散的神经视频压缩 (NVC) 的实际部署面临着严峻的挑战，包括严重的信息丢失、令人望而却步的推理延迟和较差的时间一致性。为了弥补这一差距，我们提出了 DiffVC-RT，这是第一个旨在实现基于扩散的实时感知 NVC 的框架。首先，我们介绍一个高效且信息丰富的模型架构。通过战略性模块替换和修剪，该架构显着降低了计算复杂性，同时减少了结构信息丢失。其次，为了解决生成闪烁伪影，我们提出显式和隐式一致性建模。我们通过在 U-Net 中显式合并零成本在线时间移位模块来增强时间一致性，并辅以混合隐式一致性约束。最后，我们提出了一种结合混合半精度的异步并行解码管道，它通过批量维度时间移位设计实现异步潜在解码和并行帧重建。实验表明，DiffVC-RT 在 HEVC 数据集上的 LPIPS 方面比 VTM-17.0 节省了 80.1% 的比特率，在 NVIDIA H800 GPU 上对 720p 视频的实时编码和解码速度为 206 / 30 fps，这标志着基于扩散的视频压缩的一个重要里程碑。</li>
</ul>

<h3>Title: CoBA: Integrated Deep Learning Model for Reliable Low-Altitude UAV Classification in mmWave Radio Networks</h3>
<ul>
<li><strong>Authors: </strong>Junaid Sajid, Ivo Müürsepp, Luca Reggiani, Davide Scazzoli, Federico Francesco Luigi Mariani, Maurizio Magarini, Rizwan Ahmad, Muhammad Mahtab Alam</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20605">https://arxiv.org/abs/2601.20605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20605">https://arxiv.org/pdf/2601.20605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20605]] CoBA: Integrated Deep Learning Model for Reliable Low-Altitude UAV Classification in mmWave Radio Networks(https://arxiv.org/abs/2601.20605)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Uncrewed Aerial Vehicles (UAVs) are increasingly used in civilian and industrial applications, making secure low-altitude operations crucial. In dense mmWave environments, accurately classifying low-altitude UAVs as either inside authorized or restricted airspaces remains challenging, requiring models that handle complex propagation and signal variability. This paper proposes a deep learning model, referred to as CoBA, which stands for integrated Convolutional Neural Network (CNN), Bidirectional Long Short-Term Memory (BiLSTM), and Attention which leverages Fifth Generation (5G) millimeter-wave (mmWave) radio measurements to classify UAV operations in authorized and restricted airspaces at low altitude. The proposed CoBA model integrates convolutional, bidirectional recurrent, and attention layers to capture both spatial and temporal patterns in UAV radio measurements. To validate the model, a dedicated dataset is collected using the 5G mmWave network at TalTech, with controlled low altitude UAV flights in authorized and restricted scenarios. The model is evaluated against conventional ML models and a fingerprinting-based benchmark. Experimental results show that CoBA achieves superior accuracy, significantly outperforming all baseline models and demonstrating its potential for reliable and regulated UAV airspace monitoring.</li>
<li><strong>摘要：</strong>无人驾驶飞行器 (UAV) 越来越多地用于民用和工业应用，这使得安全的低空作业变得至关重要。在密集的毫米波环境中，将低空无人机准确分类为授权空域或受限空域仍然具有挑战性，需要能够处理复杂传播和信号变化的模型。本文提出了一种深度学习模型，简称 CoBA，代表集成卷积神经网络 (CNN)、双向长短期记忆 (BiLSTM) 和注意力机制，利用第五代 (5G) 毫米波 (mmWave) 无线电测量对低空授权空域和受限空域中的无人机操作进行分类。所提出的 CoBA 模型集成了卷积层、双向循环层和注意力层，以捕获无人机无线电测量中的空间和时间模式。为了验证模型，使用 TalTech 的 5G 毫米波网络收集专用数据集，并在授权和限制场景中进行受控低空无人机飞行。该模型根据传统的机器学习模型和基于指纹的基准进行评估。实验结果表明，CoBA 实现了卓越的准确性，显着优于所有基线模型，并展示了其可靠且受监管的无人机空域监测的潜力。</li>
</ul>

<h3>Title: WFR-MFM: One-Step Inference for Dynamic Unbalanced Optimal Transport</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Wang, Ruoyu Wang, Qiangwei Peng, Peijie Zhou, Tiejun Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20606">https://arxiv.org/abs/2601.20606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20606">https://arxiv.org/pdf/2601.20606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20606]] WFR-MFM: One-Step Inference for Dynamic Unbalanced Optimal Transport(https://arxiv.org/abs/2601.20606)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reconstructing dynamical evolution from limited observations is a fundamental challenge in single-cell biology, where dynamic unbalanced optimal transport provides a principled framework for modeling coupled transport and mass variation. However, existing approaches rely on trajectory simulation at inference time, making inference a key bottleneck for scalable applications. In this work, we propose a mean-flow framework for unbalanced flow matching that summarizes both transport and mass-growth dynamics over arbitrary time intervals using mean velocity and mass-growth fields, enabling fast one-step generation without trajectory simulation. To solve dynamic unbalanced optimal transport under the Wasserstein-Fisher-Rao geometry, we further build on this framework to develop Wasserstein-Fisher-Rao Mean Flow Matching (WFR-MFM). Across synthetic and real single-cell RNA sequencing datasets, WFR-MFM achieves orders-of-magnitude faster inference than a range of existing baselines while maintaining high predictive accuracy, and enables efficient perturbation response prediction on large synthetic datasets with thousands of conditions.</li>
<li><strong>摘要：</strong>从有限的观察中重建动态进化是单细胞生物学的一个基本挑战，其中动态不平衡的最佳运输为耦合运输和质量变化的建模提供了原则框架。然而，现有方法依赖于推理时的轨迹模拟，这使得推理成为可扩展应用程序的关键瓶颈。在这项工作中，我们提出了一种用于不平衡流匹配的平均流框架，该框架使用平均速度和质量增长场总结了任意时间间隔内的传输和质量增长动态，从而无需轨迹模拟即可快速一步生成。为了解决 Wasserstein-Fisher-Rao 几何结构下的动态不平衡最优传输，我们进一步在此框架的基础上开发了 Wasserstein-Fisher-Rao 平均流匹配（WFR-MFM）。在合成和真实的单细胞 RNA 测序数据集中，WFR-MFM 的推理速度比一系列现有基线快几个数量级，同时保持高预测精度，并能够对具有数千种条件的大型合成数据集进行有效的扰动响应预测。</li>
</ul>

<h3>Title: GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection</h3>
<ul>
<li><strong>Authors: </strong>Shuguang Zhang, Junhong Lian, Guoxin Yu, Baoxun Xu, Xiang Ao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20618">https://arxiv.org/abs/2601.20618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20618">https://arxiv.org/pdf/2601.20618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20618]] GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection(https://arxiv.org/abs/2601.20618)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Multimodal sarcasm detection (MSD) aims to identify sarcasm within image-text pairs by modeling semantic incongruities across modalities. Existing methods often exploit cross-modal embedding misalignment to detect inconsistency but struggle when visual and textual content are loosely related or semantically indirect. While recent approaches leverage large language models (LLMs) to generate sarcastic cues, the inherent diversity and subjectivity of these generations often introduce noise. To address these limitations, we propose the Generative Discrepancy Comparison Network (GDCNet). This framework captures cross-modal conflicts by utilizing descriptive, factually grounded image captions generated by Multimodal LLMs (MLLMs) as stable semantic anchors. Specifically, GDCNet computes semantic and sentiment discrepancies between the generated objective description and the original text, alongside measuring visual-textual fidelity. These discrepancy features are then fused with visual and textual representations via a gated module to adaptively balance modality contributions. Extensive experiments on MSD benchmarks demonstrate GDCNet's superior accuracy and robustness, establishing a new state-of-the-art on the MMSD2.0 benchmark.</li>
<li><strong>摘要：</strong>多模态讽刺检测 (MSD) 旨在通过对跨模​​态的语义不一致进行建模来识别图像文本对中的讽刺。现有的方法通常利用跨模式嵌入错位来检测不一致，但当视觉和文本内容松散相关或语义间接时，就会遇到困难。虽然最近的方法利用大型语言模型（LLM）来生成讽刺线索，但这些世代固有的多样性和主观性经常会引入噪音。为了解决这些限制，我们提出了生成差异比较网络（GDCNet）。该框架利用多模态 LLM (MLLM) 生成的描述性、基于事实的图像说明作为稳定的语义锚来捕获跨模态冲突。具体来说，GDCNet 计算生成的目标描述与原始文本之间的语义和情感差异，同时测量视觉文本保真度。然后，通过门控模块将这些差异特征与视觉和文本表示融合，以自适应地平衡模态贡献。 MSD 基准上的大量实验证明了 GDCNet 卓越的准确性和鲁棒性，在 MMSD2.0 基准上建立了新的最先进水平。</li>
</ul>

<h3>Title: Detecting and Mitigating Memorization in Diffusion Models through Anisotropy of the Log-Probability</h3>
<ul>
<li><strong>Authors: </strong>Rohan Asthana, Vasileios Belagiannis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20642">https://arxiv.org/abs/2601.20642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20642">https://arxiv.org/pdf/2601.20642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20642]] Detecting and Mitigating Memorization in Diffusion Models through Anisotropy of the Log-Probability(https://arxiv.org/abs/2601.20642)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion-based image generative models produce high-fidelity images through iterative denoising but remain vulnerable to memorization, where they unintentionally reproduce exact copies or parts of training images. Recent memorization detection methods are primarily based on the norm of score difference as indicators of memorization. We prove that such norm-based metrics are mainly effective under the assumption of isotropic log-probability distributions, which generally holds at high or medium noise levels. In contrast, analyzing the anisotropic regime reveals that memorized samples exhibit strong angular alignment between the guidance vector and unconditional scores in the low-noise setting. Through these insights, we develop a memorization detection metric by integrating isotropic norm and anisotropic alignment. Our detection metric can be computed directly on pure noise inputs via two conditional and unconditional forward passes, eliminating the need for costly denoising steps. Detection experiments on Stable Diffusion v1.4 and v2 show that our metric outperforms existing denoising-free detection methods while being at least approximately 5x faster than the previous best approach. Finally, we demonstrate the effectiveness of our approach by utilizing a mitigation strategy that adapts memorized prompts based on our developed metric.</li>
<li><strong>摘要：</strong>基于扩散的图像生成模型通过迭代去噪生成高保真图像，但仍然容易被记忆，它们会无意中复制训练图像的精确副本或部分。最近的记忆检测方法主要基于分数差的范数作为记忆的指标。我们证明，这种基于范数的指标主要在各向同性对数概率分布的假设下有效，该分布通常适用于高或中等噪声水平。相反，分析各向异性状态表明，记忆的样本在低噪声设置下在引导向量和无条件分数之间表现出强烈的角度对齐。通过这些见解，我们通过整合各向同性范数和各向异性对齐来开发记忆检测指标。我们的检测指标可以通过两个条件和无条件前向传递直接在纯噪声输入上计算，从而无需昂贵的去噪步骤。 Stable Diffusion v1.4 和 v2 上的检测实验表明，我们的指标优于现有的无噪检测方法，同时比之前的最佳方法至少快大约 5 倍。最后，我们通过利用缓解策略来证明我们方法的有效性，该策略根据我们开发的指标调整记忆的提示。</li>
</ul>

<h3>Title: Positive-Unlabeled Reinforcement Learning Distillation for On-Premise Small Models</h3>
<ul>
<li><strong>Authors: </strong>Zhiqiang Kou, Junyang Chen, Xin-Qiang Cai, Xiaobo Xia, Ming-Kun Xie, Dong-Dong Wu, Biao Liu, Yuheng Jia, Xin Geng, Masashi Sugiyama, Tat-Seng Chua</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20687">https://arxiv.org/abs/2601.20687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20687">https://arxiv.org/pdf/2601.20687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20687]] Positive-Unlabeled Reinforcement Learning Distillation for On-Premise Small Models(https://arxiv.org/abs/2601.20687)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Due to constraints on privacy, cost, and latency, on-premise deployment of small models is increasingly common. However, most practical pipelines stop at supervised fine-tuning (SFT) and fail to reach the reinforcement learning (RL) alignment stage. The main reason is that RL alignment typically requires either expensive human preference annotation or heavy reliance on high-quality reward models with large-scale API usage and ongoing engineering maintenance, both of which are ill-suited to on-premise settings. To bridge this gap, we propose a positive-unlabeled (PU) RL distillation method for on-premise small-model deployment. Without human-labeled preferences or a reward model, our method distills the teacher's preference-optimization capability from black-box generations into a locally trainable student. For each prompt, we query the teacher once to obtain an anchor response, locally sample multiple student candidates, and perform anchor-conditioned self-ranking to induce pairwise or listwise preferences, enabling a fully local training loop via direct preference optimization or group relative policy optimization. Theoretical analysis justifies that the induced preference signal by our method is order-consistent and concentrates on near-optimal candidates, supporting its stability for preference optimization. Experiments demonstrate that our method achieves consistently strong performance under a low-cost setting.</li>
<li><strong>摘要：</strong>由于隐私、成本和延迟的限制，小型模型的本地部署越来越普遍。然而，大多数实际的流程都停留在监督微调（SFT）阶段，而未能达到强化学习（RL）对齐阶段。主要原因是 RL 对齐通常需要昂贵的人类偏好注释或严重依赖具有大规模 API 使用和持续工程维护的高质量奖励模型，这两者都不适合本地设置。为了弥补这一差距，我们提出了一种用于本地小模型部署的正向无标签 (PU) RL 蒸馏方法。在没有人类标记的偏好或奖励模型的情况下，我们的方法将教师的偏好优化能力从黑盒生成中提炼成本地可训练的学生。对于每个提示，我们查询老师一次以获得锚响应，对多个学生候选人进行本地采样，并执行锚条件自排序以诱导成对或列表偏好，从而通过直接偏好优化或组相对策略优化实现完全本地训练循环。理论分析证明，我们的方法诱导的偏好信号是顺序一致的，并且集中于接近最优的候选者，支持其偏好优化的稳定性。实验表明，我们的方法在低成本环境下实现了始终如一的强大性能。</li>
</ul>

<h3>Title: Decoupling Perception and Calibration: Label-Efficient Image Quality Assessment Framework</h3>
<ul>
<li><strong>Authors: </strong>Xinyue Li, Zhichao Zhang, Zhiming Xu, Shubo Xu, Xiongkuo Min, Yitong Chen, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20689">https://arxiv.org/abs/2601.20689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20689">https://arxiv.org/pdf/2601.20689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20689]] Decoupling Perception and Calibration: Label-Efficient Image Quality Assessment Framework(https://arxiv.org/abs/2601.20689)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Recent multimodal large language models (MLLMs) have demonstrated strong capabilities in image quality assessment (IQA) tasks. However, adapting such large-scale models is computationally expensive and still relies on substantial Mean Opinion Score (MOS) annotations. We argue that for MLLM-based IQA, the core bottleneck lies not in the quality perception capacity of MLLMs, but in MOS scale calibration. Therefore, we propose LEAF, a Label-Efficient Image Quality Assessment Framework that distills perceptual quality priors from an MLLM teacher into a lightweight student regressor, enabling MOS calibration with minimal human supervision. Specifically, the teacher conducts dense supervision through point-wise judgments and pair-wise preferences, with an estimate of decision reliability. Guided by these signals, the student learns the teacher's quality perception patterns through joint distillation and is calibrated on a small MOS subset to align with human annotations. Experiments on both user-generated and AI-generated IQA benchmarks demonstrate that our method significantly reduces the need for human annotations while maintaining strong MOS-aligned correlations, making lightweight IQA practical under limited annotation budgets.</li>
<li><strong>摘要：</strong>最近的多模态大语言模型（MLLM）在图像质量评估（IQA）任务中表现出了强大的能力。然而，适应如此大规模的模型在计算上是昂贵的，并且仍然依赖于大量的平均意见得分（MOS）注释。我们认为，对于基于 MLLM 的 IQA，核心瓶颈不在于 MLLM 的质量感知能力，而在于 MOS 尺度校准。因此，我们提出了 LEAF，一种标签高效的图像质量评估框架，它将 MLLM 教师的感知质量先验提炼为轻量级的学生回归器，从而在最少的人工监督下实现 MOS 校准。具体来说，教师通过逐点判断和成对偏好进行密集监督，并对决策可靠性进行估计。在这些信号的指导下，学生通过联合蒸馏学习教师的质量感知模式，并在一个小的 MOS 子集上进行校准，以与人类注释保持一致。对用户生成和 AI 生成的 IQA 基准的实验表明，我们的方法显着减少了对人工注释的需求，同时保持强大的 MOS 对齐相关性，使得轻量级 IQA 在有限的注释预算下变得实用。</li>
</ul>

<h3>Title: LEMON: How Well Do MLLMs Perform Temporal Multimodal Understanding on Instructional Videos?</h3>
<ul>
<li><strong>Authors: </strong>Zhuang Yu, Lei Shen, Jing Zhao, Shiliang Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20705">https://arxiv.org/abs/2601.20705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20705">https://arxiv.org/pdf/2601.20705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20705]] LEMON: How Well Do MLLMs Perform Temporal Multimodal Understanding on Instructional Videos?(https://arxiv.org/abs/2601.20705)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent multimodal large language models (MLLMs) have shown remarkable progress across vision, audio, and language tasks, yet their performance on long-form, knowledge-intensive, and temporally structured educational content remains largely unexplored. To bridge this gap, we introduce LEMON, a Lecture-based Evaluation benchmark for MultimOdal uNderstanding, focusing on STEM lecture videos that require long-horizon reasoning and cross-modal integration. LEMON comprises 2,277 video segments spanning 5 disciplines and 29 courses, with an average duration of 196.1 seconds, yielding 4,181 high-quality QA pairs, including 3,413 multiple-choice and 768 open-ended questions. Distinct from existing video benchmarks, LEMON features: (1) semantic richness and disciplinary density, (2) tightly coupled video-audio-text modalities, (3) explicit temporal and pedagogical structure, and (4) contextually linked multi-turn questioning. It further encompasses six major tasks and twelve subtasks, covering the full cognitive spectrum from perception to reasoning and then to generation. Comprehensive experiments reveal substantial performance gaps across tasks, highlighting that even state-of-the-art MLLMs like GPT-4o struggle with temporal reasoning and instructional prediction. We expect LEMON to serve as an extensible and challenging benchmark for advancing multimodal perception, reasoning, and generation in long-form instructional contents.</li>
<li><strong>摘要：</strong>最近的多模态大语言模型（MLLM）在视觉、音频和语言任务方面取得了显着进展，但它们在长篇、知识密集型和时间结构的教育内容上的表现在很大程度上仍未得到探索。为了弥补这一差距，我们引入了 LEMON，这是一种基于讲座的多模态理解评估基准，重点关注需要长视野推理和跨模态集成的 STEM 讲座视频。 LEMON 包含 2,277 个视频片段，涵盖 5 个学科和 29 个课程，平均时长为 196.1 秒，产生 4,181 个高质量问答对，其中包括 3,413 个多项选择题和 768 个开放式问题。与现有的视频基准不同，LEMON 具有以下特点：(1) 语义丰富性和学科密度，(2) 紧密耦合的视频-音频-文本模式，(3) 明确的时间和教学结构，以及 (4) 上下文相关的多轮提问。它还包含六项主要任务和十二项子任务，涵盖从感知到推理再到生成的完整认知范围。综合实验揭示了不同任务之间巨大的性能差距，这凸显出即使是像 GPT-4o 这样最先进的 MLLM 也难以应对时间推理和教学预测。我们期望 LEMON 能够成为一个可扩展且具有挑战性的基准，以推进长篇教学内容的多模式感知、推理和生成。</li>
</ul>

<h3>Title: Compression Tells Intelligence: Visual Coding, Visual Token Technology, and the Unification</h3>
<ul>
<li><strong>Authors: </strong>Xin Jin, Jinming Liu, Yuntao Wei, Junyan Lin, Zhicheng Wang, Jianguo Huang, Xudong Yang, Yanxiao Liu, Wenjun Zeng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20742">https://arxiv.org/abs/2601.20742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20742">https://arxiv.org/pdf/2601.20742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20742]] Compression Tells Intelligence: Visual Coding, Visual Token Technology, and the Unification(https://arxiv.org/abs/2601.20742)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>"Compression Tells Intelligence", is supported by research in artificial intelligence, particularly concerning (multimodal) large language models (LLMs/MLLMs), where compression efficiency often correlates with improved model performance and capabilities. For compression, classical visual coding based on traditional information theory has developed over decades, achieving great success with numerous international industrial standards widely applied in multimedia (e.g., image/video) systems. Except that, the recent emergingvisual token technology of generative multi-modal large models also shares a similar fundamental objective like visual coding: maximizing semantic information fidelity during the representation learning while minimizing computational cost. Therefore, this paper provides a comprehensive overview of two dominant technique families first -- Visual Coding and Vision Token Technology -- then we further unify them from the aspect of optimization, discussing the essence of compression efficiency and model performance trade-off behind. Next, based on the proposed unified formulation bridging visual coding andvisual token technology, we synthesize bidirectional insights of themselves and forecast the next-gen visual codec and token techniques. Last but not least, we experimentally show a large potential of the task-oriented token developments in the more practical tasks like multimodal LLMs (MLLMs), AI-generated content (AIGC), and embodied AI, as well as shedding light on the future possibility of standardizing a general token technology like the traditional codecs (e.g., H.264/265) with high efficiency for a wide range of intelligent tasks in a unified and effective manner.</li>
<li><strong>摘要：</strong>“压缩告诉智能”得到了人工智能研究的支持，特别是有关（多模式）大型语言模型（LLM/MLLM）的研究，其中压缩效率通常与改进的模型性能和功能相关。在压缩方面，基于传统信息论的经典视觉编码已经发展了数十年，取得了巨大成功，众多国际工业标准广泛应用于多媒体（例如图像/视频）系统。除此之外，最近新兴的生成多模态大模型的视觉标记技术也与视觉编码具有相似的基本目标：在表示学习过程中最大化语义信息保真度，同时最小化计算成本。因此，本文首先对视觉编码和视觉令牌技术这两个主导技术家族进行了全面的概述，然后从优化的角度进一步统一它们，讨论背后压缩效率和模型性能权衡的本质。接下来，基于所提出的桥接视觉编码和视觉令牌技术的统一公式，我们综合了它们之间的双向见解并预测了下一代视觉编解码器和令牌技术。最后但并非最不重要的一点是，我们通过实验展示了面向任务的代币开发在多模态 LLM（MLLM）、人工智能生成内容（AIGC）和体现人工智能等更实际的任务中的巨大潜力，并揭示了标准化通用代币技术（如传统编解码器（例如 H.264/265））的未来可能性，以统一和有效的方式高效地执行各种智能任务。</li>
</ul>

<h3>Title: FAIRT2V: Training-Free Debiasing for Text-to-Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Haonan Zhong, Wei Song, Tingxu Han, Maurice Pagnucco, Jingling Xue, Yang Song</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20791">https://arxiv.org/abs/2601.20791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20791">https://arxiv.org/pdf/2601.20791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20791]] FAIRT2V: Training-Free Debiasing for Text-to-Video Diffusion Models(https://arxiv.org/abs/2601.20791)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-video (T2V) diffusion models have achieved rapid progress, yet their demographic biases, particularly gender bias, remain largely unexplored. We present FairT2V, a training-free debiasing framework for text-to-video generation that mitigates encoder-induced bias without finetuning. We first analyze demographic bias in T2V models and show that it primarily originates from pretrained text encoders, which encode implicit gender associations even for neutral prompts. We quantify this effect with a gender-leaning score that correlates with bias in generated videos. Based on this insight, FairT2V mitigates demographic bias by neutralizing prompt embeddings via anchor-based spherical geodesic transformations while preserving semantics. To maintain temporal coherence, we apply debiasing only during early identity-forming steps through a dynamic denoising schedule. We further propose a video-level fairness evaluation protocol combining VideoLLM-based reasoning with human verification. Experiments on the modern T2V model Open-Sora show that FairT2V substantially reduces demographic bias across occupations with minimal impact on video quality.</li>
<li><strong>摘要：</strong>文本到视频（T2V）传播模型取得了快速进展，但其人口统计学偏见，特别是性别偏见，在很大程度上仍未得到探索。我们提出了 FairT2V，一种用于文本到视频生成的免训练去偏差框架，无需微调即可减轻编码器引起的偏差。我们首先分析了 T2V 模型中的人口统计偏差，并表明它主要源自预训练的文本编码器，即使对于中性提示，它也会编码隐含的性别关联。我们通过与生成视频中的偏见相关的性别倾向分数来量化这种影响。基于这一见解，FairT2V 通过基于锚点的球形测地线变换来中和提示嵌入，同时保留语义，从而减轻了人口统计偏差。为了保持时间一致性，我们仅在早期身份形成步骤中通过动态去噪计划应用去偏。我们进一步提出了一种将基于 VideoLLM 的推理与人工验证相结合的视频级公平性评估协议。现代 T2V 模型 Open-Sora 的实验表明，FairT2V 大大减少了不同职业的人口统计偏差，同时对视频质量的影响最小。</li>
</ul>

<h3>Title: Open-Vocabulary Functional 3D Human-Scene Interaction Generation</h3>
<ul>
<li><strong>Authors: </strong>Jie Liu, Yu Sun, Alpar Cseke, Yao Feng, Nicolas Heron, Michael J. Black, Yan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20835">https://arxiv.org/abs/2601.20835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20835">https://arxiv.org/pdf/2601.20835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20835]] Open-Vocabulary Functional 3D Human-Scene Interaction Generation(https://arxiv.org/abs/2601.20835)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. In this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts. Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness. In contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as "sitting on a sofa'', while supporting fine-grained functional human-scene interactions, e.g., "increasing the room temperature''. Extensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes.</li>
<li><strong>摘要：</strong>生成与 3D 场景进行功能交互的 3D 人类仍然是实体 AI、机器人和交互式内容创建应用程序中的一个悬而未决的问题。关键挑战涉及推理 3D 场景中功能元素的语义以及实现功能感知交互所需的 3D 人体姿势。不幸的是，现有方法通常缺乏对对象功能和相应的人类场景接触的明确推理，导致令人难以置信或功能上不正确的交互。在这项工作中，我们提出了 FunHSI，这是一种免训练、功能驱动的框架，可以根据开放词汇任务提示实现功能上正确的人类场景交互。给定任务提示，FunHSI 执行功能感知的接触推理，以识别功能场景元素、重建其 3D 几何形状，并通过接触图对高级交互进行建模。然后，我们利用视觉语言模型来合成执行图像中任务的人类，并估计建议的 3D 身体和手部姿势。最后，通过分阶段优化对所提出的 3D 身体配置进行细化，以确保物理合理性和功能正确性。与现有方法相比，FunHSI 不仅合成了更合理的一般 3D 交互，例如“坐在沙发上”，同时支持细粒度的功能性人景交互，例如“升高室温”。大量实验表明，FunHSI 在不同的室内和室外场景中始终能够生成功能正确且物理上合理的人类场景交互。</li>
</ul>

<h3>Title: Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Aníbal Silva, Moisés Santos, André Restivo, Carlos Soares</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20854">https://arxiv.org/abs/2601.20854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20854">https://arxiv.org/pdf/2601.20854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20854]] Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation(https://arxiv.org/abs/2601.20854)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Tabular data remains a challenging domain for generative models. In particular, the standard Variational Autoencoder (VAE) architecture, typically composed of multilayer perceptrons, struggles to model relationships between features, especially when handling mixed data types. In contrast, Transformers, through their attention mechanism, are better suited for capturing complex feature interactions. In this paper, we empirically investigate the impact of integrating Transformers into different components of a VAE. We conduct experiments on 57 datasets from the OpenML CC18 suite and draw two main conclusions. First, results indicate that positioning Transformers to leverage latent and decoder representations leads to a trade-off between fidelity and diversity. Second, we observe a high similarity between consecutive blocks of a Transformer in all components. In particular, in the decoder, the relationship between the input and output of a Transformer is approximately linear.</li>
<li><strong>摘要：</strong>对于生成模型来说，表格数据仍然是一个具有挑战性的领域。特别是，通常由多层感知器组成的标准变分自动编码器 (VAE) 架构很难对特征之间的关系进行建模，尤其是在处理混合数据类型时。相比之下，Transformers 通过其注意力机制，更适合捕获复杂的特征交互。在本文中，我们实证研究了将 Transformer 集成到 VAE 不同组件中的影响。我们对 OpenML CC18 套件中的 57 个数据集进行了实验，得出两个主要结论。首先，结果表明，将 Transformer 定位为利用潜在表示和解码器表示会导致保真度和多样性之间的权衡。其次，我们观察到 Transformer 的所有组件中的连续块之间具有高度相似性。特别地，在解码器中，Transformer的输入和输出之间的关系近似线性。</li>
</ul>

<h3>Title: FreeFix: Boosting 3D Gaussian Splatting via Fine-Tuning-Free Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Hongyu Zhou, Zisen Shao, Sheng Miao, Pan Wang, Dongfeng Bai, Bingbing Liu, Yiyi Liao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20857">https://arxiv.org/abs/2601.20857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20857">https://arxiv.org/pdf/2601.20857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20857]] FreeFix: Boosting 3D Gaussian Splatting via Fine-Tuning-Free Diffusion Models(https://arxiv.org/abs/2601.20857)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Neural Radiance Fields and 3D Gaussian Splatting have advanced novel view synthesis, yet still rely on dense inputs and often degrade at extrapolated views. Recent approaches leverage generative models, such as diffusion models, to provide additional supervision, but face a trade-off between generalization and fidelity: fine-tuning diffusion models for artifact removal improves fidelity but risks overfitting, while fine-tuning-free methods preserve generalization but often yield lower fidelity. We introduce FreeFix, a fine-tuning-free approach that pushes the boundary of this trade-off by enhancing extrapolated rendering with pretrained image diffusion models. We present an interleaved 2D-3D refinement strategy, showing that image diffusion models can be leveraged for consistent refinement without relying on costly video diffusion models. Furthermore, we take a closer look at the guidance signal for 2D refinement and propose a per-pixel confidence mask to identify uncertain regions for targeted improvement. Experiments across multiple datasets show that FreeFix improves multi-frame consistency and achieves performance comparable to or surpassing fine-tuning-based methods, while retaining strong generalization ability.</li>
<li><strong>摘要：</strong>神经辐射场和 3D 高斯分布具有先进的新颖视图合成，但仍然依赖于密集输入，并且通常在外推视图中性能下降。最近的方法利用生成模型（例如扩散模型）来提供额外的监督，但面临泛化和保真度之间的权衡：微调扩散模型以去除伪影可以提高保真度，但存在过度拟合的风险，而免微调方法可以保留泛化性，但通常会产生较低的保真度。我们引入了 FreeFix，这是一种无需微调的方法，它通过使用预训练的图像扩散模型增强外推渲染来突破这种权衡的界限。我们提出了一种交错的 2D-3D 细化策略，表明可以利用图像扩散模型进行一致的细化，而无需依赖昂贵的视频扩散模型。此外，我们仔细研究了二维细化的引导信号，并提出了每像素置信掩模来识别不确定区域以进行有针对性的改进。跨多个数据集的实验表明，FreeFix 提高了多帧一致性，实现了与基于微调的方法相当或超越的性能，同时保留了强大的泛化能力。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
