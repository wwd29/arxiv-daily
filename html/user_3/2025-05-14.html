<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-05-14</h1>
<h3>Title: Latent Behavior Diffusion for Sequential Reaction Generation in Dyadic Setting</h3>
<ul>
<li><strong>Authors: </strong>Minh-Duc Nguyen, Hyung-Jeong Yang, Soo-Hyung Kim, Ji-Eun Shin, Seung-Won Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.07901">https://arxiv.org/abs/2505.07901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.07901">https://arxiv.org/pdf/2505.07901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.07901]] Latent Behavior Diffusion for Sequential Reaction Generation in Dyadic Setting(https://arxiv.org/abs/2505.07901)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The dyadic reaction generation task involves synthesizing responsive facial reactions that align closely with the behaviors of a conversational partner, enhancing the naturalness and effectiveness of human-like interaction simulations. This paper introduces a novel approach, the Latent Behavior Diffusion Model, comprising a context-aware autoencoder and a diffusion-based conditional generator that addresses the challenge of generating diverse and contextually relevant facial reactions from input speaker behaviors. The autoencoder compresses high-dimensional input features, capturing dynamic patterns in listener reactions while condensing complex input data into a concise latent representation, facilitating more expressive and contextually appropriate reaction synthesis. The diffusion-based conditional generator operates on the latent space generated by the autoencoder to predict realistic facial reactions in a non-autoregressive manner. This approach allows for generating diverse facial reactions that reflect subtle variations in conversational cues and emotional states. Experimental results demonstrate the effectiveness of our approach in achieving superior performance in dyadic reaction synthesis tasks compared to existing methods.</li>
<li><strong>摘要：</strong>二元反应产生的任务涉及综合反应性面部反应，这些反应与对话伴侣的行为紧密相符，从而增强了类似人类相互作用模拟的自然性和有效性。本文介绍了一种新颖的方法，即潜在行为扩散模型，其中包括一种上下文感知的自动编码器和基于扩散的条件发生器，该生成器解决了从输入扬声器行为产生多样化和上下文相关的面部反应的挑战。自动编码器会压缩高维输入特征，在侦听器反应中捕获动态模式，同时将复杂的输入数据凝结成简洁的潜在表示，从而促进表达和上下文适当的反应综合。基于扩散的条件发电机在自动编码器生成的潜在空间上运行，以以非自动性方式预测现实的面部反应。这种方法允许产生各种面部反应，反映了会话线索和情绪状态的细微变化。实验结果表明，与现有方法相比，我们方法在二元反应综合任务中实现卓越性能的有效性。</li>
</ul>

<h3>Title: Combining Bayesian Inference and Reinforcement Learning for Agent Decision Making: A Review</h3>
<ul>
<li><strong>Authors: </strong>Chengmin Zhou, Ville Kyrki, Pasi Fränti, Laura Ruotsalainen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.07911">https://arxiv.org/abs/2505.07911</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.07911">https://arxiv.org/pdf/2505.07911</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.07911]] Combining Bayesian Inference and Reinforcement Learning for Agent Decision Making: A Review(https://arxiv.org/abs/2505.07911)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Bayesian inference has many advantages in decision making of agents (e.g. robotics/simulative agent) over a regular data-driven black-box neural network: Data-efficiency, generalization, interpretability, and safety where these advantages benefit directly/indirectly from the uncertainty quantification of Bayesian inference. However, there are few comprehensive reviews to summarize the progress of Bayesian inference on reinforcement learning (RL) for decision making to give researchers a systematic understanding. This paper focuses on combining Bayesian inference with RL that nowadays is an important approach in agent decision making. To be exact, this paper discusses the following five topics: 1) Bayesian methods that have potential for agent decision making. First basic Bayesian methods and models (Bayesian rule, Bayesian learning, and Bayesian conjugate models) are discussed followed by variational inference, Bayesian optimization, Bayesian deep learning, Bayesian active learning, Bayesian generative models, Bayesian meta-learning, and lifelong Bayesian learning. 2) Classical combinations of Bayesian methods with model-based RL (with approximation methods), model-free RL, and inverse RL. 3) Latest combinations of potential Bayesian methods with RL. 4) Analytical comparisons of methods that combine Bayesian methods with RL with respect to data-efficiency, generalization, interpretability, and safety. 5) In-depth discussions in six complex problem variants of RL, including unknown reward, partial-observability, multi-agent, multi-task, non-linear non-Gaussian, and hierarchical RL problems and the summary of how Bayesian methods work in the data collection, data processing and policy learning stages of RL to pave the way for better agent decision-making strategies.</li>
<li><strong>摘要：</strong>贝叶斯推论在代理的决策（例如机器人/模拟剂）上具有许多优势，而不是数据驱动的黑盒神经网络：数据效率，概括性，解释性和安全性，这些优势直接/间接受益于贝叶斯的不确定性量化的不确定性。但是，很少有全面的评论总结了贝叶斯对强化学习的推断（RL）的进步，以使研究人员有系统的理解。本文着重于将贝叶斯推断与当今代理决策中重要的方法相结合。准确地说，本文讨论了以下五个主题：1）具有代理决策潜力的贝叶斯方法。讨论了第一个基本的贝叶斯方法和模型（贝叶斯规则，贝叶斯学习和贝叶斯结合模型），然后讨论各种推断，贝叶斯优化，贝叶斯深度学习，贝叶斯深度学习，贝叶斯活跃学习，贝叶斯生成模型，贝叶斯元模型，贝叶斯元学习和Lifelong贝叶斯学习。 2）贝叶斯方法与基于模型的RL（具有近似方法），无模型RL和逆RL的经典组合。 3）潜在的贝叶斯方法与RL的最新组合。 4）在数据效率，概括，可解释性和安全性方面，将贝叶斯方法与RL相结合的方法的分析比较。 5）在RL的六个复杂问题变体中进行了深入讨论，包括未知的奖励，部分观察性，多任务，多任务，多任务，非线性非高斯和分层RL问题以及贝叶斯方法如何在数据收集，数据处理，数据处理，数据处理，数据处理，策略学习阶段来构成改善策略策略的策略中的贝叶斯方法的摘要。</li>
</ul>

<h3>Title: Symbolic Regression with Multimodal Large Language Models and Kolmogorov Arnold Networks</h3>
<ul>
<li><strong>Authors: </strong>Thomas R. Harvey, Fabian Ruehle, Cristofero S. Fraser-Taliente, James Halverson</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE, cs.SC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.07956">https://arxiv.org/abs/2505.07956</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.07956">https://arxiv.org/pdf/2505.07956</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.07956]] Symbolic Regression with Multimodal Large Language Models and Kolmogorov Arnold Networks(https://arxiv.org/abs/2505.07956)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present a novel approach to symbolic regression using vision-capable large language models (LLMs) and the ideas behind Google DeepMind's Funsearch. The LLM is given a plot of a univariate function and tasked with proposing an ansatz for that function. The free parameters of the ansatz are fitted using standard numerical optimisers, and a collection of such ansätze make up the population of a genetic algorithm. Unlike other symbolic regression techniques, our method does not require the specification of a set of functions to be used in regression, but with appropriate prompt engineering, we can arbitrarily condition the generative step. By using Kolmogorov Arnold Networks (KANs), we demonstrate that ``univariate is all you need'' for symbolic regression, and extend this method to multivariate functions by learning the univariate function on each edge of a trained KAN. The combined expression is then simplified by further processing with a language model.</li>
<li><strong>摘要：</strong>我们提出了一种新颖的方法，使用具有视觉能力的大语言模型（LLM）以及Google DeepMind的FunSearch背后的想法。为LLM提供了单变量函数的图，并任务是为该功能提出ANSATZ。 ANSATZ的自由参数使用标准数值优化器拟合，并且此类Ansätze的集合构成了遗传算法的种群。与其他符号回归技术不同，我们的方法不需要在回归中使用一组功能的规范，但是使用适当的及时工程，我们可以任意调节生成步骤。通过使用Kolmogorov Arnold Networks（KANS），我们证明了``Univariate is hirvariate is hiles is hiles''用于符号回归，并通过学习训练有素的KAN每个边缘上的单变量功能，将此方法扩展到多变量函数。然后，通过使用语言模型进行进一步处理来简化组合表达式。</li>
</ul>

<h3>Title: Fréchet Power-Scenario Distance: A Metric for Evaluating Generative AI Models across Multiple Time-Scales in Smart Grids</h3>
<ul>
<li><strong>Authors: </strong>Yuting Cai, Shaohuai Liu, Chao Tian, Le Xie</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.08082">https://arxiv.org/abs/2505.08082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.08082">https://arxiv.org/pdf/2505.08082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.08082]] Fréchet Power-Scenario Distance: A Metric for Evaluating Generative AI Models across Multiple Time-Scales in Smart Grids(https://arxiv.org/abs/2505.08082)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative artificial intelligence (AI) models in smart grids have advanced significantly in recent years due to their ability to generate large amounts of synthetic data, which would otherwise be difficult to obtain in the real world due to confidentiality constraints. A key challenge in utilizing such synthetic data is how to assess the data quality produced from such generative models. Traditional Euclidean distance-based metrics only reflect pair-wise relations between two individual samples, and could fail in evaluating quality differences between groups of synthetic datasets. In this work, we propose a novel metric based on the Fréchet Distance (FD) estimated between two datasets in a learned feature space. The proposed method evaluates the quality of generation from a distributional perspective. Empirical results demonstrate the superiority of the proposed metric across timescales and models, enhancing the reliability of data-driven decision-making in smart grid operations.</li>
<li><strong>摘要：</strong>智能电网中的生成人工智能（AI）模型在近年来由于能够生成大量合成数据的能力而显着提高，否则由于机密性约束，否则在现实世界中很难获得。利用这种合成数据的关键挑战是如何评估此类生成模型产生的数据质量。传统的欧几里得基于距离的指标仅反映了两个单个样本之间的成对关系，并且可能无法评估合成数据集组之间的质量差异。在这项工作中，我们提出了一个基于在学习特征空间中两个数据集之间估计的Fréchet距离（FD）的新型度量标准。提出的方法从分布的角度评估了发电质量。经验结果证明了跨时间尺度和模型所提出的度量的优势，从而提高了智能电网操作中数据驱动决策的可靠性。</li>
</ul>

<h3>Title: Unsupervised Raindrop Removal from a Single Image using Conditional Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Lhuqita Fazry, Valentino Vito</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.08190">https://arxiv.org/abs/2505.08190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.08190">https://arxiv.org/pdf/2505.08190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.08190]] Unsupervised Raindrop Removal from a Single Image using Conditional Diffusion Models(https://arxiv.org/abs/2505.08190)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generative</a></li>
<li><strong>Abstract: </strong>Raindrop removal is a challenging task in image processing. Removing raindrops while relying solely on a single image further increases the difficulty of the task. Common approaches include the detection of raindrop regions in the image, followed by performing a background restoration process conditioned on those regions. While various methods can be applied for the detection step, the most common architecture used for background restoration is the Generative Adversarial Network (GAN). Recent advances in the use of diffusion models have led to state-of-the-art image inpainting techniques. In this paper, we introduce a novel technique for raindrop removal from a single image using diffusion-based image inpainting.</li>
<li><strong>摘要：</strong>去除雨滴是图像处理中的一项具有挑战性的任务。仅依靠单个图像的同时去除雨滴进一步增加了任务的难度。常见方法包括图像中雨滴区域的检测，然后执行以这些区域为条件的背景修复过程。虽然可以将各种方法应用于检测步骤，但用于背景修复的最常见架构是生成对抗网络（GAN）。扩散模型的使用最新进展导致了最新的图像介绍技术。在本文中，我们使用基于扩散的图像插入的图像介绍了一种从单个图像中删除雨滴的新技术。</li>
</ul>

<h3>Title: Visual Watermarking in the Era of Diffusion Models: Advances and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Junxian Duan, Jiyang Guang, Wenkui Yang, Ran He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.08197">https://arxiv.org/abs/2505.08197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.08197">https://arxiv.org/pdf/2505.08197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.08197]] Visual Watermarking in the Era of Diffusion Models: Advances and Challenges(https://arxiv.org/abs/2505.08197)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>As generative artificial intelligence technologies like Stable Diffusion advance, visual content becomes more vulnerable to misuse, raising concerns about copyright infringement. Visual watermarks serve as effective protection mechanisms, asserting ownership and deterring unauthorized use. Traditional deepfake detection methods often rely on passive techniques that struggle with sophisticated manipulations. In contrast, diffusion models enhance detection accuracy by allowing for the effective learning of features, enabling the embedding of imperceptible and robust watermarks. We analyze the strengths and challenges of watermark techniques related to diffusion models, focusing on their robustness and application in watermark generation. By exploring the integration of advanced diffusion models and watermarking security, we aim to advance the discourse on preserving watermark robustness against evolving forgery threats. It emphasizes the critical importance of developing innovative solutions to protect digital content and ensure the preservation of ownership rights in the era of generative AI.</li>
<li><strong>摘要：</strong>随着生成人工智能技术等稳定扩散的进步，视觉内容变得越来越容易滥用，引起了对侵犯版权的关注。视觉水印是有效的保护机制，主张所有权并阻止未经授权的使用。传统的深层检测方法通常依赖于与复杂的操纵斗争的被动技术。相比之下，扩散模型通过允许有效学习特征，从而提高检测精度，从而嵌入不可察觉和可靠的水印。我们分析了与扩散模型相关的水印技术的优势和挑战，重点是它们在水印产生中的稳健性和应用。通过探索先进的扩散模型和水印安全的整合，我们旨在推进有关保留水印鲁棒性，防止不断发展的伪造威胁的论述。它强调了开发创新解决方案以保护数字内容并确保在生成AI时代保存所有权的关键重要性。</li>
</ul>

<h3>Title: G-MSGINet: A Grouped Multi-Scale Graph-Involution Network for Contactless Fingerprint Recognition</h3>
<ul>
<li><strong>Authors: </strong>Santhoshkumar Peddi, Soham Bandyopadhyay, Debasis Samanta</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.08233">https://arxiv.org/abs/2505.08233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.08233">https://arxiv.org/pdf/2505.08233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.08233]] G-MSGINet: A Grouped Multi-Scale Graph-Involution Network for Contactless Fingerprint Recognition(https://arxiv.org/abs/2505.08233)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This paper presents G-MSGINet, a unified and efficient framework for robust contactless fingerprint recognition that jointly performs minutiae localization and identity embedding directly from raw input images. Existing approaches rely on multi-branch architectures, orientation labels, or complex preprocessing steps, which limit scalability and generalization across real-world acquisition scenarios. In contrast, the proposed architecture introduces the GMSGI layer, a novel computational module that integrates grouped pixel-level involution, dynamic multi-scale kernel generation, and graph-based relational modelling into a single processing unit. Stacked GMSGI layers progressively refine both local minutiae-sensitive features and global topological representations through end-to-end optimization. The architecture eliminates explicit orientation supervision and adapts graph connectivity directly from learned kernel descriptors, thereby capturing meaningful structural relationships among fingerprint regions without fixed heuristics. Extensive experiments on three benchmark datasets, namely PolyU, CFPose, and Benchmark 2D/3D, demonstrate that G-MSGINet consistently achieves minutiae F1-scores in the range of $0.83\pm0.02$ and Rank-1 identification accuracies between 97.0% and 99.1%, while maintaining an Equal Error Rate (EER) as low as 0.5%. These results correspond to improvements of up to 4.8% in F1-score and 1.4% in Rank-1 accuracy when compared to prior methods, using only 0.38 million parameters and 6.63 giga floating-point operations, which represents up to ten times fewer parameters than competitive baselines. This highlights the scalability and effectiveness of G-MSGINet in real-world contactless biometric recognition scenarios.</li>
<li><strong>摘要：</strong>本文介绍了G-Msginet，这是一个统一，有效的框架，用于鲁棒的非接触式指纹识别，共同执行细节定位和直接从原始输入图像中嵌入的身份。现有的方法依赖于多分支体系结构，定向标签或复杂的预处理步骤，这些步骤限制了现实世界中采集方案的可扩展性和概括性。相比之下，提出的体系结构引入了GMSGI层，GMSGI层是一种新型的计算模块，该模块集成了分组的像素级对不起，动态多尺度内核生成以及基于图形的关系建模为单个处理单元。通过端到端优化，堆叠的GMSGI层逐渐完善了局部细节敏感的特征和全球拓扑表示。该体系结构消除了直接从学习的内核描述符中的明确取向监督和调整图形连接，从而捕获了没有固定启发式方法的指纹区域之间有意义的结构关系。在三个基准数据集上进行的广泛实验，即polyu，cfpose和Benchmark 2d/3d，表明G-MSGINET始终达到$ 0.83 \ pm0.02 $ and calt-1识别准确度的范围为$ 0.83 \ pm0.02 $且在97.0％和99.1％之间的差异，同时又有0.5％的级别（0.5％），同时达到0.5％（erer）。与先前方法相比，这些结果对应于F1得分的提高高达4.8％，排名1.4％，使用38万参数和6.63 GIGA浮点操作，这比竞争激烈的基础底线少了十倍。这突出了G-Msginet在现实世界非接触式生物识别识别方案中的可伸缩性和有效性。</li>
</ul>

<h3>Title: Removing Watermarks with Partial Regeneration using Semantic Information</h3>
<ul>
<li><strong>Authors: </strong>Krti Tallam, John Kevin Cava, Caleb Geniesse, N. Benjamin Erichson, Michael W. Mahoney</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.08234">https://arxiv.org/abs/2505.08234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.08234">https://arxiv.org/pdf/2505.08234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.08234]] Removing Watermarks with Partial Regeneration using Semantic Information(https://arxiv.org/abs/2505.08234)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>As AI-generated imagery becomes ubiquitous, invisible watermarks have emerged as a primary line of defense for copyright and provenance. The newest watermarking schemes embed semantic signals - content-aware patterns that are designed to survive common image manipulations - yet their true robustness against adaptive adversaries remains under-explored. We expose a previously unreported vulnerability and introduce SemanticRegen, a three-stage, label-free attack that erases state-of-the-art semantic and invisible watermarks while leaving an image's apparent meaning intact. Our pipeline (i) uses a vision-language model to obtain fine-grained captions, (ii) extracts foreground masks with zero-shot segmentation, and (iii) inpaints only the background via an LLM-guided diffusion model, thereby preserving salient objects and style cues. Evaluated on 1,000 prompts across four watermarking systems - TreeRing, StegaStamp, StableSig, and DWT/DCT - SemanticRegen is the only method to defeat the semantic TreeRing watermark (p = 0.10 > 0.05) and reduces bit-accuracy below 0.75 for the remaining schemes, all while maintaining high perceptual quality (masked SSIM = 0.94 +/- 0.01). We further introduce masked SSIM (mSSIM) to quantify fidelity within foreground regions, showing that our attack achieves up to 12 percent higher mSSIM than prior diffusion-based attackers. These results highlight an urgent gap between current watermark defenses and the capabilities of adaptive, semantics-aware adversaries, underscoring the need for watermarking algorithms that are resilient to content-preserving regenerative attacks.</li>
<li><strong>摘要：</strong>随着AI生成的图像变得无处不在，无形的水印已成为版权和出处的主要防御线。最新的水印方案嵌入了语义信号 - 旨在生存常见图像操作的内容感知的模式 - 但它们对适应性对手的真正鲁棒性仍然不足。我们暴露了一个以前未报告的脆弱性，并引入了Semanticregen，这是一种三阶段的无标签攻击，它消除了最新的语义和无形的水印，同时使图像的明显含义完好无损。我们的管道（i）使用视觉模型来获得细粒字幕，（ii）提取带有零拍摄的前景口罩，而（iii）仅通过LLM引导的扩散模型进行背景，从而保留了明显的对象和样式线索。在四个水标系统的1,000个提示中进行评估 -  treing，Stegastamp，Stablesig和DWT/DCT -Senanticregen -Semanticregen是唯一击败语义Treering Watermark（P = 0.10> 0.05）的唯一方法，并将位于0.75的位 - 剩余方案低于0.75，同时维持高质量（均可保持高质量）（毫无意义）（0.94）。我们进一步介绍了蒙面的SSIM（MSSIM）来量化前景区域内的保真度，这表明我们的攻击比以前的基于扩散的攻击者高达12％。这些结果表明，当前水印的防御能力与适应性语义感知的对手的能力之间存在紧迫的差距，强调了对具有对内容具有内容的再生攻击有弹性的水印算法的需求。</li>
</ul>

<h3>Title: Identifying Memorization of Diffusion Models through p-Laplace Analysis</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Brokman, Amit Giloni, Omer Hofman, Roman Vainshtein, Hisashi Kojima, Guy Gilboa</a></li>
<li><strong>Subjects: </strong>cs.CV, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.08246">https://arxiv.org/abs/2505.08246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.08246">https://arxiv.org/pdf/2505.08246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.08246]] Identifying Memorization of Diffusion Models through p-Laplace Analysis(https://arxiv.org/abs/2505.08246)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion models, today's leading image generative models, estimate the score function, i.e. the gradient of the log probability of (perturbed) data samples, without direct access to the underlying probability distribution. This work investigates whether the estimated score function can be leveraged to compute higher-order differentials, namely p-Laplace operators. We show here these operators can be employed to identify memorized training data. We propose a numerical p-Laplace approximation based on the learned score functions, showing its effectiveness in identifying key features of the probability landscape. We analyze the structured case of Gaussian mixture models, and demonstrate the results carry-over to image generative models, where memorization identification based on the p-Laplace operator is performed for the first time.</li>
<li><strong>摘要：</strong>扩散模型是当今领先的图像生成模型，估算了分数函数，即（扰动）数据样本的对数概率的梯度，而无需直接访问潜在的概率分布。这项工作研究了是否可以利用估计的得分函数来计算高阶差异，即p-Laplace运算符。我们在这里显示这些操作员可以使用来识别记忆的培训数据。我们提出了基于学习分数函数的数值p-laplace近似值，显示了其在识别概率格局的关键特征方面的有效性。我们分析了高斯混合模型的结构化情况，并演示了结果为图像生成模型的结果，其中首次基于P-Laplace操作员的记忆识别。</li>
</ul>

<h3>Title: IrrMap: A Large-Scale Comprehensive Dataset for Irrigation Method Mapping</h3>
<ul>
<li><strong>Authors: </strong>Nibir Chandra Mandal, Oishee Bintey Hoque, Abhijin Adiga, Samarth Swarup, Mandy Wilson, Lu Feng, Yangfeng Ji, Miaomiao Zhang, Geoffrey Fox, Madhav Marathe</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.08273">https://arxiv.org/abs/2505.08273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.08273">https://arxiv.org/pdf/2505.08273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.08273]] IrrMap: A Large-Scale Comprehensive Dataset for Irrigation Method Mapping(https://arxiv.org/abs/2505.08273)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce IrrMap, the first large-scale dataset (1.1 million patches) for irrigation method mapping across regions. IrrMap consists of multi-resolution satellite imagery from LandSat and Sentinel, along with key auxiliary data such as crop type, land use, and vegetation indices. The dataset spans 1,687,899 farms and 14,117,330 acres across multiple western U.S. states from 2013 to 2023, providing a rich and diverse foundation for irrigation analysis and ensuring geospatial alignment and quality control. The dataset is ML-ready, with standardized 224x224 GeoTIFF patches, the multiple input modalities, carefully chosen train-test-split data, and accompanying dataloaders for seamless deep learning model training andbenchmarking in irrigation mapping. The dataset is also accompanied by a complete pipeline for dataset generation, enabling researchers to extend IrrMap to new regions for irrigation data collection or adapt it with minimal effort for other similar applications in agricultural and geospatial analysis. We also analyze the irrigation method distribution across crop groups, spatial irrigation patterns (using Shannon diversity indices), and irrigated area variations for both LandSat and Sentinel, providing insights into regional and resolution-based differences. To promote further exploration, we openly release IrrMap, along with the derived datasets, benchmark models, and pipeline code, through a GitHub repository: this https URL and Data repository: this https URL, providing comprehensive documentation and implementation details.</li>
<li><strong>摘要：</strong>我们介绍了IRRMAP，这是第一个大型数据集（110万个补丁），用于跨区域的灌溉方法映射。 IRRMAP由Landsat和Sentinel的多分辨率卫星图像以及关键的辅助数据（例如作物类型，土地使用和植被指数）组成。该数据集跨越了2013年至2023年美国多个西部各州的1,687,899个农场和14,117,330英亩的土地，为灌溉分析和确保地理空间一致性和质量控制提供了丰富而多样的基础。该数据集已准备使用ML，具有标准化的224x224 GEOTIFF补丁，多个输入模式，精心选择的火车测试数据数据，以及随附的数据层加载器，用于无缝的深度学习模型培训并在灌溉映射中进行基础测试。该数据集还伴随着数据集生成的完整管道，使研究人员能够将IRRMAP扩展到新区域进行灌溉数据收集，或者以最小的努力对农业和地理空间分析中的其他类似应用程序进行调整。我们还分析了跨作物群体的灌溉方法分布，空间灌溉模式（使用香农多样性指数）以及兰萨特和哨兵的灌溉面积变化，从而提供了对基于区域和解决方案的差异的见解。为了促进进一步的探索，我们通过GITHUB存储库公开发布IRRMAP，以及派生的数据集，基准模型和管道代码：此HTTPS URL和数据存储库：此HTTPS URL，提供全面的文档和实施详细信息。</li>
</ul>

<h3>Title: Ultra Lowrate Image Compression with Semantic Residual Coding and Compression-aware Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Anle Ke, Xu Zhang, Tong Chen, Ming Lu, Chao Zhou, Jiawen Gu, Zhan Ma</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.08281">https://arxiv.org/abs/2505.08281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.08281">https://arxiv.org/pdf/2505.08281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.08281]] Ultra Lowrate Image Compression with Semantic Residual Coding and Compression-aware Diffusion(https://arxiv.org/abs/2505.08281)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Existing multimodal large model-based image compression frameworks often rely on a fragmented integration of semantic retrieval, latent compression, and generative models, resulting in suboptimal performance in both reconstruction fidelity and coding efficiency. To address these challenges, we propose a residual-guided ultra lowrate image compression named ResULIC, which incorporates residual signals into both semantic retrieval and the diffusion-based generation process. Specifically, we introduce Semantic Residual Coding (SRC) to capture the semantic disparity between the original image and its compressed latent representation. A perceptual fidelity optimizer is further applied for superior reconstruction quality. Additionally, we present the Compression-aware Diffusion Model (CDM), which establishes an optimal alignment between bitrates and diffusion time steps, improving compression-reconstruction synergy. Extensive experiments demonstrate the effectiveness of ResULIC, achieving superior objective and subjective performance compared to state-of-the-art diffusion-based methods with - 80.7%, -66.3% BD-rate saving in terms of LPIPS and FID. Project page is available at https: //njuvision.this http URL.</li>
<li><strong>摘要：</strong>现有的多模式基于大型模型的图像压缩框架通常依赖于语义检索，潜在压缩和生成模型的零散集成，从而在重建保真度和编码效率中均可降低性能。为了应对这些挑战，我们提出了一个名为Resulic的残留引导的超低率图像压缩，该图像resulic将残留信号纳入语义检索和基于扩散的生成过程中。具体而言，我们引入语义残差编码（SRC），以捕获原始图像与其压缩潜在表示之间的语义差异。进一步的保真度优化器将进一步应用于出色的重建质量。此外，我们提出了压缩感知扩散模型（CDM），该模型建立了比特率和扩散时间步骤之间的最佳比对，从而改善了压缩重建协同作用。广泛的实验表明，与最新的基于扩散的方法相比，悬浮物的有效性，实现了卓越的客观和主观性能，即-80.7％，在LPIP和FID方面节省了-66.3％BD率。项目页面可在https：//njuvision.this http url上找到。</li>
</ul>

<h3>Title: FauForensics: Boosting Audio-Visual Deepfake Detection with Facial Action Units</h3>
<ul>
<li><strong>Authors: </strong>Jian Wang, Baoyuan Wu, Li Liu, Qingshan Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.08294">https://arxiv.org/abs/2505.08294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.08294">https://arxiv.org/pdf/2505.08294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.08294]] FauForensics: Boosting Audio-Visual Deepfake Detection with Facial Action Units(https://arxiv.org/abs/2505.08294)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid evolution of generative AI has increased the threat of realistic audio-visual deepfakes, demanding robust detection methods. Existing solutions primarily address unimodal (audio or visual) forgeries but struggle with multimodal manipulations due to inadequate handling of heterogeneous modality features and poor generalization across datasets. To this end, we propose a novel framework called FauForensics by introducing biologically invariant facial action units (FAUs), which is a quantitative descriptor of facial muscle activity linked to emotion physiology. It serves as forgery-resistant representations that reduce domain dependency while capturing subtle dynamics often disrupted in synthetic content. Besides, instead of comparing entire video clips as in prior works, our method computes fine-grained frame-wise audiovisual similarities via a dedicated fusion module augmented with learnable cross-modal queries. It dynamically aligns temporal-spatial lip-audio relationships while mitigating multi-modal feature heterogeneity issues. Experiments on FakeAVCeleb and LAV-DF show state-of-the-art (SOTA) performance and superior cross-dataset generalizability with up to an average of 4.83\% than existing methods.</li>
<li><strong>摘要：</strong>生成AI的快速发展增加了现实的视听深击的威胁，要求使用可靠的检测方法。现有的解决方案主要解决单峰（音频或视觉）伪造，但由于处理异质模态特征不足而与多模式操纵斗争，并且在数据集中的概括不良。为此，我们通过引入生物学上不变的面部动作单元（FAU）提出了一个名为Fauforensics的新颖框架，该单元是与情绪生理有关的面部肌肉活动的定量描述。它充当伪造的表示，可以减少域的依赖性，同时捕获经常在合成含量中破坏的微妙动态。此外，我们的方法没有像先前的工作那样比较整个视频剪辑，而是通过使用可学习的跨模式查询的专用Fusion模块来计算细粒度的视听相似性。它可以动态地对齐时间空间唇aaudio关系，同时减轻多模式特征异质性问题。 Fakeavceleb和Lav-DF的实验显示出最先进的（SOTA）性能和优越的跨数据集概括性，其平均水平高达4.83 \％，比现有方法。</li>
</ul>

<h3>Title: STORYANCHORS: Generating Consistent Multi-Scene Story Frames for Long-Form Narratives</h3>
<ul>
<li><strong>Authors: </strong>Bo Wang, Haoyang Huang, Zhiyin Lu, Fengyuan Liu, Guoqing Ma, Jianlong Yuan, Yuan Zhang, Nan Duan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.08350">https://arxiv.org/abs/2505.08350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.08350">https://arxiv.org/pdf/2505.08350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.08350]] STORYANCHORS: Generating Consistent Multi-Scene Story Frames for Long-Form Narratives(https://arxiv.org/abs/2505.08350)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This paper introduces StoryAnchors, a unified framework for generating high-quality, multi-scene story frames with strong temporal consistency. The framework employs a bidirectional story generator that integrates both past and future contexts to ensure temporal consistency, character continuity, and smooth scene transitions throughout the narrative. Specific conditions are introduced to distinguish story frame generation from standard video synthesis, facilitating greater scene diversity and enhancing narrative richness. To further improve generation quality, StoryAnchors integrates Multi-Event Story Frame Labeling and Progressive Story Frame Training, enabling the model to capture both overarching narrative flow and event-level dynamics. This approach supports the creation of editable and expandable story frames, allowing for manual modifications and the generation of longer, more complex sequences. Extensive experiments show that StoryAnchors outperforms existing open-source models in key areas such as consistency, narrative coherence, and scene diversity. Its performance in narrative consistency and story richness is also on par with GPT-4o. Ultimately, StoryAnchors pushes the boundaries of story-driven frame generation, offering a scalable, flexible, and highly editable foundation for future research.</li>
<li><strong>摘要：</strong>本文介绍了Storyanceans，这是一个统一的框架，用于产生具有强大时间一致性的高质量的多场景故事框架。该框架采用了双向故事生成器，该生成器整合了过去和将来的上下文，以确保整个叙述中的时间一致性，角色连续性和平稳的场景过渡。引入特定条件是为了区分故事框架的生成与标准视频综合，促进了更大的场景多样性和增强叙事丰富性。为了进一步提高发电质量，故事通讯员整合了多事件的故事框架标签和渐进式的故事框架培训，从而使模型能够捕获总体叙事流程和事件级的动态。这种方法支持创建可编辑且可扩展的故事框架，从而可以进行手动修改以及更长，更复杂的序列的产生。广泛的实验表明，故事公寓的表现优于一致性，叙事连贯性和场景多样性等关键领域的现有开源模型。它在叙事一致性和故事丰富性方面的表现也与GPT-4O相当。归根结底，故事公寓推动了故事驱动的框架生成的界限，为未来的研究提供了可扩展，灵活且高度可编辑的基础。</li>
</ul>

<h3>Title: DHECA-SuperGaze: Dual Head-Eye Cross-Attention and Super-Resolution for Unconstrained Gaze Estimation</h3>
<ul>
<li><strong>Authors: </strong>Franko Šikić, Donik Vršnak, Sven Lončarić</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.08426">https://arxiv.org/abs/2505.08426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.08426">https://arxiv.org/pdf/2505.08426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.08426]] DHECA-SuperGaze: Dual Head-Eye Cross-Attention and Super-Resolution for Unconstrained Gaze Estimation(https://arxiv.org/abs/2505.08426)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Unconstrained gaze estimation is the process of determining where a subject is directing their visual attention in uncontrolled environments. Gaze estimation systems are important for a myriad of tasks such as driver distraction monitoring, exam proctoring, accessibility features in modern software, etc. However, these systems face challenges in real-world scenarios, partially due to the low resolution of in-the-wild images and partially due to insufficient modeling of head-eye interactions in current state-of-the-art (SOTA) methods. This paper introduces DHECA-SuperGaze, a deep learning-based method that advances gaze prediction through super-resolution (SR) and a dual head-eye cross-attention (DHECA) module. Our dual-branch convolutional backbone processes eye and multiscale SR head images, while the proposed DHECA module enables bidirectional feature refinement between the extracted visual features through cross-attention mechanisms. Furthermore, we identified critical annotation errors in one of the most diverse and widely used gaze estimation datasets, Gaze360, and rectified the mislabeled data. Performance evaluation on Gaze360 and GFIE datasets demonstrates superior within-dataset performance of the proposed method, reducing angular error (AE) by 0.48° (Gaze360) and 2.95° (GFIE) in static configurations, and 0.59° (Gaze360) and 3.00° (GFIE) in temporal settings compared to prior SOTA methods. Cross-dataset testing shows improvements in AE of more than 1.53° (Gaze360) and 3.99° (GFIE) in both static and temporal settings, validating the robust generalization properties of our approach.</li>
<li><strong>摘要：</strong>无约束的目光估计是确定受试者在不受控制的环境中指导其视觉关注的过程。凝视估计系统对于诸如驾驶员分心监测，考试，现代软件中的可访问性等多项任务非常重要。但是，这些系统在现实世界中的情况下面临挑战，部分原因是野外图像的分辨率较低，部分原因是由于目前的状态互动不足，在当前的状态互动中不足。本文介绍了DHECA-Supergaze，这是一种基于深度学习的方法，该方法通过超分辨率（SR）和双头眼交叉意见（DHECA）模块进行凝视预测。我们的双支流卷积骨架骨架处理眼睛和多尺度SR头部图像，而所提出的DHECA模块可以通过交叉注意机制在提取的视觉特征之间进行双向特征。此外，我们确定了最多样化，最广泛使用的凝视估计数据集之一的关键注释误差，即Gaze360，并纠正了错误标记的数据。 GAZE360和GFIE数据集的性能评估表明，提出方法的数据表性能在静态配置中降低了0.48°（GAZE360）和2.95°（GFIE），并在静态配置中降低了角度误差（AE），并在0.59°（gaze360）和3.00°（GFIE）中进行了比较。交叉数据表测试显示，在静态和时间设置中，AE的AE超过1.53°（Gaze360）和3.99°（GFIE）的改进，从而验证了我们方法的鲁棒概括属性。</li>
</ul>

<h3>Title: Visual Image Reconstruction from Brain Activity via Latent Representation</h3>
<ul>
<li><strong>Authors: </strong>Yukiyasu Kamitani, Misato Tanaka, Ken Shirakawa</a></li>
<li><strong>Subjects: </strong>cs.CV, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.08429">https://arxiv.org/abs/2505.08429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.08429">https://arxiv.org/pdf/2505.08429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.08429]] Visual Image Reconstruction from Brain Activity via Latent Representation(https://arxiv.org/abs/2505.08429)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Visual image reconstruction, the decoding of perceptual content from brain activity into images, has advanced significantly with the integration of deep neural networks (DNNs) and generative models. This review traces the field's evolution from early classification approaches to sophisticated reconstructions that capture detailed, subjective visual experiences, emphasizing the roles of hierarchical latent representations, compositional strategies, and modular architectures. Despite notable progress, challenges remain, such as achieving true zero-shot generalization for unseen images and accurately modeling the complex, subjective aspects of perception. We discuss the need for diverse datasets, refined evaluation metrics aligned with human perceptual judgments, and compositional representations that strengthen model robustness and generalizability. Ethical issues, including privacy, consent, and potential misuse, are underscored as critical considerations for responsible development. Visual image reconstruction offers promising insights into neural coding and enables new psychological measurements of visual experiences, with applications spanning clinical diagnostics and brain-machine interfaces.</li>
<li><strong>摘要：</strong>视觉图像重建，将感知内容从大脑活动转化为图像的解码，随着深神经网络（DNN）和生成模型的整合而显着提高。这篇评论可以追溯该领域从早期分类方法到捕获详细的主观视觉体验的复杂重建，强调了等级潜在表示，组成策略和模块化体系结构的作用。尽管取得了显着的进展，但仍存在挑战，例如为看不见的图像实现真正的零击概括，并准确地对感知的复杂，主观方面进行建模。我们讨论了对各种数据集的需求，精致的评估指标与人类感知判断一致，以及增强模型鲁棒性和可推广性的组成表示。道德问题，包括隐私，同意和潜在滥用，被强调为负责任发展的关键考虑。视觉图像重建提供了对神经编码的有希望的见解，并实现了跨越临床诊断和脑机界面的应用，可以对视觉体验进行新的心理测量。</li>
</ul>

<h3>Title: TT-DF: A Large-Scale Diffusion-Based Dataset and Benchmark for Human Body Forgery Detection</h3>
<ul>
<li><strong>Authors: </strong>Wenkui Yang, Zhida Zhang, Xiaoqiang Zhou, Junxian Duan, Jie Cao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.08437">https://arxiv.org/abs/2505.08437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.08437">https://arxiv.org/pdf/2505.08437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.08437]] TT-DF: A Large-Scale Diffusion-Based Dataset and Benchmark for Human Body Forgery Detection(https://arxiv.org/abs/2505.08437)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The emergence and popularity of facial deepfake methods spur the vigorous development of deepfake datasets and facial forgery detection, which to some extent alleviates the security concerns about facial-related artificial intelligence technologies. However, when it comes to human body forgery, there has been a persistent lack of datasets and detection methods, due to the later inception and complexity of human body generation methods. To mitigate this issue, we introduce TikTok-DeepFake (TT-DF), a novel large-scale diffusion-based dataset containing 6,120 forged videos with 1,378,857 synthetic frames, specifically tailored for body forgery detection. TT-DF offers a wide variety of forgery methods, involving multiple advanced human image animation models utilized for manipulation, two generative configurations based on the disentanglement of identity and pose information, as well as different compressed versions. The aim is to simulate any potential unseen forged data in the wild as comprehensively as possible, and we also furnish a benchmark on TT-DF. Additionally, we propose an adapted body forgery detection model, Temporal Optical Flow Network (TOF-Net), which exploits the spatiotemporal inconsistencies and optical flow distribution differences between natural data and forged data. Our experiments demonstrate that TOF-Net achieves favorable performance on TT-DF, outperforming current state-of-the-art extendable facial forgery detection models. For our TT-DF dataset, please refer to this https URL.</li>
<li><strong>摘要：</strong>面部深泡方法的出现和普及促进了深层数据集和面部伪造发现的迅速发展，在某种程度上，这减轻了对面部相关人工智能技术的安全关注。但是，当涉及到人体伪造时，由于人体产生方法的开始和复杂性，由于持续缺乏数据集和检测方法。为了减轻此问题，我们介绍了Tiktok-Deepfake（TT-DF），这是一种新型的基于大规模扩散的数据集，其中包含6,120个具有1,378,857合成框架的锻造视频，专门针对人体伪造发现定制。 TT-DF提供了多种伪造方法，涉及用于操纵的多个高级人类图像动画模型，两种基于身份和姿势信息的分离的生成配置以及不同的压缩版本。目的是尽可能全面地模拟野外未见的锻造数据，我们还在TT-DF上提供了基准。此外，我们提出了一个改编的身体伪造检测模型，时间流动流网络（TOF-NET），该模型利用了时空的不一致和自然数据和锻造数据之间的光流分布差异。我们的实验表明，TOF-NET在TT-DF上取得了有利的性能，表现优于最新的可扩展面部伪造检测模型。有关我们的TT-DF数据集，请参阅此HTTPS URL。</li>
</ul>

<h3>Title: Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter Impact on Performance and Efficiency</h3>
<ul>
<li><strong>Authors: </strong>Adel Ammar, Anis Koubaa, Omer Nacar, Wadii Boulila</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.08445">https://arxiv.org/abs/2505.08445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.08445">https://arxiv.org/pdf/2505.08445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.08445]] Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter Impact on Performance and Efficiency(https://arxiv.org/abs/2505.08445)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models achieve high task performance yet often hallucinate or rely on outdated knowledge. Retrieval-augmented generation (RAG) addresses these gaps by coupling generation with external search. We analyse how hyperparameters influence speed and quality in RAG systems, covering Chroma and Faiss vector stores, chunking policies, cross-encoder re-ranking, and temperature, and we evaluate six metrics: faithfulness, answer correctness, answer relevancy, context precision, context recall, and answer similarity. Chroma processes queries 13% faster, whereas Faiss yields higher retrieval precision, revealing a clear speed-accuracy trade-off. Naive fixed-length chunking with small windows and minimal overlap outperforms semantic segmentation while remaining the quickest option. Re-ranking provides modest gains in retrieval quality yet increases runtime by roughly a factor of 5, so its usefulness depends on latency constraints. These results help practitioners balance computational cost and accuracy when tuning RAG systems for transparent, up-to-date responses. Finally, we re-evaluate the top configurations with a corrective RAG workflow and show that their advantages persist when the model can iteratively request additional evidence. We obtain a near-perfect context precision (99%), which demonstrates that RAG systems can achieve extremely high retrieval accuracy with the right combination of hyperparameters, with significant implications for applications where retrieval quality directly impacts downstream task performance, such as clinical decision support in healthcare.</li>
<li><strong>摘要：</strong>大型语言模型实现高任务性能，但经常幻觉或依靠过时的知识。检索增强的生成（RAG）通过将生成与外部搜索耦合来解决这些差距。我们分析了超参数如何影响抹布系统中的速度和质量，涵盖了色度和faiss矢量商店，分解策略，重新升级和温度，我们评估了六个指标：忠诚，答案，答案，回答相关性，上下文精确度，上下文，召回和回答相似性。 Chroma Process的查询速度快13％，而FAISS的检索精度更高，显示出明确的速度准确性权衡。带有小窗户的固定长度块和最小的重叠优于语义分割，同时保持最快的选项。重新排列可提供适度的检索质量增长，但将运行时增加了大约5倍，因此其有用性取决于延迟约束。这些结果有助于实践者在调整透明，最新响应的抹布系统时平衡计算成本和准确性。最后，我们通过纠正抹布的工作流重新评估顶部配置，并表明当模型可以迭代要求其他证据时，它们的优势持续存在。我们获得了近乎完美的上下文精确度（99％），这表明抹布系统可以通过正确参数的正确组合获得极高的检索准确性，这对应用程序直接影响下游任务绩效（例如医疗保健中的临床决策支持）具有重大影响。</li>
</ul>

<h3>Title: An adaptive sampling algorithm for data-generation to build a data-manifold for physical problem surrogate modeling</h3>
<ul>
<li><strong>Authors: </strong>Chetra Mang, Axel TahmasebiMoradi, David Danan, Mouadh Yagoubi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.08487">https://arxiv.org/abs/2505.08487</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.08487">https://arxiv.org/pdf/2505.08487</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.08487]] An adaptive sampling algorithm for data-generation to build a data-manifold for physical problem surrogate modeling(https://arxiv.org/abs/2505.08487)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Physical models classically involved Partial Differential equations (PDE) and depending of their underlying complexity and the level of accuracy required, and known to be computationally expensive to numerically solve them. Thus, an idea would be to create a surrogate model relying on data generated by such solver. However, training such a model on an imbalanced data have been shown to be a very difficult task. Indeed, if the distribution of input leads to a poor response manifold representation, the model may not learn well and consequently, it may not predict the outcome with acceptable accuracy. In this work, we present an Adaptive Sampling Algorithm for Data Generation (ASADG) involving a physical model. As the initial input data may not accurately represent the response manifold in higher dimension, this algorithm iteratively adds input data into it. At each step the barycenter of each simplicial complex, that the manifold is discretized into, is added as new input data, if a certain threshold is satisfied. We demonstrate the efficiency of the data sampling algorithm in comparison with LHS method for generating more representative input data. To do so, we focus on the construction of a harmonic transport problem metamodel by generating data through a classical solver. By using such algorithm, it is possible to generate the same number of input data as LHS while providing a better representation of the response manifold.</li>
<li><strong>摘要：</strong>物理模型经典涉及部分微分方程（PDE），并取决于其潜在的复杂性和所需的准确性水平，并且已知在数值上求解它们在计算上很昂贵。因此，一个想法是创建一个依靠该求解器生成的数据的替代模型。但是，对数据不平衡的培训已被证明是一项非常艰巨的任务。实际上，如果输入的分布导致响应歧管表示差，则该模型可能无法很好地学习，因此，它可能无法以可接受的准确性来预测结果。在这项工作中，我们提出了一种涉及物理模型的数据生成（ASADG）的自适应采样算法。由于初始输入数据可能无法准确地表示较高维度中的响应歧管，因此该算法迭代将输入数据添加到其中。在每个步骤中，如果满足一定的阈值，则将流形的每个简单复合物的重中心添加到新的输入数据中。与LHS方法相比，我们证明了数据采样算法的效率，以生成更多代表性的输入数据。为此，我们专注于通过经典求解器生成数据来构建谐波传输问题元模型。通过使用这种算法，可以生成与LHS相同数量的输入数据，同时提供响应歧管的更好表示。</li>
</ul>

<h3>Title: A Deep Learning-Driven Framework for Inhalation Injury Grading Using Bronchoscopy Images</h3>
<ul>
<li><strong>Authors: </strong>Yifan Li, Alan W Pang, Jo Woon Chong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.08517">https://arxiv.org/abs/2505.08517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.08517">https://arxiv.org/pdf/2505.08517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.08517]] A Deep Learning-Driven Framework for Inhalation Injury Grading Using Bronchoscopy Images(https://arxiv.org/abs/2505.08517)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Inhalation injuries face a challenge in clinical diagnosis and grading due to the limitations of traditional methods, such as Abbreviated Injury Score (AIS), which rely on subjective assessments and show weak correlations with clinical outcomes. This study introduces a novel deep learning-based framework for grading inhalation injuries using bronchoscopy images with the duration of mechanical ventilation as an objective metric. To address the scarcity of medical imaging data, we propose enhanced StarGAN, a generative model that integrates Patch Loss and SSIM Loss to improve synthetic images' quality and clinical relevance. The augmented dataset generated by enhanced StarGAN significantly improved classification performance when evaluated using the Swin Transformer, achieving an accuracy of 77.78%, an 11.11% improvement over the original dataset. Image quality was assessed using the Fréchet Inception Distance (FID), where Enhanced StarGAN achieved the lowest FID of 30.06, outperforming baseline models. Burn surgeons confirmed the realism and clinical relevance of the generated images, particularly the preservation of bronchial structures and color distribution. These results highlight the potential of enhanced StarGAN in addressing data limitations and improving classification accuracy for inhalation injury grading.</li>
<li><strong>摘要：</strong>由于传统方法的局限性，例如缩写的损伤评分（AIS），吸入损伤在临床诊断和分级方面面临着挑战，该方法依赖于主观评估并显示出与临床结果的相关性较弱。这项研究介绍了一个新型的基于学习的深度学习框架，用于使用支气管镜图像，并以机械通气为客观度量的持续时间进行对等级吸入损伤。为了解决医学成像数据的稀缺性，我们提出了增强的Stargan，该模型会集成斑块丢失和SSIM损失，以提高合成图像的质量和临床相关性。使用SWIN Transformer评估时，由增强的Stargan生成的增强数据集可显着改善分类性能，达到77.78％的精度，比原始数据集提高了11.11％。使用FréchetInception距离（FID）评估了图像质量，在该距离中，增强的Stargan达到了30.06的最低FID，表现优于基线模型。烧伤外科医生证实了生成图像的现实主义和临床相关性，尤其是保存支气管结构和颜色分布。这些结果突出了增强Stargan在解决数据限制和提高吸入损伤分级的分类准确性方面的潜力。</li>
</ul>

<h3>Title: Attention-based Generative Latent Replay: A Continual Learning Approach for WSI Analysis</h3>
<ul>
<li><strong>Authors: </strong>Pratibha Kumari, Daniel Reisenbüchler, Afshin Bozorgpour, Nadine S. Schaadt, Friedrich Feuerhake, Dorit Merhof</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.08524">https://arxiv.org/abs/2505.08524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.08524">https://arxiv.org/pdf/2505.08524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.08524]] Attention-based Generative Latent Replay: A Continual Learning Approach for WSI Analysis(https://arxiv.org/abs/2505.08524)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Whole slide image (WSI) classification has emerged as a powerful tool in computational pathology, but remains constrained by domain shifts, e.g., due to different organs, diseases, or institution-specific variations. To address this challenge, we propose an Attention-based Generative Latent Replay Continual Learning framework (AGLR-CL), in a multiple instance learning (MIL) setup for domain incremental WSI classification. Our method employs Gaussian Mixture Models (GMMs) to synthesize WSI representations and patch count distributions, preserving knowledge of past domains without explicitly storing original data. A novel attention-based filtering step focuses on the most salient patch embeddings, ensuring high-quality synthetic samples. This privacy-aware strategy obviates the need for replay buffers and outperforms other buffer-free counterparts while matching the performance of buffer-based solutions. We validate AGLR-CL on clinically relevant biomarker detection and molecular status prediction across multiple public datasets with diverse centers, organs, and patient cohorts. Experimental results confirm its ability to retain prior knowledge and adapt to new domains, offering an effective, privacy-preserving avenue for domain incremental continual learning in WSI classification.</li>
<li><strong>摘要：</strong>整个幻灯片图像（WSI）分类已成为计算病理学中的强大工具，但由于不同的器官，疾病或特定于机构的变化，例如域移位，例如域移动。为了应对这一挑战，我们在多个实例学习（MIL）设置中为域增量WSI分类提出了一个基于注意力的潜在潜在重播持续学习框架（AGLR-CL）。我们的方法采用高斯混合模型（GMM）来综合WSI表示和补丁计数分布，从而在不明确存储原始数据的情况下保留对过去域的知识。基于注意力的新型过滤步骤着重于最显着的贴片嵌入，以确保高质量的合成样品。这种隐私感知的策略消除了对重播缓冲区的需求，并且在匹配基于缓冲区的解决方案的性能的同时，胜过其他无缓冲区的策略。我们在多个具有不同中心，器官和患者队列的公共数据集的临床相关生物标志物检测和分子状态预测上验证了AGLR-CL。实验结果证实了其保留先验知识并适应新领域的能力，为WSI分类中提供了有效的，具有隐私的途径，用于域增量持续学习。</li>
</ul>

<h3>Title: Dynamic Snake Upsampling Operater and Boundary-Skeleton Weighted Loss for Tubular Structure Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yiqi Chen, Ganghai Huang, Sheng Zhang, Jianglin Dai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.08525">https://arxiv.org/abs/2505.08525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.08525">https://arxiv.org/pdf/2505.08525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.08525]] Dynamic Snake Upsampling Operater and Boundary-Skeleton Weighted Loss for Tubular Structure Segmentation(https://arxiv.org/abs/2505.08525)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Accurate segmentation of tubular topological structures (e.g., fissures and vasculature) is critical in various fields to guarantee dependable downstream quantitative analysis and modeling. However, in dense prediction tasks such as semantic segmentation and super-resolution, conventional upsampling operators cannot accommodate the slenderness of tubular structures and the curvature of morphology. This paper introduces a dynamic snake upsampling operators and a boundary-skeleton weighted loss tailored for topological tubular structures. Specifically, we design a snake upsampling operators based on an adaptive sampling domain, which dynamically adjusts the sampling stride according to the feature map and selects a set of subpixel sampling points along the serpentine path, enabling more accurate subpixel-level feature recovery for tubular structures. Meanwhile, we propose a skeleton-to-boundary increasing weighted loss that trades off main body and boundary weight allocation based on mask class ratio and distance field, preserving main body overlap while enhancing focus on target topological continuity and boundary alignment precision. Experiments across various domain datasets and backbone networks show that this plug-and-play dynamic snake upsampling operator and boundary-skeleton weighted loss boost both pixel-wise segmentation accuracy and topological consistency of results.</li>
<li><strong>摘要：</strong>在各个领域，至关重要的是，对管状拓扑结构（例如裂缝和脉管系统）的准确分割至关重要，以确保可靠的下游定量分析和建模。但是，在诸如语义分割和超分辨率之类的密集预测任务中，常规的UPPLAMPLING OPTIONER无法适应管状结构的细长性和形态的曲率。本文引入了动态蛇上采样算子和针对拓扑管结构量身定制的边界骨骼加权损失。具体而言，我们根据自适应采样域设计了蛇上采样算子，该蛇根据特征图动态调整采样步幅，并选择沿蛇纹石路径的一组子像素采样点，从而实现了更准确的肾小管级别的肾小管特征恢复。同时，我们提出了一个骨骼到边界的加权损失，该损失基于掩盖级别的比率和距离场与主体和边界重量分配，并保持主体重叠，同时增强对目标拓扑连续性和边界对准精度的关注。各个域数据集和骨干网络的实验表明，这种插件的动态蛇上采样算子和边界 - 骨骼加权损失促进了像素分割的精度和结果的拓扑一致性。</li>
</ul>

<h3>Title: The RaspGrade Dataset: Towards Automatic Raspberry Ripeness Grading with Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Lamine Mekhalfi, Paul Chippendale, Fabio Poiesi, Samuele Bonecher, Gilberto Osler, Nicola Zancanella</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.08537">https://arxiv.org/abs/2505.08537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.08537">https://arxiv.org/pdf/2505.08537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.08537]] The RaspGrade Dataset: Towards Automatic Raspberry Ripeness Grading with Deep Learning(https://arxiv.org/abs/2505.08537)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>This research investigates the application of computer vision for rapid, accurate, and non-invasive food quality assessment, focusing on the novel challenge of real-time raspberry grading into five distinct classes within an industrial environment as the fruits move along a conveyor belt. To address this, a dedicated dataset of raspberries, namely RaspGrade, was acquired and meticulously annotated. Instance segmentation experiments revealed that accurate fruit-level masks can be obtained; however, the classification of certain raspberry grades presents challenges due to color similarities and occlusion, while others are more readily distinguishable based on color. The acquired and annotated RaspGrade dataset is accessible on HuggingFace at: this https URL.</li>
<li><strong>摘要：</strong>这项研究调查了计算机视觉在快速，准确和非侵入性的食物质量评估中的应用，重点是实时覆盆子的新颖挑战，该覆盆子分级在沿着传送带移动时，在工业环境中将实时覆盆子分级为五个不同的类别。为了解决这个问题，获取并精心注释了专用的覆盆子数据集，即覆盖物。实例分割实验表明，可以获得准确的果实水平面罩。但是，某些覆盆子等级的分类由于颜色相似性和遮挡而提出了挑战，而其他覆盆子等级则基于颜色更容易区分。可在HuggingFace上访问获取和注释的RASPGRADE数据集，网址为：此HTTPS URL。</li>
</ul>

<h3>Title: DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art</h3>
<ul>
<li><strong>Authors: </strong>Haroon Wahab, Hassan Ugail, Irfan Mehmood</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.08552">https://arxiv.org/abs/2505.08552</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.08552">https://arxiv.org/pdf/2505.08552</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.08552]] DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art(https://arxiv.org/abs/2505.08552)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent proliferation of generative AI tools for visual content creation-particularly in the context of visual artworks-has raised serious concerns about copyright infringement and forgery. The large-scale datasets used to train these models often contain a mixture of copyrighted and non-copyrighted artworks. Given the tendency of generative models to memorize training patterns, they are susceptible to varying degrees of copyright violation. Building on the recently proposed DeepfakeArt Challenge benchmark, this work introduces DFA-CON, a contrastive learning framework designed to detect copyright-infringing or forged AI-generated art. DFA-CON learns a discriminative representation space, posing affinity among original artworks and their forged counterparts within a contrastive learning framework. The model is trained across multiple attack types, including inpainting, style transfer, adversarial perturbation, and cutmix. Evaluation results demonstrate robust detection performance across most attack types, outperforming recent pretrained foundation models. Code and model checkpoints will be released publicly upon acceptance.</li>
<li><strong>摘要：</strong>在视觉艺术品的背景下，最近对视觉内容创建的生成AI工具的扩散引起了人们对版权侵权和伪造的严重关注。用于训练这些型号的大规模数据集通常包含受版权保护和无射击的艺术品的混合物。鉴于生成模型记住训练模式的趋势，它们容易受到不同程度侵犯版权的影响。这项工作以最近提出的DeepFakeart挑战基准为基础，介绍了DFA-CON，这是一个旨在检测版权侵入或锻造AI生成的艺术的对比学习框架。 DFA-CON学习了一个歧视性的代表空间，在对比度学习框架内构成了原始艺术品及其伪造的对应物之间的亲和力。该模型经过多种攻击类型的训练，包括介入，样式转移，对抗性扰动和cutmix。评估结果表明，大多数攻击类型的鲁棒检测性能表现优于最近预验证的基础模型。接受代码和模型检查点将在接受后公开发布。</li>
</ul>

<h3>Title: Rejoining fragmented ancient bamboo slips with physics-driven deep learning</h3>
<ul>
<li><strong>Authors: </strong>Jinchi Zhu, Zhou Zhao, Hailong Lei, Xiaoguang Wang, Jialiang Lu, Jing Li, Qianqian Tang, Jiachen Shen, Gui-Song Xia, Bo Du, Yongchao Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.08601">https://arxiv.org/abs/2505.08601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.08601">https://arxiv.org/pdf/2505.08601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.08601]] Rejoining fragmented ancient bamboo slips with physics-driven deep learning(https://arxiv.org/abs/2505.08601)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Bamboo slips are a crucial medium for recording ancient civilizations in East Asia, and offers invaluable archaeological insights for reconstructing the Silk Road, studying material culture exchanges, and global history. However, many excavated bamboo slips have been fragmented into thousands of irregular pieces, making their rejoining a vital yet challenging step for understanding their content. Here we introduce WisePanda, a physics-driven deep learning framework designed to rejoin fragmented bamboo slips. Based on the physics of fracture and material deterioration, WisePanda automatically generates synthetic training data that captures the physical properties of bamboo fragmentations. This approach enables the training of a matching network without requiring manually paired samples, providing ranked suggestions to facilitate the rejoining process. Compared to the leading curve matching method, WisePanda increases Top-50 matching accuracy from 36\% to 52\%. Archaeologists using WisePanda have experienced substantial efficiency improvements (approximately 20 times faster) when rejoining fragmented bamboo slips. This research demonstrates that incorporating physical principles into deep learning models can significantly enhance their performance, transforming how archaeologists restore and study fragmented artifacts. WisePanda provides a new paradigm for addressing data scarcity in ancient artifact restoration through physics-driven machine learning.</li>
<li><strong>摘要：</strong>竹单是记录东亚古代文明的关键媒介，并为重建丝绸之路，研究物质文化交流和全球历史提供了宝贵的考古见解。但是，许多挖掘的竹条被分散成成千上万的不规则作品，使他们重新加入了理解其内容的重要一步。在这里，我们介绍了Wisepanda，这是一个物理驱动的深度学习框架，旨在重新加入零碎的竹片。根据断裂和材料恶化的物理，Wisepanda会自动生成捕获竹碎片物理特性的合成训练数据。这种方法可以培训匹配网络，而无需手动配对样品，提供了排名的建议以促进重新加入过程。与领先的曲线匹配方法相比，Wisepanda将前50位的匹配精度从36 \％提高到52 \％。使用Wisepanda的考古学家在重新加入零碎的竹滑片时经历了大量效率的提高（约20倍）。这项研究表明，将物理原理纳入深度学习模型可以显着提高其性能，从而改变考古学家如何恢复和研究破碎的人工制品。 Wisepanda提供了一种新的范式，可通过物理驱动的机器学习来解决古代人工制品恢复中的数据稀缺。</li>
</ul>

<h3>Title: Boosting Zero-shot Stereo Matching using Large-scale Mixed Images Sources in the Real World</h3>
<ul>
<li><strong>Authors: </strong>Yuran Wang, Yingping Liang, Ying Fu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.08607">https://arxiv.org/abs/2505.08607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.08607">https://arxiv.org/pdf/2505.08607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.08607]] Boosting Zero-shot Stereo Matching using Large-scale Mixed Images Sources in the Real World(https://arxiv.org/abs/2505.08607)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Stereo matching methods rely on dense pixel-wise ground truth labels, which are laborious to obtain, especially for real-world datasets. The scarcity of labeled data and domain gaps between synthetic and real-world images also pose notable challenges. In this paper, we propose a novel framework, \textbf{BooSTer}, that leverages both vision foundation models and large-scale mixed image sources, including synthetic, real, and single-view images. First, to fully unleash the potential of large-scale single-view images, we design a data generation strategy combining monocular depth estimation and diffusion models to generate dense stereo matching data from single-view images. Second, to tackle sparse labels in real-world datasets, we transfer knowledge from monocular depth estimation models, using pseudo-mono depth labels and a dynamic scale- and shift-invariant loss for additional supervision. Furthermore, we incorporate vision foundation model as an encoder to extract robust and transferable features, boosting accuracy and generalization. Extensive experiments on benchmark datasets demonstrate the effectiveness of our approach, achieving significant improvements in accuracy over existing methods, particularly in scenarios with limited labeled data and domain shifts.</li>
<li><strong>摘要：</strong>立体声匹配方法依赖于密集的像素地面真相标签，这些标签努力地获得，尤其是对于现实世界数据集。合成图像和现实世界图像之间标记的数据和域间隙的稀缺也带来了显着的挑战。在本文中，我们提出了一个新颖的框架\ textbf {Booster}，该框架利用了视觉基础模型和大规模混合图像源，包括合成，真实和单视图像。首先，为了完全释放大型单视图图像的潜力，我们设计了一个数据生成策略，结合了单眼深度估计和扩散模型，以从单视图图像中生成密集的立体声匹配数据。其次，为了解决现实世界数据集中的稀疏标签，我们使用伪单声道深度标签以及动态的量表和转移不变的损失来转移知识，以进行额外的监督。此外，我们将视觉基础模型作为编码器结合起来，以提取强大而可转移的特征，从而提高准确性和概括。基准数据集上的广泛实验证明了我们方法的有效性，对现有方法的准确性有了显着提高，尤其是在标记有限的数据和域移位的情况下。</li>
</ul>

<h3>Title: OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, Yu Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.08617">https://arxiv.org/abs/2505.08617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.08617">https://arxiv.org/pdf/2505.08617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.08617]] OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning(https://arxiv.org/abs/2505.08617)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While humans can flexibly leverage interactive visual cognition for complex problem-solving, enabling Large Vision-Language Models (LVLMs) to learn similarly adaptive behaviors with visual tools remains challenging. A significant hurdle is the current lack of standardized infrastructure, which hinders integrating diverse tools, generating rich interaction data, and training robust agents effectively. To address these gaps, we introduce OpenThinkIMG, the first open-source, comprehensive end-to-end framework for tool-augmented LVLMs. It features standardized vision tool interfaces, scalable trajectory generation for policy initialization, and a flexible training environment. Furthermore, considering supervised fine-tuning (SFT) on static demonstrations offers limited policy generalization for dynamic tool invocation, we propose a novel reinforcement learning (RL) framework V-ToolRL to train LVLMs to learn adaptive policies for invoking external vision tools. V-ToolRL enables LVLMs to autonomously discover optimal tool-usage strategies by directly optimizing for task success using feedback from tool interactions. We empirically validate V-ToolRL on challenging chart reasoning tasks. Our RL-trained agent, built upon a Qwen2-VL-2B, significantly outperforms its SFT-initialized counterpart (+28.83 points) and surpasses established supervised tool-learning baselines like Taco and CogCom by an average of +12.7 points. Notably, it also surpasses prominent closed-source models like GPT-4.1 by +8.68 accuracy points. We hope OpenThinkIMG can serve as a foundational framework for advancing dynamic, tool-augmented visual reasoning, helping the community develop AI agents that can genuinely "think with images".</li>
<li><strong>摘要：</strong>尽管人类可以灵活地利用交互式的视觉认知来解决复杂的问题解决方案，从而使大型视觉模型（LVLMS）使用视觉工具学习类似的适应性行为仍然很具有挑战性。一个重大障碍是目前缺乏标准化的基础架构，这阻碍了整合多种工具，生成丰富的交互数据和有效培训稳定的代理。为了解决这些差距，我们介绍了OpenthInkimg，这是第一个开源，全面的端到端端到端框架，用于工具增强的LVLM。它具有标准化的视觉工具接口，可扩展的轨迹生成用于政策初始化以及灵活的培训环境。此外，在静态演示中考虑监督的微调（SFT）为动态工具调用提供了有限的政策概括，我们提出了一种新颖的加固学习（RL）框架V-Toolrl来培训LVLMS以学习自适应政策，以调用外部视觉工具。 V-Toolrl使LVLM可以通过使用工具交互中的反馈直接优化任务成功来自主发现最佳的工具使用策略。我们从经验上验证了V-Toolrl在具有挑战性的图表推理任务上。我们的RL训练的代理建立在QWEN2-VL-2B上，显着优于其SFT启动的对应物（+28.83分），并超过了已建立的受监督的工具学习基线，例如Taco和Cogcom，平均得出+12.7分。值得注意的是，它还超过了诸如GPT-4.1（+8.68的精度点）（例如GPT-4.1）的突出封闭式模型。我们希望Openthinkimg可以作为推进动态，工具增强视觉推理的基础框架，帮助社区开发可以真正“使用图像思考”的AI代理。</li>
</ul>

<h3>Title: Towards Foundation Models for Experimental Readout Systems Combining Discrete and Continuous Data</h3>
<ul>
<li><strong>Authors: </strong>James Giroux, Cristiano Fanelli</a></li>
<li><strong>Subjects: </strong>cs.LG, hep-ex, nucl-ex, physics.ins-det</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.08736">https://arxiv.org/abs/2505.08736</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.08736">https://arxiv.org/pdf/2505.08736</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.08736]] Towards Foundation Models for Experimental Readout Systems Combining Discrete and Continuous Data(https://arxiv.org/abs/2505.08736)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present a (proto) Foundation Model for Nuclear Physics, capable of operating on low-level detector inputs from Imaging Cherenkov Detectors at the future Electron Ion Collider. To address limitations in existing next-token prediction approaches-namely resolution loss from VQ-VAE tokenization and lack of conditional generation-we propose three key innovations: (i) separate vocabularies for discrete spatial features and continuous variates, combined via Causal Multi-Head Cross-Attention (CMHCA), (ii) continuous kinematic conditioning through prepended context embeddings, and (iii) scalable and simple, high-resolution continuous variate tokenization without joint vocabulary inflation. Our model enables fast, high-fidelity generation of pixel and time sequences for Cherenkov photons, validated through closure tests in the High Performance DIRC. We also show our model generalizes to reconstruction tasks such as pion and kaon identification, in which we show its ability to leverage fine-tuning.</li>
<li><strong>摘要：</strong>我们提出了一个（原始）核物理基础模型，能够在未来电子离子对撞机上成像Cherenkov检测器的低级检测器输入运行。解决现有的下一步预测方法中的局限性 - 即从VQ-VAE象征化中分辨出损失以及缺乏有条件的生成，我们提出了三个关键创新：（i）分离空间特征和连续变体的单独词汇，通过CAUSAL多头部跨性别（CMHCA）（ii）连续的条件（II），（ii）连续（ii）连续（ii）连续（II），（ii）连续（II），（ii），（II），（ii），（ii）且（ii），（ii），（ii），（ii），（ii），（ii），（ii）（ii）（ii）（ii）（ii）（ii）（ii）（ii）（ii）（ii）（ii）（ii）（ii）（II）高分辨率连续变化令牌化，而无需关节词汇膨胀。我们的模型可以通过高性能DIRC中的封闭测试验证，可以快速，高保真的像素和时间序列进行cherenkov光子的时间序列。我们还展示了我们的模型概括为诸如PION和KAON识别之类的重建任务，其中我们显示了其利用微调的能力。</li>
</ul>

<h3>Title: CodePDE: An Inference Framework for LLM-driven PDE Solver Generation</h3>
<ul>
<li><strong>Authors: </strong>Shanda Li, Tanya Marwah, Junhong Shen, Weiwei Sun, Andrej Risteski, Yiming Yang, Ameet Talwalkar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.08783">https://arxiv.org/abs/2505.08783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.08783">https://arxiv.org/pdf/2505.08783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.08783]] CodePDE: An Inference Framework for LLM-driven PDE Solver Generation(https://arxiv.org/abs/2505.08783)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Partial differential equations (PDEs) are fundamental to modeling physical systems, yet solving them remains a complex challenge. Traditional numerical solvers rely on expert knowledge to implement and are computationally expensive, while neural-network-based solvers require large training datasets and often lack interpretability. In this work, we frame PDE solving as a code generation task and introduce CodePDE, the first inference framework for generating PDE solvers using large language models (LLMs). Leveraging advanced inference-time algorithms and scaling strategies, CodePDE unlocks critical capacities of LLM for PDE solving: reasoning, debugging, selfrefinement, and test-time scaling -- all without task-specific tuning. CodePDE achieves superhuman performance across a range of representative PDE problems. We also present a systematic empirical analysis of LLM generated solvers, analyzing their accuracy, efficiency, and numerical scheme choices. Our findings highlight the promise and the current limitations of LLMs in PDE solving, offering a new perspective on solver design and opportunities for future model development. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>部分微分方程（PDE）是建模物理系统的基础，但是解决它们仍然是一个复杂的挑战。传统的数值求解器依靠专家知识来实施，并且在计算上很昂贵，而基于神经网络的求解器则需要大型培训数据集，并且通常缺乏可解释性。在这项工作中，我们将PDE求解作为代码生成任务，并引入Codepde，这是使用大语言模型（LLMS）生成PDE求解器的第一个推理框架。利用先进的推理时间算法和缩放策略，CodePDE解锁了LLM的关键能力来解决PDE：推理，调试，自我翻新和测试时间缩放 - 所有这些都没有特定于任务的调整。 Codepde在一系列代表性的PDE问题中实现了超人的性能。我们还对LLM生成的求解器进行了系统的经验分析，分析了它们的准确性，效率和数值方案选择。我们的发现突出了LLM在PDE解决方案中的希望和当前局限性，为求解器设计和未来模型开发的机会提供了新的观点。我们的代码可在此HTTPS URL上找到。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
