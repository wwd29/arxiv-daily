<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-07-16</h1>
<h3>Title: DALI-PD: Diffusion-based Synthetic Layout Heatmap Generation for ML in Physical Design</h3>
<ul>
<li><strong>Authors: </strong>Bing-Yue Wu, Vidya A. Chhabria</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10606">https://arxiv.org/abs/2507.10606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10606">https://arxiv.org/pdf/2507.10606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10606]] DALI-PD: Diffusion-based Synthetic Layout Heatmap Generation for ML in Physical Design(https://arxiv.org/abs/2507.10606)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Machine learning (ML) has demonstrated significant promise in various physical design (PD) tasks. However, model generalizability remains limited by the availability of high-quality, large-scale training datasets. Creating such datasets is often computationally expensive and constrained by IP. While very few public datasets are available, they are typically static, slow to generate, and require frequent updates. To address these limitations, we present DALI-PD, a scalable framework for generating synthetic layout heatmaps to accelerate ML in PD research. DALI-PD uses a diffusion model to generate diverse layout heatmaps via fast inference in seconds. The heatmaps include power, IR drop, congestion, macro placement, and cell density maps. Using DALI-PD, we created a dataset comprising over 20,000 layout configurations with varying macro counts and placements. These heatmaps closely resemble real layouts and improve ML accuracy on downstream ML tasks such as IR drop or congestion prediction.</li>
<li><strong>摘要：</strong>机器学习（ML）在各种物理设计（PD）任务中表现出了巨大的希望。但是，模型的推广性仍然受到高质量大规模培训数据集的可用性的限制。创建此类数据集通常在计算上很昂贵，并且受到IP的约束。尽管很少有公共数据集可用，但它们通常是静态的，生成缓慢，并且需要频繁的更新。为了解决这些局限性，我们提出了Dali-PD，这是一个可扩展的框架，用于生成合成布局热图以在PD研究中加速ML。 Dali-PD使用扩散模型通过快速推断在几秒钟内通过快速推断生成各种布局热图。热图包括功率，IR滴，拥塞，宏观放置和细胞密度图。使用DALI-PD，我们创建了一个数据集，该数据集包含20,000多个布局配置，并具有不同的宏计数和位置。这些热图非常类似于真实的布局，并提高了下游ML任务（例如IR下降或拥塞预测）的ML准确性。</li>
</ul>

<h3>Title: FedGSCA: Medical Federated Learning with Global Sample Selector and Client Adaptive Adjuster under Label Noise</h3>
<ul>
<li><strong>Authors: </strong>Mengwen Ye, Yingzi Huangfu, Shujian Gao, Wei Ren, Weifan Liu, Zekuan Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10611">https://arxiv.org/abs/2507.10611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10611">https://arxiv.org/pdf/2507.10611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10611]] FedGSCA: Medical Federated Learning with Global Sample Selector and Client Adaptive Adjuster under Label Noise(https://arxiv.org/abs/2507.10611)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) emerged as a solution for collaborative medical image classification while preserving data privacy. However, label noise, which arises from inter-institutional data variability, can cause training instability and degrade model performance. Existing FL methods struggle with noise heterogeneity and the imbalance in medical data. Motivated by these challenges, we propose FedGSCA, a novel framework for enhancing robustness in noisy medical FL. FedGSCA introduces a Global Sample Selector that aggregates noise knowledge from all clients, effectively addressing noise heterogeneity and improving global model stability. Furthermore, we develop a Client Adaptive Adjustment (CAA) mechanism that combines adaptive threshold pseudo-label generation and Robust Credal Labeling Loss. CAA dynamically adjusts to class distributions, ensuring the inclusion of minority samples and carefully managing noisy labels by considering multiple plausible labels. This dual approach mitigates the impact of noisy data and prevents overfitting during local training, which improves the generalizability of the model. We evaluate FedGSCA on one real-world colon slides dataset and two synthetic medical datasets under various noise conditions, including symmetric, asymmetric, extreme, and heterogeneous types. The results show that FedGSCA outperforms the state-of-the-art methods, excelling in extreme and heterogeneous noise scenarios. Moreover, FedGSCA demonstrates significant advantages in improving model stability and handling complex noise, making it well-suited for real-world medical federated learning scenarios.</li>
<li><strong>摘要：</strong>联合学习（FL）作为协作医学图像分类的解决方案，同时保留数据隐私。但是，由机构间数据变异性引起的标签噪声会导致训练不稳定性和降低模型性能。现有的FL方法与噪声异质性和医疗数据失衡的困难。在这些挑战中，我们提出了FedGSCA，这是一种增强嘈杂医疗佛罗里达鲁棒性的新型框架。 FedGSCA引入了一个全球样本选择器，该选择器汇总了所有客户的噪声知识，从而有效地解决了噪声异质性并改善了全球模型稳定性。此外，我们开发了一种客户自适应调整（CAA）机制，该机制结合了自适应阈值伪标签的生成和强大的信用标签损失。 CAA动态地调整了类分布，确保包含少数样本，并通过考虑多个合理的标签来仔细管理嘈杂的标签。这种双重方法减轻了嘈杂数据的影响，并防止在本地培训期间过度适应，从而提高了模型的普遍性。我们在一个真实世界的结肠幻灯片数据集和两个合成医学数据集中评估了FedGSCA，包括对称，不对称，极端和异质类型。结果表明，FedGSCA的表现优于最先进的方法，在极端和异构噪声方面表现出色。此外，FedGSCA在改善模型稳定性和处理复杂的噪声方面具有显着优势，使其非常适合现实的医疗联合学习方案。</li>
</ul>

<h3>Title: Flows and Diffusions on the Neural Manifold</h3>
<ul>
<li><strong>Authors: </strong>Daniel Saragih, Deyu Cao, Tejas Balaji</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10623">https://arxiv.org/abs/2507.10623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10623">https://arxiv.org/pdf/2507.10623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10623]] Flows and Diffusions on the Neural Manifold(https://arxiv.org/abs/2507.10623)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion and flow-based generative models have achieved remarkable success in domains such as image synthesis, video generation, and natural language modeling. In this work, we extend these advances to weight space learning by leveraging recent techniques to incorporate structural priors derived from optimization dynamics. Central to our approach is modeling the trajectory induced by gradient descent as a trajectory inference problem. We unify several trajectory inference techniques under the framework of gradient flow matching, providing a theoretical framework for treating optimization paths as inductive bias. We further explore architectural and algorithmic choices, including reward fine-tuning by adjoint matching, the use of autoencoders for latent weight representation, conditioning on task-specific context data, and adopting informative source distributions such as Kaiming uniform. Experiments demonstrate that our method matches or surpasses baselines in generating in-distribution weights, improves initialization for downstream training, and supports fine-tuning to enhance performance. Finally, we illustrate a practical application in safety-critical systems: detecting harmful covariate shifts, where our method outperforms the closest comparable baseline.</li>
<li><strong>摘要：</strong>扩散和基于流的生成模型在图像综合，视频生成和自然语言建模等领域取得了巨大的成功。在这项工作中，我们通过利用最近的技术结合了从优化动力学得出的结构先验来扩展这些进步，以将这些进步扩展到体重空间学习。我们方法的核心是对梯度下降引起的轨迹作为轨迹推断问题进行建模。我们在梯度流匹配的框架下统一了几种轨迹推理技术，为将优化路径视为电感偏差提供了理论框架。我们进一步探讨了建筑和算法选择，包括按隔壁匹配进行奖励微调，使用自动编码器来进行潜在的重量表示，对特定于任务的上下文数据进行条件以及采用诸如kaiming soliform的信息源分布。实验表明，我们的方法匹配或超过基准在产生分布权重，改善下游训练的初始化，并支持微调以提高性能。最后，我们说明了在安全 - 关键系统中的实际应用：检测有害的协变量变化，我们的方法的表现优于最接近的基线。</li>
</ul>

<h3>Title: ZClassifier: Temperature Tuning and Manifold Approximation via KL Divergence on Logit Space</h3>
<ul>
<li><strong>Authors: </strong>Shim Soon Yong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10638">https://arxiv.org/abs/2507.10638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10638">https://arxiv.org/pdf/2507.10638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10638]] ZClassifier: Temperature Tuning and Manifold Approximation via KL Divergence on Logit Space(https://arxiv.org/abs/2507.10638)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce a novel classification framework, ZClassifier, that replaces conventional deterministic logits with diagonal Gaussian-distributed logits. Our method simultaneously addresses temperature scaling and manifold approximation by minimizing the Kullback-Leibler (KL) divergence between the predicted Gaussian distributions and a unit isotropic Gaussian. This unifies uncertainty calibration and latent control in a principled probabilistic manner, enabling a natural interpretation of class confidence and geometric consistency. Experiments on CIFAR-10 and CIFAR-100 show that ZClassifier improves over softmax classifiers in robustness, calibration, and latent separation. We also demonstrate its effectiveness for classifier-guided generation by interpreting logits as Gaussian semantic potentials.</li>
<li><strong>摘要：</strong>我们介绍了一个新型的分类框架ZClassifier，该框架用对角线高斯分布的ligits代替常规确定逻辑。我们的方法同时解决了通过最小化预测高斯分布与单位同型高斯之间的kullback-leibler（kl）差异来解决温度缩放和歧视近似。这以原则性的概率方式统一了不确定性校准和潜在控制，从而可以自然解释阶级置信度和几何一致性。 Experiments on CIFAR-10 and CIFAR-100 show that ZClassifier improves over softmax classifiers in robustness, calibration, and latent separation. We also demonstrate its effectiveness for classifier-guided generation by interpreting logits as Gaussian semantic potentials.</li>
</ul>

<h3>Title: Spatial Reasoners for Continuous Variables in Any Domain</h3>
<ul>
<li><strong>Authors: </strong>Bart Pogodzinski, Christopher Wewer, Bernt Schiele, Jan Eric Lenssen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10768">https://arxiv.org/abs/2507.10768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10768">https://arxiv.org/pdf/2507.10768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10768]] Spatial Reasoners for Continuous Variables in Any Domain(https://arxiv.org/abs/2507.10768)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We present Spatial Reasoners, a software framework to perform spatial reasoning over continuous variables with generative denoising models. Denoising generative models have become the de-facto standard for image generation, due to their effectiveness in sampling from complex, high-dimensional distributions. Recently, they have started being explored in the context of reasoning over multiple continuous variables. Providing infrastructure for generative reasoning with such models requires a high effort, due to a wide range of different denoising formulations, samplers, and inference strategies. Our presented framework aims to facilitate research in this area, providing easy-to-use interfaces to control variable mapping from arbitrary data domains, generative model paradigms, and inference strategies. Spatial Reasoners are openly available at this https URL</li>
<li><strong>摘要：</strong>我们提出了空间推理器，这是一个软件框架，可通过使用生成的降解模型对连续变量进行空间推理。由于其在复杂的高维分布中取样的有效性，因此脱氧生成模型已成为图像生成的事实上的标准。最近，他们已经在多个连续变量的推理中开始探索它们。通过此类模型为生成推理提供基础架构，由于各种不同的脱氧配方，采样器和推理策略，需要付出很大的努力。我们提出的框架旨在促进该领域的研究，提供易于使用的界面，以控制从任意数据域，生成模型范式和推理策略的可变映射。空间推理器在此HTTPS URL上公开可用</li>
</ul>

<h3>Title: Sparse Fine-Tuning of Transformers for Generative Tasks</h3>
<ul>
<li><strong>Authors: </strong>Wei Chen, Jingxi Yu, Zichen Miao, Qiang Qiu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10855">https://arxiv.org/abs/2507.10855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10855">https://arxiv.org/pdf/2507.10855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10855]] Sparse Fine-Tuning of Transformers for Generative Tasks(https://arxiv.org/abs/2507.10855)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large pre-trained transformers have revolutionized artificial intelligence across various domains, and fine-tuning remains the dominant approach for adapting these models to downstream tasks due to the cost of training from scratch. However, in existing fine-tuning methods, the updated representations are formed as a dense combination of modified parameters, making it challenging to interpret their contributions and understand how the model adapts to new tasks. In this work, we introduce a fine-tuning framework inspired by sparse coding, where fine-tuned features are represented as a sparse combination of basic elements, i.e., feature dictionary atoms. The feature dictionary atoms function as fundamental building blocks of the representation, and tuning atoms allows for seamless adaptation to downstream tasks. Sparse coefficients then serve as indicators of atom importance, identifying the contribution of each atom to the updated representation. Leveraging the atom selection capability of sparse coefficients, we first demonstrate that our method enhances image editing performance by improving text alignment through the removal of unimportant feature dictionary atoms. Additionally, we validate the effectiveness of our approach in the text-to-image concept customization task, where our method efficiently constructs the target concept using a sparse combination of feature dictionary atoms, outperforming various baseline fine-tuning methods.</li>
<li><strong>摘要：</strong>大型预训练的变压器已彻底改变了各个领域的人工智能，并且由于从头开始训练的成本，微调仍然是使这些模型适应下游任务的主要方法。但是，在现有的微调方法中，更新的表示形成是经过修改的参数的密集组合，使解释其贡献并了解该模型如何适应新任务的挑战。在这项工作中，我们引入了一个受稀疏编码启发的微调框架，其中微调特征被表示为基本元素的稀疏组合，即特征词典原子。特征词典原子作为表示形式的基本构建块的功能，调谐原子可以无缝适应下游任务。然后，稀疏系数是原子重要性的指标，确定每个原子对更新表示的贡献。利用稀疏系数的原子选择能力，我们首先证明我们的方法通过去除不重要的特征词典原子来改善文本对齐方式来增强图像编辑性能。此外，我们在文本到图像概念自定义任务中验证方法的有效性，在该任务中，我们的方法使用特征词典原子的稀疏组合有效地构建了目标概念，从而优于各种基线微调方法。</li>
</ul>

<h3>Title: Visually grounded emotion regulation via diffusion models and user-driven reappraisal</h3>
<ul>
<li><strong>Authors: </strong>Edoardo Pinzuti, Oliver Tüscher, André Ferreira Castro</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10861">https://arxiv.org/abs/2507.10861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10861">https://arxiv.org/pdf/2507.10861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10861]] Visually grounded emotion regulation via diffusion models and user-driven reappraisal(https://arxiv.org/abs/2507.10861)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Cognitive reappraisal is a key strategy in emotion regulation, involving reinterpretation of emotionally charged stimuli to alter affective responses. Despite its central role in clinical and cognitive science, real-world reappraisal interventions remain cognitively demanding, abstract, and primarily verbal. This reliance on higher-order cognitive and linguistic processes is often impaired in individuals with trauma or depression, limiting the effectiveness of standard approaches. Here, we propose a novel, visually based augmentation of cognitive reappraisal by integrating large-scale text-to-image diffusion models into the emotional regulation process. Specifically, we introduce a system in which users reinterpret emotionally negative images via spoken reappraisals, which are transformed into supportive, emotionally congruent visualizations using stable diffusion models with a fine-tuned IP-adapter. This generative transformation visually instantiates users' reappraisals while maintaining structural similarity to the original stimuli, externalizing and reinforcing regulatory intent. To test this approach, we conducted a within-subject experiment (N = 20) using a modified cognitive emotion regulation (CER) task. Participants reappraised or described aversive images from the International Affective Picture System (IAPS), with or without AI-generated visual feedback. Results show that AI-assisted reappraisal significantly reduced negative affect compared to both non-AI and control conditions. Further analyses reveal that sentiment alignment between participant reappraisals and generated images correlates with affective relief, suggesting that multimodal coherence enhances regulatory efficacy. These findings demonstrate that generative visual input can support cogitive reappraisal and open new directions at the intersection of generative AI, affective computing, and therapeutic technology.</li>
<li><strong>摘要：</strong>认知重新评估是情绪调节的关键策略，涉及重新解释情绪激动的刺激以改变情感反应。尽管在临床和认知科学中具有核心作用，但现实世界中的重新评估干预措施仍然具有认知要求，抽象和主要口头表达。在具有创伤或抑郁症的个体中，这种对高阶认知和语言过程的依赖通常会受到损害，从而限制了标准方法的有效性。在这里，我们通过将大规模的文本对图扩散模型整合到情绪调节过程中，提出了一种基于视觉的新型认知重新评估的增强。具体而言，我们介绍了一个系统，在该系统中，用户通过口语重新评估重新解释了情感负面的图像，该图像使用稳定的扩散模型和微调的IP-Adapter转变为支持性的，情感上的可视化。这种生成转化在视觉上实例化了用户的重新评估，同时保持与原始刺激，外部化和增强监管意图的结构相似性。为了测试这种方法，我们使用改良的认知情绪调节（CER）任务进行了受试者内实验（n = 20）。参与者在有或没有AI生成的视觉反馈的情况下重新编写或描述了国际情感图片系统（IAP）的厌恶图像。结果表明，与非AI和对照条件相比，AI辅助重新评估显着降低了负面影响。进一步的分析表明，参与者重新评估和产生的图像之间的情感对齐与情感缓解相关，这表明多模式相干性提高了调节效率。这些发现表明，生成的视觉输入可以在生成的AI，情感计算和治疗技术的交集中支持连接和开放的新方向。</li>
</ul>

<h3>Title: Domain-Adaptive Small Language Models for Structured Tax Code Prediction</h3>
<ul>
<li><strong>Authors: </strong>Souvik Nath, Sumit Wadhwa, Luiz Perez</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10880">https://arxiv.org/abs/2507.10880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10880">https://arxiv.org/pdf/2507.10880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10880]] Domain-Adaptive Small Language Models for Structured Tax Code Prediction(https://arxiv.org/abs/2507.10880)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Every day, multinational firms process thousands of transactions, each of which must adhere to tax regulations that vary by jurisdiction and are often nuanced. The determination of product and service tax codes, such as HSN or SAC is a major use case in Tax compliance. An accurate determination of such codes is imperative to avoid any tax penalties. This paper proposes a domain-adaptive small language model (SLM) with an encoder-decoder architecture for the enhanced prediction of product and service tax codes. In this approach, we address the problem of predicting hierarchical tax code sequences using unstructured product and services data. We employ an SLM based upon encoder-decoder architecture as this enables sequential generation of tax codes to capture the hierarchical dependencies present within the tax codes. Our experiments demonstrate that encoder-decoder SLMs can be successfully applied to the sequential prediction of structured tax codes, a domain that remains comparatively unexplored in current NLP research. In this paper, we demonstrate the superior performance of the domain-adaptive encoder-decoder SLMs over flat classifiers when applied to the Harmonized System of Nomenclature (HSN), and achieve superior results compared to decoder-only and encoder-only architectures for structured sequence generation tasks. This approach can also be scaled to other government-mandated tax commodity codes, such as United Nations Standard Products and Services Codes (UNSPSC), or Brazil's Nomenclatura Comum do Mercosul (NCM).</li>
<li><strong>摘要：</strong>每天，跨国公司处理数千笔交易，每项交易都必须遵守因管辖权而变化并且经常细微的税收法规。确定产品和服务税法（例如HSN或SAC）是税收合规性的主要用例。必须准确确定此类代码以避免任何税收罚款。本文提出了一种域自适应小语言模型（SLM），并具有编码器架构结构，以增强产品和服务税法的预测。在这种方法中，我们解决了使用非结构化产品和服务数据预测层次税法序列的问题。我们采用基于编码器架构的SLM，因为此税法可以连续生成税收代码中存在的层次依赖性。我们的实验表明，编码器decoder SLM可以成功地应用于结构化税法的顺序预测，该税法在当前的NLP研究中仍然相对尚未探索。在本文中，我们证明了当应用于统一的命名法（HSN）时，域自适应编码器解码器SLM的卓越性能超过了平面分类器，并且与结构化序列生成任务相比，与仅解码器和仅编码器结构相比，取得了优异的结果。这种方法也可以缩放到其他政府征收的税收商品法规，例如联合国标准产品和服务法规（UNSPSC）或巴西的Nomenclatura comum do MercoSul（NCM）。</li>
</ul>

<h3>Title: Learning from Imperfect Data: Robust Inference of Dynamic Systems using Simulation-based Generative Model</h3>
<ul>
<li><strong>Authors: </strong>Hyunwoo Cho, Hyeontae Jo, Hyung Ju Hwang</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10884">https://arxiv.org/abs/2507.10884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10884">https://arxiv.org/pdf/2507.10884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10884]] Learning from Imperfect Data: Robust Inference of Dynamic Systems using Simulation-based Generative Model(https://arxiv.org/abs/2507.10884)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>System inference for nonlinear dynamic models, represented by ordinary differential equations (ODEs), remains a significant challenge in many fields, particularly when the data are noisy, sparse, or partially observable. In this paper, we propose a Simulation-based Generative Model for Imperfect Data (SiGMoID) that enables precise and robust inference for dynamic systems. The proposed approach integrates two key methods: (1) physics-informed neural networks with hyper-networks that constructs an ODE solver, and (2) Wasserstein generative adversarial networks that estimates ODE parameters by effectively capturing noisy data distributions. We demonstrate that SiGMoID quantifies data noise, estimates system parameters, and infers unobserved system components. Its effectiveness is validated validated through realistic experimental examples, showcasing its broad applicability in various domains, from scientific research to engineered systems, and enabling the discovery of full system dynamics.</li>
<li><strong>摘要：</strong>在许多领域中，以普通微分方程（ODE）表示的非线性动态模型的系统推断仍然是一个重大挑战，尤其是当数据嘈杂，稀疏或可观察到时。在本文中，我们为不完美数据（Sigmoid）提出了一个基于仿真的生成模型，该模型可以为动态系统提供精确且可靠的推断。所提出的方法集成了两种关键方法：（1）物理信息的神经网络与构造ODE求解器的超网络和（2）Wasserstein生成对抗网络，通过有效捕获嘈杂的数据分布来估计ode参数。我们证明Sigmoid量化了数据噪声，估计系统参数以及侵入未观察到的系统组件。通过现实的实验示例来验证其有效性，从科学研究到工程系统，并能够发现完整的系统动态，从而证实了其在各个领域的广泛适用性。</li>
</ul>

<h3>Title: Robust ID-Specific Face Restoration via Alignment Learning</h3>
<ul>
<li><strong>Authors: </strong>Yushun Fang, Lu Liu, Xiang Gao, Qiang Hu, Ning Cao, Jianghe Cui, Gang Chen, Xiaoyun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10943">https://arxiv.org/abs/2507.10943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10943">https://arxiv.org/pdf/2507.10943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10943]] Robust ID-Specific Face Restoration via Alignment Learning(https://arxiv.org/abs/2507.10943)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generative</a></li>
<li><strong>Abstract: </strong>The latest developments in Face Restoration have yielded significant advancements in visual quality through the utilization of diverse diffusion priors. Nevertheless, the uncertainty of face identity introduced by identity-obscure inputs and stochastic generative processes remains unresolved. To address this challenge, we present Robust ID-Specific Face Restoration (RIDFR), a novel ID-specific face restoration framework based on diffusion models. Specifically, RIDFR leverages a pre-trained diffusion model in conjunction with two parallel conditioning modules. The Content Injection Module inputs the severely degraded image, while the Identity Injection Module integrates the specific identity from a given image. Subsequently, RIDFR incorporates Alignment Learning, which aligns the restoration results from multiple references with the same identity in order to suppress the interference of ID-irrelevant face semantics (e.g. pose, expression, make-up, hair style). Experiments demonstrate that our framework outperforms the state-of-the-art methods, reconstructing high-quality ID-specific results with high identity fidelity and demonstrating strong robustness.</li>
<li><strong>摘要：</strong>面部恢复的最新发展通过利用不同的扩散先验，在视觉质量方面取得了重大进步。然而，通过身份 - 观察输入和随机生成过程引入的面部身份的不确定性尚未解决。为了应对这一挑战，我们提出了特定于ID的面部恢复（RIDFR），这是一个基于扩散模型的新型ID特定面部恢复框架。具体而言，RidFR与两个平行条件模块结合使用了预训练的扩散模型。内容注入模块输入严重降级的图像，而身份注入模块从给定图像中整合了特定的身份。随后，RIDFR结合了对齐学习，该学习是由具有相同身份的多个参考引起的恢复，以抑制ID-IRRELERERERVERVANT面部语义的干扰（例如姿势，表达，表情，化妆，发型）。实验表明，我们的框架优于最先进的方法，重建具有高认同忠诚度的高质量ID结果并表现出强大的鲁棒性。</li>
</ul>

<h3>Title: Diffusion Decoding for Peptide De Novo Sequencing</h3>
<ul>
<li><strong>Authors: </strong>Chi-en Amy Tai, Alexander Wong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10955">https://arxiv.org/abs/2507.10955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10955">https://arxiv.org/pdf/2507.10955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10955]] Diffusion Decoding for Peptide De Novo Sequencing(https://arxiv.org/abs/2507.10955)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Peptide de novo sequencing is a method used to reconstruct amino acid sequences from tandem mass spectrometry data without relying on existing protein sequence databases. Traditional deep learning approaches, such as Casanovo, mainly utilize autoregressive decoders and predict amino acids sequentially. Subsequently, they encounter cascading errors and fail to leverage high-confidence regions effectively. To address these issues, this paper investigates using diffusion decoders adapted for the discrete data domain. These decoders provide a different approach, allowing sequence generation to start from any peptide segment, thereby enhancing prediction accuracy. We experiment with three different diffusion decoder designs, knapsack beam search, and various loss functions. We find knapsack beam search did not improve performance metrics and simply replacing the transformer decoder with a diffusion decoder lowered performance. Although peptide precision and recall were still 0, the best diffusion decoder design with the DINOISER loss function obtained a statistically significant improvement in amino acid recall by 0.373 compared to the baseline autoregressive decoder-based Casanovo model. These findings highlight the potential of diffusion decoders to not only enhance model sensitivity but also drive significant advancements in peptide de novo sequencing.</li>
<li><strong>摘要：</strong>从肽开始测序是一种用于从串联质谱数据中重建氨基酸序列的方法，而无需依赖现有的蛋白质序列数据库。传统的深度学习方法，例如Casanovo，主要使用自回归解码器并顺序预测氨基酸。随后，他们遇到级联错误，无法有效利用高信心区域。为了解决这些问题，本文使用适用于离散数据域的扩散解码器进行了研究。这些解码器提供了一种不同的方法，从而使序列产生从任何肽段开始，从而提高了预测准确性。我们尝试三个不同的扩散解码器设计，背包束搜索和各种损耗函数。我们发现背包梁搜索并不能改善性能指标，而只是通过扩散解码器降低性能替换变压器解码器。尽管肽的精度和回忆仍然为0，但与基线自回归解码器的Casanovo模型相比，dinoiser损耗函数的最佳扩散解码器设计在氨基酸回忆中获得了统计学上的显着改善。这些发现突出了扩散解码器不仅提高模型灵敏度的潜力，还可以推动从头测序的肽中的显着进步。</li>
</ul>

<h3>Title: Crafting Imperceptible On-Manifold Adversarial Attacks for Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Zhipeng He, Alexander Stevens, Chun Ouyang, Johannes De Smedt, Alistair Barros, Catarina Moreira</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10998">https://arxiv.org/abs/2507.10998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10998">https://arxiv.org/pdf/2507.10998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10998]] Crafting Imperceptible On-Manifold Adversarial Attacks for Tabular Data(https://arxiv.org/abs/2507.10998)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Adversarial attacks on tabular data present fundamental challenges distinct from image or text domains due to the heterogeneous nature of mixed categorical and numerical features. Unlike images where pixel perturbations maintain visual similarity, tabular data lacks intuitive similarity metrics, making it difficult to define imperceptible modifications. Additionally, traditional gradient-based methods prioritise $\ell_p$-norm constraints, often producing adversarial examples that deviate from the original data distributions, making them detectable. We propose a latent space perturbation framework using a mixed-input Variational Autoencoder (VAE) to generate imperceptible adversarial examples. The proposed VAE integrates categorical embeddings and numerical features into a unified latent manifold, enabling perturbations that preserve statistical consistency. We specify In-Distribution Success Rate (IDSR) to measure the proportion of adversarial examples that remain statistically indistinguishable from the input distribution. Evaluation across six publicly available datasets and three model architectures demonstrates that our method achieves substantially lower outlier rates and more consistent performance compared to traditional input-space attacks and other VAE-based methods adapted from image domain approaches. Our comprehensive analysis includes hyperparameter sensitivity, sparsity control mechanisms, and generative architectural comparisons, revealing that VAE-based attacks depend critically on reconstruction quality but offer superior practical utility when sufficient training data is available. This work highlights the importance of on-manifold perturbations for realistic adversarial attacks on tabular data, offering a robust approach for practical deployment. The source code can be accessed through this https URL.</li>
<li><strong>摘要：</strong>对表格数据的对抗性攻击提出的基本挑战与图像或文本域不同，这是由于混合分类和数值特征的异质性质。与像素扰动保持视觉相似性的图像不同，表格数据缺乏直观的相似性指标，因此很难定义不可察觉的修改。此外，传统的基于梯度的方法优先考虑$ \ ell_p $ -norm约束，通常会产生偏离原始数据分布的对抗性示例，从而使其可检测到。我们建议使用混合输入变异自动编码器（VAE）提出潜在空间扰动框架，以生成不可察觉的对抗示例。所提出的VAE将分类嵌入和数值特征整合到统一的潜在歧管中，从而使能够保留统计一致性的扰动。我们指定分布成功率（IDSR），以测量与输入分布在统计学上保持统计学差异的对抗示例的比例。与传统的输入空间攻击和其他基于VAE的方法相比，对六个公开数据集和三个模型架构进行的评估表明，我们的方法的异常比率和更一致的性能要大大降低，并且具有更高的性能。我们的全面分析包括高参数敏感性，稀疏控制机制和生成架构比较，表明基于VAE的攻击急性取决于重建质量，但在提供足够的培训数据时提供了出色的实用性。这项工作强调了对术中对对抗数据的现实对抗攻击的重要性，从而为实践部署提供了强大的方法。可以通过此HTTPS URL访问源代码。</li>
</ul>

<h3>Title: Semantically Informed Salient Regions Guided Radiology Report Generation</h3>
<ul>
<li><strong>Authors: </strong>Zeyi Hou, Zeqiang Wei, Ruixin Yan, Ning Lang, Xiuzhuang Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11015">https://arxiv.org/abs/2507.11015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11015">https://arxiv.org/pdf/2507.11015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11015]] Semantically Informed Salient Regions Guided Radiology Report Generation(https://arxiv.org/abs/2507.11015)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in automated radiology report generation from chest X-rays using deep learning algorithms have the potential to significantly reduce the arduous workload of radiologists. However, due to the inherent massive data bias in radiology images, where abnormalities are typically subtle and sparsely distributed, existing methods often produce fluent yet medically inaccurate reports, limiting their applicability in clinical practice. To address this issue effectively, we propose a Semantically Informed Salient Regions-guided (SISRNet) report generation method. Specifically, our approach explicitly identifies salient regions with medically critical characteristics using fine-grained cross-modal semantics. Then, SISRNet systematically focuses on these high-information regions during both image modeling and report generation, effectively capturing subtle abnormal findings, mitigating the negative impact of data bias, and ultimately generating clinically accurate reports. Compared to its peers, SISRNet demonstrates superior performance on widely used IU-Xray and MIMIC-CXR datasets.</li>
<li><strong>摘要：</strong>使用深度学习算法从胸部X射线产生自动放射学报告的最新进展有可能显着减少放射科医生的艰巨工作量。但是，由于放射学图像中固有的巨大数据偏见，因为异常通常是微妙且分布稀少的，因此现有方法通常会产生流利但医学上不准确的报告，从而限制了它们在临床实践中的适用性。为了有效地解决此问题，我们提出了一个知情的显着区域引导（SISRNET）报告生成方法。具体而言，我们的方法明确地使用细粒度的跨模式语义来识别具有医学关键特征的显着区域。然后，SISRNET在图像建模和报告生成过程中系统地专注于这些高信息区域，有效地捕获了微妙的异常发现，减轻数据偏见的负面影响，并最终产生临床准确的报告。与其同行相比，Sisrnet在广泛使用的IU-XRAY和MIMIC-CXR数据集上表现出了出色的性能。</li>
</ul>

<h3>Title: Human-Guided Shade Artifact Suppression in CBCT-to-MDCT Translation via Schrödinger Bridge with Conditional Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Sung Ho Kang, Hyun-Cheol Park</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11025">https://arxiv.org/abs/2507.11025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11025">https://arxiv.org/pdf/2507.11025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11025]] Human-Guided Shade Artifact Suppression in CBCT-to-MDCT Translation via Schrödinger Bridge with Conditional Diffusion(https://arxiv.org/abs/2507.11025)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present a novel framework for CBCT-to-MDCT translation, grounded in the Schrodinger Bridge (SB) formulation, which integrates GAN-derived priors with human-guided conditional diffusion. Unlike conventional GANs or diffusion models, our approach explicitly enforces boundary consistency between CBCT inputs and pseudo targets, ensuring both anatomical fidelity and perceptual controllability. Binary human feedback is incorporated via classifier-free guidance (CFG), effectively steering the generative process toward clinically preferred outcomes. Through iterative refinement and tournament-based preference selection, the model internalizes human preferences without relying on a reward model. Subtraction image visualizations reveal that the proposed method selectively attenuates shade artifacts in key anatomical regions while preserving fine structural detail. Quantitative evaluations further demonstrate superior performance across RMSE, SSIM, LPIPS, and Dice metrics on clinical datasets -- outperforming prior GAN- and fine-tuning-based feedback methods -- while requiring only 10 sampling steps. These findings underscore the effectiveness and efficiency of our framework for real-time, preference-aligned medical image translation.</li>
<li><strong>摘要：</strong>我们提出了一个基于Schrodinger Bridge（SB）公式的CBCT-TO-MDCT翻译的新框架，该公式将GAN衍生的先验与人类引导的条件扩散相结合。与传统的gan或扩散模型不同，我们的方法明确地在CBCT输入和伪目标之间实现了边界一致性，从而确保了解剖学保真度和感知可控性。通过无分类器指导（CFG）纳入了二进制人类反馈，有效地将生成过程转向了临床优先的结果。通过迭代精致和基于锦标赛的偏好选择，该模型将人类的偏好内在，而无需依赖奖励模型。减法图像可视化表明，所提出的方法有选择地减弱关键解剖区域中的阴影伪像，同时保留精细的结构细节。定量评估进一步证明了临床数据集的RMSE，SSIM，LPIP和骰子指标的卓越性能 - 表现优于先前的基于GAN和微调的反馈方法 - 同时只需10个采样步骤。这些发现强调了我们在实时，偏爱的医学图像翻译中的框架的有效性和效率。</li>
</ul>

<h3>Title: Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling</h3>
<ul>
<li><strong>Authors: </strong>Hayeon Kim, Ji Ha Jang, Se Young Chun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11061">https://arxiv.org/abs/2507.11061</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11061">https://arxiv.org/pdf/2507.11061</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11061]] Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling(https://arxiv.org/abs/2507.11061)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in 3D neural representations and instance-level editing models have enabled the efficient creation of high-quality 3D content. However, achieving precise local 3D edits remains challenging, especially for Gaussian Splatting, due to inconsistent multi-view 2D part segmentations and inherently ambiguous nature of Score Distillation Sampling (SDS) loss. To address these limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that enables precise and drastic part-level modifications. First, we introduce a robust 3D mask generation module with our 3D-Geometry Aware Label Prediction (3D-GALP), which uses spherical harmonics (SH) coefficients to model view-dependent label variations and soft-label property, yielding accurate and consistent part segmentations across viewpoints. Second, we propose a regularized SDS loss that combines the standard SDS loss with additional regularizers. In particular, an L1 anchor loss is introduced via our Scheduled Latent Mixing and Part (SLaMP) editing method, which generates high-quality part-edited 2D images and confines modifications only to the target region while preserving contextual coherence. Additional regularizers, such as Gaussian prior removal, further improve flexibility by allowing changes beyond the existing context, and robust 3D masking prevents unintended edits. Experimental results demonstrate that our RoMaP achieves state-of-the-art local 3D editing on both reconstructed and generated Gaussian scenes and objects qualitatively and quantitatively, making it possible for more robust and flexible part-level 3D Gaussian editing.</li>
<li><strong>摘要：</strong>3D神经表示和实例级编辑模型的最新进展已使高质量3D内容的有效创建。但是，由于不一致的多视图2D零件分段以及得分蒸馏采样（SDS）损失的固有性质，因此实现精确的本地3D编辑仍然具有挑战性，尤其是对于高斯裂缝。为了解决这些局限性，我们提出了Romap，这是一种新型的本地3D高斯编辑框架，可以实现精确而剧烈的零件级修改。首先，我们使用3D几何的标签预测（3D-GALP）引入了强大的3D掩码生成模块，该预测（3D-GALP）使用球形谐波（SH）系数（SH）系数来模拟与视图相关的标签变化和软标签属性，从而产生跨视点的准确且一致的零件段。其次，我们提出了一个正规的SDS损失，将标准SDS损失与其他正规化器结合在一起。特别是，通过计划的潜在混合和零件（缩水）编辑方法引入L1锚损失，该方法生成高质量的部分编辑的2D图像，并仅将修改限制在目标区域的同时，同时保留上下文相干性。诸如高斯事先删除之类的其他正规机构通过允许超出现有上下文的变化来进一步提高灵活性，而强大的3D遮罩可以阻止意外编辑。实验结果表明，我们的ROMAP在重构和生成的高斯场景和物体上都在定性和定量上实现了最先进的本地3D编辑，从而实现了更强大和灵活的零件级别3D高斯编辑。</li>
</ul>

<h3>Title: Atmos-Bench: 3D Atmospheric Structures for Climate Insight</h3>
<ul>
<li><strong>Authors: </strong>Tianchi Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11085">https://arxiv.org/abs/2507.11085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11085">https://arxiv.org/pdf/2507.11085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11085]] Atmos-Bench: 3D Atmospheric Structures for Climate Insight(https://arxiv.org/abs/2507.11085)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Atmospheric structure, represented by backscatter coefficients (BC) recovered from satellite LiDAR attenuated backscatter (ATB), provides a volumetric view of clouds, aerosols, and molecules, playing a critical role in human activities, climate understanding, and extreme weather forecasting. Existing methods often rely on auxiliary inputs and simplified physics-based approximations, and lack a standardized 3D benchmark for fair evaluation. However, such approaches may introduce additional uncertainties and insufficiently capture realistic radiative transfer and atmospheric scattering-absorption effects. To bridge these gaps, we present Atmos-Bench: the first 3D atmospheric benchmark, along with a novel FourCastX: Frequency-enhanced Spatio-Temporal Mixture-of-Experts Network that (a) generates 921,600 image slices from 3D scattering volumes simulated at 532 nm and 355 nm by coupling WRF with an enhanced COSP simulator over 384 land-ocean time steps, yielding high-quality voxel-wise references; (b) embeds ATB-BC physical constraints into the model architecture, promoting energy consistency during restoration; (c) achieves consistent improvements on the Atmos-Bench dataset across both 355 nm and 532 nm bands, outperforming state-of-the-art baseline models without relying on auxiliary inputs. Atmos-Bench establishes a new standard for satellite-based 3D atmospheric structure recovery and paves the way for deeper climate insight.</li>
<li><strong>摘要：</strong>从卫星激光雷达（ATB）中恢复的反向散射系数（BC）表示的大气结构提供了云，气溶胶和分子的体积视图，在人类活动，气候理解，气候理解和极端天气预测中起着至关重要的作用。现有方法通常依赖于辅助输入和简化的基于物理的近似值，并且缺乏标准化的3D基准进行公平评估。但是，这种方法可能会引入其他不确定性，并不足以捕获逼真的辐射转移和大气散射吸收效应。为了弥合这些差距，我们提出了Atmos台式：第一个3D大气基准，以及新颖的FourcastX：频率增强时空的交换网络，该网络（a）从3D散射量中生成921,600张图像切片，以532 Nm和355 nm的隔离率与355 nm的simplanged sim forff and coups inffort and-infform infform coups infform coups infform infform infform coups infform and coupt 3高质量的体素参考； （b）将ATB-BC物理约束嵌入模型结构中，从而促进恢复过程中的能量一致性； （c）在355 nm和532 nm频段上，在大气台基础数据集上实现一致的改进，在不依赖辅助输入的情况下优于最先进的基线模型。 Atmos Bench为基于卫星的3D大气结构恢复建立了新的标准，并为更深的气候见解铺平了道路。</li>
</ul>

<h3>Title: Try Harder: Hard Sample Generation and Learning for Clothes-Changing Person Re-ID</h3>
<ul>
<li><strong>Authors: </strong>Hankun Liu, Yujian Zhao, Guanglin Niu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11119">https://arxiv.org/abs/2507.11119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11119">https://arxiv.org/pdf/2507.11119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11119]] Try Harder: Hard Sample Generation and Learning for Clothes-Changing Person Re-ID(https://arxiv.org/abs/2507.11119)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Hard samples pose a significant challenge in person re-identification (ReID) tasks, particularly in clothing-changing person Re-ID (CC-ReID). Their inherent ambiguity or similarity, coupled with the lack of explicit definitions, makes them a fundamental bottleneck. These issues not only limit the design of targeted learning strategies but also diminish the model's robustness under clothing or viewpoint changes. In this paper, we propose a novel multimodal-guided Hard Sample Generation and Learning (HSGL) framework, which is the first effort to unify textual and visual modalities to explicitly define, generate, and optimize hard samples within a unified paradigm. HSGL comprises two core components: (1) Dual-Granularity Hard Sample Generation (DGHSG), which leverages multimodal cues to synthesize semantically consistent samples, including both coarse- and fine-grained hard positives and negatives for effectively increasing the hardness and diversity of the training data. (2) Hard Sample Adaptive Learning (HSAL), which introduces a hardness-aware optimization strategy that adjusts feature distances based on textual semantic labels, encouraging the separation of hard positives and drawing hard negatives closer in the embedding space to enhance the model's discriminative capability and robustness to hard samples. Extensive experiments on multiple CC-ReID benchmarks demonstrate the effectiveness of our approach and highlight the potential of multimodal-guided hard sample generation and learning for robust CC-ReID. Notably, HSAL significantly accelerates the convergence of the targeted learning procedure and achieves state-of-the-art performance on both PRCC and LTCC datasets. The code is available at this https URL.</li>
<li><strong>摘要：</strong>硬样品对人的重新识别（REID）任务提出了重大挑战，尤其是在改变衣服的人Re-ID（CC-REID）中。它们固有的歧义或相似性，再加上缺乏明确的定义，使它们成为基本的瓶颈。这些问题不仅限制了有针对性的学习策略的设计，而且还会减少在服装或观点变化下模型的鲁棒性。在本文中，我们提出了一种新型的多模式引导的硬样品生成和学习（HSGL）框架，这是统一文本和视觉方式以明确定义，生成和优化统一范式内的硬样品的第一个努力。 HSGL包括两个核心组成部分：（1）双颗粒性硬样品产生（DGHSG），它们利用多模式线索来综合语义一致的样品，包括粗糙和细粒度的硬质和负面效应以及有效地增加训练数据的硬度和多样性。 （2）硬样品自适应学习（HSAL），它引入了一种硬度感知的优化策略，该策略根据文本语义标签调整特征距离，鼓励将硬积极性分离，并在嵌入空间中更紧密地绘制硬质量，以增强模型的歧视能力和稳健性，并与硬样品的稳健性。对多个CC-REID基准测试的广泛实验证明了我们方法的有效性，并突出了多模式引导的硬样品产生和学习鲁棒CC-REID的潜力。值得注意的是，HSAL显着加速了目标学习程序的融合，并在PRCC和LTCC数据集上实现了最先进的性能。该代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Latent Space Consistency for Sparse-View CT Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Duoyou Chen, Yunqing Chen, Can Zhang, Zhou Wang, Cheng Chen, Ruoxiu Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11152">https://arxiv.org/abs/2507.11152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11152">https://arxiv.org/pdf/2507.11152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11152]] Latent Space Consistency for Sparse-View CT Reconstruction(https://arxiv.org/abs/2507.11152)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Computed Tomography (CT) is a widely utilized imaging modality in clinical settings. Using densely acquired rotational X-ray arrays, CT can capture 3D spatial features. However, it is confronted with challenged such as significant time consumption and high radiation exposure. CT reconstruction methods based on sparse-view X-ray images have garnered substantial attention from researchers as they present a means to mitigate costs and risks. In recent years, diffusion models, particularly the Latent Diffusion Model (LDM), have demonstrated promising potential in the domain of 3D CT reconstruction. Nonetheless, due to the substantial differences between the 2D latent representation of X-ray modalities and the 3D latent representation of CT modalities, the vanilla LDM is incapable of achieving effective alignment within the latent space. To address this issue, we propose the Consistent Latent Space Diffusion Model (CLS-DM), which incorporates cross-modal feature contrastive learning to efficiently extract latent 3D information from 2D X-ray images and achieve latent space alignment between modalities. Experimental results indicate that CLS-DM outperforms classical and state-of-the-art generative models in terms of standard voxel-level metrics (PSNR, SSIM) on the LIDC-IDRI and CTSpine1K datasets. This methodology not only aids in enhancing the effectiveness and economic viability of sparse X-ray reconstructed CT but can also be generalized to other cross-modal transformation tasks, such as text-to-image synthesis. We have made our code publicly available at this https URL to facilitate further research and applications in other domains.</li>
<li><strong>摘要：</strong>计算机断层扫描（CT）是临床环境中广泛使用的成像方式。使用密集的旋转X射线阵列，CT可以捕获3D空间特征。但是，它面临着挑战，例如大量的时间消耗和高辐射暴露。基于稀疏视图X射线图像的CT重建方法吸引了研究人员的大量关注，因为它们提出了一种减轻成本和风险的手段。近年来，扩散模型，特别是潜在扩散模型（LDM），在3D CT重建的域中表现出了有希望的潜力。但是，由于X射线模式的2D潜在表示与CT模式的3D潜在表示之间存在实质性差异，因此香草LDM无法在潜在空间内实现有效的一致性。为了解决此问题，我们提出了一致的潜在空间扩散模型（CLS-DM），该模型结合了跨模式的对比度学习，以从2D X射线图像中有效提取潜在的3D信息，并在模态之间实现潜在的空间对齐。实验结果表明，在LIDC-IDRI和CTSPINE1K数据集上，CLS-DM在标准素级指标（PSNR，SSIM）方面优于经典和最先进的生成模型。这种方法不仅有助于增强稀疏X射线重建的CT的有效性和经济可行性，而且还可以推广到其他跨模式转换任务，例如文本对图像综合。我们已在此HTTPS URL上公开提供代码，以促进其他域中的进一步研究和应用。</li>
</ul>

<h3>Title: An Explainable AI-Enhanced Machine Learning Approach for Cardiovascular Disease Detection and Risk Assessment</h3>
<ul>
<li><strong>Authors: </strong>Md. Emon Akter Sourov, Md. Sabbir Hossen, Pabon Shaha, Mohammad Minoar Hossain, Md Sadiq Iqbal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11185">https://arxiv.org/abs/2507.11185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11185">https://arxiv.org/pdf/2507.11185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11185]] An Explainable AI-Enhanced Machine Learning Approach for Cardiovascular Disease Detection and Risk Assessment(https://arxiv.org/abs/2507.11185)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Heart disease remains a major global health concern, particularly in regions with limited access to medical resources and diagnostic facilities. Traditional diagnostic methods often fail to accurately identify and manage heart disease risks, leading to adverse outcomes. Machine learning has the potential to significantly enhance the accuracy, efficiency, and speed of heart disease diagnosis. In this study, we proposed a comprehensive framework that combines classification models for heart disease detection and regression models for risk prediction. We employed the Heart Disease dataset, which comprises 1,035 cases. To address the issue of class imbalance, the Synthetic Minority Oversampling Technique (SMOTE) was applied, resulting in the generation of an additional 100,000 synthetic data points. Performance metrics, including accuracy, precision, recall, F1-score, R2, MSE, RMSE, and MAE, were used to evaluate the model's effectiveness. Among the classification models, Random Forest emerged as the standout performer, achieving an accuracy of 97.2% on real data and 97.6% on synthetic data. For regression tasks, Linear Regression demonstrated the highest R2 values of 0.992 and 0.984 on real and synthetic datasets, respectively, with the lowest error metrics. Additionally, Explainable AI techniques were employed to enhance the interpretability of the models. This study highlights the potential of machine learning to revolutionize heart disease diagnosis and risk prediction, thereby facilitating early intervention and enhancing clinical decision-making.</li>
<li><strong>摘要：</strong>心脏病仍然是全球主要的健康问题，特别是在获得医疗资源和诊断设施有限的地区。传统的诊断方法通常无法准确识别和管理心脏病风险，从而导致不良后果。机器学习有可能显着提高心脏病诊断的准确性，效率和速度。在这项研究中，我们提出了一个综合框架，该框架结合了心脏病检测的分类模型和风险预测的回归模型。我们采用了心脏病数据集，其中包括1,035例。为了解决阶级不平衡问题，应用了合成的少数族裔过采样技术（SMOTE），从而产生了另外100,000个合成数据点。性能指标，包括准确性，精度，召回，F1得分，R2，MSE，RMSE和MAE，用于评估模型的有效性。在分类模型中，随机森林成为出色的表演者，实际数据的准确度为97.2％，合成数据的准确度为97.6％。对于回归任务，线性回归分别显示了最高的R2值为0.992和0.984，分别具有最低的误差指标。此外，还采用了可解释的AI技术来增强模型的解释性。这项研究强调了机器学习革新心脏病诊断和风险预测的潜力，从而促进了早期干预并增强临床决策。</li>
</ul>

<h3>Title: NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models</h3>
<ul>
<li><strong>Authors: </strong>X. Feng, H. Yu, M. Wu, S. Hu, J. Chen, C. Zhu, J. Wu, X. Chu, K. Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11245">https://arxiv.org/abs/2507.11245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11245">https://arxiv.org/pdf/2507.11245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11245]] NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models(https://arxiv.org/abs/2507.11245)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>With the rapid development of foundation video generation technologies, long video generation models have exhibited promising research potential thanks to expanded content creation space. Recent studies reveal that the goal of long video generation tasks is not only to extend video duration but also to accurately express richer narrative content within longer videos. However, due to the lack of evaluation benchmarks specifically designed for long video generation models, the current assessment of these models primarily relies on benchmarks with simple narrative prompts (e.g., VBench). To the best of our knowledge, our proposed NarrLV is the first benchmark to comprehensively evaluate the Narrative expression capabilities of Long Video generation models. Inspired by film narrative theory, (i) we first introduce the basic narrative unit maintaining continuous visual presentation in videos as Temporal Narrative Atom (TNA), and use its count to quantitatively measure narrative richness. Guided by three key film narrative elements influencing TNA changes, we construct an automatic prompt generation pipeline capable of producing evaluation prompts with a flexibly expandable number of TNAs. (ii) Then, based on the three progressive levels of narrative content expression, we design an effective evaluation metric using the MLLM-based question generation and answering framework. (iii) Finally, we conduct extensive evaluations on existing long video generation models and the foundation generation models. Experimental results demonstrate that our metric aligns closely with human judgments. The derived evaluation outcomes reveal the detailed capability boundaries of current video generation models in narrative content expression.</li>
<li><strong>摘要：</strong>随着基础视频生成技术的快速发展，由于内容创建空间的扩大，长期视频生成模型已表现出有希望的研究潜力。最近的研究表明，长期视频生成任务的目标不仅是延长视频持续时间，而且还可以准确地在更长的视频中表达更丰富的叙事内容。但是，由于缺乏专门为长时间视频生成模型设计的评估基准，因此对这些模型的当前评估主要依赖于具有简单叙事提示的基准（例如，Vbench）。据我们所知，我们提出的narrlv是第一个全面评估长视频生成模型的叙事表达能力的基准。受电影叙事理论的启发，（i）我们首先介绍了基本的叙事单元，该单元将视频中的连续视觉呈现作为时间叙事原子（TNA），并利用其计数来定量测量叙事丰富性。在影响TNA变化的三个关键膜叙事元素的指导下，我们构建了一个自动及时的生成管道，能够生产具有可灵活扩展的TNA的评估提示。 （ii）然后，基于叙事内容表达的三个渐进水平，我们使用基于MLLM的问题生成和回答框架设计有效的评估度量。 （iii）最后，我们对现有的长视频生成模型和基础生成模型进行了广泛的评估。实验结果表明，我们的度量与人类判断紧密保持一致。派生的评估结果揭示了叙事内容表达中当前视频生成模型的详细能力边界。</li>
</ul>

<h3>Title: Generative Click-through Rate Prediction with Applications to Search Advertising</h3>
<ul>
<li><strong>Authors: </strong>Lingwei Kong, Lu Wang, Changping Peng, Zhangang Lin, Ching Law, Jingping Shao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11246">https://arxiv.org/abs/2507.11246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11246">https://arxiv.org/pdf/2507.11246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11246]] Generative Click-through Rate Prediction with Applications to Search Advertising(https://arxiv.org/abs/2507.11246)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Click-Through Rate (CTR) prediction models are integral to a myriad of industrial settings, such as personalized search advertising. Current methods typically involve feature extraction from users' historical behavior sequences combined with product information, feeding into a discriminative model that is trained on user feedback to estimate CTR. With the success of models such as GPT, the potential for generative models to enrich expressive power beyond discriminative models has become apparent. In light of this, we introduce a novel model that leverages generative models to enhance the precision of CTR predictions in discriminative models. To reconcile the disparate data aggregation needs of both model types, we design a two-stage training process: 1) Generative pre-training for next-item prediction with the given item category in user behavior sequences; 2) Fine-tuning the well-trained generative model within a discriminative CTR prediction framework. Our method's efficacy is substantiated through extensive experiments on a new dataset, and its significant utility is further corroborated by online A/B testing results. Currently, the model is deployed on one of the world's largest e-commerce platforms, and we intend to release the associated code and dataset in the future.</li>
<li><strong>摘要：</strong>点击率（CTR）预测模型是无数工业环境（例如个性化搜索广告）不可或缺的。当前方法通常涉及从用户的历史行为序列与产品信息结合使用的特征提取，并将其进一步为歧视性模型，该模型在用户反馈中训练以估算CTR。借助GPT等模型的成功，生成模型的潜力超出了歧视模型以外的表达能力。鉴于此，我们引入了一种新型模型，该模型利用生成模型来增强判别模型中CTR预测的精度。为了调和这两种模型类型的不同数据聚合需求，我们设计了一个两阶段的训练过程：1）用户行为序列中给定项目类别的下一项目预测的生成预培训； 2）在区分CTR预测框架内微调训练良好的生成模型。通过在新数据集上进行广泛的实验，我们的方法的功效得到了证实，并且通过在线A/B测试结果进一步证实了它的重要实用程序。目前，该模型已部署在世界上最大的电子商务平台之一，我们打算将来发布相关的代码和数据集。</li>
</ul>

<h3>Title: MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection</h3>
<ul>
<li><strong>Authors: </strong>Guanghao Wu, Chen Xu, Hai Song, Chong Wang, Qixing Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11252">https://arxiv.org/abs/2507.11252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11252">https://arxiv.org/pdf/2507.11252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11252]] MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection(https://arxiv.org/abs/2507.11252)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Smoke is the first visible indicator of a this http URL the advancement of deep learning, image-based smoke detection has become a crucial method for detecting and preventing forest fires. However, the scarcity of smoke image data from forest fires is one of the significant factors hindering the detection of forest fire smoke. Image generation models offer a promising solution for synthesizing realistic smoke images. However, current inpainting models exhibit limitations in generating high-quality smoke representations, particularly manifesting as inconsistencies between synthesized smoke and background contexts. To solve these problems, we proposed a comprehensive framework for generating forest fire smoke images. Firstly, we employed the pre-trained segmentation model and the multimodal model to obtain smoke masks and image this http URL, to address the insufficient utilization of masks and masked images by inpainting models, we introduced a network architecture guided by mask and masked image features. We also proposed a new loss function, the mask random difference loss, which enhances the consistency of the generated effects around the mask by randomly expanding and eroding the mask this http URL, to generate a smoke image dataset using random masks for subsequent detection tasks, we incorporated smoke characteristics and use a multimodal large language model as a filtering tool to select diverse and reasonable smoke images, thereby improving the quality of the synthetic dataset. Experiments showed that our generated smoke images are realistic and diverse, and effectively enhance the performance of forest fire smoke detection models. Code is available at this https URL.</li>
<li><strong>摘要：</strong>烟雾是该HTTP URL的第一个可见指标。深度学习的进步，基于图像的烟雾检测已成为检测和防止森林火灾的关键方法。但是，森林火灾的烟雾图像数据的稀缺是阻碍检测森林火烟的重要因素之一。图像生成模型为合成逼真的烟雾图像提供了有希望的解决方案。然而，当前的涂上模型在产生高质量的烟雾表示方面表现出局限性，尤其表现为综合烟雾和背景环境之间的不一致。为了解决这些问题，我们提出了一个综合框架来产生森林火烟图像。首先，我们采用了预先训练的分割模型和多模式模型来获取烟雾掩膜并图像此HTTP URL，以通过indpainting模型来解决掩模和掩盖图像的充分利用，我们引入了一个由掩码指导的网络体系结构，由掩码和掩盖图像特征引入。我们还提出了一种新的损失函数，即面膜随机差损失，通过随机扩展和侵蚀掩模，从而增强了面具周围生成的效果的一致性，以使用随机掩码来生成一个烟雾图像数据集，以随后的检测任务，我们将烟雾特征和合理的烟雾构成了多样的烟雾，并使用多样化的烟雾来选择多样的烟雾，并将数据集。实验表明，我们产生的烟雾图像是现实的和多样的，并有效地增强了森林烟雾检测模型的性能。代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Implementing Adaptations for Vision AutoRegressive Model</h3>
<ul>
<li><strong>Authors: </strong>Kaif Shaikh, Antoni Kowalczuk, Franziska Boenisch, Adam Dziedzic</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11441">https://arxiv.org/abs/2507.11441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11441">https://arxiv.org/pdf/2507.11441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11441]] Implementing Adaptations for Vision AutoRegressive Model(https://arxiv.org/abs/2507.11441)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision AutoRegressive model (VAR) was recently introduced as an alternative to Diffusion Models (DMs) in image generation domain. In this work we focus on its adaptations, which aim to fine-tune pre-trained models to perform specific downstream tasks, like medical data generation. While for DMs there exist many techniques, adaptations for VAR remain underexplored. Similarly, differentially private (DP) adaptations-ones that aim to preserve privacy of the adaptation data-have been extensively studied for DMs, while VAR lacks such solutions. In our work, we implement and benchmark many strategies for VAR, and compare them to state-of-the-art DM adaptation strategies. We observe that VAR outperforms DMs for non-DP adaptations, however, the performance of DP suffers, which necessitates further research in private adaptations for VAR. Code is available at this https URL.</li>
<li><strong>摘要：</strong>最近引入了视觉自回旋模型（VAR）作为图像生成域中扩散模型（DM）的替代方法。在这项工作中，我们着重于其适应性，旨在微调预训练的模型以执行特定的下游任务，例如医疗数据的生成。尽管对于DMS存在许多技术，但VAR的适应性仍然没有被逐渐解散。同样，旨在维护适应数据的隐私数据的差异私有（DP）适应器对DMS进行了广泛的研究，而VAR缺乏此类解决方案。在我们的工作中，我们实施并基准了许多VAR策略，并将其与最先进的DM适应策略进行比较。我们观察到，VAR的表现优于DM的非DP适应，但是DP的性能会受到损害，这需要对VAR的私人适应进行进一步的研究。代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: HUG-VAS: A Hierarchical NURBS-Based Generative Model for Aortic Geometry Synthesis and Controllable Editing</h3>
<ul>
<li><strong>Authors: </strong>Pan Du, Mingqi Xu, Xiaozhi Zhu, Jian-xun Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11474">https://arxiv.org/abs/2507.11474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11474">https://arxiv.org/pdf/2507.11474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11474]] HUG-VAS: A Hierarchical NURBS-Based Generative Model for Aortic Geometry Synthesis and Controllable Editing(https://arxiv.org/abs/2507.11474)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Accurate characterization of vascular geometry is essential for cardiovascular diagnosis and treatment planning. Traditional statistical shape modeling (SSM) methods rely on linear assumptions, limiting their expressivity and scalability to complex topologies such as multi-branch vascular structures. We introduce HUG-VAS, a Hierarchical NURBS Generative model for Vascular geometry Synthesis, which integrates NURBS surface parameterization with diffusion-based generative modeling to synthesize realistic, fine-grained aortic geometries. Trained with 21 patient-specific samples, HUG-VAS generates anatomically faithful aortas with supra-aortic branches, yielding biomarker distributions that closely match those of the original dataset. HUG-VAS adopts a hierarchical architecture comprising a denoising diffusion model that generates centerlines and a guided diffusion model that synthesizes radial profiles conditioned on those centerlines, thereby capturing two layers of anatomical variability. Critically, the framework supports zero-shot conditional generation from image-derived priors, enabling practical applications such as interactive semi-automatic segmentation, robust reconstruction under degraded imaging conditions, and implantable device optimization. To our knowledge, HUG-VAS is the first SSM framework to bridge image-derived priors with generative shape modeling via a unified integration of NURBS parameterization and hierarchical diffusion processes.</li>
<li><strong>摘要：</strong>血管几何形状的准确表征对于心血管诊断和治疗计划至关重要。传统的统计形状建模（SSM）方法依赖于线性假设，将其表现力和可扩展性限制在复杂的拓扑结构（例如多支分支血管结构）上。我们介绍了Hug-Vas，这是一种用于血管几何合成的分层NURBS生成模型，将NURBS表面参数化与基于扩散的生成建模相结合，以合成逼真的，细粒的主动脉几何形状。 Hug-Vas经过21个特定于患者的样本培训，可产生带有忠实的主动脉，并具有上主管上的分支，产生的生物标志物分布与原始数据集的分布非常匹配。 Hug-Vas采用了一个分层体系结构，其中包括一个脱氧扩散模型，该模型生成中心线和一个引导扩散模型，该模型综合了基于这些中心线的径向轮廓，从而捕获了两层解剖变异性。至关重要的是，该框架支持从图像衍生的先验中零摄像的有条件生成，从而实现了实用的应用，例如交互式半自动分割，在退化的成像条件下进行稳健的重建以及可植入的设备优化。据我们所知，HUG-VAS是第一个通过NURBS参数化和分层扩散过程的统一集成通过统一的集成来桥接具有生成形状建模的SSM框架。</li>
</ul>

<h3>Title: CATVis: Context-Aware Thought Visualization</h3>
<ul>
<li><strong>Authors: </strong>Tariq Mehmood, Hamza Ahmad, Muhammad Haroon Shakeel, Murtaza Taj</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11522">https://arxiv.org/abs/2507.11522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11522">https://arxiv.org/pdf/2507.11522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11522]] CATVis: Context-Aware Thought Visualization(https://arxiv.org/abs/2507.11522)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>EEG-based brain-computer interfaces (BCIs) have shown promise in various applications, such as motor imagery and cognitive state monitoring. However, decoding visual representations from EEG signals remains a significant challenge due to their complex and noisy nature. We thus propose a novel 5-stage framework for decoding visual representations from EEG signals: (1) an EEG encoder for concept classification, (2) cross-modal alignment of EEG and text embeddings in CLIP feature space, (3) caption refinement via re-ranking, (4) weighted interpolation of concept and caption embeddings for richer semantics, and (5) image generation using a pre-trained Stable Diffusion model. We enable context-aware EEG-to-image generation through cross-modal alignment and re-ranking. Experimental results demonstrate that our method generates high-quality images aligned with visual stimuli, outperforming SOTA approaches by 13.43% in Classification Accuracy, 15.21% in Generation Accuracy and reducing Fréchet Inception Distance by 36.61%, indicating superior semantic alignment and image quality.</li>
<li><strong>摘要：</strong>基于EEG的脑部计算机界面（BCIS）在各种应用中显示了有望，例如运动图像和认知状态监测。但是，由于其复杂和嘈杂的性质，来自EEG信号的视觉表示仍然是一个重大挑战。因此，我们提出了一个新颖的5阶段框架，用于从EEG信号解码视觉表示：（1）EEG概念分类的编码器，（2）EEG的跨模式对齐和文本嵌入剪辑特征空间中的嵌入，（3）通过重新排列的字幕修补，（4）使用概念和图形插入的概念插入式的Interporation for a Semeddings和Caption semedsick，Richersick-richants，Richers-richants，Richantsick，（5）扩散模型。我们通过跨模式的一致性和重新排列来启用上下文感知的脑电图生成。实验结果表明，我们的方法生成与视觉刺激对齐的高质量图像，分类精度的表现优于SOTA方法，生成准确性的15.21％，并将Fréchet的启动距离降低了36.61％，表明了出色的语义一致性和图像质量和图像质量。</li>
</ul>

<h3>Title: CharaConsist: Fine-Grained Consistent Character Generation</h3>
<ul>
<li><strong>Authors: </strong>Mengyu Wang, Henghui Ding, Jianing Peng, Yao Zhao, Yunpeng Chen, Yunchao Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11533">https://arxiv.org/abs/2507.11533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11533">https://arxiv.org/pdf/2507.11533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11533]] CharaConsist: Fine-Grained Consistent Character Generation(https://arxiv.org/abs/2507.11533)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In text-to-image generation, producing a series of consistent contents that preserve the same identity is highly valuable for real-world applications. Although a few works have explored training-free methods to enhance the consistency of generated subjects, we observe that they suffer from the following problems. First, they fail to maintain consistent background details, which limits their applicability. Furthermore, when the foreground character undergoes large motion variations, inconsistencies in identity and clothing details become evident. To address these problems, we propose CharaConsist, which employs point-tracking attention and adaptive token merge along with decoupled control of the foreground and background. CharaConsist enables fine-grained consistency for both foreground and background, supporting the generation of one character in continuous shots within a fixed scene or in discrete shots across different scenes. Moreover, CharaConsist is the first consistent generation method tailored for text-to-image DiT model. Its ability to maintain fine-grained consistency, combined with the larger capacity of latest base model, enables it to produce high-quality visual outputs, broadening its applicability to a wider range of real-world scenarios. The source code has been released at this https URL</li>
<li><strong>摘要：</strong>在文本到图像生成中，生产一系列保留相同身份的一致内容对于现实世界应用非常有价值。尽管一些作品探索了无培训的方法来增强生成的受试者的一致性，但我们观察到它们遭受了以下问题的困扰。首先，他们无法保持一致的背景细节，从而限制了其适用性。此外，当前景角色经历大型运动变化时，身份和衣服细节的不一致就变得明显了。为了解决这些问题，我们提出了特征主义者，该角色采用了点跟踪的关注和适应性令牌合并以及对前景和背景的脱耦控制。 Chanaconsist可以为前景和背景提供细粒度的一致性，从而在固定场景中连续拍摄或在不同场景中的离散拍摄中支持一个角色的产生。此外，CharaconSist是第一个针对文本对图像DIT模型量身定制的一致生成方法。它具有保持细粒度一致性的能力，再加上最新基本模型的较大容量，使其能够产生高质量的视觉输出，从而扩大了其对更广泛的现实世界情景的适用性。源代码已在此HTTPS URL上发布</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
