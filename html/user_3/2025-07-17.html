<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-07-17</h1>
<h3>Title: Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Zejian Li, Yize Li, Chenye Meng, Zhongni Liu, Yang Ling, Shengyuan Zhang, Guang Yang, Changyuan Yang, Zhiyuan Yang, Lingyun Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11554">https://arxiv.org/abs/2507.11554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11554">https://arxiv.org/pdf/2507.11554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11554]] Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models(https://arxiv.org/abs/2507.11554)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in diffusion models (DMs) have been propelled by alignment methods that post-train models to better conform to human preferences. However, these approaches typically require computation-intensive training of a base model and a reward model, which not only incurs substantial computational overhead but may also compromise model accuracy and training efficiency. To address these limitations, we propose Inversion-DPO, a novel alignment framework that circumvents reward modeling by reformulating Direct Preference Optimization (DPO) with DDIM inversion for DMs. Our method conducts intractable posterior sampling in Diffusion-DPO with the deterministic inversion from winning and losing samples to noise and thus derive a new post-training paradigm. This paradigm eliminates the need for auxiliary reward models or inaccurate appromixation, significantly enhancing both precision and efficiency of training. We apply Inversion-DPO to a basic task of text-to-image generation and a challenging task of compositional image generation. Extensive experiments show substantial performance improvements achieved by Inversion-DPO compared to existing post-training methods and highlight the ability of the trained generative models to generate high-fidelity compositionally coherent images. For the post-training of compostitional image geneation, we curate a paired dataset consisting of 11,140 images with complex structural annotations and comprehensive scores, designed to enhance the compositional capabilities of generative models. Inversion-DPO explores a new avenue for efficient, high-precision alignment in diffusion models, advancing their applicability to complex realistic generation tasks. Our code is available at this https URL</li>
<li><strong>摘要：</strong>扩散模型（DMS）的最新进展已通过对齐方式推动，这些方法是在训练后模型更好地符合人类偏好的。但是，这些方法通常需要对基本模型和奖励模型进行计算密集型培训，这不仅会造成大量的计算开销，而且还可能损害模型的准确性和训练效率。为了解决这些局限性，我们提出了倒置DPO，这是一个新颖的对齐框架，通过使用DMS的DDIM反转来重新设计直接偏好优化（DPO）来避免奖励建模。我们的方法在扩散-DPO中进行了棘手的后验采样，从胜利和丢失样品到噪声的确定性反转，从而得出了新的训练后范式。这种范式消除了对辅助奖励模型的需求或不正确的批准，从而显着提高了培训的精度和效率。我们将Invernion-DPO应用于文本到图像生成的基本任务以及组成图像生成的具有挑战性的任务。广泛的实验表明，与现有的训练后方法相比，反转DPO实现了实质性的改进，并突出了受过训练的生成模型生成高保真性构图相干图像的能力。为了进行堆肥图像基因的训练，我们策划了一个配对的数据集，该数据集由11,140张图像组成，具有复杂的结构注释和全面的分数，旨在增强生成模型的组成能力。 Invernion-DPO探索了一种在扩散模型中高效，高精度对齐的新途径，从而将其适用于复杂的现实生成任务。我们的代码可在此HTTPS URL上找到</li>
</ul>

<h3>Title: Expert Operational GANS: Towards Real-Color Underwater Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Ozer Can Devecioglu, Serkan Kiranyaz, Mehmet Yamac, Moncef Gabbouj</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11562">https://arxiv.org/abs/2507.11562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11562">https://arxiv.org/pdf/2507.11562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11562]] Expert Operational GANS: Towards Real-Color Underwater Image Restoration(https://arxiv.org/abs/2507.11562)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>The wide range of deformation artifacts that arise from complex light propagation, scattering, and depth-dependent attenuation makes the underwater image restoration to remain a challenging problem. Like other single deep regressor networks, conventional GAN-based restoration methods struggle to perform well across this heterogeneous domain, since a single generator network is typically insufficient to capture the full range of visual degradations. In order to overcome this limitation, we propose xOp-GAN, a novel GAN model with several expert generator networks, each trained solely on a particular subset with a certain image quality. Thus, each generator can learn to maximize its restoration performance for a particular quality range. Once a xOp-GAN is trained, each generator can restore the input image and the best restored image can then be selected by the discriminator based on its perceptual confidence score. As a result, xOP-GAN is the first GAN model with multiple generators where the discriminator is being used during the inference of the regression task. Experimental results on benchmark Large Scale Underwater Image (LSUI) dataset demonstrates that xOp-GAN achieves PSNR levels up to 25.16 dB, surpassing all single-regressor models by a large margin even, with reduced complexity.</li>
<li><strong>摘要：</strong>由复杂的光传播，散射和深度依赖性衰减引起的多种变形伪像，使水下图像恢复成为一个具有挑战性的问题。与其他单一的深层回归网络一样，基于GAN的常规恢复方法难以在此异质域进行良好的表现，因为单个发电机网络通常不足以捕获各种视觉降级。为了克服这一限制，我们提出了Xop-gan，这是一个具有多个专家生成器网络的新型GAN模型，每个模型都仅在具有一定图像质量的特定子集上进行训练。因此，每个发电机都可以学会在特定质量范围内最大化其恢复性能。一旦训练了Xop-gan，每个发电机就可以恢复输入图像，然后可以根据其感知置信度得分来选择最佳的恢复图像。结果，Xop-gan是第一个具有多个发电机的GAN模型，其中在回归任务推理期间使用鉴别器。基准大规模水下图像（LSUI）数据集的实验结果表明，XOP-GAN达到的PSNR水平高达25.16 dB，甚至较大的余量都超过了所有单重型模型，甚至具有降低的复杂性。</li>
</ul>

<h3>Title: SketchDNN: Joint Continuous-Discrete Diffusion for CAD Sketch Generation</h3>
<ul>
<li><strong>Authors: </strong>Sathvik Chereddy, John Femiani</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11579">https://arxiv.org/abs/2507.11579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11579">https://arxiv.org/pdf/2507.11579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11579]] SketchDNN: Joint Continuous-Discrete Diffusion for CAD Sketch Generation(https://arxiv.org/abs/2507.11579)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We present SketchDNN, a generative model for synthesizing CAD sketches that jointly models both continuous parameters and discrete class labels through a unified continuous-discrete diffusion process. Our core innovation is Gaussian-Softmax diffusion, where logits perturbed with Gaussian noise are projected onto the probability simplex via a softmax transformation, facilitating blended class labels for discrete variables. This formulation addresses 2 key challenges, namely, the heterogeneity of primitive parameterizations and the permutation invariance of primitives in CAD sketches. Our approach significantly improves generation quality, reducing Fréchet Inception Distance (FID) from 16.04 to 7.80 and negative log-likelihood (NLL) from 84.8 to 81.33, establishing a new state-of-the-art in CAD sketch generation on the SketchGraphs dataset.</li>
<li><strong>摘要：</strong>我们提出了SketchDNN，这是一种用于合成CAD草图的生成模型，该模型通过统一的连续二异化扩散过程共同建模连续参数和离散类标签。我们的核心创新是高斯 - 柔软的扩散，其中通过软磁性转换将带有高斯噪声扰动的logits投射到概率上，从而促进了混合类标签，以实现离散变量。该公式解决了2个关键挑战，即原始参数化的异质性和在CAD草图中原语的置换不变性。我们的方法将发电质量显着提高，将FréchetInception距离（FID）从16.04降低到7.80，并从84.8降低到81.33，从而在素描格言数据集中建立了一个新的CAD草图生成中的新最先进的素描。</li>
</ul>

<h3>Title: Synthetic Tabular Data Generation: A Comparative Survey for Modern Techniques</h3>
<ul>
<li><strong>Authors: </strong>Raju Challagundla, Mohsen Dorodchi, Pu Wang, Minwoo Lee</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11590">https://arxiv.org/abs/2507.11590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11590">https://arxiv.org/pdf/2507.11590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11590]] Synthetic Tabular Data Generation: A Comparative Survey for Modern Techniques(https://arxiv.org/abs/2507.11590)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>As privacy regulations become more stringent and access to real-world data becomes increasingly constrained, synthetic data generation has emerged as a vital solution, especially for tabular datasets, which are central to domains like finance, healthcare and the social sciences. This survey presents a comprehensive and focused review of recent advances in synthetic tabular data generation, emphasizing methods that preserve complex feature relationships, maintain statistical fidelity, and satisfy privacy requirements. A key contribution of this work is the introduction of a novel taxonomy based on practical generation objectives, including intended downstream applications, privacy guarantees, and data utility, directly informing methodological design and evaluation strategies. Therefore, this review prioritizes the actionable goals that drive synthetic data creation, including conditional generation and risk-sensitive modeling. Additionally, the survey proposes a benchmark framework to align technical innovation with real-world demands. By bridging theoretical foundations with practical deployment, this work serves as both a roadmap for future research and a guide for implementing synthetic tabular data in privacy-critical environments.</li>
<li><strong>摘要：</strong>随着隐私法规变得越来越严格，对现实数据的访问变得越来越受到限制，综合数据生成已成为重要的解决方案，尤其是对于表格数据集，这对于金融，医疗保健和社会科学等领域至关重要。这项调查对合成表格数据生成的最新进展进行了全面的重点回顾，强调了保持复杂特征关系，保持统计保真度和满足隐私要求的方法。这项工作的一个关键贡献是基于实践生成目标的新型分类法，包括预期的下游应用程序，隐私保证和数据实用程序，直接为方法论设计和评估策略提供了信息。因此，本综述优先考虑驱动合成数据创建的可行目标，包括有条件的生成和风险敏感的建模。此外，调查还提出了一个基准框架，以使技术创新与现实世界的需求保持一致。通过将理论基础与实际部署桥接，这项工作既是未来研究的路线图，又是在关键环境中实施综合表格数据的指南。</li>
</ul>

<h3>Title: Interpretable Prediction of Lymph Node Metastasis in Rectal Cancer MRI Using Variational Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Keel, Aaron Quyn, David Jayne, Maryam Mohsin, Samuel D. Relton</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11638">https://arxiv.org/abs/2507.11638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11638">https://arxiv.org/pdf/2507.11638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11638]] Interpretable Prediction of Lymph Node Metastasis in Rectal Cancer MRI Using Variational Autoencoders(https://arxiv.org/abs/2507.11638)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Effective treatment for rectal cancer relies on accurate lymph node metastasis (LNM) staging. However, radiological criteria based on lymph node (LN) size, shape and texture morphology have limited diagnostic accuracy. In this work, we investigate applying a Variational Autoencoder (VAE) as a feature encoder model to replace the large pre-trained Convolutional Neural Network (CNN) used in existing approaches. The motivation for using a VAE is that the generative model aims to reconstruct the images, so it directly encodes visual features and meaningful patterns across the data. This leads to a disentangled and structured latent space which can be more interpretable than a CNN. Models are deployed on an in-house MRI dataset with 168 patients who did not undergo neo-adjuvant treatment. The post-operative pathological N stage was used as the ground truth to evaluate model predictions. Our proposed model 'VAE-MLP' achieved state-of-the-art performance on the MRI dataset, with cross-validated metrics of AUC 0.86 +/- 0.05, Sensitivity 0.79 +/- 0.06, and Specificity 0.85 +/- 0.05. Code is available at: this https URL.</li>
<li><strong>摘要：</strong>直肠癌的有效治疗依赖于准确的淋巴结转移（LNM）分期。但是，基于淋巴结（LN）大小，形状和纹理形态的放射学标准的诊断精度有限。在这项工作中，我们调查了将变异自动编码器（VAE）作为特征编码器模型来替代现有方法中使用的大型预训练的卷积神经网络（CNN）。使用VAE的动机是生成模型旨在重建图像，因此它直接编码了整个数据中的视觉特征和有意义的模式。这导致了一个分离且结构化的潜在空间，比CNN更容易解释。模型被部署在内部MRI数据集上，其中168例未接受新辅助治疗的患者。术后病理N阶段用作评估模型预测的基础真理。我们提出的模型“ VAE-MLP”在MRI数据集上实现了最先进的性能，AUC 0.86 +/- 0.05的交叉验证指标，灵敏度为0.79 +/- 0.06，特异性为0.85 +//- 0.05。代码可用：此HTTPS URL。</li>
</ul>

<h3>Title: Deep Generative Methods and Tire Architecture Design</h3>
<ul>
<li><strong>Authors: </strong>Fouad Oubari, Raphael Meunier, Rodrigue Décatoire, Mathilde Mougeot</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11639">https://arxiv.org/abs/2507.11639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11639">https://arxiv.org/pdf/2507.11639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11639]] Deep Generative Methods and Tire Architecture Design(https://arxiv.org/abs/2507.11639)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>As deep generative models proliferate across the AI landscape, industrial practitioners still face critical yet unanswered questions about which deep generative models best suit complex manufacturing design tasks. This work addresses this question through a complete study of five representative models (Variational Autoencoder, Generative Adversarial Network, multimodal Variational Autoencoder, Denoising Diffusion Probabilistic Model, and Multinomial Diffusion Model) on industrial tire architecture generation. Our evaluation spans three key industrial scenarios: (i) unconditional generation of complete multi-component designs, (ii) component-conditioned generation (reconstructing architectures from partial observations), and (iii) dimension-constrained generation (creating designs that satisfy specific dimensional requirements). To enable discrete diffusion models to handle conditional scenarios, we introduce categorical inpainting, a mask-aware reverse diffusion process that preserves known labels without requiring additional training. Our evaluation employs geometry-aware metrics specifically calibrated for industrial requirements, quantifying spatial coherence, component interaction, structural connectivity, and perceptual fidelity. Our findings reveal that diffusion models achieve the strongest overall performance; a masking-trained VAE nonetheless outperforms the multimodal variant MMVAE\textsuperscript{+} on nearly all component-conditioned metrics, and within the diffusion family MDM leads in-distribution whereas DDPM generalises better to out-of-distribution dimensional constraints.</li>
<li><strong>摘要：</strong>随着深层生成模型在AI景观中扩散，工业从业人员仍然面临着关键但未解决的问题，即有关哪些深层生成模型最适合复杂的制造设计任务。这项工作通过对五个代表性模型的完整研究（变异自动编码器，生成对抗网络，多模式变异自动编码器，deo概率模型和多义扩散模型）在工业轮胎架构上解决。我们的评估涵盖了三个关键的工业方案：（i）无条件生成完整的多组分设计，（ii）组件条件的生成（从部分观察到重建体系结构），以及（iii）维度受限的生成（创建满足特定尺寸要求的设计）。为了使离散的扩散模型能够处理有条件的情况，我们引入了分类涂料，这是一种面膜感知的反向扩散过程，可保留已知标签的情况，而无需进行额外的培训。我们的评估采用专门校准工业需求的几何学指标，量化空间连贯性，组件相互作用，结构连接性和知觉忠诚度。我们的发现表明，扩散模型的总体表现最强。尽管如此，受掩蔽训练的VAE还是超过了几乎所有组件条件指标的多模式变体MMVAE \ TextSuperScript {+}，并且在扩散家族中，MDM领导MDM在分布中引起分布，而DDPM ddpm可以更好地将ddpm通用到分布量度分布较小的点结构。</li>
</ul>

<h3>Title: Subgraph Generation for Generalizing on Out-of-Distribution Links</h3>
<ul>
<li><strong>Authors: </strong>Jay Revolinsky, Harry Shomer, Jiliang Tang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11710">https://arxiv.org/abs/2507.11710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11710">https://arxiv.org/pdf/2507.11710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11710]] Subgraph Generation for Generalizing on Out-of-Distribution Links(https://arxiv.org/abs/2507.11710)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Graphs Neural Networks (GNNs) demonstrate high-performance on the link prediction (LP) task. However, these models often rely on all dataset samples being drawn from the same distribution. In addition, graph generative models (GGMs) show a pronounced ability to generate novel output graphs. Despite this, GGM applications remain largely limited to domain-specific tasks. To bridge this gap, we propose FLEX as a GGM framework which leverages two mechanism: (1) structurally-conditioned graph generation, and (2) adversarial co-training between an auto-encoder and GNN. As such, FLEX ensures structural-alignment between sample distributions to enhance link-prediction performance in out-of-distribution (OOD) scenarios. Notably, FLEX does not require expert knowledge to function in different OOD scenarios. Numerous experiments are conducted in synthetic and real-world OOD settings to demonstrate FLEX's performance-enhancing ability, with further analysis for understanding the effects of graph data augmentation on link structures. The source code is available here: this https URL.</li>
<li><strong>摘要：</strong>图形神经网络（GNNS）在链接预测（LP）任务上展示了高性能。但是，这些模型通常依赖于从相同分布中绘制的所有数据集样本。此外，图生成模型（GGM）显示出生成新型输出图的明显能力。尽管如此，GGM应用程序仍然在很大程度上仅限于特定领域的任务。为了弥合这一差距，我们提出了flex作为GGM框架，该框架利用了两个机制：（1）结构条件的图生成，以及（2）自动编码器和GNN之间的对抗共同训练。因此，flex确保样本分布之间的结构对齐，以增强链接预测的性能（OOD）方案。值得注意的是，Flex不需要专家知识在不同的OOD方案中起作用。在合成和现实世界中，进行了许多实验，以证明Flex的增强性能能力，并进一步分析以了解图数据增强对链接结构的影响。源代码可在此处可用：此HTTPS URL。</li>
</ul>

<h3>Title: Torsional-GFN: a conditional conformation generator for small molecules</h3>
<ul>
<li><strong>Authors: </strong>Alexandra Volokhova, Léna Néhale Ezzine, Piotr Gaiński, Luca Scimeca, Emmanuel Bengio, Prudencio Tossou, Yoshua Bengio, Alex Hernandez-Garcia</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11759">https://arxiv.org/abs/2507.11759</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11759">https://arxiv.org/pdf/2507.11759</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11759]] Torsional-GFN: a conditional conformation generator for small molecules(https://arxiv.org/abs/2507.11759)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generating stable molecular conformations is crucial in several drug discovery applications, such as estimating the binding affinity of a molecule to a target. Recently, generative machine learning methods have emerged as a promising, more efficient method than molecular dynamics for sampling of conformations from the Boltzmann distribution. In this paper, we introduce Torsional-GFN, a conditional GFlowNet specifically designed to sample conformations of molecules proportionally to their Boltzmann distribution, using only a reward function as training signal. Conditioned on a molecular graph and its local structure (bond lengths and angles), Torsional-GFN samples rotations of its torsion angles. Our results demonstrate that Torsional-GFN is able to sample conformations approximately proportional to the Boltzmann distribution for multiple molecules with a single model, and allows for zero-shot generalization to unseen bond lengths and angles coming from the MD simulations for such molecules. Our work presents a promising avenue for scaling the proposed approach to larger molecular systems, achieving zero-shot generalization to unseen molecules, and including the generation of the local structure into the GFlowNet model.</li>
<li><strong>摘要：</strong>在几种药物发现应用中，产生稳定的分子构象至关重要，例如估计分子与靶标的结合亲和力。最近，与分子动力学相比，生成机器学习方法已成为一种有希望的，更有效的方法，用于从玻尔兹曼分布中取样构象。在本文中，我们介绍了扭转GFN，这是一种条件GFLOWNET，专为与其玻尔兹曼分布成比例地采样分子的构象，仅使用奖励函数作为训练信号。扭转GFN在分子图及其局部结构（键长和角度）上进行调节，样品旋转其扭转角。我们的结果表明，扭转-GFN能够采样与单个模型的多个分子的玻尔兹曼分布成正比的构象，并允许零拍的概括以未看到的键长和角度来自此类分子的MD模拟。我们的工作提出了一个有希望的途径，用于将提出的方法扩展到更大的分子系统，从而实现零拍的概括，以使其对看不见的分子，并将局部结构的产生包括在GFLOWNET模型中。</li>
</ul>

<h3>Title: Beyond Task-Specific Reasoning: A Unified Conditional Generative Framework for Abstract Visual Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Fan Shi, Bin Li, Xiangyang Xue</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11761">https://arxiv.org/abs/2507.11761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11761">https://arxiv.org/pdf/2507.11761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11761]] Beyond Task-Specific Reasoning: A Unified Conditional Generative Framework for Abstract Visual Reasoning(https://arxiv.org/abs/2507.11761)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Abstract visual reasoning (AVR) enables humans to quickly discover and generalize abstract rules to new scenarios. Designing intelligent systems with human-like AVR abilities has been a long-standing topic in the artificial intelligence community. Deep AVR solvers have recently achieved remarkable success in various AVR tasks. However, they usually use task-specific designs or parameters in different tasks. In such a paradigm, solving new tasks often means retraining the model, and sometimes retuning the model architectures, which increases the cost of solving AVR problems. In contrast to task-specific approaches, this paper proposes a novel Unified Conditional Generative Solver (UCGS), aiming to address multiple AVR tasks in a unified framework. First, we prove that some well-known AVR tasks can be reformulated as the problem of estimating the predictability of target images in problem panels. Then, we illustrate that, under the proposed framework, training one conditional generative model can solve various AVR tasks. The experiments show that with a single round of multi-task training, UCGS demonstrates abstract reasoning ability across various AVR tasks. Especially, UCGS exhibits the ability of zero-shot reasoning, enabling it to perform abstract reasoning on problems from unseen AVR tasks in the testing phase.</li>
<li><strong>摘要：</strong>抽象的视觉推理（AVR）使人类能够快速发现并将抽象规则概括为新场景。具有类似人类的AVR能力的智能系统一直是人工智能界的长期话题。 Deep AVR求解器最近在各种AVR任务中取得了巨大的成功。但是，他们通常在不同任务中使用特定于任务的设计或参数。在这样的范式中，解决新任务通常意味着重新训练模型，有时会重新调整模型体系结构，从而增加了解决AVR问题的成本。与特定于任务的方法相反，本文提出了一种新型的统一条件生成求解器（UCGS），旨在解决统一框架中的多个AVR任务。首先，我们证明可以将一些众所周知的AVR任务重新重新制定为估计问题面板中目标图像的可预测性的问题。然后，我们说明，在拟议的框架下，训练一个条件生成模型可以解决各种AVR任务。实验表明，通过一轮多任务训练，UCGS展示了各种AVR任务的抽象推理能力。尤其是，UCG表现出零拍的推理的能力，使其能够在测试阶段对未见AVR任务的问题进行抽象推理。</li>
</ul>

<h3>Title: Enforcing Latent Euclidean Geometry in Single-Cell VAEs for Manifold Interpolation</h3>
<ul>
<li><strong>Authors: </strong>Alessandro Palma, Sergei Rybakov, Leon Hetzel, Stephan Günnemann, Fabian J. Theis</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11789">https://arxiv.org/abs/2507.11789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11789">https://arxiv.org/pdf/2507.11789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11789]] Enforcing Latent Euclidean Geometry in Single-Cell VAEs for Manifold Interpolation(https://arxiv.org/abs/2507.11789)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Latent space interpolations are a powerful tool for navigating deep generative models in applied settings. An example is single-cell RNA sequencing, where existing methods model cellular state transitions as latent space interpolations with variational autoencoders, often assuming linear shifts and Euclidean geometry. However, unless explicitly enforced, linear interpolations in the latent space may not correspond to geodesic paths on the data manifold, limiting methods that assume Euclidean geometry in the data representations. We introduce FlatVI, a novel training framework that regularises the latent manifold of discrete-likelihood variational autoencoders towards Euclidean geometry, specifically tailored for modelling single-cell count data. By encouraging straight lines in the latent space to approximate geodesic interpolations on the decoded single-cell manifold, FlatVI enhances compatibility with downstream approaches that assume Euclidean latent geometry. Experiments on synthetic data support the theoretical soundness of our approach, while applications to time-resolved single-cell RNA sequencing data demonstrate improved trajectory reconstruction and manifold interpolation.</li>
<li><strong>摘要：</strong>潜在空间插值是在应用设置中导航深层生成模型的强大工具。一个示例是单细胞RNA测序，其中现有方法将细胞状态转变模型为带有变异自动编码器的潜在空间插值，通常假设线性移位和欧几里得几何形状。但是，除非明确强制执行，否则潜在空间中的线性插值可能与数据歧管上的地理路径相对应，限制了在数据表示中假设欧几里得几何形状的限制方法。我们介绍了Flatvi，这是一个新颖的训练框架，将离散易变的变异自动编码器的潜在型号定期为Euclidean几何形状，专门针对单细胞计数数据进行建模。通过鼓励潜在空间中的直线在解码的单细胞歧管上近似地球插值，FlatVI可以增强与假定欧几里得潜在几何形状的下游方法相兼容。关于合成数据的实验支持我们方法的理论声音，而在时间分辨的单细胞RNA测序数据中的应用显示了改善的轨迹重建和歧管插值。</li>
</ul>

<h3>Title: SynCoGen: Synthesizable 3D Molecule Generation via Joint Reaction and Coordinate Modeling</h3>
<ul>
<li><strong>Authors: </strong>Andrei Rekesh, Miruna Cretu, Dmytro Shevchuk, Vignesh Ram Somnath, Pietro Liò, Robert A. Batey, Mike Tyers, Michał Koziarski, Cheng-Hao Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11818">https://arxiv.org/abs/2507.11818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11818">https://arxiv.org/pdf/2507.11818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11818]] SynCoGen: Synthesizable 3D Molecule Generation via Joint Reaction and Coordinate Modeling(https://arxiv.org/abs/2507.11818)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Ensuring synthesizability in generative small molecule design remains a major challenge. While recent developments in synthesizable molecule generation have demonstrated promising results, these efforts have been largely confined to 2D molecular graph representations, limiting the ability to perform geometry-based conditional generation. In this work, we present SynCoGen (Synthesizable Co-Generation), a single framework that combines simultaneous masked graph diffusion and flow matching for synthesizable 3D molecule generation. SynCoGen samples from the joint distribution of molecular building blocks, chemical reactions, and atomic coordinates. To train the model, we curated SynSpace, a dataset containing over 600K synthesis-aware building block graphs and 3.3M conformers. SynCoGen achieves state-of-the-art performance in unconditional small molecule graph and conformer generation, and the model delivers competitive performance in zero-shot molecular linker design for protein ligand generation in drug discovery. Overall, this multimodal formulation represents a foundation for future applications enabled by non-autoregressive molecular generation, including analog expansion, lead optimization, and direct structure conditioning.</li>
<li><strong>摘要：</strong>确保生成小分子设计中的合成性仍然是一个重大挑战。尽管可合综合分子产生的最新发展已显示出令人鼓舞的结果，但这些努力在很大程度上限于2D分子图表示，从而限制了执行基于几何条件产生的能力。在这项工作中，我们提出了Syncogen（可合成的共同生成），该框架结合了同时掩盖的图形扩散和可合成3D分子生成的流动匹配。从分子构建块，化学反应和原子坐标的联合分布中的Syncogen样品。为了训练模型，我们策划了Synspace，该数据集包含超过600K合成的构建块图和330万构象异构体。 Syncogen在无条件的小分子图和构象生成中实现了最先进的性能，该模型在零发分子接头设计中提供了竞争性能，以用于药物发现中的蛋白质配体生成。总体而言，这种多模式公式代表了非自动回归分子产生实现的未来应用的基础，包括模拟扩展，铅优化和直接结构调节。</li>
</ul>

<h3>Title: MNIST-Gen: A Modular MNIST-Style Dataset Generation Using Hierarchical Semantics, Reinforcement Learning, and Category Theory</h3>
<ul>
<li><strong>Authors: </strong>Pouya Shaeri, Arash Karimi, Ariane Middel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11821">https://arxiv.org/abs/2507.11821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11821">https://arxiv.org/pdf/2507.11821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11821]] MNIST-Gen: A Modular MNIST-Style Dataset Generation Using Hierarchical Semantics, Reinforcement Learning, and Category Theory(https://arxiv.org/abs/2507.11821)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Neural networks are often benchmarked using standard datasets such as MNIST, FashionMNIST, or other variants of MNIST, which, while accessible, are limited to generic classes such as digits or clothing items. For researchers working on domain-specific tasks, such as classifying trees, food items, or other real-world objects, these data sets are insufficient and irrelevant. Additionally, creating and publishing a custom dataset can be time consuming, legally constrained, or beyond the scope of individual projects. We present MNIST-Gen, an automated, modular, and adaptive framework for generating MNIST-style image datasets tailored to user-specified categories using hierarchical semantic categorization. The system combines CLIP-based semantic understanding with reinforcement learning and human feedback to achieve intelligent categorization with minimal manual intervention. Our hierarchical approach supports complex category structures with semantic characteristics, enabling fine-grained subcategorization and multiple processing modes: individual review for maximum control, smart batch processing for large datasets, and fast batch processing for rapid creation. Inspired by category theory, MNIST-Gen models each data transformation stage as a composable morphism, enhancing clarity, modularity, and extensibility. As proof of concept, we generate and benchmark two novel datasets-\textit{Tree-MNIST} and \textit{Food-MNIST}-demonstrating MNIST-Gen's utility for producing task-specific evaluation data while achieving 85\% automatic categorization accuracy and 80\% time savings compared to manual approaches.</li>
<li><strong>摘要：</strong>神经网络通常使用标准数据集（例如MNIST，FashionMnist或其他MNIST变体）进行基准测试，虽然可以访问，但仅限于通用类，例如数字或服装项目。对于从事特定领域任务的研究人员，例如对树木，食品或其他现实世界进行分类，这些数据集不足和无关紧要。此外，创建和发布自定义数据集可能会耗时，法律限制或超越单个项目的范围。我们提出了MNIST-GEN，这是一种自动化，模块化和自适应框架，用于生成使用分层语义分类为用户指定类别量身定制的MNIST式图像数据集。该系统将基于剪辑的语义理解与强化学习和人类反馈结合在一起，以最少的手动干预来实现智能分类。我们的层次结构方法支持具有语义特征的复杂类别结构，实现了细粒度的子分类和多个处理模式：最大程度地控制的个人审查，大型数据集的智能批处理处理以及快速创建的快速批处理处理。受类别理论的启发，MNIST-GEN将每个数据转换阶段建模为一个可组合的形态，增强了清晰度，模块化和可扩展性。作为概念证明，我们生成和基准测试了两个新颖的数据集 -  \ textit {tree-mnist}和\ textit {food-mnist}  - 示出了MNIST-GEN的实用性，用于生成特定于任务的评估数据，同时实现85 \％自动分类准确性和80 \％的时间来节省时间，并且相比之下。</li>
</ul>

<h3>Title: CompressedVQA-HDR: Generalized Full-reference and No-reference Quality Assessment Models for Compressed High Dynamic Range Videos</h3>
<ul>
<li><strong>Authors: </strong>Wei Sun, Linhan Cao, Kang Fu, Dandan Zhu, Jun Jia, Menghan Hu, Xiongkuo Min, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11900">https://arxiv.org/abs/2507.11900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11900">https://arxiv.org/pdf/2507.11900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11900]] CompressedVQA-HDR: Generalized Full-reference and No-reference Quality Assessment Models for Compressed High Dynamic Range Videos(https://arxiv.org/abs/2507.11900)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Video compression is a standard procedure applied to all videos to minimize storage and transmission demands while preserving visual quality as much as possible. Therefore, evaluating the visual quality of compressed videos is crucial for guiding the practical usage and further development of video compression algorithms. Although numerous compressed video quality assessment (VQA) methods have been proposed, they often lack the generalization capability needed to handle the increasing diversity of video types, particularly high dynamic range (HDR) content. In this paper, we introduce CompressedVQA-HDR, an effective VQA framework designed to address the challenges of HDR video quality assessment. Specifically, we adopt the Swin Transformer and SigLip 2 as the backbone networks for the proposed full-reference (FR) and no-reference (NR) VQA models, respectively. For the FR model, we compute deep structural and textural similarities between reference and distorted frames using intermediate-layer features extracted from the Swin Transformer as its quality-aware feature representation. For the NR model, we extract the global mean of the final-layer feature maps from SigLip 2 as its quality-aware representation. To mitigate the issue of limited HDR training data, we pre-train the FR model on a large-scale standard dynamic range (SDR) VQA dataset and fine-tune it on the HDRSDR-VQA dataset. For the NR model, we employ an iterative mixed-dataset training strategy across multiple compressed VQA datasets, followed by fine-tuning on the HDRSDR-VQA dataset. Experimental results show that our models achieve state-of-the-art performance compared to existing FR and NR VQA models. Moreover, CompressedVQA-HDR-FR won first place in the FR track of the Generalizable HDR & SDR Video Quality Measurement Grand Challenge at IEEE ICME 2025. The code is available at this https URL.</li>
<li><strong>摘要：</strong>视频压缩是应用于所有视频的标准过程，以最大程度地减少存储和传输需求，同时尽可能保留视觉质量。因此，评估压缩视频的视觉质量对于指导视频压缩算法的实际使用和进一步开发至关重要。尽管已经提出了许多压缩视频质量评估（VQA）方法，但它们通常缺乏处理视频类型不断增长（尤其是高动态范围（HDR）内容）所需的概括能力。在本文中，我们引入了CompressedVQA-HDR，这是一个有效的VQA框架，旨在应对HDR视频质量评估的挑战。具体而言，我们采用Swin Transformer和Siglip 2作为拟议的全参考（FR）和No-Reference（NR）VQA模型的骨干网络。对于FR模型，我们使用从Swin Transformer中提取的中间层特征来计算参考和变形框架之间的深层结构和纹理相似性，作为其质量感知的特征表示。对于NR模型，我们从Siglip 2中提取最终特征图的全局平均值作为其质量感知的表示。为了减轻HDR培训数据有限的问题，我们将FR模型预先培训，以大规模的标准动态范围（SDR）VQA数据集对HDRSDR-VQA数据集进行微调。对于NR模型，我们在多个压缩VQA数据集中采用迭代混合数据库训练策略，然后在HDRSDR-VQA数据集上进行微调。实验结果表明，与现有的FR和NR VQA模型相比，我们的模型实现了最先进的性能。此外，CompressedVQA-HDR-FR在IEEE ICME 2025的可推广的HDR＆SDR视频质量测量大挑战的FR轨道上获得了第一名。该代码可在此HTTPS URL上获得。</li>
</ul>

<h3>Title: From Generative to Episodic: Sample-Efficient Replicable Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Max Hopkins, Sihan Liu, Christopher Ye, Yuichi Yoshida</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11926">https://arxiv.org/abs/2507.11926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11926">https://arxiv.org/pdf/2507.11926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11926]] From Generative to Episodic: Sample-Efficient Replicable Reinforcement Learning(https://arxiv.org/abs/2507.11926)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The epidemic failure of replicability across empirical science and machine learning has recently motivated the formal study of replicable learning algorithms [Impagliazzo et al. (2022)]. In batch settings where data comes from a fixed i.i.d. source (e.g., hypothesis testing, supervised learning), the design of data-efficient replicable algorithms is now more or less understood. In contrast, there remain significant gaps in our knowledge for control settings like reinforcement learning where an agent must interact directly with a shifting environment. Karbasi et. al show that with access to a generative model of an environment with $S$ states and $A$ actions (the RL 'batch setting'), replicably learning a near-optimal policy costs only $\tilde{O}(S^2A^2)$ samples. On the other hand, the best upper bound without a generative model jumps to $\tilde{O}(S^7 A^7)$ [Eaton et al. (2024)] due to the substantial difficulty of environment exploration. This gap raises a key question in the broader theory of replicability: Is replicable exploration inherently more expensive than batch learning? Is sample-efficient replicable RL even possible? In this work, we (nearly) resolve this problem (for low-horizon tabular MDPs): exploration is not a significant barrier to replicable learning! Our main result is a replicable RL algorithm on $\tilde{O}(S^2A)$ samples, bridging the gap between the generative and episodic settings. We complement this with a matching $\tilde{\Omega}(S^2A)$ lower bound in the generative setting (under the common parallel sampling assumption) and an unconditional lower bound in the episodic setting of $\tilde{\Omega}(S^2)$ showcasing the near-optimality of our algorithm with respect to the state space $S$.</li>
<li><strong>摘要：</strong>跨经验科学和机器学习的可复制性的流行失败最近激发了可复制学习算法的正式研究[Impagliazzo等。 （2022）]。在数据来自固定的I.I.D.的批处理设置中。来源（例如，假设检验，监督学习），现在或多或少地理解了数据有效可复制算法的设计。相比之下，我们的知识中对于控制设置（例如加强学习），代理必须直接与变化的环境互动。 Karbasi等。 Al表明，通过访问具有$ s $状态和$ a $ ACTION（RL“批处理设置”）环境的生成模型，可复制地学习近乎最佳的政策仅费用$ \ tilde {o} {o}（s^2a^2）$样本。另一方面，没有生成模型的最佳上限跳到$ \ tilde {o}（s^7 a^7）$ [Eaton等。 （2024）]由于环境勘探的实质困难。这个差距在更广泛的可复制性理论中提出了一个关键问题：可复制的探索本质上比批处理学习更昂贵？样品有效的可复制RL甚至可能吗？在这项工作中，我们（几乎）解决此问题（对于低摩根表格MDP）：探索不是可复制学习的重要障碍！我们的主要结果是在$ \ tilde {o}（s^2a）$样品上可复制的RL算法，弥合了生成和情节设置之间的差距。我们以匹配的$ \ tilde {\ omega}（s^2a）$在生成设置（在常见的并行采样假设下）和无条件的下限在$ \ tilde {\ tilde {\ omega}（s^2）$ apoptimality $ aptimality congorith的情况下，对此进行补充。</li>
</ul>

<h3>Title: RaDL: Relation-aware Disentangled Learning for Multi-Instance Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Geon Park, Seon Bin Kim, Gunho Jung, Seong-Whan Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11947">https://arxiv.org/abs/2507.11947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11947">https://arxiv.org/pdf/2507.11947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11947]] RaDL: Relation-aware Disentangled Learning for Multi-Instance Text-to-Image Generation(https://arxiv.org/abs/2507.11947)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>With recent advancements in text-to-image (T2I) models, effectively generating multiple instances within a single image prompt has become a crucial challenge. Existing methods, while successful in generating positions of individual instances, often struggle to account for relationship discrepancy and multiple attributes leakage. To address these limitations, this paper proposes the relation-aware disentangled learning (RaDL) framework. RaDL enhances instance-specific attributes through learnable parameters and generates relation-aware image features via Relation Attention, utilizing action verbs extracted from the global prompt. Through extensive evaluations on benchmarks such as COCO-Position, COCO-MIG, and DrawBench, we demonstrate that RaDL outperforms existing methods, showing significant improvements in positional accuracy, multiple attributes consideration, and the relationships between instances. Our results present RaDL as the solution for generating images that consider both the relationships and multiple attributes of each instance within the multi-instance image.</li>
<li><strong>摘要：</strong>随着文本到图像（T2I）模型的最新进展，在单个图像提示中有效生成多个实例已成为一个至关重要的挑战。现有的方法虽然成功地生成各个实例的位置，但通常很难说明关系差异和多个属性泄漏。为了解决这些局限性，本文提出了与之相关的解剖学习（RADL）框架。 RADL通过可学习的参数增强了实例特定的属性，并通过关注关系来生成关系感知的图像特征，利用从全局提示中提取的动作动词。通过对Coco-Posion，Coco-Mig和Drawbench等基准的广泛评估，我们证明RADL胜过现有方法，显示出位置准确性，多个属性考虑以及实例之间的关系的显着提高。我们的结果将RADL作为生成图像的解决方案，以考虑多个实体图像中每个实例的关系和多个属性。</li>
</ul>

<h3>Title: Kevin: Multi-Turn RL for Generating CUDA Kernels</h3>
<ul>
<li><strong>Authors: </strong>Carlo Baronio, Pietro Marsella, Ben Pan, Simon Guo, Silas Alberti</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.PF, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11948">https://arxiv.org/abs/2507.11948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11948">https://arxiv.org/pdf/2507.11948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11948]] Kevin: Multi-Turn RL for Generating CUDA Kernels(https://arxiv.org/abs/2507.11948)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Writing GPU kernels is a challenging task and critical for AI systems' efficiency. It is also highly iterative: domain experts write code and improve performance through execution feedback. Moreover, it presents verifiable rewards like correctness and speedup, making it a natural environment to apply Reinforcement Learning (RL). To explicitly incorporate the iterative nature of this process into training, we develop a flexible multi-turn RL recipe that addresses unique challenges encountered in real-world settings, such as learning from long trajectories and effective reward attribution across turns. We present Kevin - K(ernel D)evin, the first model trained with multi-turn RL for CUDA kernel generation and optimization. In our evaluation setup, Kevin shows significant gains over its base model (QwQ-32B), improving correctness of generated kernels (in pure CUDA) from 56% to 82% and mean speedup from 0.53x to 1.10x of baseline (PyTorch Eager), and surpassing frontier models like o4-mini (0.78x). Finally, we study its behavior across test-time scaling axes: we found scaling serial refinement more beneficial than parallel sampling. In particular, when given more refinement turns, Kevin shows a higher rate of improvement.</li>
<li><strong>摘要：</strong>编写GPU内核是一项具有挑战性的任务，对于AI系统的效率至关重要。它也是高度迭代的：域专家编写代码并通过执行反馈提高性能。此外，它提供了可验证的奖励，例如正确性和加速性，使其成为应用强化学习（RL）的自然环境。为了将该过程的迭代性质明确融合到培训中，我们开发了一种灵活的多转弯RL配方，该配方解决了现实世界中遇到的独特挑战，例如从长途轨迹学习和跨回合的有效奖励归因。我们提出了Kevin -K（Ernel d）Evin，这是第一个经过多转弯RL训练CUDA内核生成和优化的模型。在我们的评估设置中，凯文（Kevin）在其基本模型（QWQ-32B）上显示出显着的收益，将产生的内核（纯CUDA）的正确性从56％提高到82％，并平均速度从0.53 x基线（pytorch狂热）（Pytorch急需）（Pytorch急需）（Pytorch急需），并超越了O4-Mini（0.78 x）。最后，我们研究了其跨测试时间缩放轴的行为：我们发现缩放序列完善比平行采样更有益。特别是，当给予更多的改进时，凯文会显示出更高的改进率。</li>
</ul>

<h3>Title: EC-Diff: Fast and High-Quality Edge-Cloud Collaborative Inference for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Jiajian Xie, Shengyu Zhang, Zhou Zhao, Fan Wu, Fei Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11980">https://arxiv.org/abs/2507.11980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11980">https://arxiv.org/pdf/2507.11980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11980]] EC-Diff: Fast and High-Quality Edge-Cloud Collaborative Inference for Diffusion Models(https://arxiv.org/abs/2507.11980)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion Models have shown remarkable proficiency in image and video synthesis. As model size and latency increase limit user experience, hybrid edge-cloud collaborative framework was recently proposed to realize fast inference and high-quality generation, where the cloud model initiates high-quality semantic planning and the edge model expedites later-stage refinement. However, excessive cloud denoising prolongs inference time, while insufficient steps cause semantic ambiguity, leading to inconsistency in edge model output. To address these challenges, we propose EC-Diff that accelerates cloud inference through gradient-based noise estimation while identifying the optimal point for cloud-edge handoff to maintain generation quality. Specifically, we design a K-step noise approximation strategy to reduce cloud inference frequency by using noise gradients between steps and applying cloud inference periodically to adjust errors. Then we design a two-stage greedy search algorithm to efficiently find the optimal parameters for noise approximation and edge model switching. Extensive experiments demonstrate that our method significantly enhances generation quality compared to edge inference, while achieving up to an average $2\times$ speedup in inference compared to cloud inference. Video samples and source code are available at this https URL.</li>
<li><strong>摘要：</strong>扩散模型已经表现出非常熟练的图像和视频合成。随着模型大小和潜伏期的增加限制用户体验，最近提议混合边缘云协作框架实现快速推理和高质量的生成，云模型启动了高质量的语义规划，而边缘模型加快了后期的细化。但是，过多的云降解会延长推理时间，而步骤不足会导致语义歧义，从而导致边缘模型输出不一致。为了应对这些挑战，我们提出EC-DIFF，通过基于梯度的噪声估计来加速云推断，同时确定云边缘交接的最佳点以保持发电质量。具体而言，我们设计了K-Step噪声近似策略，以通过在步骤之间使用噪声梯度并定期应用云推理来调整错误，以降低云推理频率。然后，我们设计了一个两阶段的贪婪搜索算法，以有效地找到噪声近似和边缘模型切换的最佳参数。广泛的实验表明，与边缘推理相比，我们的方法显着提高了发电质量，而与云推断相比，推理的平均$ 2 \ times $速度。视频样本和源代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Unsupervised Part Discovery via Descriptor-Based Masked Image Restoration with Optimized Constraints</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Xia, Yike Wu, Wenjian Huang, Jianguo Zhang, Jian Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11985">https://arxiv.org/abs/2507.11985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11985">https://arxiv.org/pdf/2507.11985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11985]] Unsupervised Part Discovery via Descriptor-Based Masked Image Restoration with Optimized Constraints(https://arxiv.org/abs/2507.11985)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Part-level features are crucial for image understanding, but few studies focus on them because of the lack of fine-grained labels. Although unsupervised part discovery can eliminate the reliance on labels, most of them cannot maintain robustness across various categories and scenarios, which restricts their application range. To overcome this limitation, we present a more effective paradigm for unsupervised part discovery, named Masked Part Autoencoder (MPAE). It first learns part descriptors as well as a feature map from the inputs and produces patch features from a masked version of the original images. Then, the masked regions are filled with the learned part descriptors based on the similarity between the local features and descriptors. By restoring these masked patches using the part descriptors, they become better aligned with their part shapes, guided by appearance features from unmasked patches. Finally, MPAE robustly discovers meaningful parts that closely match the actual object shapes, even in complex scenarios. Moreover, several looser yet more effective constraints are proposed to enable MPAE to identify the presence of parts across various scenarios and categories in an unsupervised manner. This provides the foundation for addressing challenges posed by occlusion and for exploring part similarity across multiple categories. Extensive experiments demonstrate that our method robustly discovers meaningful parts across various categories and scenarios. The code is available at the project this https URL.</li>
<li><strong>摘要：</strong>零件级的特征对于图像理解至关重要，但是由于缺乏细粒度的标签，很少有研究重点放在它们上。尽管无监督的部分发现可以消除对标签的依赖，但大多数人无法在各种类别和场景上保持稳健性，从而限制了其应用程序范围。为了克服这一局限性，我们提出了一个更有效的范式，用于无监督的部分发现，称为蒙面零件自动编码器（MPAE）。它首先从输入中学习零件描述符以及功能映射，并从原始图像的蒙版版本中产生补丁功能。然后，根据本地特征和描述符之间的相似性，蒙面区域中充满了学习的零件描述符。通过使用零件描述符恢复这些蒙版贴片，它们可以更好地与零件形状保持一致，并在未掩盖的贴片的外观特征的指导下。最后，MPAE坚强地发现有意义的部分，即使在复杂的情况下，也可以与实际对象形状紧密匹配。此外，提出了一些更宽松而更有效的约束，以使MPAE以无监督的方式在各种情况和类别中识别零件的存在。这为解决遮挡带来的挑战和探索多个类别的零件相似性奠定了基础。广泛的实验表明，我们的方法强烈地发现了各种类别和场景的有意义的部分。该代码可在此https URL的项目中可用。</li>
</ul>

<h3>Title: ID-EA: Identity-driven Text Enhancement and Adaptation with Textual Inversion for Personalized Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Hyun-Jun Jin, Young-Eun Kim, Seong-Whan Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11990">https://arxiv.org/abs/2507.11990</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11990">https://arxiv.org/pdf/2507.11990</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11990]] ID-EA: Identity-driven Text Enhancement and Adaptation with Textual Inversion for Personalized Text-to-Image Generation(https://arxiv.org/abs/2507.11990)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recently, personalized portrait generation with a text-to-image diffusion model has significantly advanced with Textual Inversion, emerging as a promising approach for creating high-fidelity personalized images. Despite its potential, current Textual Inversion methods struggle to maintain consistent facial identity due to semantic misalignments between textual and visual embedding spaces regarding identity. We introduce ID-EA, a novel framework that guides text embeddings to align with visual identity embeddings, thereby improving identity preservation in a personalized generation. ID-EA comprises two key components: the ID-driven Enhancer (ID-Enhancer) and the ID-conditioned Adapter (ID-Adapter). First, the ID-Enhancer integrates identity embeddings with a textual ID anchor, refining visual identity embeddings derived from a face recognition model using representative text embeddings. Then, the ID-Adapter leverages the identity-enhanced embedding to adapt the text condition, ensuring identity preservation by adjusting the cross-attention module in the pre-trained UNet model. This process encourages the text features to find the most related visual clues across the foreground snippets. Extensive quantitative and qualitative evaluations demonstrate that ID-EA substantially outperforms state-of-the-art methods in identity preservation metrics while achieving remarkable computational efficiency, generating personalized portraits approximately 15 times faster than existing approaches.</li>
<li><strong>摘要：</strong>最近，通过文本反演，具有文本形象扩散模型的个性化肖像生成已经显着提高，这是创建高保真个性化图像的有前途的方法。尽管具有潜力，但由于文本和视觉嵌入空间在身份方面的语义不对对准，当前的文本反演方法努力保持一致的面部身份。我们介绍了ID-EA，这是一个新颖的框架，可以指导文本嵌入以与视觉识别嵌入保持一致，从而改善个性化一代中的身份保存。 ID-EA包括两个关键组件：ID驱动的增强器（ID-Enhancer）和ID条件适配器（ID-ADAPTER）。首先，ID-Enhancer将身份嵌入与文本ID锚集成，并使用代表性文本嵌入式源自面部识别模型得出的视觉标识嵌入。然后，ID适配器利用身份增强的嵌入来调整文本条件，从而通过调整预训练的UNET模型中的交叉意见模块来确保身份保存。此过程鼓励文本功能找到前景片段上最相关的视觉线索。广泛的定量和定性评估表明，ID-EA在身份保存指标中的最先进方法基本上优于最先进的方法，同时达到了出色的计算效率，从而产生了个性化的肖像，比现有方法快15倍。</li>
</ul>

<h3>Title: SAMST: A Transformer framework based on SAM pseudo label filtering for remote sensing semi-supervised semantic segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jun Yin, Fei Wu, Yupeng Ren, Jisheng Huang, Qiankun Li, Heng jin, Jianhai Fu, Chanjie Cui</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.11994">https://arxiv.org/abs/2507.11994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.11994">https://arxiv.org/pdf/2507.11994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.11994]] SAMST: A Transformer framework based on SAM pseudo label filtering for remote sensing semi-supervised semantic segmentation(https://arxiv.org/abs/2507.11994)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Public remote sensing datasets often face limitations in universality due to resolution variability and inconsistent land cover category definitions. To harness the vast pool of unlabeled remote sensing data, we propose SAMST, a semi-supervised semantic segmentation method. SAMST leverages the strengths of the Segment Anything Model (SAM) in zero-shot generalization and boundary detection. SAMST iteratively refines pseudo-labels through two main components: supervised model self-training using both labeled and pseudo-labeled data, and a SAM-based Pseudo-label Refiner. The Pseudo-label Refiner comprises three modules: a Threshold Filter Module for preprocessing, a Prompt Generation Module for extracting connected regions and generating prompts for SAM, and a Label Refinement Module for final label stitching. By integrating the generalization power of large models with the training efficiency of small models, SAMST improves pseudo-label accuracy, thereby enhancing overall model performance. Experiments on the Potsdam dataset validate the effectiveness and feasibility of SAMST, demonstrating its potential to address the challenges posed by limited labeled data in remote sensing semantic segmentation.</li>
<li><strong>摘要：</strong>由于解决方案可变性和不一致的土地覆盖类别定义，公共遥感数据集通常会面临普遍性的局限性。为了利用大量未标记的遥感数据，我们提出了Samst，一种半监督的语义分割方法。 SAMST在零弹性概括和边界检测中利用了段的任何模型（SAM）的强度。 SAMST迭代通过两个主要组成部分来完善伪标记：使用标记和伪标记数据的监督模型自我训练，以及基于SAM的伪标签炼油厂。伪标签炼油厂包括三个模块：用于预处理的阈值滤波器模块，用于提取连接区域的及时生成模块和为SAM生成提示，以及用于最终标签缝制的标签细化模块。通过将大型模型的概括功率与小型模型的训练效率相结合，SAMST提高了伪标签的精度，从而提高了整体模型性能。 POTSDAM数据集的实验验证了SAMST的有效性和可行性，证明了其潜力解决遥感语义分割中有限标记的数据所带来的挑战。</li>
</ul>

<h3>Title: 3D-MoRe: Unified Modal-Contextual Reasoning for Embodied Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Rongtao Xu, Han Gao, Mingming Yu, Dong An, Shunpeng Chen, Changwei Wang, Li Guo, Xiaodan Liang, Shibiao Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.12026">https://arxiv.org/abs/2507.12026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.12026">https://arxiv.org/pdf/2507.12026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.12026]] 3D-MoRe: Unified Modal-Contextual Reasoning for Embodied Question Answering(https://arxiv.org/abs/2507.12026)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>With the growing need for diverse and scalable data in indoor scene tasks, such as question answering and dense captioning, we propose 3D-MoRe, a novel paradigm designed to generate large-scale 3D-language datasets by leveraging the strengths of foundational models. The framework integrates key components, including multi-modal embedding, cross-modal interaction, and a language model decoder, to process natural language instructions and 3D scene data. This approach facilitates enhanced reasoning and response generation in complex 3D environments. Using the ScanNet 3D scene dataset, along with text annotations from ScanQA and ScanRefer, 3D-MoRe generates 62,000 question-answer (QA) pairs and 73,000 object descriptions across 1,513 scenes. We also employ various data augmentation techniques and implement semantic filtering to ensure high-quality data. Experiments on ScanQA demonstrate that 3D-MoRe significantly outperforms state-of-the-art baselines, with the CIDEr score improving by 2.15\%. Similarly, on ScanRefer, our approach achieves a notable increase in CIDEr@0.5 by 1.84\%, highlighting its effectiveness in both tasks. Our code and generated datasets will be publicly released to benefit the community, and both can be accessed on the this https URL.</li>
<li><strong>摘要：</strong>随着室内场景任务中对各种可扩展数据的需求日益增长，例如问答和密集的字幕，我们提出了3D-More，这是一种新颖的范式，旨在通过利用基础模型的优势来生成大型3D语言数据集。该框架集成了关键组件，包括多模式嵌入，跨模式交互和语言模型解码器，以处理自然语言指令和3D场景数据。这种方法促进了在复杂的3D环境中增强的推理和响应产生。使用Scannet 3D场景数据集，以及Scanqa和ScanRefer的文本注释，3D-More在1,513场场景中生成了62,000个问题解答（QA）对（QA）对（QA）对（QA）和73,000个对象描述。我们还采用了各种数据增强技术并实施语义过滤以确保高质量的数据。 Scanqa的实验表明，3D-More的表现明显优于最先进的基线，苹果酒得分提高了2.15 \％。同样，在ScanRefer上，我们的方法在Cider@0.5升高1.84 \％，突出了其在这两个任务中的有效性。我们的代码和生成的数据集将公开发布以使社区受益，并且可以在此HTTPS URL上访问。</li>
</ul>

<h3>Title: FloGAN: Scenario-Based Urban Mobility Flow Generation via Conditional GANs and Dynamic Region Decoupling</h3>
<ul>
<li><strong>Authors: </strong>Seanglidet Yean, Jiazu Zhou, Bu-Sung Lee, Markus Schläpfer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.12053">https://arxiv.org/abs/2507.12053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.12053">https://arxiv.org/pdf/2507.12053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.12053]] FloGAN: Scenario-Based Urban Mobility Flow Generation via Conditional GANs and Dynamic Region Decoupling(https://arxiv.org/abs/2507.12053)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The mobility patterns of people in cities evolve alongside changes in land use and population. This makes it crucial for urban planners to simulate and analyze human mobility patterns for purposes such as transportation optimization and sustainable urban development. Existing generative models borrowed from machine learning rely heavily on historical trajectories and often overlook evolving factors like changes in population density and land use. Mechanistic approaches incorporate population density and facility distribution but assume static scenarios, limiting their utility for future projections where historical data for calibration is unavailable. This study introduces a novel, data-driven approach for generating origin-destination mobility flows tailored to simulated urban scenarios. Our method leverages adaptive factors such as dynamic region sizes and land use archetypes, and it utilizes conditional generative adversarial networks (cGANs) to blend historical data with these adaptive parameters. The approach facilitates rapid mobility flow generation with adjustable spatial granularity based on regions of interest, without requiring extensive calibration data or complex behavior modeling. The promising performance of our approach is demonstrated by its application to mobile phone data from Singapore, and by its comparison with existing methods.</li>
<li><strong>摘要：</strong>城市人民的流动方式随着土地使用和人口的变化而发展。这对于城市规划师来说，为诸如运输优化和可持续城市发展等目的模拟和分析人类流动性模式至关重要。从机器学习中借来的现有生成模型在很大程度上取决于历史轨迹，并且通常忽略了不断发展的因素，例如人口密度和土地使用的变化。机械方法结合了人口密度和设施分布，但采用静态场景，限制了其效用以在未来的预测中进行校准的历史数据。这项研究介绍了一种新型的，数据驱动的方法，用于生成针对模拟城市场景量身定制的来源用途动机流。我们的方法利用自适应因素，例如动态区域大小和土地使用原型，并利用条件生成的对抗网络（CGAN）将历史数据与这些自适应参数融合在一起。该方法促进了基于感兴趣区域的可调节空间粒度的快速流动流，而无需大量的校准数据或复杂的行为建模。我们的方法的有希望的表现通过了新加坡的移动电话数据以及与现有方法的比较来证明。</li>
</ul>

<h3>Title: MS-DETR: Towards Effective Video Moment Retrieval and Highlight Detection by Joint Motion-Semantic Learning</h3>
<ul>
<li><strong>Authors: </strong>Hongxu Ma, Guanshuo Wang, Fufu Yu, Qiong Jia, Shouhong Ding</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.12062">https://arxiv.org/abs/2507.12062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.12062">https://arxiv.org/pdf/2507.12062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.12062]] MS-DETR: Towards Effective Video Moment Retrieval and Highlight Detection by Joint Motion-Semantic Learning(https://arxiv.org/abs/2507.12062)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Video Moment Retrieval (MR) and Highlight Detection (HD) aim to pinpoint specific moments and assess clip-wise relevance based on the text query. While DETR-based joint frameworks have made significant strides, there remains untapped potential in harnessing the intricate relationships between temporal motion and spatial semantics within video content. In this paper, we propose the Motion-Semantics DETR (MS-DETR), a framework that captures rich motion-semantics features through unified learning for MR/HD tasks. The encoder first explicitly models disentangled intra-modal correlations within motion and semantics dimensions, guided by the given text queries. Subsequently, the decoder utilizes the task-wise correlation across temporal motion and spatial semantics dimensions to enable precise query-guided localization for MR and refined highlight boundary delineation for HD. Furthermore, we observe the inherent sparsity dilemma within the motion and semantics dimensions of MR/HD datasets. To address this issue, we enrich the corpus from both dimensions by generation strategies and propose contrastive denoising learning to ensure the above components learn robustly and effectively. Extensive experiments on four MR/HD benchmarks demonstrate that our method outperforms existing state-of-the-art models by a margin. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>视频力矩检索（MR）并突出显示检测（HD）旨在根据文本查询来查明特定的时刻并评估剪辑的相关性。尽管基于DITR的联合框架取得了长足的进步，但在利用视频内容中的时间运动与空间语义之间的复杂关系方面仍未开发潜力。在本文中，我们提出了运动 - 仪式DETR（MS-DETR），该框架通过MR/HD任务的统一学习来捕获丰富的运动听觉功能。编码器首先明确模拟运动和语义维度内的模式内相关性，并在给定文本查询的指导下。随后，解码器利用了时间运动和空间语义尺寸的任务相关性，以实现MR的精确查询引导的定位，并为HD提供了高清的亮点边界描述。此外，我们观察到MR/HD数据集的运动和语义尺寸内的固有稀疏难题。为了解决这个问题，我们通过一代策略从两个维度中丰富了语料库，并提出了对比性的学习学习，以确保上述成分富有和有效地学习。对四个MR/HD基准测试的广泛实验表明，我们的方法以边距优于现有的最新模型。我们的代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Foresight in Motion: Reinforcing Trajectory Prediction with Reward Heuristics</h3>
<ul>
<li><strong>Authors: </strong>Muleilan Pei, Shaoshuai Shi, Xuesong Chen, Xu Liu, Shaojie Shen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.12083">https://arxiv.org/abs/2507.12083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.12083">https://arxiv.org/pdf/2507.12083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.12083]] Foresight in Motion: Reinforcing Trajectory Prediction with Reward Heuristics(https://arxiv.org/abs/2507.12083)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Motion forecasting for on-road traffic agents presents both a significant challenge and a critical necessity for ensuring safety in autonomous driving systems. In contrast to most existing data-driven approaches that directly predict future trajectories, we rethink this task from a planning perspective, advocating a "First Reasoning, Then Forecasting" strategy that explicitly incorporates behavior intentions as spatial guidance for trajectory prediction. To achieve this, we introduce an interpretable, reward-driven intention reasoner grounded in a novel query-centric Inverse Reinforcement Learning (IRL) scheme. Our method first encodes traffic agents and scene elements into a unified vectorized representation, then aggregates contextual features through a query-centric paradigm. This enables the derivation of a reward distribution, a compact yet informative representation of the target agent's behavior within the given scene context via IRL. Guided by this reward heuristic, we perform policy rollouts to reason about multiple plausible intentions, providing valuable priors for subsequent trajectory generation. Finally, we develop a hierarchical DETR-like decoder integrated with bidirectional selective state space models to produce accurate future trajectories along with their associated probabilities. Extensive experiments on the large-scale Argoverse and nuScenes motion forecasting datasets demonstrate that our approach significantly enhances trajectory prediction confidence, achieving highly competitive performance relative to state-of-the-art methods.</li>
<li><strong>摘要：</strong>公路交通代理商的运动预测既提出了一个重大挑战，也是确保自动驾驶系统安全的重要必要性。与直接预测未来轨迹的大多数现有数据驱动的方法相反，我们从计划的角度重新考虑了这项任务，主张“首先推理，然后预测”策略，该策略将行为意图明确地纳入了轨迹预测的空间指导。为了实现这一目标，我们引入了一个以新颖的以查询为中心的逆增强学习（IRL）方案为基础的可解释的，奖励驱动的意图推理者。我们的方法首先将流量代理和场景元素编码为统一的矢量化表示形式，然后通过以查询为中心的范式汇总上下文特征。这使得奖励分布的推导，这是通过IRL在给定场景上下文中对目标代理行为的紧凑而有益的表示。在这种奖励启发式方面的指导下，我们执行政策推广，以推理多种合理的意图，为随后的轨迹生成提供了宝贵的先验。最后，我们开发了与双向选择性状态空间模型集成的分层detr样解码器，以产生准确的将来的轨迹及其相关概率。大规模argoverse和Nuscenes运动预测数据集的广泛实验表明，我们的方法显着增强了轨迹预测的置信度，相对于最新方法，实现了高度竞争性的性能。</li>
</ul>

<h3>Title: DeepShade: Enable Shade Simulation by Text-conditioned Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Longchao Da, Xiangrui Liu, Mithun Shivakoti, Thirulogasankar Pranav Kutralingam, Yezhou Yang, Hua Wei</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.12103">https://arxiv.org/abs/2507.12103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.12103">https://arxiv.org/pdf/2507.12103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.12103]] DeepShade: Enable Shade Simulation by Text-conditioned Image Generation(https://arxiv.org/abs/2507.12103)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Heatwaves pose a significant threat to public health, especially as global warming intensifies. However, current routing systems (e.g., online maps) fail to incorporate shade information due to the difficulty of estimating shades directly from noisy satellite imagery and the limited availability of training data for generative models. In this paper, we address these challenges through two main contributions. First, we build an extensive dataset covering diverse longitude-latitude regions, varying levels of building density, and different urban layouts. Leveraging Blender-based 3D simulations alongside building outlines, we capture building shadows under various solar zenith angles throughout the year and at different times of day. These simulated shadows are aligned with satellite images, providing a rich resource for learning shade patterns. Second, we propose the DeepShade, a diffusion-based model designed to learn and synthesize shade variations over time. It emphasizes the nuance of edge features by jointly considering RGB with the Canny edge layer, and incorporates contrastive learning to capture the temporal change rules of shade. Then, by conditioning on textual descriptions of known conditions (e.g., time of day, solar angles), our framework provides improved performance in generating shade images. We demonstrate the utility of our approach by using our shade predictions to calculate shade ratios for real-world route planning in Tempe, Arizona. We believe this work will benefit society by providing a reference for urban planning in extreme heat weather and its potential practical applications in the environment.</li>
<li><strong>摘要：</strong>热浪对公共卫生构成了重大威胁，尤其是随着全球变暖的加剧。但是，由于难以直接从嘈杂的卫星图像中估算阴影以及生成模型的培训数据的有限可用性，因此当前的路由系统（例如在线地图）无法合并阴影信息。在本文中，我们通过两个主要贡献解决了这些挑战。首先，我们构建了一个广泛的数据集，涵盖了不同的经度纬度区域，建筑物密度的不同水平以及不同的城市布局。利用基于搅拌器的3D模拟在建筑轮廓的一边，我们全年和一天中的不同时间都捕获了各种太阳天顶角下的建筑阴影。这些模拟的阴影与卫星图像对齐，为学习阴影模式提供了丰富的资源。其次，我们提出了DeepShade，这是一个基于扩散的模型，旨在随着时间的推移学习和综合阴影变化。它通过共同考虑使用Canny Edge层的RGB来强调边缘特征的细微差别，并结合了对比度学习以捕获阴影的时间变化规则。然后，通过根据已知条件的文本描述（例如，一天中的时间，太阳角）进行调节，我们的框架在生成阴影图像时提供了改进的性能。我们通过使用我们的阴影预测来计算亚利桑那州坦佩现实世界路线计划的阴影比率来证明我们的方法的实用性。我们认为，这项工作将通过在极端高温天气及其在环境中的潜在实际应用中提供城市规划的参考来使社会受益。</li>
</ul>

<h3>Title: Non-Adaptive Adversarial Face Generation</h3>
<ul>
<li><strong>Authors: </strong>Sunpill Kim, Seunghun Paik, Chanwoo Hwang, Minsu Kim, Jae Hong Seo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.12107">https://arxiv.org/abs/2507.12107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.12107">https://arxiv.org/pdf/2507.12107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.12107]] Non-Adaptive Adversarial Face Generation(https://arxiv.org/abs/2507.12107)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Adversarial attacks on face recognition systems (FRSs) pose serious security and privacy threats, especially when these systems are used for identity verification. In this paper, we propose a novel method for generating adversarial faces-synthetic facial images that are visually distinct yet recognized as a target identity by the FRS. Unlike iterative optimization-based approaches (e.g., gradient descent or other iterative solvers), our method leverages the structural characteristics of the FRS feature space. We figure out that individuals sharing the same attribute (e.g., gender or race) form an attributed subsphere. By utilizing such subspheres, our method achieves both non-adaptiveness and a remarkably small number of queries. This eliminates the need for relying on transferability and open-source surrogate models, which have been a typical strategy when repeated adaptive queries to commercial FRSs are impossible. Despite requiring only a single non-adaptive query consisting of 100 face images, our method achieves a high success rate of over 93% against AWS's CompareFaces API at its default threshold. Furthermore, unlike many existing attacks that perturb a given image, our method can deliberately produce adversarial faces that impersonate the target identity while exhibiting high-level attributes chosen by the adversary.</li>
<li><strong>摘要：</strong>对面部识别系统（FRS）的对抗性攻击构成了严重的安全性和隐私威胁，尤其是当这些系统用于身份验证时。在本文中，我们提出了一种新的方法，用于产生对抗性面部的面部图像，这些面部图像在视觉上截然不同但被FRS识别为目标身份。与基于迭代优化的方法（例如梯度下降或其他迭代求解器）不同，我们的方法利用FRS的结构特征具有空间。我们发现，共享相同属性（例如性别或种族）的个人形成了归因的子球。通过利用此类子球，我们的方法既可以实现非适应性，又达到了少量的查询。这消除了依靠可转让性和开源替代模型的需求，这是不可能重复对商业FRS的自适应查询时的典型策略。尽管仅需要一个非自适应查询，其中包括100张脸部图像，但我们的方法在默认阈值下，与AWS的compareFaces API相比，成功率超过93％。此外，与给定图像扰动的许多现有攻击不同，我们的方法可以故意产生对对抗性的面孔，这些面孔在表现出对手选择的高级属性的同时模仿目标身份。</li>
</ul>

<h3>Title: LidarPainter: One-Step Away From Any Lidar View To Novel Guidance</h3>
<ul>
<li><strong>Authors: </strong>Yuzhou Ji, Ke Ma, Hong Cai, Anchun Zhang, Lizhuang Ma, Xin Tan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.12114">https://arxiv.org/abs/2507.12114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.12114">https://arxiv.org/pdf/2507.12114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.12114]] LidarPainter: One-Step Away From Any Lidar View To Novel Guidance(https://arxiv.org/abs/2507.12114)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Dynamic driving scene reconstruction is of great importance in fields like digital twin system and autonomous driving simulation. However, unacceptable degradation occurs when the view deviates from the input trajectory, leading to corrupted background and vehicle models. To improve reconstruction quality on novel trajectory, existing methods are subject to various limitations including inconsistency, deformation, and time consumption. This paper proposes LidarPainter, a one-step diffusion model that recovers consistent driving views from sparse LiDAR condition and artifact-corrupted renderings in real-time, enabling high-fidelity lane shifts in driving scene reconstruction. Extensive experiments show that LidarPainter outperforms state-of-the-art methods in speed, quality and resource efficiency, specifically 7 x faster than StreetCrafter with only one fifth of GPU memory required. LidarPainter also supports stylized generation using text prompts such as "foggy" and "night", allowing for a diverse expansion of the existing asset library.</li>
<li><strong>摘要：</strong>在数字双胞胎系统和自动驾驶模拟等领域，动态驾驶场景重建非常重要。但是，当视图偏离输入轨迹时，发生不可接受的降解发生，导致背景和车辆模型损坏。为了提高新型轨迹的重建质量，现有方法会受到各种限制，包括不一致，变形和时间消耗。本文提出了Lidarpainter，这是一种单步扩散模型，可实时从稀疏的LiDAR条件和人工腐败的效果图中恢复一致的驾驶视图，从而在驾驶场景重建中实现了高保真车道的变化。广泛的实验表明，LidarPainter的速度，质量和资源效率优于最先进的方法，特别是比StreetCrafter快7 x，只有五分之一的GPU内存。 Lidarpainter还使用“有雾”和“夜”等文本提示来支持风格化的生成，从而可以对现有资产库进行多样化的扩展。</li>
</ul>

<h3>Title: Learning Pixel-adaptive Multi-layer Perceptrons for Real-time Image Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Junyu Lou, Xiaorui Zhao, Kexuan Shi, Shuhang Gu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.12135">https://arxiv.org/abs/2507.12135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.12135">https://arxiv.org/pdf/2507.12135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.12135]] Learning Pixel-adaptive Multi-layer Perceptrons for Real-time Image Enhancement(https://arxiv.org/abs/2507.12135)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Deep learning-based bilateral grid processing has emerged as a promising solution for image enhancement, inherently encoding spatial and intensity information while enabling efficient full-resolution processing through slicing operations. However, existing approaches are limited to linear affine transformations, hindering their ability to model complex color relationships. Meanwhile, while multi-layer perceptrons (MLPs) excel at non-linear mappings, traditional MLP-based methods employ globally shared parameters, which is hard to deal with localized variations. To overcome these dual challenges, we propose a Bilateral Grid-based Pixel-Adaptive Multi-layer Perceptron (BPAM) framework. Our approach synergizes the spatial modeling of bilateral grids with the non-linear capabilities of MLPs. Specifically, we generate bilateral grids containing MLP parameters, where each pixel dynamically retrieves its unique transformation parameters and obtain a distinct MLP for color mapping based on spatial coordinates and intensity values. In addition, we propose a novel grid decomposition strategy that categorizes MLP parameters into distinct types stored in separate subgrids. Multi-channel guidance maps are used to extract category-specific parameters from corresponding subgrids, ensuring effective utilization of color information during slicing while guiding precise parameter generation. Extensive experiments on public datasets demonstrate that our method outperforms state-of-the-art methods in performance while maintaining real-time processing capabilities.</li>
<li><strong>摘要：</strong>基于深度学习的双边网格处理已成为图像增强的有前途的解决方案，固有地编码空间和强度信息，同时通过切片操作实现有效的全分辨率处理。但是，现有方法仅限于线性仿射变换，阻碍了它们建模复杂颜色关系的能力。同时，尽管多层感知器（MLP）在非线性映射上出色，但传统的基于MLP的方法采用了全球共享参数，这很难处理局部变化。为了克服这些双重挑战，我们提出了一个基于双边网格的像素自动自适应多层感知器（BPAM）框架。我们的方法与MLP的非线性功能协同双侧网格的空间建模。具体而言，我们生成包含MLP参数的双侧网格，其中每个像素都会动态检索其唯一的转换参数，并根据空间坐标和强度值获得了与颜色映射的独特MLP。此外，我们提出了一种新型的网格分解策略，该策略将MLP参数分类为存储在单独的子网格中的不同类型。多通道指导图用于从相应的子格里德群中提取特定于类别的参数，从而确保在切片过程中有效利用颜色信息，同时指导精确的参数生成。公共数据集上的广泛实验表明，我们的方法在保持实时处理功能的同时，在性能方面的表现优于最先进的方法。</li>
</ul>

<h3>Title: Multi-Component VAE with Gaussian Markov Random Field</h3>
<ul>
<li><strong>Authors: </strong>Fouad Oubari, Mohamed El-Baha, Raphael Meunier, Rodrigue Décatoire, Mathilde Mougeot</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.12165">https://arxiv.org/abs/2507.12165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.12165">https://arxiv.org/pdf/2507.12165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.12165]] Multi-Component VAE with Gaussian Markov Random Field(https://arxiv.org/abs/2507.12165)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Multi-component datasets with intricate dependencies, like industrial assemblies or multi-modal imaging, challenge current generative modeling techniques. Existing Multi-component Variational AutoEncoders typically rely on simplified aggregation strategies, neglecting critical nuances and consequently compromising structural coherence across generated components. To explicitly address this gap, we introduce the Gaussian Markov Random Field Multi-Component Variational AutoEncoder , a novel generative framework embedding Gaussian Markov Random Fields into both prior and posterior distributions. This design choice explicitly models cross-component relationships, enabling richer representation and faithful reproduction of complex interactions. Empirically, our GMRF MCVAE achieves state-of-the-art performance on a synthetic Copula dataset specifically constructed to evaluate intricate component relationships, demonstrates competitive results on the PolyMNIST benchmark, and significantly enhances structural coherence on the real-world BIKED dataset. Our results indicate that the GMRF MCVAE is especially suited for practical applications demanding robust and realistic modeling of multi-component coherence</li>
<li><strong>摘要：</strong>具有复杂依赖性的多组分数据集，例如工业组件或多模式成像，挑战当前的生成建模技术。现有的多组分变量自动编码器通常依赖于简化的聚合策略，忽略了关键的细微差别，因此损害了生成的组件之间的结构一致性。为了明确解决此差距，我们将高斯马尔可夫随机场多组分变量自动编码器介绍，这是一种新颖的生成框架，将高斯马尔可夫随机字段嵌入到先验和后分布中。这种设计选择明确地对跨组件关系进行了建模，从而实现了更丰富的代表和忠实的复杂互动的再现。从经验上讲，我们的GMRF MCVAE在专门构建的用于评估复杂组件关系的合成Copula数据集上实现了最先进的性能，在多符号基准上展示了竞争成果，并显着提高了对现实世界自行车数据集的结构相干性。我们的结果表明，GMRF MCVAE特别适合于实用应用，要求对多组分相干性的强大和现实建模</li>
</ul>

<h3>Title: RadioDiff-3D: A 3D$\times$3D Radio Map Dataset and Generative Diffusion Based Benchmark for 6G Environment-Aware Communication</h3>
<ul>
<li><strong>Authors: </strong>Xiucheng Wang, Qiming Zhang, Nan Cheng, Junting Chen, Zezhong Zhang, Zan Li, Shuguang Cui, Xuemin Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.12166">https://arxiv.org/abs/2507.12166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.12166">https://arxiv.org/pdf/2507.12166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.12166]] RadioDiff-3D: A 3D$\times$3D Radio Map Dataset and Generative Diffusion Based Benchmark for 6G Environment-Aware Communication(https://arxiv.org/abs/2507.12166)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Radio maps (RMs) serve as a critical foundation for enabling environment-aware wireless communication, as they provide the spatial distribution of wireless channel characteristics. Despite recent progress in RM construction using data-driven approaches, most existing methods focus solely on pathloss prediction in a fixed 2D plane, neglecting key parameters such as direction of arrival (DoA), time of arrival (ToA), and vertical spatial variations. Such a limitation is primarily due to the reliance on static learning paradigms, which hinder generalization beyond the training data distribution. To address these challenges, we propose UrbanRadio3D, a large-scale, high-resolution 3D RM dataset constructed via ray tracing in realistic urban environments. UrbanRadio3D is over 37$\times$3 larger than previous datasets across a 3D space with 3 metrics as pathloss, DoA, and ToA, forming a novel 3D$\times$33D dataset with 7$\times$3 more height layers than prior state-of-the-art (SOTA) dataset. To benchmark 3D RM construction, a UNet with 3D convolutional operators is proposed. Moreover, we further introduce RadioDiff-3D, a diffusion-model-based generative framework utilizing the 3D convolutional architecture. RadioDiff-3D supports both radiation-aware scenarios with known transmitter locations and radiation-unaware settings based on sparse spatial observations. Extensive evaluations on UrbanRadio3D validate that RadioDiff-3D achieves superior performance in constructing rich, high-dimensional radio maps under diverse environmental dynamics. This work provides a foundational dataset and benchmark for future research in 3D environment-aware communication. The dataset is available at this https URL.</li>
<li><strong>摘要：</strong>无线电图（RMS）是实现环境意识无线通信的关键基础，因为它们提供了无线通道特征的空间分布。尽管使用数据驱动的方法在RM构建方面最近进行了进展，但大多数现有方法仅着眼于固定2D平面中的路径预测，忽略了关键参数，例如到达方向（DOA），到达时间（TOA）和垂直空间变化。这样的限制主要是由于依赖静态学习范式的依赖，这阻碍了训练数据分布之外的概括。为了应对这些挑战，我们提出了Urbanradio3d，这是一种通过射线追踪在现实的城市环境中构建的大规模高分辨率3D RM数据集。 Urbanradio3d的37 $ \ times $ 3以上的$ 3比以前的数据集大于3D空间，其中3个指标为Pathloss，DOA和TOA，形成了一个新颖的3D $ \ times $ 33D数据集，其中7 $ \ times $ 3 $ 3 $ 3 $ \ $ 3的高度比先前的先前的州 -  ART-ART-ART（SOTA）数据集高。为了基准3D RM构建，提出了一个带有3D卷积操作员的UNET。此外，我们进一步推出了Radiodiff-3D，这是一种利用3D卷积体系结构的基于扩散模型的生成框架。 Radiodiff-3D基于稀疏的空间观测值支持具有已知的发射器位置和辐射态设置的辐射感知场景。对UrbanRadio3D的广泛评估验证了Radiodiff-3D在不同的环境动力学下构建丰富的高维无广播图方面取得了卓越的性能。这项工作为3D环境感知通信的未来研究提供了基础数据集和基准。该数据集可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Revealing the Ancient Beauty: Digital Reconstruction of Temple Tiles using Computer Vision</h3>
<ul>
<li><strong>Authors: </strong>Arkaprabha Basu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.12195">https://arxiv.org/abs/2507.12195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.12195">https://arxiv.org/pdf/2507.12195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.12195]] Revealing the Ancient Beauty: Digital Reconstruction of Temple Tiles using Computer Vision(https://arxiv.org/abs/2507.12195)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Modern digitised approaches have dramatically changed the preservation and restoration of cultural treasures, integrating computer scientists into multidisciplinary projects with ease. Machine learning, deep learning, and computer vision techniques have revolutionised developing sectors like 3D reconstruction, picture inpainting,IoT-based methods, genetic algorithms, and image processing with the integration of computer scientists into multidisciplinary initiatives. We suggest three cutting-edge techniques in recognition of the special qualities of Indian monuments, which are famous for their architectural skill and aesthetic appeal. First is the Fractal Convolution methodology, a segmentation method based on image processing that successfully reveals subtle architectural patterns within these irreplaceable cultural buildings. The second is a revolutionary Self-Sensitive Tile Filling (SSTF) method created especially for West Bengal's mesmerising Bankura Terracotta Temples with a brand-new data augmentation method called MosaicSlice on the third. Furthermore, we delve deeper into the Super Resolution strategy to upscale the images without losing significant amount of quality. Our methods allow for the development of seamless region-filling and highly detailed tiles while maintaining authenticity using a novel data augmentation strategy within affordable costs introducing automation. By providing effective solutions that preserve the delicate balance between tradition and innovation, this study improves the subject and eventually ensures unrivalled efficiency and aesthetic excellence in cultural heritage protection. The suggested approaches advance the field into an era of unmatched efficiency and aesthetic quality while carefully upholding the delicate equilibrium between tradition and innovation.</li>
<li><strong>摘要：</strong>现代数字化的方法极大地改变了文化宝藏的保存和恢复，轻松地将计算机科学家纳入了多学科项目。机器学习，深度学习和计算机视觉技术已彻底改变了开发领域，例如3D重建，图片介绍，基于物联网的方法，遗传算法和图像处理，并将计算机科学家集成到多学科计划中。我们建议采用三种尖端技术，以表彰以其建筑技巧和美学吸引力而闻名的印度古迹的特殊品质。首先是分形卷积方法，这是一种基于图像处理的分割方法，成功地揭示了这些不可替代的文化建筑中微妙的建筑模式。第二个是一种革命性的自我敏感瓷砖填充（SSTF）方法，尤其是为西孟加拉邦迷人的Bankura Terracotta寺庙，采用了第三次称为Mosaicslice的全新数据增强方法。此外，我们更深入地研究了超级分辨率策略，以提高图像而不会失去大量质量。我们的方法允许开发无缝的区域填充和高度详细的瓷砖，同时使用新颖的数据增强策略在负担得起的成本引入自动化的情况下保持真实性。通过提供有效的解决方案来维护传统与创新之间的微妙平衡，这项研究改善了主题，并最终确保了文化遗产保护方面的无与伦比的效率和美学卓越。建议的方法将领域推向了无与伦比的效率和美学质量时代，同时仔细维护了传统与创新之间的微妙平衡。</li>
</ul>

<h3>Title: RODS: Robust Optimization Inspired Diffusion Sampling for Detecting and Reducing Hallucination in Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Yiqi Tian, Pengfei Jin, Mingze Yuan, Na Li, Bo Zeng, Quanzheng Li</a></li>
<li><strong>Subjects: </strong>cs.CV, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.12201">https://arxiv.org/abs/2507.12201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.12201">https://arxiv.org/pdf/2507.12201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.12201]] RODS: Robust Optimization Inspired Diffusion Sampling for Detecting and Reducing Hallucination in Generative Models(https://arxiv.org/abs/2507.12201)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved state-of-the-art performance in generative modeling, yet their sampling procedures remain vulnerable to hallucinations, often stemming from inaccuracies in score approximation. In this work, we reinterpret diffusion sampling through the lens of optimization and introduce RODS (Robust Optimization-inspired Diffusion Sampler), a novel method that detects and corrects high-risk sampling steps using geometric cues from the loss landscape. RODS enforces smoother sampling trajectories and adaptively adjusts perturbations, reducing hallucinations without retraining and at minimal additional inference cost. Experiments on AFHQv2, FFHQ, and 11k-hands demonstrate that RODS improves both sampling fidelity and robustness, detecting over 70% of hallucinated samples and correcting more than 25%, all while avoiding the introduction of new artifacts.</li>
<li><strong>摘要：</strong>扩散模型已经在生成建模中实现了最先进的性能，但是它们的抽样程序仍然容易受到幻觉的影响，这通常是由于得分近似的不准确而引起的。在这项工作中，我们重新解释了通过优化镜头进行的扩散采样，并引入了杆（强大的优化启发的扩散采样器），这是一种新的方法，该方法使用损失景观中的几何形状提示来检测并纠正高风险采样步骤。杆执行更平滑的采样轨迹，并自适应地调节扰动，减少幻觉，而无需重新训练，并且以最少的额外推断成本。关于AFHQV2，FFHQ和11K手的实验表明，杆可以提高采样忠诚度和鲁棒性，检测到超过70％的幻觉样品并纠正25％以上，同时避免引入新的文物。</li>
</ul>

<h3>Title: Generate to Ground: Multimodal Text Conditioning Boosts Phrase Grounding in Medical Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Felix Nützel, Mischa Dombrowski, Bernhard Kainz</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.12236">https://arxiv.org/abs/2507.12236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.12236">https://arxiv.org/pdf/2507.12236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.12236]] Generate to Ground: Multimodal Text Conditioning Boosts Phrase Grounding in Medical Vision-Language Models(https://arxiv.org/abs/2507.12236)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Phrase grounding, i.e., mapping natural language phrases to specific image regions, holds significant potential for disease localization in medical imaging through clinical reports. While current state-of-the-art methods rely on discriminative, self-supervised contrastive models, we demonstrate that generative text-to-image diffusion models, leveraging cross-attention maps, can achieve superior zero-shot phrase grounding performance. Contrary to prior assumptions, we show that fine-tuning diffusion models with a frozen, domain-specific language model, such as CXR-BERT, substantially outperforms domain-agnostic counterparts. This setup achieves remarkable improvements, with mIoU scores doubling those of current discriminative methods. These findings highlight the underexplored potential of generative models for phrase grounding tasks. To further enhance performance, we introduce Bimodal Bias Merging (BBM), a novel post-processing technique that aligns text and image biases to identify regions of high certainty. BBM refines cross-attention maps, achieving even greater localization accuracy. Our results establish generative approaches as a more effective paradigm for phrase grounding in the medical imaging domain, paving the way for more robust and interpretable applications in clinical practice. The source code and model weights are available at this https URL.</li>
<li><strong>摘要：</strong>短语接地，即将自然语言短语映射到特定图像区域，具有通过临床报告中疾病定位的巨大潜力。尽管当前的最新方法依赖于判别性，自我监督的对比模型，但我们证明，利用跨注意图的生成性文本到图像扩散模型可以实现出色的零击短语接地性能。与先前的假设相反，我们表明，具有冷冻的，特定于域的语言模型的微调扩散模型，例如CXR-BERT，基本上优于域 - 不可替代的对应物。这种设置取得了显着的改进，MIOU得分将当前歧视方法的分数加倍。这些发现突出了针对短语接地任务的生成模型的毫无疑问的潜力。为了进一步提高性能，我们引入了双峰偏置合并（BBM），这是一种新型的后处理技术，可将文本和图像偏见对准以识别高确定性的区域。 BBM优化了跨注意地图，实现了更高的定位准确性。我们的结果将生成方法建立为在医学成像领域中基础的更有效的范式，为在临床实践中更强大和可解释的应用铺平了道路。源代码和型号的权重可在此HTTPS URL上获得。</li>
</ul>

<h3>Title: FADE: Adversarial Concept Erasure in Flow Models</h3>
<ul>
<li><strong>Authors: </strong>Zixuan Fu, Yan Ren, Finn Carter, Chenyue Wang, Ze Niu, Dacheng Yu, Emily Davis, Bo Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.12283">https://arxiv.org/abs/2507.12283</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.12283">https://arxiv.org/pdf/2507.12283</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.12283]] FADE: Adversarial Concept Erasure in Flow Models(https://arxiv.org/abs/2507.12283)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated remarkable image generation capabilities, but also pose risks in privacy and fairness by memorizing sensitive concepts or perpetuating biases. We propose a novel \textbf{concept erasure} method for text-to-image diffusion models, designed to remove specified concepts (e.g., a private individual or a harmful stereotype) from the model's generative repertoire. Our method, termed \textbf{FADE} (Fair Adversarial Diffusion Erasure), combines a trajectory-aware fine-tuning strategy with an adversarial objective to ensure the concept is reliably removed while preserving overall model fidelity. Theoretically, we prove a formal guarantee that our approach minimizes the mutual information between the erased concept and the model's outputs, ensuring privacy and fairness. Empirically, we evaluate FADE on Stable Diffusion and FLUX, using benchmarks from prior work (e.g., object, celebrity, explicit content, and style erasure tasks from MACE). FADE achieves state-of-the-art concept removal performance, surpassing recent baselines like ESD, UCE, MACE, and ANT in terms of removal efficacy and image quality. Notably, FADE improves the harmonic mean of concept removal and fidelity by 5--10\% over the best prior method. We also conduct an ablation study to validate each component of FADE, confirming that our adversarial and trajectory-preserving objectives each contribute to its superior performance. Our work sets a new standard for safe and fair generative modeling by unlearning specified concepts without retraining from scratch.</li>
<li><strong>摘要：</strong>扩散模型表现出了显着的图像产生能力，但也通过记住敏感概念或永久偏见来构成隐私和公平的风险。我们为文本到图像扩散模型提出了一种新颖的\ textbf {概念擦除}方法，旨在从模型的生成曲目中删除指定的概念（例如，私人个人或有害的刻板印象）。我们的方法称为\ textbf {vade}（公平的对抗性扩散擦除），将轨迹感知的微调策略与对抗性目标相结合，以确保可靠地删除该概念，同时保留整体模型保真度。从理论上讲，我们证明了正式的保证，即我们的方法最大程度地减少了删除概念与模型的产出之间的相互信息，从而确保了隐私和公平。从经验上讲，我们使用先前工作的基准（例如，对象，名人，明确的内容和样式的擦除任务）来评估稳定扩散和通量的淡出。 Vade可以在去除功效和图像质量方面实现最新的概念删除性能，超过ESD，UCE，MACE和ANT等最新基线。值得注意的是，淡出将概念删除和保真度的谐波平均值提高了5--10 \％，而不是最佳先验方法。我们还进行了一项消融研究，以验证淡出的每个组成部分，证实我们的对抗性和轨迹保护目标每个都会有助于其出色的表现。我们的工作通过在不从头划痕的情况下学习指定的概念来确定安全且公平的生成建模的新标准。</li>
</ul>

<h3>Title: Thought Purity: Defense Paradigm For Chain-of-Thought Attack</h3>
<ul>
<li><strong>Authors: </strong>Zihao Xue, Zhen Bi, Long Ma, Zhenlin Hu, Yan Wang, Zhenfang Liu, Qing Sheng, Jie Xiao, Jungang Lou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.12314">https://arxiv.org/abs/2507.12314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.12314">https://arxiv.org/pdf/2507.12314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.12314]] Thought Purity: Defense Paradigm For Chain-of-Thought Attack(https://arxiv.org/abs/2507.12314)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While reinforcement learning-trained Large Reasoning Models (LRMs, e.g., Deepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large Language Models (LLMs) domain, their susceptibility to security threats remains a critical vulnerability. This weakness is particularly evident in Chain-of-Thought (CoT) generation processes, where adversarial methods like backdoor prompt attacks can systematically subvert the model's core reasoning mechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this vulnerability through exploiting prompt controllability, simultaneously degrading both CoT safety and task performance with low-cost interventions. To address this compounded security-performance vulnerability, we propose Thought Purity (TP): a defense paradigm that systematically strengthens resistance to malicious content while preserving operational efficacy. Our solution achieves this through three synergistic components: (1) a safety-optimized data processing pipeline (2) reinforcement learning-enhanced rule constraints (3) adaptive monitoring metrics. Our approach establishes the first comprehensive defense mechanism against CoTA vulnerabilities in reinforcement learning-aligned reasoning systems, significantly advancing the security-functionality equilibrium for next-generation AI architectures.</li>
<li><strong>摘要：</strong>尽管强化学习培训的大型推理模型（例如LRMS，例如DeepSeek-R1）在不断发展的大语言模型（LLMS）领域中表现出高级推理能力，但它们对安全威胁的敏感性仍然是一个至关重要的脆弱性。这种弱点尤其明显，在经营链（COT）生成过程中，在这种过程中，诸如后门及时攻击之类的对抗方法可以系统地颠覆模型的核心推理机制。新兴链条攻击（COTA）通过利用快速可控性，同时通过低成本干预措施同时降低了COT安全性和任务性能，从而揭示了这种脆弱性。为了解决这种复杂的安全性绩效脆弱性，我们提出了思想纯度（TP）：一种防御范式，可以系统地增强对恶意内容的抵制，同时保留操作效率。我们的解决方案通过三个协同组件实现了这一点：（1）安全性的数据处理管道（2）强化学习增强规则约束（3）自适应监视指标。我们的方法建立了针对强化学习与推理系统的第一个全面防御机制，可大大提高下一代AI体系结构的安全功能平衡。</li>
</ul>

<h3>Title: Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Samuel Lavoie, Michael Noukhovitch, Aaron Courville</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.12318">https://arxiv.org/abs/2507.12318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.12318">https://arxiv.org/pdf/2507.12318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.12318]] Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models(https://arxiv.org/abs/2507.12318)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We argue that diffusion models' success in modeling complex distributions is, for the most part, coming from their input conditioning. This paper investigates the representation used to condition diffusion models from the perspective that ideal representations should improve sample fidelity, be easy to generate, and be compositional to allow out-of-training samples generation. We introduce Discrete Latent Code (DLC), an image representation derived from Simplicial Embeddings trained with a self-supervised learning objective. DLCs are sequences of discrete tokens, as opposed to the standard continuous image embeddings. They are easy to generate and their compositionality enables sampling of novel images beyond the training distribution. Diffusion models trained with DLCs have improved generation fidelity, establishing a new state-of-the-art for unconditional image generation on ImageNet. Additionally, we show that composing DLCs allows the image generator to produce out-of-distribution samples that coherently combine the semantics of images in diverse ways. Finally, we showcase how DLCs can enable text-to-image generation by leveraging large-scale pretrained language models. We efficiently finetune a text diffusion language model to generate DLCs that produce novel samples outside of the image generator training distribution.</li>
<li><strong>摘要：</strong>我们认为，扩散模型在建模复杂分布方面的成功是其输入条件。本文从理想表示应该改善样品保真度，易于生成并保持组成以允许训练外样品的产生的角度研究了用于调节扩散模型的表示形式。我们引入了离散潜在代码（DLC），这是一种图像表示，该图像表示是从训练有自我监督的学习目标训练的简单嵌入式。 DLC是离散令牌的序列，而不是标准连续图像嵌入。它们易于生成，并且它们的组成性可以在训练分布之外对新型图像进行取样。经过DLC训练的扩散模型具有改善的产生忠诚度，为ImageNet上的无条件形象生成建立了新的最先进的忠诚度。此外，我们表明，组成DLC允许图像发生器生成分布式样品，这些样本将图像以各种方式相干地结合了图像的语义。最后，我们展示了DLC如何通过利用大规模预处理的语言模型来启用文本对图像的生成。我们有效地对文本扩散语言模型进行了捕获，以生成在图像发生器训练分布之外产生新样本的DLC。</li>
</ul>

<h3>Title: Improving Lightweight Weed Detection via Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Ahmet Oğuz Saltık, Max Voigt, Sourav Modak, Mike Beckworth, Anthony Stein</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.12344">https://arxiv.org/abs/2507.12344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.12344">https://arxiv.org/pdf/2507.12344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.12344]] Improving Lightweight Weed Detection via Knowledge Distillation(https://arxiv.org/abs/2507.12344)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Weed detection is a critical component of precision agriculture, facilitating targeted herbicide application and reducing environmental impact. However, deploying accurate object detection models on resource-limited platforms remains challenging, particularly when differentiating visually similar weed species commonly encountered in plant phenotyping applications. In this work, we investigate Channel-wise Knowledge Distillation (CWD) and Masked Generative Distillation (MGD) to enhance the performance of lightweight models for real-time smart spraying systems. Utilizing YOLO11x as the teacher model and YOLO11n as both reference and student, both CWD and MGD effectively transfer knowledge from the teacher to the student model. Our experiments, conducted on a real-world dataset comprising sugar beet crops and four weed types (Cirsium, Convolvulus, Fallopia, and Echinochloa), consistently show increased AP50 across all classes. The distilled CWD student model achieves a notable improvement of 2.5% and MGD achieves 1.9% in mAP50 over the baseline without increasing model complexity. Additionally, we validate real-time deployment feasibility by evaluating the student YOLO11n model on Jetson Orin Nano and Raspberry Pi 5 embedded devices, performing five independent runs to evaluate performance stability across random seeds. These findings confirm CWD and MGD as an effective, efficient, and practical approach for improving deep learning-based weed detection accuracy in precision agriculture and plant phenotyping scenarios.</li>
<li><strong>摘要：</strong>杂草检测是精确农业的关键组成部分，促进靶向除草剂的应用并减少环境影响。但是，在资源有限的平台上部署准确的对象检测模型仍然具有挑战性，尤其是在区分植物表型应用中通常遇到的视觉上相似的杂草物种时。在这项工作中，我们调查了渠道知识蒸馏（CWD）和掩盖生成蒸馏（MGD），以增强实时智能喷涂系统的轻质模型的性能。 CWD和MGD都利用Yolo11x作为教师模型和Yolo11n作为参考和学生，有效地将知识从教师转移到学生模型。我们的实验是在包括甜菜作物和四种杂草类型（Cirsium，Chervolvulus，Fellopia和Echinochloa）的现实世界数据集上进行的，在所有类别中始终显示出AP50的增加。蒸馏的CWD学生模型可取得显着提高2.5％，而MGD在MAP50的基线中获得1.9％，而不会增加模型复杂性。此外，我们通过评估Jetson Orin Nano和Raspberry Pi 5嵌入式设备上的学生YOLO11N模型来验证实时部署可行性，进行五次独立运行以评估跨随机种子的性能稳定性。这些发现证实CWD和MGD是一种有效，有效和实用的方法，用于提高精确农业和植物表型方案中的深度学习杂草检测准确性。</li>
</ul>

<h3>Title: Mitigating Object Hallucinations via Sentence-Level Early Intervention</h3>
<ul>
<li><strong>Authors: </strong>Shangpin Peng, Senqiao Yang, Li Jiang, Zhuotao Tian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.12455">https://arxiv.org/abs/2507.12455</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.12455">https://arxiv.org/pdf/2507.12455</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.12455]] Mitigating Object Hallucinations via Sentence-Level Early Intervention(https://arxiv.org/abs/2507.12455)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have revolutionized cross-modal understanding but continue to struggle with hallucinations - fabricated content contradicting visual inputs. Existing hallucination mitigation methods either incur prohibitive computational costs or introduce distribution mismatches between training data and model outputs. We identify a critical insight: hallucinations predominantly emerge at the early stages of text generation and propagate through subsequent outputs. To address this, we propose **SENTINEL** (**S**entence-level **E**arly i**N**tervention **T**hrough **IN**-domain pr**E**ference **L**earning), a framework that eliminates dependency on human annotations. Specifically, we first bootstrap high-quality in-domain preference pairs by iteratively sampling model outputs, validating object existence through cross-checking with two open-vocabulary detectors, and classifying sentences into hallucinated/non-hallucinated categories. Subsequently, we use context-coherent positive samples and hallucinated negative samples to build context-aware preference data iteratively. Finally, we train models using a context-aware preference loss (C-DPO) that emphasizes discriminative learning at the sentence level where hallucinations initially manifest. Experimental results show that SENTINEL can reduce hallucinations by over 90\% compared to the original model and outperforms the previous state-of-the-art method on both hallucination benchmarks and general capabilities benchmarks, demonstrating its superiority and generalization ability. The models, datasets, and code are available at this https URL.</li>
<li><strong>摘要：</strong>多模式的大语言模型（MLLM）彻底改变了跨模式的理解，但继续在幻觉中挣扎 - 捏造的内容与视觉输入相矛盾。现有的缓解幻觉方法要么产生过度的计算成本，要么引入训练数据和模型输出之间的分布不匹配。我们确定了一个关键的见识：幻觉主要在文本生成的早期阶段出现，并通过随后的输出传播。为了解决这个问题，我们提出** sentinel **（** s ** entence-level ** e ** a arly i ** n ** tervention ** t ** t ** t ** hrough ** in **  - 域pr ** e ** e ** e ** e ** ference ** l ** genning），一个框架，消除了对人类注释的依赖。具体而言，我们首先通过迭代采样模型输出来引导高质量的内域偏好对，通过与两个开放式摄氏探测器进行交叉检查，并将句子分类为幻觉/非抗击类别，从而验证对象存在。 Subsequently, we use context-coherent positive samples and hallucinated negative samples to build context-aware preference data iteratively.最后，我们使用上下文感知的偏好损失（C-DPO）训练模型，该模型强调幻觉最初表现出的句子级别的歧视性学习。实验结果表明，与原始模型相比，前哨可以将幻觉减少超过90％，并且在幻觉基准和一般能力基准上胜过先前的最新方法，表明其优越性和泛化能力。 The models, datasets, and code are available at this https URL.</li>
</ul>

<h3>Title: MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior Understanding</h3>
<ul>
<li><strong>Authors: </strong>Renjie Li, Ruijie Ye, Mingyang Wu, Hao Frank Yang, Zhiwen Fan, Hezhen Hu, Zhengzhong Tu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.12463">https://arxiv.org/abs/2507.12463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.12463">https://arxiv.org/pdf/2507.12463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.12463]] MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior Understanding(https://arxiv.org/abs/2507.12463)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Humans are integral components of the transportation ecosystem, and understanding their behaviors is crucial to facilitating the development of safe driving systems. Although recent progress has explored various aspects of human behavior$\unicode{x2014}$such as motion, trajectories, and intention$\unicode{x2014}$a comprehensive benchmark for evaluating human behavior understanding in autonomous driving remains unavailable. In this work, we propose $\textbf{MMHU}$, a large-scale benchmark for human behavior analysis featuring rich annotations, such as human motion and trajectories, text description for human motions, human intention, and critical behavior labels relevant to driving safety. Our dataset encompasses 57k human motion clips and 1.73M frames gathered from diverse sources, including established driving datasets such as Waymo, in-the-wild videos from YouTube, and self-collected data. A human-in-the-loop annotation pipeline is developed to generate rich behavior captions. We provide a thorough dataset analysis and benchmark multiple tasks$\unicode{x2014}$ranging from motion prediction to motion generation and human behavior question answering$\unicode{x2014}$thereby offering a broad evaluation suite. Project page : this https URL.</li>
<li><strong>摘要：</strong>人类是运输生态系统的组成部分，了解其行为对于促进安全驾驶系统的发展至关重要。尽管最近的进步探索了人类行为的各个方面$ \ unicode {x2014} $，例如运动，轨迹和意图$ \ unicode {x2014} $是评估自主驾驶中人类行为理解的综合基准，仍然不可自行。在这项工作中，我们提出了$ \ textbf {mmhu} $，这是一种大规模的人类行为分析基准，具有丰富的注释，例如人类运动和轨迹，人类动议的文本描述，人类意图，与驱动安全性相关的批判性行为标签。我们的数据集包括从不同来源收集的57K人体运动夹和173万帧，包括已建立的驾驶数据集，例如Waymo，YouTube的野外视频以及自收集的数据。开发了人类在环上的注释管道，以产生丰富的行为标题。我们提供了彻底的数据集分析，并基准了多个任务$ \ unicode {x2014} $，从运动预测到运动产生到运动和人类行为问题回答$ \ unicode {x2014} $，从而提供广泛的评估套件。项目页面：此HTTPS URL。</li>
</ul>

<h3>Title: PhysX: Physical-Grounded 3D Asset Generation</h3>
<ul>
<li><strong>Authors: </strong>Ziang Cao, Zhaoxi Chen, Linag Pan, Ziwei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.12465">https://arxiv.org/abs/2507.12465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.12465">https://arxiv.org/pdf/2507.12465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.12465]] PhysX: Physical-Grounded 3D Asset Generation(https://arxiv.org/abs/2507.12465)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>3D modeling is moving from virtual to physical. Existing 3D generation primarily emphasizes geometries and textures while neglecting physical-grounded modeling. Consequently, despite the rapid development of 3D generative models, the synthesized 3D assets often overlook rich and important physical properties, hampering their real-world application in physical domains like simulation and embodied AI. As an initial attempt to address this challenge, we propose \textbf{PhysX}, an end-to-end paradigm for physical-grounded 3D asset generation. 1) To bridge the critical gap in physics-annotated 3D datasets, we present PhysXNet - the first physics-grounded 3D dataset systematically annotated across five foundational dimensions: absolute scale, material, affordance, kinematics, and function description. In particular, we devise a scalable human-in-the-loop annotation pipeline based on vision-language models, which enables efficient creation of physics-first assets from raw 3D assets.2) Furthermore, we propose \textbf{PhysXGen}, a feed-forward framework for physics-grounded image-to-3D asset generation, injecting physical knowledge into the pre-trained 3D structural space. Specifically, PhysXGen employs a dual-branch architecture to explicitly model the latent correlations between 3D structures and physical properties, thereby producing 3D assets with plausible physical predictions while preserving the native geometry quality. Extensive experiments validate the superior performance and promising generalization capability of our framework. All the code, data, and models will be released to facilitate future research in generative physical AI.</li>
<li><strong>摘要：</strong>3D建模正在从虚拟变为物理。现有的3D代主要强调几何形状和纹理，同时忽略了物理基础的建模。因此，尽管3D生成模型的发展迅速，但合成的3D资产通常会忽略丰富而重要的物理特性，从而阻碍了其在仿真和体现AI等物理领域中的实际应用。作为解决这一挑战的初步尝试，我们提出了\ textbf {physx}，这是一种用于物理基础3D资产生成的端到端范式。 1）为了弥合物理注销的3D数据集中的临界差距，我们提出了Physxnet-在五个基础维度上系统地注释了第一个物理界面的3D数据集：绝对规模，材料，负担能力，运动学和功能描述。特别是，我们基于视觉模型设计了一个可扩展的人类注释管道，该管道可以有效地创建原始3D资产的物理 - 优先资产。2）此外，我们建议\ textbf {physxgen}，fordbf {physxgen}，一种用于物理 - 图像 - 图像 - 图像 - 图像至3D的物理知识的ford-forderward forderfork，构建物理知识，构造了3D的构造，以插入式3D构建3D。具体而言，PhysXGEN采用双分支结构来明确对3D结构与物理特性之间的潜在相关性进行建模，从而在保留天然几何质量的同时生产具有合理的物理预测的3D资产。广泛的实验验证了我们框架的出色性能和有希望的概括能力。将发布所有代码，数据和模型，以促进生成物理AI的未来研究。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
