<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-07-23</h1>
<h3>Title: PAT++: a cautionary tale about generative visual augmentation for Object Re-identification</h3>
<ul>
<li><strong>Authors: </strong>Leonardo Santiago Benitez Pereira, Arathy Jeevan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15888">https://arxiv.org/abs/2507.15888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15888">https://arxiv.org/pdf/2507.15888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15888]] PAT++: a cautionary tale about generative visual augmentation for Object Re-identification(https://arxiv.org/abs/2507.15888)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative data augmentation has demonstrated gains in several vision tasks, but its impact on object re-identification - where preserving fine-grained visual details is essential - remains largely unexplored. In this work, we assess the effectiveness of identity-preserving image generation for object re-identification. Our novel pipeline, named PAT++, incorporates Diffusion Self-Distillation into the well-established Part-Aware Transformer. Using the Urban Elements ReID Challenge dataset, we conduct extensive experiments with generated images used for both model training and query expansion. Our results show consistent performance degradation, driven by domain shifts and failure to retain identity-defining features. These findings challenge assumptions about the transferability of generative models to fine-grained recognition tasks and expose key limitations in current approaches to visual augmentation for identity-preserving applications.</li>
<li><strong>摘要：</strong>生成数据的增强已证明了几项视觉任务的收益，但是它对对象重新识别的影响 - 保存细颗粒的视觉细节至关重要 - 在很大程度上尚未探索。在这项工作中，我们评估了具有身份的图像生成对象重新识别的有效性。我们的新型管道（名为PAT ++）将扩散的自我鉴定纳入了建立的零件感知变压器中。使用城市元素REID挑战数据集，我们通过用于模型训练和查询扩展的生成图像进行了广泛的实验。我们的结果表明，在域移动和未能保留身份定义特征的驱动下，性能降级一致。这些发现挑战了有关生成模型转移到细粒度识别任务并在当前视觉增强方法中的关键限制中的可转移性的假设。</li>
</ul>

<h3>Title: ReDi: Rectified Discrete Flow</h3>
<ul>
<li><strong>Authors: </strong>Jaehoon Yoo, Wonjung Kim, Seunghoon Hong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15897">https://arxiv.org/abs/2507.15897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15897">https://arxiv.org/pdf/2507.15897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15897]] ReDi: Rectified Discrete Flow(https://arxiv.org/abs/2507.15897)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Discrete Flow-based Models (DFMs) are powerful generative models for high-quality discrete data but typically suffer from slow sampling speeds due to their reliance on iterative decoding processes. This reliance on a multi-step process originates from the factorization approximation of DFMs, which is necessary for handling high-dimensional data. In this paper, we rigorously characterize the approximation error from factorization using Conditional Total Correlation (TC), which depends on the coupling. To reduce the Conditional TC and enable efficient few-step generation, we propose Rectified Discrete Flow (ReDi), a novel iterative method that reduces factorization error by rectifying the coupling between source and target distributions. We theoretically prove that each ReDi step guarantees a monotonic decreasing Conditional TC, ensuring its convergence. Empirically, ReDi significantly reduces Conditional TC and enables few-step generation. Moreover, we demonstrate that the rectified couplings are well-suited for training efficient one-step models on image generation. ReDi offers a simple and theoretically grounded approach for tackling the few-step challenge, providing a new perspective on efficient discrete data synthesis. Code is available at this https URL</li>
<li><strong>摘要：</strong>基于流动流的模型（DFM）是用于高质量离散数据的强大生成模型，但由于其依赖迭代解码过程，通常会遭受缓慢的采样速度。对多步骤过程的这种依赖源于DFM的分解近似，这对于处理高维数据是必需的。在本文中，我们严格地表征了使用条件总相关（TC）因耦合而分解的近似误差。为了减少有条件的TC并启用有效的几步生成，我们提出了整流的离散流（REDI），这是一种新型的迭代方法，可通过纠正源和目标分布之间的耦合来减少分解误差。从理论上讲，我们证明每个redi步骤都可以保证单调降低条件TC，从而确保其收敛性。从经验上讲，REDI显着减少了有条件的TC，并且可以产生几步的生成。此外，我们证明了校正后的耦合非常适合训练图像生成的有效的一步模型。雷迪（Redi）提供了一种简单且理论上的方法来应对几步挑战，为有效的离散数据综合提供了新的观点。代码可在此HTTPS URL上找到</li>
</ul>

<h3>Title: Improving the Generation of VAEs with High Dimensional Latent Spaces by the use of Hyperspherical Coordinates</h3>
<ul>
<li><strong>Authors: </strong>Alejandro Ascarate, Leo Lebrat, Rodrigo Santa Cruz, Clinton Fookes, Olivier Salvado</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15900">https://arxiv.org/abs/2507.15900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15900">https://arxiv.org/pdf/2507.15900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15900]] Improving the Generation of VAEs with High Dimensional Latent Spaces by the use of Hyperspherical Coordinates(https://arxiv.org/abs/2507.15900)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Variational autoencoders (VAE) encode data into lower-dimensional latent vectors before decoding those vectors back to data. Once trained, decoding a random latent vector from the prior usually does not produce meaningful data, at least when the latent space has more than a dozen dimensions. In this paper, we investigate this issue by drawing insight from high dimensional statistics: in these regimes, the latent vectors of a standard VAE are by construction distributed uniformly on a hypersphere. We propose to formulate the latent variables of a VAE using hyperspherical coordinates, which allows compressing the latent vectors towards an island on the hypersphere, thereby reducing the latent sparsity and we show that this improves the generation ability of the VAE. We propose a new parameterization of the latent space with limited computational overhead.</li>
<li><strong>摘要：</strong>变性自动编码器（VAE）将数据编码为较低维的潜在向量，然后再将这些向量解码回数据。一旦经过训练，从先前的训练中解码随机潜在向量通常不会产生有意义的数据，至少当潜在空间具有超过十几个维度时。在本文中，我们通过从高维统计数据中汲取见解来调查这个问题：在这些制度中，标准VAE的潜在向量是通过在超晶体上统一分布的施工。我们建议使用超球形坐标制定VAE的潜在变量，从而使潜在向量压缩到高孔上的一个岛上，从而降低了潜在的稀疏性，我们表明这提高了VAE的产生能力。我们提出了具有有限的计算开销的潜在空间的新参数化。</li>
</ul>

<h3>Title: HyDRA: A Hybrid-Driven Reasoning Architecture for Verifiable Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Adrian Kaiser, Claudiu Leoveanu-Condrei, Ryan Gold, Marius-Constantin Dinu, Markus Hofmarcher</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15917">https://arxiv.org/abs/2507.15917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15917">https://arxiv.org/pdf/2507.15917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15917]] HyDRA: A Hybrid-Driven Reasoning Architecture for Verifiable Knowledge Graphs(https://arxiv.org/abs/2507.15917)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The synergy between symbolic knowledge, often represented by Knowledge Graphs (KGs), and the generative capabilities of neural networks is central to advancing neurosymbolic AI. A primary bottleneck in realizing this potential is the difficulty of automating KG construction, which faces challenges related to output reliability, consistency, and verifiability. These issues can manifest as structural inconsistencies within the generated graphs, such as the formation of disconnected $\textit{isolated islands}$ of data or the inaccurate conflation of abstract classes with specific instances. To address these challenges, we propose HyDRA, a $\textbf{Hy}$brid-$\textbf{D}$riven $\textbf{R}$easoning $\textbf{A}$rchitecture designed for verifiable KG automation. Given a domain or an initial set of documents, HyDRA first constructs an ontology via a panel of collaborative neurosymbolic agents. These agents collaboratively agree on a set of competency questions (CQs) that define the scope and requirements the ontology must be able to answer. Given these CQs, we build an ontology graph that subsequently guides the automated extraction of triplets for KG generation from arbitrary documents. Inspired by design-by-contracts (DbC) principles, our method leverages verifiable contracts as the primary control mechanism to steer the generative process of Large Language Models (LLMs). To verify the output of our approach, we extend beyond standard benchmarks and propose an evaluation framework that assesses the functional correctness of the resulting KG by leveraging symbolic verifications as described by the neurosymbolic AI framework, $\textit{SymbolicAI}$. This work contributes a hybrid-driven architecture for improving the reliability of automated KG construction and the exploration of evaluation methods for measuring the functional integrity of its output. The code is publicly available.</li>
<li><strong>摘要：</strong>通常由知识图（KG）代表的符号知识之间的协同作用，而神经网络的生成能力对于推进神经肌肯定AI至关重要。意识到这一潜力的主要瓶颈是自动化kg构造的困难，这面临着与产出可靠性，一致性和可验证性相关的挑战。这些问题可能表现为生成图中的结构上不一致，例如数据的$ \ textit {隔离岛} $的数据的形成或抽象类与特定实例的不准确混合。为了应对这些挑战，我们提出了hydra，$ \ textbf {hy} $ brid-$ \ textbf {d} $ riven $ \ textbf {r} $恢复$ \ textbf {a} $ rchitecture test $ rchitecture decrifiable kg kg自动化设计。给定域或初始文档集，Hydra首先通过一组协作神经 - 理论剂构建本体。这些代理人协同同意定义范围和要求本体学必须能够回答的一系列能力问题（CQ）。鉴于这些CQ，我们构建了一个本体图，该图随后指导从任意文档中生成KG的三胞胎自动提取。受到逐个设计（DBC）原则的启发，我们的方法利用可验证的合同作为主要控制机制来指导大语言模型（LLMS）的生成过程。为了验证我们的方法的输出，我们扩展了超越标准基准测试，并提出了一个评估框架，该框架通过利用神经成像AI框架所描述的符号验证来评估所得kg的功能正确性，$ \ textit {smybolicai} $。这项工作贡献了一种混合驱动的体系结构，以提高自动化KG构造的可靠性以及探索评估方法，以衡量其产出的功能完整性。该代码公开可用。</li>
</ul>

<h3>Title: A Lightweight Face Quality Assessment Framework to Improve Face Verification Performance in Real-Time Screening Applications</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Aman Ibrahim, Hamad Mansour Alawar, Abdulnasser Abbas Zehi, Ahmed Mohammad Alkendi, Bilal Shafi Ashfaq Ahmed Mirza, Shan Ullah, Ismail Lujain Jaleel, Hassan Ugail</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15961">https://arxiv.org/abs/2507.15961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15961">https://arxiv.org/pdf/2507.15961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15961]] A Lightweight Face Quality Assessment Framework to Improve Face Verification Performance in Real-Time Screening Applications(https://arxiv.org/abs/2507.15961)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Face image quality plays a critical role in determining the accuracy and reliability of face verification systems, particularly in real-time screening applications such as surveillance, identity verification, and access control. Low-quality face images, often caused by factors such as motion blur, poor lighting conditions, occlusions, and extreme pose variations, significantly degrade the performance of face recognition models, leading to higher false rejection and false acceptance rates. In this work, we propose a lightweight yet effective framework for automatic face quality assessment, which aims to pre-filter low-quality face images before they are passed to the verification pipeline. Our approach utilises normalised facial landmarks in conjunction with a Random Forest Regression classifier to assess image quality, achieving an accuracy of 96.67\%. By integrating this quality assessment module into the face verification process, we observe a substantial improvement in performance, including a comfortable 99.7\% reduction in the false rejection rate and enhanced cosine similarity scores when paired with the ArcFace face verification model. To validate our approach, we have conducted experiments on a real-world dataset collected comprising over 600 subjects captured from CCTV footage in unconstrained environments within Dubai Police. Our results demonstrate that the proposed framework effectively mitigates the impact of poor-quality face images, outperforming existing face quality assessment techniques while maintaining computational efficiency. Moreover, the framework specifically addresses two critical challenges in real-time screening: variations in face resolution and pose deviations, both of which are prevalent in practical surveillance scenarios.</li>
<li><strong>摘要：</strong>面部图像质量在确定面部验证系统的准确性和可靠性中起着至关重要的作用，尤其是在实时筛选应用程序（例如监视，身份验证和访问控制）中。低质量的面部图像通常是由运动模糊，较差的照明条件，遮挡和极端姿势变化引起的，大大降低了面部识别模型的性能，从而导致较高的错误拒绝和错误的接受率。在这项工作中，我们为自动面部质量评估提出了一个轻巧但有效的框架，该框架旨在在将其传递给验证管道之前先进行过滤较低质量的面部图像。我们的方法与随机的森林回归分类器结合使用标准化的面部标志来评估图像质量，获得了96.67 \％的精度。通过将此质量评估模块整合到面部验证过程中，我们观察到了性能的实质性改善，包括舒适的99.7 \％降低错误拒绝率和与Arcface Face验证模型配对时的余弦相似性得分。为了验证我们的方法，我们已经对收集的现实数据集进行了实验，该数据集在迪拜警察局的无约束环境中收集了600多名受试者。我们的结果表明，所提出的框架有效地减轻了面部质量图像的影响，在维持计算效率的同时，表现优于现有的面部质量评估技术。此外，该框架专门解决了实时筛查中的两个关键挑战：面部分辨率和姿势偏差的变化，这两者在实际监视方案中都普遍存在。</li>
</ul>

<h3>Title: Improving Personalized Image Generation through Social Context Feedback</h3>
<ul>
<li><strong>Authors: </strong>Parul Gupta, Abhinav Dhall, Thanh-Toan Do</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16095">https://arxiv.org/abs/2507.16095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16095">https://arxiv.org/pdf/2507.16095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16095]] Improving Personalized Image Generation through Social Context Feedback(https://arxiv.org/abs/2507.16095)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Personalized image generation, where reference images of one or more subjects are used to generate their image according to a scene description, has gathered significant interest in the community. However, such generated images suffer from three major limitations -- complex activities, such as $<$man, pushing, motorcycle$>$ are not generated properly with incorrect human poses, reference human identities are not preserved, and generated human gaze patterns are unnatural/inconsistent with the scene description. In this work, we propose to overcome these shortcomings through feedback-based fine-tuning of existing personalized generation methods, wherein, state-of-art detectors of pose, human-object-interaction, human facial recognition and human gaze-point estimation are used to refine the diffusion model. We also propose timestep-based inculcation of different feedback modules, depending upon whether the signal is low-level (such as human pose), or high-level (such as gaze point). The images generated in this manner show an improvement in the generated interactions, facial identities and image quality over three benchmark datasets.</li>
<li><strong>摘要：</strong>个性化的图像生成，其中一个或多个主题的参考图像根据场景描述来生成其图像，对社区产生了极大的兴趣。但是，这种产生的图像遭受了三个主要局限性 - 复杂的活动，例如$ <$的人，推动，摩托车$> $没有适当地产生人类姿势，没有保留人类身份，并且产生的人类凝视模式与场景描述不自然/不一致。在这项工作中，我们建议通过基于反馈的现有个性化生成方法的微调来克服这些缺点，其中使用姿势，人类对象相互作用，人面部识别和人类凝视点估计的最先进的检测器用于完善扩散模型。我们还提出了基于时间段的不同反馈模块的灌输，具体取决于信号是低级（例如人姿势）还是高级（例如凝视点）。以这种方式生成的图像显示了三个基准数据集对生成的交互，面部身份和图像质量的改进。</li>
</ul>

<h3>Title: PUSA V1.0: Surpassing Wan-I2V with $500 Training Cost by Vectorized Timestep Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Yaofang Liu, Yumeng Ren, Aitor Artola, Yuxuan Hu, Xiaodong Cun, Xiaotong Zhao, Alan Zhao, Raymond H. Chan, Suiyun Zhang, Rui Liu, Dandan Tu, Jean-Michel Morel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16116">https://arxiv.org/abs/2507.16116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16116">https://arxiv.org/pdf/2507.16116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16116]] PUSA V1.0: Surpassing Wan-I2V with $500 Training Cost by Vectorized Timestep Adaptation(https://arxiv.org/abs/2507.16116)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of video diffusion models has been hindered by fundamental limitations in temporal modeling, particularly the rigid synchronization of frame evolution imposed by conventional scalar timestep variables. While task-specific adaptations and autoregressive models have sought to address these challenges, they remain constrained by computational inefficiency, catastrophic forgetting, or narrow applicability. In this work, we present Pusa, a groundbreaking paradigm that leverages vectorized timestep adaptation (VTA) to enable fine-grained temporal control within a unified video diffusion framework. Besides, VTA is a non-destructive adaptation, which means it fully preserves the capabilities of the base model. By finetuning the SOTA Wan2.1-T2V-14B model with VTA, we achieve unprecedented efficiency -- surpassing the performance of Wan-I2V-14B with $\leq$ 1/200 of the training cost (\$500 vs. $\geq$ \$100,000) and $\leq$ 1/2500 of the dataset size (4K vs. $\geq$ 10M samples). Pusa not only sets a new standard for image-to-video (I2V) generation, achieving a VBench-I2V total score of 87.32\% (vs. 86.86\% of Wan-I2V-14B), but also unlocks many zero-shot multi-task capabilities such as start-end frames and video extension -- all without task-specific training. Meanwhile, Pusa can still perform text-to-video generation. Mechanistic analyses reveal that our approach preserves the foundation model's generative priors while surgically injecting temporal dynamics, avoiding the combinatorial explosion inherent to vectorized timesteps. This work establishes a scalable, efficient, and versatile paradigm for next-generation video synthesis, democratizing high-fidelity video generation for research and industry alike. Code is open-sourced at this https URL</li>
<li><strong>摘要：</strong>视频扩散模型的快速发展受到时间建模的基本局限性的阻碍，尤其是传统标量时间段变量施加的框架进化的刚性同步。尽管特定于任务的适应和自回归模型试图解决这些挑战，但它们仍然受到计算效率低下，灾难性遗忘或狭窄的适用性的限制。在这项工作中，我们提出了PUSA，这是一个开创性的范式，利用矢量性时间播放适应（VTA）在统一的视频扩散框架内实现细粒度的时间控制。此外，VTA是一种无损的适应性，这意味着它完全保留了基本模型的功能。通过使用VTA对SOTA WAN2.1-T2V-14B型号进行固定，我们实现了前所未有的效率 - 超过了WAN-I2V-14B的性能，并使用$ \ leq $ \ leq $ 1/200的培训成本（\ $ 500 vs. $ 500 vs. $ \ \ \ \ feq $ \ $ 100,000）和$ \ $ \ $ \ $ \ leq $ 1/2500码。样本）。 PUSA不仅设定了图像到视频（I2V）一代的新标准，达到VBENCH-I2V总分为87.32 \％（vs. 86.86 \％的WAN-I2V-14B％），而且还可以解锁诸如启动式FrameSive和视频范围的较高型多任务功能，而不受诸如启动式的型号训练，而没有任命任务 - 同样是零件 - 同样跨越任务 - 同样是任务 - 同样是任务 - 同时，PUSA仍然可以执行文本到视频生成。机械分析表明，我们的方法在手术注射时间动力学的同时保留了基础模型的生成先验，避免了矢量化时间段固有的组合爆炸。这项工作为下一代视频综合建立了可扩展，高效和多功能的范式，使研究和行业的高保真视频生成民主化。代码在此HTTPS URL上开源</li>
</ul>

<h3>Title: LongSplat: Online Generalizable 3D Gaussian Splatting from Long Sequence Images</h3>
<ul>
<li><strong>Authors: </strong>Guichen Huang, Ruoyu Wang, Xiangjun Gao, Che Sun, Yuwei Wu, Shenghua Gao, Yunde Jia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16144">https://arxiv.org/abs/2507.16144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16144">https://arxiv.org/pdf/2507.16144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16144]] LongSplat: Online Generalizable 3D Gaussian Splatting from Long Sequence Images(https://arxiv.org/abs/2507.16144)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>3D Gaussian Splatting achieves high-fidelity novel view synthesis, but its application to online long-sequence scenarios is still limited. Existing methods either rely on slow per-scene optimization or fail to provide efficient incremental updates, hindering continuous performance. In this paper, we propose LongSplat, an online real-time 3D Gaussian reconstruction framework designed for long-sequence image input. The core idea is a streaming update mechanism that incrementally integrates current-view observations while selectively compressing redundant historical Gaussians. Crucial to this mechanism is our Gaussian-Image Representation (GIR), a representation that encodes 3D Gaussian parameters into a structured, image-like 2D format. GIR simultaneously enables efficient fusion of current-view and historical Gaussians and identity-aware redundancy compression. These functions enable online reconstruction and adapt the model to long sequences without overwhelming memory or computational costs. Furthermore, we leverage an existing image compression method to guide the generation of more compact and higher-quality 3D Gaussians. Extensive evaluations demonstrate that LongSplat achieves state-of-the-art efficiency-quality trade-offs in real-time novel view synthesis, delivering real-time reconstruction while reducing Gaussian counts by 44\% compared to existing per-pixel Gaussian prediction methods.</li>
<li><strong>摘要：</strong>3D高斯脱衣舞达到了高保真的小说视图综合，但其在线长期情景中的应用仍然有限。现有方法要么依赖于每场局部优化缓慢，要么无法提供有效的增量更新，从而阻碍了连续的性能。在本文中，我们提出了longsplat，这是一个在线实时3D高斯重建框架，旨在长期图像输入。核心想法是一种流媒体更新机制，该机制逐步整合了当前视图观测值，同时选择性地压缩了冗余的历史高斯人。对这种机制至关重要的是我们的高斯图像表示（GIR），该表示的表示，将3D高斯参数编码为结构化的，图像样的2D格式。 GIR同时实现了当前视图和历史高斯和身份感知的冗余压缩的有效融合。这些功能可以在线重建，并将模型调整为长序列，而无需压倒性的内存或计算成本。此外，我们利用一种现有的图像压缩方法来指导产生更紧凑和更高质量的3D高斯人。广泛的评估表明，与现有的人均高斯预测方法相比，Longsplat在实时小说合成中实现了最先进的效率质量折衷，实时重建，同时将高斯计数减少44％。</li>
</ul>

<h3>Title: Learning Patient-Specific Spatial Biomarker Dynamics via Operator Learning for Alzheimer's Disease Progression</h3>
<ul>
<li><strong>Authors: </strong>Jindong Wang, Yutong Mao, Xiao Liu, Wenrui Hao</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16148">https://arxiv.org/abs/2507.16148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16148">https://arxiv.org/pdf/2507.16148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16148]] Learning Patient-Specific Spatial Biomarker Dynamics via Operator Learning for Alzheimer's Disease Progression(https://arxiv.org/abs/2507.16148)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Alzheimer's disease (AD) is a complex, multifactorial neurodegenerative disorder with substantial heterogeneity in progression and treatment response. Despite recent therapeutic advances, predictive models capable of accurately forecasting individualized disease trajectories remain limited. Here, we present a machine learning-based operator learning framework for personalized modeling of AD progression, integrating longitudinal multimodal imaging, biomarker, and clinical data. Unlike conventional models with prespecified dynamics, our approach directly learns patient-specific disease operators governing the spatiotemporal evolution of amyloid, tau, and neurodegeneration biomarkers. Using Laplacian eigenfunction bases, we construct geometry-aware neural operators capable of capturing complex brain dynamics. Embedded within a digital twin paradigm, the framework enables individualized predictions, simulation of therapeutic interventions, and in silico clinical trials. Applied to AD clinical data, our method achieves high prediction accuracy exceeding 90% across multiple biomarkers, substantially outperforming existing approaches. This work offers a scalable, interpretable platform for precision modeling and personalized therapeutic optimization in neurodegenerative diseases.</li>
<li><strong>摘要：</strong>阿尔茨海默氏病（AD）是一种复杂的多因素神经退行性疾病，在进展和治疗反应中具有很大的异质性。尽管最近的治疗进展，但能够准确预测个性化疾病轨迹的预测模型仍然有限。在这里，我们提出了一个基于机器学习的操作员学习框架，用于对AD进展的个性化建模，整合纵向多模式成像，生物标志物和临床数据。与具有预先指定动力学的常规模型不同，我们的方法直接学习了淀粉样蛋白，TAU和神经变性生物标志物的时空演化的患者特异性疾病操作员。使用laplacian本本函数碱基，我们构建了能够捕获复杂脑动力学的几何学知觉神经操作员。该框架嵌入了数字双胞胎范式中，可以实现个性化预测，对治疗干预措施的模拟以及计算机临床试验。应用于AD临床数据，我们的方法实现了高预测准确性超过多个生物标志物的90％，其表现大大超过了现有方法。这项工作提供了一个可扩展的，可解释的平台，用于神经退行性疾病中的精确建模和个性化的治疗优化。</li>
</ul>

<h3>Title: LSSGen: Leveraging Latent Space Scaling in Flow and Diffusion for Efficient Text to Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Jyun-Ze Tang, Chih-Fan Hsu, Jeng-Lin Li, Ming-Ching Chang, Wei-Chao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16154">https://arxiv.org/abs/2507.16154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16154">https://arxiv.org/pdf/2507.16154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16154]] LSSGen: Leveraging Latent Space Scaling in Flow and Diffusion for Efficient Text to Image Generation(https://arxiv.org/abs/2507.16154)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Flow matching and diffusion models have shown impressive results in text-to-image generation, producing photorealistic images through an iterative denoising process. A common strategy to speed up synthesis is to perform early denoising at lower resolutions. However, traditional methods that downscale and upscale in pixel space often introduce artifacts and distortions. These issues arise when the upscaled images are re-encoded into the latent space, leading to degraded final image quality. To address this, we propose {\bf Latent Space Scaling Generation (LSSGen)}, a framework that performs resolution scaling directly in the latent space using a lightweight latent upsampler. Without altering the Transformer or U-Net architecture, LSSGen improves both efficiency and visual quality while supporting flexible multi-resolution generation. Our comprehensive evaluation covering text-image alignment and perceptual quality shows that LSSGen significantly outperforms conventional scaling approaches. When generating $1024^2$ images at similar speeds, it achieves up to 246\% TOPIQ score improvement.</li>
<li><strong>摘要：</strong>流量匹配和扩散模型在文本到图像的生成中显示出令人印象深刻的结果，从而通过迭代降解过程产生了逼真的图像。加快合成加快的常见策略是在较低的分辨率下进行早期降级。但是，在像素空间中降低和高档的传统方法通常会引入伪影和扭曲。当将高度图像重新编码为潜在空间时，就会出现这些问题，从而导致最终图像质量降低。为了解决这个问题，我们建议{\ bf潜在空间缩放生成（LSSGEN）}，该框架使用轻质潜在的UPS采样器直接在潜在空间中执行分辨率缩放。在不改变变压器或U-NET体系结构的情况下，LSSGEN可提高效率和视觉质量，同时支持灵活的多分辨率生成。我们的全面评估涵盖了文本图像一致性和感知质量，表明LSSGEN的表现明显优于常规的扩展方法。当以类似速度生成$ 1024^2 $图像时，它可获得246 \％Topiq得分的提高。</li>
</ul>

<h3>Title: AMMNet: An Asymmetric Multi-Modal Network for Remote Sensing Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Hui Ye, Haodong Chen, Zeke Zexi Hu, Xiaoming Chen, Yuk Ying Chung</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16158">https://arxiv.org/abs/2507.16158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16158">https://arxiv.org/pdf/2507.16158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16158]] AMMNet: An Asymmetric Multi-Modal Network for Remote Sensing Semantic Segmentation(https://arxiv.org/abs/2507.16158)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation in remote sensing (RS) has advanced significantly with the incorporation of multi-modal data, particularly the integration of RGB imagery and the Digital Surface Model (DSM), which provides complementary contextual and structural information about the ground object. However, integrating RGB and DSM often faces two major limitations: increased computational complexity due to architectural redundancy, and degraded segmentation performance caused by modality misalignment. These issues undermine the efficiency and robustness of semantic segmentation, particularly in complex urban environments where precise multi-modal integration is essential. To overcome these limitations, we propose Asymmetric Multi-Modal Network (AMMNet), a novel asymmetric architecture that achieves robust and efficient semantic segmentation through three designs tailored for RGB-DSM input pairs. To reduce architectural redundancy, the Asymmetric Dual Encoder (ADE) module assigns representational capacity based on modality-specific characteristics, employing a deeper encoder for RGB imagery to capture rich contextual information and a lightweight encoder for DSM to extract sparse structural features. Besides, to facilitate modality alignment, the Asymmetric Prior Fuser (APF) integrates a modality-aware prior matrix into the fusion process, enabling the generation of structure-aware contextual features. Additionally, the Distribution Alignment (DA) module enhances cross-modal compatibility by aligning feature distributions through divergence minimization. Extensive experiments on the ISPRS Vaihingen and Potsdam datasets demonstrate that AMMNet attains state-of-the-art segmentation accuracy among multi-modal networks while reducing computational and memory requirements.</li>
<li><strong>摘要：</strong>遥感（RS）中的语义分割已随着多模式数据的结合，尤其是RGB图像和数字表面模型（DSM）的整合，它具有显着提高，该数据提供了有关地面对象的互补上下文和结构信息。但是，集成RGB和DSM通常面临两个主要局限性：由于建筑冗余而增加的计算复杂性，以及由模态差异而引起的分割性能降解。这些问题破坏了语义分割的效率和鲁棒性，尤其是在精确多模式整合至关重要的复杂城市环境中。为了克服这些局限性，我们提出了一种不对称的多模式网络（AMMNET），这是一种新型的不对称结构，通过针对RGB-DSM输入对量身定制的三种设计，可实现强大而有效的语义分割。为了减少建筑冗余，不对称双编码器（ADE）模块根据特定于模态特征分配了代表能力，采用RGB图像的更深的编码器来捕获丰富的上下文信息，并为DSM提供轻巧的编码器，以提取稀疏的结构特征。此外，为了促进模态对准，不对称的先验定影（APF）将模态感知的先验矩阵集成到融合过程中，从而能够生成结构感知的上下文特征。此外，分布比对（DA）模块通过通过差异最小化来使特征分布提高了交叉模式的兼容性。 ISPRS Vaihingen和Potsdam数据集的广泛实验表明，AMMNET达到多模式网络之间的最新分割精度，同时降低了计算和内存要求。</li>
</ul>

<h3>Title: EBaReT: Expert-guided Bag Reward Transformer for Auto Bidding</h3>
<ul>
<li><strong>Authors: </strong>Kaiyuan Li, Pengyu Wang, Yunshan Peng, Pengjia Yuan, Yanxiang Zeng, Rui Xiang, Yanhua Cheng, Xialong Liu, Peng Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16186">https://arxiv.org/abs/2507.16186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16186">https://arxiv.org/pdf/2507.16186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16186]] EBaReT: Expert-guided Bag Reward Transformer for Auto Bidding(https://arxiv.org/abs/2507.16186)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Reinforcement learning has been widely applied in automated bidding. Traditional approaches model bidding as a Markov Decision Process (MDP). Recently, some studies have explored using generative reinforcement learning methods to address long-term dependency issues in bidding environments. Although effective, these methods typically rely on supervised learning approaches, which are vulnerable to low data quality due to the amount of sub-optimal bids and low probability rewards resulting from the low click and conversion rates. Unfortunately, few studies have addressed these challenges. In this paper, we formalize the automated bidding as a sequence decision-making problem and propose a novel Expert-guided Bag Reward Transformer (EBaReT) to address concerns related to data quality and uncertainty rewards. Specifically, to tackle data quality issues, we generate a set of expert trajectories to serve as supplementary data in the training process and employ a Positive-Unlabeled (PU) learning-based discriminator to identify expert transitions. To ensure the decision also meets the expert level, we further design a novel expert-guided inference strategy. Moreover, to mitigate the uncertainty of rewards, we consider the transitions within a certain period as a "bag" and carefully design a reward function that leads to a smoother acquisition of rewards. Extensive experiments demonstrate that our model achieves superior performance compared to state-of-the-art bidding methods.</li>
<li><strong>摘要：</strong>强化学习已被广泛应用于自动招标中。传统方法将模型竞标作为马尔可夫决策过程（MDP）。最近，一些研究使用生成强化学习方法探讨了解决竞标环境中的长期依赖问题。尽管有效，但这些方法通常依赖于监督的学习方法，由于低点击率和转换率引起的次优率和低概率奖励，因此容易受到低数据质量的影响。不幸的是，很少有研究解决这些挑战。在本文中，我们将自动竞标正式为序列决策问题，并提出了一种新颖的专家指导的奖励奖励变压器（EBARET），以解决与数据质量和不确定性奖励有关的问题。具体来说，为了解决数据质量问题，我们生成了一组专家轨迹，以作为培训过程中的补充数据，并采用积极的（PU）基于学习的歧视者来识别专家过渡。为了确保该决定还达到专家级别，我们进一步设计了一种新颖的专家指导推理策略。此外，为了减轻奖励的不确定性，我们将特定时期内的过渡视为“袋子”，并仔细设计了奖励功能，从而导致奖励更加顺畅。广泛的实验表明，与最先进的招标方法相比，我们的模型实现了卓越的性能。</li>
</ul>

<h3>Title: RealBench: Benchmarking Verilog Generation Models with Real-World IP Designs</h3>
<ul>
<li><strong>Authors: </strong>Pengwei Jin, Di Huang, Chongxiao Li, Shuyao Cheng, Yang Zhao, Xinyao Zheng, Jiaguo Zhu, Shuyi Xing, Bohan Dou, Rui Zhang, Zidong Du, Qi Guo, Xing Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16200">https://arxiv.org/abs/2507.16200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16200">https://arxiv.org/pdf/2507.16200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16200]] RealBench: Benchmarking Verilog Generation Models with Real-World IP Designs(https://arxiv.org/abs/2507.16200)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The automatic generation of Verilog code using Large Language Models (LLMs) has garnered significant interest in hardware design automation. However, existing benchmarks for evaluating LLMs in Verilog generation fall short in replicating real-world design workflows due to their designs' simplicity, inadequate design specifications, and less rigorous verification environments. To address these limitations, we present RealBench, the first benchmark aiming at real-world IP-level Verilog generation tasks. RealBench features complex, structured, real-world open-source IP designs, multi-modal and formatted design specifications, and rigorous verification environments, including 100% line coverage testbenches and a formal checker. It supports both module-level and system-level tasks, enabling comprehensive assessments of LLM capabilities. Evaluations on various LLMs and agents reveal that even one of the best-performing LLMs, o1-preview, achieves only a 13.3% pass@1 on module-level tasks and 0% on system-level tasks, highlighting the need for stronger Verilog generation models in the future. The benchmark is open-sourced at this https URL.</li>
<li><strong>摘要：</strong>使用大语言模型（LLM）自动生成Verilog代码对硬件设计自动化产生了浓厚的兴趣。但是，由于设计规格的简单性，不足的设计规格和不太严格的验证环境，用于评估Verilog生成中LLM的现有基准在复制现实世界设计工作流方面缺乏。为了解决这些局限性，我们提出了RealBench，这是针对现实世界IP级Verilog生成任务的第一个基准。 RealBench具有复杂的，结构化的，现实的开源IP设计，多模式和格式化设计规范以及严格的验证环境，包括100％的线路覆盖台测试台和正式检查器。它支持模块级和系统级任务，从而实现对LLM功能的全面评估。对各种LLM和代理商的评估表明，即使是表现最好的LLM，O1-preiview，在模块级任务上仅达到13.3％的通行证，而在系统级别的任务上也只能达到0％，强调了将来对强verilog生成模型的需求。基准在此HTTPS URL上开源。</li>
</ul>

<h3>Title: METER: Multi-modal Evidence-based Thinking and Explainable Reasoning -- Algorithm and Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Xu Yang, Qi Zhang, Shuming Jiang, Yaowen Xu, Zhaofan Zou, Hao Sun, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16206">https://arxiv.org/abs/2507.16206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16206">https://arxiv.org/pdf/2507.16206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16206]] METER: Multi-modal Evidence-based Thinking and Explainable Reasoning -- Algorithm and Benchmark(https://arxiv.org/abs/2507.16206)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of generative AI, synthetic content across images, videos, and audio has become increasingly realistic, amplifying the risk of misinformation. Existing detection approaches predominantly focus on binary classification while lacking detailed and interpretable explanations of forgeries, which limits their applicability in safety-critical scenarios. Moreover, current methods often treat each modality separately, without a unified benchmark for cross-modal forgery detection and interpretation. To address these challenges, we introduce METER, a unified, multi-modal benchmark for interpretable forgery detection spanning images, videos, audio, and audio-visual content. Our dataset comprises four tracks, each requiring not only real-vs-fake classification but also evidence-chain-based explanations, including spatio-temporal localization, textual rationales, and forgery type tracing. Compared to prior benchmarks, METER offers broader modality coverage and richer interpretability metrics such as spatial/temporal IoU, multi-class tracing, and evidence consistency. We further propose a human-aligned, three-stage Chain-of-Thought (CoT) training strategy combining SFT, DPO, and a novel GRPO stage that integrates a human-aligned evaluator with CoT reasoning. We hope METER will serve as a standardized foundation for advancing generalizable and interpretable forgery detection in the era of generative media.</li>
<li><strong>摘要：</strong>随着生成AI的快速发展，图像，视频和音频的合成内容已变得越来越现实，从而扩大了错误信息的风险。现有的检测方法主要集中于二进制分类，同时缺乏对伪造的详细和可解释的解释，这限制了其在安全至关重要的情况下的适用性。此外，当前方法通常会分别分别处理每种模式，而无需用于跨模式伪造的检测和解释的统一基准。为了应对这些挑战，我们介绍了仪表，这是一种统一的多模式基准，用于可解释的伪造检测图像，视频，音频和视听内容。我们的数据集包括四个轨道，每个轨道不仅需要Real-Vs-Fake分类，还需要基于证据链的解释，包括时空定位，文本理由和伪造类型的跟踪。与先前的基准相比，仪表提供更广泛的方式覆盖范围和更丰富的可解释性指标，例如空间/时间iOU，多级跟踪和证据一致性。我们进一步提出了一个与人类一致的三阶段链（COT）培训策略，结合了SFT，DPO和一个新颖的GRPO阶段，将人类一致的评估者与COT推理集成在一起。我们希望仪表将成为在生成媒体时代推进可推广和可解释的伪造发现的标准化基础。</li>
</ul>

<h3>Title: Advancing Visual Large Language Model for Multi-granular Versatile Perception</h3>
<ul>
<li><strong>Authors: </strong>Wentao Xiang, Haoxian Tan, Cong Wei, Yujie Zhong, Dengjie Li, Yujiu Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16213">https://arxiv.org/abs/2507.16213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16213">https://arxiv.org/pdf/2507.16213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16213]] Advancing Visual Large Language Model for Multi-granular Versatile Perception(https://arxiv.org/abs/2507.16213)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Perception is a fundamental task in the field of computer vision, encompassing a diverse set of subtasks that can be systematically categorized into four distinct groups based on two dimensions: prediction type and instruction type. Notably, existing researches often focus solely on a limited subset of these potential combinations, which constrains their applicability and versatility across various contexts. In response to this challenge, we present MVP-LM, a Multi-granular and Versatile Perception framework incorporating Visual Large Language Model. Our framework is designed to integrate both word-based and sentence-based perception tasks alongside box and mask predictions within a single architecture. MVP-LM features an innovative multi-granularity decoder in conjunction with a CoT-inspired dataset unification strategy, enabling seamless supervised fine-tuning across a wide spectrum of tasks, including but not limited to panoptic segmentation, detection, grounding, and referring expression segmentation. Furthermore, we introduce a query enhancement strategy aimed at harnessing the decoding and generative capabilities inherent in VLLMs. Extensive experiments conducted across a range of benchmarks in both word-based and sentence-based perception tasks substantiate the efficacy of our framework. The code will be available at this https URL.</li>
<li><strong>摘要：</strong>感知是计算机视觉领域的一项基本任务，涵盖了一组各种子任务，这些子任务可以根据两个维度系统地分类为四个不同的组：预测类型和指令类型。值得注意的是，现有的研究通常仅关注这些潜在组合的有限子集，从而限制了它们在各种情况下的适用性和多功能性。为了应对这一挑战，我们提出了MVP-LM，这是一种包含视觉大语言模型的多粒和多功能感知框架。我们的框架旨在将基于单词和基于句子的感知任务与单个体系结构中的框和掩盖预测集成在一起。 MVP-LM与COT启发的数据集统一策略结合使用了创新的多粒性解码器，从而实现了各种任务的无缝监督微调，包括但不限于全磁带细分，检测，接地，接地，接地，接地，参考表达分裂。此外，我们引入了一种旨在利用VLLM固有的解码和生成能力的查询增强策略。基于单词和基于句子的感知任务的一系列基准进行了广泛的实验证实了我们框架的功效。该代码将在此HTTPS URL上可用。</li>
</ul>

<h3>Title: Scale Your Instructions: Enhance the Instruction-Following Fidelity of Unified Image Generation Model by Self-Adaptive Attention Scaling</h3>
<ul>
<li><strong>Authors: </strong>Chao Zhou, Tianyi Wei, Nenghai Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16240">https://arxiv.org/abs/2507.16240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16240">https://arxiv.org/pdf/2507.16240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16240]] Scale Your Instructions: Enhance the Instruction-Following Fidelity of Unified Image Generation Model by Self-Adaptive Attention Scaling(https://arxiv.org/abs/2507.16240)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advancements in unified image generation models, such as OmniGen, have enabled the handling of diverse image generation and editing tasks within a single framework, accepting multimodal, interleaved texts and images in free form. This unified architecture eliminates the need for text encoders, greatly reducing model complexity and standardizing various image generation and editing tasks, making it more user-friendly. However, we found that it suffers from text instruction neglect, especially when the text instruction contains multiple sub-instructions. To explore this issue, we performed a perturbation analysis on the input to identify critical steps and layers. By examining the cross-attention maps of these key steps, we observed significant conflicts between neglected sub-instructions and the activations of the input image. In response, we propose Self-Adaptive Attention Scaling (SaaS), a method that leverages the consistency of cross-attention between adjacent timesteps to dynamically scale the attention activation for each sub-instruction. Our SaaS enhances instruction-following fidelity without requiring additional training or test-time optimization. Experimental results on instruction-based image editing and visual conditional image generation validate the effectiveness of our SaaS, showing superior instruction-following fidelity over existing methods. The code is available this https URL.</li>
<li><strong>摘要：</strong>统一图像生成模型（例如Omnigen）的最新进步已使单个框架内的多样化图像生成和编辑任务可以接受自由形式的多模式，交织的文本和图像。这种统一的体系结构消除了对文本编码器的需求，大大降低了模型的复杂性并标准化了各种图像生成和编辑任务，从而使其更加用户友好。但是，我们发现它遭受了文本指令的忽视，尤其是当文本指令包含多个子指导时。为了探索这个问题，我们对输入进行了扰动分析，以识别关键步骤和层。通过检查这些关键步骤的跨注意地图，我们观察到被忽视的子指导与输入图像的激活之间存在显着冲突。作为回应，我们提出了自适应注意力缩放（SAAS），这种方法利用相邻时间段之间的交叉注意力一致性，以动态扩展每个子指导的注意力激活。我们的SaaS增强了跟随忠诚度的指导，而无需额外的培训或测试时间优化。基于教学的图像编辑和视觉条件图像产生的实验结果验证了我们的SaaS的有效性，显示出比现有方法相比的较高的指令忠诚度。该代码可用此HTTPS URL。</li>
</ul>

<h3>Title: Edge-case Synthesis for Fisheye Object Detection: A Data-centric Perspective</h3>
<ul>
<li><strong>Authors: </strong>Seunghyeon Kim, Kyeongryeol Go</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16254">https://arxiv.org/abs/2507.16254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16254">https://arxiv.org/pdf/2507.16254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16254]] Edge-case Synthesis for Fisheye Object Detection: A Data-centric Perspective(https://arxiv.org/abs/2507.16254)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Fisheye cameras introduce significant distortion and pose unique challenges to object detection models trained on conventional datasets. In this work, we propose a data-centric pipeline that systematically improves detection performance by focusing on the key question of identifying the blind spots of the model. Through detailed error analysis, we identify critical edge-cases such as confusing class pairs, peripheral distortions, and underrepresented contexts. Then we directly address them through edge-case synthesis. We fine-tuned an image generative model and guided it with carefully crafted prompts to produce images that replicate real-world failure modes. These synthetic images are pseudo-labeled using a high-quality detector and integrated into training. Our approach results in consistent performance gains, highlighting how deeply understanding data and selectively fixing its weaknesses can be impactful in specialized domains like fisheye object detection.</li>
<li><strong>摘要：</strong>Fisheye摄像机引起了严重的失真，并对在常规数据集进行训练的对象检测模型构成了独特的挑战。在这项工作中，我们提出了一个以数据为中心的管道，该管道通过关注识别模型的盲点的关键问题来系统地改善检测性能。通过详细的错误分析，我们确定了关键的边缘案例，例如混淆的班级对，周围扭曲和代表性不足的上下文。然后，我们通过边缘案例合成直接解决它们。我们对图像生成模型进行了微调，并用精心制作的提示来指导它，以产生复制现实世界故障模式的图像。这些合成图像使用高质量检测器进行伪标记，并集成到训练中。我们的方法会导致一致的性能提高，强调了如何深入了解数据并有选择地固定其弱点可能会影响Fisheye对象检测等专业领域。</li>
</ul>

<h3>Title: Towards Resilient Safety-driven Unlearning for Diffusion Models against Downstream Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Boheng Li, Renjie Gu, Junjie Wang, Leyi Qi, Yiming Li, Run Wang, Zhan Qin, Tianwei Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16302">https://arxiv.org/abs/2507.16302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16302">https://arxiv.org/pdf/2507.16302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16302]] Towards Resilient Safety-driven Unlearning for Diffusion Models against Downstream Fine-tuning(https://arxiv.org/abs/2507.16302)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) diffusion models have achieved impressive image generation quality and are increasingly fine-tuned for personalized applications. However, these models often inherit unsafe behaviors from toxic pretraining data, raising growing safety concerns. While recent safety-driven unlearning methods have made promising progress in suppressing model toxicity, they are identified to be fragile to downstream fine-tuning, where we reveal that state-of-the-art methods largely fail to retain their effectiveness even when fine-tuned on entirely benign datasets. To mitigate this problem, in this paper, we propose ResAlign, a safety-driven unlearning framework with enhanced resilience against downstream fine-tuning. By modeling downstream fine-tuning as an implicit optimization problem with a Moreau Envelope-based reformulation, ResAlign enables efficient gradient estimation to minimize the recovery of harmful behaviors. Additionally, a meta-learning strategy is proposed to simulate a diverse distribution of fine-tuning scenarios to improve generalization. Extensive experiments across a wide range of datasets, fine-tuning methods, and configurations demonstrate that ResAlign consistently outperforms prior unlearning approaches in retaining safety after downstream fine-tuning while preserving benign generation capability well.</li>
<li><strong>摘要：</strong>文本对图像（T2I）扩散模型已达到令人印象深刻的图像产生质量，并且越来越多地针对个性化应用程序进行了微调。但是，这些模型通常会从有毒预处理数据中继承不安全的行为，从而增加了安全问题。尽管最近以安全驱动的学习方法在抑制模型毒性方面取得了希望的进展，但它们被确定为下游微调脆弱，我们揭示了最先进的方法在很大程度上也无法保持其有效性，即使在完全对完全良性数据集上进行了微调。为了减轻这个问题，在本文中，我们提出了重新安装，这是一个由安全驱动的未学习框架，具有增强的弹性，反对下游微调。通过将下游微调建模为基于Moreau Invelope的重新制定的隐式优化问题，Resalign可以实现有效的梯度估计，以最大程度地减少有害行为的恢复。此外，提出了一种元学习策略，以模拟微调方案的各种分布以改善概括。跨广泛的数据集，微调方法和配置进行了广泛的实验表明，在下游微调之后，重新统一在保持安全性的同时保持良性生成能力后，始终超越了先前的学习方法。</li>
</ul>

<h3>Title: Perovskite-R1: A Domain-Specialized LLM for Intelligent Discovery of Precursor Additives and Experimental Design</h3>
<ul>
<li><strong>Authors: </strong>Xin-De Wang, Zhi-Rui Chen, Peng-Jie Guo, Ze-Feng Gao, Cheng Mu, Zhong-Yi Lu</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci, cs.AI, physics.chem-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16307">https://arxiv.org/abs/2507.16307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16307">https://arxiv.org/pdf/2507.16307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16307]] Perovskite-R1: A Domain-Specialized LLM for Intelligent Discovery of Precursor Additives and Experimental Design(https://arxiv.org/abs/2507.16307)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Perovskite solar cells (PSCs) have rapidly emerged as a leading contender in next-generation photovoltaic technologies, owing to their exceptional power conversion efficiencies and advantageous material properties. Despite these advances, challenges such as long-term stability, environmental sustainability, and scalable manufacturing continue to hinder their commercialization. Precursor additive engineering has shown promise in addressing these issues by enhancing both the performance and durability of PSCs. However, the explosive growth of scientific literature and the complex interplay of materials, processes, and device architectures make it increasingly difficult for researchers to efficiently access, organize, and utilize domain knowledge in this rapidly evolving field. To address this gap, we introduce Perovskite-R1, a specialized large language model (LLM) with advanced reasoning capabilities tailored for the discovery and design of PSC precursor additives. By systematically mining and curating 1,232 high-quality scientific publications and integrating a comprehensive library of 33,269 candidate materials, we constructed a domain-specific instruction-tuning dataset using automated question-answer generation and chain-of-thought reasoning. Fine-tuning the QwQ-32B model on this dataset resulted in Perovskite-R1, which can intelligently synthesize literature insights and generate innovative and practical solutions for defect passivation and the selection of precursor additives. Experimental validation of several model-proposed strategies confirms their effectiveness in improving material stability and performance. Our work demonstrates the potential of domain-adapted LLMs in accelerating materials discovery and provides a closed-loop framework for intelligent, data-driven advancements in perovskite photovoltaic research.</li>
<li><strong>摘要：</strong>由于其出色的功率转换效率和有利的材料特性，钙钛矿太阳能电池（PSC）已迅速成为下一代光伏技术的主要竞争者。尽管有这些进展，但诸如长期稳定，环境可持续性和可扩展制造之类的挑战继续阻碍其商业化。 Precursor添加剂工程通过提高PSC的性能和耐用性来解决这些问题的希望。但是，科学文献的爆炸性增长以及材料，工艺和设备架构的复杂相互作用使研究人员越来越难以在这个迅速发展的领域中有效访问，组织和利用领域知识。为了解决这一差距，我们介绍了Perovskite-R1，这是一种专门的大型语言模型（LLM），其高级推理能力是为PSC前体添加剂的发现和设计而定制的。通过系统地采矿和策划1,232个高质量的科学出版物，并整合了33,269种候选材料的综合库，我们使用自动化的问答产生和经过思考的推理构建了一个特定领域的指令数据集。对该数据集上的QWQ-32B模型进行微调导致了Perovskite-R1，可以明智地综合文献见解并生成创新和实用的解决方案，以实现缺陷钝化和选择前体添加剂。对几种模型策略的实验验证证实了它们在改善材料稳定性和性能方面的有效性。我们的工作证明了适应域的LLM在加速材料发现中的潜力，并为perovskite光伏研究中的智能，数据驱动的进步提供了一个闭环框架。</li>
</ul>

<h3>Title: MotionShot: Adaptive Motion Transfer across Arbitrary Objects for Text-to-Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Yanchen Liu, Yanan Sun, Zhening Xing, Junyao Gao, Kai Chen, Wenjie Pei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16310">https://arxiv.org/abs/2507.16310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16310">https://arxiv.org/pdf/2507.16310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16310]] MotionShot: Adaptive Motion Transfer across Arbitrary Objects for Text-to-Video Generation(https://arxiv.org/abs/2507.16310)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Existing text-to-video methods struggle to transfer motion smoothly from a reference object to a target object with significant differences in appearance or structure between them. To address this challenge, we introduce MotionShot, a training-free framework capable of parsing reference-target correspondences in a fine-grained manner, thereby achieving high-fidelity motion transfer while preserving coherence in appearance. To be specific, MotionShot first performs semantic feature matching to ensure high-level alignments between the reference and target objects. It then further establishes low-level morphological alignments through reference-to-target shape retargeting. By encoding motion with temporal attention, our MotionShot can coherently transfer motion across objects, even in the presence of significant appearance and structure disparities, demonstrated by extensive experiments. The project page is available at: this https URL.</li>
<li><strong>摘要：</strong>现有的文本到视频方法难以将运动从参考对象顺利传输到目标对象，其外观或结构之间存在显着差异。为了应对这一挑战，我们介绍了Motionshot，这是一个无训练的框架，能够以细粒度的方式解析参考目标对应，从而实现了高保真运动转移，同时保持外观相干性。具体而言，Motionshot首先执行语义功能匹配，以确保参考对象和目标对象之间的高级比对。然后，它通过重新定位靶向靶向形状进一步建立了低水平的形态对准。通过以时间关注编码运动，我们的动作可以连贯地转移运动，即使在存在明显的外观和结构差异的情况下，也可以通过广泛的实验证明。项目页面可用：此HTTPS URL。</li>
</ul>

<h3>Title: One Polyp Identifies All: One-Shot Polyp Segmentation with SAM via Cascaded Priors and Iterative Prompt Evolution</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Mao, Xiaohan Xing, Fei Meng, Jianbang Liu, Fan Bai, Qiang Nie, Max Meng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16337">https://arxiv.org/abs/2507.16337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16337">https://arxiv.org/pdf/2507.16337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16337]] One Polyp Identifies All: One-Shot Polyp Segmentation with SAM via Cascaded Priors and Iterative Prompt Evolution(https://arxiv.org/abs/2507.16337)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Polyp segmentation is vital for early colorectal cancer detection, yet traditional fully supervised methods struggle with morphological variability and domain shifts, requiring frequent retraining. Additionally, reliance on large-scale annotations is a major bottleneck due to the time-consuming and error-prone nature of polyp boundary labeling. Recently, vision foundation models like Segment Anything Model (SAM) have demonstrated strong generalizability and fine-grained boundary detection with sparse prompts, effectively addressing key polyp segmentation challenges. However, SAM's prompt-dependent nature limits automation in medical applications, since manually inputting prompts for each image is labor-intensive and time-consuming. We propose OP-SAM, a One-shot Polyp segmentation framework based on SAM that automatically generates prompts from a single annotated image, ensuring accurate and generalizable segmentation without additional annotation burdens. Our method introduces Correlation-based Prior Generation (CPG) for semantic label transfer and Scale-cascaded Prior Fusion (SPF) to adapt to polyp size variations as well as filter out noisy transfers. Instead of dumping all prompts at once, we devise Euclidean Prompt Evolution (EPE) for iterative prompt refinement, progressively enhancing segmentation quality. Extensive evaluations across five datasets validate OP-SAM's effectiveness. Notably, on Kvasir, it achieves 76.93% IoU, surpassing the state-of-the-art by 11.44%.</li>
<li><strong>摘要：</strong>息肉分割对于早期结直肠癌检测至关重要，但是传统的完全监督的方法与形态学变异性和域移位障碍，需要经常进行重新训练。此外，由于息肉边界标记的时间耗时且容易发生，对大规模注释的依赖是主要的瓶颈。最近，视力基础模型（例如任何模型（SAM））表现出强烈的概括性和稀疏提示的细粒边界检测，有效地解决了关键的息肉细分挑战。但是，山姆迅速依赖性的性质限制了医疗应用中的自动化，因为为每个图像的手动输入提示是劳动力密集的且耗时的。我们提出了OP-SAM，这是一种基于SAM的单发息肉分割框架，该框架会自动从单个注释的图像中生成提示，从而确保准确且可推广的分割，而无需额外的注释负担。我们的方法引入了基于相关的先前一代（CPG），用于语义标签传输和比例粘附的先前融合（SPF），以适应息肉大小的变化，并过滤噪音转移。我们没有立即倾倒所有提示，而是设计了欧几里得及时进化（EPE）进行迭代及时精致，从而逐步提高了细分质量。五个数据集的广泛评估验证了OP-SAM的有效性。值得注意的是，在Kvasir上，它可以达到76.93％的IOU，超过了11.44％的最先进。</li>
</ul>

<h3>Title: STAR: A Benchmark for Astronomical Star Fields Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Kuo-Cheng Wu, Guohang Zhuang, Jinyang Huang, Xiang Zhang, Wanli Ouyang, Yan Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16385">https://arxiv.org/abs/2507.16385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16385">https://arxiv.org/pdf/2507.16385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16385]] STAR: A Benchmark for Astronomical Star Fields Super-Resolution(https://arxiv.org/abs/2507.16385)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generation</a></li>
<li><strong>Abstract: </strong>Super-resolution (SR) advances astronomical imaging by enabling cost-effective high-resolution capture, crucial for detecting faraway celestial objects and precise structural analysis. However, existing datasets for astronomical SR (ASR) exhibit three critical limitations: flux inconsistency, object-crop setting, and insufficient data diversity, significantly impeding ASR development. We propose STAR, a large-scale astronomical SR dataset containing 54,738 flux-consistent star field image pairs covering wide celestial regions. These pairs combine Hubble Space Telescope high-resolution observations with physically faithful low-resolution counterparts generated through a flux-preserving data generation pipeline, enabling systematic development of field-level ASR models. To further empower the ASR community, STAR provides a novel Flux Error (FE) to evaluate SR models in physical view. Leveraging this benchmark, we propose a Flux-Invariant Super Resolution (FISR) model that could accurately infer the flux-consistent high-resolution images from input photometry, suppressing several SR state-of-the-art methods by 24.84% on a novel designed flux consistency metric, showing the priority of our method for astrophysics. Extensive experiments demonstrate the effectiveness of our proposed method and the value of our dataset. Code and models are available at this https URL.</li>
<li><strong>摘要：</strong>超分辨率（SR）通过实现具有成本效益的高分辨率捕获来推进天文成像，对于检测遥远的天体对象和精确的结构分析至关重要。但是，现有的天文学SR（ASR）数据集表现出三个关键局限性：通量不一致，对象扭曲设置和数据多样性不足，会大大阻碍ASR的发展。我们提出了一个大规模的天文SR数据集，其中包含54,738次通量的恒星野外图像对，覆盖了宽阔的天体区域。这些对结合了哈勃太空望远镜高分辨率观测值与通过易变的数据生成管道生成的物理忠实的低分辨率对应物，从而实现了现场级别ASR模型的系统开发。为了进一步增强ASR社区的能力，Star提供了一种新颖的通量误差（FE）来评估物理视图中的SR模型。利用此基准测试，我们提出了一种通量不变的超级分辨率（FISR）模型，该模型可以从输入光度法中准确地推断出通量一致的高分辨率图像，从而在一种新型设计的磁通量指标上抑制了几种SR的SR先进方法，以24.84％的速度抑制了我们的天文学方法的优先级。广泛的实验证明了我们提出的方法的有效性和数据集的价值。代码和型号可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Sparse-View 3D Reconstruction: Recent Advances and Open Challenges</h3>
<ul>
<li><strong>Authors: </strong>Tanveer Younis, Zhanglin Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16406">https://arxiv.org/abs/2507.16406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16406">https://arxiv.org/pdf/2507.16406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16406]] Sparse-View 3D Reconstruction: Recent Advances and Open Challenges(https://arxiv.org/abs/2507.16406)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Sparse-view 3D reconstruction is essential for applications in which dense image acquisition is impractical, such as robotics, augmented/virtual reality (AR/VR), and autonomous systems. In these settings, minimal image overlap prevents reliable correspondence matching, causing traditional methods, such as structure-from-motion (SfM) and multiview stereo (MVS), to fail. This survey reviews the latest advances in neural implicit models (e.g., NeRF and its regularized versions), explicit point-cloud-based approaches (e.g., 3D Gaussian Splatting), and hybrid frameworks that leverage priors from diffusion and vision foundation models (VFMs).We analyze how geometric regularization, explicit shape modeling, and generative inference are used to mitigate artifacts such as floaters and pose ambiguities in sparse-view settings. Comparative results on standard benchmarks reveal key trade-offs between the reconstruction accuracy, efficiency, and generalization. Unlike previous reviews, our survey provides a unified perspective on geometry-based, neural implicit, and generative (diffusion-based) methods. We highlight the persistent challenges in domain generalization and pose-free reconstruction and outline future directions for developing 3D-native generative priors and achieving real-time, unconstrained sparse-view reconstruction.</li>
<li><strong>摘要：</strong>稀疏视图3D重建对于密集图像采集不切实际的应用至关重要，例如机器人技术，增强/虚拟现实（AR/VR）和自主系统。在这些设置中，最小图像重叠会阻止可靠的对应关系匹配，从而导致传统方法，例如结构 - 轻度（SFM）和Multiview STEREO（MVS）失败。这项调查回顾了神经隐式模型（例如NERF及其正则化版本）的最新进展，明确的基于点云的方法（例如3D高斯分裂）以及利用传统和视觉基础模型（VFMS）中的质量定期化的模型，以及生成的模型，并将其利用的混合框架，这些框架利用了范围的构图，并分解了视觉和视觉模型（VFMS）。稀疏视图设置中的浮动器和姿势歧义。标准基准测试的比较结果揭示了重建精度，效率和概括之间的关键权衡。与以前的评论不同，我们的调查提供了基于几何，神经隐式和生成（基于扩散的）方法的统一观点。我们强调了领域概括和无姿势重建方面的持续挑战，并概述了未来的方向，以开发3D新的生成先验，并实现实时，无约束的稀疏视图重建。</li>
</ul>

<h3>Title: DenseSR: Image Shadow Removal as Dense Prediction</h3>
<ul>
<li><strong>Authors: </strong>Yu-Fan Lin, Chia-Ming Lee, Chih-Chung Hsu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16472">https://arxiv.org/abs/2507.16472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16472">https://arxiv.org/pdf/2507.16472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16472]] DenseSR: Image Shadow Removal as Dense Prediction(https://arxiv.org/abs/2507.16472)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Shadows are a common factor degrading image quality. Single-image shadow removal (SR), particularly under challenging indirect illumination, is hampered by non-uniform content degradation and inherent ambiguity. Consequently, traditional methods often fail to simultaneously recover intra-shadow details and maintain sharp boundaries, resulting in inconsistent restoration and blurring that negatively affect both downstream applications and the overall viewing experience. To overcome these limitations, we propose the DenseSR, approaching the problem from a dense prediction perspective to emphasize restoration quality. This framework uniquely synergizes two key strategies: (1) deep scene understanding guided by geometric-semantic priors to resolve ambiguity and implicitly localize shadows, and (2) high-fidelity restoration via a novel Dense Fusion Block (DFB) in the decoder. The DFB employs adaptive component processing-using an Adaptive Content Smoothing Module (ACSM) for consistent appearance and a Texture-Boundary Recuperation Module (TBRM) for fine textures and sharp boundaries-thereby directly tackling the inconsistent restoration and blurring issues. These purposefully processed components are effectively fused, yielding an optimized feature representation preserving both consistency and fidelity. Extensive experimental results demonstrate the merits of our approach over existing methods. Our code can be available on https://github$.$com/VanLinLin/DenseSR</li>
<li><strong>摘要：</strong>阴影是降低图像质量的常见因素。单位图像去除（SR），尤其是在有挑战性的间接照明下，受到不一致的内容退化和固有的歧义的阻碍。因此，传统方法通常无法同时恢复阴影内部细节并保持尖锐的边界，从而导致不一致的恢复和模糊，从而对下游应用和整体观看体验产生负面影响。为了克服这些局限性，我们提出了登记，从密集的预测角度来解决问题，以强调恢复质量。该框架独特地协同了两种关键策略：（1）由几何语言先验者指导的深层场景，以解决歧义并隐式地定位阴影，以及（2）通过在解码器中通过新颖的密集融合块（DFB）进行高保真修复。 DFB采用自适应成分处理 - 使用自适应内容平滑模块（ACSM），以保持一致的外观和纹理结合的恢复模块（TBRM），以进行细质地，尖锐的边界 - 直接解决不一致的恢复和问题。这些有目的处理的组件有效地融合在一起，得出了保持一致性和保真度的优化特征表示。广泛的实验结果证明了我们在现有方法上的优点。我们的代码可以在https：//githubqum.qum/vanlinlin/densesr上提供</li>
</ul>

<h3>Title: EarthCrafter: Scalable 3D Earth Generation via Dual-Sparse Latent Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Shang Liu, Chenjie Cao, Chaohui Yu, Wen Qian, Jing Wang, Fan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16535">https://arxiv.org/abs/2507.16535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16535">https://arxiv.org/pdf/2507.16535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16535]] EarthCrafter: Scalable 3D Earth Generation via Dual-Sparse Latent Diffusion(https://arxiv.org/abs/2507.16535)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite the remarkable developments achieved by recent 3D generation works, scaling these methods to geographic extents, such as modeling thousands of square kilometers of Earth's surface, remains an open challenge. We address this through a dual innovation in data infrastructure and model architecture. First, we introduce Aerial-Earth3D, the largest 3D aerial dataset to date, consisting of 50k curated scenes (each measuring 600m x 600m) captured across the U.S. mainland, comprising 45M multi-view Google Earth frames. Each scene provides pose-annotated multi-view images, depth maps, normals, semantic segmentation, and camera poses, with explicit quality control to ensure terrain diversity. Building on this foundation, we propose EarthCrafter, a tailored framework for large-scale 3D Earth generation via sparse-decoupled latent diffusion. Our architecture separates structural and textural generation: 1) Dual sparse 3D-VAEs compress high-resolution geometric voxels and textural 2D Gaussian Splats (2DGS) into compact latent spaces, largely alleviating the costly computation suffering from vast geographic scales while preserving critical information. 2) We propose condition-aware flow matching models trained on mixed inputs (semantics, images, or neither) to flexibly model latent geometry and texture features independently. Extensive experiments demonstrate that EarthCrafter performs substantially better in extremely large-scale generation. The framework further supports versatile applications, from semantic-guided urban layout generation to unconditional terrain synthesis, while maintaining geographic plausibility through our rich data priors from Aerial-Earth3D.</li>
<li><strong>摘要：</strong>尽管最近的3D代作品取得了显着的发展，但将这些方法扩展到地理范围（例如对数千平方公里的地球表面建模）仍然是一个开放的挑战。我们通过数据基础架构和模型体系结构的双重创新来解决此问题。首先，我们介绍了迄今为止最大的3D空中数据集的Perial-Earth3d，由50K策划的场景（每个均衡的场景（每个尺寸为600m x 600m））组成，包括4500万多视图的Google Earth Frames。每个场景都提供姿势注销的多视图图像，深度图，正常地位，语义分割和相机姿势，并具有明确的质量控制，以确保地形多样性。在这个基础的基础上，我们提出了EarthCrafter，这是一个通过稀疏耦合的潜在扩散来量身定制的3D地球生成的框架。我们的架构将结构和纹理产生分开：1）双重稀疏3D-VAE压缩高分辨率几何体素和纹理2D高斯（2DGS）（2DGS）中的紧凑型潜在空间，从而在很大程度上减轻了在保留关键信息的同时，在很大程度上减轻了遭受巨大地理位置的昂贵计算。 2）我们提出了在混合输入（语义，图像或两者）上训练的条件感知流量匹配模型，以灵活地对潜在的几何形状和纹理进行独立建模。广泛的实验表明，在极大的生成中，Earthcrafter的性能要好得多。该框架进一步支持多功能应用，从语义引导的城市布局产生到无条件的地形综合，同时通过我们的Aerial-Earth3d来维持地理上的合理性。</li>
</ul>

<h3>Title: Automatic Fine-grained Segmentation-assisted Report Generation</h3>
<ul>
<li><strong>Authors: </strong>Frederic Jonske, Constantin Seibold, Osman Alperen Koras, Fin Bahnsen, Marie Bauer, Amin Dada, Hamza Kalisch, Anton Schily, Jens Kleesiek</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16623">https://arxiv.org/abs/2507.16623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16623">https://arxiv.org/pdf/2507.16623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16623]] Automatic Fine-grained Segmentation-assisted Report Generation(https://arxiv.org/abs/2507.16623)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reliable end-to-end clinical report generation has been a longstanding goal of medical ML research. The end goal for this process is to alleviate radiologists' workloads and provide second opinions to clinicians or patients. Thus, a necessary prerequisite for report generation models is a strong general performance and some type of innate grounding capability, to convince clinicians or patients of the veracity of the generated reports. In this paper, we present ASaRG (\textbf{A}utomatic \textbf{S}egmentation-\textbf{a}ssisted \textbf{R}eport \textbf{G}eneration), an extension of the popular LLaVA architecture that aims to tackle both of these problems. ASaRG proposes to fuse intermediate features and fine-grained segmentation maps created by specialist radiological models into LLaVA's multi-modal projection layer via simple concatenation. With a small number of added parameters, our approach achieves a +0.89\% performance gain ($p=0.012$) in CE F1 score compared to the LLaVA baseline when using only intermediate features, and +2.77\% performance gain ($p<0.001$) when adding a combination of intermediate features and fine-grained segmentation maps. Compared with COMG and ORID, two other report generation methods that utilize segmentations, the performance gain amounts to 6.98\% and 6.28\% in F1 score, respectively. ASaRG is not mutually exclusive with other changes made to the LLaVA architecture, potentially allowing our method to be combined with other advances in the field. Finally, the use of an arbitrary number of segmentations as part of the input demonstrably allows tracing elements of the report to the corresponding segmentation maps and verifying the groundedness of assessments. Our code will be made publicly available at a later date.</li>
<li><strong>摘要：</strong>可靠的端到端临床报告生成是医学ML研究的长期目标。该过程的最终目标是减轻放射科医生的工作量，并向临床医生或患者提供第二意见。因此，报告生成模型的必要先决条件是强大的一般表现和某种先天的接地能力，以说服临床医生或患者对生成的报告的真实性。在本文中，我们介绍了asarg（\ textbf {a} utomatic \ textbf {s} ementementation- \ textbf {a} ssisted \ textbf {r} eport \ textbf {g} energation \ textbf {g} energation），这是一个流行的llava架构的扩展，以解决这些问题。 Asarg提议通过简单的串联融合由专业放射学模型创建的中间特征和细粒分割图将其融合到LLAVA的多模式投影层中。借助少数附加参数，我们的方法在CE F1分数中获得了A +0.89 \％的性能增益（$ p = 0.012 $），而LLAVA基线仅使用中间功能时，在添加InterMedieDement功能组合时，仅使用中间功能（$ 2.77 \％\％\％的性能增益（$ p <0.001 $））。与COMG和ORID相比，使用分割的另外两种报告生成方法，绩效增益分别为6.98 \％和6.28 \％。 Asarg并非与对Llava架构的其他更改相互排斥，可能允许将我们的方法与该领域的其他进步相结合。最后，使用任意数量的分割作为输入的一部分，可以明显地将报告元素追溯到相应的分割图并验证评估的基础。我们的代码将在以后公开提供。</li>
</ul>

<h3>Title: Synthetic Data Matters: Re-training with Geo-typical Synthetic Labels for Building Detection</h3>
<ul>
<li><strong>Authors: </strong>Shuang Song, Yang Tang, Rongjun Qin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16657">https://arxiv.org/abs/2507.16657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16657">https://arxiv.org/pdf/2507.16657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16657]] Synthetic Data Matters: Re-training with Geo-typical Synthetic Labels for Building Detection(https://arxiv.org/abs/2507.16657)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Deep learning has significantly advanced building segmentation in remote sensing, yet models struggle to generalize on data of diverse geographic regions due to variations in city layouts and the distribution of building types, sizes and locations. However, the amount of time-consuming annotated data for capturing worldwide diversity may never catch up with the demands of increasingly data-hungry models. Thus, we propose a novel approach: re-training models at test time using synthetic data tailored to the target region's city layout. This method generates geo-typical synthetic data that closely replicates the urban structure of a target area by leveraging geospatial data such as street network from OpenStreetMap. Using procedural modeling and physics-based rendering, very high-resolution synthetic images are created, incorporating domain randomization in building shapes, materials, and environmental illumination. This enables the generation of virtually unlimited training samples that maintain the essential characteristics of the target environment. To overcome synthetic-to-real domain gaps, our approach integrates geo-typical data into an adversarial domain adaptation framework for building segmentation. Experiments demonstrate significant performance enhancements, with median improvements of up to 12%, depending on the domain gap. This scalable and cost-effective method blends partial geographic knowledge with synthetic imagery, providing a promising solution to the "model collapse" issue in purely synthetic datasets. It offers a practical pathway to improving generalization in remote sensing building segmentation without extensive real-world annotations.</li>
<li><strong>摘要：</strong>深度学习在遥感中具有显着高级的建筑细分，但是由于城市布局的变化以及建筑物类型，大小和位置的分布，模型很难概括到不同地理区域的数据。但是，用于捕获全球多样性的耗时的注释数据可能永远不会赶上越来越多的数据模型的需求。因此，我们提出了一种新颖的方法：使用针对目标地区城市布局量身定制的合成数据在测试时间进行重新训练模型。该方法生成地理典型的合成数据，通过利用诸如OpenStreetMap的街道网络等地理空间数据，可以密切复制目标区域的城市结构。使用基于程序建模和基于物理的渲染，创建了非常高分辨率的合成图像，将域随机化结合在建筑物形状，材料和环境照明中。这使几乎无限的培训样本生成了目标环境的基本特征。为了克服综合域间隙，我们的方法将地理典型数据集成到一个对抗性域的适应性框架中，以构建分割。实验显示出明显的性能增强，其中位改善高达12％，具体取决于域间隙。这种可扩展且具有成本效益的方法将部分地理知识与合成图像融合在一起，为纯合成数据集中的“模型崩溃”问题提供了有希望的解决方案。它为改善遥感建筑细分的概括提供了一种实用的途径，而无需大量的现实注释。</li>
</ul>

<h3>Title: Meta-Learning for Cold-Start Personalization in Prompt-Tuned LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yushang Zhao, Huijie Shen, Dannier Li, Lu Chang, Chengrui Zhou, Yinuo Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16672">https://arxiv.org/abs/2507.16672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16672">https://arxiv.org/pdf/2507.16672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16672]] Meta-Learning for Cold-Start Personalization in Prompt-Tuned LLMs(https://arxiv.org/abs/2507.16672)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative, explainable, and flexible recommender systems, derived using Large Language Models (LLM) are promising and poorly adapted to the cold-start user situation, where there is little to no history of interaction. The current solutions i.e. supervised fine-tuning and collaborative filtering are dense-user-item focused and would be expensive to maintain and update. This paper introduces a meta-learning framework, that can be used to perform parameter-efficient prompt-tuning, to effectively personalize LLM-based recommender systems quickly at cold-start. The model learns soft prompt embeddings with first-order (Reptile) and second-order (MAML) optimization by treating each of the users as the tasks. As augmentations to the input tokens, these learnable vectors are the differentiable control variables that represent user behavioral priors. The prompts are meta-optimized through episodic sampling, inner-loop adaptation, and outer-loop generalization. On MovieLens-1M, Amazon Reviews, and Recbole, we can see that our adaptive model outperforms strong baselines in NDCG@10, HR@10, and MRR, and it runs in real-time (i.e., below 300 ms) on consumer GPUs. Zero-history personalization is also supported by this scalable solution, and its 275 ms rate of adaptation allows successful real-time risk profiling of financial systems by shortening detection latency and improving payment network stability. Crucially, the 275 ms adaptation capability can enable real-time risk profiling for financial institutions, reducing systemic vulnerability detection latency significantly versus traditional compliance checks. By preventing contagion in payment networks (e.g., Fedwire), the framework strengthens national financial infrastructure resilience.</li>
<li><strong>摘要：</strong>使用大语言模型（LLM）得出的生成性，可解释和灵活的推荐系统是有希望的，并且适应了冷启动的用户状况，那里几乎没有相互作用的历史。当前的解决方案，即监督的微调和协作过滤是密集的用户，以维护和更新成本昂贵。本文介绍了一个元学习框架，可用于执行参数有效的及时调整，以有效地在Cold-Start上快速个性化基于LLM的建议系统。该模型通过将每个用户视为任务来学习使用一阶（爬行动物）和二阶（MAML）优化的软提示嵌入。作为输入令牌的增强，这些可学习的向量是代表用户行为先验的可区分控制变量。提示通过情节采样，内环适应和外环概括来元优化。在Movielens-1M，Amazon评论和Recbole上，我们可以看到我们的自适应模型在NDCG@10，HR@10和MRR中的表现优于强大的基线，并且它在消费者GPU上实时运行（即300毫秒以下）。这种可扩展的解决方案还支持零历史的个性化，其275毫秒的适应速率可以通过缩短检测潜伏期和改善支付网络稳定性来成功地实时对金融系统进行实时风险分析。至关重要的是，275毫秒的适应能力可以为金融机构提供实时风险分析，从而减少系统性脆弱性检测潜伏期与传统合规性检查相比。通过防止支付网络（例如FedWire）的传染，该框架增强了国家金融基础设施的弹性。</li>
</ul>

<h3>Title: Screen2AX: Vision-Based Approach for Automatic macOS Accessibility Generation</h3>
<ul>
<li><strong>Authors: </strong>Viktor Muryn, Marta Sumyk, Mariya Hirna, Sofiya Garkot, Maksym Shamrai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16704">https://arxiv.org/abs/2507.16704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16704">https://arxiv.org/pdf/2507.16704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16704]] Screen2AX: Vision-Based Approach for Automatic macOS Accessibility Generation(https://arxiv.org/abs/2507.16704)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Desktop accessibility metadata enables AI agents to interpret screens and supports users who depend on tools like screen readers. Yet, many applications remain largely inaccessible due to incomplete or missing metadata provided by developers - our investigation shows that only 33% of applications on macOS offer full accessibility support. While recent work on structured screen representation has primarily addressed specific challenges, such as UI element detection or captioning, none has attempted to capture the full complexity of desktop interfaces by replicating their entire hierarchical structure. To bridge this gap, we introduce Screen2AX, the first framework to automatically create real-time, tree-structured accessibility metadata from a single screenshot. Our method uses vision-language and object detection models to detect, describe, and organize UI elements hierarchically, mirroring macOS's system-level accessibility structure. To tackle the limited availability of data for macOS desktop applications, we compiled and publicly released three datasets encompassing 112 macOS applications, each annotated for UI element detection, grouping, and hierarchical accessibility metadata alongside corresponding screenshots. Screen2AX accurately infers hierarchy trees, achieving a 77% F1 score in reconstructing a complete accessibility tree. Crucially, these hierarchy trees improve the ability of autonomous agents to interpret and interact with complex desktop interfaces. We introduce Screen2AX-Task, a benchmark specifically designed for evaluating autonomous agent task execution in macOS desktop environments. Using this benchmark, we demonstrate that Screen2AX delivers a 2.2x performance improvement over native accessibility representations and surpasses the state-of-the-art OmniParser V2 system on the ScreenSpot benchmark.</li>
<li><strong>摘要：</strong>桌面可访问性元数据使AI代理能够解释屏幕并支持依赖屏幕读取器等工具的用户。然而，由于开发人员提供的不完整或缺少元数据，许多应用程序在很大程度上仍然无法访问 - 我们的调查表明，MacOS上只有33％的申请提供了全部可访问性支持。尽管结构化屏幕表示的最新工作主要解决了特定的挑战，例如UI元素检测或字幕，但没有人试图通过复制其整个层次结构来捕获桌面接口的全部复杂性。为了弥合此差距，我们介绍了Screen2AX，这是第一个从单个屏幕截图中自动创建实时的，树结构的可访问性元数据的框架。我们的方法使用视觉语言和对象检测模型来层次检测，描述和组织UI元素，从而反映MACOS的系统级可访问性结构。为了解决MACOS桌面应用程序的有限数据可用性，我们汇编并公开发布了包含112个MacOS应用程序的三个数据集，每个数据集都在UI元素检测，分组和层次可访问性元数据以及相应的屏幕快照旁进行了注释。 Screen2AX准确地渗透了层次树，在重建完整的可访问性树时获得了77％的F1得分。至关重要的是，这些层次树提高了自主剂与复杂桌面界面进行解释和相互作用的能力。我们介绍了Screen2AX-TASK，这是一种专门设计用于评估MACOS桌面环境中自主代理任务执行的基准。使用此基准测试，我们证明了Screen2AX对本机可访问性表示的2.2倍性能提高，并超过了ScreenSpot基准上的最新omniparser V2系统。</li>
</ul>

<h3>Title: Enhancing Remote Sensing Vision-Language Models Through MLLM and LLM-Based High-Quality Image-Text Dataset Generation</h3>
<ul>
<li><strong>Authors: </strong>Yiguo He, Junjie Zhu, Yiying Li, Xiaoyu Zhang, Chunping Qiu, Jun Wang, Qiangjuan Huang, Ke Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16716">https://arxiv.org/abs/2507.16716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16716">https://arxiv.org/pdf/2507.16716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16716]] Enhancing Remote Sensing Vision-Language Models Through MLLM and LLM-Based High-Quality Image-Text Dataset Generation(https://arxiv.org/abs/2507.16716)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The application of Vision-language foundation models (VLFMs) to remote sensing (RS) imagery has garnered significant attention due to their superior capability in various downstream tasks. A key challenge lies in the scarcity of high-quality, large-scale, image-text paired training data. Recently, several works introduced extensive image-text datasets for RS and trained their VLFMs. However, due to the rudimentary methods used for generating captions, the quality of datasets is suboptimal, requiring larger volumes of training data, while only yielding modest performance improvements. In this paper, we propose a two-stage method named MpGI(Multi-Perspective Generation and Integration) for generating high-quality text captions for RS images. Firstly, we generate distinct and detailed descriptions from different perspectives using Rule-MLLM(Multimodal Large Language Model) Relay Generation and MLLMs generation methods. Next, we utilize Large Language Models (LLMs) to integrate these diverse descriptions into comprehensive captions, capturing details from multiple perspectives. Finally, we have created the HQRS-IT-210K dataset, including about 210,000 RS images and 1.3 million captions. We fine-tuned two VLFMs using our dataset: CLIP, a discriminative model, and CoCa, an image-to-text generative model. This process resulted in our proposed HQRS-CLIP and RS-CoCa models. Experimental results demonstrate that HQRS-CLIP surpassed the previous SOTA RS CLIP model in various downstream tasks while using only 4.2\% of the training data. RS-CoCa outperforms other advanced approaches across benchmark datasets and can generate captions for RS images that rival or even exceed manual annotations. Dataset, pre-trained models, and codes will be released at this https URL.</li>
<li><strong>摘要：</strong>视觉基础模型（VLFM）应用于遥感（RS）图像，由于其在各种下游任务中的卓越能力，因此引起了极大的关注。一个关键的挑战在于缺乏高质量的大规模，图像文本配对的训练数据。最近，一些作品引入了广泛的图像文本数据集，并培训了其VLFM。但是，由于用于生成字幕的基本方法，数据集的质量是次优的，需要大量的培训数据，同时仅产生适度的性能改进。在本文中，我们提出了一种名为MPGI（多观点生成和集成）的两阶段方法，用于为RS图像生成高质量的文本字幕。首先，我们使用Rule-Mllm（多模式大型语言模型）中继生成和MLLMS生成方法从不同角度生成独特的详细描述。接下来，我们利用大型语言模型（LLMS）将这些不同的描述集成到全面的标题中，从多个角度捕获细节。最后，我们创建了HQRS-IT-210K数据集，其中包括大约210,000 RS图像和130万个字幕。我们使用数据集对两个VLFM进行了微调：夹子，一个歧视模型和可可，是图像到文本生成模型。该过程导致了我们提出的HQRS-CLIP和RS-COCA模型。实验结果表明，HQRS-CLIP在各种下游任务中超过了先前的SOTA RS夹模型，而仅使用4.2％的训练数据。 RS-COCA在基准数据集上的其他高级方法优于其他高级方法，并且可以为RS图像生成字幕，甚至超过手动注释。数据集，预训练模型和代码将在此HTTPS URL上发布。</li>
</ul>

<h3>Title: Improving Model Classification by Optimizing the Training Dataset</h3>
<ul>
<li><strong>Authors: </strong>Morad Tukan, Loay Mualem, Eitan Netzer, Liran Sigalat</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16729">https://arxiv.org/abs/2507.16729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16729">https://arxiv.org/pdf/2507.16729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16729]] Improving Model Classification by Optimizing the Training Dataset(https://arxiv.org/abs/2507.16729)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In the era of data-centric AI, the ability to curate high-quality training data is as crucial as model design. Coresets offer a principled approach to data reduction, enabling efficient learning on large datasets through importance sampling. However, conventional sensitivity-based coreset construction often falls short in optimizing for classification performance metrics, e.g., $F1$ score, focusing instead on loss approximation. In this work, we present a systematic framework for tuning the coreset generation process to enhance downstream classification quality. Our method introduces new tunable parameters--including deterministic sampling, class-wise allocation, and refinement via active sampling, beyond traditional sensitivity scores. Through extensive experiments on diverse datasets and classifiers, we demonstrate that tuned coresets can significantly outperform both vanilla coresets and full dataset training on key classification metrics, offering an effective path towards better and more efficient model training.</li>
<li><strong>摘要：</strong>在以数据为中心的AI时代，策划高质量培训数据的能力与模型设计至关重要。核心提供了一种原则性的方法来减少数据，从而通过重要性抽样在大型数据集上有效学习。但是，传统的基于灵敏度的核心结构通常在优化分类性能指标（例如$ f1 $得分）方面通常不足，而是专注于损失近似。在这项工作中，我们提出了一个系统的框架，用于调整核心生成过程，以增强下游分类质量。我们的方法引入了新的可调参数 - 包括确定性抽样，班级分配以及通过主动采样（超出传统灵敏度得分）。通过对各种数据集和分类器进行的大量实验，我们证明了调谐核心可以在关键分类指标上大大优于香草核心和完整的数据集培训，从而为更好，更有效的模型培训提供了有效的途径。</li>
</ul>

<h3>Title: CMP: A Composable Meta Prompt for SAM-Based Cross-Domain Few-Shot Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Shuai Chen, Fanman Meng, Chunjin Yang, Haoran Wei, Chenhao Wu, Qingbo Wu, Hongliang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16753">https://arxiv.org/abs/2507.16753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16753">https://arxiv.org/pdf/2507.16753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16753]] CMP: A Composable Meta Prompt for SAM-Based Cross-Domain Few-Shot Segmentation(https://arxiv.org/abs/2507.16753)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Cross-Domain Few-Shot Segmentation (CD-FSS) remains challenging due to limited data and domain shifts. Recent foundation models like the Segment Anything Model (SAM) have shown remarkable zero-shot generalization capability in general segmentation tasks, making it a promising solution for few-shot scenarios. However, adapting SAM to CD-FSS faces two critical challenges: reliance on manual prompt and limited cross-domain ability. Therefore, we propose the Composable Meta-Prompt (CMP) framework that introduces three key modules: (i) the Reference Complement and Transformation (RCT) module for semantic expansion, (ii) the Composable Meta-Prompt Generation (CMPG) module for automated meta-prompt synthesis, and (iii) the Frequency-Aware Interaction (FAI) module for domain discrepancy mitigation. Evaluations across four cross-domain datasets demonstrate CMP's state-of-the-art performance, achieving 71.8\% and 74.5\% mIoU in 1-shot and 5-shot scenarios respectively.</li>
<li><strong>摘要：</strong>由于数据有限和域移动，跨域很少的射击分割（CD-FSS）仍然具有挑战性。最近的基础模型（例如任何模型（SAM））在一般分段任务中显示出显着的零击概括能力，这使其成为几场比赛的有前途的解决方案。但是，将SAM适应CD-FSS面临两个关键挑战：依赖手动提示和有限的跨域能力。因此，我们提出了介绍三个关键模块的可合并元提示（CMP）框架：（i）用于语义扩展的参考补体和转换（RCT）模块，（ii）可合元元数据（CMPG）模块用于自动元元数据的自动元元素合成和（iiii）的频率 - 差异（iii）的模块。四个跨域数据集的评估证明了CMP的最先进性能，分别以1-Shot和5-Shot方案的形式达到71.8 \％和74.5 \％MIOU。</li>
</ul>

<h3>Title: A Partitioned Sparse Variational Gaussian Process for Fast, Distributed Spatial Modeling</h3>
<ul>
<li><strong>Authors: </strong>Michael Grosskopf, Kellin Rumsey, Ayan Biswas, Earl Lawrence</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16771">https://arxiv.org/abs/2507.16771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16771">https://arxiv.org/pdf/2507.16771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16771]] A Partitioned Sparse Variational Gaussian Process for Fast, Distributed Spatial Modeling(https://arxiv.org/abs/2507.16771)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The next generation of Department of Energy supercomputers will be capable of exascale computation. For these machines, far more computation will be possible than that which can be saved to disk. As a result, users will be unable to rely on post-hoc access to data for uncertainty quantification and other statistical analyses and there will be an urgent need for sophisticated machine learning algorithms which can be trained in situ. Algorithms deployed in this setting must be highly scalable, memory efficient and capable of handling data which is distributed across nodes as spatially contiguous partitions. One suitable approach involves fitting a sparse variational Gaussian process (SVGP) model independently and in parallel to each spatial partition. The resulting model is scalable, efficient and generally accurate, but produces the undesirable effect of constructing discontinuous response surfaces due to the disagreement between neighboring models at their shared boundary. In this paper, we extend this idea by allowing for a small amount of communication between neighboring spatial partitions which encourages better alignment of the local models, leading to smoother spatial predictions and a better fit in general. Due to our decentralized communication scheme, the proposed extension remains highly scalable and adds very little overhead in terms of computation (and none, in terms of memory). We demonstrate this Partitioned SVGP (PSVGP) approach for the Energy Exascale Earth System Model (E3SM) and compare the results to the independent SVGP case.</li>
<li><strong>摘要：</strong>下一代能源超级计算机将能够进行外部计算。对于这些机器，比可以将磁盘保存的计算要多得多。结果，用户将无法依靠事后访问数据进行不确定性量化和其他统计分析，并且将迫切需要对可以进行原位培训的复杂机器学习算法。在此设置中部署的算法必须高度扩展，内存有效且能够处理数据，该数据分布在节点上，作为空间连续分区。一种合适的方法是独立且与每个空间分区平行地拟合稀疏的变异高斯过程（SVGP）模型。所得模型是可扩展的，有效的且通常是准确的，但由于相邻模型在共享边界处的相邻模型之间存在分歧，因此产生了构建不连续响应表面的不良效果。在本文中，我们通过允许在相邻的空间分区之间进行少量交流来扩展这个想法，从而鼓励更好地对齐本地模型，从而使空间预测更加顺畅，并且通常可以更好地拟合。由于我们的分散交流方案，所提出的扩展仍然可扩展，并且在计算方面添加了很少的开销（在内存方面无）。我们演示了该分区的SVGP（PSVGP）方法，用于Energe Exascale Earth System模型（E3SM），并将结果与独立的SVGP情况进行比较。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
