<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-01-07</h1>
<h3>Title: Physical Transformer</h3>
<ul>
<li><strong>Authors: </strong>Tao Xu, Zhixin Hu, Li Luo, Momiao Xiong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02433">https://arxiv.org/abs/2601.02433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02433">https://arxiv.org/pdf/2601.02433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02433]] Physical Transformer(https://arxiv.org/abs/2601.02433)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Digital AI systems spanning large language models, vision models, and generative architectures that operate primarily in symbolic, linguistic, or pixel domains. They have achieved striking progress, but almost all of this progress lives in virtual spaces. These systems transform embeddings and tokens, yet do not themselves touch the world and rarely admit a physical interpretation. In this work we propose a physical transformer that couples modern transformer style computation with geometric representation and physical dynamics. At the micro level, attention heads, and feed-forward blocks are modeled as interacting spins governed by effective Hamiltonians plus non-Hamiltonian bath terms. At the meso level, their aggregated state evolves on a learned Neural Differential Manifold (NDM) under Hamiltonian flows and Hamilton, Jacobi, Bellman (HJB) optimal control, discretized by symplectic layers that approximately preserve geometric and energetic invariants. At the macro level, the model maintains a generative semantic workspace and a two-dimensional information-phase portrait that tracks uncertainty and information gain over a reasoning trajectory. Within this hierarchy, reasoning tasks are formulated as controlled information flows on the manifold, with solutions corresponding to low cost trajectories that satisfy geometric, energetic, and workspace-consistency constraints. On simple toy problems involving numerical integration and dynamical systems, the physical transformer outperforms naive baselines in stability and long-horizon accuracy, highlighting the benefits of respecting underlying geometric and Hamiltonian structure. More broadly, the framework suggests a path toward physical AI that unify digital reasoning with physically grounded manifolds, opening a route to more interpretable and potentially unified models of reasoning, control, and interaction with the real world.</li>
<li><strong>摘要：</strong>数字人工智能系统涵盖大型语言模型、视觉模型和生成架构，主要在符号、语言或像素域中运行。他们取得了惊人的进步，但几乎所有这些进步都存在于虚拟空间中。这些系统改变了嵌入和标记，但它们本身并不接触世界，也很少接受物理解释。在这项工作中，我们提出了一种物理变压器，它将现代变压器式计算与几何表示和物理动力学结合起来。在微观层面，注意力头和前馈块被建模为由有效哈密顿量和非哈密顿浴项控制的相互作用的自旋。在中观层面，它们的聚合状态在哈密顿流和哈密尔顿、雅可比、贝尔曼（HJB）最优控制下在学习的神经微分流形（NDM）上演化，并通过近似保留几何和能量不变量的辛层进行离散。在宏观层面上，该模型维护生成语义工作空间和二维信息相肖像，用于跟踪推理轨迹上的不确定性和信息增益。在这个层次结构中，推理任务被表述为流形上的受控信息流，其解决方案对应于满足几何、能量和工作空间一致性约束的低成本轨迹。在涉及数值积分和动力系统的简单玩具问题上，物理变换器在稳定性和长期精度方面优于朴素基线，凸显了尊重基础几何和哈密顿结构的好处。更广泛地说，该框架提出了一条通往物理人工智能的道路，将数字推理与物理接地流形相结合，为推理、控制和与现实世界的交互的更可解释和潜在统一的模型开辟了一条道路。</li>
</ul>

<h3>Title: Understanding Pure Textual Reasoning for Blind Image Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Yuan Li, Shin'ya Nishida</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02441">https://arxiv.org/abs/2601.02441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02441">https://arxiv.org/pdf/2601.02441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02441]] Understanding Pure Textual Reasoning for Blind Image Quality Assessment(https://arxiv.org/abs/2601.02441)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Textual reasoning has recently been widely adopted in Blind Image Quality Assessment (BIQA). However, it remains unclear how textual information contributes to quality prediction and to what extent text can represent the score-related image contents. This work addresses these questions from an information-flow perspective by comparing existing BIQA models with three paradigms designed to learn the image-text-score relationship: Chain-of-Thought, Self-Consistency, and Autoencoder. Our experiments show that the score prediction performance of the existing model significantly drops when only textual information is used for prediction. Whereas the Chain-of-Thought paradigm introduces little improvement in BIQA performance, the Self-Consistency paradigm significantly reduces the gap between image- and text-conditioned predictions, narrowing the PLCC/SRCC difference to 0.02/0.03. The Autoencoder-like paradigm is less effective in closing the image-text gap, yet it reveals a direction for further optimization. These findings provide insights into how to improve the textual reasoning for BIQA and high-level vision tasks.</li>
<li><strong>摘要：</strong>文本推理最近在盲图像质量评估（BIQA）中被广泛采用。然而，目前尚不清楚文本信息如何有助于质量预测以及文本在多大程度上可以表示与分数相关的图像内容。这项工作通过将现有的 BIQA 模型与旨在学习图像-文本-评分关系的三种范式进行比较，从信息流的角度解决了这些问题：思想链、自洽和自动编码器。我们的实验表明，当仅使用文本信息进行预测时，现有模型的分数预测性能显着下降。虽然思想链范式对 BIQA 性能几乎没有什么改进，但自洽范式显着缩小了图像条件预测和文本条件预测之间的差距，将 PLCC/SRCC 差异缩小到 0.02/0.03。类似自动编码器的范式在缩小图像文本差距方面效果较差，但它揭示了进一步优化的方向。这些发现为如何改进 BIQA 和高级视觉任务的文本推理提供了见解。</li>
</ul>

<h3>Title: Evaluating the Diagnostic Classification Ability of Multimodal Large Language Models: Insights from the Osteoarthritis Initiative</h3>
<ul>
<li><strong>Authors: </strong>Li Wang, Xi Chen, XiangWen Deng, HuaHui Yi, ZeKun Jiang, Kang Li, Jian Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02443">https://arxiv.org/abs/2601.02443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02443">https://arxiv.org/pdf/2601.02443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02443]] Evaluating the Diagnostic Classification Ability of Multimodal Large Language Models: Insights from the Osteoarthritis Initiative(https://arxiv.org/abs/2601.02443)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) show promising performance on medical visual question answering (VQA) and report generation, but these generation and explanation abilities do not reliably transfer to disease-specific classification. We evaluated MLLM architectures on knee osteoarthritis (OA) radiograph classification, which remains underrepresented in existing medical MLLM benchmarks, even though knee OA affects an estimated 300 to 400 million people worldwide. Through systematic ablation studies manipulating the vision encoder, the connector, and the large language model (LLM) across diverse training strategies, we measured each component's contribution to diagnostic accuracy. In our classification task, a trained vision encoder alone could outperform full MLLM pipelines in classification accuracy and fine-tuning the LLM provided no meaningful improvement over prompt-based guidance. And LoRA fine-tuning on a small, class-balanced dataset (500 images) gave better results than training on a much larger but class-imbalanced set (5,778 images), indicating that data balance and quality can matter more than raw scale for this task. These findings suggest that for domain-specific medical classification, LLMs are more effective as interpreters and report generators rather than as primary classifiers. Therefore, the MLLM architecture appears less suitable for medical image diagnostic classification tasks that demand high certainty. We recommend prioritizing vision encoder optimization and careful dataset curation when developing clinically applicable systems.</li>
<li><strong>摘要：</strong>多模态大语言模型 (MLLM) 在医学视觉问答 (VQA) 和报告生成方面表现出良好的性能，但这些生成和解释能力并不能可靠地转移到特定疾病的分类。我们评估了膝骨关节炎 (OA) 放射线照片分类的 MLLM 架构，尽管膝关节 OA 影响了全球约 300 至 4 亿人，但该架构在现有的医学 MLLM 基准中仍然代表性不足。通过跨不同训练策略操作视觉编码器、连接器和大语言模型 (LLM) 的系统消融研究，我们测量了每个组件对诊断准确性的贡献。在我们的分类任务中，仅经过训练的视觉编码器就可以在分类精度方面优于完整的 MLLM 流程，并且对 LLM 进行微调并没有比基于提示的指导提供任何有意义的改进。 LoRA 在小型、类别平衡的数据集（500 张图像）上进行微调比在更大但类别不平衡的数据集（5,778 张图像）上进行训练得到了更好的结果，这表明对于此任务来说，数据平衡和质量比原始规模更重要。这些发现表明，对于特定领域的医学分类，法学硕士作为解释者和报告生成者比作为主要分类者更有效。因此，MLLM架构似乎不太适合需要高确定性的医学图像诊断分类任务。我们建议在开发临床适用的系统时优先考虑视觉编码器优化和仔细的数据集管理。</li>
</ul>

<h3>Title: Polynomial Convergence of Riemannian Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Xingyu Xu, Ziyi Zhang, Yorie Nakahira, Guannan Qu, Yuejie Chi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02499">https://arxiv.org/abs/2601.02499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02499">https://arxiv.org/pdf/2601.02499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02499]] Polynomial Convergence of Riemannian Diffusion Models(https://arxiv.org/abs/2601.02499)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated remarkable empirical success in the recent years and are considered one of the state-of-the-art generative models in modern AI. These models consist of a forward process, which gradually diffuses the data distribution to a noise distribution spanning the whole space, and a backward process, which inverts this transformation to recover the data distribution from noise. Most of the existing literature assumes that the underlying space is Euclidean. However, in many practical applications, the data are constrained to lie on a submanifold of Euclidean space. Addressing this setting, De Bortoli et al. (2022) introduced Riemannian diffusion models and proved that using an exponentially small step size yields a small sampling error in the Wasserstein distance, provided the data distribution is smooth and strictly positive, and the score estimate is $L_\infty$-accurate. In this paper, we greatly strengthen this theory by establishing that, under $L_2$-accurate score estimate, a {\em polynomially small stepsize} suffices to guarantee small sampling error in the total variation distance, without requiring smoothness or positivity of the data distribution. Our analysis only requires mild and standard curvature assumptions on the underlying manifold. The main ingredients in our analysis are Li-Yau estimate for the log-gradient of heat kernel, and Minakshisundaram-Pleijel parametrix expansion of the perturbed heat equation. Our approach opens the door to a sharper analysis of diffusion models on non-Euclidean spaces.</li>
<li><strong>摘要：</strong>近年来，扩散模型在实证上取得了显着的成功，被认为是现代人工智能中最先进的生成模型之一。这些模型由一个前向过程和一个后向过程组成，前向过程逐渐将数据分布扩散到跨越整个空间的噪声分布，后向过程反转此变换以从噪声中恢复数据分布。大多数现有文献都假设基础空间是欧几里得空间。然而，在许多实际应用中，数据被限制在欧几里得空间的子流形上。针对这一情况，De Bortoli 等人。 (2022) 引入了黎曼扩散模型，并证明使用指数小步长会在 Wasserstein 距离中产生较小的采样误差，前提是数据分布平滑且严格为正，并且分数估计为 $L_\infty$ 准确。在本文中，我们通过建立在$L_2$精确分数估计下，{\em多项式小步长}足以保证总变异距离中的较小采样误差，而不需要数据分布的平滑性或正性，来极大地强化了这一理论。我们的分析仅需要对基础流形进行温和且标准的曲率假设。我们分析的主要内容是热核对数梯度的 Li-Yau 估计，以及扰动热方程的 Minakshisundaram-Pleijel 参数矩阵展开。我们的方法为对非欧几里得空间上的扩散模型进行更清晰的分析打开了大门。</li>
</ul>

<h3>Title: CT Scans As Video: Efficient Intracranial Hemorrhage Detection Using Multi-Object Tracking</h3>
<ul>
<li><strong>Authors: </strong>Amirreza Parvahan, Mohammad Hoseyni, Javad Khoramdel, Amirhossein Nikoofard</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02521">https://arxiv.org/abs/2601.02521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02521">https://arxiv.org/pdf/2601.02521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02521]] CT Scans As Video: Efficient Intracranial Hemorrhage Detection Using Multi-Object Tracking(https://arxiv.org/abs/2601.02521)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Automated analysis of volumetric medical imaging on edge devices is severely constrained by the high memory and computational demands of 3D Convolutional Neural Networks (CNNs). This paper develops a lightweight computer vision framework that reconciles the efficiency of 2D detection with the necessity of 3D context by reformulating volumetric Computer Tomography (CT) data as sequential video streams. This video-viewpoint paradigm is applied to the time-sensitive task of Intracranial Hemorrhage (ICH) detection using the Hemorica dataset. To ensure operational efficiency, we benchmarked multiple generations of the YOLO architecture (v8, v10, v11 and v12) in their Nano configurations, selecting the version with the highest mAP@50 to serve as the slice-level backbone. A ByteTrack algorithm is then introduced to enforce anatomical consistency across the $z$-axis. To address the initialization lag inherent in video trackers, a hybrid inference strategy and a spatiotemporal consistency filter are proposed to distinguish true pathology from transient prediction noise. Experimental results on independent test data demonstrate that the proposed framework serves as a rigorous temporal validator, increasing detection Precision from 0.703 to 0.779 compared to the baseline 2D detector, while maintaining high sensitivity. By approximating 3D contextual reasoning at a fraction of the computational cost, this method provides a scalable solution for real-time patient prioritization in resource-constrained environments, such as mobile stroke units and IoT-enabled remote clinics.</li>
<li><strong>摘要：</strong>边缘设备上体积医学成像的自动分析受到 3D 卷积神经网络 (CNN) 的高内存和计算需求的严重限制。本文开发了一种轻量级计算机视觉框架，通过将体积计算机断层扫描 (CT) 数据重新表述为顺序视频流，协调 2D 检测的效率与 3D 上下文的必要性。这种视频视点范式适用于使用 Hemorica 数据集进行颅内出血 (ICH) 检测的时间敏感任务。为了确保运行效率，我们在 Nano 配置中对多代 YOLO 架构（v8、v10、v11 和 v12）进行了基准测试，选择 mAP@50 最高的版本作为切片级主干。然后引入 ByteTrack 算法来强制 $z$ 轴上的解剖学一致性。为了解决视频跟踪器固有的初始化滞后问题，提出了混合推理策略和时空一致性滤波器，以区分真实病理与瞬态预测噪声。独立测试数据的实验结果表明，所提出的框架可以作为严格的时间验证器，与基线 2D 检测器相比，将检测精度从 0.703 提高到 0.779，同时保持高灵敏度。通过以一小部分计算成本进行近似 3D 上下文推理，该方法为资源受限环境（例如移动中风病房和支持物联网的远程诊所）中的实时患者优先级排序提供了可扩展的解决方案。</li>
</ul>

<h3>Title: DreamLoop: Controllable Cinemagraph Generation from a Single Photograph</h3>
<ul>
<li><strong>Authors: </strong>Aniruddha Mahapatra, Long Mai, Cusuh Ham, Feng Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02646">https://arxiv.org/abs/2601.02646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02646">https://arxiv.org/pdf/2601.02646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02646]] DreamLoop: Controllable Cinemagraph Generation from a Single Photograph(https://arxiv.org/abs/2601.02646)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Cinemagraphs, which combine static photographs with selective, looping motion, offer unique artistic appeal. Generating them from a single photograph in a controllable manner is particularly challenging. Existing image-animation techniques are restricted to simple, low-frequency motions and operate only in narrow domains with repetitive textures like water and smoke. In contrast, large-scale video diffusion models are not tailored for cinemagraph constraints and lack the specialized data required to generate seamless, controlled loops. We present DreamLoop, a controllable video synthesis framework dedicated to generating cinemagraphs from a single photo without requiring any cinemagraph training data. Our key idea is to adapt a general video diffusion model by training it on two objectives: temporal bridging and motion conditioning. This strategy enables flexible cinemagraph generation. During inference, by using the input image as both the first- and last- frame condition, we enforce a seamless loop. By conditioning on static tracks, we maintain a static background. Finally, by providing a user-specified motion path for a target object, our method provides intuitive control over the animation's trajectory and timing. To our knowledge, DreamLoop is the first method to enable cinemagraph generation for general scenes with flexible and intuitive controls. We demonstrate that our method produces high-quality, complex cinemagraphs that align with user intent, outperforming existing approaches.</li>
<li><strong>摘要：</strong>动态照片将静态照片与选择性循环运动相结合，具有独特的艺术吸引力。以可控的方式从单张照片生成它们尤其具有挑战性。现有的图像动画技术仅限于简单的低频运动，并且只能在具有重复纹理（例如水和烟雾）的狭窄区域中运行。相比之下，大规模视频扩散模型并非针对电影图像限制而定制，并且缺乏生成无缝、受控循环所需的专门数据。我们推出了 DreamLoop，这是一种可控视频合成框架，专用于从单张照片生成电影图片，而不需要任何电影图片训练数据。我们的关键思想是通过针对两个目标进行训练来适应通用视频扩散模型：时间桥接和运动调节。这种策略可以实现灵活的电影图像生成。在推理过程中，通过使用输入图像作为第一帧和最后一帧条件，我们强制执行无缝循环。通过调节静态轨道，我们保持静态背景。最后，通过为目标对象提供用户指定的运动路径，我们的方法提供了对动画轨迹和时间的直观控制。据我们所知，DreamLoop 是第一种通过灵活直观的控制为一般场景生成电影图像的方法。我们证明，我们的方法可以生成符合用户意图的高质量、复杂的电影图像，其性能优于现有方法。</li>
</ul>

<h3>Title: GRRE: Leveraging G-Channel Removed Reconstruction Error for Robust Detection of AI-Generated Images</h3>
<ul>
<li><strong>Authors: </strong>Shuman He, Xiehua Li, Xioaju Yang, Yang Xiong, Keqin Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02709">https://arxiv.org/abs/2601.02709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02709">https://arxiv.org/pdf/2601.02709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02709]] GRRE: Leveraging G-Channel Removed Reconstruction Error for Robust Detection of AI-Generated Images(https://arxiv.org/abs/2601.02709)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid progress of generative models, particularly diffusion models and GANs, has greatly increased the difficulty of distinguishing synthetic images from real ones. Although numerous detection methods have been proposed, their accuracy often degrades when applied to images generated by novel or unseen generative models, highlighting the challenge of achieving strong generalization. To address this challenge, we introduce a novel detection paradigm based on channel removal reconstruction. Specifically, we observe that when the green (G) channel is removed from real images and reconstructed, the resulting reconstruction errors differ significantly from those of AI-generated images. Building upon this insight, we propose G-channel Removed Reconstruction Error (GRRE), a simple yet effective method that exploits this discrepancy for robust AI-generated image detection. Extensive experiments demonstrate that GRRE consistently achieves high detection accuracy across multiple generative models, including those unseen during training. Compared with existing approaches, GRRE not only maintains strong robustness against various perturbations and post-processing operations but also exhibits superior cross-model generalization. These results highlight the potential of channel-removal-based reconstruction as a powerful forensic tool for safeguarding image authenticity in the era of generative AI.</li>
<li><strong>摘要：</strong>生成模型，特别是扩散模型和 GAN 的快速进步，大大增加了区分合成图像和真实图像的难度。尽管已经提出了许多检测方法，但当应用于新颖或看不见的生成模型生成的图像时，它们的准确性通常会降低，这凸显了实现强泛化的挑战。为了应对这一挑战，我们引入了一种基于通道去除重建的新型检测范例。具体来说，我们观察到，当从真实图像中删除绿色（G）通道并进行重建时，产生的重建误差与人工智能生成的图像的重建误差显着不同。基于这一见解，我们提出了 G 通道去除重建误差（GRRE），这是一种简单而有效的方法，利用这种差异进行鲁棒的人工智能生成图像检测。大量实验表明，GRRE 在多个生成模型（包括训练期间未见过的模型）中始终保持较高的检测精度。与现有方法相比，GRRE不仅对各种扰动和后处理操作保持强大的鲁棒性，而且还表现出优异的跨模型泛化能力。这些结果凸显了基于通道移除的重建作为生成人工智能时代保护图像真实性的强大取证工具的潜力。</li>
</ul>

<h3>Title: Foreground-Aware Dataset Distillation via Dynamic Patch Selection</h3>
<ul>
<li><strong>Authors: </strong>Longzhen Li, Guang Li, Ren Togo, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02727">https://arxiv.org/abs/2601.02727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02727">https://arxiv.org/pdf/2601.02727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02727]] Foreground-Aware Dataset Distillation via Dynamic Patch Selection(https://arxiv.org/abs/2601.02727)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a foreground-aware dataset distillation method that enhances patch selection in a content-adaptive manner. With the rising computational cost of training large-scale deep models, dataset distillation has emerged as a promising approach for constructing compact synthetic datasets that retain the knowledge of their large original counterparts. However, traditional optimization-based methods often suffer from high computational overhead, memory constraints, and the generation of unrealistic, noise-like images with limited architectural generalization. Recent non-optimization methods alleviate some of these issues by constructing distilled data from real image patches, but the used rigid patch selection strategies can still discard critical information about the main objects. To solve this problem, we first leverage Grounded SAM2 to identify foreground objects and compute per-image foreground occupancy, from which we derive a category-wise patch decision threshold. Guided by these thresholds, we design a dynamic patch selection strategy that, for each image, either selects the most informative patch from multiple candidates or directly resizes the full image when the foreground dominates. This dual-path mechanism preserves more key information about the main objects while reducing redundant background content. Extensive experiments on multiple benchmarks show that the proposed method consistently improves distillation performance over existing approaches, producing more informative and representative distilled datasets and enhancing robustness across different architectures and image compositions.</li>
<li><strong>摘要：</strong>在本文中，我们提出了一种前景感知的数据集蒸馏方法，以内容自适应的方式增强补丁选择。随着训练大规模深度模型的计算成本不断上升，数据集蒸馏已成为构建紧凑的合成数据集的一种有前途的方法，该数据集保留了大型原始数据集的知识。然而，传统的基于优化的方法经常遭受高计算开销、内存限制以及生成不切实际的、类似噪声的图像且架构泛化有限的问题。最近的非优化方法通过从真实图像块构建蒸馏数据来缓解其中一些问题，但使用的刚性块选择策略仍然可以丢弃有关主要对象的关键信息。为了解决这个问题，我们首先利用 Grounded SAM2 来识别前景对象并计算每幅图像的前景占用率，从中得出按类别的补丁决策阈值。在这些阈值的指导下，我们设计了一种动态补丁选择策略，对于每个图像，要么从多个候选者中选择信息最丰富的补丁，要么在前景占主导地位时直接调整整个图像的大小。这种双路径机制保留了主要对象的更多关键信息，同时减少了冗余的背景内容。对多个基准的广泛实验表明，所提出的方法相对于现有方法持续提高了蒸馏性能，产生了更具信息性和代表性的蒸馏数据集，并增强了不同架构和图像组合的鲁棒性。</li>
</ul>

<h3>Title: Unveiling and Bridging the Functional Perception Gap in MLLMs: Atomic Visual Alignment and Hierarchical Evaluation via PET-Bench</h3>
<ul>
<li><strong>Authors: </strong>Zanting Ye, Xiaolong Niu, Xuanbin Wu, Xu Han, Shengyuan Liu, Jing Hao, Zhihao Peng, Hao Sun, Jieqin Lv, Fanghu Wang, Yanchao Huang, Hubing Wu, Yixuan Yuan, Habib Zaidi, Arman Rahmim, Yefeng Zheng, Lijun Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02737">https://arxiv.org/abs/2601.02737</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02737">https://arxiv.org/pdf/2601.02737</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02737]] Unveiling and Bridging the Functional Perception Gap in MLLMs: Atomic Visual Alignment and Hierarchical Evaluation via PET-Bench(https://arxiv.org/abs/2601.02737)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in tasks such as abnormality detection and report generation for anatomical modalities, their capability in functional imaging remains largely unexplored. In this work, we identify and quantify a fundamental functional perception gap: the inability of current vision encoders to decode functional tracer biodistribution independent of morphological priors. Identifying Positron Emission Tomography (PET) as the quintessential modality to investigate this disconnect, we introduce PET-Bench, the first large-scale functional imaging benchmark comprising 52,308 hierarchical QA pairs from 9,732 multi-site, multi-tracer PET studies. Extensive evaluation of 19 state-of-the-art MLLMs reveals a critical safety hazard termed the Chain-of-Thought (CoT) hallucination trap. We observe that standard CoT prompting, widely considered to enhance reasoning, paradoxically decouples linguistic generation from visual evidence in PET, producing clinically fluent but factually ungrounded diagnoses. To resolve this, we propose Atomic Visual Alignment (AVA), a simple fine-tuning strategy that enforces the mastery of low-level functional perception prior to high-level diagnostic reasoning. Our results demonstrate that AVA effectively bridges the perception gap, transforming CoT from a source of hallucination into a robust inference tool and improving diagnostic accuracy by up to 14.83%. Code and data are available at this https URL.</li>
<li><strong>摘要：</strong>虽然多模态大语言模型 (MLLM) 在异常检测和解剖模式报告生成等任务中表现出卓越的熟练程度，但它们在功能成像方面的能力在很大程度上仍未得到探索。在这项工作中，我们确定并量化了一个基本的功能感知差距：当前的视觉编码器无法独立于形态学先验来解码功能示踪剂生物分布。我们将正电子发射断层扫描 (PET) 确定为研究这种脱节的典型方式，因此推出了 PET-Bench，这是第一个大规模功能成像基准，包含来自 9,732 个多站点、多示踪剂 PET 研究的 52,308 个分层 QA 对。对 19 个最先进的 MLLM 的广泛评估揭示了一个被称为思想链 (CoT) 幻觉陷阱的严重安全隐患。我们观察到，标准 CoT 提示被广泛认为可以增强推理能力，但它却矛盾地将语言生成与 PET 中的视觉证据脱钩，从而产生临床流畅但实际上缺乏根据的诊断。为了解决这个问题，我们提出了原子视觉对齐（AVA），这是一种简单的微调策略，可以在高级诊断推理之前强制掌握低级功能感知。我们的结果表明，AVA 有效地弥合了感知差距，将 CoT 从幻觉来源转变为强大的推理工具，并将诊断准确性提高高达 14.83%。代码和数据可从此 https URL 获取。</li>
</ul>

<h3>Title: Q-Regularized Generative Auto-Bidding: From Suboptimal Trajectories to Optimal Policies</h3>
<ul>
<li><strong>Authors: </strong>Mingming Zhang, Na Li, Zhuang Feiqing, Hongyang Zheng, Jiangbing Zhou, Wang Wuyin, Sheng-jie Sun, XiaoWei Chen, Junxiong Zhu, Lixin Zou, Chenliang Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02754">https://arxiv.org/abs/2601.02754</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02754">https://arxiv.org/pdf/2601.02754</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02754]] Q-Regularized Generative Auto-Bidding: From Suboptimal Trajectories to Optimal Policies(https://arxiv.org/abs/2601.02754)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the rapid development of e-commerce, auto-bidding has become a key asset in optimizing advertising performance under diverse advertiser environments. The current approaches focus on reinforcement learning (RL) and generative models. These efforts imitate offline historical behaviors by utilizing a complex structure with expensive hyperparameter tuning. The suboptimal trajectories further exacerbate the difficulty of policy learning. To address these challenges, we proposes QGA, a novel Q-value regularized Generative Auto-bidding method. In QGA, we propose to plug a Q-value regularization with double Q-learning strategy into the Decision Transformer backbone. This design enables joint optimization of policy imitation and action-value maximization, allowing the learned bidding policy to both leverage experience from the dataset and alleviate the adverse impact of the suboptimal trajectories. Furthermore, to safely explore the policy space beyond the data distribution, we propose a Q-value guided dual-exploration mechanism, in which the DT model is conditioned on multiple return-to-go targets and locally perturbed actions. This entire exploration process is dynamically guided by the aforementioned Q-value module, which provides principled evaluation for each candidate action. Experiments on public benchmarks and simulation environments demonstrate that QGA consistently achieves superior or highly competitive results compared to existing alternatives. Notably, in large-scale real-world A/B testing, QGA achieves a 3.27% increase in Ad GMV and a 2.49% improvement in Ad ROI.</li>
<li><strong>摘要：</strong>随着电子商务的快速发展，自动出价已成为在多样化广告主环境下优化广告效果的关键资产。当前的方法侧重于强化学习（RL）和生成模型。这些努力通过利用复杂的结构和昂贵的超参数调整来模仿离线历史行为。次优轨迹进一步加剧了政策学习的难度。为了应对这些挑战，我们提出了 QGA，一种新颖的 Q 值正则化生成自动出价方法。在 QGA 中，我们建议将具有双 Q 学习策略的 Q 值正则化插入到 Decision Transformer 主干中。这种设计可以实现策略模仿和行动价值最大化的联合优化，使学习到的竞价策略既可以利用数据集中的经验，又可以减轻次优轨迹的不利影响。此外，为了安全地探索数据分布之外的策略空间，我们提出了一种 Q 值引导的双重探索机制，其中 DT 模型以多个返回目标和局部扰动动作为条件。整个探索过程由前面提到的 Q 值模块动态引导，该模块为每个候选动作提供原则性评估。公共基准和模拟环境的实验表明，与现有替代方案相比，QGA 始终能够实现卓越或极具竞争力的结果。值得注意的是，在大规模的真实 A/B 测试中，QGA 实现了广告 GMV 提升 3.27%，广告 ROI 提升 2.49%。</li>
</ul>

<h3>Title: ClearAIR: A Human-Visual-Perception-Inspired All-in-One Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Xu Zhang, Huan Zhang, Guoli Wang, Qian Zhang, Lefei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02763">https://arxiv.org/abs/2601.02763</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02763">https://arxiv.org/pdf/2601.02763</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02763]] ClearAIR: A Human-Visual-Perception-Inspired All-in-One Image Restoration(https://arxiv.org/abs/2601.02763)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, quality assessment</a></li>
<li><strong>Abstract: </strong>All-in-One Image Restoration (AiOIR) has advanced significantly, offering promising solutions for complex real-world degradations. However, most existing approaches rely heavily on degradation-specific representations, often resulting in oversmoothing and artifacts. To address this, we propose ClearAIR, a novel AiOIR framework inspired by Human Visual Perception (HVP) and designed with a hierarchical, coarse-to-fine restoration strategy. First, leveraging the global priority of early HVP, we employ a Multimodal Large Language Model (MLLM)-based Image Quality Assessment (IQA) model for overall evaluation. Unlike conventional IQA, our method integrates cross-modal understanding to more accurately characterize complex, composite degradations. Building upon this overall assessment, we then introduce a region awareness and task recognition pipeline. A semantic cross-attention, leveraging semantic guidance unit, first produces coarse semantic prompts. Guided by this regional context, a degradation-aware module implicitly captures region-specific degradation characteristics, enabling more precise local restoration. Finally, to recover fine details, we propose an internal clue reuse mechanism. It operates in a self-supervised manner to mine and leverage the intrinsic information of the image itself, substantially enhancing detail restoration. Experimental results show that ClearAIR achieves superior performance across diverse synthetic and real-world datasets.</li>
<li><strong>摘要：</strong>一体式图像恢复 (AiOIR) 取得了显着进步，为复杂的现实世界退化提供了有前景的解决方案。然而，大多数现有方法严重依赖于特定于退化的表示，通常会导致过度平滑和伪影。为了解决这个问题，我们提出了 ClearAIR，这是一种新颖的 AiOIR 框架，其灵感来自人类视觉感知 (HVP)，并采用分层、从粗到细的恢复策略进行设计。首先，利用早​​期 HVP 的全局优先级，我们采用基于多模态大语言模型 (MLLM) 的图像质量评估 (IQA) 模型进行总体评估。与传统的 IQA 不同，我们的方法集成了跨模式理解，可以更准确地表征复杂的复合退化。在此总体评估的基础上，我们引入了区域意识和任务识别管道。语义交叉注意力利用语义指导单元，首先产生粗略的语义提示。在该区域背景的指导下，退化感知模块隐式捕获特定区域的退化特征，从而实现更精确的局部恢复。最后，为了恢复细节，我们提出了一种内部线索重用机制。它以自我监督的方式运行，挖掘和利用图像本身的内在信息，从而大大增强细节恢复。实验结果表明，ClearAIR 在不同的合成数据集和真实数据集上实现了卓越的性能。</li>
</ul>

<h3>Title: EarthVL: A Progressive Earth Vision-Language Understanding and Generation Framework</h3>
<ul>
<li><strong>Authors: </strong>Junjue Wang, Yanfei Zhong, Zihang Chen, Zhuo Zheng, Ailong Ma, Liangpei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02783">https://arxiv.org/abs/2601.02783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02783">https://arxiv.org/pdf/2601.02783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02783]] EarthVL: A Progressive Earth Vision-Language Understanding and Generation Framework(https://arxiv.org/abs/2601.02783)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Earth vision has achieved milestones in geospatial object recognition but lacks exploration in object-relational reasoning, limiting comprehensive scene understanding. To address this, a progressive Earth vision-language understanding and generation framework is proposed, including a multi-task dataset (EarthVLSet) and a semantic-guided network (EarthVLNet). Focusing on city planning applications, EarthVLSet includes 10.9k sub-meter resolution remote sensing images, land-cover masks, and 761.5k textual pairs involving both multiple-choice and open-ended visual question answering (VQA) tasks. In an object-centric way, EarthVLNet is proposed to progressively achieve semantic segmentation, relational reasoning, and comprehensive understanding. The first stage involves land-cover segmentation to generate object semantics for VQA guidance. Guided by pixel-wise semantics, the object awareness based large language model (LLM) performs relational reasoning and knowledge summarization to generate the required answers. As for optimization, the numerical difference loss is proposed to dynamically add difference penalties, addressing the various objects' statistics. Three benchmarks, including semantic segmentation, multiple-choice, and open-ended VQA demonstrated the superiorities of EarthVLNet, yielding three future directions: 1) segmentation features consistently enhance VQA performance even in cross-dataset scenarios; 2) multiple-choice tasks show greater sensitivity to the vision encoder than to the language decoder; and 3) open-ended tasks necessitate advanced vision encoders and language decoders for an optimal performance. We believe this dataset and method will provide a beneficial benchmark that connects ''image-mask-text'', advancing geographical applications for Earth vision.</li>
<li><strong>摘要：</strong>地球视觉在地理空间物体识别方面取得了里程碑式的成就，但缺乏对物体关系推理的探索，限制了全面的场景理解。为了解决这个问题，提出了一种渐进式地球视觉语言理解和生成框架，包括多任务数据集（EarthVLSet）和语义引导网络（EarthVLNet）。 EarthVLSet 专注于城市规划应用，包括 10.9k 亚米分辨率遥感图像、土地覆盖掩模和 761.5k 文本对，涉及多项选择和开放式视觉问答 (VQA) 任务。 EarthVLNet以以对象为中心的方式逐步实现语义分割、关系推理和综合理解。第一阶段涉及土地覆盖分割，以生成用于 VQA 指导的对象语义。在像素语义的指导下，基于对象感知的大语言模型（LLM）执行关系推理和知识总结以生成所需的答案。至于优化，提出了数值差异损失来动态添加差异惩罚，解决各种对象的统计数据。包括语义分割、多项选择和开放式 VQA 在内的三个基准测试证明了 EarthVLNet 的优越性，并产生了三个未来方向：1）分割特征即使在跨数据集场景下也能持续增强 VQA 性能； 2）多项选择任务对视觉编码器比对语言解码器表现出更大的敏感性； 3) 开放式任务需要先进的视觉编码器和语言解码器才能获得最佳性能。我们相信这个数据集和方法将提供一个连接“图像-掩模-文本”的有益基准，推进地球视觉的地理应用。</li>
</ul>

<h3>Title: DreamStyle: A Unified Framework for Video Stylization</h3>
<ul>
<li><strong>Authors: </strong>Mengtian Li, Jinshu Chen, Songtao Zhao, Wanquan Feng, Pengqi Tu, Qian He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02785">https://arxiv.org/abs/2601.02785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02785">https://arxiv.org/pdf/2601.02785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02785]] DreamStyle: A Unified Framework for Video Stylization(https://arxiv.org/abs/2601.02785)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Video stylization, an important downstream task of video generation models, has not yet been thoroughly explored. Its input style conditions typically include text, style image, and stylized first frame. Each condition has a characteristic advantage: text is more flexible, style image provides a more accurate visual anchor, and stylized first frame makes long-video stylization feasible. However, existing methods are largely confined to a single type of style condition, which limits their scope of application. Additionally, their lack of high-quality datasets leads to style inconsistency and temporal flicker. To address these limitations, we introduce DreamStyle, a unified framework for video stylization, supporting (1) text-guided, (2) style-image-guided, and (3) first-frame-guided video stylization, accompanied by a well-designed data curation pipeline to acquire high-quality paired video data. DreamStyle is built on a vanilla Image-to-Video (I2V) model and trained using a Low-Rank Adaptation (LoRA) with token-specific up matrices that reduces the confusion among different condition tokens. Both qualitative and quantitative evaluations demonstrate that DreamStyle is competent in all three video stylization tasks, and outperforms the competitors in style consistency and video quality.</li>
<li><strong>摘要：</strong>视频风格化是视频生成模型的重要下游任务，尚未得到彻底探索。其输入样式条件通常包括文本、样式图像和样式化的第一帧。每种条件都有其独特的优势：文本更灵活，风格图像提供更准确的视觉锚点，风格化的第一帧使长视频风格化变得可行。然而，现有方法很大程度上局限于单一类型的样式条件，这限制了它们的应用范围。此外，缺乏高质量的数据集会导致风格不一致和时间闪烁。为了解决这些限制，我们引入了 DreamStyle，一个统一的视频风格化框架，支持 (1) 文本引导、(2) 风格图像引导和 (3) 第一帧引导视频风格化，并配有精心设计的数据管理管道来获取高质量的配对视频数据。 DreamStyle 基于普通图像到视频 (I2V) 模型构建，并使用低秩适应 (LoRA) 和特定于令牌的向上矩阵进行训练，以减少不同条件令牌之间的混淆。定性和定量评估都表明，DreamStyle 能够胜任所有三项视频风格化任务，并且在风格一致性和视频质量方面优于竞争对手。</li>
</ul>

<h3>Title: RadioDiff-Flux: Efficient Radio Map Construction via Generative Denoise Diffusion Model Trajectory Midpoint Reuse</h3>
<ul>
<li><strong>Authors: </strong>Xiucheng Wang, Peilin Zheng, Honggang Jia, Nan Cheng, Ruijin Sun, Conghao Zhou, Xuemin Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02790">https://arxiv.org/abs/2601.02790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02790">https://arxiv.org/pdf/2601.02790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02790]] RadioDiff-Flux: Efficient Radio Map Construction via Generative Denoise Diffusion Model Trajectory Midpoint Reuse(https://arxiv.org/abs/2601.02790)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Accurate radio map (RM) construction is essential to enabling environment-aware and adaptive wireless communication. However, in future 6G scenarios characterized by high-speed network entities and fast-changing environments, it is very challenging to meet real-time requirements. Although generative diffusion models (DMs) can achieve state-of-the-art accuracy with second-level delay, their iterative nature leads to prohibitive inference latency in delay-sensitive scenarios. In this paper, by uncovering a key structural property of diffusion processes: the latent midpoints remain highly consistent across semantically similar scenes, we propose RadioDiff-Flux, a novel two-stage latent diffusion framework that decouples static environmental modeling from dynamic refinement, enabling the reuse of precomputed midpoints to bypass redundant denoising. In particular, the first stage generates a coarse latent representation using only static scene features, which can be cached and shared across similar scenarios. The second stage adapts this representation to dynamic conditions and transmitter locations using a pre-trained model, thereby avoiding repeated early-stage computation. The proposed RadioDiff-Flux significantly reduces inference time while preserving fidelity. Experiment results show that RadioDiff-Flux can achieve up to 50 acceleration with less than 0.15% accuracy loss, demonstrating its practical utility for fast, scalable RM generation in future 6G networks.</li>
<li><strong>摘要：</strong>准确的无线电地图 (RM) 构建对于实现环境感知和自适应无线通信至关重要。然而，在未来以高速网络实体和快速变化的环境为特征的6G场景中，满足实时性要求非常具有挑战性。尽管生成扩散模型 (DM) 可以通过秒级延迟实现最先进的精度，但其迭代性质会导致在延迟敏感场景中出现令人望而却步的推理延迟。在本文中，通过揭示扩散过程的一个关键结构特性：潜在中点在语义相似的场景中保持高度一致，我们提出了RadioDiff-Flux，一种新颖的两阶段潜在扩散框架，它将静态环境建模与动态细化分离，从而能够重用预先计算的中点来绕过冗余去噪。特别是，第一阶段仅使用静态场景特征生成粗略的潜在表示，该表示可以在类似的场景中缓存和共享。第二阶段使用预先训练的模型使这种表示适应动态条件和发射机位置，从而避免重复的早期计算。所提出的 RadioDiff-Flux 显着减少了推理时间，同时保持了保真度。实验结果表明，RadioDiff-Flux 可以实现高达 50 的加速，且精度损失小于 0.15%，展示了其在未来 6G 网络中快速、可扩展的 RM 生成的实用性。</li>
</ul>

<h3>Title: Stratified Hazard Sampling: Minimal-Variance Event Scheduling for CTMC/DTMC Discrete Diffusion and Flow Models</h3>
<ul>
<li><strong>Authors: </strong>Seunghwan Jang, SooJean Han</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02799">https://arxiv.org/abs/2601.02799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02799">https://arxiv.org/pdf/2601.02799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02799]] Stratified Hazard Sampling: Minimal-Variance Event Scheduling for CTMC/DTMC Discrete Diffusion and Flow Models(https://arxiv.org/abs/2601.02799)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>CTMC/DTMC-based discrete generative models, including uniform-noise discrete diffusion (e.g., D3PM/CTDD) and discrete flow matching, enable non-autoregressive sequence generation by repeatedly replacing tokens through a time-inhomogeneous Markov process. Inference is typically implemented with step-based simulation: each token decides to jump via independent Bernoulli (or categorical) draws at every discretization step. Under uniform-noise initialization, where self-correction requires multiple edits per position, these independent decisions induce substantial variance in both the number and timing of edits, leading to characteristic failure modes such as under-editing (residual noise) or over-editing (cascading unnecessary substitutions), decreasing reproducibility. We propose Stratified Hazard Sampling (SHS), a drop-in and hyperparameter-free inference principle for any sampler that admits a stay-vs.-replace decomposition. SHS models per-token edits as events driven by cumulative hazard (CTMC) or cumulative jump mass (DTMC) and places events by stratifying this cumulative quantity: with a single random phase per position, a token jumps whenever its accumulated hazard crosses unit-spaced thresholds. This preserves the expected number of jumps while achieving the minimum possible variance among unbiased integer estimators (bounded by 1/4), without altering per-jump destination sampling and thus retaining multimodality. We also introduce a phase-allocation variant for blacklist-style lexical constraints that prioritizes early edits at high-risk positions to mitigate late-masking artifacts.</li>
<li><strong>摘要：</strong>基于 CTMC/DTMC 的离散生成模型，包括均匀噪声离散扩散（例如 D3PM/CTDD）和离散流匹配，通过时间非均匀马尔可夫过程重复替换标记来实现非自回归序列生成。推理通常通过基于步骤的模拟来实现：每个标记决定在每个离散化步骤中通过独立的伯努利（或分类）绘制进行跳跃。在统一噪声初始化下，自我校正需要对每个位置进行多次编辑，这些独立的决定会导致编辑数量和时间的巨大差异，导致典型的故障模式，例如编辑不足（残留噪声）或过度编辑（级联不必要的替换），从而降低再现性。我们提出了分层危险采样（SHS），这是一种适用于任何允许保留与替换分解的采样器的直接且无超参数的推理原理。 SHS 将每个令牌编辑建模为由累积危险 (CTMC) 或累积跳跃质量 (DTMC) 驱动的事件，并通过对累积数量进行分层来放置事件：每个位置有一个随机阶段，每当其累积危险超过单位间隔阈值时，令牌就会跳跃。这保留了预期的跳转次数，同时实现无偏整数估计器之间的最小可能方差（以 1/4 为界），而不改变每次跳转目的地采样，从而保留多模态。我们还引入了黑名单式词汇约束的阶段分配变体，该变体优先考虑高风险位置的早期编辑，以减轻后期掩蔽伪影。</li>
</ul>

<h3>Title: Topology-aware Pathological Consistency Matching for Weakly-Paired IHC Virtual Staining</h3>
<ul>
<li><strong>Authors: </strong>Mingzhou Jiang, Jiaying Zhou, Nan Zeng, Mickael Li, Qijie Tang, Chao He, Huazhu Fu, Honghui He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02806">https://arxiv.org/abs/2601.02806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02806">https://arxiv.org/pdf/2601.02806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02806]] Topology-aware Pathological Consistency Matching for Weakly-Paired IHC Virtual Staining(https://arxiv.org/abs/2601.02806)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Immunohistochemical (IHC) staining provides crucial molecular characterization of tissue samples and plays an indispensable role in the clinical examination and diagnosis of cancers. However, compared with the commonly used Hematoxylin and Eosin (H&E) staining, IHC staining involves complex procedures and is both time-consuming and expensive, which limits its widespread clinical use. Virtual staining converts H&E images to IHC images, offering a cost-effective alternative to clinical IHC staining. Nevertheless, using adjacent slides as ground truth often results in weakly-paired data with spatial misalignment and local deformations, hindering effective supervised learning. To address these challenges, we propose a novel topology-aware framework for H&E-to-IHC virtual staining. Specifically, we introduce a Topology-aware Consistency Matching (TACM) mechanism that employs graph contrastive learning and topological perturbations to learn robust matching patterns despite spatial misalignments, ensuring structural consistency. Furthermore, we propose a Topology-constrained Pathological Matching (TCPM) mechanism that aligns pathological positive regions based on node importance to enhance pathological consistency. Extensive experiments on two benchmarks across four staining tasks demonstrate that our method outperforms state-of-the-art approaches, achieving superior generation quality with higher clinical relevance.</li>
<li><strong>摘要：</strong>免疫组织化学 (IHC) 染色提供了组织样本的重要分子特征，在癌症的临床检查和诊断中发挥着不可或缺的作用。然而，与常用的苏木精和伊红（H＆E）染色相比，IHC染色操作复杂，耗时且昂贵，限制了其广泛的临床应用。虚拟染色将 H&E 图像转换为 IHC 图像，为临床 IHC 染色提供了一种经济高效的替代方案。然而，使用相邻幻灯片作为基本事实通常会导致弱配对数据具有空间错位和局部变形，从而阻碍有效的监督学习。为了应对这些挑战，我们提出了一种用于 H&E 到 IHC 虚拟染色的新型拓扑感知框架。具体来说，我们引入了一种拓扑感知一致性匹配（TACM）机制，该机制采用图对比学习和拓扑扰动来学习鲁棒的匹配模式，尽管存在空间错位，从而确保结构一致性。此外，我们提出了一种拓扑约束病理匹配（TCPM）机制，该机制根据节点重要性对齐病理阳性区域，以增强病理一致性。对四个染色任务的两个基准进行的广泛实验表明，我们的方法优于最先进的方法，实现了具有更高临床相关性的卓越生成质量。</li>
</ul>

<h3>Title: Electricity Price Forecasting: Bridging Linear Models, Neural Networks and Online Learning</h3>
<ul>
<li><strong>Authors: </strong>Btissame El Mahtout, Florian Ziel</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02856">https://arxiv.org/abs/2601.02856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02856">https://arxiv.org/pdf/2601.02856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02856]] Electricity Price Forecasting: Bridging Linear Models, Neural Networks and Online Learning(https://arxiv.org/abs/2601.02856)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Precise day-ahead forecasts for electricity prices are crucial to ensure efficient portfolio management, support strategic decision-making for power plant operations, enable efficient battery storage optimization, and facilitate demand response planning. However, developing an accurate prediction model is highly challenging in an uncertain and volatile market environment. For instance, although linear models generally exhibit competitive performance in predicting electricity prices with minimal computational requirements, they fail to capture relevant nonlinear relationships. Nonlinear models, on the other hand, can improve forecasting accuracy with a surge in computational costs. We propose a novel multivariate neural network approach that combines linear and nonlinear feed-forward neural structures. Unlike previous hybrid models, our approach integrates online learning and forecast combination for efficient training and accuracy improvement. It also incorporates all relevant characteristics, particularly the fundamental relationships arising from wind and solar generation, electricity demand patterns, related energy fuel and carbon markets, in addition to autoregressive dynamics and calendar effects. Compared to the current state-of-the-art benchmark models, the proposed forecasting method significantly reduces computational cost while delivering superior forecasting accuracy (12-13% RMSE and 15-18% MAE reductions). Our results are derived from a six-year forecasting study conducted on major European electricity markets.</li>
<li><strong>摘要：</strong>准确的日前电价预测对于确保高效的投资组合管理、支持发电厂运营的战略决策、实现高效的电池存储优化以及促进需求响应规划至关重要。然而，在不确定且波动的市场环境中，开发准确的预测模型极具挑战性。例如，尽管线性模型通常在以最少的计算要求预测电价方面表现出竞争性能，但它们无法捕获相关的非线性关系。另一方面，非线性模型可以通过计算成本的激增来提高预测精度。我们提出了一种新颖的多元神经网络方法，结合了线性和非线性前馈神经结构。与之前的混合模型不同，我们的方法集成了在线学习和预测组合，以实现高效训练和准确性提高。除了自回归动态和日历效应之外，它还包含所有相关特征，特别是风能和太阳能发电、电力需求模式、相关能源燃料和碳市场产生的基本关系。与当前最先进的基准模型相比，所提出的预测方法显着降低了计算成本，同时提供了卓越的预测精度（RMSE 降低了 12-13%，MAE 降低了 15-18%）。我们的结果来自对欧洲主要电力市场进行的六年预测研究。</li>
</ul>

<h3>Title: ChemBART: A Pre-trained BART Model Assisting Organic Chemistry Analysis</h3>
<ul>
<li><strong>Authors: </strong>Kenan Li, Yijian Zhang, Jin Wang, Haipeng Gan, Zeying Sun, Xiaoguang Lei, Hao Dong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02915">https://arxiv.org/abs/2601.02915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02915">https://arxiv.org/pdf/2601.02915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02915]] ChemBART: A Pre-trained BART Model Assisting Organic Chemistry Analysis(https://arxiv.org/abs/2601.02915)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have demonstrated transformative potential across diverse fields. While LLMs have been applied to molecular simplified molecular input line entry system (SMILES) in computer-aided synthesis planning (CASP), existing methodologies typically address single tasks, such as precursor prediction. We introduce ChemBART, a SMILES-based LLM pre-trained on chemical reactions, which enables a unified model for multiple downstream chemical tasks--achieving the paradigm of "one model, one pre-training, multiple tasks." By leveraging outputs from a mask-filling pre-training task on reaction expressions, ChemBART effectively solves a variety of chemical problems, including precursor/reagent generation, temperature-yield regression, molecular property classification, and optimizing the policy and value functions within a reinforcement learning framework, integrated with Monte Carlo tree search for multi-step synthesis route design. Unlike single-molecule pre-trained LLMs constrained to specific applications, ChemBART addresses broader chemical challenges and integrates them for comprehensive synthesis planning. Crucially, ChemBART-designed multi-step synthesis routes and reaction conditions directly inspired wet-lab validation, which confirmed shorter pathways with ~30% yield improvement over literature benchmarks. Our work validates the power of reaction-focused pre-training and showcases the broad utility of ChemBART in advancing the complete synthesis planning cycle.</li>
<li><strong>摘要：</strong>大语言模型 (LLM) 的最新进展已经证明了跨不同领域的变革潜力。虽然法学硕士已应用于计算机辅助合成规划（CASP）中的分子简化分子输入线输入系统（SMILES），但现有方法通常只解决单一任务，例如前体预测。我们推出了 ChemBART，这是一种基于 SMILES 的化学反应预训练法学硕士，可以为多个下游化学任务提供统一的模型，实现“一个模型、一个预训练、多个任务”的范式。通过利用反应表达式掩模填充预训练任务的输出，ChemBART 有效解决了各种化学问题，包括前体/试剂生成、温度产率回归、分子特性分类以及在强化学习框架内优化策略和价值函数，并与蒙特卡罗树搜索集成以进行多步合成路线设计。与受限于特定应用的单分子预训练法学硕士不同，ChemBART 解决了更广泛的化学挑战，并将其集成以进行全面的合成规划。至关重要的是，ChemBART 设计的多步合成路线和反应条件直接激发了湿实验室验证，证实了更短的途径，产量比文献基准提高了约 30%。我们的工作验证了以反应为中心的预训练的力量，并展示了 ChemBART 在推进完整合成计划周期方面的广泛实用性。</li>
</ul>

<h3>Title: Zoom-IQA: Image Quality Assessment with Reliable Region-Aware Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Guoqiang Liang, Jianyi Wang, Zhonghua Wu, Shangchen Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02918">https://arxiv.org/abs/2601.02918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02918">https://arxiv.org/pdf/2601.02918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02918]] Zoom-IQA: Image Quality Assessment with Reliable Region-Aware Reasoning(https://arxiv.org/abs/2601.02918)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generation, quality assessment</a></li>
<li><strong>Abstract: </strong>Image Quality Assessment (IQA) is a long-standing problem in computer vision. Previous methods typically focus on predicting numerical scores without explanation or provide low-level descriptions lacking precise scores. Recent reasoning-based vision language models (VLMs) have shown strong potential for IQA, enabling joint generation of quality descriptions and scores. However, we notice that existing VLM-based IQA methods tend to exhibit unreliable reasoning due to their limited capability of integrating visual and textual cues. In this work, we introduce Zoom-IQA, a VLM-based IQA model to explicitly emulate key cognitive behaviors: uncertainty awareness, region reasoning, and iterative refinement. Specifically, we present a two-stage training pipeline: 1) supervised fine-tuning (SFT) on our Grounded-Rationale-IQA (GR-IQA) dataset to teach the model to ground its assessments in key regions; and 2) reinforcement learning (RL) for dynamic policy exploration, primarily stabilized by our KL-Coverage regularizer to prevent reasoning and scoring diversity collapse, and supported by a Progressive Re-sampling Strategy to mitigate annotation bias. Extensive experiments show that Zoom-IQA achieves improved robustness, explainability, and generalization. The application to downstream tasks, such as image restoration, further demonstrates the effectiveness of Zoom-IQA.</li>
<li><strong>摘要：</strong>图像质量评估（IQA）是计算机视觉领域长期存在的问题。以前的方法通常侧重于预测数字分数而不进行解释，或者提供缺乏精确分数的低级描述。最近基于推理的视觉语言模型 (VLM) 在 IQA 方面显示出强大的潜力，可以联合生成质量描述和分数。然而，我们注意到现有的基于 VLM 的 IQA 方法由于整合视觉和文本线索的能力有限，往往表现出不可靠的推理。在这项工作中，我们引入了 Zoom-IQA，这是一种基于 VLM 的 IQA 模型，用于显式模拟关键认知行为：不确定性意识、区域推理和迭代细化。具体来说，我们提出了一个两阶段的训练流程：1）在我们的 Grounded-Rationale-IQA（GR-IQA）数据集上进行监督微调（SFT），以教导模型在关键区域进行评估； 2）用于动态策略探索的强化学习（RL），主要由我们的 KL-Coverage 正则化器稳定以防止推理和评分多样性崩溃，并由渐进式重采样策略支持以减轻注释偏差。大量实验表明 Zoom-IQA 提高了鲁棒性、可解释性和泛化性。在图像修复等下游任务中的应用进一步证明了Zoom-IQA的有效性。</li>
</ul>

<h3>Title: VTONQA: A Multi-Dimensional Quality Assessment Dataset for Virtual Try-on</h3>
<ul>
<li><strong>Authors: </strong>Xinyi Wei, Sijing Wu, Zitong Xu, Yunhao Li, Huiyu Duan, Xiongkuo Min, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02945">https://arxiv.org/abs/2601.02945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02945">https://arxiv.org/pdf/2601.02945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02945]] VTONQA: A Multi-Dimensional Quality Assessment Dataset for Virtual Try-on(https://arxiv.org/abs/2601.02945)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>With the rapid development of e-commerce and digital fashion, image-based virtual try-on (VTON) has attracted increasing attention. However, existing VTON models often suffer from artifacts such as garment distortion and body inconsistency, highlighting the need for reliable quality evaluation of VTON-generated images. To this end, we construct VTONQA, the first multi-dimensional quality assessment dataset specifically designed for VTON, which contains 8,132 images generated by 11 representative VTON models, along with 24,396 mean opinion scores (MOSs) across three evaluation dimensions (i.e., clothing fit, body compatibility, and overall quality). Based on VTONQA, we benchmark both VTON models and a diverse set of image quality assessment (IQA) metrics, revealing the limitations of existing methods and highlighting the value of the proposed dataset. We believe that the VTONQA dataset and corresponding benchmarks will provide a solid foundation for perceptually aligned evaluation, benefiting both the development of quality assessment methods and the advancement of VTON models.</li>
<li><strong>摘要：</strong>随着电子商务和数字时尚的快速发展，基于图像的虚拟试穿（VTON）越来越受到关注。然而，现有的 VTON 模型经常存在服装变形和身体不一致等伪影，这凸显了对 VTON 生成的图像进行可靠的质量评估的必要性。为此，我们构建了第一个专门为 VTON 设计的多维度质量评估数据集 VTONQA，其中包含由 11 个代表性 VTON 模型生成的 8,132 张图像，以及三个评估维度（即服装合身性、身体兼容性和整体质量）的 24,396 个平均意见得分（MOS）。基于 VTONQA，我们对 VTON 模型和一组不同的图像质量评估（IQA）指标进行了基准测试，揭示了现有方法的局限性并突出了所提出的数据集的价值。我们相信，VTONQA 数据集和相应的基准将为感知一致评估提供坚实的基础，有利于质量评估方法的发展和 VTON 模型的进步。</li>
</ul>

<h3>Title: LAMS-Edit: Latent and Attention Mixing with Schedulers for Improved Content Preservation in Diffusion-Based Image and Style Editing</h3>
<ul>
<li><strong>Authors: </strong>Wingwa Fu, Takayuki Okatani</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02987">https://arxiv.org/abs/2601.02987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02987">https://arxiv.org/pdf/2601.02987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02987]] LAMS-Edit: Latent and Attention Mixing with Schedulers for Improved Content Preservation in Diffusion-Based Image and Style Editing(https://arxiv.org/abs/2601.02987)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-Image editing using diffusion models faces challenges in balancing content preservation with edit application and handling real-image editing. To address these, we propose LAMS-Edit, leveraging intermediate states from the inversion process--an essential step in real-image editing--during edited image generation. Specifically, latent representations and attention maps from both processes are combined at each step using weighted interpolation, controlled by a scheduler. This technique, Latent and Attention Mixing with Schedulers (LAMS), integrates with Prompt-to-Prompt (P2P) to form LAMS-Edit--an extensible framework that supports precise editing with region masks and enables style transfer via LoRA. Extensive experiments demonstrate that LAMS-Edit effectively balances content preservation and edit application.</li>
<li><strong>摘要：</strong>使用扩散模型的文本到图像编辑面临着平衡内容保存与编辑应用程序以及处理真实图像编辑的挑战。为了解决这些问题，我们提出了 LAMS-Edit，在编辑图像生成期间利用反转过程（真实图像编辑的重要步骤）中的中间状态。具体来说，来自两个过程的潜在表示和注意力图在每个步骤中使用加权插值进行组合，并由调度程序控制。这种技术，即潜在和注意力与调度程序混合 (LAMS)，与提示到提示 (P2P) 集成，形成 LAMS-Edit——一个可扩展框架，支持使用区域蒙版进行精确编辑，并通过 LoRA 实现风格传输。大量实验表明，LAMS-Edit 有效地平衡了内容保存和编辑应用。</li>
</ul>

<h3>Title: Towards Faithful Reasoning in Comics for Small MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Chengcheng Feng, Haojie Yin, Yucheng Jin, Kaizhu Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02991">https://arxiv.org/abs/2601.02991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02991">https://arxiv.org/pdf/2601.02991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02991]] Towards Faithful Reasoning in Comics for Small MLLMs(https://arxiv.org/abs/2601.02991)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Comic-based visual question answering (CVQA) poses distinct challenges to multimodal large language models (MLLMs) due to its reliance on symbolic abstraction, narrative logic, and humor, which differ from conventional VQA tasks. Although Chain-of-Thought (CoT) prompting is widely used to enhance MLLM reasoning, surprisingly, its direct application to CVQA often degrades performance, especially in small-scale models. Our theoretical and empirical analyses reveal that standard CoT in CVQA suffers from state entanglement, spurious transitions, and exploration inefficiency, with small models particularly vulnerable in resource-constrained settings. To address these issues, we propose a novel comic reasoning framework, designed to produce more faithful and transferable reasoning chains in small MLLMs. Specifically, our framework combines modular CoT generation with GRPO-based reinforcement fine-tuning and a novel structured reward. Beyond comic VQA, we further evaluate our approach on a broader class of humor-centric and abstract visual reasoning tasks, including meme understanding and editorial cartoon interpretation. Across five challenging benchmarks, our 3B model outperforms state-of-the-art methods, and plug-in experiments yield an additional average improvement of $\mathbf{12.1\%}$ across different MLLMs.</li>
<li><strong>摘要：</strong>基于漫画的视觉问答 (CVQA) 对多模态大语言模型 (MLLM) 提出了明显的挑战，因为它依赖于符号抽象、叙事逻辑和幽默，这与传统的 VQA 任务不同。尽管思想链 (CoT) 提示被广泛用于增强 MLLM 推理，但令人惊讶的是，将其直接应用于 CVQA 往往会降低性能，尤其是在小规模模型中。我们的理论和实证分析表明，CVQA 中的标准 CoT 存在状态纠缠、虚假转换和探索效率低下的问题，小型模型在资源受限的环境中尤其脆弱。为了解决这些问题，我们提出了一种新颖的漫画推理框架，旨在在小型 MLLM 中产生更忠实和可转移的推理链。具体来说，我们的框架将模块化 CoT 生成与基于 GRPO 的强化微调和新颖的结构化奖励相结合。除了漫画 VQA 之外，我们还进一步评估了我们在更广泛的以幽默为中心和抽象视觉推理任务中的方法，包括模因理解和编辑卡通解释。在五个具有挑战性的基准测试中，我们的 3B 模型优于最先进的方法，并且插件实验在不同的 MLLM 上产生了 $\mathbf{12.1\%}$ 的额外平均改进。</li>
</ul>

<h3>Title: From Memorization to Creativity: LLM as a Designer of Novel Neural-Architectures</h3>
<ul>
<li><strong>Authors: </strong>Waleed Khalid, Dmitry Ignatov, Radu Timofte</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02997">https://arxiv.org/abs/2601.02997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02997">https://arxiv.org/pdf/2601.02997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02997]] From Memorization to Creativity: LLM as a Designer of Novel Neural-Architectures(https://arxiv.org/abs/2601.02997)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) excel in program synthesis, yet their ability to autonomously navigate neural architecture design--balancing syntactic reliability, performance, and structural novelty--remains underexplored. We address this by placing a code-oriented LLM within a closed-loop synthesis framework, analyzing its evolution over 22 supervised fine-tuning cycles. The model synthesizes PyTorch convolutional networks which are validated, evaluated via low-fidelity performance signals (single-epoch accuracy), and filtered using a MinHash-Jaccard criterion to prevent structural redundancy. High-performing, novel architectures are converted into prompt-code pairs for iterative fine-tuning via parameter-efficient LoRA adaptation, initialized from the LEMUR dataset. Across cycles, the LLM internalizes empirical architectural priors, becoming a robust generator. The valid generation rate stabilizes at 50.6 percent (peaking at 74.5 percent), while mean first-epoch accuracy rises from 28.06 percent to 50.99 percent, and the fraction of candidates exceeding 40 percent accuracy grows from 2.04 percent to 96.81 percent. Analyses confirm the model moves beyond replicating existing motifs, synthesizing 455 high-performing architectures absent from the original corpus. By grounding code synthesis in execution feedback, this work provides a scalable blueprint for transforming stochastic generators into autonomous, performance-driven neural designers, establishing that LLMs can internalize empirical, non-textual rewards to transcend their training data.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在程序合成方面表现出色，但它们自主导航神经架构设计（平衡句法可靠性、性能和结构新颖性）的能力仍未得到充分开发。我们通过将面向代码的 LLM 放置在闭环综合框架内来解决这个问题，分析其在 22 个监督微调周期中的演变。该模型综合了 PyTorch 卷积网络，该网络经过验证，通过低保真性能信号（单周期精度）进行评估，并使用 MinHash-Jaccard 标准进行过滤以防止结构冗余。高性能、新颖的架构被转换为提示代码对，以便通过从 LEMUR 数据集初始化的参数高效 LoRA 适应进行迭代微调。在各个周期中，法学硕士将经验架构先验内化，成为一个强大的生成器。有效生成率稳定在 50.6%（峰值为 74.5%），而平均第一轮准确率从 28.06% 上升到 50.99%，超过 40% 准确率的候选者比例从 2.04% 上升到 96.81%。分析证实该模型超越了复制现有主题，合成了原始语料库中不存在的 455 个高性能架构。通过将代码合成置于执行反馈中，这项工作提供了一个可扩展的蓝图，用于将随机生成器转变为自主的、性能驱动的神经设计器，并确定法学硕士可以内化经验的、非文本的奖励，以超越其训练数据。</li>
</ul>

<h3>Title: Flow Matching and Diffusion Models via PointNet for Generating Fluid Fields on Irregular Geometries</h3>
<ul>
<li><strong>Authors: </strong>Ali Kashefi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03030">https://arxiv.org/abs/2601.03030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03030">https://arxiv.org/pdf/2601.03030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03030]] Flow Matching and Diffusion Models via PointNet for Generating Fluid Fields on Irregular Geometries(https://arxiv.org/abs/2601.03030)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present two novel generative geometric deep learning frameworks, termed Flow Matching PointNet and Diffusion PointNet, for predicting fluid flow variables on irregular geometries by incorporating PointNet into flow matching and diffusion models, respectively. In these frameworks, a reverse generative process reconstructs physical fields from standard Gaussian noise conditioned on unseen geometries. The proposed approaches operate directly on point-cloud representations of computational domains (e.g., grid vertices of finite-volume meshes) and therefore avoid the limitations of pixelation used to project geometries onto uniform lattices. In contrast to graph neural network-based diffusion models, Flow Matching PointNet and Diffusion PointNet do not exhibit high-frequency noise artifacts in the predicted fields. Moreover, unlike such approaches, which require auxiliary intermediate networks to condition geometry, the proposed frameworks rely solely on PointNet, resulting in a simple and unified architecture. The performance of the proposed frameworks is evaluated on steady incompressible flow past a cylinder, using a geometric dataset constructed by varying the cylinder's cross-sectional shape and orientation across samples. The results demonstrate that Flow Matching PointNet and Diffusion PointNet achieve more accurate predictions of velocity and pressure fields, as well as lift and drag forces, and exhibit greater robustness to incomplete geometries compared to a vanilla PointNet with the same number of trainable parameters.</li>
<li><strong>摘要：</strong>我们提出了两种新颖的生成几何深度学习框架，称为 Flow Matching PointNet 和 Diffusion PointNet，用于通过将 PointNet 分别纳入流匹配和扩散模型来预测不规则几何形状上的流体流动变量。在这些框架中，反向生成过程根据看不见的几何形状的标准高斯噪声重建物理场。所提出的方法直接对计算域的点云表示（例如，有限体积网格的网格顶点）进行操作，因此避免了用于将几何形状投影到均匀晶格上的像素化​​的限制。与基于图神经网络的扩散模型相比，Flow Matching PointNet 和 Diffusion PointNet 在预测场中不会表现出高频噪声伪影。此外，与需要辅助中间网络来调节几何形状的方法不同，所提出的框架仅依赖于 PointNet，从而形成简单且统一的架构。所提出的框架的性能是在通过圆柱体的稳定不可压缩流上进行评估的，使用通过改变圆柱体的横截面形状和跨样本的方向构建的几何数据集。结果表明，与具有相同数量可训练参数的普通 PointNet 相比，Flow Matching PointNet 和 Diffusion PointNet 可以更准确地预测速度和压力场以及升力和阻力，并且对不完整的几何形状表现出更高的鲁棒性。</li>
</ul>

<h3>Title: Causal Manifold Fairness: Enforcing Geometric Invariance in Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Vidhi Rathore</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03032">https://arxiv.org/abs/2601.03032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03032">https://arxiv.org/pdf/2601.03032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03032]] Causal Manifold Fairness: Enforcing Geometric Invariance in Representation Learning(https://arxiv.org/abs/2601.03032)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Fairness in machine learning is increasingly critical, yet standard approaches often treat data as static points in a high-dimensional space, ignoring the underlying generative structure. We posit that sensitive attributes (e.g., race, gender) do not merely shift data distributions but causally warp the geometry of the data manifold itself. To address this, we introduce Causal Manifold Fairness (CMF), a novel framework that bridges causal inference and geometric deep learning. CMF learns a latent representation where the local Riemannian geometry, defined by the metric tensor and curvature, remains invariant under counterfactual interventions on sensitive attributes. By enforcing constraints on the Jacobian and Hessian of the decoder, CMF ensures that the rules of the latent space (distances and shapes) are preserved across demographic groups. We validate CMF on synthetic Structural Causal Models (SCMs), demonstrating that it effectively disentangles sensitive geometric warping while preserving task utility, offering a rigorous quantification of the fairness-utility trade-off via geometric metrics.</li>
<li><strong>摘要：</strong>机器学习中的公平性越来越重要，但标准方法通常将数据视为高维空间中的静态点，而忽略了底层的生成结构。我们假设敏感属性（例如种族、性别）不仅会改变数据分布，还会扭曲数据流形本身的几何形状。为了解决这个问题，我们引入了因果流形公平性（CMF），这是一种连接因果推理和几何深度学习的新颖框架。 CMF 学习一种潜在表示，其中由度量张量和曲率定义的局部黎曼几何在敏感属性的反事实干预下保持不变。通过对解码器的雅可比行列式和海森行列式施加约束，CMF 确保潜在空间的规则（距离和形状）在不同人口群体中得到保留。我们在合成结构因果模型 (SCM) 上验证了 CMF，证明它可以有效地消除敏感的几何扭曲，同时保留任务效用，通过几何指标提供对公平性与效用权衡的严格量化。</li>
</ul>

<h3>Title: Motion Blur Robust Wheat Pest Damage Detection with Dynamic Fuzzy Feature Fusion</h3>
<ul>
<li><strong>Authors: </strong>Han Zhang, Yanwei Wang, Fang Li, Hongjun Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03046">https://arxiv.org/abs/2601.03046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03046">https://arxiv.org/pdf/2601.03046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03046]] Motion Blur Robust Wheat Pest Damage Detection with Dynamic Fuzzy Feature Fusion(https://arxiv.org/abs/2601.03046)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Motion blur caused by camera shake produces ghosting artifacts that substantially degrade edge side object detection. Existing approaches either suppress blur as noise and lose discriminative structure, or apply full image restoration that increases latency and limits deployment on resource constrained devices. We propose DFRCP, a Dynamic Fuzzy Robust Convolutional Pyramid, as a plug in upgrade to YOLOv11 for blur robust detection. DFRCP enhances the YOLOv11 feature pyramid by combining large scale and medium scale features while preserving native representations, and by introducing Dynamic Robust Switch units that adaptively inject fuzzy features to strengthen global perception under jitter. Fuzzy features are synthesized by rotating and nonlinearly interpolating multiscale features, then merged through a transparency convolution that learns a content adaptive trade off between original and fuzzy cues. We further develop a CUDA parallel rotation and interpolation kernel that avoids boundary overflow and delivers more than 400 times speedup, making the design practical for edge deployment. We train with paired supervision on a private wheat pest damage dataset of about 3,500 images, augmented threefold using two blur regimes, uniform image wide motion blur and bounding box confined rotational blur. On blurred test sets, YOLOv11 with DFRCP achieves about 10.4 percent higher accuracy than the YOLOv11 baseline with only a modest training time overhead, reducing the need for manual filtering after data collection.</li>
<li><strong>摘要：</strong>由相机抖动引起的运动模糊会产生重影伪影，从而大大降低边缘侧物体检测的质量。现有方法要么将模糊抑制为噪声并丢失辨别结构，要么应用完整图像恢复，这会增加延迟并限制在资源受限设备上的部署。我们提出 DFRCP（一种动态模糊鲁棒卷积金字塔）作为 YOLOv11 的插件升级，用于模糊鲁棒检测。 DFRCP 通过结合大规模和中规模特征同时保留本机表示，并引入动态鲁棒开关单元来增强 YOLOv11 特征金字塔，动态鲁棒开关单元自适应地注入模糊特征以增强抖动下的全局感知。模糊特征是通过旋转和非线性插值多尺度特征来合成的，然后通过透明度卷积进行合并，该透明度卷积学习原始线索和模糊线索之间的内容自适应权衡。我们进一步开发了 CUDA 并行旋转和插值内核，可避免边界溢出并提供超过 400 倍的加速，使该设计适用于边缘部署。我们对大约 3,500 张图像的私人小麦害虫损害数据集进行配对监督训练，使用两种模糊机制（均匀图像宽运动模糊和边界框限制旋转模糊）将其增强三倍。在模糊测试集上，采用 DFRCP 的 YOLOv11 的准确度比 YOLOv11 基线高出约 10.4%，而训练时间开销不大，从而减少了数据收集后手动过滤的需要。</li>
</ul>

<h3>Title: Real-Time Adaptive Anomaly Detection in Industrial IoT Environments</h3>
<ul>
<li><strong>Authors: </strong>Mahsa Raeiszadeh, Amin Ebrahimzadeh, Roch H. Glitho, Johan Eker, Raquel A. F. Mini</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03085">https://arxiv.org/abs/2601.03085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03085">https://arxiv.org/pdf/2601.03085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03085]] Real-Time Adaptive Anomaly Detection in Industrial IoT Environments(https://arxiv.org/abs/2601.03085)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>To ensure reliability and service availability, next-generation networks are expected to rely on automated anomaly detection systems powered by advanced machine learning methods with the capability of handling multi-dimensional data. Such multi-dimensional, heterogeneous data occurs mostly in today's industrial Internet of Things (IIoT), where real-time detection of anomalies is critical to prevent impending failures and resolve them in a timely manner. However, existing anomaly detection methods often fall short of effectively coping with the complexity and dynamism of multi-dimensional data streams in IIoT. In this paper, we propose an adaptive method for detecting anomalies in IIoT streaming data utilizing a multi-source prediction model and concept drift adaptation. The proposed anomaly detection algorithm merges a prediction model into a novel drift adaptation method resulting in accurate and efficient anomaly detection that exhibits improved scalability. Our trace-driven evaluations indicate that the proposed method outperforms the state-of-the-art anomaly detection methods by achieving up to an 89.71% accuracy (in terms of Area under the Curve (AUC)) while meeting the given efficiency and scalability requirements.</li>
<li><strong>摘要：</strong>为了确保可靠性和服务可用性，下一代网络预计将依赖于自动异常检测系统，该系统由先进的机器学习方法提供支持，具有处理多维数据的能力。这种多维、异构的数据主要出现在当今的工业物联网（IIoT）中，实时检测异常对于预防即将发生的故障并及时解决它们至关重要。然而，现有的异常检测方法往往无法有效应对工业物联网中多维数据流的复杂性和动态性。在本文中，我们提出了一种利用多源预测模型和概念漂移自适应来检测 IIoT 流数据异常的自适应方法。所提出的异常检测算法将预测模型合并到新颖的漂移适应方法中，从而实现准确有效的异常检测，并具有改进的可扩展性。我们的跟踪驱动评估表明，所提出的方法优于最先进的异常检测方法，准确率高达 89.71%（就曲线下面积 (AUC) 而言），同时满足给定的效率和可扩展性要求。</li>
</ul>

<h3>Title: Time-Aware Synthetic Control</h3>
<ul>
<li><strong>Authors: </strong>Saeyoung Rho, Cyrus Illick, Samhitha Narasipura, Alberto Abadie, Daniel Hsu, Vishal Misra</a></li>
<li><strong>Subjects: </strong>cs.LG, econ.EM, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03099">https://arxiv.org/abs/2601.03099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03099">https://arxiv.org/pdf/2601.03099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03099]] Time-Aware Synthetic Control(https://arxiv.org/abs/2601.03099)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The synthetic control (SC) framework is widely used for observational causal inference with time-series panel data. SC has been successful in diverse applications, but existing methods typically treat the ordering of pre-intervention time indices interchangeable. This invariance means they may not fully take advantage of temporal structure when strong trends are present. We propose Time-Aware Synthetic Control (TASC), which employs a state-space model with a constant trend while preserving a low-rank structure of the signal. TASC uses the Kalman filter and Rauch-Tung-Striebel smoother: it first fits a generative time-series model with expectation-maximization and then performs counterfactual inference. We evaluate TASC on both simulated and real-world datasets, including policy evaluation and sports prediction. Our results suggest that TASC offers advantages in settings with strong temporal trends and high levels of observation noise.</li>
<li><strong>摘要：</strong>综合控制（SC）框架广泛用于时间序列面板数据的观察因果推断。 SC 在各种应用中都取得了成功，但现有方法通常将干预前时间指数的顺序视为可互换的。这种不变性意味着当出现强烈趋势时，他们可能无法充分利用时间结构。我们提出时间感知综合控制（TASC），它采用具有恒定趋势的状态空间模型，同时保留信号的低秩结构。 TASC 使用卡尔曼滤波器和 Rauch-Tung-Striebel 平滑器：它首先拟合具有期望最大化的生成时间序列模型，然后执行反事实推理。我们在模拟和现实数据集上评估 TASC，包括政策评估和体育预测。我们的结果表明，TASC 在具有强烈时间趋势和高观测噪声水平的环境中具有优势。</li>
</ul>

<h3>Title: Unified Thinker: A General Reasoning Modular Core for Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Sashuai Zhou, Qiang Zhou, Jijin Hu, Hanqing Yang, Yue Cao, Junpeng Ma, Yinchao Ma, Jun Song, Tiezheng Ge, Cheng Yu, Bo Zheng, Zhou Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03127">https://arxiv.org/abs/2601.03127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03127">https://arxiv.org/pdf/2601.03127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03127]] Unified Thinker: A General Reasoning Modular Core for Image Generation(https://arxiv.org/abs/2601.03127)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Despite impressive progress in high-fidelity image synthesis, generative models still struggle with logic-intensive instruction following, exposing a persistent reasoning--execution gap. Meanwhile, closed-source systems (e.g., Nano Banana) have demonstrated strong reasoning-driven image generation, highlighting a substantial gap to current open-source models. We argue that closing this gap requires not merely better visual generators, but executable reasoning: decomposing high-level intents into grounded, verifiable plans that directly steer the generative process. To this end, we propose Unified Thinker, a task-agnostic reasoning architecture for general image generation, designed as a unified planning core that can plug into diverse generators and workflows. Unified Thinker decouples a dedicated Thinker from the image Generator, enabling modular upgrades of reasoning without retraining the entire generative model. We further introduce a two-stage training paradigm: we first build a structured planning interface for the Thinker, then apply reinforcement learning to ground its policy in pixel-level feedback, encouraging plans that optimize visual correctness over textual plausibility. Extensive experiments on text-to-image generation and image editing show that Unified Thinker substantially improves image reasoning and generation quality.</li>
<li><strong>摘要：</strong>尽管在高保真图像合成方面取得了令人印象深刻的进展，但生成模型仍然难以遵循逻辑密集型指令，暴露出持续存在的推理-执行差距。与此同时，闭源系统（例如 Nano Banana）已经表现出强大的推理驱动图像生成能力，凸显了与当前开源模型的巨大差距。我们认为，缩小这一差距不仅需要更好的视觉生成器，还需要可执行的推理：将高级意图分解为扎根的、可验证的计划，直接引导生成过程。为此，我们提出了 Unified Thinker，一种用于一般图像生成的任务无关推理架构，被设计为一个统一的规划核心，可以插入不同的生成器和工作流程。 Unified Thinker 将专用 Thinker 与图像生成器解耦，从而实现推理的模块化升级，而无需重新训练整个生成模型。我们进一步引入了一个两阶段的训练范例：我们首先为 Thinker 构建一个结构化的规划界面，然后应用强化学习将其策略建立在像素级反馈的基础上，鼓励优化视觉正确性而不是文本合理性的计划。对文本到图像生成和图像编辑的大量实验表明，Unified Thinker 极大地提高了图像推理和生成质量。</li>
</ul>

<h3>Title: PersonaLedger: Generating Realistic Financial Transactions with Persona Conditioned LLMs and Rule Grounded Feedback</h3>
<ul>
<li><strong>Authors: </strong>Dehao Yuan, Tyler Farnan, Stefan Tesliuc, Doron L Bergman, Yulun Wu, Xiaoyu Liu, Minghui Liu, James Montgomery, Nam H Nguyen, C. Bayan Bruss, Furong Huang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03149">https://arxiv.org/abs/2601.03149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03149">https://arxiv.org/pdf/2601.03149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03149]] PersonaLedger: Generating Realistic Financial Transactions with Persona Conditioned LLMs and Rule Grounded Feedback(https://arxiv.org/abs/2601.03149)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Strict privacy regulations limit access to real transaction data, slowing open research in financial AI. Synthetic data can bridge this gap, but existing generators do not jointly achieve behavioral diversity and logical groundedness. Rule-driven simulators rely on hand-crafted workflows and shallow stochasticity, which miss the richness of human behavior. Learning-based generators such as GANs capture correlations yet often violate hard financial constraints and still require training on private data. We introduce PersonaLedger, a generation engine that uses a large language model conditioned on rich user personas to produce diverse transaction streams, coupled with an expert configurable programmatic engine that maintains correctness. The LLM and engine interact in a closed loop: after each event, the engine updates the user state, enforces financial rules, and returns a context aware "nextprompt" that guides the LLM toward feasible next actions. With this engine, we create a public dataset of 30 million transactions from 23,000 users and a benchmark suite with two tasks, illiquidity classification and identity theft segmentation. PersonaLedger offers a realistic, privacy preserving resource that supports rigorous evaluation of forecasting and anomaly detection models. PersonaLedger offers the community a rich, realistic, and privacy preserving resource -- complete with code, rules, and generation logs -- to accelerate innovation in financial AI and enable rigorous, reproducible evaluation.</li>
<li><strong>摘要：</strong>严格的隐私法规限制了对真实交易数据的访问，减缓了金融人工智能的开放研究。合成数据可以弥补这一差距，但现有的生成器并不能共同实现行为多样性和逻辑基础。规则驱动的模拟器依赖于手工制作的工作流程和浅层随机性，而忽略了人类行为的丰富性。基于学习的生成器（例如 GAN）可以捕获相关性，但通常会违反严格的财务限制，并且仍然需要对私人数据进行培训。我们引入了 PersonaLedger，这是一种生成引擎，它使用以丰富的用户角色为条件的大型语言模型来生成不同的交易流，并结合专家可配置的编程引擎来保持正确性。 LLM 和引擎在闭环中交互：每次事件发生后，引擎都会更新用户状态，执行财务规则，并返回上下文感知的“下一个提示”，指导 LLM 采取可行的下一步行动。借助该引擎，我们创建了一个包含 23,000 个用户的 3000 万笔交易的公共数据集，以及一个包含两项任务的基准套件：非流动性分类和身份盗窃分段。 PersonaLedger 提供了一种现实的、隐私保护的资源，支持对预测和异常检测模型的严格评估。 PersonaLedger 为社区提供了丰富、现实且保护隐私的资源（包括代码、规则和生成日志），以加速金融人工智能的创新并实现严格、可重复的评估。</li>
</ul>

<h3>Title: Prompt-Counterfactual Explanations for Generative AI System Behavior</h3>
<ul>
<li><strong>Authors: </strong>Sofie Goethals, Foster Provost, João Sedoc</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03156">https://arxiv.org/abs/2601.03156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03156">https://arxiv.org/pdf/2601.03156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03156]] Prompt-Counterfactual Explanations for Generative AI System Behavior(https://arxiv.org/abs/2601.03156)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>As generative AI systems become integrated into real-world applications, organizations increasingly need to be able to understand and interpret their behavior. In particular, decision-makers need to understand what causes generative AI systems to exhibit specific output characteristics. Within this general topic, this paper examines a key question: what is it about the input -the prompt- that causes an LLM-based generative AI system to produce output that exhibits specific characteristics, such as toxicity, negative sentiment, or political bias. To examine this question, we adapt a common technique from the Explainable AI literature: counterfactual explanations. We explain why traditional counterfactual explanations cannot be applied directly to generative AI systems, due to several differences in how generative AI systems function. We then propose a flexible framework that adapts counterfactual explanations to non-deterministic, generative AI systems in scenarios where downstream classifiers can reveal key characteristics of their outputs. Based on this framework, we introduce an algorithm for generating prompt-counterfactual explanations (PCEs). Finally, we demonstrate the production of counterfactual explanations for generative AI systems with three case studies, examining different output characteristics (viz., political leaning, toxicity, and sentiment). The case studies further show that PCEs can streamline prompt engineering to suppress undesirable output characteristics and can enhance red-teaming efforts to uncover additional prompts that elicit undesirable outputs. Ultimately, this work lays a foundation for prompt-focused interpretability in generative AI: a capability that will become indispensable as these models are entrusted with higher-stakes tasks and subject to emerging regulatory requirements for transparency and accountability.</li>
<li><strong>摘要：</strong>随着生成式人工智能系统融入现实世界的应用程序，组织越来越需要能够理解和解释他们的行为。特别是，决策者需要了解是什么导致生成式人工智能系统表现出特定的输出特征。在这个一般主题中，本文研究了一个关键问题：输入（提示）是什么导致基于法学硕士的生成人工智能系统产生表现出特定特征的输出，例如毒性、负面情绪或政治偏见。为了研究这个问题，我们采用了可解释人工智能文献中的一种常用技术：反事实解释。我们解释了为什么传统的反事实解释不能直接应用于生成式人工智能系统，因为生成式人工智能系统的运作方式存在一些差异。然后，我们提出了一个灵活的框架，在下游分类器可以揭示其输出的关键特征的情况下，将反事实解释适应非确定性的生成人工智能系统。基于这个框架，我们引入了一种生成即时反事实解释（PCE）的算法。最后，我们通过三个案例研究展示了生成式人工智能系统的反事实解释的产生，检查了不同的输出特征（即政治倾向、毒性和情绪）。案例研究进一步表明，PCE 可以简化提示工程以抑制不良输出特征，并可以加强红队工作以发现引起不良输出的其他提示。最终，这项工作为生成式人工智能的即时可解释性奠定了基础：随着这些模型被赋予更高风险的任务，并受到新的透明度和问责制监管要求的影响，这种能力将变得不可或缺。</li>
</ul>

<h3>Title: DiffBench Meets DiffAgent: End-to-End LLM-Driven Diffusion Acceleration Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiajun jiao, Haowei Zhu, Puyuan Yang, Jianghui Wang, Ji Liu, Ziqiong Liu, Dong Li, Yuejian Fang, Junhai Yong, Bin Wang, Emad Barsoum</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03178">https://arxiv.org/abs/2601.03178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03178">https://arxiv.org/pdf/2601.03178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03178]] DiffBench Meets DiffAgent: End-to-End LLM-Driven Diffusion Acceleration Code Generation(https://arxiv.org/abs/2601.03178)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved remarkable success in image and video generation. However, their inherently multiple step inference process imposes substantial computational overhead, hindering real-world deployment. Accelerating diffusion models is therefore essential, yet determining how to combine multiple model acceleration techniques remains a significant challenge. To address this issue, we introduce a framework driven by large language models (LLMs) for automated acceleration code generation and evaluation. First, we present DiffBench, a comprehensive benchmark that implements a three stage automated evaluation pipeline across diverse diffusion architectures, optimization combinations and deployment scenarios. Second, we propose DiffAgent, an agent that generates optimal acceleration strategies and codes for arbitrary diffusion models. DiffAgent employs a closed-loop workflow in which a planning component and a debugging component iteratively refine the output of a code generation component, while a genetic algorithm extracts performance feedback from the execution environment to guide subsequent code refinements. We provide a detailed explanation of the DiffBench construction and the design principles underlying DiffAgent. Extensive experiments show that DiffBench offers a thorough evaluation of generated codes and that DiffAgent significantly outperforms existing LLMs in producing effective diffusion acceleration strategies.</li>
<li><strong>摘要：</strong>扩散模型在图像和视频生成方面取得了显着的成功。然而，它们固有的多步骤推理过程会带来大量的计算开销，阻碍了现实世界的部署。因此，加速扩散模型至关重要，但确定如何结合多种模型加速技术仍然是一个重大挑战。为了解决这个问题，我们引入了一个由大型语言模型（LLM）驱动的框架，用于自动加速代码生成和评估。首先，我们介绍 DiffBench，这是一个综合基准测试，它跨不同的扩散架构、优化组合和部署场景实现了三阶段自动评估管道。其次，我们提出了 DiffAgent，一种为任意扩散模型生成最佳加速策略和代码的代理。 DiffAgent 采用闭环工作流程，其中规划组件和调试组件迭代地细化代码生成组件的输出，而遗传算法从执行环境中提取性能反馈以指导后续代码细化。我们详细解释了 DiffBench 的构造以及 DiffAgent 的设计原理。大量实验表明，DiffBench 可以对生成的代码进行全面评估，并且 DiffAgent 在生成有效的扩散加速策略方面显着优于现有的 LLM。</li>
</ul>

<h3>Title: Decentralized Autoregressive Generation</h3>
<ul>
<li><strong>Authors: </strong>Stepan Maschan, Haoxuan Qu, Jun Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03184">https://arxiv.org/abs/2601.03184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03184">https://arxiv.org/pdf/2601.03184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03184]] Decentralized Autoregressive Generation(https://arxiv.org/abs/2601.03184)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present a theoretical analysis of decentralization of autoregressive generation. We define the Decentralized Discrete Flow Matching objective, by expressing probability generating velocity as a linear combination of expert flows. We also conduct experiments demonstrat- ing the equivalence between decentralized and centralized training settings for multimodal language models across diverse set of benchmarks. Specifically, we compare two distinct paradigms: LLaVA and InternVL 2.5-1B, which uses a fixed CLIP vision encoder and per- forms full-parameter fine-tuning (ViT+MLP+LLM) during the instruction tuning stage.</li>
<li><strong>摘要：</strong>我们提出了自回归发电去中心化的理论分析。我们通过将概率生成速度表示为专家流的线性组合来定义去中心化离散流匹配目标。我们还进行了实验，证明跨不同基准集的多模态语言模型的分散式和集中式训练设置之间的等效性。具体来说，我们比较了两种不同的范例：LLaVA 和 InternVL 2.5-1B，它使用固定的 CLIP 视觉编码器，并在指令调优阶段执行全参数微调（ViT+MLP+LLM）。</li>
</ul>

<h3>Title: AnatomiX, an Anatomy-Aware Grounded Multimodal Large Language Model for Chest X-Ray Interpretation</h3>
<ul>
<li><strong>Authors: </strong>Anees Ur Rehman Hashmi, Numan Saeed, Christoph Lippert</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03191">https://arxiv.org/abs/2601.03191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03191">https://arxiv.org/pdf/2601.03191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03191]] AnatomiX, an Anatomy-Aware Grounded Multimodal Large Language Model for Chest X-Ray Interpretation(https://arxiv.org/abs/2601.03191)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal medical large language models have shown impressive progress in chest X-ray interpretation but continue to face challenges in spatial reasoning and anatomical understanding. Although existing grounding techniques improve overall performance, they often fail to establish a true anatomical correspondence, resulting in incorrect anatomical understanding in the medical domain. To address this gap, we introduce AnatomiX, a multitask multimodal large language model explicitly designed for anatomically grounded chest X-ray interpretation. Inspired by the radiological workflow, AnatomiX adopts a two stage approach: first, it identifies anatomical structures and extracts their features, and then leverages a large language model to perform diverse downstream tasks such as phrase grounding, report generation, visual question answering, and image understanding. Extensive experiments across multiple benchmarks demonstrate that AnatomiX achieves superior anatomical reasoning and delivers over 25% improvement in performance on anatomy grounding, phrase grounding, grounded diagnosis and grounded captioning tasks compared to existing approaches. Code and pretrained model are available at this https URL</li>
<li><strong>摘要：</strong>多模态医学大语言模型在胸部 X 射线解释方面取得了令人瞩目的进展，但在空间推理和解剖理解方面仍然面临挑战。尽管现有的接地技术提高了整体性能，但它们常常无法建立真正的解剖学对应关系，导致医学领域的解剖学理解不正确。为了解决这一差距，我们引入了 AnatomiX，这是一种多任务多模态大语言模型，专门为基于解剖学的胸部 X 射线解释而设计。受放射工作流程的启发，AnatomiX 采用两阶段方法：首先，识别解剖结构并提取其特征，然后利用大型语言模型执行各种下游任务，例如短语基础、报告生成、视觉问答和图像理解。跨多个基准的大量实验表明，与现有方法相比，AnatomiX 实现了卓越的解剖推理，并且在解剖基础、短语基础、基础诊断和基础字幕任务方面的性能提高了 25% 以上。代码和预训练模型可在此 https URL 获取</li>
</ul>

<h3>Title: UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision</h3>
<ul>
<li><strong>Authors: </strong>Ruiyan Han, Zhen Fang, XinYu Sun, Yuchen Ma, Ziheng Wang, Yu Zeng, Zehui Chen, Lin Chen, Wenxuan Huang, Wei-Jie Xu, Yi Cao, Feng Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03193">https://arxiv.org/abs/2601.03193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03193">https://arxiv.org/pdf/2601.03193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03193]] UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision(https://arxiv.org/abs/2601.03193)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generation, generative</a></li>
<li><strong>Abstract: </strong>While Unified Multimodal Models (UMMs) have achieved remarkable success in cross-modal comprehension, a significant gap persists in their ability to leverage such internal knowledge for high-quality generation. We formalize this discrepancy as Conduction Aphasia, a phenomenon where models accurately interpret multimodal inputs but struggle to translate that understanding into faithful and controllable synthesis. To address this, we propose UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision. By partitioning a single UMM into three collaborative roles: Proposer, Solver, and Judge, UniCorn generates high-quality interactions via self-play and employs cognitive pattern reconstruction to distill latent understanding into explicit generative signals. To validate the restoration of multimodal coherence, we introduce UniCycle, a cycle-consistency benchmark based on a Text to Image to Text reconstruction loop. Extensive experiments demonstrate that UniCorn achieves comprehensive and substantial improvements over the base model across six general image generation benchmarks. Notably, it achieves SOTA performance on TIIF(73.8), DPG(86.8), CompBench(88.5), and UniCycle while further delivering substantial gains of +5.0 on WISE and +6.5 on OneIG. These results highlight that our method significantly enhances T2I generation while maintaining robust comprehension, demonstrating the scalability of fully self-supervised refinement for unified multimodal intelligence.</li>
<li><strong>摘要：</strong>虽然统一多模态模型（UMM）在跨模态理解方面取得了显着的成功，但它们利用此类内部知识进行高质量生成的能力仍然存在巨大差距。我们将这种差异形式化为传导失语症，这是一种模型准确解释多模态输入但难以将这种理解转化为忠实且可控的合成的现象。为了解决这个问题，我们提出了 UniCorn，这是一个简单而优雅的自我完善框架，无需外部数据或教师监督。通过将单个 UMM 划分为三个协作角色：提议者、解决者和判断者，UniCorn 通过自我游戏生成高质量的交互，并采用认知模式重建将潜在理解提炼为明确的生成信号。为了验证多模态一致性的恢复，我们引入了 UniCycle，这是一种基于文本到图像到文本重建循环的循环一致性基准。大量实验表明，UniCorn 在六个通用图像生成基准上比基本模型实现了全面且实质性的改进。值得注意的是，它在 TIIF(73.8)、DPG(86.8)、CompBench(88.5) 和 UniCycle 上实现了 SOTA 性能，同时在 WISE 上进一步实现了 +5.0 的大幅提升，在 OneIG 上实现了 +6.5 的大幅提升。这些结果强调，我们的方法显着增强了 T2I 生成，同时保持了强大的理解能力，证明了统一多模态智能的完全自我监督细化的可扩展性。</li>
</ul>

<h3>Title: LTX-2: Efficient Joint Audio-Visual Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Yoav HaCohen, Benny Brazowski, Nisan Chiprut, Yaki Bitterman, Andrew Kvochko, Avishai Berkowitz, Daniel Shalem, Daphna Lifschitz, Dudu Moshe, Eitan Porat, Eitan Richardson, Guy Shiran, Itay Chachy, Jonathan Chetboun, Michael Finkelson, Michael Kupchick, Nir Zabari, Nitzan Guetta, Noa Kotler, Ofir Bibi, Ori Gordon, Poriya Panet, Roi Benita, Shahar Armon, Victor Kulikov, Yaron Inger, Yonatan Shiftan, Zeev Melumian, Zeev Farbman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03233">https://arxiv.org/abs/2601.03233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03233">https://arxiv.org/pdf/2601.03233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03233]] LTX-2: Efficient Joint Audio-Visual Foundation Model(https://arxiv.org/abs/2601.03233)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.</li>
<li><strong>摘要：</strong>最近的文本到视频扩散模型可以生成引人注目的视频序列，但它们仍然保持沉默——缺少音频提供的语义、情感和氛围线索。我们引入了 LTX-2，这是一种开源基础模型，能够以统一的方式生成高质量、时间同步的视听内容。 LTX-2 由具有 14B 参数视频流和 5B 参数音频流的非对称双流变压器组成，通过具有时间位置嵌入的双向音频-视频交叉注意层和用于共享时间步调节的跨模态 AdaLN 进行耦合。该架构可以实现统一视听模型的高效训练和推理，同时为视频生成分配比音频生成更多的容量。我们采用多语言文本编码器来实现更广泛的及时理解，并引入模态感知的无分类器指导（模态-CFG）机制来改进视听对齐和可控性。除了生成语音之外，LTX-2 还可以生成丰富、连贯的音轨，遵循每个场景的人物、环境、风格和情感，并配有自然背景和拟音元素。在我们的评估中，该模型实现了最先进的视听质量并迅速遵守开源系统，同时以专有模型的一小部分计算成本和推理时间提供与专有模型相当的结果。所有模型权重和代码均公开发布。</li>
</ul>

<h3>Title: A Versatile Multimodal Agent for Multimedia Content Generation</h3>
<ul>
<li><strong>Authors: </strong>Daoan Zhang, Wenlin Yao, Xiaoyang Wang, Yebowen Hu, Jiebo Luo, Dong Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03250">https://arxiv.org/abs/2601.03250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03250">https://arxiv.org/pdf/2601.03250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03250]] A Versatile Multimodal Agent for Multimedia Content Generation(https://arxiv.org/abs/2601.03250)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>With the advancement of AIGC (AI-generated content) technologies, an increasing number of generative models are revolutionizing fields such as video editing, music generation, and even film production. However, due to the limitations of current AIGC models, most models can only serve as individual components within specific application scenarios and are not capable of completing tasks end-to-end in real-world applications. In real-world applications, editing experts often work with a wide variety of images and video inputs, producing multimodal outputs -- a video typically includes audio, text, and other elements. This level of integration across multiple modalities is something current models are unable to achieve effectively. However, the rise of agent-based systems has made it possible to use AI tools to tackle complex content generation tasks. To deal with the complex scenarios, in this paper, we propose a MultiMedia-Agent designed to automate complex content creation. Our agent system includes a data generation pipeline, a tool library for content creation, and a set of metrics for evaluating preference alignment. Notably, we introduce the skill acquisition theory to model the training data curation and agent training. We designed a two-stage correlation strategy for plan optimization, including self-correlation and model preference correlation. Additionally, we utilized the generated plans to train the MultiMedia-Agent via a three stage approach including base/success plan finetune and preference optimization. The comparison results demonstrate that the our approaches are effective and the MultiMedia-Agent can generate better multimedia content compared to novel models.</li>
<li><strong>摘要：</strong>随着AIGC（人工智能生成内容）技术的进步，越来越多的生成模型正在给视频编辑、音乐生成甚至电影制作等领域带来革命性的变化。然而，由于当前AIGC模型的局限性，大多数模型只能作为特定应用场景中的单独组件，无法在实际应用中端到端地完成任务。在现实应用中，编辑专家经常处理各种图像和视频输入，产生多模式输出——视频通常包括音频、文本和其他元素。当前模型无法有效实现这种跨多种模式的集成水平。然而，基于代理的系统的兴起使得使用人工智能工具来处理复杂的内容生成任务成为可能。为了处理复杂的场景，在本文中，我们提出了一种多媒体代理，旨在自动创建复杂的内容。我们的代理系统包括数据生成管道、用于内容创建的工具库以及一组用于评估偏好一致性的指标。值得注意的是，我们引入了技能获取理论来对训练数据管理和代理训练进行建模。我们设计了用于计划优化的两阶段相关策略，包括自相关和模型偏好相关。此外，我们利用生成的计划通过三阶段方法训练多媒体代理，包括基本/成功计划微调和偏好优化。比较结果表明，我们的方法是有效的，并且与新模型相比，多媒体代理可以生成更好的多媒体内容。</li>
</ul>

<h3>Title: Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training</h3>
<ul>
<li><strong>Authors: </strong>Hexiao Lu, Xiaokun Sun, Zeyu Cai, Hao Guo, Ying Tai, Jian Yang, Zhenyu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03256">https://arxiv.org/abs/2601.03256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03256">https://arxiv.org/pdf/2601.03256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03256]] Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training(https://arxiv.org/abs/2601.03256)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present Muses, the first training-free method for fantastic 3D creature generation in a feed-forward paradigm. Previous methods, which rely on part-aware optimization, manual assembly, or 2D image generation, often produce unrealistic or incoherent 3D assets due to the challenges of intricate part-level manipulation and limited out-of-domain generation. In contrast, Muses leverages the 3D skeleton, a fundamental representation of biological forms, to explicitly and rationally compose diverse elements. This skeletal foundation formalizes 3D content creation as a structure-aware pipeline of design, composition, and generation. Muses begins by constructing a creatively composed 3D skeleton with coherent layout and scale through graph-constrained reasoning. This skeleton then guides a voxel-based assembly process within a structured latent space, integrating regions from different objects. Finally, image-guided appearance modeling under skeletal conditions is applied to generate a style-consistent and harmonious texture for the assembled shape. Extensive experiments establish Muses' state-of-the-art performance in terms of visual fidelity and alignment with textual descriptions, and potential on flexible 3D object editing. Project page: this https URL.</li>
<li><strong>摘要：</strong>我们推出了 Muses，这是第一个无需训练的方法，可以在前馈范式中生成奇妙的 3D 生物。以前的方法依赖于零件感知优化、手动组装或 2D 图像生成，由于复杂的零件级操作和有限的域外生成的挑战，通常会产生不切实际或不连贯的 3D 资产。相比之下，Muses 利用 3D 骨架（生物形态的基本表示）来明确、合理地组成不同的元素。这个骨架基础将 3D 内容创建形式化为设计、合成和生成的结构感知管道。 Muses 首先通过图形约束推理构建一个具有连贯布局和比例的创意组合 3D 骨架。然后，该骨架在结构化潜在空间内引导基于体素的组装过程，整合来自不同对象的区域。最后，应用骨骼条件下的图像引导外观建模，为组装后的形状生成风格一致且和谐的纹理。大量的实验确立了 Muses 在视觉保真度、与文本描述的一致性以及灵活 3D 对象编辑潜力方面的最先进性能。项目页面：此 https URL。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
