<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-27</h1>
<h3>Title: HA-RAG: Hotness-Aware RAG Acceleration via Mixed Precision and Data Placement</h3>
<ul>
<li><strong>Authors: </strong>Danying Ge, Jianhua Gao, Yixue Yang, Weixing Ji</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20878">https://arxiv.org/abs/2510.20878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20878">https://arxiv.org/pdf/2510.20878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20878]] HA-RAG: Hotness-Aware RAG Acceleration via Mixed Precision and Data Placement(https://arxiv.org/abs/2510.20878)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) improves model output accuracy by leveraging external knowledge bases, serving as an effective solution to address hallucination issues and knowledge-update delays in Large Language Models (LLMs). However, the introduction of external knowledge bases presents RAG with challenges in long-context processing, significantly increasing memory consumption and inference latency. Existing research accelerates inference by precomputing Key and Value (KV) of the knowledge base and loading them on-demand during inference. Based on the access frequency of different KV chunks within the external knowledge base, this paper proposes a hotness-aware RAG (HA-RAG) inference optimization system. First, leveraging the numerical distribution of KV chunks, we introduce a hotness-aware mixed-precision compressing and loading method to reduce disk I/O and memory access overhead. Second, we design a hotness-aware data placement strategy that prioritizes storing frequently accessed KV chunks in high-speed memory to improve data access efficiency. Experimental results demonstrate that, compared with TurboRAG, the proposed HA-RAG achieves an average speedup of 2.10x and maximum speedup of 10.49x in Time-To-First-Token (TTFT) with negligible accuracy loss.</li>
<li><strong>摘要：</strong>检索增强生成（RAG）通过利用外部知识库来提高模型输出的准确性，是解决大型语言模型（LLM）中的幻觉问题和知识更新延迟的有效解决方案。然而，外部知识库的引入给RAG带来了长上下文处理的挑战，显着增加了内存消耗和推理延迟。现有研究通过预先计算知识库的键和值（KV）并在推理过程中按需加载来加速推理。基于外部知识库内不同KV块的访问频率，本文提出了一种热度感知RAG（HA-RAG）推理优化系统。首先，利用 KV 块的数值分布，我们引入了一种热感知混合精度压缩和加载方法，以减少磁盘 I/O 和内存访问开销。其次，我们设计了一种热感知数据放置策略，优先将频繁访问的 KV 块存储在高速内存中，以提高数据访问效率。实验结果表明，与 TurboRAG 相比，所提出的 HA-RAG 在首次令牌时间（TTFT）方面实现了 2.10 倍的平均加速和 10.49 倍的最大加速，并且精度损失可以忽略不计。</li>
</ul>

<h3>Title: Preventing Shortcuts in Adapter Training via Providing the Shortcuts</h3>
<ul>
<li><strong>Authors: </strong>Anujraaj Argo Goyal, Guocheng Gordon Qian, Huseyin Coskun, Aarush Gupta, Himmy Tam, Daniil Ostashev, Ju Hu, Dhritiman Sagar, Sergey Tulyakov, Kfir Aberman, Kuan-Chieh Jackson Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20887">https://arxiv.org/abs/2510.20887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20887">https://arxiv.org/pdf/2510.20887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20887]] Preventing Shortcuts in Adapter Training via Providing the Shortcuts(https://arxiv.org/abs/2510.20887)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Adapter-based training has emerged as a key mechanism for extending the capabilities of powerful foundation image generators, enabling personalized and stylized text-to-image synthesis. These adapters are typically trained to capture a specific target attribute, such as subject identity, using single-image reconstruction objectives. However, because the input image inevitably contains a mixture of visual factors, adapters are prone to entangle the target attribute with incidental ones, such as pose, expression, and lighting. This spurious correlation problem limits generalization and obstructs the model's ability to adhere to the input text prompt. In this work, we uncover a simple yet effective solution: provide the very shortcuts we wish to eliminate during adapter training. In Shortcut-Rerouted Adapter Training, confounding factors are routed through auxiliary modules, such as ControlNet or LoRA, eliminating the incentive for the adapter to internalize them. The auxiliary modules are then removed during inference. When applied to tasks like facial and full-body identity injection, our approach improves generation quality, diversity, and prompt adherence. These results point to a general design principle in the era of large models: when seeking disentangled representations, the most effective path may be to establish shortcuts for what should NOT be learned.</li>
<li><strong>摘要：</strong>基于适配器的训练已成为扩展强大的基础图像生成器功能的关键机制，从而实现个性化和风格化的文本到图像的合成。这些适配器通常经过训练，可以使用单图像重建目标捕获特定的目标属性，例如主体身份。然而，由于输入图像不可避免地包含视觉因素的混合，适配器很容易将目标属性与偶然的属性（例如姿势、表情和光照）纠缠在一起。这种虚假的相关性问题限制了泛化并阻碍了模型遵循输入文本提示的能力。在这项工作中，我们发现了一个简单而有效的解决方案：提供我们希望在适配器训练期间消除的快捷方式。在快捷重新路由适配器训练中，混杂因素通过 ControlNet 或 LoRA 等辅助模块进行路由，从而消除了适配器将其内化的动机。然后在推理过程中删除辅助模块。当应用于面部和全身身份注入等任务时，我们的方法可以提高生成质量、多样性和及时依从性。这些结果指出了大型模型时代的一般设计原则：在寻求解开的表示时，最有效的路径可能是为不应该学习的内容建立捷径。</li>
</ul>

<h3>Title: Video-As-Prompt: Unified Semantic Control for Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Bian, Xin Chen, Zenan Li, Tiancheng Zhi, Shen Sang, Linjie Luo, Qiang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20888">https://arxiv.org/abs/2510.20888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20888">https://arxiv.org/pdf/2510.20888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20888]] Video-As-Prompt: Unified Semantic Control for Video Generation(https://arxiv.org/abs/2510.20888)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Unified, generalizable semantic control in video generation remains a critical open challenge. Existing methods either introduce artifacts by enforcing inappropriate pixel-wise priors from structure-based controls, or rely on non-generalizable, condition-specific finetuning or task-specific architectures. We introduce Video-As-Prompt (VAP), a new paradigm that reframes this problem as in-context generation. VAP leverages a reference video as a direct semantic prompt, guiding a frozen Video Diffusion Transformer (DiT) via a plug-and-play Mixture-of-Transformers (MoT) expert. This architecture prevents catastrophic forgetting and is guided by a temporally biased position embedding that eliminates spurious mapping priors for robust context retrieval. To power this approach and catalyze future research, we built VAP-Data, the largest dataset for semantic-controlled video generation with over 100K paired videos across 100 semantic conditions. As a single unified model, VAP sets a new state-of-the-art for open-source methods, achieving a 38.7% user preference rate that rivals leading condition-specific commercial models. VAP's strong zero-shot generalization and support for various downstream applications mark a significant advance toward general-purpose, controllable video generation.</li>
<li><strong>摘要：</strong>视频生成中统一的、可推广的语义控制仍然是一个关键的开放挑战。现有的方法要么通过基于结构的控制强制执行不适当的像素先验来引入伪像，要么依赖于不可泛化的、特定于条件的微调或特定于任务的架构。我们引入了视频提示（VAP），这是一种新的范式，将这个问题重新定义为上下文生成。 VAP 利用参考视频作为直接语义提示，通过即插即用的混合变压器 (MoT) 专家来指导冻结的视频扩散变压器 (DiT)。该架构可防止灾难性遗忘，并由时间偏置位置嵌入引导，消除虚假映射先验，实现稳健的上下文检索。为了支持这种方法并促进未来的研究，我们构建了 VAP-Data，这是用于语义控制视频生成的最大数据集，包含跨越 100 个语义条件的超过 10 万个配对视频。作为单一统一模型，VAP 为开源方法树立了新的最先进水平，实现了 38.7% 的用户偏好率，可与领先的特定条件商业模型相媲美。 VAP 强大的零样本泛化能力和对各种下游应用的支持标志着通用、可控视频生成的重大进步。</li>
</ul>

<h3>Title: Generative Point Tracking with Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Mattie Tesfaldet, Adam W. Harley, Konstantinos G. Derpanis, Derek Nowrouzezahrai, Christopher Pal</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20951">https://arxiv.org/abs/2510.20951</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20951">https://arxiv.org/pdf/2510.20951</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20951]] Generative Point Tracking with Flow Matching(https://arxiv.org/abs/2510.20951)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Tracking a point through a video can be a challenging task due to uncertainty arising from visual obfuscations, such as appearance changes and occlusions. Although current state-of-the-art discriminative models excel in regressing long-term point trajectory estimates -- even through occlusions -- they are limited to regressing to a mean (or mode) in the presence of uncertainty, and fail to capture multi-modality. To overcome this limitation, we introduce Generative Point Tracker (GenPT), a generative framework for modelling multi-modal trajectories. GenPT is trained with a novel flow matching formulation that combines the iterative refinement of discriminative trackers, a window-dependent prior for cross-window consistency, and a variance schedule tuned specifically for point coordinates. We show how our model's generative capabilities can be leveraged to improve point trajectory estimates by utilizing a best-first search strategy on generated samples during inference, guided by the model's own confidence of its predictions. Empirically, we evaluate GenPT against the current state of the art on the standard PointOdyssey, Dynamic Replica, and TAP-Vid benchmarks. Further, we introduce a TAP-Vid variant with additional occlusions to assess occluded point tracking performance and highlight our model's ability to capture multi-modality. GenPT is capable of capturing the multi-modality in point trajectories, which translates to state-of-the-art tracking accuracy on occluded points, while maintaining competitive tracking accuracy on visible points compared to extant discriminative point trackers.</li>
<li><strong>摘要：</strong>由于视觉混淆（例如外观变化和遮挡）带来的不确定性，通过视频跟踪点可能是一项具有挑战性的任务。尽管当前最先进的判别模型擅长回归长期点轨迹估计（即使通过遮挡），但它们仅限于在存在不确定性的情况下回归到平均值（或模式），并且无法捕获多模态。为了克服这一限制，我们引入了生成点跟踪器（GenPT），这是一种用于建模多模态轨迹的生成框架。 GenPT 采用新颖的流匹配公式进行训练，该公式结合了判别跟踪器的迭代细化、跨窗口一致性的窗口相关先验以及专门针对点坐标调整的方差计划。我们展示了如何利用模型的生成能力来改进点轨迹估计，方法是在推理过程中对生成的样本使用最佳优先搜索策略，并以模型自身对其预测的置信度为指导。根据经验，我们根据标准 PointOdyssey、Dynamic Replica 和 TAP-Vid 基准测试的当前技术水平来评估 GenPT。此外，我们引入了具有附加遮挡的 TAP-Vid 变体，以评估遮挡点跟踪性能并突出我们的模型捕获多模态的能力。 GenPT 能够捕获点轨迹中的多模态，这意味着对遮挡点的最先进的跟踪精度，同时与现有的判别点跟踪器相比，在可见点上保持有竞争力的跟踪精度。</li>
</ul>

<h3>Title: L^2M^3OF: A Large Language Multimodal Model for Metal-Organic Frameworks</h3>
<ul>
<li><strong>Authors: </strong>Jiyu Cui, Fang Wu, Haokai Zhao, Minggao Feng, Xenophon Evangelopoulos, Andrew I. Cooper, Yejin Choi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20976">https://arxiv.org/abs/2510.20976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20976">https://arxiv.org/pdf/2510.20976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20976]] L^2M^3OF: A Large Language Multimodal Model for Metal-Organic Frameworks(https://arxiv.org/abs/2510.20976)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models have demonstrated remarkable reasoning capabilities across diverse natural language tasks. However, comparable breakthroughs in scientific discovery are more limited, because understanding complex physical phenomena demands multifaceted representations far beyond language alone. A compelling example is the design of functional materials such as MOFs-critical for a range of impactful applications like carbon capture and hydrogen storage. Navigating their vast and intricate design space in language-based representations interpretable by LLMs is challenging due to the numerous possible three-dimensional atomic arrangements and strict reticular rules of coordination geometry and topology. Despite promising early results in LLM-assisted discovery for simpler materials systems, MOF design remains heavily reliant on tacit human expertise rarely codified in textual information alone. To overcome this barrier, we introduce L2M3OF, the first multimodal LLM for MOFs. L2M3OF integrates crystal representation learning with language understanding to process structural, textual, and knowledge modalities jointly. L2M3OF employs a pre-trained crystal encoder with a lightweight projection layer to compress structural information into a token space, enabling efficient alignment with language instructions. To facilitate training and evaluation, we curate a structure-property-knowledge database of crystalline materials and benchmark L2M3OF against state-of-the-art closed-source LLMs such as GPT-5, Gemini-2.5-Pro and DeepSeek-R1. Experiments show that L2M3OF outperforms leading text-based closed-source LLMs in property prediction and knowledge generation tasks, despite using far fewer parameters. These results highlight the importance of multimodal approaches for porous material understanding and establish L2M3OF as a foundation for next-generation AI systems in materials discovery.</li>
<li><strong>摘要：</strong>大型语言模型在不同的自然语言任务中表现出了卓越的推理能力。然而，科学发现中的类似突破更加有限，因为理解复杂的物理现象需要多方面的表征，远远超出了语言本身的范围。一个引人注目的例子是功能材料（例如 MOF）的设计，这对于碳捕获和氢存储等一系列有影响力的应用至关重要。由于存在大量可能的三维原子排列以及协调几何和拓扑的严格网状规则，在法学硕士可解释的基于语言的表示中导航其巨大而复杂的设计空间具有挑战性。尽管法学硕士辅助发现更简单的材料系统取得了有希望的早期成果，但 MOF 设计仍然严重依赖于隐性的人类专业知识，很少单独编码为文本信息。为了克服这一障碍，我们引入了 L2M3OF，这是第一个针对 MOF 的多模式法学硕士。 L2M3OF 将晶体表示学习与语言理解相结合，共同处理结构、文本和知识模态。 L2M3OF 采用带有轻量级投影层的预训练晶体编码器，将结构信息压缩到令牌空间中，从而实现与语言指令的高效对齐。为了促进培训和评估，我们策划了一个晶体材料的结构-性能-知识数据库，并将 L2M3OF 与 GPT-5、Gemini-2.5-Pro 和 DeepSeek-R1 等最先进的闭源法学硕士进行基准测试。实验表明，尽管使用的参数少得多，但 L2M3OF 在属性预测和知识生成任务方面优于领先的基于文本的闭源 LLM。这些结果凸显了多模态方法对于理解多孔材料的重要性，并将 L2M3OF 确立为下一代材料发现人工智能系统的基础。</li>
</ul>

<h3>Title: Learning Grouped Lattice Vector Quantizers for Low-Bit LLM Compression</h3>
<ul>
<li><strong>Authors: </strong>Xi Zhang, Xiaolin Wu, Jiamang Wang, Weisi Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20984">https://arxiv.org/abs/2510.20984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20984">https://arxiv.org/pdf/2510.20984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20984]] Learning Grouped Lattice Vector Quantizers for Low-Bit LLM Compression(https://arxiv.org/abs/2510.20984)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities but typically require extensive computational resources and memory for inference. Post-training quantization (PTQ) can effectively reduce these demands by storing weights in lower bit-width formats. However, standard uniform quantization often leads to notable performance degradation, particularly in low-bit scenarios. In this work, we introduce a Grouped Lattice Vector Quantization (GLVQ) framework that assigns each group of weights a customized lattice codebook, defined by a learnable generation matrix. To address the non-differentiability of the quantization process, we adopt Babai rounding to approximate nearest-lattice-point search during training, which enables stable optimization of the generation matrices. Once trained, decoding reduces to a simple matrix-vector multiplication, yielding an efficient and practical quantization pipeline. Experiments on multiple benchmarks show that our approach achieves a better trade-off between model size and accuracy compared to existing post-training quantization baselines, highlighting its effectiveness in deploying large models under stringent resource constraints. Our source code is available on GitHub repository: this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已展现出卓越的功能，但通常需要大量计算资源和内存来进行推理。训练后量化（PTQ）可以通过以较低位宽格式存储权重来有效减少这些需求。然而，标准统一量化通常会导致性能显着下降，特别是在低位场景中。在这项工作中，我们引入了分组格向量量化（GLVQ）框架，该框架为每组权重分配一个定制的格码本，由可学习的生成矩阵定义。为了解决量化过程的不可微性，我们在训练过程中采用 Babai 舍入来近似最近格点搜索，从而实现生成矩阵的稳定优化。经过训练后，解码可简化为简单的矩阵向量乘法，从而产生高效且实用的量化管道。多个基准的实验表明，与现有的训练后量化基线相比，我们的方法在模型大小和准确性之间实现了更好的权衡，突显了其在严格的资源限制下部署大型模型的有效性。我们的源代码可在 GitHub 存储库上找到：此 https URL。</li>
</ul>

<h3>Title: VESSA: Video-based objEct-centric Self-Supervised Adaptation for Visual Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Jesimon Barreto, Carlos Caetano, André Araujo, William Robson Schwartz</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20994">https://arxiv.org/abs/2510.20994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20994">https://arxiv.org/pdf/2510.20994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20994]] VESSA: Video-based objEct-centric Self-Supervised Adaptation for Visual Foundation Models(https://arxiv.org/abs/2510.20994)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Foundation models have advanced computer vision by enabling strong performance across diverse tasks through large-scale pretraining and supervised fine-tuning. However, they may underperform in domains with distribution shifts and scarce labels, where supervised fine-tuning may be infeasible. While continued self-supervised learning for model adaptation is common for generative language models, this strategy has not proven effective for vision-centric encoder models. To address this challenge, we introduce a novel formulation of self-supervised fine-tuning for vision foundation models, where the model is adapted to a new domain without requiring annotations, leveraging only short multi-view object-centric videos. Our method is referred to as VESSA: Video-based objEct-centric Self-Supervised Adaptation for visual foundation models. VESSA's training technique is based on a self-distillation paradigm, where it is critical to carefully tune prediction heads and deploy parameter-efficient adaptation techniques - otherwise, the model may quickly forget its pretrained knowledge and reach a degraded state. VESSA benefits significantly from multi-view object observations sourced from different frames in an object-centric video, efficiently learning robustness to varied capture conditions, without the need of annotations. Through comprehensive experiments with 3 vision foundation models on 2 datasets, VESSA demonstrates consistent improvements in downstream classification tasks, compared to the base models and previous adaptation methods. Code is publicly available at this https URL.</li>
<li><strong>摘要：</strong>基础模型通过大规模预训练和监督微调，在不同任务中实现强大的性能，从而拥有先进的计算机视觉。然而，它们在分布变化和标签稀缺的领域可能表现不佳，在这些领域中监督微调可能不可行。虽然持续的模型适应自监督学习对于生成语言模型来说很常见，但这种策略尚未证明对于以视觉为中心的编码器模型有效。为了应对这一挑战，我们引入了一种新颖的视觉基础模型自监督微调公式，该模型无需注释即可适应新领域，仅利用以对象为中心的短多视图视频。我们的方法被称为 VESSA：基于视频的以对象为中心的视觉基础模型的自我监督适应。 VESSA 的训练技术基于自蒸馏范式，其中仔细调整预测头并部署参数高效的适应技术至关重要 - 否则，模型可能会很快忘记其预先训练的知识并达到降级状态。 VESSA 受益于来自以对象为中心的视频中不同帧的多视图对象观察，可以有效地学习对不同捕获条件的鲁棒性，而无需注释。通过在 2 个数据集上使用 3 个视觉基础模型进行综合实验，与基础模型和之前的适应方法相比，VESSA 展示了下游分类任务的持续改进。 Code is publicly available at this https URL.</li>
</ul>

<h3>Title: Distilled Decoding 2: One-step Sampling of Image Auto-regressive Models with Conditional Score Distillation</h3>
<ul>
<li><strong>Authors: </strong>Enshu Liu, Qian Chen, Xuefei Ning, Shengen Yan, Guohao Dai, Zinan Lin, Yu Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21003">https://arxiv.org/abs/2510.21003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21003">https://arxiv.org/pdf/2510.21003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21003]] Distilled Decoding 2: One-step Sampling of Image Auto-regressive Models with Conditional Score Distillation(https://arxiv.org/abs/2510.21003)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Image Auto-regressive (AR) models have emerged as a powerful paradigm of visual generative models. Despite their promising performance, they suffer from slow generation speed due to the large number of sampling steps required. Although Distilled Decoding 1 (DD1) was recently proposed to enable few-step sampling for image AR models, it still incurs significant performance degradation in the one-step setting, and relies on a pre-defined mapping that limits its flexibility. In this work, we propose a new method, Distilled Decoding 2 (DD2), to further advances the feasibility of one-step sampling for image AR models. Unlike DD1, DD2 does not without rely on a pre-defined mapping. We view the original AR model as a teacher model which provides the ground truth conditional score in the latent embedding space at each token position. Based on this, we propose a novel \emph{conditional score distillation loss} to train a one-step generator. Specifically, we train a separate network to predict the conditional score of the generated distribution and apply score distillation at every token position conditioned on previous tokens. Experimental results show that DD2 enables one-step sampling for image AR models with an minimal FID increase from 3.40 to 5.43 on ImageNet-256. Compared to the strongest baseline DD1, DD2 reduces the gap between the one-step sampling and original AR model by 67%, with up to 12.3$\times$ training speed-up simultaneously. DD2 takes a significant step toward the goal of one-step AR generation, opening up new possibilities for fast and high-quality AR modeling. Code is available at this https URL.</li>
<li><strong>摘要：</strong>Image Auto-regressive (AR) models have emerged as a powerful paradigm of visual generative models. Despite their promising performance, they suffer from slow generation speed due to the large number of sampling steps required.尽管蒸馏解码 1 (DD1) 最近被提出来支持图像 AR 模型的少步采样，但它在一步设置中仍然会导致性能显着下降，并且依赖于预定义的映射，这限制了其灵活性。 In this work, we propose a new method, Distilled Decoding 2 (DD2), to further advances the feasibility of one-step sampling for image AR models.与 DD1 不同，DD2 并非不依赖于预定义的映射。我们将原始 AR 模型视为教师模型，它在每个标记位置的潜在嵌入空间中提供地面真实条件分数。 Based on this, we propose a novel \emph{conditional score distillation loss} to train a one-step generator.具体来说，我们训练一个单独的网络来预测生成的分布的条件分数，并在以先前标记为条件的每个标记位置应用分数蒸馏。 Experimental results show that DD2 enables one-step sampling for image AR models with an minimal FID increase from 3.40 to 5.43 on ImageNet-256.与最强基线 DD1 相比，DD2 将一步采样与原始 AR 模型之间的差距缩小了 67%，同时训练速度提升高达 12.3$\times$。 DD2 takes a significant step toward the goal of one-step AR generation, opening up new possibilities for fast and high-quality AR modeling.代码可从此 https URL 获取。</li>
</ul>

<h3>Title: From Information to Generative Exponent: Learning Rate Induces Phase Transitions in SGD</h3>
<ul>
<li><strong>Authors: </strong>Konstantinos Christopher Tsiolis, Alireza Mousavi-Hosseini, Murat A. Erdogdu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21020">https://arxiv.org/abs/2510.21020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21020">https://arxiv.org/pdf/2510.21020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21020]] From Information to Generative Exponent: Learning Rate Induces Phase Transitions in SGD(https://arxiv.org/abs/2510.21020)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>To understand feature learning dynamics in neural networks, recent theoretical works have focused on gradient-based learning of Gaussian single-index models, where the label is a nonlinear function of a latent one-dimensional projection of the input. While the sample complexity of online SGD is determined by the information exponent of the link function, recent works improved this by performing multiple gradient steps on the same sample with different learning rates -- yielding a non-correlational update rule -- and instead are limited by the (potentially much smaller) generative exponent. However, this picture is only valid when these learning rates are sufficiently large. In this paper, we characterize the relationship between learning rate(s) and sample complexity for a broad class of gradient-based algorithms that encapsulates both correlational and non-correlational updates. We demonstrate that, in certain cases, there is a phase transition from an "information exponent regime" with small learning rate to a "generative exponent regime" with large learning rate. Our framework covers prior analyses of one-pass SGD and SGD with batch reuse, while also introducing a new layer-wise training algorithm that leverages a two-timescales approach (via different learning rates for each layer) to go beyond correlational queries without reusing samples or modifying the loss from squared error. Our theoretical study demonstrates that the choice of learning rate is as important as the design of the algorithm in achieving statistical and computational efficiency.</li>
<li><strong>摘要：</strong>为了理解神经网络中的特征学习动态，最近的理论工作集中在高斯单指数模型的基于梯度的学习，其中标签是输入的潜在一维投影的非线性函数。虽然在线 SGD 的样本复杂性是由链接函数的信息指数决定的，但最近的工作通过对具有不同学习率的同一样本执行多个梯度步骤（产生非相关更新规则）来改进这一点，而是受到（可能更小）生成指数的限制。然而，只有当这些学习率足够大时，这张图才有效。在本文中，我们描述了一大类基于梯度的算法的学习率和样本复杂性之间的关系，这些算法封装了相关和非相关更新。我们证明，在某些情况下，存在从小学习率的“信息指数体系”到具有大学习率的“生成指数体系”的相变。我们的框架涵盖了一次性 SGD 和批量重用 SGD 的先前分析，同时还引入了一种新的分层训练算法，该算法利用双时间尺度方法（通过每层的不同学习率）超越相关查询，而无需重用样本或修改平方误差的损失。我们的理论研究表明，在实现统计和计算效率方面，学习率的选择与算法的设计同样重要。</li>
</ul>

<h3>Title: Amortized Active Generation of Pareto Sets</h3>
<ul>
<li><strong>Authors: </strong>Daniel M. Steinberg, Asiri Wijesinghe, Rafael Oliveira, Piotr Koniusz, Cheng Soon Ong, Edwin V. Bonilla</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21052">https://arxiv.org/abs/2510.21052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21052">https://arxiv.org/pdf/2510.21052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21052]] Amortized Active Generation of Pareto Sets(https://arxiv.org/abs/2510.21052)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We introduce active generation of Pareto sets (A-GPS), a new framework for online discrete black-box multi-objective optimization (MOO). A-GPS learns a generative model of the Pareto set that supports a-posteriori conditioning on user preferences. The method employs a class probability estimator (CPE) to predict non-dominance relations and to condition the generative model toward high-performing regions of the search space. We also show that this non-dominance CPE implicitly estimates the probability of hypervolume improvement (PHVI). To incorporate subjective trade-offs, A-GPS introduces preference direction vectors that encode user-specified preferences in objective space. At each iteration, the model is updated using both Pareto membership and alignment with these preference directions, producing an amortized generative model capable of sampling across the Pareto front without retraining. The result is a simple yet powerful approach that achieves high-quality Pareto set approximations, avoids explicit hypervolume computation, and flexibly captures user preferences. Empirical results on synthetic benchmarks and protein design tasks demonstrate strong sample efficiency and effective preference incorporation.</li>
<li><strong>摘要：</strong>我们引入主动生成帕累托集（A-GPS），这是一种用于在线离散黑盒多目标优化（MOO）的新框架。 A-GPS 学习帕累托集的生成模型，该模型支持用户偏好的后验调节。该方法采用类概率估计器 (CPE) 来预测非支配关系，并将生成模型调整为搜索空间的高性能区域。我们还表明，这种非显性 CPE 隐式估计了超容量改善 (PHVI) 的概率。为了纳入主观权衡，A-GPS 引入了偏好方向向量，对客观空间中用户指定的偏好进行编码。在每次迭代中，模型都会使用 Pareto 隶属度和与这些偏好方向的对齐来更新，从而生成能够在 Pareto 前沿进行采样而无需重新训练的摊销生成模型。结果是一种简单而强大的方法，可以实现高质量的帕累托集逼近，避免显式超体积计算，并灵活地捕获用户偏好。合成基准和蛋白质设计任务的实证结果证明了强大的样本效率和有效的偏好合并。</li>
</ul>

<h3>Title: ZING-3D: Zero-shot Incremental 3D Scene Graphs via Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Pranav Saxena, Jimmy Chiun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21069">https://arxiv.org/abs/2510.21069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21069">https://arxiv.org/pdf/2510.21069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21069]] ZING-3D: Zero-shot Incremental 3D Scene Graphs via Vision-Language Models(https://arxiv.org/abs/2510.21069)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Understanding and reasoning about complex 3D environments requires structured scene representations that capture not only objects but also their semantic and spatial relationships. While recent works on 3D scene graph generation have leveraged pretrained VLMs without task-specific fine-tuning, they are largely confined to single-view settings, fail to support incremental updates as new observations arrive and lack explicit geometric grounding in 3D space, all of which are essential for embodied scenarios. In this paper, we propose, ZING-3D, a framework that leverages the vast knowledge of pretrained foundation models to enable open-vocabulary recognition and generate a rich semantic representation of the scene in a zero-shot manner while also enabling incremental updates and geometric grounding in 3D space, making it suitable for downstream robotics applications. Our approach leverages VLM reasoning to generate a rich 2D scene graph, which is grounded in 3D using depth information. Nodes represent open-vocabulary objects with features, 3D locations, and semantic context, while edges capture spatial and semantic relations with inter-object distances. Our experiments on scenes from the Replica and HM3D dataset show that ZING-3D is effective at capturing spatial and relational knowledge without the need of task-specific training.</li>
<li><strong>摘要：</strong>理解和推理复杂的 3D 环境需要结构化的场景表示，不仅要捕获对象，还要捕获它们的语义和空间关系。虽然最近关于 3D 场景图生成的工作利用了预训练的 VLM，没有针对特定任务进行微调，但它们在很大程度上仅限于单视图设置，无法支持新观察到达时的增量更新，并且缺乏 3D 空间中的明确几何基础，所有这些对于具体场景都是至关重要的。在本文中，我们提出了 ZING-3D，这是一个利用预训练基础模型的丰富知识来实现​​开放词汇识别并以零样本方式生成场景的丰富语义表示的框架，同时还支持 3D 空间中的增量更新和几何基础，使其适合下游机器人应用。我们的方法利用 VLM 推理来生成丰富的 2D 场景图，该场景图基于使用深度信息的 3D。节点表示具有特征、3D 位置和语义上下文的开放词汇对象，而边缘则捕获具有对象间距离的空间和语义关系。我们对 Replica 和 HM3D 数据集的场景进行的实验表明，ZING-3D 可以有效地捕获空间和关系知识，而无需进行特定于任务的训练。</li>
</ul>

<h3>Title: SafetyPairs: Isolating Safety Critical Image Features with Counterfactual Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Alec Helbling, Shruti Palaskar, Kundan Krishna, Polo Chau, Leon Gatys, Joseph Yitan Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21120">https://arxiv.org/abs/2510.21120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21120">https://arxiv.org/pdf/2510.21120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21120]] SafetyPairs: Isolating Safety Critical Image Features with Counterfactual Image Generation(https://arxiv.org/abs/2510.21120)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>What exactly makes a particular image unsafe? Systematically differentiating between benign and problematic images is a challenging problem, as subtle changes to an image, such as an insulting gesture or symbol, can drastically alter its safety implications. However, existing image safety datasets are coarse and ambiguous, offering only broad safety labels without isolating the specific features that drive these differences. We introduce SafetyPairs, a scalable framework for generating counterfactual pairs of images, that differ only in the features relevant to the given safety policy, thus flipping their safety label. By leveraging image editing models, we make targeted changes to images that alter their safety labels while leaving safety-irrelevant details unchanged. Using SafetyPairs, we construct a new safety benchmark, which serves as a powerful source of evaluation data that highlights weaknesses in vision-language models' abilities to distinguish between subtly different images. Beyond evaluation, we find our pipeline serves as an effective data augmentation strategy that improves the sample efficiency of training lightweight guard models. We release a benchmark containing over 3,020 SafetyPair images spanning a diverse taxonomy of 9 safety categories, providing the first systematic resource for studying fine-grained image safety distinctions.</li>
<li><strong>摘要：</strong>What exactly makes a particular image unsafe?系统地区分良性图像和有问题的图像是一个具有挑战性的问题，因为图像的细微变化（例如侮辱性手势或符号）可能会极大地改变其安全含义。然而，现有的图像安全数据集粗糙且模糊，仅提供广泛的安全标签，而没有隔离导致这些差异的特定特征。我们引入了 SafetyPairs，这是一个用于生成反事实图像对的可扩展框架，这些图像仅在与给定安全策略相关的特征上有所不同，从而翻转了它们的安全标签。通过利用图像编辑模型，我们对图像进行有针对性的更改，改变其安全标签，同时保持与安全无关的细节不变。使用 SafetyPairs，我们构建了一个新的安全基准，它作为评估数据的强大来源，突出了视觉语言模型区分细微不同图像的能力的弱点。除了评估之外，我们发现我们的管道可以作为一种有效的数据增强策略，可以提高训练轻量级防护模型的样本效率。我们发布了一个基准，其中包含 3,020 多个 SafetyPair 图像，涵盖 9 个安全类别的不同分类法，为研究细粒度图像安全差异提供了第一个系统资源。</li>
</ul>

<h3>Title: Digital Contrast CT Pulmonary Angiography Synthesis from Non-contrast CT for Pulmonary Vascular Disease</h3>
<ul>
<li><strong>Authors: </strong>Ying Ming (1), Yue Lin (3), Longfei Zhao (2), Gengwan Li (2), Zuopeng Tan (2), Bing Li (2), Sheng Xie (3), Wei Song (1), Qiqi Xu (2) ((1) Department of Radiology Peking Union Medical College Hospital Chinese Academy of Medical Sciences and Peking Union Medical College, (2) Research and Development Center Canon Medical Systems China, (3) Department of Radiology, China-Japan Friendship Hospital, Beijing, China)</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21140">https://arxiv.org/abs/2510.21140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21140">https://arxiv.org/pdf/2510.21140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21140]] Digital Contrast CT Pulmonary Angiography Synthesis from Non-contrast CT for Pulmonary Vascular Disease(https://arxiv.org/abs/2510.21140)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Computed Tomography Pulmonary Angiography (CTPA) is the reference standard for diagnosing pulmonary vascular diseases such as Pulmonary Embolism (PE) and Chronic Thromboembolic Pulmonary Hypertension (CTEPH). However, its reliance on iodinated contrast agents poses risks including nephrotoxicity and allergic reactions, particularly in high-risk patients. This study proposes a method to generate Digital Contrast CTPA (DCCTPA) from Non-Contrast CT (NCCT) scans using a cascaded synthesizer based on Cycle-Consistent Generative Adversarial Networks (CycleGAN). Totally retrospective 410 paired CTPA and NCCT scans were obtained from three centers. The model was trained and validated internally on 249 paired images. Extra dataset that comprising 161 paired images was as test set for model generalization evaluation and downstream clinical tasks validation. Compared with state-of-the-art (SOTA) methods, the proposed method achieved the best comprehensive performance by evaluating quantitative metrics (For validation, MAE: 156.28, PSNR: 20.71 and SSIM: 0.98; For test, MAE: 165.12, PSNR: 20.27 and SSIM: 0.98) and qualitative visualization, demonstrating valid vessel enhancement, superior image fidelity and structural preservation. The approach was further applied to downstream tasks of pulmonary vessel segmentation and vascular quantification. On the test set, the average Dice, clDice, and clRecall of artery and vein pulmonary segmentation was 0.70, 0.71, 0.73 and 0.70, 0.72, 0.75 respectively, all markedly improved compared with NCCT inputs.\@ Inter-class Correlation Coefficient (ICC) for vessel volume between DCCTPA and CTPA was significantly better than that between NCCT and CTPA (Average ICC : 0.81 vs 0.70), indicating effective vascular enhancement in DCCTPA, especially for small vessels.</li>
<li><strong>摘要：</strong>计算机断层扫描肺血管造影（CTPA）是诊断肺栓塞（PE）和慢性血栓栓塞性肺动脉高压（CTEPH）等肺血管疾病的参考标准。然而，它对碘造影剂的依赖带来了风险，包括肾毒性和过敏反应，特别是对于高危患者。本研究提出了一种使用基于循环一致生成对抗网络 (CycleGAN) 的级联合成器从非造影 CT (NCCT) 扫描生成数字造影 CTPA (DCCTPA) 的方法。 Totally retrospective 410 paired CTPA and NCCT scans were obtained from three centers.该模型在 249 张配对图像上进行了内部训练和验证。包含 161 个配对图像的额外数据集作为模型泛化评估和下游临床任务验证的测试集。与最先进的（SOTA）方法相比，所提出的方法通过评估定量指标（对于验证，MAE：156.28，PSNR：20.71和SSIM：0.98；对于测试，MAE：165.12，PSNR：20.27和SSIM：0.98）和定性可视化实现了最佳综合性能，展示了有效的血管增强、卓越的图像保真度和 结构保存。该方法进一步应用于肺血管分割和血管量化的下游任务。在测试集上，动脉和静脉肺分割的平均 Dice、clDice 和 clRecall 分别为 0.70、0.71、0.73 和 0.70、0.72、0.75，均较 NCCT 输入显着提高。\@ DCCTPA 和 CTPA 之间血管体积的类间相关系数（ICC）显着优于 NCCT 和 CTPA 之间 （平均 ICC：0.81 vs 0.70），表明 DCCTPA 中的血管增强有效，尤其是小血管。</li>
</ul>

<h3>Title: Uncertainty-Aware Multi-Objective Reinforcement Learning-Guided Diffusion Models for 3D De Novo Molecular Design</h3>
<ul>
<li><strong>Authors: </strong>Lianghong Chen, Dongkyu Eugene Kim, Mike Domaratzki, Pingzhao Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21153">https://arxiv.org/abs/2510.21153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21153">https://arxiv.org/pdf/2510.21153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21153]] Uncertainty-Aware Multi-Objective Reinforcement Learning-Guided Diffusion Models for 3D De Novo Molecular Design(https://arxiv.org/abs/2510.21153)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Designing de novo 3D molecules with desirable properties remains a fundamental challenge in drug discovery and molecular engineering. While diffusion models have demonstrated remarkable capabilities in generating high-quality 3D molecular structures, they often struggle to effectively control complex multi-objective constraints critical for real-world applications. In this study, we propose an uncertainty-aware Reinforcement Learning (RL) framework to guide the optimization of 3D molecular diffusion models toward multiple property objectives while enhancing the overall quality of the generated molecules. Our method leverages surrogate models with predictive uncertainty estimation to dynamically shape reward functions, facilitating balance across multiple optimization objectives. We comprehensively evaluate our framework across three benchmark datasets and multiple diffusion model architectures, consistently outperforming baselines for molecular quality and property optimization. Additionally, Molecular Dynamics (MD) simulations and ADMET profiling of top generated candidates indicate promising drug-like behavior and binding stability, comparable to known Epidermal Growth Factor Receptor (EGFR) inhibitors. Our results demonstrate the strong potential of RL-guided generative diffusion models for advancing automated molecular design.</li>
<li><strong>摘要：</strong>从头设计具有所需特性的 3D 分子仍然是药物发现和分子工程中的基本挑战。 While diffusion models have demonstrated remarkable capabilities in generating high-quality 3D molecular structures, they often struggle to effectively control complex multi-objective constraints critical for real-world applications. In this study, we propose an uncertainty-aware Reinforcement Learning (RL) framework to guide the optimization of 3D molecular diffusion models toward multiple property objectives while enhancing the overall quality of the generated molecules. Our method leverages surrogate models with predictive uncertainty estimation to dynamically shape reward functions, facilitating balance across multiple optimization objectives. We comprehensively evaluate our framework across three benchmark datasets and multiple diffusion model architectures, consistently outperforming baselines for molecular quality and property optimization. Additionally, Molecular Dynamics (MD) simulations and ADMET profiling of top generated candidates indicate promising drug-like behavior and binding stability, comparable to known Epidermal Growth Factor Receptor (EGFR) inhibitors.我们的结果证明了强化学习引导的生成扩散模型在推进自动化分子设计方面的巨大潜力。</li>
</ul>

<h3>Title: Blockwise Flow Matching: Improving Flow Matching Models For Efficient High-Quality Generation</h3>
<ul>
<li><strong>Authors: </strong>Dogyun Park, Taehoon Lee, Minseok Joo, Hyunwoo J. Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21167">https://arxiv.org/abs/2510.21167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21167">https://arxiv.org/pdf/2510.21167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21167]] Blockwise Flow Matching: Improving Flow Matching Models For Efficient High-Quality Generation(https://arxiv.org/abs/2510.21167)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recently, Flow Matching models have pushed the boundaries of high-fidelity data generation across a wide range of domains. It typically employs a single large network to learn the entire generative trajectory from noise to data. Despite their effectiveness, this design struggles to capture distinct signal characteristics across timesteps simultaneously and incurs substantial inference costs due to the iterative evaluation of the entire model. To address these limitations, we propose Blockwise Flow Matching (BFM), a novel framework that partitions the generative trajectory into multiple temporal segments, each modeled by smaller but specialized velocity blocks. This blockwise design enables each block to specialize effectively in its designated interval, improving inference efficiency and sample quality. To further enhance generation fidelity, we introduce a Semantic Feature Guidance module that explicitly conditions velocity blocks on semantically rich features aligned with pretrained representations. Additionally, we propose a lightweight Feature Residual Approximation strategy that preserves semantic quality while significantly reducing inference cost. Extensive experiments on ImageNet 256x256 demonstrate that BFM establishes a substantially improved Pareto frontier over existing Flow Matching methods, achieving 2.1x to 4.9x accelerations in inference complexity at comparable generation performance. Code is available at this https URL.</li>
<li><strong>摘要：</strong>最近，流匹配模型已经突破了各个领域的高保真数据生成的界限。它通常采用单个大型网络来学习从噪声到数据的整个生成轨迹。尽管有效，但该设计难以同时捕获跨时间步长的不同信号特征，并且由于整个模型的迭代评估而产生大量推理成本。为了解决这些限制，我们提出了块式流匹配（BFM），这是一种新颖的框架，它将生成轨迹划分为多个时间段，每个时间段都由较小但专门的速度块建模。这种分块设计使每个块能够在其指定的区间内有效地专业化，从而提高推理效率和样本质量。为了进一步提高生成保真度，我们引入了语义特征指导模块，该模块显式地根据与预训练表示对齐的语义丰富的特征来调节速度块。此外，我们提出了一种轻量级的特征残差逼近策略，可以保留语义质量，同时显着降低推理成本。 ImageNet 256x256 上的大量实验表明，BFM 相对于现有的流匹配方法建立了显着改进的帕累托前沿，在可比的生成性能下实现了 2.1 倍至 4.9 倍的推理复杂性加速。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: On the flow matching interpretability</h3>
<ul>
<li><strong>Authors: </strong>Francesco Pivi, Simone Gazza, Davide Evangelista, Roberto Amadini, Maurizio Gabbrielli</a></li>
<li><strong>Subjects: </strong>cs.LG, math.ST, physics.app-ph, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21210">https://arxiv.org/abs/2510.21210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21210">https://arxiv.org/pdf/2510.21210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21210]] On the flow matching interpretability(https://arxiv.org/abs/2510.21210)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative models based on flow matching have demonstrated remarkable success in various domains, yet they suffer from a fundamental limitation: the lack of interpretability in their intermediate generation steps. In fact these models learn to transform noise into data through a series of vector field updates, however the meaning of each step remains opaque. We address this problem by proposing a general framework constraining each flow step to be sampled from a known physical distribution. Flow trajectories are mapped to (and constrained to traverse) the equilibrium states of the simulated physical process. We implement this approach through the 2D Ising model in such a way that flow steps become thermal equilibrium points along a parametric cooling schedule. Our proposed architecture includes an encoder that maps discrete Ising configurations into a continuous latent space, a flow-matching network that performs temperature-driven diffusion, and a projector that returns to discrete Ising states while preserving physical constraints. We validate this framework across multiple lattice sizes, showing that it preserves physical fidelity while outperforming Monte Carlo generation in speed as the lattice size increases. In contrast with standard flow matching, each vector field represents a meaningful stepwise transition in the 2D Ising model's latent space. This demonstrates that embedding physical semantics into generative flows transforms opaque neural trajectories into interpretable physical processes.</li>
<li><strong>摘要：</strong>基于流匹配的生成模型在各个领域都取得了显着的成功，但它们存在一个根本性的限制：中间生成步骤缺乏可解释性。事实上，这些模型学习通过一系列向量场更新将噪声转换为数据，但每个步骤的含义仍然不透明。我们通过提出一个通用框架来解决这个问题，该框架限制每个流程步骤从已知的物理分布中采样。流动轨迹被映射到（并被限制为遍历）模拟物理过程的平衡状态。我们通过 2D Ising 模型实现这种方法，使流动步骤成为参数冷却计划中的热平衡点。我们提出的架构包括一个将离散 Ising 配置映射到连续潜在空间的编码器、一个执行温度驱动扩散的流匹配网络，以及一个在保留物理约束的同时返回离散 Ising 状态的投影仪。我们在多个晶格尺寸上验证了该框架，表明随着晶格尺寸的增加，它保持了物理保真度，同时在速度上优于蒙特卡罗生成。与标准流匹配相比，每个向量场代表 2D Ising 模型潜在空间中有意义的逐步过渡。这表明，将物理语义嵌入到生成流中可以将不透明的神经轨迹转变为可解释的物理过程。</li>
</ul>

<h3>Title: Improved Training Technique for Shortcut Models</h3>
<ul>
<li><strong>Authors: </strong>Anh Nguyen, Viet Nguyen, Duc Vu, Trung Dao, Chi Tran, Toan Tran, Anh Tran</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21250">https://arxiv.org/abs/2510.21250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21250">https://arxiv.org/pdf/2510.21250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21250]] Improved Training Technique for Shortcut Models(https://arxiv.org/abs/2510.21250)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Shortcut models represent a promising, non-adversarial paradigm for generative modeling, uniquely supporting one-step, few-step, and multi-step sampling from a single trained network. However, their widespread adoption has been stymied by critical performance bottlenecks. This paper tackles the five core issues that held shortcut models back: (1) the hidden flaw of compounding guidance, which we are the first to formalize, causing severe image artifacts; (2) inflexible fixed guidance that restricts inference-time control; (3) a pervasive frequency bias driven by a reliance on low-level distances in the direct domain, which biases reconstructions toward low frequencies; (4) divergent self-consistency arising from a conflict with EMA training; and (5) curvy flow trajectories that impede convergence. To address these challenges, we introduce iSM, a unified training framework that systematically resolves each limitation. Our framework is built on four key improvements: Intrinsic Guidance provides explicit, dynamic control over guidance strength, resolving both compounding guidance and inflexibility. A Multi-Level Wavelet Loss mitigates frequency bias to restore high-frequency details. Scaling Optimal Transport (sOT) reduces training variance and learns straighter, more stable generative paths. Finally, a Twin EMA strategy reconciles training stability with self-consistency. Extensive experiments on ImageNet 256 x 256 demonstrate that our approach yields substantial FID improvements over baseline shortcut models across one-step, few-step, and multi-step generation, making shortcut models a viable and competitive class of generative models.</li>
<li><strong>摘要：</strong>快捷模型代表了一种有前途的、非对抗性的生成建模范式，独特地支持从单个训练网络中进行一步、几步和多步采样。然而，它们的广泛采用却因关键性能瓶颈而受到阻碍。本文解决了阻碍快捷模型发展的五个核心问题：（1）复合引导的隐藏缺陷，我们是第一个将其形式化的，导致严重的图像伪影； （2）不灵活的固定指导限制了推理时间控制； (3) 由于对直接域中的低级距离的依赖而导致普遍的频率偏差，这使得重建偏向于低频； (4) 因与EMA培训冲突而产生的自我一致性分歧； (5)阻碍收敛的弯曲流动轨迹。为了应对这些挑战，我们引入了 iSM，这是一个统一的培训框架，可以系统地解决每个限制。我们的框架建立在四个关键改进的基础上： 内在指导提供了对指导强度的明确、动态控制，解决了复合指导和不灵活性的问题。多级小波损失可减轻频率偏差以恢复高频细节。缩放最佳传输 (sOT) 可以减少训练方差并学习更直、更稳定的生成路径。最后，双 EMA 策略协调了训练稳定性和自我一致性。在 ImageNet 256 x 256 上进行的大量实验表明，我们的方法在一步、少步和多步生成方面比基线快捷方式模型产生了显着的 FID 改进，使快捷方式模型成为一类可行且有竞争力的生成模型。</li>
</ul>

<h3>Title: Topology Sculptor, Shape Refiner: Discrete Diffusion Model for High-Fidelity 3D Meshes Generation</h3>
<ul>
<li><strong>Authors: </strong>Kaiyu Song, Hanjiang Lai, Yaqing Zhang, Chuangjian Cai, Yan Pan Kun Yue, Jian Yin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21264">https://arxiv.org/abs/2510.21264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21264">https://arxiv.org/pdf/2510.21264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21264]] Topology Sculptor, Shape Refiner: Discrete Diffusion Model for High-Fidelity 3D Meshes Generation(https://arxiv.org/abs/2510.21264)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce Topology Sculptor, Shape Refiner (TSSR), a novel method for generating high-quality, artist-style 3D meshes based on Discrete Diffusion Models (DDMs). Our primary motivation for TSSR is to achieve highly accurate token prediction while enabling parallel generation, a significant advantage over sequential autoregressive methods. By allowing TSSR to "see" all mesh tokens concurrently, we unlock a new level of efficiency and control. We leverage this parallel generation capability through three key innovations: 1) Decoupled Training and Hybrid Inference, which distinctly separates the DDM-based generation into a topology sculpting stage and a subsequent shape refinement stage. This strategic decoupling enables TSSR to effectively capture both intricate local topology and overarching global shape. 2) An Improved Hourglass Architecture, featuring bidirectional attention enriched by face-vertex-sequence level Rotational Positional Embeddings (RoPE), thereby capturing richer contextual information across the mesh structure. 3) A novel Connection Loss, which acts as a topological constraint to further enhance the realism and fidelity of the generated meshes. Extensive experiments on complex datasets demonstrate that TSSR generates high-quality 3D artist-style meshes, capable of achieving up to 10,000 faces at a remarkable spatial resolution of $1024^3$. The code will be released at: this https URL.</li>
<li><strong>摘要：</strong>在本文中，我们介绍了拓扑雕塑、形状细化器 (TSSR)，这是一种基于离散扩散模型 (DDM) 生成高质量、艺术风格 3D 网格的新颖方法。我们使用 TSSR 的主要动机是实现高度准确的标记预测，同时实现并行生成，这比顺序自回归方法具有显着优势。通过允许 TSSR 同时“查看”所有网格代币，我们将效率和控制提升到了一个新的水平。我们通过三个关键创新来利用这种并行生成功能：1）解耦训练和混合推理，它将基于 DDM 的生成明显分为拓扑雕刻阶段和随后的形状细化阶段。这种战略解耦使 TSSR 能够有效捕获复杂的局部拓扑和总体全局形状。 2）改进的沙漏架构，具有通过面部顶点序列级旋转位置嵌入（RoPE）丰富的双向注意力，从而在网格结构中捕获更丰富的上下文信息。 3）一种新颖的连接损失，它充当拓扑约束，以进一步增强生成的网格的真实性和保真度。对复杂数据集的大量实验表明，TSSR 可以生成高质量的 3D 艺术家风格网格，能够以 1024^3 美元的卓越空间分辨率实现多达 10,000 个面孔。代码将发布在：此 https URL。</li>
</ul>

<h3>Title: Morphologically Intelligent Perturbation Prediction with FORM</h3>
<ul>
<li><strong>Authors: </strong>Reed Naidoo, Matt De Vries, Olga Fourkioti, Vicky Bousgouni, Mar Arias-Garcia, Maria Portillo-Malumbres, Chris Bakal</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21337">https://arxiv.org/abs/2510.21337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21337">https://arxiv.org/pdf/2510.21337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21337]] Morphologically Intelligent Perturbation Prediction with FORM(https://arxiv.org/abs/2510.21337)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Understanding how cells respond to external stimuli is a central challenge in biomedical research and drug development. Current computational frameworks for modelling cellular responses remain restricted to two-dimensional representations, limiting their capacity to capture the complexity of cell morphology under perturbation. This dimensional constraint poses a critical bottleneck for the development of accurate virtual cell models. Here, we present FORM, a machine learning framework for predicting perturbation-induced changes in three-dimensional cellular structure. FORM consists of two components: a morphology encoder, trained end-to-end via a novel multi-channel VQGAN to learn compact 3D representations of cell shape, and a diffusion-based perturbation trajectory module that captures how morphology evolves across perturbation conditions. Trained on a large-scale dataset of over 65,000 multi-fluorescence 3D cell volumes spanning diverse chemical and genetic perturbations, FORM supports both unconditional morphology synthesis and conditional simulation of perturbed cell states. Beyond generation, FORM can predict downstream signalling activity, simulate combinatorial perturbation effects, and model morphodynamic transitions between states of unseen perturbations. To evaluate performance, we introduce MorphoEval, a benchmarking suite that quantifies perturbation-induced morphological changes in structural, statistical, and biological dimensions. Together, FORM and MorphoEval work toward the realisation of the 3D virtual cell by linking morphology, perturbation, and function through high-resolution predictive simulation.</li>
<li><strong>摘要：</strong>了解细胞如何响应外部刺激是生物医学研究和药物开发的核心挑战。当前用于建模细胞反应的计算框架仍然局限于二维表示，限制了它们捕获扰动下细胞形态复杂性的能力。这种尺寸限制对精确虚拟细胞模型的开发构成了关键瓶颈。在这里，我们提出了 FORM，一种机器学习框架，用于预测扰动引起的三维细胞结构变化。 FORM 由两个组件组成：形态编码器，通过新颖的多通道 VQGAN 进行端到端训练，以学习细胞形状的紧凑 3D 表示；以及基于扩散的扰动轨迹模块，捕获形态在扰动条件下如何演变。 FORM 在超过 65,000 个多荧光 3D 细胞体积的大型数据集上进行训练，涵盖不同的化学和遗传扰动，支持无条件形态合成和扰动细胞状态的条件模拟。除了生成之外，FORM 还可以预测下游信号活动、模拟组合扰动效应以及对看不见的扰动状态之间的形态动力学转换进行建模。为了评估性能，我们引入了 MorphoEval，这是一个基准测试套件，可以量化结构、统计和生物维度上扰动引起的形态变化。 FORM 和 MorphoEval 共同致力于通过高分辨率预测模拟将形态、扰动和功能联系起来，从而实现 3D 虚拟细胞。</li>
</ul>

<h3>Title: Why Registration Quality Matters: Enhancing sCT Synthesis with IMPACT-Based Registration</h3>
<ul>
<li><strong>Authors: </strong>Valentin Boussot, Cédric Hémon, Jean-Claude Nunes, Jean-Louis Dillenseger</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21358">https://arxiv.org/abs/2510.21358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21358">https://arxiv.org/pdf/2510.21358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21358]] Why Registration Quality Matters: Enhancing sCT Synthesis with IMPACT-Based Registration(https://arxiv.org/abs/2510.21358)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We participated in the SynthRAD2025 challenge (Tasks 1 and 2) with a unified pipeline for synthetic CT (sCT) generation from MRI and CBCT, implemented using the KonfAI framework. Our model is a 2.5D U-Net++ with a ResNet-34 encoder, trained jointly across anatomical regions and fine-tuned per region. The loss function combined pixel-wise L1 loss with IMPACT-Synth, a perceptual loss derived from SAM and TotalSegmentator to enhance structural fidelity. Training was performed using AdamW (initial learning rate = 0.001, halved every 25k steps) on patch-based, normalized, body-masked inputs (320x320 for MRI, 256x256 for CBCT), with random flipping as the only augmentation. No post-processing was applied. Final predictions leveraged test-time augmentation and five-fold ensembling. The best model was selected based on validation MAE. Two registration strategies were evaluated: (i) Elastix with mutual information, consistent with the challenge pipeline, and (ii) IMPACT, a feature-based similarity metric leveraging pretrained segmentation networks. On the local test sets, IMPACT-based registration achieved more accurate and anatomically consistent alignments than mutual-information-based registration, resulting in improved sCT synthesis with lower MAE and more realistic anatomical structures. On the public validation set, however, models trained with Elastix-aligned data achieved higher scores, reflecting a registration bias favoring alignment strategies consistent with the evaluation pipeline. This highlights how registration errors can propagate into supervised learning, influencing both training and evaluation, and potentially inflating performance metrics at the expense of anatomical fidelity. By promoting anatomically consistent alignment, IMPACT helps mitigate this bias and supports the development of more robust and generalizable sCT synthesis models.</li>
<li><strong>摘要：</strong>我们参加了 SynthRAD2025 挑战赛（任务 1 和 2），并使用 KonfAI 框架实现了从 MRI 和 CBCT 生成合成 CT (sCT) 的统一流程。我们的模型是带有 ResNet-34 编码器的 2.5D U-Net++，跨解剖区域联合训练并按区域进行微调。损失函数将像素级 L1 损失与 IMPACT-Synth 相结合，IMPACT-Synth 是一种源自 SAM 和 TotalSegmentator 的感知损失，用于增强结构保真度。使用 AdamW（初始学习率 = 0.001，每 25k 步减半）对基于补丁的、标准化的、身体屏蔽的输入（MRI 为 320x320，CBCT 为 256x256）进行训练，随机翻转作为唯一的增强。未应用任何后处理。最终预测利用了测试时间增强和五倍集成。根据验证 MAE 选择最佳模型。评估了两种注册策略：(i) 具有互信息的 Elastix，与挑战管道一致；(ii) IMPACT，一种利用预训练分割网络的基于特征的相似性度量。在本地测试集上，基于 IMPACT 的配准比基于互信息的配准实现了更准确、解剖学上一致的对齐，从而改进了 sCT 合成，具有更低的 MAE 和更真实的解剖结构。然而，在公共验证集上，使用 Elastix 对齐数据训练的模型获得了更高的分数，反映出有利于与评估流程一致的对齐策略的配准偏差。这凸显了配准错误如何传播到监督学习中，影响训练和评估，并可能以牺牲解剖保真度为代价夸大性能指标。通过促进解剖学上一致的对齐，IMPACT 有助于减轻这种偏差，并支持开发更强大和更通用的 sCT 合成模型。</li>
</ul>

<h3>Title: Compositional Monte Carlo Tree Diffusion for Extendable Planning</h3>
<ul>
<li><strong>Authors: </strong>Jaesik Yoon, Hyeonseo Cho, Sungjin Ahn</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21361">https://arxiv.org/abs/2510.21361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21361">https://arxiv.org/pdf/2510.21361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21361]] Compositional Monte Carlo Tree Diffusion for Extendable Planning(https://arxiv.org/abs/2510.21361)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured tree search to enable effective trajectory exploration through stepwise reasoning. However, MCTD remains fundamentally limited by training trajectory lengths. While periodic replanning allows plan concatenation for longer plan generation, the planning process remains locally confined, as MCTD searches within individual trajectories without access to global context. We propose Compositional Monte Carlo Tree Diffusion (C-MCTD), a framework that elevates planning from individual trajectory optimization to reasoning over complete plan compositions. C-MCTD introduces three complementary components: (1) Online Composer, which performs globally-aware planning by searching across entire plan compositions; (2) Distributed Composer, which reduces search complexity through parallel exploration from multiple starting points; and (3) Preplan Composer, which accelerates inference by leveraging cached plan graphs.</li>
<li><strong>摘要：</strong>蒙特卡罗树扩散 (MCTD) 将扩散模型与结构化树搜索相结合，通过逐步推理实现有效的轨迹探索。然而，MCTD 从根本上仍然受到训练轨迹长度的限制。虽然定期重新规划允许计划串联以生成更长的计划，但规划过程仍然受到局部限制，因为 MCTD 在单个轨迹内搜索而无法访问全局上下文。我们提出了组合蒙特卡罗树扩散（C-MCTD），这是一个将规划从个体轨迹优化提升到对完整计划组合进行推理的框架。 C-MCTD 引入了三个互补组件：(1) Online Composer，它通过搜索整个计划组合来执行全局感知规划； （2）Distributed Composer，通过多起点并行探索，降低搜索复杂度； (3) Preplan Composer，它通过利用缓存的计划图来加速推理。</li>
</ul>

<h3>Title: FairImagen: Post-Processing for Bias Mitigation in Text-to-Image Models</h3>
<ul>
<li><strong>Authors: </strong>Zihao Fu, Ryan Brown, Shun Shao, Kai Rawal, Eoin Delaney, Chris Russell</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21363">https://arxiv.org/abs/2510.21363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21363">https://arxiv.org/pdf/2510.21363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21363]] FairImagen: Post-Processing for Bias Mitigation in Text-to-Image Models(https://arxiv.org/abs/2510.21363)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models, such as Stable Diffusion, have demonstrated remarkable capabilities in generating high-quality and diverse images from natural language prompts. However, recent studies reveal that these models often replicate and amplify societal biases, particularly along demographic attributes like gender and race. In this paper, we introduce FairImagen (this https URL), a post-hoc debiasing framework that operates on prompt embeddings to mitigate such biases without retraining or modifying the underlying diffusion model. Our method integrates Fair Principal Component Analysis to project CLIP-based input embeddings into a subspace that minimizes group-specific information while preserving semantic content. We further enhance debiasing effectiveness through empirical noise injection and propose a unified cross-demographic projection method that enables simultaneous debiasing across multiple demographic attributes. Extensive experiments across gender, race, and intersectional settings demonstrate that FairImagen significantly improves fairness with a moderate trade-off in image quality and prompt fidelity. Our framework outperforms existing post-hoc methods and offers a simple, scalable, and model-agnostic solution for equitable text-to-image generation.</li>
<li><strong>摘要：</strong>文本到图像的扩散模型，例如稳定扩散，在根据自然语言提示生成高质量和多样化的图像方面表现出了卓越的能力。然而，最近的研究表明，这些模型经常复制和放大社会偏见，特别是在性别和种族等人口统计属性方面。在本文中，我们介绍了 FairImagen（此 https URL），这是一种事后去偏差框架，可在提示嵌入上运行以减轻此类偏差，而无需重新训练或修改底层扩散模型。我们的方法集成了公平主成分分析，将基于 CLIP 的输入嵌入投影到子空间中，在保留语义内容的同时最大限度地减少特定于组的信息。我们通过经验噪声注入进一步增强去偏有效性，并提出了一种统一的跨人口统计投影方法，可以跨多个人口统计属性同时去偏。跨性别、种族和交叉设置的广泛实验表明，FairImagen 通过在图像质量和即时保真度之间进行适度权衡，显着提高了公平性。我们的框架优于现有的事后方法，并为公平的文本到图像生成提供了简单、可扩展且与模型无关的解决方案。</li>
</ul>

<h3>Title: BADiff: Bandwidth Adaptive Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Xi Zhang, Hanwei Zhu, Yan Zhong, Jiamang Wang, Weisi Lin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21366">https://arxiv.org/abs/2510.21366</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21366">https://arxiv.org/pdf/2510.21366</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21366]] BADiff: Bandwidth Adaptive Diffusion Model(https://arxiv.org/abs/2510.21366)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In this work, we propose a novel framework to enable diffusion models to adapt their generation quality based on real-time network bandwidth constraints. Traditional diffusion models produce high-fidelity images by performing a fixed number of denoising steps, regardless of downstream transmission limitations. However, in practical cloud-to-device scenarios, limited bandwidth often necessitates heavy compression, leading to loss of fine textures and wasted computation. To address this, we introduce a joint end-to-end training strategy where the diffusion model is conditioned on a target quality level derived from the available bandwidth. During training, the model learns to adaptively modulate the denoising process, enabling early-stop sampling that maintains perceptual quality appropriate to the target transmission condition. Our method requires minimal architectural changes and leverages a lightweight quality embedding to guide the denoising trajectory. Experimental results demonstrate that our approach significantly improves the visual fidelity of bandwidth-adapted generations compared to naive early-stopping, offering a promising solution for efficient image delivery in bandwidth-constrained environments. Code is available at: this https URL.</li>
<li><strong>摘要：</strong>在这项工作中，我们提出了一种新颖的框架，使扩散模型能够根据实时网络带宽约束来调整其生成质量。传统的扩散模型通过执行固定数量的去噪步骤来生成高保真图像，而不管下游传输的限制如何。然而，在实际的云到设备场景中，有限的带宽通常需要大量压缩，从而导致精细纹理的损失和计算的浪费。为了解决这个问题，我们引入了一种联合端到端训练策略，其中扩散模型以从可用带宽得出的目标质量水平为条件。在训练过程中，模型学习自适应地调节去噪过程，从而实现提前停止采样，从而保持适合目标传输条件的感知质量。我们的方法需要最少的架构更改，并利用轻量级质量嵌入来指导去噪轨迹。实验结果表明，与简单的提前停止相比，我们的方法显着提高了带宽适应一代的视觉保真度，为带宽受限环境中的高效图像传输提供了一种有前景的解决方案。代码可在以下位置获得：此 https URL。</li>
</ul>

<h3>Title: TerraGen: A Unified Multi-Task Layout Generation Framework for Remote Sensing Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Datao Tang, Hao Wang, Yudeng Xin, Hui Qiao, Dongsheng Jiang, Yin Li, Zhiheng Yu, Xiangyong Cao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21391">https://arxiv.org/abs/2510.21391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21391">https://arxiv.org/pdf/2510.21391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21391]] TerraGen: A Unified Multi-Task Layout Generation Framework for Remote Sensing Data Augmentation(https://arxiv.org/abs/2510.21391)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Remote sensing vision tasks require extensive labeled data across multiple, interconnected domains. However, current generative data augmentation frameworks are task-isolated, i.e., each vision task requires training an independent generative model, and ignores the modeling of geographical information and spatial constraints. To address these issues, we propose \textbf{TerraGen}, a unified layout-to-image generation framework that enables flexible, spatially controllable synthesis of remote sensing imagery for various high-level vision tasks, e.g., detection, segmentation, and extraction. Specifically, TerraGen introduces a geographic-spatial layout encoder that unifies bounding box and segmentation mask inputs, combined with a multi-scale injection scheme and mask-weighted loss to explicitly encode spatial constraints, from global structures to fine details. Also, we construct the first large-scale multi-task remote sensing layout generation dataset containing 45k images and establish a standardized evaluation protocol for this task. Experimental results show that our TerraGen can achieve the best generation image quality across diverse tasks. Additionally, TerraGen can be used as a universal data-augmentation generator, enhancing downstream task performance significantly and demonstrating robust cross-task generalisation in both full-data and few-shot scenarios.</li>
<li><strong>摘要：</strong>遥感视觉任务需要跨多个互连领域的大量标记数据。然而，当前的生成数据增强框架是任务隔离的，即每个视觉任务都需要训练独立的生成模型，并且忽略了地理信息和空间约束的建模。为了解决这些问题，我们提出了 \textbf{TerraGen}，这是一个统一的布局到图像生成框架，可以为各种高级视觉任务（例如检测、分割和提取）实现灵活、空间可控的遥感图像合成。具体来说，TerraGen 引入了一种地理空间布局编码器，它统一了边界框和分段掩模输入，并结合多尺度注入方案和掩模加权损失来显式编码从全局结构到精细细节的空间约束。此外，我们构建了第一个包含 45k 图像的大规模多任务遥感布局生成数据集，并为此任务建立了标准化评估协议。实验结果表明，我们的 TerraGen 可以在不同的任务中实现最佳的生成图像质量。此外，TerraGen 可以用作通用数据增强生成器，显着增强下游任务性能，并在全数据和少样本场景中展示强大的跨任务泛化能力。</li>
</ul>

<h3>Title: Self-diffusion for Solving Inverse Problems</h3>
<ul>
<li><strong>Authors: </strong>Guanxiong Luo, Shoujin Huang, Yanlong Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21417">https://arxiv.org/abs/2510.21417</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21417">https://arxiv.org/pdf/2510.21417</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21417]] Self-diffusion for Solving Inverse Problems(https://arxiv.org/abs/2510.21417)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We propose self-diffusion, a novel framework for solving inverse problems without relying on pretrained generative models. Traditional diffusion-based approaches require training a model on a clean dataset to learn to reverse the forward noising process. This model is then used to sample clean solutions -- corresponding to posterior sampling from a Bayesian perspective -- that are consistent with the observed data under a specific task. In contrast, self-diffusion introduces a self-contained iterative process that alternates between noising and denoising steps to progressively refine its estimate of the solution. At each step of self-diffusion, noise is added to the current estimate, and a self-denoiser, which is a single untrained convolutional network randomly initialized from scratch, is continuously trained for certain iterations via a data fidelity loss to predict the solution from the noisy estimate. Essentially, self-diffusion exploits the spectral bias of neural networks and modulates it through a scheduled noise process. Without relying on pretrained score functions or external denoisers, this approach still remains adaptive to arbitrary forward operators and noisy observations, making it highly flexible and broadly applicable. We demonstrate the effectiveness of our approach on a variety of linear inverse problems, showing that self-diffusion achieves competitive or superior performance compared to other methods.</li>
<li><strong>摘要：</strong>我们提出了自扩散，一种无需依赖预训练生成模型即可解决逆问题的新颖框架。传统的基于扩散的方法需要在干净的数据集上训练模型，以学习逆转前向噪声过程。然后，该模型用于对干净的解决方案进行采样（对应于贝叶斯角度的后验采样），这些解决方案与特定任务下观察到的数据一致。相反，自扩散引入了一个独立的迭代过程，该过程在噪声和去噪步骤之间交替，以逐步完善其对解的估计。在自扩散的每个步骤中，噪声都会添加到当前估计中，并且自降噪器是从头开始随机初始化的单个未经训练的卷积网络，通过数据保真度损失对某些迭代进行连续训练，以根据噪声估计来预测解决方案。本质上，自扩散利用了神经网络的频谱偏差，并通过预定的噪声过程对其进行调制。在不依赖预训练得分函数或外部降噪器的情况下，这种方法仍然能够适应任意前向算子和噪声观测，使其高度灵活且适用范围广泛。我们证明了我们的方法在各种线性逆问题上的有效性，表明与其他方法相比，自扩散实现了有竞争力或优越的性能。</li>
</ul>

<h3>Title: ArtiLatent: Realistic Articulated 3D Object Generation via Structured Latents</h3>
<ul>
<li><strong>Authors: </strong>Honghua Chen, Yushi Lan, Yongwei Chen, Xingang Pan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21432">https://arxiv.org/abs/2510.21432</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21432">https://arxiv.org/pdf/2510.21432</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21432]] ArtiLatent: Realistic Articulated 3D Object Generation via Structured Latents(https://arxiv.org/abs/2510.21432)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We propose ArtiLatent, a generative framework that synthesizes human-made 3D objects with fine-grained geometry, accurate articulation, and realistic appearance. Our approach jointly models part geometry and articulation dynamics by embedding sparse voxel representations and associated articulation properties, including joint type, axis, origin, range, and part category, into a unified latent space via a variational autoencoder. A latent diffusion model is then trained over this space to enable diverse yet physically plausible sampling. To reconstruct photorealistic 3D shapes, we introduce an articulation-aware Gaussian decoder that accounts for articulation-dependent visibility changes (e.g., revealing the interior of a drawer when opened). By conditioning appearance decoding on articulation state, our method assigns plausible texture features to regions that are typically occluded in static poses, significantly improving visual realism across articulation configurations. Extensive experiments on furniture-like objects from PartNet-Mobility and ACD datasets demonstrate that ArtiLatent outperforms existing approaches in geometric consistency and appearance fidelity. Our framework provides a scalable solution for articulated 3D object synthesis and manipulation.</li>
<li><strong>摘要：</strong>我们提出了 ArtiLatent，一个生成框架，可以合成具有细粒度几何、精确清晰度和逼真外观的人造 3D 对象。我们的方法通过变分自动编码器将稀疏体素表示和相关的关节属性（包括关节类型、轴、原点、范围和零件类别）嵌入到统一的潜在空间中，对零件几何形状和关节动力学进行联合建模。然后在这个空间上训练潜在扩散模型，以实现多样化但物理上合理的采样。为了重建逼真的 3D 形状，我们引入了一种关节感知的高斯解码器，它可以解释与关节相关的可见性变化（例如，打开抽屉时显示抽屉的内部）。通过对清晰度状态进行外观解码，我们的方法将合理的纹理特征分配给通常在静态姿势中被遮挡的区域，从而显着提高了各个清晰度配置的视觉真实感。对 PartNet-Mobility 和 ACD 数据集的家具类物体进行的大量实验表明，ArtiLatent 在几何一致性和外观保真度方面优于现有方法。我们的框架为铰接式 3D 对象合成和操作提供了可扩展的解决方案。</li>
</ul>

<h3>Title: Anisotropic Pooling for LUT-realizable CNN Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Xi Zhang, Xiaolin Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21437">https://arxiv.org/abs/2510.21437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21437">https://arxiv.org/pdf/2510.21437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21437]] Anisotropic Pooling for LUT-realizable CNN Image Restoration(https://arxiv.org/abs/2510.21437)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Table look-up realization of image restoration CNNs has the potential of achieving competitive image quality while being much faster and resource frugal than the straightforward CNN implementation. The main technical challenge facing the LUT-based CNN algorithm designers is to manage the table size without overly restricting the receptive field. The prevailing strategy is to reuse the table for small pixel patches of different orientations (apparently assuming a degree of isotropy) and then fuse the look-up results. The fusion is currently done by average pooling, which we find being ill suited to anisotropic signal structures. To alleviate the problem, we investigate and discuss anisotropic pooling methods to replace naive averaging for improving the performance of the current LUT-realizable CNN restoration methods. First, we introduce the method of generalized median pooling which leads to measurable gains over average pooling. We then extend this idea by learning data-dependent pooling coefficients for each orientation, so that they can adaptively weigh the contributions of differently oriented pixel patches. Experimental results on various restoration benchmarks show that our anisotropic pooling strategy yields both perceptually and numerically superior results compared to existing LUT-realizable CNN methods.</li>
<li><strong>摘要：</strong>图像恢复 CNN 的表查找实现具有实现有竞争力的图像质量的潜力，同时比直接的 CNN 实现速度更快且资源节约。基于LUT的CNN算法设计者面临的主要技术挑战是在不过度限制感受野的情况下管理表大小。普遍的策略是对不同方向的小像素块重用该表（显然假设一定程度的各向同性），然后融合查找结果。目前，融合是通过平均池来完成的，我们发现它不适合各向异性信号结构。为了缓解这个问题，我们研究和讨论了各向异性池化方法来取代朴素平均，以提高当前 LUT 可实现的 CNN 恢复方法的性能。首先，我们引入广义中值池化方法，该方法比平均池化带来可测量的收益。然后，我们通过学习每个方向的数据相关池系数来扩展这个想法，以便它们可以自适应地权衡不同方向像素块的贡献。各种恢复基准的实验结果表明，与现有的 LUT 可实现的 CNN 方法相比，我们的各向异性池化策略在感知和数值上都产生了更好的结果。</li>
</ul>

<h3>Title: VidSplice: Towards Coherent Video Inpainting via Explicit Spaced Frame Guidance</h3>
<ul>
<li><strong>Authors: </strong>Ming Xie, Junqiu Yu, Qiaole Dong, Xiangyang Xue, Yanwei Fu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21461">https://arxiv.org/abs/2510.21461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21461">https://arxiv.org/pdf/2510.21461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21461]] VidSplice: Towards Coherent Video Inpainting via Explicit Spaced Frame Guidance(https://arxiv.org/abs/2510.21461)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent video inpainting methods often employ image-to-video (I2V) priors to model temporal consistency across masked frames. While effective in moderate cases, these methods struggle under severe content degradation and tend to overlook spatiotemporal stability, resulting in insufficient control over the latter parts of the video. To address these limitations, we decouple video inpainting into two sub-tasks: multi-frame consistent image inpainting and masked area motion propagation. We propose VidSplice, a novel framework that introduces spaced-frame priors to guide the inpainting process with spatiotemporal cues. To enhance spatial coherence, we design a CoSpliced Module to perform first-frame propagation strategy that diffuses the initial frame content into subsequent reference frames through a splicing mechanism. Additionally, we introduce a delicate context controller module that encodes coherent priors after frame duplication and injects the spliced video into the I2V generative backbone, effectively constraining content distortion during generation. Extensive evaluations demonstrate that VidSplice achieves competitive performance across diverse video inpainting scenarios. Moreover, its design significantly improves both foreground alignment and motion stability, outperforming existing approaches.</li>
<li><strong>摘要：</strong>最近的视频修复方法通常采用图像到视频（I2V）先验来对屏蔽帧之间的时间一致性进行建模。虽然在中等情况下有效，但这些方法在严重的内容降级情况下举步维艰，并且往往忽视时空稳定性，导致对视频后半部分的控制不足。为了解决这些限制，我们将视频修复分解为两个子任务：多帧一致图像修复和遮罩区域运动传播。我们提出了 VidSplice，这是一种新颖的框架，它引入了间隔帧先验，以通过时空线索指导修复过程。为了增强空间连贯性，我们设计了一个 CoSpliced 模块来执行第一帧传播策略，通过拼接机制将初始帧内容扩散到后续参考帧中。此外，我们引入了一个精致的上下文控制器模块，该模块在帧复制后对连贯先验进行编码，并将拼接视频注入 I2V 生成主干，从而有效地限制生成过程中的内容失真。广泛的评估表明，VidSplice 在不同的视频修复场景中实现了具有竞争力的性能。此外，其设计显着改善了前景对齐和运动稳定性，优于现有方法。</li>
</ul>

<h3>Title: Towards a Golden Classifier-Free Guidance Path via Foresight Fixed Point Iterations</h3>
<ul>
<li><strong>Authors: </strong>Kaibo Wang, Jianda Mao, Tong Wu, Yang Xiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21512">https://arxiv.org/abs/2510.21512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21512">https://arxiv.org/pdf/2510.21512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21512]] Towards a Golden Classifier-Free Guidance Path via Foresight Fixed Point Iterations(https://arxiv.org/abs/2510.21512)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Classifier-Free Guidance (CFG) is an essential component of text-to-image diffusion models, and understanding and advancing its operational mechanisms remains a central focus of research. Existing approaches stem from divergent theoretical interpretations, thereby limiting the design space and obscuring key design choices. To address this, we propose a unified perspective that reframes conditional guidance as fixed point iterations, seeking to identify a golden path where latents produce consistent outputs under both conditional and unconditional generation. We demonstrate that CFG and its variants constitute a special case of single-step short-interval iteration, which is theoretically proven to exhibit inefficiency. To this end, we introduce Foresight Guidance (FSG), which prioritizes solving longer-interval subproblems in early diffusion stages with increased iterations. Extensive experiments across diverse datasets and model architectures validate the superiority of FSG over state-of-the-art methods in both image quality and computational efficiency. Our work offers novel perspectives for conditional guidance and unlocks the potential of adaptive design.</li>
<li><strong>摘要：</strong>无分类器指导（CFG）是文本到图像扩散模型的重要组成部分，理解和推进其运行机制仍然是研究的中心焦点。现有的方法源于不同的理论解释，从而限制了设计空间并模糊了关键的设计选择。为了解决这个问题，我们提出了一个统一的视角，将条件指导重新构建为定点迭代，试图找到一条黄金路径，使潜在变量在条件和无条件生成下产生一致的输出。我们证明 CFG 及其变体构成了单步短间隔迭代的特例，理论上证明其效率低下。为此，我们引入了前瞻指导（FSG），它优先考虑在早期扩散阶段通过增加迭代来解决较长间隔的子问题。跨不同数据集和模型架构的广泛实验验证了 FSG 在图像质量和计算效率方面优于最先进的方法。我们的工作为条件指导提供了新颖的视角，并释放了自适应设计的潜力。</li>
</ul>

<h3>Title: Head Pursuit: Probing Attention Specialization in Multimodal Transformers</h3>
<ul>
<li><strong>Authors: </strong>Lorenzo Basile, Valentino Maiorca, Diego Doimo, Francesco Locatello, Alberto Cazzaniga</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21518">https://arxiv.org/abs/2510.21518</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21518">https://arxiv.org/pdf/2510.21518</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21518]] Head Pursuit: Probing Attention Specialization in Multimodal Transformers(https://arxiv.org/abs/2510.21518)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Language and vision-language models have shown impressive performance across a wide range of tasks, but their internal mechanisms remain only partly understood. In this work, we study how individual attention heads in text-generative models specialize in specific semantic or visual attributes. Building on an established interpretability method, we reinterpret the practice of probing intermediate activations with the final decoding layer through the lens of signal processing. This lets us analyze multiple samples in a principled way and rank attention heads based on their relevance to target concepts. Our results show consistent patterns of specialization at the head level across both unimodal and multimodal transformers. Remarkably, we find that editing as few as 1% of the heads, selected using our method, can reliably suppress or enhance targeted concepts in the model output. We validate our approach on language tasks such as question answering and toxicity mitigation, as well as vision-language tasks including image classification and captioning. Our findings highlight an interpretable and controllable structure within attention layers, offering simple tools for understanding and editing large-scale generative models.</li>
<li><strong>摘要：</strong>语言和视觉语言模型在广泛的任务中表现出了令人印象深刻的性能，但它们的内部机制仍然只有部分被了解。在这项工作中，我们研究文本生成模型中的个体注意力头如何专注于特定的语义或视觉属性。基于已建立的可解释性方法，我们从信号处理的角度重新解释了利用最终解码层探测中间激活的实践。这让我们能够以原则性的方式分析多个样本，并根据注意力头与目标概念的相关性对注意力头进行排名。我们的结果显示单模态和多模态变压器的高层专业化模式一致。值得注意的是，我们发现，使用我们的方法选择编辑少至 1% 的头部，就可以可靠地抑制或增强模型输出中的目标概念。我们在诸如问答和毒性缓解等语言任务以及包括图像分类和字幕等视觉语言任务上验证了我们的方法。我们的研究结果强调了注意力层内的可解释和可控结构，为理解和编辑大规模生成模型提供了简单的工具。</li>
</ul>

<h3>Title: Surrogate-based quantification of policy uncertainty in generative flow networks</h3>
<ul>
<li><strong>Authors: </strong>Ramón Nartallo-Kaluarachchi, Robert Manson-Sawko, Shashanka Ubaru, Dongsung Huh, Małgorzata J Zimoń, Lior Horesh, Yoshua Bengio</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21523">https://arxiv.org/abs/2510.21523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21523">https://arxiv.org/pdf/2510.21523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21523]] Surrogate-based quantification of policy uncertainty in generative flow networks(https://arxiv.org/abs/2510.21523)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative flow networks are able to sample, via sequential construction, high-reward, complex objects according to a reward function. However, such reward functions are often estimated approximately from noisy data, leading to epistemic uncertainty in the learnt policy. We present an approach to quantify this uncertainty by constructing a surrogate model composed of a polynomial chaos expansion, fit on a small ensemble of trained flow networks. This model learns the relationship between reward functions, parametrised in a low-dimensional space, and the probability distributions over actions at each step along a trajectory of the flow network. The surrogate model can then be used for inexpensive Monte Carlo sampling to estimate the uncertainty in the policy given uncertain rewards. We illustrate the performance of our approach on a discrete and continuous grid-world, symbolic regression, and a Bayesian structure learning task.</li>
<li><strong>摘要：</strong>生成流网络能够根据奖励函数通过顺序构造对高奖励、复杂的对象进行采样。然而，这种奖励函数通常是根据噪声数据近似估计的，导致学习策略的认知不确定性。我们提出了一种通过构建由多项式混沌展开组成的代理模型来量化这种不确定性的方法，该模型适合训练有素的流网络的小型集合。该模型学习低维空间中参数化的奖励函数之间的关系，以及沿着流网络轨迹的每一步动作的概率分布。然后，代理模型可以用于廉价的蒙特卡罗抽样，以估计给定不确定奖励的政策的不确定性。我们说明了我们的方法在离散和连续网格世界、符号回归和贝叶斯结构学习任务上的性能。</li>
</ul>

<h3>Title: FrameShield: Adversarially Robust Video Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Mojtaba Nafez, Mobina Poulaei, Nikan Vasei, Bardia Soltani Moakhar, Mohammad Sabokrou, MohammadHossein Rohban</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21532">https://arxiv.org/abs/2510.21532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21532">https://arxiv.org/pdf/2510.21532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21532]] FrameShield: Adversarially Robust Video Anomaly Detection(https://arxiv.org/abs/2510.21532)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Weakly Supervised Video Anomaly Detection (WSVAD) has achieved notable advancements, yet existing models remain vulnerable to adversarial attacks, limiting their reliability. Due to the inherent constraints of weak supervision, where only video-level labels are provided despite the need for frame-level predictions, traditional adversarial defense mechanisms, such as adversarial training, are not effective since video-level adversarial perturbations are typically weak and inadequate. To address this limitation, pseudo-labels generated directly from the model can enable frame-level adversarial training; however, these pseudo-labels are inherently noisy, significantly degrading performance. We therefore introduce a novel Pseudo-Anomaly Generation method called Spatiotemporal Region Distortion (SRD), which creates synthetic anomalies by applying severe augmentations to localized regions in normal videos while preserving temporal consistency. Integrating these precisely annotated synthetic anomalies with the noisy pseudo-labels substantially reduces label noise, enabling effective adversarial training. Extensive experiments demonstrate that our method significantly enhances the robustness of WSVAD models against adversarial attacks, outperforming state-of-the-art methods by an average of 71.0\% in overall AUROC performance across multiple benchmarks. The implementation and code are publicly available at this https URL.</li>
<li><strong>摘要：</strong>弱监督视频异常检测（WSVAD）取得了显着的进步，但现有模型仍然容易受到对抗性攻击，限制了其可靠性。由于弱监督的固有限制，尽管需要帧级预测，但仅提供视频级标签，传统的对抗性防御机制（例如对抗性训练）并不有效，因为视频级对抗性扰动通常很弱且不充分。为了解决这个限制，直接从模型生成的伪标签可以实现帧级对抗训练；然而，这些伪标签本质上是有噪声的，会显着降低性能。因此，我们引入了一种称为时空区域扭曲（SRD）的新型伪异常生成方法，该方法通过对正常视频中的局部区域进行严重增强，同时保持时间一致性来创建合成异常。将这些精确注释的合成异常与嘈杂的伪标签相结合，可以大大减少标签噪声，从而实现有效的对抗训练。大量实验表明，我们的方法显着增强了 WSVAD 模型针对对抗性攻击的鲁棒性，在多个基准测试中的整体 AUROC 性能平均优于最先进的方法 71.0%。实现和代码可通过此 https URL 公开获得。</li>
</ul>

<h3>Title: Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Yifu Luo, Penghui Du, Bo Li, Sinan Du, Tiantian Zhang, Yongzhe Chang, Kai Wu, Kun Gai, Xueqian Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21583">https://arxiv.org/abs/2510.21583</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21583">https://arxiv.org/pdf/2510.21583</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21583]] Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image Generation(https://arxiv.org/abs/2510.21583)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Group Relative Policy Optimization (GRPO) has shown strong potential for flow-matching-based text-to-image (T2I) generation, but it faces two key limitations: inaccurate advantage attribution, and the neglect of temporal dynamics of generation. In this work, we argue that shifting the optimization paradigm from the step level to the chunk level can effectively alleviate these issues. Building on this idea, we propose Chunk-GRPO, the first chunk-level GRPO-based approach for T2I generation. The insight is to group consecutive steps into coherent 'chunk's that capture the intrinsic temporal dynamics of flow matching, and to optimize policies at the chunk level. In addition, we introduce an optional weighted sampling strategy to further enhance performance. Extensive experiments show that ChunkGRPO achieves superior results in both preference alignment and image quality, highlighting the promise of chunk-level optimization for GRPO-based methods.</li>
<li><strong>摘要：</strong>组相对策略优化（GRPO）在基于流匹配的文本到图像（T2I）生成方面显示出强大的潜力，但它面临两个关键限制：不准确的优势归因以及忽略生成的时间动态。在这项工作中，我们认为将优化范式从步骤级别转移到块级别可以有效缓解这些问题。基于这个想法，我们提出了 Chunk-GRPO，这是第一个基于块级 GRPO 的 T2I 生成方法。其见解是将连续的步骤分组为连贯的“块”，捕获流匹配的内在时间动态，并在块级别优化策略。此外，我们引入了可选的加权采样策略以进一步提高性能。大量实验表明，ChunkGRPO 在偏好对齐和图像质量方面均取得了优异的结果，凸显了基于 GRPO 的方法进行块级优化的前景。</li>
</ul>

<h3>Title: Restore Text First, Enhance Image Later: Two-Stage Scene Text Image Super-Resolution with Glyph Structure Guidance</h3>
<ul>
<li><strong>Authors: </strong>Minxing Luo, Linlong Fan, Wang Qiushi, Ge Wu, Yiyan Luo, Yuhang Yu, Jinwei Chen, Yaxing Wang, Qingnan Fan, Jian Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21590">https://arxiv.org/abs/2510.21590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21590">https://arxiv.org/pdf/2510.21590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21590]] Restore Text First, Enhance Image Later: Two-Stage Scene Text Image Super-Resolution with Glyph Structure Guidance(https://arxiv.org/abs/2510.21590)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, super-resolution, generative</a></li>
<li><strong>Abstract: </strong>Current generative super-resolution methods show strong performance on natural images but distort text, creating a fundamental trade-off between image quality and textual readability. To address this, we introduce \textbf{TIGER} (\textbf{T}ext-\textbf{I}mage \textbf{G}uided sup\textbf{E}r-\textbf{R}esolution), a novel two-stage framework that breaks this trade-off through a \textit{"text-first, image-later"} paradigm. \textbf{TIGER} explicitly decouples glyph restoration from image enhancement: it first reconstructs precise text structures and then uses them to guide subsequent full-image super-resolution. This glyph-to-image guidance ensures both high fidelity and visual consistency. To support comprehensive training and evaluation, we also contribute the \textbf{UltraZoom-ST} (UltraZoom-Scene Text), the first scene text dataset with extreme zoom (\textbf{$\times$14.29}). Extensive experiments show that \textbf{TIGER} achieves \textbf{state-of-the-art} performance, enhancing readability while preserving overall image quality.</li>
<li><strong>摘要：</strong>当前的生成超分辨率方法在自然图像上表现出强大的性能，但会扭曲文本，从而在图像质量和文本可读性之间建立了基本的权衡。为了解决这个问题，我们引入了 \textbf{TIGER} (\textbf{T}ext-\textbf{I}mage \textbf{G}uided super\textbf{E}r-\textbf{R}esolution)，这是一种新颖的两阶段框架，它通过 \textit{"text-first, image-later"} 打破了这种权衡 范例。 \textbf{TIGER} 明确地将字形恢复与图像增强解耦：它首先重建精确的文本结构，然后使用它们来指导后续的全图像超分辨率。这种字形到图像的指导确保了高保真度和视觉一致性。为了支持全面的训练和评估，我们还贡献了 \textbf{UltraZoom-ST} (UltraZoom-Scene Text)，这是第一个具有极限缩放的场景文本数据集 (\textbf{$\times$14.29})。大量实验表明 \textbf{TIGER} 实现了 \textbf{state-of-the-art} 性能，增强了可读性，同时保持了整体图像质量。</li>
</ul>

<h3>Title: Accelerating Data Generation for Nonlinear temporal PDEs via homologous perturbation in solution space</h3>
<ul>
<li><strong>Authors: </strong>Lei Liu, Zhenxin Huang, Hong Wang, huanshuo dong, Haiyang Xin, Hongwei Zhao, Bin Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21592">https://arxiv.org/abs/2510.21592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21592">https://arxiv.org/pdf/2510.21592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21592]] Accelerating Data Generation for Nonlinear temporal PDEs via homologous perturbation in solution space(https://arxiv.org/abs/2510.21592)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Data-driven deep learning methods like neural operators have advanced in solving nonlinear temporal partial differential equations (PDEs). However, these methods require large quantities of solution pairs\u2014the solution functions and right-hand sides (RHS) of the equations. These pairs are typically generated via traditional numerical methods, which need thousands of time steps iterations far more than the dozens required for training, creating heavy computational and temporal overheads. To address these challenges, we propose a novel data generation algorithm, called HOmologous Perturbation in Solution Space (HOPSS), which directly generates training datasets with fewer time steps rather than following the traditional approach of generating large time steps datasets. This algorithm simultaneously accelerates dataset generation and preserves the approximate precision required for model training. Specifically, we first obtain a set of base solution functions from a reliable solver, usually with thousands of time steps, and then align them in time steps with training datasets by downsampling. Subsequently, we propose a "homologous perturbation" approach: by combining two solution functions (one as the primary function, the other as a homologous perturbation term scaled by a small scalar) with random noise, we efficiently generate comparable-precision PDE data points. Finally, using these data points, we compute the variation in the original equation's RHS to form new solution pairs. Theoretical and experimental results show HOPSS lowers time complexity. For example, on the Navier-Stokes equation, it generates 10,000 samples in approximately 10% of traditional methods' time, with comparable model training performance.</li>
<li><strong>摘要：</strong>神经算子等数据驱动的深度学习方法在求解非线性时间偏微分方程 (PDE) 方面取得了进展。然而，这些方法需要大量的解对——解函数和方程的右侧 (RHS)。这些对通常是通过传统的数值方法生成的，需要数千次时间步迭代，远远超过训练所需的数十次，从而产生大量的计算和时间开销。为了解决这些挑战，我们提出了一种新颖的数据生成算法，称为解空间中的同源扰动（HOPSS），它直接生成具有较少时间步长的训练数据集，而不是遵循生成大时间步长数据集的传统方法。该算法同时加速数据集生成并保留模型训练所需的近似精度。具体来说，我们首先从可靠的求解器获得一组基本解函数，通常具有数千个时间步长，然后通过下采样将它们与训练数据集在时间步长中对齐。随后，我们提出了一种“同源扰动”方法：通过将两个解函数（一个作为主函数，另一个作为按小标量缩放的同源扰动项）与随机噪声相结合，我们有效地生成可比较精度的 PDE 数据点。最后，使用这些数据点，我们计算原始方程 RHS 的变化以形成新的解对。理论和实验结果表明HOPSS降低了时间复杂度。例如，在 Navier-Stokes 方程上，它生成 10,000 个样本所需的时间约为传统方法的 10%，且模型训练性能相当。</li>
</ul>

<h3>Title: S3OD: Towards Generalizable Salient Object Detection with Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Orest Kupyn, Hirokatsu Kataoka, Christian Rupprecht</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21605">https://arxiv.org/abs/2510.21605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21605">https://arxiv.org/pdf/2510.21605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21605]] S3OD: Towards Generalizable Salient Object Detection with Synthetic Data(https://arxiv.org/abs/2510.21605)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Salient object detection exemplifies data-bounded tasks where expensive pixel-precise annotations force separate model training for related subtasks like DIS and HR-SOD. We present a method that dramatically improves generalization through large-scale synthetic data generation and ambiguity-aware architecture. We introduce S3OD, a dataset of over 139,000 high-resolution images created through our multi-modal diffusion pipeline that extracts labels from diffusion and DINO-v3 features. The iterative generation framework prioritizes challenging categories based on model performance. We propose a streamlined multi-mask decoder that naturally handles the inherent ambiguity in salient object detection by predicting multiple valid interpretations. Models trained solely on synthetic data achieve 20-50% error reduction in cross-dataset generalization, while fine-tuned versions reach state-of-the-art performance across DIS and HR-SOD benchmarks.</li>
<li><strong>摘要：</strong>显着对象检测举例说明了数据有限的任务，其中昂贵的像素精确注释迫使对 DIS 和 HR-SOD 等相关子任务进行单独的模型训练。我们提出了一种通过大规模合成数据生成和歧义感知架构显着提高泛化能力的方法。我们引入了 S3OD，这是一个包含超过 139,000 张高分辨率图像的数据集，通过我们的多模式扩散管道创建，该管道从扩散和 DINO-v3 特征中提取标签。迭代生成框架根据模型性能优先考虑具有挑战性的类别。我们提出了一种简化的多掩模解码器，它通过预测多个有效的解释来自然地处理显着对象检测中固有的模糊性。仅基于合成数据训练的模型在跨数据集泛化方面实现了 20-50% 的错误减少，而微调版本则在 DIS 和 HR-SOD 基准测试中达到了最先进的性能。</li>
</ul>

<h3>Title: Generalised Flow Maps for Few-Step Generative Modelling on Riemannian Manifolds</h3>
<ul>
<li><strong>Authors: </strong>Oscar Davis, Michael S. Albergo, Nicholas M. Boffi, Michael M. Bronstein, Avishek Joey Bose</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21608">https://arxiv.org/abs/2510.21608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21608">https://arxiv.org/pdf/2510.21608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21608]] Generalised Flow Maps for Few-Step Generative Modelling on Riemannian Manifolds(https://arxiv.org/abs/2510.21608)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Geometric data and purpose-built generative models on them have become ubiquitous in high-impact deep learning application domains, ranging from protein backbone generation and computational chemistry to geospatial data. Current geometric generative models remain computationally expensive at inference -- requiring many steps of complex numerical simulation -- as they are derived from dynamical measure transport frameworks such as diffusion and flow-matching on Riemannian manifolds. In this paper, we propose Generalised Flow Maps (GFM), a new class of few-step generative models that generalises the Flow Map framework in Euclidean spaces to arbitrary Riemannian manifolds. We instantiate GFMs with three self-distillation-based training methods: Generalised Lagrangian Flow Maps, Generalised Eulerian Flow Maps, and Generalised Progressive Flow Maps. We theoretically show that GFMs, under specific design decisions, unify and elevate existing Euclidean few-step generative models, such as consistency models, shortcut models, and meanflows, to the Riemannian setting. We benchmark GFMs against other geometric generative models on a suite of geometric datasets, including geospatial data, RNA torsion angles, and hyperbolic manifolds, and achieve state-of-the-art sample quality for single- and few-step evaluations, and superior or competitive log-likelihoods using the implicit probability flow.</li>
<li><strong>摘要：</strong>几何数据和专门构建的生成模型在高影响力的深度学习应用领域中已经变得无处不在，从蛋白质主干生成和计算化学到地理空间数据。当前的几何生成模型在推理方面的计算成本仍然很高，需要许多复杂的数值模拟步骤，因为它们是从动态测量传输框架（例如黎曼流形上的扩散和流动匹配）导出的。在本文中，我们提出了广义流图（GFM），这是一类新的少步生成模型，它将欧几里得空间中的流图框架推广到任意黎曼流形。我们用三种基于自蒸馏的训练方法实例化 GFM：广义拉格朗日流图、广义欧拉流图和广义渐进流图。我们从理论上证明，GFM 在特定的设计决策下，将现有的欧几里得少步生成模型（例如一致性模型、快捷模型和平均流）统一并提升到黎曼设置。我们在一系列几何数据集（包括地理空间数据、RNA 扭转角和双曲流形）上将 GFM 与其他几何生成模型进行基准测试，并使用隐式概率流实现单步和少步评估的最先进的样本质量以及卓越或有竞争力的对数似然。</li>
</ul>

<h3>Title: Generative Correlation Manifolds: Generating Synthetic Data with Preserved Higher-Order Correlations</h3>
<ul>
<li><strong>Authors: </strong>Jens E. d'Hondt, Wieger R. Punter, Odysseas Papapetrou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21610">https://arxiv.org/abs/2510.21610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21610">https://arxiv.org/pdf/2510.21610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21610]] Generative Correlation Manifolds: Generating Synthetic Data with Preserved Higher-Order Correlations(https://arxiv.org/abs/2510.21610)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The increasing need for data privacy and the demand for robust machine learning models have fueled the development of synthetic data generation techniques. However, current methods often succeed in replicating simple summary statistics but fail to preserve both the pairwise and higher-order correlation structure of the data that define the complex, multi-variable interactions inherent in real-world systems. This limitation can lead to synthetic data that is superficially realistic but fails when used for sophisticated modeling tasks. In this white paper, we introduce Generative Correlation Manifolds (GCM), a computationally efficient method for generating synthetic data. The technique uses Cholesky decomposition of a target correlation matrix to produce datasets that, by mathematical proof, preserve the entire correlation structure -- from simple pairwise relationships to higher-order interactions -- of the source dataset. We argue that this method provides a new approach to synthetic data generation with potential applications in privacy-preserving data sharing, robust model training, and simulation.</li>
<li><strong>摘要：</strong>对数据隐私日益增长的需求和对强大机器学习模型的需求推动了合成数据生成技术的发展。然而，当前的方法通常可以成功地复制简单的汇总统计数据，但无法保留数据的成对和高阶相关结构，这些结构定义了现实系统中固有的复杂、多变量交互。这种限制可能会导致合成数据表面上很真实，但在用于复杂的建模任务时却会失败。在本白皮书中，我们介绍了生成相关流形 (GCM)，这是一种用于生成合成数据的计算高效方法。该技术使用目标相关矩阵的 Cholesky 分解来生成数据集，通过数学证明，这些数据集保留了源数据集的整个相关结构（从简单的成对关系到高阶交互）。我们认为，这种方法提供了一种新的合成数据生成方法，在隐私保护数据共享、鲁棒模型训练和模拟方面具有潜在的应用。</li>
</ul>

<h3>Title: Epipolar Geometry Improves Video Generation Models</h3>
<ul>
<li><strong>Authors: </strong>Orest Kupyn, Fabian Manhardt, Federico Tombari, Christian Rupprecht</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21615">https://arxiv.org/abs/2510.21615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21615">https://arxiv.org/pdf/2510.21615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21615]] Epipolar Geometry Improves Video Generation Models(https://arxiv.org/abs/2510.21615)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Video generation models have progressed tremendously through large latent diffusion transformers trained with rectified flow techniques. Yet these models still struggle with geometric inconsistencies, unstable motion, and visual artifacts that break the illusion of realistic 3D scenes. 3D-consistent video generation could significantly impact numerous downstream applications in generation and reconstruction tasks. We explore how epipolar geometry constraints improve modern video diffusion models. Despite massive training data, these models fail to capture fundamental geometric principles underlying visual content. We align diffusion models using pairwise epipolar geometry constraints via preference-based optimization, directly addressing unstable camera trajectories and geometric artifacts through mathematically principled geometric enforcement. Our approach efficiently enforces geometric principles without requiring end-to-end differentiability. Evaluation demonstrates that classical geometric constraints provide more stable optimization signals than modern learned metrics, which produce noisy targets that compromise alignment quality. Training on static scenes with dynamic cameras ensures high-quality measurements while the model generalizes effectively to diverse dynamic content. By bridging data-driven deep learning with classical geometric computer vision, we present a practical method for generating spatially consistent videos without compromising visual quality.</li>
<li><strong>摘要：</strong>通过使用整流流技术训练的大型潜在扩散变压器，视频生成模型取得了巨大进步。然而，这些模型仍然面临着几何不一致、不稳定运动和破坏真实 3D 场景幻觉的视觉伪影的问题。 3D 一致的视频生成可能会对生成和重建任务中的众多下游应用产生重大影响。我们探索对极几何约束如何改进现代视频扩散模型。尽管有大量的训练数据，这些模型仍无法捕捉视觉内容背后的基本几何原理。我们通过基于偏好的优化，使用成对极几何约束来对齐扩散模型，通过数学原理的几何执行直接解决不稳定的相机轨迹和几何伪影。我们的方法有效地执行几何原理，而不需要端到端的可微性。评估表明，经典几何约束提供比现代学习指标更稳定的优化信号，现代学习指标会产生噪声目标，从而损害对齐质量。使用动态相机对静态场景进行训练可确保高质量的测量，同时模型可以有效地推广到各种动态内容。通过将数据驱动的深度学习与经典几何计算机视觉结合起来，我们提出了一种实用方法，可以在不影响视觉质量的情况下生成空间一致的视频。</li>
</ul>

<h3>Title: Foundation Models in Dermatopathology: Skin Tissue Classification</h3>
<ul>
<li><strong>Authors: </strong>Riya Gupta, Yiwei Zong, Dennis H. Murphree</a></li>
<li><strong>Subjects: </strong>cs.CV, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21664">https://arxiv.org/abs/2510.21664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21664">https://arxiv.org/pdf/2510.21664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21664]] Foundation Models in Dermatopathology: Skin Tissue Classification(https://arxiv.org/abs/2510.21664)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The rapid generation of whole-slide images (WSIs) in dermatopathology necessitates automated methods for efficient processing and accurate classification. This study evaluates the performance of two foundation models, UNI and Virchow2, as feature extractors for classifying WSIs into three diagnostic categories: melanocytic, basaloid, and squamous lesions. Patch-level embeddings were aggregated into slide-level features using a mean-aggregation strategy and subsequently used to train multiple machine learning classifiers, including logistic regression, gradient-boosted trees, and random forest models. Performance was assessed using precision, recall, true positive rate, false positive rate, and the area under the receiver operating characteristic curve (AUROC) on the test set. Results demonstrate that patch-level features extracted using Virchow2 outperformed those extracted via UNI across most slide-level classifiers, with logistic regression achieving the highest accuracy (90%) for Virchow2, though the difference was not statistically significant. The study also explored data augmentation techniques and image normalization to enhance model robustness and generalizability. The mean-aggregation approach provided reliable slide-level feature representations. All experimental results and metrics were tracked and visualized using this http URL, facilitating reproducibility and interpretability. This research highlights the potential of foundation models for automated WSI classification, providing a scalable and effective approach for dermatopathological diagnosis while paving the way for future advancements in slide-level representation learning.</li>
<li><strong>摘要：</strong>皮肤病理学中全玻片图像 (WSI) 的快速生成需要自动化方法来进行高效处理和准确分类。本研究评估了两种基础模型 UNI 和 Virchow2 的性能，作为将 WSI 分为三个诊断类别的特征提取器：黑素细胞病变、基底样病变和鳞状病变。使用均值聚合策略将补丁级嵌入聚合为幻灯片级特征，随后用于训练多个机器学习分类器，包括逻辑回归、梯度增强树和随机森林模型。使用测试集上的精确率、召回率、真阳性率、假阳性率和受试者工作特征曲线下面积 (AUROC) 来评估性能。结果表明，在大多数滑动级分类器中，使用 Virchow2 提取的补丁级特征优于通过 UNI 提取的特征，其中逻辑回归为 Virchow2 实现了最高的准确率 (90%)，但差异并不具有统计显着性。该研究还探索了数据增强技术和图像归一化，以增强模型的稳健性和泛化性。均值聚合方法提供了可靠的幻灯片级特征表示。所有实验结果和指标均使用此 http URL 进行跟踪和可视化，从而促进可重复性和可解释性。这项研究强调了自动 WSI 分类基础模型的潜力，为皮肤病理学诊断提供了一种可扩展且有效的方法，同时为幻灯片级表示学习的未来进步铺平了道路。</li>
</ul>

<h3>Title: WorldGrow: Generating Infinite 3D World</h3>
<ul>
<li><strong>Authors: </strong>Sikuang Li, Chen Yang, Jiemin Fang, Taoran Yi, Jia Lu, Jiazhong Cen, Lingxi Xie, Wei Shen, Qi Tian</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21682">https://arxiv.org/abs/2510.21682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21682">https://arxiv.org/pdf/2510.21682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21682]] WorldGrow: Generating Infinite 3D World(https://arxiv.org/abs/2510.21682)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We tackle the challenge of generating the infinitely extendable 3D world -- large, continuous environments with coherent geometry and realistic appearance. Existing methods face key challenges: 2D-lifting approaches suffer from geometric and appearance inconsistencies across views, 3D implicit representations are hard to scale up, and current 3D foundation models are mostly object-centric, limiting their applicability to scene-level generation. Our key insight is leveraging strong generation priors from pre-trained 3D models for structured scene block generation. To this end, we propose WorldGrow, a hierarchical framework for unbounded 3D scene synthesis. Our method features three core components: (1) a data curation pipeline that extracts high-quality scene blocks for training, making the 3D structured latent representations suitable for scene generation; (2) a 3D block inpainting mechanism that enables context-aware scene extension; and (3) a coarse-to-fine generation strategy that ensures both global layout plausibility and local geometric/textural fidelity. Evaluated on the large-scale 3D-FRONT dataset, WorldGrow achieves SOTA performance in geometry reconstruction, while uniquely supporting infinite scene generation with photorealistic and structurally consistent outputs. These results highlight its capability for constructing large-scale virtual environments and potential for building future world models.</li>
<li><strong>摘要：</strong>我们应对生成无限可扩展的 3D 世界的挑战 - 具有连贯几何形状和逼真外观的大型连续环境。现有方法面临关键挑战：2D 提升方法面临着不同视图之间几何和外观不一致的问题，3D 隐式表示难以扩展，当前的 3D 基础模型大多以对象为中心，限制了它们在场景级生成中的适用性。我们的主要见解是利用预训练 3D 模型的强大生成先验来生成结构化场景块。为此，我们提出了 WorldGrow，一个用于无界 3D 场景合成的分层框架。我们的方法具有三个核心组件：（1）数据管理管道，提取高质量场景块进行训练，使 3D 结构化潜在表示适合场景生成； (2) 3D 块修复机制，支持上下文感知场景扩展； (3)从粗到细的生成策略，确保全局布局的合理性和局部几何/纹理的保真度。在大规模 3D-FRONT 数据集上进行评估，WorldGrow 在几何重建方面实现了 SOTA 性能，同时独特地支持无限场景生成，具有逼真且结构一致的输出。这些结果凸显了其构建大规模虚拟环境的能力和构建未来世界模型的潜力。</li>
</ul>

<h3>Title: BachVid: Training-Free Video Generation with Consistent Background and Character</h3>
<ul>
<li><strong>Authors: </strong>Han Yan, Xibin Song, Yifu Wang, Hongdong Li, Pan Ji, Chao Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21696">https://arxiv.org/abs/2510.21696</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21696">https://arxiv.org/pdf/2510.21696</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21696]] BachVid: Training-Free Video Generation with Consistent Background and Character(https://arxiv.org/abs/2510.21696)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion Transformers (DiTs) have recently driven significant progress in text-to-video (T2V) generation. However, generating multiple videos with consistent characters and backgrounds remains a significant challenge. Existing methods typically rely on reference images or extensive training, and often only address character consistency, leaving background consistency to image-to-video models. We introduce BachVid, the first training-free method that achieves consistent video generation without needing any reference images. Our approach is based on a systematic analysis of DiT's attention mechanism and intermediate features, revealing its ability to extract foreground masks and identify matching points during the denoising process. Our method leverages this finding by first generating an identity video and caching the intermediate variables, and then inject these cached variables into corresponding positions in newly generated videos, ensuring both foreground and background consistency across multiple videos. Experimental results demonstrate that BachVid achieves robust consistency in generated videos without requiring additional training, offering a novel and efficient solution for consistent video generation without relying on reference images or additional training.</li>
<li><strong>摘要：</strong>Diffusion Transformers (DiT) 最近推动了文本到视频 (T2V) 生成的重大进展。然而，生成具有一致的角色和背景的多个视频仍然是一个重大挑战。现有方法通常依赖于参考图像或广泛的训练，并且通常仅解决字符一致性问题，而将背景一致性留给图像到视频模型。我们推出了 BachVid，这是第一个免训练方法，无需任何参考图像即可实现一致的视频生成。我们的方法基于对 DiT 注意力机制和中间特征的系统分析，揭示了其在去噪过程中提取前景掩模和识别匹配点的能力。我们的方法利用这一发现，首先生成身份视频并缓存中间变量，然后将这些缓存的变量注入到新生成的视频中的相应位置，确保多个视频的前景和背景一致性。实验结果表明，BachVid 在生成的视频中实现了鲁棒的一致性，无需额外的训练，为一致的视频生成提供了一种新颖且高效的解决方案，无需依赖参考图像或额外的训练。</li>
</ul>

<h3>Title: Visual Diffusion Models are Geometric Solvers</h3>
<ul>
<li><strong>Authors: </strong>Nir Goren, Shai Yehezkel, Omer Dahary, Andrey Voynov, Or Patashnik, Daniel Cohen-Or</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21697">https://arxiv.org/abs/2510.21697</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21697">https://arxiv.org/pdf/2510.21697</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21697]] Visual Diffusion Models are Geometric Solvers(https://arxiv.org/abs/2510.21697)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In this paper we show that visual diffusion models can serve as effective geometric solvers: they can directly reason about geometric problems by working in pixel space. We first demonstrate this on the Inscribed Square Problem, a long-standing problem in geometry that asks whether every Jordan curve contains four points forming a square. We then extend the approach to two other well-known hard geometric problems: the Steiner Tree Problem and the Simple Polygon Problem. Our method treats each problem instance as an image and trains a standard visual diffusion model that transforms Gaussian noise into an image representing a valid approximate solution that closely matches the exact one. The model learns to transform noisy geometric structures into correct configurations, effectively recasting geometric reasoning as image generation. Unlike prior work that necessitates specialized architectures and domain-specific adaptations when applying diffusion to parametric geometric representations, we employ a standard visual diffusion model that operates on the visual representation of the problem. This simplicity highlights a surprising bridge between generative modeling and geometric problem solving. Beyond the specific problems studied here, our results point toward a broader paradigm: operating in image space provides a general and practical framework for approximating notoriously hard problems, and opens the door to tackling a far wider class of challenging geometric tasks.</li>
<li><strong>摘要：</strong>在本文中，我们证明视觉扩散模型可以作为有效的几何求解器：它们可以通过在像素空间中工作直接推理几何问题。我们首先在内接正方形问题上证明这一点，这是一个长期存在的几何问题，该问题询问是否每条乔丹曲线都包含形成正方形的四个点。然后，我们将该方法扩展到另外两个著名的硬几何问题：斯坦纳树问题和简单多边形问题。我们的方法将每个问题实例视为一幅图像，并训练一个标准视觉扩散模型，该模型将高斯噪声转换为代表与精确匹配的有效近似解的图像。该模型学习将噪声几何结构转换为正确的配置，有效地将几何推理重塑为图像生成。与之前的工作不同，在将扩散应用于参数几何表示时需要专门的架构和特定领域的适应，我们采用了一个标准的视觉扩散模型，该模型对问题的视觉表示进行操作。这种简单性凸显了生成建模和几何问题解决之间令人惊讶的桥梁。除了这里研究的具体问题之外，我们的结果还指向更广泛的范式：在图像空间中进行操作提供了一个通用且实用的框架来近似众所周知的难题，并为解决更广泛的具有挑战性的几何任务打开了大门。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
