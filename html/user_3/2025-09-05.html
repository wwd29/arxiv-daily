<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-05</h1>
<h3>Title: Towards Efficient General Feature Prediction in Masked Skeleton Modeling</h3>
<ul>
<li><strong>Authors: </strong>Shengkai Sun, Zefan Zhang, Jianfeng Dong, Zhiyong Cheng, Xiaojun Chang, Meng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03609">https://arxiv.org/abs/2509.03609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03609">https://arxiv.org/pdf/2509.03609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03609]] Towards Efficient General Feature Prediction in Masked Skeleton Modeling(https://arxiv.org/abs/2509.03609)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in the masked autoencoder (MAE) paradigm have significantly propelled self-supervised skeleton-based action recognition. However, most existing approaches limit reconstruction targets to raw joint coordinates or their simple variants, resulting in computational redundancy and limited semantic representation. To address this, we propose a novel General Feature Prediction framework (GFP) for efficient mask skeleton modeling. Our key innovation is replacing conventional low-level reconstruction with high-level feature prediction that spans from local motion patterns to global semantic representations. Specifically, we introduce a collaborative learning framework where a lightweight target generation network dynamically produces diversified supervision signals across spatial-temporal hierarchies, avoiding reliance on pre-computed offline features. The framework incorporates constrained optimization to ensure feature diversity while preventing model collapse. Experiments on NTU RGB+D 60, NTU RGB+D 120 and PKU-MMD demonstrate the benefits of our approach: Computational efficiency (with 6.2$\times$ faster training than standard masked skeleton modeling methods) and superior representation quality, achieving state-of-the-art performance in various downstream tasks.</li>
<li><strong>摘要：</strong>蒙面自动编码器（MAE）范式的最新进展显着推动了基于自我监视的骨架识别。但是，大多数现有方法将重建目标限制为原始的关节坐标或其简单变体，从而导致计算冗余和有限的语义表示。为了解决这个问题，我们提出了一个新型的一般特征预测框架（GFP），以实现有效的掩模骨骼建模。我们的关键创新是用高级特征预测替换传统的低级重建，从本地运动模式到全球语义表示。具体来说，我们介绍了一个协作学习框架，轻巧的目标生成网络在空间阶段的层次结构上动态产生多样化的监督信号，从而避免依赖预先计算的离线功能。该框架结合了受限的优化，以确保特征多样性，同时防止模型崩溃。 NTU RGB+D 60，NTU RGB+D 120和PKU-MMD进行的实验证明了我们方法的好处：计算效率（6.2 $ \ times $ $ \ times $比标准掩盖的骨架建模方法更快）和出色的表示质量，可以在各种下游任务中实现您的现实表现。</li>
</ul>

<h3>Title: treeX: Unsupervised Tree Instance Segmentation in Dense Forest Point Clouds</h3>
<ul>
<li><strong>Authors: </strong>Josafat-Mattias Burmeister, Andreas Tockner, Stefan Reder, Markus Engel, Rico Richter, Jan-Peter Mund, Jürgen Döllner</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03633">https://arxiv.org/abs/2509.03633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03633">https://arxiv.org/pdf/2509.03633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03633]] treeX: Unsupervised Tree Instance Segmentation in Dense Forest Point Clouds(https://arxiv.org/abs/2509.03633)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Close-range laser scanning provides detailed 3D captures of forest stands but requires efficient software for processing 3D point cloud data and extracting individual trees. Although recent studies have introduced deep learning methods for tree instance segmentation, these approaches require large annotated datasets and substantial computational resources. As a resource-efficient alternative, we present a revised version of the treeX algorithm, an unsupervised method that combines clustering-based stem detection with region growing for crown delineation. While the original treeX algorithm was developed for personal laser scanning (PLS) data, we provide two parameter presets, one for ground-based laser scanning (stationary terrestrial - TLS and PLS), and one for UAV-borne laser scanning (ULS). We evaluated the method on six public datasets (FOR-instance, ForestSemantic, LAUTx, NIBIO MLS, TreeLearn, Wytham Woods) and compared it to six open-source methods (original treeX, treeiso, RayCloudTools, ForAINet, SegmentAnyTree, TreeLearn). Compared to the original treeX algorithm, our revision reduces runtime and improves accuracy, with instance detection F$_1$-score gains of +0.11 to +0.49 for ground-based data. For ULS data, our preset achieves an F$_1$-score of 0.58, whereas the original algorithm fails to segment any correct instances. For TLS and PLS data, our algorithm achieves accuracy similar to recent open-source methods, including deep learning. Given its algorithmic design, we see two main applications for our method: (1) as a resource-efficient alternative to deep learning approaches in scenarios where the data characteristics align with the method design (sufficient stem visibility and point density), and (2) for the semi-automatic generation of labels for deep learning models. To enable broader adoption, we provide an open-source Python implementation in the pointtree package.</li>
<li><strong>摘要：</strong>近距离激光扫描提供了详细的3D捕获森林支架的捕获，但需要有效的软件来处理3D点云数据并提取单个树。尽管最近的研究介绍了树木实例细分的深度学习方法，但这些方法需要大量注释的数据集和大量的计算资源。作为资源有效的替代方案，我们提出了TWERX算法的修订版，这是一种无监督的方法，将基于聚类的STEM检测与皇冠描述的区域增长相结合。虽然开发了用于个人激光扫描（PLS）数据的原始TEERX算法，但我们提供了两个参数预设，一种用于地面激光扫描（固定地面-TLS和PLS），一个用于无人用的激光扫描（ULS）。我们评估了六个公共数据集（用于现实，森林语义，Lautx，Nibio MLS，Treelearn，Wytham Woods）的方法，并将其与六种开源方法（原始Treex，Treex，Raycloudtools，Raycloudtools，Forainet，Forainet，f​​orainet，segmentantree，treelearn）进行了比较。与原始的TEERX算法相比，我们的修订降低了运行时并提高了准确性，实例检测F $ _1 $  - 分数的增长率为+0.11至+0.49，用于地面数据。对于ULS数据，我们的预设达到0.58的f $ _1 $分数，而原始算法未能分割任何正确的实例。对于TLS和PLS数据，我们的算法达到了与最近的开源方法相似的精度，包括深度学习。鉴于其算法设计，我们看到了我们方法的两个主要应用程序：（1）作为在方案中的深度学习方法的一种资源效率替代方案，其中数据特征与方法设计保持一致（足够的STEM可见性和点密度），以及（2）对深度学习模型的半自动化生成标签。为了实现更广泛的采用，我们在PointTree软件包中提供了开源Python实现。</li>
</ul>

<h3>Title: CEHR-GPT: A Scalable Multi-Task Foundation Model for Electronic Health Records</h3>
<ul>
<li><strong>Authors: </strong>Chao Pang, Jiheum Park, Xinzhuo Jiang, Nishanth Parameshwar Pavinkurve, Krishna S. Kalluri, Shalmali Joshi, Noémie Elhadad, Karthik Natarajan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03643">https://arxiv.org/abs/2509.03643</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03643">https://arxiv.org/pdf/2509.03643</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03643]] CEHR-GPT: A Scalable Multi-Task Foundation Model for Electronic Health Records(https://arxiv.org/abs/2509.03643)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Electronic Health Records (EHRs) provide a rich, longitudinal view of patient health and hold significant potential for advancing clinical decision support, risk prediction, and data-driven healthcare research. However, most artificial intelligence (AI) models for EHRs are designed for narrow, single-purpose tasks, limiting their generalizability and utility in real-world settings. Here, we present CEHR-GPT, a general-purpose foundation model for EHR data that unifies three essential capabilities - feature representation, zero-shot prediction, and synthetic data generation - within a single architecture. To support temporal reasoning over clinical sequences, \cehrgpt{} incorporates a novel time-token-based learning framework that explicitly encodes patients' dynamic timelines into the model structure. CEHR-GPT demonstrates strong performance across all three tasks and generalizes effectively to external datasets through vocabulary expansion and fine-tuning. Its versatility enables rapid model development, cohort discovery, and patient outcome forecasting without the need for task-specific retraining.</li>
<li><strong>摘要：</strong>电子健康记录（EHRS）提供了对患者健康的丰富，纵向的看法，并具有推进临床决策支持，风险预测和数据驱动的医疗保健研究的巨大潜力。但是，大多数针对EHR的人工智能（AI）模型都是为狭窄的单用途任务而设计的，从而限制了它们在现实环境中的普遍性和实用性。在这里，我们提出了CEHR-GPT，这是一个在单个体系结构中统一三个基本功能的EHR数据的通用基础模型 - 特征表示，零摄像预测和合成数据生成。为了支持临床序列上的时间推理，\ cehrgpt {}结合了一个新型的基于时间的学习框架，该框架将患者的动态时间表明确地编码到模型结构中。 CEHR-GPT在这三个任务中表现出强大的性能，并通过词汇扩展和微调有效地将其推广到外部数据集。它的多功能性使得无需特定于任务的重新培训即可快速模型开发，队列发现和患者结果预测。</li>
</ul>

<h3>Title: AutoGrid AI: Deep Reinforcement Learning Framework for Autonomous Microgrid Management</h3>
<ul>
<li><strong>Authors: </strong>Kenny Guo, Nicholas Eckhert, Krish Chhajer, Luthira Abeykoon, Lorne Schell</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03666">https://arxiv.org/abs/2509.03666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03666">https://arxiv.org/pdf/2509.03666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03666]] AutoGrid AI: Deep Reinforcement Learning Framework for Autonomous Microgrid Management(https://arxiv.org/abs/2509.03666)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present a deep reinforcement learning-based framework for autonomous microgrid management. tailored for remote communities. Using deep reinforcement learning and time-series forecasting models, we optimize microgrid energy dispatch strategies to minimize costs and maximize the utilization of renewable energy sources such as solar and wind. Our approach integrates the transformer architecture for forecasting of renewable generation and a proximal-policy optimization (PPO) agent to make decisions in a simulated environment. Our experimental results demonstrate significant improvements in both energy efficiency and operational resilience when compared to traditional rule-based methods. This work contributes to advancing smart-grid technologies in pursuit of zero-carbon energy systems. We finally provide an open-source framework for simulating several microgrid environments.</li>
<li><strong>摘要：</strong>我们提出了一个基于自主微电网管理的基于强化学习的框架。为偏远社区量身定制。使用深度强化学习和时间序列的预测模型，我们优化了微电网能源调度策略，以最大程度地降低成本并最大程度地利用可再生能源（例如太阳能和风能）。我们的方法集成了变压器体系结构，以预测可再生生成和近端政策优化（PPO）代理，以在模拟环境中做出决策。与传统的基于规则的方法相比，我们的实验结果表明，能源效率和运营弹性的显着提高。这项工作有助于推进智能碳能源系统的智能网格技术。我们最终提供了一个开源框架，用于模拟几个微电网环境。</li>
</ul>

<h3>Title: Learning an Adversarial World Model for Automated Curriculum Generation in MARL</h3>
<ul>
<li><strong>Authors: </strong>Brennen Hill</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03771">https://arxiv.org/abs/2509.03771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03771">https://arxiv.org/pdf/2509.03771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03771]] Learning an Adversarial World Model for Automated Curriculum Generation in MARL(https://arxiv.org/abs/2509.03771)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>World models that infer and predict environmental dynamics are foundational to embodied intelligence. However, their potential is often limited by the finite complexity and implicit biases of hand-crafted training environments. To develop truly generalizable and robust agents, we need environments that scale in complexity alongside the agents learning within them. In this work, we reframe the challenge of environment generation as the problem of learning a goal-conditioned, generative world model. We propose a system where a generative **Attacker** agent learns an implicit world model to synthesize increasingly difficult challenges for a team of cooperative **Defender** agents. The Attacker's objective is not passive prediction, but active, goal-driven interaction: it models and generates world states (i.e., configurations of enemy units) specifically to exploit the Defenders' weaknesses. Concurrently, the embodied Defender team learns a cooperative policy to overcome these generated worlds. This co-evolutionary dynamic creates a self-scaling curriculum where the world model continuously adapts to challenge the decision-making policy of the agents, providing an effectively infinite stream of novel and relevant training scenarios. We demonstrate that this framework leads to the emergence of complex behaviors, such as the world model learning to generate flanking and shielding formations, and the defenders learning coordinated focus-fire and spreading tactics. Our findings position adversarial co-evolution as a powerful method for learning instrumental world models that drive agents toward greater strategic depth and robustness.</li>
<li><strong>摘要：</strong>推断和预测环境动态的世界模型是体现智能的基础。但是，它们的潜力通常受到手工训练环境的有限复杂性和隐性偏见的限制。为了开发真正的概括和健壮的代理，我们需要与内部学习的代理人一起进行复杂性扩展的环境。在这项工作中，我们将环境生成的挑战重新构想为学习目标条件，生成的世界模型的问题。我们提出了一个系统，即生成**攻击者**代理商学习一个隐式世界模型，以综合合作**辩护人**代理人的越来越困难的挑战。攻击者的目标不是被动预测，而是主动的，目标驱动的互动：它建模并生成世界状态（即敌方单位的配置），以利用捍卫者的弱点。同时，体现的后卫小组学习了一项合作政策，以克服这些产生的世界。这种共同进化的动态创建了一个自我缩放的课程，世界模型不断适应挑战代理商的决策政策，提供了有效的新颖和相关培训方案的无限流。我们证明，该框架会导致复杂行为的出现，例如世界模型学习以产生侧翼和屏蔽形成，而捍卫者学习了协调的焦点和传播策略。我们的发现位置对抗共同进化是一种学习工具世界模型的有力方法，可以推动代理人迈向更大的战略深度和鲁棒性。</li>
</ul>

<h3>Title: Fitting Image Diffusion Models on Video Datasets</h3>
<ul>
<li><strong>Authors: </strong>Juhun Lee, Simon S. Woo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03794">https://arxiv.org/abs/2509.03794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03794">https://arxiv.org/pdf/2509.03794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03794]] Fitting Image Diffusion Models on Video Datasets(https://arxiv.org/abs/2509.03794)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Image diffusion models are trained on independently sampled static images. While this is the bedrock task protocol in generative modeling, capturing the temporal world through the lens of static snapshots is information-deficient by design. This limitation leads to slower convergence, limited distributional coverage, and reduced generalization. In this work, we propose a simple and effective training strategy that leverages the temporal inductive bias present in continuous video frames to improve diffusion training. Notably, the proposed method requires no architectural modification and can be seamlessly integrated into standard diffusion training pipelines. We evaluate our method on the HandCo dataset, where hand-object interactions exhibit dense temporal coherence and subtle variations in finger articulation often result in semantically distinct motions. Empirically, our method accelerates convergence by over 2$\text{x}$ faster and achieves lower FID on both training and validation distributions. It also improves generative diversity by encouraging the model to capture meaningful temporal variations. We further provide an optimization analysis showing that our regularization reduces the gradient variance, which contributes to faster convergence.</li>
<li><strong>摘要：</strong>图像扩散模型对独立采样的静态图像进行训练。虽然这是生成建模中的基岩任务协议，但通过静态快照捕获时间世界是通过设计信息缺失的。这种局限性导致收敛速度较慢，分布覆盖率有限和概括减少。在这项工作中，我们提出了一种简单有效的训练策略，该策略利用连续视频框架中存在的时间感应偏见来改善扩散训练。值得注意的是，所提出的方法不需要架构修改，并且可以无缝集成到标准扩散训练管道中。我们在Handco数据集上评估了我们的方法，其中手对象相互作用表现出密集的时间连贯性和指关节的细微变化通常会导致语义上不同的动作。从经验上讲，我们的方法将收敛加速超过2 $ \ text {x} $加快速度，并在培训和验证分布中实现较低的FID。它还通过鼓励模型捕获有意义的时间变化来改善生成性多样性。我们进一步提供了优化分析，表明我们的正则化降低了梯度方差，这有助于更快的收敛。</li>
</ul>

<h3>Title: EGTM: Event-guided Efficient Turbulence Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Huanan Li, Rui Fan, Juntao Guan, Weidong Hao, Lai Rui, Tong Wu, Yikai Wang, Lin Gu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03808">https://arxiv.org/abs/2509.03808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03808">https://arxiv.org/pdf/2509.03808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03808]] EGTM: Event-guided Efficient Turbulence Mitigation(https://arxiv.org/abs/2509.03808)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Turbulence mitigation (TM) aims to remove the stochastic distortions and blurs introduced by atmospheric turbulence into frame cameras. Existing state-of-the-art deep-learning TM methods extract turbulence cues from multiple degraded frames to find the so-called "lucky'', not distorted patch, for "lucky fusion''. However, it requires high-capacity network to learn from coarse-grained turbulence dynamics between synchronous frames with limited frame-rate, thus fall short in computational and storage efficiency. Event cameras, with microsecond-level temporal resolution, have the potential to fundamentally address this bottleneck with efficient sparse and asynchronous imaging mechanism. In light of this, we (i) present the fundamental \textbf{``event-lucky insight''} to reveal the correlation between turbulence distortions and inverse spatiotemporal distribution of event streams. Then, build upon this insight, we (ii) propose a novel EGTM framework that extracts pixel-level reliable turbulence-free guidance from the explicit but noisy turbulent events for temporal lucky fusion. Moreover, we (iii) build the first turbulence data acquisition system to contribute the first real-world event-driven TM dataset. Extensive experimental results demonstrate that our approach significantly surpass the existing SOTA TM method by 710 times, 214 times and 224 times in model size, inference latency and model complexity respectively, while achieving the state-of-the-art in restoration quality (+0.94 PSNR and +0.08 SSIM) on our real-world EGTM dataset. This demonstrating the great efficiency merit of introducing event modality into TM task. Demo code and data have been uploaded in supplementary material and will be released once accepted.</li>
<li><strong>摘要：</strong>湍流缓解（TM）旨在消除大气湍流中引入的随机变形和模糊。现有的最先进的深度学习TM方法从多个退化的框架中提取湍流提示，以找到所谓的“幸运”而不是扭曲的补丁，以“幸运的融合”。但是，它要求高容量网络从帧速率有限的同步帧之间的粗粒湍流动力学中学习，从而在计算和存储效率方面缺乏。带有微秒级时间分辨率的事件摄像机具有从根本上以有效的稀疏和异步成像机制来解决这种瓶颈的潜力。鉴于此，我们（i）介绍了基本\ textbf {````````````event））））））然后，基于这种见识，我们（ii）提出了一个新型的EGTM框架，该框架从显式但嘈杂的湍流事件中提取了像素级可靠的无湍流指导，以实现暂时的幸运融合。此外，我们（iii）构建了第一个湍流数据采集系统，以贡献第一个现实世界事件驱动的TM数据集。广泛的实验结果表明，我们的方法分别在模型大小，推理潜伏期和模型复杂性上显着超过了710次，214次和224次，同时在我们的现实World EGTM数据集中实现了恢复质量的最新恢复质量（+0.94 PSNR和+0.08 SSIM）。这表明将事件模式引入TM任务的效率很高。演示代码和数据已在补充材料中上传，并且一旦接受就会发布。</li>
</ul>

<h3>Title: Machine Learning for LiDAR-Based Indoor Surface Classification in Intelligent Wireless Environments</h3>
<ul>
<li><strong>Authors: </strong>Parth Ashokbhai Shiroya, Swarnagowri Shashidhar, Amod Ashtekar, Krishna Aindrila Kar, Rafaela Lomboy, Dalton Davis, Mohammed E. Eltayeb</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03813">https://arxiv.org/abs/2509.03813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03813">https://arxiv.org/pdf/2509.03813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03813]] Machine Learning for LiDAR-Based Indoor Surface Classification in Intelligent Wireless Environments(https://arxiv.org/abs/2509.03813)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reliable connectivity in millimeter-wave (mmWave) and sub-terahertz (sub-THz) networks depends on reflections from surrounding surfaces, as high-frequency signals are highly vulnerable to blockage. The scattering behavior of a surface is determined not only by material permittivity but also by roughness, which governs whether energy remains in the specular direction or is diffusely scattered. This paper presents a LiDAR-driven machine learning framework for classifying indoor surfaces into semi-specular and low-specular categories, using optical reflectivity as a proxy for electromagnetic scattering behavior. A dataset of over 78,000 points from 15 representative indoor materials was collected and partitioned into 3 cm x 3 cm patches to enable classification from partial views. Patch-level features capturing geometry and intensity, including elevation angle, natural-log-scaled intensity, and max-to-mean ratio, were extracted and used to train Random Forest, XGBoost, and neural network classifiers. Results show that ensemble tree-based models consistently provide the best trade-off between accuracy and robustness, confirming that LiDAR-derived features capture roughness-induced scattering effects. The proposed framework enables the generation of scatter aware environment maps and digital twins, supporting adaptive beam management, blockage recovery, and environment-aware connectivity in next-generation networks.</li>
<li><strong>摘要：</strong>毫米波（MMWAVE）和子terahertz（Sub-Thz）网络中的可靠连通性取决于周围表面的反射，因为高频信号非常容易受到阻塞的影响。表面的散射行为不仅取决于材料介电常数，而且还取决于粗糙度，这决定了能量是否保持在镜面方向或扩散散射。本文提出了一种通过光学反射率作为电磁散射行为的代理，将激光驱动的机器学习框架分类为半特定和低特征类别的室内表面。收集了15个代表性室内材料的78,000点的数据集并将其分配到3厘米x 3厘米的补丁中，以从部分视图中启用分类。提取贴片级特征，这些几何形状和强度，包括高程角，天然量表的强度以及最大值比率，并用于训练随机森林，XGBOOST和神经网络分类器。结果表明，基于整体树的模型始终提供准确性和鲁棒性之间的最佳权衡，证实了激光雷达衍生的特征捕获粗糙度引起的散射效果。所提出的框架可以生成散射感知的环境图和数字双胞胎，从而支持下一代网络中的自适应束管理，阻塞恢复和环境感知的连接。</li>
</ul>

<h3>Title: Human Motion Video Generation: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Haiwei Xue, Xiangyang Luo, Zhanghao Hu, Xin Zhang, Xunzhi Xiang, Yuqin Dai, Jianzhuang Liu, Zhensong Zhang, Minglei Li, Jian Yang, Fei Ma, Zhiyong Wu, Changpeng Yang, Zonghong Dai, Fei Richard Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03883">https://arxiv.org/abs/2509.03883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03883">https://arxiv.org/pdf/2509.03883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03883]] Human Motion Video Generation: A Survey(https://arxiv.org/abs/2509.03883)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Human motion video generation has garnered significant research interest due to its broad applications, enabling innovations such as photorealistic singing heads or dynamic avatars that seamlessly dance to music. However, existing surveys in this field focus on individual methods, lacking a comprehensive overview of the entire generative process. This paper addresses this gap by providing an in-depth survey of human motion video generation, encompassing over ten sub-tasks, and detailing the five key phases of the generation process: input, motion planning, motion video generation, refinement, and output. Notably, this is the first survey that discusses the potential of large language models in enhancing human motion video generation. Our survey reviews the latest developments and technological trends in human motion video generation across three primary modalities: vision, text, and audio. By covering over two hundred papers, we offer a thorough overview of the field and highlight milestone works that have driven significant technological breakthroughs. Our goal for this survey is to unveil the prospects of human motion video generation and serve as a valuable resource for advancing the comprehensive applications of digital humans. A complete list of the models examined in this survey is available in Our Repository this https URL.</li>
<li><strong>摘要：</strong>由于其广泛的应用，人类运动视频的产生引起了重大的研究兴趣，从而实现了诸如逼真的唱歌头或动态化的化身之类的创新，这些化身无缝地跳舞音乐。但是，该领域的现有调查集中在各个方法上，缺乏对整个生成过程的全面概述。本文通过提供对人类运动视频生成的深入调查，包括超过十个子任务，并详细介绍发电过程的五个关键阶段：输入，运动计划，运动视频生成，改进和输出。值得注意的是，这是第一次讨论大型语言模型增强人类运动视频生成的潜力的调查。我们的调查回顾了三种主要方式的人类运动视频生成的最新发展和技术趋势：视觉，文本和音频。通过覆盖200多篇论文，我们提供了该领域的详细概述，并突出了里程碑式的作品，这些作品引起了重要的技术突破。这项调查的目标是揭示人类运动视频生成的前景，并成为推动数字人类全面应用的宝贵资源。本调查中检查的模型的完整列表可在我们的HTTPS URL的存储库中获得。</li>
</ul>

<h3>Title: OccTENS: 3D Occupancy World Model via Temporal Next-Scale Prediction</h3>
<ul>
<li><strong>Authors: </strong>Bu Jin, Songen Gu, Xiaotao Hu, Yupeng Zheng, Xiaoyang Guo, Qian Zhang, Xiaoxiao Long, Wei Yin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03887">https://arxiv.org/abs/2509.03887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03887">https://arxiv.org/pdf/2509.03887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03887]] OccTENS: 3D Occupancy World Model via Temporal Next-Scale Prediction(https://arxiv.org/abs/2509.03887)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In this paper, we propose OccTENS, a generative occupancy world model that enables controllable, high-fidelity long-term occupancy generation while maintaining computational efficiency. Different from visual generation, the occupancy world model must capture the fine-grained 3D geometry and dynamic evolution of the 3D scenes, posing great challenges for the generative models. Recent approaches based on autoregression (AR) have demonstrated the potential to predict vehicle movement and future occupancy scenes simultaneously from historical observations, but they typically suffer from \textbf{inefficiency}, \textbf{temporal degradation} in long-term generation and \textbf{lack of controllability}. To holistically address these issues, we reformulate the occupancy world model as a temporal next-scale prediction (TENS) task, which decomposes the temporal sequence modeling problem into the modeling of spatial scale-by-scale generation and temporal scene-by-scene prediction. With a \textbf{TensFormer}, OccTENS can effectively manage the temporal causality and spatial relationships of occupancy sequences in a flexible and scalable way. To enhance the pose controllability, we further propose a holistic pose aggregation strategy, which features a unified sequence modeling for occupancy and ego-motion. Experiments show that OccTENS outperforms the state-of-the-art method with both higher occupancy quality and faster inference time.</li>
<li><strong>摘要：</strong>在本文中，我们提出了Occtens，这是一种生成性的占用世界模型，可以在维持计算效率的同时，可控制，高保真的长期占用产生。与视觉产生不同，占用世界模型必须捕获3D场景的细粒3D几何和动态演变，这为生成模型带来了巨大的挑战。基于自动性（AR）的最新方法表明，从历史观察结果中同时预测车辆运动和未来的占用场景的潜力，但它们通常会遭受\ textbf {效率}，\ textbf {暂时性降级}的长期生成和\ textbf {\ textbf {缺乏可控性}。为了整体解决这些问题，我们将占用世界模型重新制定为临时预测（TENS）任务，该任务将时间序列建模问题分解为逐尺度生成和逐个场景预测的空间规模的建模。使用\ textbf {tensFormer}，OCTENS可以以灵活且可扩展的方式有效地管理占用序列的时间因果关系和空间关系。为了增强姿势可控性，我们进一步提出了整体姿势聚集策略，该策略具有统一的占用序列建模，用于占用和自我。实验表明，OCCTENS优于最先进的方法，其占用质量和更快的推理时间。</li>
</ul>

<h3>Title: A Generative Foundation Model for Chest Radiography</h3>
<ul>
<li><strong>Authors: </strong>Yuanfeng Ji, Dan Lin, Xiyue Wang, Lu Zhang, Wenhui Zhou, Chongjian Ge, Ruihang Chu, Xiaoli Yang, Junhan Zhao, Junsong Chen, Xiangde Luo, Sen Yang, Jin Fang, Ping Luo, Ruijiang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03903">https://arxiv.org/abs/2509.03903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03903">https://arxiv.org/pdf/2509.03903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03903]] A Generative Foundation Model for Chest Radiography(https://arxiv.org/abs/2509.03903)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The scarcity of well-annotated diverse medical images is a major hurdle for developing reliable AI models in healthcare. Substantial technical advances have been made in generative foundation models for natural images. Here we develop `ChexGen', a generative vision-language foundation model that introduces a unified framework for text-, mask-, and bounding box-guided synthesis of chest radiographs. Built upon the latent diffusion transformer architecture, ChexGen was pretrained on the largest curated chest X-ray dataset to date, consisting of 960,000 radiograph-report pairs. ChexGen achieves accurate synthesis of radiographs through expert evaluations and quantitative metrics. We demonstrate the utility of ChexGen for training data augmentation and supervised pretraining, which led to performance improvements across disease classification, detection, and segmentation tasks using a small fraction of training data. Further, our model enables the creation of diverse patient cohorts that enhance model fairness by detecting and mitigating demographic biases. Our study supports the transformative role of generative foundation models in building more accurate, data-efficient, and equitable medical AI systems.</li>
<li><strong>摘要：</strong>众所周知的多元化医学图像的稀缺性是开发医疗保健中可靠的AI模型的主要障碍。在自然图像的生成基础模型中，已经取得了重大的技术进步。在这里，我们开发了``Chexgen''，这是一种生成视觉语言基础模型，它引入了用于文本，掩模和边界盒引导的胸部X光片的统一框架。 Chexgen建立在潜在扩散变压器架构的基础上，迄今为止，在最大的策划胸部X射线数据集上预估计，由960,000张X光片报告对组成。 Chexgen通过专家评估和定量指标可以准确合成X光片。我们证明了Chexgen用于培训数据增强和监督预处理的实用性，从而通过一小部分培训数据来改善疾病分类，检测和分割任务的性能。此外，我们的模型可以创建多样化的患者人群，从而通过检测和减轻人口偏见来增强模型的公平性。我们的研究支持生成基础模型在建立更准确，数据效率和公平医学AI系统中的变革性作用。</li>
</ul>

<h3>Title: On Aligning Prediction Models with Clinical Experiential Learning: A Prostate Cancer Case Study</h3>
<ul>
<li><strong>Authors: </strong>Jacqueline J. Vallon, William Overman, Wanqiao Xu, Neil Panjwani, Xi Ling, Sushmita Vij, Hilary P. Bagshaw, John T. Leppert, Sumit Shah, Geoffrey Sonn, Sandy Srinivas, Erqi Pollom, Mark K. Buyyounouski, Mohsen Bayati</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04053">https://arxiv.org/abs/2509.04053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04053">https://arxiv.org/pdf/2509.04053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04053]] On Aligning Prediction Models with Clinical Experiential Learning: A Prostate Cancer Case Study(https://arxiv.org/abs/2509.04053)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Over the past decade, the use of machine learning (ML) models in healthcare applications has rapidly increased. Despite high performance, modern ML models do not always capture patterns the end user requires. For example, a model may predict a non-monotonically decreasing relationship between cancer stage and survival, keeping all other features fixed. In this paper, we present a reproducible framework for investigating this misalignment between model behavior and clinical experiential learning, focusing on the effects of underspecification of modern ML pipelines. In a prostate cancer outcome prediction case study, we first identify and address these inconsistencies by incorporating clinical knowledge, collected by a survey, via constraints into the ML model, and subsequently analyze the impact on model performance and behavior across degrees of underspecification. The approach shows that aligning the ML model with clinical experiential learning is possible without compromising performance. Motivated by recent literature in generative AI, we further examine the feasibility of a feedback-driven alignment approach in non-generative AI clinical risk prediction models through a randomized experiment with clinicians. Our findings illustrate that, by eliciting clinicians' model preferences using our proposed methodology, the larger the difference in how the constrained and unconstrained models make predictions for a patient, the more apparent the difference is in clinical interpretation.</li>
<li><strong>摘要：</strong>在过去的十年中，在医疗保健应用中使用机器学习（ML）模型已迅速增加。尽管高性能，现代ML模型并不总是捕获最终用户所需的模式。例如，模型可以预测癌症阶段与生存之间的非单调降低关系，从而使所有其他特征保持固定。在本文中，我们提出了一个可重现的框架，用于研究模型行为和临床体验学习之间的这种未对准，重点关注现代ML管道指定的效果。在前列腺癌结果预测案例研究中，我们首先通过将调查收集的临床知识纳入ML模型来识别和解决这些矛盾，并随后分析了跨指定性程度的模型性能和行为的影响。该方法表明，将ML模型与临床体验学习对齐是可能的，而不会损害性能。在最近的生成AI文献中，我们进一步研究了反馈驱动的对准方法在非基础AI临床风险预测模型中通过与临床医生进行的随机实验的可行性。我们的发现表明，通过使用我们提出的方法来启发临床医生的模型偏好，约束和无约束模型如何对患者进行预测的差异越大，差异就越明显在临床解释中。</li>
</ul>

<h3>Title: TaleDiffusion: Multi-Character Story Generation with Dialogue Rendering</h3>
<ul>
<li><strong>Authors: </strong>Ayan Banerjee, Josep Lladós, Umapada Pal, Anjan Dutta</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04123">https://arxiv.org/abs/2509.04123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04123">https://arxiv.org/pdf/2509.04123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04123]] TaleDiffusion: Multi-Character Story Generation with Dialogue Rendering(https://arxiv.org/abs/2509.04123)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-story visualization is challenging due to the need for consistent interaction among multiple characters across frames. Existing methods struggle with character consistency, leading to artifact generation and inaccurate dialogue rendering, which results in disjointed storytelling. In response, we introduce TaleDiffusion, a novel framework for generating multi-character stories with an iterative process, maintaining character consistency, and accurate dialogue assignment via postprocessing. Given a story, we use a pre-trained LLM to generate per-frame descriptions, character details, and dialogues via in-context learning, followed by a bounded attention-based per-box mask technique to control character interactions and minimize artifacts. We then apply an identity-consistent self-attention mechanism to ensure character consistency across frames and region-aware cross-attention for precise object placement. Dialogues are also rendered as bubbles and assigned to characters via CLIPSeg. Experimental results demonstrate that TaleDiffusion outperforms existing methods in consistency, noise reduction, and dialogue rendering.</li>
<li><strong>摘要：</strong>由于需要在跨帧的多个字符之间进行一致的互动，因此文本到故事的可视化具有挑战性。现有的方法与性格一致性斗争，导致人工制品产生和对话渲染不正确，从而导致讲故事分开。作为回应，我们介绍了Talediffusion，这是一个新颖的框架，用于通过迭代过程生成多个特定故事，维持角色一致性和通过后处理的准确对话分配。给出一个故事，我们使用预先训练的LLM通过内在学习来生成人均描述，角色细节和对话，然后采用有限的基于注意力的每个盒掩码技术来控制角色相互作用并最大程度地减少文物。然后，我们应用一种符合身份的自我意见机制，以确保跨帧和区域感知的跨注意事项的性格一致性，以确切的对象放置。对话也被渲染为气泡，并通过Clipseg分配给字符。实验结果表明，在一致性，降低降噪和对话渲染方面，TALEDIFFUSION优于现有方法。</li>
</ul>

<h3>Title: MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Yuan Zhao, Liu Lin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04126">https://arxiv.org/abs/2509.04126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04126">https://arxiv.org/pdf/2509.04126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04126]] MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image Generation(https://arxiv.org/abs/2509.04126)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models have achieved remarkable image quality, but they still struggle with complex, multiele ment prompts, and limited stylistic diversity. To address these limitations, we propose a Multi-Expert Planning and Gen eration Framework (MEPG) that synergistically integrates position- and style-aware large language models (LLMs) with spatial-semantic expert modules. The framework comprises two core components: (1) a Position-Style-Aware (PSA) module that utilizes a supervised fine-tuned LLM to decom pose input prompts into precise spatial coordinates and style encoded semantic instructions; and (2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera tion through dynamic expert routing across both local regions and global areas. During the generation process for each lo cal region, specialized models (e.g., realism experts, styliza tion specialists) are selectively activated for each spatial par tition via attention-based gating mechanisms. The architec ture supports lightweight integration and replacement of ex pert models, providing strong extensibility. Additionally, an interactive interface enables real-time spatial layout editing and per-region style selection from a portfolio of experts. Ex periments show that MEPG significantly outperforms base line models with the same backbone in both image quality and style diversity.</li>
<li><strong>摘要：</strong>文本到图像扩散模型已经达到了出色的图像质量，但它们仍然在复杂，多元的提示和有限的风格多样性方面挣扎。为了解决这些限制，我们提出了一个多专家计划和Gen Eration框架（MEPG），该框架协同将位置和风格感知的大语言模型（LLMS）与空间语义专家模块相结合。该框架包括两个核心组成部分：（1）使用监督的微调LLM来将输入提示置于精确的空间坐标和样式编码语义指令中； （2）通过在本地区域和全球区域之间的动态专家路由来实现跨区域的多型扩散（MED）模块。在每个Lo Cal区域的生成过程中，专门模型（例如现实主义专家，样式专家）通过基于注意的门控机制选择性地激活了每个空间分析。 Architec Ture支持轻巧的集成和替换前PERT模型，提供了强大的可扩展性。此外，交互式界面可从专家组合中进行实时空间布局编辑和各个区域样式选择。事实表明，MEPG在图像质量和样式多样性方面都显着优于具有相同主链的基线模型。</li>
</ul>

<h3>Title: TAGAL: Tabular Data Generation using Agentic LLM Methods</h3>
<ul>
<li><strong>Authors: </strong>Benoît Ronval, Pierre Dupont, Siegfried Nijssen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04152">https://arxiv.org/abs/2509.04152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04152">https://arxiv.org/pdf/2509.04152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04152]] TAGAL: Tabular Data Generation using Agentic LLM Methods(https://arxiv.org/abs/2509.04152)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The generation of data is a common approach to improve the performance of machine learning tasks, among which is the training of models for classification. In this paper, we present TAGAL, a collection of methods able to generate synthetic tabular data using an agentic workflow. The methods leverage Large Language Models (LLMs) for an automatic and iterative process that uses feedback to improve the generated data without any further LLM training. The use of LLMs also allows for the addition of external knowledge in the generation process. We evaluate TAGAL across diverse datasets and different aspects of quality for the generated data. We look at the utility of downstream ML models, both by training classifiers on synthetic data only and by combining real and synthetic data. Moreover, we compare the similarities between the real and the generated data. We show that TAGAL is able to perform on par with state-of-the-art approaches that require LLM training and generally outperforms other training-free approaches. These findings highlight the potential of agentic workflow and open new directions for LLM-based data generation methods.</li>
<li><strong>摘要：</strong>数据的产生是提高机器学习任务性能的常见方法，其中包括对分类模型的培训。在本文中，我们提出了他加禄语，这是一种能够使用代理工作流生成合成表格数据的方法的集合。这些方法利用大型语言模型（LLMS）进行自动和迭代过程，该过程使用反馈来改进生成的数据而无需进行任何进一步的LLM培训。 LLM的使用还允许在生成过程中添加外部知识。我们评估生成数据的各种数据集和质量各个方面的不同方面。我们通过仅通过合成数据以及结合实际和合成数据来培训分类器来查看下游ML模型的实用性。此外，我们比较了真实数据和生成数据之间的相似性。我们表明，塔加尔能够与需要LLM培训的最先进方法相同，并且通常超过其他无训练方法。这些发现突出了代理工作流程的潜力和基于LLM的数据生成方法的新方向的潜力。</li>
</ul>

<h3>Title: Set Block Decoding is a Language Model Inference Accelerator</h3>
<ul>
<li><strong>Authors: </strong>Itai Gat, Heli Ben-Hamu, Marton Havasi, Daniel Haziza, Jeremy Reizenstein, Gabriel Synnaeve, David Lopez-Paz, Brian Karrer, Yaron Lipman</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04185">https://arxiv.org/abs/2509.04185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04185">https://arxiv.org/pdf/2509.04185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04185]] Set Block Decoding is a Language Model Inference Accelerator(https://arxiv.org/abs/2509.04185)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Autoregressive next token prediction language models offer powerful capabilities but face significant challenges in practical deployment due to the high computational and memory costs of inference, particularly during the decoding stage. We introduce Set Block Decoding (SBD), a simple and flexible paradigm that accelerates generation by integrating standard next token prediction (NTP) and masked token prediction (MATP) within a single architecture. SBD allows the model to sample multiple, not necessarily consecutive, future tokens in parallel, a key distinction from previous acceleration methods. This flexibility allows the use of advanced solvers from the discrete diffusion literature, offering significant speedups without sacrificing accuracy. SBD requires no architectural changes or extra training hyperparameters, maintains compatibility with exact KV-caching, and can be implemented by fine-tuning existing next token prediction models. By fine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x reduction in the number of forward passes required for generation while achieving same performance as equivalent NTP training.</li>
<li><strong>摘要：</strong>自回归的隔壁预测语言模型提供了强大的功能，但是由于推理的高计算和记忆成本，尤其是在解码阶段，因此在实际部署方面面临重大挑战。我们介绍了Set Block解码（SBD），这是一种简单而灵活的范式，通过在单个体系结构中集成标准的隔壁标记预测（NTP）和掩盖的令牌预测（MATP）来加速生成。 SBD允许该模型并行采样多个，不一定是连续的未来令牌，这是与以前的加速方法的关键区别。这种灵活性允许从离散扩散文献中使用高级求解器，从而在不牺牲准确性的情况下提供了显着的加速。 SBD不需要架构更改或额外的培训超级计，可以保持与精确的KV辅助的兼容性，并且可以通过对现有的隔壁预测模型进行微调来实现。通过微调Llama-3.1 8B和QWEN-3 8B，我们证明SBD可以减少生成所需的前向通行证数量3-5倍，同时获得与等效NTP培训相同的性能。</li>
</ul>

<h3>Title: DUDE: Diffusion-Based Unsupervised Cross-Domain Image Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Ruohong Yang, Peng Hu, Yunfan Li, Xi Peng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04193">https://arxiv.org/abs/2509.04193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04193">https://arxiv.org/pdf/2509.04193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04193]] DUDE: Diffusion-Based Unsupervised Cross-Domain Image Retrieval(https://arxiv.org/abs/2509.04193)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Unsupervised cross-domain image retrieval (UCIR) aims to retrieve images of the same category across diverse domains without relying on annotations. Existing UCIR methods, which align cross-domain features for the entire image, often struggle with the domain gap, as the object features critical for retrieval are frequently entangled with domain-specific styles. To address this challenge, we propose DUDE, a novel UCIR method building upon feature disentanglement. In brief, DUDE leverages a text-to-image generative model to disentangle object features from domain-specific styles, thus facilitating semantical image retrieval. To further achieve reliable alignment of the disentangled object features, DUDE aligns mutual neighbors from within domains to across domains in a progressive manner. Extensive experiments demonstrate that DUDE achieves state-of-the-art performance across three benchmark datasets over 13 domains. The code will be released.</li>
<li><strong>摘要：</strong>无监督的跨域图像检索（UCIR）旨在在不依赖注释的情况下检索各种域的同一类别的图像。现有的UCIR方法（将整个图像的跨域特征都与域间隙相结合，因为检索至关重要的对象特征经常与特定于域的样式纠缠在一起。为了应对这一挑战，我们提出了Dude，这是一种新型的UCIR方法，建立在特征分解的基础上。简而言之，花花公子利用文本对图像生成模型将对象与特定于领域的样式脱离，从而促进语义图像检索。为了进一步实现分离的对象特征的可靠对齐，花花公子以渐进的方式将来自域内的相互邻居与跨域的跨邻居对准。广泛的实验表明，Dude在13个域上的三个基准数据集中实现了最先进的性能。代码将发布。</li>
</ul>

<h3>Title: Synthetic Survival Data Generation for Heart Failure Prognosis Using Deep Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Chanon Puttanawarut, Natcha Fongsrisin, Porntep Amornritvanich, Cholatid Ratanatharathorn, Panu Looareesuwan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04245">https://arxiv.org/abs/2509.04245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04245">https://arxiv.org/pdf/2509.04245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04245]] Synthetic Survival Data Generation for Heart Failure Prognosis Using Deep Generative Models(https://arxiv.org/abs/2509.04245)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Background: Heart failure (HF) research is constrained by limited access to large, shareable datasets due to privacy regulations and institutional barriers. Synthetic data generation offers a promising solution to overcome these challenges while preserving patient confidentiality. Methods: We generated synthetic HF datasets from institutional data comprising 12,552 unique patients using five deep learning models: tabular variational autoencoder (TVAE), normalizing flow, ADSGAN, SurvivalGAN, and tabular denoising diffusion probabilistic models (TabDDPM). We comprehensively evaluated synthetic data utility through statistical similarity metrics, survival prediction using machine learning and privacy assessments. Results: SurvivalGAN and TabDDPM demonstrated high fidelity to the original dataset, exhibiting similar variable distributions and survival curves after applying histogram equalization. SurvivalGAN (C-indices: 0.71-0.76) and TVAE (C-indices: 0.73-0.76) achieved the strongest performance in survival prediction evaluation, closely matched real data performance (C-indices: 0.73-0.76). Privacy evaluation confirmed protection against re-identification attacks. Conclusions: Deep learning-based synthetic data generation can produce high-fidelity, privacy-preserving HF datasets suitable for research applications. This publicly available synthetic dataset addresses critical data sharing barriers and provides a valuable resource for advancing HF research and predictive modeling.</li>
<li><strong>摘要：</strong>背景：由于隐私法规和机构障碍，心力衰竭（HF）的研究受到有限访问大型，可共享数据集的限制。合成数据生成提供了一种有希望的解决方案，可以克服这些挑战，同时保留患者机密性。方法：我们使用五个深度学习模型的机构数据从机构数据中生成了合成的HF数据集：表格变异自动编码器（TVAE），归一化流量，Adsgan，SurvivalGan和表图形的扩散扩散概率模型（TABDDDPM）。我们通过统计相似性指标，使用机器学习和隐私评估来全面评估合成数据实用性。结果：Survivalgan和TABDDPM表现出对原始数据集的高保真度，在应用直方图均衡后表现出相似的变量分布和生存曲线。 Survivalgan（C-Indices：0.71-0.76）和TVAE（C-Indices：0.73-0.76）在生存预测评估中达到了最强的性能，紧密匹配的真实数据性能（C-Indices：0.73-0.76）。隐私评估确认了防止重新识别攻击的保护。结论：基于深度学习的综合数据生成可以产生适合研究应用程序的高保真性，保护隐私的HF数据集。该公开可用的合成数据集解决了关键数据共享障碍，并为推进HF研究和预测建模提供了宝贵的资源。</li>
</ul>

<h3>Title: From Editor to Dense Geometry Estimator</h3>
<ul>
<li><strong>Authors: </strong>JiYuan Wang, Chunyu Lin, Lei Sun, Rongying Liu, Lang Nie, Mingxing Li, Kang Liao, Xiangxiang Chu, Yao Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04338">https://arxiv.org/abs/2509.04338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04338">https://arxiv.org/pdf/2509.04338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04338]] From Editor to Dense Geometry Estimator(https://arxiv.org/abs/2509.04338)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Leveraging visual priors from pre-trained text-to-image (T2I) generative models has shown success in dense prediction. However, dense prediction is inherently an image-to-image task, suggesting that image editing models, rather than T2I generative models, may be a more suitable foundation for fine-tuning. Motivated by this, we conduct a systematic analysis of the fine-tuning behaviors of both editors and generators for dense geometry estimation. Our findings show that editing models possess inherent structural priors, which enable them to converge more stably by ``refining" their innate features, and ultimately achieve higher performance than their generative counterparts. Based on these findings, we introduce \textbf{FE2E}, a framework that pioneeringly adapts an advanced editing model based on Diffusion Transformer (DiT) architecture for dense geometry prediction. Specifically, to tailor the editor for this deterministic task, we reformulate the editor's original flow matching loss into the ``consistent velocity" training objective. And we use logarithmic quantization to resolve the precision conflict between the editor's native BFloat16 format and the high precision demand of our tasks. Additionally, we leverage the DiT's global attention for a cost-free joint estimation of depth and normals in a single forward pass, enabling their supervisory signals to mutually enhance each other. Without scaling up the training data, FE2E achieves impressive performance improvements in zero-shot monocular depth and normal estimation across multiple datasets. Notably, it achieves over 35\% performance gains on the ETH3D dataset and outperforms the DepthAnything series, which is trained on 100$\times$ data. The project page can be accessed \href{this https URL}{here}.</li>
<li><strong>摘要：</strong>利用预先训练的文本对图像（T2I）生成模型的视觉先验已显示在密集预测中的成功。但是，密集的预测本质上是图像到图像的任务，表明图像编辑模型而不是T2i生成模型可能是进行微调的更合适的基础。在此激励的情况下，我们对编辑器和发电机的微调行为进行系统分析，以进行密集的几何估计。 Our findings show that editing models possess inherent structural priors, which enable them to converge more stably by ``refining" their innate features, and ultimately achieve higher performance than their generative counterparts. Based on these findings, we introduce \textbf{FE2E}, a framework that pioneeringly adapts an advanced editing model based on Diffusion Transformer (DiT) architecture for dense geometry prediction.具体来说，为了定制编辑的这项确定性任务，我们将编辑器的原始流匹配损失重新制定为``一致的速度''训练目标。而且，我们使用对数量化来解决编辑器的本机Bfloat16格式与我们任务的高精度需求之间的精确冲突。此外，我们利用DIT的全球关注，以在单个前向通行证中对深度和正态的无成本关节估计，从而使他们的监督信号相互增强。 FE2E在不扩大训练数据的情况下，可以在多个数据集中实现零拍的单眼和正常估计的令人印象深刻的性能。值得注意的是，它在ETH3D数据集上实现了超过35％的性能增长，并且优于DepThything系列，该系列经过100 $ \ times $ data的培训。可以访问项目页面\ href {this https url} {there}。</li>
</ul>

<h3>Title: AnomalyLMM: Bridging Generative Knowledge and Discriminative Retrieval for Text-Based Person Anomaly Search</h3>
<ul>
<li><strong>Authors: </strong>Hao Ju, Hu Zhang, Zhedong Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04376">https://arxiv.org/abs/2509.04376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04376">https://arxiv.org/pdf/2509.04376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04376]] AnomalyLMM: Bridging Generative Knowledge and Discriminative Retrieval for Text-Based Person Anomaly Search(https://arxiv.org/abs/2509.04376)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With growing public safety demands, text-based person anomaly search has emerged as a critical task, aiming to retrieve individuals with abnormal behaviors via natural language descriptions. Unlike conventional person search, this task presents two unique challenges: (1) fine-grained cross-modal alignment between textual anomalies and visual behaviors, and (2) anomaly recognition under sparse real-world samples. While Large Multi-modal Models (LMMs) excel in multi-modal understanding, their potential for fine-grained anomaly retrieval remains underexplored, hindered by: (1) a domain gap between generative knowledge and discriminative retrieval, and (2) the absence of efficient adaptation strategies for deployment. In this work, we propose AnomalyLMM, the first framework that harnesses LMMs for text-based person anomaly search. Our key contributions are: (1) A novel coarse-to-fine pipeline integrating LMMs to bridge generative world knowledge with retrieval-centric anomaly detection; (2) A training-free adaptation cookbook featuring masked cross-modal prompting, behavioral saliency prediction, and knowledge-aware re-ranking, enabling zero-shot focus on subtle anomaly cues. As the first study to explore LMMs for this task, we conduct a rigorous evaluation on the PAB dataset, the only publicly available benchmark for text-based person anomaly search, with its curated real-world anomalies covering diverse scenarios (e.g., falling, collision, and being hit). Experiments show the effectiveness of the proposed method, surpassing the competitive baseline by +0.96% Recall@1 accuracy. Notably, our method reveals interpretable alignment between textual anomalies and visual behaviors, validated via qualitative analysis. Our code and models will be released for future research.</li>
<li><strong>摘要：</strong>随着公共安全要求的增长，基于文本的人的异常搜索已成为一项关键任务，旨在通过自然语言描述来检索具有异常行为的人。与传统的人搜索不同，此任务提出了两个独特的挑战：（1）文本异常和视觉行为之间的细粒度跨模式对齐，以及（2）在稀疏现实世界样本下的异常识别。尽管大型多模型模型（LMM）在多模式的理解中出色，但它们进行细粒度异常检索的潜力仍未散发出来，受到以下方式的阻碍：（1）生成知识和歧视性检索之间的域间隙，以及（2）缺乏有效的适应策略的部署策略。在这项工作中，我们提出了Anomalylmm，这是利用LMM的第一个框架，用于基于文本的人的异常搜索。我们的主要贡献是：（1）一种新型的粗到精细管道，将LMMS整合到以检索为中心的异常检测中桥接生成世界知识； （2）一本无训练的适应性食谱，其中包含蒙版的跨模式提示，行为显着性预测和知识感知的重新排列，从而使零摄影专注于微妙的异常线索。作为探索该任务LMM的首次研究，我们对PAB数据集进行了严格的评估，PAB数据集是基于文本的人异常搜索的唯一公开可用的基准测试，其策划的现实世界异常涵盖了各种情况（例如，下降，碰撞，碰撞和受到打击）。实验显示了所提出的方法的有效性，超过竞争性基线 +0.96％的回忆@1精度。值得注意的是，我们的方法揭示了通过定性分析验证的文本异常和视觉行为之间的可解释对齐。我们的代码和模型将发布以供将来的研究。</li>
</ul>

<h3>Title: PagedEviction: Structured Block-wise KV Cache Pruning for Efficient Large Language Model Inference</h3>
<ul>
<li><strong>Authors: </strong>Krishna Teja Chitty-Venkata, Jie Ye, Xian-He Sun, Anthony Kougkas, Murali Emani, Venkatram Vishwanath, Bogdan Nicolae</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04377">https://arxiv.org/abs/2509.04377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04377">https://arxiv.org/pdf/2509.04377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04377]] PagedEviction: Structured Block-wise KV Cache Pruning for Efficient Large Language Model Inference(https://arxiv.org/abs/2509.04377)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>KV caching significantly improves the efficiency of Large Language Model (LLM) inference by storing attention states from previously processed tokens, enabling faster generation of subsequent tokens. However, as sequence length increases, the KV cache quickly becomes a major memory bottleneck. To address this, we propose PagedEviction, a novel fine-grained, structured KV cache pruning strategy that enhances the memory efficiency of vLLM's PagedAttention. Unlike existing approaches that rely on attention-based token importance or evict tokens across different vLLM pages, PagedEviction introduces an efficient block-wise eviction algorithm tailored for paged memory layouts. Our method integrates seamlessly with PagedAttention without requiring any modifications to its CUDA attention kernels. We evaluate PagedEviction across Llama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models on the LongBench benchmark suite, demonstrating improved memory usage with better accuracy than baselines on long context tasks.</li>
<li><strong>摘要：</strong>KV缓存可显着提高大语言模型（LLM）推断的效率，通过存储先前处理的令牌的注意状态，从而更快地生成后来的令牌。但是，随着序列长度的增加，KV缓存迅速成为主要的内存瓶颈。为了解决这个问题，我们提出了PageDeviction，这是一种新型的细粒度，结构化的KV缓存修剪策略，可提高VLLM pageNation的记忆效率。与现有的方法依赖于基于注意力的令牌重要性或在不同VLLM页面上驱逐令牌的方法不同，PageDeviction引入了一种有效的块驱逐算法，该算法量身定制了针对分类记忆布局的量身定制的。我们的方法无缝地集成了与PageNTICTION，而无需对其CUDA注意力内核进行任何修改。我们在Longbench基准套件上评估了Llama-3.1-8b-Instruct，Llama-3.2-1b-Instruct和Llama-3.2-3b-Instruct模型的Pagedeviction，在长篇小说中，与盆地相比，在长篇小说中表现出更好的准确记忆力。</li>
</ul>

<h3>Title: Transition Models: Rethinking the Generative Learning Objective</h3>
<ul>
<li><strong>Authors: </strong>Zidong Wang, Yiyuan Zhang, Xiaoyu Yue, Xiangyu Yue, Yangguang Li, Wanli Ouyang, Lei Bai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04394">https://arxiv.org/abs/2509.04394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04394">https://arxiv.org/pdf/2509.04394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04394]] Transition Models: Rethinking the Generative Learning Objective(https://arxiv.org/abs/2509.04394)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>A fundamental dilemma in generative modeling persists: iterative diffusion models achieve outstanding fidelity, but at a significant computational cost, while efficient few-step alternatives are constrained by a hard quality ceiling. This conflict between generation steps and output quality arises from restrictive training objectives that focus exclusively on either infinitesimal dynamics (PF-ODEs) or direct endpoint prediction. We address this challenge by introducing an exact, continuous-time dynamics equation that analytically defines state transitions across any finite time interval. This leads to a novel generative paradigm, Transition Models (TiM), which adapt to arbitrary-step transitions, seamlessly traversing the generative trajectory from single leaps to fine-grained refinement with more steps. Despite having only 865M parameters, TiM achieves state-of-the-art performance, surpassing leading models such as SD3.5 (8B parameters) and FLUX.1 (12B parameters) across all evaluated step counts. Importantly, unlike previous few-step generators, TiM demonstrates monotonic quality improvement as the sampling budget increases. Additionally, when employing our native-resolution strategy, TiM delivers exceptional fidelity at resolutions up to 4096x4096.</li>
<li><strong>摘要：</strong>生成建模的基本困境持续存在：迭代扩散模型实现了出色的保真度，但是以巨大的计算成本，而有效的少量替代方案则受到硬质量上限的限制。生成步骤和输出质量之间的这种冲突来自限制性培训目标，这些目标仅关注无限动态（PF-odes）或直接终点预测。我们通过引入一个精确的连续时间动力学方程来解决这一挑战，该方程在分析上定义了任何有限的时间间隔的状态转换。这导致了一种新颖的生成范式，即过渡模型（TIM），该模型适应了任意步骤的过渡，并通过更多步骤无缝地穿越了从单个飞跃到细粒细化的生成轨迹。尽管只有8.65亿个参数，但Tim还是达到了最新的性能，超过了所有评估的步骤计数等领先的模型，例如SD3.5（8B参数）和Flux.1（12B参数）。重要的是，与以前的几步发电机不同，随着抽样预算的增加，蒂姆表现出单调质量的改进。此外，在采用我们的本地分辨率策略时，蒂姆在最高4096x4096的决议中提供了杰出的保真度。</li>
</ul>

<h3>Title: Few-step Flow for 3D Generation via Marginal-Data Transport Distillation</h3>
<ul>
<li><strong>Authors: </strong>Zanwei Zhou, Taoran Yi, Jiemin Fang, Chen Yang, Lingxi Xie, Xinggang Wang, Wei Shen, Qi Tian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04406">https://arxiv.org/abs/2509.04406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04406">https://arxiv.org/pdf/2509.04406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04406]] Few-step Flow for 3D Generation via Marginal-Data Transport Distillation(https://arxiv.org/abs/2509.04406)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Flow-based 3D generation models typically require dozens of sampling steps during inference. Though few-step distillation methods, particularly Consistency Models (CMs), have achieved substantial advancements in accelerating 2D diffusion models, they remain under-explored for more complex 3D generation tasks. In this study, we propose a novel framework, MDT-dist, for few-step 3D flow distillation. Our approach is built upon a primary objective: distilling the pretrained model to learn the Marginal-Data Transport. Directly learning this objective needs to integrate the velocity fields, while this integral is intractable to be implemented. Therefore, we propose two optimizable objectives, Velocity Matching (VM) and Velocity Distillation (VD), to equivalently convert the optimization target from the transport level to the velocity and the distribution level respectively. Velocity Matching (VM) learns to stably match the velocity fields between the student and the teacher, but inevitably provides biased gradient estimates. Velocity Distillation (VD) further enhances the optimization process by leveraging the learned velocity fields to perform probability density distillation. When evaluated on the pioneer 3D generation framework TRELLIS, our method reduces sampling steps of each flow transformer from 25 to 1 or 2, achieving 0.68s (1 step x 2) and 0.94s (2 steps x 2) latency with 9.0x and 6.5x speedup on A800, while preserving high visual and geometric fidelity. Extensive experiments demonstrate that our method significantly outperforms existing CM distillation methods, and enables TRELLIS to achieve superior performance in few-step 3D generation.</li>
<li><strong>摘要：</strong>基于流量的3D生成模型通常需要在推断过程中进行数十个采样步骤。尽管在加速2D扩散模型方面，很少有步骤蒸馏方法，尤其是一致性模型（CMS）取得了重大进步，但对于更复杂的3D生成任务，它们仍未探索。在这项研究中，我们提出了一个新型框架，即MDT-DIST，用于几步3D流动蒸馏。我们的方法是建立在一个主要目标的基础上的：将验证的模型提取以学习边缘数据运输。直接学习这个目标需要整合速度字段，而该积分却棘手要实现。因此，我们提出了两个优化的目标，即速度匹配（VM）和速度蒸馏（VD），以分别将优化目标从传输级别转换为速度和分布水平。速度匹配（VM）学会稳定匹配学生与老师之间的速度场，但不可避免地提供了有偏见的梯度估计。速度蒸馏（VD）通过利用学习速度场进行概率密度蒸馏而进一步增强了优化过程。当在先锋3D生成框架格子上进行评估时，我们的方法将每个流动变压器的采样步骤从25降低到1或2，从而达到0.68（1步X 2）和0.94和0.94s（2步X 2）（2步X 2）在A800上以9.0x和6.5倍的速度延迟，同时维护高视觉和几何效率。广泛的实验表明，我们的方法显着胜过现有的CM蒸馏方法，并使格子可以在几步3D代中实现出色的性能。</li>
</ul>

<h3>Title: Durian: Dual Reference-guided Portrait Animation with Attribute Transfer</h3>
<ul>
<li><strong>Authors: </strong>Hyunsoo Cha, Byungjun Kim, Hanbyul Joo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04434">https://arxiv.org/abs/2509.04434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04434">https://arxiv.org/pdf/2509.04434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04434]] Durian: Dual Reference-guided Portrait Animation with Attribute Transfer(https://arxiv.org/abs/2509.04434)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present Durian, the first method for generating portrait animation videos with facial attribute transfer from a given reference image to a target portrait in a zero-shot manner. To enable high-fidelity and spatially consistent attribute transfer across frames, we introduce dual reference networks that inject spatial features from both the portrait and attribute images into the denoising process of a diffusion model. We train the model using a self-reconstruction formulation, where two frames are sampled from the same portrait video: one is treated as the attribute reference and the other as the target portrait, and the remaining frames are reconstructed conditioned on these inputs and their corresponding masks. To support the transfer of attributes with varying spatial extent, we propose a mask expansion strategy using keypoint-conditioned image generation for training. In addition, we further augment the attribute and portrait images with spatial and appearance-level transformations to improve robustness to positional misalignment between them. These strategies allow the model to effectively generalize across diverse attributes and in-the-wild reference combinations, despite being trained without explicit triplet supervision. Durian achieves state-of-the-art performance on portrait animation with attribute transfer, and notably, its dual reference design enables multi-attribute composition in a single generation pass without additional training.</li>
<li><strong>摘要：</strong>我们介绍榴莲，这是第一种以面部属性从给定参考图像转移到目标肖像以零拍的方式生成肖像画视频的方法。为了启用跨帧的高保真性和空间一致的属性传输，我们介绍了双参考网络，这些网络将肖像和属性图像的空间特征注射到扩散模型的去索过程中。我们使用自我重建公式训练模型，其中两个帧是从同一肖像视频中采样的：一个被视为属性参考，另一个被视为目标肖像，其余框架在这些输入及其相应的掩码上进行了重建。为了以不同的空间范围来支持属性的传递，我们建议使用按键调节图像生成训练的掩模扩展策略。此外，我们进一步增强了具有空间和外观级变换的属性和肖像图像，以改善它们之间的稳健性。这些策略使该模型能够有效地跨越各种属性和野外参考组合，尽管接受了未经明确的三胞胎监督的培训。榴莲通过属性转移在肖像画中实现最先进的性能，尤其是，其双重参考设计可以在单一传球中进行多属性构图，而无需其他培训。</li>
</ul>

<h3>Title: From Lines to Shapes: Geometric-Constrained Segmentation of X-Ray Collimators via Hough Transform</h3>
<ul>
<li><strong>Authors: </strong>Benjamin El-Zein, Dominik Eckert, Andreas Fieselmann, Christopher Syben, Ludwig Ritschl, Steffen Kappler, Sebastian Stober</a></li>
<li><strong>Subjects: </strong>cs.CV, physics.med-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04437">https://arxiv.org/abs/2509.04437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04437">https://arxiv.org/pdf/2509.04437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04437]] From Lines to Shapes: Geometric-Constrained Segmentation of X-Ray Collimators via Hough Transform(https://arxiv.org/abs/2509.04437)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Collimation in X-ray imaging restricts exposure to the region-of-interest (ROI) and minimizes the radiation dose applied to the patient. The detection of collimator shadows is an essential image-based preprocessing step in digital radiography posing a challenge when edges get obscured by scattered X-ray radiation. Regardless, the prior knowledge that collimation forms polygonal-shaped shadows is evident. For this reason, we introduce a deep learning-based segmentation that is inherently constrained to its geometry. We achieve this by incorporating a differentiable Hough transform-based network to detect the collimation borders and enhance its capability to extract the information about the ROI center. During inference, we combine the information of both tasks to enable the generation of refined, line-constrained segmentation masks. We demonstrate robust reconstruction of collimated regions achieving median Hausdorff distances of 4.3-5.0mm on diverse test sets of real Xray images. While this application involves at most four shadow borders, our method is not fundamentally limited by a specific number of edges.</li>
<li><strong>摘要：</strong>X射线成像中的准直限制了暴露于利益区域（ROI），并最大程度地减少适用于患者的辐射剂量。准直颗粒阴影的检测是数字射线照相中基于图像的必不可少的预处理步骤，当边缘被散射的X射线辐射掩盖时，提出了挑战。无论如何，显而易见的是，即准直形成多边形阴影的知识是显而易见的。因此，我们引入了一种基于深度学习的细分，该分割本质上构成了其几何形状。我们通过合并基于Hough的转换网络来检测准确的边界并增强其提取有关ROI中心的信息的能力来实现这一目标。在推断期间，我们结合了这两个任务的信息，以使生成精制的，线约束的分割面罩。我们证明了在各种真实X射线图像的测试集上，实现中位数Hausdorff距离为4.3-5.0mm的中位数距离为4.3-5.0mm的强大重建。尽管此应用程序最多涉及四个阴影边界，但我们的方法在根本上不受特定数量的边缘的限制。</li>
</ul>

<h3>Title: The Telephone Game: Evaluating Semantic Drift in Unified Models</h3>
<ul>
<li><strong>Authors: </strong>Sabbir Mollah, Rohit Gupta, Sirnam Swetha, Qingyang Liu, Ahnaf Munir, Mubarak Shah</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04438">https://arxiv.org/abs/2509.04438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04438">https://arxiv.org/pdf/2509.04438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04438]] The Telephone Game: Evaluating Semantic Drift in Unified Models(https://arxiv.org/abs/2509.04438)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Employing a single, unified model (UM) for both visual understanding (image-to-text: I2T) and and visual generation (text-to-image: T2I) has opened a new direction in Visual Language Model (VLM) research. While UMs can also support broader unimodal tasks (e.g., text-to-text, image-to-image), we focus on the core cross-modal pair T2I and I2T, as consistency between understanding and generation is critical for downstream use. Existing evaluations consider these capabilities in isolation: FID and GenEval for T2I, and benchmarks such as MME, MMBench for I2T. These single-pass metrics do not reveal whether a model that understands a concept can also render it, nor whether meaning is preserved when cycling between image and text modalities. To address this, we introduce the Unified Consistency Framework for Unified Models (UCF-UM), a cyclic evaluation protocol that alternates I2T and T2I over multiple generations to quantify semantic drift. UCF formulates 3 metrics: (i) Mean Cumulative Drift (MCD), an embedding-based measure of overall semantic loss; (ii) Semantic Drift Rate (SDR), that summarizes semantic decay rate; and (iii) Multi-Generation GenEval (MGG), an object-level compliance score extending GenEval. To assess generalization beyond COCO, which is widely used in training; we create a new benchmark ND400, sampled from NoCaps and DOCCI and evaluate on seven recent models. UCF-UM reveals substantial variation in cross-modal stability: some models like BAGEL maintain semantics over many alternations, whereas others like Vila-u drift quickly despite strong single-pass scores. Our results highlight cyclic consistency as a necessary complement to standard I2T and T2I evaluations, and provide practical metrics to consistently assess unified model's cross-modal stability and strength of their shared representations. Code: this https URL</li>
<li><strong>摘要：</strong>采用单个统一模型（UM）进行视觉理解（图像到文本：I2T）和视觉生成（文本到图像：T2I），已经在视觉语言模型（VLM）研究中开了一个新的方向。尽管UMS还可以支持更广泛的单峰任务（例如，文本到文本，图像到图像），但我们专注于核心跨模式对T2I和I2T，因为理解和生成之间的一致性对于下游使用至关重要。现有评估将这些功能隔离考虑：T2i的FID和Geneval，以及MME，MMBENCH等基准，用于I2T。这些单通量指标并未透露一个理解概念的模型是否也可以渲染它，也可以在图像和文本模式之间循环时保留含义。为了解决这个问题，我们介绍了统一模型（UCF-UM）的统一一致性框架，这是一种循环评估协议，该协议在多代内交替I2T和T2I以量化语义漂移。 UCF制定了3个指标：（i）累积漂移（MCD），这是一种基于嵌入的总体语义损失的度量； （ii）语义漂移率（SDR），总结了语义衰减率； （iii）多代元音（MGG），一个对象级的依从性得分延伸了遗传学。评估可可以外的概括，可可广泛用于训练；我们创建了一个新的基准ND400，并从NOCAPS和DOCCI采样，并对最近的七个模型进行了评估。 UCF-UM揭示了跨模式稳定性的实质性变化：某些模型（例如百吉饼）在许多交替方面都保持语义，而其他一些模型则像Vila-u一样迅速漂移，尽管单通得分很高。我们的结果强调了循环一致性是对标准I2T和T2I评估的必要补充，并提供了实用的指标，以始终如一地评估统一模型的跨模式稳定性和共享表示的强度。代码：此HTTPS URL</li>
</ul>

<h3>Title: One Flight Over the Gap: A Survey from Perspective to Panoramic Vision</h3>
<ul>
<li><strong>Authors: </strong>Xin Lin, Xian Ge, Dizhe Zhang, Zhaoliang Wan, Xianshun Wang, Xiangtai Li, Wenjie Jiang, Bo Du, Dacheng Tao, Ming-Hsuan Yang, Lu Qi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04444">https://arxiv.org/abs/2509.04444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04444">https://arxiv.org/pdf/2509.04444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04444]] One Flight Over the Gap: A Survey from Perspective to Panoramic Vision(https://arxiv.org/abs/2509.04444)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Driven by the demand for spatial intelligence and holistic scene perception, omnidirectional images (ODIs), which provide a complete 360\textdegree{} field of view, are receiving growing attention across diverse applications such as virtual reality, autonomous driving, and embodied robotics. Despite their unique characteristics, ODIs exhibit remarkable differences from perspective images in geometric projection, spatial distribution, and boundary continuity, making it challenging for direct domain adaption from perspective methods. This survey reviews recent panoramic vision techniques with a particular emphasis on the perspective-to-panorama adaptation. We first revisit the panoramic imaging pipeline and projection methods to build the prior knowledge required for analyzing the structural disparities. Then, we summarize three challenges of domain adaptation: severe geometric distortions near the poles, non-uniform sampling in Equirectangular Projection (ERP), and periodic boundary continuity. Building on this, we cover 20+ representative tasks drawn from more than 300 research papers in two dimensions. On one hand, we present a cross-method analysis of representative strategies for addressing panoramic specific challenges across different tasks. On the other hand, we conduct a cross-task comparison and classify panoramic vision into four major categories: visual quality enhancement and assessment, visual understanding, multimodal understanding, and visual generation. In addition, we discuss open challenges and future directions in data, models, and applications that will drive the advancement of panoramic vision research. We hope that our work can provide new insight and forward looking perspectives to advance the development of panoramic vision technologies. Our project page is this https URL</li>
<li><strong>摘要：</strong>在对空间智能和整体场景感知的需求的驱动下，提供完整的360 \ textDegree {}视野的全向图像（ODI）正在引起跨虚拟现实，自动驾驶和体现的机器人技术等多样化应用程序的越来越多的关注。尽管具有独特的特征，但ODI从几何投影，空间分布和边界连续性中表现出显着的差异，从而使其对直接域适应的挑战。这项调查回顾了最新的全景技术，并特别强调了透视镜的适应。我们首先重新审视全景成像管道和投影方法，以建立分析结构差异所需的先验知识。然后，我们总结了域适应的三个挑战：极点附近的严重几何变形，等应角投影（ERP）中的不均匀采样以及周期性的边界连续性。在此基础上，我们涵盖了20多个在两个维度的研究论文中绘制的代表性任务。一方面，我们对代表性策略进行了跨方法分析，以应对各种任务的全景特定挑战。另一方面，我们进行了交叉任务比较，并将全景视觉分为四个主要类别：视觉质量增强和评估，视觉理解，多模式理解和视觉产生。此外，我们讨论了将推动全景研究进步的数据，模型和应用程序中的开放挑战和未来方向。我们希望我们的工作能够提供新的见识和前瞻性观点，以推动全景视觉技术的发展。我们的项目页面是此HTTPS URL</li>
</ul>

<h3>Title: Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Kiymet Akdemir, Jing Shi, Kushal Kafle, Brian Price, Pinar Yanardag</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04446">https://arxiv.org/abs/2509.04446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04446">https://arxiv.org/pdf/2509.04446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04446]] Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models(https://arxiv.org/abs/2509.04446)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models have demonstrated significant capabilities to generate diverse and detailed visuals in various domains, and story visualization is emerging as a particularly promising application. However, as their use in real-world creative domains increases, the need for providing enhanced control, refinement, and the ability to modify images post-generation in a consistent manner becomes an important challenge. Existing methods often lack the flexibility to apply fine or coarse edits while maintaining visual and narrative consistency across multiple frames, preventing creators from seamlessly crafting and refining their visual stories. To address these challenges, we introduce Plot'n Polish, a zero-shot framework that enables consistent story generation and provides fine-grained control over story visualizations at various levels of detail.</li>
<li><strong>摘要：</strong>文本到图像扩散模型已显示出在各个领域中产生多样化和详细的视觉效果的重要功能，而故事的可视化正在成为一种特别有希望的应用。但是，随着它们在现实世界中创造性领域的使用增加，提供增强的控制，改进和以一致的方式修改图像的能力的需求成为重要的挑战。现有的方法通常缺乏灵活的应用精细或粗略的编辑，同时保持多个框架的视觉和叙事一致性，从而阻止创作者无缝制作和完善其视觉故事。为了应对这些挑战，我们介绍了Plot'n Polish，这是一个零拍的框架，可以使故事产生一致，并以各种细节层面提供对故事可视化的精细控制。</li>
</ul>

<h3>Title: TRUST-VL: An Explainable News Assistant for General Multimodal Misinformation Detection</h3>
<ul>
<li><strong>Authors: </strong>Zehong Yan, Peng Qi, Wynne Hsu, Mong Li Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04448">https://arxiv.org/abs/2509.04448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04448">https://arxiv.org/pdf/2509.04448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04448]] TRUST-VL: An Explainable News Assistant for General Multimodal Misinformation Detection(https://arxiv.org/abs/2509.04448)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Multimodal misinformation, encompassing textual, visual, and cross-modal distortions, poses an increasing societal threat that is amplified by generative AI. Existing methods typically focus on a single type of distortion and struggle to generalize to unseen scenarios. In this work, we observe that different distortion types share common reasoning capabilities while also requiring task-specific skills. We hypothesize that joint training across distortion types facilitates knowledge sharing and enhances the model's ability to generalize. To this end, we introduce TRUST-VL, a unified and explainable vision-language model for general multimodal misinformation detection. TRUST-VL incorporates a novel Question-Aware Visual Amplifier module, designed to extract task-specific visual features. To support training, we also construct TRUST-Instruct, a large-scale instruction dataset containing 198K samples featuring structured reasoning chains aligned with human fact-checking workflows. Extensive experiments on both in-domain and zero-shot benchmarks demonstrate that TRUST-VL achieves state-of-the-art performance, while also offering strong generalization and interpretability.</li>
<li><strong>摘要：</strong>多模式的错误信息，包括文本，视觉和跨模式扭曲，构成了越来越多的社会威胁，这种威胁被生成的AI放大。现有方法通常集中于单一类型的失真，并难以推广到看不见的情况。在这项工作中，我们观察到不同的失真类型具有共同的推理能力，同时还需要特定于任务的技能。我们假设跨失真类型的联合培训有助于知识共享并增强模型的概括能力。为此，我们介绍了Trust-VL，这是一种用于一般多模式错误信息检测的统一且可解释的视觉模型。 Trust-VL结合了一个新颖的问题吸引的视觉放大器模块，旨在提取特定于任务的视觉特征。为了支持培训，我们还构建了信任教学，这是一个大规模的指令数据集，其中包含198K样本，其中包含结构化推理链，与人类事实检查工作流相符。对内域和零拍的基准的广泛实验表明，Trust-VL实现了最先进的性能，同时还提供了强大的概括和可解释性。</li>
</ul>

<h3>Title: Virtual Fitting Room: Generating Arbitrarily Long Videos of Virtual Try-On from a Single Image -- Technical Preview</h3>
<ul>
<li><strong>Authors: </strong>Jun-Kun Chen, Aayush Bansal, Minh Phuoc Vo, Yu-Xiong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04450">https://arxiv.org/abs/2509.04450</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04450">https://arxiv.org/pdf/2509.04450</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04450]] Virtual Fitting Room: Generating Arbitrarily Long Videos of Virtual Try-On from a Single Image -- Technical Preview(https://arxiv.org/abs/2509.04450)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We introduce the Virtual Fitting Room (VFR), a novel video generative model that produces arbitrarily long virtual try-on videos. Our VFR models long video generation tasks as an auto-regressive, segment-by-segment generation process, eliminating the need for resource-intensive generation and lengthy video data, while providing the flexibility to generate videos of arbitrary length. The key challenges of this task are twofold: ensuring local smoothness between adjacent segments and maintaining global temporal consistency across different segments. To address these challenges, we propose our VFR framework, which ensures smoothness through a prefix video condition and enforces consistency with the anchor video -- a 360-degree video that comprehensively captures the human's wholebody appearance. Our VFR generates minute-scale virtual try-on videos with both local smoothness and global temporal consistency under various motions, making it a pioneering work in long virtual try-on video generation.</li>
<li><strong>摘要：</strong>我们介绍了虚拟拟合室（VFR），这是一种新颖的视频生成模型，可制作任意长的虚拟尝试视频。我们的VFR将长时间的视频生成任务模型为自动回火，逐段生成过程，消除了对资源密集型生成和冗长视频数据的需求，同时提供了生成任意长度视频的灵活性。该任务的主要挑战是双重的：确保相邻细分市场之间的局部平滑性并在不同细分市场之间保持全球时间一致性。为了应对这些挑战，我们提出了VFR框架，该框架可以通过前缀视频条件确保平滑度，并与锚视频保持一致性 - 一个360度的视频，可全面捕获人类的整体外观。我们的VFR在各种动作下生成了具有本地平滑度和全球时间一致性的微小尺度虚拟试用视频，这使其成为长期虚拟尝试视频生成的开创性工作。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
