<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-11-10</h1>
<h3>Title: Stateful KV Cache Management for LLMs: Balancing Space, Time, Accuracy, and Positional Fidelity</h3>
<ul>
<li><strong>Authors: </strong>Pratik Poudel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04686">https://arxiv.org/abs/2511.04686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04686">https://arxiv.org/pdf/2511.04686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04686]] Stateful KV Cache Management for LLMs: Balancing Space, Time, Accuracy, and Positional Fidelity(https://arxiv.org/abs/2511.04686)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The Key-Value (KV) cache is integral to efficient autoregressive inference in large language models (LLMs), yet its unbounded growth in stateful multi-turn scenarios presents major challenges. This paper examines the interplay between KV cache management strategies, the architectural context limits of models like meta-llama/Meta-Llama-3-8b-instruct, and the often-overlooked integrity of positional encodings. Through empirical analysis using a stateful benchmarking framework, we show that LLM generation quality degrades sharply when the accumulated KV cache approaches or exceeds the model's trained context window (e.g., 8192 tokens for Llama 3), a failure mode distinct from GPU memory exhaustion. Common eviction strategies, even high-retention ones (e.g., 99% via AttentionTop), can worsen performance if they disrupt positional coherence. Because LLMs rely on consistent positional signals (e.g., RoPE), compacting a cache by removing non-contiguous tokens can scramble these signals and lead to degenerative outputs. We further show that simple strategies preserving contiguous context blocks (e.g., keeping an initial "gist") can yield more coherent generations than complex or positionally disruptive ones. We advocate for eviction techniques that respect architectural limits, preserve positional structure, and view "cache health" holistically beyond mere size.</li>
<li><strong>摘要：</strong>键值 (KV) 缓存是大型语言模型 (LLM) 中高效自回归推理不可或缺的一部分，但其在有状态多轮场景中的无限增长带来了重大挑战。本文研究了 KV 缓存管理策略、meta-llama/Meta-Llama-3-8b-instruct 等模型的架构上下文限制以及经常被忽视的位置编码的完整性之间的相互作用。通过使用状态基准测试框架的实证分析，我们表明，当累积的 KV 缓存接近或超过模型的训练上下文窗口（例如，Llama 3 的 8192 个令牌）时，LLM 生成质量会急剧下降，这是一种与 GPU 内存耗尽不同的故障模式。常见的驱逐策略，即使是高保留策略（例如，99% 通过 AttentionTop），如果破坏位置连贯性，也会使性能恶化。由于 LLM 依赖于一致的位置信号（例如 RoPE），因此通过删除不连续的标记来压缩缓存可能会扰乱这些信号并导致退化输出。我们进一步表明，保留连续上下文块的简单策略（例如，保留初始“要点”）可以比复杂或位置破坏性的策略产生更连贯的生成。我们提倡尊重架构限制、保留位置结构并从整体上看待“缓存健康状况”而不仅仅是大小的驱逐技术。</li>
</ul>

<h3>Title: Knowledge-based anomaly detection for identifying network-induced shape artifacts</h3>
<ul>
<li><strong>Authors: </strong>Rucha Deshpande, Tahsin Rahman, Miguel Lago, Adarsh Subbaswamy, Jana G. Delfino, Ghada Zamzmi, Elim Thompson, Aldo Badano, Seyed Kahaki</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04729">https://arxiv.org/abs/2511.04729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04729">https://arxiv.org/pdf/2511.04729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04729]] Knowledge-based anomaly detection for identifying network-induced shape artifacts(https://arxiv.org/abs/2511.04729)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Synthetic data provides a promising approach to address data scarcity for training machine learning models; however, adoption without proper quality assessments may introduce artifacts, distortions, and unrealistic features that compromise model performance and clinical utility. This work introduces a novel knowledge-based anomaly detection method for detecting network-induced shape artifacts in synthetic images. The introduced method utilizes a two-stage framework comprising (i) a novel feature extractor that constructs a specialized feature space by analyzing the per-image distribution of angle gradients along anatomical boundaries, and (ii) an isolation forest-based anomaly detector. We demonstrate the effectiveness of the method for identifying network-induced shape artifacts in two synthetic mammography datasets from models trained on CSAW-M and VinDr-Mammo patient datasets respectively. Quantitative evaluation shows that the method successfully concentrates artifacts in the most anomalous partition (1st percentile), with AUC values of 0.97 (CSAW-syn) and 0.91 (VMLO-syn). In addition, a reader study involving three imaging scientists confirmed that images identified by the method as containing network-induced shape artifacts were also flagged by human readers with mean agreement rates of 66% (CSAW-syn) and 68% (VMLO-syn) for the most anomalous partition, approximately 1.5-2 times higher than the least anomalous partition. Kendall-Tau correlations between algorithmic and human rankings were 0.45 and 0.43 for the two datasets, indicating reasonable agreement despite the challenging nature of subtle artifact detection. This method is a step forward in the responsible use of synthetic data, as it allows developers to evaluate synthetic images for known anatomic constraints and pinpoint and address specific issues to improve the overall quality of a synthetic dataset.</li>
<li><strong>摘要：</strong>合成数据为解决训练机器学习模型的数据稀缺问题提供了一种有前景的方法；然而，在没有适当质量评估的情况下采用可能会引入伪影、扭曲和不切实际的特征，从而损害模型性能和临床实用性。这项工作介绍了一种新颖的基于知识的异常检测方法，用于检测合成图像中网络引起的形状伪影。所引入的方法利用两阶段框架，包括（i）新颖的特征提取器，通过分析沿着解剖边界的角度梯度的每图像分布来构建专门的特征空间，以及（ii）基于隔离森林的异常检测器。我们证明了该方法在两个合成乳腺 X 线摄影数据集中识别网络引起的形状伪影的方法的有效性，这些数据集分别来自 CSAW-M 和 VinDr-Mammo 患者数据集训练的模型。定量评估表明，该方法成功地将伪影集中在最异常的分区（第一个百分位）中，AUC 值为 0.97 (CSAW-syn) 和 0.91 (VMLO-syn)。此外，一项涉及三名成像科学家的读者研究证实，通过该方法识别出的包含网络引起的形状伪影的图像也被人类读者标记，对于最异常的分区，平均一致性率为 66% (CSAW-syn) 和 68% (VMLO-syn)，大约比最不异常的分区高 1.5-2 倍。两个数据集的算法排名和人类排名之间的 Kendall-Tau 相关性分别为 0.45 和 0.43，这表明尽管细微伪影检测具有挑战性，但仍具有合理的一致性。这种方法是负责任地使用合成数据的一个进步，因为它允许开发人员评估合成图像的已知解剖约束，并查明和解决特定问题，以提高合成数据集的整体质量。</li>
</ul>

<h3>Title: CPO: Condition Preference Optimization for Controllable Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Zonglin Lyu, Ming Li, Xinxin Liu, Chen Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04753">https://arxiv.org/abs/2511.04753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04753">https://arxiv.org/pdf/2511.04753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04753]] CPO: Condition Preference Optimization for Controllable Image Generation(https://arxiv.org/abs/2511.04753)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>To enhance controllability in text-to-image generation, ControlNet introduces image-based control signals, while ControlNet++ improves pixel-level cycle consistency between generated images and the input control signal. To avoid the prohibitive cost of back-propagating through the sampling process, ControlNet++ optimizes only low-noise timesteps (e.g., $t < 200$) using a single-step approximation, which not only ignores the contribution of high-noise timesteps but also introduces additional approximation errors. A straightforward alternative for optimizing controllability across all timesteps is Direct Preference Optimization (DPO), a fine-tuning method that increases model preference for more controllable images ($I^{w}$) over less controllable ones ($I^{l}$). However, due to uncertainty in generative models, it is difficult to ensure that win--lose image pairs differ only in controllability while keeping other factors, such as image quality, fixed. To address this, we propose performing preference learning over control conditions rather than generated images. Specifically, we construct winning and losing control signals, $\mathbf{c}^{w}$ and $\mathbf{c}^{l}$, and train the model to prefer $\mathbf{c}^{w}$. This method, which we term \textit{Condition Preference Optimization} (CPO), eliminates confounding factors and yields a low-variance training objective. Our approach theoretically exhibits lower contrastive loss variance than DPO and empirically achieves superior results. Moreover, CPO requires less computation and storage for dataset curation. Extensive experiments show that CPO significantly improves controllability over the state-of-the-art ControlNet++ across multiple control types: over $10\%$ error rate reduction in segmentation, $70$--$80\%$ in human pose, and consistent $2$--$5\%$ reductions in edge and depth maps.</li>
<li><strong>摘要：</strong>为了增强文本到图像生成的可控性，ControlNet 引入了基于图像的控制信号，而 ControlNet++ 则提高了生成图像和输入控制信号之间的像素级循环一致性。为了避免采样过程中反向传播的高昂成本，ControlNet++ 使用单步近似仅优化低噪声时间步长（例如 $t < 200$），这不仅忽略了高噪声时间步长的贡献，而且还引入了额外的近似误差。优化所有时间步的可控性的一个直接替代方案是直接偏好优化 (DPO)，这是一种微调方法，可提高模型对可控性较高的图像 ($I^{w}$) 的偏好，而不是可控性较差的图像 ($I^{l}$)。然而，由于生成模型的不确定性，很难确保输赢图像对仅在可控性上有所不同，同时保持其他因素（例如图像质量）固定。为了解决这个问题，我们建议对控制条件而不是生成的图像进行偏好学习。具体来说，我们构建获胜和失败的控制信号 $\mathbf{c}^{w}$ 和 $\mathbf{c}^{l}$，并训练模型更喜欢 $\mathbf{c}^{w}$。这种方法，我们称之为 \textit{条件偏好优化} (CPO)，消除了混杂因素并产生了低方差的训练目标。我们的方法理论上表现出比 DPO 更低的对比损失方差，并且在经验上取得了更好的结果。此外，CPO 需要较少的数据集管理计算和存储。大量实验表明，与最先进的 ControlNet++ 相比，CPO 在多种控制类型上显着提高了可控性：分割错误率降低了超过 $10\%$，人体姿势错误率降低了 $70$--$80\%$，边缘和深度图一致降低了 $2$-$5\%$。</li>
</ul>

<h3>Title: DuetServe: Harmonizing Prefill and Decode for LLM Serving via Adaptive GPU Multiplexing</h3>
<ul>
<li><strong>Authors: </strong>Lei Gao, Chaoyi Jiang, Hossein Entezari Zarch, Daniel Wong, Murali Annavaram</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04791">https://arxiv.org/abs/2511.04791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04791">https://arxiv.org/pdf/2511.04791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04791]] DuetServe: Harmonizing Prefill and Decode for LLM Serving via Adaptive GPU Multiplexing(https://arxiv.org/abs/2511.04791)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Modern LLM serving systems must sustain high throughput while meeting strict latency SLOs across two distinct inference phases: compute-intensive prefill and memory-bound decode phases. Existing approaches either (1) aggregate both phases on shared GPUs, leading to interference between prefill and decode phases, which degrades time-between-tokens (TBT); or (2) disaggregate the two phases across GPUs, improving latency but wasting resources through duplicated models and KV cache transfers. We present DuetServe, a unified LLM serving framework that achieves disaggregation-level isolation within a single GPU. DuetServe operates in aggregated mode by default and dynamically activates SM-level GPU spatial multiplexing when TBT degradation is predicted. Its key idea is to decouple prefill and decode execution only when needed through fine-grained, adaptive SM partitioning that provides phase isolation only when contention threatens latency service level objectives (SLOs). DuetServe integrates (1) an attention-aware roofline model to forecast iteration latency, (2) a partitioning optimizer that selects the optimal SM split to maximize throughput under TBT constraints, and (3) an interruption-free execution engine that eliminates CPU-GPU synchronization overhead. Evaluations show that DuetServe improves total throughput by up to 1.3x while maintaining low generation latency compared to state-of-the-art frameworks.</li>
<li><strong>摘要：</strong>现代 LLM 服务系统必须维持高吞吐量，同时满足两个不同推理阶段的严格延迟 SLO：计算密集型预填充和内存限制解码阶段。现有方法要么 (1) 在共享 GPU 上聚合两个阶段，导致预填充和解码阶段之间的干扰，从而降低令牌之间的时间 (TBT)； (2) 在 GPU 上分解这两个阶段，通过重复模型和 KV 缓存传输来改善延迟，但浪费资源。我们推出 DuetServe，一个统一的 LLM 服务框架，可在单个 GPU 内实现分解级别的隔离。 DuetServe 默认以聚合模式运行，并在预测 TBT 退化时动态激活 SM 级 GPU 空间复用。其关键思想是仅在需要时通过细粒度的自适应 SM 分区来解耦预填充和解码执行，仅当争用威胁到延迟服务级别目标 (SLO) 时才提供阶段隔离。 DuetServe 集成了 (1) 注意力感知屋顶线模型来预测迭代延迟，(2) 分区优化器，选择最佳 SM 分割以在 TBT 约束下最大化吞吐量，以及 (3) 消除 CPU-GPU 同步开销的无中断执行引擎。评估表明，与最先进的框架相比，DuetServe 将总吞吐量提高了 1.3 倍，同时保持了较低的生成延迟。</li>
</ul>

<h3>Title: Prompt-Based Safety Guidance Is Ineffective for Unlearned Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Jiwoo Shin, Byeonghu Na, Mina Kang, Wonhyeok Choi, Il-chul Moon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04834">https://arxiv.org/abs/2511.04834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04834">https://arxiv.org/pdf/2511.04834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04834]] Prompt-Based Safety Guidance Is Ineffective for Unlearned Text-to-Image Diffusion Models(https://arxiv.org/abs/2511.04834)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in text-to-image generative models have raised concerns about their potential to produce harmful content when provided with malicious input text prompts. To address this issue, two main approaches have emerged: (1) fine-tuning the model to unlearn harmful concepts and (2) training-free guidance methods that leverage negative prompts. However, we observe that combining these two orthogonal approaches often leads to marginal or even degraded defense performance. This observation indicates a critical incompatibility between two paradigms, which hinders their combined effectiveness. In this work, we address this issue by proposing a conceptually simple yet experimentally robust method: replacing the negative prompts used in training-free methods with implicit negative embeddings obtained through concept inversion. Our method requires no modification to either approach and can be easily integrated into existing pipelines. We experimentally validate its effectiveness on nudity and violence benchmarks, demonstrating consistent improvements in defense success rate while preserving the core semantics of input prompts.</li>
<li><strong>摘要：</strong>文本到图像生成模型的最新进展引起了人们的担忧，即当提供恶意输入文本提示时，它们可能会产生有害内容。为了解决这个问题，出现了两种主要方法：（1）微调模型以忘却有害概念；（2）利用负面提示的免培训指导方法。然而，我们观察到，结合这两种正交方法通常会导致防御性能边际甚至下降。这一观察结果表明两种范式之间存在严重的不兼容性，这阻碍了它们的综合有效性。在这项工作中，我们通过提出一种概念上简单但实验上稳健的方法来解决这个问题：用通过概念反转获得的隐式负嵌入替换免训练方法中使用的负提示。我们的方法不需要对任何一种方法进行修改，并且可以轻松集成到现有的管道中。我们通过实验验证了其在裸露和暴力基准上的有效性，证明了防御成功率的持续提高，同时保留了输入提示的核心语义。</li>
</ul>

<h3>Title: Sublinear iterations can suffice even for DDPMs</h3>
<ul>
<li><strong>Authors: </strong>Matthew S. Zhang, Stephen Huan, Jerry Huang, Nicholas M. Boffi, Sitan Chen, Sinho Chewi</a></li>
<li><strong>Subjects: </strong>cs.LG, math.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04844">https://arxiv.org/abs/2511.04844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04844">https://arxiv.org/pdf/2511.04844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04844]] Sublinear iterations can suffice even for DDPMs(https://arxiv.org/abs/2511.04844)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>SDE-based methods such as denoising diffusion probabilistic models (DDPMs) have shown remarkable success in real-world sample generation tasks. Prior analyses of DDPMs have been focused on the exponential Euler discretization, showing guarantees that generally depend at least linearly on the dimension or initial Fisher information. Inspired by works in log-concave sampling (Shen and Lee, 2019), we analyze an integrator -- the denoising diffusion randomized midpoint method (DDRaM) -- that leverages an additional randomized midpoint to better approximate the SDE. Using a recently-developed analytic framework called the "shifted composition rule", we show that this algorithm enjoys favorable discretization properties under appropriate smoothness assumptions, with sublinear $\widetilde{O}(\sqrt{d})$ score evaluations needed to ensure convergence. This is the first sublinear complexity bound for pure DDPM sampling -- prior works which obtained such bounds worked instead with ODE-based sampling and had to make modifications to the sampler which deviate from how they are used in practice. We also provide experimental validation of the advantages of our method, showing that it performs well in practice with pre-trained image synthesis models.</li>
<li><strong>摘要：</strong>基于 SDE 的方法，例如去噪扩散概率模型 (DDPM)，在现实世界的样本生成任务中取得了显着的成功。 DDPM 的先前分析主要集中在指数欧拉离散化上，显示出通常至少线性依赖于维数或初始 Fisher 信息的保证。受到对数凹采样工作（Shen 和 Lee，2019）的启发，我们分析了一种积分器——去噪扩散随机中点法 (DDRaM)——利用额外的随机中点来更好地逼近 SDE。使用最近开发的称为“移位合成规则”的分析框架，我们表明该算法在适当的平滑度假设下具有良好的离散化特性，并且需要次线性 $\widetilde{O}(\sqrt{d})$ 分数评估来确保收敛。这是纯 DDPM 采样的第一个次线性复杂度界限——之前获得这种界限的工作使用基于 ODE 的采样，并且必须对采样器进行修改，这偏离了它们在实践中的使用方式。我们还对我们的方法的优点进行了实验验证，表明它在预先训练的图像合成模型的实践中表现良好。</li>
</ul>

<h3>Title: SigmaDock: Untwisting Molecular Docking With Fragment-Based SE(3) Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Alvaro Prat, Leo Zhang, Charlotte M. Deane, Yee Whye Teh, Garrett M. Morris</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04854">https://arxiv.org/abs/2511.04854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04854">https://arxiv.org/pdf/2511.04854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04854]] SigmaDock: Untwisting Molecular Docking With Fragment-Based SE(3) Diffusion(https://arxiv.org/abs/2511.04854)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Determining the binding pose of a ligand to a protein, known as molecular docking, is a fundamental task in drug discovery. Generative approaches promise faster, improved, and more diverse pose sampling than physics-based methods, but are often hindered by chemically implausible outputs, poor generalisability, and high computational cost. To address these challenges, we introduce a novel fragmentation scheme, leveraging inductive biases from structural chemistry, to decompose ligands into rigid-body fragments. Building on this decomposition, we present SigmaDock, an SE(3) Riemannian diffusion model that generates poses by learning to reassemble these rigid bodies within the binding pocket. By operating at the level of fragments in SE(3), SigmaDock exploits well-established geometric priors while avoiding overly complex diffusion processes and unstable training dynamics. Experimentally, we show SigmaDock achieves state-of-the-art performance, reaching Top-1 success rates (RMSD<2 & PB-valid) above 79.9% on the PoseBusters set, compared to 12.7-30.8% reported by recent deep learning approaches, whilst demonstrating consistent generalisation to unseen proteins. SigmaDock is the first deep learning approach to surpass classical physics-based docking under the PB train-test split, marking a significant leap forward in the reliability and feasibility of deep learning for molecular modelling.</li>
<li><strong>摘要：</strong>确定配体与蛋白质的结合姿势（称为分子对接）是药物发现的一项基本任务。与基于物理的方法相比，生成方法有望实现更快、改进和更多样化的姿态采样，但往往受到化学上不可信的输出、较差的通用性和高计算成本的阻碍。为了应对这些挑战，我们引入了一种新颖的碎片方案，利用结构化学的归纳偏差，将配体分解成刚体片段。在此分解的基础上，我们提出了 SigmaDock，这是一种 SE(3) 黎曼扩散模型，它通过学习在绑定口袋内重新组装这些刚体来生成姿势。通过在 SE(3) 中的片段级别进行操作，SigmaDock 利用了完善的几何先验，同时避免了过于复杂的扩散过程和不稳定的训练动态。通过实验，我们表明 SigmaDock 实现了最先进的性能，在 PoseBusters 集上达到了 79.9% 以上的 Top-1 成功率（RMSD<2 和 PB 有效），而最近的深度学习方法报告的成功率为 12.7-30.8%，同时证明了对看不见的蛋白质的一致泛化。 SigmaDock是第一个在PB训练-测试分割下超越基于经典物理的对接的深度学习方法，标志着深度学习用于分子建模的可靠性和可行性的重大飞跃。</li>
</ul>

<h3>Title: Clinical-ComBAT: a diffusion-weighted MRI harmonization method for clinical applications</h3>
<ul>
<li><strong>Authors: </strong>Gabriel Girard, Manon Edde, Félix Dumais, Yoan David, Matthieu Dumont, Guillaume Theaud, Jean-Christophe Houde, Arnaud Boré, Maxime Descoteaux, Pierre-Marc Jodoin</a></li>
<li><strong>Subjects: </strong>cs.CV, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04871">https://arxiv.org/abs/2511.04871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04871">https://arxiv.org/pdf/2511.04871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04871]] Clinical-ComBAT: a diffusion-weighted MRI harmonization method for clinical applications(https://arxiv.org/abs/2511.04871)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion-weighted magnetic resonance imaging (DW-MRI) derived scalar maps are effective for assessing neurodegenerative diseases and microstructural properties of white matter in large number of brain conditions. However, DW-MRI inherently limits the combination of data from multiple acquisition sites without harmonization to mitigate scanner-specific biases. While the widely used ComBAT method reduces site effects in research, its reliance on linear covariate relationships, homogeneous populations, fixed site numbers, and well populated sites constrains its clinical use. To overcome these limitations, we propose Clinical-ComBAT, a method designed for real-world clinical scenarios. Clinical-ComBAT harmonizes each site independently, enabling flexibility as new data and clinics are introduced. It incorporates a non-linear polynomial data model, site-specific harmonization referenced to a normative site, and variance priors adaptable to small cohorts. It further includes hyperparameter tuning and a goodness-of-fit metric for harmonization assessment. We demonstrate its effectiveness on simulated and real data, showing improved alignment of diffusion metrics and enhanced applicability for normative modeling.</li>
<li><strong>摘要：</strong>扩散加权磁共振成像 (DW-MRI) 衍生的标量图可有效评估大量大脑疾病中的神经退行性疾病和白质的微观结构特性。然而，DW-MRI 本质上限制了来自多个采集站点的数据组合，而没有协调一致以减轻扫描仪特定的偏差。虽然广泛使用的 ComBAT 方法减少了研究中的位点效应，但其对线性协变量关系、同质群体、固定位点数量和人口密集的位点的依赖限制了其临床应用。为了克服这些限制，我们提出了 Clinical-ComBAT，这是一种专为真实临床场景设计的方法。 Clinical-ComBAT 独立协调每个站点，在引入新数据和诊所时实现灵活性。它结合了非线性多项式数据模型、参考规范站点的特定站点协调以及适用于小群体的方差先验。它还包括超参数调整和用于协调评估的拟合优度指标。我们证明了其在模拟和真实数据上的有效性，显示了扩散指标的改进一致性和规范建模的增强适用性。</li>
</ul>

<h3>Title: Learning to Restore Multi-Degraded Images via Ingredient Decoupling and Task-Aware Path Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Hu Gao, Xiaoning Lei, Ying Zhang, Xichen Xu, Guannan Jiang, Lizhuang Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04920">https://arxiv.org/abs/2511.04920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04920">https://arxiv.org/pdf/2511.04920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04920]] Learning to Restore Multi-Degraded Images via Ingredient Decoupling and Task-Aware Path Adaptation(https://arxiv.org/abs/2511.04920)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Image restoration (IR) aims to recover clean images from degraded observations. Despite remarkable progress, most existing methods focus on a single degradation type, whereas real-world images often suffer from multiple coexisting degradations, such as rain, noise, and haze coexisting in a single image, which limits their practical effectiveness. In this paper, we propose an adaptive multi-degradation image restoration network that reconstructs images by leveraging decoupled representations of degradation ingredients to guide path selection. Specifically, we design a degradation ingredient decoupling block (DIDBlock) in the encoder to separate degradation ingredients statistically by integrating spatial and frequency domain information, enhancing the recognition of multiple degradation types and making their feature representations independent. In addition, we present fusion block (FBlock) to integrate degradation information across all levels using learnable matrices. In the decoder, we further introduce a task adaptation block (TABlock) that dynamically activates or fuses functional branches based on the multi-degradation representation, flexibly selecting optimal restoration paths under diverse degradation conditions. The resulting tightly integrated architecture, termed IMDNet, is extensively validated through experiments, showing superior performance on multi-degradation restoration while maintaining strong competitiveness on single-degradation tasks.</li>
<li><strong>摘要：</strong>图像恢复（IR）旨在从退化的观测中恢复清晰的图像。尽管取得了显着的进展，但大多数现有方法都专注于单一退化类型，而现实世界的图像经常遭受多种共存的退化，例如雨、噪声和雾霾共存于单个图像中，这限制了它们的实际有效性。在本文中，我们提出了一种自适应多退化图像恢复网络，该网络通过利用退化成分的解耦表示来重建图像以指导路径选择。具体来说，我们在编码器中设计了一个退化成分解耦块（DIDBlock），通过整合空间和频域信息来统计分离退化成分，增强对多种退化类型的识别并使其特征表示独立。此外，我们提出了融合块（FBlock），使用可学习矩阵来集成所有级别的退化信息。在解码器中，我们进一步引入了任务适应块（TABlock），它基于多退化表示动态激活或融合功能分支，在不同退化条件下灵活选择最佳恢复路径。由此产生的紧密集成的架构，称为 IMDNet，通过实验进行了广泛验证，在多降解恢复方面表现出卓越的性能，同时在单降解任务上保持强大的竞争力。</li>
</ul>

<h3>Title: Leak@$k$: Unlearning Does Not Make LLMs Forget Under Probabilistic Decoding</h3>
<ul>
<li><strong>Authors: </strong>Hadi Reisizadeh, Jiajun Ruan, Yiwei Chen, Soumyadeep Pal, Sijia Liu, Mingyi Hong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04934">https://arxiv.org/abs/2511.04934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04934">https://arxiv.org/pdf/2511.04934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04934]] Leak@$k$: Unlearning Does Not Make LLMs Forget Under Probabilistic Decoding(https://arxiv.org/abs/2511.04934)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Unlearning in large language models (LLMs) is critical for regulatory compliance and for building ethical generative AI systems that avoid producing private, toxic, illegal, or copyrighted content. Despite rapid progress, in this work we show that \textit{almost all} existing unlearning methods fail to achieve true forgetting in practice. Specifically, while evaluations of these `unlearned' models under deterministic (greedy) decoding often suggest successful knowledge removal using standard benchmarks (as has been done in the literature), we show that sensitive information reliably resurfaces when models are sampled with standard probabilistic decoding. To rigorously capture this vulnerability, we introduce \texttt{leak@$k$}, a new meta-evaluation metric that quantifies the likelihood of forgotten knowledge reappearing when generating $k$ samples from the model under realistic decoding strategies. Using three widely adopted benchmarks, TOFU, MUSE, and WMDP, we conduct the first large-scale, systematic study of unlearning reliability using our newly defined \texttt{leak@$k$} metric. Our findings demonstrate that knowledge leakage persists across methods and tasks, underscoring that current state-of-the-art unlearning techniques provide only limited forgetting and highlighting the urgent need for more robust approaches to LLM unlearning.</li>
<li><strong>摘要：</strong>忘却大语言模型 (LLM) 中的学习对于遵守法规和构建道德生成人工智能系统至关重要，以避免产生私人、有毒、非法或受版权保护的内容。尽管进展很快，但在这项工作中，我们表明\textit{几乎所有}现有的遗忘方法在实践中都无法实现真正​​的遗忘。具体来说，虽然在确定性（贪婪）解码下对这些“未学习”模型的评估通常表明使用标准基准成功地去除了知识（正如文献中所做的那样），但我们表明，当使用标准概率解码对模型进行采样时，敏感信息可靠地重新出现。为了严格捕获此漏洞，我们引入了 \texttt{leak@$k$}，这是一种新的元评估指标，可量化在实际解码策略下从模型生成 $k$ 样本时被遗忘的知识重新出现的可能性。使用 TOFU、MUSE 和 WMDP 三个广泛采用的基准，我们使用新定义的 \texttt{leak@$k$} 指标对遗忘可靠性进行了首次大规模、系统的研究。我们的研究结果表明，知识泄漏在方法和任务中持续存在，强调了当前最先进的遗忘技术只能提供有限的遗忘，并强调迫切需要更强大的法学硕士遗忘方法。</li>
</ul>

<h3>Title: A benchmark multimodal oro-dental dataset for large vision-language models</h3>
<ul>
<li><strong>Authors: </strong>Haoxin Lv, Ijazul Haq, Jin Du, Jiaxin Ma, Binnian Zhu, Xiaobing Dang, Chaoan Liang, Ruxu Du, Yingjie Zhang, Muhammad Saqib</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04948">https://arxiv.org/abs/2511.04948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04948">https://arxiv.org/pdf/2511.04948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04948]] A benchmark multimodal oro-dental dataset for large vision-language models(https://arxiv.org/abs/2511.04948)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The advancement of artificial intelligence in oral healthcare relies on the availability of large-scale multimodal datasets that capture the complexity of clinical practice. In this paper, we present a comprehensive multimodal dataset, comprising 8775 dental checkups from 4800 patients collected over eight years (2018-2025), with patients ranging from 10 to 90 years of age. The dataset includes 50000 intraoral images, 8056 radiographs, and detailed textual records, including diagnoses, treatment plans, and follow-up notes. The data were collected under standard ethical guidelines and annotated for benchmarking. To demonstrate its utility, we fine-tuned state-of-the-art large vision-language models, Qwen-VL 3B and 7B, and evaluated them on two tasks: classification of six oro-dental anomalies and generation of complete diagnostic reports from multimodal inputs. We compared the fine-tuned models with their base counterparts and GPT-4o. The fine-tuned models achieved substantial gains over these baselines, validating the dataset and underscoring its effectiveness in advancing AI-driven oro-dental healthcare solutions. The dataset is publicly available, providing an essential resource for future research in AI dentistry.</li>
<li><strong>摘要：</strong>人工智能在口腔保健领域的进步依赖于捕获临床实践复杂性的大规模多模式数据集的可用性。在本文中，我们提出了一个全面的多模式数据集，包括八年（2018-2025）年间收集的 4800 名患者的 8775 次牙科检查，患者年龄范围为 10 至 90 岁。该数据集包括 50000 张口内图像、8056 张射线照片和详细的文本记录，包括诊断、治疗计划和随访记录。数据是根据标准道德准则收集的，并注释用于基准测试。为了证明其实用性，我们对最先进的大型视觉语言模型 Qwen-VL 3B 和 7B 进行了微调，并在两项任务上对其进行了评估：六种口腔牙齿异常的分类以及从多模式输入生成完整的诊断报告。我们将微调模型与其基础模型和 GPT-4o 进行了比较。经过微调的模型在这些基线上取得了显着的进步，验证了数据集并强调了其在推进人工智能驱动的口腔牙科医疗保健解决方案方面的有效性。该数据集是公开的，为人工智能牙科的未来研究提供了重要资源。</li>
</ul>

<h3>Title: DeepForgeSeal: Latent Space-Driven Semi-Fragile Watermarking for Deepfake Detection Using Multi-Agent Adversarial Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Tharindu Fernando, Clinton Fookes, Sridha Sridharan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04949">https://arxiv.org/abs/2511.04949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04949">https://arxiv.org/pdf/2511.04949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04949]] DeepForgeSeal: Latent Space-Driven Semi-Fragile Watermarking for Deepfake Detection Using Multi-Agent Adversarial Reinforcement Learning(https://arxiv.org/abs/2511.04949)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Rapid advances in generative AI have led to increasingly realistic deepfakes, posing growing challenges for law enforcement and public trust. Existing passive deepfake detectors struggle to keep pace, largely due to their dependence on specific forgery artifacts, which limits their ability to generalize to new deepfake types. Proactive deepfake detection using watermarks has emerged to address the challenge of identifying high-quality synthetic media. However, these methods often struggle to balance robustness against benign distortions with sensitivity to malicious tampering. This paper introduces a novel deep learning framework that harnesses high-dimensional latent space representations and the Multi-Agent Adversarial Reinforcement Learning (MAARL) paradigm to develop a robust and adaptive watermarking approach. Specifically, we develop a learnable watermark embedder that operates in the latent space, capturing high-level image semantics, while offering precise control over message encoding and extraction. The MAARL paradigm empowers the learnable watermarking agent to pursue an optimal balance between robustness and fragility by interacting with a dynamic curriculum of benign and malicious image manipulations simulated by an adversarial attacker agent. Comprehensive evaluations on the CelebA and CelebA-HQ benchmarks reveal that our method consistently outperforms state-of-the-art approaches, achieving improvements of over 4.5% on CelebA and more than 5.3% on CelebA-HQ under challenging manipulation scenarios.</li>
<li><strong>摘要：</strong>生成式人工智能的快速发展导致了越来越真实的深度伪造，给执法和公众信任带来了越来越大的挑战。现有的被动深度换脸检测器难以跟上步伐，很大程度上是因为它们依赖于特定的伪造制品，这限制了它们推广到新的深度换脸类型的能力。使用水印的主动深度伪造检测已经出现，以解决识别高质量合成媒体的挑战。然而，这些方法通常难以平衡针对良性扭曲的鲁棒性和针对恶意篡改的敏感性。本文介绍了一种新颖的深度学习框架，该框架利用高维潜在空间表示和多智能体对抗强化学习（MAARL）范式来开发鲁棒且自适应的水印方法。具体来说，我们开发了一种可学习的水印嵌入器，它在潜在空间中运行，捕获高级图像语义，同时提供对消息编码和提取的精确控制。 MAARL 范式使可学习水印代理能够通过与对抗性攻击代理模拟的良性和恶意图像操作的动态课程进行交互，从而在鲁棒性和脆弱性之间寻求最佳平衡。对 CelebA 和 CelebA-HQ 基准的综合评估表明，我们的方法始终优于最先进的方法，在具有挑战性的操作场景下，在 CelebA 上实现了超过 4.5% 的改进，在 CelebA-HQ 上实现了超过 5.3% 的改进。</li>
</ul>

<h3>Title: Pattern-Aware Diffusion Synthesis of fMRI/dMRI with Tissue and Microstructural Refinement</h3>
<ul>
<li><strong>Authors: </strong>Xiongri Shen, Jiaqi Wang, Yi Zhong, Zhenxi Song, Leilei Zhao, Yichen Wei, Lingyan Liang, Shuqiang Wang, Baiying Lei, Demao Deng, Zhiguo Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04963">https://arxiv.org/abs/2511.04963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04963">https://arxiv.org/pdf/2511.04963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04963]] Pattern-Aware Diffusion Synthesis of fMRI/dMRI with Tissue and Microstructural Refinement(https://arxiv.org/abs/2511.04963)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Magnetic resonance imaging (MRI), especially functional MRI (fMRI) and diffusion MRI (dMRI), is essential for studying neurodegenerative diseases. However, missing modalities pose a major barrier to their clinical use. Although GAN- and diffusion model-based approaches have shown some promise in modality completion, they remain limited in fMRI-dMRI synthesis due to (1) significant BOLD vs. diffusion-weighted signal differences between fMRI and dMRI in time/gradient axis, and (2) inadequate integration of disease-related neuroanatomical patterns during generation. To address these challenges, we propose PDS, introducing two key innovations: (1) a pattern-aware dual-modal 3D diffusion framework for cross-modality learning, and (2) a tissue refinement network integrated with a efficient microstructure refinement to maintain structural fidelity and fine details. Evaluated on OASIS-3, ADNI, and in-house datasets, our method achieves state-of-the-art results, with PSNR/SSIM scores of 29.83 dB/90.84\% for fMRI synthesis (+1.54 dB/+4.12\% over baselines) and 30.00 dB/77.55\% for dMRI synthesis (+1.02 dB/+2.2\%). In clinical validation, the synthesized data show strong diagnostic performance, achieving 67.92\%/66.02\%/64.15\% accuracy (NC vs. MCI vs. AD) in hybrid real-synthetic experiments. Code is available in \href{this https URL}{PDS GitHub Repository}</li>
<li><strong>摘要：</strong>磁共振成像（MRI），特别是功能性磁共振成像（fMRI）和扩散磁共振成像（dMRI）对于研究神经退行性疾病至关重要。然而，缺失的模式对其临床应用构成了主要障碍。尽管基于 GAN 和扩散模型的方法在模态完成方面显示出了一些希望，但它们在 fMRI-dMRI 合成中仍然受到限制，因为（1）fMRI 和 dMRI 之间在时间/梯度轴上的 BOLD 与扩散加权信号差异显着，以及（2）生成过程中疾病相关神经解剖模式的整合不足。为了应对这些挑战，我们提出了 PDS，引入了两项关键创新：(1) 用于跨模态学习的模式感知双模态 3D 扩散框架，以及 (2) 与高效微观结构细化相结合的组织细化网络，以保持结构保真度和精细细节。在 OASIS-3、ADNI 和内部数据集上进行评估，我们的方法取得了最先进的结果，fMRI 合成的 PSNR/SSIM 分数为 29.83 dB/90.84\%（较基线+1.54 dB/+4.12\%），dMRI 合成的 PSNR/SSIM 分数为 30.00 dB/77.55\%（+1.02 dB/+2.2\%）。在临床验证中，合成数据显示出强大的诊断性能，在混合真实合成实验中达到了 67.92\%/66.02\%/64.15\% 的准确率（NC vs. MCI vs. AD）。代码可在 \href{此 https URL}{PDS GitHub 存储库}中找到</li>
</ul>

<h3>Title: Challenges in 3D Data Synthesis for Training Neural Networks on Topological Features</h3>
<ul>
<li><strong>Authors: </strong>Dylan Peek, Matthew P. Skerritt, Siddharth Pritam, Stephan Chalup</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04972">https://arxiv.org/abs/2511.04972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04972">https://arxiv.org/pdf/2511.04972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04972]] Challenges in 3D Data Synthesis for Training Neural Networks on Topological Features(https://arxiv.org/abs/2511.04972)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Topological Data Analysis (TDA) involves techniques of analyzing the underlying structure and connectivity of data. However, traditional methods like persistent homology can be computationally demanding, motivating the development of neural network-based estimators capable of reducing computational overhead and inference time. A key barrier to advancing these methods is the lack of labeled 3D data with class distributions and diversity tailored specifically for supervised learning in TDA tasks. To address this, we introduce a novel approach for systematically generating labeled 3D datasets using the Repulsive Surface algorithm, allowing control over topological invariants, such as hole count. The resulting dataset offers varied geometry with topological labeling, making it suitable for training and benchmarking neural network estimators. This paper uses a synthetic 3D dataset to train a genus estimator network, created using a 3D convolutional transformer architecture. An observed decrease in accuracy as deformations increase highlights the role of not just topological complexity, but also geometric complexity, when training generalized estimators. This dataset fills a gap in labeled 3D datasets and generation for training and evaluating models and techniques for TDA.</li>
<li><strong>摘要：</strong>拓扑数据分析（TDA）涉及分析数据的底层结构和连接性的技术。然而，像持久同源性这样的传统方法可能对计算要求很高，这激发了基于神经网络的估计器的开发，这些估计器能够减少计算开销和推理时间。推进这些方法的一个关键障碍是缺乏专门为 TDA 任务中的监督学习定制的具有类别分布和多样性的标记 3D 数据。为了解决这个问题，我们引入了一种新方法，使用排斥表面算法系统地生成标记的 3D 数据集，从而允许控制拓扑不变量，例如孔数。生成的数据集提供带有拓扑标记的各种几何形状，使其适合训练和基准测试神经网络估计器。本文使用合成 3D 数据集来训练属估计器网络，该网络是使用 3D 卷积变换器架构创建的。在训练广义估计器时，随着变形的增加，观察到的精度下降凸显了拓扑复杂性和几何复杂性的作用。该数据集填补了用于训练和评估 TDA 模型和技术的标记 3D 数据集和生成的空白。</li>
</ul>

<h3>Title: Less Is More: Generating Time Series with LLaMA-Style Autoregression in Simple Factorized Latent Spaces</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Li, Yifan Sun, Lei Cheng, Lewen Wang, Yang Liu, Weiqing Liu, Jianlong Li, Jiang Bian, Shikai Fang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04973">https://arxiv.org/abs/2511.04973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04973">https://arxiv.org/pdf/2511.04973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04973]] Less Is More: Generating Time Series with LLaMA-Style Autoregression in Simple Factorized Latent Spaces(https://arxiv.org/abs/2511.04973)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative models for multivariate time series are essential for data augmentation, simulation, and privacy preservation, yet current state-of-the-art diffusion-based approaches are slow and limited to fixed-length windows. We propose FAR-TS, a simple yet effective framework that combines disentangled factorization with an autoregressive Transformer over a discrete, quantized latent space to generate time series. Each time series is decomposed into a data-adaptive basis that captures static cross-channel correlations and temporal coefficients that are vector-quantized into discrete tokens. A LLaMA-style autoregressive Transformer then models these token sequences, enabling fast and controllable generation of sequences with arbitrary length. Owing to its streamlined design, FAR-TS achieves orders-of-magnitude faster generation than Diffusion-TS while preserving cross-channel correlations and an interpretable latent space, enabling high-quality and flexible time series synthesis.</li>
<li><strong>摘要：</strong>多元时间序列的生成模型对于数据增强、模拟和隐私保护至关重要，但当前最先进的基于扩散的方法速度缓慢且仅限于固定长度的窗口。我们提出了 FAR-TS，这是一个简单而有效的框架，它将解缠因式分解与自回归 Transformer 在离散的量化潜在空间上结合起来以生成时间序列。每个时间序列都被分解为数据自适应基础，捕获静态跨通道相关性和时间系数，并将其矢量量化为离散标记。然后，LLaMA 风格的自回归 Transformer 对这些标记序列进行建模，从而能够快速且可控地生成任意长度的序列。由于其简化的设计，FAR-TS 的生成速度比 Diffusion-TS 快几个数量级，同时保留跨通道相关性和可解释的潜在空间，从而实现高质量和灵活的时间序列合成。</li>
</ul>

<h3>Title: GSE: Evaluating Sticker Visual Semantic Similarity via a General Sticker Encoder</h3>
<ul>
<li><strong>Authors: </strong>Heng Er Metilda Chee, Jiayin Wang, Zhiqiang Guo, Weizhi Ma, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04977">https://arxiv.org/abs/2511.04977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04977">https://arxiv.org/pdf/2511.04977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04977]] GSE: Evaluating Sticker Visual Semantic Similarity via a General Sticker Encoder(https://arxiv.org/abs/2511.04977)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Stickers have become a popular form of visual communication, yet understanding their semantic relationships remains challenging due to their highly diverse and symbolic content. In this work, we formally {define the Sticker Semantic Similarity task} and introduce {Triple-S}, the first benchmark for this task, consisting of 905 human-annotated positive and negative sticker pairs. Through extensive evaluation, we show that existing pretrained vision and multimodal models struggle to capture nuanced sticker semantics. To address this, we propose the {General Sticker Encoder (GSE)}, a lightweight and versatile model that learns robust sticker embeddings using both Triple-S and additional datasets. GSE achieves superior performance on unseen stickers, and demonstrates strong results on downstream tasks such as emotion classification and sticker-to-sticker retrieval. By releasing both Triple-S and GSE, we provide standardized evaluation tools and robust embeddings, enabling future research in sticker understanding, retrieval, and multimodal content generation. The Triple-S benchmark and GSE have been publicly released and are available here.</li>
<li><strong>摘要：</strong>贴纸已成为一种流行的视觉交流形式，但由于其高度多样化和象征性的内容，理解它们的语义关系仍然具有挑战性。在这项工作中，我们正式{定义贴纸语义相似性任务}并引入 {Triple-S}，这是该任务的第一个基准，由 905 个人工注释的正面和负面贴纸对组成。通过广泛的评估，我们表明现有的预训练视觉和多模态模型很难捕捉微妙的贴纸语义。为了解决这个问题，我们提出了{通用贴纸编码器（GSE）}，这是一种轻量级且多功能的模型，可以使用 Triple-S 和其他数据集来学习强大的贴纸嵌入。 GSE 在看不见的贴纸上实现了卓越的性能，并在情感分类和贴纸到贴纸检索等下游任务上展示了出色的结果。通过发布 Triple-S 和 GSE，我们提供了标准化的评估工具和强大的嵌入，从而支持贴纸理解、检索和多模式内容生成方面的未来研究。 Triple-S 基准测试和 GSE 已公开发布，可在此处获取。</li>
</ul>

<h3>Title: Peptide2Mol: A Diffusion Model for Generating Small Molecules as Peptide Mimics for Targeted Protein Binding</h3>
<ul>
<li><strong>Authors: </strong>Xinheng He, Yijia Zhang, Haowei Lin, Xingang Peng, Xiangzhe Kong, Mingyu Li, Jianzhu Ma</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04984">https://arxiv.org/abs/2511.04984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04984">https://arxiv.org/pdf/2511.04984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04984]] Peptide2Mol: A Diffusion Model for Generating Small Molecules as Peptide Mimics for Targeted Protein Binding(https://arxiv.org/abs/2511.04984)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Structure-based drug design has seen significant advancements with the integration of artificial intelligence (AI), particularly in the generation of hit and lead compounds. However, most AI-driven approaches neglect the importance of endogenous protein interactions with peptides, which may result in suboptimal molecule designs. In this work, we present Peptide2Mol, an E(3)-equivariant graph neural network diffusion model that generates small molecules by referencing both the original peptide binders and their surrounding protein pocket environments. Trained on large datasets and leveraging sophisticated modeling techniques, Peptide2Mol not only achieves state-of-the-art performance in non-autoregressive generative tasks, but also produces molecules with similarity to the original peptide binder. Additionally, the model allows for molecule optimization and peptidomimetic design through a partial diffusion process. Our results highlight Peptide2Mol as an effective deep generative model for generating and optimizing bioactive small molecules from protein binding pockets.</li>
<li><strong>摘要：</strong>随着人工智能 (AI) 的集成，基于结构的药物设计取得了显着进步，特别是在命中化合物和先导化合物的生成方面。然而，大多数人工智能驱动的方法忽视了内源蛋白质与肽相互作用的重要性，这可能导致分子设计不理想。在这项工作中，我们提出了 Peptide2Mol，一种 E(3) 等变图神经网络扩散模型，它通过参考原始肽结合物及其周围的蛋白质口袋环境来生成小分子。经过大型数据集的训练并利用复杂的建模技术，Peptide2Mol 不仅在非自回归生成任务中实现了最先进的性能，而且还产生了与原始肽结合剂相似的分子。此外，该模型允许通过部分扩散过程进行分子优化和肽模拟设计。我们的结果强调 Peptide2Mol 作为一种有效的深度生成模型，用于从蛋白质结合袋生成和优化生物活性小分子。</li>
</ul>

<h3>Title: Pressure2Motion: Hierarchical Motion Synthesis from Ground Pressure with Text Guidance</h3>
<ul>
<li><strong>Authors: </strong>Zhengxuan Li, Qinhui Yang, Yiyu Zhuang, Chuan Guo, Xinxin Zuo, Xiaoxiao Long, Yao Yao, Xun Cao, Qiu Shen, Hao Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05038">https://arxiv.org/abs/2511.05038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05038">https://arxiv.org/pdf/2511.05038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05038]] Pressure2Motion: Hierarchical Motion Synthesis from Ground Pressure with Text Guidance(https://arxiv.org/abs/2511.05038)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We present Pressure2Motion, a novel motion capture algorithm that synthesizes human motion from a ground pressure sequence and text prompt. It eliminates the need for specialized lighting setups, cameras, or wearable devices, making it suitable for privacy-preserving, low-light, and low-cost motion capture scenarios. Such a task is severely ill-posed due to the indeterminate nature of the pressure signals to full-body motion. To address this issue, we introduce Pressure2Motion, a generative model that leverages pressure features as input and utilizes a text prompt as a high-level guiding constraint. Specifically, our model utilizes a dual-level feature extractor that accurately interprets pressure data, followed by a hierarchical diffusion model that discerns broad-scale movement trajectories and subtle posture adjustments. Both the physical cues gained from the pressure sequence and the semantic guidance derived from descriptive texts are leveraged to guide the motion generation with precision. To the best of our knowledge, Pressure2Motion is a pioneering work in leveraging both pressure data and linguistic priors for motion generation, and the established MPL benchmark is the first benchmark for this task. Experiments show our method generates high-fidelity, physically plausible motions, establishing a new state-of-the-art for this task. The codes and benchmarks will be publicly released upon publication.</li>
<li><strong>摘要：</strong>我们提出了Pressure2Motion，这是一种新颖的运动捕捉算法，可以根据地面压力序列和文本提示合成人体运动。它不需要专门的照明设置、相机或可穿戴设备，使其适合隐私保护、低光和低成本的动作捕捉场景。由于全身运动压力信号的不确定性，这样的任务是严重不适定的。为了解决这个问题，我们引入了Pressure2Motion，这是一种利用压力特征作为输入并利用文本提示作为高级指导约束的生成模型。具体来说，我们的模型利用双级特征提取器来准确解释压力数据，然后采用分层扩散模型来识别大范围的运动轨迹和细微的姿势调整。从压力序列中获得的物理线索和从描述性文本中获得的语​​义指导都被用来精确地指导运动生成。据我们所知，Pressure2Motion 是利用压力数据和语言先验进行运动生成的开创性工作，而已建立的 MPL 基准是该任务的第一个基准。实验表明，我们的方法可以生成高保真度、物理上合理的运动，为这项任务建立了新的最先进技术。规范和基准将在发布后公开发布。</li>
</ul>

<h3>Title: Role-SynthCLIP: A Role Play Driven Diverse Synthetic Data Approach</h3>
<ul>
<li><strong>Authors: </strong>Yuanxiang Huangfu, Chaochao Wang, Weilei Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05057">https://arxiv.org/abs/2511.05057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05057">https://arxiv.org/pdf/2511.05057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05057]] Role-SynthCLIP: A Role Play Driven Diverse Synthetic Data Approach(https://arxiv.org/abs/2511.05057)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The effectiveness of Contrastive Language-Image Pre-training (CLIP) models critically depends on the semantic diversity and quality of their training data. However, while existing synthetic data generation methods primarily focus on increasing data volume, such emphasis often leads to limited semantic diversity and redundant or shallow captions. To address this limitation, we propose Role-SynthCLIP, a novel data synthesis framework that leverages multi-perspective role-playing prompts (e.g., a compositional analyst, an interpreter of image context) to guide Multimodal Large Language Models (MLLMs) in generating semantically diverse captions from distinct viewpoints. This mechanism enhances the semantic diversity and fine-grained image-text alignment of synthetic pairs, thereby improving caption expressiveness and accuracy while keeping the total number of image-text pairs unchanged. Experimental results demonstrate the effectiveness and efficiency of our method. A CLIP-B/16 model trained on only 1 million Role-SynthCLIP pairs achieves a Recall@1 of 64.1% on the MS COCO validation set, surpassing the best existing synthetic data baseline (trained on 5M pairs) by 2.8 percentage points. The code and trained models are released at this https URL.</li>
<li><strong>摘要：</strong>对比语言图像预训练（CLIP）模型的有效性关键取决于训练数据的语义多样性和质量。然而，虽然现有的合成数据生成方法主要关注增加数据量，但这种强调通常会导致有限的语义多样性和冗余或浅层标题。为了解决这个限制，我们提出了 Role-SynthCLIP，这是一种新颖的数据合成框架，它利用多视角角色扮演提示（例如，合成分析师、图像上下文解释器）来指导多模态大语言模型（MLLM）从不同的角度生成语义多样的标题。该机制增强了合成对的语义多样性和细粒度图像文本对齐，从而在保持图像文本对总数不变的情况下提高了字幕表达力和准确性。实验结果证明了我们方法的有效性和效率。仅在 100 万个 Role-SynthCLIP 对上训练的 CLIP-B/16 模型在 MS COCO 验证集上实现了 64.1% 的 Recall@1，比现有最佳合成数据基线（在 500 万对上训练）高出 2.8 个百分点。代码和经过训练的模型在此 https URL 发布。</li>
</ul>

<h3>Title: SurgiATM: A Physics-Guided Plug-and-Play Model for Deep Learning-Based Smoke Removal in Laparoscopic Surgery</h3>
<ul>
<li><strong>Authors: </strong>Mingyu Sheng, Jianan Fan, Dongnan Liu, Guoyan Zheng, Ron Kikinis, Weidong Cai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05059">https://arxiv.org/abs/2511.05059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05059">https://arxiv.org/pdf/2511.05059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05059]] SurgiATM: A Physics-Guided Plug-and-Play Model for Deep Learning-Based Smoke Removal in Laparoscopic Surgery(https://arxiv.org/abs/2511.05059)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>During laparoscopic surgery, smoke generated by tissue cauterization can significantly degrade the visual quality of endoscopic frames, increasing the risk of surgical errors and hindering both clinical decision-making and computer-assisted visual analysis. Consequently, removing surgical smoke is critical to ensuring patient safety and maintaining operative efficiency. In this study, we propose the Surgical Atmospheric Model (SurgiATM) for surgical smoke removal. SurgiATM statistically bridges a physics-based atmospheric model and data-driven deep learning models, combining the superior generalizability of the former with the high accuracy of the latter. Furthermore, SurgiATM is designed as a lightweight, plug-and-play module that can be seamlessly integrated into diverse surgical desmoking architectures to enhance their accuracy and stability, better meeting clinical requirements. It introduces only two hyperparameters and no additional trainable weights, preserving the original network architecture with minimal computational and modification overhead. We conduct extensive experiments on three public surgical datasets with ten desmoking methods, involving multiple network architectures and covering diverse procedures, including cholecystectomy, partial nephrectomy, and diaphragm dissection. The results demonstrate that incorporating SurgiATM commonly reduces the restoration errors of existing models and relatively enhances their generalizability, without adding any trainable layers or weights. This highlights the convenience, low cost, effectiveness, and generalizability of the proposed method. The code for SurgiATM is released at this https URL.</li>
<li><strong>摘要：</strong>在腹腔镜手术期间，组织烧灼产生的烟雾会显着降低内窥镜框架的视觉质量，增加手术错误的风险，并阻碍临床决策和计算机辅助视觉分析。因此，去除手术烟雾对于确保患者安全和保持手术效率至关重要。在这项研究中，我们提出了用于手术烟雾清除的手术大气模型（SurgiATM）。 SurgiATM 在统计上连接了基于物理的大气模型和数据驱动的深度学习模型，将前者的卓越通用性与后者的高精度结合在一起。此外，SurgiATM被设计为轻量级、即插即用的模块，可以无缝集成到各种手术除烟架构中，以提高其准确性和稳定性，更好地满足临床需求。它仅引入两个超参数，并且没有额外的可训练权重，以最小的计算和修改开销保留了原始网络架构。我们对三个公共手术数据集和十种除烟方法进行了广泛的实验，涉及多种网络架构并涵盖不同的手术，包括胆囊切除术、肾部分切除术和膈肌解剖术。结果表明，结合 SurgiATM 通常会减少现有模型的恢复误差并相对增强其通用性，而无需添加任何可训练层或权重。这突出了该方法的方便性、低成本、有效性和通用性。 SurgiATM 的代码在此 https URL 发布。</li>
</ul>

<h3>Title: Real-World Adverse Weather Image Restoration via Dual-Level Reinforcement Learning with High-Quality Cold Start</h3>
<ul>
<li><strong>Authors: </strong>Fuyang Liu, Jiaqi Xu, Xiaowei Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05095">https://arxiv.org/abs/2511.05095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05095">https://arxiv.org/pdf/2511.05095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05095]] Real-World Adverse Weather Image Restoration via Dual-Level Reinforcement Learning with High-Quality Cold Start(https://arxiv.org/abs/2511.05095)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Adverse weather severely impairs real-world visual perception, while existing vision models trained on synthetic data with fixed parameters struggle to generalize to complex degradations. To address this, we first construct HFLS-Weather, a physics-driven, high-fidelity dataset that simulates diverse weather phenomena, and then design a dual-level reinforcement learning framework initialized with HFLS-Weather for cold-start training. Within this framework, at the local level, weather-specific restoration models are refined through perturbation-driven image quality optimization, enabling reward-based learning without paired supervision; at the global level, a meta-controller dynamically orchestrates model selection and execution order according to scene degradation. This framework enables continuous adaptation to real-world conditions and achieves state-of-the-art performance across a wide range of adverse weather scenarios. Code is available at this https URL</li>
<li><strong>摘要：</strong>恶劣的天气严重损害了现实世界的视觉感知，而现有的视觉模型在具有固定参数的合成数据上训练，很难推广到复杂的退化。为了解决这个问题，我们首先构建了 HFLS-Weather，一个物理驱动的高保真数据集，用于模拟各种天气现象，然后设计一个用 HFLS-Weather 初始化的双层强化学习框架进行冷启动训练。在此框架内，在本地层面，通过扰动驱动的图像质量优化来完善针对特定天气的恢复模型，从而实现基于奖励的学习，而无需配对监督；在全局层面，元控制器根据场景退化动态协调模型选择和执行顺序。该框架能够持续适应现实条件，并在各种恶劣天气场景下实现最先进的性能。代码可在此 https URL 获取</li>
</ul>

<h3>Title: Early Alzheimer's Disease Detection from Retinal OCT Images: A UK Biobank Study</h3>
<ul>
<li><strong>Authors: </strong>Yasemin Turkan, F. Boray Tek, M. Serdar Nazlı, Öykü Eren</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05106">https://arxiv.org/abs/2511.05106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05106">https://arxiv.org/pdf/2511.05106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05106]] Early Alzheimer's Disease Detection from Retinal OCT Images: A UK Biobank Study(https://arxiv.org/abs/2511.05106)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Alterations in retinal layer thickness, measurable using Optical Coherence Tomography (OCT), have been associated with neurodegenerative diseases such as Alzheimer's disease (AD). While previous studies have mainly focused on segmented layer thickness measurements, this study explored the direct classification of OCT B-scan images for the early detection of AD. To our knowledge, this is the first application of deep learning to raw OCT B-scans for AD prediction in the literature. Unlike conventional medical image classification tasks, early detection is more challenging than diagnosis because imaging precedes clinical diagnosis by several years. We fine-tuned and evaluated multiple pretrained models, including ImageNet-based networks and the OCT-specific RETFound transformer, using subject-level cross-validation datasets matched for age, sex, and imaging instances from the UK Biobank cohort. To reduce overfitting in this small, high-dimensional dataset, both standard and OCT-specific augmentation techniques were applied, along with a year-weighted loss function that prioritized cases diagnosed within four years of imaging. ResNet-34 produced the most stable results, achieving an AUC of 0.62 in the 4-year cohort. Although below the threshold for clinical application, our explainability analyses confirmed localized structural differences in the central macular subfield between the AD and control groups. These findings provide a baseline for OCT-based AD prediction, highlight the challenges of detecting subtle retinal biomarkers years before AD diagnosis, and point to the need for larger datasets and multimodal approaches.</li>
<li><strong>摘要：</strong>可使用光学相干断层扫描 (OCT) 测量的视网膜层厚度的变化与阿尔茨海默氏病 (AD) 等神经退行性疾病有关。虽然之前的研究主要集中于分段层厚度测量，但本研究探索了 OCT B 扫描图像的直接分类以用于 AD 的早期检测。据我们所知，这是文献中首次将深度学习应用于原始 OCT B 扫描以进行 AD 预测。与传统的医学图像分类任务不同，早期检测比诊断更具挑战性，因为成像比临床诊断早几年。我们使用与英国生物银行队列中的年龄、性别和成像实例相匹配的受试者级交叉验证数据集，对多个预训练模型进行了微调和评估，包括基于 ImageNet 的网络和 OCT 特定的 RETFound 转换器。为了减少这个小型高维数据集中的过度拟合，应用了标准和 OCT 特定的增强技术，以及年加权损失函数，该函数优先考虑成像四年内诊断的病例。 ResNet-34 产生了最稳定的结果，在 4 年队列中实现了 0.62 的 AUC。尽管低于临床应用的阈值，但我们的可解释性分析证实了 AD 组和对照组之间中央黄斑亚区的局部结构差异。这些发现为基于 OCT 的 AD 预测提供了基线，强调了在 AD 诊断前几年检测微妙的视网膜生物标志物的挑战，并指出需要更大的数据集和多模式方法。</li>
</ul>

<h3>Title: Associative Poisoning to Generative Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Mathias Lundteigen Mohus, Jingyue Li, Zhirong Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05177">https://arxiv.org/abs/2511.05177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05177">https://arxiv.org/pdf/2511.05177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05177]] Associative Poisoning to Generative Machine Learning(https://arxiv.org/abs/2511.05177)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The widespread adoption of generative models such as Stable Diffusion and ChatGPT has made them increasingly attractive targets for malicious exploitation, particularly through data poisoning. Existing poisoning attacks compromising synthesised data typically either cause broad degradation of generated data or require control over the training process, limiting their applicability in real-world scenarios. In this paper, we introduce a novel data poisoning technique called associative poisoning, which compromises fine-grained features of the generated data without requiring control of the training process. This attack perturbs only the training data to manipulate statistical associations between specific feature pairs in the generated outputs. We provide a formal mathematical formulation of the attack and prove its theoretical feasibility and stealthiness. Empirical evaluations using two state-of-the-art generative models demonstrate that associative poisoning effectively induces or suppresses feature associations while preserving the marginal distributions of the targeted features and maintaining high-quality outputs, thereby evading visual detection. These results suggest that generative systems used in image synthesis, synthetic dataset generation, and natural language processing are susceptible to subtle, stealthy manipulations that compromise their statistical integrity. To address this risk, we examine the limitations of existing defensive strategies and propose a novel countermeasure strategy.</li>
<li><strong>摘要：</strong>Stable Diffusion 和 ChatGPT 等生成模型的广泛采用使它们越来越成为恶意利用的目标，特别是通过数据中毒。现有的损害合成数据的中毒攻击通常会导致生成的数据广泛退化，或者需要控制训练过程，从而限制了它们在现实场景中的适用性。在本文中，我们介绍了一种称为关联中毒的新型数据中毒技术，该技术会损害生成数据的细粒度特征，而无需控制训练过程。这种攻击仅扰乱训练数据，以操纵生成的输出中特定特征对之间的统计关联。我们提供了攻击的正式数学公式，并证明了其理论上的可行性和隐蔽性。使用两种最先进的生成模型进行的实证评估表明，关联中毒有效地诱导或抑制特征关联，同时保留目标特征的边缘分布并保持高质量的输出，从而逃避视觉检测。这些结果表明，用于图像合成、合成数据集生成和自然语言处理的生成系统很容易受到微妙、隐秘的操作的影响，从而损害其统计完整性。为了解决这一风险，我们研究了现有防御策略的局限性，并提出了一种新的对策策略。</li>
</ul>

<h3>Title: Walk the Lines 2: Contour Tracking for Detailed Segmentation</h3>
<ul>
<li><strong>Authors: </strong>André Peter Kelm, Max Braeschke, Emre Gülsoylu, Simone Frintrop</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05210">https://arxiv.org/abs/2511.05210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05210">https://arxiv.org/pdf/2511.05210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05210]] Walk the Lines 2: Contour Tracking for Detailed Segmentation(https://arxiv.org/abs/2511.05210)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This paper presents Walk the Lines 2 (WtL2), a unique contour tracking algorithm specifically adapted for detailed segmentation of infrared (IR) ships and various objects in RGB.1 This extends the original Walk the Lines (WtL) [12], which focused solely on detailed ship segmentation in color. These innovative WtLs can replace the standard non-maximum suppression (NMS) by using contour tracking to refine the object contour until a 1-pixel-wide closed shape can be binarized, forming a segmentable area in foreground-background scenarios. WtL2 broadens the application range of WtL beyond its original scope, adapting to IR and expanding to diverse objects within the RGB context. To achieve IR segmentation, we adapt its input, the object contour detector, to IR ships. In addition, the algorithm is enhanced to process a wide range of RGB objects, outperforming the latest generation of contour-based methods when achieving a closed object contour, offering high peak Intersection over Union (IoU) with impressive details. This positions WtL2 as a compelling method for specialized applications that require detailed segmentation or high-quality samples, potentially accelerating progress in several niche areas of image segmentation.</li>
<li><strong>摘要：</strong>本文介绍了 Walk the Lines 2 (WtL2)，这是一种独特的轮廓跟踪算法，专门适用于红外 (IR) 船舶和 RGB 中的各种对象的详细分割。1 这扩展了原始的 Walk the Lines (WtL) [12]，该算法仅专注于详细的船舶颜色分割。这些创新的WtL可以通过使用轮廓跟踪来细化对象轮廓，直到可以二值化1像素宽的闭合形状，从而在前景-背景场景中形成可分割的区域，从而取代标准的非极大值抑制（NMS）。 WtL2 拓宽了 WtL 的应用范围，超出了其原始范围，适应 IR 并扩展到 RGB 环境中的各种对象。为了实现红外分割，我们将其输入（物体轮廓检测器）调整为红外船舶。此外，该算法经过增强，可以处理各种 RGB 对象，在实现闭合对象轮廓时优于最新一代基于轮廓的方法，提供具有令人印象深刻的细节的高峰值交并集 (IoU)。这使得 WtL2 成为需要详细分割或高质量样本的专业应用程序的一种引人注目的方法，有可能加速图像分割的几个利基领域的进展。</li>
</ul>

<h3>Title: FreeControl: Efficient, Training-Free Structural Control via One-Step Attention Extraction</h3>
<ul>
<li><strong>Authors: </strong>Jiang Lin, Xinyu Chen, Song Wu, Zhiqiu Zhang, Jizhi Zhang, Ye Wang, Qiang Tang, Qian Wang, Jian Yang, Zili Yi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05219">https://arxiv.org/abs/2511.05219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05219">https://arxiv.org/pdf/2511.05219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05219]] FreeControl: Efficient, Training-Free Structural Control via One-Step Attention Extraction(https://arxiv.org/abs/2511.05219)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Controlling the spatial and semantic structure of diffusion-generated images remains a challenge. Existing methods like ControlNet rely on handcrafted condition maps and retraining, limiting flexibility and generalization. Inversion-based approaches offer stronger alignment but incur high inference cost due to dual-path denoising. We present FreeControl, a training-free framework for semantic structural control in diffusion models. Unlike prior methods that extract attention across multiple timesteps, FreeControl performs one-step attention extraction from a single, optimally chosen key timestep and reuses it throughout denoising. This enables efficient structural guidance without inversion or retraining. To further improve quality and stability, we introduce Latent-Condition Decoupling (LCD): a principled separation of the key timestep and the noised latent used in attention extraction. LCD provides finer control over attention quality and eliminates structural artifacts. FreeControl also supports compositional control via reference images assembled from multiple sources - enabling intuitive scene layout design and stronger prompt alignment. FreeControl introduces a new paradigm for test-time control, enabling structurally and semantically aligned, visually coherent generation directly from raw images, with the flexibility for intuitive compositional design and compatibility with modern diffusion models at approximately 5 percent additional cost.</li>
<li><strong>摘要：</strong>控制扩散生成图像的空间和语义结构仍然是一个挑战。 ControlNet 等现有方法依赖于手工制作的条件图和重新训练，限制了灵活性和泛化性。基于反演的方法提供了更强的对齐能力，但由于双路径去噪而导致较高的推理成本。我们提出了 FreeControl，一种用于扩散模型中语义结构控制的免训练框架。与之前跨多个时间步提取注意力的方法不同，FreeControl 从单个最优选择的关键时间步中执行一步注意力提取，并在整个去噪过程中重复使用它。这可以实现高效的结构引导，而无需反转或重新训练。为了进一步提高质量和稳定性，我们引入了潜在条件解耦（LCD）：关键时间步长和注意力提取中使用的噪声潜在的原则性分离。 LCD 可更好地控制注意力质量并消除结构伪影。 FreeControl 还支持通过从多个来源组装的参考图像进行构图控制 - 实现直观的场景布局设计和更强的提示对齐。 FreeControl 引入了一种新的测试时间控制范例，可直接从原始图像生成结构和语义一致、视觉连贯的生成，具有直观组合设计的灵活性以及与现代扩散模型的兼容性，但成本增加约 5%。</li>
</ul>

<h3>Title: The Causal Round Trip: Generating Authentic Counterfactuals by Eliminating Information Loss</h3>
<ul>
<li><strong>Authors: </strong>Rui Wu, Lizheng Wang, Yongjun Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05236">https://arxiv.org/abs/2511.05236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05236">https://arxiv.org/pdf/2511.05236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05236]] The Causal Round Trip: Generating Authentic Counterfactuals by Eliminating Information Loss(https://arxiv.org/abs/2511.05236)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Judea Pearl's vision of Structural Causal Models (SCMs) as engines for counterfactual reasoning hinges on faithful abduction: the precise inference of latent exogenous noise. For decades, operationalizing this step for complex, non-linear mechanisms has remained a significant computational challenge. The advent of diffusion models, powerful universal function approximators, offers a promising solution. However, we argue that their standard design, optimized for perceptual generation over logical inference, introduces a fundamental flaw for this classical problem: an inherent information loss we term the Structural Reconstruction Error (SRE). To address this challenge, we formalize the principle of Causal Information Conservation (CIC) as the necessary condition for faithful abduction. We then introduce BELM-MDCM, the first diffusion-based framework engineered to be causally sound by eliminating SRE by construction through an analytically invertible mechanism. To operationalize this framework, a Targeted Modeling strategy provides structural regularization, while a Hybrid Training Objective instills a strong causal inductive bias. Rigorous experiments demonstrate that our Zero-SRE framework not only achieves state-of-the-art accuracy but, more importantly, enables the high-fidelity, individual-level counterfactuals required for deep causal inquiries. Our work provides a foundational blueprint that reconciles the power of modern generative models with the rigor of classical causal theory, establishing a new and more rigorous standard for this emerging field.</li>
<li><strong>摘要：</strong>Judea Pearl 将结构因果模型 (SCM) 作为反事实推理引擎的愿景取决于忠实的溯因：对潜在外源噪声的精确推理。几十年来，将这一步骤应用于复杂的非线性机制仍然是一个重大的计算挑战。扩散模型和强大的通用函数逼近器的出现提供了一个有前途的解决方案。然而，我们认为，他们的标准设计针对逻辑推理的感知生成进行了优化，为这个经典问题引入了一个根本缺陷：固有的信息丢失，我们称之为结构重建错误（SRE）。为了应对这一挑战，我们将因果信息守恒（CIC）原则正式化为忠实绑架的必要条件。然后，我们介绍 BELM-MDCM，这是第一个基于扩散的框架，通过分析可逆机制构建消除 SRE，从而实现因果健全。为了实施该框架，目标建模策略提供了结构正则化，而混合训练目标则灌输了强烈的因果归纳偏差。严格的实验表明，我们的零 SRE 框架不仅实现了最先进的准确性，更重要的是，实现了深度因果查询所需的高保真度、个人级别的反事实。我们的工作提供了一个基础蓝图，将现代生成模型的力量与经典因果理论的严谨性相协调，为这个新兴领域建立了一个新的、更严格的标准。</li>
</ul>

<h3>Title: Rethinking Metrics and Diffusion Architecture for 3D Point Cloud Generation</h3>
<ul>
<li><strong>Authors: </strong>Matteo Bastico, David Ryckelynck, Laurent Corté, Yannick Tillier, Etienne Decencière</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05308">https://arxiv.org/abs/2511.05308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05308">https://arxiv.org/pdf/2511.05308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05308]] Rethinking Metrics and Diffusion Architecture for 3D Point Cloud Generation(https://arxiv.org/abs/2511.05308)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>As 3D point clouds become a cornerstone of modern technology, the need for sophisticated generative models and reliable evaluation metrics has grown exponentially. In this work, we first expose that some commonly used metrics for evaluating generated point clouds, particularly those based on Chamfer Distance (CD), lack robustness against defects and fail to capture geometric fidelity and local shape consistency when used as quality indicators. We further show that introducing samples alignment prior to distance calculation and replacing CD with Density-Aware Chamfer Distance (DCD) are simple yet essential steps to ensure the consistency and robustness of point cloud generative model evaluation metrics. While existing metrics primarily focus on directly comparing 3D Euclidean coordinates, we present a novel metric, named Surface Normal Concordance (SNC), which approximates surface similarity by comparing estimated point normals. This new metric, when combined with traditional ones, provides a more comprehensive evaluation of the quality of generated samples. Finally, leveraging recent advancements in transformer-based models for point cloud analysis, such as serialized patch attention , we propose a new architecture for generating high-fidelity 3D structures, the Diffusion Point Transformer. We perform extensive experiments and comparisons on the ShapeNet dataset, showing that our model outperforms previous solutions, particularly in terms of quality of generated point clouds, achieving new state-of-the-art. Code available at this https URL.</li>
<li><strong>摘要：</strong>随着 3D 点云成为现代技术的基石，对复杂的生成模型和可靠的评估指标的需求呈指数级增长。在这项工作中，我们首先揭示了一些用于评估生成点云的常用指标，特别是基于倒角距离（CD）的指标，缺乏针对缺陷的鲁棒性，并且在用作质量指标时无法捕获几何保真度和局部形状一致性。我们进一步表明，在距离计算之前引入样本对齐以及用密度感知倒角距离（DCD）替换 CD 是确保点云生成模型评估指标的一致性和鲁棒性的简单但必不可少的步骤。虽然现有的指标主要集中于直接比较 3D 欧几里德坐标，但我们提出了一种新颖的指标，称为表面法线一致性 (SNC)，它通过比较估计的点法线来近似表面相似性。这种新指标与传统指标相结合，可以对生成样本的质量提供更全面的评估。最后，利用基于变压器的点云分析模型的最新进展，例如序列化补丁注意力，我们提出了一种用于生成高保真 3D 结构的新架构，即扩散点变压器。我们对 ShapeNet 数据集进行了广泛的实验和比较，表明我们的模型优于以前的解决方案，特别是在生成点云的质量方面，实现了新的最先进技术。代码可在此 https URL 获取。</li>
</ul>

<h3>Title: SAD-Flower: Flow Matching for Safe, Admissible, and Dynamically Consistent Planning</h3>
<ul>
<li><strong>Authors: </strong>Tzu-Yuan Huang, Armin Lederer, Dai-Jie Wu, Xiaobing Dai, Sihua Zhang, Stefan Sosnowski, Shao-Hua Sun, Sandra Hirche</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05355">https://arxiv.org/abs/2511.05355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05355">https://arxiv.org/pdf/2511.05355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05355]] SAD-Flower: Flow Matching for Safe, Admissible, and Dynamically Consistent Planning(https://arxiv.org/abs/2511.05355)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Flow matching (FM) has shown promising results in data-driven planning. However, it inherently lacks formal guarantees for ensuring state and action constraints, whose satisfaction is a fundamental and crucial requirement for the safety and admissibility of planned trajectories on various systems. Moreover, existing FM planners do not ensure the dynamical consistency, which potentially renders trajectories inexecutable. We address these shortcomings by proposing SAD-Flower, a novel framework for generating Safe, Admissible, and Dynamically consistent trajectories. Our approach relies on an augmentation of the flow with a virtual control input. Thereby, principled guidance can be derived using techniques from nonlinear control theory, providing formal guarantees for state constraints, action constraints, and dynamic consistency. Crucially, SAD-Flower operates without retraining, enabling test-time satisfaction of unseen constraints. Through extensive experiments across several tasks, we demonstrate that SAD-Flower outperforms various generative-model-based baselines in ensuring constraint satisfaction.</li>
<li><strong>摘要：</strong>流量匹配（FM）在数据驱动的规划中显示出了有希望的结果。然而，它本质上缺乏确保状态和行动约束的正式保证，而状态和行动约束的满足是各种系统上计划轨迹的安全性和可接受性的基本和关键要求。此外，现有的 FM 规划器无法确保动态一致性，这可能导致轨迹无法执行。我们通过提出 SAD-Flower 来解决这些缺点，这是一种用于生成安全、可接受和动态一致轨迹的新颖框架。我们的方法依赖于通过虚拟控制输入来增强流程。因此，可以使用非线性控制理论的技术导出原则指导，为状态约束、动作约束和动态一致性提供形式保证。至关重要的是，SAD-Flower 无需重新训练即可运行，从而能够在测试时满足看不见的约束。通过跨多个任务的广泛实验，我们证明 SAD-Flower 在确保约束满足方面优于各种基于生成模型的基线。</li>
</ul>

<h3>Title: Diffusion-Based Electromagnetic Inverse Design of Scattering Structured Media</h3>
<ul>
<li><strong>Authors: </strong>Mikhail Tsukerman, Konstantin Grotov, Pavel Ginzburg</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.app-ph, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05357">https://arxiv.org/abs/2511.05357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05357">https://arxiv.org/pdf/2511.05357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05357]] Diffusion-Based Electromagnetic Inverse Design of Scattering Structured Media(https://arxiv.org/abs/2511.05357)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present a conditional diffusion model for electromagnetic inverse design that generates structured media geometries directly from target differential scattering cross-section profiles, bypassing expensive iterative optimization. Our 1D U-Net architecture with Feature-wise Linear Modulation learns to map desired angular scattering patterns to 2x2 dielectric sphere structure, naturally handling the non-uniqueness of inverse problems by sampling diverse valid designs. Trained on 11,000 simulated metasurfaces, the model achieves median MPE below 19% on unseen targets (best: 1.39%), outperforming CMA-ES evolutionary optimization while reducing design time from hours to seconds. These results demonstrate that employing diffusion models is promising for advancing electromagnetic inverse design research, potentially enabling rapid exploration of complex metasurface architectures and accelerating the development of next-generation photonic and wireless communication systems. The code is publicly available at this https URL.</li>
<li><strong>摘要：</strong>我们提出了一种用于电磁逆设计的条件扩散模型，该模型直接从目标差分散射截面轮廓生成结构化介质几何形状，绕过昂贵的迭代优化。我们具有特征线性调制的 1D U-Net 架构学习将所需的角度散射模式映射到 2x2 介电球结构，通过采样不同的有效设计自然地处理逆问题的非唯一性。该模型在 11,000 个模拟超表面上进行训练，在未见过的目标上实现了低于 19% 的中值 MPE（最佳：1.39%），优于 CMA-ES 进化优化，同时将设计时间从几小时缩短到几秒。这些结果表明，采用扩散模型有望推进电磁逆设计研究，有可能实现复杂超表面结构的快速探索，并加速下一代光子和无线通信系统的开发。该代码可通过此 https URL 公开获取。</li>
</ul>

<h3>Title: Dense Motion Captioning</h3>
<ul>
<li><strong>Authors: </strong>Shiyao Xu, Benedetta Liberatori, Gül Varol, Paolo Rota</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05369">https://arxiv.org/abs/2511.05369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05369">https://arxiv.org/pdf/2511.05369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05369]] Dense Motion Captioning(https://arxiv.org/abs/2511.05369)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in 3D human motion and language integration have primarily focused on text-to-motion generation, leaving the task of motion understanding relatively unexplored. We introduce Dense Motion Captioning, a novel task that aims to temporally localize and caption actions within 3D human motion sequences. Current datasets fall short in providing detailed temporal annotations and predominantly consist of short sequences featuring few actions. To overcome these limitations, we present the Complex Motion Dataset (CompMo), the first large-scale dataset featuring richly annotated, complex motion sequences with precise temporal boundaries. Built through a carefully designed data generation pipeline, CompMo includes 60,000 motion sequences, each composed of multiple actions ranging from at least two to ten, accurately annotated with their temporal extents. We further present DEMO, a model that integrates a large language model with a simple motion adapter, trained to generate dense, temporally grounded captions. Our experiments show that DEMO substantially outperforms existing methods on CompMo as well as on adapted benchmarks, establishing a robust baseline for future research in 3D motion understanding and captioning.</li>
<li><strong>摘要：</strong>3D 人体动作和语言集成的最新进展主要集中在文本到动作的生成上，而相对而言，动作理解的任务尚未得到探索。我们引入了密集运动字幕，这是一项新颖的任务，旨在对 3D 人体运动序列中的动作进行时间定位和字幕。当前的数据集无法提供详细的时间注释，并且主要由具有很少动作的短序列组成。为了克服这些限制，我们提出了复杂运动数据集（CompMo），这是第一个大规模数据集，具有注释丰富、具有精确时间边界的复杂运动序列。 CompMo 通过精心设计的数据生成管道构建，包含 60,000 个运动序列，每个序列由至少 2 到 10 个范围内的多个动作组成，并准确注释了它们的时间范围。我们进一步提出了 DEMO，这是一个将大型语言模型与简单运动适配器集成在一起的模型，经过训练可以生成密集的、基于时间的字幕。我们的实验表明，DEMO 大大优于 CompMo 以及改编基准上的现有方法，为 3D 运动理解和字幕的未来研究建立了稳健的基线。</li>
</ul>

<h3>Title: PreResQ-R1: Towards Fine-Grained Rank-and-Score Reinforcement Learning for Visual Quality Assessment via Preference-Response Disentangled Policy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Zehui Feng, Tian Qiu, Tong Wu, Junxuan Li, Huayuan Xu, Ting Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05393">https://arxiv.org/abs/2511.05393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05393">https://arxiv.org/pdf/2511.05393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05393]] PreResQ-R1: Towards Fine-Grained Rank-and-Score Reinforcement Learning for Visual Quality Assessment via Preference-Response Disentangled Policy Optimization(https://arxiv.org/abs/2511.05393)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Visual Quality Assessment (QA) seeks to predict human perceptual judgments of visual fidelity. While recent multimodal large language models (MLLMs) show promise in reasoning about image and video quality, existing approaches mainly rely on supervised fine-tuning or rank-only objectives, resulting in shallow reasoning, poor score calibration, and limited cross-domain generalization. We propose PreResQ-R1, a Preference-Response Disentangled Reinforcement Learning framework that unifies absolute score regression and relative ranking consistency within a single reasoning-driven optimization scheme. Unlike prior QA methods, PreResQ-R1 introduces a dual-branch reward formulation that separately models intra-sample response coherence and inter-sample preference alignment, optimized via Group Relative Policy Optimization (GRPO). This design encourages fine-grained, stable, and interpretable chain-of-thought reasoning about perceptual quality. To extend beyond static imagery, we further design a global-temporal and local-spatial data flow strategy for Video Quality Assessment. Remarkably, with reinforcement fine-tuning on only 6K images and 28K videos, PreResQ-R1 achieves state-of-the-art results across 10 IQA and 5 VQA benchmarks under both SRCC and PLCC metrics, surpassing by margins of 5.30% and textbf2.15% in IQA task, respectively. Beyond quantitative gains, it produces human-aligned reasoning traces that reveal the perceptual cues underlying quality judgments. Code and model are available.</li>
<li><strong>摘要：</strong>视觉质量评估 (QA) 旨在预测人类对视觉保真度的感知判断。虽然最近的多模态大语言模型（MLLM）在图像和视频质量推理方面表现出了良好的前景，但现有方法主要依赖于监督微调或仅排名目标，导致推理浅层、分数校准不佳和跨域泛化有限。我们提出了 PreResQ-R1，一种偏好响应解缠强化学习框架，它将绝对分数回归和相对排名一致性统一在单个推理驱动的优化方案中。与之前的 QA 方法不同，PreResQ-R1 引入了双分支奖励公式，该公式分别对样本内响应一致性和样本间偏好对齐进行建模，并通过组相对策略优化 (GRPO) 进行优化。这种设计鼓励关于感知质量的细粒度、稳定且可解释的思想链推理。为了超越静态图像，我们进一步设计了用于视频质量评估的全局时间和局部空间数据流策略。值得注意的是，通过仅对 6K 图像和 28K 视频进行强化微调，PreResQ-R1 在 SRCC 和 PLCC 指标下的 10 个 IQA 和 5 个 VQA 基准测试中取得了最先进的结果，在 IQA 任务中分别领先 5.30% 和 textbf2.15%。除了数量上的收益之外，它还产生了与人类一致的推理痕迹，揭示了质量判断背后的感知线索。代码和型号均可用。</li>
</ul>

<h3>Title: Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction</h3>
<ul>
<li><strong>Authors: </strong>Yiting He, Zhishuai Liu, Weixin Wang, Pan Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05396">https://arxiv.org/abs/2511.05396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05396">https://arxiv.org/pdf/2511.05396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05396]] Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction(https://arxiv.org/abs/2511.05396)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Off-dynamics reinforcement learning (RL), where training and deployment transition dynamics are different, can be formulated as learning in a robust Markov decision process (RMDP) where uncertainties in transition dynamics are imposed. Existing literature mostly assumes access to generative models allowing arbitrary state-action queries or pre-collected datasets with a good state coverage of the deployment environment, bypassing the challenge of exploration. In this work, we study a more realistic and challenging setting where the agent is limited to online interaction with the training environment. To capture the intrinsic difficulty of exploration in online RMDPs, we introduce the supremal visitation ratio, a novel quantity that measures the mismatch between the training dynamics and the deployment dynamics. We show that if this ratio is unbounded, online learning becomes exponentially hard. We propose the first computationally efficient algorithm that achieves sublinear regret in online RMDPs with $f$-divergence based transition uncertainties. We also establish matching regret lower bounds, demonstrating that our algorithm achieves optimal dependence on both the supremal visitation ratio and the number of interaction episodes. Finally, we validate our theoretical results through comprehensive numerical experiments.</li>
<li><strong>摘要：</strong>训练和部署过渡动态不同的非动态强化学习 (RL) 可以表示为鲁棒马尔可夫决策过程 (RMDP) 中的学习，其中强加了过渡动态的不确定性。现有文献大多假设访问生成模型，允许任意状态动作查询或预先收集的数据集，对部署环境具有良好的状态覆盖，从而绕过了探索的挑战。在这项工作中，我们研究了一个更现实和更具挑战性的环境，其中代理仅限于与训练环境的在线交互。为了捕捉在线 RMDP 探索的内在困难，我们引入了最高访问率，这是一个衡量训练动态和部署动态之间不匹配的新量。我们证明，如果这个比率是无限的，在线学习就会变得指数级困难。我们提出了第一个计算高效的算法，该算法在具有基于 $f$ 散度的转移不确定性的在线 RMDP 中实现亚线性遗憾。我们还建立了匹配后悔下限，证明我们的算法实现了对最高访问率和交互事件数量的最佳依赖。最后，我们通过综合数值实验验证了我们的理论结果。</li>
</ul>

<h3>Title: Sharing the Learned Knowledge-base to Estimate Convolutional Filter Parameters for Continual Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Aupendu Kar, Krishnendu Ghosh, Prabir Kumar Biswas</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05421">https://arxiv.org/abs/2511.05421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05421">https://arxiv.org/pdf/2511.05421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05421]] Sharing the Learned Knowledge-base to Estimate Convolutional Filter Parameters for Continual Image Restoration(https://arxiv.org/abs/2511.05421)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Continual learning is an emerging topic in the field of deep learning, where a model is expected to learn continuously for new upcoming tasks without forgetting previous experiences. This field has witnessed numerous advancements, but few works have been attempted in the direction of image restoration. Handling large image sizes and the divergent nature of various degradation poses a unique challenge in the restoration domain. However, existing works require heavily engineered architectural modifications for new task adaptation, resulting in significant computational overhead. Regularization-based methods are unsuitable for restoration, as different restoration challenges require different kinds of feature processing. In this direction, we propose a simple modification of the convolution layer to adapt the knowledge from previous restoration tasks without touching the main backbone architecture. Therefore, it can be seamlessly applied to any deep architecture without any structural modifications. Unlike other approaches, we demonstrate that our model can increase the number of trainable parameters without significantly increasing computational overhead or inference time. Experimental validation demonstrates that new restoration tasks can be introduced without compromising the performance of existing tasks. We also show that performance on new restoration tasks improves by adapting the knowledge from the knowledge base created by previous restoration tasks. The code is available at this https URL.</li>
<li><strong>摘要：</strong>持续学习是深度学习领域的一个新兴主题，模型期望能够持续学习即将到来的新任务，而不会忘记以前的经验。该领域已经取得了许多进步，但在图像修复方向上尝试的工作却很少。处理大图像尺寸和各种退化的不同性质在恢复领域提出了独特的挑战。然而，现有的工作需要大量工程架构修改才能适应新任务，从而导致大量的计算开销。基于正则化的方法不适合恢复，因为不同的恢复挑战需要不同类型的特征处理。在这个方向上，我们提出了对卷积层的简单修改，以适应先前恢复任务的知识，而无需触及主干架构。因此，它可以无缝地应用于任何深层架构，而无需进行任何结构修改。与其他方法不同，我们证明我们的模型可以增加可训练参数的数量，而不会显着增加计算开销或推理时间。实验验证表明，可以在不影响现有任务性能的情况下引入新的恢复任务。我们还表明，通过调整先前恢复任务创建的知识库中的知识，可以提高新恢复任务的性能。该代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Shared Latent Representation for Joint Text-to-Audio-Visual Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Dogucan Yaman, Seymanur Akti, Fevziye Irem Eyiokur, Alexander Waibel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05432">https://arxiv.org/abs/2511.05432</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05432">https://arxiv.org/pdf/2511.05432</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05432]] Shared Latent Representation for Joint Text-to-Audio-Visual Synthesis(https://arxiv.org/abs/2511.05432)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We propose a text-to-talking-face synthesis framework leveraging latent speech representations from HierSpeech++. A Text-to-Vec module generates Wav2Vec2 embeddings from text, which jointly condition speech and face generation. To handle distribution shifts between clean and TTS-predicted features, we adopt a two-stage training: pretraining on Wav2Vec2 embeddings and finetuning on TTS outputs. This enables tight audio-visual alignment, preserves speaker identity, and produces natural, expressive speech and synchronized facial motion without ground-truth audio at inference. Experiments show that conditioning on TTS-predicted latent features outperforms cascaded pipelines, improving both lip-sync and visual realism.</li>
<li><strong>摘要：</strong>我们提出了一个利用 HierSpeech++ 的潜在语音表示的文本到人脸合成框架。 Text-to-Vec 模块从文本生成 Wav2Vec2 嵌入，共同调节语音和面部生成。为了处理干净特征和 TTS 预测特征之间的分布变化，我们采用两阶段训练：对 Wav2Vec2 嵌入进行预训练，并对 TTS 输出进行微调。这可以实现紧密的视听对齐，保留说话者身份，并产生自然、富有表现力的语音和同步的面部动作，而无需推理时的真实音频。实验表明，对 TTS 预测的潜在特征进行调节的效果优于级联管道，从而提高了口型同步和视觉真实感。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
