<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-08-29</h1>
<h3>Title: CrystalICL: Enabling In-Context Learning for Crystal Generation</h3>
<ul>
<li><strong>Authors: </strong>Ruobing Wang, Qiaoyu Tan, Yili Wang, Ying Wang, Xin Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20143">https://arxiv.org/abs/2508.20143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20143">https://arxiv.org/pdf/2508.20143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20143]] CrystalICL: Enabling In-Context Learning for Crystal Generation(https://arxiv.org/abs/2508.20143)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Designing crystal materials with desired physicochemical properties remains a fundamental challenge in materials science. While large language models (LLMs) have demonstrated strong in-context learning (ICL) capabilities, existing LLM-based crystal generation approaches are limited to zero-shot scenarios and are unable to benefit from few-shot scenarios. In contrast, human experts typically design new materials by modifying relevant known structures which aligns closely with the few-shot ICL paradigm. Motivated by this, we propose CrystalICL, a novel model designed for few-shot crystal generation. Specifically, we introduce a space-group based crystal tokenization method, which effectively reduces the complexity of modeling crystal symmetry in LLMs. We further introduce a condition-structure aware hybrid instruction tuning framework and a multi-task instruction tuning strategy, enabling the model to better exploit ICL by capturing structure-property relationships from limited data. Extensive experiments on four crystal generation benchmarks demonstrate the superiority of CrystalICL over the leading baseline methods on conditional and unconditional generation tasks.</li>
<li><strong>摘要：</strong>设计具有所需物理化学特性的晶体材料仍然是材料科学的基本挑战。尽管大型语言模型（LLM）表现出强大的内在学习能力（ICL）功能，但现有的基于LLM的水晶生成方法仅限于零拍摄的方案，并且无法从几乎没有射击的情况下受益。相比之下，人类专家通常通过修改相关的已知结构来设计新材料，该结构与少数弹药范式紧密一致。在此激励的情况下，我们提出了Crystalicl，这是一种专为少量晶体生成而设计的新型模型。具体而言，我们引入了一种基于空间组的晶体代币化方法，该方法有效地降低了LLMS中建模晶体对称性的复杂性。我们进一步介绍了条件结构的混合指令调整框架和多任务指令调整策略，从而使模型能够通过从有限数据中捕获结构 - 毛皮关系来更好地利用ICL。在四个晶体生成基准上进行的广泛实验表明，在条件和无条件生成任务上，晶体质量优于领先的基线方法。</li>
</ul>

<h3>Title: SDiFL: Stable Diffusion-Driven Framework for Image Forgery Localization</h3>
<ul>
<li><strong>Authors: </strong>Yang Su, Shunquan Tan, Jiwu Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20182">https://arxiv.org/abs/2508.20182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20182">https://arxiv.org/pdf/2508.20182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20182]] SDiFL: Stable Diffusion-Driven Framework for Image Forgery Localization(https://arxiv.org/abs/2508.20182)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Driven by the new generation of multi-modal large models, such as Stable Diffusion (SD), image manipulation technologies have advanced rapidly, posing significant challenges to image forensics. However, existing image forgery localization methods, which heavily rely on labor-intensive and costly annotated data, are struggling to keep pace with these emerging image manipulation technologies. To address these challenges, we are the first to integrate both image generation and powerful perceptual capabilities of SD into an image forensic framework, enabling more efficient and accurate forgery localization. First, we theoretically show that the multi-modal architecture of SD can be conditioned on forgery-related information, enabling the model to inherently output forgery localization results. Then, building on this foundation, we specifically leverage the multimodal framework of Stable DiffusionV3 (SD3) to enhance forgery localization this http URL leverage the multi-modal processing capabilities of SD3 in the latent space by treating image forgery residuals -- high-frequency signals extracted using specific highpass filters -- as an explicit modality. This modality is fused into the latent space during training to enhance forgery localization performance. Notably, our method fully preserves the latent features extracted by SD3, thereby retaining the rich semantic information of the input image. Experimental results show that our framework achieves up to 12% improvements in performance on widely used benchmarking datasets compared to current state-of-the-art image forgery localization models. Encouragingly, the model demonstrates strong performance on forensic tasks involving real-world document forgery images and natural scene forging images, even when such data were entirely unseen during training.</li>
<li><strong>摘要：</strong>在新一代的多模式大型模型（例如稳定扩散（SD））的驱动下，图像操作技术已迅速发展，对图像取证构成了重大挑战。但是，现有的图像伪造定位方法在很大程度上依赖于劳动密集型和昂贵的注释数据，他们正在努力与这些新兴的图像操纵技术保持同步。为了应对这些挑战，我们是第一个将图像产生和强大的SD感知能力集成到图像法医框架中的人，从而实现了更有效，更准确的伪造定位。首先，我们从理论上表明，SD的多模式结构可以根据伪造的信息进行条件，从而使模型能够固有地输出伪造的定位结果。然后，在这个基础上，我们专门利用了稳定的扩散v3（SD3）的多模式框架来增强伪造本地化，该HTTP URL通过使用特定的高频备案用户使用特定的高速备案申请者来处理潜在空间中SD3的多模式处理能力。在训练期间，这种方式融合到潜在空间中，以增强伪造的定位性能。值得注意的是，我们的方法完全保留了SD3提取的潜在特征，从而保留了输入图像的丰富语义信息。实验结果表明，与当前最新的图像伪造模型相比，我们的框架在广泛使用的基准数据集上的性能提高了12％。令人鼓舞的是，该模型在涉及现实世界文档伪造图像和自然场景的法医任务上表现出强大的性能，即使在培训过程中完全看不见这些数据。</li>
</ul>

<h3>Title: MedNet-PVS: A MedNeXt-Based Deep Learning Model for Automated Segmentation of Perivascular Spaces</h3>
<ul>
<li><strong>Authors: </strong>Zhen Xuen Brandon Low, Rory Zhang, Hang Min, William Pham, Lucy Vivash, Jasmine Moses, Miranda Lynch, Karina Dorfman, Cassandra Marotta, Shaun Koh, Jacob Bunyamin, Ella Rowsthorn, Alex Jarema, Himashi Peiris, Zhaolin Chen, Sandy R. Shultz, David K. Wright, Dexiao Kong, Sharon L. Naismith, Terence J. O'Brien, Ying Xia, Meng Law, Benjamin Sinclair</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20256">https://arxiv.org/abs/2508.20256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20256">https://arxiv.org/pdf/2508.20256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20256]] MedNet-PVS: A MedNeXt-Based Deep Learning Model for Automated Segmentation of Perivascular Spaces(https://arxiv.org/abs/2508.20256)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Enlarged perivascular spaces (PVS) are increasingly recognized as biomarkers of cerebral small vessel disease, Alzheimer's disease, stroke, and aging-related neurodegeneration. However, manual segmentation of PVS is time-consuming and subject to moderate inter-rater reliability, while existing automated deep learning models have moderate performance and typically fail to generalize across diverse clinical and research MRI datasets. We adapted MedNeXt-L-k5, a Transformer-inspired 3D encoder-decoder convolutional network, for automated PVS segmentation. Two models were trained: one using a homogeneous dataset of 200 T2-weighted (T2w) MRI scans from the Human Connectome Project-Aging (HCP-Aging) dataset and another using 40 heterogeneous T1-weighted (T1w) MRI volumes from seven studies across six scanners. Model performance was evaluated using internal 5-fold cross validation (5FCV) and leave-one-site-out cross validation (LOSOCV). MedNeXt-L-k5 models trained on the T2w images of the HCP-Aging dataset achieved voxel-level Dice scores of 0.88+/-0.06 (white matter, WM), comparable to the reported inter-rater reliability of that dataset, and the highest yet reported in the literature. The same models trained on the T1w images of the HCP-Aging dataset achieved a substantially lower Dice score of 0.58+/-0.09 (WM). Under LOSOCV, the model had voxel-level Dice scores of 0.38+/-0.16 (WM) and 0.35+/-0.12 (BG), and cluster-level Dice scores of 0.61+/-0.19 (WM) and 0.62+/-0.21 (BG). MedNeXt-L-k5 provides an efficient solution for automated PVS segmentation across diverse T1w and T2w MRI datasets. MedNeXt-L-k5 did not outperform the nnU-Net, indicating that the attention-based mechanisms present in transformer-inspired models to provide global context are not required for high accuracy in PVS segmentation.</li>
<li><strong>摘要：</strong>血管周围空间扩大（PVS）越来越被公认为是脑小血管疾病，阿尔茨海默氏病，中风和与衰老相关的神经变性的生物标志物。但是，PVS的手动分割是耗时的，并且具有适度的评估者间可靠性，而现有的自动化深度学习模型的性能中等，并且通常无法跨越各种临床和研究MRI数据集。我们为自动化的PVS分割调整了一种由变压器启发的3D编码器卷积网络的Mednext-L-K5。对两种模型进行了训练：一种使用人类连接组项目 - 型（HCP-GAIGE）基准的200 T2加权（T2W）MRI扫描的均质数据集，另一个使用40个异质T1加权（T1W）MRI量使用40个研究中的六项研究中的六项研究中的MRI量。使用内部5倍交叉验证（5FCV）和保留的位点交叉验证（LOSOCV）评估模型性能。在HCP-Agging数据集的T2W图像上训练的Mednext-L-K5模型获得了0.88 +/- 0.06（白奇，WM）的体素级骰子得分，与该数据集的报道间可靠性相当，并且是文献中尚未报道的最高的。在HCP-GIGE数据集的T1W图像上训练的相同模型的骰子得分达到0.58 +/- 0.09（WM）。在LOSOCV下，该模型的体素级骰子得分为0.38 +/- 0.16（WM）和0.35 +/- 0.12（BG），群集 - 级骰子分数为0.61 +/- 0.19（WM）（WM）和0.62 +/- 0.21（BG）。 MEDNEXT-L-K5为跨不同T1W和T2W MRI数据集的自动PVS分割提供了有效的解决方案。 MEDNEXT-L-K5并没有胜过NNU-NET，这表明由变压器启发的模型中存在的基于注意力的机制提供全局上下文，对于PVS分割的高精度并不是必需的。</li>
</ul>

<h3>Title: A Systematic Review on the Generative AI Applications in Human Medical Genomics</h3>
<ul>
<li><strong>Authors: </strong>Anton Changalidis, Yury Barbitoff, Yulia Nasykhova, Andrey Glotov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20275">https://arxiv.org/abs/2508.20275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20275">https://arxiv.org/pdf/2508.20275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20275]] A Systematic Review on the Generative AI Applications in Human Medical Genomics(https://arxiv.org/abs/2508.20275)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Although traditional statistical techniques and machine learning methods have contributed significantly to genetics and, in particular, inherited disease diagnosis, they often struggle with complex, high-dimensional data, a challenge now addressed by state-of-the-art deep learning models. Large language models (LLMs), based on transformer architectures, have excelled in tasks requiring contextual comprehension of unstructured medical data. This systematic review examines the role of LLMs in the genetic research and diagnostics of both rare and common diseases. Automated keyword-based search in PubMed, bioRxiv, medRxiv, and arXiv was conducted, targeting studies on LLM applications in diagnostics and education within genetics and removing irrelevant or outdated models. A total of 172 studies were analyzed, highlighting applications in genomic variant identification, annotation, and interpretation, as well as medical imaging advancements through vision transformers. Key findings indicate that while transformer-based models significantly advance disease and risk stratification, variant interpretation, medical imaging analysis, and report generation, major challenges persist in integrating multimodal data (genomic sequences, imaging, and clinical records) into unified and clinically robust pipelines, facing limitations in generalizability and practical implementation in clinical settings. This review provides a comprehensive classification and assessment of the current capabilities and limitations of LLMs in transforming hereditary disease diagnostics and supporting genetic education, serving as a guide to navigate this rapidly evolving field.</li>
<li><strong>摘要：</strong>尽管传统的统计技术和机器学习方法对遗传学，尤其是遗传疾病的诊断做出了重大贡献，但它们经常在复杂的高维数据中挣扎，这是最先进的深度学习模型现在提出的挑战。大型语言模型（LLMS）基于变压器体系结构，在需要上下文理解非结构化医学数据的任务上表现出色。该系统评价研究了LLM在稀有疾病和常见疾病的遗传研究和诊断中的作用。在PubMed，Biorxiv，MedRxiv和Arxiv中进行了基于自动关键字的搜索，针对遗传学内诊断和教育中LLM应用的研究，并删除无关或过时的模型。总共分析了172项研究，突出了基因组变异鉴定，注释和解释以及通过视觉变压器的医学成像进步的应用。关键发现表明，尽管基于变压器的模型可显着提高疾病和风险分层，变异解释，医学成像分析和报告的产生，但主要挑战持续存在于将多模式数据（基因组序列，成像和临床记录）整合到统一和临床上强大的管道中，在临床设置中实现概括性和实践实施中的限制。这篇综述提供了对LLMS转化遗传性疾病诊断和支持基因教育的当前能力和局限性的全面分类和评估，并作为导航这一快速发展的领域的指南。</li>
</ul>

<h3>Title: Disentangling Latent Embeddings with Sparse Linear Concept Subspaces (SLiCS)</h3>
<ul>
<li><strong>Authors: </strong>Zhi Li, Hau Phan, Matthew Emigh, Austin J. Brockmeier</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20322">https://arxiv.org/abs/2508.20322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20322">https://arxiv.org/pdf/2508.20322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20322]] Disentangling Latent Embeddings with Sparse Linear Concept Subspaces (SLiCS)(https://arxiv.org/abs/2508.20322)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision-language co-embedding networks, such as CLIP, provide a latent embedding space with semantic information that is useful for downstream tasks. We hypothesize that the embedding space can be disentangled to separate the information on the content of complex scenes by decomposing the embedding into multiple concept-specific component vectors that lie in different subspaces. We propose a supervised dictionary learning approach to estimate a linear synthesis model consisting of sparse, non-negative combinations of groups of vectors in the dictionary (atoms), whose group-wise activity matches the multi-label information. Each concept-specific component is a non-negative combination of atoms associated to a label. The group-structured dictionary is optimized through a novel alternating optimization with guaranteed convergence. Exploiting the text co-embeddings, we detail how semantically meaningful descriptions can be found based on text embeddings of words best approximated by a concept's group of atoms, and unsupervised dictionary learning can exploit zero-shot classification of training set images using the text embeddings of concept labels to provide instance-wise multi-labels. We show that the disentangled embeddings provided by our sparse linear concept subspaces (SLiCS) enable concept-filtered image retrieval (and conditional generation using image-to-prompt) that is more precise. We also apply SLiCS to highly-compressed autoencoder embeddings from TiTok and the latent embedding from self-supervised DINOv2. Quantitative and qualitative results highlight the improved precision of the concept-filtered image retrieval for all embeddings.</li>
<li><strong>摘要：</strong>视觉语言共同安装网络（例如剪辑）提供了一个潜在的嵌入空间，其中包含对下游任务有用的语义信息。我们假设可以通过将嵌入到不同子空间中的多个特定于概念的组件向量中，将嵌入空间分开以分离复杂场景的内容。我们提出了一种监督的词典学习方法，以估算一个线性合成模型，该模型由词典（原子）中较少的非阴性组成组成，该词（原子）的群体活动与多标签信息相匹配。每个特定于概念的成分都是与标签相关的原子的非负组合。组结构的词典通过新颖的交替优化和保证的收敛来优化。利用文本共同构思，我们详细介绍了如何根据概念的原子组最近似的文字嵌入语义上的描述，而无监督的词典学习可以利用训练集的零弹药分类，使用概念标签的文本嵌入来提供实例的多人层。我们表明，我们稀疏的线性概念子空间（SLIC）提供的嵌入嵌入启用更精确的概念过滤图像检索（以及使用图像到预测的有条件生成）。我们还将SLICS应用于Titok的高度压缩自动编码器嵌入以及自我监管的Dinov2的潜在嵌入。定量和定性结果突出了所有嵌入的概念过滤图像检索的精度。</li>
</ul>

<h3>Title: FORGE: Foundational Optimization Representations from Graph Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Zohair Shafi, Serdar Kadioglu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20330">https://arxiv.org/abs/2508.20330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20330">https://arxiv.org/pdf/2508.20330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20330]] FORGE: Foundational Optimization Representations from Graph Embeddings(https://arxiv.org/abs/2508.20330)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Combinatorial optimization problems are ubiquitous in science and engineering, yet learning-based approaches to accelerate their solution often require solving a large number of hard-to-solve optimization instances to collect training data, incurring significant computational overhead. Existing methods require training dedicated models for each problem distribution for each downstream task, severely limiting their scalability and generalization. In this work, we introduce Forge, a method of pre-training a vector-quantized graph autoencoder on a large and diverse collection of mixed-integer programming (MIP) instances in an unsupervised fashion without dependency on their solution. The vector quantization process creates discrete code assignments that act as a vocabulary to represent optimization instances. We evaluate our approach under both supervised and unsupervised settings. For the unsupervised setting, we demonstrate that Forge embeddings effectively differentiate and cluster unseen instances. For the supervised setting, we fine-tune Forge embeddings and show that a single model predicts both the variables for warm-starts and integrality gaps for cut-generation across multiple problem type distributions. Both predictions help improve performance of a state-of-the-art, commercial optimization solver. Finally, we release our code and pre-trained Forge weights to encourage further research and practical use of instance-level MIP embeddings at this https URL</li>
<li><strong>摘要：</strong>组合优化问题在科学和工程中无处不在，但基于学习的方法来加速其解决方案通常需要解决大量难以解决的优化实例，以收集培训数据，从而产生了重要的计算开销。现有方法需要针对每个下游任务的每个问题分布的培训专用模型，从而严重限制了其可扩展性和概括性。在这项工作中，我们介绍了Forge，这是一种预先训练矢量量化的图形自动编码器，以无需依赖于解决方案的方式，以无监督的方式收集了大量而多样化的混合智能编程（MIP）实例。向量量化过程创建了离散的代码分配，用作代表优化实例的词汇。我们在监督和无监督的设置下评估我们的方法。对于无监督的设置，我们证明锻造嵌入有效区分和群集看不见的实例。对于监督的设置，我们对锻炼嵌入进行微调，并表明单个模型可以预测温暖启动的变量，也可以预测跨多个问题类型分布的切割生成的完整性差距。这两个预测都有助于提高最先进的商业优化求解器的性能。最后，我们发布了代码和预训练的锻造权重，以鼓励在此HTTPS URL上进行进一步研究和实际使用实例级MIP嵌入</li>
</ul>

<h3>Title: MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Xiao Li, Yanfan Zhu, Ruining Deng, Wei-Qi Wei, Yu Wang, Shilin Zhao, Yaohong Wang, Haichun Yang, Yuankai Huo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20345">https://arxiv.org/abs/2508.20345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20345">https://arxiv.org/pdf/2508.20345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20345]] MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models(https://arxiv.org/abs/2508.20345)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in medical vision-language models (VLMs) open up remarkable opportunities for clinical applications such as automated report generation, copilots for physicians, and uncertainty quantification. However, despite their promise, medical VLMs introduce serious security concerns, most notably risks of Protected Health Information (PHI) exposure, data leakage, and vulnerability to cyberthreats - which are especially critical in hospital environments. Even when adopted for research or non-clinical purposes, healthcare organizations must exercise caution and implement safeguards. To address these challenges, we present MedFoundationHub, a graphical user interface (GUI) toolkit that: (1) enables physicians to manually select and use different models without programming expertise, (2) supports engineers in efficiently deploying medical VLMs in a plug-and-play fashion, with seamless integration of Hugging Face open-source models, and (3) ensures privacy-preserving inference through Docker-orchestrated, operating system agnostic deployment. MedFoundationHub requires only an offline local workstation equipped with a single NVIDIA A6000 GPU, making it both secure and accessible within the typical resources of academic research labs. To evaluate current capabilities, we engaged board-certified pathologists to deploy and assess five state-of-the-art VLMs (Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and LLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases, yielding 1015 clinician-model scoring events. These assessments revealed recurring limitations, including off-target answers, vague reasoning, and inconsistent pathology terminology.</li>
<li><strong>摘要：</strong>医学视觉模型（VLMS）的最新进展为临床应用提供了显着的机会，例如自动报告的生成，医师的副驾驶和不确定性量化。然而，尽管有希望，医疗VLMS引入了严重的安全问题，最著名的是受保护的健康信息（PHI）暴露，数据泄漏和对网络威胁的脆弱性的风险 - 这在医院环境中尤其重要。即使用于研究或非临床目的，医疗机构也必须谨慎行事并实施保障措施。 To address these challenges, we present MedFoundationHub, a graphical user interface (GUI) toolkit that: (1) enables physicians to manually select and use different models without programming expertise, (2) supports engineers in efficiently deploying medical VLMs in a plug-and-play fashion, with seamless integration of Hugging Face open-source models, and (3) ensures privacy-preserving inference through Docker-Orchested，操作系统不可知的部署。 MedFoundationHub仅需要一个配备了单个NVIDIA A6000 GPU的离线当地工作站，这使其在学术研究实验室的典型资源中既安全又易于使用。为了评估当前功能，我们聘请了董事会认证的病理学家部署和评估五个最先进的VLM（Google-Medgemma3-4b，qwen2-vl-7b-7b-instruction，qwen2.5-vl-vl-7b-7b-instruct和lllava-1.5-7b/13b）。专家评估涵盖了结肠病例和肾脏病例，产生了1015个临床医生模型评分事件。这些评估表明，重复的局限性，包括脱靶答案，模糊的推理和病理术语不一致。</li>
</ul>

<h3>Title: DFAMS: Dynamic-flow guided Federated Alignment based Multi-prototype Search</h3>
<ul>
<li><strong>Authors: </strong>Zhibang Yang, Xinke Jiang, Rihong Qiu, Ruiqing Li, Yihang Zhang, Yue Fang, Yongxin Xu, Hongxin Ding, Xu Chu, Junfeng Zhao, Yasha Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20353">https://arxiv.org/abs/2508.20353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20353">https://arxiv.org/pdf/2508.20353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20353]] DFAMS: Dynamic-flow guided Federated Alignment based Multi-prototype Search(https://arxiv.org/abs/2508.20353)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Federated Retrieval (FR) routes queries across multiple external knowledge sources, to mitigate hallucinations of LLMs, when necessary external knowledge is distributed. However, existing methods struggle to retrieve high-quality and relevant documents for ambiguous queries, especially in cross-domain scenarios, which significantly limits their effectiveness in supporting downstream generation tasks. Inspired by dynamic information flow (DIF), we propose DFAMS, a novel framework that leverages DIF to identify latent query intents and construct semantically aligned knowledge partitions for accurate retrieval across heterogeneous sources. Specifically, DFAMS probes the DIF in LLMs by leveraging gradient signals from a few annotated queries and employing Shapley value-based attribution to trace neuron activation paths associated with intent recognition and subdomain boundary detection. Then, DFAMS leverages DIF to train an alignment module via multi-prototype contrastive learning, enabling fine-grained intra-source modeling and inter-source semantic alignment across knowledge bases. Experimental results across five benchmarks show that DFAMS outperforms advanced FR methods by up to 14.37% in knowledge classification accuracy, 5.38% in retrieval recall, and 6.45% in downstream QA accuracy, demonstrating its effectiveness in complex FR scenarios.</li>
<li><strong>摘要：</strong>联合检索（FR）路线跨多个外部知识来源的查询，以减轻LLM的幻觉，并在必要时分发外部知识。但是，现有的方法难以检索模棱两可的查询的高质量和相关文档，尤其是在跨域情景中，这大大限制了它们在支持下游生成任务方面的有效性。受动态信息流的启发（DIF），我们提出了DFAM，这是一个新型框架，利用DIF来识别潜在的查询意图并构建语义对准知识分区，以在异构源中进行准确检索。具体而言，DFAM通过利用一些注释的查询利用梯度信号并采用基于沙普利的归因来探测LLMS中的DIF，以跟踪与意图识别和子域边界检测相关的神经元激活路径。然后，DFAM利用DIF来通过多型对比度学习来训练一个对齐模块，从而实现了跨知识库的精细元素内源建模和源源源语义对齐。五个基准测试的实验结果表明，DFAM的知识分类准确性高达14.37％，在检索召回率中高达14.37％，下游QA的准确性为6.45％，在复杂的FR场景中表现出其有效性。</li>
</ul>

<h3>Title: A Spatial-Frequency Aware Multi-Scale Fusion Network for Real-Time Deepfake Detection</h3>
<ul>
<li><strong>Authors: </strong>Libo Lv, Tianyi Wang, Mengxiao Huang, Ruixia Liu, Yinglong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20449">https://arxiv.org/abs/2508.20449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20449">https://arxiv.org/pdf/2508.20449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20449]] A Spatial-Frequency Aware Multi-Scale Fusion Network for Real-Time Deepfake Detection(https://arxiv.org/abs/2508.20449)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of real-time deepfake generation techniques, forged content is becoming increasingly realistic and widespread across applications like video conferencing and social media. Although state-of-the-art detectors achieve high accuracy on standard benchmarks, their heavy computational cost hinders real-time deployment in practical applications. To address this, we propose the Spatial-Frequency Aware Multi-Scale Fusion Network (SFMFNet), a lightweight yet effective architecture for real-time deepfake detection. We design a spatial-frequency hybrid aware module that jointly leverages spatial textures and frequency artifacts through a gated mechanism, enhancing sensitivity to subtle manipulations. A token-selective cross attention mechanism enables efficient multi-level feature interaction, while a residual-enhanced blur pooling structure helps retain key semantic cues during downsampling. Experiments on several benchmark datasets show that SFMFNet achieves a favorable balance between accuracy and efficiency, with strong generalization and practical value for real-time applications.</li>
<li><strong>摘要：</strong>随着实时深层生成技术的快速发展，在视频会议和社交媒体等应用程序中，锻造内容变得越来越现实和广泛。尽管最先进的探测器在标准基准上具有很高的精度，但它们的大量计算成本阻碍了实际应用中的实时部署。为了解决这个问题，我们提出了空间频率意识到的多尺度融合网络（SFMFNET），这是一种轻巧但有效的实时深层检测体系结构。我们设计了一个空间频率混合意识模块，该模块通过封闭的机制共同利用空间纹理和频率伪像，从而增强了对微妙操纵的敏感性。令牌选择性的交叉注意机制可实现有效的多级特征相互作用，而残留增强的模糊池结构有助于在下采样过程中保留关键的语义线索。几个基准数据集的实验表明，SFMFNET在准确性和效率之间取得了良好的平衡，具有强大的概括和实时应用的实践价值。</li>
</ul>

<h3>Title: Evaluating Differentially Private Generation of Domain-Specific Text</h3>
<ul>
<li><strong>Authors: </strong>Yidan Sun, Viktor Schlegel, Srinivasan Nandakumar, Iqra Zahid, Yuping Wu, Warren Del-Pinto, Goran Nenadic, Siew-Kei Lam, Jie Zhang, Anil A Bharath</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20452">https://arxiv.org/abs/2508.20452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20452">https://arxiv.org/pdf/2508.20452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20452]] Evaluating Differentially Private Generation of Domain-Specific Text(https://arxiv.org/abs/2508.20452)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative AI offers transformative potential for high-stakes domains such as healthcare and finance, yet privacy and regulatory barriers hinder the use of real-world data. To address this, differentially private synthetic data generation has emerged as a promising alternative. In this work, we introduce a unified benchmark to systematically evaluate the utility and fidelity of text datasets generated under formal Differential Privacy (DP) guarantees. Our benchmark addresses key challenges in domain-specific benchmarking, including choice of representative data and realistic privacy budgets, accounting for pre-training and a variety of evaluation metrics. We assess state-of-the-art privacy-preserving generation methods across five domain-specific datasets, revealing significant utility and fidelity degradation compared to real data, especially under strict privacy constraints. These findings underscore the limitations of current approaches, outline the need for advanced privacy-preserving data sharing methods and set a precedent regarding their evaluation in realistic scenarios.</li>
<li><strong>摘要：</strong>生成的AI为医疗保健和金融等高风险领域提供了变革潜力，但隐私和监管障碍阻碍了现实世界数据的使用。为了解决这个问题，差异化的综合数据生成已成为一种有希望的选择。在这项工作中，我们引入了一个统一的基准测试，以系统地评估在正式差异隐私（DP）保证下生成的文本数据集的实用性和保真度。我们的基准测试解决了特定于领域的基准测试中的主要挑战，包括选择代表性数据和现实的隐私预算，对培训预先培训和各种评估指标的核算。我们评估了五个特定领域数据集的最新隐私生成方法，与真实数据相比，揭示了巨大的实用性和忠诚度降级，尤其是在严格的隐私限制下。这些发现强调了当前方法的局限性，概述了对高级隐私数据共享方法的需求，并在现实情况下为其评估设定了先例。</li>
</ul>

<h3>Title: Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiaochuan Li, Guoguang Du, Runze Zhang, Liang Jin, Qi Jia, Lihua Lu, Zhenhua Guo, Yaqian Zhao, Haiyang Liu, Tianqi Wang, Changsheng Li, Xiaoli Gong, Rengang Li, Baoyu Fan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20470">https://arxiv.org/abs/2508.20470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20470">https://arxiv.org/pdf/2508.20470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20470]] Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation(https://arxiv.org/abs/2508.20470)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Scaling laws have validated the success and promise of large-data-trained models in creative generation across text, image, and video domains. However, this paradigm faces data scarcity in the 3D domain, as there is far less of it available on the internet compared to the aforementioned modalities. Fortunately, there exist adequate videos that inherently contain commonsense priors, offering an alternative supervisory signal to mitigate the generalization bottleneck caused by limited native 3D data. On the one hand, videos capturing multiple views of an object or scene provide a spatial consistency prior for 3D generation. On the other hand, the rich semantic information contained within the videos enables the generated content to be more faithful to the text prompts and semantically plausible. This paper explores how to apply the video modality in 3D asset generation, spanning datasets to models. We introduce Droplet3D-4M, the first large-scale video dataset with multi-view level annotations, and train Droplet3D, a generative model supporting both image and dense text input. Extensive experiments validate the effectiveness of our approach, demonstrating its ability to produce spatially consistent and semantically plausible content. Moreover, in contrast to the prevailing 3D solutions, our approach exhibits the potential for extension to scene-level applications. This indicates that the commonsense priors from the videos significantly facilitate 3D creation. We have open-sourced all resources including the dataset, code, technical framework, and model weights: this https URL.</li>
<li><strong>摘要：</strong>缩放定律已验证了跨文本，图像和视频域中创意生成中大型培训模型的成功和希望。但是，这种范式在3D域中面临数据稀缺，因为与上述方式相比，Internet上的可用性少得多。幸运的是，存在足够的视频固有地包含常识性先验，并提供了替代的监督信号，以减轻由有限的本机3D数据引起的概括瓶颈。一方面，捕获对象或场景的多个视图的视频提供了3D生成之前的空间一致性。另一方面，视频中包含的丰富语义信息使生成的内容更忠实于文本提示和语义上的合理。本文探讨了如何将视频模式应用于3D资产生成中，将数据集跨越模型。我们介绍了滴滴3D-4M，这是第一个带有多视图级别注释的大型视频数据集，以及Train Droplet3D，这是一个支持图像和密集文本输入的生成模型。广泛的实验验证了我们方法的有效性，证明了其产生空间一致和语义上合理的内容的能力。此外，与流行的3D解决方案相反，我们的方法具有扩展到场景级应用的潜力。这表明来自视频的常识性先验极大地促进了3D的创建。我们已经开源了所有资源，包括数据集，代码，技术框架和模型权重：此HTTPS URL。</li>
</ul>

<h3>Title: Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiusi Li, Jackson Jiang, Jinyu Miao, Miao Long, Tuopu Wen, Peijin Jia, Shengxiang Liu, Chunlei Yu, Maolin Liu, Yuzhan Cai, Kun Jiang, Mengmeng Yang, Diange Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20471">https://arxiv.org/abs/2508.20471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20471">https://arxiv.org/pdf/2508.20471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20471]] Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation(https://arxiv.org/abs/2508.20471)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Corner cases are crucial for training and validating autonomous driving systems, yet collecting them from the real world is often costly and hazardous. Editing objects within captured sensor data offers an effective alternative for generating diverse scenarios, commonly achieved through 3D Gaussian Splatting or image generative models. However, these approaches often suffer from limited visual fidelity or imprecise pose control. To address these issues, we propose G^2Editor, a framework designed for photorealistic and precise object editing in driving videos. Our method leverages a 3D Gaussian representation of the edited object as a dense prior, injected into the denoising process to ensure accurate pose control and spatial consistency. A scene-level 3D bounding box layout is employed to reconstruct occluded areas of non-target objects. Furthermore, to guide the appearance details of the edited object, we incorporate hierarchical fine-grained features as additional conditions during generation. Experiments on the Waymo Open Dataset demonstrate that G^2Editor effectively supports object repositioning, insertion, and deletion within a unified framework, outperforming existing methods in both pose controllability and visual quality, while also benefiting downstream data-driven tasks.</li>
<li><strong>摘要：</strong>角案件对于培训和验证自主驾驶系统至关重要，但是从现实世界中收集它们通常是昂贵和危险的。捕获的传感器数据中的编辑对象提供了一种有效的替代方案，可以通过3D高斯碎片或图像生成模型来生成各种场景。但是，这些方法通常受到视觉保真度有限或姿势控制不精确的影响。为了解决这些问题，我们提出了G^2Editor，该框架是为播放视频中的感性和精确对象编辑而设计的框架。我们的方法利用编辑对象作为密集的先验的3D高斯表示，并注入了剥离过程中，以确保准确的姿势控制和空间一致性。场景级别的3D边界框布局用于重建非目标对象的遮挡区域。此外，为了指导编辑对象的外观细节，我们将层次的细颗粒特征作为生成期间的其他条件。 Waymo打开数据集的实验表明，G^2Editor在统一框架中有效地支持对象重新定位，插入和删除，从而在姿势可控性和视觉质量中都优于现有方法，同时也使下游数据驱动的任务受益。</li>
</ul>

<h3>Title: Enhancing Corpus Callosum Segmentation in Fetal MRI via Pathology-Informed Domain Randomization</h3>
<ul>
<li><strong>Authors: </strong>Marina Grifell i Plana, Vladyslav Zalevskyi, Léa Schmidt, Yvan Gomez, Thomas Sanchez, Vincent Dunet, Mériam Koob, Vanessa Siffredi, Meritxell Bach Cuadra</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20475">https://arxiv.org/abs/2508.20475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20475">https://arxiv.org/pdf/2508.20475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20475]] Enhancing Corpus Callosum Segmentation in Fetal MRI via Pathology-Informed Domain Randomization(https://arxiv.org/abs/2508.20475)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Accurate fetal brain segmentation is crucial for extracting biomarkers and assessing neurodevelopment, especially in conditions such as corpus callosum dysgenesis (CCD), which can induce drastic anatomical changes. However, the rarity of CCD severely limits annotated data, hindering the generalization of deep learning models. To address this, we propose a pathology-informed domain randomization strategy that embeds prior knowledge of CCD manifestations into a synthetic data generation pipeline. By simulating diverse brain alterations from healthy data alone, our approach enables robust segmentation without requiring pathological annotations. We validate our method on a cohort comprising 248 healthy fetuses, 26 with CCD, and 47 with other brain pathologies, achieving substantial improvements on CCD cases while maintaining performance on both healthy fetuses and those with other pathologies. From the predicted segmentations, we derive clinically relevant biomarkers, such as corpus callosum length (LCC) and volume, and show their utility in distinguishing CCD subtypes. Our pathology-informed augmentation reduces the LCC estimation error from 1.89 mm to 0.80 mm in healthy cases and from 10.9 mm to 0.7 mm in CCD cases. Beyond these quantitative gains, our approach yields segmentations with improved topological consistency relative to available ground truth, enabling more reliable shape-based analyses. Overall, this work demonstrates that incorporating domain-specific anatomical priors into synthetic data pipelines can effectively mitigate data scarcity and enhance analysis of rare but clinically significant malformations.</li>
<li><strong>摘要：</strong>准确的胎儿脑分割对于提取生物标志物和评估神经发育至关重要，尤其是在诸如call体性疾病失调（CCD）之类的条件下，这可以诱导急剧的解剖学变化。但是，CCD的稀有性严重限制了注释数据，阻碍了深度学习模型的概括。为了解决这个问题，我们提出了一种病理信息的域随机化策略，该策略将CCD表现的先验知识嵌入到合成数据生成管道中。通过模拟单独使用健康数据的大脑改变，我们的方法可实现强大的分割，而无需进行病理注释。我们验证了包括248个健康胎儿的队列方法，有26个具有CCD，有47个具有其他脑病理学的方法，在CCD病例上取得了重大改善，同时维持健康胎儿和其他病理学患者的表现。根据预测的分割，我们得出了临床上相关的生物标志物，例如call体长度（LCC）和体积，并显示了它们在区分CCD亚型方面的效用。在健康病例中，我们的病理信息增强量将LCC估计误差从1.89 mm降低至0.80 mm，在CCD病例中，LCC估计误差从10.80 mm降低至10.7 mm。除了这些定量增长之外，我们的方法还产生了相对于可用地面真理的拓扑一致性提高的分割，从而实现了更可靠的基于形状的分析。总体而言，这项工作表明，将特定区域的解剖学先验纳入合成数据管道中可以有效地减轻数据稀缺性并增强对罕见但临床上显着畸形的分析。</li>
</ul>

<h3>Title: Towards Inclusive Communication: A Unified LLM-Based Framework for Sign Language, Lip Movements, and Audio Understanding</h3>
<ul>
<li><strong>Authors: </strong>Jeong Hun Yeo, Hyeongseop Rha, Sungjune Park, Junil Won, Yong Man Ro</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM, eess.AS, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20476">https://arxiv.org/abs/2508.20476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20476">https://arxiv.org/pdf/2508.20476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20476]] Towards Inclusive Communication: A Unified LLM-Based Framework for Sign Language, Lip Movements, and Audio Understanding(https://arxiv.org/abs/2508.20476)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Audio is the primary modality for human communication and has driven the success of Automatic Speech Recognition (ASR) technologies. However, such systems remain inherently inaccessible to individuals who are deaf or hard of hearing. Visual alternatives such as sign language and lip reading offer effective substitutes, and recent advances in Sign Language Translation (SLT) and Visual Speech Recognition (VSR) have improved audio-less communication. Yet, these modalities have largely been studied in isolation, and their integration within a unified framework remains underexplored. In this paper, we introduce the first unified framework capable of handling diverse combinations of sign language, lip movements, and audio for spoken-language text generation. We focus on three main objectives: (i) designing a unified, modality-agnostic architecture capable of effectively processing heterogeneous inputs; (ii) exploring the underexamined synergy among modalities, particularly the role of lip movements as non-manual cues in sign language comprehension; and (iii) achieving performance on par with or superior to state-of-the-art models specialized for individual tasks. Building on this framework, we achieve performance on par with or better than task-specific state-of-the-art models across SLT, VSR, ASR, and AVSR. Furthermore, our analysis reveals that explicitly modeling lip movements as a separate modality significantly improves SLT performance.</li>
<li><strong>摘要：</strong>音频是人类交流的主要方式，并推动了自动语音识别（ASR）技术的成功。但是，对于聋哑人或难以听见的个体，这种系统固有地无法访问。诸如手语和唇部阅读之类的视觉替代方案提供了有效的替代品，手语翻译（SLT）和视觉语音识别（VSR）的最新进展已改善了无音频的通信。然而，这些模式在很大程度上是孤立研究的，它们在统一框架中的整合仍然没有被解散。在本文中，我们介绍了第一个统一框架，能够处理语言，唇部运动和语音文字生成音频的各种组合。我们专注于三个主要目标：（i）设计一个能够有效处理异质输入的统一的，模态的架构； （ii）探索模式之间不渗透的协同作用，尤其是唇部运动在手语理解中的非手册线索的作用； （iii）与专门用于单个任务的最先进模型达到或优越。在此框架的基础上，我们在SLT，VSR，ASR和AVSR的特定于任务特定于任务的最先进模型上取得了表现。此外，我们的分析表明，将唇部运动作为一种单独的方式显式建模可显着提高SLT性能。</li>
</ul>

<h3>Title: Describe, Don't Dictate: Semantic Image Editing with Natural Language Intent</h3>
<ul>
<li><strong>Authors: </strong>En Ci, Shanyan Guan, Yanhao Ge, Yilin Zhang, Wei Li, Zhenyu Zhang, Jian Yang, Ying Tai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20505">https://arxiv.org/abs/2508.20505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20505">https://arxiv.org/pdf/2508.20505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20505]] Describe, Don't Dictate: Semantic Image Editing with Natural Language Intent(https://arxiv.org/abs/2508.20505)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Despite the progress in text-to-image generation, semantic image editing remains a challenge. Inversion-based algorithms unavoidably introduce reconstruction errors, while instruction-based models mainly suffer from limited dataset quality and scale. To address these problems, we propose a descriptive-prompt-based editing framework, named DescriptiveEdit. The core idea is to re-frame `instruction-based image editing' as `reference-image-based text-to-image generation', which preserves the generative power of well-trained Text-to-Image models without architectural modifications or inversion. Specifically, taking the reference image and a prompt as input, we introduce a Cross-Attentive UNet, which newly adds attention bridges to inject reference image features into the prompt-to-edit-image generation process. Owing to its text-to-image nature, DescriptiveEdit overcomes limitations in instruction dataset quality, integrates seamlessly with ControlNet, IP-Adapter, and other extensions, and is more scalable. Experiments on the Emu Edit benchmark show it improves editing accuracy and consistency.</li>
<li><strong>摘要：</strong>尽管文本到图像的生成取得了进展，但语义图像编辑仍然是一个挑战。基于反演的算法不可避免地会引入重建错误，而基于指令的模型主要受到数据集质量和规模有限的影响。为了解决这些问题，我们提出了一个基于描述性的编辑框架，名为DescriptiveEdit。核心思想是将基于指令的图像编辑重新构架为“基于参考图像的文本到图像生成”，该生成可以保留训练有素的文本到图像模型的生成力，而无需进行体系结构修改或倒置。具体而言，以参考图像和提示为输入，我们引入了一个跨指挥性的UNET，该UNET新将注意力桥梁添加到及时到达图像形象生成过程中。由于其文本到图像性质，描述性edit克服了指令数据集质量的限制，与ControlNet，IP-Adapter和其他扩展程序无缝集成，并且更可扩展。 EMU编辑基准测试的实验表明它提高了编辑的准确性和一致性。</li>
</ul>

<h3>Title: Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for Unsupervised 3D Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Mingqian Ji, Jian Yang, Shanshan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20530">https://arxiv.org/abs/2508.20530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20530">https://arxiv.org/pdf/2508.20530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20530]] Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for Unsupervised 3D Object Detection(https://arxiv.org/abs/2508.20530)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Existing LiDAR-based 3D object detectors typically rely on manually annotated labels for training to achieve good performance. However, obtaining high-quality 3D labels is time-consuming and labor-intensive. To address this issue, recent works explore unsupervised 3D object detection by introducing RGB images as an auxiliary modal to assist pseudo-box generation. However, these methods simply integrate pseudo-boxes generated by LiDAR point clouds and RGB images. Yet, such a label-level fusion strategy brings limited improvements to the quality of pseudo-boxes, as it overlooks the complementary nature in terms of LiDAR and RGB image data. To overcome the above limitations, we propose a novel data-level fusion framework that integrates RGB images and LiDAR data at an early stage. Specifically, we utilize vision foundation models for instance segmentation and depth estimation on images and introduce a bi-directional fusion method, where real points acquire category labels from the 2D space, while 2D pixels are projected onto 3D to enhance real point density. To mitigate noise from depth and segmentation estimations, we propose a local and global filtering method, which applies local radius filtering to suppress depth estimation errors and global statistical filtering to remove segmentation-induced outliers. Furthermore, we propose a data-level fusion based dynamic self-evolution strategy, which iteratively refines pseudo-boxes under a dense representation, significantly improving localization accuracy. Extensive experiments on the nuScenes dataset demonstrate that the detector trained by our method significantly outperforms that trained by previous state-of-the-art methods with 28.4$\%$ mAP on the nuScenes validation benchmark.</li>
<li><strong>摘要：</strong>现有的基于LIDAR的3D对象检测器通常依靠手动注释的标签进行训练以实现良好的性能。但是，获得高质量的3D标签是耗时且劳动密集型的。为了解决这个问题，最近的工作通过引入RGB图像作为辅助模态来探索无监督的3D对象检测，以帮助伪盒生成。但是，这些方法只是整合了LiDar Point Cloud和RGB图像生成的伪盒。然而，这样的标签级融合策略给伪盒的质量带来了有限的改进，因为它可以忽略LIDAR和RGB图像数据的互补性。为了克服上述局限性，我们提出了一个新颖的数据级融合框架，该框架在早期阶段整合RGB图像和LiDAR数据。具体而言，我们利用视觉基础模型，例如对图像进行分割和深度估计，并引入了双向融合方法，其中实际点从2D空间中获取类别标签，而2D像素则投影到3D上以增强实际点密度。为了减轻深度和分割估计的噪声，我们提出了一种局部和全局过滤方法，该方法应用局部半径过滤来抑制深度估计误差和全局统计过滤以删除分割诱导的异常值。此外，我们提出了一种基于数据级融合的动态自我进化策略，该策略在密集表示下迭代地完善了伪盒，从而显着提高了本地化精度。在Nuscenes数据集上进行的广泛实验表明，通过我们的方法训练的检测器大大优于先前最先进的方法，并在Nuscenes验证基准中使用28.4 $ \％$ $ MAP培训。</li>
</ul>

<h3>Title: MedGR$^2$: Breaking the Data Barrier for Medical Reasoning via Generative Reward Learning</h3>
<ul>
<li><strong>Authors: </strong>Weihai Zhi, Jiayan Guo, Shangyang Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20549">https://arxiv.org/abs/2508.20549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20549">https://arxiv.org/pdf/2508.20549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20549]] MedGR$^2$: Breaking the Data Barrier for Medical Reasoning via Generative Reward Learning(https://arxiv.org/abs/2508.20549)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The application of Vision-Language Models (VLMs) in medicine is critically hampered by the scarcity of high-quality, expert-annotated data. Supervised Fine-Tuning (SFT) on existing datasets often leads to poor generalization on unseen modalities and tasks, while Reinforcement Learning (RL), a promising alternative, is stymied by the lack of reliable reward signals in this data-scarce domain. To break this impasse, we introduce Generative Reward Learning for Medical Reasoning (MedGR$^2$), a novel framework that creates a self-improving virtuous cycle. MedGR$^2$ co-develops a data generator and a reward model, enabling the automated, continuous creation of high-quality, multi-modal medical data that serves as both a superior training source for SFT and RL. Our experiments demonstrate that SFT with MedGR$^2$-produced data already surpasses baselines trained on large-scale, human-curated datasets. Crucially, when leveraging this data for RL via Group Relative Policy Optimization (GRPO), our model achieves state-of-the-art cross-modality and cross-task generalization, significantly outperforming specialized RL-based methods. Furthermore, our compact model, empowered by MedGR$^2$, achieves performance competitive with foundation models possessing over 10 times more parameters. MedGR$^2$ presents a new paradigm for data-efficient learning in high-stakes domains, transforming the problem from data scarcity to data generation and unlocking the full potential of RL for building truly generalizable medical AI.</li>
<li><strong>摘要：</strong>视觉模型（VLM）在医学中的应用严重阻碍了高质量的专家注释的数据。现有数据集的监督微调（SFT）通常会导致对看不见的方式和任务的普遍性不佳，而强化学习（RL）是一种有希望的替代方案，由于缺乏可靠的奖励信号而阻碍了该数据范围的范围。为了打破这种僵局，我们为医学推理引入了生成奖励学习（Medgr $^2 $），这是一个新颖的框架，创造了一个自我改善的良性周期。 Medgr $^2 $共同开发数据生成器和奖励模型，从而实现了自动化的，连续创建高质量的多模式医学数据，既是SFT和RL的出色培训来源。我们的实验表明，使用MEDGR $^2 $生产的数据已经超过了在大规模的人类策划数据集中训练的基线。至关重要的是，当通过组相对策略优化（GRPO）利用此数据为RL时，我们的模型可实现最新的交叉模式和交叉任务概括，并显着优于基于RL的专业方法。此外，我们的紧凑型模型，由Medgr $^2 $授权，可以与具有超过10倍参数的基础模型达到竞争性竞争。 Medgr $^2 $提出了一种新的范式，用于在高风险域中进行数据效率学习，从而将问题从数据稀缺转变为数据生成，并解锁了RL的全部潜力，以构建真正的可推广医疗AI。</li>
</ul>

<h3>Title: Towards Mechanistic Defenses Against Typographic Attacks in CLIP</h3>
<ul>
<li><strong>Authors: </strong>Lorenz Hufe, Constantin Venhoff, Maximilian Dreyer, Sebastian Lapuschkin, Wojciech Samek</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20570">https://arxiv.org/abs/2508.20570</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20570">https://arxiv.org/pdf/2508.20570</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20570]] Towards Mechanistic Defenses Against Typographic Attacks in CLIP(https://arxiv.org/abs/2508.20570)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Typographic attacks exploit multi-modal systems by injecting text into images, leading to targeted misclassifications, malicious content generation and even Vision-Language Model jailbreaks. In this work, we analyze how CLIP vision encoders behave under typographic attacks, locating specialized attention heads in the latter half of the model's layers that causally extract and transmit typographic information to the cls token. Building on these insights, we introduce a method to defend CLIP models against typographic attacks by selectively ablating a typographic circuit, consisting of attention heads. Without requiring finetuning, our method improves performance by up to 19.6% on a typographic variant of ImageNet-100, while reducing standard ImageNet-100 accuracy by less than 1%. Notably, our training-free approach remains competitive with current state-of-the-art typographic defenses that rely on finetuning. To this end, we release a family of dyslexic CLIP models which are significantly more robust against typographic attacks. These models serve as suitable drop-in replacements for a broad range of safety-critical applications, where the risks of text-based manipulation outweigh the utility of text recognition.</li>
<li><strong>摘要：</strong>印刷攻击通过将文本注入图像来利用多模式系统，从而导致针对性的错误分类，恶意的内容产生甚至视觉语言模型越狱。在这项工作中，我们分析了剪辑视觉编码在印刷攻击下的表现，并在模型的后半层中找到专门的注意力头，该层是在因果下提取并将印刷信息传输到CLS令牌的。在这些见解的基础上，我们引入了一种方法，通过选择性地烧毁由注意力头组成的印刷电路来捍卫剪辑模型免受印刷攻击的方法。在不需要填充的情况下，我们的方法在Imagenet-100的印刷版本上提高了高达19.6％的性能，同时将标准Imagenet-100精度降低了不到1％。值得注意的是，我们的无训练方法与依赖鉴定的当前最新印刷防御能力保持竞争力。为此，我们发布了一系列阅读障碍夹模型，该模型在印刷攻击方面明显更强。这些模型可作为广泛的安全至关重要应用的合适置换式替代方法，在这种应用中，基于文本的操作的风险大于文本识别的实用性。</li>
</ul>

<h3>Title: Disruptive Attacks on Face Swapping via Low-Frequency Perceptual Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Mengxiao Huang, Minglei Shu, Shuwang Zhou, Zhaoyang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20595">https://arxiv.org/abs/2508.20595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20595">https://arxiv.org/pdf/2508.20595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20595]] Disruptive Attacks on Face Swapping via Low-Frequency Perceptual Perturbations(https://arxiv.org/abs/2508.20595)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deepfake technology, driven by Generative Adversarial Networks (GANs), poses significant risks to privacy and societal security. Existing detection methods are predominantly passive, focusing on post-event analysis without preventing attacks. To address this, we propose an active defense method based on low-frequency perceptual perturbations to disrupt face swapping manipulation, reducing the performance and naturalness of generated content. Unlike prior approaches that used low-frequency perturbations to impact classification accuracy,our method directly targets the generative process of deepfake techniques. We combine frequency and spatial domain features to strengthen defenses. By introducing artifacts through low-frequency perturbations while preserving high-frequency details, we ensure the output remains visually plausible. Additionally, we design a complete architecture featuring an encoder, a perturbation generator, and a decoder, leveraging discrete wavelet transform (DWT) to extract low-frequency components and generate perturbations that disrupt facial manipulation models. Experiments on CelebA-HQ and LFW demonstrate significant reductions in face-swapping effectiveness, improved defense success rates, and preservation of visual quality.</li>
<li><strong>摘要：</strong>由生成对抗网络（GAN）驱动的DeepFake技术对隐私和社会安全构成了重大风险。现有的检测方法主要是被动的，重点是事后分析而不预防攻击。为了解决这个问题，我们提出了一种基于低频感知扰动的主动防御方法，以破坏面部交换操作，从而降低产生内容的性能和自然性。与使用低频扰动影响分类准确性的先前方法不同，我们的方法直接针对深泡技术的生成过程。我们结合了频率和空间域特征以增强防御能力。通过在保留高频细节的同时，通过低频扰动引入工件，我们确保输出在视觉上保持可见。此外，我们设计了一个完整的体系结构，具有编码器，扰动发生器和解码器，利用离散小波变换（DWT）来提取低频组件并产生破坏面部操纵模型的扰动。关于Celeba-HQ和LFW的实验表明，面部交换有效性，提高了防御成功率以及视觉质量的保存显着降低。</li>
</ul>

<h3>Title: Embracing Aleatoric Uncertainty: Generating Diverse 3D Human Motion</h3>
<ul>
<li><strong>Authors: </strong>Zheng Qin, Yabing Wang, Minghui Yang, Sanping Zhou, Ming Yang, Le Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20604">https://arxiv.org/abs/2508.20604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20604">https://arxiv.org/pdf/2508.20604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20604]] Embracing Aleatoric Uncertainty: Generating Diverse 3D Human Motion(https://arxiv.org/abs/2508.20604)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating 3D human motions from text is a challenging yet valuable task. The key aspects of this task are ensuring text-motion consistency and achieving generation diversity. Although recent advancements have enabled the generation of precise and high-quality human motions from text, achieving diversity in the generated motions remains a significant challenge. In this paper, we aim to overcome the above challenge by designing a simple yet effective text-to-motion generation method, \textit{i.e.}, Diverse-T2M. Our method introduces uncertainty into the generation process, enabling the generation of highly diverse motions while preserving the semantic consistency of the text. Specifically, we propose a novel perspective that utilizes noise signals as carriers of diversity information in transformer-based methods, facilitating a explicit modeling of uncertainty. Moreover, we construct a latent space where text is projected into a continuous representation, instead of a rigid one-to-one mapping, and integrate a latent space sampler to introduce stochastic sampling into the generation process, thereby enhancing the diversity and uncertainty of the outputs. Our results on text-to-motion generation benchmark datasets~(HumanML3D and KIT-ML) demonstrate that our method significantly enhances diversity while maintaining state-of-the-art performance in text consistency.</li>
<li><strong>摘要：</strong>从文本中产生3D人类动作是一项具有挑战性但有价值的任务。这项任务的关键方面是确保文本动作的一致性和实现生成多样性。尽管最近的进步使文本从文本中产生了精确和高质量的人类动作，但在产生的动作中实现多样性仍然是一个重大挑战。在本文中，我们旨在通过设计一种简单而有效的文本到动作生成方法来克服上述挑战，即\ textit {i.e。}，多样化的t2m。我们的方法将不确定性引入了生成过程中，从而可以产生高度多样化的动作，同时保留文本的语义一致性。具体而言，我们提出了一种新颖的观点，该视角将噪声信号用作基于变压器方法中多样性信息的载体，从而促进了不确定性的明确建模。此外，我们构建了一个潜在空间，其中文本被投影到连续的表示中，而不是刚性的一对一映射，并将潜在的空间采样器集成以将随机抽样引入生成过程，从而增强了输出的多样性和不确定性。我们在文本到动作生成基准数据集〜（HumanML3D和Kit-ML）上的结果表明，我们的方法显着增强了多样性，同时保持文本一致性的最先进性能。</li>
</ul>

<h3>Title: Physics Informed Generative Models for Magnetic Field Images</h3>
<ul>
<li><strong>Authors: </strong>Aye Phyu Phyu Aung, Lucas Lum, Zhansen Shi, Wen Qiu, Bernice Zee, JM Chin, Yeow Kheng Lim, J.Senthilnath</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20612">https://arxiv.org/abs/2508.20612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20612">https://arxiv.org/pdf/2508.20612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20612]] Physics Informed Generative Models for Magnetic Field Images(https://arxiv.org/abs/2508.20612)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In semiconductor manufacturing, defect detection and localization are critical to ensuring product quality and yield. While X-ray imaging is a reliable non-destructive testing method, it is memory-intensive and time-consuming for large-scale scanning, Magnetic Field Imaging (MFI) offers a more efficient means to localize regions of interest (ROI) for targeted X-ray scanning. However, the limited availability of MFI datasets due to proprietary concerns presents a significant bottleneck for training machine learning (ML) models using MFI. To address this challenge, we consider an ML-driven approach leveraging diffusion models with two physical constraints. We propose Physics Informed Generative Models for Magnetic Field Images (PI-GenMFI) to generate synthetic MFI samples by integrating specific physical information. We generate MFI images for the most common defect types: power shorts. These synthetic images will serve as training data for ML algorithms designed to localize defect areas efficiently. To evaluate generated MFIs, we compare our model to SOTA generative models from both variational autoencoder (VAE) and diffusion methods. We present a domain expert evaluation to assess the generated samples. In addition, we present qualitative and quantitative evaluation using various metrics used for image generation and signal processing, showing promising results to optimize the defect localization process.</li>
<li><strong>摘要：</strong>在半导体制造中，缺陷检测和定位对于确保产品质量和产量至关重要。尽管X射线成像是一种可靠的非破坏性测试方法，但它是内存密集型的，并且用于大规模扫描，磁场成像（MFI）为目标X射线扫描提供了更有效的方法（ROI）。但是，由于专有问题而引起的MFI数据集的可用性有限为使用MFI提供了训练机学习（ML）模型的重要瓶颈。为了应对这一挑战，我们考虑了一种由ML驱动的方法利用具有两个物理约束的扩散模型。我们建议通过集成特定的物理信息来生成合成的MFI样品的物理学知情模型（PI-GENMFI），以生成合成的MFI样品。我们为最常见的缺陷类型生成MFI图像：功率短。这些合成图像将用作旨在有效定位缺陷区域的ML算法的训练数据。为了评估生成的MFI，我们将我们的模型与来自变化自动编码器（VAE）和扩散方法的SOTA生成模型进行了比较。我们提出了一个域专家评估，以评估生成的样本。此外，我们使用用于图像生成和信号处理的各种指标进行定性和定量评估，显示出令人鼓舞的结果，以优化缺陷定位过程。</li>
</ul>

<h3>Title: AvatarBack: Back-Head Generation for Complete 3D Avatars from Front-View Images</h3>
<ul>
<li><strong>Authors: </strong>Shiqi Xin, Xiaolin Zhang, Yanbin Liu, Peng Zhang, Caifeng Shan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20623">https://arxiv.org/abs/2508.20623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20623">https://arxiv.org/pdf/2508.20623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20623]] AvatarBack: Back-Head Generation for Complete 3D Avatars from Front-View Images(https://arxiv.org/abs/2508.20623)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in Gaussian Splatting have significantly boosted the reconstruction of head avatars, enabling high-quality facial modeling by representing an 3D avatar as a collection of 3D Gaussians. However, existing methods predominantly rely on frontal-view images, leaving the back-head poorly constructed. This leads to geometric inconsistencies, structural blurring, and reduced realism in the rear regions, ultimately limiting the fidelity of reconstructed avatars. To address this challenge, we propose AvatarBack, a novel plug-and-play framework specifically designed to reconstruct complete and consistent 3D Gaussian avatars by explicitly modeling the missing back-head regions. AvatarBack integrates two core technical innovations,i.e., the Subject-specific Generator (SSG) and the Adaptive Spatial Alignment Strategy (ASA). The former leverages a generative prior to synthesize identity-consistent, plausible back-view pseudo-images from sparse frontal inputs, providing robust multi-view supervision. To achieve precise geometric alignment between these synthetic views and the 3D Gaussian representation, the later employs learnable transformation matrices optimized during training, effectively resolving inherent pose and coordinate discrepancies. Extensive experiments on NeRSemble and K-hairstyle datasets, evaluated using geometric, photometric, and GPT-4o-based perceptual metrics, demonstrate that AvatarBack significantly enhances back-head reconstruction quality while preserving frontal fidelity. Moreover, the reconstructed avatars maintain consistent visual realism under diverse motions and remain fully animatable.</li>
<li><strong>摘要：</strong>高斯脱落的最新进展显着提高了头像的重建，从而通过代表3D化身作为3D高斯人的收藏来实现高质量的面部建模。但是，现有的方法主要依赖于额叶视图图像，而后头构造的构造效果不佳。这导致了几何不一致，结构模糊和后方区域的现实主义减少，最终限制了重建的头像的忠诚度。为了应对这一挑战，我们提出了Avatarback，这是一个新颖的插件框架，专门设计，旨在通过对缺失的背面区域进行显式建模，以重建完整且一致的3D高斯化身。 Avatarback集成了两项核心技术创新，即特定于主题的发电机（SSG）和自适应空间对齐策略（ASA）。前者在合成身份符合性的，合理的背景伪图像之前利用生成剂，从稀疏的额叶输入中，提供了强大的多视图监督。为了实现这些综合观点与3D高斯表示之间的精确几何对齐，后来采用了在训练过程中优化的可学习变换矩阵，有效地解决了固有的姿势和坐标差异。使用几何，光度法和基于GPT-4O的知觉指标进行评估的Nerseble和K率数据集的广泛实验表明，Avatarback显着提高了背部重建质量，同时保留了额额延伸性。此外，重建的化身在不同的动作下保持一致的视觉现实主义，并保持完全动画。</li>
</ul>

<h3>Title: CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Ayan Banerjee, Fernando Vilariño, Josep Lladós</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20640">https://arxiv.org/abs/2508.20640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20640">https://arxiv.org/pdf/2508.20640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20640]] CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models(https://arxiv.org/abs/2508.20640)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Preserving facial identity under extreme stylistic transformation remains a major challenge in generative art. In graffiti, a high-contrast, abstract medium, subtle distortions to the eyes, nose, or mouth can erase the subject's recognizability, undermining both personal and cultural authenticity. We present CraftGraffiti, an end-to-end text-guided graffiti generation framework designed with facial feature preservation as a primary objective. Given an input image and a style and pose descriptive prompt, CraftGraffiti first applies graffiti style transfer via LoRA-fine-tuned pretrained diffusion transformer, then enforces identity fidelity through a face-consistent self-attention mechanism that augments attention layers with explicit identity embeddings. Pose customization is achieved without keypoints, using CLIP-guided prompt extension to enable dynamic re-posing while retaining facial coherence. We formally justify and empirically validate the "style-first, identity-after" paradigm, showing it reduces attribute drift compared to the reverse order. Quantitative results demonstrate competitive facial feature consistency and state-of-the-art aesthetic and human preference scores, while qualitative analyses and a live deployment at the Cruilla Festival highlight the system's real-world creative impact. CraftGraffiti advances the goal of identity-respectful AI-assisted artistry, offering a principled approach for blending stylistic freedom with recognizability in creative AI applications.</li>
<li><strong>摘要：</strong>在极端的风格转化下，保持面部身份仍然是生成艺术的主要挑战。在涂鸦中，一种高对比度，抽象的媒介，对眼睛，鼻子或嘴巴的细微扭曲可以消除受试者的可识别性，破坏个人和文化真实性。我们提出了CraftGraffiti，这是一种端到端的文本引导的涂鸦生成框架，其设计为主要目标。鉴于输入图像以及样式和姿势的描述及时，CraftGraffiti首先通过洛拉（Lora-Fin）调整的预审前的扩散变压器应用涂鸦样式转移，然后通过面部一致的自我关注机制来实现身份保真度，从而增强具有明确身份嵌入的注意力层。使用夹子引导的及时扩展可以实现姿势定制，可以实现动态重新配置，同时保持面部连贯性。我们正式证明并在经验上验证“风格优先，身份 - 之后”范式，表明与反向顺序相比，它减少了属性漂移。定量结果表明，面部特征一致性和最新美学和人类偏好得分，而定性分析和在克鲁拉音乐节上进行了实时部署，突显了该系统的现实创造性影响。 Craftgraffiti促进了身份尊重的AI辅助艺术性的目标，为在创意AI应用中提供了将风格自由与可识别性融为一体的原则方法。</li>
</ul>

<h3>Title: VarDiU: A Variational Diffusive Upper Bound for One-Step Diffusion Distillation</h3>
<ul>
<li><strong>Authors: </strong>Leyang Wang, Mingtian Zhang, Zijing Ou, David Barber</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20646">https://arxiv.org/abs/2508.20646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20646">https://arxiv.org/pdf/2508.20646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20646]] VarDiU: A Variational Diffusive Upper Bound for One-Step Diffusion Distillation(https://arxiv.org/abs/2508.20646)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recently, diffusion distillation methods have compressed thousand-step teacher diffusion models into one-step student generators while preserving sample quality. Most existing approaches train the student model using a diffusive divergence whose gradient is approximated via the student's score function, learned through denoising score matching (DSM). Since DSM training is imperfect, the resulting gradient estimate is inevitably biased, leading to sub-optimal performance. In this paper, we propose VarDiU (pronounced /va:rdju:/), a Variational Diffusive Upper Bound that admits an unbiased gradient estimator and can be directly applied to diffusion distillation. Using this objective, we compare our method with Diff-Instruct and demonstrate that it achieves higher generation quality and enables a more efficient and stable training procedure for one-step diffusion distillation.</li>
<li><strong>摘要：</strong>最近，扩散蒸馏方法已将千步教师扩散模型压缩为单步学生发生器，同时保留样本质量。大多数现有方法使用扩散差异训练学生模型，该差异通过学生的得分函数近似，该梯度通过DENOTO匹配（DSM）学习。由于DSM训练是不完善的，因此不可避免地会偏向于梯度估计，从而导致次优性能。在本文中，我们提出了vardiu（发音 /va：rdju： /），这是一种差异扩散的上限，该上限允许无偏梯度估计器，并且可以直接应用于扩散蒸馏。使用此目标，我们将我们的方法与DIFF教学进行了比较，并证明它可以达到更高的生成质量，并为一步扩散蒸馏提供了更高效，更稳定的训练程序。</li>
</ul>

<h3>Title: Compositionality in Time Series: A Proof of Concept using Symbolic Dynamics and Compositional Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Michael Hagmann, Michael Staniek, Stefan Riezler</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20656">https://arxiv.org/abs/2508.20656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20656">https://arxiv.org/pdf/2508.20656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20656]] Compositionality in Time Series: A Proof of Concept using Symbolic Dynamics and Compositional Data Augmentation(https://arxiv.org/abs/2508.20656)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This work investigates whether time series of natural phenomena can be understood as being generated by sequences of latent states which are ordered in systematic and regular ways. We focus on clinical time series and ask whether clinical measurements can be interpreted as being generated by meaningful physiological states whose succession follows systematic principles. Uncovering the underlying compositional structure will allow us to create synthetic data to alleviate the notorious problem of sparse and low-resource data settings in clinical time series forecasting, and deepen our understanding of clinical data. We start by conceptualizing compositionality for time series as a property of the data generation process, and then study data-driven procedures that can reconstruct the elementary states and composition rules of this process. We evaluate the success of this methods using two empirical tests originating from a domain adaptation perspective. Both tests infer the similarity of the original time series distribution and the synthetic time series distribution from the similarity of expected risk of time series forecasting models trained and tested on original and synthesized data in specific ways. Our experimental results show that the test set performance achieved by training on compositionally synthesized data is comparable to training on original clinical time series data, and that evaluation of models on compositionally synthesized test data shows similar results to evaluating on original test data, outperforming randomization-based data augmentation. An additional downstream evaluation of the prediction task of sequential organ failure assessment (SOFA) scores shows significant performance gains when model training is entirely based on compositionally synthesized data compared to training on original data.</li>
<li><strong>摘要：</strong>这项工作调查了是否可以将自然现象的时间序列理解为由潜在状态的序列产生的，这些状态以系统和常规方式订购。我们专注于临床时间序列，并询问是否可以将临床测量值解释为由有意义的生理状态产生的，其继承遵循系统原理。揭示潜在的组成结构将使我们能够创建合成数据，以减轻临床时间序列中稀疏和低资源数据设置的臭名昭著的问题，并加深我们对临床数据的理解。我们首先将时间序列的组成性概念化为数据生成过程的属性，然后研究可以重建该过程的基本状态和组成规则的数据驱动过程。我们使用两个源自域适应性的源自的经验测试来评估这种方法的成功。两项测试都从原始时间序列分布和合成时间序列分布的相似性从预期的时间序列预测模型的相似性以特定方式训练和测试。我们的实验结果表明，通过对组合合成数据进行培训实现的测试集性能与原始临床时间序列数据的培训相当，并且对组合合成的测试数据的评估显示出与对原始测试数据的评估相似的结果，超过了基于随机化的数据增强。与原始数据相比，当模型训练完全基于组合合成的数据时，对顺序器官失效评估（SOFA）的预测任务的额外下游评估显示，绩效的增长很大。</li>
</ul>

<h3>Title: MobileCLIP2: Improving Multi-Modal Reinforced Training</h3>
<ul>
<li><strong>Authors: </strong>Fartash Faghri, Pavan Kumar Anasosalu Vasu, Cem Koc, Vaishaal Shankar, Alexander Toshev, Oncel Tuzel, Hadi Pouransari</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20691">https://arxiv.org/abs/2508.20691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20691">https://arxiv.org/pdf/2508.20691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20691]] MobileCLIP2: Improving Multi-Modal Reinforced Training(https://arxiv.org/abs/2508.20691)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Foundation image-text models such as CLIP with zero-shot capabilities enable a wide array of applications. MobileCLIP is a recent family of image-text models at 3-15ms latency and 50-150M parameters with state-of-the-art zero-shot accuracy. The main ingredients in MobileCLIP were its low-latency and light architectures and a novel multi-modal reinforced training that made knowledge distillation from multiple caption-generators and CLIP teachers efficient, scalable, and reproducible. In this paper, we improve the multi-modal reinforced training of MobileCLIP through: 1) better CLIP teacher ensembles trained on the DFN dataset, 2) improved captioner teachers trained on the DFN dataset and fine-tuned on a diverse selection of high-quality image-caption datasets. We discover new insights through ablations such as the importance of temperature tuning in contrastive knowledge distillation, the effectiveness of caption-generator fine-tuning for caption diversity, and the additive improvement from combining synthetic captions generated by multiple models. We train a new family of models called MobileCLIP2 and achieve state-of-the-art ImageNet-1k zero-shot accuracies at low latencies. In particular, we observe 2.2% improvement in ImageNet-1k accuracy for MobileCLIP2-B compared with MobileCLIP-B architecture. Notably, MobileCLIP2-S4 matches the zero-shot accuracy of SigLIP-SO400M/14 on ImageNet-1k while being 2$\times$ smaller and improves on DFN ViT-L/14 at 2.5$\times$ lower latency. We release our pretrained models (this https URL) and the data generation code (this https URL). The data generation code makes it easy to create new reinforced datasets with arbitrary teachers using distributed scalable processing.</li>
<li><strong>摘要：</strong>基础图像文本模型，例如具有零拍功能的剪辑，可实现各种应用程序。 Mobileclip是3-15ms延迟的图像文本模型的最新家族，具有最先进的零弹药精度的50-150m参数。 Mobileclip中的主要成分是其低延迟和光结构以及一种新型的多模式增强训练，从而使知识蒸馏从多个字幕生成器和剪辑教师中有效，可扩展性和可重现。在本文中，我们通过：1）在DFN数据集中训练的更好的剪贴画教师合奏，2）改进了在DFN数据集中培训的标题教师，并在多样化的高质量图像贴图数据集中进行了微调。我们通过消融发现了新的见解，例如温度调整在对比知识蒸馏中的重要性，字幕生成器对字幕多样性的有效性以及结合由多个模型产生的合成字幕所带来的添加剂改进。我们训练一个名为Mobileclip2的新模型家族，并在低潜伏期实现了最先进的Imagenet-1k零射击精度。特别是，与Mobileclip-B架构相比，我们观察到MobileClip2-B的ImabEnet-1K精度提高了2.2％。值得注意的是，MobileClip2-S4与Imagenet-1k上Siglip-So400m/14的零射击精度相匹配，而在2 $ \ times $ himber上，DFN VIT-L/14在2.5 $ \ times $降低潜伏期。我们发布了验证的模型（此HTTPS URL）和数据生成代码（此HTTPS URL）。数据生成代码使使用分布式可扩展处理的任意教师可以轻松地使用任意教师创建新的增强数据集。</li>
</ul>

<h3>Title: EEGDM: Learning EEG Representation with Latent Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Shaocong Wang, Tong Liu, Ming Li, Minjing Yu, Yong-Jin Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20705">https://arxiv.org/abs/2508.20705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20705">https://arxiv.org/pdf/2508.20705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20705]] EEGDM: Learning EEG Representation with Latent Diffusion Model(https://arxiv.org/abs/2508.20705)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>While electroencephalography (EEG) signal analysis using deep learning has shown great promise, existing approaches still face significant challenges in learning generalizable representations that perform well across diverse tasks, particularly when training data is limited. Current EEG representation learning methods including EEGPT and LaBraM typically rely on simple masked reconstruction objective, which may not fully capture the rich semantic information and complex patterns inherent in EEG signals. In this paper, we propose EEGDM, a novel self-supervised EEG representation learning method based on the latent diffusion model, which leverages EEG signal generation as a self-supervised objective, turning the diffusion model into a strong representation learner capable of capturing EEG semantics. EEGDM incorporates an EEG encoder that distills EEG signals and their channel augmentations into a compact representation, acting as conditional information to guide the diffusion model for generating EEG signals. This design endows EEGDM with a compact latent space, which not only offers ample control over the generative process but also can be leveraged for downstream tasks. Experimental results show that EEGDM (1) can reconstruct high-quality EEG signals, (2) effectively learns robust representations, and (3) achieves competitive performance with modest pre-training data size across diverse downstream tasks, underscoring its generalizability and practical utility.</li>
<li><strong>摘要：</strong>尽管使用深度学习的脑电图（EEG）信号分析表现出了巨大的希望，但现有方法仍然面临着在学习跨不同任务良好的可推广表示方面面临重大挑战，尤其是在培训数据受到限制时。当前的脑电图表示方法在内，包括EEGPT和LABRAM通常依赖于简单的掩盖重建目标，这可能无法完全捕获EEG信号中固有的丰富语义信息和复杂模式。在本文中，我们提出了一种基于潜在扩散模型的新型自我监督的脑电图表示方法EEGDM，该方法将EEG信号产生作为一个自我监督的目标，将扩散模型转变为能够捕获EEG语言学的强大代表性学习者。 EEGDM合并了一个EEG编码器，该编码器将EEG信号及其通道增强量提取到紧凑的表示形式中，充当有条件信息，以指导用于生成EEG信号的扩散模型。该设计赋予EEGDM具有紧凑的潜在空间，该空间不仅可以提供对生成过程的充分控制，而且可以利用下游任务。实验结果表明，EEGDM（1）可以重建高质量的脑电图信号，（2）有效地学习了强大的表示形式，（3）通过在各种下游任务中进行适度的预训练数据大小来实现竞争性能，从而强调其普遍性和实用性。</li>
</ul>

<h3>Title: Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Yibin Wang, Zhimin Li, Yuhang Zang, Yujie Zhou, Jiazi Bu, Chunyu Wang, Qinglin Lu, Cheng Jin, Jiaqi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20751">https://arxiv.org/abs/2508.20751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20751">https://arxiv.org/pdf/2508.20751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20751]] Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning(https://arxiv.org/abs/2508.20751)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advancements highlight the importance of GRPO-based reinforcement learning methods and benchmarking in enhancing text-to-image (T2I) generation. However, current methods using pointwise reward models (RM) for scoring generated images are susceptible to reward hacking. We reveal that this happens when minimal score differences between images are amplified after normalization, creating illusory advantages that drive the model to over-optimize for trivial gains, ultimately destabilizing the image generation process. To address this, we propose Pref-GRPO, a pairwise preference reward-based GRPO method that shifts the optimization objective from score maximization to preference fitting, ensuring more stable training. In Pref-GRPO, images are pairwise compared within each group using preference RM, and the win rate is used as the reward signal. Extensive experiments demonstrate that PREF-GRPO differentiates subtle image quality differences, providing more stable advantages and mitigating reward hacking. Additionally, existing T2I benchmarks are limited by coarse evaluation criteria, hindering comprehensive model assessment. To solve this, we introduce UniGenBench, a unified T2I benchmark comprising 600 prompts across 5 main themes and 20 subthemes. It evaluates semantic consistency through 10 primary and 27 sub-criteria, leveraging MLLM for benchmark construction and evaluation. Our benchmarks uncover the strengths and weaknesses of both open and closed-source T2I models and validate the effectiveness of Pref-GRPO.</li>
<li><strong>摘要：</strong>最近的进步强调了基于GRPO的增强学习方法和基准测试在增强文本形象（T2i）一代中的重要性。但是，使用点奖励模型（RM）进行评分的图像的当前方法容易奖励黑客。我们透露，当标准化后图像之间的分数差异被放大时，就会发生这种情况，从而创造出虚幻的优势，使模型驱动模型过度耗尽琐碎的收益，最终破坏了图像生成过程的稳定。为了解决这个问题，我们提出了Pref-grpo，这是一种基于成对的基于奖励奖励的GRPO方法，该方法将优化目标从得分最大化转变为偏好拟合，从而确保了更稳定的训练。在PERF-GRPO中，使用偏好RM在每个组中对图像进行成对比较，并将获胜率用作奖励信号。广泛的实验表明，Pref-Grpo可以区分微妙的图像质量差异，提供更稳定的优势并减轻奖励黑客攻击。此外，现有的T2I基准受到粗略评估标准的限制，阻碍了全面的模型评估。为了解决这个问题，我们引入了Unigenbench，这是一个统一的T2I基准测试，其中包括5个主要主题和20个子主题的600个提示。它通过10个主要和27个亚标准评估语义一致性，利用MLLM进行基准构造和评估。我们的基准测试揭示了开放和封闭源T2I模型的优势和劣势，并验证了Pref-Grpo的有效性。</li>
</ul>

<h3>Title: Unleashing Uncertainty: Efficient Machine Unlearning for Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Christoforos N. Spartalis, Theodoros Semertzidis, Petros Daras, Efstratios Gavves</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20773">https://arxiv.org/abs/2508.20773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20773">https://arxiv.org/pdf/2508.20773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20773]] Unleashing Uncertainty: Efficient Machine Unlearning for Generative AI(https://arxiv.org/abs/2508.20773)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce SAFEMax, a novel method for Machine Unlearning in diffusion models. Grounded in information-theoretic principles, SAFEMax maximizes the entropy in generated images, causing the model to generate Gaussian noise when conditioned on impermissible classes by ultimately halting its denoising process. Also, our method controls the balance between forgetting and retention by selectively focusing on the early diffusion steps, where class-specific information is prominent. Our results demonstrate the effectiveness of SAFEMax and highlight its substantial efficiency gains over state-of-the-art methods.</li>
<li><strong>摘要：</strong>我们介绍了Safemax，这是一种在扩散模型中进行的新方法。以信息理论原理为基础，Safemax最大化生成的图像中的熵，从而导致该模型在不允许的类别以最终停止其去核过程时产生高斯噪声。同样，我们的方法通过选择性地关注早期扩散步骤，在遗忘和保留之间控制遗忘和保留之间的平衡，而这些步骤特定于班级的信息很突出。我们的结果证明了Safemax的有效性，并强调了其对最先进方法的效率提高。</li>
</ul>

<h3>Title: Evaluating Compositional Generalisation in VLMs and Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Beth Pearson, Bilal Boulbarss, Michael Wray, Martha Lewis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20783">https://arxiv.org/abs/2508.20783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20783">https://arxiv.org/pdf/2508.20783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20783]] Evaluating Compositional Generalisation in VLMs and Diffusion Models(https://arxiv.org/abs/2508.20783)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>A fundamental aspect of the semantics of natural language is that novel meanings can be formed from the composition of previously known parts. Vision-language models (VLMs) have made significant progress in recent years, however, there is evidence that they are unable to perform this kind of composition. For example, given an image of a red cube and a blue cylinder, a VLM such as CLIP is likely to incorrectly label the image as a red cylinder or a blue cube, indicating it represents the image as a `bag-of-words' and fails to capture compositional semantics. Diffusion models have recently gained significant attention for their impressive generative abilities, and zero-shot classifiers based on diffusion models have been shown to perform competitively with CLIP in certain compositional tasks. In this work we explore whether the generative Diffusion Classifier has improved compositional generalisation abilities compared to discriminative models. We assess three models -- Diffusion Classifier, CLIP, and ViLT -- on their ability to bind objects with attributes and relations in both zero-shot learning (ZSL) and generalised zero-shot learning (GZSL) settings. Our results show that the Diffusion Classifier and ViLT perform well at concept binding tasks, but that all models struggle significantly with the relational GZSL task, underscoring the broader challenges VLMs face with relational reasoning. Analysis of CLIP embeddings suggests that the difficulty may stem from overly similar representations of relational concepts such as left and right. Code and dataset are available at: this https URL</li>
<li><strong>摘要：</strong>自然语言语义的一个基本方面是，可以从先前已知的部分的组成中形成新颖的含义。近年来，视觉语言模型（VLM）取得了重大进展，但是有证据表明他们无法执行这种组成。例如，给定红色立方体和蓝色圆柱体的图像，诸如夹子之类的VLM可能会错误地将图像标记为红色圆柱或蓝色立方体，这表明它将图像表示为“词袋”，并且未能捕获组成的语义。扩散模型最近对其令人印象深刻的生成能力引起了极大的关注，并且已经证明基于扩散模型的零射击分类器可以在某些组成任务中使用剪辑竞争性能。在这项工作中，我们探讨了与判别模型相比，生成扩散分类器是否提高了组成概括能力。我们评估了三个模型 - 扩散分类器，剪辑和vilt-在其零摄取学习（ZSL）和广义零局部学习（GZSL）设置中绑定具有属性和关系的对象的能力。我们的结果表明，扩散分类器和vilt在概念约束任务上表现良好，但是所有模型都在关系GZSL任务中巨大努力，强调了VLMS与关系推理所面临的更广泛的挑战。对剪辑嵌入的分析表明，困难可能源于关系概念（例如左右）的过度相似表示。代码和数据集可用：此HTTPS URL</li>
</ul>

<h3>Title: GPT-FT: An Efficient Automated Feature Transformation Using GPT for Sequence Reconstruction and Performance Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Yang Gao, Dongjie Wang, Scott Piersall, Ye Zhang, Liqiang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20824">https://arxiv.org/abs/2508.20824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20824">https://arxiv.org/pdf/2508.20824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20824]] GPT-FT: An Efficient Automated Feature Transformation Using GPT for Sequence Reconstruction and Performance Enhancement(https://arxiv.org/abs/2508.20824)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Feature transformation plays a critical role in enhancing machine learning model performance by optimizing data representations. Recent state-of-the-art approaches address this task as a continuous embedding optimization problem, converting discrete search into a learnable process. Although effective, these methods often rely on sequential encoder-decoder structures that cause high computational costs and parameter requirements, limiting scalability and efficiency. To address these limitations, we propose a novel framework that accomplishes automated feature transformation through four steps: transformation records collection, embedding space construction with a revised Generative Pre-trained Transformer (GPT) model, gradient-ascent search, and autoregressive reconstruction. In our approach, the revised GPT model serves two primary functions: (a) feature transformation sequence reconstruction and (b) model performance estimation and enhancement for downstream tasks by constructing the embedding space. Such a multi-objective optimization framework reduces parameter size and accelerates transformation processes. Experimental results on benchmark datasets show that the proposed framework matches or exceeds baseline performance, with significant gains in computational efficiency. This work highlights the potential of transformer-based architectures for scalable, high-performance automated feature transformation.</li>
<li><strong>摘要：</strong>特征转换在通过优化数据表示来增强机器学习模型性能中起着至关重要的作用。最新的最新方法将这项任务作为连续嵌入优化问题，将离散搜索转换为可学习的过程。尽管有效，但这些方法通常依赖于造成高计算成本和参数要求的顺序编码器结构，从而限制了可扩展性和效率。为了解决这些局限性，我们提出了一个新颖的框架，该框架通过四个步骤来完成自动化特征转换：转换记录收集，嵌入空间构建具有修订的生成性预训练的预训练的变压器（GPT）模型，梯度为渐变的搜索和自动性重构。在我们的方法中，修订后的GPT模型提供了两个主要功能：（a）特征转换序列重建和（b）通过构造嵌入空间来实现下游任务的模型性能估计和增强。这种多目标优化框架可降低参数大小并加速转换过程。基准数据集的实验结果表明，所提出的框架匹配或超过基线性能，并具有显着的计算效率提高。这项工作突出了基于变压器的架构的潜力，可扩展，高性能自动化特征变换。</li>
</ul>

<h3>Title: PathMR: Multimodal Visual Reasoning for Interpretable Pathology Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Ye Zhang, Yu Zhou, Jingwen Qi, Yongbing Zhang, Simon Puettmann, Finn Wichmann, Larissa Pereira Ferreira, Lara Sichward, Julius Keyl, Sylvia Hartmann, Shuo Zhao, Hongxiao Wang, Xiaowei Xu, Jianxu Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20851">https://arxiv.org/abs/2508.20851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20851">https://arxiv.org/pdf/2508.20851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20851]] PathMR: Multimodal Visual Reasoning for Interpretable Pathology Diagnosis(https://arxiv.org/abs/2508.20851)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Deep learning based automated pathological diagnosis has markedly improved diagnostic efficiency and reduced variability between observers, yet its clinical adoption remains limited by opaque model decisions and a lack of traceable rationale. To address this, recent multimodal visual reasoning architectures provide a unified framework that generates segmentation masks at the pixel level alongside semantically aligned textual explanations. By localizing lesion regions and producing expert style diagnostic narratives, these models deliver the transparent and interpretable insights necessary for dependable AI assisted pathology. Building on these advancements, we propose PathMR, a cell-level Multimodal visual Reasoning framework for Pathological image analysis. Given a pathological image and a textual query, PathMR generates expert-level diagnostic explanations while simultaneously predicting cell distribution patterns. To benchmark its performance, we evaluated our approach on the publicly available PathGen dataset as well as on our newly developed GADVR dataset. Extensive experiments on these two datasets demonstrate that PathMR consistently outperforms state-of-the-art visual reasoning methods in text generation quality, segmentation accuracy, and cross-modal alignment. These results highlight the potential of PathMR for improving interpretability in AI-driven pathological diagnosis. The code will be publicly available in this https URL.</li>
<li><strong>摘要：</strong>基于深度学习的自动化病理诊断显着提高了诊断效率和观察者之间的变异性降低，但其临床采用仍然受不​​透明模型决策和缺乏可追溯的理由的限制。为了解决这个问题，最近的多模式视觉推理体系结构提供了一个统一的框架，该框架在像素级别上生成分段掩模，并与语义上的文本解释一起。通过定位病变区域并生产专家样式的诊断叙事，这些模型提供了可靠的AI辅助病理所必需的透明且可解释的见解。在这些进步的基础上，我们提出了PathMR，这是一种用于病理图像分析的细胞级多模式视觉推理框架。给定病理图像和文本查询，PathMR生成专家级诊断解释，同时预测细胞分布模式。为了基准其性能，我们在公开可用的Pathgen数据集以及新开发的GADVR数据集上评估了我们的方法。这两个数据集的广泛实验表明，在文本生成质量，分割精度和跨模式对齐中，PATHMR始终优于最先进的视觉推理方法。这些结果突出了PATHMR在AI驱动的病理诊断中改善可解释性的潜力。该代码将在此HTTPS URL中公开使用。</li>
</ul>

<h3>Title: Deep Learning Framework for Early Detection of Pancreatic Cancer Using Multi-Modal Medical Imaging Analysis</h3>
<ul>
<li><strong>Authors: </strong>Dennis Slobodzian, Karissa Tilbury, Amir Kordijazi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20877">https://arxiv.org/abs/2508.20877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20877">https://arxiv.org/pdf/2508.20877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20877]] Deep Learning Framework for Early Detection of Pancreatic Cancer Using Multi-Modal Medical Imaging Analysis(https://arxiv.org/abs/2508.20877)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Pacreatic ductal adenocarcinoma (PDAC) remains one of the most lethal forms of cancer, with a five-year survival rate below 10% primarily due to late detection. This research develops and validates a deep learning framework for early PDAC detection through analysis of dual-modality imaging: autofluorescence and second harmonic generation (SHG). We analyzed 40 unique patient samples to create a specialized neural network capable of distinguishing between normal, fibrotic, and cancerous tissue. Our methodology evaluated six distinct deep learning architectures, comparing traditional Convolutional Neural Networks (CNNs) with modern Vision Transformers (ViTs). Through systematic experimentation, we identified and overcome significant challenges in medical image analysis, including limited dataset size and class imbalance. The final optimized framework, based on a modified ResNet architecture with frozen pre-trained layers and class-weighted training, achieved over 90% accuracy in cancer detection. This represents a significant improvement over current manual analysis methods an demonstrates potential for clinical deployment. This work establishes a robust pipeline for automated PDAC detection that can augment pathologists' capabilities while providing a foundation for future expansion to other cancer types. The developed methodology also offers valuable insights for applying deep learning to limited-size medical imaging datasets, a common challenge in clinical applications.</li>
<li><strong>摘要：</strong>链酸导管腺癌（PDAC）仍然是最致命的癌症形式之一，五年生存率低于10％，主要是由于晚期检测。这项研究通过分析双模式成像：自动荧光和第二次谐波生成（SHG）来开发并验证了早期PDAC检测的深度学习框架。我们分析了40个独特的患者样品，以创建一个专门的神经网络，能够区分正常，纤维化和癌组织。我们的方法论评估了六个不同的深度学习体系结构，将传统的卷积神经网络（CNN）与现代视觉变形金刚（VIT）进行了比较。通过系统的实验，我们确定并克服了医学图像分析中的重大挑战，包括数据集大小和类不平衡。最终的优化框架基于经过修改的重新系统结构，并具有冷冻的预训练层和类加权的训练，在癌症检测中的精度超过90％。这代表了与当前手动分析方法的显着改善，证明了临床部署的潜力。这项工作为自动PDAC检测建立了强大的管道，可以增强病理学家的能力，同时为将来扩展到其他癌症类型的基础。开发的方法还提供了有价值的见解，用于将深度学习应用于有限尺寸的医学成像数据集，这是临床应用中的常见挑战。</li>
</ul>

<h3>Title: Understanding and evaluating computer vision models through the lens of counterfactuals</h3>
<ul>
<li><strong>Authors: </strong>Pushkar Shukla</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20881">https://arxiv.org/abs/2508.20881</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20881">https://arxiv.org/pdf/2508.20881</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20881]] Understanding and evaluating computer vision models through the lens of counterfactuals(https://arxiv.org/abs/2508.20881)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Counterfactual reasoning -- the practice of asking ``what if'' by varying inputs and observing changes in model behavior -- has become central to interpretable and fair AI. This thesis develops frameworks that use counterfactuals to explain, audit, and mitigate bias in vision classifiers and generative models. By systematically altering semantically meaningful attributes while holding others fixed, these methods uncover spurious correlations, probe causal dependencies, and help build more robust systems. The first part addresses vision classifiers. CAVLI integrates attribution (LIME) with concept-level analysis (TCAV) to quantify how strongly decisions rely on human-interpretable concepts. With localized heatmaps and a Concept Dependency Score, CAVLI shows when models depend on irrelevant cues like backgrounds. Extending this, ASAC introduces adversarial counterfactuals that perturb protected attributes while preserving semantics. Through curriculum learning, ASAC fine-tunes biased models for improved fairness and accuracy while avoiding stereotype-laden artifacts. The second part targets generative Text-to-Image (TTI) models. TIBET provides a scalable pipeline for evaluating prompt-sensitive biases by varying identity-related terms, enabling causal auditing of how race, gender, and age affect image generation. To capture interactions, BiasConnect builds causal graphs diagnosing intersectional biases. Finally, InterMit offers a modular, training-free algorithm that mitigates intersectional bias via causal sensitivity scores and user-defined fairness goals. Together, these contributions show counterfactuals as a unifying lens for interpretability, fairness, and causality in both discriminative and generative models, establishing principled, scalable methods for socially responsible bias evaluation and mitigation.</li>
<li><strong>摘要：</strong>反事实推理 - 询问``如果通过变化的投入和观察模型行为的变化''的做法已成为可解释和公平的AI的核心。本论文开发了使用反事实来解释，审核和减轻视觉分类器和生成模型的偏见的框架。通过系统地改变语义上有意义的属性，同时将其他人固定为固定，这些方法会发现虚假的相关性，探测因果关系依赖性，并有助于构建更健壮的系统。第一部分解决了视力分类器。 Cavli将归因（LIME）与概念级分析（TCAV）相结合，以量化决策如何依赖于人类解动的概念。凭借局部热图和概念依赖性评分，Cavli显示模型何时取决于背景等无关线索。扩展这一点，ASAC引入了对抗性的反事实，这些反事实是在保留语义的同时受到保护的属性。通过课程学习，ASAC微调偏向模型，以提高公平性和准确性，同时避免刻板印象。第二部分针对生成文本形象（TTI）模型。西藏提供了可扩展的管道，用于通过改变与身份相关的术语来评估及时敏感的偏见，从而对种族，性别和年龄的因果审核如何影响图像产生。为了捕获相互作用，biasConnect构建了诊断相交偏差的因果图。最后，Intermit提供了一种模块化，无训练的算法，该算法通过因果灵敏度得分和用户定义的公平目标来减轻交叉偏见。这些贡献一起表明了反事实是歧视性和生成模型中的可解释性，公平性和因果关系的统一镜头，以建立有原则的，可扩展的方法来实现社会负责的偏见评估和缓解措施。</li>
</ul>

<h3>Title: ExpertSim: Fast Particle Detector Simulation Using Mixture-of-Generative-Experts</h3>
<ul>
<li><strong>Authors: </strong>Patryk Będkowski, Jan Dubiński, Filip Szatkowski, Kamil Deja, Przemysław Rokita, Tomasz Trzciński</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20991">https://arxiv.org/abs/2508.20991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20991">https://arxiv.org/pdf/2508.20991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20991]] ExpertSim: Fast Particle Detector Simulation Using Mixture-of-Generative-Experts(https://arxiv.org/abs/2508.20991)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Simulating detector responses is a crucial part of understanding the inner workings of particle collisions in the Large Hadron Collider at CERN. Such simulations are currently performed with statistical Monte Carlo methods, which are computationally expensive and put a significant strain on CERN's computational grid. Therefore, recent proposals advocate for generative machine learning methods to enable more efficient simulations. However, the distribution of the data varies significantly across the simulations, which is hard to capture with out-of-the-box methods. In this study, we present ExpertSim - a deep learning simulation approach tailored for the Zero Degree Calorimeter in the ALICE experiment. Our method utilizes a Mixture-of-Generative-Experts architecture, where each expert specializes in simulating a different subset of the data. This allows for a more precise and efficient generation process, as each expert focuses on a specific aspect of the calorimeter response. ExpertSim not only improves accuracy, but also provides a significant speedup compared to the traditional Monte-Carlo methods, offering a promising solution for high-efficiency detector simulations in particle physics experiments at CERN. We make the code available at this https URL.</li>
<li><strong>摘要：</strong>模拟检测器响应是理解CERN大型强子对撞机中粒子碰撞的内部起作用的关键部分。目前，使用统计的蒙特卡洛方法进行此类仿真，这些方法在计算上昂贵，并对CERN的计算网格产生了重大压力。因此，最近提倡生成机器学习方法的提案，以实现更有效的模拟。但是，数据的分布在整个仿真过程中差异很大，这很难用开箱即用的方法捕获。在这项研究中，我们介绍了专家 - 在爱丽丝实验中为零度量热仪量身定制的一种深度学习模拟方法。我们的方法利用了产生的专家架构的混合物，每个专家都专门模拟数据的不同子集。这可以实现更精确，更有效的生成过程，因为每个专家都专注于量热计响应的特定方面。与传统的蒙特卡洛方法相比，Expertsim不仅提高了准确性，而且还提供了显着的加速，为CERN粒子物理实验中的高效检测器模拟提供了有希望的解决方案。我们在此HTTPS URL上提供代码。</li>
</ul>

<h3>Title: ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Paritosh Parmar, Eric Peh, Basura Fernando</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21010">https://arxiv.org/abs/2508.21010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21010">https://arxiv.org/pdf/2508.21010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21010]] ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering(https://arxiv.org/abs/2508.21010)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Existing Causal-Why Video Question Answering (VideoQA) models often struggle with higher-order reasoning, relying on opaque, monolithic pipelines that entangle video understanding, causal inference, and answer generation. These black-box approaches offer limited interpretability and tend to depend on shallow heuristics. We propose a novel, modular framework that explicitly decouples causal reasoning from answer generation, introducing natural language causal chains as interpretable intermediate representations. Inspired by human cognitive models, these structured cause-effect sequences bridge low-level video content with high-level causal reasoning, enabling transparent and logically coherent inference. Our two-stage architecture comprises a Causal Chain Extractor (CCE) that generates causal chains from video-question pairs, and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in these chains. To address the lack of annotated reasoning traces, we introduce a scalable method for generating high-quality causal chains from existing datasets using large language models. We also propose CauCo, a new evaluation metric for causality-oriented captioning. Experiments on three large-scale benchmarks demonstrate that our approach not only outperforms state-of-the-art models, but also yields substantial gains in explainability, user trust, and generalization -- positioning the CCE as a reusable causal reasoning engine across diverse domains. Project page: this https URL</li>
<li><strong>摘要：</strong>现有的因果 - 为什么视频问题回答（VideoQA）模型通常会在高阶推理方面挣扎，依靠不透明的单片管道纠缠视频理解，因果推理和答案生成。这些黑盒方法提供了有限的解释性，并且倾向于依赖浅启发式方法。我们提出了一个新颖的模块化框架，该框架明确地将因果推理与答案产生相结合，将自然语言因果关系链作为可解释的中间表示。受人类认知模型的启发，这些结构化原因效应序列将低级视频内容带有高级因果推理，从而实现了透明和逻辑上相干的推断。我们的两阶段架构包括一个因果链提取器（CCE），该链链提取器（CCE）从视频问题对产生因果关系，以及因果链驱动的答案（CCDA），产生以这些链为基础的答案。为了解决缺乏注释的推理迹线，我们引入了一种可扩展的方法，用于使用大语言模型从现有数据集中生成高质量的因果链。我们还提出了Cauco，这是一种针对因果关系字幕的新评估指标。在三个大规模基准上进行的实验表明，我们的方法不仅要优于最先进的模型，而且在解释性，用户信任和概括方面也带来了可观的增长 - 将CCE定位为跨不同领域的可重复使用的因果推理引擎。项目页面：此HTTPS URL</li>
</ul>

<h3>Title: Inference-Time Alignment Control for Diffusion Models with Reinforcement Learning Guidance</h3>
<ul>
<li><strong>Authors: </strong>Luozhijie Jin, Zijie Qiu, Jie Liu, Zijie Diao, Lifeng Qiao, Ning Ding, Alex Lamb, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21016">https://arxiv.org/abs/2508.21016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21016">https://arxiv.org/pdf/2508.21016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21016]] Inference-Time Alignment Control for Diffusion Models with Reinforcement Learning Guidance(https://arxiv.org/abs/2508.21016)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Denoising-based generative models, particularly diffusion and flow matching algorithms, have achieved remarkable success. However, aligning their output distributions with complex downstream objectives, such as human preferences, compositional accuracy, or data compressibility, remains challenging. While reinforcement learning (RL) fine-tuning methods, inspired by advances in RL from human feedback (RLHF) for large language models, have been adapted to these generative frameworks, current RL approaches are suboptimal for diffusion models and offer limited flexibility in controlling alignment strength after fine-tuning. In this work, we reinterpret RL fine-tuning for diffusion models through the lens of stochastic differential equations and implicit reward conditioning. We introduce Reinforcement Learning Guidance (RLG), an inference-time method that adapts Classifier-Free Guidance (CFG) by combining the outputs of the base and RL fine-tuned models via a geometric average. Our theoretical analysis shows that RLG's guidance scale is mathematically equivalent to adjusting the KL-regularization coefficient in standard RL objectives, enabling dynamic control over the alignment-quality trade-off without further training. Extensive experiments demonstrate that RLG consistently improves the performance of RL fine-tuned models across various architectures, RL algorithms, and downstream tasks, including human preferences, compositional control, compressibility, and text rendering. Furthermore, RLG supports both interpolation and extrapolation, thereby offering unprecedented flexibility in controlling generative alignment. Our approach provides a practical and theoretically sound solution for enhancing and controlling diffusion model alignment at inference. The source code for RLG is publicly available at the Github: this https URL.</li>
<li><strong>摘要：</strong>基于脱氧的生成模型，尤其是扩散和流量匹配算法，取得了显着的成功。但是，将其输出分布与复杂的下游目标（例如人类偏好，组成精度或数据可压缩性）保持一致。虽然受到大型语言模型的RL进步（RLHF）的进步的启发，但已适应了这些生成性框架的启发，但当前的RL方法是扩散模型的次优，并且在调整后控制一致性方面具有有限的灵活性。在这项工作中，我们通过随机微分方程和隐式奖励条件重新解释了扩散模型的RL微调。我们引入了加强学习指南（RLG），这是一种推理时间方法，通过通过几何平均值组合基础和RL微调模型来适应无分类器指导（CFG）。我们的理论分析表明，RLG的指导量表在数学上等同于在标准RL目标中调整KL登记系数，从而在没有进一步培训的情况下对对齐质量的权衡进行了动态控制。广泛的实验表明，RLG始终提高各种体系结构，RL算法和下游任务的RL微调模型的性能，包括人类的偏好，组成控制，可压缩性和文本渲染。此外，RLG支持插值和外推，从而在控制生成对齐时提供了前所未有的灵活性。我们的方法提供了一种实用且理论上的声音解决方案，以增强和控制推理时的扩散模型对齐。 RLG的源代码可在GitHub公开获取：此HTTPS URL。</li>
</ul>

<h3>Title: POSE: Phased One-Step Adversarial Equilibrium for Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Jiaxiang Cheng, Bing Ma, Xuhua Ren, Hongyi Jin, Kai Yu, Peng Zhang, Wenyue Li, Yuan Zhou, Tianxiang Zheng, Qinglin Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21019">https://arxiv.org/abs/2508.21019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21019">https://arxiv.org/pdf/2508.21019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21019]] POSE: Phased One-Step Adversarial Equilibrium for Video Diffusion Models(https://arxiv.org/abs/2508.21019)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The field of video diffusion generation faces critical bottlenecks in sampling efficiency, especially for large-scale models and long sequences. Existing video acceleration methods adopt image-based techniques but suffer from fundamental limitations: they neither model the temporal coherence of video frames nor provide single-step distillation for large-scale video models. To bridge this gap, we propose POSE (Phased One-Step Equilibrium), a distillation framework that reduces the sampling steps of large-scale video diffusion models, enabling the generation of high-quality videos in a single step. POSE employs a carefully designed two-phase process to distill video models:(i) stability priming: a warm-up mechanism to stabilize adversarial distillation that adapts the high-quality trajectory of the one-step generator from high to low signal-to-noise ratio regimes, optimizing the video quality of single-step mappings near the endpoints of flow trajectories. (ii) unified adversarial equilibrium: a flexible self-adversarial distillation mechanism that promotes stable single-step adversarial training towards a Nash equilibrium within the Gaussian noise space, generating realistic single-step videos close to real videos. For conditional video generation, we propose (iii) conditional adversarial consistency, a method to improve both semantic consistency and frame consistency between conditional frames and generated frames. Comprehensive experiments demonstrate that POSE outperforms other acceleration methods on VBench-I2V by average 7.15% in semantic alignment, temporal conference and frame quality, reducing the latency of the pre-trained model by 100$\times$, from 1000 seconds to 10 seconds, while maintaining competitive performance.</li>
<li><strong>摘要：</strong>视频扩散生成领域在抽样效率中面临着关键的瓶颈，尤其是对于大型模型和长序列。现有的视频加速方法采用了基于图像的技术，但受到基本局限性：它们既不建模视频框架的时间连贯性，也没有为大型视频模型提供单步蒸馏。为了弥合这个间隙，我们提出了姿势（分阶段的一步平衡），这是一个蒸馏框架，可减少大规模视频扩散模型的采样步骤，从而在单一步骤中生成高质量的视频。 Pose采用精心设计的两阶段过程来提炼视频模型：（i）稳定性启动：一种稳定对抗性蒸馏的热身机制，可适应一步生成器的高质量轨迹，从高到低信号到低点的比例方案，优化了接近流量的单步映射的视频质量的视频质量。 （ii）统一的对抗平衡：一种灵活的自我分离蒸馏机制，可在高斯噪声空间内促进稳定的单步对逆向训练，从而产生靠近真实视频的现实单步视频。对于有条件的视频生成，我们提出（III）条件对抗性一致性，这是一种提高条件框架和生成帧之间语义一致性和框架一致性的方法。全面的实验表明，在语义一致性，时间会议和框架质量上，对VBENCH-I2V的其他加速方法的表现平均高7.15％，从而使预训练的模型的潜伏期从1000秒降低到10秒，同时保持有竞争力的绩效。</li>
</ul>

<h3>Title: Reusing Computation in Text-to-Image Diffusion for Efficient Generation of Image Sets</h3>
<ul>
<li><strong>Authors: </strong>Dale Decatur, Thibault Groueix, Wang Yifan, Rana Hanocka, Vladimir Kim, Matheus Gadelha</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21032">https://arxiv.org/abs/2508.21032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21032">https://arxiv.org/pdf/2508.21032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21032]] Reusing Computation in Text-to-Image Diffusion for Efficient Generation of Image Sets(https://arxiv.org/abs/2508.21032)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models enable high-quality image generation but are computationally expensive. While prior work optimizes per-inference efficiency, we explore an orthogonal approach: reducing redundancy across correlated prompts. Our method leverages the coarse-to-fine nature of diffusion models, where early denoising steps capture shared structures among similar prompts. We propose a training-free approach that clusters prompts based on semantic similarity and shares computation in early diffusion steps. Experiments show that for models trained conditioned on image embeddings, our approach significantly reduces compute cost while improving image quality. By leveraging UnClip's text-to-image prior, we enhance diffusion step allocation for greater efficiency. Our method seamlessly integrates with existing pipelines, scales with prompt sets, and reduces the environmental and financial burden of large-scale text-to-image generation. Project page: this https URL</li>
<li><strong>摘要：</strong>文本到图像扩散模型可实现高质量的图像生成，但在计算上很昂贵。虽然先前的工作优化了每次推理效率，但我们探讨了一种正交方法：降低相关提示之间的冗余。我们的方法利用了扩散模型的粗到最新性质，其中早期的脱氧步骤捕获了相似提示之间的共享结构。我们提出了一种无训练的方法，该方法基于语义相似性提示，并在早期扩散步骤中共享计算。实验表明，对于经过图像嵌入的训练的模型，我们的方法大大降低了计算成本，同时提高了图像质量。通过利用Unclip的文本对图像之前，我们可以增强扩散步骤分配以提高效率。我们的方法无缝地与现有管道，及时集合的比例无缝集成，并减少了大规模文本到图像生成的环境和财务负担。项目页面：此HTTPS URL</li>
</ul>

<h3>Title: CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification</h3>
<ul>
<li><strong>Authors: </strong>Wei Li, Renshan Zhang, Rui Shao, Jie He, Liqiang Nie</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21046">https://arxiv.org/abs/2508.21046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21046">https://arxiv.org/pdf/2508.21046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21046]] CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification(https://arxiv.org/abs/2508.21046)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent Vision-Language-Action (VLA) models built on pre-trained Vision-Language Models (VLMs) require extensive post-training, resulting in high computational overhead that limits scalability and this http URL propose CogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages instruction-driven routing and sparsification to improve both efficiency and performance. CogVLA draws inspiration from human multimodal coordination and introduces a 3-stage progressive architecture. 1) Encoder-FiLM based Aggregation Routing (EFA-Routing) injects instruction information into the vision encoder to selectively aggregate and compress dual-stream visual tokens, forming a instruction-aware latent representation. 2) Building upon this compact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing) introduces action intent into the language model by pruning instruction-irrelevant visually grounded tokens, thereby achieving token-level sparsity. 3) To ensure that compressed perception inputs can still support accurate and coherent action generation, we introduce V-L-A Coupled Attention (CAtten), which combines causal vision-language attention with bidirectional action parallel decoding. Extensive experiments on the LIBERO benchmark and real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art performance with success rates of 97.4% and 70.0%, respectively, while reducing training costs by 2.5-fold and decreasing inference latency by 2.8-fold compared to OpenVLA. CogVLA is open-sourced and publicly available at this https URL.</li>
<li><strong>摘要：</strong>建立在预训练的视觉语言模型（VLM）上的最新视觉语言操作（VLA）模型需要广泛的培训，从而导致高计算开销，从而限制了可扩展性，并且该HTTP URL提出了Cognition Cognition Alignition Alignition Alignition-Cognition-Live-Langiagagagagagagagagagagagagagagagagagagagagagagagaigagage-action-Action框架，以指导指导驱动的驱动和明确的效率，以提高效率和绩效的效率，并提高了效率的效率和效率。 Cogvla从人类的多模式协调中汲取灵感，并引入了三阶段的进步建筑。 1）基于编码器 - 基于编码器的聚合路由（EFA路由）将指令信息注入视觉编码器，以选择性地汇总和压缩双流式视觉令牌，形成指令意识到的潜在表示。 2）建立在这种紧凑的视觉编码，基于LLM-FILM基于LLM-FILM的修剪路由（LFP路由），通过修剪教学指令 -  iRretar-irrethly上的视觉扎根代币，从而实现了令牌级的稀疏性，从而对语言模型进行了动作。 3）为了确保压缩感知输入仍然可以支持准确且连贯的动作生成，我们引入了V-L-A耦合注意力（Catten），该耦合的注意力（Catten）结合了因果视力语言的关注与双向动作平行解码。对Libero基准和现实世界的机器人任务进行了广泛的实验表明，Cogvla的成功率分别为97.4％和70.0％，与OpenVLA相比，成功率分别为97.4％和70.0％，而推理延迟则降低了2.5倍。 Cogvla是开源的，可在此HTTPS URL上公开使用。</li>
</ul>

<h3>Title: OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning</h3>
<ul>
<li><strong>Authors: </strong>Yuan Gong, Xionghui Wang, Jie Wu, Shiyin Wang, Yitong Wang, Xinglong Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21066">https://arxiv.org/abs/2508.21066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21066">https://arxiv.org/pdf/2508.21066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21066]] OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning(https://arxiv.org/abs/2508.21066)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce OneReward, a unified reinforcement learning framework that enhances the model's generative capabilities across multiple tasks under different evaluation criteria using only \textit{One Reward} model. By employing a single vision-language model (VLM) as the generative reward model, which can distinguish the winner and loser for a given task and a given evaluation criterion, it can be effectively applied to multi-task generation models, particularly in contexts with varied data and diverse task objectives. We utilize OneReward for mask-guided image generation, which can be further divided into several sub-tasks such as image fill, image extend, object removal, and text rendering, involving a binary mask as the edit area. Although these domain-specific tasks share same conditioning paradigm, they differ significantly in underlying data distributions and evaluation metrics. Existing methods often rely on task-specific supervised fine-tuning (SFT), which limits generalization and training efficiency. Building on OneReward, we develop Seedream 3.0 Fill, a mask-guided generation model trained via multi-task reinforcement learning directly on a pre-trained base model, eliminating the need for task-specific SFT. Experimental results demonstrate that our unified edit model consistently outperforms both commercial and open-source competitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across multiple evaluation dimensions. Code and model are available at: this https URL</li>
<li><strong>摘要：</strong>在本文中，我们介绍了OneReard，这是一个统一的增强学习框架，可在不同的评估标准下仅使用\ textit {一个奖励}模型在不同的评估标准下增强模型的生成能力。通过采用单一视觉模型（VLM）作为生成奖励模型，该模型可以区分给定任务的获胜者和失败者以及给定的评估标准，它可以有效地应用于多任务生成模型，尤其是在具有多样的数据和多样化目标目标的上下文中。我们利用屏蔽引导的图像生成，可以将其进一步分为几个子任务，例如图像填充，图像扩展，对象删除和文本渲染，涉及二进制掩码作为编辑区域。尽管这些特定领域的任务具有相同的条件范式，但它们在基本数据分布和评估指标中差异很大。现有方法通常依赖于特定于任务的监督微调（SFT），这限制了概括和培训效率。在OneReard的基础上，我们开发了SeedReam 3.0 Fill，这是一种直接在预训练的基础模型上通过多任务增强学习训练的面具引导的生成模型，从而消除了对特定于任务的SFT的需求。实验结果表明，我们的统一编辑模型在多个评估维度上始终优于商业和开源竞争者，例如意识形态图，Adobe Photoshop和Flux Fill [Pro]。代码和型号可用：此HTTPS URL</li>
</ul>

<h3>Title: First-Place Solution to NeurIPS 2024 Invisible Watermark Removal Challenge</h3>
<ul>
<li><strong>Authors: </strong>Fahad Shamshad, Tameem Bakr, Yahia Shaaban, Noor Hussein, Karthik Nandakumar, Nils Lukas</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21072">https://arxiv.org/abs/2508.21072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21072">https://arxiv.org/pdf/2508.21072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21072]] First-Place Solution to NeurIPS 2024 Invisible Watermark Removal Challenge(https://arxiv.org/abs/2508.21072)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Content watermarking is an important tool for the authentication and copyright protection of digital media. However, it is unclear whether existing watermarks are robust against adversarial attacks. We present the winning solution to the NeurIPS 2024 Erasing the Invisible challenge, which stress-tests watermark robustness under varying degrees of adversary knowledge. The challenge consisted of two tracks: a black-box and beige-box track, depending on whether the adversary knows which watermarking method was used by the provider. For the beige-box track, we leverage an adaptive VAE-based evasion attack, with a test-time optimization and color-contrast restoration in CIELAB space to preserve the image's quality. For the black-box track, we first cluster images based on their artifacts in the spatial or frequency-domain. Then, we apply image-to-image diffusion models with controlled noise injection and semantic priors from ChatGPT-generated captions to each cluster with optimized parameter settings. Empirical evaluations demonstrate that our method successfully achieves near-perfect watermark removal (95.7%) with negligible impact on the residual image's quality. We hope that our attacks inspire the development of more robust image watermarking methods.</li>
<li><strong>摘要：</strong>内容水印是对数字媒体进行身份验证和版权保护的重要工具。但是，目前尚不清楚现有水印是否适用于对抗攻击。我们提出了2024年神经的获奖解决方案，以消除无形的挑战，该挑战在不同程度的对手知识下强调了水印的鲁棒性。挑战包括两个曲目：一个黑盒和米色框曲目，具体取决于对手是否知道提供商使用了哪种水印方法。对于米色框轨道，我们利用基于自适应的VAE逃避攻击，并在Cielab空间中进行了测试时间优化和颜色对比度修复，以保持图像的质量。对于Black-Box轨道，我们首先根据其伪像在空间或频域中的伪影聚集。然后，我们将带有控制噪声注入的图像到图像扩散模型应用于Chatgpt生成的字幕的语义注射和使用优化参数设置的每个群集。经验评估表明，我们的方法成功地取得了接近完美的水印（95.7％），对残留图像的质量的影响微不足道。我们希望我们的攻击激发了更强大的图像水印方法的发展。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
