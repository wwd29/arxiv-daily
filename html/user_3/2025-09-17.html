<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-17</h1>
<h3>Title: PowerGrow: Feasible Co-Growth of Structures and Dynamics for Power Grid Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Xinyu He, Chenhan Xiao, Haoran Li, Ruizhong Qiu, Zhe Xu, Yang Weng, Jingrui He, Hanghang Tong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12212">https://arxiv.org/abs/2509.12212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12212">https://arxiv.org/pdf/2509.12212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12212]] PowerGrow: Feasible Co-Growth of Structures and Dynamics for Power Grid Synthesis(https://arxiv.org/abs/2509.12212)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Modern power systems are becoming increasingly dynamic, with changing topologies and time-varying loads driven by renewable energy variability, electric vehicle adoption, and active grid reconfiguration. Despite these changes, publicly available test cases remain scarce, due to security concerns and the significant effort required to anonymize real systems. Such limitations call for generative tools that can jointly synthesize grid structure and nodal dynamics. However, modeling the joint distribution of network topology, branch attributes, bus properties, and dynamic load profiles remains a major challenge, while preserving physical feasibility and avoiding prohibitive computational costs. We present PowerGrow, a co-generative framework that significantly reduces computational overhead while maintaining operational validity. The core idea is dependence decomposition: the complex joint distribution is factorized into a chain of conditional distributions over feasible grid topologies, time-series bus loads, and other system attributes, leveraging their mutual dependencies. By constraining the generation process at each stage, we implement a hierarchical graph beta-diffusion process for structural synthesis, paired with a temporal autoencoder that embeds time-series data into a compact latent space, improving both training stability and sample fidelity. Experiments across benchmark settings show that PowerGrow not only outperforms prior diffusion models in fidelity and diversity but also achieves a 98.9\% power flow convergence rate and improved N-1 contingency resilience. This demonstrates its ability to generate operationally valid and realistic power grid scenarios.</li>
<li><strong>摘要：</strong>现代电力系统变得越来越动态，随着可再生能源可变性，采用电动汽车的采用和主动网格重新配置，拓扑变化和随时间变化的负载。尽管有这些变化，但由于安全问题以及匿名化实际系统所需的重大努力，公开可用的测试案例仍然很少。这种限制要求生成工具可以共同合成网格结构和节点动力学。但是，建模网络拓扑，分支属性，总线特性和动态负载轮廓的联合分布仍然是一个主要的挑战，同时保持身体可行性并避免过度的计算成本。我们提出了PowerGrow，这是一个共同基础的框架，可在维持运营有效性的同时大大降低计算开销。核心思想是依赖性分解：复杂的关节分布被分解为可行的网格拓扑，时间序列总线负载和其他系统属性的条件分布链，利用其相互依赖性。通过限制每个阶段的生成过程，我们实现了一个分层图β-扩散过程来进行结构合成，并与时间序列数据嵌入时间序列数据中的时间序列编码器配对，从而改善了训练稳定性和样品保真度。跨基准设置的实验表明，PowerFrow不仅要优于忠诚度和多样性中的先前扩散模型，而且还达到了98.9 \％的功率流收敛速率和提高的N-1偶性弹性。这表明了其生成操作有效和现实的电网方案的能力。</li>
</ul>

<h3>Title: TripOptimizer: Generative 3D Shape Optimization and Drag Prediction using Triplane VAE Networks</h3>
<ul>
<li><strong>Authors: </strong>Parsa Vatani, Mohamed Elrefaie, Farhad Nazarpour, Faez Ahmed</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12224">https://arxiv.org/abs/2509.12224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12224">https://arxiv.org/pdf/2509.12224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12224]] TripOptimizer: Generative 3D Shape Optimization and Drag Prediction using Triplane VAE Networks(https://arxiv.org/abs/2509.12224)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The computational cost of traditional Computational Fluid Dynamics-based Aerodynamic Shape Optimization severely restricts design space exploration. This paper introduces TripOptimizer, a fully differentiable deep learning framework for rapid aerodynamic analysis and shape optimization directly from vehicle point cloud data. TripOptimizer employs a Variational Autoencoder featuring a triplane-based implicit neural representation for high-fidelity 3D geometry reconstruction and a drag coefficient prediction head. Trained on DrivAerNet++, a large-scale dataset of 8,000 unique vehicle geometries with corresponding drag coefficients computed via Reynolds-Averaged Navier-Stokes simulations, the model learns a latent representation that encodes aerodynamically salient geometric features. We propose an optimization strategy that modifies a subset of the encoder parameters to steer an initial geometry towards a target drag value, and demonstrate its efficacy in case studies where optimized designs achieved drag coefficient reductions up to 11.8\%. These results were subsequently validated by using independent, high-fidelity Computational Fluid Dynamics simulations with more than 150 million cells. A key advantage of the implicit representation is its inherent robustness to geometric imperfections, enabling optimization of non-watertight meshes, a significant challenge for traditional adjoint-based methods. The framework enables a more agile Aerodynamic Shape Optimization workflow, reducing reliance on computationally intensive CFD simulations, especially during early design stages.</li>
<li><strong>摘要：</strong>传统计算流体动力学的计算成本严重限制了设计空间探索。本文介绍了Tripoptimizer，这是一个完全可区分的深度学习框架，可直接从车辆点云数据中直接进行空气动力学分析和形状优化。 Tripoptimizer采用了一个差异自动编码器，该自动编码器具有基于三倍的隐式神经表示，用于高保真3D几何重建和阻力系数预测头。该模型在Drivaernet ++（由8,000个独特的车辆几何形状的大规模数据集中受过训练，其通过雷诺（Reynolds）播放的Navier-Stokes模拟计算出相应的阻力系数，该模型将学习一个编码空气动力学上显着的几何特征的潜在表示。我们提出了一种优化策略，该策略修改了编码器参数的子集，以将初始几何形状转向目标阻力值，并在案例研究中证明其功效，在案例研究中，优化设计实现了最高为11.8％的阻力系数减少。随后，通过使用超过1.5亿个细胞的独立，高保真计算流体动力学模拟来验证这些结果。隐式表示的一个关键优势是它对几何缺陷的固有鲁棒性，从而优化了非紧密网格，这是传统基于伴随的方法的重大挑战。该框架可以实现更敏捷的空气动力学优化工作流程，从而减少了对计算密集型CFD模拟的依赖，尤其是在早期设计阶段。</li>
</ul>

<h3>Title: Flexible Multimodal Neuroimaging Fusion for Alzheimer's Disease Progression Prediction</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Burns, Yuan Xue, Douglas W. Scharre, Xia Ning</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12234">https://arxiv.org/abs/2509.12234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12234">https://arxiv.org/pdf/2509.12234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12234]] Flexible Multimodal Neuroimaging Fusion for Alzheimer's Disease Progression Prediction(https://arxiv.org/abs/2509.12234)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Alzheimer's disease (AD) is a progressive neurodegenerative disease with high inter-patient variance in rate of cognitive decline. AD progression prediction aims to forecast patient cognitive decline and benefits from incorporating multiple neuroimaging modalities. However, existing multimodal models fail to make accurate predictions when many modalities are missing during inference, as is often the case in clinical settings. To increase multimodal model flexibility under high modality missingness, we introduce PerM-MoE, a novel sparse mixture-of-experts method that uses independent routers for each modality in place of the conventional, single router. Using T1-weighted MRI, FLAIR, amyloid beta PET, and tau PET neuroimaging data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), we evaluate PerM-MoE, state-of-the-art Flex-MoE, and unimodal neuroimaging models on predicting two-year change in Clinical Dementia Rating-Sum of Boxes (CDR-SB) scores under varying levels of modality missingness. PerM-MoE outperforms the state of the art in most variations of modality missingness and demonstrates more effective utility of experts than Flex-MoE.</li>
<li><strong>摘要：</strong>阿尔茨海默氏病（AD）是一种进行性神经退行性疾病，其认知能力下降率很高。 AD进展预测旨在预测患者认知能力下降，并纳入多种神经影像模式。但是，在推断过程中缺少许多模式时，现有的多模式无法做出准确的预测，就像临床环境中一样。为了提高高模态缺失的多模型模型灵活性，我们引入了Perm-Moe，这是一种新型的稀疏Experts方法，它为每种模式使用独立的路由器代替了常规的单一路由器。 Using T1-weighted MRI, FLAIR, amyloid beta PET, and tau PET neuroimaging data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), we evaluate PerM-MoE, state-of-the-art Flex-MoE, and unimodal neuroimaging models on predicting two-year change in Clinical Dementia Rating-Sum of Boxes (CDR-SB) scores under varying levels of modality缺失。 Perm-Moe在大多数方式失踪方面都胜过了最新技术的状态，并且与Flex-MoE相比，专家的实用性更有效。</li>
</ul>

<h3>Title: RL Fine-Tuning Heals OOD Forgetting in SFT</h3>
<ul>
<li><strong>Authors: </strong>Hangzhan Jin, Sitao Luan, Sicheng Lyu, Guillaume Rabusseau, Reihaneh Rabbany, Doina Precup, Mohammad Hamdaqa</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12235">https://arxiv.org/abs/2509.12235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12235">https://arxiv.org/pdf/2509.12235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12235]] RL Fine-Tuning Heals OOD Forgetting in SFT(https://arxiv.org/abs/2509.12235)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>The two-stage fine-tuning paradigm of Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) has empirically shown better reasoning performance than one-stage SFT for the post-training of Large Language Models (LLMs). However, the evolution and mechanism behind the synergy of SFT and RL are still under-explored and inconclusive. In our study, we find the well-known claim "SFT memorizes, RL generalizes" is over-simplified, and discover that: (1) OOD performance peaks at the early stage of SFT and then declines (OOD forgetting), the best SFT checkpoint cannot be captured by training/test loss; (2) the subsequent RL stage does not generate fundamentally better OOD capability, instead it plays an \textbf{OOD restoration} role, recovering the lost reasoning ability during SFT; (3) The recovery ability has boundaries, \ie{} \textbf{if SFT trains for too short or too long, RL cannot recover the lost OOD ability;} (4) To uncover the underlying mechanisms behind the forgetting and restoration process, we employ SVD analysis on parameter matrices, manually edit them, and observe their impacts on model performance. Unlike the common belief that the shift of model capacity mainly results from the changes of singular values, we find that they are actually quite stable throughout fine-tuning. Instead, the OOD behavior strongly correlates with the \textbf{rotation of singular vectors}. Our findings re-identify the roles of SFT and RL in the two-stage fine-tuning and discover the rotation of singular vectors as the key mechanism. %reversing the rotations induced by SFT, which shows recovery from forgetting, whereas imposing the SFT parameter directions onto a RL-tuned model results in performance degradation. Code is available at this https URL</li>
<li><strong>摘要：</strong>有监督的微调（SFT）的两阶段微调范式随后进行了加强学习（RL），其凭经验表现出了比一阶段SFT更好的推理性能，用于大型语言模型（LLMS）的后培训。但是，SFT和RL协同作用背后的演变和机制仍然不足且尚无定论。在我们的研究中，我们发现众所周知的“ SFT记忆，RL概括”被过度简化，并发现：（1）OOD性能在SFT的早期阶段达到峰值，然后下降（OOD忘记），最佳的SFT检查点无法通过训练/测试损失来捕获； （2）随后的RL阶段没有从根本上产生更好的OOD能力，而是扮演\ textbf {ood restoration}角色，从而恢复了SFT期间丢失的推理能力； （3）恢复能力具有界限，\ ie {} \ textbf {如果SFT训练太短或太长，RL无法恢复丢失的OOD能力;}（4）以揭示遗忘和恢复过程背后的基本机制，则我们对SVD分析进行了对参数矩阵的SVD分析，并将其培训编辑它们，并观察其效果上的模型表演。与普遍认为模型能力的转移主要是由于奇异值的变化而导致的，我们发现它们实际上在整个微调过程中非常稳定。相反，OOD行为与\ textbf {单数向量的旋转}密切相关。我们的发现重新识别了SFT和RL在两阶段微调中的作用，并发现单数矢量作为关键机制的旋转。逆转SFT引起的旋转的％，该旋转显示出忘记的恢复，而将SFT参数方向施加到RL调整的模型上会导致性能下降。代码可在此HTTPS URL上找到</li>
</ul>

<h3>Title: InJecteD: Analyzing Trajectories and Drift Dynamics in Denoising Diffusion Probabilistic Models for 2D Point Cloud Generation</h3>
<ul>
<li><strong>Authors: </strong>Sanyam Jain, Khuram Naveed, Illia Oleksiienko, Alexandros Iosifidis, Ruben Pauwels</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12239">https://arxiv.org/abs/2509.12239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12239">https://arxiv.org/pdf/2509.12239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12239]] InJecteD: Analyzing Trajectories and Drift Dynamics in Denoising Diffusion Probabilistic Models for 2D Point Cloud Generation(https://arxiv.org/abs/2509.12239)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>This work introduces InJecteD, a framework for interpreting Denoising Diffusion Probabilistic Models (DDPMs) by analyzing sample trajectories during the denoising process of 2D point cloud generation. We apply this framework to three datasets from the Datasaurus Dozen bullseye, dino, and circle using a simplified DDPM architecture with customizable input and time embeddings. Our approach quantifies trajectory properties, including displacement, velocity, clustering, and drift field dynamics, using statistical metrics such as Wasserstein distance and cosine similarity. By enhancing model transparency, InJecteD supports human AI collaboration by enabling practitioners to debug and refine generative models. Experiments reveal distinct denoising phases: initial noise exploration, rapid shape formation, and final refinement, with dataset-specific behaviors example, bullseyes concentric convergence vs. dinos complex contour formation. We evaluate four model configurations, varying embeddings and noise schedules, demonstrating that Fourier based embeddings improve trajectory stability and reconstruction quality</li>
<li><strong>摘要：</strong>这项工作引入了注入，这是一个解释脱氧扩散概率模型（DDPM）的框架，通过分析2D点云生成过程中的样品轨迹。我们使用具有可自定义输入和时间嵌入式的简化DDPM体系结构将此框架应用于Datasaurus十二尔牛仔，Dino和Circle的三个数据集。我们的方法使用统计指标（例如Wasserstein距离和余弦相似性）量化了轨迹特性，包括位移，速度，聚类和漂移场动力学。通过提高模型透明度，注入的注射支持通过使从业人员可以调试和完善生成模型来支持人AI协作。实验揭示了明显的降解阶段：初始噪声探索，快速形状的形成和最终的改进，具有数据集特异性的行为示例，Bullseyes同心收敛与恐龙复杂轮廓形成。我们评估了四种型号配置，不同的嵌入和噪声时间表，表明基于傅立叶的嵌入可以改善轨迹稳定性和重建质量</li>
</ul>

<h3>Title: OnlineHOI: Towards Online Human-Object Interaction Generation and Perception</h3>
<ul>
<li><strong>Authors: </strong>Yihong Ji, Yunze Liu, Yiyao Zhuo, Weijiang Yu, Fei Ma, Joshua Huang, Fei Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12250">https://arxiv.org/abs/2509.12250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12250">https://arxiv.org/pdf/2509.12250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12250]] OnlineHOI: Towards Online Human-Object Interaction Generation and Perception(https://arxiv.org/abs/2509.12250)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The perception and generation of Human-Object Interaction (HOI) are crucial for fields such as robotics, AR/VR, and human behavior understanding. However, current approaches model this task in an offline setting, where information at each time step can be drawn from the entire interaction sequence. In contrast, in real-world scenarios, the information available at each time step comes only from the current moment and historical data, i.e., an online setting. We find that offline methods perform poorly in an online context. Based on this observation, we propose two new tasks: Online HOI Generation and Perception. To address this task, we introduce the OnlineHOI framework, a network architecture based on the Mamba framework that employs a memory mechanism. By leveraging Mamba's powerful modeling capabilities for streaming data and the Memory mechanism's efficient integration of historical information, we achieve state-of-the-art results on the Core4D and OAKINK2 online generation tasks, as well as the online HOI4D perception task.</li>
<li><strong>摘要：</strong>人类对象相互作用（HOI）的感知和产生对于机器人，AR/VR和人类行为理解等领域至关重要。但是，当前方法在离线设置中对此任务进行了建模，在该设置中，每个时间步骤中的信息都可以从整个交互序列中获取。相反，在实际情况下，每个时间步骤可用的信息仅来自当前时刻和历史数据，即在线设置。我们发现离线方法在在线环境中的性能差。基于此观察，我们提出了两个新任务：在线HOI产生和感知。为了解决此任务，我们介绍了在线HOI框架，这是一种基于采用内存机制的MAMBA框架的网络体系结构。通过利用Mamba的强大建模功能来流数据和记忆机制的历史信息有效整合，我们在Core4D和Oakink2在线生成任务以及在线HOI4D感知任务上实现了最先进的结果。</li>
</ul>

<h3>Title: Prediction of Stocks Index Price using Quantum GANs</h3>
<ul>
<li><strong>Authors: </strong>Sangram Deshpande, Gopal Ramesh Dahale, Sai Nandan Morapakula, Uday Wad</a></li>
<li><strong>Subjects: </strong>cs.LG, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12286">https://arxiv.org/abs/2509.12286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12286">https://arxiv.org/pdf/2509.12286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12286]] Prediction of Stocks Index Price using Quantum GANs(https://arxiv.org/abs/2509.12286)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper investigates the application of Quantum Generative Adversarial Networks (QGANs) for stock price prediction. Financial markets are inherently complex, marked by high volatility and intricate patterns that traditional models often fail to capture. QGANs, leveraging the power of quantum computing, offer a novel approach by combining the strengths of generative models with quantum machine learning techniques. We implement a QGAN model tailored for stock price prediction and evaluate its performance using historical stock market data. Our results demonstrate that QGANs can generate synthetic data closely resembling actual market behavior, leading to enhanced prediction accuracy. The experiment was conducted using the Stocks index price data and the AWS Braket SV1 simulator for training the QGAN circuits. The quantum-enhanced model outperforms classical Long Short-Term Memory (LSTM) and GAN models in terms of convergence speed and prediction accuracy. This research represents a key step toward integrating quantum computing in financial forecasting, offering potential advantages in speed and precision over traditional methods. The findings suggest important implications for traders, financial analysts, and researchers seeking advanced tools for market analysis.</li>
<li><strong>摘要：</strong>本文研究了量子生成对抗网络（QGAN）在股票价格预测中的应用。金融市场本质上是复杂的，以传统模型通常无法捕获的高波动性和复杂的模式为特征。利用量子计算的力量，QGAN通过将生成模型的优势与量子机学习技术相结合，提供了一种新颖的方法。我们实施了针对股票价格预测的QGAN模型，并使用历史股票市场数据评估了其绩效。我们的结果表明，QGAN可以生成与实际市场行为相似的合成数据，从而提高了预测准确性。该实验是使用股票指数数据数据和AWS Braket SV1模拟器进行训练QGAN电路的。量子增强模型在收敛速度和预测准确性方面优于经典的长期记忆（LSTM）和GAN模型。这项研究代表了将量子计算集成在财务预测中的关键步骤，从而在传统方法上具有速度和精度的潜在优势。这些发现表明了对贸易商，财务分析师和研究人员寻求高级工具进行市场分析的重要影响。</li>
</ul>

<h3>Title: Unsupervised Atomic Data Mining via Multi-Kernel Graph Autoencoders for Machine Learning Force Fields</h3>
<ul>
<li><strong>Authors: </strong>Hong Sun, Joshua A. Vita, Amit Samanta, Vincenzo Lordi</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12358">https://arxiv.org/abs/2509.12358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12358">https://arxiv.org/pdf/2509.12358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12358]] Unsupervised Atomic Data Mining via Multi-Kernel Graph Autoencoders for Machine Learning Force Fields(https://arxiv.org/abs/2509.12358)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Constructing a chemically diverse dataset while avoiding sampling bias is critical to training efficient and generalizable force fields. However, in computational chemistry and materials science, many common dataset generation techniques are prone to oversampling regions of the potential energy surface. Furthermore, these regions can be difficult to identify and isolate from each other or may not align well with human intuition, making it challenging to systematically remove bias in the dataset. While traditional clustering and pruning (down-sampling) approaches can be useful for this, they can often lead to information loss or a failure to properly identify distinct regions of the potential energy surface due to difficulties associated with the high dimensionality of atomic descriptors. In this work, we introduce the Multi-kernel Edge Attention-based Graph Autoencoder (MEAGraph) model, an unsupervised approach for analyzing atomic datasets. MEAGraph combines multiple linear kernel transformations with attention-based message passing to capture geometric sensitivity and enable effective dataset pruning without relying on labels or extensive training. Demonstrated applications on niobium, tantalum, and iron datasets show that MEAGraph efficiently groups similar atomic environments, allowing for the use of basic pruning techniques for removing sampling bias. This approach provides an effective method for representation learning and clustering that can be used for data analysis, outlier detection, and dataset optimization.</li>
<li><strong>摘要：</strong>在避免采样偏差的同时构建化学多样的数据集对于训练有效且可推广的力场至关重要。但是，在计算化学和材料科学中，许多常见的数据集生成技术容易出现势能表面的过度采样区域。此外，这些区域可能很难识别和隔离，或者可能与人类直觉不符，这使其在系统消除数据集中的偏见方面具有挑战性。尽管传统的聚类和修剪（下采样）方法对此很有用，但由于与原子描述符的高维度相关的困难，它们通常会导致信息丢失或无法正确识别势能表面的不同区域。在这项工作中，我们介绍了基于多内核边缘的基于注意的图形自动编码器（MEAGRAPH）模型，这是一种无监督的方法，用于分析原子数据集。 Meagraph将多个线性内核转换与基于注意力的消息传递结合在一起，以捕获几何灵敏度，并在不依赖标签或广泛培训的情况下实现有效的数据集修剪。在尼伯族，塔塔勒姆和铁数据集上展示了应用，表明Meagraph有效地分组了相似的原子环境，从而可以使用基本的修剪技术来消除采样偏差。这种方法为表示学习和聚类提供了一种有效的方法，可用于数据分析，离群检测和数据集优化。</li>
</ul>

<h3>Title: Explainable Unsupervised Multi-Anomaly Detection and Temporal Localization in Nuclear Times Series Data with a Dual Attention-Based Autoencoder</h3>
<ul>
<li><strong>Authors: </strong>Konstantinos Vasili, Zachery T. Dahm, Stylianos Chatzidakis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12372">https://arxiv.org/abs/2509.12372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12372">https://arxiv.org/pdf/2509.12372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12372]] Explainable Unsupervised Multi-Anomaly Detection and Temporal Localization in Nuclear Times Series Data with a Dual Attention-Based Autoencoder(https://arxiv.org/abs/2509.12372)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The nuclear industry is advancing toward more new reactor designs, with next-generation reactors expected to be smaller in scale and power output. These systems have the potential to produce large volumes of information in the form of multivariate time-series data, which could be used for enhanced real-time monitoring and control. In this context, the development of remote autonomous or semi-autonomous control systems for reactor operation has gained significant interest. A critical first step toward such systems is an accurate diagnostics module capable of detecting and localizing anomalies within the reactor system. Recent studies have proposed various ML and DL approaches for anomaly detection in the nuclear domain. Despite promising results, key challenges remain, including limited to no explainability, lack of access to real-world data, and scarcity of abnormal events, which impedes benchmarking and characterization. Most existing studies treat these methods as black boxes, while recent work highlights the need for greater interpretability of ML/DL outputs in safety-critical domains. Here, we propose an unsupervised methodology based on an LSTM autoencoder with a dual attention mechanism for characterization of abnormal events in a real-world reactor radiation area monitoring system. The framework includes not only detection but also localization of the event and was evaluated using real-world datasets of increasing complexity from the PUR-1 research reactor. The attention mechanisms operate in both the feature and temporal dimensions, where the feature attention assigns weights to radiation sensors exhibiting abnormal patterns, while time attention highlights the specific timesteps where irregularities occur, thus enabling localization. By combining the results, the framework can identify both the affected sensors and the duration of each anomaly within a single unified network.</li>
<li><strong>摘要：</strong>核工业正朝着更多的新反应堆设计发展，预计下一代反应堆的规模和功率输出将较小。这些系统有可能以多元时间序列数据的形式生成大量信息，这些信息可用于增强实时监视和控制。在这种情况下，用于反应堆操作的远程自主或半自主控制系统的开发引起了人们的重大兴趣。迈向此类系统的关键第一步是能够在反应堆系统中检测和定位异常的准确诊断模块。最近的研究提出了各种ML和DL方法用于核域中的异常检测。尽管结果有希望，但仍然存在关键挑战，包括不限于没有解释性，缺乏获得现实世界数据的访问以及异常事件的稀缺性，这阻碍了基准测试和表征。大多数现有研究将这些方法视为黑匣子，而最近的工作则强调了在安全 - 关键领域中对ML/DL输出更大的解释性的需求。在这里，我们提出了一种基于LSTM自动编码器的无监督方法，具有双重注意机制，用于表征现实世界反应器辐射区监测系统中异常事件。该框架不仅包括检测事件的检测，还包括事件的定位，并使用PUR-1研究反应器增加复杂性的现实世界数据集进行了评估。注意机制在特征和时间维度都起作用，其中特征注意力将权重分配给表现出异常模式的辐射传感器，而时间的注意则突出了发生不规则的特定时间段，从而实现了定位。通过组合结果，框架可以识别单个统一网络中每个异常的影响传感器和持续时间。</li>
</ul>

<h3>Title: Diffusion-Based Generation and Imputation of Driving Scenarios from Limited Vehicle CAN Data</h3>
<ul>
<li><strong>Authors: </strong>Julian Ripper, Ousama Esbel, Rafael Fietzek, Max Mühlhäuser, Thomas Kreutz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12375">https://arxiv.org/abs/2509.12375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12375">https://arxiv.org/pdf/2509.12375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12375]] Diffusion-Based Generation and Imputation of Driving Scenarios from Limited Vehicle CAN Data(https://arxiv.org/abs/2509.12375)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Training deep learning methods on small time series datasets that also include corrupted samples is challenging. Diffusion models have shown to be effective to generate realistic and synthetic data, and correct corrupted samples through imputation. In this context, this paper focuses on generating synthetic yet realistic samples of automotive time series data. We show that denoising diffusion probabilistic models (DDPMs) can effectively solve this task by applying them to a challenging vehicle CAN-dataset with long-term data and a limited number of samples. Therefore, we propose a hybrid generative approach that combines autoregressive and non-autoregressive techniques. We evaluate our approach with two recently proposed DDPM architectures for time series generation, for which we propose several improvements. To evaluate the generated samples, we propose three metrics that quantify physical correctness and test track adherence. Our best model is able to outperform even the training data in terms of physical correctness, while showing plausible driving behavior. Finally, we use our best model to successfully impute physically implausible regions in the training data, thereby improving the data quality.</li>
<li><strong>摘要：</strong>在小时序列数据集上培训深度学习方法，其中还包括损坏的样本是具有挑战性的。扩散模型已证明可以有效地生成逼真的和合成数据，并通过插补纠正损坏的样本。在这种情况下，本文着重于生成汽车时间序列数据的合成但现实的样本。我们表明，通过将其应用于具有长期数据和有限数量的样本的具有挑战性的车辆dataset，可以通过将其应用于具有挑战性的车辆可以通过将其应用于具有挑战性的车辆来有效地解决此任务，从而有效地解决此任务。因此，我们提出了一种混合生成方法，结合了自回归和非自动回归技术。我们使用最近提出的两个DDPM体系结构来评估我们的方法，以进行时间序列的生成，为此我们提出了一些改进。为了评估生成的样品，我们提出了三个量化物理正确性和测试轨道依从性的指标。我们的最佳模型能够在身体正确性方面胜过训练数据，同时显示出合理的驾驶行为。最后，我们使用最佳模型在培训数据中成功地将身体上不可思议的区域归为不可思议的区域，从而提高了数据质量。</li>
</ul>

<h3>Title: Image Tokenizer Needs Post-Training</h3>
<ul>
<li><strong>Authors: </strong>Kai Qiu, Xiang Li, Hao Chen, Jason Kuen, Xiaohao Xu, Jiuxiang Gu, Yinyi Luo, Bhiksha Raj, Zhe Lin, Marios Savvides</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12474">https://arxiv.org/abs/2509.12474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12474">https://arxiv.org/pdf/2509.12474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12474]] Image Tokenizer Needs Post-Training(https://arxiv.org/abs/2509.12474)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent image generative models typically capture the image distribution in a pre-constructed latent space, relying on a frozen image tokenizer. However, there exists a significant discrepancy between the reconstruction and generation distribution, where current tokenizers only prioritize the reconstruction task that happens before generative training without considering the generation errors during sampling. In this paper, we comprehensively analyze the reason for this discrepancy in a discrete latent space, and, from which, we propose a novel tokenizer training scheme including both main-training and post-training, focusing on improving latent space construction and decoding respectively. During the main training, a latent perturbation strategy is proposed to simulate sampling noises, \ie, the unexpected tokens generated in generative inference. Specifically, we propose a plug-and-play tokenizer training scheme, which significantly enhances the robustness of tokenizer, thus boosting the generation quality and convergence speed, and a novel tokenizer evaluation metric, \ie, pFID, which successfully correlates the tokenizer performance to generation quality. During post-training, we further optimize the tokenizer decoder regarding a well-trained generative model to mitigate the distribution difference between generated and reconstructed tokens. With a $\sim$400M generator, a discrete tokenizer trained with our proposed main training achieves a notable 1.60 gFID and further obtains 1.36 gFID with the additional post-training. Further experiments are conducted to broadly validate the effectiveness of our post-training strategy on off-the-shelf discrete and continuous tokenizers, coupled with autoregressive and diffusion-based generators.</li>
<li><strong>摘要：</strong>最近的图像生成模型通常依靠冷冻图像令牌捕获预构建的潜在空间中的图像分布。但是，重建和发电分布之间存在很大的差异，当前的引导者仅优先考虑在生成训练之前发生的重建任务，而无需考虑采样过程中的产生错误。在本文中，我们全面分析了离散潜在空间中这种差异的原因，并从中提出了一种新颖的令牌训练计划，包括主要训练和培训后，分别着重于改善潜在的空间构建和解码。在主要训练期间，提出了一种潜在的扰动策略来模拟采样噪声，即\ ie，这是生成推断中产生的意外令牌。具体而言，我们提出了一种插件的令牌训练方案，可显着增强令牌剂的鲁棒性，从而提高发电质量和收敛速度，以及一种新颖的令牌评估指标，\ ie，PFID，成功地将令牌性能与产物质量相关联。在训练后，我们进一步优化了有关训练有素的生成模型的代币解码器，以减轻生成和重建令牌之间的分布差。凭借$ \ sim $ 4亿发电机，经过我们拟议的主培训培训的离散令牌可以实现1.60 GFID，并在额外的培训后进一步获得1.36 GFID。进行了进一步的实验，以广泛验证我们的训练后策略对现成的离散和连续的引物的有效性，再加上自回归和基于扩散的发电机。</li>
</ul>

<h3>Title: Instance-Guided Class Activation Mapping for Weakly Supervised Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ali Torabi, Sanjog Gaihre, MD Mahbubur Rahman, Yaqoob Majeed</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12496">https://arxiv.org/abs/2509.12496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12496">https://arxiv.org/pdf/2509.12496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12496]] Instance-Guided Class Activation Mapping for Weakly Supervised Semantic Segmentation(https://arxiv.org/abs/2509.12496)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Weakly Supervised Semantic Segmentation (WSSS) addresses the challenge of training segmentation models using only image-level annotations, eliminating the need for expensive pixel-level labeling. While existing methods struggle with precise object boundary localization and often focus only on the most discriminative regions, we propose IG-CAM (Instance-Guided Class Activation Mapping), a novel approach that leverages instance-level cues and influence functions to generate high-quality, boundary-aware localization maps. Our method introduces three key innovations: (1) Instance-Guided Refinement that uses ground truth segmentation masks to guide CAM generation, ensuring complete object coverage rather than just discriminative parts; (2) Influence Function Integration that captures the relationship between training samples and model predictions, leading to more robust feature representations; and (3) Multi-Scale Boundary Enhancement that employs progressive refinement strategies to achieve sharp, precise object boundaries. IG-CAM achieves state-of-the-art performance on the PASCAL VOC 2012 dataset with an mIoU of 82.3% before post-processing, which further improves to 86.6% after applying Conditional Random Field (CRF) refinement, significantly outperforming previous WSSS methods. Our approach demonstrates superior localization accuracy, with complete object coverage and precise boundary delineation, while maintaining computational efficiency. Extensive ablation studies validate the contribution of each component, and qualitative comparisons across 600 diverse images showcase the method's robustness and generalization capability. The results establish IG-CAM as a new benchmark for weakly supervised semantic segmentation, offering a practical solution for scenarios where pixel-level annotations are unavailable or prohibitively expensive.</li>
<li><strong>摘要：</strong>弱监督的语义细分（WSSS）仅使用图像级注释来解决训练分割模型的挑战，从而消除了对昂贵的像素级标签的需求。尽管现有方法在精确的对象边界定位中遇到困难，并且通常仅关注最歧视区域，但我们提出了IG-CAM（实例引导的类激活映射），这是一种利用实例级别的提示并影响功能以生成高质量，边界意识到的本地化映射的新颖方法。我们的方法介绍了三个关键创新：（1）实例指导的改进，使用地面真理分割面具来指导CAM生成，确保完整的对象覆盖范围，而不仅仅是判别零件； （2）影响函数集成，以捕获训练样本与模型预测之间的关系，从而导致更强大的特征表示； （3）采用渐进式改进策略实现尖锐，精确的对象边界的多尺度边界增强。 IG-CAM在Pascal VOC 2012数据集上实现了最先进的性能，在后处理前的MIOU为82.3％，在应用有条件的随机场（CRF）改进后进一步提高了86.6％，显着超过了先前的WSSS方法。我们的方法表明，具有完整的对象覆盖范围和精确的边界描述，同时保持了计算效率。广泛的消融研究验证了每个组件的贡献，以及600个不同图像的定性比较展示了该方法的鲁棒性和概括能力。结果将IG-CAM确定为弱监督语义细分的新基准，为像素级注释不可用或过于昂贵的场景提供了实用的解决方案。</li>
</ul>

<h3>Title: Artist-Created Mesh Generation from Raw Observation</h3>
<ul>
<li><strong>Authors: </strong>Yao He, Youngjoong Kwon, Wenxiao Cai, Ehsan Adeli</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12501">https://arxiv.org/abs/2509.12501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12501">https://arxiv.org/pdf/2509.12501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12501]] Artist-Created Mesh Generation from Raw Observation(https://arxiv.org/abs/2509.12501)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We present an end-to-end framework for generating artist-style meshes from noisy or incomplete point clouds, such as those captured by real-world sensors like LiDAR or mobile RGB-D cameras. Artist-created meshes are crucial for commercial graphics pipelines due to their compatibility with animation and texturing tools and their efficiency in rendering. However, existing approaches often assume clean, complete inputs or rely on complex multi-stage pipelines, limiting their applicability in real-world scenarios. To address this, we propose an end-to-end method that refines the input point cloud and directly produces high-quality, artist-style meshes. At the core of our approach is a novel reformulation of 3D point cloud refinement as a 2D inpainting task, enabling the use of powerful generative models. Preliminary results on the ShapeNet dataset demonstrate the promise of our framework in producing clean, complete meshes.</li>
<li><strong>摘要：</strong>我们提出了一个端到端框架，用于从嘈杂或不完整的点云中生成艺术家风格的网格，例如Lidar或Mobile RGB-D摄像头（Mobile RGB-D摄像头）捕获的框架。艺术家创建的网格对于商业图形管道至关重要，因为它们与动画和纹理工具的兼容性及其渲染效率。但是，现有方法通常假设清洁，完整的输入或依靠复杂的多阶段管道，从而限制了它们在实际情况下的适用性。为了解决这个问题，我们提出了一种端到端方法，该方法可以完善输入点云并直接产生高质量的艺术家式网格。我们方法的核心是将3D点云改进作为2D覆盖任务的新颖重新制定，从而实现了强大的生成模型。 Shapenet数据集的初步结果证明了我们在生产清洁，完整网格的框架中的希望。</li>
</ul>

<h3>Title: Adaptive Sampling Scheduler</h3>
<ul>
<li><strong>Authors: </strong>Qi Wang, Shuliang Zhu, Jinjia Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12569">https://arxiv.org/abs/2509.12569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12569">https://arxiv.org/pdf/2509.12569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12569]] Adaptive Sampling Scheduler(https://arxiv.org/abs/2509.12569)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Consistent distillation methods have evolved into effective techniques that significantly accelerate the sampling process of diffusion models. Although existing methods have achieved remarkable results, the selection of target timesteps during distillation mainly relies on deterministic or stochastic strategies, which often require sampling schedulers to be designed specifically for different distillation processes. Moreover, this pattern severely limits flexibility, thereby restricting the full sampling potential of diffusion models in practical applications. To overcome these limitations, this paper proposes an adaptive sampling scheduler that is applicable to various consistency distillation frameworks. The scheduler introduces three innovative strategies: (i) dynamic target timestep selection, which adapts to different consistency distillation frameworks by selecting timesteps based on their computed importance; (ii) Optimized alternating sampling along the solution trajectory by guiding forward denoising and backward noise addition based on the proposed time step importance, enabling more effective exploration of the solution space to enhance generation performance; and (iii) Utilization of smoothing clipping and color balancing techniques to achieve stable and high-quality generation results at high guidance scales, thereby expanding the applicability of consistency distillation models in complex generation scenarios. We validated the effectiveness and flexibility of the adaptive sampling scheduler across various consistency distillation methods through comprehensive experimental evaluations. Experimental results consistently demonstrated significant improvements in generative performance, highlighting the strong adaptability achieved by our method.</li>
<li><strong>摘要：</strong>一致的蒸馏方法已演变为有效的技术，可以显着加速扩散模型的采样过程。尽管现有方法取得了显着的结果，但蒸馏过程中目标时间段的选择主要取决于确定性或随机策略，这些策略通常需要针对不同的蒸馏过程设计采样调度程序。此外，这种模式严重限制了灵活性，从而限制了实用应用中扩散模型的完整采样潜力。为了克服这些限制，本文提出了适用于各种一致性蒸馏框架的自适应采样调度程序。调度程序介绍了三种创新策略：（i）动态目标时间段选择，该策略通过根据其计算的重要性选择时间段来适应不同的一致性蒸馏框架； （ii）根据提议的时间步长指导向前降解和向后噪声增加沿解决方案轨迹的交替采样，从而更有效地探索解决方案空间以增强发电性能； （iii）利用平滑剪辑和颜色平衡技术在高导度尺度上实现稳定和高质量的生成结果，从而在复杂的生成场景中扩大了一致性蒸馏模型的适用性。我们通过全面的实验评估验证了自适应采样调度程序在各种一致性蒸馏方法上的有效性和灵活性。实验结果始终显示出生成性能的显着改善，突出了我们方法实现的强大适应性。</li>
</ul>

<h3>Title: Exploring Spectral Characteristics for Single Image Reflection Removal</h3>
<ul>
<li><strong>Authors: </strong>Pengbo Guo, Chengxu Liu, Guoshuai Zhao, Xingsong Hou, Jialie Shen, Xueming Qian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12627">https://arxiv.org/abs/2509.12627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12627">https://arxiv.org/pdf/2509.12627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12627]] Exploring Spectral Characteristics for Single Image Reflection Removal(https://arxiv.org/abs/2509.12627)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Eliminating reflections caused by incident light interacting with reflective medium remains an ill-posed problem in the image restoration area. The primary challenge arises from the overlapping of reflection and transmission components in the captured images, which complicates the task of accurately distinguishing and recovering the clean background. Existing approaches typically address reflection removal solely in the image domain, ignoring the spectral property variations of reflected light, which hinders their ability to effectively discern reflections. In this paper, we start with a new perspective on spectral learning, and propose the Spectral Codebook to reconstruct the optical spectrum of the reflection image. The reflections can be effectively distinguished by perceiving the wavelength differences between different light sources in the spectrum. To leverage the reconstructed spectrum, we design two spectral prior refinement modules to re-distribute pixels in the spatial dimension and adaptively enhance the spectral differences along the wavelength dimension. Furthermore, we present the Spectrum-Aware Transformer to jointly recover the transmitted content in spectral and pixel domains. Experimental results on three different reflection benchmarks demonstrate the superiority and generalization ability of our method compared to state-of-the-art models.</li>
<li><strong>摘要：</strong>消除与反射培养基相互作用的事件光相互作用引起的反射仍然是图像恢复区域中的一个问题。主要的挑战是由捕获图像中反射和传输组件的重叠引起的，这使准确区分和恢复干净背景的任务变得复杂。现有方法通常仅在图像域中仅解决反射去除，而忽略了反射光的光谱属性变化，从而阻碍了它们有效辨别反射的能力。在本文中，我们从光谱学习的新观点开始，然后提出光谱代码手册，以重建反射图像的光谱。通过感知光谱中不同光源之间的波长差异，可以有效地区分这些反射。为了利用重建的光谱，我们设计了两个光谱先验的改进模块，以在空间尺寸中重新分布像素，并自适应增强沿波长尺寸的光谱差异。此外，我们介绍了光谱感知的变压器，以共同恢复光谱和像素域中的传输含量。与最新模型相比，三种不同反射基准的实验结果证明了我们方法的优越性和泛化能力。</li>
</ul>

<h3>Title: Beyond Artificial Misalignment: Detecting and Grounding Semantic-Coordinated Multimodal Manipulations</h3>
<ul>
<li><strong>Authors: </strong>Jinjie Shen, Yaxiong Wang, Lechao Cheng, Nan Pu, Zhun Zhong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12653">https://arxiv.org/abs/2509.12653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12653">https://arxiv.org/pdf/2509.12653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12653]] Beyond Artificial Misalignment: Detecting and Grounding Semantic-Coordinated Multimodal Manipulations(https://arxiv.org/abs/2509.12653)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The detection and grounding of manipulated content in multimodal data has emerged as a critical challenge in media forensics. While existing benchmarks demonstrate technical progress, they suffer from misalignment artifacts that poorly reflect real-world manipulation patterns: practical attacks typically maintain semantic consistency across modalities, whereas current datasets artificially disrupt cross-modal alignment, creating easily detectable anomalies. To bridge this gap, we pioneer the detection of semantically-coordinated manipulations where visual edits are systematically paired with semantically consistent textual descriptions. Our approach begins with constructing the first Semantic-Aligned Multimodal Manipulation (SAMM) dataset, generated through a two-stage pipeline: 1) applying state-of-the-art image manipulations, followed by 2) generation of contextually-plausible textual narratives that reinforce the visual deception. Building on this foundation, we propose a Retrieval-Augmented Manipulation Detection and Grounding (RamDG) framework. RamDG commences by harnessing external knowledge repositories to retrieve contextual evidence, which serves as the auxiliary texts and encoded together with the inputs through our image forgery grounding and deep manipulation detection modules to trace all manipulations. Extensive experiments demonstrate our framework significantly outperforms existing methods, achieving 2.06\% higher detection accuracy on SAMM compared to state-of-the-art approaches. The dataset and code are publicly available at this https URL.</li>
<li><strong>摘要：</strong>在媒体取证中，在多模式数据中对操纵内容的检测和接地已成为一个关键挑战。尽管现有的基准测试表明技术进步，但它们遭受了不符合现实的操纵模式的未对准伪像：实际攻击通常会保持跨模态的语义一致性，而当前数据集则可以人工中断跨模式的对准，从而造成了易于检测的障碍。为了弥合这一差距，我们开创了检测语义协调的操作，其中视觉编辑系统地与语义一致的文本描述进行了系统配对。我们的方法始于通过两阶段管道生成的第一个语义对准的多模式操纵（SAMM）数据集：1）应用最先进的图像操作，然后使用2）生成上下文上可行的文本叙事，从而强化了视觉欺骗。在这个基础的基础上，我们提出了一个检索调查的操纵检测和接地（RAMDG）框架。 RAMDG是通过利用外部知识存储库来检索上下文证据的开始的，该证据用作辅助文本，并通过我们的图像伪造接地和深层操纵检测模块与输入一起编码，以追踪所有操纵。广泛的实验证明了我们的框架明显优于现有方法，与最新方法相比，SAMM的检测准确性高2.06 \％。该数据集和代码在此HTTPS URL上可公开可用。</li>
</ul>

<h3>Title: Soft Graph Transformer for MIMO Detection</h3>
<ul>
<li><strong>Authors: </strong>Jiadong Hong, Lei Liu, Xinyu Bian, Wenjie Wang, Zhaoyang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12694">https://arxiv.org/abs/2509.12694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12694">https://arxiv.org/pdf/2509.12694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12694]] Soft Graph Transformer for MIMO Detection(https://arxiv.org/abs/2509.12694)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We propose the Soft Graph Transformer (SGT), a Soft-Input-Soft-Output neural architecture tailored for MIMO detection. While Maximum Likelihood (ML) detection achieves optimal accuracy, its prohibitive exponential complexity renders it impractical for real-world systems. Conventional message passing algorithms offer tractable alternatives but rely on large-system asymptotics and random matrix assumptions, both of which break down under practical implementations. Prior Transformer-based detectors, on the other hand, fail to incorporate the MIMO factor graph structure and cannot utilize decoder-side soft information, limiting their standalone performance and their applicability in iterative detection-decoding (IDD). To overcome these limitations, SGT integrates message passing directly into a graph-aware attention mechanism and supports decoder-informed updates through soft-input embeddings. This design enables effective soft-output generation while preserving computational efficiency. As a standalone detector, SGT closely approaches ML performance and surpasses prior Transformer-based approaches.</li>
<li><strong>摘要：</strong>我们提出了软图形变压器（SGT），这是一种针对MIMO检测的软输入输出输出神经架构。虽然最大似然（ML）检测可实现最佳精度，但其过度的指数复杂性使其对于现实世界系统不切实际。传统消息传递算法提供可拖动的替代方案，但依赖于大型系统的渐近学和随机矩阵假设，这两者都在实际实现下分解。另一方面，先前基于变压器的检测器未能合并MIMO因子图结构，并且无法利用解码器侧软信息，从而限制了其独立性能以及其在迭代检测解码（IDD）中的适用性。为了克服这些局限性，SGT将消息直接整合到图形感知的注意机制中，并通过软输入嵌入来支持解码器信息的更新。该设计可实现有效的软输入生成，同时保持计算效率。作为独立检测器，中士紧密接近ML性能并超过先前的基于变压器的方法。</li>
</ul>

<h3>Title: SPGen: Spherical Projection as Consistent and Flexible Representation for Single Image 3D Shape Generation</h3>
<ul>
<li><strong>Authors: </strong>Jingdong Zhang, Weikai Chen, Yuan Liu, Jionghao Wang, Zhengming Yu, Zhuowen Shen, Bo Yang, Wenping Wang, Xin Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12721">https://arxiv.org/abs/2509.12721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12721">https://arxiv.org/pdf/2509.12721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12721]] SPGen: Spherical Projection as Consistent and Flexible Representation for Single Image 3D Shape Generation(https://arxiv.org/abs/2509.12721)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Existing single-view 3D generative models typically adopt multiview diffusion priors to reconstruct object surfaces, yet they remain prone to inter-view inconsistencies and are unable to faithfully represent complex internal structure or nontrivial topologies. In particular, we encode geometry information by projecting it onto a bounding sphere and unwrapping it into a compact and structural multi-layer 2D Spherical Projection (SP) representation. Operating solely in the image domain, SPGen offers three key advantages simultaneously: (1) Consistency. The injective SP mapping encodes surface geometry with a single viewpoint which naturally eliminates view inconsistency and ambiguity; (2) Flexibility. Multi-layer SP maps represent nested internal structures and support direct lifting to watertight or open 3D surfaces; (3) Efficiency. The image-domain formulation allows the direct inheritance of powerful 2D diffusion priors and enables efficient finetuning with limited computational resources. Extensive experiments demonstrate that SPGen significantly outperforms existing baselines in geometric quality and computational efficiency.</li>
<li><strong>摘要：</strong>现有的单视3D生成模型通常采用多视图扩散先验来重建对象表面，但是它们仍然容易出现间距不一致，并且无法忠实地代表复杂的内部结构或非平凡的拓扑。特别是，我们通过将其投影到边界球体上并将其解开为紧凑而结构的多层2D球形投影（SP）表示来编码几何信息。 SPGEN仅在图像域操作，同时提供三个关键优势：（1）一致性。 Injective SP映射编码表面几何形状，具有单个视点，自然消除了视图不一致和歧义。 （2）灵活性。多层SP地图代表嵌套的内部结构，并支持直接提升至水密或开放的3D表面； （3）效率。图像域公式允许直接继承强大的2D扩散先验，并在有限的计算资源中实现有效的壁炉固定。广泛的实验表明，SPGEN在几何质量和计算效率方面显着优于现有基准。</li>
</ul>

<h3>Title: What Makes a Good Generated Image? Investigating Human and Multimodal LLM Image Preference Alignment</h3>
<ul>
<li><strong>Authors: </strong>Rishab Parthasarathy, Jasmine Collins, Cory Stephenson</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12750">https://arxiv.org/abs/2509.12750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12750">https://arxiv.org/pdf/2509.12750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12750]] What Makes a Good Generated Image? Investigating Human and Multimodal LLM Image Preference Alignment(https://arxiv.org/abs/2509.12750)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Automated evaluation of generative text-to-image models remains a challenging problem. Recent works have proposed using multimodal LLMs to judge the quality of images, but these works offer little insight into how multimodal LLMs make use of concepts relevant to humans, such as image style or composition, to generate their overall assessment. In this work, we study what attributes of an image--specifically aesthetics, lack of artifacts, anatomical accuracy, compositional correctness, object adherence, and style--are important for both LLMs and humans to make judgments on image quality. We first curate a dataset of human preferences using synthetically generated image pairs. We use inter-task correlation between each pair of image quality attributes to understand which attributes are related in making human judgments. Repeating the same analysis with LLMs, we find that the relationships between image quality attributes are much weaker. Finally, we study individual image quality attributes by generating synthetic datasets with a high degree of control for each axis. Humans are able to easily judge the quality of an image with respect to all of the specific image quality attributes (e.g. high vs. low aesthetic image), however we find that some attributes, such as anatomical accuracy, are much more difficult for multimodal LLMs to learn to judge. Taken together, these findings reveal interesting differences between how humans and multimodal LLMs perceive images.</li>
<li><strong>摘要：</strong>生成文本对图像模型的自动评估仍然是一个具有挑战性的问题。最近的作品提出了使用多模式LLM判断图像质量的作品，但是这些作品对多模式LLM如何利用与人类相关的概念（例如图像样式或组成）来生成其整体评估的概念几乎没有深入了解。在这项工作中，我们研究了图像特定美学的属性，缺乏文物，解剖学精度，构图正确性，对象依从性和样式 - 对LLMS和人类对图像质量做出判断都很重要。我们首先使用合成生成的图像对来策划人类偏好的数据集。我们使用每对图像质量属性之间的任务之间的相关性来了解哪些属性在做出人类判断时相关。通过LLM重复相同的分析，我们发现图像质量属性之间的关系要弱得多。最后，我们通过为每个轴对高度控制的合成数据集来研究个体图像质量属性。人类可以轻松地判断图像的质量相对于所有特定图像质量属性（例如，高审美图像），但是我们发现，对于多模式LLM来说，某些属性（例如解剖精度）很难学会判断。综上所述，这些发现揭示了人类和多模式LLMS感知图像之间的有趣差异。</li>
</ul>

<h3>Title: A-TDOM: Active TDOM via On-the-Fly 3DGS</h3>
<ul>
<li><strong>Authors: </strong>Yiwei Xu, Xiang Wang, Yifei Yu, Wentian Gan, Luca Morelli, Giulio Perda, Xiongwu Xiao, Zongqian Zhan, Xin Wang, Fabio Remondino</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12759">https://arxiv.org/abs/2509.12759</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12759">https://arxiv.org/pdf/2509.12759</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12759]] A-TDOM: Active TDOM via On-the-Fly 3DGS(https://arxiv.org/abs/2509.12759)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>True Digital Orthophoto Map (TDOM) serves as a crucial geospatial product in various fields such as urban management, city planning, land surveying, etc. However, traditional TDOM generation methods generally rely on a complex offline photogrammetric pipeline, resulting in delays that hinder real-time applications. Moreover, the quality of TDOM may degrade due to various challenges, such as inaccurate camera poses or Digital Surface Model (DSM) and scene occlusions. To address these challenges, this work introduces A-TDOM, a near real-time TDOM generation method based on On-the-Fly 3DGS optimization. As each image is acquired, its pose and sparse point cloud are computed via On-the-Fly SfM. Then new Gaussians are integrated and optimized into previously unseen or coarsely reconstructed regions. By integrating with orthogonal splatting, A-TDOM can render just after each update of a new 3DGS field. Initial experiments on multiple benchmarks show that the proposed A-TDOM is capable of actively rendering TDOM in near real-time, with 3DGS optimization for each new image in seconds while maintaining acceptable rendering quality and TDOM geometric accuracy.</li>
<li><strong>摘要：</strong>真正的数字矫正图（TDOR）在各个领域（例如城市管理，城市规划，土地测量等）中充当至关重要的地理空间产品。但是，传统的TDOM生成方法通常依赖于复杂的离线摄影指控管道，从而导致延迟延误，从而阻碍实时应用。此外，由于各种挑战，例如不准确的相机姿势或数字表面模型（DSM）和场景遮挡，TDOR的质量可能会降低。为了应对这些挑战，这项工作介绍了A-TDOR，这是一种基于即时3DGS优化的近乎实时的TDOR生成方法。当获取每个图像时，其姿势和稀疏点云将通过直觉的SFM计算。然后，将新高斯人整合并优化为以前看不见的或粗糙的重建区域。通过与正交裂纹集成，A-TDOM可以在每次更新新的3DGS字段之后呈现。在多个基准上进行的初始实验表明，所提出的A-TDOM能够在接近实时渲染TDOR，对每个新图像进行3DG优化，同时保持可接受的渲染质量和TTOR几何准确性。</li>
</ul>

<h3>Title: Double Helix Diffusion for Cross-Domain Anomaly Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Linchun Wu, Qin Zou, Xianbiao Qi, Bo Du, Zhongyuan Wang, Qingquan Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12787">https://arxiv.org/abs/2509.12787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12787">https://arxiv.org/pdf/2509.12787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12787]] Double Helix Diffusion for Cross-Domain Anomaly Image Generation(https://arxiv.org/abs/2509.12787)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Visual anomaly inspection is critical in manufacturing, yet hampered by the scarcity of real anomaly samples for training robust detectors. Synthetic data generation presents a viable strategy for data augmentation; however, current methods remain constrained by two principal limitations: 1) the generation of anomalies that are structurally inconsistent with the normal background, and 2) the presence of undesirable feature entanglement between synthesized images and their corresponding annotation masks, which undermines the perceptual realism of the output. This paper introduces Double Helix Diffusion (DH-Diff), a novel cross-domain generative framework designed to simultaneously synthesize high-fidelity anomaly images and their pixel-level annotation masks, explicitly addressing these challenges. DH-Diff employs a unique architecture inspired by a double helix, cycling through distinct modules for feature separation, connection, and merging. Specifically, a domain-decoupled attention mechanism mitigates feature entanglement by enhancing image and annotation features independently, and meanwhile a semantic score map alignment module ensures structural authenticity by coherently integrating anomaly foregrounds. DH-Diff offers flexible control via text prompts and optional graphical guidance. Extensive experiments demonstrate that DH-Diff significantly outperforms state-of-the-art methods in diversity and authenticity, leading to significant improvements in downstream anomaly detection performance.</li>
<li><strong>摘要：</strong>视觉异常检查对于制造至关重要，但由于实际异常样品的稀缺而阻碍了训练可靠的探测器。合成数据生成提出了可行的数据增强策略；但是，当前方法仍然受到两个主要局限性的限制：1）在结构上与正常背景不一致的异常产生，以及2）综合图像及其相应的注释蒙版之间存在不良特征纠缠的存在，这破坏了输出的感知现实主义。本文引入了双螺旋扩散（DH-DIFF），这是一种新型的跨域生成框架，旨在同时综合高保真异常图像及其像素级注释掩码，明确应对这些挑战。 DH-DIFF采用了一个独特的架构，灵感来自双螺旋，在不同的模块中骑自行车以进行特征分离，连接和合并。具体而言，域折叠的注意机制通过独立增强图像和注释特征来减轻特征纠缠，同时，语义得分图映射模块模块可以通过整合异常的前景来确保结构真实性。 DH-DIFF通过文本提示和可选的图形指南提供灵活的控制。广泛的实验表明，DH-DIFF在多样性和真实性方面显着优于最先进的方法，从而导致下游异常检测性能的显着改善。</li>
</ul>

<h3>Title: Superpixel Anything: A general object-based framework for accurate yet regular superpixel segmentation</h3>
<ul>
<li><strong>Authors: </strong>Julien Walther, Rémi Giraud, Michaël Clément</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12791">https://arxiv.org/abs/2509.12791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12791">https://arxiv.org/pdf/2509.12791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12791]] Superpixel Anything: A general object-based framework for accurate yet regular superpixel segmentation(https://arxiv.org/abs/2509.12791)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Superpixels are widely used in computer vision to simplify image representation and reduce computational complexity. While traditional methods rely on low-level features, deep learning-based approaches leverage high-level features but also tend to sacrifice regularity of superpixels to capture complex objects, leading to accurate but less interpretable segmentations. In this work, we introduce SPAM (SuperPixel Anything Model), a versatile framework for segmenting images into accurate yet regular superpixels. We train a model to extract image features for superpixel generation, and at inference, we leverage a large-scale pretrained model for semantic-agnostic segmentation to ensure that superpixels align with object masks. SPAM can handle any prior high-level segmentation, resolving uncertainty regions, and is able to interactively focus on specific objects. Comprehensive experiments demonstrate that SPAM qualitatively and quantitatively outperforms state-of-the-art methods on segmentation tasks, making it a valuable and robust tool for various applications. Code and pre-trained models are available here: this https URL.</li>
<li><strong>摘要：</strong>超级像素被广泛用于计算机视觉中，以简化图像表示并降低计算复杂性。尽管传统方法依赖于低水平的特征，但基于深度学习的方法利用高级特征，但也倾向于牺牲超像素的规律性来捕获复杂的对象，从而导致准确但不容易解释的分段。在这项工作中，我们介绍了垃圾邮件（Superpixel Athy Model），这是将图像分割为准确但常规超像素的多功能框架。我们训练模型以提取超像素生成的图像特征，并在推断时，我们利用了一个大规模预处理模型进行语义分割，以确保Superpixels与对象掩码保持一致。垃圾邮件可以处理任何先前的高级细分，解决不确定性区域，并能够交互关注特定对象。全面的实验表明，垃圾邮件在定性和定量上优于分割任务的最先进方法，使其成为各种应用程序的宝贵和强大的工具。代码和预训练的模型可在此处提供：此HTTPS URL。</li>
</ul>

<h3>Title: Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset Generation</h3>
<ul>
<li><strong>Authors: </strong>Biwen Lei, Yang Li, Xinhai Liu, Shuhui Yang, Lixin Xu, Jingwei Huang, Ruining Tang, Haohan Weng, Jian Liu, Jing Xu, Zhen Zhou, Yiling Zhu, Jiankai Xing, Jiachen Xu, Changfeng Ma, Xinhao Yan, Yunhan Yang, Chunshi Wang, Duoteng Xu, Xueqi Ma, Yuguang Chen, Jing Li, Mingxin Yang, Sheng Zhang, Yifei Feng, Xin Huang, Di Luo, Zebin He, Puhua Jiang, Changrong Hu, Zihan Qin, Shiwei Miao, Haolin Liu, Yunfei Zhao, Zeqiang Lai, Qingxiang Lin, Zibo Zhao, Kunhong Li, Xianghui Yang, Huiwen Shi, Xin Yang, Yuxuan Wang, Zebin Yao, Yihang Lian, Sicong Liu, Xintong Han, Wangchen Qin, Caisheng Ouyang, Jianyin Liu, Tianwen Yuan, Shuai Jiang, Hong Duan, Yanqi Niu, Wencong Lin, Yifu Sun, Shirui Huang, Lin Niu, Gu Gong, Guojian Xiao, Bojian Zheng, Xiang Yuan, Qi Chen, Jie Xiao, Dongyang Zheng, Xiaofeng Yang, Kai Liu, Jianchen Zhu, Lifu Wang, Qinglin Lu, Jie Liu, Liang Dong, Fan Jiang, Ruibin Chen, Lei Wang, Chao Zhang, Jiaxin Lin, Hao Zhang, Zheng Ye, Peng He, Runzhou Wu, Yinhe Wu, Jiayao Du, Jupeng Chen, Xinyue Mao, Dongyuan Guo, Yixuan Tang, Yulin Tsai, Yonghao Tan, Jiaao Yu, Junlin Yu, Keren Zhang, Yifan Li, Peng Chen, Tian Liu, Di Wang, Yuhong Liu, Linus, Jie Jiang, Zhuo Chen, Chunchao Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12815">https://arxiv.org/abs/2509.12815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12815">https://arxiv.org/pdf/2509.12815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12815]] Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset Generation(https://arxiv.org/abs/2509.12815)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The creation of high-quality 3D assets, a cornerstone of modern game development, has long been characterized by labor-intensive and specialized workflows. This paper presents Hunyuan3D Studio, an end-to-end AI-powered content creation platform designed to revolutionize the game production pipeline by automating and streamlining the generation of game-ready 3D assets. At its core, Hunyuan3D Studio integrates a suite of advanced neural modules (such as Part-level 3D Generation, Polygon Generation, Semantic UV, etc.) into a cohesive and user-friendly system. This unified framework allows for the rapid transformation of a single concept image or textual description into a fully-realized, production-quality 3D model complete with optimized geometry and high-fidelity PBR textures. We demonstrate that assets generated by Hunyuan3D Studio are not only visually compelling but also adhere to the stringent technical requirements of contemporary game engines, significantly reducing iteration time and lowering the barrier to entry for 3D content creation. By providing a seamless bridge from creative intent to technical asset, Hunyuan3D Studio represents a significant leap forward for AI-assisted workflows in game development and interactive media.</li>
<li><strong>摘要：</strong>高质量的3D资产的创建是现代游戏开发的基石，长期以来一直以劳动密集型和专业的工作流程为特征。本文介绍了Hunyuan3D Studio，这是一个端到端AI驱动的内容创建平台，旨在通过自动化和简化游戏准备就绪的3D资产来彻底改变游戏生产管道。 Hunyuan3D Studio的核心将一组高级神经模块（例如零件级3D代，多边形生成，语义紫外线等）集成到一个凝聚力和用户友好的系统中。这个统一的框架允许将单个概念图像或文本描述快速转换为完全实现的，生产质量的3D模型，并具有优化的几何形状和高保真PBR纹理。我们证明，Hunyuan3D Studio生成的资产不仅在视觉上引人注目，而且还遵守当代游戏引擎的严格技术要求，从而大大缩短了迭代时间，并降低了创建3D内容的进入障碍。通过提供从创意意图到技术资产的无缝桥梁，Hunyuan3D Studio代表了游戏开发和互动媒体中AI辅助工作流程的重大飞跃。</li>
</ul>

<h3>Title: Data Scaling Laws for Radiology Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Maximilian Ilse, Harshita Sharma, Anton Schwaighofer, Sam Bond-Taylor, Fernando Pérez-García, Olesya Melnichenko, Anne-Marie G. Sykes, Kelly K. Horst, Ashish Khandelwal, Maxwell Reynolds, Maria T. Wetscherek, Noel C. F. Codella, Javier Alvarez-Valle, Korfiatis Panagiotis, Valentina Salvatelli</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12818">https://arxiv.org/abs/2509.12818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12818">https://arxiv.org/pdf/2509.12818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12818]] Data Scaling Laws for Radiology Foundation Models(https://arxiv.org/abs/2509.12818)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Foundation vision encoders such as CLIP and DINOv2, trained on web-scale data, exhibit strong transfer performance across tasks and datasets. However, medical imaging foundation models remain constrained by smaller datasets, limiting our understanding of how data scale and pretraining paradigms affect performance in this setting. In this work, we systematically study continual pretraining of two vision encoders, MedImageInsight (MI2) and RAD-DINO representing the two major encoder paradigms CLIP and DINOv2, on up to 3.5M chest x-rays from a single institution, holding compute and evaluation protocols constant. We evaluate on classification (radiology findings, lines and tubes), segmentation (lines and tubes), and radiology report generation. While prior work has primarily focused on tasks related to radiology findings, we include lines and tubes tasks to counterbalance this bias and evaluate a model's ability to extract features that preserve continuity along elongated structures. Our experiments show that MI2 scales more effectively for finding-related tasks, while RAD-DINO is stronger on tube-related tasks. Surprisingly, continually pretraining MI2 with both reports and structured labels using UniCL improves performance, underscoring the value of structured supervision at scale. We further show that for some tasks, as few as 30k in-domain samples are sufficient to surpass open-weights foundation models. These results highlight the utility of center-specific continual pretraining, enabling medical institutions to derive significant performance gains by utilizing in-domain data.</li>
<li><strong>摘要：</strong>基金会视觉编码器（例如剪辑和Dinov2）接受了网络规模数据培训，在任务和数据集中表现出强大的转移性能。但是，医学成像基础模型仍然受到较小数据集的限制，从而限制了我们对在这种情况下如何影响数据量表和预处理范式如何影响性能的理解。在这项工作中，我们系统地研究了两种代表两个主要编码器范式剪辑和Dinov2的两个视力编码器（MI2）和Rad-Dino的持续预处理，该胸部X射线最高为350万个机构，持有一个机构，持有计算和评估协议常数。我们评估分类（放射学发现，线和管），分割（线和管）和放射学报告的生成。虽然先前的工作主要集中在与放射学发现有关的任务上，但我们包括线条和试管任务，以抵消这种偏见，并评估模型提取沿细长结构的连续性的功能的能力。我们的实验表明，MI2对与发现相关的任务更有效，而Rad-Dino在与管子相关的任务上更强。令人惊讶的是，使用UNICL的报告和结构化标签不断预处理MI2可改善性能，从而强调了结构化监督的价值。我们进一步表明，对于某些任务，只有3万30k的内域样品就足以超越开放型基础模型。这些结果突出了中心特异性持续预处理的实用性，从而使医疗机构通过利用内域数据获得了显着的性能增长。</li>
</ul>

<h3>Title: Few to Big: Prototype Expansion Network via Diffusion Learner for Point Cloud Few-shot Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Qianguang Zhao, Dongli Wang, Yan Zhou, Jianxun Li, Richard Irampa</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12878">https://arxiv.org/abs/2509.12878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12878">https://arxiv.org/pdf/2509.12878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12878]] Few to Big: Prototype Expansion Network via Diffusion Learner for Point Cloud Few-shot Semantic Segmentation(https://arxiv.org/abs/2509.12878)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Few-shot 3D point cloud semantic segmentation aims to segment novel categories using a minimal number of annotated support samples. While existing prototype-based methods have shown promise, they are constrained by two critical challenges: (1) Intra-class Diversity, where a prototype's limited representational capacity fails to cover a class's full variations, and (2) Inter-set Inconsistency, where prototypes derived from the support set are misaligned with the query feature space. Motivated by the powerful generative capability of diffusion model, we re-purpose its pre-trained conditional encoder to provide a novel source of generalizable features for expanding the prototype's representational range. Under this setup, we introduce the Prototype Expansion Network (PENet), a framework that constructs big-capacity prototypes from two complementary feature sources. PENet employs a dual-stream learner architecture: it retains a conventional fully supervised Intrinsic Learner (IL) to distill representative features, while introducing a novel Diffusion Learner (DL) to provide rich generalizable features. The resulting dual prototypes are then processed by a Prototype Assimilation Module (PAM), which adopts a novel push-pull cross-guidance attention block to iteratively align the prototypes with the query space. Furthermore, a Prototype Calibration Mechanism (PCM) regularizes the final big capacity prototype to prevent semantic drift. Extensive experiments on the S3DIS and ScanNet datasets demonstrate that PENet significantly outperforms state-of-the-art methods across various few-shot settings.</li>
<li><strong>摘要：</strong>很少有3D点云语义分段旨在使用最少数量的带注释的支持样本来细分新类别。尽管现有的基于原型的方法已显示出希望，但它们受到两个关键挑战的约束：（1）阶级内多样性，其中原型的有限代表能力无法涵盖班级的完整变化，以及（2）间间的不一致性，其中从支撑集中衍生出的原型是与Query特征空间错误地对齐的。通过强大的扩散模型的生成能力的启发，我们重新使用了其预训练的条件编码器，以提供可推广的新颖特征来扩展原型的代表性范围。在此设置下，我们介绍了原型扩展网络（PENET），该框架构建了来自两个互补特征来源的大容量原型。 Penet采用双流学习者体系结构：它保留了传统的完全监督的内在学习者（IL）来提炼代表性特征，同时引入了一种新颖的扩散学习者（DL），以提供丰富的可通用特征。然后，由原型同化模块（PAM）处理所得的双重原型，该模块（PAM）采用了一种新型的推杆交叉辅助注意力块，以迭代地将原型与查询空间保持一致。此外，原型校准机制（PCM）正规化了最终的大容量原型，以防止语义漂移。 S3DIS和SCANNET数据集的广泛实验表明，penet在各种少量设置上都显着胜过最先进的方法。</li>
</ul>

<h3>Title: Runge-Kutta Approximation and Decoupled Attention for Rectified Flow Inversion and Semantic Editing</h3>
<ul>
<li><strong>Authors: </strong>Weiming Chen, Zhihan Zhu, Yijia Wang, Zhihai He</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12888">https://arxiv.org/abs/2509.12888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12888">https://arxiv.org/pdf/2509.12888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12888]] Runge-Kutta Approximation and Decoupled Attention for Rectified Flow Inversion and Semantic Editing(https://arxiv.org/abs/2509.12888)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Rectified flow (RF) models have recently demonstrated superior generative performance compared to DDIM-based diffusion models. However, in real-world applications, they suffer from two major challenges: (1) low inversion accuracy that hinders the consistency with the source image, and (2) entangled multimodal attention in diffusion transformers, which hinders precise attention control. To address the first challenge, we propose an efficient high-order inversion method for rectified flow models based on the Runge-Kutta solver of differential equations. To tackle the second challenge, we introduce Decoupled Diffusion Transformer Attention (DDTA), a novel mechanism that disentangles text and image attention inside the multimodal diffusion transformers, enabling more precise semantic control. Extensive experiments on image reconstruction and text-guided editing tasks demonstrate that our method achieves state-of-the-art performance in terms of fidelity and editability. Code is available at this https URL.</li>
<li><strong>摘要：</strong>与基于DDIM的扩散模型相比，矫正流量（RF）模型最近表现出了出色的生成性能。但是，在实际应用中，它们面临两个主要挑战：（1）较低的反转精度阻碍与源图像的一致性，以及（2）扩散变压器中的纠缠多模式注意，这阻碍了精确的注意力控制。为了应对第一个挑战，我们提出了一种基于微分方程的runge-kutta求解器的有效的高阶方法。为了应对第二个挑战，我们引入了解耦的扩散变压器注意（DDTA），这是一种新型机制，可以将文本和图像注意力放在多模式扩散变压器内，从而实现更精确的语义控制。关于图像重建和文本指导的编辑任务的广泛实验表明，我们的方法在忠诚度和编辑性方面实现了最先进的性能。代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: AREPAS: Anomaly Detection in Fine-Grained Anatomy with Reconstruction-Based Semantic Patch-Scoring</h3>
<ul>
<li><strong>Authors: </strong>Branko Mitic, Philipp Seeböck, Helmut Prosch, Georg Langs</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12905">https://arxiv.org/abs/2509.12905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12905">https://arxiv.org/pdf/2509.12905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12905]] AREPAS: Anomaly Detection in Fine-Grained Anatomy with Reconstruction-Based Semantic Patch-Scoring(https://arxiv.org/abs/2509.12905)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Early detection of newly emerging diseases, lesion severity assessment, differentiation of medical conditions and automated screening are examples for the wide applicability and importance of anomaly detection (AD) and unsupervised segmentation in medicine. Normal fine-grained tissue variability such as present in pulmonary anatomy is a major challenge for existing generative AD methods. Here, we propose a novel generative AD approach addressing this issue. It consists of an image-to-image translation for anomaly-free reconstruction and a subsequent patch similarity scoring between observed and generated image-pairs for precise anomaly localization. We validate the new method on chest computed tomography (CT) scans for the detection and segmentation of infectious disease lesions. To assess generalizability, we evaluate the method on an ischemic stroke lesion segmentation task in T1-weighted brain MRI. Results show improved pixel-level anomaly segmentation in both chest CTs and brain MRIs, with relative DICE score improvements of +1.9% and +4.4%, respectively, compared to other state-of-the-art reconstruction-based methods.</li>
<li><strong>摘要：</strong>早期发现新出现的疾病，病变严重程度评估，医疗状况的区分和自动筛查是广泛适用性和异常检测（AD）的重要性以及在医学中无监督分段的例子。正常的细粒组织变异性（例如肺解剖学中存在）是现有生成AD方法的主要挑战。在这里，我们提出了一种解决这个问题的新颖生成广告方法。它由用于无异常重建的图像到图像翻译以及观察到的图像对和生成的图像对之间的随后贴片相似性评分，以精确的异常定位。我们验证了有关胸部计算机断层扫描（CT）扫描的新方法，以检测和分割传染病病变。为了评估概括性，我们评估了T1加权大脑MRI中缺血性中风病变分割任务的方法。结果表明，与其他基于先进的基于重建的方法相比，胸部CT和脑MRIS的像素级异常分割的改善，相对骰子得分的提高分别为 +1.9％和 +4.4％。</li>
</ul>

<h3>Title: PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era</h3>
<ul>
<li><strong>Authors: </strong>Xu Zheng, Chenfei Liao, Ziqiao Weng, Kaiyu Lei, Zihao Dongfang, Haocong He, Yuanhuiyi Lyu, Lutao Jiang, Lu Qi, Li Chen, Danda Pani Paudel, Kailun Yang, Linfeng Zhang, Luc Van Gool, Xuming Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12989">https://arxiv.org/abs/2509.12989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12989">https://arxiv.org/pdf/2509.12989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12989]] PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era(https://arxiv.org/abs/2509.12989)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Omnidirectional vision, using 360-degree vision to understand the environment, has become increasingly critical across domains like robotics, industrial inspection, and environmental monitoring. Compared to traditional pinhole vision, omnidirectional vision provides holistic environmental awareness, significantly enhancing the completeness of scene perception and the reliability of decision-making. However, foundational research in this area has historically lagged behind traditional pinhole vision. This talk presents an emerging trend in the embodied AI era: the rapid development of omnidirectional vision, driven by growing industrial demand and academic interest. We highlight recent breakthroughs in omnidirectional generation, omnidirectional perception, omnidirectional understanding, and related datasets. Drawing on insights from both academia and industry, we propose an ideal panoramic system architecture in the embodied AI era, PANORAMA, which consists of four key subsystems. Moreover, we offer in-depth opinions related to emerging trends and cross-community impacts at the intersection of panoramic vision and embodied AI, along with the future roadmap and open challenges. This overview synthesizes state-of-the-art advancements and outlines challenges and opportunities for future research in building robust, general-purpose omnidirectional AI systems in the embodied AI era.</li>
<li><strong>摘要：</strong>使用360度愿景来了解环境的全向视觉已变得越来越关键，例如机器人技术，工业检查和环境监测。与传统的针孔视觉相比，全向视觉提供了整体的环境意识，可显着提高场景感知的完整性和决策的可靠性。但是，这一领域的基础研究历来落后于传统的针孔视觉。本演讲呈现出体现的AI时代的新兴趋势：在工业需求和学术兴趣不断增长的推动下，全向视觉的快速发展。我们重点介绍了全向产生，全向感知，全向理解和相关数据集的最新突破。利用学术界和行业的见解，我们提出了体现的AI时代Panorama的理想全景体系结构，该系统由四个关键子系统组成。此外，我们提供了与新兴趋势和跨社区影响有关的深入意见，以及全景视野与体现AI的交集以及未来的路线图和开放挑战。这一概述综合了最新的进步，并概述了在体现的AI时代建立强大的通用全向AI系统方面未来研究的挑战和机会。</li>
</ul>

<h3>Title: ReTrack: Data Unlearning in Diffusion Models through Redirecting the Denoising Trajectory</h3>
<ul>
<li><strong>Authors: </strong>Qitan Shi, Cheng Jin, Jiawei Zhang, Yuantao Gu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13007">https://arxiv.org/abs/2509.13007</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13007">https://arxiv.org/pdf/2509.13007</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13007]] ReTrack: Data Unlearning in Diffusion Models through Redirecting the Denoising Trajectory(https://arxiv.org/abs/2509.13007)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models excel at generating high-quality, diverse images but suffer from training data memorization, raising critical privacy and safety concerns. Data unlearning has emerged to mitigate this issue by removing the influence of specific data without retraining from scratch. We propose ReTrack, a fast and effective data unlearning method for diffusion models. ReTrack employs importance sampling to construct a more efficient fine-tuning loss, which we approximate by retaining only dominant terms. This yields an interpretable objective that redirects denoising trajectories toward the $k$-nearest neighbors, enabling efficient unlearning while preserving generative quality. Experiments on MNIST T-Shirt, CelebA-HQ, CIFAR-10, and Stable Diffusion show that ReTrack achieves state-of-the-art performance, striking the best trade-off between unlearning strength and generation quality preservation.</li>
<li><strong>摘要：</strong>扩散模型在产生高质量，多样化的图像方面表现出色，但遭受培训数据记忆，提高关键隐私和安全问题。通过消除特定数据的影响而无需从头开始，已经出现了数据，以减轻此问题。我们提出了Retrack，这是一种扩散模型的快速有效的数据学习方法。 ReTrack采用重要的抽样来构建更有效的微调损失，我们仅保留优势术语来近似。这产生了一个可解释的目标，可以将剥离轨迹重定向到$ k $ neart的邻居，从而在保持生成质量的同时有效地学习。 MNIST T恤，Celeba-HQ，CIFAR-10和稳定扩散的实验表明，Realack可以实现最先进的性能，从而达到了未学习强度和发电质量保护之间的最佳权衡。</li>
</ul>

<h3>Title: Dream3DAvatar: Text-Controlled 3D Avatar Reconstruction from a Single Image</h3>
<ul>
<li><strong>Authors: </strong>Gaofeng Liu, Hengsen Li, Ruoyu Gao, Xuetong Li, Zhiyuan Ma, Tao Fang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13013">https://arxiv.org/abs/2509.13013</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13013">https://arxiv.org/pdf/2509.13013</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13013]] Dream3DAvatar: Text-Controlled 3D Avatar Reconstruction from a Single Image(https://arxiv.org/abs/2509.13013)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of 3D representation techniques and generative models, substantial progress has been made in reconstructing full-body 3D avatars from a single image. However, this task remains fundamentally ill-posedness due to the limited information available from monocular input, making it difficult to control the geometry and texture of occluded regions during generation. To address these challenges, we redesign the reconstruction pipeline and propose Dream3DAvatar, an efficient and text-controllable two-stage framework for 3D avatar generation. In the first stage, we develop a lightweight, adapter-enhanced multi-view generation model. Specifically, we introduce the Pose-Adapter to inject SMPL-X renderings and skeletal information into SDXL, enforcing geometric and pose consistency across views. To preserve facial identity, we incorporate ID-Adapter-G, which injects high-resolution facial features into the generation process. Additionally, we leverage BLIP2 to generate high-quality textual descriptions of the multi-view images, enhancing text-driven controllability in occluded regions. In the second stage, we design a feedforward Transformer model equipped with a multi-view feature fusion module to reconstruct high-fidelity 3D Gaussian Splat representations (3DGS) from the generated images. Furthermore, we introduce ID-Adapter-R, which utilizes a gating mechanism to effectively fuse facial features into the reconstruction process, improving high-frequency detail recovery. Extensive experiments demonstrate that our method can generate realistic, animation-ready 3D avatars without any post-processing and consistently outperforms existing baselines across multiple evaluation metrics.</li>
<li><strong>摘要：</strong>随着3D表示技术和生成模型的快速发展，在从单个图像中重建全身3D化身方面已取得了很大的进步。但是，由于单眼输入可获得的有限信息，因此该任务基本上仍然是不良性，因此很难控制一代中遮挡区域的几何形状和纹理。为了应对这些挑战，我们重新设计了重建管道，并提出了Dream3Davatar，这是3D Avatar生成的有效且可控制的两阶段框架。在第一阶段，我们开发了一种轻巧的，适配器增强的多视图生成模型。具体而言，我们将姿势适应器介绍给SMPL-X渲染和骨骼信息，将SDXL注入SDXL，在视图上执行几何和姿势一致性。为了保持面部身份，我们合并了ID-ADAPTER-G，该ID-ADAPTER-G将高分辨率的面部特征注入生成过程。此外，我们利用Blip2生成多视图图像的高质量文本描述，从而增强了闭塞区域中文本驱动的可控性。在第二阶段，我们设计了一个配备了多视图特征融合模块的进发液变压器模型，可从生成的图像重建高保真3D高斯SPLAT表示（3DGS）。此外，我们介绍了ID-ADAPTER-R，它利用门控机制将面部特征有效地融合到重建过程中，从而改善了高频细节恢复。广泛的实验表明，我们的方法可以生成现实的，动画就绪的3D化身，而无需任何后处理，并且始终超过多个评估指标的现有基线。</li>
</ul>

<h3>Title: Enhancing Dual Network Based Semi-Supervised Medical Image Segmentation with Uncertainty-Guided Pseudo-Labeling</h3>
<ul>
<li><strong>Authors: </strong>Yunyao Lu, Yihang Wu, Ahmad Chaddad, Tareef Daqqaq, Reem Kateb</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13084">https://arxiv.org/abs/2509.13084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13084">https://arxiv.org/pdf/2509.13084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13084]] Enhancing Dual Network Based Semi-Supervised Medical Image Segmentation with Uncertainty-Guided Pseudo-Labeling(https://arxiv.org/abs/2509.13084)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite the remarkable performance of supervised medical image segmentation models, relying on a large amount of labeled data is impractical in real-world situations. Semi-supervised learning approaches aim to alleviate this challenge using unlabeled data through pseudo-label generation. Yet, existing semi-supervised segmentation methods still suffer from noisy pseudo-labels and insufficient supervision within the feature space. To solve these challenges, this paper proposes a novel semi-supervised 3D medical image segmentation framework based on a dual-network architecture. Specifically, we investigate a Cross Consistency Enhancement module using both cross pseudo and entropy-filtered supervision to reduce the noisy pseudo-labels, while we design a dynamic weighting strategy to adjust the contributions of pseudo-labels using an uncertainty-aware mechanism (i.e., Kullback-Leibler divergence). In addition, we use a self-supervised contrastive learning mechanism to align uncertain voxel features with reliable class prototypes by effectively differentiating between trustworthy and uncertain predictions, thus reducing prediction uncertainty. Extensive experiments are conducted on three 3D segmentation datasets, Left Atrial, NIH Pancreas and BraTS-2019. The proposed approach consistently exhibits superior performance across various settings (e.g., 89.95\% Dice score on left Atrial with 10\% labeled data) compared to the state-of-the-art methods. Furthermore, the usefulness of the proposed modules is further validated via ablation experiments.</li>
<li><strong>摘要：</strong>尽管有监督的医学图像分割模型的表现出色，但在现实情况下，依赖大量标记的数据是不切实际的。半监督的学习方法旨在通过伪标签生成使用未标记的数据来缓解这一挑战。然而，现有的半监督分割方法仍然遭受嘈杂的伪标签和特征空间内的监督不足。为了解决这些挑战，本文提出了一个基于双网络体系结构的新型半监督3D医学图像分割框架。具体而言，我们使用交叉伪和熵过滤的监督研究了交叉一致性增强模块，以减少嘈杂的伪标签，同时我们设计了动态的加权策略，以使用不确定性意识的机制（即Kullback-Beleibler-Lebler divergence）调整伪标签的贡献。此外，我们使用一种自我监视的对比学习机制来使不确定的体素特征与可靠的类原型相结合，通过有效地区分可信赖和不确定的预测，从而减少了预测不确定性。在三个3D分割数据集上进行了广泛的实验，左心房，NIH胰腺和Brats-2019。与最先进的方法相比，所提出的方法始终在各种环境中表现出卓越的性能（例如，左心房的89.95 \％骰子得分，具有10 \％标记的数据）。此外，通过消融实验进一步验证了所提出的模块的有用性。</li>
</ul>

<h3>Title: A Synthetic Data Pipeline for Supporting Manufacturing SMEs in Visual Assembly Control</h3>
<ul>
<li><strong>Authors: </strong>Jonas Werheid, Shengjie He, Aymen Gannouni, Anas Abdelrazeq, Robert H. Schmitt</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13089">https://arxiv.org/abs/2509.13089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13089">https://arxiv.org/pdf/2509.13089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13089]] A Synthetic Data Pipeline for Supporting Manufacturing SMEs in Visual Assembly Control(https://arxiv.org/abs/2509.13089)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Quality control of assembly processes is essential in manufacturing to ensure not only the quality of individual components but also their proper integration into the final product. To assist in this matter, automated assembly control using computer vision methods has been widely implemented. However, the costs associated with image acquisition, annotation, and training of computer vision algorithms pose challenges for integration, especially for small- and medium-sized enterprises (SMEs), which often lack the resources for extensive training, data collection, and manual image annotation. Synthetic data offers the potential to reduce manual data collection and labeling. Nevertheless, its practical application in the context of assembly quality remains limited. In this work, we present a novel approach for easily integrable and data-efficient visual assembly control. Our approach leverages simulated scene generation based on computer-aided design (CAD) data and object detection algorithms. The results demonstrate a time-saving pipeline for generating image data in manufacturing environments, achieving a mean Average Precision (mAP@0.5:0.95) up to 99,5% for correctly identifying instances of synthetic planetary gear system components within our simulated training data, and up to 93% when transferred to real-world camera-captured testing data. This research highlights the effectiveness of synthetic data generation within an adaptable pipeline and underscores its potential to support SMEs in implementing resource-efficient visual assembly control solutions.</li>
<li><strong>摘要：</strong>组装过程的质量控制对于制造过程至关重要，不仅要确保单个组件的质量，而且要确保其与最终产品的正确集成。为了协助此事，已广泛实施了使用计算机视觉方法的自动装配控制。但是，与图像获取，注释和培训计算机视觉算法相关的成本构成了集成的挑战，尤其是对于中小型企业（SME），通常缺乏广泛培训，数据收集和手动图像注释的资源。合成数据提供了减少手动数据收集和标签的潜力。然而，在组装质量的背景下，其实际应用仍然有限。在这项工作中，我们提出了一种新颖的方法，以易于集成和数据有效的视觉组装控制。我们的方法利用基于计算机辅助设计（CAD）数据和对象检测算法的模拟场景生成。结果证明了一条用于在制造环境中生成图像数据的省时管道，达到平均平均精度（map@0.5：0.95），最高99.5％，以正确识别我们模拟的训练数据中合成行星齿轮系统组件的正确识别，当转移到现实世界中摄像机摄像机摄像机测试的测试数据时，最高可达93％。这项研究强调了合成数据在适应性管道中的有效性，并强调了其在实施资源有效的视觉组装控制解决方案方面支持中小企业的潜力。</li>
</ul>

<h3>Title: MSDNet: Efficient 4D Radar Super-Resolution via Multi-Stage Distillation</h3>
<ul>
<li><strong>Authors: </strong>Minqing Huang, Shouyi Lu, Boyuan Zheng, Ziyao Li, Xiao Tang, Guirong Zhuo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13149">https://arxiv.org/abs/2509.13149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13149">https://arxiv.org/pdf/2509.13149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13149]] MSDNet: Efficient 4D Radar Super-Resolution via Multi-Stage Distillation(https://arxiv.org/abs/2509.13149)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>4D radar super-resolution, which aims to reconstruct sparse and noisy point clouds into dense and geometrically consistent representations, is a foundational problem in autonomous perception. However, existing methods often suffer from high training cost or rely on complex diffusion-based sampling, resulting in high inference latency and poor generalization, making it difficult to balance accuracy and efficiency. To address these limitations, we propose MSDNet, a multi-stage distillation framework that efficiently transfers dense LiDAR priors to 4D radar features to achieve both high reconstruction quality and computational efficiency. The first stage performs reconstruction-guided feature distillation, aligning and densifying the student's features through feature reconstruction. In the second stage, we propose diffusion-guided feature distillation, which treats the stage-one distilled features as a noisy version of the teacher's representations and refines them via a lightweight diffusion network. Furthermore, we introduce a noise adapter that adaptively aligns the noise level of the feature with a predefined diffusion timestep, enabling a more precise denoising. Extensive experiments on the VoD and in-house datasets demonstrate that MSDNet achieves both high-fidelity reconstruction and low-latency inference in the task of 4D radar point cloud super-resolution, and consistently improves performance on downstream tasks. The code will be publicly available upon publication.</li>
<li><strong>摘要：</strong>4D雷达超分辨率旨在将稀疏和嘈杂的点云重建为密集且几何一致的表示，是自主感知中的基本问题。但是，现有方法通常会遭受高训练成本或依赖基于复杂的基于扩散的采样，从而导致推理潜伏期高和泛化不良，从而使准确性和效率保持平衡。为了解决这些局限性，我们提出了MSDNET，这是一个多阶段蒸馏框架，可有效地将密集的LiDar先验转移到4D雷达特征，以达到高重建质量和计算效率。第一阶段执行重建引导的特征蒸馏，通过功能重建来对齐和致密学生的特征。在第二阶段，我们提出了扩散引导的特征蒸馏，它将阶段的蒸馏特征视为教师表示的嘈杂版本，并通过轻量级扩散网络来完善它们。此外，我们引入了一个噪声适配器，该噪声适配器可与预定义的扩散时间段适应性地对齐该特征的噪声水平，从而更精确地降解。在VOD和内部数据集上进行的大量实验表明，MSDNET在4D雷达点云超级分辨率的任务中既可以实现高保真重建和低延迟推断，又可以始终提高下游任务的性能。该代码将在出版后公开使用。</li>
</ul>

<h3>Title: On the Correlation between Individual Fairness and Predictive Accuracy in Probabilistic Models</h3>
<ul>
<li><strong>Authors: </strong>Alessandro Antonucci, Eric Rossetto, Ivan Duvnjak</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13165">https://arxiv.org/abs/2509.13165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13165">https://arxiv.org/pdf/2509.13165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13165]] On the Correlation between Individual Fairness and Predictive Accuracy in Probabilistic Models(https://arxiv.org/abs/2509.13165)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We investigate individual fairness in generative probabilistic classifiers by analysing the robustness of posterior inferences to perturbations in private features. Building on established results in robustness analysis, we hypothesise a correlation between robustness and predictive accuracy, specifically, instances exhibiting greater robustness are more likely to be classified accurately. We empirically assess this hypothesis using a benchmark of fourteen datasets with fairness concerns, employing Bayesian networks as the underlying generative models. To address the computational complexity associated with robustness analysis over multiple private features with Bayesian networks, we reformulate the problem as a most probable explanation task in an auxiliary Markov random field. Our experiments confirm the hypothesis about the correlation, suggesting novel directions to mitigate the traditional trade-off between fairness and accuracy.</li>
<li><strong>摘要：</strong>我们通过分析后验推断对私人特征的扰动的鲁棒性来研究生成概率分类器中的个人公平性。在既定的鲁棒性分析结果基础上，我们假设鲁棒性与预测精度之间存在相关性，特别是，表现出更大鲁棒性的实例更有可能被准确地分类。我们使用贝叶斯网络作为基本生成模型的十四个数据集的基准来凭经验评估这一假设。为了解决与贝叶斯网络在多个私人特征上与鲁棒性分析相关的计算复杂性，我们将问题重新制定为辅助马尔可夫随机字段中最可能的解释任务。我们的实验证实了有关相关性的假设，这表明了减轻公平和准确性之间传统权衡的新方向。</li>
</ul>

<h3>Title: End4: End-to-end Denoising Diffusion for Diffusion-Based Inpainting Detection</h3>
<ul>
<li><strong>Authors: </strong>Fei Wang, Xuecheng Wu, Zheng Zhang, Danlei Huang, Yuheng Huang, BoWang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13214">https://arxiv.org/abs/2509.13214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13214">https://arxiv.org/pdf/2509.13214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13214]] End4: End-to-end Denoising Diffusion for Diffusion-Based Inpainting Detection(https://arxiv.org/abs/2509.13214)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The powerful generative capabilities of diffusion models have significantly advanced the field of image synthesis, enhancing both full image generation and inpainting-based image editing. Despite their remarkable advancements, diffusion models also raise concerns about potential misuse for malicious purposes. However, existing approaches struggle to identify images generated by diffusion-based inpainting models, even when similar inpainted images are included in their training data. To address this challenge, we propose a novel detection method based on End-to-end denoising diffusion (End4). Specifically, End4 designs a denoising reconstruction model to improve the alignment degree between the latent spaces of the reconstruction and detection processes, thus reconstructing features that are more conducive to detection. Meanwhile, it leverages a Scale-aware Pyramid-like Fusion Module (SPFM) that refines local image features under the guidance of attention pyramid layers at different scales, enhancing feature discriminability. Additionally, to evaluate detection performance on inpainted images, we establish a comprehensive benchmark comprising images generated from five distinct masked regions. Extensive experiments demonstrate that our End4 effectively generalizes to unseen masking patterns and remains robust under various perturbations. Our code and dataset will be released soon.</li>
<li><strong>摘要：</strong>扩散模型的强大生成能力已显着提高了图像合成领域，从而增强了完整的图像生成和基于介入​​的图像编辑。尽管取得了显着的进步，但扩散模型也引起了人们对恶意目的潜在滥用的担忧。但是，现有的方法难以识别由基于扩散的镶嵌模型产生的图像，即使在其训练数据中包括类似的涂有图像的图像。为了应对这一挑战，我们提出了一种基于端到端去胶扩散（END4）的新型检测方法。具体而言，End4设计了一个剥夺重建模型，以提高重建和检测过程的潜在空间之间的比对度，从而重建更有利于检测的功能。同时，它利用了量表感知的金字塔样融合模块（SPFM），该模块（SPFM）在注意力金字塔层在不同尺度的指导下完善了本地图像特征，从而增强了特征可区分性。此外，为了评估在原始图像上的检测性能，我们建立了一个综合基准，该基准包括由五个不同的蒙版区域产生的图像。广泛的实验表明，我们的End4有效地概括了看不见的掩蔽模式，并且在各种扰动下保持强大。我们的代码和数据集将很快发布。</li>
</ul>

<h3>Title: Single-stream Policy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Zhongwen Xu, Zihan Ding</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13232">https://arxiv.org/abs/2509.13232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13232">https://arxiv.org/pdf/2509.13232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13232]] Single-stream Policy Optimization(https://arxiv.org/abs/2509.13232)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We revisit policy-gradient optimization for Large Language Models (LLMs) from a single-stream perspective. Prevailing group-based methods like GRPO reduce variance with on-the-fly baselines but suffer from critical flaws: frequent degenerate groups erase learning signals, and synchronization barriers hinder scalability. We introduce Single-stream Policy Optimization (SPO), which eliminates these issues by design. SPO replaces per-group baselines with a persistent, KL-adaptive value tracker and normalizes advantages globally across the batch, providing a stable, low-variance learning signal for every sample. Being group-free, SPO enables higher throughput and scales effectively in long-horizon or tool-integrated settings where generation times vary. Furthermore, the persistent value tracker naturally enables an adaptive curriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO converges more smoothly and attains higher accuracy than GRPO, while eliminating computation wasted on degenerate groups. Ablation studies confirm that SPO's gains stem from its principled approach to baseline estimation and advantage normalization, offering a more robust and efficient path for LLM reasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the average maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial absolute point gains on challenging datasets, including +7.3 pp on BRUMO 25, +4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain in pass@$k$ across the evaluated $k$ values. SPO's success challenges the prevailing trend of adding incidental complexity to RL algorithms, highlighting a path where fundamental principles, not architectural workarounds, drive the next wave of progress in LLM reasoning.</li>
<li><strong>摘要：</strong>我们从单流的角度重新审视针对大语言模型（LLMS）的政策颁奖典礼优化。诸如GRPO之类的基于群体的主要方法降低了与现有基线的差异，但患有关键缺陷：频繁的退化组擦除学习信号和同步障碍会阻碍可扩展性。我们引入了单流策略优化（SPO），该策略优化通过设计消除了这些问题。 SPO用持续的KL自适应值跟踪器替代了每组基线，并在整个批处理中均具有归一化的优势，为每个样本提供了一个稳定的低变化学习信号。 SPO无组，可以在长期或工具集成的设置中有效地逐渐缩放和尺度，而生成时间有所不同。此外，持久的值跟踪器自然可以通过优先采样来实现自适应课程。使用QWEN3-8B进行的实验表明，SPO的收敛性更高，并且比GRPO更高的精度，同时消除了在退化组上浪费的计算。消融研究证实，SPO的收益源于其原则上的基线估计方法和优势归一化的方法，为LLM推理提供了更强大，更有效的路径。 Across five hard math benchmarks with Qwen3 8B, SPO improves the average maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial absolute point gains on challenging datasets, including +7.3 pp on BRUMO 25, +4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain in pass@$k$ across the evaluated $k$ values. SPO的成功挑战了为RL算法增加偶然复杂性的主要趋势，突出了一条道路，即基本原理而不是建筑解决方法，推动了LLM推理中的下一步进步。</li>
</ul>

<h3>Title: Don't Forget the Nonlinearity: Unlocking Activation Functions in Efficient Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Bo Yin, Xingyi Yang, Xinchao Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13240">https://arxiv.org/abs/2509.13240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13240">https://arxiv.org/pdf/2509.13240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13240]] Don't Forget the Nonlinearity: Unlocking Activation Functions in Efficient Fine-Tuning(https://arxiv.org/abs/2509.13240)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Existing parameter-efficient fine-tuning (PEFT) methods primarily adapt weight matrices while keeping activation functions fixed. We introduce \textbf{NoRA}, the first PEFT framework that directly adapts nonlinear activation functions in pretrained transformer-based models. NoRA replaces fixed activations with learnable rational functions and applies structured low-rank updates to numerator and denominator coefficients, with a group-wise design that localizes adaptation and improves stability at minimal cost. On vision transformers trained on CIFAR-10 and CIFAR-100, NoRA matches or exceeds full fine-tuning while updating only 0.4\% of parameters (0.02M), achieving accuracy gains of +0.17\% and +0.27\%. When combined with LoRA (\textbf{NoRA++}), it outperforms LoRA and DoRA under matched training budgets by adding fewer trainable parameters. On LLaMA3-8B instruction tuning, NoRA++ consistently improves generation quality, yielding average MMLU gains of +0.3\%--0.8\%, including +1.6\% on STEM (Alpaca) and +1.3\% on OpenOrca. We further show that NoRA constrains adaptation to a low-dimensional functional subspace, implicitly regularizing update magnitude and direction. These results establish activation-space tuning as a complementary and highly parameter-efficient alternative to weight-based PEFT, positioning activation functions as first-class objects for model adaptation.</li>
<li><strong>摘要：</strong>现有的参数效率微调（PEFT）方法主要调整重量矩阵，同时保持激活功能固定。我们介绍了\ textbf {nora}，这是第一个直接适应基于变压器的模型中非线性激活函数的PEFT框架。 NORA用可学习的有理功能代替了固定的激活，并将结构化的低级更新应用于分子和分母系数，并采用小组设计的设计来定位适应性并以最低的成本提高稳定性。在接受CIFAR-10和CIFAR-100训练的视觉变压器上，NORA匹配或超过完整的微调，同时仅更新0.4％的参数（0.021万），可实现+0.17 \％\％和+0.27 \％的准确性提高。当与Lora（\ textbf {Nora ++}）结合使用时，它通过添加较少的可训练参数来优于洛拉和多拉。在LLAMA3-8B指令调整中，NORA ++始终提高发电质量，平均MMLU增长+0.3 \％-0.8 \％，包括STEM（Alpaca）的+1.6 \％和Openorca的+1.3 \％。我们进一步表明，NORA将适应性限制为低维功能子空间，隐含地正规化更新幅度和方向。这些结果将激活空间调谐作为基于权重的PEFT的互补和高度参数效率的替代品，将激活功能定位为模型适应的一流对象。</li>
</ul>

<h3>Title: ResidualViT for Efficient Temporally Dense Video Encoding</h3>
<ul>
<li><strong>Authors: </strong>Mattia Soldan, Fabian Caba Heilbron, Bernard Ghanem, Josef Sivic, Bryan Russell</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.IR, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13255">https://arxiv.org/abs/2509.13255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13255">https://arxiv.org/pdf/2509.13255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13255]] ResidualViT for Efficient Temporally Dense Video Encoding(https://arxiv.org/abs/2509.13255)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Several video understanding tasks, such as natural language temporal video grounding, temporal activity localization, and audio description generation, require "temporally dense" reasoning over frames sampled at high temporal resolution. However, computing frame-level features for these tasks is computationally expensive given the temporal resolution requirements. In this paper, we make three contributions to reduce the cost of computing features for temporally dense tasks. First, we introduce a vision transformer (ViT) architecture, dubbed ResidualViT, that leverages the large temporal redundancy in videos to efficiently compute temporally dense frame-level features. Our architecture incorporates (i) learnable residual connections that ensure temporal consistency across consecutive frames and (ii) a token reduction module that enhances processing speed by selectively discarding temporally redundant information while reusing weights of a pretrained foundation model. Second, we propose a lightweight distillation strategy to approximate the frame-level features of the original foundation model. Finally, we evaluate our approach across four tasks and five datasets, in both zero-shot and fully supervised settings, demonstrating significant reductions in computational cost (up to 60%) and improvements in inference speed (up to 2.5x faster), all while closely approximating the accuracy of the original foundation model.</li>
<li><strong>摘要：</strong>几个视频理解任务，例如自然语言时间视频接地，时间活动定位和音频描述生成，都需要在高时间分辨率下采样的框架上“暂时密集”的推理。但是，考虑到时间分辨率要求，这些任务的计算框架级特征在计算上昂贵。在本文中，我们做出了三项贡献，以降低时间密集任务的计算功能成本。首先，我们介绍了一个称为“残留”的视觉变压器（VIT）体系结构，该体系结构利用视频中的较大时间冗余，以有效地计算时间密集的框架级特征。我们的架构结合了（i）可学习的残差连接，可确保连续帧的时间一致性和（ii）一个令牌减少模块，通过选择性地丢弃时间多余的信息，同时重复预审预周化的基础模型，从而提高了处理速度。其次，我们提出了一种轻巧的蒸馏策略，以近似原始基础模型的框架级特征。最后，我们在零射门和完全监督的设置中评估了四个任务和五个数据集的方法，这表明计算成本（最高60％）的大幅降低，并提高了推理速度（更快的速度2.5倍），同时紧密近似于原始基础模型的准确性。</li>
</ul>

<h3>Title: JANUS: A Dual-Constraint Generative Framework for Stealthy Node Injection Attacks</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Zhang, Xiaobing Pei, Zhaokun Zhong, Wenqiang Hao, Zhenghao Tang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13266">https://arxiv.org/abs/2509.13266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13266">https://arxiv.org/pdf/2509.13266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13266]] JANUS: A Dual-Constraint Generative Framework for Stealthy Node Injection Attacks(https://arxiv.org/abs/2509.13266)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have demonstrated remarkable performance across various applications, yet they are vulnerable to sophisticated adversarial attacks, particularly node injection attacks. The success of such attacks heavily relies on their stealthiness, the ability to blend in with the original graph and evade detection. However, existing methods often achieve stealthiness by relying on indirect proxy metrics, lacking consideration for the fundamental characteristics of the injected content, or focusing only on imitating local structures, which leads to the problem of local myopia. To overcome these limitations, we propose a dual-constraint stealthy node injection framework, called Joint Alignment of Nodal and Universal Structures (JANUS). At the local level, we introduce a local feature manifold alignment strategy to achieve geometric consistency in the feature space. At the global level, we incorporate structured latent variables and maximize the mutual information with the generated structures, ensuring the injected structures are consistent with the semantic patterns of the original graph. We model the injection attack as a sequential decision process, which is optimized by a reinforcement learning agent. Experiments on multiple standard datasets demonstrate that the JANUS framework significantly outperforms existing methods in terms of both attack effectiveness and stealthiness.</li>
<li><strong>摘要：</strong>图形神经网络（GNN）在各种应用中都表现出了出色的性能，但它们容易受到复杂的对抗攻击，尤其是节点注射攻击。这种攻击的成功在很大程度上取决于它们的隐身性，即与原始图和逃避检测融合的能力。但是，现有方法通常通过依靠间接代理指标来实现隐身性，缺乏对注射内容的基本特征的考虑，或者仅专注于模仿局部结构，这导致了局部近视问题。为了克服这些局限性，我们提出了一个双重约束隐形淋巴结注射框架，称为淋巴结和通用结构（Janus）。在本地层面，我们引入了局部特征歧管策略，以实现特征空间中的几何一致性。在全球层面，我们将结构化的潜在变量结合在一起，并将相互信息与生成的结构最大化，以确保注入的结构与原始图的语义模式一致。我们将注射攻击作为一个顺序决策过程进行建模，该过程由加固学习代理进行了优化。多个标准数据集的实验表明，Janus框架在攻击效率和隐形方面都显着优于现有方法。</li>
</ul>

<h3>Title: RadGame: An AI-Powered Platform for Radiology Education</h3>
<ul>
<li><strong>Authors: </strong>Mohammed Baharoon, Siavash Raissi, John S. Jun, Thibault Heintz, Mahmoud Alabbad, Ali Alburkani, Sung Eun Kim, Kent Kleinschmidt, Abdulrahman O. Alhumaydhi, Mohannad Mohammed G. Alghamdi, Jeremy Francis Palacio, Mohammed Bukhaytan, Noah Michael Prudlo, Rithvik Akula, Brady Chrisler, Benjamin Galligos, Mohammed O. Almutairi, Mazeen Mohammed Alanazi, Nasser M. Alrashdi, Joel Jihwan Hwang, Sri Sai Dinesh Jaliparthi, Luke David Nelson, Nathaniel Nguyen, Sathvik Suryadevara, Steven Kim, Mohammed F. Mohammed, Yevgeniy R. Semenov, Kun-Hsing Yu, Abdulrhman Aljouie, Hassan AlOmaish, Adam Rodman, Pranav Rajpurkar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13270">https://arxiv.org/abs/2509.13270</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13270">https://arxiv.org/pdf/2509.13270</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13270]] RadGame: An AI-Powered Platform for Radiology Education(https://arxiv.org/abs/2509.13270)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce RadGame, an AI-powered gamified platform for radiology education that targets two core skills: localizing findings and generating reports. Traditional radiology training is based on passive exposure to cases or active practice with real-time input from supervising radiologists, limiting opportunities for immediate and scalable feedback. RadGame addresses this gap by combining gamification with large-scale public datasets and automated, AI-driven feedback that provides clear, structured guidance to human learners. In RadGame Localize, players draw bounding boxes around abnormalities, which are automatically compared to radiologist-drawn annotations from public datasets, and visual explanations are generated by vision-language models for user missed findings. In RadGame Report, players compose findings given a chest X-ray, patient age and indication, and receive structured AI feedback based on radiology report generation metrics, highlighting errors and omissions compared to a radiologist's written ground truth report from public datasets, producing a final performance and style score. In a prospective evaluation, participants using RadGame achieved a 68% improvement in localization accuracy compared to 17% with traditional passive methods and a 31% improvement in report-writing accuracy compared to 4% with traditional methods after seeing the same cases. RadGame highlights the potential of AI-driven gamification to deliver scalable, feedback-rich radiology training and reimagines the application of medical AI resources in education.</li>
<li><strong>摘要：</strong>我们介绍了Radgame，这是一个针对AI驱动的放射学教育的游戏平台，该平台针对两种核心技能：本地化发现和生成报告。传统的放射学培训是基于被动暴露于案件或积极练习的基础，并从监督放射科医生的实时输入中限制了立即和可扩展反馈的机会。 Radgame通过将游戏化与大型公共数据集和自动化的AI驱动反馈相结合来解决这一差距，从而为人类学习者提供了清晰的结构化指导。在Radgame本地化中，玩家绘制了围绕异常的边界框，将其与公共数据集的放射科医生绘制的注释自动进行比较，并且视觉解释是由视觉语言模型生成的，用于用户错过的发现。在Radgame报告中，玩家根据胸部X射线，患者的年龄和指示构成了发现，并根据放射学报告的生成指标接收结构化的AI反馈，与放射线医生在公共数据集中的书面地面真相报告相比，突出了错误和遗漏，并产生了最终表现和样式得分。在预期的评估中，使用RadGame的参与者的本地化准确性提高了68％，而传统的被动方法为17％，报告写作精度提高了31％，而在看到相同案例后使用传统方法的4％，与4％相比提高了本地化精度。 Radgame强调了AI驱动的游戏化提供可扩展的，反馈丰富的放射学培训的潜力，并重新构想了医学AI资源在教育中的应用。</li>
</ul>

<h3>Title: Image Realness Assessment and Localization with Multimodal Features</h3>
<ul>
<li><strong>Authors: </strong>Lovish Kaushik, Agnij Biswas, Somdyuti Paul</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13289">https://arxiv.org/abs/2509.13289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13289">https://arxiv.org/pdf/2509.13289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13289]] Image Realness Assessment and Localization with Multimodal Features(https://arxiv.org/abs/2509.13289)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>A reliable method of quantifying the perceptual realness of AI-generated images and identifying visually inconsistent regions is crucial for practical use of AI-generated images and for improving photorealism of generative AI via realness feedback during training. This paper introduces a framework that accomplishes both overall objective realness assessment and local inconsistency identification of AI-generated images using textual descriptions of visual inconsistencies generated by vision-language models trained on large datasets that serve as reliable substitutes for human annotations. Our results demonstrate that the proposed multimodal approach improves objective realness prediction performance and produces dense realness maps that effectively distinguish between realistic and unrealistic spatial regions.</li>
<li><strong>摘要：</strong>量化AI生成图像的感知现实性并识别视觉上不一致区域的可靠方法对于实际使用AI生成的图像以及在训练过程中通过训练中的真实反馈来改善生成AI的光真相至关重要。本文介绍了一个框架，该框架使用文本描述是通过在大型数据集中训练的视觉模型产生的视觉不一致的文本描述，可以完成整体客观的现实评估和局部不一致的识别，以作为人类注释可靠的替代者而产生的视觉不一致。我们的结果表明，所提出的多模式方法改善了客观的现实性预测性能，并产生了有效区分现实和不切实际的空间区域的密集现实图。</li>
</ul>

<h3>Title: StyleSculptor: Zero-Shot Style-Controllable 3D Asset Generation with Texture-Geometry Dual Guidance</h3>
<ul>
<li><strong>Authors: </strong>Zefan Qu, Zhenwei Wang, Haoyuan Wang, Ke Xu, Gerhard Hancke, Rynson W.H. Lau</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13301">https://arxiv.org/abs/2509.13301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13301">https://arxiv.org/pdf/2509.13301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13301]] StyleSculptor: Zero-Shot Style-Controllable 3D Asset Generation with Texture-Geometry Dual Guidance(https://arxiv.org/abs/2509.13301)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Creating 3D assets that follow the texture and geometry style of existing ones is often desirable or even inevitable in practical applications like video gaming and virtual reality. While impressive progress has been made in generating 3D objects from text or images, creating style-controllable 3D assets remains a complex and challenging problem. In this work, we propose StyleSculptor, a novel training-free approach for generating style-guided 3D assets from a content image and one or more style images. Unlike previous works, StyleSculptor achieves style-guided 3D generation in a zero-shot manner, enabling fine-grained 3D style control that captures the texture, geometry, or both styles of user-provided style images. At the core of StyleSculptor is a novel Style Disentangled Attention (SD-Attn) module, which establishes a dynamic interaction between the input content image and style image for style-guided 3D asset generation via a cross-3D attention mechanism, enabling stable feature fusion and effective style-guided generation. To alleviate semantic content leakage, we also introduce a style-disentangled feature selection strategy within the SD-Attn module, which leverages the variance of 3D feature patches to disentangle style- and content-significant channels, allowing selective feature injection within the attention framework. With SD-Attn, the network can dynamically compute texture-, geometry-, or both-guided features to steer the 3D generation process. Built upon this, we further propose the Style Guided Control (SGC) mechanism, which enables exclusive geometry- or texture-only stylization, as well as adjustable style intensity control. Extensive experiments demonstrate that StyleSculptor outperforms existing baseline methods in producing high-fidelity 3D assets.</li>
<li><strong>摘要：</strong>在视频游戏和虚拟现实等实用应用中，创建遵循现有质地和几何样式的3D资产通常是必需的，甚至是不可避免的。尽管从文本或图像生成3D对象方面已经取得了令人印象深刻的进展，但创建可控制风格的3D资产仍然是一个复杂且具有挑战性的问题。在这项工作中，我们提出了Stylesculptor，这是一种从内容图像和一个或多个样式图像中生成样式引导的3D资产的新型方法。与以前的作品不同，Stylesculptor以零拍的方式实现了样式引导的3D生成，从而实现了捕获纹理，几何形状或用户提供的样式图像的两种样式的细​​粒3D样式控制。 Stylesculptor的核心是一种新型的样式散开注意力（SD-ATTN）模块，该模块通过交叉3D注意机制来建立输入内容图像与样式引导3D资产生成的样式图像之间的动态相互作用，从而实现稳定的特征融合和有效的样式引导产生。为了减轻语义内容泄漏，我们还在SD-ATTN模块中引入了样式触发的特征选择策略，该模块利用3D功能补丁的差异来删除样式和内容很重要的频道，从而可以在注意力框架内进行选择性功能注入。使用SD-ATTN，网络可以动态地计算纹理，几何或两种引导功能来引导3D生成过程。在此基础上，我们进一步提出了样式指导控制（SGC）机制，该机制可实现独家的几何形状或仅纹理样式化以及可调节的样式强度控制。广泛的实验表明，Stylesculptor在产生高保真3D资产方面的现有基线方法优于现有的基线方法。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
