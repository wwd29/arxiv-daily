<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-22</h1>
<h3>Title: Pre to Post-Treatment Glioblastoma MRI Prediction using a Latent Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Alexandre G. Leclercq, Sébastien Bougleux, Noémie N. Moreau, Alexis Desmonts, Romain Hérault, Aurélien Corroyer-Dulmont</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17851">https://arxiv.org/abs/2510.17851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17851">https://arxiv.org/pdf/2510.17851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17851]] Pre to Post-Treatment Glioblastoma MRI Prediction using a Latent Diffusion Model(https://arxiv.org/abs/2510.17851)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Glioblastoma (GBM) is an aggressive primary brain tumor with a median survival of approximately 15 months. In clinical practice, the Stupp protocol serves as the standard first-line treatment. However, patients exhibit highly heterogeneous therapeutic responses which required at least two months before first visual impact can be observed, typically with MRI. Early prediction treatment response is crucial for advancing personalized medicine. Disease Progression Modeling (DPM) aims to capture the trajectory of disease evolution, while Treatment Response Prediction (TRP) focuses on assessing the impact of therapeutic interventions. Whereas most TRP approaches primarly rely on timeseries data, we consider the problem of early visual TRP as a slice-to-slice translation model generating post-treatment MRI from a pre-treatment MRI, thus reflecting the tumor evolution. To address this problem we propose a Latent Diffusion Model with a concatenation-based conditioning from the pre-treatment MRI and the tumor localization, and a classifier-free guidance to enhance generation quality using survival information, in particular post-treatment tumor evolution. Our model were trained and tested on a local dataset consisting of 140 GBM patients collected at Centre François Baclesse. For each patient we collected pre and post T1-Gd MRI, tumor localization manually delineated in the pre-treatment MRI by medical experts, and survival information.</li>
<li><strong>摘要：</strong>胶质母细胞瘤 (GBM) 是一种侵袭性原发性脑肿瘤，中位生存期约为 15 个月。在临床实践中，Stupp 方案作为标准的一线治疗。然而，患者表现出高度异质的治疗反应，通常需要至少两个月才能观察到第一次视觉影响（通常通过 MRI）。早期预测治疗反应对于推进个性化医疗至关重要。疾病进展模型（DPM）旨在捕捉疾病演变的轨迹，而治疗反应预测（TRP）则侧重于评估治疗干预的影响。虽然大多数 TRP 方法主要依赖于时间序列数据，但我们将早期视觉 TRP 的问题视为切片到切片转换模型，从治疗前 MRI 生成治疗后 MRI，从而反映肿瘤的演变。为了解决这个问题，我们提出了一种潜在扩散模型，该模型具有来自治疗前 MRI 和肿瘤定位的基于串联的调节，以及无分类器指导，以使用生存信息（特别是治疗后肿瘤进化）来提高生成质量。我们的模型在由 François Baclesse 中心收集的 140 名 GBM 患者组成的本地数据集上进行了训练和测试。对于每位患者，我们收集了治疗前和治疗后的 T1-Gd MRI、医学专家在治疗前 MRI 中手动描绘的肿瘤定位以及生存信息。</li>
</ul>

<h3>Title: Provenance of AI-Generated Images: A Vector Similarity and Blockchain-based Approach</h3>
<ul>
<li><strong>Authors: </strong>Jitendra Sharma, Arthur Carvalho, Suman Bhunia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17854">https://arxiv.org/abs/2510.17854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17854">https://arxiv.org/pdf/2510.17854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17854]] Provenance of AI-Generated Images: A Vector Similarity and Blockchain-based Approach(https://arxiv.org/abs/2510.17854)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Rapid advancement in generative AI and large language models (LLMs) has enabled the generation of highly realistic and contextually relevant digital content. LLMs such as ChatGPT with DALL-E integration and Stable Diffusion techniques can produce images that are often indistinguishable from those created by humans, which poses challenges for digital content authentication. Verifying the integrity and origin of digital data to ensure it remains unaltered and genuine is crucial to maintaining trust and legality in digital media. In this paper, we propose an embedding-based AI image detection framework that utilizes image embeddings and a vector similarity to distinguish AI-generated images from real (human-created) ones. Our methodology is built on the hypothesis that AI-generated images demonstrate closer embedding proximity to other AI-generated content, while human-created images cluster similarly within their domain. To validate this hypothesis, we developed a system that processes a diverse dataset of AI and human-generated images through five benchmark embedding models. Extensive experimentation demonstrates the robustness of our approach, and our results confirm that moderate to high perturbations minimally impact the embedding signatures, with perturbed images maintaining close similarity matches to their original versions. Our solution provides a generalizable framework for AI-generated image detection that balances accuracy with computational efficiency.</li>
<li><strong>摘要：</strong>生成式人工智能和大语言模型 (LLM) 的快速发展使得能够生成高度真实且与上下文相关的数字内容。 LLM（例如采用 DALL-E 集成和稳定扩散技术的 ChatGPT）可以生成通常与人类创建的图像无法区分的图像，这给数字内容身份验证带来了挑战。验证数字数据的完整性和来源以确保其保持不变且真实，对于维护数字媒体的信任和合法性至关重要。在本文中，我们提出了一种基于嵌入的人工智能图像检测框架，该框架利用图像嵌入和向量相似性来区分人工智能生成的图像和真实（人类创建的）图像。我们的方法建立在这样的假设之上：人工智能生成的图像表现出与其他人工智能生成的内容更接近的嵌入性，而人类创建的图像在其领域内的聚类类似。为了验证这一假设，我们开发了一个系统，通过五个基准嵌入模型处理人工智能和人类生成图像的多样化数据集。广泛的实验证明了我们方法的稳健性，我们的结果证实，中度到高度的扰动对嵌入特征的影响最小，扰动的图像与其原始版本保持密切的相似性匹配。我们的解决方案为人工智能生成的图像检测提供了一个通用框架，可以平衡准确性和计算效率。</li>
</ul>

<h3>Title: InsideOut: Integrated RGB-Radiative Gaussian Splatting for Comprehensive 3D Object Representation</h3>
<ul>
<li><strong>Authors: </strong>Jungmin Lee, Seonghyuk Hong, Juyong Lee, Jaeyoon Lee, Jongwon Choi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17864">https://arxiv.org/abs/2510.17864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17864">https://arxiv.org/pdf/2510.17864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17864]] InsideOut: Integrated RGB-Radiative Gaussian Splatting for Comprehensive 3D Object Representation(https://arxiv.org/abs/2510.17864)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>We introduce InsideOut, an extension of 3D Gaussian splatting (3DGS) that bridges the gap between high-fidelity RGB surface details and subsurface X-ray structures. The fusion of RGB and X-ray imaging is invaluable in fields such as medical diagnostics, cultural heritage restoration, and manufacturing. We collect new paired RGB and X-ray data, perform hierarchical fitting to align RGB and X-ray radiative Gaussian splats, and propose an X-ray reference loss to ensure consistent internal structures. InsideOut effectively addresses the challenges posed by disparate data representations between the two modalities and limited paired datasets. This approach significantly extends the applicability of 3DGS, enhancing visualization, simulation, and non-destructive testing capabilities across various domains.</li>
<li><strong>摘要：</strong>我们推出了 InsideOut，它是 3D 高斯溅射 (3DGS) 的扩展，它弥合了高保真 RGB 表面细节和次表面 X 射线结构之间的差距。 RGB 和 X 射线成像的融合在医疗诊断、文化遗产修复和制造等领域具有不可估量的价值。我们收集新的成对 RGB 和 X 射线数据，执行分层拟合以对齐 RGB 和 X 射线辐射高斯图，并提出 X 射线参考损失以确保一致的内部结构。 InsideOut 有效地解决了两种模式之间不同的数据表示和有限的配对数据集所带来的挑战。这种方法显着扩展了 3DGS 的适用性，增强了各个领域的可视化、模拟和无损测试能力。</li>
</ul>

<h3>Title: GAN-based Content-Conditioned Generation of Handwritten Musical Symbols</h3>
<ul>
<li><strong>Authors: </strong>Gerard Asbert, Pau Torras, Lei Kang, Alicia Fornés, Josep Lladós</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17869">https://arxiv.org/abs/2510.17869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17869">https://arxiv.org/pdf/2510.17869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17869]] GAN-based Content-Conditioned Generation of Handwritten Musical Symbols(https://arxiv.org/abs/2510.17869)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The field of Optical Music Recognition (OMR) is currently hindered by the scarcity of real annotated data, particularly when dealing with handwritten historical musical scores. In similar fields, such as Handwritten Text Recognition, it was proven that synthetic examples produced with image generation techniques could help to train better-performing recognition architectures. This study explores the generation of realistic, handwritten-looking scores by implementing a music symbol-level Generative Adversarial Network (GAN) and assembling its output into a full score using the Smashcima engraving software. We have systematically evaluated the visual fidelity of these generated samples, concluding that the generated symbols exhibit a high degree of realism, marking significant progress in synthetic score generation.</li>
<li><strong>摘要：</strong>光学音乐识别（OMR）领域目前因缺乏真实注释数据而受到阻碍，特别是在处理手写历史乐谱时。在类似的领域，例如手写文本识别，事实证明，使用图像生成技术生成的合成示例可以帮助训练性能更好的识别架构。本研究通过实施音乐符号级生成对抗网络 (GAN) 并使用 Smashcima 雕刻软件将其输出组装成完整乐谱，探索生成逼真的手写乐谱。我们系统地评估了这些生成的样本的视觉保真度，得出的结论是，生成的符号表现出高度的真实感，标志着合成乐谱生成方面的重大进展。</li>
</ul>

<h3>Title: Automated Algorithm Design for Auto-Tuning Optimizers</h3>
<ul>
<li><strong>Authors: </strong>Floris-Jan Willemsen, Niki van Stein, Ben van Werkhoven</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17899">https://arxiv.org/abs/2510.17899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17899">https://arxiv.org/pdf/2510.17899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17899]] Automated Algorithm Design for Auto-Tuning Optimizers(https://arxiv.org/abs/2510.17899)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Automatic performance tuning (auto-tuning) is essential for optimizing high-performance applications, where vast and irregular parameter spaces make manual exploration infeasible. Traditionally, auto-tuning relies on well-established optimization algorithms such as evolutionary algorithms, annealing methods, or surrogate model-based optimizers to efficiently find near-optimal configurations. However, designing effective optimizers remains challenging, as no single method performs best across all tuning tasks. In this work, we explore a new paradigm: using large language models (LLMs) to automatically generate optimization algorithms tailored to auto-tuning problems. We introduce a framework that prompts LLMs with problem descriptions and search-space characteristics results to produce specialized optimization strategies, which are iteratively examined and improved. These generated algorithms are evaluated on four real-world auto-tuning applications across six hardware platforms and compared against the state-of-the-art in optimization algorithms of two contemporary auto-tuning frameworks. The evaluation demonstrates that providing additional application- and search space-specific information in the generation stage results in an average performance improvement of 30.7\% and 14.6\%, respectively. In addition, our results show that LLM-generated optimizers can rival, and in various cases outperform, existing human-designed algorithms, with our best-performing generated optimization algorithms achieving, on average, 72.4\% improvement over state-of-the-art optimizers for auto-tuning.</li>
<li><strong>摘要：</strong>自动性能调优（auto-tuning）对于优化高性能应用程序至关重要，其中巨大且不规则的参数空间使得手动探索变得不可行。传统上，自动调整依赖于完善的优化算法，例如进化算法、退火方法或基于代理模型的优化器来有效地找到接近最优的配置。然而，设计有效的优化器仍然具有挑战性，因为没有一种方法能够在所有调优任务中表现最佳。在这项工作中，我们探索了一种新的范式：使用大型语言模型（LLM）自动生成针对自动调优问题的优化算法。我们引入了一个框架，提示法学硕士提供问题描述和搜索空间特征结果，以产生专门的优化策略，并对其进行迭代检查和改进。这些生成的算法在六个硬件平台上的四个真实自动调优应用程序上进行了评估，并与两个当代自动调优框架的最先进的优化算法进行了比较。评估表明，在生成阶段提供额外的特定于应用程序和搜索空间的信息，平均性能分别提高 30.7% 和 14.6%。此外，我们的结果表明，LLM 生成的优化器可以与现有的人工设计算法相媲美，并且在各种情况下都优于现有的人类设计算法，我们性能最佳的生成优化算法平均比最先进的自动调优优化器提高了 72.4%。</li>
</ul>

<h3>Title: Data Unlearning Beyond Uniform Forgetting via Diffusion Time and Frequency Selection</h3>
<ul>
<li><strong>Authors: </strong>Jinseong Park, Mijung Park</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17917">https://arxiv.org/abs/2510.17917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17917">https://arxiv.org/pdf/2510.17917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17917]] Data Unlearning Beyond Uniform Forgetting via Diffusion Time and Frequency Selection(https://arxiv.org/abs/2510.17917)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Data unlearning aims to remove the influence of specific training samples from a trained model without requiring full retraining. Unlike concept unlearning, data unlearning in diffusion models remains underexplored and often suffers from quality degradation or incomplete forgetting. To address this, we first observe that most existing methods attempt to unlearn the samples at all diffusion time steps equally, leading to poor-quality generation. We argue that forgetting occurs disproportionately across time and frequency, depending on the model and scenarios. By selectively focusing on specific time-frequency ranges during training, we achieve samples with higher aesthetic quality and lower noise. We validate this improvement by applying our time-frequency selective approach to diverse settings, including gradient-based and preference optimization objectives, as well as both image-level and text-to-image tasks. Finally, to evaluate both deletion and quality of unlearned data samples, we propose a simple normalized version of SSCD. Together, our analysis and methods establish a clearer understanding of the unique challenges in data unlearning for diffusion models, providing practical strategies to improve both evaluation and unlearning performance.</li>
<li><strong>摘要：</strong>数据去学习的目的是从训练模型中消除特定训练样本的影响，而不需要完全重新训练。与概念忘却不同，扩散模型中的数据忘却仍未得到充分探索，并且经常遭受质量下降或不完全遗忘的困扰。为了解决这个问题，我们首先观察到大多数现有方法试图在所有扩散时间步上均等地忘却样本，从而导致生成质量差。我们认为，遗忘发生的时间和频率不成比例，具体取决于模型和场景。通过在训练期间有选择地关注特定的时间频率范围，我们获得了具有更高美学质量和更低噪声的样本。我们通过将时频选择性方法应用于不同的设置来验证这种改进，包括基于梯度和偏好优化目标，以及图像级和文本到图像任务。最后，为了评估未学习数据样本的删除和质量，我们提出了一个简单的标准化版本的 SSCD。我们的分析和方法共同建立了对扩散模型数据遗忘的独特挑战的更清晰的理解，提供了提高评估和遗忘性能的实用策略。</li>
</ul>

<h3>Title: Rewarding the Journey, Not Just the Destination: A Composite Path and Answer Self-Scoring Reward Mechanism for Test-Time Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Chenwei Tang, Jingyu Xing, Xinyu Liu, Wei Ju, Jiancheng Lv, Deng Xiong, Ziyue Qiao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17923">https://arxiv.org/abs/2510.17923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17923">https://arxiv.org/pdf/2510.17923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17923]] Rewarding the Journey, Not Just the Destination: A Composite Path and Answer Self-Scoring Reward Mechanism for Test-Time Reinforcement Learning(https://arxiv.org/abs/2510.17923)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) has emerged as a powerful paradigm for advancing Large Language Models (LLMs), achieving remarkable performance in complex reasoning domains such as mathematics and code generation. However, current RL methods face a fundamental scalability bottleneck due to their heavy reliance on human-curated preference data or labeled datasets for reward modeling. To overcome this limitation, we explore RL on unlabeled data where models learn autonomously from continuous experience streams. The core challenge in this setting lies in reliable reward estimation without ground-truth supervision. Existing approaches like Test-Time RL address this through self-consistent consensus, but risk reinforcing incorrect pseudo-labels derived from majority voting. We introduce COMPASS (Composite Path and Answer Self-Scoring), a novel test-time reward mechanism that operates without external supervision. COMPASS integrates two complementary components: the Dual-Calibration Answer Reward (DCAR), which stabilizes training by establishing trustworthy pseudo-labels through confidence and credibility calibration, and the Decisive Path Reward (DPR), which directly optimizes the reasoning process quality beyond mere outcome supervision. By jointly reinforcing trustworthy consensus answers and highly decisive reasoning chains, the COMPASS systematically enhances the model's analytical capabilities. Extensive experiments show that COMPASS achieves significant and consistent performance gains across diverse reasoning tasks and model architectures, advancing a more scalable direction for LLMs to learn from continuous experience.</li>
<li><strong>摘要：</strong>强化学习 (RL) 已成为推进大型语言模型 (LLM) 的强大范例，在数学和代码生成等复杂推理领域取得了卓越的性能。然而，当前的强化学习方法面临着根本的可扩展性瓶颈，因为它们严重依赖于人类策划的偏好数据或用于奖励建模的标记数据集。为了克服这一限制，我们在未标记的数据上探索强化学习，其中模型从连续的经验流中自主学习。这种情况下的核心挑战在于在没有真实监督的情况下进行可靠的奖励估计。 Test-Time RL 等现有方法通过自洽共识解决了这个问题，但存在强化多数投票得出的不正确伪标签的风险。我们引入了 COMPASS（复合路径和答案自我评分），这是一种新颖的测试时奖励机制，无需外部监督即可运行。 COMPASS 集成了两个互补的组件：双重校准答案奖励（DCAR）和决策路径奖励（DPR），前者通过置信度和可信度校准建立可信的伪标签来稳定训练，后者直接优化推理过程质量，而不仅仅是结果监督。通过共同强化可信的共识答案和高度果断的推理链，COMPASS系统地增强了模型的分析能力。大量实验表明，COMPASS 在不同的推理任务和模型架构中实现了显着且一致的性能提升，为法学硕士从持续经验中学习提供了更具可扩展性的方向。</li>
</ul>

<h3>Title: EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable Learning</h3>
<ul>
<li><strong>Authors: </strong>He Du, Bowen Li, Aijun Yang, Siyang He, Qipeng Guo, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17928">https://arxiv.org/abs/2510.17928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17928">https://arxiv.org/pdf/2510.17928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17928]] EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable Learning(https://arxiv.org/abs/2510.17928)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reliable verifiable data has become a key driver of capability gains in modern language models, enabling stable reinforcement learning with verifiable rewards and effective distillation that transfers competence across math, coding, and agentic tasks. Yet constructing generalizable synthetic verifiable data remains difficult due to hallucination-prone generation, and weak or trivial verification artifacts that fail to separate strong from weak solutions. Existing approaches often rely on task-specific heuristics or post-hoc filters that do not transfer across domains and lack a principled, universal evaluator of verifiability. In this work, we introduce an evolutionary, task-agnostic, strategy-guided, executably-checkable data synthesis framework that, from minimal seed supervision, jointly synthesizes problems, diverse candidate solutions, and verification artifacts, and iteratively discovers strategies via a consistency-based evaluator that enforces agreement between human-annotated and strategy-induced checks. This pipeline upgrades filtering into principled synthesis: it reliably assembles coherent, verifiable training instances and generalizes without domain-specific rules. Our experiments demonstrate the effectiveness of the proposed approach under both RLVR and model distillation training paradigms. The results show that training with our synthesized data yields significant improvements on both the LiveCodeBench and AgentBench-OS tasks, highlighting the robust generalization of our framework.</li>
<li><strong>摘要：</strong>可靠的可验证数据已成为现代语言模型能力提升的关键驱动因素，通过可验证的奖励和有效的蒸馏实现稳定的强化学习，从而在数学、编码和代理任务之间转移能力。然而，由于容易产生幻觉，以及无法区分强解和弱解的弱或微不足道的验证工件，构建可推广的合成可验证数据仍然很困难。现有的方法通常依赖于特定于任务的启发式方法或事后过滤器，这些方法不会跨域转移，并且缺乏有原则的、通用的可验证性评估器。在这项工作中，我们引入了一种进化的、与任务无关的、策略引导的、可执行检查的数据合成框架，该框架从最小的种子监督开始，共同综合问题、多样化的候选解决方案和验证工件，并通过基于一致性的评估器迭代地发现策略，该评估器强制人工注释和策略诱导的检查之间的一致性。该管道将​​过滤升级为有原则的合成：它可靠地组装连贯的、可验证的训练实例，并在没有特定于领域的规则的情况下进行概括。我们的实验证明了所提出的方法在 RLVR 和模型蒸馏训练范例下的有效性。结果表明，使用我们的合成数据进行训练对 LiveCodeBench 和 AgentBench-OS 任务均产生了显着改进，凸显了我们框架的稳健泛化能力。</li>
</ul>

<h3>Title: UniRL-Zero: Reinforcement Learning on Unified Models with Joint Language Model and Diffusion Model Experts</h3>
<ul>
<li><strong>Authors: </strong>Fu-Yun Wang, Han Zhang, Michael Gharbi, Hongsheng Li, Taesung Park</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17937">https://arxiv.org/abs/2510.17937</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17937">https://arxiv.org/pdf/2510.17937</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17937]] UniRL-Zero: Reinforcement Learning on Unified Models with Joint Language Model and Diffusion Model Experts(https://arxiv.org/abs/2510.17937)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present UniRL-Zero, a unified reinforcement learning (RL) framework that boosts, multimodal language model understanding and reasoning, diffusion model multimedia generation, and their beneficial interaction capabilities within a unified model. Our work defines six scenarios for unified model reinforcement learning, providing systematic baselines for reinforcement learning of unified understanding and generation model. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>我们提出了 UniRL-Zero，这是一个统一的强化学习 (RL) 框架，可增强多模态语言模型理解和推理、扩散模型多媒体生成及其在统一模型中的有益交互功能。我们的工作定义了统一模型强化学习的六种场景，为统一理解和生成模型的强化学习提供了系统的基线。我们的代码可以在这个 https URL 上找到。</li>
</ul>

<h3>Title: Demystifying Transition Matching: When and Why It Can Beat Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Jaihoon Kim, Rajarshi Saha, Minhyuk Sung, Youngsuk Park</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17991">https://arxiv.org/abs/2510.17991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17991">https://arxiv.org/pdf/2510.17991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17991]] Demystifying Transition Matching: When and Why It Can Beat Flow Matching(https://arxiv.org/abs/2510.17991)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Flow Matching (FM) underpins many state-of-the-art generative models, yet recent results indicate that Transition Matching (TM) can achieve higher quality with fewer sampling steps. This work answers the question of when and why TM outperforms FM. First, when the target is a unimodal Gaussian distribution, we prove that TM attains strictly lower KL divergence than FM for finite number of steps. The improvement arises from stochastic difference latent updates in TM, which preserve target covariance that deterministic FM underestimates. We then characterize convergence rates, showing that TM achieves faster convergence than FM under a fixed compute budget, establishing its advantage in the unimodal Gaussian setting. Second, we extend the analysis to Gaussian mixtures and identify local-unimodality regimes in which the sampling dynamics approximate the unimodal case, where TM can outperform FM. The approximation error decreases as the minimal distance between component means increases, highlighting that TM is favored when the modes are well separated. However, when the target variance approaches zero, each TM update converges to the FM update, and the performance advantage of TM diminishes. In summary, we show that TM outperforms FM when the target distribution has well-separated modes and non-negligible variances. We validate our theoretical results with controlled experiments on Gaussian distributions, and extend the comparison to real-world applications in image and video generation.</li>
<li><strong>摘要：</strong>流匹配 (FM) 是许多最先进的生成模型的基础，但最近的结果表明，转换匹配 (TM) 可以通过更少的采样步骤实现更高的质量。这项工作回答了 TM 何时以及为何优于 FM 的问题。首先，当目标是单峰高斯分布时，我们证明在有限步数下，TM 的 KL 散度严格低于 FM。这种改进源于 TM 中的随机差异潜在更新，它保留了确定性 FM 低估的目标协方差。然后，我们描述了收敛速度，表明在固定计算预算下，TM 比 FM 实现了更快的收敛，从而确立了其在单峰高斯设置中的优势。其次，我们将分析扩展到高斯混合并确定局部单峰状态，其中采样动态近似单峰情况，其中 TM 可以优于 FM。随着分量均值之间的最小距离增加，近似误差减小，这突出表明当模式分离良好时，TM 更受欢迎。然而，当目标方差接近零时，每次TM更新都会收敛到FM更新，TM的性能优势就会减弱。总之，我们表明，当目标分布具有良好分离的模式和不可忽略的方差时，TM 优于 FM。我们通过高斯分布的受控实验验证了我们的理论结果，并将比较扩展到图像和视频生成中的实际应用。</li>
</ul>

<h3>Title: Adaptive Divergence Regularized Policy Optimization for Fine-tuning Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Jiajun Fan, Tong Wei, Chaoran Cheng, Yuxin Chen, Ge Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18053">https://arxiv.org/abs/2510.18053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18053">https://arxiv.org/pdf/2510.18053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18053]] Adaptive Divergence Regularized Policy Optimization for Fine-tuning Generative Models(https://arxiv.org/abs/2510.18053)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Balancing exploration and exploitation during reinforcement learning fine-tuning of generative models presents a critical challenge, as existing approaches rely on fixed divergence regularization that creates an inherent dilemma: strong regularization preserves model capabilities but limits reward optimization, while weak regularization enables greater alignment but risks instability or reward hacking. We introduce Adaptive Divergence Regularized Policy Optimization (ADRPO), which automatically adjusts regularization strength based on advantage estimates-reducing regularization for high-value samples while applying stronger regularization to poor samples, enabling policies to navigate between exploration and aggressive exploitation according to data quality. Our implementation with Wasserstein-2 regularization for flow matching generative models achieves remarkable results on text-to-image generation, achieving better semantic alignment and diversity than offline methods like DPO and online methods with fixed regularization like ORW-CFM-W2. ADRPO enables a 2B parameter SD3 model to surpass much larger models with 4.8B and 12B parameters in attribute binding, semantic consistency, artistic style transfer, and compositional control while maintaining generation diversity. ADRPO generalizes to KL-regularized fine-tuning of both text-only LLMs and multi-modal reasoning models, enhancing existing online RL methods like GRPO. In LLM fine-tuning, ADRPO demonstrates an emergent ability to escape local optima through active exploration, while in multi-modal audio reasoning, it outperforms GRPO through superior step-by-step reasoning, enabling a 7B model to outperform substantially larger commercial models including Gemini 2.5 Pro and GPT-4o Audio, offering an effective plug-and-play solution to the exploration-exploitation challenge across diverse generative architectures and modalities.</li>
<li><strong>摘要：</strong>在生成模型的强化学习微调过程中平衡探索和利用提出了严峻的挑战，因为现有方法依赖于固定散度正则化，这会造成固有的困境：强正则化可以保留模型功能，但限制奖励优化，而弱正则化可以实现更大的对齐，但存在不稳定或奖励黑客的风险。我们引入了自适应发散正则化策略优化（ADRPO），它根据优势估计自动调整正则化强度——减少高价值样本的正则化，同时对不良样本应用更强的正则化，使策略能够根据数据质量在探索和积极利用之间导航。我们使用 Wasserstein-2 正则化实现流匹配生成模型，在文本到图像生成方面取得了显着的效果，比 DPO 等离线方法和 ORW-CFM-W2 等具有固定正则化的在线方法实现了更好的语义对齐和多样性。 ADRPO 使 2B 参数 SD3 模型在属性绑定、语义一致性、艺术风格转移和构图控制方面超越具有 4.8B 和 12B 参数的更大模型，同时保持生成多样性。 ADRPO 推广到纯文本 LLM 和多模态推理模型的 KL 正则化微调，增强了 GRPO 等现有的在线 RL 方法。在 LLM 微调中，ADRPO 展示了通过主动探索摆脱局部最优的新兴能力，而在多模态音频推理中，它通过卓越的逐步推理超越 GRPO，使 7B 模型能够超越包括 Gemini 2.5 Pro 和 GPT-4o Audio 在内的更大的商业模型，为跨不同生成的探索利用挑战提供有效的即插即用解决方案 架构和模式。</li>
</ul>

<h3>Title: HouseTour: A Virtual Real Estate A(I)gent</h3>
<ul>
<li><strong>Authors: </strong>Ata Çelen, Marc Pollefeys, Daniel Barath, Iro Armeni</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18054">https://arxiv.org/abs/2510.18054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18054">https://arxiv.org/pdf/2510.18054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18054]] HouseTour: A Virtual Real Estate A(I)gent(https://arxiv.org/abs/2510.18054)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce HouseTour, a method for spatially-aware 3D camera trajectory and natural language summary generation from a collection of images depicting an existing 3D space. Unlike existing vision-language models (VLMs), which struggle with geometric reasoning, our approach generates smooth video trajectories via a diffusion process constrained by known camera poses and integrates this information into the VLM for 3D-grounded descriptions. We synthesize the final video using 3D Gaussian splatting to render novel views along the trajectory. To support this task, we present the HouseTour dataset, which includes over 1,200 house-tour videos with camera poses, 3D reconstructions, and real estate descriptions. Experiments demonstrate that incorporating 3D camera trajectories into the text generation process improves performance over methods handling each task independently. We evaluate both individual and end-to-end performance, introducing a new joint metric. Our work enables automated, professional-quality video creation for real estate and touristic applications without requiring specialized expertise or equipment.</li>
<li><strong>摘要：</strong>我们介绍了 HouseTour，这是一种空间感知 3D 相机轨迹和从描绘现有 3D 空间的图像集合生成自然语言摘要的方法。与难以进行几何推理的现有视觉语言模型 (VLM) 不同，我们的方法通过受已知相机姿势约束的扩散过程生成平滑的视频轨迹，并将这些信息集成到 VLM 中以进行基于 3D 的描述。我们使用 3D 高斯喷射来合成最终视频，以沿轨迹呈现新颖的视图。为了支持这项任务，我们提出了 HouseTour 数据集，其中包括 1,200 多个带有相机姿势、3D 重建和房地产描述的房屋游览视频。实验表明，将 3D 相机轨迹合并到文本生成过程中，比独立处理每个任务的方法提高了性能。我们评估个人和端到端的表现，引入新的联合指标。我们的工作可以为房地产和旅游应用程序实现自动化、专业品质的视频创建，而无需专门的专业知识或设备。</li>
</ul>

<h3>Title: SPACeR: Self-Play Anchoring with Centralized Reference Models</h3>
<ul>
<li><strong>Authors: </strong>Wei-Jer Chang, Akshay Rangesh, Kevin Joseph, Matthew Strong, Masayoshi Tomizuka, Yihan Hu, Wei Zhan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18060">https://arxiv.org/abs/2510.18060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18060">https://arxiv.org/pdf/2510.18060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18060]] SPACeR: Self-Play Anchoring with Centralized Reference Models(https://arxiv.org/abs/2510.18060)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Developing autonomous vehicles (AVs) requires not only safety and efficiency, but also realistic, human-like behaviors that are socially aware and predictable. Achieving this requires sim agent policies that are human-like, fast, and scalable in multi-agent settings. Recent progress in imitation learning with large diffusion-based or tokenized models has shown that behaviors can be captured directly from human driving data, producing realistic policies. However, these models are computationally expensive, slow during inference, and struggle to adapt in reactive, closed-loop scenarios. In contrast, self-play reinforcement learning (RL) scales efficiently and naturally captures multi-agent interactions, but it often relies on heuristics and reward shaping, and the resulting policies can diverge from human norms. We propose SPACeR, a framework that leverages a pretrained tokenized autoregressive motion model as a centralized reference policy to guide decentralized self-play. The reference model provides likelihood rewards and KL divergence, anchoring policies to the human driving distribution while preserving RL scalability. Evaluated on the Waymo Sim Agents Challenge, our method achieves competitive performance with imitation-learned policies while being up to 10x faster at inference and 50x smaller in parameter size than large generative models. In addition, we demonstrate in closed-loop ego planning evaluation tasks that our sim agents can effectively measure planner quality with fast and scalable traffic simulation, establishing a new paradigm for testing autonomous driving policies.</li>
<li><strong>摘要：</strong>开发自动驾驶汽车 (AV) 不仅需要安全性和效率，还需要具有社会意识和可预测性的现实、类人行为。要实现这一目标，需要在多代理设置中具有类人性、快速且可扩展的 sim 代理策略。利用大型扩散或标记化模型进行模仿学习的最新进展表明，可以直接从人类驾驶数据中捕获行为，从而产生现实的政策。然而，这些模型的计算成本高昂，推理过程缓慢，并且难以适应反应式闭环场景。相比之下，自对弈强化学习（RL）可以有效地扩展并自然地捕获多智能体交互，但它通常依赖于启发式和奖励塑造，并且由此产生的策略可能会偏离人类规范。我们提出了 SPACeR，一个利用预训练的标记化自回归运动模型作为集中参考策略来指导去中心化自我游戏的框架。参考模型提供似然奖励和 KL 散度，将策略锚定到人类驾驶分布，同时保持 RL 可扩展性。经过 Waymo Sim Agents Challenge 的评估，我们的方法通过模仿学习策略实现了具有竞争力的性能，同时推理速度比大型生成模型快 10 倍，参数大小小 50 倍。此外，我们在闭环自我规划评估任务中证明，我们的模拟代理可以通过快速且可扩展的交通模拟来有效衡量规划者的质量，从而建立测试自动驾驶策略的新范例。</li>
</ul>

<h3>Title: Fine-tuning Flow Matching Generative Models with Intermediate Feedback</h3>
<ul>
<li><strong>Authors: </strong>Jiajun Fan, Chaoran Cheng, Shuaike Shen, Xiangxin Zhou, Ge Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18072">https://arxiv.org/abs/2510.18072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18072">https://arxiv.org/pdf/2510.18072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18072]] Fine-tuning Flow Matching Generative Models with Intermediate Feedback(https://arxiv.org/abs/2510.18072)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Flow-based generative models have shown remarkable success in text-to-image generation, yet fine-tuning them with intermediate feedback remains challenging, especially for continuous-time flow matching models. Most existing approaches solely learn from outcome rewards, struggling with the credit assignment problem. Alternative methods that attempt to learn a critic via direct regression on cumulative rewards often face training instabilities and model collapse in online settings. We present AC-Flow, a robust actor-critic framework that addresses these challenges through three key innovations: (1) reward shaping that provides well-normalized learning signals to enable stable intermediate value learning and gradient control, (2) a novel dual-stability mechanism that combines advantage clipping to prevent destructive policy updates with a warm-up phase that allows the critic to mature before influencing the actor, and (3) a scalable generalized critic weighting scheme that extends traditional reward-weighted methods while preserving model diversity through Wasserstein regularization. Through extensive experiments on Stable Diffusion 3, we demonstrate that AC-Flow achieves state-of-the-art performance in text-to-image alignment tasks and generalization to unseen human preference models. Our results demonstrate that even with a computationally efficient critic model, we can robustly finetune flow models without compromising generative quality, diversity, or stability.</li>
<li><strong>摘要：</strong>基于流的生成模型在文本到图像的生成方面取得了显着的成功，但利用中间反馈对其进行微调仍然具有挑战性，特别是对于连续时间流匹配模型。大多数现有方法仅从结果奖励中学习，难以解决学分分配问题。尝试通过累积奖励的直接回归来学习批评者的替代方法通常会面临在线环境中的训练不稳定和模型崩溃。我们提出了 AC-Flow，一个强大的行动者-批评者框架，通过三个关键创新来应对这些挑战：(1) 奖励塑造，提供良好标准化的学习信号，以实现稳定的中间值学习和梯度控制；(2) 一种新颖的双稳定性机制，它将优势剪裁与预热阶段相结合，以防止破坏性的策略更新，使批评者在影响行动者之前成熟；(3) 可扩展的广义批评者 加权方案扩展了传统的奖励加权方法，同时通过 Wasserstein 正则化保留了模型多样性。通过对 Stable Diffusion 3 的大量实验，我们证明了 AC-Flow 在文本到图像对齐任务以及对看不见的人类偏好模型的泛化方面实现了最先进的性能。我们的结果表明，即使使用计算效率高的批评模型，我们也可以在不影响生成质量、多样性或稳定性的情况下稳健地微调流模型。</li>
</ul>

<h3>Title: Any-Depth Alignment: Unlocking Innate Safety Alignment of LLMs to Any-Depth</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Zhang, Andrew Estornell, David D. Baek, Bo Li, Xiaojun Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18081">https://arxiv.org/abs/2510.18081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18081">https://arxiv.org/pdf/2510.18081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18081]] Any-Depth Alignment: Unlocking Innate Safety Alignment of LLMs to Any-Depth(https://arxiv.org/abs/2510.18081)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) exhibit strong but shallow alignment: they directly refuse harmful queries when a refusal is expected at the very start of an assistant turn, yet this protection collapses once a harmful continuation is underway (either through the adversarial attacks or via harmful assistant-prefill attacks). This raises a fundamental question: Can the innate shallow alignment in LLMs be unlocked to ensure safety at arbitrary generation depths? To achieve this goal, we propose Any-Depth Alignment (ADA), an effective inference-time defense with negligible overhead. ADA is built based on our observation that alignment is concentrated in the assistant header tokens through repeated use in shallow-refusal training, and these tokens possess the model's strong alignment priors. By reintroducing these tokens mid-stream, ADA induces the model to reassess harmfulness and recover refusals at any point in generation. Across diverse open-source model families (Llama, Gemma, Mistral, Qwen, DeepSeek, and gpt-oss), ADA achieves robust safety performance without requiring any changes to the base model's parameters. It secures a near-100% refusal rate against challenging adversarial prefill attacks ranging from dozens to thousands of tokens. Furthermore, ADA reduces the average success rate of prominent adversarial prompt attacks (such as GCG, AutoDAN, PAIR, and TAP) to below 3%. This is all accomplished while preserving utility on benign tasks with minimal over-refusal. ADA maintains this resilience even after the base model undergoes subsequent instruction tuning (benign or adversarial).</li>
<li><strong>摘要：</strong>大型语言模型（LLM）表现出强大但浅层的一致性：当助理轮流一开始就预计会拒绝时，它们会直接拒绝有害的查询，但一旦有害的延续正在进行（无论是通过对抗性攻击还是通过有害的助理预填充攻击），这种保护就会崩溃。这就提出了一个基本问题：LLM 中固有的浅对齐是否可以被解锁以确保任意生成深度的安全性？为了实现这一目标，我们提出了任意深度对齐（ADA），这是一种有效的推理时间防御，开销可以忽略不计。 ADA 是基于我们的观察而构建的，即通​​过在浅层拒绝训练中重复使用，对齐集中在辅助标头标记中，并且这些标记拥有模型的强对齐先验。通过在中流中重新引入这些代币，ADA 会引导模型重新评估危害性并在生成过程中的任何时刻恢复拒绝。在不同的开源模型系列（Llama、Gemma、Mistral、Qwen、DeepSeek 和 gpt-oss）中，ADA 无需对基础模型的参数进行任何更改即可实现强大的安全性能。它可以确保对数十个到数千个代币的具有挑战性的对抗性预填充攻击的拒绝率接近 100%。此外，ADA 将突出的对抗性即时攻击（例如 GCG、AutoDAN、PAIR 和 TAP）的平均成功率降低到 3% 以下。这一切都是在保持良性任务的实用性的同时以最小的过度拒绝来实现的。即使基础模型经过后续指令调整（良性或对抗性），ADA 仍能保持这种弹性。</li>
</ul>

<h3>Title: Chimera: Compositional Image Generation using Part-based Concepting</h3>
<ul>
<li><strong>Authors: </strong>Shivam Singh, Yiming Chen, Agneet Chatterjee, Amit Raj, James Hays, Yezhou Yang, Chitra Baral</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18083">https://arxiv.org/abs/2510.18083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18083">https://arxiv.org/pdf/2510.18083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18083]] Chimera: Compositional Image Generation using Part-based Concepting(https://arxiv.org/abs/2510.18083)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Personalized image generative models are highly proficient at synthesizing images from text or a single image, yet they lack explicit control for composing objects from specific parts of multiple source images without user specified masks or annotations. To address this, we introduce Chimera, a personalized image generation model that generates novel objects by combining specified parts from different source images according to textual instructions. To train our model, we first construct a dataset from a taxonomy built on 464 unique (part, subject) pairs, which we term semantic atoms. From this, we generate 37k prompts and synthesize the corresponding images with a high-fidelity text-to-image model. We train a custom diffusion prior model with part-conditional guidance, which steers the image-conditioning features to enforce both semantic identity and spatial layout. We also introduce an objective metric PartEval to assess the fidelity and compositional accuracy of generation pipelines. Human evaluations and our proposed metric show that Chimera outperforms other baselines by 14% in part alignment and compositional accuracy and 21% in visual quality.</li>
<li><strong>摘要：</strong>个性化图像生成模型非常擅长从文本或单个图像合成图像，但在没有用户指定的掩模或注释的情况下，它们缺乏对从多个源图像的特定部分组成对象的明确控制。为了解决这个问题，我们引入了 Chimera，一种个性化图像生成模型，它根据文本指令组合不同源图像中的指定部分来生成新颖的对象。为了训练我们的模型，我们首先根据基于 464 个独特（部分、主题）对（我们称之为语义原子）的分类法构建数据集。由此，我们生成 37k 提示并使用高保真文本到图像模型合成相应的图像。我们训练具有部分条件指导的自定义扩散先验模型，该模型引导图像调节特征以强制执行语义身份和空间布局。我们还引入了客观指标 PartEval 来评估生成管道的保真度和合成准确性。人类评估和我们提出的指标表明，Chimera 在部分对齐和构图准确性方面优于其他基线 14%，在视觉质量方面优于其他基线 21%。</li>
</ul>

<h3>Title: From Volume Rendering to 3D Gaussian Splatting: Theory and Applications</h3>
<ul>
<li><strong>Authors: </strong>Vitor Pereira Matias, Daniel Perazzo, Vinicius Silva, Alberto Raposo, Luiz Velho, Afonso Paiva, Tiago Novello</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18101">https://arxiv.org/abs/2510.18101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18101">https://arxiv.org/pdf/2510.18101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18101]] From Volume Rendering to 3D Gaussian Splatting: Theory and Applications(https://arxiv.org/abs/2510.18101)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The problem of 3D reconstruction from posed images is undergoing a fundamental transformation, driven by continuous advances in 3D Gaussian Splatting (3DGS). By modeling scenes explicitly as collections of 3D Gaussians, 3DGS enables efficient rasterization through volumetric splatting, offering thus a seamless integration with common graphics pipelines. Despite its real-time rendering capabilities for novel view synthesis, 3DGS suffers from a high memory footprint, the tendency to bake lighting effects directly into its representation, and limited support for secondary-ray effects. This tutorial provides a concise yet comprehensive overview of the 3DGS pipeline, starting from its splatting formulation and then exploring the main efforts in addressing its limitations. Finally, we survey a range of applications that leverage 3DGS for surface reconstruction, avatar modeling, animation, and content generation-highlighting its efficient rendering and suitability for feed-forward pipelines.</li>
<li><strong>摘要：</strong>在 3D 高斯分布 (3DGS) 不断进步的推动下，从摆姿图像进行 3D 重建的问题正在经历根本性的转变。通过将场景明确建模为 3D 高斯集合，3DGS 通过体积喷射实现高效光栅化，从而提供与常见图形管道的无缝集成。尽管 3DGS 具有用于新颖视图合成的实时渲染功能，但其内存占用较高、倾向于将照明效果直接烘焙到其表示中以及对二次光线效果的支持有限。本教程提供了 3DGS 管道的简洁而全面的概述，从其泼溅公式开始，然后探索解决其局限性的主要努力。最后，我们调查了一系列利用 3DGS 进行表面重建、头像建模、动画和内容生成的应用程序，强调了其高效渲染和前馈管道的适用性。</li>
</ul>

<h3>Title: Latent Discrete Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Dario Shariatian, Alain Durmus, Stefano Peluchetti</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18114">https://arxiv.org/abs/2510.18114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18114">https://arxiv.org/pdf/2510.18114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18114]] Latent Discrete Diffusion Models(https://arxiv.org/abs/2510.18114)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We study discrete diffusion for language and other categorical data and focus on a common limitation of masked denoisers: reverse transitions typically factorize across positions, which can weaken joint structure and degrade quality in few-step generation. We propose \emph{Latent Discrete Diffusion Models} (LDDMs), which couple a masked discrete diffusion over tokens with a continuous diffusion over latent embeddings. The latent channel provides a softer signal and carries cross-token dependencies that help resolve ambiguities. We present two instantiations: (i) FUJI-LDDMs, which perform fully joint denoising of tokens and latents, and (ii) SEQ-LDDMs, which sequentially resolve the latent and then the discrete chain conditionally on it. For both variants we derive ELBO-style objectives and discuss design choices to learn informative latents yet amenable to diffusoin modeling. In experiments, LDDMs yield improvements on unconditional generation metrics as compared to state-of-the-art masked discrete diffusion baselines, and are effective at lower sampling budgets, where unmasking many tokens per step is desirable.</li>
<li><strong>摘要：</strong>我们研究语言和其他分类数据的离散扩散，并关注掩蔽降噪器的常见局限性：反向转换通常会跨位置进行因式分解，这会削弱联合结构并降低几步生成的质量。我们提出\emph{潜在离散扩散模型}（LDDM），它将令牌上的屏蔽离散扩散与潜在嵌入上的连续扩散结合起来。潜在通道提供更柔和的信号并携带有助于解决歧义的跨令牌依赖性。我们提出了两个实例：(i) FUJI-LDDM，它对标记和潜在变量执行完全联合去噪；(ii) SEQ-LDDM，它顺序解析潜在变量，然后有条件地解析离散链。对于这两种变体，我们得出 ELBO 风格的目标并讨论设计选择，以学习信息丰富的潜在因素，但适合扩散建模。在实验中，与最先进的屏蔽离散扩散基线相比，LDDM 在无条件生成指标上产生了改进，并且在较低的采样预算下有效，其中每步揭露许多令牌是可取的。</li>
</ul>

<h3>Title: Gradient Variance Reveals Failure Modes in Flow-Based Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Teodora Reu, Sixtine Dromigny, Michael Bronstein, Francisco Vargas</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18118">https://arxiv.org/abs/2510.18118</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18118">https://arxiv.org/pdf/2510.18118</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18118]] Gradient Variance Reveals Failure Modes in Flow-Based Generative Models(https://arxiv.org/abs/2510.18118)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Rectified Flows learn ODE vector fields whose trajectories are straight between source and target distributions, enabling near one-step inference. We show that this straight-path objective conceals fundamental failure modes: under deterministic training, low gradient variance drives memorization of arbitrary training pairings, even when interpolant lines between pairs intersect. To analyze this mechanism, we study Gaussian-to-Gaussian transport and use the loss gradient variance across stochastic and deterministic regimes to characterize which vector fields optimization favors in each setting. We then show that, in a setting where all interpolating lines intersect, applying Rectified Flow yields the same specific pairings at inference as during training. More generally, we prove that a memorizing vector field exists even when training interpolants intersect, and that optimizing the straight-path objective converges to this ill-defined field. At inference, deterministic integration reproduces the exact training pairings. We validate our findings empirically on the CelebA dataset, confirming that deterministic interpolants induce memorization, while the injection of small noise restores generalization.</li>
<li><strong>摘要：</strong>整流流学习 ODE 向量场，其轨迹在源分布和目标分布之间是直线，从而实现近乎一步的推理。我们表明，这种直线路径目标隐藏了基本的故障模式：在确定性训练下，低梯度方差驱动任意训练对的记忆，即使当对之间的插值线相交时也是如此。为了分析这种机制，我们研究高斯到高斯传输，并使用随机和确定性机制中的损失梯度方差来表征每种设置中哪些向量场优化有利。然后，我们表明，在所有插值线相交的设置中，应用整流流会在推理时产生与训练期间相同的特定配对。更一般地说，我们证明即使训练插值相交时也存在记忆向量场，并且优化直线路径目标会收敛到这个定义不明确的场。在推理时，确定性集成再现了精确的训练配对。我们在 CelebA 数据集上凭经验验证了我们的发现，证实确定性插值可以诱导记忆，而注入小噪声可以恢复泛化。</li>
</ul>

<h3>Title: HyperDiffusionFields (HyDiF): Diffusion-Guided Hypernetworks for Learning Implicit Molecular Neural Fields</h3>
<ul>
<li><strong>Authors: </strong>Sudarshan Babu, Phillip Lo, Xiao Zhang, Aadi Srivastava, Ali Davariashtiyani, Jason Perera, Michael Maire, Aly A. Khan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18122">https://arxiv.org/abs/2510.18122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18122">https://arxiv.org/pdf/2510.18122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18122]] HyperDiffusionFields (HyDiF): Diffusion-Guided Hypernetworks for Learning Implicit Molecular Neural Fields(https://arxiv.org/abs/2510.18122)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We introduce HyperDiffusionFields (HyDiF), a framework that models 3D molecular conformers as continuous fields rather than discrete atomic coordinates or graphs. At the core of our approach is the Molecular Directional Field (MDF), a vector field that maps any point in space to the direction of the nearest atom of a particular type. We represent MDFs using molecule-specific neural implicit fields, which we call Molecular Neural Fields (MNFs). To enable learning across molecules and facilitate generalization, we adopt an approach where a shared hypernetwork, conditioned on a molecule, generates the weights of the given molecule's MNF. To endow the model with generative capabilities, we train the hypernetwork as a denoising diffusion model, enabling sampling in the function space of molecular fields. Our design naturally extends to a masked diffusion mechanism to support structure-conditioned generation tasks, such as molecular inpainting, by selectively noising regions of the field. Beyond generation, the localized and continuous nature of MDFs enables spatially fine-grained feature extraction for molecular property prediction, something not easily achievable with graph or point cloud based methods. Furthermore, we demonstrate that our approach scales to larger biomolecules, illustrating a promising direction for field-based molecular modeling.</li>
<li><strong>摘要：</strong>我们引入了 HyperDiffusionFields (HyDiF)，这是一个将 3D 分子构象异构体建模为连续场而不是离散原子坐标或图形的框架。我们方法的核心是分子方向场（MDF），这是一种矢量场，可将空间中的任何点映射到最近的特定类型原子的方向。我们使用分子特定的神经隐式场（我们称之为分子神经场（MNF））来表示 MDF。为了实现跨分子学习并促进泛化，我们采用了一种方法，其中以分子为条件的共享超网络生成给定分子的 MNF 的权重。为了赋予模型生成能力，我们将超网络训练为去噪扩散模型，从而能够在分子场的函数空间中进行采样。我们的设计自然地扩展到掩蔽扩散机制，以通过选择性地对场域区域进行噪声处理来支持结构条件生成任务，例如分子修复。除了生成之外，MDF 的局部和连续性质可以实现分子属性预测的空间细粒度特征提取，这是使用基于图形或点云的方法不容易实现的。此外，我们证明了我们的方法可以扩展到更大的生物分子，这为基于现场的分子建模指明了一个有希望的方向。</li>
</ul>

<h3>Title: World-in-World: World Models in a Closed-Loop World</h3>
<ul>
<li><strong>Authors: </strong>Jiahan Zhang, Muqing Jiang, Nanru Dai, Taiming Lu, Arda Uzunoglu, Shunchi Zhang, Yana Wei, Jiahao Wang, Vishal M. Patel, Paul Pu Liang, Daniel Khashabi, Cheng Peng, Rama Chellappa, Tianmin Shu, Alan Yuille, Yilun Du, Jieneng Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18135">https://arxiv.org/abs/2510.18135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18135">https://arxiv.org/pdf/2510.18135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18135]] World-in-World: World Models in a Closed-Loop World(https://arxiv.org/abs/2510.18135)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative world models (WMs) can now simulate worlds with striking visual realism, which naturally raises the question of whether they can endow embodied agents with predictive perception for decision making. Progress on this question has been limited by fragmented evaluation: most existing benchmarks adopt open-loop protocols that emphasize visual quality in isolation, leaving the core issue of embodied utility unresolved, i.e., do WMs actually help agents succeed at embodied tasks? To address this gap, we introduce World-in-World, the first open platform that benchmarks WMs in a closed-loop world that mirrors real agent-environment interactions. World-in-World provides a unified online planning strategy and a standardized action API, enabling heterogeneous WMs for decision making. We curate four closed-loop environments that rigorously evaluate diverse WMs, prioritize task success as the primary metric, and move beyond the common focus on visual quality; we also present the first data scaling law for world models in embodied settings. Our study uncovers three surprises: (1) visual quality alone does not guarantee task success, controllability matters more; (2) scaling post-training with action-observation data is more effective than upgrading the pretrained video generators; and (3) allocating more inference-time compute allows WMs to substantially improve closed-loop performance.</li>
<li><strong>摘要：</strong>生成世界模型（WM）现在可以模拟具有惊人视觉真实感的世界，这自然提出了一个问题：它们是否可以赋予实体代理决策的预测感知。这个问题的进展受到碎片化评估的限制：大多数现有基准采用开环协议，孤立地强调视觉质量，而没有解决体现效用的核心问题，即 WM 是否真的帮助智能体成功完成体现任务？为了解决这一差距，我们引入了 World-in-World，这是第一个在闭环世界中对 WM 进行基准测试的开放平台，反映了真实的代理与环境交互。 World-in-World提供统一的在线规划策略和标准化的操作API，支持异构WM进行决策。我们策划了四个闭环环境，严格评估不同的 WM，将任务成功作为主要指标，并超越对视觉质量的共同关注；我们还为具体环境中的世界模型提出了第一个数据缩放定律。我们的研究揭示了三个惊喜：（1）视觉质量本身并不能保证任务成功，可控性更重要； (2) 使用动作观察数据扩展训练后比升级预训练视频生成器更有效； (3) 分配更多的推理时间计算使 WM 能够显着提高闭环性能。</li>
</ul>

<h3>Title: RadDiagSeg-M: A Vision Language Model for Joint Diagnosis and Multi-Target Segmentation in Radiology</h3>
<ul>
<li><strong>Authors: </strong>Chengrun Li, Corentin Royer, Haozhe Luo, Bastian Wittmann, Xia Li, Ibrahim Hamamci, Sezgin Er, Anjany Sekuboyina, Bjoern Menze</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18188">https://arxiv.org/abs/2510.18188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18188">https://arxiv.org/pdf/2510.18188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18188]] RadDiagSeg-M: A Vision Language Model for Joint Diagnosis and Multi-Target Segmentation in Radiology(https://arxiv.org/abs/2510.18188)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Most current medical vision language models struggle to jointly generate diagnostic text and pixel-level segmentation masks in response to complex visual questions. This represents a major limitation towards clinical application, as assistive systems that fail to provide both modalities simultaneously offer limited value to medical practitioners. To alleviate this limitation, we first introduce RadDiagSeg-D, a dataset combining abnormality detection, diagnosis, and multi-target segmentation into a unified and hierarchical task. RadDiagSeg-D covers multiple imaging modalities and is precisely designed to support the development of models that produce descriptive text and corresponding segmentation masks in tandem. Subsequently, we leverage the dataset to propose a novel vision-language model, RadDiagSeg-M, capable of joint abnormality detection, diagnosis, and flexible segmentation. RadDiagSeg-M provides highly informative and clinically useful outputs, effectively addressing the need to enrich contextual information for assistive diagnosis. Finally, we benchmark RadDiagSeg-M and showcase its strong performance across all components involved in the task of multi-target text-and-mask generation, establishing a robust and competitive baseline.</li>
<li><strong>摘要：</strong>当前大多数医学视觉语言模型都难以联合生成诊断文本和像素级分割掩模以响应复杂的视觉问题。这代表了临床应用的主要限制，因为无法同时提供两种模式的辅助系统为医疗从业者提供的价值有限。为了缓解这一限制，我们首先引入 RadDiagSeg-D，这是一个将异常检测、诊断和多目标分割结合到统一和分层任务中的数据集。 RadDiagSeg-D 涵盖多种成像模式，经过精心设计，可支持同时生成描述性文本和相应分割掩模的模型的开发。随后，我们利用该数据集提出了一种新颖的视觉语言模型 RadDiagSeg-M，能够进行联合异常检测、诊断和灵活分割。 RadDiagSeg-M 提供信息丰富、临床有用的输出，有效满足丰富辅助诊断背景信息的需求。最后，我们对 RadDiagSeg-M 进行了基准测试，并展示了其在多目标文本和掩码生成任务所涉及的所有组件中的强大性能，建立了强大且有竞争力的基线。</li>
</ul>

<h3>Title: Towards Fast LLM Fine-tuning through Zeroth-Order Optimization with Projected Gradient-Aligned Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Zhendong Mi, Qitao Tan, Grace Li Zhang, Zhaozhuo Xu, Geng Yuan, Shaoyi Huang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18228">https://arxiv.org/abs/2510.18228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18228">https://arxiv.org/pdf/2510.18228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18228]] Towards Fast LLM Fine-tuning through Zeroth-Order Optimization with Projected Gradient-Aligned Perturbations(https://arxiv.org/abs/2510.18228)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models (LLMs) using zeroth-order (ZO) optimization has emerged as a promising alternative to traditional gradient-based methods due to its reduced memory footprint requirement. However, existing ZO methods suffer from high variance in gradient estimation, leading to slow convergence and suboptimal performance on large-scale models. In this work, we propose P-GAP, a fast LLM fine-tuning approach through zeroth-order optimization with Projected Gradient-Aligned Perturbations. Specifically, we first estimate a low-dimensional gradient space and then align perturbations in projected gradients' direction within the space. This approach enables reduced the number of perturbed parameters and decreased variance, therefore accelerated convergence for LLM fine-tuning. Experiments on LLMs show that P-GAP consistently surpasses the baselines, achieving up to 6% increase in accuracy on classification tasks and up to 12% higher accuracy on generation tasks, with up to about 81% less training iterations and 70% less GPU hours. These results demonstrate that P-GAP enables fast, scalable, and resource-efficient ZO LLM fine-tuning.</li>
<li><strong>摘要：</strong>使用零阶 (ZO) 优化对大型语言模型 (LLM) 进行微调已成为传统基于梯度的方法的一种有前途的替代方案，因为它减少了内存占用需求。然而，现有的 ZO 方法在梯度估计方面存在较大方差，导致收敛速度慢且在大规模模型上性能不佳。在这项工作中，我们提出了 P-GAP，这是一种通过投影梯度对齐扰动进行零阶优化的快速 LLM 微调方法。具体来说，我们首先估计一个低维梯度空间，然后在空间内沿投影梯度方向对齐扰动。这种方法可以减少扰动参数的数量并减少方差，从而加速 LLM 微调的收敛。 LLM 上的实验表明，P-GAP 始终超越基线，分类任务的准确性提高了 6%，生成任务的准确性提高了 12%，训练迭代次数减少了约 81%，GPU 时间减少了 70%。这些结果表明，P-GAP 能够实现快速、可扩展且资源高效的 ZO LLM 微调。</li>
</ul>

<h3>Title: Beyond Frequency: Scoring-Driven Debiasing for Object Detection via Blueprint-Prompted Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Xinhao Cai, Liulei Li, Gensheng Pei, Tao Chen, Jinshan Pan, Yazhou Yao, Wenguan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18229">https://arxiv.org/abs/2510.18229</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18229">https://arxiv.org/pdf/2510.18229</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18229]] Beyond Frequency: Scoring-Driven Debiasing for Object Detection via Blueprint-Prompted Image Synthesis(https://arxiv.org/abs/2510.18229)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>This paper presents a generation-based debiasing framework for object detection. Prior debiasing methods are often limited by the representation diversity of samples, while naive generative augmentation often preserves the biases it aims to solve. Moreover, our analysis reveals that simply generating more data for rare classes is suboptimal due to two core issues: i) instance frequency is an incomplete proxy for the true data needs of a model, and ii) current layout-to-image synthesis lacks the fidelity and control to generate high-quality, complex scenes. To overcome this, we introduce the representation score (RS) to diagnose representational gaps beyond mere frequency, guiding the creation of new, unbiased layouts. To ensure high-quality synthesis, we replace ambiguous text prompts with a precise visual blueprint and employ a generative alignment strategy, which fosters communication between the detector and generator. Our method significantly narrows the performance gap for underrepresented object groups, \eg, improving large/rare instances by 4.4/3.6 mAP over the baseline, and surpassing prior L2I synthesis models by 15.9 mAP for layout accuracy in generated images.</li>
<li><strong>摘要：</strong>本文提出了一种基于生成的目标检测去偏框架。先前的去偏差方法通常受到样本表示多样性的限制，而朴素的生成增强通常保留其旨在解决的偏差。此外，我们的分析表明，由于两个核心问题，简单地为稀有类别生成更多数据并不是最理想的：i）实例频率不能完全代表模型的真实数据需求，ii）当前的布局到图像合成缺乏生成高质量、复杂场景的保真度和控制。为了克服这个问题，我们引入了表征得分（RS）来诊断频率之外的表征差距，指导创建新的、公正的布局。为了确保高质量的合成，我们用精确的视觉蓝图替换模糊的文本提示，并采用生成对齐策略，以促进检测器和生成器之间的通信。我们的方法显着缩小了代表性不足的对象组的性能差距，例如，将大型/稀有实例比基线提高了 4.4/3.6 mAP，并在生成图像的布局精度方面超越了先前的 L2I 合成模型 15.9 mAP。</li>
</ul>

<h3>Title: ACTG-ARL: Differentially Private Conditional Text Generation with RL-Boosted Control</h3>
<ul>
<li><strong>Authors: </strong>Yuzheng Hu, Ryan McKenna, Da Yu, Shanshan Wu, Han Zhao, Zheng Xu, Peter Kairouz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18232">https://arxiv.org/abs/2510.18232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18232">https://arxiv.org/pdf/2510.18232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18232]] ACTG-ARL: Differentially Private Conditional Text Generation with RL-Boosted Control(https://arxiv.org/abs/2510.18232)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating high-quality synthetic text under differential privacy (DP) is critical for training and evaluating language models without compromising user privacy. Prior work on synthesizing DP datasets often fail to preserve key statistical attributes, suffer utility loss from the noise required by DP, and lack fine-grained control over generation. To address these challenges, we make two contributions. First, we introduce a hierarchical framework that decomposes DP synthetic text generation into two subtasks: feature learning and conditional text generation. This design explicitly incorporates learned features into the generation process and simplifies the end-to-end synthesis task. Through systematic ablations, we identify the most effective configuration: a rich tabular schema as feature, a DP tabular synthesizer, and a DP fine-tuned conditional generator, which we term ACTG (Attribute-Conditioned Text Generation). Second, we propose Anchored RL (ARL), a post-training method that improves the instruction-following ability of ACTG for conditional generation. ARL combines RL to boost control with an SFT anchor on best-of-$N$ data to prevent reward hacking. Together, these components form our end-to-end algorithm ACTG-ARL, which advances both the quality of DP synthetic text (+20% MAUVE over prior work) and the control of the conditional generator under strong privacy guarantees.</li>
<li><strong>摘要：</strong>在差分隐私（DP）下生成高质量的合成文本对于在不损害用户隐私的情况下训练和评估语言模型至关重要。先前合成 DP 数据集的工作通常无法保留关键的统计属性，因 DP 所需的噪声而遭受效用损失，并且缺乏对生成的细粒度控制。为了应对这些挑战，我们做出了两项贡献。首先，我们引入一个分层框架，将 DP 合成文本生成分解为两个子任务：特征学习和条件文本生成。这种设计明确地将学习的特征合并到生成过程中，并简化了端到端的综合任务。通过系统化的消融，我们确定了最有效的配置：丰富的表格模式作为特征，DP表格合成器和DP微调条件生成器，我们称之为ACTG（属性条件文本生成）。其次，我们提出了锚定强化学习（ARL），这是一种训练后方法，可以提高 ACTG 条件生成的指令跟踪能力。 ARL 将 RL 与 SFT 锚点结合起来，增强对 best-of-$N$ 数据的控制，以防止奖励黑客行为。这些组件共同构成了我们的端到端算法 ACTG-ARL，该算法既提高了 DP 合成文本的质量（比之前的工作提高了 20% MAUVE），又提高了在强大的隐私保证下对条件生成器的控制。</li>
</ul>

<h3>Title: From Competition to Synergy: Unlocking Reinforcement Learning for Subject-Driven Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Ziwei Huang, Ying Shu, Hao Fang, Quanyu Long, Wenya Wang, Qiushi Guo, Tiezheng Ge, Leilei Gan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18263">https://arxiv.org/abs/2510.18263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18263">https://arxiv.org/pdf/2510.18263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18263]] From Competition to Synergy: Unlocking Reinforcement Learning for Subject-Driven Image Generation(https://arxiv.org/abs/2510.18263)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Subject-driven image generation models face a fundamental trade-off between identity preservation (fidelity) and prompt adherence (editability). While online reinforcement learning (RL), specifically GPRO, offers a promising solution, we find that a naive application of GRPO leads to competitive degradation, as the simple linear aggregation of rewards with static weights causes conflicting gradient signals and a misalignment with the temporal dynamics of the diffusion process. To overcome these limitations, we propose Customized-GRPO, a novel framework featuring two key innovations: (i) Synergy-Aware Reward Shaping (SARS), a non-linear mechanism that explicitly penalizes conflicted reward signals and amplifies synergistic ones, providing a sharper and more decisive gradient. (ii) Time-Aware Dynamic Weighting (TDW), which aligns the optimization pressure with the model's temporal dynamics by prioritizing prompt-following in the early, identity preservation in the later. Extensive experiments demonstrate that our method significantly outperforms naive GRPO baselines, successfully mitigating competitive degradation. Our model achieves a superior balance, generating images that both preserve key identity features and accurately adhere to complex textual prompts.</li>
<li><strong>摘要：</strong>主题驱动的图像生成模型面临身份保留（保真度）和及时遵守（可编辑性）之间的基本权衡。虽然在线强化学习（RL），特别是 GPRO，提供了一个有前途的解决方案，但我们发现 GRPO 的简单应用会导致竞争性下降，因为带有静态权重的奖励的简单线性聚合会导致梯度信号冲突以及与扩散过程的时间动态不一致。为了克服这些限制，我们提出了定制-GRPO，这是一个具有两个关键创新的新颖框架：（i）协同感知奖励塑造（SARS），这是一种非线性机制，明确惩罚冲突的奖励信号并放大协同信号，提供更尖锐和更具决定性的梯度。 (ii) 时间感知动态权重 (TDW)，通过优先考虑早期的提示跟踪和后期的身份保留，将优化压力与模型的时间动态保持一致。大量实验表明，我们的方法明显优于原始 GRPO 基线，成功缓解了竞争退化。我们的模型实现了卓越的平衡，生成的图像既保留了关键身份特征，又准确地遵循复杂的文本提示。</li>
</ul>

<h3>Title: Efficient Few-shot Identity Preserving Attribute Editing for 3D-aware Deep Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Vishal Vinod</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18287">https://arxiv.org/abs/2510.18287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18287">https://arxiv.org/pdf/2510.18287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18287]] Efficient Few-shot Identity Preserving Attribute Editing for 3D-aware Deep Generative Models(https://arxiv.org/abs/2510.18287)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Identity preserving editing of faces is a generative task that enables modifying the illumination, adding/removing eyeglasses, face aging, editing hairstyles, modifying expression etc., while preserving the identity of the face. Recent progress in 2D generative models have enabled photorealistic editing of faces using simple techniques leveraging the compositionality in GANs. However, identity preserving editing for 3D faces with a given set of attributes is a challenging task as the generative model must reason about view consistency from multiple poses and render a realistic 3D face. Further, 3D portrait editing requires large-scale attribute labelled datasets and presents a trade-off between editability in low-resolution and inflexibility to editing in high resolution. In this work, we aim to alleviate some of the constraints in editing 3D faces by identifying latent space directions that correspond to photorealistic edits. To address this, we present a method that builds on recent advancements in 3D-aware deep generative models and 2D portrait editing techniques to perform efficient few-shot identity preserving attribute editing for 3D-aware generative models. We aim to show from experimental results that using just ten or fewer labelled images of an attribute is sufficient to estimate edit directions in the latent space that correspond to 3D-aware attribute editing. In this work, we leverage an existing face dataset with masks to obtain the synthetic images for few attribute examples required for estimating the edit directions. Further, to demonstrate the linearity of edits, we investigate one-shot stylization by performing sequential editing and use the (2D) Attribute Style Manipulation (ASM) technique to investigate a continuous style manifold for 3D consistent identity preserving face aging. Code and results are available at: this https URL</li>
<li><strong>摘要：</strong>面部身份保留编辑是一项生成任务，可以修改照明、添加/删除眼镜、面部老化、编辑发型、修改表情等，同时保留面部身份。 2D 生成模型的最新进展使得利用 GAN 中的组合性的简单技术能够对脸部进行逼真的编辑。然而，对具有给定属性集的 3D 人脸进行身份保留编辑是一项具有挑战性的任务，因为生成模型必须推理多个姿势的视图一致性并渲染真实的 3D 人脸。此外，3D 肖像编辑需要大规模属性标记数据集，并且需要在低分辨率可编辑性和高分辨率编辑不灵活之间进行权衡。在这项工作中，我们的目标是通过识别与真实感编辑相对应的潜在空间方向来减轻编辑 3D 脸部的一些限制。为了解决这个问题，我们提出了一种方法，该方法基于 3D 感知深度生成模型和 2D 肖像编辑技术的最新进展，为 3D 感知生成模型执行高效的少镜头身份保留属性编辑。我们的目标是从实验结果中证明，仅使用十个或更少的属性标记图像就足以估计与 3D 感知属性编辑相对应的潜在空间中的编辑方向。在这项工作中，我们利用带有掩模的现有人脸数据集来获取估计编辑方向所需的少数属性示例的合成图像。此外，为了证明编辑的线性性，我们通过执行顺序编辑来研究一次性风格化，并使用 (2D) 属性样式操作 (ASM) 技术来研究连续样式流形，以实现 3D 一致的身份保留面部老化。代码和结果可在以下位置获取：此 https URL</li>
</ul>

<h3>Title: Proactive Reasoning-with-Retrieval Framework for Medical Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Lehan Wang, Yi Qin, Honglong Yang, Xiaomeng Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18303">https://arxiv.org/abs/2510.18303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18303">https://arxiv.org/pdf/2510.18303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18303]] Proactive Reasoning-with-Retrieval Framework for Medical Multimodal Large Language Models(https://arxiv.org/abs/2510.18303)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Incentivizing the reasoning ability of Multimodal Large Language Models (MLLMs) is essential for medical applications to transparently analyze medical scans and provide reliable diagnosis. However, existing medical MLLMs rely solely on internal knowledge during reasoning, leading to hallucinated reasoning and factual inaccuracies when encountering cases beyond their training scope. Although recent Agentic Retrieval-Augmented Generation (RAG) methods elicit the medical model's proactive retrieval ability during reasoning, they are confined to unimodal LLMs, neglecting the crucial visual information during reasoning and retrieval. Consequently, we propose the first Multimodal Medical Reasoning-with-Retrieval framework, Med-RwR, which actively retrieves external knowledge by querying observed symptoms or domain-specific medical concepts during reasoning. Specifically, we design a two-stage reinforcement learning strategy with tailored rewards that stimulate the model to leverage both visual diagnostic findings and textual clinical information for effective retrieval. Building on this foundation, we further propose a Confidence-Driven Image Re-retrieval (CDIR) method for test-time scaling when low prediction confidence is detected. Evaluation on various public medical benchmarks demonstrates Med-RwR's significant improvements over baseline models, proving the effectiveness of enhancing reasoning capabilities with external knowledge integration. Furthermore, Med-RwR demonstrates remarkable generalizability to unfamiliar domains, evidenced by 8.8% performance gain on our proposed EchoCardiography Benchmark (ECBench), despite the scarcity of echocardiography data in the training corpus. Our data, model, and codes will be made publicly available at this https URL.</li>
<li><strong>摘要：</strong>激励多模态大语言模型 (MLLM) 的推理能力对于医疗应用程序透明地分析医学扫描并提供可靠的诊断至关重要。然而，现有的医学MLLM在推理时仅依靠内部知识，在遇到超出其训练范围的病例时，会导致推理产生幻觉和事实不准确。尽管最近的代理检索增强生成（RAG）方法在推理过程中引发了医学模型的主动检索能力，但它们仅限于单峰LLM，忽略了推理和检索过程中关键的视觉信息。因此，我们提出了第一个多模态医学推理与检索框架 Med-RwR，它通过在推理过程中查询观察到的症状或特定领域的医学概念来主动检索外部知识。具体来说，我们设计了一个具有定制奖励的两阶段强化学习策略，可刺激模型利用视觉诊断结果和文本临床信息进行有效检索。在此基础上，我们进一步提出了一种置信度驱动的图像重新检索（CDIR）方法，用于在检测到低预测置信度时进行测试时间缩放。对各种公共医学基准的评估表明Med-RwR相对于基线模型有显着改进，证明了通过外部知识整合增强推理能力的有效性。此外，尽管训练语料库中缺乏超声心动图数据，Med-RwR 对不熟悉的领域表现出显着的通用性，我们提出的超声心动图基准 (ECBench) 的性能提升了 8.8%。我们的数据、模型和代码将在此 https URL 上公开提供。</li>
</ul>

<h3>Title: Towards Identifiability of Hierarchical Temporal Causal Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Zijian Li, Minghao Fu, Junxian Huang, Yifan Shen, Ruichu Cai, Yuewen Sun, Guangyi Chen, Kun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18310">https://arxiv.org/abs/2510.18310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18310">https://arxiv.org/pdf/2510.18310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18310]] Towards Identifiability of Hierarchical Temporal Causal Representation Learning(https://arxiv.org/abs/2510.18310)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Modeling hierarchical latent dynamics behind time series data is critical for capturing temporal dependencies across multiple levels of abstraction in real-world tasks. However, existing temporal causal representation learning methods fail to capture such dynamics, as they fail to recover the joint distribution of hierarchical latent variables from \textit{single-timestep observed variables}. Interestingly, we find that the joint distribution of hierarchical latent variables can be uniquely determined using three conditionally independent observations. Building on this insight, we propose a Causally Hierarchical Latent Dynamic (CHiLD) identification framework. Our approach first employs temporal contextual observed variables to identify the joint distribution of multi-layer latent variables. Sequentially, we exploit the natural sparsity of the hierarchical structure among latent variables to identify latent variables within each layer. Guided by the theoretical results, we develop a time series generative model grounded in variational inference. This model incorporates a contextual encoder to reconstruct multi-layer latent variables and normalize flow-based hierarchical prior networks to impose the independent noise condition of hierarchical latent dynamics. Empirical evaluations on both synthetic and real-world datasets validate our theoretical claims and demonstrate the effectiveness of CHiLD in modeling hierarchical latent dynamics.</li>
<li><strong>摘要：</strong>对时间序列数据背后的分层潜在动态进行建模对于捕获现实世界任务中多个抽象级别的时间依赖性至关重要。然而，现有的时间因果表示学习方法无法捕获这种动态，因为它们无法从 \textit{单时间步观察变量} 恢复分层潜在变量的联合分布。有趣的是，我们发现分层潜变量的联合分布可以使用三个条件独立的观察来唯一确定。基于这一见解，我们提出了因果层次潜在动态（CHiLD）识别框架。我们的方法首先采用时间上下文观察变量来识别多层潜在变量的联合分布。接下来，我们利用潜在变量之间层次结构的自然稀疏性来识别每层内的潜在变量。在理论结果的指导下，我们开发了一种基于变分推理的时间序列生成模型。该模型结合了上下文编码器来重建多层潜在变量，并对基于流的分层先验网络进行归一化，以施加分层潜在动态的独立噪声条件。对合成数据集和现实世界数据集的实证评估验证了我们的理论主张，并证明了 CHiLD 在建模分层潜在动态方面的有效性。</li>
</ul>

<h3>Title: OmniNWM: Omniscient Driving Navigation World Models</h3>
<ul>
<li><strong>Authors: </strong>Bohan Li, Zhuang Ma, Dalong Du, Baorui Peng, Zhujin Liang, Zhenqiang Liu, Chao Ma, Yueming Jin, Hao Zhao, Wenjun Zeng, Xin Jin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18313">https://arxiv.org/abs/2510.18313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18313">https://arxiv.org/pdf/2510.18313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18313]] OmniNWM: Omniscient Driving Navigation World Models(https://arxiv.org/abs/2510.18313)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Autonomous driving world models are expected to work effectively across three core dimensions: state, action, and reward. Existing models, however, are typically restricted to limited state modalities, short video sequences, imprecise action control, and a lack of reward awareness. In this paper, we introduce OmniNWM, an omniscient panoramic navigation world model that addresses all three dimensions within a unified framework. For state, OmniNWM jointly generates panoramic videos of RGB, semantics, metric depth, and 3D occupancy. A flexible forcing strategy enables high-quality long-horizon auto-regressive generation. For action, we introduce a normalized panoramic Plucker ray-map representation that encodes input trajectories into pixel-level signals, enabling highly precise and generalizable control over panoramic video generation. Regarding reward, we move beyond learning reward functions with external image-based models: instead, we leverage the generated 3D occupancy to directly define rule-based dense rewards for driving compliance and safety. Extensive experiments demonstrate that OmniNWM achieves state-of-the-art performance in video generation, control accuracy, and long-horizon stability, while providing a reliable closed-loop evaluation framework through occupancy-grounded rewards. Project page is available at this https URL.</li>
<li><strong>摘要：</strong>自动驾驶世界模型预计将在三个核心维度上有效发挥作用：状态、行动和奖励。然而，现有模型通常仅限于有限的状态模式、短视频序列、不精确的动作控制以及缺乏奖励意识。在本文中，我们介绍了 OmniNWM，这是一种全知全景导航世界模型，可在统一框架内解决所有三个维度。对于状态，OmniNWM 联合生成 RGB、语义、度量深度和 3D 占用的全景视频。灵活的强制策略可实现高质量的长视野自回归生成。对于动作，我们引入了标准化的全景 Plucker 射线图表示，将输入轨迹编码为像素级信号，从而实现对全景视频生成的高精度和通用控制。关于奖励，我们超越了使用基于外部图像的模型学习奖励函数：相反，我们利用生成的 3D 占用率直接定义基于规则的密集奖励，以推动合规性和安全性。大量实验表明，OmniNWM 在视频生成、控制精度和长视野稳定性方面实现了最先进的性能，同时通过基于占用的奖励提供了可靠的闭环评估框架。项目页面可通过此 https URL 获取。</li>
</ul>

<h3>Title: Scalable, Explainable and Provably Robust Anomaly Detection with One-Step Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Zhong Li, Qi Huang, Yuxuan Zhu, Lincen Yang, Mohammad Mohammadi Amiri, Niki van Stein, Matthijs van Leeuwen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18328">https://arxiv.org/abs/2510.18328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18328">https://arxiv.org/pdf/2510.18328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18328]] Scalable, Explainable and Provably Robust Anomaly Detection with One-Step Flow Matching(https://arxiv.org/abs/2510.18328)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce Time-Conditioned Contraction Matching (TCCM), a novel method for semi-supervised anomaly detection in tabular data. TCCM is inspired by flow matching, a recent generative modeling framework that learns velocity fields between probability distributions and has shown strong performance compared to diffusion models and generative adversarial networks. Instead of directly applying flow matching as originally formulated, TCCM builds on its core idea -- learning velocity fields between distributions -- but simplifies the framework by predicting a time-conditioned contraction vector toward a fixed target (the origin) at each sampled time step. This design offers three key advantages: (1) a lightweight and scalable training objective that removes the need for solving ordinary differential equations during training and inference; (2) an efficient scoring strategy called one time-step deviation, which quantifies deviation from expected contraction behavior in a single forward pass, addressing the inference bottleneck of existing continuous-time models such as DTE (a diffusion-based model with leading anomaly detection accuracy but heavy inference cost); and (3) explainability and provable robustness, as the learned velocity field operates directly in input space, making the anomaly score inherently feature-wise attributable; moreover, the score function is Lipschitz-continuous with respect to the input, providing theoretical guarantees under small perturbations. Extensive experiments on the ADBench benchmark show that TCCM strikes a favorable balance between detection accuracy and inference cost, outperforming state-of-the-art methods -- especially on high-dimensional and large-scale datasets. The source code is available at our GitHub repository.</li>
<li><strong>摘要：</strong>我们引入时间条件收缩匹配（TCCM），这是一种在表格数据中进行半监督异常检测的新方法。 TCCM 受到流匹配的启发，流匹配是一种最新的生成建模框架，可以学习概率分布之间的速度场，并且与扩散模型和生成对抗网络相比，表现出了强大的性能。 TCCM 没有像最初那样直接应用流匹配，而是建立在其核心思想（学习分布之间的速度场）的基础上，但通过在每个采​​样时间步预测朝向固定目标（原点）的时间条件收缩向量来简化框架。该设计具有三个关键优势：（1）轻量级且可扩展的训练目标，无需在训练和推理过程中求解常微分方程； （2）一种称为单时间步偏差的高效评分策略，它量化单次前向传递中与预期收缩行为的偏差，解决现有连续时间模型的推理瓶颈，例如DTE（一种基于扩散的模型，具有领先的异常检测精度，但推理成本很高）； (3)可解释性和可证明的鲁棒性，因为学习的速度场直接在输入空间中运行，使得异常分数本质上可归因于特征；此外，得分函数相对于输入是Lipschitz连续的，在小扰动下提供了理论保证。 ADBench 基准测试的大量实验表明，TCCM 在检测精度和推理成本之间取得了良好的平衡，优于最先进的方法，尤其是在高维和大规模数据集上。源代码可在我们的 GitHub 存储库中获取。</li>
</ul>

<h3>Title: ViSE: A Systematic Approach to Vision-Only Street-View Extrapolation</h3>
<ul>
<li><strong>Authors: </strong>Kaiyuan Tan, Yingying Shen, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18341">https://arxiv.org/abs/2510.18341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18341">https://arxiv.org/pdf/2510.18341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18341]] ViSE: A Systematic Approach to Vision-Only Street-View Extrapolation(https://arxiv.org/abs/2510.18341)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Realistic view extrapolation is critical for closed-loop simulation in autonomous driving, yet it remains a significant challenge for current Novel View Synthesis (NVS) methods, which often produce distorted and inconsistent images beyond the original trajectory. This report presents our winning solution which ctook first place in the RealADSim Workshop NVS track at ICCV 2025. To address the core challenges of street view extrapolation, we introduce a comprehensive four-stage pipeline. First, we employ a data-driven initialization strategy to generate a robust pseudo-LiDAR point cloud, avoiding local minima. Second, we inject strong geometric priors by modeling the road surface with a novel dimension-reduced SDF termed 2D-SDF. Third, we leverage a generative prior to create pseudo ground truth for extrapolated viewpoints, providing auxilary supervision. Finally, a data-driven adaptation network removes time-specific artifacts. On the RealADSim-NVS benchmark, our method achieves a final score of 0.441, ranking first among all participants.</li>
<li><strong>摘要：</strong>真实的视图外推对于自动驾驶中的闭环仿真至关重要，但对于当前的新视图合成（NVS）方法来说仍然是一个重大挑战，该方法通常会产生超出原始轨迹的扭曲和不一致的图像。本报告介绍了我们的获奖解决方案，该解决方案在 ICCV 2025 的 RealADSim Workshop NVS 赛道中获得第一名。为了解决街景外推的核心挑战，我们引入了全面的四阶段管道。首先，我们采用数据驱动的初始化策略来生成鲁棒的伪激光雷达点云，避免局部最小值。其次，我们通过使用一种新颖的降维 SDF（称为 2D-SDF）对路面进行建模来注入强大的几何先验。第三，我们利用生成先验为推断的观点创建伪地面事实，提供辅助监督。最后，数据驱动的适应网络消除了特定时间的伪影。在 RealADSim-NVS 基准测试中，我们的方法最终得分为 0.441，在所有参与者中排名第一。</li>
</ul>

<h3>Title: GPTFace: Generative Pre-training of Facial-Linguistic Transformer by Span Masking and Weakly Correlated Text-image Data</h3>
<ul>
<li><strong>Authors: </strong>Yudong Li, Hao Li, Xianxu Hou, Linlin Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18345">https://arxiv.org/abs/2510.18345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18345">https://arxiv.org/pdf/2510.18345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18345]] GPTFace: Generative Pre-training of Facial-Linguistic Transformer by Span Masking and Weakly Correlated Text-image Data(https://arxiv.org/abs/2510.18345)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Compared to the prosperity of pre-training models in natural image understanding, the research on large-scale pre-training models for facial knowledge learning is still limited. Current approaches mainly rely on manually assembled and annotated face datasets for training, but labeling such datasets is labor-intensive and the trained models have limited scalability beyond the training data. To address these limitations, we present a generative pre-training model for facial knowledge learning that leverages large-scale web-built data for training. We use texts and images containing human faces crawled from the internet and conduct pre-training on self-supervised tasks, including masked image/language modeling (MILM) and image-text matching (ITM). During the generation stage, we further utilize the image-text matching loss to pull the generation distribution towards the control signal for controllable image/text generation. Experimental results demonstrate that our model achieves comparable performance to state-of-the-art pre-training models for various facial downstream tasks, such as attribution classification and expression recognition. Furthermore, our approach is also applicable to a wide range of face editing tasks, including face attribute editing, expression manipulation, mask removal, and photo inpainting.</li>
<li><strong>摘要：</strong>与自然图像理解中预训练模型的繁荣相比，用于面部知识学习的大规模预训练模型的研究仍然有限。当前的方法主要依靠手动组装和注释的人脸数据集进行训练，但标记此类数据集是劳动密集型的，并且训练后的模型除了训练数据之外的可扩展性有限。为了解决这些限制，我们提出了一种用于面部知识学习的生成预训练模型，该模型利用大规模网络构建数据进行训练。我们使用从互联网上爬取的包含人脸的文本和图像，对自我监督任务进行预训练，包括蒙版图像/语言建模（MILM）和图像文本匹配（ITM）。在生成阶段，我们进一步利用图像文本匹配损失将生成分布拉向控制信号，以实现可控图像/文本生成。实验结果表明，我们的模型在各种面部下游任务（例如归因分类和表情识别）中实现了与最先进的预训练模型相当的性能。此外，我们的方法还适用于广泛的面部编辑任务，包括面部属性编辑、表情操作、掩模去除和照片修复。</li>
</ul>

<h3>Title: Ranking-based Preference Optimization for Diffusion Models from Implicit User Feedback</h3>
<ul>
<li><strong>Authors: </strong>Yi-Lun Wu, Bo-Kai Ruan, Chiang Tseng, Hong-Han Shuai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18353">https://arxiv.org/abs/2510.18353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18353">https://arxiv.org/pdf/2510.18353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18353]] Ranking-based Preference Optimization for Diffusion Models from Implicit User Feedback(https://arxiv.org/abs/2510.18353)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Direct preference optimization (DPO) methods have shown strong potential in aligning text-to-image diffusion models with human preferences by training on paired comparisons. These methods improve training stability by avoiding the REINFORCE algorithm but still struggle with challenges such as accurately estimating image probabilities due to the non-linear nature of the sigmoid function and the limited diversity of offline datasets. In this paper, we introduce Diffusion Denoising Ranking Optimization (Diffusion-DRO), a new preference learning framework grounded in inverse reinforcement learning. Diffusion-DRO removes the dependency on a reward model by casting preference learning as a ranking problem, thereby simplifying the training objective into a denoising formulation and overcoming the non-linear estimation issues found in prior methods. Moreover, Diffusion-DRO uniquely integrates offline expert demonstrations with online policy-generated negative samples, enabling it to effectively capture human preferences while addressing the limitations of offline data. Comprehensive experiments show that Diffusion-DRO delivers improved generation quality across a range of challenging and unseen prompts, outperforming state-of-the-art baselines in both both quantitative metrics and user studies. Our source code and pre-trained models are available at this https URL.</li>
<li><strong>摘要：</strong>直接偏好优化（DPO）方法通过配对比较训练，在将文本到图像扩散模型与人类偏好保持一致方面表现出了强大的潜力。这些方法通过避免 REINFORCE 算法来提高训练稳定性，但仍然面临着一些挑战，例如由于 sigmoid 函数的非线性性质和离线数据集的有限多样性，准确估计图像概率。在本文中，我们介绍了扩散去噪排名优化（Diffusion-DRO），这是一种基于逆强化学习的新偏好学习框架。 Diffusion-DRO 通过将偏好学习视为排序问题来消除对奖励模型的依赖，从而将训练目标简化为去噪公式，并克服了先前方法中发现的非线性估计问题。此外，Diffusion-DRO独特地将线下专家演示与线上策略生成的负样本相结合，使其能够有效捕捉人类偏好，同时解决线下数据的局限性。综合实验表明，Diffusion-DRO 在一系列具有挑战性和看不见的提示下提供了更高的发电质量，在定量指标和用户研究方面均优于最先进的基线。我们的源代码和预训练模型可从此 https URL 获取。</li>
</ul>

<h3>Title: Learning to Flow from Generative Pretext Tasks for Neural Architecture Encoding</h3>
<ul>
<li><strong>Authors: </strong>Sunwoo Kim, Hyunjin Hwang, Kijung Shin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18360">https://arxiv.org/abs/2510.18360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18360">https://arxiv.org/pdf/2510.18360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18360]] Learning to Flow from Generative Pretext Tasks for Neural Architecture Encoding(https://arxiv.org/abs/2510.18360)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The performance of a deep learning model on a specific task and dataset depends heavily on its neural architecture, motivating considerable efforts to rapidly and accurately identify architectures suited to the target task and dataset. To achieve this, researchers use machine learning models-typically neural architecture encoders-to predict the performance of a neural architecture. Many state-of-the-art encoders aim to capture information flow within a neural architecture, which reflects how information moves through the forward pass and backpropagation, via a specialized model structure. However, due to their complicated structures, these flow-based encoders are significantly slower to process neural architectures compared to simpler encoders, presenting a notable practical challenge. To address this, we propose FGP, a novel pre-training method for neural architecture encoding that trains an encoder to capture the information flow without requiring specialized model structures. FGP trains an encoder to reconstruct a flow surrogate, our proposed representation of the neural architecture's information flow. Our experiments show that FGP boosts encoder performance by up to 106% in Precision-1%, compared to the same encoder trained solely with supervised learning.</li>
<li><strong>摘要：</strong>深度学习模型在特定任务和数据集上的性能在很大程度上取决于其神经架构，这促使人们付出大量努力来快速准确地识别适合目标任务和数据集的架构。为了实现这一目标，研究人员使用机器学习模型（通常是神经架构编码器）来预测神经架构的性能。许多最先进的编码器旨在捕获神经架构内的信息流，这反映了信息如何通过专门的模型结构通过前向传播和反向传播。然而，由于其复杂的结构，与更简单的编码器相比，这些基于流的编码器处理神经架构的速度要慢得多，这带来了显着的实际挑战。为了解决这个问题，我们提出了 FGP，一种用于神经架构编码的新型预训练方法，可以训练编码器捕获信息流，而不需要专门的模型结构。 FGP 训练编码器来重建流代理，即我们提出的神经架构信息流的表示。我们的实验表明，与仅使用监督学习训练的相同编码器相比，FGP 在精度为 1% 的情况下将编码器性能提高了 106%。</li>
</ul>

<h3>Title: Learning from N-Tuple Data with M Positive Instances: Unbiased Risk Estimation and Theoretical Guarantees</h3>
<ul>
<li><strong>Authors: </strong>Miao Zhang, Junpeng Li, ChangChun HUa, Yana Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18406">https://arxiv.org/abs/2510.18406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18406">https://arxiv.org/pdf/2510.18406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18406]] Learning from N-Tuple Data with M Positive Instances: Unbiased Risk Estimation and Theoretical Guarantees(https://arxiv.org/abs/2510.18406)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Weakly supervised learning often operates with coarse aggregate signals rather than instance labels. We study a setting where each training example is an $n$-tuple containing exactly m positives, while only the count m per tuple is observed. This NTMP (N-tuple with M positives) supervision arises in, e.g., image classification with region proposals and multi-instance measurements. We show that tuple counts admit a trainable unbiased risk estimator (URE) by linking the tuple-generation process to latent instance marginals. Starting from fixed (n,m), we derive a closed-form URE and extend it to variable tuple sizes, variable counts, and their combination. Identification holds whenever the effective mixing rate is separated from the class prior. We establish generalization bounds via Rademacher complexity and prove statistical consistency with standard rates under mild regularity assumptions. To improve finite-sample stability, we introduce simple ReLU corrections to the URE that preserve asymptotic correctness. Across benchmarks converted to NTMP tasks, the approach consistently outperforms representative weak-supervision baselines and yields favorable precision-recall and F1 trade-offs. It remains robust under class-prior imbalance and across diverse tuple configurations, demonstrating that count-only supervision can be exploited effectively through a theoretically grounded and practically stable objective.</li>
<li><strong>摘要：</strong>弱监督学习通常使用粗略的聚合信号而不是实例标签进行操作。我们研究了一个设置，其中每个训练示例都是一个 $n$ 元组，其中恰好包含 m 个正值，而仅观察每个元组的 m 个计数。这种 NTMP（具有 M 个正值的 N 元组）监督出现在例如具有区域建议和多实例测量的图像分类中。我们通过将元组生成过程与潜在实例边际联系起来，证明元组计数承认可训练的无偏风险估计器（URE）。从固定的 (n,m) 开始，我们导出一个封闭形式的 URE 并将其扩展到可变元组大小、变量计数及其组合。只要有效混合率与先前的类别分开，识别就成立。我们通过 Rademacher 复杂度建立泛化界限，并在温和的规律性假设下证明与标准率的统计一致性。为了提高有限样本稳定性，我们对 URE 引入了简单的 ReLU 校正，以保持渐近正确性。在转换为 NTMP 任务的基准测试中，该方法始终优于代表性的弱监督基线，并产生有利的精确率、召回率和 F1 权衡。它在类先验不平衡和跨不同元组配置的情况下仍然保持稳健，这表明可以通过理论基础和实践稳定的目标有效地利用仅计数监督。</li>
</ul>

<h3>Title: ImageGem: In-the-wild Generative Image Interaction Dataset for Generative Model Personalization</h3>
<ul>
<li><strong>Authors: </strong>Yuanhe Guo, Linxi Xie, Zhuoran Chen, Kangrui Yu, Ryan Po, Guandao Yang, Gordon Wetztein, Hongyi Wen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18433">https://arxiv.org/abs/2510.18433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18433">https://arxiv.org/pdf/2510.18433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18433]] ImageGem: In-the-wild Generative Image Interaction Dataset for Generative Model Personalization(https://arxiv.org/abs/2510.18433)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce ImageGem, a dataset for studying generative models that understand fine-grained individual preferences. We posit that a key challenge hindering the development of such a generative model is the lack of in-the-wild and fine-grained user preference annotations. Our dataset features real-world interaction data from 57K users, who collectively have built 242K customized LoRAs, written 3M text prompts, and created 5M generated images. With user preference annotations from our dataset, we were able to train better preference alignment models. In addition, leveraging individual user preference, we investigated the performance of retrieval models and a vision-language model on personalized image retrieval and generative model recommendation. Finally, we propose an end-to-end framework for editing customized diffusion models in a latent weight space to align with individual user preferences. Our results demonstrate that the ImageGem dataset enables, for the first time, a new paradigm for generative model personalization.</li>
<li><strong>摘要：</strong>我们引入 ImageGem，这是一个用于研究生成模型的数据集，该模型了解细粒度的个人偏好。我们认为阻碍这种生成模型发展的一个关键挑战是缺乏野外和细粒度的用户偏好注释。我们的数据集包含来自 57K 用户的真实交互数据，这些用户总共构建了 242K 个定制 LoRA，编写了 300 万条文本提示，并创建了 500 万张生成图像。通过数据集中的用户偏好注释，我们能够训练更好的偏好对齐模型。此外，利用个人用户偏好，我们研究了检索模型和视觉语言模型在个性化图像检索和生成模型推荐方面的性能。最后，我们提出了一个端到端框架，用于在潜在权重空间中编辑定制的扩散模型，以符合个人用户的偏好。我们的结果表明，ImageGem 数据集首次为生成模型个性化提供了新的范式。</li>
</ul>

<h3>Title: LAND: Lung and Nodule Diffusion for 3D Chest CT Synthesis with Anatomical Guidance</h3>
<ul>
<li><strong>Authors: </strong>Anna Oliveras, Roger Marí, Rafael Redondo, Oriol Guardià, Ana Tost, Bhalaji Nagarajan, Carolina Migliorelli, Vicent Ribas, Petia Radeva</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18446">https://arxiv.org/abs/2510.18446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18446">https://arxiv.org/pdf/2510.18446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18446]] LAND: Lung and Nodule Diffusion for 3D Chest CT Synthesis with Anatomical Guidance(https://arxiv.org/abs/2510.18446)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This work introduces a new latent diffusion model to generate high-quality 3D chest CT scans conditioned on 3D anatomical masks. The method synthesizes volumetric images of size 256x256x256 at 1 mm isotropic resolution using a single mid-range GPU, significantly lowering the computational cost compared to existing approaches. The conditioning masks delineate lung and nodule regions, enabling precise control over the output anatomical features. Experimental results demonstrate that conditioning solely on nodule masks leads to anatomically incorrect outputs, highlighting the importance of incorporating global lung structure for accurate conditional synthesis. The proposed approach supports the generation of diverse CT volumes with and without lung nodules of varying attributes, providing a valuable tool for training AI models or healthcare professionals.</li>
<li><strong>摘要：</strong>这项工作引入了一种新的潜在扩散模型，可生成以 3D 解剖面罩为条件的高质量 3D 胸部 CT 扫描。该方法使用单个中档 GPU 以 1 毫米各向同性分辨率合成尺寸为 256x256x256 的体积图像，与现有方法相比，显着降低了计算成本。调节面罩描绘出肺部和结节区域，从而能够精确控制输出的解剖特征。实验结果表明，仅对结节掩模进行调节会导致解剖学上不正确的输出，这凸显了整合全局肺结构对于准确的条件合成的重要性。所提出的方法支持生成具有或不具有不同属性的肺结节的不同 CT 体积，为培训 AI 模型或医疗保健专业人员提供了宝贵的工具。</li>
</ul>

<h3>Title: Zero-Shot Vehicle Model Recognition via Text-Based Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Wei-Chia Chang, Yan-Ann Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18502">https://arxiv.org/abs/2510.18502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18502">https://arxiv.org/pdf/2510.18502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18502]] Zero-Shot Vehicle Model Recognition via Text-Based Retrieval-Augmented Generation(https://arxiv.org/abs/2510.18502)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vehicle make and model recognition (VMMR) is an important task in intelligent transportation systems, but existing approaches struggle to adapt to newly released models. Contrastive Language-Image Pretraining (CLIP) provides strong visual-text alignment, yet its fixed pretrained weights limit performance without costly image-specific finetuning. We propose a pipeline that integrates vision language models (VLMs) with Retrieval-Augmented Generation (RAG) to support zero-shot recognition through text-based reasoning. A VLM converts vehicle images into descriptive attributes, which are compared against a database of textual features. Relevant entries are retrieved and combined with the description to form a prompt, and a language model (LM) infers the make and model. This design avoids large-scale retraining and enables rapid updates by adding textual descriptions of new vehicles. Experiments show that the proposed method improves recognition by nearly 20% over the CLIP baseline, demonstrating the potential of RAG-enhanced LM reasoning for scalable VMMR in smart-city applications.</li>
<li><strong>摘要：</strong>车辆品牌和型号识别（VMMR）是智能交通系统中的一项重要任务，但现有方法很难适应新发布的车型。对比语言图像预训练 (CLIP) 提供了强大的视觉文本对齐功能，但其固定的预训练权重限制了性能，而无需进行昂贵的特定于图像的微调。我们提出了一种将视觉语言模型（VLM）与检索增强生成（RAG）集成的管道，以通过基于文本的推理支持零样本识别。 VLM 将车辆图像转换为描述性属性，然后与文本特征数据库进行比较。检索相关条目并将其与描述结合起来形成提示，语言模型 (LM) 推断品牌和型号。这种设计避免了大规模的再训练，并通过添加新车的文字描述来实现快速更新。实验表明，所提出的方法比 CLIP 基线提高了近 20% 的识别率，展示了 RAG 增强型 LM 推理在智慧城市应用中可扩展 VMMR 的潜力。</li>
</ul>

<h3>Title: DWaste: Greener AI for Waste Sorting using Mobile and Edge Devices</h3>
<ul>
<li><strong>Authors: </strong>Suman Kunwar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18513">https://arxiv.org/abs/2510.18513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18513">https://arxiv.org/pdf/2510.18513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18513]] DWaste: Greener AI for Waste Sorting using Mobile and Edge Devices(https://arxiv.org/abs/2510.18513)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The rise of convenience packaging has led to generation of enormous waste, making efficient waste sorting crucial for sustainable waste management. To address this, we developed DWaste, a computer vision-powered platform designed for real-time waste sorting on resource-constrained smartphones and edge devices, including offline functionality. We benchmarked various image classification models (EfficientNetV2S/M, ResNet50/101, MobileNet) and object detection (YOLOv8n, YOLOv11n) using a subset of our own waste data set and annotated it using the custom tool Annotated Lab. We found a clear trade-off between accuracy and resource consumption: the best classifier, EfficientNetV2S, achieved high accuracy (~ 96%) but suffered from high latency (~ 0.22s) and elevated carbon emissions. In contrast, lightweight object detection models delivered strong performance (up to 77% mAP) with ultra-fast inference (~ 0.03s) and significantly smaller model sizes (< 7MB), making them ideal for real-time, low-power use. Model quantization further maximized efficiency, substantially reducing model size and VRAM usage by up to 75%. Our work demonstrates the successful implementation of "Greener AI" models to support real-time, sustainable waste sorting on edge devices.</li>
<li><strong>摘要：</strong>便利包装的兴起导致了大量废物的产生，因此有效的废物分类对于可持续废物管理至关重要。为了解决这个问题，我们开发了 DWaste，这是一个计算机视觉驱动的平台，专为在资源有限的智能手机和边缘设备上进行实时垃圾分类而设计，包括离线功能。我们使用我们自己的废物数据集的子集对各种图像分类模型（EfficientNetV2S/M、ResNet50/101、MobileNet）和对象检测（YOLOv8n、YOLOv11n）进行基准测试，并使用自定义工具 Annotated Lab 对其进行注释。我们发现准确性和资源消耗之间存在明显的权衡：最好的分类器 EfficientNetV2S 实现了高精度（约 96%），但存在高延迟（约 0.22 秒）和碳排放增加。相比之下，轻量级目标检测模型提供了强大的性能（高达 77% mAP）、超快推理（约 0.03 秒）和显着更小的模型大小（< 7MB），使其成为实时、低功耗使用的理想选择。模型量化进一步最大限度地提高了效率，大幅减少了模型大小和 VRAM 使用量高达 75%。我们的工作展示了“绿色人工智能”模型的成功实施，以支持边缘设备上的实时、可持续废物分类。</li>
</ul>

<h3>Title: Descriptor: Occluded nuScenes: A Multi-Sensor Dataset for Evaluating Perception Robustness in Automated Driving</h3>
<ul>
<li><strong>Authors: </strong>Sanjay Kumar, Tim Brophy, Reenu Mohandas, Eoin Martino Grua, Ganesh Sistu, Valentina Donzella, Ciaran Eising</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18552">https://arxiv.org/abs/2510.18552</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18552">https://arxiv.org/pdf/2510.18552</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18552]] Descriptor: Occluded nuScenes: A Multi-Sensor Dataset for Evaluating Perception Robustness in Automated Driving(https://arxiv.org/abs/2510.18552)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Robust perception in automated driving requires reliable performance under adverse conditions, where sensors may be affected by partial failures or environmental occlusions. Although existing autonomous driving datasets inherently contain sensor noise and environmental variability, very few enable controlled, parameterised, and reproducible degradations across multiple sensing modalities. This gap limits the ability to systematically evaluate how perception and fusion architectures perform under well-defined adverse conditions. To address this limitation, we introduce the Occluded nuScenes Dataset, a novel extension of the widely used nuScenes benchmark. For the camera modality, we release both the full and mini versions with four types of occlusions, two adapted from public implementations and two newly designed. For radar and LiDAR, we provide parameterised occlusion scripts that implement three types of degradations each, enabling flexible and repeatable generation of occluded data. This resource supports consistent, reproducible evaluation of perception models under partial sensor failures and environmental interference. By releasing the first multi-sensor occlusion dataset with controlled and reproducible degradations, we aim to advance research on robust sensor fusion, resilience analysis, and safety-critical perception in automated driving.</li>
<li><strong>摘要：</strong>自动驾驶中的鲁棒感知需要在不利条件下具有可靠的性能，在这种条件下，传感器可能会受到部分故障或环境遮挡的影响。尽管现有的自动驾驶数据集本质上包含传感器噪声和环境变化，但很少有数据集能够跨多种传感模式实现受控、参数化和可重现的退化。这一差距限制了系统评估感知和融合架构在明确定义的不利条件下如何执行的能力。为了解决这个限制，我们引入了被遮挡的 nuScenes 数据集，这是广泛使用的 nuScenes 基准的新颖扩展。对于相机模式，我们发布了完整版本和迷你版本，具有四种类型的遮挡，其中两种是根据公共实现改编的，两种是新设计的。对于雷达和激光雷达，我们提供参数化遮挡脚本，分别实现三种类型的降级，从而能够灵活且可重复地生成遮挡数据。该资源支持在部分传感器故障和环境干扰下对感知模型进行一致、可重复的评估。通过发布第一个具有受控和可重现退化的多传感器遮挡数据集，我们的目标是推进自动驾驶中鲁棒传感器融合、弹性分析和安全关键感知的研究。</li>
</ul>

<h3>Title: Kaleido: Open-Sourced Multi-Subject Reference Video Generation Model</h3>
<ul>
<li><strong>Authors: </strong>Zhenxing Zhang, Jiayan Teng, Zhuoyi Yang, Tiankun Cao, Cheng Wang, Xiaotao Gu, Jie Tang, Dan Guo, Meng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18573">https://arxiv.org/abs/2510.18573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18573">https://arxiv.org/pdf/2510.18573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18573]] Kaleido: Open-Sourced Multi-Subject Reference Video Generation Model(https://arxiv.org/abs/2510.18573)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present Kaleido, a subject-to-video~(S2V) generation framework, which aims to synthesize subject-consistent videos conditioned on multiple reference images of target subjects. Despite recent progress in S2V generation models, existing approaches remain inadequate at maintaining multi-subject consistency and at handling background disentanglement, often resulting in lower reference fidelity and semantic drift under multi-image conditioning. These shortcomings can be attributed to several factors. Primarily, the training dataset suffers from a lack of diversity and high-quality samples, as well as cross-paired data, i.e., paired samples whose components originate from different instances. In addition, the current mechanism for integrating multiple reference images is suboptimal, potentially resulting in the confusion of multiple subjects. To overcome these limitations, we propose a dedicated data construction pipeline, incorporating low-quality sample filtering and diverse data synthesis, to produce consistency-preserving training data. Moreover, we introduce Reference Rotary Positional Encoding (R-RoPE) to process reference images, enabling stable and precise multi-image integration. Extensive experiments across numerous benchmarks demonstrate that Kaleido significantly outperforms previous methods in consistency, fidelity, and generalization, marking an advance in S2V generation.</li>
<li><strong>摘要：</strong>我们提出了 Kaleido，一种主题到视频（S2V）生成框架，旨在根据目标主题的多个参考图像合成主题一致的视频。尽管 S2V 生成模型最近取得了进展，但现有方法在维持多主体一致性和处理背景解开方面仍然不足，通常会​​导致多图像条件下的参考保真度较低和语义漂移。这些缺点可归因于几个因素。主要是，训练数据集缺乏多样性和高质量样本，以及交叉配对数据，即组成部分来自不同实例的配对样本。此外，当前整合多个参考图像的机制并不理想，可能会导致多个对象的混淆。为了克服这些限制，我们提出了一个专用的数据构建管道，结合低质量样本过滤和多样化的数据合成，以生成保持一致性的训练数据。此外，我们引入参考旋转位置编码（R-RoPE）来处理参考图像，从而实现稳定且精确的多图像集成。跨多个基准的大量实验表明，Kaleido 在一致性、保真度和泛化方面显着优于以前的方法，标志着 S2V 生成的进步。</li>
</ul>

<h3>Title: MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Weinan Jia, Yuning Lu, Mengqi Huang, Hualiang Wang, Binyuan Huang, Nan Chen, Mu Liu, Jidong Jiang, Zhendong Mao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18692">https://arxiv.org/abs/2510.18692</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18692">https://arxiv.org/pdf/2510.18692</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18692]] MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation(https://arxiv.org/abs/2510.18692)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Long video generation with Diffusion Transformers (DiTs) is bottlenecked by the quadratic scaling of full attention with sequence length. Since attention is highly redundant, outputs are dominated by a small subset of query-key pairs. Existing sparse methods rely on blockwise coarse estimation, whose accuracy-efficiency trade-offs are constrained by block size. This paper introduces Mixture-of-Groups Attention (MoGA), an efficient sparse attention that uses a lightweight, learnable token router to precisely match tokens without blockwise estimation. Through semantic-aware routing, MoGA enables effective long-range interactions. As a kernel-free method, MoGA integrates seamlessly with modern attention stacks, including FlashAttention and sequence parallelism. Building on MoGA, we develop an efficient long video generation model that end-to-end produces minute-level, multi-shot, 480p videos at 24 fps, with a context length of approximately 580k. Comprehensive experiments on various video generation tasks validate the effectiveness of our approach.</li>
<li><strong>摘要：</strong>使用扩散变压器（DiT）生成长视频的瓶颈是完全注意力与序列长度的二次缩放。由于注意力高度冗余，输出由一小部分查询密钥对主导。现有的稀疏方法依赖于分块粗略估计，其精度-效率权衡受到块大小的限制。本文介绍了混合组注意力（MoGA），这是一种高效的稀疏注意力，它使用轻量级、可学习的令牌路由器来精确匹配令牌，而无需按块进行估计。通过语义感知路由，MoGA 可实现有效的远程交互。作为一种无内核方法，MoGA 与现代注意力堆栈无缝集成，包括 FlashAttention 和序列并行性。在 MoGA 的基础上，我们开发了一种高效的长视频生成模型，该模型可以端到端地以 24 fps 生成分钟级、多镜头、480p 视频，上下文长度约为 580k。对各种视频生成任务的综合实验验证了我们方法的有效性。</li>
</ul>

<h3>Title: UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Yibin Wang, Zhimin Li, Yuhang Zang, Jiazi Bu, Yujie Zhou, Yi Xin, Junjun He, Chunyu Wang, Qinglin Lu, Cheng Jin, Jiaqi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18701">https://arxiv.org/abs/2510.18701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18701">https://arxiv.org/pdf/2510.18701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18701]] UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image Generation(https://arxiv.org/abs/2510.18701)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent progress in text-to-image (T2I) generation underscores the importance of reliable benchmarks in evaluating how accurately generated images reflect the semantics of their textual prompt. However, (1) existing benchmarks lack the diversity of prompt scenarios and multilingual support, both essential for real-world applicability; (2) they offer only coarse evaluations across primary dimensions, covering a narrow range of sub-dimensions, and fall short in fine-grained sub-dimension assessment. To address these limitations, we introduce UniGenBench++, a unified semantic assessment benchmark for T2I generation. Specifically, it comprises 600 prompts organized hierarchically to ensure both coverage and efficiency: (1) spans across diverse real-world scenarios, i.e., 5 main prompt themes and 20 subthemes; (2) comprehensively probes T2I models' semantic consistency over 10 primary and 27 sub evaluation criteria, with each prompt assessing multiple testpoints. To rigorously assess model robustness to variations in language and prompt length, we provide both English and Chinese versions of each prompt in short and long forms. Leveraging the general world knowledge and fine-grained image understanding capabilities of a closed-source Multi-modal Large Language Model (MLLM), i.e., Gemini-2.5-Pro, an effective pipeline is developed for reliable benchmark construction and streamlined model assessment. Moreover, to further facilitate community use, we train a robust evaluation model that enables offline assessment of T2I model outputs. Through comprehensive benchmarking of both open- and closed-sourced T2I models, we systematically reveal their strengths and weaknesses across various aspects.</li>
<li><strong>摘要：</strong>文本到图像 (T2I) 生成的最新进展强调了可靠基准在评估生成的图像如何准确地反映其文本提示的语义方面的重要性。然而，（1）现有基准缺乏提示场景的多样性和多语言支持，而这对于现实世界的适用性至关重要； (2)仅对主要维度进行粗略评估，涵盖的子维度范围较窄，细粒度的子维度评估不足。为了解决这些限制，我们引入了 UniGenBench++，这是一个用于 T2I 生成的统一语义评估基准。具体来说，它由600个提示组成，分层组织，以确保覆盖范围和效率：（1）跨越不同的现实场景，即5个主要提示主题和20个副主题； (2)全面探讨T2I模型在10个主要和27个子评估标准上的语义一致性，每个评估标准评估多个测试点。为了严格评估模型对语言和提示长度变化的鲁棒性，我们提供了每个提示的英文和中文版本的短形式和长形式。利用闭源多模态大语言模型（MLLM）（即 Gemini-2.5-Pro）的一般世界知识和细粒度图像理解能力，开发了一个有效的管道，用于可靠的基准构建和简化的模型评估。此外，为了进一步促进社区使用，我们训练了一个强大的评估模型，可以对 T2I 模型输出进行离线评估。通过对开源和闭源 T2I 模型进行全面的基准测试，我们系统地揭示了它们在各个方面的优势和劣势。</li>
</ul>

<h3>Title: SSD: Spatial-Semantic Head Decoupling for Efficient Autoregressive Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Siyong Jian, Huan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18716">https://arxiv.org/abs/2510.18716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18716">https://arxiv.org/pdf/2510.18716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18716]] SSD: Spatial-Semantic Head Decoupling for Efficient Autoregressive Image Generation(https://arxiv.org/abs/2510.18716)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Autoregressive image generation models like Janus-Pro produce high-quality images, but at the significant cost of high memory and ever-growing computational demands due to the large number of visual tokens. While KV cache compression has been extensively studied in language modeling, it still remains largely unexplored for the image generation domain. In this work, we begin by identifying a distinct and prominent attention phenomenon, which we term spatial locality and emergent semantic sink. To leverage this key insight, we introduce a novel KV cache compression framework. Specifically, we compress the KV cache for all visual tokens by adaptively decoupling attention heads into two separate types: for spatial-locality heads, our method maintains a short recent token window; for semantic-sink heads, it strategically preserves a compact set of highly-attended tokens. Our extensive experiments demonstrate that the proposed method achieves a 5$\times$ reduction in memory usage and a notable 6.6$\times$ speedup in overall throughput with only minimal visual quality loss, thereby enabling highly efficient native autoregressive image generation on resource-constrained hardware.</li>
<li><strong>摘要：</strong>像 Janus-Pro 这样的自回归图像生成模型可以生成高质量的图像，但由于大量的视觉标记，内存成本很高，而且计算需求不断增长。虽然 KV 缓存压缩已在语言建模中得到广泛研究，但在图像生成领域中仍然很大程度上未被探索。在这项工作中，我们首先确定一种独特且突出的注意现象，我们将其称为空间局部性和新兴语义汇。为了利用这一关键见解，我们引入了一种新颖的 KV 缓存压缩框架。具体来说，我们通过自适应地将注意力头解耦为两种不同的类型来压缩所有视觉令牌的 KV 缓存：对于空间局部性头，我们的方法维护一个短的最近令牌窗口；对于语义接收器头，它战略性地保留了一组紧凑的高度关注的标记。我们的大量实验表明，所提出的方法可将内存使用量减少 5$\times$，并将整体吞吐量显着提高 6.6$\times$，同时视觉质量损失极小，从而在资源受限的硬件上实现高效的本机自回归图像生成。</li>
</ul>

<h3>Title: Improving the Generation and Evaluation of Synthetic Data for Downstream Medical Causal Inference</h3>
<ul>
<li><strong>Authors: </strong>Harry Amad, Zhaozhi Qian, Dennis Frauen, Julianna Piskorz, Stefan Feuerriegel, Mihaela van der Schaar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18768">https://arxiv.org/abs/2510.18768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18768">https://arxiv.org/pdf/2510.18768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18768]] Improving the Generation and Evaluation of Synthetic Data for Downstream Medical Causal Inference(https://arxiv.org/abs/2510.18768)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Causal inference is essential for developing and evaluating medical interventions, yet real-world medical datasets are often difficult to access due to regulatory barriers. This makes synthetic data a potentially valuable asset that enables these medical analyses, along with the development of new inference methods themselves. Generative models can produce synthetic data that closely approximate real data distributions, yet existing methods do not consider the unique challenges that downstream causal inference tasks, and specifically those focused on treatments, pose. We establish a set of desiderata that synthetic data containing treatments should satisfy to maximise downstream utility: preservation of (i) the covariate distribution, (ii) the treatment assignment mechanism, and (iii) the outcome generation mechanism. Based on these desiderata, we propose a set of evaluation metrics to assess such synthetic data. Finally, we present STEAM: a novel method for generating Synthetic data for Treatment Effect Analysis in Medicine that mimics the data-generating process of data containing treatments and optimises for our desiderata. We empirically demonstrate that STEAM achieves state-of-the-art performance across our metrics as compared to existing generative models, particularly as the complexity of the true data-generating process increases.</li>
<li><strong>摘要：</strong>因果推断对于开发和评估医疗干预措施至关重要，但由于监管障碍，现实世界的医疗数据集通常难以访问。这使得合成数据成为一种潜在的有价值的资产，可以实现这些医学分析以及新的推理方法本身的开发。生成模型可以生成非常接近真实数据分布的合成数据，但现有方法没有考虑下游因果推理任务，特别是那些专注于治疗的任务所带来的独特挑战。我们建立了一组包含处理的合成数据应满足的需求，以最大化下游效用：保留（i）协变量分布，（ii）处理分配机制，以及（iii）结果生成机制。基于这些需求，我们提出了一组评估指标来评估此类合成数据。最后，我们提出了 STEAM：一种用于生成医学治疗效果分析综合数据的新方法，它模仿包含治疗的数据的数据生成过程，并针对我们的需求进行优化。我们凭经验证明，与现有的生成模型相比，STEAM 在我们的指标上实现了最先进的性能，特别是当真实数据生成过程的复杂性增加时。</li>
</ul>

<h3>Title: UltraGen: High-Resolution Video Generation with Hierarchical Attention</h3>
<ul>
<li><strong>Authors: </strong>Teng Hu, Jiangning Zhang, Zihan Su, Ran Yi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18775">https://arxiv.org/abs/2510.18775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18775">https://arxiv.org/pdf/2510.18775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18775]] UltraGen: High-Resolution Video Generation with Hierarchical Attention(https://arxiv.org/abs/2510.18775)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generation</a></li>
<li><strong>Abstract: </strong>Recent advances in video generation have made it possible to produce visually compelling videos, with wide-ranging applications in content creation, entertainment, and virtual reality. However, most existing diffusion transformer based video generation models are limited to low-resolution outputs (<=720P) due to the quadratic computational complexity of the attention mechanism with respect to the output width and height. This computational bottleneck makes native high-resolution video generation (1080P/2K/4K) impractical for both training and inference. To address this challenge, we present UltraGen, a novel video generation framework that enables i) efficient and ii) end-to-end native high-resolution video synthesis. Specifically, UltraGen features a hierarchical dual-branch attention architecture based on global-local attention decomposition, which decouples full attention into a local attention branch for high-fidelity regional content and a global attention branch for overall semantic consistency. We further propose a spatially compressed global modeling strategy to efficiently learn global dependencies, and a hierarchical cross-window local attention mechanism to reduce computational costs while enhancing information flow across different local windows. Extensive experiments demonstrate that UltraGen can effectively scale pre-trained low-resolution video models to 1080P and even 4K resolution for the first time, outperforming existing state-of-the-art methods and super-resolution based two-stage pipelines in both qualitative and quantitative evaluations.</li>
<li><strong>摘要：</strong>视频生成领域的最新进展使得制作视觉上引人注目的视频成为可能，并在内容创建、娱乐和虚拟现实方面具有广泛的应用。然而，由于注意力机制相对于输出宽度和高度的二次计算复杂性，大多数现有的基于扩散变压器的视频生成模型仅限于低分辨率输出（<=720P）。这种计算瓶颈使得原生高分辨率视频生成 (1080P/2K/4K) 对于训练和推理来说都是不切实际的。为了应对这一挑战，我们推出了 UltraGen，这是一种新颖的视频生成框架，可实现 i) 高效和 ii) 端到端原生高分辨率视频合成。具体来说，UltraGen 采用基于全局-局部注意力分解的分层双分支注意力架构，它将全部注意力解耦为用于高保真区域内容的局部注意力分支和用于整体语义一致性的全局注意力分支。我们进一步提出了一种空间压缩的全局建模策略来有效地学习全局依赖性，以及一种分层的跨窗口局部注意机制来减少计算成本，同时增强跨不同局部窗口的信息流。大量实验表明，UltraGen首次可以有效地将预训练的低分辨率视频模型扩展到1080P甚至4K分辨率，在定性和定量评估方面均优于现有最先进的方法和基于超分辨率的两级管道。</li>
</ul>

<h3>Title: Search Self-play: Pushing the Frontier of Agent Capability without Supervision</h3>
<ul>
<li><strong>Authors: </strong>Hongliang Lu, Yuhang Wen, Pengyu Cheng, Ruijin Ding, Haotian Xu, Jiaqi Guo, Chutian Wang, Haonan Chen, Xiaoxi Jiang, Guanjun Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18821">https://arxiv.org/abs/2510.18821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18821">https://arxiv.org/pdf/2510.18821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18821]] Search Self-play: Pushing the Frontier of Agent Capability without Supervision(https://arxiv.org/abs/2510.18821)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reinforcement learning with verifiable rewards (RLVR) has become the mainstream technique for training LLM agents. However, RLVR highly depends on well-crafted task queries and corresponding ground-truth answers to provide accurate rewards, which requires massive human efforts and hinders the RL scaling processes, especially under agentic scenarios. Although a few recent works explore task synthesis methods, the difficulty of generated agentic tasks can hardly be controlled to provide effective RL training advantages. To achieve agentic RLVR with higher scalability, we explore self-play training for deep search agents, in which the learning LLM utilizes multi-turn search engine calling and acts simultaneously as both a task proposer and a problem solver. The task proposer aims to generate deep search queries with well-defined ground-truth answers and increasing task difficulty. The problem solver tries to handle the generated search queries and output the correct answer predictions. To ensure that each generated search query has accurate ground truth, we collect all the searching results from the proposer's trajectory as external knowledge, then conduct retrieval-augmentation generation (RAG) to test whether the proposed query can be correctly answered with all necessary search documents provided. In this search self-play (SSP) game, the proposer and the solver co-evolve their agent capabilities through both competition and cooperation. With substantial experimental results, we find that SSP can significantly improve search agents' performance uniformly on various benchmarks without any supervision under both from-scratch and continuous RL training setups. The code is at this https URL.</li>
<li><strong>摘要：</strong>具有可验证奖励的强化学习（RLVR）已成为训练 LLM 代理的主流技术。然而，RLVR 高度依赖于精心设计的任务查询和相应的真实答案来提供准确的奖励，这需要大量的人力并阻碍 RL 扩展过程，尤其是在代理场景下。尽管最近的一些工作探索了任务合成方法，但生成的代理任务的难度很难控制以提供有效的强化学习训练优势。为了实现具有更高可扩展性的代理 RLVR，我们探索了深度搜索代理的自玩训练，其中学习 LLM 利用多轮搜索引擎调用，并同时充当任务提议者和问题解决者。任务提议者的目标是生成具有明确定义的真实答案的深度搜索查询并增加任务难度。问题解决器尝试处理生成的搜索查询并输出正确的答案预测。为了确保每个生成的搜索查询具有准确的地面事实，我们从提议者的轨迹中收集所有搜索结果作为外部知识，然后进行检索增强生成（RAG）以测试所提供的所有必要搜索文档是否可以正确回答所提出的查询。在这个搜索自我博弈（SSP）游戏中，提议者和求解者通过竞争和合作共同进化他们的代理能力。通过大量的实验结果，我们发现 SSP 可以在从头开始和连续 RL 训练设置下无需任何监督的情况下，在各种基准上统一显着提高搜索代理的性能。代码位于此 https URL。</li>
</ul>

<h3>Title: A Hybrid Enumeration Framework for Optimal Counterfactual Generation in Post-Acute COVID-19 Heart Failure</h3>
<ul>
<li><strong>Authors: </strong>Jingya Cheng, Alaleh Azhir, Jiazi Tian, Hossein Estiri</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18841">https://arxiv.org/abs/2510.18841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18841">https://arxiv.org/pdf/2510.18841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18841]] A Hybrid Enumeration Framework for Optimal Counterfactual Generation in Post-Acute COVID-19 Heart Failure(https://arxiv.org/abs/2510.18841)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Counterfactual inference provides a mathematical framework for reasoning about hypothetical outcomes under alternative interventions, bridging causal reasoning and predictive modeling. We present a counterfactual inference framework for individualized risk estimation and intervention analysis, illustrated through a clinical application to post-acute sequelae of COVID-19 (PASC) among patients with pre-existing heart failure (HF). Using longitudinal diagnosis, laboratory, and medication data from a large health-system cohort, we integrate regularized predictive modeling with counterfactual search to identify actionable pathways to PASC-related HF hospital admissions. The framework combines exact enumeration with optimization-based methods, including the Nearest Instance Counterfactual Explanations (NICE) and Multi-Objective Counterfactuals (MOC) algorithms, to efficiently explore high-dimensional intervention spaces. Applied to more than 2700 individuals with confirmed SARS-CoV-2 infection and prior HF, the model achieved strong discriminative performance (AUROC: 0.88, 95% CI: 0.84-0.91) and generated interpretable, patient-specific counterfactuals that quantify how modifying comorbidity patterns or treatment factors could alter predicted outcomes. This work demonstrates how counterfactual reasoning can be formalized as an optimization problem over predictive functions, offering a rigorous, interpretable, and computationally efficient approach to personalized inference in complex biomedical systems.</li>
<li><strong>摘要：</strong>反事实推理提供了一个数学框架，用于推理替代干预下的假设结果，连接因果推理和预测模型。我们提出了一个用于个体化风险评估和干预分析的反事实推理框架，通过对既往心力衰竭 (HF) 患者的 COVID-19 (PASC) 急性后遗症的临床应用进行说明。利用来自大型卫生系统队列的纵向诊断、实验室和药物数据，我们将正则化预测模型与反事实搜索相结合，以确定与 PASC 相关的心力衰竭入院的可行途径。该框架将精确枚举与基于优化的方法相结合，包括最近实例反事实解释（NICE）和多目标反事实（MOC）算法，以有效地探索高维干预空间。该模型应用于 2700 多名确诊感染 SARS-CoV-2 且既往有心力衰竭的个体，取得了很强的判别性能（AUROC：0.88，95% CI：0.84-0.91），并生成了可解释的、针对患者的反事实，量化了修改共病模式或治疗因素如何改变预测结果。这项工作演示了如何将反事实推理形式化为预测函数的优化问题，为复杂生物医学系统中的个性化推理提供严格、可解释且计算高效的方法。</li>
</ul>

<h3>Title: DP$^2$O-SR: Direct Perceptual Preference Optimization for Real-World Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Rongyuan Wu, Lingchen Sun, Zhengqiang Zhang, Shihao Wang, Tianhe Wu, Qiaosi Yi, Shuai Li, Lei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18851">https://arxiv.org/abs/2510.18851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18851">https://arxiv.org/pdf/2510.18851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18851]] DP$^2$O-SR: Direct Perceptual Preference Optimization for Real-World Image Super-Resolution(https://arxiv.org/abs/2510.18851)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generative, quality assessment</a></li>
<li><strong>Abstract: </strong>Benefiting from pre-trained text-to-image (T2I) diffusion models, real-world image super-resolution (Real-ISR) methods can synthesize rich and realistic details. However, due to the inherent stochasticity of T2I models, different noise inputs often lead to outputs with varying perceptual quality. Although this randomness is sometimes seen as a limitation, it also introduces a wider perceptual quality range, which can be exploited to improve Real-ISR performance. To this end, we introduce Direct Perceptual Preference Optimization for Real-ISR (DP$^2$O-SR), a framework that aligns generative models with perceptual preferences without requiring costly human annotations. We construct a hybrid reward signal by combining full-reference and no-reference image quality assessment (IQA) models trained on large-scale human preference datasets. This reward encourages both structural fidelity and natural appearance. To better utilize perceptual diversity, we move beyond the standard best-vs-worst selection and construct multiple preference pairs from outputs of the same model. Our analysis reveals that the optimal selection ratio depends on model capacity: smaller models benefit from broader coverage, while larger models respond better to stronger contrast in supervision. Furthermore, we propose hierarchical preference optimization, which adaptively weights training pairs based on intra-group reward gaps and inter-group diversity, enabling more efficient and stable learning. Extensive experiments across both diffusion- and flow-based T2I backbones demonstrate that DP$^2$O-SR significantly improves perceptual quality and generalizes well to real-world benchmarks.</li>
<li><strong>摘要：</strong>受益于预先训练的文本到图像 (T2I) 扩散模型，真实图像超分辨率 (Real-ISR) 方法可以合成丰富且真实的细节。然而，由于 T2I 模型固有的随机性，不同的噪声输入通常会导致输出具有不同的感知质量。尽管这种随机性有时被视为一种限制，但它也引入了更广泛的感知质量范围，可用于提高 Real-ISR 性能。为此，我们引入了 Real-ISR 的直接感知偏好优化（DP$^2$O-SR），该框架将生成模型与感知偏好保持一致，而不需要昂贵的人工注释。我们通过结合在大规模人类偏好数据集上训练的全参考和无参考图像质量评估（IQA）模型来构建混合奖励信号。这种奖励鼓励结构保真度和自然外观。为了更好地利用感知多样性，我们超越了标准的最佳与最差选择，并根据同一模型的输出构建了多个偏好对。我们的分析表明，最佳选择率取决于模型容量：较小的模型受益于更广泛的覆盖范围，而较大的模型对更强的监督对比反应更好。此外，我们提出了分层偏好优化，它根据组内奖励差距和组间多样性自适应地对训练对进行加权，从而实现更高效、更稳定的学习。基于扩散和基于流的 T2I 主干网的大量实验表明，DP$^2$O-SR 显着提高了感知质量，并很好地推广到现实世界的基准。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
