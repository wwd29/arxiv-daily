<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-12-02</h1>
<h3>Title: Adapter Shield: A Unified Framework with Built-in Authentication for Preventing Unauthorized Zero-Shot Image-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Jun Jia, Hongyi Miao, Yingjie Zhou, Wangqiu Zhou, Jianbo Zhang, Linhan Cao, Dandan Zhu, Hua Yang, Xiongkuo Min, Wei Sun, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00075">https://arxiv.org/abs/2512.00075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00075">https://arxiv.org/pdf/2512.00075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00075]] Adapter Shield: A Unified Framework with Built-in Authentication for Preventing Unauthorized Zero-Shot Image-to-Image Generation(https://arxiv.org/abs/2512.00075)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>With the rapid progress in diffusion models, image synthesis has advanced to the stage of zero-shot image-to-image generation, where high-fidelity replication of facial identities or artistic styles can be achieved using just one portrait or artwork, without modifying any model weights. Although these techniques significantly enhance creative possibilities, they also pose substantial risks related to intellectual property violations, including unauthorized identity cloning and stylistic imitation. To counter such threats, this work presents Adapter Shield, the first universal and authentication-integrated solution aimed at defending personal images from misuse in zero-shot generation scenarios. We first investigate how current zero-shot methods employ image encoders to extract embeddings from input images, which are subsequently fed into the UNet of diffusion models through cross-attention layers. Inspired by this mechanism, we construct a reversible encryption system that maps original embeddings into distinct encrypted representations according to different secret keys. The authorized users can restore the authentic embeddings via a decryption module and the correct key, enabling normal usage for authorized generation tasks. For protection purposes, we design a multi-target adversarial perturbation method that actively shifts the original embeddings toward designated encrypted patterns. Consequently, protected images are embedded with a defensive layer that ensures unauthorized users can only produce distorted or encrypted outputs. Extensive evaluations demonstrate that our method surpasses existing state-of-the-art defenses in blocking unauthorized zero-shot image synthesis, while supporting flexible and secure access control for verified users.</li>
<li><strong>摘要：</strong>随着扩散模型的快速进步，图像合成已发展到零样本图像到图像生成的阶段，仅使用一幅肖像或艺术品即可实现面部身份或艺术风格的高保真复制，而无需修改任何模型权重。尽管这些技术显着增强了创造的可能性，但它们也带来了与知识产权侵权相关的巨大风险，包括未经授权的身份克隆和风格模仿。为了应对此类威胁，这项工作提出了 Adapter Shield，这是第一个通用的身份验证集成解决方案，旨在防止个人图像在零样本生成场景中被滥用。我们首先研究当前的零样本方法如何使用图像编码器从输入图像中提取嵌入，然后通过交叉注意力层将其输入到扩散模型的 UNet 中。受这种机制的启发，我们构建了一个可逆加密系统，根据不同的密钥将原始嵌入映射为不同的加密表示。授权用户可以通过解密模块和正确的密钥恢复真实的嵌入，从而使授权生成任务能够正常使用。出于保护目的，我们设计了一种多目标对抗扰动方法，该方法主动将原始嵌入转变为指定的加密模式。因此，受保护的图像嵌入了一个防御层，确保未经授权的用户只能产生扭曲或加密的输出。广泛的评估表明，我们的方法在阻止未经授权的零样本图像合成方面超越了现有的最先进的防御措施，同时支持对经过验证的用户进行灵活且安全的访问控制。</li>
</ul>

<h3>Title: Diffusion-Based Synthetic Brightfield Microscopy Images for Enhanced Single Cell Detection</h3>
<ul>
<li><strong>Authors: </strong>Mario de Jesus da Graca, Jörg Dahlkemper, Peer Stelldinger</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00078">https://arxiv.org/abs/2512.00078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00078">https://arxiv.org/pdf/2512.00078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00078]] Diffusion-Based Synthetic Brightfield Microscopy Images for Enhanced Single Cell Detection(https://arxiv.org/abs/2512.00078)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Accurate single cell detection in brightfield microscopy is crucial for biological research, yet data scarcity and annotation bottlenecks limit the progress of deep learning methods. We investigate the use of unconditional models to generate synthetic brightfield microscopy images and evaluate their impact on object detection performance. A U-Net based diffusion model was trained and used to create datasets with varying ratios of synthetic and real images. Experiments with YOLOv8, YOLOv9 and RT-DETR reveal that training with synthetic data can achieve improved detection accuracies (at minimal costs). A human expert survey demonstrates the high realism of generated images, with experts not capable to distinguish them from real microscopy images (accuracy 50%). Our findings suggest that diffusion-based synthetic data generation is a promising avenue for augmenting real datasets in microscopy image analysis, reducing the reliance on extensive manual annotation and potentially improving the robustness of cell detection models.</li>
<li><strong>摘要：</strong>明场显微镜中准确的单细胞检测对于生物学研究至关重要，但数据稀缺和注释瓶颈限制了深度学习方法的进展。我们研究使用无条件模型生成合成明场显微镜图像并评估其对物体检测性能的影响。基于 U-Net 的扩散模型经过训练并用于创建具有不同比例的合成图像和真实图像的数据集。 YOLOv8、YOLOv9 和 RT-DETR 的实验表明，使用合成数据进行训练可以提高检测精度（以最小的成本）。人类专家调查表明，生成的图像具有高度真实性，专家无法将它们与真实的显微镜图像区分开来（准确度 50%）。我们的研究结果表明，基于扩散的合成数据生成是增强显微图像分析中真实数据集、减少对大量手动注释的依赖并可能提高细胞检测模型稳健性的有前途的途径。</li>
</ul>

<h3>Title: A Fast and Efficient Modern BERT based Text-Conditioned Diffusion Model for Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Venkata Siddharth Dhara, Pawan Kumar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00084">https://arxiv.org/abs/2512.00084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00084">https://arxiv.org/pdf/2512.00084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00084]] A Fast and Efficient Modern BERT based Text-Conditioned Diffusion Model for Medical Image Segmentation(https://arxiv.org/abs/2512.00084)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In recent times, denoising diffusion probabilistic models (DPMs) have proven effective for medical image generation and denoising, and as representation learners for downstream segmentation. However, segmentation performance is limited by the need for dense pixel-wise labels, which are expensive, time-consuming, and require expert knowledge. We propose FastTextDiff, a label-efficient diffusion-based segmentation model that integrates medical text annotations to enhance semantic representations. Our approach uses ModernBERT, a transformer capable of processing long clinical notes, to tightly link textual annotations with semantic content in medical images. Trained on MIMIC-III and MIMIC-IV, ModernBERT encodes clinical knowledge that guides cross-modal attention between visual and textual features. This study validates ModernBERT as a fast, scalable alternative to Clinical BioBERT in diffusion-based segmentation pipelines and highlights the promise of multi-modal techniques for medical image analysis. By replacing Clinical BioBERT with ModernBERT, FastTextDiff benefits from FlashAttention 2, an alternating attention mechanism, and a 2-trillion-token corpus, improving both segmentation accuracy and training efficiency over traditional diffusion-based models.</li>
<li><strong>摘要：</strong>近年来，去噪扩散概率模型（DPM）已被证明对于医学图像生成和去噪以及作为下游分割的表示学习器是有效的。然而，分割性能受到对密集像素级标签的需求的限制，这些标签昂贵、耗时，并且需要专业知识。我们提出了 FastTextDiff，一种基于标签高效扩散的分割模型，它集成了医学文本注释以增强语义表示。我们的方法使用 ModernBERT（一种能够处理长临床笔记的转换器）将文本注释与医学图像中的语义内容紧密联系起来。 ModernBERT 经过 MIMIC-III 和 MIMIC-IV 的训练，对临床知识进行编码，指导视觉和文本特征之间的跨模式注意力。这项研究验证了 ModernBERT 在基于扩散的分割流程中是临床 BioBERT 的快速、可扩展的替代方案，并强调了多模态技术在医学图像分析中的前景。通过用 ModernBERT 替换 Clinical BioBERT，FastTextDiff 受益于 FlashAttention 2（一种交替注意力机制）和 2 万亿令牌语料库，与传统的基于扩散的模型相比，提高了分割准确性和训练效率。</li>
</ul>

<h3>Title: Hybrid Synthetic Data Generation with Domain Randomization Enables Zero-Shot Vision-Based Part Inspection Under Extreme Class Imbalance</h3>
<ul>
<li><strong>Authors: </strong>Ruo-Syuan Mei, Sixian Jia, Guangze Li, Soo Yeon Lee, Brian Musser, William Keller, Sreten Zakula, Jorge Arinez, Chenhui Shao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00125">https://arxiv.org/abs/2512.00125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00125">https://arxiv.org/pdf/2512.00125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00125]] Hybrid Synthetic Data Generation with Domain Randomization Enables Zero-Shot Vision-Based Part Inspection Under Extreme Class Imbalance(https://arxiv.org/abs/2512.00125)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Machine learning, particularly deep learning, is transforming industrial quality inspection. Yet, training robust machine learning models typically requires large volumes of high-quality labeled data, which are expensive, time-consuming, and labor-intensive to obtain in manufacturing. Moreover, defective samples are intrinsically rare, leading to severe class imbalance that degrades model performance. These data constraints hinder the widespread adoption of machine learning-based quality inspection methods in real production environments. Synthetic data generation (SDG) offers a promising solution by enabling the creation of large, balanced, and fully annotated datasets in an efficient, cost-effective, and scalable manner. This paper presents a hybrid SDG framework that integrates simulation-based rendering, domain randomization, and real background compositing to enable zero-shot learning for computer vision-based industrial part inspection without manual annotation. The SDG pipeline generates 12,960 labeled images in one hour by varying part geometry, lighting, and surface properties, and then compositing synthetic parts onto real image backgrounds. A two-stage architecture utilizing a YOLOv8n backbone for object detection and MobileNetV3-small for quality classification is trained exclusively on synthetic data and evaluated on 300 real industrial parts. The proposed approach achieves an mAP@0.5 of 0.995 for detection, 96% classification accuracy, and 90.1% balanced accuracy. Comparative evaluation against few-shot real-data baseline approaches demonstrates significant improvement. The proposed SDG-based approach achieves 90-91% balanced accuracy under severe class imbalance, while the baselines reach only 50% accuracy. These results demonstrate that the proposed method enables annotation-free, scalable, and robust quality inspection for real-world manufacturing applications.</li>
<li><strong>摘要：</strong>机器学习，特别是深度学习，正在改变工业质量检测。然而，训练强大的机器学习模型通常需要大量高质量的标记数据，而在制造过程中获取这些数据既昂贵、耗时又费力。此外，有缺陷的样本本质上很少见，导致严重的类别不平衡，从而降低模型性能。这些数据限制阻碍了基于机器学习的质量检测方法在实际生产环境中的广泛采用。合成数据生成 (SDG) 提供了一种有前途的解决方案，能够以高效、经济高效且可扩展的方式创建大型、平衡且完全注释的数据集。本文提出了一种混合 SDG 框架，集成了基于模拟的渲染、域随机化和真实背景合成，以实现基于计算机视觉的工业零件检测的零样本学习，无需手动注释。 SDG 管道通过改变零件几何形状、照明和表面属性，然后将合成零件合成到真实图像背景上，在一小时内生成 12,960 个标记图像。利用 YOLOv8n 主干进行对象检测和 MobileNetV3-small 进行质量分类的两阶段架构仅在合成数据上进行训练，并在 300 个真实工业零件上进行评估。该方法的检测 mAP@0.5 为 0.995，分类精度为 96%，平衡精度为 90.1%。与少量真实数据基线方法的比较评估显示出显着的改进。所提出的基于 SDG 的方法在严重类别不平衡的情况下实现了 90-91% 的平衡准确度，而基线仅达到 50% 的准确度。这些结果表明，所提出的方法能够为实际制造应用提供无注释、可扩展且稳健的质量检查。</li>
</ul>

<h3>Title: Mammo-FM: Breast-specific foundational model for Integrated Mammographic Diagnosis, Prognosis, and Reporting</h3>
<ul>
<li><strong>Authors: </strong>Shantanu Ghosh, Vedant Parthesh Joshi, Rayan Syed, Aya Kassem, Abhishek Varshney, Payel Basak, Weicheng Dai, Judy Wawira Gichoya, Hari M. Trivedi, Imon Banerjee, Shyam Visweswaran, Clare B. Poynton, Kayhan Batmanghelich</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00198">https://arxiv.org/abs/2512.00198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00198">https://arxiv.org/pdf/2512.00198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00198]] Mammo-FM: Breast-specific foundational model for Integrated Mammographic Diagnosis, Prognosis, and Reporting(https://arxiv.org/abs/2512.00198)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Breast cancer is one of the leading causes of death among women worldwide. We introduce Mammo-FM, the first foundation model specifically for mammography, pretrained on the largest and most diverse dataset to date - 140,677 patients (821,326 mammograms) across four U.S. institutions. Mammo-FM provides a unified foundation for core clinical tasks in breast imaging, including cancer diagnosis, pathology localization, structured report generation, and cancer risk prognosis within a single framework. Its alignment between images and text enables both visual and textual interpretability, improving transparency and clinical auditability, which are essential for real-world adoption. We rigorously evaluate Mammo-FM across diagnosis, prognosis, and report-generation tasks in in- and out-of-distribution datasets. Despite operating on native-resolution mammograms and using only one-third of the parameters of state-of-the-art generalist FMs, Mammo-FM consistently outperforms them across multiple public and private benchmarks. These results highlight the efficiency and value of domain-specific foundation models designed around the full spectrum of tasks within a clinical domain and emphasize the importance of rigorous, domain-aligned evaluation.</li>
<li><strong>摘要：</strong>乳腺癌是全世界女性死亡的主要原因之一。我们推出 Mammo-FM，这是第一个专门用于乳房 X 光检查的基础模型，在迄今为止最大、最多样化的数据集上进行了预训练 - 来自四个美国机构的 140,677 名患者（821,326 张乳房 X 光照片）。 Mammo-FM 为乳腺成像的核心临床任务提供了统一的基础，包括单一框架内的癌症诊断、病理定位、结构化报告生成和癌症风险预测。图像和文本之间的对齐实现了视觉和文本的可解释性，提高了透明度和临床可审核性，这对于现实世界的采用至关重要。我们在分布内外数据集中的诊断、预后和报告生成任务中严格评估 Mammo-FM。尽管在原始分辨率乳房 X 光检查上运行并且仅使用最先进的通用 FM 参数的三分之一，但 Mammo-FM 在多个公共和私人基准测试中始终优于它们。这些结果突出了围绕临床领域内的全方位任务设计的特定领域基础模型的效率和价值，并强调了严格的、领域一致的评估的重要性。</li>
</ul>

<h3>Title: ReactionMamba: Generating Short &Long Human Reaction Sequences</h3>
<ul>
<li><strong>Authors: </strong>Hajra Anwar Beg, Baptiste Chopin, Hao Tang, Mohamed Daoudi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00208">https://arxiv.org/abs/2512.00208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00208">https://arxiv.org/pdf/2512.00208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00208]] ReactionMamba: Generating Short &Long Human Reaction Sequences(https://arxiv.org/abs/2512.00208)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present ReactionMamba, a novel framework for generating long 3D human reaction motions. Reaction-Mamba integrates a motion VAE for efficient motion encoding with Mamba-based state-space models to decode temporally consistent reactions. This design enables ReactionMamba to generate both short sequences of simple motions and long sequences of complex motions, such as dance and martial arts. We evaluate ReactionMamba on three datasets--NTU120-AS, Lindy Hop, and InterX--and demonstrate competitive performance in terms of realism, diversity, and long-sequence generation compared to previous methods, including InterFormer, ReMoS, and Ready-to-React, while achieving substantial improvements in inference speed.</li>
<li><strong>摘要：</strong>我们推出了 ReactionMamba，这是一种用于生成长 3D 人体反应运动的新颖框架。 Reaction-Mamba 将用于高效运动编码的运动 VAE 与基于 Mamba 的状态空间模型集成，以解码时间一致的反应。这种设计使 ReactionMamba 能够生成短的简单动作序列和长的复杂动作序列，例如舞蹈和武术。我们在三个数据集（NTU120-AS、Lindy Hop 和 InterX）上评估 ReactionMamba，并与之前的方法（包括 InterFormer、ReMoS 和 Ready-to React）相比，在真实性、多样性和长序列生成方面展示了具有竞争力的性能，同时在推理速度方面实现了实质性改进。</li>
</ul>

<h3>Title: DenseScan: Advancing 3D Scene Understanding with 2D Dense Annotation</h3>
<ul>
<li><strong>Authors: </strong>Zirui Wang, Tao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00226">https://arxiv.org/abs/2512.00226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00226">https://arxiv.org/pdf/2512.00226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00226]] DenseScan: Advancing 3D Scene Understanding with 2D Dense Annotation(https://arxiv.org/abs/2512.00226)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>3D understanding is a key capability for real-world AI assistance. High-quality data plays an important role in driving the development of the 3D understanding community. Current 3D scene understanding datasets often provide geometric and instance-level information, yet they lack the rich semantic annotations necessary for nuanced visual-language this http URL this work, we introduce DenseScan, a novel dataset with detailed multi-level descriptions generated by an automated pipeline leveraging multi-view 2D images and multimodal large language models (MLLMs). Our approach enables dense captioning of scene elements, ensuring comprehensive object-level descriptions that capture context-sensitive details. Furthermore, we extend these annotations through scenario-based question generation, producing high-level queries that integrate object properties, spatial relationships, and scene context. By coupling geometric detail with semantic richness, DenseScan broadens the range of downstream tasks, from detailed visual-language navigation to interactive question answering. Experimental results demonstrate that our method significantly enhances object-level understanding and question-answering performance in 3D environments compared to traditional annotation pipelines. We release both the annotated dataset and our annotation pipeline to facilitate future research and applications in robotics, augmented reality, and beyond. Through DenseScan, we aim to catalyze new avenues in 3D scene understanding, allowing researchers and practitioners to tackle the complexities of real-world environments with richer, more contextually aware annotations.</li>
<li><strong>摘要：</strong>3D 理解是现实世界人工智能辅助的关键能力。高质量的数据对于推动3D理解社区的发展发挥着重要作用。当前的 3D 场景理解数据集通常提供几何和实例级信息，但它们缺乏细致入微的视觉语言所需的丰富语义注释，在这项工作中，我们介绍了 DenseScan，这是一个新颖的数据集，具有由利用多视图 2D 图像和多模态大语言模型 (MLLM) 的自动管道生成的详细多级描述。我们的方法可以实现场景元素的密集字幕，确保全面的对象级描述捕获上下文相关的细节。此外，我们通过基于场景的问题生成来扩展这些注释，生成集成对象属性、空间关系和场景上下文的高级查询。通过将几何细节与丰富的语义结合起来，DenseScan 拓宽了下游任务的范围，从详细的视觉语言导航到交互式问答。实验结果表明，与传统注释管道相比，我们的方法显着增强了 3D 环境中的对象级理解和问答性能。我们发布了带注释的数据集和注释管道，以促进未来在机器人、增强现实等领域的研究和应用。通过 DenseScan，我们的目标是促进 3D 场景理解的新途径，使研究人员和从业者能够通过更丰富、更具上下文感知的注释来解决现实世界环境的复杂性。</li>
</ul>

<h3>Title: Self-Supervised Dynamical System Representations for Physiological Time-Series</h3>
<ul>
<li><strong>Authors: </strong>Yenho Chen, Maxwell A. Xu, James M. Rehg, Christopher J. Rozell</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00239">https://arxiv.org/abs/2512.00239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00239">https://arxiv.org/pdf/2512.00239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00239]] Self-Supervised Dynamical System Representations for Physiological Time-Series(https://arxiv.org/abs/2512.00239)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The effectiveness of self-supervised learning (SSL) for physiological time series depends on the ability of a pretraining objective to preserve information about the underlying physiological state while filtering out unrelated noise. However, existing strategies are limited due to reliance on heuristic principles or poorly constrained generative tasks. To address this limitation, we propose a pretraining framework that exploits the information structure of a dynamical systems generative model across multiple time-series. This framework reveals our key insight that class identity can be efficiently captured by extracting information about the generative variables related to the system parameters shared across similar time series samples, while noise unique to individual samples should be discarded. Building on this insight, we propose PULSE, a cross-reconstruction-based pretraining objective for physiological time series datasets that explicitly extracts system information while discarding non-transferrable sample-specific ones. We establish theory that provides sufficient conditions for the system information to be recovered, and empirically validate it using a synthetic dynamical systems experiment. Furthermore, we apply our method to diverse real-world datasets, demonstrating that PULSE learns representations that can broadly distinguish semantic classes, increase label efficiency, and improve transfer learning.</li>
<li><strong>摘要：</strong>生理时间序列自监督学习 (SSL) 的有效性取决于预训练目标保留有关潜在生理状态信息同时滤除不相关噪声的能力。然而，由于依赖启发式原则或约束不佳的生成任务，现有策略受到限制。为了解决这个限制，我们提出了一个预训练框架，该框架利用跨多个时间序列的动态系统生成模型的信息结构。该框架揭示了我们的关键见解，即通过提取与相似时间序列样本共享的系统参数相关的生成变量的信息，可以有效地捕获类别身份，而应丢弃单个样本特有的噪声。基于这一见解，我们提出了 PULSE，这是一种针对生理时间序列数据集的基于交叉重建的预训练目标，它显式地提取系统信息，同时丢弃不可转移的特定于样本的信息。我们建立了为系统信息恢复提供充分条件的理论，并使用综合动力系统实验对其进行了实证验证。此外，我们将我们的方法应用于不同的现实世界数据集，证明 PULSE 学习的表示可以广泛地区分语义类别、提高标签效率并改进迁移学习。</li>
</ul>

<h3>Title: SD-CGAN: Conditional Sinkhorn Divergence GAN for DDoS Anomaly Detection in IoT Networks</h3>
<ul>
<li><strong>Authors: </strong>Henry Onyeka, Emmanuel Samson, Liang Hong, Tariqul Islam, Imtiaz Ahmed, Kamrul Hasan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00251">https://arxiv.org/abs/2512.00251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00251">https://arxiv.org/pdf/2512.00251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00251]] SD-CGAN: Conditional Sinkhorn Divergence GAN for DDoS Anomaly Detection in IoT Networks(https://arxiv.org/abs/2512.00251)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The increasing complexity of IoT edge networks presents significant challenges for anomaly detection, particularly in identifying sophisticated Denial-of-Service (DoS) attacks and zero-day exploits under highly dynamic and imbalanced traffic conditions. This paper proposes SD-CGAN, a Conditional Generative Adversarial Network framework enhanced with Sinkhorn Divergence, tailored for robust anomaly detection in IoT edge environments. The framework incorporates CTGAN-based synthetic data augmentation to address class imbalance and leverages Sinkhorn Divergence as a geometry-aware loss function to improve training stability and reduce mode collapse. The model is evaluated on exploitative attack subsets from the CICDDoS2019 dataset and compared against baseline deep learning and GAN-based approaches. Results show that SD-CGAN achieves superior detection accuracy, precision, recall, and F1-score while maintaining computational efficiency suitable for deployment in edge-enabled IoT environments.</li>
<li><strong>摘要：</strong>物联网边缘网络日益复杂，对异常检测提出了重大挑战，特别是在高度动态和不平衡的流量条件下识别复杂的拒绝服务 (DoS) 攻击和零日攻击。本文提出了 SD-CGAN，这是一种使用 Sinkhorn Divergence 增强的条件生成对抗网络框架，专为物联网边缘环境中的稳健异常检测而定制。该框架采用基于 CTGAN 的合成数据增强来解决类别不平衡问题，并利用 Sinkhorn Divergence 作为几何感知损失函数来提高训练稳定性并减少模式崩溃。该模型根据 CICDDoS2019 数据集中的剥削性攻击子集进行评估，并与基线深度学习和基于 GAN 的方法进行比较。结果表明，SD-CGAN 实现了卓越的检测准确度、精确度、召回率和 F1 分数，同时保持了适合在边缘支持的物联网环境中部署的计算效率。</li>
</ul>

<h3>Title: Relightable Holoported Characters: Capturing and Relighting Dynamic Human Performance from Sparse Views</h3>
<ul>
<li><strong>Authors: </strong>Kunwar Maheep Singh, Jianchun Chen, Vladislav Golyanik, Stephan J. Garbin, Thabo Beeler, Rishabh Dabral, Marc Habermann, Christian Theobalt</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00255">https://arxiv.org/abs/2512.00255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00255">https://arxiv.org/pdf/2512.00255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00255]] Relightable Holoported Characters: Capturing and Relighting Dynamic Human Performance from Sparse Views(https://arxiv.org/abs/2512.00255)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present Relightable Holoported Characters (RHC), a novel person-specific method for free-view rendering and relighting of full-body and highly dynamic humans solely observed from sparse-view RGB videos at inference. In contrast to classical one-light-at-a-time (OLAT)-based human relighting, our transformer-based RelightNet predicts relit appearance within a single network pass, avoiding costly OLAT-basis capture and generation. For training such a model, we introduce a new capture strategy and dataset recorded in a multi-view lightstage, where we alternate frames lit by random environment maps with uniformly lit tracking frames, simultaneously enabling accurate motion tracking and diverse illumination as well as dynamics coverage. Inspired by the rendering equation, we derive physics-informed features that encode geometry, albedo, shading, and the virtual camera view from a coarse human mesh proxy and the input views. Our RelightNet then takes these features as input and cross-attends them with a novel lighting condition, and regresses the relit appearance in the form of texel-aligned 3D Gaussian splats attached to the coarse mesh proxy. Consequently, our RelightNet implicitly learns to efficiently compute the rendering equation for novel lighting conditions within a single feed-forward pass. Experiments demonstrate our method's superior visual fidelity and lighting reproduction compared to state-of-the-art approaches. Project page: this https URL</li>
<li><strong>摘要：</strong>我们提出了 Relightable Holoported Characters (RHC)，这是一种新颖的特定于人的方法，用于自由视图渲染和重新照明全身和高度动态的人类，仅在推理时从稀疏视图 RGB 视频中观察到。与经典的基于一次一灯 (OLAT) 的人类重新照明相比，我们基于变压器的 RelightNet 可在单个网络通道内预测重新照明的外观，从而避免昂贵的基于 OLAT 的捕获和生成。为了训练这样的模型，我们引入了一种新的捕捉策略和记录在多视图光舞台中的数据集，其中我们交替使用随机环境图照明的帧和均匀照明的跟踪帧，同时实现精确的运动跟踪和多样化的照明以及动态覆盖。受渲染方程的启发，我们从粗略的人体网格代理和输入视图中推导出物理信息特征，这些特征对几何、反照率、阴影和虚拟相机视图进行编码。然后，我们的 RelightNet 将这些特征作为输入，并使用新颖的光照条件对它们进行交叉处理，并以附加到粗网格代理的纹理元素对齐的 3D 高斯图的形式回归重新照明的外观。因此，我们的 RelightNet 隐式学习在单次前馈传递中有效计算新颖照明条件的渲染方程。实验证明，与最先进的方法相比，我们的方法具有卓越的视觉保真度和照明再现能力。项目页面：此 https URL</li>
</ul>

<h3>Title: USB: Unified Synthetic Brain Framework for Bidirectional Pathology-Healthy Generation and Editing</h3>
<ul>
<li><strong>Authors: </strong>Jun Wang, Peirong Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00269">https://arxiv.org/abs/2512.00269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00269">https://arxiv.org/pdf/2512.00269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00269]] USB: Unified Synthetic Brain Framework for Bidirectional Pathology-Healthy Generation and Editing(https://arxiv.org/abs/2512.00269)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Understanding the relationship between pathological and healthy brain structures is fundamental to neuroimaging, connecting disease diagnosis and detection with modeling, prediction, and treatment planning. However, paired pathological-healthy data are extremely difficult to obtain, as they rely on pre- and post-treatment imaging, constrained by clinical outcomes and longitudinal data availability. Consequently, most existing brain image generation and editing methods focus on visual quality yet remain domain-specific, treating pathological and healthy image modeling independently. We introduce USB (Unified Synthetic Brain), the first end-to-end framework that unifies bidirectional generation and editing of pathological and healthy brain images. USB models the joint distribution of lesions and brain anatomy through a paired diffusion mechanism and achieves both pathological and healthy image generation. A consistency guidance algorithm further preserves anatomical consistency and lesion correspondence during bidirectional pathology-healthy editing. Extensive experiments on six public brain MRI datasets including healthy controls, stroke, and Alzheimer's patients, demonstrate USB's ability to produce diverse and realistic results. By establishing the first unified benchmark for brain image generation and editing, USB opens opportunities for scalable dataset creation and robust neuroimaging analysis. Code is available at this https URL.</li>
<li><strong>摘要：</strong>了解病理和健康大脑结构之间的关系对于神经影像学、将疾病诊断和检测与建模、预测和治疗计划联系起来至关重要。然而，配对的病理-健康数据极难获得，因为它们依赖于治疗前和治疗后的成像，并受到临床结果和纵向数据可用性的限制。因此，大多数现有的大脑图像生成和编辑方法都注重视觉质量，但仍保持特定领域，独立处理病理和健康图像建模。我们介绍 USB（统一合成大脑），这是第一个统一病理和健康大脑图像的双向生成和编辑的端到端框架。 USB通过配对扩散机制对病变和大脑解剖结构的联合分布进行建模，并实现病理和健康图像的生成。一致性指导算法在双向病理健康编辑过程中进一步保留解剖学一致性和病变对应性。对六个公共脑 MRI 数据集（包括健康对照、中风和阿尔茨海默病患者）进行的广泛实验证明了 USB 能够产生多样化且真实的结果。通过建立第一个脑图像生成和编辑的统一基准，USB 为可扩展的数据集创建和强大的神经影像分析提供了机会。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: HIMOSA: Efficient Remote Sensing Image Super-Resolution with Hierarchical Mixture of Sparse Attention</h3>
<ul>
<li><strong>Authors: </strong>Yi Liu, Yi Wan, Xinyi Liu, Qiong Wu, Panwang Xia, Xuejun Huang, Yongjun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00275">https://arxiv.org/abs/2512.00275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00275">https://arxiv.org/pdf/2512.00275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00275]] HIMOSA: Efficient Remote Sensing Image Super-Resolution with Hierarchical Mixture of Sparse Attention(https://arxiv.org/abs/2512.00275)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>In remote sensing applications, such as disaster detection and response, real-time efficiency and model lightweighting are of critical importance. Consequently, existing remote sensing image super-resolution methods often face a trade-off between model performance and computational efficiency. In this paper, we propose a lightweight super-resolution framework for remote sensing imagery, named HIMOSA. Specifically, HIMOSA leverages the inherent redundancy in remote sensing imagery and introduces a content-aware sparse attention mechanism, enabling the model to achieve fast inference while maintaining strong reconstruction performance. Furthermore, to effectively leverage the multi-scale repetitive patterns found in remote sensing imagery, we introduce a hierarchical window expansion and reduce the computational complexity by adjusting the sparsity of the attention. Extensive experiments on multiple remote sensing datasets demonstrate that our method achieves state-of-the-art performance while maintaining computational efficiency.</li>
<li><strong>摘要：</strong>在灾害检测和响应等遥感应用中，实时效率和模型轻量化至关重要。因此，现有的遥感图像超分辨率方法常常面临模型性能和计算效率之间的权衡。在本文中，我们提出了一种用于遥感图像的轻量级超分辨率框架，名为 HIMOSA。具体来说，HIMOSA利用遥感图像固有的冗余性，引入内容感知的稀疏注意力机制，使模型能够实现快速推理，同时保持强大的重建性能。此外，为了有效利用遥感图像中发现的多尺度重复模式，我们引入了分层窗口扩展，并通过调整注意力的稀疏性来降低计算复杂性。对多个遥感数据集的大量实验表明，我们的方法在保持计算效率的同时实现了最先进的性能。</li>
</ul>

<h3>Title: BioArc: Discovering Optimal Neural Architectures for Biological Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Yi Fang, Haoran Xu, Jiaxin Han, Sirui Ding, Yizhi Wang, Yue Wang, Xuan Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00283">https://arxiv.org/abs/2512.00283</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00283">https://arxiv.org/pdf/2512.00283</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00283]] BioArc: Discovering Optimal Neural Architectures for Biological Foundation Models(https://arxiv.org/abs/2512.00283)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Foundation models have revolutionized various fields such as natural language processing (NLP) and computer vision (CV). While efforts have been made to transfer the success of the foundation models in general AI domains to biology, existing works focus on directly adopting the existing foundation model architectures from general machine learning domains without a systematic design considering the unique physicochemical and structural properties of each biological data modality. This leads to suboptimal performance, as these repurposed architectures struggle to capture the long-range dependencies, sparse information, and complex underlying ``grammars'' inherent to biological data. To address this gap, we introduce BioArc, a novel framework designed to move beyond intuition-driven architecture design towards principled, automated architecture discovery for biological foundation models. Leveraging Neural Architecture Search (NAS), BioArc systematically explores a vast architecture design space, evaluating architectures across multiple biological modalities while rigorously analyzing the interplay between architecture, tokenization, and training strategies. This large-scale analysis identifies novel, high-performance architectures, allowing us to distill a set of empirical design principles to guide future model development. Furthermore, to make the best of this set of discovered principled architectures, we propose and compare several architecture prediction methods that effectively and efficiently predict optimal architectures for new biological tasks. Overall, our work provides a foundational resource and a principled methodology to guide the creation of the next generation of task-specific and foundation models for biology.</li>
<li><strong>摘要：</strong>基础模型彻底改变了自然语言处理（NLP）和计算机视觉（CV）等各个领域。虽然人们已经努力将通用人工智能领域的基础模型的成功转移到生物学上，但现有的工作重点是直接采用通用机器学习领域的现有基础模型架构，而没有考虑每种生物数据模态独特的物理化学和结构特性进行系统设计。这会导致性能不佳，因为这些重新调整用途的架构很难捕获生物数据固有的远程依赖性、稀疏信息和复杂的底层“语法”。为了解决这一差距，我们引入了 BioArc，这是一种新颖的框架，旨在超越直觉驱动的架构设计，转向生物基础模型的有原则的、自动化的架构发现。 BioArc 利用神经架构搜索 (NAS)，系统地探索广阔的架构设计空间，评估多种生物模态的架构，同时严格分析架构、标记化和训练策略之间的相互作用。这种大规模分析确定了新颖的高性能架构，使我们能够提炼出一套经验设计原则来指导未来的模型开发。此外，为了充分利用这组已发现的原则架构，我们提出并比较了几种架构预测方法，这些方法可以有效且高效地预测新生物任务的最佳架构。总的来说，我们的工作提供了基础资源和原则方法来指导下一代生物学特定任务和基础模型的创建。</li>
</ul>

<h3>Title: Optimizing Distributional Geometry Alignment with Optimal Transport for Generative Dataset Distillation</h3>
<ul>
<li><strong>Authors: </strong>Xiao Cui, Yulei Qin, Wengang Zhou, Hongsheng Li, Houqiang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00308">https://arxiv.org/abs/2512.00308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00308">https://arxiv.org/pdf/2512.00308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00308]] Optimizing Distributional Geometry Alignment with Optimal Transport for Generative Dataset Distillation(https://arxiv.org/abs/2512.00308)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Dataset distillation seeks to synthesize a compact distilled dataset, enabling models trained on it to achieve performance comparable to models trained on the full dataset. Recent methods for large-scale datasets focus on matching global distributional statistics (e.g., mean and variance), but overlook critical instance-level characteristics and intraclass variations, leading to suboptimal generalization. We address this limitation by reformulating dataset distillation as an Optimal Transport (OT) distance minimization problem, enabling fine-grained alignment at both global and instance levels throughout the pipeline. OT offers a geometrically faithful framework for distribution matching. It effectively preserves local modes, intra-class patterns, and fine-grained variations that characterize the geometry of complex, high-dimensional distributions. Our method comprises three components tailored for preserving distributional geometry: (1) OT-guided diffusion sampling, which aligns latent distributions of real and distilled images; (2) label-image-aligned soft relabeling, which adapts label distributions based on the complexity of distilled image distributions; and (3) OT-based logit matching, which aligns the output of student models with soft-label distributions. Extensive experiments across diverse architectures and large-scale datasets demonstrate that our method consistently outperforms state-of-the-art approaches in an efficient manner, achieving at least 4% accuracy improvement under IPC=10 settings for each architecture on ImageNet-1K.</li>
<li><strong>摘要：</strong>数据集蒸馏旨在合成一个紧凑的蒸馏数据集，使在其上训练的模型能够达到与在完整数据集上训练的模型相当的性能。最近的大规模数据集方法侧重于匹配全局分布统计数据（例如均值和方差），但忽略了关键的实例级特征和类内变化，导致泛化不理想。我们通过将数据集蒸馏重新表述为最佳传输 (OT) 距离最小化问题来解决此限制，从而在整个管道的全局和实例级别实现细粒度对齐。 OT 为分布匹配提供了几何上忠实的框架。它有效地保留了表征复杂高维分布几何形状的局部模式、类内模式和细粒度变化。我们的方法包括三个为保留分布几何而定制的组件：（1）OT引导的扩散采样，它对齐真实图像和蒸馏图像的潜在分布； (2) 标签图像对齐软重标签，根据蒸馏图像分布的复杂性来调整标签分布； (3) 基于 OT 的 logit 匹配，将学生模型的输出与软标签分布对齐。跨不同架构和大规模数据集的大量实验表明，我们的方法始终以有效的方式优于最先进的方法，在 ImageNet-1K 上的每个架构的 IPC=10 设置下实现了至少 4% 的精度提升。</li>
</ul>

<h3>Title: MVAD : A Comprehensive Multimodal Video-Audio Dataset for AIGC Detection</h3>
<ul>
<li><strong>Authors: </strong>Mengxue Hu, Yunfeng Diao, Changtao Miao, Jianshu Li, Zhe Li, Joey Tianyi Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00336">https://arxiv.org/abs/2512.00336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00336">https://arxiv.org/pdf/2512.00336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00336]] MVAD : A Comprehensive Multimodal Video-Audio Dataset for AIGC Detection(https://arxiv.org/abs/2512.00336)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of AI-generated multimodal video-audio content has raised significant concerns regarding information security and content authenticity. Existing synthetic video datasets predominantly focus on the visual modality alone, while the few incorporating audio are largely confined to facial deepfakes--a limitation that fails to address the expanding landscape of general multimodal AI-generated content and substantially impedes the development of trustworthy detection systems. To bridge this critical gap, we introduce the Multimodal Video-Audio Dataset (MVAD), the first comprehensive dataset specifically designed for detecting AI-generated multimodal video-audio content. Our dataset exhibits three key characteristics: (1) genuine multimodality with samples generated according to three realistic video-audio forgery patterns; (2) high perceptual quality achieved through diverse state-of-the-art generative models; and (3) comprehensive diversity spanning realistic and anime visual styles, four content categories (humans, animals, objects, and scenes), and four video-audio multimodal data types. Our dataset will be available at this https URL.</li>
<li><strong>摘要：</strong>人工智能生成的多模态视频音频内容的快速发展引起了人们对信息安全和内容真实性的严重担忧。现有的合成视频数据集主要只关注视觉形态，而少数包含音频的数据集主要局限于面部深度伪造——这一限制无法解决一般多模态人工智能生成内容的不断扩大的前景，并严重阻碍了值得信赖的检测系统的开发。为了弥补这一关键差距，我们引入了多模态视频音频数据集（MVAD），这是第一个专门为检测人工智能生成的多模态视频音频内容而设计的综合数据集。我们的数据集表现出三个关键特征：（1）真正的多模态，根据三种真实的视频音频伪造模式生成样本； （2）通过多种最先进的生成模型实现高感知质量； (3) 全面的多样性，涵盖现实和动漫视觉风格、四种内容类别（人类、动物、物体和场景）和四种视频音频多模态数据类型。我们的数据集将在此 https URL 中提供。</li>
</ul>

<h3>Title: mmPred: Radar-based Human Motion Prediction in the Dark</h3>
<ul>
<li><strong>Authors: </strong>Junqiao Fan, Haocong Rao, Jiarui Zhang, Jianfei Yang, Lihua Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00345">https://arxiv.org/abs/2512.00345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00345">https://arxiv.org/pdf/2512.00345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00345]] mmPred: Radar-based Human Motion Prediction in the Dark(https://arxiv.org/abs/2512.00345)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Existing Human Motion Prediction (HMP) methods based on RGB-D cameras are sensitive to lighting conditions and raise privacy concerns, limiting their real-world applications such as firefighting and healthcare. Motivated by the robustness and privacy-preserving nature of millimeter-wave (mmWave) radar, this work introduces radar as a novel sensing modality for HMP, for the first time. Nevertheless, radar signals often suffer from specular reflections and multipath effects, resulting in noisy and temporally inconsistent measurements, such as body-part miss-detection. To address these radar-specific artifacts, we propose mmPred, the first diffusion-based framework tailored for radar-based HMP. mmPred introduces a dual-domain historical motion representation to guide the generation process, combining a Time-domain Pose Refinement (TPR) branch for learning fine-grained details and a Frequency-domain Dominant Motion (FDM) branch for capturing global motion trends and suppressing frame-level inconsistency. Furthermore, we design a Global Skeleton-relational Transformer (GST) as the diffusion backbone to model global inter-joint cooperation, enabling corrupted joints to dynamically aggregate information from others. Extensive experiments show that mmPred achieves state-of-the-art performance, outperforming existing methods by 8.6% on mmBody and 22% on mm-Fi.</li>
<li><strong>摘要：</strong>现有的基于 RGB-D 摄像头的人体运动预测 (HMP) 方法对照明条件敏感，并会引发隐私问题，限制了其在消防和医疗保健等现实世界中的应用。受毫米波 (mmWave) 雷达的鲁棒性和隐私保护特性的启发，这项工作首次将雷达作为 HMP 的一种新型传感方式引入。然而，雷达信号经常受到镜面反射和多径效应的影响，导致测量结果出现噪声和时间不一致，例如身体部位的漏检。为了解决这些雷达特定的伪影，我们提出了 mmPred，这是第一个为基于雷达的 HMP 量身定制的基于扩散的框架。 mmPred 引入了双域历史运动表示来指导生成过程，结合了用于学习细粒度细节的时域姿态细化 (TPR) 分支和用于捕获全局运动趋势并抑制帧级不一致的频域主导运动 (FDM) 分支。此外，我们设计了一个全局骨架关系变换器（GST）作为扩散主干来模拟全局关节间合作，使损坏的关节能够动态聚合来自其他关节的信息。大量实验表明，mmPred 实现了最先进的性能，在 mmBody 上比现有方法高出 8.6%，在 mm-Fi 上比现有方法高出 22%。</li>
</ul>

<h3>Title: POLARIS: Projection-Orthogonal Least Squares for Robust and Adaptive Inversion in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Wenshuo Chen, Haosen Li, Shaofeng Liang, Lei Wang, Haozhe Jia, Kaishen Yuan, Jieming Wu, Bowen Tian, Yutao Yue</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00369">https://arxiv.org/abs/2512.00369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00369">https://arxiv.org/pdf/2512.00369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00369]] POLARIS: Projection-Orthogonal Least Squares for Robust and Adaptive Inversion in Diffusion Models(https://arxiv.org/abs/2512.00369)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>The Inversion-Denoising Paradigm, which is based on diffusion models, excels in diverse image editing and restoration tasks. We revisit its mechanism and reveal a critical, overlooked factor in reconstruction degradation: the approximate noise error. This error stems from approximating the noise at step t with the prediction at step t-1, resulting in severe error accumulation throughout the inversion process. We introduce Projection-Orthogonal Least Squares for Robust and Adaptive Inversion (POLARIS), which reformulates inversion from an error-compensation problem into an error-origin problem. Rather than optimizing embeddings or latent codes to offset accumulated drift, POLARIS treats the guidance scale {\omega} as a step-wise variable and derives a mathematically grounded formula to minimize inversion error at each step. Remarkably, POLARIS improves inversion latent quality with just one line of code. With negligible performance overhead, it substantially mitigates noise approximation errors and consistently improves the accuracy of downstream tasks.</li>
<li><strong>摘要：</strong>基于扩散模型的反演降噪范式在各种图像编辑和恢复任务中表现出色。我们重新审视其机制，并揭示了重建退化中一个关键的、被忽视的因素：近似噪声误差。该误差源于用步骤 t-1 的预测来逼近步骤 t 的噪声，导致整个反演过程中出现严重的误差累积。我们引入了稳健自适应反演的投影正交最小二乘法（POLARIS），它将反演从误差补偿问题重新表述为误差源问题。 POLARIS 不是通过优化嵌入或潜在代码来抵消累积漂移，而是将制导尺度 {\omega} 视为逐步变量，并推导出一个数学基础公式以最小化每一步的反演误差。值得注意的是，POLARIS 只需一行代码即可提高反演潜在质量。由于性能开销可以忽略不计，它大大减少了噪声近似误差，并持续提高了下游任务的准确性。</li>
</ul>

<h3>Title: Efficient and Programmable Exploration of Synthesizable Chemical Space</h3>
<ul>
<li><strong>Authors: </strong>Shitong Luo, Connor W. Coley</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00384">https://arxiv.org/abs/2512.00384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00384">https://arxiv.org/pdf/2512.00384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00384]] Efficient and Programmable Exploration of Synthesizable Chemical Space(https://arxiv.org/abs/2512.00384)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The constrained nature of synthesizable chemical space poses a significant challenge for sampling molecules that are both synthetically accessible and possess desired properties. In this work, we present PrexSyn, an efficient and programmable model for molecular discovery within synthesizable chemical space. PrexSyn is based on a decoder-only transformer trained on a billion-scale datastream of synthesizable pathways paired with molecular properties, enabled by a real-time, high-throughput C++-based data generation engine. The large-scale training data allows PrexSyn to reconstruct the synthesizable chemical space nearly perfectly at a high inference speed and learn the association between properties and synthesizable molecules. Based on its learned property-pathway mappings, PrexSyn can generate synthesizable molecules that satisfy not only single-property conditions but also composite property queries joined by logical operators, thereby allowing users to ``program'' generation objectives. Moreover, by exploiting this property-based querying capability, PrexSyn can efficiently optimize molecules against black-box oracle functions via iterative query refinement, achieving higher sampling efficiency than even synthesis-agnostic baselines, making PrexSyn a powerful general-purpose molecular optimization tool. Overall, PrexSyn pushes the frontier of synthesizable molecular design by setting a new state of the art in synthesizable chemical space coverage, molecular sampling efficiency, and inference speed.</li>
<li><strong>摘要：</strong>可合成化学空间的受限性质对可合成且具有所需特性的分子采样提出了重大挑战。在这项工作中，我们提出了 PrexSyn，这是一种用于在可合成化学空间内进行分子发现的高效且可编程的模型。 PrexSyn 基于仅解码器的变压器，该变压器经过十亿级可合成路径数据流的训练，并与分子特性相匹配，并由基于 C++ 的实时、高通量数据生成引擎启用。大规模的训练数据使 PrexSyn 能够以高推理速度近乎完美地重建可合成的化学空间，并学习性质与可合成分子之间的关联。基于其学习的属性路径映射，PrexSyn 可以生成可合成的分子，不仅满足单属性条件，还满足由逻辑运算符连接的复合属性查询，从而允许用户“编程”生成目标。此外，通过利用这种基于属性的查询功能，PrexSyn 可以通过迭代查询细化，针对黑盒预言函数有效优化分子，实现比合成不可知基线更高的采样效率，使 PrexSyn 成为强大的通用分子优化工具。总体而言，PrexSyn 通过在可合成化学空间覆盖、分子采样效率和推理速度方面设定了新的技术水平，推动了可合成分子设计的前沿。</li>
</ul>

<h3>Title: Better, Stronger, Faster: Tackling the Trilemma in MLLM-based Segmentation with Simultaneous Textual Mask Prediction</h3>
<ul>
<li><strong>Authors: </strong>Jiazhen Liu, Mingkuan Feng, Long Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00395">https://arxiv.org/abs/2512.00395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00395">https://arxiv.org/pdf/2512.00395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00395]] Better, Stronger, Faster: Tackling the Trilemma in MLLM-based Segmentation with Simultaneous Textual Mask Prediction(https://arxiv.org/abs/2512.00395)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Integrating segmentation into Multimodal Large Language Models (MLLMs) presents a core trilemma: simultaneously preserving dialogue ability, achieving high segmentation performance, and ensuring fast inference. Prevailing paradigms are forced into a compromise. Embedding prediction methods introduce a conflicting pixel-level objective that degrades the MLLM's general dialogue abilities. The alternative, next-token prediction, reframes segmentation as an autoregressive task, which preserves dialogue but forces a trade-off between poor segmentation performance with sparse outputs or prohibitive inference speeds with rich ones. We resolve this trilemma with all-mask prediction, a novel paradigm that decouples autoregressive dialogue generation from non-autoregressive mask prediction. We present STAMP: Simultaneous Textual All-Mask Prediction, an MLLM that embodies this paradigm. After generating a textual response, STAMP predicts an entire segmentation mask in a single forward pass by treating it as a parallel "fill-in-the-blank" task over image patches. This design maintains the MLLM's dialogue ability by avoiding conflicting objectives, enables high segmentation performance by leveraging rich, bidirectional spatial context for all mask tokens, and achieves exceptional speed. Extensive experiments show that STAMP significantly outperforms state-of-the-art methods across multiple segmentation benchmarks, providing a solution that excels in dialogue, segmentation, and speed without compromise.</li>
<li><strong>摘要：</strong>将分割集成到多模态大语言模型（MLLM）中会出现一个核心的三难困境：同时保留对话能力、实现高分割性能并确保快速推理。流行的范式被迫妥协。嵌入预测方法引入了一个相互冲突的像素级目标，从而降低了 MLLM 的一般对话能力。另一种方法是下一个令牌预测，它将分割重新构建为自回归任务，它保留了对话，但迫使在较差的分割性能与稀疏的输出或令人望而却步的推理速度与丰富的输出之间进行权衡。我们通过全掩模预测解决了这个难题，这是一种将自回归对话生成与非自回归掩模预测分离的新颖范式。我们提出了 STAMP：同步文本全掩模预测，这是一种体现这种范式的 MLLM。生成文本响应后，STAMP 通过将其视为图像块上的并行“填空”任务，在单个前向传递中预测整个分割掩码。该设计通过避免目标冲突来保持 MLLM 的对话能力，通过利用所有掩模标记的丰富的双向空间上下文来实现高分割性能，并实现卓越的速度。大量实验表明，STAMP 在多个分割基准上显着优于最先进的方法，提供了一种在对话、分割和速度方面表现出色的解决方案，而且毫不妥协。</li>
</ul>

<h3>Title: Low-Bitrate Video Compression through Semantic-Conditioned Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Lingdong Wang, Guan-Ming Su, Divya Kothandaraman, Tsung-Wei Huang, Mohammad Hajiesmaili, Ramesh K. Sitaraman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00408">https://arxiv.org/abs/2512.00408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00408">https://arxiv.org/pdf/2512.00408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00408]] Low-Bitrate Video Compression through Semantic-Conditioned Diffusion(https://arxiv.org/abs/2512.00408)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Traditional video codecs optimized for pixel fidelity collapse at ultra-low bitrates and produce severe artifacts. This failure arises from a fundamental misalignment between pixel accuracy and human perception. We propose a semantic video compression framework named DiSCo that transmits only the most meaningful information while relying on generative priors for detail synthesis. The source video is decomposed into three compact modalities: a textual description, a spatiotemporally degraded video, and optional sketches or poses that respectively capture semantic, appearance, and motion cues. A conditional video diffusion model then reconstructs high-quality, temporally coherent videos from these compact representations. Temporal forward filling, token interleaving, and modality-specific codecs are proposed to improve multimodal generation and modality compactness. Experiments show that our method outperforms baseline semantic and traditional codecs by 2-10X on perceptual metrics at low bitrates.</li>
<li><strong>摘要：</strong>针对像素保真度进行优化的传统视频编解码器在超低比特率下会崩溃并产生严重的伪影。这种失败源于像素精度和人类感知之间的根本失调。我们提出了一种名为 DiSCo 的语义视频压缩框架，它仅传输最有意义的信息，同时依赖生成先验进行细节合成。源视频被分解为三种紧凑的模式：文本描述、时空降级视频以及分别捕获语义、外观和运动线索的可选草图或姿势。然后，条件视频扩散模型根据这些紧凑的表示重建高质量、时间连贯的视频。提出了时间前向填充、令牌交织和特定于模态的编解码器来改进多模态生成和模态紧凑性。实验表明，我们的方法在低比特率下的感知指标上优于基线语义和传统编解码器 2-10 倍。</li>
</ul>

<h3>Title: SplatFont3D: Structure-Aware Text-to-3D Artistic Font Generation with Part-Level Style Control</h3>
<ul>
<li><strong>Authors: </strong>Ji Gan, Lingxu Chen, Jiaxu Leng, Xinbo Gao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00413">https://arxiv.org/abs/2512.00413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00413">https://arxiv.org/pdf/2512.00413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00413]] SplatFont3D: Structure-Aware Text-to-3D Artistic Font Generation with Part-Level Style Control(https://arxiv.org/abs/2512.00413)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Artistic font generation (AFG) can assist human designers in creating innovative artistic fonts. However, most previous studies primarily focus on 2D artistic fonts in flat design, leaving personalized 3D-AFG largely underexplored. 3D-AFG not only enables applications in immersive 3D environments such as video games and animations, but also may enhance 2D-AFG by rendering 2D fonts of novel views. Moreover, unlike general 3D objects, 3D fonts exhibit precise semantics with strong structural constraints and also demand fine-grained part-level style control. To address these challenges, we propose SplatFont3D, a novel structure-aware text-to-3D AFG framework with 3D Gaussian splatting, which enables the creation of 3D artistic fonts from diverse style text prompts with precise part-level style control. Specifically, we first introduce a Glyph2Cloud module, which progressively enhances both the shapes and styles of 2D glyphs (or components) and produces their corresponding 3D point clouds for Gaussian initialization. The initialized 3D Gaussians are further optimized through interaction with a pretrained 2D diffusion model using score distillation sampling. To enable part-level control, we present a dynamic component assignment strategy that exploits the geometric priors of 3D Gaussians to partition components, while alleviating drift-induced entanglement during 3D Gaussian optimization. Our SplatFont3D provides more explicit and effective part-level style control than NeRF, attaining faster rendering efficiency. Experiments show that our SplatFont3D outperforms existing 3D models for 3D-AFG in style-text consistency, visual quality, and rendering efficiency.</li>
<li><strong>摘要：</strong>艺术字体生成（AFG）可以帮助人类设计师创建创新的艺术字体。然而，之前的大多数研究主要集中在平面设计中的 2D 艺术字体，而个性化 3D-AFG 很大程度上尚未得到充分探索。 3D-AFG 不仅支持视频游戏和动画等沉浸式 3D 环境中的应用，还可以通过渲染新颖视图的 2D 字体来增强 2D-AFG。此外，与一般的 3D 对象不同，3D 字体表现出精确的语义，具有很强的结构约束，并且还需要细粒度的部分级样式控制。为了应对这些挑战，我们提出了 SplatFont3D，这是一种新颖的结构感知文本到 3D AFG 框架，具有 3D 高斯泼溅功能，可以通过精确的部分级样式控制从不同样式的文本提示创建 3D 艺术字体。具体来说，我们首先引入一个 Glyph2Cloud 模块，它逐步增强 2D 字形（或组件）的形状和样式，并为高斯初始化生成相应的 3D 点云。通过与使用分数蒸馏采样的预训练 2D 扩散模型交互，进一步优化初始化的 3D 高斯。为了实现零件级控制，我们提出了一种动态组件分配策略，该策略利用 3D 高斯的几何先验来划分组件，同时减轻 3D 高斯优化期间漂移引起的纠缠。我们的SplatFont3D提供了比NeRF更明确、更有效的部件级样式控制，从而获得更快的渲染效率。实验表明，我们的 SplatFont3D 在样式文本一致性、视觉质量和渲染效率方面优于 3D-AFG 的现有 3D 模型。</li>
</ul>

<h3>Title: PhysGen: Physically Grounded 3D Shape Generation for Industrial Design</h3>
<ul>
<li><strong>Authors: </strong>Yingxuan You, Chen Zhao, Hantao Zhang, Mingda Xu, Pascal Fua</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00422">https://arxiv.org/abs/2512.00422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00422">https://arxiv.org/pdf/2512.00422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00422]] PhysGen: Physically Grounded 3D Shape Generation for Industrial Design(https://arxiv.org/abs/2512.00422)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Existing generative models for 3D shapes can synthesize high-fidelity and visually plausible shapes. For certain classes of shapes that have undergone an engineering design process, the realism of the shape is tightly coupled with the underlying physical properties, e.g., aerodynamic efficiency for automobiles. Since existing methods lack knowledge of such physics, they are unable to use this knowledge to enhance the realism of shape generation. Motivated by this, we propose a unified physics-based 3D shape generation pipeline, with a focus on industrial design applications. Specifically, we introduce a new flow matching model with explicit physical guidance, consisting of an alternating update process. We iteratively perform a velocity-based update and a physics-based refinement, progressively adjusting the latent code to align with the desired 3D shapes and physical properties. We further strengthen physical validity by incorporating a physics-aware regularization term into the velocity-based update step. To support such physics-guided updates, we build a shape-and-physics variational autoencoder (SP-VAE) that jointly encodes shape and physics information into a unified latent space. The experiments on three benchmarks show that this synergistic formulation improves shape realism beyond mere visual plausibility.</li>
<li><strong>摘要：</strong>现有的 3D 形状生成模型可以合成高保真且视觉上合理的形状。对于经过工程设计过程的某些类别的形状，形状的真实性与潜在的物理特性紧密耦合，例如汽车的空气动力学效率。由于现有方法缺乏此类物理学知识，因此无法利用这些知识来增强形状生成的真实性。受此启发，我们提出了一个基于物理的统一 3D 形状生成管道，重点关注工业设计应用。具体来说，我们引入了一种具有明确物理指导的新流匹配模型，由交替更新过程组成。我们迭代地执行基于速度的更新和基于物理的细化，逐步调整潜在代码以与所需的 3D 形状和物理属性保持一致。我们通过将物理感知正则化项纳入基于速度的更新步骤来进一步增强物理有效性。为了支持这种物理引导的更新，我们构建了一个形状和物理变分自动编码器（SP-VAE），它将形状和物理信息联合编码到统一的潜在空间中。三个基准的实验表明，这种协同配方改善了形状的真实感，而不仅仅是视觉上的合理性。</li>
</ul>

<h3>Title: What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards</h3>
<ul>
<li><strong>Authors: </strong>Minh-Quan Le, Yuanzhi Zhu, Vicky Kalogeiton, Dimitris Samaras</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00425">https://arxiv.org/abs/2512.00425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00425">https://arxiv.org/pdf/2512.00425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00425]] What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards(https://arxiv.org/abs/2512.00425)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent video diffusion models can synthesize visually compelling clips, yet often violate basic physical laws-objects float, accelerations drift, and collisions behave inconsistently-revealing a persistent gap between visual realism and physical realism. We propose $\texttt{NewtonRewards}$, the first physics-grounded post-training framework for video generation based on $\textit{verifiable rewards}$. Instead of relying on human or VLM feedback, $\texttt{NewtonRewards}$ extracts $\textit{measurable proxies}$ from generated videos using frozen utility models: optical flow serves as a proxy for velocity, while high-level appearance features serve as a proxy for mass. These proxies enable explicit enforcement of Newtonian structure through two complementary rewards: a Newtonian kinematic constraint enforcing constant-acceleration dynamics, and a mass conservation reward preventing trivial, degenerate solutions. We evaluate $\texttt{NewtonRewards}$ on five Newtonian Motion Primitives (free fall, horizontal/parabolic throw, and ramp sliding down/up) using our newly constructed large-scale benchmark, $\texttt{NewtonBench-60K}$. Across all primitives in visual and physics metrics, $\texttt{NewtonRewards}$ consistently improves physical plausibility, motion smoothness, and temporal coherence over prior post-training methods. It further maintains strong performance under out-of-distribution shifts in height, speed, and friction. Our results show that physics-grounded verifiable rewards offer a scalable path toward physics-aware video generation.</li>
<li><strong>摘要：</strong>最近的视频扩散模型可以合成视觉上引人注目的剪辑，但常常违反基本物理定律——物体漂浮、加速度漂移和碰撞行为不一致——揭示了视觉真实感和物理真实感之间持续存在的差距。我们提出$\texttt{NewtonRewards}$，这是第一个基于物理的基于$\textit{可验证奖励}$的视频生成后训练框架。 $\texttt{NewtonRewards}$ 不依赖人类或 VLM 反馈，而是使用冻结实用模型从生成的视频中提取 $\textit{可测量代理}$：光流充当速度的代理，而高级外观特征充当质量的代理。这些代理可以通过两个互补的奖励来显式执行牛顿结构：强制恒定加速度动力学的牛顿运动学约束，以及防止琐碎的退化解决方案的质量守恒奖励。我们使用我们新构建的大规模基准 $\texttt{NewtonBench-60K}$ 在五个牛顿运动基元（自由落体、水平/抛物线投掷和斜坡向下/向上滑动）上评估 $\texttt{NewtonRewards}$。在视觉和物理指标的所有基元中，与之前的训练后方法相比，$\texttt{NewtonRewards}$ 始终如一地提高了物理合理性、运动平滑度和时间连贯性。它进一步在高度、速度和摩擦力的分布外变化下保持强劲的性能。我们的结果表明，基于物理的可验证奖励为生成物理感知视频提供了一条可扩展的路径。</li>
</ul>

<h3>Title: Recognizing Pneumonia in Real-World Chest X-rays with a Classifier Trained with Images Synthetically Generated by Nano Banana</h3>
<ul>
<li><strong>Authors: </strong>Jiachuan Peng, Kyle Lam, Jianing Qiu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00428">https://arxiv.org/abs/2512.00428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00428">https://arxiv.org/pdf/2512.00428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00428]] Recognizing Pneumonia in Real-World Chest X-rays with a Classifier Trained with Images Synthetically Generated by Nano Banana(https://arxiv.org/abs/2512.00428)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We trained a classifier with synthetic chest X-ray (CXR) images generated by Nano Banana, the latest AI model for image generation and editing, released by Google. When directly applied to real-world CXRs having only been trained with synthetic data, the classifier achieved an AUROC of 0.923 (95% CI: 0.919 - 0.927), and an AUPR of 0.900 (95% CI: 0.894 - 0.907) in recognizing pneumonia in the 2018 RSNA Pneumonia Detection dataset (14,863 CXRs), and an AUROC of 0.824 (95% CI: 0.810 - 0.836), and an AUPR of 0.913 (95% CI: 0.904 - 0.922) in the Chest X-Ray dataset (5,856 CXRs). These external validation results on real-world data demonstrate the feasibility of this approach and suggest potential for synthetic data in medical AI development. Nonetheless, several limitations remain at present, including challenges in prompt design for controlling the diversity of synthetic CXR data and the requirement for post-processing to ensure alignment with real-world data. However, the growing sophistication and accessibility of medical intelligence will necessitate substantial validation, regulatory approval, and ethical oversight prior to clinical translation.</li>
<li><strong>摘要：</strong>我们使用 Nano Banana 生成的合成胸部 X 射线 (CXR) 图像训练了一个分类器，Nano Banana 是 Google 发布的用于图像生成和编辑的最新 AI 模型。当直接应用于仅使用合成数据进行训练的现实世界 CXR 时，分类器在 2018 年 RSNA 肺炎检测数据集中识别肺炎时取得了 0.923 的 AUROC（95% CI：0.919 - 0.927）和 0.900 的 AUPR（95% CI：0.894 - 0.907）（14,863） CXR），胸部 X 射线数据集（5,856 个 CXR）中的 AUROC 为 0.824（95% CI：0.810 - 0.836），AUPR 为 0.913（95% CI：0.904 - 0.922）。这些对现实世界数据的外部验证结果证明了这种方法的可行性，并表明合成数据在医疗人工智能开发中的潜力。尽管如此，目前仍然存在一些局限性，包括控制合成 CXR 数据多样性的快速设计方面的挑战以及后处理的要求以确保与真实世界数据的一致性。然而，医学情报的日益复杂性和可及性将需要在临床转化之前进行大量验证、监管批准和道德监督。</li>
</ul>

<h3>Title: Privacy-Preserving Generative Modeling and Clinical Validation of Longitudinal Health Records for Chronic Disease</h3>
<ul>
<li><strong>Authors: </strong>Benjamin D. Ballyk, Ankit Gupta, Sujay Konda, Kavitha Subramanian, Chris Landon, Ahmed Ammar Naseer, Georg Maierhofer, Sumanth Swaminathan, Vasudevan Venkateshwaran</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00434">https://arxiv.org/abs/2512.00434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00434">https://arxiv.org/pdf/2512.00434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00434]] Privacy-Preserving Generative Modeling and Clinical Validation of Longitudinal Health Records for Chronic Disease(https://arxiv.org/abs/2512.00434)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Data privacy is a critical challenge in modern medical workflows as the adoption of electronic patient records has grown rapidly. Stringent data protection regulations limit access to clinical records for training and integrating machine learning models that have shown promise in improving diagnostic accuracy and personalized care outcomes. Synthetic data offers a promising alternative; however, current generative models either struggle with time-series data or lack formal privacy guaranties. In this paper, we enhance a state-of-the-art time-series generative model to better handle longitudinal clinical data while incorporating quantifiable privacy safeguards. Using real data from chronic kidney disease and ICU patients, we evaluate our method through statistical tests, a Train-on-Synthetic-Test-on-Real (TSTR) setup, and expert clinical review. Our non-private model (Augmented TimeGAN) outperforms transformer- and flow-based models on statistical metrics in several datasets, while our private model (DP-TimeGAN) maintains a mean authenticity of 0.778 on the CKD dataset, outperforming existing state-of-the-art models on the privacy-utility frontier. Both models achieve performance comparable to real data in clinician evaluations, providing robust input data necessary for developing models for complex chronic conditions without compromising data privacy.</li>
<li><strong>摘要：</strong>随着电子病历的采用迅速增长，数据隐私是现代医疗工作流程中的一个关键挑战。严格的数据保护法规限制了对临床记录的访问，以进行培训和集成机器学习模型，而这些模型在提高诊断准确性和个性化护理结果方面已显示出希望。综合数据提供了一个有前途的替代方案；然而，当前的生成模型要么难以处理时间序列数据，要么缺乏正式的隐私保证。在本文中，我们增强了最先进的时间序列生成模型，以更好地处理纵向临床数据，同时纳入可量化的隐私保护措施。使用来自慢性肾病和 ICU 患者的真实数据，我们通过统计测试、真实合成测试训练 (TSTR) 设置和专家临床审查来评估我们的方法。我们的非私有模型 (Augmented TimeGAN) 在多个数据集中的统计指标上优于基于变压器和流的模型，而我们的私有模型 (DP-TimeGAN) 在 CKD 数据集上保持了 0.778 的平均真实性，在隐私实用性前沿方面优于现有的最先进模型。这两种模型的性能可与临床医生评估中的真实数据相媲美，为开发复杂慢性病模型提供所需的可靠输入数据，同时又不损害数据隐私。</li>
</ul>

<h3>Title: FR-TTS: Test-Time Scaling for NTP-based Image Generation with Effective Filling-based Reward Signal</h3>
<ul>
<li><strong>Authors: </strong>Hang Xu, Linjiang Huang, Feng Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00438">https://arxiv.org/abs/2512.00438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00438">https://arxiv.org/pdf/2512.00438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00438]] FR-TTS: Test-Time Scaling for NTP-based Image Generation with Effective Filling-based Reward Signal(https://arxiv.org/abs/2512.00438)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Test-time scaling (TTS) has become a prevalent technique in image generation, significantly boosting output quality by expanding the number of parallel samples and filtering them using pre-trained reward models. However, applying this powerful methodology to the next-token prediction (NTP) paradigm remains challenging. The primary obstacle is the low correlation between the reward of an image decoded from an intermediate token sequence and the reward of the fully generated image. Consequently, these incomplete intermediate representations prove to be poor indicators for guiding the pruning direction, a limitation that stems from their inherent incompleteness in scale or semantic content. To effectively address this critical issue, we introduce the Filling-Based Reward (FR). This novel design estimates the approximate future trajectory of an intermediate sample by finding and applying a reasonable filling scheme to complete the sequence. Both the correlation coefficient between rewards of intermediate samples and final samples, as well as multiple intrinsic signals like token confidence, indicate that the FR provides an excellent and reliable metric for accurately evaluating the quality of intermediate samples. Building upon this foundation, we propose FR-TTS, a sophisticated scaling strategy. FR-TTS efficiently searches for good filling schemes and incorporates a diversity reward with a dynamic weighting schedule to achieve a balanced and comprehensive evaluation of intermediate samples. We experimentally validate the superiority of FR-TTS over multiple established benchmarks and various reward models. Code is available at \href{this https URL}{this https URL}.</li>
<li><strong>摘要：</strong>测试时间缩放 (TTS) 已成为图像生成中的一种流行技术，通过扩展并行样本的数量并使用预先训练的奖励模型对其进行过滤，显着提高了输出质量。然而，将这种强大的方法应用于下一个令牌预测（NTP）范式仍然具有挑战性。主要障碍是从中间令牌序列解码的图像的奖励与完全生成的图像的奖励之间的相关性较低。因此，这些不完整的中间表示被证明是指导修剪方向的不良指标，这是由于它们在规模或语义内容上固有的不完整性而产生的限制。为了有效解决这个关键问题，我们引入了基于填充的奖励（FR）。这种新颖的设计通过寻找并应用合理的填充方案来完成序列来估计中间样本的大致未来轨迹。中间样本和最终样本的奖励之间的相关系数，以及令牌置信度等多个内在信号都表明，FR 为准确评估中间样本的质量提供了一个优秀且可靠的指标。在此基础上，我们提出了 FR-TTS，一种复杂的扩展策略。 FR-TTS有效地搜索良好的填充方案，并将多样性奖励与动态加权方案相结合，以实现对中间样本的平衡和综合评估。我们通过实验验证了 FR-TTS 相对于多个既定基准和各种奖励模型的优越性。代码可在 \href{此 https URL}{此 https URL} 中找到。</li>
</ul>

<h3>Title: RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards</h3>
<ul>
<li><strong>Authors: </strong>Junyan Ye, Leiqi Zhu, Yuncheng Guo, Dongzhi Jiang, Zilong Huang, Yifan Zhang, Zhiyuan Yan, Haohuan Fu, Conghui He, Weijia Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00473">https://arxiv.org/abs/2512.00473</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00473">https://arxiv.org/pdf/2512.00473</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00473]] RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards(https://arxiv.org/abs/2512.00473)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>With the continuous advancement of image generation technology, advanced models such as GPT-Image-1 and Qwen-Image have achieved remarkable text-to-image consistency and world knowledge However, these models still fall short in photorealistic image generation. Even on simple T2I tasks, they tend to produce " fake" images with distinct AI artifacts, often characterized by "overly smooth skin" and "oily facial sheens". To recapture the original goal of "indistinguishable-from-reality" generation, we propose RealGen, a photorealistic text-to-image framework. RealGen integrates an LLM component for prompt optimization and a diffusion model for realistic image generation. Inspired by adversarial generation, RealGen introduces a "Detector Reward" mechanism, which quantifies artifacts and assesses realism using both semantic-level and feature-level synthetic image detectors. We leverage this reward signal with the GRPO algorithm to optimize the entire generation pipeline, significantly enhancing image realism and detail. Furthermore, we propose RealBench, an automated evaluation benchmark employing Detector-Scoring and Arena-Scoring. It enables human-free photorealism assessment, yielding results that are more accurate and aligned with real user experience. Experiments demonstrate that RealGen significantly outperforms general models like GPT-Image-1 and Qwen-Image, as well as specialized photorealistic models like FLUX-Krea, in terms of realism, detail, and aesthetics. The code is available at this https URL.</li>
<li><strong>摘要：</strong>随着图像生成技术的不断进步，GPT-Image-1和Qwen-Image等先进模型已经实现了显着的文本到图像一致性和世界知识，但是这些模型在真实感图像生成方面仍然存在不足。即使在简单的 T2I 任务中，他们也倾向于生成具有明显 AI 伪影的“假”图像，通常以“皮肤过于光滑”和“油性面部光泽”为特征。为了重新实现“与现实无法区分”的生成的最初目标，我们提出了 RealGen，一种逼真的文本到图像框架。 RealGen 集成了用于快速优化的 LLM 组件和用于生成真实图像的扩散模型。受对抗生成的启发，RealGen 引入了“检测器奖励”机制，该机制使用语义级和特征级合成图像检测器来量化伪影并评估真实性。我们利用 GRPO 算法的奖励信号来优化整个生成流程，显着增强图像的真实感和细节。此外，我们提出了 RealBench，这是一种采用 Detector-Scoring 和 Arena-Scoring 的自动评估基准。它可以实现无人操作的真实感评估，产生更准确且符合真实用户体验的结果。实验表明，RealGen 在真实感、细节和美观方面显着优于 GPT-Image-1 和 Qwen-Image 等通用模型以及 FLUX-Krea 等专业真实感模型。该代码可从此 https URL 获取。</li>
</ul>

<h3>Title: CC-FMO: Camera-Conditioned Zero-Shot Single Image to 3D Scene Generation with Foundation Model Orchestration</h3>
<ul>
<li><strong>Authors: </strong>Boshi Tang, Henry Zheng, Rui Huang, Gao Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00493">https://arxiv.org/abs/2512.00493</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00493">https://arxiv.org/pdf/2512.00493</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00493]] CC-FMO: Camera-Conditioned Zero-Shot Single Image to 3D Scene Generation with Foundation Model Orchestration(https://arxiv.org/abs/2512.00493)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>High-quality 3D scene generation from a single image is crucial for AR/VR and embodied AI applications. Early approaches struggle to generalize due to reliance on specialized models trained on curated small datasets. While recent advancements in large-scale 3D foundation models have significantly enhanced instance-level generation, coherent scene generation remains a challenge, where performance is limited by inaccurate per-object pose estimations and spatial inconsistency. To this end, this paper introduces CC-FMO, a zero-shot, camera-conditioned pipeline for single-image to 3D scene generation that jointly conforms to the object layout in input image and preserves instance fidelity. CC-FMO employs a hybrid instance generator that combines semantics-aware vector-set representation with detail-rich structured latent representation, yielding object geometries that are both semantically plausible and high-quality. Furthermore, CC-FMO enables the application of foundational pose estimation models in the scene generation task via a simple yet effective camera-conditioned scale-solving algorithm, to enforce scene-level coherence. Extensive experiments demonstrate that CC-FMO consistently generates high-fidelity camera-aligned compositional scenes, outperforming all state-of-the-art methods.</li>
<li><strong>摘要：</strong>从单个图像生成高质量 3D 场景对于 AR/VR 和嵌入式 AI 应用至关重要。由于依赖于在精心策划的小型数据集上训练的专门模型，早期的方法很难泛化。虽然大规模 3D 基础模型的最新进展显着增强了实例级生成，但连贯场景生成仍然是一个挑战，其中性能受到每个对象姿态估计不准确和空间不一致的限制。为此，本文引入了 CC-FMO，这是一种零镜头、相机条件管道，用于单图像到 3D 场景生成，共同符合输入图像中的对象布局并保留实例保真度。 CC-FMO 采用混合实例生成器，将语义感知向量集表示与细节丰富的结构化潜在表示相结合，产生语义上合理且高质量的对象几何形状。此外，CC-FMO 通过简单而有效的相机条件尺度求解算法，可以在场景生成任务中应用基础姿态估计模型，以增强场景级一致性。大量实验表明，CC-FMO 始终能够生成高保真相机对齐的合成场景，优于所有最先进的方法。</li>
</ul>

<h3>Title: Image Generation as a Visual Planner for Robotic Manipulation</h3>
<ul>
<li><strong>Authors: </strong>Ye Pang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00532">https://arxiv.org/abs/2512.00532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00532">https://arxiv.org/pdf/2512.00532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00532]] Image Generation as a Visual Planner for Robotic Manipulation(https://arxiv.org/abs/2512.00532)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating realistic robotic manipulation videos is an important step toward unifying perception, planning, and action in embodied agents. While existing video diffusion models require large domain-specific datasets and struggle to generalize, recent image generation models trained on language-image corpora exhibit strong compositionality, including the ability to synthesize temporally coherent grid images. This suggests a latent capacity for video-like generation even without explicit temporal modeling. We explore whether such models can serve as visual planners for robots when lightly adapted using LoRA finetuning. We propose a two-part framework that includes: (1) text-conditioned generation, which uses a language instruction and the first frame, and (2) trajectory-conditioned generation, which uses a 2D trajectory overlay and the same initial frame. Experiments on the Jaco Play dataset, Bridge V2, and the RT1 dataset show that both modes produce smooth, coherent robot videos aligned with their respective conditions. Our findings indicate that pretrained image generators encode transferable temporal priors and can function as video-like robotic planners under minimal supervision. Code is released at \href{this https URL}{this https URL}.</li>
<li><strong>摘要：</strong>生成逼真的机器人操作视频是统一具体代理的感知、规划和行动的重要一步。虽然现有的视频扩散模型需要大量特定领域的数据集并且难以泛化，但最近在语言图像语料库上训练的图像生成模型表现出很强的组合性，包括合成时间相干网格图像的能力。这表明即使没有明确的时间建模，也具有类似视频生成的潜在能力。我们探索这些模型在使用 LoRA 微调进行轻微调整后是否可以充当机器人的视觉规划器。我们提出了一个由两部分组成的框架，其中包括：(1) 文本条件生成，它使用语言指令和第一帧；(2) 轨迹条件生成，它使用 2D 轨迹叠加和相同的初始帧。 Jaco Play 数据集、Bridge V2 和 RT1 数据集上的实验表明，两种模式都能生成与其各自条件相符的平滑、连贯的机器人视频。我们的研究结果表明，预训练的图像生成器对可转移的时间先验进行编码，并且可以在最少的监督下充当类似视频的机器人规划器。代码发布于\href{此 https URL}{此 https URL}。</li>
</ul>

<h3>Title: SAIDO: Generalizable Detection of AI-Generated Images via Scene-Aware and Importance-Guided Dynamic Optimization in Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>Yongkang Hu, Yu Cheng, Yushuo Zhang, Yuan Xie, Zhaoxia Yin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00539">https://arxiv.org/abs/2512.00539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00539">https://arxiv.org/pdf/2512.00539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00539]] SAIDO: Generalizable Detection of AI-Generated Images via Scene-Aware and Importance-Guided Dynamic Optimization in Continual Learning(https://arxiv.org/abs/2512.00539)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The widespread misuse of image generation technologies has raised security concerns, driving the development of AI-generated image detection methods. However, generalization has become a key challenge and open problem: existing approaches struggle to adapt to emerging generative methods and content types in real-world scenarios. To address this issue, we propose a Scene-Aware and Importance-Guided Dynamic Optimization detection framework with continual learning (SAIDO). Specifically, we design Scene-Awareness-Based Expert Module (SAEM) that dynamically identifies and incorporates new scenes using VLLMs. For each scene, independent expert modules are dynamically allocated, enabling the framework to capture scene-specific forgery features better and enhance cross-scene generalization. To mitigate catastrophic forgetting when learning from multiple image generative methods, we introduce Importance-Guided Dynamic Optimization Mechanism (IDOM), which optimizes each neuron through an importance-guided gradient projection strategy, thereby achieving an effective balance between model plasticity and stability. Extensive experiments on continual learning tasks demonstrate that our method outperforms the current SOTA method in both stability and plasticity, achieving 44.22\% and 40.57\% relative reductions in average detection error rate and forgetting rate, respectively. On open-world datasets, it improves the average detection accuracy by 9.47\% compared to the current SOTA method.</li>
<li><strong>摘要：</strong>图像生成技术的广泛滥用引发了安全问题，推动了人工智能生成图像检测方法的发展。然而，泛化已成为一个关键挑战和开放性问题：现有方法很难适应现实场景中新兴的生成方法和内容类型。为了解决这个问题，我们提出了一种具有持续学习功能的场景感知和重要性引导动态优化检测框架（SAIDO）。具体来说，我们设计了基于场景感知的专家模块 (SAEM)，它使用 VLLM 动态识别和合并新场景。对于每个场景，动态分配独立的专家模块，使框架能够更好地捕获特定场景的伪造特征并增强跨场景泛化能力。为了减轻学习多种图像生成方法时的灾难性遗忘，我们引入了重要性引导的动态优化机制（IDOM），它通过重要性引导的梯度投影策略来优化每个神经元，从而实现模型可塑性和稳定性之间的有效平衡。对持续学习任务的大量实验表明，我们的方法在稳定性和可塑性方面都优于当前的 SOTA 方法，平均检测错误率和遗忘率分别相对降低了 44.22% 和 40.57%。在开放世界数据集上，与当前的 SOTA 方法相比，它的平均检测精度提高了 9.47%。</li>
</ul>

<h3>Title: Asset-Driven Sematic Reconstruction of Dynamic Scene with Multi-Human-Object Interactions</h3>
<ul>
<li><strong>Authors: </strong>Sandika Biswas, Qianyi Wu, Biplab Banerjee, Hamid Rezatofighi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00547">https://arxiv.org/abs/2512.00547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00547">https://arxiv.org/pdf/2512.00547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00547]] Asset-Driven Sematic Reconstruction of Dynamic Scene with Multi-Human-Object Interactions(https://arxiv.org/abs/2512.00547)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Real-world human-built environments are highly dynamic, involving multiple humans and their complex interactions with surrounding objects. While 3D geometry modeling of such scenes is crucial for applications like AR/VR, gaming, and embodied AI, it remains underexplored due to challenges like diverse motion patterns and frequent occlusions. Beyond novel view rendering, 3D Gaussian Splatting (GS) has demonstrated remarkable progress in producing detailed, high-quality surface geometry with fast optimization of the underlying structure. However, very few GS-based methods address multihuman, multiobject scenarios, primarily due to the above-mentioned inherent challenges. In a monocular setup, these challenges are further amplified, as maintaining structural consistency under severe occlusion becomes difficult when the scene is optimized solely based on GS-based rendering loss. To tackle the challenges of such a multihuman, multiobject dynamic scene, we propose a hybrid approach that effectively combines the advantages of 1) 3D generative models for generating high-fidelity meshes of the scene elements, 2) Semantic-aware deformation, \ie rigid transformation of the rigid objects and LBS-based deformation of the humans, and mapping of the deformed high-fidelity meshes in the dynamic scene, and 3) GS-based optimization of the individual elements for further refining their alignments in the scene. Such a hybrid approach helps maintain the object structures even under severe occlusion and can produce multiview and temporally consistent geometry. We choose HOI-M3 for evaluation, as, to the best of our knowledge, this is the only dataset featuring multihuman, multiobject interactions in a dynamic scene. Our method outperforms the state-of-the-art method in producing better surface reconstruction of such scenes.</li>
<li><strong>摘要：</strong>现实世界中的人造环境是高度动态的，涉及多人及其与周围物体的复杂交互。虽然此类场景的 3D 几何建模对于 AR/VR、游戏和嵌入式 AI 等应用至关重要，但由于运动模式多样化和频繁遮挡等挑战，它的探索仍然不足。除了新颖的视图渲染之外，3D 高斯喷射 (GS) 在生成详细、高质量的表面几何形状以及快速优化底层结构方面也取得了显着进展。然而，基于 GS 的方法很少能够解决多人、多对象场景，这主要是由于上述固有的挑战。在单目设置中，这些挑战进一步放大，因为当场景仅基于基于 GS 的渲染损失进行优化时，在严重遮挡下保持结构一致性变得困难。为了应对这种多人、多对象动态场景的挑战，我们提出了一种混合方法，该方法有效地结合了以下优点：1）用于生成场景元素的高保真网格的 3D 生成模型，2）语义感知变形，即刚性对象的刚性变换和基于 LBS 的人体变形，以及动态场景中变形的高保真网格的映射，以及 3）基于 GS 的单个元素的优化，以进一步细化它们在现场。这种混合方法即使在严重遮挡的情况下也有助于保持对象结构，并且可以产生多视图和时间一致的几何形状。我们选择 HOI-M3 进行评估，因为据我们所知，这是唯一一个在动态场景中实现多人、多对象交互的数据集。我们的方法在对此类场景进行更好的表面重建方面优于最先进的方法。</li>
</ul>

<h3>Title: NeuroVolve: Evolving Visual Stimuli toward Programmable Neural Objectives</h3>
<ul>
<li><strong>Authors: </strong>Haomiao Chen, Keith W Jamison, Mert R. Sabuncu, Amy Kuceyeski</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00557">https://arxiv.org/abs/2512.00557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00557">https://arxiv.org/pdf/2512.00557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00557]] NeuroVolve: Evolving Visual Stimuli toward Programmable Neural Objectives(https://arxiv.org/abs/2512.00557)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>What visual information is encoded in individual brain regions, and how do distributed patterns combine to create their neural representations? Prior work has used generative models to replicate known category selectivity in isolated regions (e.g., faces in FFA), but these approaches offer limited insight into how regions interact during complex, naturalistic vision. We introduce NeuroVolve, a generative framework that provides brain-guided image synthesis via optimization of a neural objective function in the embedding space of a pretrained vision-language model. Images are generated under the guidance of a programmable neural objective, i.e., activating or deactivating single regions or multiple regions together. NeuroVolve is validated by recovering known selectivity for individual brain regions, while expanding to synthesize coherent scenes that satisfy complex, multi-region constraints. By tracking optimization steps, it reveals semantic trajectories through embedding space, unifying brain-guided image editing and preferred stimulus generation in a single process. We show that NeuroVolve can generate both low-level and semantic feature-specific stimuli for single ROIs, as well as stimuli aligned to curated neural objectives. These include co-activation and decorrelation between regions, exposing cooperative and antagonistic tuning relationships. Notably, the framework captures subject-specific preferences, supporting personalized brain-driven synthesis and offering interpretable constraints for mapping, analyzing, and probing neural representations of visual information.</li>
<li><strong>摘要：</strong>哪些视觉信息被编码在各个大脑区域中，以及分布式模式如何组合以创建它们的神经表征？先前的工作已使用生成模型来复制孤立区域（例如 FFA 中的人脸）中已知的类别选择性，但这些方法对区域在复杂、自然的视觉过程中如何相互作用的了解有限。我们介绍 NeuroVolve，一个生成框架，通过优化预训练视觉语言模型的嵌入空间中的神经目标函数来提供大脑引导的图像合成。图像是在可编程神经目标的指导下生成的，即激活或停用单个区域或多个区域。 NeuroVolve 通过恢复单个大脑区域的已知选择性进行验证，同时扩展以合成满足复杂的多区域约束的连贯场景。通过跟踪优化步骤，它通过嵌入空间揭示语义轨迹，在单个过程中统一大脑引导图像编辑和首选刺激生成。我们证明 NeuroVolve 可以为单个 ROI 生成低级和语义特征特定的刺激，以及与策划的神经目标一致的刺激。这些包括区域之间的共同激活和去相关，暴露合作和对抗的调谐关系。值得注意的是，该框架捕获了特定主题的偏好，支持个性化的大脑驱动合成，并为映射、分析和探测视觉信息的神经表示提供可解释的约束。</li>
</ul>

<h3>Title: Generalized Graph Transformer Variational Autoencoder</h3>
<ul>
<li><strong>Authors: </strong>Siddhant Karki</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00612">https://arxiv.org/abs/2512.00612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00612">https://arxiv.org/pdf/2512.00612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00612]] Generalized Graph Transformer Variational Autoencoder(https://arxiv.org/abs/2512.00612)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Graph link prediction has long been a central problem in graph representation learning in both network analysis and generative modeling. Recent progress in deep learning has introduced increasingly sophisticated architectures for capturing relational dependencies within graph-structured data. In this work, we propose the Generalized Graph Transformer Variational Autoencoder (GGT-VAE). Our model integrates Generalized Graph Transformer Architecture with Variational Autoencoder framework for link prediction. Unlike prior GraphVAE, GCN, or GNN approaches, GGT-VAE leverages transformer style global self-attention mechanism along with laplacian positional encoding to model structural patterns across nodes into a latent space without relying on message passing. Experimental results on several benchmark datasets demonstrate that GGT-VAE consistently achieves above-baseline performance in terms of ROC-AUC and Average Precision. To the best of our knowledge, this is among the first studies to explore graph structure generation using a generalized graph transformer backbone in a variational framework.</li>
<li><strong>摘要：</strong>图链接预测长期以来一直是网络分析和生成建模中图表示学习的中心问题。深度学习的最新进展引入了越来越复杂的架构，用于捕获图结构数据中的关系依赖关系。在这项工作中，我们提出了广义图变换器变分自动编码器（GGT-VAE）。我们的模型将广义图变换器架构与变分自动编码器框架集成以进行链接预测。与之前的 GraphVAE、GCN 或 GNN 方法不同，GGT-VAE 利用 Transformer 风格的全局自注意力机制以及拉普拉斯位置编码，将跨节点的结构模式建模到潜在空间中，而不依赖于消息传递。多个基准数据集上的实验结果表明，GGT-VAE 在 ROC-AUC 和平均精度方面始终达到高于基线的性能。据我们所知，这是在变分框架中使用广义图转换器主干探索图结构生成的首批研究之一。</li>
</ul>

<h3>Title: Neuroscience-Inspired Memory Replay for Continual Learning: A Comparative Study of Predictive Coding and Backpropagation-Based Strategies</h3>
<ul>
<li><strong>Authors: </strong>Goutham Nalagatla, Shreyas Grandhe</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00619">https://arxiv.org/abs/2512.00619</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00619">https://arxiv.org/pdf/2512.00619</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00619]] Neuroscience-Inspired Memory Replay for Continual Learning: A Comparative Study of Predictive Coding and Backpropagation-Based Strategies(https://arxiv.org/abs/2512.00619)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Continual learning remains a fundamental challenge in artificial intelligence, with catastrophic forgetting posing a significant barrier to deploying neural networks in dynamic environments. Inspired by biological memory consolidation mechanisms, we propose a novel framework for generative replay that leverages predictive coding principles to mitigate forgetting. We present a comprehensive comparison between predictive coding-based and backpropagation-based gen- erative replay strategies, evaluating their effectiveness on task retention and transfer efficiency across multiple benchmark datasets. Our experimental results demonstrate that predictive coding-based replay achieves superior retention performance (average 15.3% improvement) while maintaining competitive transfer efficiency, suggesting that biologically-inspired mechanisms can offer principled solutions to continual learning challenges. The proposed framework provides insights into the relationship between biological memory processes and artificial learning systems, opening new avenues for neuroscience-inspired AI research.</li>
<li><strong>摘要：</strong>持续学习仍然是人工智能的一个基本挑战，灾难性遗忘对在动态环境中部署神经网络构成了重大障碍。受生物记忆巩固机制的启发，我们提出了一种新颖的生成重放框架，利用预测编码原理来减轻遗忘。我们对基于预测编码和基于反向传播的生成重放策略进行了全面比较，评估了它们在多个基准数据集上的任务保留和传输效率方面的有效性。我们的实验结果表明，基于预测编码的重放实现了卓越的保留性能（平均提高了 15.3%），同时保持了有竞争力的传输效率，这表明受生物学启发的机制可以为持续学习挑战提供有原则的解决方案。所提出的框架提供了对生物记忆过程和人工学习系统之间关系的见解，为神经科学启发的人工智能研究开辟了新途径。</li>
</ul>

<h3>Title: XAI-Driven Skin Disease Classification: Leveraging GANs to Augment ResNet-50 Performance</h3>
<ul>
<li><strong>Authors: </strong>Kim Gerard A. Villanueva, Priyanka Kumar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00626">https://arxiv.org/abs/2512.00626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00626">https://arxiv.org/pdf/2512.00626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00626]] XAI-Driven Skin Disease Classification: Leveraging GANs to Augment ResNet-50 Performance(https://arxiv.org/abs/2512.00626)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Accurate and timely diagnosis of multi-class skin lesions is hampered by subjective methods, inherent data imbalance in datasets like HAM10000, and the "black box" nature of Deep Learning (DL) models. This study proposes a trustworthy and highly accurate Computer-Aided Diagnosis (CAD) system to overcome these limitations. The approach utilizes Deep Convolutional Generative Adversarial Networks (DCGANs) for per class data augmentation to resolve the critical class imbalance problem. A fine-tuned ResNet-50 classifier is then trained on the augmented dataset to classify seven skin disease categories. Crucially, LIME and SHAP Explainable AI (XAI) techniques are integrated to provide transparency by confirming that predictions are based on clinically relevant features like irregular morphology. The system achieved a high overall Accuracy of 92.50 % and a Macro-AUC of 98.82 %, successfully outperforming various prior benchmarked architectures. This work successfully validates a verifiable framework that combines high performance with the essential clinical interpretability required for safe diagnostic deployment. Future research should prioritize enhancing discrimination for critical categories, such as Melanoma NOS (F1-Score is 0.8602).</li>
<li><strong>摘要：</strong>主观方法、HAM10000等数据集中固有的数据不平衡以及深度学习（DL）模型的“黑匣子”性质阻碍了多类皮肤病变的准确及时诊断。本研究提出了一种值得信赖且高度准确的计算机辅助诊断 (CAD) 系统来克服这些限制。该方法利用深度卷积生成对抗网络（DCGAN）进行每类数据增强，以解决关键的类不平衡问题。然后在增强数据集上训练经过微调的 ResNet-50 分类器，以对七种皮肤疾病类别进行分类。至关重要的是，LIME 和 SHAP 可解释人工智能 (XAI) 技术相结合，通过确认预测是基于不规则形态等临床相关特征来提供透明度。该系统实现了 92.50% 的整体准确率和 98.82% 的宏观 AUC，成功超越了各种先前的基准测试架构。这项工作成功验证了一个可验证的框架，该框架将高性能与安全诊断部署所需的基本临床可解释性相结合。未来的研究应优先考虑加强对关键类别的区分，例如黑色素瘤 NOS（F1 得分为 0.8602）。</li>
</ul>

<h3>Title: Privacy Preserving Diffusion Models for Mixed-Type Tabular Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Timur Sattarov, Marco Schreyer, Damian Borth</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00638">https://arxiv.org/abs/2512.00638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00638">https://arxiv.org/pdf/2512.00638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00638]] Privacy Preserving Diffusion Models for Mixed-Type Tabular Data Generation(https://arxiv.org/abs/2512.00638)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce DP-FinDiff, a differentially private diffusion framework for synthesizing mixed-type tabular data. DP-FinDiff employs embedding-based representations for categorical features, reducing encoding overhead and scaling to high-dimensional datasets. To adapt DP-training to the diffusion process, we propose two privacy-aware training strategies: an adaptive timestep sampler that aligns updates with diffusion dynamics, and a feature-aggregated loss that mitigates clipping-induced bias. Together, these enhancements improve fidelity and downstream utility without weakening privacy guarantees. On financial and medical datasets, DP-FinDiff achieves 16-42% higher utility than DP baselines at comparable privacy levels, demonstrating its promise for safe and effective data sharing in sensitive domains.</li>
<li><strong>摘要：</strong>我们引入了 DP-FinDiff，这是一种用于合成混合类型表格数据的差分隐私扩散框架。 DP-FinDiff 对分类特征采用基于嵌入的表示，减少编码开销并扩展到高维数据集。为了使 DP 训练适应扩散过程，我们提出了两种隐私意识训练策略：将更新与扩散动态保持一致的自适应时间步采样器，以及减轻剪切引起的偏差的特征聚合损失。这些增强功能共同提高了保真度和下游实用性，而不会削弱隐私保证。在金融和医疗数据集上，DP-FinDiff 在同等隐私级别下的效用比 DP 基线高出 16-42%，这证明了其在敏感领域安全有效地共享数据的承诺。</li>
</ul>

<h3>Title: ML-Tool-Bench: Tool-Augmented Planning for ML Tasks</h3>
<ul>
<li><strong>Authors: </strong>Yaswanth Chittepu, Raghavendra Addanki, Tung Mai, Anup Rao, Branislav Kveton</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00672">https://arxiv.org/abs/2512.00672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00672">https://arxiv.org/pdf/2512.00672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00672]] ML-Tool-Bench: Tool-Augmented Planning for ML Tasks(https://arxiv.org/abs/2512.00672)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The development of autonomous machine learning (ML) agents capable of end-to-end data science workflows represents a significant frontier in artificial intelligence. These agents must orchestrate complex sequences of data analysis, feature engineering, model selection, and hyperparameter optimization, tasks that require sophisticated planning and iteration. While recent work on building ML agents has explored using large language models (LLMs) for direct code generation, tool-augmented approaches offer greater modularity and reliability. However, existing tool-use benchmarks focus primarily on task-specific tool selection or argument extraction for tool invocation, failing to evaluate the sophisticated planning capabilities required for ML Agents. In this work, we introduce a comprehensive benchmark for evaluating tool-augmented ML agents using a curated set of 61 specialized tools and 15 tabular ML challenges from Kaggle. Our benchmark goes beyond traditional tool-use evaluation by incorporating an in-memory named object management, allowing agents to flexibly name, save, and retrieve intermediate results throughout the workflows. We demonstrate that standard ReAct-style approaches struggle to generate valid tool sequences for complex ML pipelines, and that tree search methods with LLM-based evaluation underperform due to inconsistent state scoring. To address these limitations, we propose two simple approaches: 1) using shaped deterministic rewards with structured textual feedback, and 2) decomposing the original problem into a sequence of sub-tasks, which significantly improves trajectory validity and task performance. Using GPT-4o, our approach improves over ReAct by 16.52 percentile positions, taking the median across all Kaggle challenges. We believe our work provides a foundation for developing more capable tool-augmented planning ML agents.</li>
<li><strong>摘要：</strong>能够实现端到端数据科学工作流程的自主机器学习（ML）代理的开发代表了人工智能的重要前沿。这些代理必须协调复杂的数据分析、特征工程、模型选择和超参数优化序列，以及需要复杂规划和迭代的任务。虽然最近构建 ML 代理的工作已经探索使用大型语言模型 (LLM) 来直接生成代码，但工具增强方法提供了更高的模块化性和可靠性。然而，现有的工具使用基准主要关注特定于任务的工具选择或工具调用的参数提取，未能评估 ML Agent 所需的复杂规划功能。在这项工作中，我们引入了一个综合基准，用于使用来自 Kaggle 的一组精选的 61 个专用工具和 15 个表格 ML 挑战来评估工具增强的 ML 代理。我们的基准测试超越了传统的工具使用评估，结合了内存中命名对象管理，允许代理在整个工作流程中灵活命名、保存和检索中间结果。我们证明了标准的 ReAct 风格的方法很难为复杂的 ML 管道生成有效的工具序列，并且由于状态评分不一致，基于 LLM 的评估的树搜索方法表现不佳。为了解决这些限制，我们提出了两种简单的方法：1）使用具有结构化文本反馈的形状确定性奖励，2）将原始问题分解为一系列子任务，这显着提高了轨迹有效性和任务性能。使用 GPT-4o，我们的方法比 ReAct 提高了 16.52 个百分点，在所有 Kaggle 挑战中取得了中位数。我们相信我们的工作为开发更强大的工具增强规划机器学习代理奠定了基础。</li>
</ul>

<h3>Title: Flow Matching for Tabular Data Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Bahrul Ilmi Nasution, Floor Eijkelboom, Mark Elliot, Richard Allmendinger, Christian A. Naesseth</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00698">https://arxiv.org/abs/2512.00698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00698">https://arxiv.org/pdf/2512.00698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00698]] Flow Matching for Tabular Data Synthesis(https://arxiv.org/abs/2512.00698)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Synthetic data generation is an important tool for privacy-preserving data sharing. While diffusion models have set recent benchmarks, flow matching (FM) offers a promising alternative. This paper presents different ways to implement flow matching for tabular data synthesis. We provide a comprehensive empirical study that compares flow matching (FM and variational FM) with a state-of-the-art diffusion method (TabDDPM and TabSyn) in tabular data synthesis. We evaluate both the standard Optimal Transport (OT) and the Variance Preserving (VP) probability paths, and also compare deterministic and stochastic samplers -- something possible when learning to generate using \textit{variational} flow matching -- characterising the empirical relationship between data utility and privacy risk. Our key findings reveal that flow matching, particularly TabbyFlow, outperforms diffusion baselines. Flow matching methods also achieves better performance with remarkably low function evaluations ($\leq$ 100 steps), offering a substantial computational advantage. The choice of probability path is also crucial, as using the OT path demonstrates superior performance, while VP has potential for producing synthetic data with lower disclosure risk. Lastly, our results show that making flows stochastic not only preserves marginal distributions but, in some instances, enables the generation of high utility synthetic data with reduced disclosure risk.</li>
<li><strong>摘要：</strong>合成数据生成是保护隐私的数据共享的重要工具。虽然扩散模型已经设定了最近的基准，但流量匹配（FM）提供了一个有前途的替代方案。本文提出了实现表格数据合成的流匹配的不同方法。我们提供了一项全面的实证研究，将表格数据合成中的流匹配（FM 和变分 FM）与最先进的扩散方法（TabDDPM 和 TabSyn）进行比较。我们评估标准最优传输（OT）和方差保持（VP）概率路径，并比较确定性和随机采样器——在学习使用 \textit{variational} 流匹配生成时可能发生的情况——表征数据效用和隐私风险之间的经验关系。我们的主要发现表明，流量匹配（尤其是 TabbyFlow）的性能优于扩散基线。流匹配方法还以非常低的函数评估（$\leq$ 100 步骤）实现了更好的性能，提供了巨大的计算优势。概率路径的选择也至关重要，因为使用 OT 路径表现出优越的性能，而 VP 有潜力生成具有较低披露风险的合成数据。最后，我们的结果表明，使流量随机化不仅可以保留边际分布，而且在某些情况下可以生成高效用的合成数据，同时降低披露风险。</li>
</ul>

<h3>Title: Optimizing LVLMs with On-Policy Data for Effective Hallucination Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Chengzhi Yu, Yifan Xu, Yifan Chen, Wenyi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00706">https://arxiv.org/abs/2512.00706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00706">https://arxiv.org/pdf/2512.00706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00706]] Optimizing LVLMs with On-Policy Data for Effective Hallucination Mitigation(https://arxiv.org/abs/2512.00706)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recently, large vision-language models (LVLMs) have risen to be a promising approach for multimodal tasks. However, principled hallucination mitigation remains a critical this http URL this work, we first analyze the data generation process in LVLM hallucination mitigation and affirm that on-policy data significantly outperforms off-policy data, which thus calls for efficient and reliable preference annotation of on-policy data. We then point out that, existing annotation methods introduce additional hallucination in training samples, which may enhance the model's hallucination patterns, to address this problem, we propose training a hallucination classifier giving binary annotations, which guarantee clean chosen samples for the subsequent alignment. To further harness of the power of on-policy data, we design a robust iterative direct preference optimization (DPO) algorithm adopting a dynamic sample reweighting scheme. We conduct comprehensive experiments on three benchmarks with comparison to 8 state-of-the-art baselines. In particular, our approach reduces the hallucination rate of LLaVA-1.5-7B on MMHalBench by 50.8% and the average hallucination rate on Object HalBench by 79.5%; more significantly, our method fully taps into the potential of open-source models, enabling LLaVA-1.5-13B to even surpass the performance of GPT-4V.</li>
<li><strong>摘要：</strong>最近，大型视觉语言模型（LVLM）已成为多模态任务的一种有前途的方法。然而，有原则的幻觉缓解仍然是本次http URL工作的关键，我们首先分析了LVLM幻觉缓解中的数据生成过程，并确认在政策数据显着优于离政策数据，因此需要对政策数据进行高效可靠的偏好注释。然后我们指出，现有的注释方法在训练样本中引入了额外的幻觉，这可能会增强模型的幻觉模式，为了解决这个问题，我们建议训练一个给出二进制注释的幻觉分类器，这保证了后续对齐的干净选择样本。为了进一步利用同策略数据的力量，我们设计了一种采用动态样本重新加权方案的鲁棒迭代直接偏好优化（DPO）算法。我们对三个基准进行了全面的实验，并与 8 个最先进的基准进行了比较。特别是，我们的方法将MMHalBench上LLaVA-1.5-7B的幻觉率降低了50.8%，将Object HalBench上的平均幻觉率降低了79.5%；更重要的是，我们的方法充分挖掘了开源模型的潜力，使LLaVA-1.5-13B甚至超越了GPT-4V的性能。</li>
</ul>

<h3>Title: TrajDiff: End-to-end Autonomous Driving without Perception Annotation</h3>
<ul>
<li><strong>Authors: </strong>Xingtai Gui, Jianbo Zhao, Wencheng Han, Jikai Wang, Jiahao Gong, Feiyang Tan, Cheng-zhong Xu, Jianbing Shen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00723">https://arxiv.org/abs/2512.00723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00723">https://arxiv.org/pdf/2512.00723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00723]] TrajDiff: End-to-end Autonomous Driving without Perception Annotation(https://arxiv.org/abs/2512.00723)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>End-to-end autonomous driving systems directly generate driving policies from raw sensor inputs. While these systems can extract effective environmental features for planning, relying on auxiliary perception tasks, developing perception annotation-free planning paradigms has become increasingly critical due to the high cost of manual perception annotation. In this work, we propose TrajDiff, a Trajectory-oriented BEV Conditioned Diffusion framework that establishes a fully perception annotation-free generative method for end-to-end autonomous driving. TrajDiff requires only raw sensor inputs and future trajectory, constructing Gaussian BEV heatmap targets that inherently capture driving modalities. We design a simple yet effective trajectory-oriented BEV encoder to extract the TrajBEV feature without perceptual supervision. Furthermore, we introduce Trajectory-oriented BEV Diffusion Transformer (TB-DiT), which leverages ego-state information and the predicted TrajBEV features to directly generate diverse yet plausible trajectories, eliminating the need for handcrafted motion priors. Beyond architectural innovations, TrajDiff enables exploration of data scaling benefits in the annotation-free setting. Evaluated on the NAVSIM benchmark, TrajDiff achieves 87.5 PDMS, establishing state-of-the-art performance among all annotation-free methods. With data scaling, it further improves to 88.5 PDMS, which is comparable to advanced perception-based approaches. Our code and model will be made publicly available.</li>
<li><strong>摘要：</strong>端到端自动驾驶系统直接根据原始传感器输入生成驾驶策略。虽然这些系统可以提取有效的环境特征进行规划，但依赖于辅助感知任务，由于手动感知标注的成本高昂，开发无感知标注的规划范式变得越来越重要。在这项工作中，我们提出了 TrajDiff，一种面向轨迹的 BEV 条件扩散框架，为端到端自动驾驶建立了完全感知无注释的生成方法。 TrajDiff 仅需要原始传感器输入和未来轨迹，构建本质上捕获驾驶模式的高斯 BEV 热图目标。我们设计了一个简单而有效的面向轨迹的 BEV 编码器，无需感知监督即可提取 TrajBEV 特征。此外，我们引入了面向轨迹的 BEV 扩散变压器（TB-DiT），它利用自我状态信息和预测的 TrajBEV 特征来直接生成多样化但合理的轨迹，从而消除了手工制作运动先验的需要。除了架构创新之外，TrajDiff 还可以在无注释的环境中探索数据扩展的优势。根据 NAVSIM 基准评估，TrajDiff 达到 87.5 PDMS，在所有无注释方法中建立了最先进的性能。通过数据扩展，它进一步提高到 88.5 PDMS，与先进的基于感知的方法相当。我们的代码和模型将公开。</li>
</ul>

<h3>Title: Deep Learning for Modeling and Dispatching Hybrid Wind Farm Power Generation</h3>
<ul>
<li><strong>Authors: </strong>Zach Lawrence, Jessica Yao, Chris Qin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00728">https://arxiv.org/abs/2512.00728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00728">https://arxiv.org/pdf/2512.00728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00728]] Deep Learning for Modeling and Dispatching Hybrid Wind Farm Power Generation(https://arxiv.org/abs/2512.00728)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Wind farms with integrated energy storage, or hybrid wind farms, are able to store energy and dispatch it to the grid following an operational strategy. For individual wind farms with integrated energy storage capacity, data-driven dispatch strategies using localized grid demand and market conditions as input parameters stand to maximize wind energy value. Synthetic power generation data modeled on atmospheric conditions provide another avenue for improving the robustness of data-driven dispatch strategies. To these ends, the present work develops two deep learning frameworks: COVE-NN, an LSTM-based dispatch strategy tailored to individual wind farms, which reduced annual COVE by 32.3% over 43 years of simulated operations in a case study at the Pyron site; and a power generation modeling framework that reduced RMSE by 9.5% and improved power curve similarity by 18.9% when validated on the Palouse wind farm. Together, these models pave the way for more robust, data-driven dispatch strategies and potential extensions to other renewable energy systems.</li>
<li><strong>摘要：</strong>具有集成储能功能​​的风电场或混合风电场能够存储能量并按照运营策略将其调度到电网。对于具有综合储能能力的单个风电场，使用本地电网需求和市场条件作为输入参数的数据驱动的调度策略可以最大化风能价值。基于大气条件建模的综合发电数据为提高数据驱动的调度策略的鲁棒性提供了另一种途径。为此，目前的工作开发了两个深度学习框架：COVE-NN，一种针对单个风电场的基于 LSTM 的调度策略，在 Pyron 站点的案例研究中，经过 43 年的模拟运行，每年 COVE 降低了 32.3%；在帕卢斯风电场进行验证时，发电建模框架将 RMSE 降低了 9.5%，功率曲线相似度提高了 18.9%。这些模型共同为更强大、数据驱动的调度策略和其他可再生能源系统的潜在扩展铺平了道路。</li>
</ul>

<h3>Title: Multi-GRPO: Multi-Group Advantage Estimation for Text-to-Image Generation with Tree-Based Trajectories and Multiple Rewards</h3>
<ul>
<li><strong>Authors: </strong>Qiang Lyu, Zicong Chen, Chongxiao Wang, Haolin Shi, Shibo Gao, Ran Piao, Youwei Zeng, Jianlou Si, Fei Ding, Jing Li, Chun Pong Lau, Weiqiang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00743">https://arxiv.org/abs/2512.00743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00743">https://arxiv.org/pdf/2512.00743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00743]] Multi-GRPO: Multi-Group Advantage Estimation for Text-to-Image Generation with Tree-Based Trajectories and Multiple Rewards(https://arxiv.org/abs/2512.00743)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recently, Group Relative Policy Optimization (GRPO) has shown promising potential for aligning text-to-image (T2I) models, yet existing GRPO-based methods suffer from two critical limitations. (1) \textit{Shared credit assignment}: trajectory-level advantages derived from group-normalized sparse terminal rewards are uniformly applied across timesteps, failing to accurately estimate the potential of early denoising steps with vast exploration spaces. (2) \textit{Reward-mixing}: predefined weights for combining multi-objective rewards (e.g., text accuracy, visual quality, text color)--which have mismatched scales and variances--lead to unstable gradients and conflicting updates. To address these issues, we propose \textbf{Multi-GRPO}, a multi-group advantage estimation framework with two orthogonal grouping mechanisms. For better credit assignment, we introduce tree-based trajectories inspired by Monte Carlo Tree Search: branching trajectories at selected early denoising steps naturally forms \emph{temporal groups}, enabling accurate advantage estimation for early steps via descendant leaves while amortizing computation through shared prefixes. For multi-objective optimization, we introduce \emph{reward-based grouping} to compute advantages for each reward function \textit{independently} before aggregation, disentangling conflicting signals. To facilitate evaluation of multiple objective alignment, we curate \textit{OCR-Color-10}, a visual text rendering dataset with explicit color constraints. Across the single-reward \textit{PickScore-25k} and multi-objective \textit{OCR-Color-10} benchmarks, Multi-GRPO achieves superior stability and alignment performance, effectively balancing conflicting objectives. Code will be publicly available at \href{this https URL}{this https URL}.</li>
<li><strong>摘要：</strong>最近，组相对策略优化 (GRPO) 在对齐文本到图像 (T2I) 模型方面显示出了巨大的潜力，但现有的基于 GRPO 的方法存在两个关键限制。 (1) \textit{共享信用分配}：从群体归一化稀疏终端奖励中得出的轨迹级优势在时间步上统一应用，无法准确估计具有巨大探索空间的早期去噪步骤的潜力。 (2) \textit{奖励混合}：用于组合多目标奖励（例如，文本准确性、视觉质量、文本颜色）的预定义权重——其尺度和方差不匹配——导致不稳定的梯度和冲突的更新。为了解决这些问题，我们提出了 \textbf{Multi-GRPO}，一种具有两种正交分组机制的多组优势估计框架。为了更好地分配信用，我们引入了受蒙特卡洛树搜索启发的基于树的轨迹：选定的早期去噪步骤的分支轨迹自然地形成\emph{时间组}，从而能够通过后代叶对早期步骤进行准确的优势估计，同时通过共享前缀分摊计算。对于多目标优化，我们引入 \emph{基于奖励的分组} 来在聚合之前计算每个奖励函数 \textit{独立}的优势，从而理清冲突信号。为了促进多目标对齐的评估，我们策划了 \textit{OCR-Color-10}，一个具有明确颜色约束的视觉文本渲染数据集。在单奖励 \textit{PickScore-25k} 和多目标 \textit{OCR-Color-10} 基准测试中，Multi-GRPO 实现了卓越的稳定性和对齐性能，有效平衡了相互冲突的目标。代码将在 \href{此 https URL}{此 https URL} 中公开提供。</li>
</ul>

<h3>Title: Charts Are Not Images: On the Challenges of Scientific Chart Editing</h3>
<ul>
<li><strong>Authors: </strong>Shawn Li, Ryan Rossi, Sungchul Kim, Sunav Choudhary, Franck Dernoncourt, Puneet Mathur, Zhengzhong Tu, Yue Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00752">https://arxiv.org/abs/2512.00752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00752">https://arxiv.org/pdf/2512.00752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00752]] Charts Are Not Images: On the Challenges of Scientific Chart Editing(https://arxiv.org/abs/2512.00752)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models, such as diffusion and autoregressive approaches, have demonstrated impressive capabilities in editing natural images. However, applying these tools to scientific charts rests on a flawed assumption: a chart is not merely an arrangement of pixels but a visual representation of structured data governed by a graphical grammar. Consequently, chart editing is not a pixel-manipulation task but a structured transformation problem. To address this fundamental mismatch, we introduce \textit{FigEdit}, a large-scale benchmark for scientific figure editing comprising over 30,000 samples. Grounded in real-world data, our benchmark is distinguished by its diversity, covering 10 distinct chart types and a rich vocabulary of complex editing instructions. The benchmark is organized into five distinct and progressively challenging tasks: single edits, multi edits, conversational edits, visual-guidance-based edits, and style transfer. Our evaluation of a range of state-of-the-art models on this benchmark reveals their poor performance on scientific figures, as they consistently fail to handle the underlying structured transformations required for valid edits. Furthermore, our analysis indicates that traditional evaluation metrics (e.g., SSIM, PSNR) have limitations in capturing the semantic correctness of chart edits. Our benchmark demonstrates the profound limitations of pixel-level manipulation and provides a robust foundation for developing and evaluating future structure-aware models. By releasing \textit{FigEdit} (this https URL), we aim to enable systematic progress in structure-aware figure editing, provide a common ground for fair comparison, and encourage future research on models that understand both the visual and semantic layers of scientific charts.</li>
<li><strong>摘要：</strong>生成模型，例如扩散和自回归方法，在编辑自然图像方面表现出了令人印象深刻的能力。然而，将这些工具应用于科学图表取决于一个有缺陷的假设：图表不仅仅是像素的排列，而且是受图形语法控制的结构化数据的视觉表示。因此，图表编辑不是像素操作任务，而是结构化转换问题。为了解决这种根本性的不匹配问题，我们引入了 \textit{FigEdit}，这是一个包含 30,000 多个样本的科学图形编辑的大规模基准。我们的基准以现实世界的数据为基础，以其多样性而著称，涵盖 10 种不同的图表类型和丰富的复杂编辑指令词汇。该基准分为五个不同且逐渐具有挑战性的任务：单次编辑、多次编辑、对话式编辑、基于视觉指导的编辑和风格转换。我们在此基准上对一系列最先进模型的评估表明，它们在科学数据上的表现不佳，因为它们始终无法处理有效编辑所需的底层结构化转换。此外，我们的分析表明，传统的评估指标（例如 SSIM、PSNR）在捕获图表编辑的语义正确性方面存在局限性。我们的基准测试展示了像素级操作的深刻局限性，并为开发和评估未来的结构感知模型提供了坚实的基础。通过发布 \textit{FigEdit} （此 https URL），我们的目标是在结构感知图形编辑方面取得系统性进展，为公平比较提供共同基础，并鼓励未来对能够理解科学图表的视觉和语义层的模型进行研究。</li>
</ul>

<h3>Title: Preventing Model Collapse via Contraction-Conditioned Neural Filters</h3>
<ul>
<li><strong>Authors: </strong>Zongjian Han, Yiran Liang, Ruiwen Wang, Yiwei Luo, Yilin Huang, Xiaotong Song, Dongqing Wei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00757">https://arxiv.org/abs/2512.00757</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00757">https://arxiv.org/pdf/2512.00757</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00757]] Preventing Model Collapse via Contraction-Conditioned Neural Filters(https://arxiv.org/abs/2512.00757)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper presents a neural network filter method based on contraction operators to address model collapse in recursive training of generative models. Unlike \cite{xu2024probabilistic}, which requires superlinear sample growth ($O(t^{1+s})$), our approach completely eliminates the dependence on increasing sample sizes within an unbiased estimation framework by designing a neural filter that learns to satisfy contraction conditions. We develop specialized neural network architectures and loss functions that enable the filter to actively learn contraction conditions satisfying Assumption 2.3 in exponential family distributions, thereby ensuring practical application of our theoretical results. Theoretical analysis demonstrates that when the learned contraction conditions are satisfied, estimation errors converge probabilistically even with constant sample sizes, i.e., $\limsup_{t\to\infty}\mathbb{P}(\|\mathbf{e}_t\|>\delta)=0$ for any $\delta>0$. Experimental results show that our neural network filter effectively learns contraction conditions and prevents model collapse under fixed sample size settings, providing an end-to-end solution for practical applications.</li>
<li><strong>摘要：</strong>本文提出了一种基于收缩算子的神经网络过滤方法，以解决生成模型递归训练中的模型崩溃问题。与需要超线性样本增长（$O(t^{1+s})$）的 \cite{xu2024probabilistic} 不同，我们的方法通过设计一个学习满足收缩条件的神经滤波器，完全消除了对无偏估计框架内增加样本大小的依赖。我们开发了专门的神经网络架构和损失函数，使滤波器能够主动学习指数族分布中满足假设2.3的收缩条件，从而确保我们的理论结果的实际应用。理论分析表明，当满足学习的收缩条件时，即使样本大小恒定，估计误差也会概率收敛，即对于任何 $\delta>0$，$\limsup_{t\to\infty}\mathbb{P}(\|\mathbf{e}_t\|>\delta)=0$。实验结果表明，我们的神经网络滤波器在固定样本大小设置下有效学习收缩条件并防止模型崩溃，为实际应用提供了端到端的解决方案。</li>
</ul>

<h3>Title: Seeing the Wind from a Falling Leaf</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Gao, Jiageng Mao, Hong-Xing Yu, Haozhe Lou, Emily Yue-Ting Jia, Jernej Barbic, Jiajun Wu, Yue Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00762">https://arxiv.org/abs/2512.00762</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00762">https://arxiv.org/pdf/2512.00762</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00762]] Seeing the Wind from a Falling Leaf(https://arxiv.org/abs/2512.00762)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>A longstanding goal in computer vision is to model motions from videos, while the representations behind motions, i.e. the invisible physical interactions that cause objects to deform and move, remain largely unexplored. In this paper, we study how to recover the invisible forces from visual observations, e.g., estimating the wind field by observing a leaf falling to the ground. Our key innovation is an end-to-end differentiable inverse graphics framework, which jointly models object geometry, physical properties, and interactions directly from videos. Through backpropagation, our approach enables the recovery of force representations from object motions. We validate our method on both synthetic and real-world scenarios, and the results demonstrate its ability to infer plausible force fields from videos. Furthermore, we show the potential applications of our approach, including physics-based video generation and editing. We hope our approach sheds light on understanding and modeling the physical process behind pixels, bridging the gap between vision and physics. Please check more video results in our \href{this https URL}{project page}.</li>
<li><strong>摘要：</strong>计算机视觉的一个长期目标是对视频中的运动进行建模，而运动背后的表示，即导致物体变形和移动的不可见的物理交互，在很大程度上仍未被探索。在本文中，我们研究如何从视觉观察中恢复看不见的力，例如，通过观察掉落到地面的叶子来估计风场。我们的关键创新是端到端可微逆图形框架，该框架直接对视频中的对象几何形状、物理属性和交互进行联合建模。通过反向传播，我们的方法能够从物体运动中恢复力的表示。我们在合成场景和现实场景中验证了我们的方法，结果证明了其从视频推断合理力场的能力。此外，我们展示了我们的方法的潜在应用，包括基于物理的视频生成和编辑。我们希望我们的方法有助于理解和建模像素背后的物理过程，弥合视觉和物理之间的差距。请在我们的\href{此 https URL}{项目页面}中查看更多视频结果。</li>
</ul>

<h3>Title: AI Agent for Source Finding by SoFiA-2 for SKA-SDC2</h3>
<ul>
<li><strong>Authors: </strong>Xingchen Zhou, Nan Li, Peng Jia, Yingfeng Liu, Furen Deng, Shuanghao Shu, Ying Li, Liang Cao, Huanyuan Shan, Ayodeji Ibitoye</a></li>
<li><strong>Subjects: </strong>cs.LG, astro-ph.GA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00769">https://arxiv.org/abs/2512.00769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00769">https://arxiv.org/pdf/2512.00769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00769]] AI Agent for Source Finding by SoFiA-2 for SKA-SDC2(https://arxiv.org/abs/2512.00769)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Source extraction is crucial in analyzing data from next-generation, large-scale sky surveys in radio bands, such as the Square Kilometre Array (SKA). Several source extraction programs, including SoFiA and Aegean, have been developed to address this challenge. However, finding optimal parameter configurations when applying these programs to real observations is non-trivial. For example, the outcomes of SoFiA intensely depend on several key parameters across its preconditioning, source-finding, and reliability-filtering modules. To address this issue, we propose a framework to automatically optimize these parameters using an AI agent based on a state-of-the-art reinforcement learning (RL) algorithm, i.e., Soft Actor-Critic (SAC). The SKA Science Data Challenge 2 (SDC2) dataset is utilized to assess the feasibility and reliability of this framework. The AI agent interacts with the environment by adjusting parameters based on the feedback from the SDC2 score defined by the SDC2 Team, progressively learning to select parameter sets that yield improved performance. After sufficient training, the AI agent can automatically identify an optimal parameter configuration that outperform the benchmark set by Team SoFiA within only 100 evaluation steps and with reduced time consumption. Our approach could address similar problems requiring complex parameter tuning, beyond radio band surveys and source extraction. Yet, high-quality training sets containing representative observations and catalogs of ground truth are essential.</li>
<li><strong>摘要：</strong>源提取对于分析下一代大规模射电波段天空勘测（例如平方公里阵列 (SKA)）的数据至关重要。为了应对这一挑战，已经开发了多个源提取程序，包括 SoFiA 和 Aegean。然而，将这些程序应用于实际观测时找到最佳参数配置并非易事。例如，SoFiA 的结果很大程度上取决于其预处理、源查找和可靠性过滤模块中的几个关键参数。为了解决这个问题，我们提出了一个框架，使用基于最先进的强化学习 (RL) 算法（即 Soft Actor-Critic (SAC)）的人工智能代理来自动优化这些参数。 SKA Science Data Challenge 2 (SDC2) 数据集用于评估该框架的可行性和可靠性。 AI 代理通过根据 SDC2 团队定义的 SDC2 分数的反馈调整参数来与环境交互，逐步学习选择可提高性能的参数集。经过充分的训练后，AI 代理只需 100 个评估步骤即可自动识别出优于 Team SoFiA 设定的基准的最佳参数配置，并减少了时间消耗。我们的方法可以解决需要复杂参数调整的类似问题，超越无线电频段调查和源提取。然而，包含代表性观察结果和基本事实目录的高质量训练集至关重要。</li>
</ul>

<h3>Title: IRPO: Boosting Image Restoration via Post-training GRPO</h3>
<ul>
<li><strong>Authors: </strong>Haoxuan Xu. Yi Liu, Boyuan Jiang, Jinlong Peng, Donghao Luo, Xiaobin Hu, Shuicheng Yan, Haoang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00814">https://arxiv.org/abs/2512.00814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00814">https://arxiv.org/pdf/2512.00814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00814]] IRPO: Boosting Image Restoration via Post-training GRPO(https://arxiv.org/abs/2512.00814)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generation</a></li>
<li><strong>Abstract: </strong>Recent advances in post-training paradigms have achieved remarkable success in high-level generation tasks, yet their potential for low-level vision remains rarely explored. Existing image restoration (IR) methods rely on pixel-level hard-fitting to ground-truth images, struggling with over-smoothing and poor generalization. To address these limitations, we propose IRPO, a low-level GRPO-based post-training paradigm that systematically explores both data formulation and reward modeling. We first explore a data formulation principle for low-level post-training paradigm, in which selecting underperforming samples from the pre-training stage yields optimal performance and improved efficiency. Furthermore, we model a reward-level criteria system that balances objective accuracy and human perceptual preference through three complementary components: a General Reward for structural fidelity, an Expert Reward leveraging Qwen-VL for perceptual alignment, and a Restoration Reward for task-specific low-level quality. Comprehensive experiments on six in-domain and five out-of-domain (OOD) low-level benchmarks demonstrate that IRPO achieves state-of-the-art results across diverse degradation types, surpassing the AdaIR baseline by 0.83 dB on in-domain tasks and 3.43 dB on OOD settings. Our code can be shown in this https URL.</li>
<li><strong>摘要：</strong>训练后范式的最新进展在高级生成任务中取得了显着的成功，但它们在低级视觉方面的潜力仍然很少被探索。现有的图像恢复（IR）方法依赖于像素级硬拟合地面实况图像，面临过度平滑和泛化不良的问题。为了解决这些限制，我们提出了 IRPO，这是一种基于 GRPO 的低级训练后范例，系统地探索数据公式和奖励建模。我们首先探索低水平后训练范式的数据制定原则，其中从预训练阶段选择表现不佳的样本可以产生最佳性能并提高效率。此外，我们建模了一个奖励级别标准系统，通过三个互补的组件来平衡客观准确性和人类感知偏好：结构保真度的一般奖励、利用 Qwen-VL 进行感知对齐的专家奖励以及针对特定任务的低水平质量的恢复奖励。对 6 个域内和 5 个域外 (OOD) 低级基准的综合实验表明，IRPO 在不同的退化类型中均取得了最先进的结果，在域内任务上超过 AdaIR 基线 0.83 dB，在 OOD 设置上超过 AdaIR 基线 3.43 dB。我们的代码可以在这个 https URL 中显示。</li>
</ul>

<h3>Title: PanFlow: Decoupled Motion Control for Panoramic Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Cheng Zhang, Hanwen Liang, Donny Y. Chen, Qianyi Wu, Konstantinos N. Plataniotis, Camilo Cruz Gambardella, Jianfei Cai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00832">https://arxiv.org/abs/2512.00832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00832">https://arxiv.org/pdf/2512.00832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00832]] PanFlow: Decoupled Motion Control for Panoramic Video Generation(https://arxiv.org/abs/2512.00832)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Panoramic video generation has attracted growing attention due to its applications in virtual reality and immersive media. However, existing methods lack explicit motion control and struggle to generate scenes with large and complex motions. We propose PanFlow, a novel approach that exploits the spherical nature of panoramas to decouple the highly dynamic camera rotation from the input optical flow condition, enabling more precise control over large and dynamic motions. We further introduce a spherical noise warping strategy to promote loop consistency in motion across panorama boundaries. To support effective training, we curate a large-scale, motion-rich panoramic video dataset with frame-level pose and flow annotations. We also showcase the effectiveness of our method in various applications, including motion transfer and video editing. Extensive experiments demonstrate that PanFlow significantly outperforms prior methods in motion fidelity, visual quality, and temporal coherence. Our code, dataset, and models are available at this https URL.</li>
<li><strong>摘要：</strong>全景视频生成因其在虚拟现实和沉浸式媒体中的应用而受到越来越多的关注。然而，现有的方法缺乏明确的运动控制，并且难以生成具有大而复杂运动的场景。我们提出了 PanFlow，这是一种利用全景图的球形性质将高动态相机旋转与输入光流条件解耦的新颖方法，从而能够更精确地控制大型动态运动。我们进一步引入球形噪声扭曲策略来促进跨全景边界运动的循环一致性。为了支持有效的训练，我们策划了一个具有帧级姿态和流注释的大规模、运动丰富的全景视频数据集。我们还展示了我们的方法在各种应用中的有效性，包括运动传输和视频编辑。大量实验表明，PanFlow 在运动保真度、视觉质量和时间连贯性方面显着优于现有方法。我们的代码、数据集和模型可从此 https URL 获取。</li>
</ul>

<h3>Title: Towards Active Synthetic Data Generation for Finetuning Language Models</h3>
<ul>
<li><strong>Authors: </strong>Samuel Kessler, Menglin Xia, Daniel Madrigal Diaz, Dongge Han, Helia Heshemi, Saravan Rajmohan, Victor Ruehle, Jordan T. Ash</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00884">https://arxiv.org/abs/2512.00884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00884">https://arxiv.org/pdf/2512.00884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00884]] Towards Active Synthetic Data Generation for Finetuning Language Models(https://arxiv.org/abs/2512.00884)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>A common and effective means for improving language model capabilities involves finetuning a ``student'' language model's parameters on generations from a more proficient ``teacher'' model. Termed ``synthetic data'', these generations are often produced before any student finetuning, but some work has considered generating new synthetic samples as training progresses. This paper studies and advocates for the latter case, where data are generated in an iterative, closed-loop fashion that is guided by the current state of the student model. For a fixed budget of generated samples, or a budget in terms of compute spent querying a teacher, we show that this curation of finetuning data affords improved student performance over static generation. Further, while there have been several LLM-specific methods proposed that operate in this regime, we find that simple, inexpensive selection criteria from the active learning literature tend to be most performant. We validate these claims across four mathematical and logical reasoning datasets using four different small language models.</li>
<li><strong>摘要：</strong>提高语言模型能力的一种常见且有效的方法是在更熟练的“教师”模型的基础上对“学生”语言模型的参数进行微调。这些生成数据被称为“合成数据”，通常是在任何学生微调之前生成的，但有些工作已经考虑随着训练的进展生成新的合成样本。本文研究并提倡后一种情况，即数据以迭代、闭环的方式生成，并由学生模型的当前状态引导。对于生成样本的固定预算，或者查询教师所花费的计算预算，我们表明，这种微调数据的管理可以比静态生成提高学生的表现。此外，虽然已经提出了几种适用于这种制度的法学硕士特定方法，但我们发现来自主动学习文献的简单、廉价的选择标准往往是最有效的。我们使用四种不同的小语言模型在四个数学和逻辑推理数据集上验证这些主张。</li>
</ul>

<h3>Title: SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead</h3>
<ul>
<li><strong>Authors: </strong>Chaojun Ni, Cheng Chen, Xiaofeng Wang, Zheng Zhu, Wenzhao Zheng, Boyuan Wang, Tianrun Chen, Guosheng Zhao, Haoyun Li, Zhehao Dong, Qiang Zhang, Yun Ye, Yang Wang, Guan Huang, Wenjun Mei</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00903">https://arxiv.org/abs/2512.00903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00903">https://arxiv.org/pdf/2512.00903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00903]] SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead(https://arxiv.org/abs/2512.00903)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision-Language-Action (VLA) models built on pretrained Vision-Language Models (VLMs) show strong potential but are limited in practicality due to their large parameter counts. To mitigate this issue, using a lightweight VLM has been explored, but it compromises spatiotemporal reasoning. Although some methods suggest that incorporating additional 3D inputs can help, they usually rely on large VLMs to fuse 3D and 2D inputs and still lack temporal understanding. Therefore, we propose SwiftVLA, an architecture that enhances a compact model with 4D understanding while preserving design efficiency. Specifically, our approach features a pretrained 4D visual geometry transformer with a temporal cache that extracts 4D features from 2D images. Then, to enhance the VLM's ability to exploit both 2D images and 4D features, we introduce Fusion Tokens, a set of learnable tokens trained with a future prediction objective to generate unified representations for action generation. Finally, we introduce a mask-and-reconstruct strategy that masks 4D inputs to the VLM and trains the VLA to reconstruct them, enabling the VLM to learn effective 4D representations and allowing the 4D branch to be dropped at inference with minimal performance loss. Experiments in real and simulated environments show that SwiftVLA outperforms lightweight baselines and rivals VLAs up to 7 times larger, achieving comparable performance on edge devices while being 18 times faster and reducing memory footprint by 12 times.</li>
<li><strong>摘要：</strong>基于预训练视觉语言模型 (VLM) 构建的视觉语言动作 (VLA) 模型显示出强大的潜力，但由于参数数量较多，实用性受到限制。为了缓解这个问题，人们已经探索使用轻量级 VLM，但它会损害时空推理。尽管一些方法表明合并额外的 3D 输入会有所帮助，但它们通常依赖大型 VLM 来融合 3D 和 2D 输入，并且仍然缺乏时间理解。因此，我们提出了 SwiftVLA，这种架构可以增强具有 4D 理解的紧凑模型，同时保持设计效率。具体来说，我们的方法具有预训练的 4D 视觉几何变换器和时间缓存，可从 2D 图像中提取 4D 特征。然后，为了增强 VLM 利用 2D 图像和 4D 特征的能力，我们引入了 Fusion Token，这是一组可学习的 token，经过未来预测目标的训练，可以生成用于生成动作的统一表示。最后，我们引入了一种屏蔽和重建策略，该策略屏蔽 VLM 的 4D 输入并训练 VLA 来重建它们，使 VLM 能够学习有效的 4D 表示，并允许在推理时丢弃 4D 分支，同时性能损失最小。真实和模拟环境中的实验表明，SwiftVLA 的性能优于轻量级基线，与 VLA 相比，其性能提高了 7 倍，在边缘设备上实现了可比的性能，同时速度提高了 18 倍，内存占用减少了 12 倍。</li>
</ul>

<h3>Title: TalkingPose: Efficient Face and Gesture Animation with Feedback-guided Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Alireza Javanmardi, Pragati Jaiswal, Tewodros Amberbir Habtegebrial, Christen Millerdurai, Shaoxiang Wang, Alain Pagani, Didier Stricker</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00909">https://arxiv.org/abs/2512.00909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00909">https://arxiv.org/pdf/2512.00909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00909]] TalkingPose: Efficient Face and Gesture Animation with Feedback-guided Diffusion Model(https://arxiv.org/abs/2512.00909)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advancements in diffusion models have significantly improved the realism and generalizability of character-driven animation, enabling the synthesis of high-quality motion from just a single RGB image and a set of driving poses. Nevertheless, generating temporally coherent long-form content remains challenging. Existing approaches are constrained by computational and memory limitations, as they are typically trained on short video segments, thus performing effectively only over limited frame lengths and hindering their potential for extended coherent generation. To address these constraints, we propose TalkingPose, a novel diffusion-based framework specifically designed for producing long-form, temporally consistent human upper-body animations. TalkingPose leverages driving frames to precisely capture expressive facial and hand movements, transferring these seamlessly to a target actor through a stable diffusion backbone. To ensure continuous motion and enhance temporal coherence, we introduce a feedback-driven mechanism built upon image-based diffusion models. Notably, this mechanism does not incur additional computational costs or require secondary training stages, enabling the generation of animations with unlimited duration. Additionally, we introduce a comprehensive, large-scale dataset to serve as a new benchmark for human upper-body animation.</li>
<li><strong>摘要：</strong>扩散模型的最新进展显着提高了角色驱动动画的真实性和通用性，使得仅从单个 RGB 图像和一组驾驶姿势即可合成高质量的运动。然而，生成时间连贯的长格式内容仍然具有挑战性。现有方法受到计算和内存限制，因为它们通常在短视频片段上进行训练，因此只能在有限的帧长度上有效执行，并阻碍了它们扩展相干生成的潜力。为了解决这些限制，我们提出了 TalkingPose，这是一种新颖的基于扩散的框架，专门设计用于制作长格式、时间一致的人体上半身动画。 TalkingPose 利用驱动帧精确捕捉富有表现力的面部和手部动作，通过稳定的扩散骨干将这些动作无缝地传输给目标演员。为了确保连续运动并增强时间一致性，我们引入了一种基于图像扩散模型的反馈驱动机制。值得注意的是，这种机制不会产生额外的计算成本或需要二次训练阶段，从而能够生成无限持续时间的动画。此外，我们引入了一个全面的大规模数据集，作为人体上半身动画的新基准。</li>
</ul>

<h3>Title: Dual-Projection Fusion for Accurate Upright Panorama Generation in Robotic Vision</h3>
<ul>
<li><strong>Authors: </strong>Yuhao Shan, Qianyi Yuan, Jingguo Liu, Shigang Li, Jianfeng Li, Tong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00911">https://arxiv.org/abs/2512.00911</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00911">https://arxiv.org/pdf/2512.00911</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00911]] Dual-Projection Fusion for Accurate Upright Panorama Generation in Robotic Vision(https://arxiv.org/abs/2512.00911)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Panoramic cameras, capable of capturing a 360-degree field of view, are crucial in robotic vision, particularly in environments with sparse features. However, non-upright panoramas due to unstable robot postures hinder downstream tasks. Traditional IMU-based correction methods suffer from drift and external disturbances, while vision-based approaches offer a promising alternative. This study presents a dual-stream angle-aware generation network that jointly estimates camera inclination angles and reconstructs upright panoramic images. The network comprises a CNN branch that extracts local geometric structures from equirectangular projections and a ViT branch that captures global contextual cues from cubemap projections. These are integrated through a dual-projection adaptive fusion module that aligns spatial features across both domains. To further enhance performance, we introduce a high-frequency enhancement block, circular padding, and channel attention mechanisms to preserve 360° continuity and improve geometric sensitivity. Experiments on the SUN360 and M3D datasets demonstrate that our method outperforms existing approaches in both inclination estimation and upright panorama generation. Ablation studies further validate the contribution of each module and highlight the synergy between the two tasks. The code and related datasets can be found at: this https URL.</li>
<li><strong>摘要：</strong>能够捕捉 360 度视野的全景相机对于机器人视觉至关重要，尤其是在特征稀疏的环境中。然而，由于机器人姿势不稳定而导致的非直立全景阻碍了下游任务。传统的基于 IMU 的校正方法会受到漂移和外部干扰的影响，而基于视觉的方法提供了一种有前途的替代方法。本研究提出了一种双流角度感知生成网络，可联合估计相机倾斜角度并重建直立全景图像。该网络包括一个从等距矩形投影中提取局部几何结构的 CNN 分支和一个从立方体贴图投影中捕获全局上下文线索的 ViT 分支。这些通过双投影自适应融合模块集成，该模块跨两个域对齐空间特征。为了进一步增强性能，我们引入了高频增强块、圆形填充和通道注意机制，以保持 360° 连续性并提高几何灵敏度。在 SUN360 和 M3D 数据集上的实验表明，我们的方法在倾斜估计和直立全景生成方面都优于现有方法。消融研究进一步验证了每个模块的贡献并强调了两个任务之间的协同作用。代码和相关数据集可以在以下位置找到：此 https URL。</li>
</ul>

<h3>Title: MM-ACT: Learn from Multimodal Parallel Generation to Act</h3>
<ul>
<li><strong>Authors: </strong>Haotian Liang, Xinyi Chen, Bin Wang, Mingkang Chen, Yitian Liu, Yuhao Zhang, Zanxin Chen, Tianshuo Yang, Yilun Chen, Jiangmiao Pang, Dong Liu, Xiaokang Yang, Yao Mu, Wenqi Shao, Ping Luo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00975">https://arxiv.org/abs/2512.00975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00975">https://arxiv.org/pdf/2512.00975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00975]] MM-ACT: Learn from Multimodal Parallel Generation to Act(https://arxiv.org/abs/2512.00975)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>A generalist robotic policy needs both semantic understanding for task planning and the ability to interact with the environment through predictive capabilities. To tackle this, we present MM-ACT, a unified Vision-Language-Action (VLA) model that integrates text, image, and action in shared token space and performs generation across all three modalities. MM-ACT adopts a re-mask parallel decoding strategy for text and image generation, and employs a one-step parallel decoding strategy for action generation to improve efficiency. We introduce Context-Shared Multimodal Learning, a unified training paradigm that supervises generation in all three modalities from a shared context, enhancing action generation through cross-modal learning. Experiments were conducted on the LIBERO simulation and Franka real-robot setups as well as RoboTwin2.0 to assess in-domain and out-of-domain performances respectively. Our approach achieves a success rate of 96.3% on LIBERO, 72.0% across three tasks of real Franka, and 52.38% across eight bimanual tasks of RoboTwin2.0 with an additional gain of 9.25% from cross-modal learning. We release our codes, models and data at this https URL.</li>
<li><strong>摘要：</strong>通用机器人策略既需要任务规划的语义理解，又需要通过预测能力与环境交互的能力。为了解决这个问题，我们提出了 MM-ACT，这是一个统一的视觉-语言-动作 (VLA) 模型，它将文本、图像和动作集成在共享令牌空间中，并跨所有三种模式执行生成。 MM-ACT对于文本和图像生成采用re-mask并行解码策略，对于动作生成采用一步并行解码策略以提高效率。我们引入了上下文共享多模态学习，这是一种统一的训练范式，可以从共享上下文中监督所有三种模态的生成，通过跨模态学习增强动作生成。在 LIBERO 模拟和 Franka 真实机器人设置以及 RoboTwin2.0 上进行了实验，以分别评估域内和域外性能。我们的方法在 LIBERO 上实现了 96.3% 的成功率，在真实 Franka 的三个任务上实现了 72.0% 的成功率，在 RoboTwin2.0 的八个双手任务上实现了 52.38% 的成功率，并且从跨模式学习中获得了 9.25% 的额外增益。我们在此 https URL 发布我们的代码、模型和数据。</li>
</ul>

<h3>Title: Provenance-Driven Reliable Semantic Medical Image Vector Reconstruction via Lightweight Blockchain-Verified Latent Fingerprints</h3>
<ul>
<li><strong>Authors: </strong>Mohsin Rasheed, Abdullah Al-Mamun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00999">https://arxiv.org/abs/2512.00999</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00999">https://arxiv.org/pdf/2512.00999</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00999]] Provenance-Driven Reliable Semantic Medical Image Vector Reconstruction via Lightweight Blockchain-Verified Latent Fingerprints(https://arxiv.org/abs/2512.00999)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Medical imaging is essential for clinical diagnosis, yet real-world data frequently suffers from corruption, noise, and potential tampering, challenging the reliability of AI-assisted interpretation. Conventional reconstruction techniques prioritize pixel-level recovery and may produce visually plausible outputs while compromising anatomical fidelity, an issue that can directly impact clinical outcomes. We propose a semantic-aware medical image reconstruction framework that integrates high-level latent embeddings with a hybrid U-Net architecture to preserve clinically relevant structures during restoration. To ensure trust and accountability, we incorporate a lightweight blockchain-based provenance layer using scale-free graph design, enabling verifiable recording of each reconstruction event without imposing significant overhead. Extensive evaluation across multiple datasets and corruption types demonstrates improved structural consistency, restoration accuracy, and provenance integrity compared with existing approaches. By uniting semantic-guided reconstruction with secure traceability, our solution advances dependable AI for medical imaging, enhancing both diagnostic confidence and regulatory compliance in healthcare environments.</li>
<li><strong>摘要：</strong>医学成像对于临床诊断至关重要，但现实世界的数据经常遭受损坏、噪声和潜在的篡改，这对人工智能辅助解释的可靠性提出了挑战。传统的重建技术优先考虑像素级恢复，可能会产生视觉上合理的输出，同时会损害解剖保真度，这是一个可以直接影响临床结果的问题。我们提出了一种语义感知的医学图像重建框架，该框架将高级潜在嵌入与混合 U-Net 架构相集成，以在恢复过程中保留临床相关结构。为了确保信任和责任，我们使用无标度图形设计整合了一个基于区块链的轻量级来源层，从而可以对每个重建事件进行可验证的记录，而不会产生大量开销。与现有方法相比，跨多个数据集和损坏类型的广泛评估表明结构一致性、恢复准确性和来源完整性得到了改善。通过将语义引导重建与安全可追溯性相结合，我们的解决方案推进了用于医学成像的可靠人工智能，增强了医疗保健环境中的诊断信心和监管合规性。</li>
</ul>

<h3>Title: Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model</h3>
<ul>
<li><strong>Authors: </strong>Jing He, Haodong Li, Mingzhi Sheng, Ying-Cong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01030">https://arxiv.org/abs/2512.01030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01030">https://arxiv.org/pdf/2512.01030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01030]] Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model(https://arxiv.org/abs/2512.01030)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recovering pixel-wise geometric properties from a single image is fundamentally ill-posed due to appearance ambiguity and non-injective mappings between 2D observations and 3D structures. While discriminative regression models achieve strong performance through large-scale supervision, their success is bounded by the scale, quality and diversity of available data and limited physical reasoning. Recent diffusion models exhibit powerful world priors that encode geometry and semantics learned from massive image-text data, yet directly reusing their stochastic generative formulation is suboptimal for deterministic geometric inference: the former is optimized for diverse and high-fidelity image generation, whereas the latter requires stable and accurate predictions. In this work, we propose Lotus-2, a two-stage deterministic framework for stable, accurate and fine-grained geometric dense prediction, aiming to provide an optimal adaption protocol to fully exploit the pre-trained generative priors. Specifically, in the first stage, the core predictor employs a single-step deterministic formulation with a clean-data objective and a lightweight local continuity module (LCM) to generate globally coherent structures without grid artifacts. In the second stage, the detail sharpener performs a constrained multi-step rectified-flow refinement within the manifold defined by the core predictor, enhancing fine-grained geometry through noise-free deterministic flow matching. Using only 59K training samples, less than 1% of existing large-scale datasets, Lotus-2 establishes new state-of-the-art results in monocular depth estimation and highly competitive surface normal prediction. These results demonstrate that diffusion models can serve as deterministic world priors, enabling high-quality geometric reasoning beyond traditional discriminative and generative paradigms.</li>
<li><strong>摘要：</strong>由于 2D 观察和 3D 结构之间的外观模糊性和非内射映射，从单个图像中恢复像素级几何属性从根本上来说是不适定的。虽然判别回归模型通过大规模监督实现了强大的性能，但其成功受到可用数据的规模、质量和多样性以及有限的物理推理的限制。最近的扩散模型展示了强大的世界先验，对从大量图像文本数据中学习到的几何和语义进行编码，但直接重用其随机生成公式对于确定性几何推理来说并不是最优的：前者针对多样化和高保真图像生成进行了优化，而后者则需要稳定且准确的预测。在这项工作中，我们提出了 Lotus-2，一个用于稳定、准确和细粒度几何密集预测的两阶段确定性框架，旨在提供最佳适应协议以充分利用预先训练的生成先验。具体来说，在第一阶段，核心预测器采用具有干净数据目标的单步确定性公式和轻量级局部连续性模块（LCM）来生成没有网格伪影的全局相干结构。在第二阶段，细节锐化器在核心预测器定义的流形内执行约束多步整流流细化，通过无噪声的确定性流匹配增强细粒度几何形状。 Lotus-2 仅使用 59K 训练样本（不到现有大型数据集的 1%），在单目深度估计和极具竞争力的表面法线预测方面建立了新的最先进结果。这些结果表明，扩散模型可以作为确定性的世界先验，从而实现超越传统判别和生成范式的高质量几何推理。</li>
</ul>

<h3>Title: Adaptive-lambda Subtracted Importance Sampled Scores in Machine Unlearning for DDPMs and VAEs</h3>
<ul>
<li><strong>Authors: </strong>MohammadParsa Dini, Human Jafari, Sajjad Amini, MohammadMahdi Mojahedian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01054">https://arxiv.org/abs/2512.01054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01054">https://arxiv.org/pdf/2512.01054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01054]] Adaptive-lambda Subtracted Importance Sampled Scores in Machine Unlearning for DDPMs and VAEs(https://arxiv.org/abs/2512.01054)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Machine Unlearning is essential for large generative models (VAEs, DDPMs) to comply with the right to be forgotten and prevent undesired content generation without costly retraining. Existing approaches, such as Static-lambda SISS for diffusion models, rely on a fixed mixing weight lambda, which is suboptimal because the required unlearning strength varies across samples and training stages. We propose Adaptive-lambda SISS, a principled extension that turns lambda into a latent variable dynamically inferred at each training step. A lightweight inference network parameterizes an adaptive posterior over lambda, conditioned on contextual features derived from the instantaneous SISS loss terms (retain/forget losses and their gradients). This enables joint optimization of the diffusion model and the lambda-inference mechanism via a variational objective, yielding significantly better trade-offs. We further extend the adaptive-lambda principle to score-based unlearning and introduce a multi-class variant of Score Forgetting Distillation. In addition, we present two new directions: (i) a hybrid objective combining the data-free efficiency of Score Forgetting Distillation with the direct gradient control of SISS, and (ii) a Reinforcement Learning formulation that treats unlearning as a sequential decision process, learning an optimal policy over a state space defined by the model's current memory of the forget set. Experiments on an augmented MNIST benchmark show that Adaptive-lambda SISS substantially outperforms the original static-lambda SISS, achieving stronger removal of forgotten classes while better preserving generation quality on the retain set.</li>
<li><strong>摘要：</strong>机器遗忘对于大型生成模型（VAE、DDPM）至关重要，可以遵守被遗忘权并防止生成不需要的内容，而无需进行昂贵的再培训。现有的方法，例如用于扩散模型的静态 lambda SISS，依赖于固定的混合权重 lambda，这是次优的，因为所需的遗忘强度因样本和训练阶段而异。我们提出了 Adaptive-lambda SISS，这是一种原则性扩展，可将 lambda 转换为在每个训练步骤动态推断的潜在变量。轻量级推理网络参数化 lambda 上的自适应后验，以从瞬时 SISS 损失项（保留/忘记损失及其梯度）导出的上下文特征为条件。这使得能够通过变分目标联合优化扩散模型和 lambda 推理机制，从而产生更好的权衡。我们进一步将自适应 lambda 原理扩展到基于分数的遗忘，并引入分数遗忘蒸馏的多类变体。此外，我们提出了两个新方向：（i）将分数遗忘蒸馏的无数据效率与 SISS 的直接梯度控制相结合的混合目标，以及（ii）将遗忘视为顺序决策过程的强化学习公式，在由模型当前的遗忘集记忆定义的状态空间上学习最优策略。增强的 MNIST 基准测试表明，Adaptive-lambda SISS 的性能大大优于原始的 static-lambda SISS，实现了更强的遗忘类去除，同时更好地保留了保留集上的生成质量。</li>
</ul>

<h3>Title: PIANO: Physics-informed Dual Neural Operator for Precipitation Nowcasting</h3>
<ul>
<li><strong>Authors: </strong>Seokhyun Chin, Junghwan Park, Woojin Cho</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01062">https://arxiv.org/abs/2512.01062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01062">https://arxiv.org/pdf/2512.01062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01062]] PIANO: Physics-informed Dual Neural Operator for Precipitation Nowcasting(https://arxiv.org/abs/2512.01062)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Precipitation nowcasting, key for early warning of disasters, currently relies on computationally expensive and restrictive methods that limit access to many countries. To overcome this challenge, we propose precipitation nowcasting using satellite imagery with physics constraints for improved accuracy and physical consistency. We use a novel physics-informed dual neural operator (PIANO) structure to enforce the fundamental equation of advection-diffusion during training to predict satellite imagery using a PINN loss. Then, we use a generative model to convert satellite images to radar images, which are used for precipitation nowcasting. Compared to baseline models, our proposed model shows a notable improvement in moderate (4mm/h) precipitation event prediction alongside short-term heavy (8mm/h) precipitation event prediction. It also demonstrates low seasonal variability in predictions, indicating robustness for generalization. This study suggests the potential of the PIANO and serves as a good baseline for physics-informed precipitation nowcasting.</li>
<li><strong>摘要：</strong>降水临近预报是灾害早期预警的关键，目前依赖于计算成本昂贵且限制性的方法，这些方法限制了许多国家的使用。为了克服这一挑战，我们建议使用具有物理约束的卫星图像进行临近降水预报，以提高准确性和物理一致性。我们使用一种新颖的物理信息双神经算子 (PIANO) 结构来在训练过程中强制执行平流扩散的基本方程，以使用 PINN 损失来预测卫星图像。然后，我们使用生成模型将卫星图像转换为雷达图像，用于降水临近预报。与基线模型相比，我们提出的模型在中度（4毫米/小时）降水事件预测以及短期强（8毫米/小时）降水事件预测方面显示出显着改进。它还表明预测的季节性变化较低，表明泛化的稳健性。这项研究表明了 PIANO 的潜力，并为基于物理的降水临近预报提供了良好的基线。</li>
</ul>

<h3>Title: Generalized Medical Phrase Grounding</h3>
<ul>
<li><strong>Authors: </strong>Wenjun Zhang, Shekhar S. Chandra, Aaron Nicolson</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01085">https://arxiv.org/abs/2512.01085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01085">https://arxiv.org/pdf/2512.01085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01085]] Generalized Medical Phrase Grounding(https://arxiv.org/abs/2512.01085)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Medical phrase grounding (MPG) maps textual descriptions of radiological findings to corresponding image regions. These grounded reports are easier to interpret, especially for non-experts. Existing MPG systems mostly follow the referring expression comprehension (REC) paradigm and return exactly one bounding box per phrase. Real reports often violate this assumption. They contain multi-region findings, non-diagnostic text, and non-groundable phrases, such as negations or descriptions of normal anatomy. Motivated by this, we reformulate the task as generalised medical phrase grounding (GMPG), where each sentence is mapped to zero, one, or multiple scored regions. To realise this formulation, we introduce the first GMPG model: MedGrounder. We adopted a two-stage training regime: pre-training on report sentence--anatomy box alignment datasets and fine-tuning on report sentence--human annotated box datasets. Experiments on PadChest-GR and MS-CXR show that MedGrounder achieves strong zero-shot transfer and outperforms REC-style and grounded report generation baselines on multi-region and non-groundable phrases, while using far fewer human box annotations. Finally, we show that MedGrounder can be composed with existing report generators to produce grounded reports without retraining the generator.</li>
<li><strong>摘要：</strong>医学短语基础 (MPG) 将放射学发现的文本描述映射到相应的图像区域。这些有根据的报告更容易解释，尤其是对于非专家而言。现有的 MPG 系统大多遵循引用表达理解 (REC) 范例，并为每个短语返回一个边界框。真实的报道常常违反这一假设。它们包含多区域发现、非诊断文本和不可根据的短语，例如对正常解剖结构的否定或描述。受此启发，我们将任务重新表述为广义医学短语基础（GMPG），其中每个句子都映射到零、一个或多个评分区域。为了实现这个配方，我们引入了第一个 GMPG 模型：MedGrounder。我们采用了两阶段的训练制度：对报告句子——解剖框对齐数据集进行预训练，对报告句子——人类注释框数据集进行微调。 PadChest-GR 和 MS-CXR 上的实验表明，MedGrounder 实现了强大的零样本传输，并且在多区域和不可接地短语上优于 REC 式和接地报告生成基线，同时使用更少的人类框注释。最后，我们展示了 MedGrounder 可以与现有的报告生成器组合来生成接地报告，而无需重新训练生成器。</li>
</ul>

<h3>Title: Accelerating Inference of Masked Image Generators via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Pranav Subbaraman, Shufan Li, Siyan Zhao, Aditya Grover</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01094">https://arxiv.org/abs/2512.01094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01094">https://arxiv.org/pdf/2512.01094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01094]] Accelerating Inference of Masked Image Generators via Reinforcement Learning(https://arxiv.org/abs/2512.01094)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Masked Generative Models (MGM)s demonstrate strong capabilities in generating high-fidelity images. However, they need many sampling steps to create high-quality generations, resulting in slow inference speed. In this work, we propose Speed-RL, a novel paradigm for accelerating a pretrained MGMs to generate high-quality images in fewer steps. Unlike conventional distillation methods which formulate the acceleration problem as a distribution matching problem, where a few-step student model is trained to match the distribution generated by a many-step teacher model, we consider this problem as a reinforcement learning problem. Since the goal of acceleration is to generate high quality images in fewer steps, we can combine a quality reward with a speed reward and finetune the base model using reinforcement learning with the combined reward as the optimization target. Through extensive experiments, we show that the proposed method was able to accelerate the base model by a factor of 3x while maintaining comparable image quality.</li>
<li><strong>摘要：</strong>蒙版生成模型（MGM）展示了生成高保真图像的强大能力。然而，他们需要许多采样步骤来创建高质量的生成，从而导致推理速度缓慢。在这项工作中，我们提出了 Speed-RL，这是一种加速预训练 MGM 以更少的步骤生成高质量图像的新颖范例。与将加速问题表述为分布匹配问题的传统蒸馏方法不同，在传统的蒸馏方法中，训练几个步骤的学生模型以匹配多步骤的教师模型生成的分布，我们将此问题视为强化学习问题。由于加速的目标是用更少的步骤生成高质量的图像，因此我们可以将质量奖励与速度奖励结合起来，并使用强化学习对基础模型进行微调，并以组合奖励为优化目标。通过大量实验，我们表明所提出的方法能够将基础模型加速 3 倍，同时保持可比较的图像质量。</li>
</ul>

<h3>Title: DPAC: Distribution-Preserving Adversarial Control for Diffusion Sampling</h3>
<ul>
<li><strong>Authors: </strong>Han-Jin Lee, Han-Ju Lee, Jin-Seong Kim, Seok-Hwan Choi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01153">https://arxiv.org/abs/2512.01153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01153">https://arxiv.org/pdf/2512.01153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01153]] DPAC: Distribution-Preserving Adversarial Control for Diffusion Sampling(https://arxiv.org/abs/2512.01153)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Adversarially guided diffusion sampling often achieves the target class, but sample quality degrades as deviations between the adversarially controlled and nominal trajectories accumulate. We formalize this degradation as a path-space Kullback-Leibler divergence(path-KL) between controlled and nominal (uncontrolled) diffusion processes, thereby showing via Girsanov's theorem that it exactly equals the control energy. Building on this stochastic optimal control (SOC) view, we theoretically establish that minimizing this path-KL simultaneously tightens upper bounds on both the 2-Wasserstein distance and Fréchet Inception Distance (FID), revealing a principled connection between adversarial control energy and perceptual fidelity. From a variational perspective, we derive a first-order optimality condition for the control: among all directions that yield the same classification gain, the component tangent to iso-(log-)density surfaces (i.e., orthogonal to the score) minimizes path-KL, whereas the normal component directly increases distributional drift. This leads to DPAC (Distribution-Preserving Adversarial Control), a diffusion guidance rule that projects adversarial gradients onto the tangent space defined by the generative score geometry. We further show that in discrete solvers, the tangent projection cancels the O({\Delta}t) leading error term in the Wasserstein distance, achieving an O({\Delta}t^2) quality gap; moreover, it remains second-order robust to score or metric approximation. Empirical studies on ImageNet-100 validate the theoretical predictions, confirming that DPAC achieves lower FID and estimated path-KL at matched attack success rates.</li>
<li><strong>摘要：</strong>对抗性引导的扩散采样通常可以达到目标类别，但随着对抗性控制轨迹和名义轨迹之间的偏差累积，样本质量会下降。我们将这种退化形式化为受控扩散过程和标称（非受控）扩散过程之间的路径空间 Kullback-Leibler 散度（路径-KL），从而通过吉尔萨诺夫定理表明它完全等于控制能量。基于这种随机最优控制（SOC）观点，我们从理论上确定，最小化这条路径-KL同时收紧了2-Wasserstein距离和Fréchet起始距离（FID）的上限，揭示了对抗性控制能量和感知保真度之间的原则性联系。从变分的角度来看，我们推导出控制的一阶最优条件：在产生相同分类增益的所有方向中，与等（对数）密度表面相切的分量（即与分数正交）最小化路径KL，而法线分量直接增加分布漂移。这导致了 DPAC（保留分布对抗控制），这是一种扩散指导规则，它将对抗梯度投影到由生成分数几何定义的切线空间上。我们进一步表明，在离散求解器中，切线投影消除了 Wasserstein 距离中的 O({\Delta}t) 前导误差项，实现了 O({\Delta}t^2) 质量差距；此外，它对于评分或度量近似仍然具有二阶鲁棒性。 ImageNet-100 的实证研究验证了理论预测，证实 DPAC 在匹配的攻击成功率下实现了较低的 FID 和估计路径 KL。</li>
</ul>

<h3>Title: 2D-ThermAl: Physics-Informed Framework for Thermal Analysis of Circuits using Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Soumyadeep Chandra, Sayeed Shafayet Chowdhury, Kaushik Roy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01163">https://arxiv.org/abs/2512.01163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01163">https://arxiv.org/pdf/2512.01163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01163]] 2D-ThermAl: Physics-Informed Framework for Thermal Analysis of Circuits using Generative AI(https://arxiv.org/abs/2512.01163)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Thermal analysis is increasingly critical in modern integrated circuits, where non-uniform power dissipation and high transistor densities can cause rapid temperature spikes and reliability concerns. Traditional methods, such as FEM-based simulations offer high accuracy but computationally prohibitive for early-stage design, often requiring multiple iterative redesign cycles to resolve late-stage thermal failures. To address these challenges, we propose 'ThermAl', a physics-informed generative AI framework which effectively identifies heat sources and estimates full-chip transient and steady-state thermal distributions directly from input activity profiles. ThermAl employs a hybrid U-Net architecture enhanced with positional encoding and a Boltzmann regularizer to maintain physical fidelity. Our model is trained on an extensive dataset of heat dissipation maps, ranging from simple logic gates (e.g., inverters, NAND, XOR) to complex designs, generated via COMSOL. Experimental results demonstrate that ThermAl delivers precise temperature mappings for large circuits, with a root mean squared error (RMSE) of only 0.71°C, and outperforms conventional FEM tools by running up to ~200 times faster. We analyze performance across diverse layouts and workloads, and discuss its applicability to large-scale EDA workflows. While thermal reliability assessments often extend beyond 85°C for post-layout signoff, our focus here is on early-stage hotspot detection and thermal pattern learning. To ensure generalization beyond the nominal operating range 25-55°C, we additionally performed cross-validation on an extended dataset spanning 25-95°C maintaining a high accuracy (<2.2% full-scale RMSE) even under elevated temperature conditions representative of peak power and stress scenarios.</li>
<li><strong>摘要：</strong>热分析在现代集成电路中变得越来越重要，其中不均匀的功耗和高晶体管密度可能会导致温度快速峰值和可靠性问题。传统方法（例如基于 FEM 的仿真）可提供高精度，但对于早期设计而言计算量过高，通常需要多个迭代重新设计周期来解决后期热故障。为了应对这些挑战，我们提出了“ThermAl”，这是一种基于物理的生成人工智能框架，可有效识别热源并直接根据输入活动曲线估计全芯片瞬态和稳态热分布。 ThermAl 采用混合 U-Net 架构，通过位置编码和玻尔兹曼正则器增强，以保持物理保真度。我们的模型在广泛的散热图数据集上进行训练，范围从简单的逻辑门（例如逆变器、NAND、XOR）到通过 COMSOL 生成的复杂设计。实验结果表明，ThermAl 可为大型电路提供精确的温度映射，均方根误差 (RMSE) 仅 0.71°C，并且运行速度比传统 FEM 工具快约 200 倍。我们分析不同布局和工作负载的性能，并讨论其对大规模 EDA 工作流程的适用性。虽然布局后签核的热可靠性评估通常会超出 85°C，但我们的重点是早期热点检测和热模式学习。为了确保泛化超出 25-55°C 的标称工作范围，我们还对跨越 25-95°C 的扩展数据集进行了交叉验证，即使在代表峰值功率和应力场景的高温条件下也能保持高精度（<2.2% 满量程 RMSE）。</li>
</ul>

<h3>Title: Real-Time On-the-Go Annotation Framework Using YOLO for Automated Dataset Generation</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Abdallah Salem (1), Ahmed Harb Rabia (1) ((1) North Dakota State University)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01165">https://arxiv.org/abs/2512.01165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01165">https://arxiv.org/pdf/2512.01165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01165]] Real-Time On-the-Go Annotation Framework Using YOLO for Automated Dataset Generation(https://arxiv.org/abs/2512.01165)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Efficient and accurate annotation of datasets remains a significant challenge for deploying object detection models such as You Only Look Once (YOLO) in real-world applications, particularly in agriculture where rapid decision-making is critical. Traditional annotation techniques are labor-intensive, requiring extensive manual labeling post data collection. This paper presents a novel real-time annotation approach leveraging YOLO models deployed on edge devices, enabling immediate labeling during image capture. To comprehensively evaluate the efficiency and accuracy of our proposed system, we conducted an extensive comparative analysis using three prominent YOLO architectures (YOLOv5, YOLOv8, YOLOv12) under various configurations: single-class versus multi-class annotation and pretrained versus scratch-based training. Our analysis includes detailed statistical tests and learning dynamics, demonstrating significant advantages of pretrained and single-class configurations in terms of model convergence, performance, and robustness. Results strongly validate the feasibility and effectiveness of our real-time annotation framework, highlighting its capability to drastically reduce dataset preparation time while maintaining high annotation quality.</li>
<li><strong>摘要：</strong>高效、准确的数据集注释仍然是在现实应用中部署诸如 You Only Look Once (YOLO) 等对象检测模型的重大挑战，特别是在快速决策至关重要的农业中。传统的注释技术是劳动密集型的，需要在数据收集后进行大量的手动标记。本文提出了一种新颖的实时注释方法，利用部署在边缘设备上的 YOLO 模型，实现图像捕获期间的即时标记。为了全面评估我们提出的系统的效率和准确性，我们使用三种著名的 YOLO 架构（YOLOv5、YOLOv8、YOLOv12）在各种配置下进行了广泛的比较分析：单类与多类注释以及预训练与基于划痕的训练。我们的分析包括详细的统计测试和学习动态，证明了预训练和单类配置在模型收敛、性能和鲁棒性方面的显着优势。结果有力地验证了我们的实时注释框架的可行性和有效性，突显了其在保持高注释质量的同时大幅减少数据集准备时间的能力。</li>
</ul>

<h3>Title: LGDC: Latent Graph Diffusion via Spectrum-Preserving Coarsening</h3>
<ul>
<li><strong>Authors: </strong>Nagham Osman, Keyue Jiang, Davide Buffelli, Xiaowen Dong, Laura Toni</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01190">https://arxiv.org/abs/2512.01190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01190">https://arxiv.org/pdf/2512.01190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01190]] LGDC: Latent Graph Diffusion via Spectrum-Preserving Coarsening(https://arxiv.org/abs/2512.01190)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Graph generation is a critical task across scientific domains. Existing methods fall broadly into two categories: autoregressive models, which iteratively expand graphs, and one-shot models, such as diffusion, which generate the full graph at once. In this work, we provide an analysis of these two paradigms and reveal a key trade-off: autoregressive models stand out in capturing fine-grained local structures, such as degree and clustering properties, whereas one-shot models excel at modeling global patterns, such as spectral distributions. Building on this, we propose LGDC (latent graph diffusion via spectrum-preserving coarsening), a hybrid framework that combines strengths of both approaches. LGDC employs a spectrum-preserving coarsening-decoarsening to bidirectionally map between graphs and a latent space, where diffusion efficiently generates latent graphs before expansion restores detail. This design captures both local and global properties with improved efficiency. Empirically, LGDC matches autoregressive models on locally structured datasets (Tree) and diffusion models on globally structured ones (Planar, Community-20), validating the benefits of hybrid generation.</li>
<li><strong>摘要：</strong>图生成是跨科学领域的一项关键任务。现有的方法大致分为两类：自回归模型，它迭代地扩展图；以及一次性模型，例如扩散，它一次生成完整的图。在这项工作中，我们对这两种范式进行了分析，并揭示了一个关键的权衡：自回归模型在捕获细粒度局部结构（例如度和聚类属性）方面表现出色，而一次性模型则擅长对全局模式（例如谱分布）进行建模。在此基础上，我们提出了 LGDC（通过频谱保留粗化的潜在图扩散），这是一种结合了两种方法优点的混合框架。 LGDC 采用保留频谱的粗化-去粗化在图和潜在空间之间进行双向映射，其中扩散在扩展恢复细节之前有效地生成潜在图。这种设计捕获了本地和全局属性并提高了效率。根据经验，LGDC 将局部结构化数据集（Tree）上的自回归模型与全局结构化数据集（Planar、Community-20）上的扩散模型相匹配，验证了混合发电的优势。</li>
</ul>

<h3>Title: TabletopGen: Instance-Level Interactive 3D Tabletop Scene Generation from Text or Single Image</h3>
<ul>
<li><strong>Authors: </strong>Ziqian Wang, Yonghao He, Licheng Yang, Wei Zou, Hongxuan Ma, Liu Liu, Wei Sui, Yuxin Guo, Hu Su</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01204">https://arxiv.org/abs/2512.01204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01204">https://arxiv.org/pdf/2512.01204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01204]] TabletopGen: Instance-Level Interactive 3D Tabletop Scene Generation from Text or Single Image(https://arxiv.org/abs/2512.01204)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating high-fidelity, physically interactive 3D simulated tabletop scenes is essential for embodied AI--especially for robotic manipulation policy learning and data synthesis. However, current text- or image-driven 3D scene generation methods mainly focus on large-scale scenes, struggling to capture the high-density layouts and complex spatial relations that characterize tabletop scenes. To address these challenges, we propose TabletopGen, a training-free, fully automatic framework that generates diverse, instance-level interactive 3D tabletop scenes. TabletopGen accepts a reference image as input, which can be synthesized by a text-to-image model to enhance scene diversity. We then perform instance segmentation and completion on the reference to obtain per-instance images. Each instance is reconstructed into a 3D model followed by canonical coordinate alignment. The aligned 3D models then undergo pose and scale estimation before being assembled into a collision-free, simulation-ready tabletop scene. A key component of our framework is a novel pose and scale alignment approach that decouples the complex spatial reasoning into two stages: a Differentiable Rotation Optimizer for precise rotation recovery and a Top-view Spatial Alignment mechanism for robust translation and scale estimation, enabling accurate 3D reconstruction from 2D reference. Extensive experiments and user studies show that TabletopGen achieves state-of-the-art performance, markedly surpassing existing methods in visual fidelity, layout accuracy, and physical plausibility, capable of generating realistic tabletop scenes with rich stylistic and spatial diversity. Our code will be publicly available.</li>
<li><strong>摘要：</strong>生成高保真、物理交互的 3D 模拟桌面场景对于实体 AI 至关重要，尤其是对于机器人操作策略学习和数据合成而言。然而，当前文本或图像驱动的 3D 场景生成方法主要关注大规模场景，难以捕捉桌面场景的高密度布局和复杂的空间关系。为了应对这些挑战，我们提出了 TabletopGen，这是一种无需训练的全自动框架，可以生成多样化的实例级交互式 3D 桌面场景。 TabletopGen 接受参考图像作为输入，可以通过文本到图像模型合成该参考图像以增强场景多样性。然后，我们对参考执行实例分割和补全以获得每个实例的图像。每个实例都被重建为 3D 模型，然后进行规范坐标对齐。然后，对齐的 3D 模型会进行姿态和比例估计，然后组装成无碰撞、可用于模拟的桌面场景。我们框架的一个关键组成部分是一种新颖的姿态和尺度对齐方法，它将复杂的空间推理分解为两个阶段：用于精确旋转恢复的可微旋转优化器和用于鲁棒平移和尺度估计的顶视图空间对齐机制，从而实现从 2D 参考进行精确的 3D 重建。大量的实验和用户研究表明，TabletopGen 实现了最先进的性能，在视觉保真度、布局准确性和物理合理性方面明显超越了现有方法，能够生成具有丰富风格和空间多样性的逼真桌面场景。我们的代码将公开。</li>
</ul>

<h3>Title: A Comparative Study of Machine Learning Algorithms for Electricity Price Forecasting with LIME-Based Interpretability</h3>
<ul>
<li><strong>Authors: </strong>Xuanyi Zhao, Jiawen Ding, Xueting Huang, Yibo Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01212">https://arxiv.org/abs/2512.01212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01212">https://arxiv.org/pdf/2512.01212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01212]] A Comparative Study of Machine Learning Algorithms for Electricity Price Forecasting with LIME-Based Interpretability(https://arxiv.org/abs/2512.01212)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>With the rapid development of electricity markets, price volatility has significantly increased, making accurate forecasting crucial for power system operations and market decisions. Traditional linear models cannot capture the complex nonlinear characteristics of electricity pricing, necessitating advanced machine learning approaches. This study compares eight machine learning models using Spanish electricity market data, integrating consumption, generation, and meteorological variables. The models evaluated include linear regression, ridge regression, decision tree, KNN, random forest, gradient boosting, SVR, and XGBoost. Results show that KNN achieves the best performance with R^2 of 0.865, MAE of 3.556, and RMSE of 5.240. To enhance interpretability, LIME analysis reveals that meteorological factors and supply-demand indicators significantly influence price fluctuations through nonlinear relationships. This work demonstrates the effectiveness of machine learning models in electricity price forecasting while improving decision transparency through interpretability analysis.</li>
<li><strong>摘要：</strong>随着电力市场的快速发展，电价波动性显着加大，准确预测对电力系统运行和市场决策至关重要。传统的线性模型无法捕捉电价复杂的非线性特征，需要先进的机器学习方法。这项研究使用西班牙电力市场数据比较了八种机器学习模型，整合了消费、发电和气象变量。评估的模型包括线性回归、岭回归、决策树、KNN、随机森林、梯度提升、SVR 和 XGBoost。结果表明，KNN 取得了最佳性能，R^2 为 0.865，MAE 为 3.556，RMSE 为 5.240。为了增强可解释性，LIME 分析表明，气象因素和供需指标通过非线性关系显着影响价格波动。这项工作展示了机器学习模型在电价预测中的有效性，同时通过可解释性分析提高了决策透明度。</li>
</ul>

<h3>Title: Neural Network Optimal Power Flow via Energy Gradient Flow and Unified Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Xuezhi Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01219">https://arxiv.org/abs/2512.01219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01219">https://arxiv.org/pdf/2512.01219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01219]] Neural Network Optimal Power Flow via Energy Gradient Flow and Unified Dynamics(https://arxiv.org/abs/2512.01219)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Optimal Power Flow (OPF) is a core optimization problem in power system operation and planning, aiming to minimize generation costs while satisfying physical constraints such as power flow equations, generator limits, and voltage limits. Traditional OPF solving methods typically employ iterative optimization algorithms (such as interior point methods, sequential quadratic programming, etc.), with limitations including low computational efficiency, initial value sensitivity, and low batch computation efficiency. Most existing deep learning-based OPF methods rely on supervised learning, requiring pre-solving large numbers of cases, and have difficulty guaranteeing physical consistency. This paper proposes an Optimal Power Flow solving method based on neural network dynamics and energy gradient flow, transforming OPF problems into energy minimization problems. By constructing an energy function to measure the degree of deviation from the constraint manifold, and guiding networks to learn optimal solutions that simultaneously satisfy power flow constraints and minimize costs through gradient flow. Neural networks are trained unsupervised by directly minimizing physical residuals, requiring no labeled data, achieving true "end-to-end" physics-constrained learning.</li>
<li><strong>摘要：</strong>最优潮流（OPF）是电力系统运行和规划中的核心优化问题，旨在最小化发电成本，同时满足潮流方程、发电机限制和电压限制等物理约束。传统的OPF求解方法通常采用迭代优化算法（如内点法、顺序二次规划等），其局限性包括计算效率低、初值敏感、批量计算效率低等。现有的基于深度学习的OPF方法大多依赖于监督学习，需要预先解决大量案例，并且难以保证物理一致性。本文提出一种基于神经网络动力学和能量梯度流的最优潮流求解方法，将OPF问题转化为能量最小化问题。通过构造能量函数来衡量偏离约束流形的程度，并引导网络学习同时满足潮流约束并通过梯度流最小化成本的最优解决方案。神经网络通过直接最小化物理残差进行无监督训练，不需要标记数据，实现真正的“端到端”物理约束学习。</li>
</ul>

<h3>Title: PSR: Scaling Multi-Subject Personalized Image Generation with Pairwise Subject-Consistency Rewards</h3>
<ul>
<li><strong>Authors: </strong>Shulei Wang, Longhui Wei, Xin He, Jianbo Ouyang, Hui Lu, Zhou Zhao, Qi Tian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01236">https://arxiv.org/abs/2512.01236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01236">https://arxiv.org/pdf/2512.01236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01236]] PSR: Scaling Multi-Subject Personalized Image Generation with Pairwise Subject-Consistency Rewards(https://arxiv.org/abs/2512.01236)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Personalized generation models for a single subject have demonstrated remarkable effectiveness, highlighting their significant potential. However, when extended to multiple subjects, existing models often exhibit degraded performance, particularly in maintaining subject consistency and adhering to textual prompts. We attribute these limitations to the absence of high-quality multi-subject datasets and refined post-training strategies. To address these challenges, we propose a scalable multi-subject data generation pipeline that leverages powerful single-subject generation models to construct diverse and high-quality multi-subject training data. Through this dataset, we first enable single-subject personalization models to acquire knowledge of synthesizing multi-image and multi-subject scenarios. Furthermore, to enhance both subject consistency and text controllability, we design a set of Pairwise Subject-Consistency Rewards and general-purpose rewards, which are incorporated into a refined reinforcement learning stage. To comprehensively evaluate multi-subject personalization, we introduce a new benchmark that assesses model performance using seven subsets across three dimensions. Extensive experiments demonstrate the effectiveness of our approach in advancing multi-subject personalized image generation. Github Link: this https URL</li>
<li><strong>摘要：</strong>针对单个主题的个性化生成模型已表现出显着的有效性，凸显了其巨大的潜力。然而，当扩展到多个主题时，现有模型通常表现出性能下降，特别是在保持主题一致性和遵守文本提示方面。我们将这些限制归因于缺乏高质量的多主题数据集和完善的培训后策略。为了应对这些挑战，我们提出了一种可扩展的多主题数据生成管道，利用强大的单主题生成模型来构建多样化且高质量的多主题训练数据。通过该数据集，我们首先使单主题个性化模型能够获取合成多图像和多主题场景的知识。此外，为了增强主题一致性和文本可控性，我们设计了一套成对主题一致性奖励和通用奖励，并将其纳入细化的强化学习阶段。为了全面评估多主体个性化，我们引入了一个新的基准，该基准使用三个维度的七个子集来评估模型性能。大量的实验证明了我们的方法在推进多主体个性化图像生成方面的有效性。 Github 链接：此 https URL</li>
</ul>

<h3>Title: Generative Adversarial Gumbel MCTS for Abstract Visual Composition Generation</h3>
<ul>
<li><strong>Authors: </strong>Zirui Zhao, Boye Niu, David Hsu, Wee Sun Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01242">https://arxiv.org/abs/2512.01242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01242">https://arxiv.org/pdf/2512.01242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01242]] Generative Adversarial Gumbel MCTS for Abstract Visual Composition Generation(https://arxiv.org/abs/2512.01242)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We study abstract visual composition, in which identity is primarily determined by the spatial configuration and relations among a small set of geometric primitives (e.g., parts, symmetry, topology). They are invariant primarily to texture and photorealistic detail. Composing such structures from fixed components under geometric constraints and vague goal specification (such as text) is non-trivial due to combinatorial placement choices, limited data, and discrete feasibility (overlap-free, allowable orientations), which create a sparse solution manifold ill-suited to purely statistical pixel-space generators. We propose a constraint-guided framework that combines explicit geometric reasoning with neural semantics. An AlphaGo-style search enforces feasibility, while a fine-tuned vision-language model scores semantic alignment as reward signals. Our algorithm uses a policy network as a heuristic in Monte-Carlo Tree Search and fine-tunes the network via search-generated plans. Inspired by the Generative Adversarial Network, we use the generated instances for adversarial reward refinement. Over time, the generation should approach the actual data more closely when the reward model cannot distinguish between generated instances and ground-truth. In the Tangram Assembly task, our approach yields higher validity and semantic fidelity than diffusion and auto-regressive baselines, especially as constraints tighten.</li>
<li><strong>摘要：</strong>我们研究抽象视觉构图，其中身份主要由一小组几何基元（例如零件、对称性、拓扑）之间的空间配置和关系决定。它们主要在纹理和逼真的细节方面保持不变。由于组合放置选择、有限数据和离散可行性（无重叠、允许的方向），在几何约束和模糊目标规范（例如文本）下由固定组件组成此类结构并非易事，这会创建不适合纯统计像素空间生成器的稀疏解流形。我们提出了一个约束引导的框架，将显式几何推理与神经语义相结合。 AlphaGo 式的搜索增强了可行性，而微调的视觉语言模型将语义对齐作为奖励信号进行评分。我们的算法使用策略网络作为蒙特卡洛树搜索的启发式方法，并通过搜索生成的计划对网络进行微调。受生成对抗网络的启发，我们使用生成的实例进行对抗奖励细化。随着时间的推移，当奖励模型无法区分生成的实例和真实数据时，生成应该更接近实际数据。在七巧板组装任务中，我们的方法比扩散和自回归基线产生更高的有效性和语义保真度，特别是在约束收紧的情况下。</li>
</ul>

<h3>Title: Accelerating Large-Scale Reasoning Model Inference with Sparse Self-Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Yilong Zhao, Jiaming Tang, Kan Zhu, Zihao Ye, Chi-Chih Chang, Chaofan Lin, Jongseok Park, Guangxuan Xiao, Mohamed S. Abdelfattah, Mingyu Gao, Baris Kasikci, Song Han, Ion Stoica</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01278">https://arxiv.org/abs/2512.01278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01278">https://arxiv.org/pdf/2512.01278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01278]] Accelerating Large-Scale Reasoning Model Inference with Sparse Self-Speculative Decoding(https://arxiv.org/abs/2512.01278)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reasoning language models have demonstrated remarkable capabilities on challenging tasks by generating elaborate chain-of-thought (CoT) solutions. However, such lengthy generation shifts the inference bottleneck from compute-bound to memory-bound. To generate each token, the model applies full attention to all previously generated tokens, requiring memory access to an increasingly large KV-Cache. Consequently, longer generations demand more memory access for every step, leading to substantial pressure on memory bandwidth. To address this, we introduce SparseSpec, a speculative decoding framework that reuses the same model as the draft and target models (i.e., self-speculation). SparseSpec features a novel sparse attention mechanism, PillarAttn, as the draft model, which accurately selects critical tokens via elegantly reusing information from the verification stage. Furthermore, SparseSpec co-designs self-speculation with three system innovations: (1) a unified scheduler to batch token drafting and verification, (2) delayed verification for CPU/GPU overlap, and (3) dynamic KV-Cache management to maximize memory utilization. Across various models and datasets, SparseSpec outperforms state-of-the-art solutions, with an up to 2.13x throughput speedup.</li>
<li><strong>摘要：</strong>推理语言模型通过生成复杂的思想链 (CoT) 解决方案，在处理具有挑战性的任务时表现出了卓越的能力。然而，如此冗长的生成将推理瓶颈从计算限制转移到内存限制。为了生成每个令牌，该模型会充分关注所有先前生成的令牌，从而需要对越来越大的 KV 缓存进行内存访问。因此，更长的代数需要每一步更多的内存访问，从而导致内存带宽面临巨大压力。为了解决这个问题，我们引入了 SparseSpec，这是一个推测性解码框架，它重用与草稿模型和目标模型相同的模型（即自我推测）。 SparseSpec 采用新颖的稀疏注意力机制 PillarAttn 作为草案模型，它通过优雅地重用验证阶段的信息来准确选择关键标记。此外，SparseSpec 通过三个系统创新共同设计了自我推测：(1) 统一调度程序，用于批量令牌起草和验证，(2) CPU/GPU 重叠的延迟验证，以及 (3) 动态 KV 缓存管理，以最大限度地提高内存利用率。在各种模型和数据集上，SparseSpec 的性能优于最先进的解决方案，吞吐量加速高达 2.13 倍。</li>
</ul>

<h3>Title: Generative Modeling with Continuous Flows: Sample Complexity of Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Mudit Gaur, Prashant Trivedi, Shuchin Aeron, Amrit Singh Bedi, George K. Atia, Vaneet Aggarwal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01286">https://arxiv.org/abs/2512.01286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01286">https://arxiv.org/pdf/2512.01286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01286]] Generative Modeling with Continuous Flows: Sample Complexity of Flow Matching(https://arxiv.org/abs/2512.01286)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Flow matching has recently emerged as a promising alternative to diffusion-based generative models, offering faster sampling and simpler training by learning continuous flows governed by ordinary differential equations. Despite growing empirical success, the theoretical understanding of flow matching remains limited, particularly in terms of sample complexity results. In this work, we provide the first analysis of the sample complexity for flow-matching based generative models without assuming access to the empirical risk minimizer (ERM) of the loss function for estimating the velocity field. Under standard assumptions on the loss function for velocity field estimation and boundedness of the data distribution, we show that a sufficiently expressive neural network can learn a velocity field such that with $\mathcal{O}(\epsilon^{-4})$ samples, such that the Wasserstein-2 distance between the learned and the true distribution is less than $\mathcal{O}(\epsilon)$. The key technical idea is to decompose the velocity field estimation error into neural-network approximation error, statistical error due to the finite sample size, and optimization error due to the finite number of optimization steps for estimating the velocity field. Each of these terms are then handled via techniques that may be of independent interest.</li>
<li><strong>摘要：</strong>流匹配最近成为基于扩散的生成模型的一种有前景的替代方案，通过学习常微分方程控制的连续流来提供更快的采样和更简单的训练。尽管实证上取得了越来越多的成功，但对流匹配的理论理解仍然有限，特别是在样本复杂性结果方面。在这项工作中，我们首次分析了基于流匹配的生成模型的样本复杂性，而不假设可以使用损失函数的经验风险最小化器（ERM）来估计速度场。在速度场估计损失函数和数据分布有界性的标准假设下，我们表明，具有足够表达能力的神经网络可以通过 $\mathcal{O}(\epsilon^{-4})$ 样本学习速度场，使得学习分布与真实分布之间的 Wasserstein-2 距离小于 $\mathcal{O}(\epsilon)$。其关键技术思想是将速度场估计误差分解为神经网络逼近误差、有限样本量导致的统计误差以及速度场估计优化步骤有限导致的优化误差。然后通过可能具有独立兴趣的技术来处理这些术语中的每一个。</li>
</ul>

<h3>Title: Diffusion Model in Latent Space for Medical Image Segmentation Task</h3>
<ul>
<li><strong>Authors: </strong>Huynh Trinh Ngoc, Toan Nguyen Hai, Ba Luong Son, Long Tran Quoc</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01292">https://arxiv.org/abs/2512.01292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01292">https://arxiv.org/pdf/2512.01292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01292]] Diffusion Model in Latent Space for Medical Image Segmentation Task(https://arxiv.org/abs/2512.01292)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Medical image segmentation is crucial for clinical diagnosis and treatment planning. Traditional methods typically produce a single segmentation mask, failing to capture inherent uncertainty. Recent generative models enable the creation of multiple plausible masks per image, mimicking the collaborative interpretation of several clinicians. However, these approaches remain computationally heavy. We propose MedSegLatDiff, a diffusion based framework that combines a variational autoencoder (VAE) with a latent diffusion model for efficient medical image segmentation. The VAE compresses the input into a low dimensional latent space, reducing noise and accelerating training, while the diffusion process operates directly in this compact representation. We further replace the conventional MSE loss with weighted cross entropy in the VAE mask reconstruction path to better preserve tiny structures such as small nodules. MedSegLatDiff is evaluated on ISIC-2018 (skin lesions), CVC-Clinic (polyps), and LIDC-IDRI (lung nodules). It achieves state of the art or highly competitive Dice and IoU scores while simultaneously generating diverse segmentation hypotheses and confidence maps. This provides enhanced interpretability and reliability compared to deterministic baselines, making the model particularly suitable for clinical deployment.</li>
<li><strong>摘要：</strong>医学图像分割对于临床诊断和治疗计划至关重要。传统方法通常会产生单个分割掩模，无法捕获固有的不确定性。最近的生成模型能够为每个图像创建多个看似合理的掩模，模仿几位临床医生的协作解释。然而，这些方法的计算量仍然很大。我们提出了 MedSegLatDiff，一种基于扩散的框架，它将变分自动编码器（VAE）与潜在扩散模型相结合，以实现高效的医学图像分割。 VAE 将输入压缩到低维潜在空间，减少噪声并加速训练，而扩散过程直接在这种紧凑的表示中运行。我们进一步在 VAE 掩模重建路径中用加权交叉熵代替传统的 MSE 损失，以更好地保留小结节等微小结构。 MedSegLatDiff 根据 ISIC-2018（皮肤病变）、CVC-Clinic（息肉）和 LIDC-IDRI（肺结节）进行评估。它实现了最先进的或高度竞争的 Dice 和 IoU 分数，同时生成不同的分割假设和置信度图。与确定性基线相比，这提供了增强的可解释性和可靠性，使该模型特别适合临床部署。</li>
</ul>

<h3>Title: DCText: Scheduled Attention Masking for Visual Text Generation via Divide-and-Conquer Strategy</h3>
<ul>
<li><strong>Authors: </strong>Jaewoo Song, Jooyoung Choi, Kanghyun Baek, Sangyub Lee, Daemin Park, Sungroh Yoon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01302">https://arxiv.org/abs/2512.01302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01302">https://arxiv.org/pdf/2512.01302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01302]] DCText: Scheduled Attention Masking for Visual Text Generation via Divide-and-Conquer Strategy(https://arxiv.org/abs/2512.01302)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite recent text-to-image models achieving highfidelity text rendering, they still struggle with long or multiple texts due to diluted global attention. We propose DCText, a training-free visual text generation method that adopts a divide-and-conquer strategy, leveraging the reliable short-text generation of Multi-Modal Diffusion Transformers. Our method first decomposes a prompt by extracting and dividing the target text, then assigns each to a designated region. To accurately render each segment within their regions while preserving overall image coherence, we introduce two attention masks - Text-Focus and Context-Expansion - applied sequentially during denoising. Additionally, Localized Noise Initialization further improves text accuracy and region alignment without increasing computational cost. Extensive experiments on single- and multisentence benchmarks show that DCText achieves the best text accuracy without compromising image quality while also delivering the lowest generation latency.</li>
<li><strong>摘要：</strong>尽管最近的文本到图像模型实现了高保真文本渲染，但由于全局注意力的淡化，它们仍然难以处理长文本或多个文本。我们提出了 DCText，这是一种免训练的视觉文本生成方法，采用分而治之的策略，利用多模态扩散变压器可靠的短文本生成功能。我们的方法首先通过提取和划分目标文本来分解提示，然后将每个提示分配到指定的区域。为了准确地渲染其区域内的每个片段，同时保持整体图像的连贯性，我们引入了两个注意掩模 - 文本焦点和上下文扩展 - 在去噪期间顺序应用。此外，局部噪声初始化进一步提高了文本准确性和区域对齐，而无需增加计算成本。对单句和多句基准的大量实验表明，DCText 在不影响图像质量的情况下实现了最佳文本准确性，同时还提供了最低的生成延迟。</li>
</ul>

<h3>Title: TokenPure: Watermark Removal through Tokenized Appearance and Structural Guidance</h3>
<ul>
<li><strong>Authors: </strong>Pei Yang, Yepeng Liu, Kelly Peng, Yuan Gao, Yiren Song</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01314">https://arxiv.org/abs/2512.01314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01314">https://arxiv.org/pdf/2512.01314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01314]] TokenPure: Watermark Removal through Tokenized Appearance and Structural Guidance(https://arxiv.org/abs/2512.01314)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In the digital economy era, digital watermarking serves as a critical basis for ownership proof of massive replicable content, including AI-generated and other virtual assets. Designing robust watermarks capable of withstanding various attacks and processing operations is even more paramount. We introduce TokenPure, a novel Diffusion Transformer-based framework designed for effective and consistent watermark removal. TokenPure solves the trade-off between thorough watermark destruction and content consistency by leveraging token-based conditional reconstruction. It reframes the task as conditional generation, entirely bypassing the initial watermark-carrying noise. We achieve this by decomposing the watermarked image into two complementary token sets: visual tokens for texture and structural tokens for geometry. These tokens jointly condition the diffusion process, enabling the framework to synthesize watermark-free images with fine-grained consistency and structural integrity. Comprehensive experiments show that TokenPure achieves state-of-the-art watermark removal and reconstruction fidelity, substantially outperforming existing baselines in both perceptual quality and consistency.</li>
<li><strong>摘要：</strong>在数字经济时代，数字水印是海量可复制内容（包括人工智能生成的和其他虚拟资产）所有权证明的重要基础。设计能够抵御各种攻击和处理操作的强大水印更为重要。我们推出 TokenPure，这是一种基于 Diffusion Transformer 的新型框架，旨在有效且一致地去除水印。 TokenPure 通过利用基于令牌的条件重建来解决彻底水印破坏和内容一致性之间的权衡。它将任务重新构建为条件生成，完全绕过了初始的带有水印的噪声。我们通过将带水印的图像分解为两个互补的标记集来实现这一点：纹理的视觉标记和几何的结构标记。这些令牌共同调节扩散过程，使框架能够合成具有细粒度一致性和结构完整性的无水印图像。综合实验表明，TokenPure 实现了最先进的水印去除和重建保真度，在感知质量和一致性方面大大优于现有基线。</li>
</ul>

<h3>Title: AlignVid: Training-Free Attention Scaling for Semantic Fidelity in Text-Guided Image-to-Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Yexin Liu, Wen-Jie Shu, Zile Huang, Haoze Zheng, Yueze Wang, Manyuan Zhang, Ser-Nam Lim, Harry Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01334">https://arxiv.org/abs/2512.01334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01334">https://arxiv.org/pdf/2512.01334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01334]] AlignVid: Training-Free Attention Scaling for Semantic Fidelity in Text-Guided Image-to-Video Generation(https://arxiv.org/abs/2512.01334)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-guided image-to-video (TI2V) generation has recently achieved remarkable progress, particularly in maintaining subject consistency and temporal coherence. However, existing methods still struggle to adhere to fine-grained prompt semantics, especially when prompts entail substantial transformations of the input image (e.g., object addition, deletion, or modification), a shortcoming we term semantic negligence. In a pilot study, we find that applying a Gaussian blur to the input image improves semantic adherence. Analyzing attention maps, we observe clearer foreground-background separation. From an energy perspective, this corresponds to a lower-entropy cross-attention distribution. Motivated by this, we introduce AlignVid, a training-free framework with two components: (i) Attention Scaling Modulation (ASM), which directly reweights attention via lightweight Q or K scaling, and (ii) Guidance Scheduling (GS), which applies ASM selectively across transformer blocks and denoising steps to reduce visual quality degradation. This minimal intervention improves prompt adherence while limiting aesthetic degradation. In addition, we introduce OmitI2V to evaluate semantic negligence in TI2V generation, comprising 367 human-annotated samples that span addition, deletion, and modification scenarios. Extensive experiments demonstrate that AlignVid can enhance semantic fidelity.</li>
<li><strong>摘要：</strong>文本引导的图像到视频（TI2V）生成最近取得了显着的进展，特别是在保持主题一致性和时间连贯性方面。然而，现有的方法仍然难以遵守细粒度的提示语义，特别是当提示需要对输入图像进行大量转换（例如，对象添加、删除或修改）时，我们将这种缺点称为语义疏忽。在一项试点研究中，我们发现对输入图像应用高斯模糊可以提高语义依从性。分析注意力图，我们观察到更清晰的前景-背景分离。从能量的角度来看，这对应于较低熵的交叉注意力分布。受此启发，我们引入了 AlignVid，这是一个免训练框架，由两个组件组成：（i）注意力缩放调制（ASM），它通过轻量级 Q 或 K 缩放直接重新加权注意力；（ii）指导调度（GS），它有选择地将 ASM 应用于变压器块和去噪步骤，以减少视觉质量下降。这种最小的干预提高了及时的依从性，同时限制了美观的退化。此外，我们引入了 OmitI2V 来评估 TI2V 生成中的语义疏忽，包括 367 个人工注释的样本，涵盖添加、删除和修改场景。大量实验表明 AlignVid 可以增强语义保真度。</li>
</ul>

<h3>Title: EvalTalker: Learning to Evaluate Real-Portrait-Driven Multi-Subject Talking Humans</h3>
<ul>
<li><strong>Authors: </strong>Yingjie Zhou, Xilei Zhu, Siyu Ren, Ziyi Zhao, Ziwen Wang, Farong Wen, Yu Zhou, Jiezhang Cao, Xiongkuo Min, Fengjiao Chen, Xiaoyu Li, Xuezhi Cao, Guangtao Zhai, Xiaohong Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01340">https://arxiv.org/abs/2512.01340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01340">https://arxiv.org/pdf/2512.01340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01340]] EvalTalker: Learning to Evaluate Real-Portrait-Driven Multi-Subject Talking Humans(https://arxiv.org/abs/2512.01340)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, quality assessment</a></li>
<li><strong>Abstract: </strong>Speech-driven Talking Human (TH) generation, commonly known as "Talker," currently faces limitations in multi-subject driving capabilities. Extending this paradigm to "Multi-Talker," capable of animating multiple subjects simultaneously, introduces richer interactivity and stronger immersion in audiovisual communication. However, current Multi-Talkers still exhibit noticeable quality degradation caused by technical limitations, resulting in suboptimal user experiences. To address this challenge, we construct THQA-MT, the first large-scale Multi-Talker-generated Talking Human Quality Assessment dataset, consisting of 5,492 Multi-Talker-generated THs (MTHs) from 15 representative Multi-Talkers using 400 real portraits collected online. Through subjective experiments, we analyze perceptual discrepancies among different Multi-Talkers and identify 12 common types of distortion. Furthermore, we introduce EvalTalker, a novel TH quality assessment framework. This framework possesses the ability to perceive global quality, human characteristics, and identity consistency, while integrating Qwen-Sync to perceive multimodal synchrony. Experimental results demonstrate that EvalTalker achieves superior correlation with subjective scores, providing a robust foundation for future research on high-quality Multi-Talker generation and evaluation.</li>
<li><strong>摘要：</strong>语音驱动的会说话的人类（TH）一代，俗称“Talker”，目前面临多主体驱动能力的限制。将此范式扩展到“多说话者”，能够同时为多个主题设置动画，从而在视听通信中引入更丰富的交互性和更强的沉浸感。然而，由于技术限制，当前的多方通话器仍然表现出明显的质量下降，导致用户体验不佳。为了应对这一挑战，我们构建了 THQA-MT，这是第一个大规模多说话者生成的说话人类质量评估数据集，由 15 个代表性多说话者使用在线收集的 400 个真实肖像生成的 5,492 个多说话者生成的 TH（MTH）组成。通过主观实验，我们分析了不同多说话者之间的感知差异，并识别了 12 种常见的失真类型。此外，我们还介绍了 EvalTalker，一种新颖的 TH 质量评估框架。该框架具备感知全局质量、人类特征和身份一致性的能力，同时集成Qwen-Sync感知多模态同步。实验结果表明，EvalTalker 与主观评分实现了良好的相关性，为未来高质量 Multi-Talker 生成和评估的研究奠定了坚实的基础。</li>
</ul>

<h3>Title: Handwritten Text Recognition for Low Resource Languages</h3>
<ul>
<li><strong>Authors: </strong>Sayantan Dey, Alireza Alaei, Partha Pratim Roy</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01348">https://arxiv.org/abs/2512.01348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01348">https://arxiv.org/pdf/2512.01348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01348]] Handwritten Text Recognition for Low Resource Languages(https://arxiv.org/abs/2512.01348)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Despite considerable progress in handwritten text recognition, paragraph-level handwritten text recognition, especially in low-resource languages, such as Hindi, Urdu and similar scripts, remains a challenging problem. These languages, often lacking comprehensive linguistic resources, require special attention to develop robust systems for accurate optical character recognition (OCR). This paper introduces BharatOCR, a novel segmentation-free paragraph-level handwritten Hindi and Urdu text recognition. We propose a ViT-Transformer Decoder-LM architecture for handwritten text recognition, where a Vision Transformer (ViT) extracts visual features, a Transformer decoder generates text sequences, and a pre-trained language model (LM) refines the output to improve accuracy, fluency, and coherence. Our model utilizes a Data-efficient Image Transformer (DeiT) model proposed for masked image modeling in this research work. In addition, we adopt a RoBERTa architecture optimized for masked language modeling (MLM) to enhance the linguistic comprehension and generative capabilities of the proposed model. The transformer decoder generates text sequences from visual embeddings. This model is designed to iteratively process a paragraph image line by line, called implicit line segmentation. The proposed model was evaluated using our custom dataset ('Parimal Urdu') and ('Parimal Hindi'), introduced in this research work, as well as two public datasets. The proposed model achieved benchmark results in the NUST-UHWR, PUCIT-OUHL, and Parimal-Urdu datasets, achieving character recognition rates of 96.24%, 92.05%, and 94.80%, respectively. The model also provided benchmark results using the Hindi dataset achieving a character recognition rate of 80.64%. The results obtained from our proposed model indicated that it outperformed several state-of-the-art Urdu text recognition methods.</li>
<li><strong>摘要：</strong>尽管手写文本识别取得了相当大的进展，但段落级手写文本识别，特别是在印地语、乌尔都语和类似文字等资源匮乏的语言中，仍然是一个具有挑战性的问题。这些语言通常缺乏全面的语言资源，需要特别注意开发强大的系统来实现准确的光学字符识别 (OCR)。本文介绍了 BharatOCR，一种新颖的无分段段落级手写印地语和乌尔都语文本识别。我们提出了一种用于手写文本识别的 ViT-Transformer Decoder-LM 架构，其中视觉变换器 (ViT) 提取视觉特征，变换器解码器生成文本序列，预训练语言模型 (LM) 细化输出以提高准确性、流畅性和连贯性。我们的模型利用了本研究工作中为蒙版图像建模而提出的数据高效图像转换器（DeiT）模型。此外，我们采用针对掩码语言建模（MLM）优化的 RoBERTa 架构，以增强所提出模型的语言理解和生成能力。转换器解码器从视觉嵌入生成文本序列。该模型旨在逐行迭代处理段落图像，称为隐式行分割。使用本研究工作中引入的自定义数据集（“Parimal Urdu”）和（“Parimal Hindi”）以及两个公共数据集对所提出的模型进行了评估。所提出的模型在 NUST-UHWR、PUCIT-OUHL 和 Parimal-Urdu 数据集上取得了基准结果，字符识别率分别达到 96.24%、92.05% 和 94.80%。该模型还使用印地语数据集提供了基准结果，实现了 80.64% 的字符识别率。从我们提出的模型获得的结果表明它优于几种最先进的乌尔都语文本识别方法。</li>
</ul>

<h3>Title: SRAM: Shape-Realism Alignment Metric for No Reference 3D Shape Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Sheng Liu, Tianyu Luan, Phani Nuney, Xuelu Feng, Junsong Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01373">https://arxiv.org/abs/2512.01373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01373">https://arxiv.org/pdf/2512.01373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01373]] SRAM: Shape-Realism Alignment Metric for No Reference 3D Shape Evaluation(https://arxiv.org/abs/2512.01373)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>3D generation and reconstruction techniques have been widely used in computer games, film, and other content creation areas. As the application grows, there is a growing demand for 3D shapes that look truly realistic. Traditional evaluation methods rely on a ground truth to measure mesh fidelity. However, in many practical cases, a shape's realism does not depend on having a ground truth reference. In this work, we propose a Shape-Realism Alignment Metric that leverages a large language model (LLM) as a bridge between mesh shape information and realism evaluation. To achieve this, we adopt a mesh encoding approach that converts 3D shapes into the language token space. A dedicated realism decoder is designed to align the language model's output with human perception of realism. Additionally, we introduce a new dataset, RealismGrading, which provides human-annotated realism scores without the need for ground truth shapes. Our dataset includes shapes generated by 16 different algorithms on over a dozen objects, making it more representative of practical 3D shape distributions. We validate our metric's performance and generalizability through k-fold cross-validation across different objects. Experimental results show that our metric correlates well with human perceptions and outperforms existing methods, and has good generalizability.</li>
<li><strong>摘要：</strong>3D生成和重建技术已广泛应用于计算机游戏、电影和其他内容创作领域。随着应用程序的发展，对看起来真正逼真的 3D 形状的需求也不断增长。传统的评估方法依靠基本事实来测量网格保真度。然而，在许多实际情况下，形状的真实感并不取决于是否有地面实况参考。在这项工作中，我们提出了一种形状真实性对齐度量，该度量利用大型语言模型（LLM）作为网格形状信息和真实性评估之间的桥梁。为了实现这一目标，我们采用网格编码方法，将 3D 形状转换为语言标记空间。专用的现实主义解码器旨在使语言模型的输出与人类对现实主义的感知保持一致。此外，我们引入了一个新的数据集 RealismGrading，它提供人工注释的真实度分数，而不需要地面真实形状。我们的数据集包括由 16 种不同算法在十多个对象上生成的形状，使其更能代表实际的 3D 形状分布。我们通过跨不同对象的 k 倍交叉验证来验证指标的性能和通用性。实验结果表明，我们的指标与人类感知有很好的相关性，并且优于现有方法，并且具有良好的通用性。</li>
</ul>

<h3>Title: Textured Geometry Evaluation: Perceptual 3D Textured Shape Metric via 3D Latent-Geometry Network</h3>
<ul>
<li><strong>Authors: </strong>Tianyu Luan, Xuelu Feng, Zixin Zhu, Phani Nuney, Sheng Liu, Xuan Gong, David Doermann, Chunming Qiao, Junsong Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01380">https://arxiv.org/abs/2512.01380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01380">https://arxiv.org/pdf/2512.01380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01380]] Textured Geometry Evaluation: Perceptual 3D Textured Shape Metric via 3D Latent-Geometry Network(https://arxiv.org/abs/2512.01380)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Textured high-fidelity 3D models are crucial for games, AR/VR, and film, but human-aligned evaluation methods still fall behind despite recent advances in 3D reconstruction and generation. Existing metrics, such as Chamfer Distance, often fail to align with how humans evaluate the fidelity of 3D shapes. Recent learning-based metrics attempt to improve this by relying on rendered images and 2D image quality metrics. However, these approaches face limitations due to incomplete structural coverage and sensitivity to viewpoint choices. Moreover, most methods are trained on synthetic distortions, which differ significantly from real-world distortions, resulting in a domain gap. To address these challenges, we propose a new fidelity evaluation method that is based directly on 3D meshes with texture, without relying on rendering. Our method, named Textured Geometry Evaluation TGE, jointly uses the geometry and color information to calculate the fidelity of the input textured mesh with comparison to a reference colored shape. To train and evaluate our metric, we design a human-annotated dataset with real-world distortions. Experiments show that TGE outperforms rendering-based and geometry-only methods on real-world distortion dataset.</li>
<li><strong>摘要：</strong>带纹理的高保真 3D 模型对于游戏、AR/VR 和电影至关重要，但尽管 3D 重建和生成方面最近取得了进展，但以人为本的评估方法仍然落后。现有指标（例如倒角距离）通常无法与人类评估 3D 形状保真度的方式保持一致。最近基于学习的指标试图通过依赖渲染图像和 2D 图像质量指标来改进这一点。然而，由于结构覆盖不完整和对视点选择的敏感性，这些方法面临局限性。此外，大多数方法都是针对合成失真进行训练的，这与现实世界的失真有很大不同，从而导致了域差距。为了解决这些挑战，我们提出了一种新的保真度评估方法，该方法直接基于具有纹理的 3D 网格，而不依赖于渲染。我们的方法名为纹理几何评估 TGE，联合使用几何和颜色信息来计算输入纹理网格的保真度，并与参考彩色形状进行比较。为了训练和评估我们的指标，我们设计了一个带有真实世界扭曲的人工注释数据集。实验表明，TGE 在现实世界的畸变数据集上优于基于渲染和纯几何的方法。</li>
</ul>

<h3>Title: FRAMER: Frequency-Aligned Self-Distillation with Adaptive Modulation Leveraging Diffusion Priors for Real-World Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Seungho Choi, Jeahun Sung, Jihyong Oh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01390">https://arxiv.org/abs/2512.01390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01390">https://arxiv.org/pdf/2512.01390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01390]] FRAMER: Frequency-Aligned Self-Distillation with Adaptive Modulation Leveraging Diffusion Priors for Real-World Image Super-Resolution(https://arxiv.org/abs/2512.01390)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Real-image super-resolution (Real-ISR) seeks to recover HR images from LR inputs with mixed, unknown degradations. While diffusion models surpass GANs in perceptual quality, they under-reconstruct high-frequency (HF) details due to a low-frequency (LF) bias and a depth-wise "low-first, high-later" hierarchy. We introduce FRAMER, a plug-and-play training scheme that exploits diffusion priors without changing the backbone or inference. At each denoising step, the final-layer feature map teaches all intermediate layers. Teacher and student feature maps are decomposed into LF/HF bands via FFT masks to align supervision with the model's internal frequency hierarchy. For LF, an Intra Contrastive Loss (IntraCL) stabilizes globally shared structure. For HF, an Inter Contrastive Loss (InterCL) sharpens instance-specific details using random-layer and in-batch negatives. Two adaptive modulators, Frequency-based Adaptive Weight (FAW) and Frequency-based Alignment Modulation (FAM), reweight per-layer LF/HF signals and gate distillation by current similarity. Across U-Net and DiT backbones (e.g., Stable Diffusion 2, 3), FRAMER consistently improves PSNR/SSIM and perceptual metrics (LPIPS, NIQE, MANIQA, MUSIQ). Ablations validate the final-layer teacher and random-layer negatives.</li>
<li><strong>摘要：</strong>真实图像超分辨率 (Real-ISR) 旨在从具有混合、未知退化的 LR 输入中恢复 HR 图像。虽然扩散模型在感知质量上超越了 GAN，但由于低频 (LF) 偏差和深度方面的“先低后高”层次结构，它们未能充分重建高频 (HF) 细节。我们引入了 FRAMER，这是一种即插即用的训练方案，它利用扩散先验而不改变主干或推理。在每个去噪步骤中，最终层特征图都会教导所有中间层。教师和学生特征图通过 FFT 掩模分解为 LF/HF 频段，以使监督与模型的内部频率层次结构保持一致。对于 LF，帧内对比损失 (IntraCL) 稳定了全局共享结构。对于 HF，间对比损失 (InterCL) 使用随机层和批内负片来锐化特定于实例的细节。两个自适应调制器，基于频率的自适应权重 (FAW) 和基于频率的对齐调制 (FAM)，根据当前相似性重新加权每层 LF/HF 信号和门蒸馏。在 U-Net 和 DiT 主干网（例如，Stable Diffusion 2、3）中，FRAMER 持续改进 PSNR/SSIM 和感知指标（LPIPS、NIQE、MANIQA、MUSIQ）。消融验证了最后一层教师和随机层底片。</li>
</ul>

<h3>Title: RE-LLM: Integrating Large Language Models into Renewable Energy Systems</h3>
<ul>
<li><strong>Authors: </strong>Ali Forootani, Mohammad Sadr, Danial Esmaeili Aliabadi, Daniela Thraen</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01392">https://arxiv.org/abs/2512.01392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01392">https://arxiv.org/pdf/2512.01392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01392]] RE-LLM: Integrating Large Language Models into Renewable Energy Systems(https://arxiv.org/abs/2512.01392)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Energy system models are increasingly employed to guide long-term planning in multi-sectoral environments where decisions span electricity, heat, transport, land use, and industry. While these models provide rigorous quantitative insights, their outputs are often highly technical, making them difficult to interpret for non-expert stakeholders such as policymakers, planners, and the public. This communication gap limits the accessibility and practical impact of scenario-based modeling, particularly as energy transitions grow more complex with rising shares of renewables, sectoral integration, and deep uncertainties. To address this challenge, we propose the Renewable Energy Large Language Model (RE-LLM), a hybrid framework that integrates Large Language Models (LLMs) directly into the energy system modeling workflow. RE-LLM combines three core elements: (i) optimization-based scenario exploration, (ii) machine learning surrogates that accelerate computationally intensive simulations, and (iii) LLM-powered natural language generation that translates complex results into clear, stakeholder-oriented explanations. This integrated design not only reduces computational burden but also enhances inter-pretability, enabling real-time reasoning about trade-offs, sensitivities, and policy implications. The framework is adaptable across different optimization platforms and energy system models, ensuring broad applicability beyond the case study presented. By merging speed, rigor, and interpretability, RE-LLM advances a new paradigm of human-centric energy modeling. It enables interactive, multilingual, and accessible engagement with future energy pathways, ultimately bridging the final gap between data-driven analysis and actionable decision-making for sustainable transitions.</li>
<li><strong>摘要：</strong>能源系统模型越来越多地用于指导多部门环境中的长期规划，其中决策涉及电力、热力、交通、土地利用和工业。虽然这些模型提供了严格的定量见解，但它们的输出通常技术性很强，使得决策者、规划者和公众等非专家利益相关者难以解释它们。这种沟通差距限制了基于场景的建模的可及性和实际影响，特别是随着可再生能源份额的不断增加、部门一体化和深度不确定性，能源转型变得更加复杂。为了应对这一挑战，我们提出了可再生能源大语言模型（RE-LLM），这是一种将大语言模型（LLM）直接集成到能源系统建模工作流程中的混合框架。 RE-LLM 结合了三个核心要素：(i) 基于优化的场景探索，(ii) 加速计算密集型模拟的机器学习代理，以及 (iii) 由 LLM 驱动的自然语言生成，将复杂的结果转化为清晰的、面向利益相关者的解释。这种集成设计不仅减少了计算负担，还增强了可解释性，从而能够对权衡、敏感性和政策影响进行实时推理。该框架可适应不同的优化平台和能源系统模型，确保除所介绍的案例研究之外的广泛适用性。通过融合速度、严谨性和可解释性，RE-LLM 提出了一种以人为本的能源建模的新范式。它实现了与未来能源路径的互动、多语言和可访问的参与，最终弥合了数据驱动的分析和可持续转型的可行决策之间的最终差距。</li>
</ul>

<h3>Title: On Global Applicability and Location Transferability of Generative Deep Learning Models for Precipitation Downscaling</h3>
<ul>
<li><strong>Authors: </strong>Paula Harder, Christian Lessig, Matthew Chantry, Francis Pelletier, David Rolnick</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01400">https://arxiv.org/abs/2512.01400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01400">https://arxiv.org/pdf/2512.01400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01400]] On Global Applicability and Location Transferability of Generative Deep Learning Models for Precipitation Downscaling(https://arxiv.org/abs/2512.01400)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep learning offers promising capabilities for the statistical downscaling of climate and weather forecasts, with generative approaches showing particular success in capturing fine-scale precipitation patterns. However, most existing models are region-specific, and their ability to generalize to unseen geographic areas remains largely unexplored. In this study, we evaluate the generalization performance of generative downscaling models across diverse regions. Using a global framework, we employ ERA5 reanalysis data as predictors and IMERG precipitation estimates at $0.1^\circ$ resolution as targets. A hierarchical location-based data split enables a systematic assessment of model performance across 15 regions around the world.</li>
<li><strong>摘要：</strong>深度学习为气候和天气预报的统计降尺度提供了有前景的能力，生成方法在捕捉精细尺度降水模式方面显示出特别成功。然而，大多数现有模型都是针对特定区域的，并且它们推广到未知地理区域的能力在很大程度上仍未得到探索。在这项研究中，我们评估了不同地区的生成降尺度模型的泛化性能。使用全球框架，我们采用 ERA5 再分析数据作为预测变量，并以 $0.1^\circ$ 分辨率的 IMERG 降水估计作为目标。基于位置的分层数据分割可以对全球 15 个地区的模型性能进行系统评估。</li>
</ul>

<h3>Title: ResDiT: Evoking the Intrinsic Resolution Scalability in Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Yiyang Ma, Feng Zhou, Xuedan Yin, Pu Cao, Yonghao Dang, Jianqin Yin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01426">https://arxiv.org/abs/2512.01426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01426">https://arxiv.org/pdf/2512.01426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01426]] ResDiT: Evoking the Intrinsic Resolution Scalability in Diffusion Transformers(https://arxiv.org/abs/2512.01426)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Leveraging pre-trained Diffusion Transformers (DiTs) for high-resolution (HR) image synthesis often leads to spatial layout collapse and degraded texture fidelity. Prior work mitigates these issues with complex pipelines that first perform a base-resolution (i.e., training-resolution) denoising process to guide HR generation. We instead explore the intrinsic generative mechanisms of DiTs and propose ResDiT, a training-free method that scales resolution efficiently. We identify the core factor governing spatial layout, position embeddings (PEs), and show that the original PEs encode incorrect positional information when extrapolated to HR, which triggers layout collapse. To address this, we introduce a PE scaling technique that rectifies positional encoding under resolution changes. To further remedy low-fidelity details, we develop a local-enhancement mechanism grounded in base-resolution local attention. We design a patch-level fusion module that aggregates global and local cues, together with a Gaussian-weighted splicing strategy that eliminates grid artifacts. Comprehensive evaluations demonstrate that ResDiT consistently delivers high-fidelity, high-resolution image synthesis and integrates seamlessly with downstream tasks, including spatially controlled generation.</li>
<li><strong>摘要：</strong>利用预先训练的扩散变压器 (DiT) 进行高分辨率 (HR) 图像合成通常会导致空间布局崩溃和纹理保真度下降。之前的工作通过复杂的管道缓解了这些问题，这些管道首先执行基本分辨率（即训练分辨率）去噪过程来指导 HR 生成。相反，我们探索了 DiT 的内在生成机制，并提出了 ResDiT，这是一种无需训练即可有效扩展分辨率的方法。我们确定了控制空间布局的核心因素——位置嵌入（PE），并表明原始 PE 在外推到 HR 时编码了错误的位置信息，从而触发了布局崩溃。为了解决这个问题，我们引入了一种 PE 缩放技术，可以在分辨率变化时纠正位置编码。为了进一步纠正低保真细节，我们开发了一种基于基本分辨率局部注意力的局部增强机制。我们设计了一个聚合全局和局部线索的补丁级融合模块，以及消除网格伪影的高斯加权拼接策略。综合评估表明，ResDiT 始终如一地提供高保真、高分辨率图像合成，并与下游任务（包括空间控制生成）无缝集成。</li>
</ul>

<h3>Title: FastAnimate: Towards Learnable Template Construction and Pose Deformation for Fast 3D Human Avatar Animation</h3>
<ul>
<li><strong>Authors: </strong>Jian Shu, Nanjie Yao, Gangjian Zhang, Junlong Ren, Yu Feng, Hao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01444">https://arxiv.org/abs/2512.01444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01444">https://arxiv.org/pdf/2512.01444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01444]] FastAnimate: Towards Learnable Template Construction and Pose Deformation for Fast 3D Human Avatar Animation(https://arxiv.org/abs/2512.01444)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>3D human avatar animation aims at transforming a human avatar from an arbitrary initial pose to a specified target pose using deformation algorithms. Existing approaches typically divide this task into two stages: canonical template construction and target pose deformation. However, current template construction methods demand extensive skeletal rigging and often produce artifacts for specific poses. Moreover, target pose deformation suffers from structural distortions caused by Linear Blend Skinning (LBS), which significantly undermines animation realism. To address these problems, we propose a unified learning-based framework to address both challenges in two phases. For the former phase, to overcome the inefficiencies and artifacts during template construction, we leverage a U-Net architecture that decouples texture and pose information in a feed-forward process, enabling fast generation of a human template. For the latter phase, we propose a data-driven refinement technique that enhances structural integrity. Extensive experiments show that our model delivers consistent performance across diverse poses with an optimal balance between efficiency and quality,surpassing state-of-the-art (SOTA) methods.</li>
<li><strong>摘要：</strong>3D 人体头像动画旨在使用变形算法将人体头像从任意初始姿势转换为指定的目标姿势。现有方法通常将此任务分为两个阶段：规范模板构建和目标姿态变形。然而，当前的模板构建方法需要大量的骨骼装备，并且经常会产生特定姿势的工件。此外，目标姿势变形会受到线性混合蒙皮 (LBS) 引起的结构扭曲的影响，这会显着破坏动画的真实感。为了解决这些问题，我们提出了一个基于学习的统一框架来分两个阶段应对这两个挑战。对于前一阶段，为了克服模板构建过程中的低效率和伪影，我们利用 U-Net 架构，在前馈过程中解耦纹理和姿势信息，从而能够快速生成人体模板。对于后一阶段，我们提出了一种数据驱动的细化技术，可以增强结构完整性。大量实验表明，我们的模型在不同姿势下都能提供一致的性能，并在效率和质量之间实现最佳平衡，超越了最先进的 (SOTA) 方法。</li>
</ul>

<h3>Title: ZIP-RC: Zero-overhead Inference-time Prediction of Reward and Cost for Adaptive and Interpretable Generation</h3>
<ul>
<li><strong>Authors: </strong>Rohin Manvi, Joey Hong, Tim Seyde, Maxime Labonne, Mathias Lechner, Sergey Levine</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01457">https://arxiv.org/abs/2512.01457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01457">https://arxiv.org/pdf/2512.01457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01457]] ZIP-RC: Zero-overhead Inference-time Prediction of Reward and Cost for Adaptive and Interpretable Generation(https://arxiv.org/abs/2512.01457)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models excel at reasoning but lack key aspects of introspection, including anticipating their own success and the computation required to achieve it. Humans use real-time introspection to decide how much effort to invest, when to make multiple attempts, when to stop, and when to signal success or failure. Without this, LLMs struggle to make intelligent meta-cognition decisions. Test-time scaling methods like Best-of-N drive up cost and latency by using a fixed budget of samples regardless of the marginal benefit of each one at any point in generation, and the absence of confidence signals can mislead people, prevent appropriate escalation to better tools, and undermine trustworthiness. Learned verifiers or reward models can provide confidence estimates, but do not enable adaptive inference and add substantial cost by requiring extra models or forward passes. We present ZIP-RC, an adaptive inference method that equips models with zero-overhead inference-time predictions of reward and cost. At every token, ZIP-RC reuses reserved or unused logits in the same forward pass as next-token prediction to output a joint distribution over final reward and remaining length -- no extra models, architecture change, or inference overhead. This full joint distribution is used to compute a sampling utility which is the linear combination of the expected maximum reward, total compute, and latency of set of samples if generated to completion. During inference, we maximize this utility with meta-actions that determine which prefix of tokens to continue or initiate sampling from. On mixed-difficulty mathematical benchmarks, ZIP-RC improves accuracy by up to 12% over majority voting at equal or lower average cost, and traces smooth Pareto frontiers between quality, compute, and latency. By providing real-time reward-cost introspection, ZIP-RC enables adaptive, efficient reasoning.</li>
<li><strong>摘要：</strong>大型语言模型擅长推理，但缺乏内省的关键方面，包括预测自己的成功以及实现成功所需的计算。人类利用实时内省来决定投入多少努力、何时进行多次尝试、何时停止以及何时发出成功或失败的信号。如果没有这一点，法学硕士就很难做出明智的元认知决策。 Best-of-N 等测试时间缩放方法通过使用固定的样本预算来推高成本和延迟，而不管每个样本在生成过程中的任何时刻的边际收益如何，并且缺乏置信信号可能会误导人们，阻止适当升级到更好的工具，并破坏可信度。学习验证器或奖励模型可以提供置信度估计，但无法实现自适应推理，并且需要额外的模型或前向传递，从而增加大量成本。我们提出了 ZIP-RC，一种自适应推理方法，为模型配备了奖励和成本的零开销推理时间预测。对于每个令牌，ZIP-RC 在同一前向传递中重用保留或未使用的逻辑作为下一个令牌预测，以输出最终奖励和剩余长度的联合分布 - 没有额外的模型、架构更改或推理开销。这种完全联合分布用于计算采样效用，它是预期最大奖励、总计算量和样本集（如果生成完成）的延迟的线性组合。在推理过程中，我们通过元操作来最大化此效用，这些元操作确定要继续或启动采样的令牌前缀。在混合难度的数学基准上，ZIP-RC 在相同或更低的平均成本下比多数投票的准确性提高了 12%，并在质量、计算和延迟之间追踪平滑的帕累托边界。通过提供实时奖励-成本自省，ZIP-RC 实现了自适应、高效的推理。</li>
</ul>

<h3>Title: ChronosObserver: Taming 4D World with Hyperspace Diffusion Sampling</h3>
<ul>
<li><strong>Authors: </strong>Qisen Wang, Yifan Zhao, Peisen Shen, Jialu Li, Jia Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01481">https://arxiv.org/abs/2512.01481</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01481">https://arxiv.org/pdf/2512.01481</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01481]] ChronosObserver: Taming 4D World with Hyperspace Diffusion Sampling(https://arxiv.org/abs/2512.01481)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Although prevailing camera-controlled video generation models can produce cinematic results, lifting them directly to the generation of 3D-consistent and high-fidelity time-synchronized multi-view videos remains challenging, which is a pivotal capability for taming 4D worlds. Some works resort to data augmentation or test-time optimization, but these strategies are constrained by limited model generalization and scalability issues. To this end, we propose ChronosObserver, a training-free method including World State Hyperspace to represent the spatiotemporal constraints of a 4D world scene, and Hyperspace Guided Sampling to synchronize the diffusion sampling trajectories of multiple views using the hyperspace. Experimental results demonstrate that our method achieves high-fidelity and 3D-consistent time-synchronized multi-view videos generation without training or fine-tuning for diffusion models.</li>
<li><strong>摘要：</strong>尽管流行的摄像机控制视频生成模型可以产生电影效果，但将其直接提升为生成 3D 一致且高保真时间同步的多视图视频仍然具有挑战性，而这是驯服 4D 世界的关键能力。一些工作诉诸数据增强或测试时优化，但这些策略受到有限的模型泛化和可扩展性问题的限制。为此，我们提出了 ChronosObserver，一种免训练方法，包括表示 4D 世界场景的时空约束的世界状态超空间，以及使用超空间同步多个视图的扩散采样轨迹的超空间引导采样。实验结果表明，我们的方法无需对扩散模型进行训练或微调即可实现高保真且 3D 一致的时间同步多视图视频生成。</li>
</ul>

<h3>Title: Reconstructing Multi-Scale Physical Fields from Extremely Sparse Measurements with an Autoencoder-Diffusion Cascade</h3>
<ul>
<li><strong>Authors: </strong>Letian Yi, Tingpeng Zhang, Mingyuan Zhou, Guannan Wang, Quanke Su, Zhilu Lai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.app-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01572">https://arxiv.org/abs/2512.01572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01572">https://arxiv.org/pdf/2512.01572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01572]] Reconstructing Multi-Scale Physical Fields from Extremely Sparse Measurements with an Autoencoder-Diffusion Cascade(https://arxiv.org/abs/2512.01572)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reconstructing full fields from extremely sparse and random measurements is a longstanding ill-posed inverse problem. A powerful framework for addressing such challenges is hierarchical probabilistic modeling, where uncertainty is represented by intermediate variables and resolved through marginalization during inference. Inspired by this principle, we propose Cascaded Sensing (Cas-Sensing), a hierarchical reconstruction framework that integrates an autoencoder-diffusion cascade. First, a neural operator-based functional autoencoder reconstructs the dominant structures of the original field - including large-scale components and geometric boundaries - from arbitrary sparse inputs, serving as an intermediate variable. Then, a conditional diffusion model, trained with a mask-cascade strategy, generates fine-scale details conditioned on these large-scale structures. To further enhance fidelity, measurement consistency is enforced via the manifold constrained gradient based on Bayesian posterior sampling during the generation process. This cascaded pipeline substantially alleviates ill-posedness, delivering accurate and robust reconstructions. Experiments on both simulation and real-world datasets demonstrate that Cas-Sensing generalizes well across varying sensor configurations and geometric boundaries, making it a promising tool for practical deployment in scientific and engineering applications.</li>
<li><strong>摘要：</strong>从极其稀疏和随机的测量中重建全场是一个长期存在的不适定逆问题。解决此类挑战的一个强​​大框架是分层概率建模，其中不确定性由中间变量表示，并通过推理过程中的边缘化来解决。受这一原理的启发，我们提出了级联传感（Cas-Sensing），这是一种集成了自动编码器扩散级联的分层重建框架。首先，基于神经算子的函数自动编码器从任意稀疏输入重建原始场的主要结构（包括大规模组件和几何边界），充当中间变量。然后，使用掩模级联策略训练的条件扩散模型可生成以这些大规模结构为条件的精细尺度细节。为了进一步提高保真度，在生成过程中通过基于贝叶斯后验采样的流形约束梯度来强制测量一致性。这种级联管道大大减轻了不适定性，提供了准确而稳健的重建。对模拟和现实世界数据集的实验表明，Cas-Sensing 在不同的传感器配置和几何边界上具有良好的泛化能力，使其成为科学和工程应用中实际部署的有前途的工具。</li>
</ul>

<h3>Title: RoleMotion: A Large-Scale Dataset towards Robust Scene-Specific Role-Playing Motion Synthesis with Fine-grained Descriptions</h3>
<ul>
<li><strong>Authors: </strong>Junran Peng, Yiheng Huang, Silei Shen, Zeji Wei, Jingwei Yang, Baojie Wang, Yonghao He, Chuanchen Luo, Man Zhang, Xucheng Yin, Wei Sui</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01582">https://arxiv.org/abs/2512.01582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01582">https://arxiv.org/pdf/2512.01582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01582]] RoleMotion: A Large-Scale Dataset towards Robust Scene-Specific Role-Playing Motion Synthesis with Fine-grained Descriptions(https://arxiv.org/abs/2512.01582)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce RoleMotion, a large-scale human motion dataset that encompasses a wealth of role-playing and functional motion data tailored to fit various specific scenes. Existing text datasets are mainly constructed decentrally as amalgamation of assorted subsets that their data are nonfunctional and isolated to work together to cover social activities in various scenes. Also, the quality of motion data is inconsistent, and textual annotation lacks fine-grained details in these datasets. In contrast, RoleMotion is meticulously designed and collected with a particular focus on scenes and roles. The dataset features 25 classic scenes, 110 functional roles, over 500 behaviors, and 10296 high-quality human motion sequences of body and hands, annotated with 27831 fine-grained text descriptions. We build an evaluator stronger than existing counterparts, prove its reliability, and evaluate various text-to-motion methods on our dataset. Finally, we explore the interplay of motion generation of body and hands. Experimental results demonstrate the high-quality and functionality of our dataset on text-driven whole-body generation.</li>
<li><strong>摘要：</strong>在本文中，我们介绍了 RoleMotion，这是一个大规模的人体运动数据集，其中包含大量为适应各种特定场景而定制的角色扮演和功能运动数据。现有的文本数据集主要是分散构建的各种子集的合并，它们的数据是非功能性的并且是孤立的，可以一起工作以涵盖各种场景中的社会活动。此外，运动数据的质量不一致，并且这些数据集中的文本注释缺乏细粒度的细节。相比之下，RoleMotion 则经过精心设计和收集，特别注重场景和角色。该数据集包含25个经典场景、110个功能角色、500多种行为以及10296个高质量的人体和手部动作序列，并标注有27831条细粒度的文本描述。我们构建了一个比现有同行更强大的评估器，证明其可靠性，并在我们的数据集上评估各种文本到运动方法。最后，我们探索身体和手运动生成的相互作用。实验结果证明了我们的数据集在文本驱动的全身生成方面的高质量和功能性。</li>
</ul>

<h3>Title: SPARK: Sim-ready Part-level Articulated Reconstruction with VLM Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Yumeng He, Ying Jiang, Jiayin Lu, Yin Yang, Chenfanfu Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01629">https://arxiv.org/abs/2512.01629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01629">https://arxiv.org/pdf/2512.01629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01629]] SPARK: Sim-ready Part-level Articulated Reconstruction with VLM Knowledge(https://arxiv.org/abs/2512.01629)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Articulated 3D objects are critical for embodied AI, robotics, and interactive scene understanding, yet creating simulation-ready assets remains labor-intensive and requires expert modeling of part hierarchies and motion structures. We introduce SPARK, a framework for reconstructing physically consistent, kinematic part-level articulated objects from a single RGB image. Given an input image, we first leverage VLMs to extract coarse URDF parameters and generate part-level reference images. We then integrate the part-image guidance and the inferred structure graph into a generative diffusion transformer to synthesize consistent part and complete shapes of articulated objects. To further refine the URDF parameters, we incorporate differentiable forward kinematics and differentiable rendering to optimize joint types, axes, and origins under VLM-generated open-state supervision. Extensive experiments show that SPARK produces high-quality, simulation-ready articulated assets across diverse categories, enabling downstream applications such as robotic manipulation and interaction modeling.</li>
<li><strong>摘要：</strong>铰接的 3D 对象对于具体人工智能、机器人技术和交互式场景理解至关重要，但创建模拟就绪资产仍然是劳动密集型的，并且需要对零件层次结构和运动结构进行专家建模。我们引入了 SPARK，一个用于从单个 RGB 图像重建物理一致、运动学部分级铰接对象的框架。给定输入图像，我们首先利用 VLM 提取粗略的 URDF 参数并生成零件级参考图像。然后，我们将部分图像引导和推断的结构图集成到生成扩散转换器中，以合成铰接对象的一致部分和完整形状。为了进一步细化 URDF 参数，我们结合了可微正向运动学和可微渲染，以在 VLM 生成的开放状态监督下优化关节类型、轴和原点。大量实验表明，SPARK 可以生产跨不同类别的高质量、可用于仿真的铰接式资产，从而支持机器人操作和交互建模等下游应用。</li>
</ul>

<h3>Title: Generative Editing in the Joint Vision-Language Space for Zero-Shot Composed Image Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Xin Wang, Haipeng Zhang, Mang Li, Zhaohui Xia, Yueguo Chen, Yu Zhang, Chunyu Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01636">https://arxiv.org/abs/2512.01636</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01636">https://arxiv.org/pdf/2512.01636</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01636]] Generative Editing in the Joint Vision-Language Space for Zero-Shot Composed Image Retrieval(https://arxiv.org/abs/2512.01636)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Composed Image Retrieval (CIR) enables fine-grained visual search by combining a reference image with a textual modification. While supervised CIR methods achieve high accuracy, their reliance on costly triplet annotations motivates zero-shot solutions. The core challenge in zero-shot CIR (ZS-CIR) stems from a fundamental dilemma: existing text-centric or diffusion-based approaches struggle to effectively bridge the vision-language modality gap. To address this, we propose Fusion-Diff, a novel generative editing framework with high effectiveness and data efficiency designed for multimodal alignment. First, it introduces a multimodal fusion feature editing strategy within a joint vision-language (VL) space, substantially narrowing the modality gap. Second, to maximize data efficiency, the framework incorporates a lightweight Control-Adapter, enabling state-of-the-art performance through fine-tuning on only a limited-scale synthetic dataset of 200K samples. Extensive experiments on standard CIR benchmarks (CIRR, FashionIQ, and CIRCO) demonstrate that Fusion-Diff significantly outperforms prior zero-shot approaches. We further enhance the interpretability of our model by visualizing the fused multimodal representations.</li>
<li><strong>摘要：</strong>组合图像检索 (CIR) 通过将参考图像与文本修改相结合来实现细粒度的视觉搜索。虽然有监督的 CIR 方法实现了高精度，但它们对昂贵的三元组注释的依赖激发了零样本解决方案。零样本 CIR (ZS-CIR) 的核心挑战源于一个基本困境：现有的以文本为中心或基于扩散的方法难以有效地弥合视觉-语言模态差距。为了解决这个问题，我们提出了 Fusion-Diff，这是一种新颖的生成编辑框架，具有高效率和数据效率，专为多模式对齐而设计。首先，它在联合视觉语言（VL）空间内引入了多模态融合特征编辑策略，大大缩小了模态差距。其次，为了最大限度地提高数据效率，该框架结合了一个轻量级的控制适配器，通过仅对 20 万个样本的有限规模合成数据集进行微调，实现最先进的性能。对标准 CIR 基准（CIRR、FashionIQ 和 CIRCO）的大量实验表明，Fusion-Diff 的性能显着优于之前的零样本方法。我们通过可视化融合的多模态表示进一步增强了模型的可解释性。</li>
</ul>

<h3>Title: ViT$^3$: Unlocking Test-Time Training in Vision</h3>
<ul>
<li><strong>Authors: </strong>Dongchen Han, Yining Li, Tianyu Li, Zixuan Cao, Ziming Wang, Jun Song, Yu Cheng, Bo Zheng, Gao Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01643">https://arxiv.org/abs/2512.01643</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01643">https://arxiv.org/pdf/2512.01643</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01643]] ViT$^3$: Unlocking Test-Time Training in Vision(https://arxiv.org/abs/2512.01643)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Test-Time Training (TTT) has recently emerged as a promising direction for efficient sequence modeling. TTT reformulates attention operation as an online learning problem, constructing a compact inner model from key-value pairs at test time. This reformulation opens a rich and flexible design space while achieving linear computational complexity. However, crafting a powerful visual TTT design remains challenging: fundamental choices for the inner module and inner training lack comprehensive understanding and practical guidelines. To bridge this critical gap, in this paper, we present a systematic empirical study of TTT designs for visual sequence modeling. From a series of experiments and analyses, we distill six practical insights that establish design principles for effective visual TTT and illuminate paths for future improvement. These findings culminate in the Vision Test-Time Training (ViT$^3$) model, a pure TTT architecture that achieves linear complexity and parallelizable computation. We evaluate ViT$^3$ across diverse visual tasks, including image classification, image generation, object detection, and semantic segmentation. Results show that ViT$^3$ consistently matches or outperforms advanced linear-complexity models (e.g., Mamba and linear attention variants) and effectively narrows the gap to highly optimized vision Transformers. We hope this study and the ViT$^3$ baseline can facilitate future work on visual TTT models. Code is available at this https URL.</li>
<li><strong>摘要：</strong>测试时训练（TTT）最近成为高效序列建模的一个有前途的方向。 TTT 将注意力操作重新表述为在线学习问题，在测试时从键值对构建紧凑的内部模型。这种重新表述开辟了丰富而灵活的设计空间，同时实现了线性计算复杂性。然而，打造强大的视觉 TTT 设计仍然具有挑战性：内部模块和内部训练的基本选择缺乏全面的理解和实用指南。为了弥补这一关键差距，在本文中，我们对视觉序列建模的 TTT 设计进行了系统的实证研究。从一系列实验和分析中，我们提炼出了六种实用见解，为有效的视觉 TTT 建立了设计原则，并阐明了未来改进的路径。这些发现最终形成了视觉测试时训练 (ViT$^3$) 模型，这是一种实现线性复杂性和可并行计算的纯 TTT 架构。我们在不同的视觉任务中评估 ViT$^3$，包括图像分类、图像生成、对象检测和语义分割。结果表明，ViT$^3$ 始终匹配或优于先进的线性复杂性模型（例如 Mamba 和线性注意力变体），并有效缩小了与高度优化的视觉 Transformer 的差距。我们希望这项研究和 ViT$^3$ 基线能够促进视觉 TTT 模型的未来工作。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: HalluGraph: Auditable Hallucination Detection for Legal RAG Systems via Knowledge Graph Alignment</h3>
<ul>
<li><strong>Authors: </strong>Valentin Noël, Elimane Yassine Seidou, Charly Ken Capo-Chichi, Ghanem Amari</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01659">https://arxiv.org/abs/2512.01659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01659">https://arxiv.org/pdf/2512.01659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01659]] HalluGraph: Auditable Hallucination Detection for Legal RAG Systems via Knowledge Graph Alignment(https://arxiv.org/abs/2512.01659)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Legal AI systems powered by retrieval-augmented generation (RAG) face a critical accountability challenge: when an AI assistant cites case law, statutes, or contractual clauses, practitioners need verifiable guarantees that generated text faithfully represents source documents. Existing hallucination detectors rely on semantic similarity metrics that tolerate entity substitutions, a dangerous failure mode when confusing parties, dates, or legal provisions can have material consequences. We introduce HalluGraph, a graph-theoretic framework that quantifies hallucinations through structural alignment between knowledge graphs extracted from context, query, and response. Our approach produces bounded, interpretable metrics decomposed into \textit{Entity Grounding} (EG), measuring whether entities in the response appear in source documents, and \textit{Relation Preservation} (RP), verifying that asserted relationships are supported by context. On structured control documents, HalluGraph achieves near-perfect discrimination ($>$400 words, $>$20 entities), HalluGraph achieves $AUC = 0.979$, while maintaining robust performance ($AUC \approx 0.89$) on challenging generative legal task, consistently outperforming semantic similarity baselines. The framework provides the transparency and traceability required for high-stakes legal applications, enabling full audit trails from generated assertions back to source passages.</li>
<li><strong>摘要：</strong>由检索增强生成（RAG）支持的法律人工智能系统面临着严峻的问责挑战：当人工智能助手引用判例法、法规或合同条款时，从业者需要可验证的保证，以确保生成的文本忠实地代表源文档。现有的幻觉检测器依赖于容忍实体替换的语义相似性度量，当混淆各方、日期或法律条款时，这是一种危险的失败模式，可能会产生重大后果。我们引入 HalluGraph，这是一个图论框架，它通过从上下文、查询和响应中提取的知识图之间的结构对齐来量化幻觉。我们的方法产生有界的、可解释的度量，分解为 \textit{Entity Grounding} (EG)，测量响应中的实体是否出现在源文档中，和 \textit{Relation Preservation} (RP)，验证上下文支持断言的关系。在结构化控制文档上，HalluGraph 实现了近乎完美的区分（$>$400 个单词，$>$20 个实体），HalluGraph 实现了 $AUC = 0.979$，同时在具有挑战性的生成法律任务上保持稳健的性能（$AUC \约 0.89$），始终优于语义相似性基线。该框架提供了高风险法律应用程序所需的透明度和可追溯性，从而实现从生成的断言到源段落的完整审计跟踪。</li>
</ul>

<h3>Title: GRASP: Guided Residual Adapters with Sample-wise Partitioning</h3>
<ul>
<li><strong>Authors: </strong>Felix Nützel, Mischa Dombrowski, Bernhard Kainz</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01675">https://arxiv.org/abs/2512.01675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01675">https://arxiv.org/pdf/2512.01675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01675]] GRASP: Guided Residual Adapters with Sample-wise Partitioning(https://arxiv.org/abs/2512.01675)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in text-to-image diffusion models enable high-fidelity generation across diverse prompts. However, these models falter in long-tail settings, such as medical imaging, where rare pathologies comprise a small fraction of data. This results in mode collapse: tail-class outputs lack quality and diversity, undermining the goal of synthetic data augmentation for underrepresented conditions. We pinpoint gradient conflicts between frequent head and rare tail classes as the primary culprit, a factor unaddressed by existing sampling or conditioning methods that mainly steer inference without altering the learned distribution. To resolve this, we propose GRASP: Guided Residual Adapters with Sample-wise Partitioning. GRASP uses external priors to statically partition samples into clusters that minimize intra-group gradient clashes. It then fine-tunes pre-trained models by injecting cluster-specific residual adapters into transformer feedforward layers, bypassing learned gating for stability and efficiency. On the long-tail MIMIC-CXR-LT dataset, GRASP yields superior FID and diversity metrics, especially for rare classes, outperforming baselines like vanilla fine-tuning and Mixture of Experts variants. Downstream classification on NIH-CXR-LT improves considerably for tail labels. Generalization to ImageNet-LT confirms broad applicability. Our method is lightweight, scalable, and readily integrates with diffusion pipelines.</li>
<li><strong>摘要：</strong>文本到图像扩散模型的最新进展使得能够在不同的提示下进行高保真生成。然而，这些模型在长尾环境中表现不佳，例如医学成像，其中罕见的病理只占数据的一小部分。这会导致模式崩溃：尾级输出缺乏质量和多样性，破坏了针对代表性不足的条件进行合成数据增强的目标。我们将频繁的头类和罕见的尾类之间的梯度冲突确定为罪魁祸首，这是现有采样或条件方法未解决的因素，这些方法主要引导推理而不改变学习的分布。为了解决这个问题，我们提出了 GRASP：带有样本分区的引导残差适配器。 GRASP 使用外部先验将样本静态划分为簇，从而最大限度地减少组内梯度冲突。然后，它通过将特定于集群的残差适配器注入变压器前馈层来微调预训练模型，绕过学习门控以提高稳定性和效率。在长尾 MIMIC-CXR-LT 数据集上，GRASP 产生卓越的 FID 和多样性指标，特别是对于稀有类别，优于普通微调和专家混合变体等基线。 NIH-CXR-LT 上的下游分类显着改善了尾部标签。对 ImageNet-LT 的推广证实了其广泛的适用性。我们的方法是轻量级的、可扩展的，并且很容易与扩散管道集成。</li>
</ul>

<h3>Title: Open-world Hand-Object Interaction Video Generation Based on Structure and Contact-aware Representation</h3>
<ul>
<li><strong>Authors: </strong>Haodong Yan, Hang Yu, Zhide Zhong, Weilin Yuan, Xin Gong, Zehang Luo, Chengxi Heyu, Junfeng Li, Wenxuan Song, Shunbo Zhou, Haoang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01677">https://arxiv.org/abs/2512.01677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01677">https://arxiv.org/pdf/2512.01677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01677]] Open-world Hand-Object Interaction Video Generation Based on Structure and Contact-aware Representation(https://arxiv.org/abs/2512.01677)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generating realistic hand-object interactions (HOI) videos is a significant challenge due to the difficulty of modeling physical constraints (e.g., contact and occlusion between hands and manipulated objects). Current methods utilize HOI representation as an auxiliary generative objective to guide video synthesis. However, there is a dilemma between 2D and 3D representations that cannot simultaneously guarantee scalability and interaction fidelity. To address this limitation, we propose a structure and contact-aware representation that captures hand-object contact, hand-object occlusion, and holistic structure context without 3D annotations. This interaction-oriented and scalable supervision signal enables the model to learn fine-grained interaction physics and generalize to open-world scenarios. To fully exploit the proposed representation, we introduce a joint-generation paradigm with a share-and-specialization strategy that generates interaction-oriented representations and videos. Extensive experiments demonstrate that our method outperforms state-of-the-art methods on two real-world datasets in generating physics-realistic and temporally coherent HOI videos. Furthermore, our approach exhibits strong generalization to challenging open-world scenarios, highlighting the benefit of our scalable design. Our project page is this https URL.</li>
<li><strong>摘要：</strong>由于对物理约束（例如手与被操纵对象之间的接触和遮挡）进行建模的困难，生成逼真的手与对象交互（HOI）视频是一项重大挑战。当前的方法利用 HOI 表示作为辅助生成目标来指导视频合成。然而，2D 和 3D 表示之间存在一个困境，即无法同时保证可扩展性和交互保真度。为了解决这个限制，我们提出了一种结构和接触感知表示，可以捕获手部物体接触、手部物体遮挡和整体结构上下文，而无需 3D 注释。这种面向交互且可扩展的监督信号使模型能够学习细粒度的交互物理原理并推广到开放世界场景。为了充分利用所提出的表示，我们引入了一种具有共享和专业化策略的联合生成范例，可生成面向交互的表示和视频。大量实验表明，我们的方法在生成物理真实且时间相干的 HOI 视频方面优于两个现实世界数据集上最先进的方法。此外，我们的方法对具有挑战性的开放世界场景表现出很强的通用性，突出了我们可扩展设计的优势。我们的项目页面就是这个https URL。</li>
</ul>

<h3>Title: DreamingComics: A Story Visualization Pipeline via Subject and Layout Customized Generation using Video Models</h3>
<ul>
<li><strong>Authors: </strong>Patrick Kwon, Chen Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01686">https://arxiv.org/abs/2512.01686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01686">https://arxiv.org/pdf/2512.01686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01686]] DreamingComics: A Story Visualization Pipeline via Subject and Layout Customized Generation using Video Models(https://arxiv.org/abs/2512.01686)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Current story visualization methods tend to position subjects solely by text and face challenges in maintaining artistic consistency. To address these limitations, we introduce DreamingComics, a layout-aware story visualization framework. We build upon a pretrained video diffusion-transformer (DiT) model, leveraging its spatiotemporal priors to enhance identity and style consistency. For layout-based position control, we propose RegionalRoPE, a region-aware positional encoding scheme that re-indexes embeddings based on the target layout. Additionally, we introduce a masked condition loss to further constrain each subject's visual features to their designated region. To infer layouts from natural language scripts, we integrate an LLM-based layout generator trained to produce comic-style layouts, enabling flexible and controllable layout conditioning. We present a comprehensive evaluation of our approach, showing a 29.2% increase in character consistency and a 36.2% increase in style similarity compared to previous methods, while displaying high spatial accuracy. Our project page is available at this https URL</li>
<li><strong>摘要：</strong>当前的故事可视化方法倾向于仅通过文本来定位主题，并在保持艺术一致性方面面临挑战。为了解决这些限制，我们引入了 DreamingComics，一个布局感知的故事可视化框架。我们建立在预训练的视频扩散变换器（DiT）模型的基础上，利用其时空先验来增强身份和风格的一致性。对于基于布局的位置控制，我们提出了 RegionalRoPE，这是一种区域感知位置编码方案，可根据目标布局重新索引嵌入。此外，我们引入了掩蔽条件损失，以进一步将每个受试者的视觉特征限制在其指定区域。为了从自然语言脚本推断布局，我们集成了一个基于 LLM 的布局生成器，经过训练可以生成漫画风格的布局，从而实现灵活且可控的布局调节。我们对我们的方法进行了综合评估，结果显示与之前的方法相比，字符一致性提高了 29.2%，风格相似性提高了 36.2%，同时显示出较高的空间精度。我们的项目页面可通过此 https URL 访问</li>
</ul>

<h3>Title: StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos</h3>
<ul>
<li><strong>Authors: </strong>Daeun Lee, Subhojyoti Mukherjee, Branislav Kveton, Ryan A. Rossi, Viet Dac Lai, Seunghyun Yoon, Trung Bui, Franck Dernoncourt, Mohit Bansal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01707">https://arxiv.org/abs/2512.01707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01707">https://arxiv.org/pdf/2512.01707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01707]] StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos(https://arxiv.org/abs/2512.01707)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Streaming video understanding requires models not only to process temporally incoming frames, but also to anticipate user intention for realistic applications like AR glasses. While prior streaming benchmarks evaluate temporal reasoning, none measure whether MLLMs can interpret or leverage human gaze signals within a streaming setting. To fill this gap, we introduce StreamGaze, the first benchmark designed to evaluate how effectively MLLMs use gaze for temporal and proactive reasoning in streaming videos. StreamGaze introduces gaze-guided past, present, and proactive tasks that comprehensively evaluate streaming video understanding. These tasks assess whether models can use real-time gaze to follow shifting attention and infer user intentions from only past and currently observed frames. To build StreamGaze, we develop a gaze-video QA generation pipeline that aligns egocentric videos with raw gaze trajectories via fixation extraction, region-specific visual prompting, and scanpath construction. This pipeline produces spatio-temporally grounded QA pairs that closely reflect human perceptual dynamics. Across all StreamGaze tasks, we observe substantial performance gaps between state-of-the-art MLLMs and human performance, revealing fundamental limitations in gaze-based temporal reasoning, intention modeling, and proactive prediction. We further provide detailed analyses of gaze-prompting strategies, reasoning behaviors, and task-specific failure modes, offering deeper insight into why current MLLMs struggle and what capabilities future models must develop. All data and code will be publicly released to support continued research in gaze-guided streaming video understanding.</li>
<li><strong>摘要：</strong>流媒体视频理解不仅需要模型处理临时传入的帧，还需要预测用户对 AR 眼镜等实际应用的意图。虽然之前的流媒体基准评估了时间推理，但没有一个衡量 MLLM 是否可以在流媒体设置中解释或利用人类注视信号。为了填补这一空白，我们引入了 StreamGaze，这是第一个基准测试，旨在评估 MLLM 在流视频中如何有效地使用凝视进行时间和主动推理。 StreamGaze 引入了凝视引导的过去、现在和主动任务，可全面评估流视频理解。这些任务评估模型是否可以使用实时凝视来跟踪注意力转移并仅从过去和当前观察到的帧推断用户意图。为了构建 StreamGaze，我们开发了一个注视视频 QA 生成管道，通过注视点提取、特定区域的视觉提示和扫描路径构建，将自我中心视频与原始注视轨迹对齐。该管道产生时空接地的 QA 对，密切反映人类感知动态。在所有 StreamGaze 任务中，我们观察到最先进的 MLLM 与人类表现之间存在巨大的性能差距，揭示了基于凝视的时间推理、意图建模和主动预测的基本局限性。我们进一步提供了对凝视提示策略、推理行为和特定任务失败模式的详细分析，从而更深入地了解当前 MLLM 为何陷入困境以及未来模型必须发展哪些能力。所有数据和代码都将公开发布，以支持凝视引导流视频理解的持续研究。</li>
</ul>

<h3>Title: Automating modeling in mechanics: LLMs as designers of physics-constrained neural networks for constitutive modeling of materials</h3>
<ul>
<li><strong>Authors: </strong>Marius Tacke, Matthias Busch, Kian Abdolazizi, Jonas Eichinger, Kevin Linka, Christian Cyron, Roland Aydin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01735">https://arxiv.org/abs/2512.01735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01735">https://arxiv.org/pdf/2512.01735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01735]] Automating modeling in mechanics: LLMs as designers of physics-constrained neural networks for constitutive modeling of materials(https://arxiv.org/abs/2512.01735)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language model (LLM)-based agentic frameworks increasingly adopt the paradigm of dynamically generating task-specific agents. We suggest that not only agents but also specialized software modules for scientific and engineering tasks can be generated on demand. We demonstrate this concept in the field of solid mechanics. There, so-called constitutive models are required to describe the relationship between mechanical stress and body deformation. Constitutive models are essential for both the scientific understanding and industrial application of materials. However, even recent data-driven methods of constitutive modeling, such as constitutive artificial neural networks (CANNs), still require substantial expert knowledge and human labor. We present a framework in which an LLM generates a CANN on demand, tailored to a given material class and dataset provided by the user. The framework covers LLM-based architecture selection, integration of physical constraints, and complete code generation. Evaluation on three benchmark problems demonstrates that LLM-generated CANNs achieve accuracy comparable to or greater than manually engineered counterparts, while also exhibiting reliable generalization to unseen loading scenarios and extrapolation to large deformations. These findings indicate that LLM-based generation of physics-constrained neural networks can substantially reduce the expertise required for constitutive modeling and represent a step toward practical end-to-end automation.</li>
<li><strong>摘要：</strong>基于大型语言模型（LLM）的代理框架越来越多地采用动态生成特定任务代理的范例。我们建议不仅可以按需生成代理，还可以生成用于科学和工程任务的专用软件模块。我们在固体力学领域证明了这个概念。在那里，需要所谓的本构模型来描述机械应力和身体变形之间的关系。本构模型对于材料的科学理解和工业应用至关重要。然而，即使是最近的数据驱动的本构建模方法，例如本构人工神经网络（CANN），仍然需要大量的专业知识和人力。我们提出了一个框架，其中法学硕士可以根据需要生成 CANN，并根据用户提供的给定材料类别和数据集进行定制。该框架涵盖了基于LLM的架构选择、物理约束的集成以及完整的代码生成。对三个基准问题的评估表明，LLM 生成的 CANN 的精度可与手动设计的对应问题相当或更高，同时还表现出对未见过的加载场景的可靠泛化以及对大变形的外推。这些发现表明，基于 LLM 的物理约束神经网络的生成可以大大减少本构建模所需的专业知识，并代表着迈向实用的端到端自动化的一步。</li>
</ul>

<h3>Title: Mofasa: A Step Change in Metal-Organic Framework Generation</h3>
<ul>
<li><strong>Authors: </strong>Vaidotas Simkus, Anders Christensen, Steven Bennett, Ian Johnson, Mark Neumann, James Gin, Jonathan Godwin, Benjamin Rhodes</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01756">https://arxiv.org/abs/2512.01756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01756">https://arxiv.org/pdf/2512.01756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01756]] Mofasa: A Step Change in Metal-Organic Framework Generation(https://arxiv.org/abs/2512.01756)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Mofasa is an all-atom latent diffusion model with state-of-the-art performance for generating Metal-Organic Frameworks (MOFs). These are highly porous crystalline materials used to harvest water from desert air, capture carbon dioxide, store toxic gases and catalyse chemical reactions. In recognition of their value, the development of MOFs recently received a Nobel Prize in Chemistry. In many ways, MOFs are well-suited for exploiting generative models in chemistry: they are rationally-designable materials with a large combinatorial design space and strong structure-property couplings. And yet, to date, a high performance generative model has been lacking. To fill this gap, we introduce Mofasa, a general-purpose latent diffusion model that jointly samples positions, atom-types and lattice vectors for systems as large as 500 atoms. Mofasa avoids handcrafted assembly algorithms common in the literature, unlocking the simultaneous discovery of metal nodes, linkers and topologies. To help the scientific community build on our work, we release MofasaDB, an annotated library of hundreds of thousands of sampled MOF structures, along with a user-friendly web interface for search and discovery: this https URL .</li>
<li><strong>摘要：</strong>Mofasa 是一种全原子潜在扩散模型，具有用于生成金属有机框架 (MOF) 的最先进性能。这些是高度多孔的结晶材料，用于从沙漠空气中收集水、捕获二氧化碳、储存有毒气体和催化化学反应。为了表彰其价值，MOF 的开发最近获得了诺贝尔化学奖。在许多方面，MOF 非常适合开发化学中的生成模型：它们是可合理设计的材料，具有较大的组合设计空间和强大的结构-性能耦合。然而，迄今为止，仍然缺乏高性能的生成模型。为了填补这一空白，我们引入了 Mofasa，这是一种通用潜在扩散模型，可以对多达 500 个原子的系统的位置、原子类型和晶格向量进行联合采样。 Mofasa 避免了文献中常见的手工组装算法，解锁了金属节点、链接器和拓扑的同时发现。为了帮助科学界以我们的工作为基础，我们发布了 MofasaDB，这是一个包含数十万个 MOF 结构样本的带注释的库，以及一个用于搜索和发现的用户友好的 Web 界面：此 https URL 。</li>
</ul>

<h3>Title: Weight Space Representation Learning with Neural Fields</h3>
<ul>
<li><strong>Authors: </strong>Zhuoqian Yang, Mathieu Salzmann, Sabine Süsstrunk</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01759">https://arxiv.org/abs/2512.01759</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01759">https://arxiv.org/pdf/2512.01759</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01759]] Weight Space Representation Learning with Neural Fields(https://arxiv.org/abs/2512.01759)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In this work, we investigate the potential of weights to serve as effective representations, focusing on neural fields. Our key insight is that constraining the optimization space through a pre-trained base model and low-rank adaptation (LoRA) can induce structure in weight space. Across reconstruction, generation, and analysis tasks on 2D and 3D data, we find that multiplicative LoRA weights achieve high representation quality while exhibiting distinctiveness and semantic structure. When used with latent diffusion models, multiplicative LoRA weights enable higher-quality generation than existing weight-space methods.</li>
<li><strong>摘要：</strong>在这项工作中，我们研究了权重作为有效表示的潜力，重点关注神经领域。我们的主要见解是，通过预训练的基础模型和低秩适应（LoRA）来约束优化空间可以在权重空间中引入结构。在 2D 和 3D 数据的重建、生成和分析任务中，我们发现乘法 LoRA 权重实现了高表示质量，同时表现出独特性和语义结构。当与潜在扩散模型一起使用时，乘法 LoRA 权重可实现比现有权重空间方法更高质量的生成。</li>
</ul>

<h3>Title: Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos</h3>
<ul>
<li><strong>Authors: </strong>Xavier Thomas, Youngsun Lim, Ananya Srinivasan, Audrey Zheng, Deepti Ghadiyaram</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01803">https://arxiv.org/abs/2512.01803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01803">https://arxiv.org/pdf/2512.01803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01803]] Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos(https://arxiv.org/abs/2512.01803)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Despite rapid advances in video generative models, robust metrics for evaluating visual and temporal correctness of complex human actions remain elusive. Critically, existing pure-vision encoders and Multimodal Large Language Models (MLLMs) are strongly appearance-biased, lack temporal understanding, and thus struggle to discern intricate motion dynamics and anatomical implausibilities in generated videos. We tackle this gap by introducing a novel evaluation metric derived from a learned latent space of real-world human actions. Our method first captures the nuances, constraints, and temporal smoothness of real-world motion by fusing appearance-agnostic human skeletal geometry features with appearance-based features. We posit that this combined feature space provides a robust representation of action plausibility. Given a generated video, our metric quantifies its action quality by measuring the distance between its underlying representations and this learned real-world action distribution. For rigorous validation, we develop a new multi-faceted benchmark specifically designed to probe temporally challenging aspects of human action fidelity. Through extensive experiments, we show that our metric achieves substantial improvement of more than 68% compared to existing state-of-the-art methods on our benchmark, performs competitively on established external benchmarks, and has a stronger correlation with human perception. Our in-depth analysis reveals critical limitations in current video generative models and establishes a new standard for advanced research in video generation.</li>
<li><strong>摘要：</strong>尽管视频生成模型取得了快速进步，但用于评估复杂人类行为的视觉和时间正确性的可靠指标仍然难以捉摸。至关重要的是，现有的纯视觉编码器和多模态大语言模型（MLLM）具有强烈的外观偏差，缺乏时间理解，因此很难辨别生成视频中复杂的运动动力学和解剖学上的不可信之处。我们通过引入一种新颖的评估指标来解决这一差距，该指标源自现实世界人类行为的学习潜在空间。我们的方法首先通过将与外观无关的人体骨骼几何特征与基于外观的特征融合来捕获现实世界运动的细微差别、约束和时间平滑度。我们假设这个组合的特征空间提供了动作合理性的稳健表示。给定生成的视频，我们的指标通过测量其底层表示与学习到的真实世界动作分布之间的距离来量化其动作质量。为了进行严格的验证，我们开发了一个新的多方面基准，专门用于探索人类行为保真度的暂时挑战性方面。通过大量的实验，我们表明，与基准上现有的最先进方法相比，我们的指标实现了 68% 以上的实质性改进，在既定的外部基准上表现出竞争力，并且与人类感知具有更强的相关性。我们的深入分析揭示了当前视频生成模型的关键局限性，并为视频生成的高级研究建立了新标准。</li>
</ul>

<h3>Title: Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights</h3>
<ul>
<li><strong>Authors: </strong>Juanxi Tian, Siyuan Li, Conghui He, Lijun Wu, Cheng Tan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01816">https://arxiv.org/abs/2512.01816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01816">https://arxiv.org/pdf/2512.01816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01816]] Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights(https://arxiv.org/abs/2512.01816)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.</li>
<li><strong>摘要：</strong>当前的多模态模型旨在通过统一理解和生成来超越单模态表示的局限性，通常使用文本到图像（T2I）任务来校准语义一致性。然而，它们在训练和评估中对静态单图像生成的依赖导致了对静态模式匹配和语义融合的过度拟合，同时从根本上阻碍了它们对随时间展开的动态过程进行建模的能力。为了解决这些限制，我们提出了 Envision——用于链式文本到多图像生成的因果事件进展基准。它以世界知识为基础，以时空因果关系为结构，重新组织了现有的评估维度，并包括跨越六个科学和​​人文领域的 1,000 个四阶段提示。为了将评估从单个图像过渡到连续帧，并评估模型是否真正内化了世界知识，同时遵守因果时间约束，我们引入了 Envision-Score，这是一种集成多维一致性、物理性和美学的整体指标。对 15 个模型（10 个专业 T2I 模型，5 个统一模型）的综合评估发现：专业 T2I 模型表现出审美渲染的熟练程度，但缺乏内在的世界知识。统一的多模态模型弥补了这一差距，在因果叙事连贯性方面始终优于专业模型。然而，即使这些统一的架构仍然服从于闭源模型，并且难以克服时空一致性的核心挑战。这表明，对因果隔离的单个图像的关注会阻碍多帧推理和生成，促进静态模式匹配而不是动态世界建模，最终限制世界知识的内化和生成。</li>
</ul>

<h3>Title: Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling</h3>
<ul>
<li><strong>Authors: </strong>Meng Cao, Haokun Lin, Haoyuan Li, Haoran Tang, Rongtao Xu, Dong An, Xue Liu, Ian Reid, Xiaodan Liang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01821">https://arxiv.org/abs/2512.01821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01821">https://arxiv.org/pdf/2512.01821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01821]] Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling(https://arxiv.org/abs/2512.01821)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Spatial reasoning, the ability to understand and interpret the 3D structure of the world, is a critical yet underdeveloped capability in Multimodal Large Language Models (MLLMs). Current methods predominantly rely on verbal descriptive tuning, which suffers from visual illiteracy, i.e., they learn spatial concepts through textual symbols alone, devoid of connection to their visual manifestations. To bridge this gap, this paper introduces MILO, an Implicit spatIaL wOrld modeling paradigm that simulates human-like spatial imagination. MILO integrates a visual generator to provide geometry-aware feedback, thereby implicitly grounding the MLLM's symbolic reasoning in perceptual experience. Complementing this paradigm, we propose RePE (Relative Positional Encoding), a novel encoding scheme that captures relative camera-pose transformations, offering superior performance over absolute coordinate systems. To support the training, we construct GeoGen, a large-scale Geometry-aware Generative dataset with approximately 2,241 videos and 67,827 observation-action-outcome triplets. Experiments demonstrate that our approach significantly enhances spatial reasoning capabilities across multiple baselines and benchmarks, offering a more holistic understanding of 3D space.</li>
<li><strong>摘要：</strong>空间推理，即理解和解释世界 3D 结构的能力，是多模态大型语言模型 (MLLM) 中一项关键但尚未开发的功能。目前的方法主要依赖于语言描述性调整，这存在视觉文盲的问题，即它们仅通过文本符号来学习空间概念，缺乏与其视觉表现的联系。为了弥补这一差距，本文引入了 MILO，一种模拟人类空间想象力的​​隐式空间世界建模范式。 MILO 集成了一个视觉生成器来提供几何感知反馈，从而隐式地将 MLLM 的符号推理建立在感知体验中。为了补充这一范例，我们提出了 RePE（相对位置编码），这是一种新颖的编码方案，可以捕获相对相机姿势变换，提供优于绝对坐标系统的性能。为了支持训练，我们构建了 GeoGen，这是一个大型几何感知生成数据集，包含大约 2,241 个视频和 67,827 个观察-动作-结果三元组。实验表明，我们的方法显着增强了跨多个基线和基准的空间推理能力，提供对 3D 空间的更全面的理解。</li>
</ul>

<h3>Title: Deconstructing Generative Diversity: An Information Bottleneck Analysis of Discrete Latent Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Yudi Wu, Wenhao Zhao, Dianbo Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01831">https://arxiv.org/abs/2512.01831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01831">https://arxiv.org/pdf/2512.01831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01831]] Deconstructing Generative Diversity: An Information Bottleneck Analysis of Discrete Latent Generative Models(https://arxiv.org/abs/2512.01831)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative diversity varies significantly across discrete latent generative models such as AR, MIM, and Diffusion. We propose a diagnostic framework, grounded in Information Bottleneck (IB) theory, to analyze the underlying strategies resolving this behavior. The framework models generation as a conflict between a 'Compression Pressure' - a drive to minimize overall codebook entropy - and a 'Diversity Pressure' - a drive to maximize conditional entropy given an input. We further decompose this diversity into two primary sources: 'Path Diversity', representing the choice of high-level generative strategies, and 'Execution Diversity', the randomness in executing a chosen strategy. To make this decomposition operational, we introduce three zero-shot, inference-time interventions that directly perturb the latent generative process and reveal how models allocate and express diversity. Application of this probe-based framework to representative AR, MIM, and Diffusion systems reveals three distinct strategies: "Diversity-Prioritized" (MIM), "Compression-Prioritized" (AR), and "Decoupled" (Diffusion). Our analysis provides a principled explanation for their behavioral differences and informs a novel inference-time diversity enhancement technique.</li>
<li><strong>摘要：</strong>AR、MIM 和 Diffusion 等离散潜在生成模型的生成多样性差异很大。我们提出了一个基于信息瓶颈（IB）理论的诊断框架，来分析解决这种行为的基本策略。该框架将生成建模为“压缩压力”（最小化整体码本熵的驱动力）和“多样性压力”（给定输入的条件熵最大化的驱动力）之间的冲突。我们进一步将这种多样性分解为两个主要来源：“路径多样性”，代表高级生成策略的选择，以及“执行多样性”，即执行所选策略的随机性。为了使这种分解具有可操作性，我们引入了三种零样本推理时间干预措施，它们直接扰乱潜在的生成过程并揭示模型如何分配和表达多样性。将这种基于探针的框架应用于代表性的 AR、MIM 和扩散系统揭示了三种不同的策略：“多样性优先”（MIM）、“压缩优先”（AR）和“解耦”（扩散）。我们的分析为它们的行为差异提供了原则性解释，并提供了一种新颖的推理时间多样性增强技术。</li>
</ul>

<h3>Title: PhyDetEx: Detecting and Explaining the Physical Plausibility of T2V Models</h3>
<ul>
<li><strong>Authors: </strong>Zeqing Wang, Keze Wang, Lei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01843">https://arxiv.org/abs/2512.01843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01843">https://arxiv.org/pdf/2512.01843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01843]] PhyDetEx: Detecting and Explaining the Physical Plausibility of T2V Models(https://arxiv.org/abs/2512.01843)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Driven by the growing capacity and training scale, Text-to-Video (T2V) generation models have recently achieved substantial progress in video quality, length, and instruction-following capability. However, whether these models can understand physics and generate physically plausible videos remains a question. While Vision-Language Models (VLMs) have been widely used as general-purpose evaluators in various applications, they struggle to identify the physically impossible content from generated videos. To investigate this issue, we construct a \textbf{PID} (\textbf{P}hysical \textbf{I}mplausibility \textbf{D}etection) dataset, which consists of a \textit{test split} of 500 manually annotated videos and a \textit{train split} of 2,588 paired videos, where each implausible video is generated by carefully rewriting the caption of its corresponding real-world video to induce T2V models producing physically implausible content. With the constructed dataset, we introduce a lightweight fine-tuning approach, enabling VLMs to not only detect physically implausible events but also generate textual explanations on the violated physical principles. Taking the fine-tuned VLM as a physical plausibility detector and explainer, namely \textbf{PhyDetEx}, we benchmark a series of state-of-the-art T2V models to assess their adherence to physical laws. Our findings show that although recent T2V models have made notable progress toward generating physically plausible content, understanding and adhering to physical laws remains a challenging issue, especially for open-source models. Our dataset, training code, and checkpoints are available at \href{this https URL}{this https URL}.</li>
<li><strong>摘要：</strong>在不断增长的容量和训练规模的推动下，文本到视频（T2V）生成模型最近在视频质量、长度和指令跟踪能力方面取得了实质性进展。然而，这些模型是否能够理解物理并生成物理上合理的视频仍然是一个问题。虽然视觉语言模型 (VLM) 已广泛用作各种应用中的通用评估器，但它们很难从生成的视频中识别物理上不可能的内容。为了研究这个问题，我们构建了一个 \textbf{PID} (\textbf{P}hysical \textbf{I}mplausibility \textbf{D}etection) 数据集，其中包含 500 个手动注释视频的 \textit{test split} 和包含 2,588 个配对视频的 \textit{train split}，其中每个不可信视频是通过仔细重写其对应的真实视频的标题来生成的，以诱导 T2V模型产生物理上难以置信的内容。通过构建的数据集，我们引入了一种轻量级微调方法，使 VLM 不仅能够检测物理上不可信的事件，还能生成有关违反物理原理的文本解释。将微调的 VLM 作为物理合理性检测器和解释器（即 \textbf{PhyDetEx}），我们对一系列最先进的 T2V 模型进行基准测试，以评估它们对物理定律的遵守情况。我们的研究结果表明，尽管最近的 T2V 模型在生成物理上合理的内容方面取得了显着进展，但理解和遵守物理定律仍然是一个具有挑战性的问题，特别是对于开源模型而言。我们的数据集、训练代码和检查点可在 \href{this https URL}{this https URL} 中获取。</li>
</ul>

<h3>Title: Register Any Point: Scaling 3D Point Cloud Registration by Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Yue Pan, Tao Sun, Liyuan Zhu, Lucas Nunes, Iro Armeni, Jens Behley, Cyrill Stachniss</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01850">https://arxiv.org/abs/2512.01850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01850">https://arxiv.org/pdf/2512.01850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01850]] Register Any Point: Scaling 3D Point Cloud Registration by Flow Matching(https://arxiv.org/abs/2512.01850)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Point cloud registration aligns multiple unposed point clouds into a common frame, and is a core step for 3D reconstruction and robot localization. In this work, we cast registration as conditional generation: a learned continuous, point-wise velocity field transports noisy points to a registered scene, from which the pose of each view is recovered. Unlike previous methods that conduct correspondence matching to estimate the transformation between a pair of point clouds and then optimize the pairwise transformations to realize multi-view registration, our model directly generates the registered point cloud. With a lightweight local feature extractor and test-time rigidity enforcement, our approach achieves state-of-the-art results on pairwise and multi-view registration benchmarks, particularly with low overlap, and generalizes across scales and sensor modalities. It further supports downstream tasks including relocalization, multi-robot SLAM, and multi-session map merging. Source code available at: this https URL.</li>
<li><strong>摘要：</strong>点云配准将多个未摆设的点云对齐到一个公共框架中，是 3D 重建和机器人定位的核心步骤。在这项工作中，我们将配准视为条件生成：学习的连续、逐点速度场将噪声点传输到配准场景，从中恢复每个视图的姿态。与之前通过对应匹配来估计一对点云之间的变换，然后优化成对变换以实现多视图配准的方法不同，我们的模型直接生成配准的点云。通过轻量级局部特征提取器和测试时刚性强制执行，我们的方法在成对和多视图配准基准上实现了最先进的结果，特别是在低重叠的情况下，并且跨尺度和传感器模式进行了泛化。它还支持下游任务，包括重新定位、多机器人 SLAM 和多会话地图合并。源代码位于：此 https URL。</li>
</ul>

<h3>Title: COACH: Collaborative Agents for Contextual Highlighting - A Multi-Agent Framework for Sports Video Analysis</h3>
<ul>
<li><strong>Authors: </strong>Tsz-To Wong, Ching-Chun Huang, Hong-Han Shuai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01853">https://arxiv.org/abs/2512.01853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01853">https://arxiv.org/pdf/2512.01853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01853]] COACH: Collaborative Agents for Contextual Highlighting - A Multi-Agent Framework for Sports Video Analysis(https://arxiv.org/abs/2512.01853)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Intelligent sports video analysis demands a comprehensive understanding of temporal context, from micro-level actions to macro-level game strategies. Existing end-to-end models often struggle with this temporal hierarchy, offering solutions that lack generalization, incur high development costs for new tasks, and suffer from poor interpretability. To overcome these limitations, we propose a reconfigurable Multi-Agent System (MAS) as a foundational framework for sports video understanding. In our system, each agent functions as a distinct "cognitive tool" specializing in a specific aspect of analysis. The system's architecture is not confined to a single temporal dimension or task. By leveraging iterative invocation and flexible composition of these agents, our framework can construct adaptive pipelines for both short-term analytic reasoning (e.g., Rally QA) and long-term generative summarization (e.g., match summaries). We demonstrate the adaptability of this framework using two representative tasks in badminton analysis, showcasing its ability to bridge fine-grained event detection and global semantic organization. This work presents a paradigm shift towards a flexible, scalable, and interpretable system for robust, cross-task sports video this http URL project homepage is available at this https URL</li>
<li><strong>摘要：</strong>智能体育视频分析需要对时间背景的全面理解，从微观层面的动作到宏观层面的比赛策略。现有的端到端模型经常与这种时间层次结构作斗争，提供的解决方案缺乏泛化性，新任务的开发成本很高，并且可解释性差。为了克服这些限制，我们提出了一种可重新配置的多智能体系统（MAS）作为体育视频理解的基础框架。在我们的系统中，每个代理都充当一个独特的“认知工具”，专门从事特定方面的分析。系统的架构并不局限于单一的时间维度或任务。通过利用这些代理的迭代调用和灵活组合，我们的框架可以为短期分析推理（例如，Rally QA）和长期生成摘要（例​​如，匹配摘要）构建自适应管道。我们使用羽毛球分析中的两个代表性任务来演示该框架的适应性，展示其连接细粒度事件检测和全局语义组织的能力。这项工作提出了向灵活、可扩展和可解释的系统的范式转变，用于强大的跨任务体育视频此 http URL 项目主页可在此 https URL 上找到</li>
</ul>

<h3>Title: Med-VCD: Mitigating Hallucination for Medical Large Vision Language Models through Visual Contrastive Decoding</h3>
<ul>
<li><strong>Authors: </strong>Zahra Mahdavi, Zahra Khodakaramimaghsoud, Hooman Khaloo, Sina Bakhshandeh Taleshani, Erfan Hashemi, Javad Mirzapour Kaleybar, Omid Nejati Manzari</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01922">https://arxiv.org/abs/2512.01922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01922">https://arxiv.org/pdf/2512.01922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01922]] Med-VCD: Mitigating Hallucination for Medical Large Vision Language Models through Visual Contrastive Decoding(https://arxiv.org/abs/2512.01922)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large vision-language models (LVLMs) are now central to healthcare applications such as medical visual question answering and imaging report generation. Yet, these models remain vulnerable to hallucination outputs that appear plausible but are in fact incorrect. In the natural image domain, several decoding strategies have been proposed to mitigate hallucinations by reinforcing visual evidence, but most rely on secondary decoding or rollback procedures that substantially slow inference. Moreover, existing solutions are often domain-specific and may introduce misalignment between modalities or between generated and ground-truth content. We introduce Med-VCD, a sparse visual-contrastive decoding method that mitigates hallucinations in medical LVLMs without the time overhead of secondary decoding. Med-VCD incorporates a novel token-sparsification strategy that selects visually informed tokens on the fly, trimming redundancy while retaining critical visual context and thus balancing efficiency with reliability. Evaluations on eight medical datasets, spanning ophthalmology, radiology, and pathology tasks in visual question answering, report generation, and dedicated hallucination benchmarks, show that Med-VCD raises factual accuracy by an average of 13\% and improves hallucination accuracy by 6\% relative to baseline medical LVLMs.</li>
<li><strong>摘要：</strong>大型视觉语言模型 (LVLM) 现在是医疗视觉问答和成像报告生成等医疗保健应用的核心。然而，这些模型仍然容易受到看似合理但实际上不正确的幻觉输出的影响。在自然图像领域，已经提出了几种解码策略，通过增强视觉证据来减轻幻觉，但大多数依赖于二次解码或回滚过程，这会大大减慢推理速度。此外，现有的解决方案通常是特定于领域的，并且可能会导致模式之间或生成的内容与真实内容之间的不一致。我们引入了 Med-VCD，这是一种稀疏视觉对比解码方法，可以减轻医学 LVLM 中的幻觉，而无需二次解码的时间开销。 Med-VCD 采用了一种新颖的令牌稀疏策略，可以动态选择视觉信息丰富的令牌，在保留关键视觉上下文的同时消除冗余，从而平衡效率与可靠性。对八个医学数据集（涵盖视觉问答、报告生成和专用幻觉基准中的眼科、放射学和病理学任务）的评估表明，相对于基线医学 LVLM，Med-VCD 的事实准确性平均提高了 13%，幻觉准确性提高了 6%。</li>
</ul>

<h3>Title: GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment</h3>
<ul>
<li><strong>Authors: </strong>Haoyang He, Jay Patrikar, Dong-Ki Kim, Max Smith, Daniel McGann, Ali-akbar Agha-mohammadi, Shayegan Omidshafiei, Sebastian Scherer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01952">https://arxiv.org/abs/2512.01952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01952">https://arxiv.org/pdf/2512.01952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01952]] GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment(https://arxiv.org/abs/2512.01952)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in video world modeling have enabled large-scale generative models to simulate embodied environments with high visual fidelity, providing strong priors for prediction, planning, and control. Yet, despite their realism, these models often lack geometric grounding, limiting their use in navigation tasks that require spatial coherence and long-horizon stability. We introduce Reinforcement Learning with World Grounding (RLWG), a self-supervised post-training framework that aligns pretrained world models with a physically verifiable structure through geometric and perceptual rewards. Analogous to reinforcement learning from verifiable feedback (RLVR) in language models, RLWG can use multiple rewards that measure pose cycle-consistency, depth reprojection, and temporal coherence. We instantiate this framework with GrndCtrl, a reward-aligned adaptation method based on Group Relative Policy Optimization (GRPO), yielding world models that maintain stable trajectories, consistent geometry, and reliable rollouts for embodied navigation. Like post-training alignment in large language models, GrndCtrl leverages verifiable rewards to bridge generative pretraining and grounded behavior, achieving superior spatial coherence and navigation stability over supervised fine-tuning in outdoor environments.</li>
<li><strong>摘要：</strong>视频世界建模的最新进展使得大规模生成模型能够以高视觉保真度模拟具体环境，为预测、规划和控制提供强大的先验。然而，尽管它们很现实，但这些模型通常缺乏几何基础，限制了它们在需要空间相干性和长视距稳定性的导航任务中的使用。我们引入了带有世界基础的强化学习（RLWG），这是一种自我监督的训练后框架，通过几何和感知奖励将预训练的世界模型与物理可验证的结构结合起来。与语言模型中可验证反馈 (RLVR) 的强化学习类似，RLWG 可以使用多种奖励来衡量姿势循环一致性、深度重投影和时间连贯性。我们使用 GrndCtrl 实例化该框架，这是一种基于组相对策略优化 (GRPO) 的奖励对齐适应方法，生成的世界模型可以保持稳定的轨迹、一致的几何形状和可靠的实体导航推出。与大型语言模型中的训练后对齐一样，GrndCtrl 利用可验证的奖励来连接生成性预训练和扎根行为，从而在室外环境中的监督微调上实现卓越的空间连贯性和导航稳定性。</li>
</ul>

<h3>Title: SpriteHand: Real-Time Versatile Hand-Object Interaction with Autoregressive Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Zisu Li, Hengye Lyu, Jiaxin Shi, Yufeng Zeng, Mingming Fan, Hanwang Zhang, Chen Liang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01960">https://arxiv.org/abs/2512.01960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01960">https://arxiv.org/pdf/2512.01960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01960]] SpriteHand: Real-Time Versatile Hand-Object Interaction with Autoregressive Video Generation(https://arxiv.org/abs/2512.01960)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Modeling and synthesizing complex hand-object interactions remains a significant challenge, even for state-of-the-art physics engines. Conventional simulation-based approaches rely on explicitly defined rigid object models and pre-scripted hand gestures, making them inadequate for capturing dynamic interactions with non-rigid or articulated entities such as deformable fabrics, elastic materials, hinge-based structures, furry surfaces, or even living creatures. In this paper, we present SpriteHand, an autoregressive video generation framework for real-time synthesis of versatile hand-object interaction videos across a wide range of object types and motion patterns. SpriteHand takes as input a static object image and a video stream in which the hands are imagined to interact with the virtual object embedded in a real-world scene, and generates corresponding hand-object interaction effects in real time. Our model employs a causal inference architecture for autoregressive generation and leverages a hybrid post-training approach to enhance visual realism and temporal coherence. Our 1.3B model supports real-time streaming generation at around 18 FPS and 640x368 resolution, with an approximate 150 ms latency on a single NVIDIA RTX 5090 GPU, and more than a minute of continuous output. Experiments demonstrate superior visual quality, physical plausibility, and interaction fidelity compared to both generative and engine-based baselines.</li>
<li><strong>摘要：</strong>即使对于最先进的物理引擎来说，建模和合成复杂的手部物体交互仍然是一个重大挑战。传统的基于模拟的方法依赖于明确定义的刚性对象模型和预先编写的手势，这使得它们不足以捕获与非刚性或铰接实体（例如可变形织物、弹性材料、基于铰链的结构、毛茸茸的表面甚至生物）的动态交互。在本文中，我们提出了 SpriteHand，这是一种自回归视频生成框架，用于实时合成各种对象类型和运动模式的多功能手部对象交互视频。 SpriteHand 以静态物体图像和视频流为输入，其中想象手部与嵌入在现实世界场景中的虚拟物体进行交互，并实时生成相应的手部物体交互效果。我们的模型采用因果推理架构进行自回归生成，并利用混合后训练方法来增强视觉真实感和时间连贯性。我们的 1.3B 模型支持约 18 FPS 和 640x368 分辨率的实时流生成，在单个 NVIDIA RTX 5090 GPU 上的延迟约为 150 毫秒，连续输出超过一分钟。实验证明，与生成基线和基于引擎的基线相比，具有卓越的视觉质量、物理合理性和交互保真度。</li>
</ul>

<h3>Title: Forecasting in Offline Reinforcement Learning for Non-stationary Environments</h3>
<ul>
<li><strong>Authors: </strong>Suzan Ece Ada, Georg Martius, Emre Ugur, Erhan Oztop</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01987">https://arxiv.org/abs/2512.01987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01987">https://arxiv.org/pdf/2512.01987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01987]] Forecasting in Offline Reinforcement Learning for Non-stationary Environments(https://arxiv.org/abs/2512.01987)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Offline Reinforcement Learning (RL) provides a promising avenue for training policies from pre-collected datasets when gathering additional interaction data is infeasible. However, existing offline RL methods often assume stationarity or only consider synthetic perturbations at test time, assumptions that often fail in real-world scenarios characterized by abrupt, time-varying offsets. These offsets can lead to partial observability, causing agents to misperceive their true state and degrade performance. To overcome this challenge, we introduce Forecasting in Non-stationary Offline RL (FORL), a framework that unifies (i) conditional diffusion-based candidate state generation, trained without presupposing any specific pattern of future non-stationarity, and (ii) zero-shot time-series foundation models. FORL targets environments prone to unexpected, potentially non-Markovian offsets, requiring robust agent performance from the onset of each episode. Empirical evaluations on offline RL benchmarks, augmented with real-world time-series data to simulate realistic non-stationarity, demonstrate that FORL consistently improves performance compared to competitive baselines. By integrating zero-shot forecasting with the agent's experience, we aim to bridge the gap between offline RL and the complexities of real-world, non-stationary environments.</li>
<li><strong>摘要：</strong>当收集额外的交互数据不可行时，离线强化学习（RL）为根据预先收集的数据集训练策略提供了一种有前途的途径。然而，现有的离线强化学习方法通​​常假设平稳性或仅考虑测试时的合成扰动，这些假设在以突然的时变偏移为特征的现实场景中经常失败。这些偏移可能会导致部分可观察性，导致代理错误地感知其真实状态并降低性能。为了克服这一挑战，我们引入了非平稳离线 RL 预测（FORL），该框架统一了（i）基于条件扩散的候选状态生成（在不预设未来非平稳性的任何特定模式的情况下进行训练）和（ii）零样本时间序列基础模型。 FORL 的目标环境容易出现意外的、潜在的非马尔可夫偏移，需要从每个事件开始就具有强大的代理性能。对离线 RL 基准的实证评估，加上现实世界的时间序列数据来模拟现实的非平稳性，表明与竞争基准相比，FORL 持续提高了性能。通过将零样本预测与代理的经验相结合，我们的目标是弥合离线强化学习与现实世界、非平稳环境的复杂性之间的差距。</li>
</ul>

<h3>Title: PAI-Bench: A Comprehensive Benchmark For Physical AI</h3>
<ul>
<li><strong>Authors: </strong>Fengzhe Zhou, Jiannan Huang, Jialuo Li, Deva Ramanan, Humphrey Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01989">https://arxiv.org/abs/2512.01989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01989">https://arxiv.org/pdf/2512.01989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01989]] PAI-Bench: A Comprehensive Benchmark For Physical AI(https://arxiv.org/abs/2512.01989)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Physical AI aims to develop models that can perceive and predict real-world dynamics; yet, the extent to which current multi-modal large language models and video generative models support these abilities is insufficiently understood. We introduce Physical AI Bench (PAI-Bench), a unified and comprehensive benchmark that evaluates perception and prediction capabilities across video generation, conditional video generation, and video understanding, comprising 2,808 real-world cases with task-aligned metrics designed to capture physical plausibility and domain-specific reasoning. Our study provides a systematic assessment of recent models and shows that video generative models, despite strong visual fidelity, often struggle to maintain physically coherent dynamics, while multi-modal large language models exhibit limited performance in forecasting and causal interpretation. These observations suggest that current systems are still at an early stage in handling the perceptual and predictive demands of Physical AI. In summary, PAI-Bench establishes a realistic foundation for evaluating Physical AI and highlights key gaps that future systems must address.</li>
<li><strong>摘要：</strong>物理人工智能旨在开发能够感知和预测现实世界动态的模型；然而，当前的多模态大语言模型和视频生成模型对这些能力的支持程度尚不清楚。我们推出了物理 AI Bench (PAI-Bench)，这是一个统一、全面的基准，用于评估视频生成、条件视频生成和视频理解的感知和预测能力，包含 2,808 个现实案例，其任务相关指标旨在捕获物理合理性和特定领域推理。我们的研究对最新模型进行了系统评估，并表明视频生成模型尽管具有很强的视觉保真度，但通常难以保持物理连贯的动态性，而多模态大语言模型在预测和因果解释方面表现有限。这些观察结果表明，当前系统在处理物理人工智能的感知和预测需求方面仍处于早期阶段。总之，PAI-Bench 为评估物理人工智能奠定了现实基础，并强调了未来系统必须解决的关键差距。</li>
</ul>

<h3>Title: AirSim360: A Panoramic Simulation Platform within Drone View</h3>
<ul>
<li><strong>Authors: </strong>Xian Ge, Yuling Pan, Yuhang Zhang, Xiang Li, Weijun Zhang, Dizhe Zhang, Zhaoliang Wan, Xin Lin, Xiangkai Zhang, Juntao Liang, Jason Li, Wenjie Jiang, Bo Du, Ming-Hsuan Yang, Lu Qi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.02009">https://arxiv.org/abs/2512.02009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.02009">https://arxiv.org/pdf/2512.02009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.02009]] AirSim360: A Panoramic Simulation Platform within Drone View(https://arxiv.org/abs/2512.02009)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The field of 360-degree omnidirectional understanding has been receiving increasing attention for advancing spatial intelligence. However, the lack of large-scale and diverse data remains a major limitation. In this work, we propose AirSim360, a simulation platform for omnidirectional data from aerial viewpoints, enabling wide-ranging scene sampling with drones. Specifically, AirSim360 focuses on three key aspects: a render-aligned data and labeling paradigm for pixel-level geometric, semantic, and entity-level understanding; an interactive pedestrian-aware system for modeling human behavior; and an automated trajectory generation paradigm to support navigation tasks. Furthermore, we collect more than 60K panoramic samples and conduct extensive experiments across various tasks to demonstrate the effectiveness of our simulator. Unlike existing simulators, our work is the first to systematically model the 4D real world under an omnidirectional setting. The entire platform, including the toolkit, plugins, and collected datasets, will be made publicly available at this https URL.</li>
<li><strong>摘要：</strong>360度全方位理解领域因推进空间智能而受到越来越多的关注。然而，缺乏大规模和多样化的数据仍然是一个主要限制。在这项工作中，我们提出了 AirSim360，这是一个从空中视角获取全向数据的模拟平台，可以使用无人机进行大范围的场景采样。具体来说，AirSim360 专注于三个关键方面：用于像素级几何、语义和实体级理解的渲染对齐数据和标签范例；用于模拟人类行为的交互式行人感知系统；以及支持导航任务的自动轨迹生成范例。此外，我们收集了超过 60K 的全景样本，并在各种任务中进行了广泛的实验，以证明我们的模拟器的有效性。与现有的模拟器不同，我们的工作是第一个在全方位设置下系统地模拟 4D 现实世界的工作。整个平台，包括工具包、插件和收集的数据集，将通过此 https URL 公开提供。</li>
</ul>

<h3>Title: Improved Mean Flows: On the Challenges of Fastforward Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Zhengyang Geng, Yiyang Lu, Zongze Wu, Eli Shechtman, J. Zico Kolter, Kaiming He</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.02012">https://arxiv.org/abs/2512.02012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.02012">https://arxiv.org/pdf/2512.02012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.02012]] Improved Mean Flows: On the Challenges of Fastforward Generative Models(https://arxiv.org/abs/2512.02012)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>MeanFlow (MF) has recently been established as a framework for one-step generative modeling. However, its ``fastforward'' nature introduces key challenges in both the training objective and the guidance mechanism. First, the original MF's training target depends not only on the underlying ground-truth fields but also on the network itself. To address this issue, we recast the objective as a loss on the instantaneous velocity $v$, re-parameterized by a network that predicts the average velocity $u$. Our reformulation yields a more standard regression problem and improves the training stability. Second, the original MF fixes the classifier-free guidance scale during training, which sacrifices flexibility. We tackle this issue by formulating guidance as explicit conditioning variables, thereby retaining flexibility at test time. The diverse conditions are processed through in-context conditioning, which reduces model size and benefits performance. Overall, our $\textbf{improved MeanFlow}$ ($\textbf{iMF}$) method, trained entirely from scratch, achieves $\textbf{1.72}$ FID with a single function evaluation (1-NFE) on ImageNet 256$\times$256. iMF substantially outperforms prior methods of this kind and closes the gap with multi-step methods while using no distillation. We hope our work will further advance fastforward generative modeling as a stand-alone paradigm.</li>
<li><strong>摘要：</strong>MeanFlow (MF) 最近被建立为一步生成建模的框架。然而，其“快进”性质给培训目标和指导机制带来了关键挑战。首先，原始 MF 的训练目标不仅取决于底层真实字段，还取决于网络本身。为了解决这个问题，我们将目标重新定义为瞬时速度 $v$ 的损失，并由预测平均速度 $u$ 的网络重新参数化。我们的重新制定产生了更标准的回归问题并提高了训练稳定性。其次，原始MF在训练过程中固定了无分类器的指导尺度，这牺牲了灵活性。我们通过将指导制定为明确的条件变量来解决这个问题，从而在测试时保持灵活性。通过上下文条件处理不同的条件，这可以减小模型大小并提高性能。总体而言，我们的 $\textbf{improved MeanFlow}$ ($\textbf{iMF}$) 方法完全从头开始训练，在 ImageNet 256$\times$256 上通过单函数评估 (1-NFE) 实现了 $\textbf{1.72}$ FID。 iMF 大大优于此类现有方法，并且在不使用蒸馏的情况下缩小了与多步骤方法的差距。我们希望我们的工作能够进一步推动快速生成模型成为一种独立的范式。</li>
</ul>

<h3>Title: TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models</h3>
<ul>
<li><strong>Authors: </strong>Zhiheng Liu, Weiming Ren, Haozhe Liu, Zijian Zhou, Shoufa Chen, Haonan Qiu, Xiaoke Huang, Zhaochong An, Fanny Yang, Aditya Patel, Viktar Atliha, Tony Ng, Xiao Han, Chuyan Zhu, Chenyang Zhang, Ding Liu, Juan-Manuel Perez-Rua, Sen He, Jürgen Schmidhuber, Wenhu Chen, Ping Luo, Wei Liu, Tao Xiang, Jonas Schult, Yuren Cong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.02014">https://arxiv.org/abs/2512.02014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.02014">https://arxiv.org/pdf/2512.02014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.02014]] TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models(https://arxiv.org/abs/2512.02014)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Unified multimodal models (UMMs) aim to jointly perform multimodal understanding and generation within a single framework. We present TUNA, a native UMM that builds a unified continuous visual representation by cascading a VAE encoder with a representation encoder. This unified representation space allows end-to-end processing of images and videos for both understanding and generation tasks. Compared to prior UMMs with decoupled representations, TUNA's unified visual space avoids representation format mismatches introduced by separate encoders, outperforming decoupled alternatives in both understanding and generation. Moreover, we observe that stronger pretrained representation encoders consistently yield better performance across all multimodal tasks, highlighting the importance of the representation encoder. Finally, in this unified setting, jointly training on both understanding and generation data allows the two tasks to benefit from each other rather than interfere. Our extensive experiments on multimodal understanding and generation benchmarks show that TUNA achieves state-of-the-art results in image and video understanding, image and video generation, and image editing, demonstrating the effectiveness and scalability of its unified representation design.</li>
<li><strong>摘要：</strong>统一多模态模型（UMM）旨在在单一框架内联合执行多模态理解和生成。我们提出了 TUNA，一种原生 UMM，它通过级联 VAE 编码器和表示编码器来构建统一的连续视觉表示。这种统一的表示空间允许对图像和视频进行端到端处理，以实现理解和生成任务。与之前具有解耦表示的 UMM 相比，TUNA 的统一视觉空间避免了单独编码器引入的表示格式不匹配，在理解和生成方面都优于解耦替代方案。此外，我们观察到，更强的预训练表示编码器在所有多模态任务中始终能产生更好的性能，这凸显了表示编码器的重要性。最后，在这个统一的环境中，对理解和生成数据的联合训练使这两个任务能够相互受益而不是相互干扰。我们在多模态理解和生成基准方面进行的大量实验表明，TUNA 在图像和视频理解、图像和视频生成以及图像编辑方面取得了最先进的结果，展示了其统一表示设计的有效性和可扩展性。</li>
</ul>

<h3>Title: Generative Video Motion Editing with 3D Point Tracks</h3>
<ul>
<li><strong>Authors: </strong>Yao-Chih Lee, Zhoutong Zhang, Jiahui Huang, Jui-Hsien Wang, Joon-Young Lee, Jia-Bin Huang, Eli Shechtman, Zhengqi Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.02015">https://arxiv.org/abs/2512.02015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.02015">https://arxiv.org/pdf/2512.02015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.02015]] Generative Video Motion Editing with 3D Point Tracks(https://arxiv.org/abs/2512.02015)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Camera and object motions are central to a video's narrative. However, precisely editing these captured motions remains a significant challenge, especially under complex object movements. Current motion-controlled image-to-video (I2V) approaches often lack full-scene context for consistent video editing, while video-to-video (V2V) methods provide viewpoint changes or basic object translation, but offer limited control over fine-grained object motion. We present a track-conditioned V2V framework that enables joint editing of camera and object motion. We achieve this by conditioning a video generation model on a source video and paired 3D point tracks representing source and target motions. These 3D tracks establish sparse correspondences that transfer rich context from the source video to new motions while preserving spatiotemporal coherence. Crucially, compared to 2D tracks, 3D tracks provide explicit depth cues, allowing the model to resolve depth order and handle occlusions for precise motion editing. Trained in two stages on synthetic and real data, our model supports diverse motion edits, including joint camera/object manipulation, motion transfer, and non-rigid deformation, unlocking new creative potential in video editing.</li>
<li><strong>摘要：</strong>摄像机和物体运动是视频叙事的核心。然而，精确编辑这些捕获的运动仍然是一个重大挑战，特别是在复杂的物体运动下。当前的运动控制图像到视频 (I2V) 方法通常缺乏用于一致视频编辑的全场景上下文，而视频到视频 (V2V) 方法提供视点更改或基本对象转换，但对细粒度对象运动的控制有限。我们提出了一个跟踪调节的 V2V 框架，可以对摄像机和物体运动进行联合编辑。我们通过在源视频和代表源运动和目标运动的成对 3D 点轨迹上调节视频生成模型来实现这一点。这些 3D 轨道建立稀疏对应关系，将丰富的上下文从源视频转移到新的动作，同时保持时空连贯性。至关重要的是，与 2D 轨迹相比，3D 轨迹提供了明确的深度提示，允许模型解析深度顺序并处理遮挡以进行精确的运动编辑。我们的模型经过合成数据和真实数据的两个阶段的训练，支持多种运动编辑，包括联合相机/对象操纵、运动传输和非刚性变形，释放视频编辑中的新创意潜力。</li>
</ul>

<h3>Title: Data-Centric Visual Development for Self-Driving Labs</h3>
<ul>
<li><strong>Authors: </strong>Anbang Liu, Guanzhong Hu, Jiayi Wang, Ping Guo, Han Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.02018">https://arxiv.org/abs/2512.02018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.02018">https://arxiv.org/pdf/2512.02018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.02018]] Data-Centric Visual Development for Self-Driving Labs(https://arxiv.org/abs/2512.02018)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Self-driving laboratories offer a promising path toward reducing the labor-intensive, time-consuming, and often irreproducible workflows in the biological sciences. Yet their stringent precision requirements demand highly robust models whose training relies on large amounts of annotated data. However, this kind of data is difficult to obtain in routine practice, especially negative samples. In this work, we focus on pipetting, the most critical and precision sensitive action in SDLs. To overcome the scarcity of training data, we build a hybrid pipeline that fuses real and virtual data generation. The real track adopts a human-in-the-loop scheme that couples automated acquisition with selective human verification to maximize accuracy with minimal effort. The virtual track augments the real data using reference-conditioned, prompt-guided image generation, which is further screened and validated for reliability. Together, these two tracks yield a class-balanced dataset that enables robust bubble detection training. On a held-out real test set, a model trained entirely on automatically acquired real images reaches 99.6% accuracy, and mixing real and generated data during training sustains 99.4% accuracy while reducing collection and review load. Our approach offers a scalable and cost-effective strategy for supplying visual feedback data to SDL workflows and provides a practical solution to data scarcity in rare event detection and broader vision tasks.</li>
<li><strong>摘要：</strong>自动驾驶实验室为减少生物科学中劳动密集型、耗时且通常不可重复的工作流程提供了一条有希望的途径。然而，它们严格的精度要求需要高度稳健的模型，其训练依赖于大量注释数据。然而，这种数据在日常实践中很难获得，尤其是负样本。在这项工作中，我们重点关注移液，这是 SDL 中最关键、最精确的操作。为了克服训练数据的稀缺性，我们构建了一个融合真实和虚拟数据生成的混合管道。真实赛道采用人机交互方案，将自动采集与选择性人工验证相结合，以最小的努力最大限度地提高准确性。虚拟赛道使用参考条件、提示引导的图像生成来增强真实数据，并进一步筛选和验证其可靠性。这两个轨道一起产生一个类平衡的数据集，可以实现强大的气泡检测训练。在保留的真实测试集上，完全基于自动获取的真实图像进行训练的模型达到了 99.6% 的准确率，并且在训练过程中混合真实数据和生成的数据可以维持 99.4% 的准确率，同时减少收集和审查负载。我们的方法提供了一种可扩展且经济高效的策略，用于向 SDL 工作流程提供视觉反馈数据，并为罕见事件检测和更广泛的视觉任务中的数据稀缺问题提供实用的解决方案。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
