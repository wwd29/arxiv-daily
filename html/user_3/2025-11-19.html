<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-11-19</h1>
<h3>Title: SCALEX: Scalable Concept and Latent Exploration for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>E. Zhixuan Zeng, Yuhao Chen, Alexander Wong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13750">https://arxiv.org/abs/2511.13750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13750">https://arxiv.org/pdf/2511.13750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13750]] SCALEX: Scalable Concept and Latent Exploration for Diffusion Models(https://arxiv.org/abs/2511.13750)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Image generation models frequently encode social biases, including stereotypes tied to gender, race, and profession. Existing methods for analyzing these biases in diffusion models either focus narrowly on predefined categories or depend on manual interpretation of latent directions. These constraints limit scalability and hinder the discovery of subtle or unanticipated patterns. We introduce SCALEX, a framework for scalable and automated exploration of diffusion model latent spaces. SCALEX extracts semantically meaningful directions from H-space using only natural language prompts, enabling zero-shot interpretation without retraining or labelling. This allows systematic comparison across arbitrary concepts and large-scale discovery of internal model associations. We show that SCALEX detects gender bias in profession prompts, ranks semantic alignment across identity descriptors, and reveals clustered conceptual structure without supervision. By linking prompts to latent directions directly, SCALEX makes bias analysis in diffusion models more scalable, interpretable, and extensible than prior approaches.</li>
<li><strong>摘要：</strong>图像生成模型经常编码社会偏见，包括与性别、种族和职业相关的刻板印象。分析扩散模型中这些偏差的现有方法要么狭隘地关注预定义的类别，要么依赖于对潜在方向的手动解释。这些限制限制了可扩展性并阻碍了微妙或意外模式的发现。我们引入了 SCALEX，一个用于可扩展和自动探索扩散模型潜在空间的框架。 SCALEX 仅使用自然语言提示从 H 空间中提取语义上有意义的方向，从而无需重新训练或标记即可实现零样本解释。这允许对任意概念进行系统比较并大规模发现内部模型关联。我们表明，SCALEX 可以检测职业提示中的性别偏见，对身份描述符之间的语义对齐进行排名，并在没有监督的情况下揭示集群概念结构。通过将提示直接链接到潜在方向，SCALEX 使扩散模型中的偏差分析比以前的方法更具可扩展性、可解释性和可扩展性。</li>
</ul>

<h3>Title: ChemFixer: Correcting Invalid Molecules to Unlock Previously Unseen Chemical Space</h3>
<ul>
<li><strong>Authors: </strong>Jun-Hyoung Park, Ho-Jun Song, Seong-Whan Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13758">https://arxiv.org/abs/2511.13758</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13758">https://arxiv.org/pdf/2511.13758</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13758]] ChemFixer: Correcting Invalid Molecules to Unlock Previously Unseen Chemical Space(https://arxiv.org/abs/2511.13758)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Deep learning-based molecular generation models have shown great potential in efficiently exploring vast chemical spaces by generating potential drug candidates with desired properties. However, these models often produce chemically invalid molecules, which limits the usable scope of the learned chemical space and poses significant challenges for practical applications. To address this issue, we propose ChemFixer, a framework designed to correct invalid molecules into valid ones. ChemFixer is built on a transformer architecture, pre-trained using masking techniques, and fine-tuned on a large-scale dataset of valid/invalid molecular pairs that we constructed. Through comprehensive evaluations across diverse generative models, ChemFixer improved molecular validity while effectively preserving the chemical and biological distributional properties of the original outputs. This indicates that ChemFixer can recover molecules that could not be previously generated, thereby expanding the diversity of potential drug candidates. Furthermore, ChemFixer was effectively applied to a drug-target interaction (DTI) prediction task using limited data, improving the validity of generated ligands and discovering promising ligand-protein pairs. These results suggest that ChemFixer is not only effective in data-limited scenarios, but also extensible to a wide range of downstream tasks. Taken together, ChemFixer shows promise as a practical tool for various stages of deep learning-based drug discovery, enhancing molecular validity and expanding accessible chemical space.</li>
<li><strong>摘要：</strong>基于深度学习的分子生成模型通过生成具有所需特性的潜在候选药物，在有效探索广阔的化学空间方面显示出巨大的潜力。然而，这些模型经常产生化学上无效的分子，这限制了所学习的化学空间的可用范围，并对实际应用提出了重大挑战。为了解决这个问题，我们提出了 ChemFixer，一个旨在将无效分子纠正为有效分子的框架。 ChemFixer 基于变压器架构构建，使用掩蔽技术进行预训练，并在我们构建的有效/无效分子对的大规模数据集上进行微调。通过对不同生成模型的综合评估，ChemFixer 提高了分子有效性，同时有效保留了原始输出的化学和生物分布特性。这表明 ChemFixer 可以恢复以前无法生成的分子，从而扩大潜在候选药物的多样性。此外，ChemFixer 使用有限的数据有效地应用于药物-靶点相互作用 (DTI) 预测任务，提高了生成配体的有效性并发现了有前途的配体-蛋白质对。这些结果表明 ChemFixer 不仅在数据有限的场景中有效，而且可以扩展到广泛的下游任务。总而言之，ChemFixer 显示出作为基于深度学习的药物发现各个阶段的实用工具的前景，可增强分子有效性并扩展可访问的化学空间。</li>
</ul>

<h3>Title: PROF: An LLM-based Reward Code Preference Optimization Framework for Offline Imitation Learning</h3>
<ul>
<li><strong>Authors: </strong>Shengjie Sun, Jiafei Lyu, Runze Liu, Mengbei Yan, Bo Liu, Deheng Ye, Xiu Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13765">https://arxiv.org/abs/2511.13765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13765">https://arxiv.org/pdf/2511.13765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13765]] PROF: An LLM-based Reward Code Preference Optimization Framework for Offline Imitation Learning(https://arxiv.org/abs/2511.13765)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Offline imitation learning (offline IL) enables training effective policies without requiring explicit reward annotations. Recent approaches attempt to estimate rewards for unlabeled datasets using a small set of expert demonstrations. However, these methods often assume that the similarity between a trajectory and an expert demonstration is positively correlated with the reward, which oversimplifies the underlying reward structure. We propose PROF, a novel framework that leverages large language models (LLMs) to generate and improve executable reward function codes from natural language descriptions and a single expert trajectory. We propose Reward Preference Ranking (RPR), a novel reward function quality assessment and ranking strategy without requiring environment interactions or RL training. RPR calculates the dominance scores of the reward functions, where higher scores indicate better alignment with expert preferences. By alternating between RPR and text-based gradient optimization, PROF fully automates the selection and refinement of optimal reward functions for downstream policy learning. Empirical results on D4RL demonstrate that PROF surpasses or matches recent strong baselines across numerous datasets and domains, highlighting the effectiveness of our approach.</li>
<li><strong>摘要：</strong>离线模仿学习（离线IL）可以训练有效的策略，而不需要明确的奖励注释。最近的方法尝试使用一小组专家演示来估计未标记数据集的奖励。然而，这些方法通常假设轨迹和专家演示之间的相似性与奖励正相关，这过度简化了潜在的奖励结构。我们提出了 PROF，这是一种新颖的框架，它利用大型语言模型（LLM）从自然语言描述和单个专家轨迹生成和改进可执行的奖励函数代码。我们提出了奖励偏好排名（RPR），这是一种新颖的奖励函数质量评估和排名策略，无需环境交互或强化学习训练。 RPR 计算奖励函数的优势分数，其中分数越高表明与专家偏好的一致性越好。通过在 RPR 和基于文本的梯度优化之间交替，PROF 完全自动化地选择和细化下游策略学习的最佳奖励函数。 D4RL 的实证结果表明，PROF 超越或匹配了众多数据集和领域的最新强基线，凸显了我们方法的有效性。</li>
</ul>

<h3>Title: Exploring Transferability of Self-Supervised Learning by Task Conflict Calibration</h3>
<ul>
<li><strong>Authors: </strong>Huijie Guo, Jingyao Wang, Peizheng Guo, Xingchen Shen, Changwen Zheng, Wenwen Qiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13787">https://arxiv.org/abs/2511.13787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13787">https://arxiv.org/pdf/2511.13787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13787]] Exploring Transferability of Self-Supervised Learning by Task Conflict Calibration(https://arxiv.org/abs/2511.13787)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this paper, we explore the transferability of SSL by addressing two central questions: (i) what is the representation transferability of SSL, and (ii) how can we effectively model this transferability? Transferability is defined as the ability of a representation learned from one task to support the objective of another. Inspired by the meta-learning paradigm, we construct multiple SSL tasks within each training batch to support explicitly modeling transferability. Based on empirical evidence and causal analysis, we find that although introducing task-level information improves transferability, it is still hindered by task conflict. To address this issue, we propose a Task Conflict Calibration (TC$^2$) method to alleviate the impact of task conflict. Specifically, it first splits batches to create multiple SSL tasks, infusing task-level information. Next, it uses a factor extraction network to produce causal generative factors for all tasks and a weight extraction network to assign dedicated weights to each sample, employing data reconstruction, orthogonality, and sparsity to ensure effectiveness. Finally, TC$^2$ calibrates sample representations during SSL training and integrates into the pipeline via a two-stage bi-level optimization framework to boost the transferability of learned representations. Experimental results on multiple downstream tasks demonstrate that our method consistently improves the transferability of SSL models.</li>
<li><strong>摘要：</strong>在本文中，我们通过解决两个核心问题来探索 SSL 的可转移性：(i) SSL 的表示可转移性是什么，以及 (ii) 我们如何有效地建模这种可转移性？可迁移性被定义为从一项任务中学习到的表示支持另一项任务目标的能力。受元学习范式的启发，我们在每个训练批次中构建多个 SSL 任务，以支持显式建模可迁移性。基于经验证据和因果分析，我们发现虽然引入任务级信息提高了可迁移性，但仍然受到任务冲突的阻碍。为了解决这个问题，我们提出了一种任务冲突校准（TC$^2$）方法来减轻任务冲突的影响。具体来说，它首先拆分批次以创建多个 SSL 任务，并注入任务级信息。接下来，它使用因子提取网络为所有任务生成因果生成因子，并使用权重提取网络为每个样本分配专用权重，并采用数据重建、正交性和稀疏性来确保有效性。最后，TC$^2$ 在 SSL 训练期间校准样本表示，并通过两阶段双层优化框架集成到管道中，以提高学习表示的可迁移性。多个下游任务的实验结果表明，我们的方法持续提高了 SSL 模型的可转移性。</li>
</ul>

<h3>Title: FusionFM: All-in-One Multi-Modal Image Fusion with Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Huayi Zhu, Xiu Shu, Youqiang Xiong, Qiao Liu, Rui Chen, Di Yuan, Xiaojun Chang, Zhenyu He</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13794">https://arxiv.org/abs/2511.13794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13794">https://arxiv.org/pdf/2511.13794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13794]] FusionFM: All-in-One Multi-Modal Image Fusion with Flow Matching(https://arxiv.org/abs/2511.13794)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Current multi-modal image fusion methods typically rely on task-specific models, leading to high training costs and limited scalability. While generative methods provide a unified modeling perspective, they often suffer from slow inference due to the complex sampling trajectories from noise to image. To address this, we formulate image fusion as a direct probabilistic transport from source modalities to the fused image distribution, leveraging the flow matching paradigm to improve sampling efficiency and structural consistency. To mitigate the lack of high-quality fused images for supervision, we collect fusion results from multiple state-of-the-art models as priors, and employ a task-aware selection function to select the most reliable pseudo-labels for each task. We further introduce a Fusion Refiner module that employs a divide-and-conquer strategy to systematically identify, decompose, and enhance degraded components in selected pseudo-labels. For multi-task scenarios, we integrate elastic weight consolidation and experience replay mechanisms to preserve cross-task performance and enhance continual learning ability from both parameter stability and memory retention perspectives. Our approach achieves competitive performance across diverse fusion tasks, while significantly improving sampling efficiency and maintaining a lightweight model design. The code will be available at: this https URL.</li>
<li><strong>摘要：</strong>当前的多模态图像融合方法通常依赖于特定于任务的模型，导致训练成本高且可扩展性有限。虽然生成方法提供了统一的建模视角，但由于从噪声到图像的复杂采样轨迹，它们常常会出现推理缓慢的问题。为了解决这个问题，我们将图像融合制定为从源模态到融合图像分布的直接概率传输，利用流匹配范例来提高采样效率和结构一致性。为了缓解缺乏用于监督的高质量融合图像的问题，我们从多个最先进的模型中收集融合结果作为先验，并采用任务感知选择函数为每个任务选择最可靠的伪标签。我们进一步介绍了 Fusion Refiner 模块，该模块采用分而治之的策略来系统地识别、分解和增强所选伪标签中的降级组件。对于多任务场景，我们集成了弹性权重合并和经验回放机制，以保持跨任务性能，并从参数稳定性和记忆保留的角度增强持续学习能力。我们的方法在不同的融合任务中实现了具有竞争力的性能，同时显着提高了采样效率并保持了轻量级模型设计。该代码可在以下位置获取：此 https URL。</li>
</ul>

<h3>Title: A Trajectory-free Crash Detection Framework with Generative Approach and Segment Map Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Weiying Shen, Hao Yu, Yu Dong, Pan Liu, Yu Han, Xin Wen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13795">https://arxiv.org/abs/2511.13795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13795">https://arxiv.org/pdf/2511.13795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13795]] A Trajectory-free Crash Detection Framework with Generative Approach and Segment Map Diffusion(https://arxiv.org/abs/2511.13795)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Real-time crash detection is essential for developing proactive safety management strategy and enhancing overall traffic efficiency. To address the limitations associated with trajectory acquisition and vehicle tracking, road segment maps recording the individual-level traffic dynamic data were directly served in crash detection. A novel two-stage trajectory-free crash detection framework, was present to generate the rational future road segment map and identify crashes. The first-stage diffusion-based segment map generation model, Mapfusion, conducts a noisy-to-normal process that progressively adds noise to the road segment map until the map is corrupted to pure Gaussian noise. The denoising process is guided by sequential embedding components capturing the temporal dynamics of segment map sequences. Furthermore, the generation model is designed to incorporate background context through ControlNet to enhance generation control. Crash detection is achieved by comparing the monitored segment map with the generations from diffusion model in second stage. Trained on non-crash vehicle motion data, Mapfusion successfully generates realistic road segment evolution maps based on learned motion patterns and remains robust across different sampling intervals. Experiments on real-world crashes indicate the effectiveness of the proposed two-stage method in accurately detecting crashes.</li>
<li><strong>摘要：</strong>实时碰撞检测对于制定主动安全管理策略和提高整体交通效率至关重要。为了解决与轨迹采集和车辆跟踪相关的限制，记录个人级交通动态数据的路段地图直接用于碰撞检测。提出了一种新颖的两阶段无轨迹碰撞检测框架，用于生成合理的未来路段地图并识别碰撞。第一阶段基于扩散的路段地图生成模型 Mapfusion 执行从噪声到正常的过程，逐渐向道路路段地图添加噪声，直到地图损坏为纯高斯噪声。去噪过程由捕获片段图序列的时间动态的顺序嵌入组件引导。此外，生成模型旨在通过 ControlNet 合并背景上下文，以增强生成控制。碰撞检测是通过将监控的分段图与第二阶段的扩散模型的生成进行比较来实现的。经过非碰撞车辆运动数据的训练，Mapfusion 根据学习的运动模式成功生成了真实的路段演化地图，并在不同的采样间隔内保持稳健。对现实世界碰撞的实验表明了所提出的两阶段方法在准确检测碰撞方面的有效性。</li>
</ul>

<h3>Title: Beat the long tail: Distribution-Aware Speculative Decoding for RL Training</h3>
<ul>
<li><strong>Authors: </strong>Zelei Shao, Vikranth Srivatsa, Sanjana Srivastava, Qingyang Wu, Alpay Ariyak, Xiaoxia Wu, Ameen Patel, Jue Wang, Percy Liang, Tri Dao, Ce Zhang, Yiying Zhang, Ben Athiwaratkun, Chenfeng Xu, Junxiong Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13841">https://arxiv.org/abs/2511.13841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13841">https://arxiv.org/pdf/2511.13841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13841]] Beat the long tail: Distribution-Aware Speculative Decoding for RL Training(https://arxiv.org/abs/2511.13841)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reinforcement learning(RL) post-training has become essential for aligning large language models (LLMs), yet its efficiency is increasingly constrained by the rollout phase, where long trajectories are generated token by token. We identify a major bottleneck:the long-tail distribution of rollout lengths, where a small fraction of long generations dominates wall clock time and a complementary opportunity; the availability of historical rollouts that reveal stable prompt level patterns across training epochs. Motivated by these observations, we propose DAS, a Distribution Aware Speculative decoding framework that accelerates RL rollouts without altering model outputs. DAS integrates two key ideas: an adaptive, nonparametric drafter built from recent rollouts using an incrementally maintained suffix tree, and a length aware speculation policy that allocates more aggressive draft budgets to long trajectories that dominate makespan. This design exploits rollout history to sustain acceptance while balancing base and token level costs during decoding. Experiments on math and code reasoning tasks show that DAS reduces rollout time up to 50% while preserving identical training curves, demonstrating that distribution-aware speculative decoding can significantly accelerate RL post training without compromising learning quality.</li>
<li><strong>摘要：</strong>强化学习 (RL) 后训练对于调整大型语言模型 (LLM) 至关重要，但其效率越来越受到部署阶段的限制，在部署阶段，长轨迹是逐个令牌生成的。我们确定了一个主要瓶颈：推出长度的长尾分布，其中长代的一小部分主导了挂钟时间和补充机会；历史推出的可用性揭示了整个训练时期稳定的提示级别模式。受这些观察的启发，我们提出了 DAS，一种分布感知推测解码框架，可以在不改变模型输出的情况下加速 RL 的推出。 DAS 集成了两个关键思想：使用增量维护的后缀树从最近推出的版本中构建的自适应非参数绘图器，以及长度感知推测策略，该策略将更积极的草案预算分配给主导完工时间的长轨迹。该设计利用推出历史来维持接受度，同时在解码过程中平衡基本成本和令牌级别成本。数学和代码推理任务的实验表明，DAS 在保持相同训练曲线的同时将推出时间缩短了 50%，这表明分布感知推测解码可以显着加速 RL 后期训练，而不会影响学习质量。</li>
</ul>

<h3>Title: Can World Simulators Reason? Gen-ViRe: A Generative Visual Reasoning Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Xinxin Liu, Zhaopan Xu, Kai Wang, Yong Jae Lee, Yuzhang Shang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13853">https://arxiv.org/abs/2511.13853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13853">https://arxiv.org/pdf/2511.13853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13853]] Can World Simulators Reason? Gen-ViRe: A Generative Visual Reasoning Benchmark(https://arxiv.org/abs/2511.13853)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>While Chain-of-Thought (CoT) prompting enables sophisticated symbolic reasoning in LLMs, it remains confined to discrete text and cannot simulate the continuous, physics-governed dynamics of the real world. Recent video generation models have emerged as potential world simulators through Chain-of-Frames (CoF) reasoning -- materializing thought as frame-by-frame visual sequences, with each frame representing a physically-grounded reasoning step. Despite compelling demonstrations, a challenge persists: existing benchmarks, focusing on fidelity or alignment, do not assess CoF reasoning and thus cannot measure core cognitive abilities in multi-step planning, algorithmic logic, or abstract pattern extrapolation. This evaluation void prevents systematic understanding of model capabilities and principled guidance for improvement. We introduce Gen-ViRe (Generative Visual Reasoning Benchmark), a framework grounded in cognitive science and real-world AI applications, which decomposes CoF reasoning into six cognitive dimensions -- from perceptual logic to abstract planning -- and 24 subtasks. Through multi-source data curation, minimal prompting protocols, and hybrid VLM-assisted evaluation with detailed criteria, Gen-ViRe delivers the first quantitative assessment of video models as reasoners. Our experiments on SOTA systems reveal substantial discrepancies between impressive visual quality and actual reasoning depth, establishing baselines and diagnostic tools to advance genuine world simulators.</li>
<li><strong>摘要：</strong>虽然思想链 (CoT) 提示可以在法学硕士中实现复杂的符号推理，但它仍然局限于离散文本，无法模拟现实世界的连续、物理控制的动态。最近的视频生成模型已经通过帧链（CoF）推理成为潜在的世界模拟器——将思想具体化为逐帧视觉序列，每一帧代表一个基于物理的推理步骤。尽管有令人信服的演示，但挑战仍然存在：现有的基准侧重于保真度或对齐，不评估 CoF 推理，因此无法衡量多步骤规划、算法逻辑或抽象模式外推中的核心认知能力。这种评估空白阻碍了对模型功能的系统理解和改进的原则指导。我们介绍 Gen-ViRe（生成视觉推理基准），这是一个基于认知科学和现实世界人工智能应用的框架，它将 CoF 推理分解为六个认知维度（从感知逻辑到抽象规划）和 24 个子任务。通过多源数据管理、最少的提示协议以及具有详细标准的混合 VLM 辅助评估，Gen-ViRe 首次对作为推理器的视频模型进行定量评估。我们在 SOTA 系统上的实验揭示了令人印象深刻的视觉质量和实际推理深度之间的巨大差异，建立了基线和诊断工具来推进真实的世界模拟器。</li>
</ul>

<h3>Title: Hybrid Convolution Neural Network Integrated with Pseudo-Newton Boosting for Lumbar Spine Degeneration Detection</h3>
<ul>
<li><strong>Authors: </strong>Pandiyaraju V, Abishek Karthik, Jaspin K, Kannan A, Jaime Lloret</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13877">https://arxiv.org/abs/2511.13877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13877">https://arxiv.org/pdf/2511.13877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13877]] Hybrid Convolution Neural Network Integrated with Pseudo-Newton Boosting for Lumbar Spine Degeneration Detection(https://arxiv.org/abs/2511.13877)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This paper proposes a new enhanced model architecture to perform classification of lumbar spine degeneration with DICOM images while using a hybrid approach, integrating EfficientNet and VGG19 together with custom-designed components. The proposed model is differentiated from traditional transfer learning methods as it incorporates a Pseudo-Newton Boosting layer along with a Sparsity-Induced Feature Reduction Layer that forms a multi-tiered framework, further improving feature selection and representation. The Pseudo-Newton Boosting layer makes smart variations of feature weights, with more detailed anatomical features, which are mostly left out in a transfer learning setup. In addition, the Sparsity-Induced Layer removes redundancy for learned features, producing lean yet robust representations for pathology in the lumbar spine. This architecture is novel as it overcomes the constraints in the traditional transfer learning approach, especially in the high-dimensional context of medical images, and achieves a significant performance boost, reaching a precision of 0.9, recall of 0.861, F1 score of 0.88, loss of 0.18, and an accuracy of 88.1%, compared to the baseline model, EfficientNet. This work will present the architectures, preprocessing pipeline, and experimental results. The results contribute to the development of automated diagnostic tools for medical images.</li>
<li><strong>摘要：</strong>本文提出了一种新的增强模型架构，使用 DICOM 图像对腰椎退变进行分类，同时使用混合方法，将 EfficientNet 和 VGG19 与定制设计的组件集成在一起。所提出的模型与传统的迁移学习方法不同，因为它结合了伪牛顿增强层和稀疏诱导特征缩减层，形成了多层框架，进一步改进了特征选择和表示。伪牛顿增强层使特征权重发生智能变化，并具有更详细的解剖特征，而这些特征在迁移学习设置中大多被忽略。此外，稀疏诱导层消除了学习特征的冗余，为腰椎病理产生精简而稳健的表示。这种架构很新颖，因为它克服了传统迁移学习方法的限制，特别是在医学图像的高维背景下，并且与基线模型 EfficientNet 相比，实现了显着的性能提升，达到了 0.9 的精度、0.861 的召回率、0.88 的 F1 分数、0.18 的损失和 88.1% 的准确率。这项工作将展示架构、预处理管道和实验结果。研究结果有助于医学图像自动化诊断工具的开发。</li>
</ul>

<h3>Title: Tractable Probabilistic Models for Investment Planning</h3>
<ul>
<li><strong>Authors: </strong>Nicolas M. Cuadrado A., Mohannad Takrouri, Jiří Němeček, Martin Takáč, Jakub Mareček</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13888">https://arxiv.org/abs/2511.13888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13888">https://arxiv.org/pdf/2511.13888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13888]] Tractable Probabilistic Models for Investment Planning(https://arxiv.org/abs/2511.13888)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Investment planning in power utilities, such as generation and transmission expansion, requires decade-long forecasts under profound uncertainty. Forecasting of energy mix and energy use decades ahead is nontrivial. Classical approaches focus on generating a finite number of scenarios (modeled as a mixture of Diracs in statistical theory terms), which limits insight into scenario-specific volatility and hinders robust decision-making. We propose an alternative using tractable probabilistic models (TPMs), particularly sum-product networks (SPNs). These models enable exact, scalable inference of key quantities such as scenario likelihoods, marginals, and conditional probabilities, supporting robust scenario expansion and risk assessment. This framework enables direct embedding of chance-constrained optimization into investment planning, enforcing safety or reliability with prescribed confidence levels. TPMs allow both scenario analysis and volatility quantification by compactly representing high-dimensional uncertainties. We demonstrate the approach's effectiveness through a representative power system planning case study, illustrating computational and reliability advantages over traditional scenario-based models.</li>
<li><strong>摘要：</strong>电力公用事业的投资规划，例如发电和输电扩建，需要在高度不确定的情况下进行长达十年的预测。预测未来几十年的能源结构和能源使用并非易事。经典方法侧重于生成有限数量的情景（用统计理论术语建模为狄拉克的混合体），这限制了对特定情景波动性的洞察，并阻碍了稳健的决策。我们提出了一种使用易处理概率模型（TPM）的替代方案，特别是和积网络（SPN）。这些模型能够对场景可能性、边际值和条件概率等关键量进行精确、可扩展的推断，支持稳健的场景扩展和风险评估。该框架可以将机会约束优化直接嵌入到投资规划中，以规定的置信水平增强安全性或可靠性。 TPM 通过紧凑地表示高维不确定性来实现情景分析和波动性量化。我们通过代表性电力系统规划案例研究证明了该方法的有效性，说明了相对于传统基于场景的模型的计算和可靠性优势。</li>
</ul>

<h3>Title: Temporal Realism Evaluation of Generated Videos Using Compressed-Domain Motion Vectors</h3>
<ul>
<li><strong>Authors: </strong>Mert Onur Cakiroglu, Idil Bilge Altun, Zhihe Lu, Mehmet Dalkilic, Hasan Kurban</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13897">https://arxiv.org/abs/2511.13897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13897">https://arxiv.org/pdf/2511.13897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13897]] Temporal Realism Evaluation of Generated Videos Using Compressed-Domain Motion Vectors(https://arxiv.org/abs/2511.13897)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Temporal realism remains a central weakness of current generative video models, as most evaluation metrics prioritize spatial appearance and offer limited sensitivity to motion. We introduce a scalable, model-agnostic framework that assesses temporal behavior using motion vectors (MVs) extracted directly from compressed video streams. Codec-generated MVs from standards such as H.264 and HEVC provide lightweight, resolution-consistent descriptors of motion dynamics. We quantify realism by computing Kullback-Leibler, Jensen-Shannon, and Wasserstein divergences between MV statistics of real and generated videos. Experiments on the GenVidBench dataset containing videos from eight state-of-the-art generators reveal systematic discrepancies from real motion: entropy-based divergences rank Pika and SVD as closest to real videos, MV-sum statistics favor VC2 and Text2Video-Zero, and CogVideo shows the largest deviations across both measures. Visualizations of MV fields and class-conditional motion heatmaps further reveal center bias, sparse and piecewise constant flows, and grid-like artifacts that frame-level metrics do not capture. Beyond evaluation, we investigate MV-RGB fusion through channel concatenation, cross-attention, joint embedding, and a motion-aware fusion module. Incorporating MVs improves downstream classification across ResNet, I3D, and TSN backbones, with ResNet-18 and ResNet-34 reaching up to 97.4% accuracy and I3D achieving 99.0% accuracy on real-versus-generated discrimination. These findings demonstrate that compressed-domain MVs provide an effective temporal signal for diagnosing motion defects in generative videos and for strengthening temporal reasoning in discriminative models. The implementation is available at: this https URL</li>
<li><strong>摘要：</strong>时间真实性仍然是当前生成视频模型的主要弱点，因为大多数评估指标优先考虑空间外观，并且对运动的敏感度有限。我们引入了一个可扩展的、与模型无关的框架，该框架使用直接从压缩视频流中提取的运动向量（MV）来评估时间行为。编解码器根据 H.264 和 HEVC 等标准生成的 MV 提供轻量级、分辨率一致的运动动态描述符。我们通过计算真实视频和生成视频的 MV 统计数据之间的 Kullback-Leibler、Jensen-Shannon 和 Wasserstein 差异来量化真实性。在包含来自八个最先进生成器的视频的 GenVidBench 数据集上进行的实验揭示了与真实运动的系统差异：基于熵的差异将 Pika 和 SVD 评为最接近真实视频，MV-sum 统计数据有利于 VC2 和 Text2Video-Zero，CogVideo 显示了这两种度量的最大偏差。 MV 场和类条件运动热图的可视化进一步揭示了中心偏差、稀疏和分段恒定流以及帧级度量无法捕获的网格状伪影。除了评估之外，我们还通过通道串联、交叉注意力、联合嵌入和运动感知融合模块来研究 MV-RGB 融合。结合 MV 改进了 ResNet、I3D 和 TSN 主干网的下游分类，ResNet-18 和 ResNet-34 的准确率高达 97.4%，I3D 在真实与生成的区分方面达到 99.0% 的准确率。这些发现表明，压缩域 MV 为诊断生成视频中的运动缺陷和加强判别模型中的时间推理提供了有效的时间信号。实现可在以下位置找到：此 https URL</li>
</ul>

<h3>Title: CD-DPE: Dual-Prompt Expert Network based on Convolutional Dictionary Feature Decoupling for Multi-Contrast MRI Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Xianming Gu, Lihui Wang, Ying Cao, Zeyu Deng, Yingfeng Ou, Guodong Hu, Yi Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14014">https://arxiv.org/abs/2511.14014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14014">https://arxiv.org/pdf/2511.14014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14014]] CD-DPE: Dual-Prompt Expert Network based on Convolutional Dictionary Feature Decoupling for Multi-Contrast MRI Super-Resolution(https://arxiv.org/abs/2511.14014)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Multi-contrast magnetic resonance imaging (MRI) super-resolution intends to reconstruct high-resolution (HR) images from low-resolution (LR) scans by leveraging structural information present in HR reference images acquired with different contrasts. This technique enhances anatomical detail and soft tissue differentiation, which is vital for early diagnosis and clinical decision-making. However, inherent contrasts disparities between modalities pose fundamental challenges in effectively utilizing reference image textures to guide target image reconstruction, often resulting in suboptimal feature integration. To address this issue, we propose a dual-prompt expert network based on a convolutional dictionary feature decoupling (CD-DPE) strategy for multi-contrast MRI super-resolution. Specifically, we introduce an iterative convolutional dictionary feature decoupling module (CD-FDM) to separate features into cross-contrast and intra-contrast components, thereby reducing redundancy and interference. To fully integrate these features, a novel dual-prompt feature fusion expert module (DP-FFEM) is proposed. This module uses a frequency prompt to guide the selection of relevant reference features for incorporation into the target image, while an adaptive routing prompt determines the optimal method for fusing reference and target features to enhance reconstruction quality. Extensive experiments on public multi-contrast MRI datasets demonstrate that CD-DPE outperforms state-of-the-art methods in reconstructing fine details. Additionally, experiments on unseen datasets demonstrated that CD-DPE exhibits strong generalization capabilities.</li>
<li><strong>摘要：</strong>多对比磁共振成像 (MRI) 超分辨率旨在通过利用不同对比度获取的 HR 参考图像中存在的结构信息，从低分辨率 (LR) 扫描重建高分辨率 (HR) 图像。该技术增强了解剖细节和软组织分化，这对于早期诊断和临床决策至关重要。然而，模态之间固有的对比度差异对有效利用参考图像纹理来指导目标图像重建提出了根本性挑战，通常导致次优的特征集成。为了解决这个问题，我们提出了一种基于卷积字典特征解耦（CD-DPE）策略的双提示专家网络，用于多对比 MRI 超分辨率。具体来说，我们引入迭代卷积字典特征解耦模块（CD-FDM）将特征分离为交叉对比和内部对比分量，从而减少冗余和干扰。为了充分集成这些特征，提出了一种新颖的双提示特征融合专家模块（DP-FFEM）。该模块使用频率提示来指导选择相关参考特征以合并到目标图像中，而自适应路由提示则确定融合参考特征和目标特征以提高重建质量的最佳方法。对公共多对比 MRI 数据集的大量实验表明，CD-DPE 在重建精细细节方面优于最先进的方法。此外，对未见过的数据集的实验表明 CD-DPE 表现出强大的泛化能力。</li>
</ul>

<h3>Title: Training-free Detection of AI-generated images via Cropping Robustness</h3>
<ul>
<li><strong>Authors: </strong>Sungik Choi, Hankook Lee, Moontae Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14030">https://arxiv.org/abs/2511.14030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14030">https://arxiv.org/pdf/2511.14030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14030]] Training-free Detection of AI-generated images via Cropping Robustness(https://arxiv.org/abs/2511.14030)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>AI-generated image detection has become crucial with the rapid advancement of vision-generative models. Instead of training detectors tailored to specific datasets, we study a training-free approach leveraging self-supervised models without requiring prior data knowledge. These models, pre-trained with augmentations like RandomResizedCrop, learn to produce consistent representations across varying resolutions. Motivated by this, we propose WaRPAD, a training-free AI-generated image detection algorithm based on self-supervised models. Since neighborhood pixel differences in images are highly sensitive to resizing operations, WaRPAD first defines a base score function that quantifies the sensitivity of image embeddings to perturbations along high-frequency directions extracted via Haar wavelet decomposition. To simulate robustness against cropping augmentation, we rescale each image to a multiple of the models input size, divide it into smaller patches, and compute the base score for each patch. The final detection score is then obtained by averaging the scores across all patches. We validate WaRPAD on real datasets of diverse resolutions and domains, and images generated by 23 different generative models. Our method consistently achieves competitive performance and demonstrates strong robustness to test-time corruptions. Furthermore, as invariance to RandomResizedCrop is a common training scheme across self-supervised models, we show that WaRPAD is applicable across self-supervised models.</li>
<li><strong>摘要：</strong>随着视觉生成模型的快速发展，人工智能生成的图像检测变得至关重要。我们不是训练针对特定数据集的检测器，而是研究一种利用自我监督模型的免训练方法，无需先验数据知识。这些模型经过 RandomResizedCrop 等增强功能的预训练，能够学习在不同分辨率下生成一致的表示。受此启发，我们提出了 WaRPAD，一种基于自监督模型的免训练人工智能生成图像检测算法。由于图像中的邻域像素差异对调整大小操作高度敏感，因此 WaRPAD 首先定义了一个基本评分函数，该函数量化图像嵌入对通过 Haar 小波分解提取的高频方向扰动的敏感性。为了模拟针对裁剪增强的鲁棒性，我们将每个图像重新缩放为模型输入大小的倍数，将其划分为更小的补丁，并计算每个补丁的基本分数。然后通过对所有补丁的分数进行平均来获得最终的检测分数。我们在不同分辨率和域的真实数据集以及 23 个不同生成模型生成的图像上验证了 WaRPAD。我们的方法始终如一地实现了具有竞争力的性能，并对测试时损坏表现出强大的鲁棒性。此外，由于 RandomResizedCrop 的不变性是自监督模型中的常见训练方案，因此我们表明 WaRPAD 适用于自监督模型。</li>
</ul>

<h3>Title: FashionMAC: Deformation-Free Fashion Image Generation with Fine-Grained Model Appearance Customization</h3>
<ul>
<li><strong>Authors: </strong>Rong Zhang, Jinxiao Li, Jingnan Wang, Zhiwen Zuo, Jianfeng Dong, Wei Li, Chi Wang, Weiwei Xu, Xun Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14031">https://arxiv.org/abs/2511.14031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14031">https://arxiv.org/pdf/2511.14031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14031]] FashionMAC: Deformation-Free Fashion Image Generation with Fine-Grained Model Appearance Customization(https://arxiv.org/abs/2511.14031)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Garment-centric fashion image generation aims to synthesize realistic and controllable human models dressing a given garment, which has attracted growing interest due to its practical applications in e-commerce. The key challenges of the task lie in two aspects: (1) faithfully preserving the garment details, and (2) gaining fine-grained controllability over the model's appearance. Existing methods typically require performing garment deformation in the generation process, which often leads to garment texture distortions. Also, they fail to control the fine-grained attributes of the generated models, due to the lack of specifically designed mechanisms. To address these issues, we propose FashionMAC, a novel diffusion-based deformation-free framework that achieves high-quality and controllable fashion showcase image generation. The core idea of our framework is to eliminate the need for performing garment deformation and directly outpaint the garment segmented from a dressed person, which enables faithful preservation of the intricate garment details. Moreover, we propose a novel region-adaptive decoupled attention (RADA) mechanism along with a chained mask injection strategy to achieve fine-grained appearance controllability over the synthesized human models. Specifically, RADA adaptively predicts the generated regions for each fine-grained text attribute and enforces the text attribute to focus on the predicted regions by a chained mask injection strategy, significantly enhancing the visual fidelity and the controllability. Extensive experiments validate the superior performance of our framework compared to existing state-of-the-art methods.</li>
<li><strong>摘要：</strong>以服装为中心的时尚图像生成旨在合成穿着给定服装的逼真且可控的人体模型，由于其在电子商务中的实际应用而引起了越来越多的兴趣。该任务的关键挑战在于两个方面：（1）忠实保留服装细节，（2）获得对模型外观的细粒度可控性。现有方法通常需要在生成过程中进行服装变形，这通常会导致服装纹理扭曲。此外，由于缺乏专门设计的机制，它们无法控制生成模型的细粒度属性。为了解决这些问题，我们提出了FashionMAC，一种新颖的基于扩散的无变形框架，可以实现高质量且可控的时尚展示图像生成。我们框架的核心思想是消除进行服装变形的需要，直接对从穿着者身上分割出来的服装进行涂色，从而能够忠实地保存复杂的服装细节。此外，我们提出了一种新颖的区域自适应解耦注意（RADA）机制以及链式掩模注入策略，以实现对合成人体模型的细粒度外观可控性。具体来说，RADA自适应地预测每个细粒度文本属性的生成区域，并通过链式掩模注入策略强制文本属性集中在预测区域，显着增强了视觉保真度和可控性。大量的实验验证了我们的框架与现有最先进方法相比的优越性能。</li>
</ul>

<h3>Title: Flood-LDM: Generalizable Latent Diffusion Models for rapid and accurate zero-shot High-Resolution Flood Mapping</h3>
<ul>
<li><strong>Authors: </strong>Sun Han Neo, Sachith Seneviratne, Herath Mudiyanselage Viraj Vidura Herath, Abhishek Saha, Sanka Rasnayaka, Lucy Amanda Marshall</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14033">https://arxiv.org/abs/2511.14033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14033">https://arxiv.org/pdf/2511.14033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14033]] Flood-LDM: Generalizable Latent Diffusion Models for rapid and accurate zero-shot High-Resolution Flood Mapping(https://arxiv.org/abs/2511.14033)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Flood prediction is critical for emergency planning and response to mitigate human and economic losses. Traditional physics-based hydrodynamic models generate high-resolution flood maps using numerical methods requiring fine-grid discretization; which are computationally intensive and impractical for real-time large-scale applications. While recent studies have applied convolutional neural networks for flood map super-resolution with good accuracy and speed, they suffer from limited generalizability to unseen areas. In this paper, we propose a novel approach that leverages latent diffusion models to perform super-resolution on coarse-grid flood maps, with the objective of achieving the accuracy of fine-grid flood maps while significantly reducing inference time. Experimental results demonstrate that latent diffusion models substantially decrease the computational time required to produce high-fidelity flood maps without compromising on accuracy, enabling their use in real-time flood risk management. Moreover, diffusion models exhibit superior generalizability across different physical locations, with transfer learning further accelerating adaptation to new geographic regions. Our approach also incorporates physics-informed inputs, addressing the common limitation of black-box behavior in machine learning, thereby enhancing interpretability. Code is available at this https URL.</li>
<li><strong>摘要：</strong>洪水预测对于应急计划和响应以减轻人员和经济损失至关重要。传统的基于物理的水动力模型使用需要精细网格离散化的数值方法来生成高分辨率洪水图；这对于实时大规模应用来说是计算密集型且不切实际的。虽然最近的研究将卷积神经网络应用于洪水地图超分辨率，具有良好的准确性和速度，但它们对未见区域的泛化能力有限。在本文中，我们提出了一种利用潜在扩散模型对粗网格洪水图执行超分辨率的新方法，其目标是实现细网格洪水图的准确性，同时显着减少推理时间。实验结果表明，潜在扩散模型大大减少了生成高保真洪水地图所需的计算时间，而不会影响准确性，从而使其能够用于实时洪水风险管理。此外，扩散模型在不同物理位置上表现出卓越的通用性，迁移学习进一步加速了对新地理区域的适应。我们的方法还结合了物理信息输入，解决了机器学习中黑盒行为的常见限制，从而增强了可解释性。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Radial Compensation: Stable and Semantically Decoupled Generative Models on Riemannian Manifolds</h3>
<ul>
<li><strong>Authors: </strong>Marios Papamichals, Regina Ruane</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IT, math.DG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14056">https://arxiv.org/abs/2511.14056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14056">https://arxiv.org/pdf/2511.14056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14056]] Radial Compensation: Stable and Semantically Decoupled Generative Models on Riemannian Manifolds(https://arxiv.org/abs/2511.14056)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models on curved spaces rely on charts to map Euclidean spaces to manifolds. Exponential maps preserve geodesics but have stiff, radius-dependent Jacobians, while volume-preserving charts maintain densities but distort geodesic distances. Both approaches entangle curvature with model parameters, inflating gradient variance. In high-dimensional latent normalizing flows, the wrapped exponential prior can stretch radii far beyond the curvature scale, leading to poor test likelihoods and stiff solvers. We introduce Radial Compensation (RC), an information-geometric method that selects the base density in the tangent space so that the likelihood depends only on geodesic distance from a pole, decoupling parameter semantics from curvature. RC lets radial parameters retain their usual meaning in geodesic units, while the chart can be tuned as a numerical preconditioner. We extend RC to manifolds with known geodesic polar volume and show that RC is the only construction for geodesic-radial likelihoods with curvature-invariant Fisher information. We derive the Balanced-Exponential (bExp) chart family, balancing volume distortion and geodesic error. Under RC, all bExp settings preserve the same manifold density and Fisher information, with smaller dial values reducing gradient variance and flow cost. Empirically, RC yields stable generative models across densities, VAEs, flows on images and graphs, and protein models. RC improves likelihoods, restores clean geodesic radii, and prevents radius blow-ups in high-dimensional flows, making RC-bExp a robust default for likelihood-trained generative models on manifolds.</li>
<li><strong>摘要：</strong>弯曲空间上的生成模型依赖于图表将欧几里得空间映射到流形。指数地图保留测地线，但具有僵硬的、依赖于半径的雅可比行列式，而体积保留图保留密度，但扭曲测地距离。两种方法都将曲率与模型参数纠缠在一起，导致梯度方差膨胀。在高维潜在归一化流中，包裹指数先验可以将半径拉伸到远远超出曲率尺度，从而导致测试可能性较差和求解器僵硬。我们引入了径向补偿（RC），这是一种信息几何方法，它选择切线空间中的基本密度，以便可能性仅取决于距极点的测地距离，从而将参数语义与曲率解耦。 RC 让径向参数保留其在测地线单位中的通常含义，而图表可以作为数值预处理器进行调整。我们将 RC 扩展到具有已知测地线极体积的流形，并表明 RC 是具有曲率不变费舍尔信息的测地线径向似然的唯一构造。我们推导出平衡指数 (bExp) 图表系列，平衡体积失真和测地误差。在 RC 下，所有 bExp 设置保留相同的流形密度和 Fisher 信息，较小的刻度盘值可减少梯度方差和流量成本。根据经验，RC 在密度、VAE、图像和图表流以及蛋白质模型方面产生稳定的生成模型。 RC 提高了似然性，恢复了干净的测地线半径，并防止高维流中的半径爆炸，使 RC-bExp 成为流形上似然性训练生成模型的稳健默认值。</li>
</ul>

<h3>Title: Semantic Context Matters: Improving Conditioning for Autoregressive Models</h3>
<ul>
<li><strong>Authors: </strong>Dongyang Jin, Ryan Xu, Jianhao Zeng, Rui Lan, Yancheng Bai, Lei Sun, Xiangxiang Chu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14063">https://arxiv.org/abs/2511.14063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14063">https://arxiv.org/pdf/2511.14063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14063]] Semantic Context Matters: Improving Conditioning for Autoregressive Models(https://arxiv.org/abs/2511.14063)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recently, autoregressive (AR) models have shown strong potential in image generation, offering better scalability and easier integration with unified multi-modal systems compared to diffusion-based methods. However, extending AR models to general image editing remains challenging due to weak and inefficient conditioning, often leading to poor instruction adherence and visual artifacts. To address this, we propose SCAR, a Semantic-Context-driven method for Autoregressive models. SCAR introduces two key components: Compressed Semantic Prefilling, which encodes high-level semantics into a compact and efficient prefix, and Semantic Alignment Guidance, which aligns the last visual hidden states with target semantics during autoregressive decoding to enhance instruction fidelity. Unlike decoding-stage injection methods, SCAR builds upon the flexibility and generality of vector-quantized-based prefilling while overcoming its semantic limitations and high cost. It generalizes across both next-token and next-set AR paradigms with minimal architectural changes. SCAR achieves superior visual fidelity and semantic alignment on both instruction editing and controllable generation benchmarks, outperforming prior AR-based methods while maintaining controllability. All code will be released.</li>
<li><strong>摘要：</strong>最近，自回归（AR）模型在图像生成方面显示出强大的潜力，与基于扩散的方法相比，它提供了更好的可扩展性，并且更容易与统一的多模态系统集成。然而，由于调节能力薄弱且效率低下，将 AR 模型扩展到一般图像编辑仍然具有挑战性，通常会导致指令依从性差和视觉伪影。为了解决这个问题，我们提出了 SCAR，一种用于自回归模型的语义上下文驱动方法。 SCAR 引入了两个关键组件：压缩语义预填充（将高级语义编码为紧凑且高效的前缀）和语义对齐指导（在自回归解码期间将最后的视觉隐藏状态与目标语义对齐，以增强指令保真度）。与解码阶段注入方法不同，SCAR 建立在基于矢量量化的预填充的灵活性和通用性之上，同时克服了其语义限制和高成本。它以最小的架构变化概括了下一代代币和下一代 AR 范式。 SCAR 在指令编辑和可控生成基准上实现了卓越的视觉保真度和语义对齐，超越了之前基于 AR 的方法，同时保持了可控性。所有代码将被发布。</li>
</ul>

<h3>Title: CFG-EC: Error Correction Classifier-Free Guidance</h3>
<ul>
<li><strong>Authors: </strong>Nakkyu Yang, Yechan Lee, SooJean Han</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14075">https://arxiv.org/abs/2511.14075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14075">https://arxiv.org/pdf/2511.14075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14075]] CFG-EC: Error Correction Classifier-Free Guidance(https://arxiv.org/abs/2511.14075)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Classifier-Free Guidance (CFG) has become a mainstream approach for simultaneously improving prompt fidelity and generation quality in conditional generative models. During training, CFG stochastically alternates between conditional and null prompts to enable both conditional and unconditional generation. However, during sampling, CFG outputs both null and conditional prompts simultaneously, leading to inconsistent noise estimates between the training and sampling processes. To reduce this error, we propose CFG-EC, a versatile correction scheme augmentable to any CFG-based method by refining the unconditional noise predictions. CFG-EC actively realigns the unconditional noise error component to be orthogonal to the conditional error component. This corrective maneuver prevents interference between the two guidance components, thereby constraining the sampling error's upper bound and establishing more reliable guidance trajectories for high-fidelity image generation. Our numerical experiments show that CFG-EC handles the unconditional component more effectively than CFG and CFG++, delivering a marked performance increase in the low guidance sampling regime and consistently higher prompt alignment across the board.</li>
<li><strong>摘要：</strong>无分类器指导（CFG）已成为同时提高条件生成模型中的提示保真度和生成质量的主流方法。在训练期间，CFG 在条件提示和空提示之间随机交替，以启用条件和无条件生成。然而，在采样过程中，CFG 同时输出空提示和条件提示，导致训练和采样过程之间的噪声估计不一致。为了减少这个误差，我们提出了 CFG-EC，这是一种通用的校正方案，可以通过细化无条件噪声预测来扩展到任何基于 CFG 的方法。 CFG-EC 主动重新调整无条件噪声误差分量，使其与条件误差分量正交。这种校正操作可以防止两个制导组件之间的干扰，从而限制采样误差的上限并为高保真图像生成建立更可靠的制导轨迹。我们的数值实验表明，CFG-EC 比 CFG 和 CFG++ 更有效地处理无条件分量，在低指导采样方案中提供显着的性能提升，并始终保持更高的即时对准。</li>
</ul>

<h3>Title: Zero-Training Task-Specific Model Synthesis for Few-Shot Medical Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Yao Qin, Yangyang Yan, YuanChao Yang, Jinhua Pang, Huanyong Bi, Yuan Liu, HaiHua Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14082">https://arxiv.org/abs/2511.14082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14082">https://arxiv.org/pdf/2511.14082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14082]] Zero-Training Task-Specific Model Synthesis for Few-Shot Medical Image Classification(https://arxiv.org/abs/2511.14082)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep learning models have achieved remarkable success in medical image analysis but are fundamentally constrained by the requirement for large-scale, meticulously annotated datasets. This dependency on "big data" is a critical bottleneck in the medical domain, where patient data is inherently difficult to acquire and expert annotation is expensive, particularly for rare diseases where samples are scarce by definition. To overcome this fundamental challenge, we propose a novel paradigm: Zero-Training Task-Specific Model Synthesis (ZS-TMS). Instead of adapting a pre-existing model or training a new one, our approach leverages a large-scale, pre-trained generative engine to directly synthesize the entire set of parameters for a task-specific classifier. Our framework, the Semantic-Guided Parameter Synthesizer (SGPS), takes as input minimal, multi-modal task information as little as a single example image (1-shot) and a corresponding clinical text description to directly synthesize the entire set of parameters for a task-specific classifier. The generative engine interprets these inputs to generate the weights for a lightweight, efficient classifier (e.g., an EfficientNet-V2), which can be deployed for inference immediately without any task-specific training or fine-tuning. We conduct extensive evaluations on challenging few-shot classification benchmarks derived from the ISIC 2018 skin lesion dataset and a custom rare disease dataset. Our results demonstrate that SGPS establishes a new state-of-the-art, significantly outperforming advanced few-shot and zero-shot learning methods, especially in the ultra-low data regimes of 1-shot and 5-shot classification. This work paves the way for the rapid development and deployment of AI-powered diagnostic tools, particularly for the long tail of rare diseases where data is critically limited.</li>
<li><strong>摘要：</strong>深度学习模型在医学图像分析方面取得了显着的成功，但从根本上受到对大规模、精心注释的数据集的要求的限制。这种对“大数据”的依赖是医疗领域的一个关键瓶颈，患者数据本质上很难获取，专家注释也很昂贵，特别是对于样本稀缺的罕见疾病。为了克服这一根本挑战，我们提出了一种新的范式：零训练任务特定模型合成（ZS-TMS）。我们的方法不是采用预先存在的模型或训练新的模型，而是利用大规模的、预先训练的生成引擎来直接合成特定于任务的分类器的整套参数。我们的框架，语义引导参数合成器（SGPS），将最小的多模态任务信息作为输入，如单个示例图像（1-shot）和相应的临床文本描述，以直接合成特定于任务的分类器的整个参数集。生成引擎解释这些输入以生成轻量级、高效分类器（例如 EfficientNet-V2）的权重，可以立即部署该分类器进行推理，无需任何特定于任务的训练或微调。我们对源自 ISIC 2018 皮肤病变数据集和自定义罕见疾病数据集的具有挑战性的小样本分类基准进行了广泛的评估。我们的结果表明，SGPS 建立了一种新的最先进的方法，显着优于先进的少样本和零样本学习方法，特别是在 1 样本和 5 样本分类的超低数据情况下。这项工作为人工智能驱动的诊断工具的快速开发和部署铺平了道路，特别是对于数据严重有限的罕见疾病的长尾而言。</li>
</ul>

<h3>Title: FAPE-IR: Frequency-Aware Planning and Execution Framework for All-in-One Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Jingren Liu, Shuning Xu, Qirui Yang, Yun Wang, Xiangyu Chen, Zhong Ji</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14099">https://arxiv.org/abs/2511.14099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14099">https://arxiv.org/pdf/2511.14099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14099]] FAPE-IR: Frequency-Aware Planning and Execution Framework for All-in-One Image Restoration(https://arxiv.org/abs/2511.14099)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>All-in-One Image Restoration (AIO-IR) aims to develop a unified model that can handle multiple degradations under complex conditions. However, existing methods often rely on task-specific designs or latent routing strategies, making it hard to adapt to real-world scenarios with various degradations. We propose FAPE-IR, a Frequency-Aware Planning and Execution framework for image restoration. It uses a frozen Multimodal Large Language Model (MLLM) as a planner to analyze degraded images and generate concise, frequency-aware restoration plans. These plans guide a LoRA-based Mixture-of-Experts (LoRA-MoE) module within a diffusion-based executor, which dynamically selects high- or low-frequency experts, complemented by frequency features of the input image. To further improve restoration quality and reduce artifacts, we introduce adversarial training and a frequency regularization loss. By coupling semantic planning with frequency-based restoration, FAPE-IR offers a unified and interpretable solution for all-in-one image restoration. Extensive experiments show that FAPE-IR achieves state-of-the-art performance across seven restoration tasks and exhibits strong zero-shot generalization under mixed degradations.</li>
<li><strong>摘要：</strong>一体式图像恢复（AIO-IR）旨在开发一种统一的模型，可以处理复杂条件下的多种退化。然而，现有的方法通常依赖于特定于任务的设计或潜在的路由策略，使得很难适应具有各种退化的现实场景。我们提出了 FAPE-IR，一种用于图像恢复的频率感知规划和执行框架。它使用冻结的多模态大语言模型 (MLLM) 作为规划器来分析退化图像并生成简洁的、频率感知的恢复计划。这些计划指导基于扩散的执行器中基于 LoRA 的专家混合 (LoRA-MoE) 模块，该模块动态选择高频或低频专家，并辅以输入图像的频率特征。为了进一步提高恢复质量并减少伪影，我们引入了对抗性训练和频率正则化损失。通过将语义规划与基于频率的恢复相结合，FAPE-IR 为一体化图像恢复提供了一种统一且可解释的解决方案。大量实验表明，FAPE-IR 在七个恢复任务中实现了最先进的性能，并在混合退化下表现出强大的零样本泛化能力。</li>
</ul>

<h3>Title: Text-Driven Reasoning Video Editing via Reinforcement Learning on Digital Twin Representations</h3>
<ul>
<li><strong>Authors: </strong>Yiqing Shen, Chenjia Li, Mathias Unberath</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14100">https://arxiv.org/abs/2511.14100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14100">https://arxiv.org/pdf/2511.14100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14100]] Text-Driven Reasoning Video Editing via Reinforcement Learning on Digital Twin Representations(https://arxiv.org/abs/2511.14100)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-driven video editing enables users to modify video content only using text queries. While existing methods can modify video content if explicit descriptions of editing targets with precise spatial locations and temporal boundaries are provided, these requirements become impractical when users attempt to conceptualize edits through implicit queries referencing semantic properties or object relationships. We introduce reasoning video editing, a task where video editing models must interpret implicit queries through multi-hop reasoning to infer editing targets before executing modifications, and a first model attempting to solve this complex task, RIVER (Reasoning-based Implicit Video Editor). RIVER decouples reasoning from generation through digital twin representations of video content that preserve spatial relationships, temporal trajectories, and semantic attributes. A large language model then processes this representation jointly with the implicit query, performing multi-hop reasoning to determine modifications, then outputs structured instructions that guide a diffusion-based editor to execute pixel-level changes. RIVER training uses reinforcement learning with rewards that evaluate reasoning accuracy and generation quality. Finally, we introduce RVEBenchmark, a benchmark of 100 videos with 519 implicit queries spanning three levels and categories of reasoning complexity specifically for reasoning video editing. RIVER demonstrates best performance on the proposed RVEBenchmark and also achieves state-of-the-art performance on two additional video editing benchmarks (VegGIE and FiVE), where it surpasses six baseline methods.</li>
<li><strong>摘要：</strong>文本驱动的视频编辑使用户仅使用文本查询即可修改视频内容。虽然如果提供具有精确空间位置和时间边界的编辑目标的显式描述，则现有方法可以修改视频内容，但是当用户尝试通过引用语义属性或对象关系的隐式查询来概念化编辑时，这些要求变得不切实际。我们介绍了推理视频编辑，这是一项视频编辑模型必须通过多跳推理解释隐式查询以在执行修改之前推断编辑目标的任务，以及尝试解决此复杂任务的第一个模型 RIVER（基于推理的隐式视频编辑器）。 RIVER 通过保留空间关系、时间轨迹和语义属性的视频内容的数字孪生表示，将推理与生成分离。然后，大型语言模型与隐式查询联合处理该表示，执行多跳推理以确定修改，然后输出结构化指令，指导基于扩散的编辑器执行像素级更改。 RIVER 训练使用强化学习和奖励来评估推理准确性和生成质量。最后，我们介绍 RVEBenchmark，这是一个由 100 个视频组成的基准测试，包含 519 个隐式查询，涵盖推理复杂性的三个级别和类别，专门用于推理视频编辑。 RIVER 在提议的 RVEBenchmark 上展示了最佳性能，并且还在另外两个视频编辑基准（VegGIE 和 FiVE）上实现了最先进的性能，超过了六种基准方法。</li>
</ul>

<h3>Title: A Comprehensive Study of Implicit and Explicit Biases in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Fatima Kazi, Alex Young, Yash Inani, Setareh Rafatirad</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14153">https://arxiv.org/abs/2511.14153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14153">https://arxiv.org/pdf/2511.14153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14153]] A Comprehensive Study of Implicit and Explicit Biases in Large Language Models(https://arxiv.org/abs/2511.14153)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) inherit explicit and implicit biases from their training datasets. Identifying and mitigating biases in LLMs is crucial to ensure fair outputs, as they can perpetuate harmful stereotypes and misinformation. This study highlights the need to address biases in LLMs amid growing generative AI. We studied bias-specific benchmarks such as StereoSet and CrowSPairs to evaluate the existence of various biases in multiple generative models such as BERT and GPT 3.5. We proposed an automated Bias-Identification Framework to recognize various social biases in LLMs such as gender, race, profession, and religion. We adopted a two-pronged approach to detect explicit and implicit biases in text data. Results indicated fine-tuned models struggle with gender biases but excelled at identifying and avoiding racial biases. Our findings illustrated that despite having some success, LLMs often over-relied on keywords. To illuminate the capability of the analyzed LLMs in detecting implicit biases, we employed Bag-of-Words analysis and unveiled indications of implicit stereotyping within the vocabulary. To bolster the model performance, we applied an enhancement strategy involving fine-tuning models using prompting techniques and data augmentation of the bias benchmarks. The fine-tuned models exhibited promising adaptability during cross-dataset testing and significantly enhanced performance on implicit bias benchmarks, with performance gains of up to 20%.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 从其训练数据集中继承了显式和隐式偏差。识别和减轻法学硕士的偏见对于确保公平输出至关重要，因为它们可能会延续有害的刻板印象和错误信息。这项研究强调了在生成型人工智能不断发展的背景下解决法学硕士偏见的必要性。我们研究了 StereoSet 和 CrowSPairs 等特定于偏差的基准，以评估 BERT 和 GPT 3.5 等多个生成模型中是否存在各种偏差。我们提出了一个自动偏见识别框架，以识别法学硕士中的各种社会偏见，例如性别、种族、职业和宗教。我们采用了双管齐下的方法来检测文本数据中的显性和隐性偏差。结果表明，经过微调的模型与性别偏见作斗争，但擅长识别和避免种族偏见。我们的研究结果表明，尽管法学硕士取得了一些成功，但他们常常过度依赖关键词。为了阐明所分析的法学硕士在检测隐性偏见方面的能力，我们采用了词袋分析并揭示了词汇中隐性刻板印象的迹象。为了增强模型性能，我们应用了一种增强策略，包括使用提示技术和偏差基准的数据增强来微调模型。经过微调的模型在跨数据集测试中表现出了良好的适应性，并显着增强了隐式偏差基准的性能，性能提升高达 20%。</li>
</ul>

<h3>Title: UniSER: A Foundation Model for Unified Soft Effects Removal</h3>
<ul>
<li><strong>Authors: </strong>Jingdong Zhang, Lingzhi Zhang, Qing Liu, Mang Tik Chiu, Connelly Barnes, Yizhou Wang, Haoran You, Xiaoyang Liu, Yuqian Zhou, Zhe Lin, Eli Shechtman, Sohrab Amirghodsi, Xin Li, Wenping Wang, Xiaohang Zhan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14183">https://arxiv.org/abs/2511.14183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14183">https://arxiv.org/pdf/2511.14183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14183]] UniSER: A Foundation Model for Unified Soft Effects Removal(https://arxiv.org/abs/2511.14183)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Digital images are often degraded by soft effects such as lens flare, haze, shadows, and reflections, which reduce aesthetics even though the underlying pixels remain partially visible. The prevailing works address these degradations in isolation, developing highly specialized, specialist models that lack scalability and fail to exploit the shared underlying essences of these restoration problems. While specialist models are limited, recent large-scale pretrained generalist models offer powerful, text-driven image editing capabilities. while recent general-purpose systems (e.g., GPT-4o, Flux Kontext, Nano Banana) require detailed prompts and often fail to achieve robust removal on these fine-grained tasks or preserve identity of the scene. Leveraging the common essence of soft effects, i.e., semi-transparent occlusions, we introduce a foundational versatile model UniSER, capable of addressing diverse degradations caused by soft effects within a single framework. Our methodology centers on curating a massive 3.8M-pair dataset to ensure robustness and generalization, which includes novel, physically-plausible data to fill critical gaps in public benchmarks, and a tailored training pipeline that fine-tunes a Diffusion Transformer to learn robust restoration priors from this diverse data, integrating fine-grained mask and strength controls. This synergistic approach allows UniSER to significantly outperform both specialist and generalist models, achieving robust, high-fidelity restoration in the wild.</li>
<li><strong>摘要：</strong>数字图像通常会因镜头光晕、雾霾、阴影和反射等软效果而降质，即使底层像素仍然部分可见，但这些效果也会降低美观性。流行的工作孤立地解决了这些退化问题，开发了高度专业化的专业模型，这些模型缺乏可扩展性，并且无法利用这些恢复问题的共同基本本质。虽然专业模型有限，但最近的大规模预训练通才模型提供了强大的文本驱动的图像编辑功能。而最近的通用系统（例如 GPT-4o、Flux Kontext、Nano Banana）需要详细的提示，并且通常无法对这些细粒度任务实现稳健的删除或保留场景的身份。利用软效果的共同本质，即半透明遮挡，我们引入了一个基础的多功能模型 UniSER，能够在单个框架内解决由软效果引起的各种退化。我们的方法集中于整理一个庞大的 380 万对数据集，以确保鲁棒性和泛化性，其中包括新颖的、物理上合理的数据，以填补公共基准中的关键空白，以及定制的训练管道，可微调 Diffusion Transformer 以从这些不同的数据中学习稳健的恢复先验，集成细粒度掩模和强度控制。这种协同方法使 UniSER 的性能显着优于专业模型和通才模型，从而在野外实现稳健、高保真的恢复。</li>
</ul>

<h3>Title: GloTok: Global Perspective Tokenizer for Image Reconstruction and Generation</h3>
<ul>
<li><strong>Authors: </strong>Xuan Zhao, Zhongyu Zhang, Yuge Huang, Yuxi Mi, Guodong Mu, Shouhong Ding, Jun Wang, Rizen Guo, Shuigeng Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14184">https://arxiv.org/abs/2511.14184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14184">https://arxiv.org/pdf/2511.14184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14184]] GloTok: Global Perspective Tokenizer for Image Reconstruction and Generation(https://arxiv.org/abs/2511.14184)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Existing state-of-the-art image tokenization methods leverage diverse semantic features from pre-trained vision models for additional supervision, to expand the distribution of latent representations and thereby improve the quality of image reconstruction and generation. These methods employ a locally supervised approach for semantic supervision, which limits the uniformity of semantic distribution. However, VA-VAE proves that a more uniform feature distribution yields better generation performance. In this work, we introduce a Global Perspective Tokenizer (GloTok), which utilizes global relational information to model a more uniform semantic distribution of tokenized features. Specifically, a codebook-wise histogram relation learning method is proposed to transfer the semantics, which are modeled by pre-trained models on the entire dataset, to the semantic codebook. Then, we design a residual learning module that recovers the fine-grained details to minimize the reconstruction error caused by quantization. Through the above design, GloTok delivers more uniformly distributed semantic latent representations, which facilitates the training of autoregressive (AR) models for generating high-quality images without requiring direct access to pre-trained models during the training process. Experiments on the standard ImageNet-1k benchmark clearly show that our proposed method achieves state-of-the-art reconstruction performance and generation quality.</li>
<li><strong>摘要：</strong>现有最先进的图像标记化方法利用预先训练的视觉模型中的不同语义特征进行额外的监督，以扩展潜在表示的分布，从而提高图像重建和生成的质量。这些方法采用局部监督方法进行语义监督，这限制了语义分布的均匀性。然而，VA-VAE 证明更均匀的特征分布可以产生更好的生成性能。在这项工作中，我们引入了全局视角分词器（GloTok），它利用全局关系信息来建模分词特征的更均匀的语义分布。具体来说，提出了一种码本直方图关系学习方法，将通过整个数据集上的预训练模型建模的语义转移到语义码本。然后，我们设计了一个残差学习模块，可以恢复细粒度的细节，以最小化量化引起的重建误差。通过上述设计，GloTok 提供了更均匀分布的语义潜在表示，这有利于自回归（AR）模型的训练，以生成高质量图像，而无需在训练过程中直接访问预先训练的模型。标准 ImageNet-1k 基准测试的实验清楚地表明，我们提出的方法实现了最先进的重建性能和生成质量。</li>
</ul>

<h3>Title: N-GLARE: An Non-Generative Latent Representation-Efficient LLM Safety Evaluator</h3>
<ul>
<li><strong>Authors: </strong>Zheyu Lin, Jirui Yang, Hengqi Guo, Yubing Bao, Yao Guan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14195">https://arxiv.org/abs/2511.14195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14195">https://arxiv.org/pdf/2511.14195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14195]] N-GLARE: An Non-Generative Latent Representation-Efficient LLM Safety Evaluator(https://arxiv.org/abs/2511.14195)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Evaluating the safety robustness of LLMs is critical for their deployment. However, mainstream Red Teaming methods rely on online generation and black-box output analysis. These approaches are not only costly but also suffer from feedback latency, making them unsuitable for agile diagnostics after training a new model. To address this, we propose N-GLARE (A Non-Generative, Latent Representation-Efficient LLM Safety Evaluator). N-GLARE operates entirely on the model's latent representations, bypassing the need for full text generation. It characterizes hidden layer dynamics by analyzing the APT (Angular-Probabilistic Trajectory) of latent representations and introducing the JSS (Jensen-Shannon Separability) metric. Experiments on over 40 models and 20 red teaming strategies demonstrate that the JSS metric exhibits high consistency with the safety rankings derived from Red Teaming. N-GLARE reproduces the discriminative trends of large-scale red-teaming tests at less than 1\% of the token cost and the runtime cost, providing an efficient output-free evaluation proxy for real-time diagnostics.</li>
<li><strong>摘要：</strong>评估法学硕士的安全稳健性对于其部署至关重要。然而，主流的红队方法依赖于在线生成和黑盒输出分析。这些方法不仅成本高昂，而且还存在反馈延迟，使得它们不适合在训练新模型后进行敏捷诊断。为了解决这个问题，我们提出了 N-GLARE（一种非生成式、潜在表示效率的 LLM 安全评估器）。 N-GLARE 完全基于模型的潜在表示进行操作，无需生成全文。它通过分析潜在表示的 APT（角度概率轨迹）并引入 JSS（Jensen-Shannon 可分离性）度量来表征隐藏层动态。对 40 多个模型和 20 种红队策略的实验表明，JSS 指标与红队得出的安全排名具有高度一致性。 N-GLARE 以不到 1% 的代币成本和运行时成本再现大规模红队测试的判别趋势，为实时诊断提供高效的无输出评估代理。</li>
</ul>

<h3>Title: InstantViR: Real-Time Video Inverse Problem Solver with Distilled Diffusion Prior</h3>
<ul>
<li><strong>Authors: </strong>Weimin Bai, Suzhe Xu, Yiwei Ren, Jinhua Hao, Ming Sun, Wenzheng Chen, He Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14208">https://arxiv.org/abs/2511.14208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14208">https://arxiv.org/pdf/2511.14208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14208]] InstantViR: Real-Time Video Inverse Problem Solver with Distilled Diffusion Prior(https://arxiv.org/abs/2511.14208)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, super-resolution</a></li>
<li><strong>Abstract: </strong>Video inverse problems are fundamental to streaming, telepresence, and AR/VR, where high perceptual quality must coexist with tight latency constraints. Diffusion-based priors currently deliver state-of-the-art reconstructions, but existing approaches either adapt image diffusion models with ad hoc temporal regularizers - leading to temporal artifacts - or rely on native video diffusion models whose iterative posterior sampling is far too slow for real-time use. We introduce InstantViR, an amortized inference framework for ultra-fast video reconstruction powered by a pre-trained video diffusion prior. We distill a powerful bidirectional video diffusion model (teacher) into a causal autoregressive student that maps a degraded video directly to its restored version in a single forward pass, inheriting the teacher's strong temporal modeling while completely removing iterative test-time optimization. The distillation is prior-driven: it only requires the teacher diffusion model and known degradation operators, and does not rely on externally paired clean/noisy video data. To further boost throughput, we replace the video-diffusion backbone VAE with a high-efficiency LeanVAE via an innovative teacher-space regularized distillation scheme, enabling low-latency latent-space processing. Across streaming random inpainting, Gaussian deblurring and super-resolution, InstantViR matches or surpasses the reconstruction quality of diffusion-based baselines while running at over 35 FPS on NVIDIA A100 GPUs, achieving up to 100 times speedups over iterative video diffusion solvers. These results show that diffusion-based video reconstruction is compatible with real-time, interactive, editable, streaming scenarios, turning high-quality video restoration into a practical component of modern vision systems.</li>
<li><strong>摘要：</strong>视频逆问题是流媒体、远程呈现和 AR/VR 的基础，其中高感知质量必须与严格的延迟限制共存。基于扩散的先验目前提供最先进的重建，但现有方法要么使用临时时间正则化器调整图像扩散模型（导致时间伪影），要么依赖本机视频扩散模型，其迭代后验采样对于实时使用来说太慢。我们引入了 InstantViR，这是一种由预先训练的视频扩散先验提供支持的超快速视频重建的摊销推理框架。我们将强大的双向视频扩散模型（教师）提炼为因果自回归学生，该学生在一次前向传递中将降级视频直接映射到其恢复版本，继承了教师强大的时间建模，同时完全消除了迭代测试时间优化。蒸馏是先验驱动的：它只需要教师扩散模型和已知的退化算子，并且不依赖于外部配对的干净/噪声视频数据。为了进一步提高吞吐量，我们通过创新的教师空间正则化蒸馏方案，将视频扩散主干 VAE 替换为高效 LeanVAE，从而实现低延迟的潜在空间处理。在流式随机修复、高斯去模糊和超分辨率方面，InstantViR 匹配或超越了基于扩散的基线的重建质量，同时在 NVIDIA A100 GPU 上以超过 35 FPS 的速度运行，与迭代视频扩散求解器相比，速度提高了 100 倍。这些结果表明，基于扩散的视频重建与实时、交互式、可编辑、流媒体场景兼容，将高质量视频恢复变成现代视觉系统的实用组成部分。</li>
</ul>

<h3>Title: Measurement-Constrained Sampling for Text-Prompted Blind Face Restoration</h3>
<ul>
<li><strong>Authors: </strong>Wenjie Li, Yulun Zhang, Guangwei Gao, Heng Guo, Zhanyu Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14213">https://arxiv.org/abs/2511.14213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14213">https://arxiv.org/pdf/2511.14213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14213]] Measurement-Constrained Sampling for Text-Prompted Blind Face Restoration(https://arxiv.org/abs/2511.14213)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generative</a></li>
<li><strong>Abstract: </strong>Blind face restoration (BFR) may correspond to multiple plausible high-quality (HQ) reconstructions under extremely low-quality (LQ) inputs. However, existing methods typically produce deterministic results, struggling to capture this one-to-many nature. In this paper, we propose a Measurement-Constrained Sampling (MCS) approach that enables diverse LQ face reconstructions conditioned on different textual prompts. Specifically, we formulate BFR as a measurement-constrained generative task by constructing an inverse problem through controlled degradations of coarse restorations, which allows posterior-guided sampling within text-to-image diffusion. Measurement constraints include both Forward Measurement, which ensures results align with input structures, and Reverse Measurement, which produces projection spaces, ensuring that the solution can align with various prompts. Experiments show that our MCS can generate prompt-aligned results and outperforms existing BFR methods. Codes will be released after acceptance.</li>
<li><strong>摘要：</strong>盲脸恢复（BFR）可能对应于极低质量（LQ）输入下的多个看似合理的高质量（HQ）重建。然而，现有方法通常会产生确定性结果，难以捕捉这种一对多的性质。在本文中，我们提出了一种测量约束采样（MCS）方法，该方法能够根据不同的文本提示进行不同的 LQ 面部重建。具体来说，我们通过粗略恢复的受控退化构建逆问题，将 BFR 制定为测量约束的生成任务，这允许在文本到图像扩散中进行后验引导采样。测量约束包括前向测量和反向测量，前向测量确保结果与输入结构一致，反向测量产生投影空间，确保解决方案能够与各种提示保持一致。实验表明，我们的 MCS 可以生成即时一致的结果，并且优于现有的 BFR 方法。代码将在接受后发布。</li>
</ul>

<h3>Title: ManipShield: A Unified Framework for Image Manipulation Detection, Localization and Explanation</h3>
<ul>
<li><strong>Authors: </strong>Zitong Xu, Huiyu Duan, Xiaoyu Wang, Zhaolin Cai, Kaiwei Zhang, Qiang Hu, Jing Liu, Xiongkuo Min, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14259">https://arxiv.org/abs/2511.14259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14259">https://arxiv.org/pdf/2511.14259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14259]] ManipShield: A Unified Framework for Image Manipulation Detection, Localization and Explanation(https://arxiv.org/abs/2511.14259)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of generative models, powerful image editing methods now enable diverse and highly realistic image manipulations that far surpass traditional deepfake techniques, posing new challenges for manipulation detection. Existing image manipulation detection and localization (IMDL) benchmarks suffer from limited content diversity, narrow generative-model coverage, and insufficient interpretability, which hinders the generalization and explanation capabilities of current manipulation detection methods. To address these limitations, we introduce \textbf{ManipBench}, a large-scale benchmark for image manipulation detection and localization focusing on AI-edited images. ManipBench contains over 450K manipulated images produced by 25 state-of-the-art image editing models across 12 manipulation categories, among which 100K images are further annotated with bounding boxes, judgment cues, and textual explanations to support interpretable detection. Building upon ManipBench, we propose \textbf{ManipShield}, an all-in-one model based on a Multimodal Large Language Model (MLLM) that leverages contrastive LoRA fine-tuning and task-specific decoders to achieve unified image manipulation detection, localization, and explanation. Extensive experiments on ManipBench and several public datasets demonstrate that ManipShield achieves state-of-the-art performance and exhibits strong generality to unseen manipulation models. Both ManipBench and ManipShield will be released upon publication.</li>
<li><strong>摘要：</strong>随着生成模型的快速发展，强大的图像编辑方法现在可以实现多样化且高度逼真的图像操作，远远超越传统的 Deepfake 技术，为操作检测提出了新的挑战。现有的图像操纵检测和定位（IMDL）基准受到内容多样性有限、生成模型覆盖范围窄和可解释性不足的影响，这阻碍了当前操纵检测方法的泛化和解释能力。为了解决这些限制，我们引入了 \textbf{ManipBench}，这是一个针对人工智能编辑图像的图像操纵检测和定位的大型基准。 ManipBench 包含超过 45 万张经过操作的图像，这些图像由 25 个最先进的图像编辑模型生成，涵盖 12 个操作类别，其中 10 万张图像进一步用边界框、判断线索和文本解释进行注释，以支持可解释的检测。在 ManipBench 的基础上，我们提出了 \textbf{ManipShield}，这是一种基于多模态大语言模型（MLLM）的一体化模型，利用对比性 LoRA 微调和特定于任务的解码器来实现统一的图像操作检测、定位和解释。在 ManipBench 和多个公共数据集上进行的大量实验表明，ManipShield 实现了最先进的性能，并对看不见的操作模型表现出很强的通用性。 ManipBench 和 ManipShield 都将在发布后发布。</li>
</ul>

<h3>Title: Let Language Constrain Geometry: Vision-Language Models as Semantic and Spatial Critics for 3D Generation</h3>
<ul>
<li><strong>Authors: </strong>Weimin Bai, Yubo Li, Weijian Luo, Zeqiang Lai, Yequan Wang, Wenzheng Chen, He Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14271">https://arxiv.org/abs/2511.14271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14271">https://arxiv.org/pdf/2511.14271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14271]] Let Language Constrain Geometry: Vision-Language Models as Semantic and Spatial Critics for 3D Generation(https://arxiv.org/abs/2511.14271)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Text-to-3D generation has advanced rapidly, yet state-of-the-art models, encompassing both optimization-based and feed-forward architectures, still face two fundamental limitations. First, they struggle with coarse semantic alignment, often failing to capture fine-grained prompt details. Second, they lack robust 3D spatial understanding, leading to geometric inconsistencies and catastrophic failures in part assembly and spatial relationships. To address these challenges, we propose VLM3D, a general framework that repurposes large vision-language models (VLMs) as powerful, differentiable semantic and spatial critics. Our core contribution is a dual-query critic signal derived from the VLM's Yes or No log-odds, which assesses both semantic fidelity and geometric coherence. We demonstrate the generality of this guidance signal across two distinct paradigms: (1) As a reward objective for optimization-based pipelines, VLM3D significantly outperforms existing methods on standard benchmarks. (2) As a test-time guidance module for feed-forward pipelines, it actively steers the iterative sampling process of SOTA native 3D models to correct severe spatial errors. VLM3D establishes a principled and generalizable path to inject the VLM's rich, language-grounded understanding of both semantics and space into diverse 3D generative pipelines.</li>
<li><strong>摘要：</strong>文本到 3D 生成技术发展迅速，但包含基于优化和前馈架构的最先进模型仍然面临两个基本限制。首先，他们在粗略的语义对齐方面遇到困难，常常无法捕获细粒度的提示细节。其次，它们缺乏强大的 3D 空间理解，导致零件装配和空间关系中的几何不一致和灾难性故障。为了应对这些挑战，我们提出了 VLM3D，这是一个通用框架，它将大型视觉语言模型 (VLM) 重新定位为强大的、可区分的语义和空间批评器。我们的核心贡献是从 VLM 的 Yes 或 No log-odds 导出的双查询批评信号，它评估语义保真度和几何一致性。我们通过两个不同的范式证明了该指导信号的通用性：（1）作为基于优化的管道的奖励目标，VLM3D 在标准基准上显着优于现有方法。 (2) 作为前馈管道的测试时引导模块，它主动引导 SOTA 原生 3D 模型的迭代采样过程，以纠正严重的空间错误。 VLM3D 建立了一条有原则且可推广的路径，将 VLM 对语义和空间的丰富的、基于语言的理解注入到不同的 3D 生成管道中。</li>
</ul>

<h3>Title: GEN3D: Generating Domain-Free 3D Scenes from a Single Image</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Zhang, Ziyu Lu, Hongbo Duan, Keyu Fan, Pengting Luo, Peiyu Zhuang, Mengyu Yang, Houde Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14291">https://arxiv.org/abs/2511.14291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14291">https://arxiv.org/pdf/2511.14291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14291]] GEN3D: Generating Domain-Free 3D Scenes from a Single Image(https://arxiv.org/abs/2511.14291)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite recent advancements in neural 3D reconstruction, the dependence on dense multi-view captures restricts their broader applicability. Additionally, 3D scene generation is vital for advancing embodied AI and world models, which depend on diverse, high-quality scenes for learning and evaluation. In this work, we propose Gen3d, a novel method for generation of high-quality, wide-scope, and generic 3D scenes from a single image. After the initial point cloud is created by lifting the RGBD image, Gen3d maintains and expands its world model. The 3D scene is finalized through optimizing a Gaussian splatting representation. Extensive experiments on diverse datasets demonstrate the strong generalization capability and superior performance of our method in generating a world model and Synthesizing high-fidelity and consistent novel views.</li>
<li><strong>摘要：</strong>尽管最近在神经 3D 重建方面取得了进展，但对密集多视图捕获的依赖限制了其更广泛的适用性。此外，3D 场景生成对于推进具体人工智能和世界模型至关重要，这些模型依赖于多样化、高质量的场景进行学习和评估。在这项工作中，我们提出了 Gen3d，一种从单个图像生成高质量、宽范围和通用 3D 场景的新颖方法。通过提升 RGBD 图像创建初始点云后，Gen3d 维护并扩展其世界模型。 3D 场景是通过优化高斯泼溅表示来最终确定的。对不同数据集的大量实验证明了我们的方法在生成世界模型和合成高保真且一致的新颖视图方面具有强大的泛化能力和卓越的性能。</li>
</ul>

<h3>Title: Blur-Robust Detection via Feature Restoration: An End-to-End Framework for Prior-Guided Infrared UAV Target Detection</h3>
<ul>
<li><strong>Authors: </strong>Xiaolin Wang, Houzhang Fang, Qingshan Li, Lu Wang, Yi Chang, Luxin Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14371">https://arxiv.org/abs/2511.14371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14371">https://arxiv.org/pdf/2511.14371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14371]] Blur-Robust Detection via Feature Restoration: An End-to-End Framework for Prior-Guided Infrared UAV Target Detection(https://arxiv.org/abs/2511.14371)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Infrared unmanned aerial vehicle (UAV) target images often suffer from motion blur degradation caused by rapid sensor movement, significantly reducing contrast between target and background. Generally, detection performance heavily depends on the discriminative feature representation between target and background. Existing methods typically treat deblurring as a preprocessing step focused on visual quality, while neglecting the enhancement of task-relevant features crucial for detection. Improving feature representation for detection under blur conditions remains challenging. In this paper, we propose a novel Joint Feature-Domain Deblurring and Detection end-to-end framework, dubbed JFD3. We design a dual-branch architecture with shared weights, where the clear branch guides the blurred branch to enhance discriminative feature representation. Specifically, we first introduce a lightweight feature restoration network, where features from the clear branch serve as feature-level supervision to guide the blurred branch, thereby enhancing its distinctive capability for detection. We then propose a frequency structure guidance module that refines the structure prior from the restoration network and integrates it into shallow detection layers to enrich target structural information. Finally, a feature consistency self-supervised loss is imposed between the dual-branch detection backbones, driving the blurred branch to approximate the feature representations of the clear one. Wealso construct a benchmark, named IRBlurUAV, containing 30,000 simulated and 4,118 real infrared UAV target images with diverse motion blur. Extensive experiments on IRBlurUAV demonstrate that JFD3 achieves superior detection performance while maintaining real-time efficiency.</li>
<li><strong>摘要：</strong>红外无人机 (UAV) 目标图像经常会因传感器快速移动而导致运动模糊退化，从而显着降低目标与背景之间的对比度。一般来说，检测性能很大程度上取决于目标和背景之间的区分特征表示。现有方法通常将去模糊视为专注于视觉质量的预处理步骤，而忽略了对检测至关重要的任务相关特征的增强。改进模糊条件下检测的特征表示仍然具有挑战性。在本文中，我们提出了一种新颖的联合特征域去模糊和检测端到端框架，称为 JFD3。我们设计了具有共享权重的双分支架构，其中清晰的分支引导模糊的分支以增强判别性特征表示。具体来说，我们首先引入一个轻量级特征恢复网络，其中来自清晰分支的特征作为特征级监督来指导模糊分支，从而增强其独特的检测能力。然后，我们提出了一种频率结构引导模块，该模块可以从恢复网络中细化先验结构并将其集成到浅层检测层中以丰富目标结构信息。最后，在双分支检测主干之间施加特征一致性自监督损失，驱动模糊分支逼近清晰分支的特征表示。我们还构建了一个名为 IRBlurUAV 的基准，包含 30,000 个模拟和 4,118 个具有不同运动模糊的真实红外无人机目标图像。在IRBlurUAV上的大量实验表明，JFD3在保持实时效率的同时实现了卓越的检测性能。</li>
</ul>

<h3>Title: A Quantitative Method for Shoulder Presentation Evaluation in Biometric Identity Documents</h3>
<ul>
<li><strong>Authors: </strong>Alfonso Pedro Ridao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14376">https://arxiv.org/abs/2511.14376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14376">https://arxiv.org/pdf/2511.14376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14376]] A Quantitative Method for Shoulder Presentation Evaluation in Biometric Identity Documents(https://arxiv.org/abs/2511.14376)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>International standards for biometric identity documents mandate strict compliance with pose requirements, including the square presentation of a subject's shoulders. However, the literature on automated quality assessment offers few quantitative methods for evaluating this specific attribute. This paper proposes a Shoulder Presentation Evaluation (SPE) algorithm to address this gap. The method quantifies shoulder yaw and roll using only the 3D coordinates of two shoulder landmarks provided by common pose estimation frameworks. The algorithm was evaluated on a dataset of 121 portrait images. The resulting SPE scores demonstrated a strong Pearson correlation (r approx. 0.80) with human-assigned labels. An analysis of the metric's filtering performance, using an adapted Error-versus-Discard methodology, confirmed its utility in identifying non-compliant samples. The proposed algorithm is a viable lightweight tool for automated compliance checking in enrolment systems.</li>
<li><strong>摘要：</strong>生物识别身份证件的国际标准要求严格遵守姿势要求，包括受试者肩膀的方形呈现。然而，有关自动质量评估的文献几乎没有提供评估这一特定属性的定量方法。本文提出了一种肩部呈现评估（SPE）算法来解决这一差距。该方法仅使用常见姿势估计框架提供的两个肩部标志的 3D 坐标来量化肩部偏航和滚动。该算法在 121 张肖像图像的数据集上进行了评估。由此产生的 SPE 分数表明与人工分配的标签具有很强的 Pearson 相关性（r 约为 0.80）。使用改编的错误与丢弃方法对该指标的过滤性能进行分析，证实了其在识别不合规样本方面的实用性。所提出的算法是一种可行的轻量级工具，用于在注册系统中进行自动合规性检查。</li>
</ul>

<h3>Title: MiAD: Mirage Atom Diffusion for De Novo Crystal Generation</h3>
<ul>
<li><strong>Authors: </strong>Andrey Okhotin, Maksim Nakhodnov, Nikita Kazeev, Andrey E Ustyuzhanin, Dmitry Vetrov</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci, cs.AI, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14426">https://arxiv.org/abs/2511.14426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14426">https://arxiv.org/pdf/2511.14426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14426]] MiAD: Mirage Atom Diffusion for De Novo Crystal Generation(https://arxiv.org/abs/2511.14426)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In recent years, diffusion-based models have demonstrated exceptional performance in searching for simultaneously stable, unique, and novel (S.U.N.) crystalline materials. However, most of these models don't have the ability to change the number of atoms in the crystal during the generation process, which limits the variability of model sampling trajectories. In this paper, we demonstrate the severity of this restriction and introduce a simple yet powerful technique, mirage infusion, which enables diffusion models to change the state of the atoms that make up the crystal from existent to non-existent (mirage) and vice versa. We show that this technique improves model quality by up to $\times2.5$ compared to the same model without this modification. The resulting model, Mirage Atom Diffusion (MiAD), is an equivariant joint diffusion model for de novo crystal generation that is capable of altering the number of atoms during the generation process. MiAD achieves an $8.2\%$ S.U.N. rate on the MP-20 dataset, which substantially exceeds existing state-of-the-art approaches. The source code can be found at \href{this https URL}{\texttt{this http URL}}.</li>
<li><strong>摘要：</strong>近年来，基于扩散的模型在寻找同时稳定、独特和新颖（S.U.N.）的晶体材料方面表现出了卓越的性能。然而，大多数这些模型不具备在生成过程中改变晶体中原子数量的能力，这限制了模型采样轨迹的可变性。在本文中，我们证明了这种限制的严重性，并介绍了一种简单而强大的技术，即海市蜃楼注入，它使扩散模型能够将构成晶体的原子状态从存在更改为不存在（海市蜃楼），反之亦然。我们表明，与未经此修改的相同模型相比，该技术将模型质量提高了 $\times2.5$。由此产生的模型 Mirage Atom Diffusion (MiAD) 是用于从头晶体生成的等变联合扩散模型，能够在生成过程中改变原子数量。 MiAD 获得 $8.2\%$ S.U.N. MP-20 数据集上的速率，大大超过了现有的最先进的方法。源代码可以在 \href{此 https URL}{\texttt{此 http URL}} 中找到。</li>
</ul>

<h3>Title: Nonparametric estimation of conditional probability distributions using a generative approach based on conditional push-forward neural networks</h3>
<ul>
<li><strong>Authors: </strong>Nicola Rares Franco, Lorenzo Tedesco</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14455">https://arxiv.org/abs/2511.14455</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14455">https://arxiv.org/pdf/2511.14455</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14455]] Nonparametric estimation of conditional probability distributions using a generative approach based on conditional push-forward neural networks(https://arxiv.org/abs/2511.14455)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce conditional push-forward neural networks (CPFN), a generative framework for conditional distribution estimation. Instead of directly modeling the conditional density $f_{Y|X}$, CPFN learns a stochastic map $\varphi=\varphi(x,u)$ such that $\varphi(x,U)$ and $Y|X=x$ follow approximately the same law, with $U$ a suitable random vector of pre-defined latent variables. This enables efficient conditional sampling and straightforward estimation of conditional statistics through Monte Carlo methods. The model is trained via an objective function derived from a Kullback-Leibler formulation, without requiring invertibility or adversarial training. We establish a near-asymptotic consistency result and demonstrate experimentally that CPFN can achieve performance competitive with, or even superior to, state-of-the-art methods, including kernel estimators, tree-based algorithms, and popular deep learning techniques, all while remaining lightweight and easy to train.</li>
<li><strong>摘要：</strong>我们引入条件前推神经网络（CPFN），这是一种用于条件分布估计的生成框架。 CPFN 不是直接对条件密度 $f_{Y|X}$ 进行建模，而是学习随机映射 $\varphi=\varphi(x,u)$，使得 $\varphi(x,U)$ 和 $Y|X=x$ 遵循大致相同的规律，其中 $U$ 是预定义潜在变量的合适随机向量。这使得能够通过蒙特卡罗方法进行有效的条件采样和条件统计的直接估计。该模型通过源自 Kullback-Leibler 公式的目标函数进行训练，无需可逆性或对抗性训练。我们建立了近乎渐近的一致性结果，并通过实验证明，CPFN 可以实现与最先进的方法相媲美甚至优于最先进的方法的性能，包括核估计器、基于树的算法和流行的深度学习技术，同时保持轻量级且易于训练。</li>
</ul>

<h3>Title: CompEvent: Complex-valued Event-RGB Fusion for Low-light Video Enhancement and Deblurring</h3>
<ul>
<li><strong>Authors: </strong>Mingchen Zhong, Xin Lu, Dong Li, Senyan Xu, Ruixuan Jiang, Xueyang Fu, Baocai Yin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14469">https://arxiv.org/abs/2511.14469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14469">https://arxiv.org/pdf/2511.14469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14469]] CompEvent: Complex-valued Event-RGB Fusion for Low-light Video Enhancement and Deblurring(https://arxiv.org/abs/2511.14469)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Low-light video deblurring poses significant challenges in applications like nighttime surveillance and autonomous driving due to dim lighting and long exposures. While event cameras offer potential solutions with superior low-light sensitivity and high temporal resolution, existing fusion methods typically employ staged strategies, limiting their effectiveness against combined low-light and motion blur degradations. To overcome this, we propose CompEvent, a complex neural network framework enabling holistic full-process fusion of event data and RGB frames for enhanced joint restoration. CompEvent features two core components: 1) Complex Temporal Alignment GRU, which utilizes complex-valued convolutions and processes video and event streams iteratively via GRU to achieve temporal alignment and continuous fusion; and 2) Complex Space-Frequency Learning module, which performs unified complex-valued signal processing in both spatial and frequency domains, facilitating deep fusion through spatial structures and system-level characteristics. By leveraging the holistic representation capability of complex-valued neural networks, CompEvent achieves full-process spatiotemporal fusion, maximizes complementary learning between modalities, and significantly strengthens low-light video deblurring capability. Extensive experiments demonstrate that CompEvent outperforms SOTA methods in addressing this challenging task. The code is available at this https URL.</li>
<li><strong>摘要：</strong>由于昏暗的灯光和长时间的曝光，低光视频去模糊给夜间监控和自动驾驶等应用带来了重大挑战。虽然事件相机提供了具有卓越的低光灵敏度和高时间分辨率的潜在解决方案，但现有的融合方法通常采用分阶段策略，限制了其针对低光和运动模糊综合退化的有效性。为了克服这个问题，我们提出了 CompEvent，这是一个复杂的神经网络框架，能够对事件数据和 RGB 帧进行整体全流程融合，以增强联合恢复。 CompEvent有两个核心组件：1）复杂时间对齐GRU，利用复值卷积，通过GRU迭代处理视频和事件流，实现时间对齐和连续融合； 2）复杂空频学习模块，在空间和频率域上进行统一的复值信号处理，通过空间结构和系统级特征促进深度融合。 CompEvent利用复值神经网络的整体表示能力，实现全流程时空融合，最大化模态间的互补学习，显着增强弱光视频去模糊能力。大量实验表明，CompEvent 在解决这一具有挑战性的任务方面优于 SOTA 方法。该代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Segmentation-Aware Latent Diffusion for Satellite Image Super-Resolution: Enabling Smallholder Farm Boundary Delineation</h3>
<ul>
<li><strong>Authors: </strong>Aditi Agarwal, Anjali Jain, Nikita Saxena, Ishan Deshpande, Michal Kazmierski, Abigail Annkah, Nadav Sherman, Karthikeyan Shanmugam, Alok Talekar, Vaibhav Rajan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14481">https://arxiv.org/abs/2511.14481</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14481">https://arxiv.org/pdf/2511.14481</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14481]] Segmentation-Aware Latent Diffusion for Satellite Image Super-Resolution: Enabling Smallholder Farm Boundary Delineation(https://arxiv.org/abs/2511.14481)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Delineating farm boundaries through segmentation of satellite images is a fundamental step in many agricultural applications. The task is particularly challenging for smallholder farms, where accurate delineation requires the use of high resolution (HR) imagery which are available only at low revisit frequencies (e.g., annually). To support more frequent (sub-) seasonal monitoring, HR images could be combined as references (ref) with low resolution (LR) images -- having higher revisit frequency (e.g., weekly) -- using reference-based super-resolution (Ref-SR) methods. However, current Ref-SR methods optimize perceptual quality and smooth over crucial features needed for downstream tasks, and are unable to meet the large scale-factor requirements for this task. Further, previous two-step approaches of SR followed by segmentation do not effectively utilize diverse satellite sources as inputs. We address these problems through a new approach, $\textbf{SEED-SR}$, which uses a combination of conditional latent diffusion models and large-scale multi-spectral, multi-source geo-spatial foundation models. Our key innovation is to bypass the explicit SR task in the pixel space and instead perform SR in a segmentation-aware latent space. This unique approach enables us to generate segmentation maps at an unprecedented 20$\times$ scale factor, and rigorous experiments on two large, real datasets demonstrate up to $\textbf{25.5}$ and $\textbf{12.9}$ relative improvement in instance and semantic segmentation metrics respectively over approaches based on state-of-the-art Ref-SR methods.</li>
<li><strong>摘要：</strong>通过卫星图像分割来描绘农场边界是许多农业应用中的基本步骤。对于小农农场来说，这项任务尤其具有挑战性，因为准确的描绘需要使用高分辨率 (HR) 图像，而这些图像只能以较低的重访频率（例如每年一次）获得。为了支持更频繁的（次）季节性监测，可以使用基于参考的超分辨率（Ref-SR）方法将 HR 图像作为参考（ref）与具有更高重访频率（例如每周）的低分辨率（LR）图像组合。然而，当前的 Ref-SR 方法优化了感知质量并平滑了下游任务所需的关键特征，并且无法满足该任务的大尺度因子要求。此外，先前的 SR 和分段两步方法不能有效地利用不同的卫星源作为输入。我们通过一种新方法 $\textbf{SEED-SR}$ 来解决这些问题，该方法结合使用条件潜在扩散模型和大规模多光谱、多源地理空间基础模型。我们的关键创新是绕过像素空间中的显式 SR 任务，而是在分段感知的潜在空间中执行 SR。这种独特的方法使我们能够以前所未有的 20$\times$ 比例因子生成分割图，并且对两个大型真实数据集的严格实验表明，与基于最先进的 Ref-SR 方法的方法相比，实例分割指标和语义分割指标分别达到 $\textbf{25.5}$ 和 $\textbf{12.9}$ 的相对改进。</li>
</ul>

<h3>Title: Towards Stable and Structured Time Series Generation with Perturbation-Aware Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Jintao Zhang, Mingyue Cheng, Zirui Liu, Xianquan Wang, Yitong Zhou, Qi Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14488">https://arxiv.org/abs/2511.14488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14488">https://arxiv.org/pdf/2511.14488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14488]] Towards Stable and Structured Time Series Generation with Perturbation-Aware Flow Matching(https://arxiv.org/abs/2511.14488)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Time series generation is critical for a wide range of applications, which greatly supports downstream analytical and decision-making tasks. However, the inherent temporal heterogeneous induced by localized perturbations present significant challenges for generating structurally consistent time series. While flow matching provides a promising paradigm by modeling temporal dynamics through trajectory-level supervision, it fails to adequately capture abrupt transitions in perturbed time series, as the use of globally shared parameters constrains the velocity field to a unified representation. To address these limitations, we introduce \textbf{PAFM}, a \textbf{P}erturbation-\textbf{A}ware \textbf{F}low \textbf{M}atching framework that models perturbed trajectories to ensure stable and structurally consistent time series generation. The framework incorporates perturbation-guided training to simulate localized disturbances and leverages a dual-path velocity field to capture trajectory deviations under perturbation, enabling refined modeling of perturbed behavior to enhance the structural coherence. In order to further improve sensitivity to trajectory perturbations while enhancing expressiveness, a mixture-of-experts decoder with flow routing dynamically allocates modeling capacity in response to different trajectory dynamics. Extensive experiments on both unconditional and conditional generation tasks demonstrate that PAFM consistently outperforms strong baselines. Code is available at this https URL.</li>
<li><strong>摘要：</strong>时间序列生成对于广泛的应用至关重要，它极大地支持下游分析和决策任务。然而，局部扰动引起的固有时间异质性对生成结构一致的时间序列提出了重大挑战。虽然流匹配通过轨迹级监督对时间动态进行建模，提供了一种有前途的范例，但它无法充分捕获扰动时间序列中的突变，因为全局共享参数的使用将速度场限制为统一的表示。为了解决这些限制，我们引入了 \textbf{PAFM}，这是一个 \textbf{P}erturbation-\textbf{A}ware \textbf{F}low \textbf{M} 匹配框架，用于对扰动轨迹进行建模，以确保稳定且结构一致的时间序列生成。该框架结合了扰动引导训练来模拟局部扰动，并利用双路径速度场来捕获扰动下的轨迹偏差，从而能够对扰动行为进行精细建模，以增强结构的一致性。为了进一步提高对轨迹扰动的敏感性，同时增强表现力，具有流路由的混合专家解码器可以根据不同的轨迹动态动态分配建模能力。对无条件和条件生成任务的大量实验表明，PAFM 始终优于强基线。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Full Atom Peptide Design via Riemannian Euclidean Bayesian Flow Networks</h3>
<ul>
<li><strong>Authors: </strong>Hao Qian, Shikui Tu, Lei Xu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14516">https://arxiv.org/abs/2511.14516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14516">https://arxiv.org/pdf/2511.14516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14516]] Full Atom Peptide Design via Riemannian Euclidean Bayesian Flow Networks(https://arxiv.org/abs/2511.14516)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion and flow matching models have recently emerged as promising approaches for peptide binder design. Despite their progress, these models still face two major challenges. First, categorical sampling of discrete residue types collapses their continuous parameters into onehot assignments, while continuous variables (e.g., atom positions) evolve smoothly throughout the generation process. This mismatch disrupts the update dynamics and results in suboptimal performance. Second, current models assume unimodal distributions for side-chain torsion angles, which conflicts with the inherently multimodal nature of side chain rotameric states and limits prediction accuracy. To address these limitations, we introduce PepBFN, the first Bayesian flow network for full atom peptide design that directly models parameter distributions in fully continuous space. Specifically, PepBFN models discrete residue types by learning their continuous parameter distributions, enabling joint and smooth Bayesian updates with other continuous structural parameters. It further employs a novel Gaussian mixture based Bayesian flow to capture the multimodal side chain rotameric states and a Matrix Fisher based Riemannian flow to directly model residue orientations on the $\mathrm{SO}(3)$ manifold. Together, these parameter distributions are progressively refined via Bayesian updates, yielding smooth and coherent peptide generation. Experiments on side chain packing, reverse folding, and binder design tasks demonstrate the strong potential of PepBFN in computational peptide design.</li>
<li><strong>摘要：</strong>扩散和流动匹配模型最近已成为肽结合剂设计的有前途的方法。尽管取得了进展，这些模型仍然面临两大挑战。首先，离散残基类型的分类采样将其连续参数折叠为单热分配，而连续变量（例如原子位置）在整个生成过程中平稳演变。这种不匹配会破坏更新动态并导致性能不佳。其次，当前模型假设侧链扭转角呈单峰分布，这与侧链旋转异构体固有的多峰性质相冲突，并限制了预测精度。为了解决这些限制，我们引入了 PepBFN，这是第一个用于全原子肽设计的贝叶斯流网络，可直接对完全连续空间中的参数分布进行建模。具体来说，PepBFN 通过学习离散残基类型的连续参数分布来对其进行建模，从而实现与其他连续结构参数的联合且平滑的贝叶斯更新。它进一步采用基于贝叶斯流的新型高斯混合来捕获多模态侧链旋转异构体状态，并采用基于矩阵费希尔的黎曼流来直接对 $\mathrm{SO}(3)$ 流形上的残基方向进行建模。总之，这些参数分布通过贝叶斯更新逐步细化，产生平滑且连贯的肽生成。侧链堆积、反向折叠和粘合剂设计任务的实验证明了 PepBFN 在计算肽设计中的强大潜力。</li>
</ul>

<h3>Title: A Generative Data Framework with Authentic Supervision for Underwater Image Restoration and Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Yufeng Tian, Yifan Chen, Zhe Sun, Libang Chen, Mingyu Dou, Jijun Lu, Ye Zheng, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14521">https://arxiv.org/abs/2511.14521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14521">https://arxiv.org/pdf/2511.14521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14521]] A Generative Data Framework with Authentic Supervision for Underwater Image Restoration and Enhancement(https://arxiv.org/abs/2511.14521)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generative</a></li>
<li><strong>Abstract: </strong>Underwater image restoration and enhancement are crucial for correcting color distortion and restoring image details, thereby establishing a fundamental basis for subsequent underwater visual tasks. However, current deep learning methodologies in this area are frequently constrained by the scarcity of high-quality paired datasets. Since it is difficult to obtain pristine reference labels in underwater scenes, existing benchmarks often rely on manually selected results from enhancement algorithms, providing debatable reference images that lack globally consistent color and authentic supervision. This limits the model's capabilities in color restoration, image enhancement, and generalization. To overcome this limitation, we propose using in-air natural images as unambiguous reference targets and translating them into underwater-degraded versions, thereby constructing synthetic datasets that provide authentic supervision signals for model learning. Specifically, we establish a generative data framework based on unpaired image-to-image translation, producing a large-scale dataset that covers 6 representative underwater degradation types. The framework constructs synthetic datasets with precise ground-truth labels, which facilitate the learning of an accurate mapping from degraded underwater images to their pristine scene appearances. Extensive quantitative and qualitative experiments across 6 representative network architectures and 3 independent test sets show that models trained on our synthetic data achieve comparable or superior color restoration and generalization performance to those trained on existing benchmarks. This research provides a reliable and scalable data-driven solution for underwater image restoration and enhancement. The generated dataset is publicly available at: this https URL.</li>
<li><strong>摘要：</strong>水下图像修复和增强对于校正颜色失真和恢复图像细节至关重要，从而为后续水下视觉任务奠定基础。然而，当前该领域的深度学习方法经常受到高质量配对数据集稀缺的限制。由于很难在水下场景中获得原始参考标签，现有基准通常依赖于增强算法手动选择的结果，提供缺乏全局一致颜色和真实监督的有争议的参考图像。这限制了模型在颜色恢复、图像增强和泛化方面的能力。为了克服这一限制，我们建议使用空中自然图像作为明确的参考目标，并将其转换为水下降解版本，从而构建为模型学习提供真实监督信号的合成数据集。具体来说，我们建立了一个基于不成对图像到图像转换的生成数据框架，生成涵盖 6 种代表性水下退化类型的大规模数据集。该框架构建了具有精确地面实况标签的合成数据集，这有助于学习从退化的水下图像到原始场景外观的准确映射。跨 6 个代表性网络架构和 3 个独立测试集的广泛定量和定性实验表明，在我们的合成数据上训练的模型与在现有基准上训练的模型相比，实现了可比或更好的颜色恢复和泛化性能。这项研究为水下图像恢复和增强提供了可靠且可扩展的数据驱动解决方案。生成的数据集可在以下位置公开获取：此 https URL。</li>
</ul>

<h3>Title: MissHDD: Hybrid Deterministic Diffusion for Hetrogeneous Incomplete Data Imputation</h3>
<ul>
<li><strong>Authors: </strong>Youran Zhou, Mohamed Reda Bouadjenek, Sunil Aryal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14543">https://arxiv.org/abs/2511.14543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14543">https://arxiv.org/pdf/2511.14543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14543]] MissHDD: Hybrid Deterministic Diffusion for Hetrogeneous Incomplete Data Imputation(https://arxiv.org/abs/2511.14543)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Incomplete data are common in real-world tabular applications, where numerical, categorical, and discrete attributes coexist within a single dataset. This heterogeneous structure presents significant challenges for existing diffusion-based imputation models, which typically assume a homogeneous feature space and rely on stochastic denoising trajectories. Such assumptions make it difficult to maintain conditional consistency, and they often lead to information collapse for categorical variables or instability when numerical variables require deterministic updates. These limitations indicate that a single diffusion process is insufficient for mixed-type tabular imputation. We propose a hybrid deterministic diffusion framework that separates heterogeneous features into two complementary generative channels. A continuous DDIM-based channel provides efficient and stable deterministic denoising for numerical variables, while a discrete latent-path diffusion channel, inspired by loopholing-based discrete diffusion, models categorical and discrete features without leaving their valid sample manifolds. The two channels are trained under a unified conditional imputation objective, enabling coherent reconstruction of mixed-type incomplete data. Extensive experiments on multiple real-world datasets show that the proposed framework achieves higher imputation accuracy, more stable sampling trajectories, and improved robustness across MCAR, MAR, and MNAR settings compared with existing diffusion-based and classical methods. These results demonstrate the importance of structure-aware diffusion processes for advancing deep learning approaches to incomplete tabular data.</li>
<li><strong>摘要：</strong>不完整的数据在现实世界的表格应用程序中很常见，其中数值、分类和离散属性共存于单个数据集中。这种异构结构对现有的基于扩散的插补模型提出了重大挑战，这些模型通常假设均匀的特征空间并依赖于随机去噪轨迹。这种假设使得保持条件一致性变得困难，并且当数值变量需要确定性更新时，它们通常会导致分类变量的信息崩溃或不稳定。这些限制表明单一扩散过程不足以进行混合型表格插补。我们提出了一种混合确定性扩散框架，将异构特征分成两个互补的生成通道。基于连续 DDIM 的通道为数值变量提供高效且稳定的确定性去噪，而受基于漏洞的离散扩散启发的离散潜在路径扩散通道可以对分类和离散特征进行建模，而无需留下有效的样本流形。两个通道在统一的条件插补目标下进行训练，从而能够对混合类型的不完整数据进行相干重建。对多个真实世界数据集的大量实验表明，与现有的基于扩散的经典方法相比，所提出的框架在 MCAR、MAR 和 MNAR 设置中实现了更高的插补精度、更稳定的采样轨迹以及更高的鲁棒性。这些结果证明了结构感知扩散过程对于推进深度学习方法处理不完整表格数据的重要性。</li>
</ul>

<h3>Title: Deep Learning-Based Regional White Matter Hyperintensity Mapping as a Robust Biomarker for Alzheimer's Disease</h3>
<ul>
<li><strong>Authors: </strong>Julia Machnio, Mads Nielsen, Mostafa Mehdipour Ghazi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14588">https://arxiv.org/abs/2511.14588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14588">https://arxiv.org/pdf/2511.14588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14588]] Deep Learning-Based Regional White Matter Hyperintensity Mapping as a Robust Biomarker for Alzheimer's Disease(https://arxiv.org/abs/2511.14588)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>White matter hyperintensities (WMH) are key imaging markers in cognitive aging, Alzheimer's disease (AD), and related dementias. Although automated methods for WMH segmentation have advanced, most provide only global lesion load and overlook their spatial distribution across distinct white matter regions. We propose a deep learning framework for robust WMH segmentation and localization, evaluated across public datasets and an independent Alzheimer's Disease Neuroimaging Initiative (ADNI) cohort. Our results show that the predicted lesion loads are in line with the reference WMH estimates, confirming the robustness to variations in lesion load, acquisition, and demographics. Beyond accurate segmentation, we quantify WMH load within anatomically defined regions and combine these measures with brain structure volumes to assess diagnostic value. Regional WMH volumes consistently outperform global lesion burden for disease classification, and integration with brain atrophy metrics further improves performance, reaching area under the curve (AUC) values up to 0.97. Several spatially distinct regions, particularly within anterior white matter tracts, are reproducibly associated with diagnostic status, indicating localized vulnerability in AD. These results highlight the added value of regional WMH quantification. Incorporating localized lesion metrics alongside atrophy markers may enhance early diagnosis and stratification in neurodegenerative disorders.</li>
<li><strong>摘要：</strong>白质高信号 (WMH) 是认知衰老、阿尔茨海默病 (AD) 和相关痴呆症的关键影像学标志物。尽管 WMH 分割的自动化方法已经取得了进步，但大多数只提供全局病变负荷，而忽略了它们在不同白质区域的空间分布。我们提出了一个用于强大的 WMH 分割和定位的深度学习框架，并在公共数据集和独立的阿尔茨海默病神经影像倡议 (ADNI) 队列中进行了评估。我们的结果表明，预测的病变负荷与参考 WMH 估计值一致，证实了病变负荷、采集和人口统计变化的稳健性。除了精确分割之外，我们还量化解剖学定义区域内的 WMH 负荷，并将这些测量值与大脑结构体积相结合，以评估诊断价值。区域 WMH 量在疾病分类方面始终优于全球病变负担，并且与脑萎缩指标的整合进一步提高了性能，使曲线下面积 (AUC) 值高达 0.97。几个空间上不同的区域，特别是前白质束内，与诊断状态可重复相关，表明 AD 的局部脆弱性。这些结果凸显了区域 WMH 量化的附加值。将局部病变指标与萎缩标记结合起来可能会增强神经退行性疾病的早期诊断和分层。</li>
</ul>

<h3>Title: 3D-Guided Scalable Flow Matching for Generating Volumetric Tissue Spatial Transcriptomics from Serial Histology</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Vali Sanian, Arshia Hemmat, Amirhossein Vahidi, Jonas Maaskola, Jimmy Tsz Hang Lee, Stanislaw Makarchuk, Yeliz Demirci, Nana-Jane Chipampe, Omer Bayraktar, Lassi Paavolainen, Mohammad Lotfollahi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14613">https://arxiv.org/abs/2511.14613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14613">https://arxiv.org/pdf/2511.14613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14613]] 3D-Guided Scalable Flow Matching for Generating Volumetric Tissue Spatial Transcriptomics from Serial Histology(https://arxiv.org/abs/2511.14613)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>A scalable and robust 3D tissue transcriptomics profile can enable a holistic understanding of tissue organization and provide deeper insights into human biology and disease. Most predictive algorithms that infer ST directly from histology treat each section independently and ignore 3D structure, while existing 3D-aware approaches are not generative and do not scale well. We present Holographic Tissue Expression Inpainting and Analysis (HoloTea), a 3D-aware flow-matching framework that imputes spot-level gene expression from H&E while explicitly using information from adjacent sections. Our key idea is to retrieve morphologically corresponding spots on neighboring slides in a shared feature space and fuse this cross section context into a lightweight ControlNet, allowing conditioning to follow anatomical continuity. To better capture the count nature of the data, we introduce a 3D-consistent prior for flow matching that combines a learned zero-inflated negative binomial (ZINB) prior with a spatial-empirical prior constructed from neighboring sections. A global attention block introduces 3D H&E scaling linearly with the number of spots in the slide, enabling training and inference on large 3D ST datasets. Across three spatial transcriptomics datasets spanning different tissue types and resolutions, HoloTea consistently improves 3D expression accuracy and generalization compared to 2D and 3D baselines. We envision HoloTea advancing the creation of accurate 3D virtual tissues, ultimately accelerating biomarker discovery and deepening our understanding of disease.</li>
<li><strong>摘要：</strong>可扩展且强大的 3D 组织转录组学概况可以实现对组织组织的整体了解，并提供对人类生物学和疾病的更深入的见解。大多数直接从组织学推断 ST 的预测算法独立处理每个部分并忽略 3D 结构，而现有的 3D 感知方法不具有生成性并且不能很好地扩展。我们提出了全息组织表达修复和分析 (HoloTea)，这是一种 3D 感知流匹配框架，可估算 H&E 的点级基因表达，同时明确使用相邻部分的信息。我们的关键思想是在共享特征空间中检索相邻幻灯片上形态上对应的点，并将此横截面上下文融合到轻量级 ControlNet 中，从而允许调节遵循解剖连续性。为了更好地捕获数据的计数性质，我们引入了用于流匹配的 3D 一致先验，它将学习的零膨胀负二项式 (ZINB) 先验与从相邻部分构造的空间经验先验相结合。全局注意力模块引入了 3D H&E 缩放，其与幻灯片中的点数量呈线性关系，从而能够对大型 3D ST 数据集进行训练和推理。在涵盖不同组织类型和分辨率的三个空间转录组学数据集中，与 2D 和 3D 基线相比，HoloTea 持续提高了 3D 表达准确性和泛化能力。我们设想 HoloTea 推进精确 3D 虚拟组织的创建，最终加速生物标志物的发现并加深我们对疾病的理解。</li>
</ul>

<h3>Title: Failure to Mix: Large language models struggle to answer according to desired probability distributions</h3>
<ul>
<li><strong>Authors: </strong>Ivy Yuqian Yang, David Yu Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14630">https://arxiv.org/abs/2511.14630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14630">https://arxiv.org/pdf/2511.14630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14630]] Failure to Mix: Large language models struggle to answer according to desired probability distributions(https://arxiv.org/abs/2511.14630)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Scientific idea generation and selection requires exploration following a target probability distribution. In contrast, current AI benchmarks have objectively correct answers, and training large language models (LLMs) via reinforcement learning against these benchmarks discourages probabilistic exploration. Here, we conducted systematic experiments requesting LLMs to produce outputs following simple probabilistic distributions, and found that all modern LLMs tested grossly fail to follow the distributions. For example, requesting a binary output of "1" 49% of the time produces an answer of "0" nearly 100% of the time. This step function-like behavior of near-exclusively generating the output with marginally highest probability even overrules even strong in-built LLM biases.</li>
<li><strong>摘要：</strong>科学思想的产生和选择需要遵循目标概率分布进行探索。相比之下，当前的人工智能基准具有客观正确的答案，并且通过针对这些基准的强化学习来训练大型语言模型（LLM）会阻碍概率探索。在这里，我们进行了系统实验，要求法学硕士产生遵循简单概率分布的输出，结果发现所有测试的现代法学硕士都严重未能遵循分布。例如，在 49% 的情况下请求二进制输出“1”，则几乎 100% 的情况下会产生“0”的答案。这种类似阶跃函数的行为几乎完全以最高概率生成输出，甚至推翻了强烈的内置 LLM 偏差。</li>
</ul>

<h3>Title: FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Yunfeng Wu, Jiayi Song, Zhenxiong Tan, Zihao He, Songhua Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14712">https://arxiv.org/abs/2511.14712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14712">https://arxiv.org/pdf/2511.14712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14712]] FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation(https://arxiv.org/abs/2511.14712)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The quadratic time and memory complexity of the attention mechanism in modern Transformer based video generators makes end-to-end training for ultra high resolution videos prohibitively expensive. Motivated by this limitation, we introduce a training-free approach that leverages video Diffusion Transformers pretrained at their native scale to synthesize higher resolution videos without any additional training or adaptation. At the core of our method lies an inward sliding window attention mechanism, which originates from a key observation: maintaining each query token's training scale receptive field is crucial for preserving visual fidelity and detail. However, naive local window attention, unfortunately, often leads to repetitive content and exhibits a lack of global coherence in the generated results. To overcome this challenge, we devise a dual-path pipeline that backs up window attention with a novel cross-attention override strategy, enabling the semantic content produced by local attention to be guided by another branch with a full receptive field and, therefore, ensuring holistic consistency. Furthermore, to improve efficiency, we incorporate a cross-attention caching strategy for this branch to avoid the frequent computation of full 3D attention. Extensive experiments demonstrate that our method delivers ultra-high-resolution videos with fine-grained visual details and high efficiency in a training-free paradigm. Meanwhile, it achieves superior performance on VBench, even compared to training-based alternatives, with competitive or improved efficiency. Codes are available at: this https URL</li>
<li><strong>摘要：</strong>现代基于 Transformer 的视频生成器中注意力机制的二次时间和内存复杂性使得超高分辨率视频的端到端训练变得异常昂贵。受此限制的启发，我们引入了一种免训练方法，该方法利用按其本机规模预训练的视频扩散变压器来合成更高分辨率的视频，而无需任何额外的训练或适应。我们方法的核心在于向内滑动窗口注意机制，它源于一个关键观察：维持每个查询标记的训练尺度感受野对于保持视觉保真度和细节至关重要。然而，不幸的是，天真的本地窗口注意力通常会导致内容重复，并且生成的结果缺乏全局一致性。为了克服这一挑战，我们设计了一种双路径管道，通过新颖的交叉注意力覆盖策略来支持窗口注意力，使局部注意力产生的语义内容能够由具有完整感受野的另一个分支引导，从而确保整体一致性。此外，为了提高效率，我们为此分支引入了交叉注意力缓存策略，以避免频繁计算全 3D 注意力。大量的实验表明，我们的方法可以在免训练的范例中提供具有细粒度视觉细节和高效率的超高分辨率视频。同时，它在 VBench 上实现了卓越的性能，甚至与基于训练的替代方案相比，具有竞争力或提高了效率。代码可在以下位置获得：此 https URL</li>
</ul>

<h3>Title: Diffusion As Self-Distillation: End-to-End Latent Diffusion In One Model</h3>
<ul>
<li><strong>Authors: </strong>Xiyuan Wang, Muhan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14716">https://arxiv.org/abs/2511.14716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14716">https://arxiv.org/pdf/2511.14716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14716]] Diffusion As Self-Distillation: End-to-End Latent Diffusion In One Model(https://arxiv.org/abs/2511.14716)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Standard Latent Diffusion Models rely on a complex, three-part architecture consisting of a separate encoder, decoder, and diffusion network, which are trained in multiple stages. This modular design is computationally inefficient, leads to suboptimal performance, and prevents the unification of diffusion with the single-network architectures common in vision foundation models. Our goal is to unify these three components into a single, end-to-end trainable network. We first demonstrate that a naive joint training approach fails catastrophically due to ``latent collapse'', where the diffusion training objective interferes with the network's ability to learn a good latent representation. We identify the root causes of this instability by drawing a novel analogy between diffusion and self-distillation based unsupervised learning method. Based on this insight, we propose Diffusion as Self-Distillation (DSD), a new framework with key modifications to the training objective that stabilize the latent space. This approach enables, for the first time, the stable end-to-end training of a single network that simultaneously learns to encode, decode, and perform diffusion. DSD achieves outstanding performance on the ImageNet $256\times 256$ conditional generation task: FID=13.44/6.38/4.25 with only 42M/118M/205M parameters and 50 training epochs on ImageNet, without using classifier-free-guidance.</li>
<li><strong>摘要：</strong>标准潜在扩散模型依赖于复杂的三部分架构，由单独的编码器、解码器和扩散网络组成，并在多个阶段进行训练。这种模块化设计的计算效率低下，导致性能不佳，并且阻碍了扩散与视觉基础模型中常见的单网络架构的统一。我们的目标是将这三个组件统一为一个端到端的可训练网络。我们首先证明，由于“潜在崩溃”，天真的联合训练方法会灾难性地失败，其中扩散训练目标会干扰网络学习良好潜在表示的能力。我们通过在基于扩散和自蒸馏的无监督学习方法之间进行新颖的类比来确定这种不稳定性的根本原因。基于这一见解，我们提出了扩散作为自蒸馏（DSD），这是一种新框架，对稳定潜在空间的训练目标进行了关键修改。这种方法首次实现了单个网络的稳定端到端训练，同时学习编码、解码和执行扩散。 DSD 在 ImageNet $256\times 256$ 条件生成任务上取得了出色的性能：FID=13.44/6.38/4.25，仅使用 42M/118M/205M 参数和 ImageNet 上的 50 个训练周期，无需使用无分类器指导。</li>
</ul>

<h3>Title: Zero-shot Synthetic Video Realism Enhancement via Structure-aware Denoising</h3>
<ul>
<li><strong>Authors: </strong>Yifan Wang, Liya Ji, Zhanghan Ke, Harry Yang, Ser-Nam Lim, Qifeng Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14719">https://arxiv.org/abs/2511.14719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14719">https://arxiv.org/pdf/2511.14719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14719]] Zero-shot Synthetic Video Realism Enhancement via Structure-aware Denoising(https://arxiv.org/abs/2511.14719)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We propose an approach to enhancing synthetic video realism, which can re-render synthetic videos from a simulator in photorealistic fashion. Our realism enhancement approach is a zero-shot framework that focuses on preserving the multi-level structures from synthetic videos into the enhanced one in both spatial and temporal domains, built upon a diffusion video foundational model without further fine-tuning. Specifically, we incorporate an effective modification to have the generation/denoising process conditioned on estimated structure-aware information from the synthetic video, such as depth maps, semantic maps, and edge maps, by an auxiliary model, rather than extracting the information from a simulator. This guidance ensures that the enhanced videos are consistent with the original synthetic video at both the structural and semantic levels. Our approach is a simple yet general and powerful approach to enhancing synthetic video realism: we show that our approach outperforms existing baselines in structural consistency with the original video while maintaining state-of-the-art photorealism quality in our experiments.</li>
<li><strong>摘要：</strong>我们提出了一种增强合成视频真实感的方法，它可以以逼真的方式从模拟器中重新渲染合成视频。我们的真实感增强方法是一种零镜头框架，专注于将合成视频的多级结构保留为空间和时间域中的增强视频，建立在扩散视频基础模型的基础上，无需进一步微调。具体来说，我们进行了有效的修改，通过辅助模型使生成/去噪过程以合成视频中估计的结构感知信息为条件，例如深度图、语义图和边缘图，而不是从模拟器中提取信息。该指南确保增强视频在结构和语义层面与原始合成视频保持一致。我们的方法是一种简单但通用且强大的增强合成视频真实感的方法：我们表明，我们的方法在与原始视频的结构一致性方面优于现有基线，同时在我们的实验中保持最先进的照片真实感质量。</li>
</ul>

<h3>Title: AdamHD: Decoupled Huber Decay Regularization for Language Model Pre-Training</h3>
<ul>
<li><strong>Authors: </strong>Fu-Ming Guo, Yingfang Fan</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14721">https://arxiv.org/abs/2511.14721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14721">https://arxiv.org/pdf/2511.14721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14721]] AdamHD: Decoupled Huber Decay Regularization for Language Model Pre-Training(https://arxiv.org/abs/2511.14721)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Adaptive optimizers with decoupled weight decay, such as AdamW, are the de facto standard for pre-training large transformer-based generative models. Yet the quadratic nature of the $\ell_2$ penalty embedded in weight decay drives all parameters toward the origin at the same rate, making the update vulnerable to rare but extreme gradient directions and often over-penalizing well-conditioned coordinates. We propose AdamHuberDecay, a drop-in replacement for AdamW that substitutes the $\ell_2$ penalty with a decoupled smooth Huber regularizer. The resulting update decays parameters quadratically while their magnitude remains below a threshold $\delta$, and linearly ($\ell_1$-like) once they exceed $\delta$, yielding (i) bounded regularization gradients, (ii) invariance to per-coordinate second-moment rescaling, and (iii) stronger sparsity pressure on overgrown weights. We derive the closed-form decoupled Huber decay step and show how to integrate it with any Adam-family optimizer at $O(1)$ extra cost. Extensive experiments on GPT-2 and GPT-3 pre-training demonstrate that AdamHuberDecay (a) converges 10-15% faster in wall-clock time, (b) reduces validation perplexity by up to 4 points, (c) delivers performance improvements of 2.5-4.7% across downstream tasks, and (d) yields visibly sparser weight histograms that translate into 20-30% memory savings after magnitude pruning, without tuning the decay coefficient beyond the default grid used for AdamW. Ablations confirm robustness to outlier gradients and large-batch regimes, together with theoretical analyses that bound the expected parameter norm under noisy updates. AdamHuberDecay therefore provides a simple, principled path toward more efficient and resilient training of next-generation foundational generative transformers.</li>
<li><strong>摘要：</strong>具有解耦权重衰减的自适应优化器（例如 AdamW）是预训练大型基于 Transformer 的生成模型的事实上的标准。然而，权重衰减中嵌入的 $\ell_2$ 惩罚的二次性质以相同的速率驱动所有参数向原点移动，使得更新容易受到罕见但极端的梯度方向的影响，并且经常过度惩罚条件良好的坐标。我们提出 AdamHuberDecay，它是 AdamW 的直接替代品，用解耦的平滑 Huber 正则化器代替 $\ell_2$ 惩罚。由此产生的更新在参数大小保持低于阈值$\delta$时以二次方方式衰减参数，一旦超过$\delta$则以线性方式（类似$\ell_1$）衰减参数，产生（i）有界正则化梯度，（ii）每坐标二阶矩重新缩放的不变性，以及（iii）对过度增长的权重更强的稀疏压力。我们推导了封闭式解耦 Huber 衰减步骤，并展示了如何以 $O(1)$ 额外成本将其与任何 Adam 系列优化器集成。 GPT-2 和 GPT-3 预训练的大量实验表明，AdamHuberDecay (a) 在挂钟时间内收敛速度提高了 10-15%，(b) 将验证困惑度降低了多达 4 个点，(c) 在下游任务中实现了 2.5-4.7% 的性能改进，以及 (d) 产生了明显稀疏的权重直方图，在幅度剪枝后可节省 20-30% 的内存，而无需调整衰减系数超出 AdamW 使用的默认网格。消融证实了对异常梯度和大批量制度的鲁棒性，以及在噪声更新下限制预期参数范数的理论分析。因此，AdamHuberDecay 提供了一条简单、有原则的路径，以实现更高效、更有弹性的下一代基础生成变压器的训练。</li>
</ul>

<h3>Title: UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Rui Tian, Mingfei Gao, Haiming Gang, Jiasen Lu, Zhe Gan, Yinfei Yang, Zuxuan Wu, Afshin Dehghan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14760">https://arxiv.org/abs/2511.14760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14760">https://arxiv.org/pdf/2511.14760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14760]] UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning(https://arxiv.org/abs/2511.14760)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present UniGen-1.5, a unified multimodal large language model (MLLM) for advanced image understanding, generation and editing. Building upon UniGen, we comprehensively enhance the model architecture and training pipeline to strengthen the image understanding and generation capabilities while unlocking strong image editing ability. Especially, we propose a unified Reinforcement Learning (RL) strategy that improves both image generation and image editing jointly via shared reward models. To further enhance image editing performance, we propose a light Edit Instruction Alignment stage that significantly improves the editing instruction comprehension that is essential for the success of the RL training. Experimental results show that UniGen-1.5 demonstrates competitive understanding and generation performance. Specifically, UniGen-1.5 achieves 0.89 and 4.31 overall scores on GenEval and ImgEdit that surpass the state-of-the-art models such as BAGEL and reaching performance comparable to proprietary models such as GPT-Image-1.</li>
<li><strong>摘要：</strong>我们推出 UniGen-1.5，这是一种用于高级图像理解、生成和编辑的统一多模态大语言模型 (MLLM)。在UniGen的基础上，我们全面增强了模型架构和训练流程，以增强图像理解和生成能力，同时释放强大的图像编辑能力。特别是，我们提出了一种统一的强化学习（RL）策略，通过共享奖励模型共同改进图像生成和图像编辑。为了进一步增强图像编辑性能，我们提出了一个轻型编辑指令对齐阶段，该阶段可以显着提高编辑指令的理解力，这对于 RL 训练的成功至关重要。实验结果表明 UniGen-1.5 表现出有竞争力的理解和生成性能。具体来说，UniGen-1.5 在 GenEval 和 ImgEdit 上取得了 0.89 和 4.31 的总分，超越了 BAGEL 等最先进的模型，并达到了与 GPT-Image-1 等专有模型相当的性能。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
