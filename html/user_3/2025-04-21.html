<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-04-21</h1>
<h3>Title: Wavelet-based Variational Autoencoders for High-Resolution Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Andrew Kiruluta</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.13214">https://arxiv.org/abs/2504.13214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.13214">https://arxiv.org/pdf/2504.13214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.13214]] Wavelet-based Variational Autoencoders for High-Resolution Image Generation(https://arxiv.org/abs/2504.13214)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Variational Autoencoders (VAEs) are powerful generative models capable of learning compact latent representations. However, conventional VAEs often generate relatively blurry images due to their assumption of an isotropic Gaussian latent space and constraints in capturing high-frequency details. In this paper, we explore a novel wavelet-based approach (Wavelet-VAE) in which the latent space is constructed using multi-scale Haar wavelet coefficients. We propose a comprehensive method to encode the image features into multi-scale detail and approximation coefficients and introduce a learnable noise parameter to maintain stochasticity. We thoroughly discuss how to reformulate the reparameterization trick, address the KL divergence term, and integrate wavelet sparsity principles into the training objective. Our experimental evaluation on CIFAR-10 and other high-resolution datasets demonstrates that the Wavelet-VAE improves visual fidelity and recovers higher-resolution details compared to conventional VAEs. We conclude with a discussion of advantages, potential limitations, and future research directions for wavelet-based generative modeling.</li>
<li><strong>摘要：</strong>变分自动编码器（VAE）是能够学习紧凑的潜在表示的强大生成模型。但是，传统的VAE由于假设各向同性高斯潜在空间以及捕获高频细节时的限制，通常会产生相对模糊的图像。在本文中，我们探讨了一种基于小波的方法（小波VAE），其中潜在空间是使用多尺度HAAR小波系数构建的。我们提出了一种综合方法，将图像特征编码为多尺度细节和近似系数，并引入可学习的噪声参数以维持随机性。我们彻底讨论了如何重新重新重新参数化的技巧，解决KL分歧术语，并将小波稀疏原理整合到训练目标中。我们对CIFAR-10和其他高分辨率数据集的实验评估表明，与常规VAE相比，小波VAE可以改善视觉效果并恢复高分辨率的细节。最后，我们讨论了基于小波的生成建模的优势，潜在局限性和未来的研究方向。</li>
</ul>

<h3>Title: Auto-FEDUS: Autoregressive Generative Modeling of Doppler Ultrasound Signals from Fetal Electrocardiograms</h3>
<ul>
<li><strong>Authors: </strong>Alireza Rafiei, Gari D. Clifford, Nasim Katebi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.13233">https://arxiv.org/abs/2504.13233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.13233">https://arxiv.org/pdf/2504.13233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.13233]] Auto-FEDUS: Autoregressive Generative Modeling of Doppler Ultrasound Signals from Fetal Electrocardiograms(https://arxiv.org/abs/2504.13233)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, quality assessment</a></li>
<li><strong>Abstract: </strong>Fetal health monitoring through one-dimensional Doppler ultrasound (DUS) signals offers a cost-effective and accessible approach that is increasingly gaining interest. Despite its potential, the development of machine learning based techniques to assess the health condition of mothers and fetuses using DUS signals remains limited. This scarcity is primarily due to the lack of extensive DUS datasets with a reliable reference for interpretation and data imbalance across different gestational ages. In response, we introduce a novel autoregressive generative model designed to map fetal electrocardiogram (FECG) signals to corresponding DUS waveforms (Auto-FEDUS). By leveraging a neural temporal network based on dilated causal convolutions that operate directly on the waveform level, the model effectively captures both short and long-range dependencies within the signals, preserving the integrity of generated data. Cross-subject experiments demonstrate that Auto-FEDUS outperforms conventional generative architectures across both time and frequency domain evaluations, producing DUS signals that closely resemble the morphology of their real counterparts. The realism of these synthesized signals was further gauged using a quality assessment model, which classified all as good quality, and a heart rate estimation model, which produced comparable results for generated and real data, with a Bland-Altman limit of 4.5 beats per minute. This advancement offers a promising solution for mitigating limited data availability and enhancing the training of DUS-based fetal models, making them more effective and generalizable.</li>
<li><strong>摘要：</strong>通过一维多普勒超声（DUS）信号进行胎儿健康监测提供了一种经济高效且易于访问的方法，越来越引起人们的兴趣。尽管具有潜力，但基于机器学习的技术来评估使用DUS信号评估母亲和胎儿的健康状况的技术仍然有限。这种稀缺性主要是由于缺乏广泛的DUS数据集所致，并且对不同胎龄的解释和数据不平衡有了可靠的参考。作为响应，我们引入了一种新型的自回旋生成模型，该模型旨在将胎儿心电图（FECG）信号映射到相应的DUS波形（自动填充）。通过利用基于直接在波形水平运行的扩张因果卷积的神经时间网络，该模型可以有效地捕获信号内的短和远程依赖性，从而保留了生成数据的完整性。跨主题实验表明，自动效果在时间和频域评估上都优于常规生成体系结构，从而产生的DUS信号非常类似于其真实对应物的形态。这些综合信号的现实主义使用质量评估模型进一步衡量，该模型将所有质量分类为良好的质量，而心率估计模型为生成和真实数据产生了可比的结果，平淡的altman限制为每分钟4.5次。这一进步提供了一种有希望的解决方案，可减轻有限的数据可用性并增强基于DUS的胎儿模型的培训，从而使其更有效和可推广。</li>
</ul>

<h3>Title: ChartQA-X: Generating Explanations for Charts</h3>
<ul>
<li><strong>Authors: </strong>Shamanthak Hegde, Pooyan Fazli, Hasti Seifi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.13275">https://arxiv.org/abs/2504.13275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.13275">https://arxiv.org/pdf/2504.13275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.13275]] ChartQA-X: Generating Explanations for Charts(https://arxiv.org/abs/2504.13275)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The ability to interpret and explain complex information from visual data in charts is crucial for data-driven decision-making. In this work, we address the challenge of providing explanations alongside answering questions about chart images. We present ChartQA-X, a comprehensive dataset comprising various chart types with 28,299 contextually relevant questions, answers, and detailed explanations. These explanations are generated by prompting six different models and selecting the best responses based on metrics such as faithfulness, informativeness, coherence, and perplexity. Our experiments show that models fine-tuned on our dataset for explanation generation achieve superior performance across various metrics and demonstrate improved accuracy in question-answering tasks on new datasets. By integrating answers with explanatory narratives, our approach enhances the ability of intelligent agents to convey complex information effectively, improve user understanding, and foster trust in the generated responses.</li>
<li><strong>摘要：</strong>从图表中的视觉数据中解释和解释复杂信息的能力对于数据驱动的决策至关重要。在这项工作中，我们解决了提供解释以及回答有关图表图像的问题的挑战。我们提出ChartQA-X，这是一个综合数据集，其中包括各种图表类型，具有28,299个上下文相关的问题，答案和详细说明。这些解释是通过提示六个不同模型并根据忠诚，信息性，连贯性和困惑等指标选择最佳响应来产生的。我们的实验表明，在我们的数据集上进行了微调以说明生成的模型在各种指标上实现了卓越的性能，并在新数据集上证明了提高问题的准确性。通过将答案与解释性叙述相结合，我们的方法增强了智能代理有效传达复杂信息，改善用户理解并促进对生成的答案的信任的能力。</li>
</ul>

<h3>Title: POET: Supporting Prompting Creativity and Personalization with Automated Expansion of Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Evans Xu Han, Alice Qian Zhang, Hong Shen, Haiyi Zhu, Paul Pu Liang, Jane Hsieh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.13392">https://arxiv.org/abs/2504.13392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.13392">https://arxiv.org/pdf/2504.13392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.13392]] POET: Supporting Prompting Creativity and Personalization with Automated Expansion of Text-to-Image Generation(https://arxiv.org/abs/2504.13392)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>State-of-the-art visual generative AI tools hold immense potential to assist users in the early ideation stages of creative tasks -- offering the ability to generate (rather than search for) novel and unprecedented (instead of existing) images of considerable quality that also adhere to boundless combinations of user specifications. However, many large-scale text-to-image systems are designed for broad applicability, yielding conventional output that may limit creative exploration. They also employ interaction methods that may be difficult for beginners. Given that creative end users often operate in diverse, context-specific ways that are often unpredictable, more variation and personalization are necessary. We introduce POET, a real-time interactive tool that (1) automatically discovers dimensions of homogeneity in text-to-image generative models, (2) expands these dimensions to diversify the output space of generated images, and (3) learns from user feedback to personalize expansions. An evaluation with 28 users spanning four creative task domains demonstrated POET's ability to generate results with higher perceived diversity and help users reach satisfaction in fewer prompts during creative tasks, thereby prompting them to deliberate and reflect more on a wider range of possible produced results during the co-creative process. Focusing on visual creativity, POET offers a first glimpse of how interaction techniques of future text-to-image generation tools may support and align with more pluralistic values and the needs of end users during the ideation stages of their work.</li>
<li><strong>摘要：</strong>最先进的视觉生成AI工具具有巨大的潜力，可以帮助用户进入创意任务的早期构想阶段 - 提供了生成（而不是搜索）新颖和前所未有的（而不是现有）相当质量的图像的能力，这些图像也符合无限的用户规格组合。但是，许多大规模的文本到图像系统都是为了广泛的适用性而设计的，可以产生可能限制创意探索的常规输出。他们还采用了对初学者可能很难的互动方法。鉴于创造性最终用户通常以多种多样的，特定于上下文的方式运行，这些方式通常是不可预测的，因此需要更多的变化和个性化。我们介绍了诗人，这是一种实时交互式工具，（1）自动发现文本到图像生成模型中同质性的维度，（2）扩展这些维度以使生成图像的输出空间多样化，（3）从用户反馈中学习以个性化扩展。跨四个创意任务域的28个用户的评估证明了诗人能够在创造性任务期间较少提示的较高的提示中产生结果的能力，从而提示他们在共同创造过程中进行故意并更多地反思可能产生的结果。诗人着眼于视觉创造力，首先了解了未来文本到图像生成工具的交互技术如何支持并与更多元化的价值观以及在其作品的构想阶段相符的最终用户的需求。</li>
</ul>

<h3>Title: Equilibrium Conserving Neural Operators for Super-Resolution Learning</h3>
<ul>
<li><strong>Authors: </strong>Vivek Oommen, Andreas E. Robertson, Daniel Diaz, Coleman Alleman, Zhen Zhang, Anthony D. Rollett, George E. Karniadakis, Rémi Dingreville</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.13422">https://arxiv.org/abs/2504.13422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.13422">https://arxiv.org/pdf/2504.13422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.13422]] Equilibrium Conserving Neural Operators for Super-Resolution Learning(https://arxiv.org/abs/2504.13422)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Neural surrogate solvers can estimate solutions to partial differential equations in physical problems more efficiently than standard numerical methods, but require extensive high-resolution training data. In this paper, we break this limitation; we introduce a framework for super-resolution learning in solid mechanics problems. Our approach allows one to train a high-resolution neural network using only low-resolution data. Our Equilibrium Conserving Operator (ECO) architecture embeds known physics directly into the network to make up for missing high-resolution information during training. We evaluate this ECO-based super-resolution framework that strongly enforces conservation-laws in the predicted solutions on two working examples: embedded pores in a homogenized matrix and randomly textured polycrystalline materials. ECO eliminates the reliance on high-fidelity data and reduces the upfront cost of data collection by two orders of magnitude, offering a robust pathway for resource-efficient surrogate modeling in materials modeling. ECO is readily generalizable to other physics-based problems.</li>
<li><strong>摘要：</strong>与标准数值方法相比，神经替代求解器可以更有效地估算物理问题中的部分微分方程，但需要广泛的高分辨率培训数据。在本文中，我们打破了这一限制。我们介绍了一个用于固体力学问题的超分辨率学习的框架。我们的方法允许仅使用低分辨率数据训练高分辨率神经网络。我们的平衡保护操作员（ECO）体系结构将已知物理学直接嵌入网络中，以弥补培训期间缺少高分辨率信息。我们评估了这个基于ECO的超分辨率框架，该框架在两个工作示例中强烈强烈执行了预测的解决方案：嵌入式孔中的嵌合孔和随机纹理质感的多晶材料。 Eco消除了对高保真数据的依赖，并将数据收集的前期成本降低了两个数量级，为材料建模中的资源有效替代建模提供了强大的途径。 ECO很容易推广到其他基于物理的问题。</li>
</ul>

<h3>Title: Bounded and Uniform Energy-based Out-of-distribution Detection for Graphs</h3>
<ul>
<li><strong>Authors: </strong>Shenzhi Yang, Bin Liang, An Liu, Lin Gui, Xingkai Yao, Xiaofang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.13429">https://arxiv.org/abs/2504.13429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.13429">https://arxiv.org/pdf/2504.13429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.13429]] Bounded and Uniform Energy-based Out-of-distribution Detection for Graphs(https://arxiv.org/abs/2504.13429)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Given the critical role of graphs in real-world applications and their high-security requirements, improving the ability of graph neural networks (GNNs) to detect out-of-distribution (OOD) data is an urgent research problem. The recent work GNNSAFE proposes a framework based on the aggregation of negative energy scores that significantly improves the performance of GNNs to detect node-level OOD data. However, our study finds that score aggregation among nodes is susceptible to extreme values due to the unboundedness of the negative energy scores and logit shifts, which severely limits the accuracy of GNNs in detecting node-level OOD data. In this paper, we propose NODESAFE: reducing the generation of extreme scores of nodes by adding two optimization terms that make the negative energy scores bounded and mitigate the logit shift. Experimental results show that our approach dramatically improves the ability of GNNs to detect OOD data at the node level, e.g., in detecting OOD data induced by Structure Manipulation, the metric of FPR95 (lower is better) in scenarios without (with) OOD data exposure are reduced from the current SOTA by 28.4% (22.7%).</li>
<li><strong>摘要：</strong>鉴于图在现实世界应用中的关键作用及其高安全性要求，提高图形神经网络（GNNS）检测脱离分布（OOD）数据的能力是一个紧迫的研究问题。 GNNSAFE最近的工作提出了一个基于负能量评分的聚合的框架，该框架显着提高了GNN的性能以检测节点级别的OOD数据。但是，我们的研究发现，由于负能量得分和logit偏移的无限性，节点之间的得分聚集易受极端值，这严重限制了GNN在检测节点级别的OOD数据中的准确性。在本文中，我们提出了Nodesafe：通过添加两个优化术语来减少节点的极端分数的产生，从而使负能量得分界定并减轻logit移位。实验结果表明，我们的方法极大地提高了GNN在节点级别检测OOD数据的能力，例如，在检测由结构操作诱导的OOD数据时，FPR95的度量（在没有（带有）OOD数据暴露的情况下，FPR95的度量较低（较低）从当前的SOTA中减少了28.4％（22.7％）。</li>
</ul>

<h3>Title: Circular Image Deturbulence using Quasi-conformal Geometry</h3>
<ul>
<li><strong>Authors: </strong>Chu Chen, Han Zhang, Lok Ming Lui</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.13432">https://arxiv.org/abs/2504.13432</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.13432">https://arxiv.org/pdf/2504.13432</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.13432]] Circular Image Deturbulence using Quasi-conformal Geometry(https://arxiv.org/abs/2504.13432)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>The presence of inhomogeneous media between optical sensors and objects leads to distorted imaging outputs, significantly complicating downstream image-processing tasks. A key challenge in image restoration is the lack of high-quality, paired-label images required for training supervised models. In this paper, we introduce the Circular Quasi-Conformal Deturbulence (CQCD) framework, an unsupervised approach for removing image distortions through a circular architecture. This design ensures that the restored image remains both geometrically accurate and visually faithful while preventing the accumulation of incorrect this http URL circular restoration process involves both forward and inverse mapping. To ensure the bijectivity of the estimated non-rigid deformations, computational quasi-conformal geometry theories are leveraged to regularize the mapping, enforcing its homeomorphic properties. This guarantees a well-defined transformation that preserves structural integrity and prevents unwanted artifacts. Furthermore, tight-frame blocks are integrated to encode distortion-sensitive features for precise recovery. To validate the performance of our approach, we conduct evaluations on various synthetic and real-world captured images. Experimental results demonstrate that CQCD not only outperforms existing state-of-the-art deturbulence methods in terms of image restoration quality but also provides highly accurate deformation field estimations.</li>
<li><strong>摘要：</strong>光传感器和物体之间存在不均匀介质会导致成像输出扭曲，从而使下游图像处理任务显着复杂化。图像修复中的一个关键挑战是缺乏训练监督模型所需的高质量，配对的图像。在本文中，我们介绍了圆形准符合偏移率（CQCD）框架，这是一种无监督的方法，用于通过圆形结构消除图像扭曲。该设计可确保恢复的图像在几何上保持准确和视觉忠实，同时又可以防止此HTTP URL圆形恢复过程的积累不正确，涉及前进和逆映射。为了确保估计的非刚性变形的射击性，将计算准式形式的几何理论杠杆化以使映射正规化，从而实现其同质形态特性。这确保了定义明确的转换，可保留结构完整性并防止不必要的人工制品。此外，将紧密框架集成到编码失真敏感的特征以进行精确恢复。为了验证我们的方法的性能，我们对各种合成和现实世界捕获的图像进行评估。实验结果表明，在图像恢复质量方面，CQCD不仅胜过现有的最新替换方法，而且还提供了高度准确的变形场估计。</li>
</ul>

<h3>Title: Early Timestep Zero-Shot Candidate Selection for Instruction-Guided Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Joowon Kim, Ziseok Lee, Donghyeon Cho, Sanghyun Jo, Yeonsung Jung, Kyungsu Kim, Eunho Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.13490">https://arxiv.org/abs/2504.13490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.13490">https://arxiv.org/pdf/2504.13490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.13490]] Early Timestep Zero-Shot Candidate Selection for Instruction-Guided Image Editing(https://arxiv.org/abs/2504.13490)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite recent advances in diffusion models, achieving reliable image generation and editing remains challenging due to the inherent diversity induced by stochastic noise in the sampling process. Instruction-guided image editing with diffusion models offers user-friendly capabilities, yet editing failures, such as background distortion, frequently occur. Users often resort to trial and error, adjusting seeds or prompts to achieve satisfactory results, which is inefficient. While seed selection methods exist for Text-to-Image (T2I) generation, they depend on external verifiers, limiting applicability, and evaluating multiple seeds increases computational complexity. To address this, we first establish a multiple-seed-based image editing baseline using background consistency scores, achieving Best-of-N performance without supervision. Building on this, we introduce ELECT (Early-timestep Latent Evaluation for Candidate Selection), a zero-shot framework that selects reliable seeds by estimating background mismatches at early diffusion timesteps, identifying the seed that retains the background while modifying only the foreground. ELECT ranks seed candidates by a background inconsistency score, filtering unsuitable samples early based on background consistency while preserving editability. Beyond standalone seed selection, ELECT integrates into instruction-guided editing pipelines and extends to Multimodal Large-Language Models (MLLMs) for joint seed and prompt selection, further improving results when seed selection alone is insufficient. Experiments show that ELECT reduces computational costs (by 41 percent on average and up to 61 percent) while improving background consistency and instruction adherence, achieving around 40 percent success rates in previously failed cases - without any external supervision or training.</li>
<li><strong>摘要：</strong>尽管扩散模型最近取得了进步，但由于采样过程中随机噪声引起的固有多样性，实现可靠的图像产生和编辑仍然具有挑战性。使用扩散模型的指令指导的图像编辑提供了用户友好的功能，但编辑失败（例如背景失真）经常发生。用户通常会诉诸反复试验，调整种子或提示以达到令人满意的结果，这是效率低下的结果。尽管存在用于文本图像（T2I）生成的种子选择方法，但它们依赖于外部验证器，限制适用性并评估多种种子会增加计算复杂性。为了解决这个问题，我们首先使用背景一致性分数建立一个基于多物种的图像编辑基线，从而在没有监督的情况下实现了最佳N性能。在此基础上，我们介绍了选举（早期的潜在评估候选人选择），这是一个零拍的框架，通过在早期扩散时间段上估算背景不匹配来选择可靠的种子，从而识别保留背景的种子，同时仅修改前景。选举通过背景不一致的评分对种子候选者进行排名，并根据背景一致性提早过滤不合适的样本，同时保持编辑性。除了独立种子选择之外，选举中还将指导指导的编辑管道集成到多模式大型模型（MLLMS）中进行关节种子和及时选择，当单独选择种子选择不足时，进一步改善了结果。实验表明，选举将计算成本降低（平均为41％，最高61％），同时提高背景一致性和指导依从性，在以前失败的情况下达到了40％的成功率，而没有任何外部监督或培训。</li>
</ul>

<h3>Title: U-Shape Mamba: State Space Model for faster diffusion</h3>
<ul>
<li><strong>Authors: </strong>Alex Ergasti, Filippo Botti, Tomaso Fontanini, Claudio Ferrari, Massimo Bertozzi, Andrea Prati</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.13499">https://arxiv.org/abs/2504.13499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.13499">https://arxiv.org/pdf/2504.13499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.13499]] U-Shape Mamba: State Space Model for faster diffusion(https://arxiv.org/abs/2504.13499)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have become the most popular approach for high-quality image generation, but their high computational cost still remains a significant challenge. To address this problem, we propose U-Shape Mamba (USM), a novel diffusion model that leverages Mamba-based layers within a U-Net-like hierarchical structure. By progressively reducing sequence length in the encoder and restoring it in the decoder through Mamba blocks, USM significantly lowers computational overhead while maintaining strong generative capabilities. Experimental results against Zigma, which is currently the most efficient Mamba-based diffusion model, demonstrate that USM achieves one-third the GFlops, requires less memory and is faster, while outperforming Zigma in image quality. Frechet Inception Distance (FID) is improved by 15.3, 0.84 and 2.7 points on AFHQ, CelebAHQ and COCO datasets, respectively. These findings highlight USM as a highly efficient and scalable solution for diffusion-based generative models, making high-quality image synthesis more accessible to the research community while reducing computational costs.</li>
<li><strong>摘要：</strong>扩散模型已成为高质量图像产生的最受欢迎的方法，但是它们的高计算成本仍然是一个重大挑战。为了解决这个问题，我们提出了U-Shape Mamba（USM），这是一种新型扩散模型，该模型利用了U-NET样层次结构内利用基于Mamba的层。通过逐步降低编码器中的序列长度并通过Mamba块在解码器中恢复它，USM显着降低了计算开销，同时保持强大的生成能力。针对Zigma的实验结果是目前是最有效的基于MAMBA的扩散模型，它表明USM获得了三分之一的GFLOPS，需要更少的内存，并且更快，同时表现出比图像质量的Zigma的表现。 Frechet Inception距离（FID）分别在AFHQ，Celebahq和可可数据集上提高了15.3、0.84和2.7点。这些发现将USM强调为基于扩散的生成模型的高效和可扩展的解决方案，从而使研究社区更容易访问高质量的图像合成，同时降低了计算成本。</li>
</ul>

<h3>Title: Zero-Shot Industrial Anomaly Segmentation with Image-Aware Prompt Generation</h3>
<ul>
<li><strong>Authors: </strong>SoYoung Park, Hyewon Lee, Mingyu Choi, Seunghoon Han, Jong-Ryul Lee, Sungsu Lim, Tae-Ho Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.13560">https://arxiv.org/abs/2504.13560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.13560">https://arxiv.org/pdf/2504.13560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.13560]] Zero-Shot Industrial Anomaly Segmentation with Image-Aware Prompt Generation(https://arxiv.org/abs/2504.13560)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Anomaly segmentation is essential for industrial quality, maintenance, and stability. Existing text-guided zero-shot anomaly segmentation models are effective but rely on fixed prompts, limiting adaptability in diverse industrial scenarios. This highlights the need for flexible, context-aware prompting strategies. We propose Image-Aware Prompt Anomaly Segmentation (IAP-AS), which enhances anomaly segmentation by generating dynamic, context-aware prompts using an image tagging model and a large language model (LLM). IAP-AS extracts object attributes from images to generate context-aware prompts, improving adaptability and generalization in dynamic and unstructured industrial environments. In our experiments, IAP-AS improves the F1-max metric by up to 10%, demonstrating superior adaptability and generalization. It provides a scalable solution for anomaly segmentation across industries</li>
<li><strong>摘要：</strong>异常细分对于工业质量，维护和稳定性至关重要。现有的文本引导的零射线分段模型是有效的，但依赖于固定提示，从而限制了各种工业场景的适应性。这凸显了需要灵活的，上下文感知的提示策略。我们提出了图像感知的提示异常分割（IAP-AS），该分段通过使用图像标记模型和大型语言模型（LLM）生成动态，上下文感知提示来增强异常分割。 IAP-AS提取物从图像中的对象属性生成上下文感知的提示，改善动态和非结构化工业环境中的适应性和概括。在我们的实验中，IAP-AS将F1-MAX指标提高了10％，表现出了卓越的适应性和泛化。它为跨行业的异常细分提供了可扩展的解决方案</li>
</ul>

<h3>Title: WeatherGen: A Unified Diverse Weather Generator for LiDAR Point Clouds via Spider Mamba Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Yang Wu, Yun Zhu, Kaihua Zhang, Jianjun Qian, Jin Xie, Jian Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.13561">https://arxiv.org/abs/2504.13561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.13561">https://arxiv.org/pdf/2504.13561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.13561]] WeatherGen: A Unified Diverse Weather Generator for LiDAR Point Clouds via Spider Mamba Diffusion(https://arxiv.org/abs/2504.13561)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>3D scene perception demands a large amount of adverse-weather LiDAR data, yet the cost of LiDAR data collection presents a significant scaling-up challenge. To this end, a series of LiDAR simulators have been proposed. Yet, they can only simulate a single adverse weather with a single physical model, and the fidelity of the generated data is quite limited. This paper presents WeatherGen, the first unified diverse-weather LiDAR data diffusion generation framework, significantly improving fidelity. Specifically, we first design a map-based data producer, which can provide a vast amount of high-quality diverse-weather data for training purposes. Then, we utilize the diffusion-denoising paradigm to construct a diffusion model. Among them, we propose a spider mamba generator to restore the disturbed diverse weather data gradually. The spider mamba models the feature interactions by scanning the LiDAR beam circle or central ray, excellently maintaining the physical structure of the LiDAR data. Subsequently, following the generator to transfer real-world knowledge, we design a latent feature aligner. Afterward, we devise a contrastive learning-based controller, which equips weather control signals with compact semantic knowledge through language supervision, guiding the diffusion model to generate more discriminative data. Extensive evaluations demonstrate the high generation quality of WeatherGen. Through WeatherGen, we construct the mini-weather dataset, promoting the performance of the downstream task under adverse weather conditions. Code is available: this https URL</li>
<li><strong>摘要：</strong>3D场景的感知需要大量的不良林中数据，但是LIDAR数据收集的成本提出了一个重大的扩大挑战。为此，已经提出了一系列激光雷达模拟器。但是，他们只能通过单个物理模型模拟单个不利天气，而生成的数据的保真度非常有限。本文介绍了Weathergen，这是第一个统一的多元化激光雷达数据扩散生成框架，可大大提高保真度。具体来说，我们首先设计了一个基于地图的数据生产商，该数据生产商可以为培训目的提供大量高质量的不同天气数据。然后，我们利用扩散降解范式来构建扩散模型。其中，我们提出了一个蜘蛛Mamba发电机，以逐渐恢复受干扰的不同天气数据。蜘蛛mamba通过扫描激光束圆或中央射线来对特征相互作用进行建模，从而很好地维护LIDAR数据的物理结构。随后，遵循发电机传输现实世界知识，我们设计了一个潜在的功能对齐器。之后，我们设计了一个基于对比的学习控制器，该控制器通过语言监督将天气控制信号与紧凑的语义知识配置，从而指导扩散模型以生成更具歧视性数据。广泛的评估证明了Weathergen的高发电质量。通过Weathergen，我们构建了迷你天气数据集，从而在不利天气条件下促进了下游任务的性能。代码可用：此HTTPS URL</li>
</ul>

<h3>Title: Entropic Time Schedulers for Generative Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Dejan Stancevic, Luca Ambrogioni</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.13612">https://arxiv.org/abs/2504.13612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.13612">https://arxiv.org/pdf/2504.13612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.13612]] Entropic Time Schedulers for Generative Diffusion Models(https://arxiv.org/abs/2504.13612)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The practical performance of generative diffusion models depends on the appropriate choice of the noise scheduling function, which can also be equivalently expressed as a time reparameterization. In this paper, we present a time scheduler that selects sampling points based on entropy rather than uniform time spacing, ensuring that each point contributes an equal amount of information to the final generation. We prove that this time reparameterization does not depend on the initial choice of time. Furthermore, we provide a tractable exact formula to estimate this \emph{entropic time} for a trained model using the training loss without substantial overhead. Alongside the entropic time, inspired by the optimality results, we introduce a rescaled entropic time. In our experiments with mixtures of Gaussian distributions and ImageNet, we show that using the (rescaled) entropic times greatly improves the inference performance of trained models. In particular, we found that the image quality in pretrained EDM2 models, as evaluated by FID and FD-DINO scores, can be substantially increased by the rescaled entropic time reparameterization without increasing the number of function evaluations, with greater improvements in the few NFEs regime.</li>
<li><strong>摘要：</strong>生成扩散模型的实际性能取决于噪声调度函数的适当选择，这也可以等效地表示为时间重新聚集。在本文中，我们提出了一个时间调度程序，该时间调度程序根据熵而不是统一的时间间距选择采样点，以确保每个点为最终一代提供相同的信息。我们证明，这次重新聚集不取决于最初的时间选择。此外，我们提供了一个可拖动的精确公式，以估算使用训练损失没有实质性开销的训练损失的训练模型的此\ emph {entopic Time}。除熵时间外，受到最佳结果的启发，我们引入了一个重新续签的熵时间。在我们与高斯分布和成像网的混合物的实验中，我们表明，使用（重新缩放的）熵时代可以大大提高受过训练的模型的推理性能。特别是，我们发现，经过FID和FD-DINO分数评估的经过预定的EDM2模型中的图像质量可以通过重新验证的熵时间重新聚集化而大大提高，而不会增加函数评估的数量，并且在少数NFES方案中的改进都会有所改善。</li>
</ul>

<h3>Title: AnyTSR: Any-Scale Thermal Super-Resolution for UAV</h3>
<ul>
<li><strong>Authors: </strong>Mengyuan Li, Changhong Fu, Ziyu Lu, Zijie Zhang, Haobo Zuo, Liangliang Yao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.13682">https://arxiv.org/abs/2504.13682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.13682">https://arxiv.org/pdf/2504.13682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.13682]] AnyTSR: Any-Scale Thermal Super-Resolution for UAV(https://arxiv.org/abs/2504.13682)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Thermal imaging can greatly enhance the application of intelligent unmanned aerial vehicles (UAV) in challenging environments. However, the inherent low resolution of thermal sensors leads to insufficient details and blurred boundaries. Super-resolution (SR) offers a promising solution to address this issue, while most existing SR methods are designed for fixed-scale SR. They are computationally expensive and inflexible in practical applications. To address above issues, this work proposes a novel any-scale thermal SR method (AnyTSR) for UAV within a single model. Specifically, a new image encoder is proposed to explicitly assign specific feature code to enable more accurate and flexible representation. Additionally, by effectively embedding coordinate offset information into the local feature ensemble, an innovative any-scale upsampler is proposed to better understand spatial relationships and reduce artifacts. Moreover, a novel dataset (UAV-TSR), covering both land and water scenes, is constructed for thermal SR tasks. Experimental results demonstrate that the proposed method consistently outperforms state-of-the-art methods across all scaling factors as well as generates more accurate and detailed high-resolution images. The code is located at this https URL.</li>
<li><strong>摘要：</strong>热成像可以大大增强智能无人机（UAV）在具有挑战性的环境中的应用。但是，热传感器的固有低分辨率导致细节不足和边界模糊。超分辨率（SR）提供了一个有希望的解决方案来解决此问题，而大多数现有的SR方法都是为固定尺度SR设计的。它们在计算上很昂贵且在实际应用中不灵活。为了解决上述问题，这项工作提出了一个新颖的单个模型中无人机的任何规模热SR方法（Anytsr）。具体而言，提出了一个新的图像编码器，以明确分配特定的特征代码，以实现更准确，更灵活的表示。此外，通过有效地将坐标偏移信息嵌入本地功能集合中，提出了创新的任何规模UPS采样器，以更好地理解空间关系并减少伪像。此外，为热SR任务构建了一个涵盖土地和水景的新型数据集（UAV-TSR）。实验结果表明，所提出的方法始终超过所有缩放因素的最先进方法，并生成更准确和详细的高分辨率图像。该代码位于此HTTPS URL。</li>
</ul>

<h3>Title: MLEP: Multi-granularity Local Entropy Patterns for Universal AI-generated Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Lin Yuan, Xiaowan Li, Yan Zhang, Jiawei Zhang, Hongbo Li, Xinbo Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.13726">https://arxiv.org/abs/2504.13726</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.13726">https://arxiv.org/pdf/2504.13726</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.13726]] MLEP: Multi-granularity Local Entropy Patterns for Universal AI-generated Image Detection(https://arxiv.org/abs/2504.13726)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Advancements in image generation technologies have raised significant concerns about their potential misuse, such as producing misinformation and deepfakes. Therefore, there is an urgent need for effective methods to detect AI-generated images (AIGI). Despite progress in AIGI detection, achieving reliable performance across diverse generation models and scenes remains challenging due to the lack of source-invariant features and limited generalization capabilities in existing methods. In this work, we explore the potential of using image entropy as a cue for AIGI detection and propose Multi-granularity Local Entropy Patterns (MLEP), a set of entropy feature maps computed across shuffled small patches over multiple image scaled. MLEP comprehensively captures pixel relationships across dimensions and scales while significantly disrupting image semantics, reducing potential content bias. Leveraging MLEP, a robust CNN-based classifier for AIGI detection can be trained. Extensive experiments conducted in an open-world scenario, evaluating images synthesized by 32 distinct generative models, demonstrate significant improvements over state-of-the-art methods in both accuracy and generalization.</li>
<li><strong>摘要：</strong>图像产生技术的进步引起了人们对它们潜在滥用的重大关注，例如产生错误信息和深层效果。因此，迫切需要有效的方法来检测AI生成的图像（AIGI）。尽管AIGI检测取得了进展，但由于缺乏源不变特征和现有方法中的概括能力有限，在不同的一代模型和场景中实现可靠的性能仍然具有挑战性。在这项工作中，我们探讨了将图像熵用作AIGI检测的提示的潜力，并提出了多个局部熵模式（MLEP），这是一组熵特征图，该特征在多个图像缩放上计算出跨洗牌的小斑块计算出来。 MLEP全面捕获跨维度和尺度的像素关系，同时显着破坏图像语义，从而减少潜在的内容偏差。可以训练利用MLEP，这是一个基于CNN的强大分类器进行AIGI检测。在开放世界的情况下进行的广泛实验，评估了由32种不同生成模型合成的图像，在准确性和概括方面表现出对最先进方法的显着改善。</li>
</ul>

<h3>Title: ESPLoRA: Enhanced Spatial Precision with Low-Rank Adaption in Text-to-Image Diffusion Models for High-Definition Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Andrea Rigo, Luca Stornaiuolo, Mauro Martino, Bruno Lepri, Nicu Sebe</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.13745">https://arxiv.org/abs/2504.13745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.13745">https://arxiv.org/pdf/2504.13745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.13745]] ESPLoRA: Enhanced Spatial Precision with Low-Rank Adaption in Text-to-Image Diffusion Models for High-Definition Synthesis(https://arxiv.org/abs/2504.13745)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have revolutionized text-to-image (T2I) synthesis, producing high-quality, photorealistic images. However, they still struggle to properly render the spatial relationships described in text prompts. To address the lack of spatial information in T2I generations, existing methods typically use external network conditioning and predefined layouts, resulting in higher computational costs and reduced flexibility. Our approach builds upon a curated dataset of spatially explicit prompts, meticulously extracted and synthesized from LAION-400M to ensure precise alignment between textual descriptions and spatial layouts. Alongside this dataset, we present ESPLoRA, a flexible fine-tuning framework based on Low-Rank Adaptation, specifically designed to enhance spatial consistency in generative models without increasing generation time or compromising the quality of the outputs. In addition to ESPLoRA, we propose refined evaluation metrics grounded in geometric constraints, capturing 3D spatial relations such as \textit{in front of} or \textit{behind}. These metrics also expose spatial biases in T2I models which, even when not fully mitigated, can be strategically exploited by our TORE algorithm to further improve the spatial consistency of generated images. Our method outperforms the current state-of-the-art framework, CoMPaSS, by 13.33% on established spatial consistency benchmarks.</li>
<li><strong>摘要：</strong>扩散模型已彻底改变了文本对图像（T2I）的合成，从而产生了高质量的影像图像。但是，他们仍然很难正确地呈现文本提示中描述的空间关系。为了解决T2i代缺乏空间信息，现有方法通常使用外部网络条件和预定义的布局，从而导致更高的计算成本和降低的灵活性。我们的方法建立在策划的空间显式提示的数据集上，并从Laion-400m中精心提取并合成，以确保文本描述和空间布局之间的精确对齐。除此数据集外，我们提出了Esplora，这是一个基于低级适应性的灵活的微调框架，该框架专门设计用于增强生成模型中的空间一致性，而不会增加产生时间或损害输出质量。除Esplora外，我们还提出了基于几何约束的精致评估指标，并捕获了3D空间关系，例如\ textit {在}或\ textit {后面}。这些指标还暴露了T2I模型中的空间偏见，即使没有完全缓解，我们的撕裂算法也可以在策略上利用这些偏差，以进一步提高生成图像的空间一致性。在既定的空间一致性基准方面，我们的方法优于当前的最新框架指南针，指南针13.33％。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
