<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-22</h1>
<h3>Title: RespoDiff: Dual-Module Bottleneck Transformation for Responsible & Faithful T2I Generation</h3>
<ul>
<li><strong>Authors: </strong>Silpa Vadakkeeveetil Sreelatha, Sauradip Nag, Muhammad Awais, Serge Belongie, Anjan Dutta</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15257">https://arxiv.org/abs/2509.15257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15257">https://arxiv.org/pdf/2509.15257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15257]] RespoDiff: Dual-Module Bottleneck Transformation for Responsible & Faithful T2I Generation(https://arxiv.org/abs/2509.15257)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The rapid advancement of diffusion models has enabled high-fidelity and semantically rich text-to-image generation; however, ensuring fairness and safety remains an open challenge. Existing methods typically improve fairness and safety at the expense of semantic fidelity and image quality. In this work, we propose RespoDiff, a novel framework for responsible text-to-image generation that incorporates a dual-module transformation on the intermediate bottleneck representations of diffusion models. Our approach introduces two distinct learnable modules: one focused on capturing and enforcing responsible concepts, such as fairness and safety, and the other dedicated to maintaining semantic alignment with neutral prompts. To facilitate the dual learning process, we introduce a novel score-matching objective that enables effective coordination between the modules. Our method outperforms state-of-the-art methods in responsible generation by ensuring semantic alignment while optimizing both objectives without compromising image fidelity. Our approach improves responsible and semantically coherent generation by 20% across diverse, unseen prompts. Moreover, it integrates seamlessly into large-scale models like SDXL, enhancing fairness and safety. Code will be released upon acceptance.</li>
<li><strong>摘要：</strong>扩散模型的快速发展使高保真性和语义上丰富的文本对图像产生。但是，确保公平和安全仍然是一个公开挑战。现有方法通常以牺牲语义保真度和图像质量为代价提高公平和安全性。在这项工作中，我们提出了Restodiff，这是负责任的文本到图像生成的新型框架，该框架在扩散模型的中间瓶颈表示上结合了双模块转换。我们的方法介绍了两个不同的可学习模块：一个专注于捕获和执行负责任的概念，例如公平和安全，另一个致力于用中性提示保持语义一致性。为了促进双重学习过程，我们引入了一个新颖的分数匹配目标，该目标可以在模块之间有效协调。我们的方法通过确保语义对齐方式在不损害图像保真度的情况下优化两个目标的同时，优于负责任生成中最先进的方法。我们的方法在不同的，看不见的提示中将负责任的和语义上的一致产生提高了20％。此外，它将无缝集成到SDXL等大型模型中，从而增强了公平和安全性。代码将在接受后发布。</li>
</ul>

<h3>Title: Generative AI Meets Wireless Sensing: Towards Wireless Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Zheng Yang, Guoxuan Chi, Chenshu Wu, Hanyu Liu, Yuchong Gao, Yunhao Liu, Jie Xu, Tony Xiao Han</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15258">https://arxiv.org/abs/2509.15258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15258">https://arxiv.org/pdf/2509.15258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15258]] Generative AI Meets Wireless Sensing: Towards Wireless Foundation Model(https://arxiv.org/abs/2509.15258)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative Artificial Intelligence (GenAI) has made significant advancements in fields such as computer vision (CV) and natural language processing (NLP), demonstrating its capability to synthesize high-fidelity data and improve generalization. Recently, there has been growing interest in integrating GenAI into wireless sensing systems. By leveraging generative techniques such as data augmentation, domain adaptation, and denoising, wireless sensing applications, including device localization, human activity recognition, and environmental monitoring, can be significantly improved. This survey investigates the convergence of GenAI and wireless sensing from two complementary perspectives. First, we explore how GenAI can be integrated into wireless sensing pipelines, focusing on two modes of integration: as a plugin to augment task-specific models and as a solver to directly address sensing tasks. Second, we analyze the characteristics of mainstream generative models, such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and diffusion models, and discuss their applicability and unique advantages across various wireless sensing tasks. We further identify key challenges in applying GenAI to wireless sensing and outline a future direction toward a wireless foundation model: a unified, pre-trained design capable of scalable, adaptable, and efficient signal understanding across diverse sensing tasks.</li>
<li><strong>摘要：</strong>生成人工智能（Genai）在计算机视觉（CV）和自然语言处理（NLP）等领域取得了重大进步，证明了其合成高保真数据并改善概括的能力。最近，人们对将Genai整合到无线传感系统中一直越来越兴趣。通过利用生成技术，例如数据增强，域的适应性以及无线传感应用，包括设备定位，人类活动识别和环境监测，可以显着改善。这项调查从两个互补的角度研究了Genai和无线传感的收敛。首先，我们探讨了如何将Genai集成到无线传感管道中，重点关注两种集成模式：作为增强特定任务模型的插件，以及作为直接解决传感任务的求解器。其次，我们分析了主流生成模型的特征，例如生成对抗网络（GAN），变异自动编码器（VAE）和扩散模型，并讨论了各种无线传感任务的适用性和独特的优势。我们进一步确定了将Genai应用于无线传感和概述无线基础模型的未来方向的关键挑战：统一的，预先训练的设计，能够跨不同的感应任务，可扩展，适应性和有效的信号理解。</li>
</ul>

<h3>Title: Autoguided Online Data Curation for Diffusion Model Training</h3>
<ul>
<li><strong>Authors: </strong>Valeria Pais, Luis Oala, Daniele Faccio, Marco Aversa</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15267">https://arxiv.org/abs/2509.15267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15267">https://arxiv.org/pdf/2509.15267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15267]] Autoguided Online Data Curation for Diffusion Model Training(https://arxiv.org/abs/2509.15267)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The costs of generative model compute rekindled promises and hopes for efficient data curation. In this work, we investigate whether recently developed autoguidance and online data selection methods can improve the time and sample efficiency of training generative diffusion models. We integrate joint example selection (JEST) and autoguidance into a unified code base for fast ablation and benchmarking. We evaluate combinations of data curation on a controlled 2-D synthetic data generation task as well as (3x64x64)-D image generation. Our comparisons are made at equal wall-clock time and equal number of samples, explicitly accounting for the overhead of selection. Across experiments, autoguidance consistently improves sample quality and diversity. Early AJEST (applying selection only at the beginning of training) can match or modestly exceed autoguidance alone in data efficiency on both tasks. However, its time overhead and added complexity make autoguidance or uniform random data selection preferable in most situations. These findings suggest that while targeted online selection can yield efficiency gains in early training, robust sample quality improvements are primarily driven by autoguidance. We discuss limitations and scope, and outline when data selection may be beneficial.</li>
<li><strong>摘要：</strong>生成模型计算的成本重新点燃了有效数据策展的承诺和希望。在这项工作中，我们调查了最近开发的自动化和在线数据选择方法是否可以提高训练生成扩散模型的时间和样本效率。我们将联合示例选择（JEST）和自动化集成为快速消融和基准测试的统一代码库。我们在受控的2-D合成数据生成任务以及（3x64x64）-D图像生成上评估数据策划的组合。我们的比较是在相等的壁式时间和相等数量的样品时进行的，明确考虑了选择的开销。在整个实验中，自动化始终提高样本质量和多样性。早期的Ajest（仅在培训开始时仅应用选择）就可以匹配或适度超过这两个任务的数据效率。但是，它的时间开销和增加的复杂性使在大多数情况下更可取自动化或统一的随机数据选择。这些发现表明，尽管有针对性的在线选择可以在早期培训中带来效率的提高，但强大的样本质量改进主要是由自动化驱动的。我们讨论局限性和范围，并概述数据选择可能是有益的。</li>
</ul>

<h3>Title: PRISM: Phase-enhanced Radial-based Image Signature Mapping framework for fingerprinting AI-generated images</h3>
<ul>
<li><strong>Authors: </strong>Emanuele Ricco, Elia Onofri, Lorenzo Cima, Stefano Cresci, Roberto Di Pietro</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15270">https://arxiv.org/abs/2509.15270</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15270">https://arxiv.org/pdf/2509.15270</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15270]] PRISM: Phase-enhanced Radial-based Image Signature Mapping framework for fingerprinting AI-generated images(https://arxiv.org/abs/2509.15270)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>A critical need has emerged for generative AI: attribution methods. That is, solutions that can identify the model originating AI-generated content. This feature, generally relevant in multimodal applications, is especially sensitive in commercial settings where users subscribe to paid proprietary services and expect guarantees about the source of the content they receive. To address these issues, we introduce PRISM, a scalable Phase-enhanced Radial-based Image Signature Mapping framework for fingerprinting AI-generated images. PRISM is based on a radial reduction of the discrete Fourier transform that leverages amplitude and phase information to capture model-specific signatures. The output of the above process is subsequently clustered via linear discriminant analysis to achieve reliable model attribution in diverse settings, even if the model's internal details are inaccessible. To support our work, we construct PRISM-36K, a novel dataset of 36,000 images generated by six text-to-image GAN- and diffusion-based models. On this dataset, PRISM achieves an attribution accuracy of 92.04%. We additionally evaluate our method on four benchmarks from the literature, reaching an average accuracy of 81.60%. Finally, we evaluate our methodology also in the binary task of detecting real vs fake images, achieving an average accuracy of 88.41%. We obtain our best result on GenImage with an accuracy of 95.06%, whereas the original benchmark achieved 82.20%. Our results demonstrate the effectiveness of frequency-domain fingerprinting for cross-architecture and cross-dataset model attribution, offering a viable solution for enforcing accountability and trust in generative AI systems.</li>
<li><strong>摘要：</strong>生成AI：归因方法已经出现了关键需求。也就是说，可以识别源自AI生成内容的模型的解决方案。此功能通常在多模式应用程序中相关，在用户订阅付费专有服务并期望对他们收到的内容来源的保证的商业环境中特别敏感。为了解决这些问题，我们引入了Prism，这是一种可扩展的相位增强的基于径向的图像签名映射框架，用于指纹AI生成的图像。棱镜基于径向减少离散的傅立叶变换，该变换利用幅度和相位信息来捕获特定于模型的特定签名。随后，通过线性判别分析聚集了上述过程的输出，即使模型的内部细节无法访问，也可以在不同的设置中实现可靠的模型归因。为了支持我们的工作，我们构建了Prism-36K，这是一个由六个基于文本的gan和基于扩散的模型生成的36,000张图像的新型数据集。在此数据集上，棱镜达到了92.04％的归因精度。我们还对文献的四个基准进行评估我们的方法，平均准确性为81.60％。最后，我们在检测真实图像的二进制任务中也评估了我们的方法，达到了88.41％的平均准确性。我们以95.06％的准确性获得了Genimage的最佳结果，而原始基准的精度为82.20％。我们的结果证明了频域指纹对跨架构和跨数据库模型归因的有效性，为生成AI系统的责任制和信任提供了可行的解决方案。</li>
</ul>

<h3>Title: Partial Column Generation with Graph Neural Networks for Team Formation and Routing</h3>
<ul>
<li><strong>Authors: </strong>Giacomo Dall'Olio, Rainer Kolisch, Yaoxin Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15275">https://arxiv.org/abs/2509.15275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15275">https://arxiv.org/pdf/2509.15275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15275]] Partial Column Generation with Graph Neural Networks for Team Formation and Routing(https://arxiv.org/abs/2509.15275)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The team formation and routing problem is a challenging optimization problem with several real-world applications in fields such as airport, healthcare, and maintenance operations. To solve this problem, exact solution methods based on column generation have been proposed in the literature. In this paper, we propose a novel partial column generation strategy for settings with multiple pricing problems, based on predicting which ones are likely to yield columns with a negative reduced cost. We develop a machine learning model tailored to the team formation and routing problem that leverages graph neural networks for these predictions. Computational experiments demonstrate that applying our strategy enhances the solution method and outperforms traditional partial column generation approaches from the literature, particularly on hard instances solved under a tight time limit.</li>
<li><strong>摘要：</strong>在机场，医疗保健和维护操作等领域的几个现实世界应用程序中，团队组建和路由问题是一个具有挑战性的优化问题。为了解决这个问题，文献中已经提出了基于列生成的精确解决方案方法。在本文中，我们提出了一种针对具有多个定价问题的设置的新型部分列生成策略，基于预测哪些可能会产生降低成本负数的列。我们开发了一个针对团队组建和路由问题量身定制的机器学习模型，该模型利用图形神经网络进行这些预测。计算实验表明，应用我们的策略增强了解决方案方法，并优于文献中传统的部分列生成方法，尤其是在紧密的时间限制下解决的硬实例上。</li>
</ul>

<h3>Title: How Good are Foundation Models in Step-by-Step Embodied Reasoning?</h3>
<ul>
<li><strong>Authors: </strong>Dinura Dissanayake, Ahmed Heakl, Omkar Thawakar, Noor Ahsan, Ritesh Thawkar, Ketan More, Jean Lahoud, Rao Anwer, Hisham Cholakkal, Ivan Laptev, Fahad Shahbaz Khan, Salman Khan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15293">https://arxiv.org/abs/2509.15293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15293">https://arxiv.org/pdf/2509.15293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15293]] How Good are Foundation Models in Step-by-Step Embodied Reasoning?(https://arxiv.org/abs/2509.15293)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Embodied agents operating in the physical world must make decisions that are not only effective but also safe, spatially coherent, and grounded in context. While recent advances in large multimodal models (LMMs) have shown promising capabilities in visual understanding and language generation, their ability to perform structured reasoning for real-world embodied tasks remains underexplored. In this work, we aim to understand how well foundation models can perform step-by-step reasoning in embodied environments. To this end, we propose the Foundation Model Embodied Reasoning (FoMER) benchmark, designed to evaluate the reasoning capabilities of LMMs in complex embodied decision-making scenarios. Our benchmark spans a diverse set of tasks that require agents to interpret multimodal observations, reason about physical constraints and safety, and generate valid next actions in natural language. We present (i) a large-scale, curated suite of embodied reasoning tasks, (ii) a novel evaluation framework that disentangles perceptual grounding from action reasoning, and (iii) empirical analysis of several leading LMMs under this setting. Our benchmark includes over 1.1k samples with detailed step-by-step reasoning across 10 tasks and 8 embodiments, covering three different robot types. Our results highlight both the potential and current limitations of LMMs in embodied reasoning, pointing towards key challenges and opportunities for future research in robot intelligence. Our data and code will be made publicly available.</li>
<li><strong>摘要：</strong>在物理世界中运作的体现的代理必须做出不仅有效，而且在空间上保持一致并在上下文中扎根的决策。尽管大型多模型模型（LMM）的最新进展表现出有希望的视觉理解和语言能力，但它们为现实世界中体现的任务执行结构化推理的能力仍然没有得到充满反感。在这项工作中，我们旨在了解基础模型如何在具体的环境中执行分步推理。为此，我们提出了基础模型体现的推理（FOMER）基准，旨在评估LMM在复杂体现的决策方案中的推理能力。我们的基准测试涵盖了各种各样的任务，这些任务需要代理来解释多模式观察，有关物理约束和安全性的原因，并在自然语言中产生有效的下一步动作。我们提出了（i）一组大规模的，由体现的推理任务组成的套件，（ii）一个新颖的评估框架，将感知基础与动作推理相关，以及（iii）对这种情况下的几个领先LMM的经验分析。我们的基准包括超过1.1K样品，涵盖了10个任务和8个实施例的详细逐步推理，涵盖了三种不同的机器人类型。我们的结果凸显了LMM在体现推理中的潜在和当前局限性，指出了在机器人智能中进行未来研究的关键挑战和机会。我们的数据和代码将公开可用。</li>
</ul>

<h3>Title: Kuramoto Orientation Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yue Song, T. Anderson Keller, Sevan Brodjian, Takeru Miyato, Yisong Yue, Pietro Perona, Max Welling</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15328">https://arxiv.org/abs/2509.15328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15328">https://arxiv.org/pdf/2509.15328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15328]] Kuramoto Orientation Diffusion Models(https://arxiv.org/abs/2509.15328)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Orientation-rich images, such as fingerprints and textures, often exhibit coherent angular directional patterns that are challenging to model using standard generative approaches based on isotropic Euclidean diffusion. Motivated by the role of phase synchronization in biological systems, we propose a score-based generative model built on periodic domains by leveraging stochastic Kuramoto dynamics in the diffusion process. In neural and physical systems, Kuramoto models capture synchronization phenomena across coupled oscillators -- a behavior that we re-purpose here as an inductive bias for structured image generation. In our framework, the forward process performs \textit{synchronization} among phase variables through globally or locally coupled oscillator interactions and attraction to a global reference phase, gradually collapsing the data into a low-entropy von Mises distribution. The reverse process then performs \textit{desynchronization}, generating diverse patterns by reversing the dynamics with a learned score function. This approach enables structured destruction during forward diffusion and a hierarchical generation process that progressively refines global coherence into fine-scale details. We implement wrapped Gaussian transition kernels and periodicity-aware networks to account for the circular geometry. Our method achieves competitive results on general image benchmarks and significantly improves generation quality on orientation-dense datasets like fingerprints and textures. Ultimately, this work demonstrates the promise of biologically inspired synchronization dynamics as structured priors in generative modeling.</li>
<li><strong>摘要：</strong>诸如指纹和纹理之类的方向图像经常表现出相干的角度方向模式，这些图形在使用基于各向同性欧几里得扩散的标准生成方法进行建模而具有挑战性。通过相同步在生物系统中的作用的动机，我们提出了一种基于分数的生成模型，该模型通过在扩散过程中利用随机的库拉莫托动力学来构建基于周期域。在神经系统和物理系统中，库拉莫托模型捕获了跨耦合振荡器的同步现象 - 我们在这里重新使用的行为是结构化图像产生的归纳偏见。在我们的框架中，正向过程通过全球或局部耦合的振荡器相互作用以及对全局参考阶段的吸引力在相变之间执行\ textIt {同步}，并将数据逐渐折叠成低渗透率的von Mises分布。然后，反向过程执行\ textit {desynchronization}，通过使用学习的分数函数逆转动力学来生成多种模式。这种方法可以在向前扩散期间结构化破坏和层次生成过程，从而逐步完善了全球连贯性为细节细节。我们实施包裹的高斯过渡内核和周期性感知网络来解释圆形几何形状。我们的方法在一般图像基准上取得了竞争成果，并显着提高了指纹和纹理等方向密集数据集的发电质量。最终，这项工作证明了生物学启发的同步动力学作为生成建模中的结构化先验的希望。</li>
</ul>

<h3>Title: LowDiff: Efficient Diffusion Sampling with Low-Resolution Condition</h3>
<ul>
<li><strong>Authors: </strong>Jiuyi Xu, Qing Jin, Meida Chen, Andrew Feng, Yang Sui, Yangming Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15342">https://arxiv.org/abs/2509.15342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15342">https://arxiv.org/pdf/2509.15342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15342]] LowDiff: Efficient Diffusion Sampling with Low-Resolution Condition(https://arxiv.org/abs/2509.15342)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved remarkable success in image generation but their practical application is often hindered by the slow sampling speed. Prior efforts of improving efficiency primarily focus on compressing models or reducing the total number of denoising steps, largely neglecting the possibility to leverage multiple input resolutions in the generation process. In this work, we propose LowDiff, a novel and efficient diffusion framework based on a cascaded approach by generating increasingly higher resolution outputs. Besides, LowDiff employs a unified model to progressively refine images from low resolution to the desired resolution. With the proposed architecture design and generation techniques, we achieve comparable or even superior performance with much fewer high-resolution sampling steps. LowDiff is applicable to diffusion models in both pixel space and latent space. Extensive experiments on both conditional and unconditional generation tasks across CIFAR-10, FFHQ and ImageNet demonstrate the effectiveness and generality of our method. Results show over 50% throughput improvement across all datasets and settings while maintaining comparable or better quality. On unconditional CIFAR-10, LowDiff achieves an FID of 2.11 and IS of 9.87, while on conditional CIFAR-10, an FID of 1.94 and IS of 10.03. On FFHQ 64x64, LowDiff achieves an FID of 2.43, and on ImageNet 256x256, LowDiff built on LightningDiT-B/1 produces high-quality samples with a FID of 4.00 and an IS of 195.06, together with substantial efficiency gains.</li>
<li><strong>摘要：</strong>扩散模型在图像生成方面取得了显着的成功，但是它们的实际应用通常受到缓慢的采样速度的阻碍。提高效率的先前努力主要集中于压缩模型或减少剥离步骤的总数，在很大程度上忽略了在生成过程中利用多重输入分辨率的可能性。在这项工作中，我们提出了Lowdiff，这是一种基于级联方法的新颖而有效的扩散框架，通过产生越来越高的分辨率输出。此外，Lowdiff还采用统一模型来逐步完善从低分辨率到所需分辨率的图像。借助提出的架构设计和发电技术，我们以更少的高分辨率采样步骤实现了可比甚至优越的性能。 Lowdiff适用于像素空间和潜在空间中的扩散模型。 CIFAR-10，FFHQ和Imagenet的有条件和无条件生成任务的广泛实验证明了我们方法的有效性和通用性。结果表明，所有数据集和设置中的吞吐量改进超过50％，同时保持可比或更高的质量。在无条件的CIFAR-10上，Lowdiff的FID为2.11，为9.87，而条件CIFAR-10，FID为1.94，为10.03。在FFHQ 64x64上，Lowdiff的FID为2.43，在ImageNet 256x256上，Lowdiff在LightningDit-B/1上构建的LowDiff可产生高质量的样品，其FID为4.00，AN为195.06，以及大量效率增长。</li>
</ul>

<h3>Title: MaskAttn-SDXL: Controllable Region-Level Text-To-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Yu Chang, Jiahao Chen, Anzhe Cheng, Paul Bogdan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15357">https://arxiv.org/abs/2509.15357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15357">https://arxiv.org/pdf/2509.15357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15357]] MaskAttn-SDXL: Controllable Region-Level Text-To-Image Generation(https://arxiv.org/abs/2509.15357)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models achieve impressive realism but often suffer from compositional failures on prompts with multiple objects, attributes, and spatial relations, resulting in cross-token interference where entities entangle, attributes mix across objects, and spatial cues are violated. To address these failures, we propose MaskAttn-SDXL,a region-level gating mechanism applied to the cross-attention logits of Stable Diffusion XL(SDXL)'s UNet. MaskAttn-SDXL learns a binary mask per layer, injecting it into each cross-attention logit map before softmax to sparsify token-to-latent interactions so that only semantically relevant connections remain active. The method requires no positional encodings, auxiliary tokens, or external region masks, and preserves the original inference path with negligible overhead. In practice, our model improves spatial compliance and attribute binding in multi-object prompts while preserving overall image quality and diversity. These findings demonstrate that logit-level maksed cross-attention is an data-efficient primitve for enforcing compositional control, and our method thus serves as a practical extension for spatial control in text-to-image generation.</li>
<li><strong>摘要：</strong>文本对图像扩散模型实现了令人印象深刻的现实主义，但经常在具有多个对象，属性和空间关系的提示上遭受组成故障，从而导致交叉干扰，使实体纠缠，跨物体属性和空间线索的属性混合。为了解决这些失败，我们提出了Maskattn-SDXL，这是一种应用于稳定扩散XL（SDXL）的unet的区域级别的门控机制。 MaskAttn-SDXL学习每个层的二进制掩码，将其注入SoftMax之前的每个跨注意点映射，以稀疏令牌到lantent的相互作用，以便只有语义相关的连接保持活跃。该方法不需要位置编码，辅助令牌或外部区域掩模，并且可以保持原始推理路径的开销可忽略不计。在实践中，我们的模型可以提高空间合规性和属性在多对象提示中的绑定，同时保留整体图像质量和多样性。这些发现表明，logit级的交叉注意是用于执行组成控制的数据有效的原始，因此我们的方法是文本到图像生成中空间控制的实际扩展。</li>
</ul>

<h3>Title: RaceGAN: A Framework for Preserving Individuality while Converting Racial Information for Image-to-Image Translation</h3>
<ul>
<li><strong>Authors: </strong>Mst Tasnim Pervin, George Bebis, Fang Jiang, Alireza Tavakkoli</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15391">https://arxiv.org/abs/2509.15391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15391">https://arxiv.org/pdf/2509.15391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15391]] RaceGAN: A Framework for Preserving Individuality while Converting Racial Information for Image-to-Image Translation(https://arxiv.org/abs/2509.15391)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative adversarial networks (GANs) have demonstrated significant progress in unpaired image-to-image translation in recent years for several applications. CycleGAN was the first to lead the way, although it was restricted to a pair of domains. StarGAN overcame this constraint by tackling image-to-image translation across various domains, although it was not able to map in-depth low-level style changes for these domains. Style mapping via reference-guided image synthesis has been made possible by the innovations of StarGANv2 and StyleGAN. However, these models do not maintain individuality and need an extra reference image in addition to the input. Our study aims to translate racial traits by means of multi-domain image-to-image translation. We present RaceGAN, a novel framework capable of mapping style codes over several domains during racial attribute translation while maintaining individuality and high level semantics without relying on a reference image. RaceGAN outperforms other models in translating racial features (i.e., Asian, White, and Black) when tested on Chicago Face Dataset. We also give quantitative findings utilizing InceptionReNetv2-based classification to demonstrate the effectiveness of our racial translation. Moreover, we investigate how well the model partitions the latent space into distinct clusters of faces for each ethnic group.</li>
<li><strong>摘要：</strong>近年来，对于几种应用，生成的对抗网络（GAN）在未配对的图像到图像翻译中表现出了重大进展。 Cyclegan是第一个引导道路的人，尽管它仅限于一对域。 Stargan通过在各个域上处理图像到图像的翻译来克服此约束，尽管它无法为这些域绘制深入的低级样式更改。通过参考引导图像合成的样式映射已通过Starganv2和Stylegan的创新使其成为可能。但是，除了输入外，这些模型不保持个性，并且还需要额外的参考图像。我们的研究旨在通过多域图像到图像翻译来翻译种族特征。我们提出了Racegan，这是一个新颖的框架，该框架能够在种族属性翻译过程中在几个域上绘制样式代码，同时在不依赖参考图像的情况下保持个性和高级语义。在芝加哥面部数据集测试时，Racegan在翻译种族特征（即亚洲，白色和黑色）方面的表现要优于其他模型。我们还利用基于InceptionReneTV2的分类来证明种族翻译的有效性。此外，我们研究了模型如何将潜在空间分为每个种族群体的不同面积。</li>
</ul>

<h3>Title: Generating Part-Based Global Explanations Via Correspondence</h3>
<ul>
<li><strong>Authors: </strong>Kunal Rathore, Prasad Tadepalli</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15393">https://arxiv.org/abs/2509.15393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15393">https://arxiv.org/pdf/2509.15393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15393]] Generating Part-Based Global Explanations Via Correspondence(https://arxiv.org/abs/2509.15393)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Deep learning models are notoriously opaque. Existing explanation methods often focus on localized visual explanations for individual images. Concept-based explanations, while offering global insights, require extensive annotations, incurring significant labeling cost. We propose an approach that leverages user-defined part labels from a limited set of images and efficiently transfers them to a larger dataset. This enables the generation of global symbolic explanations by aggregating part-based local explanations, ultimately providing human-understandable explanations for model decisions on a large scale.</li>
<li><strong>摘要：</strong>众所周知，深度学习模型是不透明的。现有的解释方法通常集中在单个图像的局部视觉解释上。基于概念的解释在提供全球见解的同时需要广泛的注释，从而产生大量标签成本。我们提出了一种从有限的图像集中利用用户定义的零件标签的方法，并有效地将其传输到较大的数据集。这可以通过汇总基于部分的本地解释来产生全球符号的解释，最终为大规模的模型决策提供了人为理解的解释。</li>
</ul>

<h3>Title: Causal Fingerprints of AI Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Hui Xu, Chi Liu, Congcong Zhu, Minghao Wang, Youyang Qu, Longxiang Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15406">https://arxiv.org/abs/2509.15406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15406">https://arxiv.org/pdf/2509.15406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15406]] Causal Fingerprints of AI Generative Models(https://arxiv.org/abs/2509.15406)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>AI generative models leave implicit traces in their generated images, which are commonly referred to as model fingerprints and are exploited for source attribution. Prior methods rely on model-specific cues or synthesis artifacts, yielding limited fingerprints that may generalize poorly across different generative models. We argue that a complete model fingerprint should reflect the causality between image provenance and model traces, a direction largely unexplored. To this end, we conceptualize the \emph{causal fingerprint} of generative models, and propose a causality-decoupling framework that disentangles it from image-specific content and style in a semantic-invariant latent space derived from pre-trained diffusion reconstruction residual. We further enhance fingerprint granularity with diverse feature representations. We validate causality by assessing attribution performance across representative GANs and diffusion models and by achieving source anonymization using counterfactual examples generated from causal fingerprints. Experiments show our approach outperforms existing methods in model attribution, indicating strong potential for forgery detection, model copyright tracing, and identity protection.</li>
<li><strong>摘要：</strong>AI生成模型在其生成的图像中留下隐式痕迹，这些图像通常称为模型指纹，并被利用以进行源属性。先前的方法依赖于模型特异性的提示或合成伪像，产生有限的指纹，这些指纹可能在不同的生成模型中概括地概括。我们认为，完整的模型指纹应反映图像出处和模型痕迹之间的因果关系，这在很大程度上没有探索。为此，我们概念化了生成模型的\ emph {因果指纹}，并提出了一个因果关系 - 耦合框架，该框架将其与图像特定的内容和样式中的样式分散在语义上不变的潜伏空间中，该框架是从预先经历的扩散扩散重建残基中得出的。我们进一步增强了具有不同特征表示的指纹粒度。我们通过评估代表性gan和扩散模型的归因性能以及使用因果指纹产生的反事实示例来验证因果关系。实验表明，我们的方法在模型归因中的现有方法优于现有方法，这表明伪造检测，模型版权追踪和身份保护的强大潜力。</li>
</ul>

<h3>Title: Efficient Multimodal Dataset Distillation via Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Zhenghao Zhao, Haoxuan Wang, Junyi Wu, Yuzhang Shang, Gaowen Liu, Yan Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15472">https://arxiv.org/abs/2509.15472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15472">https://arxiv.org/pdf/2509.15472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15472]] Efficient Multimodal Dataset Distillation via Generative Models(https://arxiv.org/abs/2509.15472)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Dataset distillation aims to synthesize a small dataset from a large dataset, enabling the model trained on it to perform well on the original dataset. With the blooming of large language models and multimodal large language models, the importance of multimodal datasets, particularly image-text datasets, has grown significantly. However, existing multimodal dataset distillation methods are constrained by the Matching Training Trajectories algorithm, which significantly increases the computing resource requirement, and takes days to process the distillation. In this work, we introduce EDGE, a generative distillation method for efficient multimodal dataset distillation. Specifically, we identify two key challenges of distilling multimodal datasets with generative models: 1) The lack of correlation between generated images and captions. 2) The lack of diversity among generated samples. To address the aforementioned issues, we propose a novel generative model training workflow with a bi-directional contrastive loss and a diversity loss. Furthermore, we propose a caption synthesis strategy to further improve text-to-image retrieval performance by introducing more text information. Our method is evaluated on Flickr30K, COCO, and CC3M datasets, demonstrating superior performance and efficiency compared to existing approaches. Notably, our method achieves results 18x faster than the state-of-the-art method.</li>
<li><strong>摘要：</strong>数据集蒸馏旨在从大型数据集中合成一个小数据集，从而使经过训练的模型在原始数据集上表现良好。随着大语言模型和多模式大语言模型的开花，多模式数据集（尤其是图像文本数据集）的重要性已大大增长。但是，现有的多模式数据集蒸馏方法受到匹配训练轨迹算法的约束，该算法大大增加了计算资源的需求，并且需要几天的时间来处理蒸馏。在这项工作中，我们引入了Edge，这是一种生成蒸馏方法，用于有效的多模式数据集蒸馏。具体而言，我们确定了用生成模型提取多模式数据集的两个关键挑战：1）生成的图像和字幕之间缺乏相关性。 2）生成的样本之间缺乏多样性。为了解决上述问题，我们提出了一种新型的生成模型培训工作流程，并具有双向对比度损失和多样性损失。此外，我们提出了一个标题综合策略，以通过引入更多文本信息来进一步提高文本对图像检索性能。我们的方法在FlickR30K，Coco和CC3M数据集上进行了评估，与现有方法相比，表明性能和效率均出色。值得注意的是，我们的方法比最先进的方法快18倍。</li>
</ul>

<h3>Title: OpenViGA: Video Generation for Automotive Driving Scenes by Streamlining and Fine-Tuning Open Source Models with Public Data</h3>
<ul>
<li><strong>Authors: </strong>Björn Möller, Zhengyang Li, Malte Stelzer, Thomas Graave, Fabian Bettels, Muaaz Ataya, Tim Fingscheidt</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15479">https://arxiv.org/abs/2509.15479</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15479">https://arxiv.org/pdf/2509.15479</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15479]] OpenViGA: Video Generation for Automotive Driving Scenes by Streamlining and Fine-Tuning Open Source Models with Public Data(https://arxiv.org/abs/2509.15479)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent successful video generation systems that predict and create realistic automotive driving scenes from short video inputs assign tokenization, future state prediction (world model), and video decoding to dedicated models. These approaches often utilize large models that require significant training resources, offer limited insight into design choices, and lack publicly available code and datasets. In this work, we address these deficiencies and present OpenViGA, an open video generation system for automotive driving scenes. Our contributions are: Unlike several earlier works for video generation, such as GAIA-1, we provide a deep analysis of the three components of our system by separate quantitative and qualitative evaluation: Image tokenizer, world model, video decoder. Second, we purely build upon powerful pre-trained open source models from various domains, which we fine-tune by publicly available automotive data (BDD100K) on GPU hardware at academic scale. Third, we build a coherent video generation system by streamlining interfaces of our components. Fourth, due to public availability of the underlying models and data, we allow full reproducibility. Finally, we also publish our code and models on Github. For an image size of 256x256 at 4 fps we are able to predict realistic driving scene videos frame-by-frame with only one frame of algorithmic latency.</li>
<li><strong>摘要：</strong>最近成功的视频生成系统可以预测和创建现实的汽车驾驶场景，从短视频输入分配令牌化，未来的状态预测（世界模型）和视频解码为专用模型。这些方法经常利用需要大量培训资源的大型模型，对设计选择提供有限的见解，并且缺乏公开可用的代码和数据集。在这项工作中，我们解决了这些缺陷，并介绍OpenViga，这是一种用于汽车驾驶场景的开放视频生成系统。我们的贡献是：与诸如GAIA-1等视频生成的几项早期作品不同，我们通过单独的定量和定性评估对系统的三个组成部分进行了深入的分析：图像令牌，世界模型，视频解码器。其次，我们纯粹基于来自各个领域的强大预培训的开源模型，我们通过在GPU硬件上按公共可用的汽车数据（BDD100K）在学术范围内进行了微调。第三，我们通过简化组件的界面来构建一个连贯的视频生成系统。第四，由于基础模型和数据的公共可用性，我们允许完全可重复性。最后，我们还在GitHub上发布了代码和模型。对于4 fps的256x256的图像大小，我们能够通过仅使用一个算法延迟框架来预测现实的驾驶场景视频。</li>
</ul>

<h3>Title: Lynx: Towards High-Fidelity Personalized Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Shen Sang, Tiancheng Zhi, Tianpei Gu, Jing Liu, Linjie Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15496">https://arxiv.org/abs/2509.15496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15496">https://arxiv.org/pdf/2509.15496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15496]] Lynx: Towards High-Fidelity Personalized Video Generation(https://arxiv.org/abs/2509.15496)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present Lynx, a high-fidelity model for personalized video synthesis from a single input image. Built on an open-source Diffusion Transformer (DiT) foundation model, Lynx introduces two lightweight adapters to ensure identity fidelity. The ID-adapter employs a Perceiver Resampler to convert ArcFace-derived facial embeddings into compact identity tokens for conditioning, while the Ref-adapter integrates dense VAE features from a frozen reference pathway, injecting fine-grained details across all transformer layers through cross-attention. These modules collectively enable robust identity preservation while maintaining temporal coherence and visual realism. Through evaluation on a curated benchmark of 40 subjects and 20 unbiased prompts, which yielded 800 test cases, Lynx has demonstrated superior face resemblance, competitive prompt following, and strong video quality, thereby advancing the state of personalized video generation.</li>
<li><strong>摘要：</strong>我们提出了Lynx，这是一种来自单个输入图像的个性化视频合成的高保真模型。 Lynx建立在开源扩散变压器（DIT）基础模型的基础上，引入了两个轻巧的适配器，以确保身份保真度。 ID-ADAPTER采用感知器的重新采样器将斜面衍生的面部嵌入转换为紧凑的身份代币进行调节，而Ref-Adapter则通过交叉注意从冻结的参考途径中整合了冷冻参考途径的密集VAE特征，从而在所有变压器层中注入细粒度的细节。这些模块共同实现了稳健的身份，同时保持时间连贯性和视觉现实主义。通过对40个受试者和20个无偏的提示的精心策划的基准进行评估，产生了800个测试用例，Lynx表现出了出色的面部相似之处，竞争性的提示和强大的视频质量，从而提高了个性化的视频生成状态。</li>
</ul>

<h3>Title: SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models</h3>
<ul>
<li><strong>Authors: </strong>Sen Wang, Jingyi Tian, Le Wang, Zhimin Liao, Jiayi Li, Huaiyi Dong, Kun Xia, Sanping Zhou, Wei Tang, Hua Gang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15536">https://arxiv.org/abs/2509.15536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15536">https://arxiv.org/pdf/2509.15536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15536]] SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models(https://arxiv.org/abs/2509.15536)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>World models allow agents to simulate the consequences of actions in imagined environments for planning, control, and long-horizon decision-making. However, existing autoregressive world models struggle with visually coherent predictions due to disrupted spatial structure, inefficient decoding, and inadequate motion modeling. In response, we propose \textbf{S}cale-wise \textbf{A}utoregression with \textbf{M}otion \textbf{P}r\textbf{O}mpt (\textbf{SAMPO}), a hybrid framework that combines visual autoregressive modeling for intra-frame generation with causal modeling for next-frame generation. Specifically, SAMPO integrates temporal causal decoding with bidirectional spatial attention, which preserves spatial locality and supports parallel decoding within each scale. This design significantly enhances both temporal consistency and rollout efficiency. To further improve dynamic scene understanding, we devise an asymmetric multi-scale tokenizer that preserves spatial details in observed frames and extracts compact dynamic representations for future frames, optimizing both memory usage and model performance. Additionally, we introduce a trajectory-aware motion prompt module that injects spatiotemporal cues about object and robot trajectories, focusing attention on dynamic regions and improving temporal consistency and physical realism. Extensive experiments show that SAMPO achieves competitive performance in action-conditioned video prediction and model-based control, improving generation quality with 4.4$\times$ faster inference. We also evaluate SAMPO's zero-shot generalization and scaling behavior, demonstrating its ability to generalize to unseen tasks and benefit from larger model sizes.</li>
<li><strong>摘要：</strong>世界模型允许代理商模拟在计划，控制和长期决策的想象环境中行动的后果。但是，由于空间结构破坏，解码效率低下和运动建模不足，现有的自回归世界模型在视觉上相干预测而努力。 In response, we propose \textbf{S}cale-wise \textbf{A}utoregression with \textbf{M}otion \textbf{P}r\textbf{O}mpt (\textbf{SAMPO}), a hybrid framework that combines visual autoregressive modeling for intra-frame generation with causal modeling for下一代。具体而言，Sampo将时间因果解码与双向空间关注相结合，该解码保留了空间位置并支持每个尺度内的平行解码。该设计显着提高了时间一致性和推出效率。为了进一步提高动态场景的理解，我们设计了一个不对称的多尺度令牌器，该代币将空间细节保留在观察到的帧中，并提取了未来框架的紧凑动态表示，从而优化了内存使用和模型性能。此外，我们引入了一个轨迹感知运动提示模块，该模块注入了有关对象和机器人轨迹的时空提示，将注意力集中在动态区域上并提高时间一致性和物理现实主义。广泛的实验表明，Sampo在动作条件的视频预测和基于模型的控制中实现了竞争性能，从而以4.4 $ \ times $ $更快的推断改善了发电质量。我们还评估了Sampo的零拍概括和缩放行为，证明了其概括地看不见任务并受益于更大的模型大小的能力。</li>
</ul>

<h3>Title: BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent</h3>
<ul>
<li><strong>Authors: </strong>Shaojie Zhang, Ruoceng Zhang, Pei Fu, Shaokang Wang, Jiahui Yang, Xin Du, Shiqi Cui, Bin Qin, Ying Huang, Zhenbo Luo, Jian Luan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15566">https://arxiv.org/abs/2509.15566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15566">https://arxiv.org/pdf/2509.15566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15566]] BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent(https://arxiv.org/abs/2509.15566)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In the field of AI-driven human-GUI interaction automation, while rapid advances in multimodal large language models and reinforcement fine-tuning techniques have yielded remarkable progress, a fundamental challenge persists: their interaction logic significantly deviates from natural human-GUI communication patterns. To fill this gap, we propose "Blink-Think-Link" (BTL), a brain-inspired framework for human-GUI interaction that mimics the human cognitive process between users and graphical interfaces. The system decomposes interactions into three biologically plausible phases: (1) Blink - rapid detection and attention to relevant screen areas, analogous to saccadic eye movements; (2) Think - higher-level reasoning and decision-making, mirroring cognitive planning; and (3) Link - generation of executable commands for precise motor control, emulating human action selection mechanisms. Additionally, we introduce two key technical innovations for the BTL framework: (1) Blink Data Generation - an automated annotation pipeline specifically optimized for blink data, and (2) BTL Reward -- the first rule-based reward mechanism that enables reinforcement learning driven by both process and outcome. Building upon this framework, we develop a GUI agent model named BTL-UI, which demonstrates consistent state-of-the-art performance across both static GUI understanding and dynamic interaction tasks in comprehensive benchmarks. These results provide conclusive empirical validation of the framework's efficacy in developing advanced GUI Agents.</li>
<li><strong>摘要：</strong>在AI驱动的人类GUI相互作用自动化领域中，尽管多模式大语言模型和强化微调技术的快速进步取得了显着的进步，但基本的挑战仍然存在：它们的相互作用逻辑显着偏离了自然的人类GUI沟通模式。为了填补这一空白，我们提出了“眨眼 - 思考链接”（BTL），这是一种脑启发的人类gui相互作用的框架，模拟了用户和图形接口之间的人类认知过程。该系统将相互作用分解为三个具有生物学上合理的阶段：（1）眨眼 - 快速检测和对相关屏幕区域的关注，类似于萨卡迪克眼动运动； （2）思考 - 高级推理和决策，反映认知计划； （3）链接 - 生成可执行的命令，以进行精确的电机控制，模仿人类行动选择机制。此外，我们为BTL框架介绍了两项关键的技术创新：（1）眨眼数据生成 - 专门针对眨眼数据优化的自动注释管道，以及（2）BTL奖励 - 第一个基于规则的奖励机制，可以使过程和结果均能增强增强学习。在此框架的基础上，我们开发了一个名为BTL-UI的GUI代理模型，该模型在静态GUI理解和全面基准中的动态互动任务中表现出一致的最新性能。这些结果对框架在开发高级GUI代理方面的功效提供了确定的经验验证。</li>
</ul>

<h3>Title: Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification</h3>
<ul>
<li><strong>Authors: </strong>Zinan Lin, Enshu Liu, Xuefei Ning, Junyi Zhu, Wenyu Wang, Sergey Yekhanin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15591">https://arxiv.org/abs/2509.15591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15591">https://arxiv.org/pdf/2509.15591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15591]] Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification(https://arxiv.org/abs/2509.15591)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative modeling, representation learning, and classification are three core problems in machine learning (ML), yet their state-of-the-art (SoTA) solutions remain largely disjoint. In this paper, we ask: Can a unified principle address all three? Such unification could simplify ML pipelines and foster greater synergy across tasks. We introduce Latent Zoning Network (LZN) as a step toward this goal. At its core, LZN creates a shared Gaussian latent space that encodes information across all tasks. Each data type (e.g., images, text, labels) is equipped with an encoder that maps samples to disjoint latent zones, and a decoder that maps latents back to data. ML tasks are expressed as compositions of these encoders and decoders: for example, label-conditional image generation uses a label encoder and image decoder; image embedding uses an image encoder; classification uses an image encoder and label decoder. We demonstrate the promise of LZN in three increasingly complex scenarios: (1) LZN can enhance existing models (image generation): When combined with the SoTA Rectified Flow model, LZN improves FID on CIFAR10 from 2.76 to 2.59-without modifying the training objective. (2) LZN can solve tasks independently (representation learning): LZN can implement unsupervised representation learning without auxiliary loss functions, outperforming the seminal MoCo and SimCLR methods by 9.3% and 0.2%, respectively, on downstream linear classification on ImageNet. (3) LZN can solve multiple tasks simultaneously (joint generation and classification): With image and label encoders/decoders, LZN performs both tasks jointly by design, improving FID and achieving SoTA classification accuracy on CIFAR10. The code and trained models are available at this https URL. The project website is at this https URL.</li>
<li><strong>摘要：</strong>生成建模，表示学习和分类是机器学习（ML）的三个核心问题，但是他们的最新解决方案（SOTA）解决方案仍然很大程度上不相关。在本文中，我们问：统一的原则可以解决这三个吗？这种统一可以简化ML管道，并在任务中促进更大的协同作用。我们介绍潜在分区网络（LZN），以朝着这一目标迈出一步。 LZN以其核心创建了共享的高斯潜在空间，该空间跨所有任务编码信息。每种数据类型（例如，图像，文本，标签）配备了一个编码器，该编码器将样本映射到分离潜在区域，以及将潜在映射回数据的解码器。 ML任务表示为这些编码器和解码器的组成：例如，标签条件图像生成使用标签编码器和图像解码器；图像嵌入使用图像编码器；分类使用图像编码器和标签解码器。我们证明了LZN在三个日益复杂的方案中的希望：（1）LZN可以增强现有模型（图像生成）：与SOTA整流流模型结合使用时，LZN将CIFAR10上的FID从2.76提高到2.76至2.59，从而改变了训练目标。 （2）LZN可以独立解决任务（表示学习）：LZN可以在没有辅助损失函数的情况下实施无监督的表示学习，在Imagenet上的下游线性分类上分别超过了9.3％和0.2％的精确MOCO和SIMCLR方法。 （3）LZN可以同时解决多个任务（联合生成和分类）：使用图像和标签编码器/解码器，LZN通过设计共同执行这两个任务，改善FID并实现CIFAR10上的SOTA分类精度。代码和训练有素的模型可在此HTTPS URL上找到。项目网站位于此HTTPS URL。</li>
</ul>

<h3>Title: TennisTV: Do Multimodal Large Language Models Understand Tennis Rallies?</h3>
<ul>
<li><strong>Authors: </strong>Zhongyuan Bao, Lejun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15602">https://arxiv.org/abs/2509.15602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15602">https://arxiv.org/pdf/2509.15602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15602]] TennisTV: Do Multimodal Large Language Models Understand Tennis Rallies?(https://arxiv.org/abs/2509.15602)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) excel at general video understanding but struggle with fast, high-frequency sports like tennis, where rally clips are short yet information-dense. To systematically evaluate MLLMs in this challenging domain, we present TennisTV, the first and most comprehensive benchmark for tennis video understanding. TennisTV models each rally as a temporal-ordered sequence of consecutive stroke events, using automated pipelines for filtering and question generation. It covers 8 tasks at rally and stroke levels and includes 2,500 human-verified questions. Evaluating 16 representative MLLMs, we provide the first systematic assessment of tennis video understanding. Results reveal substantial shortcomings and yield two key insights: (i) frame-sampling density should be tailored and balanced across tasks, and (ii) improving temporal grounding is essential for stronger reasoning.</li>
<li><strong>摘要：</strong>多模式的大语模型（MLLM）在一般视频理解方面表现出色，但在快速，高频运动等网球上挣扎，那里的集会剪辑短暂而信息密集。为了系统地评估这个具有挑战性的领域中的MLLM，我们提出了Tennistv，这是网球视频理解的第一个也是最全面的基准。 Tennistv使用自动化管道进行过滤和问题产生，将每个集会模型为连续中风事件的时间顺序序列。它涵盖了集会和中风级别的8个任务，其中包括2,500个人文验证的问题。评估16个代表性MLLM，我们提供了对网球视频理解的首次系统评估。结果揭示了实质性的缺点并产生了两个关键见解：（i）框架采样密度应在任务中量身定制和平衡，并且（ii）改善时间基础对于更强的推理至关重要。</li>
</ul>

<h3>Title: FingerSplat: Contactless Fingerprint 3D Reconstruction and Generation based on 3D Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Yuwei Jia, Yutang Lu, Zhe Cui, Fei Su</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15648">https://arxiv.org/abs/2509.15648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15648">https://arxiv.org/pdf/2509.15648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15648]] FingerSplat: Contactless Fingerprint 3D Reconstruction and Generation based on 3D Gaussian Splatting(https://arxiv.org/abs/2509.15648)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Researchers have conducted many pioneer researches on contactless fingerprints, yet the performance of contactless fingerprint recognition still lags behind contact-based methods primary due to the insufficient contactless fingerprint data with pose variations and lack of the usage of implicit 3D fingerprint representations. In this paper, we introduce a novel contactless fingerprint 3D registration, reconstruction and generation framework by integrating 3D Gaussian Splatting, with the goal of offering a new paradigm for contactless fingerprint recognition that integrates 3D fingerprint reconstruction and generation. To our knowledge, this is the first work to apply 3D Gaussian Splatting to the field of fingerprint recognition, and the first to achieve effective 3D registration and complete reconstruction of contactless fingerprints with sparse input images and without requiring camera parameters information. Experiments on 3D fingerprint registration, reconstruction, and generation prove that our method can accurately align and reconstruct 3D fingerprints from 2D images, and sequentially generates high-quality contactless fingerprints from 3D model, thus increasing the performances for contactless fingerprint recognition.</li>
<li><strong>摘要：</strong>研究人员已经对非接触式指纹进行了许多先驱研究，但是由于非接触式指纹数据的不足和缺乏使用隐式3D指纹表示的使用不足，因此非接触式指纹识别仍然落后于基于接触的方法。在本文中，我们通过整合3D高斯裂缝，介绍了一种新颖的非接触式指纹3D注册，重建和发电框架，目的是为无接触式的指纹识别提供新的范式，以整合3D指纹重建和发电。据我们所知，这是第一项将3D高斯碎片施加到指纹识别领域的工作，也是第一个实现有效的3D注册并完全重建具有稀疏输入图像并且不需要摄像机参数信息的非接触式指纹的重建。对3D指纹注册，重建和产生的实验证明，我们的方法可以准确地对齐和重建2D图像的3D指纹，并顺序从3D模型中生成高质量的非接触式指纹，从而增加无接触式指纹识别的性能。</li>
</ul>

<h3>Title: Layout Stroke Imitation: A Layout Guided Handwriting Stroke Generation for Style Imitation with Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Sidra Hanif, Longin Jan Latecki</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15678">https://arxiv.org/abs/2509.15678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15678">https://arxiv.org/pdf/2509.15678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15678]] Layout Stroke Imitation: A Layout Guided Handwriting Stroke Generation for Style Imitation with Diffusion Model(https://arxiv.org/abs/2509.15678)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Handwriting stroke generation is crucial for improving the performance of tasks such as handwriting recognition and writers order recovery. In handwriting stroke generation, it is significantly important to imitate the sample calligraphic style. The previous studies have suggested utilizing the calligraphic features of the handwriting. However, they had not considered word spacing (word layout) as an explicit handwriting feature, which results in inconsistent word spacing for style imitation. Firstly, this work proposes multi-scale attention features for calligraphic style imitation. These multi-scale feature embeddings highlight the local and global style features. Secondly, we propose to include the words layout, which facilitates word spacing for handwriting stroke generation. Moreover, we propose a conditional diffusion model to predict strokes in contrast to previous work, which directly generated style images. Stroke generation provides additional temporal coordinate information, which is lacking in image generation. Hence, our proposed conditional diffusion model for stroke generation is guided by calligraphic style and word layout for better handwriting imitation and stroke generation in a calligraphic style. Our experimentation shows that the proposed diffusion model outperforms the current state-of-the-art stroke generation and is competitive with recent image generation networks.</li>
<li><strong>摘要：</strong>手写中风的生成对于改善手写识别和作家订购恢复等任务的执行至关重要。在手写中风的生成中，模仿样本书法风格非常重要。先前的研究表明，使用笔迹的书法特征。但是，他们并未将单词间距（单词布局）视为明确的手写功能，这导致了不一致的单词间距，以模仿样式。首先，这项工作提出了仿真风格模仿的多尺度关注功能。这些多尺度功能嵌入着局部和全球样式的功能。其次，我们建议包括布局单词，该单词有助于单词间距进行手写。此外，我们提出了一个条件扩散模型，以预测与以前的工作形成鲜明对比的笔触，该工作直接生成样式图像。中风生成提供了其他时间坐标信息，这些信息缺乏图像生成。因此，我们提出的中风生成的条件扩散模型以书法风格和单词布局为指导，以更好地模仿手写，并以书法方式产生。我们的实验表明，所提出的扩散模型的表现优于当前的最新中风生成，并且与最近的图像生成网络具有竞争力。</li>
</ul>

<h3>Title: Toward Medical Deepfake Detection: A Comprehensive Dataset and Novel Method</h3>
<ul>
<li><strong>Authors: </strong>Shuaibo Li, Zhaohu Xing, Hongqiu Wang, Pengfei Hao, Xingyu Li, Zekai Liu, Lei Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15711">https://arxiv.org/abs/2509.15711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15711">https://arxiv.org/pdf/2509.15711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15711]] Toward Medical Deepfake Detection: A Comprehensive Dataset and Novel Method(https://arxiv.org/abs/2509.15711)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of generative AI in medical imaging has introduced both significant opportunities and serious challenges, especially the risk that fake medical images could undermine healthcare systems. These synthetic images pose serious risks, such as diagnostic deception, financial fraud, and misinformation. However, research on medical forensics to counter these threats remains limited, and there is a critical lack of comprehensive datasets specifically tailored for this field. Additionally, existing media forensic methods, which are primarily designed for natural or facial images, are inadequate for capturing the distinct characteristics and subtle artifacts of AI-generated medical images. To tackle these challenges, we introduce \textbf{MedForensics}, a large-scale medical forensics dataset encompassing six medical modalities and twelve state-of-the-art medical generative models. We also propose \textbf{DSKI}, a novel \textbf{D}ual-\textbf{S}tage \textbf{K}nowledge \textbf{I}nfusing detector that constructs a vision-language feature space tailored for the detection of AI-generated medical images. DSKI comprises two core components: 1) a cross-domain fine-trace adapter (CDFA) for extracting subtle forgery clues from both spatial and noise domains during training, and 2) a medical forensic retrieval module (MFRM) that boosts detection accuracy through few-shot retrieval during testing. Experimental results demonstrate that DSKI significantly outperforms both existing methods and human experts, achieving superior accuracy across multiple medical modalities.</li>
<li><strong>摘要：</strong>生成AI在医学成像中的快速发展引入了重大机遇和严重的挑战，尤其是假医学图像可能破坏医疗保健系统的风险。这些合成图像构成了严重的风险，例如诊断欺骗，财务欺诈和错误信息。但是，对应对这些威胁的医学取证的研究仍然有限，并且严重缺乏专门针对该领域量身定制的全面数据集。此外，主要是为自然图像设计的现有媒体法医方法，不足以捕获AI生成的医学图像的独特特征和微妙的伪像。为了应对这些挑战，我们介绍了\ textbf {medforensics}，这是一个大规模的医疗法医数据集，其中包含六种医学方式和十二个最先进的医疗生成模型。我们还提出了\ textbf {dski}，这是一种小说\ textbf {d} ual- \ textbf {s} tage \ textbf {k} nowledge \ textbf {k} nowledge \ textbf {i} nfusing检测器，该检测器构造了一个构造的视觉 - 语言功能，该功能构造了用于检测AI型医疗图像的视觉空间。 DSKI包括两个核心组成部分：1）跨域精细迹线适配器（CDFA），用于在训练过程中从空间和噪声域中提取微妙的伪造线索，以及2）在测试过程中通过几次拍摄的检索来提高几次射击检测的医学法医检索模块（MFRM）。实验结果表明，DSKI显着胜过现有方法和人类专家，从而在多种医学方式上达到了卓越的准确性。</li>
</ul>

<h3>Title: GUI-ReWalk: Massive Data Generation for GUI Agent via Stochastic Exploration and Intent-Aware Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Musen Lin, Minghao Liu, Taoran Lu, Lichen Yuan, Yiwei Liu, Haonan Xu, Yu Miao, Yuhao Chao, Zhaojian Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15738">https://arxiv.org/abs/2509.15738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15738">https://arxiv.org/pdf/2509.15738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15738]] GUI-ReWalk: Massive Data Generation for GUI Agent via Stochastic Exploration and Intent-Aware Reasoning(https://arxiv.org/abs/2509.15738)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Graphical User Interface (GUI) Agents, powered by large language and vision-language models, hold promise for enabling end-to-end automation in digital environments. However, their progress is fundamentally constrained by the scarcity of scalable, high-quality trajectory data. Existing data collection strategies either rely on costly and inconsistent manual annotations or on synthetic generation methods that trade off between diversity and meaningful task coverage. To bridge this gap, we present GUI-ReWalk: a reasoning-enhanced, multi-stage framework for synthesizing realistic and diverse GUI trajectories. GUI-ReWalk begins with a stochastic exploration phase that emulates human trial-and-error behaviors, and progressively transitions into a reasoning-guided phase where inferred goals drive coherent and purposeful interactions. Moreover, it supports multi-stride task generation, enabling the construction of long-horizon workflows across multiple applications. By combining randomness for diversity with goal-aware reasoning for structure, GUI-ReWalk produces data that better reflects the intent-aware, adaptive nature of human-computer interaction. We further train Qwen2.5-VL-7B on the GUI-ReWalk dataset and evaluate it across multiple benchmarks, including Screenspot-Pro, OSWorld-G, UI-Vision, AndroidControl, and GUI-Odyssey. Results demonstrate that GUI-ReWalk enables superior coverage of diverse interaction flows, higher trajectory entropy, and more realistic user intent. These findings establish GUI-ReWalk as a scalable and data-efficient framework for advancing GUI agent research and enabling robust real-world automation.</li>
<li><strong>摘要：</strong>图形用户界面（GUI）代理，由大语言和视觉语言模型提供支持，有望在数字环境中启用端到端自动化。但是，它们的进步从根本上受到可扩展，高质量轨迹数据的稀缺性的限制。现有的数据收集策略要么依赖于昂贵和不一致的手动注释，要么依靠多样性和有意义的任务覆盖率之间进行权衡的合成生成方法。为了弥合这一差距，我们提出了Gui-ewalk：一个综合现实和多样化的GUI轨迹的推理增强的多阶段框架。 Gui-ewalk始于一个随机探索阶段，该阶段模仿了人类的反复试验行为，并逐渐过渡到推理引导阶段，其中推断目标推动了连贯和有目的的相互作用。此外，它支持多阶段的任务生成，从而可以在多个应用程序中构建长马工作流程。通过将多样性的随机性与结构的目标感知推理相结合，Gui-ewalk产生的数据可以更好地反映人类计算机相互作用的意图意识到的自适应性质。我们在GUI-Rewalk数据集上进一步训练QWEN2.5-VL-7B，并跨多个基准测试，包括ScreenSpot-Pro，Osworld-G，UI-Vision，AndroidControl和Gui-Odyssey。结果表明，Gui-ewalk可以覆盖各种交互作用流，更高的轨迹熵和更现实的用户意图。这些发现将GUI-Rewalk建立为可扩展且具有数据效率的框架，用于推进GUI代理研究并实现可靠的现实世界自动化。</li>
</ul>

<h3>Title: Incremental Multistep Forecasting of Battery Degradation Using Pseudo Targets</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Adam Rico, Nagarajan Raghavan, Senthilnath Jayavelu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15740">https://arxiv.org/abs/2509.15740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15740">https://arxiv.org/pdf/2509.15740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15740]] Incremental Multistep Forecasting of Battery Degradation Using Pseudo Targets(https://arxiv.org/abs/2509.15740)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Data-driven models accurately perform early battery prognosis to prevent equipment failure and further safety hazards. Most existing machine learning (ML) models work in offline mode which must consider their retraining post-deployment every time new data distribution is encountered. Hence, there is a need for an online ML approach where the model can adapt to varying distributions. However, existing online incremental multistep forecasts are a great challenge as there is no way to correct the model of its forecasts at the current instance. Also, these methods need to wait for a considerable amount of time to acquire enough streaming data before retraining. In this study, we propose iFSNet (incremental Fast and Slow learning Network) which is a modified version of FSNet for a single-pass mode (sample-by-sample) to achieve multistep forecasting using pseudo targets. It uses a simple linear regressor of the input sequence to extrapolate pseudo future samples (pseudo targets) and calculate the loss from the rest of the forecast and keep updating the model. The model benefits from the associative memory and adaptive structure mechanisms of FSNet, at the same time the model incrementally improves by using pseudo targets. The proposed model achieved 0.00197 RMSE and 0.00154 MAE on datasets with smooth degradation trajectories while it achieved 0.01588 RMSE and 0.01234 MAE on datasets having irregular degradation trajectories with capacity regeneration spikes.</li>
<li><strong>摘要：</strong>数据驱动的模型准确地执行了早期电池预后，以防止设备故障和进一步的安全危害。大多数现有的机器学习（ML）模型在离线模式下工作，每次遇到新的数据分布时，都必须考虑其重新投放后。因此，需要采用在线ML方法，该方法可以适应不同的分布。但是，现有的在线增量多标准预测是一个巨大的挑战，因为没有办法在当前实例上纠正其预测模型。同样，这些方法需要等待大量时间才能获取足够的流数据，然后再进行重新培训。在这项研究中，我们提出了IFSNET（增量快速学习网络），该网络是单通道模式（按样本样本）的FSNET的修改版本，以实现使用伪目标进行多步骤预测。它使用输入序列的简单线性回归器来推断伪未来样本（伪目标），并计算预测的其余部分中的损失并继续更新模型。该模型受益于FSNET的关联记忆和自适应结构机制，同时，该模型通过使用伪目标逐渐改善。所提出的模型在具有光滑降解轨迹的数据集上实现了0.00197 RMSE和0.00154 MAE，同时在具有不规则降解轨迹的数据集上实现了0.01588 RMSE和0.01234 MAE，具有容量再生峰值。</li>
</ul>

<h3>Title: TrueMoE: Dual-Routing Mixture of Discriminative Experts for Synthetic Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Laixin Zhang, Shuaibo Li, Wei Ma, Hongbin Zha</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15741">https://arxiv.org/abs/2509.15741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15741">https://arxiv.org/pdf/2509.15741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15741]] TrueMoE: Dual-Routing Mixture of Discriminative Experts for Synthetic Image Detection(https://arxiv.org/abs/2509.15741)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid progress of generative models has made synthetic image detection an increasingly critical task. Most existing approaches attempt to construct a single, universal discriminative space to separate real from fake content. However, such unified spaces tend to be complex and brittle, often struggling to generalize to unseen generative patterns. In this work, we propose TrueMoE, a novel dual-routing Mixture-of-Discriminative-Experts framework that reformulates the detection task as a collaborative inference across multiple specialized and lightweight discriminative subspaces. At the core of TrueMoE is a Discriminative Expert Array (DEA) organized along complementary axes of manifold structure and perceptual granularity, enabling diverse forgery cues to be captured across subspaces. A dual-routing mechanism, comprising a granularity-aware sparse router and a manifold-aware dense router, adaptively assigns input images to the most relevant experts. Extensive experiments across a wide spectrum of generative models demonstrate that TrueMoE achieves superior generalization and robustness.</li>
<li><strong>摘要：</strong>生成模型的快速进步使合成图像检测成为日益关键的任务。大多数现有方法试图构建一个通用的歧视空间，以将真实的空间与假内容分开。但是，这种统一的空间往往是复杂而脆弱的，通常会努力概括地看不见的生成模式。在这项工作中，我们提出了Truemoe，这是一种新型的双路由混合物框架框架框架，将检测任务重新设计为跨多个专业和轻量级判别子空间的协作推断。 Truemoe的核心是沿歧管结构和感知粒度的互补轴组织的歧视性专家阵列（DEA），从而使各种伪造线索能够跨子空间捕获。一种双路由机制，包括粒度意识到的稀疏路由器和流形的密集路由器，将输入图像分配给最相关的专家。广泛的生成模型的广泛实验表明，Truemoe可以实现出色的概括和鲁棒性。</li>
</ul>

<h3>Title: On Optimal Steering to Achieve Exact Fairness</h3>
<ul>
<li><strong>Authors: </strong>Mohit Sharma, Amit Jayant Deshpande, Chiranjib Bhattacharyya, Rajiv Ratn Shah</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15759">https://arxiv.org/abs/2509.15759</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15759">https://arxiv.org/pdf/2509.15759</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15759]] On Optimal Steering to Achieve Exact Fairness(https://arxiv.org/abs/2509.15759)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>To fix the 'bias in, bias out' problem in fair machine learning, it is important to steer feature distributions of data or internal representations of Large Language Models (LLMs) to ideal ones that guarantee group-fair outcomes. Previous work on fair generative models and representation steering could greatly benefit from provable fairness guarantees on the model output. We define a distribution as ideal if the minimizer of any cost-sensitive risk on it is guaranteed to have exact group-fair outcomes (e.g., demographic parity, equal opportunity)-in other words, it has no fairness-utility trade-off. We formulate an optimization program for optimal steering by finding the nearest ideal distribution in KL-divergence, and provide efficient algorithms for it when the underlying distributions come from well-known parametric families (e.g., normal, log-normal). Empirically, our optimal steering techniques on both synthetic and real-world datasets improve fairness without diminishing utility (and sometimes even improve utility). We demonstrate affine steering of LLM representations to reduce bias in multi-class classification, e.g., occupation prediction from a short biography in Bios dataset (De-Arteaga et al.). Furthermore, we steer internal representations of LLMs towards desired outputs so that it works equally well across different groups.</li>
<li><strong>摘要：</strong>为了解决公平机器学习中的“偏见，偏见”问题，将数据的特征分布或大型语言模型（LLMS）的内部表示特征分布（LLMS）转移到保证群体成果的理想情况非常重要。以前关于公平生成模型和表示转向的工作可以从模型输出中可证明的公平性保证中受益匪浅。如果在其上有任何成本敏感风险的最小化器可以保证具有确切的团体成果（例如人口统计学奇偶校验，机会均等），那么我们将分布定义为理想的选择，而换句话说，它没有公平的实用性权衡。我们通过在KL-Divergence中找到最近的理想分布来制定一个优化程序，以进行最佳转向，并在基础分布来自众所周知的参数族（例如，正常，log-Normalal）时为其提供有效的算法。从经验上讲，我们对合成和现实数据集的最佳转向技术提高了公平性而不会减少效用（有时甚至可以改善效用）。我们展示了LLM表示的仿射转向，以减少多类分类中的偏差，例如，从BIOS数据集中的短传记中的职业预测（De-Arteaga等人）。此外，我们将LLMS的内部表示转向所需的输出，以使其在不同组之间同样效果。</li>
</ul>

<h3>Title: Vision-Language Models as Differentiable Semantic and Spatial Rewards for Text-to-3D Generation</h3>
<ul>
<li><strong>Authors: </strong>Weimin Bai, Yubo Li, Weijian Luo, Wenzheng Chen, He Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15772">https://arxiv.org/abs/2509.15772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15772">https://arxiv.org/pdf/2509.15772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15772]] Vision-Language Models as Differentiable Semantic and Spatial Rewards for Text-to-3D Generation(https://arxiv.org/abs/2509.15772)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Score Distillation Sampling (SDS) enables high-quality text-to-3D generation by supervising 3D models through the denoising of multi-view 2D renderings, using a pretrained text-to-image diffusion model to align with the input prompt and ensure 3D consistency. However, existing SDS-based methods face two fundamental limitations: (1) their reliance on CLIP-style text encoders leads to coarse semantic alignment and struggles with fine-grained prompts; and (2) 2D diffusion priors lack explicit 3D spatial constraints, resulting in geometric inconsistencies and inaccurate object relationships in multi-object scenes. To address these challenges, we propose VLM3D, a novel text-to-3D generation framework that integrates large vision-language models (VLMs) into the SDS pipeline as differentiable semantic and spatial priors. Unlike standard text-to-image diffusion priors, VLMs leverage rich language-grounded supervision that enables fine-grained prompt alignment. Moreover, their inherent vision language modeling provides strong spatial understanding, which significantly enhances 3D consistency for single-object generation and improves relational reasoning in multi-object scenes. We instantiate VLM3D based on the open-source Qwen2.5-VL model and evaluate it on the GPTeval3D benchmark. Experiments across diverse objects and complex scenes show that VLM3D significantly outperforms prior SDS-based methods in semantic fidelity, geometric coherence, and spatial correctness.</li>
<li><strong>摘要：</strong>得分蒸馏采样（SDS）通过使用预验证的文本对图像扩散模型来监督3D模型，通过监督3D模型来实现高质量的文本对3D生成，以与输入提示保持一致并确保3D一致性。但是，现有的基于SDS的方法面临两个基本局限性：（1）它们对剪辑式文本编码器的依赖会导致粗糙的语义一致性和与细粒度提示的斗争； （2）2D扩散先验缺乏明确的3D空间约束，导致几何不一致和多对象场景中的对象关系不准确。为了应对这些挑战，我们提出了VLM3D，这是一种新型的文本到3D生成框架，将大型视觉模型（VLMS）集成到SDS管道中，将其作为可区分的语义和空间先验。与标准的文本到图像扩散先验不同，VLMS利用了丰富的语言接地监督，可以实现细粒度的及时对齐。此外，它们固有的视觉语言建模提供了强烈的空间理解，从而显着提高了单对象生成的3D一致性，并改善了多对象场景中的关系推理。我们根据开源QWEN2.5-VL模型实例化VLM3D，并在GPTEVAL3D基准测试上进行评估。跨不同对象和复杂场景的实验表明，VLM3D在语义忠诚度，几何相干性和空间正确性方面显着优于先前基于SDS的方法。</li>
</ul>

<h3>Title: Monte Carlo Tree Diffusion with Multiple Experts for Protein Design</h3>
<ul>
<li><strong>Authors: </strong>Xuefeng Liu, Mingxuan Cao, Songhao Jiang, Xiao Luo, Xiaotian Duan, Mengdi Wang, Tobin R. Sosnick, Jinbo Xu, Rick Stevens</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15796">https://arxiv.org/abs/2509.15796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15796">https://arxiv.org/pdf/2509.15796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15796]] Monte Carlo Tree Diffusion with Multiple Experts for Protein Design(https://arxiv.org/abs/2509.15796)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The goal of protein design is to generate amino acid sequences that fold into functional structures with desired properties. Prior methods combining autoregressive language models with Monte Carlo Tree Search (MCTS) struggle with long-range dependencies and suffer from an impractically large search space. We propose MCTD-ME, Monte Carlo Tree Diffusion with Multiple Experts, which integrates masked diffusion models with tree search to enable multi-token planning and efficient exploration. Unlike autoregressive planners, MCTD-ME uses biophysical-fidelity-enhanced diffusion denoising as the rollout engine, jointly revising multiple positions and scaling to large sequence spaces. It further leverages experts of varying capacities to enrich exploration, guided by a pLDDT-based masking schedule that targets low-confidence regions while preserving reliable residues. We propose a novel multi-expert selection rule (PH-UCT-ME) extends predictive-entropy UCT to expert ensembles. On the inverse folding task (CAMEO and PDB benchmarks), MCTD-ME outperforms single-expert and unguided baselines in both sequence recovery (AAR) and structural similarity (scTM), with gains increasing for longer proteins and benefiting from multi-expert guidance. More generally, the framework is model-agnostic and applicable beyond inverse folding, including de novo protein engineering and multi-objective molecular generation.</li>
<li><strong>摘要：</strong>蛋白质设计的目的是生成将氨基酸序列折叠成具有所需特性的功能结构。将自回旋语言模型与蒙特卡洛树搜索（MCTS）相结合的先前方法与长期依赖性斗争，并且遇到了一个不切实际的搜索空间。我们提出了MCTD-ME，Monte Carlo树的扩散与多个专家，该专家将蒙面扩散模型与树搜索集成在一起，以实现多言语计划和有效的探索。与自回旋的计划者不同，MCTD-ME使用生物物理 - 增强性扩散的扩散作为推出发动机，共同修改了多个位置并扩展到大序列空间。它进一步利用了不同能力的专家来丰富探索，并在基于PLDDT的掩盖计划的指导下，该计划针对低信心区域，同时保留可靠的残留物。我们提出了一项新型的多专家选择规则（pH-uct-me）将预测性渗透性UCT扩展到专家合奏。在反折叠任务（客串和PDB基准）上，MCTD-ME在序列恢复（AAR）和结构相似性（SCTM）中均优于单一杂音和无指导基线，并且增加了更长的蛋白质，并受益于多expert Guidance。更一般而言，该框架是模型不足的，并且适用于反折叠的超越，包括从头蛋白工程和多目标分子产生。</li>
</ul>

<h3>Title: CIDER: A Causal Cure for Brand-Obsessed Text-to-Image Models</h3>
<ul>
<li><strong>Authors: </strong>Fangjian Shen, Zifeng Liang, Chao Wang, Wushao Wen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15803">https://arxiv.org/abs/2509.15803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15803">https://arxiv.org/pdf/2509.15803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15803]] CIDER: A Causal Cure for Brand-Obsessed Text-to-Image Models(https://arxiv.org/abs/2509.15803)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) models exhibit a significant yet under-explored "brand bias", a tendency to generate contents featuring dominant commercial brands from generic prompts, posing ethical and legal risks. We propose CIDER, a novel, model-agnostic framework to mitigate bias at inference-time through prompt refinement to avoid costly retraining. CIDER uses a lightweight detector to identify branded content and a Vision-Language Model (VLM) to generate stylistically divergent alternatives. We introduce the Brand Neutrality Score (BNS) to quantify this issue and perform extensive experiments on leading T2I models. Results show CIDER significantly reduces both explicit and implicit biases while maintaining image quality and aesthetic appeal. Our work offers a practical solution for more original and equitable content, contributing to the development of trustworthy generative AI.</li>
<li><strong>摘要：</strong>文本对图像（T2I）模型表现出严重但爆炸不足的“品牌偏见”，这种趋势倾向于从通用提示中产生占主导地位的商业品牌的内容，从而带来道德和法律风险。我们提出了一种新颖的，模型的无形框架苹果酒，以通过及时的细化来减轻推理时的偏见，以避免昂贵的再训练。苹果酒使用轻质检测器来识别品牌内容和视觉语言模型（VLM），以生成风格上不同的替代方案。我们介绍了品牌中立分数（BNS），以量化此问题并对领先的T2I模型进行广泛的实验。结果表明，苹果酒大大减少了显式和隐性偏见，同时保持图像质量和美学吸引力。我们的工作为更原始和公平的内容提供了实用的解决方案，这有助于可信赖的生成AI的发展。</li>
</ul>

<h3>Title: Instance Generation for Meta-Black-Box Optimization through Latent Space Reverse Engineering</h3>
<ul>
<li><strong>Authors: </strong>Chen Wang, Zeyuan Ma, Zhiguang Cao, Yue-Jiao Gong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15810">https://arxiv.org/abs/2509.15810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15810">https://arxiv.org/pdf/2509.15810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15810]] Instance Generation for Meta-Black-Box Optimization through Latent Space Reverse Engineering(https://arxiv.org/abs/2509.15810)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>To relieve intensive human-expertise required to design optimization algorithms, recent Meta-Black-Box Optimization (MetaBBO) researches leverage generalization strength of meta-learning to train neural network-based algorithm design policies over a predefined training problem set, which automates the adaptability of the low-level optimizers on unseen problem instances. Currently, a common training problem set choice in existing MetaBBOs is well-known benchmark suites CoCo-BBOB. Although such choice facilitates the MetaBBO's development, problem instances in CoCo-BBOB are more or less limited in diversity, raising the risk of overfitting of MetaBBOs, which might further results in poor generalization. In this paper, we propose an instance generation approach, termed as \textbf{LSRE}, which could generate diverse training problem instances for MetaBBOs to learn more generalizable policies. LSRE first trains an autoencoder which maps high-dimensional problem features into a 2-dimensional latent space. Uniform-grid sampling in this latent space leads to hidden representations of problem instances with sufficient diversity. By leveraging a genetic-programming approach to search function formulas with minimal L2-distance to these hidden representations, LSRE reverse engineers a diversified problem set, termed as \textbf{Diverse-BBO}. We validate the effectiveness of LSRE by training various MetaBBOs on Diverse-BBO and observe their generalization performances on either synthetic or realistic scenarios. Extensive experimental results underscore the superiority of Diverse-BBO to existing training set choices in MetaBBOs. Further ablation studies not only demonstrate the effectiveness of design choices in LSRE, but also reveal interesting insights on instance diversity and MetaBBO's generalization.</li>
<li><strong>摘要：</strong>为了缓解设计优化算法所需的密集型人类专家，最近的元黑色优化优化（METABBO）研究利用元学习的概括强度来培训基于神经网络的算法设计政策，而不是预定义的训练问题集，从而使低级优化者对尚未证明的实例的适应性自动化。当前，现有的元BOS中常见的培训问题设定选择是众所周知的基准套件可可bob。尽管这种选择促进了Metabbo的发展，但可可bob的问题实例或多或少受到多样性的限制，从而增加了Metabbos过度拟合的风险，这可能会进一步导致概括不佳。在本文中，我们提出了一种实例生成方法，称为\ textbf {lsre}，该方法可能会为Metabbos生成多样化的培训问题实例，以学习更多可推广的策略。 LSRE首先训练一个自动编码器，该自动编码器将高维问题映射到二维潜在空间中。在这个潜在空间中的统一网格采样导致具有足够多样性的问题实例的隐藏表示。通过利用一种基因编程方法来搜索功能公式，对这些隐藏表示形式具有最小的L2距离，LSRE反向工程师一个多元化的问题集，称为\ textbf {diversee-bbo}。我们通过训练各种元BBO来验证LSRE的有效性，并在合成或现实情况下观察它们的概括性能。广泛的实验结果强调了不同BBO对Metabbos中现有的训练集选择的优势。进一步的消融研究不仅证明了LSRE设计选择的有效性，而且还揭示了有关实例多样性和Metabbo概括的有趣见解。</li>
</ul>

<h3>Title: HyP-ASO: A Hybrid Policy-based Adaptive Search Optimization Framework for Large-Scale Integer Linear Programs</h3>
<ul>
<li><strong>Authors: </strong>Ning Xu, Junkai Zhang, Yang Wu, Huigen Ye, Hua Xu, Huiling Xu, Yifan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15828">https://arxiv.org/abs/2509.15828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15828">https://arxiv.org/pdf/2509.15828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15828]] HyP-ASO: A Hybrid Policy-based Adaptive Search Optimization Framework for Large-Scale Integer Linear Programs(https://arxiv.org/abs/2509.15828)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Directly solving large-scale Integer Linear Programs (ILPs) using traditional solvers is slow due to their NP-hard nature. While recent frameworks based on Large Neighborhood Search (LNS) can accelerate the solving process, their performance is often constrained by the difficulty in generating sufficiently effective neighborhoods. To address this challenge, we propose HyP-ASO, a hybrid policy-based adaptive search optimization framework that combines a customized formula with deep Reinforcement Learning (RL). The formula leverages feasible solutions to calculate the selection probabilities for each variable in the neighborhood generation process, and the RL policy network predicts the neighborhood size. Extensive experiments demonstrate that HyP-ASO significantly outperforms existing LNS-based approaches for large-scale ILPs. Additional experiments show it is lightweight and highly scalable, making it well-suited for solving large-scale ILPs.</li>
<li><strong>摘要：</strong>直接使用传统求解器直接求解大型整数线性程序（ILP），由于其NP-HARD性质，因此很慢。尽管基于大型邻里搜索（LN）的最新框架可以加速解决过程，但它们的性能通常受到产生足够有效的社区的困难的限制。为了应对这一挑战，我们提出了Hyp-Aso，这是一种基于混合政策的自适应搜索优化框架，将定制公式与深入增强学习（RL）相结合。该公式利用可行的解决方案来计算邻里生成过程中每个变量的选择概率，RL策略网络预测了邻域的大小。广泛的实验表明，Hyp-Aso明显优于大规模ILP的现有基于LNS的方法。其他实验表明它是轻巧且高度可扩展的，非常适合解决大规模ILP。</li>
</ul>

<h3>Title: SAGE: Semantic-Aware Shared Sampling for Efficient Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Haoran Zhao, Tong Bai, Lei Huang, Xiaoyu Liang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15865">https://arxiv.org/abs/2509.15865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15865">https://arxiv.org/pdf/2509.15865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15865]] SAGE: Semantic-Aware Shared Sampling for Efficient Diffusion(https://arxiv.org/abs/2509.15865)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models manifest evident benefits across diverse domains, yet their high sampling cost, requiring dozens of sequential model evaluations, remains a major limitation. Prior efforts mainly accelerate sampling via optimized solvers or distillation, which treat each query independently. In contrast, we reduce total number of steps by sharing early-stage sampling across semantically similar queries. To enable such efficiency gains without sacrificing quality, we propose SAGE, a semantic-aware shared sampling framework that integrates a shared sampling scheme for efficiency and a tailored training strategy for quality preservation. Extensive experiments show that SAGE reduces sampling cost by 25.5%, while improving generation quality with 5.0% lower FID, 5.4% higher CLIP, and 160% higher diversity over baselines.</li>
<li><strong>摘要：</strong>扩散模型表现出各种领域的明显收益，但是它们的高采样成本（需要数十个模型评估）仍然是一个主要限制。先前的努力主要通过优化的求解器或蒸馏加速采样，这些求解器独立处理每个查询。相比之下，我们通过在语义上相似的查询中共享早期抽样来减少步骤总数。为了使这种效率提高而不牺牲质量，我们提出了Sage，这是一种语义意识的共享抽样框架，该框架集成了共享的抽样方案，以供效率和量身定制的培训策略，以保存质量。广泛的实验表明，SAGE将采样成本降低了25.5％，同时以低于5.0％的FID，夹提高5.4％和比基线的多样性提高了5.0％的发电质量。</li>
</ul>

<h3>Title: Foundation Models as World Models: A Foundational Study in Text-Based GridWorlds</h3>
<ul>
<li><strong>Authors: </strong>Remo Sasso, Michelangelo Conserva, Dominik Jeurissen, Paulo Rauber</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15915">https://arxiv.org/abs/2509.15915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15915">https://arxiv.org/pdf/2509.15915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15915]] Foundation Models as World Models: A Foundational Study in Text-Based GridWorlds(https://arxiv.org/abs/2509.15915)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While reinforcement learning from scratch has shown impressive results in solving sequential decision-making tasks with efficient simulators, real-world applications with expensive interactions require more sample-efficient agents. Foundation models (FMs) are natural candidates to improve sample efficiency as they possess broad knowledge and reasoning capabilities, but it is yet unclear how to effectively integrate them into the reinforcement learning framework. In this paper, we anticipate and, most importantly, evaluate two promising strategies. First, we consider the use of foundation world models (FWMs) that exploit the prior knowledge of FMs to enable training and evaluating agents with simulated interactions. Second, we consider the use of foundation agents (FAs) that exploit the reasoning capabilities of FMs for decision-making. We evaluate both approaches empirically in a family of grid-world environments that are suitable for the current generation of large language models (LLMs). Our results suggest that improvements in LLMs already translate into better FWMs and FAs; that FAs based on current LLMs can already provide excellent policies for sufficiently simple environments; and that the coupling of FWMs and reinforcement learning agents is highly promising for more complex settings with partial observability and stochastic elements.</li>
<li><strong>摘要：</strong>虽然从头开始的加强学习在通过有效的模拟器解决顺序决策任务方面显示出令人印象深刻的结果，但具有昂贵相互作用的现实世界应用需要更高的样品效率的代理。基础模型（FMS）是自然候选人，可以提高样本效率，因为它们具有广泛的知识和推理能力，但尚不清楚如何将它们有效地整合到增强学习框架中。在本文中，我们预期，最重要的是，评估了两种有希望的策略。首先，我们考虑使用基础世界模型（FWM）来利用FMS的先验知识来实现​​通过模拟相互作用进行培训和评估代理。其次，我们考虑使用基础代理（FAS）利用FMS的推理能力进行决策。我们在适合当前一代大型语言模型（LLMS）的网格世界家族中进行经验评估这两种方法。我们的结果表明，LLM的改进已经转化为更好的FWM和FAS。基于当前LLM的FA已经可以为足够简单的环境提供出色的政策； FWM和增强学习剂的耦合对于具有部分可观察性和随机元素的更为复杂的环境非常有前途。</li>
</ul>

<h3>Title: Enhancing Generative Auto-bidding with Offline Reward Evaluation and Policy Search</h3>
<ul>
<li><strong>Authors: </strong>Zhiyu Mou, Yiqin Lv, Miao Xu, Cheems Wang, Yixiu Mao, Qichen Ye, Chao Li, Rongquan Bai, Chuan Yu, Jian Xu, Bo Zheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15927">https://arxiv.org/abs/2509.15927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15927">https://arxiv.org/pdf/2509.15927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15927]] Enhancing Generative Auto-bidding with Offline Reward Evaluation and Policy Search(https://arxiv.org/abs/2509.15927)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Auto-bidding is an essential tool for advertisers to enhance their advertising performance. Recent progress has shown that AI-Generated Bidding (AIGB), which formulates the auto-bidding as a trajectory generation task and trains a conditional diffusion-based planner on offline data, achieves superior and stable performance compared to typical offline reinforcement learning (RL)-based auto-bidding methods. However, existing AIGB methods still encounter a performance bottleneck due to their neglect of fine-grained generation quality evaluation and inability to explore beyond static datasets. To address this, we propose AIGB-Pearl (\emph{Planning with EvAluator via RL}), a novel method that integrates generative planning and policy optimization. The key to AIGB-Pearl is to construct a non-bootstrapped \emph{trajectory evaluator} to assign rewards and guide policy search, enabling the planner to optimize its generation quality iteratively through interaction. Furthermore, to enhance trajectory evaluator accuracy in offline settings, we incorporate three key techniques: (i) a Large Language Model (LLM)-based architecture for better representational capacity, (ii) hybrid point-wise and pair-wise losses for better score learning, and (iii) adaptive integration of expert feedback for better generalization ability. Extensive experiments on both simulated and real-world advertising systems demonstrate the state-of-the-art performance of our approach.</li>
<li><strong>摘要：</strong>自动投标是广告商增强广告性能的重要工具。最近的进步表明，AI生成的竞标（AIGB）将自动铸造作为轨迹生成任务，并在离线数据上训练基于条件扩散的计划者，与典型的离线加固学习（RL）基于典型的离线增强学习方法相比，取得了卓越和稳定的性能。但是，由于忽视了细粒度的生成质量评估和无法探索静态数据集的探索，因此现有的AIGB方法仍然遇到性能瓶颈。为了解决这个问题，我们提出了AIGB-PEARL（\ emph {通过RL}进行评估者计划），这是一种集成生成计划和策略优化的新方法。 AIGB-PEARL的关键是构建非引导\ emph {轨迹评估器}来分配奖励和指导策略搜索，从而使计划者能够通过交互通过交互来优化其生成质量。此外，为了提高离线设置中的轨迹评估器的精度，我们合并了三种关键技术：（i）基于大的语言模型（LLM）基于更好的代表能力的架构，（ii）用于更好的分数学习的混合点和配对损失，以及（III）（iii）专家反馈的适应性整合以更好地整合一般的总体化能力。对模拟和现实世界广告系统的广泛实验证明了我们方法的最新性能。</li>
</ul>

<h3>Title: Rethinking Molecule Synthesizability with Chain-of-Reaction</h3>
<ul>
<li><strong>Authors: </strong>Seul Lee, Karsten Kreis, Srimukh Prasad Veccham, Meng Liu, Danny Reidenbach, Saee Paliwal, Weili Nie, Arash Vahdat</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16084">https://arxiv.org/abs/2509.16084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16084">https://arxiv.org/pdf/2509.16084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16084]] Rethinking Molecule Synthesizability with Chain-of-Reaction(https://arxiv.org/abs/2509.16084)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>A well-known pitfall of molecular generative models is that they are not guaranteed to generate synthesizable molecules. There have been considerable attempts to address this problem, but given the exponentially large combinatorial space of synthesizable molecules, existing methods have shown limited coverage of the space and poor molecular optimization performance. To tackle these problems, we introduce ReaSyn, a generative framework for synthesizable projection where the model explores the neighborhood of given molecules in the synthesizable space by generating pathways that result in synthesizable analogs. To fully utilize the chemical knowledge contained in the synthetic pathways, we propose a novel perspective that views synthetic pathways akin to reasoning paths in large language models (LLMs). Specifically, inspired by chain-of-thought (CoT) reasoning in LLMs, we introduce the chain-of-reaction (CoR) notation that explicitly states reactants, reaction types, and intermediate products for each step in a pathway. With the CoR notation, ReaSyn can get dense supervision in every reaction step to explicitly learn chemical reaction rules during supervised training and perform step-by-step reasoning. In addition, to further enhance the reasoning capability of ReaSyn, we propose reinforcement learning (RL)-based finetuning and goal-directed test-time compute scaling tailored for synthesizable projection. ReaSyn achieves the highest reconstruction rate and pathway diversity in synthesizable molecule reconstruction and the highest optimization performance in synthesizable goal-directed molecular optimization, and significantly outperforms previous synthesizable projection methods in synthesizable hit expansion. These results highlight ReaSyn's superior ability to navigate combinatorially-large synthesizable chemical space.</li>
<li><strong>摘要：</strong>分子生成模型的一个众所周知的陷阱是，不能保证它们会产生合成的分子。已经进行了大量尝试来解决这个问题，但是鉴于可合成分子的指数组合空间，现有方法显示出对空间的覆盖范围有限，并且分子优化性能差。为了解决这些问题，我们引入了Reasyn，这是一个可合成投影的生成框架，该模型通过生成可导致可综合类似物的途径来探索合成空间中给定分子的邻域。为了充分利用合成途径中包含的化学知识，我们提出了一种新颖的观点，该视角认为类似于大语言模型（LLMS）的推理路径的合成途径。具体而言，受到LLMS中的经营链（COT）推理的启发，我们引入了反应链（COR）符号，即在途径中的每个步骤中明确指出反应物，反应类型和中间产品。使用COR符号，Reasyn可以在每个反应步骤中得到严格的监督，以在监督培训期间明确学习化学反应规则并进行逐步推理。此外，为了进一步增强Reasyn的推理能力，我们提出了加固学习（RL）基于基于综合投影量身定制的基于基于目标的测试时间计算缩放标度。 REASYN在合成的分子重建中实现了最高的重建率和途径多样性，并且在合成目标指导的分子优化中达到了最高的优化性能，并且在合成的命中率膨胀中显着优于先前合成的投影方法。这些结果突出了Reasyn的卓越能力，可以导航大量合成的化学空间。</li>
</ul>

<h3>Title: Randomized Smoothing Meets Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Emmanouil Seferis, Changshun Wu, Stefanos Kollias, Saddek Bensalem, Chih-Hong Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16088">https://arxiv.org/abs/2509.16088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16088">https://arxiv.org/pdf/2509.16088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16088]] Randomized Smoothing Meets Vision-Language Models(https://arxiv.org/abs/2509.16088)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Randomized smoothing (RS) is one of the prominent techniques to ensure the correctness of machine learning models, where point-wise robustness certificates can be derived analytically. While RS is well understood for classification, its application to generative models is unclear, since their outputs are sequences rather than labels. We resolve this by connecting generative outputs to an oracle classification task and showing that RS can still be enabled: the final response can be classified as a discrete action (e.g., service-robot commands in VLAs), as harmful vs. harmless (content moderation or toxicity detection in VLMs), or even applying oracles to cluster answers into semantically equivalent ones. Provided that the error rate for the oracle classifier comparison is bounded, we develop the theory that associates the number of samples with the corresponding robustness radius. We further derive improved scaling laws analytically relating the certified radius and accuracy to the number of samples, showing that the earlier result of 2 to 3 orders of magnitude fewer samples sufficing with minimal loss remains valid even under weaker assumptions. Together, these advances make robustness certification both well-defined and computationally feasible for state-of-the-art VLMs, as validated against recent jailbreak-style adversarial attacks.</li>
<li><strong>摘要：</strong>随机平滑（RS）是确保机器学习模型正确性的突出技术之一，可以通过分析得出稳健性证书。尽管RS用于分类，但其应用于生成模型尚不清楚，因为它们的输出是序列而不是标签。我们通过将生成性输出连接到Oracle分类任务来解决此问题，并表明RS仍然可以启用：可以将最终响应归类为离散的操作（例如，VLAS中的服务 - 机器人命令），有害与无害的无害（VLMS中的内容调节或毒性检测），甚至将Oracles的答案应用于语义上的答案中。前提是对Oracle分类器比较的错误率有界限，我们将开发将样品数与相应鲁棒性半径相关联的理论。我们进一步得出了分析的缩放定律，分析了验证半径和准确性与样品数量的数量，表明即使在较弱的假设下，足以最小损失的样本的较早结果较少。这些进步共同使最先进的VLM的鲁棒性认证在定义明确和计算上是可行的，这是针对最近的越狱风格的对抗性攻击的确认。</li>
</ul>

<h3>Title: DiffusionNFT: Online Diffusion Reinforcement with Forward Process</h3>
<ul>
<li><strong>Authors: </strong>Kaiwen Zheng, Huayu Chen, Haotian Ye, Haoxiang Wang, Qinsheng Zhang, Kai Jiang, Hang Su, Stefano Ermon, Jun Zhu, Ming-Yu Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16117">https://arxiv.org/abs/2509.16117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16117">https://arxiv.org/pdf/2509.16117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16117]] DiffusionNFT: Online Diffusion Reinforcement with Forward Process(https://arxiv.org/abs/2509.16117)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Online reinforcement learning (RL) has been central to post-training language models, but its extension to diffusion models remains challenging due to intractable likelihoods. Recent works discretize the reverse sampling process to enable GRPO-style training, yet they inherit fundamental drawbacks, including solver restrictions, forward-reverse inconsistency, and complicated integration with classifier-free guidance (CFG). We introduce Diffusion Negative-aware FineTuning (DiffusionNFT), a new online RL paradigm that optimizes diffusion models directly on the forward process via flow matching. DiffusionNFT contrasts positive and negative generations to define an implicit policy improvement direction, naturally incorporating reinforcement signals into the supervised learning objective. This formulation enables training with arbitrary black-box solvers, eliminates the need for likelihood estimation, and requires only clean images rather than sampling trajectories for policy optimization. DiffusionNFT is up to $25\times$ more efficient than FlowGRPO in head-to-head comparisons, while being CFG-free. For instance, DiffusionNFT improves the GenEval score from 0.24 to 0.98 within 1k steps, while FlowGRPO achieves 0.95 with over 5k steps and additional CFG employment. By leveraging multiple reward models, DiffusionNFT significantly boosts the performance of SD3.5-Medium in every benchmark tested.</li>
<li><strong>摘要：</strong>在线增强学习（RL）对培训后语言模型至关重要，但是由于棘手的可能性，其扩散模型的扩展模型仍然具有挑战性。最近的工作将反向采样过程离散以实现GRPO风格的训练，但它们继承了基本缺点，包括求解器限制，前向反向不一致以及与无分类器指导（CFG）的复杂集成。我们介绍了扩散的负感知芬特（DiffusionNFT），这是一种新的在线RL范式，可通过流匹配直接在正向过程上优化扩散模型。 diffusionnft将正面和负面的世代对比，以定义隐性的政策改进方向，自然地将强化信号纳入监督学习目标。该公式可以通过任意黑框求解器进行培训，消除了对似然估计的需求，并且仅需要干净的图像而不是采样轨迹进行策略优化。 diffusionnft的$ 25 \ times $ $比Flowgrpo在正面比较的同时不含CFG。例如，diffusionnft在1k步骤内将遗传评分从0.24提高到0.98，而FlowGrpo则以超过5k步骤和其他CFG就业方式达到0.95。通过利用多个奖励模型，diffusionnft显着提高了所测试的每个基准测试中的SD3.5-米形的性能。</li>
</ul>

<h3>Title: BaseReward: A Strong Baseline for Multimodal Reward Model</h3>
<ul>
<li><strong>Authors: </strong>Yi-Fan Zhang, Haihua Yang, Huanyu Zhang, Yang Shi, Zezhou Chen, Haochen Tian, Chaoyou Fu, Haotian Wang, Kai Wu, Bo Cui, Xu Wang, Jianfei Pan, Haotian Wang, Zhang Zhang, Liang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16127">https://arxiv.org/abs/2509.16127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16127">https://arxiv.org/pdf/2509.16127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16127]] BaseReward: A Strong Baseline for Multimodal Reward Model(https://arxiv.org/abs/2509.16127)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of Multimodal Large Language Models (MLLMs) has made aligning them with human preferences a critical challenge. Reward Models (RMs) are a core technology for achieving this goal, but a systematic guide for building state-of-the-art Multimodal Reward Models (MRMs) is currently lacking in both academia and industry. Through exhaustive experimental analysis, this paper aims to provide a clear ``recipe'' for constructing high-performance MRMs. We systematically investigate every crucial component in the MRM development pipeline, including \textit{reward modeling paradigms} (e.g., Naive-RM, Critic-based RM, and Generative RM), \textit{reward head architecture}, \textit{training strategies}, \textit{data curation} (covering over ten multimodal and text-only preference datasets), \textit{backbone model} and \textit{model scale}, and \textit{ensemble methods}. Based on these experimental insights, we introduce \textbf{BaseReward}, a powerful and efficient baseline for multimodal reward modeling. BaseReward adopts a simple yet effective architecture, built upon a {Qwen2.5-VL} backbone, featuring an optimized two-layer reward head, and is trained on a carefully curated mixture of high-quality multimodal and text-only preference data. Our results show that BaseReward establishes a new SOTA on major benchmarks such as MM-RLHF-Reward Bench, VL-Reward Bench, and Multimodal Reward Bench, outperforming previous models. Furthermore, to validate its practical utility beyond static benchmarks, we integrate BaseReward into a real-world reinforcement learning pipeline, successfully enhancing an MLLM's performance across various perception, reasoning, and conversational tasks. This work not only delivers a top-tier MRM but, more importantly, provides the community with a clear, empirically-backed guide for developing robust reward models for the next generation of MLLMs.</li>
<li><strong>摘要：</strong>多模式大语言模型（MLLM）的快速发展使他们与人类偏好保持一致。奖励模型（RMS）是实现这一目标的核心技术，但是目前在学术界和行业中都缺乏建立最先进的多模式奖励模型（MRM）的系统指南。通过详尽的实验分析，本文旨在提供清晰的``食谱''，用于构建高性能MRMS。 We systematically investigate every crucial component in the MRM development pipeline, including \textit{reward modeling paradigms} (e.g., Naive-RM, Critic-based RM, and Generative RM), \textit{reward head architecture}, \textit{training strategies}, \textit{data curation} (covering over ten multimodal and text-only preference datasets), \ textIt {backbone模型}和\ textit {模型刻度}，\ textit {elesmember方法}。基于这些实验见解，我们介绍了\ textbf {baseRERWARD}，这是多模式奖励建模的强大而有效的基线。 BaseRERWERD采用了一个简单而有效的体系结构，该体系结构建立在{QWEN2.5-VL}骨干线上，具有优化的两层奖励头，并接受了精心策划的高质量多模式和仅文本优先偏好数据的精心策划的混合物。我们的结果表明，BaseRERWERD在主要基准上建立了新的SOTA，例如MM-RLHF-Reward-Reward Banch，VL-Reward Banch和多模式奖励台，表现优于先前的模型。此外，为了验证其实际实用程序以外的静态基准，我们将Baseardward整合到现实世界中的强化学习管道中，成功地在各种感知，推理和对话任务中成功提高了MLLM的表现。这项工作不仅提供了顶级MRM，而且更重要的是，为社区提供了一个清晰的，经验丰富的指南，以为下一代MLLM开发可靠的奖励模型。</li>
</ul>

<h3>Title: Dynamic Classifier-Free Diffusion Guidance via Online Feedback</h3>
<ul>
<li><strong>Authors: </strong>Pinelopi Papalampidi, Olivia Wiles, Ira Ktena, Aleksandar Shtedritski, Emanuele Bugliarello, Ivana Kajic, Isabela Albuquerque, Aida Nematzadeh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16131">https://arxiv.org/abs/2509.16131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16131">https://arxiv.org/pdf/2509.16131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16131]] Dynamic Classifier-Free Diffusion Guidance via Online Feedback(https://arxiv.org/abs/2509.16131)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Classifier-free guidance (CFG) is a cornerstone of text-to-image diffusion models, yet its effectiveness is limited by the use of static guidance scales. This "one-size-fits-all" approach fails to adapt to the diverse requirements of different prompts; moreover, prior solutions like gradient-based correction or fixed heuristic schedules introduce additional complexities and fail to generalize. In this work, we challeng this static paradigm by introducing a framework for dynamic CFG scheduling. Our method leverages online feedback from a suite of general-purpose and specialized small-scale latent-space evaluations, such as CLIP for alignment, a discriminator for fidelity and a human preference reward model, to assess generation quality at each step of the reverse diffusion process. Based on this feedback, we perform a greedy search to select the optimal CFG scale for each timestep, creating a unique guidance schedule tailored to every prompt and sample. We demonstrate the effectiveness of our approach on both small-scale models and the state-of-the-art Imagen 3, showing significant improvements in text alignment, visual quality, text rendering and numerical reasoning. Notably, when compared against the default Imagen 3 baseline, our method achieves up to 53.8% human preference win-rate for overall preference, a figure that increases up to to 55.5% on prompts targeting specific capabilities like text rendering. Our work establishes that the optimal guidance schedule is inherently dynamic and prompt-dependent, and provides an efficient and generalizable framework to achieve it.</li>
<li><strong>摘要：</strong>无分类器指导（CFG）是文本对图像扩散模型的基石，但其有效性受到使用静态指导量表的限制。这种“千篇一律的”方法无法适应不同提示的各种要求。此外，诸如基于梯度的校正或固定启发式时间表之类的先前解决方案会引入其他复杂性，并且无法概括。在这项工作中，我们通过引入动态CFG调度框架来挑战这种静态范式。我们的方法利用了一套通用和专业的小型潜在空间评估的在线反馈，例如clip for Alignment，fordelity和人类偏好奖励模型，以评估反向扩散过程的每个步骤的发电质量。基于此反馈，我们执行贪婪的搜索，以选择每个时间段的最佳CFG量表，从而为每个提示和样本量身定制一个独特的指导时间表。我们证明了方法对小型模型和最先进的成像3的有效性，显示了文本对齐，视觉质量，文本渲染和数值推理的显着改善。值得注意的是，与默认成像3基线相比，我们的方法在整体偏好方面达到了53.8％的人类偏好率，这一数字在针对特定功能（例如文本渲染）的提示中最高可提高到55.5％。我们的工作确定，最佳的指导时间表本质上是动态的和迅速依赖的，并提供了一个有效且可推广的框架来实现它。</li>
</ul>

<h3>Title: AcT2I: Evaluating and Improving Action Depiction in Text-to-Image Models</h3>
<ul>
<li><strong>Authors: </strong>Vatsal Malaviya, Agneet Chatterjee, Maitreya Patel, Yezhou Yang, Chitta Baral</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16141">https://arxiv.org/abs/2509.16141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16141">https://arxiv.org/pdf/2509.16141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16141]] AcT2I: Evaluating and Improving Action Depiction in Text-to-Image Models(https://arxiv.org/abs/2509.16141)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-Image (T2I) models have recently achieved remarkable success in generating images from textual descriptions. However, challenges still persist in accurately rendering complex scenes where actions and interactions form the primary semantic focus. Our key observation in this work is that T2I models frequently struggle to capture nuanced and often implicit attributes inherent in action depiction, leading to generating images that lack key contextual details. To enable systematic evaluation, we introduce AcT2I, a benchmark designed to evaluate the performance of T2I models in generating images from action-centric prompts. We experimentally validate that leading T2I models do not fare well on AcT2I. We further hypothesize that this shortcoming arises from the incomplete representation of the inherent attributes and contextual dependencies in the training corpora of existing T2I models. We build upon this by developing a training-free, knowledge distillation technique utilizing Large Language Models to address this limitation. Specifically, we enhance prompts by incorporating dense information across three dimensions, observing that injecting prompts with temporal details significantly improves image generation accuracy, with our best model achieving an increase of 72%. Our findings highlight the limitations of current T2I methods in generating images that require complex reasoning and demonstrate that integrating linguistic knowledge in a systematic way can notably advance the generation of nuanced and contextually accurate images.</li>
<li><strong>摘要：</strong>文本对图像（T2I）模型最近在从文本描述中生成图像方面取得了巨大的成功。但是，挑战仍然存在于准确地渲染复杂场景的情况下，在这些场景中，动作和互动构成了主要语义重点。我们在这项工作中的主要观察结果是，T2I模型经常难以捕获动作描述中固有的细微差异和隐含属性，从而导致产生缺乏关键上下文细节的图像。为了实现系统评估，我们介绍了ACT2I，这是一种基准测试，旨在评估T2i模型在以动作为中心提示中生成图像的性能。我们通过实验验证了领先的T2I模型在ACT2I上的表现不佳。我们进一步假设，这种缺点是源于现有T2I模型培训公司中固有属性和上下文依赖性的不完整表示。我们通过利用大型语言模型来解决这种限制的无培训，知识蒸馏技术来建立这一点。具体而言，我们通过在三个维度上合并密集的信息来增强提示，并观察到为时间细节注射提示可以显着提高图像的生成精度，而我们的最佳模型可以增长72％。我们的发现突出了当前T2I方法在生成需要复杂推理的图像中的局限性，并证明以系统的方式整合语言知识可以显着提高细微差别和上下文准确的图像的产生。</li>
</ul>

<h3>Title: MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer</h3>
<ul>
<li><strong>Authors: </strong>Yanghao Li, Rui Qian, Bowen Pan, Haotian Zhang, Haoshuo Huang, Bowen Zhang, Jialing Tong, Haoxuan You, Xianzhi Du, Zhe Gan, Hyunjik Kim, Chao Jia, Zhenbang Wang, Yinfei Yang, Mingfei Gao, Zi-Yi Dou, Wenze Hu, Chang Gao, Dongxu Li, Philipp Dufter, Zirui Wang, Guoli Yin, Zhengdong Zhang, Chen Chen, Yang Zhao, Ruoming Pang, Zhifeng Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16197">https://arxiv.org/abs/2509.16197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16197">https://arxiv.org/pdf/2509.16197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16197]] MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer(https://arxiv.org/abs/2509.16197)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Unified multimodal Large Language Models (LLMs) that can both understand and generate visual content hold immense potential. However, existing open-source models often suffer from a performance trade-off between these capabilities. We present Manzano, a simple and scalable unified framework that substantially reduces this tension by coupling a hybrid image tokenizer with a well-curated training recipe. A single shared vision encoder feeds two lightweight adapters that produce continuous embeddings for image-to-text understanding and discrete tokens for text-to-image generation within a common semantic space. A unified autoregressive LLM predicts high-level semantics in the form of text and image tokens, with an auxiliary diffusion decoder subsequently translating the image tokens into pixels. The architecture, together with a unified training recipe over understanding and generation data, enables scalable joint learning of both capabilities. Manzano achieves state-of-the-art results among unified models, and is competitive with specialist models, particularly on text-rich evaluation. Our studies show minimal task conflicts and consistent gains from scaling model size, validating our design choice of a hybrid tokenizer.</li>
<li><strong>摘要：</strong>既可以理解又产生视觉内容的统一多模式大型语言模型（LLM）具有巨大的潜力。但是，现有的开源模型通常会遭受这些功能之间的性能权衡。我们提出了Manzano，这是一个简单且可扩展的统一框架，可通过将混合图像令牌与精心策划的训练配方耦合，从而大大降低了这种张力。一个共享的视觉编码器为两个轻巧的适配器提供了连续的嵌入，以在公共语义空间内产生图像到文本的理解和离散令牌的连续嵌入。统一的自回归LLM以文本和图像令牌的形式预测了高级语义，辅助扩散解码器随后将图像令牌转换为像素。该体系结构以及对理解和生成数据的统一培训配方，可实现这两种功能的可扩展联合学习。曼萨诺（Manzano）在统一模型中取得了最新的结果，并且与专业模型具有竞争力，尤其是在文本丰富的评估方面。我们的研究表明，任务冲突最小，并从缩放模型大小中获得一致的收益，从而验证了我们对混合代币仪的设计选择。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
