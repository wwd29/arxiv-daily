<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-12-10</h1>
<h3>Title: Command & Control (C2) Traffic Detection Via Algorithm Generated Domain (Dga) Classification Using Deep Learning And Natural Language Processing</h3>
<ul>
<li><strong>Authors: </strong>Maria Milena Araujo Felix</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.07866">https://arxiv.org/abs/2512.07866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.07866">https://arxiv.org/pdf/2512.07866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.07866]] Command & Control (C2) Traffic Detection Via Algorithm Generated Domain (Dga) Classification Using Deep Learning And Natural Language Processing(https://arxiv.org/abs/2512.07866)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The sophistication of modern malware, specifically regarding communication with Command and Control (C2) servers, has rendered static blacklist-based defenses obsolete. The use of Domain Generation Algorithms (DGA) allows attackers to generate thousands of dynamic addresses daily, hindering blocking by traditional firewalls. This paper aims to propose and evaluate a method for detecting DGA domains using Deep Learning and Natural Language Processing (NLP) techniques. The methodology consisted of collecting a hybrid database containing 50,000 legitimate and 50,000 malicious domains, followed by the extraction of lexical features and the training of a Recurrent Neural Network (LSTM). Results demonstrated that while statistical entropy analysis is effective for simple DGAs, the Neural Network approach presents superiority in detecting complex patterns, reaching 97.2% accuracy and reducing the false positive rate in ambiguous lawful traffic scenarios.</li>
<li><strong>摘要：</strong>现代恶意软件的复杂性，特别是与命令和控制 (C2) 服务器的通信，使得基于静态黑名单的防御变得过时。域生成算法 (DGA) 的使用允许攻击者每天生成数千个动态地址，从而阻碍传统防火墙的阻止。本文旨在提出并评估一种使用深度学习和自然语言处理（NLP）技术检测 DGA 域的方法。该方法包括收集包含 50,000 个合法域和 50,000 个恶意域的混合数据库，然后提取词汇特征并训练循环神经网络 (LSTM)。结果表明，虽然统计熵分析对于简单的 DGA 是有效的，但神经网络方法在检测复杂模式方面具有优势，达到 97.2% 的准确率，并降低了模糊合法交通场景中的误报率。</li>
</ul>

<h3>Title: Controllable risk scenario generation from human crash data for autonomous vehicle testing</h3>
<ul>
<li><strong>Authors: </strong>Qiujing Lu, Xuanhan Wang, Runze Yuan, Wei Lu, Xinyi Gong, Shuo Feng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.07874">https://arxiv.org/abs/2512.07874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.07874">https://arxiv.org/pdf/2512.07874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.07874]] Controllable risk scenario generation from human crash data for autonomous vehicle testing(https://arxiv.org/abs/2512.07874)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Ensuring the safety of autonomous vehicles (AV) requires rigorous testing under both everyday driving and rare, safety-critical conditions. A key challenge lies in simulating environment agents, including background vehicles (BVs) and vulnerable road users (VRUs), that behave realistically in nominal traffic while also exhibiting risk-prone behaviors consistent with real-world accidents. We introduce Controllable Risk Agent Generation (CRAG), a framework designed to unify the modeling of dominant nominal behaviors and rare safety-critical behaviors. CRAG constructs a structured latent space that disentangles normal and risk-related behaviors, enabling efficient use of limited crash data. By combining risk-aware latent representations with optimization-based mode-transition mechanisms, the framework allows agents to shift smoothly and plausibly from safe to risk states over extended horizons, while maintaining high fidelity in both regimes. Extensive experiments show that CRAG improves diversity compared to existing baselines, while also enabling controllable generation of risk scenarios for targeted and efficient evaluation of AV robustness.</li>
<li><strong>摘要：</strong>确保自动驾驶汽车 (AV) 的安全需要在日常驾驶和罕见的安全关键条件下进行严格的测试。一个关键的挑战在于模拟环境主体，包括背景车辆 (BV) 和弱势道路使用者 (VRU)，它们在名义交通中表现真实，同时也表现出与现实世界事故一致的风险倾向行为。我们引入了可控风险代理生成（CRAG），这是一个旨在统一主导名义行为和罕见安全关键行为的建模的框架。 CRAG 构建了一个结构化的潜在空间，可以理清正常行为和风险相关行为，从而能够有效利用有限的崩溃数据。通过将风险感知的潜在表示与基于优化的模式转换机制相结合，该框架允许代理在较长的范围内从安全状态平稳且合理地转变为风险状态，同时在两种情况下保持高保真度。大量实验表明，与现有基线相比，CRAG 提高了多样性，同时还能够可控地生成风险场景，以便有针对性地、高效地评估 AV 稳健性。</li>
</ul>

<h3>Title: Benchmarking Offline Multi-Objective Reinforcement Learning in Critical Care</h3>
<ul>
<li><strong>Authors: </strong>Aryaman Bansal, Divya Sharma</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08012">https://arxiv.org/abs/2512.08012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08012">https://arxiv.org/pdf/2512.08012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08012]] Benchmarking Offline Multi-Objective Reinforcement Learning in Critical Care(https://arxiv.org/abs/2512.08012)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In critical care settings such as the Intensive Care Unit, clinicians face the complex challenge of balancing conflicting objectives, primarily maximizing patient survival while minimizing resource utilization (e.g., length of stay). Single-objective Reinforcement Learning approaches typically address this by optimizing a fixed scalarized reward function, resulting in rigid policies that fail to adapt to varying clinical priorities. Multi-objective Reinforcement Learning (MORL) offers a solution by learning a set of optimal policies along the Pareto Frontier, allowing for dynamic preference selection at test time. However, applying MORL in healthcare necessitates strict offline learning from historical data. In this paper, we benchmark three offline MORL algorithms, Conditioned Conservative Pareto Q-Learning (CPQL), Adaptive CPQL, and a modified Pareto Efficient Decision Agent (PEDA) Decision Transformer (PEDA DT), against three scalarized single-objective baselines (BC, CQL, and DDQN) on the MIMIC-IV dataset. Using Off-Policy Evaluation (OPE) metrics, we demonstrate that PEDA DT algorithm offers superior flexibility compared to static scalarized baselines. Notably, our results extend previous findings on single-objective Decision Transformers in healthcare, confirming that sequence modeling architectures remain robust and effective when scaled to multi-objective conditioned generation. These findings suggest that offline MORL is a promising framework for enabling personalized, adjustable decision-making in critical care without the need for retraining.</li>
<li><strong>摘要：</strong>在重症监护室等重症监护环境中，临床医生面临着平衡相互冲突的目标的复杂挑战，主要是最大限度地提高患者生存率，同时最大限度地减少资源利用率（例如住院时间）。单目标强化学习方法通​​常通过优化固定的标准化奖励函数来解决这个问题，从而导致僵化的策略无法适应不同的临床优先事项。多目标强化学习 (MORL) 提供了一种解决方案，通过沿着帕累托前沿学习一组最优策略，允许在测试时进行动态偏好选择。然而，在医疗保健中应用 MORL 需要从历史数据中进行严格的离线学习。在本文中，我们在 MIMIC-IV 数据集上针对三个标化单目标基线（BC、CQL 和 DDQN）对三种离线 MORL 算法、条件保守帕累托 Q 学习 (CPQL)、自适应 CPQL 和改进的帕累托高效决策代理 (PEDA) 决策转换器 (PEDA DT) 进行了基准测试。使用离策略评估 (OPE) 指标，我们证明 PEDA DT 算法与静态标量化基线相比具有卓越的灵活性。值得注意的是，我们的结果扩展了之前关于医疗保健中单目标决策转换器的发现，证实序列建模架构在扩展到多目标条件生成时仍然保持稳健和有效。这些发现表明，离线 MORL 是一个很有前途的框架，可以在重症监护中实现个性化、可调整的决策，而无需再培训。</li>
</ul>

<h3>Title: CLARITY: Medical World Model for Guiding Treatment Decisions by Modeling Context-Aware Disease Trajectories in Latent Space</h3>
<ul>
<li><strong>Authors: </strong>Tianxingjian Ding, Yuanhao Zou, Chen Chen, Mubarak Shah, Yu Tian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08029">https://arxiv.org/abs/2512.08029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08029">https://arxiv.org/pdf/2512.08029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08029]] CLARITY: Medical World Model for Guiding Treatment Decisions by Modeling Context-Aware Disease Trajectories in Latent Space(https://arxiv.org/abs/2512.08029)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Clinical decision-making in oncology requires predicting dynamic disease evolution, a task current static AI predictors cannot perform. While world models (WMs) offer a paradigm for generative prediction, existing medical applications remain limited. Existing methods often rely on stochastic diffusion models, focusing on visual reconstruction rather than causal, physiological transitions. Furthermore, in medical domain, models like MeWM typically ignore patient-specific temporal and clinical contexts and lack a feedback mechanism to link predictions to treatment decisions. To address these gaps, we introduce CLARITY, a medical world model that forecasts disease evolution directly within a structured latent space. It explicitly integrates time intervals (temporal context) and patient-specific data (clinical context) to model treatment-conditioned progression as a smooth, interpretable trajectory, and thus generate physiologically faithful, individualized treatment plans. Finally, CLARITY introduces a novel prediction-to-decision framework, translating latent rollouts into transparent, actionable recommendations. CLARITY demonstrates state-of-the-art performance in treatment planning. On the MU-Glioma-Post dataset, our approach outperforms recent MeWM by 12\%, and significantly surpasses all other medical-specific large language models.</li>
<li><strong>摘要：</strong>肿瘤学的临床决策需要预测动态疾病演变，这是当前静态人工智能预测器无法执行的任务。虽然世界模型（WM）提供了生成预测的范例，但现有的医学应用仍然有限。现有方法通常依赖于随机扩散模型，侧重于视觉重建而不是因果生理转变。此外，在医学领域，像 MeWM 这样的模型通常会忽略患者特定的时间和临床背景，并且缺乏将预测与治疗决策联系起来的反馈机制。为了解决这些差距，我们引入了 CLARITY，这是一种医学世界模型，可以直接在结构化的潜在空间内预测疾病的演变。它明确整合时间间隔（时间背景）和患者特定数据（临床背景），将治疗条件进展建模为平滑、可解释的轨迹，从而生成生理上忠实的个性化治疗计划。最后，CLARITY 引入了一种新颖的预测到决策框架，将潜在的部署转化为透明、可操作的建议。 CLARITY 展示了治疗计划方面最先进的性能。在 MU-Glioma-Post 数据集上，我们的方法比最近的 MeWM 性能提高了 12%，并且显着超过了所有其他医学特定的大型语言模型。</li>
</ul>

<h3>Title: Towards Sustainable Universal Deepfake Detection with Frequency-Domain Masking</h3>
<ul>
<li><strong>Authors: </strong>Chandler Timm C. Doloriel, Habib Ullah, Kristian Hovde Liland, Fadi Al Machot, Ngai-Man Cheung</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08042">https://arxiv.org/abs/2512.08042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08042">https://arxiv.org/pdf/2512.08042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08042]] Towards Sustainable Universal Deepfake Detection with Frequency-Domain Masking(https://arxiv.org/abs/2512.08042)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Universal deepfake detection aims to identify AI-generated images across a broad range of generative models, including unseen ones. This requires robust generalization to new and unseen deepfakes, which emerge frequently, while minimizing computational overhead to enable large-scale deepfake screening, a critical objective in the era of Green AI. In this work, we explore frequency-domain masking as a training strategy for deepfake detectors. Unlike traditional methods that rely heavily on spatial features or large-scale pretrained models, our approach introduces random masking and geometric transformations, with a focus on frequency masking due to its superior generalization properties. We demonstrate that frequency masking not only enhances detection accuracy across diverse generators but also maintains performance under significant model pruning, offering a scalable and resource-conscious solution. Our method achieves state-of-the-art generalization on GAN- and diffusion-generated image datasets and exhibits consistent robustness under structured pruning. These results highlight the potential of frequency-based masking as a practical step toward sustainable and generalizable deepfake detection. Code and models are available at: [this https URL](this https URL).</li>
<li><strong>摘要：</strong>通用深度伪造检测旨在识别各种生成模型中人工智能生成的图像，包括看不见的图像。这需要对经常出现的新的和看不见的深度伪造品进行强大的泛化，同时最大限度地减少计算开销以实现大规模深度伪造品筛选，这是绿色人工智能时代的一个关键目标。在这项工作中，我们探索频域掩蔽作为深度伪造检测器的训练策略。与严重依赖空间特征或大规模预训练模型的传统方法不同，我们的方法引入了随机掩蔽和几何变换，并且由于其优越的泛化特性而重点关注频率掩蔽。我们证明，频率掩蔽不仅可以提高不同生成器的检测精度，而且可以在重要的模型修剪下保持性能，从而提供可扩展且资源敏感的解决方案。我们的方法在 GAN 和扩散生成的图像数据集上实现了最先进的泛化，并在结构化剪枝下表现出一致的鲁棒性。这些结果凸显了基于频率的掩蔽作为实现可持续和可推广的深度伪造检测的实际步骤的潜力。代码和模型可在以下位置获取：[此 https URL]（此 https URL）。</li>
</ul>

<h3>Title: Unveiling Latent Knowledge in Chemistry Language Models through Sparse Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Jaron Cohen, Alexander G. Hasson, Sara Tanovic</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08077">https://arxiv.org/abs/2512.08077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08077">https://arxiv.org/pdf/2512.08077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08077]] Unveiling Latent Knowledge in Chemistry Language Models through Sparse Autoencoders(https://arxiv.org/abs/2512.08077)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Since the advent of machine learning, interpretability has remained a persistent challenge, becoming increasingly urgent as generative models support high-stakes applications in drug and material discovery. Recent advances in large language model (LLM) architectures have yielded chemistry language models (CLMs) with impressive capabilities in molecular property prediction and molecular generation. However, how these models internally represent chemical knowledge remains poorly understood. In this work, we extend sparse autoencoder techniques to uncover and examine interpretable features within CLMs. Applying our methodology to the Foundation Models for Materials (FM4M) SMI-TED chemistry foundation model, we extract semantically meaningful latent features and analyse their activation patterns across diverse molecular datasets. Our findings reveal that these models encode a rich landscape of chemical concepts. We identify correlations between specific latent features and distinct domains of chemical knowledge, including structural motifs, physicochemical properties, and pharmacological drug classes. Our approach provides a generalisable framework for uncovering latent knowledge in chemistry-focused AI systems. This work has implications for both foundational understanding and practical deployment; with the potential to accelerate computational chemistry research.</li>
<li><strong>摘要：</strong>自机器学习出现以来，可解释性仍然是一个持续存在的挑战，随着生成模型支持药物和材料发现中的高风险应用，可解释性变得越来越紧迫。大语言模型（LLM）架构的最新进展已经产生了化学语言模型（CLM），在分子特性预测和分子生成方面具有令人印象深刻的能力。然而，这些模型如何在内部表示化学知识仍然知之甚少。在这项工作中，我们扩展了稀疏自动编码器技术来发现和检查 CLM 中的可解释特征。将我们的方法应用于材料基础模型 (FM4M) SMI-TED 化学基础模型，我们提取语义上有意义的潜在特征，并分析它们在不同分子数据集中的激活模式。我们的研究结果表明，这些模型编码了丰富的化学概念。我们确定特定潜在特征和化学知识的不同领域之间的相关性，包括结构基序、物理化学性质和药理学药物类别。我们的方法提供了一个通用框架，用于发现以化学为中心的人工智能系统中的潜在知识。这项工作对基础理解和实际部署都有影响；具有加速计算化学研究的潜力。</li>
</ul>

<h3>Title: Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models I: The Task-Query Architecture</h3>
<ul>
<li><strong>Authors: </strong>Gary Ackerman, Brandon Behlendorf, Zachary Kallenborn, Sheriff Almakki, Doug Clifford, Jenna LaTourette, Hayley Peterson, Noah Sheinbaum, Olivia Shoemaker, Anna Wetzel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08130">https://arxiv.org/abs/2512.08130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08130">https://arxiv.org/pdf/2512.08130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08130]] Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models I: The Task-Query Architecture(https://arxiv.org/abs/2512.08130)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Both model developers and policymakers seek to quantify and mitigate the risk of rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons. An important element of such efforts is the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper describes the first component of a novel Biothreat Benchmark Generation (BBG) Framework. The BBG approach is designed to help model developers and evaluators reliably measure and assess the biosecurity risk uplift and general harm potential of existing and future AI models, while accounting for key aspects of the threat itself that are often overlooked in other benchmarking efforts, including different actor capability levels, and operational (in addition to purely technical) risk factors. As a pilot, the BBG is first being developed to address bacterial biological threats only. The BBG is built upon a hierarchical structure of biothreat categories, elements and tasks, which then serves as the basis for the development of task-aligned queries. This paper outlines the development of this biothreat task-query architecture, which we have named the Bacterial Biothreat Schema, while future papers will describe follow-on efforts to turn queries into model prompts, as well as how the resulting benchmarks can be implemented for model evaluation. Overall, the BBG Framework, including the Bacterial Biothreat Schema, seeks to offer a robust, re-usable structure for evaluating bacterial biological risks arising from LLMs across multiple levels of aggregation, which captures the full scope of technical and operational requirements for biological adversaries, and which accounts for a wide spectrum of biological adversary capabilities.</li>
<li><strong>摘要：</strong>模型开发者和政策制定者都寻求量化和减轻快速发展的前沿人工智能（AI）模型，特别是大语言模型（LLM）的风险，以促进生物恐怖主义或获取生物武器。这种努力的一个重要组成部分是开发模型基准，可以评估特定模型带来的生物安全风险。本文描述了新型生物威胁基准生成（BBG）框架的第一个组成部分。 BBG 方法旨在帮助模型开发人员和评估人员可靠地衡量和评估现有和未来人工智能模型的生物安全风险提升和一般危害潜力，同时考虑到威胁本身在其他基准测试工作中经常被忽视的关键方面，包括不同的参与者能力水平和操作（除了纯技术之外）风险因素。作为试点，BBG 最初的开发仅用于解决细菌生物威胁。 BBG 建立在生物威胁类别、元素和任务的层次结构之上，然后作为开发任务对齐查询的基础。本文概述了这种生物威胁任务查询架构的开发，我们将其命名为细菌生物威胁架构，而未来的论文将描述将查询转化为模型提示的后续工作，以及如何实施由此产生的基准来进行模型评估。总体而言，包括细菌生物威胁模式在内的 BBG 框架旨在提供一个强大的、可重复使用的结构，用于评估跨多个聚合级别的 LLM 产生的细菌生物风险，该结构捕获了生物对手的全部技术和操作要求，并考虑了广泛的生物对手能力。</li>
</ul>

<h3>Title: Robust Agents in Open-Ended Worlds</h3>
<ul>
<li><strong>Authors: </strong>Mikayel Samvelyan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08139">https://arxiv.org/abs/2512.08139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08139">https://arxiv.org/pdf/2512.08139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08139]] Robust Agents in Open-Ended Worlds(https://arxiv.org/abs/2512.08139)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The growing prevalence of artificial intelligence (AI) in various applications underscores the need for agents that can successfully navigate and adapt to an ever-changing, open-ended world. A key challenge is ensuring these AI agents are robust, excelling not only in familiar settings observed during training but also effectively generalising to previously unseen and varied scenarios. In this thesis, we harness methodologies from open-endedness and multi-agent learning to train and evaluate robust AI agents capable of generalising to novel environments, out-of-distribution inputs, and interactions with other co-player agents. We begin by introducing MiniHack, a sandbox framework for creating diverse environments through procedural content generation. Based on the game of NetHack, MiniHack enables the construction of new tasks for reinforcement learning (RL) agents with a focus on generalisation. We then present Maestro, a novel approach for generating adversarial curricula that progressively enhance the robustness and generality of RL agents in two-player zero-sum games. We further probe robustness in multi-agent domains, utilising quality-diversity methods to systematically identify vulnerabilities in state-of-the-art, pre-trained RL policies within the complex video game football domain, characterised by intertwined cooperative and competitive dynamics. Finally, we extend our exploration of robustness to the domain of LLMs. Here, our focus is on diagnosing and enhancing the robustness of LLMs against adversarial prompts, employing evolutionary search to generate a diverse range of effective inputs that aim to elicit undesirable outputs from an LLM. This work collectively paves the way for future advancements in AI robustness, enabling the development of agents that not only adapt to an ever-evolving world but also thrive in the face of unforeseen challenges and interactions.</li>
<li><strong>摘要：</strong>人工智能 (AI) 在各种应用中的日益普及，凸显了对能够成功导航并适应不断变化的开放世界的智能体的需求。一个关键的挑战是确保这些人工智能代理稳健，不仅在训练期间观察到的熟悉环境中表现出色，而且还能有效地推广到以前未见过的各种场景。在本论文中，我们利用开放式和多智能体学习的方法来训练和评估强大的人工智能智能体，这些智能体能够泛化到新环境、分布外输入以及与其他合作智能体的交互。我们首先介绍 MiniHack，这是一个沙箱框架，用于通过程序内容生成来创建多样化的环境。基于 NetHack 游戏，MiniHack 能够为强化学习 (RL) 代理构建新任务，重点是泛化。然后，我们提出 Maestro，这是一种生成对抗性课程的新颖方法，可逐步增强两人零和游戏中 RL 代理的鲁棒性和通用性。我们进一步探讨多智能体领域的鲁棒性，利用质量多样性方法系统地识别复杂视频游戏足球领域中最先进的、预先训练的 RL 策略中的漏洞，其特点是合作和竞争动态交织在一起。最后，我们将对稳健性的探索扩展到法学硕士领域。在这里，我们的重点是诊断和增强法学硕士针对对抗性提示的稳健性，利用进化搜索来生成各种有效输入，旨在从法学硕士中引出不需要的输出。这项工作共同为人工智能稳健性的未来进步铺平了道路，使代理的开发不仅能够适应不断变化的世界，而且能够在面对不可预见的挑战和交互时蓬勃发展。</li>
</ul>

<h3>Title: TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Zheng Ding, Weirui Ye</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08153">https://arxiv.org/abs/2512.08153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08153">https://arxiv.org/pdf/2512.08153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08153]] TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models(https://arxiv.org/abs/2512.08153)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \emph{High sample efficiency}, achieving better performance under same training samples (2) \emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \textbf{2.4$\times$ faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at this http URL.</li>
<li><strong>摘要：</strong>强化学习（RL）训练后对于使生成模型与人类偏好保持一致至关重要，但其高昂的计算成本仍然是广泛采用的主要障碍。我们引入了 \textbf{TreeGRPO}，这是一种新颖的 RL 框架，它通过将去噪过程重新设计为搜索树来显着提高训练效率。从共享的初始噪声样本中，TreeGRPO 战略性地分支以生成多个候选轨迹，同时有效地重用它们的公共前缀。这种树结构方法具有三个关键优势：（1）\emph{高样本效率}，在相同训练样本下实现更好的性能（2）\emph{细粒度信用分配}通过奖励反向传播计算特定于步骤的优势，克服基于轨迹的方法的统一信用分配限制，以及（3）\emph{摊销计算}，其中多子分支允许每个前向传递多个策略更新。对基于扩散和基于流的模型的大量实验表明，TreeGRPO 实现了 \textbf{2.4$\times$ 更快的训练速度}，同时在效率回报权衡空间中建立了优越的帕累托前沿。我们的方法在多个基准和奖励模型中始终优于 GRPO 基线，为基于 RL 的视觉生成模型对齐提供了可扩展且有效的途径。该项目网站可通过此 http URL 获取。</li>
</ul>

<h3>Title: Fourier-RWKV: A Multi-State Perception Network for Efficient Image Dehazing</h3>
<ul>
<li><strong>Authors: </strong>Lirong Zheng, Yanshan Li, Rui Yu, Kaihao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08161">https://arxiv.org/abs/2512.08161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08161">https://arxiv.org/pdf/2512.08161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08161]] Fourier-RWKV: A Multi-State Perception Network for Efficient Image Dehazing(https://arxiv.org/abs/2512.08161)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Image dehazing is crucial for reliable visual perception, yet it remains highly challenging under real-world non-uniform haze conditions. Although Transformer-based methods excel at capturing global context, their quadratic computational complexity hinders real-time deployment. To address this, we propose Fourier Receptance Weighted Key Value (Fourier-RWKV), a novel dehazing framework based on a Multi-State Perception paradigm. The model achieves comprehensive haze degradation modeling with linear complexity by synergistically integrating three distinct perceptual states: (1) Spatial-form Perception, realized through the Deformable Quad-directional Token Shift (DQ-Shift) operation, which dynamically adjusts receptive fields to accommodate local haze variations; (2) Frequency-domain Perception, implemented within the Fourier Mix block, which extends the core WKV attention mechanism of RWKV from the spatial domain to the Fourier domain, preserving the long-range dependencies essential for global haze estimation while mitigating spatial attenuation; (3) Semantic-relation Perception, facilitated by the Semantic Bridge Module (SBM), which utilizes Dynamic Semantic Kernel Fusion (DSK-Fusion) to precisely align encoder-decoder features and suppress artifacts. Extensive experiments on multiple benchmarks demonstrate that Fourier-RWKV delivers state-of-the-art performance across diverse haze scenarios while significantly reducing computational overhead, establishing a favorable trade-off between restoration quality and practical efficiency. Code is available at: this https URL.</li>
<li><strong>摘要：</strong>图像去雾对于可靠的视觉感知至关重要，但在现实世界的不均匀雾霾条件下仍然极具挑战性。尽管基于 Transformer 的方法擅长捕获全局上下文，但其二次计算复杂性阻碍了实时部署。为了解决这个问题，我们提出了傅里叶接收加权键值（Fourier-RWKV），这是一种基于多状态感知范式的新型去雾框架。该模型通过协同集成三种不同的感知状态，实现了线性复杂度的综合雾霾退化建模：（1）空间形式感知，通过可变形四向令牌移位（DQ-Shift）操作实现，动态调整感受野以适应局部雾霾变化； (2) 频域感知，在 Fourier Mix 模块中实现，它将 RWKV 的核心 WKV 注意力机制从空间域扩展到傅里叶域，保留了全局雾霾估计所必需的远程依赖性，同时减轻了空间衰减； (3) 语义关系感知，由语义桥模块 (SBM) 促进，该模块利用动态语义内核融合 (DSK-Fusion) 来精确对齐编码器-解码器特征并抑制伪影。对多个基准的大量实验表明，Fourier-RWKV 在不同的雾霾场景中提供了最先进的性能，同时显着降低了计算开销，在恢复质量和实际效率之间建立了有利的权衡。代码可在以下位置获得：此 https URL。</li>
</ul>

<h3>Title: GeoLoom: High-quality Geometric Diagram Generation from Textual Input</h3>
<ul>
<li><strong>Authors: </strong>Xiaojing Wei, Ting Zhang, Wei He, Jingdong Wang, Hua Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08180">https://arxiv.org/abs/2512.08180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08180">https://arxiv.org/pdf/2512.08180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08180]] GeoLoom: High-quality Geometric Diagram Generation from Textual Input(https://arxiv.org/abs/2512.08180)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>High-quality geometric diagram generation presents both a challenge and an opportunity: it demands strict spatial accuracy while offering well-defined constraints to guide generation. Inspired by recent advances in geometry problem solving that employ formal languages and symbolic solvers for enhanced correctness and interpretability, we propose GeoLoom, a novel framework for text-to-diagram generation in geometric domains. GeoLoom comprises two core components: an autoformalization module that translates natural language into a specifically designed generation-oriented formal language GeoLingua, and a coordinate solver that maps formal constraints to precise coordinates using the efficient Monte Carlo optimization. To support this framework, we introduce GeoNF, a dataset aligning natural language geometric descriptions with formal GeoLingua descriptions. We further propose a constraint-based evaluation metric that quantifies structural deviation, offering mathematically grounded supervision for iterative refinement. Empirical results demonstrate that GeoLoom significantly outperforms state-of-the-art baselines in structural fidelity, providing a principled foundation for interpretable and scalable diagram generation.</li>
<li><strong>摘要：</strong>高质量的几何图生成既是挑战也是机遇：它要求严格的空间精度，同时提供明确的约束来指导生成。受到几何问题解决方面最新进展的启发，利用形式语言和符号求解器来增强正确性和可解释性，我们提出了 GeoLoom，一种用于几何领域中文本到图表生成的新颖框架。 GeoLoom 包含两个核心组件：自动形式化模块，将自然语言转换为专门设计的面向生成的形式语言 GeoLingua；以及坐标求解器，使用高效的蒙特卡罗优化将形式约束映射到精确坐标。为了支持这个框架，我们引入了 GeoNF，一个将自然语言几何描述与正式 GeoLingua 描述对齐的数据集。我们进一步提出了一种基于约束的评估指标，可以量化结构偏差，为迭代细化提供基于数学的监督。实证结果表明，GeoLoom 在结构保真度方面显着优于最先进的基线，为可解释和可扩展的图表生成提供了原则基础。</li>
</ul>

<h3>Title: Blur2Sharp: Human Novel Pose and View Synthesis with Generative Prior Refinement</h3>
<ul>
<li><strong>Authors: </strong>Chia-Hern Lai, I-Hsuan Lo, Yen-Ku Yeh, Thanh-Nguyen Truong, Ching-Chun Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08215">https://arxiv.org/abs/2512.08215</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08215">https://arxiv.org/pdf/2512.08215</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08215]] Blur2Sharp: Human Novel Pose and View Synthesis with Generative Prior Refinement(https://arxiv.org/abs/2512.08215)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The creation of lifelike human avatars capable of realistic pose variation and viewpoint flexibility remains a fundamental challenge in computer vision and graphics. Current approaches typically yield either geometrically inconsistent multi-view images or sacrifice photorealism, resulting in blurry outputs under diverse viewing angles and complex motions. To address these issues, we propose Blur2Sharp, a novel framework integrating 3D-aware neural rendering and diffusion models to generate sharp, geometrically consistent novel-view images from only a single reference view. Our method employs a dual-conditioning architecture: initially, a Human NeRF model generates geometrically coherent multi-view renderings for target poses, explicitly encoding 3D structural guidance. Subsequently, a diffusion model conditioned on these renderings refines the generated images, preserving fine-grained details and structural fidelity. We further enhance visual quality through hierarchical feature fusion, incorporating texture, normal, and semantic priors extracted from parametric SMPL models to simultaneously improve global coherence and local detail accuracy. Extensive experiments demonstrate that Blur2Sharp consistently surpasses state-of-the-art techniques in both novel pose and view generation tasks, particularly excelling under challenging scenarios involving loose clothing and occlusions.</li>
<li><strong>摘要：</strong>创建能够实现逼真的姿势变化和视角灵活性的逼真的人类化身仍然是计算机视觉和图形领域的基本挑战。目前的方法通常要么产生几何不一致的多视图图像，要么牺牲照片真实感，导致在不同视角和复杂运动下输出模糊。为了解决这些问题，我们提出了 Blur2Sharp，这是一种集成 3D 感知神经渲染和扩散模型的新颖框架，仅从单个参考视图生成清晰、几何一致的新颖视图图像。我们的方法采用双条件架构：最初，Human NeRF 模型为目标姿势生成几何连贯的多视图渲染，显式编码 3D 结构指导。随后，以这些渲染为条件的扩散模型细化生成的图像，保留细粒度的细节和结构保真度。我们通过分层特征融合进一步增强视觉质量，结合从参数 SMPL 模型中提取的纹理、法线和语义先验，同时提高全局一致性和局部细节准确性。大量实验表明，Blur2Sharp 在新颖的姿势和视图生成任务中始终超越最先进的技术，特别是在涉及宽松衣服和遮挡的挑战性场景下表现出色。</li>
</ul>

<h3>Title: MM-CoT:A Benchmark for Probing Visual Chain-of-Thought Reasoning in Multimodal Models</h3>
<ul>
<li><strong>Authors: </strong>Jusheng Zhang, Kaitong Cai, Xiaoyang Guo, Sidi Liu, Qinhan Lv, Ruiqi Chen, Jing Yang, Yijia Fan, Xiaofei Sun, Jian Wang, Ziliang Chen, Liang Lin, Keze Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08228">https://arxiv.org/abs/2512.08228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08228">https://arxiv.org/pdf/2512.08228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08228]] MM-CoT:A Benchmark for Probing Visual Chain-of-Thought Reasoning in Multimodal Models(https://arxiv.org/abs/2512.08228)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The ability to perform Chain-of-Thought (CoT) reasoning marks a major milestone for multimodal models (MMs), enabling them to solve complex visual reasoning problems. Yet a critical question remains: is such reasoning genuinely grounded in visual evidence and logically coherent? Existing benchmarks emphasize generation but neglect verification, i.e., the capacity to assess whether a reasoning chain is both visually consistent and logically valid. To fill this gap, we introduce MM-CoT, a diagnostic benchmark specifically designed to probe the visual grounding and logical coherence of CoT reasoning in MMs. Instead of generating free-form explanations, models must select the sole event chain that satisfies two orthogonal constraints: (i) visual consistency, ensuring all steps are anchored in observable evidence, and (ii) logical coherence, ensuring causal and commonsense validity. Adversarial distractors are engineered to violate one of these constraints, exposing distinct reasoning failures. We evaluate leading vision-language models on MM-CoT and find that even the most advanced systems struggle, revealing a sharp discrepancy between generative fluency and true reasoning fidelity. MM-CoT shows low correlation with existing benchmarks, confirming that it measures a unique combination of visual grounding and logical reasoning. This benchmark provides a foundation for developing future models that reason not just plausibly, but faithfully and coherently within the visual world.</li>
<li><strong>摘要：</strong>执行思维链 (CoT) 推理的能力标志着多模态模型 (MM) 的一个重要里程碑，使它们能够解决复杂的视觉推理问题。然而，一个关键问题仍然存在：这种推理是否真正基于视觉证据并且逻辑上连贯？现有的基准强调生成而忽视验证，即评估推理链是否视觉上一致且逻辑上有效的能力。为了填补这一空白，我们引入了 MM-CoT，这是一种专门设计用于探索 MM 中 CoT 推理的视觉基础和逻辑连贯性的诊断基准。模型必须选择满足两个正交约束的唯一事件链，而不是生成自由形式的解释：（i）视觉一致性，确保所有步骤都以可观察的证据为基础；（ii）逻辑连贯性，确保因果和常识的有效性。对抗性干扰物被设计为违反这些约束之一，暴露明显的推理失败。我们评估了 MM-CoT 上领先的视觉语言模型，发现即使是最先进的系统也很困难，揭示了生成流畅性和真实推理保真度之间的巨大差异。 MM-CoT 与现有基准的相关性较低，证实它衡量的是视觉基础和逻辑推理的独特组合。该基准为开发未来模型提供了基础，这些模型不仅在视觉世界中合理地进行推理，而且忠实且连贯地进行推理。</li>
</ul>

<h3>Title: Geometry-Aware Sparse Depth Sampling for High-Fidelity RGB-D Depth Completion in Robotic Systems</h3>
<ul>
<li><strong>Authors: </strong>Tony Salloom, Dandi Zhou, Xinhai Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08229">https://arxiv.org/abs/2512.08229</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08229">https://arxiv.org/pdf/2512.08229</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08229]] Geometry-Aware Sparse Depth Sampling for High-Fidelity RGB-D Depth Completion in Robotic Systems(https://arxiv.org/abs/2512.08229)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Accurate three-dimensional perception is essential for modern industrial robotic systems that perform manipulation, inspection, and navigation tasks. RGB-D and stereo vision sensors are widely used for this purpose, but the depth maps they produce are often noisy, incomplete, or biased due to sensor limitations and environmental conditions. Depth completion methods aim to generate dense, reliable depth maps from RGB images and sparse depth input. However, a key limitation in current depth completion pipelines is the unrealistic generation of sparse depth: sparse pixels are typically selected uniformly at random from dense ground-truth depth, ignoring the fact that real sensors exhibit geometry-dependent and spatially nonuniform reliability. In this work, we propose a normal-guided sparse depth sampling strategy that leverages PCA-based surface normal estimation on the RGB-D point cloud to compute a per-pixel depth reliability measure. The sparse depth samples are then drawn according to this reliability distribution. We integrate this sampling method with the Marigold-DC diffusion-based depth completion model and evaluate it on NYU Depth v2 using the standard metrics. Experiments show that our geometry-aware sparse depth improves accuracy, reduces artifacts near edges and discontinuities, and produces more realistic training conditions that better reflect real sensor behavior.</li>
<li><strong>摘要：</strong>准确的三维感知对于执行操纵、检查和导航任务的现代工业机器人系统至关重要。 RGB-D 和立体视觉传感器广泛用于此目的，但由于传感器限制和环境条件，它们生成的深度图通常有噪声、不完整或有偏差。深度补全方法旨在从 RGB 图像和稀疏深度输入生成密集、可靠的深度图。然而，当前深度完成管道的一个关键限制是稀疏深度的不切实际的生成：稀疏像素通常是从密集的地面实况深度中随机均匀选择的，忽略了实际传感器表现出依赖于几何形状和空间不均匀可靠性的事实。在这项工作中，我们提出了一种法线引导的稀疏深度采样策略，该策略利用 RGB-D 点云上基于 PCA 的表面法线估计来计算每像素深度可靠性度量。然后根据该可靠性分布绘制稀疏深度样本。我们将此采样方法与基于 Marigold-DC 扩散的深度完成模型集成，并使用标准指标在 NYU Depth v2 上对其进行评估。实验表明，我们的几何感知稀疏深度提高了准确性，减少了边缘和不连续性附近的伪影，并产生了更真实的训练条件，可以更好地反映真实的传感器行为。</li>
</ul>

<h3>Title: Query-aware Hub Prototype Learning for Few-Shot 3D Point Cloud Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>YiLin Zhou, Lili Wei, Zheming Xu, Ziyi Chen, Congyan Lang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08253">https://arxiv.org/abs/2512.08253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08253">https://arxiv.org/pdf/2512.08253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08253]] Query-aware Hub Prototype Learning for Few-Shot 3D Point Cloud Semantic Segmentation(https://arxiv.org/abs/2512.08253)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Few-shot 3D point cloud semantic segmentation (FS-3DSeg) aims to segment novel classes with only a few labeled samples. However, existing metric-based prototype learning methods generate prototypes solely from the support set, without considering their relevance to query data. This often results in prototype bias, where prototypes overfit support-specific characteristics and fail to generalize to the query distribution, especially in the presence of distribution shifts, which leads to degraded segmentation performance. To address this issue, we propose a novel Query-aware Hub Prototype (QHP) learning method that explicitly models semantic correlations between support and query sets. Specifically, we propose a Hub Prototype Generation (HPG) module that constructs a bipartite graph connecting query and support points, identifies frequently linked support hubs, and generates query-relevant prototypes that better capture cross-set semantics. To further mitigate the influence of bad hubs and ambiguous prototypes near class boundaries, we introduce a Prototype Distribution Optimization (PDO) module, which employs a purity-reweighted contrastive loss to refine prototype representations by pulling bad hubs and outlier prototypes closer to their corresponding class centers. Extensive experiments on S3DIS and ScanNet demonstrate that QHP achieves substantial performance gains over state-of-the-art methods, effectively narrowing the semantic gap between prototypes and query sets in FS-3DSeg.</li>
<li><strong>摘要：</strong>Few-shot 3D 点云语义分割 (FS-3DSeg) 旨在仅使用少量标记样本来分割新类别。然而，现有的基于度量的原型学习方法仅从支持集生成原型，而没有考虑它们与查询数据的相关性。这通常会导致原型偏差，即原型过度拟合特定于支持的特征，并且无法泛化到查询分布，尤其是在存在分布变化的情况下，从而导致分段性能下降。为了解决这个问题，我们提出了一种新颖的查询感知集线器原型（QHP）学习方法，该方法显式地建模支持集和查询集之间的语义相关性。具体来说，我们提出了一个中心原型生成（HPG）模块，该模块构建连接查询和支持点的二分图，识别频繁链接的支持中心，并生成更好地捕获跨集语义的查询相关原型。为了进一步减轻类边界附近不良中心和不明确原型的影响，我们引入了原型分布优化（PDO）模块，该模块采用纯度重新加权对比损失，通过将不良中心和离群原型拉近其相应的类中心来细化原型表示。 S3DIS 和 ScanNet 上的大量实验表明，QHP 比最先进的方法实现了显着的性能提升，有效缩小了 FS-3DSeg 中原型和查询集之间的语义差距。</li>
</ul>

<h3>Title: SFP: Real-World Scene Recovery Using Spatial and Frequency Priors</h3>
<ul>
<li><strong>Authors: </strong>Yun Liu, Tao Li, Cosmin Ancuti, Wenqi Ren, Weisi Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08254">https://arxiv.org/abs/2512.08254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08254">https://arxiv.org/pdf/2512.08254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08254]] SFP: Real-World Scene Recovery Using Spatial and Frequency Priors(https://arxiv.org/abs/2512.08254)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Scene recovery serves as a critical task for various computer vision applications. Existing methods typically rely on a single prior, which is inherently insufficient to handle multiple degradations, or employ complex network architectures trained on synthetic data, which suffer from poor generalization for diverse real-world scenarios. In this paper, we propose Spatial and Frequency Priors (SFP) for real-world scene recovery. In the spatial domain, we observe that the inverse of the degraded image exhibits a projection along its spectral direction that resembles the scene transmission. Leveraging this spatial prior, the transmission map is estimated to recover the scene from scattering degradation. In the frequency domain, a mask is constructed for adaptive frequency enhancement, with two parameters estimated using our proposed novel priors. Specifically, one prior assumes that the mean intensity of the degraded image's direct current (DC) components across three channels in the frequency domain closely approximates that of each channel in the clear image. The second prior is based on the observation that, for clear images, the magnitude of low radial frequencies below 0.001 constitutes approximately 1% of the total spectrum. Finally, we design a weighted fusion strategy to integrate spatial-domain restoration, frequency-domain enhancement, and salient features from the input image, yielding the final recovered result. Extensive evaluations demonstrate the effectiveness and superiority of our proposed SFP for scene recovery under various degradation conditions.</li>
<li><strong>摘要：</strong>场景恢复是各种计算机视觉应用的一项关键任务。现有的方法通常依赖于单个先验，这本质上不足以处理多重退化，或者采用在合成数据上训练的复杂网络架构，这些架构对于不同的现实世界场景的泛化能力较差。在本文中，我们提出了用于现实世界场景恢复的空间和频率先验（SFP）。在空间域中，我们观察到退化图像的反演呈现出沿着其光谱方向的投影，类似于场景传输。利用该空间先验，估计传输图以从散射退化中恢复场景。在频域中，构建了用于自适应频率增强的掩模，并使用我们提出的新颖先验估计了两个参数。具体来说，先验假设频域中三个通道的降级图像的直流 (DC) 分量的平均强度非常接近清晰图像中每个通道的平均强度。第二个先验基于这样的观察：对于清晰的图像，低于 0.001 的低径向频率的幅度约占总频谱的 1%。最后，我们设计了一种加权融合策略来集成空域恢复、频域增强和输入图像的显着特征，产生最终的恢复结果。广泛的评估证明了我们提出的 SFP 在各种退化条件下场景恢复的有效性和优越性。</li>
</ul>

<h3>Title: EgoX: Egocentric Video Generation from a Single Exocentric Video</h3>
<ul>
<li><strong>Authors: </strong>Taewoong Kang, Kinam Kim, Dohyeon Kim, Minho Park, Junha Hyung, Jaegul Choo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08269">https://arxiv.org/abs/2512.08269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08269">https://arxiv.org/pdf/2512.08269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08269]] EgoX: Egocentric Video Generation from a Single Exocentric Video(https://arxiv.org/abs/2512.08269)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Egocentric perception enables humans to experience and understand the world directly from their own point of view. Translating exocentric (third-person) videos into egocentric (first-person) videos opens up new possibilities for immersive understanding but remains highly challenging due to extreme camera pose variations and minimal view overlap. This task requires faithfully preserving visible content while synthesizing unseen regions in a geometrically consistent manner. To achieve this, we present EgoX, a novel framework for generating egocentric videos from a single exocentric input. EgoX leverages the pretrained spatio temporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation and introduces a unified conditioning strategy that combines exocentric and egocentric priors via width and channel wise concatenation. Additionally, a geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity. Our approach achieves coherent and realistic egocentric video generation while demonstrating strong scalability and robustness across unseen and in-the-wild videos.</li>
<li><strong>摘要：</strong>自我中心的感知使人类能够直接从自己的角度体验和理解世界。将外向中心（第三人称）视频转换为自我中心（第一人称）视频为沉浸式理解开辟了新的可能性，但由于极端的相机姿势变化和最小的视图重叠，仍然具有很大的挑战性。这项任务需要忠实地保留可见内容，同时以几何一致的方式合成不可见的区域。为了实现这一目标，我们提出了 EgoX，这是一种新颖的框架，用于从单个外中心输入生成以自我为中心的视频。 EgoX 通过轻量级 LoRA 适应，利用大规模视频扩散模型的预训练时空知识，并引入统一的调节策略，通过宽度和通道级联将外心和自我中心先验结合起来。此外，几何引导的自注意力机制选择性地关注空间相关区域，确保几何一致性和高视觉保真度。我们的方法实现了连贯且真实的以自我为中心的视频生成，同时在未见过的和野外的视频中展示了强大的可扩展性和鲁棒性。</li>
</ul>

<h3>Title: PAVAS: Physics-Aware Video-to-Audio Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Oh Hyun-Bin, Yuhta Takida, Toshimitsu Uesaka, Tae-Hyun Oh, Yuki Mitsufuji</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08282">https://arxiv.org/abs/2512.08282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08282">https://arxiv.org/pdf/2512.08282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08282]] PAVAS: Physics-Aware Video-to-Audio Synthesis(https://arxiv.org/abs/2512.08282)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in Video-to-Audio (V2A) generation have achieved impressive perceptual quality and temporal synchronization, yet most models remain appearance-driven, capturing visual-acoustic correlations without considering the physical factors that shape real-world sounds. We present Physics-Aware Video-to-Audio Synthesis (PAVAS), a method that incorporates physical reasoning into a latent diffusion-based V2A generation through the Physics-Driven Audio Adapter (Phy-Adapter). The adapter receives object-level physical parameters estimated by the Physical Parameter Estimator (PPE), which uses a Vision-Language Model (VLM) to infer the moving-object mass and a segmentation-based dynamic 3D reconstruction module to recover its motion trajectory for velocity computation. These physical cues enable the model to synthesize sounds that reflect underlying physical factors. To assess physical realism, we curate VGG-Impact, a benchmark focusing on object-object interactions, and introduce Audio-Physics Correlation Coefficient (APCC), an evaluation metric that measures consistency between physical and auditory attributes. Comprehensive experiments show that PAVAS produces physically plausible and perceptually coherent audio, outperforming existing V2A models in both quantitative and qualitative evaluations. Visit this https URL for demo videos.</li>
<li><strong>摘要：</strong>视频到音频 (V2A) 生成的最新进展已经实现了令人印象深刻的感知质量和时间同步，但大多数模型仍然是外观驱动的，捕获视觉声学相关性，而不考虑塑造现实世界声音的物理因素。我们提出了物理感知视频音频合成 (PAVAS)，这是一种通过物理驱动音频适配器 (Phy-Adapter) 将物理推理融入基于潜在扩散的 V2A 生成的方法。该适配器接收由物理参数估计器 (PPE) 估计的对象级物理参数，该参数使用视觉语言模型 (VLM) 来推断移动对象质量，并使用基于分段的动态 3D 重建模块来恢复其运动轨迹以进行速度计算。这些物理线索使模型能够合成反映潜在物理因素的声音。为了评估物理真实感，我们策划了 VGG-Impact（一个专注于物体与物体交互的基准），并引入了音频物理相关系数（APCC），这是一种衡量物理和听觉属性之间一致性的评估指标。综合实验表明，PAVAS 可以产生物理上合理且感知上一致的音频，在定量和定性评估方面均优于现有的 V2A 模型。访问此 https URL 观看演示视频。</li>
</ul>

<h3>Title: OpenSubject: Leveraging Video-Derived Identity and Diversity Priors for Subject-driven Image Generation and Manipulation</h3>
<ul>
<li><strong>Authors: </strong>Yexin Liu, Manyuan Zhang, Yueze Wang, Hongyu Li, Dian Zheng, Weiming Zhang, Changsheng Lu, Xunliang Cai, Yan Feng, Peng Pei, Harry Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08294">https://arxiv.org/abs/2512.08294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08294">https://arxiv.org/pdf/2512.08294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08294]] OpenSubject: Leveraging Video-Derived Identity and Diversity Priors for Subject-driven Image Generation and Manipulation(https://arxiv.org/abs/2512.08294)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite the promising progress in subject-driven image generation, current models often deviate from the reference identities and struggle in complex scenes with multiple subjects. To address this challenge, we introduce OpenSubject, a video-derived large-scale corpus with 2.5M samples and 4.35M images for subject-driven generation and manipulation. The dataset is built with a four-stage pipeline that exploits cross-frame identity priors. (i) Video Curation. We apply resolution and aesthetic filtering to obtain high-quality clips. (ii) Cross-Frame Subject Mining and Pairing. We utilize vision-language model (VLM)-based category consensus, local grounding, and diversity-aware pairing to select image pairs. (iii) Identity-Preserving Reference Image Synthesis. We introduce segmentation map-guided outpainting to synthesize the input images for subject-driven generation and box-guided inpainting to generate input images for subject-driven manipulation, together with geometry-aware augmentations and irregular boundary erosion. (iv) Verification and Captioning. We utilize a VLM to validate synthesized samples, re-synthesize failed samples based on stage (iii), and then construct short and long captions. In addition, we introduce a benchmark covering subject-driven generation and manipulation, and then evaluate identity fidelity, prompt adherence, manipulation consistency, and background consistency with a VLM judge. Extensive experiments show that training with OpenSubject improves generation and manipulation performance, particularly in complex scenes.</li>
<li><strong>摘要：</strong>尽管在主题驱动的图像生成方面取得了有希望的进展，但当前的模型经常偏离参考身份，并且在具有多个主题的复杂场景中陷入困境。为了应对这一挑战，我们引入了 OpenSubject，这是一个视频衍生的大规模语料库，具有 250 万个样本和 435 万张图像，用于主题驱动的生成和操作。该数据集是通过利用跨框架身份先验的四阶段管道构建的。 (i) 视频策展。我们应用分辨率和美学过滤来获得高质量的剪辑。 (ii)跨框架主题挖掘和配对。我们利用基于视觉语言模型（VLM）的类别共识、本地基础和多样性感知配对来选择图像对。 (iii) 保留身份的参考图像合成。我们引入了分割图引导的外绘来合成用于主题驱动生成的输入图像和框引导修复来生成用于主题驱动操作的输入图像，以及几何感知增强和不规则边界侵蚀。 (iv) 验证和字幕。我们利用 VLM 来验证合成的样本，根据阶段 (iii) 重新合成失败的样本，然后构建短标题和长标题。此外，我们引入了一个涵盖主题驱动生成和操作的基准，然后使用 VLM 判断来评估身份保真度、即时依从性、操作一致性和背景一致性。大量实验表明，使用 OpenSubject 进行训练可以提高生成和操作性能，尤其是在复杂场景中。</li>
</ul>

<h3>Title: Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation</h3>
<ul>
<li><strong>Authors: </strong>Alexander Goslin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08309">https://arxiv.org/abs/2512.08309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08309">https://arxiv.org/pdf/2512.08309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08309]] Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation(https://arxiv.org/abs/2512.08309)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>For decades, procedural worlds have been built on procedural noise functions such as Perlin noise, which are fast and infinite, yet fundamentally limited in realism and large-scale coherence. We introduce Terrain Diffusion, an AI-era successor to Perlin noise that bridges the fidelity of diffusion models with the properties that made procedural noise indispensable: seamless infinite extent, seed-consistency, and constant-time random access. At its core is InfiniteDiffusion, a novel algorithm for infinite generation, enabling seamless, real-time synthesis of boundless landscapes. A hierarchical stack of diffusion models couples planetary context with local detail, while a compact Laplacian encoding stabilizes outputs across Earth-scale dynamic ranges. An open-source infinite-tensor framework supports constant-memory manipulation of unbounded tensors, and few-step consistency distillation enables efficient generation. Together, these components establish diffusion models as a practical foundation for procedural world generation, capable of synthesizing entire planets coherently, controllably, and without limits.</li>
<li><strong>摘要：</strong>几十年来，程序世界一直建立在程序噪声函数（例如柏林噪声）的基础上，这些函数速度快且无限，但在真实性和大规模相干性方面从根本上受到限制。我们引入了 Terrain Diffusion，它是 Perlin 噪声的 AI 时代后继者，它将扩散模型的保真度与程序噪声不可或缺的属性连接起来：无缝无限范围、种子一致性和恒定时间随机访问。其核心是 InfiniteDiffusion，这是一种用于无限生成的新颖算法，可实现无限景观的无缝、实时合成。扩散模型的分层堆栈将行星环境与局部细节结合起来，而紧凑的拉普拉斯编码则稳定了整个地球尺度动态范围的输出。开源无限张量框架支持无界张量的常量内存操作，并且少步一致性蒸馏可以实现高效生成。这些组件共同建立了扩散模型，作为程序世界生成的实用基础，能够连贯、可控且不受限制地合成整个行星。</li>
</ul>

<h3>Title: Interpreting Structured Perturbations in Image Protection Methods for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Michael R. Martin, Garrick Chan, Kwan-Liu Ma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08329">https://arxiv.org/abs/2512.08329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08329">https://arxiv.org/pdf/2512.08329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08329]] Interpreting Structured Perturbations in Image Protection Methods for Diffusion Models(https://arxiv.org/abs/2512.08329)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent image protection mechanisms such as Glaze and Nightshade introduce imperceptible, adversarially designed perturbations intended to disrupt downstream text-to-image generative models. While their empirical effectiveness is known, the internal structure, detectability, and representational behavior of these perturbations remain poorly understood. This study provides a systematic, explainable AI analysis using a unified framework that integrates white-box feature-space inspection and black-box signal-level probing. Through latent-space clustering, feature-channel activation analysis, occlusion-based spatial sensitivity mapping, and frequency-domain characterization, we show that protection mechanisms operate as structured, low-entropy perturbations tightly coupled to underlying image content across representational, spatial, and spectral domains. Protected images preserve content-driven feature organization with protection-specific substructure rather than inducing global representational drift. Detectability is governed by interacting effects of perturbation entropy, spatial deployment, and frequency alignment, with sequential protection amplifying detectable structure rather than suppressing it. Frequency-domain analysis shows that Glaze and Nightshade redistribute energy along dominant image-aligned frequency axes rather than introducing diffuse noise. These findings indicate that contemporary image protection operates through structured feature-level deformation rather than semantic dislocation, explaining why protection signals remain visually subtle yet consistently detectable. This work advances the interpretability of adversarial image protection and informs the design of future defenses and detection strategies for generative AI systems.</li>
<li><strong>摘要：</strong>最近的图像保护机制（例如 Glaze 和 Nightshade）引入了难以察觉的、对抗性设计的扰动，旨在破坏下游文本到图像的生成模型。虽然它们的经验有效性是已知的，但这些扰动的内部结构、可检测性和代表性行为仍然知之甚少。这项研究使用集成了白盒特征空间检查和黑盒信号级探测的统一框架提供了系统的、可解释的人工智能分析。通过潜在空间聚类、特征通道激活分析、基于遮挡的空间灵敏度映射和频域表征，我们表明保护机制作为结构化、低熵扰动与跨表征、空间和谱域的底层图像内容紧密耦合。受保护的图像通过特定于保护的子结构保留内容驱动的特征组织，而不是引起全局代表性漂移。可检测性受扰动熵、空间部署和频率对准的相互作用影响，顺序保护放大而不是抑制可检测结构。频域分析表明，Glaze 和 Nightshade 沿主要图像对齐频率轴重新分配能量，而不是引入扩散噪声。这些发现表明，当代图像保护是通过结构化特征级变形而不是语义错位来运作的，这解释了为什么保护信号在视觉上仍然微妙但始终可检测到。这项工作提高了对抗性图像保护的可解释性，并为生成人工智能系统的未来防御和检测策略的设计提供了信息。</li>
</ul>

<h3>Title: PointDico: Contrastive 3D Representation Learning Guided by Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Pengbo Li, Yiding Sun, Haozhe Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08330">https://arxiv.org/abs/2512.08330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08330">https://arxiv.org/pdf/2512.08330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08330]] PointDico: Contrastive 3D Representation Learning Guided by Diffusion Models(https://arxiv.org/abs/2512.08330)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Self-supervised representation learning has shown significant improvement in Natural Language Processing and 2D Computer Vision. However, existing methods face difficulties in representing 3D data because of its unordered and uneven density. Through an in-depth analysis of mainstream contrastive and generative approaches, we find that contrastive models tend to suffer from overfitting, while 3D Mask Autoencoders struggle to handle unordered point clouds. This motivates us to learn 3D representations by sharing the merits of diffusion and contrast models, which is non-trivial due to the pattern difference between the two paradigms. In this paper, we propose \textit{PointDico}, a novel model that seamlessly integrates these methods. \textit{PointDico} learns from both denoising generative modeling and cross-modal contrastive learning through knowledge distillation, where the diffusion model serves as a guide for the contrastive model. We introduce a hierarchical pyramid conditional generator for multi-scale geometric feature extraction and employ a dual-channel design to effectively integrate local and global contextual information. \textit{PointDico} achieves a new state-of-the-art in 3D representation learning, \textit{e.g.}, \textbf{94.32\%} accuracy on ScanObjectNN, \textbf{86.5\%} Inst. mIoU on ShapeNetPart.</li>
<li><strong>摘要：</strong>自监督表示学习在自然语言处理和 2D 计算机视觉方面显示出显着的进步。然而，由于 3D 数据无序且密度不均匀，现有方法在表示 3D 数据时面临困难。通过对主流对比和生成方法的深入分析，我们发现对比模型往往会出现过度拟合，而 3D 掩模自动编码器则难以处理无序点云。这激励我们通过分享扩散模型和对比度模型的优点来学习 3D 表示，由于两种范式之间的模式差异，这并非易事。在本文中，我们提出了 \textit{PointDico}，一种无缝集成这些方法的新颖模型。 \textit{PointDico} 通过知识蒸馏从去噪生成模型和跨模态对比学习中学习，其中扩散模型作为对比模型的指导。我们引入了用于多尺度几何特征提取的分层金字塔条件生成器，并采用双通道设计来有效地集成局部和全局上下文信息。 \textit{PointDico} 在 3D 表示学习中实现了新的最先进水平，\textit{e.g.}、\textbf{94.32\%} 在 ScanObjectNN 上的准确度、\textbf{86.5\%} Inst。 ShapeNetPart 上的 MIoU。</li>
</ul>

<h3>Title: DINO-BOLDNet: A DINOv3-Guided Multi-Slice Attention Network for T1-to-BOLD Generation</h3>
<ul>
<li><strong>Authors: </strong>Jianwei Wang, Qing Wang, Menglan Ruan, Rongjun Ge, Chunfeng Yang, Yang Chen, Chunming Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08337">https://arxiv.org/abs/2512.08337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08337">https://arxiv.org/pdf/2512.08337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08337]] DINO-BOLDNet: A DINOv3-Guided Multi-Slice Attention Network for T1-to-BOLD Generation(https://arxiv.org/abs/2512.08337)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating BOLD images from T1w images offers a promising solution for recovering missing BOLD information and enabling downstream tasks when BOLD images are corrupted or unavailable. Motivated by this, we propose DINO-BOLDNet, a DINOv3-guided multi-slice attention framework that integrates a frozen self-supervised DINOv3 encoder with a lightweight trainable decoder. The model uses DINOv3 to extract within-slice structural representations, and a separate slice-attention module to fuse contextual information across neighboring slices. A multi-scale generation decoder then restores fine-grained functional contrast, while a DINO-based perceptual loss encourages structural and textural consistency between predictions and ground-truth BOLD in the transformer feature space. Experiments on a clinical dataset of 248 subjects show that DINO-BOLDNet surpasses a conditional GAN baseline in both PSNR and MS-SSIM. To our knowledge, this is the first framework capable of generating mean BOLD images directly from T1w images, highlighting the potential of self-supervised transformer guidance for structural-to-functional mapping.</li>
<li><strong>摘要：</strong>从 T1w 图像生成 BOLD 图像提供了一种很有前途的解决方案，用于恢复丢失的 BOLD 信息并在 BOLD 图像损坏或不可用时启用下游任务。受此启发，我们提出了 DINO-BOLDNet，这是一种 DINOv3 引导的多切片注意力框架，它将冻结的自监督 DINOv3 编码器与轻量级可训练解码器集成在一起。该模型使用 DINOv3 提取切片内的结构表示，并使用单独的切片注意模块来融合相邻切片之间的上下文信息。然后，多尺度生成解码器恢复细粒度的功能对比度，而基于 DINO 的感知损失则鼓励变压器特征空间中的预测与真实 BOLD 之间的结构和纹理一致性。对 248 名受试者的临床数据集进行的实验表明，DINO-BOLDNet 在 PSNR 和 MS-SSIM 方面均超过了条件 GAN 基线。据我们所知，这是第一个能够直接从 T1w 图像生成平均 BOLD 图像的框架，凸显了自监督 Transformer 指导在结构到功能映射方面的潜力。</li>
</ul>

<h3>Title: SCU-CGAN: Enhancing Fire Detection through Synthetic Fire Image Generation and Dataset Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Ju-Young Kim, Ji-Hong Park, Gun-Woo Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08362">https://arxiv.org/abs/2512.08362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08362">https://arxiv.org/pdf/2512.08362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08362]] SCU-CGAN: Enhancing Fire Detection through Synthetic Fire Image Generation and Dataset Augmentation(https://arxiv.org/abs/2512.08362)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Fire has long been linked to human life, causing severe disasters and losses. Early detection is crucial, and with the rise of home IoT technologies, household fire detection systems have emerged. However, the lack of sufficient fire datasets limits the performance of detection models. We propose the SCU-CGAN model, which integrates U-Net, CBAM, and an additional discriminator to generate realistic fire images from nonfire images. We evaluate the image quality and confirm that SCU-CGAN outperforms existing models. Specifically, SCU-CGAN achieved a 41.5% improvement in KID score compared to CycleGAN, demonstrating the superior quality of the generated fire images. Furthermore, experiments demonstrate that the augmented dataset significantly improves the accuracy of fire detection models without altering their structure. For the YOLOv5 nano model, the most notable improvement was observed in the mAP@0.5:0.95 metric, which increased by 56.5%, highlighting the effectiveness of the proposed approach.</li>
<li><strong>摘要：</strong>火灾长期以来与人类生命息息相关，造成了严重的灾害和损失。早期检测至关重要，随着家庭物联网技术的兴起，家庭火灾检测系统应运而生。然而，缺乏足够的火灾数据集限制了检测模型的性能。我们提出了 SCU-CGAN 模型，它集成了 U-Net、CBAM 和一个额外的鉴别器，可以从非火灾图像生成真实的火灾图像。我们评估图像质量并确认 SCU-CGAN 优于现有模型。具体来说，与 CycleGAN 相比，SCU-CGAN 的 KID 分数提高了 41.5%，证明了生成的火灾图像的卓越质量。此外，实验表明，增强的数据集在不改变火灾探测模型结构的情况下显着提高了火灾探测模型的准确性。对于 YOLOv5 nano 模型，最显着的改进是 mAP@0.5:0.95 指标，增加了 56.5%，凸显了所提出方法的有效性。</li>
</ul>

<h3>Title: Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models II: Benchmark Generation Process</h3>
<ul>
<li><strong>Authors: </strong>Gary Ackerman, Zachary Kallenborn, Anna Wetzel, Hayley Peterson, Jenna LaTourette, Olivia Shoemaker, Brandon Behlendorf, Sheriff Almakki, Doug Clifford, Noah Sheinbaum</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08451">https://arxiv.org/abs/2512.08451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08451">https://arxiv.org/pdf/2512.08451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08451]] Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models II: Benchmark Generation Process(https://arxiv.org/abs/2512.08451)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper, the second in a series of three, describes the second component of a novel Biothreat Benchmark Generation (BBG) framework: the generation of the Bacterial Biothreat Benchmark (B3) dataset. The development process involved three complementary approaches: 1) web-based prompt generation, 2) red teaming, and 3) mining existing benchmark corpora, to generate over 7,000 potential benchmarks linked to the Task-Query Architecture that was developed during the first component of the project. A process of de-duplication, followed by an assessment of uplift diagnosticity, and general quality control measures, reduced the candidates to a set of 1,010 final benchmarks. This procedure ensured that these benchmarks are a) diagnostic in terms of providing uplift; b) directly relevant to biosecurity threats; and c) are aligned with a larger biosecurity architecture permitting nuanced analysis at different levels of analysis.</li>
<li><strong>摘要：</strong>快速发展的前沿人工智能（AI）模型，特别是大型语言模型（LLM），促进生物恐怖主义或获取生物武器的潜力已经引起了政策、学术和公众的重大关注。模型开发者和政策制定者都寻求量化和减轻任何风险，其中一个重要因素是开发可以评估特定模型带来的生物安全风险的模型基准。本文是三篇系列文章中的第二篇，描述了新型生物威胁基准生成 (BBG) 框架的第二个组成部分：细菌生物威胁基准 (B3) 数据集的生成。开发过程涉及三种互补方法：1) 基于网络的提示生成，2) 红队，以及 3) 挖掘现有基准语料库，以生成与项目第一部分期间开发的任务查询架构相关的 7,000 多个潜在基准。经过重复数据删除过程，然后进行提升诊断性评估和一般质量控制措施，将候选者减少到一组 1,010 个最终基准。该程序确保这些基准是 a) 在提供提升方面的诊断性； b) 与生物安全威胁直接相关； c) 与更大的生物安全架构保持一致，允许在不同分析级别进行细致入微的分析。</li>
</ul>

<h3>Title: Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models III: Implementing the Bacterial Biothreat Benchmark (B3) Dataset</h3>
<ul>
<li><strong>Authors: </strong>Gary Ackerman, Theodore Wilson, Zachary Kallenborn, Olivia Shoemaker, Anna Wetzel, Hayley Peterson, Abigail Danfora, Jenna LaTourette, Brandon Behlendorf, Douglas Clifford</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08459">https://arxiv.org/abs/2512.08459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08459">https://arxiv.org/pdf/2512.08459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08459]] Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models III: Implementing the Bacterial Biothreat Benchmark (B3) Dataset(https://arxiv.org/abs/2512.08459)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper discusses the pilot implementation of the Bacterial Biothreat Benchmark (B3) dataset. It is the third in a series of three papers describing an overall Biothreat Benchmark Generation (BBG) framework, with previous papers detailing the development of the B3 dataset. The pilot involved running the benchmarks through a sample frontier AI model, followed by human evaluation of model responses, and an applied risk analysis of the results along several dimensions. Overall, the pilot demonstrated that the B3 dataset offers a viable, nuanced method for rapidly assessing the biosecurity risk posed by a LLM, identifying the key sources of that risk and providing guidance for priority areas of mitigation priority.</li>
<li><strong>摘要：</strong>快速发展的前沿人工智能（AI）模型，特别是大型语言模型（LLM），促进生物恐怖主义或获取生物武器的潜力已经引起了政策、学术和公众的重大关注。模型开发者和政策制定者都寻求量化和减轻任何风险，其中一个重要因素是开发可以评估特定模型带来的生物安全风险的模型基准。本文讨论了细菌生物威胁基准 (B3) 数据集的试点实施。这是描述整体生物威胁基准生成 (BBG) 框架的三篇论文系列中的第三篇，之前的论文详细介绍了 B3 数据集的开发。该试点项目涉及通过样本前沿人工智能模型运行基准，然后对模型响应进行人工评估，并对多个维度的结果进行应用风险分析。总体而言，该试点项目表明，B3 数据集提供了一种可行的、细致入微的方法，可以快速评估法学硕士带来的生物安全风险，确定该风险的关键来源，并为缓解优先领域的优先领域提供指导。</li>
</ul>

<h3>Title: Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform</h3>
<ul>
<li><strong>Authors: </strong>Yuning Gong, Yifei Liu, Yifan Zhan, Muyao Niu, Xueying Li, Yuanjun Liao, Jiaming Chen, Yuanyuan Gao, Jiaqi Chen, Minming Chen, Li Zhou, Yuning Zhang, Wei Wang, Xiaoqing Hou, Huaxi Huang, Shixiang Tang, Le Ma, Dingwen Zhang, Xue Yang, Junchi Yan, Yanchi Zhang, Yinqiang Zheng, Xiao Sun, Zhihang Zhong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08478">https://arxiv.org/abs/2512.08478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08478">https://arxiv.org/pdf/2512.08478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08478]] Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform(https://arxiv.org/abs/2512.08478)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, "click-to-run" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in this http URL library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.</li>
<li><strong>摘要：</strong>神经渲染，特别是 3D 高斯分布 (3DGS)，已经迅速发展并成为构建世界模型的关键组件。然而，现有的查看器解决方案仍然分散、笨重或受到遗留管道的限制，导致部署摩擦较大，并且对动态内容和生成模型的支持有限。在这项工作中，我们展示了 Visionary，一个开放的、网络原生的平台，用于实时各种高斯泼溅和网格渲染。 Visionary 基于高效的 WebGPU 渲染器和每帧 ONNX 推理而构建，可实现动态神经处理，同时保持轻量级的“即点即用”浏览器体验。它引入了标准化的高斯生成器合约，不仅支持标准的3DGS渲染，还允许即插即用算法生成或更新每帧的高斯。这种推断还使我们能够应用前馈生成后处理。该平台还在此 http URL 库中提供了一个插件，其中包含简洁的 TypeScript API，可以无缝集成到现有的 Web 应用程序中。实验表明，在相同的 3DGS 资源下，由于基于 GPU 的图元排序，Visionary 比当前的 Web 查看器实现了更高的渲染效率。它已经支持多种变体，包括基于 MLP 的 3DGS、4DGS、神经化身以及风格转换或增强网络。通过直接在浏览器中统一推理和渲染，Visionary 显着降低了 3DGS 系列方法的再现、比较和部署的障碍，成为重建和生成范式的统一世界模型载体。</li>
</ul>

<h3>Title: Temporal Concept Dynamics in Diffusion Models via Prompt-Conditioned Interventions</h3>
<ul>
<li><strong>Authors: </strong>Ada Gorgun, Fawaz Sammani, Nikos Deligiannis, Bernt Schiele, Jonas Fischer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08486">https://arxiv.org/abs/2512.08486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08486">https://arxiv.org/pdf/2512.08486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08486]] Temporal Concept Dynamics in Diffusion Models via Prompt-Conditioned Interventions(https://arxiv.org/abs/2512.08486)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models are usually evaluated by their final outputs, gradually denoising random noise into meaningful images. Yet, generation unfolds along a trajectory, and analyzing this dynamic process is crucial for understanding how controllable, reliable, and predictable these models are in terms of their success/failure modes. In this work, we ask the question: when does noise turn into a specific concept (e.g., age) and lock in the denoising trajectory? We propose PCI (Prompt-Conditioned Intervention) to study this question. PCI is a training-free and model-agnostic framework for analyzing concept dynamics through diffusion time. The central idea is the analysis of Concept Insertion Success (CIS), defined as the probability that a concept inserted at a given timestep is preserved and reflected in the final image, offering a way to characterize the temporal dynamics of concept formation. Applied to several state-of-the-art text-to-image diffusion models and a broad taxonomy of concepts, PCI reveals diverse temporal behaviors across diffusion models, in which certain phases of the trajectory are more favorable to specific concepts even within the same concept type. These findings also provide actionable insights for text-driven image editing, highlighting when interventions are most effective without requiring access to model internals or training, and yielding quantitatively stronger edits that achieve a balance of semantic accuracy and content preservation than strong baselines. Code is available at: this https URL</li>
<li><strong>摘要：</strong>扩散模型通常通过其最终输出进行评估，逐渐将随机噪声去噪为有意义的图像。然而，生成是沿着轨迹展开的，分析这一动态过程对于理解这些模型在成功/失败模式方面的可控性、可靠性和可预测性至关重要。在这项工作中，我们提出一个问题：噪声何时变成特定概念（例如年龄）并锁定去噪轨迹？我们提出PCI（即时条件干预）来研究这个问题。 PCI 是一个免训练且与模型无关的框架，用于通过扩散时间分析概念动态。中心思想是概念插入成功（CIS）的分析，定义为在给定时间步插入的概念被保留并反映在最终图像中的概率，提供了一种表征概念形成的时间动态的方法。 PCI 应用于几种最先进的文本到图像扩散模型和广泛的概念分类，揭示了扩散模型中的不同时间行为，其中轨迹的某些阶段更适合特定概念，即使在同一概念类型中也是如此。这些发现还为文本驱动的图像编辑提供了可行的见解，强调了干预何时最有效，而无需访问模型内​​部或培训，并产生了定量更强的编辑，与强大的基线相比，实现了语义准确性和内容保留的平衡。代码位于：此 https URL</li>
</ul>

<h3>Title: Beyond the Noise: Aligning Prompts with Latent Representations in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Vasco Ramos, Regev Cohen, Idan Szpektor, Joao Magalhaes</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08505">https://arxiv.org/abs/2512.08505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08505">https://arxiv.org/pdf/2512.08505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08505]] Beyond the Noise: Aligning Prompts with Latent Representations in Diffusion Models(https://arxiv.org/abs/2512.08505)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Conditional diffusion models rely on language-to-image alignment methods to steer the generation towards semantically accurate outputs. Despite the success of this architecture, misalignment and hallucinations remain common issues and require automatic misalignment detection tools to improve quality, for example by applying them in a Best-of-N (BoN) post-generation setting. Unfortunately, measuring the alignment after the generation is an expensive step since we need to wait for the overall generation to finish to determine prompt adherence. In contrast, this work hypothesizes that text/image misalignments can be detected early in the denoising process, enabling real-time alignment assessment without waiting for the complete generation. In particular, we propose NoisyCLIP a method that measures semantic alignment in the noisy latent space. This work is the first to explore and benchmark prompt-to-latent misalignment detection during image generation using dual encoders in the reverse diffusion process. We evaluate NoisyCLIP qualitatively and quantitatively and find it reduces computational cost by 50% while achieving 98% of CLIP alignment performance in BoN settings. This approach enables real-time alignment assessment during generation, reducing costs without sacrificing semantic fidelity.</li>
<li><strong>摘要：</strong>条件扩散模型依赖于语言到图像的对齐方法来引导生成语义准确的输出。尽管这种架构取得了成功，但错位和幻觉仍然是常见问题，需要自动错位检测工具来提高质量，例如通过将它们应用在 Best-of-N (BoN) 后生成设置中。不幸的是，在生成后测量对齐是一个昂贵的步骤，因为我们需要等待整个生成完成才能确定及时的遵守情况。相反，这项工作假设可以在去噪过程的早期检测到文本/图像未对齐，从而无需等待完整生成即可进行实时对齐评估。特别是，我们提出了 NoisyCLIP 一种测量噪声潜在空间中语义对齐的方法。这项工作是第一个探索和基准测试在反向扩散过程中使用双编码器生成图像期间的提示到潜在错位检测。我们对 NoisyCLIP 进行了定性和定量评估，发现它可将计算成本降低 50%，同时在 BoN 设置中实现 98% 的 CLIP 对齐性能。这种方法可以在生成过程中进行实时对齐评估，从而在不牺牲语义保真度的情况下降低成本。</li>
</ul>

<h3>Title: OCCDiff: Occupancy Diffusion Model for High-Fidelity 3D Building Reconstruction from Noisy Point Clouds</h3>
<ul>
<li><strong>Authors: </strong>Jialu Sui, Rui Liu, Hongsheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08506">https://arxiv.org/abs/2512.08506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08506">https://arxiv.org/pdf/2512.08506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08506]] OCCDiff: Occupancy Diffusion Model for High-Fidelity 3D Building Reconstruction from Noisy Point Clouds(https://arxiv.org/abs/2512.08506)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>A major challenge in reconstructing buildings from LiDAR point clouds lies in accurately capturing building surfaces under varying point densities and noise interference. To flexibly gather high-quality 3D profiles of the building in diverse resolution, we propose OCCDiff applying latent diffusion in the occupancy function space. Our OCCDiff combines a latent diffusion process with a function autoencoder architecture to generate continuous occupancy functions evaluable at arbitrary locations. Moreover, a point encoder is proposed to provide condition features to diffusion learning, constraint the final occupancy prediction for occupancy decoder, and insert multi-modal features for latent generation to latent encoder. To further enhance the model performance, a multi-task training strategy is employed, ensuring that the point encoder learns diverse and robust feature representations. Empirical results show that our method generates physically consistent samples with high fidelity to the target distribution and exhibits robustness to noisy data.</li>
<li><strong>摘要：</strong>从激光雷达点云重建建筑物的一个主要挑战在于在不同的点密度和噪声干扰下准确捕获建筑物表面。为了以不同的分辨率灵活收集建筑物的高质量 3D 轮廓，我们建议 OCCDiff 在占用函数空间中应用潜在扩散。我们的 OCCDiff 将潜在扩散过程与函数自动编码器架构相结合，生成可在任意位置评估的连续占用函数。此外，提出了一种点编码器来为扩散学习提供条件特征，约束占用解码器的最终占用预测，并将用于潜在生成的多模态特征插入到潜在编码器。为了进一步提高模型性能，采用了多任务训练策略，确保点编码器学习多样化且鲁棒的特征表示。经验结果表明，我们的方法生成了物理上一致的样本，对目标分布具有高保真度，并对噪声数据表现出鲁棒性。</li>
</ul>

<h3>Title: PaintFlow: A Unified Framework for Interactive Oil Paintings Editing and Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhangli Hu, Ye Chen, Jiajun Yao, Bingbing Ni</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08534">https://arxiv.org/abs/2512.08534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08534">https://arxiv.org/pdf/2512.08534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08534]] PaintFlow: A Unified Framework for Interactive Oil Paintings Editing and Generation(https://arxiv.org/abs/2512.08534)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generation</a></li>
<li><strong>Abstract: </strong>Oil painting, as a high-level medium that blends human abstract thinking with artistic expression, poses substantial challenges for digital generation and editing due to its intricate brushstroke dynamics and stylized characteristics. Existing generation and editing techniques are often constrained by the distribution of training data and primarily focus on modifying real photographs. In this work, we introduce a unified multimodal framework for oil painting generation and editing. The proposed system allows users to incorporate reference images for precise semantic control, hand-drawn sketches for spatial structure alignment, and natural language prompts for high-level semantic guidance, while consistently maintaining a unified painting style across all outputs. Our method achieves interactive oil painting creation through three crucial technical advancements. First, we enhance the training stage with spatial alignment and semantic enhancement conditioning strategy, which map masks and sketches into spatial constraints, and encode contextual embedding from reference images and text into feature constraints, enabling object-level semantic alignment. Second, to overcome data scarcity, we propose a self-supervised style transfer pipeline based on Stroke-Based Rendering (SBR), which simulates the inpainting dynamics of oil painting restoration, converting real images into stylized oil paintings with preserved brushstroke textures to construct a large-scale paired training dataset. Finally, during inference, we integrate features using the AdaIN operator to ensure stylistic consistency. Extensive experiments demonstrate that our interactive system enables fine-grained editing while preserving the artistic qualities of oil paintings, achieving an unprecedented level of imagination realization in stylized oil paintings generation and editing.</li>
<li><strong>摘要：</strong>油画作为一种融合人类抽象思维与艺术表达的高级媒介，其复杂的笔触动态和风格化特征给数字生成和编辑带来了巨大的挑战。现有的生成和编辑技术通常受到训练数据分布的限制，并且主要集中于修改真实照片。在这项工作中，我们引入了用于油画生成和编辑的统一多模式框架。所提出的系统允许用户合并用于精确语义控制的参考图像、用于空间结构对齐的手绘草图以及用于高级语义指导的自然语言提示，同时在所有输出中一致地保持统一的绘画风格。我们的方法通过三个关键的技术进步实现了交互式油画创作。首先，我们通过空间对齐和语义增强调节策略来增强训练阶段，将掩模和草图映射到空间约束中，并将参考图像和文本的上下文嵌入编码到特征约束中，从而实现对象级语义对齐。其次，为了克服数据稀缺性，我们提出了一种基于笔画渲染（SBR）的自监督风格转移管道，它模拟油画修复的修复动态，将真实图像转换为保留笔触纹理的风格化油画，以构建大规模配对训练数据集。最后，在推理过程中，我们使用 AdaIN 运算符集成特征以确保风格一致性。大量的实验表明，我们的交互系统能够在保留油画艺术品质的同时实现细粒度的编辑，在风格化油画生成和编辑方面达到了前所未有的想象力实现水平。</li>
</ul>

<h3>Title: Photo3D: Advancing Photorealistic 3D Generation through Structure-Aligned Detail Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Xinyue Liang, Zhinyuan Ma, Lingchen Sun, Yanjun Guo, Lei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08535">https://arxiv.org/abs/2512.08535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08535">https://arxiv.org/pdf/2512.08535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08535]] Photo3D: Advancing Photorealistic 3D Generation through Structure-Aligned Detail Enhancement(https://arxiv.org/abs/2512.08535)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Although recent 3D-native generators have made great progress in synthesizing reliable geometry, they still fall short in achieving realistic appearances. A key obstacle lies in the lack of diverse and high-quality real-world 3D assets with rich texture details, since capturing such data is intrinsically difficult due to the diverse scales of scenes, non-rigid motions of objects, and the limited precision of 3D scanners. We introduce Photo3D, a framework for advancing photorealistic 3D generation, which is driven by the image data generated by the GPT-4o-Image model. Considering that the generated images can distort 3D structures due to their lack of multi-view consistency, we design a structure-aligned multi-view synthesis pipeline and construct a detail-enhanced multi-view dataset paired with 3D geometry. Building on it, we present a realistic detail enhancement scheme that leverages perceptual feature adaptation and semantic structure matching to enforce appearance consistency with realistic details while preserving the structural consistency with the 3D-native geometry. Our scheme is general to different 3D-native generators, and we present dedicated training strategies to facilitate the optimization of geometry-texture coupled and decoupled 3D-native generation paradigms. Experiments demonstrate that Photo3D generalizes well across diverse 3D-native generation paradigms and achieves state-of-the-art photorealistic 3D generation performance.</li>
<li><strong>摘要：</strong>尽管最近的 3D 原生生成器在合成可靠的几何体方面取得了巨大进步，但它们在实现逼真的外观方面仍然存在不足。一个关键障碍在于缺乏具有丰富纹理细节的多样化、高质量的现实世界 3D 资产，因为由于场景规模不同、物体的非刚性运动以及 3D 扫描仪的精度有限，捕获此类数据本质上是困难的。我们介绍 Photo3D，这是一个用于推进逼真 3D 生成的框架，它由 GPT-4o-Image 模型生成的图像数据驱动。考虑到生成的图像由于缺乏多视图一致性而可能扭曲 3D 结构，我们设计了结构对齐的多视图合成管道，并构建了与 3D 几何配对的细节增强的多视图数据集。在此基础上，我们提出了一种真实的细节增强方案，该方案利用感知特征适应和语义结构匹配来强制外观与真实细节的一致性，同时保持与 3D 原生几何的结构一致性。我们的方案适用于不同的 3D 原生生成器，并且我们提出了专门的训练策略，以促进几何纹理耦合和解耦 3D 原生生成范例的优化。实验表明，Photo3D 可以很好地概括各种 3D 原生生成范例，并实现最先进的照片级真实感 3D 生成性能。</li>
</ul>

<h3>Title: Fast-ARDiff: An Entropy-informed Acceleration Framework for Continuous Space Autoregressive Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhen Zou, Xiaoxiao Ma, Jie Huang, Zichao Yu, Feng Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08537">https://arxiv.org/abs/2512.08537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08537">https://arxiv.org/pdf/2512.08537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08537]] Fast-ARDiff: An Entropy-informed Acceleration Framework for Continuous Space Autoregressive Generation(https://arxiv.org/abs/2512.08537)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Autoregressive(AR)-diffusion hybrid paradigms combine AR's structured modeling with diffusion's photorealistic synthesis, yet suffer from high latency due to sequential AR generation and iterative denoising. In this work, we tackle this bottleneck and propose a unified AR-diffusion framework Fast-ARDiff that jointly optimizes both components, accelerating AR speculative decoding while simultaneously facilitating faster diffusion decoding. Specifically: (1) The entropy-informed speculative strategy encourages draft model to produce higher-entropy representations aligned with target model's entropy characteristics, mitigating entropy mismatch and high rejection rates caused by draft overconfidence. (2) For diffusion decoding, rather than treating it as an independent module, we integrate it into the same end-to-end framework using a dynamic scheduler that prioritizes AR optimization to guide the diffusion part in further steps. The diffusion part is optimized through a joint distillation framework combining trajectory and distribution matching, ensuring stable training and high-quality synthesis with extremely few steps. During inference, shallow feature entropy from AR module is used to pre-filter low-entropy drafts, avoiding redundant computation and improving latency. Fast-ARDiff achieves state-of-the-art acceleration across diverse models: on ImageNet 256$\times$256, TransDiff attains 4.3$\times$ lossless speedup, and NextStep-1 achieves 3$\times$ acceleration on text-conditioned generation. Code will be available at this https URL.</li>
<li><strong>摘要：</strong>自回归 (AR)-扩散混合范例将 AR 的结构化建模与扩散的真实感合成相结合，但由于顺序 AR 生成和迭代去噪而存在高延迟。在这项工作中，我们解决了这个瓶颈，并提出了一个统一的 AR 扩散框架 Fast-ARDiff，它联合优化这两个组件，加速 AR 推测解码，同时促进更快的扩散解码。具体来说：（1）基于熵的推测策略鼓励选秀模型产生与目标模型的熵特征一致的更高熵表示，从而减轻由于选秀过度自信而导致的熵不匹配和高拒绝率。 (2) 对于扩散解码，我们没有将其视为独立模块，而是使用动态调度器将其集成到相同的端到端框架中，该调度器优先考虑 AR 优化以指导扩散部分进行进一步的步骤。扩散部分通过结合轨迹和分布匹配的联合蒸馏框架进行优化，以极少的步骤确保稳定的训练和高质量的合成。在推理过程中，AR模块的浅层特征熵用于预过滤低熵草稿，避免冗余计算并改善延迟。 Fast-ARDiff 在不同模型上实现了最先进的加速：在 ImageNet 256$\times$256 上，TransDiff 实现了 4.3$\times$ 无损加速，NextStep-1 在文本条件生成上实现了 3$\times$ 加速。代码将在此 https URL 中提供。</li>
</ul>

<h3>Title: A Novel Wasserstein Quaternion Generative Adversarial Network for Color Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhigang Jia, Duan Wang, Hengkai Wang, Yajun Xie, Meixiang Zhao, Xiaoyu Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08542">https://arxiv.org/abs/2512.08542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08542">https://arxiv.org/pdf/2512.08542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08542]] A Novel Wasserstein Quaternion Generative Adversarial Network for Color Image Generation(https://arxiv.org/abs/2512.08542)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Color image generation has a wide range of applications, but the existing generation models ignore the correlation among color channels, which may lead to chromatic aberration problems. In addition, the data distribution problem of color images has not been systematically elaborated and explained, so that there is still the lack of the theory about measuring different color images datasets. In this paper, we define a new quaternion Wasserstein distance and develop its dual theory. To deal with the quaternion linear programming problem, we derive the strong duality form with helps of quaternion convex set separation theorem and quaternion Farkas lemma. With using quaternion Wasserstein distance, we propose a novel Wasserstein quaternion generative adversarial network. Experiments demonstrate that this novel model surpasses both the (quaternion) generative adversarial networks and the Wasserstein generative adversarial network in terms of generation efficiency and image quality.</li>
<li><strong>摘要：</strong>彩色图像生成有着广泛的应用，但现有的生成模型忽略了颜色通道之间的相关性，这可能会导致色差问题。此外，彩色图像的数据分布问题尚未得到系统的阐述和解释，因此仍然缺乏测量不同彩色图像数据集的理论。在本文中，我们定义了一个新的四元数 Wasserstein 距离并发展了其对偶理论。为了解决四元数线性规划问题，我们借助四元数凸集分离定理和四元数Farkas引理推导了强对偶形式。通过使用四元数 Wasserstein 距离，我们提出了一种新颖的 Wasserstein 四元数生成对抗网络。实验表明，这种新颖的模型在生成效率和图像质量方面都超越了（四元数）生成对抗网络和 Wasserstein 生成对抗网络。</li>
</ul>

<h3>Title: Disturbance-Free Surgical Video Generation from Multi-Camera Shadowless Lamps for Open Surgery</h3>
<ul>
<li><strong>Authors: </strong>Yuna Kato, Shohei Mori, Hideo Saito, Yoshifumi Takatsume, Hiroki Kajita, Mariko Isogawa</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08577">https://arxiv.org/abs/2512.08577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08577">https://arxiv.org/pdf/2512.08577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08577]] Disturbance-Free Surgical Video Generation from Multi-Camera Shadowless Lamps for Open Surgery(https://arxiv.org/abs/2512.08577)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Video recordings of open surgeries are greatly required for education and research purposes. However, capturing unobstructed videos is challenging since surgeons frequently block the camera field of view. To avoid occlusion, the positions and angles of the camera must be frequently adjusted, which is highly labor-intensive. Prior work has addressed this issue by installing multiple cameras on a shadowless lamp and arranging them to fully surround the surgical area. This setup increases the chances of some cameras capturing an unobstructed view. However, manual image alignment is needed in post-processing since camera configurations change every time surgeons move the lamp for optimal lighting. This paper aims to fully automate this alignment task. The proposed method identifies frames in which the lighting system moves, realigns them, and selects the camera with the least occlusion to generate a video that consistently presents the surgical field from a fixed perspective. A user study involving surgeons demonstrated that videos generated by our method were superior to those produced by conventional methods in terms of the ease of confirming the surgical area and the comfort during video viewing. Additionally, our approach showed improvements in video quality over existing techniques. Furthermore, we implemented several synthesis options for the proposed view-synthesis method and conducted a user study to assess surgeons' preferences for each option.</li>
<li><strong>摘要：</strong>出于教育和研究目的，非常需要开放式手术的视频记录。然而，捕捉无障碍视频具有挑战性，因为外科医生经常遮挡摄像机视野。为了避免遮挡，必须经常调整相机的位置和角度，这是一个高度劳动密集型的过程。之前的工作已经通过在无影灯上安装多个摄像头并将它们排列成完全包围手术区域来解决这个问题。这种设置增加了一些摄像机捕捉无障碍视野的机会。然而，在后处理中需要手动图像对齐，因为每次外科医生移动灯以获得最佳照明时，相机配置都会发生变化。本文旨在完全自动化此对齐任务。所提出的方法识别照明系统移动的帧，重新对齐它们，并选择遮挡最少的摄像机来生成从固定视角一致呈现手术视野的视频。一项涉及外科医生的用户研究表明，在确认手术区域的容易性和视频观看期间的舒适度方面，通过我们的方法生成的视频优于传统方法生成的视频。此外，我们的方法显示视频质量比现有技术有所提高。此外，我们为所提出的视图合成方法实现了多个合成选项，并进行了用户研究以评估外科医生对每个选项的偏好。</li>
</ul>

<h3>Title: Chain-of-Image Generation: Toward Monitorable and Controllable Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Young Kyung Kim, Oded Schlesinger, Yuzhou Zhao, J. Matias Di Martino, Guillermo Sapiro</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08645">https://arxiv.org/abs/2512.08645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08645">https://arxiv.org/pdf/2512.08645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08645]] Chain-of-Image Generation: Toward Monitorable and Controllable Image Generation(https://arxiv.org/abs/2512.08645)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>While state-of-the-art image generation models achieve remarkable visual quality, their internal generative processes remain a "black box." This opacity limits human observation and intervention, and poses a barrier to ensuring model reliability, safety, and control. Furthermore, their non-human-like workflows make them difficult for human observers to interpret. To address this, we introduce the Chain-of-Image Generation (CoIG) framework, which reframes image generation as a sequential, semantic process analogous to how humans create art. Similar to the advantages in monitorability and performance that Chain-of-Thought (CoT) brought to large language models (LLMs), CoIG can produce equivalent benefits in text-to-image generation. CoIG utilizes an LLM to decompose a complex prompt into a sequence of simple, step-by-step instructions. The image generation model then executes this plan by progressively generating and editing the image. Each step focuses on a single semantic entity, enabling direct monitoring. We formally assess this property using two novel metrics: CoIG Readability, which evaluates the clarity of each intermediate step via its corresponding output; and Causal Relevance, which quantifies the impact of each procedural step on the final generated image. We further show that our framework mitigates entity collapse by decomposing the complex generation task into simple subproblems, analogous to the procedural reasoning employed by CoT. Our experimental results indicate that CoIG substantially enhances quantitative monitorability while achieving competitive compositional robustness compared to established baseline models. The framework is model-agnostic and can be integrated with any image generation model.</li>
<li><strong>摘要：</strong>虽然最先进的图像生成模型实现了卓越的视觉质量，但其内部生成过程仍然是一个“黑匣子”。这种不透明性限制了人类的观察和干预，并为确保模型的可靠性、安全性和控制造成了障碍。此外，它们的非人类工作流程使人类观察者难以解释。为了解决这个问题，我们引入了图像生成链（CoIG）框架，它将图像生成重新定义为一个连续的语义过程，类似于人类创造艺术的方式。与思想链 (CoT) 为大型语言模型 (LLM) 带来的可监控性和性能优势类似，CoIG 可以在文本到图像生成方面产生同等的优势。 CoIG 利用 LLM 将复杂的提示分解为一系列简单的分步指令。然后，图像生成模型通过逐步生成和编辑图像来执行该计划。每个步骤都专注于单个语义实体，从而实现直接监控。我们使用两个新颖的指标正式评估此属性：CoIG 可读性，它通过相应的输出评估每个中间步骤的清晰度；因果相关性，量化每个程序步骤对最终生成图像的影响。我们进一步表明，我们的框架通过将复杂的生成任务分解为简单的子问题来减轻实体崩溃，类似于 CoT 采用的程序推理。我们的实验结果表明，与已建立的基线模型相比，CoIG 显着增强了定量可监测性，同时实现了有竞争力的成分稳健性。该框架与模型无关，可以与任何图像生成模型集成。</li>
</ul>

<h3>Title: Repulsor: Accelerating Generative Modeling with a Contrastive Memory Bank</h3>
<ul>
<li><strong>Authors: </strong>Shaofeng Zhang, Xuanqi Chen, Ning Liao, Haoxiang Zhao, Xiaoxing Wang, Haoru Tan, Sitong Wu, Xiaosong Jia, Qi Fan, Junchi Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08648">https://arxiv.org/abs/2512.08648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08648">https://arxiv.org/pdf/2512.08648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08648]] Repulsor: Accelerating Generative Modeling with a Contrastive Memory Bank(https://arxiv.org/abs/2512.08648)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The dominance of denoising generative models (e.g., diffusion, flow-matching) in visual synthesis is tempered by their substantial training costs and inefficiencies in representation learning. While injecting discriminative representations via auxiliary alignment has proven effective, this approach still faces key limitations: the reliance on external, pre-trained encoders introduces overhead and domain shift. A dispersed-based strategy that encourages strong separation among in-batch latent representations alleviates this specific dependency. To assess the effect of the number of negative samples in generative modeling, we propose {\mname}, a plug-and-play training framework that requires no external encoders. Our method integrates a memory bank mechanism that maintains a large, dynamically updated queue of negative samples across training iterations. This decouples the number of negatives from the mini-batch size, providing abundant and high-quality negatives for a contrastive objective without a multiplicative increase in computational cost. A low-dimensional projection head is used to further minimize memory and bandwidth overhead. {\mname} offers three principal advantages: (1) it is self-contained, eliminating dependency on pretrained vision foundation models and their associated forward-pass overhead; (2) it introduces no additional parameters or computational cost during inference; and (3) it enables substantially faster convergence, achieving superior generative quality more efficiently. On ImageNet-256, {\mname} achieves a state-of-the-art FID of \textbf{2.40} within 400k steps, significantly outperforming comparable methods.</li>
<li><strong>摘要：</strong>去噪生成模型（例如扩散、流匹配）在视觉合成中的主导地位因其巨大的训练成本和表示学习的低效率而受到削弱。虽然通过辅助对齐注入判别性表示已被证明是有效的，但这种方法仍然面临关键限制：对外部预训练编码器的依赖会带来开销和域移位。基于分散的策略鼓励批量潜在表示之间的强烈分离，从而减轻了这种特定的依赖性。为了评估生成模型中负样本数量的影响，我们提出了 {\mname}，一种无需外部编码器的即插即用训练框架。我们的方法集成了一个内存库机制，该机制在训练迭代中维护一个大型的、动态更新的负样本队列。这将负片的数量与小批量大小解耦，为对比目标提供丰富且高质量的负片，而不会成倍增加计算成本。低维投影头用于进一步最小化内存和带宽开销。 {\mname} 提供了三个主要优点：（1）它是独立的，消除了对预训练视觉基础模型及其相关前向传递开销的依赖； (2)在推理过程中不引入额外的参数或计算成本； (3) 它能够显着加快收敛速度​​，更有效地实现卓越的生成质量。在 ImageNet-256 上，{\mname} 在 400k 步内实现了最先进的 FID \textbf{2.40}，显着优于同类方法。</li>
</ul>

<h3>Title: Dual-Branch Center-Surrounding Contrast: Rethinking Contrastive Learning for 3D Point Clouds</h3>
<ul>
<li><strong>Authors: </strong>Shaofeng Zhang, Xuanqi Chen, Xiangdong Zhang, Sitong Wu, Junchi Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08673">https://arxiv.org/abs/2512.08673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08673">https://arxiv.org/pdf/2512.08673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08673]] Dual-Branch Center-Surrounding Contrast: Rethinking Contrastive Learning for 3D Point Clouds(https://arxiv.org/abs/2512.08673)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Most existing self-supervised learning (SSL) approaches for 3D point clouds are dominated by generative methods based on Masked Autoencoders (MAE). However, these generative methods have been proven to struggle to capture high-level discriminative features effectively, leading to poor performance on linear probing and other downstream tasks. In contrast, contrastive methods excel in discriminative feature representation and generalization ability on image data. Despite this, contrastive learning (CL) in 3D data remains scarce. Besides, simply applying CL methods designed for 2D data to 3D fails to effectively learn 3D local details. To address these challenges, we propose a novel Dual-Branch \textbf{C}enter-\textbf{S}urrounding \textbf{Con}trast (CSCon) framework. Specifically, we apply masking to the center and surrounding parts separately, constructing dual-branch inputs with center-biased and surrounding-biased representations to better capture rich geometric information. Meanwhile, we introduce a patch-level contrastive loss to further enhance both high-level information and local sensitivity. Under the FULL and ALL protocols, CSCon achieves performance comparable to generative methods; under the MLP-LINEAR, MLP-3, and ONLY-NEW protocols, our method attains state-of-the-art results, even surpassing cross-modal approaches. In particular, under the MLP-LINEAR protocol, our method outperforms the baseline (Point-MAE) by \textbf{7.9\%}, \textbf{6.7\%}, and \textbf{10.3\%} on the three variants of ScanObjectNN, respectively. The code will be made publicly available.</li>
<li><strong>摘要：</strong>大多数现有的 3D 点云自监督学习 (SSL) 方法主要以基于掩码自动编码器 (MAE) 的生成方法为主。然而，这些生成方法已被证明难以有效捕获高级判别特征，导致线性探测和其他下游任务的性能不佳。相比之下，对比方法在图像数据的判别性特征表示和泛化能力方面表现出色。尽管如此，3D 数据中的对比学习 (CL) 仍然很少。此外，简单地将针对 2D 数据设计的 CL 方法应用于 3D 无法有效学习 3D 局部细节。为了应对这些挑战，我们提出了一种新颖的双分支 \textbf{C}enter-\textbf{S}urrounding \textbf{Con}trast (CSCon) 框架。具体来说，我们分别对中心和周围部分应用掩蔽，构建具有中心偏向和周围偏向表示的双分支输入，以更好地捕获丰富的几何信息。同时，我们引入了补丁级对比损失，以进一步增强高级信息和局部敏感性。在FULL和ALL协议下，CSCon实现了与生成方法相当的性能；在 MLP-LINEAR、MLP-3 和 ONLY-NEW 协议下，我们的方法获得了最先进的结果，甚至超越了跨模态方法。特别是，在 MLP-LINEAR 协议下，我们的方法在 ScanObjectNN 的三个变体上分别优于基线（Point-MAE） \textbf{7.9\%}、\textbf{6.7\%} 和 \textbf{10.3\%}。该代码将公开。</li>
</ul>

<h3>Title: Exposing Hidden Biases in Text-to-Image Models via Automated Prompt Search</h3>
<ul>
<li><strong>Authors: </strong>Manos Plitsis, Giorgos Bouritsas, Vassilis Katsouros, Yannis Panagakis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08724">https://arxiv.org/abs/2512.08724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08724">https://arxiv.org/pdf/2512.08724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08724]] Exposing Hidden Biases in Text-to-Image Models via Automated Prompt Search(https://arxiv.org/abs/2512.08724)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-image (TTI) diffusion models have achieved remarkable visual quality, yet they have been repeatedly shown to exhibit social biases across sensitive attributes such as gender, race and age. To mitigate these biases, existing approaches frequently depend on curated prompt datasets - either manually constructed or generated with large language models (LLMs) - as part of their training and/or evaluation procedures. Beside the curation cost, this also risks overlooking unanticipated, less obvious prompts that trigger biased generation, even in models that have undergone debiasing. In this work, we introduce Bias-Guided Prompt Search (BGPS), a framework that automatically generates prompts that aim to maximize the presence of biases in the resulting images. BGPS comprises two components: (1) an LLM instructed to produce attribute-neutral prompts and (2) attribute classifiers acting on the TTI's internal representations that steer the decoding process of the LLM toward regions of the prompt space that amplify the image attributes of interest. We conduct extensive experiments on Stable Diffusion 1.5 and a state-of-the-art debiased model and discover an array of subtle and previously undocumented biases that severely deteriorate fairness metrics. Crucially, the discovered prompts are interpretable, i.e they may be entered by a typical user, quantitatively improving the perplexity metric compared to a prominent hard prompt optimization counterpart. Our findings uncover TTI vulnerabilities, while BGPS expands the bias search space and can act as a new evaluation tool for bias mitigation.</li>
<li><strong>摘要：</strong>文本到图像（TTI）扩散模型已经实现了卓越的视觉质量，但它们已被反复证明在性别、种族和年龄等敏感属性上表现出社会偏见。为了减轻这些偏差，现有方法经常依赖于精心策划的提示数据集（手动构建或使用大型语言模型 (LLM) 生成）作为其培训和/或评估程序的一部分。除了管理成本之外，这还存在忽视意外的、不太明显的提示的风险，这些提示会触发有偏差的生成，即使在已经进行去偏差的模型中也是如此。在这项工作中，我们引入了偏差引导提示搜索（BGPS），这是一个自动生成提示的框架，旨在最大限度地提高结果图像中存在的偏差。 BGPS 包含两个组件：(1) 指示产生属性中性提示的 LLM，以及 (2) 作用于 TTI 内部表示的属性分类器，将 LLM 的解码过程引导至提示空间的区域，从而放大感兴趣的图像属性。我们对 Stable Diffusion 1.5 和最先进的去偏差模型进行了广泛的实验，发现了一系列微妙且以前未记录的偏差，这些偏差严重恶化了公平性指标。至关重要的是，发现的提示是可解释的，即它们可以由典型用户输入，与突出的硬提示优化对应物相比，定量地提高了困惑度指标。我们的研究结果揭示了 TTI 漏洞，而 BGPS 扩大了偏差搜索空间，可以作为减轻偏差的新评估工具。</li>
</ul>

<h3>Title: Neural Ordinary Differential Equations for Simulating Metabolic Pathway Dynamics from Time-Series Multiomics Data</h3>
<ul>
<li><strong>Authors: </strong>Udesh Habaraduwa, Andrei Lixandru</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.SC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08732">https://arxiv.org/abs/2512.08732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08732">https://arxiv.org/pdf/2512.08732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08732]] Neural Ordinary Differential Equations for Simulating Metabolic Pathway Dynamics from Time-Series Multiomics Data(https://arxiv.org/abs/2512.08732)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The advancement of human healthspan and bioengineering relies heavily on predicting the behavior of complex biological systems. While high-throughput multiomics data is becoming increasingly abundant, converting this data into actionable predictive models remains a bottleneck. High-capacity, datadriven simulation systems are critical in this landscape; unlike classical mechanistic models restricted by prior knowledge, these architectures can infer latent interactions directly from observational data, allowing for the simulation of temporal trajectories and the anticipation of downstream intervention effects in personalized medicine and synthetic biology. To address this challenge, we introduce Neural Ordinary Differential Equations (NODEs) as a dynamic framework for learning the complex interplay between the proteome and metabolome. We applied this framework to time-series data derived from engineered Escherichia coli strains, modeling the continuous dynamics of metabolic pathways. The proposed NODE architecture demonstrates superior performance in capturing system dynamics compared to traditional machine learning pipelines. Our results show a greater than 90% improvement in root mean squared error over baselines across both Limonene (up to 94.38% improvement) and Isopentenol (up to 97.65% improvement) pathway datasets. Furthermore, the NODE models demonstrated a 1000x acceleration in inference time, establishing them as a scalable, high-fidelity tool for the next generation of metabolic engineering and biological discovery.</li>
<li><strong>摘要：</strong>人类健康寿命和生物工程的进步在很大程度上依赖于预测复杂生物系统的行为。尽管高通量多组学数据变得越来越丰富，但将这些数据转换为可操作的预测模型仍然是一个瓶颈。高容量、数据驱动的模拟系统在这一领域至关重要；与受先验知识限制的经典机械模型不同，这些架构可以直接从观察数据推断潜在的相互作用，从而可以模拟时间轨迹并预测个性化医疗和合成生物学中的下游干预效果。为了应对这一挑战，我们引入神经常微分方程（NODE）作为学习蛋白质组和代谢组之间复杂相互作用的动态框架。我们将该框架应用于源自工程大肠杆菌菌株的时间序列数据，对代谢途径的连续动态进行建模。与传统的机器学习管道相比，所提出的 NODE 架构在捕获系统动态方面表现出卓越的性能。我们的结果显示，柠檬烯（高达 94.38% 的改进）和异戊烯醇（高达 97.65% 的改进）路径数据集的均方根误差较基线提高了 90% 以上。此外，NODE 模型的推理时间加速了 1000 倍，使其成为下一代代谢工程和生物发现的可扩展、高保真工具。</li>
</ul>

<h3>Title: A Scalable Pipeline Combining Procedural 3D Graphics and Guided Diffusion for Photorealistic Synthetic Training Data Generation in White Button Mushroom Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Artúr I. Károly, Péter Galambos</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08747">https://arxiv.org/abs/2512.08747</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08747">https://arxiv.org/pdf/2512.08747</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08747]] A Scalable Pipeline Combining Procedural 3D Graphics and Guided Diffusion for Photorealistic Synthetic Training Data Generation in White Button Mushroom Segmentation(https://arxiv.org/abs/2512.08747)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Industrial mushroom cultivation increasingly relies on computer vision for monitoring and automated harvesting. However, developing accurate detection and segmentation models requires large, precisely annotated datasets that are costly to produce. Synthetic data provides a scalable alternative, yet often lacks sufficient realism to generalize to real-world scenarios. This paper presents a novel workflow that integrates 3D rendering in Blender with a constrained diffusion model to automatically generate high-quality annotated, photorealistic synthetic images of Agaricus Bisporus mushrooms. This approach preserves full control over 3D scene configuration and annotations while achieving photorealism without the need for specialized computer graphics expertise. We release two synthetic datasets (each containing 6,000 images depicting over 250k mushroom instances) and evaluate Mask R-CNN models trained on them in a zero-shot setting. When tested on two independent real-world datasets (including a newly collected benchmark), our method achieves state-of-the-art segmentation performance (F1 = 0.859 on M18K), despite using only synthetic training data. Although the approach is demonstrated on Agaricus Bisporus mushrooms, the proposed pipeline can be readily adapted to other mushroom species or to other agricultural domains, such as fruit and leaf detection.</li>
<li><strong>摘要：</strong>工业蘑菇种植越来越依赖计算机视觉进行监控和自动收获。然而，开发准确的检测和分割模型需要大量、精确注释的数据集，而这些数据集的生产成本很高。合成数据提供了一种可扩展的替代方案，但通常缺乏足够的现实性来推广到现实世界的场景。本文提出了一种新颖的工作流程，它将 Blender 中的 3D 渲染与约束扩散模型相集成，以自动生成高质量的带注释、逼真的双孢蘑菇合成图像。这种方法保留了对 3D 场景配置和注释的完全控制，同时实现照片级真实感，而无需专门的计算机图形专业知识。我们发布了两个合成数据集（每个数据集包含 6,000 张图像，描绘了超过 25 万个蘑菇实例），并评估了在零样本设置中对其进行训练的 Mask R-CNN 模型。当在两个独立的真实数据集（包括新收集的基准）上进行测试时，我们的方法实现了最先进的分割性能（M18K 上的 F1 = 0.859），尽管仅使用合成训练数据。尽管该方法在双孢蘑菇上得到了验证，但所提出的管道可以很容易地适应其他蘑菇物种或其他农业领域，例如水果和叶子检测。</li>
</ul>

<h3>Title: Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance</h3>
<ul>
<li><strong>Authors: </strong>Ruihang Chu, Yefei He, Zhekai Chen, Shiwei Zhang, Xiaogang Xu, Bin Xia, Dingdong Wang, Hongwei Yi, Xihui Liu, Hengshuang Zhao, Yu Liu, Yingya Zhang, Yujiu Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08765">https://arxiv.org/abs/2512.08765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08765">https://arxiv.org/pdf/2512.08765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08765]] Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance(https://arxiv.org/abs/2512.08765)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.</li>
<li><strong>摘要：</strong>我们推出了 Wan-Move，这是一个简单且可扩展的框架，可为视频生成模型带来运动控制。现有的运动可控方法通常存在控制粒度粗和可扩展性有限的问题，导致其输出不足以实际使用。我们通过实现精确和高质量的运动控制来缩小这一差距。我们的核心思想是直接使原始条件特征具有运动感知能力，以指导视频合成。为此，我们首先用密集点轨迹表示对象运动，从而允许对场景进行细粒度控制。然后，我们将这些轨迹投影到潜在空间中，并沿着每个轨迹传播第一帧的特征，生成一个对齐的时空特征图，告诉每个场景元素应该如何移动。该特征图作为更新的潜在条件，自然地集成到现成的图像到视频模型中，例如 Wan-I2V-14B，作为运动指导，无需任何架构更改。它消除了对辅助运动编码器的需求，并使微调基础模型易于扩展。用户研究表明，通过大规模训练，Wan-Move 可以生成 5 秒、480p 的视频，其运动可控性可与 Kling 1.5 Pro 的商业 Motion Brush 相媲美。为了支持综合评估，我们进一步设计了 MoveBench，这是一个经过严格策划的基准测试，具有多样化的内容类别和混合验证的注释。它的特点是更大的数据量、更长的视频时长和高质量的运动注释。 MoveBench 和公共数据集上的大量实验一致证明了 Wan-Move 卓越的运动质量。代码、模型和基准数据都是公开的。</li>
</ul>

<h3>Title: De novo generation of functional terpene synthases using TpsGPT</h3>
<ul>
<li><strong>Authors: </strong>Hamsini Ramanathan, Roman Bushuiev, Matouš Soldát, Jirí Kohout, Téo Hebra, Joshua David Smith, Josef Sivic, Tomáš Pluskal</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08772">https://arxiv.org/abs/2512.08772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08772">https://arxiv.org/pdf/2512.08772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08772]] De novo generation of functional terpene synthases using TpsGPT(https://arxiv.org/abs/2512.08772)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Terpene synthases (TPS) are a key family of enzymes responsible for generating the diverse terpene scaffolds that underpin many natural products, including front-line anticancer drugs such as Taxol. However, de novo TPS design through directed evolution is costly and slow. We introduce TpsGPT, a generative model for scalable TPS protein design, built by fine-tuning the protein language model ProtGPT2 on 79k TPS sequences mined from UniProt. TpsGPT generated de novo enzyme candidates in silico and we evaluated them using multiple validation metrics, including EnzymeExplorer classification, ESMFold structural confidence (pLDDT), sequence diversity, CLEAN classification, InterPro domain detection, and Foldseek structure alignment. From an initial pool of 28k generated sequences, we identified seven putative TPS enzymes that satisfied all validation criteria. Experimental validation confirmed TPS enzymatic activity in at least two of these sequences. Our results show that fine-tuning of a protein language model on a carefully curated, enzyme-class-specific dataset, combined with rigorous filtering, can enable the de novo generation of functional, evolutionarily distant enzymes.</li>
<li><strong>摘要：</strong>萜烯合酶 (TPS) 是一个重要的酶家族，负责生成多种萜烯支架，这些支架是许多天然产物的基础，包括紫杉醇等一线抗癌药物。然而，通过定向进化从头设计 TPS 成本高昂且缓慢。我们介绍了 TpsGPT，这是一种用于可扩展 TPS 蛋白质设计的生成模型，通过在从 UniProt 挖掘的 79k TPS 序列上微调蛋白质语言模型 ProtGPT2 来构建。 TpsGPT 在计算机上从头生成了候选酶，我们使用多种验证指标对其进行评估，包括 EnzymeExplorer 分类、ESMFold 结构置信度 (pLDDT)、序列多样性、CLEAN 分类、InterPro 域检测和 Foldseek 结构比对。从 28k 生成序列的初始池中，我们确定了七种满足所有验证标准的推定 TPS 酶。实验验证证实了其中至少两个序列中的 TPS 酶活性。我们的结果表明，在精心策划的特定酶类数据集上微调蛋白质语言模型，并结合严格的过滤，可以从头生成功能性的、进化上遥远的酶。</li>
</ul>

<h3>Title: Refining Visual Artifacts in Diffusion Models via Explainable AI-based Flaw Activation Maps</h3>
<ul>
<li><strong>Authors: </strong>Seoyeon Lee, Gwangyeol Yu, Chaewon Kim, Jonghyuk Park</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08774">https://arxiv.org/abs/2512.08774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08774">https://arxiv.org/pdf/2512.08774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08774]] Refining Visual Artifacts in Diffusion Models via Explainable AI-based Flaw Activation Maps(https://arxiv.org/abs/2512.08774)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved remarkable success in image synthesis. However, addressing artifacts and unrealistic regions remains a critical challenge. We propose self-refining diffusion, a novel framework that enhances image generation quality by detecting these flaws. The framework employs an explainable artificial intelligence (XAI)-based flaw highlighter to produce flaw activation maps (FAMs) that identify artifacts and unrealistic regions. These FAMs improve reconstruction quality by amplifying noise in flawed regions during the forward process and by focusing on these regions during the reverse process. The proposed approach achieves up to a 27.3% improvement in Fréchet inception distance across various diffusion-based models, demonstrating consistently strong performance on diverse datasets. It also shows robust effectiveness across different tasks, including image generation, text-to-image generation, and inpainting. These results demonstrate that explainable AI techniques can extend beyond interpretability to actively contribute to image refinement. The proposed framework offers a versatile and effective approach applicable to various diffusion models and tasks, significantly advancing the field of image synthesis.</li>
<li><strong>摘要：</strong>扩散模型在图像合成方面取得了显着的成功。然而，解决伪影和不切实际的区域仍然是一个严峻的挑战。我们提出了自我改进扩散，这是一种新颖的框架，可以通过检测这些缺陷来提高图像生成质量。该框架采用基于可解释人工智能 (XAI) 的缺陷荧光笔来生成缺陷激活图 (FAM)，以识别伪影和不切实际的区域。这些 FAM 通过在正向过程中放大有缺陷区域中的噪声并在反向过程中关注这些区域来提高重建质量。所提出的方法在各种基于扩散的模型中将 Fréchet 起始距离提高了 27.3%，在不同的数据集上表现出了一致的强大性能。它还在不同的任务中显示出强大的有效性，包括图像生成、文本到图像生成和修复。这些结果表明，可解释的人工智能技术可以超越可解释性，积极促进图像细化。所提出的框架提供了一种适用于各种扩散模型和任务的通用且有效的方法，显着推进了图像合成领域。</li>
</ul>

<h3>Title: LoFA: Learning to Predict Personalized Priors for Fast Adaptation of Visual Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Yiming Hao, Mutian Xu, Chongjie Ye, Jie Qin, Shunlin Lu, Yipeng Qin, Xiaoguang Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08785">https://arxiv.org/abs/2512.08785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08785">https://arxiv.org/pdf/2512.08785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08785]] LoFA: Learning to Predict Personalized Priors for Fast Adaptation of Visual Generative Models(https://arxiv.org/abs/2512.08785)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Personalizing visual generative models to meet specific user needs has gained increasing attention, yet current methods like Low-Rank Adaptation (LoRA) remain impractical due to their demand for task-specific data and lengthy optimization. While a few hypernetwork-based approaches attempt to predict adaptation weights directly, they struggle to map fine-grained user prompts to complex LoRA distributions, limiting their practical applicability. To bridge this gap, we propose LoFA, a general framework that efficiently predicts personalized priors for fast model adaptation. We first identify a key property of LoRA: structured distribution patterns emerge in the relative changes between LoRA and base model parameters. Building on this, we design a two-stage hypernetwork: first predicting relative distribution patterns that capture key adaptation regions, then using these to guide final LoRA weight prediction. Extensive experiments demonstrate that our method consistently predicts high-quality personalized priors within seconds, across multiple tasks and user prompts, even outperforming conventional LoRA that requires hours of processing. Project page: this https URL.</li>
<li><strong>摘要：</strong>个性化视觉生成模型以满足特定用户需求已引起越来越多的关注，但由于需要特定于任务的数据和冗长的优化，当前的方法（例如低秩适应（LoRA））仍然不切实际。虽然一些基于超网络的方法尝试直接预测适应权重，但它们很难将细粒度的用户提示映射到复杂的 LoRA 分布，从而限制了它们的实际适用性。为了弥补这一差距，我们提出了 LoFA，这是一种通用框架，可以有效预测个性化先验以实现快速模型适应。我们首先确定 LoRA 的一个关键属性：结构化分布模式出现在 LoRA 和基础模型参数之间的相对变化中。在此基础上，我们设计了一个两阶段的超网络：首先预测捕获关键适应区域的相对分布模式，然后使用它们来指导最终的 LoRA 权重预测。大量实验表明，我们的方法可以在几秒钟内一致地预测高质量的个性化先验，跨多个任务和用户提示，甚至优于需要数小时处理的传统 LoRA。项目页面：此 https URL。</li>
</ul>

<h3>Title: Generation is Required for Data-Efficient Perception</h3>
<ul>
<li><strong>Authors: </strong>Jack Brady, Bernhard Schölkopf, Thomas Kipf, Simon Buchholz, Wieland Brendel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08854">https://arxiv.org/abs/2512.08854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08854">https://arxiv.org/pdf/2512.08854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08854]] Generation is Required for Data-Efficient Perception(https://arxiv.org/abs/2512.08854)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>It has been hypothesized that human-level visual perception requires a generative approach in which internal representations result from inverting a decoder. Yet today's most successful vision models are non-generative, relying on an encoder that maps images to representations without decoder inversion. This raises the question of whether generation is, in fact, necessary for machines to achieve human-level visual perception. To address this, we study whether generative and non-generative methods can achieve compositional generalization, a hallmark of human perception. Under a compositional data generating process, we formalize the inductive biases required to guarantee compositional generalization in decoder-based (generative) and encoder-based (non-generative) methods. We then show theoretically that enforcing these inductive biases on encoders is generally infeasible using regularization or architectural constraints. In contrast, for generative methods, the inductive biases can be enforced straightforwardly, thereby enabling compositional generalization by constraining a decoder and inverting it. We highlight how this inversion can be performed efficiently, either online through gradient-based search or offline through generative replay. We examine the empirical implications of our theory by training a range of generative and non-generative methods on photorealistic image datasets. We find that, without the necessary inductive biases, non-generative methods often fail to generalize compositionally and require large-scale pretraining or added supervision to improve generalization. By comparison, generative methods yield significant improvements in compositional generalization, without requiring additional data, by leveraging suitable inductive biases on a decoder along with search and replay.</li>
<li><strong>摘要：</strong>有人假设，人类水平的视觉感知需要一种生成方法，其中内部表示是通过反转解码器产生的。然而，当今最成功的视觉模型是非生成式的，依赖于将图像映射到表示的编码器，而无需解码器反转。这就提出了一个问题：机器要实现人类水平的视觉感知，生成实际上是否是必要的。为了解决这个问题，我们研究生成和非生成方法是否可以实现组合泛化，这是人类感知的标志。在组合数据生成过程中，我们形式化了保证基于解码器（生成）和基于编码器（非生成）方法中的组合泛化所需的归纳偏差。然后，我们从理论上证明，使用正则化或架构约束在编码器上强制执行这些归纳偏差通常是不可行的。相反，对于生成方法，可以直接强制执行归纳偏差，从而通过约束解码器并将其反转来实现组合泛化。我们强调如何有效地执行这种反演，无论是通过基于梯度的搜索在线还是通过生成重放离线。我们通过在真实感图像数据集上训练一系列生成和非生成方法来检验我们理论的实证意义。我们发现，如果没有必要的归纳偏差，非生成方法通常无法进行组合泛化，并且需要大规模预训练或增加监督来提高泛化能力。相比之下，生成方法通过利用解码器上适当的归纳偏差以及搜索和重放，在不需要额外数据的情况下，在组合泛化方面产生了显着的改进。</li>
</ul>

<h3>Title: Refining Diffusion Models for Motion Synthesis with an Acceleration Loss to Generate Realistic IMU Data</h3>
<ul>
<li><strong>Authors: </strong>Lars Ole Häusler, Lena Uhlenberg, Göran Köber, Diyora Salimova, Oliver Amft</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08859">https://arxiv.org/abs/2512.08859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08859">https://arxiv.org/pdf/2512.08859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08859]] Refining Diffusion Models for Motion Synthesis with an Acceleration Loss to Generate Realistic IMU Data(https://arxiv.org/abs/2512.08859)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We propose a text-to-IMU (inertial measurement unit) motion-synthesis framework to obtain realistic IMU data by fine-tuning a pretrained diffusion model with an acceleration-based second-order loss (L_acc). L_acc enforces consistency in the discrete second-order temporal differences of the generated motion, thereby aligning the diffusion prior with IMU-specific acceleration patterns. We integrate L_acc into the training objective of an existing diffusion model, finetune the model to obtain an IMU-specific motion prior, and evaluate the model with an existing text-to-IMU framework that comprises surface modelling and virtual sensor simulation. We analysed acceleration signal fidelity and differences between synthetic motion representation and actual IMU recordings. As a downstream application, we evaluated Human Activity Recognition (HAR) and compared the classification performance using data of our method with the earlier diffusion model and two additional diffusion model baselines. When we augmented the earlier diffusion model objective with L_acc and continued training, L_acc decreased by 12.7% relative to the original model. The improvements were considerably larger in high-dynamic activities (i.e., running, jumping) compared to low-dynamic activities~(i.e., sitting, standing). In a low-dimensional embedding, the synthetic IMU data produced by our refined model shifts closer to the distribution of real IMU recordings. HAR classification trained exclusively on our refined synthetic IMU data improved performance by 8.7% compared to the earlier diffusion model and by 7.6% over the best-performing comparison diffusion model. We conclude that acceleration-aware diffusion refinement provides an effective approach to align motion generation and IMU synthesis and highlights how flexible deep learning pipelines are for specialising generic text-to-motion priors to sensor-specific tasks.</li>
<li><strong>摘要：</strong>我们提出了一种文本到 IMU（惯性测量单元）运动合成框架，通过使用基于加速度的二阶损失 (L_acc) 微调预训练的扩散模型来获得真实的 IMU 数据。 L_acc 强制生成运动的离散二阶时间差异的一致性，从而将扩散先验与 IMU 特定的加速度模式对齐。我们将 L_acc 集成到现有扩散模型的训练目标中，微调模型以获得 IMU 特定运动先验，并使用现有的文本到 IMU 框架（包括表面建模和虚拟传感器模拟）评估模型。我们分析了加速度信号保真度以及合成运动表示与实际 IMU 记录之间的差异。作为下游应用程序，我们评估了人类活动识别（HAR），并将使用我们的方法的数据与早期扩散模型和两个附加扩散模型基线进行的分类性能进行了比较。当我们用 L_acc 增强早期扩散模型目标并继续训练时，L_acc 相对于原始模型下降了 12.7%。与低动态活动（即坐、站）相比，高动态活动（即跑步、跳跃）的改善要大得多。在低维嵌入中，我们的改进模型生成的合成 IMU 数据更接近真实 IMU 记录的分布。专门针对我们精炼的合成 IMU 数据进行训练的 HAR 分类与早期的扩散模型相比，性能提高了 8.7%，与性能最佳的比较扩散模型相比，性能提高了 7.6%。我们得出的结论是，加速感知扩散细化提供了一种有效的方法来协调运动生成和 IMU 合成，并强调了深度学习管道在特定于传感器的任务之前专门处理通用文本到运动的灵活性。</li>
</ul>

<h3>Title: Differentially Private Synthetic Data Generation Using Context-Aware GANs</h3>
<ul>
<li><strong>Authors: </strong>Anantaa Kotal, Anupam Joshi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08869">https://arxiv.org/abs/2512.08869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08869">https://arxiv.org/pdf/2512.08869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08869]] Differentially Private Synthetic Data Generation Using Context-Aware GANs(https://arxiv.org/abs/2512.08869)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The widespread use of big data across sectors has raised major privacy concerns, especially when sensitive information is shared or analyzed. Regulations such as GDPR and HIPAA impose strict controls on data handling, making it difficult to balance the need for insights with privacy requirements. Synthetic data offers a promising solution by creating artificial datasets that reflect real patterns without exposing sensitive information. However, traditional synthetic data methods often fail to capture complex, implicit rules that link different elements of the data and are essential in domains like healthcare. They may reproduce explicit patterns but overlook domain-specific constraints that are not directly stated yet crucial for realism and utility. For example, prescription guidelines that restrict certain medications for specific conditions or prevent harmful drug interactions may not appear explicitly in the original data. Synthetic data generated without these implicit rules can lead to medically inappropriate or unrealistic profiles. To address this gap, we propose ContextGAN, a Context-Aware Differentially Private Generative Adversarial Network that integrates domain-specific rules through a constraint matrix encoding both explicit and implicit knowledge. The constraint-aware discriminator evaluates synthetic data against these rules to ensure adherence to domain constraints, while differential privacy protects sensitive details from the original data. We validate ContextGAN across healthcare, security, and finance, showing that it produces high-quality synthetic data that respects domain rules and preserves privacy. Our results demonstrate that ContextGAN improves realism and utility by enforcing domain constraints, making it suitable for applications that require compliance with both explicit patterns and implicit rules under strict privacy guarantees.</li>
<li><strong>摘要：</strong>大数据在各个领域的广泛使用引发了重大的隐私问题，特别是在共享或分析敏感信息时。 GDPR 和 HIPAA 等法规对数据处理施加严格控制，因此很难平衡洞察需求与隐私要求。合成数据通过创建反映真实模式而不暴露敏感信息的人工数据集，提供了一种有前景的解决方案。然而，传统的合成数据方法通常无法捕获链接不同数据元素的复杂隐式规则，而这些规则在医疗保健等领域至关重要。他们可能会重现明确的模式，但忽略了特定领域的约束，这些约束没有直接说明，但对于现实性和实用性至关重要。例如，针对特定情况限制某些药物或防止有害药物相互作用的处方指南可能不会明确出现在原始数据中。如果没有这些隐含规则生成的合成数据可能会导致医学上不合适或不切实际的概况。为了解决这一差距，我们提出了 ContextGAN，一种上下文感知的差分隐私生成对抗网络，它通过编码显式和隐式知识的约束矩阵集成特定于领域的规则。约束感知鉴别器根据这些规则评估合成数据，以确保遵守域约束，而差异隐私则保护原始数据的敏感细节。我们在医疗保健、安全和金融领域验证了 ContextGAN，结果表明它可以生成尊重领域规则并保护隐私的高质量合成数据。我们的结果表明，ContextGAN 通过实施领域约束来提高真实性和实用性，使其适合需要在严格隐私保证下遵守显式模式和隐式规则的应用程序。</li>
</ul>

<h3>Title: When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Joshua Ward, Bochao Gu, Chi-Hua Wang, Guang Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08875">https://arxiv.org/abs/2512.08875</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08875">https://arxiv.org/pdf/2512.08875</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08875]] When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation(https://arxiv.org/abs/2512.08875)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have recently demonstrated remarkable performance in generating high-quality tabular synthetic data. In practice, two primary approaches have emerged for adapting LLMs to tabular data generation: (i) fine-tuning smaller models directly on tabular datasets, and (ii) prompting larger models with examples provided in context. In this work, we show that popular implementations from both regimes exhibit a tendency to compromise privacy by reproducing memorized patterns of numeric digits from their training data. To systematically analyze this risk, we introduce a simple No-box Membership Inference Attack (MIA) called LevAtt that assumes adversarial access to only the generated synthetic data and targets the string sequences of numeric digits in synthetic observations. Using this approach, our attack exposes substantial privacy leakage across a wide range of models and datasets, and in some cases, is even a perfect membership classifier on state-of-the-art models. Our findings highlight a unique privacy vulnerability of LLM-based synthetic data generation and the need for effective defenses. To this end, we propose two methods, including a novel sampling strategy that strategically perturbs digits during generation. Our evaluation demonstrates that this approach can defeat these attacks with minimal loss of fidelity and utility of the synthetic data.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 最近在生成高质量表格合成数据方面表现出了卓越的性能。在实践中，出现了两种使法学硕士适应表格数据生成的主要方法：（i）直接在表格数据集上微调较小的模型，以及（ii）通过上下文中提供的示例提示更大的模型。在这项工作中，我们表明，这两种制度的流行实现都表现出通过从训练数据中复制记忆的数字模式来损害隐私的倾向。为了系统地分析这种风险，我们引入了一种称为 LevAtt 的简单的开箱成员推理攻击 (MIA)，它假设对抗性访问仅生成的合成数据，并针对合成观察中的数字字符串序列。使用这种方法，我们的攻击暴露了各种模型和数据集的大量隐私泄露，在某些情况下，甚至是最先进模型上的完美成员分类器。我们的研究结果强调了基于法学硕士的合成数据生成的独特隐私漏洞以及有效防御的必要性。为此，我们提出了两种方法，包括一种新颖的采样策略，可以在生成过程中策略性地扰乱数字。我们的评估表明，这种方法可以击败这些攻击，同时合成数据的保真度和实用性损失最小。</li>
</ul>

<h3>Title: Open Polymer Challenge: Post-Competition Report</h3>
<ul>
<li><strong>Authors: </strong>Gang Liu, Sobin Alosious, Subhamoy Mahajan, Eric Inae, Yihan Zhu, Yuhan Liu, Renzheng Zhang, Jiaxin Xu, Addison Howard, Ying Li, Tengfei Luo, Meng Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08896">https://arxiv.org/abs/2512.08896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08896">https://arxiv.org/pdf/2512.08896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08896]] Open Polymer Challenge: Post-Competition Report(https://arxiv.org/abs/2512.08896)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Machine learning (ML) offers a powerful path toward discovering sustainable polymer materials, but progress has been limited by the lack of large, high-quality, and openly accessible polymer datasets. The Open Polymer Challenge (OPC) addresses this gap by releasing the first community-developed benchmark for polymer informatics, featuring a dataset with 10K polymers and 5 properties: thermal conductivity, radius of gyration, density, fractional free volume, and glass transition temperature. The challenge centers on multi-task polymer property prediction, a core step in virtual screening pipelines for materials discovery. Participants developed models under realistic constraints that include small data, label imbalance, and heterogeneous simulation sources, using techniques such as feature-based augmentation, transfer learning, self-supervised pretraining, and targeted ensemble strategies. The competition also revealed important lessons about data preparation, distribution shifts, and cross-group simulation consistency, informing best practices for future large-scale polymer datasets. The resulting models, analysis, and released data create a new foundation for molecular AI in polymer science and are expected to accelerate the development of sustainable and energy-efficient materials. Along with the competition, we release the test dataset at this https URL. We also release the data generation pipeline at this https URL, which simulates more than 25 properties, including thermal conductivity, radius of gyration, and density.</li>
<li><strong>摘要：</strong>机器学习 (ML) 为发现可持续聚合物材料提供了一条强大的途径，但由于缺乏大型、高质量且可公开访问的聚合物数据集，进展受到限制。开放聚合物挑战赛 (OPC) 通过发布第一个社区开发的聚合物信息学基准来解决这一差距，该基准具有包含 10K 聚合物和 5 个属性的数据集：导热率、回转半径、密度、自由体积分数和玻璃化转变温度。挑战的核心是多任务聚合物特性预测，这是材料发现虚拟筛选流程的核心步骤。参与者使用基于特征的增强、迁移学习、自监督预训练和目标集成策略等技术，在现实约束下开发模型，包括小数据、标签不平衡和异构模拟源。比赛还揭示了有关数据准备、分布变化和跨组模拟一致性的重要经验教训，为未来大规模聚合物数据集的最佳实践提供了信息。由此产生的模型、分析和发布的数据为聚合物科学中的分子人工智能奠定了新的基础，预计将加速可持续和节能材料的开发。随着比赛的进行，我们在此 https URL 上发布了测试数据集。我们还在这个 https URL 上发布了数据生成管道，它模拟了超过 25 个属性，包括导热率、回转半径和密度。</li>
</ul>

<h3>Title: UniLayDiff: A Unified Diffusion Transformer for Content-Aware Layout Generation</h3>
<ul>
<li><strong>Authors: </strong>Zeyang Liu, Le Wang, Sanping Zhou, Yuxuan Wu, Xiaolong Sun, Gang Hua, Haoxiang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08897">https://arxiv.org/abs/2512.08897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08897">https://arxiv.org/pdf/2512.08897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08897]] UniLayDiff: A Unified Diffusion Transformer for Content-Aware Layout Generation(https://arxiv.org/abs/2512.08897)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Content-aware layout generation is a critical task in graphic design automation, focused on creating visually appealing arrangements of elements that seamlessly blend with a given background image. The variety of real-world applications makes it highly challenging to develop a single model capable of unifying the diverse range of input-constrained generation sub-tasks, such as those conditioned by element types, sizes, or their relationships. Current methods either address only a subset of these tasks or necessitate separate model parameters for different conditions, failing to offer a truly unified solution. In this paper, we propose UniLayDiff: a Unified Diffusion Transformer, that for the first time, addresses various content-aware layout generation tasks with a single, end-to-end trainable model. Specifically, we treat layout constraints as a distinct modality and employ Multi-Modal Diffusion Transformer framework to capture the complex interplay between the background image, layout elements, and diverse constraints. Moreover, we integrate relation constraints through fine-tuning the model with LoRA after pretraining the model on other tasks. Such a schema not only achieves unified conditional generation but also enhances overall layout quality. Extensive experiments demonstrate that UniLayDiff achieves state-of-the-art performance across from unconditional to various conditional generation tasks and, to the best of our knowledge, is the first model to unify the full range of content-aware layout generation tasks.</li>
<li><strong>摘要：</strong>内容感知布局生成是图形设计自动化中的一项关键任务，专注于创建具有视觉吸引力的元素排列，与给定的背景图像无缝融合。现实世界应用的多样性使得开发能够统一各种输入约束生成子任务（例如受元素类型、大小或其关系限制的子任务）的单一模型变得极具挑战性。当前的方法要么仅解决这些任务的子集，要么需要针对不同条件使用单独的模型参数，无法提供真正统一的解决方案。在本文中，我们提出了 UniLayDiff：统一扩散变压器，它首次使用单个端到端可训练模型来解决各种内容感知布局生成任务。具体来说，我们将布局约束视为一种独特的模态，并采用多模态扩散变压器框架来捕获背景图像、布局元素和不同约束之间的复杂相互作用。此外，在其他任务上对模型进行预训练后，我们通过 LoRA 微调模型来集成关系约束。这样的模式不仅实现了统一的条件生成，而且提高了整体布局质量。大量实验表明，UniLayDiff 在从无条件到各种条件生成任务中实现了最先进的性能，并且据我们所知，它是第一个统一全方位内容感知布局生成任务的模型。</li>
</ul>

<h3>Title: Self-Evolving 3D Scene Generation from a Single Image</h3>
<ul>
<li><strong>Authors: </strong>Kaizhi Zheng, Yue Fan, Jing Gu, Zishuo Xu, Xuehai He, Xin Eric Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08905">https://arxiv.org/abs/2512.08905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08905">https://arxiv.org/pdf/2512.08905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08905]] Self-Evolving 3D Scene Generation from a Single Image(https://arxiv.org/abs/2512.08905)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating high-quality, textured 3D scenes from a single image remains a fundamental challenge in vision and graphics. Recent image-to-3D generators recover reasonable geometry from single views, but their object-centric training limits generalization to complex, large-scale scenes with faithful structure and texture. We present EvoScene, a self-evolving, training-free framework that progressively reconstructs complete 3D scenes from single images. The key idea is combining the complementary strengths of existing models: geometric reasoning from 3D generation models and visual knowledge from video generation models. Through three iterative stages--Spatial Prior Initialization, Visual-guided 3D Scene Mesh Generation, and Spatial-guided Novel View Generation--EvoScene alternates between 2D and 3D domains, gradually improving both structure and appearance. Experiments on diverse scenes demonstrate that EvoScene achieves superior geometric stability, view-consistent textures, and unseen-region completion compared to strong baselines, producing ready-to-use 3D meshes for practical applications.</li>
<li><strong>摘要：</strong>从单个图像生成高质量、有纹理的 3D 场景仍然是视觉和图形领域的一项基本挑战。最近的图像到 3D 生成器可以从单一视图中恢复合理的几何形状，但它们以对象为中心的训练限制了对具有忠实结构和纹理的复杂、大规模场景的泛化。我们推出了 EvoScene，这是一个自我进化、免训练的框架，可以从单个图像逐步重建完整的 3D 场景。关键思想是结合现有模型的互补优势：来自 3D 生成模型的几何推理和来自视频生成模型的视觉知识。通过三个迭代阶段——空间优先初始化、视觉引导的 3D 场景网格生成和空间引导的小说视图生成——EvoScene 在 2D 和 3D 域之间交替，逐渐改善结构和外观。对不同场景的实验表明，与强基线相比，EvoScene 实现了卓越的几何稳定性、视图一致的纹理和看不见的区域完成，为实际应用生成了即用型 3D 网格。</li>
</ul>

<h3>Title: Unified Diffusion Transformer for High-fidelity Text-Aware Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Jin Hyeon Kim, Paul Hyunbin Cho, Claire Kim, Jaewon Min, Jaeeun Lee, Jihye Park, Yeji Choi, Seungryong Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08922">https://arxiv.org/abs/2512.08922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08922">https://arxiv.org/pdf/2512.08922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08922]] Unified Diffusion Transformer for High-fidelity Text-Aware Image Restoration(https://arxiv.org/abs/2512.08922)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Text-Aware Image Restoration (TAIR) aims to recover high- quality images from low-quality inputs containing degraded textual content. While diffusion models provide strong gen- erative priors for general image restoration, they often pro- duce text hallucinations in text-centric tasks due to the ab- sence of explicit linguistic knowledge. To address this, we propose UniT, a unified text restoration framework that in- tegrates a Diffusion Transformer (DiT), a Vision-Language Model (VLM), and a Text Spotting Module (TSM) in an it- erative fashion for high-fidelity text restoration. In UniT, the VLM extracts textual content from degraded images to provide explicit textual guidance. Simultaneously, the TSM, trained on diffusion features, generates intermedi- ate OCR predictions at each denoising step, enabling the VLM to iteratively refine its guidance during the denoising process. Finally, the DiT backbone, leveraging its strong representational power, exploit these cues to recover fine- grained textual content while effectively suppressing text hallucinations. Experiments on the SA-Text and Real-Text benchmarks demonstrate that UniT faithfully reconstructs degraded text, substantially reduces hallucinations, and achieves state-of-the-art end-to-end F1-score performance in TAIR task.</li>
<li><strong>摘要：</strong>文本感知图像恢复（TAIR）旨在从包含降级文本内容的低质量输入中恢复高质量图像。虽然扩散模型为一般图像恢复提供了强大的生成先验，但由于缺乏明确的语言知识，它们经常在以文本为中心的任务中产生文本幻觉。为了解决这个问题，我们提出了UnitT，一个统一的文本恢复框架，它以迭代方式集成了扩散变换器（DiT）、视觉语言模型（VLM）和文本识别模块（TSM），以实现高保真文本恢复。在 UnitT 中，VLM 从降级图像中提取文本内容以提供明确的文本指导。同时，经过扩散特征训练的 TSM 在每个去噪步骤中生成中间 OCR 预测，使 VLM 能够在去噪过程中迭代地完善其指导。最后，DiT 主干网利用其强大的表征能力，利用这些线索来恢复细粒度的文本内容，同时有效地抑制文本幻觉。 SA-Text 和 Real-Text 基准测试表明，UnitT 忠实地重建了退化的文本，大大减少了幻觉，并在 TAIR 任务中实现了最先进的端到端 F1 分数性能。</li>
</ul>

<h3>Title: Astra: General Interactive World Model with Autoregressive Denoising</h3>
<ul>
<li><strong>Authors: </strong>Yixuan Zhu, Jiaqi Feng, Wenzhao Zheng, Yuan Gao, Xin Tao, Pengfei Wan, Jie Zhou, Jiwen Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08931">https://arxiv.org/abs/2512.08931</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08931">https://arxiv.org/pdf/2512.08931</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08931]] Astra: General Interactive World Model with Autoregressive Denoising(https://arxiv.org/abs/2512.08931)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion transformers have empowered video generation models to generate high-quality video clips from texts or images. However, world models with the ability to predict long-horizon futures from past observations and actions remain underexplored, especially for general-purpose scenarios and various forms of actions. To bridge this gap, we introduce Astra, an interactive general world model that generates real-world futures for diverse scenarios (e.g., autonomous driving, robot grasping) with precise action interactions (e.g., camera motion, robot action). We propose an autoregressive denoising architecture and use temporal causal attention to aggregate past observations and support streaming outputs. We use a noise-augmented history memory to avoid over-reliance on past frames to balance responsiveness with temporal coherence. For precise action control, we introduce an action-aware adapter that directly injects action signals into the denoising process. We further develop a mixture of action experts that dynamically route heterogeneous action modalities, enhancing versatility across diverse real-world tasks such as exploration, manipulation, and camera control. Astra achieves interactive, consistent, and general long-term video prediction and supports various forms of interactions. Experiments across multiple datasets demonstrate the improvements of Astra in fidelity, long-range prediction, and action alignment over existing state-of-the-art world models.</li>
<li><strong>摘要：</strong>扩散变压器的最新进展使视频生成模型能够从文本或图像生成高质量的视频剪辑。然而，能够根据过去的观察和行动预测长期未来的世界模型仍未得到充分探索，特别是对于通用场景和各种形式的行动。为了弥补这一差距，我们引入了 Astra，这是一种交互式通用世界模型，它可以通过精确的动作交互（例如相机运动、机器人动作）为不同场景（例如自动驾驶、机器人抓取）生成现实世界的未来。我们提出了一种自回归去噪架构，并使用时间因果注意力来聚合过去的观察结果并支持流输出。我们使用噪声增强的历史记忆来避免过度依赖过去的帧来平衡响应性与时间连贯性。为了精确的动作控制，我们引入了一个动作感知适配器，它直接将动作信号注入到去噪过程中。我们进一步开发了一系列动作专家，可以动态路由异构动作模式，增强探索、操纵和摄像机控制等不同现实世界任务的多功能性。 Astra实现了交互、一致、通用的长期视频预测，支持多种形式的交互。跨多个数据集的实验证明了 Astra 在保真度、远程预测和动作对齐方面比现有最先进的世界模型有所改进。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
