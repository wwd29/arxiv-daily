<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-08-22</h1>
<h3>Title: Aura-CAPTCHA: A Reinforcement Learning and GAN-Enhanced Multi-Modal CAPTCHA System</h3>
<ul>
<li><strong>Authors: </strong>Joydeep Chandra, Prabal Manhas, Ramanjot Kaur, Rashi Sahay</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14976">https://arxiv.org/abs/2508.14976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14976">https://arxiv.org/pdf/2508.14976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14976]] Aura-CAPTCHA: A Reinforcement Learning and GAN-Enhanced Multi-Modal CAPTCHA System(https://arxiv.org/abs/2508.14976)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Aura-CAPTCHA was developed as a multi-modal CAPTCHA system to address vulnerabilities in traditional methods that are increasingly bypassed by AI technologies, such as Optical Character Recognition (OCR) and adversarial image processing. The design integrated Generative Adversarial Networks (GANs) for generating dynamic image challenges, Reinforcement Learning (RL) for adaptive difficulty tuning, and Large Language Models (LLMs) for creating text and audio prompts. Visual challenges included 3x3 grid selections with at least three correct images, while audio challenges combined randomized numbers and words into a single task. RL adjusted difficulty based on incorrect attempts, response time, and suspicious user behavior. Evaluations on real-world traffic demonstrated a 92% human success rate and a 10% bot bypass rate, significantly outperforming existing CAPTCHA systems. The system provided a robust and scalable approach for securing online applications while remaining accessible to users, addressing gaps highlighted in previous research.</li>
<li><strong>摘要：</strong>AURA-CAPTCHA是作为多模式验证码系统开发的，用于解决传统方法中越来越多地被AI技术绕过的漏洞，例如光学特征识别（OCR）和对抗图像处理。该设计集成了生成对抗网络（GAN），以生成动态图像挑战，增强型学习（RL），以进行自适应难度调整以及大型语言模型（LLMS），以创建文本和音频提示。视觉挑战包括具有至少三个正确图像的3x3网格选择，而音频挑战将随机数和单词组合为单个任务。 RL根据错误的尝试，响应时间和可疑用户行为调整了难度。对现实世界流量的评估表明，人类的成功率为92％，Bot旁路率为10％，大大优于现有的验证码系统。该系统为确保在线应用程序的同时可以访问用户可访问在线应用程序，并解决了以前的研究中突出显示的差距。</li>
</ul>

<h3>Title: Generative Neural Operators of Log-Complexity Can Simultaneously Solve Infinitely Many Convex Programs</h3>
<ul>
<li><strong>Authors: </strong>Anastasis Kratsios, Ariel Neufeld, Philipp Schmocker</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA, math.OC, q-fin.CP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14995">https://arxiv.org/abs/2508.14995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14995">https://arxiv.org/pdf/2508.14995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14995]] Generative Neural Operators of Log-Complexity Can Simultaneously Solve Infinitely Many Convex Programs(https://arxiv.org/abs/2508.14995)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Neural operators (NOs) are a class of deep learning models designed to simultaneously solve infinitely many related problems by casting them into an infinite-dimensional space, whereon these NOs operate. A significant gap remains between theory and practice: worst-case parameter bounds from universal approximation theorems suggest that NOs may require an unrealistically large number of parameters to solve most operator learning problems, which stands in direct opposition to a slew of experimental evidence. This paper closes that gap for a specific class of {NOs}, generative {equilibrium operators} (GEOs), using (realistic) finite-dimensional deep equilibrium layers, when solving families of convex optimization problems over a separable Hilbert space $X$. Here, the inputs are smooth, convex loss functions on $X$, and outputs are the associated (approximate) solutions to the optimization problem defined by each input loss. We show that when the input losses lie in suitable infinite-dimensional compact sets, our GEO can uniformly approximate the corresponding solutions to arbitrary precision, with rank, depth, and width growing only logarithmically in the reciprocal of the approximation error. We then validate both our theoretical results and the trainability of GEOs on three applications: (1) nonlinear PDEs, (2) stochastic optimal control problems, and (3) hedging problems in mathematical finance under liquidity constraints.</li>
<li><strong>摘要：</strong>神经操作员（NOS）是一类深度学习模型，旨在通过将它们投入到无限的二维空间中，同时解决许多相关问题，其中这些NOS可以运行。理论和实践之间存在一个显着的差距：通用近似定理的最坏情况参数界限表明，NOS可能需要不现实的大量参数来解决大多数操作员学习问题，这直接反对大量实验证据。本文闭合了特定类别的{nos}，生成{equilibrium operator}（GEO）的差距，使用（现实）有限维的深度平衡层，当解决可分离的希尔伯特空间的凸优化问题的家族时。在这里，输入是平滑的，在$ x $上的凸损失函数，输出是每个输入损耗定义的优化问题的相关（近似）解决方案。我们表明，当输入损失位于合适的无限二二型紧凑型集中时，我们的地理可以统一地近似于任意精度的相应解决方案，而在近似误差的互惠中，等级，深度和宽度仅在对数中增加。然后，我们验证了我们在三种应用上的理论结果和GEO的训练性：（1）非线性PDE，（2）随机最佳控制问题，以及（3）在流动性约束下，数学金融中的对冲问题。</li>
</ul>

<h3>Title: TAIGen: Training-Free Adversarial Image Generation via Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Susim Roy, Anubhooti Jain, Mayank Vatsa, Richa Singh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.15020">https://arxiv.org/abs/2508.15020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.15020">https://arxiv.org/pdf/2508.15020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.15020]] TAIGen: Training-Free Adversarial Image Generation via Diffusion Models(https://arxiv.org/abs/2508.15020)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Adversarial attacks from generative models often produce low-quality images and require substantial computational resources. Diffusion models, though capable of high-quality generation, typically need hundreds of sampling steps for adversarial generation. This paper introduces TAIGen, a training-free black-box method for efficient adversarial image generation. TAIGen produces adversarial examples using only 3-20 sampling steps from unconditional diffusion models. Our key finding is that perturbations injected during the mixing step interval achieve comparable attack effectiveness without processing all timesteps. We develop a selective RGB channel strategy that applies attention maps to the red channel while using GradCAM-guided perturbations on green and blue channels. This design preserves image structure while maximizing misclassification in target models. TAIGen maintains visual quality with PSNR above 30 dB across all tested datasets. On ImageNet with VGGNet as source, TAIGen achieves 70.6% success against ResNet, 80.8% against MNASNet, and 97.8% against ShuffleNet. The method generates adversarial examples 10x faster than existing diffusion-based attacks. Our method achieves the lowest robust accuracy, indicating it is the most impactful attack as the defense mechanism is least successful in purifying the images generated by TAIGen.</li>
<li><strong>摘要：</strong>生成模型的对抗攻击通常会产生低质量的图像，并需要大量的计算资源。扩散模型虽然能够具有高质量的生成，但通常需要数百个采样步骤才能产生对抗性。本文介绍了Taigen，Taigen是一种无训练的黑盒方法，用于有效的对抗图像产生。 Taigen仅使用来自无条件扩散模型的3-20个采样步骤产生对抗示例。我们的主要发现是，在混合步骤间隔期间注入的扰动实现了可比的攻击效果，而无需处理所有时间段。我们开发了一种选择性的RGB通道策略，该策略将注意力图应用于红色通道，同时使用绿色和蓝色通道上的GradCam引导的扰动。该设计保留了图像结构，同时最大化目标模型中的错误分类。 Taigen在所有测试的数据集中以高于30 dB的PSNR保持视觉质量。在带有VGGNET作为来源的ImageNet上，Taigen在Resnet上取得了70.6％的成功，对MNASNET的80.8％，而Shufflenet取得了97.8％的成功。该方法生成的对抗示例比现有基于扩散的攻击快10倍。我们的方法达到了最低的鲁棒精度，这表明它是最有影响力的攻击，因为防御机制在净化Taigen产生的图像方面最不成功。</li>
</ul>

<h3>Title: Reversible Unfolding Network for Concealed Visual Perception with Generative Refinement</h3>
<ul>
<li><strong>Authors: </strong>Chunming He, Fengyang Xiao, Rihan Zhang, Chengyu Fang, Deng-Ping Fan, Sina Farsiu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.15027">https://arxiv.org/abs/2508.15027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.15027">https://arxiv.org/pdf/2508.15027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.15027]] Reversible Unfolding Network for Concealed Visual Perception with Generative Refinement(https://arxiv.org/abs/2508.15027)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generative</a></li>
<li><strong>Abstract: </strong>Existing methods for concealed visual perception (CVP) often leverage reversible strategies to decrease uncertainty, yet these are typically confined to the mask domain, leaving the potential of the RGB domain underexplored. To address this, we propose a reversible unfolding network with generative refinement, termed RUN++. Specifically, RUN++ first formulates the CVP task as a mathematical optimization problem and unfolds the iterative solution into a multi-stage deep network. This approach provides a principled way to apply reversible modeling across both mask and RGB domains while leveraging a diffusion model to resolve the resulting uncertainty. Each stage of the network integrates three purpose-driven modules: a Concealed Object Region Extraction (CORE) module applies reversible modeling to the mask domain to identify core object regions; a Context-Aware Region Enhancement (CARE) module extends this principle to the RGB domain to foster better foreground-background separation; and a Finetuning Iteration via Noise-based Enhancement (FINE) module provides a final refinement. The FINE module introduces a targeted Bernoulli diffusion model that refines only the uncertain regions of the segmentation mask, harnessing the generative power of diffusion for fine-detail restoration without the prohibitive computational cost of a full-image process. This unique synergy, where the unfolding network provides a strong uncertainty prior for the diffusion model, allows RUN++ to efficiently direct its focus toward ambiguous areas, significantly mitigating false positives and negatives. Furthermore, we introduce a new paradigm for building robust CVP systems that remain effective under real-world degradations and extend this concept into a broader bi-level optimization framework.</li>
<li><strong>摘要：</strong>隐藏视觉感知（CVP）的现有方法通常利用可逆策略来降低不确定性，但这些策略通常局限于掩模域，从而使RGB域的潜力毫无疑问。为了解决这个问题，我们提出了一个可逆的展开网络，具有生成精致，称为Run ++。具体而言，运行++首先将CVP任务作为数学优化问题制定，并将迭代解决方案展开为多阶段的深网。这种方法提供了一种原则性的方法，可以在掩模和RGB域上应用可逆建模，同时利用扩散模型来解决所得的不确定性。网络的每个阶段都集成了三个目的驱动的模块：隐藏的对象区域提取（CORE）模块将可逆建模应用于掩模域以识别核心对象区域；上下文感知的区域增强（CARE）模块将此原理扩展到RGB域，以促进更好的前景背景分离。通过基于噪声的增强（Fine）模块的填充迭代提供了最终的改进。 Fine模块引入了目标的Bernoulli扩散模型，该模型仅完善了分割面膜的不确定区域，从而利用了扩散的生成能力来进行细节恢复，而没有全图像过程的刺激性计算成本。这种独特的协同作用，即展开网络为扩散模型提供了强烈的不确定性，它使Run ++可以有效地将其专注于模棱两可的区域，从而大大减轻误报和负面影响。此外，我们引入了一种新的范式，用于构建强大的CVP系统，该系统在现实世界中的降级下保持有效，并将此概念扩展到更广泛的双层优化框架中。</li>
</ul>

<h3>Title: CurveFlow: Curvature-Guided Flow Matching for Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Yan Luo, Drake Du, Hao Huang, Yi Fang, Mengyu Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.15093">https://arxiv.org/abs/2508.15093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.15093">https://arxiv.org/pdf/2508.15093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.15093]] CurveFlow: Curvature-Guided Flow Matching for Image Generation(https://arxiv.org/abs/2508.15093)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Existing rectified flow models are based on linear trajectories between data and noise distributions. This linearity enforces zero curvature, which can inadvertently force the image generation process through low-probability regions of the data manifold. A key question remains underexplored: how does the curvature of these trajectories correlate with the semantic alignment between generated images and their corresponding captions, i.e., instructional compliance? To address this, we introduce CurveFlow, a novel flow matching framework designed to learn smooth, non-linear trajectories by directly incorporating curvature guidance into the flow path. Our method features a robust curvature regularization technique that penalizes abrupt changes in the trajectory's intrinsic this http URL experiments on MS COCO 2014 and 2017 demonstrate that CurveFlow achieves state-of-the-art performance in text-to-image generation, significantly outperforming both standard rectified flow variants and other non-linear baselines like Rectified Diffusion. The improvements are especially evident in semantic consistency metrics such as BLEU, METEOR, ROUGE, and CLAIR. This confirms that our curvature-aware modeling substantially enhances the model's ability to faithfully follow complex instructions while simultaneously maintaining high image quality. The code is made publicly available at this https URL.</li>
<li><strong>摘要：</strong>现有的整流流模型基于数据和噪声分布之间的线性轨迹。这种线性性强制执行零曲率，可以无意间通过数据歧管的低概率区域迫使图像生成过程。一个关键的问题仍然没有被解散：这些轨迹的曲率与生成的图像及其相应的字幕（即教学合规性）之间的语义一致性如何相关？为了解决这个问题，我们介绍了曲线流，这是一个新型的流量匹配框架，旨在通过将曲率引导直接纳入流道中，以学习平滑的非线性轨迹。我们的方法具有强大的曲率正则化技术，该技术惩罚了轨迹固有的突然变化。在2014年和2017年Coco女士上进行的HTTP URL实验表明，CurveFlow在文本到图像生成中实现了最先进的性能，显着超过了标准的循环流动变体和其他非线性基础和其他类似的基本基础和其他类似的基础。在Bleu，Meteor，Rouge和Clair等语义一致性指标中，这些改进尤为明显。这证实，我们的曲率感知建模实质上增强了模型忠实地遵循复杂说明的能力，同时保持高图像质量。该代码在此HTTPS URL上公开可用。</li>
</ul>

<h3>Title: Side Effects of Erasing Concepts from Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Shaswati Saha, Sourajit Saha, Manas Gaur, Tejas Gokhale</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.15124">https://arxiv.org/abs/2508.15124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.15124">https://arxiv.org/pdf/2508.15124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.15124]] Side Effects of Erasing Concepts from Diffusion Models(https://arxiv.org/abs/2508.15124)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Concerns about text-to-image (T2I) generative models infringing on privacy, copyright, and safety have led to the development of Concept Erasure Techniques (CETs). The goal of an effective CET is to prohibit the generation of undesired ``target'' concepts specified by the user, while preserving the ability to synthesize high-quality images of the remaining concepts. In this work, we demonstrate that CETs can be easily circumvented and present several side effects of concept erasure. For a comprehensive measurement of the robustness of CETs, we present Side Effect Evaluation (\see), an evaluation benchmark that consists of hierarchical and compositional prompts that describe objects and their attributes. This dataset and our automated evaluation pipeline quantify side effects of CETs across three aspects: impact on neighboring concepts, evasion of targets, and attribute leakage. Our experiments reveal that CETs can be circumvented by using superclass-subclass hierarchy and semantically similar prompts, such as compositional variants of the target. We show that CETs suffer from attribute leakage and counterintuitive phenomena of attention concentration or dispersal. We release our dataset, code, and evaluation tools to aid future work on robust concept erasure.</li>
<li><strong>摘要：</strong>对侵犯隐私，版权和安全性的文本形象（T2I）生成模型的担忧已导致概念擦除技术（CETS）的发展。有效的CET的目的是禁止用户指定的不需要的``目标''概念的生成，同时保留综合其余概念的高质量图像的能力。在这项工作中，我们证明了CET可以轻松绕过概念擦除的几种副作用。为了全面测量CET的鲁棒性，我们提出了副作用评估（\请参见），这是一个由层次和组成提示组成的评估基准，描述了对象及其属性。该数据集和我们的自动评估管道量化了CET在三个方面的副作用：对相邻概念，逃避目标和属性泄漏的影响。我们的实验表明，可以通过使用超类 - 串联层次结构和语义上相似的提示（例如目标的组成变体）来规避CET。我们表明，CET遭受了注意力集中或分散的属性泄漏和违反直觉现象。我们发布了我们的数据集，代码和评估工具，以帮助未来的关于Robust Concept擦除的工作。</li>
</ul>

<h3>Title: HiRQA: Hierarchical Ranking and Quality Alignment for Opinion-Unaware Image Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Vaishnav Ramesh, Haining Wang, Md Jahidul Islam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.15130">https://arxiv.org/abs/2508.15130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.15130">https://arxiv.org/pdf/2508.15130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.15130]] HiRQA: Hierarchical Ranking and Quality Alignment for Opinion-Unaware Image Quality Assessment(https://arxiv.org/abs/2508.15130)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Despite significant progress in no-reference image quality assessment (NR-IQA), dataset biases and reliance on subjective labels continue to hinder their generalization performance. We propose HiRQA, Hierarchical Ranking and Quality Alignment), a self-supervised, opinion-unaware framework that offers a hierarchical, quality-aware embedding through a combination of ranking and contrastive learning. Unlike prior approaches that depend on pristine references or auxiliary modalities at inference time, HiRQA predicts quality scores using only the input image. We introduce a novel higher-order ranking loss that supervises quality predictions through relational ordering across distortion pairs, along with an embedding distance loss that enforces consistency between feature distances and perceptual differences. A training-time contrastive alignment loss, guided by structured textual prompts, further enhances the learned representation. Trained only on synthetic distortions, HiRQA generalizes effectively to authentic degradations, as demonstrated through evaluation on various distortions such as lens flare, haze, motion blur, and low-light conditions. For real-time deployment, we introduce \textbf{HiRQA-S}, a lightweight variant with an inference time of only 3.5 ms per image. Extensive experiments across synthetic and authentic benchmarks validate HiRQA's state-of-the-art (SOTA) performance, strong generalization ability, and scalability.</li>
<li><strong>摘要：</strong>尽管无参考图像质量评估（NR-IQA）取得了重大进展，但数据集的偏见和对主观标签的依赖继续阻碍其概括性能。我们提出了HIRQA，层次排名和质量对齐），这是一个自我监管的，意见 - 统一的框架，通过排名和对比度学习的结合，可以提供层次，质量意识的嵌入。与先前依赖原始参考或推理时辅助模式的方法不同，HIRQA仅使用输入图像预测质量得分。我们介绍了一种新颖的高阶排名损失，该损失通过跨失真对的关系顺序来监督质量预测，以及嵌入距离损失，从而在特征距离和感知差异之间实现一致性。在结构化的文本提示的指导下，训练时间对比度损失进一步增强了学习的表示。 HIRQA仅对合成畸变进行训练，有效地将其概括为正宗的降解，如对各种扭曲（例如镜片耀斑，雾霾，运动模糊和弱光条件）的评估所证明的。对于实时部署，我们介绍了\ textbf {hirqa-s}，这是一个轻巧的变体，每个图像的推理时间仅为3.5 ms。跨合成和正宗基准的广泛实验验证了HIRQA的最新性能（SOTA）性能，强大的概括能力和可伸缩性。</li>
</ul>

<h3>Title: MeSS: City Mesh-Guided Outdoor Scene Generation with Cross-View Consistent Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Xuyang Chen, Zhijun Zhai, Kaixuan Zhou, Zengmao Wang, Jianan He, Dong Wang, Yanfeng Zhang, mingwei Sun, Rüdiger Westermann, Konrad Schindler, Liqiu Meng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.15169">https://arxiv.org/abs/2508.15169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.15169">https://arxiv.org/pdf/2508.15169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.15169]] MeSS: City Mesh-Guided Outdoor Scene Generation with Cross-View Consistent Diffusion(https://arxiv.org/abs/2508.15169)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Mesh models have become increasingly accessible for numerous cities; however, the lack of realistic textures restricts their application in virtual urban navigation and autonomous driving. To address this, this paper proposes MeSS (Meshbased Scene Synthesis) for generating high-quality, styleconsistent outdoor scenes with city mesh models serving as the geometric prior. While image and video diffusion models can leverage spatial layouts (such as depth maps or HD maps) as control conditions to generate street-level perspective views, they are not directly applicable to 3D scene generation. Video diffusion models excel at synthesizing consistent view sequences that depict scenes but often struggle to adhere to predefined camera paths or align accurately with rendered control videos. In contrast, image diffusion models, though unable to guarantee cross-view visual consistency, can produce more geometry-aligned results when combined with ControlNet. Building on this insight, our approach enhances image diffusion models by improving cross-view consistency. The pipeline comprises three key stages: first, we generate geometrically consistent sparse views using Cascaded Outpainting ControlNets; second, we propagate denser intermediate views via a component dubbed AGInpaint; and third, we globally eliminate visual inconsistencies (e.g., varying exposure) using the GCAlign module. Concurrently with generation, a 3D Gaussian Splatting (3DGS) scene is reconstructed by initializing Gaussian balls on the mesh surface. Our method outperforms existing approaches in both geometric alignment and generation quality. Once synthesized, the scene can be rendered in diverse styles through relighting and style transfer techniques.</li>
<li><strong>摘要：</strong>对于众多城市，网格模型变得越来越易于​​使用。但是，缺乏逼真的纹理限制了它们在虚拟城市导航和自动驾驶中的应用。为了解决这个问题，本文提出了杂物（基于网格的场景综合），以生成具有城市网格型号的高质量，样式固定的户外场景，这些场景是几何事先的。尽管图像和视频扩散模型可以利用空间布局（例如深度图或高清图）作为控制条件来生成街道级别的透视图，但它们并不直接适用于3D场景生成。视频扩散模型在合成一致的视图序列方面表现出色，该序列描绘了场景，但通常很难遵守预定义的相机路径或与渲染的控制视频准确对齐。相反，尽管无法保证交叉视觉视觉一致性，但图像扩散模型与ControlNET结合时会产生更多与几何形状的结果。在这种见解的基础上，我们的方法通过提高跨视图一致性来增强图像扩散模型。管道包括三个关键阶段：首先，我们使用级联的支出控制网络生成几何一致的稀疏视图；其次，我们通过称为aginpaint的组件来传播密集的中间视图。第三，我们在全球范围内消除了使用Gcalign模块的视觉上不一致（例如，变化的暴露）。同时，与一代同时，通过在网状表面上初始化高斯球来重建一个3D高斯裂（3DGS）场景。我们的方法在几何对齐和发电质量方面都优于现有方法。一旦合成，就可以通过重新和风格的转移技术以各种样式呈现场景。</li>
</ul>

<h3>Title: SafeLLM: Unlearning Harmful Outputs from Large Language Models against Jailbreak Attacks</h3>
<ul>
<li><strong>Authors: </strong>Xiangman Li, Xiaodong Wu, Qi Li, Jianbing Ni, Rongxing Lu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.15182">https://arxiv.org/abs/2508.15182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.15182">https://arxiv.org/pdf/2508.15182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.15182]] SafeLLM: Unlearning Harmful Outputs from Large Language Models against Jailbreak Attacks(https://arxiv.org/abs/2508.15182)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Jailbreak attacks pose a serious threat to the safety of Large Language Models (LLMs) by crafting adversarial prompts that bypass alignment mechanisms, causing the models to produce harmful, restricted, or biased content. In this paper, we propose SafeLLM, a novel unlearning-based defense framework that unlearn the harmful knowledge from LLMs while preserving linguistic fluency and general capabilities. SafeLLM employs a three-stage pipeline: (1) dynamic unsafe output detection using a hybrid approach that integrates external classifiers with model-internal evaluations; (2) token-level harmful content tracing through feedforward network (FFN) activations to localize harmful knowledge; and (3) constrained optimization to suppress unsafe behavior without degrading overall model quality. SafeLLM achieves targeted and irreversible forgetting by identifying and neutralizing FFN substructures responsible for harmful generation pathways. Extensive experiments on prominent LLMs (Vicuna, LLaMA, and GPT-J) across multiple jailbreak benchmarks show that SafeLLM substantially reduces attack success rates while maintaining high general-purpose performance. Compared to standard defense methods such as supervised fine-tuning and direct preference optimization, SafeLLM offers stronger safety guarantees, more precise control over harmful behavior, and greater robustness to unseen attacks. Moreover, SafeLLM maintains the general performance after the harmful knowledge unlearned. These results highlight unlearning as a promising direction for scalable and effective LLM safety.</li>
<li><strong>摘要：</strong>越狱攻击通过制定对抗性提示绕过对齐机制，导致模型产生有害，有限或有偏见的内容，对大语言模型（LLM）的安全构成了严重威胁。在本文中，我们提出了Safellm，这是一种新颖的基于学习的防御框架，该框架从LLM中汲取了有害知识，同时保持语言流利和一般能力。 Safellm采用三阶段管道：（1）使用将外部分类器与模型内部评估集成的混合方法动态不安全输出检测； （2）通过前馈网络（FFN）激活来追踪的令牌有害内容，以定位有害知识； （3）限制优化以抑制不安全行为而不会降低整体模型质量。 Safellm通过识别和中和负责有害生成途径的FFN子结构来实现针对性和不可逆转的遗忘。对多个越狱基准的知名LLM（Vicuna，Llama和GPT-J）进行的广泛实验表明，Safellm大大降低了攻击成功率，同时保持了高通用性能。与标准的防御方法（例如受监督的微调和直接偏好优化）相比，Safellm提供了更强的安全保证，对有害行为的更精确控制以及对看不见的攻击的更大鲁棒性。此外，在没有有害知识之后，Safellm保持了一般表现。这些结果突出了学习，这是可扩展有效的LLM安全性的有希望的方向。</li>
</ul>

<h3>Title: SurgWound-Bench: A Benchmark for Surgical Wound Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Xu (Ohio State University, USA), Changchang Yin (Ohio State University Wexner Medical Center, USA), Odysseas Chatzipanagiotou (Ohio State University Wexner Medical Center, USA), Diamantis Tsilimigras (Ohio State University Wexner Medical Center, USA), Kevin Clear (Ohio State University Wexner Medical Center, USA), Bingsheng Yao (Northeastern University, USA), Dakuo Wang (Northeastern University, USA), Timothy Pawlik (Ohio State University Wexner Medical Center, USA), Ping Zhang (Ohio State University, USA)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.15189">https://arxiv.org/abs/2508.15189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.15189">https://arxiv.org/pdf/2508.15189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.15189]] SurgWound-Bench: A Benchmark for Surgical Wound Diagnosis(https://arxiv.org/abs/2508.15189)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Surgical site infection (SSI) is one of the most common and costly healthcare-associated infections and and surgical wound care remains a significant clinical challenge in preventing SSIs and improving patient outcomes. While recent studies have explored the use of deep learning for preliminary surgical wound screening, progress has been hindered by concerns over data privacy and the high costs associated with expert annotation. Currently, no publicly available dataset or benchmark encompasses various types of surgical wounds, resulting in the absence of an open-source Surgical-Wound screening tool. To address this gap: (1) we present SurgWound, the first open-source dataset featuring a diverse array of surgical wound types. It contains 697 surgical wound images annotated by 3 professional surgeons with eight fine-grained clinical attributes. (2) Based on SurgWound, we introduce the first benchmark for surgical wound diagnosis, which includes visual question answering (VQA) and report generation tasks to comprehensively evaluate model performance. (3) Furthermore, we propose a three-stage learning framework, WoundQwen, for surgical wound diagnosis. In the first stage, we employ five independent MLLMs to accurately predict specific surgical wound characteristics. In the second stage, these predictions serve as additional knowledge inputs to two MLLMs responsible for diagnosing outcomes, which assess infection risk and guide subsequent interventions. In the third stage, we train a MLLM that integrates the diagnostic results from the previous two stages to produce a comprehensive report. This three-stage framework can analyze detailed surgical wound characteristics and provide subsequent instructions to patients based on surgical images, paving the way for personalized wound care, timely intervention, and improved patient outcomes.</li>
<li><strong>摘要：</strong>手术部位感染（SSI）是最常见且昂贵的医疗保健相关感染之一，并且手术伤口护理仍然是预防SSIS和改善患者结局的重大临床挑战。尽管最近的研究探索了对初步手术伤口筛查的使用，但对数据隐私的担忧以及与专家注释相关的高昂成本所阻碍了进步。当前，尚无公开可用的数据集或基准测试包括各种类型的手术伤口，导致没有开源外科手术筛查工具。为了解决这一差距：（1）我们提供外科手术，这是第一个开源数据集，具有各种各样的手术伤口类型。它包含3名具有八个细粒临床属性的专业外科医生注释的手术伤口图像。 （2）基于外科手术，我们介绍了第一个用于手术伤口诊断的基准，其中包括视觉问题答案（VQA）和报告生成任务，以全面评估模型性能。 （3）此外，我们提出了一个三阶段的学习框架，即伤口，用于手术伤口诊断。在第一阶段，我们采用五个独立的MLLM来准确预测特定的手术伤口特征。在第二阶段，这些预测是负责诊断结果的两个MLLM的其他知识输入，这些MLLM评估感染风险并指导后续干预措施。在第三阶段，我们训练一个MLLM，该MLLM整合了前两个阶段的诊断结果，以产生全面的报告。这个三阶段的框架可以分析详细的手术伤口特征，并根据手术图像为患者提供随后的说明，为个性化伤口护理，及时干预和改善患者结局铺平道路。</li>
</ul>

<h3>Title: SleepDIFFormer: Sleep Stage Classification via Multivariate Differential Transformer</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Wei Hao Chin, Yuin Torng Yew, Haocheng Wu, Lanxin Liang, Chow Khuen Chan, Norita Mohd Zain, Siti Balqis Samdin, Sim Kuan Goh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.15215">https://arxiv.org/abs/2508.15215</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.15215">https://arxiv.org/pdf/2508.15215</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.15215]] SleepDIFFormer: Sleep Stage Classification via Multivariate Differential Transformer(https://arxiv.org/abs/2508.15215)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Classification of sleep stages is essential for assessing sleep quality and diagnosing sleep disorders such as insomnia. However, manual inspection of EEG characteristics for each stage is time-consuming and prone to human error. Although machine learning and deep learning methods have been actively developed, they continue to face challenges from the non-stationarity and variability of electroencephalography (EEG) and electrooculography (EOG) signals, often leading to poor generalization on unseen datasets. This research proposed a Sleep Stage Classification method by developing Multivariate Differential Transformer (SleepDIFFormer) for joint EEG and EOG representation learning. Specifically, SleepDIFFormer was developed to process EEG and EOG signals using our Multivariate Differential Transformer Architecture (MDTA) for time series, trained with cross-domain alignment. Our method mitigated spatial and temporal attention noise while learning a domain-invariant joint EEG-EOG representation through feature distribution alignment, thereby enabling generalization to unseen target datasets. Empirically, we evaluated our method on five different sleep staging datasets and compared it with existing approaches, achieving state-of-the-art performance. We also conducted thorough ablation analyses of SleepDIFFormer and interpreted the differential attention weights, highlighting their relevance to characteristic sleep EEG patterns. These findings have implications for advancing automated sleep stage classification and its application to sleep quality assessment. Our source code is publicly available at this https URL</li>
<li><strong>摘要：</strong>睡眠阶段的分类对于评估睡眠质量和诊断睡眠障碍（例如失眠症）至关重要。但是，每个阶段的脑电图特征的手动检查是耗时的，容易出现人为错误。尽管机器学习和深度学习方法已经积极开发，但它们继续面对脑电图（EEG）（EEG）和电学（EOG）信号的非平稳性和可变性的挑战，通常导致对看不见的数据集的概括不佳。这项研究提出了一种通过开发与EEG和EOG表示的多元差分变压器（Sleep -Difformer）来开发多元差分变压器（Sleep -Difformer）。具体而言，使用我们的多变量差分变压器体系结构（MDTA）进行时间序列，开发了Sleepdifformer来处理脑电图和EOG信号，并接受了跨域对齐的训练。我们的方法通过特征分布对齐来学习域不变的关节EEG-EOG表示，从而减轻了空间和时间注意力噪声，从而使概括能够看不见目标数据集。从经验上讲，我们在五个不同的睡眠阶段数据集上评估了我们的方法，并将其与现有方法进行了比较，从而实现了最先进的性能。我们还对Sleepdifformer进行了彻底的消融分析，并解释了差异注意力的重量，突出了它们与特征性睡眠脑电图模式的相关性。这些发现对推进自动睡眠阶段分类及其在睡眠质量评估中的应用有影响。我们的源代码可在此HTTPS URL上公开获得</li>
</ul>

<h3>Title: See Beyond a Single View: Multi-Attribution Learning Leads to Better Conversion Rate Prediction</h3>
<ul>
<li><strong>Authors: </strong>Sishuo Chen, Zhangming Chan, Xiang-Rong Sheng, Lei Zhang, Sheng Chen, Chenghuan Hou, Han Zhu, Jian Xu, Bo Zheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.15217">https://arxiv.org/abs/2508.15217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.15217">https://arxiv.org/pdf/2508.15217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.15217]] See Beyond a Single View: Multi-Attribution Learning Leads to Better Conversion Rate Prediction(https://arxiv.org/abs/2508.15217)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Conversion rate (CVR) prediction is a core component of online advertising systems, where the attribution mechanisms-rules for allocating conversion credit across user touchpoints-fundamentally determine label generation and model optimization. While many industrial platforms support diverse attribution mechanisms (e.g., First-Click, Last-Click, Linear, and Data-Driven Multi-Touch Attribution), conventional approaches restrict model training to labels from a single production-critical attribution mechanism, discarding complementary signals in alternative attribution perspectives. To address this limitation, we propose a novel Multi-Attribution Learning (MAL) framework for CVR prediction that integrates signals from multiple attribution perspectives to better capture the underlying patterns driving user conversions. Specifically, MAL is a joint learning framework consisting of two core components: the Attribution Knowledge Aggregator (AKA) and the Primary Target Predictor (PTP). AKA is implemented as a multi-task learner that integrates knowledge extracted from diverse attribution labels. PTP, in contrast, focuses on the task of generating well-calibrated conversion probabilities that align with the system-optimized attribution metric (e.g., CVR under the Last-Click attribution), ensuring direct compatibility with industrial deployment requirements. Additionally, we propose CAT, a novel training strategy that leverages the Cartesian product of all attribution label combinations to generate enriched supervision signals. This design substantially enhances the performance of the attribution knowledge aggregator. Empirical evaluations demonstrate the superiority of MAL over single-attribution learning baselines, achieving +0.51% GAUC improvement on offline metrics. Online experiments demonstrate that MAL achieved a +2.6% increase in ROI (Return on Investment).</li>
<li><strong>摘要：</strong>转换率（CVR）预测是在线广告系统的核心组成部分，其中用于在用户触摸点上分配转换信用的归因机制规则基本上确定标签生成和模型优化。尽管许多工业平台都支持各种归因机制（例如，首次单击，最后单击，线性和数据驱动的多点触摸归因），但常规方法将模型培训限制在单个生产至关重要的归因机制中，从而丢弃了互补信号，从而限制了单个生产归因机制的标签。为了解决此限制，我们为CVR预测提出了一个新颖的多属性学习（MAL）框架，该框架从多个属性角度集成了信号，以更好地捕获驱动用户转换的基本模式。具体而言，MAL是一个由两个核心组成部分组成的联合学习框架：归因知识聚合器（aka）和主要目标预测指标（PTP）。 AKA被实施为多任务学习者，该学习者集成了从不同属性标签中提取的知识。相比之下，PTP着重于生成与系统优化归因度量度量相一致的良好校准转换概率的任务（例如，在最后点击归因下的CVR），确保与工业部署要求的直接兼容。此外，我们提出了一种新型的培训策略CAT，该培训策略利用所有归因标签组合的笛卡尔产品来产生丰富的监督信号。该设计大大提高了归因知识聚合器的性能。经验评估表明，MAL优于单属性学习基线，在离线指标上可获得 +0.51％GAUC的改善。在线实验表明，MAL的投资回报率增加了2.6％（投资回报率）。</li>
</ul>

<h3>Title: Collaborative Multi-Modal Coding for High-Quality 3D Generation</h3>
<ul>
<li><strong>Authors: </strong>Ziang Cao, Zhaoxi Chen, Liang Pan, Ziwei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.15228">https://arxiv.org/abs/2508.15228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.15228">https://arxiv.org/pdf/2508.15228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.15228]] Collaborative Multi-Modal Coding for High-Quality 3D Generation(https://arxiv.org/abs/2508.15228)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>3D content inherently encompasses multi-modal characteristics and can be projected into different modalities (e.g., RGB images, RGBD, and point clouds). Each modality exhibits distinct advantages in 3D asset modeling: RGB images contain vivid 3D textures, whereas point clouds define fine-grained 3D geometries. However, most existing 3D-native generative architectures either operate predominantly within single-modality paradigms-thus overlooking the complementary benefits of multi-modality data-or restrict themselves to 3D structures, thereby limiting the scope of available training datasets. To holistically harness multi-modalities for 3D modeling, we present TriMM, the first feed-forward 3D-native generative model that learns from basic multi-modalities (e.g., RGB, RGBD, and point cloud). Specifically, 1) TriMM first introduces collaborative multi-modal coding, which integrates modality-specific features while preserving their unique representational strengths. 2) Furthermore, auxiliary 2D and 3D supervision are introduced to raise the robustness and performance of multi-modal coding. 3) Based on the embedded multi-modal code, TriMM employs a triplane latent diffusion model to generate 3D assets of superior quality, enhancing both the texture and the geometric detail. Extensive experiments on multiple well-known datasets demonstrate that TriMM, by effectively leveraging multi-modality, achieves competitive performance with models trained on large-scale datasets, despite utilizing a small amount of training data. Furthermore, we conduct additional experiments on recent RGB-D datasets, verifying the feasibility of incorporating other multi-modal datasets into 3D generation.</li>
<li><strong>摘要：</strong>3D内容固有地包含多模式特征，并且可以投影到不同的模式（例如RGB图像，RGBD和Point Clouds）。每种模式在3D资产建模中都表现出不同的优势：RGB图像包含生动的3D纹理，而点云则定义了细颗粒的3D几何形状。但是，大多数现有的3D新生成体系结构要么主要在单模式范式中运行，因此可以俯瞰多模式数据的互补益处，或者将自己限制在3D结构中，从而限制了可用培训数据集的范围。为了整体利用3D建模的多模式，我们提出了TRIMM，这是第一个从基本的多模式（例如RGB，RGBD和Point Cloud）中学习的第一个馈送3D-NENENT生成模型。具体而言，1）TRIMM首先引入了协作多模式编码，该编码集成了特定于模式的功能，同时保留其独特的代表优势。 2）此外，引入了辅助2D和3D监督，以提高多模式编码的稳健性和性能。 3）基于嵌入式的多模式代码，Trimm采用了三倍潜在扩散模型来生成优质质量的3D资产，从而增强了纹理和几何细节。在多个知名数据集上进行的广泛实验表明，TRIMM通过有效利用多模式，通过在大规模数据集中训练的模型来实现竞争性能，尽管使用了少量的培训数据。此外，我们对最近的RGB-D数据集进行了其他实验，从而验证将其他多模式数据集纳入3D生成的可行性。</li>
</ul>

<h3>Title: Pretrained Diffusion Models Are Inherently Skipped-Step Samplers</h3>
<ul>
<li><strong>Authors: </strong>Wenju Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.15233">https://arxiv.org/abs/2508.15233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.15233">https://arxiv.org/pdf/2508.15233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.15233]] Pretrained Diffusion Models Are Inherently Skipped-Step Samplers(https://arxiv.org/abs/2508.15233)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models have been achieving state-of-the-art results across various generation tasks. However, a notable drawback is their sequential generation process, requiring long-sequence step-by-step generation. Existing methods, such as DDIM, attempt to reduce sampling steps by constructing a class of non-Markovian diffusion processes that maintain the same training objective. However, there remains a gap in understanding whether the original diffusion process can achieve the same efficiency without resorting to non-Markovian processes. In this paper, we provide a confirmative answer and introduce skipped-step sampling, a mechanism that bypasses multiple intermediate denoising steps in the iterative generation process, in contrast with the traditional step-by-step refinement of standard diffusion inference. Crucially, we demonstrate that this skipped-step sampling mechanism is derived from the same training objective as the standard diffusion model, indicating that accelerated sampling via skipped-step sampling via a Markovian way is an intrinsic property of pretrained diffusion models. Additionally, we propose an enhanced generation method by integrating our accelerated sampling technique with DDIM. Extensive experiments on popular pretrained diffusion models, including the OpenAI ADM, Stable Diffusion, and Open Sora models, show that our method achieves high-quality generation with significantly reduced sampling steps.</li>
<li><strong>摘要：</strong>扩散模型已经在各种一代任务中实现了最新的结果。但是，一个值得注意的缺点是他们的顺序生成过程，需要长期逐步生成。现有的方法（例如DDIM）试图通过构建一类保持相同训练目标的非马克维亚扩散过程来减少采样步骤。但是，理解原始扩散过程是否可以达到相同的效率而无需诉诸于非马克维亚过程的效率仍然存在差距。在本文中，我们提供了一个确认的答案，并引入了跳动式采样，该机制绕过了迭代生成过程中多个中间的剥离步骤，与传统的标准扩散推断的逐步完善相比。至关重要的是，我们证明了这种跳动步骤采样机制是从与标准扩散模型相同的训练目标中得出的，表明通过Markovian方法通过跳过步骤采样加速采样是预审美的扩散模型的内在特性。此外，我们通过将加速抽样技术与DDIM整合在一起，提出了一种增强的生成方法。对流行预处理的扩散模型的广泛实验，包括OpenAI ADM，稳定的扩散和Open Sora模型，表明我们的方法通过大大减少采样步骤实现了高质量的生成。</li>
</ul>

<h3>Title: Deep Think with Confidence</h3>
<ul>
<li><strong>Authors: </strong>Yichao Fu, Xuewei Wang, Yuandong Tian, Jiawei Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.15260">https://arxiv.org/abs/2508.15260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.15260">https://arxiv.org/pdf/2508.15260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.15260]] Deep Think with Confidence(https://arxiv.org/abs/2508.15260)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown great potential in reasoning tasks through test-time scaling methods like self-consistency with majority voting. However, this approach often leads to diminishing returns in accuracy and high computational overhead. To address these challenges, we introduce Deep Think with Confidence (DeepConf), a simple yet powerful method that enhances both reasoning efficiency and performance at test time. DeepConf leverages model-internal confidence signals to dynamically filter out low-quality reasoning traces during or after generation. It requires no additional model training or hyperparameter tuning and can be seamlessly integrated into existing serving frameworks. We evaluate DeepConf across a variety of reasoning tasks and the latest open-source models, including Qwen 3 and GPT-OSS series. Notably, on challenging benchmarks such as AIME 2025, DeepConf@512 achieves up to 99.9% accuracy and reduces generated tokens by up to 84.7% compared to full parallel thinking.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）通过测试时间缩放方法（例如自遇到性能和多数投票）在推理任务方面表现出了巨大的潜力。但是，这种方法通常会导致准确性和高计算开销的回报降低。为了应对这些挑战，我们充满信心地介绍了深层思考（DeepConf），这是一种简单而强大的方法，可在测试时提高推理效率和性能。 DeepConf利用模型内部置信信号在生成期间或之后动态滤除低质量的推理痕迹。它不需要其他模型培训或超参数调整，并且可以无缝集成到现有的服务框架中。我们在各种推理任务和最新的开源模型中评估了DeepConf，包括QWEN 3和GPT-soss系列。值得注意的是，在诸如AIME 2025之类的具有挑战性的基准上，DeepConf@512的准确性高达99.9％，并且与完全并行思维相比，产生的令牌最多可将代币产生的代币降低多达84.7％。</li>
</ul>

<h3>Title: First RAG, Second SEG: A Training-Free Paradigm for Camouflaged Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Wutao Liu, YiDan Wang, Pan Gao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.15313">https://arxiv.org/abs/2508.15313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.15313">https://arxiv.org/pdf/2508.15313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.15313]] First RAG, Second SEG: A Training-Free Paradigm for Camouflaged Object Detection(https://arxiv.org/abs/2508.15313)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Camouflaged object detection (COD) poses a significant challenge in computer vision due to the high similarity between objects and their backgrounds. Existing approaches often rely on heavy training and large computational resources. While foundation models such as the Segment Anything Model (SAM) offer strong generalization, they still struggle to handle COD tasks without fine-tuning and require high-quality prompts to yield good performance. However, generating such prompts manually is costly and inefficient. To address these challenges, we propose \textbf{First RAG, Second SEG (RAG-SEG)}, a training-free paradigm that decouples COD into two stages: Retrieval-Augmented Generation (RAG) for generating coarse masks as prompts, followed by SAM-based segmentation (SEG) for refinement. RAG-SEG constructs a compact retrieval database via unsupervised clustering, enabling fast and effective feature retrieval. During inference, the retrieved features produce pseudo-labels that guide precise mask generation using SAM2. Our method eliminates the need for conventional training while maintaining competitive performance. Extensive experiments on benchmark COD datasets demonstrate that RAG-SEG performs on par with or surpasses state-of-the-art methods. Notably, all experiments are conducted on a \textbf{personal laptop}, highlighting the computational efficiency and practicality of our approach. We present further analysis in the Appendix, covering limitations, salient object detection extension, and possible improvements.</li>
<li><strong>摘要：</strong>伪装的对象检测（COD）由于对象及其背景之间的高相似性，在计算机视觉中构成了重大挑战。现有的方法通常依赖于繁重的培训和大量的计算资源。尽管诸如任何模型（SAM）之类的基础模型提供了强大的概括，但他们仍然很难处理鳕鱼任务而无需微调，并且需要高质量的提示以产生良好的性能。但是，手动产生此类提示是昂贵且效率低下的。为了应对这些挑战，我们提出\ textbf {第一抹布，第二seg（rag-seg）}，这是一种无训练的范式，将鳕鱼分解为两个阶段：检索效果的生成（rag），用于将粗膜作为提示生成提示，然后是基于SAM基于SAM的序列（SEG）。 RAG-SEG通过无监督的聚类构建一个紧凑的检索数据库，从而可以快速有效地检索。在推断过程中，检索到的特征会产生伪标记，可使用SAM2指导精确的面膜生成。我们的方法消除了对传统培训的需求，同时保持竞争性能。基准COD数据集上的广泛实验表明，RAG-SEG在与最先进的方法相当或超过最新方法上执行。值得注意的是，所有实验均在\ textbf {个人笔记本电脑}上进行，突出了我们方法的计算效率和实用性。我们在附录中介绍了进一步的分析，涵盖了局限性，显着对象检测扩展以及可能的改进。</li>
</ul>

<h3>Title: VideoEraser: Concept Erasure in Text-to-Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Naen Xu, Jinghuai Zhang, Changjiang Li, Zhi Chen, Chunyi Zhou, Qingming Li, Tianyu Du, Shouling Ji</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.15314">https://arxiv.org/abs/2508.15314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.15314">https://arxiv.org/pdf/2508.15314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.15314]] VideoEraser: Concept Erasure in Text-to-Video Diffusion Models(https://arxiv.org/abs/2508.15314)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The rapid growth of text-to-video (T2V) diffusion models has raised concerns about privacy, copyright, and safety due to their potential misuse in generating harmful or misleading content. These models are often trained on numerous datasets, including unauthorized personal identities, artistic creations, and harmful materials, which can lead to uncontrolled production and distribution of such content. To address this, we propose VideoEraser, a training-free framework that prevents T2V diffusion models from generating videos with undesirable concepts, even when explicitly prompted with those concepts. Designed as a plug-and-play module, VideoEraser can seamlessly integrate with representative T2V diffusion models via a two-stage process: Selective Prompt Embedding Adjustment (SPEA) and Adversarial-Resilient Noise Guidance (ARNG). We conduct extensive evaluations across four tasks, including object erasure, artistic style erasure, celebrity erasure, and explicit content erasure. Experimental results show that VideoEraser consistently outperforms prior methods regarding efficacy, integrity, fidelity, robustness, and generalizability. Notably, VideoEraser achieves state-of-the-art performance in suppressing undesirable content during T2V generation, reducing it by 46% on average across four tasks compared to baselines.</li>
<li><strong>摘要：</strong>文本对视频（T2V）扩散模型的快速增长引起了人们对隐私，版权和安全性的担忧，因为它们在产生有害或误导性内容方面的潜在滥用。这些模型通常在许多数据集上进行培训，包括未经授权的个人身份，艺术创作和有害材料，这可能导致这种内容的生产和分发。为了解决这个问题，我们提出了Videoeraser，这是一个无训练的框架，可以防止T2V扩散模型生成具有不良概念的视频，即使明确提示了这些概念。视频使用者设计为即插即用模块，可以通过两阶段的过程与代表性的T2V扩散模型无缝集成：选择性提示嵌入调整（SPEA）和对抗性弹性噪声引导（ARNG）。我们跨四个任务进行了广泛的评估，包括对象擦除，艺术风格的擦除，名人擦除和明确的内容擦除。实验结果表明，Videoeraser始终胜过有关功效，完整性，保真度，鲁棒性和概括性的先前方法。值得注意的是，Videoeraser在抑制T2V生成期间不良内容方面取得了最新的性能，与基线相比，在四个任务中，在四个任务中平均将其降低了46％。</li>
</ul>

<h3>Title: ExtraGS: Geometric-Aware Trajectory Extrapolation with Uncertainty-Guided Generative Priors</h3>
<ul>
<li><strong>Authors: </strong>Kaiyuan Tan, Yingying Shen, Haohui Zhu, Zhiwei Zhan, Shan Zhao, Mingfei Tu, Hongcheng Luo, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.15529">https://arxiv.org/abs/2508.15529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.15529">https://arxiv.org/pdf/2508.15529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.15529]] ExtraGS: Geometric-Aware Trajectory Extrapolation with Uncertainty-Guided Generative Priors(https://arxiv.org/abs/2508.15529)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Synthesizing extrapolated views from recorded driving logs is critical for simulating driving scenes for autonomous driving vehicles, yet it remains a challenging task. Recent methods leverage generative priors as pseudo ground truth, but often lead to poor geometric consistency and over-smoothed renderings. To address these limitations, we propose ExtraGS, a holistic framework for trajectory extrapolation that integrates both geometric and generative priors. At the core of ExtraGS is a novel Road Surface Gaussian(RSG) representation based on a hybrid Gaussian-Signed Distance Function (SDF) design, and Far Field Gaussians (FFG) that use learnable scaling factors to efficiently handle distant objects. Furthermore, we develop a self-supervised uncertainty estimation framework based on spherical harmonics that enables selective integration of generative priors only where extrapolation artifacts occur. Extensive experiments on multiple datasets, diverse multi-camera setups, and various generative priors demonstrate that ExtraGS significantly enhances the realism and geometric consistency of extrapolated views, while preserving high fidelity along the original trajectory.</li>
<li><strong>摘要：</strong>从记录的驾驶日志中综合推断的视图对于模拟自动驾驶车辆的驾驶场景至关重要，但这仍然是一项艰巨的任务。最近的方法利用生成先验作为伪基的真理，但通常会导致几何一致性和过度平滑的效果。为了解决这些局限性，我们提出了外推，这是轨迹外推的整体框架，同时整合了几何和生成先验。 Extrags的核心是基于混合高斯签名距离功能（SDF）设计的新型路面高斯（RSG）表示，以及使用可学习的缩放因子有效处理远处对象的远处高斯（FFG）。此外，我们基于球形谐波开发了一个自我监督的不确定性估计框架，该框架仅在发生外推文伪像的情况下才能选择性整合生成先验。在多个数据集，各种多相机设置以及各种生成先验的各种数据集上进行了广泛的实验，这表明，外推大大提高了外推视视图的现实主义和几何一致性，同时沿原始轨迹保留了高忠诚度。</li>
</ul>

<h3>Title: Multi-Object Sketch Animation with Grouping and Motion Trajectory Priors</h3>
<ul>
<li><strong>Authors: </strong>Guotao Liang, Juncheng Hu, Ximing Xing, Jing Zhang, Qian Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.15535">https://arxiv.org/abs/2508.15535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.15535">https://arxiv.org/pdf/2508.15535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.15535]] Multi-Object Sketch Animation with Grouping and Motion Trajectory Priors(https://arxiv.org/abs/2508.15535)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce GroupSketch, a novel method for vector sketch animation that effectively handles multi-object interactions and complex motions. Existing approaches struggle with these scenarios, either being limited to single-object cases or suffering from temporal inconsistency and poor generalization. To address these limitations, our method adopts a two-stage pipeline comprising Motion Initialization and Motion Refinement. In the first stage, the input sketch is interactively divided into semantic groups and key frames are defined, enabling the generation of a coarse animation via interpolation. In the second stage, we propose a Group-based Displacement Network (GDN), which refines the coarse animation by predicting group-specific displacement fields, leveraging priors from a text-to-video model. GDN further incorporates specialized modules, such as Context-conditioned Feature Enhancement (CCFE), to improve temporal consistency. Extensive experiments demonstrate that our approach significantly outperforms existing methods in generating high-quality, temporally consistent animations for complex, multi-object sketches, thus expanding the practical applications of sketch animation.</li>
<li><strong>摘要：</strong>我们介绍了grouptketch，这是一种用于矢量草图动画的新方法，可有效处理多对象相互作用和复杂的运动。现有的方法在这些情况下遇到困难，要么仅限于单对象案件，要么遭受时间不一致和泛化。为了解决这些局限性，我们的方法采用了包括运动初始化和运动改进的两阶段管道。在第一阶段，输入草图被交互分为语义组，并定义了关键帧，从而可以通过插值生成粗糙的动画。在第二阶段，我们提出了一个基于组的位移网络（GDN），该网络通过预测特定于小组的位移字段来完善粗糙的动画，从文本到视频模型中利用先验。 GDN进一步结合了专门的模块，例如上下文条件功能增强（CCFE），以提高时间一致性。广泛的实验表明，我们的方法在为复杂的多对象草图生成高质量，时间一致的动画方面极大地胜过现有的方法，从而扩展了草图动画的实际应用。</li>
</ul>

<h3>Title: Weakly-Supervised Learning for Tree Instances Segmentation in Airborne Lidar Point Clouds</h3>
<ul>
<li><strong>Authors: </strong>Swann Emilien Céleste Destouches, Jesse Lahaye, Laurent Valentin Jospin, Jan Skaloud</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.15646">https://arxiv.org/abs/2508.15646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.15646">https://arxiv.org/pdf/2508.15646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.15646]] Weakly-Supervised Learning for Tree Instances Segmentation in Airborne Lidar Point Clouds(https://arxiv.org/abs/2508.15646)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Tree instance segmentation of airborne laser scanning (ALS) data is of utmost importance for forest monitoring, but remains challenging due to variations in the data caused by factors such as sensor resolution, vegetation state at acquisition time, terrain characteristics, etc. Moreover, obtaining a sufficient amount of precisely labeled data to train fully supervised instance segmentation methods is expensive. To address these challenges, we propose a weakly supervised approach where labels of an initial segmentation result obtained either by a non-finetuned model or a closed form algorithm are provided as a quality rating by a human operator. The labels produced during the quality assessment are then used to train a rating model, whose task is to classify a segmentation output into the same classes as specified by the human operator. Finally, the segmentation model is finetuned using feedback from the rating model. This in turn improves the original segmentation model by 34\% in terms of correctly identified tree instances while considerably reducing the number of non-tree instances predicted. Challenges still remain in data over sparsely forested regions characterized by small trees (less than two meters in height) or within complex surroundings containing shrubs, boulders, etc. which can be confused as trees where the performance of the proposed method is reduced.</li>
<li><strong>摘要：</strong>树木实例对空气传播激光扫描（ALS）数据的细分对于森林监测至关重要，但是由于因素，诸如传感器解决方案，习得时间，地形特征等因素所引起的数据的变化，因此仍然具有挑战性。为了应对这些挑战，我们提出了一种弱监督的方法，在该方法中，通过非重点模型或封闭形式算法获得的初始分割结果的标签是由人类操作员作为质量评级提供的。然后使用在质量评估期间产生的标签来训练评级模型，其任务是将分段输出分类为与人类操作员指定的相同类别。最后，使用评级模型的反馈对分割模型进行了修订。反过来，这将原始分割模型提高了34 \％，以正确识别的树实例，同时大大减少了预测的非树实例数量。挑战仍然存在于稀疏森林区域的数据中，这些区域的特征是小树（高度不到两米）或包含灌木，巨石等复杂环境中的挑战，它们可以像降低所提出方法的性能的树木一样混淆。</li>
</ul>

<h3>Title: CM2LoD3: Reconstructing LoD3 Building Models Using Semantic Conflict Maps</h3>
<ul>
<li><strong>Authors: </strong>Franz Hanke, Antonia Bieringer, Olaf Wysocki, Boris Jutzi</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.15672">https://arxiv.org/abs/2508.15672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.15672">https://arxiv.org/pdf/2508.15672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.15672]] CM2LoD3: Reconstructing LoD3 Building Models Using Semantic Conflict Maps(https://arxiv.org/abs/2508.15672)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Detailed 3D building models are crucial for urban planning, digital twins, and disaster management applications. While Level of Detail 1 (LoD)1 and LoD2 building models are widely available, they lack detailed facade elements essential for advanced urban analysis. In contrast, LoD3 models address this limitation by incorporating facade elements such as windows, doors, and underpasses. However, their generation has traditionally required manual modeling, making large-scale adoption challenging. In this contribution, CM2LoD3, we present a novel method for reconstructing LoD3 building models leveraging Conflict Maps (CMs) obtained from ray-to-model-prior analysis. Unlike previous works, we concentrate on semantically segmenting real-world CMs with synthetically generated CMs from our developed Semantic Conflict Map Generator (SCMG). We also observe that additional segmentation of textured models can be fused with CMs using confidence scores to further increase segmentation performance and thus increase 3D reconstruction accuracy. Experimental results demonstrate the effectiveness of our CM2LoD3 method in segmenting and reconstructing building openings, with the 61% performance with uncertainty-aware fusion of segmented building textures. This research contributes to the advancement of automated LoD3 model reconstruction, paving the way for scalable and efficient 3D city modeling. Our project is available: this https URL</li>
<li><strong>摘要：</strong>详细的3D建筑模型对于城市规划，数字双胞胎和灾难管理应用程序至关重要。尽管细节1（LOD）1和LOD2建筑模型的水平已广泛可用，但它们缺乏高级城市分析必不可少的详细立面元素。相比之下，LOD3模型通过合并窗户，门和地下通道等立面元素来解决此限制。但是，他们这一代传统上需要手动建模，从而使大规模采用具有挑战性。在此贡献中，CM2LOD3，我们提出了一种新的方法，用于重建利用冲突图（CMS）从Ray-Model-Prior分析中获得的冲突图（CMS）。与以前的作品不同，我们专注于通过我们开发的语义冲突图生成器（SCMG）的合成生成的CMS进行语义分割现实世界的CMS。我们还观察到，可以使用置信度得分将纹理模型的其他分割与CMS融合，以进一步提高分割性能，从而提高3D重建精度。实验结果证明了我们的CM2LOD3方法在分割和重建建筑开放方面的有效性，其性能为61％，并具有不确定性的分割建筑纹理融合。这项研究有助于自动LOD3模型重建的进步，为可扩展有效的3D城市建模铺平了道路。我们的项目可用：此HTTPS URL</li>
</ul>

<h3>Title: Investigation of D-Wave quantum annealing for training Restricted Boltzmann Machines and mitigating catastrophic forgetting</h3>
<ul>
<li><strong>Authors: </strong>Abdelmoula El-Yazizi, Yaroslav Koshka</a></li>
<li><strong>Subjects: </strong>cs.LG, quant-ph, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.15697">https://arxiv.org/abs/2508.15697</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.15697">https://arxiv.org/pdf/2508.15697</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.15697]] Investigation of D-Wave quantum annealing for training Restricted Boltzmann Machines and mitigating catastrophic forgetting(https://arxiv.org/abs/2508.15697)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Modest statistical differences between the sampling performances of the D-Wave quantum annealer (QA) and the classical Markov Chain Monte Carlo (MCMC), when applied to Restricted Boltzmann Machines (RBMs), are explored to explain, and possibly address, the absence of significant and consistent improvements in RBM trainability when the D-Wave sampling was used in previous investigations. A novel hybrid sampling approach, combining the classical and the QA contributions, is investigated as a promising way to benefit from the modest differences between the two sampling methods. No improvements in the RBM training are achieved in this work, thereby suggesting that the differences between the QA-based and MCMC sampling, mainly found in the medium-to-low probability regions of the distribution, which are less important for the quality of the sample, are insufficient to benefit the training. Difficulties in achieving sufficiently high quality of embedding RBMs into the lattice of the newer generation of D-Wave hardware could be further complicating the task. On the other hand, the ability to generate samples of sufficient variety from lower-probability parts of the distribution has a potential to benefit other machine learning applications, such as the mitigation of catastrophic forgetting (CF) during incremental learning. The feasibility of using QA-generated patterns of desirable classes for CF mitigation by the generative replay is demonstrated in this work for the first time. While the efficiency of the CF mitigation using the D-Wave QA was comparable to that of the classical mitigation, both the speed of generating a large number of distinct desirable patterns and the potential for further improvement make this approach promising for a variety of challenging machine learning applications.</li>
<li><strong>摘要：</strong>Modest statistical differences between the sampling performances of the D-Wave quantum annealer (QA) and the classical Markov Chain Monte Carlo (MCMC), when applied to Restricted Boltzmann Machines (RBMs), are explored to explain, and possibly address, the absence of significant and consistent improvements in RBM trainability when the D-Wave sampling was used in previous investigations.一种新型的混合抽样方法，结合了经典和质量检查的贡献，是一种有希望的方法，可以从两种采样方法之间的适度差异中受益。在这项工作中没有进行RBM培训的改进，从而表明基于质量检查和MCMC采样之间的差异主要在分布的中低概率区域中发现，这对于样本的质量不太重要，这是不足以使训练受益。将RBM嵌入新一代D-Wave硬件的晶格中的足够高质量的困难可能会使任务更加复杂。另一方面，从分布的低概率部分生成足够种类的样本的能力有可能使其他机器学习应用程序受益，例如减轻灾难性遗忘（CF）在增量学习过程中。首次在这项工作中证明了使用QA生成类别的QA生成类模式来缓解CF的CF。尽管使用D-WAVE QA缓解CF的效率与经典缓解措施相当，但产生大量独特的期望模式的速度以及进一步改进的潜力使这种方法有望为各种具有挑战性的机器学习应用提供。</li>
</ul>

<h3>Title: Tutorial on the Probabilistic Unification of Estimation Theory, Machine Learning, and Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Mohammed Elmusrati</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.15719">https://arxiv.org/abs/2508.15719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.15719">https://arxiv.org/pdf/2508.15719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.15719]] Tutorial on the Probabilistic Unification of Estimation Theory, Machine Learning, and Generative AI(https://arxiv.org/abs/2508.15719)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Extracting meaning from uncertain, noisy data is a fundamental problem across time series analysis, pattern recognition, and language modeling. This survey presents a unified mathematical framework that connects classical estimation theory, statistical inference, and modern machine learning, including deep learning and large language models. By analyzing how techniques such as maximum likelihood estimation, Bayesian inference, and attention mechanisms address uncertainty, the paper illustrates that many AI methods are rooted in shared probabilistic principles. Through illustrative scenarios including system identification, image classification, and language generation, we show how increasingly complex models build upon these foundations to tackle practical challenges like overfitting, data sparsity, and interpretability. In other words, the work demonstrates that maximum likelihood, MAP estimation, Bayesian classification, and deep learning all represent different facets of a shared goal: inferring hidden causes from noisy and/or biased observations. It serves as both a theoretical synthesis and a practical guide for students and researchers navigating the evolving landscape of machine learning.</li>
<li><strong>摘要：</strong>从不确定的嘈杂数据中提取含义是时间序列分析，模式识别和语言建模的基本问题。该调查提出了一个统一的数学框架，该框架将经典估计理论，统计推断和现代机器学习（包括深度学习和大语言模型）联系起来。通过分析诸如最大似然估计，贝叶斯推论和注意机制等技术如何解决不确定性，该论文说明了许多AI方法源于共同的概率原理。通过包括系统识别，图像分类和语言生成在内的说明性场景，我们展示了越来越复杂的模型如何建立在这些基础上，以应对过度拟合，数据稀疏和解释性等实用挑战。换句话说，这项工作表明，最大可能性，地图估计，贝叶斯分类和深度学习都代表了共同目标的不同方面：从嘈杂和/或有偏见的观察结果中推断出隐藏的原因。它既是理论的综合，也是针对学生和研究人员浏览机器学习不断发展的景观的实用指南。</li>
</ul>

<h3>Title: WorldWeaver: Generating Long-Horizon Video Worlds via Rich Perception</h3>
<ul>
<li><strong>Authors: </strong>Zhiheng Liu, Xueqing Deng, Shoufa Chen, Angtian Wang, Qiushan Guo, Mingfei Han, Zeyue Xue, Mengzhao Chen, Ping Luo, Linjie Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.15720">https://arxiv.org/abs/2508.15720</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.15720">https://arxiv.org/pdf/2508.15720</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.15720]] WorldWeaver: Generating Long-Horizon Video Worlds via Rich Perception(https://arxiv.org/abs/2508.15720)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative video modeling has made significant strides, yet ensuring structural and temporal consistency over long sequences remains a challenge. Current methods predominantly rely on RGB signals, leading to accumulated errors in object structure and motion over extended durations. To address these issues, we introduce WorldWeaver, a robust framework for long video generation that jointly models RGB frames and perceptual conditions within a unified long-horizon modeling scheme. Our training framework offers three key advantages. First, by jointly predicting perceptual conditions and color information from a unified representation, it significantly enhances temporal consistency and motion dynamics. Second, by leveraging depth cues, which we observe to be more resistant to drift than RGB, we construct a memory bank that preserves clearer contextual information, improving quality in long-horizon video generation. Third, we employ segmented noise scheduling for training prediction groups, which further mitigates drift and reduces computational cost. Extensive experiments on both diffusion- and rectified flow-based models demonstrate the effectiveness of WorldWeaver in reducing temporal drift and improving the fidelity of generated videos.</li>
<li><strong>摘要：</strong>生成的视频建模已取得了长足的进步，但确保长序列的结构和时间一致性仍然是一个挑战。当前方法主要取决于RGB信号，导致对象结构和运动的累积错误在延长的持续时间内。为了解决这些问题，我们介绍了WorldWeaver，这是一个长期视频生成的强大框架，该框架在统一的长距建模方案中共同对RGB框架和感知条件进行建模。我们的培训框架提供了三个关键优势。首先，通过共同预测统一表示的感知条件和颜色信息，它可以显着提高时间一致性和运动动态。其次，通过利用深度提示，我们观察到比RGB更具抗性的抗性，我们构建了一个保存更清晰的上下文信息的内存库，从而提高了长远的视频生成质量。第三，我们对培训预测组采用分段的噪声调度，这进一步减轻了漂移并降低计算成本。基于扩散和整流流的模型的广泛实验证明了全球师在减少时间漂移和改善产生视频的忠诚度方面的有效性。</li>
</ul>

<h3>Title: Probability Density from Latent Diffusion Models for Out-of-Distribution Detection</h3>
<ul>
<li><strong>Authors: </strong>Joonas Järve, Karl Kaspar Haavel, Meelis Kull</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.15737">https://arxiv.org/abs/2508.15737</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.15737">https://arxiv.org/pdf/2508.15737</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.15737]] Probability Density from Latent Diffusion Models for Out-of-Distribution Detection(https://arxiv.org/abs/2508.15737)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Despite rapid advances in AI, safety remains the main bottleneck to deploying machine-learning systems. A critical safety component is out-of-distribution detection: given an input, decide whether it comes from the same distribution as the training data. In generative models, the most natural OOD score is the data likelihood. Actually, under the assumption of uniformly distributed OOD data, the likelihood is even the optimal OOD detector, as we show in this work. However, earlier work reported that likelihood often fails in practice, raising doubts about its usefulness. We explore whether, in practice, the representation space also suffers from the inability to learn good density estimation for OOD detection, or if it is merely a problem of the pixel space typically used in generative models. To test this, we trained a Variational Diffusion Model not on images, but on the representation space of a pre-trained ResNet-18 to assess the performance of our likelihood-based detector in comparison to state-of-the-art methods from the OpenOOD suite.</li>
<li><strong>摘要：</strong>尽管AI取得了迅速的进步，但安全仍然是部署机器学习系统的主要瓶颈。关键的安全组件是分布外检测：给定输入，确定它是否来自与培训数据相同的分布。在生成模型中，最自然的OOD得分是数据的可能性。实际上，在假设均匀分布的OOD数据的假设下，正如我们在这项工作中所显示的那样，可能性甚至是最佳OOD检测器。但是，较早的工作报告说，可能性通常在实践中失败，引起对其有用性的怀疑。在实践中，我们探讨了表示空间是否也无法学习OOD检测的良好密度估计，还是仅仅是生成模型中通常使用的像素空间的问题。为了测试这一点，我们训练了一个不是在图像上的变异扩散模型，而是在预训练的RESNET-18的表示空间上培训了与OpenOOD SUITE的最新方法相比，评估了基于可能性的检测器的性能。</li>
</ul>

<h3>Title: Waver: Wave Your Way to Lifelike Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Yifu Zhang, Hao Yang, Yuqi Zhang, Yifei Hu, Fengda Zhu, Chuang Lin, Xiaofeng Mei, Yi Jiang, Zehuan Yuan, Bingyue Peng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.15761">https://arxiv.org/abs/2508.15761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.15761">https://arxiv.org/pdf/2508.15761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.15761]] Waver: Wave Your Way to Lifelike Video Generation(https://arxiv.org/abs/2508.15761)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present Waver, a high-performance foundation model for unified image and video generation. Waver can directly generate videos with durations ranging from 5 to 10 seconds at a native resolution of 720p, which are subsequently upscaled to 1080p. The model simultaneously supports text-to-video (T2V), image-to-video (I2V), and text-to-image (T2I) generation within a single, integrated framework. We introduce a Hybrid Stream DiT architecture to enhance modality alignment and accelerate training convergence. To ensure training data quality, we establish a comprehensive data curation pipeline and manually annotate and train an MLLM-based video quality model to filter for the highest-quality samples. Furthermore, we provide detailed training and inference recipes to facilitate the generation of high-quality videos. Building on these contributions, Waver excels at capturing complex motion, achieving superior motion amplitude and temporal consistency in video synthesis. Notably, it ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial Analysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming existing open-source models and matching or surpassing state-of-the-art commercial solutions. We hope this technical report will help the community more efficiently train high-quality video generation models and accelerate progress in video generation technologies. Official page: this https URL.</li>
<li><strong>摘要：</strong>我们提出Wawver，这是一种用于统一图像和视频生成的高性能基础模型。 Waver可以直接生成视频，其持续时间为5到10秒，其本机分辨率为720p，随后将其升级为1080p。该模型同时支持单个集成框架中文本对视频（T2V），图像到视频（I2V）和文本对图像（T2I）的生成。我们引入了混合流dit体系结构，以增强模态对准并加速训练融合。为了确保培训数据质量，我们建立了全面的数据策划管道，并手动注释并训练基于MLLM的视频质量模型，以过滤最高质量的样品。此外，我们提供详细的培训和推理食谱，以促进高质量视频的产生。在这些贡献的基础上，Waver擅长捕获复杂的运动，实现了视频合成中的出色运动振幅和时间一致性。值得注意的是，它在人工分析中（截至2025-07-30 10:00 GMT+8）在T2V和I2V排行榜上排名前三，始终超过现有的开源模型，并匹配或超越了最新的商业解决方案。我们希望这份技术报告能够帮助社区更有效地培训高质量的视频生成模型，并加速视频生成技术的进度。官方页面：此HTTPS URL。</li>
</ul>

<h3>Title: Discovering Hidden Algebraic Structures via Transformers with Rank-Aware Beam GRPO</h3>
<ul>
<li><strong>Authors: </strong>Jaeha Lee, Gio Huh, Ning Su, Tony Yue YU</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.15766">https://arxiv.org/abs/2508.15766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.15766">https://arxiv.org/pdf/2508.15766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.15766]] Discovering Hidden Algebraic Structures via Transformers with Rank-Aware Beam GRPO(https://arxiv.org/abs/2508.15766)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent efforts have extended the capabilities of transformers in logical reasoning and symbolic computations. In this work, we investigate their capacity for non-linear latent pattern discovery in the context of functional decomposition, focusing on the challenging algebraic task of multivariate polynomial decomposition. This problem, with widespread applications in science and engineering, is proved to be NP-hard, and demands both precision and insight. Our contributions are threefold: First, we develop a synthetic data generation pipeline providing fine-grained control over problem complexity. Second, we train transformer models via supervised learning and evaluate them across four key dimensions involving scaling behavior and generalizability. Third, we propose Beam Grouped Relative Policy Optimization (BGRPO), a rank-aware reinforcement learning method suitable for hard algebraic problems. Finetuning with BGRPO improves accuracy while reducing beam width by up to half, resulting in approximately 75% lower inference compute. Additionally, our model demonstrates competitive performance in polynomial simplification, outperforming Mathematica in various cases.</li>
<li><strong>摘要：</strong>最近的努力扩大了变压器在逻辑推理和符号计算中的功能。在这项工作中，我们研究了它们在功能分解的背景下进行非线性潜在模式发现的能力，重点是多元多项式分解的具有挑战性的代数任务。事实证明，这个问题在科学和工程中广泛应用被证明是NP的，并且需要精确和洞察力。我们的贡献是三个方面的：首先，我们开发了一个合成数据生成管道，可提供对问题复杂性的细粒度控制。其次，我们通过监督学习训练变压器模型，并在涉及缩放行为和概括性的四个关键维度上进行评估。第三，我们提出了梁分组的相对策略优化（BGRPO），这是一种适合硬式代数问题的等级感知增强学习方法。用BGRPO进行填充可提高准确性，同时将束宽度降低多达一半，从而使推理计算降低约75％。此外，我们的模型在多项式简化方面表现出竞争性能，在各种情况下表现优于Mathematica。</li>
</ul>

<h3>Title: SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass</h3>
<ul>
<li><strong>Authors: </strong>Yanxu Meng, Haoning Wu, Ya Zhang, Weidi Xie</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.15769">https://arxiv.org/abs/2508.15769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.15769">https://arxiv.org/pdf/2508.15769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.15769]] SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass(https://arxiv.org/abs/2508.15769)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>3D content generation has recently attracted significant research interest due to its applications in VR/AR and embodied AI. In this work, we address the challenging task of synthesizing multiple 3D assets within a single scene image. Concretely, our contributions are fourfold: (i) we present SceneGen, a novel framework that takes a scene image and corresponding object masks as input, simultaneously producing multiple 3D assets with geometry and texture. Notably, SceneGen operates with no need for optimization or asset retrieval; (ii) we introduce a novel feature aggregation module that integrates local and global scene information from visual and geometric encoders within the feature extraction module. Coupled with a position head, this enables the generation of 3D assets and their relative spatial positions in a single feedforward pass; (iii) we demonstrate SceneGen's direct extensibility to multi-image input scenarios. Despite being trained solely on single-image inputs, our architectural design enables improved generation performance with multi-image inputs; and (iv) extensive quantitative and qualitative evaluations confirm the efficiency and robust generation abilities of our approach. We believe this paradigm offers a novel solution for high-quality 3D content generation, potentially advancing its practical applications in downstream tasks. The code and model will be publicly available at: this https URL.</li>
<li><strong>摘要：</strong>由于其在VR/AR中的应用并体现了AI，因此3D内容产生最近引起了重大的研究兴趣。在这项工作中，我们解决了在单个场景图像中综合多个3D资产的具有挑战性的任务。具体而言，我们的贡献是四重：（i）我们提出了SceneGen，这是一个新颖的框架，将场景图像和相应的对象掩码作为输入，同时生产具有几何形状和纹理的多个3D资产。值得注意的是，SceneGen不需要优化或资产检索工作； （ii）我们引入了一个新颖的功能聚合模块，该模块从特征提取模块中的视觉和几何编码器中整合了本地和全局场景信息。再加上位置头，这使得在单个进发pass中产生了3D资产及其相对空间位置。 （iii）我们演示了SceneGen对多图像输入方案的直接扩展性。尽管仅接受单像输入的培训，但我们的建筑设计可以通过多图像输入来提高发电性能。 （iv）广泛的定量和定性评估证实了我们方法的效率和强大的产生能力。我们认为，该范式为高质量的3D内容生成提供了一种新颖的解决方案，有可能在下游任务中推进其实际应用。代码和模型将在以下网址公开可用：此HTTPS URL。</li>
</ul>

<h3>Title: Scaling Group Inference for Diverse and High-Quality Generation</h3>
<ul>
<li><strong>Authors: </strong>Gaurav Parmar, Or Patashnik, Daniil Ostashev, Kuan-Chieh Wang, Kfir Aberman, Srinivasa Narasimhan, Jun-Yan Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.15773">https://arxiv.org/abs/2508.15773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.15773">https://arxiv.org/pdf/2508.15773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.15773]] Scaling Group Inference for Diverse and High-Quality Generation(https://arxiv.org/abs/2508.15773)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative models typically sample outputs independently, and recent inference-time guidance and scaling algorithms focus on improving the quality of individual samples. However, in real-world applications, users are often presented with a set of multiple images (e.g., 4-8) for each prompt, where independent sampling tends to lead to redundant results, limiting user choices and hindering idea exploration. In this work, we introduce a scalable group inference method that improves both the diversity and quality of a group of samples. We formulate group inference as a quadratic integer assignment problem: candidate outputs are modeled as graph nodes, and a subset is selected to optimize sample quality (unary term) while maximizing group diversity (binary term). To substantially improve runtime efficiency, we progressively prune the candidate set using intermediate predictions, allowing our method to scale up to large candidate sets. Extensive experiments show that our method significantly improves group diversity and quality compared to independent sampling baselines and recent inference algorithms. Our framework generalizes across a wide range of tasks, including text-to-image, image-to-image, image prompting, and video generation, enabling generative models to treat multiple outputs as cohesive groups rather than independent samples.</li>
<li><strong>摘要：</strong>生成模型通常独立采样，最近的推理时间指导和缩放算法集中于提高单个样品的质量。但是，在实际应用程序中，用户通常会为每个提示提供一组多个图像（例如4-8），其中独立的采样倾向于导致冗余结果，限制用户选择并阻碍Idea Exploration。在这项工作中，我们引入了一种可扩展的推理方法，该方法可以改善一组样品的多样性和质量。我们将组推断作为二次整数分配问题：候选输出以图形节点进行建模，并选择一个子集以优化样本质量（一单术语），同时最大化群体多样性（二进制术语）。为了实质上提高运行时效率，我们使用中间预测逐步修剪候选候选集合，从而使我们的方法扩展到大型候选集合。广泛的实验表明，与独立的采样基准和最近的推理算法相比，我们的方法显着提高了群体的多样性和质量。我们的框架跨越了广泛的任务，包括文本对图像，图像对图像，图像提示和视频生成，使生成模型能够将多个输出视为凝聚力组，而不是独立的样本。</li>
</ul>

<h3>Title: CineScale: Free Lunch in High-Resolution Cinematic Visual Generation</h3>
<ul>
<li><strong>Authors: </strong>Haonan Qiu, Ning Yu, Ziqi Huang, Paul Debevec, Ziwei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.15774">https://arxiv.org/abs/2508.15774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.15774">https://arxiv.org/pdf/2508.15774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.15774]] CineScale: Free Lunch in High-Resolution Cinematic Visual Generation(https://arxiv.org/abs/2508.15774)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Visual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of high-resolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions. Recent efforts have explored tuning-free strategies to exhibit the untapped potential higher-resolution visual generation of pre-trained models. However, these methods are still prone to producing low-quality visual content with repetitive patterns. The key obstacle lies in the inevitable increase in high-frequency information when the model generates visual content exceeding its training resolution, leading to undesirable repetitive patterns deriving from the accumulated errors. In this work, we propose CineScale, a novel inference paradigm to enable higher-resolution visual generation. To tackle the various issues introduced by the two types of video generation architectures, we propose dedicated variants tailored to each. Unlike existing baseline methods that are confined to high-resolution T2I and T2V generation, CineScale broadens the scope by enabling high-resolution I2V and V2V synthesis, built atop state-of-the-art open-source video generation frameworks. Extensive experiments validate the superiority of our paradigm in extending the capabilities of higher-resolution visual generation for both image and video models. Remarkably, our approach enables 8k image generation without any fine-tuning, and achieves 4k video generation with only minimal LoRA fine-tuning. Generated video samples are available at our website: this https URL.</li>
<li><strong>摘要：</strong>视觉扩散模型取得了显着的进步，但是由于缺乏高分辨率数据和限制计算资源，它们通常在有限的分辨率下进行培训，从而阻碍了它们在更高分辨率下生成高保真图像或视频的能力。最近的努力探索了无调的策略，以展示未开发的潜在的高分辨率视觉生成预训练的模型。但是，这些方法仍然容易产生具有重复模式的低质量视觉内容。关键障碍在于，当模型生成超过其训练分辨率的视觉内容时，高频信息不可避免地增加，从而导致累积错误的不良重复模式。在这项工作中，我们提出了Cinescale，这是一种新型的推理范式，以实现高分辨率的视觉产生。为了解决两种类型的视频生成体系结构引入的各种问题，我们提出了针对每种量身定制的专用变体。与局限于高分辨率T2I和T2V生成的现有基线方法不同，Cinescale通过启用高分辨率I2V和V2V合成来扩大范围，该范围是在先进的开源视频生成框架之上建立的。广泛的实验验证了我们范式在扩展图像和视频模型的高分辨率视觉生成能力方面的优势。值得注意的是，我们的方法可实现8K图像的生成，而无需任何微调，并且只需最少的Lora微调即可获得4K视频。生成的视频样本可在我们的网站：此HTTPS URL上找到。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
