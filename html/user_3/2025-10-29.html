<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-29</h1>
<h3>Title: Noise is All You Need: Solving Linear Inverse Problems by Noise Combination Sampling with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Xun Su, Hiroyuki Kasai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23633">https://arxiv.org/abs/2510.23633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23633">https://arxiv.org/pdf/2510.23633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23633]] Noise is All You Need: Solving Linear Inverse Problems by Noise Combination Sampling with Diffusion Models(https://arxiv.org/abs/2510.23633)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Pretrained diffusion models have demonstrated strong capabilities in zero-shot inverse problem solving by incorporating observation information into the generation process of the diffusion models. However, this presents an inherent dilemma: excessive integration can disrupt the generative process, while insufficient integration fails to emphasize the constraints imposed by the inverse problem. To address this, we propose \emph{Noise Combination Sampling}, a novel method that synthesizes an optimal noise vector from a noise subspace to approximate the measurement score, replacing the noise term in the standard Denoising Diffusion Probabilistic Models process. This enables conditional information to be naturally embedded into the generation process without reliance on step-wise hyperparameter tuning. Our method can be applied to a wide range of inverse problem solvers, including image compression, and, particularly when the number of generation steps $T$ is small, achieves superior performance with negligible computational overhead, significantly improving robustness and stability.</li>
<li><strong>摘要：</strong>通过将观测信息纳入扩散模型的生成过程，预训练的扩散模型在零样本逆问题解决方面表现出了强大的能力。然而，这提出了一个固有的困境：过度整合会扰乱生成过程，而整合不足则无法强调逆问题所施加的约束。为了解决这个问题，我们提出了 \emph{噪声组合采样}，这是一种从噪声子空间合成最佳噪声向量来近似测量分数的新方法，取代了标准去噪扩散概率模型过程中的噪声项。这使得条件信息能够自然地嵌入到生成过程中，而不依赖于逐步的超参数调整。我们的方法可以应用于广泛的逆向问题求解器，包括图像压缩，并且，特别是当生成步骤 $T$ 的数量很小时，以可忽略的计算开销实现卓越的性能，显着提高鲁棒性和稳定性。</li>
</ul>

<h3>Title: Integrating Genomics into Multimodal EHR Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Amar, Edward Liu, Alessandra Breschi, Liangliang Zhang, Pouya Kheradpour, Sylvia Li, Lisa Soleymani Lehmann, Alessandro Giulianelli, Matt Edwards, Yugang Jia, David Nola, Raghav Mani, Pankaj Vats, Jesse Tetreault, T.J. Chen, Cory Y. McLean</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23639">https://arxiv.org/abs/2510.23639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23639">https://arxiv.org/pdf/2510.23639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23639]] Integrating Genomics into Multimodal EHR Foundation Models(https://arxiv.org/abs/2510.23639)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>This paper introduces an innovative Electronic Health Record (EHR) foundation model that integrates Polygenic Risk Scores (PRS) as a foundational data modality, moving beyond traditional EHR-only approaches to build more holistic health profiles. Leveraging the extensive and diverse data from the All of Us (AoU) Research Program, this multimodal framework aims to learn complex relationships between clinical data and genetic predispositions. The methodology extends advancements in generative AI to the EHR foundation model space, enhancing predictive capabilities and interpretability. Evaluation on AoU data demonstrates the model's predictive value for the onset of various conditions, particularly Type 2 Diabetes (T2D), and illustrates the interplay between PRS and EHR data. The work also explores transfer learning for custom classification tasks, showcasing the architecture's versatility and efficiency. This approach is pivotal for unlocking new insights into disease prediction, proactive health management, risk stratification, and personalized treatment strategies, laying the groundwork for more personalized, equitable, and actionable real-world evidence generation in healthcare.</li>
<li><strong>摘要：</strong>本文介绍了一种创新的电子健康记录 (EHR) 基础模型，该模型将多基因风险评分 (PRS) 作为基础数据模式，超越传统的仅 EHR 方法来构建更全面的健康档案。该多模式框架利用来自 All of Us (AoU) 研究计划的广泛多样的数据，旨在了解临床数据和遗传倾向之间的复杂关系。该方法将生成人工智能的进步扩展到 EHR 基础模型空间，增强预测能力和可解释性。对 AoU 数据的评估证明了该模型对各种病症（特别是 2 型糖尿病 (T2D)）发病的预测价值，并说明了 PRS 和 EHR 数据之间的相互作用。这项工作还探索了自定义分类任务的迁移学习，展示了该架构的多功能性和效率。这种方法对于解锁疾病预测、主动健康管理、风险分层和个性化治疗策略的新见解至关重要，为医疗保健领域更个性化、公平和可操作的现实世界证据生成奠定基础。</li>
</ul>

<h3>Title: Optimize Any Topology: A Foundation Model for Shape- and Resolution-Free Structural Topology Optimization</h3>
<ul>
<li><strong>Authors: </strong>Amin Heyrani Nobari, Lyle Regenwetter, Cyril Picard, Ligong Han, Faez Ahmed</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23667">https://arxiv.org/abs/2510.23667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23667">https://arxiv.org/pdf/2510.23667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23667]] Optimize Any Topology: A Foundation Model for Shape- and Resolution-Free Structural Topology Optimization(https://arxiv.org/abs/2510.23667)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Structural topology optimization (TO) is central to engineering design but remains computationally intensive due to complex physics and hard constraints. Existing deep-learning methods are limited to fixed square grids, a few hand-coded boundary conditions, and post-hoc optimization, preventing general deployment. We introduce Optimize Any Topology (OAT), a foundation-model framework that directly predicts minimum-compliance layouts for arbitrary aspect ratios, resolutions, volume fractions, loads, and fixtures. OAT combines a resolution- and shape-agnostic autoencoder with an implicit neural-field decoder and a conditional latent-diffusion model trained on OpenTO, a new corpus of 2.2 million optimized structures covering 2 million unique boundary-condition configurations. On four public benchmarks and two challenging unseen tests, OAT lowers mean compliance up to 90% relative to the best prior models and delivers sub-1 second inference on a single GPU across resolutions from 64 x 64 to 256 x 256 and aspect ratios as high as 10:1. These results establish OAT as a general, fast, and resolution-free framework for physics-aware topology optimization and provide a large-scale dataset to spur further research in generative modeling for inverse design. Code & data can be found at this https URL.</li>
<li><strong>摘要：</strong>结构拓扑优化 (TO) 是工程设计的核心，但由于复杂的物理和硬约束，计算量仍然很大。现有的深度学习方法仅限于固定的方形网格、一些手工编码的边界条件和事后优化，阻碍了通用部署。我们引入了优化任意拓扑 (OAT)，这是一个基础模型框架，可直接预测任意长宽比、分辨率、体积分数、负载和固定装置的最小合规性布局。 OAT 将与分辨率和形状无关的自动编码器与隐式神经场解码器和在 OpenTO 上训练的条件潜在扩散模型结合在一起，OpenTO 是一个包含 220 万个优化结构的新语料库，涵盖 200 万个独特的边界条件配置。在四个公共基准测试和两个具有挑战性的未见过的测试中，OAT 相对于之前最好的模型，将平均合规性降低了高达 90%，并在 64 x 64 到 256 x 256 的分辨率和高达 10:1 的宽高比下，在单个 GPU 上提供不到 1 秒的推理。这些结果将 OAT 确立为物理感知拓扑优化的通用、快速、无分辨率框架，并提供大规模数据集以促进逆向设计生成建模的进一步研究。代码和数据可以在此 https URL 中找到。</li>
</ul>

<h3>Title: On the Societal Impact of Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Joachim Baumann</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23693">https://arxiv.org/abs/2510.23693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23693">https://arxiv.org/pdf/2510.23693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23693]] On the Societal Impact of Machine Learning(https://arxiv.org/abs/2510.23693)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This PhD thesis investigates the societal impact of machine learning (ML). ML increasingly informs consequential decisions and recommendations, significantly affecting many aspects of our lives. As these data-driven systems are often developed without explicit fairness considerations, they carry the risk of discriminatory effects. The contributions in this thesis enable more appropriate measurement of fairness in ML systems, systematic decomposition of ML systems to anticipate bias dynamics, and effective interventions that reduce algorithmic discrimination while maintaining system utility. I conclude by discussing ongoing challenges and future research directions as ML systems, including generative artificial intelligence, become increasingly integrated into society. This work offers a foundation for ensuring that ML's societal impact aligns with broader social values.</li>
<li><strong>摘要：</strong>这篇博士论文研究了机器学习 (ML) 的社会影响。机器学习越来越多地为后续决策和建议提供信息，极大地影响着我们生活的许多方面。由于这些数据驱动的系统通常是在没有明确公平考虑的情况下开发的，因此它们存在产生歧视性影响的风险。本论文的贡献使得能够更适当地衡量机器学习系统的公平性、对机器学习系统进行系统分解以预测偏差动态，以及在保持系统实用性的同时减少算法歧视的有效干预措施。最后，我讨论了随着机器学习系统（包括生成人工智能）日益融入社会，当前面临的挑战和未来的研究方向。这项工作为确保机器学习的社会影响与更广泛的社会价值观保持一致奠定了基础。</li>
</ul>

<h3>Title: Explainable Detection of AI-Generated Images with Artifact Localization Using Faster-Than-Lies and Vision-Language Models for Edge Devices</h3>
<ul>
<li><strong>Authors: </strong>Aryan Mathur, Asaduddin Ahmed, Pushti Amit Vasoya, Simeon Kandan Sonar, Yasir Z, Madesh Kuppusamy</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23775">https://arxiv.org/abs/2510.23775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23775">https://arxiv.org/pdf/2510.23775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23775]] Explainable Detection of AI-Generated Images with Artifact Localization Using Faster-Than-Lies and Vision-Language Models for Edge Devices(https://arxiv.org/abs/2510.23775)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The increasing realism of AI-generated imagery poses challenges for verifying visual authenticity. We present an explainable image authenticity detection system that combines a lightweight convolutional classifier ("Faster-Than-Lies") with a Vision-Language Model (Qwen2-VL-7B) to classify, localize, and explain artifacts in 32x32 images. Our model achieves 96.5% accuracy on the extended CiFAKE dataset augmented with adversarial perturbations and maintains an inference time of 175ms on 8-core CPUs, enabling deployment on local or edge devices. Using autoencoder-based reconstruction error maps, we generate artifact localization heatmaps, which enhance interpretability for both humans and the VLM. We further categorize 70 visual artifact types into eight semantic groups and demonstrate explainable text generation for each detected anomaly. This work highlights the feasibility of combining visual and linguistic reasoning for interpretable authenticity detection in low-resolution imagery and outlines potential cross-domain applications in forensics, industrial inspection, and social media moderation.</li>
<li><strong>摘要：</strong>人工智能生成的图像日益真实，给验证视觉真实性带来了挑战。我们提出了一种可解释的图像真实性检测系统，它将轻量级卷积分类器（“Faster-Than-Lies”）与视觉语言模型（Qwen2-VL-7B）相结合，以对 32x32 图像中的伪影进行分类、定位和解释。我们的模型在经过对抗性扰动增强的扩展 CiFAKE 数据集上实现了 96.5% 的准确率，并在 8 核 CPU 上保持 175 毫秒的推理时间，从而能够在本地或边缘设备上部署。使用基于自动编码器的重建误差图，我们生成工件定位热图，这增强了人类和 VLM 的可解释性。我们进一步将 70 种视觉伪影类型分为八个语义组，并为每个检测到的异常演示了可解释的文本生成。这项工作强调了将视觉和语言推理相结合以在低分辨率图像中进行可解释的真实性检测的可行性，并概述了在取证、工业检查和社交媒体审核中潜在的跨领域应用。</li>
</ul>

<h3>Title: Revealing the Potential of Learnable Perturbation Ensemble Forecast Model for Tropical Cyclone Prediction</h3>
<ul>
<li><strong>Authors: </strong>Jun Liu, Tao Zhou, Jiarui Li, Xiaohui Zhong, Peng Zhang, Jie Feng, Lei Chen, Hao Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23794">https://arxiv.org/abs/2510.23794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23794">https://arxiv.org/pdf/2510.23794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23794]] Revealing the Potential of Learnable Perturbation Ensemble Forecast Model for Tropical Cyclone Prediction(https://arxiv.org/abs/2510.23794)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Tropical cyclones (TCs) are highly destructive and inherently uncertain weather systems. Ensemble forecasting helps quantify these uncertainties, yet traditional systems are constrained by high computational costs and limited capability to fully represent atmospheric nonlinearity. FuXi-ENS introduces a learnable perturbation scheme for ensemble generation, representing a novel AI-based forecasting paradigm. Here, we systematically compare FuXi-ENS with ECMWF-ENS using all 90 global TCs in 2018, examining their performance in TC-related physical variables, track and intensity forecasts, and the associated dynamical and thermodynamical fields. FuXi-ENS demonstrates clear advantages in predicting TC-related physical variables, and achieves more accurate track forecasts with reduced ensemble spread, though it still underestimates intensity relative to observations. Further dynamical and thermodynamical analyses reveal that FuXi-ENS better captures large-scale circulation, with moisture turbulent energy more tightly concentrated around the TC warm core, whereas ECMWF-ENS exhibits a more dispersed distribution. These findings highlight the potential of learnable perturbations to improve TC forecasting skill and provide valuable insights for advancing AI-based ensemble prediction of extreme weather events that have significant societal impacts.</li>
<li><strong>摘要：</strong>热带气旋 (TC) 是一种具有高度破坏性且本质上具有不确定性的天气系统。集合预报有助于量化这些不确定性，但传统系统受到计算成本高和完全表示大气非线性的能力有限的限制。 FuXi-ENS 引入了一种用于集成生成的可学习扰动方案，代表了一种新颖的基于人工智能的预测范式。在这里，我们使用 2018 年所有 90 个全球 TC 系统地比较 FuXi-ENS 与 ECMWF-ENS，检查它们在 TC 相关物理变量、路径和强度预报以及相关动力和热力学领域的性能。 FuXi-ENS 在预测与 TC 相关的物理变量方面表现出明显的优势，并通过减少集合传播实现了更准确的轨迹预测，尽管它仍然低估了相对于观测的强度。进一步的动力学和热力学分析表明，FuXi-ENS 更好地捕获了大规模环流，水汽湍流能量更紧密地集中在 TC 暖核周围，而 ECMWF-ENS 表现出更分散的分布。这些发现凸显了可学习扰动在提高热带气旋预报技能方面的潜力，并为推进基于人工智能的对具有重大社会影响的极端天气事件的集合预测提供了宝贵的见解。</li>
</ul>

<h3>Title: Learning Interpretable Features in Audio Latent Spaces via Sparse Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Nathan Paek, Yongyi Zang, Qihui Yang, Randal Leistikow</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23802">https://arxiv.org/abs/2510.23802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23802">https://arxiv.org/pdf/2510.23802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23802]] Learning Interpretable Features in Audio Latent Spaces via Sparse Autoencoders(https://arxiv.org/abs/2510.23802)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>While sparse autoencoders (SAEs) successfully extract interpretable features from language models, applying them to audio generation faces unique challenges: audio's dense nature requires compression that obscures semantic meaning, and automatic feature characterization remains limited. We propose a framework for interpreting audio generative models by mapping their latent representations to human-interpretable acoustic concepts. We train SAEs on audio autoencoder latents, then learn linear mappings from SAE features to discretized acoustic properties (pitch, amplitude, and timbre). This enables both controllable manipulation and analysis of the AI music generation process, revealing how acoustic properties emerge during synthesis. We validate our approach on continuous (DiffRhythm-VAE) and discrete (EnCodec, WavTokenizer) audio latent spaces, and analyze DiffRhythm, a state-of-the-art text-to-music model, to demonstrate how pitch, timbre, and loudness evolve throughout generation. While our work is only done on audio modality, our framework can be extended to interpretable analysis of visual latent space generation models.</li>
<li><strong>摘要：</strong>虽然稀疏自动编码器 (SAE) 成功地从语言模型中提取可解释的特征，但将其应用于音频生成面临着独特的挑战：音频的密集性质需要压缩，从而掩盖语义含义，并且自动特征表征仍然有限。我们提出了一个框架，通过将音频生成模型的潜在表示映射到人类可解释的声学概念来解释音频生成模型。我们在音频自动编码器潜伏上训练 SAE，然后学习从 SAE 特征到离散声学属性（音高、振幅和音色）的线性映射。这使得人工智能音乐生成过程的可控操作和分析成为可能，揭示了合成过程中声学特性是如何出现的。我们在连续 (DiffRhythm-VAE) 和离散 (EnCodec、WavTokenizer) 音频潜在空间上验证我们的方法，并分析 DiffRhythm（一种最先进的文本到音乐模型），以演示音调、音色和响度在整个生成过程中如何演变。虽然我们的工作仅在音频模态上完成，但我们的框架可以扩展到视觉潜在空间生成模型的可解释分析。</li>
</ul>

<h3>Title: RareFlow: Physics-Aware Flow-Matching for Cross-Sensor Super-Resolution of Rare-Earth Features</h3>
<ul>
<li><strong>Authors: </strong>Forouzan Fallah, Wenwen Li, Chia-Yu Hsu, Hyunho Lee, Yezhou Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23816">https://arxiv.org/abs/2510.23816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23816">https://arxiv.org/pdf/2510.23816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23816]] RareFlow: Physics-Aware Flow-Matching for Cross-Sensor Super-Resolution of Rare-Earth Features(https://arxiv.org/abs/2510.23816)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generation</a></li>
<li><strong>Abstract: </strong>Super-resolution (SR) for remote sensing imagery often fails under out-of-distribution (OOD) conditions, such as rare geomorphic features captured by diverse sensors, producing visually plausible but physically inaccurate results. We present RareFlow, a physics-aware SR framework designed for OOD robustness. RareFlow's core is a dual-conditioning architecture. A Gated ControlNet preserves fine-grained geometric fidelity from the low-resolution input, while textual prompts provide semantic guidance for synthesizing complex features. To ensure physically sound outputs, we introduce a multifaceted loss function that enforces both spectral and radiometric consistency with sensor properties. Furthermore, the framework quantifies its own predictive uncertainty by employing a stochastic forward pass approach; the resulting output variance directly identifies unfamiliar inputs, mitigating feature hallucination. We validate RareFlow on a new, curated benchmark of multi-sensor satellite imagery. In blind evaluations, geophysical experts rated our model's outputs as approaching the fidelity of ground truth imagery, significantly outperforming state-of-the-art baselines. This qualitative superiority is corroborated by quantitative gains in perceptual metrics, including a nearly 40\% reduction in FID. RareFlow provides a robust framework for high-fidelity synthesis in data-scarce scientific domains and offers a new paradigm for controlled generation under severe domain shift.</li>
<li><strong>摘要：</strong>遥感图像的超分辨率 (SR) 在分布不均 (OOD) 条件下通常会失败，例如不同传感器捕获的罕见地貌特征，产生视觉上合理但物理上不准确的结果。我们推出 RareFlow，一种物理感知 SR 框架，专为 OOD 鲁棒性而设计。 RareFlow 的核心是双调节架构。门控 ControlNet 保留低分辨率输入的细粒度几何保真度，而文本提示则为合成复杂特征提供语义指导。为了确保物理上的声音输出，我们引入了多方面的损失函数，该函数强制光谱和辐射与传感器属性的一致性。此外，该框架通过采用随机前向传递方法来量化其自身的预测不确定性；由此产生的输出方差直接识别不熟悉的输入，从而减轻特征幻觉。我们在新的、精心策划的多传感器卫星图像基准上验证 RareFlow。在盲目评估中，地球物理专家认为我们模型的输出接近地面真实图像的保真度，明显优于最先进的基线。感知指标的定量收益证实了这种定性优势，包括 FID 减少近 40%。 RareFlow 为数据稀缺科学领域的高保真合成提供了一个强大的框架，并为严重领域转移下的受控生成提供了新的范例。</li>
</ul>

<h3>Title: GIFT: Group-relative Implicit Fine Tuning Integrates GRPO with DPO and UNA</h3>
<ul>
<li><strong>Authors: </strong>Zhichao Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23868">https://arxiv.org/abs/2510.23868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23868">https://arxiv.org/pdf/2510.23868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23868]] GIFT: Group-relative Implicit Fine Tuning Integrates GRPO with DPO and UNA(https://arxiv.org/abs/2510.23868)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>I propose \textbf{G}roup-relative \textbf{I}mplicit \textbf{F}ine \textbf{T}uning (GIFT), a novel reinforcement learning framework for aligning LLMs. Instead of directly maximizing cumulative rewards like PPO or GRPO, GIFT minimizes the discrepancy between implicit and explicit reward models. It combines three key ideas: (1) the online multi-response generation and normalization of GRPO, (2) the implicit reward formulation of DPO, and (3) the implicit-explicit reward alignment principle of UNA. By jointly normalizing the implicit and explicit rewards, GIFT eliminates an otherwise intractable term that prevents effective use of implicit rewards. This normalization transforms the complex reward maximization objective into a simple mean squared error (MSE) loss between the normalized reward functions, converting a non-convex optimization problem into a convex, stable, and analytically differentiable formulation. Unlike offline methods such as DPO and UNA, GIFT remains on-policy and thus retains exploration capability. Compared to GRPO, it requires fewer hyperparameters, converges faster, and generalizes better with significantly reduced training overfitting. Empirically, GIFT achieves superior reasoning and alignment performance on mathematical benchmarks while remaining computationally efficient.</li>
<li><strong>摘要：</strong>我提出 \textbf{G}roup-relative \textbf{I}mplicit \textbf{F}ine \textbf{T}uning (GIFT)，一种用于对齐 LLM 的新型强化学习框架。 GIFT 不像 PPO 或 GRPO 那样直接最大化累积奖励，而是最小化隐性奖励模型和显性奖励模型之间的差异。它结合了三个关键思想：（1）GRPO的在线多响应生成和标准化，（2）DPO的隐式奖励制定，以及（3）UNA的隐式-显式奖励对齐原则。通过共同标准化隐性奖励和显性奖励，GIFT 消除了一个阻碍隐性奖励有效使用的棘手术语。这种归一化将复杂的奖励最大化目标转换为归一化奖励函数之间的简单均方误差 (MSE) 损失，将非凸优化问题转换为凸、稳定且分析可微的公式。与 DPO 和 UNA 等离线方法不同，GIFT 保持在策略上，从而保留了探索能力。与 GRPO 相比，它需要的超参数更少，收敛速度更快，泛化能力更好，并且显着减少了训练过度拟合。根据经验，GIFT 在数学基准上实现了卓越的推理和对齐性能，同时保持了计算效率。</li>
</ul>

<h3>Title: TRELLISWorld: Training-Free World Generation from Object Generators</h3>
<ul>
<li><strong>Authors: </strong>Hanke Chen, Yuan Liu, Minchen Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23880">https://arxiv.org/abs/2510.23880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23880">https://arxiv.org/pdf/2510.23880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23880]] TRELLISWorld: Training-Free World Generation from Object Generators(https://arxiv.org/abs/2510.23880)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-driven 3D scene generation holds promise for a wide range of applications, from virtual prototyping to AR/VR and simulation. However, existing methods are often constrained to single-object generation, require domain-specific training, or lack support for full 360-degree viewability. In this work, we present a training-free approach to 3D scene synthesis by repurposing general-purpose text-to-3D object diffusion models as modular tile generators. We reformulate scene generation as a multi-tile denoising problem, where overlapping 3D regions are independently generated and seamlessly blended via weighted averaging. This enables scalable synthesis of large, coherent scenes while preserving local semantic control. Our method eliminates the need for scene-level datasets or retraining, relies on minimal heuristics, and inherits the generalization capabilities of object-level priors. We demonstrate that our approach supports diverse scene layouts, efficient generation, and flexible editing, establishing a simple yet powerful foundation for general-purpose, language-driven 3D scene construction.</li>
<li><strong>摘要：</strong>文本驱动的 3D 场景生成有望实现从虚拟原型到 AR/VR 和模拟的广泛应用。然而，现有方法通常仅限于单个对象生成，需要特定领域的训练，或者缺乏对完整 360 度可视性的支持。在这项工作中，我们通过将通用文本到 3D 对象扩散模型重新用作模块化图块生成器，提出了一种无需训练的 3D 场景合成方法。我们将场景生成重新表述为多图块去噪问题，其中重叠的 3D 区域独立生成并通过加权平均无缝混合。这使得大型、连贯场景的可扩展合成成为可能，同时保留本地语义控制。我们的方法消除了对场景级数据集或重新训练的需要，依赖于最小的启发式，并继承了对象级先验的泛化能力。我们证明了我们的方法支持多样化的场景布局、高效生成和灵活编辑，为通用、语言驱动的 3D 场景构建奠定了简单而强大的基础。</li>
</ul>

<h3>Title: DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning</h3>
<ul>
<li><strong>Authors: </strong>Eddison Pham, Prisha Priyadarshini, Adrian Maliackel, Kanishk Bandi, Cristian Meo, Kevin Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23907">https://arxiv.org/abs/2510.23907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23907">https://arxiv.org/pdf/2510.23907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23907]] DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning(https://arxiv.org/abs/2510.23907)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Scene-level captioning in instructional videos can enhance learning by requiring an understanding of both visual cues and temporal structure. By aligning visual cues with textual guidance, this understanding supports procedural learning and multimodal reasoning, providing a richer context for skill acquisition. However, captions that fail to capture this structure may lack coherence and quality, which can create confusion and undermine the video's educational intent. To address this gap, we introduce DynaStride, a pipeline to generate coherent, scene-level captions without requiring manual scene segmentation. Using the YouCookII dataset's scene annotations, DynaStride performs adaptive frame sampling and multimodal windowing to capture key transitions within each scene. It then employs a multimodal chain-of-thought process to produce multiple action-object pairs, which are refined and fused using a dynamic stride window selection algorithm that adaptively balances temporal context and redundancy. The final scene-level caption integrates visual semantics and temporal reasoning in a single instructional caption. Empirical evaluations against strong baselines, including VLLaMA3 and GPT-4o, demonstrate consistent gains on both N-gram-based metrics (BLEU, METEOR) and semantic similarity measures (BERTScore, CLIPScore). Qualitative analyses further show that DynaStride produces captions that are more temporally coherent and informative, suggesting a promising direction for improving AI-powered instructional content generation.</li>
<li><strong>摘要：</strong>教学视频中的场景级字幕需要理解视觉线索和时间结构，从而可以增强学习效果。通过将视觉线索与文本指导结合起来，这种理解支持程序学习和多模式推理，为技能获取提供更丰富的背景。然而，未能捕捉到这种结构的字幕可能缺乏连贯性和质量，这可能会造成混乱并破坏视频的教育意图。为了解决这一差距，我们引入了 DynaStride，这是一种无需手动场景分割即可生成连贯的场景级字幕的管道。使用 YouCookII 数据集的场景注释，DynaStride 执行自适应帧采样和多模态窗口以捕获每个场景内的关键转换。然后，它采用多模式思想链过程来生成多个动作-对象对，并使用自适应平衡时间上下文和冗余的动态步幅窗口选择算法对这些动作-对象对进行细化和融合。最终的场景级字幕将视觉语义和时间推理集成在单个教学字幕中。针对强基线（包括 VLLaMA3 和 GPT-4o）的实证评估表明，基于 N-gram 的指标（BLEU、METEOR）和语义相似性度量（BERTScore、CLIPScore）都取得了一致的收益。定性分析进一步表明，DynaStride 生成的字幕在时间上更加连贯且信息丰富，这为改进人工智能驱动的教学内容生成提供了一个有希望的方向。</li>
</ul>

<h3>Title: TurboPortrait3D: Single-step diffusion-based fast portrait novel-view synthesis</h3>
<ul>
<li><strong>Authors: </strong>Emily Kim, Julieta Martinez, Timur Bagautdinov, Jessica Hodgins</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23929">https://arxiv.org/abs/2510.23929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23929">https://arxiv.org/pdf/2510.23929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23929]] TurboPortrait3D: Single-step diffusion-based fast portrait novel-view synthesis(https://arxiv.org/abs/2510.23929)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce TurboPortrait3D: a method for low-latency novel-view synthesis of human portraits. Our approach builds on the observation that existing image-to-3D models for portrait generation, while capable of producing renderable 3D representations, are prone to visual artifacts, often lack of detail, and tend to fail at fully preserving the identity of the subject. On the other hand, image diffusion models excel at generating high-quality images, but besides being computationally expensive, are not grounded in 3D and thus are not directly capable of producing multi-view consistent outputs. In this work, we demonstrate that image-space diffusion models can be used to significantly enhance the quality of existing image-to-avatar methods, while maintaining 3D-awareness and running with low-latency. Our method takes a single frontal image of a subject as input, and applies a feedforward image-to-avatar generation pipeline to obtain an initial 3D representation and corresponding noisy renders. These noisy renders are then fed to a single-step diffusion model which is conditioned on input image(s), and is specifically trained to refine the renders in a multi-view consistent way. Moreover, we introduce a novel effective training strategy that includes pre-training on a large corpus of synthetic multi-view data, followed by fine-tuning on high-quality real images. We demonstrate that our approach both qualitatively and quantitatively outperforms current state-of-the-art for portrait novel-view synthesis, while being efficient in time.</li>
<li><strong>摘要：</strong>我们介绍 TurboPortrait3D：一种低延迟小说视角合成人物肖像的方法。我们的方法基于这样的观察：现有的用于肖像生成的图像到 3D 模型虽然能够生成可渲染的 3D 表示，但容易出现视觉伪影，通常缺乏细节，并且往往无法完全保留主体的身份。另一方面，图像扩散模型擅长生成高质量图像，但除了计算成本昂贵外，它并不以 3D 为基础，因此不能直接生成多视图一致的输出。在这项工作中，我们证明图像空间扩散模型可用于显着提高现有图像到头像方法的质量，同时保持 3D 感知并以低延迟运行。我们的方法采用主体的单个正面图像作为输入，并应用前馈图像到头像生成管道来获得初始 3D 表示和相应的噪声渲染。然后，这些噪声渲染被馈送到单步扩散模型，该模型以输入图像为条件，并经过专门训练，以多视图一致的方式细化渲染。此外，我们引入了一种新颖的有效训练策略，包括对大量合成多视图数据进行预训练，然后对高质量真实图像进行微调。我们证明，我们的方法在质量和数量上都优于当前肖像小说视图合成的最先进技术，同时在时间上高效。</li>
</ul>

<h3>Title: Neural USD: An object-centric framework for iterative editing and control</h3>
<ul>
<li><strong>Authors: </strong>Alejandro Escontrela, Shrinu Kushagra, Sjoerd van Steenkiste, Yulia Rubanova, Aleksander Holynski, Kelsey Allen, Kevin Murphy, Thomas Kipf</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23956">https://arxiv.org/abs/2510.23956</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23956">https://arxiv.org/pdf/2510.23956</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23956]] Neural USD: An object-centric framework for iterative editing and control(https://arxiv.org/abs/2510.23956)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Amazing progress has been made in controllable generative modeling, especially over the last few years. However, some challenges remain. One of them is precise and iterative object editing. In many of the current methods, trying to edit the generated image (for example, changing the color of a particular object in the scene or changing the background while keeping other elements unchanged) by changing the conditioning signals often leads to unintended global changes in the scene. In this work, we take the first steps to address the above challenges. Taking inspiration from the Universal Scene Descriptor (USD) standard developed in the computer graphics community, we introduce the "Neural Universal Scene Descriptor" or Neural USD. In this framework, we represent scenes and objects in a structured, hierarchical manner. This accommodates diverse signals, minimizes model-specific constraints, and enables per-object control over appearance, geometry, and pose. We further apply a fine-tuning approach which ensures that the above control signals are disentangled from one another. We evaluate several design considerations for our framework, demonstrating how Neural USD enables iterative and incremental workflows. More information at: this https URL .</li>
<li><strong>摘要：</strong>可控生成模型取得了惊人的进展，特别是在过去几年中。然而，一些挑战仍然存在。其中之一是精确和迭代的​​对象编辑。在当前的许多方法中，尝试通过改变条件信号来编辑生成的图像（例如，改变场景中特定对象的颜色或改变背景，同时保持其他元素不变）通常会导致场景中意外的全局变化。在这项工作中，我们采取了应对上述挑战的第一步。受到计算机图形学界开发的通用场景描述符（USD）标准的启发，我们引入了“神经通用场景描述符”或神经 USD。在这个框架中，我们以结构化、分层的方式表示场景和对象。这可以容纳不同的信号，最大限度地减少特定于模型的约束，并支持对每个对象的外观、几何形状和姿势进行控制。我们进一步应用一种微调方法，确保上述控制信号彼此分离。我们评估了框架的几个设计注意事项，展示了 Neural USD 如何实现迭代和增量工作流程。更多信息请访问：此 https URL 。</li>
</ul>

<h3>Title: SafeVision: Efficient Image Guardrail with Robust Policy Adherence and Explainability</h3>
<ul>
<li><strong>Authors: </strong>Peiyang Xu, Minzhou Pan, Zhaorun Chen, Shuang Yang, Chaowei Xiao, Bo Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23960">https://arxiv.org/abs/2510.23960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23960">https://arxiv.org/pdf/2510.23960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23960]] SafeVision: Efficient Image Guardrail with Robust Policy Adherence and Explainability(https://arxiv.org/abs/2510.23960)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>With the rapid proliferation of digital media, the need for efficient and transparent safeguards against unsafe content is more critical than ever. Traditional image guardrail models, constrained by predefined categories, often misclassify content due to their pure feature-based learning without semantic reasoning. Moreover, these models struggle to adapt to emerging threats, requiring costly retraining for new threats. To address these limitations, we introduce SafeVision, a novel image guardrail that integrates human-like reasoning to enhance adaptability and transparency. Our approach incorporates an effective data collection and generation framework, a policy-following training pipeline, and a customized loss function. We also propose a diverse QA generation and training strategy to enhance learning effectiveness. SafeVision dynamically aligns with evolving safety policies at inference time, eliminating the need for retraining while ensuring precise risk assessments and explanations. Recognizing the limitations of existing unsafe image benchmarks, which either lack granularity or cover limited risks, we introduce VisionHarm, a high-quality dataset comprising two subsets: VisionHarm Third-party (VisionHarm-T) and VisionHarm Comprehensive(VisionHarm-C), spanning diverse harmful categories. Through extensive experiments, we show that SafeVision achieves state-of-the-art performance on different benchmarks. SafeVision outperforms GPT-4o by 8.6% on VisionHarm-T and by 15.5% on VisionHarm-C, while being over 16x faster. SafeVision sets a comprehensive, policy-following, and explainable image guardrail with dynamic adaptation to emerging threats.</li>
<li><strong>摘要：</strong>随着数字媒体的快速普及，针对不安全内容采取高效、透明的防护措施的需求比以往任何时候都更加重要。传统的图像护栏模型受预定义类别的限制，由于其纯粹基于特征的学习而没有语义推理，因此经常对内容进行错误分类。此外，这些模型很难适应新出现的威胁，需要针对新威胁进行昂贵的再培训。为了解决这些限制，我们引入了 SafeVision，这是一种新颖的图像护栏，它集成了类人推理以增强适应性和透明度。我们的方法结合了有效的数据收集和生成框架、遵循策略的训练管道和定制的损失函数。我们还提出了多样化的 QA 生成和培训策略，以提高学习效率。 SafeVision 在推理时动态地与不断变化的安全策略保持一致，无需再培训，同时确保精确的风险评估和解释。认识到现有不安全图像基准的局限性，即缺乏粒度或覆盖的风险有限，我们引入了 VisionHarm，这是一个高质量的数据集，包含两个子集：VisionHarm 第三方 (VisionHarm-T​​) 和 VisionHarm 综合 (VisionHarm-C)，涵盖不同的有害类别。通过大量的实验，我们表明 SafeVision 在不同的基准上实现了最先进的性能。 SafeVision 在 VisionHarm-T​​ 上的性能比 GPT-4o 高 8.6%，在 VisionHarm-C 上的性能比 GPT-4o 高 15.5%，同时速度提高了 16 倍以上。 SafeVision 设置了全面、遵循策略且可解释的图像护栏，可动态适应新出现的威胁。</li>
</ul>

<h3>Title: Diffusion Adaptive Text Embedding for Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Byeonghu Na, Minsang Park, Gyuwon Sim, Donghyeok Shin, HeeSun Bae, Mina Kang, Se Jung Kwon, Wanmo Kang, Il-Chul Moon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23974">https://arxiv.org/abs/2510.23974</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23974">https://arxiv.org/pdf/2510.23974</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23974]] Diffusion Adaptive Text Embedding for Text-to-Image Diffusion Models(https://arxiv.org/abs/2510.23974)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models rely on text embeddings from a pre-trained text encoder, but these embeddings remain fixed across all diffusion timesteps, limiting their adaptability to the generative process. We propose Diffusion Adaptive Text Embedding (DATE), which dynamically updates text embeddings at each diffusion timestep based on intermediate perturbed data. We formulate an optimization problem and derive an update rule that refines the text embeddings at each sampling step to improve alignment and preference between the mean predicted image and the text. This allows DATE to dynamically adapts the text conditions to the reverse-diffused images throughout diffusion sampling without requiring additional model training. Through theoretical analysis and empirical results, we show that DATE maintains the generative capability of the model while providing superior text-image alignment over fixed text embeddings across various tasks, including multi-concept generation and text-guided image editing. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>文本到图像的扩散模型依赖于预先训练的文本编码器的文本嵌入，但这些嵌入在所有扩散时间步长中保持固定，限制了它们对生成过程的适应性。我们提出了扩散自适应文本嵌入（DATE），它根据中间扰动数据在每个扩散时间步动态更新文本嵌入。我们制定了一个优化问题并推导了一个更新规则，该规则在每个采样步骤中细化文本嵌入，以改善平均预测图像和文本之间的对齐和偏好。这使得 DATE 能够在整个扩散采样过程中动态调整文本条件以适应反向扩散图像，而无需额外的模型训练。通过理论分析和实证结果，我们表明 DATE 保持了模型的生成能力，同时在各种任务（包括多概念生成和文本引导图像编辑）中提供优于固定文本嵌入的文本图像对齐。我们的代码可以在这个 https URL 上找到。</li>
</ul>

<h3>Title: Synergistic Neural Forecasting of Air Pollution with Stochastic Sampling</h3>
<ul>
<li><strong>Authors: </strong>Yohan Abeysinghe, Muhammad Akhtar Munir, Sanoojan Baliah, Ron Sarafian, Fahad Shahbaz Khan, Yinon Rudich, Salman Khan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23977">https://arxiv.org/abs/2510.23977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23977">https://arxiv.org/pdf/2510.23977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23977]] Synergistic Neural Forecasting of Air Pollution with Stochastic Sampling(https://arxiv.org/abs/2510.23977)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Air pollution remains a leading global health and environmental risk, particularly in regions vulnerable to episodic air pollution spikes due to wildfires, urban haze and dust storms. Accurate forecasting of particulate matter (PM) concentrations is essential to enable timely public health warnings and interventions, yet existing models often underestimate rare but hazardous pollution events. Here, we present SynCast, a high-resolution neural forecasting model that integrates meteorological and air composition data to improve predictions of both average and extreme pollution levels. Built on a regionally adapted transformer backbone and enhanced with a diffusion-based stochastic refinement module, SynCast captures the nonlinear dynamics driving PM spikes more accurately than existing approaches. Leveraging on harmonized ERA5 and CAMS datasets, our model shows substantial gains in forecasting fidelity across multiple PM variables (PM$_1$, PM$_{2.5}$, PM$_{10}$), especially under extreme conditions. We demonstrate that conventional loss functions underrepresent distributional tails (rare pollution events) and show that SynCast, guided by domain-aware objectives and extreme value theory, significantly enhances performance in highly impacted regions without compromising global accuracy. This approach provides a scalable foundation for next-generation air quality early warning systems and supports climate-health risk mitigation in vulnerable regions.</li>
<li><strong>摘要：</strong>空气污染仍然是全球主要的健康和环境风险，特别是在因野火、城市雾霾和沙尘暴而容易遭受间歇性空气污染高峰的地区。准确预测颗粒物 (PM) 浓度对于及时发出公共卫生警报和干预措施至关重要，但现有模型往往低估罕见但危险的污染事件。在这里，我们推出了 SynCast，这是一种高分辨率神经预测模型，它集成了气象和空气成分数据，以改进对平均和极端污染水平的预测。 SynCast 建立在区域适应的变压器骨干上，并通过基于扩散的随机细化模块进行增强，比现有方法更准确地捕获驱动 PM 尖峰的非线性动态。利用统一的 ERA5 和 CAMS 数据集，我们的模型在多个 PM 变量（PM$_1$、PM$_{2.5}$、PM$_{10}$）的预测保真度方面显示出巨大的进步，特别是在极端条件下。我们证明了传统的损失函数不足以代表分布尾部（罕见的污染事件），并表明 SynCast 在领域感知目标和极值理论的指导下，可以显着提高受影响严重地区的性能，而不会影响全局准确性。这种方法为下一代空气质量预警系统提供了可扩展的基础，并支持缓解脆弱地区的气候健康风险。</li>
</ul>

<h3>Title: Efficient Cost-and-Quality Controllable Arbitrary-scale Super-resolution with Fourier Constraints</h3>
<ul>
<li><strong>Authors: </strong>Kazutoshi Akita, Norimichi Ukita</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23978">https://arxiv.org/abs/2510.23978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23978">https://arxiv.org/pdf/2510.23978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23978]] Efficient Cost-and-Quality Controllable Arbitrary-scale Super-resolution with Fourier Constraints(https://arxiv.org/abs/2510.23978)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Cost-and-Quality (CQ) controllability in arbitrary-scale super-resolution is crucial. Existing methods predict Fourier components one by one using a recurrent neural network. However, this approach leads to performance degradation and inefficiency due to independent prediction. This paper proposes predicting multiple components jointly to improve both quality and efficiency.</li>
<li><strong>摘要：</strong>任意尺度超分辨率中的成本与质量（CQ）可控性至关重要。现有方法使用循环神经网络一一预测傅立叶分量。然而，这种方法由于独立预测而导致性能下降和效率低下。本文提出联合预测多个组件以提高质量和效率。</li>
</ul>

<h3>Title: Training-Free Safe Text Embedding Guidance for Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Byeonghu Na, Mina Kang, Jiseok Kwak, Minsang Park, Jiwoo Shin, SeJoon Jun, Gayoung Lee, Jin-Hwa Kim, Il-Chul Moon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.24012">https://arxiv.org/abs/2510.24012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.24012">https://arxiv.org/pdf/2510.24012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.24012]] Training-Free Safe Text Embedding Guidance for Text-to-Image Diffusion Models(https://arxiv.org/abs/2510.24012)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-image models have recently made significant advances in generating realistic and semantically coherent images, driven by advanced diffusion models and large-scale web-crawled datasets. However, these datasets often contain inappropriate or biased content, raising concerns about the generation of harmful outputs when provided with malicious text prompts. We propose Safe Text embedding Guidance (STG), a training-free approach to improve the safety of diffusion models by guiding the text embeddings during sampling. STG adjusts the text embeddings based on a safety function evaluated on the expected final denoised image, allowing the model to generate safer outputs without additional training. Theoretically, we show that STG aligns the underlying model distribution with safety constraints, thereby achieving safer outputs while minimally affecting generation quality. Experiments on various safety scenarios, including nudity, violence, and artist-style removal, show that STG consistently outperforms both training-based and training-free baselines in removing unsafe content while preserving the core semantic intent of input prompts. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>在先进的扩散模型和大规模网络爬行数据集的推动下，文本到图像模型最近在生成真实且语义一致的图像方面取得了重大进展。然而，这些数据集通常包含不适当或有偏见的内容，引发人们对在提供恶意文本提示时生成有害输出的担忧。我们提出了安全文本嵌入指南（STG），这是一种无需训练的方法，通过在采样过程中指导文本嵌入来提高扩散模型的安全性。 STG 根据对预期最终去噪图像评估的安全函数来调整文本嵌入，从而使模型无需额外训练即可生成更安全的输出。理论上，我们表明 STG 将底层模型分布与安全约束保持一致，从而实现更安全的输出，同时对发电质量的影响最小。对各种安全场景（包括裸体、暴力和艺术家式删除）的实验表明，STG 在删除不安全内容同时保留输入提示的核心语义意图方面始终优于基于训练和无训练的基线。我们的代码可以在这个 https URL 上找到。</li>
</ul>

<h3>Title: AutoPrompt: Automated Red-Teaming of Text-to-Image Models via LLM-Driven Adversarial Prompts</h3>
<ul>
<li><strong>Authors: </strong>Yufan Liu, Wanqian Zhang, Huashan Chen, Lin Wang, Xiaojun Jia, Zheng Lin, Weiping Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.24034">https://arxiv.org/abs/2510.24034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.24034">https://arxiv.org/pdf/2510.24034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.24034]] AutoPrompt: Automated Red-Teaming of Text-to-Image Models via LLM-Driven Adversarial Prompts(https://arxiv.org/abs/2510.24034)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite rapid advancements in text-to-image (T2I) models, their safety mechanisms are vulnerable to adversarial prompts, which maliciously generate unsafe images. Current red-teaming methods for proactively assessing such vulnerabilities usually require white-box access to T2I models, and rely on inefficient per-prompt optimization, as well as inevitably generate semantically meaningless prompts easily blocked by filters. In this paper, we propose APT (AutoPrompT), a black-box framework that leverages large language models (LLMs) to automatically generate human-readable adversarial suffixes for benign prompts. We first introduce an alternating optimization-finetuning pipeline between adversarial suffix optimization and fine-tuning the LLM utilizing the optimized suffix. Furthermore, we integrates a dual-evasion strategy in optimization phase, enabling the bypass of both perplexity-based filter and blacklist word filter: (1) we constrain the LLM generating human-readable prompts through an auxiliary LLM perplexity scoring, which starkly contrasts with prior token-level gibberish, and (2) we also introduce banned-token penalties to suppress the explicit generation of banned-tokens in blacklist. Extensive experiments demonstrate the excellent red-teaming performance of our human-readable, filter-resistant adversarial prompts, as well as superior zero-shot transferability which enables instant adaptation to unseen prompts and exposes critical vulnerabilities even in commercial APIs (e.g., this http URL.).</li>
<li><strong>摘要：</strong>尽管文本到图像（T2I）模型取得了快速进步，但它们的安全机制很容易受到对抗性提示的影响，从而恶意生成不安全的图像。当前主动评估此类漏洞的红队方法通常需要对 T2I 模型进行白盒访问，并且依赖于低效的每个提示优化，并且不可避免地会生成容易被过滤器阻止的语义上无意义的提示。在本文中，我们提出了 APT (AutoPrompT)，这是一种黑盒框架，利用大型语言模型 (LLM) 自动生成人类可读的对抗性后缀以实现良性提示。我们首先在对抗性后缀优化和利用优化后缀微调 LLM 之间引入交替优化微调管道。此外，我们在优化阶段集成了双重规避策略，能够绕过基于困惑的过滤器和黑名单单词过滤器：（1）我们通过辅助LLM困惑评分来限制LLM生成人类可读的提示，这与之前的令牌级乱码形成鲜明对比，（2）我们还引入了禁止令牌惩罚来抑制黑名单中禁止令牌的显式生成。大量的实验证明了我们的人类可读、抗过滤的对抗性提示具有出色的红队性能，以及卓越的零样本可转移性，即使在商业 API（例如，此 http URL）中也能即时适应看不见的提示并暴露关键漏洞。</li>
</ul>

<h3>Title: Kernelized Sparse Fine-Tuning with Bi-level Parameter Competition for Vision Models</h3>
<ul>
<li><strong>Authors: </strong>Shufan Shen, Junshu Sun, Shuhui Wang, Qingming Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.24037">https://arxiv.org/abs/2510.24037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.24037">https://arxiv.org/pdf/2510.24037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.24037]] Kernelized Sparse Fine-Tuning with Bi-level Parameter Competition for Vision Models(https://arxiv.org/abs/2510.24037)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Parameter-efficient fine-tuning (PEFT) aims to adapt pre-trained vision models to downstream tasks. Among PEFT paradigms, sparse tuning achieves remarkable performance by adjusting only the weights most relevant to downstream tasks, rather than densely tuning the entire weight matrix. Current methods follow a two-stage paradigm. First, it locates task-relevant weights by gradient information, which overlooks the parameter adjustments during fine-tuning and limits the performance. Second, it updates only the located weights by applying a sparse mask to the gradient of the weight matrix, which results in high memory usage due to the storage of all weight matrices in the optimizer. In this paper, we propose a one-stage method named SNELLA to overcome the above limitations. For memory usage, SNELLA selectively updates the weight matrix by adding it to another sparse matrix that is merged by two low-rank learnable matrices. We extend the low-rank decomposition by introducing nonlinear kernel functions, thereby increasing the rank of the resulting merged matrix to prevent the interdependency among weight updates, enabling better adaptation to downstream tasks. For locating task-relevant weights, we propose an adaptive bi-level sparsity allocation mechanism that encourages weights to compete across and inside layers based on their importance scores in an end-to-end manner. Extensive experiments are conducted on classification, segmentation, and generation tasks using different pre-trained vision models. The results show that SNELLA achieves SOTA performance with low memory usage. Notably, SNELLA obtains 1.8% (91.9% v.s. 90.1%) higher Top-1 accuracy on the FGVC benchmark compared to SPT-LoRA. Compared to previous methods, SNELLA achieves a memory reduction of 31.1%-39.9% across models with parameter scales from 86M to 632M. Our source codes are available at this https URL.</li>
<li><strong>摘要：</strong>参数高效微调（PEFT）旨在使预先训练的视觉模型适应下游任务。在 PEFT 范式中，稀疏调整通过仅调整与下游任务最相关的权重而不是密集调整整个权重矩阵来实现卓越的性能。当前的方法遵循两阶段范例。首先，它通过梯度信息来定位与任务相关的权重，这忽略了微调过程中的参数调整并限制了性能。其次，它通过对权重矩阵的梯度应用稀疏掩码来仅更新定位的权重，由于优化器中存储了所有权重矩阵，因此导致较高的内存使用率。在本文中，我们提出了一种名为 SNELLA 的单阶段方法来克服上述限制。对于内存使用，SNELLA 通过将权重矩阵添加到另一个由两个低秩可学习矩阵合并的稀疏矩阵来选择性地更新权重矩阵。我们通过引入非线性核函数来扩展低秩分解，从而提高所得合并矩阵的秩，以防止权重更新之间的相互依赖，从而更好地适应下游任务。为了定位与任务相关的权重，我们提出了一种自适应双层稀疏分配机制，该机制鼓励权重以端到端的方式根据其重要性分数在层间和层内进行竞争。使用不同的预训练视觉模型对分类、分割和生成任务进行了大量的实验。结果表明，SNELLA 在低内存占用的情况下实现了 SOTA 性能。值得注意的是，与 SPT-LoRA 相比，SNELLA 在 FGVC 基准上的 Top-1 准确率提高了 1.8%（91.9% vs 90.1%）。与之前的方法相比，SNELLA 在参数规模从 86M 到 632M 的模型中实现了 31.1%-39.9% 的内存减少。我们的源代码可通过此 https URL 获取。</li>
</ul>

<h3>Title: Causal-Aware Generative Adversarial Networks with Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Tu Anh Hoang Nguyen, Dang Nguyen, Tri-Nhan Vo, Thuc Duy Le, Sunil Gupta</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.24046">https://arxiv.org/abs/2510.24046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.24046">https://arxiv.org/pdf/2510.24046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.24046]] Causal-Aware Generative Adversarial Networks with Reinforcement Learning(https://arxiv.org/abs/2510.24046)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The utility of tabular data for tasks ranging from model training to large-scale data analysis is often constrained by privacy concerns or regulatory hurdles. While existing data generation methods, particularly those based on Generative Adversarial Networks (GANs), have shown promise, they frequently struggle with capturing complex causal relationship, maintaining data utility, and providing provable privacy guarantees suitable for enterprise deployment. We introduce CA-GAN, a novel generative framework specifically engineered to address these challenges for real-world tabular datasets. CA-GAN utilizes a two-step approach: causal graph extraction to learn a robust, comprehensive causal relationship in the data's manifold, followed by a custom Conditional WGAN-GP (Wasserstein GAN with Gradient Penalty) that operates exclusively as per the structure of nodes in the causal graph. More importantly, the generator is trained with a new Reinforcement Learning-based objective that aligns the causal graphs constructed from real and fake data, ensuring the causal awareness in both training and sampling phases. We demonstrate CA-GAN superiority over six SOTA methods across 14 tabular datasets. Our evaluations, focused on core data engineering metrics: causal preservation, utility preservation, and privacy preservation. Our method offers a practical, high-performance solution for data engineers seeking to create high-quality, privacy-compliant synthetic datasets to benchmark database systems, accelerate software development, and facilitate secure data-driven research.</li>
<li><strong>摘要：</strong>表格数据在从模型训练到大规模数据分析等任务中的效用通常受到隐私问题或监管障碍的限制。虽然现有的数据生成方法，特别是基于生成对抗网络 (GAN) 的数据生成方法，已经显示出前景，但它们经常在捕获复杂的因果关系、维护数据实用性以及提供适合企业部署的可证明的隐私保证方面遇到困难。我们引入了 CA-GAN，这是一种专门为解决现实世界表格数据集的这些挑战而设计的新型生成框架。 CA-GAN 采用​​两步方法：因果图提取来学习数据流形中稳健、全面的因果关系，然后是自定义条件 WGAN-GP（带有梯度惩罚的 Wasserstein GAN），专门根据因果图中的节点结构进行操作。更重要的是，生成器接受了基于强化学习的新目标的训练，该目标对齐由真实数据和虚假数据构建的因果图，确保训练和采样阶段的因果意识。我们在 14 个表格数据集中证明了 CA-GAN 相对于 6 种 SOTA 方法的优越性。我们的评估重点关注核心数据工程指标：因果保存、效用保存和隐私保护。我们的方法为寻求创建高质量、符合隐私的合成数据集来对数据库系统进行基准测试、加速软件开发并促进安全的数据驱动研究的数据工程师提供了实用的高性能解决方案。</li>
</ul>

<h3>Title: Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification</h3>
<ul>
<li><strong>Authors: </strong>William Yang, Xindi Wu, Zhiwei Deng, Esin Tureci, Olga Russakovsky</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.24078">https://arxiv.org/abs/2510.24078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.24078">https://arxiv.org/pdf/2510.24078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.24078]] Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification(https://arxiv.org/abs/2510.24078)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) models are increasingly used for synthetic dataset generation, but generating effective synthetic training data for classification remains challenging. Fine-tuning a T2I model with a few real examples can help improve the quality of synthetic training data; however, it may also cause overfitting and reduce diversity in the generated samples. We propose a fine-tuning strategy BOB (BeyondOBjects) to mitigate these concerns for fine-grained classification. Given a small set of real examples, we first extract class-agnostic attributes such as scene background and object pose. We then explicitly condition on these attributes during fine-tuning of the T2I model and marginalize them out during generation. This design mitigates overfitting, preserves the T2I model's generative prior, reduces estimation errors, and further minimizes unintended inter-class associations. Extensive experiments across multiple T2I models, backbones, and datasets show that our method achieves state-of-the-art performance in low-shot fine-grained classification when augmented with synthetic data. Concretely, BOB outperforms DataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning a CLIP classifier with five real images augmented with 100 synthetic images). In three of the four benchmarks, fine-tuning downstream models with 5 real images augmented with BOB achieves better performance than fine-tuning with 10 real images. Collectively, BOB outperforms prior art in 18 of 24 experimental settings, with 2+% accuracy improvements in 14 of these settings.</li>
<li><strong>摘要：</strong>文本到图像 (T2I) 模型越来越多地用于合成数据集生成，但生成用于分类的有效合成训练数据仍然具有挑战性。用一些真实的例子来微调 T2I 模型可以帮助提高合成训练数据的质量；然而，它也可能导致过度拟合并减少生成样本的多样性。我们提出了一种微调策略 BOB（BeyondOBjects）来减轻这些对细粒度分类的担忧。给定一小组真实示例，我们首先提取与类别无关的属性，例如场景背景和对象姿势。然后，我们在 T2I 模型的微调过程中明确条件化这些属性，并在生成过程中将它们边缘化。这种设计减轻了过度拟合，保留了 T2I 模型的生成先验，减少了估计误差，并进一步最小化了意外的类间关联。跨多个 T2I 模型、骨干网和数据集的大量实验表明，当使用合成数据进行增强时，我们的方法在低样本细粒度分类中实现了最先进的性能。具体来说，BOB 在 Aircraft 数据集上的表现比 DataDream 高出 7.4%（当使用 5 个真实图像和 100 个合成图像增强的 CLIP 分类器进行微调时，从 50.0% 提高到 57.4%）。在四个基准测试中的三个中，使用 BOB 增强的 5 个真实图像微调下游模型比使用 10 个真实图像进行微调获得了更好的性能。总的来说，BOB 在 24 个实验设置中的 18 个中优于现有技术，其中 14 个设置的准确度提高了 2% 以上。</li>
</ul>

<h3>Title: Graph-Guided Concept Selection for Efficient Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Ziyu Liu, Yijing Liu, Jianfei Yuan, Minzhi Yan, Le Yue, Honghui Xiong, Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.24120">https://arxiv.org/abs/2510.24120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.24120">https://arxiv.org/pdf/2510.24120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.24120]] Graph-Guided Concept Selection for Efficient Retrieval-Augmented Generation(https://arxiv.org/abs/2510.24120)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Graph-based RAG constructs a knowledge graph (KG) from text chunks to enhance retrieval in Large Language Model (LLM)-based question answering. It is especially beneficial in domains such as biomedicine, law, and political science, where effective retrieval often involves multi-hop reasoning over proprietary documents. However, these methods demand numerous LLM calls to extract entities and relations from text chunks, incurring prohibitive costs at scale. Through a carefully designed ablation study, we observe that certain words (termed concepts) and their associated documents are more important. Based on this insight, we propose Graph-Guided Concept Selection (G2ConS). Its core comprises a chunk selection method and an LLM-independent concept graph. The former selects salient document chunks to reduce KG construction costs; the latter closes knowledge gaps introduced by chunk selection at zero cost. Evaluations on multiple real-world datasets show that G2ConS outperforms all baselines in construction cost, retrieval effectiveness, and answering quality.</li>
<li><strong>摘要：</strong>基于图的 RAG 从文本块构建知识图 (KG)，以增强基于大语言模型 (LLM) 的问答中的检索。它在生物医学、法律和政治学等领域尤其有用，这些领域的有效检索通常涉及对专有文档的多跳推理。然而，这些方法需要大量的 LLM 调用来从文本块中提取实体和关系，从而导致大规模成本过高。通过精心设计的消融研究，我们观察到某些单词（称为概念）及其相关文档更为重要。基于这一见解，我们提出了图引导概念选择（G2ConS）。其核心包括块选择方法和独立于LLM的概念图。前者选择显着的文档块来降低KG构建成本；后者以零成本弥补了块选择带来的知识差距。对多个真实世界数据集的评估表明，G2ConS 在构建成本、检索效率和回答质量方面优于所有基线。</li>
</ul>

<h3>Title: ETC: training-free diffusion models acceleration with Error-aware Trend Consistency</h3>
<ul>
<li><strong>Authors: </strong>Jiajian Xie, Hubery Yin, Chen Li, Zhou Zhao, Shengyu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.24129">https://arxiv.org/abs/2510.24129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.24129">https://arxiv.org/pdf/2510.24129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.24129]] ETC: training-free diffusion models acceleration with Error-aware Trend Consistency(https://arxiv.org/abs/2510.24129)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved remarkable generative quality but remain bottlenecked by costly iterative sampling. Recent training-free methods accelerate diffusion process by reusing model outputs. However, these methods ignore denoising trends and lack error control for model-specific tolerance, leading to trajectory deviations under multi-step reuse and exacerbating inconsistencies in the generated results. To address these issues, we introduce Error-aware Trend Consistency (ETC), a framework that (1) introduces a consistent trend predictor that leverages the smooth continuity of diffusion trajectories, projecting historical denoising patterns into stable future directions and progressively distributing them across multiple approximation steps to achieve acceleration without deviating; (2) proposes a model-specific error tolerance search mechanism that derives corrective thresholds by identifying transition points from volatile semantic planning to stable quality refinement. Experiments show that ETC achieves a 2.65x acceleration over FLUX with negligible (-0.074 SSIM score) degradation of consistency.</li>
<li><strong>摘要：</strong>扩散模型已经取得了显着的生成质量，但仍然受到昂贵的迭代采样的瓶颈。最近的免训练方法通过重用模型输出来加速扩散过程。然而，这些方法忽略了去噪趋势，并且缺乏对模型特定容差的误差控制，导致多步骤重用下的轨迹偏差，并加剧了生成结果的不一致。为了解决这些问题，我们引入了错误感知趋势一致性（ETC），该框架（1）引入了一致的趋势预测器，利用扩散轨迹的平滑连续性，将历史去噪模式投影到稳定的未来方向，并逐步将它们分布在多个近似步骤中，以实现不偏离的加速； (2) 提出了一种特定于模型的容错搜索机制，该机制通过识别从不稳定的语义规划到稳定的质量细化的过渡点来导出纠正阈值。实验表明，ETC 比 FLUX 实现了 2.65 倍的加速，并且一致性下降可以忽略不计（-0.074 SSIM 分数）。</li>
</ul>

<h3>Title: Compositional Image Synthesis with Inference-Time Scaling</h3>
<ul>
<li><strong>Authors: </strong>Minsuk Ji, Sanghyeok Lee, Namhyuk Ahn</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.24133">https://arxiv.org/abs/2510.24133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.24133">https://arxiv.org/pdf/2510.24133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.24133]] Compositional Image Synthesis with Inference-Time Scaling(https://arxiv.org/abs/2510.24133)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite their impressive realism, modern text-to-image models still struggle with compositionality, often failing to render accurate object counts, attributes, and spatial relations. To address this challenge, we present a training-free framework that combines an object-centric approach with self-refinement to improve layout faithfulness while preserving aesthetic quality. Specifically, we leverage large language models (LLMs) to synthesize explicit layouts from input prompts, and we inject these layouts into the image generation process, where a object-centric vision-language model (VLM) judge reranks multiple candidates to select the most prompt-aligned outcome iteratively. By unifying explicit layout-grounding with self-refine-based inference-time scaling, our framework achieves stronger scene alignment with prompts compared to recent text-to-image models. The code are available at this https URL.</li>
<li><strong>摘要：</strong>尽管具有令人印象深刻的真实感，现代文本到图像模型仍然在组合性方面遇到困难，通常无法渲染准确的对象计数、属性和空间关系。为了应对这一挑战，我们提出了一个免培训的框架，它将以对象为中心的方法与自我完善相结合，以提高布局的忠实度，同时保持美学质量。具体来说，我们利用大型语言模型（LLM）根据输入提示合成显式布局，并将这些布局注入到图像生成过程中，其中以对象为中心的视觉语言模型（VLM）法官对多个候选者重新排序，以迭代地选择最符合提示的结果。通过将显式布局基础与基于自我优化的推理时间缩放相结合，与最近的文本到图像模型相比，我们的框架通过提示实现了更强的场景对齐。该代码可从此 https URL 获取。</li>
</ul>

<h3>Title: VC4VG: Optimizing Video Captions for Text-to-Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Yang Du, Zhuoran Lin, Kaiqiang Song, Biao Wang, Zhicheng Zheng, Tiezheng Ge, Bo Zheng, Qin Jin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.24134">https://arxiv.org/abs/2510.24134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.24134">https://arxiv.org/pdf/2510.24134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.24134]] VC4VG: Optimizing Video Captions for Text-to-Video Generation(https://arxiv.org/abs/2510.24134)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in text-to-video (T2V) generation highlight the critical role of high-quality video-text pairs in training models capable of producing coherent and instruction-aligned videos. However, strategies for optimizing video captions specifically for T2V training remain underexplored. In this paper, we introduce VC4VG (Video Captioning for Video Generation), a comprehensive caption optimization framework tailored to the needs of T2V this http URL begin by analyzing caption content from a T2V perspective, decomposing the essential elements required for video reconstruction into multiple dimensions, and proposing a principled caption design methodology. To support evaluation, we construct VC4VG-Bench, a new benchmark featuring fine-grained, multi-dimensional, and necessity-graded metrics aligned with T2V-specific this http URL T2V fine-tuning experiments demonstrate a strong correlation between improved caption quality and video generation performance, validating the effectiveness of our approach. We release all benchmark tools and code at this https URL to support further research.</li>
<li><strong>摘要：</strong>文本到视频 (T2V) 生成的最新进展凸显了高质量视频文本对在能够生成连贯且指令一致的视频的训练模型中的关键作用。然而，专门针对 T2V 训练优化视频字幕的策略仍未得到充分探索。在本文中，我们介绍了 VC4VG（Video Captioning for Video Generation），这是一个针对 T2V 需求量身定制的综合字幕优化框架，该 http URL 首先从 T2V 角度分析字幕内容，将视频重建所需的基本要素分解为多个维度，并提出原则性的字幕设计方法。为了支持评估，我们构建了 VC4VG-Bench，这是一个新的基准，具有细粒度、多维度和必要性分级指标，与 T2V 特定的此 http URL T2V 微调实验证明了改进的字幕质量和视频生成性能之间的强相关性，验证了我们方法的有效性。我们在此 https URL 发布所有基准测试工具和代码以支持进一步的研究。</li>
</ul>

<h3>Title: V-SAT: Video Subtitle Annotation Tool</h3>
<ul>
<li><strong>Authors: </strong>Arpita Kundu, Joyita Chakraborty, Anindita Desarkar, Aritra Sen, Srushti Anil Patil, Vishwanathan Raman</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.24180">https://arxiv.org/abs/2510.24180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.24180">https://arxiv.org/pdf/2510.24180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.24180]] V-SAT: Video Subtitle Annotation Tool(https://arxiv.org/abs/2510.24180)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The surge of audiovisual content on streaming platforms and social media has heightened the demand for accurate and accessible subtitles. However, existing subtitle generation methods primarily speech-based transcription or OCR-based extraction suffer from several shortcomings, including poor synchronization, incorrect or harmful text, inconsistent formatting, inappropriate reading speeds, and the inability to adapt to dynamic audio-visual contexts. Current approaches often address isolated issues, leaving post-editing as a labor-intensive and time-consuming process. In this paper, we introduce V-SAT (Video Subtitle Annotation Tool), a unified framework that automatically detects and corrects a wide range of subtitle quality issues. By combining Large Language Models(LLMs), Vision-Language Models (VLMs), Image Processing, and Automatic Speech Recognition (ASR), V-SAT leverages contextual cues from both audio and video. Subtitle quality improved, with the SUBER score reduced from 9.6 to 3.54 after resolving all language mode issues and F1-scores of ~0.80 for image mode issues. Human-in-the-loop validation ensures high-quality results, providing the first comprehensive solution for robust subtitle annotation.</li>
<li><strong>摘要：</strong>流媒体平台和社交媒体上视听内容的激增提高了对准确且易于理解的字幕的需求。然而，现有的字幕生成方法主要是基于语音的转录或基于 OCR 的提取，但存在一些缺点，包括同步性差、文本不正确或有害、格式不一致、阅读速度不合适以及无法适应动态视听上下文。当前的方法通常解决孤立的问题，使译后编辑成为一个劳动密集型且耗时的过程。在本文中，我们介绍了 V-SAT（视频字幕注释工具），这是一个统一的框架，可以自动检测并纠正各种字幕质量问题。通过结合大型语言模型 (LLM)、视觉语言模型 (VLM)、图像处理和自动语音识别 (ASR)，V-SAT 利用音频和视频的上下文线索。字幕质量得到改善，解决所有语言模式问题后，SUBER 分数从 9.6 降至 3.54，图像模式问题的 F1 分数约为 0.80。人机交互验证可确保高质量的结果，为稳健的字幕注释提供第一个全面的解决方案。</li>
</ul>

<h3>Title: MC-SJD : Maximal Coupling Speculative Jacobi Decoding for Autoregressive Visual Generation Acceleration</h3>
<ul>
<li><strong>Authors: </strong>Junhyuk So, Hyunho Kook, Chaeyeon Jang, Eunhyeok Park</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.24211">https://arxiv.org/abs/2510.24211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.24211">https://arxiv.org/pdf/2510.24211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.24211]] MC-SJD : Maximal Coupling Speculative Jacobi Decoding for Autoregressive Visual Generation Acceleration(https://arxiv.org/abs/2510.24211)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While autoregressive (AR) modeling has recently emerged as a new paradigm in visual generation, its practical adoption is severely constrained by the slow inference speed of per-token generation, which often requires thousands of steps to produce a single sample. To address this challenge, we propose MC-SJD, a training-free, lossless parallel decoding framework designed to accelerate AR visual generation by extending the recently introduced Speculative Jacobi Decoding (SJD). Although SJD shows strong potential for accelerating AR generation, we demonstrate that token instability across iterations significantly reduces the acceptance rate, a limitation that primarily arises from the independent sampling process used during draft token generation. To overcome this, we introduce MC-SJD, an information-theoretic approach based on coupling, which substantially accelerates standard SJD by maximizing the probability of sampling identical draft tokens across consecutive iterations, all while preserving its lossless property. Remarkably, this method requires only a single-line modification to the existing algorithm, yet achieves substantial performance gains, delivering up to a ~4.2x acceleration in image generation and ~13.3x acceleration in video generation compared to standard AR decoding, without any degradation in output quality.</li>
<li><strong>摘要：</strong>虽然自回归 (AR) 建模最近已成为视觉生成中的新范例，但其实际采用受到每个标记生成的推理速度缓慢的严重限制，这通常需要数千个步骤才能生成单个样本。为了应对这一挑战，我们提出了 MC-SJD，这是一种免训练、无损并行解码框架，旨在通过扩展最近引入的推测雅可比解码 (SJD) 来加速 AR 视觉生成。尽管 SJD 显示出加速 AR 生成的强大潜力，但我们证明迭代过程中的令牌不稳定会显着降低接受率，这一限制主要来自草稿令牌生成过程中使用的独立采样过程。为了克服这个问题，我们引入了 MC-SJD，这是一种基于耦合的信息论方法，它通过在连续迭代中最大化采样相同草案令牌的概率来显着加速标准 SJD，同时保留其无损属性。值得注意的是，该方法仅需要对现有算法进行单行修改，但却实现了显着的性能提升，与标准 AR 解码相比，图像生成速度提高了约 4.2 倍，视频生成速度提高了约 13.3 倍，而且输出质量没有任何下降。</li>
</ul>

<h3>Title: Delving into Cascaded Instability: A Lipschitz Continuity View on Image Restoration and Object Detection Synergy</h3>
<ul>
<li><strong>Authors: </strong>Qing Zhao, Weijian Deng, Pengxu Wei, ZiYi Dong, Hannan Lu, Xiangyang Ji, Liang Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.24232">https://arxiv.org/abs/2510.24232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.24232">https://arxiv.org/pdf/2510.24232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.24232]] Delving into Cascaded Instability: A Lipschitz Continuity View on Image Restoration and Object Detection Synergy(https://arxiv.org/abs/2510.24232)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>To improve detection robustness in adverse conditions (e.g., haze and low light), image restoration is commonly applied as a pre-processing step to enhance image quality for the detector. However, the functional mismatch between restoration and detection networks can introduce instability and hinder effective integration -- an issue that remains underexplored. We revisit this limitation through the lens of Lipschitz continuity, analyzing the functional differences between restoration and detection networks in both the input space and the parameter space. Our analysis shows that restoration networks perform smooth, continuous transformations, while object detectors operate with discontinuous decision boundaries, making them highly sensitive to minor perturbations. This mismatch introduces instability in traditional cascade frameworks, where even imperceptible noise from restoration is amplified during detection, disrupting gradient flow and hindering optimization. To address this, we propose Lipschitz-regularized object detection (LROD), a simple yet effective framework that integrates image restoration directly into the detector's feature learning, harmonizing the Lipschitz continuity of both tasks during training. We implement this framework as Lipschitz-regularized YOLO (LR-YOLO), extending seamlessly to existing YOLO detectors. Extensive experiments on haze and low-light benchmarks demonstrate that LR-YOLO consistently improves detection stability, optimization smoothness, and overall accuracy.</li>
<li><strong>摘要：</strong>为了提高不利条件（例如雾霾和低光）下的检测鲁棒性，图像恢复通常用作预处理步骤，以提高检测器的图像质量。然而，恢复网络和检测网络之间的功能不匹配可能会带来不稳定并阻碍有效集成——这个问题仍未得到充分探索。我们通过 Lipschitz 连续性的视角重新审视这一限制，分析恢复网络和检测网络在输入空间和参数空间中的功能差异。我们的分析表明，恢复网络执行平滑、连续的转换，而目标检测器则以不连续的决策边界运行，这使得它们对微小的扰动高度敏感。这种不匹配会导致传统级联框架的不稳定性，在检测过程中，即使是难以察觉的恢复噪声也会被放大，从而扰乱梯度流并阻碍优化。为了解决这个问题，我们提出了 Lipschitz 正则化目标检测（LROD），这是一个简单而有效的框架，它将图像恢复直接集成到检测器的特征学习中，从而在训练期间协调两项任务的 Lipschitz 连续性。我们将此框架实现为 Lipschitz 正则化 YOLO (LR-YOLO)，无缝扩展到现有的 YOLO 检测器。对雾度和低光基准的大量实验表明，LR-YOLO 持续提高了检测稳定性、优化平滑度和整体准确性。</li>
</ul>

<h3>Title: PRIVET: Privacy Metric Based on Extreme Value Theory</h3>
<ul>
<li><strong>Authors: </strong>Antoine Szatkownik (TAU, BioInfo), Aurélien Decelle, Beatriz Seoane (TAU), Nicolas Bereux (TAU), Léo Planche (BioInfo), Guillaume Charpiat (TAU), Burak Yelmen, Flora Jay (BioInfo, TAU), Cyril Furtlehner (TAU)</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.24233">https://arxiv.org/abs/2510.24233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.24233">https://arxiv.org/pdf/2510.24233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.24233]] PRIVET: Privacy Metric Based on Extreme Value Theory(https://arxiv.org/abs/2510.24233)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep generative models are often trained on sensitive data, such as genetic sequences, health data, or more broadly, any copyrighted, licensed or protected content. This raises critical concerns around privacy-preserving synthetic data, and more specifically around privacy leakage, an issue closely tied to overfitting. Existing methods almost exclusively rely on global criteria to estimate the risk of privacy failure associated to a model, offering only quantitative non interpretable insights. The absence of rigorous evaluation methods for data privacy at the sample-level may hinder the practical deployment of synthetic data in real-world applications. Using extreme value statistics on nearest-neighbor distances, we propose PRIVET, a generic sample-based, modality-agnostic algorithm that assigns an individual privacy leak score to each synthetic sample. We empirically demonstrate that PRIVET reliably detects instances of memorization and privacy leakage across diverse data modalities, including settings with very high dimensionality, limited sample sizes such as genetic data and even under underfitting regimes. We compare our method to existing approaches under controlled settings and show its advantage in providing both dataset level and sample level assessments through qualitative and quantitative outputs. Additionally, our analysis reveals limitations in existing computer vision embeddings to yield perceptually meaningful distances when identifying near-duplicate samples.</li>
<li><strong>摘要：</strong>深度生成模型通常针对敏感数据进行训练，例如基因序列、健康数据，或更广泛地说，任何受版权保护、许可或受保护的内容。这引起了人们对保护隐私的合成数据的严重担忧，更具体地说，是对隐私泄露这一与过度拟合密切相关的问题。现有方法几乎完全依赖全球标准来估计与模型相关的隐私失败风险，仅提供定量的不可解释的见解。在样本级别缺乏严格的数据隐私评估方法可能会阻碍合成数据在现实应用中的实际部署。使用最近邻距离的极值统计，我们提出了 PRIVET，这是一种基于样本的通用、模态不可知的算法，可为每个合成样本分配单独的隐私泄漏分数。我们凭经验证明 PRIVET 能够可靠地检测不同数据模式中的记忆和隐私泄露实例，包括具有非常高维度的设置、有限的样本量（例如遗传数据），甚至在欠拟合的情况下。我们将我们的方法与受控设置下的现有方法进行比较，并通过定性和定量输出展示其在提供数据集级别和样本级别评估方面的优势。此外，我们的分析揭示了现有计算机视觉嵌入在识别近似重复样本时产生感知上有意义的距离的局限性。</li>
</ul>

<h3>Title: PaTaRM: Bridging Pairwise and Pointwise Signals via Preference-Aware Task-Adaptive Reward Modeling</h3>
<ul>
<li><strong>Authors: </strong>Ai Jian, Jingqing Ruan, Xing Ma, Dailin Li, QianLin Zhou, Ke Zeng, Xunliang Cai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.24235">https://arxiv.org/abs/2510.24235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.24235">https://arxiv.org/pdf/2510.24235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.24235]] PaTaRM: Bridging Pairwise and Pointwise Signals via Preference-Aware Task-Adaptive Reward Modeling(https://arxiv.org/abs/2510.24235)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Reward models (RMs) are central to reinforcement learning from human feedback (RLHF), providing the critical supervision signals that align large language models (LLMs) with human preferences. While generative reward models (GRMs) offer greater interpretability than traditional scalar RMs, current training paradigms remain limited. Pair-wise methods rely on binary good-versus-bad labels, which cause mismatches for point-wise inference and necessitate complex pairing strategies for effective application in RLHF. On the other hand, point-wise methods require more elaborate absolute labeling with rubric-driven criteria, resulting in poor adaptability and high annotation costs. In this work, we propose the Preference-Aware Task-Adaptive Reward Model (PaTaRM), a unified framework that integrates a preference-aware reward (PAR) mechanism with dynamic rubric adaptation. PaTaRM leverages relative preference information from pairwise data to construct robust point-wise training signals, eliminating the need for explicit point-wise labels. Simultaneously, it employs a task-adaptive rubric system that flexibly generates evaluation criteria for both global task consistency and instance-specific fine-grained reasoning. This design enables efficient, generalizable, and interpretable reward modeling for RLHF. Extensive experiments show that PaTaRM achieves an average relative improvement of 4.7% on RewardBench and RMBench across Qwen3-8B and Qwen3-14B models. Furthermore, PaTaRM boosts downstream RLHF performance, with an average improvement of 13.6% across IFEval and InFoBench benchmarks, confirming its effectiveness and robustness. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>奖励模型 (RM) 是基于人类反馈的强化学习 (RLHF) 的核心，它提供了使大型语言模型 (LLM) 与人类偏好保持一致的关键监督信号。虽然生成奖励模型 (GRM) 比传统标量 RM 提供了更好的可解释性，但当前的训练范例仍然有限。成对方法依赖于二元好坏标签，这会导致逐点推理不匹配，并且需要复杂的配对策略才能在 RLHF 中有效应用。另一方面，逐点方法需要更精细的绝对标记和规则驱动的标准，导致适应性差和注释成本高。在这项工作中，我们提出了偏好感知任务自适应奖励模型（PaTaRM），这是一个将偏好感知奖励（PAR）机制与动态标题适应相结合的统一框架。 PaTaRM 利用成对数据的相对偏好信息来构建强大的逐点训练信号，从而消除了对显式逐点标签的需要。同时，它采用任务自适应的评分系统，可以灵活地生成全局任务一致性和特定于实例的细粒度推理的评估标准。这种设计为 RLHF 提供了高效、可推广且可解释的奖励建模。大量实验表明，PaTaRM 在 Qwen3-8B 和 Qwen3-14B 模型上的 RewardBench 和 RMBench 上实现了 4.7% 的平均相对改进。此外，PaTaRM 还提高了下游 RLHF 性能，在 IFEval 和 InFoBench 基准测试中平均提高了 13.6%，证实了其有效性和稳健性。我们的代码可以在这个 https URL 上找到。</li>
</ul>

<h3>Title: DeshadowMamba: Deshadowing as 1D Sequential Similarity</h3>
<ul>
<li><strong>Authors: </strong>Zhaotong Yang, Yi Chen, Yanying Li, Shengfeng He, Yangyang Xu, Junyu Dong, Jian Yang, Yong Du</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.24260">https://arxiv.org/abs/2510.24260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.24260">https://arxiv.org/pdf/2510.24260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.24260]] DeshadowMamba: Deshadowing as 1D Sequential Similarity(https://arxiv.org/abs/2510.24260)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Recent deep models for image shadow removal often rely on attention-based architectures to capture long-range dependencies. However, their fixed attention patterns tend to mix illumination cues from irrelevant regions, leading to distorted structures and inconsistent colors. In this work, we revisit shadow removal from a sequence modeling perspective and explore the use of Mamba, a selective state space model that propagates global context through directional state transitions. These transitions yield an efficient global receptive field while preserving positional continuity. Despite its potential, directly applying Mamba to image data is suboptimal, since it lacks awareness of shadow-non-shadow semantics and remains susceptible to color interference from nearby regions. To address these limitations, we propose CrossGate, a directional modulation mechanism that injects shadow-aware similarity into Mamba's input gate, allowing selective integration of relevant context along transition axes. To further ensure appearance fidelity, we introduce ColorShift regularization, a contrastive learning objective driven by global color statistics. By synthesizing structured informative negatives, it guides the model to suppress color contamination and achieve robust color restoration. Together, these components adapt sequence modeling to the structural integrity and chromatic consistency required for shadow removal. Extensive experiments on public benchmarks demonstrate that DeshadowMamba achieves state-of-the-art visual quality and strong quantitative performance.</li>
<li><strong>摘要：</strong>最近用于图像阴影去除的深度模型通常依赖于基于注意力的架构来捕获远程依赖性。然而，他们的固定注意力模式往往会混合来自不相关区域的照明线索，导致结构扭曲和颜色不一致。在这项工作中，我们从序列建模的角度重新审视阴影去除，并探索 Mamba 的使用，Mamba 是一种选择性状态空间模型，可通过定向状态转换传播全局上下文。这些转变产生了有效的全局感受野，同时保持了位置连续性。尽管有潜力，但直接将 Mamba 应用于图像数据并不是最理想的，因为它缺乏对阴影-非阴影语义的认识，并且仍然容易受到附近区域的颜色干扰。为了解决这些限制，我们提出了 CrossGate，这是一种定向调制机制，可将阴影感知相似性注入 Mamba 的输入门，从而允许沿过渡轴选择性地集成相关上下文。为了进一步确保外观保真度，我们引入了 ColorShift 正则化，这是一种由全局颜色统计数据驱动的对比学习目标。通过合成结构化信息底片，它指导模型抑制颜色污染并实现稳健的颜色恢复。这些组件共同使序列建模适应阴影去除所需的结构完整性和色彩一致性。对公共基准的大量实验表明，DeshadowMamba 实现了最先进的视觉质量和强大的定量性能。</li>
</ul>

<h3>Title: UtilGen: Utility-Centric Generative Data Augmentation with Dual-Level Task Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Jiyu Guo, Shuo Yang, Yiming Huang, Yancheng Long, Xiaobo Xia, Xiu Su, Bo Zhao, Zeke Xie, Liqiang Nie</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.24262">https://arxiv.org/abs/2510.24262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.24262">https://arxiv.org/pdf/2510.24262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.24262]] UtilGen: Utility-Centric Generative Data Augmentation with Dual-Level Task Adaptation(https://arxiv.org/abs/2510.24262)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Data augmentation using generative models has emerged as a powerful paradigm for enhancing performance in computer vision tasks. However, most existing augmentation approaches primarily focus on optimizing intrinsic data attributes -- such as fidelity and diversity -- to generate visually high-quality synthetic data, while often neglecting task-specific requirements. Yet, it is essential for data generators to account for the needs of downstream tasks, as training data requirements can vary significantly across different tasks and network architectures. To address these limitations, we propose UtilGen, a novel utility-centric data augmentation framework that adaptively optimizes the data generation process to produce task-specific, high-utility training data via downstream task feedback. Specifically, we first introduce a weight allocation network to evaluate the task-specific utility of each synthetic sample. Guided by these evaluations, UtilGen iteratively refines the data generation process using a dual-level optimization strategy to maximize the synthetic data utility: (1) model-level optimization tailors the generative model to the downstream task, and (2) instance-level optimization adjusts generation policies -- such as prompt embeddings and initial noise -- at each generation round. Extensive experiments on eight benchmark datasets of varying complexity and granularity demonstrate that UtilGen consistently achieves superior performance, with an average accuracy improvement of 3.87% over previous SOTA. Further analysis of data influence and distribution reveals that UtilGen produces more impactful and task-relevant synthetic data, validating the effectiveness of the paradigm shift from visual characteristics-centric to task utility-centric data augmentation.</li>
<li><strong>摘要：</strong>使用生成模型的数据增强已成为增强计算机视觉任务性能的强大范例。然而，大多数现有的增强方法主要侧重于优化内在数据属性（例如保真度和多样性）以生成视觉上高质量的合成数据，而常常忽略特定于任务的要求。然而，数据生成器必须考虑下游任务的需求，因为不同任务和网络架构的训练数据要求可能存在很大差异。为了解决这些限制，我们提出了 UtilGen，一种新颖的以效用为中心的数据增强框架，它自适应地优化数据生成过程，通过下游任务反馈生成特定于任务的高效用训练数据。 Specifically, we first introduce a weight allocation network to evaluate the task-specific utility of each synthetic sample.在这些评估的指导下，UtilGen 使用双级优化策略迭代地细化数据生成过程，以最大化合成数据效用：(1) 模型级优化根据下游任务定制生成模型，(2) 实例级优化在每轮生成中调整生成策略，例如提示嵌入和初始噪声。对八个不同复杂度和粒度的基准数据集进行的大量实验表明，UtilGen 始终如一地实现了卓越的性能，与之前的 SOTA 相比，平均准确度提高了 3.87%。对数据影响和分布的进一步分析表明，UtilGen 生成更具影响力和与任务相关的合成数据，验证了从以视觉特征为中心到以任务效用为中心的数据增强范式转变的有效性。</li>
</ul>

<h3>Title: Training-free Source Attribution of AI-generated Images via Resynthesis</h3>
<ul>
<li><strong>Authors: </strong>Pietro Bongini, Valentina Molinari, Andrea Costanzo, Benedetta Tondi, Mauro Barni</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.24278">https://arxiv.org/abs/2510.24278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.24278">https://arxiv.org/pdf/2510.24278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.24278]] Training-free Source Attribution of AI-generated Images via Resynthesis(https://arxiv.org/abs/2510.24278)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Synthetic image source attribution is a challenging task, especially in data scarcity conditions requiring few-shot or zero-shot classification capabilities. We present a new training-free one-shot attribution method based on image resynthesis. A prompt describing the image under analysis is generated, then it is used to resynthesize the image with all the candidate sources. The image is attributed to the model which produced the resynthesis closest to the original image in a proper feature space. We also introduce a new dataset for synthetic image attribution consisting of face images from commercial and open-source text-to-image generators. The dataset provides a challenging attribution framework, useful for developing new attribution models and testing their capabilities on different generative architectures. The dataset structure allows to test approaches based on resynthesis and to compare them to few-shot methods. Results from state-of-the-art few-shot approaches and other baselines show that the proposed resynthesis method outperforms existing techniques when only a few samples are available for training or fine-tuning. The experiments also demonstrate that the new dataset is a challenging one and represents a valuable benchmark for developing and evaluating future few-shot and zero-shot methods.</li>
<li><strong>摘要：</strong>合成图像源归因是一项具有挑战性的任务，特别是在需要少样本或零样本分类能力的数据稀缺条件下。我们提出了一种新的基于图像重新合成的免训练一次性归因方法。生成描述正在分析的图像的提示，然后使用它与所有候选源重新合成图像。该图像归因于在适当的特征空间中产生最接近原始图像的重新合成的模型。我们还引入了一个用于合成图像归因的新数据集，其中包含来自商业和开源文本到图像生成器的面部图像。该数据集提供了一个具有挑战性的归因框架，可用于开发新的归因模型并测试其在不同生成架构上的功能。数据集结构允许测试基于再合成的方法并将它们与小样本方法进行比较。最先进的少样本方法和其他基线的结果表明，当只有少数样本可用于训练或微调时，所提出的再合成方法优于现有技术。实验还表明，新的数据集是一个具有挑战性的数据集，为开发和评估未来的少样本和零样本方法提供了宝贵的基准。</li>
</ul>

<h3>Title: ViPER: Empowering the Self-Evolution of Visual Perception Abilities in Vision-Language Model</h3>
<ul>
<li><strong>Authors: </strong>Juntian Zhang, Song Jin, Chuanqi Cheng, Yuhan Liu, Yankai Lin, Xun Zhang, Yufei Zhang, Fei Jiang, Guojun Yin, Wei Lin, Rui Yan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.24285">https://arxiv.org/abs/2510.24285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.24285">https://arxiv.org/pdf/2510.24285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.24285]] ViPER: Empowering the Self-Evolution of Visual Perception Abilities in Vision-Language Model(https://arxiv.org/abs/2510.24285)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The limited capacity for fine-grained visual perception presents a critical bottleneck for Vision-Language Models (VLMs) in real-world applications. Addressing this is challenging due to the scarcity of high-quality data and the limitations of existing methods: supervised fine-tuning (SFT) often compromises general capabilities, while reinforcement fine-tuning (RFT) prioritizes textual reasoning over visual perception. To bridge this gap, we propose a novel two-stage task that structures visual perception learning as a coarse-to-fine progressive process. Based on this task formulation, we develop ViPER, a self-bootstrapping framework specifically designed to enable iterative evolution through self-critiquing and self-prediction. By synergistically integrating image-level and instance-level reconstruction with a two-stage reinforcement learning strategy, ViPER establishes a closed-loop training paradigm, where internally synthesized data directly fuel the enhancement of perceptual ability. Applied to the Qwen2.5-VL family, ViPER produces the Qwen-Viper series. With an average gain of 1.7% on seven comprehensive benchmarks spanning various tasks and up to 6.0% on fine-grained perception, Qwen-Viper consistently demonstrates superior performance across different vision-language scenarios while maintaining generalizability. Beyond enabling self-improvement in perceptual capabilities, ViPER provides concrete evidence for the reciprocal relationship between generation and understanding, a breakthrough to developing more autonomous and capable VLMs.</li>
<li><strong>摘要：</strong>细粒度视觉感知的有限能力成为现实应用中视觉语言模型（VLM）的关键瓶颈。由于高质量数据的稀缺和现有方法的局限性，解决这个问题具有挑战性：监督微调（SFT）通常会损害一般能力，而强化微调（RFT）则优先考虑文本推理而不是视觉感知。为了弥补这一差距，我们提出了一种新颖的两阶段任务，将视觉感知学习构建为一个从粗到细的渐进过程。基于此任务制定，我们开发了 ViPER，这是一个自引导框架，专门设计用于通过自我批评和自我预测实现迭代进化。通过将图像级和实例级重建与两阶段强化学习策略协同集成，ViPER 建立了闭环训练范式，内部合成的数据直接促进感知能力的增强。应用于Qwen2.5-VL系列，ViPER生产了Qwen-Viper系列。 Qwen-Viper 在涵盖各种任务的 7 个综合基准测试中平均提升了 1.7%，在细粒度感知方面平均提升了 6.0%，Qwen-Viper 在不同视觉语言场景中始终表现出卓越的性能，同时保持了普遍性。除了实现感知能力的自我改进之外，ViPER 还为生成和理解之间的相互关系提供了具体证据，这是开发更自主、更强大的 VLM 的突破。</li>
</ul>

<h3>Title: A Comprehensive Evaluation Framework for Synthetic Trip Data Generation in Public Transport</h3>
<ul>
<li><strong>Authors: </strong>Yuanyuan Wu, Zhenlin Qin, Zhenliang Ma</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.24375">https://arxiv.org/abs/2510.24375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.24375">https://arxiv.org/pdf/2510.24375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.24375]] A Comprehensive Evaluation Framework for Synthetic Trip Data Generation in Public Transport(https://arxiv.org/abs/2510.24375)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Synthetic data offers a promising solution to the privacy and accessibility challenges of using smart card data in public transport research. Despite rapid progress in generative modeling, there is limited attention to comprehensive evaluation, leaving unclear how reliable, safe, and useful synthetic data truly are. Existing evaluations remain fragmented, typically limited to population-level representativeness or record-level privacy, without considering group-level variations or task-specific utility. To address this gap, we propose a Representativeness-Privacy-Utility (RPU) framework that systematically evaluates synthetic trip data across three complementary dimensions and three hierarchical levels (record, group, population). The framework integrates a consistent set of metrics to quantify similarity, disclosure risk, and practical usefulness, enabling transparent and balanced assessment of synthetic data quality. We apply the framework to benchmark twelve representative generation methods, spanning conventional statistical models, deep generative networks, and privacy-enhanced variants. Results show that synthetic data do not inherently guarantee privacy and there is no "one-size-fits-all" model, the trade-off between privacy and representativeness/utility is obvious. Conditional Tabular generative adversarial network (CTGAN) provide the most balanced trade-off and is suggested for practical applications. The RPU framework provides a systematic and reproducible basis for researchers and practitioners to compare synthetic data generation techniques and select appropriate methods in public transport applications.</li>
<li><strong>摘要：</strong>合成数据为公共交通研究中使用智能卡数据的隐私和可访问性挑战提供了一个有前景的解决方案。尽管生成模型取得了快速进展，但对综合评估的关注有限，导致人们不清楚合成数据到底有多可靠、安全和有用。现有的评估仍然分散，通常仅限于人口级别的代表性或记录级别的隐私，而没有考虑群体级别的变化或特定于任务的效用。为了解决这一差距，我们提出了一个代表性-隐私-效用（RPU）框架，该框架系统地评估三个互补维度和三个层次级别（记录、群体、人口）的综合旅行数据。该框架集成了一组一致的指标来量化相似性、披露风险和实际有用性，从而能够对合成数据质量进行透明和平衡的评估。我们应用该框架对十二种代表性生成方法进行基准测试，涵盖传统统计模型、深度生成网络和隐私增强变体。结果表明，合成数据本身并不保证隐私，并且不存在“一刀切”的模型，隐私与代表性/实用性之间的权衡是显而易见的。条件表格生成对抗网络（CTGAN）提供了最平衡的权衡，建议用于实际应用。 RPU 框架为研究人员和从业人员提供了系统且可重复的基础，以比较合成数据生成技术并在公共交通应用中选择适当的方法。</li>
</ul>

<h3>Title: Unsupervised Detection of Post-Stroke Brain Abnormalities</h3>
<ul>
<li><strong>Authors: </strong>Youwan Mahé, Elise Bannier, Stéphanie Leplaideur, Elisa Fromont, Francesca Galassi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.24398">https://arxiv.org/abs/2510.24398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.24398">https://arxiv.org/pdf/2510.24398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.24398]] Unsupervised Detection of Post-Stroke Brain Abnormalities(https://arxiv.org/abs/2510.24398)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Post-stroke MRI not only delineates focal lesions but also reveals secondary structural changes, such as atrophy and ventricular enlargement. These abnormalities, increasingly recognised as imaging biomarkers of recovery and outcome, remain poorly captured by supervised segmentation methods. We evaluate REFLECT, a flow-based generative model, for unsupervised detection of both focal and non-lesional abnormalities in post-stroke patients. Using dual-expert central-slice annotations on ATLAS data, performance was assessed at the object level with Free-Response ROC analysis for anomaly maps. Two models were trained on lesion-free slices from stroke patients (ATLAS) and on healthy controls (IXI) to test the effect of training data. On ATLAS test subjects, the IXI-trained model achieved higher lesion segmentation (Dice = 0.37 vs 0.27) and improved sensitivity to non-lesional abnormalities (FROC = 0.62 vs 0.43). Training on fully healthy anatomy improves the modelling of normal variability, enabling broader and more reliable detection of structural abnormalities.</li>
<li><strong>摘要：</strong>中风后 MRI 不仅可以描绘局灶性病变，还可以揭示继发性结构变化，例如萎缩和心室扩大。这些异常现象越来越被认为是恢复和结果的成像生物标志物，但监督分割方法仍然很难捕捉到这些异常情况。我们评估 REFLECT（一种基于流的生成模型），用于无监督检测中风后患者的局灶性和非病变异常。使用 ATLAS 数据上的双专家中央切片注释，通过异常图的自由响应 ROC 分析在对象级别评估性能。在中风患者 (ATLAS) 和健康对照 (IXI) 的无病变切片上训练两个模型，以测试训练数据的效果。在 ATLAS 测试对象上，IXI 训练的模型实现了更高的病灶分割（Dice = 0.37 vs 0.27），并提高了对非病灶异常的敏感性（FROC = 0.62 vs 0.43）。对完全健康的解剖学进行培训可以改善正常变异性的建模，从而能够更广泛、更可靠地检测结构异常。</li>
</ul>

<h3>Title: GenTrack: A New Generation of Multi-Object Tracking</h3>
<ul>
<li><strong>Authors: </strong>Toan Van Nguyen, Rasmus G. K. Christiansen, Dirk Kraft, Leon Bodenhagen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.24399">https://arxiv.org/abs/2510.24399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.24399">https://arxiv.org/pdf/2510.24399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.24399]] GenTrack: A New Generation of Multi-Object Tracking(https://arxiv.org/abs/2510.24399)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel multi-object tracking (MOT) method, dubbed GenTrack, whose main contributions include: a hybrid tracking approach employing both stochastic and deterministic manners to robustly handle unknown and time-varying numbers of targets, particularly in maintaining target identity (ID) consistency and managing nonlinear dynamics, leveraging particle swarm optimization (PSO) with some proposed fitness measures to guide stochastic particles toward their target distribution modes, enabling effective tracking even with weak and noisy object detectors, integration of social interactions among targets to enhance PSO-guided particles as well as improve continuous updates of both strong (matched) and weak (unmatched) tracks, thereby reducing ID switches and track loss, especially during occlusions, a GenTrack-based redefined visual MOT baseline incorporating a comprehensive state and observation model based on space consistency, appearance, detection confidence, track penalties, and social scores for systematic and efficient target updates, and the first-ever publicly available source-code reference implementation with minimal dependencies, featuring three variants, including GenTrack Basic, PSO, and PSO-Social, facilitating flexible reimplementation. Experimental results have shown that GenTrack provides superior performance on standard benchmarks and real-world scenarios compared to state-of-the-art trackers, with integrated implementations of baselines for fair comparison. Potential directions for future work are also discussed. The source-code reference implementations of both the proposed method and compared-trackers are provided on GitHub: this https URL</li>
<li><strong>摘要：</strong>本文介绍了一种新颖的多目标跟踪（MOT）方法，称为 GenTrack，其主要贡献包括：一种混合跟踪方法，采用随机和确定性方式来鲁棒地处理未知和随时间变化的目标数量，特别是在保持目标身份（ID）一致性和管理非线性动态方面，利用粒子群优化（PSO）和一些提出的适应度措施来引导随机粒子朝向其目标分布模式，即使使用弱和有噪声的目标检测器也能实现有效跟踪，将目标之间的社交互动集成到增强 PSO 引导粒子并改进强（匹配）和弱（不匹配）轨迹的连续更新，从而减少 ID 切换和轨迹丢失，特别是在遮挡期间，基于 GenTrack 的重新定义的视觉 MOT 基线，结合了基于空间一致性、外观、检测置信度、轨迹惩罚和社会分数的综合状态和观察模型，以实现系统和高效的目标更新，以及第一个公开可用的源代码参考实现，具有最小的依赖性，具有三种变体，包括 GenTrack Basic、PSO 和PSO-Social，促进灵活的重新实施。实验结果表明，与最先进的跟踪器相比，GenTrack 在标准基准和现实场景中提供了卓越的性能，并集成了基线实现以进行公平比较。还讨论了未来工作的潜在方向。 GitHub 上提供了所提出的方法和比较跟踪器的源代码参考实现：此 https URL</li>
</ul>

<h3>Title: Decoupled MeanFlow: Turning Flow Models into Flow Maps for Accelerated Sampling</h3>
<ul>
<li><strong>Authors: </strong>Kyungmin Lee, Sihyun Yu, Jinwoo Shin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.24474">https://arxiv.org/abs/2510.24474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.24474">https://arxiv.org/pdf/2510.24474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.24474]] Decoupled MeanFlow: Turning Flow Models into Flow Maps for Accelerated Sampling(https://arxiv.org/abs/2510.24474)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Denoising generative models, such as diffusion and flow-based models, produce high-quality samples but require many denoising steps due to discretization error. Flow maps, which estimate the average velocity between timesteps, mitigate this error and enable faster sampling. However, their training typically demands architectural changes that limit compatibility with pretrained flow models. We introduce Decoupled MeanFlow, a simple decoding strategy that converts flow models into flow map models without architectural modifications. Our method conditions the final blocks of diffusion transformers on the subsequent timestep, allowing pretrained flow models to be directly repurposed as flow maps. Combined with enhanced training techniques, this design enables high-quality generation in as few as 1 to 4 steps. Notably, we find that training flow models and subsequently converting them is more efficient and effective than training flow maps from scratch. On ImageNet 256x256 and 512x512, our models attain 1-step FID of 2.16 and 2.12, respectively, surpassing prior art by a large margin. Furthermore, we achieve FID of 1.51 and 1.68 when increasing the steps to 4, which nearly matches the performance of flow models while delivering over 100x faster inference.</li>
<li><strong>摘要：</strong>去噪生成模型，例如基于扩散和流的模型，可以产生高质量的样本，但由于离散化误差，需要许多去噪步骤。流量图可以估计时间步之间的平均速度，可以减轻这种误差并实现更快的采样。然而，他们的训练通常需要进行架构更改，从而限制了与预训练流模型的兼容性。我们引入了 De Coupled MeanFlow，这是一种简单的解码策略，无需修改架构即可将流模型转换为流图模型。我们的方法在后续时间步上调节扩散变压器的最终块，允许将预训练的流模型直接重新用作流图。结合增强的训练技术，该设计只需 1 到 4 个步骤即可实现高质量生成。值得注意的是，我们发现训练流模型并随后转换它们比从头开始训练流图更加高效和有效。在 ImageNet 256x256 和 512x512 上，我们的模型分别获得了 2.16 和 2.12 的 1-step FID，大幅超越了现有技术。此外，当步数增加到 4 时，我们实现了 1.51 和 1.68 的 FID，这几乎与流模型的性能相当，同时推理速度提高了 100 倍以上。</li>
</ul>

<h3>Title: Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Huanyu Zhang, Wenshan Wu, Chengzu Li, Ning Shang, Yan Xia, Yangyu Huang, Yifan Zhang, Li Dong, Zhang Zhang, Liang Wang, Tieniu Tan, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.24514">https://arxiv.org/abs/2510.24514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.24514">https://arxiv.org/pdf/2510.24514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.24514]] Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs(https://arxiv.org/abs/2510.24514)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>While Multimodal Large Language Models (MLLMs) excel at visual understanding, they often struggle in complex scenarios that require visual planning and imagination. Inspired by how humans use sketching as a form of visual thinking to develop and communicate ideas, we introduce Latent Sketchpad, a framework that equips MLLMs with an internal visual scratchpad. The internal visual representations of MLLMs have traditionally been confined to perceptual understanding. We repurpose them to support generative visual thought without compromising reasoning ability. Building on frontier MLLMs, our approach integrates visual generation directly into their native autoregressive reasoning process. It allows the model to interleave textual reasoning with the generation of visual latents. These latents guide the internal thought process and can be translated into sketch images for interpretability. To realize this, we introduce two components: a Context-Aware Vision Head autoregressively produces visual representations, and a pretrained Sketch Decoder renders these into human-interpretable images. We evaluate the framework on our new dataset MazePlanning. Experiments across various MLLMs show that Latent Sketchpad delivers comparable or even superior reasoning performance to their backbone. It further generalizes across distinct frontier MLLMs, including Gemma3 and Qwen2.5-VL. By extending model's textual reasoning to visual thinking, our framework opens new opportunities for richer human-computer interaction and broader applications. More details and resources are available on our project page: this https URL.</li>
<li><strong>摘要：</strong>虽然多模态大型语言模型 (MLLM) 擅长视觉理解，但它们常常在需要视觉规划和想象力的复杂场景中陷入困境。受到人类如何使用草图作为一种视觉思维形式来开发和交流想法的启发，我们推出了潜在画板，这是一个为 MLLM 配备内部视觉便笺本的框架。 MLLM 的内部视觉表征传统上仅限于感知理解。我们重新利用它们来支持生成视觉思维，而不影响推理能力。我们的方法以前沿 MLLM 为基础，将视觉生成直接集成到其本机自回归推理过程中。它允许模型将文本推理与视觉潜在的生成交织在一起。这些潜在因素指导内部思维过程，并且可以转化为草图图像以供解释。为了实现这一点，我们引入了两个组件：上下文感知视觉头自动生成视觉表示，而预训练的 Sketch 解码器将这些呈现为人类可解释的图像。我们在新数据集 MazePlanning 上评估该框架。各种 MLLM 的实验表明，Latent Sketchpad 可以提供与其主干相当甚至更出色的推理性能。它进一步概括了不同的前沿 MLLM，包括 Gemma3 和 Qwen2.5-VL。通过将模型的文本推理扩展到视觉思维，我们的框架为更丰富的人机交互和更广泛的应用开辟了新的机会。我们的项目页面上提供了更多详细信息和资源：此 https URL。</li>
</ul>

<h3>Title: OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents</h3>
<ul>
<li><strong>Authors: </strong>Hongrui Jia, Jitong Liao, Xi Zhang, Haiyang Xu, Tianbao Xie, Chaoya Jiang, Ming Yan, Si Liu, Wei Ye, Fei Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.24563">https://arxiv.org/abs/2510.24563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.24563">https://arxiv.org/pdf/2510.24563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.24563]] OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents(https://arxiv.org/abs/2510.24563)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>With advances in decision-making and reasoning capabilities, multimodal agents show strong potential in computer application scenarios. Past evaluations have mainly assessed GUI interaction skills, while tool invocation abilities, such as those enabled by the Model Context Protocol (MCP), have been largely overlooked. Comparing agents with integrated tool invocation to those evaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP, the first comprehensive and fair benchmark for assessing computer-use agents' tool invocation, GUI operation, and decision-making abilities in a real-world environment. We design a novel automated code-generation pipeline to create tools and combine them with a curated selection from existing tools. Rigorous manual validation yields 158 high-quality tools (covering 7 common applications), each verified for correct functionality, practical applicability, and versatility. Extensive evaluations of state-of-the-art multimodal agents on OSWorld-MCP show that MCP tools generally improve task success rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1% to 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of assessing tool invocation capabilities. However, even the strongest models have relatively low tool invocation rates, Only 36.3%, indicating room for improvement and highlighting the benchmark's challenge. By explicitly measuring MCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents and sets a new standard for evaluating performance in complex, tool-assisted environments. Our code, environment, and data are publicly available at this https URL.</li>
<li><strong>摘要：</strong>随着决策和推理能力的进步，多模态智能体在计算机应用场景中展现出强大的潜力。过去的评估主要评估GUI交互技能，而工具调用能力，例如模型上下文协议（MCP）启用的工具调用能力，在很大程度上被忽视了。将具有集成工具调用的代理与仅在 GUI 交互上评估的代理进行比较本质上是不公平的。我们推出了 OSWorld-MCP，这是第一个全面、公平的基准，用于评估现实环境中计算机使用代理的工具调用、GUI 操作和决策能力。我们设计了一种新颖的自动代码生成管道来创建工具，并将它们与现有工具中精选的工具相结合。严格的手动验证产生了 158 个高质量工具（涵盖 7 种常见应用），每个工具都经过了正确功能、实际适用性和多功能性的验证。对 OSWorld-MCP 上最先进的多模式代理的广泛评估表明，MCP 工具通常可以提高任务成功率（例如，OpenAI o3 在 15 步时从 8.3% 提高到 20.4%，Claude 4 Sonnet 在 50 步时从 40.1% 提高到 43.3%），强调了评估工具调用能力的重要性。然而，即使是最强的模型，工具调用率也相对较低，仅为 36.3%，这表明还有改进的空间，也凸显了基准测试的挑战。通过明确测量 MCP 工具使用技能，OSWorld-MCP 加深了对多模式代理的理解，并为评估复杂的工具辅助环境中的性能制定了新标准。我们的代码、环境和数据可通过此 https URL 公开获取。</li>
</ul>

<h3>Title: A Dual-Branch CNN for Robust Detection of AI-Generated Facial Forgeries</h3>
<ul>
<li><strong>Authors: </strong>Xin Zhang, Yuqi Song, Fei Zuo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.24640">https://arxiv.org/abs/2510.24640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.24640">https://arxiv.org/pdf/2510.24640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.24640]] A Dual-Branch CNN for Robust Detection of AI-Generated Facial Forgeries(https://arxiv.org/abs/2510.24640)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of generative AI has enabled the creation of highly realistic forged facial images, posing significant threats to AI security, digital media integrity, and public trust. Face forgery techniques, ranging from face swapping and attribute editing to powerful diffusion-based image synthesis, are increasingly being used for malicious purposes such as misinformation, identity fraud, and defamation. This growing challenge underscores the urgent need for robust and generalizable face forgery detection methods as a critical component of AI security infrastructure. In this work, we propose a novel dual-branch convolutional neural network for face forgery detection that leverages complementary cues from both spatial and frequency domains. The RGB branch captures semantic information, while the frequency branch focuses on high-frequency artifacts that are difficult for generative models to suppress. A channel attention module is introduced to adaptively fuse these heterogeneous features, highlighting the most informative channels for forgery discrimination. To guide the network's learning process, we design a unified loss function, FSC Loss, that combines focal loss, supervised contrastive loss, and a frequency center margin loss to enhance class separability and robustness. We evaluate our model on the DiFF benchmark, which includes forged images generated from four representative methods: text-to-image, image-to-image, face swap, and face edit. Our method achieves strong performance across all categories and outperforms average human accuracy. These results demonstrate the model's effectiveness and its potential contribution to safeguarding AI ecosystems against visual forgery attacks.</li>
<li><strong>摘要：</strong>生成式人工智能的快速发展使得能够创建高度逼真的伪造面部图像，对人工智能安全、数字媒体完整性和公众信任构成重大威胁。面部伪造技术，从面部交换和属性编辑到强大的基于扩散的图像合成，越来越多地被用于恶意目的，例如错误信息、身份欺诈和诽谤。这一日益严峻的挑战凸显了对强大且通用的人脸伪造检测方法作为人工智能安全基础设施的关键组成部分的迫切需求。在这项工作中，我们提出了一种新颖的双分支卷积神经网络，用于人脸伪造检测，该网络利用空间和频率域的互补线索。 RGB 分支捕获语义信息，而频率分支则专注于生成模型难以抑制的高频伪影。引入通道注意模块来自适应地融合这些异构特征，突出显示信息最丰富的伪造鉴别通道。为了指导网络的学习过程，我们设计了一个统一的损失函数 FSC Loss，它结合了焦点损失、监督对比损失和频率中心裕度损失，以增强类可分离性和鲁棒性。我们在 DiFF 基准上评估我们的模型，其中包括通过四种代表性方法生成的伪造图像：文本到图像、图像到图像、面部交换和面部编辑。我们的方法在所有类别中都取得了出色的性能，并且优于人类的平均准确度。这些结果证明了该模型的有效性及其对保护人工智能生态系统免受视觉伪造攻击的潜在贡献。</li>
</ul>

<h3>Title: SAGE: Structure-Aware Generative Video Transitions between Diverse Clips</h3>
<ul>
<li><strong>Authors: </strong>Mia Kan, Yilin Liu, Niloy Mitra</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.24667">https://arxiv.org/abs/2510.24667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.24667">https://arxiv.org/pdf/2510.24667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.24667]] SAGE: Structure-Aware Generative Video Transitions between Diverse Clips(https://arxiv.org/abs/2510.24667)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Video transitions aim to synthesize intermediate frames between two clips, but naive approaches such as linear blending introduce artifacts that limit professional use or break temporal coherence. Traditional techniques (cross-fades, morphing, frame interpolation) and recent generative inbetweening methods can produce high-quality plausible intermediates, but they struggle with bridging diverse clips involving large temporal gaps or significant semantic differences, leaving a gap for content-aware and visually coherent transitions. We address this challenge by drawing on artistic workflows, distilling strategies such as aligning silhouettes and interpolating salient features to preserve structure and perceptual continuity. Building on this, we propose SAGE (Structure-Aware Generative vidEo transitions) as a zeroshot approach that combines structural guidance, provided via line maps and motion flow, with generative synthesis, enabling smooth, semantically consistent transitions without fine-tuning. Extensive experiments and comparison with current alternatives, namely [FILM, TVG, DiffMorpher, VACE, GI], demonstrate that SAGE outperforms both classical and generative baselines on quantitative metrics and user studies for producing transitions between diverse clips. Code to be released on acceptance.</li>
<li><strong>摘要：</strong>视频过渡旨在合成两个剪辑之间的中间帧，但线性混合等简单方法会引入限制专业使用或破坏时间连贯性的伪影。传统技术（交叉淡入淡出、变形、帧插值）和最近的生成式中间方法可以产生高质量的可信中间体，但它们难以桥接涉及较大时间间隙或显着语义差异的不同剪辑，从而为内容感知和视觉连贯的过渡留下了间隙。我们通过利用艺术工作流程、提炼策略（例如对齐轮廓和插入显着特征）来应对这一挑战，以保持结构和感知连续性。在此基础上，我们提出 SAGE（结构感知生成 vidEo 过渡）作为一种零样本方法，它将通过线图和运动流提供的结构指导与生成合成相结合，无需微调即可实现平滑、语义一致的过渡。广泛的实验以及与当前替代方案（即 [FILM、TVG、DiffMorpher、VACE、GI]）的比较表明，SAGE 在定量指标和用于在不同剪辑之间生成过渡的用户研究方面优于经典基线和生成基线。代码在接受后发布。</li>
</ul>

<h3>Title: Uniform Discrete Diffusion with Metric Path for Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Haoge Deng, Ting Pan, Fan Zhang, Yang Liu, Zhuoyan Luo, Yufeng Cui, Wenxuan Wang, Chunhua Shen, Shiguang Shan, Zhaoxiang Zhang, Xinlong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.24717">https://arxiv.org/abs/2510.24717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.24717">https://arxiv.org/pdf/2510.24717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.24717]] Uniform Discrete Diffusion with Metric Path for Video Generation(https://arxiv.org/abs/2510.24717)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Continuous-space video generation has advanced rapidly, while discrete approaches lag behind due to error accumulation and long-context inconsistency. In this work, we revisit discrete generative modeling and present Uniform discRete diffuSion with metric pAth (URSA), a simple yet powerful framework that bridges the gap with continuous approaches for the scalable video generation. At its core, URSA formulates the video generation task as an iterative global refinement of discrete spatiotemporal tokens. It integrates two key designs: a Linearized Metric Path and a Resolution-dependent Timestep Shifting mechanism. These designs enable URSA to scale efficiently to high-resolution image synthesis and long-duration video generation, while requiring significantly fewer inference steps. Additionally, we introduce an asynchronous temporal fine-tuning strategy that unifies versatile tasks within a single model, including interpolation and image-to-video generation. Extensive experiments on challenging video and image generation benchmarks demonstrate that URSA consistently outperforms existing discrete methods and achieves performance comparable to state-of-the-art continuous diffusion methods. Code and models are available at this https URL</li>
<li><strong>摘要：</strong>连续空间视频生成发展迅速，而离散方法由于错误累积和长上下文不一致而落后。在这项工作中，我们重新审视离散生成建模，并提出带有度量路径的均匀光盘扩散（URSA），这是一个简单但功能强大的框架，它弥补了与可扩展视频生成的连续方法之间的差距。 URSA 的核心是将视频生成任务制定为离散时空标记的迭代全局细化。它集成了两个关键设计：线性化度量路径和分辨率相关的时间步长移位机制。这些设计使 URSA 能够有效地扩展到高分辨率图像合成和长时间视频生成，同时需要显着减少的推理步骤。此外，我们引入了一种异步时间微调策略，该策略将多种任务统一在单个模型中，包括插值和图像到视频生成。在具有挑战性的视频和图像生成基准上进行的大量实验表明，URSA 始终优于现有的离散方法，并实现了与最先进的连续扩散方法相当的性能。代码和模型可在此 https URL 获取</li>
</ul>

<h3>Title: Generative View Stitching</h3>
<ul>
<li><strong>Authors: </strong>Chonghyuk Song, Michal Stary, Boyuan Chen, George Kopanas, Vincent Sitzmann</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.24718">https://arxiv.org/abs/2510.24718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.24718">https://arxiv.org/pdf/2510.24718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.24718]] Generative View Stitching(https://arxiv.org/abs/2510.24718)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Autoregressive video diffusion models are capable of long rollouts that are stable and consistent with history, but they are unable to guide the current generation with conditioning from the future. In camera-guided video generation with a predefined camera trajectory, this limitation leads to collisions with the generated scene, after which autoregression quickly collapses. To address this, we propose Generative View Stitching (GVS), which samples the entire sequence in parallel such that the generated scene is faithful to every part of the predefined camera trajectory. Our main contribution is a sampling algorithm that extends prior work on diffusion stitching for robot planning to video generation. While such stitching methods usually require a specially trained model, GVS is compatible with any off-the-shelf video model trained with Diffusion Forcing, a prevalent sequence diffusion framework that we show already provides the affordances necessary for stitching. We then introduce Omni Guidance, a technique that enhances the temporal consistency in stitching by conditioning on both the past and future, and that enables our proposed loop-closing mechanism for delivering long-range coherence. Overall, GVS achieves camera-guided video generation that is stable, collision-free, frame-to-frame consistent, and closes loops for a variety of predefined camera paths, including Oscar Reutersvärd's Impossible Staircase. Results are best viewed as videos at this https URL.</li>
<li><strong>摘要：</strong>自回归视频扩散模型能够长期稳定且与历史一致地推出，但它们无法通过未来的调节来指导当前一代。在具有预定义摄像机轨迹的摄像机引导视频生成中，这种限制会导致与生成的场景发生碰撞，之后自回归很快就会崩溃。为了解决这个问题，我们提出了生成视图拼接（GVS），它并行采样整个序列，以便生成的场景忠实于预定义相机轨迹的每个部分。我们的主要贡献是一种采样算法，它将机器人规划的扩散拼接的先前工作扩展到视频生成。虽然这种拼接方法通常需要经过专门训练的模型，但 GVS 与任何使用扩散强迫训练的现成视频模型兼容，我们展示了一种流行的序列扩散框架，它已经提供了拼接所需的功能可供性。然后，我们引入 Omni Guidance，这是一种通过调节过去和未来来增强拼接的时间一致性的技术，并使我们提出的闭环机制能够提供远程一致性。总体而言，GVS 实现了稳定、无碰撞、帧与帧一致的摄像机引导视频生成，并针对各种预定义摄像机路径（包括 OscarReutersvärd 的不可能楼梯）形成闭环。最好通过此 https URL 以视频形式查看结果。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
