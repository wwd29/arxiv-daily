<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-11-14</h1>
<h3>Title: HeatGen: A Guided Diffusion Framework for Multiphysics Heat Sink Design Optimization</h3>
<ul>
<li><strong>Authors: </strong>Hadi Keramati, Morteza Sadeghi, Rajeev K. Jaiman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09578">https://arxiv.org/abs/2511.09578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09578">https://arxiv.org/pdf/2511.09578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09578]] HeatGen: A Guided Diffusion Framework for Multiphysics Heat Sink Design Optimization(https://arxiv.org/abs/2511.09578)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>This study presents a generative optimization framework based on a guided denoising diffusion probabilistic model (DDPM) that leverages surrogate gradients to generate heat sink designs minimizing pressure drop while maintaining surface temperatures below a specified threshold. Geometries are represented using boundary representations of multiple fins, and a multi-fidelity approach is employed to generate training data. Using this dataset, along with vectors representing the boundary representation geometries, we train a denoising diffusion probabilistic model to generate heat sinks with characteristics consistent with those observed in the data. We train two different residual neural networks to predict the pressure drop and surface temperature for each geometry. We use the gradients of these surrogate models with respect to the design variables to guide the geometry generation process toward satisfying the low-pressure and surface temperature constraints. This inference-time guidance directs the generative process toward heat sink designs that not only prevent overheating but also achieve lower pressure drops compared to traditional optimization methods such as CMA-ES. In contrast to traditional black-box optimization approaches, our method is scalable, provided sufficient training data is available. Unlike traditional topology optimization methods, once the model is trained and the heat sink world model is saved, inference under new constraints (e.g., temperature) is computationally inexpensive and does not require retraining. Samples generated using the guided diffusion model achieve pressure drops up to 10 percent lower than the limits obtained by traditional black-box optimization methods. This work represents a step toward building a foundational generative model for electronics cooling.</li>
<li><strong>摘要：</strong>本研究提出了一种基于引导降噪扩散概率模型 (DDPM) 的生成优化框架，该框架利用替代梯度来生成散热器设计，最大限度地减少压降，同时保持表面温度低于指定阈值。使用多个鳍的边界表示来表示几何形状，并采用多保真度方法来生成训练数据。使用该数据集以及表示边界表示几何形状的向量，我们训练去噪扩散概率模型以生成具有与数据中观察到的特征一致的散热器。我们训练两个不同的残差神经网络来预测每个几何形状的压降和表面温度。我们使用这些替代模型相对于设计变量的梯度来指导几何生成过程以满足低压和表面温度约束。这种推理时间指导将生成过程引导至散热器设计，与 CMA-ES 等传统优化方法相比，不仅可以防止过热，而且可以实现更低的压降。与传统的黑盒优化方法相比，只要有足够的训练数据，我们的方法是可扩展的。与传统的拓扑优化方法不同，一旦模型经过训练并保存散热器世界模型，新约束（例如温度）下的推理在计算上成本低廉，并且不需要重新训练。使用引导扩散模型生成的样品实现的压降比传统黑盒优化方法获得的极限低 10%。这项工作代表了朝着构建电子冷却基础生成模型迈出的一步。</li>
</ul>

<h3>Title: Scaling Environments for LLM Agents in the Era of Learning from Interaction: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Huang, Sijia Li, Minghao Liu, Wei Liu, Shijue Huang, Zhiyuan Fan, Hou Pong Chan, Yi R. Fung</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09586">https://arxiv.org/abs/2511.09586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09586">https://arxiv.org/pdf/2511.09586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09586]] Scaling Environments for LLM Agents in the Era of Learning from Interaction: A Survey(https://arxiv.org/abs/2511.09586)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>LLM-based agents can autonomously accomplish complex tasks across various domains. However, to further cultivate capabilities such as adaptive behavior and long-term decision-making, training on static datasets built from human-level knowledge is insufficient. These datasets are costly to construct and lack both dynamism and realism. A growing consensus is that agents should instead interact directly with environments and learn from experience through reinforcement learning. We formalize this iterative process as the Generation-Execution-Feedback (GEF) loop, where environments generate tasks to challenge agents, return observations in response to agents' actions during task execution, and provide evaluative feedback on rollouts for subsequent learning. Under this paradigm, environments function as indispensable producers of experiential data, highlighting the need to scale them toward greater complexity, realism, and interactivity. In this survey, we systematically review representative methods for environment scaling from a pioneering environment-centric perspective and organize them along the stages of the GEF loop, namely task generation, task execution, and feedback. We further analyze benchmarks, implementation strategies, and applications, consolidating fragmented advances and outlining future research directions for agent intelligence.</li>
<li><strong>摘要：</strong>基于 LLM 的代理可以自主完成跨各个领域的复杂任务。然而，为了进一步培养适应性行为和长期决策等能力，基于人类知识构建的静态数据集的训练是不够的。这些数据集的构建成本高昂，并且缺乏活力和现实性。越来越多的共识是，智能体应该直接与环境交互，并通过强化学习从经验中学习。我们将这个迭代过程形式化为生成-执行-反馈（GEF）循环，其中环境生成任务来挑战代理，返回观察结果以响应任务执行期间代理的操作，并为后续学习提供评估反馈。在这种范式下，环境作为体验数据不可或缺的生产者，强调需要将其扩展到更大的复杂性、现实性和交互性。在本次调查中，我们从以环境为中心的开创性角度系统地回顾了环境扩展的代表性方法，并按照 GEF 循环的各个阶段（即任务生成、任务执行和反馈）组织它们。我们进一步分析基准、实施策略和应用，整合零散的进展并概述代理智能的未来研究方向。</li>
</ul>

<h3>Title: Parametric Expensive Multi-Objective Optimization via Generative Solution Modeling</h3>
<ul>
<li><strong>Authors: </strong>Tingyang Wei, Jiao Liu, Abhishek Gupta, Chin Chun Ooi, Puay Siew Tan, Yew-Soon Ong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09598">https://arxiv.org/abs/2511.09598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09598">https://arxiv.org/pdf/2511.09598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09598]] Parametric Expensive Multi-Objective Optimization via Generative Solution Modeling(https://arxiv.org/abs/2511.09598)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Many real-world applications require solving families of expensive multi-objective optimization problems~(EMOPs) under varying operational conditions. This gives rise to parametric expensive multi-objective optimization problems (P-EMOPs) where each task parameter defines a distinct optimization instance. Current multi-objective Bayesian optimization methods have been widely used for finding finite sets of Pareto optimal solutions for individual tasks. However, P-EMOPs present a fundamental challenge: the continuous task parameter space can contain infinite distinct problems, each requiring separate expensive evaluations. This demands learning an inverse model that can directly predict optimized solutions for any task-preference query without expensive re-evaluation. This paper introduces the first parametric multi-objective Bayesian optimizer that learns this inverse model by alternating between (1) acquisition-driven search leveraging inter-task synergies and (2) generative solution sampling via conditional generative models. This approach enables efficient optimization across related tasks and finally achieves direct solution prediction for unseen parameterized EMOPs without additional expensive evaluations. We theoretically justify the faster convergence by leveraging inter-task synergies through task-aware Gaussian processes. Meanwhile, empirical studies in synthetic and real-world benchmarks further verify the effectiveness of our alternating framework.</li>
<li><strong>摘要：</strong>许多现实世界的应用需要在不同的操作条件下解决一系列昂贵的多目标优化问题（EMOP）。这引起了参数昂贵的多目标优化问题（P-EMOP），其中每个任务参数定义一个不同的优化实例。当前的多目标贝叶斯优化方法已广泛用于寻找单个任务的有限帕累托最优解集。然而，P-EMOP 提出了一个根本性的挑战：连续任务参数空间可能包含无限不同的问题，每个问题都需要单独的昂贵评估。这需要学习一个逆模型，该模型可以直接预测任何任务偏好查询的优化解决方案，而无需昂贵的重新评估。本文介绍了第一个参数化多目标贝叶斯优化器，它通过在（1）利用任务间协同作用的采集驱动搜索和（2）通过条件生成模型的生成解决方案采样之间交替来学习此逆模型。这种方法可以实现相关任务的高效优化，并最终实现对未见过的参数化紧急行动的直接解决方案预测，而无需额外昂贵的评估。我们从理论上证明，通过任务感知高斯过程利用任务间协同作用可以实现更快的收敛。同时，综合基准和现实基准的实证研究进一步验证了我们交替框架的有效性。</li>
</ul>

<h3>Title: MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation</h3>
<ul>
<li><strong>Authors: </strong>Ye Tian, Ling Yang, Jiongfan Yang, Anran Wang, Yu Tian, Jiani Zheng, Haochen Wang, Zhiyang Teng, Zhuochen Wang, Yinjie Wang, Yunhai Tong, Mengdi Wang, Xiangtai Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09611">https://arxiv.org/abs/2511.09611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09611">https://arxiv.org/pdf/2511.09611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09611]] MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation(https://arxiv.org/abs/2511.09611)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While thinking-aware generation aims to improve performance on complex tasks, we identify a critical failure mode where existing sequential, autoregressive approaches can paradoxically degrade performance due to error propagation. To systematically analyze this issue, we propose ParaBench, a new benchmark designed to evaluate both text and image output modalities. Our analysis using ParaBench reveals that this performance degradation is strongly correlated with poor alignment between the generated reasoning and the final image. To resolve this, we propose a parallel multimodal diffusion framework, MMaDA-Parallel, that enables continuous, bidirectional interaction between text and images throughout the entire denoising trajectory. MMaDA-Parallel is trained with supervised finetuning and then further optimized by Parallel Reinforcement Learning (ParaRL), a novel strategy that applies semantic rewards along the trajectory to enforce cross-modal consistency. Experiments validate that our model significantly improves cross-modal alignment and semantic consistency, achieving a 6.9\% improvement in Output Alignment on ParaBench compared to the state-of-the-art model, Bagel, establishing a more robust paradigm for thinking-aware image synthesis. Our code is open-sourced at this https URL</li>
<li><strong>摘要：</strong>虽然思考意识生成旨在提高复杂任务的性能，但我们确定了一种关键的故障模式，其中现有的顺序自回归方法可能会由于错误传播而自相矛盾地降低性能。为了系统地分析这个问题，我们提出了 ParaBench，这是一个旨在评估文本和图像输出模式的新基准。我们使用 ParaBench 进行的分析表明，这种性能下降与生成的推理和最终图像之间的对齐不良密切相关。为了解决这个问题，我们提出了一种并行多模态扩散框架 MMaDA-Parallel，它可以在整个去噪轨迹中实现文本和图像之间的连续、双向交互。 MMaDA-Parallel 通过监督微调进行训练，然后通过并行强化学习 (ParaRL) 进行进一步优化，这是一种沿轨迹应用语义奖励以强制跨模式一致性的新颖策略。实验验证，我们的模型显着提高了跨模态对齐和语义一致性，与最先进的模型 Bagel 相比，在 ParaBench 上的输出对齐实现了 6.9% 的改进，为思维感知图像合成建立了更强大的范例。我们的代码在此 https URL 上开源</li>
</ul>

<h3>Title: Boosted GFlowNets: Improving Exploration via Sequential Learning</h3>
<ul>
<li><strong>Authors: </strong>Pedro Dall'Antonia, Tiago da Silva, Daniel Augusto de Souza, César Lincoln C. Mattos, Diego Mesquita</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09677">https://arxiv.org/abs/2511.09677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09677">https://arxiv.org/pdf/2511.09677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09677]] Boosted GFlowNets: Improving Exploration via Sequential Learning(https://arxiv.org/abs/2511.09677)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative Flow Networks (GFlowNets) are powerful samplers for compositional objects that, by design, sample proportionally to a given non-negative reward. Nonetheless, in practice, they often struggle to explore the reward landscape evenly: trajectories toward easy-to-reach regions dominate training, while hard-to-reach modes receive vanishing or uninformative gradients, leading to poor coverage of high-reward areas. We address this imbalance with Boosted GFlowNets, a method that sequentially trains an ensemble of GFlowNets, each optimizing a residual reward that compensates for the mass already captured by previous models. This residual principle reactivates learning signals in underexplored regions and, under mild assumptions, ensures a monotone non-degradation property: adding boosters cannot worsen the learned distribution and typically improves it. Empirically, Boosted GFlowNets achieve substantially better exploration and sample diversity on multimodal synthetic benchmarks and peptide design tasks, while preserving the stability and simplicity of standard trajectory-balance training.</li>
<li><strong>摘要：</strong>生成流网络 (GFlowNets) 是针对组合对象的强大采样器，根据设计，它会根据给定的非负奖励按比例进行采样。尽管如此，在实践中，他们常常很难均匀地探索奖励景观：容易到达的区域的轨迹在训练中占主导地位，而难以到达的模式则接受消失或信息不足的梯度，导致高奖励区域的覆盖范围很差。我们通过 Boosted GFlowNets 解决了这种不平衡问题，这是一种顺序训练 GFlowNets 集合的方法，每个 GFlowNets 优化一个剩余奖励，以补偿先前模型已经捕获的质量。这种残差原理重新激活了未开发区域中的学习信号，并且在温和的假设下确保了单调的非退化特性：添加助推器不会恶化学习的分布，并且通常会改善它。根据经验，Boosted GFlowNet 在多模式合成基准和肽设计任务上实现了更好的探索和样本多样性，同时保持了标准轨迹平衡训练的稳定性和简单性。</li>
</ul>

<h3>Title: SEBA: Sample-Efficient Black-Box Attacks on Visual Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Tairan Huang, Yulin Jin, Junxu Liu, Qingqing Ye, Haibo Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09681">https://arxiv.org/abs/2511.09681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09681">https://arxiv.org/pdf/2511.09681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09681]] SEBA: Sample-Efficient Black-Box Attacks on Visual Reinforcement Learning(https://arxiv.org/abs/2511.09681)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Visual reinforcement learning has achieved remarkable progress in visual control and robotics, but its vulnerability to adversarial perturbations remains underexplored. Most existing black-box attacks focus on vector-based or discrete-action RL, and their effectiveness on image-based continuous control is limited by the large action space and excessive environment queries. We propose SEBA, a sample-efficient framework for black-box adversarial attacks on visual RL agents. SEBA integrates a shadow Q model that estimates cumulative rewards under adversarial conditions, a generative adversarial network that produces visually imperceptible perturbations, and a world model that simulates environment dynamics to reduce real-world queries. Through a two-stage iterative training procedure that alternates between learning the shadow model and refining the generator, SEBA achieves strong attack performance while maintaining efficiency. Experiments on MuJoCo and Atari benchmarks show that SEBA significantly reduces cumulative rewards, preserves visual fidelity, and greatly decreases environment interactions compared to prior black-box and white-box methods.</li>
<li><strong>摘要：</strong>视觉强化学习在视觉控制和机器人技术方面取得了显着进展，但其对抗性扰动的脆弱性仍未得到充分研究。大多数现有的黑盒攻击都集中在基于矢量或离散动作的强化学习上，其基于图像的连续控制的有效性受到大动作空间和过多环境查询的限制。我们提出了 SEBA，一种用于对视觉 RL 代理进行黑盒对抗攻击的样本高效框架。 SEBA 集成了一个在对抗条件下估计累积奖励的影子 Q 模型、一个产生视觉上难以察觉的扰动的生成对抗网络，以及一个模拟环境动态以减少现实世界查询的世界模型。通过学习影子模型和完善生成器之间交替的两阶段迭代训练过程，SEBA 在保持效率的同时实现了强大的攻击性能。 MuJoCo 和 Atari 基准测试表明，与之前的黑盒和白盒方法相比，SEBA 显着减少了累积奖励，保留了视觉保真度，并大大减少了环境交互。</li>
</ul>

<h3>Title: SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control</h3>
<ul>
<li><strong>Authors: </strong>Arman Zarei, Samyadeep Basu, Mobina Pournemat, Sayan Nag, Ryan Rossi, Soheil Feizi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09715">https://arxiv.org/abs/2511.09715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09715">https://arxiv.org/pdf/2511.09715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09715]] SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control(https://arxiv.org/abs/2511.09715)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Instruction-based image editing models have recently achieved impressive performance, enabling complex edits to an input image from a multi-instruction prompt. However, these models apply each instruction in the prompt with a fixed strength, limiting the user's ability to precisely and continuously control the intensity of individual edits. We introduce SliderEdit, a framework for continuous image editing with fine-grained, interpretable instruction control. Given a multi-part edit instruction, SliderEdit disentangles the individual instructions and exposes each as a globally trained slider, allowing smooth adjustment of its strength. Unlike prior works that introduced slider-based attribute controls in text-to-image generation, typically requiring separate training or fine-tuning for each attribute or concept, our method learns a single set of low-rank adaptation matrices that generalize across diverse edits, attributes, and compositional instructions. This enables continuous interpolation along individual edit dimensions while preserving both spatial locality and global semantic consistency. We apply SliderEdit to state-of-the-art image editing models, including FLUX-Kontext and Qwen-Image-Edit, and observe substantial improvements in edit controllability, visual consistency, and user steerability. To the best of our knowledge, we are the first to explore and propose a framework for continuous, fine-grained instruction control in instruction-based image editing models. Our results pave the way for interactive, instruction-driven image manipulation with continuous and compositional control.</li>
<li><strong>摘要：</strong>基于指令的图像编辑模型最近取得了令人印象深刻的性能，可以通过多指令提示对输入图像进行复杂的编辑。然而，这些模型以固定的强度应用提示中的每条指令，限制了用户精确且连续地控制单个编辑强度的能力。我们推出 SliderEdit，这是一个具有细粒度、可解释指令控制的连续图像编辑框架。给定多部分编辑指令，SliderEdit 会解开各个指令，并将每个指令公开为全局训练的滑块，从而可以平滑调整其强度。与之前在文本到图像生成中引入基于滑块的属性控制的工作不同，通常需要对每个属性或概念进行单独的训练或微调，我们的方法学习一组低阶适应矩阵，这些矩阵可以泛化不同的编辑、属性和组合指令。这使得能够沿着各个编辑维度进行连续插值，同时保留空间局部性和全局语义一致性。我们将 SliderEdit 应用于最先进的图像编辑模型，包括 FLUX-Kontext 和 Qwen-Image-Edit，并观察到编辑可控性、视觉一致性和用户可操纵性方面的实质性改进。据我们所知，我们是第一个探索并提出基于指令的图像编辑模型中连续、细粒度指令控制的框架的人。我们的结果为具有连续和构图控制的交互式、指令驱动的图像处理铺平了道路。</li>
</ul>

<h3>Title: FlowCast: Advancing Precipitation Nowcasting with Conditional Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Bernardo Perrone Ribeiro, Jana Faganeli Pucer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09731">https://arxiv.org/abs/2511.09731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09731">https://arxiv.org/pdf/2511.09731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09731]] FlowCast: Advancing Precipitation Nowcasting with Conditional Flow Matching(https://arxiv.org/abs/2511.09731)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Radar-based precipitation nowcasting, the task of forecasting short-term precipitation fields from previous radar images, is a critical problem for flood risk management and decision-making. While deep learning has substantially advanced this field, two challenges remain fundamental: the uncertainty of atmospheric dynamics and the efficient modeling of high-dimensional data. Diffusion models have shown strong promise by producing sharp, reliable forecasts, but their iterative sampling process is computationally prohibitive for time-critical applications. We introduce FlowCast, the first model to apply Conditional Flow Matching (CFM) to precipitation nowcasting. Unlike diffusion, CFM learns a direct noise-to-data mapping, enabling rapid, high-fidelity sample generation with drastically fewer function evaluations. Our experiments demonstrate that FlowCast establishes a new state-of-the-art in predictive accuracy. A direct comparison further reveals the CFM objective is both more accurate and significantly more efficient than a diffusion objective on the same architecture, maintaining high performance with significantly fewer sampling steps. This work positions CFM as a powerful and practical alternative for high-dimensional spatiotemporal forecasting.</li>
<li><strong>摘要：</strong>基于雷达的降水临近预报是根据先前雷达图像预测短期降水场的任务，是洪水风险管理和决策的关键问题。虽然深度学习极大地推进了这一领域的发展，但仍然存在两个根本挑战：大气动力学的不确定性和高维数据的有效建模。扩散模型通过产生敏锐、可靠的预测而显示出强大的前景，但其迭代采样过程对于时间关键型应用来说在计算上是令人望而却步的。我们介绍 FlowCast，这是第一个将条件流量匹配 (CFM) 应用于降水临近预报的模型。与扩散不同，CFM 学习直接噪声到数据的映射，从而能够快速、高保真地生成样本，同时大大减少函数评估。我们的实验表明 FlowCast 在预测准确性方面建立了新的最先进技术。直接比较进一步表明，在相同架构上，CFM 物镜比扩散物镜更准确且效率更高，能够以显着更少的采样步骤保持高性能。这项工作将 CFM 定位为高维时空预测的强大且实用的替代方案。</li>
</ul>

<h3>Title: Gradient-Guided Exploration of Generative Model's Latent Space for Controlled Iris Image Augmentations</h3>
<ul>
<li><strong>Authors: </strong>Mahsa Mitcheff, Siamul Karim Khan, Adam Czajka</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09749">https://arxiv.org/abs/2511.09749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09749">https://arxiv.org/pdf/2511.09749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09749]] Gradient-Guided Exploration of Generative Model's Latent Space for Controlled Iris Image Augmentations(https://arxiv.org/abs/2511.09749)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Developing reliable iris recognition and presentation attack detection methods requires diverse datasets that capture realistic variations in iris features and a wide spectrum of anomalies. Because of the rich texture of iris images, which spans a wide range of spatial frequencies, synthesizing same-identity iris images while controlling specific attributes remains challenging. In this work, we introduce a new iris image augmentation strategy by traversing a generative model's latent space toward latent codes that represent same-identity samples but with some desired iris image properties manipulated. The latent space traversal is guided by a gradient of specific geometrical, textural, or quality-related iris image features (e.g., sharpness, pupil size, iris size, or pupil-to-iris ratio) and preserves the identity represented by the image being manipulated. The proposed approach can be easily extended to manipulate any attribute for which a differentiable loss term can be formulated. Additionally, our approach can use either randomly generated images using either a pre-train GAN model or real-world iris images. We can utilize GAN inversion to project any given iris image into the latent space and obtain its corresponding latent code.</li>
<li><strong>摘要：</strong>开发可靠的虹膜识别和呈现攻击检测方法需要多样化的数据集来捕获虹膜特征的真实变化和广泛的异常情况。由于虹膜图像具有丰富的纹理，跨越了广泛的空间频率，因此在控制特定属性的同时合成相同身份的虹膜图像仍然具有挑战性。在这项工作中，我们引入了一种新的虹膜图像增强策略，通过遍历生成模型的潜在空间，寻找表示相同身份样本但操纵了一些所需虹膜图像属性的潜在代码。潜在空间遍历由特定几何、纹理或质量相关虹膜图像特征（例如，清晰度、瞳孔大小、虹膜大小或瞳孔与虹膜比率）的梯度引导，并保留被操纵的图像所表示的身份。所提出的方法可以很容易地扩展到操纵可以制定可微损失项的任何属性。此外，我们的方法可以使用预训练 GAN 模型随机生成的图像或真实世界的虹膜图像。我们可以利用 GAN 反转将任何给定的虹膜图像投影到潜在空间中并获得其相应的潜在代码。</li>
</ul>

<h3>Title: Lumos3D: A Single-Forward Framework for Low-Light 3D Scene Restoration</h3>
<ul>
<li><strong>Authors: </strong>Hanzhou Liu, Peng Jiang, Jia Huang, Mi Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09818">https://arxiv.org/abs/2511.09818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09818">https://arxiv.org/pdf/2511.09818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09818]] Lumos3D: A Single-Forward Framework for Low-Light 3D Scene Restoration(https://arxiv.org/abs/2511.09818)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Restoring 3D scenes captured under low-light con- ditions remains a fundamental yet challenging problem. Most existing approaches depend on precomputed camera poses and scene-specific optimization, which greatly restricts their scala- bility to dynamic real-world environments. To overcome these limitations, we introduce Lumos3D, a generalizable pose-free framework for 3D low-light scene restoration. Trained once on a single dataset, Lumos3D performs inference in a purely feed- forward manner, directly restoring illumination and structure from unposed, low-light multi-view images without any per- scene training or optimization. Built upon a geometry-grounded backbone, Lumos3D reconstructs a normal-light 3D Gaussian representation that restores illumination while faithfully pre- serving structural details. During training, a cross-illumination distillation scheme is employed, where the teacher network is distilled on normal-light ground truth to transfer accurate geometric information, such as depth, to the student model. A dedicated Lumos loss is further introduced to promote photomet- ric consistency within the reconstructed 3D space. Experiments on real-world datasets demonstrate that Lumos3D achieves high- fidelity low-light 3D scene restoration with accurate geometry and strong generalization to unseen cases. Furthermore, the framework naturally extends to handle over-exposure correction, highlighting its versatility for diverse lighting restoration tasks.</li>
<li><strong>摘要：</strong>恢复在弱光条件下捕获的 3D 场景仍然是一个基本但具有挑战性的问题。大多数现有方法依赖于预先计算的相机姿势和特定于场景的优化，这极大地限制了它们在动态现实环境中的可扩展性。为了克服这些限制，我们引入了 Lumos3D，这是一种用于 3D 低光场景恢复的通用无姿势框架。 Lumos3D 在单个数据集上训练一次，以纯粹前馈的方式执行推理，直接从未摆姿势的低光多视图图像中恢复照明和结构，无需任何每个场景的训练或优化。 Lumos3D 建立在以几何为基础的主干之上，重建了正常光 3D 高斯表示，可恢复照明，同时忠实地保留结构细节。在训练过程中，采用交叉照明蒸馏方案，其中教师网络在正常光地面实况上进行蒸馏，以将准确的几何信息（例如深度）传输到学生模型。进一步引入了专用的 Lumos 损失，以提高重建 3D 空间内的光度一致性。对真实世界数据集的实验表明，Lumos3D 能够实现高保真低光 3D 场景恢复，具有精确的几何形状和对未见过的情况的强大泛化能力。此外，该框架自然地扩展到处理过度曝光校正，突出了其针对不同照明恢复任务的多功能性。</li>
</ul>

<h3>Title: Steering Pretrained Drafters during Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Frédéric Berdoz, Peer Rheinboldt, Roger Wattenhofer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09844">https://arxiv.org/abs/2511.09844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09844">https://arxiv.org/pdf/2511.09844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09844]] Steering Pretrained Drafters during Speculative Decoding(https://arxiv.org/abs/2511.09844)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Speculative decoding accelerates language model inference by separating generation into fast drafting and parallel verification. Its main limitation is drafter-verifier misalignment, which limits token acceptance and reduces overall effectiveness. While small drafting heads trained from scratch compensate with speed, they struggle when verification dominates latency or when inputs are out of distribution. In contrast, pretrained drafters, though slower, achieve higher acceptance rates thanks to stronger standalone generation capabilities, making them competitive when drafting latency is negligible relative to verification or communication overhead. In this work, we aim to improve the acceptance rates of pretrained drafters by introducing a lightweight dynamic alignment mechanism: a steering vector computed from the verifier's hidden states and injected into the pretrained drafter. Compared to existing offline alignment methods such as distillation, our approach boosts the number of accepted tokens by up to 35\% under standard sampling and 22\% under greedy sampling, all while incurring negligible computational overhead. Importantly, our approach can be retrofitted to existing architectures and pretrained models, enabling rapid adoption.</li>
<li><strong>摘要：</strong>推测性解码通过将生成分离为快速起草和并行验证来加速语言模型推理。它的主要限制是起草者与验证者之间的不一致，这限制了代币的接受并降低了整体效率。虽然从头开始训练的小型绘图头可以补偿速度，但当验证主导延迟或输入不分配时，它们就会陷入困境。相比之下，经过预训练的绘图员虽然速度较慢，但​​由于具有更强的独立生成功能，因此获得了更高的接受率，这使得它们在绘图延迟相对于验证或通信开销可以忽略不计时具有竞争力。在这项工作中，我们的目标是通过引入轻量级动态对齐机制来提高预训练绘图器的接受率：根据验证者的隐藏状态计算并注入到预训练绘图器中的转向向量。与现有的离线对齐方法（例如蒸馏）相比，我们的方法在标准采样下将接受的令牌数量提高了 35%，在贪婪采样下提高了 22%，同时产生的计算开销可以忽略不计。重要的是，我们的方法可以针对现有架构和预训练模型进行改造，从而实现快速采用。</li>
</ul>

<h3>Title: Simulator and Experience Enhanced Diffusion Model for Comprehensive ECG Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiaoda Wang, Kaiqiao Han, Yuhao Xu, Xiao Luo, Yizhou Sun, Wei Wang, Carl Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09895">https://arxiv.org/abs/2511.09895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09895">https://arxiv.org/pdf/2511.09895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09895]] Simulator and Experience Enhanced Diffusion Model for Comprehensive ECG Generation(https://arxiv.org/abs/2511.09895)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Cardiovascular disease (CVD) is a leading cause of mortality worldwide. Electrocardiograms (ECGs) are the most widely used non-invasive tool for cardiac assessment, yet large, well-annotated ECG corpora are scarce due to cost, privacy, and workflow constraints. Generating ECGs can be beneficial for the mechanistic understanding of cardiac electrical activity, enable the construction of large, heterogeneous, and unbiased datasets, and facilitate privacy-preserving data sharing. Generating realistic ECG signals from clinical context is important yet underexplored. Recent work has leveraged diffusion models for text-to-ECG generation, but two challenges remain: (i) existing methods often overlook the physiological simulator knowledge of cardiac activity; and (ii) they ignore broader, experience-based clinical knowledge grounded in real-world practice. To address these gaps, we propose SE-Diff, a novel physiological simulator and experience enhanced diffusion model for comprehensive ECG generation. SE-Diff integrates a lightweight ordinary differential equation (ODE)-based ECG simulator into the diffusion process via a beat decoder and simulator-consistent constraints, injecting mechanistic priors that promote physiologically plausible waveforms. In parallel, we design an LLM-powered experience retrieval-augmented strategy to inject clinical knowledge, providing more guidance for ECG generation. Extensive experiments on real-world ECG datasets demonstrate that SE-Diff improves both signal fidelity and text-ECG semantic alignment over baselines, proving its superiority for text-to-ECG generation. We further show that the simulator-based and experience-based knowledge also benefit downstream ECG classification.</li>
<li><strong>摘要：</strong>心血管疾病（CVD）是全世界死亡的主要原因。心电图 (ECG) 是最广泛使用的心脏评估非侵入性工具，但由于成本、隐私和工作流程的限制，大型且注释良好的心电图语料库非常稀缺。生成心电图有利于心电活动的机制理解，能够构建大型、异构和无偏见的数据集，并促进保护隐私的数据共享。从临床背景中生成真实的心电图信号很重要，但尚未得到充分探索。最近的工作利用扩散模型来生成文本到心电图，但仍然存在两个挑战：（i）现有方法经常忽视心脏活动的生理模拟器知识； (ii) 他们忽视了基于现实世界实践的更广泛的、基于经验的临床知识。为了解决这些差距，我们提出了 SE-Diff，一种新颖的生理模拟器和体验增强扩散模型，用于生成全面的心电图。 SE-Diff 通过节拍解码器和模拟器一致的约束将基于轻量级常微分方程 (ODE) 的 ECG 模拟器集成到扩散过程中，注入促进生理上合理波形的机械先验。与此同时，我们设计了一种由法学硕士支持的经验检索增强策略来注入临床知识，为心电图生成提供更多指导。对真实心电图数据集的大量实验表明，SE-Diff 提高了信号保真度和文本心电图语义对齐能力，证明了其在文本到心电图生成方面的优越性。我们进一步表明，基于模拟器和基于经验的知识也有利于下游心电图分类。</li>
</ul>

<h3>Title: Incremental Generation is Necessity and Sufficient for Universality in Flow-Based Modelling</h3>
<ul>
<li><strong>Authors: </strong>Hossein Rouhvarzi, Anastasis Kratsios</a></li>
<li><strong>Subjects: </strong>cs.LG, math.CA, math.DS, math.NA, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09902">https://arxiv.org/abs/2511.09902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09902">https://arxiv.org/pdf/2511.09902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09902]] Incremental Generation is Necessity and Sufficient for Universality in Flow-Based Modelling(https://arxiv.org/abs/2511.09902)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Incremental flow-based denoising models have reshaped generative modelling, but their empirical advantage still lacks a rigorous approximation-theoretic foundation. We show that incremental generation is necessary and sufficient for universal flow-based generation on the largest natural class of self-maps of $[0,1]^d$ compatible with denoising pipelines, namely the orientation-preserving homeomorphisms of $[0,1]^d$. All our guarantees are uniform on the underlying maps and hence imply approximation both samplewise and in distribution. Using a new topological-dynamical argument, we first prove an impossibility theorem: the class of all single-step autonomous flows, independently of the architecture, width, depth, or Lipschitz activation of the underlying neural network, is meagre and therefore not universal in the space of orientation-preserving homeomorphisms of $[0,1]^d$. By exploiting algebraic properties of autonomous flows, we conversely show that every orientation-preserving Lipschitz homeomorphism on $[0,1]^d$ can be approximated at rate $\mathcal{O}(n^{-1/d})$ by a composition of at most $K_d$ such flows, where $K_d$ depends only on the dimension. Under additional smoothness assumptions, the approximation rate can be made dimension-free, and $K_d$ can be chosen uniformly over the class being approximated. Finally, by linearly lifting the domain into one higher dimension, we obtain structured universal approximation results for continuous functions and for probability measures on $[0,1]^d$, the latter realized as pushforwards of empirical measures with vanishing $1$-Wasserstein error.</li>
<li><strong>摘要：</strong>基于增量流的去噪模型重塑了生成模型，但其经验优势仍然缺乏严格的近似理论基础。我们证明，增量生成对于与去噪管道兼容的最大自然类 $[0,1]^d$ 的自映射（即 $[0,1]^d$ 的方向保持同态）上的通用基于流的生成是必要和充分的。我们所有的保证在底层地图上都是统一的，因此意味着样本和分布上的近似。使用新的拓扑动力学论证，我们首先证明一个不可能定理：所有单步自主流的类，与底层神经网络的架构、宽度、深度或 Lipschitz 激活无关，都是微不足道的，因此在 $[0,1]^d$ 的方向保持同态空间中不具有普遍性。通过利用自主流的代数性质，我们反过来证明，$[0,1]^d$ 上的每个保持方向的 Lipschitz 同胚都可以通过最多 $K_d$ 个这样的流的组合以 $\mathcal{O}(n^{-1/d})$ 的速率来近似，其中 $K_d$ 仅取决于维度。在额外的平滑度假设下，近似率可以是无量纲的，并且可以在被近似的类上统一选择$K_d$。最后，通过将域线性提升到一个更高的维度，我们获得了连续函数和 $[0,1]^d$ 上的概率度量的结构化通用逼近结果，后者实现为具有消失 $1$-Wasserstein 误差的经验度量的前推。</li>
</ul>

<h3>Title: PRISM: Diversifying Dataset Distillation by Decoupling Architectural Priors</h3>
<ul>
<li><strong>Authors: </strong>Brian B. Moser, Shalini Strode, Federico Raue, Stanislav Frolov, Krzysztof Adamkiewicz, Arundhati Shanbhag, Joachim Folk, Tobias C. Nauen, Andreas Dengel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09905">https://arxiv.org/abs/2511.09905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09905">https://arxiv.org/pdf/2511.09905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09905]] PRISM: Diversifying Dataset Distillation by Decoupling Architectural Priors(https://arxiv.org/abs/2511.09905)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Dataset distillation (DD) promises compact yet faithful synthetic data, but existing approaches often inherit the inductive bias of a single teacher model. As dataset size increases, this bias drives generation toward overly smooth, homogeneous samples, reducing intra-class diversity and limiting generalization. We present PRISM (PRIors from diverse Source Models), a framework that disentangles architectural priors during synthesis. PRISM decouples the logit-matching and regularization objectives, supervising them with different teacher architectures: a primary model for logits and a stochastic subset for batch-normalization (BN) alignment. On ImageNet-1K, PRISM consistently and reproducibly outperforms single-teacher methods (e.g., SRe2L) and recent multi-teacher variants (e.g., G-VBSM) at low- and mid-IPC regimes. The generated data also show significantly richer intra-class diversity, as reflected by a notable drop in cosine similarity between features. We further analyze teacher selection strategies (pre- vs. intra-distillation) and introduce a scalable cross-class batch formation scheme for fast parallel synthesis. Code will be released after the review period.</li>
<li><strong>摘要：</strong>数据集蒸馏（DD）承诺紧凑而忠实的合成数据，但现有方法通常继承单个教师模型的归纳偏差。随着数据集大小的增加，这种偏差会导致生成过于平滑、同质的样本，从而减少类内多样性并限制泛化。我们提出了 PRISM（来自不同源模型的 PRIors），这是一个在综合过程中解开架构先验的框架。 PRISM 将 logit 匹配和正则化目标解耦，用不同的教师架构来监督它们：logit 的主要模型和批量归一化 (BN) 对齐的随机子集。在 ImageNet-1K 上，PRISM 在中低 IPC 状态下始终且可重复地优于单教师方法（例如 SRe2L）和最近的多教师变体（例如 G-VBSM）。生成的数据还显示出明显更丰富的类内多样性，这反映在特征之间的余弦相似度显着下降。我们进一步分析教师选择策略（预蒸馏与内蒸馏），并引入可扩展的跨类批次形成方案以实现快速并行合成。代码将在审核期结束后发布。</li>
</ul>

<h3>Title: EEGAgent: A Unified Framework for Automated EEG Analysis Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sha Zhao, Mingyi Peng, Haiteng Jiang, Tao Li, Shijian Li, Gang Pan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09947">https://arxiv.org/abs/2511.09947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09947">https://arxiv.org/pdf/2511.09947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09947]] EEGAgent: A Unified Framework for Automated EEG Analysis Using Large Language Models(https://arxiv.org/abs/2511.09947)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Scalable and generalizable analysis of brain activity is essential for advancing both clinical diagnostics and cognitive research. Electroencephalography (EEG), a non-invasive modality with high temporal resolution, has been widely used for brain states analysis. However, most existing EEG models are usually tailored for individual specific tasks, limiting their utility in realistic scenarios where EEG analysis often involves multi-task and continuous reasoning. In this work, we introduce EEGAgent, a general-purpose framework that leverages large language models (LLMs) to schedule and plan multiple tools to automatically complete EEG-related tasks. EEGAgent is capable of performing the key functions: EEG basic information perception, spatiotemporal EEG exploration, EEG event detection, interaction with users, and EEG report generation. To realize these capabilities, we design a toolbox composed of different tools for EEG preprocessing, feature extraction, event detection, etc. These capabilities were evaluated on public datasets, and our EEGAgent can support flexible and interpretable EEG analysis, highlighting its potential for real-world clinical applications.</li>
<li><strong>摘要：</strong>对大脑活动进行可扩展和可推广的分析对于推进临床诊断和认知研究至关重要。脑电图（EEG）是一种具有高时间分辨率的非侵入性模式，已广泛用于大脑状态分析。然而，大多数现有的脑电图模型通常是针对单个特定任务量身定制的，这限制了它们在脑电图分析通常涉及多任务和连续推理的现实场景中的实用性。在这项工作中，我们介绍了 EEGAgent，这是一个通用框架，它利用大型语言模型 (LLM) 来调度和规划多个工具来自动完成脑电图相关任务。 EEGAgent能够执行以下关键功能：脑电基本信息感知、时空脑电探索、脑电事件检测、与用户交互以及脑电报告生成。为了实现这些功能，我们设计了一个由不同工具组成的工具箱，用于脑电图预处理、特征提取、事件检测等。这些功能在公共数据集上进行了评估，我们的 EEGAgent 可以支持灵活且可解释的脑电图分析，凸显了其在现实世界临床应用中的潜力。</li>
</ul>

<h3>Title: Beyond Cosine Similarity Magnitude-Aware CLIP for No-Reference Image Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Zhicheng Liao, Dongxu Wu, Zhenshan Shi, Sijie Mai, Hanwei Zhu, Lingyu Zhu, Yuncheng Jiang, Baoliang Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09948">https://arxiv.org/abs/2511.09948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09948">https://arxiv.org/pdf/2511.09948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09948]] Beyond Cosine Similarity Magnitude-Aware CLIP for No-Reference Image Quality Assessment(https://arxiv.org/abs/2511.09948)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Recent efforts have repurposed the Contrastive Language-Image Pre-training (CLIP) model for No-Reference Image Quality Assessment (NR-IQA) by measuring the cosine similarity between the image embedding and textual prompts such as "a good photo" or "a bad photo." However, this semantic similarity overlooks a critical yet underexplored cue: the magnitude of the CLIP image features, which we empirically find to exhibit a strong correlation with perceptual quality. In this work, we introduce a novel adaptive fusion framework that complements cosine similarity with a magnitude-aware quality cue. Specifically, we first extract the absolute CLIP image features and apply a Box-Cox transformation to statistically normalize the feature distribution and mitigate semantic sensitivity. The resulting scalar summary serves as a semantically-normalized auxiliary cue that complements cosine-based prompt matching. To integrate both cues effectively, we further design a confidence-guided fusion scheme that adaptively weighs each term according to its relative strength. Extensive experiments on multiple benchmark IQA datasets demonstrate that our method consistently outperforms standard CLIP-based IQA and state-of-the-art baselines, without any task-specific training.</li>
<li><strong>摘要：</strong>最近的工作通过测量图像嵌入和文本提示（例如“一张好照片”或“一张坏照片”）之间的余弦相似度，将对比语言图像预训练（CLIP）模型重新用于无参考图像质量评估（NR-IQA）。然而，这种语义相似性忽略了一个关键但尚未充分探索的线索：CLIP 图像特征的大小，我们凭经验发现它与感知质量表现出很强的相关性。在这项工作中，我们引入了一种新颖的自适应融合框架，该框架通过幅度感知质量提示补充了余弦相似性。具体来说，我们首先提取绝对 CLIP 图像特征并应用 Box-Cox 变换来统计标准化特征分布并减轻语义敏感性。生成的标量摘要充当语义标准化的辅助提示，补充基于余弦的提示匹配。为了有效地整合这两个线索，我们进一步设计了一种置信引导的融合方案，根据每个项的相对强度自适应地对其进行加权。对多个基准 IQA 数据集的广泛实验表明，我们的方法始终优于基于 CLIP 的标准 IQA 和最先进的基线，而无需任何特定于任务的训练。</li>
</ul>

<h3>Title: Equivariant Sampling for Improving Diffusion Model-based Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Chenxu Wu, Qingpeng Kong, Peiang Zhao, Wendi Yang, Wenxin Ma, Fenghe Tang, Zihang Jiang, S.Kevin Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.09965">https://arxiv.org/abs/2511.09965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.09965">https://arxiv.org/pdf/2511.09965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.09965]] Equivariant Sampling for Improving Diffusion Model-based Image Restoration(https://arxiv.org/abs/2511.09965)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative models, especially diffusion models, have significantly improved image restoration (IR) performance. However, existing problem-agnostic diffusion model-based image restoration (DMIR) methods face challenges in fully leveraging diffusion priors, resulting in suboptimal performance. In this paper, we address the limitations of current problem-agnostic DMIR methods by analyzing their sampling process and providing effective solutions. We introduce EquS, a DMIR method that imposes equivariant information through dual sampling trajectories. To further boost EquS, we propose the Timestep-Aware Schedule (TAS) and introduce EquS$^+$. TAS prioritizes deterministic steps to enhance certainty and sampling efficiency. Extensive experiments on benchmarks demonstrate that our method is compatible with previous problem-agnostic DMIR methods and significantly boosts their performance without increasing computational costs. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>生成模型（尤其是扩散模型）的最新进展显着提高了图像恢复（IR）性能。然而，现有的与问题无关的基于扩散模型的图像恢复（DMIR）方法在充分利用扩散先验方面面临挑战，导致性能不佳。在本文中，我们通过分析采样过程并提供有效的解决方案来解决当前与问题无关的 DMIR 方法的局限性。我们引入了 EquS，这是一种通过双采样轨迹施加等变信息的 DMIR 方法。为了进一步提升 EquS，我们提出了 Timestep-Aware Schedule (TAS) 并引入了 EquS$^+$。 TAS 优先考虑确定性步骤以提高确定性和采样效率。对基准的大量实验表明，我们的方法与以前的问题无关的 DMIR 方法兼容，并且在不增加计算成本的情况下显着提高了它们的性能。我们的代码可以在这个 https URL 上找到。</li>
</ul>

<h3>Title: Anomagic: Crossmodal Prompt-driven Zero-shot Anomaly Generation</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Jiang, Wei Luo, Hui Zhang, Qiyu Chen, Haiming Yao, Weiming Shen, Yunkang Cao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.10020">https://arxiv.org/abs/2511.10020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.10020">https://arxiv.org/pdf/2511.10020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.10020]] Anomagic: Crossmodal Prompt-driven Zero-shot Anomaly Generation(https://arxiv.org/abs/2511.10020)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We propose Anomagic, a zero-shot anomaly generation method that produces semantically coherent anomalies without requiring any exemplar anomalies. By unifying both visual and textual cues through a crossmodal prompt encoding scheme, Anomagic leverages rich contextual information to steer an inpainting-based generation pipeline. A subsequent contrastive refinement strategy enforces precise alignment between synthesized anomalies and their masks, thereby bolstering downstream anomaly detection accuracy. To facilitate training, we introduce AnomVerse, a collection of 12,987 anomaly-mask-caption triplets assembled from 13 publicly available datasets, where captions are automatically generated by multimodal large language models using structured visual prompts and template-based textual hints. Extensive experiments demonstrate that Anomagic trained on AnomVerse can synthesize more realistic and varied anomalies than prior methods, yielding superior improvements in downstream anomaly detection. Furthermore, Anomagic can generate anomalies for any normal-category image using user-defined prompts, establishing a versatile foundation model for anomaly generation.</li>
<li><strong>摘要：</strong>我们提出了 Anomagic，一种零样本异常生成方法，可以产生语义一致的异常，而不需要任何示例异常。通过跨模式提示编码方案统一视觉和文本提示，Anomagic 利用丰富的上下文信息来引导基于修复的生成管道。随后的对比细化策略强制合成异常与其掩模之间的精确对齐，从而提高下游异常检测的准确性。为了促进训练，我们引入了 AnomVerse，它是由 13 个公开数据集组装而成的 12,987 个异常掩码字幕三元组的集合，其中字幕是由多模式大语言模型使用结构化视觉提示和基于模板的文本提示自动生成的。大量实验表明，在 AnomVerse 上训练的 Anomagic 可以合成比之前的方法更真实、更多样化的异常，从而在下游异常检测方面产生卓越的改进。此外，Anomagic 可以使用用户定义的提示为任何正常类别图像生成异常，从而为异常生成建立通用的基础模型。</li>
</ul>

<h3>Title: LoG3D: Ultra-High-Resolution 3D Shape Modeling via Local-to-Global Partitioning</h3>
<ul>
<li><strong>Authors: </strong>Xinran Yang, Shuichang Lai, Jiangjing Lyu, Hongjie Li, Bowen Pan, Yuanqi Li, Jie Guo, Zhou Zhengkang, Yanwen Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.10040">https://arxiv.org/abs/2511.10040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.10040">https://arxiv.org/pdf/2511.10040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.10040]] LoG3D: Ultra-High-Resolution 3D Shape Modeling via Local-to-Global Partitioning(https://arxiv.org/abs/2511.10040)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generating high-fidelity 3D contents remains a fundamental challenge due to the complexity of representing arbitrary topologies-such as open surfaces and intricate internal structures-while preserving geometric details. Prevailing methods based on signed distance fields (SDFs) are hampered by costly watertight preprocessing and struggle with non-manifold geometries, while point-cloud representations often suffer from sampling artifacts and surface discontinuities. To overcome these limitations, we propose a novel 3D variational autoencoder (VAE) framework built upon unsigned distance fields (UDFs)-a more robust and computationally efficient representation that naturally handles complex and incomplete shapes. Our core innovation is a local-to-global (LoG) architecture that processes the UDF by partitioning it into uniform subvolumes, termed UBlocks. This architecture couples 3D convolutions for capturing local detail with sparse transformers for enforcing global coherence. A Pad-Average strategy further ensures smooth transitions at subvolume boundaries during reconstruction. This modular design enables seamless scaling to ultra-high resolutions up to 2048^3-a regime previously unattainable for 3D VAEs. Experiments demonstrate state-of-the-art performance in both reconstruction accuracy and generative quality, yielding superior surface smoothness and geometric flexibility.</li>
<li><strong>摘要：</strong>由于在保留几何细节的同时表示任意拓扑（例如开放表面和复杂的内部结构）的复杂性，生成高保真 3D 内容仍然是一项基本挑战。基于符号距离场 (SDF) 的流行方法受到昂贵的无懈可击的预处理的阻碍，并且难以处理非流形几何形状，而点云表示经常受到采样伪影和表面不连续性的影响。为了克服这些限制，我们提出了一种基于无符号距离场 (UDF) 的新型 3D 变分自动编码器 (VAE) 框架，这是一种更鲁棒且计算效率更高的表示，可以自然地处理复杂和不完整的形状。我们的核心创新是本地到全局 (LoG) 架构，该架构通过将 UDF 划分为统一的子卷（称为 UBlock）来对其进行处理。该架构将用于捕获局部细节的 3D 卷积与用于增强全局一致性的稀疏变换器结合起来。 Pad-Average 策略进一步确保重建期间子体积边界处的平滑过渡。这种模块化设计能够无缝扩展到高达 2048^3 的超高分辨率，这是 3D VAE 以前无法实现的。实验证明了在重建精度和生成质量方面最先进的性能，产生卓越的表面平滑度和几何灵活性。</li>
</ul>

<h3>Title: Image Aesthetic Reasoning via HCM-GRPO: Empowering Compact Model for Superior Performance</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Hu, Zheng Sun, Yi Wei, Long Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.10055">https://arxiv.org/abs/2511.10055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.10055">https://arxiv.org/pdf/2511.10055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.10055]] Image Aesthetic Reasoning via HCM-GRPO: Empowering Compact Model for Superior Performance(https://arxiv.org/abs/2511.10055)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The performance of image generation has been significantly improved in recent years. However, the study of image screening is rare and its performance with Multimodal Large Language Models (MLLMs) is unsatisfactory due to the lack of data and the weak image aesthetic reasoning ability in MLLMs. In this work, we propose a complete solution to address these problems in terms of data and methodology. For data, we collect a comprehensive image screening dataset with over 128k samples, about 640k images. Each sample consists of an original image, four generated images. The dataset evaluates the image aesthetic reasoning ability under four aspects: appearance deformation, physical shadow, placement layout, and extension rationality. Regarding data annotation, we investigate multiple approaches, including purely manual, fully automated, and answer-driven annotations, to acquire high-quality chains of thought (CoT) data in the most cost-effective manner. Methodologically, we introduce a Hard Cases Mining (HCM) strategy with a Dynamic Proportional Accuracy (DPA) reward into the Group Relative Policy Optimization (GRPO) framework, called HCM-GRPO. This enhanced method demonstrates superior image aesthetic reasoning capabilities compared to the original GRPO. Our experimental results reveal that even state-of-the-art closed-source MLLMs, such as GPT4o and Qwen-VL-Max, exhibit performance akin to random guessing in image aesthetic reasoning. In contrast, by leveraging the HCM-GRPO, we are able to surpass the scores of both large-scale open-source and leading closed-source models with a much smaller model.</li>
<li><strong>摘要：</strong>近年来，图像生成的性能得到了显着提高。然而，由于数据缺乏以及多模态大语言模型（MLLM）的图像美学推理能力较弱，图像筛选的研究很少，并且其在多模态大语言模型（MLLM）上的性能并不令人满意。在这项工作中，我们提出了一个完整的解决方案，从数据和方法上解决这些问题。对于数据，我们收集了一个全面的图像筛选数据集，包含超过 128k 个样本，约 640k 个图像。每个样本由一张原始图像、四张生成图像组成。该数据集从外观变形、物理阴影、放置布局、延伸合理性四个方面评估图像审美推理能力。关于数据注释，我们研究了多种方法，包括纯手动、全自动和答案驱动注释，以最具成本效益的方式获取高质量的思想链（CoT）数据。在方法上，我们将具有动态比例精度（DPA）奖励的硬案例挖掘（HCM）策略引入到组相对策略优化（GRPO）框架中，称为HCM-GRPO。与原始 GRPO 相比，这种增强的方法展示了卓越的图像美学推理能力。我们的实验结果表明，即使是最先进的闭源 MLLM，例如 GPT4o 和 Qwen-VL-Max，也能在图像美学推理中表现出类似于随机猜测的性能。相比之下，通过利用 HCM-GRPO，我们能够用更小的模型超越大规模开源模型和领先的闭源模型的分数。</li>
</ul>

<h3>Title: VLF-MSC: Vision-Language Feature-Based Multimodal Semantic Communication System</h3>
<ul>
<li><strong>Authors: </strong>Gwangyeon Ahn, Jiwan Seo, Joonhyuk Kang</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.10074">https://arxiv.org/abs/2511.10074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.10074">https://arxiv.org/pdf/2511.10074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.10074]] VLF-MSC: Vision-Language Feature-Based Multimodal Semantic Communication System(https://arxiv.org/abs/2511.10074)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We propose Vision-Language Feature-based Multimodal Semantic Communication (VLF-MSC), a unified system that transmits a single compact vision-language representation to support both image and text generation at the receiver. Unlike existing semantic communication techniques that process each modality separately, VLF-MSC employs a pre-trained vision-language model (VLM) to encode the source image into a vision-language semantic feature (VLF), which is transmitted over the wireless channel. At the receiver, a decoder-based language model and a diffusion-based image generator are both conditioned on the VLF to produce a descriptive text and a semantically aligned image. This unified representation eliminates the need for modality-specific streams or retransmissions, improving spectral efficiency and adaptability. By leveraging foundation models, the system achieves robustness to channel noise while preserving semantic fidelity. Experiments demonstrate that VLF-MSC outperforms text-only and image-only baselines, achieving higher semantic accuracy for both modalities under low SNR with significantly reduced bandwidth.</li>
<li><strong>摘要：</strong>我们提出了基于视觉语言特征的多模态语义通信（VLF-MSC），这是一个统一的系统，可传输单个紧凑的视觉语言表示以支持接收器处的图像和文本生成。与单独处理每种模态的现有语义通信技术不同，VLF-MSC 采用预先训练的视觉语言模型 (VLM) 将源图像编码为视觉语言语义特征 (VLF)，并通过无线信道传输。在接收器处，基于解码器的语言模型和基于扩散的图像生成器均以 VLF 为条件，以生成描述性文本和语义对齐的图像。这种统一的表示消除了对特定模态流或重传的需要，从而提高了频谱效率和适应性。通过利用基础模型，系统实现了对通道噪声的鲁棒性，同时保持了语义保真度。实验表明，VLF-MSC 的性能优于纯文本和纯图像基线，在低信噪比和显着降低的带宽下，两种模式都实现了更高的语义准确性。</li>
</ul>

<h3>Title: Mitigating Error Accumulation in Co-Speech Motion Generation via Global Rotation Diffusion and Multi-Level Constraints</h3>
<ul>
<li><strong>Authors: </strong>Xiangyue Zhang, Jianfang Li, Jianqiang Ren, Jiaxu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.10076">https://arxiv.org/abs/2511.10076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.10076">https://arxiv.org/pdf/2511.10076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.10076]] Mitigating Error Accumulation in Co-Speech Motion Generation via Global Rotation Diffusion and Multi-Level Constraints(https://arxiv.org/abs/2511.10076)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Reliable co-speech motion generation requires precise motion representation and consistent structural priors across all joints. Existing generative methods typically operate on local joint rotations, which are defined hierarchically based on the skeleton structure. This leads to cumulative errors during generation, manifesting as unstable and implausible motions at end-effectors. In this work, we propose GlobalDiff, a diffusion-based framework that operates directly in the space of global joint rotations for the first time, fundamentally decoupling each joint's prediction from upstream dependencies and alleviating hierarchical error accumulation. To compensate for the absence of structural priors in global rotation space, we introduce a multi-level constraint scheme. Specifically, a joint structure constraint introduces virtual anchor points around each joint to better capture fine-grained orientation. A skeleton structure constraint enforces angular consistency across bones to maintain structural integrity. A temporal structure constraint utilizes a multi-scale variational encoder to align the generated motion with ground-truth temporal patterns. These constraints jointly regularize the global diffusion process and reinforce structural awareness. Extensive evaluations on standard co-speech benchmarks show that GlobalDiff generates smooth and accurate motions, improving the performance by 46.0 % compared to the current SOTA under multiple speaker identities.</li>
<li><strong>摘要：</strong>可靠的协同语音运动生成需要精确的运动表示和所有关节的一致的结构先验。现有的生成方法通常对局部关节旋转进行操作，局部关节旋转是基于骨架结构分层定义的。这会导致生成过程中出现累积误差，表现为末端执行器不稳定且不可信的运动。在这项工作中，我们提出了 GlobalDiff，这是一种基于扩散的框架，首次直接在全局关节旋转空间中运行，从根本上将每个关节的预测与上游依赖关系解耦，并减轻层次误差累积。为了弥补全局旋转空间中结构先验的缺失，我们引入了多级约束方案。具体来说，关节结构约束在每个关节周围引入虚拟锚点，以更好地捕获细粒度方向。骨架结构约束强制骨骼之间的角度一致性，以保持结构完整性。时间结构约束利用多尺度变分编码器将生成的运动与地面真实时间模式对齐。这些限制共同规范了全球扩散过程并强化了结构意识。对标准协同语音基准的广泛评估表明，GlobalDiff 生成平滑且准确的动作，与当前 SOTA 相比，在多说话人身份下性能提高了 46.0%。</li>
</ul>

<h3>Title: T2IBias: Uncovering Societal Bias Encoded in the Latent Space of Text-to-Image Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Abu Sufian, Cosimo Distante, Marco Leo, Hanan Salam</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.10089">https://arxiv.org/abs/2511.10089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.10089">https://arxiv.org/pdf/2511.10089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.10089]] T2IBias: Uncovering Societal Bias Encoded in the Latent Space of Text-to-Image Generative Models(https://arxiv.org/abs/2511.10089)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) generative models are largely used in AI-powered real-world applications and value creation. However, their strategic deployment raises critical concerns for responsible AI management, particularly regarding the reproduction and amplification of race- and gender-related stereotypes that can undermine organizational ethics. In this work, we investigate whether such societal biases are systematically encoded within the pretrained latent spaces of state-of-the-art T2I models. We conduct an empirical study across the five most popular open-source models, using ten neutral, profession-related prompts to generate 100 images per profession, resulting in a dataset of 5,000 images evaluated by diverse human assessors representing different races and genders. We demonstrate that all five models encode and amplify pronounced societal skew: caregiving and nursing roles are consistently feminized, while high-status professions such as corporate CEO, politician, doctor, and lawyer are overwhelmingly represented by males and mostly White individuals. We further identify model-specific patterns, such as QWEN-Image's near-exclusive focus on East Asian outputs, Kandinsky's dominance of White individuals, and SDXL's comparatively broader but still biased distributions. These results provide critical insights for AI project managers and practitioners, enabling them to select equitable AI models and customized prompts that generate images in alignment with the principles of responsible AI. We conclude by discussing the risks of these biases and proposing actionable strategies for bias mitigation in building responsible GenAI systems.</li>
<li><strong>摘要：</strong>文本到图像（T2I）生成模型主要用于人工智能驱动的现实应用和价值创造。然而，它们的战略部署引起了对负责任的人工智能管理的严重担忧，特别是关于可能破坏组织道德的种族和性别相关刻板印象的再现和放大。在这项工作中，我们研究了这种社会偏见是否被系统地编码在最先进的 T2I 模型的预训练潜在空间中。我们对五个最流行的开源模型进行了实证研究，使用 10 个中立的、与职业相关的提示为每个职业生成 100 张图像，从而形成由代表不同种族和性别的不同人类评估员评估的 5,000 张图像的数据集。我们证明，所有五种模式都编码并放大了明显的社会偏差：看护和护理角色始终女性化，而企业首席执行官、政治家、医生和律师等高地位职业绝大多数由男性和大多数白人代表。我们进一步确定了模型特定的模式，例如 QWEN-Image 几乎只关注东亚输出、康定斯基对白人的主导地位以及 SDXL 相对更广泛但仍然存在偏差的分布。这些结果为人工智能项目经理和从业者提供了重要的见解，使他们能够选择公平的人工智能模型和定制提示，生成符合负责任人工智能原则的图像。最后，我们讨论了这些偏见的风险，并提出了在构建负责任的 GenAI 系统时减轻偏见的可行策略。</li>
</ul>

<h3>Title: How does My Model Fail? Automatic Identification and Interpretation of Physical Plausibility Failure Modes with Matryoshka Transcoders</h3>
<ul>
<li><strong>Authors: </strong>Yiming Tang, Abhijeet Sinha, Dianbo Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.10094">https://arxiv.org/abs/2511.10094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.10094">https://arxiv.org/pdf/2511.10094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.10094]] How does My Model Fail? Automatic Identification and Interpretation of Physical Plausibility Failure Modes with Matryoshka Transcoders(https://arxiv.org/abs/2511.10094)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Although recent generative models are remarkably capable of producing instruction-following and realistic outputs, they remain prone to notable physical plausibility failures. Though critical in applications, these physical plausibility errors often escape detection by existing evaluation methods. Furthermore, no framework exists for automatically identifying and interpreting specific physical error patterns in natural language, preventing targeted model improvements. We introduce Matryoshka Transcoders, a novel framework for the automatic discovery and interpretation of physical plausibility features in generative models. Our approach extends the Matryoshka representation learning paradigm to transcoder architectures, enabling hierarchical sparse feature learning at multiple granularity levels. By training on intermediate representations from a physical plausibility classifier and leveraging large multimodal models for interpretation, our method identifies diverse physics-related failure modes without manual feature engineering, achieving superior feature relevance and feature accuracy compared to existing approaches. We utilize the discovered visual patterns to establish a benchmark for evaluating physical plausibility in generative models. Our analysis of eight state-of-the-art generative models provides valuable insights into how these models fail to follow physical constraints, paving the way for further model improvements.</li>
<li><strong>摘要：</strong>尽管最近的生成模型非常能够产生遵循指令和现实的输出，但它们仍然容易出现明显的物理合理性失败。尽管在应用中至关重要，但这些物理合理性错误常常无法被现有评估方法检测到。此外，不存在以自然语言自动识别和解释特定物理错误模式的框架，从而阻碍了有针对性的模型改进。我们介绍了 Matryoshka Transcoders，这是一种用于自动发现和解释生成模型中的物理合理性特征的新颖框架。我们的方法将 Matryoshka 表示学习范式扩展到转码器架构，从而实现多粒度级别的分层稀疏特征学习。通过对物理合理性分类器的中间表示进行训练并利用大型多模态模型进行解释，我们的方法无需手动特征工程即可识别多种与物理相关的故障模式，与现有方法相比，实现了卓越的特征相关性和特征准确性。我们利用发现的视觉模式来建立评估生成模型中物理合理性的基准。我们对八个最先进的生成模型的分析为这些模型如何未能遵循物理约束提供了宝贵的见解，为进一步改进模型铺平了道路。</li>
</ul>

<h3>Title: Right Looks, Wrong Reasons: Compositional Fidelity in Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Mayank Vatsa, Aparna Bharati, Richa Singh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.10136">https://arxiv.org/abs/2511.10136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.10136">https://arxiv.org/pdf/2511.10136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.10136]] Right Looks, Wrong Reasons: Compositional Fidelity in Text-to-Image Generation(https://arxiv.org/abs/2511.10136)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The architectural blueprint of today's leading text-to-image models contains a fundamental flaw: an inability to handle logical composition. This survey investigates this breakdown across three core primitives-negation, counting, and spatial relations. Our analysis reveals a dramatic performance collapse: models that are accurate on single primitives fail precipitously when these are combined, exposing severe interference. We trace this failure to three key factors. First, training data show a near-total absence of explicit negations. Second, continuous attention architectures are fundamentally unsuitable for discrete logic. Third, evaluation metrics reward visual plausibility over constraint satisfaction. By analyzing recent benchmarks and methods, we show that current solutions and simple scaling cannot bridge this gap. Achieving genuine compositionality, we conclude, will require fundamental advances in representation and reasoning rather than incremental adjustments to existing architectures.</li>
<li><strong>摘要：</strong>当今领先的文本到图像模型的架构蓝图包含一个根本缺陷：无法处理逻辑组合。这项调查调查了三个核心原语的细分——否定、计数和空间关系。我们的分析揭示了戏剧性的性能崩溃：在单个基元上准确的模型在将它们组合在一起时会急剧失败，从而暴露出严重的干扰。我们将这一失败归因于三个关键因素。首先，训练数据显示几乎完全没有明确的否定。其次，连续注意力架构从根本上不适合离散逻辑。第三，评估指标奖励视觉合理性而不是约束满意度。通过分析最近的基准和方法，我们表明当前的解决方案和简单的扩展无法弥补这一差距。我们的结论是，要实现真正的组合性，需要在表示和推理方面取得根本性的进步，而不是对现有架构进行增量调整。</li>
</ul>

<h3>Title: GEA: Generation-Enhanced Alignment for Text-to-Image Person Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Hao Zou, Runqing Zhang, Xue Zhou, Jianxiao Zou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.10154">https://arxiv.org/abs/2511.10154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.10154">https://arxiv.org/pdf/2511.10154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.10154]] GEA: Generation-Enhanced Alignment for Text-to-Image Person Retrieval(https://arxiv.org/abs/2511.10154)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Text-to-Image Person Retrieval (TIPR) aims to retrieve person images based on natural language descriptions. Although many TIPR methods have achieved promising results, sometimes textual queries cannot accurately and comprehensively reflect the content of the image, leading to poor cross-modal alignment and overfitting to limited datasets. Moreover, the inherent modality gap between text and image further amplifies these issues, making accurate cross-modal retrieval even more challenging. To address these limitations, we propose the Generation-Enhanced Alignment (GEA) from a generative perspective. GEA contains two parallel modules: (1) Text-Guided Token Enhancement (TGTE), which introduces diffusion-generated images as intermediate semantic representations to bridge the gap between text and visual patterns. These generated images enrich the semantic representation of text and facilitate cross-modal alignment. (2) Generative Intermediate Fusion (GIF), which combines cross-attention between generated images, original images, and text features to generate a unified representation optimized by triplet alignment loss. We conduct extensive experiments on three public TIPR datasets, CUHK-PEDES, RSTPReid, and ICFG-PEDES, to evaluate the performance of GEA. The results justify the effectiveness of our method. More implementation details and extended results are available at this https URL.</li>
<li><strong>摘要：</strong>文本到图像人物检索（TIPR）旨在基于自然语言描述来检索人物图像。尽管许多 TIPR 方法取得了可喜的结果，但有时文本查询无法准确、全面地反映图像的内容，导致跨模态对齐较差以及对有限数据集的过度拟合。此外，文本和图像之间固有的模态差距进一步放大了这些问题，使得准确的跨模态检索更具挑战性。为了解决这些限制，我们从生成角度提出了生成增强对齐（GEA）。 GEA 包含两个并行模块：(1) 文本引导令牌增强 (TGTE)，它引入扩散生成的图像作为中间语义表示，以弥合文本和视觉模式之间的差距。这些生成的图像丰富了文本的语义表示并促进跨模式对齐。 （2）生成中间融合（GIF），结合生成图像、原始图像和文本特征之间的交叉注意力，生成通过三元组对齐损失优化的统一表示。我们对三个公共 TIPR 数据集 CUHK-PEDES、RSTPReid 和 ICFG-PEDES 进行了广泛的实验，以评估 GEA 的性能。结果证明了我们方法的有效性。更多实现细节和扩展结果可在此 https URL 中找到。</li>
</ul>

<h3>Title: EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yuancheng Sun, Yuxuan Ren, Zhaoming Chen, Xu Han, Kang Liu, Qiwei Ye</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.10165">https://arxiv.org/abs/2511.10165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.10165">https://arxiv.org/pdf/2511.10165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.10165]] EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization(https://arxiv.org/abs/2511.10165)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Accurate exploration of protein conformational ensembles is essential for uncovering function but remains hard because molecular-dynamics (MD) simulations suffer from high computational costs and energy-barrier trapping. This paper presents Energy Preference Optimization (EPO), an online refinement algorithm that turns a pretrained protein ensemble generator into an energy-aware sampler without extra MD trajectories. Specifically, EPO leverages stochastic differential equation sampling to explore the conformational landscape and incorporates a novel energy-ranking mechanism based on list-wise preference optimization. Crucially, EPO introduces a practical upper bound to efficiently approximate the intractable probability of long sampling trajectories in continuous-time generative models, making it easily adaptable to existing pretrained generators. On Tetrapeptides, ATLAS, and Fast-Folding benchmarks, EPO successfully generates diverse and physically realistic ensembles, establishing a new state-of-the-art in nine evaluation metrics. These results demonstrate that energy-only preference signals can efficiently steer generative models toward thermodynamically consistent conformational ensembles, providing an alternative to long MD simulations and widening the applicability of learned potentials in structural biology and drug discovery.</li>
<li><strong>摘要：</strong>准确探索蛋白质构象整体对于揭示功能至关重要，但仍然很困难，因为分子动力学 (MD) 模拟面临高计算成本和能量势垒捕获。本文提出了能量偏好优化 (EPO)，这是一种在线细化算法，可将预训练的蛋白质集成生成器转变为能量感知采样器，而无需额外的 MD 轨迹。具体来说，EPO 利用随机微分方程采样来探索构象景观，并结合了一种基于列表偏好优化的新型能量排序机制。至关重要的是，EPO 引入了一个实用的上限，可以有效地近似连续时间生成模型中长采样轨迹的棘手概率，使其轻松适应现有的预训练生成器。在四肽、ATLAS 和快速折叠基准上，EPO 成功生成了多样化且物理真实的集合，在九个评估指标中建立了新的最先进水平。这些结果表明，仅能量偏好信号可以有效地引导生成模型走向热力学一致的构象整体，为长MD模拟提供了替代方案，并扩大了学习潜力在结构生物学和药物发现中的适用性。</li>
</ul>

<h3>Title: Physically Interpretable Multi-Degradation Image Restoration via Deep Unfolding and Explainable Convolution</h3>
<ul>
<li><strong>Authors: </strong>Hu Gao, Xiaoning Lei, Xichen Xu, Depeng Dang, Lizhuang Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.10166">https://arxiv.org/abs/2511.10166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.10166">https://arxiv.org/pdf/2511.10166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.10166]] Physically Interpretable Multi-Degradation Image Restoration via Deep Unfolding and Explainable Convolution(https://arxiv.org/abs/2511.10166)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Although image restoration has advanced significantly, most existing methods target only a single type of degradation. In real-world scenarios, images often contain multiple degradations simultaneously, such as rain, noise, and haze, requiring models capable of handling diverse degradation types. Moreover, methods that improve performance through module stacking often suffer from limited interpretability. In this paper, we propose a novel interpretability-driven approach for multi-degradation image restoration, built upon a deep unfolding network that maps the iterative process of a mathematical optimization algorithm into a learnable network structure. Specifically, we employ an improved second-order semi-smooth Newton algorithm to ensure that each module maintains clear physical interpretability. To further enhance interpretability and adaptability, we design an explainable convolution module inspired by the human brain's flexible information processing and the intrinsic characteristics of images, allowing the network to flexibly leverage learned knowledge and autonomously adjust parameters for different input. The resulting tightly integrated architecture, named InterIR, demonstrates excellent performance in multi-degradation restoration while remaining highly competitive on single-degradation tasks.</li>
<li><strong>摘要：</strong>尽管图像恢复取得了显着进步，但大多数现有方法仅针对单一类型的退化。在现实场景中，图像通常同时包含多种退化，例如雨、噪声和雾霾，需要能够处理多种退化类型的模型。此外，通过模块堆叠提高性能的方法通常会受到可解释性的限制。在本文中，我们提出了一种新颖的可解释性驱动的多退化图像恢复方法，该方法建立在深度展开网络的基础上，将数学优化算法的迭代过程映射到可学习的网络结构中。具体来说，我们采用改进的二阶半光滑牛顿算法来确保每个模块保持清晰的物理可解释性。为了进一步增强可解释性和适应性，我们受人脑灵活的信息处理和图像的内在特征的启发，设计了可解释的卷积模块，使网络能够灵活地利用学到的知识并针对不同的输入自主调整参数。由此产生的紧密集成的架构，名为 InterIR，在多重降解恢复中表现出出色的性能，同时在单次降解任务上保持高度竞争力。</li>
</ul>

<h3>Title: FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Yongji Zhang, Siqi Li, Yue Gao, Yu Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.10250">https://arxiv.org/abs/2511.10250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.10250">https://arxiv.org/pdf/2511.10250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.10250]] FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment(https://arxiv.org/abs/2511.10250)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Action Quality Assessment (AQA) aims to evaluate and score sports actions, which has attracted widespread interest in recent years. Existing AQA methods primarily predict scores based on features extracted from the entire video, resulting in limited interpretability and reliability. Meanwhile, existing AQA datasets also lack fine-grained annotations for action scores, especially for deduction items and sub-score annotations. In this paper, we construct the first AQA dataset containing fine-grained sub-score and deduction annotations for aerial skiing, which will be released as a new benchmark. For the technical challenges, we propose a novel AQA method, named JudgeMind, which significantly enhances performance and reliability by simulating the judgment and scoring mindset of professional referees. Our method segments the input action video into different stages and scores each stage to enhance accuracy. Then, we propose a stage-aware feature enhancement and fusion module to boost the perception of stage-specific key regions and enhance the robustness to visual changes caused by frequent camera viewpoints switching. In addition, we propose a knowledge-based grade-aware decoder to incorporate possible deduction items as prior knowledge to predict more accurate and reliable scores. Experimental results demonstrate that our method achieves state-of-the-art performance.</li>
<li><strong>摘要：</strong>动作质量评估（AQA）旨在对体育动作进行评估和评分，近年来引起了广泛的兴趣。现有的 AQA 方法主要根据从整个视频中提取的特征来预测分数，导致可解释性和可靠性有限。同时，现有的AQA数据集也缺乏对动作分数的细粒度标注，尤其是扣分项和分项标注。在本文中，我们构建了第一个包含空中滑雪细粒度分项评分和演绎注释的AQA数据集，该数据集将作为新的基准发布。针对技术挑战，我们提出了一种新颖的AQA方法，名为JudgeMind，该方法通过模拟专业裁判的判断和评分心态，显着提高了性能和可靠性。我们的方法将输入动作视频分割成不同的阶段并对每个阶段进行评分以提高准确性。然后，我们提出了一种舞台感知特征增强和融合模块，以增强对特定舞台关键区域的感知，并增强对频繁摄像机视点切换引起的视觉变化的鲁棒性。此外，我们提出了一种基于知识的成绩感知解码器，将可能的扣除项目作为先验知识纳入其中，以预测更准确、更可靠的分数。实验结果表明我们的方法实现了最先进的性能。</li>
</ul>

<h3>Title: Unitho: A Unified Multi-Task Framework for Computational Lithography</h3>
<ul>
<li><strong>Authors: </strong>Qian Jin, Yumeng Liu, Yuqi Jiang, Qi Sun, Cheng Zhuo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.10255">https://arxiv.org/abs/2511.10255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.10255">https://arxiv.org/pdf/2511.10255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.10255]] Unitho: A Unified Multi-Task Framework for Computational Lithography(https://arxiv.org/abs/2511.10255)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reliable, generalizable data foundations are critical for enabling large-scale models in computational lithography. However, essential tasks-mask generation, rule violation detection, and layout optimization-are often handled in isolation, hindered by scarce datasets and limited modeling approaches. To address these challenges, we introduce Unitho, a unified multi-task large vision model built upon the Transformer architecture. Trained on a large-scale industrial lithography simulation dataset with hundreds of thousands of cases, Unitho supports end-to-end mask generation, lithography simulation, and rule violation detection. By enabling agile and high-fidelity lithography simulation, Unitho further facilitates the construction of robust data foundations for intelligent EDA. Experimental results validate its effectiveness and generalizability, with performance substantially surpassing academic baselines.</li>
<li><strong>摘要：</strong>可靠、可推广的数据基础对于在计算光刻中实现大规模模型至关重要。然而，基本任务——掩模生成、规则违规检测和布局优化——通常是孤立处理的，受到稀缺数据集和有限建模方法的阻碍。为了应对这些挑战，我们引入了 Unitho，一个基于 Transformer 架构构建的统一多任务大视觉模型。 Unitho 经过数十万个案例的大规模工业光刻仿真数据集的训练，支持端到端掩模生成、光刻仿真和规则违规检测。通过实现敏捷、高保真光刻仿真，Unitho 进一步促进了智能 EDA 的强大数据基础的构建。实验结果验证了其有效性和普遍性，其性能大大超过了学术基线。</li>
</ul>

<h3>Title: Adaptive Residual-Update Steering for Low-Overhead Hallucination Mitigation in Large Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhengtao Zou, Ya Gao, Jiarui Guan, Bin Li, Pekka Marttinen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.10292">https://arxiv.org/abs/2511.10292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.10292">https://arxiv.org/pdf/2511.10292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.10292]] Adaptive Residual-Update Steering for Low-Overhead Hallucination Mitigation in Large Vision Language Models(https://arxiv.org/abs/2511.10292)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Vision-Language Models (LVLMs) often suffer from object hallucination, generating text inconsistent with visual inputs, which can critically undermine their reliability. Existing inference-time interventions to mitigate this issue present a challenging trade-off: while methods that steer internal states or adjust output logits can be effective, they often incur substantial computational overhead, typically requiring extra forward passes. This efficiency bottleneck can limit their practicality for real-world, latency-sensitive deployments. In this work, we aim to address this trade-off with Residual-Update Directed DEcoding Regulation (RUDDER), a low-overhead framework that steers LVLMs towards visually-grounded generation. RUDDER is built on two key innovations: (1) Contextual Activation Residual Direction (CARD) vector, a per-sample visual evidence vector extracted from the residual update of a self-attention layer during a single, standard forward pass. (2) A Bayesian-inspired adaptive gate that performs token-wise injection, applying a corrective signal whose strength is conditioned on the model's deviation from the visual context. Extensive experiments on key hallucination benchmarks, including POPE and CHAIR, indicate that RUDDER achieves performance comparable to state-of-the-art methods while introducing negligible computational latency, validating RUDDER as a pragmatic and effective approach for improving LVLMs' reliability without a significant compromise on efficiency.</li>
<li><strong>摘要：</strong>大视觉语言模型 (LVLM) 经常会出现物体幻觉，生成与视觉输入不一致的文本，这会严重损害其可靠性。现有的用于缓解此问题的推理时间干预措施提出了一个具有挑战性的权衡：虽然引导内部状态或调整输出逻辑的方法可能是有效的，但它们通常会产生大量的计算开销，通常需要额外的前向传递。这种效率瓶颈可能会限制它们在现实世界中对延迟敏感的部署的实用性。在这项工作中，我们的目标是通过残余更新定向解码调节（RUDDER）来解决这种权衡，这是一个低开销框架，可引导 LVLM 转向基于视觉的生成。 RUDDER 建立在两项关键创新之上：(1) 上下文激活残差方向 (CARD) 向量，这是从单个标准前向传递期间自注意力层的残差更新中提取的每个样本的视觉证据向量。 (2) 受贝叶斯启发的自适应门，执行 token-wise 注入，应用校正信号，其强度取决于模型与视觉上下文的偏差。对关键幻觉基准（包括 POPE 和 CHAIR）的广泛实验表明，RUDDER 的性能可与最先进的方法相媲美，同时引入的计算延迟可以忽略不计，验证了 RUDDER 是一种实用且有效的方法，可在不显着影响效率的情况下提高 LVLM 的可靠性。</li>
</ul>

<h3>Title: CLIP4VI-ReID: Learning Modality-shared Representations via CLIP Semantic Bridge for Visible-Infrared Person Re-identification</h3>
<ul>
<li><strong>Authors: </strong>Xiaomei Yang, Xizhan Gao, Sijie Niu, Fa Zhu, Guang Feng, Xiaofeng Qu, David Camacho</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.10309">https://arxiv.org/abs/2511.10309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.10309">https://arxiv.org/pdf/2511.10309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.10309]] CLIP4VI-ReID: Learning Modality-shared Representations via CLIP Semantic Bridge for Visible-Infrared Person Re-identification(https://arxiv.org/abs/2511.10309)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This paper proposes a novel CLIP-driven modality-shared representation learning network named CLIP4VI-ReID for VI-ReID task, which consists of Text Semantic Generation (TSG), Infrared Feature Embedding (IFE), and High-level Semantic Alignment (HSA). Specifically, considering the huge gap in the physical characteristics between natural images and infrared images, the TSG is designed to generate text semantics only for visible images, thereby enabling preliminary visible-text modality alignment. Then, the IFE is proposed to rectify the feature embeddings of infrared images using the generated text semantics. This process injects id-related semantics into the shared image encoder, enhancing its adaptability to the infrared modality. Besides, with text serving as a bridge, it enables indirect visible-infrared modality alignment. Finally, the HSA is established to refine the high-level semantic alignment. This process ensures that the fine-tuned text semantics only contain id-related information, thereby achieving more accurate cross-modal alignment and enhancing the discriminability of the learned modal-shared representations. Extensive experimental results demonstrate that the proposed CLIP4VI-ReID achieves superior performance than other state-of-the-art methods on some widely used VI-ReID datasets.</li>
<li><strong>摘要：</strong>本文针对 VI-ReID 任务提出了一种新颖的 CLIP 驱动模态共享表示学习网络，名为 CLIP4VI-ReID，该网络由文本语义生成（TSG）、红外特征嵌入（IFE）和高级语义对齐（HSA）组成。具体来说，考虑到自然图像和红外图像之间物理特性的巨大差距，TSG被设计为仅针对可见图像生成文本语义，从而实现初步的可见文本模态对齐。然后，提出 IFE 使用生成的文本语义来纠正红外图像的特征嵌入。该过程将与身份相关的语义注入共享图像编码器，增强其对红外模态的适应性。此外，通过文本作为桥梁，它可以实现间接可见红外模态对齐。最后，建立 HSA 来细化高级语义对齐。这个过程确保微调后的文本语义仅包含与id相关的信息，从而实现更准确的跨模态对齐并增强所学习的模态共享表示的可辨别性。大量的实验结果表明，所提出的 CLIP4VI-ReID 在一些广泛使用的 VI-ReID 数据集上比其他最先进的方法具有更优越的性能。</li>
</ul>

<h3>Title: Fragile by Design: On the Limits of Adversarial Defenses in Personalized Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhen Chen, Yi Zhang, Xiangyu Yin, Chengxuan Qin, Xingyu Zhao, Xiaowei Huang, Wenjie Ruan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.10382">https://arxiv.org/abs/2511.10382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.10382">https://arxiv.org/pdf/2511.10382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.10382]] Fragile by Design: On the Limits of Adversarial Defenses in Personalized Generation(https://arxiv.org/abs/2511.10382)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Personalized AI applications such as DreamBooth enable the generation of customized content from user images, but also raise significant privacy concerns, particularly the risk of facial identity leakage. Recent defense mechanisms like Anti-DreamBooth attempt to mitigate this risk by injecting adversarial perturbations into user photos to prevent successful personalization. However, we identify two critical yet overlooked limitations of these methods. First, the adversarial examples often exhibit perceptible artifacts such as conspicuous patterns or stripes, making them easily detectable as manipulated content. Second, the perturbations are highly fragile, as even a simple, non-learned filter can effectively remove them, thereby restoring the model's ability to memorize and reproduce user identity. To investigate this vulnerability, we propose a novel evaluation framework, AntiDB_Purify, to systematically evaluate existing defenses under realistic purification threats, including both traditional image filters and adversarial purification. Results reveal that none of the current methods maintains their protective effectiveness under such threats. These findings highlight that current defenses offer a false sense of security and underscore the urgent need for more imperceptible and robust protections to safeguard user identity in personalized generation.</li>
<li><strong>摘要：</strong>DreamBooth 等个性化人工智能应用程序可以根据用户图像生成定制内容，但也引发了严重的隐私问题，特别是面部身份泄露的风险。最近的防御机制（例如 Anti-DreamBooth）试图通过向用户照片中注入对抗性扰动以防止成功的个性化来减轻这种风险。然而，我们发现这些方法有两个关键但被忽视的局限性。首先，对抗性示例通常表现出可感知的伪影，例如明显的图案或条纹，使它们很容易被检测为被操纵的内容。其次，扰动非常脆弱，因为即使是简单的、非学习的过滤器也可以有效地消除它们，从而恢复模型记忆和重现用户身份的能力。为了研究这个漏洞，我们提出了一种新颖的评估框架AntiDB_Purify，以系统地评估现实净化威胁下的现有防御，包括传统的图像过滤器和对抗性净化。结果表明，当前的方法在此类威胁下均无法保持其保护效果。这些发现强调，当前的防御措施提供了一种错误的安全感，并强调迫切需要更难以察觉和更强大的保护措施来保护个性化生成中的用户身份。</li>
</ul>

<h3>Title: MonkeyOCR v1.5 Technical Report: Unlocking Robust Document Parsing for Complex Patterns</h3>
<ul>
<li><strong>Authors: </strong>Jiarui Zhang, Yuliang Liu, Zijun Wu, Guosheng Pang, Zhili Ye, Yupei Zhong, Junteng Ma, Tao Wei, Haiyang Xu, Weikai Chen, Zeen Wang, Qiangjun Ji, Fanxi Zhou, Qi Zhang, Yuanrui Hu, Jiahao Liu, Zhang Li, Ziyang Zhang, Qiang Liu, Xiang Bai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.10390">https://arxiv.org/abs/2511.10390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.10390">https://arxiv.org/pdf/2511.10390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.10390]] MonkeyOCR v1.5 Technical Report: Unlocking Robust Document Parsing for Complex Patterns(https://arxiv.org/abs/2511.10390)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Document parsing is a core task in document intelligence, supporting applications such as information extraction, retrieval-augmented generation, and automated document analysis. However, real-world documents often feature complex layouts with multi-level tables, embedded images or formulas, and cross-page structures, which remain challenging for existing OCR systems. We introduce MonkeyOCR v1.5, a unified vision-language framework that enhances both layout understanding and content recognition through a two-stage parsing pipeline. The first stage employs a large multimodal model to jointly predict document layout and reading order, leveraging visual information to ensure structural and sequential consistency. The second stage performs localized recognition of text, formulas, and tables within detected regions, maintaining high visual fidelity while reducing error propagation. To address complex table structures, we propose a visual consistency-based reinforcement learning scheme that evaluates recognition quality via render-and-compare alignment, improving structural accuracy without manual annotations. Additionally, two specialized modules, Image-Decoupled Table Parsing and Type-Guided Table Merging, are introduced to enable reliable parsing of tables containing embedded images and reconstruction of tables crossing pages or columns. Comprehensive experiments on OmniDocBench v1.5 demonstrate that MonkeyOCR v1.5 achieves state-of-the-art performance, outperforming PPOCR-VL and MinerU 2.5 while showing exceptional robustness in visually complex document scenarios.</li>
<li><strong>摘要：</strong>文档解析是文档智能的核心任务，支持信息提取、检索增强生成和自动文档分析等应用。然而，现实世界的文档通常具有复杂的布局，包括多级表格、嵌入图像或公式以及跨页面结构，这对现有 OCR 系统仍然是一个挑战。我们推出 MonkeyOCR v1.5，这是一个统一的视觉语言框架，可通过两阶段解析管道增强布局理解和内容识别。第一阶段采用大型多模态模型来联合预测文档布局和阅读顺序，利用视觉信息来确保结构和顺序的一致性。第二阶段对检测区域内的文本、公式和表格进行本地化识别，保持高视觉保真度，同时减少错误传播。为了解决复杂的表格结构，我们提出了一种基于视觉一致性的强化学习方案，该方案通过渲染和比较对齐来评估识别质量，从而提高结构准确性，而无需手动注释。此外，还引入了两个专用模块：图像解耦表解析和类型引导表合并，以实现包含嵌入图像的表的可靠解析以及跨页或跨列的表的重建。 OmniDocBench v1.5 上的综合实验表明，MonkeyOCR v1.5 实现了最先进的性能，优于 PPOCR-VL 和 MinerU 2.5，同时在视觉复杂的文档场景中表现出卓越的鲁棒性。</li>
</ul>

<h3>Title: GrounDiff: Diffusion-Based Ground Surface Generation from Digital Surface Models</h3>
<ul>
<li><strong>Authors: </strong>Oussema Dhaouadi, Johannes Meier, Jacques Kaiser, Daniel Cremers</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.10391">https://arxiv.org/abs/2511.10391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.10391">https://arxiv.org/pdf/2511.10391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.10391]] GrounDiff: Diffusion-Based Ground Surface Generation from Digital Surface Models(https://arxiv.org/abs/2511.10391)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Digital Terrain Models (DTMs) represent the bare-earth elevation and are important in numerous geospatial applications. Such data models cannot be directly measured by sensors and are typically generated from Digital Surface Models (DSMs) derived from LiDAR or photogrammetry. Traditional filtering approaches rely on manually tuned parameters, while learning-based methods require well-designed architectures, often combined with post-processing. To address these challenges, we introduce Ground Diffusion (GrounDiff), the first diffusion-based framework that iteratively removes non-ground structures by formulating the problem as a denoising task. We incorporate a gated design with confidence-guided generation that enables selective filtering. To increase scalability, we further propose Prior-Guided Stitching (PrioStitch), which employs a downsampled global prior automatically generated using GrounDiff to guide local high-resolution predictions. We evaluate our method on the DSM-to-DTM translation task across diverse datasets, showing that GrounDiff consistently outperforms deep learning-based state-of-the-art methods, reducing RMSE by up to 93% on ALS2DTM and up to 47% on USGS benchmarks. In the task of road reconstruction, which requires both high precision and smoothness, our method achieves up to 81% lower distance error compared to specialized techniques on the GeRoD benchmark, while maintaining competitive surface smoothness using only DSM inputs, without task-specific optimization. Our variant for road reconstruction, GrounDiff+, is specifically designed to produce even smoother surfaces, further surpassing state-of-the-art methods. The project page is available at this https URL.</li>
<li><strong>摘要：</strong>数字地形模型 (DTM) 代表裸地高程，在众多地理空间应用中非常重要。此类数据模型无法通过传感器直接测量，通常由激光雷达或摄影测量得出的数字表面模型 (DSM) 生成。传统的过滤方法依赖于手动调整参数，而基于学习的方法需要精心设计的架构，通常与后处理相结合。为了应对这些挑战，我们引入了地面扩散（GrounDiff），这是第一个基于扩散的框架，通过将问题表述为去噪任务来迭代地去除非地面结构。我们将门控设计与置信引导生成相结合，从而实现选择性过滤。为了提高可扩展性，我们进一步提出了先验引导缝合（PrioStitch），它采用使用 GrounDiff 自动生成的下采样全局先验来指导局部高分辨率预测。我们在不同数据集的 DSM 到 DTM 翻译任务上评估了我们的方法，结果表明，GroundDiff 始终优于基于深度学习的最先进方法，在 ALS2DTM 上将 RMSE 降低高达 93%，在 USGS 基准上降低高达 47%。在需要高精度和平滑度的道路重建任务中，与 GeRoD 基准上的专业技术相比，我们的方法可将距离误差降低高达 81%，同时仅使用 DSM 输入保持有竞争力的表面平滑度，无需针对特定任务进行优化。我们的道路重建变体 GrounDiff+ 专门设计用于产生更光滑的表面，进一步超越了最先进的方法。项目页面可通过此 https URL 获取。</li>
</ul>

<h3>Title: AgentEvolver: Towards Efficient Self-Evolving Agent System</h3>
<ul>
<li><strong>Authors: </strong>Yunpeng Zhai, Shuchang Tao, Cheng Chen, Anni Zou, Ziqian Chen, Qingxu Fu, Shinji Mai, Li Yu, Jiaji Deng, Zouying Cao, Zhaoyang Liu, Bolin Ding, Jingren Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.10395">https://arxiv.org/abs/2511.10395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.10395">https://arxiv.org/pdf/2511.10395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.10395]] AgentEvolver: Towards Efficient Self-Evolving Agent System(https://arxiv.org/abs/2511.10395)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Autonomous agents powered by large language models (LLMs) have the potential to significantly enhance human productivity by reasoning, using tools, and executing complex tasks in diverse environments. However, current approaches to developing such agents remain costly and inefficient, as they typically require manually constructed task datasets and reinforcement learning (RL) pipelines with extensive random exploration. These limitations lead to prohibitively high data-construction costs, low exploration efficiency, and poor sample utilization. To address these challenges, we present AgentEvolver, a self-evolving agent system that leverages the semantic understanding and reasoning capabilities of LLMs to drive autonomous agent learning. AgentEvolver introduces three synergistic mechanisms: (i) self-questioning, which enables curiosity-driven task generation in novel environments, reducing dependence on handcrafted datasets; (ii) self-navigating, which improves exploration efficiency through experience reuse and hybrid policy guidance; and (iii) self-attributing, which enhances sample efficiency by assigning differentiated rewards to trajectory states and actions based on their contribution. By integrating these mechanisms into a unified framework, AgentEvolver enables scalable, cost-effective, and continual improvement of agent capabilities. Preliminary experiments indicate that AgentEvolver achieves more efficient exploration, better sample utilization, and faster adaptation compared to traditional RL-based baselines.</li>
<li><strong>摘要：</strong>由大型语言模型 (LLM) 提供支持的自主代理有可能通过在不同环境中推理、使用工具和执行复杂任务来显着提高人类生产力。然而，目前开发此类代理的方法仍然成本高昂且效率低下，因为它们通常需要手动构建的任务数据集和具有广泛随机探索的强化学习（RL）管道。这些限制导致数据构建成本过高、探索效率低下和样本利用率低下。为了应对这些挑战，我们推出了 AgentEvolver，这是一个自我进化的代理系统，它利用法学硕士的语义理解和推理能力来驱动自主代理学习。 AgentEvolver 引入了三种协同机制：（i）自我质疑，可以在新颖的环境中生成好奇心驱动的任务，减少对手工数据集的依赖； (ii) 自导航，通过经验重用和混合政策指导提高勘探效率； (iii)自我归因，通过根据轨迹状态和行为的贡献向其分配差异化奖励来提高样本效率。通过将这些机制集成到统一框架中，AgentEvolver 能够实现可扩展、经济高效且持续改进代理功能。初步实验表明，与传统的基于强化学习的基线相比，AgentEvolver 实现了更高效的探索、更好的样本利用率和更快的适应。</li>
</ul>

<h3>Title: OpenSR-SRGAN: A Flexible Super-Resolution Framework for Multispectral Earth Observation Data</h3>
<ul>
<li><strong>Authors: </strong>Simon Donike, Cesar Aybar, Julio Contreras, Luis Gómez-Chova</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.10461">https://arxiv.org/abs/2511.10461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.10461">https://arxiv.org/pdf/2511.10461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.10461]] OpenSR-SRGAN: A Flexible Super-Resolution Framework for Multispectral Earth Observation Data(https://arxiv.org/abs/2511.10461)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>We present OpenSR-SRGAN, an open and modular framework for single-image super-resolution in Earth Observation. The software provides a unified implementation of SRGAN-style models that is easy to configure, extend, and apply to multispectral satellite data such as Sentinel-2. Instead of requiring users to modify model code, OpenSR-SRGAN exposes generators, discriminators, loss functions, and training schedules through concise configuration files, making it straightforward to switch between architectures, scale factors, and band setups. The framework is designed as a practical tool and benchmark implementation rather than a state-of-the-art model. It ships with ready-to-use configurations for common remote sensing scenarios, sensible default settings for adversarial training, and built-in hooks for logging, validation, and large-scene inference. By turning GAN-based super-resolution into a configuration-driven workflow, OpenSR-SRGAN lowers the entry barrier for researchers and practitioners who wish to experiment with SRGANs, compare models in a reproducible way, and deploy super-resolution pipelines across diverse Earth-observation datasets.</li>
<li><strong>摘要：</strong>我们提出了 OpenSR-SRGAN，这是一种用于地球观测中单图像超分辨率的开放式模块化框架。该软件提供了 SRGAN 风格模型的统一实现，易于配置、扩展和应用于 Sentinel-2 等多光谱卫星数据。 OpenSR-SRGAN 不需要用户修改模型代码，而是通过简洁的配置文件公开生成器、鉴别器、损失函数和训练计划，从而可以在架构、比例因子和频段设置之间直接切换。该框架被设计为实用工具和基准实现，而不是最先进的模型。它配备了适用于常见遥感场景的即用型配置、适用于对抗训练的合理默认设置以及用于日志记录、验证和大场景推理的内置挂钩。通过将基于 GAN 的超分辨率转变为配置驱动的工作流程，OpenSR-SRGAN 降低了希望试验 SRGAN、以可重复的方式比较模型以及跨不同地球观测数据集部署超分辨率管道的研究人员和从业者的准入门槛。</li>
</ul>

<h3>Title: Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Isabela Albuquerque, Ira Ktena, Olivia Wiles, Ivana Kajić, Amal Rannen-Triki, Cristina Vasconcelos, Aida Nematzadeh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.10547">https://arxiv.org/abs/2511.10547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.10547">https://arxiv.org/pdf/2511.10547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.10547]] Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation(https://arxiv.org/abs/2511.10547)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite advances in generation quality, current text-to-image (T2I) models often lack diversity, generating homogeneous outputs. This work introduces a framework to address the need for robust diversity evaluation in T2I models. Our framework systematically assesses diversity by evaluating individual concepts and their relevant factors of variation. Key contributions include: (1) a novel human evaluation template for nuanced diversity assessment; (2) a curated prompt set covering diverse concepts with their identified factors of variation (e.g. prompt: An image of an apple, factor of variation: color); and (3) a methodology for comparing models in terms of human annotations via binomial tests. Furthermore, we rigorously compare various image embeddings for diversity measurement. Notably, our principled approach enables ranking of T2I models by diversity, identifying categories where they particularly struggle. This research offers a robust methodology and insights, paving the way for improvements in T2I model diversity and metric development.</li>
<li><strong>摘要：</strong>尽管生成质量取得了进步，但当前的文本到图像（T2I）模型通常缺乏多样性，生成的输出是同质的。这项工作引入了一个框架来满足 T2I 模型中稳健多样性评估的需求。我们的框架通过评估个体概念及其相关的变异因素来系统地评估多样性。主要贡献包括：（1）用于细致入微的多样性评估的新颖的人类评估模板； (2) 精心策划的提示集，涵盖不同的概念及其已识别的变化因素（例如提示：苹果的图像，变化因素：颜色）； (3) 通过二项式检验比较人类注释模型的方法。此外，我们严格比较各种图像嵌入以进行多样性测量。值得注意的是，我们的原则性方法可以根据多样性对 T2I 模型进行排名，确定它们特别困难的类别。这项研究提供了强大的方法论和见解，为 T2I 模型多样性和指标开发的改进铺平了道路。</li>
</ul>

<h3>Title: A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space</h3>
<ul>
<li><strong>Authors: </strong>Huijie Liu, Shuhao Cui, Haoxiang Cao, Shuai Ma, Kai Wu, Guoliang Kang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.10555">https://arxiv.org/abs/2511.10555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.10555">https://arxiv.org/pdf/2511.10555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.10555]] A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space(https://arxiv.org/abs/2511.10555)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Innovative visual stylization is a cornerstone of artistic creation, yet generating novel and consistent visual styles remains a significant challenge. Existing generative approaches typically rely on lengthy textual prompts, reference images, or parameter-efficient fine-tuning to guide style-aware image generation, but often struggle with style consistency, limited creativity, and complex style representations. In this paper, we affirm that a style is worth one numerical code by introducing the novel task, code-to-style image generation, which produces images with novel, consistent visual styles conditioned solely on a numerical style code. To date, this field has only been primarily explored by the industry (e.g., Midjourney), with no open-source research from the academic community. To fill this gap, we propose CoTyle, the first open-source method for this task. Specifically, we first train a discrete style codebook from a collection of images to extract style embeddings. These embeddings serve as conditions for a text-to-image diffusion model (T2I-DM) to generate stylistic images. Subsequently, we train an autoregressive style generator on the discrete style embeddings to model their distribution, allowing the synthesis of novel style embeddings. During inference, a numerical style code is mapped to a unique style embedding by the style generator, and this embedding guides the T2I-DM to generate images in the corresponding style. Unlike existing methods, our method offers unparalleled simplicity and diversity, unlocking a vast space of reproducible styles from minimal input. Extensive experiments validate that CoTyle effectively turns a numerical code into a style controller, demonstrating a style is worth one code.</li>
<li><strong>摘要：</strong>创新的视觉风格是艺术创作的基石，但生成新颖且一致的视觉风格仍然是一个重大挑战。现有的生成方法通常依赖于冗长的文本提示、参考图像或参数高效的微调来指导风格感知图像的生成，但往往会遇到风格一致性、有限的创造力和复杂的风格表示等问题。在本文中，我们通过引入新颖的任务“代码到风格图像生成”来确认一种风格值得一个数字代码，该任务仅根据数字风格代码生成具有新颖、一致的视觉风格的图像。迄今为止，该领域主要仅由业界探索（例如 Midjourney），学术界还没有开源研究。为了填补这一空白，我们提出了 CoTyle，这是该任务的第一个开源方法。具体来说，我们首先从图像集合中训练离散风格码本以提取风格嵌入。这些嵌入充当文本到图像扩散模型（T2I-DM）生成风格图像的条件。随后，我们在离散样式嵌入上训练自回归样式生成器以对其分布进行建模，从而允许合成新颖的样式嵌入。在推理过程中，样式生成器将数字样式代码映射到唯一的样式嵌入，该嵌入指导 T2I-DM 生成相应样式的图像。与现有方法不同，我们的方法提供了无与伦比的简单性和多样性，以最少的输入解锁了广阔的可复制样式空间。大量实验验证了 CoTyle 有效地将数字代码转换为样式控制器，证明一种样式值得一个代码。</li>
</ul>

<h3>Title: One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Aleksandr Razin, Danil Kazantsev, Ilya Makarov</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.10629">https://arxiv.org/abs/2511.10629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.10629">https://arxiv.org/pdf/2511.10629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.10629]] One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models(https://arxiv.org/abs/2511.10629)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generation</a></li>
<li><strong>Abstract: </strong>Diffusion models struggle to scale beyond their training resolutions, as direct high-resolution sampling is slow and costly, while post-hoc image super-resolution (ISR) introduces artifacts and additional latency by operating after decoding. We present the Latent Upscaler Adapter (LUA), a lightweight module that performs super-resolution directly on the generator's latent code before the final VAE decoding step. LUA integrates as a drop-in component, requiring no modifications to the base model or additional diffusion stages, and enables high-resolution synthesis through a single feed-forward pass in latent space. A shared Swin-style backbone with scale-specific pixel-shuffle heads supports 2x and 4x factors and remains compatible with image-space SR baselines, achieving comparable perceptual quality with nearly 3x lower decoding and upscaling time (adding only +0.42 s for 1024 px generation from 512 px, compared to 1.87 s for pixel-space SR using the same SwinIR architecture). Furthermore, LUA shows strong generalization across the latent spaces of different VAEs, making it easy to deploy without retraining from scratch for each new decoder. Extensive experiments demonstrate that LUA closely matches the fidelity of native high-resolution generation while offering a practical and efficient path to scalable, high-fidelity image synthesis in modern diffusion pipelines.</li>
<li><strong>摘要：</strong>扩散模型很难扩展到超出其训练分辨率，因为直接高分辨率采样速度慢且成本高，而事后图像超分辨率 (ISR) 通过解码后的操作引入了伪影和额外的延迟。我们提出了 Latent Upscaler Adapter (LUA)，这是一个轻量级模块，可在最终 VAE 解码步骤之前直接对生成器的潜在代码执行超分辨率。 LUA 作为嵌入式组件进行集成，无需修改基础模型或额外的扩散阶段，并通过潜在空间中的单个前馈通道实现高分辨率合成。具有特定比例像素洗牌头的共享 Swin 式主干支持 2 倍和 4 倍因子，并与图像空间 SR 基线保持兼容，以近 3 倍的解码和升级时间实现可比较的感知质量（从 512 像素生成 1024 像素仅增加 +0.42 秒，而使用相同 SwinIR 架构的像素空间 SR 为 1.87 秒）。此外，LUA 在不同 VAE 的潜在空间中表现出很强的泛化能力，使其易于部署，而无需为每个新解码器从头开始重新训练。大量实验表明，LUA 与原生高分辨率生成的保真度紧密匹配，同时为现代扩散管道中的可扩展、高保真图像合成提供了实用且有效的途径。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
