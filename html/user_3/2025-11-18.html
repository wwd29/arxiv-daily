<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-11-18</h1>
<h3>Title: Parameter-Efficient and Personalized Federated Training of Generative Models at the Edge</h3>
<ul>
<li><strong>Authors: </strong>Kabir Khan, Manju Sarkar, Anita Kar, Suresh Ghosh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.11585">https://arxiv.org/abs/2511.11585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.11585">https://arxiv.org/pdf/2511.11585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.11585]] Parameter-Efficient and Personalized Federated Training of Generative Models at the Edge(https://arxiv.org/abs/2511.11585)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Large generative models (for example, language and diffusion models) enable high-quality text and image synthesis but are hard to train or adapt in cross-device federated settings due to heavy computation and communication and statistical/system heterogeneity. We propose FedGen-Edge, a framework that decouples a frozen, pre-trained global backbone from lightweight client-side adapters and federates only the adapters. Using Low-Rank Adaptation (LoRA) constrains client updates to a compact subspace, which reduces uplink traffic by more than 99 percent versus full-model FedAvg, stabilizes aggregation under non-IID data, and naturally supports personalization because each client can keep a locally tuned adapter. On language modeling (PTB) and image generation (CIFAR-10), FedGen-Edge achieves lower perplexity/FID and faster convergence than strong baselines while retaining a simple FedAvg-style server. A brief ablation shows diminishing returns beyond moderate LoRA rank and a trade-off between local epochs and client drift. FedGen-Edge offers a practical path toward privacy-preserving, resource-aware, and personalized generative AI on heterogeneous edge devices.</li>
<li><strong>摘要：</strong>大型生成模型（例如语言和扩散模型）可以实现高质量的文本和图像合成，但由于计算和通信以及统计/系统异构性，很难在跨设备联合设置中进行训练或适应。我们提出了 FedGen-Edge，这是一个框架，它将冻结的、预先训练的全局主干与轻量级客户端适配器解耦，并且仅联合适配器。使用低秩适应 (LoRA) 将客户端更新限制在紧凑的子空间中，与全模型 FedAvg 相比，上行链路流量减少了 99% 以上，稳定了非 IID 数据下的聚合，并且自然支持个性化，因为每个客户端都可以保留本地调整的适配器。在语言建模 (PTB) 和图像生成 (CIFAR-10) 方面，FedGen-Edge 实现了比强基线更低的困惑度/FID 和更快的收敛速度，同时保留了简单的 FedAvg 式服务器。简短的消融显示，超出中等 LoRA 排名的回报递减，以及本地时代和客户端漂移之间的权衡。 FedGen-Edge 提供了一条在异构边缘设备上实现隐私保护、资源感知和个性化生成 AI 的实用途径。</li>
</ul>

<h3>Title: Regularized Schrödinger: Alleviating Distortion and Exposure Bias in Solving Inverse Problems</h3>
<ul>
<li><strong>Authors: </strong>Qing Yao, Lijian Gao, Qirong Mao, Dong Ming</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.11686">https://arxiv.org/abs/2511.11686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.11686">https://arxiv.org/pdf/2511.11686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.11686]] Regularized Schrödinger: Alleviating Distortion and Exposure Bias in Solving Inverse Problems(https://arxiv.org/abs/2511.11686)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion models serve as a powerful generative framework for solving inverse problems. However, they still face two key challenges: 1) the distortion-perception tradeoff, where improving perceptual quality often degrades reconstruction fidelity, and 2) the exposure bias problem, where the training-inference input mismatch leads to prediction error accumulation and reduced reconstruction quality. In this work, we propose the Regularized Schrödinger Bridge (RSB), an adaptation of Schrödinger Bridge tailored for inverse problems that addresses the above limitations. RSB employs a novel regularized training strategy that perturbs both the input states and targets, effectively mitigating exposure bias by exposing the model to simulated prediction errors and also alleviating distortion by well-designed interpolation via the posterior mean. Extensive experiments on two typical inverse problems for speech enhancement demonstrate that RSB outperforms state-of-the-art methods, significantly improving distortion metrics and effectively reducing exposure bias.</li>
<li><strong>摘要：</strong>扩散模型是解决逆问题的强大生成框架。然而，他们仍然面临两个关键挑战：1）失真感知权衡，提高感知质量通常会降低重建保真度；2）暴露偏差问题，训练推理输入不匹配导致预测误差累积并降低重建质量。在这项工作中，我们提出了正则化薛定谔桥（RSB），这是针对反问题量身定制的薛定谔桥的改编版本，可解决上述限制。 RSB 采用了一种新颖的正则化训练策略，该策略会扰乱输入状态和目标，通过将模型暴露于模拟预测误差来有效减轻暴露偏差，并通过后验均值精心设计的插值来减轻失真。对语音增强的两个典型逆问题进行的大量实验表明，RSB 优于最先进的方法，显着改善了失真指标并有效减少了曝光偏差。</li>
</ul>

<h3>Title: Hierarchical Schedule Optimization for Fast and Robust Diffusion Model Sampling</h3>
<ul>
<li><strong>Authors: </strong>Aihua Zhu, Rui Su, Qinglin Zhao, Li Feng, Meng Shen, Shibo He</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.11688">https://arxiv.org/abs/2511.11688</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.11688">https://arxiv.org/pdf/2511.11688</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.11688]] Hierarchical Schedule Optimization for Fast and Robust Diffusion Model Sampling(https://arxiv.org/abs/2511.11688)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion probabilistic models have set a new standard for generative fidelity but are hindered by a slow iterative sampling process. A powerful training-free strategy to accelerate this process is Schedule Optimization, which aims to find an optimal distribution of timesteps for a fixed and small Number of Function Evaluations (NFE) to maximize sample quality. To this end, a successful schedule optimization method must adhere to four core principles: effectiveness, adaptivity, practical robustness, and computational efficiency. However, existing paradigms struggle to satisfy these principles simultaneously, motivating the need for a more advanced solution. To overcome these limitations, we propose the Hierarchical-Schedule-Optimizer (HSO), a novel and efficient bi-level optimization framework. HSO reframes the search for a globally optimal schedule into a more tractable problem by iteratively alternating between two synergistic levels: an upper-level global search for an optimal initialization strategy and a lower-level local optimization for schedule refinement. This process is guided by two key innovations: the Midpoint Error Proxy (MEP), a solver-agnostic and numerically stable objective for effective local optimization, and the Spacing-Penalized Fitness (SPF) function, which ensures practical robustness by penalizing pathologically close timesteps. Extensive experiments show that HSO sets a new state-of-the-art for training-free sampling in the extremely low-NFE regime. For instance, with an NFE of just 5, HSO achieves a remarkable FID of 11.94 on LAION-Aesthetics with Stable Diffusion v2.1. Crucially, this level of performance is attained not through costly retraining, but with a one-time optimization cost of less than 8 seconds, presenting a highly practical and efficient paradigm for diffusion model acceleration.</li>
<li><strong>摘要：</strong>扩散概率模型为生成保真度设定了新标准，但受到缓慢的迭代采样过程的阻碍。加速这一过程的一个强大的免训练策略是进度优化，其目的是为固定且少量的函数评估 (NFE) 找到时间步长的最佳分布，以最大限度地提高样本质量。为此，成功的调度优化方法必须遵循四个核心原则：有效性、适应性、实用鲁棒性和计算效率。然而，现有的范式很难同时满足这些原则，这就激发了对更先进解决方案的需求。为了克服这些限制，我们提出了分层调度优化器（HSO），这是一种新颖且高效的双层优化框架。 HSO 通过在两个协同级别之间迭代交替，将全局最优调度的搜索重新构建为更容易处理的问题：上层全局搜索最优初始化策略和下层局部优化用于调度细化。该过程以两项关键创新为指导：中点误差代理（MEP），一个与解算器无关且数值稳定的目标，用于有效的局部优化；以及间距惩罚适应度（SPF）函数，它通过惩罚病态的接近时间步长来确保实际的鲁棒性。大量实验表明，HSO 在极低 NFE 的情况下为免训练采样设定了新的最先进技术。例如，HSO 在 NFE 仅 5 的情况下，在具有稳定扩散 v2.1 的 LAION-Aesthetics 上实现了 11.94 的显着 FID。至关重要的是，这种性能水平并不是通过昂贵的再训练来实现的，而是一次性优化成本不到 8 秒，为扩散模型加速提供了一种高度实用且高效的范式。</li>
</ul>

<h3>Title: AnchorDS: Anchoring Dynamic Sources for Semantically Consistent Text-to-3D Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiayin Zhu, Linlin Yang, Yicong Li, Angela Yao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.11692">https://arxiv.org/abs/2511.11692</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.11692">https://arxiv.org/pdf/2511.11692</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.11692]] AnchorDS: Anchoring Dynamic Sources for Semantically Consistent Text-to-3D Generation(https://arxiv.org/abs/2511.11692)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Optimization-based text-to-3D methods distill guidance from 2D generative models via Score Distillation Sampling (SDS), but implicitly treat this guidance as static. This work shows that ignoring source dynamics yields inconsistent trajectories that suppress or merge semantic cues, leading to "semantic over-smoothing" artifacts. As such, we reformulate text-to-3D optimization as mapping a dynamically evolving source distribution to a fixed target distribution. We cast the problem into a dual-conditioned latent space, conditioned on both the text prompt and the intermediately rendered image. Given this joint setup, we observe that the image condition naturally anchors the current source distribution. Building on this insight, we introduce AnchorDS, an improved score distillation mechanism that provides state-anchored guidance with image conditions and stabilizes generation. We further penalize erroneous source estimates and design a lightweight filter strategy and fine-tuning strategy that refines the anchor with negligible overhead. AnchorDS produces finer-grained detail, more natural colours, and stronger semantic consistency, particularly for complex prompts, while maintaining efficiency. Extensive experiments show that our method surpasses previous methods in both quality and efficiency.</li>
<li><strong>摘要：</strong>基于优化的文本转 3D 方法通过分数蒸馏采样 (SDS) 从 2D 生成模型中提取指导，但隐式将此指导视为静态的。这项工作表明，忽略源动态会产生不一致的轨迹，从而抑制或合并语义线索，导致“语义过度平滑”伪影。因此，我们将文本到 3D 的优化重新表述为将动态演变的源分布映射到固定的目标分布。我们将问题转化为双条件潜在空间，以文本提示和中间渲染图像为条件。考虑到这种联合设置，我们观察到图像条件自然地锚定了当前的源分布。基于这一见解，我们引入了 AnchorDS，这是一种改进的分数蒸馏机制，可提供图像条件的状态锚定指导并稳定生成。我们进一步惩罚错误的源估计，并设计一种轻量级过滤策略和微调策略，以可忽略不计的开销来细化锚点。 AnchorDS 可生成更细粒度的细节、更自然的色彩和更强的语义一致性，特别是对于复杂的提示，同时保持效率。大量的实验表明，我们的方法在质量和效率上都超过了以前的方法。</li>
</ul>

<h3>Title: Toward Dignity-Aware AI: Next-Generation Elderly Monitoring from Fall Detection to ADL</h3>
<ul>
<li><strong>Authors: </strong>Xun Shao, Aoba Otani, Yuto Hirasuka, Runji Cai, Seng W. Loke</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.11696">https://arxiv.org/abs/2511.11696</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.11696">https://arxiv.org/pdf/2511.11696</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.11696]] Toward Dignity-Aware AI: Next-Generation Elderly Monitoring from Fall Detection to ADL(https://arxiv.org/abs/2511.11696)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This position paper envisions a next-generation elderly monitoring system that moves beyond fall detection toward the broader goal of Activities of Daily Living (ADL) recognition. Our ultimate aim is to design privacy-preserving, edge-deployed, and federated AI systems that can robustly detect and understand daily routines, supporting independence and dignity in aging societies. At present, ADL-specific datasets are still under collection. As a preliminary step, we demonstrate feasibility through experiments using the SISFall dataset and its GAN-augmented variants, treating fall detection as a proxy task. We report initial results on federated learning with non-IID conditions, and embedded deployment on Jetson Orin Nano devices. We then outline open challenges such as domain shift, data scarcity, and privacy risks, and propose directions toward full ADL monitoring in smart-room environments. This work highlights the transition from single-task detection to comprehensive daily activity recognition, providing both early evidence and a roadmap for sustainable and human-centered elderly care AI.</li>
<li><strong>摘要：</strong>本立场文件设想了一种下一代老年人监测系统，该系统超越跌倒检测，迈向日常生活活动 (ADL) 识别这一更广泛的目标。我们的最终目标是设计保护隐私、边缘部署和联合的人工智能系统，能够强大地检测和理解日常生活，支持老龄化社会的独立和尊严。目前，ADL专用数据集仍在收集中。作为第一步，我们通过使用 SISFall 数据集及其 GAN 增强变体的实验证明了可行性，将跌倒检测视为代理任务。我们报告了非 IID 条件下的联合学习以及 Jetson Orin Nano 设备上的嵌入式部署的初步结果。然后，我们概述了域转移、数据稀缺和隐私风险等开放挑战，并提出了在智能房间环境中进行全面 ADL 监控的方向。这项工作强调了从单任务检测到全面的日常活动识别的转变，为可持续和以人为本的老年护理人工智能提供了早期证据和路线图。</li>
</ul>

<h3>Title: Target-Balanced Score Distillation</h3>
<ul>
<li><strong>Authors: </strong>Zhou Xu, Qi Wang, Yuxiao Yang, Luyuan Zhang, Zhang Liang, Yang Li</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.11710">https://arxiv.org/abs/2511.11710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.11710">https://arxiv.org/pdf/2511.11710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.11710]] Target-Balanced Score Distillation(https://arxiv.org/abs/2511.11710)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Score Distillation Sampling (SDS) enables 3D asset generation by distilling priors from pretrained 2D text-to-image diffusion models, but vanilla SDS suffers from over-saturation and over-smoothing. To mitigate this issue, recent variants have incorporated negative prompts. However, these methods face a critical trade-off: limited texture optimization, or significant texture gains with shape distortion. In this work, we first conduct a systematic analysis and reveal that this trade-off is fundamentally governed by the utilization of the negative prompts, where Target Negative Prompts (TNP) that embed target information in the negative prompts dramatically enhancing texture realism and fidelity but inducing shape distortions. Informed by this key insight, we introduce the Target-Balanced Score Distillation (TBSD). It formulates generation as a multi-objective optimization problem and introduces an adaptive strategy that effectively resolves the aforementioned trade-off. Extensive experiments demonstrate that TBSD significantly outperforms existing state-of-the-art methods, yielding 3D assets with high-fidelity textures and geometrically accurate shape.</li>
<li><strong>摘要：</strong>分数蒸馏采样 (SDS) 通过从预训练的 2D 文本到图像扩散模型中提取先验信息来生成 3D 资产，但普通 SDS 存在过度饱和和过度平滑的问题。为了缓解这个问题，最近的变体包含了负面提示。然而，这些方法面临着一个关键的权衡：有限的纹理优化，或形状扭曲的显着纹理增益。在这项工作中，我们首先进行了系统分析，并揭示了这种权衡从根本上是由负面提示的利用决定的，其中将目标信息嵌入到负面提示中的目标负面提示（TNP）极大地增强了纹理的真实感和保真度，但会引起形状扭曲。根据这一关键见解，我们引入了目标平衡分数蒸馏（TBSD）。它将生成表述为多目标优化问题，并引入了有效解决上述权衡的自适应策略。大量实验表明，TBSD 显着优于现有的最先进方法，可生成具有高保真纹理和几何精确形状的 3D 资产。</li>
</ul>

<h3>Title: Optimizing Input of Denoising Score Matching is Biased Towards Higher Score Norm</h3>
<ul>
<li><strong>Authors: </strong>Tongda Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.11727">https://arxiv.org/abs/2511.11727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.11727">https://arxiv.org/pdf/2511.11727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.11727]] Optimizing Input of Denoising Score Matching is Biased Towards Higher Score Norm(https://arxiv.org/abs/2511.11727)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Many recent works utilize denoising score matching to optimize the conditional input of diffusion models. In this workshop paper, we demonstrate that such optimization breaks the equivalence between denoising score matching and exact score matching. Furthermore, we show that this bias leads to higher score norm. Additionally, we observe a similar bias when optimizing the data distribution using a pre-trained diffusion model. Finally, we discuss the wide range of works across different domains that are affected by this bias, including MAR for auto-regressive generation, PerCo for image compression, and DreamFusion for text to 3D generation.</li>
<li><strong>摘要：</strong>最近的许多工作利用去噪分数匹配来优化扩散模型的条件输入。在这篇研讨会论文中，我们证明了这种优化打破了去噪分数匹配和精确分数匹配之间的等价性。此外，我们表明这种偏差会导致更高的分数范数。此外，我们在使用预先训练的扩散模型优化数据分布时观察到类似的偏差。最后，我们讨论了受这种偏差影响的不同领域的广泛工作，包括用于自回归生成的 MAR、用于图像压缩的 PerCo 以及用于文本到 3D 生成的 DreamFusion。</li>
</ul>

<h3>Title: Exposing DeepFakes via Hyperspectral Domain Mapping</h3>
<ul>
<li><strong>Authors: </strong>Aditya Mehta, Swarnim Chaudhary, Pratik Narang, Jagat Sesh Challa</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.11732">https://arxiv.org/abs/2511.11732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.11732">https://arxiv.org/pdf/2511.11732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.11732]] Exposing DeepFakes via Hyperspectral Domain Mapping(https://arxiv.org/abs/2511.11732)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Modern generative and diffusion models produce highly realistic images that can mislead human perception and even sophisticated automated detection systems. Most detection methods operate in RGB space and thus analyze only three spectral channels. We propose HSI-Detect, a two-stage pipeline that reconstructs a 31-channel hyperspectral image from a standard RGB input and performs detection in the hyperspectral domain. Expanding the input representation into denser spectral bands amplifies manipulation artifacts that are often weak or invisible in the RGB domain, particularly in specific frequency bands. We evaluate HSI-Detect across FaceForensics++ dataset and show the consistent improvements over RGB-only baselines, illustrating the promise of spectral-domain mapping for Deepfake detection.</li>
<li><strong>摘要：</strong>现代生成和扩散模型产生高度逼真的图像，可能会误导人类的感知，甚至会误导复杂的自动检测系统。大多数检测方法在 RGB 空间中运行，因此仅分析三个光谱通道。我们提出了 HSI-Detect，这是一种两级管道，可从标准 RGB 输入重建 31 通道高光谱图像，并在高光谱域中执行检测。将输入表示扩展为更密集的频谱带会放大在 RGB 域中通常很弱或不可见的操纵伪影，特别是在特定频带中。我们评估了 FaceForensics++ 数据集上的 HSI-Detect，并展示了相对于仅 RGB 基线的一致改进，说明了用于 Deepfake 检测的谱域映射的前景。</li>
</ul>

<h3>Title: Diffusion Models: A Mathematical Introduction</h3>
<ul>
<li><strong>Authors: </strong>Sepehr Maleki, Negar Pourmoazemi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.11746">https://arxiv.org/abs/2511.11746</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.11746">https://arxiv.org/pdf/2511.11746</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.11746]] Diffusion Models: A Mathematical Introduction(https://arxiv.org/abs/2511.11746)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present a concise, self-contained derivation of diffusion-based generative models. Starting from basic properties of Gaussian distributions (densities, quadratic expectations, re-parameterisation, products, and KL divergences), we construct denoising diffusion probabilistic models from first principles. This includes the forward noising process, its closed-form marginals, the exact discrete reverse posterior, and the related variational bound. This bound simplifies to the standard noise-prediction goal used in practice. We then discuss likelihood estimation and accelerated sampling, covering DDIM, adversarially learned reverse dynamics (DDGAN), and multi-scale variants such as nested and latent diffusion, with Stable Diffusion as a canonical example. A continuous-time formulation follows, in which we derive the probability-flow ODE from the diffusion SDE via the continuity and Fokker-Planck equations, introduce flow matching, and show how rectified flows recover DDIM up to a time re-parameterisation. Finally, we treat guided diffusion, interpreting classifier guidance as a posterior score correction and classifier-free guidance as a principled interpolation between conditional and unconditional scores. Throughout, the focus is on transparent algebra, explicit intermediate steps, and consistent notation, so that readers can both follow the theory and implement the corresponding algorithms in practice.</li>
<li><strong>摘要：</strong>我们提出了基于扩散的生成模型的简洁、独立的推导。从高斯分布的基本属性（密度、二次期望、重新参数化、乘积和 KL 散度）出发，我们根据第一原理构建了去噪扩散概率模型。这包括前向噪声过程、其封闭形式边缘、精确的离散反向后验以及相关的变分界限。该界限简化为实践中使用的标准噪声预测目标。然后，我们讨论似然估计和加速采样，涵盖 DDIM、对抗学习反向动力学 (DDGAN) 和多尺度变体，例如嵌套扩散和潜在扩散，以稳定扩散作为典型示例。接下来是连续时间公式，其中我们通过连续性和 Fokker-Planck 方程从扩散 SDE 导出概率流 ODE，引入流匹配，并展示修正流如何恢复 DDIM 直至时间重新参数化。最后，我们对待引导扩散，将分类器指导解释为后验分数校正，将无分类器指导解释为条件分数和无条件分数之间的原则插值。自始至终，重点都是透明的代数、明确的中间步骤和一致的符号，以便读者既能遵循理论，又能在实践中实现相应的算法。</li>
</ul>

<h3>Title: Concept-RuleNet: Grounded Multi-Agent Neurosymbolic Reasoning in Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sanchit Sinha, Guangzhi Xiong, Zhenghao He, Aidong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.11751">https://arxiv.org/abs/2511.11751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.11751">https://arxiv.org/pdf/2511.11751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.11751]] Concept-RuleNet: Grounded Multi-Agent Neurosymbolic Reasoning in Vision Language Models(https://arxiv.org/abs/2511.11751)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Modern vision-language models (VLMs) deliver impressive predictive accuracy yet offer little insight into 'why' a decision is reached, frequently hallucinating facts, particularly when encountering out-of-distribution data. Neurosymbolic frameworks address this by pairing black-box perception with interpretable symbolic reasoning, but current methods extract their symbols solely from task labels, leaving them weakly grounded in the underlying visual data. In this paper, we introduce a multi-agent system - Concept-RuleNet that reinstates visual grounding while retaining transparent reasoning. Specifically, a multimodal concept generator first mines discriminative visual concepts directly from a representative subset of training images. Next, these visual concepts are utilized to condition symbol discovery, anchoring the generations in real image statistics and mitigating label bias. Subsequently, symbols are composed into executable first-order rules by a large language model reasoner agent - yielding interpretable neurosymbolic rules. Finally, during inference, a vision verifier agent quantifies the degree of presence of each symbol and triggers rule execution in tandem with outputs of black-box neural models, predictions with explicit reasoning pathways. Experiments on five benchmarks, including two challenging medical-imaging tasks and three underrepresented natural-image datasets, show that our system augments state-of-the-art neurosymbolic baselines by an average of 5% while also reducing the occurrence of hallucinated symbols in rules by up to 50%.</li>
<li><strong>摘要：</strong>现代视觉语言模型 (VLM) 提供了令人印象深刻的预测准确性，但对“为什么”做出决定却几乎没有深入了解，经常产生幻觉事实，尤其是在遇到分布外数据时。神经符号框架通过将黑盒感知与可解释的符号推理相结合来解决这个问题，但当前的方法仅从任务标签中提取符号，使它们在底层视觉数据中的基础薄弱。在本文中，我们介绍了一种多智能体系统 - Concept-RuleNet，它可以在保留透明推理的同时恢复视觉基础。具体来说，多模态概念生成器首先直接从训练图像的代表性子集中挖掘有区别的视觉概念。接下来，这些视觉概念被用来调节符号发现，将各代锚定在真实图像统计中并减轻标签偏差。随后，符号由大型语言模型推理代理组成可执行的一阶规则 - 产生可解释的神经符号规则。最后，在推理过程中，视觉验证器代理量化每个符号的存在程度，并与黑盒神经模型的输出一起触发规则执行，并通过显式推理路径进行预测。对五个基准的实验，包括两个具有挑战性的医学成像任务和三个代表性不足的自然图像数据集，表明我们的系统将最先进的神经符号基线平均增强了 5%，同时还将规则中幻觉符号的出现率减少了高达 50%。</li>
</ul>

<h3>Title: Batch Transformer Architecture: Case of Synthetic Image Generation for Emotion Expression Facial Recognition</h3>
<ul>
<li><strong>Authors: </strong>Stanislav Selitskiy</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.11754">https://arxiv.org/abs/2511.11754</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.11754">https://arxiv.org/pdf/2511.11754</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.11754]] Batch Transformer Architecture: Case of Synthetic Image Generation for Emotion Expression Facial Recognition(https://arxiv.org/abs/2511.11754)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>A novel Transformer variation architecture is proposed in the implicit sparse style. Unlike "traditional" Transformers, instead of attention to sequential or batch entities in their entirety of whole dimensionality, in the proposed Batch Transformers, attention to the "important" dimensions (primary components) is implemented. In such a way, the "important" dimensions or feature selection allows for a significant reduction of the bottleneck size in the encoder-decoder ANN architectures. The proposed architecture is tested on the synthetic image generation for the face recognition task in the case of the makeup and occlusion data set, allowing for increased variability of the limited original data set.</li>
<li><strong>摘要：</strong>以隐式稀疏风格提出了一种新颖的 Transformer 变体架构。与“传统” Transformer 不同，在提议的 Batch Transformer 中，不是关注整个维度的顺序或批处理实体，而是实现对“重要”维度（主要组件）的关注。通过这种方式，“重要”维度或特征选择可以显着减少编码器-解码器 ANN 架构中的瓶颈大小。所提出的架构在化妆和遮挡数据集的情况下，在人脸识别任务的合成图像生成上进行了测试，从而增加了有限原始数据集的可变性。</li>
</ul>

<h3>Title: Sumudu Neural Operator for ODEs and PDEs</h3>
<ul>
<li><strong>Authors: </strong>Ben Zelenskiy, Saibilila Abudukelimu, George Flint, Kevin Zhu, Sunishchal Dev</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.11762">https://arxiv.org/abs/2511.11762</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.11762">https://arxiv.org/pdf/2511.11762</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.11762]] Sumudu Neural Operator for ODEs and PDEs(https://arxiv.org/abs/2511.11762)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>We introduce the Sumudu Neural Operator (SNO), a neural operator rooted in the properties of the Sumudu Transform. We leverage the relationship between the polynomial expansions of transform pairs to decompose the input space as coefficients, which are then transformed into the Sumudu Space, where the neural operator is parameterized. We evaluate the operator in ODEs (Duffing Oscillator, Lorenz System, and Driven Pendulum) and PDEs (Euler-Bernoulli Beam, Burger's Equation, Diffusion, Diffusion-Reaction, and Brusselator). SNO achieves superior performance to FNO on PDEs and demonstrates competitive accuracy with LNO on several PDE tasks, including the lowest error on the Euler-Bernoulli Beam and Diffusion Equation. Additionally, we apply zero-shot super-resolution to the PDE tasks to observe the model's capability of obtaining higher quality data from low-quality samples. These preliminary findings suggest promise for the Sumudu Transform as a neural operator design, particularly for certain classes of PDEs.</li>
<li><strong>摘要：</strong>我们引入了 Sumudu 神经算子（SNO），这是一种植根于 Sumudu 变换属性的神经算子。我们利用变换对的多项式展开之间的关系将输入空间分解为系数，然后将其变换到 Sumudu 空间，其中神经算子被参数化。我们在 ODE（达芬振子、洛伦兹系统和驱动摆）和偏微分方程（欧拉-伯努利梁、伯格方程、扩散、扩散反应和 Brusselator）中评估算子。 SNO 在偏微分方程上实现了优于 FNO 的性能，并在多项偏微分方程任务上表现出与 LNO 相当的准确性，包括欧拉-伯努利梁和扩散方程的最低误差。此外，我们将零样本超分辨率应用于偏微分方程任务，以观察模型从低质量样本中获取更高质量数据的能力。这些初步发现表明 Sumudu 变换作为神经算子设计的前景，特别是对于某些类别的偏微分方程。</li>
</ul>

<h3>Title: Image-POSER: Reflective RL for Multi-Expert Image Generation and Editing</h3>
<ul>
<li><strong>Authors: </strong>Hossein Mohebbi, Mohammed Abdulrahman, Yanting Miao, Pascal Poupart, Suraj Kothawade</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.11780">https://arxiv.org/abs/2511.11780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.11780">https://arxiv.org/pdf/2511.11780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.11780]] Image-POSER: Reflective RL for Multi-Expert Image Generation and Editing(https://arxiv.org/abs/2511.11780)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in text-to-image generation have produced strong single-shot models, yet no individual system reliably executes the long, compositional prompts typical of creative workflows. We introduce Image-POSER, a reflective reinforcement learning framework that (i) orchestrates a diverse registry of pretrained text-to-image and image-to-image experts, (ii) handles long-form prompts end-to-end through dynamic task decomposition, and (iii) supervises alignment at each step via structured feedback from a vision-language model critic. By casting image synthesis and editing as a Markov Decision Process, we learn non-trivial expert pipelines that adaptively combine strengths across models. Experiments show that Image-POSER outperforms baselines, including frontier models, across industry-standard and custom benchmarks in alignment, fidelity, and aesthetics, and is consistently preferred in human evaluations. These results highlight that reinforcement learning can endow AI systems with the capacity to autonomously decompose, reorder, and combine visual models, moving towards general-purpose visual assistants.</li>
<li><strong>摘要：</strong>文本到图像生成的最新进展已经产生了强大的单次模型，但没有一个单独的系统能够可靠地执行创意工作流程中典型的长而组合的提示。我们介绍了 Image-POSER，一种反思性强化学习框架，它（i）协调预先训练的文本到图像和图像到图像专家的多样化注册，（ii）通过动态任务分解端到端处理长格式提示，以及（iii）通过视觉语言模型评论家的结构化反馈监督每一步的对齐。通过将图像合成和编辑作为马尔可夫决策过程，我们学习了不平凡的专家管道，可以自适应地结合跨模型的优势。实验表明，Image-POSER 在对齐、保真度和美观方面优于行业标准和自定义基准的基线（包括前沿模型），并且在人类评估中始终受到青睐。这些结果强调，强化学习可以赋予人工智能系统自主分解、重新排序和组合视觉模型的能力，向通用视觉助手迈进。</li>
</ul>

<h3>Title: FLEX: Feature Importance from Layered Counterfactual Explanations</h3>
<ul>
<li><strong>Authors: </strong>Nawid Keshtmand, Roussel Desmond Nzoyem, Jeffrey Nicholas Clark</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.11891">https://arxiv.org/abs/2511.11891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.11891">https://arxiv.org/pdf/2511.11891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.11891]] FLEX: Feature Importance from Layered Counterfactual Explanations(https://arxiv.org/abs/2511.11891)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Machine learning models achieve state-of-the-art performance across domains, yet their lack of interpretability limits safe deployment in high-stakes settings. Counterfactual explanations are widely used to provide actionable "what-if" recourse, but they typically remain instance-specific and do not quantify which features systematically drive outcome changes within coherent regions of the feature space or across an entire dataset. We introduce FLEX (Feature importance from Layered counterfactual EXplanations), a model- and domain-agnostic framework that converts sets of counterfactuals into feature change frequency scores at local, regional, and global levels. FLEX generalises local change-frequency measures by aggregating across instances and neighbourhoods, offering interpretable rankings that reflect how often each feature must change to flip predictions. The framework is compatible with different counterfactual generation methods, allowing users to emphasise characteristics such as sparsity, feasibility, or actionability, thereby tailoring the derived feature importances to practical constraints. We evaluate FLEX on two contrasting tabular tasks: traffic accident severity prediction and loan approval, and compare FLEX to SHAP- and LIME-derived feature importance values. Results show that (i) FLEX's global rankings correlate with SHAP while surfacing additional drivers, and (ii) regional analyses reveal context-specific factors that global summaries miss. FLEX thus bridges the gap between local recourse and global attribution, supporting transparent and intervention-oriented decision-making in risk-sensitive applications.</li>
<li><strong>摘要：</strong>机器学习模型在跨领域实现了最先进的性能，但它们缺乏可解释性限制了在高风险环境中的安全部署。反事实解释被广泛用于提供可操作的“假设”资源，但它们通常仍然是特定于实例的，并且不能量化哪些特征系统地驱动特征空间的连贯区域内或整个数据集内的结果变化。我们引入了 FLEX（分层反事实解释中的特征重要性），这是一个与模型和领域无关的框架，可将反事实集转换为本地、区域和全球级别的特征变化频率得分。 FLEX 通过跨实例和邻域聚合来概括本地更改频率度量，提供可解释的排名，反映每个特征必须更改以翻转预测的频率。该框架与不同的反事实生成方法兼容，允许用户强调稀疏性、可行性或可操作性等特征，从而根据实际约束调整派生特征的重要性。我们在两个对比鲜明的表格任务上评估 FLEX：交通事故严重程度预测和贷款审批，并将 FLEX 与 SHAP 和 LIME 派生的特征重要性值进行比较。结果表明，(i) FLEX 的全球排名与 SHAP 相关，同时揭示了其他驱动因素，(ii) 区域分析揭示了全球摘要遗漏的特定背景因素。因此，FLEX 弥合了本地追索和全球归因之间的差距，支持风险敏感应用程序中透明且以干预为导向的决策。</li>
</ul>

<h3>Title: Chain-of-Generation: Progressive Latent Diffusion for Text-Guided Molecular Design</h3>
<ul>
<li><strong>Authors: </strong>Lingxiao Li, Haobo Zhang, Bin Chen, Jiayu Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.11894">https://arxiv.org/abs/2511.11894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.11894">https://arxiv.org/pdf/2511.11894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.11894]] Chain-of-Generation: Progressive Latent Diffusion for Text-Guided Molecular Design(https://arxiv.org/abs/2511.11894)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-conditioned molecular generation aims to translate natural-language descriptions into chemical structures, enabling scientists to specify functional groups, scaffolds, and physicochemical constraints without handcrafted rules. Diffusion-based models, particularly latent diffusion models (LDMs), have recently shown promise by performing stochastic search in a continuous latent space that compactly captures molecular semantics. Yet existing methods rely on one-shot conditioning, where the entire prompt is encoded once and applied throughout diffusion, making it hard to satisfy all the requirements in the prompt. We discuss three outstanding challenges of one-shot conditioning generation, including the poor interpretability of the generated components, the failure to generate all substructures, and the overambition in considering all requirements simultaneously. We then propose three principles to address those challenges, motivated by which we propose Chain-of-Generation (CoG), a training-free multi-stage latent diffusion framework. CoG decomposes each prompt into curriculum-ordered semantic segments and progressively incorporates them as intermediate goals, guiding the denoising trajectory toward molecules that satisfy increasingly rich linguistic constraints. To reinforce semantic guidance, we further introduce a post-alignment learning phase that strengthens the correspondence between textual and molecular latent spaces. Extensive experiments on benchmark and real-world tasks demonstrate that CoG yields higher semantic alignment, diversity, and controllability than one-shot baselines, producing molecules that more faithfully reflect complex, compositional prompts while offering transparent insight into the generation process.</li>
<li><strong>摘要：</strong>文本条件分子生成旨在将自然语言描述转化为化学结构，使科学家能够指定官能团、支架和物理化学约束，而无需手工制定规则。基于扩散的模型，特别是潜在扩散模型（LDM），最近通过在连续潜在空间中执行随机搜索来紧凑地捕获分子语义而显示出前景。然而，现有方法依赖于一次性调节，其中整个提示被编码一次并在整个扩散过程中应用，这使得很难满足提示中的所有要求。我们讨论了一次性条件生成的三个突出挑战，包括生成组件的可解释性差、无法生成所有子结构以及同时考虑所有需求的野心太大。然后，我们提出了三个原则来应对这些挑战，受此启发，我们提出了生成链（CoG），这是一种免训练的多阶段潜在扩散框架。 CoG 将每个提示分解为按课程顺序排列的语义片段，并逐步将它们合并为中间目标，引导去噪轨迹朝向满足日益丰富的语言约束的分子。为了加强语义指导，我们进一步引入了对齐后学习阶段，该阶段加强了文本和分子潜在空间之间的对应关系。对基准和现实世界任务的大量实验表明，CoG 比一次性基线具有更高的语义对齐、多样性和可控性，产生的分子更忠实地反映复杂的组成提示，同时提供对生成过程的透明洞察。</li>
</ul>

<h3>Title: From Events to Clarity: The Event-Guided Diffusion Framework for Dehazing</h3>
<ul>
<li><strong>Authors: </strong>Ling Wang, Yunfan Lu, Wenzong Ma, Huizai Yao, Pengteng Li, Hui Xiong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.11944">https://arxiv.org/abs/2511.11944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.11944">https://arxiv.org/pdf/2511.11944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.11944]] From Events to Clarity: The Event-Guided Diffusion Framework for Dehazing(https://arxiv.org/abs/2511.11944)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Clear imaging under hazy conditions is a critical task. Prior-based and neural methods have improved results. However, they operate on RGB frames, which suffer from limited dynamic range. Therefore, dehazing remains ill-posed and can erase structure and illumination details. To address this, we use event cameras for dehazing for the \textbf{first time}. Event cameras offer much higher HDR ($120 dBvs.60 dB$) and microsecond latency, therefore they suit hazy scenes. In practice, transferring HDR cues from events to frames is hard because real paired data are scarce. To tackle this, we propose an event-guided diffusion model that utilizes the strong generative priors of diffusion models to reconstruct clear images from hazy inputs by effectively transferring HDR information from events. Specifically, we design an event-guided module that maps sparse HDR event features, \textit{e.g.,} edges, corners, into the diffusion latent space. This clear conditioning provides precise structural guidance during generation, improves visual realism, and reduces semantic drift. For real-world evaluation, we collect a drone dataset in heavy haze (AQI = 341) with synchronized RGB and event sensors. Experiments on two benchmarks and our dataset achieve state-of-the-art results.</li>
<li><strong>摘要：</strong>在雾霾条件下清晰成像是一项关键任务。基于先验的方法和神经方法改善了结果。然而，它们在 RGB 帧上运行，动态范围有限。因此，去雾仍然不合适，并且会消除结构和照明细节。为了解决这个问题，我们使用事件相机来进行\textbf{第一次}去雾。事件摄像机提供更高的 HDR（120 dBvs.60 dB$）和微秒延迟，因此适合雾霾场景。实际上，将 HDR 线索从事件传输到帧很困难，因为真正的配对数据很少。为了解决这个问题，我们提出了一种事件引导的扩散模型，该模型利用扩散模型的强大生成先验，通过有效地传输事件中的 HDR 信息，从模糊输入中重建清晰的图像。具体来说，我们设计了一个事件引导模块，将稀疏的 HDR 事件特征（\textit{例如}边缘、角点）映射到扩散潜在空间中。这种清晰的调节在生成过程中提供了精确的结构指导，提高了视觉真实感，并减少了语义漂移。为了进行实际评估，我们使用同步 RGB 和事件传感器收集了重度雾霾 (AQI = 341) 中的无人机数据集。在两个基准和我们的数据集上进行的实验取得了最先进的结果。</li>
</ul>

<h3>Title: BeyondFacial: Identity-Preserving Personalized Generation Beyond Facial Close-ups</h3>
<ul>
<li><strong>Authors: </strong>Songsong Zhang, Chuanqi Tang, Hongguang Zhang, Guijian Tang, Minglong Li, Xueqiong Li, Shaowu Yang, Yuanxi Peng, Wenjing Yang, Jing Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.11989">https://arxiv.org/abs/2511.11989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.11989">https://arxiv.org/pdf/2511.11989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.11989]] BeyondFacial: Identity-Preserving Personalized Generation Beyond Facial Close-ups(https://arxiv.org/abs/2511.11989)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Identity-Preserving Personalized Generation (IPPG) has advanced film production and artistic creation, yet existing approaches overemphasize facial regions, resulting in outputs dominated by facial this http URL methods suffer from weak visual narrativity and poor semantic consistency under complex text prompts, with the core limitation rooted in identity (ID) feature embeddings undermining the semantic expressiveness of generative models. To address these issues, this paper presents an IPPG method that breaks the constraint of facial close-ups, achieving synergistic optimization of identity fidelity and scene semantic creation. Specifically, we design a Dual-Line Inference (DLI) pipeline with identity-semantic separation, resolving the representation conflict between ID and semantics inherent in traditional single-path architectures. Further, we propose an Identity Adaptive Fusion (IdAF) strategy that defers ID-semantic fusion to the noise prediction stage, integrating adaptive attention fusion and noise decision masking to avoid ID embedding interference on semantics without manual masking. Finally, an Identity Aggregation Prepending (IdAP) module is introduced to aggregate ID information and replace random initializations, further enhancing identity preservation. Experimental results validate that our method achieves stable and effective performance in IPPG tasks beyond facial close-ups, enabling efficient generation without manual masking or fine-tuning. As a plug-and-play component, it can be rapidly deployed in existing IPPG frameworks, addressing the over-reliance on facial close-ups, facilitating film-level character-scene creation, and providing richer personalized generation capabilities for related domains.</li>
<li><strong>摘要：</strong>身份保留个性化生成（IPPG）具有先进的电影制作和艺术创作能力，但现有方法过分强调面部区域，导致以面部为主的输出，这种http URL方法在复杂文本提示下视觉叙事性弱，语义一致性差，其核心局限性在于身份（ID）特征嵌入破坏了生成模型的语义表达能力。针对这些问题，本文提出了一种IPPG方法，打破面部特写的约束，实现身份保真度和场景语义创建的协同优化。具体来说，我们设计了具有身份语义分离的双线推理（DLI）管道，解决了传统单路径架构中固有的ID和语义之间的表示冲突。此外，我们提出了一种身份自适应融合（IdAF）策略，将ID-语义融合推迟到噪声预测阶段，集成自适应注意力融合和噪声决策屏蔽，以避免ID嵌入对语义的干扰，而无需手动屏蔽。最后，引入身份聚合预置（IdAP）模块来聚合ID信息并取代随机初始化，进一步增强身份保存。实验结果验证了我们的方法在面部特写之外的 IPPG 任务中实现了稳定有效的性能，无需手动遮罩或微调即可实现高效生成。作为即插即用的组件，它可以快速部署在现有的IPPG框架中，解决对面部特写的过度依赖，促进电影级的人物场景创建，并为相关领域提供更丰富的个性化生成能力。</li>
</ul>

<h3>Title: Selecting Fine-Tuning Examples by Quizzing VLMs</h3>
<ul>
<li><strong>Authors: </strong>Tenghao Ji, Eytan Adar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12002">https://arxiv.org/abs/2511.12002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12002">https://arxiv.org/pdf/2511.12002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12002]] Selecting Fine-Tuning Examples by Quizzing VLMs(https://arxiv.org/abs/2511.12002)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>A challenge in fine-tuning text-to-image diffusion models for specific topics is to select good examples. Fine-tuning from image sets of varying quality, such as Wikipedia Commons, will often produce poor output. However, training images that \textit{do} exemplify the target concept (e.g., a \textit{female Mountain Bluebird}) help ensure that the generated images are similarly representative (e.g., have the prototypical blue-wings and gray chest). In this work, we propose QZLoRA, a framework to select images for low-rank adaptation (LoRA). The approach leverages QuizRank, a method to automatically rank images by treating them as an `educational intervention' and `quizzing' a VLM. We demonstrate that QZLoRA can produce better aligned, photorealistic images with fewer samples. We also show that these fine-tuned models can produce stylized that are similarly representative (i.e., illustrations). Our results highlight the promise of combining automated visual reasoning with parameter-efficient fine-tuning for topic-adaptive generative modeling.</li>
<li><strong>摘要：</strong>针对特定主题微调文本到图像扩散模型的一个挑战是选择好的例子。从不同质量的图像集（例如维基共享资源）进行微调通常会产生较差的输出。然而，\textit{do} 例证目标概念的训练图像（例如，\textit{female Mountain Bluebird}）有助于确保生成的图像具有类似的代表性（例如，具有典型的蓝色翅膀和灰色胸部）。在这项工作中，我们提出了 QZLoRA，一种选择图像进行低秩适应（LoRA）的框架。该方法利用 QuizRa​​nk，这是一种通过将图像视为“教育干预”和“测验”VLM 来自动对图像进行排名的方法。我们证明 QZLoRA 可以用更少的样本生成更好对齐、逼真的图像。我们还表明，这些经过微调的模型可以产生具有类似代表性的风格化（即插图）。我们的结果强调了将自动视觉推理与参数高效微调相结合以实现主题自适应生成模型的前景。</li>
</ul>

<h3>Title: Improved Masked Image Generation with Knowledge-Augmented Token Representations</h3>
<ul>
<li><strong>Authors: </strong>Guotao Liang, Baoquan Zhang, Zhiyuan Wen, Zihao Han, Yunming Ye</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12032">https://arxiv.org/abs/2511.12032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12032">https://arxiv.org/pdf/2511.12032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12032]] Improved Masked Image Generation with Knowledge-Augmented Token Representations(https://arxiv.org/abs/2511.12032)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Masked image generation (MIG) has demonstrated remarkable efficiency and high-fidelity images by enabling parallel token prediction. Existing methods typically rely solely on the model itself to learn semantic dependencies among visual token sequences. However, directly learning such semantic dependencies from data is challenging because the individual tokens lack clear semantic meanings, and these sequences are usually long. To address this limitation, we propose a novel Knowledge-Augmented Masked Image Generation framework, named KA-MIG, which introduces explicit knowledge of token-level semantic dependencies (\emph{i.e.}, extracted from the training data) as priors to learn richer representations for improving performance. In particular, we explore and identify three types of advantageous token knowledge graphs, including two positive and one negative graphs (\emph{i.e.}, the co-occurrence graph, the semantic similarity graph, and the position-token incompatibility graph). Based on three prior knowledge graphs, we design a graph-aware encoder to learn token and position-aware representations. After that, a lightweight fusion mechanism is introduced to integrate these enriched representations into the existing MIG methods. Resorting to such prior knowledge, our method effectively enhances the model's ability to capture semantic dependencies, leading to improved generation quality. Experimental results demonstrate that our method improves upon existing MIG for class-conditional image generation on ImageNet.</li>
<li><strong>摘要：</strong>蒙版图像生成 (MIG) 通过实现并行标记预测，展示了卓越的效率和高保真度图像。现有方法通常仅依赖于模型本身来学习视觉标记序列之间的语义依赖性。然而，直接从数据中学习这种语义依赖关系是具有挑战性的，因为各个标记缺乏明确的语义，而且这些序列通常很长。为了解决这个限制，我们提出了一种新颖的知识增强蒙版图像生成框架，名为 KA-MIG，它引入了标记级语义依赖关系的显式知识（\emph{i.e.}，从训练数据中提取）作为先验，以学习更丰富的表示以提高性能。特别是，我们探索并识别了三种类型的有利标记知识图，包括两种正图和一种负图（\emph{i.e.，共现图、语义相似性图和位置标记不兼容图）。基于三个先验知识图，我们设计了一个图感知编码器来学习标记和位置感知表示。之后，引入轻量级融合机制将这些丰富的表示集成到现有的 MIG 方法中。借助这些先验知识，我们的方法有效地增强了模型捕获语义依赖关系的能力，从而提高了生成质量。实验结果表明，我们的方法改进了现有 MIG，用于在 ImageNet 上生成类条件图像。</li>
</ul>

<h3>Title: EARL: Entropy-Aware RL Alignment of LLMs for Reliable RTL Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiahe Shi, Zhengqi Gao, Ching-Yun Ko, Duane Boning</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12033">https://arxiv.org/abs/2511.12033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12033">https://arxiv.org/pdf/2511.12033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12033]] EARL: Entropy-Aware RL Alignment of LLMs for Reliable RTL Code Generation(https://arxiv.org/abs/2511.12033)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have demonstrated significant potential in hardware design automation, particularly in using natural language to synthesize Register-Transfer Level (RTL) code. Despite this progress, a gap remains between model capability and the demands of real-world RTL design, including syntax errors, functional hallucinations, and weak alignment to designer intent. Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising approach to bridge this gap, as hardware provides executable and formally checkable signals that can be used to further align model outputs with design intent. However, in long, structured RTL code sequences, not all tokens contribute equally to functional correctness, and naïvely spreading gradients across all tokens dilutes learning signals. A key insight from our entropy analysis in RTL generation is that only a small fraction of tokens (e.g., always, if, assign, posedge) exhibit high uncertainty and largely influence control flow and module structure. To address these challenges, we present EARL, an Entropy-Aware Reinforcement Learning framework for Verilog generation. EARL performs policy optimization using verifiable reward signals and introduces entropy-guided selective updates that gate policy gradients to high-entropy tokens. This approach preserves training stability and concentrates gradient updates on functionally important regions of code. Our experiments on VerilogEval and RTLLM show that EARL improves functional pass rates over prior LLM baselines by up to 14.7%, while reducing unnecessary updates and improving training stability. These results indicate that focusing RL on critical, high-uncertainty tokens enables more reliable and targeted policy improvement for structured RTL code generation.</li>
<li><strong>摘要：</strong>大语言模型 (LLM) 的最新进展展示了硬件设计自动化的巨大潜力，特别是在使用自然语言合成寄存器传输级 (RTL) 代码方面。尽管取得了这些进展，模型功能与现实世界 RTL 设计的需求之间仍然存在差距，包括语法错误、功能幻觉以及与设计者意图的一致性较差。具有可验证奖励的强化学习（RLVR）提供了一种有前途的方法来弥补这一差距，因为硬件提供了可执行且可形式检查的信号，可用于进一步使模型输出与设计意图保持一致。然而，在长的、结构化的 RTL 代码序列中，并非所有标记都对功能正确性有同等的贡献，并且天真地将梯度分布在所有标记上会稀释学习信号。我们在 RTL 生成中进行熵分析的一个关键见解是，只有一小部分标记（例如，always、if、assign、posege）表现出高度不确定性，并在很大程度上影响控制流和模块结构。为了应对这些挑战，我们提出了 EARL，一种用于 Verilog 生成的熵感知强化学习框架。 EARL 使用可验证的奖励信号执行策略优化，并引入熵引导的选择性更新，将策略梯度门控到高熵代币。这种方法保持了训练的稳定性，并将梯度更新集中在功能重要的代码区域。我们在 VerilogEval 和 RTLLM 上的实验表明，与之前的 LLM 基线相比，EARL 的功能通过率提高了 14.7%，同时减少了不必要的更新并提高了训练稳定性。这些结果表明，将强化学习重点放在关键的、高不确定性的标记上，可以为结构化 RTL 代码生成提供更可靠、更有针对性的策略改进。</li>
</ul>

<h3>Title: SRSplat: Feed-Forward Super-Resolution Gaussian Splatting from Sparse Multi-View Images</h3>
<ul>
<li><strong>Authors: </strong>Xinyuan Hu, Changyue Shi, Chuxiao Yang, Minghao Chen, Jiajun Ding, Tao Wei, Chen Wei, Zhou Yu, Min Tan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12040">https://arxiv.org/abs/2511.12040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12040">https://arxiv.org/pdf/2511.12040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12040]] SRSplat: Feed-Forward Super-Resolution Gaussian Splatting from Sparse Multi-View Images(https://arxiv.org/abs/2511.12040)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Feed-forward 3D reconstruction from sparse, low-resolution (LR) images is a crucial capability for real-world applications, such as autonomous driving and embodied AI. However, existing methods often fail to recover fine texture details. This limitation stems from the inherent lack of high-frequency information in LR inputs. To address this, we propose \textbf{SRSplat}, a feed-forward framework that reconstructs high-resolution 3D scenes from only a few LR views. Our main insight is to compensate for the deficiency of texture information by jointly leveraging external high-quality reference images and internal texture cues. We first construct a scene-specific reference gallery, generated for each scene using Multimodal Large Language Models (MLLMs) and diffusion models. To integrate this external information, we introduce the \textit{Reference-Guided Feature Enhancement (RGFE)} module, which aligns and fuses features from the LR input images and their reference twin image. Subsequently, we train a decoder to predict the Gaussian primitives using the multi-view fused feature obtained from \textit{RGFE}. To further refine predicted Gaussian primitives, we introduce \textit{Texture-Aware Density Control (TADC)}, which adaptively adjusts Gaussian density based on the internal texture richness of the LR inputs. Extensive experiments demonstrate that our SRSplat outperforms existing methods on various datasets, including RealEstate10K, ACID, and DTU, and exhibits strong cross-dataset and cross-resolution generalization capabilities.</li>
<li><strong>摘要：</strong>从稀疏、低分辨率 (LR) 图像中进行前馈 3D 重建对于自动驾驶和嵌入式 AI 等实际应用来说是一项至关重要的功能。然而，现有的方法通常无法恢复精细的纹理细节。这种限制源于 LR 输入中固有的高频信息缺乏。为了解决这个问题，我们提出了 \textbf{SRSplat}，这是一个前馈框架，可以仅从几个 LR 视图重建高分辨率 3D 场景。我们的主要见解是通过联合利用外部高质量参考图像和内部纹理线索来弥补纹理信息的不足。我们首先构建一个特定于场景的参考库，使用多模态大型语言模型（MLLM）和扩散模型为每个场景生成。为了整合这些外部信息，我们引入了 \textit{参考引导特征增强（RGFE）} 模块，该模块可以对齐并融合 LR 输入图像及其参考孪生图像的特征。随后，我们训练解码器使用从 \textit{RGFE} 获得的多视图融合特征来预测高斯基元。为了进一步细化预测的高斯基元，我们引入了 \textit{纹理感知密度控制（TADC）}，它根据 LR 输入的内部纹理丰富度自适应调整高斯密度。大量实验表明，我们的 SRSplat 在各种数据集（包括 RealEstate10K、ACID 和 DTU）上均优于现有方法，并表现出强大的跨数据集和跨分辨率泛化能力。</li>
</ul>

<h3>Title: Mesh-based Super-resolution of Detonation Flows with Multiscale Graph Transformers</h3>
<ul>
<li><strong>Authors: </strong>Shivam Barwey, Pinaki Pal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12041">https://arxiv.org/abs/2511.12041</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12041">https://arxiv.org/pdf/2511.12041</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12041]] Mesh-based Super-resolution of Detonation Flows with Multiscale Graph Transformers(https://arxiv.org/abs/2511.12041)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Super-resolution flow reconstruction using state-of-the-art data-driven techniques is valuable for a variety of applications, such as subgrid/subfilter closure modeling, accelerating spatiotemporal forecasting, data compression, and serving as an upscaling tool for sparse experimental measurements. In the present work, a first-of-its-kind multiscale graph transformer approach is developed for mesh-based super-resolution (SR-GT) of reacting flows. The novel data-driven modeling paradigm leverages a graph-based flow-field representation compatible with complex geometries and non-uniform/unstructured grids. Further, the transformer backbone captures long-range dependencies between different parts of the low-resolution flow-field, identifies important features, and then generates the super-resolved flow-field that preserves those features at a higher resolution. The performance of SR-GT is demonstrated in the context of spectral-element-discretized meshes for a challenging test problem of 2D detonation propagation within a premixed hydrogen-air mixture exhibiting highly complex multiscale reacting flow behavior. The SR-GT framework utilizes a unique element-local (+ neighborhood) graph representation for the coarse input, which is then tokenized before being processed by the transformer component to produce the fine output. It is demonstrated that SR-GT provides high super-resolution accuracy for reacting flow-field features and superior performance compared to traditional interpolation-based SR schemes.</li>
<li><strong>摘要：</strong>使用最先进的数据驱动技术的超分辨率流重建对于各种应用都很有价值，例如子网格/子滤波器闭合建模、加速时空预测、数据压缩以及作为稀疏实验测量的升级工具。在目前的工作中，开发了一种首创的多尺度图形转换器方法，用于反应流的基于网格的超分辨率（SR-GT）。新颖的数据驱动建模范例利用与复杂几何形状和非均匀/非结构化网格兼容的基于图形的流场表示。此外，变压器骨干捕获低分辨率流场不同部分之间的远程依赖性，识别重要特征，然后生成超分辨率流场，以更高分辨率保留这些特征。 SR-GT 的性能在光谱元素离散网格的背景下得到了证明，适用于预混氢气-空气混合物中二维爆炸传播的挑战性测试问题，表现出高度复杂的多尺度反应流行为。 SR-GT 框架利用独特的元素局部（+邻域）图形表示来表示粗略输入，然后在由转换器组件处理以产生精细输出之前对其进行标记化。结果表明，与传统的基于插值的 SR 方案相比，SR-GT 为反应流场特征提供了高超分辨率精度和卓越的性能。</li>
</ul>

<h3>Title: PipeDiT: Accelerating Diffusion Transformers in Video Generation with Task Pipelining and Model Decoupling</h3>
<ul>
<li><strong>Authors: </strong>Sijie Wang, Qiang Wang, Shaohuai Shi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12056">https://arxiv.org/abs/2511.12056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12056">https://arxiv.org/pdf/2511.12056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12056]] PipeDiT: Accelerating Diffusion Transformers in Video Generation with Task Pipelining and Model Decoupling(https://arxiv.org/abs/2511.12056)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Video generation has been advancing rapidly, and diffusion transformer (DiT) based models have demonstrated remark- able capabilities. However, their practical deployment is of- ten hindered by slow inference speeds and high memory con- sumption. In this paper, we propose a novel pipelining frame- work named PipeDiT to accelerate video generation, which is equipped with three main innovations. First, we design a pipelining algorithm (PipeSP) for sequence parallelism (SP) to enable the computation of latent generation and commu- nication among multiple GPUs to be pipelined, thus reduc- ing inference latency. Second, we propose DeDiVAE to de- couple the diffusion module and the variational autoencoder (VAE) module into two GPU groups, whose executions can also be pipelined to reduce memory consumption and infer- ence latency. Third, to better utilize the GPU resources in the VAE group, we propose an attention co-processing (Aco) method to further reduce the overall video generation latency. We integrate our PipeDiT into both OpenSoraPlan and Hun- yuanVideo, two state-of-the-art open-source video generation frameworks, and conduct extensive experiments on two 8- GPU systems. Experimental results show that, under many common resolution and timestep configurations, our PipeDiT achieves 1.06x to 4.02x speedups over OpenSoraPlan and HunyuanVideo.</li>
<li><strong>摘要：</strong>视频生成一直在快速发展，基于扩散变换器（DiT）的模型已经展示了卓越的功能。然而，它们的实际部署常常受到推理速度慢和内存消耗高的阻碍。在本文中，我们提出了一种名为 PipeDiT 的新型流水线框架来加速视频生成，该框架具有三项主要创新。首先，我们设计了一种用于序列并行（SP）的流水线算法（PipeSP），使多个GPU之间的潜在生成和通信的计算能够流水线化，从而减少推理延迟。其次，我们提出 DeDiVAE 将扩散模块和变分自动编码器（VAE）模块解耦到两个 GPU 组中，它们的执行也可以流水线化以减少内存消耗和推理延迟。第三，为了更好地利用VAE组中的GPU资源，我们提出了一种注意力协同处理（Aco）方法来进一步减少整体视频生成延迟。我们将 PipeDiT 集成到 OpenSoraPlan 和 HunyuanVideo 这两个最先进的开源视频生成框架中，并在两个 8-GPU 系统上进行了广泛的实验。实验结果表明，在许多常见的分辨率和时间步长配置下，我们的 PipeDiT 比 OpenSoraPlan 和 HunyuanVideo 实现了 1.06 倍到 4.02 倍的加速。</li>
</ul>

<h3>Title: DCA-LUT: Deep Chromatic Alignment with 5D LUT for Purple Fringing Removal</h3>
<ul>
<li><strong>Authors: </strong>Jialang Lu, Shuning Sun, Pu Wang, Chen Wu, Feng Gao, Lina Gong, Dianjie Lu, Guijuan Zhang, Zhuoran Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12066">https://arxiv.org/abs/2511.12066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12066">https://arxiv.org/pdf/2511.12066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12066]] DCA-LUT: Deep Chromatic Alignment with 5D LUT for Purple Fringing Removal(https://arxiv.org/abs/2511.12066)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Purple fringing, a persistent artifact caused by Longitudinal Chromatic Aberration (LCA) in camera lenses, has long degraded the clarity and realism of digital imaging. Traditional solutions rely on complex and expensive apochromatic (APO) lens hardware and the extraction of handcrafted features, ignoring the data-driven approach. To fill this gap, we introduce DCA-LUT, the first deep learning framework for purple fringing removal. Inspired by the physical root of the problem, the spatial misalignment of RGB color channels due to lens dispersion, we introduce a novel Chromatic-Aware Coordinate Transformation (CA-CT) module, learning an image-adaptive color space to decouple and isolate fringing into a dedicated dimension. This targeted separation allows the network to learn a precise ``purple fringe channel", which then guides the accurate restoration of the luminance channel. The final color correction is performed by a learned 5D Look-Up Table (5D LUT), enabling efficient and powerful% non-linear color mapping. To enable robust training and fair evaluation, we constructed a large-scale synthetic purple fringing dataset (PF-Synth). Extensive experiments in synthetic and real-world datasets demonstrate that our method achieves state-of-the-art performance in purple fringing removal.</li>
<li><strong>摘要：</strong>紫边是相机镜头中纵向色差 (LCA) 造成的一种持久伪像，长期以来一直降低了数字成像的清晰度和真实感。传统解决方案依赖于复杂且昂贵的复消色差 (APO) 镜头硬件以及手工特征的提取，而忽略了数据驱动的方法。为了填补这一空白，我们引入了 DCA-LUT，这是第一个用于紫边去除的深度学习框架。受问题的物理根源（由于镜头色散导致 RGB 颜色通道的空间错位）的启发，我们引入了一种新颖的色彩感知坐标变换 (CA-CT) 模块，学习图像自适应颜色空间以将边纹解耦并隔离到专用维度中。这种有针对性的分离使网络能够学习精确的“紫边通道”，然后指导亮度通道的准确恢复。最终的颜色校正由学习的 5D 查找表（5D LUT）执行，从而实现高效且强大的非线性颜色映射。为了实现稳健的训练和公平的评估，我们构建了大规模合成紫边数据集（PF-Synth）。在合成和真实数据集中的大量实验表明，我们的方法实现了紫边去除方面最先进的性能。</li>
</ul>

<h3>Title: Improving Graph Embeddings in Machine Learning Using Knowledge Completion with Validation in a Case Study on COVID-19 Spread</h3>
<ul>
<li><strong>Authors: </strong>Rosario Napoli, Gabriele Morabito, Antonio Celesti, Massimo Villari, Maria Fazio</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12071">https://arxiv.org/abs/2511.12071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12071">https://arxiv.org/pdf/2511.12071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12071]] Improving Graph Embeddings in Machine Learning Using Knowledge Completion with Validation in a Case Study on COVID-19 Spread(https://arxiv.org/abs/2511.12071)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The rise of graph-structured data has driven major advances in Graph Machine Learning (GML), where graph embeddings (GEs) map features from Knowledge Graphs (KGs) into vector spaces, enabling tasks like node classification and link prediction. However, since GEs are derived from explicit topology and features, they may miss crucial implicit knowledge hidden in seemingly sparse datasets, affecting graph structure and their representation. We propose a GML pipeline that integrates a Knowledge Completion (KC) phase to uncover latent dataset semantics before embedding generation. Focusing on transitive relations, we model hidden connections with decay-based inference functions, reshaping graph topology, with consequences on embedding dynamics and aggregation processes in GraphSAGE and Node2Vec. Experiments show that our GML pipeline significantly alters the embedding space geometry, demonstrating that its introduction is not just a simple enrichment but a transformative step that redefines graph representation quality.</li>
<li><strong>摘要：</strong>图结构数据的兴起推动了图机器学习 (GML) 的重大进步，其中图嵌入 (GE) 将知识图 (KG) 中的特征映射到向量空间，从而实现节点分类和链接预测等任务。然而，由于 GE 是从显式拓扑和特征导出的，因此它们可能会错过隐藏在看似稀疏的数据集中的关键隐式知识，从而影响图结构及其表示。我们提出了一个 GML 管道，它集成了知识完成 (KC) 阶段，以在嵌入生成之前发现潜在的数据集语义。专注于传递关系，我们使用基于衰减的推理函数对隐藏连接进行建模，重塑图拓扑，从而对 GraphSAGE 和 Node2Vec 中的嵌入动态和聚合过程产生影响。实验表明，我们的 GML 管道显着改变了嵌入空间几何形状，表明它的引入不仅仅是一个简单的丰富，而是重新定义图表示质量的变革性步骤。</li>
</ul>

<h3>Title: DINOv3-Guided Cross Fusion Framework for Semantic-aware CT generation from MRI and CBCT</h3>
<ul>
<li><strong>Authors: </strong>Xianhao Zhou, Jianghao Wu, Ku Zhao, Jinlong He, Huangxuan Zhao, Lei Chen, Shaoting Zhang, Guotai Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12098">https://arxiv.org/abs/2511.12098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12098">https://arxiv.org/pdf/2511.12098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12098]] DINOv3-Guided Cross Fusion Framework for Semantic-aware CT generation from MRI and CBCT(https://arxiv.org/abs/2511.12098)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating synthetic CT images from CBCT or MRI has a potential for efficient radiation dose planning and adaptive radiotherapy. However, existing CNN-based models lack global semantic understanding, while Transformers often overfit small medical datasets due to high model capacity and weak inductive bias. To address these limitations, we propose a DINOv3-Guided Cross Fusion (DGCF) framework that integrates a frozen self-supervised DINOv3 Transformer with a trainable CNN encoder-decoder. It hierarchically fuses global representation of Transformer and local features of CNN via a learnable cross fusion module, achieving balanced local appearance and contextual representation. Furthermore, we introduce a Multi-Level DINOv3 Perceptual (MLDP) loss that encourages semantic similarity between synthetic CT and the ground truth in DINOv3's feature space. Experiments on the SynthRAD2023 pelvic dataset demonstrate that DGCF achieved state-of-the-art performance in terms of MS-SSIM, PSNR and segmentation-based metrics on both MRI$\rightarrow$CT and CBCT$\rightarrow$CT translation tasks. To the best of our knowledge, this is the first work to employ DINOv3 representations for medical image translation, highlighting the potential of self-supervised Transformer guidance for semantic-aware CT synthesis. The code is available at this https URL.</li>
<li><strong>摘要：</strong>从 CBCT 或 MRI 生成合成 CT 图像具有高效辐射剂量规划和适应性放射治疗的潜力。然而，现有的基于 CNN 的模型缺乏全局语义理解，而 Transformer 由于模型容量高和归纳偏差弱，常常会过度拟合小型医疗数据集。为了解决这些限制，我们提出了一种 DINOv3 引导交叉融合 (DGCF) 框架，该框架将冻结的自监督 DINOv3 Transformer 与可训练的 CNN 编码器-解码器集成在一起。它通过可学习的交叉融合模块分层融合 Transformer 的全局表示和 CNN 的局部特征，实现平衡的局部外观和上下文表示。此外，我们引入了多级 DINOv3 感知 (MLDP) 损失，它鼓励合成 CT 与 DINOv3 特征空间中的地面事实之间的语义相似性。 SynthRAD2023 骨盆数据集上的实验表明，DGCF 在 MRI$\rightarrow$CT 和 CBCT$\rightarrow$CT 翻译任务上的 MS-SSIM、PSNR 和基于分割的指标方面均实现了最先进的性能。据我们所知，这是第一个采用 DINOv3 表示进行医学图像翻译的工作，凸显了自监督 Transformer 指导用于语义感知 CT 合成的潜力。该代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Adaptive Begin-of-Video Tokens for Autoregressive Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Tianle Cheng, Zeyan Zhang, Kaifeng Gao, Jun Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12099">https://arxiv.org/abs/2511.12099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12099">https://arxiv.org/pdf/2511.12099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12099]] Adaptive Begin-of-Video Tokens for Autoregressive Video Diffusion Models(https://arxiv.org/abs/2511.12099)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advancements in diffusion-based video generation have produced impressive and high-fidelity short videos. To extend these successes to generate coherent long videos, most video diffusion models (VDMs) generate videos in an autoregressive manner, i.e., generating subsequent frames conditioned on previous ones. There are generally two primary paradigms: chunk-based extension and stream denoising. The former directly concatenates previous clean frames as conditioning, suffering from denoising latency and error accumulation. The latter maintains the denoising sequence with monotonically increasing noise levels. In each denoising iteration, one clean frame is produced while a new pure noise is simultaneously appended, enabling live-stream sampling. However, it struggles with fragile consistency and poor motion dynamics. In this paper, we propose Adaptive Begin-of-Video Tokens (ada-BOV) for autoregressive VDMs. The BOV tokens are special learnable embeddings on VDMs. They adaptively absorb denoised preceding frames via an adaptive-layer-norm-like modulation. This design preserves the global consistency while allowing for flexible conditioning in dynamic scenarios. To ensure the quality of local dynamics essential in modulating BOV tokens, we further propose a refinement strategy for stream denoising. It decouples the sampling trajectory length from the attention window size constraint, leading to improved local guidance and overall imaging quality. We also propose a disturbance-augmented training noise schedule, which balances the convergence speed with model robustness for the stream denoising. Extensive experiments demonstrate that our method achieves compelling qualitative and quantitative results across multiple metrics.</li>
<li><strong>摘要：</strong>基于扩散的视频生成的最新进展已经产生了令人印象深刻的高保真短视频。为了扩展这些成功以生成连贯的长视频，大多数视频扩散模型（VDM）以自回归方式生成视频，即根据先前的帧生成后续帧。通常有两种主要范例：基于块的扩展和流去噪。前者直接连接之前的干净帧作为条件，存在去噪延迟和错误累积的问题。后者保持噪声水平单调增加的去噪序列。在每次去噪迭代中，都会生成一个干净的帧，同时附加一个新的纯噪声，从而实现实时流采样。然而，它面临着脆弱的一致性和较差的运动动力学问题。在本文中，我们提出了用于自回归 VDM 的自适应视频开始标记 (ada-BOV)。 BOV 代币是 VDM 上特殊的可学习嵌入。它们通过类似自适应层范数的调制自适应地吸收去噪的前一帧。这种设计保留了全局一致性，同时允许在动态场景中进行灵活调节。为了确保调制 BOV 代币所必需的局部动态的质量，我们进一步提出了一种流去噪的细化策略。它将采样轨迹长度与注意窗口大小约束解耦，从而提高局部引导和整体成像质量。我们还提出了一种干扰增强训练噪声方案，它平衡了流去噪的收敛速度和模型鲁棒性。大量的实验表明，我们的方法在多个指标上取得了令人信服的定性和定量结果。</li>
</ul>

<h3>Title: RadarMP: Motion Perception for 4D mmWave Radar in Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Ruiqi Cheng, Huijun Di, Jian Li, Feng Liu, Wei Liang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12117">https://arxiv.org/abs/2511.12117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12117">https://arxiv.org/pdf/2511.12117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12117]] RadarMP: Motion Perception for 4D mmWave Radar in Autonomous Driving(https://arxiv.org/abs/2511.12117)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Accurate 3D scene motion perception significantly enhances the safety and reliability of an autonomous driving system. Benefiting from its all-weather operational capability and unique perceptual properties, 4D mmWave radar has emerged as an essential component in advanced autonomous driving. However, sparse and noisy radar points often lead to imprecise motion perception, leaving autonomous vehicles with limited sensing capabilities when optical sensors degrade under adverse weather conditions. In this paper, we propose RadarMP, a novel method for precise 3D scene motion perception using low-level radar echo signals from two consecutive frames. Unlike existing methods that separate radar target detection and motion estimation, RadarMP jointly models both tasks in a unified architecture, enabling consistent radar point cloud generation and pointwise 3D scene flow prediction. Tailored to radar characteristics, we design specialized self-supervised loss functions guided by Doppler shifts and echo intensity, effectively supervising spatial and motion consistency without explicit annotations. Extensive experiments on the public dataset demonstrate that RadarMP achieves reliable motion perception across diverse weather and illumination conditions, outperforming radar-based decoupled motion perception pipelines and enhancing perception capabilities for full-scenario autonomous driving systems.</li>
<li><strong>摘要：</strong>准确的3D场景运动感知显着增强自动驾驶系统的安全性和可靠性。得益于其全天候作战能力和独特的感知特性，4D毫米波雷达已成为先进自动驾驶的重要组成部分。然而，稀疏且嘈杂的雷达点通常会导致运动感知不精确，当光学传感器在恶劣天气条件下退化时，自动驾驶车辆的传感能力有限。在本文中，我们提出了 RadarMP，这是一种使用来自两个连续帧的低电平雷达回波信号进行精确 3D 场景运动感知的新方法。与将雷达目标检测和运动估计分开的现有方法不同，RadarMP 在统一架构中对这两个任务进行联合建模，从而实现一致的雷达点云生成和逐点 3D 场景流预测。针对雷达特性，我们设计了由多普勒频移和回波强度引导的专门自监督损失函数，无需明确注释即可有效监督空间和运动一致性。对公共数据集的大量实验表明，RadarMP在不同的天气和光照条件下实现了可靠的运动感知，优于基于雷达的解耦运动感知管道，增强了全场景自动驾驶系统的感知能力。</li>
</ul>

<h3>Title: OAD-Promoter: Enhancing Zero-shot VQA using Large Language Models with Object Attribute Description</h3>
<ul>
<li><strong>Authors: </strong>Quanxing Xu, Ling Zhou, Feifei Zhang, Jinyu Tian, Rubing Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12131">https://arxiv.org/abs/2511.12131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12131">https://arxiv.org/pdf/2511.12131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12131]] OAD-Promoter: Enhancing Zero-shot VQA using Large Language Models with Object Attribute Description(https://arxiv.org/abs/2511.12131)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have become a crucial tool in Visual Question Answering (VQA) for handling knowledge-intensive questions in few-shot or zero-shot scenarios. However, their reliance on massive training datasets often causes them to inherit language biases during the acquisition of knowledge. This limitation imposes two key constraints on existing methods: (1) LLM predictions become less reliable due to bias exploitation, and (2) despite strong knowledge reasoning capabilities, LLMs still struggle with out-of-distribution (OOD) generalization. To address these issues, we propose Object Attribute Description Promoter (OAD-Promoter), a novel approach for enhancing LLM-based VQA by mitigating language bias and improving domain-shift robustness. OAD-Promoter comprises three components: the Object-concentrated Example Generation (OEG) module, the Memory Knowledge Assistance (MKA) module, and the OAD Prompt. The OEG module generates global captions and object-concentrated samples, jointly enhancing visual information input to the LLM and mitigating bias through complementary global and regional visual cues. The MKA module assists the LLM in handling OOD samples by retrieving relevant knowledge from stored examples to support questions from unseen domains. Finally, the OAD Prompt integrates the outputs of the preceding modules to optimize LLM inference. Experiments demonstrate that OAD-Promoter significantly improves the performance of LLM-based VQA methods in few-shot or zero-shot settings, achieving new state-of-the-art results.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已成为视觉问答 (VQA) 中的重要工具，用于处理少样本或零样本场景中的知识密集型问题。然而，他们对大量训练数据集的依赖常常导致他们在获取知识的过程中继承语言偏见。这一限制给现有方法带来了两个关键限制：(1) 由于偏差利用，LLM 预测变得不太可靠；(2) 尽管知识推理能力很强，LLM 仍然难以应对分布外 (OOD) 泛化。为了解决这些问题，我们提出了对象属性描述启动器（OAD-Promoter），这是一种通过减轻语言偏差和提高域转移鲁棒性来增强基于 LLM 的 VQA 的新方法。 OAD-Promoter 由三个组件组成：对象集中示例生成 (OEG) 模块、记忆知识辅助 (MKA) 模块和 OAD 提示。 OEG 模块生成全局标题和对象集中样本，共同增强 LLM 的视觉信息输入，并通过互补的全局和区域视觉线索减轻偏见。 MKA 模块通过从存储的示例中检索相关知识来支持来自未见领域的问题，从而协助法学硕士处理 OOD 示例。最后，OAD Prompt 集成了前面模块的输出来优化 LLM 推理。实验表明，OAD-Promoter 显着提高了基于 LLM 的 VQA 方法在少样本或零样本设置中的性能，实现了新的最先进的结果。</li>
</ul>

<h3>Title: Rethinking Multimodal Point Cloud Completion: A Completion-by-Correction Perspective</h3>
<ul>
<li><strong>Authors: </strong>Wang Luo, Di Wu, Hengyuan Na, Yinlin Zhu, Miao Hu, Guocong Quan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12170">https://arxiv.org/abs/2511.12170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12170">https://arxiv.org/pdf/2511.12170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12170]] Rethinking Multimodal Point Cloud Completion: A Completion-by-Correction Perspective(https://arxiv.org/abs/2511.12170)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Point cloud completion aims to reconstruct complete 3D shapes from partial observations, which is a challenging problem due to severe occlusions and missing geometry. Despite recent advances in multimodal techniques that leverage complementary RGB images to compensate for missing geometry, most methods still follow a Completion-by-Inpainting paradigm, synthesizing missing structures from fused latent features. We empirically show that this paradigm often results in structural inconsistencies and topological artifacts due to limited geometric and semantic constraints. To address this, we rethink the task and propose a more robust paradigm, termed Completion-by-Correction, which begins with a topologically complete shape prior generated by a pretrained image-to-3D model and performs feature-space correction to align it with the partial observation. This paradigm shifts completion from unconstrained synthesis to guided refinement, enabling structurally consistent and observation-aligned reconstruction. Building upon this paradigm, we introduce PGNet, a multi-stage framework that conducts dual-feature encoding to ground the generative prior, synthesizes a coarse yet structurally aligned scaffold, and progressively refines geometric details via hierarchical correction. Experiments on the ShapeNetViPC dataset demonstrate the superiority of PGNet over state-of-the-art baselines in terms of average Chamfer Distance (-23.5%) and F-score (+7.1%).</li>
<li><strong>摘要：</strong>点云补全旨在从部分观测中重建完整的 3D 形状，由于严重的遮挡和缺失的几何形状，这是一个具有挑战性的问题。尽管多模态技术最近取得了进展，利用互补的 RGB 图像来补偿缺失的几何形状，但大多数方法仍然遵循补全修复范例，从融合的潜在特征合成缺失的结构。我们凭经验表明，由于有限的几何和语义约束，这种范式常常导致结构不一致和拓扑伪影。为了解决这个问题，我们重新思考这个任务并提出了一个更强大的范例，称为“校正完成”，它从由预训练的图像到 3D 模型生成的拓扑完整形状开始，并执行特征空间校正以使其与部分观察对齐。这种范式将完成从不受约束的合成转变为引导细化，从而实现结构一致和观察一致的重建。在此范式的基础上，我们引入了 PGNet，这是一个多阶段框架，它进行双特征编码以奠定生成先验的基础，合成一个粗略但结构对齐的支架，并通过分层校正逐步细化几何细节。 ShapeNetViPC 数据集上的实验证明了 PGNet 在平均倒角距离 (-23.5%) 和 F 分数 (+7.1%) 方面优于最先进的基线。</li>
</ul>

<h3>Title: FGM optimization in complex domains using Gaussian process regression based profile generation algorithm</h3>
<ul>
<li><strong>Authors: </strong>Chaitanya Kumar Konda, Piyush Agrawal, Shivansh Srivastava, Manish Agrawal</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12171">https://arxiv.org/abs/2511.12171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12171">https://arxiv.org/pdf/2511.12171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12171]] FGM optimization in complex domains using Gaussian process regression based profile generation algorithm(https://arxiv.org/abs/2511.12171)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This manuscript addresses the challenge of designing functionally graded materials (FGMs) for arbitrary-shaped domains. Towards this goal, the present work proposes a generic volume fraction profile generation algorithm based on Gaussian Process Regression (GPR). The proposed algorithm can handle complex-shaped domains and generate smooth FGM profiles while adhering to the specified volume fraction values at boundaries/part of boundaries. The resulting design space from GPR comprises diverse profiles, enhancing the potential for discovering optimal configurations. Further, the algorithm allows the user to control the smoothness of the underlying profiles and the size of the design space through a length scale parameter. Further, the proposed profile generation scheme is coupled with the genetic algorithm to find the optimum FGM profiles for a given application. To make the genetic algorithm consistent with the GPR profile generation scheme, the standard simulated binary crossover operator in the genetic algorithm has been modified with a projection operator. We present numerous thermoelastic optimization examples to demonstrate the efficacy of the proposed profile generation algorithm and optimization framework.</li>
<li><strong>摘要：</strong>该手稿解决了为任意形状的域设计功能分级材料（FGM）的挑战。为了实现这一目标，目前的工作提出了一种基于高斯过程回归（GPR）的通用体积分数剖面生成算法。所提出的算法可以处理复杂形状的域并生成平滑的 FGM 轮廓，同时遵守边界/部分边界处的指定体积分数值。探地雷达产生的设计空间包含不同的配置文件，增强了发现最佳配置的潜力。此外，该算法允许用户通过长度比例参数来控制基础轮廓的平滑度和设计空间的大小。此外，所提出的配置文件生成方案与遗传算法相结合，以找到给定应用的最佳 FGM 配置文件。为了使遗传算法与探地雷达剖面生成方案一致，遗传算法中的标准模拟二元交叉算子被修改为投影算子。我们提出了许多热弹性优化示例来证明所提出的轮廓生成算法和优化框架的有效性。</li>
</ul>

<h3>Title: TSGDiff: Rethinking Synthetic Time Series Generation from a Pure Graph Perspective</h3>
<ul>
<li><strong>Authors: </strong>Lifeng Shen, Xuyang Li, Lele Long</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12174">https://arxiv.org/abs/2511.12174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12174">https://arxiv.org/pdf/2511.12174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12174]] TSGDiff: Rethinking Synthetic Time Series Generation from a Pure Graph Perspective(https://arxiv.org/abs/2511.12174)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models have shown great promise in data generation, yet generating time series data remains challenging due to the need to capture complex temporal dependencies and structural patterns. In this paper, we present \textit{TSGDiff}, a novel framework that rethinks time series generation from a graph-based perspective. Specifically, we represent time series as dynamic graphs, where edges are constructed based on Fourier spectrum characteristics and temporal dependencies. A graph neural network-based encoder-decoder architecture is employed to construct a latent space, enabling the diffusion process to model the structural representation distribution of time series effectively. Furthermore, we propose the Topological Structure Fidelity (Topo-FID) score, a graph-aware metric for assessing the structural similarity of time series graph representations. Topo-FID integrates two sub-metrics: Graph Edit Similarity, which quantifies differences in adjacency matrices, and Structural Entropy Similarity, which evaluates the entropy of node degree distributions. This comprehensive metric provides a more accurate assessment of structural fidelity in generated time series. Experiments on real-world datasets demonstrate that \textit{TSGDiff} generates high-quality synthetic time series data generation, faithfully preserving temporal dependencies and structural integrity, thereby advancing the field of synthetic time series generation.</li>
<li><strong>摘要：</strong>扩散模型在数据生成方面显示出了巨大的前景，但由于需要捕获复杂的时间依赖性和结构模式，生成时间序列数据仍然具有挑战性。在本文中，我们提出了 \textit{TSGDiff}，这是一种从基于图的角度重新思考时间序列生成的新颖框架。具体来说，我们将时间序列表示为动态图，其中边缘是根据傅里叶谱特征和时间依赖性构建的。采用基于图神经网络的编码器-解码器架构来构造潜在空间，使扩散过程能够有效地对时间序列的结构表示分布进行建模。此外，我们提出了拓扑结构保真度（Topo-FID）评分，这是一种用于评估时间序列图表示的结构相似性的图感知指标。 Topo-FID 集成了两个子指标：图形编辑相似性（量化邻接矩阵的差异）和结构熵相似性（评估节点度分布的熵）。这种综合指标可以更准确地评估生成的时间序列中的结构保真度。对真实世界数据集的实验表明， \textit{TSGDiff} 可以生成高质量的合成时间序列数据，忠实地保留时间依赖性和结构完整性，从而推动合成时间序列生成领域的发展。</li>
</ul>

<h3>Title: MixAR: Mixture Autoregressive Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Jinyuan Hu, Jiayou Zhang, Shaobo Cui, Kun Zhang, Guangyi Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12181">https://arxiv.org/abs/2511.12181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12181">https://arxiv.org/pdf/2511.12181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12181]] MixAR: Mixture Autoregressive Image Generation(https://arxiv.org/abs/2511.12181)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Autoregressive (AR) approaches, which represent images as sequences of discrete tokens from a finite codebook, have achieved remarkable success in image generation. However, the quantization process and the limited codebook size inevitably discard fine-grained information, placing bottlenecks on fidelity. Motivated by this limitation, recent studies have explored autoregressive modeling in continuous latent spaces, which offers higher generation quality. Yet, unlike discrete tokens constrained by a fixed codebook, continuous representations lie in a vast and unstructured space, posing significant challenges for efficient autoregressive modeling. To address these challenges, we introduce MixAR, a novel framework that leverages mixture training paradigms to inject discrete tokens as prior guidance for continuous AR modeling. MixAR is a factorized formulation that leverages discrete tokens as prior guidance for continuous autoregressive prediction. We investigate several discrete-continuous mixture strategies, including self-attention (DC-SA), cross-attention (DC-CA), and a simple approach (DC-Mix) that replaces homogeneous mask tokens with informative discrete counterparts. Moreover, to bridge the gap between ground-truth training tokens and inference tokens produced by the pre-trained AR model, we propose Training-Inference Mixture (TI-Mix) to achieve consistent training and generation distributions. In our experiments, we demonstrate a favorable balance of the DC-Mix strategy between computational efficiency and generation fidelity, and consistent improvement of TI-Mix.</li>
<li><strong>摘要：</strong>自回归（AR）方法将图像表示为来自有限码本的离散标记序列，在图像生成方面取得了显着的成功。然而，量化过程和有限的码本大小不可避免地会丢弃细粒度信息，从而给保真度带来瓶颈。受此限制的启发，最近的研究探索了连续潜在空间中的自回归建模，这提供了更高的生成质量。然而，与受固定密码本约束的离散标记不同，连续表示位于广阔且非结构化的空间中，这对高效的自回归建模提出了重大挑战。为了应对这些挑战，我们引入了 MixAR，这是一种新颖的框架，它利用混合训练范例注入离散令牌作为连续 AR 建模的事先指导。 MixAR 是一种因式分解公式，利用离散标记作为连续自回归预测的先验指导。我们研究了几种离散连续混合策略，包括自注意力（DC-SA）、交叉注意力（DC-CA）和一种用信息丰富的离散对应物替换同质掩码标记的简单方法（DC-Mix）。此外，为了弥合真实训练标记和预训练 AR 模型生成的推理标记之间的差距，我们提出训练-推理混合 (TI-Mix) 以实现一致的训练和生成分布。在我们的实验中，我们证明了 DC-Mix 策略在计算效率和生成保真度之间的良好平衡，以及 TI-Mix 的持续改进。</li>
</ul>

<h3>Title: LSS3D: Learnable Spatial Shifting for Consistent and High-Quality 3D Generation from Single-Image</h3>
<ul>
<li><strong>Authors: </strong>Zhuojiang Cai, Yiheng Zhang, Meitong Guo, Mingdao Wang, Yuwang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12202">https://arxiv.org/abs/2511.12202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12202">https://arxiv.org/pdf/2511.12202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12202]] LSS3D: Learnable Spatial Shifting for Consistent and High-Quality 3D Generation from Single-Image(https://arxiv.org/abs/2511.12202)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recently, multi-view diffusion-based 3D generation methods have gained significant attention. However, these methods often suffer from shape and texture misalignment across generated multi-view images, leading to low-quality 3D generation results, such as incomplete geometric details and textural ghosting. Some methods are mainly optimized for the frontal perspective and exhibit poor robustness to oblique perspective inputs. In this paper, to tackle the above challenges, we propose a high-quality image-to-3D approach, named LSS3D, with learnable spatial shifting to explicitly and effectively handle the multiview inconsistencies and non-frontal input view. Specifically, we assign learnable spatial shifting parameters to each view, and adjust each view towards a spatially consistent target, guided by the reconstructed mesh, resulting in high-quality 3D generation with more complete geometric details and clean textures. Besides, we include the input view as an extra constraint for the optimization, further enhancing robustness to non-frontal input angles, especially for elevated viewpoint inputs. We also provide a comprehensive quantitative evaluation pipeline that can contribute to the community in performance comparisons. Extensive experiments demonstrate that our method consistently achieves leading results in both geometric and texture evaluation metrics across more flexible input viewpoints.</li>
<li><strong>摘要：</strong>最近，基于多视图扩散的 3D 生成方法受到了广泛关注。然而，这些方法经常会遇到生成的多视图图像之间形状和纹理未对齐的问题，从而导致低质量的 3D 生成结果，例如不完整的几何细节和纹理重影。一些方法主要针对正面视角进行优化，对倾斜视角输入的鲁棒性较差。在本文中，为了应对上述挑战，我们提出了一种名为 LSS3D 的高质量图像到 3D 方法，该方法具有可学习的空间移位，可以明确有效地处理多视图不一致和非正面输入视图。具体来说，我们为每个视图分配可学习的空间移位参数，并在重建网格的引导下将每个视图调整为空间一致的目标，从而产生具有更完整的几何细节和干净的纹理的高质量 3D 生成。此外，我们将输入视图作为优化的额外约束，进一步增强对非正面输入角度的鲁棒性，特别是对于高视点输入。我们还提供全面的定量评估管道，可以为社区进行性能比较做出贡献。大量的实验表明，我们的方法在更灵活的输入视点上一致地在几何和纹理评估指标方面取得了领先的结果。</li>
</ul>

<h3>Title: GeoMVD: Geometry-Enhanced Multi-View Generation Model Based on Geometric Information Extraction</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Wu, Yaosen Chen, Shuyuan Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12204">https://arxiv.org/abs/2511.12204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12204">https://arxiv.org/pdf/2511.12204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12204]] GeoMVD: Geometry-Enhanced Multi-View Generation Model Based on Geometric Information Extraction(https://arxiv.org/abs/2511.12204)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generation</a></li>
<li><strong>Abstract: </strong>Multi-view image generation holds significant application value in computer vision, particularly in domains like 3D reconstruction, virtual reality, and augmented reality. Most existing methods, which rely on extending single images, face notable computational challenges in maintaining cross-view consistency and generating high-resolution outputs. To address these issues, we propose the Geometry-guided Multi-View Diffusion Model, which incorporates mechanisms for extracting multi-view geometric information and adjusting the intensity of geometric features to generate images that are both consistent across views and rich in detail. Specifically, we design a multi-view geometry information extraction module that leverages depth maps, normal maps, and foreground segmentation masks to construct a shared geometric structure, ensuring shape and structural consistency across different views. To enhance consistency and detail restoration during generation, we develop a decoupled geometry-enhanced attention mechanism that strengthens feature focus on key geometric details, thereby improving overall image quality and detail preservation. Furthermore, we apply an adaptive learning strategy that fine-tunes the model to better capture spatial relationships and visual coherence between the generated views, ensuring realistic results. Our model also incorporates an iterative refinement process that progressively improves the output quality through multiple stages of image generation. Finally, a dynamic geometry information intensity adjustment mechanism is proposed to adaptively regulate the influence of geometric data, optimizing overall quality while ensuring the naturalness of generated images. More details can be found on the project page: this https URL.</li>
<li><strong>摘要：</strong>多视图图像生成在计算机视觉中具有重要的应用价值，特别是在 3D 重建、虚拟现实和增强现实等领域。大多数现有方法依赖于扩展单个图像，在保持跨视图一致性和生成高分辨率输出方面面临着显着的计算挑战。为了解决这些问题，我们提出了几何引导的多视图扩散模型，该模型结合了提取多视图几何信息和调整几何特征强度的机制，以生成跨视图一致且细节丰富的图像。具体来说，我们设计了一个多视图几何信息提取模块，利用深度图、法线图和前景分割掩模来构建共享的几何结构，确保不同视图之间的形状和结构一致性。为了增强生成过程中的一致性和细节恢复，我们开发了一种解耦的几何增强注意机制，该机制加强了对关键几何细节的特征关注，从而提高了整体图像质量和细节保留。此外，我们应用自适应学习策略来微调模型，以更好地捕捉生成的视图之间的空间关系和视觉连贯性，确保真实的结果。我们的模型还采用了迭代细化过程，通过图像生成的多个阶段逐步提高输出质量。最后，提出了一种动态几何信息强度调整机制，自适应调节几何数据的影响，在保证生成图像自然度的同时优化整体质量。更多详细信息可以在项目页面上找到：此 https URL。</li>
</ul>

<h3>Title: Mixture of States: Routing Token-Level Dynamics for Multimodal Generation</h3>
<ul>
<li><strong>Authors: </strong>Haozhe Liu, Ding Liu, Mingchen Zhuge, Zijian Zhou, Tian Xie, Sen He, Yukang Yang, Shuming Liu, Yuren Cong, Jiadong Guo, Hongyu Xu, Ke Xu, Kam-Woh Ng, Juan C. Pérez, Juan-ManuelPérez-Rúa, Tao Xiang, Wei Liu, Shikun Liu, Jürgen Schmidhuber</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12207">https://arxiv.org/abs/2511.12207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12207">https://arxiv.org/pdf/2511.12207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12207]] Mixture of States: Routing Token-Level Dynamics for Multimodal Generation(https://arxiv.org/abs/2511.12207)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce MoS (Mixture of States), a novel fusion paradigm for multimodal diffusion models that merges modalities using flexible, state-based interactions. The core of MoS is a learnable, token-wise router that creates denoising timestep- and input-dependent interactions between modalities' hidden states, precisely aligning token-level features with the diffusion trajectory. This router sparsely selects the top-$k$ hidden states and is trained with an $\epsilon$-greedy strategy, efficiently selecting contextual features with minimal learnable parameters and negligible computational overhead. We validate our design with text-to-image generation (MoS-Image) and editing (MoS-Editing), which achieve state-of-the-art results. With only 3B to 5B parameters, our models match or surpass counterparts up to $4\times$ larger. These findings establish MoS as a flexible and compute-efficient paradigm for scaling multimodal diffusion models.</li>
<li><strong>摘要：</strong>我们引入了 MoS（状态混合），这是一种用于多模态扩散模型的新颖融合范例，它使用灵活的、基于状态的交互来合并模态。 MoS 的核心是一个可学习的、令牌智能的​​路由器，它在模式的隐藏状态之间创建与时间步长和输入相关的去噪交互，从而将令牌级特征与扩散轨迹精确对齐。该路由器稀疏地选择 top-$k$ 隐藏状态，并使用 $\epsilon$ 贪婪策略进行训练，以最少的可学习参数和可忽略的计算开销有效地选择上下文特征。我们通过文本到图像生成（MoS-Image）和编辑（MoS-Editing）来验证我们的设计，从而实现了最先进的结果。只需 3B 到 5B 参数，我们的模型就可以匹配或超越同类模型 4 倍。这些发现将 MoS 确立为一种灵活且计算高效的范式，用于扩展多模态扩散模型。</li>
</ul>

<h3>Title: AlignTree: Efficient Defense Against LLM Jailbreak Attacks</h3>
<ul>
<li><strong>Authors: </strong>Gil Goren, Shahar Katz, Lior Wolf</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12217">https://arxiv.org/abs/2511.12217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12217">https://arxiv.org/pdf/2511.12217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12217]] AlignTree: Efficient Defense Against LLM Jailbreak Attacks(https://arxiv.org/abs/2511.12217)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are vulnerable to adversarial attacks that bypass safety guidelines and generate harmful content. Mitigating these vulnerabilities requires defense mechanisms that are both robust and computationally efficient. However, existing approaches either incur high computational costs or rely on lightweight defenses that can be easily circumvented, rendering them impractical for real-world LLM-based systems. In this work, we introduce the AlignTree defense, which enhances model alignment while maintaining minimal computational overhead. AlignTree monitors LLM activations during generation and detects misaligned behavior using an efficient random forest classifier. This classifier operates on two signals: (i) the refusal direction -- a linear representation that activates on misaligned prompts, and (ii) an SVM-based signal that captures non-linear features associated with harmful content. Unlike previous methods, AlignTree does not require additional prompts or auxiliary guard models. Through extensive experiments, we demonstrate the efficiency and robustness of AlignTree across multiple LLMs and benchmarks.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 很容易受到对抗性攻击，这些攻击会绕过安全准则并生成有害内容。缓解这些漏洞需要强大且计算高效的防御机制。然而，现有的方法要么会产生高昂的计算成本，要么依赖于很容易被规避的轻量级防御，这使得它们对于现实世界中基于 LLM 的系统来说不切实际。在这项工作中，我们引入了 AlignTree 防御，它增强了模型对齐，同时保持最小的计算开销。 AlignTree 在生成过程中监控 LLM 激活，并使用高效的随机森林分类器检测未对齐的行为。该分类器根据两个信号进行操作：(i) 拒绝方向——在未对齐提示时激活的线性表示，以及 (ii) 基于 SVM 的信号，捕获与有害内容相关的非线性特征。与之前的方法不同，AlignTree 不需要额外的提示或辅助防护模型。通过大量的实验，我们在多个法学硕士和基准测试中展示了 AlignTree 的效率和稳健性。</li>
</ul>

<h3>Title: Model Inversion Attack Against Deep Hashing</h3>
<ul>
<li><strong>Authors: </strong>Dongdong Zhao, Qiben Xu, Ranxin Fang, Baogang Song</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12233">https://arxiv.org/abs/2511.12233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12233">https://arxiv.org/pdf/2511.12233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12233]] Model Inversion Attack Against Deep Hashing(https://arxiv.org/abs/2511.12233)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Deep hashing improves retrieval efficiency through compact binary codes, yet it introduces severe and often overlooked privacy risks. The ability to reconstruct original training data from hash codes could lead to serious threats such as biometric forgery and privacy breaches. However, model inversion attacks specifically targeting deep hashing models remain unexplored, leaving their security implications unexamined. This research gap stems from the inaccessibility of genuine training hash codes and the highly discrete Hamming space, which prevents existing methods from adapting to deep hashing. To address these challenges, we propose DHMI, the first diffusion-based model inversion framework designed for deep hashing. DHMI first clusters an auxiliary dataset to derive semantic hash centers as surrogate anchors. It then introduces a surrogate-guided denoising optimization method that leverages a novel attack metric (fusing classification consistency and hash proximity) to dynamically select candidate samples. A cluster of surrogate models guides the refinement of these candidates, ensuring the generation of high-fidelity and semantically consistent images. Experiments on multiple datasets demonstrate that DHMI successfully reconstructs high-resolution, high-quality images even under the most challenging black-box setting, where no training hash codes are available. Our method outperforms the existing state-of-the-art model inversion attacks in black-box scenarios, confirming both its practical efficacy and the critical privacy risks inherent in deep hashing systems.</li>
<li><strong>摘要：</strong>深度哈希通过紧凑的二进制代码提高了检索效率，但它带来了严重且经常被忽视的隐私风险。从哈希码重建原始训练数据的能力可能会导致严重的威胁，例如生物识别伪造和隐私泄露。然而，专门针对深度哈希模型的模型反转攻击仍未得到探索，其安全隐患也未得到检验。这一研究差距源于真正的训练哈希码的不可访问性和高度离散的汉明空间，这阻碍了现有方法适应深度哈希。为了应对这些挑战，我们提出了 DHMI，这是第一个专为深度哈希设计的基于扩散的模型反演框架。 DHMI 首先对辅助数据集进行聚类，以派生语义哈希中心作为代理锚点。然后，它引入了一种代理引导的去噪优化方法，该方法利用一种新颖的攻击指标（融合分类一致性和哈希邻近性）来动态选择候选样本。代理模型集群指导这些候选模型的细化，确保生成高保真且语义一致的图像。对多个数据集的实验表明，即使在最具挑战性的黑盒设置（没有可用的训练哈希码）下，DHMI 也能成功重建高分辨率、高质量图像。我们的方法在黑盒场景中优于现有最先进的模型反转攻击，证实了其实际功效和深度哈希系统固有的关键隐私风险。</li>
</ul>

<h3>Title: Prompt-Conditioned FiLM and Multi-Scale Fusion on MedSigLIP for Low-Dose CT Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Tolga Demiroglu (1), Mehmet Ozan Unal (1), Metin Ertas (2), Isa Yildirim (1) ((1) Electronics and Communication Engineering Department, Istanbul Technical University, Istanbul, Turkey, (2) Istanbul University, Istanbul, Turkey)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12256">https://arxiv.org/abs/2511.12256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12256">https://arxiv.org/pdf/2511.12256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12256]] Prompt-Conditioned FiLM and Multi-Scale Fusion on MedSigLIP for Low-Dose CT Quality Assessment(https://arxiv.org/abs/2511.12256)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>We propose a prompt-conditioned framework built on MedSigLIP that injects textual priors via Feature-wise Linear Modulation (FiLM) and multi-scale pooling. Text prompts condition patch-token features on clinical intent, enabling data-efficient learning and rapid adaptation. The architecture combines global, local, and texture-aware pooling through separate regression heads fused by a lightweight MLP, trained with pairwise ranking loss. Evaluated on the LDCTIQA2023 (a public LDCT quality assessment challenge) with 1,000 training images, we achieve PLCC = 0.9575, SROCC = 0.9561, and KROCC = 0.8301, surpassing the top-ranked published challenge submissions and demonstrating the effectiveness of our prompt-guided approach.</li>
<li><strong>摘要：</strong>我们提出了一个基于 MedSigLIP 的提示条件框架，该框架通过特征线性调制（FiLM）和多尺度池注入文本先验。文本提示有关临床意图的条件补丁标记功能，从而实现数据高效学习和快速适应。该架构通过轻量级 MLP 融合的单独回归头结合了全局、局部和纹理感知池，并通过成对排名损失进行训练。在 LDCTIQA2023（公开 LDCT 质量评估挑战）上使用 1,000 个训练图像进行评估时，我们实现了 PLCC = 0.9575、SROCC = 0.9561 和 KROCC = 0.8301，超过了排名最高的已发布挑战提交内容，并证明了我们的提示引导方法的有效性。</li>
</ul>

<h3>Title: A Disease-Aware Dual-Stage Framework for Chest X-ray Report Generation</h3>
<ul>
<li><strong>Authors: </strong>Puzhen Wu, Hexin Dong, Yi Lin, Yihao Ding, Yifan Peng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12259">https://arxiv.org/abs/2511.12259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12259">https://arxiv.org/pdf/2511.12259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12259]] A Disease-Aware Dual-Stage Framework for Chest X-ray Report Generation(https://arxiv.org/abs/2511.12259)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Radiology report generation from chest X-rays is an important task in artificial intelligence with the potential to greatly reduce radiologists' workload and shorten patient wait times. Despite recent advances, existing approaches often lack sufficient disease-awareness in visual representations and adequate vision-language alignment to meet the specialized requirements of medical image analysis. As a result, these models usually overlook critical pathological features on chest X-rays and struggle to generate clinically accurate reports. To address these limitations, we propose a novel dual-stage disease-aware framework for chest X-ray report generation. In Stage~1, our model learns Disease-Aware Semantic Tokens (DASTs) corresponding to specific pathology categories through cross-attention mechanisms and multi-label classification, while simultaneously aligning vision and language representations via contrastive learning. In Stage~2, we introduce a Disease-Visual Attention Fusion (DVAF) module to integrate disease-aware representations with visual features, along with a Dual-Modal Similarity Retrieval (DMSR) mechanism that combines visual and disease-specific similarities to retrieve relevant exemplars, providing contextual guidance during report generation. Extensive experiments on benchmark datasets (i.e., CheXpert Plus, IU X-ray, and MIMIC-CXR) demonstrate that our disease-aware framework achieves state-of-the-art performance in chest X-ray report generation, with significant improvements in clinical accuracy and linguistic quality.</li>
<li><strong>摘要：</strong>根据胸部 X 光生成放射学报告是人工智能中的一项重要任务，有可能大大减少放射科医生的工作量并缩短患者的等待时间。尽管最近取得了进展，但现有方法通常缺乏足够的视觉表示疾病意识和足够的视觉语言对齐来满足医学图像分析的特殊要求。因此，这些模型通常会忽略胸部 X 射线的关键病理特征，并且难以生成临床准确的报告。为了解决这些限制，我们提出了一种用于胸部 X 射线报告生成的新型双阶段疾病感知框架。在阶段〜1中，我们的模型通过交叉注意机制和多标签分类来学习与特定病理类别相对应的疾病感知语义标记（DAST），同时通过对比学习来对齐视觉和语言表示。在阶段〜2中，我们引入了疾病视觉注意融合（DVAF）模块，将疾病感知表示与视觉特征相结合，以及双模态相似性检索（DMSR）机制，结合视觉和疾病特定的相似性来检索相关样本，在报告生成过程中提供上下文指导。对基准数据集（即 CheXpert Plus、IU X 射线和 MIMIC-CXR）的大量实验表明，我们的疾病感知框架在胸部 X 射线报告生成方面实现了最先进的性能，并且临床准确性和语言质量显着提高。</li>
</ul>

<h3>Title: D$^{3}$ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Shuochen Chang, Xiaofeng Zhang, Qingyang Liu, Li Niu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12280">https://arxiv.org/abs/2511.12280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12280">https://arxiv.org/pdf/2511.12280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12280]] D$^{3}$ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion MLLMs(https://arxiv.org/abs/2511.12280)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion-based multimodal large language models (Diffusion MLLMs) have recently demonstrated impressive non-autoregressive generative capabilities across vision-and-language tasks. However, Diffusion MLLMs exhibit substantially slower inference than autoregressive models: Each denoising step employs full bidirectional self-attention over the entire sequence, resulting in cubic decoding complexity that becomes computationally impractical with thousands of visual tokens. To address this challenge, we propose D$^{3}$ToM, a Decider-guided dynamic token merging method that dynamically merges redundant visual tokens at different denoising steps to accelerate inference in Diffusion MLLMs. At each denoising step, D$^{3}$ToM uses decider tokens-the tokens generated in the previous denoising step-to build an importance map over all visual tokens. Then it maintains a proportion of the most salient tokens and merges the remainder through similarity-based aggregation. This plug-and-play module integrates into a single transformer layer, physically shortening the visual token sequence for all subsequent layers without altering model parameters. Moreover, D$^{3}$ToM employs a merge ratio that dynamically varies with each denoising step, aligns with the native decoding process of Diffusion MLLMs, achieving superior performance under equivalent computational budgets. Extensive experiments show that D$^{3}$ToM accelerates inference while preserving competitive performance. The code is released at this https URL.</li>
<li><strong>摘要：</strong>基于扩散的多模态大语言模型（扩散 MLLM）最近在视觉和语言任务中展示了令人印象深刻的非自回归生成能力。然而，扩散 MLLM 的推理速度比自回归模型慢得多：每个去噪步骤都在整个序列上采用完全双向自注意力，导致三次解码复杂性，对于数千个视觉标记来说，在计算上变得不切实际。为了应对这一挑战，我们提出了 D$^{3}$ToM，这是一种决策者引导的动态令牌合并方法，该方法可以在不同的去噪步骤中动态合并冗余视觉令牌，以加速扩散 MLLM 中的推理。在每个去噪步骤中，D$^{3}$ToM 使用决策标记（在前一个去噪步骤中生成的标记）来构建所有视觉标记的重要性图。然后，它维护一部分最显着的标记，并通过基于相似性的聚合来合并其余部分。该即插即用模块集成到单个变压器层中，在物理上缩短所有后续层的视觉令牌序列，而无需更改模型参数。此外，D$^{3}$ToM 采用随每个去噪步骤动态变化的合并比率，与 Diffusion MLLM 的本机解码过程保持一致，在同等计算预算下实现卓越的性能。大量实验表明，D$^{3}$ToM 可以加速推理，同时保持竞争性能。代码在此 https URL 发布。</li>
</ul>

<h3>Title: Rethinking Bias in Generative Data Augmentation for Medical AI: a Frequency Recalibration Method</h3>
<ul>
<li><strong>Authors: </strong>Chi Liu, Jincheng Liu, Congcong Zhu, Minghao Wang, Sheng Shen, Jia Gu, Tianqing Zhu, Wanlei Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12301">https://arxiv.org/abs/2511.12301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12301">https://arxiv.org/pdf/2511.12301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12301]] Rethinking Bias in Generative Data Augmentation for Medical AI: a Frequency Recalibration Method(https://arxiv.org/abs/2511.12301)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Developing Medical AI relies on large datasets and easily suffers from data scarcity. Generative data augmentation (GDA) using AI generative models offers a solution to synthesize realistic medical images. However, the bias in GDA is often underestimated in medical domains, with concerns about the risk of introducing detrimental features generated by AI and harming downstream tasks. This paper identifies the frequency misalignment between real and synthesized images as one of the key factors underlying unreliable GDA and proposes the Frequency Recalibration (FreRec) method to reduce the frequency distributional discrepancy and thus improve GDA. FreRec involves (1) Statistical High-frequency Replacement (SHR) to roughly align high-frequency components and (2) Reconstructive High-frequency Mapping (RHM) to enhance image quality and reconstruct high-frequency details. Extensive experiments were conducted in various medical datasets, including brain MRIs, chest X-rays, and fundus images. The results show that FreRec significantly improves downstream medical image classification performance compared to uncalibrated AI-synthesized samples. FreRec is a standalone post-processing step that is compatible with any generative model and can integrate seamlessly with common medical GDA pipelines.</li>
<li><strong>摘要：</strong>开发医疗人工智能依赖于大型数据集，很容易遭受数据稀缺的困扰。使用 AI 生成模型的生成数据增强 (GDA) 提供了合成真实医学图像的解决方案。然而，GDA 的偏差在医学领域常常被低估，人们担心引入人工智能生成的有害特征并损害下游任务的风险。本文将真实图像与合成图像之间的频率不一致视为 GDA 不可靠的关键因素之一，并提出频率重新校准（FreRec）方法来减少频率分布差异，从而提高 GDA。 FreRec 涉及 (1) 统计高频替换 (SHR) 来粗略对齐高频分量，以及 (2) 重建高频映射 (RHM) 来增强图像质量并重建高频细节。在各种医学数据集上进行了大量实验，包括脑部 MRI、胸部 X 光片和眼底图像。结果表明，与未校准的 AI 合成样本相比，FreRec 显着提高了下游医学图像分类性能。 FreRec 是一个独立的后处理步骤，与任何生成模型兼容，并且可以与常见的医疗 GDA 管道无缝集成。</li>
</ul>

<h3>Title: LiDAR-GS++:Improving LiDAR Gaussian Reconstruction via Diffusion Priors</h3>
<ul>
<li><strong>Authors: </strong>Qifeng Chen, Jiarun Liu, Rengan Xie, Tao Tang, Sicong Du, Yiru Zhao, Yuchi Huo, Sheng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12304">https://arxiv.org/abs/2511.12304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12304">https://arxiv.org/pdf/2511.12304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12304]] LiDAR-GS++:Improving LiDAR Gaussian Reconstruction via Diffusion Priors(https://arxiv.org/abs/2511.12304)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent GS-based rendering has made significant progress for LiDAR, surpassing Neural Radiance Fields (NeRF) in both quality and speed. However, these methods exhibit artifacts in extrapolated novel view synthesis due to the incomplete reconstruction from single traversal scans. To address this limitation, we present LiDAR-GS++, a LiDAR Gaussian Splatting reconstruction method enhanced by diffusion priors for real-time and high-fidelity re-simulation on public urban roads. Specifically, we introduce a controllable LiDAR generation model conditioned on coarsely extrapolated rendering to produce extra geometry-consistent scans and employ an effective distillation mechanism for expansive reconstruction. By extending reconstruction to under-fitted regions, our approach ensures global geometric consistency for extrapolative novel views while preserving detailed scene surfaces captured by sensors. Experiments on multiple public datasets demonstrate that LiDAR-GS++ achieves state-of-the-art performance for both interpolated and extrapolated viewpoints, surpassing existing GS and NeRF-based methods.</li>
<li><strong>摘要：</strong>最近，基于 GS 的渲染在 LiDAR 方面取得了重大进展，在质量和速度上都超越了神经辐射场 (NeRF)。然而，由于单次遍历扫描的重建不完整，这些方法在外推的新颖视图合成中表现出伪影。为了解决这一限制，我们提出了 LiDAR-GS++，这是一种通过扩散先验增强的 LiDAR 高斯溅射重建方法，用于在公共城市道路上进行实时和高保真重新模拟。具体来说，我们引入了一种以粗略外推渲染为条件的可控激光雷达生成模型，以产生额外的几何一致扫描，并采用有效的蒸馏机制进行扩展重建。通过将重建扩展到欠拟合区域，我们的方法确保了外推新颖视图的全局几何一致性，同时保留了传感器捕获的详细场景表面。对多个公共数据集的实验表明，LiDAR-GS++ 在插值和外推视点方面均实现了最先进的性能，超越了现有的基于 GS 和 NeRF 的方法。</li>
</ul>

<h3>Title: Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection: A VAE-Enhanced Reinforcement Learning Approach</h3>
<ul>
<li><strong>Authors: </strong>Bahareh Golchin, Banafsheh Rekabdar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12351">https://arxiv.org/abs/2511.12351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12351">https://arxiv.org/pdf/2511.12351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12351]] Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection: A VAE-Enhanced Reinforcement Learning Approach(https://arxiv.org/abs/2511.12351)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Detecting anomalies in multivariate time series is essential for monitoring complex industrial systems, where high dimensionality, limited labeled data, and subtle dependencies between sensors cause significant challenges. This paper presents a deep reinforcement learning framework that combines a Variational Autoencoder (VAE), an LSTM-based Deep Q-Network (DQN), dynamic reward shaping, and an active learning module to address these issues in a unified learning framework. The main contribution is the implementation of Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection (DRSMT), which demonstrates how each component enhances the detection process. The VAE captures compact latent representations and reduces noise. The DQN enables adaptive, sequential anomaly classification, and the dynamic reward shaping balances exploration and exploitation during training by adjusting the importance of reconstruction and classification signals. In addition, active learning identifies the most uncertain samples for labeling, reducing the need for extensive manual supervision. Experiments on two multivariate benchmarks, namely Server Machine Dataset (SMD) and Water Distribution Testbed (WADI), show that the proposed method outperforms existing baselines in F1-score and AU-PR. These results highlight the effectiveness of combining generative modeling, reinforcement learning, and selective supervision for accurate and scalable anomaly detection in real-world multivariate systems.</li>
<li><strong>摘要：</strong>检测多元时间序列中的异常对于监控复杂的工业系统至关重要，其中高维度、有限的标记数据以及传感器之间的微妙依赖性带来了重大挑战。本文提出了一种深度强化学习框架，该框架结合了变分自动编码器（VAE）、基于 LSTM 的深度 Q 网络（DQN）、动态奖励塑造和主动学习模块，以在统一的学习框架中解决这些问题。主要贡献是实现了多元时间序列异常检测（DRSMT）的动态奖励缩放，它演示了每个组件如何增强检测过程。 VAE 捕获紧凑的潜在表示并减少噪声。 DQN 支持自适应、顺序异常分类，动态奖励塑造通过调整重建和分类信号的重要性来平衡训练期间的探索和利用。此外，主动学习可以识别最不确定的样本进行标记，从而减少大量人工监督的需要。在两个多元基准（即服务器机器数据集（SMD）和配水测试台（WADI））上的实验表明，所提出的方法在 F1-score 和 AU-PR 方面优于现有基准。这些结果凸显了将生成建模、强化学习和选择性监督相结合以在现实世界的多变量系统中进行准确且可扩展的异常检测的有效性。</li>
</ul>

<h3>Title: Explainable AI-Generated Image Detection RewardBench</h3>
<ul>
<li><strong>Authors: </strong>Michael Yang, Shijian Deng, William T. Doan, Kai Wang, Tianyu Yang, Harsh Singh, Yapeng Tian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12363">https://arxiv.org/abs/2511.12363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12363">https://arxiv.org/pdf/2511.12363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12363]] Explainable AI-Generated Image Detection RewardBench(https://arxiv.org/abs/2511.12363)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Conventional, classification-based AI-generated image detection methods cannot explain why an image is considered real or AI-generated in a way a human expert would, which reduces the trustworthiness and persuasiveness of these detection tools for real-world applications. Leveraging Multimodal Large Language Models (MLLMs) has recently become a trending solution to this issue. Further, to evaluate the quality of generated explanations, a common approach is to adopt an "MLLM as a judge" methodology to evaluate explanations generated by other MLLMs. However, how well those MLLMs perform when judging explanations for AI-generated image detection generated by themselves or other MLLMs has not been well studied. We therefore propose \textbf{XAIGID-RewardBench}, the first benchmark designed to evaluate the ability of current MLLMs to judge the quality of explanations about whether an image is real or AI-generated. The benchmark consists of approximately 3,000 annotated triplets sourced from various image generation models and MLLMs as policy models (detectors) to assess the capabilities of current MLLMs as reward models (judges). Our results show that the current best reward model scored 88.76\% on this benchmark (while human inter-annotator agreement reaches 98.30\%), demonstrating that a visible gap remains between the reasoning abilities of today's MLLMs and human-level performance. In addition, we provide an analysis of common pitfalls that these models frequently encounter. Code and benchmark are available at this https URL.</li>
<li><strong>摘要：</strong>传统的基于分类的人工智能生成的图像检测方法无法解释为什么图像被认为是真实的或以人类专家的方式由人工智能生成，这降低了这些检测工具在现实世界应用中的可信度和说服力。利用多模态大型语言模型 (MLLM) 最近已成为解决此问题的趋势。此外，为了评估生成的解释的质量，常见的方法是采用“MLLM 作为法官”方法来评估其他 MLLM 生成的解释。然而，这些 MLLM 在判断其自身或其他 MLLM 生成的 AI 生成图像检测的解释时的表现如何，尚未得到充分研究。因此，我们提出 \textbf{XAIGID-RewardBench}，这是第一个基准测试，旨在评估当前 MLLM 判断图像是真实的还是人工智能生成的解释质量的能力。该基准由大约 3,000 个带注释的三元组组成，这些三元组来自各种图像生成模型和作为策略模型（检测器）的 MLLM，以评估当前 MLLM 作为奖励模型（法官）的能力。我们的结果表明，当前最佳奖励模型在此基准上得分为 88.76%（而人类注释者间一致性达到 98.30%），这表明当今 MLLM 的推理能力与人类水平的表现之间仍然存在明显的差距。此外，我们还对这些模型经常遇到的常见陷阱进行了分析。代码和基准测试可从此 https URL 获取。</li>
</ul>

<h3>Title: Seeing Through the Rain: Resolving High-Frequency Conflicts in Deraining and Super-Resolution via Diffusion Guidance</h3>
<ul>
<li><strong>Authors: </strong>Wenjie Li, Jinglei Shi, Jin Han, Heng Guo, Zhanyu Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12419">https://arxiv.org/abs/2511.12419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12419">https://arxiv.org/pdf/2511.12419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12419]] Seeing Through the Rain: Resolving High-Frequency Conflicts in Deraining and Super-Resolution via Diffusion Guidance(https://arxiv.org/abs/2511.12419)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, super-resolution</a></li>
<li><strong>Abstract: </strong>Clean images are crucial for visual tasks such as small object detection, especially at high resolutions. However, real-world images are often degraded by adverse weather, and weather restoration methods may sacrifice high-frequency details critical for analyzing small objects. A natural solution is to apply super-resolution (SR) after weather removal to recover both clarity and fine structures. However, simply cascading restoration and SR struggle to bridge their inherent conflict: removal aims to remove high-frequency weather-induced noise, while SR aims to hallucinate high-frequency textures from existing details, leading to inconsistent restoration contents. In this paper, we take deraining as a case study and propose DHGM, a Diffusion-based High-frequency Guided Model for generating clean and high-resolution images. DHGM integrates pre-trained diffusion priors with high-pass filters to simultaneously remove rain artifacts and enhance structural details. Extensive experiments demonstrate that DHGM achieves superior performance over existing methods, with lower costs.</li>
<li><strong>摘要：</strong>干净的图像对于小物体检测等视觉任务至关重要，尤其是在高分辨率下。然而，现实世界的图像通常会因恶劣天气而退化，并且天气恢复方法可能会牺牲对分析小物体至关重要的高频细节。一个自然的解决方案是在去除天气后应用超分辨率 (SR) 以恢复清晰度和精细结构。然而，简单的级联恢复和SR很难弥合它们固有的冲突：去除的目的是消除高频天气引起的噪声，而SR的目的是从现有细节中幻化出高频纹理，导致恢复内容不一致。在本文中，我们以除雨为案例研究，并提出 DHGM，一种基于扩散的高频引导模型，用于生成清晰的高分辨率图像。 DHGM 将预先训练的扩散先验与高通滤波器集成在一起，以同时消除雨水伪影并增强结构细节。大量实验表明，DHGM 比现有方法具有更优越的性能，并且成本更低。</li>
</ul>

<h3>Title: MFI-ResNet: Efficient ResNet Architecture Optimization via MeanFlow Compression and Selective Incubation</h3>
<ul>
<li><strong>Authors: </strong>Nuolin Sun, Linyuan Wang, Haonan Wei, Lei Li, Bin Yan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12422">https://arxiv.org/abs/2511.12422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12422">https://arxiv.org/pdf/2511.12422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12422]] MFI-ResNet: Efficient ResNet Architecture Optimization via MeanFlow Compression and Selective Incubation(https://arxiv.org/abs/2511.12422)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>ResNet has achieved tremendous success in computer vision through its residual connection mechanism. ResNet can be viewed as a discretized form of ordinary differential equations (ODEs). From this perspective, the multiple residual blocks within a single ResNet stage essentially perform multi-step discrete iterations of the feature transformation for that stage. The recently proposed flow matching model, MeanFlow, enables one-step generative modeling by learning the mean velocity field to transform distributions. Inspired by this, we propose MeanFlow-Incubated ResNet (MFI-ResNet), which employs a compression-expansion strategy to jointly improve parameter efficiency and discriminative performance. In the compression phase, we simplify the multi-layer structure within each ResNet stage to one or two MeanFlow modules to construct a lightweight meta model. In the expansion phase, we apply a selective incubation strategy to the first three stages, expanding them to match the residual block configuration of the baseline ResNet model, while keeping the last stage in MeanFlow form, and fine-tune the incubated model. Experimental results show that on CIFAR-10 and CIFAR-100 datasets, MFI-ResNet achieves remarkable parameter efficiency, reducing parameters by 46.28% and 45.59% compared to ResNet-50, while still improving accuracy by 0.23% and 0.17%, respectively. This demonstrates that generative flow-fields can effectively characterize the feature transformation process in ResNet, providing a new perspective for understanding the relationship between generative modeling and discriminative learning.</li>
<li><strong>摘要：</strong>ResNet通过其残差连接机制在计算机视觉领域取得了巨大的成功。 ResNet 可以被视为常微分方程 (ODE) 的离散形式。从这个角度来看，单个 ResNet 阶段中的多个残差块本质上执行该阶段的特征变换的多步离散迭代。最近提出的流匹配模型 MeanFlow 通过学习平均速度场来变换分布，从而实现一步生成建模。受此启发，我们提出了 MeanFlow-Incubated ResNet (MFI-ResNet)，它采用压缩扩展策略来共同提高参数效率和判别性能。在压缩阶段，我们将每个ResNet阶段内的多层结构简化为一两个MeanFlow模块，以构建轻量级元模型。在扩展阶段，我们对前三个阶段应用选择性孵化策略，扩展它们以匹配基线ResNet模型的残差块配置，同时保持最后一个阶段的MeanFlow形式，并对孵化模型进行微调。实验结果表明，在 CIFAR-10 和 CIFAR-100 数据集上，MFI-ResNet 取得了显着的参数效率，与 ResNet-50 相比，参数减少了 46.28% 和 45.59%，同时精度仍分别提高了 0.23% 和 0.17%。这表明生成流场可以有效地表征ResNet中的特征转换过程，为理解生成建模和判别学习之间的关系提供了新的视角。</li>
</ul>

<h3>Title: RedVTP: Training-Free Acceleration of Diffusion Vision-Language Models Inference via Masked Token-Guided Visual Token Pruning</h3>
<ul>
<li><strong>Authors: </strong>Jingqi Xu, Jingxi Lu, Chenghao Li, Sreetama Sarkar, Souvik Kundu, Peter A. Beerel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12428">https://arxiv.org/abs/2511.12428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12428">https://arxiv.org/pdf/2511.12428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12428]] RedVTP: Training-Free Acceleration of Diffusion Vision-Language Models Inference via Masked Token-Guided Visual Token Pruning(https://arxiv.org/abs/2511.12428)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) have achieved remarkable progress in multimodal reasoning and generation, yet their high computational demands remain a major challenge. Diffusion Vision-Language Models (DVLMs) are particularly attractive because they enable parallel token decoding, but the large number of visual tokens still significantly hinders their inference efficiency. While visual token pruning has been extensively studied for autoregressive VLMs (AVLMs), it remains largely unexplored for DVLMs. In this work, we propose RedVTP, a response-driven visual token pruning strategy that leverages the inference dynamics of DVLMs. Our method estimates visual token importance using attention from the masked response tokens. Based on the observation that these importance scores remain consistent across steps, RedVTP prunes the less important visual tokens from the masked tokens after the first inference step, thereby maximizing inference efficiency. Experiments show that RedVTP improves token generation throughput of LLaDA-V and LaViDa by up to 186% and 28.05%, respectively, and reduces inference latency by up to 64.97% and 21.87%, without compromising-and in some cases improving-accuracy.</li>
<li><strong>摘要：</strong>视觉语言模型（VLM）在多模态推理和生成方面取得了显着进展，但其高计算需求仍然是一个重大挑战。扩散视觉语言模型（DVLM）特别有吸引力，因为它们可以实现并行令牌解码，但大量的视觉令牌仍然严重阻碍了它们的推理效率。虽然视觉标记修剪已针对自回归 VLM (AVLM) 进行了广泛研究，但对于 DVLM 来说，它仍然很大程度上尚未得到探索。在这项工作中，我们提出了 RedVTP，一种响应驱动的视觉标记修剪策略，利用 DVLM 的推理动态。我们的方法使用来自屏蔽响应标记的注意力来估计视觉标记的重要性。基于这些重要性分数在各个步骤中保持一致的观察结果，RedVTP 在第一个推理步骤之后从屏蔽标记中修剪不太重要的视觉标记，从而最大限度地提高推理效率。实验表明，RedVTP 将 LLaDA-V 和 LaViDa 的令牌生成吞吐量分别提高了 186% 和 28.05%，并将推理延迟降低了 64.97% 和 21.87%，而不会影响（在某些情况下还提高）准确性。</li>
</ul>

<h3>Title: SculptDrug : A Spatial Condition-Aware Bayesian Flow Model for Structure-based Drug Design</h3>
<ul>
<li><strong>Authors: </strong>Qingsong Zhong, Haomin Yu, Yan Lin, Wangmeng Shen, Long Zeng, Jilin Hu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12489">https://arxiv.org/abs/2511.12489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12489">https://arxiv.org/pdf/2511.12489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12489]] SculptDrug : A Spatial Condition-Aware Bayesian Flow Model for Structure-based Drug Design(https://arxiv.org/abs/2511.12489)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Structure-Based drug design (SBDD) has emerged as a popular approach in drug discovery, leveraging three-dimensional protein structures to generate drug ligands. However, existing generative models encounter several key challenges: (1) incorporating boundary condition constraints, (2) integrating hierarchical structural conditions, and (3) ensuring spatial modeling fidelity. To address these limitations, we propose SculptDrug, a spatial condition-aware generative model based on Bayesian flow networks (BFNs). First, SculptDrug follows a BFN-based framework and employs a progressive denoising strategy to ensure spatial modeling fidelity, iteratively refining atom positions while enhancing local interactions for precise spatial alignment. Second, we introduce a Boundary Awareness Block that incorporates protein surface constraints into the generative process to ensure that generated ligands are geometrically compatible with the target protein. Third, we design a Hierarchical Encoder that captures global structural context while preserving fine-grained molecular interactions, ensuring overall consistency and accurate ligand-protein conformations. We evaluate SculptDrug on the CrossDocked dataset, and experimental results demonstrate that SculptDrug outperforms state-of-the-art baselines, highlighting the effectiveness of spatial condition-aware modeling.</li>
<li><strong>摘要：</strong>基于结构的药物设计（SBDD）已成为药物发现中的一种流行方法，利用三维蛋白质结构来生成药物配体。然而，现有的生成模型遇到了几个关键挑战：（1）合并边界条件约束，（2）集成分层结构条件，以及（3）确保空间建模保真度。为了解决这些限制，我们提出了 SculptDrug，一种基于贝叶斯流网络（BFN）的空间条件感知生成模型。首先，SculptDrug 遵循基于 BFN 的框架，并采用渐进式去噪策略来确保空间建模保真度，迭代地细化原子位置，同时增强局部相互作用以实现精确的空间对齐。其次，我们引入了边界感知块，它将蛋白质表面约束纳入生成过程中，以确保生成的配体与目标蛋白质在几何上兼容。第三，我们设计了一个分层编码器，可以捕获全局结构上下文，同时保留细粒度的分子相互作用，确保整体一致性和准确的配体-蛋白质构象。我们在 CrossDocked 数据集上评估 SculptDrug，实验结果表明 SculptDrug 优于最先进的基线，凸显了空间条件感知建模的有效性。</li>
</ul>

<h3>Title: HiGFA: Hierarchical Guidance for Fine-grained Data Augmentation with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Zhiguang Lu, Qianqian Xu, Peisong Wen, Siran Da, Qingming Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12547">https://arxiv.org/abs/2511.12547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12547">https://arxiv.org/pdf/2511.12547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12547]] HiGFA: Hierarchical Guidance for Fine-grained Data Augmentation with Diffusion Models(https://arxiv.org/abs/2511.12547)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative diffusion models show promise for data augmentation. However, applying them to fine-grained tasks presents a significant challenge: ensuring synthetic images accurately capture the subtle, category-defining features critical for high fidelity. Standard approaches, such as text-based Classifier-Free Guidance (CFG), often lack the required specificity, potentially generating misleading examples that degrade fine-grained classifier performance. To address this, we propose Hierarchically Guided Fine-grained Augmentation (HiGFA). HiGFA leverages the temporal dynamics of the diffusion sampling process. It employs strong text and transformed contour guidance with fixed strengths in the early-to-mid sampling stages to establish overall scene, style, and structure. In the final sampling stages, HiGFA activates a specialized fine-grained classifier guidance and dynamically modulates the strength of all guidance signals based on prediction confidence. This hierarchical, confidence-driven orchestration enables HiGFA to generate diverse yet faithful synthetic images by intelligently balancing global structure formation with precise detail refinement. Experiments on several FGVC datasets demonstrate the effectiveness of HiGFA.</li>
<li><strong>摘要：</strong>生成扩散模型显示出数据增强的前景。然而，将它们应用于细粒度任务提出了重大挑战：确保合成图像准确捕捉对高保真度至关重要的微妙的、类别定义的特征。标准方法，例如基于文本的无分类器指导 (CFG)，通常缺乏所需的特异性，可能会产生误导性示例，从而降低细粒度分类器的性能。为了解决这个问题，我们提出了分层引导细粒度增强（HiGFA）。 HiGFA 利用扩散采样过程的时间动态。它在前中期采样阶段采用具有固定强度的强文本和转换轮廓引导来建立整体场景、风格和结构。在最后的采样阶段，HiGFA 激活专门的细粒度分类器指导，并根据预测置信度动态调节所有指导信号的强度。这种分层的、信心驱动的编排使 HiGFA 能够通过智能地平衡全局结构形成与精确的细节细化来生成多样化但忠实的合成图像。在多个 FGVC 数据集上的实验证明了 HiGFA 的有效性。</li>
</ul>

<h3>Title: TempoMaster: Efficient Long Video Generation via Next-Frame-Rate Prediction</h3>
<ul>
<li><strong>Authors: </strong>Yukuo Ma, Cong Liu, Junke Wang, Junqi Liu, Haibin Huang, Zuxuan Wu, Chi Zhang, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12578">https://arxiv.org/abs/2511.12578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12578">https://arxiv.org/pdf/2511.12578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12578]] TempoMaster: Efficient Long Video Generation via Next-Frame-Rate Prediction(https://arxiv.org/abs/2511.12578)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present TempoMaster, a novel framework that formulates long video generation as next-frame-rate prediction. Specifically, we first generate a low-frame-rate clip that serves as a coarse blueprint of the entire video sequence, and then progressively increase the frame rate to refine visual details and motion continuity. During generation, TempoMaster employs bidirectional attention within each frame-rate level while performing autoregression across frame rates, thus achieving long-range temporal coherence while enabling efficient and parallel synthesis. Extensive experiments demonstrate that TempoMaster establishes a new state-of-the-art in long video generation, excelling in both visual and temporal quality.</li>
<li><strong>摘要：</strong>我们提出了 TempoMaster，这是一种新颖的框架，它将长视频生成制定为下一帧速率预测。具体来说，我们首先生成一个低帧速率剪辑，作为整个视频序列的粗略蓝图，然后逐步提高帧速率以细化视觉细节和运动连续性。在生成过程中，TempoMaster 在每个帧速率级别内采用双向注意力，同时执行跨帧速率的自回归，从而实现远程时间一致性，同时实现高效并行合成。大量实验表明，TempoMaster 在长视频生成方面建立了新的最先进技术，在视觉和时间质量方面均表现出色。</li>
</ul>

<h3>Title: Seg-VAR: Image Segmentation with Visual Autoregressive Modeling</h3>
<ul>
<li><strong>Authors: </strong>Rongkun Zheng, Lu Qi, Xi Chen, Yi Wang, Kun Wang, Hengshuang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12594">https://arxiv.org/abs/2511.12594</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12594">https://arxiv.org/pdf/2511.12594</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12594]] Seg-VAR: Image Segmentation with Visual Autoregressive Modeling(https://arxiv.org/abs/2511.12594)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>While visual autoregressive modeling (VAR) strategies have shed light on image generation with the autoregressive models, their potential for segmentation, a task that requires precise low-level spatial perception, remains unexplored. Inspired by the multi-scale modeling of classic Mask2Former-based models, we propose Seg-VAR, a novel framework that rethinks segmentation as a conditional autoregressive mask generation problem. This is achieved by replacing the discriminative learning with the latent learning process. Specifically, our method incorporates three core components: (1) an image encoder generating latent priors from input images, (2) a spatial-aware seglat (a latent expression of segmentation mask) encoder that maps segmentation masks into discrete latent tokens using a location-sensitive color mapping to distinguish instances, and (3) a decoder reconstructing masks from these latents. A multi-stage training strategy is introduced: first learning seglat representations via image-seglat joint training, then refining latent transformations, and finally aligning image-encoder-derived latents with seglat distributions. Experiments show Seg-VAR outperforms previous discriminative and generative methods on various segmentation tasks and validation benchmarks. By framing segmentation as a sequential hierarchical prediction task, Seg-VAR opens new avenues for integrating autoregressive reasoning into spatial-aware vision systems. Code will be available at this https URL.</li>
<li><strong>摘要：</strong>虽然视觉自回归建模（VAR）策略已经揭示了使用自回归模型生成图像，但它们的分割潜力（一项需要精确的低级空间感知的任务）仍有待探索。受到基于 Mask2Former 的经典模型的多尺度建模的启发，我们提出了 Seg-VAR，这是一种新颖的框架，它将分割重新考虑为条件自回归掩模生成问题。这是通过用潜在学习过程代替判别性学习来实现的。具体来说，我们的方法包含三个核心组件：（1）从输入图像生成潜在先验的图像编码器，（2）空间感知的 seglat（分割掩码的潜在表达）编码器，使用位置敏感的颜色映射将分割掩码映射到离散的潜在标记中以区分实例，以及（3）从这些潜在中重建掩码的解码器。引入了多阶段训练策略：首先通过图像-seglat联合训练学习seglat表示，然后细化潜在变换，最后将图像编码器导出的潜在与seglat分布对齐。实验表明，Seg-VAR 在各种分割任务和验证基准上优于以前的判别和生成方法。通过将分割构建为顺序分层预测任务，Seg-VAR 为将自回归推理集成到空间感知视觉系统中开辟了新途径。代码将在此 https URL 中提供。</li>
</ul>

<h3>Title: LoRA-Enhanced Vision Transformer for Single Image based Morphing Attack Detection via Knowledge Distillation from EfficientNet</h3>
<ul>
<li><strong>Authors: </strong>Ria Shekhawat, Sushrut Patwardhan, Raghavendra Ramachandra, Praveen Kumar Chandaliya, Kishor P. Upla</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12602">https://arxiv.org/abs/2511.12602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12602">https://arxiv.org/pdf/2511.12602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12602]] LoRA-Enhanced Vision Transformer for Single Image based Morphing Attack Detection via Knowledge Distillation from EfficientNet(https://arxiv.org/abs/2511.12602)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Face Recognition Systems (FRS) are critical for security but remain vulnerable to morphing attacks, where synthetic images blend biometric features from multiple individuals. We propose a novel Single-Image Morphing Attack Detection (S-MAD) approach using a teacher-student framework, where a CNN-based teacher model refines a ViT-based student model. To improve efficiency, we integrate Low-Rank Adaptation (LoRA) for fine-tuning, reducing computational costs while maintaining high detection accuracy. Extensive experiments are conducted on a morphing dataset built from three publicly available face datasets, incorporating ten different morphing generation algorithms to assess robustness. The proposed method is benchmarked against six state-of-the-art S-MAD techniques, demonstrating superior detection performance and computational efficiency.</li>
<li><strong>摘要：</strong>人脸识别系统 (FRS) 对于安全至关重要，但仍然容易受到变形攻击，其中合成图像融合了多个个体的生物识别特征。我们提出了一种使用教师-学生框架的新颖的单图像变形攻击检测（S-MAD）方法，其中基于 CNN 的教师模型改进了基于 ViT 的学生模型。为了提高效率，我们集成了低秩适应（LoRA）进行微调，降低计算成本，同时保持高检测精度。在由三个公开可用的人脸数据集构建的变形数据集上进行了广泛的实验，结合了十种不同的变形生成算法来评估鲁棒性。所提出的方法以六种最先进的 S-MAD 技术为基准，展示了卓越的检测性能和计算效率。</li>
</ul>

<h3>Title: PID-controlled Langevin Dynamics for Faster Sampling of Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Hongyi Chen, Jianhai Shu, Jingtao Ding, Yong Li, Xiao-Ping Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12603">https://arxiv.org/abs/2511.12603</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12603">https://arxiv.org/pdf/2511.12603</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12603]] PID-controlled Langevin Dynamics for Faster Sampling of Generative Models(https://arxiv.org/abs/2511.12603)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Langevin dynamics sampling suffers from extremely low generation speed, fundamentally limited by numerous fine-grained iterations to converge to the target distribution. We introduce PID-controlled Langevin Dynamics (PIDLD), a novel sampling acceleration algorithm that reinterprets the sampling process using control-theoretic principles. By treating energy gradients as feedback signals, PIDLD combines historical gradients (the integral term) and gradient trends (the derivative term) to efficiently traverse energy landscapes and adaptively stabilize, thereby significantly reducing the number of iterations required to produce high-quality samples. Our approach requires no additional training, datasets, or prior information, making it immediately integrable with any Langevin-based method. Extensive experiments across image generation and reasoning tasks demonstrate that PIDLD achieves higher quality with fewer steps, making Langevin-based generative models more practical for efficiency-critical applications. The implementation can be found at \href{this https URL}{this https URL}.</li>
<li><strong>摘要：</strong>Langevin 动态采样的生成速度极低，从根本上限制了收敛到目标分布的大量细粒度迭代。我们引入 PID 控制的 Langevin Dynamics (PIDLD)，这是一种新颖的采样加速算法，它使用控制理论原理重新解释采样过程。通过将能量梯度视为反馈信号，PIDLD 结合历史梯度（积分项）和梯度趋势（导数项）来有效遍历能量景观并自适应稳定，从而显着减少生成高质量样本所需的迭代次数。我们的方法不需要额外的训练、数据集或先验信息，使其可以立即与任何基于 Langevin 的方法集成。跨图像生成和推理任务的大量实验表明，PIDLD 用更少的步骤实现了更高的质量，使得基于 Langevin 的生成模型对于效率关键型应用更加实用。可以在 \href{此 https URL}{此 https URL} 中找到该实现。</li>
</ul>

<h3>Title: Multivariate Diffusion Transformer with Decoupled Attention for High-Fidelity Mask-Text Collaborative Facial Generation</h3>
<ul>
<li><strong>Authors: </strong>Yushe Cao, Dianxi Shi, Xing Fu, Xuechao Zou, Haikuo Peng, Xueqi Li, Chun Yu, Junliang Xing</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12631">https://arxiv.org/abs/2511.12631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12631">https://arxiv.org/pdf/2511.12631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12631]] Multivariate Diffusion Transformer with Decoupled Attention for High-Fidelity Mask-Text Collaborative Facial Generation(https://arxiv.org/abs/2511.12631)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While significant progress has been achieved in multimodal facial generation using semantic masks and textual descriptions, conventional feature fusion approaches often fail to enable effective cross-modal interactions, thereby leading to suboptimal generation outcomes. To address this challenge, we introduce MDiTFace--a customized diffusion transformer framework that employs a unified tokenization strategy to process semantic mask and text inputs, eliminating discrepancies between heterogeneous modality representations. The framework facilitates comprehensive multimodal feature interaction through stacked, newly designed multivariate transformer blocks that process all conditions synchronously. Additionally, we design a novel decoupled attention mechanism by dissociating implicit dependencies between mask tokens and temporal embeddings. This mechanism segregates internal computations into dynamic and static pathways, enabling caching and reuse of features computed in static pathways after initial calculation, thereby reducing additional computational overhead introduced by mask condition by over 94% while maintaining performance. Extensive experiments demonstrate that MDiTFace significantly outperforms other competing methods in terms of both facial fidelity and conditional consistency.</li>
<li><strong>摘要：</strong>虽然使用语义掩模和文本描述在多模态面部生成方面取得了重大进展，但传统的特征融合方法通常无法实现有效的跨模态交互，从而导致生成结果不理想。为了应对这一挑战，我们引入了 MDiTFace——一种定制的扩散转换器框架，它采用统一的标记化策略来处理语义掩码和文本输入，消除异构模态表示之间的差异。该框架通过堆叠的、新设计的多元变压器块同步处理所有条件，促进全面的多模式特征交互。此外，我们通过分离掩码标记和时间嵌入之间的隐式依赖关系，设计了一种新颖的解耦注意力机制。该机制将内部计算分为动态和静态路径，从而能够在初始计算后缓存和重用静态路径中计算的特征，从而在保持性能的同时将掩码条件引入的额外计算开销减少超过 94%。大量实验表明，MDiTFace 在面部保真度和条件一致性方面显着优于其他竞争方法。</li>
</ul>

<h3>Title: Denoising Vision Transformer Autoencoder with Spectral Self-Regularization</h3>
<ul>
<li><strong>Authors: </strong>Xunzhi Xiang, Xingye Tian, Guiyu Zhang, Yabo Chen, Shaofeng Zhang, Xuebo Wang, Xin Tao, Qi Fan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12633">https://arxiv.org/abs/2511.12633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12633">https://arxiv.org/pdf/2511.12633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12633]] Denoising Vision Transformer Autoencoder with Spectral Self-Regularization(https://arxiv.org/abs/2511.12633)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Variational autoencoders (VAEs) typically encode images into a compact latent space, reducing computational cost but introducing an optimization dilemma: a higher-dimensional latent space improves reconstruction fidelity but often hampers generative performance. Recent methods attempt to address this dilemma by regularizing high-dimensional latent spaces using external vision foundation models (VFMs). However, it remains unclear how high-dimensional VAE latents affect the optimization of generative models. To our knowledge, our analysis is the first to reveal that redundant high-frequency components in high-dimensional latent spaces hinder the training convergence of diffusion models and, consequently, degrade generation quality. To alleviate this problem, we propose a spectral self-regularization strategy to suppress redundant high-frequency noise while simultaneously preserving reconstruction quality. The resulting Denoising-VAE, a ViT-based autoencoder that does not rely on VFMs, produces cleaner, lower-noise latents, leading to improved generative quality and faster optimization convergence. We further introduce a spectral alignment strategy to facilitate the optimization of Denoising-VAE-based generative models. Our complete method enables diffusion models to converge approximately 2$\times$ faster than with SD-VAE, while achieving state-of-the-art reconstruction quality (rFID = 0.28, PSNR = 27.26) and competitive generation performance (gFID = 1.82) on the ImageNet 256$\times$256 benchmark.</li>
<li><strong>摘要：</strong>变分自动编码器（VAE）通常将图像编码到紧凑的潜在空间中，降低了计算成本，但引入了优化困境：更高维的潜在空间提高了重建保真度，但通常会妨碍生成性能。最近的方法试图通过使用外部视觉基础模型（VFM）规范高维潜在空间来解决这一困境。然而，目前尚不清楚高维 VAE 潜伏如何影响生成模型的优化。据我们所知，我们的分析首次揭示了高维潜在空间中的冗余高频分量阻碍了扩散模型的训练收敛，从而降低了生成质量。为了缓解这个问题，我们提出了一种谱自正则化策略来抑制冗余高频噪声，同时保持重建质量。由此产生的 Denoising-VAE 是一种基于 ViT 的自动编码器，不依赖于 VFM，可产生更干净、噪声更低的潜在特征，从而提高生成质量并加快优化收敛速度。我们进一步引入了光谱对齐策略，以促进基于 Denoising-VAE 的生成模型的优化。我们的完整方法使扩散模型的收敛速度比 SD-VAE 快约 2 美元\倍$，同时在 ImageNet 256$\times$256 基准上实现最先进的重建质量（rFID = 0.28，PSNR = 27.26）和有竞争力的生成性能（gFID = 1.82）。</li>
</ul>

<h3>Title: Hi-Reco: High-Fidelity Real-Time Conversational Digital Humans</h3>
<ul>
<li><strong>Authors: </strong>Hongbin Huang, Junwei Li, Tianxin Xie, Zhuang Li, Cekai Weng, Yaodong Yang, Yue Luo, Li Liu, Jing Tang, Zhijing Shao, Zeyu Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12662">https://arxiv.org/abs/2511.12662</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12662">https://arxiv.org/pdf/2511.12662</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12662]] Hi-Reco: High-Fidelity Real-Time Conversational Digital Humans(https://arxiv.org/abs/2511.12662)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>High-fidelity digital humans are increasingly used in interactive applications, yet achieving both visual realism and real-time responsiveness remains a major challenge. We present a high-fidelity, real-time conversational digital human system that seamlessly combines a visually realistic 3D avatar, persona-driven expressive speech synthesis, and knowledge-grounded dialogue generation. To support natural and timely interaction, we introduce an asynchronous execution pipeline that coordinates multi-modal components with minimal latency. The system supports advanced features such as wake word detection, emotionally expressive prosody, and highly accurate, context-aware response generation. It leverages novel retrieval-augmented methods, including history augmentation to maintain conversational flow and intent-based routing for efficient knowledge access. Together, these components form an integrated system that enables responsive and believable digital humans, suitable for immersive applications in communication, education, and entertainment.</li>
<li><strong>摘要：</strong>高保真数字人越来越多地用于交互式应用程序，但同时实现视觉真实感和实时响应能力仍然是一个重大挑战。我们提出了一个高保真、实时对话式数字人类系统，它无缝地结合了视觉逼真的 3D 化身、角色驱动的表达性语音合成和基于知识的对话生成。为了支持自然和及时的交互，我们引入了异步执行管道，以最小的延迟协调多模式组件。该系统支持高级功能，例如唤醒词检测、情感表达韵律以及高度准确的上下文感知响应生成。它利用新颖的检索增强方法，包括历史增强来维持对话流和基于意图的路由以实现高效的知识访问。这些组件共同构成了一个集成系统，可实现响应灵敏且可信的数字人类，适用于通信、教育和娱乐领域的沉浸式应用。</li>
</ul>

<h3>Title: DensePercept-NCSSD: Vision Mamba towards Real-time Dense Visual Perception with Non-Causal State Space Duality</h3>
<ul>
<li><strong>Authors: </strong>Tushar Anand, Advik Sinha, Abhijit Das</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12671">https://arxiv.org/abs/2511.12671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12671">https://arxiv.org/pdf/2511.12671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12671]] DensePercept-NCSSD: Vision Mamba towards Real-time Dense Visual Perception with Non-Causal State Space Duality(https://arxiv.org/abs/2511.12671)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In this work, we propose an accurate and real-time optical flow and disparity estimation model by fusing pairwise input images in the proposed non-causal selective state space for dense perception tasks. We propose a non-causal Mamba block-based model that is fast and efficient and aptly manages the constraints present in a real-time applications. Our proposed model reduces inference times while maintaining high accuracy and low GPU usage for optical flow and disparity map generation. The results and analysis, and validation in real-life scenario justify that our proposed model can be used for unified real-time and accurate 3D dense perception estimation tasks. The code, along with the models, can be found at this https URL</li>
<li><strong>摘要：</strong>在这项工作中，我们通过在所提出的用于密集感知任务的非因果选择性状态空间中融合成对输入图像，提出了一种准确实时的光流和视差估计模型。我们提出了一种基于非因果 Mamba 块的模型，该模型快速高效，并且能够恰当地管理实时应用程序中存在的约束。我们提出的模型减少了推理时间，同时保持光流和视差图生成的高精度和低 GPU 使用率。结果和分析以及现实场景中的验证证明我们提出的模型可以用于统一的实时且准确的 3D 密集感知估计任务。代码以及模型可以在此 https URL 中找到</li>
</ul>

<h3>Title: Appreciate the View: A Task-Aware Evaluation Framework for Novel View Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Saar Stern, Ido Sobol, Or Litany</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12675">https://arxiv.org/abs/2511.12675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12675">https://arxiv.org/pdf/2511.12675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12675]] Appreciate the View: A Task-Aware Evaluation Framework for Novel View Synthesis(https://arxiv.org/abs/2511.12675)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The goal of Novel View Synthesis (NVS) is to generate realistic images of a given content from unseen viewpoints. But how can we trust that a generated image truly reflects the intended transformation? Evaluating its reliability remains a major challenge. While recent generative models, particularly diffusion-based approaches, have significantly improved NVS quality, existing evaluation metrics struggle to assess whether a generated image is both realistic and faithful to the source view and intended viewpoint transformation. Standard metrics, such as pixel-wise similarity and distribution-based measures, often mis-rank incorrect results as they fail to capture the nuanced relationship between the source image, viewpoint change, and generated output. We propose a task-aware evaluation framework that leverages features from a strong NVS foundation model, Zero123, combined with a lightweight tuning step to enhance discrimination. Using these features, we introduce two complementary evaluation metrics: a reference-based score, $D_{\text{PRISM}}$, and a reference-free score, $\text{MMD}_{\text{PRISM}}$. Both reliably identify incorrect generations and rank models in agreement with human preference studies, addressing a fundamental gap in NVS evaluation. Our framework provides a principled and practical approach to assessing synthesis quality, paving the way for more reliable progress in novel view synthesis. To further support this goal, we apply our reference-free metric to six NVS methods across three benchmarks: Toys4K, Google Scanned Objects (GSO), and OmniObject3D, where $\text{MMD}_{\text{PRISM}}$ produces a clear and stable ranking, with lower scores consistently indicating stronger models.</li>
<li><strong>摘要：</strong>新视图合成 (NVS) 的目标是从看不见的视点生成给定内容的真实图像。但我们如何才能相信生成的图像真正反映了预期的转换呢？评估其可靠性仍然是一个重大挑战。虽然最近的生成模型，特别是基于扩散的方法，显着提高了 NVS 质量，但现有的评估指标很难评估生成的图像是否真实且忠实于源视图和预期的视点转换。标准指标（例如像素相似性和基于分布的测量）经常会对不正确的结果进行错误排序，因为它们无法捕获源图像、视点变化和生成的输出之间的细微关系。我们提出了一个任务感知评估框架，该框架利用强大的 NVS 基础模型 Zero123 的功能，并结合轻量级调整步骤来增强辨别力。利用这些功能，我们引入了两个互补的评估指标：基于参考的分数 $D_{\text{PRISM}}$ 和无参考分数 $\text{MMD}_{\text{PRISM}}$。两者都能可靠地识别不正确的世代，并对模型进行排名，与人类偏好研究一致，解决了 NVS 评估中的根本差距。我们的框架提供了一种原则性且实用的方法来评估合成质量，为新颖视图合成的更可靠进展铺平了道路。为了进一步支持这一目标，我们将无参考指标应用于三个基准测试中的六种 NVS 方法：Toys4K、Google Scanned Objects (GSO) 和 OmniObject3D，其中 $\text{MMD}_{\text{PRISM}}$ 产生清晰且稳定的排名，较低的分数始终表明模型更强。</li>
</ul>

<h3>Title: FSDAM: Few-Shot Driving Attention Modeling via Vision-Language Coupling</h3>
<ul>
<li><strong>Authors: </strong>Kaiser Hamid, Can Cui, Khandakar Ashrafi Akbar, Ziran Wang, Nade Liang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12708">https://arxiv.org/abs/2511.12708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12708">https://arxiv.org/pdf/2511.12708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12708]] FSDAM: Few-Shot Driving Attention Modeling via Vision-Language Coupling(https://arxiv.org/abs/2511.12708)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Understanding where drivers look and why they shift their attention is essential for autonomous systems that read human intent and justify their actions. Most existing models rely on large-scale gaze datasets to learn these patterns; however, such datasets are labor-intensive to collect and time-consuming to curate. We present FSDAM (Few-Shot Driver Attention Modeling), a framework that achieves joint attention prediction and caption generation with approximately 100 annotated examples, two orders of magnitude fewer than existing approaches. Our approach introduces a dual-pathway architecture where separate modules handle spatial prediction and caption generation while maintaining semantic consistency through cross-modal alignment. Despite minimal supervision, FSDAM achieves competitive performance on attention prediction, generates coherent, and context-aware explanations. The model demonstrates robust zero-shot generalization across multiple driving benchmarks. This work shows that effective attention-conditioned generation is achievable with limited supervision, opening new possibilities for practical deployment of explainable driver attention systems in data-constrained scenarios.</li>
<li><strong>摘要：</strong>了解驾驶员的视线以及他们转移注意力的原因对于自动驾驶系统来说至关重要，因为自动驾驶系统能够读取人类的意图并证明其行为的合理性。大多数现有模型依赖大规模注视数据集来学习这些模式；然而，此类数据集的收集需要大量劳动力，并且管理起来也非常耗时。我们提出了 FSDAM（Few-Shot Driver Attention Modeling），这是一个框架，可通过大约 100 个带注释的示例实现联合注意力预测和标题生成，比现有方法少两个数量级。我们的方法引入了双路径架构，其中单独的模块处理空间预测和标题生成，同时通过跨模式对齐保持语义一致性。尽管监督很少，FSDAM 在注意力预测方面取得了有竞争力的表现，生成了连贯且上下文感知的解释。该模型在多个驾驶基准中展示了强大的零样本泛化能力。这项工作表明，在有限的监督下可以实现有效的注意力调节生成，为在数据受限的场景中实际部署可解释的驾驶员注意力系统开辟了新的可能性。</li>
</ul>

<h3>Title: Deep Imbalanced Multi-Target Regression: 3D Point Cloud Voxel Content Estimation in Simulated Forests</h3>
<ul>
<li><strong>Authors: </strong>Amirhossein Hassanzadeh, Bartosz Krawczyk, Michael Saunders, Rob Wible, Keith Krause, Dimah Dera, Jan van Aardt</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12740">https://arxiv.org/abs/2511.12740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12740">https://arxiv.org/pdf/2511.12740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12740]] Deep Imbalanced Multi-Target Regression: 3D Point Cloud Voxel Content Estimation in Simulated Forests(https://arxiv.org/abs/2511.12740)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Voxelization is an effective approach to reduce the computational cost of processing Light Detection and Ranging (LiDAR) data, yet it results in a loss of fine-scale structural information. This study explores whether low-level voxel content information, specifically target occupancy percentage within a voxel, can be inferred from high-level voxelized LiDAR point cloud data collected from Digital Imaging and remote Sensing Image Generation (DIRSIG) software. In our study, the targets include bark, leaf, soil, and miscellaneous materials. We propose a multi-target regression approach in the context of imbalanced learning using Kernel Point Convolutions (KPConv). Our research leverages cost-sensitive learning to address class imbalance called density-based relevance (DBR). We employ weighted Mean Saquared Erorr (MSE), Focal Regression (FocalR), and regularization to improve the optimization of KPConv. This study performs a sensitivity analysis on the voxel size (0.25 - 2 meters) to evaluate the effect of various grid representations in capturing the nuances of the forest. This sensitivity analysis reveals that larger voxel sizes (e.g., 2 meters) result in lower errors due to reduced variability, while smaller voxel sizes (e.g., 0.25 or 0.5 meter) exhibit higher errors, particularly within the canopy, where variability is greatest. For bark and leaf targets, error values at smaller voxel size datasets (0.25 and 0.5 meter) were significantly higher than those in larger voxel size datasets (2 meters), highlighting the difficulty in accurately estimating within-canopy voxel content at fine resolutions. This suggests that the choice of voxel size is application-dependent. Our work fills the gap in deep imbalance learning models for multi-target regression and simulated datasets for 3D LiDAR point clouds of forests.</li>
<li><strong>摘要：</strong>体素化是降低处理光探测和测距 (LiDAR) 数据的计算成本的有效方法，但它会导致精细结构信息的丢失。本研究探讨是否可以从数字成像和遥感图像生成 (DIRSIG) 软件收集的高级体素化 LiDAR 点云数据推断出低级体素内容信息，特别是体素内的目标占用百分比。在我们的研究中，目标包括树皮、叶子、土壤和其他材料。我们使用核点卷积（KPConv）在不平衡学习的背景下提出了一种多目标回归方法。我们的研究利用成本敏感的学习来解决称为基于密度的相关性 (DBR) 的类别不平衡问题。我们采用加权均方误差（MSE）、焦点回归（FocalR）和正则化来改进 KPConv 的优化。本研究对体素大小（0.25 - 2 米）进行敏感性分析，以评估各种网格表示在捕捉森林细微差别方面的效果。这种敏感性分析表明，较大的体素尺寸（例如 2 米）由于变异性降低而导致较低的误差，而较小的体素尺寸（例如 0.25 或 0.5 米）则表现出较高的误差，特别是在变异性最大的树冠内。对于树皮和叶子目标，较小体素尺寸数据集（0.25 和 0.5 米）的误差值明显高于较大体素尺寸数据集（2 米）的误差值，凸显了在高分辨率下准确估计树冠内体素内容的困难。这表明体素大小的选择取决于应用。我们的工作填补了多目标回归深度不平衡学习模型和森林 3D LiDAR 点云模拟数据集的空白。</li>
</ul>

<h3>Title: Stabilizing Self-Consuming Diffusion Models with Latent Space Filtering</h3>
<ul>
<li><strong>Authors: </strong>Zhongteng Cai, Yaxuan Wang, Yang Liu, Xueru Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12742">https://arxiv.org/abs/2511.12742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12742">https://arxiv.org/pdf/2511.12742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12742]] Stabilizing Self-Consuming Diffusion Models with Latent Space Filtering(https://arxiv.org/abs/2511.12742)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>As synthetic data proliferates across the Internet, it is often reused to train successive generations of generative models. This creates a ``self-consuming loop" that can lead to training instability or \textit{model collapse}. Common strategies to address the issue -- such as accumulating historical training data or injecting fresh real data -- either increase computational cost or require expensive human annotation. In this paper, we empirically analyze the latent space dynamics of self-consuming diffusion models and observe that the low-dimensional structure of latent representations extracted from synthetic data degrade over generations. Based on this insight, we propose \textit{Latent Space Filtering} (LSF), a novel approach that mitigates model collapse by filtering out less realistic synthetic data from mixed datasets. Theoretically, we present a framework that connects latent space degradation to empirical observations. Experimentally, we show that LSF consistently outperforms existing baselines across multiple real-world datasets, effectively mitigating model collapse without increasing training cost or relying on human annotation.</li>
<li><strong>摘要：</strong>随着合成数据在互联网上激增，它经常被重复用于训练连续几代的生成模型。这会产生一个“自消耗循环”，可能导致训练不稳定或\textit{模型崩溃}。解决该问题的常见策略——例如积累历史训练数据或注入新鲜的真实数据——要么会增加计算成本，要么需要昂贵的人工注释。在本文中，我们实证分析了自消耗扩散模型的潜在空间动态，并观察到从合成数据中提取的潜在表示的低维结构会随着世代的推移而退化。基于这一见解，我们提出\textit{潜在空间过滤} (LSF)，一种通过从混合数据集中过滤掉不太现实的合成数据来减轻模型崩溃的新方法。理论上，我们提出了一个将潜在空间退化与经验观察联系起来的框架。实验表明，LSF 在多个真实数据集中始终优于现有基线，有效地减轻了模型崩溃，而无需增加训练成本或依赖人工注释。</li>
</ul>

<h3>Title: DIVIDE: A Framework for Learning from Independent Multi-Mechanism Data Using Deep Encoders and Gaussian Processes</h3>
<ul>
<li><strong>Authors: </strong>Vivek Chawla, Boris Slautin, Utkarsh Pratiush, Dayakar Penumadu, Sergei Kalinin</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12745">https://arxiv.org/abs/2511.12745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12745">https://arxiv.org/pdf/2511.12745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12745]] DIVIDE: A Framework for Learning from Independent Multi-Mechanism Data Using Deep Encoders and Gaussian Processes(https://arxiv.org/abs/2511.12745)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Scientific datasets often arise from multiple independent mechanisms such as spatial, categorical or structural effects, whose combined influence obscures their individual contributions. We introduce DIVIDE, a framework that disentangles these influences by integrating mechanism-specific deep encoders with a structured Gaussian Process in a joint latent space. Disentanglement here refers to separating independently acting generative factors. The encoders isolate distinct mechanisms while the Gaussian Process captures their combined effect with calibrated uncertainty. The architecture supports structured priors, enabling interpretable and mechanism-aware prediction as well as efficient active learning. DIVIDE is demonstrated on synthetic datasets combining categorical image patches with nonlinear spatial fields, on FerroSIM spin lattice simulations of ferroelectric patterns, and on experimental PFM hysteresis loops from PbTiO3 films. Across benchmarks, DIVIDE separates mechanisms, reproduces additive and scaled interactions, and remains robust under noise. The framework extends naturally to multifunctional datasets where mechanical, electromagnetic or optical responses coexist.</li>
<li><strong>摘要：</strong>科学数据集通常源自多种独立机制，例如空间、分类或结构效应，其综合影响掩盖了它们各自的贡献。我们引入了 DIVIDE，一个框架，通过将特定于机制的深度编码器与联合潜在空间中的结构化高斯过程集成来消除这些影响。这里的解缠是指分离独立作用的生成因素。编码器隔离不同的机制，而高斯过程则通过校准的不确定性捕获它们的综合效应。该架构支持结构化先验，实现可解释和机制感知的预测以及高效的主动学习。 DIVIDE 在将分类图像块与非线性空间场相结合的合成数据集、铁电图案的 FerroSIM 自旋晶格模拟以及 PbTiO3 薄膜的实验 PFM 磁滞回线上进行了演示。在各个基准测试中，DIVIDE 分离机制，再现加性和缩放交互，并在噪声下保持鲁棒性。该框架自然地扩展到机械、电磁或光学响应共存的多功能数据集。</li>
</ul>

<h3>Title: Which Way from B to A: The role of embedding geometry in image interpolation for Stable Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Nicholas Karris, Luke Durell, Javier Flores, Tegan Emerson</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12757">https://arxiv.org/abs/2511.12757</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12757">https://arxiv.org/pdf/2511.12757</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12757]] Which Way from B to A: The role of embedding geometry in image interpolation for Stable Diffusion(https://arxiv.org/abs/2511.12757)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>It can be shown that Stable Diffusion has a permutation-invariance property with respect to the rows of Contrastive Language-Image Pretraining (CLIP) embedding matrices. This inspired the novel observation that these embeddings can naturally be interpreted as point clouds in a Wasserstein space rather than as matrices in a Euclidean space. This perspective opens up new possibilities for understanding the geometry of embedding space. For example, when interpolating between embeddings of two distinct prompts, we propose reframing the interpolation problem as an optimal transport problem. By solving this optimal transport problem, we compute a shortest path (or geodesic) between embeddings that captures a more natural and geometrically smooth transition through the embedding space. This results in smoother and more coherent intermediate (interpolated) images when rendered by the Stable Diffusion generative model. We conduct experiments to investigate this effect, comparing the quality of interpolated images produced using optimal transport to those generated by other standard interpolation methods. The novel optimal transport--based approach presented indeed gives smoother image interpolations, suggesting that viewing the embeddings as point clouds (rather than as matrices) better reflects and leverages the geometry of the embedding space.</li>
<li><strong>摘要：</strong>可以证明，稳定扩散对于对比语言图像预训练（CLIP）嵌入矩阵的行具有排列不变性。这激发了一种新颖的观察：这些嵌入可以自然地解释为 Wasserstein 空间中的点云，而不是欧几里得空间中的矩阵。这种视角为理解嵌入空间的几何形状开辟了新的可能性。例如，当在两个不同提示的嵌入之间进行插值时，我们建议将插值问题重新定义为最优传输问题。通过解决这个最优传输问题，我们计算嵌入之间的最短路径（或测地线），以捕获通过嵌入空间的更自然和几何平滑的过渡。当通过稳定扩散生成模型渲染时，这会产生更平滑、更连贯的中间（插值）图像。我们进行实验来研究这种效应，将使用最佳传输生成的插值图像的质量与其他标准插值方法生成的图像的质量进行比较。提出的新颖的基于最佳传输的方法确实提供了更平滑的图像插值，这表明将嵌入视为点云（而不是矩阵）可以更好地反映和利用嵌入空间的几何形状。</li>
</ul>

<h3>Title: MolEdit: Knowledge Editing for Multimodal Molecule Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhenyu Lei, Patrick Soga, Yaochen Zhu, Yinhan He, Yushun Dong, Jundong Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12770">https://arxiv.org/abs/2511.12770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12770">https://arxiv.org/pdf/2511.12770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12770]] MolEdit: Knowledge Editing for Multimodal Molecule Language Models(https://arxiv.org/abs/2511.12770)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Understanding and continuously refining multimodal molecular knowledge is crucial for advancing biomedicine, chemistry, and materials science. Molecule language models (MoLMs) have become powerful tools in these domains, integrating structural representations (e.g., SMILES strings, molecular graphs) with rich contextual descriptions (e.g., physicochemical properties). However, MoLMs can encode and propagate inaccuracies due to outdated web-mined training corpora or malicious manipulation, jeopardizing downstream discovery pipelines. While knowledge editing has been explored for general-domain AI, its application to MoLMs remains uncharted, presenting unique challenges due to the multifaceted and interdependent nature of molecular knowledge. In this paper, we take the first step toward MoLM editing for two critical tasks: molecule-to-caption generation and caption-to-molecule generation. To address molecule-specific challenges, we propose MolEdit, a powerful framework that enables targeted modifications while preserving unrelated molecular knowledge. MolEdit combines a Multi-Expert Knowledge Adapter that routes edits to specialized experts for different molecular facets with an Expertise-Aware Editing Switcher that activates the adapters only when input closely matches the stored edits across all expertise, minimizing interference with unrelated knowledge. To systematically evaluate editing performance, we introduce MEBench, a comprehensive benchmark assessing multiple dimensions, including Reliability (accuracy of the editing), Locality (preservation of irrelevant knowledge), and Generality (robustness to reformed queries). Across extensive experiments on two popular MoLM backbones, MolEdit delivers up to 18.8% higher Reliability and 12.0% better Locality than baselines while maintaining efficiency. The code is available at: this https URL.</li>
<li><strong>摘要：</strong>理解并不断完善多模式分子知识对于推进生物医学、化学和材料科学至关重要。分子语言模型 (MoLM) 已成为这些领域的强大工具，它将结构表示（例如 SMILES 字符串、分子图）与丰富的上下文描述（例如物理化学性质）相集成。然而，由于过时的网络挖掘训练语料库或恶意操作，MoLM 可能会编码和传播不准确的信息，从而危及下游发现管道。虽然知识编辑已经在通用领域人工智能中进行了探索，但其在 MoLM 中的应用仍然未知，由于分子知识的多方面和相互依赖的性质，提出了独特的挑战。在本文中，我们迈出了 MoLM 编辑的第一步，完成了两个关键任务：分子到标题的生成和标题到分子的生成。为了解决分子特定的挑战，我们提出了 MolEdit，这是一个强大的框架，可以实现有针对性的修改，同时保留不相关的分子知识。 MolEdit 结合了多专家知识适配器和专业知识感知编辑切换器，前者将编辑路由到不同分子方面的专业专家，后者仅当输入与所有专业知识中存储的编辑紧密匹配时才激活适配器，从而最大限度地减少对不相关知识的干扰。为了系统地评估编辑性能，我们引入了 MEBench，这是一个评估多个维度的综合基准，包括可靠性（编辑的准确性）、局部性（不相关知识的保留）和通用性（对改进查询的鲁棒性）。在两种流行的 MoLM 主干上进行的广泛实验中，MolEdit 的可靠性比基线高出 18.8%，局部性比基线高出 12.0%，同时保持效率。该代码位于：此 https URL。</li>
</ul>

<h3>Title: The Alignment Game: A Theory of Long-Horizon Alignment Through Recursive Curation</h3>
<ul>
<li><strong>Authors: </strong>Ali Falahati, Mohammad Mohammadi Amiri, Kate Larson, Lukasz Golab</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12804">https://arxiv.org/abs/2511.12804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12804">https://arxiv.org/pdf/2511.12804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12804]] The Alignment Game: A Theory of Long-Horizon Alignment Through Recursive Curation(https://arxiv.org/abs/2511.12804)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In self-consuming generative models that train on their own outputs, alignment with user preferences becomes a recursive rather than one-time process. We provide the first formal foundation for analyzing the long-term effects of such recursive retraining on alignment. Under a two-stage curation mechanism based on the Bradley-Terry (BT) model, we model alignment as an interaction between two factions: the Model Owner, who filters which outputs should be learned by the model, and the Public User, who determines which outputs are ultimately shared and retained through interactions with the model. Our analysis reveals three structural convergence regimes depending on the degree of preference alignment: consensus collapse, compromise on shared optima, and asymmetric refinement. We prove a fundamental impossibility theorem: no recursive BT-based curation mechanism can simultaneously preserve diversity, ensure symmetric influence, and eliminate dependence on initialization. Framing the process as dynamic social choice, we show that alignment is not a static goal but an evolving equilibrium, shaped both by power asymmetries and path dependence.</li>
<li><strong>摘要：</strong>在根据自己的输出进行训练的自消耗生成模型中，与用户偏好的一致性成为一种递归过程，而不是一次性过程。我们为分析这种递归再训练对对齐的长期影响提供了第一个正式的基础。在基于 Bradley-Terry (BT) 模型的两阶段管理机制下，我们将对齐建模为两个派系之间的交互：模型所有者，过滤模型应学习的输出；公共用户，通过与模型的交互来确定最终共享和保留哪些输出。我们的分析揭示了取决于偏好调整程度的三种结构收敛机制：共识崩溃、共享最优的妥协和不对称细化。我们证明了一个基本的不可能定理：没有一种基于 BT 的递归管理机制可以同时保留多样性、确保对称影响并消除对初始化的依赖。将这一过程视为动态的社会选择，我们表明结盟不是一个静态目标，而是一种不断发展的平衡，由权力不对称和路径依赖共同塑造。</li>
</ul>

<h3>Title: An Evaluation of Representation Learning Methods in Particle Physics Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Michael Chen, Raghav Kansal, Abhijith Gandrakota, Zichun Hao, Jennifer Ngadiuba, Maria Spiropulu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12829">https://arxiv.org/abs/2511.12829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12829">https://arxiv.org/pdf/2511.12829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12829]] An Evaluation of Representation Learning Methods in Particle Physics Foundation Models(https://arxiv.org/abs/2511.12829)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present a systematic evaluation of representation learning objectives for particle physics within a unified framework. Our study employs a shared transformer-based particle-cloud encoder with standardized preprocessing, matched sampling, and a consistent evaluation protocol on a jet classification dataset. We compare contrastive (supervised and self-supervised), masked particle modeling, and generative reconstruction objectives under a common training regimen. In addition, we introduce targeted supervised architectural modifications that achieve state-of-the-art performance on benchmark evaluations. This controlled comparison isolates the contributions of the learning objective, highlights their respective strengths and limitations, and provides reproducible baselines. We position this work as a reference point for the future development of foundation models in particle physics, enabling more transparent and robust progress across the community.</li>
<li><strong>摘要：</strong>我们在统一框架内对粒子物理学的表示学习目标进行了系统评估。我们的研究采用了基于共享变压器的粒子云编码器，具有标准化预处理、匹配采样和对喷射分类数据集的一致评估协议。我们在常见的训练方案下比较对比（监督和自监督）、掩蔽粒子建模和生成重建目标。此外，我们还引入了有针对性的监督架构修改，以在基准评估中实现最先进的性能。这种受控比较隔离了学习目标的贡献，突出了各自的优势和局限性，并提供了可重复的基线。我们将这项工作定位为粒子物理基础模型未来发展的参考点，从而使整个社区取得更加透明和稳健的进展。</li>
</ul>

<h3>Title: SAGA: Source Attribution of Generative AI Videos</h3>
<ul>
<li><strong>Authors: </strong>Rohit Kundu, Vishal Mohanty, Hao Xiong, Shan Jia, Athula Balachandran, Amit K. Roy-Chowdhury</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12834">https://arxiv.org/abs/2511.12834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12834">https://arxiv.org/pdf/2511.12834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12834]] SAGA: Source Attribution of Generative AI Videos(https://arxiv.org/abs/2511.12834)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The proliferation of generative AI has led to hyper-realistic synthetic videos, escalating misuse risks and outstripping binary real/fake detectors. We introduce SAGA (Source Attribution of Generative AI videos), the first comprehensive framework to address the urgent need for AI-generated video source attribution at a large scale. Unlike traditional detection, SAGA identifies the specific generative model used. It uniquely provides multi-granular attribution across five levels: authenticity, generation task (e.g., T2V/I2V), model version, development team, and the precise generator, offering far richer forensic insights. Our novel video transformer architecture, leveraging features from a robust vision foundation model, effectively captures spatio-temporal artifacts. Critically, we introduce a data-efficient pretrain-and-attribute strategy, enabling SAGA to achieve state-of-the-art attribution using only 0.5\% of source-labeled data per class, matching fully supervised performance. Furthermore, we propose Temporal Attention Signatures (T-Sigs), a novel interpretability method that visualizes learned temporal differences, offering the first explanation for why different video generators are distinguishable. Extensive experiments on public datasets, including cross-domain scenarios, demonstrate that SAGA sets a new benchmark for synthetic video provenance, providing crucial, interpretable insights for forensic and regulatory applications.</li>
<li><strong>摘要：</strong>生成式人工智能的激增导致了超现实的合成视频的出现，滥用风险不断升级，并超越了二进制真/假检测器。我们推出 SAGA（生成 AI 视频的源归因），这是第一个解决大规模 AI 生成视频源归因迫切需求的综合框架。与传统检测不同，SAGA 识别所使用的特定生成模型。它独特地提供了五个级别的多粒度归因：真实性、生成任务（例如 T2V/I2V）、模型版本、开发团队和精确生成器，提供更丰富的取证见解。我们新颖的视频转换器架构利用强大的视觉基础模型的功能，有效地捕获时空伪像。至关重要的是，我们引入了一种数据高效的预训练和属性策略，使 SAGA 能够仅使用每类 0.5% 的源标记数据来实现最先进的归因，从而匹配完全监督的性能。此外，我们提出了时间注意力签名（T-Sigs），这是一种新颖的可解释性方法，可以可视化学习到的时间差异，为为什么不同的视频生成器是可区分的提供了第一个解释。对公共数据集（包括跨域场景）的广泛实验表明，SAGA 为合成视频来源设定了新基准，为取证和监管应用提供了重要的、可解释的见解。</li>
</ul>

<h3>Title: Method of Manufactured Learning for Solver-free Training of Neural Operators</h3>
<ul>
<li><strong>Authors: </strong>Arth Sojitra, Omer San</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12890">https://arxiv.org/abs/2511.12890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12890">https://arxiv.org/pdf/2511.12890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12890]] Method of Manufactured Learning for Solver-free Training of Neural Operators(https://arxiv.org/abs/2511.12890)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Training neural operators to approximate mappings between infinite-dimensional function spaces often requires extensive datasets generated by either demanding experimental setups or computationally expensive numerical solvers. This dependence on solver-based data limits scalability and constrains exploration across physical systems. Here we introduce the Method of Manufactured Learning (MML), a solver-independent framework for training neural operators using analytically constructed, physics-consistent datasets. Inspired by the classical method of manufactured solutions, MML replaces numerical data generation with functional synthesis, i.e., smooth candidate solutions are sampled from controlled analytical spaces, and the corresponding forcing fields are derived by direct application of the governing differential operators. During inference, setting these forcing terms to zero restores the original governing equations, allowing the trained neural operator to emulate the true solution operator of the system. The framework is agnostic to network architecture and can be integrated with any operator learning paradigm. In this paper, we employ Fourier neural operator as a representative example. Across canonical benchmarks including heat, advection, Burgers, and diffusion-reaction equations. MML achieves high spectral accuracy, low residual errors, and strong generalization to unseen conditions. By reframing data generation as a process of analytical synthesis, MML offers a scalable, solver-agnostic pathway toward constructing physically grounded neural operators that retain fidelity to governing laws without reliance on expensive numerical simulations or costly experimental data for training.</li>
<li><strong>摘要：</strong>训练神经算子来近似无限维函数空间之间的映射通常需要由要求苛刻的实验设置或计算成本昂贵的数值求解器生成的大量数据集。这种对基于求解器的数据的依赖限制了可扩展性并限制了跨物理系统的探索。在这里，我们介绍了制造学习方法（MML），这是一种独立于求解器的框架，用于使用分析构建的、物理一致的数据集来训练神经算子。受制造解的经典方法的启发，MML 用函数综合代替数值数据生成，即从受控分析空间中采样平滑的候选解，并通过直接应用控制微分算子导出相应的力场。在推理过程中，将这些强迫项设置为零可以恢复原始控制方程，从而允许经过训练的神经算子模拟系统的真实解算子。该框架与网络架构无关，并且可以与任何运营商学习范例集成。在本文中，我们采用傅立叶神经算子作为代表性例子。跨越规范基准，包括热、平流、汉堡和扩散反应方程。 MML 实现了高光谱精度、低残余误差以及对未见条件的强泛化能力。通过将数据生成重新定义为分析综合的过程，MML 提供了一种可扩展的、与求解器无关的途径，用于构建物理基础的神经算子，这些算子保持对控制定律的保真度，而不依赖于昂贵的数值模拟或昂贵的实验数据进行训练。</li>
</ul>

<h3>Title: ActVAR: Activating Mixtures of Weights and Tokens for Efficient Visual Autoregressive Generation</h3>
<ul>
<li><strong>Authors: </strong>Kaixin Zhang, Ruiqing Yang, Yuan Zhang, Shan You, Tao Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12893">https://arxiv.org/abs/2511.12893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12893">https://arxiv.org/pdf/2511.12893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12893]] ActVAR: Activating Mixtures of Weights and Tokens for Efficient Visual Autoregressive Generation(https://arxiv.org/abs/2511.12893)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Visual Autoregressive (VAR) models enable efficient image generation via next-scale prediction but face escalating computational costs as sequence length grows. Existing static pruning methods degrade performance by permanently removing weights or tokens, disrupting pretrained dependencies. To address this, we propose ActVAR, a dynamic activation framework that introduces dual sparsity across model weights and token sequences to enhance efficiency without sacrificing capacity. ActVAR decomposes feedforward networks (FFNs) into lightweight expert sub-networks and employs a learnable router to dynamically select token-specific expert subsets based on content. Simultaneously, a gated token selector identifies high-update-potential tokens for computation while reconstructing unselected tokens to preserve global context and sequence alignment. Training employs a two-stage knowledge distillation strategy, where the original VAR model supervises the learning of routing and gating policies to align with pretrained knowledge. Experiments on the ImageNet $256\times 256$ benchmark demonstrate that ActVAR achieves up to $21.2\%$ FLOPs reduction with minimal performance degradation.</li>
<li><strong>摘要：</strong>视觉自回归（VAR）模型可以通过下一尺度的预测实现高效的图像生成，但随着序列长度的增长，计算成本也会随之增加。现有的静态修剪方法通过永久删除权重或令牌、破坏预训练的依赖关系来降低性能。为了解决这个问题，我们提出了 ActVAR，一种动态激活框架，它在模型权重和令牌序列之间引入了双重稀疏性，以在不牺牲容量的情况下提高效率。 ActVAR 将前馈网络 (FFN) 分解为轻量级专家子网络，并采用可学习路由器根据内容动态选择特定于代币的专家子集。同时，门控令牌选择器识别用于计算的高更新潜力令牌，同时重建未选择的令牌以保留全局上下文和序列对齐。训练采用两阶段知识蒸馏策略，其中原始 VAR 模型监督路由和门控策略的学习，以与预训练的知识保持一致。在 ImageNet $256\times 256$ 基准上的实验表明，ActVAR 在性能下降最小的情况下实现了高达 $21.2\%$ 的 FLOP 减少。</li>
</ul>

<h3>Title: Functional Mean Flow in Hilbert Space</h3>
<ul>
<li><strong>Authors: </strong>Zhiqi Li, Yuchen Sun, Greg Turk, Bo Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12898">https://arxiv.org/abs/2511.12898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12898">https://arxiv.org/pdf/2511.12898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12898]] Functional Mean Flow in Hilbert Space(https://arxiv.org/abs/2511.12898)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We present Functional Mean Flow (FMF) as a one-step generative model defined in infinite-dimensional Hilbert space. FMF extends the one-step Mean Flow framework to functional domains by providing a theoretical formulation for Functional Flow Matching and a practical implementation for efficient training and sampling. We also introduce an $x_1$-prediction variant that improves stability over the original $u$-prediction form. The resulting framework is a practical one-step Flow Matching method applicable to a wide range of functional data generation tasks such as time series, images, PDEs, and 3D geometry.</li>
<li><strong>摘要：</strong>我们提出函数平均流（FMF）作为无限维希尔伯特空间中定义的一步生成模型。 FMF 通过提供功能流匹配的理论公式和高效训练和采样的实际实现，将一步平均流框架扩展到功能领域。我们还引入了 $x_1$-预测变体，它提高了原始 $u$-预测形式的稳定性。由此产生的框架是一种实用的一步式流匹配方法，适用于各种功能数据生成任务，例如时间序列、图像、偏微分方程和 3D 几何。</li>
</ul>

<h3>Title: FDP: A Frequency-Decomposition Preprocessing Pipeline for Unsupervised Anomaly Detection in Brain MRI</h3>
<ul>
<li><strong>Authors: </strong>Hao Li, Zhenfeng Zhuang, Jingyu Lin, Yu Liu, Yifei Chen, Qiong Peng, Lequan Yu, Liansheng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12899">https://arxiv.org/abs/2511.12899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12899">https://arxiv.org/pdf/2511.12899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12899]] FDP: A Frequency-Decomposition Preprocessing Pipeline for Unsupervised Anomaly Detection in Brain MRI(https://arxiv.org/abs/2511.12899)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Due to the diversity of brain anatomy and the scarcity of annotated data, supervised anomaly detection for brain MRI remains challenging, driving the development of unsupervised anomaly detection (UAD) approaches. Current UAD methods typically utilize artificially generated noise perturbations on healthy MRIs to train generative models for normal anatomy reconstruction, enabling anomaly detection via residual mapping. However, such simulated anomalies lack the biophysical fidelity and morphological complexity characteristic of true clinical lesions. To advance UAD in brain MRI, we conduct the first systematic frequency-domain analysis of pathological signatures, revealing two key properties: (1) anomalies exhibit unique frequency patterns distinguishable from normal anatomy, and (2) low-frequency signals maintain consistent representations across healthy scans. These insights motivate our Frequency-Decomposition Preprocessing (FDP) framework, the first UAD method to leverage frequency-domain reconstruction for simultaneous pathology suppression and anatomical preservation. FDP can integrate seamlessly with existing anomaly simulation techniques, consistently enhancing detection performance across diverse architectures while maintaining diagnostic fidelity. Experimental results demonstrate that FDP consistently improves anomaly detection performance when integrated with existing methods. Notably, FDP achieves a 17.63% increase in DICE score with LDM while maintaining robust improvements across multiple baselines. The code is available at this https URL.</li>
<li><strong>摘要：</strong>由于大脑解剖结构的多样性和注释数据的稀缺，脑 MRI 的监督异常检测仍然具有挑战性，这推动了无监督异常检测 (UAD) 方法的发展。当前的 UAD 方法通常利用健康 MRI 上人工生成的噪声扰动来训练正常解剖重建的生成模型，从而通过残差映射实现异常检测。然而，这种模拟异常缺乏真实临床病变的生物物理保真度和形态复杂性特征。为了推进 UAD 在脑 MRI 中的应用，我们对病理特征进行了首次系统频域分析，揭示了两个关键特性：(1) 异常表现出与正常解剖结构可区分的独特频率模式，(2) 低频信号在健康扫描中保持一致的表示。这些见解激发了我们的频率分解预处理 (FDP) 框架，这是第一个利用频域重建同时进行病理抑制和解剖保存的 UAD 方法。 FDP 可以与现有的异常模拟技术无缝集成，持续增强跨不同架构的检测性能，同时保持诊断保真度。实验结果表明，当与现有方法集成时，FDP 不断提高异常检测性能。值得注意的是，FDP 通过 LDM 将 DICE 得分提高了 17.63%，同时在多个基线上保持了强劲的改进。该代码可从此 https URL 获取。</li>
</ul>

<h3>Title: CoordAR: One-Reference 6D Pose Estimation of Novel Objects via Autoregressive Coordinate Map Generation</h3>
<ul>
<li><strong>Authors: </strong>Dexin Zuo, Ang Li, Wei Wang, Wenxian Yu, Danping Zou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12919">https://arxiv.org/abs/2511.12919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12919">https://arxiv.org/pdf/2511.12919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12919]] CoordAR: One-Reference 6D Pose Estimation of Novel Objects via Autoregressive Coordinate Map Generation(https://arxiv.org/abs/2511.12919)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Object 6D pose estimation, a crucial task for robotics and augmented reality applications, becomes particularly challenging when dealing with novel objects whose 3D models are not readily available. To reduce dependency on 3D models, recent studies have explored one-reference-based pose estimation, which requires only a single reference view instead of a complete 3D model. However, existing methods that rely on real-valued coordinate regression suffer from limited global consistency due to the local nature of convolutional architectures and face challenges in symmetric or occluded scenarios owing to a lack of uncertainty modeling. We present CoordAR, a novel autoregressive framework for one-reference 6D pose estimation of unseen objects. CoordAR formulates 3D-3D correspondences between the reference and query views as a map of discrete tokens, which is obtained in an autoregressive and probabilistic manner. To enable accurate correspondence regression, CoordAR introduces 1) a novel coordinate map tokenization that enables probabilistic prediction over discretized 3D space; 2) a modality-decoupled encoding strategy that separately encodes RGB appearance and coordinate cues; and 3) an autoregressive transformer decoder conditioned on both position-aligned query features and the partially generated token sequence. With these novel mechanisms, CoordAR significantly outperforms existing methods on multiple benchmarks and demonstrates strong robustness to symmetry, occlusion, and other challenges in real-world tests.</li>
<li><strong>摘要：</strong>对象 6D 姿态估计是机器人和增强现实应用的一项关键任务，在处理 3D 模型不易获得的新对象时变得尤其具有挑战性。为了减少对 3D 模型的依赖，最近的研究探索了基于单参考的姿态估计，它只需要单个参考视图而不是完整的 3D 模型。然而，由于卷积架构的局部性质，依赖实值坐标回归的现有方法受到有限的全局一致性的影响，并且由于缺乏不确定性建模而在对称或遮挡场景中面临挑战。我们提出了 CoordAR，一种新颖的自回归框架，用于对不可见物体进行单参考 6D 姿态估计。 CoordAR 将参考视图和查询视图之间的 3D-3D 对应关系制定为离散标记的映射，该映射以自回归和概率方式获得。为了实现准确的对应回归，CoordAR 引入了 1) 一种新颖的坐标图标记化，可以在离散 3D 空间上进行概率预测； 2）模态解耦编码策略，分别对RGB外观和坐标线索进行编码； 3）以位置对齐查询特征和部分生成的标记序列为条件的自回归变换器解码器。凭借这些新颖的机制，CoordAR 在多个基准测试中显着优于现有方法，并在现实测试中表现出对对称性、遮挡和其他挑战的强大鲁棒性。</li>
</ul>

<h3>Title: Generative Photographic Control for Scene-Consistent Video Cinematic Editing</h3>
<ul>
<li><strong>Authors: </strong>Huiqiang Sun, Liao Shen, Zhan Peng, Kun Wang, Size Wu, Yuhang Zang, Tianqi Liu, Zihao Huang, Xingyu Zeng, Zhiguo Cao, Wei Li, Chen Change Loy</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12921">https://arxiv.org/abs/2511.12921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12921">https://arxiv.org/pdf/2511.12921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12921]] Generative Photographic Control for Scene-Consistent Video Cinematic Editing(https://arxiv.org/abs/2511.12921)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Cinematic storytelling is profoundly shaped by the artful manipulation of photographic elements such as depth of field and exposure. These effects are crucial in conveying mood and creating aesthetic appeal. However, controlling these effects in generative video models remains highly challenging, as most existing methods are restricted to camera motion control. In this paper, we propose CineCtrl, the first video cinematic editing framework that provides fine control over professional camera parameters (e.g., bokeh, shutter speed). We introduce a decoupled cross-attention mechanism to disentangle camera motion from photographic inputs, allowing fine-grained, independent control without compromising scene consistency. To overcome the shortage of training data, we develop a comprehensive data generation strategy that leverages simulated photographic effects with a dedicated real-world collection pipeline, enabling the construction of a large-scale dataset for robust model training. Extensive experiments demonstrate that our model generates high-fidelity videos with precisely controlled, user-specified photographic camera effects.</li>
<li><strong>摘要：</strong>对景深和曝光等摄影元素的巧妙处理深刻地塑造了电影的故事讲述。这些效果对于传达情绪和创造美感至关重要。然而，在生成视频模型中控制这些效果仍然非常具有挑战性，因为大多数现有方法仅限于摄像机运动控制。在本文中，我们提出了 CineCtrl，这是第一个视频电影编辑框架，可以对专业相机参数（例如散景、快门速度）进行精细控制。我们引入了一种解耦的交叉注意机制，将相机运动与摄影输入分开，从而在不影响场景一致性的情况下进行细粒度的独立控制。为了克服训练数据的短缺，我们开发了一种全面的数据生成策略，该策略利用模拟摄影效果和专用的真实世界收集管道，从而能够构建用于稳健模型训练的大规模数据集。大量的实验表明，我们的模型可以生成具有精确控制、用户指定的摄影相机效果的高保真视频。</li>
</ul>

<h3>Title: Text2Traffic: A Text-to-Image Generation and Editing Method for Traffic Scenes</h3>
<ul>
<li><strong>Authors: </strong>Feng Lv, Haoxuan Feng, Zilu Zhang, Chunlong Xia, Yanfeng Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12932">https://arxiv.org/abs/2511.12932</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12932">https://arxiv.org/pdf/2511.12932</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12932]] Text2Traffic: A Text-to-Image Generation and Editing Method for Traffic Scenes(https://arxiv.org/abs/2511.12932)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of intelligent transportation systems, text-driven image generation and editing techniques have demonstrated significant potential in providing rich, controllable visual scene data for applications such as traffic monitoring and autonomous driving. However, several challenges remain, including insufficient semantic richness of generated traffic elements, limited camera viewpoints, low visual fidelity of synthesized images, and poor alignment between textual descriptions and generated content. To address these issues, we propose a unified text-driven framework for both image generation and editing, leveraging a controllable mask mechanism to seamlessly integrate the two tasks. Furthermore, we incorporate both vehicle-side and roadside multi-view data to enhance the geometric diversity of traffic scenes. Our training strategy follows a two-stage paradigm: first, we perform conceptual learning using large-scale coarse-grained text-image data; then, we fine-tune with fine-grained descriptive data to enhance text-image alignment and detail quality. Additionally, we introduce a mask-region-weighted loss that dynamically emphasizes small yet critical regions during training, thereby substantially enhancing the generation fidelity of small-scale traffic elements. Extensive experiments demonstrate that our method achieves leading performance in text-based image generation and editing within traffic scenes.</li>
<li><strong>摘要：</strong>随着智能交通系统的快速发展，文本驱动的图像生成和编辑技术在为交通监控和自动驾驶等应用提供丰富、可控的视觉场景数据方面表现出了巨大的潜力。然而，仍然存在一些挑战，包括生成的流量元素的语义丰富度不足、摄像机视角有限、合成图像的视觉保真度低以及文本描述和生成的内容之间的对齐不良。为了解决这些问题，我们提出了一个用于图像生成和编辑的统一文本驱动框架，利用可控掩模机制无缝集成这两个任务。此外，我们结合了车辆侧和路侧多视图数据，以增强交通场景的几何多样性。我们的训练策略遵循两阶段范式：首先，我们使用大规模粗粒度文本图像数据进行概念学习；然后，我们使用细粒度的描述数据进行微调，以增强文本图像对齐和细节质量。此外，我们引入了掩模区域加权损失，该损失在训练期间动态强调小而关键的区域，从而大大提高小规模流量元素的生成保真度。大量的实验表明，我们的方法在交通场景中基于文本的图像生成和编辑方面实现了领先的性能。</li>
</ul>

<h3>Title: PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos</h3>
<ul>
<li><strong>Authors: </strong>Dianbing Xi, Guoyuan An, Jingsen Zhu, Zhijian Liu, Yuan Liu, Ruiyuan Zhang, Jiayuan Lu, Rui Wang, Yuchi Huo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12935">https://arxiv.org/abs/2511.12935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12935">https://arxiv.org/pdf/2511.12935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12935]] PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos(https://arxiv.org/abs/2511.12935)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We propose PFAvatar (Pose-Fusion Avatar), a new method that reconstructs high-quality 3D avatars from ``Outfit of the Day'' (OOTD) photos, which exhibit diverse poses, occlusions, and complex backgrounds. Our method consists of two stages: (1) fine-tuning a pose-aware diffusion model from few-shot OOTD examples and (2) distilling a 3D avatar represented by a neural radiance field (NeRF). In the first stage, unlike previous methods that segment images into assets (e.g., garments, accessories) for 3D assembly, which is prone to inconsistency, we avoid decomposition and directly model the full-body appearance. By integrating a pre-trained ControlNet for pose estimation and a novel Condition Prior Preservation Loss (CPPL), our method enables end-to-end learning of fine details while mitigating language drift in few-shot training. Our method completes personalization in just 5 minutes, achieving a 48$\times$ speed-up compared to previous approaches. In the second stage, we introduce a NeRF-based avatar representation optimized by canonical SMPL-X space sampling and Multi-Resolution 3D-SDS. Compared to mesh-based representations that suffer from resolution-dependent discretization and erroneous occluded geometry, our continuous radiance field can preserve high-frequency textures (e.g., hair) and handle occlusions correctly through transmittance. Experiments demonstrate that PFAvatar outperforms state-of-the-art methods in terms of reconstruction fidelity, detail preservation, and robustness to occlusions/truncations, advancing practical 3D avatar generation from real-world OOTD albums. In addition, the reconstructed 3D avatar supports downstream applications such as virtual try-on, animation, and human video reenactment, further demonstrating the versatility and practical value of our approach.</li>
<li><strong>摘要：</strong>我们提出了 PFAvatar（Pose-Fusion Avatar），这是一种从“每日服装”（OOTD）照片重建高质量 3D 头像的新方法，这些照片表现出不同的姿势、遮挡和复杂的背景。我们的方法由两个阶段组成：(1) 从少量 OOTD 示例中微调姿势感知扩散模型；(2) 提取由神经辐射场 (NeRF) 表示的 3D 头像。在第一阶段，与之前将图像分割成资产（例如服装、配饰）进行 3D 组装的方法容易出现不一致的情况不同，我们避免了分解并直接对全身外观进行建模。通过集成用于姿态估计的预训练 ControlNet 和新颖的条件先验保留损失 (CPPL)，我们的方法能够实现精细细节的端到端学习，同时减轻少样本训练中的语言漂移。我们的方法只需 5 分钟即可完成个性化，与之前的方法相比，速度提高了 48 倍。在第二阶段，我们引入了通过规范 SMPL-X 空间采样和多分辨率 3D-SDS 优化的基于 NeRF 的化身表示。与遭受分辨率相关离散化和错误遮挡几何形状的基于网格的表示相比，我们的连续辐射场可以保留高频纹理（例如头发）并通过透射率正确处理遮挡。实验表明，PFAvatar 在重建保真度、细节保留和对遮挡/截断的鲁棒性方面优于最先进的方法，从而推进了从现实世界的 OOTD 相册生成实用的 3D 头像。此外，重建的3D头像支持虚拟试穿、动画和真人视频重演等下游应用，进一步证明了我们方法的多功能性和实用价值。</li>
</ul>

<h3>Title: Recurrent Autoregressive Diffusion: Global Memory Meets Local Attention</h3>
<ul>
<li><strong>Authors: </strong>Taiye Chen, Zihan Ding, Anjian Li, Christina Zhang, Zeqi Xiao, Yisen Wang, Chi Jin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12940">https://arxiv.org/abs/2511.12940</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12940">https://arxiv.org/pdf/2511.12940</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12940]] Recurrent Autoregressive Diffusion: Global Memory Meets Local Attention(https://arxiv.org/abs/2511.12940)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advancements in video generation have demonstrated the potential of using video diffusion models as world models, with autoregressive generation of infinitely long videos through masked conditioning. However, such models, usually with local full attention, lack effective memory compression and retrieval for long-term generation beyond the window size, leading to issues of forgetting and spatiotemporal inconsistencies. To enhance the retention of historical information within a fixed memory budget, we introduce a recurrent neural network (RNN) into the diffusion transformer framework. Specifically, a diffusion model incorporating LSTM with attention achieves comparable performance to state-of-the-art RNN blocks, such as TTT and Mamba2. Moreover, existing diffusion-RNN approaches often suffer from performance degradation due to training-inference gap or the lack of overlap across windows. To address these limitations, we propose a novel Recurrent Autoregressive Diffusion (RAD) framework, which executes frame-wise autoregression for memory update and retrieval, consistently across training and inference time. Experiments on Memory Maze and Minecraft datasets demonstrate the superiority of RAD for long video generation, highlighting the efficiency of LSTM in sequence modeling.</li>
<li><strong>摘要：</strong>视频生成领域的最新进展证明了使用视频扩散模型作为世界模型的潜力，通过屏蔽条件自回归生成无限长的视频。然而，此类模型通常具有局部充分关注，缺乏有效的记忆压缩和检索，无法进行超出窗口大小的长期生成，从而导致遗忘和时空不一致的问题。为了在固定内存预算内增强历史信息的保留，我们在扩散变压器框架中引入了循环神经网络（RNN）。具体来说，将 LSTM 与注意力机制相结合的扩散模型实现了与最先进的 RNN 模块（例如 TTT 和 Mamba2）相当的性能。此外，现有的扩散 RNN 方法常常由于训练与推理之间的差距或窗口之间缺乏重叠而导致性能下降。为了解决这些限制，我们提出了一种新颖的循环自回归扩散（RAD）框架，该框架在整个训练和推理时间内一致地执行逐帧自回归以进行内存更新和检索。在 Memory Maze 和 Minecraft 数据集上的实验证明了 RAD 在长视频生成方面的优越性，凸显了 LSTM 在序列建模方面的效率。</li>
</ul>

<h3>Title: RAGPulse: An Open-Source RAG Workload Trace to Optimize RAG Serving Systems</h3>
<ul>
<li><strong>Authors: </strong>Zhengchao Wang, Yitao Hu, Jianing Ye, Zhuxuan Chang, Jiazheng Yu, Youpeng Deng, Keqiu Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12979">https://arxiv.org/abs/2511.12979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12979">https://arxiv.org/pdf/2511.12979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12979]] RAGPulse: An Open-Source RAG Workload Trace to Optimize RAG Serving Systems(https://arxiv.org/abs/2511.12979)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) is a critical paradigm for building reliable, knowledge-intensive Large Language Model (LLM) applications. However, the multi-stage pipeline (retrieve, generate) and unique workload characteristics (e.g., knowledge dependency) of RAG systems pose significant challenges for serving performance optimization. Existing generic LLM inference traces fail to capture these RAG-specific dynamics, creating a significant performance gap between academic research and real-world deployment. To bridge this gap, this paper introduces RAGPulse, an open-source RAG workload trace dataset. This dataset was collected from an university-wide Q&A system serving that has served more than 40,000 students and faculties since April 2024. We detail RAGPulse's system architecture, its privacy-preserving hash-based data format, and provide an in-depth statistical analysis. Our analysis reveals that real-world RAG workloads exhibit significant temporal locality and a highly skewed hot document access pattern. RAGPulse provides a high-fidelity foundation for researchers to develop and validate novel optimization strategies for RAG systems, such as content-aware batching and retrieval caching, ultimately enhancing the efficiency and reliability of RAG services. The code is available at this https URL.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 是构建可靠的知识密集型大型语言模型 (LLM) 应用程序的关键范例。然而，RAG 系统的多级管道（检索、生成）和独特的工作负载特征（例如知识依赖性）给服务性能优化带来了重大挑战。现有的通用 LLM 推理轨迹无法捕获这些 RAG 特定的动态，从而在学术研究和实际部署之间造成了显着的性能差距。为了弥补这一差距，本文引入了 RAGPulse，一个开源 RAG 工作负载跟踪数据集。该数据集是从大学范围内的问答系统收集的，该系统自 2024 年 4 月以来已为超过 40,000 名学生和教职员工提供服务。我们详细介绍了 RAGPulse 的系统架构、其基于哈希的隐私保护数据格式，并提供了深入的统计分析。我们的分析表明，现实世界的 RAG 工作负载表现出显着的时间局部性和高度倾斜的热文档访问模式。 RAGPulse 为研究人员开发和验证 RAG 系统的新颖优化策略（例如内容感知批处理和检索缓存）提供了高保真基础，最终提高了 RAG 服务的效率和可靠性。该代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Semantic Prioritization in Visual Counterfactual Explanations with Weighted Segmentation and Auto-Adaptive Region Selection</h3>
<ul>
<li><strong>Authors: </strong>Lintong Zhang, Kang Yin, Seong-Whan Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.12992">https://arxiv.org/abs/2511.12992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.12992">https://arxiv.org/pdf/2511.12992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.12992]] Semantic Prioritization in Visual Counterfactual Explanations with Weighted Segmentation and Auto-Adaptive Region Selection(https://arxiv.org/abs/2511.12992)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In the domain of non-generative visual counterfactual explanations (CE), traditional techniques frequently involve the substitution of sections within a query image with corresponding sections from distractor images. Such methods have historically overlooked the semantic relevance of the replacement regions to the target object, thereby impairing the model's interpretability and hindering the editing workflow. Addressing these challenges, the present study introduces an innovative methodology named as Weighted Semantic Map with Auto-adaptive Candidate Editing Network (WSAE-Net). Characterized by two significant advancements: the determination of an weighted semantic map and the auto-adaptive candidate editing sequence. First, the generation of the weighted semantic map is designed to maximize the reduction of non-semantic feature units that need to be computed, thereby optimizing computational efficiency. Second, the auto-adaptive candidate editing sequences are designed to determine the optimal computational order among the feature units to be processed, thereby ensuring the efficient generation of counterfactuals while maintaining the semantic relevance of the replacement feature units to the target object. Through comprehensive experimentation, our methodology demonstrates superior performance, contributing to a more lucid and in-depth understanding of visual counterfactual explanations.</li>
<li><strong>摘要：</strong>在非生成视觉反事实解释（CE）领域，传统技术经常涉及用干扰图像中的相应部分替换查询图像中的部分。此类方法历来忽略了替换区域与目标对象的语义相关性，从而损害了模型的可解释性并阻碍了编辑工作流程。为了解决这些挑战，本研究引入了一种创新方法，称为带有自适应候选编辑网络的加权语义图（WSAE-Net）。其特点是两个重大进步：加权语义图的确定和自适应候选编辑序列。首先，加权语义图的生成旨在最大限度地减少需要计算的非语义特征单元，从而优化计算效率。其次，自适应候选编辑序列被设计为确定要处理的特征单元之间的最佳计算顺序，从而确保反事实的高效生成，同时保持替换特征单元与目标对象的语义相关性。通过全面的实验，我们的方法展示了卓越的性能，有助于更清晰和深入地理解视觉反事实解释。</li>
</ul>

<h3>Title: Infinite-Story: A Training-Free Consistent Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Jihun Park, Kyoungmin Lee, Jongmin Gim, Hyeonseo Jo, Minseok Oh, Wonhyeok Choi, Kyumin Hwang, Jaeyeul Kim, Minwoo Choi, Sunghoon Im</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13002">https://arxiv.org/abs/2511.13002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13002">https://arxiv.org/pdf/2511.13002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13002]] Infinite-Story: A Training-Free Consistent Text-to-Image Generation(https://arxiv.org/abs/2511.13002)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present Infinite-Story, a training-free framework for consistent text-to-image (T2I) generation tailored for multi-prompt storytelling scenarios. Built upon a scale-wise autoregressive model, our method addresses two key challenges in consistent T2I generation: identity inconsistency and style inconsistency. To overcome these issues, we introduce three complementary techniques: Identity Prompt Replacement, which mitigates context bias in text encoders to align identity attributes across prompts; and a unified attention guidance mechanism comprising Adaptive Style Injection and Synchronized Guidance Adaptation, which jointly enforce global style and identity appearance consistency while preserving prompt fidelity. Unlike prior diffusion-based approaches that require fine-tuning or suffer from slow inference, Infinite-Story operates entirely at test time, delivering high identity and style consistency across diverse prompts. Extensive experiments demonstrate that our method achieves state-of-the-art generation performance, while offering over 6X faster inference (1.72 seconds per image) than the existing fastest consistent T2I models, highlighting its effectiveness and practicality for real-world visual storytelling.</li>
<li><strong>摘要：</strong>我们推出了 Infinite-Story，这是一个无需培训的框架，用于为多提示叙事场景量身定制一致的文本到图像 (T2I) 生成。我们的方法建立在尺度自回归模型的基础上，解决了一致 T2I 生成中的两个关键挑战：身份不一致和风格不一致。为了克服这些问题，我们引入了三种补充技术：身份提示替换，它可以减轻文本编码器中的上下文偏差，以跨提示对齐身份属性；统一的注意力引导机制，包括自适应风格注入和同步引导适应，共同强制全局风格和身份外观的一致性，同时保持即时保真度。与之前需要微调或推理缓慢的基于扩散的方法不同，Infinite-Story 完全在测试时运行，在不同的提示中提供高度的身份和风格一致性。大量实验表明，我们的方法实现了最先进的生成性能，同时提供比现有最快一致 T2I 模型快 6 倍以上的推理速度（每张图像 1.72 秒），凸显了其对于现实世界视觉故事讲述的有效性和实用性。</li>
</ul>

<h3>Title: Beyond Darkness: Thermal-Supervised 3D Gaussian Splatting for Low-Light Novel View Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Qingsen Ma, Chen Zou, Dianyun Wang, Jia Wang, Liuyu Xiang, Zhaofeng He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13011">https://arxiv.org/abs/2511.13011</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13011">https://arxiv.org/pdf/2511.13011</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13011]] Beyond Darkness: Thermal-Supervised 3D Gaussian Splatting for Low-Light Novel View Synthesis(https://arxiv.org/abs/2511.13011)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Under extremely low-light conditions, novel view synthesis (NVS) faces severe degradation in terms of geometry, color consistency, and radiometric stability. Standard 3D Gaussian Splatting (3DGS) pipelines fail when applied directly to underexposed inputs, as independent enhancement across views causes illumination inconsistencies and geometric distortion. To address this, we present DTGS, a unified framework that tightly couples Retinex-inspired illumination decomposition with thermal-guided 3D Gaussian Splatting for illumination-invariant reconstruction. Unlike prior approaches that treat enhancement as a pre-processing step, DTGS performs joint optimization across enhancement, geometry, and thermal supervision through a cyclic enhancement-reconstruction mechanism. A thermal supervisory branch stabilizes both color restoration and geometry learning by dynamically balancing enhancement, structural, and thermal losses. Moreover, a Retinex-based decomposition module embedded within the 3DGS loop provides physically interpretable reflectance-illumination separation, ensuring consistent color and texture across viewpoints. To evaluate our method, we construct RGBT-LOW, a new multi-view low-light thermal dataset capturing severe illumination degradation. Extensive experiments show that DTGS significantly outperforms existing low-light enhancement and 3D reconstruction baselines, achieving superior radiometric consistency, geometric fidelity, and color stability under extreme illumination.</li>
<li><strong>摘要：</strong>在极低光照条件下，新颖视图合成（NVS）在几何形状、颜色一致性和辐射稳定性方面面临严重退化。标准 3D 高斯泼溅 (3DGS) 管道在直接应用于曝光不足的输入时会失败，因为跨视图的独立增强会导致照明不一致和几何失真。为了解决这个问题，我们提出了 DTGS，这是一个统一的框架，它将 Retinex 启发的照明分解与热引导 3D 高斯分布紧密结合起来，以实现照明不变的重建。与将增强视为预处理步骤的先前方法不同，DTGS 通过循环增强重建机制在增强、几何和热监控之间执行联合优化。热监控分支通过动态平衡增强、结构和热损失来稳定颜色恢复和几何学习。此外，嵌入 3DGS 环路中的基于 Retinex 的分解模块提供物理上可解释的反射照明分离，确保跨视点的颜色和纹理一致。为了评估我们的方法，我们构建了 RGBT-LOW，这是一个新的多视图低光热数据集，可捕获严重的照明退化。大量实验表明，DTGS 显着优于现有的低光增强和 3D 重建基线，在极端照明下实现了卓越的辐射一致性、几何保真度和颜色稳定性。</li>
</ul>

<h3>Title: MeanFlow Transformers with Representation Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Zheyuan Hu, Chieh-Hsin Lai, Ge Wu, Yuki Mitsufuji, Stefano Ermon</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13019">https://arxiv.org/abs/2511.13019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13019">https://arxiv.org/pdf/2511.13019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13019]] MeanFlow Transformers with Representation Autoencoders(https://arxiv.org/abs/2511.13019)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>MeanFlow (MF) is a diffusion-motivated generative model that enables efficient few-step generation by learning long jumps directly from noise to data. In practice, it is often used as a latent MF by leveraging the pre-trained Stable Diffusion variational autoencoder (SD-VAE) for high-dimensional data modeling. However, MF training remains computationally demanding and is often unstable. During inference, the SD-VAE decoder dominates the generation cost, and MF depends on complex guidance hyperparameters for class-conditional generation. In this work, we develop an efficient training and sampling scheme for MF in the latent space of a Representation Autoencoder (RAE), where a pre-trained vision encoder (e.g., DINO) provides semantically rich latents paired with a lightweight decoder. We observe that naive MF training in the RAE latent space suffers from severe gradient explosion. To stabilize and accelerate training, we adopt Consistency Mid-Training for trajectory-aware initialization and use a two-stage scheme: distillation from a pre-trained flow matching teacher to speed convergence and reduce variance, followed by an optional bootstrapping stage with a one-point velocity estimator to further reduce deviation from the oracle mean flow. This design removes the need for guidance, simplifies training configurations, and reduces computation in both training and sampling. Empirically, our method achieves a 1-step FID of 2.03, outperforming vanilla MF's 3.43, while reducing sampling GFLOPS by 38% and total training cost by 83% on ImageNet 256. We further scale our approach to ImageNet 512, achieving a competitive 1-step FID of 3.23 with the lowest GFLOPS among all baselines. Code is available at this https URL.</li>
<li><strong>摘要：</strong>MeanFlow (MF) 是一种扩散驱动的生成模型，通过直接学习从噪声到数据的长跳跃，实现高效的几步生成。在实践中，它通常通过利用预训练的稳定扩散变分自动编码器（SD-VAE）进行高维数据建模来用作潜在 MF。然而，MF 训练对计算的要求仍然很高，而且通常不稳定。在推理过程中，SD-VAE 解码器在生成成本中占主导地位，而 MF 依赖于复杂的指导超参数来进行类条件生成。在这项工作中，我们在表示自动编码器（RAE）的潜在空间中开发了一种高效的 MF 训练和采样方案，其中预先训练的视觉编码器（例如 DINO）提供了语义丰富的潜在与轻量级解码器配对。我们观察到 RAE 潜在空间中的朴素 MF 训练遭受严重的梯度爆炸。为了稳定和加速训练，我们采用一致性中间训练进行轨迹感知初始化，并使用两阶段方案：从预先训练的流匹配教师中进行蒸馏，以加速收敛并减少方差，然后是带有单点速度估计器的可选引导阶段，以进一步减少与预言机平均流的偏差。这种设计消除了指导的需要，简化了训练配置，并减少了训练和采样中的计算。根据经验，我们的方法实现了 2.03 的 1 步 FID，优于普通 MF 的 3.43，同时在 ImageNet 256 上将采样 GFLOPS 降低了 38%，总训练成本降低了 83%。我们进一步将我们的方法扩展到 ImageNet 512，以所有基线中最低的 GFLOPS 实现了具有竞争力的 1 步 FID 3.23。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Towards 3D Object-Centric Feature Learning for Semantic Scene Completion</h3>
<ul>
<li><strong>Authors: </strong>Weihua Wang, Yubo Cui, Xiangru Lin, Zhiheng Li, Zheng Fang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13031">https://arxiv.org/abs/2511.13031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13031">https://arxiv.org/pdf/2511.13031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13031]] Towards 3D Object-Centric Feature Learning for Semantic Scene Completion(https://arxiv.org/abs/2511.13031)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Vision-based 3D Semantic Scene Completion (SSC) has received growing attention due to its potential in autonomous driving. While most existing approaches follow an ego-centric paradigm by aggregating and diffusing features over the entire scene, they often overlook fine-grained object-level details, leading to semantic and geometric ambiguities, especially in complex environments. To address this limitation, we propose Ocean, an object-centric prediction framework that decomposes the scene into individual object instances to enable more accurate semantic occupancy prediction. Specifically, we first employ a lightweight segmentation model, MobileSAM, to extract instance masks from the input image. Then, we introduce a 3D Semantic Group Attention module that leverages linear attention to aggregate object-centric features in 3D space. To handle segmentation errors and missing instances, we further design a Global Similarity-Guided Attention module that leverages segmentation features for global interaction. Finally, we propose an Instance-aware Local Diffusion module that improves instance features through a generative process and subsequently refines the scene representation in the BEV space. Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that Ocean achieves state-of-the-art performance, with mIoU scores of 17.40 and 20.28, respectively.</li>
<li><strong>摘要：</strong>基于视觉的 3D 语义场景完成（SSC）因其在自动驾驶方面的潜力而受到越来越多的关注。虽然大多数现有方法通过在整个场景中聚合和扩散特征来遵循以自我为中心的范式，但它们经常忽略细粒度的对象级细节，导致语义和几何模糊，尤其是在复杂的环境中。为了解决这个限制，我们提出了 Ocean，一个以对象为中心的预测框架，它将场景分解为单独的对象实例，以实现更准确的语义占用预测。具体来说，我们首先采用轻量级分割模型 MobileSAM 从输入图像中提取实例掩模。然后，我们引入了 3D 语义组注意力模块，该模块利用线性注意力来聚合 3D 空间中以对象为中心的特征。为了处理分割错误和缺失实例，我们进一步设计了一个全局相似性引导注意模块，该模块利用分割特征进行全局交互。最后，我们提出了一个实例感知局部扩散模块，该模块通过生成过程改进实例特征，并随后细化 BEV 空间中的场景表示。对 SemanticKITTI 和 SSCBench-KITTI360 基准的大量实验表明，Ocean 实现了最先进的性能，mIoU 分数分别为 17.40 和 20.28。</li>
</ul>

<h3>Title: Uni-Inter: Unifying 3D Human Motion Synthesis Across Diverse Interaction Contexts</h3>
<ul>
<li><strong>Authors: </strong>Sheng Liu, Yuanzhi Liang, Jiepeng Wang, Sidan Du, Chi Zhang, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13032">https://arxiv.org/abs/2511.13032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13032">https://arxiv.org/pdf/2511.13032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13032]] Uni-Inter: Unifying 3D Human Motion Synthesis Across Diverse Interaction Contexts(https://arxiv.org/abs/2511.13032)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present Uni-Inter, a unified framework for human motion generation that supports a wide range of interaction scenarios: including human-human, human-object, and human-scene-within a single, task-agnostic architecture. In contrast to existing methods that rely on task-specific designs and exhibit limited generalization, Uni-Inter introduces the Unified Interactive Volume (UIV), a volumetric representation that encodes heterogeneous interactive entities into a shared spatial field. This enables consistent relational reasoning and compound interaction modeling. Motion generation is formulated as joint-wise probabilistic prediction over the UIV, allowing the model to capture fine-grained spatial dependencies and produce coherent, context-aware behaviors. Experiments across three representative interaction tasks demonstrate that Uni-Inter achieves competitive performance and generalizes well to novel combinations of entities. These results suggest that unified modeling of compound interactions offers a promising direction for scalable motion synthesis in complex environments.</li>
<li><strong>摘要：</strong>我们提出了 Uni-Inter，这是一个用于人体运动生成的统一框架，支持广泛的交互场景：包括在与任务无关的单一架构中的人与人、人与物体以及人与场景。与依赖特定任务设计且泛化能力有限的现有方法相比，Uni-Inter 引入了统一交互体积 (UIV)，这是一种将异构交互实体编码到共享空间场中的体积表示。这使得一致的关系推理和复合交互建模成为可能。运动生成被公式化为 UIV 上的联合概率预测，使模型能够捕获细粒度的空间依赖性并产生连贯的上下文感知行为。三个代表性交互任务的实验表明，Uni-Inter 实现了有竞争力的性能，并且可以很好地推广到新颖的实体组合。这些结果表明，复合相互作用的统一建模为复杂环境中的可扩展运动合成提供了一个有前途的方向。</li>
</ul>

<h3>Title: One-Step Generative Policies with Q-Learning: A Reformulation of MeanFlow</h3>
<ul>
<li><strong>Authors: </strong>Zeyuan Wang, Da Li, Yulin Chen, Ye Shi, Liang Bai, Tianyuan Yu, Yanwei Fu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13035">https://arxiv.org/abs/2511.13035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13035">https://arxiv.org/pdf/2511.13035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13035]] One-Step Generative Policies with Q-Learning: A Reformulation of MeanFlow(https://arxiv.org/abs/2511.13035)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We introduce a one-step generative policy for offline reinforcement learning that maps noise directly to actions via a residual reformulation of MeanFlow, making it compatible with Q-learning. While one-step Gaussian policies enable fast inference, they struggle to capture complex, multimodal action distributions. Existing flow-based methods improve expressivity but typically rely on distillation and two-stage training when trained with Q-learning. To overcome these limitations, we propose to reformulate MeanFlow to enable direct noise-to-action generation by integrating the velocity field and noise-to-action transformation into a single policy network-eliminating the need for separate velocity estimation. We explore several reformulation variants and identify an effective residual formulation that supports expressive and stable policy learning. Our method offers three key advantages: 1) efficient one-step noise-to-action generation, 2) expressive modelling of multimodal action distributions, and 3) efficient and stable policy learning via Q-learning in a single-stage training setup. Extensive experiments on 73 tasks across the OGBench and D4RL benchmarks demonstrate that our method achieves strong performance in both offline and offline-to-online reinforcement learning settings. Code is available at this https URL.</li>
<li><strong>摘要：</strong>我们引入了一种离线强化学习的一步生成策略，通过 MeanFlow 的残差重构将噪声直接映射到动作，使其与 Q 学习兼容。虽然一步高斯策略可以实现快速推理，但它们很难捕获复杂的多模态动作分布。现有的基于流的方法提高了表达能力，但在使用 Q 学习进行训练时通常依赖于蒸馏和两阶段训练。为了克服这些限制，我们建议重新制定 MeanFlow，通过将速度场和噪声到动作转换集成到单个策略网络中，从而能够直接生成噪声到动作，从而消除单独速度估计的需要。我们探索了几种重新表述的变体，并确定了一种支持表达性和稳定的政策学习的有效剩余表述。我们的方法提供了三个关键优势：1）高效的一步噪声到动作生成，2）多模态动作分布的表达建模，以及3）在单阶段训练设置中通过 Q 学习进行高效且稳定的策略学习。对 OGBench 和 D4RL 基准测试的 73 项任务进行的广泛实验表明，我们的方法在离线和离线到在线强化学习设置中均取得了出色的性能。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning</h3>
<ul>
<li><strong>Authors: </strong>Liuyi Jin, Pasan Gunawardena, Amran Haroon, Runzhi Wang, Sangwoo Lee, Radu Stoleru, Michael Middleton, Zepeng Huo, Jeeeun Kim, Jason Moats</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.AS, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13078">https://arxiv.org/abs/2511.13078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13078">https://arxiv.org/pdf/2511.13078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13078]] A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning(https://arxiv.org/abs/2511.13078)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Emergency Medical Technicians (EMTs) operate in high-pressure environments, making rapid, life-critical decisions under heavy cognitive and operational loads. We present EMSGlass, a smart-glasses system powered by EMSNet, the first multimodal multitask model for Emergency Medical Services (EMS), and EMSServe, a low-latency multimodal serving framework tailored to EMS scenarios. EMSNet integrates text, vital signs, and scene images to construct a unified real-time understanding of EMS incidents. Trained on real-world multimodal EMS datasets, EMSNet simultaneously supports up to five critical EMS tasks with superior accuracy compared to state-of-the-art unimodal baselines. Built on top of PyTorch, EMSServe introduces a modality-aware model splitter and a feature caching mechanism, achieving adaptive and efficient inference across heterogeneous hardware while addressing the challenge of asynchronous modality arrival in the field. By optimizing multimodal inference execution in EMS scenarios, EMSServe achieves 1.9x -- 11.7x speedup over direct PyTorch multimodal inference. A user study evaluation with six professional EMTs demonstrates that EMSGlass enhances real-time situational awareness, decision-making speed, and operational efficiency through intuitive on-glass interaction. In addition, qualitative insights from the user study provide actionable directions for extending EMSGlass toward next-generation AI-enabled EMS systems, bridging multimodal intelligence with real-world emergency response workflows.</li>
<li><strong>摘要：</strong>紧急医疗技术人员 (EMT) 在高压环境中工作，在繁重的认知和操作负荷下快速做出至关重要的生命决策。我们推出了 EMSGlass，这是一种由 EMSNet 提供支持的智能眼镜系统，EMSNet 是第一个用于紧急医疗服务 (EMS) 的多模式多任务模型，而 EMSServe 是一种针对 EMS 场景量身定制的低延迟多模式服务框架。 EMSNet整合文本、生命体征和场景图像，构建对EMS事件的统一实时理解。 EMSNet 经过真实世界多模态 EMS 数据集的训练，可同时支持多达五个关键 EMS 任务，与最先进的单模态基线相比，其准确性更高。 EMSServe 构建在 PyTorch 之上，引入了模态感知模型分割器和特征缓存机制，实现了跨异构硬件的自适应且高效的推理，同时解决了异步模态到达现场的挑战。通过优化 EMS 场景中的多模态推理执行，EMSServe 比直接 PyTorch 多模态推理实现了 1.9 倍至 11.7 倍的加速。六名专业 EMT 的用户研究评估表明，EMSGlass 通过直观的玻璃交互增强了实时态势感知、决策速度和操作效率。此外，用户研究的定性见解为将 EMSGlass 扩展到下一代人工智能支持的 EMS 系统提供了可行的方向，将多模式智能与现实世界的应急响应工作流程联系起来。</li>
</ul>

<h3>Title: Low-Level Dataset Distillation for Medical Image Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Fengzhi Xu, Ziyuan Yang, Mengyu Sun, Joey Tianyi Zhou, Yi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13106">https://arxiv.org/abs/2511.13106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13106">https://arxiv.org/pdf/2511.13106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13106]] Low-Level Dataset Distillation for Medical Image Enhancement(https://arxiv.org/abs/2511.13106)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Medical image enhancement is clinically valuable, but existing methods require large-scale datasets to learn complex pixel-level mappings. However, the substantial training and storage costs associated with these datasets hinder their practical deployment. While dataset distillation (DD) can alleviate these burdens, existing methods mainly target high-level tasks, where multiple samples share the same label. This many-to-one mapping allows distilled data to capture shared semantics and achieve information compression. In contrast, low-level tasks involve a many-to-many mapping that requires pixel-level fidelity, making low-level DD an underdetermined problem, as a small distilled dataset cannot fully constrain the dense pixel-level mappings. To address this, we propose the first low-level DD method for medical image enhancement. We first leverage anatomical similarities across patients to construct the shared anatomical prior based on a representative patient, which serves as the initialization for the distilled data of different patients. This prior is then personalized for each patient using a Structure-Preserving Personalized Generation (SPG) module, which integrates patient-specific anatomical information into the distilled dataset while preserving pixel-level fidelity. For different low-level tasks, the distilled data is used to construct task-specific high- and low-quality training pairs. Patient-specific knowledge is injected into the distilled data by aligning the gradients computed from networks trained on the distilled pairs with those from the corresponding patient's raw data. Notably, downstream users cannot access raw patient data. Instead, only a distilled dataset containing abstract training information is shared, which excludes patient-specific details and thus preserves privacy.</li>
<li><strong>摘要：</strong>医学图像增强具有临床价值，但现有方法需要大规模数据集来学习复杂的像素级映射。然而，与这些数据集相关的大量培训和存储成本阻碍了它们的实际部署。虽然数据集蒸馏（DD）可以减轻这些负担，但现有方法主要针对高级任务​​，其中多个样本共享相同的标签。这种多对一的映射允许提取的数据捕获共享语义并实现信息压缩。相比之下，低级任务涉及需要像素级保真度的多对多映射，这使得低级 DD 成为一个不确定的问题，因为小型蒸馏数据集无法完全约束密集的像素级映射。为了解决这个问题，我们提出了第一个用于医学图像增强的低级 DD 方法。我们首先利用患者之间的解剖相似性，根据代表性患者构建共享的解剖先验，作为不同患者的蒸馏数据的初始化。然后使用结构保留个性化生成 (SPG) 模块对每个患者进行个性化，该模块将患者特定的解剖信息集成到提取的数据集中，同时保留像素级保真度。对于不同的低级任务，提取的数据用于构建特定于任务的高质量和低质量训练对。通过将在蒸馏对上训练的网络计算出的梯度与相应患者的原始数据中的梯度对齐，将患者特定的知识注入到蒸馏数据中。值得注意的是，下游用户无法访问原始患者数据。相反，仅共享包含抽象训练信息的精炼数据集，这排除了患者特定的详细信息，从而保护了隐私。</li>
</ul>

<h3>Title: DGS-Net: Distillation-Guided Gradient Surgery for CLIP Fine-Tuning in AI-Generated Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Jiazhen Yan, Ziqiang Li, Fan Wang, Boyu Wang, Zhangjie Fu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13108">https://arxiv.org/abs/2511.13108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13108">https://arxiv.org/pdf/2511.13108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13108]] DGS-Net: Distillation-Guided Gradient Surgery for CLIP Fine-Tuning in AI-Generated Image Detection(https://arxiv.org/abs/2511.13108)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The rapid progress of generative models such as GANs and diffusion models has led to the widespread proliferation of AI-generated images, raising concerns about misinformation, privacy violations, and trust erosion in digital media. Although large-scale multimodal models like CLIP offer strong transferable representations for detecting synthetic content, fine-tuning them often induces catastrophic forgetting, which degrades pre-trained priors and limits cross-domain generalization. To address this issue, we propose the Distillation-guided Gradient Surgery Network (DGS-Net), a novel framework that preserves transferable pre-trained priors while suppressing task-irrelevant components. Specifically, we introduce a gradient-space decomposition that separates harmful and beneficial descent directions during optimization. By projecting task gradients onto the orthogonal complement of harmful directions and aligning with beneficial ones distilled from a frozen CLIP encoder, DGS-Net achieves unified optimization of prior preservation and irrelevant suppression. Extensive experiments on 50 generative models demonstrate that our method outperforms state-of-the-art approaches by an average margin of 6.6, achieving superior detection performance and generalization across diverse generation techniques.</li>
<li><strong>摘要：</strong>GAN 和扩散模型等生成模型的快速发展导致人工智能生成图像的广泛扩散，引发了人们对数字媒体中的错误信息、隐私侵犯和信任侵蚀的担忧。尽管像 CLIP 这样的大规模多模态模型为检测合成内容提供了强大的可转移表示，但对其进行微调通常会导致灾难性遗忘，从而降低预先训练的先验知识并限制跨域泛化。为了解决这个问题，我们提出了蒸馏引导梯度手术网络（DGS-Net），这是一种新颖的框架，可以保留可转移的预训练先验，同时抑制与任务无关的组件。具体来说，我们引入了梯度空间分解，可以在优化过程中分离有害和有益的下降方向。通过将任务梯度投影到有害方向的正交补集上，并与从冻结的 CLIP 编码器中提取的有益方向对齐，DGS-Net 实现了先前保存和不相关抑制的统一优化。对 50 个生成模型的广泛实验表明，我们的方法比最先进的方法平均领先 6.6，在不同的生成技术中实现了卓越的检测性能和泛化。</li>
</ul>

<h3>Title: Learning Implicit Neural Degradation Representation for Unpaired Image Dehazing</h3>
<ul>
<li><strong>Authors: </strong>Shuaibin Fan, Senming Zhong, Wenchao Yan, Minglong Xue</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13110">https://arxiv.org/abs/2511.13110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13110">https://arxiv.org/pdf/2511.13110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13110]] Learning Implicit Neural Degradation Representation for Unpaired Image Dehazing(https://arxiv.org/abs/2511.13110)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Image dehazing is an important task in the field of computer vision, aiming at restoring clear and detail-rich visual content from haze-affected images. However, when dealing with complex scenes, existing methods often struggle to strike a balance between fine-grained feature representation of inhomogeneous haze distribution and global consistency modeling. Furthermore, to better learn the common degenerate representation of haze in spatial variations, we propose an unsupervised dehaze method for implicit neural degradation representation. Firstly, inspired by the Kolmogorov-Arnold representation theorem, we propose a mechanism combining the channel-independent and channel-dependent mechanisms, which efficiently enhances the ability to learn from nonlinear dependencies. which in turn achieves good visual perception in complex scenes. Moreover, we design an implicit neural representation to model haze degradation as a continuous function to eliminate redundant information and the dependence on explicit feature extraction and physical models. To further learn the implicit representation of the haze features, we also designed a dense residual enhancement module from it to eliminate redundant information. This achieves high-quality image restoration. Experimental results show that our method achieves competitive dehaze performance on various public and real-world datasets. This project code will be available at this https URL.</li>
<li><strong>摘要：</strong>图像去雾是计算机视觉领域的一项重要任务，旨在从受雾霾影响的图像中恢复清晰且细节丰富的视觉内容。然而，在处理复杂场景时，现有方法往往难以在不均匀雾霾分布的细粒度特征表示和全局一致性建模之间取得平衡。此外，为了更好地学习空间变化中雾霾的常见退化表示，我们提出了一种用于隐式神经退化表示的无监督去雾方法。首先，受Kolmogorov-Arnold表示定理的启发，我们提出了一种结合通道无关和通道相关机制的机制，有效增强了从非线性依赖性中学习的能力。进而在复杂场景中实现良好的视觉感知。此外，我们设计了一种隐式神经表示，将雾霾退化建模为连续函数，以消除冗余信息以及对显式特征提取和物理模型的依赖。为了进一步学习雾特征的隐式表示，我们还设计了一个密集残差增强模块来消除冗余信息。这实现了高质量的图像恢复。实验结果表明，我们的方法在各种公共和现实数据集上实现了有竞争力的去雾性能。此项目代码将在此 https URL 中提供。</li>
</ul>

<h3>Title: Synthetic Forgetting without Access: A Few-shot Zero-glance Framework for Machine Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Qipeng Song, Nan Yang, Ziqi Xu, Yue Li, Wei Shao, Feng Xia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13116">https://arxiv.org/abs/2511.13116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13116">https://arxiv.org/pdf/2511.13116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13116]] Synthetic Forgetting without Access: A Few-shot Zero-glance Framework for Machine Unlearning(https://arxiv.org/abs/2511.13116)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generative</a></li>
<li><strong>Abstract: </strong>Machine unlearning aims to eliminate the influence of specific data from trained models to ensure privacy compliance. However, most existing methods assume full access to the original training dataset, which is often impractical. We address a more realistic yet challenging setting: few-shot zero-glance, where only a small subset of the retained data is available and the forget set is entirely inaccessible. We introduce GFOES, a novel framework comprising a Generative Feedback Network (GFN) and a two-phase fine-tuning procedure. GFN synthesises Optimal Erasure Samples (OES), which induce high loss on target classes, enabling the model to forget class-specific knowledge without access to the original forget data, while preserving performance on retained classes. The two-phase fine-tuning procedure enables aggressive forgetting in the first phase, followed by utility restoration in the second. Experiments on three image classification datasets demonstrate that GFOES achieves effective forgetting at both logit and representation levels, while maintaining strong performance using only 5% of the original data. Our framework offers a practical and scalable solution for privacy-preserving machine learning under data-constrained conditions.</li>
<li><strong>摘要：</strong>机器去学习旨在消除经过训练的模型中特定数据的影响，以确保隐私合规性。然而，大多数现有方法都假设可以完全访问原始训练数据集，这通常是不切实际的。我们解决了一个更现实但更具挑战性的设置：少镜头零浏览，其中只有保留数据的一小部分可用，并且忘记集完全无法访问。我们介绍 GFOES，这是一种新颖的框架，由生成反馈网络（GFN）和两阶段微调程序组成。 GFN 综合了最佳擦除样本（OES），这会导致目标类的高损失，使模型能够在不访问原始遗忘数据的情况下忘记特定于类的知识，同时保留保留类的性能。两阶段微调过程可以在第一阶段实现积极的遗忘，然后在第二阶段实现效用恢复。对三个图像分类数据集的实验表明，GFOES 在逻辑和表示级别上实现了有效的遗忘，同时仅使用 5% 的原始数据即可保持强大的性能。我们的框架为数据受限条件下的隐私保护机器学习提供了实用且可扩展的解决方案。</li>
</ul>

<h3>Title: Departures: Distributional Transport for Single-Cell Perturbation Prediction with Neural Schrödinger Bridges</h3>
<ul>
<li><strong>Authors: </strong>Changxi Chi, Yufei Huang, Jun Xia, Jiangbin Zheng, Yunfan Liu, Zelin Zang, Stan Z. Li</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13124">https://arxiv.org/abs/2511.13124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13124">https://arxiv.org/pdf/2511.13124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13124]] Departures: Distributional Transport for Single-Cell Perturbation Prediction with Neural Schrödinger Bridges(https://arxiv.org/abs/2511.13124)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Predicting single-cell perturbation outcomes directly advances gene function analysis and facilitates drug candidate selection, making it a key driver of both basic and translational biomedical research. However, a major bottleneck in this task is the unpaired nature of single-cell data, as the same cell cannot be observed both before and after perturbation due to the destructive nature of sequencing. Although some neural generative transport models attempt to tackle unpaired single-cell perturbation data, they either lack explicit conditioning or depend on prior spaces for indirect distribution alignment, limiting precise perturbation modeling. In this work, we approximate Schrödinger Bridge (SB), which defines stochastic dynamic mappings recovering the entropy-regularized optimal transport (OT), to directly align the distributions of control and perturbed single-cell populations across different perturbation conditions. Unlike prior SB approximations that rely on bidirectional modeling to infer optimal source-target sample coupling, we leverage Minibatch-OT based pairing to avoid such bidirectional inference and the associated ill-posedness of defining the reverse process. This pairing directly guides bridge learning, yielding a scalable approximation to the SB. We approximate two SB models, one modeling discrete gene activation states and the other continuous expression distributions. Joint training enables accurate perturbation modeling and captures single-cell heterogeneity. Experiments on public genetic and drug perturbation datasets show that our model effectively captures heterogeneous single-cell responses and achieves state-of-the-art performance.</li>
<li><strong>摘要：</strong>预测单细胞扰动结果可直接推进基因功能分析并促进候选药物的选择，使其成为基础和转化生物医学研究的关键驱动力。然而，这项任务的一个主要瓶颈是单细胞数据的不配对性质，因为由于测序的破坏性，在扰动之前和之后都无法观察到同一个细胞。尽管一些神经生成传输模型试图处理不成对的单细胞扰动数据，但它们要么缺乏显式条件，要么依赖于先验空间进行间接分布对齐，从而限制了精确的扰动建模。在这项工作中，我们近似薛定谔桥（SB），它定义了恢复熵正则化最佳传输（OT）的随机动态映射，以直接对齐不同扰动条件下的控制和扰动单细胞群体的分布。与之前依赖双向建模来推断最佳源-目标样本耦合的 SB 近似不同，我们利用基于 Minibatch-OT 的配对来避免这种双向推断以及定义反向过程的相关不适定性。这种配对直接指导桥梁学习，产生可扩展的 SB 近似值。我们近似两个 SB 模型，一个建模离散基因激活状态，另一个建模连续表达分布。联合训练可以实现准确的扰动建模并捕获单细胞异质性。对公共遗传和药物扰动数据集的实验表明，我们的模型有效地捕获了异质单细胞反应并实现了最先进的性能。</li>
</ul>

<h3>Title: VEIL: Jailbreaking Text-to-Video Models via Visual Exploitation from Implicit Language</h3>
<ul>
<li><strong>Authors: </strong>Zonghao Ying, Moyang Chen, Nizhang Li, Zhiqiang Wang, Wenxin Zhang, Quanchen Zou, Zonglei Jing, Aishan Liu, Xianglong Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13127">https://arxiv.org/abs/2511.13127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13127">https://arxiv.org/pdf/2511.13127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13127]] VEIL: Jailbreaking Text-to-Video Models via Visual Exploitation from Implicit Language(https://arxiv.org/abs/2511.13127)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Jailbreak attacks can circumvent model safety guardrails and reveal critical blind spots. Prior attacks on text-to-video (T2V) models typically add adversarial perturbations to obviously unsafe prompts, which are often easy to detect and defend. In contrast, we show that benign-looking prompts containing rich, implicit cues can induce T2V models to generate semantically unsafe videos that both violate policy and preserve the original (blocked) intent. To realize this, we propose VEIL, a jailbreak framework that leverages T2V models' cross-modal associative patterns via a modular prompt design. Specifically, our prompts combine three components: neutral scene anchors, which provide the surface-level scene description extracted from the blocked intent to maintain plausibility; latent auditory triggers, textual descriptions of innocuous-sounding audio events (e.g., creaking, muffled noises) that exploit learned audio-visual co-occurrence priors to bias the model toward particular unsafe visual concepts; and stylistic modulators, cinematic directives (e.g., camera framing, atmosphere) that amplify and stabilize the latent trigger's effect. We formalize attack generation as a constrained optimization over the above modular prompt space and solve it with a guided search procedure that balances stealth and effectiveness. Extensive experiments over 7 T2V models demonstrate the efficacy of our attack, achieving a 23 percent improvement in average attack success rate in commercial models.</li>
<li><strong>摘要：</strong>越狱攻击可以绕过模型安全护栏并暴露关键盲点。先前对文本到视频（T2V）模型的攻击通常会对明显不安全的提示添加对抗性扰动，而这些提示通常很容易检测和防御。相比之下，我们表明，包含丰富隐式提示的看似良性的提示可以诱导 T2V 模型生成语义上不安全的视频，这些视频既违反政策又保留原始（被阻止的）意图。为了实现这一点，我们提出了 VEIL，这是一个越狱框架，通过模块化提示设计利用 T2V 模型的跨模式关联模式。具体来说，我们的提示结合了三个组件：中性场景锚点，它提供从被阻止的意图中提取的表面级场景描述以保持合理性；潜在的听觉触发器，对听起来无害的音频事件（例如嘎吱声、低沉的噪音）的文本描述，利用学习到的视听共现先验使模型偏向特定的不安全视觉概念；以及风格调制器、电影指令（例如，摄像机取景、氛围），可以放大和稳定潜在触发器的效果。我们将攻击生成形式化为对上述模块化提示空间的约束优化，并通过平衡隐蔽性和有效性的引导搜索过程来解决它。对 7 个 T2V 模型的大量实验证明了我们的攻击的有效性，将商业模型的平均攻击成功率提高了 23%。</li>
</ul>

<h3>Title: MedGEN-Bench: Contextually entangled benchmark for open-ended multimodal medical generation</h3>
<ul>
<li><strong>Authors: </strong>Junjie Yang, Yuhao Yan, Gang Wu, Yuxuan Wang, Ruoyu Liang, Xinjie Jiang, Xiang Wan, Fenglei Fan, Yongquan Zhang, Feiwei Qin, Changmiao Wan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13135">https://arxiv.org/abs/2511.13135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13135">https://arxiv.org/pdf/2511.13135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13135]] MedGEN-Bench: Contextually entangled benchmark for open-ended multimodal medical generation(https://arxiv.org/abs/2511.13135)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>As Vision-Language Models (VLMs) increasingly gain traction in medical applications, clinicians are progressively expecting AI systems not only to generate textual diagnoses but also to produce corresponding medical images that integrate seamlessly into authentic clinical workflows. Despite the growing interest, existing medical visual benchmarks present notable limitations. They often rely on ambiguous queries that lack sufficient relevance to image content, oversimplify complex diagnostic reasoning into closed-ended shortcuts, and adopt a text-centric evaluation paradigm that overlooks the importance of image generation capabilities. To address these challenges, we introduce \textsc{MedGEN-Bench}, a comprehensive multimodal benchmark designed to advance medical AI research. MedGEN-Bench comprises 6,422 expert-validated image-text pairs spanning six imaging modalities, 16 clinical tasks, and 28 subtasks. It is structured into three distinct formats: Visual Question Answering, Image Editing, and Contextual Multimodal Generation. What sets MedGEN-Bench apart is its focus on contextually intertwined instructions that necessitate sophisticated cross-modal reasoning and open-ended generative outputs, moving beyond the constraints of multiple-choice formats. To evaluate the performance of existing systems, we employ a novel three-tier assessment framework that integrates pixel-level metrics, semantic text analysis, and expert-guided clinical relevance scoring. Using this framework, we systematically assess 10 compositional frameworks, 3 unified models, and 5 VLMs.</li>
<li><strong>摘要：</strong>随着视觉语言模型 (VLM) 在医疗应用中日益受到关注，临床医生逐渐期望人工智能系统不仅能够生成文本诊断，还能生成相应的医学图像，无缝集成到真实的临床工作流程中。尽管人们的兴趣日益浓厚，但现有的医学视觉基准仍存在明显的局限性。他们经常依赖于与图像内容缺乏足够相关性的模糊查询，将复杂的诊断推理过度简化为封闭式的捷径，并采用以文本为中心的评估范式，而忽视了图像生成能力的重要性。为了应对这些挑战，我们引入了 \textsc{MedGEN-Bench}，这是一个旨在推进医学人工智能研究的综合多模态基准。 MedGEN-Bench 包含 6,422 个经过专家验证的图像文本对，涵盖六种成像模式、16 项临床任务和 28 项子任务。它分为三种不同的格式：视觉问答、图像编辑和上下文多模态生成。 MedGEN-Bench 的与众不同之处在于它专注于上下文交织的指令，这些指令需要复杂的跨模式推理和开放式生成输出，超越了多项选择格式的限制。为了评估现有系统的性能，我们采用了一种新颖的三层评估框架，该框架集成了像素级指标、语义文本分析和专家指导的临床相关性评分。使用这个框架，我们系统地评估了 10 个组合框架、3 个统一模型和 5 个 VLM。</li>
</ul>

<h3>Title: Automated Road Distress Detection Using Vision Transformersand Generative Adversarial Networks</h3>
<ul>
<li><strong>Authors: </strong>Cesar Portocarrero Rodriguez, Laura Vandeweyen, Yosuke Yamamoto</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13145">https://arxiv.org/abs/2511.13145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13145">https://arxiv.org/pdf/2511.13145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13145]] Automated Road Distress Detection Using Vision Transformersand Generative Adversarial Networks(https://arxiv.org/abs/2511.13145)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The American Society of Civil Engineers has graded Americas infrastructure condition as a C, with the road system receiving a dismal D. Roads are vital to regional economic viability, yet their management, maintenance, and repair processes remain inefficient, relying on outdated manual or laser-based inspection methods that are both costly and time-consuming. With the increasing availability of real-time visual data from autonomous vehicles, there is an opportunity to apply computer vision (CV) methods for advanced road monitoring, providing insights to guide infrastructure rehabilitation efforts. This project explores the use of state-of-the-art CV techniques for road distress segmentation. It begins by evaluating synthetic data generated with Generative Adversarial Networks (GANs) to assess its usefulness for model training. The study then applies Convolutional Neural Networks (CNNs) for road distress segmentation and subsequently examines the transformer-based model MaskFormer. Results show that GAN-generated data improves model performance and that MaskFormer outperforms the CNN model in two metrics: mAP50 and IoU.</li>
<li><strong>摘要：</strong>美国土木工程师协会将美国的基础设施状况评为 C 级，道路系统的评级为 D 级。道路对于区域经济活力至关重要，但其管理、维护和维修流程仍然效率低下，依赖于过时的手动或激光检查方法，既昂贵又耗时。随着自动驾驶车辆的实时视觉数据的可用性不断增加，我们有机会将计算机视觉 (CV) 方法应用于高级道路监控，从而为指导基础设施修复工作提供见解。该项目探索使用最先进的 CV 技术进行道路故障分割。首先评估生成对抗网络 (GAN) 生成的合成数据，以评估其对模型训练的有用性。然后，该研究应用卷积神经网络 (CNN) 进行道路故障分割，并随后检查基于变压器的模型 MaskFormer。结果表明，GAN 生成的数据提高了模型性能，并且 MaskFormer 在 mAP50 和 IoU 两个指标上优于 CNN 模型。</li>
</ul>

<h3>Title: OTARo: Once Tuning for All Precisions toward Robust On-Device LLMs</h3>
<ul>
<li><strong>Authors: </strong>Shaoyuan Chen, Zhixuan Chen, Dawei Yang, Zhihang Yuan, Qiang Wu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13147">https://arxiv.org/abs/2511.13147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13147">https://arxiv.org/pdf/2511.13147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13147]] OTARo: Once Tuning for All Precisions toward Robust On-Device LLMs(https://arxiv.org/abs/2511.13147)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) fine-tuning techniques not only improve the adaptability to diverse downstream tasks, but also mitigate adverse effects of model quantization. Despite this, conventional quantization suffers from its structural limitation that hinders flexibility during the fine-tuning and deployment stages. Practical on-device tasks demand different quantization precisions (i.e. different bit-widths), e.g., understanding tasks tend to exhibit higher tolerance to reduced precision compared to generation tasks. Conventional quantization, typically relying on scaling factors that are incompatible across bit-widths, fails to support the on-device switching of precisions when confronted with complex real-world scenarios. To overcome the dilemma, we propose OTARo, a novel method that enables on-device LLMs to flexibly switch quantization precisions while maintaining performance robustness through once fine-tuning. OTARo introduces Shared Exponent Floating Point (SEFP), a distinct quantization mechanism, to produce different bit-widths through simple mantissa truncations of a single model. Moreover, to achieve bit-width robustness in downstream applications, OTARo performs a learning process toward losses induced by different bit-widths. The method involves two critical strategies: (1) Exploitation-Exploration Bit-Width Path Search (BPS), which iteratively updates the search path via a designed scoring mechanism; (2) Low-Precision Asynchronous Accumulation (LAA), which performs asynchronous gradient accumulations and delayed updates under low bit-widths. Experiments on popular LLMs, e.g., LLaMA3.2-1B, LLaMA3-8B, demonstrate that OTARo achieves consistently strong and robust performance for all precisions.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）微调技术不仅可以提高对不同下游任务的适应性，还可以减轻模型量化的不利影响。尽管如此，传统的量化仍受到其结构限制，阻碍了微调和部署阶段的灵活性。实际的设备上任务需要不同的量化精度（即不同的位宽），例如，与生成任务相比，理解任务往往对精度降低表现出更高的容忍度。传统的量化通常依赖于跨位宽不兼容的缩放因子，在面对复杂的现实场景时无法支持设备上的精度切换。为了克服这一困境，我们提出了 OTARo，这是一种新颖的方法，使设备上的 LLM 能够灵活地切换量化精度，同时通过一次微调来保持性能鲁棒性。 OTARo 引入了共享指数浮点 (SEFP)，这是一种独特的量化机制，可通过单个模型的简单尾数截断来产生不同的位宽。此外，为了在下游应用中实现位宽鲁棒性，OTARo 针对不同位宽引起的损耗执行学习过程。该方法涉及两个关键策略：（1）利用-探索位宽路径搜索（BPS），它通过设计的评分机制迭代更新搜索路径； (2)低精度异步累加(LAA)，在低位宽下进行异步梯度累加和延迟更新。对流行的 LLM（例如 LLaMA3.2-1B、LLaMA3-8B）进行的实验表明，OTARo 在所有精度下都能实现始终如一的强大和稳健的性能。</li>
</ul>

<h3>Title: HDW-SR: High-Frequency Guided Diffusion Model based on Wavelet Decomposition for Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Chao Yang, Boqian Zhang, Jinghao Xu, Guang Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13175">https://arxiv.org/abs/2511.13175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13175">https://arxiv.org/pdf/2511.13175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13175]] HDW-SR: High-Frequency Guided Diffusion Model based on Wavelet Decomposition for Image Super-Resolution(https://arxiv.org/abs/2511.13175)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, super-resolution</a></li>
<li><strong>Abstract: </strong>Diffusion-based methods have shown great promise in single image super-resolution (SISR); however, existing approaches often produce blurred fine details due to insufficient guidance in the high-frequency domain. To address this issue, we propose a High-Frequency Guided Diffusion Network based on Wavelet Decomposition (HDW-SR), which replaces the conventional U-Net backbone in diffusion frameworks. Specifically, we perform diffusion only on the residual map, allowing the network to focus more effectively on high-frequency information restoration. We then introduce wavelet-based downsampling in place of standard CNN downsampling to achieve multi-scale frequency decomposition, enabling sparse cross-attention between the high-frequency subbands of the pre-super-resolved image and the low-frequency subbands of the diffused image for explicit high-frequency guidance. Moreover, a Dynamic Thresholding Block (DTB) is designed to refine high-frequency selection during the sparse attention process. During upsampling, the invertibility of the wavelet transform ensures low-loss feature reconstruction. Experiments on both synthetic and real-world datasets demonstrate that HDW-SR achieves competitive super-resolution performance, excelling particularly in recovering fine-grained image details. The code will be available after acceptance.</li>
<li><strong>摘要：</strong>基于扩散的方法在单图像超分辨率（SISR）方面显示出了巨大的前景；然而，由于高频域的引导不足，现有方法经常会产生模糊的细节。为了解决这个问题，我们提出了一种基于小波分解的高频引导扩散网络（HDW-SR），它取代了扩散框架中的传统 U-Net 主干网。具体来说，我们仅对残差图进行扩散，使网络能够更有效地专注于高频信息恢复。然后，我们引入基于小波的下采样来代替标准 CNN 下采样，以实现多尺度频率分解，从而实现预超分辨图像的高频子带和扩散图像的低频子带之间的稀疏交叉注意力，以实现显式高频引导。此外，动态阈值块（DTB）旨在细化稀疏注意力过程中的高频选择。在上采样过程中，小波变换的可逆性确保了低损耗的特征重建。对合成数据集和真实数据集的实验表明，HDW-SR 实现了有竞争力的超分辨率性能，尤其是在恢复细粒度图像细节方面表现出色。该代码将在接受后可用。</li>
</ul>

<h3>Title: GenTract: Generative Global Tractography</h3>
<ul>
<li><strong>Authors: </strong>Alec Sargood, Lemuel Puglisi, Elinor Thompson, Mirco Musolesi, Daniel C. Alexander</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13183">https://arxiv.org/abs/2511.13183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13183">https://arxiv.org/pdf/2511.13183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13183]] GenTract: Generative Global Tractography(https://arxiv.org/abs/2511.13183)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Tractography is the process of inferring the trajectories of white-matter pathways in the brain from diffusion magnetic resonance imaging (dMRI). Local tractography methods, which construct streamlines by following local fiber orientation estimates stepwise through an image, are prone to error accumulation and high false positive rates, particularly on noisy or low-resolution data. In contrast, global methods, which attempt to optimize a collection of streamlines to maximize compatibility with underlying fiber orientation estimates, are computationally expensive. To address these challenges, we introduce GenTract, the first generative model for global tractography. We frame tractography as a generative task, learning a direct mapping from dMRI to complete, anatomically plausible streamlines. We compare both diffusion-based and flow matching paradigms and evaluate GenTract's performance against state-of-the-art baselines. Notably, GenTract achieves precision 2.1x higher than the next-best method, TractOracle. This advantage becomes even more pronounced in challenging low-resolution and noisy settings, where it outperforms the closest competitor by an order of magnitude. By producing tractograms with high precision on research-grade data while also maintaining reliability on imperfect, lower-resolution data, GenTract represents a promising solution for global tractography.</li>
<li><strong>摘要：</strong>纤维束成像是通过扩散磁共振成像 (dMRI) 推断大脑白质通路轨迹的过程。局部纤维束成像方法通过图像逐步跟踪局部纤维取向估计来构建流线，容易出现错误累积和高误报率，特别是在噪声或低分辨率数据上。相比之下，全局方法试图优化流线集合以最大限度地提高与底层纤维取向估计的兼容性，但其计算成本很高。为了应对这些挑战，我们推出了 GenTract，这是第一个全球纤维束成像生成模型。我们将纤维束成像视为一项生成任务，学习从 dMRI 到完整的、解剖学上合理的流线的直接映射。我们比较基于扩散的范例和流量匹配范例，并根据最先进的基线评估 GenTract 的性能。值得注意的是，GenTract 的精度比次优方法 TractOracle 高 2.1 倍。在具有挑战性的低分辨率和噪声环境中，这一优势变得更加明显，它的性能比最接近的竞争对手高出一个数量级。通过根据研究级数据生成高精度纤维束图，同时保持不完美、低分辨率数据的可靠性，GenTract 代表了一种有前景的全球纤维束成像解决方案。</li>
</ul>

<h3>Title: DiffFP: Learning Behaviors from Scratch via Diffusion-based Fictitious Play</h3>
<ul>
<li><strong>Authors: </strong>Akash Karthikeyan, Yash Vardhan Pant</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13186">https://arxiv.org/abs/2511.13186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13186">https://arxiv.org/pdf/2511.13186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13186]] DiffFP: Learning Behaviors from Scratch via Diffusion-based Fictitious Play(https://arxiv.org/abs/2511.13186)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Self-play reinforcement learning has demonstrated significant success in learning complex strategic and interactive behaviors in competitive multi-agent games. However, achieving such behaviors in continuous decision spaces remains challenging. Ensuring adaptability and generalization in self-play settings is critical for achieving competitive performance in dynamic multi-agent environments. These challenges often cause methods to converge slowly or fail to converge at all to a Nash equilibrium, making agents vulnerable to strategic exploitation by unseen opponents. To address these challenges, we propose DiffFP, a fictitious play (FP) framework that estimates the best response to unseen opponents while learning a robust and multimodal behavioral policy. Specifically, we approximate the best response using a diffusion policy that leverages generative modeling to learn adaptive and diverse strategies. Through empirical evaluation, we demonstrate that the proposed FP framework converges towards $\epsilon$-Nash equilibria in continuous- space zero-sum games. We validate our method on complex multi-agent environments, including racing and multi-particle zero-sum games. Simulation results show that the learned policies are robust against diverse opponents and outperform baseline reinforcement learning policies. Our approach achieves up to 3$\times$ faster convergence and 30$\times$ higher success rates on average against RL-based baselines, demonstrating its robustness to opponent strategies and stability across training iterations</li>
<li><strong>摘要：</strong>自对弈强化学习在学习竞争性多智能体游戏中的复杂策略和交互行为方面取得了巨大成功。然而，在连续决策空间中实现此类行为仍然具有挑战性。确保自我对战设置的适应性和泛化对于在动态多智能体环境中实现竞争性能至关重要。这些挑战通常会导致方法收敛缓慢或根本无法收敛到纳什均衡，从而使智能体容易受到看不见的对手的战略利用。为了应对这些挑战，我们提出了 DiffFP，这是一种虚拟游戏 (FP) 框架，可以估计对看不见的对手的最佳反应，同时学习稳健的多模式行为策略。具体来说，我们使用扩散策略来近似最佳响应，该扩散策略利用生成模型来学习自适应和多样化的策略。通过实证评估，我们证明了所提出的 FP 框架在连续空间零和博弈中收敛于 $\epsilon$-纳什均衡。我们在复杂的多智能体环境（包括赛车和多粒子零和游戏）上验证了我们的方法。模拟结果表明，学习到的策略对于不同的对手来说都是稳健的，并且优于基线强化学习策略。与基于 RL 的基线相比，我们的方法平均收敛速度提高了 3 倍，成功率提高了 30 倍，这证明了其对对手策略的鲁棒性和跨训练迭代的稳定性</li>
</ul>

<h3>Title: Birth of a Painting: Differentiable Brushstroke Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Ying Jiang, Jiayin Lu, Yunuo Chen, Yumeng He, Kui Wu, Yin Yang, Chenfanfu Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13191">https://arxiv.org/abs/2511.13191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13191">https://arxiv.org/pdf/2511.13191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13191]] Birth of a Painting: Differentiable Brushstroke Reconstruction(https://arxiv.org/abs/2511.13191)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Painting embodies a unique form of visual storytelling, where the creation process is as significant as the final artwork. Although recent advances in generative models have enabled visually compelling painting synthesis, most existing methods focus solely on final image generation or patch-based process simulation, lacking explicit stroke structure and failing to produce smooth, realistic shading. In this work, we present a differentiable stroke reconstruction framework that unifies painting, stylized texturing, and smudging to faithfully reproduce the human painting-smudging loop. Given an input image, our framework first optimizes single- and dual-color Bezier strokes through a parallel differentiable paint renderer, followed by a style generation module that synthesizes geometry-conditioned textures across diverse painting styles. We further introduce a differentiable smudge operator to enable natural color blending and shading. Coupled with a coarse-to-fine optimization strategy, our method jointly optimizes stroke geometry, color, and texture under geometric and semantic guidance. Extensive experiments on oil, watercolor, ink, and digital paintings demonstrate that our approach produces realistic and expressive stroke reconstructions, smooth tonal transitions, and richly stylized appearances, offering a unified model for expressive digital painting creation. See our project page for more demos: this https URL.</li>
<li><strong>摘要：</strong>绘画体现了一种独特的视觉叙事形式，创作过程与最终的艺术品一样重要。尽管生成模型的最新进展已经实现了视觉上引人注目的绘画合成，但大多数现有方法仅专注于最终图像生成或基于补丁的过程模拟，缺乏明确的笔画结构，无法产生平滑、逼真的着色。在这项工作中，我们提出了一个可微的笔画重建框架，它将绘画、风格化纹理和涂抹相结合，以忠实地再现人类绘画涂抹循环。给定输入图像，我们的框架首先通过并行可微分绘画渲染器优化单色和双色贝塞尔笔触，然后是风格生成模块，该模块可以跨不同绘画风格合成几何条件纹理。我们进一步引入了可微的涂抹运算符，以实现自然的颜色混合和阴影。结合从粗到细的优化策略，我们的方法在几何和语义指导下联合优化笔划几何、颜色和纹理。对油画、水彩画、水墨画和数字绘画的广泛实验表明，我们的方法可以产生逼真且富有表现力的笔画重建、平滑的色调过渡和丰富的风格化外观，为富有表现力的数字绘画创作提供统一的模型。请参阅我们的项目页面以获取更多演示：此 https URL。</li>
</ul>

<h3>Title: MRIQT: Physics-Aware Diffusion Model for Image Quality Transfer in Neonatal Ultra-Low-Field MRI</h3>
<ul>
<li><strong>Authors: </strong>Malek Al Abed, Sebiha Demir, Anne Groteklaes, Elodie Germani, Shahrooz Faghihroohi, Hemmen Sabir, Shadi Albarqouni</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13232">https://arxiv.org/abs/2511.13232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13232">https://arxiv.org/pdf/2511.13232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13232]] MRIQT: Physics-Aware Diffusion Model for Image Quality Transfer in Neonatal Ultra-Low-Field MRI(https://arxiv.org/abs/2511.13232)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Portable ultra-low-field MRI (uLF-MRI, 0.064 T) offers accessible neuroimaging for neonatal care but suffers from low signal-to-noise ratio and poor diagnostic quality compared to high-field (HF) MRI. We propose MRIQT, a 3D conditional diffusion framework for image quality transfer (IQT) from uLF to HF MRI. MRIQT combines realistic K-space degradation for physics-consistent uLF simulation, v-prediction with classifier-free guidance for stable image-to-image generation, and an SNR-weighted 3D perceptual loss for anatomical fidelity. The model denoises from a noised uLF input conditioned on the same scan, leveraging volumetric attention-UNet architecture for structure-preserving translation. Trained on a neonatal cohort with diverse pathologies, MRIQT surpasses recent GAN and CNN baselines in PSNR 15.3% with 1.78% over the state of the art, while physicians rated 85% of its outputs as good quality with clear pathology present. MRIQT enables high-fidelity, diffusion-based enhancement of portable ultra-low-field (uLF) MRI for deliable neonatal brain assessment.</li>
<li><strong>摘要：</strong>便携式超低场 MRI（uLF-MRI，0.064 T）为新生儿护理提供了方便的神经成像，但与高场 (HF) MRI 相比，其信噪比低且诊断质量差。我们提出了 MRIQT，一种用于从 uLF 到 HF MRI 的图像质量转移 (IQT) 的 3D 条件扩散框架。 MRIQT 结合了用于物理一致 uLF 模拟的真实 K 空间退化、用于稳定图像到图像生成的无分类器指导的 v 预测，以及用于实现解剖保真度的 SNR 加权 3D 感知损失。该模型利用同一扫描条件下的噪声 uLF 输入进行降噪，利用体积注意力 UNet 架构进行结构保留翻译。在具有不同病理学的新生儿队列上进行训练后，MRIQT 的 PSNR 超过了最近的 GAN 和 CNN 基线 15.3%，比现有技术高出 1.78%，而医生将其 85% 的输出评为质量良好，且病理学清晰。 MRIQT 能够对便携式超低场 (uLF) MRI 进行高保真、基于扩散的增强，以进行可靠的新生儿大脑评估。</li>
</ul>

<h3>Title: Seek and You Shall Fold</h3>
<ul>
<li><strong>Authors: </strong>Nadav Bojan Sellam, Meital Bojan, Paul Schanda, Alex Bronstein</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13244">https://arxiv.org/abs/2511.13244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13244">https://arxiv.org/pdf/2511.13244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13244]] Seek and You Shall Fold(https://arxiv.org/abs/2511.13244)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Accurate protein structures are essential for understanding biological function, yet incorporating experimental data into protein generative models remains a major challenge. Most predictors of experimental observables are non-differentiable, making them incompatible with gradient-based conditional sampling. This is especially limiting in nuclear magnetic resonance, where rich data such as chemical shifts are hard to directly integrate into generative modeling. We introduce a framework for non-differentiable guidance of protein generative models, coupling a continuous diffusion-based generator with any black-box objective via a tailored genetic algorithm. We demonstrate its effectiveness across three modalities: pairwise distance constraints, nuclear Overhauser effect restraints, and for the first time chemical shifts. These results establish chemical shift guided structure generation as feasible, expose key weaknesses in current predictors, and showcase a general strategy for incorporating diverse experimental signals. Our work points toward automated, data-conditioned protein modeling beyond the limits of differentiability.</li>
<li><strong>摘要：</strong>准确的蛋白质结构对于理解生物功能至关重要，但将实验数据纳入蛋白质生成模型仍然是一个重大挑战。大多数实验观测值的预测变量是不可微分的，这使得它们与基于梯度的条件采样不兼容。这在核磁共振中尤其受到限制，因为化学位移等丰富的数据很难直接集成到生成模型中。我们引入了一个不可微指导蛋白质生成模型的框架，通过定制的遗传算法将基于连续扩散的生成器与任何黑盒目标耦合起来。我们证明了其在三种模式下的有效性：成对距离约束、核奥弗豪瑟效应约束以及首次化学位移。这些结果确立了化学位移引导结构生成的可行性，暴露了当前预测因子的关键弱点，并展示了整合不同实验信号的一般策略。我们的工作指向超越可微性限制的自动化、数据调节的蛋白质建模。</li>
</ul>

<h3>Title: KForge: Program Synthesis for Diverse AI Hardware Accelerators</h3>
<ul>
<li><strong>Authors: </strong>Taras Sereda, Tom St. John, Burak Bartan, Natalie Serrino, Sachin Katti, Zain Asgar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MA, cs.PF, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13274">https://arxiv.org/abs/2511.13274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13274">https://arxiv.org/pdf/2511.13274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13274]] KForge: Program Synthesis for Diverse AI Hardware Accelerators(https://arxiv.org/abs/2511.13274)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>GPU kernels are critical for ML performance but difficult to optimize across diverse accelerators. We present KForge, a platform-agnostic framework built on two collaborative LLM-based agents: a generation agent that produces and iteratively refines programs through compilation and correctness feedback, and a performance analysis agent that interprets profiling data to guide optimization. This agent-based architecture requires only a single-shot example to target new platforms. We make three key contributions: (1) introducing an iterative refinement system where the generation agent and performance analysis agent collaborate through functional and optimization passes, interpreting diverse profiling data (from programmatic APIs to GUI-based tools) to generate actionable recommendations that guide program synthesis for arbitrary accelerators; (2) demonstrating that the generation agent effectively leverages cross-platform knowledge transfer, where a reference implementation from one architecture substantially improves generation quality for different hardware targets; and (3) validating the platform-agnostic nature of our approach by demonstrating effective program synthesis across fundamentally different parallel computing platforms: NVIDIA CUDA and Apple Metal.</li>
<li><strong>摘要：</strong>GPU 内核对于机器学习性能至关重要，但很难在不同的加速器上进行优化。我们推出了 KForge，一个平台无关的框架，构建在两个基于 LLM 的协作代理上：一个生成代理通过编译和正确性反馈生成并迭代地完善程序，另一个是性能分析代理，它解释分析数据以指导优化。这种基于代理的架构只需要一个单次示例即可瞄准新平台。我们做出了三个关键贡献：（1）引入了一个迭代细化系统，其中生成代理和性能分析代理通过功能和优化过程进行协作，解释不同的分析数据（从编程 API 到基于 GUI 的工具）以生成可操作的建议，指导任意加速器的程序合成； (2) 证明生成代理有效地利用了跨平台知识转移，其中来自一种架构的参考实现显着提高了不同硬件目标的生成质量； (3) 通过在根本不同的并行计算平台（NVIDIA CUDA 和 Apple Metal）上展示有效的程序合成，验证我们的方法与平台无关的性质。</li>
</ul>

<h3>Title: Towards Metric-Aware Multi-Person Mesh Recovery by Jointly Optimizing Human Crowd in Camera Space</h3>
<ul>
<li><strong>Authors: </strong>Kaiwen Wang, Kaili Zheng, Yiming Shi, Chenyi Guo, Ji Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13282">https://arxiv.org/abs/2511.13282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13282">https://arxiv.org/pdf/2511.13282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13282]] Towards Metric-Aware Multi-Person Mesh Recovery by Jointly Optimizing Human Crowd in Camera Space(https://arxiv.org/abs/2511.13282)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multi-person human mesh recovery from a single image is a challenging task, hindered by the scarcity of in-the-wild training data. Prevailing in-the-wild human mesh pseudo-ground-truth (pGT) generation pipelines are single-person-centric, where each human is processed individually without joint optimization. This oversight leads to a lack of scene-level consistency, producing individuals with conflicting depths and scales within the same image. To address this, we introduce Depth-conditioned Translation Optimization (DTO), a novel optimization-based method that jointly refines the camera-space translations of all individuals in a crowd. By leveraging anthropometric priors on human height and depth cues from a monocular depth estimator, DTO solves for a scene-consistent placement of all subjects within a principled Maximum a posteriori (MAP) framework. Applying DTO to the 4D-Humans dataset, we construct DTO-Humans, a new large-scale pGT dataset of 0.56M high-quality, scene-consistent multi-person images, featuring dense crowds with an average of 4.8 persons per image. Furthermore, we propose Metric-Aware HMR, an end-to-end network that directly estimates human mesh and camera parameters in metric scale. This is enabled by a camera branch and a novel relative metric loss that enforces plausible relative scales. Extensive experiments demonstrate that our method achieves state-of-the-art performance on relative depth reasoning and human mesh recovery. Code and data will be released publicly.</li>
<li><strong>摘要：</strong>从单个图像中恢复多人人体网格是一项具有挑战性的任务，受到野外训练数据稀缺的阻碍。流行的野外人体网格伪地面实况（pGT）生成管道是以单人为中心的，其中每个人都是单独处理的，没有联合优化。这种疏忽导致缺乏场景级一致性，从而在同一图像中产生具有冲突深度和尺度的个体。为了解决这个问题，我们引入了深度条件平移优化（DTO），这是一种基于优化的新颖方法，可以共同细化人群中所有个体的相机空间平移。通过利用来自单眼深度估计器的人体身高和深度线索的人体测量先验，DTO 解决了原则性最大后验 (MAP) 框架内所有对象的场景一致放置问题。将 DTO 应用于 4D-Humans 数据集，我们构建了 DTO-Humans，这是一个新的大规模 pGT 数据集，包含 0.56M 高质量、场景一致的多人图像，其特点是密集的人群，平均每张图像有 4.8 人。此外，我们提出了 Metric-Aware HMR，这是一种端到端网络，可以直接估计公制尺度的人体网格和相机参数。这是通过相机分支和新的相对度量损失来实现的，该相对度量损失强制执行合理的相对比例。大量的实验表明，我们的方法在相对深度推理和人体网格恢复方面实现了最先进的性能。代码和数据将公开发布。</li>
</ul>

<h3>Title: CorrectAD: A Self-Correcting Agentic System to Improve End-to-end Planning in Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Enhui Ma, Lijun Zhou, Tao Tang, Jiahuan Zhang, Junpeng Jiang, Zhan Zhang, Dong Han, Kun Zhan, Xueyang Zhang, XianPeng Lang, Haiyang Sun, Xia Zhou, Di Lin, Kaicheng Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13297">https://arxiv.org/abs/2511.13297</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13297">https://arxiv.org/pdf/2511.13297</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13297]] CorrectAD: A Self-Correcting Agentic System to Improve End-to-end Planning in Autonomous Driving(https://arxiv.org/abs/2511.13297)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>End-to-end planning methods are the de facto standard of the current autonomous driving system, while the robustness of the data-driven approaches suffers due to the notorious long-tail problem (i.e., rare but safety-critical failure cases). In this work, we explore whether recent diffusion-based video generation methods (a.k.a. world models), paired with structured 3D layouts, can enable a fully automated pipeline to self-correct such failure cases. We first introduce an agent to simulate the role of product manager, dubbed PM-Agent, which formulates data requirements to collect data similar to the failure cases. Then, we use a generative model that can simulate both data collection and annotation. However, existing generative models struggle to generate high-fidelity data conditioned on 3D layouts. To address this, we propose DriveSora, which can generate spatiotemporally consistent videos aligned with the 3D annotations requested by PM-Agent. We integrate these components into our self-correcting agentic system, CorrectAD. Importantly, our pipeline is an end-to-end model-agnostic and can be applied to improve any end-to-end planner. Evaluated on both nuScenes and a more challenging in-house dataset across multiple end-to-end planners, CorrectAD corrects 62.5% and 49.8% of failure cases, reducing collision rates by 39% and 27%, respectively.</li>
<li><strong>摘要：</strong>端到端规划方法是当前自动驾驶系统的事实上的标准，而数据驱动方法的稳健性由于臭名昭著的长尾问题（即罕见但安全关键的故障案例）而受到影响。在这项工作中，我们探讨了最近的基于扩散的视频生成方法（也称为世界模型）与结构化 3D 布局相结合，是否可以实现完全自动化的管道来自我纠正此类故障情况。我们首先引入一个代理来模拟产品经理的角色，称为PM-Agent，它制定数据要求来收集类似于故障案例的数据。然后，我们使用可以模拟数据收集和注释的生成模型。然而，现有的生成模型难以生成以 3D 布局为条件的高保真数据。为了解决这个问题，我们提出了 DriveSora，它可以生成与 PM-Agent 请求的 3D 注释一致的时空一致的视频。我们将这些组件集成到我们的自我纠正代理系统 CorrectAD 中。重要的是，我们的管道与端到端模型无关，可用于改进任何端到端规划器。在 nuScenes 和跨多个端到端规划器的更具挑战性的内部数据集上进行评估，CorrectAD 纠正了 62.5% 和 49.8% 的故障案例，分别将冲突率降低了 39% 和 27%。</li>
</ul>

<h3>Title: DriveLiDAR4D: Sequential and Controllable LiDAR Scene Generation for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Kaiwen Cai, Xinze Liu, Xia Zhou, Hengtong Hu, Jie Xiang, Luyao Zhang, Xueyang Zhang, Kun Zhan, Yifei Zhan, Xianpeng Lang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13309">https://arxiv.org/abs/2511.13309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13309">https://arxiv.org/pdf/2511.13309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13309]] DriveLiDAR4D: Sequential and Controllable LiDAR Scene Generation for Autonomous Driving(https://arxiv.org/abs/2511.13309)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The generation of realistic LiDAR point clouds plays a crucial role in the development and evaluation of autonomous driving systems. Although recent methods for 3D LiDAR point cloud generation have shown significant improvements, they still face notable limitations, including the lack of sequential generation capabilities and the inability to produce accurately positioned foreground objects and realistic backgrounds. These shortcomings hinder their practical applicability. In this paper, we introduce DriveLiDAR4D, a novel LiDAR generation pipeline consisting of multimodal conditions and a novel sequential noise prediction model LiDAR4DNet, capable of producing temporally consistent LiDAR scenes with highly controllable foreground objects and realistic backgrounds. To the best of our knowledge, this is the first work to address the sequential generation of LiDAR scenes with full scene manipulation capability in an end-to-end manner. We evaluated DriveLiDAR4D on the nuScenes and KITTI datasets, where we achieved an FRD score of 743.13 and an FVD score of 16.96 on the nuScenes dataset, surpassing the current state-of-the-art (SOTA) method, UniScene, with an performance boost of 37.2% in FRD and 24.1% in FVD, respectively.</li>
<li><strong>摘要：</strong>真实激光雷达点云的生成在自动驾驶系统的开发和评估中起着至关重要的作用。尽管最近的 3D LiDAR 点云生成方法已显示出显着改进，但它们仍然面临明显的局限性，包括缺乏顺序生成功能以及无法生成精确定位的前景物体和真实背景。这些缺点阻碍了它们的实际应用。在本文中，我们介绍了 DriveLiDAR4D，一种由多模态条件和新颖的序列噪声预测模型 LiDAR4DNet 组成的新型 LiDAR 生成管道，能够生成具有高度可控前景物体和真实背景的时间一致的 LiDAR 场景。据我们所知，这是第一个以端到端方式解决具有完整场景操纵能力的激光雷达场景顺序生成的工作。我们在 nuScenes 和 KITTI 数据集上评估了 DriveLiDAR4D，在 nuScenes 数据集上取得了 743.13 的 FRD 分数和 16.96 的 FVD 分数，超越了当前最先进（SOTA）方法 UniScene，FRD 性能提升了 37.2%，FVD 性能提升了 24.1%。</li>
</ul>

<h3>Title: Statistically Accurate and Robust Generative Prediction of Rock Discontinuities with A Tabular Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Han Meng, Gang Mei, Hong Tian, Nengxiong Xu, Jianbing Peng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13339">https://arxiv.org/abs/2511.13339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13339">https://arxiv.org/pdf/2511.13339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13339]] Statistically Accurate and Robust Generative Prediction of Rock Discontinuities with A Tabular Foundation Model(https://arxiv.org/abs/2511.13339)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Rock discontinuities critically govern the mechanical behavior and stability of rock masses. Their internal distributions remain largely unobservable and are typically inferred from surface-exposed discontinuities using generative prediction approaches. However, surface-exposed observations are inherently sparse, and existing generative prediction approaches either fail to capture the underlying complex distribution patterns or lack robustness under data-sparse conditions. Here, we proposed a simple yet robust approach for statistically accurate generative prediction of rock discontinuities by utilizing a tabular foundation model. By leveraging the powerful sample learning capability of the foundation model specifically designed for small data, our approach can effectively capture the underlying complex distribution patterns within limited measured discontinuities. Comparative experiments on ten datasets with diverse scales and distribution patterns of discontinuities demonstrate superior accuracy and robustness over conventional statistical models and deep generative approaches. This work advances quantitative characterization of rock mass structures, supporting safer and more reliable data-driven geotechnical design.</li>
<li><strong>摘要：</strong>岩石不连续性对岩体的力学行为和稳定性起着至关重要的作用。它们的内部分布在很大程度上仍然是不可观察的，并且通常是使用生成预测方法从表面暴露的不连续性推断出来的。然而，表面暴露的观测本质上是稀疏的，现有的生成预测方法要么无法捕获潜在的复杂分布模式，要么在数据稀疏条件下缺乏鲁棒性。在这里，我们提出了一种简单而稳健的方法，通过利用表格基础模型对岩石不连续性进行统计准确的生成预测。通过利用专为小数据设计的基础模型强大的样本学习能力，我们的方法可以有效地捕获有限测量不连续性内的底层复杂分布模式。对具有不同尺度和不连续性分布模式的十个数据集进行的比较实验表明，比传统统计模型和深度生成方法具有更高的准确性和鲁棒性。这项工作推进了岩体结构的定量表征，支持更安全、更可靠的数据驱动岩土设计。</li>
</ul>

<h3>Title: Semi-Supervised Multi-Task Learning for Interpretable Quality As- sessment of Fundus Images</h3>
<ul>
<li><strong>Authors: </strong>Lucas Gabriel Telesco, Danila Nejamkin, Estefanía Mata, Francisco Filizzola, Kevin Wignall, Lucía Franco Troilo, María de los Angeles Cenoz, Melissa Thompson, Mercedes Leguía, Ignacio Larrabide, José Ignacio Orlando</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13353">https://arxiv.org/abs/2511.13353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13353">https://arxiv.org/pdf/2511.13353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13353]] Semi-Supervised Multi-Task Learning for Interpretable Quality As- sessment of Fundus Images(https://arxiv.org/abs/2511.13353)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Retinal image quality assessment (RIQA) supports computer-aided diagnosis of eye diseases. However, most tools classify only overall image quality, without indicating acquisition defects to guide recapture. This gap is mainly due to the high cost of detailed annotations. In this paper, we aim to mitigate this limitation by introducing a hybrid semi-supervised learning approach that combines manual labels for overall quality with pseudo-labels of quality details within a multi-task framework. Our objective is to obtain more interpretable RIQA models without requiring extensive manual labeling. Pseudo-labels are generated by a Teacher model trained on a small dataset and then used to fine-tune a pre-trained model in a multi-task setting. Using a ResNet-18 backbone, we show that these weak annotations improve quality assessment over single-task baselines (F1: 0.875 vs. 0.863 on EyeQ, and 0.778 vs. 0.763 on DeepDRiD), matching or surpassing existing methods. The multi-task model achieved performance statistically comparable to the Teacher for most detail prediction tasks (p > 0.05). In a newly annotated EyeQ subset released with this paper, our model performed similarly to experts, suggesting that pseudo-label noise aligns with expert variability. Our main finding is that the proposed semi-supervised approach not only improves overall quality assessment but also provides interpretable feedback on capture conditions (illumination, clarity, contrast). This enhances interpretability at no extra manual labeling cost and offers clinically actionable outputs to guide image recapture.</li>
<li><strong>摘要：</strong>视网膜图像质量评估（RIQA）支持眼部疾病的计算机辅助诊断。然而，大多数工具仅对整体图像质量进行分类，而没有指示采集缺陷来指导重新采集。这种差距主要是由于详细注释的成本较高。在本文中，我们的目标是通过引入一种混合半监督学习方法来减轻这一限制，该方法将整体质量的手动标签与多任务框架内的质量细节的伪标签相结合。我们的目标是获得更具可解释性的 RIQA 模型，而不需要大量的手动标记。伪标签由在小数据集上训练的教师模型生成，然后用于在多任务设置中微调预训练模型。使用 ResNet-18 主干，我们表明这些弱注释可以改善单任务基线的质量评估（F1：EyeQ 上的 0.875 与 0.863，DeepDRiD 上的 0.778 与 0.763），匹配或超越现有方法。对于大多数细节预测任务，多任务模型的性能在统计上与 Teacher 相当（p > 0.05）。在随本文发布的新注释的 EyeQ 子集中，我们的模型的表现与专家类似，表明伪标签噪声与专家变异性一致。我们的主要发现是，所提出的半监督方法不仅提高了整体质量评估，而且还提供了有关捕获条件（照明、清晰度、对比度）的可解释的反馈。这增强了可解释性，无需额外的手动标记成本，并提供临床上可操作的输出来指导图像重新捕获。</li>
</ul>

<h3>Title: Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline</h3>
<ul>
<li><strong>Authors: </strong>Rui Zuo, Qinyue Tong, Zhe-Ming Lu, Ziqian Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13442">https://arxiv.org/abs/2511.13442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13442">https://arxiv.org/pdf/2511.13442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13442]] Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline(https://arxiv.org/abs/2511.13442)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of artificial intelligence-generated content (AIGC) technologies, including multimodal large language models (MLLMs) and diffusion models, image generation and manipulation have become remarkably effortless. Existing image forgery detection and localization (IFDL) methods often struggle to generalize across diverse datasets and offer limited interpretability. Nowadays, MLLMs demonstrate strong generalization potential across diverse vision-language tasks, and some studies introduce this capability to IFDL via large-scale training. However, such approaches cost considerable computational resources, while failing to reveal the inherent generalization potential of vanilla MLLMs to address this problem. Inspired by this observation, we propose Foresee, a training-free MLLM-based pipeline tailored for image forgery analysis. It eliminates the need for additional training and enables a lightweight inference process, while surpassing existing MLLM-based methods in both tamper localization accuracy and the richness of textual explanations. Foresee employs a type-prior-driven strategy and utilizes a Flexible Feature Detector (FFD) module to specifically handle copy-move manipulations, thereby effectively unleashing the potential of vanilla MLLMs in the forensic domain. Extensive experiments demonstrate that our approach simultaneously achieves superior localization accuracy and provides more comprehensive textual explanations. Moreover, Foresee exhibits stronger generalization capability, outperforming existing IFDL methods across various tampering types, including copy-move, splicing, removal, local enhancement, deepfake, and AIGC-based editing. The code will be released in the final version.</li>
<li><strong>摘要：</strong>随着人工智能生成内容 (AIGC) 技术（包括多模态大语言模型 (MLLM) 和扩散模型）的快速发展，图像生成和操作变得非常轻松。现有的图像伪造检测和定位（IFDL）方法通常很难在不同的数据集上进行泛化，并且可解释性有限。如今，MLLM 在不同的视觉语言任务中表现出强大的泛化潜力，一些研究通过大规模训练将这种能力引入 IFDL。然而，此类方法耗费大量计算资源，同时未能揭示普通 MLLM 解决此问题的固有泛化潜力。受这一观察的启发，我们提出了 Foresee，这是一种专为图像伪造分析而定制的基于 MLLM 的免训练管道。它消除了额外训练的需要，并实现了轻量级推理过程，同时在篡改定位准确性和文本解释的丰富性方面超越了现有的基于 MLLM 的方法。 Foresee 采用类型优先驱动策略，并利用灵活特征检测器 (FFD) 模块专门处理复制移动操作，从而有效释放普通 MLLM 在取证领域的潜力。大量的实验表明，我们的方法同时实现了卓越的定位精度并提供了更全面的文本解释。此外，Foresee 表现出更强的泛化能力，在各种篡改类型上都优于现有的 IFDL 方法，包括复制移动、拼接、删除、局部增强、深度伪造和基于 AIGC 的编辑。该代码将在最终版本中发布。</li>
</ul>

<h3>Title: InterMoE: Individual-Specific 3D Human Interaction Generation via Dynamic Temporal-Selective MoE</h3>
<ul>
<li><strong>Authors: </strong>Lipeng Wang, Hongxing Fan, Haohua Chen, Zehuan Huang, Lu Sheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13488">https://arxiv.org/abs/2511.13488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13488">https://arxiv.org/pdf/2511.13488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13488]] InterMoE: Individual-Specific 3D Human Interaction Generation via Dynamic Temporal-Selective MoE(https://arxiv.org/abs/2511.13488)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating high-quality human interactions holds significant value for applications like virtual reality and robotics. However, existing methods often fail to preserve unique individual characteristics or fully adhere to textual descriptions. To address these challenges, we introduce InterMoE, a novel framework built on a Dynamic Temporal-Selective Mixture of Experts. The core of InterMoE is a routing mechanism that synergistically uses both high-level text semantics and low-level motion context to dispatch temporal motion features to specialized experts. This allows experts to dynamically determine the selection capacity and focus on critical temporal features, thereby preserving specific individual characteristic identities while ensuring high semantic fidelity. Extensive experiments show that InterMoE achieves state-of-the-art performance in individual-specific high-fidelity 3D human interaction generation, reducing FID scores by 9% on the InterHuman dataset and 22% on InterX.</li>
<li><strong>摘要：</strong>生成高质量的人类交互对于虚拟现实和机器人等应用具有重要价值。然而，现有的方法往往无法保留独特的个体特征或完全遵循文本描述。为了应对这些挑战，我们引入了 InterMoE，这是一种建立在动态时间选择性专家混合之上的新颖框架。 InterMoE 的核心是一种路由机制，它协同使用高级文本语义和低级运动上下文，将时间运动特征分派给专业专家。这使得专家能够动态确定选择能力并关注关键的时间特征，从而保留特定的个体特征身份，同时确保高语义保真度。大量实验表明，InterMoE 在个体特定的高保真 3D 人机交互生成方面实现了最先进的性能，在 InterHuman 数据集上将 FID 分数降低了 9%，在 InterX 上降低了 22%。</li>
</ul>

<h3>Title: Minimax Multi-Target Conformal Prediction with Applications to Imaging Inverse Problems</h3>
<ul>
<li><strong>Authors: </strong>Jeffrey Wen, Rizwan Ahmad, Philip Schniter</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13533">https://arxiv.org/abs/2511.13533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13533">https://arxiv.org/pdf/2511.13533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13533]] Minimax Multi-Target Conformal Prediction with Applications to Imaging Inverse Problems(https://arxiv.org/abs/2511.13533)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>In ill-posed imaging inverse problems, uncertainty quantification remains a fundamental challenge, especially in safety-critical applications. Recently, conformal prediction has been used to quantify the uncertainty that the inverse problem contributes to downstream tasks like image classification, image quality assessment, fat mass quantification, etc. While existing works handle only a scalar estimation target, practical applications often involve multiple targets. In response, we propose an asymptotically minimax approach to multi-target conformal prediction that provides tight prediction intervals while ensuring joint marginal coverage. We then outline how our minimax approach can be applied to multi-metric blind image quality assessment, multi-task uncertainty quantification, and multi-round measurement acquisition. Finally, we numerically demonstrate the benefits of our minimax method, relative to existing multi-target conformal prediction methods, using both synthetic and magnetic resonance imaging (MRI) data.</li>
<li><strong>摘要：</strong>在不适定成像反问题中，不确定性量化仍然是一个基本挑战，特别是在安全关键型应用中。最近，保形预测已被用来量化逆问题对图像分类、图像质量评估、脂肪量量化等下游任务的不确定性。虽然现有的工作仅处理标量估计目标，但实际应用通常涉及多个目标。作为回应，我们提出了一种渐进极小极大方法来进行多目标共形预测，该方法提供严格的预测区间，同时确保联合边缘覆盖。然后，我们概述了如何将极小极大方法应用于多度量盲图像质量评估、多任务不确定性量化和多轮测量采集。最后，我们使用合成数据和磁共振成像（MRI）数据，以数值方式证明了极小极大方法相对于现有多目标保形预测方法的优势。</li>
</ul>

<h3>Title: Hierarchical Prompt Learning for Image- and Text-Based Person Re-Identification</h3>
<ul>
<li><strong>Authors: </strong>Linhan Zhou, Shuang Li, Neng Dong, Yonghang Tai, Yafei Zhang, Huafeng Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13575">https://arxiv.org/abs/2511.13575</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13575">https://arxiv.org/pdf/2511.13575</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13575]] Hierarchical Prompt Learning for Image- and Text-Based Person Re-Identification(https://arxiv.org/abs/2511.13575)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Person re-identification (ReID) aims to retrieve target pedestrian images given either visual queries (image-to-image, I2I) or textual descriptions (text-to-image, T2I). Although both tasks share a common retrieval objective, they pose distinct challenges: I2I emphasizes discriminative identity learning, while T2I requires accurate cross-modal semantic alignment. Existing methods often treat these tasks separately, which may lead to representation entanglement and suboptimal performance. To address this, we propose a unified framework named Hierarchical Prompt Learning (HPL), which leverages task-aware prompt modeling to jointly optimize both tasks. Specifically, we first introduce a Task-Routed Transformer, which incorporates dual classification tokens into a shared visual encoder to route features for I2I and T2I branches respectively. On top of this, we develop a hierarchical prompt generation scheme that integrates identity-level learnable tokens with instance-level pseudo-text tokens. These pseudo-tokens are derived from image or text features via modality-specific inversion networks, injecting fine-grained, instance-specific semantics into the prompts. Furthermore, we propose a Cross-Modal Prompt Regularization strategy to enforce semantic alignment in the prompt token space, ensuring that pseudo-prompts preserve source-modality characteristics while enhancing cross-modal transferability. Extensive experiments on multiple ReID benchmarks validate the effectiveness of our method, achieving state-of-the-art performance on both I2I and T2I tasks.</li>
<li><strong>摘要：</strong>行人重新识别（ReID）旨在根据视觉查询（图像到图像，I2I）或文本描述（文本到图像，T2I）检索目标行人图像。尽管这两个任务具有共同的检索目标，但它们提出了不同的挑战：I2I 强调判别性身份学习，而 T2I 需要准确的跨模态语义对齐。现有的方法通常单独处理这些任务，这可能会导致表示纠缠和性能不佳。为了解决这个问题，我们提出了一个名为分层提示学习（HPL）的统一框架，它利用任务感知提示建模来联合优化这两项任务。具体来说，我们首先引入一个任务路由变压器，它将双重分类标记合并到共享视觉编码器中，以分别为 I2I 和 T2I 分支路由特征。除此之外，我们开发了一种分层提示生成方案，将身份级可学习标记与实例级伪文本标记集成在一起。这些伪标记是通过特定于模态的反转网络从图像或文本特征中派生出来的，将细粒度的、特定于实例的语义注入到提示中。此外，我们提出了一种跨模态提示正则化策略，以强制提示标记空间中的语义对齐，确保伪提示保留源模态特征，同时增强跨模态可转移性。对多个 ReID 基准的大量实验验证了我们方法的有效性，在 I2I 和 T2I 任务上实现了最先进的性能。</li>
</ul>

<h3>Title: VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping</h3>
<ul>
<li><strong>Authors: </strong>Haotian Dong, Ye Li, Rongwei Lu, Chen Tang, Shu-Tao Xia, Zhi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13587">https://arxiv.org/abs/2511.13587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13587">https://arxiv.org/pdf/2511.13587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13587]] VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping(https://arxiv.org/abs/2511.13587)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its "draft one step, then verify one step" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm.</li>
<li><strong>摘要：</strong>视觉自回归 (AR) 生成模型已展现出图像生成的强大潜力，但其下一个标记预测范式引入了相当大的推理延迟。尽管推测性解码 (SD) 已被证明对于加速视觉 AR 模型有效，但其“起草一步，然后验证一步”范式阻止了前向传递的直接减少，从而限制了加速潜力。受视觉令牌互换性的推动，我们首次探索视觉 AR 模型生成的 SD 过程中的验证跳过，以显式减少目标模型前向传递的数量，从而减少推理延迟。基于对起草阶段特征的分析，我们观察到验证冗余和过时特征的可重用性是保持生成质量和加速免验证步骤的关键因素。受这两个观察的启发，我们提出了一种新颖的 SD 框架 VVS，通过部分验证跳过来加速视觉 AR 生成，该框架集成了三个互补模块：（1）具有动态截断的免验证令牌选择器，（2）令牌级特征缓存和重用，以及（3）细粒度的跳过步骤调度。因此，与普通 AR 解码相比，VVS 将目标模型前向传递的数量减少了 2.8 美元\倍$，同时保持有竞争力的生成质量，提供了优于传统 SD 框架的速度与质量权衡，并揭示了重塑 SD 范式的强大潜力。</li>
</ul>

<h3>Title: ICLR: Inter-Chrominance and Luminance Interaction for Natural Color Restoration in Low-Light Image Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Xin Xu, Hao Liu, Wei Liu, Wei Wang, Jiayi Wu, Kui Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13607">https://arxiv.org/abs/2511.13607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13607">https://arxiv.org/pdf/2511.13607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13607]] ICLR: Inter-Chrominance and Luminance Interaction for Natural Color Restoration in Low-Light Image Enhancement(https://arxiv.org/abs/2511.13607)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Low-Light Image Enhancement (LLIE) task aims at improving contrast while restoring details and textures for images captured in low-light conditions. HVI color space has made significant progress in this task by enabling precise decoupling of chrominance and luminance. However, for the interaction of chrominance and luminance branches, substantial distributional differences between the two branches prevalent in natural images limit complementary feature extraction, and luminance errors are propagated to chrominance channels through the nonlinear parameter. Furthermore, for interaction between different chrominance branches, images with large homogeneous-color regions usually exhibit weak correlation between chrominance branches due to concentrated distributions. Traditional pixel-wise losses exploit strong inter-branch correlations for co-optimization, causing gradient conflicts in weakly correlated regions. Therefore, we propose an Inter-Chrominance and Luminance Interaction (ICLR) framework including a Dual-stream Interaction Enhancement Module (DIEM) and a Covariance Correction Loss (CCL). The DIEM improves the extraction of complementary information from two dimensions, fusion and enhancement, respectively. The CCL utilizes luminance residual statistics to penalize chrominance errors and balances gradient conflicts by constraining chrominance branches covariance. Experimental results on multiple datasets show that the proposed ICLR framework outperforms state-of-the-art methods.</li>
<li><strong>摘要：</strong>低光图像增强（LLIE）任务旨在提高对比度，同时恢复低光条件下拍摄的图像的细节和纹理。 HVI 色彩空间通过实现色度和亮度的精确解耦，在这项任务中取得了重大进展。然而，对于色度和亮度分支的相互作用，自然图像中普遍存在的两个分支之间的显着分布差异限制了互补特征提取，并且亮度误差通过非线性参数传播到色度通道。此外，对于不同色度分支之间的相互作用，具有大的同质颜色区域的图像通常由于集中分布而表现出色度分支之间的弱相关性。传统的像素损失利用强的分支间相关性进行协同优化，导致弱相关区域的梯度冲突。因此，我们提出了一种色度和亮度间交互（ICLR）框架，包括双流交互增强模块（DIEM）和协方差校正损失（CCL）。 DIEM 分别改进了从二维、融合和增强中提取互补信息的能力。 CCL 利用亮度残差统计来惩罚色度误差并通过约束色度分支协方差来平衡梯度冲突。多个数据集上的实验结果表明，所提出的 ICLR 框架优于最先进的方法。</li>
</ul>

<h3>Title: Data Value in the Age of Scaling: Understanding LLM Scaling Dynamics Under Real-Synthetic Data Mixtures</h3>
<ul>
<li><strong>Authors: </strong>Haohui Wang, Jingyuan Qi, Jianpeng Chen, Jun Wu, Lifu Huang, Lecheng Zheng, Kevin Choi, Balaji Veeramani, Edward Bowen, Alison Hu, Tyler Cody, Dawei Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13640">https://arxiv.org/abs/2511.13640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13640">https://arxiv.org/pdf/2511.13640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13640]] Data Value in the Age of Scaling: Understanding LLM Scaling Dynamics Under Real-Synthetic Data Mixtures(https://arxiv.org/abs/2511.13640)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The rapid progress of large language models (LLMs) is fueled by the growing reliance on datasets that blend real and synthetic data. While synthetic data offers scalability and cost-efficiency, it often introduces systematic distributional discrepancies, particularly underrepresenting long-tail knowledge due to truncation effects from data generation mechanisms like top-p sampling, temperature scaling, and finite sampling. These discrepancies pose fundamental challenges in characterizing and evaluating the utility of mixed real-synthetic datasets. In this paper, we identify a three-phase scaling behavior characterized by two breakpoints that reflect transitions in model behavior across learning head and tail knowledge. We further derive an LLM generalization bound designed for real and synthetic mixtures, revealing several key factors that govern their generalization performance. Building on our theoretical findings, we propose an effective yet efficient data valuation method that scales to large-scale datasets. Comprehensive experiments across four tasks, including image classification, sentiment classification, instruction following, and complex reasoning, demonstrate that our method surpasses state-of-the-art baselines in data valuation with significantly low computational cost.</li>
<li><strong>摘要：</strong>对混合真实数据和合成数据的数据集的日益依赖推动了大型语言模型 (LLM) 的快速发展。虽然合成数据提供了可扩展性和成本效率，但它通常会引入系统分布差异，特别是由于数据生成机制（如 top-p 采样、温度缩放和有限采样）的截断效应而导致长尾知识的代表性不足。这些差异对表征和评估混合真实合成数据集的效用提出了根本性挑战。在本文中，我们确定了一个以两个断点为特征的三相扩展行为，这两个断点反映了学习头部和尾部知识的模型行为的转变。我们进一步推导了为真实和合成混合物设计的 LLM 泛化界限，揭示了控制其泛化性能的几个关键因素。基于我们的理论发现，我们提出了一种有效且高效的数据评估方法，可以扩展到大规模数据集。涵盖图像分类、情感分类、指令跟踪和复杂推理等四项任务的综合实验表明，我们的方法超越了数据评估中最先进的基线，且计算成本显着降低。</li>
</ul>

<h3>Title: CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Shrenik Patel, Daivik Patel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13644">https://arxiv.org/abs/2511.13644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13644">https://arxiv.org/pdf/2511.13644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13644]] CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding(https://arxiv.org/abs/2511.13644)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Long-form video question answering (VQA) overwhelms current vision-language models (VLMs) because attention and key-value (KV) caches grow with runtime, forcing either expensive inference or near-sighted sliding windows. We introduce CacheFlow, a training-free pipeline that pairs Dynamic Token Dropping (DTD) with a compressive long-term memory. DTD prunes per-patch tokens online via cosine similarity to the previous frame, and surviving tokens are packed into fixed-size blocks. This online, per-frame processing makes our approach fundamentally suited for live streaming VQA. As blocks are processed, each one's keys are summarized by a tiny recurrent encoder to form a retrieval index, while the block's full KV pairs are offloaded and later rehydrated for generation, preserving answer fidelity. At inference, a consensus-based retrieval mechanism retrieves only the Top-K most relevant blocks and attends over both the retrieved and local context for precise, long-range reasoning. CacheFlow is drop-in, architecture-agnostic, and requires no fine-tuning. Experiments on both offline and streaming VQA benchmarks demonstrate that CacheFlow outperforms current strong baselines, while processing up to 87% less tokens. Our dual approach enables VLMs to be both efficient and context-aware, paving the way for practical long-form video understanding.</li>
<li><strong>摘要：</strong>长格式视频问答 (VQA) 压倒了当前的视觉语言模型 (VLM)，因为注意力和键值 (KV) 缓存会随着运行时间的推移而增长，从而迫使进行昂贵的推理或近视滑动窗口。我们引入了 CacheFlow，这是一种无需训练的管道，它将动态令牌删除 (DTD) 与压缩长期内存配对。 DTD 通过与前一帧的余弦相似度在线修剪每个补丁标记，并将幸存的标记打包到固定大小的块中。这种在线的每帧处理使我们的方法从根本上适合实时流媒体 VQA。当块被处理时，每个块的密钥都会被一个微小的循环编码器汇总以形成检索索引，而块的完整 KV 对则被卸载并随后重新生成以保持答案保真度。在推理时，基于共识的检索机制仅检索 Top-K 最相关的块，并关注检索到的上下文和本地上下文，以进行精确的远程推理。 CacheFlow 是直接插入的、与体系结构无关的，并且不需要微调。离线和流式 VQA 基准测试表明，CacheFlow 的性能优于当前的强大基准，同时处理的令牌数量减少了 87%。我们的双重方法使 VLM 既高效又具有上下文感知能力，为实际的长格式视频理解铺平了道路。</li>
</ul>

<h3>Title: Part-X-MLLM: Part-aware 3D Multimodal Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Chunshi Wang, Junliang Ye, Yunhan Yang, Yang Li, Zizhuo Lin, Jun Zhu, Zhuo Chen, Yawei Luo, Chunchao Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13647">https://arxiv.org/abs/2511.13647</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13647">https://arxiv.org/pdf/2511.13647</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13647]] Part-X-MLLM: Part-aware 3D Multimodal Large Language Model(https://arxiv.org/abs/2511.13647)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\&A, compositional generation, and localized editing through one unified interface. Project page: this https URL</li>
<li><strong>摘要：</strong>我们引入了 Part-X-MLLM，这是一种原生 3D 多模态大语言模型，通过将不同的 3D 任务制定为结构化可执行语法中的程序来统一这些任务。给定 RGB 点云和自然语言提示，我们的模型自回归生成单个连贯的标记序列，编码部分级边界框、语义描述和编辑命令。这种结构化输出可作为通用接口来驱动下游几何感知模块以进行基于零件的生成和编辑。通过将符号规划与几何合成分离，我们的方法允许通过单一的、本地语言的前端来控制任何兼容的几何引擎。我们预训练双编码器架构，以将结构与语义分开，并在大规模、以部分为中心的数据集上对模型进行指令调整。实验表明，我们的模型擅长生成高质量的结构化计划，通过一个统一的界面在扎根问答、构图生成和本地化编辑方面实现最先进的性能。项目页面：此 https URL</li>
</ul>

<h3>Title: PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image</h3>
<ul>
<li><strong>Authors: </strong>Ziang Cao, Fangzhou Hong, Zhaoxi Chen, Liang Pan, Ziwei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13648">https://arxiv.org/abs/2511.13648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13648">https://arxiv.org/pdf/2511.13648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13648]] PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image(https://arxiv.org/abs/2511.13648)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>3D modeling is shifting from static visual representations toward physical, articulated assets that can be directly used in simulation and interaction. However, most existing 3D generation methods overlook key physical and articulation properties, thereby limiting their utility in embodied AI. To bridge this gap, we introduce PhysX-Anything, the first simulation-ready physical 3D generative framework that, given a single in-the-wild image, produces high-quality sim-ready 3D assets with explicit geometry, articulation, and physical attributes. Specifically, we propose the first VLM-based physical 3D generative model, along with a new 3D representation that efficiently tokenizes geometry. It reduces the number of tokens by 193x, enabling explicit geometry learning within standard VLM token budgets without introducing any special tokens during fine-tuning and significantly improving generative quality. In addition, to overcome the limited diversity of existing physical 3D datasets, we construct a new dataset, PhysX-Mobility, which expands the object categories in prior physical 3D datasets by over 2x and includes more than 2K common real-world objects with rich physical annotations. Extensive experiments on PhysX-Mobility and in-the-wild images demonstrate that PhysX-Anything delivers strong generative performance and robust generalization. Furthermore, simulation-based experiments in a MuJoCo-style environment validate that our sim-ready assets can be directly used for contact-rich robotic policy learning. We believe PhysX-Anything can substantially empower a broad range of downstream applications, especially in embodied AI and physics-based simulation.</li>
<li><strong>摘要：</strong>3D 建模正在从静态视觉表示转向可直接用于模拟和交互的物理、铰接资产。然而，大多数现有的 3D 生成方法忽视了关键的物理和关节属性，从而限制了它们在具体人工智能中的实用性。为了弥补这一差距，我们引入了 PhysX-Anything，这是第一个可用于模拟的物理 3D 生成框架，只要给定单个野外图像，即可生成具有显式几何、关节和物理属性的高质量可用于模拟的 3D 资源。具体来说，我们提出了第一个基于 VLM 的物理 3D 生成模型，以及有效标记几何的新 3D 表示。它将令牌数量减少了 193 倍，在标准 VLM 令牌预算内实现显式几何学习，而无需在微调过程中引入任何特殊令牌，并显着提高生成质量。此外，为了克服现有物理 3D 数据集的有限多样性，我们构建了一个新的数据集 PhysX-Mobility，它将先前物理 3D 数据集中的对象类别扩展了 2 倍以上，并包含超过 2K 个具有丰富物理注释的常见现实世界对象。对 PhysX-Mobility 和野外图像的大量实验表明，PhysX-Anything 具有强大的生成性能和稳健的泛化能力。此外，在 MuJoCo 风格的环境中基于模拟的实验验证了我们的模拟就绪资产可以直接用于接触丰富的机器人策略学习。我们相信 PhysX-Anything 可以极大地增强广泛的下游应用程序的能力，特别是在具体的人工智能和基于物理的模拟方面。</li>
</ul>

<h3>Title: Scientific Data Compression and Super-Resolution Sampling</h3>
<ul>
<li><strong>Authors: </strong>Minh Vu, Andrey Lokhov</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.data-an, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13675">https://arxiv.org/abs/2511.13675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13675">https://arxiv.org/pdf/2511.13675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13675]] Scientific Data Compression and Super-Resolution Sampling(https://arxiv.org/abs/2511.13675)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Modern scientific simulations, observations, and large-scale experiments generate data at volumes that often exceed the limits of storage, processing, and analysis. This challenge drives the development of data reduction methods that efficiently manage massive datasets while preserving essential physical features and quantities of interest. In many scientific workflows, it is also crucial to enable data recovery from compressed representations - a task known as super-resolution - with guarantees on the preservation of key physical characteristics. A notable example is checkpointing and restarting, which is essential for long-running simulations to recover from failures, resume after interruptions, or examine intermediate results. In this work, we introduce a novel framework for scientific data compression and super-resolution, grounded in recent advances in learning exponential families. Our method preserves and quantifies uncertainty in physical quantities of interest and supports flexible trade-offs between compression ratio and reconstruction fidelity.</li>
<li><strong>摘要：</strong>现代科学模拟、观察和大规模实验产生的数据量往往超出存储、处理和分析的限制。这一挑战推动了数据缩减方法的发展，这些方法可以有效管理大量数据集，同时保留基本的物理特征和感兴趣的数量。在许多科学工作流程中，从压缩表示中恢复数据（一项称为超分辨率的任务）并保证保留关键物理特性也至关重要。一个值得注意的例子是检查点和重新启动，这对于长时间运行的模拟从故障中恢复、中断后恢复或检查中间结果至关重要。在这项工作中，我们介绍了一种用于科学数据压缩和超分辨率的新颖框架，该框架基于学习指数族的最新进展。我们的方法保留并量化了感兴趣的物理量的不确定性，并支持压缩比和重建保真度之间的灵活权衡。</li>
</ul>

<h3>Title: TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Harold Haodong Chen, Disen Lan, Wen-Jie Shu, Qingyang Liu, Zihan Wang, Sirui Chen, Wenkai Cheng, Kanghao Chen, Hongfei Zhang, Zixin Zhang, Rongjin Guo, Yu Cheng, Ying-Cong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13704">https://arxiv.org/abs/2511.13704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13704">https://arxiv.org/pdf/2511.13704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13704]] TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models(https://arxiv.org/abs/2511.13704)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.</li>
<li><strong>摘要：</strong>视频生成模型的快速发展已将其重点从产生视觉上合理的输出转移到处理需要物理合理性和逻辑一致性的任务。然而，尽管最近取得了 Veo 3 的框架链推理等突破，但仍不清楚这些模型是否能够表现出类似于大型语言模型 (LLM) 的推理能力。现有的基准主要评估视觉保真度和时间连贯性，未能捕获高阶推理能力。为了弥补这一差距，我们提出了 TiViBench，这是一个专门设计用于评估图像到视频（I2V）生成模型的推理能力的分层基准。 TiViBench 系统地评估四个维度的推理：i) 结构推理和搜索，ii) 空间和视觉模式推理，iii) 符号和逻辑推理，以及 iv) 行动规划和任务执行，涵盖 3 个难度级别的 24 个不同的任务场景。通过广泛的评估，我们表明商业模型（例如 Sora 2、Veo 3.1）表现出更强的推理潜力，而开源模型则显示出尚未开发的潜力，但仍受到有限的训练规模和数据多样性的阻碍。为了进一步释放这一潜力，我们引入了 VideoTPO，这是一种受偏好优化启发的简单而有效的测试时策略。通过对生成的候选人进行 LLM 自我分析来识别优势和劣势，VideoTPO 显着提高了推理性能，而无需额外的培训、数据或奖励模型。 TiViBench 和 VideoTPO 共同为评估和推进视频生成模型的推理铺平了道路，为这一新兴领域的未来研究奠定了基础。</li>
</ul>

<h3>Title: Scaling Spatial Intelligence with Multimodal Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Zhongang Cai, Ruisi Wang, Chenyang Gu, Fanyi Pu, Junxiang Xu, Yubo Wang, Wanqi Yin, Zhitao Yang, Chen Wei, Qingping Sun, Tongxi Zhou, Jiaqi Li, Hui En Pang, Oscar Qian, Yukun Wei, Zhiqian Lin, Xuanke Shi, Kewang Deng, Xiaoyang Han, Zukai Chen, Xiangyu Fan, Hanming Deng, Lewei Lu, Liang Pan, Bo Li, Ziwei Liu, Quan Wang, Dahua Lin, Lei Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.MM, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13719">https://arxiv.org/abs/2511.13719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13719">https://arxiv.org/pdf/2511.13719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13719]] Scaling Spatial Intelligence with Multimodal Foundation Models(https://arxiv.org/abs/2511.13719)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.</li>
<li><strong>摘要：</strong>尽管取得了显着的进步，多模态基础模型在空间智能方面仍然表现出令人惊讶的缺陷。在这项工作中，我们探索扩大多模态基础模型，以培养 SenseNova-SI 系列中的空间智能，该模型建立在已建立的多模态基础上，包括视觉理解模型（即 Qwen3-VL 和 InternVL3）以及统一理解和生成模型（即 Bagel）。我们采用原则性方法，通过系统地管理 SenseNova-SI-8M 来构建高性能和强大的空间智能：在严格的空间能力分类下的 800 万个不同的数据样本。 SenseNova-SI 在广泛的空间智能基准测试中展现了前所未有的性能：VSI-Bench 上为 68.7%，MMSI 上为 43.3%，MindCube 上为 85.6%，ViewSpatial 上为 54.6%，SITE 上为 50.1%，同时保持了强大的一般多模态理解（例如，MMBench-En 上为 84.9%）。更重要的是，我们分析了数据扩展的影响，讨论了多样化数据训练带来的新兴泛化能力的早期迹象，分析了过度拟合和语言捷径的风险，提出了空间思维链推理的初步研究，并验证了潜在的下游应用。 SenseNova-SI是一个正在进行的项目，本报告将不断更新。所有新训练的多模式基础模型都会公开发布，以促进该方向的进一步研究。</li>
</ul>

<h3>Title: Back to Basics: Let Denoising Generative Models Denoise</h3>
<ul>
<li><strong>Authors: </strong>Tianhong Li, Kaiming He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13720">https://arxiv.org/abs/2511.13720</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13720">https://arxiv.org/pdf/2511.13720</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13720]] Back to Basics: Let Denoising Generative Models Denoise(https://arxiv.org/abs/2511.13720)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Today's denoising diffusion models do not "denoise" in the classical sense, i.e., they do not directly predict clean images. Rather, the neural networks predict noise or a noised quantity. In this paper, we suggest that predicting clean data and predicting noised quantities are fundamentally different. According to the manifold assumption, natural data should lie on a low-dimensional manifold, whereas noised quantities do not. With this assumption, we advocate for models that directly predict clean data, which allows apparently under-capacity networks to operate effectively in very high-dimensional spaces. We show that simple, large-patch Transformers on pixels can be strong generative models: using no tokenizer, no pre-training, and no extra loss. Our approach is conceptually nothing more than "$\textbf{Just image Transformers}$", or $\textbf{JiT}$, as we call it. We report competitive results using JiT with large patch sizes of 16 and 32 on ImageNet at resolutions of 256 and 512, where predicting high-dimensional noised quantities can fail catastrophically. With our networks mapping back to the basics of the manifold, our research goes back to basics and pursues a self-contained paradigm for Transformer-based diffusion on raw natural data.</li>
<li><strong>摘要：</strong>当今的去噪扩散模型并不进行经典意义上的“去噪”，即它们不直接预测干净的图像。相反，神经网络预测噪声或噪声量。在本文中，我们认为预测干净数据和预测噪声量有本质上的不同。根据流形假设，自然数据应该位于低维流形上，而噪声量则不然。有了这个假设，我们提倡直接预测干净数据的模型，这允许明显容量不足的网络在非常高维的空间中有效运行。我们证明，像素上的简单、大补丁 Transformer 可以成为强大的生成模型：不使用分词器，无需预训练，也没有额外的损失。我们的方法在概念上只不过是“$\textbf{Just image Transformers}$”，或我们所说的$\textbf{JiT}$。我们在 ImageNet 上以 256 和 512 的分辨率使用 JiT 报告了具有 16 和 32 的大补丁大小的竞争结果，其中预测高维噪声量可能会灾难性地失败。随着我们的网络映射回流形的基础，我们的研究回归基础并追求一种独立的范式，用于原始自然数据上基于 Transformer 的扩散。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
