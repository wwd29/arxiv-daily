<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-07-24</h1>
<h3>Title: TD-Interpreter: Enhancing the Understanding of Timing Diagrams with Visual-Language Learning</h3>
<ul>
<li><strong>Authors: </strong>Jie He, Vincent Theo Willem Kenbeek, Zhantao Yang, Meixun Qu, Ezio Bartocci, Dejan Ničković, Radu Grosu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16844">https://arxiv.org/abs/2507.16844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16844">https://arxiv.org/pdf/2507.16844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16844]] TD-Interpreter: Enhancing the Understanding of Timing Diagrams with Visual-Language Learning(https://arxiv.org/abs/2507.16844)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce TD-Interpreter, a specialized ML tool that assists engineers in understanding complex timing diagrams (TDs), originating from a third party, during their design and verification process. TD-Interpreter is a visual question-answer environment which allows engineers to input a set of TDs and ask design and verification queries regarding these TDs. We implemented TD-Interpreter with multimodal learning by fine-tuning LLaVA, a lightweight 7B Multimodal Large Language Model (MLLM). To address limited training data availability, we developed a synthetic data generation workflow that aligns visual information with its textual interpretation. Our experimental evaluation demonstrates the usefulness of TD-Interpreter which outperformed untuned GPT-4o by a large margin on the evaluated benchmarks.</li>
<li><strong>摘要：</strong>我们介绍了TD Interpreter，这是一种专门的ML工具，可帮助工程师在其设计和验证过程中了解源自第三方的复杂时正图（TD）。 TD Interpreter是一个视觉提问的环境，它允许工程师输入一组TD，并就这些TDS询问设计和验证查询。我们通过微型7B多模式大型语言模型（MLLM）通过微调Llava进行了多模式学习实现TD互动。为了解决有限的培训数据可用性，我们开发了一个合成数据生成工作流程，该工作流程将视觉信息与其文本解释保持一致。我们的实验评估证明了TD间断器的有用性，该限额在评估的基准测试基准上大大超过了未调节的GPT-4O。</li>
</ul>

<h3>Title: Coarse-to-fine crack cue for robust crack detection</h3>
<ul>
<li><strong>Authors: </strong>Zelong Liu, Yuliang Gu, Zhichao Sun, Huachao Zhu, Xin Xiao, Bo Du, Laurent Najman (LIGM), Yongchao Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.NE, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16851">https://arxiv.org/abs/2507.16851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16851">https://arxiv.org/pdf/2507.16851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16851]] Coarse-to-fine crack cue for robust crack detection(https://arxiv.org/abs/2507.16851)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Crack detection is an important task in computer vision. Despite impressive in-dataset performance, deep learning-based methods still struggle in generalizing to unseen domains. The thin structure property of cracks is usually overlooked by previous methods. In this work, we introduce CrackCue, a novel method for robust crack detection based on coarse-to-fine crack cue generation. The core concept lies on leveraging the thin structure property to generate a robust crack cue, guiding the crack detection. Specifically, we first employ a simple max-pooling and upsampling operation on the crack image. This results in a coarse crack-free background, based on which a fine crack-free background can be obtained via a reconstruction network. The difference between the original image and fine crack-free background provides a fine crack cue. This fine cue embeds robust crack prior information which is unaffected by complex backgrounds, shadow, and varied lighting. As a plug-and-play method, we incorporate the proposed CrackCue into three advanced crack detection networks. Extensive experimental results demonstrate that the proposed CrackCue significantly improves the generalization ability and robustness of the baseline methods. The source code will be publicly available.</li>
<li><strong>摘要：</strong>裂纹检测是计算机视觉中的重要任务。尽管令人印象深刻，但基于深度学习的方法仍在推广到看不见的领域方面仍在努力。裂纹的薄结构特性通常被以前的方法忽略。在这项工作中，我们介绍了Crackcue，这是一种基于粗到细裂纹提示产生的新型方法，可用于稳健的裂纹检测。核心概念在于利用薄结构属性产生强大的裂纹提示，从而引导裂纹检测。具体来说，我们首先在裂纹图像上采用简单的最大式泵和提升操作。这导致了无裂纹的背景，基于该背景，可以通过重建网络获得无裂纹的背景。原始图像和无裂纹背景之间的差异提供了精细的裂纹提示。这种精细的提示嵌入了强大的裂纹先验信息，该信息不受复杂背景，阴影和各种照明的影响。作为一种插件方法，我们将提出的裂纹摘要合并到三个高级裂纹检测网络中。广泛的实验结果表明，所提出的裂纹可显着提高基线方法的概括能力和鲁棒性。源代码将公开可用。</li>
</ul>

<h3>Title: Diffusion-Modeled Reinforcement Learning for Carbon and Risk-Aware Microgrid Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yunyi Zhao, Wei Zhang, Cheng Xiang, Hongyang Du, Dusit Niyato, Shuhua Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16867">https://arxiv.org/abs/2507.16867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16867">https://arxiv.org/pdf/2507.16867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16867]] Diffusion-Modeled Reinforcement Learning for Carbon and Risk-Aware Microgrid Optimization(https://arxiv.org/abs/2507.16867)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This paper introduces DiffCarl, a diffusion-modeled carbon- and risk-aware reinforcement learning algorithm for intelligent operation of multi-microgrid systems. With the growing integration of renewables and increasing system complexity, microgrid communities face significant challenges in real-time energy scheduling and optimization under uncertainty. DiffCarl integrates a diffusion model into a deep reinforcement learning (DRL) framework to enable adaptive energy scheduling under uncertainty and explicitly account for carbon emissions and operational risk. By learning action distributions through a denoising generation process, DiffCarl enhances DRL policy expressiveness and enables carbon- and risk-aware scheduling in dynamic and uncertain microgrid environments. Extensive experimental studies demonstrate that it outperforms classic algorithms and state-of-the-art DRL solutions, with 2.3-30.1% lower operational cost. It also achieves 28.7% lower carbon emissions than those of its carbon-unaware variant and reduces performance variability. These results highlight DiffCarl as a practical and forward-looking solution. Its flexible design allows efficient adaptation to different system configurations and objectives to support real-world deployment in evolving energy systems.</li>
<li><strong>摘要：</strong>本文介绍了Diffcarl，这是一种扩散模型的碳和风险感知的增强算法，用于多微晶系统的智能操作。随着可再生能源和系统复杂性不断增长的整合，微电网社区在不确定性下实时能源调度和优化面临重大挑战。 DiffCarl将扩散模型集成到深度加固学习（DRL）框架中，以在不确定性下实现自适应能量调度，并明确解释碳排放和操作风险。通过通过denoising生成过程学习行动分布，DIFFCARL增强了DRL策略表现力，并在动态和不确定的微电网环境中实现碳和风险感知的调度。广泛的实验研究表明，它的表现优于经典算法和最先进的DRL解决方案，其运营成本降低了2.3-30.1％。它的碳排放量还低28.7％，而碳排放量也比其碳变种的碳排放量低，并降低了性能变异性。这些结果突出显示了DiffCarl是一种实用和前瞻性的解决方案。它的灵活设计允许有效适应不同的系统配置和目标，以支持不断发展的能源系统中的现实部署。</li>
</ul>

<h3>Title: Finding Dori: Memorization in Text-to-Image Diffusion Models Is Less Local Than Assumed</h3>
<ul>
<li><strong>Authors: </strong>Antoni Kowalczuk, Dominik Hintersdorf, Lukas Struppek, Kristian Kersting, Adam Dziedzic, Franziska Boenisch</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16880">https://arxiv.org/abs/2507.16880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16880">https://arxiv.org/pdf/2507.16880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16880]] Finding Dori: Memorization in Text-to-Image Diffusion Models Is Less Local Than Assumed(https://arxiv.org/abs/2507.16880)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models (DMs) have achieved remarkable success in image generation. However, concerns about data privacy and intellectual property remain due to their potential to inadvertently memorize and replicate training data. Recent mitigation efforts have focused on identifying and pruning weights responsible for triggering replication, based on the assumption that memorization can be localized. Our research assesses the robustness of these pruning-based approaches. We demonstrate that even after pruning, minor adjustments to text embeddings of input prompts are sufficient to re-trigger data replication, highlighting the fragility of these defenses. Furthermore, we challenge the fundamental assumption of memorization locality, by showing that replication can be triggered from diverse locations within the text embedding space, and follows different paths in the model. Our findings indicate that existing mitigation strategies are insufficient and underscore the need for methods that truly remove memorized content, rather than attempting to suppress its retrieval. As a first step in this direction, we introduce a novel adversarial fine-tuning method that iteratively searches for replication triggers and updates the model to increase robustness. Through our research, we provide fresh insights into the nature of memorization in text-to-image DMs and a foundation for building more trustworthy and compliant generative AI.</li>
<li><strong>摘要：</strong>文本到图像扩散模型（DMS）在图像生成中取得了显着的成功。但是，由于它们无意中记住和复制培训数据的潜力，对数据隐私和知识产权的担忧仍然存在。最近的缓解工作集中在识别和修剪权重触发复制的基础上，这是基于可以定位的记忆的假设。我们的研究评估了这些基于修剪的方法的鲁棒性。我们证明，即使修剪后，对输入提示的文本嵌入的小调整也足以重新触发数据复制，从而突出了这些防御的脆弱性。此外，我们通过表明复制可以从文本嵌入空间内的不同位置触发，并遵循模型中的不同路径来触发记忆位置的基本假设。我们的发现表明，现有的缓解策略不足，并强调了真正消除记忆内容而不是试图抑制其检索的方法的需求。作为朝这个方向发展的第一步，我们引入了一种新颖的对抗微调方法，该方法迭代地搜索复制触发器并更新模型以提高鲁棒性。通过我们的研究，我们为文本到图像DMS中的记忆性质提供了新的见解，并为建立更值得信赖和合规的生成AI提供了基础。</li>
</ul>

<h3>Title: SplitMeanFlow: Interval Splitting Consistency in Few-Step Generative Modeling</h3>
<ul>
<li><strong>Authors: </strong>Yi Guo, Wei Wang, Zhihang Yuan, Rong Cao, Kuan Chen, Zhengyang Chen, Yuanyuan Huo, Yang Zhang, Yuping Wang, Shouda Liu, Yuxuan Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16884">https://arxiv.org/abs/2507.16884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16884">https://arxiv.org/pdf/2507.16884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16884]] SplitMeanFlow: Interval Splitting Consistency in Few-Step Generative Modeling(https://arxiv.org/abs/2507.16884)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative models like Flow Matching have achieved state-of-the-art performance but are often hindered by a computationally expensive iterative sampling process. To address this, recent work has focused on few-step or one-step generation by learning the average velocity field, which directly maps noise to data. MeanFlow, a leading method in this area, learns this field by enforcing a differential identity that connects the average and instantaneous velocities. In this work, we argue that this differential formulation is a limiting special case of a more fundamental principle. We return to the first principles of average velocity and leverage the additivity property of definite integrals. This leads us to derive a novel, purely algebraic identity we term Interval Splitting Consistency. This identity establishes a self-referential relationship for the average velocity field across different time intervals without resorting to any differential operators. Based on this principle, we introduce SplitMeanFlow, a new training framework that enforces this algebraic consistency directly as a learning objective. We formally prove that the differential identity at the core of MeanFlow is recovered by taking the limit of our algebraic consistency as the interval split becomes infinitesimal. This establishes SplitMeanFlow as a direct and more general foundation for learning average velocity fields. From a practical standpoint, our algebraic approach is significantly more efficient, as it eliminates the need for JVP computations, resulting in simpler implementation, more stable training, and broader hardware compatibility. One-step and two-step SplitMeanFlow models have been successfully deployed in large-scale speech synthesis products (such as Doubao), achieving speedups of 20x.</li>
<li><strong>摘要：</strong>诸如流匹配之类的生成模型已经达到了最先进的性能，但通常受到计算昂贵的迭代抽样过程的阻碍。为了解决这个问题，最近的工作通过学习平均速度字段将噪声直接映射到数据来关注几步或一步的生成。 Meanflow是该领域的一种领先方法，它通过执行连接平均速度和瞬时速度的差分身份来了解这一领域。在这项工作中，我们认为这种差异表述是一个更基本原则的有限特殊情况。我们返回平均速度的第一原理，并利用确定积分的添加性属性。这使我们得出了一个新颖的，纯粹的代数身份，我们项间隔分裂的一致性。这种身份在不同时间间隔内建立了平均速度场的自指关系，而无需诉诸任何差分运算符。基于这一原则，我们引入了SplitMeanFlow，这是一个新的培训框架，该框架将这种代数一致性直接作为学习目标。我们正式证明，随着间隔拆分变得无限量，取得我们的代数一致性的限制来恢复平均流核的差分身份。这将SplitMeanflow作为学习平均速度场的直接和更一般的基础。从实际的角度来看，我们的代数方法更加有效，因为它消除了对JVP计算的需求，从而导致更简单的实施，更稳定的培训和更广泛的硬件兼容性。一步和两步分式集合模型已成功部署在大规模的语音合成产品中（例如Doubao），可实现20倍的速度。</li>
</ul>

<h3>Title: AURA: A Multi-Modal Medical Agent for Understanding, Reasoning & Annotation</h3>
<ul>
<li><strong>Authors: </strong>Nima Fathi, Amar Kumar, Tal Arbel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.16940">https://arxiv.org/abs/2507.16940</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.16940">https://arxiv.org/pdf/2507.16940</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.16940]] AURA: A Multi-Modal Medical Agent for Understanding, Reasoning & Annotation(https://arxiv.org/abs/2507.16940)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have catalyzed a paradigm shift from static prediction systems to agentic AI agents capable of reasoning, interacting with tools, and adapting to complex tasks. While LLM-based agentic systems have shown promise across many domains, their application to medical imaging remains in its infancy. In this work, we introduce AURA, the first visual linguistic explainability agent designed specifically for comprehensive analysis, explanation, and evaluation of medical images. By enabling dynamic interactions, contextual explanations, and hypothesis testing, AURA represents a significant advancement toward more transparent, adaptable, and clinically aligned AI systems. We highlight the promise of agentic AI in transforming medical image analysis from static predictions to interactive decision support. Leveraging Qwen-32B, an LLM-based architecture, AURA integrates a modular toolbox comprising: (i) a segmentation suite with phase grounding, pathology segmentation, and anatomy segmentation to localize clinically meaningful regions; (ii) a counterfactual image-generation module that supports reasoning through image-level explanations; and (iii) a set of evaluation tools including pixel-wise difference-map analysis, classification, and advanced state-of-the-art components to assess diagnostic relevance and visual interpretability.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的最新进展催化了从静态预测系统到能够推理，与工具相互作用并适应复杂任务的代理AI代理的范式转变。尽管基于LLM的代理系统在许多领域都表现出了希望，但它们在医学成像中的应用仍处于起步阶段。在这项工作中，我们介绍了Aura，这是专门为医学图像的全面分析，解释和评估而设计的第一个视觉语言解释性剂。通过实现动态相互作用，上下文解释和假设检验，Aura代表了更加透明，适应性和临床对齐的AI系统的重大进步。我们强调了代理AI在将医学图像分析从静态预测转换为交互式决策支持方面的希望。 AURA利用基于LLM的架构QWEN-32B集成了一个模块化工具箱，其中包括：（i）具有相基接地，病理分割和解剖学分割的分割套件，以定位临床上有意义的区域； （ii）通过图像级解释支持推理的反事实形象生成模块； （iii）一组评估工具，包括像素差异图分析，分类和先进的最新组件，以评估诊断相关性和视觉解释性。</li>
</ul>

<h3>Title: Should Bias Always be Eliminated? A Principled Framework to Use Data Bias for OOD Generation</h3>
<ul>
<li><strong>Authors: </strong>Yan Li, Guangyi Chen, Yunlong Deng, Zijian Li, Zeyu Tang, Anpeng Wu, Kun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17001">https://arxiv.org/abs/2507.17001</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17001">https://arxiv.org/pdf/2507.17001</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17001]] Should Bias Always be Eliminated? A Principled Framework to Use Data Bias for OOD Generation(https://arxiv.org/abs/2507.17001)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Most existing methods for adapting models to out-of-distribution (OOD) domains rely on invariant representation learning to eliminate the influence of biased features. However, should bias always be eliminated -- and if not, when should it be retained, and how can it be leveraged? To address these questions, we first present a theoretical analysis that explores the conditions under which biased features can be identified and effectively utilized. Building on this theoretical foundation, we introduce a novel framework that strategically leverages bias to complement invariant representations during inference. The framework comprises two key components that leverage bias in both direct and indirect ways: (1) using invariance as guidance to extract predictive ingredients from bias, and (2) exploiting identified bias to estimate the environmental condition and then use it to explore appropriate bias-aware predictors to alleviate environment gaps. We validate our approach through experiments on both synthetic datasets and standard domain generalization benchmarks. Results consistently demonstrate that our method outperforms existing approaches, underscoring its robustness and adaptability.</li>
<li><strong>摘要：</strong>大多数现有的方法将模型适应分布（OOD）域（OOD）域依赖于不变表示学习来消除偏见特征的影响。但是，应该始终消除偏见吗？为了解决这些问题，我们首先提出了一个理论分析，该分析探讨了可以识别并有效利用偏见特征的条件。在这个理论基础的基础上，我们介绍了一个新颖的框架，该框架从战略上利用偏见来补充推论期间不变的表示。该框架包括两个关键组成部分，以直接和间接方式利用偏见：（1）使用不变性作为指导来从偏见中提取预测成分，（2）利用确定的偏见来估算环境状况，然后使用它来探索适当的偏见预测者来减轻环境环境的差异。我们通过对合成数据集和标准域概括基准的实验来验证我们的方法。结果始终表明，我们的方法优于现有方法，强调其鲁棒性和适应性。</li>
</ul>

<h3>Title: Bringing Balance to Hand Shape Classification: Mitigating Data Imbalance Through Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Gaston Gustavo Rios, Pedro Dal Bianco, Franco Ronchetti, Facundo Quiroga, Oscar Stanchi, Santiago Ponte Ahón, Waldo Hasperué</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17008">https://arxiv.org/abs/2507.17008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17008">https://arxiv.org/pdf/2507.17008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17008]] Bringing Balance to Hand Shape Classification: Mitigating Data Imbalance Through Generative Models(https://arxiv.org/abs/2507.17008)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Most sign language handshape datasets are severely limited and unbalanced, posing significant challenges to effective model training. In this paper, we explore the effectiveness of augmenting the training data of a handshape classifier by generating synthetic data. We use an EfficientNet classifier trained on the RWTH German sign language handshape dataset, which is small and heavily unbalanced, applying different strategies to combine generated and real images. We compare two Generative Adversarial Networks (GAN) architectures for data generation: ReACGAN, which uses label information to condition the data generation process through an auxiliary classifier, and SPADE, which utilizes spatially-adaptive normalization to condition the generation on pose information. ReACGAN allows for the generation of realistic images that align with specific handshape labels, while SPADE focuses on generating images with accurate spatial handshape configurations. Our proposed techniques improve the current state-of-the-art accuracy on the RWTH dataset by 5%, addressing the limitations of small and unbalanced datasets. Additionally, our method demonstrates the capability to generalize across different sign language datasets by leveraging pose-based generation trained on the extensive HaGRID dataset. We achieve comparable performance to single-source trained classifiers without the need for retraining the generator.</li>
<li><strong>摘要：</strong>大多数手语握手数据集受到严重限制和不平衡，对有效的模型培训构成了重大挑战。在本文中，我们探讨了通过生成合成数据来增强握手分类器的训练数据的有效性。我们使用在RWTH德语手语握手数据集上训练的高效网络分类器，该数据集很小且不平衡，采用不同的策略来结合生成的图像和真实的图像。我们比较了数据生成的两个生成对抗网络（GAN）体系结构：Recgan，它使用标签信息通过辅助分类器来调节数据生成过程，而Spade则利用了空间自适应的归一化来调节姿势信息的生成。 Recgan允许生成与特定握把标签保持一致的逼真的图像，而Spade专注于以准确的空间握手配置生成图像。我们提出的技术将RWTH数据集的当前最新准确性提高了5％，以解决小型和不平衡数据集的局限性。此外，我们的方法通过利用在广泛的海格数据集中训练的基于姿势的生成来证明在不同手语数据集中概括的能力。我们实现了与单源训练的分类器相当的性能，而无需重新训练发电机。</li>
</ul>

<h3>Title: Transformer Based Building Boundary Reconstruction using Attraction Field Maps</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Kamran, Mohammad Moein Sheikholeslami, Andreas Wichmann, Gunho Sohn</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17038">https://arxiv.org/abs/2507.17038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17038">https://arxiv.org/pdf/2507.17038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17038]] Transformer Based Building Boundary Reconstruction using Attraction Field Maps(https://arxiv.org/abs/2507.17038)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In recent years, the number of remote satellites orbiting the Earth has grown significantly, streaming vast amounts of high-resolution visual data to support diverse applications across civil, public, and military domains. Among these applications, the generation and updating of spatial maps of the built environment have become critical due to the extensive coverage and detailed imagery provided by satellites. However, reconstructing spatial maps from satellite imagery is a complex computer vision task, requiring the creation of high-level object representations, such as primitives, to accurately capture the built environment. While the past decade has witnessed remarkable advancements in object detection and representation using visual data, primitives-based object representation remains a persistent challenge in computer vision. Consequently, high-quality spatial maps often rely on labor-intensive and manual processes. This paper introduces a novel deep learning methodology leveraging Graph Convolutional Networks (GCNs) to address these challenges in building footprint reconstruction. The proposed approach enhances performance by incorporating geometric regularity into building boundaries, integrating multi-scale and multi-resolution features, and embedding Attraction Field Maps into the network. These innovations provide a scalable and precise solution for automated building footprint extraction from a single satellite image, paving the way for impactful applications in urban planning, disaster management, and large-scale spatial analysis. Our model, Decoupled-PolyGCN, outperforms existing methods by 6% in AP and 10% in AR, demonstrating its ability to deliver accurate and regularized building footprints across diverse and challenging scenarios.</li>
<li><strong>摘要：</strong>近年来，绕地球绕着地球的远程卫星数量已大大增加，播放了大量的高分辨率视觉数据，以支持在民用，公共和军事领域的各种应用。在这些应用中，由于卫星提供了广泛的覆盖范围和详细的图像，建筑环境的空间图的产生和更新变得至关重要。但是，从卫星图像中重建空间图是一项复杂的计算机视觉任务，需要创建高级对象表示（例如原始），以准确捕获建筑环境。尽管过去十年在使用视觉数据方面见证了对象检测和表示方面的显着进步，但基于原始的对象表示仍然是计算机视觉中的持续挑战。因此，高质量的空间地图通常依赖于劳动密集型和手动过程。本文介绍了一种新颖的深度学习方法，利用图形卷积网络（GCN）来解决建立足迹重建方面的这些挑战。所提出的方法通过将几何规律性纳入建筑边界，集成多尺度和多分辨率的特征以及将吸引力字段图嵌入到网络中来增强性能。这些创新为从单个卫星图像中提取自动化的建筑足迹提供了可扩展和精确的解决方案，为城市规划，灾难管理和大规模空间分析中的有影响力的应用铺平了道路。我们的模型，即脱钩的polyGCN，AP的现有方法的表现优于现有方法，而AR中的方法则优于10％，这表明其能够在各种且具有挑战性的情况下提供准确和正则化的建筑足迹。</li>
</ul>

<h3>Title: Risk In Context: Benchmarking Privacy Leakage of Foundation Models in Synthetic Tabular Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Jessup Byun, Xiaofeng Lin, Joshua Ward, Guang Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17066">https://arxiv.org/abs/2507.17066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17066">https://arxiv.org/pdf/2507.17066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17066]] Risk In Context: Benchmarking Privacy Leakage of Foundation Models in Synthetic Tabular Data Generation(https://arxiv.org/abs/2507.17066)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Synthetic tabular data is essential for machine learning workflows, especially for expanding small or imbalanced datasets and enabling privacy-preserving data sharing. However, state-of-the-art generative models (GANs, VAEs, diffusion models) rely on large datasets with thousands of examples. In low-data settings, often the primary motivation for synthetic data, these models can overfit, leak sensitive records, and require frequent retraining. Recent work uses large pre-trained transformers to generate rows via in-context learning (ICL), which needs only a few seed examples and no parameter updates, avoiding retraining. But ICL repeats seed rows verbatim, introducing a new privacy risk that has only been studied in text. The severity of this risk in tabular synthesis-where a single row may identify a person-remains unclear. We address this gap with the first benchmark of three foundation models (GPT-4o-mini, LLaMA 3.3 70B, TabPFN v2) against four baselines on 35 real-world tables from health, finance, and policy. We evaluate statistical fidelity, downstream utility, and membership inference leakage. Results show foundation models consistently have the highest privacy risk. LLaMA 3.3 70B reaches up to 54 percentage points higher true-positive rate at 1% FPR than the safest baseline. GPT-4o-mini and TabPFN are also highly vulnerable. We plot the privacy-utility frontier and show that CTGAN and GPT-4o-mini offer better tradeoffs. A factorial study finds that three zero-cost prompt tweaks-small batch size, low temperature, and using summary statistics-can reduce worst-case AUC by 14 points and rare-class leakage by up to 39 points while maintaining over 90% fidelity. Our benchmark offers a practical guide for safer low-data synthesis with foundation models.</li>
<li><strong>摘要：</strong>综合表格数据对于机器学习工作流程至关重要，尤其是扩展小型或不平衡数据集并实现隐私数据共享。但是，最新的生成模型（gan，vaes，扩散模型）依赖于具有数千个示例的大型数据集。在低数据设置中，通常是合成数据的主要动机，这些模型可以过度拟合，泄漏敏感的记录并需要经常进行重新训练。最近的工作使用大型的预训练的变压器通过封闭式学习（ICL）生成行，该研究只需要几个种子示例，而无需参数更新，避免了重新训练。但是ICL逐字地重复种子行，引入了仅在文本中研究的新隐私风险。在表格合成中，这种风险的严重性 - 单排可能会识别一个人犯人不清楚。我们以三种基础模型（GPT-4O-MINI，LLAMA 3.3 70B，TABPFN V2）的第一个基准测试解决了这一差距，该差距在健康，金融和政策中的35个现实世界中的四个基线上解决了这一差距。我们评估统计保真度，下游实用程序和会员推理泄漏。结果表明，基础模型始终具有最高的隐私风险。 Llama 3.3 70b的真实阳性率高达54个百分点，而FPR比最安全的基线高。 GPT-4O-MINI和TABPFN也很脆弱。我们绘制了隐私 - 实用边界，并表明Ctgan和GPT-4O-Mini提供了更好的权衡。一项阶乘研究发现，三个零成本及时调整了批次批量，低温和使用摘要统计范围，将最差的AUC降低了14点，稀有级别的泄漏量最多可减少39点，同时保持超过90％的保真度。我们的基准提供了与基础模型更安全的低数据合成的实用指南。</li>
</ul>

<h3>Title: Probabilistic Graphical Models: A Concise Tutorial</h3>
<ul>
<li><strong>Authors: </strong>Jacqueline Maasch, Willie Neiswanger, Stefano Ermon, Volodymyr Kuleshov</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17116">https://arxiv.org/abs/2507.17116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17116">https://arxiv.org/pdf/2507.17116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17116]] Probabilistic Graphical Models: A Concise Tutorial(https://arxiv.org/abs/2507.17116)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Probabilistic graphical modeling is a branch of machine learning that uses probability distributions to describe the world, make predictions, and support decision-making under uncertainty. Underlying this modeling framework is an elegant body of theory that bridges two mathematical traditions: probability and graph theory. This framework provides compact yet expressive representations of joint probability distributions, yielding powerful generative models for probabilistic reasoning. This tutorial provides a concise introduction to the formalisms, methods, and applications of this modeling framework. After a review of basic probability and graph theory, we explore three dominant themes: (1) the representation of multivariate distributions in the intuitive visual language of graphs, (2) algorithms for learning model parameters and graphical structures from data, and (3) algorithms for inference, both exact and approximate.</li>
<li><strong>摘要：</strong>概率图形建模是机器学习的一个分支，它使用概率分布来描述世界，做出预测并支持不确定性下的决策。这个建模框架的基础是一个优雅的理论体系，它桥接了两种数学传统：概率和图形论。该框架提供了关节概率分布的紧凑而表达的表示，为概率推理提供了强大的生成模型。本教程提供了对本建模框架的形式主义，方法和应用的简洁介绍。在综述了基本概率和图形论之后，我们探讨了三个主要主题：（1）图形直观的视觉语言中多元分布的表示，（2）从数据中学习模型参数和图形结构的算法，以及（3）推断的推断，精确和近似。</li>
</ul>

<h3>Title: SADA: Stability-guided Adaptive Diffusion Acceleration</h3>
<ul>
<li><strong>Authors: </strong>Ting Jiang, Yixiao Wang, Hancheng Ye, Zishan Shao, Jingwei Sun, Jingyang Zhang, Zekai Chen, Jianyi Zhang, Yiran Chen, Hai Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17135">https://arxiv.org/abs/2507.17135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17135">https://arxiv.org/pdf/2507.17135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17135]] SADA: Stability-guided Adaptive Diffusion Acceleration(https://arxiv.org/abs/2507.17135)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved remarkable success in generative tasks but suffer from high computational costs due to their iterative sampling process and quadratic attention costs. Existing training-free acceleration strategies that reduce per-step computation cost, while effectively reducing sampling time, demonstrate low faithfulness compared to the original baseline. We hypothesize that this fidelity gap arises because (a) different prompts correspond to varying denoising trajectory, and (b) such methods do not consider the underlying ODE formulation and its numerical solution. In this paper, we propose Stability-guided Adaptive Diffusion Acceleration (SADA), a novel paradigm that unifies step-wise and token-wise sparsity decisions via a single stability criterion to accelerate sampling of ODE-based generative models (Diffusion and Flow-matching). For (a), SADA adaptively allocates sparsity based on the sampling trajectory. For (b), SADA introduces principled approximation schemes that leverage the precise gradient information from the numerical ODE solver. Comprehensive evaluations on SD-2, SDXL, and Flux using both EDM and DPM++ solvers reveal consistent $\ge 1.8\times$ speedups with minimal fidelity degradation (LPIPS $\leq 0.10$ and FID $\leq 4.5$) compared to unmodified baselines, significantly outperforming prior methods. Moreover, SADA adapts seamlessly to other pipelines and modalities: It accelerates ControlNet without any modifications and speeds up MusicLDM by $1.8\times$ with $\sim 0.01$ spectrogram LPIPS.</li>
<li><strong>摘要：</strong>扩散模型在生成任务上取得了显着的成功，但由于其迭代采样过程和二次注意力成本而遭受了高计算成本。与原始基线相比，现有的无培训加速策略降低了每步计算成本，同时有效减少了抽样时间，这表明忠诚度较低。我们假设出现了这种保真差距，因为（a）不同的提示对应于不同的降解轨迹，并且（b）此类方法不考虑基本的ode公式及其数值解决方案。在本文中，我们提出了稳定性引导的自适应扩散加速度（SADA），这是一种新型范式，通过单个稳定性标准统一逐步和令牌的稀疏性决策，以加速基于ODE的生成模型（扩散和流程匹配）。对于（a），Sada根据采样轨迹适应稀疏性。对于（b），Sada引入了原则上的近似方案，该方案利用了数值求解器的精确梯度信息。使用EDM和DPM ++溶解器对SD-2，SDXL和通量进行的全面评估揭示了一致的$ \ ge 1.8 \ times $ $ speedups，与无效的基准相比，与无效的基准相比，与无效的基准相比，与无效的基准相比，fidel $ \ lpips $ \ leq 0.10 $和FID $ \ \ leq 4.5 $）。此外，萨达（Sada）无缝地适应其他管道和模式：它加速了控制网，而没有任何修改，并加快了$ \ sim 0.01 $ spectragram lpips的$ 1.8 \ times $ $。</li>
</ul>

<h3>Title: DOOMGAN:High-Fidelity Dynamic Identity Obfuscation Ocular Generative Morphing</h3>
<ul>
<li><strong>Authors: </strong>Bharath Krishnamurthy, Ajita Rattani</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17158">https://arxiv.org/abs/2507.17158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17158">https://arxiv.org/pdf/2507.17158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17158]] DOOMGAN:High-Fidelity Dynamic Identity Obfuscation Ocular Generative Morphing(https://arxiv.org/abs/2507.17158)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Ocular biometrics in the visible spectrum have emerged as a prominent modality due to their high accuracy, resistance to spoofing, and non-invasive nature. However, morphing attacks, synthetic biometric traits created by blending features from multiple individuals, threaten biometric system integrity. While extensively studied for near-infrared iris and face biometrics, morphing in visible-spectrum ocular data remains underexplored. Simulating such attacks demands advanced generation models that handle uncontrolled conditions while preserving detailed ocular features like iris boundaries and periocular textures. To address this gap, we introduce DOOMGAN, that encompasses landmark-driven encoding of visible ocular anatomy, attention-guided generation for realistic morph synthesis, and dynamic weighting of multi-faceted losses for optimized convergence. DOOMGAN achieves over 20% higher attack success rates than baseline methods under stringent thresholds, along with 20% better elliptical iris structure generation and 30% improved gaze consistency. We also release the first comprehensive ocular morphing dataset to support further research in this domain.</li>
<li><strong>摘要：</strong>可见光光谱中的眼部生物识别技术因其高精度，抗欺骗性和非侵入性而出现是一种突出的模态。然而，变形攻击，由多个个体的特征融合而产生的合成生物特征特征，威胁着生物识别系统的完整性。虽然对近红外虹膜进行了广泛的研究并面对生物识别技术，但可见的眼部眼睛数据中的变形仍然没有被逐渐倍增。模拟此类攻击需要高级生成模型，以处理不受控制的条件，同时保留诸如虹膜边界和眼周纹理之类的详细眼功能。为了解决这一差距，我们介绍了DOOMGAN，其中包括具有里程碑意义的眼镜解剖结构的地标驱动的编码，注意力引导的产生，用于现实的形态综合，以及对优化收敛的多方面损失的动态加权。在严格的阈值下，Doomgan的攻击成功率比基线方法高20％以上，椭圆形虹膜结构的产生更好20％，凝视一致性提高了30％。我们还发布了第一个全面的眼部变形数据集，以支持该领域的进一步研究。</li>
</ul>

<h3>Title: Hierarchical Fusion and Joint Aggregation: A Multi-Level Feature Representation Method for AIGC Image Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Linghe Meng, Jiarun Song</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17182">https://arxiv.org/abs/2507.17182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17182">https://arxiv.org/pdf/2507.17182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17182]] Hierarchical Fusion and Joint Aggregation: A Multi-Level Feature Representation Method for AIGC Image Quality Assessment(https://arxiv.org/abs/2507.17182)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>The quality assessment of AI-generated content (AIGC) faces multi-dimensional challenges, that span from low-level visual perception to high-level semantic understanding. Existing methods generally rely on single-level visual features, limiting their ability to capture complex distortions in AIGC images. To address this limitation, a multi-level visual representation paradigm is proposed with three stages, namely multi-level feature extraction, hierarchical fusion, and joint aggregation. Based on this paradigm, two networks are developed. Specifically, the Multi-Level Global-Local Fusion Network (MGLF-Net) is designed for the perceptual quality assessment, extracting complementary local and global features via dual CNN and Transformer visual backbones. The Multi-Level Prompt-Embedded Fusion Network (MPEF-Net) targets Text-to-Image correspondence by embedding prompt semantics into the visual feature fusion process at each feature level. The fused multi-level features are then aggregated for final evaluation. Experiments on benchmarks demonstrate outstanding performance on both tasks, validating the effectiveness of the proposed multi-level visual assessment paradigm.</li>
<li><strong>摘要：</strong>AI生成内容（AIGC）的质量评估面临多维挑战，涉及从低级视觉感知到高级语义理解。现有方法通常依赖于单层视觉特征，从而限制了它们在AIGC图像中捕获复杂失真的能力。为了解决这一限制，提出了三个阶段的多级视觉表示范式，即多级特征提取，分层融合和关节聚集。基于此范式，开发了两个网络。具体而言，多级全球局部融合网络（MGLF-NET）设计用于感知质量评估，通过双CNN和Transfermer Visual Backbones提取互补的本地和全球特征。多级提示式融合网络（MPEF-NET）通过将提示语义嵌入每个功能级别的视觉特征融合过程中，以文本对应关系。然后将融合的多层次功能汇总以进行最终评估。基准上的实验表明在这两个任务上都表现出色，从而验证了拟议的多层视觉评估范式的有效性。</li>
</ul>

<h3>Title: Vec2Face+ for Face Dataset Generation</h3>
<ul>
<li><strong>Authors: </strong>Haiyu Wu, Jaskirat Singh, Sicong Tian, Liang Zheng, Kevin W. Bowyer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17192">https://arxiv.org/abs/2507.17192</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17192">https://arxiv.org/pdf/2507.17192</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17192]] Vec2Face+ for Face Dataset Generation(https://arxiv.org/abs/2507.17192)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>When synthesizing identities as face recognition training data, it is generally believed that large inter-class separability and intra-class attribute variation are essential for synthesizing a quality dataset. % This belief is generally correct, and this is what we aim for. However, when increasing intra-class variation, existing methods overlook the necessity of maintaining intra-class identity consistency. % To address this and generate high-quality face training data, we propose Vec2Face+, a generative model that creates images directly from image features and allows for continuous and easy control of face identities and attributes. Using Vec2Face+, we obtain datasets with proper inter-class separability and intra-class variation and identity consistency using three strategies: 1) we sample vectors sufficiently different from others to generate well-separated identities; 2) we propose an AttrOP algorithm for increasing general attribute variations; 3) we propose LoRA-based pose control for generating images with profile head poses, which is more efficient and identity-preserving than AttrOP. % Our system generates VFace10K, a synthetic face dataset with 10K identities, which allows an FR model to achieve state-of-the-art accuracy on seven real-world test sets. Scaling the size to 4M and 12M images, the corresponding VFace100K and VFace300K datasets yield higher accuracy than the real-world training dataset, CASIA-WebFace, on five real-world test sets. This is the first time a synthetic dataset beats the CASIA-WebFace in average accuracy. In addition, we find that only 1 out of 11 synthetic datasets outperforms random guessing (\emph{i.e., 50\%}) in twin verification and that models trained with synthetic identities are more biased than those trained with real identities. Both are important aspects for future investigation.</li>
<li><strong>摘要：</strong>当将身份综合为面部识别训练数据时，人们普遍认为，较大的类间可分离性和类内属性变化对于综合质量数据集至关重要。 ％这种信念通常是正确的，这就是我们的目标。但是，当增加阶层内变化时，现有方法会忽略维持阶级内身份一致性的必要性。 ％为了解决这个问题并生成高质量的面部训练数据，我们提出了Vec2face+，它是一种生成模型，该模型直接从图像特征创建图像，并允许对面部身份和属性进行连续且易于控制。使用VEC2FACE+，我们使用三种策略获得具有适当的类间可分离性，类内的变化和身份一致性的数据集：1）我们与其他矢量进行了完全不同的矢量以产生良好的分离身份； 2）我们提出了一种用于增加一般属性变化的属性算法； 3）我们提出了基于洛拉的姿势控制，以生成具有轮廓头姿势的图像，这比attrop更有效，更具身份。 ％我们的系统生成Vface10k，这是一种具有10K身份的合成面部数据集，允许FR模型在七个现实世界测试集上实现最先进的精度。在五个现实世界中，将大小缩放到4m和12m图像，比实际的训练数据集Casia-Webface获得了相应的Vface100K和Vface300K数据集。这是合成数据集平均第一次击败casia-webface。此外，我们发现，在双验证中，只有11个合成数据集中只有11个合成数据集优于随机猜测（\ emph {i。两者都是未来研究的重要方面。</li>
</ul>

<h3>Title: DesignLab: Designing Slides Through Iterative Detection and Correction</h3>
<ul>
<li><strong>Authors: </strong>Jooyeol Yun, Heng Wang, Yotaro Shimose, Jaegul Choo, Shingo Takamatsu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17202">https://arxiv.org/abs/2507.17202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17202">https://arxiv.org/pdf/2507.17202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17202]] DesignLab: Designing Slides Through Iterative Detection and Correction(https://arxiv.org/abs/2507.17202)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Designing high-quality presentation slides can be challenging for non-experts due to the complexity involved in navigating various design choices. Numerous automated tools can suggest layouts and color schemes, yet often lack the ability to refine their own output, which is a key aspect in real-world workflows. We propose DesignLab, which separates the design process into two roles, the design reviewer, who identifies design-related issues, and the design contributor who corrects them. This decomposition enables an iterative loop where the reviewer continuously detects issues and the contributor corrects them, allowing a draft to be further polished with each iteration, reaching qualities that were unattainable. We fine-tune large language models for these roles and simulate intermediate drafts by introducing controlled perturbations, enabling the design reviewer learn design errors and the contributor learn how to fix them. Our experiments show that DesignLab outperforms existing design-generation methods, including a commercial tool, by embracing the iterative nature of designing which can result in polished, professional slides.</li>
<li><strong>摘要：</strong>由于涉及各种设计选择的复杂性，设计高质量的演示幻灯片对于非专家来说可能具有挑战性。许多自动化工具可以建议布局和配色方案，但通常缺乏完善自己的输出的能力，这是现实世界中工作流的关键方面。我们建议DesignLab，将设计过程分为两个角色，即设计审稿人，他们确定了与设计相关的问题，以及纠正它们的设计贡献者。这种分解使迭代循环能够连续检测出问题，并撰写贡献者纠正了问题，从而使草稿在每次迭代中都可以进一步抛光，从而达到了无法实现的质量。我们通过引入受控扰动，使设计审稿人学习设计错误，并学习如何修复它们，从而为这些角色微调大型语言模型，并通过引入受控的扰动来模拟中间草稿。我们的实验表明，DesignLab通过拥抱设计的迭代性质，胜过现有的设计生成方法，包括商业工具，这可能会导致抛光，专业的幻灯片。</li>
</ul>

<h3>Title: Filter-And-Refine: A MLLM Based Cascade System for Industrial-Scale Video Content Moderation</h3>
<ul>
<li><strong>Authors: </strong>Zixuan Wang, Jinghao Shi, Hanzhong Liang, Xiang Shen, Vera Wen, Zhiqian Chen, Yifan Wu, Zhixin Zhang, Hongyu Xiong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17204">https://arxiv.org/abs/2507.17204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17204">https://arxiv.org/pdf/2507.17204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17204]] Filter-And-Refine: A MLLM Based Cascade System for Industrial-Scale Video Content Moderation(https://arxiv.org/abs/2507.17204)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Effective content moderation is essential for video platforms to safeguard user experience and uphold community standards. While traditional video classification models effectively handle well-defined moderation tasks, they struggle with complicated scenarios such as implicit harmful content and contextual ambiguity. Multimodal large language models (MLLMs) offer a promising solution to these limitations with their superior cross-modal reasoning and contextual understanding. However, two key challenges hinder their industrial adoption. First, the high computational cost of MLLMs makes full-scale deployment impractical. Second, adapting generative models for discriminative classification remains an open research problem. In this paper, we first introduce an efficient method to transform a generative MLLM into a multimodal classifier using minimal discriminative training data. To enable industry-scale deployment, we then propose a router-ranking cascade system that integrates MLLMs with a lightweight router model. Offline experiments demonstrate that our MLLM-based approach improves F1 score by 66.50% over traditional classifiers while requiring only 2% of the fine-tuning data. Online evaluations show that our system increases automatic content moderation volume by 41%, while the cascading deployment reduces computational cost to only 1.5% of direct full-scale deployment.</li>
<li><strong>摘要：</strong>有效的内容审核对于维护用户体验和维护社区标准的视频平台至关重要。尽管传统的视频分类模型有效地处理了定义明确的审核任务，但它们在复杂的情况下进行了斗争，例如隐式有害内容和上下文歧义。多模式的大语言模型（MLLM）通过其出色的跨模式推理和上下文理解为这些限制提供了有希望的解决方案。但是，两个主要挑战阻碍了他们的工业采用。首先，MLLM的高计算成本使全尺度部署不切实际。其次，调整生成模型进行判别分类仍然是一个开放的研究问题。在本文中，我们首先引入了一种有效的方法，可以使用最小的判别训练数据将生成性MLLM转换为多模式分类器。为了实现行业规模的部署，我们然后提出了一个将MLLM与轻型路由器模型集成的路由器级别级联系统。离线实验表明，我们基于MLLM的方法将F1得分提高了66.50％，而传统分类器仅需要2％的微调数据。在线评估表明，我们的系统将自动内容适度量增加41％，而级联部署将计算成本降低到直接全尺度部署的1.5％。</li>
</ul>

<h3>Title: Perceptual Classifiers: Detecting Generative Images using Perceptual Features</h3>
<ul>
<li><strong>Authors: </strong>Krishna Srikar Durbha, Asvin Kumar Venkataramanan, Rajesh Sureddi, Alan C. Bovik</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17240">https://arxiv.org/abs/2507.17240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17240">https://arxiv.org/pdf/2507.17240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17240]] Perceptual Classifiers: Detecting Generative Images using Perceptual Features(https://arxiv.org/abs/2507.17240)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, quality assessment</a></li>
<li><strong>Abstract: </strong>Image Quality Assessment (IQA) models are employed in many practical image and video processing pipelines to reduce storage, minimize transmission costs, and improve the Quality of Experience (QoE) of millions of viewers. These models are sensitive to a diverse range of image distortions and can accurately predict image quality as judged by human viewers. Recent advancements in generative models have resulted in a significant influx of "GenAI" content on the internet. Existing methods for detecting GenAI content have progressed significantly with improved generalization performance on images from unseen generative models. Here, we leverage the capabilities of existing IQA models, which effectively capture the manifold of real images within a bandpass statistical space, to distinguish between real and AI-generated images. We investigate the generalization ability of these perceptual classifiers to the task of GenAI image detection and evaluate their robustness against various image degradations. Our results show that a two-layer network trained on the feature space of IQA models demonstrates state-of-the-art performance in detecting fake images across generative models, while maintaining significant robustness against image degradations.</li>
<li><strong>摘要：</strong>图像质量评估（IQA）模型用于许多实用的图像和视频处理管道中，以减少存储，最大程度地降低传输成本并提高数百万观众的体验质量（QOE）。这些模型对各种图像扭曲范围敏感，可以准确预测人类观众判断的图像质量。生成模型的最新进展导致互联网上大量的“ genai”内容涌入。现有的检测Genai含量的方法已取得了显着发展，随着对看不见的生成模型的图像的概括性能提高。在这里，我们利用现有的IQA模型的功能，该模型有效地捕获了带通统计空间内的真实图像的歧管，以区分真实图像和AI生成的图像。我们研究了这些感知分类器对Genai图像检测任务的概括能力，并评估它们对各种图像降解的鲁棒性。我们的结果表明，在IQA模型的特征空间上训练的两层网络在跨生成模型中检测假图像时展示了最新的性能，同时保持了针对图像降解的显着鲁棒性。</li>
</ul>

<h3>Title: Rethinking VAE: From Continuous to Discrete Representations Without Probabilistic Assumptions</h3>
<ul>
<li><strong>Authors: </strong>Songxuan Shi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17255">https://arxiv.org/abs/2507.17255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17255">https://arxiv.org/pdf/2507.17255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17255]] Rethinking VAE: From Continuous to Discrete Representations Without Probabilistic Assumptions(https://arxiv.org/abs/2507.17255)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper explores the generative capabilities of Autoencoders (AEs) and establishes connections between Variational Autoencoders (VAEs) and Vector Quantized-Variational Autoencoders (VQ-VAEs) through a reformulated training framework. We demonstrate that AEs exhibit generative potential via latent space interpolation and perturbation, albeit limited by undefined regions in the encoding space. To address this, we propose a new VAE-like training method that introduces clustering centers to enhance data compactness and ensure well-defined latent spaces without relying on traditional KL divergence or reparameterization techniques. Experimental results on MNIST, CelebA, and FashionMNIST datasets show smooth interpolative transitions, though blurriness persists. Extending this approach to multiple learnable vectors, we observe a natural progression toward a VQ-VAE-like model in continuous space. However, when the encoder outputs multiple vectors, the model degenerates into a discrete Autoencoder (VQ-AE), which combines image fragments without learning semantic representations. Our findings highlight the critical role of encoding space compactness and dispersion in generative modeling and provide insights into the intrinsic connections between VAEs and VQ-VAEs, offering a new perspective on their design and limitations.</li>
<li><strong>摘要：</strong>本文探讨了自动编码器（AES）的生成能力，并通过一个重新的培训框架在变化自动编码器（VAE）和量化变量自动编码器（VQ-VAE）之间建立联系。我们证明，AES通过潜在空间插值和扰动表现出生成潜力，尽管受编码空间中未定义的区域有限。为了解决这个问题，我们提出了一种新的VAE型训练方法，该方法介绍了聚类中心，以增强数据紧凑性并确保明确定义的潜在空间，而无需依赖传统的KL差异或重新竞选技术。 MNIST，Celeba和FashionMnist数据集的实验结果表现出平滑的插值过渡，尽管模糊仍然存在。将这种方法扩展到多个可学习的向量，我们观察到在连续空间中向类似VQ-VAE的模型的自然发展。但是，当编码器输出多个向量时，该模型将退化为离散的自动编码器（VQ-AE），该自动编码器（VQ-AE）结合了图像片段而无需学习语义表示。我们的发现突出了编码空间紧凑性和分散性在生成建模中的关键作用，并提供了有关VAES和VQ-VAE之间的内在联系的见解，从而为其设计和局限性提供了新的视角。</li>
</ul>

<h3>Title: PolarAnything: Diffusion-based Polarimetric Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Kailong Zhang, Youwei Lyu, Heng Guo, Si Li, Zhanyu Ma, Boxin Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17268">https://arxiv.org/abs/2507.17268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17268">https://arxiv.org/pdf/2507.17268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17268]] PolarAnything: Diffusion-based Polarimetric Image Synthesis(https://arxiv.org/abs/2507.17268)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Polarization images facilitate image enhancement and 3D reconstruction tasks, but the limited accessibility of polarization cameras hinders their broader application. This gap drives the need for synthesizing photorealistic polarization this http URL existing polarization simulator Mitsuba relies on a parametric polarization image formation model and requires extensive 3D assets covering shape and PBR materials, preventing it from generating large-scale photorealistic images. To address this problem, we propose PolarAnything, capable of synthesizing polarization images from a single RGB input with both photorealism and physical accuracy, eliminating the dependency on 3D asset collections. Drawing inspiration from the zero-shot performance of pretrained diffusion models, we introduce a diffusion-based generative framework with an effective representation strategy that preserves the fidelity of polarization properties. Experiments show that our model generates high-quality polarization images and supports downstream tasks like shape from polarization.</li>
<li><strong>摘要：</strong>极化图像促进了图像增强和3D重建任务，但是极化相机的可访问性有限阻碍其更广泛的应用。这一差距驱动了综合光真逼真极化的需求。现有的偏振模拟器三菱依赖于参数极化图像形成模型，并且需要大量的3D资产覆盖形状和PBR材料，从而阻止了它产生大型的光真主图像。为了解决这个问题，我们提出了具有二极管，能够从单个RGB输入中综合具有光真相和身体准确性的极化图像，从而消除了对3D资产收集的依赖性。从预处理扩散模型的零拍性能中汲取灵感，我们引入了一个基于扩散的生成框架，具有有效的表示策略，可保留极化属性的忠诚度。实验表明，我们的模型会生成高质量的极化图像，并支持下游任务，例如偏振的形状。</li>
</ul>

<h3>Title: Fully Automated SAM for Single-source Domain Generalization in Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Huanli Zhuo, Leilei Ma, Haifeng Zhao, Shiwei Zhou, Dengdi Sun, Yanping Fu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17281">https://arxiv.org/abs/2507.17281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17281">https://arxiv.org/pdf/2507.17281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17281]] Fully Automated SAM for Single-source Domain Generalization in Medical Image Segmentation(https://arxiv.org/abs/2507.17281)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Although SAM-based single-source domain generalization models for medical image segmentation can mitigate the impact of domain shift on the model in cross-domain scenarios, these models still face two major challenges. First, the segmentation of SAM is highly dependent on domain-specific expert-annotated prompts, which prevents SAM from achieving fully automated medical image segmentation and therefore limits its application in clinical settings. Second, providing poor prompts (such as bounding boxes that are too small or too large) to the SAM prompt encoder can mislead SAM into generating incorrect mask results. Therefore, we propose the FA-SAM, a single-source domain generalization framework for medical image segmentation that achieves fully automated SAM. FA-SAM introduces two key innovations: an Auto-prompted Generation Model (AGM) branch equipped with a Shallow Feature Uncertainty Modeling (SUFM) module, and an Image-Prompt Embedding Fusion (IPEF) module integrated into the SAM mask decoder. Specifically, AGM models the uncertainty distribution of shallow features through the SUFM module to generate bounding box prompts for the target domain, enabling fully automated segmentation with SAM. The IPEF module integrates multiscale information from SAM image embeddings and prompt embeddings to capture global and local details of the target object, enabling SAM to mitigate the impact of poor prompts. Extensive experiments on publicly available prostate and fundus vessel datasets validate the effectiveness of FA-SAM and highlight its potential to address the above challenges.</li>
<li><strong>摘要：</strong>尽管用于医学图像分割的基于SAM的单源域概括模型可以减轻域转移对跨域场景中模型的影响，但这些模型仍然面临两个主要挑战。首先，SAM的分割高度依赖于特定于域的专家注销的提示，该提示可防止SAM实现完全自动化的医疗图像分割，因此限制了其在临床环境中的应用。其次，提供较差的提示（例如太小或太大的边界框），sam提示编码器可能会误导SAM生成错误的掩码结果。因此，我们提出了FA-SAM，这是实现完全自动化SAM的医学图像分割的单源域概括框架。 FA-SAM介绍了两个关键创新：配备浅特征不确定性建模（SUFM）模块的自动促进的生成模型（AGM）分支，以及集成到SAM Mask Decoder中的Image-Prompt嵌入融合（IPEF）模块。具体而言，AGM通过SUFM模块对浅层特征的不确定性分布进行建模，以生成目标域的边界框提示，从而可以使用SAM进行全自动分割。 IPEF模块集成了来自SAM Image嵌入的多尺度信息，并提示嵌入以捕获目标对象的全球和本地详细信息，从而使SAM能够减轻不良提示的影响。公开可用的前列腺和眼底船只数据集的广泛实验验证了FA-SAM的有效性，并强调了其解决上述挑战的潜力。</li>
</ul>

<h3>Title: Decentralized Federated Learning of Probabilistic Generative Classifiers</h3>
<ul>
<li><strong>Authors: </strong>Aritz Pérez, Carlos Echegoyen, Guzmán Santafé</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17285">https://arxiv.org/abs/2507.17285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17285">https://arxiv.org/pdf/2507.17285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17285]] Decentralized Federated Learning of Probabilistic Generative Classifiers(https://arxiv.org/abs/2507.17285)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Federated learning is a paradigm of increasing relevance in real world applications, aimed at building a global model across a network of heterogeneous users without requiring the sharing of private data. We focus on model learning over decentralized architectures, where users collaborate directly to update the global model without relying on a central server. In this context, the current paper proposes a novel approach to collaboratively learn probabilistic generative classifiers with a parametric form. The framework is composed by a communication network over a set of local nodes, each of one having its own local data, and a local updating rule. The proposal involves sharing local statistics with neighboring nodes, where each node aggregates the neighbors' information and iteratively learns its own local classifier, which progressively converges to a global model. Extensive experiments demonstrate that the algorithm consistently converges to a globally competitive model across a wide range of network topologies, network sizes, local dataset sizes, and extreme non-i.i.d. data distributions.</li>
<li><strong>摘要：</strong>联邦学习是在现实世界应用中增加相关性的范式，旨在在不需要共享私人数据的情况下建立一个在异质用户网络上建立全球模型。我们专注于模型学习，而不是分散体系结构，在该体系结构中，用户直接协作以更新全局模型而不依赖中央服务器。在这种情况下，当前的论文提出了一种新颖的方法，可以通过参数形式学习概率生成分类器。该框架是由一组本地节点的通信网络组成的，每个节点都有自己的本地数据，以及本地更新规则。该提案涉及与邻近节点共享本地统计信息，每个节点汇总了邻居的信息，并迭代地学习了自己的本地分类器，该分类器逐渐收敛到全球模型。广泛的实验表明，该算法始终在各种网络拓扑，网络大小，本地数据集大小和极端非I.I.I.D上始终收敛到全球竞争模型。数据分布。</li>
</ul>

<h3>Title: EarthLink: Interpreting Climate Signals with Self-Evolving AI Agents</h3>
<ul>
<li><strong>Authors: </strong>Zijie Guo, Jiong Wang, Xiaoyu Yue, Wangxu Wei, Zhe Jiang, Wanghan Xu, Ben Fei, Wenlong Zhang, Xinyu Gu, Lijing Cheng, Jing-Jia Luo, Chao Li, Yaqiang Wang, Tao Chen, Wanli Ouyang, Fenghua Ling, Lei Bai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.ao-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17311">https://arxiv.org/abs/2507.17311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17311">https://arxiv.org/pdf/2507.17311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17311]] EarthLink: Interpreting Climate Signals with Self-Evolving AI Agents(https://arxiv.org/abs/2507.17311)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Modern Earth science is at an inflection point. The vast, fragmented, and complex nature of Earth system data, coupled with increasingly sophisticated analytical demands, creates a significant bottleneck for rapid scientific discovery. Here we introduce EarthLink, the first AI agent designed as an interactive copilot for Earth scientists. It automates the end-to-end research workflow, from planning and code generation to multi-scenario analysis. Unlike static diagnostic tools, EarthLink can learn from user interaction, continuously refining its capabilities through a dynamic feedback loop. We validated its performance on a number of core scientific tasks of climate change, ranging from model-observation comparisons to the diagnosis of complex phenomena. In a multi-expert evaluation, EarthLink produced scientifically sound analyses and demonstrated an analytical competency that was rated as comparable to specific aspects of a human junior researcher's workflow. Additionally, its transparent, auditable workflows and natural language interface empower scientists to shift from laborious manual execution to strategic oversight and hypothesis generation. EarthLink marks a pivotal step towards an efficient, trustworthy, and collaborative paradigm for Earth system research in an era of accelerating global change.</li>
<li><strong>摘要：</strong>现代地球科学处于拐点处。地球系统数据的巨大，零散且复杂的性质，再加上日益复杂的分析需求，为快速的科学发现创造了重要的瓶颈。在这里，我们介绍了Earthlink，这是第一个设计为地球科学家的交互式副本的AI代理。它可以自动化端到端的研究工作流程，从计划和代码生成到多幕科分析。与静态诊断工具不同，Earthlink可以从用户交互中学习，通过动态反馈循环不断地完善其功能。我们验证了其在气候变化的许多核心科学任务上的表现，从模型观察比较到复杂现象的诊断。在多专家评估中，Earthlink进行了科学的声音分析，并证明了一种分析能力，该分析能力与人类初级研究人员工作流的特定方面相当。此外，其透明，可审核的工作流程和自然语言界面使科学家能够从易于手动执行转变为战略监督和假设产生。 Earthlink标志着在加速全球变化的时代，迈向了地球系统研究的高效，值得信赖和协作范式的关键一步。</li>
</ul>

<h3>Title: CartoonAlive: Towards Expressive Live2D Modeling from Single Portraits</h3>
<ul>
<li><strong>Authors: </strong>Chao He, Jianqiang Ren, Jianjing Xiang, Xiejie Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17327">https://arxiv.org/abs/2507.17327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17327">https://arxiv.org/pdf/2507.17327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17327]] CartoonAlive: Towards Expressive Live2D Modeling from Single Portraits(https://arxiv.org/abs/2507.17327)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of large foundation models, AIGC, cloud rendering, and real-time motion capture technologies, digital humans are now capable of achieving synchronized facial expressions and body movements, engaging in intelligent dialogues driven by natural language, and enabling the fast creation of personalized avatars. While current mainstream approaches to digital humans primarily focus on 3D models and 2D video-based representations, interactive 2D cartoon-style digital humans have received relatively less attention. Compared to 3D digital humans that require complex modeling and high rendering costs, and 2D video-based solutions that lack flexibility and real-time interactivity, 2D cartoon-style Live2D models offer a more efficient and expressive alternative. By simulating 3D-like motion through layered segmentation without the need for traditional 3D modeling, Live2D enables dynamic and real-time manipulation. In this technical report, we present CartoonAlive, an innovative method for generating high-quality Live2D digital humans from a single input portrait image. CartoonAlive leverages the shape basis concept commonly used in 3D face modeling to construct facial blendshapes suitable for Live2D. It then infers the corresponding blendshape weights based on facial keypoints detected from the input image. This approach allows for the rapid generation of a highly expressive and visually accurate Live2D model that closely resembles the input portrait, within less than half a minute. Our work provides a practical and scalable solution for creating interactive 2D cartoon characters, opening new possibilities in digital content creation and virtual character animation. The project homepage is this https URL.</li>
<li><strong>摘要：</strong>随着大型基础模型，AIGC，云渲染和实时运动捕获技术的快速发展，数字人类现在能够实现同步的面部表情和身体运动，参与了由自然语言驱动的智能对话，并能够快速创建个性化的头像。尽管当前的数字人类主流方法主要集中在3D模型和基于2D视频的表示上，但交互式2D卡通风格的数字人类的关注相对较少。与需要复杂建模和高渲染成本以及缺乏灵活性和实时交互性的2D视频解决方案相比，2D卡通风格的Live2D模型提供了更有效和表现力的替代方案。通过在不需要传统3D建模的情况下通过分层分割模拟3D样运动，LIVE2D可以动态和实时操作。在这份技术报告中，我们提出了漫画，这是一种创新方法，用于从单个输入肖像图像中产生高质量的Live2D数字人类。漫画利用了3D面部建模中常用的形状基础概念，以构建适合Live2D的面部搅拌。然后，它根据从输入图像中检测到的面部关键点来输入相应的混合物重量。这种方法允许在不到半分钟的时间内快速生成高度表达和视觉精确的Live2D模型，该模型与输入肖像相似。我们的工作提供了一种实用且可扩展的解决方案，用于创建交互式2D卡通字符，在数字内容创建和虚拟字符动画中开辟了新的可能性。项目主页是此HTTPS URL。</li>
</ul>

<h3>Title: PARTE: Part-Guided Texturing for 3D Human Reconstruction from a Single Image</h3>
<ul>
<li><strong>Authors: </strong>Hyeongjin Nam, Donghwan Kim, Gyeongsik Moon, Kyoung Mu Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17332">https://arxiv.org/abs/2507.17332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17332">https://arxiv.org/pdf/2507.17332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17332]] PARTE: Part-Guided Texturing for 3D Human Reconstruction from a Single Image(https://arxiv.org/abs/2507.17332)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The misaligned human texture across different human parts is one of the main limitations of existing 3D human reconstruction methods. Each human part, such as a jacket or pants, should maintain a distinct texture without blending into others. The structural coherence of human parts serves as a crucial cue to infer human textures in the invisible regions of a single image. However, most existing 3D human reconstruction methods do not explicitly exploit such part segmentation priors, leading to misaligned textures in their reconstructions. In this regard, we present PARTE, which utilizes 3D human part information as a key guide to reconstruct 3D human textures. Our framework comprises two core components. First, to infer 3D human part information from a single image, we propose a 3D part segmentation module (PartSegmenter) that initially reconstructs a textureless human surface and predicts human part labels based on the textureless surface. Second, to incorporate part information into texture reconstruction, we introduce a part-guided texturing module (PartTexturer), which acquires prior knowledge from a pre-trained image generation network on texture alignment of human parts. Extensive experiments demonstrate that our framework achieves state-of-the-art quality in 3D human reconstruction. The project page is available at this https URL.</li>
<li><strong>摘要：</strong>跨不同人类部位的未对准人质地是现有3D人类重建方法的主要局限性之一。每个人类的部分，例如外套或裤子，都应保持独特的质地，而不融入他人。人类部位的结构连贯性是推断单个图像无形区域中人类纹理的关键提示。但是，大多数现有的3D人类重建方法并未明确利用此类部分分割先验，从而导致其重建中的纹理未对准。在这方面，我们介绍了Parte，该Parte利用3D人类部分信息作为重建3D人类纹理的关键指南。我们的框架包括两个核心组件。首先，为了从单个图像推断3D人类部分信息，我们提出了一个3D部分分割模块（partsementemer），该模块最初会重建无纹理的人体表面，并根据无纹理表面预测人类部分标签。其次，为了将部分信息纳入纹理重建中，我们引入了一个部分引导的纹理模块（PartTexturer），该模块从预先训练的图像生成网络中获取有关人类部位纹理比对的先验知识。广泛的实验表明，我们的框架在3D人类重建方面达到了最先进的质量。该项目页面可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: DynaSearcher: Dynamic Knowledge Graph Augmented Search Agent via Multi-Reward Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Chuzhan Hao, Wenfeng Feng, Yuewei Zhang, Hao Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17365">https://arxiv.org/abs/2507.17365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17365">https://arxiv.org/pdf/2507.17365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17365]] DynaSearcher: Dynamic Knowledge Graph Augmented Search Agent via Multi-Reward Reinforcement Learning(https://arxiv.org/abs/2507.17365)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multi-step agentic retrieval systems based on large language models (LLMs) have demonstrated remarkable performance in complex information search tasks. However, these systems still face significant challenges in practical applications, particularly in generating factually inconsistent intermediate queries and inefficient search trajectories, which can lead to reasoning deviations or redundant computations. To address these issues, we propose DynaSearcher, an innovative search agent enhanced by dynamic knowledge graphs and multi-reward reinforcement learning (RL). Specifically, our system leverages knowledge graphs as external structured knowledge to guide the search process by explicitly modeling entity relationships, thereby ensuring factual consistency in intermediate queries and mitigating biases from irrelevant information. Furthermore, we employ a multi-reward RL framework for fine-grained control over training objectives such as retrieval accuracy, efficiency, and response quality. This framework promotes the generation of high-quality intermediate queries and comprehensive final answers, while discouraging unnecessary exploration and minimizing information omissions or redundancy. Experimental results demonstrate that our approach achieves state-of-the-art answer accuracy on six multi-hop question answering datasets, matching frontier LLMs while using only small-scale models and limited computational resources. Furthermore, our approach demonstrates strong generalization and robustness across diverse retrieval environments and larger-scale models, highlighting its broad applicability.</li>
<li><strong>摘要：</strong>基于大语言模型（LLM）的多步代理检索系统在复杂的信息搜索任务中表现出了显着的性能。但是，这些系统在实际应用中仍然面临重大挑战，尤其是在产生事实不一致的中间查询和效率低下的搜索轨迹时，这可能导致推理偏差或冗余计算。为了解决这些问题，我们提出了Dynasearcher，这是一种创新的搜索代理，通过动态知识图和多回报增强学习（RL）增强。具体而言，我们的系统利用知识图作为外部结构化知识来指导搜索过程，通过明确建模实体关系，从而确保中间查询的事实一致性并减轻与无关信息的偏见。此外，我们采用多回报RL框架来对培训目标进行细粒度控制，例如检索准确性，效率和响应质量。该框架促进了高质量的中间查询和全面的最终答案的产生，同时劝阻不必要的探索并最大程度地减少信息的遗漏或冗余。实验结果表明，我们的方法在六个多跳的问题答案数据集上实现了最新的答案准确性，仅使用小规模型号和有限的计算资源，使Frontier LLM匹配了Frontier LLM。此外，我们的方法表明了各种检索环境和大型模型之间的强烈概括和鲁棒性，从而突出了其广泛的适用性。</li>
</ul>

<h3>Title: EndoGen: Conditional Autoregressive Endoscopic Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Liu, Hengyu Liu, Cheng Wang, Tianming Liu, Yixuan Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17388">https://arxiv.org/abs/2507.17388</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17388">https://arxiv.org/pdf/2507.17388</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17388]] EndoGen: Conditional Autoregressive Endoscopic Video Generation(https://arxiv.org/abs/2507.17388)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Endoscopic video generation is crucial for advancing medical imaging and enhancing diagnostic capabilities. However, prior efforts in this field have either focused on static images, lacking the dynamic context required for practical applications, or have relied on unconditional generation that fails to provide meaningful references for clinicians. Therefore, in this paper, we propose the first conditional endoscopic video generation framework, namely EndoGen. Specifically, we build an autoregressive model with a tailored Spatiotemporal Grid-Frame Patterning (SGP) strategy. It reformulates the learning of generating multiple frames as a grid-based image generation pattern, which effectively capitalizes the inherent global dependency modeling capabilities of autoregressive architectures. Furthermore, we propose a Semantic-Aware Token Masking (SAT) mechanism, which enhances the model's ability to produce rich and diverse content by selectively focusing on semantically meaningful regions during the generation process. Through extensive experiments, we demonstrate the effectiveness of our framework in generating high-quality, conditionally guided endoscopic content, and improves the performance of downstream task of polyp segmentation. Code released at this https URL.</li>
<li><strong>摘要：</strong>内窥镜视频生成对于提高医学成像和增强诊断能力至关重要。但是，该领域的先前努力要么集中在静态图像上，缺乏实际应用所需的动态环境，要么依赖于未能为临床医生提供有意义参考的无条件产生。因此，在本文中，我们提出了第一个条件内窥镜视频生成框架，即内源。具体而言，我们通过量身定制的时空网格框架（SGP）策略构建了自回旋模型。它重新制定了将多个帧作为基于网格的图像生成模式的学习，该模式有效地利用了自动回归体系结构的固有全球依赖建模能力。此外，我们提出了一种语义感知的令牌掩蔽（SAT）机制，从而增强了模型在生成过程中选择性地关注语义上有意义的区域，从而增强了模型生产丰富和多样的内容的能力。通过广泛的实验，我们证明了框架在产生高质量，有条件的内窥镜含量并提高下游息肉分割任务的性能方面的有效性。在此HTTPS URL上发布的代码。</li>
</ul>

<h3>Title: SRMambaV2: Biomimetic Attention for Sparse Point Cloud Upsampling in Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Chuang Chen, Xiaolin Qin, Jing Hu, Wenyi Ge</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17479">https://arxiv.org/abs/2507.17479</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17479">https://arxiv.org/pdf/2507.17479</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17479]] SRMambaV2: Biomimetic Attention for Sparse Point Cloud Upsampling in Autonomous Driving(https://arxiv.org/abs/2507.17479)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Upsampling LiDAR point clouds in autonomous driving scenarios remains a significant challenge due to the inherent sparsity and complex 3D structures of the data. Recent studies have attempted to address this problem by converting the complex 3D spatial scenes into 2D image super-resolution tasks. However, due to the sparse and blurry feature representation of range images, accurately reconstructing detailed and complex spatial topologies remains a major difficulty. To tackle this, we propose a novel sparse point cloud upsampling method named SRMambaV2, which enhances the upsampling accuracy in long-range sparse regions while preserving the overall geometric reconstruction quality. Specifically, inspired by human driver visual perception, we design a biomimetic 2D selective scanning self-attention (2DSSA) mechanism to model the feature distribution in distant sparse areas. Meanwhile, we introduce a dual-branch network architecture to enhance the representation of sparse features. In addition, we introduce a progressive adaptive loss (PAL) function to further refine the reconstruction of fine-grained details during the upsampling process. Experimental results demonstrate that SRMambaV2 achieves superior performance in both qualitative and quantitative evaluations, highlighting its effectiveness and practical value in automotive sparse point cloud upsampling tasks.</li>
<li><strong>摘要：</strong>由于数据的固有稀疏性和复杂的3D结构，在自主驾驶场景中提高激光雷达点云仍然是一个重大挑战。最近的研究试图通过将复杂的3D空间场景转换为2D图像超分辨率任务来解决这一问题。但是，由于范围图像的稀疏和模糊特征表示，准确地重建了详细且复杂的空间拓扑结构仍然是一个主要困难。为了解决这个问题，我们提出了一种名为SRMAMBAV2的新型稀疏点云上采样方法，该方法提高了远程稀疏区域的上采样精度，同时保留了整体几何重建质量。具体而言，受到人类驱动器视觉感知的启发，我们设计了一种仿生2D选择性扫描自我注意力（2DSSA）机制，以模拟遥远稀疏区域中的特征分布。同时，我们引入了双分支网络体系结构，以增强稀疏功能的表示。此外，我们引入了渐进式自适应损失（PAL）功能，以进一步完善在提升过程中细粒细节的重建。实验结果表明，SRMAMBAV2在定性和定量评估中都取得了卓越的性能，突出了其在汽车稀疏点云上的有效性和实际价值。</li>
</ul>

<h3>Title: Unsupervised anomaly detection using Bayesian flow networks: application to brain FDG PET in the context of Alzheimer's disease</h3>
<ul>
<li><strong>Authors: </strong>Hugues Roy, Reuben Dorent, Ninon Burgos</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17486">https://arxiv.org/abs/2507.17486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17486">https://arxiv.org/pdf/2507.17486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17486]] Unsupervised anomaly detection using Bayesian flow networks: application to brain FDG PET in the context of Alzheimer's disease(https://arxiv.org/abs/2507.17486)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Unsupervised anomaly detection (UAD) plays a crucial role in neuroimaging for identifying deviations from healthy subject data and thus facilitating the diagnosis of neurological disorders. In this work, we focus on Bayesian flow networks (BFNs), a novel class of generative models, which have not yet been applied to medical imaging or anomaly detection. BFNs combine the strength of diffusion frameworks and Bayesian inference. We introduce AnoBFN, an extension of BFNs for UAD, designed to: i) perform conditional image generation under high levels of spatially correlated noise, and ii) preserve subject specificity by incorporating a recursive feedback from the input image throughout the generative process. We evaluate AnoBFN on the challenging task of Alzheimer's disease-related anomaly detection in FDG PET images. Our approach outperforms other state-of-the-art methods based on VAEs (beta-VAE), GANs (f-AnoGAN), and diffusion models (AnoDDPM), demonstrating its effectiveness at detecting anomalies while reducing false positive rates.</li>
<li><strong>摘要：</strong>无监督的异常检测（UAD）在识别健康受试者数据的偏差并从而促进神经系统疾病的诊断方面起着至关重要的作用。在这项工作中，我们专注于贝叶斯流动网络（BFN），这是一种新型的生成模型，尚未应用于医学成像或异常检测。 BFN结合了扩散框架和贝叶斯推断的强度。我们引入了AnoBFN，即UAD的BFN扩展，旨在：i）在高水平的空间相关噪声下进行有条件的图像产生，ii）ii）通过在整个生成过程中加入输入图像中的递归反馈来保留主题特异性。我们评估了ANOBFN关于在FDG PET图像中阿尔茨海默氏病与疾病相关的异常检测的挑战性任务。我们的方法表现优于基于VAE（beta-vae），gans（F-Anogan）和扩散模型（阳极）（阳极）的其他最先进方法，证明了其在检测异常时的有效性，同时降低了假阳性速率。</li>
</ul>

<h3>Title: DFDNet: Dynamic Frequency-Guided De-Flare Network</h3>
<ul>
<li><strong>Authors: </strong>Minglong Xue, Aoxiang Ning, Shivakumara Palaiahnakote, Mingliang Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17489">https://arxiv.org/abs/2507.17489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17489">https://arxiv.org/pdf/2507.17489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17489]] DFDNet: Dynamic Frequency-Guided De-Flare Network(https://arxiv.org/abs/2507.17489)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Strong light sources in nighttime photography frequently produce flares in images, significantly degrading visual quality and impacting the performance of downstream tasks. While some progress has been made, existing methods continue to struggle with removing large-scale flare artifacts and repairing structural damage in regions near the light source. We observe that these challenging flare artifacts exhibit more significant discrepancies from the reference images in the frequency domain compared to the spatial domain. Therefore, this paper presents a novel dynamic frequency-guided deflare network (DFDNet) that decouples content information from flare artifacts in the frequency domain, effectively removing large-scale flare artifacts. Specifically, DFDNet consists mainly of a global dynamic frequency-domain guidance (GDFG) module and a local detail guidance module (LDGM). The GDFG module guides the network to perceive the frequency characteristics of flare artifacts by dynamically optimizing global frequency domain features, effectively separating flare information from content information. Additionally, we design an LDGM via a contrastive learning strategy that aligns the local features of the light source with the reference image, reduces local detail damage from flare removal, and improves fine-grained image restoration. The experimental results demonstrate that the proposed method outperforms existing state-of-the-art methods in terms of performance. The code is available at \href{this https URL}{this https URL}.</li>
<li><strong>摘要：</strong>夜间摄影中的强光源经常在图像中产生耀斑，从而显着降低视觉质量并影响下游任务的性能。尽管取得了一些进展，但现有的方法仍在消除大规模的耀斑伪像并修复光源附近地区的结构损害方面继续困难。我们观察到，与空间域相比，这些具有挑战性的耀斑伪影与频域中的参考图像显示出更大的差异。因此，本文提出了一个新型的动态频率引导网络（DFDNET），该网络将内容信息与频域中的耀斑伪像，有效地消除了大规模的耀斑伪像。具体而言，DFDNET主要由全局动态频域指导（GDFG）模块和局部详细指南指南（LDGM）组成。 GDFG模块通过动态优化全球频域特征，将网络引导感知耀斑伪像的频率特征，从而有效地将火炬信息与内容信息分开。此外，我们通过对比度学习策略设计了一个LDGM，该策略将光源的本地特征与参考图像保持一致，从而减少了除去火炬的局部细节损害，并改善了细粒度的图像恢复。实验结果表明，所提出的方法在性能方面优于现有的最新方法。该代码可在\ href {this HTTPS url} {此https url}中获得。</li>
</ul>

<h3>Title: Accelerating Parallel Diffusion Model Serving with Residual Compression</h3>
<ul>
<li><strong>Authors: </strong>Jiajun Luo, Yicheng Xiao, Jianru Xu, Yangxiu You, Rongwei Lu, Chen Tang, Jingyan Jiang, Zhi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17511">https://arxiv.org/abs/2507.17511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17511">https://arxiv.org/pdf/2507.17511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17511]] Accelerating Parallel Diffusion Model Serving with Residual Compression(https://arxiv.org/abs/2507.17511)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models produce realistic images and videos but require substantial computational resources, necessitating multi-accelerator parallelism for real-time deployment. However, parallel inference introduces significant communication overhead from exchanging large activations between devices, limiting efficiency and scalability. We present CompactFusion, a compression framework that significantly reduces communication while preserving generation quality. Our key observation is that diffusion activations exhibit strong temporal redundancy-adjacent steps produce highly similar activations, saturating bandwidth with near-duplicate data carrying little new information. To address this inefficiency, we seek a more compact representation that encodes only the essential information. CompactFusion achieves this via Residual Compression that transmits only compressed residuals (step-wise activation differences). Based on empirical analysis and theoretical justification, we show that it effectively removes redundant data, enabling substantial data reduction while maintaining high fidelity. We also integrate lightweight error feedback to prevent error accumulation. CompactFusion establishes a new paradigm for parallel diffusion inference, delivering lower latency and significantly higher generation quality than prior methods. On 4xL20, it achieves 3.0x speedup while greatly improving fidelity. It also uniquely supports communication-heavy strategies like sequence parallelism on slow networks, achieving 6.7x speedup over prior overlap-based method. CompactFusion applies broadly across diffusion models and parallel settings, and integrates easily without requiring pipeline rework. Portable implementation demonstrated on xDiT is publicly available at this https URL</li>
<li><strong>摘要：</strong>扩散模型会产生逼真的图像和视频，但需要大量的计算资源，因此需要多加速器并行进行实时部署。但是，并行推理从设备之间交换大量激活，限制效率和可扩展性，引入了大量的通信开销。我们提出了CompactFusion，这是一个压缩框架，可大大降低交流，同时保持发电质量。我们的关键观察结果是，扩散激活表现出强烈的时间冗余 - 粘附的步骤产生高度相似的激活，使带宽饱和，并带有近乎纤维化的数据，几乎没有新信息。为了解决这种效率低下，我们寻求更紧凑的表示形式，仅编码基本信息。 CompactFusion通过残留压缩来实现此功能，该压缩仅传输压缩残差（逐步激活差异）。基于经验分析和理论上的理由，我们表明它有效地消除了冗余数据，从而可以减少大量数据，同时保持高忠诚度。我们还集成了轻巧的错误反馈，以防止误差积累。 CompactFusion为平行扩散推断建立了一种新的范式，比先前的方法具有较低的潜伏期和更高的发电质量。在4XL20上，它可以达到3.0倍的速度，同时大大提高了保真度。它还独特地支持较重的策略，例如慢速网络上的序列并行性，在先前基于重叠的方法上实现了6.7倍的速度。 CompactFusion在扩散模型和并行设置中广泛应用，并在不需要管道返工的情况下轻松集成。 XDIT上展示的便携式实现可在此HTTPS URL上公开获得</li>
</ul>

<h3>Title: HOTA: Hamiltonian framework for Optimal Transport Advection</h3>
<ul>
<li><strong>Authors: </strong>Nazar Buzun, Daniil Shlenskii, Maxim Bobrin, Dmitry V. Dylov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17513">https://arxiv.org/abs/2507.17513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17513">https://arxiv.org/pdf/2507.17513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17513]] HOTA: Hamiltonian framework for Optimal Transport Advection(https://arxiv.org/abs/2507.17513)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Optimal transport (OT) has become a natural framework for guiding the probability flows. Yet, the majority of recent generative models assume trivial geometry (e.g., Euclidean) and rely on strong density-estimation assumptions, yielding trajectories that do not respect the true principles of optimality in the underlying manifold. We present Hamiltonian Optimal Transport Advection (HOTA), a Hamilton-Jacobi-Bellman based method that tackles the dual dynamical OT problem explicitly through Kantorovich potentials, enabling efficient and scalable trajectory optimization. Our approach effectively evades the need for explicit density modeling, performing even when the cost functionals are non-smooth. Empirically, HOTA outperforms all baselines in standard benchmarks, as well as in custom datasets with non-differentiable costs, both in terms of feasibility and optimality.</li>
<li><strong>摘要：</strong>最佳运输（OT）已成为指导概率流的自然框架。然而，大多数最近的生成模型都假定了微不足道的几何形状（例如欧几里得）并依赖于强密度估计假设，从而产生了不尊重基础歧管中最佳原理的轨迹。我们提出了汉密尔顿最佳运输对流（HOTA），这是一种基于汉密尔顿 - 雅各比 - 贝尔曼的方法，该方法通过坎托洛维奇电位明确解决了双重动力学问题，从而实现了有效且可扩展的轨迹优化。我们的方法有效地避免了对显式密度建模的需求，即使成本功能不平滑，也可以执行。从经验上讲，HOTA在标准基准和具有非差异成本的自定义数据集中均优于所有基准，无论是在可行性和最佳性方面。</li>
</ul>

<h3>Title: URPO: A Unified Reward & Policy Optimization Framework for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Songshuo Lu, Hua Wang, Zhi Chen, Yaohua Tang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17515">https://arxiv.org/abs/2507.17515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17515">https://arxiv.org/pdf/2507.17515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17515]] URPO: A Unified Reward & Policy Optimization Framework for Large Language Models(https://arxiv.org/abs/2507.17515)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Large-scale alignment pipelines typically pair a policy model with a separately trained reward model whose parameters remain frozen during reinforcement learning (RL). This separation creates a complex, resource-intensive pipeline and suffers from a performance ceiling due to a static reward signal. We propose a novel framework, Unified Reward & Policy Optimization (URPO), that unifies instruction-following ("player") and reward modeling ("referee") within a single model and a single training phase. Our method recasts all alignment data-including preference pairs, verifiable reasoning, and open-ended instructions-into a unified generative format optimized by a single Group-Relative Policy Optimization (GRPO) loop. This enables the model to learn from ground-truth preferences and verifiable logic while simultaneously generating its own rewards for open-ended tasks. Experiments on the Qwen2.5-7B model demonstrate URPO's superiority. Our unified model significantly outperforms a strong baseline using a separate generative reward model, boosting the instruction-following score on AlpacaEval from 42.24 to 44.84 and the composite reasoning average from 32.66 to 35.66. Furthermore, URPO cultivates a superior internal evaluator as a byproduct of training, achieving a RewardBench score of 85.15 and surpassing the dedicated reward model it replaces (83.55). By eliminating the need for a separate reward model and fostering a co-evolutionary dynamic between generation and evaluation, URPO presents a simpler, more efficient, and more effective path towards robustly aligned language models.</li>
<li><strong>摘要：</strong>大规模的对齐管道通常将策略模型与单独训练的奖励模型配对，其参数在加固学习过程中保持冻结（RL）。这种分离产生了一条复杂，资源密集的管道，由于静态奖励信号，性能上限会遭受性能。我们提出了一个新颖的框架，统一的奖励和政策优化（URPO），该框架在单个模型和单个培训阶段统一了指导跟随（“玩家”）和奖励建模（“裁判”）。我们的方法重铸了所有对齐数据，包括偏好对，可验证的推理和开放式指令 -  into通过单个组相关策略优化（GRPO）循环优化的统一生成格式。这使该模型能够从基本偏好和可验证的逻辑中学习，同时为开放式任务产生自己的奖励。 QWEN2.5-7B模型的实验证明了URPO的优势。我们的统一模型使用单独的生成奖励模型极大地优于强大的基线，从而将Alpacaeval的指令跟随得分从42.24提高到44.84，并将复合推理的平均值从32.66提高到35.66。此外，URPO培养了一个卓越的内部评估者作为培训的副产品，达到85.15的奖励台得分并超过了它代替的专用奖励模型（83.55）。通过消除对单独的奖励模型的需求，并促进了生成和评估之间的共同进化动态，URPO提出了一种更简单，更高效，更有效的路径，以牢固地排列语言模型。</li>
</ul>

<h3>Title: Multi-modal Multi-task Pre-training for Improved Point Cloud Understanding</h3>
<ul>
<li><strong>Authors: </strong>Liwen Liu, Weidong Yang, Lipeng Ma, Ben Fei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17533">https://arxiv.org/abs/2507.17533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17533">https://arxiv.org/pdf/2507.17533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17533]] Multi-modal Multi-task Pre-training for Improved Point Cloud Understanding(https://arxiv.org/abs/2507.17533)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in multi-modal pre-training methods have shown promising effectiveness in learning 3D representations by aligning multi-modal features between 3D shapes and their corresponding 2D counterparts. However, existing multi-modal pre-training frameworks primarily rely on a single pre-training task to gather multi-modal data in 3D applications. This limitation prevents the models from obtaining the abundant information provided by other relevant tasks, which can hinder their performance in downstream tasks, particularly in complex and diverse domains. In order to tackle this issue, we propose MMPT, a Multi-modal Multi-task Pre-training framework designed to enhance point cloud understanding. Specifically, three pre-training tasks are devised: (i) Token-level reconstruction (TLR) aims to recover masked point tokens, endowing the model with representative learning abilities. (ii) Point-level reconstruction (PLR) is integrated to predict the masked point positions directly, and the reconstructed point cloud can be considered as a transformed point cloud used in the subsequent task. (iii) Multi-modal contrastive learning (MCL) combines feature correspondences within and across modalities, thus assembling a rich learning signal from both 3D point cloud and 2D image modalities in a self-supervised manner. Moreover, this framework operates without requiring any 3D annotations, making it scalable for use with large datasets. The trained encoder can be effectively transferred to various downstream tasks. To demonstrate its effectiveness, we evaluated its performance compared to state-of-the-art methods in various discriminant and generative applications under widely-used benchmarks.</li>
<li><strong>摘要：</strong>多模式前训练方法的最新进展通过对齐3D形状及其相应的2D对应物之间的多模式特征来学习3D表示方面的有效性。但是，现有的多模式预训练框架主要依赖于单个训练任务来收集3D应用程序中的多模式数据。此限制阻止了模型获得其他相关任务提供的丰富信息，这可能会阻碍其在下游任务中的性能，尤其是在复杂而多样的域中。为了解决此问题，我们提出了MMPT，这是一个多模式的多任务预训练框架，旨在增强点云的理解。具体而言，设计了三个预训练任务：（i）令牌级重建（TLR）旨在恢复蒙版的点代币，并赋予该模型具有代表性的学习能力。 （ii）集成点级重建（PLR）以直接预测掩盖点位置，并且可以将重建点云视为后续任务中使用的转换点云。 （iii）多模式对比度学习（MCL）结合了跨模态内和跨模态的特征对应关系，因此以自我监督的方式从3D点云和2D图像模态组装了丰富的学习信号。此外，此框架可以运行，而无需任何3D注释，因此可扩展用于大型数据集。训练有素的编码器可以有效地转移到各种下游任务中。为了证明其有效性，我们评估了其在广泛使用基准下的各种判别和生成应用中的最新方法中的性能。</li>
</ul>

<h3>Title: An h-space Based Adversarial Attack for Protection Against Few-shot Personalization</h3>
<ul>
<li><strong>Authors: </strong>Xide Xu, Sandesh Kamath, Muhammad Atif Butt, Bogdan Raducanu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17554">https://arxiv.org/abs/2507.17554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17554">https://arxiv.org/pdf/2507.17554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17554]] An h-space Based Adversarial Attack for Protection Against Few-shot Personalization(https://arxiv.org/abs/2507.17554)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The versatility of diffusion models in generating customized images from few samples raises significant privacy concerns, particularly regarding unauthorized modifications of private content. This concerning issue has renewed the efforts in developing protection mechanisms based on adversarial attacks, which generate effective perturbations to poison diffusion models. Our work is motivated by the observation that these models exhibit a high degree of abstraction within their semantic latent space (`h-space'), which encodes critical high-level features for generating coherent and meaningful content. In this paper, we propose a novel anti-customization approach, called HAAD (h-space based Adversarial Attack for Diffusion models), that leverages adversarial attacks to craft perturbations based on the h-space that can efficiently degrade the image generation process. Building upon HAAD, we further introduce a more efficient variant, HAAD-KV, that constructs perturbations solely based on the KV parameters of the h-space. This strategy offers a stronger protection, that is computationally less expensive. Despite their simplicity, our methods outperform state-of-the-art adversarial attacks, highlighting their effectiveness.</li>
<li><strong>摘要：</strong>扩散模型从几个样本中生成定制图像的多功能性引起了重大的隐私问题，尤其是关于私人内容的未经授权修改。有关问题的这一问题已重新制定基于对抗性攻击的保护机制的努力，从而产生了有效的毒药扰动毒物扩散模型。我们的工作是由观察到的，即这些模型在其语义潜在空间（“ H-Space”）中表现出高度的抽象，该模型编码了关键的高级特征，以产生相干和有意义的内容。在本文中，我们提出了一种新型的抗燃烧方法，称为HAAD（基于H空间的对抗性攻击扩散模型），该方法利用了对抗性攻击来基于H-Space来制作扰动，从而可以有效地降低图像生成过程。在HAAD的基础上，我们进一步引入了更有效的变体HAAD-KV，该变体仅基于H空间的KV参数构建扰动。该策略提供了更强的保护，在计算上便宜。尽管它们很简单，但我们的方法表现优于最先进的对抗性攻击，突出了它们的有效性。</li>
</ul>

<h3>Title: PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Maciej K. Wozniak, Lianhang Liu, Yixi Cai, Patric Jensfelt</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17596">https://arxiv.org/abs/2507.17596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17596">https://arxiv.org/pdf/2507.17596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17596]] PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving(https://arxiv.org/abs/2507.17596)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While end-to-end autonomous driving models show promising results, their practical deployment is often hindered by large model sizes, a reliance on expensive LiDAR sensors and computationally intensive BEV feature representations. This limits their scalability, especially for mass-market vehicles equipped only with cameras. To address these challenges, we propose PRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving architecture operates using only camera data, without explicit BEV representation and forgoing the need for LiDAR. PRIX leverages a visual feature extractor coupled with a generative planning head to predict safe trajectories from raw pixel inputs directly. A core component of our architecture is the Context-aware Recalibration Transformer (CaRT), a novel module designed to effectively enhance multi-level visual features for more robust planning. We demonstrate through comprehensive experiments that PRIX achieves state-of-the-art performance on the NavSim and nuScenes benchmarks, matching the capabilities of larger, multimodal diffusion planners while being significantly more efficient in terms of inference speed and model size, making it a practical solution for real-world deployment. Our work is open-source and the code will be at this https URL.</li>
<li><strong>摘要：</strong>尽管端到端的自主驾驶模型显示出令人鼓舞的结果，但它们的实际部署通常受到大型尺寸的阻碍，依赖昂贵的LiDAR传感器和计算密集型BEV功能表示。这限制了它们的可扩展性，尤其是对于仅配备相机的大众市场车辆。为了应对这些挑战，我们提出了Prix（来自RAW PIXELS的计划）。我们的新颖而有效的端到端驾驶体系结构仅使用相机数据运行，而无需明确的BEV表示并放弃了对LiDAR的需求。 Prix利用视觉功能提取器和生成计划头，以直接预测RAW PIXEL输入的安全轨迹。我们体系结构的核心组成部分是上下文感知的重新校准变压器（CART），这是一个新型模块，旨在有效地增强多层次的视觉特征，以实现更健壮的计划。我们通过全面的实验来证明，在NAVSIM和NUSCENES基准测试中，Prix可以实现最先进的性能，从而匹配较大的，多模式扩散计划者的能力，同时在推理速度和模型尺寸方面更有效地更有效，从而使其成为现实部署的实用解决方案。我们的工作是开源的，代码将在此HTTPS URL。</li>
</ul>

<h3>Title: CNS-Bench: Benchmarking Image Classifier Robustness Under Continuous Nuisance Shifts</h3>
<ul>
<li><strong>Authors: </strong>Olaf Dünkel, Artur Jesslen, Jiahao Xie, Christian Theobalt, Christian Rupprecht, Adam Kortylewski</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17651">https://arxiv.org/abs/2507.17651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17651">https://arxiv.org/pdf/2507.17651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17651]] CNS-Bench: Benchmarking Image Classifier Robustness Under Continuous Nuisance Shifts(https://arxiv.org/abs/2507.17651)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>An important challenge when using computer vision models in the real world is to evaluate their performance in potential out-of-distribution (OOD) scenarios. While simple synthetic corruptions are commonly applied to test OOD robustness, they often fail to capture nuisance shifts that occur in the real world. Recently, diffusion models have been applied to generate realistic images for benchmarking, but they are restricted to binary nuisance shifts. In this work, we introduce CNS-Bench, a Continuous Nuisance Shift Benchmark to quantify OOD robustness of image classifiers for continuous and realistic generative nuisance shifts. CNS-Bench allows generating a wide range of individual nuisance shifts in continuous severities by applying LoRA adapters to diffusion models. To address failure cases, we propose a filtering mechanism that outperforms previous methods, thereby enabling reliable benchmarking with generative models. With the proposed benchmark, we perform a large-scale study to evaluate the robustness of more than 40 classifiers under various nuisance shifts. Through carefully designed comparisons and analyses, we find that model rankings can change for varying shifts and shift scales, which cannot be captured when applying common binary shifts. Additionally, we show that evaluating the model performance on a continuous scale allows the identification of model failure points, providing a more nuanced understanding of model robustness. Project page including code and data: this https URL.</li>
<li><strong>摘要：</strong>在现实世界中使用计算机视觉模型时，一个重要的挑战是评估其在潜在分布（OOD）方案中的性能。尽管通常将简单的合成腐败用于测试OOD的鲁棒性，但它们通常无法捕获现实世界中发生的滋扰转移。最近，扩散模型已应用于生成实现的图像以进行基准测试，但仅限于二进制滋扰转移。在这项工作中，我们引入了CNS基础，这是一种连续的滋扰基准基准，以量化图像分类器的稳健性，以进行连续和现实的生成性滋扰转移。中枢神经系统底板允许通过将洛拉适配器应用于扩散模型来产生连续严重性的各种单个滋扰转移。为了解决故障案例，我们提出了一种过滤机制，该机制优于先前的方法，从而通过生成模型实现可靠的基准测试。通过提出的基准，我们进行了一项大规模研究，以评估40多个分类器在各种滋扰转移下的鲁棒性。通过精心设计的比较和分析，我们发现模型排名可以改变变化和偏移量表，在应用常见的二进制偏移时无法捕获。此外，我们表明，在连续尺度上评估模型性能可以识别模型故障点，从而更加细微地了解模型鲁棒性。项目页面，包括代码和数据：此HTTPS URL。</li>
</ul>

<h3>Title: Attention (as Discrete-Time Markov) Chains</h3>
<ul>
<li><strong>Authors: </strong>Yotam Erel, Olaf Dünkel, Rishabh Dabral, Vladislav Golyanik, Christian Theobalt, Amit H. Bermano</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17657">https://arxiv.org/abs/2507.17657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17657">https://arxiv.org/pdf/2507.17657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17657]] Attention (as Discrete-Time Markov) Chains(https://arxiv.org/abs/2507.17657)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce a new interpretation of the attention matrix as a discrete-time Markov chain. Our interpretation sheds light on common operations involving attention scores such as selection, summation, and averaging in a unified framework. It further extends them by considering indirect attention, propagated through the Markov chain, as opposed to previous studies that only model immediate effects. Our main observation is that tokens corresponding to semantically similar regions form a set of metastable states, where the attention clusters, while noisy attention scores tend to disperse. Metastable states and their prevalence can be easily computed through simple matrix multiplication and eigenanalysis, respectively. Using these lightweight tools, we demonstrate state-of-the-art zero-shot segmentation. Lastly, we define TokenRank -- the steady state vector of the Markov chain, which measures global token importance. We demonstrate that using it brings improvements in unconditional image generation. We believe our framework offers a fresh view of how tokens are being attended in modern visual transformers.</li>
<li><strong>摘要：</strong>我们引入了将注意力矩阵作为离散时间马尔可夫链的新解释。我们的解释阐明了涉及统一框架中选择，求和和平均等注意力评分的常见操作。它通过考虑通过马尔可夫链传播的间接注意力而进一步扩展了它们，而不是先前仅模拟直接影响的研究。我们的主要观察结果是，与语义上相似区域相对应的代币形成了一组亚稳态状态，而注意力集群群体，而嘈杂的注意力评分往往会分散。可以分别通过简单的矩阵乘法和特征分析来轻松计算亚稳态状态及其患病率。使用这些轻量级工具，我们演示了最新的零摄像分段。最后，我们定义了Tokenrank  - 马尔可夫链的稳态向量，该链衡量了全球令牌的重要性。我们证明，使用它可以改善无条件的图像生成。我们认为，我们的框架为现代视觉变压器的如何参与代币提供了新的视图。</li>
</ul>

<h3>Title: Generalized Dual Discriminator GANs</h3>
<ul>
<li><strong>Authors: </strong>Penukonda Naga Chandana, Tejas Srivastava, Gowtham R. Kurri, V. Lalitha</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17684">https://arxiv.org/abs/2507.17684</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17684">https://arxiv.org/pdf/2507.17684</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17684]] Generalized Dual Discriminator GANs(https://arxiv.org/abs/2507.17684)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Dual discriminator generative adversarial networks (D2 GANs) were introduced to mitigate the problem of mode collapse in generative adversarial networks. In D2 GANs, two discriminators are employed alongside a generator: one discriminator rewards high scores for samples from the true data distribution, while the other favors samples from the generator. In this work, we first introduce dual discriminator $\alpha$-GANs (D2 $\alpha$-GANs), which combines the strengths of dual discriminators with the flexibility of a tunable loss function, $\alpha$-loss. We further generalize this approach to arbitrary functions defined on positive reals, leading to a broader class of models we refer to as generalized dual discriminator generative adversarial networks. For each of these proposed models, we provide theoretical analysis and show that the associated min-max optimization reduces to the minimization of a linear combination of an $f$-divergence and a reverse $f$-divergence. This generalizes the known simplification for D2-GANs, where the objective reduces to a linear combination of the KL-divergence and the reverse KL-divergence. Finally, we perform experiments on 2D synthetic data and use multiple performance metrics to capture various advantages of our GANs.</li>
<li><strong>摘要：</strong>引入了双重判别生成对抗网络（D2 GAN），以减轻生成对抗网络中模式崩溃的问题。在D2 GAN中，与一个生成器一起使用了两个歧视器：一个歧视器从真实数据分布中奖励样本的高分，而另一个则有利于发电机的样本。在这项工作中，我们首先介绍双歧视者$ \ alpha $ -gans（d2 $ \ alpha $ -gans），它结合了双歧视器的优势和可调损失功能的灵活性，$ \ alpha $ loss。我们进一步将这种方法推广到在积极实时定义的任意功能，从而导致更广泛的模型，我们称为广义双歧视者生成对抗网络。对于这些提出的模型，我们提供了理论分析，并表明相关的Min-Max优化降低了$ f $ dyvivergence和反向$ f $ ddivergence的线性组合的最小化。这概括了D2-GAN的已知简化，其中物镜将KL-Divergence和反向KL-Divergence的线性组合减少。最后，我们对2D综合数据进行实验，并使用多个性能指标来捕获我们gan的各种优势。</li>
</ul>

<h3>Title: Towards Effective Open-set Graph Class-incremental Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiazhen Chen, Zheng Ma, Sichao Fu, Mingbin Feng, Tony S. Wirjanto, Weihua Ou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17687">https://arxiv.org/abs/2507.17687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17687">https://arxiv.org/pdf/2507.17687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17687]] Towards Effective Open-set Graph Class-incremental Learning(https://arxiv.org/abs/2507.17687)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Graph class-incremental learning (GCIL) allows graph neural networks (GNNs) to adapt to evolving graph analytical tasks by incrementally learning new class knowledge while retaining knowledge of old classes. Existing GCIL methods primarily focus on a closed-set assumption, where all test samples are presumed to belong to previously known classes. Such an assumption restricts their applicability in real-world scenarios, where unknown classes naturally emerge during inference, and are absent during training. In this paper, we explore a more challenging open-set graph class-incremental learning scenario with two intertwined challenges: catastrophic forgetting of old classes, which impairs the detection of unknown classes, and inadequate open-set recognition, which destabilizes the retention of learned knowledge. To address the above problems, a novel OGCIL framework is proposed, which utilizes pseudo-sample embedding generation to effectively mitigate catastrophic forgetting and enable robust detection of unknown classes. To be specific, a prototypical conditional variational autoencoder is designed to synthesize node embeddings for old classes, enabling knowledge replay without storing raw graph data. To handle unknown classes, we employ a mixing-based strategy to generate out-of-distribution (OOD) samples from pseudo in-distribution and current node embeddings. A novel prototypical hypersphere classification loss is further proposed, which anchors in-distribution embeddings to their respective class prototypes, while repelling OOD embeddings away. Instead of assigning all unknown samples into one cluster, our proposed objective function explicitly models them as outliers through prototype-aware rejection regions, ensuring a robust open-set recognition. Extensive experiments on five benchmarks demonstrate the effectiveness of OGCIL over existing GCIL and open-set GNN methods.</li>
<li><strong>摘要：</strong>Graph class incremental学习（GCIL）允许图形神经网络（GNN）通过逐步学习新的类知识，同时保留对旧类的知识，从而适应不断发展的图形分析任务。现有的GCIL方法主要集中于封闭式假设，其中所有测试样本都假定属于先前已知的类别。这样的假设限制了它们在实际情况下的适用性，在推理过程中自然出现了未知的类别，并且在培训期间不存在。在本文中，我们探讨了一个更具挑战性的开放式图形课程学习场景，并带有两个交织在一起的挑战：灾难性的忘记旧阶级，损害了未知类别的检测，开放式识别不足，这破坏了学习知识的保留。为了解决上述问题，提出了一个新颖的OGCIL框架，该框架利用伪样本嵌入生成来有效缓解灾难性的遗忘，并能够强大地检测未知类别。具体而言，典型的条件变分自动编码器旨在合成旧类的节点嵌入，从而无需存储原始图形数据就可以重播知识。为了处理未知类别，我们采用基于混合的策略来从伪分布和当前的节点嵌入中生成分布式（OOD）样本。进一步提出了一种新型的原型超晶体分类损失，该损失将分布嵌入的嵌入到各自的类原型上，同时排除OOD嵌入。我们提出的目标函数没有将所有未知样本分配给一个群集，而是通过原型感知的拒绝区域将它们显式地将其模拟为异常值，从而确保了强大的开放式识别。对五个基准测试的广泛实验证明了OGCIL对现有GCIL和开放集GNN方法的有效性。</li>
</ul>

<h3>Title: Flow Matching Meets Biology and Life Science: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Zihao Li, Zhichen Zeng, Xiao Lin, Feihao Fang, Yanru Qu, Zhe Xu, Zhining Liu, Xuying Ning, Tianxin Wei, Ge Liu, Hanghang Tong, Jingrui He</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17731">https://arxiv.org/abs/2507.17731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17731">https://arxiv.org/pdf/2507.17731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17731]] Flow Matching Meets Biology and Life Science: A Survey(https://arxiv.org/abs/2507.17731)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Over the past decade, advances in generative modeling, such as generative adversarial networks, masked autoencoders, and diffusion models, have significantly transformed biological research and discovery, enabling breakthroughs in molecule design, protein generation, drug discovery, and beyond. At the same time, biological applications have served as valuable testbeds for evaluating the capabilities of generative models. Recently, flow matching has emerged as a powerful and efficient alternative to diffusion-based generative modeling, with growing interest in its application to problems in biology and life sciences. This paper presents the first comprehensive survey of recent developments in flow matching and its applications in biological domains. We begin by systematically reviewing the foundations and variants of flow matching, and then categorize its applications into three major areas: biological sequence modeling, molecule generation and design, and peptide and protein generation. For each, we provide an in-depth review of recent progress. We also summarize commonly used datasets and software tools, and conclude with a discussion of potential future directions. The corresponding curated resources are available at this https URL.</li>
<li><strong>摘要：</strong>在过去的十年中，生成建模的进步，例如生成对抗网络，掩盖的自动编码器和扩散模型，都显着改变了生物学研究和发现，从而在分子设计，蛋白质产生，药物发现以及以下方面取得了突破。同时，生物应用已用作评估生成模型功能的有价值的测试台。最近，流动匹配已成为基于扩散的生成建模的强大而有效的替代品，对其在生物学和生命科学方面的应用越来越兴趣。本文介绍了对流匹配及其在生物领域中的应用的最新发展的首次全面调查。我们首先系统地检查流匹配的基础和变体，然后将其应用分为三个主要领域：生物序列建模，分子产生和设计以及肽和蛋白质的产生。对于每个人，我们对最近的进度进行了深入的评论。我们还总结了常用的数据集和软件工具，并以讨论潜在的未来方向进行了结论。相应的策划资源可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Yume: An Interactive World Generation Model</h3>
<ul>
<li><strong>Authors: </strong>Xiaofeng Mao, Shaoheng Lin, Zhen Li, Chuanhao Li, Wenshuo Peng, Tong He, Jiangmiao Pang, Mingmin Chi, Yu Qiao, Kaipeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17744">https://arxiv.org/abs/2507.17744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17744">https://arxiv.org/pdf/2507.17744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17744]] Yume: An Interactive World Generation Model(https://arxiv.org/abs/2507.17744)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Yume aims to use images, text, or videos to create an interactive, realistic, and dynamic world, which allows exploration and control using peripheral devices or neural signals. In this report, we present a preview version of \method, which creates a dynamic world from an input image and allows exploration of the world using keyboard actions. To achieve this high-fidelity and interactive video world generation, we introduce a well-designed framework, which consists of four main components, including camera motion quantization, video generation architecture, advanced sampler, and model acceleration. First, we quantize camera motions for stable training and user-friendly interaction using keyboard inputs. Then, we introduce the Masked Video Diffusion Transformer~(MVDT) with a memory module for infinite video generation in an autoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM) and Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE) are introduced to the sampler for better visual quality and more precise control. Moreover, we investigate model acceleration by synergistic optimization of adversarial distillation and caching mechanisms. We use the high-quality world exploration dataset \sekai to train \method, and it achieves remarkable results in diverse scenes and applications. All data, codebase, and model weights are available on this https URL. Yume will update monthly to achieve its original goal. Project page: this https URL.</li>
<li><strong>摘要：</strong>Yume的目的是使用图像，文本或视频来创建一个交互式，现实和动态的世界，该世界允许使用外围设备或神经信号进行探索和控制。在本报告中，我们提出了\ method的预览版本，该版本从输入图像创建动态世界，并允许使用键盘操作探索世界。为了实现这种高保真和交互式视频世界一代，我们引入了一个精心设计的框架，该框架由四个主要组件组成，包括摄像机运动量化，视频生成体系结构，高级采样器和模型加速度。首先，我们使用键盘输入来量化摄像机运动，以进行稳定的培训和用户友好的交互。然后，我们介绍了带有内存模块的蒙版视频扩散变压器〜（MVDT），以自动回归方式进行无限视频生成。之后，将无训练的反艺术机制（AAM）和基于随机微分方程（TTS-SDE）的时间旅行采样引入采样器，以获得更好的视觉质量和更精确的控制。此外，我们通过对对抗蒸馏和缓存机制的协同优化来研究模型加速度。我们使用高质量的世界勘探数据集\ sekai来训练\方法，并在各种场景和应用程序中取得了显着的结果。所有数据，代码库和模型权重都可以在此HTTPS URL上提供。 Yume将每月更新以实现其最初的目标。项目页面：此HTTPS URL。</li>
</ul>

<h3>Title: Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention</h3>
<ul>
<li><strong>Authors: </strong>Yiwen Chen, Zhihao Li, Yikai Wang, Hu Zhang, Qin Li, Chi Zhang, Guosheng Lin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.17745">https://arxiv.org/abs/2507.17745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.17745">https://arxiv.org/pdf/2507.17745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.17745]] Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention(https://arxiv.org/abs/2507.17745)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in sparse voxel representations have significantly improved the quality of 3D content generation, enabling high-resolution modeling with fine-grained geometry. However, existing frameworks suffer from severe computational inefficiencies due to the quadratic complexity of attention mechanisms in their two-stage diffusion pipelines. In this work, we propose Ultra3D, an efficient 3D generation framework that significantly accelerates sparse voxel modeling without compromising quality. Our method leverages the compact VecSet representation to efficiently generate a coarse object layout in the first stage, reducing token count and accelerating voxel coordinate prediction. To refine per-voxel latent features in the second stage, we introduce Part Attention, a geometry-aware localized attention mechanism that restricts attention computation within semantically consistent part regions. This design preserves structural continuity while avoiding unnecessary global attention, achieving up to 6.7x speed-up in latent generation. To support this mechanism, we construct a scalable part annotation pipeline that converts raw meshes into part-labeled sparse voxels. Extensive experiments demonstrate that Ultra3D supports high-resolution 3D generation at 1024 resolution and achieves state-of-the-art performance in both visual fidelity and user preference.</li>
<li><strong>摘要：</strong>稀疏体素表示的最新进展显着提高了3D含量产生的质量，从而实现了具有细粒几何形状的高分辨率建模。但是，由于其两阶段扩散管道中注意机制的二次复杂性，现有框架遭受了严重的计算效率低下。在这项工作中，我们提出了Ultra3D，这是一个有效的3D生成框架，可显着加速稀疏体素建模而不会损害质量。我们的方法利用紧凑型VECSET表示在第一阶段有效地生成粗对象布局，从而减少了令牌计数并加速体素坐标预测。为了完善第二阶段的人均潜在特征，我们引入了部分注意力，这是一种几何学意识到的局部注意机制，限制了语义上一致的部分区域内的注意力计算。该设计可保留结构连续性，同时避免了不必要的全球关注，在潜在一代中达到了6.7倍的速度。为了支持这种机制，我们构建了可扩展的零件注释管道，该管道将原始网格转换为部分标记的稀疏体素。广泛的实验表明，Ultra3D以1024分辨率支持高分辨率的3D生成，并在视觉保真度和用户偏好方面达到最先进的性能。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
