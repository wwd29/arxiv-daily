<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-18</h1>
<h3>Title: EdiVal-Agent: An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing</h3>
<ul>
<li><strong>Authors: </strong>Tianyu Chen, Yasi Zhang, Zhi Zhang, Peiyu Yu, Shu Wang, Zhendong Wang, Kevin Lin, Xiaofei Wang, Zhengyuan Yang, Linjie Li, Chung-Ching Lin, Jianwen Xie, Oscar Leong, Lijuan Wang, Ying Nian Wu, Mingyuan Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13399">https://arxiv.org/abs/2509.13399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13399">https://arxiv.org/pdf/2509.13399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13399]] EdiVal-Agent: An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing(https://arxiv.org/abs/2509.13399)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Instruction-based image editing has advanced rapidly, yet reliable and interpretable evaluation remains a bottleneck. Current protocols either (i) depend on paired reference images -- resulting in limited coverage and inheriting biases from prior generative models -- or (ii) rely solely on zero-shot vision--language models (VLMs), whose prompt-based assessments of instruction following, content consistency, and visual quality are often imprecise. To address this, we introduce EdiVal-Agent, an automated, scalable, and fine-grained evaluation framework for multi-turn instruction-based editing from an object-centric perspective, supported by a suite of expert tools. Given an image, EdiVal-Agent first decomposes it into semantically meaningful objects, then synthesizes diverse, context-aware editing instructions. For evaluation, it integrates VLMs with open-vocabulary object detectors to assess instruction following, uses semantic-level feature extractors to evaluate content consistency, and leverages human preference models to judge visual quality. We show that combining VLMs with object detectors yields stronger agreement with human judgments in instruction-following evaluation compared to using VLMs alone and CLIP-based metrics. Furthermore, the pipeline's modular design allows future tools to be seamlessly integrated, enhancing evaluation accuracy over time. Instantiating this pipeline, we build EdiVal-Bench, a multi-turn editing benchmark covering 9 instruction types and 11 state-of-the-art editing models spanning autoregressive (AR) (including Nano Banana, GPT-Image-1), flow-matching, and diffusion paradigms. We demonstrate that EdiVal-Agent can be used to identify existing failure modes, thereby informing the development of the next generation of editing models. Project page: this https URL.</li>
<li><strong>摘要：</strong>基于指导的图像编辑已迅速发展，但可靠且可解释的评估仍然是一种瓶颈。当前的协议（i）都取决于配对的参考图像 - 导致覆盖范围有限，并从先前的生成模型中继承了偏见，或者（ii）仅依赖于零拍的视觉模型 - 语言模型（VLMS），其及时基于基于内容的教学评估，内容一致性和视觉质量通常是不正确的。为了解决这个问题，我们介绍了Edival-Agent，这是一个自动化，可扩展且细粒度的评估框架，用于从对象以中心的角度进行基于多转指令的编辑，并由一系列专家工具支持。给定图像，Edival-Agent首先将其分解为语义上有意义的对象，然后综合了多样化的，上下文感知的编辑指令。为了进行评估，它将VLM与开放式对象检测器集成在一起，以评估以下教学，使用语义级特征提取器来评估内容一致性，并利用人类偏好模型来判断视觉质量。我们表明，与单独使用VLMS和基于夹子的指标相比，将VLM与对象检测器相结合在指导遵循评估中的判断具有更强的一致性。此外，管道的模块化设计允许将来的工具无缝集成，从而提高了评估准确性。我们建立了Edival-Bench，这是一个多转弯的基准测试，涵盖了9种说明类型和11种涵盖自动回旋（AR）（包括纳米香蕉，GPT-Image-1），流量匹配和扩散范式的最先进的编辑模型。我们证明可以使用Edival-Agent来识别现有的故障模式，从而告知下一代编辑模型的发展。项目页面：此HTTPS URL。</li>
</ul>

<h3>Title: BiasMap: Leveraging Cross-Attentions to Discover and Mitigate Hidden Social Biases in Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Rajatsubhra Chakraborty, Xujun Che, Depeng Xu, Cori Faklaris, Xi Niu, Shuhan Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13496">https://arxiv.org/abs/2509.13496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13496">https://arxiv.org/pdf/2509.13496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13496]] BiasMap: Leveraging Cross-Attentions to Discover and Mitigate Hidden Social Biases in Text-to-Image Generation(https://arxiv.org/abs/2509.13496)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Bias discovery is critical for black-box generative models, especiall text-to-image (TTI) models. Existing works predominantly focus on output-level demographic distributions, which do not neces- sarily guarantee concept representations to be disentangled post- mitigation. We propose BiasMap, a model-agnostic framework for uncovering latent concept-level representational biases in stable dif- fusion models. BiasMap leverages cross-attention attribution maps to reveal structural entanglements between demographics (e.g., gender, race) and semantics (e.g., professions), going deeper into representational bias during the image generation. Using attribu- tion maps of these concepts, we quantify the spatial demographics- semantics concept entanglement via Intersection over Union (IoU), offering a lens into bias that remains hidden in existing fairness dis- covery approaches. In addition, we further utilize BiasMap for bias mitigation through energy-guided diffusion sampling that directly modifies latent noise space and minimizes the expected SoftIoU dur- ing the denoising process. Our findings show that existing fairness interventions may reduce the output distributional gap but often fail to disentangle concept-level coupling, whereas our mitigation method can mitigate concept entanglement in image generation while complementing distributional bias mitigation.</li>
<li><strong>摘要：</strong>偏差发现对于黑盒生成模型，ESPECIALL文本对图像（TTI）模型至关重要。现有作品主要集中在输出级别的人口统计分布上，这些分布并不能确保概念表示被解散后的概念表示。我们提出了biasmap，这是一种模型不合时宜的框架，用于在稳定的差异模型中发现潜在的概念级代表性偏差。偏置图利用跨注意归因图揭示了人口统计学（例如性别，种族）和语义（例如职业）之间的结构性纠缠，在图像产生期间会更深入地陷入代表性偏见。使用这些概念的归因图，我们通过联合（IOU）的交集来量化空间人口统计学概念纠缠，从而使镜头陷入了偏见，这些镜头仍然隐藏在现有的公平性差异方法中。此外，我们通过能源引导的扩散采样进一步利用偏置来缓解偏置，从而直接修改潜在的噪声空间并最大程度地减少了脱氧过程的预期软化。我们的发现表明，现有的公平干预措施可能会降低输出分布差距，但通常无法解散概念级耦合，而我们的缓解方法可以减轻图像生成中的概念纠缠，同时补充分布分布偏置缓解。</li>
</ul>

<h3>Title: Adversarial Appearance Learning in Augmented Cityscapes for Pedestrian Recognition in Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Artem Savkin, Thomas Lapotre, Kevin Strauss, Uzair Akbar, Federico Tombari</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13507">https://arxiv.org/abs/2509.13507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13507">https://arxiv.org/pdf/2509.13507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13507]] Adversarial Appearance Learning in Augmented Cityscapes for Pedestrian Recognition in Autonomous Driving(https://arxiv.org/abs/2509.13507)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In the autonomous driving area synthetic data is crucial for cover specific traffic scenarios which autonomous vehicle must handle. This data commonly introduces domain gap between synthetic and real domains. In this paper we deploy data augmentation to generate custom traffic scenarios with VRUs in order to improve pedestrian recognition. We provide a pipeline for augmentation of the Cityscapes dataset with virtual pedestrians. In order to improve augmentation realism of the pipeline we reveal a novel generative network architecture for adversarial learning of the data-set lighting conditions. We also evaluate our approach on the tasks of semantic and instance segmentation.</li>
<li><strong>摘要：</strong>在自动驾驶区中，合成数据对于涵盖自动驾驶必须处理的特定交通情况至关重要。该数据通常引入合成和真实域之间的域间隙。在本文中，我们部署了数据增强，以通过VRU生成自定义的流量方案，以改善行人识别。我们为通过虚拟行人提供了增加城市景观数据集的管道。为了改善管道的增强现实主义，我们揭示了一种新颖的生成网络体系结构，用于对数据集照明条件的对抗性学习。我们还评估了关于语义和实例细分任务的方法。</li>
</ul>

<h3>Title: AERIS: Argonne Earth Systems Model for Reliable and Skillful Predictions</h3>
<ul>
<li><strong>Authors: </strong>Väinö Hatanpää, Eugene Ku, Jason Stock, Murali Emani, Sam Foreman, Chunyong Jung, Sandeep Madireddy, Tung Nguyen, Varuni Sastry, Ray A. O. Sinurat, Sam Wheeler, Huihuo Zheng, Troy Arcomano, Venkatram Vishwanath, Rao Kotamarthi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13523">https://arxiv.org/abs/2509.13523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13523">https://arxiv.org/pdf/2509.13523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13523]] AERIS: Argonne Earth Systems Model for Reliable and Skillful Predictions(https://arxiv.org/abs/2509.13523)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative machine learning offers new opportunities to better understand complex Earth system dynamics. Recent diffusion-based methods address spectral biases and improve ensemble calibration in weather forecasting compared to deterministic methods, yet have so far proven difficult to scale stably at high resolutions. We introduce AERIS, a 1.3 to 80B parameter pixel-level Swin diffusion transformer to address this gap, and SWiPe, a generalizable technique that composes window parallelism with sequence and pipeline parallelism to shard window-based transformers without added communication cost or increased global batch size. On Aurora (10,080 nodes), AERIS sustains 10.21 ExaFLOPS (mixed precision) and a peak performance of 11.21 ExaFLOPS with $1 \times 1$ patch size on the 0.25° ERA5 dataset, achieving 95.5% weak scaling efficiency, and 81.6% strong scaling efficiency. AERIS outperforms the IFS ENS and remains stable on seasonal scales to 90 days, highlighting the potential of billion-parameter diffusion models for weather and climate prediction.</li>
<li><strong>摘要：</strong>生成机器学习提供了新的机会，可以更好地了解复杂的地球系统动态。与确定性方法相比，最近基于扩散的方法解决了频谱偏差并改善天气预测的集合校准，但迄今为止，在高分辨率上很难稳定地扩展。我们介绍了AERIS，一种1.3至80b参数像素级的Swin扩散变压器来解决此差距，Swipe是一种可推广的技术，该技术与序列和管道并行构成了窗口并行性，而无需增加通信成本或增加了全局批次大小。在Aurora（10,080个节点）上，AERIS持续10.21 Exaflops（混合精度）和11.21 Exaflops的峰值性能为$ 1 \ times 1 $ 1 $ patch ERA5数据集的贴剂大小，可实现95.5％的弱缩放效率和81％的尺度缩放效率的95.5％。 Aeris的表现优于IFS ENS，并且在季节尺度上保持稳定至90天，强调了数十亿参数扩散模型在天气和气候预测中的潜力。</li>
</ul>

<h3>Title: ColonCrafter: A Depth Estimation Model for Colonoscopy Videos Using Diffusion Priors</h3>
<ul>
<li><strong>Authors: </strong>Romain Hardy, Tyler Berzin, Pranav Rajpurkar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13525">https://arxiv.org/abs/2509.13525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13525">https://arxiv.org/pdf/2509.13525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13525]] ColonCrafter: A Depth Estimation Model for Colonoscopy Videos Using Diffusion Priors(https://arxiv.org/abs/2509.13525)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Three-dimensional (3D) scene understanding in colonoscopy presents significant challenges that necessitate automated methods for accurate depth estimation. However, existing depth estimation models for endoscopy struggle with temporal consistency across video sequences, limiting their applicability for 3D reconstruction. We present ColonCrafter, a diffusion-based depth estimation model that generates temporally consistent depth maps from monocular colonoscopy videos. Our approach learns robust geometric priors from synthetic colonoscopy sequences to generate temporally consistent depth maps. We also introduce a style transfer technique that preserves geometric structure while adapting real clinical videos to match our synthetic training domain. ColonCrafter achieves state-of-the-art zero-shot performance on the C3VD dataset, outperforming both general-purpose and endoscopy-specific approaches. Although full trajectory 3D reconstruction remains a challenge, we demonstrate clinically relevant applications of ColonCrafter, including 3D point cloud generation and surface coverage assessment.</li>
<li><strong>摘要：</strong>结肠镜检查中的三维（3D）场景理解提出了重大挑战，这些挑战需要自动化方法才能进行准确的深度估计。但是，内窥镜检查的现有深度估计模型与视频序列之间的时间一致性斗争，从而限制了其适用于3D重建。我们提出了结肠癌，这是一个基于扩散的深度估计模型，该模型从单眼结肠镜检查视频中生成时间一致的深度图。我们的方法从合成结肠镜序列中学习了强大的几何先验，以产生时间一致的深度图。我们还引入了一种样式转移技术，该技术可以保留几何结构，同时调整真实的临床视频以匹配我们的合成训练领域。 Coloncrafter在C3VD数据集上实现了最新的零拍性能，表现优于通用和内窥镜特定方法。尽管全部轨迹3D重建仍然是一个挑战，但我们证明了结肠法的临床相关应用，包括3D点云的产生和表面覆盖范围评估。</li>
</ul>

<h3>Title: Intelligent Healthcare Imaging Platform An VLM-Based Framework for Automated Medical Image Analysis and Clinical Report Generation</h3>
<ul>
<li><strong>Authors: </strong>Samer Al-Hamadani</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13590">https://arxiv.org/abs/2509.13590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13590">https://arxiv.org/pdf/2509.13590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13590]] Intelligent Healthcare Imaging Platform An VLM-Based Framework for Automated Medical Image Analysis and Clinical Report Generation(https://arxiv.org/abs/2509.13590)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The rapid advancement of artificial intelligence (AI) in healthcare imaging has revolutionized diagnostic medicine and clinical decision-making processes. This work presents an intelligent multimodal framework for medical image analysis that leverages Vision-Language Models (VLMs) in healthcare diagnostics. The framework integrates Google Gemini 2.5 Flash for automated tumor detection and clinical report generation across multiple imaging modalities including CT, MRI, X-ray, and Ultrasound. The system combines visual feature extraction with natural language processing to enable contextual image interpretation, incorporating coordinate verification mechanisms and probabilistic Gaussian modeling for anomaly distribution. Multi-layered visualization techniques generate detailed medical illustrations, overlay comparisons, and statistical representations to enhance clinical confidence, with location measurement achieving 80 pixels average deviation. Result processing utilizes precise prompt engineering and textual analysis to extract structured clinical information while maintaining interpretability. Experimental evaluations demonstrated high performance in anomaly detection across multiple modalities. The system features a user-friendly Gradio interface for clinical workflow integration and demonstrates zero-shot learning capabilities to reduce dependence on large datasets. This framework represents a significant advancement in automated diagnostic support and radiological workflow efficiency, though clinical validation and multi-center evaluation are necessary prior to widespread adoption.</li>
<li><strong>摘要：</strong>人工智能（AI）在医疗保健成像中的快速发展已彻底改变了诊断医学和临床决策过程。这项工作为医学图像分析提供了一个智能的多模式框架，该框架在医疗保健诊断中利用视觉模型（VLM）。该框架集成了Google Gemini 2.5 Flash，用于自动化肿瘤检测和临床报告生成，包括CT，MRI，X射线和超声。该系统将视觉特征提取与自然语言处理相结合，以实现上下文图像解释，结合了坐标验证机制和用于异常分布的概率高斯建模。多层可视化技术产生详细的医学插图，覆盖比较和统计表示，以增强临床信心，位置测量实现80像素的平均偏差。结果处理利用精确的及时工程和文本分析来提取结构化的临床信息，同时保持解释性。实验评估表明，跨多种模式的异常检测表现高。该系统具有用于临床工作流程集成的用户友好的Gradio接口，并演示了零拍的学习能力，以减少对大数据集的依赖。该框架代表了自动诊断支持和放射学工作流程效率的重大进步，尽管在广泛采用之前是必要的临床验证和多中心评估。</li>
</ul>

<h3>Title: Privacy-Aware In-Context Learning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Bishnu Bhusal, Manoj Acharya, Ramneet Kaur, Colin Samplawski, Anirban Roy, Adam D. Cobb, Rohit Chadha, Susmit Jha</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13625">https://arxiv.org/abs/2509.13625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13625">https://arxiv.org/pdf/2509.13625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13625]] Privacy-Aware In-Context Learning for Large Language Models(https://arxiv.org/abs/2509.13625)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have significantly transformed natural language understanding and generation, but they raise privacy concerns due to potential exposure of sensitive information. Studies have highlighted the risk of information leakage, where adversaries can extract sensitive information embedded in the prompts. In this work, we introduce a novel private prediction framework for generating high-quality synthetic text with strong privacy guarantees. Our approach leverages the Differential Privacy (DP) framework to ensure worst-case theoretical bounds on information leakage without requiring any fine-tuning of the underlying this http URL proposed method performs inference on private records and aggregates the resulting per-token output distributions. This enables the generation of longer and coherent synthetic text while maintaining privacy guarantees. Additionally, we propose a simple blending operation that combines private and public inference to further enhance utility. Empirical evaluations demonstrate that our approach outperforms previous state-of-the-art methods on in-context-learning (ICL) tasks, making it a promising direction for privacy-preserving text generation while maintaining high utility.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）显着改变了自然语言的理解和产生，但是由于潜在的敏感信息暴露，它们引起了隐私问题。研究强调了信息泄漏的风险，在这种情况下，对手可以提取提示中嵌入的敏感信息。在这项工作中，我们介绍了一个新颖的私人预测框架，用于生成具有强大隐私保证的高质量合成文本。我们的方法利用差异隐私（DP）框架来确保信息泄漏的最坏情况界限，而无需对基础本HTTP URL进行任何微调，对私有记录进行推断，并汇总了由此产生的每toke输出分布。这使得在维持隐私保证的同时，可以产生更长和一致的合成文本。此外，我们提出了一个简单的混合操作，将私人和公众推断结合在一起，以进一步增强效用。经验评估表明，我们的方法优于先前的关于文化学习（ICL）任务的先前最新方法，这使其成为保密性文本生成的有希望的方向，同时保持高实用性。</li>
</ul>

<h3>Title: Secure UAV-assisted Federated Learning: A Digital Twin-Driven Approach with Zero-Knowledge Proofs</h3>
<ul>
<li><strong>Authors: </strong>Md Bokhtiar Al Zami, Md Raihan Uddin, Dinh C. Nguyen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13634">https://arxiv.org/abs/2509.13634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13634">https://arxiv.org/pdf/2509.13634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13634]] Secure UAV-assisted Federated Learning: A Digital Twin-Driven Approach with Zero-Knowledge Proofs(https://arxiv.org/abs/2509.13634)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) has gained popularity as a privacy-preserving method of training machine learning models on decentralized networks. However to ensure reliable operation of UAV-assisted FL systems, issues like as excessive energy consumption, communication inefficiencies, and security vulnerabilities must be solved. This paper proposes an innovative framework that integrates Digital Twin (DT) technology and Zero-Knowledge Federated Learning (zkFed) to tackle these challenges. UAVs act as mobile base stations, allowing scattered devices to train FL models locally and upload model updates for aggregation. By incorporating DT technology, our approach enables real-time system monitoring and predictive maintenance, improving UAV network efficiency. Additionally, Zero-Knowledge Proofs (ZKPs) strengthen security by allowing model verification without exposing sensitive data. To optimize energy efficiency and resource management, we introduce a dynamic allocation strategy that adjusts UAV flight paths, transmission power, and processing rates based on network conditions. Using block coordinate descent and convex optimization techniques, our method significantly reduces system energy consumption by up to 29.6% compared to conventional FL approaches. Simulation results demonstrate improved learning performance, security, and scalability, positioning this framework as a promising solution for next-generation UAV-based intelligent networks.</li>
<li><strong>摘要：</strong>联邦学习（FL）已成为一种在分散网络上培训机器学习模型的隐私方法的知名度。但是，为了确保无人机辅助系统的可靠操作，必须解决诸如能源消耗过多，沟通效率低下和安全漏洞之类的问题。本文提出了一个创新的框架，该框架将数字双胞胎（DT）技术和零知识联合学习（ZKFED）集成在一起，以应对这些挑战。无人机充当移动基站，允许散落的设备在本地训练FL模型，并在集合上上传模型更新。通过合并DT技术，我们的方法可以实现实时系统监控和预测性维护，从而提高了无人机网络效率。此外，零知识证明（ZKP）通过允许模型验证而无需暴露敏感数据来增强安全性。为了优化能源效率和资源管理，我们引入了一种动态分配策略，该策略根据网络条件调整无人机飞行路径，传输功率和处理速率。与常规FL方法相比，使用块坐标下降和凸优化技术，我们的方法可显着降低系统能耗多达29.6％。仿真结果表明，学习绩效，安全性和可扩展性的提高，将此框架定位为基于下一代无人机的智能网络的有前途的解决方案。</li>
</ul>

<h3>Title: LLM-I: LLMs are Naturally Interleaved Multimodal Creators</h3>
<ul>
<li><strong>Authors: </strong>Zirun Guo, Feng Zhang, Kai Jia, Tao Jin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13642">https://arxiv.org/abs/2509.13642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13642">https://arxiv.org/pdf/2509.13642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13642]] LLM-I: LLMs are Naturally Interleaved Multimodal Creators(https://arxiv.org/abs/2509.13642)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We propose LLM-Interleaved (LLM-I), a flexible and dynamic framework that reframes interleaved image-text generation as a tool-use problem. LLM-I is designed to overcome the "one-tool" bottleneck of current unified models, which are limited to synthetic imagery and struggle with tasks requiring factual grounding or programmatic precision. Our framework empowers a central LLM or MLLM agent to intelligently orchestrate a diverse toolkit of specialized visual tools, including online image search, diffusion-based generation, code execution, and image editing. The agent is trained to select and apply these tools proficiently via a Reinforcement Learning (RL) framework that features a hybrid reward system combining rule-based logic with judgments from LLM and MLLM evaluators. Trained on a diverse new dataset using four different model backbones, LLM-I demonstrates state-of-the-art performance, outperforming existing methods by a large margin across four benchmarks. We also introduce a novel test-time scaling strategy that provides further performance gains. Project Page: this https URL.</li>
<li><strong>摘要：</strong>我们提出了一个灵活而动态的框架，它提出了llm-interleaved（llm-i），它将交织的图像文本生成重新构造为工具使用问题。 LLM-I旨在克服当前统一模型的“单芯”瓶颈，这些模型仅限于合成图像和与需要事实接地或程序化精度的任务斗争。我们的框架授权中央LLM或MLLM代理商智能协调一个专门的视觉工具的各种工具包，包括在线图像搜索，基于扩散的生成，代码执行和图像编辑。对代理商进行了培训，可以通过增强学习（RL）框架精确地选择这些工具，该框架具有混合奖励系统，结合了基于规则的逻辑与LLM和MLLM评估人员的判断。 LLM-I使用四个不同的型号在多种新数据集上进行了培训，它展示了最先进的性能，超过了四个基准的较大利润率的现有方法。我们还引入了一种新颖的测试时间缩放策略，可提供进一步的性能。项目页面：此HTTPS URL。</li>
</ul>

<h3>Title: Sequential Data Augmentation for Generative Recommendation</h3>
<ul>
<li><strong>Authors: </strong>Geon Lee, Bhuvesh Kumar, Clark Mingxuan Ju, Tong Zhao, Kijung Shin, Neil Shah, Liam Collins</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13648">https://arxiv.org/abs/2509.13648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13648">https://arxiv.org/pdf/2509.13648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13648]] Sequential Data Augmentation for Generative Recommendation(https://arxiv.org/abs/2509.13648)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative recommendation plays a crucial role in personalized systems, predicting users' future interactions from their historical behavior sequences. A critical yet underexplored factor in training these models is data augmentation, the process of constructing training data from user interaction histories. By shaping the training distribution, data augmentation directly and often substantially affects model generalization and performance. Nevertheless, in much of the existing work, this process is simplified, applied inconsistently, or treated as a minor design choice, without a systematic and principled understanding of its effects. Motivated by our empirical finding that different augmentation strategies can yield large performance disparities, we conduct an in-depth analysis of how they reshape training distributions and influence alignment with future targets and generalization to unseen inputs. To systematize this design space, we propose GenPAS, a generalized and principled framework that models augmentation as a stochastic sampling process over input-target pairs with three bias-controlled steps: sequence sampling, target sampling, and input sampling. This formulation unifies widely used strategies as special cases and enables flexible control of the resulting training distribution. Our extensive experiments on benchmark and industrial datasets demonstrate that GenPAS yields superior accuracy, data efficiency, and parameter efficiency compared to existing strategies, providing practical guidance for principled training data construction in generative recommendation.</li>
<li><strong>摘要：</strong>生成建议在个性化系统中起着至关重要的作用，从而预测了用户从其历史行为序列中的未来互动。在训练这些模型中，关键但毫无疑问的因素是数据增强，这是从用户交互历史中构建培训数据的过程。通过塑造训练分布，直接数据增强，通常会实质上影响模型的概括和性能。然而，在现有的许多工作中，此过程被简化，不一致或视为次要的设计选择，而没有对其效果的系统性和原则上的理解。通过我们的经验发现，不同的增强策略可以产生巨大的绩效差异，我们对它们如何重塑训练分布并影响未来目标的一致性以及对未见投入的概括进行了深入分析。为了系统化这个设计空间，我们提出了GenPA，这是一种广义和原则性的框架，将扩大作为随机抽样过程而不是输入目标对，并具有三个偏置控制的步骤：序列采样，目标采样和输入采样。该公式将广泛使用的策略统一为特殊情况，并可以灵活控制所得训练分布。我们在基准和工业数据集上进行的广泛实验表明，与现有策略相比，GENPA具有较高的准确性，数据效率和参数效率，从而为生成建议提供了实用的培训数据构建的实用指南。</li>
</ul>

<h3>Title: Deep Lookup Network</h3>
<ul>
<li><strong>Authors: </strong>Yulan Guo, Longguang Wang, Wendong Mao, Xiaoyu Dong, Yingqian Wang, Li Liu, Wei An</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13662">https://arxiv.org/abs/2509.13662</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13662">https://arxiv.org/pdf/2509.13662</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13662]] Deep Lookup Network(https://arxiv.org/abs/2509.13662)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Convolutional neural networks are constructed with massive operations with different types and are highly computationally intensive. Among these operations, multiplication operation is higher in computational complexity and usually requires {more} energy consumption with longer inference time than other operations, which hinders the deployment of convolutional neural networks on mobile devices. In many resource-limited edge devices, complicated operations can be calculated via lookup tables to reduce computational cost. Motivated by this, in this paper, we introduce a generic and efficient lookup operation which can be used as a basic operation for the construction of neural networks. Instead of calculating the multiplication of weights and activation values, simple yet efficient lookup operations are adopted to compute their responses. To enable end-to-end optimization of the lookup operation, we construct the lookup tables in a differentiable manner and propose several training strategies to promote their convergence. By replacing computationally expensive multiplication operations with our lookup operations, we develop lookup networks for the image classification, image super-resolution, and point cloud classification tasks. It is demonstrated that our lookup networks can benefit from the lookup operations to achieve higher efficiency in terms of energy consumption and inference speed while maintaining competitive performance to vanilla convolutional networks. Extensive experiments show that our lookup networks produce state-of-the-art performance on different tasks (both classification and regression tasks) and different data types (both images and point clouds).</li>
<li><strong>摘要：</strong>卷积神经网络由具有不同类型的大规模操作构建，并且在计算上是高度强度的。在这些操作中，计算复杂性的乘法操作比其他操作更高，通常需要{}更多的能量消耗，而推理时间更长，这阻碍了移动设备上的卷积神经网络的部署。在许多资源有限的边缘设备中，可以通过查找表计算复杂的操作，以降低计算成本。在本文中，我们引入了一个通用，有效的查找操作，该操作可以用作建造神经网络的基本操作。采用简单而有效的查找操作来计算其响应，而不是计算权重和激活值的乘法。为了实现查找操作的端到端优化，我们以可区分的方式构建查找表，并提出了几种培训策略来促进其收敛性。通过使用我们的查找操作替换计算昂贵的乘法操作，我们开发了用于图像分类，图像超分辨率和点云分类任务的查找网络。已经证明，我们的查找网络可以从查找操作中受益，以在能源消耗和推理速度方面提高效率，同时保持对香草卷积网络的竞争性能。广泛的实验表明，我们的查找网络在不同的任务（分类和回归任务）和不同的数据类型（图像和点云）上产生最先进的性能。</li>
</ul>

<h3>Title: StyleProtect: Safeguarding Artistic Identity in Fine-tuned Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Qiuyu Tang, Joshua Krinsky, Aparna Bharati</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13711">https://arxiv.org/abs/2509.13711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13711">https://arxiv.org/pdf/2509.13711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13711]] StyleProtect: Safeguarding Artistic Identity in Fine-tuned Diffusion Models(https://arxiv.org/abs/2509.13711)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of generative models, particularly diffusion-based approaches, has inadvertently facilitated their potential for misuse. Such models enable malicious exploiters to replicate artistic styles that capture an artist's creative labor, personal vision, and years of dedication in an inexpensive manner. This has led to a rise in the need and exploration of methods for protecting artworks against style mimicry. Although generic diffusion models can easily mimic an artistic style, finetuning amplifies this capability, enabling the model to internalize and reproduce the style with higher fidelity and control. We hypothesize that certain cross-attention layers exhibit heightened sensitivity to artistic styles. Sensitivity is measured through activation strengths of attention layers in response to style and content representations, and assessing their correlations with features extracted from external models. Based on our findings, we introduce an efficient and lightweight protection strategy, StyleProtect, that achieves effective style defense against fine-tuned diffusion models by updating only selected cross-attention layers. Our experiments utilize a carefully curated artwork dataset based on WikiArt, comprising representative works from 30 artists known for their distinctive and influential styles and cartoon animations from the Anita dataset. The proposed method demonstrates promising performance in safeguarding unique styles of artworks and anime from malicious diffusion customization, while maintaining competitive imperceptibility.</li>
<li><strong>摘要：</strong>生成模型的快速发展，尤其是基于扩散的方法，无意中促进了它们的滥用潜力。这样的模型使恶意剥削者能够复制艺术风格，这些风格以廉价的方式捕捉艺术家的创造力，个人视野和多年的奉献精神。这导致了对保护艺术品免受风格模仿的方法的需求和探索。尽管通用扩散模型可以轻松模仿艺术风格，但Finetuning放大了此功能，从而使模型能够以更高的忠诚度和控制力进行内部化和重现样式。我们假设某些跨注意层对艺术风格表现出更高的敏感性。通过响应样式和内容表示的注意力层的激活强度来衡量灵敏度，并评估其与从外部模型中提取的特征相关性。根据我们的发现，我们引入了一种高效且轻巧的保护策略，即StyleProtect，该策略通过仅更新选定的跨注意层来实现针对微调扩散模型的有效风格防御。我们的实验利用了基于Wikiart精心策划的艺术品数据集，其中包括来自30位艺术家的代表作品，以其独特而有影响力的风格和Anita数据集的动画动画而闻名。所提出的方法表明，在维护独特的艺术品风格和动漫范围内的表现方面有希望的表现，并保持了恶意扩散的定制，同时保持了竞争性的不可识别。</li>
</ul>

<h3>Title: UM-Depth : Uncertainty Masked Self-Supervised Monocular Depth Estimation with Visual Odometry</h3>
<ul>
<li><strong>Authors: </strong>Tae-Wook Um, Ki-Hyeon Kim, Hyun-Duck Choi, Hyo-Sung Ahn</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13713">https://arxiv.org/abs/2509.13713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13713">https://arxiv.org/pdf/2509.13713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13713]] UM-Depth : Uncertainty Masked Self-Supervised Monocular Depth Estimation with Visual Odometry(https://arxiv.org/abs/2509.13713)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Monocular depth estimation has been increasingly adopted in robotics and autonomous driving for its ability to infer scene geometry from a single camera. In self-supervised monocular depth estimation frameworks, the network jointly generates and exploits depth and pose estimates during training, thereby eliminating the need for depth labels. However, these methods remain challenged by uncertainty in the input data, such as low-texture or dynamic regions, which can cause reduced depth accuracy. To address this, we introduce UM-Depth, a framework that combines motion- and uncertainty-aware refinement to enhance depth accuracy at dynamic object boundaries and in textureless regions. Specifically, we develop a teacherstudent training strategy that embeds uncertainty estimation into both the training pipeline and network architecture, thereby strengthening supervision where photometric signals are weak. Unlike prior motion-aware approaches that incur inference-time overhead and rely on additional labels or auxiliary networks for real-time generation, our method uses optical flow exclusively within the teacher network during training, which eliminating extra labeling demands and any runtime cost. Extensive experiments on the KITTI and Cityscapes datasets demonstrate the effectiveness of our uncertainty-aware refinement. Overall, UM-Depth achieves state-of-the-art results in both self-supervised depth and pose estimation on the KITTI datasets.</li>
<li><strong>摘要：</strong>机器人技术和自动驾驶中越来越多地采用了单眼深度估计，从而可以从单个相机中推断场景几何形状。在自我监督的单眼深度估计框架中，该网络在训练过程中共同生成和利用深度和姿势估计，从而消除了对深度标签的需求。但是，这些方法仍然受到输入数据的不确定性（例如低文本或动态区域）的挑战，这可能会导致深度精度降低。为了解决这个问题，我们介绍了UM-Depth，该框架结合了运动和不确定性感知的精炼，以提高动态对象边界和无纹理区域的深度精度。具体而言，我们制定了一种教师培训策略，该培训策略将不确定性估计嵌入到训练管道和网络架构中，从而加强了光度信号较弱的监督。与先前的运动感知方法不同，这些方法会产生推理时间开销，并依靠其他标签或辅助网络进行实时生成，我们的方法在培训过程中仅在教师网络中使用光流，从而消除了额外的标签需求和任何运行时的成本。对Kitti和CityScapes数据集进行的广泛实验证明了我们不确定性感知的精炼的有效性。总体而言，UM-Depth可以实现最新的最先进，从而在Kitti数据集上进行了自我监督的深度和姿势估计。</li>
</ul>

<h3>Title: Iterative Prompt Refinement for Safer Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Jinwoo Jeon, JunHyeok Oh, Hayeong Lee, Byung-Jun Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13760">https://arxiv.org/abs/2509.13760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13760">https://arxiv.org/pdf/2509.13760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13760]] Iterative Prompt Refinement for Safer Text-to-Image Generation(https://arxiv.org/abs/2509.13760)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-Image (T2I) models have made remarkable progress in generating images from text prompts, but their output quality and safety still depend heavily on how prompts are phrased. Existing safety methods typically refine prompts using large language models (LLMs), but they overlook the images produced, which can result in unsafe outputs or unnecessary changes to already safe prompts. To address this, we propose an iterative prompt refinement algorithm that uses Vision Language Models (VLMs) to analyze both the input prompts and the generated images. By leveraging visual feedback, our method refines prompts more effectively, improving safety while maintaining user intent and reliability comparable to existing LLM-based approaches. Additionally, we introduce a new dataset labeled with both textual and visual safety signals using off-the-shelf multi-modal LLM, enabling supervised fine-tuning. Experimental results demonstrate that our approach produces safer outputs without compromising alignment with user intent, offering a practical solution for generating safer T2I content. Our code is available at this https URL. \textbf{\textcolor{red}WARNING: This paper contains examples of harmful or inappropriate images generated by models.</li>
<li><strong>摘要：</strong>文本对图像（T2I）模型在从文本提示中生成图像方面取得了显着进度，但是它们的输出质量和安全性仍然在很大程度上取决于提示的措辞。现有的安全方法通常使用大语言模型（LLM）来完善提示，但它们忽略了所产生的图像，这可能会导致不安全的输出或不必要的更改，以使其已安全提示。为了解决这个问题，我们提出了一种使用视觉语言模型（VLM）分析输入提示和生成的图像的迭代提示算法。通过利用视觉反馈，我们的方法可以更有效地提高提示，从而提高安全性，同时保持用户意图和可靠性与现有基于LLM的方法相当。此外，我们推出了一个新的数据集，该数据集使用现成的多模式LLM标记为文本和视觉安全信号，从而实现了监督的微调。实验结果表明，我们的方法会产生更安全的输出，而不会损害用户意图的对齐，从而提供了一种实用的解决方案，以生成更安全的T2I含量。我们的代码可在此HTTPS URL上找到。 \ textbf {\ textColor {red}警告：本文包含模型生成的有害或不适当图像的示例。</li>
</ul>

<h3>Title: Generative Image Coding with Diffusion Prior</h3>
<ul>
<li><strong>Authors: </strong>Jianhui Chang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13768">https://arxiv.org/abs/2509.13768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13768">https://arxiv.org/pdf/2509.13768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13768]] Generative Image Coding with Diffusion Prior(https://arxiv.org/abs/2509.13768)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>As generative technologies advance, visual content has evolved into a complex mix of natural and AI-generated images, driving the need for more efficient coding techniques that prioritize perceptual quality. Traditional codecs and learned methods struggle to maintain subjective quality at high compression ratios, while existing generative approaches face challenges in visual fidelity and generalization. To this end, we propose a novel generative coding framework leveraging diffusion priors to enhance compression performance at low bitrates. Our approach employs a pre-optimized encoder to generate generalized compressed-domain representations, integrated with the pretrained model's internal features via a lightweight adapter and an attentive fusion module. This framework effectively leverages existing pretrained diffusion models and enables efficient adaptation to different pretrained models for new requirements with minimal retraining costs. We also introduce a distribution renormalization method to further enhance reconstruction fidelity. Extensive experiments show that our method (1) outperforms existing methods in visual fidelity across low bitrates, (2) improves compression performance by up to 79% over H.266/VVC, and (3) offers an efficient solution for AI-generated content while being adaptable to broader content types.</li>
<li><strong>摘要：</strong>随着生成技术的推进，视觉内容已演变为天然和AI生成的图像的复杂组合，推动了对优先级质量质量的更有效的编码技术的需求。传统的编解码器和学识渊博的方法努力以高压比保持主观质量，而现有的生成方法则面临着视觉保真度和概括的挑战。为此，我们提出了一个新颖的生成编码框架，利用扩散先验来提高低比特率的压缩性能。我们的方法采用预先优化的编码器来生成通用的压缩域表示，并通过轻量级适配器和细心的融合模块与预审计的模型的内部特征集成在一起。该框架有效地利用了现有的经过预定的扩散模型，并可以有效适应不同的经过审慎的模型，以减少重新培训成本。我们还引入了一种分布重新规定方法，以进一步增强重建保真度。广泛的实验表明，我们的方法（1）优于低比特率的视觉保真度中的现有方法，（2）将压缩性能提高到H.266/vvc高达79％，并且（3）为AI生成的内容提供了有效的解决方案，同时可适应更广泛的内容类型。</li>
</ul>

<h3>Title: BWCache: Accelerating Video Diffusion Transformers through Block-Wise Caching</h3>
<ul>
<li><strong>Authors: </strong>Hanshuai Cui, Zhiqing Tang, Zhifei Xu, Zhi Yao, Wenyi Zeng, Weijia Jia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13789">https://arxiv.org/abs/2509.13789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13789">https://arxiv.org/pdf/2509.13789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13789]] BWCache: Accelerating Video Diffusion Transformers through Block-Wise Caching(https://arxiv.org/abs/2509.13789)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advancements in Diffusion Transformers (DiTs) have established them as the state-of-the-art method for video generation. However, their inherently sequential denoising process results in inevitable latency, limiting real-world applicability. Existing acceleration methods either compromise visual quality due to architectural modifications or fail to reuse intermediate features at proper granularity. Our analysis reveals that DiT blocks are the primary contributors to inference latency. Across diffusion timesteps, the feature variations of DiT blocks exhibit a U-shaped pattern with high similarity during intermediate timesteps, which suggests substantial computational redundancy. In this paper, we propose Block-Wise Caching (BWCache), a training-free method to accelerate DiT-based video generation. BWCache dynamically caches and reuses features from DiT blocks across diffusion timesteps. Furthermore, we introduce a similarity indicator that triggers feature reuse only when the differences between block features at adjacent timesteps fall below a threshold, thereby minimizing redundant computations while maintaining visual fidelity. Extensive experiments on several video diffusion models demonstrate that BWCache achieves up to 2.24$\times$ speedup with comparable visual quality.</li>
<li><strong>摘要：</strong>扩散变压器（DIT）的最新进步已将它们确立为视频生成的最新方法。但是，它们固有的顺序降解过程导致不可避免的延迟，从而限制了现实世界的适用性。现有的加速方法要么由于建筑修饰而损害视觉质量，要么无法在适当的粒度上重用中间特征。我们的分析表明，DIT块是推理潜伏期的主要因素。在扩散时间段中，DIT块的特征变化在中间时间段中具有高相似性的U形模式，这表明了实质性的计算冗余。在本文中，我们提出了宽阔的缓存（BWCACHE），这是一种无训练的方法，以加速基于DIT的视频生成。 BWCACHE在扩散时间步中动态缓存和重用功能。此外，我们引入了一个相似性指标，即仅在相邻时间段处的块特征之间的差异低于阈值时，才能将触发器功能重复使用，从而最大程度地减少冗余计算，同时保持视觉保真度。在几个视频扩散模型上进行的广泛实验表明，BWCACHE可达到2.24美元$ \ times $ speedup，并具有可比的视觉质量。</li>
</ul>

<h3>Title: White Aggregation and Restoration for Few-shot 3D Point Cloud Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jiyun Im, SuBeen Lee, Miso Lee, Jae-Pil Heo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13907">https://arxiv.org/abs/2509.13907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13907">https://arxiv.org/pdf/2509.13907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13907]] White Aggregation and Restoration for Few-shot 3D Point Cloud Semantic Segmentation(https://arxiv.org/abs/2509.13907)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generation</a></li>
<li><strong>Abstract: </strong>Few-Shot 3D Point Cloud Segmentation (FS-PCS) aims to predict per-point labels for an unlabeled point cloud, given only a few labeled examples. To extract discriminative representations from the limited support set, existing methods have constructed prototypes using conventional algorithms such as farthest point sampling. However, we point out that its initial randomness significantly affects FS-PCS performance and that the prototype generation process remains underexplored despite its prevalence. This motivates us to investigate an advanced prototype generation method based on attention mechanism. Despite its potential, we found that vanilla module suffers from the distributional gap between learnable prototypical tokens and support features. To overcome this, we propose White Aggregation and Restoration Module (WARM), which resolves the misalignment by sandwiching cross-attention between whitening and coloring transformations. Specifically, whitening aligns the support features to prototypical tokens before attention process, and subsequently coloring restores the original distribution to the attended tokens. This simple yet effective design enables robust attention, thereby generating representative prototypes by capturing the semantic relationships among support features. Our method achieves state-of-the-art performance with a significant margin on multiple FS-PCS benchmarks, demonstrating its effectiveness through extensive experiments.</li>
<li><strong>摘要：</strong>几乎没有标记的点云，只有几个标记的示例，几乎没有标记点云的每点标签旨在预测未标记点云的每点标签。为了从有限的支持集中提取区分性表示，现有方法使用常规算法（例如最远的点采样）构建了原型。但是，我们指出，其初始随机性显着影响FS-PCS的性能，并且尽管其流行率，但原型生成过程仍未被逐渐解散。这促使我们根据注意机制研究了先进的原型生成方法。尽管具有潜力，但我们发现香草模块遭受了可学习的原型令牌和支持特征之间的分布差距。为了克服这一点，我们提出了白色聚集和恢复模块（温暖），该模块通过夹层和着色转化之间的跨注意来解决未对准的未对准。具体而言，美白在注意过程之前将支持特征与原型令牌保持一致，随后将其着色恢复了与参与令牌的原始分布。这种简单而有效的设计可以引起强烈的关注，从而通过捕获支持功能之间的语义关系来产生代表性的原型。我们的方法可以在多个FS-PCS基准上获得最新的性能，并通过广泛的实验证明其有效性。</li>
</ul>

<h3>Title: Noise-Level Diffusion Guidance: Well Begun is Half Done</h3>
<ul>
<li><strong>Authors: </strong>Harvey Mannering, Zhiwu Huang, Adam Prugel-Bennett</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13936">https://arxiv.org/abs/2509.13936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13936">https://arxiv.org/pdf/2509.13936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13936]] Noise-Level Diffusion Guidance: Well Begun is Half Done(https://arxiv.org/abs/2509.13936)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved state-of-the-art image generation. However, the random Gaussian noise used to start the diffusion process influences the final output, causing variations in image quality and prompt adherence. Existing noise-level optimization approaches generally rely on extra dataset construction, additional networks, or backpropagation-based optimization, limiting their practicality. In this paper, we propose Noise Level Guidance (NLG), a simple, efficient, and general noise-level optimization approach that refines initial noise by increasing the likelihood of its alignment with general guidance - requiring no additional training data, auxiliary networks, or backpropagation. The proposed NLG approach provides a unified framework generalizable to both conditional and unconditional diffusion models, accommodating various forms of diffusion-level guidance. Extensive experiments on five standard benchmarks demonstrate that our approach enhances output generation quality and input condition adherence. By seamlessly integrating with existing guidance methods while maintaining computational efficiency, our method establishes NLG as a practical and scalable enhancement to diffusion models. Code can be found at this https URL.</li>
<li><strong>摘要：</strong>扩散模型已经实现了最新的图像生成。但是，用于启动扩散过程的随机高斯噪声会影响最终输出，从而导致图像质量和及时粘附的变化。现有的噪声级优化方法通常依赖于额外的数据集构建，其他网络或基于反向传播的优化，从而限制了它们的实用性。在本文中，我们提出了一种简单，高效且一般的噪声级优化方法（NLG），该方法通过增加其与一般指导的可能性来完善初始噪声 - 不需要其他培训数据，辅助网络或反向流向。提出的NLG方法提供了一个可以推广到条件和无条件扩散模型的统一框架，可满足各种形式的扩散级指导。对五个标准基准测试的广泛实验表明，我们的方法可以提高产量产生质量和输入状态依从性。通过在维持计算效率的同时，通过与现有的指导方法无缝集成，我们的方法将NLG建立为扩散模型的实用且可扩展的增强。代码可以在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Wan-Animate: Unified Character Animation and Replacement with Holistic Replication</h3>
<ul>
<li><strong>Authors: </strong>Gang Cheng, Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, Ju Li, Dechao Meng, Jinwei Qi, Penchong Qiao, Zhen Shen, Yafei Song, Ke Sun, Linrui Tian, Feng Wang, Guangyuan Wang, Qi Wang, Zhongjian Wang, Jiayu Xiao, Sheng Xu, Bang Zhang, Peng Zhang, Xindi Zhang, Zhe Zhang, Jingren Zhou, Lian Zhuo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14055">https://arxiv.org/abs/2509.14055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14055">https://arxiv.org/pdf/2509.14055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14055]] Wan-Animate: Unified Character Animation and Replacement with Holistic Replication(https://arxiv.org/abs/2509.14055)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce Wan-Animate, a unified framework for character animation and replacement. Given a character image and a reference video, Wan-Animate can animate the character by precisely replicating the expressions and movements of the character in the video to generate high-fidelity character videos. Alternatively, it can integrate the animated character into the reference video to replace the original character, replicating the scene's lighting and color tone to achieve seamless environmental integration. Wan-Animate is built upon the Wan model. To adapt it for character animation tasks, we employ a modified input paradigm to differentiate between reference conditions and regions for generation. This design unifies multiple tasks into a common symbolic representation. We use spatially-aligned skeleton signals to replicate body motion and implicit facial features extracted from source images to reenact expressions, enabling the generation of character videos with high controllability and expressiveness. Furthermore, to enhance environmental integration during character replacement, we develop an auxiliary Relighting LoRA. This module preserves the character's appearance consistency while applying the appropriate environmental lighting and color tone. Experimental results demonstrate that Wan-Animate achieves state-of-the-art performance. We are committed to open-sourcing the model weights and its source code.</li>
<li><strong>摘要：</strong>我们介绍了Wan-Animate，这是一个统一的角色动画和替代框架。给定角色图像和参考视频，Wan-Animate可以通过精确复制视频中字符的表达方式和动作来使角色动画，从而生成高保真性角色视频。另外，它可以将动画字符集成到参考视频中以替换原始角色，从而复制场景的照明和色调以实现无缝的环境集成。 Wan-Animate建立在WAN模型的基础上。为了适应角色动画任务，我们采用修改后的输入范式来区分参考条件和生成区域。该设计将多个任务统一成一个通用的符号表示。我们使用空间对准的骨骼信号来复制身体运动，并从源图像中提取的隐式面部特征来重演表达式，从而可以生成具有高可控性和表现力的角色视频。此外，为了增强角色替代过程中的环境整合，我们开发了辅助重新确认洛拉。该模块在应用适当的环境照明和色调的同时保留了角色的外观一致性。实验结果表明，Wan-Animate可以实现最新的表现。我们致力于开放型号的权重及其源代码。</li>
</ul>

<h3>Title: Generative AI for Misalignment-Resistant Virtual Staining to Accelerate Histopathology Workflows</h3>
<ul>
<li><strong>Authors: </strong>Jiabo MA, Wenqiang Li, Jinbang Li, Ziyi Liu, Linshan Wu, Fengtao Zhou, Li Liang, Ronald Cheong Kin Chan, Terence T.W. Wong, Hao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14119">https://arxiv.org/abs/2509.14119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14119">https://arxiv.org/pdf/2509.14119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14119]] Generative AI for Misalignment-Resistant Virtual Staining to Accelerate Histopathology Workflows(https://arxiv.org/abs/2509.14119)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Accurate histopathological diagnosis often requires multiple differently stained tissue sections, a process that is time-consuming, labor-intensive, and environmentally taxing due to the use of multiple chemical stains. Recently, virtual staining has emerged as a promising alternative that is faster, tissue-conserving, and environmentally friendly. However, existing virtual staining methods face significant challenges in clinical applications, primarily due to their reliance on well-aligned paired data. Obtaining such data is inherently difficult because chemical staining processes can distort tissue structures, and a single tissue section cannot undergo multiple staining procedures without damage or loss of information. As a result, most available virtual staining datasets are either unpaired or roughly paired, making it difficult for existing methods to achieve accurate pixel-level supervision. To address this challenge, we propose a robust virtual staining framework featuring cascaded registration mechanisms to resolve spatial mismatches between generated outputs and their corresponding ground truth. Experimental results demonstrate that our method significantly outperforms state-of-the-art models across five datasets, achieving an average improvement of 3.2% on internal datasets and 10.1% on external datasets. Moreover, in datasets with substantial misalignment, our approach achieves a remarkable 23.8% improvement in peak signal-to-noise ratio compared to baseline models. The exceptional robustness of the proposed method across diverse datasets simplifies the data acquisition process for virtual staining and offers new insights for advancing its development.</li>
<li><strong>摘要：</strong>准确的组织病理学诊断通常需要多个染色的组织切片，这是由于使用多种化学污渍而耗时，劳动力密集并且对环境征税的过程。最近，虚拟染色已成为一种有前途的替代方案，它更快，持有组织和环保。但是，现有的虚拟染色方法在临床应用中面临着重大挑战，这主要是由于它们依赖对齐配对的数据。获得此类数据本质上很困难，因为化学染色过程会扭曲组织结构，并且单个组织截面不能在没有损坏或信息损失的情况下进行多个染色程序。结果，大多数可用的虚拟染色数据集是未配对的或大致配对的，因此很难实现准确的像素级监督。为了应对这一挑战，我们提出了一个强大的虚拟染色框架，该框架具有级联的注册机制，以解决生成的输出之间的空间不匹配及其相应的地面真相。实验结果表明，我们的方法在五个数据集中显着胜过最先进的模型，在内部数据集的平均提高3.2％，外部数据集的平均提高为10.1％。此外，与基线模型相比，在具有很大的未对准的数据集中，我们的方法的峰值信噪比增长了23.8％。跨不同数据集的拟议方法的出色鲁棒性简化了虚拟染色的数据采集过程，并为推进其开发提供了新的见解。</li>
</ul>

<h3>Title: Deconstructing Intraocular Pressure: A Non-invasive Multi-Stage Probabilistic Inverse Framework</h3>
<ul>
<li><strong>Authors: </strong>Md Rezwan Jaher, Abul Mukid Mohammad Mukaddes, A. B. M. Abdul Malek</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM, stat.AP, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14167">https://arxiv.org/abs/2509.14167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14167">https://arxiv.org/pdf/2509.14167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14167]] Deconstructing Intraocular Pressure: A Non-invasive Multi-Stage Probabilistic Inverse Framework(https://arxiv.org/abs/2509.14167)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Many critical healthcare decisions are challenged by the inability to measure key underlying parameters. Glaucoma, a leading cause of irreversible blindness driven by elevated intraocular pressure (IOP), provides a stark example. The primary determinant of IOP, a tissue property called trabecular meshwork permeability, cannot be measured in vivo, forcing clinicians to depend on indirect surrogates. This clinical challenge is compounded by a broader computational one: developing predictive models for such ill-posed inverse problems is hindered by a lack of ground-truth data and prohibitive cost of large-scale, high-fidelity simulations. We address both challenges with an end-to-end framework to noninvasively estimate unmeasurable variables from sparse, routine data. Our approach combines a multi-stage artificial intelligence architecture to functionally separate the problem; a novel data generation strategy we term PCDS that obviates the need for hundreds of thousands of costly simulations, reducing the effective computational time from years to hours; and a Bayesian engine to quantify predictive uncertainty. Our framework deconstructs a single IOP measurement into its fundamental components from routine inputs only, yielding estimates for the unmeasurable tissue permeability and a patient's outflow facility. Our noninvasively estimated outflow facility achieved excellent agreement with state-of-the-art tonography with precision comparable to direct physical instruments. Furthermore, the newly derived permeability biomarker demonstrates high accuracy in stratifying clinical cohorts by disease risk, highlighting its diagnostic potential. More broadly, our framework establishes a generalizable blueprint for solving similar inverse problems in other data-scarce, computationally-intensive domains.</li>
<li><strong>摘要：</strong>许多关键的医疗保健决策受到无法衡量关键基础参数的挑战。青光眼是由眼内压力升高（IOP）驱动的不可逆失明的主要原因，它提供了一个鲜明的例子。 IOP的主要决定因素是一种称为小梁网渗透性的组织特性，不能在体内测量，迫使临床医生依赖间接的替代物。这种临床挑战更加复杂，更广泛的计算挑战：为这种不良反向问题开发预测模型受到了缺乏地面真相数据和大规模高保真模拟的高度成本的阻碍。我们通过端到端框架解决了这两个挑战，以从稀疏，常规数据中无创估计的变量。我们的方法结合了多阶段的人工智能体系结构，以在功能上将问题分开。一种新型的数据生成策略，我们称为PCD，该策略消除了对数十万个昂贵的模拟的需求，从而减少了数年的有效计算时间；和贝叶斯发动机以量化预测性不确定性。我们的框架仅从常规输入中将单个IOP测量分解为其基本组件，从而估计了无法衡量的组织渗透性和患者的流出设施。我们的无创估计的流出设施与最先进的音调学达成了极好的一致性，其精度与直接物理仪器相当。此外，新衍生的渗透性生物标志物在通过疾病风险对临床队列进行分层方面表现出很高的准确性，突出了其诊断潜力。更广泛地说，我们的框架建立了一个可概括的蓝图，用于解决其他数据筛选，计算密集型域中的类似逆问题。</li>
</ul>

<h3>Title: A Universal Banach--Bregman Framework for Stochastic Iterations: Unifying Stochastic Mirror Descent, Learning and LLM Training</h3>
<ul>
<li><strong>Authors: </strong>Johnny R. Zhang (Independent Researcher), Xiaomei Mi (University of Manchester), Gaoyuan Du (Amazon), Qianyi Sun (Microsoft), Shiqi Wang (Meta), Jiaxuan Li (Amazon), Wenhua Zhou (Independent Researcher)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14216">https://arxiv.org/abs/2509.14216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14216">https://arxiv.org/pdf/2509.14216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14216]] A Universal Banach--Bregman Framework for Stochastic Iterations: Unifying Stochastic Mirror Descent, Learning and LLM Training(https://arxiv.org/abs/2509.14216)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Stochastic optimization powers the scalability of modern artificial intelligence, spanning machine learning, deep learning, reinforcement learning, and large language model training. Yet, existing theory remains largely confined to Hilbert spaces, relying on inner-product frameworks and orthogonality. This paradigm fails to capture non-Euclidean settings, such as mirror descent on simplices, Bregman proximal methods for sparse learning, natural gradient descent in information geometry, or Kullback--Leibler-regularized language model training. Unlike Euclidean-based Hilbert-space methods, this approach embraces general Banach spaces. This work introduces a pioneering Banach--Bregman framework for stochastic iterations, establishing Bregman geometry as a foundation for next-generation optimization. It (i) provides a unified template via Bregman projections and Bregman--Fejer monotonicity, encompassing stochastic approximation, mirror descent, natural gradient, adaptive methods, and mirror-prox; (ii) establishes super-relaxations ($\lambda > 2$) in non-Hilbert settings, enabling flexible geometries and elucidating their acceleration effect; and (iii) delivers convergence theorems spanning almost-sure boundedness to geometric rates, validated on synthetic and real-world tasks. Empirical studies across machine learning (UCI benchmarks), deep learning (e.g., Transformer training), reinforcement learning (actor--critic), and large language models (WikiText-2 with distilGPT-2) show up to 20% faster convergence, reduced variance, and enhanced accuracy over classical baselines. These results position Banach--Bregman geometry as a cornerstone unifying optimization theory and practice across core AI paradigms.</li>
<li><strong>摘要：</strong>随机优化为现代人工智能，跨越机器学习，深度学习，强化学习和大型语言模型培训的可扩展性提供动力。然而，现有的理论在很大程度上依赖于希尔伯特的空间，依赖于内部产品框架和正交性。该范式无法捕获非欧几里得的设置，例如在简单上的镜像下降，稀疏学习的Bregman近端方法，信息几何形状中的自然梯度下降或Kullback-leibler regarler regarler语言模型培训。与基于欧几里得的希尔伯特空间方法不同，这种方法包含了一般的Banach空间。这项工作介绍了一个开创性的Banach-Bregman框架，用于随机迭代，建立了Bregman的几何形状作为下一代优化的基础。 IT（i）通过Bregman Projections和Bregman-Fejer单调性提供了一个统一的模板，包括随机近似，镜像下降，自然梯度，自适应方法和镜像 - 镜像； （ii）在非希尔伯特设置中建立超级放松（$ \ lambda> 2 $），从而实现灵活的几何形状并阐明其加速度效果； （iii）提供跨越几何界限的收敛定理，并在合成和现实世界任务上验证。跨机器学习（UCI基准），深度学习（例如变压器训练），增强学习（Actor-Critic-Critic）和大型语言模型（具有DISTILGPT-2）的大型语言模型（带有DISTILGPT-2）的实证研究表现出高达20％的收敛性，降低的差异和增强的准确性。这些结果位置Banach-Bregman几何形状是跨核AI范式的基石统一优化理论和实践。</li>
</ul>

<h3>Title: Defending Diffusion Models Against Membership Inference Attacks via Higher-Order Langevin Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Sterling, Yousef El-Laham, Mónica F. Bugallo</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14225">https://arxiv.org/abs/2509.14225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14225">https://arxiv.org/pdf/2509.14225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14225]] Defending Diffusion Models Against Membership Inference Attacks via Higher-Order Langevin Dynamics(https://arxiv.org/abs/2509.14225)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative artificial intelligence applications have raised new data security concerns. This paper focuses on defending diffusion models against membership inference attacks. This type of attack occurs when the attacker can determine if a certain data point was used to train the model. Although diffusion models are intrinsically more resistant to membership inference attacks than other generative models, they are still susceptible. The defense proposed here utilizes critically-damped higher-order Langevin dynamics, which introduces several auxiliary variables and a joint diffusion process along these variables. The idea is that the presence of auxiliary variables mixes external randomness that helps to corrupt sensitive input data earlier on in the diffusion process. This concept is theoretically investigated and validated on a toy dataset and a speech dataset using the Area Under the Receiver Operating Characteristic (AUROC) curves and the FID metric.</li>
<li><strong>摘要：</strong>生成人工智能应用程序的最新进展引起了新的数据安全问题。本文着重于为会员推理攻击辩护扩散模型。当攻击者可以确定是否使用某个数据点训练模型时，就会发生这种攻击。尽管扩散模型本质上比其他生成模型更耐构件推理攻击，但它们仍然容易受到影响。这里提出的辩护利用了严重抑制的高阶Langevin动力学，该动力学引入了几个辅助变量和沿这些变量的关节扩散过程。这个想法是，辅助变量的存在混合了外部随机性，有助于在扩散过程中较早地损坏敏感的输入数据。理论上，使用接收器操作特征（AUROC）曲线和FID指标下的区域和FID指标，在玩具数据集和语音数据集上对此概念进行了研究和验证。</li>
</ul>

<h3>Title: GenExam: A Multidisciplinary Text-to-Image Exam</h3>
<ul>
<li><strong>Authors: </strong>Zhaokai Wang, Penghao Yin, Xiangyu Zhao, Changyao Tian, Yu Qiao, Wenhai Wang, Jifeng Dai, Gen Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14232">https://arxiv.org/abs/2509.14232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14232">https://arxiv.org/pdf/2509.14232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14232]] GenExam: A Multidisciplinary Text-to-Image Exam(https://arxiv.org/abs/2509.14232)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Exams are a fundamental test of expert-level intelligence and require integrated understanding, reasoning, and generation. Existing exam-style benchmarks mainly focus on understanding and reasoning tasks, and current generation benchmarks emphasize the illustration of world knowledge and visual concepts, neglecting the evaluation of rigorous drawing exams. We introduce GenExam, the first benchmark for multidisciplinary text-to-image exams, featuring 1,000 samples across 10 subjects with exam-style prompts organized under a four-level taxonomy. Each problem is equipped with ground-truth images and fine-grained scoring points to enable a precise evaluation of semantic correctness and visual plausibility. Experiments show that even state-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve less than 15% strict scores, and most models yield almost 0%, suggesting the great challenge of our benchmark. By framing image generation as an exam, GenExam offers a rigorous assessment of models' ability to integrate knowledge, reasoning, and generation, providing insights on the path to general AGI.</li>
<li><strong>摘要：</strong>考试是对专家智能的基本测试，需要综合的理解，推理和一代。现有的考试风格的基准主要集中于理解和推理任务，而当前一代的基准强调了世界知识和视觉概念的说明，忽略了严格的绘画考试的评估。我们介绍了Genexam，这是用于多学科文本对图像考试的第一个基准，其中包含10个受试者的1,000个样本，并在四级分类法上组织了考试式提示。每个问题都配备了地面图像和细粒度的得分点，以便对语义正确性和视觉合理性进行精确评估。实验表明，即使是最新的模型，例如GPT-Image-1和Gemini-2.5-Flash-image的严格分数少于15％，并且大多数模型都会产生近0％，这表明我们的基准有了巨大的挑战。通过将图像生成作为考试，Genexam对模型整合知识，推理和生成的能力进行了严格的评估，从而提供了有关通往一般AGI道路的见解。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
