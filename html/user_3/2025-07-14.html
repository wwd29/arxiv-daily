<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-07-14</h1>
<h3>Title: ConsNoTrainLoRA: Data-driven Weight Initialization of Low-rank Adapters using Constraints</h3>
<ul>
<li><strong>Authors: </strong>Debasmit Das, Hyoungwoo Park, Munawar Hayat, Seokeon Choi, Sungrack Yun, Fatih Porikli</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08044">https://arxiv.org/abs/2507.08044</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08044">https://arxiv.org/pdf/2507.08044</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08044]] ConsNoTrainLoRA: Data-driven Weight Initialization of Low-rank Adapters using Constraints(https://arxiv.org/abs/2507.08044)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Foundation models are pre-trained on large-scale datasets and subsequently fine-tuned on small-scale datasets using parameter-efficient fine-tuning (PEFT) techniques like low-rank adapters (LoRA). In most previous works, LoRA weight matrices are randomly initialized with a fixed rank across all attachment points. In this paper, we improve convergence and final performance of LoRA fine-tuning, using our proposed data-driven weight initialization method, ConsNoTrainLoRA (CNTLoRA). We express LoRA initialization as a domain shift problem where we use multiple constraints relating the pre-training and fine-tuning activations. By reformulating these constraints, we obtain a closed-form estimate of LoRA weights that depends on pre-training weights and fine-tuning activation vectors and hence requires no training during initialization. This weight estimate is decomposed to initialize the up and down matrices with proposed flexibility of variable ranks. With the proposed initialization method, we fine-tune on downstream tasks such as image generation, image classification and image understanding. Both quantitative and qualitative results demonstrate that CNTLoRA outperforms standard and data-driven weight initialization methods. Extensive analyses and ablations further elucidate the design choices of our framework, providing an optimal recipe for faster convergence and enhanced performance.</li>
<li><strong>摘要：</strong>基础模型在大规模数据集上进行了预训练，随后使用参数有效的微调（PEFT）技术（如低级别适配器（LORA））对小规模数据集进行了微调。在以前的大多数工作中，洛拉重量矩阵在所有附件点上都具有固定等级的随机初始初始化。在本文中，我们使用我们提出的数据驱动的权重初始化方法Consnotrainlora（CNTlora）提高了Lora微调的收敛性和最终性能。我们将LORA初始化表示为域移位问题，在该问题中，我们使用了有关训练和微调激活的多个约束。通过重新制定这些约束，我们获得了洛拉权重的封闭形式估计，该估计取决于训练的权重和微调激活向量，因此在初始化过程中不需要训练。该重量估计值分解为以变量等级的灵活性初始化向上和向下矩阵。通过提出的初始化方法，我们可以微调下游任务，例如图像生成，图像分类和图像理解。定量和定性结果都表明，Cntlora优于标准和数据驱动的权重初始化方法。广泛的分析和消融进一步阐明了我们框架的设计选择，为更快的收敛和增强性能提供了最佳的配方。</li>
</ul>

<h3>Title: Rethinking Spatio-Temporal Anomaly Detection: A Vision for Causality-Driven Cybersecurity</h3>
<ul>
<li><strong>Authors: </strong>Arun Vignesh Malarkkan, Haoyue Bai, Xinyuan Wang, Anjali Kaushik, Dongjie Wang, Yanjie Fu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.ET, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08177">https://arxiv.org/abs/2507.08177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08177">https://arxiv.org/pdf/2507.08177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08177]] Rethinking Spatio-Temporal Anomaly Detection: A Vision for Causality-Driven Cybersecurity(https://arxiv.org/abs/2507.08177)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>As cyber-physical systems grow increasingly interconnected and spatially distributed, ensuring their resilience against evolving cyberattacks has become a critical priority. Spatio-Temporal Anomaly detection plays an important role in ensuring system security and operational integrity. However, current data-driven approaches, largely driven by black-box deep learning, face challenges in interpretability, adaptability to distribution shifts, and robustness under evolving system dynamics. In this paper, we advocate for a causal learning perspective to advance anomaly detection in spatially distributed infrastructures that grounds detection in structural cause-effect relationships. We identify and formalize three key directions: causal graph profiling, multi-view fusion, and continual causal graph learning, each offering distinct advantages in uncovering dynamic cause-effect structures across time and space. Drawing on real-world insights from systems such as water treatment infrastructures, we illustrate how causal models provide early warning signals and root cause attribution, addressing the limitations of black-box detectors. Looking ahead, we outline the future research agenda centered on multi-modality, generative AI-driven, and scalable adaptive causal frameworks. Our objective is to lay a new research trajectory toward scalable, adaptive, explainable, and spatially grounded anomaly detection systems. We hope to inspire a paradigm shift in cybersecurity research, promoting causality-driven approaches to address evolving threats in interconnected infrastructures.</li>
<li><strong>摘要：</strong>随着网络物理系统的增长越来越互连和空间分布，确保其抵抗不断发展的网络攻击的弹性已成为关键的优先事项。时空异常检测在确保系统安全性和操作完整性中起着重要作用。但是，当前的数据驱动方法主要由黑盒深度学习驱动，在不断发展的系统动态下面临可解释性，对分配变化的适应性以及鲁棒性的挑战。在本文中，我们主张有因果学习的观点，以推进空间分布的基础设施中的异常检测，该基础设施在结构性导致关系中检测基础。我们识别并形式化了三个关键方向：因果图谱分析，多视图融合和持续的因果图学习，每种都在跨时间和空间揭示动态效应结构时都具有明显的优势。利用水处理基础设施等系统的真实见解，我们说明了因果模型如何提供预警信号和根本原因归因，从而解决了黑盒检测器的局限性。展望未来，我们概述了未来的研究议程，围绕多模式，生成的AI驱动和可扩展的自适应因果框架。我们的目标是为可扩展，适应性，可解释和空间扎根的异常检测系统奠定新的研究轨迹。我们希望激发网络安全研究的范式转变，从而促进因果关系驱动的方法来应对互连基础设施中不断发展的威胁。</li>
</ul>

<h3>Title: HNOSeg-XS: Extremely Small Hartley Neural Operator for Efficient and Resolution-Robust 3D Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ken C. L. Wong, Hongzhi Wang, Tanveer Syeda-Mahmood</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08205">https://arxiv.org/abs/2507.08205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08205">https://arxiv.org/pdf/2507.08205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08205]] HNOSeg-XS: Extremely Small Hartley Neural Operator for Efficient and Resolution-Robust 3D Image Segmentation(https://arxiv.org/abs/2507.08205)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>In medical image segmentation, convolutional neural networks (CNNs) and transformers are dominant. For CNNs, given the local receptive fields of convolutional layers, long-range spatial correlations are captured through consecutive convolutions and pooling. However, as the computational cost and memory footprint can be prohibitively large, 3D models can only afford fewer layers than 2D models with reduced receptive fields and abstract levels. For transformers, although long-range correlations can be captured by multi-head attention, its quadratic complexity with respect to input size is computationally demanding. Therefore, either model may require input size reduction to allow more filters and layers for better segmentation. Nevertheless, given their discrete nature, models trained with patch-wise training or image downsampling may produce suboptimal results when applied on higher resolutions. To address this issue, here we propose the resolution-robust HNOSeg-XS architecture. We model image segmentation by learnable partial differential equations through the Fourier neural operator which has the zero-shot super-resolution property. By replacing the Fourier transform by the Hartley transform and reformulating the problem in the frequency domain, we created the HNOSeg-XS model, which is resolution robust, fast, memory efficient, and extremely parameter efficient. When tested on the BraTS'23, KiTS'23, and MVSeg'23 datasets with a Tesla V100 GPU, HNOSeg-XS showed its superior resolution robustness with fewer than 34.7k model parameters. It also achieved the overall best inference time (< 0.24 s) and memory efficiency (< 1.8 GiB) compared to the tested CNN and transformer models.</li>
<li><strong>摘要：</strong>在医学图像分割中，卷积神经网络（CNN）和变压器是主导的。对于CNN，鉴于卷积层的局部接收场，远程空间相关性是通过连续的卷积和合并来捕获的。但是，由于计算成本和内存足迹可能是过大的，因此3D模型只能比具有降低的接收场和抽象水平的2D型号提供的层较少。对于变压器，尽管可以通过多头关注来捕获远程相关性，但其相对于输入大小的二次复杂性在计算上是要求的。因此，任何一个模型都可能需要减小输入尺寸，以允许更多的过滤器和层以更好地分割。然而，鉴于其离散的性质，通过贴片训练或图像下采样训练的模型在应用于更高分辨率时可能会产生次优的结果。为了解决这个问题，我们在这里提出了分辨率hnoseg-XS体系结构。我们通过具有零击的超分辨率属性的傅立叶神经操作员来对图像进行分割。通过Hartley变换替换傅立叶变换并重新制定了频域中的问题，我们创建了Hnoseg-XS模型，该模型是稳健，快速，内存效率和极其参数效率的分辨率。当在Brats'23，Kits'23和具有Tesla V100 GPU的MVSEG'23数据集上进行测试时，Hnoseg-Xs显示出其出色的分辨率鲁棒性，较少34.7K模型参数。与测试的CNN和变压器模型相比，它还达到了总体最佳推理时间（<0.24 s）和记忆效率（<1.8 GIB）。</li>
</ul>

<h3>Title: InsightBuild: LLM-Powered Causal Reasoning in Smart Building Systems</h3>
<ul>
<li><strong>Authors: </strong>Pinaki Prasad Guha Neogi, Ahmad Mohammadshirazi, Rajiv Ramnath</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08235">https://arxiv.org/abs/2507.08235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08235">https://arxiv.org/pdf/2507.08235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08235]] InsightBuild: LLM-Powered Causal Reasoning in Smart Building Systems(https://arxiv.org/abs/2507.08235)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Smart buildings generate vast streams of sensor and control data, but facility managers often lack clear explanations for anomalous energy usage. We propose InsightBuild, a two-stage framework that integrates causality analysis with a fine-tuned large language model (LLM) to provide human-readable, causal explanations of energy consumption patterns. First, a lightweight causal inference module applies Granger causality tests and structural causal discovery on building telemetry (e.g., temperature, HVAC settings, occupancy) drawn from Google Smart Buildings and Berkeley Office datasets. Next, an LLM, fine-tuned on aligned pairs of sensor-level causes and textual explanations, receives as input the detected causal relations and generates concise, actionable explanations. We evaluate InsightBuild on two real-world datasets (Google: 2017-2022; Berkeley: 2018-2020), using expert-annotated ground-truth causes for a held-out set of anomalies. Our results demonstrate that combining explicit causal discovery with LLM-based natural language generation yields clear, precise explanations that assist facility managers in diagnosing and mitigating energy inefficiencies.</li>
<li><strong>摘要：</strong>智能建筑会产生大量的传感器和控制数据，但是设施经理通常缺乏对异常能量使用的明确解释。我们提出了InsightBuild，这是一个两阶段的框架，将因果关系分析与微调的大语言模型（LLM）相结合，以提供对能耗模式的人类可读，因果关系的解释。首先，轻巧的因果推理模块在建筑物遥测（例如，温度，HVAC设置，HVAC设置，占用率）上采用了Granger因果关系测试和结构性因果发现，并从Google Smart Buildings和Berkeley Office数据集中绘制。接下来，通过对齐的传感器级别原因和文本解释进行微调的LLM接收到所检测到的因果关系并产生简洁，可行的解释。我们在两个现实世界中的数据集（Google：2017-2022; Berkeley：2018-2020）上评估了InsightBuild，使用专家宣布的地面真实原因来进行一组持有的异常。我们的结果表明，将明确的因果发现与基于LLM的自然语言产生相结合可以得出清晰，精确的解释，可帮助设施经理诊断和减轻能源效率低下。</li>
</ul>

<h3>Title: Data Generation without Function Estimation</h3>
<ul>
<li><strong>Authors: </strong>Hadi Daneshmand, Ashkan Soleymani</a></li>
<li><strong>Subjects: </strong>cs.LG, math-ph, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08239">https://arxiv.org/abs/2507.08239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08239">https://arxiv.org/pdf/2507.08239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08239]] Data Generation without Function Estimation(https://arxiv.org/abs/2507.08239)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Estimating the score function (or other population-density-dependent functions) is a fundamental component of most generative models. However, such function estimation is computationally and statistically challenging. Can we avoid function estimation for data generation? We propose an estimation-free generative method: A set of points whose locations are deterministically updated with (inverse) gradient descent can transport a uniform distribution to arbitrary data distribution, in the mean field regime, without function estimation, training neural networks, and even noise injection. The proposed method is built upon recent advances in the physics of interacting particles. We show, both theoretically and experimentally, that these advances can be leveraged to develop novel generative methods.</li>
<li><strong>摘要：</strong>估计得分函数（或其他群众密度依赖性功能）是大多数生成模型的基本组成部分。但是，这种功能估计在计算和统计上具有挑战性。我们可以避免对数据生成的功能估计吗？我们提出了一种无估计生成方法：一组确定性更新的点（反梯度下降）可以将均匀分布运输到任意数据分布，在平均场地状态下，没有功能估计，训练神经网络甚至噪声注入。所提出的方法建立在相互作用粒子物理学的最新进展基础上。我们在理论上还是在实验上都表明，这些进步可以利用以开发新的生成方法。</li>
</ul>

<h3>Title: Transfer Learning and Mixup for Fine-Grained Few-Shot Fungi Classification</h3>
<ul>
<li><strong>Authors: </strong>Jason Kahei Tam, Murilo Gustineli, Anthony Miyaguchi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08248">https://arxiv.org/abs/2507.08248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08248">https://arxiv.org/pdf/2507.08248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08248]] Transfer Learning and Mixup for Fine-Grained Few-Shot Fungi Classification(https://arxiv.org/abs/2507.08248)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Accurate identification of fungi species presents a unique challenge in computer vision due to fine-grained inter-species variation and high intra-species variation. This paper presents our approach for the FungiCLEF 2025 competition, which focuses on few-shot fine-grained visual categorization (FGVC) using the FungiTastic Few-Shot dataset. Our team (DS@GT) experimented with multiple vision transformer models, data augmentation, weighted sampling, and incorporating textual information. We also explored generative AI models for zero-shot classification using structured prompting but found them to significantly underperform relative to vision-based models. Our final model outperformed both competition baselines and highlighted the effectiveness of domain specific pretraining and balanced sampling strategies. Our approach ranked 35/74 on the private test set in post-completion evaluation, this suggests additional work can be done on metadata selection and domain-adapted multi-modal learning. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>精确鉴定真菌物种在计算机视觉中提出了一个独特的挑战，这是由于种间间的变化和高种内的变化。本文介绍了我们对FungicleF 2025竞赛的方法，该竞赛的重点是使用Fungitastic几乎没有拍摄数据集进行了几次精细的视觉分类（FGVC）。我们的团队（DS@GT）尝试了多种视觉变压器模型，数据增强，加权采样以及合并文本信息。我们还使用结构化提示探索了用于零摄影分类的生成AI模型，但发现它们相对于基于视觉的模型的表现明显不足。我们的最终模型表现优于竞争基线，并强调了域特定的预处理和平衡采样策略的有效性。我们的方法在完成后评估的私人测试集中排名35/74，这表明可以在元数据选择和域适应的多模式学习方面完成其他工作。我们的代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Data-Driven Dimensional Synthesis of Diverse Planar Four-bar Function Generation Mechanisms via Direct Parameterization</h3>
<ul>
<li><strong>Authors: </strong>Woon Ryong Kim, Jaeheun Jung, Jeong Un Ha, Donghun Lee, Jae Kyung Shim</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08269">https://arxiv.org/abs/2507.08269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08269">https://arxiv.org/pdf/2507.08269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08269]] Data-Driven Dimensional Synthesis of Diverse Planar Four-bar Function Generation Mechanisms via Direct Parameterization(https://arxiv.org/abs/2507.08269)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Dimensional synthesis of planar four-bar mechanisms is a challenging inverse problem in kinematics, requiring the determination of mechanism dimensions from desired motion specifications. We propose a data-driven framework that bypasses traditional equation-solving and optimization by leveraging supervised learning. Our method combines a synthetic dataset, an LSTM-based neural network for handling sequential precision points, and a Mixture of Experts (MoE) architecture tailored to different linkage types. Each expert model is trained on type-specific data and guided by a type-specifying layer, enabling both single-type and multi-type synthesis. A novel simulation metric evaluates prediction quality by comparing desired and generated motions. Experiments show our approach produces accurate, defect-free linkages across various configurations. This enables intuitive and efficient mechanism design, even for non-expert users, and opens new possibilities for scalable and flexible synthesis in kinematic design.</li>
<li><strong>摘要：</strong>平面四杆机制的维度合成是运动学中一个具有挑战性的反问题，需要确定所需运动规格的机理维度。我们提出了一个数据驱动的框架，该框架通过利用监督学习来绕过传统的方程式解决和优化。我们的方法结合了一个合成数据集，基于LSTM的神经网络，用于处理顺序精确点，以及针对不同链接类型量身定制的专家（MOE）体系结构的混合物。每个专家模型均经过特定于类型的数据训练，并以类型指定层的指导，从而实现单型和多类型合成。一种新颖的模拟度量标准通过比较所需和产生的运动来评估预测质量。实验表明我们的方法在各种配置中产生准确的无缺陷链接。这即使对于非专家用户，也可以实现直观有效的机制设计，并为运动设计中的可扩展和灵活合成开辟了新的可能性。</li>
</ul>

<h3>Title: Lightweight Safety Guardrails via Synthetic Data and RL-guided Adversarial Training</h3>
<ul>
<li><strong>Authors: </strong>Aleksei Ilin, Gor Matevosyan, Xueying Ma, Vladimir Eremin, Suhaa Dada, Muqun Li, Riyaaz Shaik, Haluk Noyan Tokgozoglu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08284">https://arxiv.org/abs/2507.08284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08284">https://arxiv.org/pdf/2507.08284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08284]] Lightweight Safety Guardrails via Synthetic Data and RL-guided Adversarial Training(https://arxiv.org/abs/2507.08284)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We introduce a lightweight yet highly effective safety guardrail framework for language models, demonstrating that small-scale language models can achieve, and even surpass, the performance of larger counterparts in content moderation tasks. This is accomplished through high-fidelity synthetic data generation and adversarial training. The synthetic data generation process begins with human-curated seed data, which undergoes query augmentation and paraphrasing to create diverse and contextually rich examples. This augmented data is then subjected to multiple rounds of curation, ensuring high fidelity and relevance. Inspired by recent advances in the Generative Adversarial Network (GAN) architecture, our adversarial training employs reinforcement learning to guide a generator that produces challenging synthetic examples. These examples are used to fine-tune the safety classifier, enhancing its ability to detect and mitigate harmful content. Additionally, we incorporate strategies from recent research on efficient LLM training, leveraging the capabilities of smaller models to improve the performance of larger generative models. With iterative adversarial training and the generation of diverse, high-quality synthetic data, our framework enables small language models (SLMs) to serve as robust safety guardrails. This approach not only reduces computational overhead but also enhances resilience against adversarial attacks, offering a scalable and efficient solution for content moderation in AI systems.</li>
<li><strong>摘要：</strong>我们为语言模型引入了一个轻巧但高效的安全护栏框架，表明小规模的语言模型可以实现甚至超过内容审核任务中较大对应物的性能。这是通过高保真综合数据生成和对抗训练来实现的。合成数据生成过程始于人类策划的种子数据，该数据经历了查询增强和释义以创建多样化且上下文丰富的例子。然后将此增强数据进行多轮策展，以确保高忠诚度和相关性。受生成对抗网络（GAN）体系结构的最新进展的启发，我们的对抗训练采用了强化学习来指导产生具有挑战性的合成示例的发电机。这些示例用于微调安全分类器，增强其检测和减轻有害内容的能力。此外，我们纳入了有关有效LLM培训的最新研究的策略，利用较小模型的功能来提高较大生成模型的性能。借助迭代的对抗训练和产生多样化的高质量合成数据，我们的框架使小语言模型（SLM）可以充当强大的安全护栏。这种方法不仅减少了计算开销，而且还可以增强针对对抗性攻击的弹性，从而为AI系统中的内容适度提供了可扩展有效的解决方案。</li>
</ul>

<h3>Title: M2DAO-Talker: Harmonizing Multi-granular Motion Decoupling and Alternating Optimization for Talking-head Generation</h3>
<ul>
<li><strong>Authors: </strong>Kui Jiang, Shiyu Liu, Junjun Jiang, Xin Yang, Hongxun Yang, Xiaopeng Fan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08307">https://arxiv.org/abs/2507.08307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08307">https://arxiv.org/pdf/2507.08307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08307]] M2DAO-Talker: Harmonizing Multi-granular Motion Decoupling and Alternating Optimization for Talking-head Generation(https://arxiv.org/abs/2507.08307)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Audio-driven talking head generation holds significant potential for film production. While existing 3D methods have advanced motion modeling and content synthesis, they often produce rendering artifacts, such as motion blur, temporal jitter, and local penetration, due to limitations in representing stable, fine-grained motion fields. Through systematic analysis, we reformulate talking head generation into a unified framework comprising three steps: video preprocessing, motion representation, and rendering reconstruction. This framework underpins our proposed M2DAO-Talker, which addresses current limitations via multi-granular motion decoupling and alternating this http URL, we devise a novel 2D portrait preprocessing pipeline to extract frame-wise deformation control conditions (motion region segmentation masks, and camera parameters) to facilitate motion representation. To ameliorate motion modeling, we elaborate a multi-granular motion decoupling strategy, which independently models non-rigid (oral and facial) and rigid (head) motions for improved reconstruction this http URL, a motion consistency constraint is developed to ensure head-torso kinematic consistency, thereby mitigating penetration artifacts caused by motion aliasing. In addition, an alternating optimization strategy is designed to iteratively refine facial and oral motion parameters, enabling more realistic video this http URL across multiple datasets show that M2DAO-Talker achieves state-of-the-art performance, with the 2.43 dB PSNR improvement in generation quality and 0.64 gain in user-evaluated video realness versus TalkingGaussian while with 150 FPS inference speed. Our project homepage is this https URL</li>
<li><strong>摘要：</strong>音频驱动的会说话的脑电图具有巨大的电影制作潜力。尽管现有的3D方法具有先进的运动建模和内容合成，但由于代表稳定的，细粒度的运动场的限制，它们通常会产生渲染的伪像，例如运动模糊，时间抖动和局部穿透。通过系统的分析，我们将交谈的头部生成重新制定为一个统一的框架，该框架包括三个步骤：视频预处理，运动表示和渲染重建。该框架为我们提出的M2DAO-TAKER的基础，该框架通过多个晶体运动解耦合解决当前局限性，并与此HTTP URL交替，我们设计了一种新颖的2D肖像预处理管道，以提取框架框架框架的变形控制条件（运动区域分割蒙版和摄像机参数），以使运动表示运动。为了改善运动建模，我们详细阐述了一种多晶状运动脱耦策略，该策略独立建模非刚性（口服和面部）和刚性（头部）动作以改善重建此HTTP URL，开发了运动一致性约束，以确保由智力固定的渗透性动作，以确保头部tors-tors-tors-tors-tors-tors-tors toremative somitifips and渗透性。 In addition, an alternating optimization strategy is designed to iteratively refine facial and oral motion parameters, enabling more realistic video this http URL across multiple datasets show that M2DAO-Talker achieves state-of-the-art performance, with the 2.43 dB PSNR improvement in generation quality and 0.64 gain in user-evaluated video realness versus TalkingGaussian while with 150 FPS inference speed.我们的项目主页是此HTTPS URL</li>
</ul>

<h3>Title: CoCo-Bot: Energy-based Composable Concept Bottlenecks for Interpretable Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Sangwon Kim, In-su Jang, Pyongkun Kim, Kwang-Ju Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08334">https://arxiv.org/abs/2507.08334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08334">https://arxiv.org/pdf/2507.08334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08334]] CoCo-Bot: Energy-based Composable Concept Bottlenecks for Interpretable Generative Models(https://arxiv.org/abs/2507.08334)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Concept Bottleneck Models (CBMs) provide interpretable and controllable generative modeling by routing generation through explicit, human-understandable concepts. However, previous generative CBMs often rely on auxiliary visual cues at the bottleneck to compensate for information not captured by the concepts, which undermines interpretability and compositionality. We propose CoCo-Bot, a post-hoc, composable concept bottleneck generative model that eliminates the need for auxiliary cues by transmitting all information solely through explicit concepts. Guided by diffusion-based energy functions, CoCo-Bot supports robust post-hoc interventions-such as concept composition and negation-across arbitrary concepts. Experiments using StyleGAN2 pre-trained on CelebA-HQ show that CoCo-Bot improves concept-level controllability and interpretability, while maintaining competitive visual quality.</li>
<li><strong>摘要：</strong>概念瓶颈模型（CBMS）通过通过明确的，可理解的概念来扩展生成，提供了可解释和可控制的生成建模。但是，以前的生成CBM通常依靠瓶颈上的辅助视觉提示来补偿未经概念捕获的信息，从而破坏了可解释性和组成性。我们提出了可可 - 机器人，这是一种事后，可组合的概念瓶颈生成模型，该模型通过仅通过明确的概念传输所有信息来消除对辅助提示的需求。在基于扩散的能量函数的指导下，可可机器人支持稳健的事后干预措施，例如概念组成和否定的任意概念。使用stylegan2在Celeba-HQ上进行训练的实验表明，可可机器人可以改善概念级的可控性和可解释性，同时保持竞争性的视觉质量。</li>
</ul>

<h3>Title: Towards Imperceptible JPEG Image Hiding: Multi-range Representations-driven Adversarial Stego Generation</h3>
<ul>
<li><strong>Authors: </strong>Junxue Yang, Xin Liao, Weixuan Tang, Jianhua Yang, Zheng Qin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08343">https://arxiv.org/abs/2507.08343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08343">https://arxiv.org/pdf/2507.08343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08343]] Towards Imperceptible JPEG Image Hiding: Multi-range Representations-driven Adversarial Stego Generation(https://arxiv.org/abs/2507.08343)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Deep hiding has been exploring the hiding capability of deep learning-based models, aiming to conceal image-level messages into cover images and reveal them from generated stego images. Existing schemes are easily detected by steganalyzers due to their large payloads and their limitation to feature extraction based solely on either pure convolution or pure transformer operators within a single range, as well as pixel-level loss constraints. To address the issue, in this paper, we introduce generation-based adversarial attacks into color JPEG image deep hiding and propose a multi-range representations-driven adversarial stego generation framework called MRAG from a steganalysis perspective. Specifically, we integrate the local-range neighbor reception characteristic of the convolution and the global-range dependency modeling of the transformer to construct MRAG. Meanwhile, we use the transformed images obtained through coarse-grained and fine-grained frequency decomposition as inputs, introducing multi-grained information. Furthermore, a features angle-norm disentanglement loss is designed to constrain the generated stegos closer to covers in the angle and norm space of the steganalyzer's classified features. Consequently, small yet effective adversarial perturbations can be injected into the process of generating stegos, ensuring that stegos maintain favorable secret restorability and imperceptibility. Extensive experiments demonstrate that MRAG can achieve state-of-the-art performance.</li>
<li><strong>摘要：</strong>Deep Histing一直在探索基于深度学习的模型的隐藏功能，旨在将图像级信息隐藏到封面图像中，并从生成的Stego图像中揭示它们。由于其较大的有效载荷以及仅基于单个范围内的纯卷积或纯变压器操作员以及像素级损失约束，因此，斯坦分析仪很容易检测到现有方案。为了解决这个问题，在本文中，我们将基于世代的对抗性攻击引入颜色JPEG图像深隐藏，并提出了一个多范围表示的对抗性驱动的对抗性Stego生成框架，称为MRAG，称为MRAG。具体而言，我们整合了卷积的局部范围邻居接收特征以及变压器的全球范围依赖性建模以构建MRAG。同时，我们使用通过粗粒和细粒频率分解获得的转换图像作为输入，从而引入多透明信息。此外，特征角度分解损失旨在限制在脱脂仪的分类特征的角度和范围空间中靠近覆盖物的生成的segos。因此，可以将小而有效的对抗性扰动注入生成天文的过程中，以确保Stegos保持有利的秘密可再生性和不可识别性。广泛的实验表明，MRAG可以实现最新的性能。</li>
</ul>

<h3>Title: Leveraging Machine Learning and Enhanced Parallelism Detection for BPMN Model Generation from Text</h3>
<ul>
<li><strong>Authors: </strong>Phuong Nam Lê, Charlotte Schneider-Depré, Alexandre Goossens, Alexander Stevens, Aurélie Leribaux, Johannes De Smedt</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08362">https://arxiv.org/abs/2507.08362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08362">https://arxiv.org/pdf/2507.08362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08362]] Leveraging Machine Learning and Enhanced Parallelism Detection for BPMN Model Generation from Text(https://arxiv.org/abs/2507.08362)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Efficient planning, resource management, and consistent operations often rely on converting textual process documents into formal Business Process Model and Notation (BPMN) models. However, this conversion process remains time-intensive and costly. Existing approaches, whether rule-based or machine-learning-based, still struggle with writing styles and often fail to identify parallel structures in process descriptions. This paper introduces an automated pipeline for extracting BPMN models from text, leveraging the use of machine learning and large language models. A key contribution of this work is the introduction of a newly annotated dataset, which significantly enhances the training process. Specifically, we augment the PET dataset with 15 newly annotated documents containing 32 parallel gateways for model training, a critical feature often overlooked in existing datasets. This addition enables models to better capture parallel structures, a common but complex aspect of process descriptions. The proposed approach demonstrates adequate performance in terms of reconstruction accuracy, offering a promising foundation for organizations to accelerate BPMN model creation.</li>
<li><strong>摘要：</strong>有效的计划，资源管理和一致的操作通常依赖于将文本过程文档转换为正式的业务流程模型和符号（BPMN）模型。但是，这种转换过程仍然是耗时且昂贵的。现有的方法，无论是基于规则还是基于机器学习的方法，仍然在写作样式方面都很难以识别流程描述中的并行结构。本文介绍了一种自动化管道，用于从文本中提取BPMN模型，利用机器学习和大型语言模型的使用。这项工作的关键贡献是引入了新注释的数据集，该数据集大大增强了培训过程。具体而言，我们使用15个新注释的文档增强了PET数据集，其中包含32个平行网关用于模型训练，这是现有数据集经常忽略的关键功能。此添加使模型能够更好地捕获并行结构，这是过程描述的常见但复杂的方面。所提出的方法在重建准确性方面表明了足够的性能，为组织加速BPMN模型创建提供了有希望的基础。</li>
</ul>

<h3>Title: Understanding Driving Risks using Large Language Models: Toward Elderly Driver Assessment</h3>
<ul>
<li><strong>Authors: </strong>Yuki Yoshihara, Linjing Jiang, Nihan Karatas, Hitoshi Kanamori, Asuka Harada, Takahiro Tanaka</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08367">https://arxiv.org/abs/2507.08367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08367">https://arxiv.org/pdf/2507.08367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08367]] Understanding Driving Risks using Large Language Models: Toward Elderly Driver Assessment(https://arxiv.org/abs/2507.08367)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This study investigates the potential of a multimodal large language model (LLM), specifically ChatGPT-4o, to perform human-like interpretations of traffic scenes using static dashcam images. Herein, we focus on three judgment tasks relevant to elderly driver assessments: evaluating traffic density, assessing intersection visibility, and recognizing stop signs recognition. These tasks require contextual reasoning rather than simple object detection. Using zero-shot, few-shot, and multi-shot prompting strategies, we evaluated the performance of the model with human annotations serving as the reference standard. Evaluation metrics included precision, recall, and F1-score. Results indicate that prompt design considerably affects performance, with recall for intersection visibility increasing from 21.7% (zero-shot) to 57.0% (multi-shot). For traffic density, agreement increased from 53.5% to 67.6%. In stop-sign detection, the model demonstrated high precision (up to 86.3%) but a lower recall (approximately 76.7%), indicating a conservative response tendency. Output stability analysis revealed that humans and the model faced difficulties interpreting structurally ambiguous scenes. However, the model's explanatory texts corresponded with its predictions, enhancing interpretability. These findings suggest that, with well-designed prompts, LLMs hold promise as supportive tools for scene-level driving risk assessments. Future studies should explore scalability using larger datasets, diverse annotators, and next-generation model architectures for elderly driver assessments.</li>
<li><strong>摘要：</strong>这项研究研究了使用静态仪表板图像对交通场景进行类似人类的解释，研究了多模式大语言模型（LLM）的潜力。在此，我们专注于与老年驾驶员评估相关的三个判断任务：评估交通密度，评估交叉点可见性并识别停车标志的识别。这些任务需要上下文推理，而不是简单的对象检测。使用零射击，很少射击和多拍促进策略，我们以人为注释作为参考标准评估了模型的性能。评估指标包括精度，召回和F1得分。结果表明，迅速设计会极大地影响性能，召回相交可见性从21.7％（零射）增加到57.0％（多拍）。对于交通密度，协议从53.5％增加到67.6％。在停止检测中，该模型表现出很高的精度（最高86.3％），但召回率较低（约76.7％），表明保守的响应趋势。输出稳定性分析表明，人类和模型面临着解释结构模棱两可的场景的困难。但是，该模型的解释性文本与其预测相对应，从而增强了解释性。这些发现表明，通过精心设计的提示，LLMS拥有承诺作为场景级别驾驶风险评估的支持工具。未来的研究应使用较大的数据集，不同的注释者和下一代模型体系结构来探索可伸缩性，以进行老年驾驶员评估。</li>
</ul>

<h3>Title: Unsupervised Methods for Video Quality Improvement: A Survey of Restoration and Enhancement Techniques</h3>
<ul>
<li><strong>Authors: </strong>Alexandra Malyugina, Yini Li, Joanne Lin, Nantheera Anantrasirichai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08375">https://arxiv.org/abs/2507.08375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08375">https://arxiv.org/pdf/2507.08375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08375]] Unsupervised Methods for Video Quality Improvement: A Survey of Restoration and Enhancement Techniques(https://arxiv.org/abs/2507.08375)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Video restoration and enhancement are critical not only for improving visual quality, but also as essential pre-processing steps to boost the performance of a wide range of downstream computer vision tasks. This survey presents a comprehensive review of video restoration and enhancement techniques with a particular focus on unsupervised approaches. We begin by outlining the most common video degradations and their underlying causes, followed by a review of early conventional and deep learning methods-based, highlighting their strengths and limitations. We then present an in-depth overview of unsupervised methods, categorise by their fundamental approaches, including domain translation, self-supervision signal design and blind spot or noise-based methods. We also provide a categorization of loss functions employed in unsupervised video restoration and enhancement, and discuss the role of paired synthetic datasets in enabling objective evaluation. Finally, we identify key challenges and outline promising directions for future research in this field.</li>
<li><strong>摘要：</strong>视频恢复和增强不仅对于提高视觉质量至关重要，而且是提高各种下游计算机视觉任务的性能的基本预处理步骤。这项调查对视频恢复和增强技术进行了全面综述，特别关注无监督方法。我们首先概述了最常见的视频退化及其根本原因，然后对早期常规和深度学习方法进行了审查，突出了它们的优势和局限性。然后，我们介绍了无监督方法的深入概述，并按照其基本方法进行分类，包括域翻译，自学信号设计以及盲点或基于噪声的方法。我们还提供了无监督视频恢复和增强中采用的损失功能的分类，并讨论了配对合成数据集在实现客观评估中的作用。最后，我们确定了关键的挑战，并概述了该领域未来研究的有希望的方向。</li>
</ul>

<h3>Title: From Enhancement to Understanding: Build a Generalized Bridge for Low-light Vision via Semantically Consistent Unsupervised Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Sen Wang, Shao Zeng, Tianjun Gu, Zhizhong Zhang, Ruixin Zhang, Shouhong Ding, Jingyun Zhang, Jun Wang, Xin Tan, Yuan Xie, Lizhuang Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08380">https://arxiv.org/abs/2507.08380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08380">https://arxiv.org/pdf/2507.08380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08380]] From Enhancement to Understanding: Build a Generalized Bridge for Low-light Vision via Semantically Consistent Unsupervised Fine-tuning(https://arxiv.org/abs/2507.08380)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Low-level enhancement and high-level visual understanding in low-light vision have traditionally been treated separately. Low-light enhancement improves image quality for downstream tasks, but existing methods rely on physical or geometric priors, limiting generalization. Evaluation mainly focuses on visual quality rather than downstream performance. Low-light visual understanding, constrained by scarce labeled data, primarily uses task-specific domain adaptation, which lacks scalability. To address these challenges, we build a generalized bridge between low-light enhancement and low-light understanding, which we term Generalized Enhancement For Understanding (GEFU). This paradigm improves both generalization and scalability. To address the diverse causes of low-light degradation, we leverage pretrained generative diffusion models to optimize images, achieving zero-shot generalization performance. Building on this, we propose Semantically Consistent Unsupervised Fine-tuning (SCUF). Specifically, to overcome text prompt limitations, we introduce an illumination-aware image prompt to explicitly guide image generation and propose a cycle-attention adapter to maximize its semantic potential. To mitigate semantic degradation in unsupervised training, we propose caption and reflectance consistency to learn high-level semantics and image-level spatial semantics. Extensive experiments demonstrate that our proposed method outperforms current state-of-the-art methods in traditional image quality and GEFU tasks including classification, detection, and semantic segmentation.</li>
<li><strong>摘要：</strong>传统上，在弱光视力中分别处理低级增强和高级视觉理解。低光增强可改善下游任务的图像质量，但是现有方法依赖于物理或几何先验，从而限制了概括。评估主要关注视觉质量而不是下游性能。低光视觉理解，受稀缺标记的数据约束，主要使用特定于任务的域适应性，这缺乏可扩展性。为了应对这些挑战，我们在低光增强和低光理解之间建立了一座广义的桥梁，我们将其称为“理解”（GEFU）。该范式提高了概括和可扩展性。为了解决低光降解的多种原因，我们利用预验证的生成扩散模型来优化图像，实现零拍的概括性能。在此基础上，我们提出了语义上一致的无监督微调（SCUF）。具体来说，为了克服文本及时限制，我们引入了一个照明感知图像提示，以明确指导图像生成并提出一个周期注意的适配器，以最大程度地发挥其语义潜力。为了减轻无监督训练中的语义降解，我们提出了标题和反射率一致性，以学习高级语义和图像级别的空间语义。广泛的实验表明，我们所提出的方法在传统图像质量和GEFU任务中的最新方法（包括分类，检测和语义分割）优于当前的最新方法。</li>
</ul>

<h3>Title: Inference-Time Scaling of Diffusion Language Models with Particle Gibbs Sampling</h3>
<ul>
<li><strong>Authors: </strong>Meihua Dang, Jiaqi Han, Minkai Xu, Kai Xu, Akash Srivastava, Stefano Ermon</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08390">https://arxiv.org/abs/2507.08390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08390">https://arxiv.org/pdf/2507.08390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08390]] Inference-Time Scaling of Diffusion Language Models with Particle Gibbs Sampling(https://arxiv.org/abs/2507.08390)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Discrete diffusion models have emerged as a powerful paradigm for language modeling, rivaling auto-regressive models by training-time scaling. However, inference-time scaling in discrete diffusion models remains relatively under-explored. In this work, we study sampling-based approaches for achieving high-quality text generation from discrete diffusion models in reward-guided settings. We introduce a novel inference-time scaling approach based on particle Gibbs sampling for discrete diffusion models. The particle Gibbs sampling algorithm iteratively refines full diffusion trajectories using conditional Sequential Monte Carlo as its transition mechanism. This process ensures that the updated samples progressively improve and move closer to the reward-weighted target distribution. Unlike existing inference-time scaling methods, which are often limited to single diffusion trajectories, our approach leverages iterative refinement across multiple trajectories. Within this framework, we further analyze the trade-offs between four key axes for inference-time scaling under fixed compute budgets: particle Gibbs iterations, particle count, denoising steps, and reward estimation cost. Empirically, our method consistently outperforms prior inference-time strategies on reward-guided text generation tasks, achieving significant improvement in accuracy under varying compute budgets.</li>
<li><strong>摘要：</strong>离散扩散模型已成为语言建模的强大范式，通过训练时间缩放与自动回归模型匹配。但是，离散扩散模型中的推理时间缩放量仍然相对不足。在这项工作中，我们研究了基于抽样的方法，用于从奖励引导设置中的离散扩散模型中实现高质量的文本生成。我们引入了一种基于离散扩散模型的粒子吉布斯采样的新型推理时间缩放方法。粒子吉布斯采样算法迭代地使用条件顺序蒙特卡洛作为其过渡机制来完善完整的扩散轨迹。此过程确保更新的样品逐渐改进并更接近奖励加权目标分布。与通常仅限于单个扩散轨迹的现有推理时间缩放方法不同，我们的方法利用了多个轨迹之间的迭代完善。在此框架内，我们进一步分析了在固定计算预算下进行推理时间缩放的四个关键轴之间的权衡：粒子吉布斯迭代，粒子计数，降解步骤和奖励估计成本。从经验上讲，我们的方法始终优于奖励引导的文本生成任务的提前推理时间策略，在不同的计算预算下，准确性显着提高。</li>
</ul>

<h3>Title: Subject-Consistent and Pose-Diverse Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhanxin Gao, Beier Zhu, Liang Yao, Jian Yang, Ying Tai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08396">https://arxiv.org/abs/2507.08396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08396">https://arxiv.org/pdf/2507.08396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08396]] Subject-Consistent and Pose-Diverse Text-to-Image Generation(https://arxiv.org/abs/2507.08396)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Subject-consistent generation (SCG)-aiming to maintain a consistent subject identity across diverse scenes-remains a challenge for text-to-image (T2I) models. Existing training-free SCG methods often achieve consistency at the cost of layout and pose diversity, hindering expressive visual storytelling. To address the limitation, we propose subject-Consistent and pose-Diverse T2I framework, dubbed as CoDi, that enables consistent subject generation with diverse pose and layout. Motivated by the progressive nature of diffusion, where coarse structures emerge early and fine details are refined later, CoDi adopts a two-stage strategy: Identity Transport (IT) and Identity Refinement (IR). IT operates in the early denoising steps, using optimal transport to transfer identity features to each target image in a pose-aware manner. This promotes subject consistency while preserving pose diversity. IR is applied in the later denoising steps, selecting the most salient identity features to further refine subject details. Extensive qualitative and quantitative results on subject consistency, pose diversity, and prompt fidelity demonstrate that CoDi achieves both better visual perception and stronger performance across all metrics. The code is provided in this https URL.</li>
<li><strong>摘要：</strong>主体一致的一代（SCG） - 默默保持在不同场景中保持一致的主题身份，这是对文本到图像（T2I）模型的挑战。现有的无培训SCG方法通常以布局成本和构成多样性的成本，阻碍表现力的视觉讲故事。为了解决限制，我们提出了被称为Codi的主题一致和姿势多样性T2I框架，该框架可以具有多种姿势和布局的一致生成。由扩散的渐进性激励，在后来完善了粗糙结构的早期和细节，Codi采用了两阶段的策略：身份传输（IT）和身份改进（IR）。它在早期的脱氧步骤中运行，使用最佳传输以姿势感知方式将身份特征传递给每个目标图像。这促进了主题的一致性，同时保持姿势多样性。 IR在后来的Denoising步骤中应用，选择最显着的身份功能以进一步完善主题细节。关于主题一致性，构成多样性和迅速的忠诚度的广泛定性和定量结果表明，Codi在所有指标中都可以实现更好的视觉感知和更强的性能。该代码在此HTTPS URL中提供。</li>
</ul>

<h3>Title: Multi-modal Mutual-Guidance Conditional Prompt Learning for Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shijun Yang, Xiang Zhang, Wanqing Zhao, Hangzai Luo, Sheng Zhong, Jinye Peng, Jianping Fan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08410">https://arxiv.org/abs/2507.08410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08410">https://arxiv.org/pdf/2507.08410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08410]] Multi-modal Mutual-Guidance Conditional Prompt Learning for Vision-Language Models(https://arxiv.org/abs/2507.08410)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Prompt learning facilitates the efficient adaptation of Vision-Language Models (VLMs) to various downstream tasks. However, it faces two significant challenges: (1) inadequate modeling of class embedding distributions for unseen instances, leading to suboptimal generalization on novel classes; (2) prevailing methodologies predominantly confine cross-modal alignment to the final output layer of vision and text encoders, which fundamentally limits their capacity to preserve topological consistency with pre-trained multi-modal embedding spaces. To this end, we introduce MuGCP (Multi-modal Mutual-Guidance Conditional Prompt Learning), a novel paradigm designed for conditional prompt generation. MuGCP leverages Multi-modal Large Language Models (MLLMs) as conditional prompt learners to adaptively generate Semantic Conditional Prompts (SCP) that incorporate rich, fine-grained high-level semantic knowledge for image instances. To ensure effective alignment and interaction across the multi-modal space of Vision-Language Models (VLMs), we introduce the Attention Mutual-Guidance (AMG) module, which facilitates interactions between visual and semantic information. Through mutual guidance, the AMG module generates Visual Conditional Prompts (VCP), enhancing the model's performance in multi-modal tasks. Additionally, we present a Multi-Prompt Fusion (MPF) mechanism that integrates SCP and VCP with contextual prompts, ensuring seamless coordination among the different prompts and enhancing the modeling of class embeddings and instance-specific knowledge. Our MuGCP outperforms existing state-of-the-art methods on 14 different datasets. The code will be made available after publication.</li>
<li><strong>摘要：</strong>迅速学习促进了视觉模型（VLM）对各种下游任务的有效适应。但是，它面临两个重大挑战：（1）对看不见的实例的类嵌入分布的建模不足，从而导致对新阶级的次优概括； （2）流行的方法论主要将跨模式对准限制在视觉和文本编码器的最终输出层，这从根本上限制了其与预训练的多模式嵌入空间保持拓扑一致性的能力。为此，我们介绍了MUGCP（多模式相互引导条件及时学习），这是一种专为有条件及时生成的新型范式。 MUGCP利用多模式的大语言模型（MLLM）作为有条件的提示，学习者可以适应性地生成语义条件提示（SCP），该提示（SCP）包含了图像实例的丰富，细粒度的高级语义知识。为了确保在视觉模型（VLMS）的多模式空间之间进行有效的对齐和相互作用，我们介绍了注意力相互辅助（AMG）模块，该模块促进了视觉和语义信息之间的相互作用。通过相互指导，AMG模块生成视觉条件提示（VCP），从而在多模式任务中增强了模型的性能。此外，我们提出了一种多项目融合（MPF）机制，该机制将SCP和VCP与上下文提示集成在一起，确保不同提示之间的无缝协调并增强类嵌入类别的建模和实例特定知识。我们的MUGCP在14个不同的数据集上优于现有的最新方法。该代码将在出版后提供。</li>
</ul>

<h3>Title: InstaScene: Towards Complete 3D Instance Decomposition and Reconstruction from Cluttered Scenes</h3>
<ul>
<li><strong>Authors: </strong>Zesong Yang, Bangbang Yang, Wenqi Dong, Chenxuan Cao, Liyuan Cui, Yuewen Ma, Zhaopeng Cui, Hujun Bao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08416">https://arxiv.org/abs/2507.08416</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08416">https://arxiv.org/pdf/2507.08416</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08416]] InstaScene: Towards Complete 3D Instance Decomposition and Reconstruction from Cluttered Scenes(https://arxiv.org/abs/2507.08416)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Humans can naturally identify and mentally complete occluded objects in cluttered environments. However, imparting similar cognitive ability to robotics remains challenging even with advanced reconstruction techniques, which models scenes as undifferentiated wholes and fails to recognize complete object from partial observations. In this paper, we propose InstaScene, a new paradigm towards holistic 3D perception of complex scenes with a primary goal: decomposing arbitrary instances while ensuring complete reconstruction. To achieve precise decomposition, we develop a novel spatial contrastive learning by tracing rasterization of each instance across views, significantly enhancing semantic supervision in cluttered scenes. To overcome incompleteness from limited observations, we introduce in-situ generation that harnesses valuable observations and geometric cues, effectively guiding 3D generative models to reconstruct complete instances that seamlessly align with the real world. Experiments on scene decomposition and object completion across complex real-world and synthetic scenes demonstrate that our method achieves superior decomposition accuracy while producing geometrically faithful and visually intact objects.</li>
<li><strong>摘要：</strong>人类可以在混乱的环境中自然识别和精神上完整的封闭物体。但是，即使使用先进的重建技术，赋予机器人技术类似的认知能力仍然具有挑战性，该技术将场景模拟为未分化的批发，并且无法从部分观察结果中识别完整的对象。在本文中，我们提出了Instascene，这是一个新的范式，旨在以一个主要目标：分解任意实例的同时确保完全重建。为了实现精确的分解，我们通过追踪每个实例的栅格化来开发一种新颖的空间对比学习，从而显着增强了混乱的场景中的语义监督。为了克服有限观察的不完整性，我们引入了原地，利用有价值的观察和几何提示，有效地指导3D生成模型，以重建与现实世界无缝一致的完整实例。在复杂的现实世界和合成场景之间进行场景分解和对象完成的实验表明，我们的方法在产生几何忠实且视觉上完整的对象时达到了出色的分解精度。</li>
</ul>

<h3>Title: Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Wongi Jeong, Kyungryeol Lee, Hoigi Seo, Se Young Chun</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08422">https://arxiv.org/abs/2507.08422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08422">https://arxiv.org/pdf/2507.08422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08422]] Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated Diffusion Transformers(https://arxiv.org/abs/2507.08422)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion transformers have emerged as an alternative to U-net-based diffusion models for high-fidelity image and video generation, offering superior scalability. However, their heavy computation remains a major obstacle to real-world deployment. Existing acceleration methods primarily exploit the temporal dimension such as reusing cached features across diffusion timesteps. Here, we propose Region-Adaptive Latent Upsampling (RALU), a training-free framework that accelerates inference along spatial dimension. RALU performs mixed-resolution sampling across three stages: 1) low-resolution denoising latent diffusion to efficiently capture global semantic structure, 2) region-adaptive upsampling on specific regions prone to artifacts at full-resolution, and 3) all latent upsampling at full-resolution for detail refinement. To stabilize generations across resolution transitions, we leverage noise-timestep rescheduling to adapt the noise level across varying resolutions. Our method significantly reduces computation while preserving image quality by achieving up to 7.0$\times$ speed-up on FLUX and 3.0$\times$ on Stable Diffusion 3 with minimal degradation. Furthermore, RALU is complementary to existing temporal accelerations such as caching methods, thus can be seamlessly integrated to further reduce inference latency without compromising generation quality.</li>
<li><strong>摘要：</strong>扩散变压器已成为用于高保真图像和视频生成的基于U-NET的扩散模型的替代方法，提供了卓越的可扩展性。但是，他们的大量计算仍然是现实部署的主要障碍。现有的加速方法主要利用时间维度，例如在扩散时间段中重复使用缓存的特征。在这里，我们提出了区域自适应潜伏采样（RALU），这是一个无训练的框架，可加速沿空间维度的推理。 Ralu在三个阶段进行了混合分辨率抽样：1）低分辨率降低潜在扩散以有效捕获全球语义结构，2）在容易出现的特定区域，在特定区域上易于进行全分辨率，而3）所有潜在的潜伏在全分辨率上进行全分辨率，以详细介绍。为了稳定跨分辨率过渡的世代，我们利用重新安排的噪声刺激来适应各种分辨率的噪声水平。我们的方法可大大降低计算，同时通过达到7.0 $ \ times $速度的速度和3.0 $ \ times $的稳定扩散3的3.0 $ \ times $，并以最小的降级为单位。此外，Ralu与现有的时间加速度（例如缓存方法）相辅相成，因此可以无缝集成以进一步降低推理潜伏期而不会损害发电质量。</li>
</ul>

<h3>Title: RTNinja: a generalized machine learning framework for analyzing random telegraph noise signals in nanoelectronic devices</h3>
<ul>
<li><strong>Authors: </strong>Anirudh Varanasi, Robin Degraeve, Philippe Roussel, Clement Merckling</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08424">https://arxiv.org/abs/2507.08424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08424">https://arxiv.org/pdf/2507.08424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08424]] RTNinja: a generalized machine learning framework for analyzing random telegraph noise signals in nanoelectronic devices(https://arxiv.org/abs/2507.08424)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Random telegraph noise is a prevalent variability phenomenon in nanoelectronic devices, arising from stochastic carrier exchange at defect sites and critically impacting device reliability and performance. Conventional analysis techniques often rely on restrictive assumptions or manual interventions, limiting their applicability to complex, noisy datasets. Here, we introduce RTNinja, a generalized, fully automated machine learning framework for the unsupervised analysis of random telegraph noise signals. RTNinja deconvolves complex signals to identify the number and characteristics of hidden individual sources, without requiring prior knowledge of the system. The framework comprises two modular components: LevelsExtractor, which uses Bayesian inference and model selection to denoise and discretize the signal; and SourcesMapper, which infers source configurations through probabilistic clustering and optimization. To evaluate performance, we developed a Monte Carlo simulator that generates labeled datasets spanning broad signal-to-noise ratios and source complexities; across 7000 such datasets, RTNinja consistently demonstrated high-fidelity signal reconstruction and accurate extraction of source amplitudes and activity patterns. Our results demonstrate that RTNinja offers a robust, scalable, and device-agnostic tool for random telegraph noise characterization, enabling large-scale statistical benchmarking, reliability-centric technology qualification, predictive failure modeling, and device physics exploration in next-generation nanoelectronics.</li>
<li><strong>摘要：</strong>随机电报噪声是纳米电子设备中普遍存在的可变性现象，是由缺陷站点的随机载体交换引起的，并且对设备的可靠性和性能产生了严重影响。常规分析技术通常依赖于限制性假设或手动干预措施，从而将其适用性限制在复杂的嘈杂数据集中。在这里，我们介绍了Rtninja，这是一个通用的，全自动的机器学习框架，用于无监督的随机电报噪声信号分析。 rtninja deconvolves复杂信号以识别隐藏单个来源的数量和特征，而无需对系统的先验知识。该框架包括两个模块化组件：LevelSextractor，它使用贝叶斯推理和模型选择来降级和离散信号；和sourcesMapper，它通过概率聚类和优化来扩展源配置。为了评估性能，我们开发了一个蒙特卡洛模拟器，该模拟器生成横跨信噪比和源复杂性的标记数据集；在7000个这样的数据集中，Rtninja始终证明了高保真信号重建和准确提取源幅度和活动模式。我们的结果表明，Rtninja提供了一种可靠，可扩展的和设备的不合理工具，用于随机的电报噪声表征，实现大规模的统计基准测试，以可靠性为中心的技术资格，预测性故障建模和设备物理探索，在下一代纳米电子学中。</li>
</ul>

<h3>Title: Vision Foundation Models as Effective Visual Tokenizers for Autoregressive Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Anlin Zheng, Xin Wen, Xuanyang Zhang, Chuofan Ma, Tiancai Wang, Gang Yu, Xiangyu Zhang, Xiaojuan Qi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08441">https://arxiv.org/abs/2507.08441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08441">https://arxiv.org/pdf/2507.08441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08441]] Vision Foundation Models as Effective Visual Tokenizers for Autoregressive Image Generation(https://arxiv.org/abs/2507.08441)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Leveraging the powerful representations of pre-trained vision foundation models -- traditionally used for visual comprehension -- we explore a novel direction: building an image tokenizer directly atop such models, a largely underexplored area. Specifically, we employ a frozen vision foundation model as the encoder of our tokenizer. To enhance its effectiveness, we introduce two key components: (1) a region-adaptive quantization framework that reduces redundancy in the pre-trained features on regular 2D grids, and (2) a semantic reconstruction objective that aligns the tokenizer's outputs with the foundation model's representations to preserve semantic fidelity. Based on these designs, our proposed image tokenizer, VFMTok, achieves substantial improvements in image reconstruction and generation quality, while also enhancing token efficiency. It further boosts autoregressive (AR) generation -- achieving a gFID of 2.07 on ImageNet benchmarks, while accelerating model convergence by three times, and enabling high-fidelity class-conditional synthesis without the need for classifier-free guidance (CFG). The code will be released publicly to benefit the community.</li>
<li><strong>摘要：</strong>利用预训练的视觉基础模型（传统上用于视觉理解）的强大表示形式 - 我们探索了一个新颖的方向：直接在此类模型上建立图像令牌，这是一个很大程度上未经置换的区域。具体而言，我们采用冷冻视觉基础模型作为令牌的编码器。为了提高其有效性，我们介绍了两个关键组成部分：（1）一个区域自适应量化框架，可在常规2D网格上降低预训练的特征的冗余，以及（2）语义重建目标，将标记器的输出与基础模型的表示形式保持一致，以保存Senicantic Fidelity。基于这些设计，我们提出的图像令牌VFMTOK在图像重建和发电质量方面取得了重大改进，同时也提高了令牌效率。它进一步增强了自回旋（AR）生成 - 在成像网基准测试上达到2.07的GFID，同时将模型收敛加速3次，并启用高保真性班级条件合成的情况，而无需进行分类的无需分类器指导（CFG）。该代码将公开发布以使社区受益。</li>
</ul>

<h3>Title: KGRAG-Ex: Explainable Retrieval-Augmented Generation with Knowledge Graph-based Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Georgios Balanos, Evangelos Chasanis, Konstantinos Skianis, Evaggelia Pitoura</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08443">https://arxiv.org/abs/2507.08443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08443">https://arxiv.org/pdf/2507.08443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08443]] KGRAG-Ex: Explainable Retrieval-Augmented Generation with Knowledge Graph-based Perturbations(https://arxiv.org/abs/2507.08443)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) enhances language models by grounding responses in external information, yet explainability remains a critical challenge, particularly when retrieval relies on unstructured text. Knowledge graphs (KGs) offer a solution by introducing structured, semantically rich representations of entities and their relationships, enabling transparent retrieval paths and interpretable reasoning. In this work, we present KGRAG-Ex, a RAG system that improves both factual grounding and explainability by leveraging a domain-specific KG constructed via prompt-based information extraction. Given a user query, KGRAG-Ex identifies relevant entities and semantic paths in the graph, which are then transformed into pseudo-paragraphs: natural language representations of graph substructures that guide corpus retrieval. To improve interpretability and support reasoning transparency, we incorporate perturbation-based explanation methods that assess the influence of specific KG-derived components on the generated answers. We conduct a series of experiments to analyze the sensitivity of the system to different perturbation methods, the relationship between graph component importance and their structural positions, the influence of semantic node types, and how graph metrics correspond to the influence of components within the explanations process.</li>
<li><strong>摘要：</strong>检索演示的生成（RAG）通过将外部信息中的响应接地来增强语言模型，但是解释性仍然是一个关键的挑战，尤其是当检索依赖于非结构化的文本时。知识图（kgs）通过引入实体及其关系的结构化，语义丰富的表示，从而实现透明的检索路径和可解释的推理来提供解决方案。在这项工作中，我们提出了Kgrag-ex，这是一个抹布系统，通过利用通过及时的信息提取构建的域特异性kg来改善事实基础和解释性。给定用户查询，Kgrag-ex标识了图中的相关实体和语义路径，然后将其转换为伪段落：指导语料库检索的图形子结构的自然语言表示。为了提高解释性和支持推理透明度，我们合并了基于扰动的解释方法，以评估特定KG衍生的组件对生成答案的影响。我们进行了一系列实验，以分析系统对不同扰动方法的敏感性，图形重要性及其结构位置之间的关系，语义节点类型的影响以及图形指标如何对应组件在解释过程中的影响。</li>
</ul>

<h3>Title: Evaluating SAE interpretability without explanations</h3>
<ul>
<li><strong>Authors: </strong>Gonçalo Paulo, Nora Belrose</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08473">https://arxiv.org/abs/2507.08473</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08473">https://arxiv.org/pdf/2507.08473</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08473]] Evaluating SAE interpretability without explanations(https://arxiv.org/abs/2507.08473)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Sparse autoencoders (SAEs) and transcoders have become important tools for machine learning interpretability. However, measuring how interpretable they are remains challenging, with weak consensus about which benchmarks to use. Most evaluation procedures start by producing a single-sentence explanation for each latent. These explanations are then evaluated based on how well they enable an LLM to predict the activation of a latent in new contexts. This method makes it difficult to disentangle the explanation generation and evaluation process from the actual interpretability of the latents discovered. In this work, we adapt existing methods to assess the interpretability of sparse coders, with the advantage that they do not require generating natural language explanations as an intermediate step. This enables a more direct and potentially standardized assessment of interpretability. Furthermore, we compare the scores produced by our interpretability metrics with human evaluations across similar tasks and varying setups, offering suggestions for the community on improving the evaluation of these techniques.</li>
<li><strong>摘要：</strong>稀疏的自动编码器（SAE）和转码器已成为机器学习解释性的重要工具。但是，衡量它们的可解释性仍然具有挑战性，并且在使用哪些基准测试的情况下达成了薄弱的共识。大多数评估程序首先为每种潜在的单词解释。然后，根据它们使LLM能够预测新环境中潜在的激活的能力如何评估这些解释。这种方法使得很难将解释生成和评估过程从发现的潜伏期的实际解释性中解脱出来。在这项工作中，我们调整了现有方法来评估稀疏编码者的解释性，其优势是它们不需要生成自然语言解释作为中间步骤。这使得对可解释性的更直接和潜在的标准化评估。此外，我们将我们的可解释性指标与跨类似任务和不同设置的人类评估所产生的分数进行了比较，为社区提供了有关改进这些技术评估的建议。</li>
</ul>

<h3>Title: SynBridge: Bridging Reaction States via Discrete Flow for Bidirectional Reaction Prediction</h3>
<ul>
<li><strong>Authors: </strong>Haitao Lin, Junjie Wang, Zhifeng Gao, Xiaohong Ji, Rong Zhu, Linfeng Zhang, Guolin Ke, Weinan E</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08475">https://arxiv.org/abs/2507.08475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08475">https://arxiv.org/pdf/2507.08475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08475]] SynBridge: Bridging Reaction States via Discrete Flow for Bidirectional Reaction Prediction(https://arxiv.org/abs/2507.08475)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The essence of a chemical reaction lies in the redistribution and reorganization of electrons, which is often manifested through electron transfer or the migration of electron pairs. These changes are inherently discrete and abrupt in the physical world, such as alterations in the charge states of atoms or the formation and breaking of chemical bonds. To model the transition of states, we propose SynBridge, a bidirectional flow-based generative model to achieve multi-task reaction prediction. By leveraging a graph-to-graph transformer network architecture and discrete flow bridges between any two discrete distributions, SynBridge captures bidirectional chemical transformations between graphs of reactants and products through the bonds' and atoms' discrete states. We further demonstrate the effectiveness of our method through extensive experiments on three benchmark datasets (USPTO-50K, USPTO-MIT, Pistachio), achieving state-of-the-art performance in both forward and retrosynthesis tasks. Our ablation studies and noise scheduling analysis reveal the benefits of structured diffusion over discrete spaces for reaction prediction.</li>
<li><strong>摘要：</strong>化学反应的本质在于电子的重新分布和重组，这通常通过电子传递或电子对的迁移来表现。这些变化本质上是离散的和在物理世界中突然突然的，例如原子电荷状态的改变或化学键的形成和破坏。为了建模状态的过渡，我们提出了Synbridge，Synbridge是一种基于双向流量的生成模型，以实现多任务反应预测。通过利用任何两个离散分布之间的图形变压器网络架构和离散的流桥，Synbridge通过键和原子的离散状态捕获反应物和产品图之间的双向化学变换。我们通过在三个基准数据集（USPTO-50K，USPTO-MIT，开心果）上进行广泛的实验，进一步证明了我们的方法的有效性，从而在远期和逆合合成任务中实现了最先进的性能。我们的消融研究和噪声调度分析揭示了结构化扩散比离散空间进行反应预测的好处。</li>
</ul>

<h3>Title: Efficient Deployment of Vision-Language Models on Mobile Devices: A Case Study on OnePlus 13R</h3>
<ul>
<li><strong>Authors: </strong>Pablo Robin Guerrero, Yueyang Pan, Sanidhya Kashyap</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08505">https://arxiv.org/abs/2507.08505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08505">https://arxiv.org/pdf/2507.08505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08505]] Efficient Deployment of Vision-Language Models on Mobile Devices: A Case Study on OnePlus 13R(https://arxiv.org/abs/2507.08505)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) offer promising capabilities for mobile devices, but their deployment faces significant challenges due to computational limitations and energy inefficiency, especially for real-time applications. This study provides a comprehensive survey of deployment frameworks for VLMs on mobile devices, evaluating this http URL, MLC-Imp, and mllm in the context of running LLaVA-1.5 7B, MobileVLM-3B, and Imp-v1.5 3B as representative workloads on a OnePlus 13R. Each deployment framework was evaluated on the OnePlus 13R while running VLMs, with measurements covering CPU, GPU, and NPU utilization, temperature, inference time, power consumption, and user experience. Benchmarking revealed critical performance bottlenecks across frameworks: CPU resources were consistently over-utilized during token generation, while GPU and NPU accelerators were largely unused. When the GPU was used, primarily for image feature extraction, it was saturated, leading to degraded device responsiveness. The study contributes framework-level benchmarks, practical profiling tools, and an in-depth analysis of hardware utilization bottlenecks, highlighting the consistent overuse of CPUs and the ineffective or unstable use of GPUs and NPUs in current deployment frameworks.</li>
<li><strong>摘要：</strong>视觉语言模型（VLM）为移动设备提供了有希望的功能，但是由于计算限制和能源效率低下，它们的部署面临着巨大的挑战，尤其是对于实时应用程序。这项研究提供了对移动设备上VLM的部署框架的全面调查，在运行LLAVA-1.5 7B，MobileVLM-3B和IMP-V1.5 3B的背景下，评估了此HTTP URL，MLC-IMP和MLLM，作为OnePlus 13R的代表工作。在运行VLM时，在OnePlus 13R上评估了每个部署框架，涵盖CPU，GPU和NPU利用率，温度，推理时间，功耗和用户体验的测量值。基准测试揭示了跨框架的关键性能瓶颈：在代币生成过程中，CPU资源始终被过度利用，而GPU和NPU加速器在很大程度上没有使用。当使用GPU（主要用于图像特征提取）时，它已饱和，导致设备响应降低。该研究贡献了框架级别的基准，实用的分析工具以及对硬件利用瓶颈的深入分析，突出了CPU的一致性过度使用以及在当前部署框架中对GPU和NPU的无效或不稳定使用。</li>
</ul>

<h3>Title: Recursive Reward Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Yuting Tang, Yivan Zhang, Johannes Ackermann, Yu-Jie Zhang, Soichiro Nishimori, Masashi Sugiyama</a></li>
<li><strong>Subjects: </strong>cs.LG, math.CT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08537">https://arxiv.org/abs/2507.08537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08537">https://arxiv.org/pdf/2507.08537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08537]] Recursive Reward Aggregation(https://arxiv.org/abs/2507.08537)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In reinforcement learning (RL), aligning agent behavior with specific objectives typically requires careful design of the reward function, which can be challenging when the desired objectives are complex. In this work, we propose an alternative approach for flexible behavior alignment that eliminates the need to modify the reward function by selecting appropriate reward aggregation functions. By introducing an algebraic perspective on Markov decision processes (MDPs), we show that the Bellman equations naturally emerge from the recursive generation and aggregation of rewards, allowing for the generalization of the standard discounted sum to other recursive aggregations, such as discounted max and Sharpe ratio. Our approach applies to both deterministic and stochastic settings and integrates seamlessly with value-based and actor-critic algorithms. Experimental results demonstrate that our approach effectively optimizes diverse objectives, highlighting its versatility and potential for real-world applications.</li>
<li><strong>摘要：</strong>在加强学习（RL）中，将代理行为与特定目标相结合，通常需要仔细设计奖励功能，当期望的目标复杂时，这可能会具有挑战性。在这项工作中，我们提出了一种用于灵活行为对准的替代方法，该方法消除了通过选择适当的奖励聚合功能来修改奖励功能的需求。通过对马尔可夫决策过程（MDP）介绍代数观点，我们表明，贝尔曼方程自然会从递归产生和奖励的汇总中出现，从而可以将标准折扣总和的概括为其他递归聚合，例如折现最大值和尖锐的比率。我们的方法适用于确定性和随机设置，并与基于价值和参与者的批评算法无缝集成。实验结果表明，我们的方法有效地优化了各种目标，突出了其多功能性和对现实应用程序的潜力。</li>
</ul>

<h3>Title: Image Translation with Kernel Prediction Networks for Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Cristina Mata, Michael S. Ryoo, Henrik Turbell</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08554">https://arxiv.org/abs/2507.08554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08554">https://arxiv.org/pdf/2507.08554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08554]] Image Translation with Kernel Prediction Networks for Semantic Segmentation(https://arxiv.org/abs/2507.08554)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Semantic segmentation relies on many dense pixel-wise annotations to achieve the best performance, but owing to the difficulty of obtaining accurate annotations for real world data, practitioners train on large-scale synthetic datasets. Unpaired image translation is one method used to address the ensuing domain gap by generating more realistic training data in low-data regimes. Current methods for unpaired image translation train generative adversarial networks (GANs) to perform the translation and enforce pixel-level semantic matching through cycle consistency. These methods do not guarantee that the semantic matching holds, posing a problem for semantic segmentation where performance is sensitive to noisy pixel labels. We propose a novel image translation method, Domain Adversarial Kernel Prediction Network (DA-KPN), that guarantees semantic matching between the synthetic label and translation. DA-KPN estimates pixel-wise input transformation parameters of a lightweight and simple translation function. To ensure the pixel-wise transformation is realistic, DA-KPN uses multi-scale discriminators to distinguish between translated and target samples. We show DA-KPN outperforms previous GAN-based methods on syn2real benchmarks for semantic segmentation with limited access to real image labels and achieves comparable performance on face parsing.</li>
<li><strong>摘要：</strong>语义细分依赖于许多密集的像素注释来实现最佳性能，但是由于难以获得对现实世界数据的准确注释，从业者对大型合成数据集进行了培训。未配对的图像翻译是一种方法，用于通过在低数据制度中生成更现实的训练数据来解决随后的域间隙。未配对的图像翻译火车生成对抗网络（GAN）的当前方法可以通过周期一致性执行翻译和执行像素级的语义匹配。这些方法不能保证语义匹配成立，这对性能对嘈杂的像素标签敏感的语义分割提出了问题。我们提出了一种新颖的图像翻译方法，即域对抗内核预测网络（DA-KPN），可确保合成标签和翻译之间的语义匹配。 DA-KPN估计轻巧和简单的翻译功能的像素输入转换参数。为了确保像素的转换是现实的，DA-KPN使用多尺度歧视器来区分翻译和目标样本。我们显示DA-KPN在Syn2Real基准测试基准的语义分割基准上的先前基于GAN的方法，对真实图像标签的访问有限，并且在面部解析上实现了可比的性能。</li>
</ul>

<h3>Title: Remote Sensing Reveals Adoption of Sustainable Rice Farming Practices Across Punjab, India</h3>
<ul>
<li><strong>Authors: </strong>Ando Shah, Rajveer Singh, Akram Zaytar, Girmaw Abebe Tadesse, Caleb Robinson, Negar Tafti, Stephen A. Wood, Rahul Dodhia, Juan M. Lavista Ferres</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08605">https://arxiv.org/abs/2507.08605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08605">https://arxiv.org/pdf/2507.08605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08605]] Remote Sensing Reveals Adoption of Sustainable Rice Farming Practices Across Punjab, India(https://arxiv.org/abs/2507.08605)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Rice cultivation consumes 24-30% of global freshwater, creating critical water management challenges in major rice-producing regions. Sustainable irrigation practices like direct seeded rice (DSR) and alternate wetting and drying (AWD) can reduce water use by 20-40% while maintaining yields, helping secure long-term agricultural productivity as water scarcity intensifies - a key component of the Zero Hunger Sustainable Development Goal. However, limited data on adoption rates of these practices prevents evidence-based policymaking and targeted resource allocation. We developed a novel remote sensing framework to monitor sustainable water management practices at scale in Punjab, India - a region facing severe groundwater depletion of 41.6 cm/year. To collect essential ground truth data, we partnered with the Nature Conservancy's Promoting Regenerative and No-burn Agriculture (PRANA) program, which trained approximately 1,400 farmers on water-saving techniques while documenting their field-level practices. Using this data, we created a classification system with Sentinel-1 satellite imagery that separates water management along sowing and irrigation dimensions. Our approach achieved a 78% F1-score in distinguishing DSR from traditional puddled transplanted rice without requiring prior knowledge of planting dates. We demonstrated scalability by mapping DSR adoption across approximately 3 million agricultural plots in Punjab, with district-level predictions showing strong correlation (Pearson=0.77, RBO= 0.77) with government records. This study provides policymakers with a powerful tool to track sustainable water management adoption, target interventions, and measure program impacts at scale.</li>
<li><strong>摘要：</strong>水稻种植消耗了全球淡水的24-30％，在主要的水稻生产地区引起了关键的水管理挑战。可持续的灌溉习惯，例如直接种子大米（DSR）和替代润湿和干燥（AWD）可以将用水量减少20-40％，同时保持产量，从而帮助确保长期农业生产力，因为水稀缺性会加剧 - 零饥饿可持续发展目标的关键组成部分。但是，这些实践采用率的数据有限，可以阻止循证决策和针对资源分配。我们开发了一个新型的遥感框架，以监视印度旁遮普邦的大规模可持续水管理实践 - 该地区面临41.6 cm/年的严重地下水耗竭。为了收集基本的基础真相数据，我们与自然保护协会的促进和无燃烧的农业（PRANA）计划合作，该计划在记录其现场级别的实践时培训了大约1,400名农民的储蓄技术。使用这些数据，我们使用Sentinel-1卫星图像创建了一个分类系统，该系统将水管理分开沿播种和灌溉尺寸分开。我们的方法在区分DSR和传统水坑移植大米的情况下实现了78％的F1得分，而无需先验知识播种日期。我们通过在旁遮普邦大约300万个农业土地上绘制了DSR采用来证明可伸缩性，区域级的预测显示出强烈的相关性（Pearson = 0.77，RBO = 0.77）与政府记录。这项研究为决策者提供了一种强大的工具，可以跟踪可持续的水管理采用，目标干预措施和衡量计划的影响。</li>
</ul>

<h3>Title: Emergent Natural Language with Communication Games for Improving Image Captioning Capabilities without Additional Data</h3>
<ul>
<li><strong>Authors: </strong>Parag Dutta, Ambedkar Dukkipati</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08610">https://arxiv.org/abs/2507.08610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08610">https://arxiv.org/pdf/2507.08610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08610]] Emergent Natural Language with Communication Games for Improving Image Captioning Capabilities without Additional Data(https://arxiv.org/abs/2507.08610)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Image captioning is an important problem in developing various AI systems, and these tasks require large volumes of annotated images to train the models. Since all existing labelled datasets are already used for training the large Vision Language Models (VLMs), it becomes challenging to improve the performance of the same. Considering this, it is essential to consider the unsupervised image captioning performance, which remains relatively under-explored. To that end, we propose LoGIC (Lewis Communication Game for Image Captioning), a Multi-agent Reinforcement Learning game. The proposed method consists of two agents, a 'speaker' and a 'listener', with the objective of learning a strategy for communicating in natural language. We train agents in the cooperative common-reward setting using the GRPO algorithm and show that improvement in image captioning performance emerges as a consequence of the agents learning to play the game. We show that using pre-trained VLMs as the 'speaker' and Large Language Model (LLM) for language understanding in the 'listener', we achieved a $46$ BLEU score after fine-tuning using LoGIC without additional labels, a $2$ units advantage in absolute metrics compared to the $44$ BLEU score of the vanilla VLM. Additionally, we replace the VLM from the 'speaker' with lightweight components: (i) a ViT for image perception and (ii) a GPT2 language generation, and train them from scratch using LoGIC, obtaining a $31$ BLEU score in the unsupervised setting, a $10$ points advantage over existing unsupervised image-captioning methods.</li>
<li><strong>摘要：</strong>图像字幕是开发各种AI系统的重要问题，这些任务需要大量注释的图像来训练模型。由于所有现有标记的数据集都已经用于培训大型视觉语言模型（VLM），因此提高同一性能的性能变得具有挑战性。考虑到这一点，必须考虑无监督的图像字幕性能，这仍然相对爆发。为此，我们提出了逻辑（图像字幕的刘易斯通信游戏），这是一种多代理增强学习游戏。提出的方法由两个代理，一个“说话者”和“听众”组成，目的是学习一种以自然语言交流的策略。我们使用GRPO算法在合作的共同奖励环境中训练代理商，并表明图像字幕性能的改进是由于代理商学习玩游戏而出现的。我们表明，使用预先训练的VLM作为“扬声器”和大型语言模型（LLM），以在“听众”中进行语言理解，我们在使用逻辑的微调后获得了$ 46 $ bleu得分，而没有其他标签，与$ 44 $ bleu bleu bleu bleu bleu bleu bleu bleu bleu vlm vlm相比，$ 2 $单位的优势。此外，我们用轻巧的组件代替了“扬声器”中的VLM：（i）图像感知和（ii）GPT2语言生成的VIT，并使用逻辑从头开始对其进行训练，在无用的设置中获得了$ 31 $ BLEU的得分，比现有的无耐心图像拟合图像$ 10 $的优势获得了$ 10 $的优势。</li>
</ul>

<h3>Title: DatasetAgent: A Novel Multi-Agent System for Auto-Constructing Datasets from Real-World Images</h3>
<ul>
<li><strong>Authors: </strong>Haoran Sun, Haoyu Bian, Shaoning Zeng, Yunbo Rao, Xu Xu, Lin Mei, Jianping Gou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08648">https://arxiv.org/abs/2507.08648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08648">https://arxiv.org/pdf/2507.08648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08648]] DatasetAgent: A Novel Multi-Agent System for Auto-Constructing Datasets from Real-World Images(https://arxiv.org/abs/2507.08648)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Common knowledge indicates that the process of constructing image datasets usually depends on the time-intensive and inefficient method of manual collection and annotation. Large models offer a solution via data generation. Nonetheless, real-world data are obviously more valuable comparing to artificially intelligence generated data, particularly in constructing image datasets. For this reason, we propose a novel method for auto-constructing datasets from real-world images by a multiagent collaborative system, named as DatasetAgent. By coordinating four different agents equipped with Multi-modal Large Language Models (MLLMs), as well as a tool package for image optimization, DatasetAgent is able to construct high-quality image datasets according to user-specified requirements. In particular, two types of experiments are conducted, including expanding existing datasets and creating new ones from scratch, on a variety of open-source datasets. In both cases, multiple image datasets constructed by DatasetAgent are used to train various vision models for image classification, object detection, and image segmentation.</li>
<li><strong>摘要：</strong>常识表明，构造图像数据集的过程通常取决于手动收集和注释的时间密集型和效率低下的方法。大型模型通过数据生成提供了解决方案。尽管如此，与人工智能生成的数据相比，现实世界中的数据显然更有价值，尤其是在构建图像数据集时。因此，我们提出了一种新的方法，用于通过多种合作系统（称为dataSetagent）从现实世界图像中自动构造数据集。通过协调配备多模式大语言模型（MLLM）的四个不同的代理，以及用于图像优化的工具包，数据集成能够根据用户指定的要求构建高质量的图像数据集。特别是，进行了两种类型的实验，包括在各种开源数据集上扩展现有数据集并从头开始创建新数据集。在这两种情况下，数据集体构建的多个图像数据集都用于训练各种视觉模型，以进行图像分类，对象检测和图像分割。</li>
</ul>

<h3>Title: Domain-Informed Operation Excellence of Gas Turbine System with Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Waqar Muhammad Ashraf, Amir H. Keshavarzzadeh, Abdulelah S. Alshehri, Abdulrahman bin Jumah, Ramit Debnath, Vivek Dua</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08697">https://arxiv.org/abs/2507.08697</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08697">https://arxiv.org/pdf/2507.08697</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08697]] Domain-Informed Operation Excellence of Gas Turbine System with Machine Learning(https://arxiv.org/abs/2507.08697)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The domain-consistent adoption of artificial intelligence (AI) remains low in thermal power plants due to the black-box nature of AI algorithms and low representation of domain knowledge in conventional data-centric analytics. In this paper, we develop a MAhalanobis Distance-based OPTimization (MAD-OPT) framework that incorporates the Mahalanobis distance-based constraint to introduce domain knowledge into data-centric analytics. The developed MAD-OPT framework is applied to maximize thermal efficiency and minimize turbine heat rate for a 395 MW capacity gas turbine system. We demonstrate that the MAD-OPT framework can estimate domain-informed optimal process conditions under different ambient conditions, and the optimal solutions are found to be robust as evaluated by Monte Carlo simulations. We also apply the MAD-OPT framework to estimate optimal process conditions beyond the design power generation limit of the gas turbine system, and have found comparable results with the actual data of the power plant. We demonstrate that implementing data-centric optimization analytics without incorporating domain-informed constraints may provide ineffective solutions that may not be implementable in the real operation of the gas turbine system. This research advances the integration of the data-driven domain knowledge into machine learning-powered analytics that enhances the domain-informed operation excellence and paves the way for safe AI adoption in thermal power systems.</li>
<li><strong>摘要：</strong>由于AI算法的黑盒性质和在常规数据以数据为中心的分析中，由于域的一致性采用人工智能（AI）在热电厂中的采用仍然很低。在本文中，我们开发了一个基于玛哈拉的距离优化（MAD-OPT）框架，该框架结合了基于Mahalanobis距离的约束，以将域知识引入以数据为中心的分析中。开发的MAD-OPT框架可用于最大程度地提高热效率并最大程度地减少395 MW容量燃气轮机系统的涡轮热速率。我们证明，MAD-OPT框架可以在不同的环境条件下估算域信息最佳过程条件，并且发现最佳解决方案是通过Monte Carlo Simulates评估的最佳解决方案。我们还将MAD-OPT框架应用于燃气轮机系统的设计发电限制之外的最佳过程条件，并发现了与电厂的实际数据相当的结果。我们证明，实施以数据为中心的优化分析而不合并域信息约束可能会提供无效的解决方案，这些解决方案可能无法在燃气轮机系统的实际操作中实现。这项研究将数据驱动的域知识的整合到机器学习驱动的分析中，从而增强了卓越领域的操作，并为在热力系统中的安全AI采用铺平了道路。</li>
</ul>

<h3>Title: From One to More: Contextual Part Latents for 3D Generation</h3>
<ul>
<li><strong>Authors: </strong>Shaocong Dong, Lihe Ding, Xiao Chen, Yaokun Li, Yuxin Wang, Yucheng Wang, Qi Wang, Jaehyeok Kim, Chenjian Gao, Zhanpeng Huang, Zibin Wang, Tianfan Xue, Dan Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08772">https://arxiv.org/abs/2507.08772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08772">https://arxiv.org/pdf/2507.08772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08772]] From One to More: Contextual Part Latents for 3D Generation(https://arxiv.org/abs/2507.08772)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in 3D generation have transitioned from multi-view 2D rendering approaches to 3D-native latent diffusion frameworks that exploit geometric priors in ground truth data. Despite progress, three key limitations persist: (1) Single-latent representations fail to capture complex multi-part geometries, causing detail degradation; (2) Holistic latent coding neglects part independence and interrelationships critical for compositional design; (3) Global conditioning mechanisms lack fine-grained controllability. Inspired by human 3D design workflows, we propose CoPart - a part-aware diffusion framework that decomposes 3D objects into contextual part latents for coherent multi-part generation. This paradigm offers three advantages: i) Reduces encoding complexity through part decomposition; ii) Enables explicit part relationship modeling; iii) Supports part-level conditioning. We further develop a mutual guidance strategy to fine-tune pre-trained diffusion models for joint part latent denoising, ensuring both geometric coherence and foundation model priors. To enable large-scale training, we construct Partverse - a novel 3D part dataset derived from Objaverse through automated mesh segmentation and human-verified annotations. Extensive experiments demonstrate CoPart's superior capabilities in part-level editing, articulated object generation, and scene composition with unprecedented controllability.</li>
<li><strong>摘要：</strong>3D代的最新进展已从多视图2D渲染方法转变为3D本地潜在扩散框架，这些框架在地面真相数据中利用了几何学先验。尽管进步，但三个关键局限性仍然存在：（1）单纬度表示未能捕获复杂的多部分几何形状，从而导致细节降解； （2）整体潜在编码忽略了零件独立性和对组成设计至关重要的相互关系； （3）全球条件机制缺乏细粒度的可控性。受到人类3D设计工作流程的启发，我们提出了Copart，这是一个部分感知的扩散框架，将3D对象分解为连贯的多部分生成的上下文零件潜伏期。该范式提供了三个优点：i）通过部分分解来降低编码复杂性； ii）启用明确的零件关系建模； iii）支持零件级调节。我们进一步制定了一种相互的指导策略，以微调联合部分潜在降解的预训练扩散模型，从而确保几何连贯性和基础模型先验。为了实现大规模训练，我们构建了副培训 - 一种新颖的3D部分数据集，该数据集源自Objaverse，通过自动网格分割和人为验证的注释。广泛的实验证明了Copart在零件级编辑，铰接对象产生和场景组成中具有前所未有的可控性的优势功能。</li>
</ul>

<h3>Title: One Token to Fool LLM-as-a-Judge</h3>
<ul>
<li><strong>Authors: </strong>Yulai Zhao, Haolin Liu, Dian Yu, S.Y. Kung, Haitao Mi, Dong Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08794">https://arxiv.org/abs/2507.08794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08794">https://arxiv.org/pdf/2507.08794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08794]] One Token to Fool LLM-as-a-Judge(https://arxiv.org/abs/2507.08794)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative reward models (also known as LLMs-as-judges), which use large language models (LLMs) to evaluate answer quality, are increasingly adopted in reinforcement learning with verifiable rewards (RLVR). They are often preferred over rigid rule-based metrics, especially for complex reasoning tasks involving free-form outputs. In this paradigm, an LLM is typically prompted to compare a candidate answer against a ground-truth reference and assign a binary reward indicating correctness. Despite the seeming simplicity of this comparison task, we find that generative reward models exhibit surprising vulnerabilities to superficial manipulations: non-word symbols (e.g., ":" or ".") or reasoning openers like "Thought process:" and "Let's solve this problem step by step." can often lead to false positive rewards. We demonstrate that this weakness is widespread across LLMs, datasets, and prompt formats, posing a serious threat for core algorithmic paradigms that rely on generative reward models, such as rejection sampling, preference optimization, and RLVR. To mitigate this issue, we introduce a simple yet effective data augmentation strategy and train a new generative reward model with substantially improved robustness. Our findings highlight the urgent need for more reliable LLM-based evaluation methods. We release our robust, general-domain reward model and its synthetic training data at this https URL and this https URL.</li>
<li><strong>摘要：</strong>使用大型语言模型（LLMS）评估答案质量的生成奖励模型（也称为LLMS-AS-judges）在具有可验证的奖励（RLVR）的增强学习中越来越多地采用。它们通常比基于严格的规则指标更喜欢，尤其是对于涉及自由形式产出的复杂推理任务。在此范式中，通常会提示LLM将候选人答案与地面确实参考进行比较，并分配指示正确性的二进制奖励。尽管这项比较任务似乎很简单，但我们发现，生成奖励模型对表面操纵表现出令人惊讶的漏洞：非单词符号（例如，“：”或“或”。）或像“思考过程”之类的推理启动器：逐步解决此问题。”通常会导致假阳性奖励。我们证明，这种弱点在LLM，数据集和及时格式中广泛存在，对依赖生成奖励模型的核心算法范式构成了严重的威胁，例如拒绝采样，偏好优化和RLVR。为了减轻此问题，我们介绍了一种简单而有效的数据增强策略，并培训了一种具有大大改善的鲁棒性的新生成奖励模型。我们的发现突出了迫切需要基于LLM的更可靠的评估方法。我们在此HTTPS URL和此HTTPS URL上发布了强大的通用域奖励模型及其合成训练数据。</li>
</ul>

<h3>Title: NeuralOS: Towards Simulating Operating Systems via Neural Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Luke Rivard, Sun Sun, Hongyu Guo, Wenhu Chen, Yuntian Deng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08800">https://arxiv.org/abs/2507.08800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08800">https://arxiv.org/pdf/2507.08800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08800]] NeuralOS: Towards Simulating Operating Systems via Neural Generative Models(https://arxiv.org/abs/2507.08800)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce NeuralOS, a neural framework that simulates graphical user interfaces (GUIs) of operating systems by directly predicting screen frames in response to user inputs such as mouse movements, clicks, and keyboard events. NeuralOS combines a recurrent neural network (RNN), which tracks computer state, with a diffusion-based neural renderer that generates screen images. The model is trained on a large-scale dataset of Ubuntu XFCE recordings, which include both randomly generated interactions and realistic interactions produced by AI agents. Experiments show that NeuralOS successfully renders realistic GUI sequences, accurately captures mouse interactions, and reliably predicts state transitions like application launches. Although modeling fine-grained keyboard interactions precisely remains challenging, NeuralOS offers a step toward creating fully adaptive, generative neural interfaces for future human-computer interaction systems.</li>
<li><strong>摘要：</strong>我们介绍了神经元，这是一个神经框架，通过直接预测屏幕帧来响应用户输入（例如鼠标移动，点击和键盘事件）来模拟操作系统的图形用户界面（GUI）。 Neuralos结合了跟踪计算机状态的复发性神经网络（RNN），以及生成屏幕图像的基于扩散的神经渲染器。该模型在Ubuntu XFCE记录的大规模数据集上进行了训练，其中包括随机生成的交互和AI代理产生的现实交互。实验表明，神经系统成功地呈现了现实的GUI序列，可以准确捕获鼠标的相互作用，并可靠地预测了状态过渡，例如应用程序启动。尽管对细粒键盘相互作用进行建模精确挑战，但Neuralos为为未来的人类计算机相互作用系统创建完全适应性的，生成的神经界面提供了一步。</li>
</ul>

<h3>Title: Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective</h3>
<ul>
<li><strong>Authors: </strong>Hangjie Yuan, Weihua Chen, Jun Cen, Hu Yu, Jingyun Liang, Shuning Chang, Zhihui Lin, Tao Feng, Pengwei Liu, Jiazheng Xing, Hao Luo, Jiasheng Tang, Fan Wang, Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08801">https://arxiv.org/abs/2507.08801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08801">https://arxiv.org/pdf/2507.08801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08801]] Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective(https://arxiv.org/abs/2507.08801)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Autoregressive large language models (LLMs) have unified a vast range of language tasks, inspiring preliminary efforts in autoregressive video generation. Existing autoregressive video generators either diverge from standard LLM architectures, depend on bulky external text encoders, or incur prohibitive latency due to next-token decoding. In this paper, we introduce Lumos-1, an autoregressive video generator that retains the LLM architecture with minimal architectural modifications. To inject spatiotemporal correlations in LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its imbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE scheme that preserves the original textual RoPE while providing comprehensive frequency spectra and scaled 3D positions for modeling multimodal spatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy that obeys intra-frame bidirectionality and inter-frame temporal causality. Based on this dependency strategy, we identify the issue of frame-wise loss imbalance caused by spatial information redundancy and solve it by proposing Autoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal tube masking during training with a compatible inference-time masking policy to avoid quality degradation. By using memory-efficient training techniques, we pre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on GenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code and models are available at this https URL.</li>
<li><strong>摘要：</strong>自回归的大型语言模型（LLM）统一了各种各样的语言任务，激发了自回归视频生成的初步努力。现有的自动回归视频发电机要么与标准LLM架构发散，要么取决于笨重的外部文本编码器，要么由于下一步的解码而产生的延迟。在本文中，我们介绍了Lumos-1，这是一种自动回归视频生成器，该视频生成器保留了LLM架构，并具有最小的体系结构修改。为了注入LLMS的时空相关性，我们确定了结合3D绳索并诊断其不平衡频谱范围的功效。因此，我们提出了MM Rope，这是一种绳索方案，可以保留原始文本绳索，同时提供全面的频谱，并缩放3D位置，以建模多模式时空数据。此外，Lumos-1求助于遵守框架内双向性和框架间的时间因果关系的令牌依赖策略。基于这种依赖性策略，我们确定了由空间信息冗余引起的框架损失不平衡问题的问题，并通过提出自回归离散扩散强迫（AR-DF）来解决它。 AR-DF通过兼容推理时间掩盖策略在训练过程中引入时间管掩蔽，以避免质量降解。通过使用记忆效率的训练技术，我们仅在48 GPU上预先训练Lumos-1，可在geneval，vbench-i2v上的cosmos-video2world上获得与EMU3相当的性能，而VBENCH-T2V上的opensoraplan cosmos-video2world。代码和型号可在此HTTPS URL上找到。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
