<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-08-28</h1>
<h3>Title: Real-Time Intuitive AI Drawing System for Collaboration: Enhancing Human Creativity through Formal and Contextual Intent Integration</h3>
<ul>
<li><strong>Authors: </strong>Jookyung Song, Mookyoung Kang, Nojun Kwak</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19254">https://arxiv.org/abs/2508.19254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19254">https://arxiv.org/pdf/2508.19254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19254]] Real-Time Intuitive AI Drawing System for Collaboration: Enhancing Human Creativity through Formal and Contextual Intent Integration(https://arxiv.org/abs/2508.19254)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>This paper presents a real-time generative drawing system that interprets and integrates both formal intent - the structural, compositional, and stylistic attributes of a sketch - and contextual intent - the semantic and thematic meaning inferred from its visual content - into a unified transformation process. Unlike conventional text-prompt-based generative systems, which primarily capture high-level contextual descriptions, our approach simultaneously analyzes ground-level intuitive geometric features such as line trajectories, proportions, and spatial arrangement, and high-level semantic cues extracted via vision-language models. These dual intent signals are jointly conditioned in a multi-stage generation pipeline that combines contour-preserving structural control with style- and content-aware image synthesis. Implemented with a touchscreen-based interface and distributed inference architecture, the system achieves low-latency, two-stage transformation while supporting multi-user collaboration on shared canvases. The resulting platform enables participants, regardless of artistic expertise, to engage in synchronous, co-authored visual creation, redefining human-AI interaction as a process of co-creation and mutual enhancement.</li>
<li><strong>摘要：</strong>本文提出了一个实时生成绘图系统，该系统将正式意图解释和集成 - 草图的结构，组成和风格属性 - 和上下文意图 - 从其视觉内容推断出的语义和主题含义 - 纳入统一的转换过程。与主要捕获高级上下文描述的常规基于文本促进的生成系统不同，我们的方法同时分析地面级直觉几何特征，例如线条轨迹，比例和空间布置，以及通过视觉语言模型提取的高级语义提示。这些双重意图信号在多阶段的一条管道中共同调节，该管道将具有轮廓的结构控制与样式和内容感知的图像合成结合在一起。该系统采用基于触摸屏的接口和分布式推理体系结构实现，可实现低延迟，两阶段的转换，同时支持共享画布上的多用户协作。最终的平台使参与者（无论艺术专业知识如何）从事同步的，共同撰写的视觉创作，重新定义人类的相互作用作为共同创造和相互增强的过程。</li>
</ul>

<h3>Title: Seeing Like a Designer Without One: A Study on Unsupervised Slide Quality Assessment via Designer Cue Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Tai Inui, Steven Oh, Magdeline Kuan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19289">https://arxiv.org/abs/2508.19289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19289">https://arxiv.org/pdf/2508.19289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19289]] Seeing Like a Designer Without One: A Study on Unsupervised Slide Quality Assessment via Designer Cue Augmentation(https://arxiv.org/abs/2508.19289)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>We present an unsupervised slide-quality assessment pipeline that combines seven expert-inspired visual-design metrics (whitespace, colorfulness, edge density, brightness contrast, text density, color harmony, layout balance) with CLIP-ViT embeddings, using Isolation Forest-based anomaly scoring to evaluate presentation slides. Trained on 12k professional lecture slides and evaluated on six academic talks (115 slides), our method achieved Pearson correlations up to 0.83 with human visual-quality ratings-1.79x to 3.23x stronger than scores from leading vision-language models (ChatGPT o4-mini-high, ChatGPT o3, Claude Sonnet 4, Gemini 2.5 Pro). We demonstrate convergent validity with visual ratings, discriminant validity against speaker-delivery scores, and exploratory alignment with overall impressions. Our results show that augmenting low-level design cues with multimodal embeddings closely approximates audience perceptions of slide quality, enabling scalable, objective feedback in real time.</li>
<li><strong>摘要：</strong>我们提出了一条无监督的幻灯片评估管道，该管道结合了七个专家启发的视觉设计指标（空间，五颜六色，边缘密度，亮度对比，文本密度，文本密度，色彩和谐，布局平衡）与剪贴画的嵌入，并使用隔离的森林基于森林的基于基于森林的疗程分数来评估呈现呈现呈现。在12K专业演讲幻灯片上接受培训，并在六次学术演讲（115个幻灯片）上进行了评估，我们的方法实现了Pearson的相关性，最高为0.83，人类视觉质量评分1.79 x至3.23倍，比领先的视觉模型（Chatgpt O4-Mini-high，Chatgpt O3，Chatgpt O3，Claude Sonnet 4，Claude Sonnet 4，Gempemini 2.5 proper）高。我们证明了视觉评分的收敛性，对说话者传递评分的判别有效性以及与整体印象的探索性一致性。我们的结果表明，使用多模式嵌入的低水平设计提示紧密近似于受众对滑梯质量的看法，从而实现可扩展的客观反馈。</li>
</ul>

<h3>Title: Efficient Model-Based Purification Against Adversarial Attacks for LiDAR Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Alexandros Gkillas, Ioulia Kapsali, Nikos Piperigkos, Aris S. Lalos</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19290">https://arxiv.org/abs/2508.19290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19290">https://arxiv.org/pdf/2508.19290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19290]] Efficient Model-Based Purification Against Adversarial Attacks for LiDAR Segmentation(https://arxiv.org/abs/2508.19290)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>LiDAR-based segmentation is essential for reliable perception in autonomous vehicles, yet modern segmentation networks are highly susceptible to adversarial attacks that can compromise safety. Most existing defenses are designed for networks operating directly on raw 3D point clouds and rely on large, computationally intensive generative models. However, many state-of-the-art LiDAR segmentation pipelines operate on more efficient 2D range view representations. Despite their widespread adoption, dedicated lightweight adversarial defenses for this domain remain largely unexplored. We introduce an efficient model-based purification framework tailored for adversarial defense in 2D range-view LiDAR segmentation. We propose a direct attack formulation in the range-view domain and develop an explainable purification network based on a mathematical justified optimization problem, achieving strong adversarial resilience with minimal computational overhead. Our method achieves competitive performance on open benchmarks, consistently outperforming generative and adversarial training baselines. More importantly, real-world deployment on a demo vehicle demonstrates the framework's ability to deliver accurate operation in practical autonomous driving scenarios.</li>
<li><strong>摘要：</strong>基于激光雷达的分段对于自动驾驶汽车的可靠感知至关重要，但是现代的分割网络极易受到可能损害安全性的对抗性攻击。大多数现有防御措施都是为直接在RAW 3D点云上运行的网络而设计的，并依靠大型计算密集的生成模型。但是，许多最先进的激光雷达分割管道在更有效的2D范围视图表示上运行。尽管采用了广泛的采用，但该领域的专用轻量级对抗防御仍然在很大程度上没有探索。我们引入了一个高效的基于模型的纯化框架，该框架是针对2D范围视图激光雷达分段的对抗防御的。我们在范围视图域中提出了直接的攻击公式，并基于数学合理的优化问题开发可解释的纯化网络，从而实现了强大的对抗性弹性，并以最小的计算开销。我们的方法在开放基准上实现了竞争性能，始终优于生成和对抗性训练基线。更重要的是，在演示车辆上的现实世界部署展示了该框架在实际自动驾驶场景中提供准确操作的能力。</li>
</ul>

<h3>Title: Large VLM-based Stylized Sports Captioning</h3>
<ul>
<li><strong>Authors: </strong>Sauptik Dhar, Nicholas Buoncristiani, Joe Anakata, Haoyu Zhang, Michelle Munson</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19295">https://arxiv.org/abs/2508.19295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19295">https://arxiv.org/pdf/2508.19295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19295]] Large VLM-based Stylized Sports Captioning(https://arxiv.org/abs/2508.19295)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The advent of large (visual) language models (LLM / LVLM) have led to a deluge of automated human-like systems in several domains including social media content generation, search and recommendation, healthcare prognosis, AI assistants for cognitive tasks etc. Although these systems have been successfully integrated in production; very little focus has been placed on sports, particularly accurate identification and natural language description of the game play. Most existing LLM/LVLMs can explain generic sports activities, but lack sufficient domain-centric sports' jargon to create natural (human-like) descriptions. This work highlights the limitations of existing SoTA LLM/LVLMs for generating production-grade sports captions from images in a desired stylized format, and proposes a two-level fine-tuned LVLM pipeline to address that. The proposed pipeline yields an improvement > 8-10% in the F1, and > 2-10% in BERT score compared to alternative approaches. In addition, it has a small runtime memory footprint and fast execution time. During Super Bowl LIX the pipeline proved its practical application for live professional sports journalism; generating highly accurate and stylized captions at the rate of 6 images per 3-5 seconds for over 1000 images during the game play.</li>
<li><strong>摘要：</strong>大型（视觉）语言模型（LLM / LVLM）的出现导致在多个领域中泛滥成灾，包括社交媒体内容的产生，搜索和建议，医疗保健预后，AI的认知任务助理等。尽管这些系统已成功地集成在生产中；很少有重点放在运动上，尤其是对游戏游戏的准确识别和自然语言描述。大多数现有的LLM/LVLM都可以解释通用的体育活动，但缺乏足够的以领域为中心的运动术语来创建天然（人类）描述。这项工作突出了现有的SOTA LLM/LVLM的局限性，用于以所需的风格化格式从图像中生成生产级的运动标题，并提出了一条两级微调的LVLM管道来解决这一点。与替代方法相比，提出的管道在F1中的改善> 8-10％，BERT得分> 2-10％。此外，它的运行时内存足迹和快速执行时间。在超级碗利克斯（Super Bowl Lix）期间，该管道证明了其在现场职业体育新闻业中实用的应用；在游戏过程中，以每3-5秒的速度以每3-5秒的6张图像的速率生成高度准确和风格化的字幕。</li>
</ul>

<h3>Title: DemoBias: An Empirical Study to Trace Demographic Biases in Vision Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Abu Sufian, Anirudha Ghosh, Debaditya Barman, Marco Leo, Cosimo Distante</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19298">https://arxiv.org/abs/2508.19298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19298">https://arxiv.org/pdf/2508.19298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19298]] DemoBias: An Empirical Study to Trace Demographic Biases in Vision Foundation Models(https://arxiv.org/abs/2508.19298)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities across various downstream tasks, including biometric face recognition (FR) with description. However, demographic biases remain a critical concern in FR, as these foundation models often fail to perform equitably across diverse demographic groups, considering ethnicity/race, gender, and age. Therefore, through our work DemoBias, we conduct an empirical evaluation to investigate the extent of demographic biases in LVLMs for biometric FR with textual token generation tasks. We fine-tuned and evaluated three widely used pre-trained LVLMs: LLaVA, BLIP-2, and PaliGemma on our own generated demographic-balanced dataset. We utilize several evaluation metrics, like group-specific BERTScores and the Fairness Discrepancy Rate, to quantify and trace the performance disparities. The experimental results deliver compelling insights into the fairness and reliability of LVLMs across diverse demographic groups. Our empirical study uncovered demographic biases in LVLMs, with PaliGemma and LLaVA exhibiting higher disparities for Hispanic/Latino, Caucasian, and South Asian groups, whereas BLIP-2 demonstrated comparably consistent. Repository: this https URL.</li>
<li><strong>摘要：</strong>大型视觉语言模型（LVLM）在各种下游任务中表现出了出色的功能，包括具有描述的生物特征识别识别（FR）。但是，在FR中，人口偏见仍然是一个关键问题，因为考虑种族/种族，性别和年龄，这些基础模型通常无法在各种人口组中公平地表现。因此，通过我们的工作差异，我们进行了经验评估，以研究具有文本令牌生成任务的生物特征识别FR中人口统计学偏见的程度。我们在我们自己生成的人口统计数据集中微调并评估了三种广泛使用的预训练的LVLM：LLAVA，BLIP-2和PALIGEMMA。我们利用几个评估指标，例如特定于小组的bertscores和公平差异率，来量化和追踪性能差异。实验结果为各种人口组的LVLM的公平性和可靠性提供了令人信服的见解。我们的实证研究发现了LVLMS中的人口偏见，而Paligemma和Llava对西班牙裔/拉丁裔，高加索人和南亚群体表现出较高的差异，而Blip-2则表现出相当一致的。存储库：此HTTPS URL。</li>
</ul>

<h3>Title: MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Ming Chen, Liyuan Cui, Wenyuan Zhang, Haoxian Zhang, Yan Zhou, Xiaohan Li, Xiaoqiang Liu, Pengfei Wan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19320">https://arxiv.org/abs/2508.19320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19320">https://arxiv.org/pdf/2508.19320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19320]] MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation(https://arxiv.org/abs/2508.19320)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recently, interactive digital human video generation has attracted widespread attention and achieved remarkable progress. However, building such a practical system that can interact with diverse input signals in real time remains challenging to existing methods, which often struggle with high latency, heavy computational cost, and limited controllability. In this work, we introduce an autoregressive video generation framework that enables interactive multimodal control and low-latency extrapolation in a streaming manner. With minimal modifications to a standard large language model (LLM), our framework accepts multimodal condition encodings including audio, pose, and text, and outputs spatially and semantically coherent representations to guide the denoising process of a diffusion head. To support this, we construct a large-scale dialogue dataset of approximately 20,000 hours from multiple sources, providing rich conversational scenarios for training. We further introduce a deep compression autoencoder with up to 64$\times$ reduction ratio, which effectively alleviates the long-horizon inference burden of the autoregressive model. Extensive experiments on duplex conversation, multilingual human synthesis, and interactive world model highlight the advantages of our approach in low latency, high efficiency, and fine-grained multimodal controllability.</li>
<li><strong>摘要：</strong>最近，交互式数字人类视频生成引起了广泛的关注，并取得了显着的进步。但是，建立可以实时与不同输入信号交互的实用系统对现有方法的挑战，这种方法通常在高潜伏期，繁重的计算成本和有限的可控性方面挣扎。在这项工作中，我们引入了一个自回归视频生成框架，该框架可以以流方式进行交互式多模式控制和低延迟外推。通过对标准大语言模型（LLM）的最小修改，我们的框架接受了多模式条件编码，包括音频，姿势和文本，并在空间和语义上相干表示以指导扩散型头的denoising过程。为了支持这一点，我们从多个来源构建了大约20,000小时的大规模对话数据集，为培训提供了丰富的对话场景。我们进一步引入了一个深层压缩自动编码器，最多64 $ \ times $降低比率，从而有效地减轻了自回归模型的长期推理负担。关于双工对话，多语言人类综合和交互式世界模型的广泛实验突出了我们方法在低潜伏期，高效率和细粒度的多模式可控性方面的优势。</li>
</ul>

<h3>Title: EffNetViTLoRA: An Efficient Hybrid Deep Learning Approach for Alzheimer's Disease Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Mahdieh Behjat Khatooni, Mohsen Soryani</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19349">https://arxiv.org/abs/2508.19349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19349">https://arxiv.org/pdf/2508.19349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19349]] EffNetViTLoRA: An Efficient Hybrid Deep Learning Approach for Alzheimer's Disease Diagnosis(https://arxiv.org/abs/2508.19349)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Alzheimer's disease (AD) is one of the most prevalent neurodegenerative disorders worldwide. As it progresses, it leads to the deterioration of cognitive functions. Since AD is irreversible, early diagnosis is crucial for managing its progression. Mild Cognitive Impairment (MCI) represents an intermediate stage between Cognitively Normal (CN) individuals and those with AD, and is considered a transitional phase from normal cognition to Alzheimer's disease. Diagnosing MCI is particularly challenging due to the subtle differences between adjacent diagnostic categories. In this study, we propose EffNetViTLoRA, a generalized end-to-end model for AD diagnosis using the whole Alzheimer's Disease Neuroimaging Initiative (ADNI) Magnetic Resonance Imaging (MRI) dataset. Our model integrates a Convolutional Neural Network (CNN) with a Vision Transformer (ViT) to capture both local and global features from MRI images. Unlike previous studies that rely on limited subsets of data, our approach is trained on the full T1-weighted MRI dataset from ADNI, resulting in a more robust and unbiased model. This comprehensive methodology enhances the model's clinical reliability. Furthermore, fine-tuning large pretrained models often yields suboptimal results when source and target dataset domains differ. To address this, we incorporate Low-Rank Adaptation (LoRA) to effectively adapt the pretrained ViT model to our target domain. This method enables efficient knowledge transfer and reduces the risk of overfitting. Our model achieves a classification accuracy of 92.52% and an F1-score of 92.76% across three diagnostic categories: AD, MCI, and CN for full ADNI dataset.</li>
<li><strong>摘要：</strong>阿尔茨海默氏病（AD）是全球最普遍的神经退行性疾病之一。随着它的发展，它导致认知功能的恶化。由于AD是不可逆的，因此早期诊断对于管理其进展至关重要。轻度认知障碍（MCI）代表认知上正常（CN）个体与患有AD的人之间的中间阶段，被认为是从正常认知到阿尔茨海默氏病的过渡阶段。由于相邻诊断类别之间的细微差异，诊断MCI尤其具有挑战性。在这项研究中，我们提出了使用整个阿尔茨海默氏病神经影像学计划（ADNI）磁共振成像（MRI）数据集的AD诊断的广义端到端模型EFFNETVITLORA。我们的模型将卷积神经网络（CNN）与视觉变压器（VIT）集成在一起，以从MRI图像中捕获本地和全局特征。与以前依靠有限数据子集的研究不同，我们的方法对来自ADNI的完整T1加权MRI数据集进行了培训，从而产生了更强大和无偏见的模型。这种全面的方法增强了模型的临床可靠性。此外，当源和目标数据集域不同时，微调大型预审计的模型通常会产生次优的结果。为了解决这个问题，我们融合了低级适应性（LORA），以有效地使验证的VIT模型适应我们的目标域。此方法可以有效地转移知识转移并降低过度拟合的风险。在三个诊断类别中，我们的模型达到了92.52％的分类精度，F1得分为92.76％：全ADNI数据集的AD，MCI和CN。</li>
</ul>

<h3>Title: Graph Data Modeling: Molecules, Proteins, & Chemical Processes</h3>
<ul>
<li><strong>Authors: </strong>José Manuel Barraza-Chavez, Rana A. Barghout, Ricardo Almada-Monter, Benjamin Sanchez-Lengeling, Adrian Jinich, Radhakrishnan Mahadevan</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19356">https://arxiv.org/abs/2508.19356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19356">https://arxiv.org/pdf/2508.19356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19356]] Graph Data Modeling: Molecules, Proteins, & Chemical Processes(https://arxiv.org/abs/2508.19356)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Graphs are central to the chemical sciences, providing a natural language to describe molecules, proteins, reactions, and industrial processes. They capture interactions and structures that underpin materials, biology, and medicine. This primer, Graph Data Modeling: Molecules, Proteins, & Chemical Processes, introduces graphs as mathematical objects in chemistry and shows how learning algorithms (particularly graph neural networks) can operate on them. We outline the foundations of graph design, key prediction tasks, representative examples across chemical sciences, and the role of machine learning in graph-based modeling. Together, these concepts prepare readers to apply graph methods to the next generation of chemical discovery.</li>
<li><strong>摘要：</strong>图是化学科学的核心，提供了一种自然语言来描述分子，蛋白质，反应和工业过程。他们捕获了基础材料，生物学和医学的互动和结构。该底漆，图数据建模：分子，蛋白质和化学过程，将图作为化学中的数学对象引入图形，并显示学习算法（尤其是图形神经网络）如何在其上进行操作。我们概述了图形设计的基础，关键预测任务，化学科学的代表性示例以及机器学习在基于图的建模中的作用。这些概念共同使读者准备将图形方法应用于下一代化学发现。</li>
</ul>

<h3>Title: DETNO: A Diffusion-Enhanced Transformer Neural Operator for Long-Term Traffic Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Owais Ahmad, Milad Ramezankhani, Anirudh Deodhar</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19389">https://arxiv.org/abs/2508.19389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19389">https://arxiv.org/pdf/2508.19389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19389]] DETNO: A Diffusion-Enhanced Transformer Neural Operator for Long-Term Traffic Forecasting(https://arxiv.org/abs/2508.19389)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Accurate long-term traffic forecasting remains a critical challenge in intelligent transportation systems, particularly when predicting high-frequency traffic phenomena such as shock waves and congestion boundaries over extended rollout horizons. Neural operators have recently gained attention as promising tools for modeling traffic flow. While effective at learning function space mappings, they inherently produce smooth predictions that fail to reconstruct high-frequency features such as sharp density gradients which results in rapid error accumulation during multi-step rollout predictions essential for real-time traffic management. To address these fundamental limitations, we introduce a unified Diffusion-Enhanced Transformer Neural Operator (DETNO) architecture. DETNO leverages a transformer neural operator with cross-attention mechanisms, providing model expressivity and super-resolution, coupled with a diffusion-based refinement component that iteratively reconstructs high-frequency traffic details through progressive denoising. This overcomes the inherent smoothing limitations and rollout instability of standard neural operators. Through comprehensive evaluation on chaotic traffic datasets, our method demonstrates superior performance in extended rollout predictions compared to traditional and transformer-based neural operators, preserving high-frequency components and improving stability over long prediction horizons.</li>
<li><strong>摘要：</strong>在智能运输系统中，准确的长期交通预测仍然是一个至关重要的挑战，尤其是在预测高频交通现象（例如冲击波和延长的推出范围）等高频交通现象时。神经操作员最近引起了人们对交通流量建模的有前途的工具的关注。尽管有效地学习功能空间映射，但它们固有地产生了平稳的预测，这些预测无法重建高频梯度，例如尖锐的密度梯度，从而导致在实时流量管理必不可少的多步推出预测过程中迅速积累。为了解决这些基本局限性，我们引入了统一扩散增强的变压器神经操作员（detno）体系结构。 Detno利用具有跨注意机制的变压器神经操作员，提供模型的表达性和超分辨率，再加上基于扩散的改进成分，迭代通过渐进的DeNOSISISISISISISISISISERISINE迭代地重建高频流量细节。这克服了标准神经操作员的固有平滑局限性和推出不稳定性。通过对混乱流量数据集的全面评估，我们的方法表明，与传统和基于变压器的神经操作员相比，扩展推出预测的表现出色，可以保留高频组件并改善长期预测范围的稳定性。</li>
</ul>

<h3>Title: Quantum-Classical Hybrid Molecular Autoencoder for Advancing Classical Decoding</h3>
<ul>
<li><strong>Authors: </strong>Afrar Jahin, Yi Pan, Yingfeng Wang, Tianming Liu, Wei Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19394">https://arxiv.org/abs/2508.19394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19394">https://arxiv.org/pdf/2508.19394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19394]] Quantum-Classical Hybrid Molecular Autoencoder for Advancing Classical Decoding(https://arxiv.org/abs/2508.19394)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Although recent advances in quantum machine learning (QML) offer significant potential for enhancing generative models, particularly in molecular design, a large array of classical approaches still face challenges in achieving high fidelity and validity. In particular, the integration of QML with sequence-based tasks, such as Simplified Molecular Input Line Entry System (SMILES) string reconstruction, remains underexplored and usually suffers from fidelity degradation. In this work, we propose a hybrid quantum-classical architecture for SMILES reconstruction that integrates quantum encoding with classical sequence modeling to improve quantum fidelity and classical similarity. Our approach achieves a quantum fidelity of approximately 84% and a classical reconstruction similarity of 60%, surpassing existing quantum baselines. Our work lays a promising foundation for future QML applications, striking a balance between expressive quantum representations and classical sequence models and catalyzing broader research on quantum-aware sequence models for molecular and drug discovery.</li>
<li><strong>摘要：</strong>尽管量子机学习的最新进展（QML）为增强生成模型提供了重要的潜力，尤其是在分子设计中，但在实现高忠诚度和有效性方面，各种各样的经典方法仍然面临着挑战。特别是，QML与基于序列的任务的集成，例如简化的分子输入线输入系统（Smiles）字符串重建，仍然没有被驱动，并且通常患有保真度退化。在这项工作中，我们提出了一种用于微笑重建的混合量子古典体系结构，将量子编码与经典序列建模集成在一起，以提高量子保真度和经典相似性。我们的方法达到了约84％的量子保真度，经典的重建相似性为60％，超过了现有的量子基线。我们的工作为未来的QML应用奠定了有前途的基础，在表达量子表示和经典序列模型之间取得了平衡，并为分子和药物发现的量子感知序列模型进行了更广泛的研究。</li>
</ul>

<h3>Title: Efficiently Generating Multidimensional Calorimeter Data with Tensor Decomposition Parameterization</h3>
<ul>
<li><strong>Authors: </strong>Paimon Goulart, Shaan Pakala, Evangelos Papalexakis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19443">https://arxiv.org/abs/2508.19443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19443">https://arxiv.org/pdf/2508.19443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19443]] Efficiently Generating Multidimensional Calorimeter Data with Tensor Decomposition Parameterization(https://arxiv.org/abs/2508.19443)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Producing large complex simulation datasets can often be a time and resource consuming task. Especially when these experiments are very expensive, it is becoming more reasonable to generate synthetic data for downstream tasks. Recently, these methods may include using generative machine learning models such as Generative Adversarial Networks or diffusion models. As these generative models improve efficiency in producing useful data, we introduce an internal tensor decomposition to these generative models to even further reduce costs. More specifically, for multidimensional data, or tensors, we generate the smaller tensor factors instead of the full tensor, in order to significantly reduce the model's output and overall parameters. This reduces the costs of generating complex simulation data, and our experiments show the generated data remains useful. As a result, tensor decomposition has the potential to improve efficiency in generative models, especially when generating multidimensional data, or tensors.</li>
<li><strong>摘要：</strong>产生大型复杂模拟数据集通常是时间和资源消费任务。尤其是当这些实验非常昂贵时，为下游任务生成合成数据变得越来越合理。最近，这些方法可能包括使用生成机器学习模型，例如生成对抗网络或扩散模型。随着这些生成模型提高了生成有用数据的效率，我们对这些生成模型引入了内部张量分解，以进一步降低成本。更具体地说，对于多维数据或张量，我们生成较小的张量因子，而不是完整的张量，以显着减少模型的输出和整体参数。这降低了生成复杂的模拟数据的成本，我们的实验表明生成的数据仍然有用。结果，张量分解有可能提高生成模型的效率，尤其是在生成多维数据或张量时。</li>
</ul>

<h3>Title: On Surjectivity of Neural Networks: Can you elicit any behavior from your model?</h3>
<ul>
<li><strong>Authors: </strong>Haozhe Jiang, Nika Haghtalab</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19445">https://arxiv.org/abs/2508.19445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19445">https://arxiv.org/pdf/2508.19445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19445]] On Surjectivity of Neural Networks: Can you elicit any behavior from your model?(https://arxiv.org/abs/2508.19445)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Given a trained neural network, can any specified output be generated by some input? Equivalently, does the network correspond to a function that is surjective? In generative models, surjectivity implies that any output, including harmful or undesirable content, can in principle be generated by the networks, raising concerns about model safety and jailbreak vulnerabilities. In this paper, we prove that many fundamental building blocks of modern neural architectures, such as networks with pre-layer normalization and linear-attention modules, are almost always surjective. As corollaries, widely used generative frameworks, including GPT-style transformers and diffusion models with deterministic ODE solvers, admit inverse mappings for arbitrary outputs. By studying surjectivity of these modern and commonly used neural architectures, we contribute a formalism that sheds light on their unavoidable vulnerability to a broad class of adversarial attacks.</li>
<li><strong>摘要：</strong>给定受过训练的神经网络，可以通过某些输入生成任何指定的输出吗？等效地，网络是否对应于汇总的函数？在生成模型中，过滤性意味着，任何输出，包括有害或不良内容，都可以由网络产生，从而引起人们对模型安全性和越狱漏洞的担忧。在本文中，我们证明了许多现代神经体系结构的基本构建基块，例如具有前层归一化和线性注意模块的网络，几乎总是过渡性的。作为推论，广泛使用的生成框架，包括带有确定性ODE求解器的GPT风格的变压器和扩散模型，可以接受任意输出的反映射。通过研究这些现代且常用的神经体系结构的溢流性，我们贡献了一种形式主义，使它们不可避免地脆弱的脆弱性阐明了广泛的对抗性攻击。</li>
</ul>

<h3>Title: DeepAtlas: a tool for effective manifold learning</h3>
<ul>
<li><strong>Authors: </strong>Serena Hughes, Timothy Hamilton, Tom Kolokotrones, Eric J. Deeds</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19479">https://arxiv.org/abs/2508.19479</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19479">https://arxiv.org/pdf/2508.19479</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19479]] DeepAtlas: a tool for effective manifold learning(https://arxiv.org/abs/2508.19479)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Manifold learning builds on the "manifold hypothesis," which posits that data in high-dimensional datasets are drawn from lower-dimensional manifolds. Current tools generate global embeddings of data, rather than the local maps used to define manifolds mathematically. These tools also cannot assess whether the manifold hypothesis holds true for a dataset. Here, we describe DeepAtlas, an algorithm that generates lower-dimensional representations of the data's local neighborhoods, then trains deep neural networks that map between these local embeddings and the original data. Topological distortion is used to determine whether a dataset is drawn from a manifold and, if so, its dimensionality. Application to test datasets indicates that DeepAtlas can successfully learn manifold structures. Interestingly, many real datasets, including single-cell RNA-sequencing, do not conform to the manifold hypothesis. In cases where data is drawn from a manifold, DeepAtlas builds a model that can be used generatively and promises to allow the application of powerful tools from differential geometry to a variety of datasets.</li>
<li><strong>摘要：</strong>流动学习建立在“流动假设”上，该假设认为，高维数据集中的数据是从较低维的流形中得出的。当前工具会生成数据的全局嵌入数据，而不是用于数学定义歧管的本地图。这些工具还无法评估数据集的歧管假设是否正确。在这里，我们描述了DeepAtlas，这是一种生成数据本地社区的较低维表示的算法，然后训练这些局部嵌入式和原始数据之间映射的深神经网络。拓扑失真用于确定是否从歧管中绘制数据集以及其维度。测试数据集的应用表明，DeepAtlas可以成功学习多种多样的结构。有趣的是，许多真实数据集，包括单细胞RNA的测序，不符合歧管假设。如果从歧管绘制数据的情况下，Deepatlas构建了一个可以固定地使用的模型，并承诺允许将功能强大的工具从微分几何形状应用到各种数据集。</li>
</ul>

<h3>Title: Distribution Shift Aware Neural Tabular Learning</h3>
<ul>
<li><strong>Authors: </strong>Wangyang Ying, Nanxu Gong, Dongjie Wang, Xinyuan Wang, Arun Vignesh Malarkkan, Vivek Gupta, Chandan K. Reddy, Yanjie Fu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19486">https://arxiv.org/abs/2508.19486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19486">https://arxiv.org/pdf/2508.19486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19486]] Distribution Shift Aware Neural Tabular Learning(https://arxiv.org/abs/2508.19486)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Tabular learning transforms raw features into optimized spaces for downstream tasks, but its effectiveness deteriorates under distribution shifts between training and testing data. We formalize this challenge as the Distribution Shift Tabular Learning (DSTL) problem and propose a novel Shift-Aware Feature Transformation (SAFT) framework to address it. SAFT reframes tabular learning from a discrete search task into a continuous representation-generation paradigm, enabling differentiable optimization over transformed feature sets. SAFT integrates three mechanisms to ensure robustness: (i) shift-resistant representation via embedding decorrelation and sample reweighting, (ii) flatness-aware generation through suboptimal embedding averaging, and (iii) normalization-based alignment between training and test distributions. Extensive experiments show that SAFT consistently outperforms prior tabular learning methods in terms of robustness, effectiveness, and generalization ability under diverse real-world distribution shifts.</li>
<li><strong>摘要：</strong>表格学习将原始特征转换为最佳任务的优化空间，但其有效性在训练和测试数据之间的分配变化下恶化。我们将这一挑战形式化为分配移动表格学习（DSTL）问题，并提出了一种新颖的转换功能转换（SAFT）框架来解决它。 SAFT从离散的搜索任务中将表格学习重新缩放为连续表示生成范式，从而在转换的功能集上进行了可区分的优化。 SAFT整合了三种机制以确保鲁棒性：（i）通过嵌入去相关和样品重新加权的耐偏移表示形式，（ii）通过次优的嵌入平均扁平度吸引人的产生，以及（iii）基于训练和测试分布之间的基于归一化的对齐。广泛的实验表明，SAFT始终在不同的现实世界分布变化下的鲁棒性，有效性和概括能力方面持续优于先前的表格学习方法。</li>
</ul>

<h3>Title: Data-Efficient Symbolic Regression via Foundation Model Distillation</h3>
<ul>
<li><strong>Authors: </strong>Wangyang Ying, Jinghan Zhang, Haoyue Bai, Nanxu Gong, Xinyuan Wang, Kunpeng Liu, Chandan K. Reddy, Yanjie Fu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19487">https://arxiv.org/abs/2508.19487</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19487">https://arxiv.org/pdf/2508.19487</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19487]] Data-Efficient Symbolic Regression via Foundation Model Distillation(https://arxiv.org/abs/2508.19487)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Discovering interpretable mathematical equations from observed data (a.k.a. equation discovery or symbolic regression) is a cornerstone of scientific discovery, enabling transparent modeling of physical, biological, and economic systems. While foundation models pre-trained on large-scale equation datasets offer a promising starting point, they often suffer from negative transfer and poor generalization when applied to small, domain-specific datasets. In this paper, we introduce EQUATE (Equation Generation via QUality-Aligned Transfer Embeddings), a data-efficient fine-tuning framework that adapts foundation models for symbolic equation discovery in low-data regimes via distillation. EQUATE combines symbolic-numeric alignment with evaluator-guided embedding optimization, enabling a principled embedding-search-generation paradigm. Our approach reformulates discrete equation search as a continuous optimization task in a shared embedding space, guided by data-equation fitness and simplicity. Experiments across three standard public benchmarks (Feynman, Strogatz, and black-box datasets) demonstrate that EQUATE consistently outperforms state-of-the-art baselines in both accuracy and robustness, while preserving low complexity and fast inference. These results highlight EQUATE as a practical and generalizable solution for data-efficient symbolic regression in foundation model distillation settings.</li>
<li><strong>摘要：</strong>从观察到的数据（又名方程发现或符号回归）中发现可解释的数学方程是科学发现的基石，可以实现物理，生物学和经济体系的透明建模。尽管在大规模方程数据集中预先训练的基础模型提供了一个有希望的起点，但当应用于小型域特异性数据集时，它们通常会遭受负转移和不良的概括。在本文中，我们介绍了等价（通过质量对准传输嵌入的方程生成），这是一个具有数据有效的微调框架，可通过蒸馏调整基础模型，以在低数据表格中进行符号方程发现。等同于将符号数的对准与评估者指导的嵌入优化相结合，从而实现了原则性的嵌入式搜索生成范式。我们的方法将离散方程式搜索重新定义为在共享嵌入空间中的连续优化任务，并在数据方程式适应性和简单性的指导下。在三个标准公共基准（Feynman，Strogatz和Black-Box数据集）上进行的实验表明，在准确性和稳健性中，始终如一地等同于最先进的基线，同时保持低复杂性和快速推断。这些结果突出显示，在基础模型蒸馏设置中，用于数据有效的符号回归的实用且可推广的解决方案。</li>
</ul>

<h3>Title: Sat2Flow: A Structure-Aware Diffusion Framework for Human Flow Generation from Satellite Imagery</h3>
<ul>
<li><strong>Authors: </strong>Xiangxu Wang, Tianhong Zhao, Wei Tu, Bowen Zhang, Guanzhou Chen, Jinzhou Cao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19499">https://arxiv.org/abs/2508.19499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19499">https://arxiv.org/pdf/2508.19499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19499]] Sat2Flow: A Structure-Aware Diffusion Framework for Human Flow Generation from Satellite Imagery(https://arxiv.org/abs/2508.19499)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Origin-Destination (OD) flow matrices are essential for urban mobility analysis, underpinning applications in traffic forecasting, infrastructure planning, and policy design. However, existing methods suffer from two critical limitations: (1) reliance on auxiliary features (e.g., Points of Interest, socioeconomic statistics) that are costly to collect and have limited spatial coverage; and (2) sensitivity to spatial topology, where minor index reordering of urban regions (e.g., census tract relabeling) disrupts structural coherence in generated flows. To address these challenges, we propose Sat2Flow, a latent structure-aware diffusion-based framework that generates structurally coherent OD flows using solely satellite imagery as input. Our approach introduces a multi-kernel encoder to capture diverse regional interactions and employs a permutation-aware diffusion process that aligns latent representations across different regional orderings. Through a joint contrastive training objective that bridges satellite-derived features with OD patterns, combined with equivariant diffusion training that enforces structural consistency, Sat2Flow ensures topological robustness under arbitrary regional reindexing. Experimental results on real-world urban datasets demonstrate that Sat2Flow outperforms both physics-based and data-driven baselines in numerical accuracy while preserving empirical distributions and spatial structures under index permutations. Sat2Flow offers a globally scalable solution for OD flow generation in data-scarce urban environments, eliminating region-specific auxiliary data dependencies while maintaining structural invariance for robust mobility modeling.</li>
<li><strong>摘要：</strong>原产地点（OD）流矩阵对于城市流动性分析，基础预测，基础设施计划和政策设计的申请是必不可少的。但是，现有方法受到了两个关键局限性：（1）依赖辅助特征（例如，兴趣点，社会经济统计数据），收集且空间覆盖率有限； （2）对空间拓扑的敏感性，其中城市区域（例如，人口普查区域重新标记）的次要指数破坏了产生的流中的结构相干性。为了应对这些挑战，我们提出了SAT2Flow，这是一种潜在的结构感知扩散的框架，该框架使用仅卫星图像作为输入来生成结构相干的OD流。我们的方法引入了多内核编码器，以捕获各种区域相互作用，并采用了置换感知的扩散过程，该过程使在不同区域秩序中的潜在表示。通过一个联合对比度训练目标，桥接具有OD模式的卫星衍生特征，再加上均衡的扩散训练，从而实现结构一致性，SAT2Flow确保了在任意区域重新索引下的拓扑鲁棒性。现实世界中城市数据集的实验结果表明，SAT2Flow以数值精度优于基于物理和数据驱动的基准，同时保留了索引排列下的经验分布和空间结构。 SAT2Flow在数据范围的城市环境中为OD流量产生提供了全球可扩展的解决方案，从而消除了特定区域的辅助数据依赖项，同时保持结构性不变性以实现强大的移动性建模。</li>
</ul>

<h3>Title: Learning Game-Playing Agents with Generative Code Optimization</h3>
<ul>
<li><strong>Authors: </strong>Zhiyi Kuang, Ryan Rong, YuCheng Yuan, Allen Nie</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19506">https://arxiv.org/abs/2508.19506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19506">https://arxiv.org/pdf/2508.19506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19506]] Learning Game-Playing Agents with Generative Code Optimization(https://arxiv.org/abs/2508.19506)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present a generative optimization approach for learning game-playing agents, where policies are represented as Python programs and refined using large language models (LLMs). Our method treats decision-making policies as self-evolving code, with current observation as input and an in-game action as output, enabling agents to self-improve through execution traces and natural language feedback with minimal human intervention. Applied to Atari games, our game-playing Python program achieves performance competitive with deep reinforcement learning (RL) baselines while using significantly less training time and much fewer environment interactions. This work highlights the promise of programmatic policy representations for building efficient, adaptable agents capable of complex, long-horizon reasoning.</li>
<li><strong>摘要：</strong>我们提出了一种用于学习游戏玩法代理的生成优化方法，该方法将策略表示为Python程序，并使用大语言模型（LLMS）进行了完善。我们的方法将决策政策视为自我不断发展的代码，当前的观察结果是输入和游戏中的动作作为输出，使代理人能够通过执行痕迹和自然语言反馈自我侵蚀，并以最少的人为干预。应用于Atari游戏，我们的游戏Python计划通过深入强化学习（RL）基准实现了竞争性竞争，同时使用较小的训练时间和更少的环境交互。这项工作强调了编程政策表示的承诺，以建立能够具有复杂，长途推理的高效，适应能力的代理。</li>
</ul>

<h3>Title: MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment</h3>
<ul>
<li><strong>Authors: </strong>Zhiting Gao, Dan Song, Diqiong Jiang, Chao Xue, An-An Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19527">https://arxiv.org/abs/2508.19527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19527">https://arxiv.org/pdf/2508.19527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19527]] MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment(https://arxiv.org/abs/2508.19527)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Motion generation is essential for animating virtual characters and embodied agents. While recent text-driven methods have made significant strides, they often struggle with achieving precise alignment between linguistic descriptions and motion semantics, as well as with the inefficiencies of slow, multi-step inference. To address these issues, we introduce TMR++ Aligned Preference Optimization (TAPO), an innovative framework that aligns subtle motion variations with textual modifiers and incorporates iterative adjustments to reinforce semantic grounding. To further enable real-time synthesis, we propose MotionFLUX, a high-speed generation framework based on deterministic rectified flow matching. Unlike traditional diffusion models, which require hundreds of denoising steps, MotionFLUX constructs optimal transport paths between noise distributions and motion spaces, facilitating real-time synthesis. The linearized probability paths reduce the need for multi-step sampling typical of sequential methods, significantly accelerating inference time without sacrificing motion quality. Experimental results demonstrate that, together, TAPO and MotionFLUX form a unified system that outperforms state-of-the-art approaches in both semantic consistency and motion quality, while also accelerating generation speed. The code and pretrained models will be released.</li>
<li><strong>摘要：</strong>运动生成对于为虚拟字符和具体的代理动画至关重要。尽管最近以文本驱动的方法取得了长足的进步，但它们通常在达到语言描述和运动语义之间的精确比对，以及缓慢，多步推断的效率低下。为了解决这些问题，我们介绍了TMR ++对齐偏好优化（TAPO），这是一个创新的框架，将微妙的运动变化与文本修饰符保持一致，并结合了迭代调整以增强语义接地。为了进一步启用实时合成，我们提出了MotionFlux，这是一个基于确定性整流流匹配的高速生成框架。与传统的扩散模型不同，需要数百个降级步骤，而动态流量构造在噪声分布和运动空间之间的最佳传输路径，从而促进实时综合。线性化概率路径减少了顺序方法的多步取样的需求，从而显着加速了推理时间，而无需牺牲运动质量。实验结果表明，Tapo和MotionFlux构成了一个统一的系统，在语义一致性和运动质量方面都优于最先进的方法，同时也加速了生成速度。代码和预估计的模型将发布。</li>
</ul>

<h3>Title: CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Nannan Zhu, Yonghao Dong, Teng Wang, Xueqian Li, Shengjun Deng, Yijia Wang, Zheng Hong, Tiantian Geng, Guo Niu, Hanyan Huang, Xiongfei Yao, Shuaiwei Jiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19542">https://arxiv.org/abs/2508.19542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19542">https://arxiv.org/pdf/2508.19542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19542]] CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning(https://arxiv.org/abs/2508.19542)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While multimodal large language models (MLLMs) exhibit strong performance on single-video tasks (e.g., video question answering), their ability across multiple videos remains critically underexplored. However, this capability is essential for real-world applications, including multi-camera surveillance and cross-video procedural learning. To bridge this gap, we present CVBench, the first comprehensive benchmark designed to assess cross-video relational reasoning rigorously. CVBench comprises 1,000 question-answer pairs spanning three hierarchical tiers: cross-video object association (identifying shared entities), cross-video event association (linking temporal or causal event chains), and cross-video complex reasoning (integrating commonsense and domain knowledge). Built from five domain-diverse video clusters (e.g., sports, life records), the benchmark challenges models to synthesise information across dynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (including GPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thought prompting paradigms. Key findings reveal stark performance gaps: even top models, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks, compared to the 91% accuracy of human performance. Crucially, our analysis reveals fundamental bottlenecks inherent in current MLLM architectures, notably deficient inter-video context retention and poor disambiguation of overlapping entities. CVBench establishes a rigorous framework for diagnosing and advancing multi-video reasoning, offering architectural insights for next-generation this http URL data and evaluation code are available at this https URL.</li>
<li><strong>摘要：</strong>虽然多模式的大语言模型（MLLM）在单录音任务（例如，视频问答）上表现出很强的性能，但它们在多个视频中的能力仍然急切地被忽视。但是，这种功能对于现实世界应用至关重要，包括多摄像机监视和跨视频程序学习。为了弥合这一差距，我们提出了CVBench，这是第一个旨在严格评估跨Video关系推理的全面基准。 CVBENCH包括1,000对跨越三个层次层的问题解答对：跨效率对象关联（识别共享实体），跨video事件关联（链接时间或因果事件链）和跨Video复杂的理由（集成常识和域知识）。基于五个域多样性视频簇（例如，体育，生活记录），基准挑战模型，以综合动态视觉上下文的信息。在零射门或促使范式范围的零弹药或经过三通链下，对10+领先的MLLM（包括GPT-4O，GEMINI-2.0-FLASH，QWEN2.5-VL）进行了广泛的评估。关键发现揭示了鲜明的性能差距：即使是GPT-4O之类的顶级模型，与人类绩效的91％准确性相比，在因果推理任务上仅达到60％的精度。至关重要的是，我们的分析揭示了当前MLLM体系结构固有的基本瓶颈，这尤其是缺乏的视频间环境保留率，对重叠实体的歧义不良。 CVBENCH建立了一个严格的框架，用于诊断和推进多视频推理，为下一代提供架构见解。此HTTP URL数据和评估代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: MonoRelief V2: Leveraging Real Data for High-Fidelity Monocular Relief Recovery</h3>
<ul>
<li><strong>Authors: </strong>Yu-Wei Zhang, Tongju Han, Lipeng Gao, Mingqiang Wei, Hui Liu, Changbao Li, Caiming Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19555">https://arxiv.org/abs/2508.19555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19555">https://arxiv.org/pdf/2508.19555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19555]] MonoRelief V2: Leveraging Real Data for High-Fidelity Monocular Relief Recovery(https://arxiv.org/abs/2508.19555)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper presents MonoRelief V2, an end-to-end model designed for directly recovering 2.5D reliefs from single images under complex material and illumination variations. In contrast to its predecessor, MonoRelief V1 [1], which was solely trained on synthetic data, MonoRelief V2 incorporates real data to achieve improved robustness, accuracy and efficiency. To overcome the challenge of acquiring large-scale real-world dataset, we generate approximately 15,000 pseudo real images using a text-to-image generative model, and derive corresponding depth pseudo-labels through fusion of depth and normal predictions. Furthermore, we construct a small-scale real-world dataset (800 samples) via multi-view reconstruction and detail refinement. MonoRelief V2 is then progressively trained on the pseudo-real and real-world datasets. Comprehensive experiments demonstrate its state-of-the-art performance both in depth and normal predictions, highlighting its strong potential for a range of downstream applications. Code is at: this https URL.</li>
<li><strong>摘要：</strong>本文介绍了Monorelief V2，这是一种端到端模型，旨在直接从复杂的材料和照明变化下直接从单个图像中恢复2.5D浮雕。与其前身Monorelief V1 [1]相反，该[1]仅接受合成数据的训练，Monorelief V2结合了实际数据，以提高鲁棒性，准确性和效率。为了克服获取大型现实世界数据集的挑战，我们使用文本对图像生成模型生成了大约15,000个伪真实图像，并通过融合深度和正常预测来得出相应的深度伪标记。此外，我们通过多视图重建和细节改进来构建一个小规模的现实世界数据集（800个样本）。然后，Monorelief V2在伪真实和现实世界数据集中逐步培训。全面的实验证明了其深度和正常预测的最先进性能，突出了其在一系列下游应用中的强大潜力。代码为：此HTTPS URL。</li>
</ul>

<h3>Title: Generative Models for Synthetic Data: Transforming Data Mining in the GenAI Era</h3>
<ul>
<li><strong>Authors: </strong>Dawei Li, Yue Huang, Ming Li, Tianyi Zhou, Xiangliang Zhang, Huan Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19570">https://arxiv.org/abs/2508.19570</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19570">https://arxiv.org/pdf/2508.19570</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19570]] Generative Models for Synthetic Data: Transforming Data Mining in the GenAI Era(https://arxiv.org/abs/2508.19570)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative models such as Large Language Models, Diffusion Models, and generative adversarial networks have recently revolutionized the creation of synthetic data, offering scalable solutions to data scarcity, privacy, and annotation challenges in data mining. This tutorial introduces the foundations and latest advances in synthetic data generation, covers key methodologies and practical frameworks, and discusses evaluation strategies and applications. Attendees will gain actionable insights into leveraging generative synthetic data to enhance data mining research and practice. More information can be found on our website: this https URL.</li>
<li><strong>摘要：</strong>诸如大语言模型，扩散模型和生成对抗网络之类的生成模型最近彻底改变了合成数据的创建，为数据挖掘中的数据稀缺，隐私和注释挑战提供了可扩展的解决方案。本教程介绍了合成数据生成的基础和最新进展，涵盖了关键的方法和实用框架，并讨论了评估策略和应用。与会者将获得有关利用生成合成数据的可行见解，以增强数据挖掘研究和实践。更多信息可以在我们的网站上找到：此HTTPS URL。</li>
</ul>

<h3>Title: Interact-Custom: Customized Human Object Interaction Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhu Xu, Zhaowen Wang, Yuxin Peng, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19575">https://arxiv.org/abs/2508.19575</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19575">https://arxiv.org/pdf/2508.19575</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19575]] Interact-Custom: Customized Human Object Interaction Image Generation(https://arxiv.org/abs/2508.19575)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Compositional Customized Image Generation aims to customize multiple target concepts within generation content, which has gained attention for its wild this http URL approaches mainly concentrate on the target entity's appearance preservation, while neglecting the fine-grained interaction control among target this http URL enable the model of such interaction control capability, we focus on human object interaction scenario and propose the task of Customized Human Object Interaction Image Generation(CHOI), which simultaneously requires identity preservation for target human object and the interaction semantic control between this http URL primary challenges exist for CHOI:(1)simultaneous identity preservation and interaction control demands require the model to decompose the human object into self-contained identity features and pose-oriented interaction features, while the current HOI image datasets fail to provide ideal samples for such feature-decomposed learning.(2)inappropriate spatial configuration between human and object may lead to the lack of desired interaction this http URL tackle it, we first process a large-scale dataset, where each sample encompasses the same pair of human object involving different interactive this http URL we design a two-stage model Interact-Custom, which firstly explicitly models the spatial configuration by generating a foreground mask depicting the interaction behavior, then under the guidance of this mask, we generate the target human object interacting while preserving their identities this http URL, if the background image and the union location of where the target human object should appear are provided by users, Interact-Custom also provides the optional functionality to specify them, offering high content controllability. Extensive experiments on our tailored metrics for CHOI task demonstrate the effectiveness of our approach.</li>
<li><strong>摘要：</strong>Compositional Customized Image Generation aims to customize multiple target concepts within generation content, which has gained attention for its wild this http URL approaches mainly concentrate on the target entity's appearance preservation, while neglecting the fine-grained interaction control among target this http URL enable the model of such interaction control capability, we focus on human object interaction scenario and propose the task of Customized Human Object Interaction Image Generation(CHOI), which simultaneously需要对目标人物对象的身份保存以及CHOI的HTTP URL主要挑战之间存在的相互作用语义控制：（1）同时保存和互动控制需求，要求模型将人类物体分解为独立的身份特征，并将姿势与姿势相互作用特征和姿势相互作用特征分解为当前的HOI HOI型人类相互作用，而这些spaT spemation spats spatiration spatiratiate spatirate spatirate spatiratiate in Appl spatiate in Appl in Appt in Appt in Appt（2）构成了2（2）（2）（2）（2）（2）（2）（2）。并且对象可能导致缺乏这种HTTP URL解决的问题，我们首先处理一个大尺度数据集，每个样本都包含涉及不同互动的同一对人对象，我们设计了一个两个阶段的模型，我们设计了一个两阶段的模型相互作用，首先要明确地构造了一个型号，从而使该型号构成了构图，然后在构造的范围内建立了构图，然后构建构造的构图，然后构建型号的构图，使得构造互动界面界的行动，使得相互作用互动，互动，使得互动界面偶然行动，在保留其身份的同时，人类对象在此HTTP URL中进行交互，如果用户提供了目标人物对象的背景图像和联合位置，则互动custom还提供了可选功能来指定它们，并提供高内容可控性。对CHOI任务量身定制的指标进行了广泛的实验，证明了我们方法的有效性。</li>
</ul>

<h3>Title: High-Speed FHD Full-Color Video Computer-Generated Holography</h3>
<ul>
<li><strong>Authors: </strong>Haomiao Zhang, Miao Cao, Xuan Yu, Hui Luo, Yanling Piao, Mengjie Qin, Zhangyuan Li, Ping Wang, Xin Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19579">https://arxiv.org/abs/2508.19579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19579">https://arxiv.org/pdf/2508.19579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19579]] High-Speed FHD Full-Color Video Computer-Generated Holography(https://arxiv.org/abs/2508.19579)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Computer-generated holography (CGH) is a promising technology for next-generation displays. However, generating high-speed, high-quality holographic video requires both high frame rate display and efficient computation, but is constrained by two key limitations: ($i$) Learning-based models often produce over-smoothed phases with narrow angular spectra, causing severe color crosstalk in high frame rate full-color displays such as depth-division multiplexing and thus resulting in a trade-off between frame rate and color fidelity. ($ii$) Existing frame-by-frame optimization methods typically optimize frames independently, neglecting spatial-temporal correlations between consecutive frames and leading to computationally inefficient solutions. To overcome these challenges, in this paper, we propose a novel high-speed full-color video CGH generation scheme. First, we introduce Spectrum-Guided Depth Division Multiplexing (SGDDM), which optimizes phase distributions via frequency modulation, enabling high-fidelity full-color display at high frame rates. Second, we present HoloMamba, a lightweight asymmetric Mamba-Unet architecture that explicitly models spatial-temporal correlations across video sequences to enhance reconstruction quality and computational efficiency. Extensive simulated and real-world experiments demonstrate that SGDDM achieves high-fidelity full-color display without compromise in frame rate, while HoloMamba generates FHD (1080p) full-color holographic video at over 260 FPS, more than 2.6$\times$ faster than the prior state-of-the-art Divide-Conquer-and-Merge Strategy.</li>
<li><strong>摘要：</strong>计算机生成的全息图（CGH）是下一代展示的有前途的技术。但是，生成高速，高质量的全息视频需要高框架速率显示和有效的计算，但受到两个关键局限性的约束：（$ i $）基于学习的模型通常会产生具有狭窄角光谱的过度平滑相位，从而在高帧速度显示中引起严重的颜色串扰，例如，在高帧速度显示中，例如全面的显示范围，从而在划分的范围中产生了折叠率和折叠率。 （$ ii $）现有的逐帧优化方法通常独立地优化帧，从而忽略了连续帧之间的时空相关性，并导致计算效率低下的解决方案。为了克服这些挑战，在本文中，我们提出了一种新型的高速全彩视频CGH生成计划。首先，我们介绍频谱引导的深度分层多路复用（SGDDM），该多路复用（SGDDM）通过频率调制优化相位分布，以高框架速率使高保真全彩色显示。其次，我们提出了Holomamba，这是一种轻巧的不对称Mamba-Unet架构，该体系结构明确模拟了视频序列之间的时空相关性，以提高重建质量和计算效率。 Extensive simulated and real-world experiments demonstrate that SGDDM achieves high-fidelity full-color display without compromise in frame rate, while HoloMamba generates FHD (1080p) full-color holographic video at over 260 FPS, more than 2.6$\times$ faster than the prior state-of-the-art Divide-Conquer-and-Merge Strategy.</li>
</ul>

<h3>Title: Guiding Noisy Label Conditional Diffusion Models with Score-based Discriminator Correction</h3>
<ul>
<li><strong>Authors: </strong>Dat Nguyen Cong, Hieu Tran Bao, Hoang Thanh-Tung</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19581">https://arxiv.org/abs/2508.19581</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19581">https://arxiv.org/pdf/2508.19581</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19581]] Guiding Noisy Label Conditional Diffusion Models with Score-based Discriminator Correction(https://arxiv.org/abs/2508.19581)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have gained prominence as state-of-the-art techniques for synthesizing images and videos, particularly due to their ability to scale effectively with large datasets. Recent studies have uncovered that these extensive datasets often contain mistakes from manual labeling processes. However, the extent to which such errors compromise the generative capabilities and controllability of diffusion models is not well studied. This paper introduces Score-based Discriminator Correction (SBDC), a guidance technique for aligning noisy pre-trained conditional diffusion models. The guidance is built on discriminator training using adversarial loss, drawing on prior noise detection techniques to assess the authenticity of each sample. We further show that limiting the usage of our guidance to the early phase of the generation process leads to better performance. Our method is computationally efficient, only marginally increases inference time, and does not require retraining diffusion models. Experiments on different noise settings demonstrate the superiority of our method over previous state-of-the-art methods.</li>
<li><strong>摘要：</strong>扩散模型已成为合成图像和视频的最新技术，特别是由于它们有效地使用大型数据集扩展的能力。最近的研究发现，这些广泛的数据集通常包含手动标记过程中的错误。但是，这种错误损害扩散模型的生成能力和可控性的程度尚未得到很好的研究。本文介绍了基于得分的鉴别校正（SBDC），这是一种针对嘈杂的预训练的条件扩散模型的指导技术。该指南建立在使用对抗损失的歧视训练的基础上，并利用先前的噪声检测技术来评估每个样本的真实性。我们进一步表明，将指导的用法限制在生成过程的早期阶段会带来更好的性能。我们的方法在计算上是有效的，只会稍微增加推理时间，并且不需要重新进行扩散模型。在不同的噪声设置上进行的实验证明了我们方法比以前最新方法的优越性。</li>
</ul>

<h3>Title: IELDG: Suppressing Domain-Specific Noise with Inverse Evolution Layers for Domain Generalized Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Qizhe Fan, Chaoyu Liu, Zhonghua Qiao, Xiaoqin Shen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19604">https://arxiv.org/abs/2508.19604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19604">https://arxiv.org/pdf/2508.19604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19604]] IELDG: Suppressing Domain-Specific Noise with Inverse Evolution Layers for Domain Generalized Semantic Segmentation(https://arxiv.org/abs/2508.19604)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Domain Generalized Semantic Segmentation (DGSS) focuses on training a model using labeled data from a source domain, with the goal of achieving robust generalization to unseen target domains during inference. A common approach to improve generalization is to augment the source domain with synthetic data generated by diffusion models (DMs). However, the generated images often contain structural or semantic defects due to training imperfections. Training segmentation models with such flawed data can lead to performance degradation and error accumulation. To address this issue, we propose to integrate inverse evolution layers (IELs) into the generative process. IELs are designed to highlight spatial discontinuities and semantic inconsistencies using Laplacian-based priors, enabling more effective filtering of undesirable generative patterns. Based on this mechanism, we introduce IELDM, an enhanced diffusion-based data augmentation framework that can produce higher-quality images. Furthermore, we observe that the defect-suppression capability of IELs can also benefit the segmentation network by suppressing artifact propagation. Based on this insight, we embed IELs into the decoder of the DGSS model and propose IELFormer to strengthen generalization capability in cross-domain scenarios. To further strengthen the model's semantic consistency across scales, IELFormer incorporates a multi-scale frequency fusion (MFF) module, which performs frequency-domain analysis to achieve structured integration of multi-resolution features, thereby improving cross-scale coherence. Extensive experiments on benchmark datasets demonstrate that our approach achieves superior generalization performance compared to existing methods.</li>
<li><strong>摘要：</strong>域广义语义分割（DGSS）专注于使用来自源域的标记数据训练模型，以实现鲁棒的概括，以在推理过程中未见目标域。改善概括的一种常见方法是通过扩散模型（DMS）生成的合成数据来增强源域。但是，由于训练缺陷，生成的图像通常包含结构或语义缺陷。具有这种缺陷数据的训练分割模型会导致性能降解和误差积累。为了解决这个问题，我们建议将反向演化层（IEL）集成到生成过程中。 IEL旨在强调使用基于拉普拉斯的先验的空间不连续性和语义不一致，从而更有效地过滤不良的生成模式。基于这种机制，我们引入了IELDM，这是一种增强的基于扩散的数据增强框架，可以产生更高质量的图像。此外，我们观察到，IELS的缺陷支持能力也可以通过抑制伪影传播来使分割网络受益。基于这种见解，我们将IEL嵌入了DGSS模型的解码器中，并提出了IELFORMER以增强跨域场景中的概括能力。为了进一步增强模型在范围内的语义一致性，IELFormer结合了多尺度频率融合（MFF）模块，该模块执行频域分析以实现多分辨率特征的结构化整合，从而改善了跨尺度相干性。基准数据集的广泛实验表明，与现有方法相比，我们的方法可实现出色的概括性能。</li>
</ul>

<h3>Title: Towards Instance-wise Personalized Federated Learning via Semi-Implicit Bayesian Prompt Tuning</h3>
<ul>
<li><strong>Authors: </strong>Tiandi Ye, Wenyan Liu, Kai Yao, Lichun Li, Shangchao Su, Cen Chen, Xiang Li, Shan Yin, Ming Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19621">https://arxiv.org/abs/2508.19621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19621">https://arxiv.org/pdf/2508.19621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19621]] Towards Instance-wise Personalized Federated Learning via Semi-Implicit Bayesian Prompt Tuning(https://arxiv.org/abs/2508.19621)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) is a privacy-preserving machine learning paradigm that enables collaborative model training across multiple distributed clients without disclosing their raw data. Personalized federated learning (pFL) has gained increasing attention for its ability to address data heterogeneity. However, most existing pFL methods assume that each client's data follows a single distribution and learn one client-level personalized model for each client. This assumption often fails in practice, where a single client may possess data from multiple sources or domains, resulting in significant intra-client heterogeneity and suboptimal performance. To tackle this challenge, we propose pFedBayesPT, a fine-grained instance-wise pFL framework based on visual prompt tuning. Specifically, we formulate instance-wise prompt generation from a Bayesian perspective and model the prompt posterior as an implicit distribution to capture diverse visual semantics. We derive a variational training objective under the semi-implicit variational inference framework. Extensive experiments on benchmark datasets demonstrate that pFedBayesPT consistently outperforms existing pFL methods under both feature and label heterogeneity settings.</li>
<li><strong>摘要：</strong>联合学习（FL）是一个隐私的机器学习范式，可以在不披露其原始数据的情况下跨多个分布式客户端进行协作模型培训。个性化联合学习（PFL）因其解决数据异质性的能力而引起了人们的关注。但是，大多数现有的PFL方法都假定每个客户端的数据遵循单个分布，并为每个客户端学习一个客户级个性化模型。在实践中，这个假设通常会失败，在这种情况下，单个客户可能会从多个来源或域中拥有数据，从而导致委托人内的异质性和次优性能。为了应对这一挑战，我们提出了Pfedbayespt，这是一个基于视觉及时调整的细粒实例PFL框架。具体而言，我们从贝叶斯的角度从实例上制定了实例的提示，并将及时的后部建模为捕获多种视觉语义的隐式分布。我们在半图形变异推理框架下得出一个变分训练目标。基准数据集上的广泛实验表明，在功能和标签异质性设置下，Pfedbayespt始终优于现有的PFL方法。</li>
</ul>

<h3>Title: IDF: Iterative Dynamic Filtering Networks for Generalizable Image Denoising</h3>
<ul>
<li><strong>Authors: </strong>Dongjin Kim, Jaekyun Ko, Muhammad Kashif Ali, Tae Hyun Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19649">https://arxiv.org/abs/2508.19649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19649">https://arxiv.org/pdf/2508.19649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19649]] IDF: Iterative Dynamic Filtering Networks for Generalizable Image Denoising(https://arxiv.org/abs/2508.19649)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Image denoising is a fundamental challenge in computer vision, with applications in photography and medical imaging. While deep learning-based methods have shown remarkable success, their reliance on specific noise distributions limits generalization to unseen noise types and levels. Existing approaches attempt to address this with extensive training data and high computational resources but they still suffer from overfitting. To address these issues, we conduct image denoising by utilizing dynamically generated kernels via efficient operations. This approach helps prevent overfitting and improves resilience to unseen noise. Specifically, our method leverages a Feature Extraction Module for robust noise-invariant features, Global Statistics and Local Correlation Modules to capture comprehensive noise characteristics and structural correlations. The Kernel Prediction Module then employs these cues to produce pixel-wise varying kernels adapted to local structures, which are then applied iteratively for denoising. This ensures both efficiency and superior restoration quality. Despite being trained on single-level Gaussian noise, our compact model (~ 0.04 M) excels across diverse noise types and levels, demonstrating the promise of iterative dynamic filtering for practical image denoising.</li>
<li><strong>摘要：</strong>图像denoising是计算机视觉中的一个基本挑战，并在摄影和医学成像中应用。尽管基于深度学习的方法已经取得了巨大的成功，但它们对特定噪声分布的依赖将概括限制为看不见的噪声类型和水平。现有的方法试图通过广泛的培训数据和高度计算资源来解决这一问题，但它们仍然过度拟合。为了解决这些问题，我们通过通过有效的操作使用动态生成的内核来进行图像Denoise。这种方法有助于防止过度拟合并提高对看不见的噪音的弹性。具体而言，我们的方法利用特征提取模块来实现强大的噪声不变特征，全局统计和局部相关模块来捕获综合的噪声特征和结构相关性。然后，内核预测模块采用这些提示来产生适合局部结构的不同核，然后迭代地将其应用于denosing。这样可以确保效率和出色的恢复质量。尽管接受了单级高斯噪声的训练，但我们的紧凑型模型（〜0.04 m）在各种噪声类型和水平上都出色，这表明了迭代动态过滤的希望，以实用图像降解。</li>
</ul>

<h3>Title: Synthetic Image Detection via Spectral Gaps of QC-RBIM Nishimori Bethe-Hessian Operators</h3>
<ul>
<li><strong>Authors: </strong>V. S. Usatyuk, D. A. Sapozhnikov, S. I. Egorov</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IT, math.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19698">https://arxiv.org/abs/2508.19698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19698">https://arxiv.org/pdf/2508.19698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19698]] Synthetic Image Detection via Spectral Gaps of QC-RBIM Nishimori Bethe-Hessian Operators(https://arxiv.org/abs/2508.19698)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid advance of deep generative models such as GANs and diffusion networks now produces images that are virtually indistinguishable from genuine photographs, undermining media forensics and biometric security. Supervised detectors quickly lose effectiveness on unseen generators or after adversarial post-processing, while existing unsupervised methods that rely on low-level statistical cues remain fragile. We introduce a physics-inspired, model-agnostic detector that treats synthetic-image identification as a community-detection problem on a sparse weighted graph. Image features are first extracted with pretrained CNNs and reduced to 32 dimensions, each feature vector becomes a node of a Multi-Edge Type QC-LDPC graph. Pairwise similarities are transformed into edge couplings calibrated at the Nishimori temperature, producing a Random Bond Ising Model (RBIM) whose Bethe-Hessian spectrum exhibits a characteristic gap when genuine community structure (real images) is present. Synthetic images violate the Nishimori symmetry and therefore lack such gaps. We validate the approach on binary tasks cat versus dog and male versus female using real photos from Flickr-Faces-HQ and CelebA and synthetic counterparts generated by GANs and diffusion models. Without any labeled synthetic data or retraining of the feature extractor, the detector achieves over 94% accuracy. Spectral analysis shows multiple well separated gaps for real image sets and a collapsed spectrum for generated ones. Our contributions are threefold: a novel LDPC graph construction that embeds deep image features, an analytical link between Nishimori temperature RBIM and the Bethe-Hessian spectrum providing a Bayes optimal detection criterion; and a practical, unsupervised synthetic image detector robust to new generative architectures. Future work will extend the framework to video streams and multi-class anomaly detection.</li>
<li><strong>摘要：</strong>甘纳斯和扩散网络等深层生成模型的快速发展现在产生的图像几乎与真实的照片没有区别，从而破坏了媒体取证和生物识别安全性。监督探测器很快对看不见的发电机或对抗后处理后会失去效率，而依赖低级统计提示的现有无监督方法仍然脆弱。我们引入了一个由物理启发的模型 - 不合Snostic探测器，该检测器将合成图像识别视为稀疏加权图上的社区检测问题。图像特征首先用预审核的CNN提取并降低至32个维度，每个特征向量成为多层类型QC-LDPC图的节点。成对的相似性转化为在Nishimori温度下校准的边缘耦合，从而产生了随机键合模型（RBIM），其伯特·海森频谱在存在真正的社区结构（真实图像）时表现出特征性的差距。合成图像违反了Nishimori对称性，因此缺乏这种差距。我们使用Flickr-Faces-HQ和Celeba和Celeba以及由GAN和扩散模型产生的flickr-Faces-HQ和Celeba和合成对应物中的真实照片来验证猫与狗与男性的方法。没有任何标记的合成数据或特征提取器的重新培训，检测器的精度超过94％。光谱分析显示了真实图像集的多个良好分开的间隙，以及生成的图像集的折叠光谱。我们的贡献是三倍：一种新型的LDPC图结构，嵌入了深层图像特征，Nishimori温度RBIM与Bethe-Hessian Spectrum之间的分析联系，提供了贝叶斯最佳检测标准；以及一种实用，无监督的合成图像检测器对新的生成体系结构的鲁棒性。未来的工作将把框架扩展到视频流和多类异常检测。</li>
</ul>

<h3>Title: Fast 3D Diffusion for Scalable Granular Media Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Moeeze Hassan, Régis Cottereau, Filippo Gatti, Patryk Dec</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19752">https://arxiv.org/abs/2508.19752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19752">https://arxiv.org/pdf/2508.19752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19752]] Fast 3D Diffusion for Scalable Granular Media Synthesis(https://arxiv.org/abs/2508.19752)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Simulating granular media, using Discrete Element Method is a computationally intensive task. This is especially true during initialization phase, which dominates total simulation time because of large displacements involved and associated kinetic energy. We overcome this bottleneck with a novel generative pipeline based on 3D diffusion models that directly synthesizes arbitrarily large granular assemblies in their final and physically realistic configurations. The approach frames the problem as a 3D generative modeling task, consisting of a two-stage pipeline. First a diffusion model is trained to generate independent 3D voxel grids representing granular media. Second, a 3D inpainting model, adapted from 2D inpainting techniques using masked inputs, stitches these grids together seamlessly, enabling synthesis of large samples with physically realistic structure. The inpainting model explores several masking strategies for the inputs to the underlying UNets by training the network to infer missing portions of voxel grids from a concatenation of noised tensors, masks, and masked tensors as input channels. The model also adapts a 2D repainting technique of re-injecting noise scheduler output with ground truth to provide a strong guidance to the 3D model. This along with weighted losses ensures long-term coherence over generation of masked regions. Both models are trained on the same binarized 3D occupancy grids extracted from small-scale DEM simulations, achieving linear scaling of computational time with respect to sample size. Quantitatively, a 1.2 m long ballasted rail track synthesis equivalent to a 3-hour DEM simulation, was completed under 20 seconds. The generated voxel grids can also be post-processed to extract grain geometries for DEM-compatibility as well, enabling physically coherent, real-time, scalable granular media synthesis for industrial applications.</li>
<li><strong>摘要：</strong>使用离散元素方法模拟颗粒介质是一项计算密集的任务。在初始化阶段尤其如此，由于所涉及的大量位移和相关的动能，该阶段占主导地位。我们使用基于3D扩散模型的新型生成管道克服了这种瓶颈，该管道直接在其最终且物理上逼真的构型中直接综合了任意较大的颗粒组件。该方法将问题作为3D生成建模任务框架，由两阶段管道组成。首先，训练了一个扩散模型，以生成代表颗粒介质的独立3D体素电网。其次，一个使用掩盖输入的2D插入技术改编的3D涂层模型将这些网格无缝地缝合在一起，从而可以合成具有物理现实结构的大型样品。通过训练网络来推断噪声张量，口罩和掩盖张量作为输入通道的串联，通过训练网络来推断网络缺失的部分，探索了基础UNET的输入的几种掩蔽策略。该模型还适应了2D重新粉刷的技术，该技术将噪声调度程序输出重新注入地面真相，以为3D模型提供强有力的指导。这与加权损失一起确保了蒙面区域的长期连贯性。两种模型均在从小型DEM模拟中提取的相同二进制的3D占用网格上进行训练，从而相对于样本量实现了计算时间的线性缩放。在数量上，在20秒钟以下完成了相当于3小时DEM模拟的1.2 m长压载导轨合成。还可以后加工生成的体素网格以提取谷物几何形状，以实现Dem兼容性，从而实现了用于工业应用的物理相干，实时，可扩展的颗粒介质合成。</li>
</ul>

<h3>Title: The Return of Structural Handwritten Mathematical Expression Recognition</h3>
<ul>
<li><strong>Authors: </strong>Jakob Seitz, Tobias Lengfeld, Radu Timofte</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19773">https://arxiv.org/abs/2508.19773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19773">https://arxiv.org/pdf/2508.19773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19773]] The Return of Structural Handwritten Mathematical Expression Recognition(https://arxiv.org/abs/2508.19773)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Handwritten Mathematical Expression Recognition is foundational for educational technologies, enabling applications like digital note-taking and automated grading. While modern encoder-decoder architectures with large language models excel at LaTeX generation, they lack explicit symbol-to-trace alignment, a critical limitation for error analysis, interpretability, and spatially aware interactive applications requiring selective content updates. This paper introduces a structural recognition approach with two innovations: 1 an automatic annotation system that uses a neural network to map LaTeX equations to raw traces, automatically generating annotations for symbol segmentation, classification, and spatial relations, and 2 a modular structural recognition system that independently optimizes segmentation, classification, and relation prediction. By leveraging a dataset enriched with structural annotations from our auto-labeling system, the proposed recognition system combines graph-based trace sorting, a hybrid convolutional-recurrent network, and transformer-based correction to achieve competitive performance on the CROHME-2023 benchmark. Crucially, our structural recognition system generates a complete graph structure that directly links handwritten traces to predicted symbols, enabling transparent error analysis and interpretable outputs.</li>
<li><strong>摘要：</strong>手写的数学表达识别是教育技术的基础，实现了数字笔记和自动化等级等应用。尽管具有大型语言模型的现代编码器架构在乳胶生成上表现出色，但它们缺乏明确的符号对跟踪对齐，是错误分析，可解释性和空间意识的交互式应用程序的关键限制，需要选择性内容更新。本文介绍了一种结构识别方法，具有两种创新：1一个自动注释系统，该系统使用神经网络将乳胶方程映射到原始痕迹，自动生成符号分割，分类和空间关系的注释，以及2模块化结构识别系统，可独立地优化分割，分类，分类，相关性和相关性预测。通过利用来自自动标记系统的结构注释的数据集，提出的识别系统结合了基于图形的痕量排序，混合卷积反向电流网络和基于变压器的校正，以实现Crohme-2023基准的竞争性能。至关重要的是，我们的结构识别系统生成了完整的图形结构，该结构将手写的痕迹直接链接到预测的符号，从而实现透明的错误分析和可解释的输出。</li>
</ul>

<h3>Title: Not Every Gift Comes in Gold Paper or with a Red Ribbon: Exploring Color Perception in Text-to-Image Models</h3>
<ul>
<li><strong>Authors: </strong>Shay Shomer Chai, Wenxuan Peng, Bharath Hariharan, Hadar Averbuch-Elor</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19791">https://arxiv.org/abs/2508.19791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19791">https://arxiv.org/pdf/2508.19791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19791]] Not Every Gift Comes in Gold Paper or with a Red Ribbon: Exploring Color Perception in Text-to-Image Models(https://arxiv.org/abs/2508.19791)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-image generation has recently seen remarkable success, granting users with the ability to create high-quality images through the use of text. However, contemporary methods face challenges in capturing the precise semantics conveyed by complex multi-object prompts. Consequently, many works have sought to mitigate such semantic misalignments, typically via inference-time schemes that modify the attention layers of the denoising networks. However, prior work has mostly utilized coarse metrics, such as the cosine similarity between text and image CLIP embeddings, or human evaluations, which are challenging to conduct on a larger-scale. In this work, we perform a case study on colors -- a fundamental attribute commonly associated with objects in text prompts, which offer a rich test bed for rigorous evaluation. Our analysis reveals that pretrained models struggle to generate images that faithfully reflect multiple color attributes-far more so than with single-color prompts-and that neither inference-time techniques nor existing editing methods reliably resolve these semantic misalignments. Accordingly, we introduce a dedicated image editing technique, mitigating the issue of multi-object semantic alignment for prompts containing multiple colors. We demonstrate that our approach significantly boosts performance over a wide range of metrics, considering images generated by various text-to-image diffusion-based techniques.</li>
<li><strong>摘要：</strong>文本到图像生成最近已经取得了巨大的成功，使用户能够通过使用文本创建高质量的图像。但是，当代方法在捕捉复杂多对象提示传达的精确语义方面面临挑战。因此，许多作品都试图通过改变Denoising网络的注意力层的推理时间方案来减轻这种语义错位。但是，先前的工作主要利用了粗糙的指标，例如文本和图像剪辑嵌入之间的余弦相似性或人类评估，这在大规模上进行了挑战。在这项工作中，我们对颜色进行了案例研究，这是一种与文本提示中对象相关的基本属性，该属性提供了丰富的测试床进行严格的评估。我们的分析表明，经过审计的模型难以生成忠实地反映多种颜色属性的图像，而不是单色提示，并且既不可靠地解决这些语义不对准的推理时间技术或现有编辑方法。因此，我们引入了一种专用的图像编辑技术，以减轻包含多种颜色的提示的多对象语义对齐问题。我们证明，考虑到由各种基于文本图像扩散的技术产生的图像，我们的方法在广泛的指标上显着提高了性能。</li>
</ul>

<h3>Title: AutoQ-VIS: Improving Unsupervised Video Instance Segmentation via Automatic Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Kaixuan Lu, Mehmet Onurcan Kaya, Dim P. Papadopoulos</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19808">https://arxiv.org/abs/2508.19808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19808">https://arxiv.org/pdf/2508.19808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19808]] AutoQ-VIS: Improving Unsupervised Video Instance Segmentation via Automatic Quality Assessment(https://arxiv.org/abs/2508.19808)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, quality assessment</a></li>
<li><strong>Abstract: </strong>Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 $\text{AP}_{50}$ on YouTubeVIS-2019 val set, surpassing the previous state-of-the-art VideoCutLER by 4.4$\%$, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. The source code of our method is available at this https URL.</li>
<li><strong>摘要：</strong>视频实例细分（VIS）由于其对像素级面具和时间一致性标签的双重要求而面临重大注释挑战。虽然最近的无监督方法（例如VideoCuter）通过合成数据消除了光流依赖性，但它们仍受到合成到现实的域间隙的约束。我们提出了Autoq-Vis，这是一个新颖的无监督框架，它通过质量引导的自我训练来弥合这一差距。我们的方法在伪标签生成和自动质量评估之间建立了一个闭环系统，从而使从合成到真实视频进行了逐步适应。实验表明，在YouTubevis-2019 Val Set上的52.6 $ \ text {ap} _ {50} $的最新性能表明，超过了以前的最先进的视频，而不需要人类注释。这证明了质量意识的自我训练对于无监督的VIS的生存能力。我们方法的源代码可在此HTTPS URL上获得。</li>
</ul>

<h3>Title: Image Quality Assessment for Machines: Paradigm, Large-scale Database, and Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaoqi Wang, Yun Zhang, Weisi Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19850">https://arxiv.org/abs/2508.19850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19850">https://arxiv.org/pdf/2508.19850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19850]] Image Quality Assessment for Machines: Paradigm, Large-scale Database, and Models(https://arxiv.org/abs/2508.19850)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Machine vision systems (MVS) are intrinsically vulnerable to performance degradation under adverse visual conditions. To address this, we propose a machine-centric image quality assessment (MIQA) framework that quantifies the impact of image degradations on MVS performance. We establish an MIQA paradigm encompassing the end-to-end assessment workflow. To support this, we construct a machine-centric image quality database (MIQD-2.5M), comprising 2.5 million samples that capture distinctive degradation responses in both consistency and accuracy metrics, spanning 75 vision models, 250 degradation types, and three representative vision tasks. We further propose a region-aware MIQA (RA-MIQA) model to evaluate MVS visual quality through fine-grained spatial degradation analysis. Extensive experiments benchmark the proposed RA-MIQA against seven human visual system (HVS)-based IQA metrics and five retrained classical backbones. Results demonstrate RA-MIQA's superior performance in multiple dimensions, e.g., achieving SRCC gains of 13.56% on consistency and 13.37% on accuracy for image classification, while also revealing task-specific degradation sensitivities. Critically, HVS-based metrics prove inadequate for MVS quality prediction, while even specialized MIQA models struggle with background degradations, accuracy-oriented estimation, and subtle distortions. This study can advance MVS reliability and establish foundations for machine-centric image processing and optimization. The model and code are available at: this https URL.</li>
<li><strong>摘要：</strong>机器视觉系统（MV）在不利的视觉条件下本质上容易受到性能降解的影响。为了解决这个问题，我们提出了一个以机器为中心的图像质量评估（MIQA）框架，该框架量化了图像退化对MVS性能的影响。我们建立了一个涵盖端到端评估工作流程的MIQA范式。为了支持这一点，我们构建了一个以机器为中心的图像质量数据库（MIQD-2.50万），其中包括250万个样本，这些样本均在一致性和准确度量指标中捕获独特的退化响应，涵盖75种视觉模型，250个降解类型和三个代表性视觉任务。我们进一步提出了一种区域感知的MIQA（RA-MIQA）模型，以通过细粒的空间降解分析来评估MVS的视觉质量。广泛的实验基准了提议的RA-MIQA针对七个基于人类视觉系统（HVS）的IQA指标和五个训练经典骨架。结果表明，RA-MIQA在多个维度上的出色表现，例如，一致性的SRCC增长率为13.56％，图像分类的准确性为13.37％，同时也揭示了特定于任务的降级敏感性。至关重要的是，基于HVS的指标证明MVS质量预测不足，而专门的MIQA模型在背景降解，准确性的估计和微妙的扭曲方面也很难。这项研究可以提高MVS的可靠性，并为以机器为中心的图像处理和优化建立基础。该模型和代码可在以下网址提供：此HTTPS URL。</li>
</ul>

<h3>Title: Ego-centric Predictive Model Conditioned on Hand Trajectories</h3>
<ul>
<li><strong>Authors: </strong>Binjie Zhang, Mike Zheng Shou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19852">https://arxiv.org/abs/2508.19852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19852">https://arxiv.org/pdf/2508.19852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19852]] Ego-centric Predictive Model Conditioned on Hand Trajectories(https://arxiv.org/abs/2508.19852)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In egocentric scenarios, anticipating both the next action and its visual outcome is essential for understanding human-object interactions and for enabling robotic planning. However, existing paradigms fall short of jointly modeling these aspects. Vision-Language-Action (VLA) models focus on action prediction but lack explicit modeling of how actions influence the visual scene, while video prediction models generate future frames without conditioning on specific actions, often resulting in implausible or contextually inconsistent outcomes. To bridge this gap, we propose a unified two-stage predictive framework that jointly models action and visual future in egocentric scenarios, conditioned on hand trajectories. In the first stage, we perform consecutive state modeling to process heterogeneous inputs (visual observations, language, and action history) and explicitly predict future hand trajectories. In the second stage, we introduce causal cross-attention to fuse multi-modal cues, leveraging inferred action signals to guide an image-based Latent Diffusion Model (LDM) for frame-by-frame future video generation. Our approach is the first unified model designed to handle both egocentric human activity understanding and robotic manipulation tasks, providing explicit predictions of both upcoming actions and their visual consequences. Extensive experiments on Ego4D, BridgeData, and RLBench demonstrate that our method outperforms state-of-the-art baselines in both action prediction and future video synthesis.</li>
<li><strong>摘要：</strong>在以自我为中心的情况下，预期下一个动作及其视觉结果对于理解人类对象的相互作用和实现机器人计划至关重要。但是，现有的范式没有共同对这些方面进行建模。视觉语言动作（VLA）模型集中在动作预测上，但缺乏对动作如何影响视觉场景的明确建模，而视频预测模型会生成未来的帧而无需调节特定动作，通常会导致令人难以置信的或上下文上不一致的结果。为了弥合这一差距，我们提出了一个统一的两阶段预测框架，该框架在以手轨迹为条件的以自我为中心的场景中共同模拟动作和视觉未来。在第一阶段，我们执行连续的状态建模来处理异质输入（视觉观察，语言和动作历史记录），并明确预测未来的手轨迹。在第二阶段，我们将因果交叉注意力引入融合多模式线索，利用推断的动作信号指导基于图像的潜在扩散模型（LDM），以逐帧限制未来的视频生成。我们的方法是旨在处理以自我为中心的人类活动理解和机器人操纵任务的第一个统一模型，从而提供了对即将发生的动作及其视觉后果的明确预测。关于EGO4D，Bridgedata和RLBench的广泛实验表明，我们的方法在动作预测和未来视频综合中都优于最先进的基线。</li>
</ul>

<h3>Title: Quantum latent distributions in deep generative models</h3>
<ul>
<li><strong>Authors: </strong>Omar Bacarreza, Thorin Farnsworth, Alexander Makarovskiy, Hugo Wallner, Tessa Hicks, Santiago Sempere-Llagostera, John Price, Robert J. A. Francis-Jones, William R. Clements</a></li>
<li><strong>Subjects: </strong>cs.LG, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19857">https://arxiv.org/abs/2508.19857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19857">https://arxiv.org/pdf/2508.19857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19857]] Quantum latent distributions in deep generative models(https://arxiv.org/abs/2508.19857)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Many successful families of generative models leverage a low-dimensional latent distribution that is mapped to a data distribution. Though simple latent distributions are commonly used, it has been shown that more sophisticated distributions can improve performance. For instance, recent work has explored using the distributions produced by quantum processors and found empirical improvements. However, when latent space distributions produced by quantum processors can be expected to improve performance, and whether these improvements are reproducible, are open questions that we investigate in this work. We prove that, under certain conditions, these "quantum latent distributions" enable generative models to produce data distributions that classical latent distributions cannot efficiently produce. We also provide actionable intuitions to identify when such quantum advantages may arise in real-world settings. We perform benchmarking experiments on both a synthetic quantum dataset and the QM9 molecular dataset, using both simulated and real photonic quantum processors. Our results demonstrate that quantum latent distributions can lead to improved generative performance in GANs compared to a range of classical baselines. We also explore diffusion and flow matching models, identifying architectures compatible with quantum latent distributions. This work confirms that near-term quantum processors can expand the capabilities of deep generative models.</li>
<li><strong>摘要：</strong>许多生成模型的成功家族都利用了映射到数据分布的低维度分布。尽管通常使用简单的潜在分布，但已显示出更复杂的分布可以提高性能。例如，最近的工作使用量子处理器产生的分布并发现经验改进进行了探索。但是，当量子处理器产生的潜在空间分布可以提高性能，以及这些改进是否可重现的情况下，我们在这项工作中研究了开放问题。我们证明，在某些条件下，这些“量子潜在分布”使生成模型能够产生经典潜在分布无法有效产生的数据分布。我们还提供可行的直觉，以确定在现实世界中何时可能出现这种量子优势。我们使用模拟和实际光子量子处理器对合成量子数据集和QM9分子数据集进行基准测量实验。我们的结果表明，与一系列经典基线相比，量子潜在分布可以改善gan的生成性能。我们还探索扩散和流匹配模型，识别与量子潜在分布兼容的体系结构。这项工作证实，近期量子处理器可以扩大深层生成模型的功能。</li>
</ul>

<h3>Title: Multimodal Conditional MeshGAN for Personalized Aneurysm Growth Prediction</h3>
<ul>
<li><strong>Authors: </strong>Long Chen, Ashiv Patel, Mengyun Qiao, Mohammad Yousuf Salmasi, Salah A. Hammouche, Vasilis Stavrinides, Jasleen Nagi, Soodeh Kalaie, Xiao Yun Xu, Wenjia Bai, Declan P. O'Regan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19862">https://arxiv.org/abs/2508.19862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19862">https://arxiv.org/pdf/2508.19862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19862]] Multimodal Conditional MeshGAN for Personalized Aneurysm Growth Prediction(https://arxiv.org/abs/2508.19862)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Personalized, accurate prediction of aortic aneurysm progression is essential for timely intervention but remains challenging due to the need to model both subtle local deformations and global anatomical changes within complex 3D geometries. We propose MCMeshGAN, the first multimodal conditional mesh-to-mesh generative adversarial network for 3D aneurysm growth prediction. MCMeshGAN introduces a dual-branch architecture combining a novel local KNN-based convolutional network (KCN) to preserve fine-grained geometric details and a global graph convolutional network (GCN) to capture long-range structural context, overcoming the over-smoothing limitations of deep GCNs. A dedicated condition branch encodes clinical attributes (age, sex) and the target time interval to generate anatomically plausible, temporally controlled predictions, enabling retrospective and prospective modeling. We curated TAAMesh, a new longitudinal thoracic aortic aneurysm mesh dataset consisting of 590 multimodal records (CT scans, 3D meshes, and clinical data) from 208 patients. Extensive experiments demonstrate that MCMeshGAN consistently outperforms state-of-the-art baselines in both geometric accuracy and clinically important diameter estimation. This framework offers a robust step toward clinically deployable, personalized 3D disease trajectory modeling. The source code for MCMeshGAN and the baseline methods is publicly available at this https URL.</li>
<li><strong>摘要：</strong>个性化的，准确的主动脉瘤进展预测对于及时干预至关重要，但由于需要对复杂的3D几何形状内的细微局部变形和全局解剖变化进行建模，因此仍然具有挑战性。我们提出了McMeshgan，这是第一个用于3D动脉瘤增长预测的多模式的条件网格到网格生成的对抗网络。 McMeshgan引入了一个双分支结构，结合了新型的基于KNN的卷积网络（KCN），以保留细粒的几何细节和全球图卷积网络（GCN），以捕获远程结构环境，克服了深GCN的过度光滑限制。专用条件分支编码临床属性（年龄，性别）和目标时间间隔，以产生解剖学上合理的，时间控制的预测，从而实现回顾性和前瞻性建模。我们策划了Taamesh，这是一种新的纵向胸动脉瘤网格数据集，该数据集由208名患者的590个多模式记录（CT扫描，3D网格和临床数据）组成。广泛的实验表明，在几何精度和临床上重要的直径估计中，McMeshgan始终优于最先进的基线。该框架为临床可部署的个性化3D疾病轨迹建模提供了强大的步骤。 McMeshgan的源代码和基线方法可在此HTTPS URL上公开获得。</li>
</ul>

<h3>Title: PersonaAnimator: Personalized Motion Transfer from Unconstrained Videos</h3>
<ul>
<li><strong>Authors: </strong>Ziyun Qian, Runyu Xiao, Shuyuan Tu, Wei Xue, Dingkang Yang, Mingcheng Li, Dongliang Kou, Minghao Han, Zizhi Chen, Lihua Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19895">https://arxiv.org/abs/2508.19895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19895">https://arxiv.org/pdf/2508.19895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19895]] PersonaAnimator: Personalized Motion Transfer from Unconstrained Videos(https://arxiv.org/abs/2508.19895)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in motion generation show remarkable progress. However, several limitations remain: (1) Existing pose-guided character motion transfer methods merely replicate motion without learning its style characteristics, resulting in inexpressive characters. (2) Motion style transfer methods rely heavily on motion capture data, which is difficult to obtain. (3) Generated motions sometimes violate physical laws. To address these challenges, this paper pioneers a new task: Video-to-Video Motion Personalization. We propose a novel framework, PersonaAnimator, which learns personalized motion patterns directly from unconstrained videos. This enables personalized motion transfer. To support this task, we introduce PersonaVid, the first video-based personalized motion dataset. It contains 20 motion content categories and 120 motion style categories. We further propose a Physics-aware Motion Style Regularization mechanism to enforce physical plausibility in the generated motions. Extensive experiments show that PersonaAnimator outperforms state-of-the-art motion transfer methods and sets a new benchmark for the Video-to-Video Motion Personalization task.</li>
<li><strong>摘要：</strong>运动产生的最新进展表现出了显着的进步。但是，仍然存在几个局限性：（1）现有的姿势指导的角色运动传递方法仅复制运动而不学习其样式特征，从而导致特征不足。 （2）运动样式转移方法在很大程度上依赖于运动捕获数据，这很难获得。 （3）产生的动作有时会违反物理定律。为了应对这些挑战，本文开创了一项新任务：视频到视频运动个性化。我们提出了一个新颖的框架，即Personaanimator，该框架直接从无约束的视频中学习了个性化的运动模式。这使得个性化运动转移。为了支持此任务，我们介绍了第一个基于视频的个性化运动数据集Persopavid。它包含20个运动内容类别和120个运动样式类别。我们进一步提出了一种物理感知的运动样式正规化机制，以在生成的动作中实现身体上的合理性。广泛的实验表明，角色映射器的表现优于最先进的运动转移方法，并为视频到视频运动个性化任务设置了新的基准。</li>
</ul>

<h3>Title: WaveHiT-SR: Hierarchical Wavelet Network for Efficient Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Fayaz Ali, Muhammad Zawish, Steven Davy, Radu Timofte</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19927">https://arxiv.org/abs/2508.19927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19927">https://arxiv.org/pdf/2508.19927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19927]] WaveHiT-SR: Hierarchical Wavelet Network for Efficient Image Super-Resolution(https://arxiv.org/abs/2508.19927)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Transformers have demonstrated promising performance in computer vision tasks, including image super-resolution (SR). The quadratic computational complexity of window self-attention mechanisms in many transformer-based SR methods forces the use of small, fixed windows, limiting the receptive field. In this paper, we propose a new approach by embedding the wavelet transform within a hierarchical transformer framework, called (WaveHiT-SR). First, using adaptive hierarchical windows instead of static small windows allows to capture features across different levels and greatly improve the ability to model long-range dependencies. Secondly, the proposed model utilizes wavelet transforms to decompose images into multiple frequency subbands, allowing the network to focus on both global and local features while preserving structural details. By progressively reconstructing high-resolution images through hierarchical processing, the network reduces computational complexity without sacrificing performance. The multi-level decomposition strategy enables the network to capture fine-grained information in lowfrequency components while enhancing high-frequency textures. Through extensive experimentation, we confirm the effectiveness and efficiency of our WaveHiT-SR. Our refined versions of SwinIR-Light, SwinIR-NG, and SRFormer-Light deliver cutting-edge SR results, achieving higher efficiency with fewer parameters, lower FLOPs, and faster speeds.</li>
<li><strong>摘要：</strong>变形金刚在计算机视觉任务中表现出了有希望的性能，包括图像超分辨率（SR）。许多基于变压器的SR方法中窗口自发机制的二次计算复杂性迫使小固定窗户的使用限制了接受场。在本文中，我们通过将小波变换嵌入层次变压器框架中，称为（WaveHit-SR），提出了一种新方法。首先，使用自适应分层窗口而不是静态小窗口可以捕获不同级别的功能，并大大提高了建模远程依赖性的能力。其次，所提出的模型利用小波变换将图像分解为多个频率子带，从而使网络可以专注于全球和本地特征，同时保留结构细节。通过通过层次处理逐步重建高分辨率图像，网络可以降低计算复杂性而无需牺牲性能。多级分解策略使网络能够在低频组件中捕获细粒度的信息，同时增强高频纹理。通过广泛的实验，我们确认了WaveHit-SR的有效性和效率。我们精致的版本的Swinir-Light，Swinir-ng和Srformer-Light可提供最先进的SR结果，从而实现更高的效率，较少的参数，较低的失败和更快的速度。</li>
</ul>

<h3>Title: KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts</h3>
<ul>
<li><strong>Authors: </strong>Taebaek Hwang, Minseo Kim, Gisang Lee, Seonuk Kim, Hyunjun Eun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19944">https://arxiv.org/abs/2508.19944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19944">https://arxiv.org/pdf/2508.19944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19944]] KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts(https://arxiv.org/abs/2508.19944)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Understanding and reasoning over text within visual contexts poses a significant challenge for Vision-Language Models (VLMs), given the complexity and diversity of real-world scenarios. To address this challenge, text-rich Visual Question Answering (VQA) datasets and benchmarks have emerged for high-resource languages like English. However, a critical gap persists for low-resource languages such as Korean, where the lack of comprehensive benchmarks hinders robust model evaluation and comparison. To bridge this gap, we introduce KRETA, a benchmark for Korean Reading and rEasoning in Text-rich VQA Attuned to diverse visual contexts. KRETA facilitates an in-depth evaluation of both visual text understanding and reasoning capabilities, while also supporting a multifaceted assessment across 15 domains and 26 image types. Additionally, we introduce a semi-automated VQA generation pipeline specifically optimized for text-rich settings, leveraging refined stepwise image decomposition and a rigorous seven-metric evaluation protocol to ensure data quality. While KRETA is tailored for Korean, we hope our adaptable and extensible pipeline will facilitate the development of similar benchmarks in other languages, thereby accelerating multilingual VLM research. The code and dataset for KRETA are available at this https URL.</li>
<li><strong>摘要：</strong>考虑到现实世界情景的复杂性和多样性，对视觉上下文中文本的理解和推理对视觉模型（VLM）构成了重大挑战。为了应对这一挑战，文本丰富的视觉问题回答（VQA）数据集和基准已经出现在英语（例如英语）中。但是，对于韩文等低资源语言的关键差距仍然存在，因为缺乏全面的基准阻碍了强大的模型评估和比较。为了弥合这一差距，我们介绍了Kreta，这是韩国阅读和推理的基准，这些文字富含文字的VQA适应了各种视觉上下文。 Kreta促进了对视觉文本理解和推理功能的深入评估，同时还支持15个域和26种图像类型的多方面评估。此外，我们引入了专门针对文本丰富的设置进行了专门优化的半自动化VQA生成管道，利用精致的逐步图像分解和严格的七个金属评估协议以确保数据质量。尽管Kreta是针对韩国人量身定制的，但我们希望我们的适应性和可扩展的管道能够促进其他语言中类似基准的开发，从而加快多语言VLM研究。 Kreta的代码和数据集可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Assessing the Geolocation Capabilities, Limitations and Societal Risks of Generative Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Oliver Grainge, Sania Waheed, Jack Stilgoe, Michael Milford, Shoaib Ehsan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19967">https://arxiv.org/abs/2508.19967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19967">https://arxiv.org/pdf/2508.19967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19967]] Assessing the Geolocation Capabilities, Limitations and Societal Risks of Generative Vision-Language Models(https://arxiv.org/abs/2508.19967)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Geo-localization is the task of identifying the location of an image using visual cues alone. It has beneficial applications, such as improving disaster response, enhancing navigation, and geography education. Recently, Vision-Language Models (VLMs) are increasingly demonstrating capabilities as accurate image geo-locators. This brings significant privacy risks, including those related to stalking and surveillance, considering the widespread uses of AI models and sharing of photos on social media. The precision of these models is likely to improve in the future. Despite these risks, there is little work on systematically evaluating the geolocation precision of Generative VLMs, their limits and potential for unintended inferences. To bridge this gap, we conduct a comprehensive assessment of the geolocation capabilities of 25 state-of-the-art VLMs on four benchmark image datasets captured in diverse environments. Our results offer insight into the internal reasoning of VLMs and highlight their strengths, limitations, and potential societal risks. Our findings indicate that current VLMs perform poorly on generic street-level images yet achieve notably high accuracy (61\%) on images resembling social media content, raising significant and urgent privacy concerns.</li>
<li><strong>摘要：</strong>地理定位是仅使用视觉提示识别图像的位置的任务。它具有有益的应用，例如改善灾难响应，增强导航和地理教育。最近，视觉模型（VLM）越来越多地证明了作为准确的图像地理位置的功能。考虑到广泛使用AI模型并在社交媒体上共享照片，这带来了很大的隐私风险，包括与跟踪和监视有关的风险。这些模型的精度可能会在未来提高。尽管有这些风险，但在系统地评估生成VLM的地理分配精度，它们的限制和意外推论的潜力方面几乎没有工作。为了弥合这一差距，我们对在不同环境中捕获的四个基准图像数据集上对25个最先进的VLM的地理位置功能进行了全面评估。我们的结果为VLM的内部推理提供了深入的了解，并突出了它们的优势，局限性和潜在的社会风险。我们的发现表明，当前的VLM在通用的街道级图像上的表现较差，但在类似社交媒体内容的图像上获得了高度准确性（61 \％），从而引起了重大而紧急的隐私问题。</li>
</ul>

<h3>Title: GS: Generative Segmentation via Label Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Yuhao Chen, Shubin Chen, Liang Lin, Guangrun Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20020">https://arxiv.org/abs/2508.20020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20020">https://arxiv.org/pdf/2508.20020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20020]] GS: Generative Segmentation via Label Diffusion(https://arxiv.org/abs/2508.20020)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Language-driven image segmentation is a fundamental task in vision-language understanding, requiring models to segment regions of an image corresponding to natural language expressions. Traditional methods approach this as a discriminative problem, assigning each pixel to foreground or background based on semantic alignment. Recently, diffusion models have been introduced to this domain, but existing approaches remain image-centric: they either (i) use image diffusion models as visual feature extractors, (ii) synthesize segmentation data via image generation to train discriminative models, or (iii) perform diffusion inversion to extract attention cues from pre-trained image diffusion models-thereby treating segmentation as an auxiliary process. In this paper, we propose GS (Generative Segmentation), a novel framework that formulates segmentation itself as a generative task via label diffusion. Instead of generating images conditioned on label maps and text, GS reverses the generative process: it directly generates segmentation masks from noise, conditioned on both the input image and the accompanying language description. This paradigm makes label generation the primary modeling target, enabling end-to-end training with explicit control over spatial and semantic fidelity. To demonstrate the effectiveness of our approach, we evaluate GS on Panoptic Narrative Grounding (PNG), a representative and challenging benchmark for multimodal segmentation that requires panoptic-level reasoning guided by narrative captions. Experimental results show that GS significantly outperforms existing discriminative and diffusion-based methods, setting a new state-of-the-art for language-driven segmentation.</li>
<li><strong>摘要：</strong>语言驱动的图像分割是视觉理解中的一项基本任务，需要模型才能分割与自然语言表达式相对应的图像区域。传统方法将其视为一个歧视性问题，将每个像素分配给基于语义一致性的前景或背景。 Recently, diffusion models have been introduced to this domain, but existing approaches remain image-centric: they either (i) use image diffusion models as visual feature extractors, (ii) synthesize segmentation data via image generation to train discriminative models, or (iii) perform diffusion inversion to extract attention cues from pre-trained image diffusion models-thereby treating segmentation as an auxiliary process.在本文中，我们提出了GS（生成分割），这是一个新颖的框架，通过标签扩散将分割本身作为生成任务制定。 GS没有生成在标签地图和文本上的图像，而是逆转生成过程：它直接从噪声中生成分割掩码，并在输入映像和随附的语言描述上进行条件。该范式使标签生成成为主要的建模目标，从而实现了对空间和语义忠诚的明确控制。为了证明我们的方法的有效性，我们评估了GS对Panoptic叙事基础（PNG），这是多模式分割的代表性且具有挑战性的基准，需要以叙事字幕为指导的全层水平推理。实验结果表明，GS明显优于现有的歧视性和基于扩散的方法，为语言驱动的分段设定了新的最新技术。</li>
</ul>

<h3>Title: Using item recommendations and LLMs in marketing email titles</h3>
<ul>
<li><strong>Authors: </strong>Deddy Jobson, Muktti Shukla, Phuong Dinh, Julio Christian Young, Nick Pitton, Nina Chen, Ryan Ginstrom</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20024">https://arxiv.org/abs/2508.20024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20024">https://arxiv.org/pdf/2508.20024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20024]] Using item recommendations and LLMs in marketing email titles(https://arxiv.org/abs/2508.20024)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>E-commerce marketplaces make use of a number of marketing channels like emails, push notifications, etc. to reach their users and stimulate purchases. Personalized emails especially are a popular touch point for marketers to inform users of latest items in stock, especially for those who stopped visiting the marketplace. Such emails contain personalized recommendations tailored to each user's interests, enticing users to buy relevant items. A common limitation of these emails is that the primary entry point, the title of the email, tends to follow fixed templates, failing to inspire enough interest in the contents. In this work, we explore the potential of large language models (LLMs) for generating thematic titles that reflect the personalized content of the emails. We perform offline simulations and conduct online experiments on the order of millions of users, finding our techniques useful in improving the engagement between customers and our emails. We highlight key findings and learnings as we productionize the safe and automated generation of email titles for millions of users.</li>
<li><strong>摘要：</strong>电子商务市场利用了许多营销渠道，例如电子邮件，推送通知等，以吸引用户并刺激购买。个性化的电子邮件尤其是营销人员通知用户最新库存的流行接触点，尤其是对于那些停止参观市场的人。此类电子邮件包含针对每个用户兴趣的个性化建议，诱使用户购买相关商品。这些电子邮件的一个普遍局限性是，电子邮件的标题倾向于遵循固定模板，未能激发对内容的足够兴趣。在这项工作中，我们探讨了大语模型（LLMS）生成主题标题的潜力，以反映电子邮件的个性化内容。我们进行离线模拟，并根据数百万用户的顺序进行在线实验，发现我们的技术可用于改善客户与电子邮件之间的参与度。当我们为数百万用户生产安全和自动化的电子邮件标题时，我们突出了关键的发现和学习。</li>
</ul>

<h3>Title: OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations</h3>
<ul>
<li><strong>Authors: </strong>Peng-Hao Hsu, Ke Zhang, Fu-En Wang, Tao Tu, Ming-Feng Li, Yu-Lun Liu, Albert Y. C. Chen, Min Sun, Cheng-Hao Kuo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20063">https://arxiv.org/abs/2508.20063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20063">https://arxiv.org/pdf/2508.20063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20063]] OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations(https://arxiv.org/abs/2508.20063)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Open-vocabulary (OV) 3D object detection is an emerging field, yet its exploration through image-based methods remains limited compared to 3D point cloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view indoor 3D object detector trained without human annotations. In particular, OpenM3D is a single-stage detector adapting the 2D-induced voxel features from the ImGeoNet model. To support OV, it is jointly trained with a class-agnostic 3D localization loss requiring high-quality 3D pseudo boxes and a voxel-semantic alignment loss requiring diverse pre-trained CLIP features. We follow the training setting of OV-3DET where posed RGB-D images are given but no human annotations of 3D boxes or classes are available. We propose a 3D Pseudo Box Generation method using a graph embedding technique that combines 2D segments into coherent 3D structures. Our pseudo-boxes achieve higher precision and recall than other methods, including the method proposed in OV-3DET. We further sample diverse CLIP features from 2D segments associated with each coherent 3D structure to align with the corresponding voxel feature. The key to training a highly accurate single-stage detector requires both losses to be learned toward high-quality targets. At inference, OpenM3D, a highly efficient detector, requires only multi-view images for input and demonstrates superior accuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor benchmarks compared to existing methods. We outperform a strong two-stage method that leverages our class-agnostic detector with a ViT CLIP-based OV classifier and a baseline incorporating multi-view depth estimator on both accuracy and speed.</li>
<li><strong>摘要：</strong>开放式摄影库（OV）3D对象检测是一个新兴领域，但与基于3D点云的方法相比，其通过基于图像的方法的探索仍然有限。我们介绍了OpenM3D，这是一种新型的开放式摄影库多视图室内3D对象检测器，未经人为注释。特别是，OpenM3D是一个单阶段检测器，可适应Imgeonet模型的2D诱导的体素特征。为了支持OV，它通过类不足的3D定位损失进行了共同训练，需要高质量的3D伪盒和Voxel-Smantic-Enantic Arignmant损失，需要多样化的预训练的剪辑功能。我们遵循OV-3DET的训练设置，在其中给出了带姿势的RGB-D图像，但没有3D盒或类的人类注释。我们使用图形嵌入技术提出了一种3D伪盒生成方法，该技术将2D段结合到连贯的3D结构中。与其他方法相比，我们的伪盒获得了更高的精度和回忆，包括OV-3DET中提出的方法。我们进一步采样了与每个相干3D结构相关的2D段的不同夹子特征，以与相应的素特征对齐。训练高度准确的单阶段检测器的关键需要朝着高质量目标学习两种损失。在推断时，一个高效的检测器OpenM3D仅需要多视图图像以进行输入，并在Scannet200和Arkitscenes室内基准中与现有方法相比，在Scannet200上显示出了卓越的准确性和速度（每场景0.3秒）。我们胜过强大的两阶段方法，该方法利用基于VIT夹的OV分类器来利用我们的类不足的检测器，并且基线在精度和速度上都结合了多视图深度估计器。</li>
</ul>

<h3>Title: Patch Progression Masked Autoencoder with Fusion CNN Network for Classifying Evolution Between Two Pairs of 2D OCT Slices</h3>
<ul>
<li><strong>Authors: </strong>Philippe Zhang, Weili Jiang, Yihao Li, Jing Zhang, Sarah Matta, Yubo Tan, Hui Lin, Haoshen Wang, Jiangtian Pan, Hui Xu, Laurent Borderie, Alexandre Le Guilcher, Béatrice Cochener, Chubin Ou, Gwenolé Quellec, Mathieu Lamard</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20064">https://arxiv.org/abs/2508.20064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20064">https://arxiv.org/pdf/2508.20064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20064]] Patch Progression Masked Autoencoder with Fusion CNN Network for Classifying Evolution Between Two Pairs of 2D OCT Slices(https://arxiv.org/abs/2508.20064)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Age-related Macular Degeneration (AMD) is a prevalent eye condition affecting visual acuity. Anti-vascular endothelial growth factor (anti-VEGF) treatments have been effective in slowing the progression of neovascular AMD, with better outcomes achieved through timely diagnosis and consistent monitoring. Tracking the progression of neovascular activity in OCT scans of patients with exudative AMD allows for the development of more personalized and effective treatment plans. This was the focus of the Monitoring Age-related Macular Degeneration Progression in Optical Coherence Tomography (MARIO) challenge, in which we participated. In Task 1, which involved classifying the evolution between two pairs of 2D slices from consecutive OCT acquisitions, we employed a fusion CNN network with model ensembling to further enhance the model's performance. For Task 2, which focused on predicting progression over the next three months based on current exam data, we proposed the Patch Progression Masked Autoencoder that generates an OCT for the next exam and then classifies the evolution between the current OCT and the one generated using our solution from Task 1. The results we achieved allowed us to place in the Top 10 for both tasks. Some team members are part of the same organization as the challenge organizers; therefore, we are not eligible to compete for the prize.</li>
<li><strong>摘要：</strong>与年龄相关的黄斑变性（AMD）是影响视敏度的普遍眼病。抗血管内皮生长因子（抗VEGF）治疗在减慢新血管AMD的进展方面有效，通过及时诊断和一致的监测来取得更好的结果。跟踪OCT AMD患者的OCT扫描中新血管活性的进展，可以制定更个性化和有效的治疗计划。这是我们参与的光学相干断层扫描（Mario）挑战中与年龄相关的黄斑变性进展的重点。在任务1中，涉及将连续OCT收购的两对2D切片之间的演变进行分类，我们采用了一个融合CNN网络，其模型结合了，以进一步提高模型的性能。对于任务2，该任务是根据当前的考试数据预测未来三个月的进展，我们提出了贴片进度掩盖的自动编码器，该贴片进度为下一次考试生成了OCT，然后对当前OCT和使用我们从任务1产生的解决方案产生的演变进行了分类。我们获得的结果使我们能够在两项任务中获得前10名。一些团队成员与挑战组织者是同一组织的一部分。因此，我们没有资格争夺该奖项。</li>
</ul>

<h3>Title: AudioStory: Generating Long-Form Narrative Audio with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Guo, Teng Wang, Yuying Ge, Shijie Ma, Yixiao Ge, Wei Zou, Ying Shan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20088">https://arxiv.org/abs/2508.20088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20088">https://arxiv.org/pdf/2508.20088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20088]] AudioStory: Generating Long-Form Narrative Audio with Large Language Models(https://arxiv.org/abs/2508.20088)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in text-to-audio (TTA) generation excel at synthesizing short audio clips but struggle with long-form narrative audio, which requires temporal coherence and compositional reasoning. To address this gap, we propose AudioStory, a unified framework that integrates large language models (LLMs) with TTA systems to generate structured, long-form audio narratives. AudioStory possesses strong instruction-following reasoning generation capabilities. It employs LLMs to decompose complex narrative queries into temporally ordered sub-tasks with contextual cues, enabling coherent scene transitions and emotional tone consistency. AudioStory has two appealing features: (1) Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser collaboration into two specialized components, i.e., a bridging query for intra-event semantic alignment and a residual query for cross-event coherence preservation. (2) End-to-end training: By unifying instruction comprehension and audio generation within a single end-to-end framework, AudioStory eliminates the need for modular training pipelines while enhancing synergy between components. Furthermore, we establish a benchmark AudioStory-10K, encompassing diverse domains such as animated soundscapes and natural sound narratives. Extensive experiments show the superiority of AudioStory on both single-audio generation and narrative audio generation, surpassing prior TTA baselines in both instruction-following ability and audio fidelity. Our code is available at this https URL</li>
<li><strong>摘要：</strong>文本到原告（TTA）一代的最新进展在综合简短的音频剪辑方面表现出色，但与长形式的叙事音频斗争，这需要时间连贯性和构图推理。为了解决这一差距，我们提出了Audiostory，这是一个将大型语言模型（LLMS）与TTA系统集成在一起的统一框架，以生成结构化的，长形式的音频叙事。 AudioStory具有强大的指导性推理生成能力。它采用LLM将复杂的叙事查询分解为具有上下文提示的时间订购的子任务，从而实现了连贯的场景过渡和情感语调的一致性。 AudioStory具有两个吸引人的功能：（1）解耦桥接机制：AudioSostory Distangles LLM-Diffuser合作，分为两个专业组件，即，对于事实内的语义上的桥接查询，以及用于跨事物连贯性保存的残留语义查询。 （2）端到端培训：通过在单个端到端框架内统一指令理解和音频生成，AudioSostory消除了对模块化训练管道的需求，同时增强了组件之间的协同作用。此外，我们建立了一个基准Audiostory-10k，其中包括动画的音景和自然声音叙事等各种领域。广泛的实验表明，在单声道产生和叙事音频产生方面，有效性的优势在遵循教学能力和音频保真度中超过了先前的TTA基准。我们的代码可在此HTTPS URL上找到</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
