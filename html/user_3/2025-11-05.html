<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-11-05</h1>
<h3>Title: CudaForge: An Agent Framework with Hardware Feedback for CUDA Kernel Optimization</h3>
<ul>
<li><strong>Authors: </strong>Zijian Zhang, Rong Wang, Shiyang Li, Yuebo Luo, Mingyi Hong, Caiwen Ding</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01884">https://arxiv.org/abs/2511.01884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01884">https://arxiv.org/pdf/2511.01884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01884]] CudaForge: An Agent Framework with Hardware Feedback for CUDA Kernel Optimization(https://arxiv.org/abs/2511.01884)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Developing efficient CUDA kernels is increasingly critical for AI applications such as large-scale LLM training. However, manual kernel design is both costly and time-consuming, motivating automatic approaches that leverage LLMs for code generation. Existing methods for automatic kernel generation, however, often produce low-efficiency kernels, incur high computational overhead, and fail to generalize across settings. In this work, we propose CudaForge, a training-free multi-agent workflow for CUDA kernel generation and optimization. Our workflow is inspired by the iterative workflow of human experts, which contains steps such as developing initial kernels, testing correctness, analyzing hardware feedback, and iterative improvement. More specifically, CudaForge employs two LLM agents: a Coder and a Judge, that iteratively generate, correct, and optimize CUDA kernels, while integrating hardware feedback such as Nsight Compute (NCU) metrics. In extensive evaluations, we show that CudaForge, by leveraging base models like OpenAI-o3, achieves 97.6\% correctness of generated kernels and an average 1.68$\times$ speedup over PyTorch baselines, substantially surpassing state-of-the-art models including OpenAI-o3 and Kevin on KernelBench. Beyond accuracy and speed, CudaForge demonstrates strong generalization across GPUs (A100, RTX 6000, 4090, 3090) and base models (OpenAI-o3, GPT-5, gpt-oss-120B, Claude-Sonnet-4, QwQ-32B), while maintaining high efficiency. In particular, generating an optimized kernel takes about 26.5 minutes on one RTX6000 and incurs about \$ 0.3 API cost, which is significantly cheaper than existing agentic work that costs 6 H100 hours and \$ 5 API cost per kernel. Our results highlight that multi-agent, training-free workflows can enable cost-effective, generalizable, and high-performance CUDA kernel optimization. Code available at this https URL</li>
<li><strong>摘要：</strong>开发高效的 CUDA 内核对于大规模 LLM 培训等人工智能应用越来越重要。然而，手动内核设计既昂贵又耗时，这促使人们采用利用 LLM 进行代码生成的自动方法。然而，现有的自动内核生成方法通常会产生低效率的内核，产生较高的计算开销，并且无法跨设置泛化。在这项工作中，我们提出了 CudaForge，一种用于 CUDA 内核生成和优化的免训练多智能体工作流程。我们的工作流程受到人类专家迭代工作流程的启发，其中包含开发初始内核、测试正确性、分析硬件反馈和迭代改进等步骤。更具体地说，CudaForge 使用两个 LLM 代理：编码器和判断器，迭代生成、纠正和优化 CUDA 内核，同时集成 Nsight 计算 (NCU) 指标等硬件反馈。在广泛的评估中，我们表明，CudaForge 通过利用 OpenAI-o3 等基本模型，生成的内核的正确率达到 97.6%，并且比 PyTorch 基线平均加速 1.68$\times$，大大超过了 KernelBench 上包括 OpenAI-o3 和 Kevin 在内的最先进模型。除了准确性和速度之外，CudaForge 还展示了跨 GPU（A100、RTX 6000、4090、3090）和基本模型（OpenAI-o3、GPT-5、gpt-oss-120B、Claude-Sonnet-4、QwQ-32B）的强大泛化能力，同时保持了高效率。特别是，在一个 RTX6000 上生成优化内核大约需要 26.5 分钟，并产生约 0.3 美元的 API 成本，这比现有的代理工作要便宜得多，现有的代理工作需要 6 H100 小时和每个内核 5 美元的 API 成本。我们的结果强调，多代理、免训练工作流程可以实现经济高效、可通用且高性能的 CUDA 内核优化。代码可在此 https URL 获取</li>
</ul>

<h3>Title: Retrieval-Augmented Multimodal Depression Detection</h3>
<ul>
<li><strong>Authors: </strong>Ruibo Hou, Shiyu Teng, Jiaqing Liu, Shurong Chai, Yinhao Li, Lanfen Lin, Yen-Wei Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01892">https://arxiv.org/abs/2511.01892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01892">https://arxiv.org/pdf/2511.01892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01892]] Retrieval-Augmented Multimodal Depression Detection(https://arxiv.org/abs/2511.01892)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal deep learning has shown promise in depression detection by integrating text, audio, and video signals. Recent work leverages sentiment analysis to enhance emotional understanding, yet suffers from high computational cost, domain mismatch, and static knowledge limitations. To address these issues, we propose a novel Retrieval-Augmented Generation (RAG) framework. Given a depression-related text, our method retrieves semantically relevant emotional content from a sentiment dataset and uses a Large Language Model (LLM) to generate an Emotion Prompt as an auxiliary modality. This prompt enriches emotional representation and improves interpretability. Experiments on the AVEC 2019 dataset show our approach achieves state-of-the-art performance with CCC of 0.593 and MAE of 3.95, surpassing previous transfer learning and multi-task learning baselines.</li>
<li><strong>摘要：</strong>多模态深度学习通过集成文本、音频和视频信号在抑郁症检测中显示出前景。最近的工作利用情感分析来增强情感理解，但面临计算成本高、领域不匹配和静态知识限制的问题。为了解决这些问题，我们提出了一种新颖的检索增强生成（RAG）框架。给定与抑郁症相关的文本，我们的方法从情感数据集中检索语义相关的情感内容，并使用大型语言模型（LLM）生成情感提示作为辅助模态。这种提示丰富了情感表达并提高了可解释性。 AVEC 2019 数据集上的实验表明，我们的方法实现了最先进的性能，CCC 为 0.593，MAE 为 3.95，超越了之前的迁移学习和多任务学习基线。</li>
</ul>

<h3>Title: iFlyBot-VLA Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Yuan Zhang, Chenyu Xue, Wenjie Xu, Chao Ji, Jiajia wu, Jia Pan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01914">https://arxiv.org/abs/2511.01914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01914">https://arxiv.org/pdf/2511.01914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01914]] iFlyBot-VLA Technical Report(https://arxiv.org/abs/2511.01914)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce iFlyBot-VLA, a large-scale Vision-Language-Action (VLA) model trained under a novel framework. The main contributions are listed as follows: (1) a latent action model thoroughly trained on large-scale human and robotic manipulation videos; (2) a dual-level action representation framework that jointly supervises both the Vision-Language Model (VLM) and the action expert during training; (3) a mixed training strategy that combines robot trajectory data with general QA and spatial QA datasets, effectively enhancing the 3D perceptual and reasoning capabilities of the VLM backbone. Specifically, the VLM is trained to predict two complementary forms of actions: latent actions, derived from our latent action model pretrained on cross-embodiment manipulation data, which capture implicit high-level intentions; and structured discrete action tokens, obtained through frequency-domain transformations of continuous control signals, which encode explicit low-level dynamics. This dual supervision aligns the representation spaces of language, vision, and action, enabling the VLM to directly contribute to action generation. Experimental results on the LIBERO Franka benchmark demonstrate the superiority of our frame-work, while real-world evaluations further show that iFlyBot-VLA achieves competitive success rates across diverse and challenging manipulation tasks. Furthermore, we plan to open-source a portion of our self-constructed dataset to support future research in the community</li>
<li><strong>摘要：</strong>我们介绍了 iFlyBot-VLA，这是一种在新颖框架下训练的大规模视觉-语言-动作（VLA）模型。主要贡献如下：（1）在大规模人类和机器人操作视频上彻底训练的潜在动作模型； （2）双层动作表示框架，在训练过程中共同监督视觉语言模型（VLM）和动作专家； （3）混合训练策略，将机器人轨迹数据与通用QA和空间QA数据集相结合，有效增强VLM主干的3D感知和推理能力。具体来说，VLM 被训练来预测两种互补形式的动作：潜在动作，源自我们在跨实施例操作数据上预训练的潜在动作模型，它捕获隐含的高级意图；以及通过连续控制信号的频域变换获得的结构化离散动作令牌，这些令牌编码显式的低级动态。这种双重监督调整了语言、视觉和动作的表示空间，使 VLM 能够直接促进动作的生成。 LIBERO Franka 基准测试的实验结果证明了我们框架的优越性，而现实世界的评估进一步表明 iFlyBot-VLA 在各种具有挑战性的操作任务中实现了具有竞争力的成功率。此外，我们计划开源部分自建数据集，以支持社区未来的研究</li>
</ul>

<h3>Title: Dynamic Population Distribution Aware Human Trajectory Generation with Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Qingyue Long, Can Rong, Tong Li, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01929">https://arxiv.org/abs/2511.01929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01929">https://arxiv.org/pdf/2511.01929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01929]] Dynamic Population Distribution Aware Human Trajectory Generation with Diffusion Model(https://arxiv.org/abs/2511.01929)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Human trajectory data is crucial in urban planning, traffic engineering, and public health. However, directly using real-world trajectory data often faces challenges such as privacy concerns, data acquisition costs, and data quality. A practical solution to these challenges is trajectory generation, a method developed to simulate human mobility behaviors. Existing trajectory generation methods mainly focus on capturing individual movement patterns but often overlook the influence of population distribution on trajectory generation. In reality, dynamic population distribution reflects changes in population density across different regions, significantly impacting individual mobility behavior. Thus, we propose a novel trajectory generation framework based on a diffusion model, which integrates the dynamic population distribution constraints to guide high-fidelity generation outcomes. Specifically, we construct a spatial graph to enhance the spatial correlation of trajectories. Then, we design a dynamic population distribution aware denoising network to capture the spatiotemporal dependencies of human mobility behavior as well as the impact of population distribution in the denoising process. Extensive experiments show that the trajectories generated by our model can resemble real-world trajectories in terms of some critical statistical metrics, outperforming state-of-the-art algorithms by over 54%.</li>
<li><strong>摘要：</strong>人类轨迹数据对于城市规划、交通工程和公共卫生至关重要。然而，直接使用现实世界的轨迹数据往往面临隐私问题、数据获取成本和数据质量等挑战。应对这些挑战的一个实用解决方案是轨迹生成，这是一种为模拟人类移动行为而开发的方法。现有的轨迹生成方法主要侧重于捕获个体运动模式，但往往忽视人口分布对轨迹生成的影响。事实上，动态的人口分布反映了不同地区人口密度的变化，显着影响着个人的流动行为。因此，我们提出了一种基于扩散模型的新颖轨迹生成框架，该框架集成了动态人口分布约束来指导高保真生成结果。具体来说，我们构建了一个空间图来增强轨迹的空间相关性。然后，我们设计了一个动态人口分布感知去噪网络，以捕获人类流动行为的时空依赖性以及去噪过程中人口分布的影响。大量实验表明，我们的模型生成的轨迹在一些关键统计指标方面与现实世界的轨迹相似，比最先进的算法高出 54% 以上。</li>
</ul>

<h3>Title: Deciphering Personalization: Towards Fine-Grained Explainability in Natural Language for Personalized Image Generation Models</h3>
<ul>
<li><strong>Authors: </strong>Haoming Wang, Wei Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01932">https://arxiv.org/abs/2511.01932</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01932">https://arxiv.org/pdf/2511.01932</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01932]] Deciphering Personalization: Towards Fine-Grained Explainability in Natural Language for Personalized Image Generation Models(https://arxiv.org/abs/2511.01932)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Image generation models are usually personalized in practical uses in order to better meet the individual users' heterogeneous needs, but most personalized models lack explainability about how they are being personalized. Such explainability can be provided via visual features in generated images, but is difficult for human users to understand. Explainability in natural language is a better choice, but the existing approaches to explainability in natural language are limited to be coarse-grained. They are unable to precisely identify the multiple aspects of personalization, as well as the varying levels of personalization in each aspect. To address such limitation, in this paper we present a new technique, namely \textbf{FineXL}, towards \textbf{Fine}-grained e\textbf{X}plainability in natural \textbf{L}anguage for personalized image generation models. FineXL can provide natural language descriptions about each distinct aspect of personalization, along with quantitative scores indicating the level of each aspect of personalization. Experiment results show that FineXL can improve the accuracy of explainability by 56\%, when different personalization scenarios are applied to multiple types of image generation models.</li>
<li><strong>摘要：</strong>图像生成模型在实际使用中通常是个性化的，以便更好地满足各个用户的异构需求，但大多数个性化模型缺乏对如何个性化的可解释性。这种可解释性可以通过生成图像中的视觉特征来提供，但人类用户很难理解。自然语言的可解释性是一个更好的选择，但现有的自然语言可解释性方法仅限于粗粒度。他们无法准确识别个性化的多个方面，以及每个方面的不同程度的个性化。为了解决这种限制，在本文中，我们提出了一种新技术，即 \textbf{FineXL}，以自然 \textbf{L} 语言实现 \textbf{Fine} 粒度的 e\textbf{X} 易懂性，用于个性化图像生成模型。 FineXL 可以提供有关个性化每个不同方面的自然语言描述，以及指示个性化每个方面的水平的定量分数。实验结果表明，当不同的个性化场景应用于多种类型的图像生成模型时，FineXL可以将可解释性的准确性提高56％。</li>
</ul>

<h3>Title: Locally-Supervised Global Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Walder, Daniel Toader, Robert Nuster, Günther Paltauf, Peter Burgholzer, Gregor Langer, Lukas Krainer, Markus Haltmeier</a></li>
<li><strong>Subjects: </strong>cs.CV, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01998">https://arxiv.org/abs/2511.01998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01998">https://arxiv.org/pdf/2511.01998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01998]] Locally-Supervised Global Image Restoration(https://arxiv.org/abs/2511.01998)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>We address the problem of image reconstruction from incomplete measurements, encompassing both upsampling and inpainting, within a learning-based framework. Conventional supervised approaches require fully sampled ground truth data, while self-supervised methods allow incomplete ground truth but typically rely on random sampling that, in expectation, covers the entire image. In contrast, we consider fixed, deterministic sampling patterns with inherently incomplete coverage, even in expectation. To overcome this limitation, we exploit multiple invariances of the underlying image distribution, which theoretically allows us to achieve the same reconstruction performance as fully supervised approaches. We validate our method on optical-resolution image upsampling in photoacoustic microscopy (PAM), demonstrating competitive or superior results while requiring substantially less ground truth data.</li>
<li><strong>摘要：</strong>我们在基于学习的框架内解决了不完整测量的图像重建问题，包括上采样和修复。传统的监督方法需要完全采样的地面实况数据，而自监督方法允许不完整的地面实况，但通常依赖于随机采样，预计覆盖整个图像。相比之下，我们认为固定的、确定性的采样模式本质上具有不完整的覆盖范围，即使在预期中也是如此。为了克服这个限制，我们利用了底层图像分布的多个不变性，这在理论上使我们能够实现与完全监督方法相同的重建性能。我们在光声显微镜（PAM）中验证了我们的光学分辨率图像上采样方法，证明了具有竞争力或优越的结果，同时需要更少的地面实况数据。</li>
</ul>

<h3>Title: Quantum-Enhanced Generative Models for Rare Event Prediction</h3>
<ul>
<li><strong>Authors: </strong>M.Z. Haider, M.U. Ghouri, Tayyaba Noreen, M. Salman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.02042">https://arxiv.org/abs/2511.02042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.02042">https://arxiv.org/pdf/2511.02042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.02042]] Quantum-Enhanced Generative Models for Rare Event Prediction(https://arxiv.org/abs/2511.02042)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Rare events such as financial crashes, climate extremes, and biological anomalies are notoriously difficult to model due to their scarcity and heavy-tailed distributions. Classical deep generative models often struggle to capture these rare occurrences, either collapsing low-probability modes or producing poorly calibrated uncertainty estimates. In this work, we propose the Quantum-Enhanced Generative Model (QEGM), a hybrid classical-quantum framework that integrates deep latent-variable models with variational quantum circuits. The framework introduces two key innovations: (1) a hybrid loss function that jointly optimizes reconstruction fidelity and tail-aware likelihood, and (2) quantum randomness-driven noise injection to enhance sample diversity and mitigate mode collapse. Training proceeds via a hybrid loop where classical parameters are updated through backpropagation while quantum parameters are optimized using parameter-shift gradients. We evaluate QEGM on synthetic Gaussian mixtures and real-world datasets spanning finance, climate, and protein structure. Results demonstrate that QEGM reduces tail KL divergence by up to 50 percent compared to state-of-the-art baselines (GAN, VAE, Diffusion), while improving rare-event recall and coverage calibration. These findings highlight the potential of QEGM as a principled approach for rare-event prediction, offering robustness beyond what is achievable with purely classical methods.</li>
<li><strong>摘要：</strong>金融崩溃、极端气候和生物异常等罕见事件由于其稀缺性和重尾分布而难以建模。经典的深度生成模型通常很难捕获这些罕见的事件，要么导致低概率模式崩溃，要么产生校准不佳的不确定性估计。在这项工作中，我们提出了量子增强生成模型（QEGM），这是一种混合经典量子框架，将深度潜变量模型与变分量子电路集成在一起。该框架引入了两项关键创新：(1) 联合优化重建保真度和尾部感知可能性的混合损失函数，以及 (2) 量子随机性驱动的噪声注入，以增强样本多样性并减轻模式崩溃。训练通过混合循环进行，其中经典参数通过反向传播进行更新，而量子参数则使用参数移位梯度进行优化。我们在合成高斯混合物和涵盖金融、气候和蛋白质结构的真实世界数据集上评估 QEGM。结果表明，与最先进的基线（GAN、VAE、Diffusion）相比，QEGM 将尾部 KL 散度减少了高达 50%，同时改善了罕见事件召回和覆盖校准。这些发现凸显了 QEGM 作为罕见事件预测原则方法的潜力，其鲁棒性超出了纯经典方法所能达到的水平。</li>
</ul>

<h3>Title: Text-VQA Aug: Pipelined Harnessing of Large Multimodal Models for Automated Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Soham Joshi, Shwet Kamal Mishra, Viswanath Gopalakrishnan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.02046">https://arxiv.org/abs/2511.02046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.02046">https://arxiv.org/pdf/2511.02046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.02046]] Text-VQA Aug: Pipelined Harnessing of Large Multimodal Models for Automated Synthesis(https://arxiv.org/abs/2511.02046)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Creation of large-scale databases for Visual Question Answering tasks pertaining to the text data in a scene (text-VQA) involves skilful human annotation, which is tedious and challenging. With the advent of foundation models that handle vision and language modalities, and with the maturity of OCR systems, it is the need of the hour to establish an end-to-end pipeline that can synthesize Question-Answer (QA) pairs based on scene-text from a given image. We propose a pipeline for automated synthesis for text-VQA dataset that can produce faithful QA pairs, and which scales up with the availability of scene text data. Our proposed method harnesses the capabilities of multiple models and algorithms involving OCR detection and recognition (text spotting), region of interest (ROI) detection, caption generation, and question generation. These components are streamlined into a cohesive pipeline to automate the synthesis and validation of QA pairs. To the best of our knowledge, this is the first pipeline proposed to automatically synthesize and validate a large-scale text-VQA dataset comprising around 72K QA pairs based on around 44K images.</li>
<li><strong>摘要：</strong>为与场景中的文本数据相关的视觉问答任务（文本 VQA）创建大型数据库涉及熟练的人工注释，这是乏味且具有挑战性的。随着处理视觉和语言模态的基础模型的出现，以及 OCR 系统的成熟，现在需要建立一个端到端的管道，可以根据给定图像的场景文本合成问答 (QA) 对。我们提出了一种用于文本 VQA 数据集自动合成的管道，该管道可以生成忠实的 QA 对，并随着场景文本数据的可用性而扩展。我们提出的方法利用了多种模型和算法的功能，包括 OCR 检测和识别（文本识别）、感兴趣区域 (ROI) 检测、标题生成和问题生成。这些组件被简化为一个有凝聚力的管道，以自动合成和验证 QA 对。据我们所知，这是第一个自动合成和验证大规模文本 VQA 数据集的管道，该数据集包含基于约 44K 图像的约 72K QA 对。</li>
</ul>

<h3>Title: Energy Loss Functions for Physical Systems</h3>
<ul>
<li><strong>Authors: </strong>Sékou-Oumar Kaba, Kusha Sareen, Daniel Levy, Siamak Ravanbakhsh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.02087">https://arxiv.org/abs/2511.02087</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.02087">https://arxiv.org/pdf/2511.02087</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.02087]] Energy Loss Functions for Physical Systems(https://arxiv.org/abs/2511.02087)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Effectively leveraging prior knowledge of a system's physics is crucial for applications of machine learning to scientific domains. Previous approaches mostly focused on incorporating physical insights at the architectural level. In this paper, we propose a framework to leverage physical information directly into the loss function for prediction and generative modeling tasks on systems like molecules and spins. We derive energy loss functions assuming that each data sample is in thermal equilibrium with respect to an approximate energy landscape. By using the reverse KL divergence with a Boltzmann distribution around the data, we obtain the loss as an energy difference between the data and the model predictions. This perspective also recasts traditional objectives like MSE as energy-based, but with a physically meaningless energy. In contrast, our formulation yields physically grounded loss functions with gradients that better align with valid configurations, while being architecture-agnostic and computationally efficient. The energy loss functions also inherently respect physical symmetries. We demonstrate our approach on molecular generation and spin ground-state prediction and report significant improvements over baselines.</li>
<li><strong>摘要：</strong>有效利用系统物理学的先验知识对于机器学习在科学领域的应用至关重要。以前的方法主要侧重于在架构级别整合物理见解。在本文中，我们提出了一个框架，将物理信息直接利用到损失函数中，用于分子和自旋等系统的预测和生成建模任务。我们假设每个数据样本相对于近似能量景观处于热平衡状态，从而推导出能量损失函数。通过使用数据周围玻尔兹曼分布的反向 KL 散度，我们获得了作为数据和模型预测之间的能量差的损失。这种观点还将 MSE 等传统目标重新定义为基于能量的目标，但具有物理上无意义的能量。相比之下，我们的公式产生了物理接地的损失函数，其梯度更好地与有效配置保持一致，同时与架构无关并且计算效率高。能量损失函数本质上也遵循物理对称性。我们展示了我们的分子生成和自旋基态预测方法，并报告了相对于基线的显着改进。</li>
</ul>

<h3>Title: Natural Building Blocks for Structured World Models: Theory, Evidence, and Scaling</h3>
<ul>
<li><strong>Authors: </strong>Lancelot Da Costa, Sanjeev Namjoshi, Mohammed Abbas Ansari, Bernhard Schölkopf</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.02091">https://arxiv.org/abs/2511.02091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.02091">https://arxiv.org/pdf/2511.02091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.02091]] Natural Building Blocks for Structured World Models: Theory, Evidence, and Scaling(https://arxiv.org/abs/2511.02091)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The field of world modeling is fragmented, with researchers developing bespoke architectures that rarely build upon each other. We propose a framework that specifies the natural building blocks for structured world models based on the fundamental stochastic processes that any world model must capture: discrete processes (logic, symbols) and continuous processes (physics, dynamics); the world model is then defined by the hierarchical composition of these building blocks. We examine Hidden Markov Models (HMMs) and switching linear dynamical systems (sLDS) as natural building blocks for discrete and continuous modeling--which become partially-observable Markov decision processes (POMDPs) and controlled sLDS when augmented with actions. This modular approach supports both passive modeling (generation, forecasting) and active control (planning, decision-making) within the same architecture. We avoid the combinatorial explosion of traditional structure learning by largely fixing the causal architecture and searching over only four depth parameters. We review practical expressiveness through multimodal generative modeling (passive) and planning from pixels (active), with performance competitive to neural approaches while maintaining interpretability. The core outstanding challenge is scalable joint structure-parameter learning; current methods finesse this by cleverly growing structure and parameters incrementally, but are limited in their scalability. If solved, these natural building blocks could provide foundational infrastructure for world modeling, analogous to how standardized layers enabled progress in deep learning.</li>
<li><strong>摘要：</strong>世界建模领域是支离破碎的，研究人员开发的定制架构很少相互构建。我们提出了一个框架，该框架根据任何世界模型必须捕获的基本随机过程指定结构化世界模型的自然构建块：离散过程（逻辑、符号）和连续过程（物理、动力学）；然后，世界模型由这些构建块的层次结构组成来定义。我们研究隐马尔可夫模型（HMM）和切换线性动力系统（sLDS）作为离散和连续建模的自然构建块——当通过动作增强时，它们成为部分可观察马尔可夫决策过程（POMDP）和受控 sLDS。这种模块化方法支持同一架构内的被动建模（生成、预测）和主动控制（规划、决策）。我们通过很大程度上修复因果架构并仅搜索四个深度参数来避免传统结构学习的组合爆炸。我们通过多模态生成建模（被动）和像素规划（主动）来审查实际表现力，其性能与神经方法相竞争，同时保持可解释性。核心突出的挑战是可扩展的联合结构参数学习；当前的方法通过巧妙地逐步增长结构和参数来巧妙地解决这一问题，但其可扩展性受到限制。如果解决了，这些自然构建块可以为世界建模提供基础设施，类似于标准化层如何促进深度学习的进步。</li>
</ul>

<h3>Title: Uncertainty Guided Online Ensemble for Non-stationary Data Streams in Fusion Science</h3>
<ul>
<li><strong>Authors: </strong>Kishansingh Rajput, Malachi Schram, Brian Sammuli, Sen Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.plasm-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.02092">https://arxiv.org/abs/2511.02092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.02092">https://arxiv.org/pdf/2511.02092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.02092]] Uncertainty Guided Online Ensemble for Non-stationary Data Streams in Fusion Science(https://arxiv.org/abs/2511.02092)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Machine Learning (ML) is poised to play a pivotal role in the development and operation of next-generation fusion devices. Fusion data shows non-stationary behavior with distribution drifts, resulted by both experimental evolution and machine wear-and-tear. ML models assume stationary distribution and fail to maintain performance when encountered with such non-stationary data streams. Online learning techniques have been leveraged in other domains, however it has been largely unexplored for fusion applications. In this paper, we present an application of online learning to continuously adapt to drifting data stream for prediction of Toroidal Field (TF) coils deflection at the DIII-D fusion facility. The results demonstrate that online learning is critical to maintain ML model performance and reduces error by 80% compared to a static model. Moreover, traditional online learning can suffer from short-term performance degradation as ground truth is not available before making the predictions. As such, we propose an uncertainty guided online ensemble method to further improve the performance. The Deep Gaussian Process Approximation (DGPA) technique is leveraged for calibrated uncertainty estimation and the uncertainty values are then used to guide a meta-algorithm that produces predictions based on an ensemble of learners trained on different horizon of historical data. The DGPA also provides uncertainty estimation along with the predictions for decision makers. The online ensemble and the proposed uncertainty guided online ensemble reduces predictions error by about 6%, and 10% respectively over standard single model based online learning.</li>
<li><strong>摘要：</strong>机器学习 (ML) 有望在下一代融合设备的开发和运营中发挥关键作用。融合数据显示出具有分布漂移的非平稳行为，这是由实验演化和机器磨损造成的。机器学习模型假设平稳分布，并且在遇到此类非平稳数据流时无法保持性能。在线学习技术已在其他领域得到利用，但融合应用在很大程度上尚未得到探索。在本文中，我们提出了一种在线学习的应用，以持续适应漂移数据流，以预测 DIII-D 聚变设施中的环形场 (TF) 线圈偏转。结果表明，在线学习对于维持 ML 模型性能至关重要，与静态模型相比，在线学习可将错误减少 80%。此外，传统的在线学习可能会遭受短期性能下降的影响，因为在做出预测之前无法获得地面事实。因此，我们提出了一种不确定性引导的在线集成方法来进一步提高性能。利用深度高斯过程近似 (DGPA) 技术进行校准的不确定性估计，然后使用不确定性值来指导元算法，该算法基于在不同历史数据范围内训练的学习者集合来生成预测。 DGPA 还为决策者提供不确定性估计和预测。与基于标准单一模型的在线学习相比，在线集成和提出的不确定性引导在线集成分别减少了约 6% 和 10% 的预测误差。</li>
</ul>

<h3>Title: Language-Enhanced Generative Modeling for PET Synthesis from MRI and Blood Biomarkers</h3>
<ul>
<li><strong>Authors: </strong>Zhengjie Zhang, Xiaoxie Mao, Qihao Guo, Shaoting Zhang, Qi Huang, Mu Zhou, Fang Xie, Mianxin Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.02206">https://arxiv.org/abs/2511.02206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.02206">https://arxiv.org/pdf/2511.02206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.02206]] Language-Enhanced Generative Modeling for PET Synthesis from MRI and Blood Biomarkers(https://arxiv.org/abs/2511.02206)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Background: Alzheimer's disease (AD) diagnosis heavily relies on amyloid-beta positron emission tomography (Abeta-PET), which is limited by high cost and limited accessibility. This study explores whether Abeta-PET spatial patterns can be predicted from blood-based biomarkers (BBMs) and MRI scans. Methods: We collected Abeta-PET images, T1-weighted MRI scans, and BBMs from 566 participants. A language-enhanced generative model, driven by a large language model (LLM) and multimodal information fusion, was developed to synthesize PET images. Synthesized images were evaluated for image quality, diagnostic consistency, and clinical applicability within a fully automated diagnostic pipeline. Findings: The synthetic PET images closely resemble real PET scans in both structural details (SSIM = 0.920 +/- 0.003) and regional patterns (Pearson's r = 0.955 +/- 0.007). Diagnostic outcomes using synthetic PET show high agreement with real PET-based diagnoses (accuracy = 0.80). Using synthetic PET, we developed a fully automatic AD diagnostic pipeline integrating PET synthesis and classification. The synthetic PET-based model (AUC = 0.78) outperforms T1-based (AUC = 0.68) and BBM-based (AUC = 0.73) models, while combining synthetic PET and BBMs further improved performance (AUC = 0.79). Ablation analysis supports the advantages of LLM integration and prompt engineering. Interpretation: Our language-enhanced generative model synthesizes realistic PET images, enhancing the utility of MRI and BBMs for Abeta spatial pattern assessment and improving the diagnostic workflow for Alzheimer's disease.</li>
<li><strong>摘要：</strong>背景：阿尔茨海默病（AD）的诊断严重依赖淀粉样蛋白-β正电子发射断层扫描（Abeta-PET），但其成本高且可及性有限。本研究探讨是否可以通过血液生物标志物 (BBM) 和 MRI 扫描来预测 Abeta-PET 空间模式。方法：我们收集了 566 名参与者的 Abeta-PET 图像、T1 加权 MRI 扫描和 BBM。开发了一种由大语言模型 (LLM) 和多模态信息融合驱动的语言增强生成模型来合成 PET 图像。在全自动诊断流程中评估合成图像的图像质量、诊断一致性和临床适用性。结果：合成 PET 图像在结构细节 (SSIM = 0.920 +/- 0.003) 和区域模式 (Pearson r = 0.955 +/- 0.007) 方面与真实 PET 扫描非常相似。使用合成 PET 的诊断结果与基于 PET 的真实诊断高度一致（准确度 = 0.80）。利用合成PET，我们开发了集PET合成和分类于一体的全自动AD诊断流程。基于合成 PET 的模型（AUC = 0.78）优于基于 T1（AUC = 0.68）和基于 BBM（AUC = 0.73）的模型，同时合成 PET 和 BBM 的结合进一步提高了性能（AUC = 0.79）。消融分析支持法学硕士集成和即时工程的优势。解释：我们的语言增强生成模型合成了逼真的 PET 图像，增强了 MRI 和 BBM 在 Abeta 空间模式评估中的实用性，并改进了阿尔茨海默病的诊断工作流程。</li>
</ul>

<h3>Title: Opportunistic Expert Activation: Batch-Aware Expert Routing for Faster Decode Without Retraining</h3>
<ul>
<li><strong>Authors: </strong>Costin-Andrei Oncescu, Qingyang Wu, Wai Tong Chung, Robert Wu, Bryan Gopal, Junxiong Wang, Tri Dao, Ben Athiwaratkun</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.02237">https://arxiv.org/abs/2511.02237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.02237">https://arxiv.org/pdf/2511.02237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.02237]] Opportunistic Expert Activation: Batch-Aware Expert Routing for Faster Decode Without Retraining(https://arxiv.org/abs/2511.02237)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>An increasing number of LLMs employ Mixture-of-Experts (MoE) architectures where the feed-forward layer is replaced by a pool of experts and each token only activates a small subset of them. During autoregressive generation, these models often enter a memory-bound regime even for moderate batch sizes because the average expert load grows more slowly than in an equivalent dense feedforward layer. Consequently, MoE latency is governed by the number of activated experts. We introduce a framework for dynamically re-routing token-to-expert mapping to lower this number (and thus, the decode latency) while preserving a comparable quality. Our best results use a batch-aware routing that works by having tokens piggyback experts that have already been loaded into memory due to being crucial to other tokens within the same batch. Empirically, we evaluate our method on the Qwen3-30B and Qwen3-235B models with a batch size of $16$. Without any statistically significant loss in accuracy, our approach achieves latency reductions of $39\%$ and $15\%$ in the MoE layer decode latency, respectively.</li>
<li><strong>摘要：</strong>越来越多的法学硕士采用专家混合 (MoE) 架构，其中前馈层被一组专家取代，每个令牌仅激活其中的一小部分。在自回归生成过程中，即使批量大小适中，这些模型也通常会进入内存限制状态，因为平均专家负载的增长速度比等效的密集前馈层要慢。因此，MoE 延迟取决于激活专家的数量。我们引入了一个框架，用于动态重新路由令牌到专家的映射，以降低这个数字（从而降低解码延迟），同时保持可比较的质量。我们的最佳结果是使用批处理感知路由，该路由通过让已加载到内存中的令牌搭载专家来工作，因为这些专家对于同一批次中的其他令牌至关重要。根据经验，我们在批量大小为 16 美元的 Qwen3-30B 和 Qwen3-235B 模型上评估我们的方法。在没有任何统计上显着的准确性损失的情况下，我们的方法在 MoE 层解码延迟中分别实现了 $39\%$ 和 $15\%$ 的延迟减少。</li>
</ul>

<h3>Title: Medical Report Generation: A Hierarchical Task Structure-Based Cross-Modal Causal Intervention Framework</h3>
<ul>
<li><strong>Authors: </strong>Yucheng Song, Yifan Ge, Junhao Li, Zhining Liao, Zhifang Liao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.02271">https://arxiv.org/abs/2511.02271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.02271">https://arxiv.org/pdf/2511.02271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.02271]] Medical Report Generation: A Hierarchical Task Structure-Based Cross-Modal Causal Intervention Framework(https://arxiv.org/abs/2511.02271)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Medical Report Generation (MRG) is a key part of modern medical diagnostics, as it automatically generates reports from radiological images to reduce radiologists' burden. However, reliable MRG models for lesion description face three main challenges: insufficient domain knowledge understanding, poor text-visual entity embedding alignment, and spurious correlations from cross-modal biases. Previous work only addresses single challenges, while this paper tackles all three via a novel hierarchical task decomposition approach, proposing the HTSC-CIF framework. HTSC-CIF classifies the three challenges into low-, mid-, and high-level tasks: 1) Low-level: align medical entity features with spatial locations to enhance domain knowledge for visual encoders; 2) Mid-level: use Prefix Language Modeling (text) and Masked Image Modeling (images) to boost cross-modal alignment via mutual guidance; 3) High-level: a cross-modal causal intervention module (via front-door intervention) to reduce confounders and improve interpretability. Extensive experiments confirm HTSC-CIF's effectiveness, significantly outperforming state-of-the-art (SOTA) MRG methods. Code will be made public upon paper acceptance.</li>
<li><strong>摘要：</strong>医疗报告生成 (MRG) 是现代医疗诊断的关键部分，因为它会根据放射图像自动生成报告，以减轻放射科医生的负担。然而，用于病变描述的可靠 MRG 模型面临三个主要挑战：领域知识理解不足、文本视觉实体嵌入对齐不佳以及跨模式偏差产生的虚假相关性。以前的工作仅解决单一挑战，而本文通过一种新颖的分层任务分解方法解决了所有三个挑战，提出了 HTSC-CIF 框架。 HTSC-CIF 将三个挑战分为低级、中级和高级任务： 1）低级：将医疗实体特征与空间位置对齐，以增强视觉编码器的领域知识； 2）中级：使用前缀语言建模（文本）和掩模图像建模（图像）通过相互指导来促进跨模态对齐； 3）高层：跨模式因果干预模块（通过前门干预），以减少混杂因素并提高可解释性。大量实验证实了 HTSC-CIF 的有效性，显着优于最先进的 (SOTA) MRG 方法。代码将在纸质验收后公开。</li>
</ul>

<h3>Title: Federated Quantum Kernel Learning for Anomaly Detection in Multivariate IoT Time-Series</h3>
<ul>
<li><strong>Authors: </strong>Kuan-Cheng Chen, Samuel Yen-Chi Chen, Chen-Yu Liu, Kin K. Leung</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.02301">https://arxiv.org/abs/2511.02301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.02301">https://arxiv.org/pdf/2511.02301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.02301]] Federated Quantum Kernel Learning for Anomaly Detection in Multivariate IoT Time-Series(https://arxiv.org/abs/2511.02301)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The rapid growth of industrial Internet of Things (IIoT) systems has created new challenges for anomaly detection in high-dimensional, multivariate time-series, where privacy, scalability, and communication efficiency are critical. Classical federated learning approaches mitigate privacy concerns by enabling decentralized training, but they often struggle with highly non-linear decision boundaries and imbalanced anomaly distributions. To address this gap, we propose a Federated Quantum Kernel Learning (FQKL) framework that integrates quantum feature maps with federated aggregation to enable distributed, privacy-preserving anomaly detection across heterogeneous IoT networks. In our design, quantum edge nodes locally compute compressed kernel statistics using parameterized quantum circuits and share only these summaries with a central server, which constructs a global Gram matrix and trains a decision function (e.g., Fed-QSVM). Experimental results on synthetic IIoT benchmarks demonstrate that FQKL achieves superior generalization in capturing complex temporal correlations compared to classical federated baselines, while significantly reducing communication overhead. This work highlights the promise of quantum kernels in federated settings, advancing the path toward scalable, robust, and quantum-enhanced intelligence for next-generation IoT infrastructures.</li>
<li><strong>摘要：</strong>工业物联网 (IIoT) 系统的快速增长给高维、多变量时间序列中的异常检测带来了新的挑战，其中隐私、可扩展性和通信效率至关重要。经典的联邦学习方法通​​过实现去中心化训练来减轻隐私问题，但它们经常与高度非线性的决策边界和不平衡的异常分布作斗争。为了解决这一差距，我们提出了一种联邦量子核学习（FQKL）框架，该框架将量子特征图与联邦聚合相集成，以实现跨异构物联网网络的分布式、保护隐私的异常检测。在我们的设计中，量子边缘节点使用参数化量子电路在本地计算压缩的内核统计数据，并仅与中央服务器共享这些摘要，中央服务器构建全局 Gram 矩阵并训练决策函数（例如 Fed-QSVM）。综合 IIoT 基准的实验结果表明，与传统的联合基线相比，FQKL 在捕获复杂时间相关性方面实现了卓越的泛化，同时显着降低了通信开销。这项工作凸显了量子内核在联合环境中的前景，为下一代物联网基础设施推进了可扩展、稳健和量子增强智能的道路。</li>
</ul>

<h3>Title: Learning A Universal Crime Predictor with Knowledge-guided Hypernetworks</h3>
<ul>
<li><strong>Authors: </strong>Fidan Karimova, Tong Chen, Yu Yang, Shazia Sadiq</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.02336">https://arxiv.org/abs/2511.02336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.02336">https://arxiv.org/pdf/2511.02336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.02336]] Learning A Universal Crime Predictor with Knowledge-guided Hypernetworks(https://arxiv.org/abs/2511.02336)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Predicting crimes in urban environments is crucial for public safety, yet existing prediction methods often struggle to align the knowledge across diverse cities that vary dramatically in data availability of specific crime types. We propose HYpernetwork-enhanced Spatial Temporal Learning (HYSTL), a framework that can effectively train a unified, stronger crime predictor without assuming identical crime types in different cities' records. In HYSTL, instead of parameterising a dedicated predictor per crime type, a hypernetwork is designed to dynamically generate parameters for the prediction function conditioned on the crime type of interest. To bridge the semantic gap between different crime types, a structured crime knowledge graph is built, where the learned representations of crimes are used as the input to the hypernetwork to facilitate parameter generation. As such, when making predictions for each crime type, the predictor is additionally guided by its intricate association with other relevant crime types. Extensive experiments are performed on two cities with non-overlapping crime types, and the results demonstrate HYSTL outperforms state-of-the-art baselines.</li>
<li><strong>摘要：</strong>预测城市环境中的犯罪对于公共安全至关重要，但现有的预测方法往往难以协调不同城市的知识，而这些城市的特定犯罪类型的数据可用性差异很大。我们提出了超网络增强型时空学习（HYSTL），该框架可以有效地训练统一的、更强的犯罪预测器，而无需假设不同城市记录中的犯罪类型相同。在 HYSTL 中，不是参数化每个犯罪类型的专用预测器，而是设计超网络来根据感兴趣的犯罪类型动态生成预测函数的参数。为了弥合不同犯罪类型之间的语义差距，构建了结构化犯罪知识图，其中学习到的犯罪表示用作超网络的输入，以方便参数生成。因此，在对每种犯罪类型进行预测时，预测器还会受到其与其他相关犯罪类型的复杂关联的指导。在两个犯罪类型不重叠的城市进行了大量实验，结果表明 HYSTL 的性能优于最先进的基线。</li>
</ul>

<h3>Title: Evolving Graph Learning for Out-of-Distribution Generalization in Non-stationary Environments</h3>
<ul>
<li><strong>Authors: </strong>Qingyun Sun, Jiayi Luo, Haonan Yuan, Xingcheng Fu, Hao Peng, Jianxin Li, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.02354">https://arxiv.org/abs/2511.02354</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.02354">https://arxiv.org/pdf/2511.02354</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.02354]] Evolving Graph Learning for Out-of-Distribution Generalization in Non-stationary Environments(https://arxiv.org/abs/2511.02354)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Graph neural networks have shown remarkable success in exploiting the spatial and temporal patterns on dynamic graphs. However, existing GNNs exhibit poor generalization ability under distribution shifts, which is inevitable in dynamic scenarios. As dynamic graph generation progresses amid evolving latent non-stationary environments, it is imperative to explore their effects on out-of-distribution (OOD) generalization. This paper proposes a novel Evolving Graph Learning framework for OOD generalization (EvoOOD) by environment-aware invariant pattern recognition. Specifically, we first design an environment sequential variational auto-encoder to model environment evolution and infer the underlying environment distribution. Then, we introduce a mechanism for environment-aware invariant pattern recognition, tailored to address environmental diversification through inferred distributions. Finally, we conduct fine-grained causal interventions on individual nodes using a mixture of instantiated environment samples. This approach helps to distinguish spatio-temporal invariant patterns for OOD prediction, especially in non-stationary environments. Experimental results demonstrate the superiority of EvoGOOD on both real-world and synthetic dynamic datasets under distribution shifts. To the best of our knowledge, it is the first attempt to study the dynamic graph OOD generalization problem from the environment evolution perspective.</li>
<li><strong>摘要：</strong>图神经网络在利用动态图的空间和时间模式方面取得了显着的成功。然而，现有的 GNN 在分布变化下泛化能力较差，这在动态场景中是不可避免的。随着动态图生成在不断变化的潜在非平稳环境中不断发展，必须探索它们对分布外（OOD）泛化的影响。本文通过环境感知不变模式识别提出了一种新颖的 OOD 泛化（EvoOOD）进化图学习框架。具体来说，我们首先设计一个环境顺序变分自动编码器来模拟环境演化并推断底层环境分布。然后，我们引入了一种环境感知不变模式识别机制，旨在通过推断分布来解决环境多样化问题。最后，我们使用实例化环境样本的混合对各个节点进行细粒度的因果干预。这种方法有助于区分 OOD 预测的时空不变模式，特别是在非平稳环境中。实验结果证明了 EvoGOOD 在分布变化下的真实世界和合成动态数据集上的优越性。据我们所知，这是从环境演化角度研究动态图OOD泛化问题的首次尝试。</li>
</ul>

<h3>Title: LUMA-RAG: Lifelong Multimodal Agents with Provably Stable Streaming Alignment</h3>
<ul>
<li><strong>Authors: </strong>Rohan Wandre, Yash Gajewar, Namrata Patel, Vivek Dhalkari</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.02371">https://arxiv.org/abs/2511.02371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.02371">https://arxiv.org/pdf/2511.02371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.02371]] LUMA-RAG: Lifelong Multimodal Agents with Provably Stable Streaming Alignment(https://arxiv.org/abs/2511.02371)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) has emerged as the dominant paradigm for grounding large language model outputs in verifiable evidence. However, as modern AI agents transition from static knowledge bases to continuous multimodal streams encompassing text, images, video, and audio, two critical challenges arise: maintaining index freshness without prohibitive re-indexing costs, and preserving cross-modal semantic consistency across heterogeneous embedding spaces. We present LUMA-RAG, a lifelong multimodal agent architecture featuring three key innovations: (i) a streaming, multi-tier memory system that dynamically spills embeddings from a hot HNSW tier to a compressed IVFPQ tier under strict memory budgets; (ii) a streaming CLAP->CLIP alignment bridge that maintains cross-modal consistency through incremental orthogonal Procrustes updates; and (iii) stability-aware retrieval telemetry providing Safe@k guarantees by jointly bounding alignment drift and quantization error. Experiments demonstrate robust text-to-image retrieval (Recall@10 = 0.94), graceful performance degradation under product quantization offloading, and provably stable audio-to-image rankings (Safe@1 = 1.0), establishing LUMA-RAG as a practical framework for production multimodal RAG systems.</li>
<li><strong>摘要：</strong>检索增强生成（RAG）已成为将大型语言模型输出建立在可验证证据的主导范式。然而，随着现代人工智能代理从静态知识库过渡到包含文本、图像、视频和音频的连续多模态流，出现了两个关键挑战：在不产生高昂的重新索引成本的情况下保持索引新鲜度，以及在异构嵌入空间中保持跨模态语义一致性。我们推出了 LUMA-RAG，这是一种终身多模式代理架构，具有三个关键创新：（i）流式多层内存系统，可在严格的内存预算下将嵌入从热 HNSW 层动态溢出到压缩的 IVFPQ 层； (ii) 流式 CLAP->CLIP 对齐桥，通过增量正交 Procrustes 更新保持跨模式一致性； (iii) 稳定性感知检索遥测通过联合限制对准漂移和量化误差来提供 Safe@k 保证。实验证明了强大的文本到图像检索 (Recall@10 = 0.94)、产品量化卸载下的优雅性能下降以及可证明稳定的音频到图像排名 (Safe@1 = 1.0)，从而将 LUMA-RAG 确立为生产多模式 RAG 系统的实用框架。</li>
</ul>

<h3>Title: ChartM$^3$: A Multi-Stage Code-Driven Pipeline for Constructing Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension</h3>
<ul>
<li><strong>Authors: </strong>Duo Xu, Hao Cheng, Xin Lin, Zhen Xie, Hao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.02415">https://arxiv.org/abs/2511.02415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.02415">https://arxiv.org/pdf/2511.02415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.02415]] ChartM$^3$: A Multi-Stage Code-Driven Pipeline for Constructing Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension(https://arxiv.org/abs/2511.02415)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Complex chart understanding tasks demand advanced visual recognition and reasoning capabilities from multimodal large language models (MLLMs). However, current research provides limited coverage of complex chart scenarios and computation-intensive reasoning tasks prevalent in real-world applications. This study proposes an automated multi-stage code-driven pipeline for systematically generating visual reasoning datasets to address these limitations. The pipeline integrates retrieval-augmented generation (RAG) to retrieve professional chart templates and employs chain-of-thought (CoT) strategies to generate reasoning codes that simulate real data distributions, thereby driving chart rendering and question-related statistical computations. Through model-based evaluation, the pipeline enhances chart diversity and data quality. Using this framework, we construct ChartM$^3$, a multi-dimensional and multi-step dataset containing 38K charts and 142K Q&A pairs for training, along with 2,871 high-quality evaluation samples for enabling practical performance assessment. Supervised fine-tuning (SFT) and reinforcement learning (RL) experiments demonstrate that our dataset significantly improves reasoning capabilities and cross-domain generalization performance, enabling smaller models to achieve performance comparable to larger-scale models in complex chart comprehension.</li>
<li><strong>摘要：</strong>复杂的图表理解任务需要多模态大语言模型 (MLLM) 的高级视觉识别和推理能力。然而，当前的研究对现实应用中普遍存在的复杂图表场景和计算密集型推理任务的覆盖范围有限。本研究提出了一种自动化的多阶段代码驱动管道，用于系统地生成视觉推理数据集以解决这些限制。该管道集成了检索增强生成（RAG）来检索专业图表模板，并采用思维链（CoT）策略来生成模拟真实数据分布的推理代码，从而驱动图表渲染和与问题相关的统计计算。通过基于模型的评估，该管道增强了图表多样性和数据质量。使用该框架，我们构建了 ChartM$^3$，这是一个多维度、多步骤的数据集，其中包含用于训练的 38K 图表和 142K 问答对，以及用于实现实际绩效评估的 2,871 个高质量评估样本。监督微调（SFT）和强化学习（RL）实验表明，我们的数据集显着提高了推理能力和跨域泛化性能，使较小的模型能够在复杂的图表理解中实现与大规模模型相当的性能。</li>
</ul>

<h3>Title: Synthetic Crop-Weed Image Generation and its Impact on Model Generalization</h3>
<ul>
<li><strong>Authors: </strong>Garen Boyadjian (INRAE), Cyrille Pierre (INRAE), Johann Laconte (INRAE, UR TSCF), Riccardo Bertoglio (INRAE)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.02417">https://arxiv.org/abs/2511.02417</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.02417">https://arxiv.org/pdf/2511.02417</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.02417]] Synthetic Crop-Weed Image Generation and its Impact on Model Generalization(https://arxiv.org/abs/2511.02417)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Precise semantic segmentation of crops and weeds is necessary for agricultural weeding robots. However, training deep learning models requires large annotated datasets, which are costly to obtain in real fields. Synthetic data can reduce this burden, but the gap between simulated and real images remains a challenge. In this paper, we present a pipeline for procedural generation of synthetic crop-weed images using Blender, producing annotated datasets under diverse conditions of plant growth, weed density, lighting, and camera angle. We benchmark several state-of-the-art segmentation models on synthetic and real datasets and analyze their cross-domain generalization. Our results show that training on synthetic images leads to a sim-to-real gap of 10%, surpassing previous state-of-the-art methods. Moreover, synthetic data demonstrates good generalization properties, outperforming real datasets in cross-domain scenarios. These findings highlight the potential of synthetic agricultural datasets and support hybrid strategies for more efficient model training.</li>
<li><strong>摘要：</strong>农作物和杂草的精确语义分割对于农业除草机器人来说是必要的。然而，训练深度学习模型需要大量带注释的数据集，而在实际领域中获取这些数据集的成本很高。合成数据可以减轻这种负担，但模拟图像和真实图像之间的差距仍然是一个挑战。在本文中，我们提出了一个使用 Blender 程序生成合成作物杂草图像的管道，在不同的植物生长、杂草密度、照明和相机角度条件下生成带注释的数据集。我们在合成数据集和真实数据集上对几种最先进的分割模型进行了基准测试，并分析了它们的跨域泛化。我们的结果表明，对合成图像进行训练导致模拟与真实的差距达到 10%，超过了之前最先进的方法。此外，合成数据表现出良好的泛化特性，在跨域场景中优于真实数据集。这些发现凸显了合成农业数据集的潜力，并支持更有效的模型训练的混合策略。</li>
</ul>

<h3>Title: SKGE: Spherical Knowledge Graph Embedding with Geometric Regularization</h3>
<ul>
<li><strong>Authors: </strong>Xuan-Truong Quan, Xuan-Son Quan, Duc Do Minh, Vinh Nguyen Van</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.02460">https://arxiv.org/abs/2511.02460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.02460">https://arxiv.org/pdf/2511.02460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.02460]] SKGE: Spherical Knowledge Graph Embedding with Geometric Regularization(https://arxiv.org/abs/2511.02460)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Knowledge graph embedding (KGE) has become a fundamental technique for representation learning on multi-relational data. Many seminal models, such as TransE, operate in an unbounded Euclidean space, which presents inherent limitations in modeling complex relations and can lead to inefficient training. In this paper, we propose Spherical Knowledge Graph Embedding (SKGE), a model that challenges this paradigm by constraining entity representations to a compact manifold: a hypersphere. SKGE employs a learnable, non-linear Spherization Layer to map entities onto the sphere and interprets relations as a hybrid translate-then-project transformation. Through extensive experiments on three benchmark datasets, FB15k-237, CoDEx-S, and CoDEx-M, we demonstrate that SKGE consistently and significantly outperforms its strong Euclidean counterpart, TransE, particularly on large-scale benchmarks such as FB15k-237 and CoDEx-M, demonstrating the efficacy of the spherical geometric prior. We provide an in-depth analysis to reveal the sources of this advantage, showing that this geometric constraint acts as a powerful regularizer, leading to comprehensive performance gains across all relation types. More fundamentally, we prove that the spherical geometry creates an "inherently hard negative sampling" environment, naturally eliminating trivial negatives and forcing the model to learn more robust and semantically coherent representations. Our findings compellingly demonstrate that the choice of manifold is not merely an implementation detail but a fundamental design principle, advocating for geometric priors as a cornerstone for designing the next generation of powerful and stable KGE models.</li>
<li><strong>摘要：</strong>知识图嵌入（KGE）已成为多关系数据表示学习的基本技术。许多开创性模型（例如 TransE）在无界欧几里得空间中运行，这在建模复杂关系方面存在固有的局限性，并可能导致训练效率低下。在本文中，我们提出了球形知识图嵌入（SKGE），该模型通过将实体表示限制为紧凑流形：超球面来挑战这一范式。 SKGE 采用可学习的非线性球化层将实体映射到球体上，并将关系解释为混合翻译然后投影转换。通过对三个基准数据集 FB15k-237、CoDEx-S 和 CoDEx-M 进行大量实验，我们证明 SKGE 始终显着优于其强大的欧几里德对应数据集 TransE，特别是在 FB15k-237 和 CoDEx-M 等大规模基准数据集上，证明了球面几何先验的有效性。我们提供了深入的分析来揭示这一优势的来源，表明这种几何约束充当了强大的正则化器，导致所有关系类型的综合性能增益。更根本的是，我们证明球面几何创建了一个“固有的硬负采样”环境，自然地消除了微不足道的负样本，并迫使模型学习更稳健和语义上连贯的表示。我们的研究结果令人信服地证明，流形的选择不仅仅是一个实现细节，而且是一个基本的设计原则，提倡将几何先验作为设计下一代强大且稳定的 KGE 模型的基石。</li>
</ul>

<h3>Title: KAO: Kernel-Adaptive Optimization in Diffusion for Satellite Image</h3>
<ul>
<li><strong>Authors: </strong>Teerapong Panboonyuen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.02462">https://arxiv.org/abs/2511.02462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.02462">https://arxiv.org/pdf/2511.02462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.02462]] KAO: Kernel-Adaptive Optimization in Diffusion for Satellite Image(https://arxiv.org/abs/2511.02462)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Satellite image inpainting is a crucial task in remote sensing, where accurately restoring missing or occluded regions is essential for robust image analysis. In this paper, we propose KAO, a novel framework that utilizes Kernel-Adaptive Optimization within diffusion models for satellite image inpainting. KAO is specifically designed to address the challenges posed by very high-resolution (VHR) satellite datasets, such as DeepGlobe and the Massachusetts Roads Dataset. Unlike existing methods that rely on preconditioned models requiring extensive retraining or postconditioned models with significant computational overhead, KAO introduces a Latent Space Conditioning approach, optimizing a compact latent space to achieve efficient and accurate inpainting. Furthermore, we incorporate Explicit Propagation into the diffusion process, facilitating forward-backward fusion, which improves the stability and precision of the method. Experimental results demonstrate that KAO sets a new benchmark for VHR satellite image restoration, providing a scalable, high-performance solution that balances the efficiency of preconditioned models with the flexibility of postconditioned models.</li>
<li><strong>摘要：</strong>卫星图像修复是遥感中的一项关键任务，准确恢复丢失或遮挡的区域对于稳健的图像分析至关重要。在本文中，我们提出了 KAO，这是一种利用扩散模型中的内核自适应优化进行卫星图像修复的新颖框架。 KAO 专为解决极高分辨率 (VHR) 卫星数据集（例如 DeepGlobe 和马萨诸塞州道路数据集）带来的挑战而设计。与依赖于需要大量再训练的预处理模型或具有大量计算开销的后处理模型的现有方法不同，KAO 引入了潜在空间条件方法，优化紧凑的潜在空间以实现高效且准确的修复。此外，我们将显式传播纳入扩散过程，促进前向后向融合，从而提高了方法的稳定性和精度。实验结果表明，KAO 为 VHR 卫星图像恢复树立了新的基准，提供了可扩展的高性能解决方案，平衡了预处理模型的效率与后处理模型的灵活性。</li>
</ul>

<h3>Title: OLATverse: A Large-scale Real-world Object Dataset with Precise Lighting Control</h3>
<ul>
<li><strong>Authors: </strong>Xilong Zhou, Jianchun Chen, Pramod Rao, Timo Teufel, Linjie Lyu, Tigran Minasian, Oleksandr Sotnychenko, Xiaoxiao Long, Marc Habermann, Christian Theobalt</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.02483">https://arxiv.org/abs/2511.02483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.02483">https://arxiv.org/pdf/2511.02483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.02483]] OLATverse: A Large-scale Real-world Object Dataset with Precise Lighting Control(https://arxiv.org/abs/2511.02483)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce OLATverse, a large-scale dataset comprising around 9M images of 765 real-world objects, captured from multiple viewpoints under a diverse set of precisely controlled lighting conditions. While recent advances in object-centric inverse rendering, novel view synthesis and relighting have shown promising results, most techniques still heavily rely on the synthetic datasets for training and small-scale real-world datasets for benchmarking, which limits their realism and generalization. To address this gap, OLATverse offers two key advantages over existing datasets: large-scale coverage of real objects and high-fidelity appearance under precisely controlled illuminations. Specifically, OLATverse contains 765 common and uncommon real-world objects, spanning a wide range of material categories. Each object is captured using 35 DSLR cameras and 331 individually controlled light sources, enabling the simulation of diverse illumination conditions. In addition, for each object, we provide well-calibrated camera parameters, accurate object masks, photometric surface normals, and diffuse albedo as auxiliary resources. We also construct an extensive evaluation set, establishing the first comprehensive real-world object-centric benchmark for inverse rendering and normal estimation. We believe that OLATverse represents a pivotal step toward integrating the next generation of inverse rendering and relighting methods with real-world data. The full dataset, along with all post-processing workflows, will be publicly released at this https URL.</li>
<li><strong>摘要：</strong>我们引入了 OLATverse，这是一个大型数据集，包含 765 个真实世界物体的约 900 万张图像，这些图像是在多种精确控制的照明条件下从多个视点捕获的。虽然以对象为中心的逆渲染、新颖的视图合成和重新照明方面的最新进展已经显示出有希望的结果，但大多数技术仍然严重依赖用于训练的合成数据集和用于基准测试的小规模现实世界数据集，这限制了它们的真实性和泛化性。为了解决这一差距，OLATverse 提供了优于现有数据集的两个关键优势：大规模覆盖真实对象以及精确控制照明下的高保真外观。具体来说，OLATverse 包含 765 个常见和不常见的现实世界对象，涵盖广泛的材料类别。每个物体均使用 35 个 DSLR 相机和 331 个独立控制的光源进行捕捉，从而能够模拟不同的照明条件。此外，对于每个对象，我们提供了经过良好校准的相机参数、精确的对象蒙版、光度表面法线和漫反射反照率作为辅助资源。我们还构建了一个广泛的评估集，为逆渲染和法线估计建立了第一个全面的现实世界以对象为中心的基准。我们相信 OLATverse 代表了将下一代逆渲染和重新照明方法与现实世界数据集成的关键一步。完整的数据集以及所有后处理工作流程将在此 https URL 公开发布。</li>
</ul>

<h3>Title: Object Detection as an Optional Basis: A Graph Matching Network for Cross-View UAV Localization</h3>
<ul>
<li><strong>Authors: </strong>Tao Liu, Kan Ren, Qian Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.02489">https://arxiv.org/abs/2511.02489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.02489">https://arxiv.org/pdf/2511.02489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.02489]] Object Detection as an Optional Basis: A Graph Matching Network for Cross-View UAV Localization(https://arxiv.org/abs/2511.02489)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the rapid growth of the low-altitude economy, UAVs have become crucial for measurement and tracking in patrol systems. However, in GNSS-denied areas, satellite-based localization methods are prone to failure. This paper presents a cross-view UAV localization framework that performs map matching via object detection, aimed at effectively addressing cross-temporal, cross-view, heterogeneous aerial image matching. In typical pipelines, UAV visual localization is formulated as an image-retrieval problem: features are extracted to build a localization map, and the pose of a query image is estimated by matching it to a reference database with known poses. Because publicly available UAV localization datasets are limited, many approaches recast localization as a classification task and rely on scene labels in these datasets to ensure accuracy. Other methods seek to reduce cross-domain differences using polar-coordinate reprojection, perspective transformations, or generative adversarial networks; however, they can suffer from misalignment, content loss, and limited realism. In contrast, we leverage modern object detection to accurately extract salient instances from UAV and satellite images, and integrate a graph neural network to reason about inter-image and intra-image node relationships. Using a fine-grained, graph-based node-similarity metric, our method achieves strong retrieval and localization performance. Extensive experiments on public and real-world datasets show that our approach handles heterogeneous appearance differences effectively and generalizes well, making it applicable to scenarios with larger modality gaps, such as infrared-visible image matching. Our dataset will be publicly available at the following URL: this https URL.</li>
<li><strong>摘要：</strong>随着低空经济的快速增长，无人机对于巡逻系统中的测量和跟踪变得至关重要。然而，在 GNSS 无法覆盖的地区，基于卫星的定位方法很容易失败。本文提出了一种跨视角无人机定位框架，通过目标检测进行地图匹配，旨在有效解决跨时空、跨视角、异构航空图像匹配问题。在典型的流程中，无人机视觉定位被表述为图像检索问题：提取特征以构建定位图，并通过将查询图像与具有已知姿势的参考数据库匹配来估计查询图像的姿势。由于公开可用的无人机定位数据集有限，许多方法将定位重新定义为分类任务，并依赖这些数据集中的场景标签来确保准确性。其他方法试图使用极坐标重投影、透视变换或生成对抗网络来减少跨域差异；然而，它们可能会出现错位、内容丢失和真实性有限的问题。相比之下，我们利用现代目标检测从无人机和卫星图像中准确提取显着实例，并集成图神经网络来推理图像间和图像内节点关系。使用细粒度、基于图的节点相似性度量，我们的方法实现了强大的检索和定位性能。对公共和现实世界数据集的大量实验表明，我们的方法可以有效处理异构外观差异并具有良好的泛化性，使其适用于模态差距较大的场景，例如红外-可见光图像匹配。我们的数据集将通过以下 URL 公开提供：此 https URL。</li>
</ul>

<h3>Title: BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and Monitoring</h3>
<ul>
<li><strong>Authors: </strong>Rajan Das Gupta, Md Kishor Morol, Nafiz Fahad, Md Tanzib Hosain, Sumaya Binte Zilani Choya, Md Jakir Hossen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.02490">https://arxiv.org/abs/2511.02490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.02490">https://arxiv.org/pdf/2511.02490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.02490]] BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and Monitoring(https://arxiv.org/abs/2511.02490)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>As the global burden of Alzheimer's disease (AD) continues to grow, early and accurate detection has become increasingly critical, especially in regions with limited access to advanced diagnostic tools. We propose BRAINS (Biomedical Retrieval-Augmented Intelligence for Neurodegeneration Screening) to address this challenge. This novel system harnesses the powerful reasoning capabilities of Large Language Models (LLMs) for Alzheimer's detection and monitoring. BRAINS features a dual-module architecture: a cognitive diagnostic module and a case-retrieval module. The Diagnostic Module utilizes LLMs fine-tuned on cognitive and neuroimaging datasets -- including MMSE, CDR scores, and brain volume metrics -- to perform structured assessments of Alzheimer's risk. Meanwhile, the Case Retrieval Module encodes patient profiles into latent representations and retrieves similar cases from a curated knowledge base. These auxiliary cases are fused with the input profile via a Case Fusion Layer to enhance contextual understanding. The combined representation is then processed with clinical prompts for inference. Evaluations on real-world datasets demonstrate BRAINS effectiveness in classifying disease severity and identifying early signs of cognitive decline. This system not only shows strong potential as an assistive tool for scalable, explainable, and early-stage Alzheimer's disease detection, but also offers hope for future applications in the field.</li>
<li><strong>摘要：</strong>随着阿尔茨海默病 (AD) 的全球负担持续增加，早期准确的检测变得越来越重要，特别是在先进诊断工具获取有限的地区。我们提出 BRAINS（用于神经退行性疾病筛查的生物医学检索增强智能）来应对这一挑战。这个新颖的系统利用大型语言模型 (LLM) 强大的推理能力来检测和监测阿尔茨海默病。 BRAINS 采用双模块架构：认知诊断模块和病例检索模块。诊断模块利用根据认知和神经影像数据集（包括 MMSE、CDR 评分和脑容量指标）进行微调的法学硕士，对阿尔茨海默病风险进行结构化评估。同时，病例检索模块将患者档案编码为潜在表示，并从策划的知识库中检索类似病例。这些辅助案例通过案例融合层与输入配置文件融合，以增强上下文理解。然后结合临床提示对组合表示进行处理以进行推理。对现实世界数据集的评估证明了 BRAINS 在对疾病严重程度进行分类和识别认知能力下降的早期迹象方面的有效性。该系统不仅显示出作为可扩展、可解释和早期阿尔茨海默病检测的辅助工具的强大潜力，而且还为该领域的未来应用带来了希望。</li>
</ul>

<h3>Title: DetectiumFire: A Comprehensive Multi-modal Dataset Bridging Vision and Language for Fire Understanding</h3>
<ul>
<li><strong>Authors: </strong>Zixuan Liu, Siavash H. Khajavi, Guangkai Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.02495">https://arxiv.org/abs/2511.02495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.02495">https://arxiv.org/pdf/2511.02495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.02495]] DetectiumFire: A Comprehensive Multi-modal Dataset Bridging Vision and Language for Fire Understanding(https://arxiv.org/abs/2511.02495)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in multi-modal models have demonstrated strong performance in tasks such as image generation and reasoning. However, applying these models to the fire domain remains challenging due to the lack of publicly available datasets with high-quality fire domain annotations. To address this gap, we introduce DetectiumFire, a large-scale, multi-modal dataset comprising of 22.5k high-resolution fire-related images and 2.5k real-world fire-related videos covering a wide range of fire types, environments, and risk levels. The data are annotated with both traditional computer vision labels (e.g., bounding boxes) and detailed textual prompts describing the scene, enabling applications such as synthetic data generation and fire risk reasoning. DetectiumFire offers clear advantages over existing benchmarks in scale, diversity, and data quality, significantly reducing redundancy and enhancing coverage of real-world scenarios. We validate the utility of DetectiumFire across multiple tasks, including object detection, diffusion-based image generation, and vision-language reasoning. Our results highlight the potential of this dataset to advance fire-related research and support the development of intelligent safety systems. We release DetectiumFire to promote broader exploration of fire understanding in the AI community. The dataset is available at this https URL</li>
<li><strong>摘要：</strong>多模态模型的最新进展在图像生成和推理等任务中表现出了强大的性能。然而，由于缺乏具有高质量火域注释的公开数据集，将这些模型应用于火域仍然具有挑战性。为了解决这一差距，我们引入了 DetectiumFire，这是一个大规模、多模式数据集，包含 22.5k 高分辨率火灾相关图像和 2.5k 真实世界火灾相关视频，涵盖广泛的火灾类型、环境和风险级别。这些数据使用传统的计算机视觉标签（例如边界框）和描述场景的详细文本提示进行注释，从而支持合成数据生成和火灾风险推理等应用。 DetectiumFire 在规模、多样性和数据质量方面比现有基准具有明显优势，显着减少冗余并增强对现实场景的覆盖。我们验证了 DetectiumFire 在多个任务中的实用性，包括对象检测、基于扩散的图像生成和视觉语言推理。我们的结果凸显了该数据集在推进火灾相关研究和支持智能安全系统开发方面的潜力。我们发布 DetectiumFire 是为了促进 AI 社区对火灾理解的更广泛探索。该数据集可在此 https URL 获取</li>
</ul>

<h3>Title: Keeping it Local, Tiny and Real: Automated Report Generation on Edge Computing Devices for Mechatronic-Based Cognitive Systems</h3>
<ul>
<li><strong>Authors: </strong>Nicolas Schuler, Lea Dewald, Jürgen Graf</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.02507">https://arxiv.org/abs/2511.02507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.02507">https://arxiv.org/pdf/2511.02507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.02507]] Keeping it Local, Tiny and Real: Automated Report Generation on Edge Computing Devices for Mechatronic-Based Cognitive Systems(https://arxiv.org/abs/2511.02507)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advancements in Deep Learning enable hardware-based cognitive systems, that is, mechatronic systems in general and robotics in particular with integrated Artificial Intelligence, to interact with dynamic and unstructured environments. While the results are impressive, the application of such systems to critical tasks like autonomous driving as well as service and care robotics necessitate the evaluation of large amount of heterogeneous data. Automated report generation for Mobile Robotics can play a crucial role in facilitating the evaluation and acceptance of such systems in various domains. In this paper, we propose a pipeline for generating automated reports in natural language utilizing various multi-modal sensors that solely relies on local models capable of being deployed on edge computing devices, thus preserving the privacy of all actors involved and eliminating the need for external services. In particular, we evaluate our implementation on a diverse dataset spanning multiple domains including indoor, outdoor and urban environments, providing quantitative as well as qualitative evaluation results. Various generated example reports and other supplementary materials are available via a public repository.</li>
<li><strong>摘要：</strong>深度学习的最新进展使基于硬件的认知系统（即一般的机电系统和特别是具有集成人工智能的机器人系统）能够与动态和非结构化环境进行交互。虽然结果令人印象深刻，但将此类系统应用于自动驾驶以及服务和护理机器人等关键任务需要评估大量异构数据。移动机器人的自动报告生成可以在促进各个领域对此类系统的评估和接受方面发挥至关重要的作用。在本文中，我们提出了一种利用各种多模式传感器以自然语言生成自动报告的管道，该管道仅依赖于能够部署在边缘计算设备上的本地模型，从而保护所有相关参与者的隐私并消除对外部服务的需求。特别是，我们在涵盖室内、室外和城市环境等多个领域的多样化数据集上评估我们的实施情况，提供定量和定性评估结果。各种生成的示例报告和其他补充材料可通过公共存储库获得。</li>
</ul>

<h3>Title: Causal Graph Neural Networks for Healthcare</h3>
<ul>
<li><strong>Authors: </strong>Munib Mesinovic, Max Buhlan, Tingting Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.02531">https://arxiv.org/abs/2511.02531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.02531">https://arxiv.org/pdf/2511.02531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.02531]] Causal Graph Neural Networks for Healthcare(https://arxiv.org/abs/2511.02531)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Healthcare artificial intelligence systems routinely fail when deployed across institutions, with documented performance drops and perpetuation of discriminatory patterns embedded in historical data. This brittleness stems, in part, from learning statistical associations rather than causal mechanisms. Causal graph neural networks address this triple crisis of distribution shift, discrimination, and inscrutability by combining graph-based representations of biomedical data with causal inference principles to learn invariant mechanisms rather than spurious correlations. This Review examines methodological foundations spanning structural causal models, disentangled causal representation learning, and techniques for interventional prediction and counterfactual reasoning on graphs. We analyse applications demonstrating clinical value across psychiatric diagnosis through brain network analysis, cancer subtyping via multi-omics causal integration, continuous physiological monitoring with mechanistic interpretation, and drug recommendation correcting prescription bias. These advances establish foundations for patient-specific Causal Digital Twins, enabling in silico clinical experimentation, with integration of large language models for hypothesis generation and causal graph neural networks for mechanistic validation. Substantial barriers remain, including computational requirements precluding real-time deployment, validation challenges demanding multi-modal evidence triangulation beyond cross-validation, and risks of causal-washing where methods employ causal terminology without rigorous evidentiary support. We propose tiered frameworks distinguishing causally-inspired architectures from causally-validated discoveries and identify critical research priorities making causal rather than purely associational claims.</li>
<li><strong>摘要：</strong>医疗保健人工智能系统在跨机构部署时通常会失败，记录下来的性能下降以及历史数据中嵌入的歧视模式永久存在。这种脆弱性部分源于学习统计关联而不是因果机制。因果图神经网络通过将基于图的生物医学数据表示与因果推理原理相结合来学习不变机制而不是虚假相关性，解决了分布转移、歧视和不可预测性的三重危机。这篇综述探讨了跨越结构因果模型、解开因果表示学习以及图形干预预测和反事实推理技术的方法基础。我们分析了通过脑网络分析在精神病学诊断、通过多组学因果整合进行癌症亚型分型、通过机械解释进行连续生理监测以及纠正处方偏倚的药物推荐等方面展示临床价值的应用程序。这些进步为特定于患者的因果数字孪生奠定了基础，通过集成用于假设生成的大型语言模型和用于机械验证的因果图神经网络，可以进行计算机模拟临床实验。仍然存在巨大的障碍，包括妨碍实时部署的计算要求、需要交叉验证之外的多模式证据三角测量的验证挑战，以及方法在没有严格证据支持的情况下采用因果术语的因果清洗风险。我们提出分层框架，将因果启发的架构与因果验证的发现区分开来，并确定关键的研究优先事项，提出因果而非纯粹的关联主张。</li>
</ul>

<h3>Title: Forecasting Future Anatomies: Longitudianl Brain Mri-to-Mri Prediction</h3>
<ul>
<li><strong>Authors: </strong>Ali Farki, Elaheh Moradi, Deepika Koundal, Jussi Tohka</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.02558">https://arxiv.org/abs/2511.02558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.02558">https://arxiv.org/pdf/2511.02558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.02558]] Forecasting Future Anatomies: Longitudianl Brain Mri-to-Mri Prediction(https://arxiv.org/abs/2511.02558)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Predicting future brain state from a baseline magnetic resonance image (MRI) is a central challenge in neuroimaging and has important implications for studying neurodegenerative diseases such as Alzheimer's disease (AD). Most existing approaches predict future cognitive scores or clinical outcomes, such as conversion from mild cognitive impairment to dementia. Instead, here we investigate longitudinal MRI image-to-image prediction that forecasts a participant's entire brain MRI several years into the future, intrinsically modeling complex, spatially distributed neurodegenerative patterns. We implement and evaluate five deep learning architectures (UNet, U2-Net, UNETR, Time-Embedding UNet, and ODE-UNet) on two longitudinal cohorts (ADNI and AIBL). Predicted follow-up MRIs are directly compared with the actual follow-up scans using metrics that capture global similarity and local differences. The best performing models achieve high-fidelity predictions, and all models generalize well to an independent external dataset, demonstrating robust cross-cohort performance. Our results indicate that deep learning can reliably predict participant-specific brain MRI at the voxel level, offering new opportunities for individualized prognosis.</li>
<li><strong>摘要：</strong>从基线磁共振图像 (MRI) 预测未来的大脑状态是神经影像学的一个核心挑战，对于研究阿尔茨海默病 (AD) 等神经退行性疾病具有重要意义。大多数现有方法预测未来的认知评分或临床结果，例如从轻度认知障碍到痴呆的转变。相反，我们在这里研究纵向 MRI 图像到图像预测，预测参与者未来几年的整个大脑 MRI，本质上模拟复杂的、空间分布的神经退行性模式。我们在两个纵向队列（ADNI 和 AIBL）上实现并评估了五种深度学习架构（UNet、U2-Net、UNETR、Time-Embedding UNet 和 ODE-UNet）。使用捕获全局相似性和局部差异的指标，将预测的后续 MRI 直接与实际的后续扫描进行比较。性能最佳的模型可以实现高保真度预测，并且所有模型都可以很好地推广到独立的外部数据集，从而展示出强大的跨队列性能。我们的结果表明，深度学习可以在体素水平上可靠地预测参与者特定的大脑 MRI，为个体化预后提供新的机会。</li>
</ul>

<h3>Title: TAUE: Training-free Noise Transplant and Cultivation Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Daichi Nagai, Ryugo Morita, Shunsuke Kitada, Hitoshi Iyatomi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.02580">https://arxiv.org/abs/2511.02580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.02580">https://arxiv.org/pdf/2511.02580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.02580]] TAUE: Training-free Noise Transplant and Cultivation Diffusion Model(https://arxiv.org/abs/2511.02580)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Despite the remarkable success of text-to-image diffusion models, their output of a single, flattened image remains a critical bottleneck for professional applications requiring layer-wise control. Existing solutions either rely on fine-tuning with large, inaccessible datasets or are training-free yet limited to generating isolated foreground elements, failing to produce a complete and coherent scene. To address this, we introduce the Training-free Noise Transplantation and Cultivation Diffusion Model (TAUE), a novel framework for zero-shot, layer-wise image generation. Our core technique, Noise Transplantation and Cultivation (NTC), extracts intermediate latent representations from both foreground and composite generation processes, transplanting them into the initial noise for subsequent layers. This ensures semantic and structural coherence across foreground, background, and composite layers, enabling consistent, multi-layered outputs without requiring fine-tuning or auxiliary datasets. Extensive experiments show that our training-free method achieves performance comparable to fine-tuned methods, enhancing layer-wise consistency while maintaining high image quality and fidelity. TAUE not only eliminates costly training and dataset requirements but also unlocks novel downstream applications, such as complex compositional editing, paving the way for more accessible and controllable generative workflows.</li>
<li><strong>摘要：</strong>尽管文本到图像扩散模型取得了显着的成功，但它们的单个扁平图像的输出仍然是需要分层控制的专业应用程序的关键瓶颈。现有的解决方案要么依赖于对大型、不可访问的数据集进行微调，要么无需训练，但仅限于生成孤立的前景元素，无法生成完整且连贯的场景。为了解决这个问题，我们引入了免训练噪声移植和培养扩散模型（TAUE），这是一种用于零样本、分层图像生成的新颖框架。我们的核心技术，噪声移植和培养（NTC），从前景和复合生成过程中提取中间潜在表示，将它们移植到后续层的初始噪声中。这确保了前景、背景和复合层之间的语义和结构一致性，从而无需微调或辅助数据集即可实现一致的多层输出。大量实验表明，我们的免训练方法实现了与微调方法相当的性能，增强了分层一致性，同时保持了高图像质量和保真度。 TAUE 不仅消除了昂贵的培训和数据集要求，而且还解锁了新颖的下游应用程序，例如复杂的合成编辑，为更易于访问和控制的生成工作流程铺平了道路。</li>
</ul>

<h3>Title: UniChange: Unifying Change Detection with Multimodal Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Xu Zhang, Danyang Li, Xiaohang Dong, Tianhao Wu, Hualong Yu, Jianye Wang, Qicheng Li, Xiang Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.02607">https://arxiv.org/abs/2511.02607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.02607">https://arxiv.org/pdf/2511.02607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.02607]] UniChange: Unifying Change Detection with Multimodal Large Language Model(https://arxiv.org/abs/2511.02607)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Change detection (CD) is a fundamental task for monitoring and analyzing land cover dynamics. While recent high performance models and high quality datasets have significantly advanced the field, a critical limitation persists. Current models typically acquire limited knowledge from single-type annotated data and cannot concurrently leverage diverse binary change detection (BCD) and semantic change detection (SCD) datasets. This constraint leads to poor generalization and limited versatility. The recent advancements in Multimodal Large Language Models (MLLMs) introduce new possibilities for a unified CD framework. We leverage the language priors and unification capabilities of MLLMs to develop UniChange, the first MLLM-based unified change detection model. UniChange integrates generative language abilities with specialized CD functionalities. Our model successfully unifies both BCD and SCD tasks through the introduction of three special tokens: [T1], [T2], and [CHANGE]. Furthermore, UniChange utilizes text prompts to guide the identification of change categories, eliminating the reliance on predefined classification heads. This design allows UniChange to effectively acquire knowledge from multi-source datasets, even when their class definitions conflict. Experiments on four public benchmarks (WHU-CD, S2Looking, LEVIR-CD+, and SECOND) demonstrate SOTA performance, achieving IoU scores of 90.41, 53.04, 78.87, and 57.62, respectively, surpassing all previous methods. The code is available at this https URL.</li>
<li><strong>摘要：</strong>变化检测（CD）是监测和分析土地覆盖动态的一项基本任务。虽然最近的高性能模型和高质量数据集显着推进了该领域的发展，但仍然存在一个关键的限制。当前的模型通常从单一类型的注释数据中获取有限的知识，并且不能同时利用不同的二进制变化检测（BCD）和语义变化检测（SCD）数据集。这种限制导致通用性差和通用性有限。多模态大型语言模型 (MLLM) 的最新进展为统一 CD 框架带来了新的可能性。我们利用 MLLM 的语言先验和统一功能来开发 UniChange，这是第一个基于 MLLM 的统一变更检测模型。 UniChange 将生成语言能力与专门的 CD 功能集成在一起。我们的模型通过引入三个特殊标记成功地统一了 BCD 和 SCD 任务：[T1]、[T2] 和 [CHANGE]。此外，UniChange 利用文本提示来指导变更类别的识别，消除了对预定义分类头的依赖。这种设计使 UniChange 能够有效地从多源数据集中获取知识，即使它们的类定义发生冲突。在四个公共基准（WHU-CD、S2Looking、LEVIR-CD+ 和 SECOND）上的实验证明了 SOTA 性能，IoU 分数分别为 90.41、53.04、78.87 和 57.62，超越了之前的所有方法。该代码可从此 https URL 获取。</li>
</ul>

<h3>Title: A Non-Adversarial Approach to Idempotent Generative Modelling</h3>
<ul>
<li><strong>Authors: </strong>Mohammed Al-Jaff, Giovanni Luca Marchetti, Michael C Welle, Jens Lundell, Mats G. Gustafsson, Gustav Eje Henter, Hossein Azizpour, Danica Kragic</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.02614">https://arxiv.org/abs/2511.02614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.02614">https://arxiv.org/pdf/2511.02614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.02614]] A Non-Adversarial Approach to Idempotent Generative Modelling(https://arxiv.org/abs/2511.02614)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Idempotent Generative Networks (IGNs) are deep generative models that also function as local data manifold projectors, mapping arbitrary inputs back onto the manifold. They are trained to act as identity operators on the data and as idempotent operators off the data manifold. However, IGNs suffer from mode collapse, mode dropping, and training instability due to their objectives, which contain adversarial components and can cause the model to cover the data manifold only partially -- an issue shared with generative adversarial networks. We introduce Non-Adversarial Idempotent Generative Networks (NAIGNs) to address these issues. Our loss function combines reconstruction with the non-adversarial generative objective of Implicit Maximum Likelihood Estimation (IMLE). This improves on IGN's ability to restore corrupted data and generate new samples that closely match the data distribution. We moreover demonstrate that NAIGNs implicitly learn the distance field to the data manifold, as well as an energy-based model.</li>
<li><strong>摘要：</strong>幂等生成网络（IGN）是深度生成模型，也充当本地数据流形投影仪，将任意输入映射回流形。他们被训练充当数据上的身份运算符和数据流形上的幂等运算符。然而，IGN 由于其目标而遭受模式崩溃、模式下降和训练不稳定的问题，其中包含对抗性成分，并可能导致模型仅部分覆盖数据流形——这是生成对抗性网络共有的问题。我们引入非对抗幂等生成网络（NAIGN）来解决这些问题。我们的损失函数将重建与隐式最大似然估计（IMLE）的非对抗性生成目标相结合。这提高了 IGN 恢复损坏数据并生成与数据分布紧密匹配的新样本的能力。此外，我们还证明 NAIGN 隐式学习数据流形的距离场以及基于能量的模型。</li>
</ul>

<h3>Title: STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation</h3>
<ul>
<li><strong>Authors: </strong>Bum Chul Kwon, Ben Shapira, Moshiko Raboh, Shreyans Sethi, Shruti Murarka, Joseph A Morrone, Jianying Hu, Parthasarathy Suryanarayanan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.02769">https://arxiv.org/abs/2511.02769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.02769">https://arxiv.org/pdf/2511.02769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.02769]] STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation(https://arxiv.org/abs/2511.02769)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The chemical space of drug-like molecules is vast, motivating the development of generative models that must learn broad chemical distributions, enable conditional generation by capturing structure-property representations, and provide fast molecular generation. Meeting the objectives depends on modeling choices, including the probabilistic modeling approach, the conditional generative formulation, the architecture, and the molecular input representation. To address the challenges, we present STAR-VAE (Selfies-encoded, Transformer-based, AutoRegressive Variational Auto Encoder), a scalable latent-variable framework with a Transformer encoder and an autoregressive Transformer decoder. It is trained on 79 million drug-like molecules from PubChem, using SELFIES to guarantee syntactic validity. The latent-variable formulation enables conditional generation: a property predictor supplies a conditioning signal that is applied consistently to the latent prior, the inference network, and the decoder. Our contributions are: (i) a Transformer-based latent-variable encoder-decoder model trained on SELFIES representations; (ii) a principled conditional latent-variable formulation for property-guided generation; and (iii) efficient finetuning with low-rank adapters (LoRA) in both encoder and decoder, enabling fast adaptation with limited property and activity data. On the GuacaMol and MOSES benchmarks, our approach matches or exceeds baselines, and latent-space analyses reveal smooth, semantically structured representations that support both unconditional exploration and property-aware generation. On the Tartarus benchmarks, the conditional model shifts docking-score distributions toward stronger predicted binding. These results suggest that a modernized, scale-appropriate VAE remains competitive for molecular generation when paired with principled conditioning and parameter-efficient finetuning.</li>
<li><strong>摘要：</strong>类药物分子的化学空间巨大，推动了生成模型的发展，这些模型必须学习广泛的化学分布，通过捕获结构-性质表示来实现条件生成，并提供快速的分子生成。实现目标取决于建模选择，包括概率建模方法、条件生成公式、体系结构和分子输入表示。为了应对这些挑战，我们提出了 STAR-VAE（自拍编码、基于 Transformer 的自回归变分自动编码器），这是一个带有 Transformer 编码器和自回归 Transformer 解码器的可扩展潜变量框架。它接受了来自 PubChem 的 7900 万个类药物分子的训练，并使用 SELFIES 来保证语法的有效性。潜在变量公式可以实现条件生成：属性预测器提供一致应用于潜在先验、推理网络和解码器的条件信号。我们的贡献是：（i）基于 Transformer 的潜变量编码器-解码器模型，在 SELFIES 表示上进行训练； (ii) 用于属性引导生成的有原则的条件潜变量公式； (iii) 在编码器和解码器中使用低秩适配器 (LoRA) 进行高效微调，从而能够快速适应有限的属性和活动数据。在 GuacaMol 和 MOSES 基准上，我们的方法匹配或超过基线，并且潜在空间分析揭示了平滑的语义结构化表示，支持无条件探索和属性感知生成。在 Tartarus 基准上，条件模型将对接分数分布转向更强的预测结合。这些结果表明，现代化的、规模适当的 VAE 在与原则性调节和参数有效的微调相结合时，在分子生成方面仍然具有竞争力。</li>
</ul>

<h3>Title: VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation</h3>
<ul>
<li><strong>Authors: </strong>Kevin Qinghong Lin, Yuhao Zheng, Hangyu Ran, Dantong Zhu, Dongxing Mao, Linjie Li, Philip Torr, Alex Jinpeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.02778">https://arxiv.org/abs/2511.02778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.02778">https://arxiv.org/pdf/2511.02778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.02778]] VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation(https://arxiv.org/abs/2511.02778)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Code has emerged as a precise and executable medium for reasoning and action in the agent era. Yet, progress has largely focused on language-centric tasks such as program synthesis and debugging, leaving visual-centric coding underexplored. Inspired by how humans reason over sketches, we advocate SVG code as a compact, interpretable, and executable visual representation. We introduce VCode, a benchmark that reframes multimodal understanding as code generation: given an image, a model must produce SVG that preserves symbolic meaning for downstream reasoning. VCode covers three domains - general commonsense (MM-Vet), professional disciplines (MMMU), and visual-centric perception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel evaluation protocol in which a policy model answers questions over rendered SVGs; correct answers indicate faithful symbolic preservation. Empirically, frontier VLMs struggle to generate faithful SVGs, revealing a persistent gap between language-centric and visual-centric coding. To close this gap, we introduce VCoder, an agentic framework that augments VLMs along two axes: (i) Thinking with Revision, which iteratively analyzes discrepancies and refines SVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply structured cues such as objects, shapes, and text beyond the model's intrinsic capacity. Across benchmarks, frontier VLMs with strong reasoning capabilities score well overall yet remain limited in professional knowledge and 3D reasoning. VCoder delivers a 12.3-point overall gain over the top-performing Claude-4-Opus. Human studies show that both humans and VLMs perform worse on rendered SVGs, their consistency reveals the promise of symbolic visual representation. The benchmark and code are available at this https URL.</li>
<li><strong>摘要：</strong>代码已成为代理时代推理和行动的精确且可执行的媒介。然而，进展主要集中在以语言为中心的任务上，例如程序合成和调试，而以视觉为中心的编码尚未得到充分探索。受到人类如何推理草图的启发，我们提倡将 SVG 代码作为一种紧凑、可解释且可执行的视觉表示形式。我们引入了 VCode，这是一个将多模态理解重新构建为代码生成的基准：给定图像，模型必须生成 SVG，为下游推理保留符号含义。 VCode涵盖三个领域——一般常识（MM-Vet）、专业学科（MMMU）和以视觉为中心的感知（CV-Bench）。为了评估符号保真度，我们提出了 CodeVQA，这是一种新颖的评估协议，其中策略模型回答有关渲染 SVG 的问题；正确的答案表明忠实的象征性保存。根据经验，前沿 VLM 很难生成忠实的 SVG，揭示了以语言为中心的编码和以视觉为中心的编码之间持续存在的差距。为了弥补这一差距，我们引入了 VCoder，这是一个代理框架，它沿着两个轴增强了 VLM：（i）通过修订思考，迭代地分析差异并改进 SVG 代码； (ii) 使用视觉工具进行操作，其中检测器和解析器提供超出模型内在能力的结构化线索，例如对象、形状和文本。在各个基准测试中，具有强大推理能力的前沿 VLM 总体得分较高，但在专业知识和 3D 推理方面仍然有限。 VCoder 的整体增益比表现最佳的 Claude-4-Opus 提高了 12.3 点。人类研究表明，人类和 VLM 在渲染 SVG 上的表现都较差，但它们的一致性揭示了符号视觉表示的前景。基准测试和代码可从此 https URL 获取。</li>
</ul>

<h3>Title: PLUTO-4: Frontier Pathology Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Harshith Padigela, Shima Nofallah, Atchuth Naveen Chilaparasetti, Ryun Han, Andrew Walker, Judy Shen, Chintan Shah, Blake Martin, Aashish Sood, Elliot Miller, Ben Glass, Andy Beck, Harsha Pokkalla, Syed Ashar Javed</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.02826">https://arxiv.org/abs/2511.02826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.02826">https://arxiv.org/pdf/2511.02826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.02826]] PLUTO-4: Frontier Pathology Foundation Models(https://arxiv.org/abs/2511.02826)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Foundation models trained on large-scale pathology image corpora have demonstrated strong transfer capabilities across diverse histopathology tasks. Building on this progress, we introduce PLUTO-4, our next generation of pathology foundation models that extend the Pathology-Universal Transformer (PLUTO) to frontier scale. We share two complementary Vision Transformer architectures in the PLUTO-4 family: a compact and efficient PLUTO-4S model optimized for multi-scale deployment using a FlexiViT setup with 2D-RoPE embeddings, and a frontier-scale PLUTO-4G model trained with a single patch size to maximize representation capacity and stability. Both models are pretrained using a self-supervised objective derived from DINOv2 on a large multi-institutional corpus containing 551,164 WSIs from 137,144 patients across over 50 institutions, spanning over 60 disease types and over 100 stains. Comprehensive evaluation across public and internal benchmarks demonstrates that PLUTO-4 achieves state-of-the-art performance on tasks requiring varying spatial and biological context, including patch-level classification, segmentation, and slide-level diagnosis. The compact PLUTO-4S provides high-throughput and robust performance for practical deployment, while PLUTO-4G establishes new performance frontiers across multiple pathology benchmarks, including an 11% improvement in dermatopathology diagnosis. These diverse improvements underscore PLUTO-4's potential to transform real-world applications as a backbone for translational research and diagnostic use cases.</li>
<li><strong>摘要：</strong>在大规模病理学图像语料库上训练的基础模型已表现出跨不同组织病理学任务的强大传输能力。在此进展的基础上，我们推出了 PLUTO-4，这是我们的下一代病理学基础模型，它将病理学通用转换器 (PLUTO) 扩展到前沿规模。我们在 PLUTO-4 系列中共享两种互补的 Vision Transformer 架构：一个紧凑高效的 PLUTO-4S 模型，使用具有 2D-RoPE 嵌入的 FlexiViT 设置针对多规模部署进行了优化；以及一个前沿规模的 PLUTO-4G 模型，使用单个补丁大小进行训练，以最大限度地提高表示能力和稳定性。两种模型均使用源自 DINOv2 的自我监督目标进行预训练，该目标在大型多机构语料库上进行，其中包含来自 50 多个机构的 137,144 名患者的 551,164 个 WSI，涵盖 60 多种疾病类型和 100 多种染色。对公共和内部基准的综合评估表明，PLUTO-4 在需要不同空间和生物背景的任务上实现了最先进的性能，包括补丁级分类、分割和幻灯片级诊断。紧凑型 PLUTO-4S 为实际部署提供了高通量和强大的性能，而 PLUTO-4G 在多个病理学基准上建立了新的性能前沿，包括皮肤病理学诊断提高 11%。这些不同的改进强调了 PLUTO-4 作为转化研究和诊断用例的支柱来转变现实世界应用程序的潜力。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
