<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-01-14</h1>
<h3>Title: RewriteNets: End-to-End Trainable String-Rewriting for Generative Sequence Modeling</h3>
<ul>
<li><strong>Authors: </strong>Harshil Vejendla</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.07868">https://arxiv.org/abs/2601.07868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.07868">https://arxiv.org/pdf/2601.07868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.07868]] RewriteNets: End-to-End Trainable String-Rewriting for Generative Sequence Modeling(https://arxiv.org/abs/2601.07868)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Dominant sequence models like the Transformer represent structure implicitly through dense attention weights, incurring quadratic complexity. We propose RewriteNets, a novel neural architecture built on an alternative paradigm: explicit, parallel string rewriting. Each layer in a RewriteNet contains a set of learnable rules. For each position in an input sequence, the layer performs four operations: (1) fuzzy matching of rule patterns, (2) conflict resolution via a differentiable assignment operator to select non-overlapping rewrites, (3) application of the chosen rules to replace input segments with output segments of potentially different lengths, and (4) propagation of untouched tokens. While the discrete assignment of rules is non-differentiable, we employ a straight-through Gumbel-Sinkhorn estimator, enabling stable end-to-end training. We evaluate RewriteNets on algorithmic, compositional, and string manipulation tasks, comparing them against strong LSTM and Transformer baselines. Results show that RewriteNets excel at tasks requiring systematic generalization (achieving 98.7% accuracy on the SCAN benchmark's length split) and are computationally more efficient than Transformers. We also provide an analysis of learned rules and an extensive ablation study, demonstrating that this architecture presents a promising direction for sequence modeling with explicit structural inductive biases.</li>
<li><strong>摘要：</strong>像 Transformer 这样的主导序列模型通过密集的注意力权重隐式地表示结构，从而产生二次复杂度。我们提出了 RewriteNets，这是一种基于替代范式构建的新型神经架构：显式并行字符串重写。 RewriteNet 中的每一层都包含一组可学习的规则。对于输入序列中的每个位置，该层执行四个操作：（1）规则模式的模糊匹配，（2）通过可微分赋值运算符解决冲突以选择非重叠重写，（3）应用所选规则以用可能不同长度的输出段替换输入段，以及（4）传播未触及的令牌。虽然规则的离散分配是不可微的，但我们采用直通式 Gumbel-Sinkhorn 估计器，从而实现稳定的端到端训练。我们在算法、组合和字符串操作任务上评估 RewriteNet，并将其与强大的 LSTM 和 Transformer 基线进行比较。结果表明，RewriteNet 擅长需要系统泛化的任务（在 SCAN 基准的长度分割上实现 98.7% 的准确率），并且计算效率比 Transformer 更高。我们还提供了对学习规则的分析和广泛的消融研究，证明这种架构为具有显式结构归纳偏差的序列建模提供了一个有希望的方向。</li>
</ul>

<h3>Title: Coupled Diffusion-Encoder Models for Reconstruction of Flow Fields</h3>
<ul>
<li><strong>Authors: </strong>AmirPouya Hemmasian, Amir Barati Farimani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.07946">https://arxiv.org/abs/2601.07946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.07946">https://arxiv.org/pdf/2601.07946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.07946]] Coupled Diffusion-Encoder Models for Reconstruction of Flow Fields(https://arxiv.org/abs/2601.07946)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Data-driven flow-field reconstruction typically relies on autoencoder architectures that compress high-dimensional states into low-dimensional latent representations. However, classical approaches such as variational autoencoders (VAEs) often struggle to preserve the higher-order statistical structure of fluid flows when subjected to strong compression. We propose DiffCoder, a coupled framework that integrates a probabilistic diffusion model with a conventional convolutional ResNet encoder and trains both components end-to-end. The encoder compresses the flow field into a latent representation, while the diffusion model learns a generative prior over reconstructions conditioned on the compressed state. This design allows DiffCoder to recover distributional and spectral properties that are not strictly required for minimizing pointwise reconstruction loss but are critical for faithfully representing statistical properties of the flow field. We evaluate DiffCoder and VAE baselines across multiple model sizes and compression ratios on a challenging dataset of Kolmogorov flow fields. Under aggressive compression, DiffCoder significantly improves the spectral accuracy while VAEs exhibit substantial degradation. Although both methods show comparable relative L2 reconstruction error, DiffCoder better preserves the underlying distributional structure of the flow. At moderate compression levels, sufficiently large VAEs remain competitive, suggesting that diffusion-based priors provide the greatest benefit when information bottlenecks are severe. These results demonstrate that the generative decoding by diffusion offers a promising path toward compact, statistically consistent representations of complex flow fields.</li>
<li><strong>摘要：</strong>数据驱动的流场重建通常依赖于将高维状态压缩为低维潜在表示的自动编码器架构。然而，诸如变分自动编码器（VAE）之类的经典方法在受到强压缩时通常很难保留流体流动的高阶统计结构。我们提出了 DiffCoder，这是一个耦合框架，它将概率扩散模型与传统的卷积 ResNet 编码器集成在一起，并端到端地训练两个组件。编码器将流场压缩为潜在表示，而扩散模型则学习基于压缩状态的重建的生成先验。这种设计允许 DiffCoder 恢复分布和光谱特性，这些特性并不是最小化逐点重建损失所严格要求的，但对于忠实表示流场的统计特性至关重要。我们在具有挑战性的柯尔莫哥洛夫流场数据集上评估了多种模型大小和压缩比的 DiffCoder 和 VAE 基线。在积极的压缩下，DiffCoder 显着提高了频谱精度，而 VAE 则表现出显着的退化。尽管两种方法都显示出相当的相对 L2 重建误差，但 DiffCoder 更好地保留了流的底层分布结构。在中等压缩水平下，足够大的 VAE 仍然具有竞争力，这表明当信息瓶颈严重时，基于扩散的先验可以提供最大的好处。这些结果表明，通过扩散进行的生成解码为复杂流场的紧凑、统计一致的表示提供了一条有希望的途径。</li>
</ul>

<h3>Title: 3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing</h3>
<ul>
<li><strong>Authors: </strong>Jiahua Dong, Yu-Xiong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.07963">https://arxiv.org/abs/2601.07963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.07963">https://arxiv.org/pdf/2601.07963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.07963]] 3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing(https://arxiv.org/abs/2601.07963)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The transformative potential of 3D content creation has been progressively unlocked through advancements in generative models. Recently, intuitive drag editing with geometric changes has attracted significant attention in 2D editing yet remains challenging for 3D scenes. In this paper, we introduce 3DGS-Drag -- a point-based 3D editing framework that provides efficient, intuitive drag manipulation of real 3D scenes. Our approach bridges the gap between deformation-based and 2D-editing-based 3D editing methods, addressing their limitations to geometry-related content editing. We leverage two key innovations: deformation guidance utilizing 3D Gaussian Splatting for consistent geometric modifications and diffusion guidance for content correction and visual quality enhancement. A progressive editing strategy further supports aggressive 3D drag edits. Our method enables a wide range of edits, including motion change, shape adjustment, inpainting, and content extension. Experimental results demonstrate the effectiveness of 3DGS-Drag in various scenes, achieving state-of-the-art performance in geometry-related 3D content editing. Notably, the editing is efficient, taking 10 to 20 minutes on a single RTX 4090 GPU.</li>
<li><strong>摘要：</strong>通过生成模型的进步，3D 内容创作的变革潜力已逐渐释放。最近，具有几何变化的直观拖动编辑在 2D 编辑中引起了极大的关注，但对于 3D 场景仍然具有挑战性。在本文中，我们介绍了 3DGS-Drag——一种基于点的 3D 编辑框架，可为真实 3D 场景提供高效、直观的拖动操作。我们的方法弥合了基于变形和基于 2D 编辑的 3D 编辑方法之间的差距，解决了它们对几何相关内容编辑的限制。我们利用两项关键创新：利用 3D 高斯分布进行一致的几何修改的变形指导和用于内容校正和视觉质量增强的扩散指导。渐进式编辑策略进一步支持激进的 3D 拖动编辑。我们的方法可以进行广泛的编辑，包括运动改变、形状调整、修复和内容扩展。实验结果证明了 3DGS-Drag 在各种场景中的有效性，在几何相关的 3D 内容编辑中实现了最先进的性能。值得注意的是，编辑效率很高，在单个 RTX 4090 GPU 上只需 10 到 20 分钟。</li>
</ul>

<h3>Title: Decoder Generates Manufacturable Structures: A Framework for 3D-Printable Object Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Abhishek Kumar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08015">https://arxiv.org/abs/2601.08015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08015">https://arxiv.org/pdf/2601.08015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08015]] Decoder Generates Manufacturable Structures: A Framework for 3D-Printable Object Synthesis(https://arxiv.org/abs/2601.08015)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This paper presents a novel decoder-based approach for generating manufacturable 3D structures optimized for additive manufacturing. We introduce a deep learning framework that decodes latent representations into geometrically valid, printable objects while respecting manufacturing constraints such as overhang angles, wall thickness, and structural integrity. The methodology demonstrates that neural decoders can learn complex mapping functions from abstract representations to valid 3D geometries, producing parts with significantly improved manufacturability compared to naive generation approaches. We validate the approach on diverse object categories and demonstrate practical 3D printing of decoder-generated structures.</li>
<li><strong>摘要：</strong>本文提出了一种基于解码器的新颖方法，用于生成针对增材制造优化的可制造 3D 结构。我们引入了一个深度学习框架，该框架将潜在表示解码为几何有效的可打印对象，同时尊重悬垂角度、壁厚和结构完整性等制造约束。该方法表明，神经解码器可以学习从抽象表示到有效 3D 几何形状的复杂映射函数，与简单的生成方法相比，生产的零件的可制造性显着提高。我们在不同的对象类别上验证了该方法，并演示了解码器生成的结构的实用 3D 打印。</li>
</ul>

<h3>Title: From Prompts to Deployment: Auto-Curated Domain-Specific Dataset Generation via Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Dongsik Yoon, Jongeun Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08095">https://arxiv.org/abs/2601.08095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08095">https://arxiv.org/pdf/2601.08095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08095]] From Prompts to Deployment: Auto-Curated Domain-Specific Dataset Generation via Diffusion Models(https://arxiv.org/abs/2601.08095)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In this paper, we present an automated pipeline for generating domain-specific synthetic datasets with diffusion models, addressing the distribution shift between pre-trained models and real-world deployment environments. Our three-stage framework first synthesizes target objects within domain-specific backgrounds through controlled inpainting. The generated outputs are then validated via a multi-modal assessment that integrates object detection, aesthetic scoring, and vision-language alignment. Finally, a user-preference classifier is employed to capture subjective selection criteria. This pipeline enables the efficient construction of high-quality, deployable datasets while reducing reliance on extensive real-world data collection.</li>
<li><strong>摘要：</strong>在本文中，我们提出了一种自动化管道，用于使用扩散模型生成特定领域的合成数据集，解决预训练模型和实际部署环境之间的分布变化。我们的三阶段框架首先通过受控修复在特定领域的背景中合成目标对象。然后通过集成对象检测、美学评分和视觉语言对齐的多模式评估来验证生成的输出。最后，采用用户偏好分类器来捕获主观选择标准。该管道可以高效构建高质量、可部署的数据集，同时减少对广泛的现实世界数据收集的依赖。</li>
</ul>

<h3>Title: PathoGen: Diffusion-Based Synthesis of Realistic Lesions in Histopathology Images</h3>
<ul>
<li><strong>Authors: </strong>Mohamad Koohi-Moghadam, Mohammad-Ali Nikouei Mahani, Kyongtae Tyler Bae</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08127">https://arxiv.org/abs/2601.08127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08127">https://arxiv.org/pdf/2601.08127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08127]] PathoGen: Diffusion-Based Synthesis of Realistic Lesions in Histopathology Images(https://arxiv.org/abs/2601.08127)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The development of robust artificial intelligence models for histopathology diagnosis is severely constrained by the scarcity of expert-annotated lesion data, particularly for rare pathologies and underrepresented disease subtypes. While data augmentation offers a potential solution, existing methods fail to generate sufficiently realistic lesion morphologies that preserve the complex spatial relationships and cellular architectures characteristic of histopathological tissues. Here we present PathoGen, a diffusion-based generative model that enables controllable, high-fidelity inpainting of lesions into benign histopathology images. Unlike conventional augmentation techniques, PathoGen leverages the iterative refinement process of diffusion models to synthesize lesions with natural tissue boundaries, preserved cellular structures, and authentic staining characteristics. We validate PathoGen across four diverse datasets representing distinct diagnostic challenges: kidney, skin, breast, and prostate pathology. Quantitative assessment confirms that PathoGen outperforms state-of-the-art generative baselines, including conditional GAN and Stable Diffusion, in image fidelity and distributional similarity. Crucially, we show that augmenting training sets with PathoGen-synthesized lesions enhances downstream segmentation performance compared to traditional geometric augmentations, particularly in data-scarce regimes. Besides, by simultaneously generating realistic morphology and pixel-level ground truth, PathoGen effectively overcomes the manual annotation bottleneck. This approach offers a scalable pathway for developing generalizable medical AI systems despite limited expert-labeled data.</li>
<li><strong>摘要：</strong>用于组织病理学诊断的强大人工智能模型的开发受到专家注释病变数据稀缺的严重限制，特别是对于罕见病理和代表性不足的疾病亚型。虽然数据增强提供了一种潜在的解决方案，但现有方法无法生成足够真实的病变形态来保留组织病理学组织的复杂空间关系和细胞结构特征。在这里，我们介绍 PathoGen，一种基于扩散的生成模型，能够将病变可控、高保真地修复为良性组织病理学图像。与传统的增强技术不同，PathoGen 利用扩散模型的迭代细化过程来合成具有自然组织边界、保留的细胞结构和真实染色特征的病变。我们在代表不同诊断挑战的四个不同数据集上验证 PathoGen：肾脏、皮肤、乳房和前列腺病理学。定量评估证实，PathoGen 在图像保真度和分布相似性方面优于最先进的生成基线，包括条件 GAN 和稳定扩散。至关重要的是，我们表明，与传统的几何增强相比，使用 PathoGen 合成的病变增强训练集可以增强下游分割性能，特别是在数据稀缺的情况下。此外，通过同时生成逼真的形态学和像素级地面实况，PathoGen 有效克服了手动注释的瓶颈。尽管专家标记的数据有限，但这种方法为开发通用医疗人工智能系统提供了一条可扩展的途径。</li>
</ul>

<h3>Title: Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention</h3>
<ul>
<li><strong>Authors: </strong>Shezheng Song, Shasha Li, Jie Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08151">https://arxiv.org/abs/2601.08151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08151">https://arxiv.org/pdf/2601.08151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08151]] Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention(https://arxiv.org/abs/2601.08151)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have achieved remarkable progress in vision-language understanding, yet how they internally integrate visual and textual information remains poorly understood. To bridge this gap, we perform a systematic layer-wise masking analysis across multiple architectures, revealing how visual-text fusion evolves within MLLMs. The results show that fusion emerges at several specific layers rather than being uniformly distributed across the network, and certain models exhibit a late-stage "review" phenomenon where visual signals are reactivated before output generation. Besides, we further analyze layer-wise attention evolution and observe persistent high-attention noise on irrelevant regions, along with gradually increasing attention on text-aligned areas. Guided by these insights, we introduce a training-free contrastive attention framework that models the transformation between early fusion and final layers to highlight meaningful attention shifts. Extensive experiments across various MLLMs and benchmarks validate our analysis and demonstrate that the proposed approach improves multimodal reasoning performance. Code will be released.</li>
<li><strong>摘要：</strong>多模态大语言模型（MLLM）在视觉语言理解方面取得了显着进展，但它们如何在内部整合视觉和文本信息仍然知之甚少。为了弥补这一差距，我们跨多个架构进行了系统性分层屏蔽分析，揭示了视觉文本融合在 MLLM 中的演变方式。结果表明，融合出现在几个特定的​​层，而不是均匀分布在整个网络中，并且某些模型表现出后期“审查”现象，即视觉信号在输出生成之前重新激活。此外，我们进一步分析了分层注意力演化，并观察到不相关区域持续存在高注意力噪声，以及对文本对齐区域的注意力逐渐增加。在这些见解的指导下，我们引入了一个免训练的对比注意力框架，该框架对早期融合和最终层之间的转换进行建模，以突出有意义的注意力转移。跨各种 MLLM 和基准的大量实验验证了我们的分析，并证明所提出的方法提高了多模态推理性能。代码将被发布。</li>
</ul>

<h3>Title: Instruction-Driven 3D Facial Expression Generation and Transition</h3>
<ul>
<li><strong>Authors: </strong>Anh H. Vo, Tae-Seok Kim, Hulin Jin, Soo-Mi Choi, Yong-Guk Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08179">https://arxiv.org/abs/2601.08179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08179">https://arxiv.org/pdf/2601.08179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08179]] Instruction-Driven 3D Facial Expression Generation and Transition(https://arxiv.org/abs/2601.08179)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>A 3D avatar typically has one of six cardinal facial expressions. To simulate realistic emotional variation, we should be able to render a facial transition between two arbitrary expressions. This study presents a new framework for instruction-driven facial expression generation that produces a 3D face and, starting from an image of the face, transforms the facial expression from one designated facial expression to another. The Instruction-driven Facial Expression Decomposer (IFED) module is introduced to facilitate multimodal data learning and capture the correlation between textual descriptions and facial expression features. Subsequently, we propose the Instruction to Facial Expression Transition (I2FET) method, which leverages IFED and a vertex reconstruction loss function to refine the semantic comprehension of latent vectors, thus generating a facial expression sequence according to the given instruction. Lastly, we present the Facial Expression Transition model to generate smooth transitions between facial expressions. Extensive evaluation suggests that the proposed model outperforms state-of-the-art methods on the CK+ and CelebV-HQ datasets. The results show that our framework can generate facial expression trajectories according to text instruction. Considering that text prompts allow us to make diverse descriptions of human emotional states, the repertoire of facial expressions and the transitions between them can be expanded greatly. We expect our framework to find various practical applications More information about our project can be found at this https URL</li>
<li><strong>摘要：</strong>3D 化身通常具有六种基本面部表情之一。为了模拟真实的情绪变化，我们应该能够渲染两个任意表情之间的面部过渡。这项研究提出了一种用于指令驱动的面部表情生成的新框架，该框架可生成 3D 面部，并从面部图像开始，将面部表情从一种指定的面部表情转换为另一种指定的面部表情。引入指令驱动的面部表情分解器（IFED）模块来促进多模态数据学习并捕获文本描述和面部表情特征之间的相关性。随后，我们提出了面部表情转换指令（I2FET）方法，该方法利用 IFED 和顶点重建损失函数来细化潜在向量的语义理解，从而根据给定的指令生成面部表情序列。最后，我们提出面部表情转换模型来生成面部表情之间的平滑转换。广泛的评估表明，所提出的模型在 CK+ 和 CelebV-HQ 数据集上优于最先进的方法。结果表明，我们的框架可以根据文本指令生成面部表情轨迹。考虑到文本提示允许我们对人类情绪状态进行多样化的描述，面部表情的库以及它们之间的转换可以大大扩展。我们希望我们的框架能够找到各种实际应用 有关我们项目的更多信息可以在此 https URL 找到</li>
</ul>

<h3>Title: Route, Retrieve, Reflect, Repair: Self-Improving Agentic Framework for Visual Detection and Linguistic Reasoning in Medical Imaging</h3>
<ul>
<li><strong>Authors: </strong>Md. Faiyaz Abdullah Sayeedi, Rashedur Rahman, Siam Tahsin Bhuiyan, Sefatul Wasi, Ashraful Islam, Saadia Binte Alam, AKM Mahbubur Rahman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08192">https://arxiv.org/abs/2601.08192</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08192">https://arxiv.org/pdf/2601.08192</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08192]] Route, Retrieve, Reflect, Repair: Self-Improving Agentic Framework for Visual Detection and Linguistic Reasoning in Medical Imaging(https://arxiv.org/abs/2601.08192)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Medical image analysis increasingly relies on large vision-language models (VLMs), yet most systems remain single-pass black boxes that offer limited control over reasoning, safety, and spatial grounding. We propose R^4, an agentic framework that decomposes medical imaging workflows into four coordinated agents: a Router that configures task- and specialization-aware prompts from the image, patient history, and metadata; a Retriever that uses exemplar memory and pass@k sampling to jointly generate free-text reports and bounding boxes; a Reflector that critiques each draft-box pair for key clinical error modes (negation, laterality, unsupported claims, contradictions, missing findings, and localization errors); and a Repairer that iteratively revises both narrative and spatial outputs under targeted constraints while curating high-quality exemplars for future cases. Instantiated on chest X-ray analysis with multiple modern VLM backbones and evaluated on report generation and weakly supervised detection, R^4 consistently boosts LLM-as-a-Judge scores by roughly +1.7-+2.5 points and mAP50 by +2.5-+3.5 absolute points over strong single-VLM baselines, without any gradient-based fine-tuning. These results show that agentic routing, reflection, and repair can turn strong but brittle VLMs into more reliable and better grounded tools for clinical image interpretation. Our code can be found at: this https URL</li>
<li><strong>摘要：</strong>医学图像分析越来越依赖于大型视觉语言模型 (VLM)，但大多数系统仍然是单通道黑匣子，对推理、安全性和空间基础的控制有限。我们提出了 R^4，一个代理框架，它将医学成像工作流程分解为四个协调的代理：一个路由器，用于根据图像、患者历史和元数据配置任务和专业感知提示；使用示例内存和 pass@k 采样来联合生成自由文本报告和边界框的检索器；一个反射器，用于批评每个草稿箱对的关键临床错误模式（否定、偏侧性、无支持的主张、矛盾、遗漏的发现和定位错误）；修复器在目标约束下迭代修改叙事和空间输出，同时为未来案例策划高质量的范例。使用多个现代 VLM 主干进行胸部 X 射线分析实例化，并在报告生成和弱监督检测上进行评估，R^4 始终将 LLM 作为法官的分数提高了大约 +1.7-+2.5 分，将 mAP50 提高了 +2.5-+3.5 绝对分，比强大的单 VLM 基线高，无需任何基于梯度的微调。这些结果表明，代理路由、反射和修复可以将强大但脆弱的 VLM 转变为更可靠、更有基础的临床图像解释工具。我们的代码可以在以下位置找到：此 https URL</li>
</ul>

<h3>Title: GADPN: Graph Adaptive Denoising and Perturbation Networks via Singular Value Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Hao Deng, Bo Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08230">https://arxiv.org/abs/2601.08230</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08230">https://arxiv.org/pdf/2601.08230</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08230]] GADPN: Graph Adaptive Denoising and Perturbation Networks via Singular Value Decomposition(https://arxiv.org/abs/2601.08230)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While Graph Neural Networks (GNNs) excel on graph-structured data, their performance is fundamentally limited by the quality of the observed graph, which often contains noise, missing links, or structural properties misaligned with GNNs' underlying assumptions. To address this, graph structure learning aims to infer a more optimal topology. Existing methods, however, often incur high computational costs due to complex generative models and iterative joint optimization, limiting their practical utility. In this paper, we propose GADPN, a simple yet effective graph structure learning framework that adaptively refines graph topology via low-rank denoising and generalized structural perturbation. Our approach makes two key contributions: (1) we introduce Bayesian optimization to adaptively determine the optimal denoising strength, tailoring the process to each graph's homophily level; and (2) we extend the structural perturbation method to arbitrary graphs via Singular Value Decomposition (SVD), overcoming its original limitation to symmetric structures. Extensive experiments on benchmark datasets demonstrate that GADPN achieves state-of-the-art performance while significantly improving efficiency. It shows particularly strong gains on challenging disassortative graphs, validating its ability to robustly learn enhanced graph structures across diverse network types.</li>
<li><strong>摘要：</strong>虽然图神经网络 (GNN) 在图结构数据方面表现出色，但其性能从根本上受到观察图质量的限制，观察到的图通常包含噪声、缺失链接或与 GNN 的基本假设不一致的结构属性。为了解决这个问题，图结构学习旨在推断出更优化的拓扑。然而，由于复杂的生成模型和迭代联合优化，现有方法往往会产生较高的计算成本，限制了其实际应用。在本文中，我们提出了 GADPN，一种简单而有效的图结构学习框架，它通过低秩去噪和广义结构扰动自适应地细化图拓扑。我们的方法有两个关键贡献：（1）我们引入贝叶斯优化来自适应地确定最佳去噪强度，根据每个图的同质性水平定制过程； （2）我们通过奇异值分解（SVD）将结构扰动方法扩展到任意图，克服了其最初对对称结构的限制。对基准数据集的大量实验表明，GADPN 实现了最先进的性能，同时显着提高了效率。它在挑战异类图方面表现出特别强劲的收益，验证了其在不同网络类型中稳健地学习增强图结构的能力。</li>
</ul>

<h3>Title: A Usable GAN-Based Tool for Synthetic ECG Generation in Cardiac Amyloidosis Research</h3>
<ul>
<li><strong>Authors: </strong>Francesco Speziale, Ugo Lomoio, Fabiola Boccuto, Pierangelo Veltri, Pietro Hiram Guzzi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08260">https://arxiv.org/abs/2601.08260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08260">https://arxiv.org/pdf/2601.08260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08260]] A Usable GAN-Based Tool for Synthetic ECG Generation in Cardiac Amyloidosis Research(https://arxiv.org/abs/2601.08260)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Cardiac amyloidosis (CA) is a rare and underdiagnosed infiltrative cardiomyopathy, and available datasets for machine-learning models are typically small, imbalanced and heterogeneous. This paper presents a Generative Adversarial Network (GAN) and a graphical command-line interface for generating realistic synthetic electrocardiogram (ECG) beats to support early diagnosis and patient stratification in CA. The tool is designed for usability, allowing clinical researchers to train class-specific generators once and then interactively produce large volumes of labelled synthetic beats that preserve the distribution of minority classes.</li>
<li><strong>摘要：</strong>心脏淀粉样变性 (CA) 是一种罕见且诊断不足的浸润性心肌病，机器学习模型的可用数据集通常较小、不平衡且异质。本文提出了生成对抗网络 (GAN) 和图形命令行界面，用于生成真实的合成心电图 (ECG) 节拍，以支持 CA 的早期诊断和患者分层。该工具专为可用性而设计，允许临床研究人员训练特定类别的生成器一次，然后交互式地生成大量标记的合成节拍，以保留少数类别的分布。</li>
</ul>

<h3>Title: HIPPO: Accelerating Video Large Language Models Inference via Holistic-aware Parallel Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Qitan Lv, Tianyu Liu, Wen Wu, Xuenan Xu, Bowen Zhou, Feng Wu, Chao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08273">https://arxiv.org/abs/2601.08273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08273">https://arxiv.org/pdf/2601.08273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08273]] HIPPO: Accelerating Video Large Language Models Inference via Holistic-aware Parallel Speculative Decoding(https://arxiv.org/abs/2601.08273)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Speculative decoding (SD) has emerged as a promising approach to accelerate LLM inference without sacrificing output quality. Existing SD methods tailored for video-LLMs primarily focus on pruning redundant visual tokens to mitigate the computational burden of massive visual inputs. However, existing methods do not achieve inference acceleration comparable to text-only LLMs. We observe from extensive experiments that this phenomenon mainly stems from two limitations: (i) their pruning strategies inadequately preserve visual semantic tokens, degrading draft quality and acceptance rates; (ii) even with aggressive pruning (e.g., 90% visual tokens removed), the draft model's remaining inference cost limits overall speedup. To address these limitations, we propose HIPPO, a general holistic-aware parallel speculative decoding framework. Specifically, HIPPO proposes (i) a semantic-aware token preservation method, which fuses global attention scores with local visual semantics to retain semantic information at high pruning ratios; (ii) a video parallel SD algorithm that decouples and overlaps draft generation and target verification phases. Experiments on four video-LLMs across six benchmarks demonstrate HIPPO's effectiveness, yielding up to 3.51x speedup compared to vanilla auto-regressive decoding.</li>
<li><strong>摘要：</strong>推测性解码 (SD) 已成为一种有前途的方法，可以在不牺牲输出质量的情况下加速 LLM 推理。现有的为视频法学硕士量身定制的 SD 方法主要侧重于修剪冗余视觉标记，以减轻大量视觉输入的计算负担。然而，现有方法无法实现与纯文本法学硕士相比的推理加速。我们从大量的实验中观察到，这种现象主要源于两个局限性：（i）他们的修剪策略不足以保留视觉语义标记，降低了草稿质量和接受率； (ii) 即使进行积极的修剪（例如，删除 90% 的视觉标记），草稿模型的剩余推理成本也会限制整体加速。为了解决这些限制，我们提出了 HIPPO，一种通用的整体感知并行推测解码框架。具体来说，HIPPO提出了（i）一种语义感知的标记保存方法，该方法将全局注意力分数与局部视觉语义融合，以高剪枝率保留语义信息； (ii) 视频并行 SD 算法，可解耦并重叠草稿生成和目标验证阶段。在六个基准测试中对四个视频 LLM 进行的实验证明了 HIPPO 的有效性，与普通自回归解码相比，速度提高了 3.51 倍。</li>
</ul>

<h3>Title: SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices</h3>
<ul>
<li><strong>Authors: </strong>Dongting Hu, Aarush Gupta, Magzhan Gabidolla, Arpit Sahni, Huseyin Coskun, Yanyu Li, Yerlan Idelbayev, Ahsan Mahmood, Aleksei Lebedev, Dishani Lahiri, Anujraaj Goyal, Ju Hu, Mingming Gong, Sergey Tulyakov, Anil Kag</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08303">https://arxiv.org/abs/2601.08303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08303">https://arxiv.org/pdf/2601.08303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08303]] SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices(https://arxiv.org/abs/2601.08303)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion transformers (DiTs) have set new standards in image generation, yet remain impractical for on-device deployment due to their high computational and memory costs. In this work, we present an efficient DiT framework tailored for mobile and edge devices that achieves transformer-level generation quality under strict resource constraints. Our design combines three key components. First, we propose a compact DiT architecture with an adaptive global-local sparse attention mechanism that balances global context modeling and local detail preservation. Second, we propose an elastic training framework that jointly optimizes sub-DiTs of varying capacities within a unified supernetwork, allowing a single model to dynamically adjust for efficient inference across different hardware. Finally, we develop Knowledge-Guided Distribution Matching Distillation, a step-distillation pipeline that integrates the DMD objective with knowledge transfer from few-step teacher models, producing high-fidelity and low-latency generation (e.g., 4-step) suitable for real-time on-device use. Together, these contributions enable scalable, efficient, and high-quality diffusion models for deployment on diverse hardware.</li>
<li><strong>摘要：</strong>扩散变压器 (DiT) 的最新进展为图像生成设定了新标准，但由于计算和内存成本较高，对于设备上部署来说仍然不切实际。在这项工作中，我们提出了一个专为移动和边缘设备量身定制的高效 DiT 框架，可在严格的资源限制下实现变压器级的发电质量。我们的设计结合了三个关键组件。首先，我们提出了一种紧凑的 DiT 架构，具有自适应全局局部稀疏注意力机制，可以平衡全局上下文建模和局部细节保留。其次，我们提出了一种弹性训练框架，可以在统一的超级网络内联合优化不同容量的子 DiT，从而允许单个模型动态调整以实现跨不同硬件的高效推理。最后，我们开发了知识引导的分布匹配蒸馏，这是一种逐步蒸馏管道，它将 DMD 目标与来自几步教师模型的知识转移相结合，产生适合在设备上实时使用的高保真和低延迟生成（例如，4 步）。这些贡献共同实现了可扩展、高效和高质量的扩散模型，可以部署在不同的硬件上。</li>
</ul>

<h3>Title: Enhancing Image Quality Assessment Ability of LMMs via Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Kang Fu, Huiyu Duan, Zicheng Zhang, Yucheng Zhu, Jun Zhao, Xiongkuo Min, Jia Wang, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08311">https://arxiv.org/abs/2601.08311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08311">https://arxiv.org/pdf/2601.08311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08311]] Enhancing Image Quality Assessment Ability of LMMs via Retrieval-Augmented Generation(https://arxiv.org/abs/2601.08311)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, quality assessment</a></li>
<li><strong>Abstract: </strong>Large Multimodal Models (LMMs) have recently shown remarkable promise in low-level visual perception tasks, particularly in Image Quality Assessment (IQA), demonstrating strong zero-shot capability. However, achieving state-of-the-art performance often requires computationally expensive fine-tuning methods, which aim to align the distribution of quality-related token in output with image quality levels. Inspired by recent training-free works for LMM, we introduce IQARAG, a novel, training-free framework that enhances LMMs' IQA ability. IQARAG leverages Retrieval-Augmented Generation (RAG) to retrieve some semantically similar but quality-variant reference images with corresponding Mean Opinion Scores (MOSs) for input image. These retrieved images and input image are integrated into a specific prompt. Retrieved images provide the LMM with a visual perception anchor for IQA task. IQARAG contains three key phases: Retrieval Feature Extraction, Image Retrieval, and Integration & Quality Score Generation. Extensive experiments across multiple diverse IQA datasets, including KADID, KonIQ, LIVE Challenge, and SPAQ, demonstrate that the proposed IQARAG effectively boosts the IQA performance of LMMs, offering a resource-efficient alternative to fine-tuning for quality assessment.</li>
<li><strong>摘要：</strong>大型多模态模型（LMM）最近在低级视觉感知任务中表现出了非凡的前景，特别是在图像质量评估（IQA）中，展示了强大的零样本能力。然而，实现最先进的性能通常需要计算成本高昂的微调方法，其目的是使输出中与质量相关的标记的分布与图像质量水平保持一致。受最近 LMM 免训练工作的启发，我们引入了 IQARAG，这是一种新颖的免训练框架，可以增强 LMM 的 IQA 能力。 IQARAG 利用检索增强生成 (RAG) 来检索一些语义相似但质量不同的参考图像以及输入图像的相应平均意见得分 (MOS)。这些检索到的图像和输入图像被集成到特定的提示中。检索到的图像为 LMM 提供了 IQA 任务的视觉感知锚。 IQARAG 包含三个关键阶段：检索特征提取、图像检索以及集成和质量分数生成。跨多个不同 IQA 数据集（包括 KADID、KonIQ、LIVE Challenge 和 SPAQ）的广泛实验表明，所提出的 IQARAG 有效提高了 LMM 的 IQA 性能，为质量评估微调提供了一种资源高效的替代方案。</li>
</ul>

<h3>Title: UM-Text: A Unified Multimodal Model for Image Understanding</h3>
<ul>
<li><strong>Authors: </strong>Lichen Ma, Xiaolong Fu, Gaojing Zhou, Zipeng Guo, Ting Zhu, Yichun Liu, Yu Shi, Jason Li, Junshi Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08321">https://arxiv.org/abs/2601.08321</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08321">https://arxiv.org/pdf/2601.08321</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08321]] UM-Text: A Unified Multimodal Model for Image Understanding(https://arxiv.org/abs/2601.08321)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of image generation, visual text editing using natural language instructions has received increasing attention. The main challenge of this task is to fully understand the instruction and reference image, and thus generate visual text that is style-consistent with the image. Previous methods often involve complex steps of specifying the text content and attributes, such as font size, color, and layout, without considering the stylistic consistency with the reference image. To address this, we propose UM-Text, a unified multimodal model for context understanding and visual text editing by natural language instructions. Specifically, we introduce a Visual Language Model (VLM) to process the instruction and reference image, so that the text content and layout can be elaborately designed according to the context information. To generate an accurate and harmonious visual text image, we further propose the UM-Encoder to combine the embeddings of various condition information, where the combination is automatically configured by VLM according to the input instruction. During training, we propose a regional consistency loss to offer more effective supervision for glyph generation on both latent and RGB space, and design a tailored three-stage training strategy to further enhance model performance. In addition, we contribute the UM-DATA-200K, a large-scale visual text image dataset on diverse scenes for model training. Extensive qualitative and quantitative results on multiple public benchmarks demonstrate that our method achieves state-of-the-art performance.</li>
<li><strong>摘要：</strong>随着图像生成的快速进步，使用自然语言指令的可视化文本编辑受到越来越多的关注。该任务的主要挑战是充分理解指令和参考图像，从而生成与图像风格一致的视觉文本。以前的方法通常涉及指定文本内容和属性（例如字体大小、颜色和布局）的复杂步骤，而不考虑与参考图像的风格一致性。为了解决这个问题，我们提出了 UM-Text，这是一种统一的多模态模型，用于通过自然语言指令进行上下文理解和可视化文本编辑。具体来说，我们引入了视觉语言模型（VLM）来处理指令和参考图像，以便可以根据上下文信息精心设计文本内容和布局。为了生成准确和谐的视觉文本图像，我们进一步提出UM编码器来组合各种条件信息的嵌入，其中组合由VLM根据输入指令自动配置。在训练过程中，我们提出了区域一致性损失，以便为潜在空间和 RGB 空间上的字形生成提供更有效的监督，并设计定制的三阶段训练策略以进一步提高模型性能。此外，我们还贡献了 UM-DATA-200K，这是一个针对不同场景的大规模视觉文本图像数据集，用于模型训练。多个公共基准的广泛定性和定量结果表明我们的方法实现了最先进的性能。</li>
</ul>

<h3>Title: IGAN: A New Inception-based Model for Stable and High-Fidelity Image Synthesis Using Generative Adversarial Networks</h3>
<ul>
<li><strong>Authors: </strong>Ahmed A. Hashim, Ali Al-Shuwaili, Asraa Saeed, Ali Al-Bayaty</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08332">https://arxiv.org/abs/2601.08332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08332">https://arxiv.org/pdf/2601.08332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08332]] IGAN: A New Inception-based Model for Stable and High-Fidelity Image Synthesis Using Generative Adversarial Networks(https://arxiv.org/abs/2601.08332)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative Adversarial Networks (GANs) face a significant challenge of striking an optimal balance between high-quality image generation and training stability. Recent techniques, such as DCGAN, BigGAN, and StyleGAN, improve visual fidelity; however, such techniques usually struggle with mode collapse and unstable gradients at high network depth. This paper proposes a novel GAN structural model that incorporates deeper inception-inspired convolution and dilated convolution. This novel model is termed the Inception Generative Adversarial Network (IGAN). The IGAN model generates high-quality synthetic images while maintaining training stability, by reducing mode collapse as well as preventing vanishing and exploding gradients. Our proposed IGAN model achieves the Frechet Inception Distance (FID) of 13.12 and 15.08 on the CUB-200 and ImageNet datasets, respectively, representing a 28-33% improvement in FID over the state-of-the-art GANs. Additionally, the IGAN model attains an Inception Score (IS) of 9.27 and 68.25, reflecting improved image diversity and generation quality. Finally, the two techniques of dropout and spectral normalization are utilized in both the generator and discriminator structures to further mitigate gradient explosion and overfitting. These findings confirm that the IGAN model potentially balances training stability with image generation quality, constituting a scalable and computationally efficient framework for high-fidelity image synthesis.</li>
<li><strong>摘要：</strong>生成对抗网络（GAN）面临着在高质量图像生成和训练稳定性之间取得最佳平衡的重大挑战。最近的技术，例如 DCGAN、BigGAN 和 StyleGAN，提高了视觉保真度；然而，此类技术通常会在高网络深度下遇到模式崩溃和不稳定梯度的问题。本文提出了一种新颖的 GAN 结构模型，该模型结合了更深层次的 inception-inspired 卷积和扩张卷积。这种新颖的模型被称为初始生成对抗网络（IGAN）。 IGAN 模型通过减少模式崩溃以及防止梯度消失和爆炸，生成高质量的合成图像，同时保持训练稳定性。我们提出的 IGAN 模型在 CUB-200 和 ImageNet 数据集上分别实现了 13.12 和 15.08 的 Frechet 起始距离 (FID)，与最先进的 GAN 相比，FID 提高了 28-33%。此外，IGAN 模型的初始得分 (IS) 分别为 9.27 和 68.25，反映出图像多样性和生成质量的提高。最后，在生成器和鉴别器结构中都采用了 dropout 和 Spectrum Normalization 两种技术，以进一步减轻梯度爆炸和过度拟合。这些发现证实了 IGAN 模型潜在地平衡了训练稳定性和图像生成质量，为高保真图像合成构建了一个可扩展且计算高效的框架。</li>
</ul>

<h3>Title: From Local Windows to Adaptive Candidates via Individualized Exploratory: Rethinking Attention for Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Chunyu Meng, Wei Long, Shuhang Gu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08341">https://arxiv.org/abs/2601.08341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08341">https://arxiv.org/pdf/2601.08341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08341]] From Local Windows to Adaptive Candidates via Individualized Exploratory: Rethinking Attention for Image Super-Resolution(https://arxiv.org/abs/2601.08341)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Single Image Super-Resolution (SISR) is a fundamental computer vision task that aims to reconstruct a high-resolution (HR) image from a low-resolution (LR) input. Transformer-based methods have achieved remarkable performance by modeling long-range dependencies in degraded images. However, their feature-intensive attention computation incurs high computational cost. To improve efficiency, most existing approaches partition images into fixed groups and restrict attention within each group. Such group-wise attention overlooks the inherent asymmetry in token similarities, thereby failing to enable flexible and token-adaptive attention computation. To address this limitation, we propose the Individualized Exploratory Transformer (IET), which introduces a novel Individualized Exploratory Attention (IEA) mechanism that allows each token to adaptively select its own content-aware and independent attention candidates. This token-adaptive and asymmetric design enables more precise information aggregation while maintaining computational efficiency. Extensive experiments on standard SR benchmarks demonstrate that IET achieves state-of-the-art performance under comparable computational complexity.</li>
<li><strong>摘要：</strong>单图像超分辨率（SISR）是一项基本的计算机视觉任务，旨在从低分辨率（LR）输入重建高分辨率（HR）图像。基于 Transformer 的方法通过对退化图像中的远程依赖性进行建模，取得了显着的性能。然而，它们的特征密集型注意力计算会带来很高的计算成本。为了提高效率，大多数现有方法将图像划分为固定组并限制每个组内的注意力。这种分组注意力忽略了令牌相似性中固有的不对称性，从而无法实现灵活且令牌自适应的注意力计算。为了解决这个限制，我们提出了个性化探索变压器（IET），它引入了一种新颖的个性化探索性注意力（IEA）机制，允许每个令牌自适应地选择自己的内容感知和独立注意力候选者。这种令牌自适应和非对称设计可以实现更精确的信息聚合，同时保持计算效率。对标准 SR 基准的大量实验表明，IET 在相当的计算复杂度下实现了最先进的性能。</li>
</ul>

<h3>Title: Semantic Misalignment in Vision-Language Models under Perceptual Degradation</h3>
<ul>
<li><strong>Authors: </strong>Guo Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08355">https://arxiv.org/abs/2601.08355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08355">https://arxiv.org/pdf/2601.08355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08355]] Semantic Misalignment in Vision-Language Models under Perceptual Degradation(https://arxiv.org/abs/2601.08355)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) are increasingly deployed in autonomous driving and embodied AI systems, where reliable perception is critical for safe semantic reasoning and decision-making. While recent VLMs demonstrate strong performance on multimodal benchmarks, their robustness to realistic perception degradation remains poorly understood. In this work, we systematically study semantic misalignment in VLMs under controlled degradation of upstream visual perception, using semantic segmentation on the Cityscapes dataset as a representative perception module. We introduce perception-realistic corruptions that induce only moderate drops in conventional segmentation metrics, yet observe severe failures in downstream VLM behavior, including hallucinated object mentions, omission of safety-critical entities, and inconsistent safety judgments. To quantify these effects, we propose a set of language-level misalignment metrics that capture hallucination, critical omission, and safety misinterpretation, and analyze their relationship with segmentation quality across multiple contrastive and generative VLMs. Our results reveal a clear disconnect between pixel-level robustness and multimodal semantic reliability, highlighting a critical limitation of current VLM-based systems and motivating the need for evaluation frameworks that explicitly account for perception uncertainty in safety-critical applications.</li>
<li><strong>摘要：</strong>视觉语言模型 (VLM) 越来越多地部署在自动驾驶和嵌入式人工智能系统中，其中可靠的感知对于安全的语义推理和决策至关重要。虽然最近的 VLM 在多模式基准上表现出强大的性能，但它们对现实感知退化的鲁棒性仍然知之甚少。在这项工作中，我们使用 Cityscapes 数据集上的语义分割作为代表性感知模块，系统地研究了在上游视觉感知受控退化的情况下 VLM 中的语义错位。我们引入了感知真实的损坏，仅导致传统分段指标适度下降，但观察到下游 VLM 行为的严重失败，包括幻觉对象提及、安全关键实体的遗漏以及不一致的安全判断。为了量化这些影响，我们提出了一组语言级错位指标，用于捕获幻觉、关键遗漏和安全误解，并分析它们与跨多个对比和生成 VLM 的分段质量的关系。我们的结果揭示了像素级鲁棒性和多模态语义可靠性之间的明显脱节，凸显了当前基于 VLM 的系统的关键局限性，并激发了对明确考虑安全关键应用中感知不确定性的评估框架的需求。</li>
</ul>

<h3>Title: Training-Free Distribution Adaptation for Diffusion Models via Maximum Mean Discrepancy Guidance</h3>
<ul>
<li><strong>Authors: </strong>Matina Mahdizadeh Sani, Nima Jamali, Mohammad Jalali, Farzan Farnia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08379">https://arxiv.org/abs/2601.08379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08379">https://arxiv.org/pdf/2601.08379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08379]] Training-Free Distribution Adaptation for Diffusion Models via Maximum Mean Discrepancy Guidance(https://arxiv.org/abs/2601.08379)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Pre-trained diffusion models have emerged as powerful generative priors for both unconditional and conditional sample generation, yet their outputs often deviate from the characteristics of user-specific target data. Such mismatches are especially problematic in domain adaptation tasks, where only a few reference examples are available and retraining the diffusion model is infeasible. Existing inference-time guidance methods can adjust sampling trajectories, but they typically optimize surrogate objectives such as classifier likelihoods rather than directly aligning with the target distribution. We propose MMD Guidance, a training-free mechanism that augments the reverse diffusion process with gradients of the Maximum Mean Discrepancy (MMD) between generated samples and a reference dataset. MMD provides reliable distributional estimates from limited data, exhibits low variance in practice, and is efficiently differentiable, which makes it particularly well-suited for the guidance task. Our framework naturally extends to prompt-aware adaptation in conditional generation models via product kernels. Also, it can be applied with computational efficiency in latent diffusion models (LDMs), since guidance is applied in the latent space of the LDM. Experiments on synthetic and real-world benchmarks demonstrate that MMD Guidance can achieve distributional alignment while preserving sample fidelity.</li>
<li><strong>摘要：</strong>预训练的扩散模型已成为无条件和条件样本生成的强大生成先验，但其输出通常偏离用户特定目标数据的特征。这种不匹配在领域适应任务中尤其成问题，因为只有少数参考示例可用，并且重新训练扩散模型是不可行的。现有的推理时间指导方法可以调整采样轨迹，但它们通常优化代理目标，例如分类器可能性，而不是直接与目标分布对齐。我们提出了 MMD Guidance，这是一种免训练机制，可通过生成样本和参考数据集之间的最大平均差异（MMD）梯度来增强反向扩散过程。 MMD 根据有限的数据提供可靠的分布估计，在实践中表现出低方差，并且可有效微分，这使得它特别适合指导任务。我们的框架自然地通过产品内核扩展到条件生成模型中的提示感知适应。此外，它还可以在潜在扩散模型 (LDM) 中以计算效率应用，因为引导应用于 LDM 的潜在空间。对合成基准和真实世界基准的实验表明，MMD Guidance 可以在保持样本保真度的同时实现分布对齐。</li>
</ul>

<h3>Title: CoMa: Contextual Massing Generation with Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Evgenii Maslov, Valentin Khrulkov, Anastasia Volkova, Anton Gusarov, Andrey Kuznetsov, Ivan Oseledets</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08464">https://arxiv.org/abs/2601.08464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08464">https://arxiv.org/pdf/2601.08464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08464]] CoMa: Contextual Massing Generation with Vision-Language Models(https://arxiv.org/abs/2601.08464)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The conceptual design phase in architecture and urban planning, particularly building massing, is complex and heavily reliant on designer intuition and manual effort. To address this, we propose an automated framework for generating building massing based on functional requirements and site context. A primary obstacle to such data-driven methods has been the lack of suitable datasets. Consequently, we introduce the CoMa-20K dataset, a comprehensive collection that includes detailed massing geometries, associated economical and programmatic data, and visual representations of the development site within its existing urban context. We benchmark this dataset by formulating massing generation as a conditional task for Vision-Language Models (VLMs), evaluating both fine-tuned and large zero-shot models. Our experiments reveal the inherent complexity of the task while demonstrating the potential of VLMs to produce context-sensitive massing options. The dataset and analysis establish a foundational benchmark and highlight significant opportunities for future research in data-driven architectural design.</li>
<li><strong>摘要：</strong>建筑和城市规划的概念设计阶段，特别是建筑体量，非常复杂，并且严重依赖设计师的直觉和手工努力。为了解决这个问题，我们提出了一个基于功能需求和场地环境生成建筑体量的自动化框架。这种数据驱动方法的主要障碍是缺乏合适的数据集。因此，我们引入了 CoMa-20K 数据集，这是一个全面的集合，其中包括详细的体量几何形状、相关的经济和程序数据以及开发场地在现有城市背景下的视觉表示。我们通过将大规模生成制定为视觉语言模型（VLM）的条件任务来对数据集进行基准测试，评估微调模型和大型零样本模型。我们的实验揭示了任务固有的复杂性，同时展示了 VLM 产生上下文敏感的聚集选项的潜力。数据集和分析建立了基础基准，并强调了数据驱动架构设计未来研究的重大机遇。</li>
</ul>

<h3>Title: Towards Safer Mobile Agents: Scalable Generation and Evaluation of Diverse Scenarios for VLMs</h3>
<ul>
<li><strong>Authors: </strong>Takara Taniguchi, Kuniaki Saito, Atsushi Hashimoto</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08470">https://arxiv.org/abs/2601.08470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08470">https://arxiv.org/pdf/2601.08470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08470]] Towards Safer Mobile Agents: Scalable Generation and Evaluation of Diverse Scenarios for VLMs(https://arxiv.org/abs/2601.08470)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision Language Models (VLMs) are increasingly deployed in autonomous vehicles and mobile systems, making it crucial to evaluate their ability to support safer decision-making in complex environments. However, existing benchmarks inadequately cover diverse hazardous situations, especially anomalous scenarios with spatio-temporal dynamics. While image editing models are a promising means to synthesize such hazards, it remains challenging to generate well-formulated scenarios that include moving, intrusive, and distant objects frequently observed in the real world. To address this gap, we introduce \textbf{HazardForge}, a scalable pipeline that leverages image editing models to generate these scenarios with layout decision algorithms, and validation modules. Using HazardForge, we construct \textbf{MovSafeBench}, a multiple-choice question (MCQ) benchmark comprising 7,254 images and corresponding QA pairs across 13 object categories, covering both normal and anomalous objects. Experiments using MovSafeBench show that VLM performance degrades notably under conditions including anomalous objects, with the largest drop in scenarios requiring nuanced motion understanding.</li>
<li><strong>摘要：</strong>视觉语言模型 (VLM) 越来越多地部署在自动驾驶车辆和移动系统中，因此评估其在复杂环境中支持更安全决策的能力至关重要。然而，现有的基准不足以涵盖各种危险情况，特别是具有时空动态的异常情况。虽然图像编辑模型是合成此类危险的一种很有前景的方法，但生成精心设计的场景（包括现实世界中经常观察到的移动、侵入和远处物体）仍然具有挑战性。为了解决这一差距，我们引入了 \textbf{HazardForge}，这是一个可扩展的管道，它利用图像编辑模型通过布局决策算法和验证模块生成这些场景。使用 HazardForge，我们构建了 \textbf{MovSafeBench}，这是一个多项选择题 (MCQ) 基准，包含 13 个对象类别的 7,254 张图像和相应的 QA 对，涵盖正常和异常对象。使用 MovSafeBench 进行的实验表明，在包括异常物体在内的条件下，VLM 性能显着下降，其中在需要细致入微的运动理解的场景中下降幅度最大。</li>
</ul>

<h3>Title: Closed-Loop LLM Discovery of Non-Standard Channel Priors in Vision Models</h3>
<ul>
<li><strong>Authors: </strong>Tolgay Atinc Uzun, Dmitry Ignatov, Radu Timofte</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08517">https://arxiv.org/abs/2601.08517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08517">https://arxiv.org/pdf/2601.08517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08517]] Closed-Loop LLM Discovery of Non-Standard Channel Priors in Vision Models(https://arxiv.org/abs/2601.08517)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Channel configuration search the optimization of layer specifications such as layer widths in deep neural networks presents a complex combinatorial challenge constrained by tensor shape compatibility and computational budgets. We posit that Large Language Models (LLMs) offer a transformative approach to Neural Architecture Search (NAS), capable of reasoning about architectural code structure in ways that traditional heuristics cannot. In this paper, we investigate the application of an LLM-driven NAS framework to the problem of channel configuration. We formulate the search as a sequence of conditional code generation tasks, where an LLM refines architectural specifications based on performance telemetry. Crucially, we address the data scarcity problem by generating a vast corpus of valid, shape-consistent architectures via Abstract Syntax Tree (AST) mutations. While these mutated networks are not necessarily high-performing, they provide the critical volume of structural data required for the LLM to learn the latent relationship between channel configurations and model performance. This allows the LLM to internalize complex design patterns and apply them to optimize feature extraction strategies. Experimental results on CIFAR-100 validate the efficacy of this approach, demonstrating that the model yields statistically significant improvements in accuracy. Our analysis confirms that the LLM successfully acquires domain-specific architectural priors, distinguishing this method from random search and highlighting the immense potential of language-driven design in deep learning.</li>
<li><strong>摘要：</strong>通道配置搜索层规范（例如深度神经网络中的层宽度）的优化提出了受张量形状兼容性和计算预算限制的复杂组合挑战。我们认为大型语言模型（LLM）为神经架构搜索（NAS）提供了一种变革性的方法，能够以传统启发式方法无法推理的方式推理架构代码结构。在本文中，我们研究了 LLM 驱动的 NAS 框架在通道配置问题中的应用。我们将搜索制定为一系列条件代码生成任务，其中法学硕士根据性能遥测完善架构规范。至关重要的是，我们通过抽象语法树（AST）突变生成大量有效的、形状一致的架构，从而解决了数据稀缺问题。虽然这些突变网络不一定具有高性能，但它们提供了法学硕士了解通道配置和模型性能之间的潜在关系所需的关键结构数据量。这使得法学硕士能够内化复杂的设计模式，并应用它们来优化特征提取策略。 CIFAR-100 上的实验结果验证了该方法的有效性，表明该模型在准确度方面取得了统计上的显着提高。我们的分析证实，法学硕士成功获得了特定领域的架构先验，将这种方法与随机搜索区分开来，并强调了语言驱动设计在深度学习中的巨大潜力。</li>
</ul>

<h3>Title: VideoHEDGE: Entropy-Based Hallucination Detection for Video-VLMs via Semantic Clustering and Spatiotemporal Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Sushant Gautam, Cise Midoglu, Vajira Thambawita, Michael A. Riegler, Pål Halvorsen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08557">https://arxiv.org/abs/2601.08557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08557">https://arxiv.org/pdf/2601.08557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08557]] VideoHEDGE: Entropy-Based Hallucination Detection for Video-VLMs via Semantic Clustering and Spatiotemporal Perturbations(https://arxiv.org/abs/2601.08557)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Hallucinations in video-capable vision-language models (Video-VLMs) remain frequent and high-confidence, while existing uncertainty metrics often fail to align with correctness. We introduce VideoHEDGE, a modular framework for hallucination detection in video question answering that extends entropy-based reliability estimation from images to temporally structured inputs. Given a video-question pair, VideoHEDGE draws a baseline answer and multiple high-temperature generations from both clean clips and photometrically and spatiotemporally perturbed variants, then clusters the resulting textual outputs into semantic hypotheses using either Natural Language Inference (NLI)-based or embedding-based methods. Cluster-level probability masses yield three reliability scores: Semantic Entropy (SE), RadFlag, and Vision-Amplified Semantic Entropy (VASE). We evaluate VideoHEDGE on the SoccerChat benchmark using an LLM-as-a-judge to obtain binary hallucination labels. Across three 7B Video-VLMs (Qwen2-VL, Qwen2.5-VL, and a SoccerChat-finetuned model), VASE consistently achieves the highest ROC-AUC, especially at larger distortion budgets, while SE and RadFlag often operate near chance. We further show that embedding-based clustering matches NLI-based clustering in detection performance at substantially lower computational cost, and that domain fine-tuning reduces hallucination frequency but yields only modest improvements in calibration. The hedge-bench PyPI library enables reproducible and extensible benchmarking, with full code and experimental resources available at this https URL .</li>
<li><strong>摘要：</strong>视频视觉语言模型（Video-VLM）中的幻觉仍然频繁且可信度很高，而现有的不确定性指标往往无法与正确性保持一致。我们引入了 VideoHEDGE，这是一种用于视频问答中幻觉检测的模块化框架，它将基于熵的可靠性估计从图像扩展到时间结构化输入。给定视频-问题对，VideoHEDGE 从干净的剪辑以及光度和时空扰动的变体中得出基线答案和多个高温生成，然后使用基于自然语言推理 (NLI) 或基于嵌入的方法将所得文本输出聚类为语义假设。簇级概率质量产生三个可靠性分数：语义熵 (SE)、RadFlag 和视觉放大语义熵 (VASE)。我们使用法学硕士作为法官在 SoccerChat 基准上评估 VideoHEDGE，以获得二元幻觉标签。在三个 7B Video-VLM（Qwen2-VL、Qwen2.5-VL 和 SoccerChat 微调模型）中，VASE 始终实现最高的 ROC-AUC，尤其是在较大的失真预算下，而 SE 和 RadFlag 的运行往往接近偶然。我们进一步表明，基于嵌入的聚类在检测性能方面与基于 NLI 的聚类相匹配，而计算成本却大大降低，并且域微调降低了幻觉频率，但在校准方面仅产生了适度的改进。对冲基准 PyPI 库支持可重复和可扩展的基准测试，完整的代码和实验资源可在此 https URL 获取。</li>
</ul>

<h3>Title: SafeRedir: Prompt Embedding Redirection for Robust Unlearning in Image Generation Models</h3>
<ul>
<li><strong>Authors: </strong>Renyang Liu, Kangjie Chen, Han Qiu, Jie Zhang, Kwok-Yan Lam, Tianwei Zhang, See-Kiong Ng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08623">https://arxiv.org/abs/2601.08623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08623">https://arxiv.org/pdf/2601.08623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08623]] SafeRedir: Prompt Embedding Redirection for Robust Unlearning in Image Generation Models(https://arxiv.org/abs/2601.08623)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Image generation models (IGMs), while capable of producing impressive and creative content, often memorize a wide range of undesirable concepts from their training data, leading to the reproduction of unsafe content such as NSFW imagery and copyrighted artistic styles. Such behaviors pose persistent safety and compliance risks in real-world deployments and cannot be reliably mitigated by post-hoc filtering, owing to the limited robustness of such mechanisms and a lack of fine-grained semantic control. Recent unlearning methods seek to erase harmful concepts at the model level, which exhibit the limitations of requiring costly retraining, degrading the quality of benign generations, or failing to withstand prompt paraphrasing and adversarial attacks. To address these challenges, we introduce SafeRedir, a lightweight inference-time framework for robust unlearning via prompt embedding redirection. Without modifying the underlying IGMs, SafeRedir adaptively routes unsafe prompts toward safe semantic regions through token-level interventions in the embedding space. The framework comprises two core components: a latent-aware multi-modal safety classifier for identifying unsafe generation trajectories, and a token-level delta generator for precise semantic redirection, equipped with auxiliary predictors for token masking and adaptive scaling to localize and regulate the intervention. Empirical results across multiple representative unlearning tasks demonstrate that SafeRedir achieves effective unlearning capability, high semantic and perceptual preservation, robust image quality, and enhanced resistance to adversarial attacks. Furthermore, SafeRedir generalizes effectively across a variety of diffusion backbones and existing unlearned models, validating its plug-and-play compatibility and broad applicability. Code and data are available at this https URL.</li>
<li><strong>摘要：</strong>图像生成模型 (IGM) 虽然能够生成令人印象深刻且富有创意的内容，但通常会记住训练数据中的各种不良概念，从而导致复制 NSFW 图像和受版权保护的艺术风格等不安全内容。此类行为在现实部署中会带来持续的安全和合规风险，并且由于此类机制的稳健性有限且缺乏细粒度的语义控制，无法通过事后过滤可靠地缓解风险。最近的忘却方法试图消除模型级别的有害概念，这些概念表现出需要昂贵的再训练、降低良性生成的质量或无法承受即时释义和对抗性攻击的局限性。为了应对这些挑战，我们引入了 SafeRedir，这是一种轻量级推理时间框架，可通过提示嵌入重定向实现稳健的忘却。在不修改底层 IGM 的情况下，SafeRedir 通过嵌入空间中的令牌级干预，自适应地将不安全提示路由到安全语义区域。该框架由两个核心组件组成：用于识别不安全生成轨迹的潜在感知多模式安全分类器，以及用于精确语义重定向的令牌级增量生成器，配备用于令牌屏蔽和自适应缩放的辅助预测器以定位和调节干预。多个代表性遗忘任务的实证结果表明，SafeRedir 实现了有效的遗忘能力、高语义和感知保留、鲁棒的图像质量以及增强的对对抗性攻击的抵抗力。此外，SafeRedir 可以有效地推广各种扩散主干和现有的未学习模型，验证其即插即用兼容性和广泛的适用性。代码和数据可从此 https URL 获取。</li>
</ul>

<h3>Title: Salience-SGG: Enhancing Unbiased Scene Graph Generation with Iterative Salience Estimation</h3>
<ul>
<li><strong>Authors: </strong>Runfeng Qu, Ole Hall, Pia K Bideau, Julie Ouerfelli-Ethier, Martin Rolfs, Klaus Obermayer, Olaf Hellwich</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08728">https://arxiv.org/abs/2601.08728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08728">https://arxiv.org/pdf/2601.08728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08728]] Salience-SGG: Enhancing Unbiased Scene Graph Generation with Iterative Salience Estimation(https://arxiv.org/abs/2601.08728)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Scene Graph Generation (SGG) suffers from a long-tailed distribution, where a few predicate classes dominate while many others are underrepresented, leading to biased models that underperform on rare relations. Unbiased-SGG methods address this issue by implementing debiasing strategies, but often at the cost of spatial understanding, resulting in an over-reliance on semantic priors. We introduce Salience-SGG, a novel framework featuring an Iterative Salience Decoder (ISD) that emphasizes triplets with salient spatial structures. To support this, we propose semantic-agnostic salience labels guiding ISD. Evaluations on Visual Genome, Open Images V6, and GQA-200 show that Salience-SGG achieves state-of-the-art performance and improves existing Unbiased-SGG methods in their spatial understanding as demonstrated by the Pairwise Localization Average Precision</li>
<li><strong>摘要：</strong>场景图生成（SGG）受到长尾分布的影响，其中少数谓词类占主导地位，而许多其他谓词类的代表性不足，导致模型在稀有关系上表现不佳。 Unbiased-SGG 方法通过实施去偏差策略来解决这个问题，但通常以空间理解为代价，导致过度依赖语义先验。我们介绍了 Salience-SGG，这是一种新颖的框架，具有迭代显着性解码器（ISD），强调具有显着空间结构的三元组。为了支持这一点，我们提出了指导 ISD 的语义不可知的显着性标签。对 Visual Genome、Open Images V6 和 GQA-200 的评估表明，Salience-SGG 实现了最先进的性能，并在空间理解方面改进了现有的 Unbiased-SGG 方法，如成对定位平均精度所示</li>
</ul>

<h3>Title: S3-CLIP: Video Super Resolution for Person-ReID</h3>
<ul>
<li><strong>Authors: </strong>Tamas Endrei, Gyorgy Cserey</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08807">https://arxiv.org/abs/2601.08807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08807">https://arxiv.org/pdf/2601.08807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08807]] S3-CLIP: Video Super Resolution for Person-ReID(https://arxiv.org/abs/2601.08807)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Tracklet quality is often treated as an afterthought in most person re-identification (ReID) methods, with the majority of research presenting architectural modifications to foundational models. Such approaches neglect an important limitation, posing challenges when deploying ReID systems in real-world, difficult scenarios. In this paper, we introduce S3-CLIP, a video super-resolution-based CLIP-ReID framework developed for the VReID-XFD challenge at WACV 2026. The proposed method integrates recent advances in super-resolution networks with task-driven super-resolution pipelines, adapting them to the video-based person re-identification setting. To the best of our knowledge, this work represents the first systematic investigation of video super-resolution as a means of enhancing tracklet quality for person ReID, particularly under challenging cross-view conditions. Experimental results demonstrate performance competitive with the baseline, achieving 37.52% mAP in aerial-to-ground and 29.16% mAP in ground-to-aerial scenarios. In the ground-to-aerial setting, S3-CLIP achieves substantial gains in ranking accuracy, improving Rank-1, Rank-5, and Rank-10 performance by 11.24%, 13.48%, and 17.98%, respectively.</li>
<li><strong>摘要：</strong>在大多数行人重新识别 (ReID) 方法中，Tracklet 质量通常被视为事后考虑，大多数研究都提出了对基础模型的架构修改。这些方法忽略了一个重要的限制，在现实世界的困难场景中部署 ReID 系统时带来了挑战。在本文中，我们介绍了 S3-CLIP，这是一种基于视频超分辨率的 CLIP-ReID 框架，专为 WACV 2026 的 VReID-XFD 挑战而开发。所提出的方法将超分辨率网络的最新进展与任务驱动的超分辨率管道相结合，使其适应基于视频的人员重新识别设置。据我们所知，这项工作代表了对视频超分辨率作为增强行人重识别轨迹质量的一种手段的首次系统研究，特别是在具有挑战性的交叉视图条件下。实验结果表明，性能与基线相比具有竞争力，在空对地场景中实现了 37.52% mAP，在地对空场景中实现了 29.16% mAP。在地对空环境中，S3-CLIP 在排名精度方面取得了显着提升，将 Rank-1、Rank-5 和 Rank-10 性能分别提高了 11.24%、13.48% 和 17.98%。</li>
</ul>

<h3>Title: Motion Attribution for Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Xindi Wu, Despoina Paschalidou, Jun Gao, Antonio Torralba, Laura Leal-Taixé, Olga Russakovsky, Sanja Fidler, Jonathan Lorraine</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.MM, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08828">https://arxiv.org/abs/2601.08828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08828">https://arxiv.org/pdf/2601.08828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08828]] Motion Attribution for Video Generation(https://arxiv.org/abs/2601.08828)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.</li>
<li><strong>摘要：</strong>尽管视频生成模型取得了快速进展，但人们对数据在影响运动中的作用却知之甚少。我们提出了 Motive（视频生成的 MOTIon 归因），这是一种以运动为中心、基于梯度的数据归因框架，可扩展到现代、大型、高质量的视频数据集和模型。我们用它来研究哪些微调剪辑可以改善或降低时间动态。 Motive 通过运动加权损失掩模将时间动态与静态外观隔离，从而产生高效且可扩展的运动特定影响计算。在文本到视频模型上，Motive 可以识别强烈影响运动的剪辑，并指导数据管理，从而提高时间一致性和物理合理性。利用Motive选择的高影响力数据，我们的方法提高了VBench上的运动平滑度和动态程度，与预训练的基础模型相比，实现了74.1%的人类偏好获胜率。据我们所知，这是第一个在视频生成模型中归因于运动而不是视觉外观并使用它来管理微调数据的框架。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
