<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-07-02</h1>
<h3>Title: SWE-Bench-CL: Continual Learning for Coding Agents</h3>
<ul>
<li><strong>Authors: </strong>Thomas Joshi, Shayan Chowdhury, Fatih Uysal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00014">https://arxiv.org/abs/2507.00014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00014">https://arxiv.org/pdf/2507.00014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00014]] SWE-Bench-CL: Continual Learning for Coding Agents(https://arxiv.org/abs/2507.00014)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved impressive results on static code-generation benchmarks, but real-world software development unfolds as a continuous stream of evolving issues, fixes, and feature requests. We introduce SWE-Bench-CL, a novel continual learning benchmark built on the human-verified SWE-Bench Verified dataset introduced by OpenAI and Princeton-NLP in 2024. By organizing GitHub issues into chronologically ordered sequences that reflect natural repository evolution, SWE-Bench-CL enables direct evaluation of an agent's ability to accumulate experience, transfer knowledge across tasks, and resist catastrophic forgetting. We complement the dataset with (i) a preliminary analysis of inter-task structural similarity and contextual sensitivity, (ii) an interactive LangGraph-based evaluation framework augmented with a FAISS-backed semantic memory module, and (iii) a suite of specialized continual learning metrics -- including average accuracy, forgetting, forward/backward transfer, tool-use efficiency, and a generalized Composite Continual Learning Score and CL-F-beta score -- to capture the stability-plasticity trade-off. We outline a rigorous experimental protocol comparing memory-enabled and memory-disabled agents across diverse Python repositories. All code and data are publicly available at this https URL, providing the community with a reproducible platform for developing more adaptive and robust AI agents in software engineering.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在静态代码生成基准上取得了令人印象深刻的结果，但是实际的软件开发随着不断发展的问题，修复和功能请求的连续流而展开。 We introduce SWE-Bench-CL, a novel continual learning benchmark built on the human-verified SWE-Bench Verified dataset introduced by OpenAI and Princeton-NLP in 2024. By organizing GitHub issues into chronologically ordered sequences that reflect natural repository evolution, SWE-Bench-CL enables direct evaluation of an agent's ability to accumulate experience, transfer knowledge across tasks, and resist catastrophic forgetting. We complement the dataset with (i) a preliminary analysis of inter-task structural similarity and contextual sensitivity, (ii) an interactive LangGraph-based evaluation framework augmented with a FAISS-backed semantic memory module, and (iii) a suite of specialized continual learning metrics -- including average accuracy, forgetting, forward/backward transfer, tool-use efficiency, and a generalized Composite Continual Learning Score and CL-F-beta得分 - 捕获稳定性权衡。我们概述了一项严格的实验协议，该协议比较了不同的Python存储库中支持内存和内存的代理。所有代码和数据均在此HTTPS URL上公开可用，为社区提供了一个可重现的平台，用于开发软件工程中更适合自适应和健壮的AI代理。</li>
</ul>

<h3>Title: Variational Autoencoder for Generating Broader-Spectrum prior Proposals in Markov chain Monte Carlo Methods</h3>
<ul>
<li><strong>Authors: </strong>Marcio Borges, Felipe Pereira, Michel Tosin</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00020">https://arxiv.org/abs/2507.00020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00020">https://arxiv.org/pdf/2507.00020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00020]] Variational Autoencoder for Generating Broader-Spectrum prior Proposals in Markov chain Monte Carlo Methods(https://arxiv.org/abs/2507.00020)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This study uses a Variational Autoencoder method to enhance the efficiency and applicability of Markov Chain Monte Carlo (McMC) methods by generating broader-spectrum prior proposals. Traditional approaches, such as the Karhunen-Loève Expansion (KLE), require previous knowledge of the covariance function, often unavailable in practical applications. The VAE framework enables a data-driven approach to flexibly capture a broader range of correlation structures in Bayesian inverse problems, particularly subsurface flow modeling. The methodology is tested on a synthetic groundwater flow inversion problem, where pressure data is used to estimate permeability fields. Numerical experiments demonstrate that the VAE-based parameterization achieves comparable accuracy to KLE when the correlation length is known and outperforms KLE when the assumed correlation length deviates from the true value. Moreover, the VAE approach significantly reduces stochastic dimensionality, improving computational efficiency. The results suggest that leveraging deep generative models in McMC methods can lead to more adaptable and efficient Bayesian inference in high-dimensional problems.</li>
<li><strong>摘要：</strong>这项研究使用一种变异自动编码器方法来提高马尔可夫链蒙特卡洛（MCMC）方法的效率和适用性，通过产生更广泛的先验提案。传统方法，例如Karhunen-Loève扩展（KLE），需要以前了解协方差函数，这在实际应用中通常不可用。 VAE框架实现了一种数据驱动的方法，可以灵活地捕获贝叶斯反问题（尤其是地下流量建模）中更广泛的相关结构。该方法对合成地下水流动问题进行了测试，其中使用压力数据来估计渗透率场。数值实验表明，当已知相关长度已知时，基于VAE的参数化与KLE相当的精度可比性，并且当假定的相关长度与真实值偏离时，相关长度的表现优于KLE。此外，VAE方法大大降低了随机维度，从而提高了计算效率。结果表明，在MCMC方法中利用深层生成模型可以导致在高维问题中更适应性和有效的贝叶斯推断。</li>
</ul>

<h3>Title: ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiale Ding, Xiang Zheng, Cong Wang, Wei-Bin Lee, Xingjun Ma, Yu-Gang Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00026">https://arxiv.org/abs/2507.00026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00026">https://arxiv.org/pdf/2507.00026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00026]] ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models(https://arxiv.org/abs/2507.00026)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) are increasingly deployed as black-box components in real-world applications, evaluating their safety-especially under adversarial prompting-has become critical. Arguably, effective safety evaluations should be adaptive, evolving with LLM capabilities, and also cover a broad spectrum of harmful topics and real-world scenarios to fully expose potential vulnerabilities. Existing manual safety benchmarks, built on handcrafted adversarial prompts, are limited by their static nature and the intensive labor required to update them, making it difficult to keep pace with rapidly advancing LLMs. In contrast, automated adversarial prompt generation offers a promising path toward adaptive evaluation. However, current methods often suffer from insufficient adversarial topic coverage (topic-level diversity) and weak alignment with real-world contexts. These shortcomings stem from the exploration-exploitation dilemma in black-box optimization and a lack of real-world contextualization, resulting in adversarial prompts that are both topically narrow and scenario-repetitive. To address these issues, we propose Reality-Oriented Safety Evaluation (ROSE), a novel framework that uses multi-objective reinforcement learning to fine-tune an adversarial LLM for generating topically diverse and contextually rich adversarial prompts. Experiments show that ROSE outperforms existing methods in uncovering safety vulnerabilities in state-of-the-art LLMs, with notable improvements in integrated evaluation metrics. We hope ROSE represents a step toward more practical and reality-oriented safety evaluation of LLMs. WARNING: This paper contains examples of potentially harmful text.</li>
<li><strong>摘要：</strong>随着大型语言模型（LLM）越来越多地作为黑盒组件在现实世界应用程序中部署，因此评估其安全性 - 尤其是在对抗性促使促使促使之下至关重要的情况下变得至关重要。可以说，有效的安全评估应具有自适应性，随着LLM功能的发展，还涵盖了广泛的有害主题和现实世界情景，以充分暴露于潜在的脆弱性。现有的手动安全基准，建立在手工制作的对抗提示上，受其静态性质和更新它们所需的密集劳动的限制，因此很难与快速前进的LLM保持同步。相反，自动化的对抗性及时生成为自适应评估提供了有希望的途径。但是，当前的方法通常会遭受对抗性主题覆盖范围不足（主题级多样性）和与现实世界中环境的较弱的一致性。这些缺点源于黑盒优化中的探索 - 开发困境以及缺乏现实世界情境化，从而导致对抗性提示，这些提示既有局部狭窄又是情景反复的。为了解决这些问题，我们提出了以现实为导向的安全评估（ROSE），这是一个新颖的框架，使用多目标加固学习来微调对抗性LLM，以产生局部多样化和上下文丰富的对抗性提示。实验表明，Rose在发现最先进的LLM中的安全漏洞方面的现有方法优于现有方法，并在集成评估指标方面有了显着改善。我们希望Rose代表了朝着更实用和面向现实的安全评估LLMS迈出的一步。警告：本文包含潜在有害文本的示例。</li>
</ul>

<h3>Title: Evolutionary computing-based image segmentation method to detect defects and features in Additive Friction Stir Deposition Process</h3>
<ul>
<li><strong>Authors: </strong>Akshansh Mishra, Eyob Mesele Sefene, Shivraman Thapliyal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00046">https://arxiv.org/abs/2507.00046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00046">https://arxiv.org/pdf/2507.00046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00046]] Evolutionary computing-based image segmentation method to detect defects and features in Additive Friction Stir Deposition Process(https://arxiv.org/abs/2507.00046)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>This work proposes an evolutionary computing-based image segmentation approach for analyzing soundness in Additive Friction Stir Deposition (AFSD) processes. Particle Swarm Optimization (PSO) was employed to determine optimal segmentation thresholds for detecting defects and features in multilayer AFSD builds. The methodology integrates gradient magnitude analysis with distance transforms to create novel attention-weighted visualizations that highlight critical interface regions. Five AFSD samples processed under different conditions were analyzed using multiple visualization techniques i.e. self-attention maps, and multi-channel visualization. These complementary approaches reveal subtle material transition zones and potential defect regions which were not readily observable through conventional imaging. The PSO algorithm automatically identified optimal threshold values (ranging from 156-173) for each sample, enabling precise segmentation of material interfaces. The multi-channel visualization technique effectively combines boundary information (red channel), spatial relationships (green channel), and material density data (blue channel) into cohesive representations that quantify interface quality. The results demonstrate that attention-based analysis successfully identifies regions of incomplete bonding and inhomogeneities in AFSD joints, providing quantitative metrics for process optimization and quality assessment of additively manufactured components.</li>
<li><strong>摘要：</strong>这项工作提出了一种基于进化计算的图像分割方法，用于分析在添加摩擦搅拌沉积（AFSD）过程中的声音。采用粒子群优化（PSO）来确定多层AFSD构建中检测缺陷和特征的最佳分割阈值。该方法将梯度幅度分析与距离变换相结合，以创建新的注意力加权可视化，以突出关键界面区域。使用多种可视化技术，即自我注意图和多通道可视化分析了在不同条件下处理的五个AFSD样品。这些互补方法揭示了微妙的材料过渡区和潜在的缺陷区域，这些区域无法通过常规成像容易观察到。 PSO算法自动识别每个样品的最佳阈值值（范围从156-173），从而可以精确地分割材料接口。多通道可视化技术有效地结合了边界信息（红色通道），空间关系（绿色通道）和材料密度数据（蓝色通道），以量化界面质量。结果表明，基于注意力的分析成功地识别了AFSD关节中不完整的键合和不均匀性的区域，从而提供了定量指标，以实现添加性生产组件的过程优化和质量评估。</li>
</ul>

<h3>Title: Generating Heterogeneous Multi-dimensional Data : A Comparative Study</h3>
<ul>
<li><strong>Authors: </strong>Corbeau Michael, Claeys Emmanuelle, Serrurier Mathieu, Zaraté Pascale</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00090">https://arxiv.org/abs/2507.00090</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00090">https://arxiv.org/pdf/2507.00090</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00090]] Generating Heterogeneous Multi-dimensional Data : A Comparative Study(https://arxiv.org/abs/2507.00090)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Allocation of personnel and material resources is highly sensible in the case of firefighter interventions. This allocation relies on simulations to experiment with various scenarios. The main objective of this allocation is the global optimization of the firefighters response. Data generation is then mandatory to study various scenarios In this study, we propose to compare different data generation methods. Methods such as Random Sampling, Tabular Variational Autoencoders, standard Generative Adversarial Networks, Conditional Tabular Generative Adversarial Networks and Diffusion Probabilistic Models are examined to ascertain their efficacy in capturing the intricacies of firefighter interventions. Traditional evaluation metrics often fall short in capturing the nuanced requirements of synthetic datasets for real-world scenarios. To address this gap, an evaluation of synthetic data quality is conducted using a combination of domain-specific metrics tailored to the firefighting domain and standard measures such as the Wasserstein distance. Domain-specific metrics include response time distribution, spatial-temporal distribution of interventions, and accidents representation. These metrics are designed to assess data variability, the preservation of fine and complex correlations and anomalies such as event with a very low occurrence, the conformity with the initial statistical distribution and the operational relevance of the synthetic data. The distribution has the particularity of being highly unbalanced, none of the variables following a Gaussian distribution, adding complexity to the data generation process.</li>
<li><strong>摘要：</strong>在消防员干预的情况下，人员和物质资源的分配非常明智。该分配依赖于模拟来实验各种情况。该分配的主要目的是消防员响应的全球优化。然后，必须在研究这项研究中研究各种情况，我们建议比较不同的数据生成方法。检查了随机抽样，表格变分的自动编码器，标准生成对抗网络，条件表格生成的对抗网络和扩散概率模型等方法，以确定其在捕获消防员干预措施的复杂性方面的功效。传统的评估指标通常在捕获现实情况的合成数据集的细微差别要求方面通常不足。为了解决这一差距，使用针对消防域和标准措施（例如Wasserstein距离）量身定制的域特异性指标来评估合成数据质量。域特异性指标包括响应时间分布，干预措施的时空分布和事故表示。这些指标旨在评估数据的可变性，良好和复杂的相关性和异常情况，例如发生非常低的事件，符合初始统计分布以及合成数据的操作相关性。该分布具有高度不平衡的特殊性，没有一个变量遵循高斯分布，从而增加了数据生成过程的复杂性。</li>
</ul>

<h3>Title: FreeLong++: Training-Free Long Video Generation via Multi-band SpectralFusion</h3>
<ul>
<li><strong>Authors: </strong>Yu Lu, Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00162">https://arxiv.org/abs/2507.00162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00162">https://arxiv.org/pdf/2507.00162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00162]] FreeLong++: Training-Free Long Video Generation via Multi-band SpectralFusion(https://arxiv.org/abs/2507.00162)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in video generation models have enabled high-quality short video generation from text prompts. However, extending these models to longer videos remains a significant challenge, primarily due to degraded temporal consistency and visual fidelity. Our preliminary observations show that naively applying short-video generation models to longer sequences leads to noticeable quality degradation. Further analysis identifies a systematic trend where high-frequency components become increasingly distorted as video length grows, an issue we term high-frequency distortion. To address this, we propose FreeLong, a training-free framework designed to balance the frequency distribution of long video features during the denoising process. FreeLong achieves this by blending global low-frequency features, which capture holistic semantics across the full video, with local high-frequency features extracted from short temporal windows to preserve fine details. Building on this, FreeLong++ extends FreeLong dual-branch design into a multi-branch architecture with multiple attention branches, each operating at a distinct temporal scale. By arranging multiple window sizes from global to local, FreeLong++ enables multi-band frequency fusion from low to high frequencies, ensuring both semantic continuity and fine-grained motion dynamics across longer video sequences. Without any additional training, FreeLong++ can be plugged into existing video generation models (e.g. Wan2.1 and LTX-Video) to produce longer videos with substantially improved temporal consistency and visual fidelity. We demonstrate that our approach outperforms previous methods on longer video generation tasks (e.g. 4x and 8x of native length). It also supports coherent multi-prompt video generation with smooth scene transitions and enables controllable video generation using long depth or pose sequences.</li>
<li><strong>摘要：</strong>视频生成模型的最新进展已从文本提示中启用了高质量的短视频生成。但是，将这些模型扩展到更长的视频仍然是一个重大挑战，这主要是由于时间的一致性和视觉保真度降低了。我们的初步观察表明，天真地将短视频生成模型应用于更长的序列会导致明显的质量降解。进一步的分析确定了一种系统的趋势，随着视频长度的增长，高频组件越来越扭曲，这是我们称高频失真的问题。为了解决这个问题，我们提出了Freelong，这是一个无训练的框架，旨在平衡在DeNoising过程中长期视频功能的频率分布。 Freelong通过融合全球低频功能来实现这一目标，该功能在整个视频中捕获了整体语义，并从短时窗中提取了本地高频功能，以保留细节。在此基础上，Freelong ++将Freelong Dual-Branch设计扩展到具有多个注意力分支的多分支结构，每个建筑都以不同的时间尺度运行。通过排列从全局到本地的多个窗口尺寸，Freelong ++可以从低频到高频启用多波段频率融合，从而确保跨更长的视频序列的语义连续性和细粒度的运动动力学。如果没有任何其他培训，Freelong ++可以插入现有的视频生成模型（例如WAN2.1和LTX-VIDEO）中，以产生更长的视频，并具有大大改善的时间一致性和视觉保真度。我们证明，我们的方法在更长的视频生成任务（例如本机长度的4x和8x）上的表现优于以前的方法。它还可以通过平滑的场景过渡来支持连贯的多项目生成，并使用长度或姿势序列启用可控的视频生成。</li>
</ul>

<h3>Title: Text-to-Level Diffusion Models With Various Text Encoders for Super Mario Bros</h3>
<ul>
<li><strong>Authors: </strong>Jacob Schrum, Olivia Kilday, Emilio Salas, Bess Hagan, Reid Williams</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00184">https://arxiv.org/abs/2507.00184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00184">https://arxiv.org/pdf/2507.00184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00184]] Text-to-Level Diffusion Models With Various Text Encoders for Super Mario Bros(https://arxiv.org/abs/2507.00184)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent research shows how diffusion models can unconditionally generate tile-based game levels, but use of diffusion models for text-to-level generation is underexplored. There are practical considerations for creating a usable model: caption/level pairs are needed, as is a text embedding model, and a way of generating entire playable levels, rather than individual scenes. We present strategies to automatically assign descriptive captions to an existing level dataset, and train diffusion models using both pretrained text encoders and simple transformer models trained from scratch. Captions are automatically assigned to generated levels so that the degree of overlap between input and output captions can be compared. We also assess the diversity and playability of the resulting levels. Results are compared with an unconditional diffusion model and a generative adversarial network, as well as the text-to-level approaches Five-Dollar Model and MarioGPT. Notably, the best diffusion model uses a simple transformer model for text embedding, and takes less time to train than diffusion models employing more complex text encoders, indicating that reliance on larger language models is not necessary. We also present a GUI allowing designers to construct long levels from model-generated scenes.</li>
<li><strong>摘要：</strong>最近的研究表明，扩散模型如何无条件地产生基于瓷砖的游戏水平，但是将扩散模型用于文本到级别的生成并没有得到充实。创建可用模型有实际考虑：需要字幕/级别对，文本嵌入模型以及一种生成整个可玩级别而不是单个场景的方式。我们提出了将描述性标题自动分配到现有级别数据集的策略，并使用验证的文本编码器和经过从头开始训练的简单变压器模型来训练扩散模型。字幕自动分配给生成的级别，以便可以比较输入和输出字幕之间的重叠程度。我们还评估了最终水平的多样性和可玩性。将结果与无条件扩散模型和生成对抗网络进行比较，以及文本到级别的五美元模型和Mariogpt。值得注意的是，最佳扩散模型使用简单的变压器模型进行文本嵌入，而训练的时间比采用更复杂的文本编码器的扩散模型要少，这表明不需要依赖更大的语言模型。我们还提出了一个GUI，允许设计师从模型生成的场景中构建长水平。</li>
</ul>

<h3>Title: Examining Reject Relations in Stimulus Equivalence Simulations</h3>
<ul>
<li><strong>Authors: </strong>Alexis Carrillo, Asieh Abolpour Mofrad, Anis Yazidi, Moises Betancort</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00265">https://arxiv.org/abs/2507.00265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00265">https://arxiv.org/pdf/2507.00265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00265]] Examining Reject Relations in Stimulus Equivalence Simulations(https://arxiv.org/abs/2507.00265)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Simulations offer a valuable tool for exploring stimulus equivalence (SE), yet the potential of reject relations to disrupt the assessment of equivalence class formation is contentious. This study investigates the role of reject relations in the acquisition of stimulus equivalence using computational models. We examined feedforward neural networks (FFNs), bidirectional encoder representations from transformers (BERT), and generative pre-trained transformers (GPT) across 18 conditions in matching-to-sample (MTS) simulations. Conditions varied in training structure (linear series, one-to-many, and many-to-one), relation type (select-only, reject-only, and select-reject), and negative comparison selection (standard and biased). A probabilistic agent served as a benchmark, embodying purely associative learning. The primary goal was to determine whether artificial neural networks could demonstrate equivalence class formation or whether their performance reflected associative learning. Results showed that reject relations influenced agent performance. While some agents achieved high accuracy on equivalence tests, particularly with reject relations and biased negative comparisons, this performance was comparable to the probabilistic agent. These findings suggest that artificial neural networks, including transformer models, may rely on associative strategies rather than SE. This underscores the need for careful consideration of reject relations and more stringent criteria in computational models of equivalence.</li>
<li><strong>摘要：</strong>仿真为探索刺激等效性（SE）提供了有价值的工具，但是拒绝关系破坏等效阶级形成评估的潜力是有争议的。这项研究调查了拒绝关系在使用计算模型获得刺激等效性中的作用。我们在18条条件下，在匹配样本（MTS）模拟的18个条件下，我们检查了馈电神经网络（FFN），来自变压器（BERT）（BERT）（BERT）（BERT）（BERT）的双向编码器（GPT）的双向编码器。训练结构（线性序列，一对多和多对分），关系类型（仅选择，仅拒绝和选择拒绝）以及负面比较选择（标准和有偏见）的条件各不相同。概率剂用作基准，体现了纯粹的关联学习。主要目标是确定人工神经网络是否可以证明等效类别的形成，或者其表现是否反映了关联学习。结果表明，拒绝关系影响了代理的性能。尽管某些试剂在等效测试中达到了很高的精度，尤其是在拒绝关系和偏见的负面比较中，这种性能与概率剂相当。这些发现表明，包括变压器模型在内的人工神经网络可能依赖于关联策略而不是SE。这强调了在等价的计算模型中仔细考虑拒绝关系和更严格的标准的必要性。</li>
</ul>

<h3>Title: Open-ended Scientific Discovery via Bayesian Surprise</h3>
<ul>
<li><strong>Authors: </strong>Dhruv Agarwal, Bodhisattwa Prasad Majumder, Reece Adamson, Megha Chakravorty, Satvika Reddy Gavireddy, Aditya Parashar, Harshit Surana, Bhavana Dalvi Mishra, Andrew McCallum, Ashish Sabharwal, Peter Clark</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00310">https://arxiv.org/abs/2507.00310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00310">https://arxiv.org/pdf/2507.00310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00310]] Open-ended Scientific Discovery via Bayesian Surprise(https://arxiv.org/abs/2507.00310)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The promise of autonomous scientific discovery (ASD) hinges not only on answering questions, but also on knowing which questions to ask. Most recent works in ASD explore the use of large language models (LLMs) in goal-driven settings, relying on human-specified research questions to guide hypothesis generation. However, scientific discovery may be accelerated further by allowing the AI system to drive exploration by its own criteria. The few existing approaches in open-ended ASD select hypotheses based on diversity heuristics or subjective proxies for human interestingness, but the former struggles to meaningfully navigate the typically vast hypothesis space, and the latter suffers from imprecise definitions. This paper presents AutoDS -- a method for open-ended ASD that instead drives scientific exploration using Bayesian surprise. Here, we quantify the epistemic shift from the LLM's prior beliefs about a hypothesis to its posterior beliefs after gathering experimental results. To efficiently explore the space of nested hypotheses, our method employs a Monte Carlo tree search (MCTS) strategy with progressive widening using surprisal as the reward function. We evaluate AutoDS in the setting of data-driven discovery across 21 real-world datasets spanning domains such as biology, economics, finance, and behavioral science. Our results demonstrate that under a fixed budget, AutoDS substantially outperforms competitors by producing 5--29\% more discoveries deemed surprising by the LLM. Our human evaluation further finds that two-thirds of AutoDS discoveries are surprising to the domain experts, suggesting this is an important step forward towards building open-ended ASD systems.</li>
<li><strong>摘要：</strong>自主科学发现（ASD）的承诺不仅取决于回答问题，还取决于要问哪些问题。 ASD的最新作品探索了在目标驱动设置中使用大语言模型（LLM），依靠人类指定的研究问题来指导假设产生。但是，科学发现可以通过允许AI系统通过其自己的标准推动探索来进一步加速。基于多样性启发式方法或人类兴趣的主观代理的开放式ASD选择假设中现有的几种方法，但前者努力地驾驶典型的典型假设空间，后者遭受了不精确的定义。本文介绍了Autods-一种开放式ASD的方法，可以使用贝叶斯惊喜推动科学探索。在这里，我们量化了从LLM在收集实验结果后从LLM先前对假设的信念转变为其后验信仰的转变。为了有效地探索嵌套假设的空间，我们的方法采用蒙特卡洛树搜索（MCT）策略，并以惊人的形式作为奖励功能进行了逐步扩大。我们在跨越生物学，经济学，金融和行为科学等21个现实世界数据集的数据驱动发现中评估了自动座。我们的结果表明，在固定预算下，Autods通过产生5--29 \％的发现而被LLM感到惊讶的5--29 \％的发现大大优于竞争对手。我们的人类评估进一步发现，三分之二的自动发现发现对领域专家来说是令人惊讶的，这表明这是迈向建立开放式ASD系统的重要一步。</li>
</ul>

<h3>Title: $μ^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation</h3>
<ul>
<li><strong>Authors: </strong>Siyou Li, Pengyao Qin, Huanan Wu, Dong Nie, Arun J. Thirunavukarasu, Juntao Yu, Le Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00316">https://arxiv.org/abs/2507.00316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00316">https://arxiv.org/pdf/2507.00316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00316]] $μ^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation(https://arxiv.org/abs/2507.00316)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Automated radiology report generation (RRG) aims to produce detailed textual reports from clinical imaging, such as computed tomography (CT) scans, to improve the accuracy and efficiency of diagnosis and provision of management advice. RRG is complicated by two key challenges: (1) inherent complexity in extracting relevant information from imaging data under resource constraints, and (2) difficulty in objectively evaluating discrepancies between model-generated and expert-written reports. To address these challenges, we propose $\mu^2$LLM, a $\underline{\textbf{mu}}$ltiscale $\underline{\textbf{mu}}$ltimodal large language models for RRG tasks. The novel ${\mu}^2$Tokenizer, as an intermediate layer, integrates multi-modal features from the multiscale visual tokenizer and the text tokenizer, then enhances report generation quality through direct preference optimization (DPO), guided by GREEN-RedLlama. Experimental results on four large CT image-report medical datasetdemonstrate that our method outperforms existing approaches, highlighting the potential of our fine-tuned $\mu^2$LLMs on limited data for RRG tasks.</li>
<li><strong>摘要：</strong>自动放射学报告生成（RRG）旨在从临床成像（例如计算机断层扫描（CT）扫描）中产生详细的文本报告，以提高诊断的准确性和效率，并提供管理建议。 RRG的两个关键挑战使RRG复杂化：（1）在资源限制下从成像数据中提取相关信息的固有复杂性，以及（2）难以客观地评估模型生成和专家写入的报告之间的差异。为了应对这些挑战，我们提出$ \ mu^2 $ llm，$ \ usewissline {\ textbf {mu}} $ ltiscale $ \ usewissline {\ textbf {mu}} $ ltimodal大型语言模型。小说$ {\ mu}^2 $ tokenizer作为中间层，整合了多尺度视觉令牌和文本令牌中的多模式特征，然后通过直接偏好优化（DPO）提高报告生成质量（DPO），并在Green-Redllama指导下。在四个大型CT图像报告医学数据集示出的实验结果表明，我们的方法表现出现有方法，突出了我们的微型$ \ mu^2 $ llms在RRG任务中的有限数据上的潜力。</li>
</ul>

<h3>Title: Populate-A-Scene: Affordance-Aware Human Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Mengyi Shan, Zecheng He, Haoyu Ma, Felix Juefei-Xu, Peizhao Zhang, Tingbo Hou, Ching-Yao Chuang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00334">https://arxiv.org/abs/2507.00334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00334">https://arxiv.org/pdf/2507.00334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00334]] Populate-A-Scene: Affordance-Aware Human Video Generation(https://arxiv.org/abs/2507.00334)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Can a video generation model be repurposed as an interactive world simulator? We explore the affordance perception potential of text-to-video models by teaching them to predict human-environment interaction. Given a scene image and a prompt describing human actions, we fine-tune the model to insert a person into the scene, while ensuring coherent behavior, appearance, harmonization, and scene affordance. Unlike prior work, we infer human affordance for video generation (i.e., where to insert a person and how they should behave) from a single scene image, without explicit conditions like bounding boxes or body poses. An in-depth study of cross-attention heatmaps demonstrates that we can uncover the inherent affordance perception of a pre-trained video model without labeled affordance datasets.</li>
<li><strong>摘要：</strong>视频生成模型可以作为交互式世界模拟器重新使用吗？我们通过教导他们预测人类环境相互作用来探讨文本对视频模型的负担感潜力。鉴于场景图像和描述人类行为的提示，我们将模型插入现场，同时确保连贯的行为，外观，协调和场景负担能力。与先前的工作不同，我们推断出视频产生的人类负担能力（即，在哪里插入一个人以及他们应该如何行事）从单个场景图像中，而没有明确的条件，例如边界框或身体姿势。对跨意义热图的深入研究表明，我们可以发现预先训练的视频模型的固有的负担能力，而无需标记负担能力数据集。</li>
</ul>

<h3>Title: MedDiff-FT: Data-Efficient Diffusion Model Fine-tuning with Structural Guidance for Controllable Medical Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Jianhao Xie, Ziang Zhang, Zhenyu Weng, Yuesheng Zhu, Guibo Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00377">https://arxiv.org/abs/2507.00377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00377">https://arxiv.org/pdf/2507.00377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00377]] MedDiff-FT: Data-Efficient Diffusion Model Fine-tuning with Structural Guidance for Controllable Medical Image Synthesis(https://arxiv.org/abs/2507.00377)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, quality assessment</a></li>
<li><strong>Abstract: </strong>Recent advancements in deep learning for medical image segmentation are often limited by the scarcity of high-quality training this http URL diffusion models provide a potential solution by generating synthetic images, their effectiveness in medical imaging remains constrained due to their reliance on large-scale medical datasets and the need for higher image quality. To address these challenges, we present MedDiff-FT, a controllable medical image generation method that fine-tunes a diffusion foundation model to produce medical images with structural dependency and domain specificity in a data-efficient manner. During inference, a dynamic adaptive guiding mask enforces spatial constraints to ensure anatomically coherent synthesis, while a lightweight stochastic mask generator enhances diversity through hierarchical randomness injection. Additionally, an automated quality assessment protocol filters suboptimal outputs using feature-space metrics, followed by mask corrosion to refine fidelity. Evaluated on five medical segmentation datasets,MedDiff-FT's synthetic image-mask pairs improve SOTA method's segmentation performance by an average of 1% in Dice score. The framework effectively balances generation quality, diversity, and computational efficiency, offering a practical solution for medical data augmentation. The code is available at this https URL.</li>
<li><strong>摘要：</strong>对医学图像细分的深度学习进步通常受到高质量训练的稀缺性的限制。这种HTTP URL扩散模型通过产生合成图像提供了潜在的解决方案，由于它们对大型医疗数据集的依赖以及对更高图像质量的需求，它们在医疗成像中的有效性仍然受到限制。为了应对这些挑战，我们提出了Meddiff-FT，这是一种可控的医学图像生成方法，该方法微调了扩散基础模型，以产生具有结构性依赖性和域特异性的医学图像。在推断期间，动态自适应引导面膜会实施空间约束，以确保解剖上一致的合成，而轻量级的随机掩码发生器通过层次随机性注入增强了多样性。此外，一种自动质量评估协议使用功能空间指标过滤次优输出，然后进行掩模腐蚀以优化保真度。 Meddiff-FT的合成图像掩码对在五个医学分割数据集上进行了评估，将SOTA方法的分割性能提高了1％。该框架有效地平衡了发电质量，多样性和计算效率，为医疗数据增强提供了实用的解决方案。该代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Learning Dense Feature Matching via Lifting Single 2D Image to 3D Space</h3>
<ul>
<li><strong>Authors: </strong>Yingping Liang, Yutao Hu, Wenqi Shao, Ying Fu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00392">https://arxiv.org/abs/2507.00392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00392">https://arxiv.org/pdf/2507.00392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00392]] Learning Dense Feature Matching via Lifting Single 2D Image to 3D Space(https://arxiv.org/abs/2507.00392)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Feature matching plays a fundamental role in many computer vision tasks, yet existing methods heavily rely on scarce and clean multi-view image collections, which constrains their generalization to diverse and challenging scenarios. Moreover, conventional feature encoders are typically trained on single-view 2D images, limiting their capacity to capture 3D-aware correspondences. In this paper, we propose a novel two-stage framework that lifts 2D images to 3D space, named as \textbf{Lift to Match (L2M)}, taking full advantage of large-scale and diverse single-view images. To be specific, in the first stage, we learn a 3D-aware feature encoder using a combination of multi-view image synthesis and 3D feature Gaussian representation, which injects 3D geometry knowledge into the encoder. In the second stage, a novel-view rendering strategy, combined with large-scale synthetic data generation from single-view images, is employed to learn a feature decoder for robust feature matching, thus achieving generalization across diverse domains. Extensive experiments demonstrate that our method achieves superior generalization across zero-shot evaluation benchmarks, highlighting the effectiveness of the proposed framework for robust feature matching.</li>
<li><strong>摘要：</strong>功能匹配在许多计算机视觉任务中都起着基本作用，但是现有的方法在很大程度上依赖于稀缺和清洁的多视图图像收集，这将它们的概括限制在多样化和具有挑战性的情况下。此外，常规特征编码器通常在单视2D图像上进行训练，从而限制了它们捕获3D感知对应关系的能力。在本文中，我们提出了一个新颖的两阶段框架，该框架将2D图像提升到3D空间，称为\ textbf {lift tO匹配（l2m）}，充分利用了大规模和多样的单视图像。要具体而言，在第一阶段，我们使用多视图图像合成和3D特征高斯表示的组合学习了3D感知的特征编码器，该代表将3D几何知识注入编码器。在第二阶段，采用了一种新颖的视图渲染策略，再加上从单视图像中的大规模合成数据生成，用于学习用于鲁棒特征匹配的特征解码器，从而实现了跨不同域的概括。广泛的实验表明，我们的方法实现了零射门评估基准的卓越概括，从而突出了提出的框架在稳健特征匹配方面的有效性。</li>
</ul>

<h3>Title: Diffusion Disambiguation Models for Partial Label Learning</h3>
<ul>
<li><strong>Authors: </strong>Jinfu Fan, Xiaohui Zhong, Kangrui Ren, Jiangnan Li, Linqing Huang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00411">https://arxiv.org/abs/2507.00411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00411">https://arxiv.org/pdf/2507.00411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00411]] Diffusion Disambiguation Models for Partial Label Learning(https://arxiv.org/abs/2507.00411)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Learning from ambiguous labels is a long-standing problem in practical machine learning applications. The purpose of \emph{partial label learning} (PLL) is to identify the ground-truth label from a set of candidate labels associated with a given instance. Inspired by the remarkable performance of diffusion models in various generation tasks, this paper explores their potential to denoise ambiguous labels through the reverse denoising process. Therefore, this paper reformulates the label disambiguation problem from the perspective of generative models, where labels are generated by iteratively refining initial random guesses. This perspective enables the diffusion model to learn how label information is generated stochastically. By modeling the generation uncertainty, we can use the maximum likelihood estimate of the label for classification inference. However, such ambiguous labels lead to a mismatch between instance and label, which reduces the quality of generated data. To address this issue, this paper proposes a \emph{diffusion disambiguation model for PLL} (DDMP), which first uses the potential complementary information between instances and labels to construct pseudo-clean labels for initial diffusion training. Furthermore, a transition-aware matrix is introduced to estimate the potential ground-truth labels, which are dynamically updated during the diffusion generation. During training, the ground-truth label is progressively refined, improving the classifier. Experiments show the advantage of the DDMP and its suitability for PLL.</li>
<li><strong>摘要：</strong>从模棱两可的标签中学习是实用机器学习应用中的一个长期问题。 \ emph {部分标签学习}（PLL）的目的是从与给定实例关联的一组候选标签中识别地面真相标签。受传播模型在各种一代任务中的显着性能的启发，本文探讨了它们通过反向denoising过程来表达模棱两可的标签的潜力。因此，本文从生成模型的角度重新制定了标签的歧义问题，其中标签是通过迭代精炼初始随机猜测而生成的。这种观点使扩散模型能够学习如何随机生成标签信息。通过对产生不确定性进行建模，我们可以使用标签的最大似然估计来进行分类推断。但是，这种模棱两可的标签导致实例和标签之间的不匹配，从而降低了生成的数据的质量。为了解决这个问题，本文提出了PLL}（DDMP）的\ emph {扩散歧义模型，该模型首先使用实例和标签之间的潜在互补信息来构建伪清洁标签，以进行初始扩散训练。此外，引入了一个过渡感知的矩阵来估计潜在的地面真相标签，该标签在扩散生成过程中会动态更新。在培训期间，地面真相标签逐渐完善，改善了分类器。实验显示了DDMP的优势及其对PLL的适用性。</li>
</ul>

<h3>Title: Flexible Language Modeling in Continuous Space with Transformer-based Autoregressive Flows</h3>
<ul>
<li><strong>Authors: </strong>Ruixiang Zhang, Shuangfei Zhai, Jiatao Gu, Yizhe Zhang, Huangjie Zheng, Tianrong Chen, Miguel Angel Bautista, Josh Susskind, Navdeep Jaitly</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00425">https://arxiv.org/abs/2507.00425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00425">https://arxiv.org/pdf/2507.00425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00425]] Flexible Language Modeling in Continuous Space with Transformer-based Autoregressive Flows(https://arxiv.org/abs/2507.00425)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Autoregressive models have driven remarkable progress in language modeling. Their foundational reliance on discrete tokens, unidirectional context, and single-pass decoding, while central to their success, also inspires the exploration of a design space that could offer new axes of modeling flexibility. In this work, we explore an alternative paradigm, shifting language modeling from a discrete token space to a continuous latent space. We propose a novel framework TarFlowLM, that employs transformer-based autoregressive normalizing flows to model these continuous representations. This approach unlocks substantial flexibility, enabling the construction of models that can capture global bi-directional context through stacked, alternating-direction autoregressive transformations, support block-wise generation with flexible token patch sizes, and facilitate a hierarchical multi-pass generation process. We further propose new mixture-based coupling transformations designed to capture complex dependencies within the latent space shaped by discrete data, and demonstrate theoretical connections to conventional discrete autoregressive models. Extensive experiments on language modeling benchmarks demonstrate strong likelihood performance and highlight the flexible modeling capabilities inherent in our framework.</li>
<li><strong>摘要：</strong>自回归模型在语言建模方面取得了显着进步。他们对离散代币，单向环境和单频道解码的基本依赖，同时他们的成功核心也激发了对设计空间的探索，该设计空间可以提供新的建模灵活性轴。在这项工作中，我们探索了另一种范式，将语言建模从离散的令牌空间转移到连续的潜在空间。我们提出了一个新型的框架TARFLOWLM，该框架采用基于变压器的自回旋标准化流量来对这些连续表示进行建模。这种方法解锁了实质性的灵活性，从而实现了可以通过堆叠的，交替的方向自回归转换来捕获全球双向环境的模型的构建，并具有灵活的代币贴剂大小的支持块生成，并设置层次结构的多通值生成过程。我们进一步提出了新的基于混合物的耦合转换转换，旨在捕获由离散数据塑造的潜在空间中复杂的依赖性，并证明了与常规离散自动回归模型的理论连接。关于语言建模基准测试基准的广泛实验表明了强烈的表现，并突出了我们框架中固有的灵活建模功能。</li>
</ul>

<h3>Title: Iterative Distillation for Reward-Guided Fine-Tuning of Diffusion Models in Biomolecular Design</h3>
<ul>
<li><strong>Authors: </strong>Xingyu Su, Xiner Li, Masatoshi Uehara, Sunwoo Kim, Yulai Zhao, Gabriele Scalia, Ehsan Hajiramezanali, Tommaso Biancalani, Degui Zhi, Shuiwang Ji</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00445">https://arxiv.org/abs/2507.00445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00445">https://arxiv.org/pdf/2507.00445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00445]] Iterative Distillation for Reward-Guided Fine-Tuning of Diffusion Models in Biomolecular Design(https://arxiv.org/abs/2507.00445)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We address the problem of fine-tuning diffusion models for reward-guided generation in biomolecular design. While diffusion models have proven highly effective in modeling complex, high-dimensional data distributions, real-world applications often demand more than high-fidelity generation, requiring optimization with respect to potentially non-differentiable reward functions such as physics-based simulation or rewards based on scientific knowledge. Although RL methods have been explored to fine-tune diffusion models for such objectives, they often suffer from instability, low sample efficiency, and mode collapse due to their on-policy nature. In this work, we propose an iterative distillation-based fine-tuning framework that enables diffusion models to optimize for arbitrary reward functions. Our method casts the problem as policy distillation: it collects off-policy data during the roll-in phase, simulates reward-based soft-optimal policies during roll-out, and updates the model by minimizing the KL divergence between the simulated soft-optimal policy and the current model policy. Our off-policy formulation, combined with KL divergence minimization, enhances training stability and sample efficiency compared to existing RL-based methods. Empirical results demonstrate the effectiveness and superior reward optimization of our approach across diverse tasks in protein, small molecule, and regulatory DNA design.</li>
<li><strong>摘要：</strong>我们解决了生物分子设计中奖励引导产生的微调扩散模型的问题。尽管扩散模型已被证明在建模复杂，高维数据分布方面非常有效，但现实世界中的应用程序通常需要比高保真的生成更高，需要优化有关潜在的非差异性奖励功能，例如基于物理学的模拟或基于科学知识的奖励。尽管已将RL方法探讨了此类目标的微调扩散模型，但由于其上的性质，它们通常会遭受不稳定性，样本效率低和模式崩溃的困扰。在这项工作中，我们提出了一个基于迭代蒸馏的微调框架，该框架使扩散模型能够优化任意奖励功能。我们的方法将问题作为策略蒸馏而言：它在滚入阶段收集了非政策数据，模拟了基于奖励的软性策略，并通过最大程度地减少模拟的软性策略与当前模型策略之间的KL差异来更新模型。与现有的基于RL的方法相比，我们的非政策配方结合了KL差异最小化，增强了训练稳定性和样品效率。经验结果证明了我们在蛋白质，小分子和调节性DNA设计中的各种任务中的方法的有效性和优势优化。</li>
</ul>

<h3>Title: Latent Posterior-Mean Rectified Flow for Higher-Fidelity Perceptual Face Restoration</h3>
<ul>
<li><strong>Authors: </strong>Xin Luo, Menglin Zhang, Yunwei Lan, Tianyu Zhang, Rui Li, Chang Liu, Dong Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00447">https://arxiv.org/abs/2507.00447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00447">https://arxiv.org/pdf/2507.00447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00447]] Latent Posterior-Mean Rectified Flow for Higher-Fidelity Perceptual Face Restoration(https://arxiv.org/abs/2507.00447)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>The Perception-Distortion tradeoff (PD-tradeoff) theory suggests that face restoration algorithms must balance perceptual quality and fidelity. To achieve minimal distortion while maintaining perfect perceptual quality, Posterior-Mean Rectified Flow (PMRF) proposes a flow based approach where source distribution is minimum distortion estimations. Although PMRF is shown to be effective, its pixel-space modeling approach limits its ability to align with human perception, where human perception is defined as how humans distinguish between two image distributions. In this work, we propose Latent-PMRF, which reformulates PMRF in the latent space of a variational autoencoder (VAE), facilitating better alignment with human perception during optimization. By defining the source distribution on latent representations of minimum distortion estimation, we bound the minimum distortion by the VAE's reconstruction error. Moreover, we reveal the design of VAE is crucial, and our proposed VAE significantly outperforms existing VAEs in both reconstruction and restoration. Extensive experiments on blind face restoration demonstrate the superiority of Latent-PMRF, offering an improved PD-tradeoff compared to existing methods, along with remarkable convergence efficiency, achieving a 5.79X speedup over PMRF in terms of FID. Our code will be available as open-source.</li>
<li><strong>摘要：</strong>感知统一权衡（PD-Tradeoff）理论表明，面部恢复算法必须平衡感知质量和忠诚度。为了达到最小的失真，同时保持完美的感知质量，后均值整流流（PMRF）提出了一种基于流量的方法，其中源分布是最小失真估计。尽管PMRF被证明是有效的，但其像素空间建模方法限制了其与人类感知保持一致的能力，在该方法将人类的感知定义为人类如何区分两个图像分布。在这项工作中，我们提出了潜在PMRF，该潜在PMRF在各种自动编码器（VAE）的潜在空间中重新定义了PMRF，从而促进了优化期间与人类感知更好的一致性。通过在最小失真估计的潜在表示上定义源分布，我们通过VAE的重建误差绑定了最小失真。此外，我们揭示了VAE的设计至关重要，我们提出的VAE在重建和恢复中都显着优于现有的VAE。对盲面恢复的广泛实验表明了潜在PMRF的优势，与现有方法相比，PD-TradeOff的改进以及出色的收敛效率，在FID方面实现了5.79倍的速度。我们的代码将作为开源。</li>
</ul>

<h3>Title: ARIG: Autoregressive Interactive Head Generation for Real-time Conversations</h3>
<ul>
<li><strong>Authors: </strong>Ying Guo, Xi Liu, Cheng Zhen, Pengfei Yan, Xiaoming Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00472">https://arxiv.org/abs/2507.00472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00472">https://arxiv.org/pdf/2507.00472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00472]] ARIG: Autoregressive Interactive Head Generation for Real-time Conversations(https://arxiv.org/abs/2507.00472)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Face-to-face communication, as a common human activity, motivates the research on interactive head generation. A virtual agent can generate motion responses with both listening and speaking capabilities based on the audio or motion signals of the other user and itself. However, previous clip-wise generation paradigm or explicit listener/speaker generator-switching methods have limitations in future signal acquisition, contextual behavioral understanding, and switching smoothness, making it challenging to be real-time and realistic. In this paper, we propose an autoregressive (AR) based frame-wise framework called ARIG to realize the real-time generation with better interaction realism. To achieve real-time generation, we model motion prediction as a non-vector-quantized AR process. Unlike discrete codebook-index prediction, we represent motion distribution using diffusion procedure, achieving more accurate predictions in continuous space. To improve interaction realism, we emphasize interactive behavior understanding (IBU) and detailed conversational state understanding (CSU). In IBU, based on dual-track dual-modal signals, we summarize short-range behaviors through bidirectional-integrated learning and perform contextual understanding over long ranges. In CSU, we use voice activity signals and context features of IBU to understand the various states (interruption, feedback, pause, etc.) that exist in actual conversations. These serve as conditions for the final progressive motion prediction. Extensive experiments have verified the effectiveness of our model.</li>
<li><strong>摘要：</strong>作为一种共同的人类活动，面对面的交流激发了互动式脑电图的研究。虚拟代理可以根据其他用户和本身的音频或运动信号来生成运动响应，并通过听力和口语功能产生运动响应。但是，以前的剪辑生成范式或显式侦听器/扬声器生成器开关方法在未来的信号获取，上下文的行为理解和切换平稳性方面存在局限性，使实时和现实的挑战。在本文中，我们提出了一个基于自回归（AR）的框架框架，称为Arig，以实现更好的相互作用现实主义的实时生成。为了实现实时生成，我们将运动预测模拟为非矢量量化的AR过程。与离散的代码书索引预测不同，我们使用扩散过程表示运动分布，从而在连续空间中实现了更准确的预测。为了改善互动现实主义，我们强调互动行为理解（IBU）和详细的对话状态理解（CSU）。在IBU中，基于双轨双模式信号，我们通过双向综合学习总结了短期行为，并对长范围进行上下文理解。在CSU中，我们使用IBU的语音活动信号和上下文特征来了解实际对话中存在的各种状态（中断，反馈，停顿等）。这些是最终进行性运动预测的条件。广泛的实验验证了我们模型的有效性。</li>
</ul>

<h3>Title: Posterior Inference in Latent Space for Scalable Constrained Black-box Optimization</h3>
<ul>
<li><strong>Authors: </strong>Kiyoung Om, Kyuil Sim, Taeyoung Yun, Hyeongyu Kang, Jinkyoo Park</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00480">https://arxiv.org/abs/2507.00480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00480">https://arxiv.org/pdf/2507.00480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00480]] Posterior Inference in Latent Space for Scalable Constrained Black-box Optimization(https://arxiv.org/abs/2507.00480)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Optimizing high-dimensional black-box functions under black-box constraints is a pervasive task in a wide range of scientific and engineering problems. These problems are typically harder than unconstrained problems due to hard-to-find feasible regions. While Bayesian optimization (BO) methods have been developed to solve such problems, they often struggle with the curse of dimensionality. Recently, generative model-based approaches have emerged as a promising alternative for constrained optimization. However, they suffer from poor scalability and are vulnerable to mode collapse, particularly when the target distribution is highly multi-modal. In this paper, we propose a new framework to overcome these challenges. Our method iterates through two stages. First, we train flow-based models to capture the data distribution and surrogate models that predict both function values and constraint violations with uncertainty quantification. Second, we cast the candidate selection problem as a posterior inference problem to effectively search for promising candidates that have high objective values while not violating the constraints. During posterior inference, we find that the posterior distribution is highly multi-modal and has a large plateau due to constraints, especially when constraint feedback is given as binary indicators of feasibility. To mitigate this issue, we amortize the sampling from the posterior distribution in the latent space of flow-based models, which is much smoother than that in the data space. We empirically demonstrate that our method achieves superior performance on various synthetic and real-world constrained black-box optimization tasks. Our code is publicly available \href{this https URL}{here}.</li>
<li><strong>摘要：</strong>在黑盒约束下优化高维黑盒功能是在广泛的科学和工程问题中的一项普遍任务。由于难以发现的可行区域，这些问题通常比无约束的问题要困难。尽管已经开发了贝叶斯优化（BO）方法来解决此类问题，但它们通常在维度的诅咒中挣扎。最近，基于生成模型的方法已成为受约束优化的有希望的替代方法。但是，它们的可扩展性差，并且容易受到模式崩溃的影响，尤其是当目标分布高度多模式时。在本文中，我们提出了一个新框架来克服这些挑战。我们的方法通过两个阶段迭代。首先，我们训练基于流量的模型来捕获数据分布和替代模型，这些模型既可以预测功能值和约束违规，并具有不确定性量化。其次，我们将候选人选择问题作为后推理问题，以有效地寻找具有较高客观价值的且不违反约束的有前途的候选人。在后推断期间，我们发现后验分布是高度多模式的，并且由于限制而具有较大的高原，尤其是当约束反馈作为可行性的二进制指标时。为了减轻此问题，我们从基于流的模型的潜在空间中的后验分布中摊销了采样，这比数据空间中的样本更光滑。我们从经验上证明，我们的方法在各种合成和现实世界中受约束的黑盒优化任务上实现了卓越的性能。我们的代码是公开可用的\ href {this https url} {there}。</li>
</ul>

<h3>Title: Laplace-Mamba: Laplace Frequency Prior-Guided Mamba-CNN Fusion Network for Image Dehazing</h3>
<ul>
<li><strong>Authors: </strong>Yongzhen Wang, Liangliang Chen, Bingwen Hu, Heng Liu, Xiao-Ping Zhang, Mingqiang Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00501">https://arxiv.org/abs/2507.00501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00501">https://arxiv.org/pdf/2507.00501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00501]] Laplace-Mamba: Laplace Frequency Prior-Guided Mamba-CNN Fusion Network for Image Dehazing(https://arxiv.org/abs/2507.00501)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Recent progress in image restoration has underscored Spatial State Models (SSMs) as powerful tools for modeling long-range dependencies, owing to their appealing linear complexity and computational efficiency. However, SSM-based approaches exhibit limitations in reconstructing localized structures and tend to be less effective when handling high-dimensional data, frequently resulting in suboptimal recovery of fine image features. To tackle these challenges, we introduce Laplace-Mamba, a novel framework that integrates Laplace frequency prior with a hybrid Mamba-CNN architecture for efficient image dehazing. Leveraging the Laplace decomposition, the image is disentangled into low-frequency components capturing global texture and high-frequency components representing edges and fine details. This decomposition enables specialized processing via dual parallel pathways: the low-frequency branch employs SSMs for global context modeling, while the high-frequency branch utilizes CNNs to refine local structural details, effectively addressing diverse haze scenarios. Notably, the Laplace transformation facilitates information-preserving downsampling of low-frequency components in accordance with the Nyquist theory, thereby significantly improving computational efficiency. Extensive evaluations across multiple benchmarks demonstrate that our method outperforms state-of-the-art approaches in both restoration quality and efficiency. The source code and pretrained models are available at this https URL.</li>
<li><strong>摘要：</strong>图像恢复的最新进展强调了空间状态模型（SSM）作为对长期依赖性建模的强大工具，这是由于它们具有吸引力的线性复杂性和计算效率。但是，基于SSM的方法在重建局部结构时表现出局限性，并且在处理高维数据时往往较低，通常导致良好的图像特征的次优恢复。为了应对这些挑战，我们介绍了Laplace-Mamba，这是一个新颖的框架，将拉普拉斯频率与混合Mamba-CNN体系结构相结合，以进行有效的图像去悬式。利用拉普拉斯分解，图像被分解为捕获全球纹理和高频组件的低频组件，这些组件代表了边缘和细节。这种分解可以通过双重平行途径进行专门处理：低频分支采用SSM进行全球上下文建模，而高频分支则利用CNN来完善本地结构细节，从而有效地解决了各种雾霾场景。值得注意的是，拉普拉斯的转化有助于根据奈奎斯特理论对低频组件的信息降采样，从而显着提高了计算效率。跨多个基准测试的广泛评估表明，我们的方法在恢复质量和效率方面均优于最先进的方法。源代码和预估计的模型可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Foundation Models for Clinical Records at Health System Scale</h3>
<ul>
<li><strong>Authors: </strong>Haresh Rengaraj Rajamohan, Xiang Gao, Weicheng Zhu, Shih-Lun Huang, Long Chen, Kyunghyun Cho, Cem M. Deniz, Narges Razavian</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00574">https://arxiv.org/abs/2507.00574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00574">https://arxiv.org/pdf/2507.00574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00574]] Foundation Models for Clinical Records at Health System Scale(https://arxiv.org/abs/2507.00574)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large-scale pretraining has transformed modeling of language and other data types, but its potential remains underexplored in healthcare with structured electronic health records (EHRs). We present a novel generative pretraining strategy for sequential EHR data using next-visit event prediction. Our model learns to autoregressively generate various tokenized clinical events for the next visit based on patient history and inherently handles the joint prediction of heterogeneous data types. Additionally, we introduce regularization on predicting repeated events and highlight a key pitfall in EHR-based foundation model evaluations: repeated event tokens can inflate performance metrics when new onsets are not distinguished from subsequent occurrences. Our model is evaluated via zero-shot prediction for forecasting dementia and knee osteoarthritis incidence within 2 and 5 years, and the model performance rivals a fully fine-tuned masked pretrained Transformer baseline, demonstrating that our approach captures complex clinical dependencies without requiring costly task-specific fine-tuning.</li>
<li><strong>摘要：</strong>大规模预处理改变了语言和其他数据类型的建模，但其潜力在结构化电子健康记录（EHR）的医疗保健中仍未得到充实。我们提出了一种新型的生成预处理策略，该策略是使用下一访问事件预测的顺序EHR数据。我们的模型学会了根据患者病史自动进行下一次访问的各种令牌化临床事件，并固有地处理异质数据类型的联合预测。此外，我们介绍了预测重复事件的正则化，并突出了基于EHR的基础模型评估中的关键陷阱：重复事件令牌可能会在不区分新的ONETS与随后发生的情况下膨胀性能指标。我们的模型通过零拍预测进行评估，以预测痴呆症和膝关节骨关节炎在2年内发生的发病率，并且模型性能与完全微调的蒙面预处理的变压器基线相媲美，这表明我们的方法捕获了复杂的临床依赖性，而无需昂贵的任务特定于任务特异性调节。</li>
</ul>

<h3>Title: AI-Generated Video Detection via Perceptual Straightening</h3>
<ul>
<li><strong>Authors: </strong>Christian Internò, Robert Geirhos, Markus Olhofer, Sunny Liu, Barbara Hammer, David Klindt</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00583">https://arxiv.org/abs/2507.00583</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00583">https://arxiv.org/pdf/2507.00583</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00583]] AI-Generated Video Detection via Perceptual Straightening(https://arxiv.org/abs/2507.00583)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of generative AI enables highly realistic synthetic videos, posing significant challenges for content authentication and raising urgent concerns about misuse. Existing detection methods often struggle with generalization and capturing subtle temporal inconsistencies. We propose ReStraV(Representation Straightening Video), a novel approach to distinguish natural from AI-generated videos. Inspired by the "perceptual straightening" hypothesis -- which suggests real-world video trajectories become more straight in neural representation domain -- we analyze deviations from this expected geometric property. Using a pre-trained self-supervised vision transformer (DINOv2), we quantify the temporal curvature and stepwise distance in the model's representation domain. We aggregate statistics of these measures for each video and train a classifier. Our analysis shows that AI-generated videos exhibit significantly different curvature and distance patterns compared to real videos. A lightweight classifier achieves state-of-the-art detection performance (e.g., 97.17% accuracy and 98.63% AUROC on the VidProM benchmark), substantially outperforming existing image- and video-based methods. ReStraV is computationally efficient, it is offering a low-cost and effective detection solution. This work provides new insights into using neural representation geometry for AI-generated video detection.</li>
<li><strong>摘要：</strong>生成AI的快速发展使高度现实的合成视频对内容身份验证构成了重大挑战，并提出了对滥用的紧急关注。现有的检测方法通常在泛化并捕获微妙的时间不一致之中。我们提出了Drestav（表示矫直视频），这是一种新颖的方法，可以将自然与AI生成的视频区分开。受“感知拉直”假设的启发，这表明现实世界中的视频轨迹在神经代表域中变得更加直截了当 - 我们分析了与这种预期的几何特性的偏差。使用预先训练的自我监督视觉变压器（DINOV2），我们量化了模型表示域中的时间曲率和逐步距离。我们为每个视频汇总了这些措施的统计数据，并培训分类器。我们的分析表明，与真实视频相比，AI生成的视频表现出明显不同的曲率和距离模式。轻量级分类器可实现最先进的检测性能（例如，精度为97.17％，在Vidprom基准上获得了98.63％的AUROC），其表现基于现有的基于图像和视频的方法基本上都优于现有的方法。 Dristav在计算上是有效的，它提供了低成本和有效的检测解决方案。这项工作为使用神经表示几何形状进行AI生成的视频检测提供了新的见解。</li>
</ul>

<h3>Title: GANs Secretly Perform Approximate Bayesian Model Selection</h3>
<ul>
<li><strong>Authors: </strong>Maurizio Filippone, Marius P. Linhard</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00651">https://arxiv.org/abs/2507.00651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00651">https://arxiv.org/pdf/2507.00651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00651]] GANs Secretly Perform Approximate Bayesian Model Selection(https://arxiv.org/abs/2507.00651)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative Adversarial Networks (GANs) are popular and successful generative models. Despite their success, optimization is notoriously challenging and they require regularization against overfitting. In this work, we explain the success and limitations of GANs by interpreting them as probabilistic generative models. This interpretation enables us to view GANs as Bayesian neural networks with partial stochasticity, allowing us to establish conditions of universal approximation. We can then cast the adversarial-style optimization of several variants of GANs as the optimization of a proxy for the marginal likelihood. Taking advantage of the connection between marginal likelihood optimization and Occam's razor, we can define regularization and optimization strategies to smooth the loss landscape and search for solutions with minimum description length, which are associated with flat minima and good generalization. The results on a wide range of experiments indicate that these strategies lead to performance improvements and pave the way to a deeper understanding of regularization strategies for GANs.</li>
<li><strong>摘要：</strong>生成对抗网络（GAN）是流行且成功的生成模型。尽管取得了成功，但众所周知，优化是具有挑战性的，他们需要正规化，以防止过度拟合。在这项工作中，我们通过将其解释为概率生成模型来解释甘恩的成功和局限性。这种解释使我们能够将甘斯视为具有部分随机性的贝叶斯神经网络，从而使我们能够建立普遍近似的条件。然后，我们可以将几种甘恩变体变体的对抗性优化作为边缘可能性的优化。利用边际可能性优化与Occam的剃须刀之间的联系，我们可以定义正则化和优化策略，以平滑损失景观并寻找具有最小描述长度的解决方案，这些长度与平坦的最小值和良好的概括有关。在广泛的实验上的结果表明，这些策略可以改善绩效，并为对甘恩斯的正则化策略的更深入理解铺平道路。</li>
</ul>

<h3>Title: Cognitive Load-Aware Inference: A Neuro-Symbolic Framework for Optimizing the Token Economy of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yilun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00653">https://arxiv.org/abs/2507.00653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00653">https://arxiv.org/pdf/2507.00653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00653]] Cognitive Load-Aware Inference: A Neuro-Symbolic Framework for Optimizing the Token Economy of Large Language Models(https://arxiv.org/abs/2507.00653)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The escalating computational costs of Large Language Model (LLM) inference have become a critical barrier to their widespread and sustainable deployment. While existing optimization strategies are effective, they are predominantly based on statistical heuristics or architectural modifications, lacking a guiding cognitive theory to manage the inference process itself. This paper aims to bridge this gap by introducing a novel paradigm: the Cognitive Load-Aware Inference (CLAI) framework, which operationalizes principles from Cognitive Load Theory (CLT) and neuroscience for LLM inference. We formalize the concepts of Intrinsic Cognitive Load, Extraneous Cognitive Load, and Germane Cognitive Load into quantifiable LLM metrics ($ICL_{LLM}$, $ECL_{LLM}$, and $GCL_{LLM}$), thereby reframing the inference process as a cognitive economics optimization problem: based on the intrinsic complexity of a problem ($ICL_{LLM}$), minimize wasteful computation ($ECL_{LLM}$), and strategically allocate the token budget to productive reasoning ($GCL_{LLM}$). We propose two implementation paths: CLAI-Prompt, a zero-shot method that guides a base LLM through cognitive control steps via a structured meta-prompt, and CLAI-Tune, a fine-tuned model that internalizes these principles for spontaneous cognitive economy. Across a range of benchmarks in complex reasoning, long-context question answering, and code generation, our methods achieve significant reductions in token consumption (up to 45\%) without sacrificing accuracy. Furthermore, CLAI-Tune exhibits an emergent ability to autonomously decompose difficult problems, a key characteristic of human expert cognition. This work demonstrates that by emulating the brain's resource management strategies, we can build more efficient, robust, and capable artificial intelligence systems.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）推断的计算成本不断升级已成为其广泛和可持续部署的关键障碍。尽管现有的优化策略是有效的，但它们主要基于统计启发法或架构修改，缺乏指导认知理论来管理推理过程本身。本文旨在通过引入一种新颖的范式来弥合这一差距：认知负荷推理（CLAI）框架，该框架从认知负载理论（CLT）和LLM推理的神经科学中运行原理。 We formalize the concepts of Intrinsic Cognitive Load, Extraneous Cognitive Load, and Germane Cognitive Load into quantifiable LLM metrics ($ICL_{LLM}$, $ECL_{LLM}$, and $GCL_{LLM}$), thereby reframing the inference process as a cognitive economics optimization problem: based on the intrinsic complexity of a problem （$ icl_ {llm} $），最小化浪费计算（$ ecl_ {llm} $），并从战略上将令牌预算分配给生产性推理（$ GCL_ {llm} $）。我们提出了两种实施路径：Clai-Prompt，这是一种零摄像方法，该方法通过结构化的元过程通过认知控制步骤引导基础LLM，而Clai-Tune是一种微调模型，将这些原理内化来自发认知经济。在复杂的推理，长篇文章的答案和代码生成的一系列基准中，我们的方法在不牺牲准确性的情况下大大减少令牌消费（最高45 \％）。此外，Clai-Tune具有自主分解困难问题的新兴能力，这是人类专家认知的关键特征。这项工作表明，通过模拟大脑的资源管理策略，我们可以建立更高效，健壮和有能力的人工智能系统。</li>
</ul>

<h3>Title: A Unified Transformer-Based Framework with Pretraining For Whole Body Grasping Motion Generation</h3>
<ul>
<li><strong>Authors: </strong>Edward Effendy, Kuan-Wei Tseng, Rei Kawakami</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00676">https://arxiv.org/abs/2507.00676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00676">https://arxiv.org/pdf/2507.00676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00676]] A Unified Transformer-Based Framework with Pretraining For Whole Body Grasping Motion Generation(https://arxiv.org/abs/2507.00676)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Accepted in the ICIP 2025 We present a novel transformer-based framework for whole-body grasping that addresses both pose generation and motion infilling, enabling realistic and stable object interactions. Our pipeline comprises three stages: Grasp Pose Generation for full-body grasp generation, Temporal Infilling for smooth motion continuity, and a LiftUp Transformer that refines downsampled joints back to high-resolution markers. To overcome the scarcity of hand-object interaction data, we introduce a data-efficient Generalized Pretraining stage on large, diverse motion datasets, yielding robust spatio-temporal representations transferable to grasping tasks. Experiments on the GRAB dataset show that our method outperforms state-of-the-art baselines in terms of coherence, stability, and visual realism. The modular design also supports easy adaptation to other human-motion applications.</li>
<li><strong>摘要：</strong>在ICIP 2025中接受，我们提出了一个基于变压器的新型框架，用于全身抓握，该框架既解决姿势产生和运动填充，从而实现现实且稳定的对象相互作用。我们的管道包括三个阶段：用于全身掌握生成的抓姿势生成，平滑运动连续性的时间填充，以及提示降压接头的升压变压器，回到高分辨率标记。为了克服手动相互作用数据的稀缺性，我们在大型，多样化的运动数据集上引入了一个数据效率的广义预处理阶段，从而产生了可转移到掌握任务的可靠时空表示。 Grab数据集的实验表明，我们的方法在连贯性，稳定性和视觉现实主义方面优于最先进的基线。模块化设计还支持轻松适应其他人动作应用。</li>
</ul>

<h3>Title: Diffusion Classifier Guidance for Non-robust Classifiers</h3>
<ul>
<li><strong>Authors: </strong>Philipp Vaeth, Dibyanshu Kumar, Benjamin Paassen, Magda Gregorová</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00687">https://arxiv.org/abs/2507.00687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00687">https://arxiv.org/pdf/2507.00687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00687]] Diffusion Classifier Guidance for Non-robust Classifiers(https://arxiv.org/abs/2507.00687)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Classifier guidance is intended to steer a diffusion process such that a given classifier reliably recognizes the generated data point as a certain class. However, most classifier guidance approaches are restricted to robust classifiers, which were specifically trained on the noise of the diffusion forward process. We extend classifier guidance to work with general, non-robust, classifiers that were trained without noise. We analyze the sensitivity of both non-robust and robust classifiers to noise of the diffusion process on the standard CelebA data set, the specialized SportBalls data set and the high-dimensional real-world CelebA-HQ data set. Our findings reveal that non-robust classifiers exhibit significant accuracy degradation under noisy conditions, leading to unstable guidance gradients. To mitigate these issues, we propose a method that utilizes one-step denoised image predictions and implements stabilization techniques inspired by stochastic optimization methods, such as exponential moving averages. Experimental results demonstrate that our approach improves the stability of classifier guidance while maintaining sample diversity and visual quality. This work contributes to advancing conditional sampling techniques in generative models, enabling a broader range of classifiers to be used as guidance classifiers.</li>
<li><strong>摘要：</strong>分类器指导旨在引导扩散过程，以便给定的分类器可靠地将生成的数据点视为某个类别。但是，大多数分类器指导方法仅限于鲁棒分类器，这些分类器是专门针对扩散前进过程的噪声进行训练的。我们将分类器的指导扩展到与没有噪音的一般，非鲁斯的分类器一起工作。我们分析了标准Celeba数据集，专业Sportballs数据集和高维真实世界Celeba-HQ数据集上的非舒适和鲁棒分类器对扩散过程噪声的敏感性。我们的发现表明，在嘈杂条件下，非舒适分类器表现出明显的准确性降解，从而导致了不稳定的指导梯度。为了减轻这些问题，我们提出了一种利用一步的图像预测的方法，并实现了受随机优化方法（例如指数移动平均值）启发的稳定技术。实验结果表明，我们的方法可以提高分类器指导的稳定性，同时保持样本多样性和视觉质量。这项工作有助于推进生成模型中的条件抽样技术，从而使更广泛的分类器可以用作指导分类器。</li>
</ul>

<h3>Title: Rectifying Magnitude Neglect in Linear Attention</h3>
<ul>
<li><strong>Authors: </strong>Qihang Fan, Huaibo Huang, Yuang Ai, ran He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00698">https://arxiv.org/abs/2507.00698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00698">https://arxiv.org/pdf/2507.00698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00698]] Rectifying Magnitude Neglect in Linear Attention(https://arxiv.org/abs/2507.00698)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>As the core operator of Transformers, Softmax Attention exhibits excellent global modeling capabilities. However, its quadratic complexity limits its applicability to vision tasks. In contrast, Linear Attention shares a similar formulation with Softmax Attention while achieving linear complexity, enabling efficient global information modeling. Nevertheless, Linear Attention suffers from a significant performance degradation compared to standard Softmax Attention. In this paper, we analyze the underlying causes of this issue based on the formulation of Linear Attention. We find that, unlike Softmax Attention, Linear Attention entirely disregards the magnitude information of the Query. This prevents the attention score distribution from dynamically adapting as the Query scales. As a result, despite its structural similarity to Softmax Attention, Linear Attention exhibits a significantly different attention score distribution. Based on this observation, we propose Magnitude-Aware Linear Attention (MALA), which modifies the computation of Linear Attention to fully incorporate the Query's magnitude. This adjustment allows MALA to generate an attention score distribution that closely resembles Softmax Attention while exhibiting a more well-balanced structure. We evaluate the effectiveness of MALA on multiple tasks, including image classification, object detection, instance segmentation, semantic segmentation, natural language processing, speech recognition, and image generation. Our MALA achieves strong results on all of these tasks. Code will be available at this https URL</li>
<li><strong>摘要：</strong>作为变压器的核心操作员，SoftMax注意具有出色的全球建模功能。但是，其二次复杂性限制了其对视觉任务的适用性。相比之下，线性注意力具有类似的表述，同时又具有线性复杂性，从而实现了有效的全球信息建模。然而，与标准的软玛克斯注意相比，线性注意力遭受了显着的性能降解。在本文中，我们根据线性注意的表述分析了该问题的根本原因。我们发现，与SoftMax的关注不同，线性注意完全无视查询的幅度信息。这样可以防止注意力分布在查询量表时动态适应。结果，尽管与SoftMax注意的结构相似，但线性注意力表现出明显不同的注意力评分分布。基于此观察结果，我们提出了幅度感知线性注意（MALA），该观察值修改了线性注意的计算以充分结合查询的幅度。这种调整使MALA能够产生一个注意力分布，在表现出更均衡的结构时，与SoftMax的注意力非常相似。我们评估了MALA对多个任务的有效性，包括图像分类，对象检测，实例分割，语义分割，自然语言处理，语音识别和图像产生。我们的Mala在所有这些任务上取得了良好的成果。代码将在此HTTPS URL上找到</li>
</ul>

<h3>Title: BEV-VAE: Multi-view Image Generation with Spatial Consistency for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Zeming Chen, Hang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00707">https://arxiv.org/abs/2507.00707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00707">https://arxiv.org/pdf/2507.00707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00707]] BEV-VAE: Multi-view Image Generation with Spatial Consistency for Autonomous Driving(https://arxiv.org/abs/2507.00707)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multi-view image generation in autonomous driving demands consistent 3D scene understanding across camera views. Most existing methods treat this problem as a 2D image set generation task, lacking explicit 3D modeling. However, we argue that a structured representation is crucial for scene generation, especially for autonomous driving applications. This paper proposes BEV-VAE for consistent and controllable view synthesis. BEV-VAE first trains a multi-view image variational autoencoder for a compact and unified BEV latent space and then generates the scene with a latent diffusion transformer. BEV-VAE supports arbitrary view generation given camera configurations, and optionally 3D layouts. Experiments on nuScenes and Argoverse 2 (AV2) show strong performance in both 3D consistent reconstruction and generation. The code is available at: this https URL.</li>
<li><strong>摘要：</strong>自动驾驶中的多视图图像生成需要一致的3D场景理解跨相机视图。大多数现有方法将此问题视为2D图像设置生成任务，缺乏明确的3D建模。但是，我们认为结构化表示对于场景的产生至关重要，尤其是对于自动驾驶应用程序。本文提出了BEV-VAE，以构成一致且可控制的视图合成。 BEV-VAE首先训练一个多视图图像变化自动编码器，以使其成为紧凑而统一的BEV潜在空间，然后使用潜在扩散变压器生成场景。 BEV-VAE在给定相机配置和3D布局给定的任意视图生成。在3D一致的重建和产生中，对Nuscenes和Argoverse 2（AV2）的实验表现出强烈的性能。该代码可用：此HTTPS URL。</li>
</ul>

<h3>Title: A Probabilistic Approach to Wildfire Spread Prediction Using a Denoising Diffusion Surrogate Model</h3>
<ul>
<li><strong>Authors: </strong>Wenbo Yu, Anirbit Ghosh, Tobias Sebastian Finn, Rossella Arcucci, Marc Bocquet, Sibo Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00761">https://arxiv.org/abs/2507.00761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00761">https://arxiv.org/pdf/2507.00761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00761]] A Probabilistic Approach to Wildfire Spread Prediction Using a Denoising Diffusion Surrogate Model(https://arxiv.org/abs/2507.00761)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Thanks to recent advances in generative AI, computers can now simulate realistic and complex natural processes. We apply this capability to predict how wildfires spread, a task made difficult by the unpredictable nature of fire and the variety of environmental conditions it depends on. In this study, We present the first denoising diffusion model for predicting wildfire spread, a new kind of AI framework that learns to simulate fires not just as one fixed outcome, but as a range of possible scenarios. By doing so, it accounts for the inherent uncertainty of wildfire dynamics, a feature that traditional models typically fail to represent. Unlike deterministic approaches that generate a single prediction, our model produces ensembles of forecasts that reflect physically meaningful distributions of where fire might go next. This technology could help us develop smarter, faster, and more reliable tools for anticipating wildfire behavior, aiding decision-makers in fire risk assessment and response planning.</li>
<li><strong>摘要：</strong>得益于生成AI的最新进展，计算机现在可以模拟现实且复杂的自然过程。我们运用这种能力来预测野火如何传播，这项任务因火的不可预测性质以及其依赖的各种环境条件而困难。在这项研究中，我们介绍了第一个用于预测野火传播的脱氧扩散模型，这是一种新型的AI框架，它学会了模拟火灾不仅是一个固定结果，而且是一系列可能的情况。通过这样做，它说明了野火动态的固有不确定性，这是传统模型通常无法代表的功能。与产生单个预测的确定性方法不同，我们的模型会产生预测的集合，这些预测反映了对火的物理有意义的分布。这项技术可以帮助我们开发更智能，更快，更可靠的工具来预测野火行为，帮助决策者进行火灾风险评估和响应计划。</li>
</ul>

<h3>Title: Leveraging Genetic Algorithms for Efficient Demonstration Generation in Real-World Reinforcement Learning Environments</h3>
<ul>
<li><strong>Authors: </strong>Tom Maus, Asma Atamna, Tobias Glasmachers</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00762">https://arxiv.org/abs/2507.00762</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00762">https://arxiv.org/pdf/2507.00762</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00762]] Leveraging Genetic Algorithms for Efficient Demonstration Generation in Real-World Reinforcement Learning Environments(https://arxiv.org/abs/2507.00762)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) has demonstrated significant potential in certain real-world industrial applications, yet its broader deployment remains limited by inherent challenges such as sample inefficiency and unstable learning dynamics. This study investigates the utilization of Genetic Algorithms (GAs) as a mechanism for improving RL performance in an industrially inspired sorting environment. We propose a novel approach in which GA-generated expert demonstrations are used to enhance policy learning. These demonstrations are incorporated into a Deep Q-Network (DQN) replay buffer for experience-based learning and utilized as warm-start trajectories for Proximal Policy Optimization (PPO) agents to accelerate training convergence. Our experiments compare standard RL training with rule-based heuristics, brute-force optimization, and demonstration data, revealing that GA-derived demonstrations significantly improve RL performance. Notably, PPO agents initialized with GA-generated data achieved superior cumulative rewards, highlighting the potential of hybrid learning paradigms, where heuristic search methods complement data-driven RL. The utilized framework is publicly available and enables further research into adaptive RL strategies for real-world applications.</li>
<li><strong>摘要：</strong>强化学习（RL）在某些现实世界的工业应用中表现出了巨大的潜力，但是其更广泛的部署仍受到样本效率低下和不稳定的学习动态等固有挑战的限制。这项研究调查了遗传算法（气）的利用，作为在工业启发的分类环境中改善RL性能的一种机制。我们提出了一种新颖的方法，其中使用GA生成的专家演示来增强政策学习。这些演示被纳入了基于经验的学习的深Q-Network（DQN）重播缓冲液中，并用作近端政策优化（PPO）代理的温暖启动轨迹，以加速培训融合。我们的实验将标准RL训练与基于规则的启发式方法，蛮力优化和演示数据进行了比较，这表明GA衍生的演示显着提高了RL性能。值得注意的是，以GA生成的数据初始初始化的PPO代理获得了较高的累积奖励，突显了混合学习范式的潜力，其中启发式搜索方法补充了数据驱动的RL。使用的框架是公开可用的，可以进一步研究对现实世界应用的自适应RL策略。</li>
</ul>

<h3>Title: LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling</h3>
<ul>
<li><strong>Authors: </strong>Huaqiu Li, Yong Wang, Tongwen Huang, Hailang Huang, Haoqian Wang, Xiangxiang Chu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00790">https://arxiv.org/abs/2507.00790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00790">https://arxiv.org/pdf/2507.00790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00790]] LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling(https://arxiv.org/abs/2507.00790)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generative</a></li>
<li><strong>Abstract: </strong>Unified image restoration is a significantly challenging task in low-level vision. Existing methods either make tailored designs for specific tasks, limiting their generalizability across various types of degradation, or rely on training with paired datasets, thereby suffering from closed-set constraints. To address these issues, we propose a novel, dataset-free, and unified approach through recurrent posterior sampling utilizing a pretrained latent diffusion model. Our method incorporates the multimodal understanding model to provide sematic priors for the generative model under a task-blind condition. Furthermore, it utilizes a lightweight module to align the degraded input with the generated preference of the diffusion model, and employs recurrent refinement for posterior sampling. Extensive experiments demonstrate that our method outperforms state-of-the-art methods, validating its effectiveness and robustness. Our code and data will be available at this https URL.</li>
<li><strong>摘要：</strong>统一的图像修复是低级视力中的一项艰巨的任务。现有方法要么为特定任务制作量身定制的设计，从而限制了它们在各种降解中的推广性，或者依靠配对数据集的培训，从而遭受了封闭式约束的困扰。为了解决这些问题，我们通过预审预定的潜扩散模型通过复发后采样提出了一种新颖，无数据集和统一的方法。我们的方法结合了多模式的理解模型，以在任务盲条件下为生成模型提供半学位。此外，它利用一个轻量级模块将降解输入与扩散模型的生成偏好对齐，并采用了重新进行的细化进行后验采样。广泛的实验表明，我们的方法表现优于最先进的方法，从而验证其有效性和鲁棒性。我们的代码和数据将在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Real-Time Inverse Kinematics for Generating Multi-Constrained Movements of Virtual Human Characters</h3>
<ul>
<li><strong>Authors: </strong>Hendric Voss, Stefan Kopp</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00792">https://arxiv.org/abs/2507.00792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00792">https://arxiv.org/pdf/2507.00792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00792]] Real-Time Inverse Kinematics for Generating Multi-Constrained Movements of Virtual Human Characters(https://arxiv.org/abs/2507.00792)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating accurate and realistic virtual human movements in real-time is of high importance for a variety of applications in computer graphics, interactive virtual environments, robotics, and biomechanics. This paper introduces a novel real-time inverse kinematics (IK) solver specifically designed for realistic human-like movement generation. Leveraging the automatic differentiation and just-in-time compilation of TensorFlow, the proposed solver efficiently handles complex articulated human skeletons with high degrees of freedom. By treating forward and inverse kinematics as differentiable operations, our method effectively addresses common challenges such as error accumulation and complicated joint limits in multi-constrained problems, which are critical for realistic human motion modeling. We demonstrate the solver's effectiveness on the SMPLX human skeleton model, evaluating its performance against widely used iterative-based IK algorithms, like Cyclic Coordinate Descent (CCD), FABRIK, and the nonlinear optimization algorithm IPOPT. Our experiments cover both simple end-effector tasks and sophisticated, multi-constrained problems with realistic joint limits. Results indicate that our IK solver achieves real-time performance, exhibiting rapid convergence, minimal computational overhead per iteration, and improved success rates compared to existing methods. The project code is available at this https URL</li>
<li><strong>摘要：</strong>对于计算机图形，交互式虚拟环境，机器人技术和生物力学中的各种应用，实时生成准确和现实的虚拟人类运动至关重要。本文介绍了一种新型的实时反向运动学（IK）求解器，专为现实的人类运动生成而设计。拟议的求解器利用张力流的自动分化和即将到来的汇编，有效地处理具有高度自由度的复杂表达人类骨骼。通过将前向运动和反向运动学视为可区分的操作，我们的方法有效地解决了共同的挑战，例如误差积累和复杂的多约束问题的关节限制，这对于现实的人类运动建模至关重要。我们证明了该求解器对SMPLX人骨架模型的有效性，评估了其性能，以广泛使用的基于迭代的IK算法，例如环状坐标下降（CCD），FABRIK和非线性优化算法IPOPT。我们的实验涵盖了简单的最终效果任务和具有现实的联合限制的复杂，多约束的问题。结果表明，与现有方法相比，我们的IK求解器可实现实时性能，表现出快速收敛性，最小的计算开销以及提高的成功率。该项目代码可在此HTTPS URL上获得</li>
</ul>

<h3>Title: TRACE: Temporally Reliable Anatomically-Conditioned 3D CT Generation with Enhanced Efficiency</h3>
<ul>
<li><strong>Authors: </strong>Minye Shao, Xingyu Miao, Haoran Duan, Zeyu Wang, Jingkun Chen, Yawen Huang, Xian Wu, Jingjing Deng, Yang Long, Yefeng Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00802">https://arxiv.org/abs/2507.00802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00802">https://arxiv.org/pdf/2507.00802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00802]] TRACE: Temporally Reliable Anatomically-Conditioned 3D CT Generation with Enhanced Efficiency(https://arxiv.org/abs/2507.00802)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>3D medical image generation is essential for data augmentation and patient privacy, calling for reliable and efficient models suited for clinical practice. However, current methods suffer from limited anatomical fidelity, restricted axial length, and substantial computational cost, placing them beyond reach for regions with limited resources and infrastructure. We introduce TRACE, a framework that generates 3D medical images with spatiotemporal alignment using a 2D multimodal-conditioned diffusion approach. TRACE models sequential 2D slices as video frame pairs, combining segmentation priors and radiology reports for anatomical alignment, incorporating optical flow to sustain temporal coherence. During inference, an overlapping-frame strategy links frame pairs into a flexible length sequence, reconstructed into a spatiotemporally and anatomically aligned 3D volume. Experimental results demonstrate that TRACE effectively balances computational efficiency with preserving anatomical fidelity and spatiotemporal consistency. Code is available at: this https URL.</li>
<li><strong>摘要：</strong>3D医学图像生成对于数据扩展和患者隐私至关重要，呼吁适合临床实践的可靠和高效模型。但是，当前的方法遭受了有限的解剖学忠诚度，轴向长度有限和实质性的计算成本，使它们无法获得资源和基础设施有限的地区。我们介绍了Trace，该框架是使用2D多模式调节扩散方法生成具有时空比对的3D医学图像。跟踪模型顺序2D切片作为视频框架对，结合了分割先验和放射学报告，以进行解剖对齐，并结合了光流以维持时间相干性。在推断期间，重叠的框架策略将框架链接到柔性长度序列，并重建为空间和解剖上的3D体积。实验结果表明，痕量有效地平衡了计算效率与保留解剖学保真度和时空一致性。代码可用：此HTTPS URL。</li>
</ul>

<h3>Title: CAVALRY-V: A Large-Scale Generator Framework for Adversarial Attacks on Video MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Jiaming Zhang, Rui Hu, Qing Guo, Wei Yang Bryan Lim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00817">https://arxiv.org/abs/2507.00817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00817">https://arxiv.org/pdf/2507.00817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00817]] CAVALRY-V: A Large-Scale Generator Framework for Adversarial Attacks on Video MLLMs(https://arxiv.org/abs/2507.00817)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Video Multimodal Large Language Models (V-MLLMs) have shown impressive capabilities in temporal reasoning and cross-modal understanding, yet their vulnerability to adversarial attacks remains underexplored due to unique challenges: complex cross-modal reasoning mechanisms, temporal dependencies, and computational constraints. We present CAVALRY-V (Cross-modal Language-Vision Adversarial Yielding for Videos), a novel framework that directly targets the critical interface between visual perception and language generation in V-MLLMs. Our approach introduces two key innovations: (1) a dual-objective semantic-visual loss function that simultaneously disrupts the model's text generation logits and visual representations to undermine cross-modal integration, and (2) a computationally efficient two-stage generator framework that combines large-scale pre-training for cross-model transferability with specialized fine-tuning for spatiotemporal coherence. Empirical evaluation on comprehensive video understanding benchmarks demonstrates that CAVALRY-V significantly outperforms existing attack methods, achieving 22.8% average improvement over the best baseline attacks on both commercial systems (GPT-4.1, Gemini 2.0) and open-source models (QwenVL-2.5, InternVL-2.5, Llava-Video, Aria, MiniCPM-o-2.6). Our framework achieves flexibility through implicit temporal coherence modeling rather than explicit regularization, enabling significant performance improvements even on image understanding (34.4% average gain). This capability demonstrates CAVALRY-V's potential as a foundational approach for adversarial research across multimodal systems.</li>
<li><strong>摘要：</strong>视频多模式大型语言模型（V-MLLM）在时间推理和跨模式的理解中表现出了令人印象深刻的能力，但是由于独特的挑战，它们对对抗性攻击的脆弱性仍未得到充实：复杂的跨模式推理机制，时间依赖性和计算约束。我们提出了骑兵V（视频的跨模式语言视觉对抗屈服），这是一个新颖的框架，它直接针对V-MLLM中视觉感知与语言产生之间的关键接口。我们的方法介绍了两个关键创新：（1）双方语义 - 视觉损失函数，同时破坏了模型的文本生成逻辑和视觉表示，以破坏跨模式积分，以及（2）计算上有效的两阶段生成器框架，可将大型预训练与特定型号转移相结合，以使其与特定的细胞转换结合使用，以实现特殊的细胞转换。 Empirical evaluation on comprehensive video understanding benchmarks demonstrates that CAVALRY-V significantly outperforms existing attack methods, achieving 22.8% average improvement over the best baseline attacks on both commercial systems (GPT-4.1, Gemini 2.0) and open-source models (QwenVL-2.5, InternVL-2.5, Llava-Video, Aria, MiniCPM-o-2.6).我们的框架通过隐式的时间连贯建模而不是明确的正则化来实现灵活性，即使在图像理解中也可以显着提高性能（平均增益为34.4％）。该能力证明了骑兵V作为跨多模式系统对抗研究的基础方法的潜力。</li>
</ul>

<h3>Title: BoltzNCE: Learning Likelihoods for Boltzmann Generation with Stochastic Interpolants and Noise Contrastive Estimation</h3>
<ul>
<li><strong>Authors: </strong>Rishal Aggrwal, Jacky Chen, Nicholas M. Boffi, David Ryan Koes</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.bio-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00846">https://arxiv.org/abs/2507.00846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00846">https://arxiv.org/pdf/2507.00846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00846]] BoltzNCE: Learning Likelihoods for Boltzmann Generation with Stochastic Interpolants and Noise Contrastive Estimation(https://arxiv.org/abs/2507.00846)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Efficient sampling from the Boltzmann distribution defined by an energy function is a key challenge in modeling physical systems such as molecules. Boltzmann Generators tackle this by leveraging Continuous Normalizing Flows that transform a simple prior into a distribution that can be reweighted to match the Boltzmann distribution using sample likelihoods. However, obtaining likelihoods requires computing costly Jacobians during integration, making it impractical for large molecular systems. To overcome this, we propose learning the likelihood of the generated distribution via an energy-based model trained with noise contrastive estimation and score matching. By using stochastic interpolants to anneal between the prior and generated distributions, we combine both the objective functions to efficiently learn the density function. On the alanine dipeptide system, we demonstrate that our method yields free energy profiles and energy distributions comparable to those obtained with exact likelihoods. Additionally, we show that free energy differences between metastable states can be estimated accurately with orders-of-magnitude speedup.</li>
<li><strong>摘要：</strong>从能量函数定义的玻尔兹曼分布中进行的有效采样是建模物理系统（例如分子）的关键挑战。 Boltzmann发电机通过利用连续归一化的流量来解决此问题，该流程将简单的先验转换为可以重新加权以使用样品可能性匹配Boltzmann分布的分布。但是，获得可能性在整合过程中需要计算昂贵的雅各布人，这对于大分子系统来说是不切实际的。为了克服这一点，我们建议通过基于噪声对比的估计和得分匹配的基于能量的模型来学习生成分布的可能性。通过使用随机插值剂在先验分布和生成的分布之间退火，我们将两个目标函数结合在一起以有效地学习密度函数。在丙氨酸二肽系统上，我们证明我们的方法产生的自由能谱和能量分布与具有精确可能性获得的方法相当。此外，我们表明，可以通过稳定速度的速度准确估计亚稳态状态之间的自由能差。</li>
</ul>

<h3>Title: SafeMap: Robust HD Map Construction from Incomplete Observations</h3>
<ul>
<li><strong>Authors: </strong>Xiaoshuai Hao, Lingdong Kong, Rong Yin, Pengwei Wang, Jing Zhang, Yunfeng Diao, Shu Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00861">https://arxiv.org/abs/2507.00861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00861">https://arxiv.org/pdf/2507.00861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00861]] SafeMap: Robust HD Map Construction from Incomplete Observations(https://arxiv.org/abs/2507.00861)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Robust high-definition (HD) map construction is vital for autonomous driving, yet existing methods often struggle with incomplete multi-view camera data. This paper presents SafeMap, a novel framework specifically designed to secure accuracy even when certain camera views are missing. SafeMap integrates two key components: the Gaussian-based Perspective View Reconstruction (G-PVR) module and the Distillation-based Bird's-Eye-View (BEV) Correction (D-BEVC) module. G-PVR leverages prior knowledge of view importance to dynamically prioritize the most informative regions based on the relationships among available camera views. Furthermore, D-BEVC utilizes panoramic BEV features to correct the BEV representations derived from incomplete observations. Together, these components facilitate the end-to-end map reconstruction and robust HD map generation. SafeMap is easy to implement and integrates seamlessly into existing systems, offering a plug-and-play solution for enhanced robustness. Experimental results demonstrate that SafeMap significantly outperforms previous methods in both complete and incomplete scenarios, highlighting its superior performance and reliability.</li>
<li><strong>摘要：</strong>强大的高清（HD）地图结构对于自动驾驶至关重要，但是现有的方法通常与不完整的多视摄像头数据难以挣扎。本文介绍了SafeMap，这是一个新颖的框架，即使缺少某些相机视图，旨在确保准确性。 SAFEMAP集成了两个关键组成部分：基于高斯的透视图重建（G-PVR）模块和基于蒸馏的鸟类的视图（BEV）校正（D-BEVC）模块。 G-PVR利用了根据可用摄像机视图之间的关系动态优先考虑最有用的区域的意见的先验知识。此外，D-BEVC利用全景BEV特征来纠正从不完整观察结果中得出的BEV表示。这些组件共同促进了端到端地图重建和稳健的高清图生成。 SAFEMAP易于实现，并无缝地集成到现有系统中，提供了插件解决方案，以增强鲁棒性。实验结果表明，在完整和不完整的情况下，SAFEMAP明显优于先前的方法，从而突出了其出色的性能和可靠性。</li>
</ul>

<h3>Title: Is Visual in-Context Learning for Compositional Medical Tasks within Reach?</h3>
<ul>
<li><strong>Authors: </strong>Simon Reiß, Zdravko Marinov, Alexander Jaus, Constantin Seibold, M. Saquib Sarfraz, Erik Rodner, Rainer Stiefelhagen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00868">https://arxiv.org/abs/2507.00868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00868">https://arxiv.org/pdf/2507.00868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00868]] Is Visual in-Context Learning for Compositional Medical Tasks within Reach?(https://arxiv.org/abs/2507.00868)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In this paper, we explore the potential of visual in-context learning to enable a single model to handle multiple tasks and adapt to new tasks during test time without re-training. Unlike previous approaches, our focus is on training in-context learners to adapt to sequences of tasks, rather than individual tasks. Our goal is to solve complex tasks that involve multiple intermediate steps using a single model, allowing users to define entire vision pipelines flexibly at test time. To achieve this, we first examine the properties and limitations of visual in-context learning architectures, with a particular focus on the role of codebooks. We then introduce a novel method for training in-context learners using a synthetic compositional task generation engine. This engine bootstraps task sequences from arbitrary segmentation datasets, enabling the training of visual in-context learners for compositional tasks. Additionally, we investigate different masking-based training objectives to gather insights into how to train models better for solving complex, compositional tasks. Our exploration not only provides important insights especially for multi-modal medical task sequences but also highlights challenges that need to be addressed.</li>
<li><strong>摘要：</strong>在本文中，我们探讨了视觉中文学习的潜力，以使单个模型能够处理多个任务并在不重新训练的情况下适应新任务。与以前的方法不同，我们的重点是培训中文学习者以适应任务序列而不是单个任务。我们的目标是解决使用单个模型涉及多个中间步骤的复杂任务，使用户可以在测试时灵活地定义整个视觉管道。为了实现这一目标，我们首先检查了视觉内部学习体系结构的属性和局限性，并特别关注代码书的角色。然后，我们介绍了一种使用合成组成的任务生成引擎来训练中文学习者的新方法。此引擎引导程序从任意分割数据集中进行的任务序列，从而使视觉上的内在学习者可以培训组成任务。此外，我们研究了不同的基于掩盖的培训目标，以收集有关如何更好地培训模型以解决复杂，组成任务的见解。我们的探索不仅提供了重要的见解，尤其是对于多模式医疗任务序列，还突出了需要解决的挑战。</li>
</ul>

<h3>Title: ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zifu Wan, Ce Zhang, Silong Yong, Martin Q. Ma, Simon Stepputtis, Louis-Philippe Morency, Deva Ramanan, Katia Sycara, Yaqi Xie</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00898">https://arxiv.org/abs/2507.00898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00898">https://arxiv.org/pdf/2507.00898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00898]] ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models(https://arxiv.org/abs/2507.00898)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent Large Vision-Language Models (LVLMs) have introduced a new paradigm for understanding and reasoning about image input through textual responses. Although they have achieved remarkable performance across a range of multi-modal tasks, they face the persistent challenge of hallucination, which introduces practical weaknesses and raises concerns about their reliable deployment in real-world applications. Existing work has explored contrastive decoding approaches to mitigate this issue, where the output of the original LVLM is compared and contrasted with that of a perturbed version. However, these methods require two or more queries that slow down LVLM response generation, making them less suitable for real-time applications. To overcome this limitation, we propose ONLY, a training-free decoding approach that requires only a single query and a one-layer intervention during decoding, enabling efficient real-time deployment. Specifically, we enhance textual outputs by selectively amplifying crucial textual information using a text-to-visual entropy ratio for each token. Extensive experimental results demonstrate that our proposed ONLY consistently outperforms state-of-the-art methods across various benchmarks while requiring minimal implementation effort and computational cost. Code is available at this https URL.</li>
<li><strong>摘要：</strong>最近的大型视觉模型（LVLM）引入了一种新的范式，用于通过文本响应理解和推理图像输入。尽管他们在各种多模式任务中取得了出色的表现，但他们面临着幻觉的持续挑战，这引入了实际弱点，并引起了人们对它们在现实世界应用中可靠部署的担忧。现有工作探索了对比的解码方法来减轻此问题，其中比较了原始LVLM的输出并将其与扰动版本的输出进行对比。但是，这些方法需要两个或多个查询，以减慢LVLM响应的生成，从而使其不适合实时应用。为了克服这一限制，我们只提出一种无培训的解码方法，该方法在解码过程中仅需要一个查询和一层干预，从而有效地实时部署。具体而言，我们通过使用每个令牌的文本到视觉熵比选择性地放大至关重要的文本信息来增强文本输出。广泛的实验结果表明，我们提出的只能始终超过各种基准测试的最先进方法，同时需要最小的实施工作和计算成本。代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: TABASCO: A Fast, Simplified Model for Molecular Generation with Improved Physical Quality</h3>
<ul>
<li><strong>Authors: </strong>Carlos Vonessen, Charles Harris, Miruna Cretu, Pietro Liò</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00899">https://arxiv.org/abs/2507.00899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00899">https://arxiv.org/pdf/2507.00899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00899]] TABASCO: A Fast, Simplified Model for Molecular Generation with Improved Physical Quality(https://arxiv.org/abs/2507.00899)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>State-of-the-art models for 3D molecular generation are based on significant inductive biases, SE(3), permutation equivariance to respect symmetry and graph message-passing networks to capture local chemistry, yet the generated molecules still struggle with physical plausibility. We introduce TABASCO which relaxes these assumptions: The model has a standard non-equivariant transformer architecture, treats atoms in a molecule as sequences and reconstructs bonds deterministically after generation. The absence of equivariant layers and message passing allows us to significantly simplify the model architecture and scale data throughput. On the GEOM-Drugs benchmark TABASCO achieves state-of-the-art PoseBusters validity and delivers inference roughly 10x faster than the strongest baseline, while exhibiting emergent rotational equivariance despite symmetry not being hard-coded. Our work offers a blueprint for training minimalist, high-throughput generative models suited to specialised tasks such as structure- and pharmacophore-based drug design. We provide a link to our implementation at this http URL.</li>
<li><strong>摘要：</strong>3D分子生成的最新模型基于明显的电感偏见，SE（3），置换式符号符合对称性和图形消息的网络以捕获局部化学，但产生的分子仍然与物理合理性挣扎。我们介绍了放松这些假设的塔巴斯科：该模型具有标准的非等分变压器结构，以分子为序列将原子视为序列，并在生成后确定性地重建键。缺乏模棱两可的层和消息传递使我们能够显着简化模型架构和扩展数据吞吐量。在GEOM-PRUGS上，基准测试塔巴斯科实现了最先进的波塞布斯的有效性，并且推断的速度大约比最强的基线快10倍，同时尽管没有硬编码对称性，但表现出了新兴的旋转率。我们的工作提供了一种蓝图，用于培训极简主义的高通量生成模型，该模型适合于诸如结构和药效团的药物设计等专业任务。我们在此HTTP URL上提供了指向我们实施的链接。</li>
</ul>

<h3>Title: Reasoning as an Adaptive Defense for Safety</h3>
<ul>
<li><strong>Authors: </strong>Taeyoun Kim, Fahim Tajwar, Aditi Raghunathan, Aviral Kumar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00971">https://arxiv.org/abs/2507.00971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00971">https://arxiv.org/pdf/2507.00971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00971]] Reasoning as an Adaptive Defense for Safety(https://arxiv.org/abs/2507.00971)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reasoning methods that adaptively allocate test-time compute have advanced LLM performance on easy to verify domains such as math and code. In this work, we study how to utilize this approach to train models that exhibit a degree of robustness to safety vulnerabilities, and show that doing so can provide benefits. We build a recipe called $\textit{TARS}$ (Training Adaptive Reasoners for Safety), a reinforcement learning (RL) approach that trains models to reason about safety using chain-of-thought traces and a reward signal that balances safety with task completion. To build TARS, we identify three critical design choices: (1) a "lightweight" warmstart SFT stage, (2) a mix of harmful, harmless, and ambiguous prompts to prevent shortcut behaviors such as too many refusals, and (3) a reward function to prevent degeneration of reasoning capabilities during training. Models trained with TARS exhibit adaptive behaviors by spending more compute on ambiguous queries, leading to better safety-refusal trade-offs. They also internally learn to better distinguish between safe and unsafe prompts and attain greater robustness to both white-box (e.g., GCG) and black-box attacks (e.g., PAIR). Overall, our work provides an effective, open recipe for training LLMs against jailbreaks and harmful requests by reasoning per prompt.</li>
<li><strong>摘要：</strong>适应性分配测试时间计算的推理方法在易于验证域（例如数学和代码）上具有高级LLM性能。在这项工作中，我们研究了如何利用这种方法来培训对安全漏洞表现出一定稳健性的模型，并表明这样做可以提供好处。我们构建了一种称为$ \ textit {tars} $的食谱（培训自适应推理器的安全性），这是一种增强学习方法（RL）方法，该方法训练模型，使用经过经过思考的痕迹和奖励信号来对安全进行推理，以平衡安全性与任务完成。为了构建焦油，我们确定了三个关键的设计选择：（1）“轻巧”的SFT阶段，（2）有害，无害和模棱两可的提示，以防止捷径行为（例如太多的拒绝），以及（3）奖励功能，以防止训练期间的推理能力。经过焦油训练的模型通过在模棱两可的查询上花费更多的计算来表现自适应行为，从而导致更好的安全性 - 折衷权衡。他们还在内部学会更好地区分安全和不安全的提示，并对白色框（例如GCG）和黑盒攻击（例如，配对）获得更大的鲁棒性。总体而言，我们的工作为通过每提示推理提供了有效的开放式培训LLM的培训LLMS和有害要求的食谱。</li>
</ul>

<h3>Title: Evaluating Robustness of Monocular Depth Estimation with Procedural Scene Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Jack Nugent, Siyang Wu, Zeyu Ma, Beining Han, Meenal Parakh, Abhishek Joshi, Lingjie Mei, Alexander Raistrick, Xinyuan Li, Jia Deng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00981">https://arxiv.org/abs/2507.00981</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00981">https://arxiv.org/pdf/2507.00981</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00981]] Evaluating Robustness of Monocular Depth Estimation with Procedural Scene Perturbations(https://arxiv.org/abs/2507.00981)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent years have witnessed substantial progress on monocular depth estimation, particularly as measured by the success of large models on standard benchmarks. However, performance on standard benchmarks does not offer a complete assessment, because most evaluate accuracy but not robustness. In this work, we introduce PDE (Procedural Depth Evaluation), a new benchmark which enables systematic robustness evaluation. PDE uses procedural generation to create 3D scenes that test robustness to various controlled perturbations, including object, camera, material and lighting changes. Our analysis yields interesting findings on what perturbations are challenging for state-of-the-art depth models, which we hope will inform further research. Code and data are available at this https URL.</li>
<li><strong>摘要：</strong>近年来，单眼深度估计取得了长足的进步，特别是通过大型模型在标准基准上的成功来衡量。但是，标准基准测试的性能并不能提供完整的评估，因为大多数评估准确性但稳健性。在这项工作中，我们介绍了PDE（程序深度评估），这是一种新的基准测试，可实现系统性的鲁棒性评估。 PDE使用程序生成来创建3D场景，以测试对各种受控扰动的鲁棒性，包括对象，相机，材料和照明变化。我们的分析得出了关于哪些扰动对最新深度模型具有挑战性的有趣发现，我们希望这将为进一步的研究提供信息。代码和数据可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: UniGlyph: Unified Segmentation-Conditioned Diffusion for Precise Visual Text Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Yuanrui Wang, Cong Han, YafeiLi, Zhipeng Jin, Xiawei Li, SiNan Du, Wen Tao, Yi Yang, shuanglong li, Chun Yuan, Liu Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00992">https://arxiv.org/abs/2507.00992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00992">https://arxiv.org/pdf/2507.00992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00992]] UniGlyph: Unified Segmentation-Conditioned Diffusion for Precise Visual Text Synthesis(https://arxiv.org/abs/2507.00992)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-image generation has greatly advanced content creation, yet accurately rendering visual text remains a key challenge due to blurred glyphs, semantic drift, and limited style control. Existing methods often rely on pre-rendered glyph images as conditions, but these struggle to retain original font styles and color cues, necessitating complex multi-branch designs that increase model overhead and reduce flexibility. To address these issues, we propose a segmentation-guided framework that uses pixel-level visual text masks -- rich in glyph shape, color, and spatial detail -- as unified conditional inputs. Our method introduces two core components: (1) a fine-tuned bilingual segmentation model for precise text mask extraction, and (2) a streamlined diffusion model augmented with adaptive glyph conditioning and a region-specific loss to preserve textual fidelity in both content and style. Our approach achieves state-of-the-art performance on the AnyText benchmark, significantly surpassing prior methods in both Chinese and English settings. To enable more rigorous evaluation, we also introduce two new benchmarks: GlyphMM-benchmark for testing layout and glyph consistency in complex typesetting, and MiniText-benchmark for assessing generation quality in small-scale text regions. Experimental results show that our model outperforms existing methods by a large margin in both scenarios, particularly excelling at small text rendering and complex layout preservation, validating its strong generalization and deployment readiness.</li>
<li><strong>摘要：</strong>文本到图像的生成具有大量高级的内容创建，但由于字形模糊，语义漂移和有限的样式控制，因此准确地渲染视觉文本仍然是一个关键挑战。现有的方法通常依赖于预渲染的字形图像作为条件，但是这些方法努力保留原始的字体样式和颜色提示，需要进行复杂的多分支设计，以增加模型开销并降低灵活性。为了解决这些问题，我们提出了一个分割引导的框架，该框架使用像素级的视觉文本掩码 - 富含glyph形状，颜色和空间细节 - 作为统一的条件输入。我们的方法介绍了两个核心组成部分：（1）用于精确文本掩码提取的微型双语分割模型，以及（2）一种流线型扩散模型增强，并以自适应的Glyph条件和特定区域的特定损失增强，以保留内容和样式中的文本保真度。我们的方法在AnyText基准上实现了最先进的性能，在中文和英语环境中都超过了先前的方法。为了实现更严格的评估，我们还介绍了两个新的基准：用于测试布局的Glyphmm基准和复杂排版中的字形一致性，以及用于评估小规模文本区域发电质量的Minitext基准。实验结果表明，在两种情况下，我们的模型都优于现有方法，尤其是在小文本渲染和复杂的布局保存方面尤其出色，从而验证了其强大的概括和部署准备就绪。</li>
</ul>

<h3>Title: ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention</h3>
<ul>
<li><strong>Authors: </strong>Yuhong Chou, Zehao Liu, Ruijie Zhu, Xinyi Wan, Tianjian Li, Congying Chu, Qian Liu, Jibin Wu, Zejun Ma</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01004">https://arxiv.org/abs/2507.01004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01004">https://arxiv.org/pdf/2507.01004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01004]] ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention(https://arxiv.org/abs/2507.01004)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Linear attention mechanisms deliver significant advantages for Large Language Models (LLMs) by providing linear computational complexity, enabling efficient processing of ultra-long sequences (e.g., 1M context). However, existing Sequence Parallelism (SP) methods, essential for distributing these workloads across devices, become the primary bottleneck due to substantial communication overhead. In this paper, we introduce ZeCO (Zero Communication Overhead) sequence parallelism for linear attention models, a new SP method designed to overcome these limitations and achieve end-to-end near-linear scalability for long sequence training. For example, training a model with a 1M sequence length across 64 devices using ZeCO takes roughly the same time as training with an 16k sequence on a single device. At the heart of ZeCO lies All-Scan, a new collective communication primitive. All-Scan provides each SP rank with precisely the initial operator state it requires while maintaining a minimal communication footprint, effectively eliminating communication overhead. Theoretically, we prove the optimaity of ZeCO, showing that it introduces only negligible time and space overhead. Empirically, we compare the communication costs of different sequence parallelism strategies and demonstrate that All-Scan achieves the fastest communication in SP scenarios. Specifically, on 256 GPUs with an 8M sequence length, ZeCO achieves a 60\% speedup compared to the current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a clear path toward efficiently training next-generation LLMs on previously intractable sequence lengths.</li>
<li><strong>摘要：</strong>线性注意机制通过提供线性计算复杂性，从而有效地处理超长序列（例如1M上下文），从而为大语言模型（LLM）带来了显着优势。但是，现有的序列并行性（SP）方法，对于在设备上分配这些工作量必不可少的，由于大量通信开销而成为主要的瓶颈。在本文中，我们介绍了线性注意模型的Zeco（零通信开销）序列并行性，这是一种新的SP方法，旨在克服这些局限性并实现长序列训练的端到端近线性可扩展性。例如，使用Zeco在64个设备上具有1M序列长度的模型与在单个设备上使用16K序列的训练大致相同。 Zeco的核心是All-Scan，这是一种新的集体交流。 All-Scan为每个SP提供了确切的初始操作员状态，同时保持最小的通信足迹，从而有效地消除了沟通开销。从理论上讲，我们证明了Zeco的最佳性，表明它仅引入了可忽略的时间和空间开销。从经验上讲，我们比较了不同序列并行策略的沟通成本，并证明了在SP方案中，全扫描实现了最快的沟通。具体而言，与当前的最新ART（SOTA）SP方法相比，Zeco在256 GPU的256 GPU上达到60 \％的加速。我们认为，Zeco在以前棘手的序列长度上有效地训练下一​​代LLMS建立了一条清晰的途径。</li>
</ul>

<h3>Title: DAM-VSR: Disentanglement of Appearance and Motion for Video Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Zhe Kong, Le Li, Yong Zhang, Feng Gao, Shaoshu Yang, Tao Wang, Kaihao Zhang, Zhuoliang Kang, Xiaoming Wei, Guanying Chen, Wenhan Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01012">https://arxiv.org/abs/2507.01012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01012">https://arxiv.org/pdf/2507.01012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01012]] DAM-VSR: Disentanglement of Appearance and Motion for Video Super-Resolution(https://arxiv.org/abs/2507.01012)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generation, generative</a></li>
<li><strong>Abstract: </strong>Real-world video super-resolution (VSR) presents significant challenges due to complex and unpredictable degradations. Although some recent methods utilize image diffusion models for VSR and have shown improved detail generation capabilities, they still struggle to produce temporally consistent frames. We attempt to use Stable Video Diffusion (SVD) combined with ControlNet to address this issue. However, due to the intrinsic image-animation characteristics of SVD, it is challenging to generate fine details using only low-quality videos. To tackle this problem, we propose DAM-VSR, an appearance and motion disentanglement framework for VSR. This framework disentangles VSR into appearance enhancement and motion control problems. Specifically, appearance enhancement is achieved through reference image super-resolution, while motion control is achieved through video ControlNet. This disentanglement fully leverages the generative prior of video diffusion models and the detail generation capabilities of image super-resolution models. Furthermore, equipped with the proposed motion-aligned bidirectional sampling strategy, DAM-VSR can conduct VSR on longer input videos. DAM-VSR achieves state-of-the-art performance on real-world data and AIGC data, demonstrating its powerful detail generation capabilities.</li>
<li><strong>摘要：</strong>现实世界的视频超分辨率（VSR）由于复杂且不可预测的退化而提出了重大挑战。尽管一些最近的方法利用了VSR的图像扩散模型，并显示了提高的细节生成功能，但它们仍然很难产生时间一致的帧。我们尝试使用稳定的视频扩散（SVD）与ControlNet结合来解决此问题。但是，由于SVD的固有图像动画特性，仅使用低质量视频来生成细节是一项挑战。为了解决这个问题，我们提出了Dam-VSR，即VSR的外观和运动解开框架。该框架将VSR分解为外观增强和运动控制问题。具体而言，通过参考图像超分辨率实现了外观增强，而运动控制是通过视频控制网络实现的。这种解开完全利用了视频扩散模型的生成先验和图像超分辨率模型的细节生成功能。此外，DAM-VSR配备了建议的运动对准双向采样策略，可以在更长的输入视频上进行VSR。 DAM-VSR在现实世界数据和AIGC数据上实现了最先进的性能，证明了其强大的细节生成功能。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
