<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-02-04</h1>
<h3>Title: Learning ORDER-Aware Multimodal Representations for Composite Materials Design</h3>
<ul>
<li><strong>Authors: </strong>Xinyao Li, Hangwei Qian, Jingjing Li, Ivor Tsang</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02513">https://arxiv.org/abs/2602.02513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02513">https://arxiv.org/pdf/2602.02513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02513]] Learning ORDER-Aware Multimodal Representations for Composite Materials Design(https://arxiv.org/abs/2602.02513)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Artificial intelligence (AI) has shown remarkable success in materials discovery and property prediction, particularly for crystalline and polymer systems where material properties and structures are dominated by discrete graph representations. Such graph-central paradigm breaks down on composite materials, which possess continuous and nonlinear design spaces that lack well-defined graph structures. General composite descriptors, e.g., fiber volume and misalignment angle, cannot fully capture the fiber distributions that fundamentally determine microstructural characteristics, necessitating the integration of heterogeneous data sources through multimodal learning. Existing alignment-oriented multimodal frameworks have proven effective on abundant crystal or polymer data under discrete, unique graph-property mapping assumptions, but fail to address the highly continuous composite design space under extreme data scarcity. In this work, we introduce ORDinal-aware imagE-tabulaR alignment (ORDER), a multimodal pretraining framework that establishes ordinality as a core principle for composite material representations. ORDER ensures that materials with similar target properties occupy nearby regions in the latent space, which effectively preserves the continuous nature of composite properties and enables meaningful interpolation between sparsely observed designs. We evaluate ORDER on a public Nanofiber-enforced composite dataset and an internally curated dataset that simulates the construction of carbon fiber T700 with diverse fiber distributions. ORDER achieves consistent improvements over state-of-the-art multimodal baselines across property prediction, cross-modal retrieval, and microstructure generation tasks.</li>
<li><strong>摘要：</strong>人工智能 (AI) 在材料发现和性能预测方面取得了显着的成功，特别是对于材料性能和结构由离散图形表示主导的晶体和聚合物系统。这种以图为中心的范式在复合材料上崩溃了，复合材料具有连续和非线性的设计空间，缺乏明确定义的图结构。一般的复合描述符，例如纤维体积和错位角度，无法完全捕获从根本上决定微观结构特征的纤维分布，因此需要通过多模态学习来集成异构数据源。现有的面向排列的多模态框架已被证明在离散、独特的图形属性映射假设下对丰富的晶体或聚合物数据有效，但无法解决极端数据稀缺下的高度连续的复合材料设计空间。在这项工作中，我们引入了 ORDinal-aware imagE-tabulaR 对齐（ORDER），这是一种多模态预训练框架，它将序数确立为复合材料表示的核心原则。 ORDER 确保具有相似目标特性的材料占据潜在空间中的附近区域，这有效地保留了复合材料特性的连续性，并能够在稀疏观察的设计之间进行有意义的插值。我们在公共纳米纤维增强复合数据集和内部策划的数据集上评估 ORDER，该数据集模拟具有不同纤维分布的碳纤维 T700 的构造。 ORDER 在属性预测、跨模态检索和微观结构生成任务方面实现了对最先进的多模态基线的一致改进。</li>
</ul>

<h3>Title: The "Robert Boulton" Singularity: Semantic Tunneling and Manifold Unfolding in Recursive AI</h3>
<ul>
<li><strong>Authors: </strong>Pengyue Hou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02526">https://arxiv.org/abs/2602.02526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02526">https://arxiv.org/pdf/2602.02526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02526]] The "Robert Boulton" Singularity: Semantic Tunneling and Manifold Unfolding in Recursive AI(https://arxiv.org/abs/2602.02526)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The stability of generative artificial intelligence trained on recursive synthetic data is conventionally monitored via Perplexity (PPL). We demonstrate that PPL is a deceptive metric in context-stabilized regimes (L=128). Using a rigorous sliding-window protocol (N=1500), we identify a novel failure mode termed "Semantic Tunneling." While the Baseline model maintains high grammatical fluency (PPL approx. 83.9), it suffers a catastrophic loss of semantic diversity, converging within seven generations to a single, low-entropy narrative attractor: the "Robert Boulton" Singularity. This phenomenon represents a total collapse of the latent manifold (Global Effective Rank 3.62 -> 2.22), where the model discards diverse world knowledge to optimize for statistically safe syntactic templates. To address this, we apply the Multi-Scale Negative Coupled Information Systems (MNCIS) framework recently established in Hou (2026) [arXiv:2601.11594]. We demonstrate that Adaptive Spectral Negative Coupling (ASNC) acts as a topological operator that actively induces "Manifold Unfolding." MNCIS forces the model to expand its effective rank from the anisotropic baseline of 3.62 to a hyper-diverse state of 5.35, effectively constructing an "Artificial Manifold" that resists the gravitational pull of semantic attractors and preserves the long-tail distribution of the training data.</li>
<li><strong>摘要：</strong>在递归合成数据上训练的生成人工智能的稳定性通常通过 Perplexity (PPL) 进行监控。我们证明 PPL 在上下文稳定的情况下是一种欺骗性指标 (L=128)。使用严格的滑动窗口协议（N=1500），我们确定了一种称为“语义隧道”的新颖故障模式。虽然 Baseline 模型保持了较高的语法流畅性（PPL 约为 83.9），但它遭受了语义多样性的灾难性损失，在七代之内汇聚成一个单一的低熵叙事吸引子：“罗伯特·博尔顿”奇点。这种现象代表了潜在流形的彻底崩溃（全球有效排名 3.62 -> 2.22），其中模型丢弃了不同的世界知识以优化统计上安全的句法模板。为了解决这个问题，我们应用了 Hou (2026) 最近建立的多尺度负耦合信息系统 (MNCIS) 框架 [arXiv:2601.11594]。我们证明了自适应谱负耦合（ASNC）充当拓扑算子，主动诱导“流形展开”。 MNCIS 迫使模型将其有效排名从各向异性基线 3.62 扩展到超多样化状态 5.35，有效构建了一个抵抗语义吸引子引力并保留训练数据长尾分布的“人工流形”。</li>
</ul>

<h3>Title: From Sparse Decisions to Dense Reasoning: A Multi-attribute Trajectory Paradigm for Multimodal Moderation</h3>
<ul>
<li><strong>Authors: </strong>Tianle Gu, Kexin Huang, Lingyu Li, Ruilin Luo, Shiyang Huang, Zongqi Wang, Yujiu Yang, Yan Teng, Yingchun Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02536">https://arxiv.org/abs/2602.02536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02536">https://arxiv.org/pdf/2602.02536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02536]] From Sparse Decisions to Dense Reasoning: A Multi-attribute Trajectory Paradigm for Multimodal Moderation(https://arxiv.org/abs/2602.02536)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Safety moderation is pivotal for identifying harmful content. Despite the success of textual safety moderation, its multimodal counterparts remain hindered by a dual sparsity of data and supervision. Conventional reliance on binary labels lead to shortcut learning, which obscures the intrinsic classification boundaries necessary for effective multimodal discrimination. Hence, we propose a novel learning paradigm (UniMod) that transitions from sparse decision-making to dense reasoning traces. By constructing structured trajectories encompassing evidence grounding, modality assessment, risk mapping, policy decision, and response generation, we reformulate monolithic decision tasks into a multi-dimensional boundary learning process. This approach forces the model to ground its decision in explicit safety semantics, preventing the model from converging on superficial shortcuts. To facilitate this paradigm, we develop a multi-head scalar reward model (UniRM). UniRM provides multi-dimensional supervision by assigning attribute-level scores to the response generation stage. Furthermore, we introduce specialized optimization strategies to decouple task-specific parameters and rebalance training dynamics, effectively resolving interference between diverse objectives in multi-task learning. Empirical results show UniMod achieves competitive textual moderation performance and sets a new multimodal benchmark using less than 40\% of the training data used by leading baselines. Ablations further validate our multi-attribute trajectory reasoning, offering an effective and efficient framework for multimodal moderation. Supplementary materials are available at \href{this https URL}{project website}.</li>
<li><strong>摘要：</strong>安全审核对于识别有害内容至关重要。尽管文本安全审核取得了成功，但其多模式同行仍然受到数据和监管的双重稀疏性的阻碍。传统上对二元标签的依赖会导致捷径学习，从而模糊了有效多模态区分所需的内在分类边界。因此，我们提出了一种新颖的学习范式（UniMod），从稀疏决策过渡到密集推理轨迹。通过构建包含证据基础、模态评估、风险映射、政策决策和响应生成的结构化轨迹，我们将整体决策任务重新表述为多维边界学习过程。这种方法迫使模型将其决策建立在显式安全语义的基础上，从而防止模型收敛于肤浅的捷径。为了促进这种范例，我们开发了一个多头标量奖励模型（UniRM）。 UniRM 通过向响应生成阶段分配属性级别分数来提供多维监督。此外，我们引入专门的优化策略来解耦特定于任务的参数并重新平衡训练动态，有效解决多任务学习中不同目标之间的干扰。实证结果表明，UniMod 实现了具有竞争力的文本审核性能，并使用领先基线使用的不到 40% 的训练数据设定了新的多模式基准。消融进一步验证了我们的多属性轨迹推理，为多模式调节提供了有效且高效的框架。补充材料可在 \href{此 https URL}{项目网站} 获取。</li>
</ul>

<h3>Title: WorldVQA: Measuring Atomic World Knowledge in Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Runjie Zhou, Youbo Shao, Haoyu Lu, Bowei Xing, Tongtong Bai, Yujie Chen, Jie Zhao, Lin Sui, Haotian Yao, Zijia Zhao, Hao Yang, Haoning Wu, Zaida Zhou, Jinguo Zhu, Zhiqi Huang, Yiping Bao, Yangyang Liu, Y.Charles, Xinyu Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02537">https://arxiv.org/abs/2602.02537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02537">https://arxiv.org/pdf/2602.02537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02537]] WorldVQA: Measuring Atomic World Knowledge in Multimodal Large Language Models(https://arxiv.org/abs/2602.02537)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce WorldVQA, a benchmark designed to evaluate the atomic visual world knowledge of Multimodal Large Language Models (MLLMs). Unlike current evaluations, which often conflate visual knowledge retrieval with reasoning, WorldVQA decouples these capabilities to strictly measure "what the model memorizes." The benchmark assesses the atomic capability of grounding and naming visual entities across a stratified taxonomy, spanning from common head-class objects to long-tail rarities. We expect WorldVQA to serve as a rigorous test for visual factuality, thereby establishing a standard for assessing the encyclopedic breadth and hallucination rates of current and next-generation frontier models.</li>
<li><strong>摘要：</strong>我们引入了 WorldVQA，这是一个旨在评估多模态大型语言模型 (MLLM) 的原子视觉世界知识的基准。与当前的评估通常将视觉知识检索与推理混为一谈不同，WorldVQA 将这些能力解耦以严格衡量“模型记住的内容”。该基准评估了分层分类法中视觉实体的基础和命名的原子能力，涵盖从常见的头类物体到长尾稀有物体。我们期望 WorldVQA 能够作为视觉真实性的严格测试，从而建立评估当前和下一代前沿模型的百科全书广度和幻觉率的标准。</li>
</ul>

<h3>Title: SPA-Cache: Singular Proxies for Adaptive Caching in Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Sun, Rong-Cheng Tu, Yifu Ding, Zhao Jin, Jingyi Liao, Yongcheng Jing, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02544">https://arxiv.org/abs/2602.02544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02544">https://arxiv.org/pdf/2602.02544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02544]] SPA-Cache: Singular Proxies for Adaptive Caching in Diffusion Language Models(https://arxiv.org/abs/2602.02544)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While Diffusion Language Models (DLMs) offer a flexible, arbitrary-order alternative to the autoregressive paradigm, their non-causal nature precludes standard KV caching, forcing costly hidden state recomputation at every decoding step. Existing DLM caching approaches reduce this cost by selective hidden state updates; however, they are still limited by (i) costly token-wise update identification heuristics and (ii) rigid, uniform budget allocation that fails to account for heterogeneous hidden state dynamics. To address these challenges, we present SPA-Cache that jointly optimizes update identification and budget allocation in DLM cache. First, we derive a low-dimensional singular proxy that enables the identification of update-critical tokens in a low-dimensional subspace, substantially reducing the overhead of update identification. Second, we introduce an adaptive strategy that allocates fewer updates to stable layers without degrading generation quality. Together, these contributions significantly improve the efficiency of DLMs, yielding up to an $8\times$ throughput improvement over vanilla decoding and a $2$--$4\times$ speedup over existing caching baselines.</li>
<li><strong>摘要：</strong>虽然扩散语言模型 (DLM) 为自回归范式提供了灵活、任意顺序的替代方案，但其非因果性质排除了标准 KV 缓存，从而在每个解码步骤中强制执行昂贵的隐藏状态重新计算。现有的 DLM 缓存方法通过选择性隐藏状态更新来降低此成本；然而，它们仍然受到以下因素的限制：(i) 昂贵的令牌更新识别启发式方法；(ii) 严格、统一的预算分配，无法考虑异构隐藏状态动态。为了应对这些挑战，我们提出了 SPA-Cache，它联合优化了 DLM 缓存中的更新识别和预算分配。首先，我们推导出一个低维奇异代理，它能够识别低维子空间中的更新关键令牌，从而大大减少更新识别的开销。其次，我们引入了一种自适应策略，可以在不降低生成质量的情况下向稳定层分配更少的更新。总之，这些贡献显着提高了 DLM 的效率，与普通解码相比，吞吐量提高了 8 美元，与现有缓存基准相比，速度提高了 2-4 美元。</li>
</ul>

<h3>Title: BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation</h3>
<ul>
<li><strong>Authors: </strong>Jingwen Xu, Yiyang Lu, Zisu Huang, Changze Lv, Xiaohua Wang, Shizheng Li, Zhibo Xu, Zhengkang Guo, Zhengyuan Wang, Muzhao Tian, Xuanjing Huang, Xiaoqing Zheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02554">https://arxiv.org/abs/2602.02554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02554">https://arxiv.org/pdf/2602.02554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02554]] BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation(https://arxiv.org/abs/2602.02554)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Training LLMs for code-related tasks typically depends on high-quality code-documentation pairs, which are costly to curate and often scarce for niche programming languages. We introduce BatCoder, a self-supervised reinforcement learning framework designed to jointly optimize code generation and documentation production. BatCoder employs a back-translation strategy: a documentation is first generated from code, and then the generated documentation is used to reconstruct the original code. The semantic similarity between the original and reconstructed code serves as an implicit reward, enabling reinforcement learning to improve the model's performance both in generating code from documentation and vice versa. This approach allows models to be trained using only code, substantially increasing the available training examples. Evaluated on HumanEval and MBPP with a 7B model, BatCoder achieved 83.5% and 81.0% pass@1, outperforming strong open-source baselines. Moreover, the framework demonstrates consistent scaling with respect to both training corpus size and model capacity.</li>
<li><strong>摘要：</strong>为代码相关任务培训法学硕士通常依赖于高质量的代码文档对，这些代码文档对的管理成本很高，而且对于利基编程语言来说通常很稀缺。我们介绍 BatCoder，这是一个自监督强化学习框架，旨在联合优化代码生成和文档生成。 BatCoder 采用反向翻译策略：首先从代码生成文档，然后使用生成的文档重建原始代码。原始代码和重构代码之间的语义相似性充当隐式奖励，使强化学习能够提高模型在从文档生成代码方面的性能，反之亦然。这种方法允许仅使用代码来训练模型，从而大大增加了可用的训练示例。使用 7B 模型对 HumanEval 和 MBPP 进行评估，BatCoder 的通过率分别为 83.5% 和 81.0%，超过了强大的开源基线。此外，该框架在训练语料库大小和模型容量方面表现出一致的扩展能力。</li>
</ul>

<h3>Title: Learning to Explore with Parameter-Space Noise: A Deep Dive into Parameter-Space Noise for Reinforcement Learning with Verifiable Rewards</h3>
<ul>
<li><strong>Authors: </strong>Bizhe Bai, Xinyue Wang, Peng Ye, Tao Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02555">https://arxiv.org/abs/2602.02555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02555">https://arxiv.org/pdf/2602.02555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02555]] Learning to Explore with Parameter-Space Noise: A Deep Dive into Parameter-Space Noise for Reinforcement Learning with Verifiable Rewards(https://arxiv.org/abs/2602.02555)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning with Verifiable Rewards (RLVR) improves LLM reasoning, yet growing evidence indicates an exploration ceiling: it often reweights existing solution traces rather than discovering new strategies, limiting gains under large sampling budgets (e.g., pass-at-256). We address this limitation with PSN-RLVR, which perturbs policy parameters before rollout generation to induce temporally consistent, trajectory-level exploration that better preserves long-horizon chain-of-thought coherence than action-space noise. To mitigate the resulting sampling-update mismatch, we incorporate truncated importance sampling (TIS). To avoid expensive KL-based adaptive noise control, we propose a computationally efficient real-time adaptive noise scheduler driven by a lightweight surrogate that combines semantic diversity with normalized self-certainty. Instantiated on GRPO, a widely used RLVR method, PSN-GRPO consistently expands the effective reasoning capability boundary across multiple mathematical reasoning benchmarks and model families, yielding higher pass-at-k under large sampling budgets and outperforming prior exploration-oriented RLVR methods (e.g., Pass-at-k-style training) while remaining orthogonal and thus composable for additional gains.</li>
<li><strong>摘要：</strong>带可验证奖励的强化学习 (RLVR) 改进了 LLM 推理，但越来越多的证据表明存在探索上限：它经常重新权衡现有解决方案轨迹而不是发现新策略，从而限制了大样本预算下的收益（例如 pass-at-256）。我们使用 PSN-RLVR 解决了这一限制，它在推出生成之前扰乱策略参数，以诱导时间一致的轨迹级探索，与动作空间噪声相比，更好地保持长视野思想链的一致性。为了减轻由此产生的采样更新不匹配，我们采用了截断重要性采样（TIS）。为了避免昂贵的基于 KL 的自适应噪声控制，我们提出了一种计算高效的实时自适应噪声调度器，由轻量级代理驱动，将语义多样性与归一化的自我确定性相结合。 PSN-GRPO 在广泛使用的 RLVR 方法 GRPO 上进行实例化，不断扩展跨多个数学推理基准和模型系列的有效推理能力​​边界，在大采样预算下产生更高的 pass-at-k 并超越先前的面向探索的 RLVR 方法（例如 Pass-at-k 式训练），同时保持正交，因此可组合以获得额外增益。</li>
</ul>

<h3>Title: Auditing Sybil: Explaining Deep Lung Cancer Risk Prediction Through Generative Interventional Attributions</h3>
<ul>
<li><strong>Authors: </strong>Bartlomiej Sobieski, Jakub Grzywaczewski, Karol Dobiczek, Mateusz Wójcik, Tomasz Bartczak, Patryk Szatkowski, Przemysław Bombiński, Matthew Tivnan, Przemyslaw Biecek</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02560">https://arxiv.org/abs/2602.02560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02560">https://arxiv.org/pdf/2602.02560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02560]] Auditing Sybil: Explaining Deep Lung Cancer Risk Prediction Through Generative Interventional Attributions(https://arxiv.org/abs/2602.02560)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Lung cancer remains the leading cause of cancer mortality, driving the development of automated screening tools to alleviate radiologist workload. Standing at the frontier of this effort is Sybil, a deep learning model capable of predicting future risk solely from computed tomography (CT) with high precision. However, despite extensive clinical validation, current assessments rely purely on observational metrics. This correlation-based approach overlooks the model's actual reasoning mechanism, necessitating a shift to causal verification to ensure robust decision-making before clinical deployment. We propose S(H)NAP, a model-agnostic auditing framework that constructs generative interventional attributions validated by expert radiologists. By leveraging realistic 3D diffusion bridge modeling to systematically modify anatomical features, our approach isolates object-specific causal contributions to the risk score. Providing the first interventional audit of Sybil, we demonstrate that while the model often exhibits behavior akin to an expert radiologist, differentiating malignant pulmonary nodules from benign ones, it suffers from critical failure modes, including dangerous sensitivity to clinically unjustified artifacts and a distinct radial bias.</li>
<li><strong>摘要：</strong>肺癌仍然是癌症死亡的主要原因，推动了自动筛查工具的开发，以减轻放射科医生的工作量。 Sybil 是这项工作的前沿，它是一种深度学习模型，能够仅通过计算机断层扫描 (CT) 高精度预测未来风险。然而，尽管进行了广泛的临床验证，当前的评估仍然完全依赖于观察指标。这种基于相关性的方法忽视了模型的实际推理机制，需要转向因果验证，以确保在临床部署之前做出稳健的决策。我们提出了 S(H)NAP，这是一种与模型无关的审计框架，它构建了由放射科医生专家验证的生成干预归因。通过利用真实的 3D 扩散桥建模来系统地修改解剖特征，我们的方法隔离了特定对象对风险评分的因果贡献。对 Sybil 进行首次介入审计后，我们证明，虽然该模型经常表现出类似于放射科医生的行为，区分恶性肺结节和良性结节，但它存在严重的故障模式，包括对临床不合理伪影的危险敏感性和明显的径向偏差。</li>
</ul>

<h3>Title: Trajectory Consistency for One-Step Generation on Euler Mean Flows</h3>
<ul>
<li><strong>Authors: </strong>Zhiqi Li, Yuchen Sun, Duowen Chen, Jinjin He, Bo Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02571">https://arxiv.org/abs/2602.02571</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02571">https://arxiv.org/pdf/2602.02571</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02571]] Trajectory Consistency for One-Step Generation on Euler Mean Flows(https://arxiv.org/abs/2602.02571)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We propose \emph{Euler Mean Flows (EMF)}, a flow-based generative framework for one-step and few-step generation that enforces long-range trajectory consistency with minimal sampling cost. The key idea of EMF is to replace the trajectory consistency constraint, which is difficult to supervise and optimize over long time scales, with a principled linear surrogate that enables direct data supervision for long-horizon flow-map compositions. We derive this approximation from the semigroup formulation of flow-based models and show that, under mild regularity assumptions, it faithfully approximates the original consistency objective while being substantially easier to optimize. This formulation leads to a unified, JVP-free training framework that supports both $u$-prediction and $x_1$-prediction variants, avoiding explicit Jacobian computations and significantly reducing memory and computational overhead. Experiments on image synthesis, particle-based geometry generation, and functional generation demonstrate improved optimization stability and sample quality under fixed sampling budgets, together with approximately $50\%$ reductions in training time and memory consumption compared to existing one-step methods for image generation.</li>
<li><strong>摘要：</strong>我们提出了 \emph{Euler Mean Flows (EMF)}，一种基于流的生成框架，用于一步和少步生成，以最小的采样成本强制实现远程轨迹一致性。 EMF 的关键思想是用原则性的线性代理取代难以在长时间尺度上监督和优化的轨迹一致性约束，从而能够对长范围流图组合进行直接数据监督。我们从基于流的模型的半群公式中推导出这种近似，并表明，在温和的规律性假设下，它忠实地近似原始一致性目标，同时更容易优化。这种公式形成了一个统一的、无 JVP 的训练框架，支持 $u$ 预测和 $x_1$ 预测变体，避免显式雅可比计算并显着减少内存和计算开销。图像合成、基于粒子的几何生成和函数生成的实验表明，与现有的一步式图像生成方法相比，在固定采样预算下，优化稳定性和样本质量得到了改善，并且训练时间和内存消耗减少了约 50\%$。</li>
</ul>

<h3>Title: Copula-Based Aggregation and Context-Aware Conformal Prediction for Reliable Renewable Energy Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Alireza Moradi, Mathieu Tanneau, Reza Zandehshahvar, Pascal Van Hentenryck</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02583">https://arxiv.org/abs/2602.02583</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02583">https://arxiv.org/pdf/2602.02583</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02583]] Copula-Based Aggregation and Context-Aware Conformal Prediction for Reliable Renewable Energy Forecasting(https://arxiv.org/abs/2602.02583)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The rapid growth of renewable energy penetration has intensified the need for reliable probabilistic forecasts to support grid operations at aggregated (fleet or system) levels. In practice, however, system operators often lack access to fleet-level probabilistic models and instead rely on site-level forecasts produced by heterogeneous third-party providers. Constructing coherent and calibrated fleet-level probabilistic forecasts from such inputs remains challenging due to complex cross-site dependencies and aggregation-induced miscalibration. This paper proposes a calibrated probabilistic aggregation framework that directly converts site-level probabilistic forecasts into reliable fleet-level forecasts in settings where system-level models cannot be trained or maintained. The framework integrates copula-based dependence modeling to capture cross-site correlations with Context-Aware Conformal Prediction (CACP) to correct miscalibration at the aggregated level. This combination enables dependence-aware aggregation while providing valid coverage and maintaining sharp prediction intervals. Experiments on large-scale solar generation datasets from MISO, ERCOT, and SPP demonstrate that the proposed Copula+CACP approach consistently achieves near-nominal coverage with significantly sharper intervals than uncalibrated aggregation baselines.</li>
<li><strong>摘要：</strong>可再生能源渗透率的快速增长加剧了对可靠概率预测的需求，以支持总体（舰队或系统）层面的电网运营。然而，在实践中，系统运营商通常无法访问车队级概率模型，而是依赖异构第三方提供商生成的站点级预测。由于复杂的跨站点依赖性和聚合引起的校准错误，根据此类输入构建连贯且经过校准的机队级概率预测仍然具有挑战性。本文提出了一种经过校准的概率聚合框架，该框架可以在无法训练或维护系统级模型的情况下，直接将站点级概率预测转换为可靠的车队级预测。该框架集成了基于 copula 的依赖性建模，以捕获跨站点相关性和上下文感知保形预测 (CACP)，以纠正聚合级别的错误校准。这种组合可以实现依赖性感知聚合，同时提供有效的覆盖范围并保持清晰的预测间隔。对来自 MISO、ERCOT 和 SPP 的大规模太阳能发电数据集的实验表明，所提出的 Copula+CACP 方法始终能够实现接近标称的覆盖范围，且间隔比未校准的聚合基线明显更窄。</li>
</ul>

<h3>Title: Step-Wise Refusal Dynamics in Autoregressive and Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Eliron Rahimi, Elad Hirshel, Rom Himelstein, Amit LeVi, Avi Mendelson, Chaim Baskin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02600">https://arxiv.org/abs/2602.02600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02600">https://arxiv.org/pdf/2602.02600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02600]] Step-Wise Refusal Dynamics in Autoregressive and Diffusion Language Models(https://arxiv.org/abs/2602.02600)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion language models (DLMs) have recently emerged as a promising alternative to autoregressive (AR) models, offering parallel decoding and controllable sampling dynamics while achieving competitive generation quality at scale. Despite this progress, the role of sampling mechanisms in shaping refusal behavior and jailbreak robustness remains poorly understood. In this work, we present a fundamental analytical framework for step-wise refusal dynamics, enabling comparison between AR and diffusion sampling. Our analysis reveals that the sampling strategy itself plays a central role in safety behavior, as a factor distinct from the underlying learned representations. Motivated by this analysis, we introduce the Step-Wise Refusal Internal Dynamics (SRI) signal, which supports interpretability and improved safety for both AR and DLMs. We demonstrate that the geometric structure of SRI captures internal recovery dynamics, and identifies anomalous behavior in harmful generations as cases of \emph{incomplete internal recovery} that are not observable at the text level. This structure enables lightweight inference-time detectors that generalize to unseen attacks while matching or outperforming existing defenses with over $100\times$ lower inference overhead.</li>
<li><strong>摘要：</strong>扩散语言模型 (DLM) 最近已成为自回归 (AR) 模型的有前景的替代方案，提供并行解码和可控采样动态，同时实现大规模的有竞争力的生成质量。尽管取得了这些进展，但采样机制在塑造拒绝行为和越狱鲁棒性方面的作用仍然知之甚少。在这项工作中，我们提出了逐步拒绝动力学的基本分析框架，可以对 AR 和扩散采样进行比较。我们的分析表明，采样策略本身在安全行为中发挥着核心作用，作为一个不同于底层学习表征的因素。受此分析的启发，我们引入了逐步拒绝内部动力学 (SRI) 信号，该信号支持 AR 和 DLM 的可解释性并提高安全性。我们证明了 SRI 的几何结构捕获了内部恢复动态，并将有害世代中的异常行为识别为在文本级别无法观察到的 \emph{不完全内部恢复} 的情况。这种结构使得轻量级推理时间检测器能够泛化到未见过的攻击，同时匹配或超越现有防御，推理开销降低超过 100 倍。</li>
</ul>

<h3>Title: Expert-Data Alignment Governs Generation Quality in Decentralized Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Marcos Villagra, Bidhan Roy, Raihan Seraj, Zhiying Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02685">https://arxiv.org/abs/2602.02685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02685">https://arxiv.org/pdf/2602.02685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02685]] Expert-Data Alignment Governs Generation Quality in Decentralized Diffusion Models(https://arxiv.org/abs/2602.02685)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Decentralized Diffusion Models (DDMs) route denoising through experts trained independently on disjoint data clusters, which can strongly disagree in their predictions. What governs the quality of generations in such systems? We present the first ever systematic investigation of this question. A priori, the expectation is that minimizing denoising trajectory sensitivity -- minimizing how perturbations amplify during sampling -- should govern generation quality. We demonstrate this hypothesis is incorrect: a stability-quality dissociation. Full ensemble routing, which combines all expert predictions at each step, achieves the most stable sampling dynamics and best numerical convergence while producing the worst generation quality (FID 47.9 vs. 22.6 for sparse Top-2 routing). Instead, we identify expert-data alignment as the governing principle: generation quality depends on routing inputs to experts whose training distribution covers the current denoising state. Across two distinct DDM systems, we validate expert-data alignment using (i) data-cluster distance analysis, confirming sparse routing selects experts with data clusters closest to the current denoising state, and (ii) per-expert analysis, showing selected experts produce more accurate predictions than non-selected ones, and (iii) expert disagreement analysis, showing quality degrades when experts disagree. For DDM deployment, our findings establish that routing should prioritize expert-data alignment over numerical stability metrics.</li>
<li><strong>摘要：</strong>去中心化扩散模型 (DDM) 通过在不相交的数据集群上独立训练的专家进行去噪，这些专家的预测可能会存在强烈分歧。在这样的系统中，是什么决定了世代的质量？我们首次对这个问题进行系统的研究。先验的是，期望最小化去噪轨迹敏感性——最小化采样过程中扰动的放大方式——应该控制生成质量。我们证明这个假设是错误的：稳定性与质量的分离。完整集成路由结合了每一步的所有专家预测，实现了最稳定的采样动态和最佳数值收敛，同时产生最差的生成质量（FID 47.9 vs. 稀疏 Top-2 路由的 22.6）。相反，我们将专家数据对齐视为控制原则：生成质量取决于将输入路由到训练分布覆盖当前去噪状态的专家。在两个不同的 DDM 系统中，我们使用以下方法验证专家数据对齐：(i) 数据簇距离分析，确认稀疏路由选择数据簇最接近当前去噪状态的专家；(ii) 每个专家分析，显示选定的专家比未选定的专家产生更准确的预测；(iii) 专家分歧分析，显示当专家不同意时质量会下降。对于 DDM 部署，我们的研究结果表明，路由应优先考虑专家数据对齐而不是数值稳定性指标。</li>
</ul>

<h3>Title: Sparsely Supervised Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Wenshuai Zhao, Zhiyuan Li, Yi Zhao, Mohammad Hassan Vali, Martin Trapp, Joni Pajarinen, Juho Kannala, Arno Solin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02699">https://arxiv.org/abs/2602.02699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02699">https://arxiv.org/pdf/2602.02699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02699]] Sparsely Supervised Diffusion(https://arxiv.org/abs/2602.02699)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have shown remarkable success across a wide range of generative tasks. However, they often suffer from spatially inconsistent generation, arguably due to the inherent locality of their denoising mechanisms. This can yield samples that are locally plausible but globally inconsistent. To mitigate this issue, we propose sparsely supervised learning for diffusion models, a simple yet effective masking strategy that can be implemented with only a few lines of code. Interestingly, the experiments show that it is safe to mask up to 98\% of pixels during diffusion model training. Our method delivers competitive FID scores across experiments and, most importantly, avoids training instability on small datasets. Moreover, the masking strategy reduces memorization and promotes the use of essential contextual information during generation.</li>
<li><strong>摘要：</strong>扩散模型在广泛的生成任务中取得了显着的成功。然而，它们经常遭受空间不一致的生成，这可能是由于其去噪机制固有的局部性。这可以产生局部合理但全局不一致的样本。为了缓解这个问题，我们提出了扩散模型的稀疏监督学习，这是一种简单而有效的屏蔽策略，只需几行代码即可实现。有趣的是，实验表明在扩散模型训练期间屏蔽高达 98% 的像素是安全的。我们的方法在整个实验中提供有竞争力的 FID 分数，最重要的是，避免了小数据集上训练的不稳定。此外，掩蔽策略减少了记忆并促进了生成过程中基本上下文信息的使用。</li>
</ul>

<h3>Title: Maximum Likelihood Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Fahim Tajwar, Guanning Zeng, Yueer Zhou, Yuda Song, Daman Arora, Yiding Jiang, Jeff Schneider, Ruslan Salakhutdinov, Haiwen Feng, Andrea Zanette</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02710">https://arxiv.org/abs/2602.02710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02710">https://arxiv.org/pdf/2602.02710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02710]] Maximum Likelihood Reinforcement Learning(https://arxiv.org/abs/2602.02710)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reinforcement learning is the method of choice to train models in sampling-based setups with binary outcome feedback, such as navigation, code generation, and mathematical problem solving. In such settings, models implicitly induce a likelihood over correct rollouts. However, we observe that reinforcement learning does not maximize this likelihood, and instead optimizes only a lower-order approximation. Inspired by this observation, we introduce Maximum Likelihood Reinforcement Learning (MaxRL), a sampling-based framework to approximate maximum likelihood using reinforcement learning techniques. MaxRL addresses the challenges of non-differentiable sampling by defining a compute-indexed family of sample-based objectives that interpolate between standard reinforcement learning and exact maximum likelihood as additional sampling compute is allocated. The resulting objectives admit a simple, unbiased policy-gradient estimator and converge to maximum likelihood optimization in the infinite-compute limit. Empirically, we show that MaxRL Pareto-dominates existing methods in all models and tasks we tested, achieving up to 20x test-time scaling efficiency gains compared to its GRPO-trained counterpart. We also observe MaxRL to scale better with additional data and compute. Our results suggest MaxRL is a promising framework for scaling RL training in correctness based settings.</li>
<li><strong>摘要：</strong>强化学习是在具有二元结果反馈的基于采样的设置中训练模型的方法，例如导航、代码生成和数学问题解决。在这种情况下，模型隐含地诱导了正确推出的可能性。然而，我们观察到强化学习并没有最大化这种可能性，而是仅优化低阶近似。受这一观察的启发，我们引入了最大似然强化学习（MaxRL），这是一种基于采样的框架，使用强化学习技术来近似最大似然。 MaxRL 通过定义基于样本的目标的计算索引族来解决不可微采样的挑战，这些目标在分配额外的采样计算时在标准强化学习和精确最大似然之间进行插值。由此产生的目标允许一个简单、无偏的策略梯度估计器，并在无限计算限制下收敛到最大似然优化。根据经验，我们表明 MaxRL Pareto 在我们测试的所有模型和任务中都占主导地位，与 GRPO 训练的对应方法相比，测试时间扩展效率提高了 20 倍。我们还观察到 MaxRL 通过额外的数据和计算可以更好地扩展。我们的结果表明 MaxRL 是一个很有前途的框架，可以在基于正确性的设置中扩展 RL 训练。</li>
</ul>

<h3>Title: Hierarchical Entity-centric Reinforcement Learning with Factored Subgoal Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Dan Haramati, Carl Qi, Tal Daniel, Amy Zhang, Aviv Tamar, George Konidaris</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02722">https://arxiv.org/abs/2602.02722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02722">https://arxiv.org/pdf/2602.02722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02722]] Hierarchical Entity-centric Reinforcement Learning with Factored Subgoal Diffusion(https://arxiv.org/abs/2602.02722)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We propose a hierarchical entity-centric framework for offline Goal-Conditioned Reinforcement Learning (GCRL) that combines subgoal decomposition with factored structure to solve long-horizon tasks in domains with multiple entities. Achieving long-horizon goals in complex environments remains a core challenge in Reinforcement Learning (RL). Domains with multiple entities are particularly difficult due to their combinatorial complexity. GCRL facilitates generalization across goals and the use of subgoal structure, but struggles with high-dimensional observations and combinatorial state-spaces, especially under sparse reward. We employ a two-level hierarchy composed of a value-based GCRL agent and a factored subgoal-generating conditional diffusion model. The RL agent and subgoal generator are trained independently and composed post hoc through selective subgoal generation based on the value function, making the approach modular and compatible with existing GCRL algorithms. We introduce new variations to benchmark tasks that highlight the challenges of multi-entity domains, and show that our method consistently boosts performance of the underlying RL agent on image-based long-horizon tasks with sparse rewards, achieving over 150% higher success rates on the hardest task in our suite and generalizing to increasing horizons and numbers of entities. Rollout videos are provided at: this https URL</li>
<li><strong>摘要：</strong>我们提出了一种以实体为中心的分层框架，用于离线目标条件强化学习（GCRL），它将子目标分解与分解结构相结合，以解决具有多个实体的领域中的长期任务。在复杂环境中实现长期目标仍然是强化学习（RL）的核心挑战。具有多个实体的域由于其组合复杂性而特别困难。 GCRL 促进了跨目标的泛化和子目标结构的使用，但在高维观察和组合状态空间方面存在困难，尤其是在奖励稀疏的情况下。我们采用由基于价值的 GCRL 代理和因子子目标生成条件扩散模型组成的两级层次结构。 RL 代理和子目标生成器独立训练，并通过基于价值函数的选择性子目标生成事后组合，使该方法模块化并与现有 GCRL 算法兼容。我们为基准任务引入了新的变体，突出了多实体领域的挑战，并表明我们的方法持续提高了底层 RL 代理在基于图像的长视野任务上的性能，奖励稀疏，在我们套件中最困难的任务上实现了超过 150% 的成功率，并推广到不断增加的视野和实体数量。推出视频位于：此 https URL</li>
</ul>

<h3>Title: Search-Augmented Masked Diffusion Models for Constrained Generation</h3>
<ul>
<li><strong>Authors: </strong>Huu Binh Ta (1), Michael Cardei (1), Alvaro Velasquez (2), Ferdinando Fioretto (1) ((1) University of Virginia, (2) University of Colorado at Boulder)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02727">https://arxiv.org/abs/2602.02727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02727">https://arxiv.org/pdf/2602.02727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02727]] Search-Augmented Masked Diffusion Models for Constrained Generation(https://arxiv.org/abs/2602.02727)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Discrete diffusion models generate sequences by iteratively denoising samples corrupted by categorical noise, offering an appealing alternative to autoregressive decoding for structured and symbolic generation. However, standard training targets a likelihood-based objective that primarily matches the data distribution and provides no native mechanism for enforcing hard constraints or optimizing non-differentiable properties at inference time. This work addresses this limitation and introduces Search-Augmented Masked Diffusion (SearchDiff), a training-free neurosymbolic inference framework that integrates informed search directly into the reverse denoising process. At each denoising step, the model predictions define a proposal set that is optimized under a user-specified property satisfaction, yielding a modified reverse transition that steers sampling toward probable and feasible solutions. Experiments in biological design and symbolic reasoning illustrate that SearchDiff substantially improves constraint satisfaction and property adherence, while consistently outperforming discrete diffusion and autoregressive baselines.</li>
<li><strong>摘要：</strong>离散扩散模型通过迭代地对被分类噪声损坏的样本进行去噪来生成序列，为结构化和符号生成提供了自回归解码的有吸引力的替代方案。然而，标准训练的目标是基于可能性的目标，该目标主要匹配数据分布，并且不提供用于在推理时强制执行硬约束或优化不可微属性的本机机制。这项工作解决了这一限制，并引入了搜索增强掩模扩散（SearchDiff），这是一种免训练的神经符号推理框架，它将知情搜索直接集成到反向去噪过程中。在每个去噪步骤中，模型预测定义一个建议集，该建议集在用户指定的属性满意度下进行优化，产生修改后的反向转换，将采样引导至可能且可行的解决方案。生物设计和符号推理实验表明，SearchDiff 显着提高了约束满足度和属性遵守度，同时始终优于离散扩散和自回归基线。</li>
</ul>

<h3>Title: TabPFN for Zero-shot Parametric Engineering Design Generation</h3>
<ul>
<li><strong>Authors: </strong>Ke Wang, Yifan Tang, Nguyen Gia Hien Vu, Faez Ahmed, G. Gary Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02735">https://arxiv.org/abs/2602.02735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02735">https://arxiv.org/pdf/2602.02735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02735]] TabPFN for Zero-shot Parametric Engineering Design Generation(https://arxiv.org/abs/2602.02735)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Deep generative models for engineering design often require substantial computational cost, large training datasets, and extensive retraining when design requirements or datasets change, limiting their applicability in real-world engineering design workflow. In this work, we propose a zero-shot generation framework for parametric engineering design based on TabPFN, enabling conditional design generation using only a limited number of reference samples and without any task-specific model training or fine-tuning. The proposed method generates design parameters sequentially conditioned on target performance indicators, providing a flexible alternative to conventional generative models. The effectiveness of the proposed approach is evaluated on three engineering design datasets, i.e., ship hull design, BlendedNet aircraft, and UIUC airfoil. Experimental results demonstrate that the proposed method achieves competitive diversity across highly structured parametric design spaces, remains robust to variations in sampling, resolution and parameter dimensionality of geometry generation, and achieves a low performance error (e.g., less than 2% in generated ship hull designs' performance). Compared with diffusion-based generative models, the proposed framework significantly reduces computational overhead and data requirements while preserving reliable generation performance. These results highlight the potential of zero-shot, data-efficient generation as a practical and efficient tool for engineering design, enabling rapid deployment, flexible adaptation to new design settings, and ease of integration into real-world engineering workflows.</li>
<li><strong>摘要：</strong>用于工程设计的深度生成模型通常需要大量的计算成本、大量的训练数据集，以及当设计要求或数据集发生变化时进行广泛的再训练，从而限制了它们在现实工程设计工作流程中的适用性。在这项工作中，我们提出了一种基于 TabPFN 的参数化工程设计零样本生成框架，仅使用有限数量的参考样本即可生成条件设计，并且无需任何特定于任务的模型训练或微调。所提出的方法根据目标性能指标依次生成设计参数，为传统生成模型提供了灵活的替代方案。该方法的有效性在三个工程设计数据集（即船体设计、BlishedNet 飞机和 UIUC 翼型）上进行了评估。实验结果表明，所提出的方法在高度结构化的参数设计空间中实现了竞争多样性，对几何生成的采样、分辨率和参数维数的变化保持鲁棒性，并实现了较低的性能误差（例如，生成的船体设计性能小于 2%）。与基于扩散的生成模型相比，所提出的框架显着减少了计算开销和数据需求，同时保持可靠的生成性能。这些结果凸显了零样本、数据高效生成作为工程设计实用且高效工具的潜力，可实现快速部署、灵活适应新的设计设置，并易于集成到现实世界的工程工作流程中。</li>
</ul>

<h3>Title: Privately Fine-Tuned LLMs Preserve Temporal Dynamics in Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Lucas Rosenblatt, Peihan Liu, Ryan McKenna, Natalia Ponomareva</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02766">https://arxiv.org/abs/2602.02766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02766">https://arxiv.org/pdf/2602.02766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02766]] Privately Fine-Tuned LLMs Preserve Temporal Dynamics in Tabular Data(https://arxiv.org/abs/2602.02766)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Research on differentially private synthetic tabular data has largely focused on independent and identically distributed rows where each record corresponds to a unique individual. This perspective neglects the temporal complexity in longitudinal datasets, such as electronic health records, where a user contributes an entire (sub) table of sequential events. While practitioners might attempt to model such data by flattening user histories into high-dimensional vectors for use with standard marginal-based mechanisms, we demonstrate that this strategy is insufficient. Flattening fails to preserve temporal coherence even when it maintains valid marginal distributions. We introduce PATH, a novel generative framework that treats the full table as the unit of synthesis and leverages the autoregressive capabilities of privately fine-tuned large language models. Extensive evaluations show that PATH effectively captures long-range dependencies that traditional methods miss. Empirically, our method reduces the distributional distance to real trajectories by over 60% and reduces state transition errors by nearly 50% compared to leading marginal mechanisms while achieving similar marginal fidelity.</li>
<li><strong>摘要：</strong>对差异私有合成表格数据的研究主要集中在独立且同分布的行上，其中每个记录对应于一个唯一的个体。这种观点忽略了纵向数据集的时间复杂性，例如电子健康记录，其中用户贡献了顺序事件的整个（子）表。虽然从业者可能尝试通过将用户历史扁平化为高维向量以与标准的基于边际的机制一起使用来对此类数据进行建模，但我们证明这种策略是不够的。即使保持有效的边缘分布，扁平化也无法保持时间一致性。我们引入了 PATH，一种新颖的生成框架，它将整个表视为综合单元，并利用私人微调的大型语言模型的自回归功能。广泛的评估表明，PATH 有效地捕获了传统方法遗漏的远程依赖关系。根据经验，与领先的边际机制相比，我们的方法将到真实轨迹的分布距离减少了 60% 以上，并将状态转换误差减少了近 50%，同时实现了类似的边际保真度。</li>
</ul>

<h3>Title: From Tokens to Numbers: Continuous Number Modeling for SVG Generation</h3>
<ul>
<li><strong>Authors: </strong>Michael Ogezi, Martin Bell, Freda Shi, Ethan Smith</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02820">https://arxiv.org/abs/2602.02820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02820">https://arxiv.org/pdf/2602.02820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02820]] From Tokens to Numbers: Continuous Number Modeling for SVG Generation(https://arxiv.org/abs/2602.02820)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>For certain image generation tasks, vector graphics such as Scalable Vector Graphics (SVGs) offer clear benefits such as increased flexibility, size efficiency, and editing ease, but remain less explored than raster-based approaches. A core challenge is that the numerical, geometric parameters, which make up a large proportion of SVGs, are inefficiently encoded as long sequences of tokens. This slows training, reduces accuracy, and hurts generalization. To address these problems, we propose Continuous Number Modeling (CNM), an approach that directly models numbers as first-class, continuous values rather than discrete tokens. This formulation restores the mathematical elegance of the representation by aligning the model's inputs with the data's continuous nature, removing discretization artifacts introduced by token-based encoding. We then train a multimodal transformer on 2 million raster-to-SVG samples, followed by fine-tuning via reinforcement learning using perceptual feedback to further improve visual quality. Our approach improves training speed by over 30% while maintaining higher perceptual fidelity compared to alternative approaches. This work establishes CNM as a practical and efficient approach for high-quality vector generation, with potential for broader applications. We make our code available this http URL.</li>
<li><strong>摘要：</strong>对于某些图像生成任务，可缩放矢量图形 (SVG) 等矢量图形具有明显的优势，例如提高灵活性、尺寸效率和编辑简便性，但与基于光栅的方法相比，其探索较少。一个核心挑战是，构成 SVG 很大一部分的数字、几何参数被低效地编码为长令牌序列。这会减慢训练速度、降低准确性并损害泛化能力。为了解决这些问题，我们提出了连续数字建模（CNM），这是一种直接将数字建模为一流的连续值而不是离散标记的方法。该公式通过将模型的输入与数据的连续性质对齐，消除基于标记的编码引入的离散化伪影，恢复了表示的数学优雅性。然后，我们在 200 万个光栅到 SVG 样本上训练多模态转换器，然后使用感知反馈通过强化学习进行微调，以进一步提高视觉质量。与其他方法相比，我们的方法将训练速度提高了 30% 以上，同时保持了更高的感知保真度。这项工作使 CNM 成为一种实用且高效的高质量矢量生成方法，具有更广泛的应用潜力。我们通过这个 http URL 提供我们的代码。</li>
</ul>

<h3>Title: Semantics-Aware Generative Latent Data Augmentation for Learning in Low-Resource Domains</h3>
<ul>
<li><strong>Authors: </strong>Jae-Sung Bae, Minje Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02841">https://arxiv.org/abs/2602.02841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02841">https://arxiv.org/pdf/2602.02841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02841]] Semantics-Aware Generative Latent Data Augmentation for Learning in Low-Resource Domains(https://arxiv.org/abs/2602.02841)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Despite strong performance in data-rich regimes, deep learning often underperforms in the data-scarce settings common in practice. While foundation models (FMs) trained on massive datasets demonstrate strong generalization by extracting general-purpose features, they can still suffer from scarce labeled data during downstream fine-tuning. To address this, we propose GeLDA, a semantics-aware generative latent data augmentation framework that leverages conditional diffusion models to synthesize samples in an FM-induced latent space. Because this space is low-dimensional and concentrates task-relevant information compared to the input space, GeLDA enables efficient, high-quality data generation. GeLDA conditions generation on auxiliary feature vectors that capture semantic relationships among classes or subdomains, facilitating data augmentation in low-resource domains. We validate GeLDA in two large-scale recognition tasks: (a) in zero-shot language-specific speech emotion recognition, GeLDA improves the Whisper-large baseline's unweighted average recall by 6.13%; and (b) in long-tailed image classification, it achieves 74.7% tail-class accuracy on ImageNet-LT, setting a new state-of-the-art result.</li>
<li><strong>摘要：</strong>尽管深度学习在数据丰富的情况下表现强劲，但在实践中常见的数据稀缺环境中往往表现不佳。虽然在海量数据集上训练的基础模型（FM）通过提取通用特征表现出很强的泛化能力，但它们在下游微调过程中仍然会受到标记数据稀缺的影响。为了解决这个问题，我们提出了 GeLDA，一种语义感知的生成潜在数据增强框架，利用条件扩散模型在 FM 诱导的潜在空间中合成样本。由于该空间是低维的，并且与输入空间相比集中了任务相关信息，因此 GeLDA 能够生成高效、高质量的数据。 GeLDA 在辅助特征向量上生成条件，捕获类或子域之间的语义关系，促进低资源域中的数据增强。我们在两个大规模识别任务中验证了 GeLDA：(a) 在零样本语言特定语音情感识别中，GeLDA 将 Whisper-large 基线的未加权平均召回率提高了 6.13%； (b) 在长尾图像分类中，它在 ImageNet-LT 上实现了 74.7% 的尾部分类准确率，创下了新的最先进结果。</li>
</ul>

<h3>Title: Recurrent Equivariant Constraint Modulation: Learning Per-Layer Symmetry Relaxation from Data</h3>
<ul>
<li><strong>Authors: </strong>Stefanos Pertigkiozoglou, Mircea Petrache, Shubhendu Trivedi, Kostas Daniilidis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02853">https://arxiv.org/abs/2602.02853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02853">https://arxiv.org/pdf/2602.02853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02853]] Recurrent Equivariant Constraint Modulation: Learning Per-Layer Symmetry Relaxation from Data(https://arxiv.org/abs/2602.02853)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Equivariant neural networks exploit underlying task symmetries to improve generalization, but strict equivariance constraints can induce more complex optimization dynamics that can hinder learning. Prior work addresses these limitations by relaxing strict equivariance during training, but typically relies on prespecified, explicit, or implicit target levels of relaxation for each network layer, which are task-dependent and costly to tune. We propose Recurrent Equivariant Constraint Modulation (RECM), a layer-wise constraint modulation mechanism that learns appropriate relaxation levels solely from the training signal and the symmetry properties of each layer's input-target distribution, without requiring any prior knowledge about the task-dependent target relaxation level. We demonstrate that under the proposed RECM update, the relaxation level of each layer provably converges to a value upper-bounded by its symmetry gap, namely the degree to which its input-target distribution deviates from exact symmetry. Consequently, layers processing symmetric distributions recover full equivariance, while those with approximate symmetries retain sufficient flexibility to learn non-symmetric solutions when warranted by the data. Empirically, RECM outperforms prior methods across diverse exact and approximate equivariant tasks, including the challenging molecular conformer generation on the GEOM-Drugs dataset.</li>
<li><strong>摘要：</strong>等变神经网络利用潜在的任务对称性来提高泛化能力，但严格的等变约束可能会导致更复杂的优化动态，从而阻碍学习。先前的工作通过在训练期间放宽严格的等方差来解决这些限制，但通常依赖于每个网络层的预先指定的、显式的或隐式的放宽目标级别，这些级别依赖于任务并且调整成本高昂。我们提出了循环等变约束调制（RECM），这是一种分层约束调制机制，仅从训练信号和每层输入目标分布的对称性属性中学习适当的松弛级别，而不需要任何有关任务相关目标松弛级别的先验知识。我们证明，在所提出的 RECM 更新下，每层的松弛水平可证明收敛到以其对称间隙为上限的值，即其输入目标分布偏离精确对称性的程度。因此，处理对称分布的层恢复完全等方差，而具有近似对称性的层则在数据保证的情况下保留足够的灵活性来学习非对称解决方案。根据经验，RECM 在各种精确和近似等变任务中都优于先前的方法，包括 GEOM-Drugs 数据集上具有挑战性的分子构象异构体生成。</li>
</ul>

<h3>Title: ViThinker: Active Vision-Language Reasoning via Dynamic Perceptual Querying</h3>
<ul>
<li><strong>Authors: </strong>Weihang You, Qingchan Zhu, David Liu, Yi Pan, Geng Yuan, Hanqi Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02873">https://arxiv.org/abs/2602.02873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02873">https://arxiv.org/pdf/2602.02873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02873]] ViThinker: Active Vision-Language Reasoning via Dynamic Perceptual Querying(https://arxiv.org/abs/2602.02873)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Chain-of-Thought (CoT) reasoning excels in language models but struggles in vision-language models due to premature visual-to-text conversion that discards continuous information such as geometry and spatial layout. While recent methods enhance CoT through static enumeration or attention-based selection, they remain passive, i.e., processing pre-computed inputs rather than actively seeking task-relevant details. Inspired by human active perception, we introduce ViThinker, a framework that enables vision-language models to autonomously generate decision (query) tokens triggering the synthesis of expert-aligned visual features on demand. ViThinker internalizes vision-expert capabilities during training, performing generative mental simulation during inference without external tool calls. Through a two-stage curriculum: first distilling frozen experts into model parameters, then learning task-driven querying via sparsity penalties, i.e., ViThinker discovers minimal sufficient perception for each reasoning step. Evaluations across vision-centric benchmarks demonstrate consistent improvements, validating that active query generation outperforms passive approaches in both perceptual grounding and reasoning accuracy.</li>
<li><strong>摘要：</strong>思想链 (CoT) 推理在语言模型中表现出色，但在视觉语言模型中表现不佳，因为过早的视觉到文本转换会丢弃几何和空间布局等连续信息。虽然最近的方法通过静态枚举或基于注意力的选择来增强 CoT，但它们仍然是被动的，即处理预先计算的输入，而不是主动寻找与任务相关的细节。受人类主动感知的启发，我们引入了 ViThinker，这是一个框架，使视觉语言模型能够自动生成决策（查询）标记，从而根据需要触发专家对齐的视觉特征的合成。 ViThinker 在训练过程中内化视觉专家的能力，在推理过程中执行生成心理模拟，无需外部工具调用。通过两阶段的课程：首先将冻结的专家提炼为模型参数，然后通过稀疏性惩罚学习任务驱动的查询，即 ViThinker 发现每个推理步骤的最小充分感知。跨以视觉为中心的基准的评估显示出一致的改进，验证了主动查询生成在感知基础和推理准确性方面均优于被动方法。</li>
</ul>

<h3>Title: A Random Matrix Theory Perspective on the Consistency of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Binxu Wang, Jacob Zavatone-Veth, Cengiz Pehlevan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02908">https://arxiv.org/abs/2602.02908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02908">https://arxiv.org/pdf/2602.02908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02908]] A Random Matrix Theory Perspective on the Consistency of Diffusion Models(https://arxiv.org/abs/2602.02908)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion models trained on different, non-overlapping subsets of a dataset often produce strikingly similar outputs when given the same noise seed. We trace this consistency to a simple linear effect: the shared Gaussian statistics across splits already predict much of the generated images. To formalize this, we develop a random matrix theory (RMT) framework that quantifies how finite datasets shape the expectation and variance of the learned denoiser and sampling map in the linear setting. For expectations, sampling variability acts as a renormalization of the noise level through a self-consistent relation $\sigma^2 \mapsto \kappa(\sigma^2)$, explaining why limited data overshrink low-variance directions and pull samples toward the dataset mean. For fluctuations, our variance formulas reveal three key factors behind cross-split disagreement: \textit{anisotropy} across eigenmodes, \textit{inhomogeneity} across inputs, and overall scaling with dataset size. Extending deterministic-equivalence tools to fractional matrix powers further allows us to analyze entire sampling trajectories. The theory sharply predicts the behavior of linear diffusion models, and we validate its predictions on UNet and DiT architectures in their non-memorization regime, identifying where and how samples deviates across training data split. This provides a principled baseline for reproducibility in diffusion training, linking spectral properties of data to the stability of generative outputs.</li>
<li><strong>摘要：</strong>当给定相同的噪声种子时，在数据集的不同、不重叠子集上训练的扩散模型通常会产生惊人相似的输出。我们将这种一致性追溯到一个简单的线性效应：跨分割的共享高斯统计数据已经预测了大部分生成的图像。为了形式化这一点，我们开发了一个随机矩阵理论（RMT）框架，该框架量化有限数据集如何在线性设置中塑造学习降噪器和采样图的期望和方差。对于期望，采样变异性通过自洽关系 $\sigma^2 \mapsto \kappa(\sigma^2)$ 充当噪声水平的重整化，解释了为什么有限的数据会过度收缩低方差方向并将样本拉向数据集均值。对于波动，我们的方差公式揭示了交叉分割分歧背后的三个关键因素：本征模之间的 \textit{各向异性}、输入之间的 \textit{不均匀性} 以及数据集大小的整体缩放。将确定性等价工具扩展到分数矩阵幂进一步使我们能够分析整个采样轨迹。该理论敏锐地预测了线性扩散模型的行为，并且我们在 UNet 和 DiT 架构的非记忆机制中验证了其预测，识别了样本在训练数据分割中的偏差位置和方式。这为扩散训练的可重复性提供了原则性基线，将数据的光谱特性与生成输出的稳定性联系起来。</li>
</ul>

<h3>Title: FaceLinkGen: Rethinking Identity Leakage in Privacy-Preserving Face Recognition with Identity Extraction</h3>
<ul>
<li><strong>Authors: </strong>Wenqi Guo, Shan Du</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02914">https://arxiv.org/abs/2602.02914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02914">https://arxiv.org/pdf/2602.02914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02914]] FaceLinkGen: Rethinking Identity Leakage in Privacy-Preserving Face Recognition with Identity Extraction(https://arxiv.org/abs/2602.02914)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Transformation-based privacy-preserving face recognition (PPFR) aims to verify identities while hiding facial data from attackers and malicious service providers. Existing evaluations mostly treat privacy as resistance to pixel-level reconstruction, measured by PSNR and SSIM. We show that this reconstruction-centric view fails. We present FaceLinkGen, an identity extraction attack that performs linkage/matching and face regeneration directly from protected templates without recovering original pixels. On three recent PPFR systems, FaceLinkGen reaches over 98.5\% matching accuracy and above 96\% regeneration success, and still exceeds 92\% matching and 94\% regeneration in a near zero knowledge setting. These results expose a structural gap between pixel distortion metrics, which are widely used in PPFR evaluation, and real privacy. We show that visual obfuscation leaves identity information broadly exposed to both external intruders and untrusted service providers.</li>
<li><strong>摘要：</strong>基于转换的隐私保护人脸识别（PPFR）旨在验证身份，同时向攻击者和恶意服务提供商隐藏面部数据。现有的评估大多将隐私视为对像素级重建的阻力，通过 PSNR 和 SSIM 来衡量。我们证明这种以重建为中心的观点是失败的。我们提出了 FaceLinkGen，这是一种身份提取攻击，可以直接从受保护的模板执行链接/匹配和面部再生，而无需恢复原始像素。在最近的三个 PPFR 系统上，FaceLinkGen 达到了超过 98.5% 的匹配准确率和超过 96% 的再生成功率，并且在接近零知识的设置中仍然超过 92% 的匹配和 94% 的再生。这些结果暴露了广泛用于 PPFR 评估的像素失真指标与真实隐私之间的结构性差距。我们表明，视觉混淆会使身份信息广泛暴露给外部入侵者和不受信任的服务提供商。</li>
</ul>

<h3>Title: How Does the Lagrangian Guide Safe Reinforcement Learning through Diffusion Models?</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyuan Cheng, Wenxuan Yuan, Boyang Li, Yuanchao Xu, Yiming Yang, Hao Liang, Bei Peng, Robert Loftin, Zhuo Sun, Yukun Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02924">https://arxiv.org/abs/2602.02924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02924">https://arxiv.org/pdf/2602.02924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02924]] How Does the Lagrangian Guide Safe Reinforcement Learning through Diffusion Models?(https://arxiv.org/abs/2602.02924)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion policy sampling enables reinforcement learning (RL) to represent multimodal action distributions beyond suboptimal unimodal Gaussian policies. However, existing diffusion-based RL methods primarily focus on offline settings for reward maximization, with limited consideration of safety in online settings. To address this gap, we propose Augmented Lagrangian-Guided Diffusion (ALGD), a novel algorithm for off-policy safe RL. By revisiting optimization theory and energy-based model, we show that the instability of primal-dual methods arises from the non-convex Lagrangian landscape. In diffusion-based safe RL, the Lagrangian can be interpreted as an energy function guiding the denoising dynamics. Counterintuitively, direct usage destabilizes both policy generation and training. ALGD resolves this issue by introducing an augmented Lagrangian that locally convexifies the energy landscape, yielding a stabilized policy generation and training process without altering the distribution of the optimal policy. Theoretical analysis and extensive experiments demonstrate that ALGD is both theoretically grounded and empirically effective, achieving strong and stable performance across diverse environments.</li>
<li><strong>摘要：</strong>扩散策略采样使强化学习 (RL) 能够表示超出次优单峰高斯策略的多峰动作分布。然而，现有的基于扩散的强化学习方法主要关注离线设置以实现奖励最大化，而对在线设置的安全性考虑有限。为了解决这一差距，我们提出了增强拉格朗日引导扩散（ALGD），这是一种用于离策略安全强化学习的新算法。通过重新审视优化理论和基于能量的模型，我们表明原对偶方法的不稳定性源于非凸拉格朗日景观。在基于扩散的安全强化学习中，拉格朗日量可以解释为指导去噪动态的能量函数。与直觉相反，直接使用会破坏政策生成和培训的稳定性。 ALGD 通过引入增强拉格朗日函数来解决这个问题，该拉格朗日函数局部凸化能源景观，从而在不改变最优策略分布的情况下产生稳定的策略生成和训练过程。理论分析和大量实验表明，ALGD 既有理论依据，又有经验有效，在不同的环境中都能实现强大而稳定的性能。</li>
</ul>

<h3>Title: Distance Marching for Generative Modeling</h3>
<ul>
<li><strong>Authors: </strong>Zimo Wang, Ishit Mehta, Haolin Lu, Chung-En Sun, Ge Yan, Tsui-Wei Weng, Tzu-Mao Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02928">https://arxiv.org/abs/2602.02928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02928">https://arxiv.org/pdf/2602.02928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02928]] Distance Marching for Generative Modeling(https://arxiv.org/abs/2602.02928)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Time-unconditional generative models learn time-independent denoising vector fields. But without time conditioning, the same noisy input may correspond to multiple noise levels and different denoising directions, which interferes with the supervision signal. Inspired by distance field modeling, we propose Distance Marching, a new time-unconditional approach with two principled inference methods. Crucially, we design losses that focus on closer targets. This yields denoising directions better directed toward the data manifold. Across architectures, Distance Marching consistently improves FID by 13.5% on CIFAR-10 and ImageNet over recent time-unconditional baselines. For class-conditional ImageNet generation, despite removing time input, Distance Marching surpasses flow matching using our losses and inference methods. It achieves lower FID than flow matching's final performance using 60% of the sampling steps and 13.6% lower FID on average across backbone sizes. Moreover, our distance prediction is also helpful for early stopping during sampling and for OOD detection. We hope distance field modeling can serve as a principled lens for generative modeling.</li>
<li><strong>摘要：</strong>时间无条件生成模型学习时间无关的去噪向量场。但如果没有时间调节，相同的噪声输入可能对应于多个噪声水平和不同的去噪方向，这会干扰监控信号。受距离场建模的启发，我们提出了距离行进，这是一种具有两种原则推理方法的新时间无条件方法。至关重要的是，我们设计的损失集中于更近的目标。这会产生更好地针对数据流形的去噪方向。在各个架构中，与最近的时间无条件基线相比，Distance Marching 在 CIFAR-10 和 ImageNet 上持续将 FID 提高了 13.5%。对于类条件 ImageNet 生成，尽管删除了时间输入，但使用我们的损失和推理方法，距离行进超越了流匹配。它使用 60% 的采样步骤实现了比流匹配的最终性能更低的 FID，并且跨主干网大小的 FID 平均降低了 13.6%。此外，我们的距离预测也有助于采样期间的早期停止和 OOD 检测。我们希望距离场建模可以作为生成建模的原则镜头。</li>
</ul>

<h3>Title: Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization</h3>
<ul>
<li><strong>Authors: </strong>Haocheng Xi, Shuo Yang, Yilong Zhao, Muyang Li, Han Cai, Xingyang Li, Yujun Lin, Zhuoyang Zhang, Jintao Zhang, Xiuyu Li, Zhiying Xu, Jun Wu, Chenfeng Xu, Ion Stoica, Song Han, Kurt Keutzer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02958">https://arxiv.org/abs/2602.02958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02958">https://arxiv.org/pdf/2602.02958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02958]] Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization(https://arxiv.org/abs/2602.02958)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite rapid progress in autoregressive video diffusion, an emerging system algorithm bottleneck limits both deployability and generation capability: KV cache memory. In autoregressive video generation models, the KV cache grows with generation history and quickly dominates GPU memory, often exceeding 30 GB, preventing deployment on widely available hardware. More critically, constrained KV cache budgets restrict the effective working memory, directly degrading long horizon consistency in identity, layout, and motion. To address this challenge, we present Quant VideoGen (QVG), a training free KV cache quantization framework for autoregressive video diffusion models. QVG leverages video spatiotemporal redundancy through Semantic Aware Smoothing, producing low magnitude, quantization friendly residuals. It further introduces Progressive Residual Quantization, a coarse to fine multi stage scheme that reduces quantization error while enabling a smooth quality memory trade off. Across LongCat Video, HY WorldPlay, and Self Forcing benchmarks, QVG establishes a new Pareto frontier between quality and memory efficiency, reducing KV cache memory by up to 7.0 times with less than 4% end to end latency overhead while consistently outperforming existing baselines in generation quality.</li>
<li><strong>摘要：</strong>尽管自回归视频传播取得了快速进展，但一个新兴的系统算法瓶颈限制了可部署性和生成能力：KV 缓存。在自回归视频生成模型中，KV 缓存随着生成历史而增长，并迅速占据 GPU 内存，通常超过 30 GB，从而阻碍了在广泛可用的硬件上的部署。更关键的是，有限的 KV 缓存预算限制了有效的工作内存，直接降低了身份、布局和运动的长期一致性。为了应对这一挑战，我们提出了 Quant VideoGen (QVG)，这是一种用于自回归视频扩散模型的免训练 KV 缓存量化框架。 QVG 通过语义感知平滑来利用视频时空冗余，产生低幅度、量化友好的残差。它还引入了渐进残差量化，这是一种从粗到细的多级方案，可减少量化误差，同时实现平滑的质量内存权衡。在 LongCat Video、HY WorldPlay 和 Self Forcing 基准测试中，QVG 在质量和内存效率之间建立了新的帕累托前沿，将 KV 缓存内存减少多达 7.0 倍，端到端延迟开销低于 4%，同时在生成质量方面始终优于现有基准。</li>
</ul>

<h3>Title: TRACE: Temporal Radiology with Anatomical Change Explanation for Grounded X-ray Report Generation</h3>
<ul>
<li><strong>Authors: </strong>OFM Riaz Rahman Aranya, Kevin Desai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02963">https://arxiv.org/abs/2602.02963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02963">https://arxiv.org/pdf/2602.02963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02963]] TRACE: Temporal Radiology with Anatomical Change Explanation for Grounded X-ray Report Generation(https://arxiv.org/abs/2602.02963)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Temporal comparison of chest X-rays is fundamental to clinical radiology, enabling detection of disease progression, treatment response, and new findings. While vision-language models have advanced single-image report generation and visual grounding, no existing method combines these capabilities for temporal change detection. We introduce Temporal Radiology with Anatomical Change Explanation (TRACE), the first model that jointly performs temporal comparison, change classification, and spatial localization. Given a prior and current chest X-ray, TRACE generates natural language descriptions of interval changes (worsened, improved, stable) while grounding each finding with bounding box coordinates. TRACE demonstrates effective spatial localization with over 90% grounding accuracy, establishing a foundation for this challenging new task. Our ablation study uncovers an emergent capability: change detection arises only when temporal comparison and spatial grounding are jointly learned, as neither alone enables meaningful change detection. This finding suggests that grounding provides a spatial attention mechanism essential for temporal reasoning.</li>
<li><strong>摘要：</strong>胸部 X 光片的时间比较是临床放射学的基础，可以检测疾病进展、治疗反应和新发现。虽然视觉语言模型具有先进的单图像报告生成和视觉基础，但现有的方法还没有将这些功能结合起来进行时间变化检测。我们引入具有解剖变化解释的时间放射学（TRACE），这是第一个联合执行时间比较、变化分类和空间定位的模型。给定之前和当前的胸部 X 光检查结果，TRACE 会生成间隔变化的自然语言描述（恶化、改善、稳定），同时以边界框坐标为每个发现奠定基础。 TRACE 展示了有效的空间定位，接地精度超过 90%，为这项具有挑战性的新任务奠定了基础。我们的消融研究揭示了一种新兴能力：仅当时间比较和空间基础共同学习时才会出现变化检测，因为单独学习两者都无法实现有意义的变化检测。这一发现表明，接地提供了一种对于时间推理至关重要的空间注意力机制。</li>
</ul>

<h3>Title: Dynamic High-frequency Convolution for Infrared Small Target Detection</h3>
<ul>
<li><strong>Authors: </strong>Ruojing Li, Chao Xiao, Qian Yin, Wei An, Nuo Chen, Xinyi Ying, Miao Li, Yingqian Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02969">https://arxiv.org/abs/2602.02969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02969">https://arxiv.org/pdf/2602.02969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02969]] Dynamic High-frequency Convolution for Infrared Small Target Detection(https://arxiv.org/abs/2602.02969)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Infrared small targets are typically tiny and locally salient, which belong to high-frequency components (HFCs) in images. Single-frame infrared small target (SIRST) detection is challenging, since there are many HFCs along with targets, such as bright corners, broken clouds, and other clutters. Current learning-based methods rely on the powerful capabilities of deep networks, but neglect explicit modeling and discriminative representation learning of various HFCs, which is important to distinguish targets from other HFCs. To address the aforementioned issues, we propose a dynamic high-frequency convolution (DHiF) to translate the discriminative modeling process into the generation of a dynamic local filter bank. Especially, DHiF is sensitive to HFCs, owing to the dynamic parameters of its generated filters being symmetrically adjusted within a zero-centered range according to Fourier transformation properties. Combining with standard convolution operations, DHiF can adaptively and dynamically process different HFC regions and capture their distinctive grayscale variation characteristics for discriminative representation learning. DHiF functions as a drop-in replacement for standard convolution and can be used in arbitrary SIRST detection networks without significant decrease in computational efficiency. To validate the effectiveness of our DHiF, we conducted extensive experiments across different SIRST detection networks on real-scene datasets. Compared to other state-of-the-art convolution operations, DHiF exhibits superior detection performance with promising improvement. Codes are available at this https URL.</li>
<li><strong>摘要：</strong>红外小目标通常微小且局部显着，属于图像中的高频成分（HFC）。单帧红外小目标 (SIRST) 检测具有挑战性，因为目标中存在许多 HFC，例如明亮的角落、破碎的云层和其他杂波。当前基于学习的方法依赖于深度网络的强大能力，但忽略了各种 HFC 的显式建模和判别表示学习，这对于区分目标与其他 HFC 很重要。为了解决上述问题，我们提出了动态高频卷积（DHiF），将判别建模过程转化为动态局部滤波器组的生成。特别是，DHiF 对 HFC 很敏感，因为其生成的滤波器的动态参数根据傅立叶变换特性在零中心范围内对称调整。结合标准卷积运算，DHiF 可以自适应地动态处理不同的 HFC 区域，并捕获其独特的灰度变化特征，用于判别表示学习。 DHiF 可以作为标准卷积的直接替代品，并且可以在任意 SIRST 检测网络中使用，而不会显着降低计算效率。为了验证 DHiF 的有效性，我们在不同的 SIRST 检测网络上对真实场景数据集进行了广泛的实验。与其他最先进的卷积运算相比，DHiF 表现出卓越的检测性能，并且有望取得显着的改进。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: SceneLinker: Compositional 3D Scene Generation via Semantic Scene Graph from RGB Sequences</h3>
<ul>
<li><strong>Authors: </strong>Seok-Young Kim, Dooyoung Kim, Woojin Cho, Hail Song, Suji Kang, Woontack Woo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02974">https://arxiv.org/abs/2602.02974</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02974">https://arxiv.org/pdf/2602.02974</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02974]] SceneLinker: Compositional 3D Scene Generation via Semantic Scene Graph from RGB Sequences(https://arxiv.org/abs/2602.02974)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce SceneLinker, a novel framework that generates compositional 3D scenes via semantic scene graph from RGB sequences. To adaptively experience Mixed Reality (MR) content based on each user's space, it is essential to generate a 3D scene that reflects the real-world layout by compactly capturing the semantic cues of the surroundings. Prior works struggled to fully capture the contextual relationship between objects or mainly focused on synthesizing diverse shapes, making it challenging to generate 3D scenes aligned with object arrangements. We address these challenges by designing a graph network with cross-check feature attention for scene graph prediction and constructing a graph-variational autoencoder (graph-VAE), which consists of a joint shape and layout block for 3D scene generation. Experiments on the 3RScan/3DSSG and SG-FRONT datasets demonstrate that our approach outperforms state-of-the-art methods in both quantitative and qualitative evaluations, even in complex indoor environments and under challenging scene graph constraints. Our work enables users to generate consistent 3D spaces from their physical environments via scene graphs, allowing them to create spatial MR content. Project page is this https URL.</li>
<li><strong>摘要：</strong>我们介绍 SceneLinker，这是一种新颖的框架，可通过 RGB 序列的语义场景图生成组合 3D 场景。为了根据每个用户的空间自适应地体验混合现实 (MR) 内容，必须通过紧凑地捕获周围环境的语义线索来生成反映现实世界布局的 3D 场景。之前的工作难以充分捕捉对象之间的上下文关系，或者主要集中于合成不同的形状，这使得生成与对象排列一致的 3D 场景变得具有挑战性。我们通过设计一个具有交叉检查特征注意力的图网络来解决这些挑战，以进行场景图预测，并构建一个图变分自动编码器（graph-VAE），该编码器由用于 3D 场景生成的联合形状和布局块组成。 3RScan/3DSSG 和 SG-FRONT 数据集上的实验表明，即使在复杂的室内环境和具有挑战性的场景图约束下，我们的方法在定量和定性评估方面都优于最先进的方法。我们的工作使用户能够通过场景图从其物理环境生成一致的 3D 空间，从而使他们能够创建空间 MR 内容。项目页面就是这个https URL。</li>
</ul>

<h3>Title: MUSE: A Multi-agent Framework for Unconstrained Story Envisioning via Closed-Loop Cognitive Orchestration</h3>
<ul>
<li><strong>Authors: </strong>Wenzhang Sun, Zhenyu Wang, Zhangchi Hu, Chunfeng Wang, Hao Li, Wei Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03028">https://arxiv.org/abs/2602.03028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03028">https://arxiv.org/pdf/2602.03028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03028]] MUSE: A Multi-agent Framework for Unconstrained Story Envisioning via Closed-Loop Cognitive Orchestration(https://arxiv.org/abs/2602.03028)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating long-form audio-visual stories from a short user prompt remains challenging due to an intent-execution gap, where high-level narrative intent must be preserved across coherent, shot-level multimodal generation over long horizons. Existing approaches typically rely on feed-forward pipelines or prompt-only refinement, which often leads to semantic drift and identity inconsistency as sequences grow longer. We address this challenge by formulating storytelling as a closed-loop constraint enforcement problem and propose MUSE, a multi-agent framework that coordinates generation through an iterative plan-execute-verify-revise loop. MUSE translates narrative intent into explicit, machine-executable controls over identity, spatial composition, and temporal continuity, and applies targeted multimodal feedback to correct violations during generation. To evaluate open-ended storytelling without ground-truth references, we introduce MUSEBench, a reference-free evaluation protocol validated by human judgments. Experiments demonstrate that MUSE substantially improves long-horizon narrative coherence, cross-modal identity consistency, and cinematic quality compared with representative baselines.</li>
<li><strong>摘要：</strong>由于意图执行差距，从简短的用户提示生成长篇视听故事仍然具有挑战性，其中必须在长期的连贯、镜头级多模式生成中保留高级叙事意图。现有的方法通常依赖于前馈管道或仅提示的细化，随着序列变长，这通常会导致语义漂移和身份不一致。我们通过将讲故事表述为闭环约束执行问题来应对这一挑战，并提出了 MUSE，这是一种多代理框架，通过迭代计划-执行-验证-修改循环来协调生成。 MUSE 将叙事意图转化为对身份、空间构成和时间连续性的明确的、机器可执行的控制，并应用有针对性的多模态反馈来纠正生成过程中的违规行为。为了在没有真实参考的情况下评估开放式故事讲述，我们引入了 MUSEBench，这是一种经过人类判断验证的无参考评估协议。实验表明，与代表性基线相比，MUSE 显着提高了长视野叙事连贯性、跨模式身份一致性和电影质量。</li>
</ul>

<h3>Title: HP-GAN: Harnessing pretrained networks for GAN improvement with FakeTwins and discriminator consistency</h3>
<ul>
<li><strong>Authors: </strong>Geonhui Son, Jeong Ryong Lee, Dosik Hwang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03039">https://arxiv.org/abs/2602.03039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03039">https://arxiv.org/pdf/2602.03039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03039]] HP-GAN: Harnessing pretrained networks for GAN improvement with FakeTwins and discriminator consistency(https://arxiv.org/abs/2602.03039)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative Adversarial Networks (GANs) have made significant progress in enhancing the quality of image synthesis. Recent methods frequently leverage pretrained networks to calculate perceptual losses or utilize pretrained feature spaces. In this paper, we extend the capabilities of pretrained networks by incorporating innovative self-supervised learning techniques and enforcing consistency between discriminators during GAN training. Our proposed method, named HP-GAN, effectively exploits neural network priors through two primary strategies: FakeTwins and discriminator consistency. FakeTwins leverages pretrained networks as encoders to compute a self-supervised loss and applies this through the generated images to train the generator, thereby enabling the generation of more diverse and high quality images. Additionally, we introduce a consistency mechanism between discriminators that evaluate feature maps extracted from Convolutional Neural Network (CNN) and Vision Transformer (ViT) feature networks. Discriminator consistency promotes coherent learning among discriminators and enhances training robustness by aligning their assessments of image quality. Our extensive evaluation across seventeen datasets-including scenarios with large, small, and limited data, and covering a variety of image domains-demonstrates that HP-GAN consistently outperforms current state-of-the-art methods in terms of Fréchet Inception Distance (FID), achieving significant improvements in image diversity and quality. Code is available at: this https URL.</li>
<li><strong>摘要：</strong>生成对抗网络（GAN）在提高图像合成质量方面取得了重大进展。最近的方法经常利用预训练的网络来计算感知损失或利用预训练的特征空间。在本文中，我们通过结合创新的自监督学习技术并在 GAN 训练期间强制判别器之间的一致性来扩展预训练网络的功能。我们提出的方法名为 HP-GAN，通过两种主要策略有效地利用神经网络先验：FakeTwins 和判别器一致性。 FakeTwins 利用预训练网络作为编码器来计算自监督损失，并将其应用到生成的图像中来训练生成器，从而能够生成更加多样化和高质量的图像。此外，我们在鉴别器之间引入了一种一致性机制，用于评估从卷积神经网络（CNN）和视觉变换器（ViT）特征网络中提取的特征图。鉴别器一致性促进鉴别器之间的连贯学习，并通过调整图像质量的评估来增强训练的稳健性。我们对 17 个数据集（包括大数据、小数据和有限数据的场景，并涵盖各种图像域）进行了广泛的评估，表明 HP-GAN 在 Fréchet 起始距离 (FID) 方面始终优于当前最先进的方法，在图像多样性和质量方面实现了显着改进。代码可在以下位置获得：此 https URL。</li>
</ul>

<h3>Title: Clarify Before You Draw: Proactive Agents for Robust Text-to-CAD Generation</h3>
<ul>
<li><strong>Authors: </strong>Bo Yuan, Zelin Zhao, Petr Molodyk, Bin Hu, Yongxin Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03045">https://arxiv.org/abs/2602.03045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03045">https://arxiv.org/pdf/2602.03045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03045]] Clarify Before You Draw: Proactive Agents for Robust Text-to-CAD Generation(https://arxiv.org/abs/2602.03045)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models have recently enabled text-to-CAD systems that synthesize parametric CAD programs (e.g., CadQuery) from natural language prompts. In practice, however, geometric descriptions can be under-specified or internally inconsistent: critical dimensions may be missing and constraints may conflict. Existing fine-tuned models tend to reactively follow user instructions and hallucinate dimensions when the text is ambiguous. To address this, we propose a proactive agentic framework for text-to-CadQuery generation, named ProCAD, that resolves specification issues before code synthesis. Our framework pairs a proactive clarifying agent, which audits the prompt and asks targeted clarification questions only when necessary to produce a self-consistent specification, with a CAD coding agent that translates the specification into an executable CadQuery program. We fine-tune the coding agent on a curated high-quality text-to-CadQuery dataset and train the clarifying agent via agentic SFT on clarification trajectories. Experiments show that proactive clarification significantly improves robustness to ambiguous prompts while keeping interaction overhead low. ProCAD outperforms frontier closed-source models, including Claude Sonnet 4.5, reducing the mean Chamfer distance by 79.9 percent and lowering the invalidity ratio from 4.8 percent to 0.9 percent. Our code and datasets will be made publicly available.</li>
<li><strong>摘要：</strong>大型语言模型最近启用了文本到 CAD 系统，可以根据自然语言提示合成参数化 CAD 程序（例如 CadQuery）。然而，在实践中，几何描述可能不明确或内部不一致：关键尺寸可能丢失并且约束可能发生冲突。当文本不明确时，现有的微调模型往往会反应性地遵循用户指令并产生尺寸幻觉。为了解决这个问题，我们提出了一个用于文本到 CadQuery 生成的主动代理框架，名为 ProCAD，它可以在代码合成之前解决规范问题。我们的框架将主动澄清代理与 CAD 编码代理配对，前者会审核提示并仅在必要时提出有针对性的澄清问题，以生成自洽的规范，后者将规范转换为可执行的 CadQuery 程序。我们在精选的高质量文本到 CadQuery 数据集上微调编码代理，并通过代理 SFT 在澄清轨迹上训练澄清代理。实验表明，主动澄清可以显着提高对模糊提示的鲁棒性，同时保持较低的交互开销。 ProCAD 的性能优于前沿闭源模型，包括 Claude Sonnet 4.5，将平均 Chamfer 距离减少了 79.9%，并将无效率从 4.8% 降低到 0.9%。我们的代码和数据集将公开。</li>
</ul>

<h3>Title: Consensus Group Relative Policy Optimization for Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Yuki Ichihara, Yuu Jinnai, Kaito Ariu, Eiji Uchibe</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03102">https://arxiv.org/abs/2602.03102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03102">https://arxiv.org/pdf/2602.03102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03102]] Consensus Group Relative Policy Optimization for Text Generation(https://arxiv.org/abs/2602.03102)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Many strong decoding methods for text generation follow a sample-and-rerank paradigm: they draw multiple candidates, score each under a utility (reward) function using consensus across samples, and return the best one. Although effective, these methods incur high computational costs during inference due to repeated sampling and scoring. Prior attempts to amortize inference-time computation typically rely on gold references, teacher labels, or curated preference data, increasing dataset construction effort and the demand for high-fidelity reward models. We propose Consensus Group Relative Policy Optimization (C-GRPO), which distills Minimum Bayes Risk (MBR) decoding into training by formulating the consensus utility as a group-relative objective within GRPO. C-GRPO requires only a utility function and policy samples, without gold references or explicit preference labels. Under ideal conditions, we show that the objective function of C-GRPO is directionally aligned with the gradient of the expected-utility objective underlying MBR decoding, leading to a convergence guarantee. Experiments on machine translation (WMT 2024) and text summarization (XSum) demonstrate that C-GRPO successfully achieves performance comparable to MBR decoding without the associated inference-time overhead, while outperforming reference-free baseline methods.</li>
<li><strong>摘要：</strong>许多用于文本生成的强大解码方法都遵循样本和重新排序范例：它们抽取多个候选者，使用跨样本的共识在效用（奖励）函数下对每个候选者进行评分，然后返回最佳候选者。尽管有效，但由于重复采样和评分，这些方法在推理过程中会产生很高的计算成本。之前摊销推理时间计算的尝试通常依赖于黄金参考、教师标签或精选偏好数据，增加了数据集构建工作量和对高保真奖励模型的需求。我们提出共识组相对策略优化（C-GRPO），它通过将共识效用制定为 GRPO 内的组相对目标，将最小贝叶斯风险（MBR）解码提炼到训练中。 C-GRPO 仅需要效用函数和策略样本，无需黄金参考或明确的偏好标签。在理想条件下，我们表明 C-GRPO 的目标函数与 MBR 解码的预期效用目标的梯度方向一致，从而保证收敛。机器翻译 (WMT 2024) 和文本摘要 (XSum) 实验表明，C-GRPO 成功实现了与 MBR 解码相当的性能，且没有相关的推理时间开销，同时优于无参考基线方法。</li>
</ul>

<h3>Title: Beyond Cropping and Rotation: Automated Evolution of Powerful Task-Specific Augmentations with Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Judah Goldfeder, Shreyes Kaliyur, Vaibhav Sourirajan, Patrick Minwan Puma, Philippe Martin Wyder, Yuhang Hu, Jiong Lin, Hod Lipson</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03123">https://arxiv.org/abs/2602.03123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03123">https://arxiv.org/pdf/2602.03123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03123]] Beyond Cropping and Rotation: Automated Evolution of Powerful Task-Specific Augmentations with Generative Models(https://arxiv.org/abs/2602.03123)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Data augmentation has long been a cornerstone for reducing overfitting in vision models, with methods like AutoAugment automating the design of task-specific augmentations. Recent advances in generative models, such as conditional diffusion and few-shot NeRFs, offer a new paradigm for data augmentation by synthesizing data with significantly greater diversity and realism. However, unlike traditional augmentations like cropping or rotation, these methods introduce substantial changes that enhance robustness but also risk degrading performance if the augmentations are poorly matched to the task. In this work, we present EvoAug, an automated augmentation learning pipeline, which leverages these generative models alongside an efficient evolutionary algorithm to learn optimal task-specific augmentations. Our pipeline introduces a novel approach to image augmentation that learns stochastic augmentation trees that hierarchically compose augmentations, enabling more structured and adaptive transformations. We demonstrate strong performance across fine-grained classification and few-shot learning tasks. Notably, our pipeline discovers augmentations that align with domain knowledge, even in low-data settings. These results highlight the potential of learned generative augmentations, unlocking new possibilities for robust model training.</li>
<li><strong>摘要：</strong>长期以来，数据增强一直是减少视觉模型过度拟合的基石，AutoAugment 等方法可以自动设计特定于任务的增强。生成模型的最新进展，例如条件扩散和少样本 NeRF，通过合成具有显着更大多样性和真实性的数据，为数据增强提供了新的范例。然而，与裁剪或旋转等传统增强不同，这些方法引入了实质性的变化，增强了鲁棒性，但如果增强与任务不匹配，也可能导致性能下降。在这项工作中，我们提出了 EvoAug，一种自动增强学习管道，它利用这些生成模型以及高效的进化算法来学习最佳的特定于任务的增强。我们的管道引入了一种新颖的图像增强方法，该方法可以学习分层组成增强的随机增强树，从而实现更加结构化和自适应的转换。我们在细粒度分类和小样本学习任务中展示了强大的性能。值得注意的是，我们的管道发现了与领域知识相一致的增强，即使在低数据环境中也是如此。这些结果凸显了学习生成增强的潜力，为稳健的模型训练释放了新的可能性。</li>
</ul>

<h3>Title: Flexible Geometric Guidance for Probabilistic Human Pose Estimation with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Francis Snelgar, Ming Xu, Stephen Gould, Liang Zheng, Akshay Asthana</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03126">https://arxiv.org/abs/2602.03126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03126">https://arxiv.org/pdf/2602.03126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03126]] Flexible Geometric Guidance for Probabilistic Human Pose Estimation with Diffusion Models(https://arxiv.org/abs/2602.03126)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>3D human pose estimation from 2D images is a challenging problem due to depth ambiguity and occlusion. Because of these challenges the task is underdetermined, where there exists multiple -- possibly infinite -- poses that are plausible given the image. Despite this, many prior works assume the existence of a deterministic mapping and estimate a single pose given an image. Furthermore, methods based on machine learning require a large amount of paired 2D-3D data to train and suffer from generalization issues to unseen scenarios. To address both of these issues, we propose a framework for pose estimation using diffusion models, which enables sampling from a probability distribution over plausible poses which are consistent with a 2D image. Our approach falls under the guidance framework for conditional generation, and guides samples from an unconditional diffusion model, trained only on 3D data, using the gradients of the heatmaps from a 2D keypoint detector. We evaluate our method on the Human 3.6M dataset under best-of-$m$ multiple hypothesis evaluation, showing state-of-the-art performance among methods which do not require paired 2D-3D data for training. We additionally evaluate the generalization ability using the MPI-INF-3DHP and 3DPW datasets and demonstrate competitive performance. Finally, we demonstrate the flexibility of our framework by using it for novel tasks including pose generation and pose completion, without the need to train bespoke conditional models. We make code available at this https URL .</li>
<li><strong>摘要：</strong>由于深度模糊和遮挡，从 2D 图像进行 3D 人体姿态估计是一个具有挑战性的问题。由于这些挑战，任务是不确定的，在给定图像的情况下，存在多个（可能是无限的）看似合理的姿势。尽管如此，许多先前的工作都假设存在确定性映射并估计给定图像的单个姿势。此外，基于机器学习的方法需要大量配对的 2D-3D 数据进行训练，并且存在对未见过的场景的泛化问题。为了解决这两个问题，我们提出了一个使用扩散模型进行姿势估计的框架，该框架可以从与 2D 图像一致的合理姿势的概率分布中进行采样。我们的方法属于条件生成的指导框架，并使用 2D 关键点检测器的热图梯度引导来自无条件扩散模型的样本，仅在 3D 数据上进行训练。我们在 best-of-$m$ 多重假设评估下在人类 3.6M 数据集上评估我们的方法，显示了不需要配对 2D-3D 数据进行训练的方法中最先进的性能。我们还使用 MPI-INF-3DHP 和 3DPW 数据集评估泛化能力，并展示竞争性能。最后，我们通过将框架用于姿势生成和姿势完成等新任务来展示框架的灵活性，而无需训练定制的条件模型。我们在此 https URL 提供代码。</li>
</ul>

<h3>Title: Enhanced Parcel Arrival Forecasting for Logistic Hubs: An Ensemble Deep Learning Approach</h3>
<ul>
<li><strong>Authors: </strong>Xinyue Pan, Yujia Xu, Benoit Montreuil</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03135">https://arxiv.org/abs/2602.03135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03135">https://arxiv.org/pdf/2602.03135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03135]] Enhanced Parcel Arrival Forecasting for Logistic Hubs: An Ensemble Deep Learning Approach(https://arxiv.org/abs/2602.03135)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The rapid expansion of online shopping has increased the demand for timely parcel delivery, compelling logistics service providers to enhance the efficiency, agility, and predictability of their hub networks. In order to solve the problem, we propose a novel deep learning-based ensemble framework that leverages historical arrival patterns and real-time parcel status updates to forecast upcoming workloads at logistic hubs. This approach not only facilitates the generation of short-term forecasts, but also improves the accuracy of future hub workload predictions for more strategic planning and resource management. Empirical tests of the algorithm, conducted through a case study of a major city's parcel logistics, demonstrate the ensemble method's superiority over both traditional forecasting techniques and standalone deep learning models. Our findings highlight the significant potential of this method to improve operational efficiency in logistics hubs and advocate for its broader adoption.</li>
<li><strong>摘要：</strong>在线购物的快速扩张增加了对及时包裹递送的需求，迫使物流服务提供商提高其枢纽网络的效率、敏捷性和可预测性。为了解决这个问题，我们提出了一种新颖的基于深度学习的集成框架，该框架利用历史到达模式和实时包裹状态更新来预测物流中心即将到来的工作负载。这种方法不仅有助于生成短期预测，而且还提高了未来枢纽工作负载预测的准确性，以实现更具战略性的规划和资源管理。通过对一个主要城市包裹物流的案例研究对该算法进行了实证测试，证明了集成方法相对于传统预测技术和独立深度学习模型的优越性。我们的研究结果强调了这种方法在提高物流中心运营效率方面的巨大潜力，并倡导其更广泛的采用。</li>
</ul>

<h3>Title: FSOD-VFM: Few-Shot Object Detection with Vision Foundation Models and Graph Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Chen-Bin Feng, Youyang Sha, Longfei Liu, Yongjun Yu, Chi Man Vong, Xuanlong Yu, Xi Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03137">https://arxiv.org/abs/2602.03137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03137">https://arxiv.org/pdf/2602.03137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03137]] FSOD-VFM: Few-Shot Object Detection with Vision Foundation Models and Graph Diffusion(https://arxiv.org/abs/2602.03137)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In this paper, we present FSOD-VFM: Few-Shot Object Detectors with Vision Foundation Models, a framework that leverages vision foundation models to tackle the challenge of few-shot object detection. FSOD-VFM integrates three key components: a universal proposal network (UPN) for category-agnostic bounding box generation, SAM2 for accurate mask extraction, and DINOv2 features for efficient adaptation to new object categories. Despite the strong generalization capabilities of foundation models, the bounding boxes generated by UPN often suffer from overfragmentation, covering only partial object regions and leading to numerous small, false-positive proposals rather than accurate, complete object detections. To address this issue, we introduce a novel graph-based confidence reweighting method. In our approach, predicted bounding boxes are modeled as nodes in a directed graph, with graph diffusion operations applied to propagate confidence scores across the network. This reweighting process refines the scores of proposals, assigning higher confidence to whole objects and lower confidence to local, fragmented parts. This strategy improves detection granularity and effectively reduces the occurrence of false-positive bounding box proposals. Through extensive experiments on Pascal-5$^i$, COCO-20$^i$, and CD-FSOD datasets, we demonstrate that our method substantially outperforms existing approaches, achieving superior performance without requiring additional training. Notably, on the challenging CD-FSOD dataset, which spans multiple datasets and domains, our FSOD-VFM achieves 31.6 AP in the 10-shot setting, substantially outperforming previous training-free methods that reach only 21.4 AP. Code is available at: this https URL.</li>
<li><strong>摘要：</strong>在本文中，我们提出了 FSOD-VFM：具有视觉基础模型的少样本目标检测器，这是一个利用视觉基础模型来应对少样本目标检测挑战的框架。 FSOD-VFM 集成了三个关键组件：用于生成与类别无关的边界框的通用提议网络 (UPN)、用于精确掩模提取的 SAM2 以及用于高效适应新对象类别的 DINOv2 特征。尽管基础模型具有很强的泛化能力，但 UPN 生成的边界框经常受到过度碎片化的影响，仅覆盖部分对象区域，并导致大量小的误报建议，而不是准确、完整的对象检测。为了解决这个问题，我们引入了一种新颖的基于图的置信度重新加权方法。在我们的方法中，预测的边界框被建模为有向图中的节点，并应用图扩散操作在网络中传播置信度分数。这种重新加权过程细化了提案的分数，为整个对象分配了更高的置信度，为局部、碎片化的部分分配了较低的置信度。该策略提高了检测粒度，有效减少了误报边界框提案的发生。通过对 Pascal-5$^i$、COCO-20$^i$ 和 CD-FSOD 数据集的广泛实验，我们证明我们的方法大大优于现有方法，无需额外训练即可实现卓越的性能。值得注意的是，在跨越多个数据集和领域的具有挑战性的 CD-FSOD 数据集上，我们的 FSOD-VFM 在 10 次拍摄设置中实现了 31.6 AP，大大优于以前仅达到 21.4 AP 的免训练方法。代码可在以下位置获得：此 https URL。</li>
</ul>

<h3>Title: Diversity-Preserved Distribution Matching Distillation for Fast Visual Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Tianhe Wu, Ruibin Li, Lei Zhang, Kede Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03139">https://arxiv.org/abs/2602.03139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03139">https://arxiv.org/pdf/2602.03139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03139]] Diversity-Preserved Distribution Matching Distillation for Fast Visual Synthesis(https://arxiv.org/abs/2602.03139)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Distribution matching distillation (DMD) aligns a multi-step generator with its few-step counterpart to enable high-quality generation under low inference cost. However, DMD tends to suffer from mode collapse, as its reverse-KL formulation inherently encourages mode-seeking behavior, for which existing remedies typically rely on perceptual or adversarial regularization, thereby incurring substantial computational overhead and training instability. In this work, we propose a role-separated distillation framework that explicitly disentangles the roles of distilled steps: the first step is dedicated to preserving sample diversity via a target-prediction (e.g., v-prediction) objective, while subsequent steps focus on quality refinement under the standard DMD loss, with gradients from the DMD objective blocked at the first step. We term this approach Diversity-Preserved DMD (DP-DMD), which, despite its simplicity -- no perceptual backbone, no discriminator, no auxiliary networks, and no additional ground-truth images -- preserves sample diversity while maintaining visual quality on par with state-of-the-art methods in extensive text-to-image experiments.</li>
<li><strong>摘要：</strong>分布匹配蒸馏 (DMD) 将多步生成器与其对应的少步生成器对齐，以在低推理成本下实现高质量生成。然而，DMD 往往会遭受模式崩溃的困扰，因为其反向 KL 公式本质上会鼓励模式搜索行为，而现有的补救措施通常依赖于感知或对抗性正则化，从而产生大量的计算开销和训练不稳定。在这项工作中，我们提出了一个角色分离的蒸馏框架，明确地分解了蒸馏步骤的作用：第一步致力于通过目标预测（例如，v-预测）目标来保留样本多样性，而后续步骤则专注于标准 DMD 损失下的质量细化，其中来自第一步的 DMD 目标的梯度被阻止。我们将这种方法称为多样性保留 DMD (DP-DMD)，尽管它很简单——没有感知主干，没有鉴别器，没有辅助网络，也没有额外的真实图像——但在广泛的文本到图像实验中保持了与最先进的方法相同的视觉质量，同时保留了样本多样性。</li>
</ul>

<h3>Title: BinaryDemoire: Moiré-Aware Binarization for Image Demoiréing</h3>
<ul>
<li><strong>Authors: </strong>Zheng Chen, Zhi Yang, Xiaoyang Liu, Weihang Zhang, Mengfan Wang, Yifan Fu, Linghe Kong, Yulun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03176">https://arxiv.org/abs/2602.03176</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03176">https://arxiv.org/pdf/2602.03176</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03176]] BinaryDemoire: Moiré-Aware Binarization for Image Demoiréing(https://arxiv.org/abs/2602.03176)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Image demoiréing aims to remove structured moiré artifacts in recaptured imagery, where degradations are highly frequency-dependent and vary across scales and directions. While recent deep networks achieve high-quality restoration, their full-precision designs remain costly for deployment. Binarization offers an extreme compression regime by quantizing both activations and weights to 1-bit. Yet, it has been rarely studied for demoiréing and performs poorly when naively applied. In this work, we propose BinaryDemoire, a binarized demoiréing framework that explicitly accommodates the frequency structure of moiré degradations. First, we introduce a moiré-aware binary gate (MABG) that extracts lightweight frequency descriptors together with activation statistics. It predicts channel-wise gating coefficients to condition the aggregation of binary convolution responses. Second, we design a shuffle-grouped residual adapter (SGRA) that performs structured sparse shortcut alignment. It further integrates interleaved mixing to promote information exchange across different channel partitions. Extensive experiments on four benchmarks demonstrate that the proposed BinaryDemoire surpasses current binarization methods. Code: this https URL.</li>
<li><strong>摘要：</strong>图像去波纹旨在消除重新捕获的图像中的结构化莫尔伪影，其中的退化与频率高度相关，并且随尺度和方向的不同而变化。虽然最近的深度网络实现了高质量的恢复，但其全精度设计的部署成本仍然很高。二值化通过将激活和权重量化为 1 位来提供极端的压缩机制。然而，人们很少对它进行去纹研究，并且在天真应用时表现不佳。在这项工作中，我们提出了 BinaryDemoire，一种二值化的去波纹框架，它明确地适应莫尔条纹退化的频率结构。首先，我们引入了一种莫尔感知二元门（MABG），它可以提取轻量级频率描述符和激活统计数据。它预测通道门控系数以调节二进制卷积响应的聚合。其次，我们设计了一个执行结构化稀疏快捷对齐的随机分组残差适配器（SGRA）。它进一步集成了交错混合，以促进不同通道分区之间的信息交换。对四个基准的大量实验表明，所提出的 BinaryDemoire 超越了当前的二值化方法。代码：此 https URL。</li>
</ul>

<h3>Title: LSGQuant: Layer-Sensitivity Guided Quantization for One-Step Diffusion Real-World Video Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Tianxing Wu, Zheng Chen, Cirou Xu, Bowen Chai, Yong Guo, Yutong Liu, Linghe Kong, Yulun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03182">https://arxiv.org/abs/2602.03182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03182">https://arxiv.org/pdf/2602.03182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03182]] LSGQuant: Layer-Sensitivity Guided Quantization for One-Step Diffusion Real-World Video Super-Resolution(https://arxiv.org/abs/2602.03182)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>One-Step Diffusion Models have demonstrated promising capability and fast inference in video super-resolution (VSR) for real-world. Nevertheless, the substantial model size and high computational cost of Diffusion Transformers (DiTs) limit downstream applications. While low-bit quantization is a common approach for model compression, the effectiveness of quantized models is challenged by the high dynamic range of input latent and diverse layer behaviors. To deal with these challenges, we introduce LSGQuant, a layer-sensitivity guided quantizing approach for one-step diffusion-based real-world VSR. Our method incorporates a Dynamic Range Adaptive Quantizer (DRAQ) to fit video token activations. Furthermore, we estimate layer sensitivity and implement a Variance-Oriented Layer Training Strategy (VOLTS) by analyzing layer-wise statistics in calibration. We also introduce Quantization-Aware Optimization (QAO) to jointly refine the quantized branch and a retained high-precision branch. Extensive experiments demonstrate that our method has nearly performance to origin model with full-precision and significantly exceeds existing quantization techniques. Code is available at: this https URL.</li>
<li><strong>摘要：</strong>一步扩散模型在现实世界的视频超分辨率 (VSR) 中展现出了良好的能力和快速推理能力。然而，扩散变压器（DiT）的巨大模型尺寸和高计算成本限制了下游应用。虽然低位量化是模型压缩的常见方法，但量化模型的有效性受到输入潜在和不同层行为的高动态范围的挑战。为了应对这些挑战，我们引入了 LSGQuant，一种层敏感性引导量化方法，用于基于单步扩散的现实世界 VSR。我们的方法结合了动态范围自适应量化器（DRAQ）来适应视频令牌激活。此外，我们通过分析校准中的分层统计数据来估计层敏感性并实施面向方差的层训练策略（VOLTS）。我们还引入了量化感知优化（QAO）来共同细化量化分支和保留的高精度分支。大量的实验表明，我们的方法具有接近原始模型的全精度性能，并且显着超过了现有的量化技术。代码可在以下位置获得：此 https URL。</li>
</ul>

<h3>Title: Reinforcement Learning with Promising Tokens for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jing-Cheng Pang, Liang Lu, Xian Tang, Kun Jiang, Sijie Wu, Kai Zhang, Xubin Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03195">https://arxiv.org/abs/2602.03195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03195">https://arxiv.org/pdf/2602.03195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03195]] Reinforcement Learning with Promising Tokens for Large Language Models(https://arxiv.org/abs/2602.03195)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has emerged as a key paradigm for aligning and optimizing large language models (LLMs). Standard approaches treat the LLM as the policy and apply RL directly over the full vocabulary space. However, this formulation includes the massive tail of contextually irrelevant tokens in the action space, which could distract the policy from focusing on decision-making among the truly reasonable tokens. In this work, we verify that valid reasoning paths could inherently concentrate within a low-rank subspace. Based on this insight, we introduce Reinforcement Learning with Promising Tokens (RLPT), a framework that mitigates the action space issue by decoupling strategic decision-making from token generation. Specifically, RLPT leverages the semantic priors of the base model to identify a dynamic set of \emph{promising tokens} and constrains policy optimization exclusively to this refined subset via masking. Theoretical analysis and empirical results demonstrate that RLPT effectively reduces gradient variance, stabilizes the training process, and improves sample efficiency. Experiment results on math, coding, and telecom reasoning show that RLPT outperforms standard RL baselines and integrates effectively across various model sizes (4B and 8B) and RL algorithms (GRPO and DAPO).</li>
<li><strong>摘要：</strong>强化学习 (RL) 已成为调整和优化大型语言模型 (LLM) 的关键范例。标准方法将 LLM 视为策略，并将 RL 直接应用于整个词汇空间。然而，这种表述在操作空间中包含大量与上下文无关的令牌，这可能会分散策略对真正合理令牌中决策的关注。在这项工作中，我们验证了有效的推理路径本质上可以集中在低秩子空间内。基于这一见解，我们引入了有前途代币强化学习（RLPT），这是一个通过将战略决策与代币生成分离来缓解行动空间问题的框架。具体来说，RLPT 利用基本模型的语义先验来识别一组动态的 \emph{promising tokens}，并通过屏蔽将策略优化专门限制在这个精炼的子集上。理论分析和实证结果表明，RLPT有效降低了梯度方差，稳定了训练过程，提高了样本效率。数学、编码和电信推理的实验结果表明，RLPT 的性能优于标准 RL 基线，并且可以有效集成各种模型大小（4B 和 8B）和 RL 算法（GRPO 和 DAPO）。</li>
</ul>

<h3>Title: From Single Scan to Sequential Consistency: A New Paradigm for LIDAR Relocalization</h3>
<ul>
<li><strong>Authors: </strong>Minghang Zhu, Zhijing Wang, Yuxin Guo, Wen Li, Sheng Ao, Cheng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03198">https://arxiv.org/abs/2602.03198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03198">https://arxiv.org/pdf/2602.03198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03198]] From Single Scan to Sequential Consistency: A New Paradigm for LIDAR Relocalization(https://arxiv.org/abs/2602.03198)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>LiDAR relocalization aims to estimate the global 6-DoF pose of a sensor in the environment. However, existing regression-based approaches are prone to dynamic or ambiguous scenarios, as they either solely rely on single-frame inference or neglect the spatio-temporal consistency across scans. In this paper, we propose TempLoc, a new LiDAR relocalization framework that enhances the robustness of localization by effectively modeling sequential consistency. Specifically, a Global Coordinate Estimation module is first introduced to predict point-wise global coordinates and associated uncertainties for each LiDAR scan. A Prior Coordinate Generation module is then presented to estimate inter-frame point correspondences by the attention mechanism. Lastly, an Uncertainty-Guided Coordinate Fusion module is deployed to integrate both predictions of point correspondence in an end-to-end fashion, yielding a more temporally consistent and accurate global 6-DoF pose. Experimental results on the NCLT and Oxford Robot-Car benchmarks show that our TempLoc outperforms stateof-the-art methods by a large margin, demonstrating the effectiveness of temporal-aware correspondence modeling in LiDAR relocalization. Our code will be released soon.</li>
<li><strong>摘要：</strong>LiDAR 重定位旨在估计环境中传感器的全局 6-DoF 位姿。然而，现有的基于回归的方法很容易出现动态或模糊的场景，因为它们要么仅仅依赖于单帧推理，要么忽略扫描之间的时空一致性。在本文中，我们提出了 TempLoc，一种新的 LiDAR 重定位框架，它通过有效建模顺序一致性来增强定位的鲁棒性。具体来说，首先引入全局坐标估计模块来预测每次激光雷达扫描的逐点全局坐标和相关的不确定性。然后提出先验坐标生成模块，以通过注意机制估计帧间点对应关系。最后，部署了不确定性引导坐标融合模块，以端到端的方式集成点对应的两个预测，从而产生时间上更加一致和准确的全局 6-DoF 姿态。 NCLT 和 Oxford Robot-Car 基准测试的实验结果表明，我们的 TempLoc 大幅优于最先进的方法，证明了时间感知对应建模在 LiDAR 重定位中的有效性。我们的代码很快就会发布。</li>
</ul>

<h3>Title: Spectral Evolution Search: Efficient Inference-Time Scaling for Reward-Aligned Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Jinyan Ye, Zhongjie Duan, Zhiwen Li, Cen Chen, Daoyuan Chen, Yaliang Li, Yingda Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03208">https://arxiv.org/abs/2602.03208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03208">https://arxiv.org/pdf/2602.03208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03208]] Spectral Evolution Search: Efficient Inference-Time Scaling for Reward-Aligned Image Generation(https://arxiv.org/abs/2602.03208)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Inference-time scaling offers a versatile paradigm for aligning visual generative models with downstream objectives without parameter updates. However, existing approaches that optimize the high-dimensional initial noise suffer from severe inefficiency, as many search directions exert negligible influence on the final generation. We show that this inefficiency is closely related to a spectral bias in generative dynamics: model sensitivity to initial perturbations diminishes rapidly as frequency increases. Building on this insight, we propose Spectral Evolution Search (SES), a plug-and-play framework for initial noise optimization that executes gradient-free evolutionary search within a low-frequency subspace. Theoretically, we derive the Spectral Scaling Prediction from perturbation propagation dynamics, which explains the systematic differences in the impact of perturbations across frequencies. Extensive experiments demonstrate that SES significantly advances the Pareto frontier of generation quality versus computational cost, consistently outperforming strong baselines under equivalent budgets.</li>
<li><strong>摘要：</strong>推理时间缩放提供了一种通用范例，可以将视觉生成模型与下游目标对齐，而无需更新参数。然而，优化高维初始噪声的现有方法效率严重低下，因为许多搜索方向对最终生成的影响可以忽略不计。我们表明，这种低效率与生成动力学中的谱偏差密切相关：随着频率的增加，模型对初始扰动的敏感性迅速减弱。基于这一见解，我们提出了频谱进化搜索（SES），这是一种用于初始噪声优化的即插即用框架，可在低频子空间内执行无梯度进化搜索。理论上，我们从扰动传播动力学中推导出频谱缩放预测，这解释了不同频率的扰动影响的系统差异。大量实验表明，SES 显着推进了发电质量与计算成本的帕累托前沿，在同等预算下始终优于强大的基线。</li>
</ul>

<h3>Title: VIRAL: Visual In-Context Reasoning via Analogy in Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Zhiwen Li, Zhongjie Duan, Jinyan Ye, Cen Chen, Daoyuan Chen, Yaliang Li, Yingda Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03210">https://arxiv.org/abs/2602.03210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03210">https://arxiv.org/pdf/2602.03210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03210]] VIRAL: Visual In-Context Reasoning via Analogy in Diffusion Transformers(https://arxiv.org/abs/2602.03210)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generation</a></li>
<li><strong>Abstract: </strong>Replicating In-Context Learning (ICL) in computer vision remains challenging due to task heterogeneity. We propose \textbf{VIRAL}, a framework that elicits visual reasoning from a pre-trained image editing model by formulating ICL as conditional generation via visual analogy ($x_s : x_t :: x_q : y_q$). We adapt a frozen Diffusion Transformer (DiT) using role-aware multi-image conditioning and introduce a Mixture-of-Experts LoRA to mitigate gradient interference across diverse tasks. Additionally, to bridge the gaps in current visual context datasets, we curate a large-scale dataset spanning perception, restoration, and editing. Experiments demonstrate that VIRAL outperforms existing methods, validating that a unified V-ICL paradigm can handle the majority of visual tasks, including open-domain editing. Our code is available at this https URL</li>
<li><strong>摘要：</strong>由于任务异构性，在计算机视觉中复制上下文学习（ICL）仍然具有挑战性。我们提出了 \textbf{VIRAL}，这是一个框架，通过视觉类比将 ICL 制定为条件生成（$x_s : x_t :: x_q : y_q$），从预先训练的图像编辑模型中引发视觉推理。我们使用角色感知多图像调节来适应冻结的扩散变压器 (DiT)，并引入专家混合 LoRA 来减轻不同任务之间的梯度干扰。此外，为了弥补当前视觉上下文数据集的差距，我们策划了一个涵盖感知、恢复和编辑的大规模数据集。实验表明，VIRAL 优于现有方法，验证了统一的 V-ICL 范例可以处理大多数视觉任务，包括开放域编辑。我们的代码可在此 https URL 获取</li>
</ul>

<h3>Title: Lookahead Sample Reward Guidance for Test-Time Scaling of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yeongmin Kim, Donghyeok Shin, Byeonghu Na, Minsang Park, Richard Lee Kim, Il-Chul Moon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03211">https://arxiv.org/abs/2602.03211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03211">https://arxiv.org/pdf/2602.03211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03211]] Lookahead Sample Reward Guidance for Test-Time Scaling of Diffusion Models(https://arxiv.org/abs/2602.03211)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated strong generative performance; however, generated samples often fail to fully align with human intent. This paper studies a test-time scaling method that enables sampling from regions with higher human-aligned reward values. Existing gradient guidance methods approximate the expected future reward (EFR) at an intermediate particle $\mathbf{x}_t$ using a Taylor approximation, but this approximation at each time step incurs high computational cost due to sequential neural backpropagation. We show that the EFR at any $\mathbf{x}_t$ can be computed using only marginal samples from a pre-trained diffusion model. The proposed EFR formulation detaches the neural dependency between $\mathbf{x}_t$ and the EFR, enabling closed-form guidance computation without neural backpropagation. To further improve efficiency, we introduce lookahead sampling to collect marginal samples. For final sample generation, we use an accurate solver that guides particles toward high-reward lookahead samples. We refer to this sampling scheme as LiDAR sampling. LiDAR achieves substantial performance improvements using only three samples with a 3-step lookahead solver, exhibiting steep performance gains as lookahead accuracy and sample count increase; notably, it reaches the same GenEval performance as the latest gradient guidance method for SDXL with a 9.5x speedup.</li>
<li><strong>摘要：</strong>扩散模型表现出了强大的生成性能；然而，生成的样本通常无法完全符合人类的意图。本文研究了一种测试时间缩放方法，该方法能够从具有较高人类对齐奖励值的区域进行采样。现有的梯度引导方法使用泰勒近似来近似中间粒子 $\mathbf{x}_t$ 处的预期未来奖励（EFR），但由于顺序神经反向传播，每个时间步的这种近似都会产生较高的计算成本。我们证明，任何 $\mathbf{x}_t$ 处的 EFR 都可以仅使用预先训练的扩散模型中的边际样本来计算。所提出的 EFR 公式分离了 $\mathbf{x}_t$ 和 EFR 之间的神经依赖性，从而无需神经反向传播即可实现封闭式引导计算。为了进一步提高效率，我们引入了前瞻采样来收集边际样本。对于最终的样本生成，我们使用精确的求解器来引导粒子走向高回报的前瞻样本。我们将此采样方案称为 LiDAR 采样。 LiDAR 仅使用 3 个样本和 3 步前瞻求解器即可实现显着的性能改进，随着前瞻精度和样本数量的增加，表现出急剧的性能提升；值得注意的是，它达到了与 SDXL 最新梯度引导方法相同的 GenEval 性能，加速率为 9.5 倍。</li>
</ul>

<h3>Title: ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask</h3>
<ul>
<li><strong>Authors: </strong>Zhuoran Yang, Yanyong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03213">https://arxiv.org/abs/2602.03213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03213">https://arxiv.org/pdf/2602.03213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03213]] ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask(https://arxiv.org/abs/2602.03213)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Autonomous driving relies on robust models trained on large-scale, high-quality multi-view driving videos. Although world models provide a cost-effective solution for generating realistic driving data, they often suffer from identity drift, where the same object changes its appearance or category across frames due to the absence of instance-level temporal constraints. We introduce ConsisDrive, an identity-preserving driving world model designed to enforce temporal consistency at the instance level. Our framework incorporates two key components: (1) Instance-Masked Attention, which applies instance identity masks and trajectory masks within attention blocks to ensure that visual tokens interact only with their corresponding instance features across spatial and temporal dimensions, thereby preserving object identity consistency; and (2) Instance-Masked Loss, which adaptively emphasizes foreground regions with probabilistic instance masking, reducing background noise while maintaining overall scene fidelity. By integrating these mechanisms, ConsisDrive achieves state-of-the-art driving video generation quality and demonstrates significant improvements in downstream autonomous driving tasks on the nuScenes dataset. Our project page is this https URL.</li>
<li><strong>摘要：</strong>自动驾驶依赖于在大规模、高质量多视图驾驶视频上训练的强大模型。尽管世界模型为生成真实驾驶数据提供了一种经济高效的解决方案，但它们经常遭受身份漂移的困扰，即由于缺乏实例级时间约束，同一对象在不同帧之间改变其外观或类别。我们引入了 ConsisDrive，这是一种身份保护的驾驶世界模型，旨在在实例级别强制执行时间一致性。我们的框架包含两个关键组成部分：（1）实例掩码注意力，它在注意力块中应用实例身份掩码和轨迹掩码，以确保视觉标记仅与跨空间和时间维度的相应实例特征交互，从而保持对象身份一致性； (2) 实例掩蔽损失，通过概率实例掩蔽自适应地强调前景区域，减少背景噪声，同时保持整体场景保真度。通过集成这些机制，ConsisDrive 实现了最先进的驾驶视频生成质量，并在 nuScenes 数据集上展示了下游自动驾驶任务的显着改进。我们的项目页面就是这个https URL。</li>
</ul>

<h3>Title: PokeFusion Attention: Enhancing Reference-Free Style-Conditioned Generation</h3>
<ul>
<li><strong>Authors: </strong>Jingbang Tang (James)</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03220">https://arxiv.org/abs/2602.03220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03220">https://arxiv.org/pdf/2602.03220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03220]] PokeFusion Attention: Enhancing Reference-Free Style-Conditioned Generation(https://arxiv.org/abs/2602.03220)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This paper studies reference-free style-conditioned character generation in text-to-image diffusion models, where high-quality synthesis requires both stable character structure and consistent, fine-grained style expression across diverse prompts. Existing approaches primarily rely on text-only prompting, which is often under-specified for visual style and tends to produce noticeable style drift and geometric inconsistency, or introduce reference-based adapters that depend on external images at inference time, increasing architectural complexity and limiting deployment this http URL propose PokeFusion Attention, a lightweight decoder-level cross-attention mechanism that fuses textual semantics with learned style embeddings directly inside the diffusion decoder. By decoupling text and style conditioning at the attention level, our method enables effective reference-free stylized generation while keeping the pretrained diffusion backbone fully this http URL Attention trains only decoder cross-attention layers together with a compact style projection module, resulting in a parameter-efficient and plug-and-play control component that can be easily integrated into existing diffusion pipelines and transferred across different this http URL on a stylized character generation benchmark (Pokemon-style) demonstrate that our method consistently improves style fidelity, semantic alignment, and character shape consistency compared with representative adapter-based baselines, while maintaining low parameter overhead and inference-time simplicity.</li>
<li><strong>摘要：</strong>本文研究了文本到图像扩散模型中的无参考样式条件字符生成，其中高质量合成需要稳定的字符结构和跨不同提示的一致、细粒度的样式表达。现有的方法主要依赖于纯文本提示，这种提示通常对视觉风格的指定不足，并且往往会产生明显的风格漂移和几何不一致，或者引入在推理时依赖于外部图像的基于参考的适配器，增加架构复杂性并限制部署。这个http URL提出了PokeFusion Attention，一种轻量级的解码器级交叉注意机制，它将文本语义与直接在扩散解码器内部学习的风格嵌入融合在一起。通过在注意力级别解耦文本和样式调节，我们的方法实现了有效的无参考风格化生成，同时完全保持预训练的扩散主干。该http URL注意力仅训练解码器交叉注意力层和紧凑风格投影模块，从而产生参数高效且即插即用的控制组件，可以轻松集成到现有的扩散管道中，并在风格化字符生成基准（Pokemon风格）上跨不同的http URL传输。与基于适配器的代表性基线相比，字符形状一致性，同时保持低参数开销和推理时间简单性。</li>
</ul>

<h3>Title: Spiral RoPE: Rotate Your Rotary Positional Embeddings in the 2D Plane</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Liu, Sucheng Ren, Tingyu Zhu, Peng Wang, Cihang Xie, Alan Yuille, Zeyu Zheng, Feng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03227">https://arxiv.org/abs/2602.03227</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03227">https://arxiv.org/pdf/2602.03227</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03227]] Spiral RoPE: Rotate Your Rotary Positional Embeddings in the 2D Plane(https://arxiv.org/abs/2602.03227)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Rotary Position Embedding (RoPE) is the de facto positional encoding in large language models due to its ability to encode relative positions and support length extrapolation. When adapted to vision transformers, the standard axial formulation decomposes two-dimensional spatial positions into horizontal and vertical components, implicitly restricting positional encoding to axis-aligned directions. We identify this directional constraint as a fundamental limitation of the standard axial 2D RoPE, which hinders the modeling of oblique spatial relationships that naturally exist in natural images. To overcome this limitation, we propose Spiral RoPE, a simple yet effective extension that enables multi-directional positional encoding by partitioning embedding channels into multiple groups associated with uniformly distributed directions. Each group is rotated according to the projection of the patch position onto its corresponding direction, allowing spatial relationships to be encoded beyond the horizontal and vertical axes. Across a wide range of vision tasks including classification, segmentation, and generation, Spiral RoPE consistently improves performance. Qualitative analysis of attention maps further show that Spiral RoPE exhibits more concentrated activations on semantically relevant objects and better respects local object boundaries, highlighting the importance of multi-directional positional encoding in vision transformers.</li>
<li><strong>摘要：</strong>旋转位置嵌入 (RoPE) 是大型语言模型中事实上的位置编码，因为它能够编码相对位置并支持长度外推。当适应视觉变换器时，标准轴向公式将二维空间位置分解为水平和垂直分量，隐式地将位置编码限制为轴对齐方向。我们认为这种方向约束是标准轴向 2D RoPE 的基本限制，它阻碍了自然图像中自然存在的倾斜空间关系的建模。为了克服这一限制，我们提出了 Spiral RoPE，这是一种简单而有效的扩展，通过将嵌入通道划分为与均匀分布方向相关的多个组来实现多方向位置编码。每组根据块位置在其相应方向上的投影进行旋转，从而允许在水平轴和垂直轴之外对空间关系进行编码。在分类、分割和生成等广泛的视觉任务中，Spiral RoPE 不断提高性能。对注意力图的定性分析进一步表明，Spiral RoPE 对语义相关的对象表现出更集中的激活，并且更好地尊重局部对象边界，凸显了视觉变换器中多方向位置编码的重要性。</li>
</ul>

<h3>Title: InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhuoran Yang, Xi Guo, Chenjing Ding, Chiyu Wang, Wei Wu, Yanyong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03242">https://arxiv.org/abs/2602.03242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03242">https://arxiv.org/pdf/2602.03242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03242]] InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation(https://arxiv.org/abs/2602.03242)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA's autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is this https URL.</li>
<li><strong>摘要：</strong>自动驾驶依赖于经过高质量、大规模多视图驾驶视频训练的稳健模型。虽然世界模型为生成逼真的驾驶视频提供了一种经济高效的解决方案，但它们很难保持实例级的时间一致性和空间几何保真度。为了应对这些挑战，我们提出了 InstaDrive，这是一种新颖的框架，它通过两个关键的进步来增强驾驶视频的真实感：(1) Instance Flow Guider，它跨帧提取和传播实例特征以强制时间一致性，随着时间的推移保留实例身份。 (2) 空间几何对齐器，它改进了空间推理，确保精确的实例定位，并显式地建模遮挡层次结构。通过整合这些实例感知机制，InstaDrive 实现了最先进的视频生成质量，并增强了 nuScenes 数据集上的下游自动驾驶任务。此外，我们利用 CARLA 的自动驾驶仪以程序和随机方式模拟跨不同地图和区域的罕见但安全关键的驾驶场景，从而为自动驾驶系统提供严格的安全评估。我们的项目页面就是这个https URL。</li>
</ul>

<h3>Title: A3-TTA: Adaptive Anchor Alignment Test-Time Adaptation for Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jianghao Wu, Xiangde Luo, Yubo Zhou, Lianming Wu, Guotai Wang, Shaoting Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03292">https://arxiv.org/abs/2602.03292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03292">https://arxiv.org/pdf/2602.03292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03292]] A3-TTA: Adaptive Anchor Alignment Test-Time Adaptation for Image Segmentation(https://arxiv.org/abs/2602.03292)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Test-Time Adaptation (TTA) offers a practical solution for deploying image segmentation models under domain shift without accessing source data or retraining. Among existing TTA strategies, pseudo-label-based methods have shown promising performance. However, they often rely on perturbation-ensemble heuristics (e.g., dropout sampling, test-time augmentation, Gaussian noise), which lack distributional grounding and yield unstable training signals. This can trigger error accumulation and catastrophic forgetting during adaptation. To address this, we propose \textbf{A3-TTA}, a TTA framework that constructs reliable pseudo-labels through anchor-guided supervision. Specifically, we identify well-predicted target domain images using a class compact density metric, under the assumption that confident predictions imply distributional proximity to the source domain. These anchors serve as stable references to guide pseudo-label generation, which is further regularized via semantic consistency and boundary-aware entropy minimization. Additionally, we introduce a self-adaptive exponential moving average strategy to mitigate label noise and stabilize model update during adaptation. Evaluated on both multi-domain medical images (heart structure and prostate segmentation) and natural images, A3-TTA significantly improves average Dice scores by 10.40 to 17.68 percentage points compared to the source model, outperforming several state-of-the-art TTA methods under different segmentation model architectures. A3-TTA also excels in continual TTA, maintaining high performance across sequential target domains with strong anti-forgetting ability. The code will be made publicly available at this https URL.</li>
<li><strong>摘要：</strong>测试时适应 (TTA) 提供了一种实用的解决方案，用于在域转移下部署图像分割模型，而无需访问源数据或重新训练。在现有的 TTA 策略中，基于伪标签的方法表现出了良好的性能。然而，它们通常依赖于扰动集合启发法（例如，丢失采样、测试时间增强、高斯噪声），​​这些方法缺乏分布式基础并产生不稳定的训练信号。这可能会在适应过程中引发错误积累和灾难性遗忘。为了解决这个问题，我们提出了 \textbf{A3-TTA}，这是一个通过锚引导监督构建可靠伪标签的 TTA 框架。具体来说，我们使用类紧凑密度度量来识别预测良好的目标域图像，假设置信的预测意味着与源域的分布接近。这些锚作为稳定的参考来指导伪标签生成，通过语义一致性和边界感知熵最小化进一步规范化。此外，我们引入了自适应指数移动平均策略，以减轻标签噪声并稳定适应过程中的模型更新。在多领域医学图像（心脏结构和前列腺分割）和自然图像上进行评估，与源模型相比，A3-TTA 的平均 Dice 分数显着提高了 10.40 至 17.68 个百分点，在不同分割模型架构下优于几种最先进的 TTA 方法。 A3-TTA在连续TTA方面也表现出色，在连续目标域上保持高性能，具有很强的抗遗忘能力。该代码将在此 https URL 上公开提供。</li>
</ul>

<h3>Title: Anomaly Detection via Mean Shift Density Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Pritam Kar, Rahul Bordoloi, Olaf Wolkenhauer, Saptarshi Bej</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03293">https://arxiv.org/abs/2602.03293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03293">https://arxiv.org/pdf/2602.03293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03293]] Anomaly Detection via Mean Shift Density Enhancement(https://arxiv.org/abs/2602.03293)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Unsupervised anomaly detection stands as an important problem in machine learning, with applications in financial fraud prevention, network security and medical diagnostics. Existing unsupervised anomaly detection algorithms rarely perform well across different anomaly types, often excelling only under specific structural assumptions. This lack of robustness also becomes particularly evident under noisy settings. We propose Mean Shift Density Enhancement (MSDE), a fully unsupervised framework that detects anomalies through their geometric response to density-driven manifold evolution. MSDE is based on the principle that normal samples, being well supported by local density, remain stable under iterative density enhancement, whereas anomalous samples undergo large cumulative displacements as they are attracted toward nearby density modes. To operationalize this idea, MSDE employs a weighted mean-shift procedure with adaptive, sample-specific density weights derived from a UMAP-based fuzzy neighborhood graph. Anomaly scores are defined by the total displacement accumulated across a small number of mean-shift iterations. We evaluate MSDE on the ADBench benchmark, comprising forty six real-world tabular datasets, four realistic anomaly generation mechanisms, and six noise levels. Compared to 13 established unsupervised baselines, MSDE achieves consistently strong, balanced and robust performance for AUC-ROC, AUC-PR, and Precision@n, at several noise levels and on average over several types of anomalies. These results demonstrate that displacement-based scoring provides a robust alternative to the existing state-of-the-art for unsupervised anomaly detection.</li>
<li><strong>摘要：</strong>无监督异常检测是机器学习中的一个重要问题，在金融欺诈预防、网络安全和医疗诊断等领域都有应用。现有的无监督异常检测算法很少能在不同的异常类型上表现良好，通常仅在特定的结构假设下表现出色。这种鲁棒性的缺乏在嘈杂的环境下也变得尤为明显。我们提出了均值平移密度增强（MSDE），这是一种完全无监督的框架，可通过对密度驱动的流形演化的几何响应来检测异常。 MSDE 的原理是，正常样本在局部密度的良好支持下，在迭代密度增强下保持稳定，而异常样本因被附近的密度模式吸引而经历较大的累积位移。为了实现这一想法，MSDE 采用了加权均值漂移过程，该过程具有从基于 UMAP 的模糊邻域图派生的自适应、特定于样本的密度权重。异常分数由少量均值漂移迭代中累积的总位移定义。我们在 ADBench 基准上评估 MSDE，包括 46 个真实世界的表格数据集、四种现实的异常生成机制和 6 个噪声级别。与 13 个已建立的无监督基线相比，MSDE 在 AUC-ROC、AUC-PR 和 Precision@n 方面在多种噪声水平和多种异常类型的平均水平上实现了一致的强大、平衡和稳健的性能。这些结果表明，基于位移的评分为现有最先进的无监督异常检测提供了可靠的替代方案。</li>
</ul>

<h3>Title: R1-SyntheticVL: Is Synthetic Data from Generative Models Ready for Multimodal Large Language Model?</h3>
<ul>
<li><strong>Authors: </strong>Jingyi Zhang, Tianyi Lin, Huanjin Yao, Xiang Lan, Shunyu Liu, Jiaxing Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03300">https://arxiv.org/abs/2602.03300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03300">https://arxiv.org/pdf/2602.03300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03300]] R1-SyntheticVL: Is Synthetic Data from Generative Models Ready for Multimodal Large Language Model?(https://arxiv.org/abs/2602.03300)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In this work, we aim to develop effective data synthesis techniques that autonomously synthesize multimodal training data for enhancing MLLMs in solving complex real-world tasks. To this end, we propose Collective Adversarial Data Synthesis (CADS), a novel and general approach to synthesize high-quality, diverse and challenging multimodal data for MLLMs. The core idea of CADS is to leverage collective intelligence to ensure high-quality and diverse generation, while exploring adversarial learning to synthesize challenging samples for effectively driving model improvement. Specifically, CADS operates with two cyclic phases, i.e., Collective Adversarial Data Generation (CAD-Generate) and Collective Adversarial Data Judgment (CAD-Judge). CAD-Generate leverages collective knowledge to jointly generate new and diverse multimodal data, while CAD-Judge collaboratively assesses the quality of synthesized data. In addition, CADS introduces an Adversarial Context Optimization mechanism to optimize the generation context to encourage challenging and high-value data generation. With CADS, we construct MMSynthetic-20K and train our model R1-SyntheticVL, which demonstrates superior performance on various benchmarks.</li>
<li><strong>摘要：</strong>在这项工作中，我们的目标是开发有效的数据合成技术，自动合成多模态训练数据，以增强 MLLM 解决复杂的现实世界任务的能力。为此，我们提出了集体对抗数据合成（CADS），这是一种为 MLLM 合成高质量、多样化且具有挑战性的多模态数据的新颖且通用的方法。 CADS的核心思想是利用集体智慧确保高质量和多样化的生成，同时探索对抗性学习来合成具有挑战性的样本，从而有效推动模型改进。具体来说，CADS 以两个循环阶段运行，即集体对抗数据生成（CAD-Generate）和集体对抗数据判断（CAD-Judge）。 CAD-Generate 利用集体知识共同生成新的、多样化的多模式数据，而 CAD-Judge 则协作评估合成数据的质量。此外，CADS 引入了对抗性上下文优化机制来优化生成上下文，以鼓励具有挑战性和高价值的数据生成。通过 CADS，我们构建了 MMSynthetic-20K 并训练了我们的模型 R1-SyntheticVL，该模型在各种基准测试中表现出了卓越的性能。</li>
</ul>

<h3>Title: Full end-to-end diagnostic workflow automation of 3D OCT via foundation model-driven AI for retinal diseases</h3>
<ul>
<li><strong>Authors: </strong>Jinze Zhang, Jian Zhong, Li Lin, Jiaxiong Li, Ke Ma, Naiyang Li, Meng Li, Yuan Pan, Zeyu Meng, Mengyun Zhou, Shang Huang, Shilong Yu, Zhengyu Duan, Sutong Li, Honghui Xia, Juping Liu, Dan Liang, Yantao Wei, Xiaoying Tang, Jin Yuan, Peng Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03302">https://arxiv.org/abs/2602.03302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03302">https://arxiv.org/pdf/2602.03302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03302]] Full end-to-end diagnostic workflow automation of 3D OCT via foundation model-driven AI for retinal diseases(https://arxiv.org/abs/2602.03302)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Optical coherence tomography (OCT) has revolutionized retinal disease diagnosis with its high-resolution and three-dimensional imaging nature, yet its full diagnostic automation in clinical practices remains constrained by multi-stage workflows and conventional single-slice single-task AI models. We present Full-process OCT-based Clinical Utility System (FOCUS), a foundation model-driven framework enabling end-to-end automation of 3D OCT retinal disease diagnosis. FOCUS sequentially performs image quality assessment with EfficientNetV2-S, followed by abnormality detection and multi-disease classification using a fine-tuned Vision Foundation Model. Crucially, FOCUS leverages a unified adaptive aggregation method to intelligently integrate 2D slices-level predictions into comprehensive 3D patient-level diagnosis. Trained and tested on 3,300 patients (40,672 slices), and externally validated on 1,345 patients (18,498 slices) across four different-tier centers and diverse OCT devices, FOCUS achieved high F1 scores for quality assessment (99.01%), abnormally detection (97.46%), and patient-level diagnosis (94.39%). Real-world validation across centers also showed stable performance (F1: 90.22%-95.24%). In human-machine comparisons, FOCUS matched expert performance in abnormality detection (F1: 95.47% vs 90.91%) and multi-disease diagnosis (F1: 93.49% vs 91.35%), while demonstrating better efficiency. FOCUS automates the image-to-diagnosis pipeline, representing a critical advance towards unmanned ophthalmology with a validated blueprint for autonomous screening to enhance population scale retinal care accessibility and efficiency.</li>
<li><strong>摘要：</strong>光学相干断层扫描（OCT）以其高分辨率和三维成像特性彻底改变了视网膜疾病的诊断，但其在临床实践中的全面诊断自动化仍然受到多阶段工作流程和传统单切片单任务人工智能模型的限制。我们推出基于 OCT 的全流程临床实用系统 (FOCUS)，这是一个基础模型驱动框架，可实现 3D OCT 视网膜疾病诊断的端到端自动化。 FOCUS 使用 EfficientNetV2-S 依次执行图像质量评估，然后使用微调的视觉基础模型进行异常检测和多疾病分类。至关重要的是，FOCUS 利用统一的自适应聚合方法将 2D 切片级预测智能地集成到全面的 3D 患者级诊断中。 FOCUS 在 3,300 名患者（40,672 个切片）上进行了培训和测试，并在四个不同级别中心和不同 OCT 设备上对 1,345 名患者（18,498 个切片）进行了外部验证，在质量评估 (99.01%)、异常检测 (97.46%) 和患者级诊断 (94.39%) 方面取得了较高的 F1 分数。跨中心的真实世界验证也显示出稳定的性能（F1：90.22%-95.24%）。在人机对比中，FOCUS在异常检测（F1：95.47% vs 90.91%）和多疾病诊断（F1：93.49% vs 91.35%）方面与专家表现相当，同时表现出更好的效率。 FOCUS 实现了图像到诊断流程的自动化，代表了无人眼科的重大进步，为自主筛查提供了经过验证的蓝图，以提高人口规模视网膜护理的可及性和效率。</li>
</ul>

<h3>Title: Entropy-Gated Selective Policy Optimization:Token-Level Gradient Allocation for Hybrid Training of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuelin Hu, Zhengxue Cheng, Wei Liu, Li Song</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03309">https://arxiv.org/abs/2602.03309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03309">https://arxiv.org/pdf/2602.03309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03309]] Entropy-Gated Selective Policy Optimization:Token-Level Gradient Allocation for Hybrid Training of Large Language Models(https://arxiv.org/abs/2602.03309)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Hybrid training methods for large language models combine supervised fine tuning (SFT) on expert demonstrations with reinforcement learning (RL) on model rollouts, typically at the sample level. We propose Entropy Gated Selective Policy Optimization (EGSPO), a three stage framework that extends sample level mixing with token level gradient modulation. Stage 1, SFT expert learning, establishes a reliable warm up policy using expert demonstrations with a pure SFT loss. Stage 2, RL rollout generation, samples trajectories from the current policy and computes per token predictive entropy. Stage 3, the EGSPO mechanism, applies entropy gated gradient allocation: a predictive entropy module routes high entropy tokens to full PPO updates to encourage exploration, and low entropy tokens to attenuated PPO updates to reduce variance and preserve knowledge. Critically, both branches incorporate the advantage function A_t, ensuring that incorrect trajectories receive consistent negative learning signals and preventing reinforcement of confident errors. EGSPO achieves consistent improvements on mathematical reasoning benchmarks, with gains of 3.8 percent on AIME and 2.9 percent on MATH over the CHORD phi baseline, while incurring only 3.4 percent additional computational overhead.</li>
<li><strong>摘要：</strong>大型语言模型的混合训练方法将专家演示的监督微调 (SFT) 与模型推出的强化学习 (RL) 相结合，通常在样本级别。我们提出熵门选择性策略优化（EGISPO），这是一个三阶段框架，通过令牌级别梯度调制扩展样本级别混合。第一阶段，SFT 专家学习，使用具有纯 SFT 损失的专家演示建立可靠的预热策略。第 2 阶段，RL 推出生成，从当前策略中采样轨迹并计算每个令牌的预测熵。第三阶段，EGSPO 机制，应用熵门控梯度分配：预测熵模块将高熵令牌路由到完整的 PPO 更新以鼓励探索，将低熵令牌路由到衰减的 PPO 更新以减少方差并保留知识。至关重要的是，两个分支都包含优势函数 A_t，确保不正确的轨迹收到一致的负学习信号并防止强化置信错误。 EGSPO 在数学推理基准上实现了持续改进，与 CHORD phi 基线相比，AIME 提高了 3.8%，MATH 提高了 2.9%，而仅增加了 3.4% 的额外计算开销。</li>
</ul>

<h3>Title: Invisible Clean-Label Backdoor Attacks for Generative Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Ting Xiang, Jinhui Zhao, Changjian Chen, Zhuo Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03316">https://arxiv.org/abs/2602.03316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03316">https://arxiv.org/pdf/2602.03316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03316]] Invisible Clean-Label Backdoor Attacks for Generative Data Augmentation(https://arxiv.org/abs/2602.03316)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of image generative models, generative data augmentation has become an effective way to enrich training images, especially when only small-scale datasets are available. At the same time, in practical applications, generative data augmentation can be vulnerable to clean-label backdoor attacks, which aim to bypass human inspection. However, based on theoretical analysis and preliminary experiments, we observe that directly applying existing pixel-level clean-label backdoor attack methods (e.g., COMBAT) to generated images results in low attack success rates. This motivates us to move beyond pixel-level triggers and focus instead on the latent feature level. To this end, we propose InvLBA, an invisible clean-label backdoor attack method for generative data augmentation by latent perturbation. We theoretically prove that the generalization of the clean accuracy and attack success rates of InvLBA can be guaranteed. Experiments on multiple datasets show that our method improves the attack success rate by 46.43% on average, with almost no reduction in clean accuracy and high robustness against SOTA defense methods.</li>
<li><strong>摘要：</strong>随着图像生成模型的快速发展，生成数据增强已成为丰富训练图像的有效方法，特别是当只有小规模数据集可用时。与此同时，在实际应用中，生成数据增强可能容易受到旨在绕过人类检查的清洁标签后门攻击。然而，基于理论分析和初步实验，我们观察到直接将现有的像素级清洁标签后门攻击方法（例如COMBAT）应用于生成的图像会导致攻击成功率较低。这促使我们超越像素级触发器，转而关注潜在的特征级别。为此，我们提出了 InvLBA，这是一种通过潜在扰动来增强生成数据的隐形清洁标签后门攻击方法。我们从理论上证明了InvLBA的干净准确率和攻击成功率的泛化是可以保证的。在多个数据集上的实验表明，我们的方法平均将攻击成功率提高了 46.43%，并且干净精度几乎没有降低，并且对 SOTA 防御方法具有很高的鲁棒性。</li>
</ul>

<h3>Title: MedSAM-Agent: Empowering Interactive Medical Image Segmentation with Multi-turn Agentic Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Shengyuan Liu, Liuxin Bao, Qi Yang, Wanting Geng, Boyun Zheng, Chenxin Li, Wenting Chen, Houwen Peng, Yixuan Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03320">https://arxiv.org/abs/2602.03320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03320">https://arxiv.org/pdf/2602.03320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03320]] MedSAM-Agent: Empowering Interactive Medical Image Segmentation with Multi-turn Agentic Reinforcement Learning(https://arxiv.org/abs/2602.03320)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Medical image segmentation is evolving from task-specific models toward generalizable frameworks. Recent research leverages Multi-modal Large Language Models (MLLMs) as autonomous agents, employing reinforcement learning with verifiable reward (RLVR) to orchestrate specialized tools like the Segment Anything Model (SAM). However, these approaches often rely on single-turn, rigid interaction strategies and lack process-level supervision during training, which hinders their ability to fully exploit the dynamic potential of interactive tools and leads to redundant actions. To bridge this gap, we propose MedSAM-Agent, a framework that reformulates interactive segmentation as a multi-step autonomous decision-making process. First, we introduce a hybrid prompting strategy for expert-curated trajectory generation, enabling the model to internalize human-like decision heuristics and adaptive refinement strategies. Furthermore, we develop a two-stage training pipeline that integrates multi-turn, end-to-end outcome verification with a clinical-fidelity process reward design to promote interaction parsimony and decision efficiency. Extensive experiments across 6 medical modalities and 21 datasets demonstrate that MedSAM-Agent achieves state-of-the-art performance, effectively unifying autonomous medical reasoning with robust, iterative optimization. Code is available \href{this https URL}{here}.</li>
<li><strong>摘要：</strong>医学图像分割正在从特定于任务的模型发展到通用框架。最近的研究利用多模态大型语言模型 (MLLM) 作为自主代理，采用具有可验证奖励的强化学习 (RLVR) 来编排分段任意模型 (SAM) 等专用工具。然而，这些方法往往依赖于单轮、僵化的交互策略，并且在训练过程中缺乏过程级监督，这阻碍了它们充分利用交互工具的动态潜力的能力，并导致冗余动作。为了弥补这一差距，我们提出了 MedSAM-Agent，这是一个将交互式细分重新表述为多步骤自主决策过程的框架。首先，我们引入了一种用于专家策划的轨迹生成的混合提示策略，使模型能够内化类人决策启发法和自适应细化策略。此外，我们开发了一个两阶段的训练管道，将多轮、端到端结果验证与临床保真度过程奖励设计相结合，以促进交互简约性和决策效率。跨越 6 种医疗模式和 21 个数据集的广泛实验表明，MedSAM-Agent 实现了最先进的性能，有效地将自主医疗推理与稳健的迭代优化相结合。代码可在\href{此 https URL}{此处}获取。</li>
</ul>

<h3>Title: Composable Visual Tokenizers with Generator-Free Diagnostics of Learnability</h3>
<ul>
<li><strong>Authors: </strong>Bingchen Zhao, Qiushan Guo, Ye Wang, Yixuan Huang, Zhonghua Zhai, Yu Tian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03339">https://arxiv.org/abs/2602.03339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03339">https://arxiv.org/pdf/2602.03339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03339]] Composable Visual Tokenizers with Generator-Free Diagnostics of Learnability(https://arxiv.org/abs/2602.03339)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce CompTok, a training framework for learning visual tokenizers whose tokens are enhanced for compositionality. CompTok uses a token-conditioned diffusion decoder. By employing an InfoGAN-style objective, where we train a recognition model to predict the tokens used to condition the diffusion decoder using the decoded images, we enforce the decoder to not ignore any of the tokens. To promote compositional control, besides the original images, CompTok also trains on tokens formed by swapping token subsets between images, enabling more compositional control of the token over the decoder. As the swapped tokens between images do not have ground truth image targets, we apply a manifold constraint via an adversarial flow regularizer to keep unpaired swap generations on the natural-image distribution. The resulting tokenizer not only achieves state-of-the-art performance on image class-conditioned generation, but also demonstrates properties such as swapping tokens between images to achieve high level semantic editing of an image. Additionally, we propose two metrics that measures the landscape of the token space that can be useful to describe not only the compositionality of the tokens, but also how easy to learn the landscape is for a generator to be trained on this space. We show in experiments that CompTok can improve on both of the metrics as well as supporting state-of-the-art generators for class conditioned generation.</li>
<li><strong>摘要：</strong>我们介绍了 CompTok，这是一个用于学习视觉标记器的训练框架，其标记的组合性得到了增强。 CompTok 使用令牌条件扩散解码器。通过采用 InfoGAN 风格的目标，我们训练一个识别模型来预测用于使用解码图像调节扩散解码器的标记，我们强制解码器不忽略任何标记。为了促进构图控制，除了原始图像之外，CompTok 还对通过在图像之间交换 token 子集形成的 token 进行训练，从而在解码器上实现对 token 的更多构图控制。由于图像之间交换的标记没有地面真实图像目标，因此我们通过对抗流正则化器应用流形约束，以在自然图像分布上保持不成对的交换生成。由此产生的标记生成器不仅在图像类条件生成方面实现了最先进的性能，而且还演示了诸如在图像之间交换标记以实现图像的高级语义编辑等属性。此外，我们提出了两个衡量令牌空间景观的指标，它们不仅可用于描述令牌的组成性，还可用于描述在该空间上训练的生成器学习景观的容易程度。我们在实验中表明，CompTok 可以改进这两个指标，并支持最先进的生成器进行类条件生成。</li>
</ul>

<h3>Title: Tiled Prompts: Overcoming Prompt Underspecification in Image and Video Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Bryan Sangwoo Kim, Jonghyun Park, Jong Chul Ye</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03342">https://arxiv.org/abs/2602.03342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03342">https://arxiv.org/pdf/2602.03342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03342]] Tiled Prompts: Overcoming Prompt Underspecification in Image and Video Super-Resolution(https://arxiv.org/abs/2602.03342)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Text-conditioned diffusion models have advanced image and video super-resolution by using prompts as semantic priors, but modern super-resolution pipelines typically rely on latent tiling to scale to high resolutions, where a single global caption causes prompt underspecification. A coarse global prompt often misses localized details (prompt sparsity) and provides locally irrelevant guidance (prompt misguidance) that can be amplified by classifier-free guidance. We propose Tiled Prompts, a unified framework for image and video super-resolution that generates a tile-specific prompt for each latent tile and performs super-resolution under locally text-conditioned posteriors, providing high-information guidance that resolves prompt underspecification with minimal overhead. Experiments on high resolution real-world images and videos show consistent gains in perceptual quality and text alignment, while reducing hallucinations and tile-level artifacts relative to global-prompt baselines.</li>
<li><strong>摘要：</strong>文本条件扩散模型通过使用提示作为语义先验来具有先进的图像和视频超分辨率，但现代超分辨率管道通常依赖于潜在平铺来缩放到高分辨率，其中单个全局标题会导致提示规格不足。粗略的全局提示通常会错过局部细节（提示稀疏性），并提供局部不相关的指导（提示误导），这些指导可以通过无分类器的指导来放大。我们提出了 Tiled Prompts，这是一个用于图像和视频超分辨率的统一框架，它为每个潜在图块生成特定于图块的提示，并在本地文本条件后验下执行超分辨率，提供高信息指导，以最小的开销解决提示不足的问题。对高分辨率现实世界图像和视频的实验表明，相对于全局提示基线，感知质量和文本对齐方面取得了一致的进步，同时减少了幻觉和图块级伪影。</li>
</ul>

<h3>Title: MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling</h3>
<ul>
<li><strong>Authors: </strong>Ning Ding, Fangcheng Liu, Kyungrae Kim, Linji Hao, Kyeng-Hun Lee, Hyeonmok Ko, Yehui Tang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03359">https://arxiv.org/abs/2602.03359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03359">https://arxiv.org/pdf/2602.03359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03359]] MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling(https://arxiv.org/abs/2602.03359)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Scaling Large Language Models (LLMs) typically relies on increasing the number of parameters or test-time computations to boost performance. However, these strategies are impractical for edge device deployment due to limited RAM and NPU resources. Despite hardware constraints, deploying performant LLM on edge devices such as smartphone remains crucial for user experience. To address this, we propose MeKi (Memory-based Expert Knowledge Injection), a novel system that scales LLM capacity via storage space rather than FLOPs. MeKi equips each Transformer layer with token-level memory experts that injects pre-stored semantic knowledge into the generation process. To bridge the gap between training capacity and inference efficiency, we employ a re-parameterization strategy to fold parameter matrices used during training into a compact static lookup table. By offloading the knowledge to ROM, MeKi decouples model capacity from computational cost, introducing zero inference latency overhead. Extensive experiments demonstrate that MeKi significantly outperforms dense LLM baselines with identical inference speed, validating the effectiveness of memory-based scaling paradigm for on-device LLMs. Project homepage is at this https URL.</li>
<li><strong>摘要：</strong>扩展大型语言模型 (LLM) 通常依赖于增加参数数量或测试时计算来提高性能。然而，由于 RAM 和 NPU 资源有限，这些策略对于边缘设备部署来说是不切实际的。尽管存在硬件限制，但在智能手机等边缘设备上部署高性能的法学硕士对于用户体验仍然至关重要。为了解决这个问题，我们提出了 MeKi（基于内存的专家知识注入），这是一种通过存储空间而不是 FLOP 来扩展 LLM 容量的新颖系统。 MeKi 为每个 Transformer 层配备了令牌级内存专家，将预存储的语义知识注入到生成过程中。为了弥合训练能力和推理效率之间的差距，我们采用重新参数化策略将训练期间使用的参数矩阵折叠成紧凑的静态查找表。通过将知识卸载到 ROM，MeKi 将模型容量与计算成本分离，从而引入零推理延迟开销。大量实验表明，MeKi 在相同的推理速度下显着优于密集 LLM 基线，验证了设备上 LLM 基于内存的扩展范例的有效性。项目主页位于此 https URL。</li>
</ul>

<h3>Title: Symbol-Aware Reasoning with Masked Discrete Diffusion for Handwritten Mathematical Expression Recognition</h3>
<ul>
<li><strong>Authors: </strong>Takaya Kawakatsu, Ryo Ishiyama</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03370">https://arxiv.org/abs/2602.03370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03370">https://arxiv.org/pdf/2602.03370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03370]] Symbol-Aware Reasoning with Masked Discrete Diffusion for Handwritten Mathematical Expression Recognition(https://arxiv.org/abs/2602.03370)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Handwritten Mathematical Expression Recognition (HMER) requires reasoning over diverse symbols and 2D structural layouts, yet autoregressive models struggle with exposure bias and syntactic inconsistency. We present a discrete diffusion framework that reformulates HMER as iterative symbolic refinement instead of sequential generation. Through multi-step remasking, the proposal progressively refines both symbols and structural relations, removing causal dependencies and improving structural consistency. A symbol-aware tokenization and Random-Masking Mutual Learning further enhance syntactic alignment and robustness to handwriting diversity. On the MathWriting benchmark, the proposal achieves 5.56\% CER and 60.42\% EM, outperforming strong Transformer and commercial baselines. Consistent gains on CROHME 2014--2023 demonstrate that discrete diffusion provides a new paradigm for structure-aware visual recognition beyond generative modeling.</li>
<li><strong>摘要：</strong>手写数学表达式识别 (HMER) 需要对不同符号和 2D 结构布局进行推理，但自回归模型却面临暴露偏差和句法不一致的问题。我们提出了一个离散扩散框架，将 HMER 重新表述为迭代符号细化而不是顺序生成。通过多步骤重新屏蔽，该提案逐步细化符号和结构关系，消除因果依赖性并提高结构一致性。符号感知标记化和随机掩码相互学习进一步增强了句法对齐和手写多样性的鲁棒性。在 MathWriting 基准上，该提案实现了 5.56\% CER 和 60.42\% EM，优于强大的 Transformer 和商业基准。 CROHME 2014--2023 的持续收益表明，离散扩散为生成建模之外的结构感知视觉识别提供了新范例。</li>
</ul>

<h3>Title: SLIM-Diff: Shared Latent Image-Mask Diffusion with Lp loss for Data-Scarce Epilepsy FLAIR MRI</h3>
<ul>
<li><strong>Authors: </strong>Mario Pascual-González, Ariadna Jiménez-Partinen, R.M. Luque-Baena, Fátima Nagib-Raya, Ezequiel López-Rubio</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03372">https://arxiv.org/abs/2602.03372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03372">https://arxiv.org/pdf/2602.03372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03372]] SLIM-Diff: Shared Latent Image-Mask Diffusion with Lp loss for Data-Scarce Epilepsy FLAIR MRI(https://arxiv.org/abs/2602.03372)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Focal cortical dysplasia (FCD) lesions in epilepsy FLAIR MRI are subtle and scarce, making joint image--mask generative modeling prone to instability and memorization. We propose SLIM-Diff, a compact joint diffusion model whose main contributions are (i) a single shared-bottleneck U-Net that enforces tight coupling between anatomy and lesion geometry from a 2-channel image+mask representation, and (ii) loss-geometry tuning via a tunable $L_p$ objective. As an internal baseline, we include the canonical DDPM-style objective ($\epsilon$-prediction with $L_2$ loss) and isolate the effect of prediction parameterization and $L_p$ geometry under a matched setup. Experiments show that $x_0$-prediction is consistently the strongest choice for joint synthesis, and that fractional sub-quadratic penalties ($L_{1.5}$) improve image fidelity while $L_2$ better preserves lesion mask morphology. Our code and model weights are available in this https URL</li>
<li><strong>摘要：</strong>癫痫 FLAIR MRI 中的局灶性皮质发育不良 (FCD) 病变微妙且稀缺，使得联合图像掩模生成模型容易不稳定和记忆。我们提出了 SLIM-Diff，一种紧凑的联合扩散模型，其主要贡献是（i）单个共享瓶颈 U-Net，通过 2 通道图像+掩模表示强制解剖结构和病变几何结构之间的紧密耦合，以及（ii）通过可调 $L_p$ 目标进行损失几何调整。作为内部基线，我们包括规范的 DDPM 式目标（带有 $L_2$ 损失的 $\epsilon$-预测），并在匹配的设置下隔离预测参数化和 $L_p$ 几何的影响。实验表明，$x_0$ 预测始终是联合合成的最强选择，分数次二次惩罚 ($L_{1.5}$) 提高了图像保真度，而 $L_2$ 更好地保留了病变掩模形态。我们的代码和模型权重可以在此 https URL 中找到</li>
</ul>

<h3>Title: UnHype: CLIP-Guided Hypernetworks for Dynamic LoRA Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Piotr Wójcik, Maksym Petrenko, Wojciech Gromski, Przemysław Spurek, Maciej Zieba</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03410">https://arxiv.org/abs/2602.03410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03410">https://arxiv.org/pdf/2602.03410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03410]] UnHype: CLIP-Guided Hypernetworks for Dynamic LoRA Unlearning(https://arxiv.org/abs/2602.03410)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in large-scale diffusion models have intensified concerns about their potential misuse, particularly in generating realistic yet harmful or socially disruptive content. This challenge has spurred growing interest in effective machine unlearning, the process of selectively removing specific knowledge or concepts from a model without compromising its overall generative capabilities. Among various approaches, Low-Rank Adaptation (LoRA) has emerged as an effective and efficient method for fine-tuning models toward targeted unlearning. However, LoRA-based methods often exhibit limited adaptability to concept semantics and struggle to balance removing closely related concepts with maintaining generalization across broader meanings. Moreover, these methods face scalability challenges when multiple concepts must be erased simultaneously. To address these limitations, we introduce UnHype, a framework that incorporates hypernetworks into single- and multi-concept LoRA training. The proposed architecture can be directly plugged into Stable Diffusion as well as modern flow-based text-to-image models, where it demonstrates stable training behavior and effective concept control. During inference, the hypernetwork dynamically generates adaptive LoRA weights based on the CLIP embedding, enabling more context-aware, scalable unlearning. We evaluate UnHype across several challenging tasks, including object erasure, celebrity erasure, and explicit content removal, demonstrating its effectiveness and versatility. Repository: this https URL.</li>
<li><strong>摘要：</strong>大规模传播模型的最新进展加剧了人们对其潜在滥用的担忧，特别是在生成现实但有害或具有社会破坏性的内容方面。这一挑战激发了人们对有效机器取消学习的兴趣日益增长，即有选择地从模型中删除特定知识或概念而不损害其整体生成能力的过程。在各种方法中，低秩适应（LoRA）已成为一种有效且高效的方法，用于微调模型以实现有针对性的忘却。然而，基于 LoRA 的方法通常对概念语义的适应性有限，并且难以在删除密切相关的概念与保持更广泛含义的泛化之间取得平衡。此外，当必须同时删除多个概念时，这些方法面临可扩展性挑战。为了解决这些限制，我们引入了 UnHype，这是一个将超网络纳入单概念和多概念 LoRA 训练的框架。所提出的架构可以直接插入稳定扩散以及现代基于流的文本到图像模型，其中它展示了稳定的训练行为和有效的概念控制。在推理过程中，超网络基于 CLIP 嵌入动态生成自适应 LoRA 权重，从而实现更多上下文感知、可扩展的取消学习。我们在多项具有挑战性的任务中评估了 UnHype，包括对象擦除、名人擦除和显式内容删除，证明了其有效性和多功能性。存储库：此 https URL。</li>
</ul>

<h3>Title: Socratic-Geo: Synthetic Data Generation and Geometric Reasoning via Multi-Agent Interaction</h3>
<ul>
<li><strong>Authors: </strong>Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Wei Wang, Bing Zhao, Hu Wei, Linfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03414">https://arxiv.org/abs/2602.03414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03414">https://arxiv.org/pdf/2602.03414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03414]] Socratic-Geo: Synthetic Data Generation and Geometric Reasoning via Multi-Agent Interaction(https://arxiv.org/abs/2602.03414)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have significantly advanced vision-language understanding. However, even state-of-the-art models struggle with geometric reasoning, revealing a critical bottleneck: the extreme scarcity of high-quality image-text pairs. Human annotation is prohibitively expensive, while automated methods fail to ensure fidelity and training effectiveness. Existing approaches either passively adapt to available images or employ inefficient random exploration with filtering, decoupling generation from learning needs. We propose Socratic-Geo, a fully autonomous framework that dynamically couples data synthesis with model learning through multi-agent interaction. The Teacher agent generates parameterized Python scripts with reflective feedback (Reflect for solvability, RePI for visual validity), ensuring image-text pair purity. The Solver agent optimizes reasoning through preference learning, with failure paths guiding Teacher's targeted augmentation. Independently, the Generator learns image generation capabilities on accumulated "image-code-instruction" triplets, distilling programmatic drawing intelligence into visual generation. Starting from only 108 seed problems, Socratic-Solver achieves 49.11 on six benchmarks using one-quarter of baseline data, surpassing strong baselines by 2.43 points. Socratic-Generator achieves 42.4% on GenExam, establishing new state-of-the-art for open-source models, surpassing Seedream-4.0 (39.8%) and approaching Gemini-2.5-Flash-Image (43.1%).</li>
<li><strong>摘要：</strong>多模态大语言模型 (MLLM) 具有显着先进的视觉语言理解能力。然而，即使是最先进的模型也难以进行几何推理，揭示了一个关键瓶颈：高质量图像文本对的极度稀缺。人工注释成本高昂，而自动化方法无法确保保真度和训练有效性。现有的方法要么被动地适应可用图像，要么采用低效的随机探索和过滤，将生成与学习需求脱钩。我们提出了 Socratic-Geo，一个完全自主的框架，通过多智能体交互动态地将数据合成与模型学习结合起来。教师代理生成带有反射反馈的参数化 Python 脚本（Reflect 用于解决可解性，RePI 用于视觉有效性），确保图像-文本对的纯度。求解器代理通过偏好学习优化推理，并用失败路径指导教师的有针对性的增强。生成器独立地学习累积的“图像-代码-指令”三元组的图像生成功能，将程序化绘图智能提炼为视觉生成。从仅 108 个种子问题开始，Socratic-Solver 使用四分之一的基线数据在 6 个基准测试中取得了 49.11 的成绩，比强基线高出 2.43 分。 Socratic-Generator 在 GenExam 上取得了 42.4% 的成绩，为开源模型建立了新的最先进水平，超过了 Seedream-4.0 (39.8%) 并接近 Gemini-2.5-Flash-Image (43.1%)。</li>
</ul>

<h3>Title: Hierarchical Concept-to-Appearance Guidance for Multi-Subject Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Yijia Xu, Zihao Wang, Jinshi Cui</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03448">https://arxiv.org/abs/2602.03448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03448">https://arxiv.org/pdf/2602.03448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03448]] Hierarchical Concept-to-Appearance Guidance for Multi-Subject Image Generation(https://arxiv.org/abs/2602.03448)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multi-subject image generation aims to synthesize images that faithfully preserve the identities of multiple reference subjects while following textual instructions. However, existing methods often suffer from identity inconsistency and limited compositional control, as they rely on diffusion models to implicitly associate text prompts with reference images. In this work, we propose Hierarchical Concept-to-Appearance Guidance (CAG), a framework that provides explicit, structured supervision from high-level concepts to fine-grained appearances. At the conceptual level, we introduce a VAE dropout training strategy that randomly omits reference VAE features, encouraging the model to rely more on robust semantic signals from a Visual Language Model (VLM) and thereby promoting consistent concept-level generation in the absence of complete appearance cues. At the appearance level, we integrate the VLM-derived correspondences into a correspondence-aware masked attention module within the Diffusion Transformer (DiT). This module restricts each text token to attend only to its matched reference regions, ensuring precise attribute binding and reliable multi-subject composition. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the multi-subject image generation, substantially improving prompt following and subject consistency.</li>
<li><strong>摘要：</strong>多主体图像生成旨在合成图像，在遵循文本指令的同时忠实地保留多个参考主体的身份。然而，现有方法经常遭受身份不一致和有限的构图控制，因为它们依赖扩散模型来隐式地将文本提示与参考图像关联起来。在这项工作中，我们提出了分层概念到外观指导（CAG），这是一个提供从高级概念到细粒度外观的明确、结构化监督的框架。在概念层面，我们引入了一种 VAE dropout 训练策略，该策略随机省略参考 VAE 特征，鼓励模型更多地依赖来自视觉语言模型（VLM）的强大语义信号，从而在缺乏完整外观线索的情况下促进一致的概念级生成。在外观层面，我们将 VLM 派生的对应关系集成到扩散变压器 (DiT) 内的对应感知屏蔽注意模块中。该模块限制每个文本标记仅关注其匹配的参考区域，确保精确的属性绑定和可靠的多主题组合。大量的实验表明，我们的方法在多主体图像生成方面实现了最先进的性能，显着提高了及时跟踪和主体一致性。</li>
</ul>

<h3>Title: Contextualized Visual Personalization in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yeongtak Oh, Sangwon Yu, Junsung Park, Han Cheol Moon, Jisoo Mok, Sungroh Yoon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03454">https://arxiv.org/abs/2602.03454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03454">https://arxiv.org/pdf/2602.03454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03454]] Contextualized Visual Personalization in Vision-Language Models(https://arxiv.org/abs/2602.03454)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite recent progress in vision-language models (VLMs), existing approaches often fail to generate personalized responses based on the user's specific experiences, as they lack the ability to associate visual inputs with a user's accumulated visual-textual context. We newly formalize this challenge as contextualized visual personalization, which requires the visual recognition and textual retrieval of personalized visual experiences by VLMs when interpreting new images. To address this issue, we propose CoViP, a unified framework that treats personalized image captioning as a core task for contextualized visual personalization and improves this capability through reinforcement-learning-based post-training and caption-augmented generation. We further introduce diagnostic evaluations that explicitly rule out textual shortcut solutions and verify whether VLMs truly leverage visual context. Extensive experiments demonstrate that existing open-source and proprietary VLMs exhibit substantial limitations, while CoViP not only improves personalized image captioning but also yields holistic gains across downstream personalization tasks. These results highlight CoViP as a crucial stage for enabling robust and generalizable contextualized visual personalization.</li>
<li><strong>摘要：</strong>尽管视觉语言模型（VLM）最近取得了进展，但现有方法通常无法根据用户的特定体验生成个性化响应，因为它们缺乏将视觉输入与用户积累的视觉文本上下文相关联的能力。我们将这一挑战新形式化为情境化视觉个性化，这需要 VLM 在解释新图像时对个性化视觉体验进行视觉识别和文本检索。为了解决这个问题，我们提出了 CoViP，一个统一的框架，将个性化图像字幕视为上下文视觉个性化的核心任务，并通过基于强化学习的后训练和字幕增强生成来提高这种能力。我们进一步引入诊断评估，明确排除文本快捷方式解决方案，并验证 VLM 是否真正利用视觉上下文。大量实验表明，现有的开源和专有 VLM 表现出很大的局限性，而 CoViP 不仅改进了个性化图像字幕，而且还为下游个性化任务带来了整体收益。这些结果凸显了 CoViP 是实现稳健且可推广的情境化视觉个性化的关键阶段。</li>
</ul>

<h3>Title: ScDiVa: Masked Discrete Diffusion for Joint Modeling of Single-Cell Identity and Expression</h3>
<ul>
<li><strong>Authors: </strong>Mingxuan Wang, Cheng Chen, Gaoyang Jiang, Zijia Ren, Chuangxin Zhao, Lu Shi, Yanbiao Ma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03477">https://arxiv.org/abs/2602.03477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03477">https://arxiv.org/pdf/2602.03477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03477]] ScDiVa: Masked Discrete Diffusion for Joint Modeling of Single-Cell Identity and Expression(https://arxiv.org/abs/2602.03477)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Single-cell RNA-seq profiles are high-dimensional, sparse, and unordered, causing autoregressive generation to impose an artificial ordering bias and suffer from error accumulation. To address this, we propose scDiVa, a masked discrete diffusion foundation model that aligns generation with the dropout-like corruption process by defining a continuous-time forward masking mechanism in token space. ScDiVa features a bidirectional denoiser that jointly models discrete gene identities and continuous values, utilizing entropy-normalized serialization and a latent anchor token to maximize information efficiency and preserve global cell identity. The model is trained via depth-invariant time sampling and a dual denoising objective to simulate varying sparsity levels while ensuring precise recovery of both identity and magnitude. Pre-trained on 59 million cells, scDiVa achieves strong transfer performance across major benchmarks, including batch integration, cell type annotation, and perturbation response prediction. These results suggest that masked discrete diffusion serves as a biologically coherent and effective alternative to autoregression.</li>
<li><strong>摘要：</strong>单细胞 RNA-seq 谱是高维、稀疏和无序的，导致自回归生成施加人为排序偏差并遭受错误累积。为了解决这个问题，我们提出了 scDiVa，一种屏蔽离散扩散基础模型，通过在代币空间中定义连续时间前向屏蔽机制，将生成与类似 dropout 的腐败过程对齐。 ScDiVa 具有双向降噪器，可联合建模离散基因身份和连续值，利用熵归一化序列化和潜在锚标记来最大限度地提高信息效率并保留全局细胞身份。该模型通过深度不变时间采样和双重去噪目标进行训练，以模拟不同的稀疏程度，同时确保身份和幅度的精确恢复。 scDiVa 在 5900 万个细胞上进行了预训练，在主要基准测试中实现了强大的传输性能，包括批量集成、细胞类型注释和扰动响应预测。这些结果表明，掩蔽离散扩散可以作为自回归的生物学相干且有效的替代方案。</li>
</ul>

<h3>Title: Lookahead Path Likelihood Optimization for Diffusion LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xuejie Liu, Yap Vit Chun, Yitao Liang, Anji Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03496">https://arxiv.org/abs/2602.03496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03496">https://arxiv.org/pdf/2602.03496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03496]] Lookahead Path Likelihood Optimization for Diffusion LLMs(https://arxiv.org/abs/2602.03496)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion Large Language Models (dLLMs) support arbitrary-order generation, yet their inference performance critically depends on the unmasking order. Existing strategies rely on heuristics that greedily optimize local confidence, offering limited guidance for identifying unmasking paths that are globally consistent and accurate. To bridge this gap, we introduce path log-likelihood (Path LL), a trajectory-conditioned objective that strongly correlates with downstream accuracy and enables principled selection of unmasking paths. To optimize Path LL at inference time, we propose POKE, an efficient value estimator that predicts the expected future Path LL of a partial decoding trajectory. We then integrate this lookahead signal into POKE-SMC, a Sequential Monte Carlo-based search framework for dynamically identifying optimal unmasking paths. Extensive experiments across 6 reasoning tasks show that POKE-SMC consistently improves accuracy, achieving 2%--3% average gains over strong decoding-time scaling baselines at comparable inference overhead on LLaDA models and advancing the accuracy--compute Pareto frontier.</li>
<li><strong>摘要：</strong>扩散大型语言模型 (dLLM) 支持任意顺序生成，但其推理性能关键取决于揭开顺序。现有策略依赖于贪婪地优化局部置信度的启发式方法，为识别全局一致且准确的揭露路径提供有限的指导。为了弥补这一差距，我们引入了路径对数似然（Path LL），这是一种轨迹条件目标，与下游精度密切相关，并能够原则性地选择揭露路径。为了在推理时优化路径 LL，我们提出了 POKE，一种有效的值估计器，可以预测部分解码轨迹的预期未来路径 LL。然后，我们将此前瞻信号集成到 POKE-SMC 中，POKE-SMC 是一个基于顺序蒙特卡罗的搜索框架，用于动态识别最佳的揭露路径。跨越 6 个推理任务的大量实验表明，POKE-SMC 持续提高了准确性，在 LLaDA 模型上的推理开销相当的情况下，比强大的解码时间扩展基线实现了 2%--3% 的平均增益，并提高了计算 Pareto 前沿的准确性。</li>
</ul>

<h3>Title: Reparameterization Flow Policy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Hai Zhong, Zhuoran Li, Xun Wang, Longbo Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03501">https://arxiv.org/abs/2602.03501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03501">https://arxiv.org/pdf/2602.03501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03501]] Reparameterization Flow Policy Optimization(https://arxiv.org/abs/2602.03501)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Reparameterization Policy Gradient (RPG) has emerged as a powerful paradigm for model-based reinforcement learning, enabling high sample efficiency by backpropagating gradients through differentiable dynamics. However, prior RPG approaches have been predominantly restricted to Gaussian policies, limiting their performance and failing to leverage recent advances in generative models. In this work, we identify that flow policies, which generate actions via differentiable ODE integration, naturally align with the RPG framework, a connection not established in prior work. However, naively exploiting this synergy proves ineffective, often suffering from training instability and a lack of exploration. We propose Reparameterization Flow Policy Optimization (RFO). RFO computes policy gradients by backpropagating jointly through the flow generation process and system dynamics, unlocking high sample efficiency without requiring intractable log-likelihood calculations. RFO includes two tailored regularization terms for stability and exploration. We also propose a variant of RFO with action chunking. Extensive experiments on diverse locomotion and manipulation tasks, involving both rigid and soft bodies with state or visual inputs, demonstrate the effectiveness of RFO. Notably, on a challenging locomotion task controlling a soft-body quadruped, RFO achieves almost $2\times$ the reward of the state-of-the-art baseline.</li>
<li><strong>摘要：</strong>重新参数化策略梯度（RPG）已成为基于模型的强化学习的强大范例，通过可微动态反向传播梯度来实现高样本效率。然而，之前的 RPG 方法主要局限于高斯策略，限制了它们的性能并且未能利用生成模型的最新进展。在这项工作中，我们发现流策略通过可微分 ODE 集成生成动作，自然地与 RPG 框架保持一致，这是之前工作中未建立的连接。然而，天真地利用这种协同作用被证明是无效的，往往会受到训练不稳定和缺乏探索的影响。我们提出重新参数化流策略优化（RFO）。 RFO 通过流生成过程和系统动力学联合反向传播来计算策略梯度，从而释放高样本效率，而不需要复杂的对数似然计算。 RFO 包括两个针对稳定性和探索度身定制的正则化项。我们还提出了一种带有动作分块的 RFO 变体。对各种运动和操作任务（涉及具有状态或视觉输入的刚体和软体）进行的广泛实验证明了 RFO 的有效性。值得注意的是，在控制软体四足动物的挑战性运动任务中，RFO 获得了最先进基线奖励的近 2 美元\倍$。</li>
</ul>

<h3>Title: Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models</h3>
<ul>
<li><strong>Authors: </strong>Arco van Breda, Erman Acar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03506">https://arxiv.org/abs/2602.03506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03506">https://arxiv.org/pdf/2602.03506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03506]] Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models(https://arxiv.org/abs/2602.03506)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Following their success across many domains, transformers have also proven effective for symbolic regression (SR); however, the internal mechanisms underlying their generation of mathematical operators remain largely unexplored. Although mechanistic interpretability has successfully identified circuits in language and vision models, it has not yet been applied to SR. In this article, we introduce PATCHES, an evolutionary circuit discovery algorithm that identifies compact and correct circuits for SR. Using PATCHES, we isolate 28 circuits, providing the first circuit-level characterisation of an SR transformer. We validate these findings through a robust causal evaluation framework based on key notions such as faithfulness, completeness, and minimality. Our analysis shows that mean patching with performance-based evaluation most reliably isolates functionally correct circuits. In contrast, we demonstrate that direct logit attribution and probing classifiers primarily capture correlational features rather than causal ones, limiting their utility for circuit discovery. Overall, these results establish SR as a high-potential application domain for mechanistic interpretability and propose a principled methodology for circuit discovery.</li>
<li><strong>摘要：</strong>继在许多领域取得成功之后，Transformer 也被证明对符号回归 (SR) 有效；然而，其生成数学运算符的内部机制在很大程度上仍未被探索。尽管机械可解释性已成功识别语言和视觉模型中的回路，但尚未应用于 SR。在本文中，我们介绍 PATCHES，这是一种进化电路发现算法，可识别 SR 的紧凑且正确的电路。使用 PATCHES，我们隔离了 28 个电路，提供了 SR 变压器的第一个电路级表征。我们通过基于忠实性、完整性和最小性等关键概念的强大因果评估框架来验证这些发现。我们的分析表明，使用基于性能的评估进行平均修补可以最可靠地隔离功能正确的电路。相比之下，我们证明直接 Logit 归因和探测分类器主要捕获相关特征而不是因果特征，限制了它们在电路发现中的实用性。总体而言，这些结果将 SR 确立为机械可解释性的高潜力应用领域，并提出了电路发现的原则方法。</li>
</ul>

<h3>Title: Semantic Routing: Exploring Multi-Layer LLM Feature Weighting for Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Bozhou Li, Yushuo Guan, Haolin Li, Bohan Zeng, Yiyan Ji, Yue Ding, Pengfei Wan, Kun Gai, Yuanxing Zhang, Wentao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03510">https://arxiv.org/abs/2602.03510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03510">https://arxiv.org/pdf/2602.03510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03510]] Semantic Routing: Exploring Multi-Layer LLM Feature Weighting for Diffusion Transformers(https://arxiv.org/abs/2602.03510)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent DiT-based text-to-image models increasingly adopt LLMs as text encoders, yet text conditioning remains largely static and often utilizes only a single LLM layer, despite pronounced semantic hierarchy across LLM layers and non-stationary denoising dynamics over both diffusion time and network depth. To better match the dynamic process of DiT generation and thereby enhance the diffusion model's generative capability, we introduce a unified normalized convex fusion framework equipped with lightweight gates to systematically organize multi-layer LLM hidden states via time-wise, depth-wise, and joint fusion. Experiments establish Depth-wise Semantic Routing as the superior conditioning strategy, consistently improving text-image alignment and compositional generation (e.g., +9.97 on the GenAI-Bench Counting task). Conversely, we find that purely time-wise fusion can paradoxically degrade visual generation fidelity. We attribute this to a train-inference trajectory mismatch: under classifier-free guidance, nominal timesteps fail to track the effective SNR, causing semantically mistimed feature injection during inference. Overall, our results position depth-wise routing as a strong and effective baseline and highlight the critical need for trajectory-aware signals to enable robust time-dependent conditioning.</li>
<li><strong>摘要：</strong>最近基于 DiT 的文本到图像模型越来越多地采用 LLM 作为文本编码器，但文本调节在很大程度上仍然是静态的，并且通常仅使用单个 LLM 层，尽管跨 LLM 层的语义层次结构明显，并且在扩散时间和网络深度上都具有非平稳的去噪动态。为了更好地匹配DiT生成的动态过程，从而增强扩散模型的生成能力，我们引入了一个配备轻量级门的统一归一化凸融合框架，通过时间、深度和联合融合系统地组织多层LLM隐藏状态。实验将深度语义路由确立为卓越的调节策略，持续改进文本图像对齐和组合生成（例如，在 GenAI-Bench 计数任务上为 +9.97）。相反，我们发现纯粹的时间融合会矛盾地降低视觉生成保真度。我们将此归因于训练推理轨迹不匹配：在无分类器指导下，标称时间步长无法跟踪有效 SNR，导致推理过程中语义上的特征注入不及时。总体而言，我们的结果将深度路由定位为强大而有效的基线，并强调了对轨迹感知信号以实现强大的时间相关调节的迫切需求。</li>
</ul>

<h3>Title: PnP-U3D: Plug-and-Play 3D Framework Bridging Autoregression and Diffusion for Unified Understanding and Generation</h3>
<ul>
<li><strong>Authors: </strong>Yongwei Chen, Tianyi Wei, Yushi Lan, Zhaoyang Lyu, Shangchen Zhou, Xudong Xu, Xingang Pan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03533">https://arxiv.org/abs/2602.03533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03533">https://arxiv.org/pdf/2602.03533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03533]] PnP-U3D: Plug-and-Play 3D Framework Bridging Autoregression and Diffusion for Unified Understanding and Generation(https://arxiv.org/abs/2602.03533)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The rapid progress of large multimodal models has inspired efforts toward unified frameworks that couple understanding and generation. While such paradigms have shown remarkable success in 2D, extending them to 3D remains largely underexplored. Existing attempts to unify 3D tasks under a single autoregressive (AR) paradigm lead to significant performance degradation due to forced signal quantization and prohibitive training cost. Our key insight is that the essential challenge lies not in enforcing a unified autoregressive paradigm, but in enabling effective information interaction between generation and understanding while minimally compromising their inherent capabilities and leveraging pretrained models to reduce training cost. Guided by this perspective, we present the first unified framework for 3D understanding and generation that combines autoregression with diffusion. Specifically, we adopt an autoregressive next-token prediction paradigm for 3D understanding, and a continuous diffusion paradigm for 3D generation. A lightweight transformer bridges the feature space of large language models and the conditional space of 3D diffusion models, enabling effective cross-modal information exchange while preserving the priors learned by standalone models. Extensive experiments demonstrate that our framework achieves state-of-the-art performance across diverse 3D understanding and generation benchmarks, while also excelling in 3D editing tasks. These results highlight the potential of unified AR+diffusion models as a promising direction for building more general-purpose 3D intelligence.</li>
<li><strong>摘要：</strong>大型多模态模型的快速进展激发了人们对将理解和生成结合起来的统一框架的努力。虽然此类范式在 2D 领域取得了显着的成功，但将其扩展到 3D 领域仍然很大程度上尚未得到充分探索。由于强制信号量化和过高的训练成本，现有的在单一自回归 (AR) 范式下统一 3D 任务的尝试会导致性能显着下降。我们的主要见解是，根本的挑战不在于执行统一的自回归范式，而在于在生成和理解之间实现有效的信息交互，同时最大限度地降低其固有能力并利用预训练模型来降低训练成本。在此视角的指导下，我们提出了第一个将自回归与扩散相结合的 3D 理解和生成统一框架。具体来说，我们采用自回归下一个令牌预测范式进行 3D 理解，并采用连续扩散范式进行 3D 生成。轻量级 Transformer 连接了大型语言模型的特征空间和 3D 扩散模型的条件空间，实现了有效的跨模态信息交换，同时保留了独立模型学到的先验知识。大量实验表明，我们的框架在不同的 3D 理解和生成基准中实现了最先进的性能，同时在 3D 编辑任务中也表现出色。这些结果凸显了统一 A​​R+扩散模型作为构建更通用 3D 智能的有前途方向的潜力。</li>
</ul>

<h3>Title: ELIQ: A Label-Free Framework for Quality Assessment of Evolving AI-Generated Images</h3>
<ul>
<li><strong>Authors: </strong>Xinyue Li, Zhiming Xu, Zhichao Zhang, Zhaolin Cai, Sijing Wu, Xiongkuo Min, Yitong Chen, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03558">https://arxiv.org/abs/2602.03558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03558">https://arxiv.org/pdf/2602.03558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03558]] ELIQ: A Label-Free Framework for Quality Assessment of Evolving AI-Generated Images(https://arxiv.org/abs/2602.03558)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative, quality assessment</a></li>
<li><strong>Abstract: </strong>Generative text-to-image models are advancing at an unprecedented pace, continuously shifting the perceptual quality ceiling and rendering previously collected labels unreliable for newer generations. To address this, we present ELIQ, a Label-free Framework for Quality Assessment of Evolving AI-generated Images. Specifically, ELIQ focuses on visual quality and prompt-image alignment, automatically constructs positive and aspect-specific negative pairs to cover both conventional distortions and AIGC-specific distortion modes, enabling transferable supervision without human annotations. Building on these pairs, ELIQ adapts a pre-trained multimodal model into a quality-aware critic via instruction tuning and predicts two-dimensional quality using lightweight gated fusion and a Quality Query Transformer. Experiments across multiple benchmarks demonstrate that ELIQ consistently outperforms existing label-free methods, generalizes from AI-generated content (AIGC) to user-generated content (UGC) scenarios without modification, and paves the way for scalable and label-free quality assessment under continuously evolving generative models. The code will be released upon publication.</li>
<li><strong>摘要：</strong>生成文本到图像的模型正在以前所未有的速度发展，不断改变感知质量上限，并使以前收集的标签对新一代来说变得不可靠。为了解决这个问题，我们提出了 ELIQ，一种用于对不断变化的人工智能生成图像进行质量评估的无标签框架。具体来说，ELIQ 专注于视觉质量和提示图像对齐，自动构建正对和特定方面的负对，以涵盖传统的失真和 AIGC 特定的失真模式，从而无需人工注释即可实现可转移的监督。在这些对的基础上，ELIQ 通过指令调整将预先训练的多模态模型调整为质量感知批评者，并使用轻量级门控融合和质量查询转换器来预测二维质量。跨多个基准的实验表明，ELIQ 始终优于现有的无标签方法，无需修改即可从人工智能生成内容 (AIGC) 推广到用户生成内容 (UGC) 场景，并为不断发展的生成模型下的可扩展和无标签质量评估铺平了道路。该代码将在发布后发布。</li>
</ul>

<h3>Title: CoGenCast: A Coupled Autoregressive-Flow Generative Framework for Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Yaguo Liu, Mingyue Cheng, Daoyu Wang, Xiaoyu Tao, Qi Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03564">https://arxiv.org/abs/2602.03564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03564">https://arxiv.org/pdf/2602.03564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03564]] CoGenCast: A Coupled Autoregressive-Flow Generative Framework for Time Series Forecasting(https://arxiv.org/abs/2602.03564)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Time series forecasting can be viewed as a generative problem that requires both semantic understanding over contextual conditions and stochastic modeling of continuous temporal dynamics. Existing approaches typically rely on either autoregressive large language models (LLMs) for semantic context modeling or diffusion-like models for continuous probabilistic generation. However, neither method alone can adequately model both aspects simultaneously. In this work, we propose CoGenCast, a hybrid generative framework that couples pre-trained LLMs with flow-matching mechanism for effective time series forecasting. Specifically, we reconfigure pre-trained decoder-only LLMs into a native forecasting encoder-decoder backbone by modifying only the attention topology, enabling bidirectional context encoding and causal representation generation. Building on this, a flow-matching mechanism is further integrated to model temporal evolution, capturing continuous stochastic dynamics conditioned on the autoregressively generated representation. Notably, CoGenCast naturally supports multimodal forecasting and cross-domain unified training. Extensive experiments on multiple benchmarks show that CoGenCast consistently outperforms previous compared baselines. Code is available at this https URL.</li>
<li><strong>摘要：</strong>时间序列预测可以被视为一个生成问题，既需要对上下文条件进行语义理解，又需要对连续时间动态进行随机建模。现有方法通常依赖于用于语义上下文建模的自回归大语言模型（LLM）或用于连续概率生成的类扩散模型。然而，单独使用这两种方法都无法同时充分模拟这两个方面。在这项工作中，我们提出了 CoGenCast，这是一种混合生成框架，它将预先训练的 LLM 与流程匹配机制结合起来，以进行有效的时间序列预测。具体来说，我们通过仅修改注意力拓扑，将预训练的仅解码器 LLM 重新配置为本机预测编码器-解码器主干，从而实现双向上下文编码和因果表示生成。在此基础上，进一步集成了流匹配机制来模拟时间演化，捕获以自回归生成的表示为条件的连续随机动态。值得注意的是，CoGenCast天然支持多模态预测和跨域统一训练。对多个基准的广泛实验表明，CoGenCast 始终优于之前的比较基准。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Riemannian Neural Optimal Transport</h3>
<ul>
<li><strong>Authors: </strong>Alessandro Micheli, Yueqi Cao, Anthea Monod, Samir Bhatt</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03566">https://arxiv.org/abs/2602.03566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03566">https://arxiv.org/pdf/2602.03566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03566]] Riemannian Neural Optimal Transport(https://arxiv.org/abs/2602.03566)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Computational optimal transport (OT) offers a principled framework for generative modeling. Neural OT methods, which use neural networks to learn an OT map (or potential) from data in an amortized way, can be evaluated out of sample after training, but existing approaches are tailored to Euclidean geometry. Extending neural OT to high-dimensional Riemannian manifolds remains an open challenge. In this paper, we prove that any method for OT on manifolds that produces discrete approximations of transport maps necessarily suffers from the curse of dimensionality: achieving a fixed accuracy requires a number of parameters that grows exponentially with the manifold dimension. Motivated by this limitation, we introduce Riemannian Neural OT (RNOT) maps, which are continuous neural-network parameterizations of OT maps on manifolds that avoid discretization and incorporate geometric structure by construction. Under mild regularity assumptions, we prove that RNOT maps approximate Riemannian OT maps with sub-exponential complexity in the dimension. Experiments on synthetic and real datasets demonstrate improved scalability and competitive performance relative to discretization-based baselines.</li>
<li><strong>摘要：</strong>计算最优传输 (OT) 为生成建模提供了原则框架。神经 OT 方法使用神经网络以摊销方式从数据中学习 OT 图（或势），可以在训练后从样本中进行评估，但现有方法是针对欧几里得几何量身定制的。将神经 OT 扩展到高维黎曼流形仍然是一个开放的挑战。在本文中，我们证明任何在流形上进行 OT 的、产生离散近似传输图的方法都必然会受到维数灾难的影响：实现固定的精度需要许多参数随着流形维数呈指数增长。受此限制的启发，我们引入了黎曼神经 OT (RNOT) 图，它是流形上 OT 图的连续神经网络参数化，避免离散化并通过构造合并几何结构。在温和的规律性假设下，我们证明 RNOT 映射近似于黎曼 OT 映射，其维度复杂度为次指数。对合成数据集和真实数据集的实验表明，相对于基于离散化的基线，可扩展性和竞争性能得到了改善。</li>
</ul>

<h3>Title: EVE: Efficient Verification of Data Erasure through Customized Perturbation in Approximate Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Weiqi Wang, Zhiyi Tian, Chenhan Zhang, Luoyu Chen, Shui Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03567">https://arxiv.org/abs/2602.03567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03567">https://arxiv.org/pdf/2602.03567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03567]] EVE: Efficient Verification of Data Erasure through Customized Perturbation in Approximate Unlearning(https://arxiv.org/abs/2602.03567)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Verifying whether the machine unlearning process has been properly executed is critical but remains underexplored. Some existing approaches propose unlearning verification methods based on backdooring techniques. However, these methods typically require participation in the model's initial training phase to backdoor the model for later verification, which is inefficient and impractical. In this paper, we propose an efficient verification of erasure method (EVE) for verifying machine unlearning without requiring involvement in the model's initial training process. The core idea is to perturb the unlearning data to ensure the model prediction of the specified samples will change before and after unlearning with perturbed data. The unlearning users can leverage the observation of the changes as a verification signal. Specifically, the perturbations are designed with two key objectives: ensuring the unlearning effect and altering the unlearned model's prediction of target samples. We formalize the perturbation generation as an adversarial optimization problem, solving it by aligning the unlearning gradient with the gradient of boundary change for target samples. We conducted extensive experiments, and the results show that EVE can verify machine unlearning without involving the model's initial training process, unlike backdoor-based methods. Moreover, EVE significantly outperforms state-of-the-art unlearning verification methods, offering significant speedup in efficiency while enhancing verification accuracy. The source code of EVE is released at \uline{this https URL}, providing a novel tool for verification of machine unlearning.</li>
<li><strong>摘要：</strong>验证机器取消学习过程是否已正确执行至关重要，但仍未得到充分探索。一些现有方法提出了基于后门技术的遗忘验证方法。然而，这些方法通常需要参与模型的初始训练阶段，为模型设置后门以供后续验证，这是低效且不切实际的。在本文中，我们提出了一种有效的擦除验证方法（EVE），用于验证机器取消学习，而无需参与模型的初始训练过程。其核心思想是扰动失学习数据，以保证指定样本的模型预测在扰动数据失学习前后发生变化。忘记学习的用户可以利用对变化的观察作为验证信号。具体来说，扰动的设计有两个关键目标：确保遗忘效果和改变未学习模型对目标样本的预测。我们将扰动生成形式化为对抗性优化问题，通过将未学习梯度与目标样本的边界变化梯度对齐来解决它。我们进行了大量的实验，结果表明，与基于后门的方法不同，EVE 可以在不涉及模型初始训练过程的情况下验证机器失学习。此外，EVE 的性能显着优于最先进的免学习验证方法，在提高验证准确性的同时显着提高效率。 EVE 的源代码发布于 \uline{此 https URL}，为验证机器遗忘提供了一种新颖的工具。</li>
</ul>

<h3>Title: Optimization and Generation in Aerodynamics Inverse Design</h3>
<ul>
<li><strong>Authors: </strong>Huaguan Chen, Ning Lin, Luxi Chen, Rui Zhang, Wenbing Huang, Chongxuan Li, Hao Sun</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03582">https://arxiv.org/abs/2602.03582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03582">https://arxiv.org/pdf/2602.03582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03582]] Optimization and Generation in Aerodynamics Inverse Design(https://arxiv.org/abs/2602.03582)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Inverse design with physics-based objectives is challenging because it couples high-dimensional geometry with expensive simulations, as exemplified by aerodynamic shape optimization for drag reduction. We revisit inverse design through two canonical solutions, the optimal design point and the optimal design distribution, and relate them to optimization and guided generation. Building on this view, we propose a new training loss for cost predictors and a density-gradient optimization method that improves objectives while preserving plausible shapes. We further unify existing training-free guided generation methods. To address their inability to approximate conditional covariance in high dimensions, we develop a time- and memory-efficient algorithm for approximate covariance estimation. Experiments on a controlled 2D study and high-fidelity 3D aerodynamic benchmarks (car and aircraft), validated by OpenFOAM simulations and miniature wind-tunnel tests with 3D-printed prototypes, demonstrate consistent gains in both optimization and guided generation. Additional offline RL results further support the generality of our approach.</li>
<li><strong>摘要：</strong>基于物理目标的逆向设计具有挑战性，因为它将高维几何与昂贵的模拟结合在一起，例如用于减阻的空气动力学形状优化。我们通过两个典型的解决方案（最优设计点和最优设计分布）重新审视逆向设计，并将它们与优化和引导生成联系起来。基于这一观点，我们提出了一种新的成本预测器训练损失和密度梯度优化方法，可以在保留合理形状的同时改进目标。我们进一步统一现有的免训练引导生成方法。为了解决它们无法在高维中近似条件协方差的问题，我们开发了一种节省时间和内存的近似协方差估计算法。受控 2D 研究和高保真 3D 空气动力学基准（汽车和飞机）实验，通过 OpenFOAM 模拟和 3D 打印原型的微型风洞测试进行验证，证明了优化和引导生成方面的一致增益。额外的离线强化学习结果进一步支持了我们方法的通用性。</li>
</ul>

<h3>Title: A Lightweight Library for Energy-Based Joint-Embedding Predictive Architectures</h3>
<ul>
<li><strong>Authors: </strong>Basile Terver, Randall Balestriero, Megi Dervishi, David Fan, Quentin Garrido, Tushar Nagarajan, Koustuv Sinha, Wancong Zhang, Mike Rabbat, Yann LeCun, Amir Bar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03604">https://arxiv.org/abs/2602.03604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03604">https://arxiv.org/pdf/2602.03604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03604]] A Lightweight Library for Energy-Based Joint-Embedding Predictive Architectures(https://arxiv.org/abs/2602.03604)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present EB-JEPA, an open-source library for learning representations and world models using Joint-Embedding Predictive Architectures (JEPAs). JEPAs learn to predict in representation space rather than pixel space, avoiding the pitfalls of generative modeling while capturing semantically meaningful features suitable for downstream tasks. Our library provides modular, self-contained implementations that illustrate how representation learning techniques developed for image-level self-supervised learning can transfer to video, where temporal dynamics add complexity, and ultimately to action-conditioned world models, where the model must additionally learn to predict the effects of control inputs. Each example is designed for single-GPU training within a few hours, making energy-based self-supervised learning accessible for research and education. We provide ablations of JEA components on CIFAR-10. Probing these representations yields 91% accuracy, indicating that the model learns useful features. Extending to video, we include a multi-step prediction example on Moving MNIST that demonstrates how the same principles scale to temporal modeling. Finally, we show how these representations can drive action-conditioned world models, achieving a 97% planning success rate on the Two Rooms navigation task. Comprehensive ablations reveal the critical importance of each regularization component for preventing representation collapse. Code is available at this https URL.</li>
<li><strong>摘要：</strong>我们推出了 EB-JEPA，这是一个使用联合嵌入预测架构 (JEPA) 学习表示和世界模型的开源库。 JEPA 学习在表示空间而不是像素空间中进行预测，避免生成建模的陷阱，同时捕获适合下游任务的语义上有意义的特征。我们的库提供了模块化、独立的实现，说明了为图像级自监督学习开发的表示学习技术如何转移到视频（其中时间动态增加了复杂性），并最终转移到动作条件的世界模型（其中模型必须另外学习预测控制输入的效果）。每个示例都专为在几个小时内进行单 GPU 训练而设计，使基于能量的自我监督学习可用于研究和教育。我们在 CIFAR-10 上提供 JEA 组件的消融。探测这些表示的准确率达到 91%，这表明该模型学习了有用的特征。扩展到视频，我们在 Moving MNIST 上提供了一个多步骤预测示例，演示了相同的原理如何扩展到时间建模。最后，我们展示了这些表示如何驱动动作条件世界模型，从而在两室导航任务中实现 97% 的规划成功率。全面的消融揭示了每个正则化组件对于防止表示崩溃的至关重要性。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Multi-Objective Optimization for Synthetic-to-Real Style Transfer</h3>
<ul>
<li><strong>Authors: </strong>Estelle Chigot, Thomas Oberlin, Manon Huguenin, Dennis Wilson</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03625">https://arxiv.org/abs/2602.03625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03625">https://arxiv.org/pdf/2602.03625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03625]] Multi-Objective Optimization for Synthetic-to-Real Style Transfer(https://arxiv.org/abs/2602.03625)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation networks require large amounts of pixel-level annotated data, which are costly to obtain for real-world images. Computer graphics engines can generate synthetic images alongside their ground-truth annotations. However, models trained on such images can perform poorly on real images due to the domain gap between real and synthetic images. Style transfer methods can reduce this difference by applying a realistic style to synthetic images. Choosing effective data transformations and their sequence is difficult due to the large combinatorial search space of style transfer operators. Using multi-objective genetic algorithms, we optimize pipelines to balance structural coherence and style similarity to target domains. We study the use of paired-image metrics on individual image samples during evolution to enable rapid pipeline evaluation, as opposed to standard distributional metrics that require the generation of many images. After optimization, we evaluate the resulting Pareto front using distributional metrics and segmentation performance. We apply this approach to standard datasets in synthetic-to-real domain adaptation: from the video game GTA5 to real image datasets Cityscapes and ACDC, focusing on adverse conditions. Results demonstrate that evolutionary algorithms can propose diverse augmentation pipelines adapted to different objectives. The contribution of this work is the formulation of style transfer as a sequencing problem suitable for evolutionary optimization and the study of efficient metrics that enable feasible search in this space. The source code is available at: this https URL.</li>
<li><strong>摘要：</strong>语义分割网络需要大量的像素级注释数据，而对于现实世界的图像来说，获取这些数据的成本很高。计算机图形引擎可以生成合成图像及其真实注释。然而，由于真实图像和合成图像之间的域差距，在此类图像上训练的模型可能在真实图像上表现不佳。风格迁移方法可以通过将逼真的风格应用于合成图像来减少这种差异。由于风格转移算子的组合搜索空间很大，选择有效的数据转换及其顺序很困难。使用多目标遗传算法，我们优化管道以平衡目标领域的结构一致性和风格相似性。我们研究在进化过程中对单个图像样本使用配对图像度量，以实现快速管道评估，而不是需要生成许多图像的标准分布度量。优化后，我们使用分布指标和分割性能评估生成的帕累托前沿。我们将这种方法应用于合成到真实域适应中的标准数据集：从视频游戏 GTA5 到真实图像数据集 Cityscapes 和 ACDC，重点关注不利条件。结果表明，进化算法可以提出适应不同目标的多种增强管道。这项工作的贡献是将风格迁移表述为适合进化优化的排序问题，以及研究在该空间中实现可行搜索的有效指标。源代码位于：此 https URL。</li>
</ul>

<h3>Title: Ultra Fast PDE Solving via Physics Guided Few-step Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Cindy Xiangrui Kong, Yueqi Wang, Haoyang Zheng, Weijian Luo, Guang Lin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03627">https://arxiv.org/abs/2602.03627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03627">https://arxiv.org/pdf/2602.03627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03627]] Ultra Fast PDE Solving via Physics Guided Few-step Diffusion(https://arxiv.org/abs/2602.03627)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion-based models have demonstrated impressive accuracy and generalization in solving partial differential equations (PDEs). However, they still face significant limitations, such as high sampling costs and insufficient physical consistency, stemming from their many-step iterative sampling mechanism and lack of explicit physics constraints. To address these issues, we propose Phys-Instruct, a novel physics-guided distillation framework which not only (1) compresses a pre-trained diffusion PDE solver into a few-step generator via matching generator and prior diffusion distributions to enable rapid sampling, but also (2) enhances the physics consistency by explicitly injecting PDE knowledge through a PDE distillation guidance. Physic-Instruct is built upon a solid theoretical foundation, leading to a practical physics-constrained training objective that admits tractable gradients. Across five PDE benchmarks, Phys-Instruct achieves orders-of-magnitude faster inference while reducing PDE error by more than 8 times compared to state-of-the-art diffusion baselines. Moreover, the resulting unconditional student model functions as a compact prior, enabling efficient and physically consistent inference for various downstream conditional tasks. Our results indicate that Phys-Instruct is a novel, effective, and efficient framework for ultra-fast PDE solving powered by deep generative models.</li>
<li><strong>摘要：</strong>基于扩散的模型在求解偏微分方程 (PDE) 方面表现出了令人印象深刻的准确性和泛化性。然而，由于多步迭代采样机制和缺乏明确的物理约束，它们仍然面临着重大限制，例如采样成本高和物理一致性不足。为了解决这些问题，我们提出了 Phys-Instruct，一种新颖的物理引导蒸馏框架，它不仅 (1) 通过匹配生成器和先验扩散分布将预训练的扩散 PDE 求解器压缩为几步生成器以实现快速采样，而且 (2) 通过 PDE 蒸馏指导显式注入 PDE 知识来增强物理一致性。 Physic-Instruct 建立在坚实的理论基础之上，可实现可处理梯度的实用物理约束训练目标。在五个 PDE 基准中，Phys-Instruct 实现了几个数量级的推理速度，同时与最先进的扩散基准相比，PDE 误差减少了 8 倍以上。此外，生成的无条件学生模型充当紧凑先验，为各种下游条件任务提供高效且物理一致的推理。我们的结果表明，Phys-Instruct 是一种新颖、有效且高效的框架，用于由深度生成模型支持的超快速 PDE 求解。</li>
</ul>

<h3>Title: CTTVAE: Latent Space Structuring for Conditional Tabular Data Generation on Imbalanced Datasets</h3>
<ul>
<li><strong>Authors: </strong>Milosh Devic, Jordan Gierschendorf, David Garson</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03641">https://arxiv.org/abs/2602.03641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03641">https://arxiv.org/pdf/2602.03641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03641]] CTTVAE: Latent Space Structuring for Conditional Tabular Data Generation on Imbalanced Datasets(https://arxiv.org/abs/2602.03641)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generating synthetic tabular data under severe class imbalance is essential for domains where rare but high-impact events drive decision-making. However, most generative models either overlook minority groups or fail to produce samples that are useful for downstream learning. We introduce CTTVAE, a Conditional Transformer-based Tabular Variational Autoencoder equipped with two complementary mechanisms: (i) a class-aware triplet margin loss that restructures the latent space for sharper intra-class compactness and inter-class separation, and (ii) a training-by-sampling strategy that adaptively increases exposure to underrepresented groups. Together, these components form CTTVAE+TBS, a framework that consistently yields more representative and utility-aligned samples without destabilizing training. Across six real-world benchmarks, CTTVAE+TBS achieves the strongest downstream utility on minority classes, often surpassing models trained on the original imbalanced data while maintaining competitive fidelity and bridging the gap for privacy for interpolation-based sampling methods and deep generative methods. Ablation studies further confirm that both latent structuring and targeted sampling contribute to these gains. By explicitly prioritizing downstream performance in rare categories, CTTVAE+TBS provides a robust and interpretable solution for conditional tabular data generation, with direct applicability to industries such as healthcare, fraud detection, and predictive maintenance where even small gains in minority cases can be critical.</li>
<li><strong>摘要：</strong>对于罕见但高影响力事件驱动决策的领域来说，在严重类别不平衡的情况下生成合成表格数据至关重要。然而，大多数生成模型要么忽视少数群体，要么无法生成对下游学习有用的样本。我们引入了 CTTVAE，一种基于条件变换器的表格变分自动编码器，配备了两种互补机制：（i）类感知三元组边缘损失，可重构潜在空间以实现更清晰的类内紧凑性和类间分离；（ii）采样训练策略，可自适应地增加对代表性不足群体的暴露。这些组件共同构成了 CTTVAE+TBS，这个框架能够持续生成更具代表性和实用性的样本，而不会破坏训练的稳定性。在六个现实世界基准中，CTTVAE+TBS 在少数类别上实现了最强的下游效用，通常超越了在原始不平衡数据上训练的模型，同时保持了竞争保真度并弥合了基于插值的采样方法和深度生成方法的隐私差距。消融研究进一步证实，潜在的结构化和有针对性的抽样都有助于这些成果。通过明确优先考虑稀有类别中的下游性能，CTTVAE+TBS 为条件表格数据生成提供了强大且可解释的解决方案，可直接应用于医疗保健、欺诈检测和预测性维护等行业，在这些行业中，即使少数情况下的微小收益也可能至关重要。</li>
</ul>

<h3>Title: Reinforcement Fine-Tuning for History-Aware Dense Retriever in RAG</h3>
<ul>
<li><strong>Authors: </strong>Yicheng Zhang, Zhen Qin, Zhaomin Wu, Wenqi Zhang, Shuiguang Deng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03645">https://arxiv.org/abs/2602.03645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03645">https://arxiv.org/pdf/2602.03645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03645]] Reinforcement Fine-Tuning for History-Aware Dense Retriever in RAG(https://arxiv.org/abs/2602.03645)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) enables large language models (LLMs) to produce evidence-based responses, and its performance hinges on the matching between the retriever and LLMs. Retriever optimization has emerged as an efficient alternative to fine-tuning LLMs. However, existing solutions suffer from objective mismatch between retriever optimization and the goal of RAG pipeline. Reinforcement learning (RL) provides a promising solution to address this limitation, yet applying RL to retriever optimization introduces two fundamental challenges: 1) the deterministic retrieval is incompatible with RL formulations, and 2) state aliasing arises from query-only retrieval in multi-hop reasoning. To address these challenges, we replace deterministic retrieval with stochastic sampling and formulate RAG as a Markov decision process, making retriever optimizable by RL. Further, we incorporate retrieval history into the state at each retrieval step to mitigate state aliasing. Extensive experiments across diverse RAG pipelines, datasets, and retriever scales demonstrate consistent improvements of our approach in RAG performance.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 使大型语言模型 (LLM) 能够产生基于证据的响应，其性能取决于检索器和 LLM 之间的匹配。检索器优化已成为微调 LLM 的有效替代方案。然而，现有解决方案存在检索器优化与 RAG 管道目标之间客观不匹配的问题。强化学习 (RL) 为解决这一限制提供了一种很有前景的解决方案，但将 RL 应用于检索器优化会带来两个基本挑战：1）确定性检索与 RL 公式不兼容；2）多跳推理中的仅查询检索会产生状态混叠。为了应对这些挑战，我们用随机采样代替确定性检索，并将 RAG 制定为马尔可夫决策过程，使检索器可以通过 RL 进行优化。此外，我们在每个检索步骤将检索历史合并到状态中，以减轻状态混叠。跨不同 RAG 管道、数据集和检索器规模的广泛实验证明了我们的方法在 RAG 性能方面的持续改进。</li>
</ul>

<h3>Title: Efficient Training of Boltzmann Generators Using Off-Policy Log-Dispersion Regularization</h3>
<ul>
<li><strong>Authors: </strong>Henrik Schopmans, Christopher von Klitzing, Pascal Friederich</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03729">https://arxiv.org/abs/2602.03729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03729">https://arxiv.org/pdf/2602.03729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03729]] Efficient Training of Boltzmann Generators Using Off-Policy Log-Dispersion Regularization(https://arxiv.org/abs/2602.03729)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Sampling from unnormalized probability densities is a central challenge in computational science. Boltzmann generators are generative models that enable independent sampling from the Boltzmann distribution of physical systems at a given temperature. However, their practical success depends on data-efficient training, as both simulation data and target energy evaluations are costly. To this end, we propose off-policy log-dispersion regularization (LDR), a novel regularization framework that builds on a generalization of the log-variance objective. We apply LDR in the off-policy setting in combination with standard data-based training objectives, without requiring additional on-policy samples. LDR acts as a shape regularizer of the energy landscape by leveraging additional information in the form of target energy labels. The proposed regularization framework is broadly applicable, supporting unbiased or biased simulation datasets as well as purely variational training without access to target samples. Across all benchmarks, LDR improves both final performance and data efficiency, with sample efficiency gains of up to one order of magnitude.</li>
<li><strong>摘要：</strong>从非标准化概率密度中采样是计算科学的一个核心挑战。玻尔兹曼发生器是生成模型，可以在给定温度下对物理系统的玻尔兹曼分布进行独立采样。然而，它们的实际成功取决于数据高效的训练，因为模拟数据和目标能量评估都很昂贵。为此，我们提出了离策略对数离散正则化（LDR），这是一种基于对数方差目标泛化的新颖正则化框架。我们将 LDR 应用于离策略环境中，并结合标准的基于数据的训练目标，而不需要额外的在策略样本。 LDR 通过利用目标能源标签形式的附加信息，充当能源景观的形状调节器。所提出的正则化框架具有广泛的适用性，支持无偏或有偏的模拟数据集以及无需访问目标样本的纯变分训练。在所有基准测试中，LDR 都提高了最终性能和数据效率，样本效率提升高达一个数量级。</li>
</ul>

<h3>Title: Edge-Optimized Vision-Language Models for Underground Infrastructure Assessment</h3>
<ul>
<li><strong>Authors: </strong>Johny J. Lopez, Md Meftahul Ferdaus, Mahdi Abdelguerfi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03742">https://arxiv.org/abs/2602.03742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03742">https://arxiv.org/pdf/2602.03742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03742]] Edge-Optimized Vision-Language Models for Underground Infrastructure Assessment(https://arxiv.org/abs/2602.03742)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Autonomous inspection of underground infrastructure, such as sewer and culvert systems, is critical to public safety and urban sustainability. Although robotic platforms equipped with visual sensors can efficiently detect structural deficiencies, the automated generation of human-readable summaries from these detections remains a significant challenge, especially on resource-constrained edge devices. This paper presents a novel two-stage pipeline for end-to-end summarization of underground deficiencies, combining our lightweight RAPID-SCAN segmentation model with a fine-tuned Vision-Language Model (VLM) deployed on an edge computing platform. The first stage employs RAPID-SCAN (Resource-Aware Pipeline Inspection and Defect Segmentation using Compact Adaptive Network), achieving 0.834 F1-score with only 0.64M parameters for efficient defect segmentation. The second stage utilizes a fine-tuned Phi-3.5 VLM that generates concise, domain-specific summaries in natural language from the segmentation outputs. We introduce a curated dataset of inspection images with manually verified descriptions for VLM fine-tuning and evaluation. To enable real-time performance, we employ post-training quantization with hardware-specific optimization, achieving significant reductions in model size and inference latency without compromising summarization quality. We deploy and evaluate our complete pipeline on a mobile robotic platform, demonstrating its effectiveness in real-world inspection scenarios. Our results show the potential of edge-deployable integrated AI systems to bridge the gap between automated defect detection and actionable insights for infrastructure maintenance, paving the way for more scalable and autonomous inspection solutions.</li>
<li><strong>摘要：</strong>下水道和涵洞系统等地下基础设施的自主检查对于公共安全和城市可持续发展至关重要。尽管配备视觉传感器的机器人平台可以有效地检测结构缺陷，但从这些检测中自动生成人类可读的摘要仍然是一个重大挑战，特别是在资源有限的边缘设备上。本文提出了一种新颖的两级管道，用于对地下缺陷进行端到端总结，将我们的轻量级 RAPID-SCAN 分割模型与部署在边缘计算平台上的微调视觉语言模型（VLM）相结合。第一阶段采用 RAPID-SCAN（使用紧凑自适应网络的资源感知管道检查和缺陷分割），仅用 0.64M 参数即可实现 0.834 F1 分数，实现高效缺陷分割。第二阶段利用经过微调的 Phi-3.5 VLM，从分割输出中以自然语言生成简洁的、特定领域的摘要。我们引入了一个精选的检查图像数据集，其中包含用于 VLM 微调和评估的手动验证的描述。为了实现实时性能，我们采用训练后量化和特定于硬件的优化，在不影响摘要质量的情况下显着减少模型大小和推理延迟。我们在移动机器人平台上部署和评估我们的完整管道，展示其在实际检查场景中的有效性。我们的结果表明，可边缘部署的集成人工智能系统有潜力弥合自动缺陷检测和基础设施维护的可操作见解之间的差距，为更具可扩展性和自主性的检测解决方案铺平道路。</li>
</ul>

<h3>Title: LIVE: Long-horizon Interactive Video World Modeling</h3>
<ul>
<li><strong>Authors: </strong>Junchao Huang, Ziyang Ye, Xinting Hu, Tianyu He, Guiyu Zhang, Shaoshuai Shi, Jiang Bian, Li Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03747">https://arxiv.org/abs/2602.03747</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03747">https://arxiv.org/pdf/2602.03747</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03747]] LIVE: Long-horizon Interactive Video World Modeling(https://arxiv.org/abs/2602.03747)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Autoregressive video world models predict future visual observations conditioned on actions. While effective over short horizons, these models often struggle with long-horizon generation, as small prediction errors accumulate over time. Prior methods alleviate this by introducing pre-trained teacher models and sequence-level distribution matching, which incur additional computational cost and fail to prevent error propagation beyond the training horizon. In this work, we propose LIVE, a Long-horizon Interactive Video world modEl that enforces bounded error accumulation via a novel cycle-consistency objective, thereby eliminating the need for teacher-based distillation. Specifically, LIVE first performs a forward rollout from ground-truth frames and then applies a reverse generation process to reconstruct the initial state. The diffusion loss is subsequently computed on the reconstructed terminal state, providing an explicit constraint on long-horizon error propagation. Moreover, we provide an unified view that encompasses different approaches and introduce progressive training curriculum to stabilize training. Experiments demonstrate that LIVE achieves state-of-the-art performance on long-horizon benchmarks, generating stable, high-quality videos far beyond training rollout lengths.</li>
<li><strong>摘要：</strong>自回归视频世界模型预测以动作为条件的未来视觉观察。虽然这些模型在短期内有效，但在长期生成时往往会遇到困难，因为小的预测误差会随着时间的推移而累积。现有方法通过引入预训练的教师模型和序列级分布匹配来缓解这一问题，但这会产生额外的计算成本，并且无法防止错误传播超出训练范围。在这项工作中，我们提出了 LIVE，一种长视野交互式视频世界模型，它通过新颖的循环一致性目标强制限制误差累积，从而消除了基于教师的蒸馏的需要。具体来说，LIVE 首先从真实帧执行前向推出，然后应用反向生成过程来重建初始状态。随后在重建的最终状态上计算扩散损失，为长范围误差传播提供明确的约束。此外，我们提供包含不同方法的统一观点，并引入渐进式培训课程以稳定培训。实验表明，LIVE 在长期基准测试中实现了最先进的性能，生成了远远超出训练部署长度的稳定、高质量的视频。</li>
</ul>

<h3>Title: Test-Time Conditioning with Representation-Aligned Visual Features</h3>
<ul>
<li><strong>Authors: </strong>Nicolas Sereyjol-Garros, Ellington Kirby, Victor Letzelter, Victor Besnier, Nermin Samet</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03753">https://arxiv.org/abs/2602.03753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03753">https://arxiv.org/pdf/2602.03753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03753]] Test-Time Conditioning with Representation-Aligned Visual Features(https://arxiv.org/abs/2602.03753)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While representation alignment with self-supervised models has been shown to improve diffusion model training, its potential for enhancing inference-time conditioning remains largely unexplored. We introduce Representation-Aligned Guidance (REPA-G), a framework that leverages these aligned representations, with rich semantic properties, to enable test-time conditioning from features in generation. By optimizing a similarity objective (the potential) at inference, we steer the denoising process toward a conditioned representation extracted from a pre-trained feature extractor. Our method provides versatile control at multiple scales, ranging from fine-grained texture matching via single patches to broad semantic guidance using global image feature tokens. We further extend this to multi-concept composition, allowing for the faithful combination of distinct concepts. REPA-G operates entirely at inference time, offering a flexible and precise alternative to often ambiguous text prompts or coarse class labels. We theoretically justify how this guidance enables sampling from the potential-induced tilted distribution. Quantitative results on ImageNet and COCO demonstrate that our approach achieves high-quality, diverse generations. Code is available at this https URL.</li>
<li><strong>摘要：</strong>虽然与自监督模型的表示对齐已被证明可以改善扩散模型训练，但其增强推理时间调节的潜力在很大程度上仍未得到探索。我们引入了表示对齐指导（REPA-G），这是一个利用这些对齐表示和丰富语义属性的框架，能够根据生成中的特征进行测试时调节。通过优化推理时的相似性目标（潜力），我们将去噪过程转向从预先训练的特征提取器中提取的条件表示。我们的方法提供了多种尺度的多功能控制，从通过单个补丁的细粒度纹理匹配到使用全局图像特征标记的广泛语义指导。我们进一步将其扩展到多概念组合，允许不同概念的忠实组合。 REPA-G 完全在推理时运行，为通常不明确的文本提示或粗略的类标签提供灵活而精确的替代方案。我们从理论上证明了该指导如何能够从势引起的倾斜分布中进行采样。 ImageNet 和 COCO 上的定量结果表明我们的方法实现了高质量、多样化的世代。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL</h3>
<ul>
<li><strong>Authors: </strong>Ian Wu, Yuxiao Qu, Amrith Setlur, Aviral Kumar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03773">https://arxiv.org/abs/2602.03773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03773">https://arxiv.org/pdf/2602.03773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03773]] Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL(https://arxiv.org/abs/2602.03773)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) that can continually improve beyond their training budgets are able to solve increasingly difficult problems by adapting at test time, a property we refer to as extrapolation. However, standard reinforcement learning (RL) operates over fixed problem distributions and training budgets, which limits extrapolation amidst distribution shift at test time. To address this, we introduce RC, an iterative decoding algorithm that replaces standard autoregressive decoding during both training and inference. RC exploits an asymmetry between the response generation and summarization capabilities of LLMs to construct reasoning chains that consistently improve across iterations. Models trained to use RC can extrapolate and continually improve over reasoning horizons more than an order of magnitude longer than those seen during training. Empirically, training a 4B model with RC using a 16k-token training budget improves performance on HMMT 2025 from 40% to nearly 70% with 0.5m tokens at test time, outperforming both comparably sized models and many larger reasoning LLMs. Finally, we also show that models trained with RC can more effectively leverage existing scaffolds to further scale test-time performance, due to the improved summary-conditioned generation abilities learned through training.</li>
<li><strong>摘要：</strong>可以不断改进超出其培训预算的大型语言模型（LLM）能够通过在测试时进行调整来解决日益困难的问题，我们将这种特性称为外推法。然而，标准强化学习 (RL) 在固定问题分布和训练预算上运行，这限制了测试时分布变化的外推。为了解决这个问题，我们引入了 RC，一种迭代解码算法，可以在训练和推理过程中取代标准自回归解码。 RC 利用 LLM 的响应生成和总结能力之间的不对称性来构建在迭代中持续改进的推理链。经过训练使用 RC 的模型可以推断并持续改进推理范围，比训练期间的推理范围长一个数量级以上。根据经验，在测试时使用 50 万个令牌，使用 16k 令牌训练预算通过 RC 训练 4B 模型，可将 HMMT 2025 的性能从 40% 提高到近 70%，优于同等大小的模型和许多更大的推理 LLM。最后，我们还表明，由于通过训练学习到了改进的摘要条件生成能力，因此使用 RC 训练的模型可以更有效地利用现有支架来进一步扩展测试时性能。</li>
</ul>

<h3>Title: Efficient Estimation of Kernel Surrogate Models for Task Attribution</h3>
<ul>
<li><strong>Authors: </strong>Zhenshuo Zhang, Minxuan Duan, Hongyang R. Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03783">https://arxiv.org/abs/2602.03783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03783">https://arxiv.org/pdf/2602.03783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03783]] Efficient Estimation of Kernel Surrogate Models for Task Attribution(https://arxiv.org/abs/2602.03783)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Modern AI agents such as large language models are trained on diverse tasks -- translation, code generation, mathematical reasoning, and text prediction -- simultaneously. A key question is to quantify how each individual training task influences performance on a target task, a problem we refer to as task attribution. The direct approach, leave-one-out retraining, measures the effect of removing each task, but is computationally infeasible at scale. An alternative approach that builds surrogate models to predict a target task's performance for any subset of training tasks has emerged in recent literature. Prior work focuses on linear surrogate models, which capture first-order relationships, but miss nonlinear interactions such as synergy, antagonism, or XOR-type effects. In this paper, we first consider a unified task weighting framework for analyzing task attribution methods, and show a new connection between linear surrogate models and influence functions through a second-order analysis. Then, we introduce kernel surrogate models, which more effectively represent second-order task interactions. To efficiently learn the kernel surrogate, we develop a gradient-based estimation procedure that leverages a first-order approximation of pretrained models; empirically, this yields accurate estimates with less than $2\%$ relative error without repeated retraining. Experiments across multiple domains -- including math reasoning in transformers, in-context learning, and multi-objective reinforcement learning -- demonstrate the effectiveness of kernel surrogate models. They achieve a $25\%$ higher correlation with the leave-one-out ground truth than linear surrogates and influence-function baselines. When used for downstream task selection, kernel surrogate models yield a $40\%$ improvement in demonstration selection for in-context learning and multi-objective reinforcement learning benchmarks.</li>
<li><strong>摘要：</strong>现代人工智能代理（例如大型语言模型）同时接受多种任务的训练——翻译、代码生成、数学推理和文本预测。一个关键问题是量化每个单独的训练任务如何影响目标任务的表现，我们将这个问题称为任务归因。直接的方法，即留一法再训练，衡量删除每项任务的效果，但在大规模计算上是不可行的。最近的文献中出现了另一种方法，即构建代理模型来预测训练任务的任何子集的目标任务的性能。之前的工作重点是线性代理模型，它捕获一阶关系，但忽略了非线性相互作用，例如协同、拮抗或异或型效应。在本文中，我们首先考虑用于分析任务归因方法的统一任务加权框架，并通过二阶分析展示线性代理模型和影响函数之间的新联系。然后，我们引入内核代理模型，它更有效地表示二阶任务交互。为了有效地学习内核代理，我们开发了一种基于梯度的估计程序，该程序利用预训练模型的一阶近似；根据经验，这会产生准确的估计，相对误差小于 $2\%$，而无需重复重新训练。跨多个领域的实验——包括 Transformer 中的数学推理、上下文学习和多目标强化学习——证明了核代理模型的有效性。与线性替代项和影响函数基线相比，它们与留一法基本事实的相关性提高了 25\%$。当用于下游任务选择时，内核代理模型在上下文学习和多目标强化学习基准的演示选择方面产生了 40\%$ 的改进。</li>
</ul>

<h3>Title: Inference-time Unlearning Using Conformal Prediction</h3>
<ul>
<li><strong>Authors: </strong>Somnath Basu Roy Chowdhury, Rahul Kidambi, Avinava Dubey, David Wang, Gokhan Mergen, Amr Ahmed, Aranyak Mehta</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03787">https://arxiv.org/abs/2602.03787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03787">https://arxiv.org/pdf/2602.03787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03787]] Inference-time Unlearning Using Conformal Prediction(https://arxiv.org/abs/2602.03787)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Machine unlearning is the process of efficiently removing specific information from a trained machine learning model without retraining from scratch. Existing unlearning methods, which often provide provable guarantees, typically involve retraining a subset of model parameters based on a forget set. While these approaches show promise in certain scenarios, their underlying assumptions are often challenged in real-world applications -- particularly when applied to generative models. Furthermore, updating parameters using these unlearning procedures often degrades the general-purpose capabilities the model acquired during pre-training. Motivated by these shortcomings, this paper considers the paradigm of inference time unlearning -- wherein, the generative model is equipped with an (approximately correct) verifier that judges whether the model's response satisfies appropriate unlearning guarantees. This paper introduces a framework that iteratively refines the quality of the generated responses using feedback from the verifier without updating the model parameters. The proposed framework leverages conformal prediction to reduce computational overhead and provide distribution-free unlearning guarantees. This paper's approach significantly outperforms existing state-of-the-art methods, reducing unlearning error by up to 93% across challenging unlearning benchmarks.</li>
<li><strong>摘要：</strong>机器取消学习是从经过训练的机器学习模型中有效删除特定信息而无需从头开始重新训练的过程。现有的遗忘方法通常提供可证明的保证，通常涉及基于遗忘集重新训练模型参数的子集。虽然这些方法在某些情况下显示出希望，但它们的基本假设在现实应用中经常受到挑战——特别是在应用于生成模型时。此外，使用这些遗忘过程更新参数通常会降低模型在预训练期间获得的通用功能。受这些缺点的启发，本文考虑了推理时间遗忘的范式——其中，生成模型配备了一个（近似正确的）验证器，用于判断模型的响应是否满足适当的遗忘保证。本文介绍了一个框架，该框架可以使用验证者的反馈迭代地改进生成的响应的质量，而无需更新模型参数。所提出的框架利用共形预测来减少计算开销并提供无分布的遗忘保证。本文的方法显着优于现有的最先进方法，在具有挑战性的遗忘基准中将遗忘错误减少高达 93%。</li>
</ul>

<h3>Title: Should I use Synthetic Data for That? An Analysis of the Suitability of Synthetic Data for Data Sharing and Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Bogdan Kulynych, Theresa Stadler, Jean Louis Raisaro, Carmela Troncoso</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03791">https://arxiv.org/abs/2602.03791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03791">https://arxiv.org/pdf/2602.03791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03791]] Should I use Synthetic Data for That? An Analysis of the Suitability of Synthetic Data for Data Sharing and Augmentation(https://arxiv.org/abs/2602.03791)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative modelling have led many to see synthetic data as the go-to solution for a range of problems around data access, scarcity, and under-representation. In this paper, we study three prominent use cases: (1) Sharing synthetic data as a proxy for proprietary datasets to enable statistical analyses while protecting privacy, (2) Augmenting machine learning training sets with synthetic data to improve model performance, and (3) Augmenting datasets with synthetic data to reduce variance in statistical estimation. For each use case, we formalise the problem setting and study, through formal analysis and case studies, under which conditions synthetic data can achieve its intended objectives. We identify fundamental and practical limits that constrain when synthetic data can serve as an effective solution for a particular problem. Our analysis reveals that due to these limits many existing or envisioned use cases of synthetic data are a poor problem fit. Our formalisations and classification of synthetic data use cases enable decision makers to assess whether synthetic data is a suitable approach for their specific data availability problem.</li>
<li><strong>摘要：</strong>生成建模的最新进展使许多人将合成数据视为解决数据访问、稀缺和代表性不足等一系列问题的首选解决方案。在本文中，我们研究了三个突出的用例：（1）共享合成数据作为专有数据集的代理，以便在保护隐私的同时进行统计分析；（2）使用合成数据增强机器学习训练集以提高模型性能；（3）使用合成数据增强数据集以减少统计估计的方差。对于每个用例，我们通过形式分析和案例研究将问题设置和研究形式化，在这种条件下合成数据可以实现其预期目标。我们确定了合成数据何时可以作为特定问题的有效解决方案的基本和实际限制。我们的分析表明，由于这些限制，许多现有或设想的合成数据用例不太适合问题。我们对合成数据用例的形式化和分类使决策者能够评估合成数据是否适合其特定的数据可用性问题。</li>
</ul>

<h3>Title: 3D-Aware Implicit Motion Control for View-Adaptive Human Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhixue Fang, Xu He, Songlin Tang, Haoxian Zhang, Qingfeng Li, Xiaoqiang Liu, Pengfei Wan, Kun Gai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03796">https://arxiv.org/abs/2602.03796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03796">https://arxiv.org/pdf/2602.03796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03796]] 3D-Aware Implicit Motion Control for View-Adaptive Human Video Generation(https://arxiv.org/abs/2602.03796)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Existing methods for human motion control in video generation typically rely on either 2D poses or explicit 3D parametric models (e.g., SMPL) as control signals. However, 2D poses rigidly bind motion to the driving viewpoint, precluding novel-view synthesis. Explicit 3D models, though structurally informative, suffer from inherent inaccuracies (e.g., depth ambiguity and inaccurate dynamics) which, when used as a strong constraint, override the powerful intrinsic 3D awareness of large-scale video generators. In this work, we revisit motion control from a 3D-aware perspective, advocating for an implicit, view-agnostic motion representation that naturally aligns with the generator's spatial priors rather than depending on externally reconstructed constraints. We introduce 3DiMo, which jointly trains a motion encoder with a pretrained video generator to distill driving frames into compact, view-agnostic motion tokens, injected semantically via cross-attention. To foster 3D awareness, we train with view-rich supervision (i.e., single-view, multi-view, and moving-camera videos), forcing motion consistency across diverse viewpoints. Additionally, we use auxiliary geometric supervision that leverages SMPL only for early initialization and is annealed to zero, enabling the model to transition from external 3D guidance to learning genuine 3D spatial motion understanding from the data and the generator's priors. Experiments confirm that 3DiMo faithfully reproduces driving motions with flexible, text-driven camera control, significantly surpassing existing methods in both motion fidelity and visual quality.</li>
<li><strong>摘要：</strong>视频生成中现有的人体运动控制方法通常依赖 2D 姿势或显式 3D 参数模型（例如 SMPL）作为控制信号。然而，2D 将运动严格绑定到驾驶视点，从而妨碍了新颖的视图合成。显式 3D 模型虽然在结构上信息丰富，但存在固有的不准确性（例如，深度模糊性和不准确的动态），当用作强约束时，会覆盖大型视频生成器强大的内在 3D 感知能力。在这项工作中，我们从 3D 感知的角度重新审视运动控制，提倡一种隐式的、与视图无关的运动表示，它自然地与生成器的空间先验对齐，而不是依赖于外部重建的约束。我们引入了 3DiMo，它联合训练运动编码器和预先训练的视频生成器，将驱动帧提取为紧凑的、与视图无关的运动标记，并通过交叉注意力进行语义注入。为了培养 3D 意识，我们通过丰富视图的监督（即单视图、多视图和移动摄像机视频）进行训练，强制不同视点之间的运动一致性。此外，我们使用辅助几何监督，仅利用 SMPL 进行早期初始化，并退火至零，使模型能够从外部 3D 指导过渡到从数据和生成器先验中学习真正的 3D 空间运动理解。实验证实，3DiMo 通过灵活的文本驱动摄像头控制忠实地再现了驾驶动作，在运动保真度和视觉质量方面都显着超越了现有方法。</li>
</ul>

<h3>Title: Bridging Online and Offline RL: Contextual Bandit Learning for Multi-Turn Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Ziru Chen, Dongdong Chen, Ruinan Jin, Yingbin Liang, Yujia Xie, Huan Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03806">https://arxiv.org/abs/2602.03806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03806">https://arxiv.org/pdf/2602.03806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03806]] Bridging Online and Offline RL: Contextual Bandit Learning for Multi-Turn Code Generation(https://arxiv.org/abs/2602.03806)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recently, there have been significant research interests in training large language models (LLMs) with reinforcement learning (RL) on real-world tasks, such as multi-turn code generation. While online RL tends to perform better than offline RL, its higher training cost and instability hinders wide adoption. In this paper, we build on the observation that multi-turn code generation can be formulated as a one-step recoverable Markov decision process and propose contextual bandit learning with offline trajectories (Cobalt), a new method that combines the benefits of online and offline RL. Cobalt first collects code generation trajectories using a reference LLM and divides them into partial trajectories as contextual prompts. Then, during online bandit learning, the LLM is trained to complete each partial trajectory prompt through single-step code generation. Cobalt outperforms two multi-turn online RL baselines based on GRPO and VeRPO, and substantially improves R1-Distill 8B and Qwen3 8B by up to 9.0 and 6.2 absolute Pass@1 scores on LiveCodeBench. Also, we analyze LLMs' in-context reward hacking behaviors and augment Cobalt training with perturbed trajectories to mitigate this issue. Overall, our results demonstrate Cobalt as a promising solution for iterative decision-making tasks like multi-turn code generation. Our code and data are available at this https URL.</li>
<li><strong>摘要：</strong>最近，人们对在现实世界任务（例如多轮代码生成）上使用强化学习（RL）训练大型语言模型（LLM）产生了浓厚的研究兴趣。虽然在线强化学习往往比离线强化学习表现更好，但其较高的训练成本和不稳定性阻碍了广泛采用。在本文中，我们基于多轮代码生成可以被表述为一步可恢复马尔可夫决策过程的观察，并提出了具有离线轨迹的上下文强盗学习（Cobalt），这是一种结合了在线和离线强化学习优点的新方法。 Cobalt 首先使用参考 LLM 收集代码生成轨迹，并将它们分为部分轨迹作为上下文提示。然后，在在线强盗学习过程中，LLM被训练以通过单步代码生成来完成每个部分轨迹提示。 Cobalt 的性能优于基于 GRPO 和 VeRPO 的两个多轮在线 RL 基线，并且在 LiveCodeBench 上将 R1-Distill 8B 和 Qwen3 8B 的绝对 Pass@1 得分分别提高了 9.0 和 6.2。此外，我们还分析了法学硕士的上下文奖励黑客行为，并通过扰动轨迹增强 Cobalt 训练，以缓解这一问题。总的来说，我们的结果表明，Cobalt 对于多轮代码生成等迭代决策任务来说是一种很有前景的解决方案。我们的代码和数据可在此 https URL 中获取。</li>
</ul>

<h3>Title: Progressive Checkerboards for Autoregressive Multiscale Image Generation</h3>
<ul>
<li><strong>Authors: </strong>David Eigen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03811">https://arxiv.org/abs/2602.03811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03811">https://arxiv.org/pdf/2602.03811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03811]] Progressive Checkerboards for Autoregressive Multiscale Image Generation(https://arxiv.org/abs/2602.03811)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>A key challenge in autoregressive image generation is to efficiently sample independent locations in parallel, while still modeling mutual dependencies with serial conditioning. Some recent works have addressed this by conditioning between scales in a multiscale pyramid. Others have looked at parallelizing samples in a single image using regular partitions or randomized orders. In this work we examine a flexible, fixed ordering based on progressive checkerboards for multiscale autoregressive image generation. Our ordering draws samples in parallel from evenly spaced regions at each scale, maintaining full balance in all levels of a quadtree subdivision at each step. This enables effective conditioning both between and within scales. Intriguingly, we find evidence that in our balanced setting, a wide range of scale-up factors lead to similar results, so long as the total number of serial steps is constant. On class-conditional ImageNet, our method achieves competitive performance compared to recent state-of-the-art autoregressive systems with like model capacity, using fewer sampling steps.</li>
<li><strong>摘要：</strong>自回归图像生成的一个关键挑战是有效地并行采样独立位置，同时仍然通过串行条件对相互依赖关系进行建模。最近的一些工作通过在多尺度金字塔中的尺度之间进行调节来解决这个问题。其他人研究了使用常规分区或随机顺序并行化单个图像中的样本。在这项工作中，我们研究了一种基于渐进棋盘的灵活、固定排序，用于多尺度自回归图像生成。我们的排序从每个尺度的均匀间隔区域并行抽取样本，在每一步的四叉树细分的所有级别上保持完全平衡。这使得能够在尺度之间和尺度内进行有效调节。有趣的是，我们发现证据表明，在我们的平衡设置中，只要串行步骤的总数恒定，各种放大因素都会导致类似的结果。在类条件 ImageNet 上，与具有类似模型容量的最新最先进的自回归系统相比，我们的方法使用更少的采样步骤实现了具有竞争力的性能。</li>
</ul>

<h3>Title: Antidistillation Fingerprinting</h3>
<ul>
<li><strong>Authors: </strong>Yixuan Even Xu, John Kirchenbauer, Yash Savani, Asher Trockman, Alexander Robey, Tom Goldstein, Fei Fang, J. Zico Kolter</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03812">https://arxiv.org/abs/2602.03812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03812">https://arxiv.org/pdf/2602.03812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03812]] Antidistillation Fingerprinting(https://arxiv.org/abs/2602.03812)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Model distillation enables efficient emulation of frontier large language models (LLMs), creating a need for robust mechanisms to detect when a third-party student model has trained on a teacher model's outputs. However, existing fingerprinting techniques that could be used to detect such distillation rely on heuristic perturbations that impose a steep trade-off between generation quality and fingerprinting strength, often requiring significant degradation of utility to ensure the fingerprint is effectively internalized by the student. We introduce antidistillation fingerprinting (ADFP), a principled approach that aligns the fingerprinting objective with the student's learning dynamics. Building upon the gradient-based framework of antidistillation sampling, ADFP utilizes a proxy model to identify and sample tokens that directly maximize the expected detectability of the fingerprint in the student after fine-tuning, rather than relying on the incidental absorption of the un-targeted biases of a more naive watermark. Experiments on GSM8K and OASST1 benchmarks demonstrate that ADFP achieves a significant Pareto improvement over state-of-the-art baselines, yielding stronger detection confidence with minimal impact on utility, even when the student model's architecture is unknown.</li>
<li><strong>摘要：</strong>模型蒸馏可以有效模拟前沿大语言模型 (LLM)，从而需要强大的机制来检测第三方学生模型何时接受了教师模型输出的训练。然而，可用于检测这种蒸馏的现有指纹识别技术依赖于启发式扰动，这种扰动在生成质量和指纹识别强度之间强加了急剧的权衡，通常需要显着降低效用以确保指纹被学生有效地内化。我们引入了反蒸馏指纹识别（ADFP），这是一种将指纹识别目标与学生的学习动态相结合的原则性方法。 ADFP 基于基于梯度的反蒸馏采样框架，利用代理模型来识别和采样令牌，在微调后直接最大化学生指纹的预期可检测性，而不是依赖于偶然吸收更幼稚的水印的非目标偏差。 GSM8K 和 OASST1 基准测试表明，ADFP 相对于最先进的基线实现了显着的帕累托改进，即使在学生模型的架构未知的情况下，也能在对实用性影响最小的情况下产生更强的检测置信度。</li>
</ul>

<h3>Title: Continuous Control of Editing Models via Adaptive-Origin Guidance</h3>
<ul>
<li><strong>Authors: </strong>Alon Wolf, Chen Katzir, Kfir Aberman, Or Patashnik</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03826">https://arxiv.org/abs/2602.03826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03826">https://arxiv.org/pdf/2602.03826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03826]] Continuous Control of Editing Models via Adaptive-Origin Guidance(https://arxiv.org/abs/2602.03826)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion-based editing models have emerged as a powerful tool for semantic image and video manipulation. However, existing models lack a mechanism for smoothly controlling the intensity of text-guided edits. In standard text-conditioned generation, Classifier-Free Guidance (CFG) impacts prompt adherence, suggesting it as a potential control for edit intensity in editing models. However, we show that scaling CFG in these models does not produce a smooth transition between the input and the edited result. We attribute this behavior to the unconditional prediction, which serves as the guidance origin and dominates the generation at low guidance scales, while representing an arbitrary manipulation of the input content. To enable continuous control, we introduce Adaptive-Origin Guidance (AdaOr), a method that adjusts this standard guidance origin with an identity-conditioned adaptive origin, using an identity instruction corresponding to the identity manipulation. By interpolating this identity prediction with the standard unconditional prediction according to the edit strength, we ensure a continuous transition from the input to the edited result. We evaluate our method on image and video editing tasks, demonstrating that it provides smoother and more consistent control compared to current slider-based editing approaches. Our method incorporates an identity instruction into the standard training framework, enabling fine-grained control at inference time without per-edit procedure or reliance on specialized datasets.</li>
<li><strong>摘要：</strong>基于扩散的编辑模型已成为语义图像和视频操作的强大工具。然而，现有模型缺乏平稳控制文本引导编辑强度的机制。在标准文本条件生成中，无分类器指导 (CFG) 会影响提示依从性，表明它是编辑模型中编辑强度的潜在控制。然而，我们表明在这些模型中缩放 CFG 不会在输入和编辑结果之间产生平滑过渡。我们将这种行为归因于无条件预测，它充当指导原点并在低指导尺度下主导生成，同时代表对输入内容的任意操纵。为了实现连续控制，我们引入了自适应原点引导（AdaOr），这是一种使用与身份操作相对应的身份指令，通过身份条件自适应原点调整此标准引导原点的方法。通过根据编辑强度将此身份预测与标准无条件预测进行插值，我们确保从输入到编辑结果的连续过渡。我们在图像和视频编辑任务上评估了我们的方法，证明与当前基于滑块的编辑方法相比，它提供了更平滑、更一致的控制。我们的方法将身份指令纳入标准训练框架，从而在推理时实现细粒度控制，无需每次编辑程序或依赖专门的数据集。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
