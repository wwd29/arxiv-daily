<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-02</h1>
<h3>Title: Methodological Framework for Quantifying Semantic Test Coverage in RAG Systems</h3>
<ul>
<li><strong>Authors: </strong>Noah Broestl, Adel Nasser Abdalla, Rajprakash Bale, Hersh Gupta, Max Struever</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00001">https://arxiv.org/abs/2510.00001</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00001">https://arxiv.org/pdf/2510.00001</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00001]] Methodological Framework for Quantifying Semantic Test Coverage in RAG Systems(https://arxiv.org/abs/2510.00001)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reliably determining the performance of Retrieval-Augmented Generation (RAG) systems depends on comprehensive test questions. While a proliferation of evaluation frameworks for LLM-powered applications exists, current practices lack a systematic method to ensure these test sets adequately cover the underlying knowledge base, leaving developers with significant blind spots. To address this, we present a novel, applied methodology to quantify the semantic coverage of RAG test questions against their underlying documents. Our approach leverages existing technologies, including vector embeddings and clustering algorithms, to create a practical framework for validating test comprehensiveness. Our methodology embeds document chunks and test questions into a unified vector space, enabling the calculation of multiple coverage metrics: basic proximity, content-weighted coverage, and multi-topic question coverage. Furthermore, we incorporate outlier detection to filter irrelevant questions, allowing for the refinement of test sets. Experimental evidence from two distinct use cases demonstrates that our framework effectively quantifies test coverage, identifies specific content areas with inadequate representation, and provides concrete recommendations for generating new, high-value test questions. This work provides RAG developers with essential tools to build more robust test suites, thereby improving system reliability and extending to applications such as identifying misaligned documents.</li>
<li><strong>摘要：</strong>可靠地确定检索功能生成（RAG）系统的性能取决于全面的测试问题。尽管存在针对LLM驱动的应用程序的评估框架的扩散，但当前的实践缺乏系统的方法来确保这些测试集充分覆盖基本知识库，从而使开发人员拥有大量的盲点。为了解决这个问题，我们提出了一种新颖的应用方法，以量化针对其基本文件的抹布测试问题的语义介绍。我们的方法利用现有技术，包括向量嵌入和聚类算法，创建一个实用框架来验证测试的全面性。我们的方法将文档块和测试问题嵌入到统一的向量空间中，从而可以计算多个覆盖范围指标：基本接近，内容加权覆盖范围和多主题问题覆盖范围。此外，我们将离群检测纳入了无关紧要的问题，从而可以对测试集进行完善。来自两个不同用例的实验证据表明，我们的框架有效地量化了测试覆盖范围，确定了代表不足的特定内容领域，并提供了用于产生新的高价值测试问题的具体建议。这项工作为RAG开发人员提供了基本的工具来构建更强大的测试套件，从而提高了系统的可靠性并扩展到诸如识别未对准文档的应用程序。</li>
</ul>

<h3>Title: Hybrid Deep Learning for Hyperspectral Single Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Usman Muhammad, Jorma Laaksonen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00033">https://arxiv.org/abs/2510.00033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00033">https://arxiv.org/pdf/2510.00033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00033]] Hybrid Deep Learning for Hyperspectral Single Image Super-Resolution(https://arxiv.org/abs/2510.00033)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Hyperspectral single image super-resolution (SISR) is a challenging task due to the difficulty of restoring fine spatial details while preserving spectral fidelity across a wide range of wavelengths, which limits the performance of conventional deep learning models. To address this challenge, we introduce Spectral-Spatial Unmixing Fusion (SSUF), a novel module that can be seamlessly integrated into standard 2D convolutional architectures to enhance both spatial resolution and spectral integrity. The SSUF combines spectral unmixing with spectral--spatial feature extraction and guides a ResNet-based convolutional neural network for improved reconstruction. In addition, we propose a custom Spatial-Spectral Gradient Loss function that integrates mean squared error with spatial and spectral gradient components, encouraging accurate reconstruction of both spatial and spectral features. Experiments on three public remote sensing hyperspectral datasets demonstrate that the proposed hybrid deep learning model achieves competitive performance while reducing model complexity.</li>
<li><strong>摘要：</strong>高光谱单图像超分辨率（SISR）是一项具有挑战性的任务，因为难以恢复精细的空间细节，同时在各种波长范围内保留了光谱保真度，这限制了传统深度学习模型的性能。为了应对这一挑战，我们引入了频谱空间的Unmixing Fusion（SSUF），该模块可以无缝集成到标准的2D卷积体系结构中，以增强空间分辨率和光谱完整性。 SSUF结合了光谱体和光谱 - 空间特征提取，并指导基于重新连接的卷积神经网络，以改善重建。此外，我们提出了一种自定义的空间段梯度损耗函数，该函数将平方误差与空间和光谱梯度组件集成在一起，从而鼓励对空间和光谱特征的准确重建。三个公共遥感高光谱数据集的实验表明，所提出的混合深度学习模型可实现竞争性能，同时降低模型的复杂性。</li>
</ul>

<h3>Title: Review of Hallucination Understanding in Large Language and Vision Models</h3>
<ul>
<li><strong>Authors: </strong>Zhengyi Ho, Siyuan Liang, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00034">https://arxiv.org/abs/2510.00034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00034">https://arxiv.org/pdf/2510.00034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00034]] Review of Hallucination Understanding in Large Language and Vision Models(https://arxiv.org/abs/2510.00034)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The widespread adoption of large language and vision models in real-world applications has made urgent the need to address hallucinations -- instances where models produce incorrect or nonsensical outputs. These errors can propagate misinformation during deployment, leading to both financial and operational harm. Although much research has been devoted to mitigating hallucinations, our understanding of it is still incomplete and fragmented. Without a coherent understanding of hallucinations, proposed solutions risk mitigating surface symptoms rather than underlying causes, limiting their effectiveness and generalizability in deployment. To tackle this gap, we first present a unified, multi-level framework for characterizing both image and text hallucinations across diverse applications, aiming to reduce conceptual fragmentation. We then link these hallucinations to specific mechanisms within a model's lifecycle, using a task-modality interleaved approach to promote a more integrated understanding. Our investigations reveal that hallucinations often stem from predictable patterns in data distributions and inherited biases. By deepening our understanding, this survey provides a foundation for developing more robust and effective solutions to hallucinations in real-world generative AI systems.</li>
<li><strong>摘要：</strong>在现实世界应用中，大型语言和视觉模型的广泛采用使得迫切需要解决幻觉 - 模型产生不正确或荒谬的产出的实例。这些错误可能会在部署期间传播错误信息，从而导致财务和运营危害。尽管许多研究专门用于减轻幻觉，但我们对它的理解仍然不完整和分散。没有对幻觉的一致理解，建议的解决方案可能会减轻表面症状而不是基本原因，从而限制了它们在部署中的有效性和普遍性。为了解决这一差距，我们首先提出了一个统一的多层次框架，用于表征各种应用程序之间的图像和文本幻觉，旨在减少概念分散。然后，我们使用任务模式交织方法来促进更一体化的理解，将这些幻觉与模型生命周期内的特定机制联系起来。我们的研究表明，幻觉通常源于数据分布和遗传偏见的可预测模式。通过加深我们的理解，这项调查为在现实世界中生成的AI系统中开发了更强大，更有效的解决方案提供了基础。</li>
</ul>

<h3>Title: Culture In a Frame: C$^3$B as a Comic-Based Benchmark for Multimodal Culturally Awareness</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Song, Andong Chen, Wenxin Zhu, Kehai Chen, Xuefeng Bai, Muyun Yang, Tiejun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00041">https://arxiv.org/abs/2510.00041</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00041">https://arxiv.org/pdf/2510.00041</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00041]] Culture In a Frame: C$^3$B as a Comic-Based Benchmark for Multimodal Culturally Awareness(https://arxiv.org/abs/2510.00041)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Cultural awareness capabilities has emerged as a critical capability for Multimodal Large Language Models (MLLMs). However, current benchmarks lack progressed difficulty in their task design and are deficient in cross-lingual tasks. Moreover, current benchmarks often use real-world images. Each real-world image typically contains one culture, making these benchmarks relatively easy for MLLMs. Based on this, we propose C$^3$B ($\textbf{C}$omics $\textbf{C}$ross-$\textbf{C}$ultural $\textbf{B}$enchmark), a novel multicultural, multitask and multilingual cultural awareness capabilities benchmark. C$^3$B comprises over 2000 images and over 18000 QA pairs, constructed on three tasks with progressed difficulties, from basic visual recognition to higher-level cultural conflict understanding, and finally to cultural content generation. We conducted evaluations on 11 open-source MLLMs, revealing a significant performance gap between MLLMs and human performance. The gap demonstrates that C$^3$B poses substantial challenges for current MLLMs, encouraging future research to advance the cultural awareness capabilities of MLLMs.</li>
<li><strong>摘要：</strong>文化意识能力已成为多模式模型（MLLM）的关键能力。但是，当前的基准在其任务设计上缺乏困难，并且缺乏跨语性任务。此外，当前的基准通常使用现实世界图像。每个现实世界图像通常都包含一种文化，使得MLLM的这些基准相对容易。基于此，我们提出了C $^3 $ b（$ \ textbf {c} $ omics $ \ textbf {c} $ ross- $ \ $ \ textbf {c} $ textbf {c} $ ultural $ \ textbf {b} $ enchmark），一种新颖的多种语言，多语言文化，多语言文化，多语言文化，多语言文化质量良好。 C $^3 $ b包括2000张图像和18000多个QA对，这些图像是在具有进步困难的三个任务上构建的，从基本的视觉识别到高级文化冲突理解，最后是文化内容的生成。我们对11个开源MLLM进行了评估，揭示了MLLM和人类绩效之间的显着性能差距。该差距表明，C $^3 $ b对当前的MLLM构成了重大挑战，鼓励未来的研究提高MLLM的文化意识能力。</li>
</ul>

<h3>Title: Beyond the Prompt: Gender Bias in Text-to-Image Models, with a Case Study on Hospital Professions</h3>
<ul>
<li><strong>Authors: </strong>Franck Vandewiele, Remi Synave, Samuel Delepoulle, Remi Cozot</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00045">https://arxiv.org/abs/2510.00045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00045">https://arxiv.org/pdf/2510.00045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00045]] Beyond the Prompt: Gender Bias in Text-to-Image Models, with a Case Study on Hospital Professions(https://arxiv.org/abs/2510.00045)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Text-to-image (TTI) models are increasingly used in professional, educational, and creative contexts, yet their outputs often embed and amplify social biases. This paper investigates gender representation in six state-of-the-art open-weight models: HunyuanImage 2.1, HiDream-I1-dev, Qwen-Image, FLUX.1-dev, Stable-Diffusion 3.5 Large, and Stable-Diffusion-XL. Using carefully designed prompts, we generated 100 images for each combination of five hospital-related professions (cardiologist, hospital director, nurse, paramedic, surgeon) and five portrait qualifiers ("", corporate, neutral, aesthetic, beautiful). Our analysis reveals systematic occupational stereotypes: all models produced nurses exclusively as women and surgeons predominantly as men. However, differences emerge across models: Qwen-Image and SDXL enforce rigid male dominance, HiDream-I1-dev shows mixed outcomes, and FLUX.1-dev skews female in most roles. HunyuanImage 2.1 and Stable-Diffusion 3.5 Large also reproduce gender stereotypes but with varying degrees of sensitivity to prompt formulation. Portrait qualifiers further modulate gender balance, with terms like corporate reinforcing male depictions and beautiful favoring female ones. Sensitivity varies widely: Qwen-Image remains nearly unaffected, while FLUX.1-dev, SDXL, and SD3.5 show strong prompt dependence. These findings demonstrate that gender bias in TTI models is both systematic and model-specific. Beyond documenting disparities, we argue that prompt wording plays a critical role in shaping demographic outcomes. The results underscore the need for bias-aware design, balanced defaults, and user guidance to prevent the reinforcement of occupational stereotypes in generative AI.</li>
<li><strong>摘要：</strong>文本对图像（TTI）模型越来越多地用于专业，教育和创造性的环境中，但是它们的产出经常嵌入并扩大社会偏见。本文研究了六种最先进的开放权重模型中的性别代表：Hunyuanimage 2.1，Hidream-I1-Dev，Qwen-image，Flux.1-DEV，稳定扩散3.5大型和稳定的散至差异。使用精心设计的提示，我们为五个与医院相关职业的组合（心脏病学家，医院导演，护士，护理人员，外科医生）和五个肖像预选赛（“”，企业，中立，审美，美丽，美丽）生成了100张图像。我们的分析揭示了系统的职业刻板印象：所有模型都以男性为主导的妇女和外科医生。然而，跨模型的差异出现：QWEN图像和SDXL强制执行刚性男性优势，Hidream-I1-DEV在大多数角色中显示出混合的结果，而Flux.1-DEV偏向女性。 Hunyuanimage 2.1和稳定的扩散3.5大型还繁殖性别刻板印象，但具有不同程度的敏感性以迅速制定。肖像预选赛进一步调节性别平衡，诸如公司加强男性描述和美丽的女性含义之类的术语。灵敏度差异很大：QWEN图像几乎不受影响，而Flux.1-DEV，SDXL和SD3.5显示出强烈的及时依赖性。这些发现表明，TTI模型中的性别偏差既系统又是模型的。除了记录差异外，我们认为促使措辞在塑造人口结果中起着至关重要的作用。结果强调了对偏见的设计，平衡默认值和用户指导的需求，以防止生成AI中职业刻板印象的增强。</li>
</ul>

<h3>Title: Explanation-Driven Counterfactual Testing for Faithfulness in Vision-Language Model Explanations</h3>
<ul>
<li><strong>Authors: </strong>Sihao Ding, Santosh Vasa, Aditi Ramadwar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00047">https://arxiv.org/abs/2510.00047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00047">https://arxiv.org/pdf/2510.00047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00047]] Explanation-Driven Counterfactual Testing for Faithfulness in Vision-Language Model Explanations(https://arxiv.org/abs/2510.00047)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) often produce fluent Natural Language Explanations (NLEs) that sound convincing but may not reflect the causal factors driving predictions. This mismatch of plausibility and faithfulness poses technical and governance risks. We introduce Explanation-Driven Counterfactual Testing (EDCT), a fully automated verification procedure for a target VLM that treats the model's own explanation as a falsifiable hypothesis. Given an image-question pair, EDCT: (1) obtains the model's answer and NLE, (2) parses the NLE into testable visual concepts, (3) generates targeted counterfactual edits via generative inpainting, and (4) computes a Counterfactual Consistency Score (CCS) using LLM-assisted analysis of changes in both answers and explanations. Across 120 curated OK-VQA examples and multiple VLMs, EDCT uncovers substantial faithfulness gaps and provides regulator-aligned audit artifacts indicating when cited concepts fail causal tests.</li>
<li><strong>摘要：</strong>视觉语言模型（VLM）通常会产生流利的自然语言解释（NLE），这些解释听起来令人信服，但可能无法反映出驱动预测的因果因素。这种合理性和忠诚的不匹配带来了技术和治理风险。我们介绍了解释驱动的反事实测试（EDCT），这是目标VLM的完全自动化的验证程序，将模型自己的解释视为可伪造的假设。给定图像问题对，EDCT：（1）获得模型的答案和NLE，（2）将NLE解析为可测试的视觉概念，（3）通过生成介绍生成有针对性的反事实编辑，（4）计算使用LLM辅助分析对答案和换句话说的分析。在120个精心策划的OK-VQA示例和多个VLM中，EDCT发现了巨大的忠诚差距，并提供了由监管机构对准的审计工件，指示何时引用的概念失败了因果测试。</li>
</ul>

<h3>Title: Less is More: Lean yet Powerful Vision-Language Model for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Sheng Yang, Tong Zhan, Guancheng Chen, Yanfeng Lu, Jian Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00060">https://arxiv.org/abs/2510.00060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00060">https://arxiv.org/pdf/2510.00060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00060]] Less is More: Lean yet Powerful Vision-Language Model for Autonomous Driving(https://arxiv.org/abs/2510.00060)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In this work, we reconceptualize autonomous driving as a generalized language and formulate the trajectory planning task as next waypoint prediction. We introduce Max-V1, a novel framework for one-stage end-to-end autonomous driving. Our framework presents a single-pass generation paradigm that aligns with the inherent sequentiality of driving. This approach leverages the generative capacity of the VLM (Vision-Language Model) to enable end-to-end trajectory prediction directly from front-view camera input. The efficacy of this method is underpinned by a principled supervision strategy derived from statistical modeling. This provides a well-defined learning objective, which makes the framework highly amenable to master complex driving policies through imitation learning from large-scale expert demonstrations. Empirically, our method achieves the state-of-the-art performance on the nuScenes dataset, delivers an overall improvement of over 30% compared to prior baselines. Furthermore, it exhibits superior generalization performance on cross-domain datasets acquired from diverse vehicles, demonstrating notable potential for cross-vehicle robustness and adaptability. Due to these empirical strengths, this work introduces a model enabling fundamental driving behaviors, laying the foundation for the development of more capable self-driving agents. Code will be available upon publication.</li>
<li><strong>摘要：</strong>在这项工作中，我们将自主驾驶作为一种通用语言重新概念化，并将轨迹计划任务作为下一Waypoint预测。我们介绍了Max-V1，这是一个新型端到端自动驾驶的新型框架。我们的框架提出了一个单通行证范式，该范式与驾驶的固有顺序一致。这种方法利用VLM（视觉语言模型）的生成能力直接从前视摄像头输入启用端到端轨迹预测。该方法的功效是由统计建模得出的原则监督策略的基础。这提供了一个定义明确的学习目标，这使该框架高度适合通过模仿大规模专家演示的模仿学习来掌握复杂的驾驶政策。从经验上讲，与先前的基线相比，我们的方法在Nuscenes数据集上实现了最先进的性能，总体提高了30％以上。此外，它在从不同车辆中获取的跨域数据集上表现出了出色的概括性能，这表明了跨车辆鲁棒性和适应性的显着潜力。由于这些经验优势，这项工作引入了一个模型，实现了基本驾驶行为，为开发更有能力的自动驾驶代理奠定了基础。代码将在出版时提供。</li>
</ul>

<h3>Title: Nonparametric Identification of Latent Concepts</h3>
<ul>
<li><strong>Authors: </strong>Yujia Zheng, Shaoan Xie, Kun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.PR, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00136">https://arxiv.org/abs/2510.00136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00136">https://arxiv.org/pdf/2510.00136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00136]] Nonparametric Identification of Latent Concepts(https://arxiv.org/abs/2510.00136)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We are born with the ability to learn concepts by comparing diverse observations. This helps us to understand the new world in a compositional manner and facilitates extrapolation, as objects naturally consist of multiple concepts. In this work, we argue that the cognitive mechanism of comparison, fundamental to human learning, is also vital for machines to recover true concepts underlying the data. This offers correctness guarantees for the field of concept learning, which, despite its impressive empirical successes, still lacks general theoretical support. Specifically, we aim to develop a theoretical framework for the identifiability of concepts with multiple classes of observations. We show that with sufficient diversity across classes, hidden concepts can be identified without assuming specific concept types, functional relations, or parametric generative models. Interestingly, even when conditions are not globally satisfied, we can still provide alternative guarantees for as many concepts as possible based on local comparisons, thereby extending the applicability of our theory to more flexible scenarios. Moreover, the hidden structure between classes and concepts can also be identified nonparametrically. We validate our theoretical results in both synthetic and real-world settings.</li>
<li><strong>摘要：</strong>我们天生具有通过比较各种观察结果来学习概念的能力。这有助于我们以组成方式理解新世界并促进外推，因为对象自然由多个概念组成。在这项工作中，我们认为比较的认知机制（人类学习的基础）对于机器恢复数据基础的真实概念也至关重要。这为概念学习领域提供了正确的保证，尽管它具有令人印象深刻的经验成功，但仍然缺乏一般的理论支持。具体而言，我们旨在为具有多种观察类别的概念的识别性开发一个理论框架。我们表明，在各个类别的多样性中，可以在不假设特定概念类型，功能关系或参数生成模型的情况下确定隐藏的概念。有趣的是，即使条件在全球范围内不满足时，我们仍然可以根据本地比较为尽可能多的概念提供替代保证，从而将理论的适用性扩展到更灵活的场景中。此外，类和概念之间的隐藏结构也可以非参数识别。我们在合成和现实世界中验证我们的理论结果。</li>
</ul>

<h3>Title: PrunedLoRA: Robust Gradient-Based structured pruning for Low-rank Adaptation in Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Xin Yu, Cong Xie, Ziyu Zhao, Tiantian Fan, Lingzhou Xue, Zhi Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00192">https://arxiv.org/abs/2510.00192</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00192">https://arxiv.org/pdf/2510.00192</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00192]] PrunedLoRA: Robust Gradient-Based structured pruning for Low-rank Adaptation in Fine-tuning(https://arxiv.org/abs/2510.00192)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Low-rank adaptation (LoRA) has become a widely used paradigm for parameter-efficient fine-tuning of large language models, yet its representational capacity often lags behind full fine-tuning. Within the context of LoRA, a key open question is how to obtain expressive low-rank adapters from over-parameterized spaces. We propose \textit{PrunedLoRA}, a new framework that leverages structured pruning to obtain highly representative low-rank adapters from an over-parameterized initialization. Unlike prior approaches that impose a fixed low-rank budget, PrunedLoRA dynamically prunes less important components during fine-tuning and prevents their reactivation, enabling flexible and adaptive rank allocation. For structured pruning, by minimizing the pruning error for overall loss, we provide fine-grained pruning and recovery updates in a gradient-based pruning strategy with grounded interpretation. We provide the first theoretical analysis of the robustness of structured pruning and provably show that under the impact of weight perturbation, gradient-based pruning is more robust than activation-based pruning with respect to overall loss. Empirically, PrunedLoRA consistently outperforms LoRA and its variants across supervised fine-tuning tasks in mathematical reasoning, code generation, and natural language understanding, and it also demonstrates advantages over existing structured pruning methods across diverse sparsity levels.</li>
<li><strong>摘要：</strong>低级适应（LORA）已成为大型语言模型参数效率微调的广泛使用的范式，但其表示能力通常落后于完整的微调。在洛拉（Lora）的背景下，一个关键的开放问题是如何从过度参数的空间中获取表达性低级适配器。我们提出\ textit {prunedlora}，这是一个新框架，利用结构化修剪从过度参数初始化中获得高度代表性的低级适配器。与以前的方法施加固定的低级预算不同，prunedlora在微调过程中动态修剪不太重要的组件，并防止其重新激活，从而使灵活和适应性的等级分配。对于结构化修剪，通过最大程度地减少整体损失的修剪误差，我们在基于梯度的修剪策略中提供了细粒度的修剪和恢复更新，并提供了扎根的解释。我们提供了结构化修剪的鲁棒性的第一个理论分析，并证明在体重扰动的影响下，基于梯度的修剪比基于整体损失的基于激活的修剪更健壮。从经验上讲，在数学推理，代码产生和自然语言理解方面，prunedlora始终优于洛拉及其变体，并且在不同的稀疏度跨越现有的结构化修剪方法方面也证明了优势。</li>
</ul>

<h3>Title: GRPO-$λ$: Credit Assignment improves LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Prasanna Parthasarathi, Mathieu Reymond, Boxing Chen, Yufei Cui, Sarath Chandar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00194">https://arxiv.org/abs/2510.00194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00194">https://arxiv.org/pdf/2510.00194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00194]] GRPO-$λ$: Credit Assignment improves LLM Reasoning(https://arxiv.org/abs/2510.00194)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly deployed for tasks requiring complex reasoning, prompting significant interest in improving their reasoning abilities through post-training. Especially RL based methods using verifiable reward, like the state-of-the-art GRPO, have shown to tremendously improve reasoning behaviors when applied as post-training methods. However, the lack of an explicit reward or critic model limits GRPO's ability to assign fine-grained credit across token sequences. In this work, we present GRPO-$\lambda$, a novel extension to GRPO that enhances credit assignment in RL finetuning of LLMs for complex reasoning tasks. We approximate learning from $\lambda$-return with a reformulation of eligibility traces using token-level log-probabilities applied after each sequence generation, and a novel critic-free approximation of the temporal-difference error. We introduce a few variations for the weighting of the $\lambda$-return, and their applications to the eligibility-trace, where all the variations provide significant gains over GRPO. We compare GRPO-$\lambda$ against GRPO by training models from 1.5B to 7B parameters on $4$ different math reasoning datasets. The training plots demonstrate 30-40% improved performance during RL training on both LLaMA-3.1 and Qwen-2.5 architectures. Finally, we show that with GRPO-$\lambda$, the resulting average performance on AIME24, Math500, OlympiadMath, MinervaMath, and AMC improves over GRPO by over $3$ points and a $4.5$ points improvement on the 7B model.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）越来越多地用于需要复杂推理的任务，这引起了人们对通过培训后提高其推理能力的重大兴趣。尤其是使用可验证奖励（例如最先进的GRPO）的基于RL的方法，在用作训练后方法时，已显示出可极大地改善推理行为。但是，缺乏明确的奖励或评论家模型限制了GRPO在代币序列中分配精细信贷的能力。在这项工作中，我们提出了GRPO-$ \ lambda $，这是GRPO的新型扩展，可增强LLMS fineTunting for RLM的信用分配，以完成复杂的推理任务。我们从$ \ lambda $ return近似学习，并使用代币级的对数概率进行了重新制定的资格痕迹，并在每个序列产生后应用了，以及对时间分歧错误的新颖批评者的近似。我们为$ \ lambda $ return的加权及其应用程序介绍了一些变体，并将其应用于资格追踪，在此，所有变化都为GRPO带来了显着的收益。我们将GRPO-$ \ lambda $与GRPO进行比较，通过培训模型从1.5B到7B参数，通过$ 4 $不同的数学推理数据集进行了比较。该培训地图显示在Llama-3.1和Qwen-2.5架构的RL培训期间的性能提高了30-40％。最后，我们表明，使用GRPO-$ \ lambda $，AIME24，MATH500，OLYMPIADMATH，MINERVAMATH和AMC的平均表现可提高GRPO的3美元以上，并提高了4.5 $ $ 4.5 $ 4.5 $的积分。</li>
</ul>

<h3>Title: Learning Energy-based Variational Latent Prior for VAEs</h3>
<ul>
<li><strong>Authors: </strong>Debottam Dutta, Chaitanya Amballa, Zhongweiyang Xu, Yu-Lin Wei, Romit Roy Choudhury</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00260">https://arxiv.org/abs/2510.00260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00260">https://arxiv.org/pdf/2510.00260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00260]] Learning Energy-based Variational Latent Prior for VAEs(https://arxiv.org/abs/2510.00260)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Variational Auto-Encoders (VAEs) are known to generate blurry and inconsistent samples. One reason for this is the "prior hole" problem. A prior hole refers to regions that have high probability under the VAE's prior but low probability under the VAE's posterior. This means that during data generation, high probability samples from the prior could have low probability under the posterior, resulting in poor quality data. Ideally, a prior needs to be flexible enough to match the posterior while retaining the ability to generate samples fast. Generative models continue to address this tradeoff. This paper proposes to model the prior as an energy-based model (EBM). While EBMs are known to offer the flexibility to match posteriors (and also improving the ELBO), they are traditionally slow in sample generation due to their dependency on MCMC methods. Our key idea is to bring a variational approach to tackle the normalization constant in EBMs, thus bypassing the expensive MCMC approaches. The variational form can be approximated with a sampler network, and we show that such an approach to training priors can be formulated as an alternating optimization problem. Moreover, the same sampler reduces to an implicit variational prior during generation, providing efficient and fast sampling. We compare our Energy-based Variational Latent Prior (EVaLP) method to multiple SOTA baselines and show improvements in image generation quality, reduced prior holes, and better sampling efficiency.</li>
<li><strong>摘要：</strong>已知变异自动编码器（VAE）会产生模糊和不一致的样本。原因之一是“先前的孔”问题。先前的孔是指在VAE的先前但vae后部较低概率下具有很高概率的区域。这意味着，在数据生成期间，在后部下，先前的高概率样本可能具有较低的概率，从而导致质量差。理想情况下，先验需要足够灵活，以使后部匹配，同时保持快速生成样品的能力。生成模型继续解决这一权衡。本文建议将先验建模为基于能量的模型（EBM）。虽然已知EBM具有匹配后者的灵活性（并改善ELBO），但由于它们依赖MCMC方法，它们传统上的样本生成速度很慢。我们的关键思想是携带一种差异方法来应对EBM中的归一化常量，从而绕开了昂贵的MCMC方法。可以使用采样器网络近似变化形式，我们表明这种训练先验的方法可以作为交替的优化问题进行配合。此外，相同的采样器在发电期间还原为隐式变异先验，提供有效而快速的采样。我们将基于能量的变异潜在先验（evatp）方法与多个SOTA基准进行比较，并显示出图像产生质量的改善，降低的先前孔以及更好的采样效率。</li>
</ul>

<h3>Title: MOLM: Mixture of LoRA Markers</h3>
<ul>
<li><strong>Authors: </strong>Samar Fares, Nurbek Tastan, Noor Hussein, Karthik Nandakumar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00293">https://arxiv.org/abs/2510.00293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00293">https://arxiv.org/pdf/2510.00293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00293]] MOLM: Mixture of LoRA Markers(https://arxiv.org/abs/2510.00293)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative models can generate photorealistic images at scale. This raises urgent concerns about the ability to detect synthetically generated images and attribute these images to specific sources. While watermarking has emerged as a possible solution, existing methods remain fragile to realistic distortions, susceptible to adaptive removal, and expensive to update when the underlying watermarking key changes. We propose a general watermarking framework that formulates the encoding problem as key-dependent perturbation of the parameters of a generative model. Within this framework, we introduce Mixture of LoRA Markers (MOLM), a routing-based instantiation in which binary keys activate lightweight LoRA adapters inside residual and attention blocks. This design avoids key-specific re-training and achieves the desired properties such as imperceptibility, fidelity, verifiability, and robustness. Experiments on Stable Diffusion and FLUX show that MOLM preserves image quality while achieving robust key recovery against distortions, compression and regeneration, averaging attacks, and black-box adversarial attacks on the extractor.</li>
<li><strong>摘要：</strong>生成模型可以大规模生成逼真的图像。这引起了人们对检测合成生成图像并将这些图像归因于特定源的能力的紧急关注。尽管水印已成为一种可能的解决方案，但现有的方法仍然脆弱地扭曲，容易自适应去除，并且在基础水印的主要关键变化时进行更新。我们提出了一个一般的水印框架，该框架将编码问题提出为生成模型参数的键依赖性扰动。在此框架内，我们引入了Lora标记（MOLM）的混合物，Lora标记（MOLM）是一种基于路由的实例化，其中二进制键激活了残留和注意力块内的轻质Lora适配器。该设计避免了特定于密钥的重新训练，并实现了所需的特性，例如不可识别，保真度，可验证性和鲁棒性。关于稳定扩散和通量的实验表明，MOLM可以保留图像质量，同时可以针对扭曲，压缩和再生，平均攻击​​和对提取器的黑盒对抗攻击实现强大的钥匙恢复。</li>
</ul>

<h3>Title: Free Draft-and-Verification: Toward Lossless Parallel Decoding for Diffusion Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shutong Wu, Jiawei Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00294">https://arxiv.org/abs/2510.00294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00294">https://arxiv.org/pdf/2510.00294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00294]] Free Draft-and-Verification: Toward Lossless Parallel Decoding for Diffusion Large Language Models(https://arxiv.org/abs/2510.00294)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion Large Language Models (DLLMs) have emerged as a new paradigm of language modeling beyond autoregressive next-token prediction. Thanks to their bidirectional attention mechanism, DLLMs are more capable of capturing the connection of context, and thus show unique advantages in challenges like the famous "reversal curse" or learning under data-constrained scenarios. However, this bidirectional nature also brings an obstacle that DLLMs are not inherently compatible with KV Cache, and consequently, the inference efficiency is not competitive compared with autoregressive models. Taking advantage of their inherent capability of multi-token prediction, existing parallel decoding algorithms can speed up the DLLM inference, but at the cost of non-negligible performance degradation. To overcome this challenge, we introduce Free Draft-and-Verification (Freedave), a novel fast sampling algorithm tailored for DLLMs that achieves lossless parallel decoding. Specifically, we propose a pipeline of parallel-decoded candidate generation and verification, which is guaranteed to reproduce the same sequence generated by static sampling, without introducing extra model forward calls. By applying Freedave, the throughput of DLLMs can be boosted up to $2.8\times$ without performance degradation on math reasoning tasks.</li>
<li><strong>摘要：</strong>扩散大语言模型（DLLM）已成为一种新的语言建模范式，超出自回归的下一步预测。由于其双向关注机制，DLLM更有能力捕获上下文的联系，因此在著名的“逆转诅咒”或在数据约束的情况下学习的挑战表现出了独特的优势。但是，这种双向性质也带来了一个障碍，即DLLM与KV缓存本质上不兼容，因此，与自回归模型相比，推断效率不具有竞争力。利用其固有的多态预测能力，现有的平行解码算法可以加快DLLM推理的速度，但以不可忽略的性能退化为代价。为了克服这一挑战，我们引入了免费的草稿和验证（FreedAve），这是一种针对DLLM量身定制的新型快速采样算法，可实现无损平行解码。具体而言，我们提出了一条平行编码的候选生成和验证的管道，该管道可以保证可以复制与静态采样生成的相同序列，而无需引入额外的模型正向调用。通过应用Freedave，可以将DLLM的吞吐量提高到$ 2.8 \ times $，而无需在数学推理任务上的性能下降。</li>
</ul>

<h3>Title: Flow Autoencoders are Effective Protein Tokenizers</h3>
<ul>
<li><strong>Authors: </strong>Rohit Dilip, Evan Zhang, Ayush Varshney, David Van Valen</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00351">https://arxiv.org/abs/2510.00351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00351">https://arxiv.org/pdf/2510.00351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00351]] Flow Autoencoders are Effective Protein Tokenizers(https://arxiv.org/abs/2510.00351)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Protein structure tokenizers enable the creation of multimodal models of protein structure, sequence, and function. Current approaches to protein structure tokenization rely on bespoke components that are invariant to spatial symmetries, but that are challenging to optimize and scale. We present Kanzi, a flow-based tokenizer for tokenization and generation of protein structures. Kanzi consists of a diffusion autoencoder trained with a flow matching loss. We show that this approach simplifies several aspects of protein structure tokenizers: frame-based representations can be replaced with global coordinates, complex losses are replaced with a single flow matching loss, and SE(3)-invariant attention operations can be replaced with standard attention. We find that these changes stabilize the training of parameter-efficient models that outperform existing tokenizers on reconstruction metrics at a fraction of the model size and training cost. An autoregressive model trained with Kanzi outperforms similar generative models that operate over tokens, although it does not yet match the performance of state-of-the-art continuous diffusion models. Code is available here: this https URL.</li>
<li><strong>摘要：</strong>蛋白质结构引导者可以创建蛋白质结构，序列和功能的多模型模型。当前的蛋白质结构令牌化方法依赖于空间对称性不变的定制组件，但在优化和扩展方面具有挑战性。我们提出坎兹，这是一种基于流动的令牌，用于令牌化和蛋白质结构的产生。坎兹由一个扩散自动编码器组成，该自动编码器训练有匹配损失。我们表明，这种方法简化了蛋白质结构令牌的几个方面：可以用全球坐标代替基于框架的表示形式，复杂的损失被单个流匹配损失代替，而SE（3） - 引人注目的注意操作可以用标准注意力代替。我们发现，这些变化稳定了参数有效模型的训练，这些模型的训练在重建指标上以型号大小和培训成本的一小部分胜过现有的引物。一种接受kanzi训练的自回归模型优于超过令牌的类似生成模型，尽管它尚未与最新的连续扩散模型的性能相匹配。代码可在此处提供：此HTTPS URL。</li>
</ul>

<h3>Title: AReUReDi: Annealed Rectified Updates for Refining Discrete Flows with Multi-Objective Guidance</h3>
<ul>
<li><strong>Authors: </strong>Tong Chen, Yinuo Zhang, Pranam Chatterjee</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00352">https://arxiv.org/abs/2510.00352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00352">https://arxiv.org/pdf/2510.00352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00352]] AReUReDi: Annealed Rectified Updates for Refining Discrete Flows with Multi-Objective Guidance(https://arxiv.org/abs/2510.00352)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Designing sequences that satisfy multiple, often conflicting, objectives is a central challenge in therapeutic and biomolecular engineering. Existing generative frameworks largely operate in continuous spaces with single-objective guidance, while discrete approaches lack guarantees for multi-objective Pareto optimality. We introduce AReUReDi (Annealed Rectified Updates for Refining Discrete Flows), a discrete optimization algorithm with theoretical guarantees of convergence to the Pareto front. Building on Rectified Discrete Flows (ReDi), AReUReDi combines Tchebycheff scalarization, locally balanced proposals, and annealed Metropolis-Hastings updates to bias sampling toward Pareto-optimal states while preserving distributional invariance. Applied to peptide and SMILES sequence design, AReUReDi simultaneously optimizes up to five therapeutic properties (including affinity, solubility, hemolysis, half-life, and non-fouling) and outperforms both evolutionary and diffusion-based baselines. These results establish AReUReDi as a powerful, sequence-based framework for multi-property biomolecule generation.</li>
<li><strong>摘要：</strong>设计满足多个（通常是冲突目标）的序列是治疗和生物分子工程的核心挑战。现有的生成框架在很大程度上在具有单目标指导的连续空间中运行，而离散方法则缺乏多目标帕累托最优性的保证。我们介绍了Isuredi（用于精炼离散流的退火校正更新），这是一种离散优化算法，具有融合到Pareto Front的理论保证。 areuredi以纠正的离散流（REDI）为基础，结合了Tchebycheff标量，局部平衡的建议和退火大都市杂货店的更新，以偏向于帕累托最佳状态，同时保持分布不变性。应用于肽和微笑序列设计，同时优化了多达五种治疗特性（包括亲和力，溶解度，溶血，半寿命和非污染），并且均优于基于进化和扩散的碱。这些结果确立了isuredi是一种基于序列的强大的，基于序列的框架，用于多发性生物分子生成。</li>
</ul>

<h3>Title: Efficient Probabilistic Tensor Networks</h3>
<ul>
<li><strong>Authors: </strong>Marawan Gamal Abdel Hameed, Guillaume Rabusseau</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00382">https://arxiv.org/abs/2510.00382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00382">https://arxiv.org/pdf/2510.00382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00382]] Efficient Probabilistic Tensor Networks(https://arxiv.org/abs/2510.00382)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Tensor networks (TNs) enable compact representations of large tensors through shared parameters. Their use in probabilistic modeling is particularly appealing, as probabilistic tensor networks (PTNs) allow for tractable computation of marginals. However, existing approaches for learning parameters of PTNs are either computationally demanding and not fully compatible with automatic differentiation frameworks, or numerically unstable. In this work, we propose a conceptually simple approach for learning PTNs efficiently, that is numerically stable. We show our method provides significant improvements in time and space complexity, achieving 10x reduction in latency for generative modeling on the MNIST dataset. Furthermore, our approach enables learning of distributions with 10x more variables than previous approaches when applied to a variety of density estimation benchmarks. Our code is publicly available at this http URL.</li>
<li><strong>摘要：</strong>张量网络（TNS）通过共享参数启用大型张量的紧凑表示。由于概率张量网络（PTN）允许对边缘的易于计算，因此它们在概率建模中的使用特别有吸引力。但是，现有的PTN学习参数的方法在计算上是要求的，并且与自动分化框架不完全兼容，或者在数值上不稳定。在这项工作中，我们提出了一种有效地学习PTN的概念上简单的方法，在数字上是稳定的。我们表明，我们的方法可以显着改善时间和空间的复杂性，从而减少了MNIST数据集上生成建模的潜伏期10倍。此外，当应用于各种密度估计基准时，我们的方法可以学习比以前的方法多10倍的分布。我们的代码在此HTTP URL上公开可用。</li>
</ul>

<h3>Title: Graph2Region: Efficient Graph Similarity Learning with Structure and Scale Restoration</h3>
<ul>
<li><strong>Authors: </strong>Zhouyang Liu, Yixin Chen, Ning Liu, Jiezhong He, Dongsheng Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00394">https://arxiv.org/abs/2510.00394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00394">https://arxiv.org/pdf/2510.00394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00394]] Graph2Region: Efficient Graph Similarity Learning with Structure and Scale Restoration(https://arxiv.org/abs/2510.00394)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Graph similarity is critical in graph-related tasks such as graph retrieval, where metrics like maximum common subgraph (MCS) and graph edit distance (GED) are commonly used. However, exact computations of these metrics are known to be NP-Hard. Recent neural network-based approaches approximate the similarity score in embedding spaces to alleviate the computational burden, but they either involve expensive pairwise node comparisons or fail to effectively utilize structural and scale information of graphs. To tackle these issues, we propose a novel geometric-based graph embedding method called Graph2Region (G2R). G2R represents nodes as closed regions and recovers their adjacency patterns within graphs in the embedding space. By incorporating the node features and adjacency patterns of graphs, G2R summarizes graph regions, i.e., graph embeddings, where the shape captures the underlying graph structures and the volume reflects the graph size. Consequently, the overlap between graph regions can serve as an approximation of MCS, signifying similar node regions and adjacency patterns. We further analyze the relationship between MCS and GED and propose using disjoint parts as a proxy for GED similarity. This analysis enables concurrent computation of MCS and GED, incorporating local and global structural information. Experimental evaluation highlights G2R's competitive performance in graph similarity computation. It achieves up to a 60.0\% relative accuracy improvement over state-of-the-art methods in MCS similarity learning, while maintaining efficiency in both training and inference. Moreover, G2R showcases remarkable capability in predicting both MCS and GED similarities simultaneously, providing a holistic assessment of graph similarity. Code available at this https URL.</li>
<li><strong>摘要：</strong>图形相似性在与图相关的任务（例如图形检索）中至关重要，其中通常使用的指标（例如最大公共子图（MC）和图形编辑距离（GED））通常使用。但是，已知这些指标的精确计算是NP-HARD。基于神经网络的最新方法近似嵌入空间中的相似性得分以减轻计算负担，但它们要么涉及昂贵的成对节点比较，要么无法有效利用图的结构和规模信息。为了解决这些问题，我们提出了一种称为Graph2Region（G2R）的新型基于几何的图形嵌入方法。 G2R表示节点为封闭区域，并在嵌入空间中的图中恢复其邻接模式。通过合并图形的节点特征和邻接模式，G2R总结了图区域（即图形嵌入），其中形状捕获了基础图结构，并且体积反映了图形大小。因此，图区域之间的重叠可以用作MC的近似值，表示相似的节点区域和邻接模式。我们进一步分析了MCS和GED之间的关系，并建议使用不相交的部分作为GED相似性的代理。该分析可以同时计算MC和GED，并结合本地和全球结构信息。实验评估突出了G2R在图形相似性计算中的竞争性能。与MCS相似性学习的最新方法相比，它的相对准确性提高了60.0 \％，同时保持训练和推理的效率。此外，G2R在同时预测MC和GED相似性方面展示了出色的能力，从而提供了图形相似性的整体评估。可在此HTTPS URL上找到代码。</li>
</ul>

<h3>Title: Automated Structured Radiology Report Generation with Rich Clinical Context</h3>
<ul>
<li><strong>Authors: </strong>Seongjae Kang, Dong Bok Lee, Juho Jung, Dongseop Kim, Won Hwa Kim, Sunghoon Joo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00428">https://arxiv.org/abs/2510.00428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00428">https://arxiv.org/pdf/2510.00428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00428]] Automated Structured Radiology Report Generation with Rich Clinical Context(https://arxiv.org/abs/2510.00428)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Automated structured radiology report generation (SRRG) from chest X-ray images offers significant potential to reduce workload of radiologists by generating reports in structured formats that ensure clarity, consistency, and adherence to clinical reporting standards. While radiologists effectively utilize available clinical contexts in their diagnostic reasoning, existing SRRG systems overlook these essential elements. This fundamental gap leads to critical problems including temporal hallucinations when referencing non-existent clinical contexts. To address these limitations, we propose contextualized SRRG (C-SRRG) that comprehensively incorporates rich clinical context for SRRG. We curate C-SRRG dataset by integrating comprehensive clinical context encompassing 1) multi-view X-ray images, 2) clinical indication, 3) imaging techniques, and 4) prior studies with corresponding comparisons based on patient histories. Through extensive benchmarking with state-of-the-art multimodal large language models, we demonstrate that incorporating clinical context with the proposed C-SRRG significantly improves report generation quality. We publicly release dataset, code, and checkpoints to facilitate future research for clinically-aligned automated RRG at this https URL.</li>
<li><strong>摘要：</strong>胸部X射线图像的自动结构放射报告生成（SRRG）通过以结构化格式生成报告，以确保清晰度，一致性和遵守临床报告标准，从而减少放射科医生的工作量。尽管放射科医生在诊断推理中有效利用了可用的临床环境，但现有的SRRG系统忽略了这些基本要素。这一基本差距导致关键问题，包括参考不存在的临床环境时时间幻觉。为了解决这些局限性，我们提出了上下文化的SRRG（C-SRRG），该SRRG（C-SRRG）全面地融合了SRRG的丰富临床环境。我们通过整合包含全面的临床环境1）多视图X射线图像，2）临床指示，3）成像技术和4）先前的研究与基于患者病史的相应比较进行策划C-SRRG数据集。通过使用最先进的多模式大语言模型进行广泛的基准测试，我们证明将临床环境与拟议的C-SRG融合可显着提高报告的生成质量。我们公开发布数据集，代码和检查点，以促进此HTTPS URL的临床对准自动化RRG的未来研究。</li>
</ul>

<h3>Title: BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration</h3>
<ul>
<li><strong>Authors: </strong>Zhaoyang Li, Dongjun Qian, Kai Su, Qishuai Diao, Xiangyang Xia, Chang Liu, Wenfei Yang, Tianzhu Zhang, Zehuan Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00438">https://arxiv.org/abs/2510.00438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00438">https://arxiv.org/pdf/2510.00438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00438]] BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration(https://arxiv.org/abs/2510.00438)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion Transformer has shown remarkable abilities in generating high-fidelity videos, delivering visually coherent frames and rich details over extended durations. However, existing video generation models still fall short in subject-consistent video generation due to an inherent difficulty in parsing prompts that specify complex spatial relationships, temporal logic, and interactions among multiple subjects. To address this issue, we propose BindWeave, a unified framework that handles a broad range of subject-to-video scenarios from single-subject cases to complex multi-subject scenes with heterogeneous entities. To bind complex prompt semantics to concrete visual subjects, we introduce an MLLM-DiT framework in which a pretrained multimodal large language model performs deep cross-modal reasoning to ground entities and disentangle roles, attributes, and interactions, yielding subject-aware hidden states that condition the diffusion transformer for high-fidelity subject-consistent video generation. Experiments on the OpenS2V benchmark demonstrate that our method achieves superior performance across subject consistency, naturalness, and text relevance in generated videos, outperforming existing open-source and commercial models.</li>
<li><strong>摘要：</strong>扩散变压器在生成高保真视频，在延长的持续时间内提供了相干的框架和丰富的细节，表现出了显着的能力。但是，由于解析提示的固有困难，可以指定复杂的空间关系，时间逻辑和多个受试者之间的相互作用，因此现有的视频生成模型仍然缺乏主题一致的视频生成。为了解决这个问题，我们提出了bindweave，这是一个统一的框架，该框架处理从单个主体案例到具有异质实体的复杂多主体场景的广泛主题对视频。为了将复杂的迅速语义绑定到具体的视觉主题，我们引入了一个MLLM-DIT框架，在该框架中，预处理的多模式大型语言模型对地面实体进行了深层的跨模式推理，并散布了地面实体，并散发出词性，属性和相互作用，从而产生主题的隐藏状态，从而导致了对高效率主题视频构成视频构成的扩散变换器。 OpenS2V基准上的实验表明，我们的方法在生成视频中的主题一致性，自然性和文本相关性方面取得了卓越的性能，表现优于现有的开源和商业模型。</li>
</ul>

<h3>Title: UrbanGraph: Physics-Informed Spatio-Temporal Dynamic Heterogeneous Graphs for Urban Microclimate Prediction</h3>
<ul>
<li><strong>Authors: </strong>Weilin Xin, Chenyu Huang, Peilin Li, Jing Zhong, Jiawei Yao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00457">https://arxiv.org/abs/2510.00457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00457">https://arxiv.org/pdf/2510.00457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00457]] UrbanGraph: Physics-Informed Spatio-Temporal Dynamic Heterogeneous Graphs for Urban Microclimate Prediction(https://arxiv.org/abs/2510.00457)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With rapid urbanization, predicting urban microclimates has become critical, as it affects building energy demand and public health risks. However, existing generative and homogeneous graph approaches fall short in capturing physical consistency, spatial dependencies, and temporal variability. To address this, we introduce UrbanGraph, a physics-informed framework integrating heterogeneous and dynamic spatio-temporal graphs. It encodes key physical processes -- vegetation evapotranspiration, shading, and convective diffusion -- while modeling complex spatial dependencies among diverse urban entities and their temporal evolution. We evaluate UrbanGraph on UMC4/12, a physics-based simulation dataset covering diverse urban configurations and climates. Results show that UrbanGraph improves $R^2$ by up to 10.8% and reduces FLOPs by 17.0% over all baselines, with heterogeneous and dynamic graphs contributing 3.5% and 7.1% gains. Our dataset provides the first high-resolution benchmark for spatio-temporal microclimate modeling, and our method extends to broader urban heterogeneous dynamic computing tasks.</li>
<li><strong>摘要：</strong>随着快速的城市化，预测城市微气候已经变得至关重要，因为它影响了建筑能源需求和公共卫生风险。但是，现有的生成和同质图方法在捕获物理一致性，空间依赖性和时间变异性方面缺乏。为了解决这个问题，我们介绍了Urbangraph，这是一个集成了异质和动态时空图形的物理信息框架。它编码关键的物理过程 - 植被蒸散，阴影和对流扩散 - 同时对各种城市实体之间的复杂空间依赖性及其时间演变进行建模。我们在UMC4/12上评估了URBANGRAPH，这是一个基于物理的模拟数据集，涵盖了各种城市配置和气候。结果表明，Urbangraph将$ R^2 $提高到10.8％，并使所有基线的拖鞋降低了17.0％，异质和动态图的贡献贡献了3.5％和7.1％。我们的数据集为时空小气候建模提供了第一个高分辨率基准，我们的方法扩展到更广泛的城市异质动态计算任务。</li>
</ul>

<h3>Title: Diagnosing Shortcut-Induced Rigidity in Continual Learning: The Einstellung Rigidity Index (ERI)</h3>
<ul>
<li><strong>Authors: </strong>Kai Gu, Weishi Shi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00475">https://arxiv.org/abs/2510.00475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00475">https://arxiv.org/pdf/2510.00475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00475]] Diagnosing Shortcut-Induced Rigidity in Continual Learning: The Einstellung Rigidity Index (ERI)(https://arxiv.org/abs/2510.00475)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep neural networks frequently exploit shortcut features, defined as incidental correlations between inputs and labels without causal meaning. Shortcut features undermine robustness and reduce reliability under distribution shifts. In continual learning (CL), the consequences of shortcut exploitation can persist and intensify: weights inherited from earlier tasks bias representation reuse toward whatever features most easily satisfied prior labels, mirroring the cognitive Einstellung effect, a phenomenon where past habits block optimal solutions. Whereas catastrophic forgetting erodes past skills, shortcut-induced rigidity throttles the acquisition of new ones. We introduce the Einstellung Rigidity Index (ERI), a compact diagnostic that disentangles genuine transfer from cue-inflated performance using three interpretable facets: (i) Adaptation Delay (AD), (ii) Performance Deficit (PD), and (iii) Relative Suboptimal Feature Reliance (SFR_rel). On a two-phase CIFAR-100 CL benchmark with a deliberately spurious magenta patch in Phase 2, we evaluate Naive fine-tuning (SGD), online Elastic Weight Consolidation (EWC_on), Dark Experience Replay (DER++), Gradient Projection Memory (GPM), and Deep Generative Replay (DGR). Across these continual learning methods, we observe that CL methods reach accuracy thresholds earlier than a Scratch-T2 baseline (negative AD) but achieve slightly lower final accuracy on patched shortcut classes (positive PD). Masking the patch improves accuracy for CL methods while slightly reducing Scratch-T2, yielding negative SFR_rel. This pattern indicates the patch acted as a distractor for CL models in this setting rather than a helpful shortcut.</li>
<li><strong>摘要：</strong>深度神经网络经常利用快捷方式特征，该特征定义为没有因素含义的输入和标签之间的偶然相关性。快捷方式特征破坏了鲁棒性并降低分配变化下的可靠性。在持续学习（CL）中，捷径开发的后果可以持续和加强：从较早的任务中继承的权重偏向表示最容易满足的先前标签的任何功能，反映了认知能力Einstellung效应，这是过去习惯最佳解决方案的现象。尽管灾难性忘记侵蚀了过去的技能，但捷径引起的刚度会导致收购新的技能。我们介绍了Einstellung刚度指数（ERI），这是一种紧凑的诊断，使用三个可解释的方面将真正的转移从提示膨胀的性能中解散：（i）适应性延迟（AD），（ii）性能不足（PD）和（iii）相对次优特征依赖（SFR_REL）（SFR_REL）。在第2阶段的两阶段CIFAR-100 CLENMARC上，有意地具有虚假的洋红色补丁，我们评估了幼稚的微调（SGD），在线弹性重量合并（EWC_ON），黑暗体验重播（DER ++），梯度预测记忆（GPM）和深层生成性重播（DGR）（DGR）。在这些持续学习方法中，我们观察到CL方法比Scratch-T2基线（负AD）提前达到准确性阈值，但在修补的快捷方式类别（正PD）上达到了略低的最终精度。掩盖贴片可提高CL方法的准确性，同时略微降低Scratch-T2，从而产生负SFR_REL。此模式表明该补丁在此设置中充当CL模型的干扰物，而不是有用的快捷方式。</li>
</ul>

<h3>Title: Vicinity-Guided Discriminative Latent Diffusion for Privacy-Preserving Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Jing Wang, Wonho Bae, Jiahong Chen, Wenxu Wang, Junhyug Noh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00478">https://arxiv.org/abs/2510.00478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00478">https://arxiv.org/pdf/2510.00478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00478]] Vicinity-Guided Discriminative Latent Diffusion for Privacy-Preserving Domain Adaptation(https://arxiv.org/abs/2510.00478)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent work on latent diffusion models (LDMs) has focused almost exclusively on generative tasks, leaving their potential for discriminative transfer largely unexplored. We introduce Discriminative Vicinity Diffusion (DVD), a novel LDM-based framework for a more practical variant of source-free domain adaptation (SFDA): the source provider may share not only a pre-trained classifier but also an auxiliary latent diffusion module, trained once on the source data and never exposing raw source samples. DVD encodes each source feature's label information into its latent vicinity by fitting a Gaussian prior over its k-nearest neighbors and training the diffusion network to drift noisy samples back to label-consistent representations. During adaptation, we sample from each target feature's latent vicinity, apply the frozen diffusion module to generate source-like cues, and use a simple InfoNCE loss to align the target encoder to these cues, explicitly transferring decision boundaries without source access. Across standard SFDA benchmarks, DVD outperforms state-of-the-art methods. We further show that the same latent diffusion module enhances the source classifier's accuracy on in-domain data and boosts performance in supervised classification and domain generalization experiments. DVD thus reinterprets LDMs as practical, privacy-preserving bridges for explicit knowledge transfer, addressing a core challenge in source-free domain adaptation that prior methods have yet to solve.</li>
<li><strong>摘要：</strong>最新的潜在扩散模型（LDMS）几乎专注于生成任务，从而使判别转移的潜力在很大程度上没有探索。我们介绍了判别性附近扩散（DVD），这是一种基于LDM的新型框架，用于更实用的无源域适应性变体（SFDA）：源提供商不仅可以共享预训练的分类器，还可以共享辅助潜在扩散模块，对源数据和从未有过访问原始源样品进行了训练。 DVD通过在其K-Nearthear的邻居上拟合高斯之前将每个源特征的标签信息编码到其潜在附近，并训练扩散网络以将嘈杂的样本漂移回标签符合的表示。在适应过程中，我们从每个目标特征的潜在附近进行采样，应用冷冻扩散模块生成类似源的线索，并使用简单的Infonce损失将目标编码器与这些提示保持一致，从而明确传输决策边界，而无需源访问。在标准的SFDA基准测试中，DVD的表现优于最先进的方法。我们进一步表明，相同的潜扩散模块增强了源分类器对内域数据的准确性，并提高了监督分类和域概括实验中的性能。 DVD因此将LDMS重新解释为实用的，保存隐私的桥梁，以进行明确的知识转移，从而解决了先前方法尚未解决的无源域适应性核心挑战。</li>
</ul>

<h3>Title: Affordance-Guided Diffusion Prior for 3D Hand Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Naru Suzuki, Takehiko Ohkawa, Tatsuro Banno, Jihyun Lee, Ryosuke Furuta, Yoichi Sato</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00506">https://arxiv.org/abs/2510.00506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00506">https://arxiv.org/pdf/2510.00506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00506]] Affordance-Guided Diffusion Prior for 3D Hand Reconstruction(https://arxiv.org/abs/2510.00506)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>How can we reconstruct 3D hand poses when large portions of the hand are heavily occluded by itself or by objects? Humans often resolve such ambiguities by leveraging contextual knowledge -- such as affordances, where an object's shape and function suggest how the object is typically grasped. Inspired by this observation, we propose a generative prior for hand pose refinement guided by affordance-aware textual descriptions of hand-object interactions (HOI). Our method employs a diffusion-based generative model that learns the distribution of plausible hand poses conditioned on affordance descriptions, which are inferred from a large vision-language model (VLM). This enables the refinement of occluded regions into more accurate and functionally coherent hand poses. Extensive experiments on HOGraspNet, a 3D hand-affordance dataset with severe occlusions, demonstrate that our affordance-guided refinement significantly improves hand pose estimation over both recent regression methods and diffusion-based refinement lacking contextual reasoning.</li>
<li><strong>摘要：</strong>当大部分手被本身或物体大量遮住时，我们如何重建3D手摆姿势？人类经常通过利用上下文知识来解决这种歧义，例如负担能力，其中对象的形状和功能表明对象通常是如何掌握的。受到这一观察的启发，我们提出了由手动相互作用（HOI）引导的手工姿势完善的生成性先验。我们的方法采用了基于扩散的生成模型，该模型学习以负担性描述为条件的合理手姿势的分布，这是从大型视觉模型（VLM）推断出来的。这使得被遮挡区域的细化成更准确，功能相干的手姿势。关于HograspNet的广泛实验，这是一个带有严重遮挡的3D手动数据集，表明我们的负担能力引导的精炼显着改善了在最近的回归方法和缺乏上下文推理的基于回归方法和基于扩散的精炼方面的手工姿势估计。</li>
</ul>

<h3>Title: Panorama: Fast-Track Nearest Neighbors</h3>
<ul>
<li><strong>Authors: </strong>Vansh Ramani, Alexis Schlomer, Akash Nayar, Panagiotis Karras, Sayan Ranu, Jignesh M. Patel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00566">https://arxiv.org/abs/2510.00566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00566">https://arxiv.org/pdf/2510.00566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00566]] Panorama: Fast-Track Nearest Neighbors(https://arxiv.org/abs/2510.00566)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose embeddings are close to that of a given query in a high-dimensional space, aiming to balance accuracy with speed. Used in recommendation systems, image and video retrieval, natural language processing, and retrieval-augmented generation (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT utilize graph, tree, clustering, and quantization techniques to navigate large vector spaces. Despite this progress, ANNS systems spend up to 99\% of query time to compute distances in their final refinement phase. In this paper, we present PANORAMA, a machine learning-driven approach that tackles the ANNS verification bottleneck through data-adaptive learned orthogonal transforms that facilitate the accretive refinement of distance bounds. Such transforms compact over 90\% of signal energy into the first half of dimensions, enabling early candidate pruning with partial distance computations. We integrate PANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and Annoy, without index modification, using level-major memory layouts, SIMD-vectorized partial distance computations, and cache-aware access patterns. Experiments across diverse datasets -- from image-based CIFAR-10 and GIST to modern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate that PANORAMA affords a 2--30$\times$ end-to-end speedup with no recall loss.</li>
<li><strong>摘要：</strong>大约最近的邻居搜索（ANNS）有效地找到了其嵌入在高维空间中给定查询的数据项，旨在平衡准确性与速度。在建议系统，图像和视频检索，自然语言处理以及检索功能增强的生成（RAG）中，用于ANN算法，例如IVFPQ，HNSW图，烦恼和MRPT，并利用图形，树，聚类和量化技术来导航大型矢量空间。尽管取得了这种进展，ANNS系统最多将查询时间的99％花费在其最终改进阶段计算距离。在本文中，我们介绍了一种机器学习驱动的方法，该方法通过数据自适应学习的正交转换来解决ANN验证瓶颈，从而促进距离范围的增强性。这样的信号能量超过90％的信号能将其转换为尺寸的上半部分，从而使早期候选者可以通过部分距离计算进行修剪。我们将Panorama集成到最新的ANNS方法中，即IVFPQ/FLAT，HNSW，MRPT，MRPT和烦恼，没有索引修改，使用级别的MAJOR存储器布局，SIMD矢量化的部分距离计算以及Cache Awan Away Away Away访问模式。各种数据集的实验 - 从基于图像的CIFAR-10到GIST到包括OpenAI的ADA 2和大型3在内的现代嵌入空间 - 证明全景提供了2---30 $ \ times $端到端的端到端速度，没有召回损失。</li>
</ul>

<h3>Title: Arbitrary Generative Video Interpolation</h3>
<ul>
<li><strong>Authors: </strong>Guozhen Zhang, Haiguang Wang, Chunyu Wang, Yuan Zhou, Qinglin Lu, Limin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00578">https://arxiv.org/abs/2510.00578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00578">https://arxiv.org/pdf/2510.00578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00578]] Arbitrary Generative Video Interpolation(https://arxiv.org/abs/2510.00578)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Video frame interpolation (VFI), which generates intermediate frames from given start and end frames, has become a fundamental function in video generation applications. However, existing generative VFI methods are constrained to synthesize a fixed number of intermediate frames, lacking the flexibility to adjust generated frame rates or total sequence duration. In this work, we present ArbInterp, a novel generative VFI framework that enables efficient interpolation at any timestamp and of any length. Specifically, to support interpolation at any timestamp, we propose the Timestamp-aware Rotary Position Embedding (TaRoPE), which modulates positions in temporal RoPE to align generated frames with target normalized timestamps. This design enables fine-grained control over frame timestamps, addressing the inflexibility of fixed-position paradigms in prior work. For any-length interpolation, we decompose long-sequence generation into segment-wise frame synthesis. We further design a novel appearance-motion decoupled conditioning strategy: it leverages prior segment endpoints to enforce appearance consistency and temporal semantics to maintain motion coherence, ensuring seamless spatiotemporal transitions across segments. Experimentally, we build comprehensive benchmarks for multi-scale frame interpolation (2x to 32x) to assess generalizability across arbitrary interpolation factors. Results show that ArbInterp outperforms prior methods across all scenarios with higher fidelity and more seamless spatiotemporal continuity. Project website: this https URL.</li>
<li><strong>摘要：</strong>视频框架插值（VFI）从给定的启动和最终框架中生成中间帧，已成为视频生成应用程序中的基本功能。但是，现有的生成VFI方法限制为合成固定数量的中间帧，缺乏调整生成的帧速率或总序列持续时间的灵活性。在这项工作中，我们提出了Arbinterp，这是一种新颖的生成VFI框架，可在任何时间戳和任何长度上实现有效的插值。具体而言，为了支持任何时间戳上的插值，我们提出了时间戳感知的旋转位置嵌入（Tarope），该位置嵌入（TAROPE），该位置调节了时间绳中的位置，以使生成的帧与目标归一化时间戳对齐。该设计可以对框架时间戳进行细粒度的控制，以解决固定位置范式在先前工作中的僵化性。对于任何长度的插值，我们将长期生成分解为细分框架合成。我们进一步设计了一种新型的外观运动脱钩的调节策略：它利用先前的段端点来执行外观一致性和时间语义，以保持运动相干性，从而确保跨段的无缝时空过渡。在实验上，我们为多尺度框架插值（2x至32x）建立了全面的基准，以评估跨任意插值因子的普遍性。结果表明，在所有情况下，Arbinterp的表现都优于先前的方法，并且具有更高的忠诚度和更无缝的时空连续性。项目网站：此HTTPS URL。</li>
</ul>

<h3>Title: Eyes-on-Me: Scalable RAG Poisoning through Transferable Attention-Steering Attractors</h3>
<ul>
<li><strong>Authors: </strong>Yen-Shan Chen, Sian-Yao Huang, Cheng-Lin Yang, Yun-Nung Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00586">https://arxiv.org/abs/2510.00586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00586">https://arxiv.org/pdf/2510.00586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00586]] Eyes-on-Me: Scalable RAG Poisoning through Transferable Attention-Steering Attractors(https://arxiv.org/abs/2510.00586)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Existing data poisoning attacks on retrieval-augmented generation (RAG) systems scale poorly because they require costly optimization of poisoned documents for each target phrase. We introduce Eyes-on-Me, a modular attack that decomposes an adversarial document into reusable Attention Attractors and Focus Regions. Attractors are optimized to direct attention to the Focus Region. Attackers can then insert semantic baits for the retriever or malicious instructions for the generator, adapting to new targets at near zero cost. This is achieved by steering a small subset of attention heads that we empirically identify as strongly correlated with attack success. Across 18 end-to-end RAG settings (3 datasets $\times$ 2 retrievers $\times$ 3 generators), Eyes-on-Me raises average attack success rates from 21.9 to 57.8 (+35.9 points, 2.6$\times$ over prior work). A single optimized attractor transfers to unseen black box retrievers and generators without retraining. Our findings establish a scalable paradigm for RAG data poisoning and show that modular, reusable components pose a practical threat to modern AI systems. They also reveal a strong link between attention concentration and model outputs, informing interpretability research.</li>
<li><strong>摘要：</strong>现有的数据中毒攻击对检索增强发电（RAG）系统的规模较差，因为它们需要为每个目标短语对中毒文档进行昂贵的优化。我们介绍了我的眼睛，这是一种模块化攻击，将对抗文档分解为可重复使用的吸引者和重点区域。吸引子被优化以将注意力转移到焦点区域。然后，攻击者可以为发电机插入语义诱饵或恶意说明，以接近零成本适应新目标。这是通过转向一小部分注意力头来实现的，我们从经验上识别为与攻击成功密切相关。在18个端到端的抹布设置（3个数据集$ \ tims $ 2回收者$ \ times $ 3发电机）中，我的眼睛对我的眼睛将平均攻击成功率从21.9.9提高到57.8（+35.9点，2.6 $ \ tims $ times $ times $ times $ times $ tim tim $ tim $一个优化的吸引子转移到看不见的黑匣子猎犬和发电机，而无需再培训。我们的发现建立了用于抹布数据中毒的可扩展范式，并表明模块化，可重复使用的组件对现代AI系统构成了实际威胁。他们还揭示了注意力集中和模型输出之间的密切联系，从而为解释性研究提供了信息。</li>
</ul>

<h3>Title: UCD: Unconditional Discriminator Promotes Nash Equilibrium in GANs</h3>
<ul>
<li><strong>Authors: </strong>Mengfei Xia, Nan Xue, Jiapeng Zhu, Yujun Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00624">https://arxiv.org/abs/2510.00624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00624">https://arxiv.org/pdf/2510.00624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00624]] UCD: Unconditional Discriminator Promotes Nash Equilibrium in GANs(https://arxiv.org/abs/2510.00624)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Adversarial training turns out to be the key to one-step generation, especially for Generative Adversarial Network (GAN) and diffusion model distillation. Yet in practice, GAN training hardly converges properly and struggles in mode collapse. In this work, we quantitatively analyze the extent of Nash equilibrium in GAN training, and conclude that redundant shortcuts by inputting condition in $D$ disables meaningful knowledge extraction. We thereby propose to employ an unconditional discriminator (UCD), in which $D$ is enforced to extract more comprehensive and robust features with no condition injection. In this way, $D$ is able to leverage better knowledge to supervise $G$, which promotes Nash equilibrium in GAN literature. Theoretical guarantee on compatibility with vanilla GAN theory indicates that UCD can be implemented in a plug-in manner. Extensive experiments confirm the significant performance improvements with high efficiency. For instance, we achieved \textbf{1.47 FID} on the ImageNet-64 dataset, surpassing StyleGAN-XL and several state-of-the-art one-step diffusion models. The code will be made publicly available.</li>
<li><strong>摘要：</strong>对抗性训练原来是一步生成的关键，尤其是对于生成对抗网络（GAN）和扩散模型蒸馏而言。然而，实际上，甘恩训练几乎无法正确收敛，并且在模式下挣扎。在这项工作中，我们定量分析了GAN训练中NASH平衡的程度，并得出结论，通过在$ d $中输入条件来禁用有意义的知识提取来得出冗余的捷径。因此，我们建议采用无条件歧视者（UCD），其中$ d $被强制提取更全面，更健壮的功能而无需注入条件。这样，$ d $可以利用更好的知识来监督$ g $，从而促进了纳什文学中的纳什均衡。与香草gan理论兼容的理论保证表明，UCD可以以插件的方式实现。广泛的实验证实了具有高效率的显着性能提高。例如，我们在Imagenet-64数据集，超越stylegan-xl和几个最新的一步一步扩散模型上实现了\ textbf {1.47 fid}。该代码将公开可用。</li>
</ul>

<h3>Title: Virtual Fashion Photo-Shoots: Building a Large-Scale Garment-Lookbook Dataset</h3>
<ul>
<li><strong>Authors: </strong>Yannick Hauri, Luca A. Lanzendörfer, Till Aczel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00633">https://arxiv.org/abs/2510.00633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00633">https://arxiv.org/pdf/2510.00633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00633]] Virtual Fashion Photo-Shoots: Building a Large-Scale Garment-Lookbook Dataset(https://arxiv.org/abs/2510.00633)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Fashion image generation has so far focused on narrow tasks such as virtual try-on, where garments appear in clean studio environments. In contrast, editorial fashion presents garments through dynamic poses, diverse locations, and carefully crafted visual narratives. We introduce the task of virtual fashion photo-shoot, which seeks to capture this richness by transforming standardized garment images into contextually grounded editorial imagery. To enable this new direction, we construct the first large-scale dataset of garment-lookbook pairs, bridging the gap between e-commerce and fashion media. Because such pairs are not readily available, we design an automated retrieval pipeline that aligns garments across domains, combining visual-language reasoning with object-level localization. We construct a dataset with three garment-lookbook pair accuracy levels: high quality (10,000 pairs), medium quality (50,000 pairs), and low quality (300,000 pairs). This dataset offers a foundation for models that move beyond catalog-style generation and toward fashion imagery that reflects creativity, atmosphere, and storytelling.</li>
<li><strong>摘要：</strong>到目前为止，时尚形象的生成集中在狭窄的任务上，例如虚拟试验，衣服出现在干净的工作室环境中。相比之下，社论时尚通过动态姿势，不同的位置和精心制作的视觉叙述来展示服装。我们介绍了虚拟时尚摄影的任务，该任务旨在通过将标准化的服装图像转换为上下文扎根的编辑图像来捕捉这种丰富性。为了启用这个新的方向，我们构建了第一个大规模的服装外观书籍对数据集，弥合了电子商务和时尚媒体之间的差距。由于此类对不容易获得，因此我们设计了一种自动检索管道，该管道将跨域的服装对齐，将视觉语言推理与对象级定位相结合。我们构建了一个具有三个服装上看起来对的数据集精度：高质量（10,000对），中等质量（50,000对）和低质量（300,000对）。该数据集为模型提供了一个基础，该模型超越了目录风格的生成和反映创造力，氛围和讲故事的时尚图像。</li>
</ul>

<h3>Title: LAKAN: Landmark-assisted Adaptive Kolmogorov-Arnold Network for Face Forgery Detection</h3>
<ul>
<li><strong>Authors: </strong>Jiayao Jiang, Siran Peng, Bin Liu, Qi Chu, Nenghai Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00634">https://arxiv.org/abs/2510.00634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00634">https://arxiv.org/pdf/2510.00634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00634]] LAKAN: Landmark-assisted Adaptive Kolmogorov-Arnold Network for Face Forgery Detection(https://arxiv.org/abs/2510.00634)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The rapid development of deepfake generation techniques necessitates robust face forgery detection algorithms. While methods based on Convolutional Neural Networks (CNNs) and Transformers are effective, there is still room for improvement in modeling the highly complex and non-linear nature of forgery artifacts. To address this issue, we propose a novel detection method based on the Kolmogorov-Arnold Network (KAN). By replacing fixed activation functions with learnable splines, our KAN-based approach is better suited to this challenge. Furthermore, to guide the network's focus towards critical facial areas, we introduce a Landmark-assisted Adaptive Kolmogorov-Arnold Network (LAKAN) module. This module uses facial landmarks as a structural prior to dynamically generate the internal parameters of the KAN, creating an instance-specific signal that steers a general-purpose image encoder towards the most informative facial regions with artifacts. This core innovation creates a powerful combination between geometric priors and the network's learning process. Extensive experiments on multiple public datasets show that our proposed method achieves superior performance.</li>
<li><strong>摘要：</strong>深泡产生技术的快速发展需要强大的面部伪造算法。尽管基于卷积神经网络（CNN）和变压器有效的方法是有效的，但在建模伪造的高度复杂和非线性性质方面仍然有改善的余地。为了解决这个问题，我们提出了一种基于Kolmogorov-Arnold网络（KAN）的新型检测方法。通过用可学习的花样替换固定的激活功能，我们基于KAN的方法更适合这一挑战。此外，为了指导网络对关键面部区域的关注，我们引入了具有里程碑意义的适应性Kolmogorov-Arnold网络（Lakan）模块。该模块在动态生成KAN的内部参数之前使用面部地标作为结构，从而创建了一个特定于实例的信号，该信号将通用图像编码器引导到具有工件的最有用的面部区域。这种核心创新在几何先验和网络的学习过程之间创造了强大的组合。多个公共数据集的广泛实验表明，我们提出的方法可实现出色的性能。</li>
</ul>

<h3>Title: Erased, But Not Forgotten: Erased Rectified Flow Transformers Still Remain Unsafe Under Concept Attack</h3>
<ul>
<li><strong>Authors: </strong>Nanxiang Jiang, Zhaoxin Fan, Enhan Kang, Daiheng Gao, Yun Zhou, Yanxia Chang, Zheng Zhu, Yeying Jin, Wenjun Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00635">https://arxiv.org/abs/2510.00635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00635">https://arxiv.org/pdf/2510.00635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00635]] Erased, But Not Forgotten: Erased Rectified Flow Transformers Still Remain Unsafe Under Concept Attack(https://arxiv.org/abs/2510.00635)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in text-to-image (T2I) diffusion models have enabled impressive generative capabilities, but they also raise significant safety concerns due to the potential to produce harmful or undesirable content. While concept erasure has been explored as a mitigation strategy, most existing approaches and corresponding attack evaluations are tailored to Stable Diffusion (SD) and exhibit limited effectiveness when transferred to next-generation rectified flow transformers such as Flux. In this work, we present ReFlux, the first concept attack method specifically designed to assess the robustness of concept erasure in the latest rectified flow-based T2I framework. Our approach is motivated by the observation that existing concept erasure techniques, when applied to Flux, fundamentally rely on a phenomenon known as attention localization. Building on this insight, we propose a simple yet effective attack strategy that specifically targets this property. At its core, a reverse-attention optimization strategy is introduced to effectively reactivate suppressed signals while stabilizing attention. This is further reinforced by a velocity-guided dynamic that enhances the robustness of concept reactivation by steering the flow matching process, and a consistency-preserving objective that maintains the global layout and preserves unrelated content. Extensive experiments consistently demonstrate the effectiveness and efficiency of the proposed attack method, establishing a reliable benchmark for evaluating the robustness of concept erasure strategies in rectified flow transformers.</li>
<li><strong>摘要：</strong>文本到图像（T2I）扩散模型的最新进展已使其具有令人印象深刻的生成能力，但由于可能产生有害或不良内容的潜力，它们也引起了重大的安全问题。尽管概念擦除已被探讨为缓解策略，但大多数现有的方法和相应的攻击评估都是针对稳定扩散（SD）量身定制的，并且当转移到下一代整流的流动变压器（如磁通）时，效果有限。在这项工作中，我们提出了回顾，这是第一种概念攻击方法，专门旨在评估最新基于流动的T2I框架中概念擦除的鲁棒性。我们的方法是由现有概念擦除技术应用于通量的观察到的动机，从根本上讲，它从根本上依赖于一种被称为注意力定位的现象。在这种见解的基础上，我们提出了一种简单而有效的攻击策略，专门针对该物业。从本质上讲，引入了反向注意的优化策略，以有效地重新激活抑制信号，同时稳定注意力。速度引导的动态进一步加强了这一点，该动态通过转向流匹配过程来增强概念重新激活的鲁棒性，以及一个保持全局布局并保留无关内容的一致性保留目标。广泛的实验始终证明了拟议的攻击方法的有效性和效率，建立了可靠的基准，用于评估整流流变压器中概念擦除策略的鲁棒性。</li>
</ul>

<h3>Title: A Geometric Unification of Generative AI with Manifold-Probabilistic Projection Models</h3>
<ul>
<li><strong>Authors: </strong>Leah Bar, Liron Mor Yosef, Shai Zucker, Neta Shoham, Inbar Seroussi, Nir Sochen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00666">https://arxiv.org/abs/2510.00666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00666">https://arxiv.org/pdf/2510.00666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00666]] A Geometric Unification of Generative AI with Manifold-Probabilistic Projection Models(https://arxiv.org/abs/2510.00666)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generation, generative</a></li>
<li><strong>Abstract: </strong>The foundational premise of generative AI for images is the assumption that images are inherently low-dimensional objects embedded within a high-dimensional space. Additionally, it is often implicitly assumed that thematic image datasets form smooth or piecewise smooth manifolds. Common approaches overlook the geometric structure and focus solely on probabilistic methods, approximating the probability distribution through universal approximation techniques such as the kernel method. In some generative models, the low dimensional nature of the data manifest itself by the introduction of a lower dimensional latent space. Yet, the probability distribution in the latent or the manifold coordinate space is considered uninteresting and is predefined or considered uniform. This study unifies the geometric and probabilistic perspectives by providing a geometric framework and a kernel-based probabilistic method simultaneously. The resulting framework demystifies diffusion models by interpreting them as a projection mechanism onto the manifold of ``good images''. This interpretation leads to the construction of a new deterministic model, the Manifold-Probabilistic Projection Model (MPPM), which operates in both the representation (pixel) space and the latent space. We demonstrate that the Latent MPPM (LMPPM) outperforms the Latent Diffusion Model (LDM) across various datasets, achieving superior results in terms of image restoration and generation.</li>
<li><strong>摘要：</strong>对于图像的生成AI的基础前提是，假设图像是固有的低维对象，该对象嵌入了高维空间中。此外，通常隐式假定主题图像数据集形成平滑或分段光滑的歧管。常见方法忽略了几何结构，仅关注概率方法，通过通用近似技术（例如内核方法）近似概率分布。在某些生成模型中，数据的低维质通过引入较低的潜在空间来表现出来。但是，潜在或歧管坐标空间中的概率分布被认为是无趣的，并且被认为是预定义的或被认为是统一的。这项研究通过同时提供几何框架和基于内核的概率方法来统一几何和概率的观点。最终的框架通过将扩散模型解释为``好图像''的流形来揭示扩散模型。这种解释导致建立一个新的确定性模型，即歧管型投影模型（MPPM），该模型在表示形式（像素）空间和潜在空间中都运行。我们证明，潜在MPPM（LMPPM）在各种数据集上优于潜在扩散模型（LDM），从而在图像恢复和生成方面取得了卓越的结果。</li>
</ul>

<h3>Title: Extreme Blind Image Restoration via Prompt-Conditioned Information Bottleneck</h3>
<ul>
<li><strong>Authors: </strong>Hongeun Kim, Bryan Sangwoo Kim, Jong Chul Ye</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00728">https://arxiv.org/abs/2510.00728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00728">https://arxiv.org/pdf/2510.00728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00728]] Extreme Blind Image Restoration via Prompt-Conditioned Information Bottleneck(https://arxiv.org/abs/2510.00728)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Blind Image Restoration (BIR) methods have achieved remarkable success but falter when faced with Extreme Blind Image Restoration (EBIR), where inputs suffer from severe, compounded degradations beyond their training scope. Directly learning a mapping from extremely low-quality (ELQ) to high-quality (HQ) images is challenging due to the massive domain gap, often leading to unnatural artifacts and loss of detail. To address this, we propose a novel framework that decomposes the intractable ELQ-to-HQ restoration process. We first learn a projector that maps an ELQ image onto an intermediate, less-degraded LQ manifold. This intermediate image is then restored to HQ using a frozen, off-the-shelf BIR model. Our approach is grounded in information theory; we provide a novel perspective of image restoration as an Information Bottleneck problem and derive a theoretically-driven objective to train our projector. This loss function effectively stabilizes training by balancing a low-quality reconstruction term with a high-quality prior-matching term. Our framework enables Look Forward Once (LFO) for inference-time prompt refinement, and supports plug-and-play strengthening of existing image restoration models without need for finetuning. Extensive experiments under severe degradation regimes provide a thorough analysis of the effectiveness of our work.</li>
<li><strong>摘要：</strong>盲图恢复（BIR）方法取得了显着的成功，但面对极端盲图恢复（EBIR），在其训练范围之外遭受了严重的复合降解。由于巨大的域间隙，直接学习从极低质量（ELQ）到高质量（HQ）图像的映射非常具有挑战性，通常会导致不自然的人工制品和细节丢失。为了解决这个问题，我们提出了一个新颖的框架，该框架分解了棘手的ELQ-TO-HQ恢复过程。我们首先学习一个将ELQ图像映射到中间，衰落的LQ歧管上的投影仪。然后，使用冷冻的，现成的BIR模型将此中间图像恢复为HQ。我们的方法基于信息理论。我们提供了图像恢复的新观点，作为信息瓶颈问题，并得出了一个理论上驱动的目标来训练投影仪。这种损失功能通过平衡低质量重建项和高质量的先前匹配项来有效稳定训练。我们的框架使一旦期待（LFO）进行推理时间提示，并支持无需填充的现有图像修复模型的插件加强。在严重退化方案下进行的广泛实验对我们工作的有效性进行了彻底的分析。</li>
</ul>

<h3>Title: Solar PV Installation Potential Assessment on Building Facades Based on Vision and Language Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Ruyu Liu, Dongxu Zhuang, Jianhua Zhang, Arega Getaneh Abate, Per Sieverts Nielsen, Ben Wang, Xiufeng Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00797">https://arxiv.org/abs/2510.00797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00797">https://arxiv.org/pdf/2510.00797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00797]] Solar PV Installation Potential Assessment on Building Facades Based on Vision and Language Foundation Models(https://arxiv.org/abs/2510.00797)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Building facades represent a significant untapped resource for solar energy generation in dense urban environments, yet assessing their photovoltaic (PV) potential remains challenging due to complex geometries and semantic com ponents. This study introduces SF-SPA (Semantic Facade Solar-PV Assessment), an automated framework that transforms street-view photographs into quantitative PV deployment assessments. The approach combines com puter vision and artificial intelligence techniques to address three key challenges: perspective distortion correction, semantic understanding of facade elements, and spatial reasoning for PV layout optimization. Our four-stage pipeline processes images through geometric rectification, zero-shot semantic segmentation, Large Language Model (LLM) guided spatial reasoning, and energy simulation. Validation across 80 buildings in four countries demonstrates ro bust performance with mean area estimation errors of 6.2% &#177; 2.8% compared to expert annotations. The auto mated assessment requires approximately 100 seconds per building, a substantial gain in efficiency over manual methods. Simulated energy yield predictions confirm the method's reliability and applicability for regional poten tial studies, urban energy planning, and building-integrated photovoltaic (BIPV) deployment. Code is available at: https:github.com/CodeAXu/Solar-PV-Installation</li>
<li><strong>摘要：</strong>建筑外墙代表着在密集的城市环境中太阳能发电的重要资源，但由于复杂的几何形状和语义com的影响，评估其光伏（PV）潜力仍然具有挑战性。这项研究介绍了SF-SPA（语义外观太阳能电视评估），这是一个自动化框架，将街道视图照片转换为定量的PV部署评估。该方法结合了COM PUTER愿景和人工智能技术，以应对三个关键挑战：透视失真校正，对立面元素的语义理解以及用于PV布局优化的空间推理。我们的四个阶段管道通过几何整流，零射击语义分割，大语言模型（LLM）引导的空间推理和能量模拟来处理图像。在四个国家的80个建筑物中进行的验证表明，RO破裂性能，平均面积估计错误为6.2％＆＃177;与专家注释相比，为2.8％。自动交配的评估需要每个建筑物约100秒，这是对手动方法的效率大幅提高。模拟的能源产量预测证实了该方法对区域潜在研究，城市能源规划和建筑集成的光伏（BIPV）部署的可靠性和适用性。代码可用：https：github.com/codeaxu/solar-pv-installation</li>
</ul>

<h3>Title: Guiding Evolutionary Molecular Design: Adding Reinforcement Learning for Mutation Selection</h3>
<ul>
<li><strong>Authors: </strong>Gaelle Milon-Harnois, Chaimaa Touhami, Nicolas Gutowski, Benoit Da Mota, Thomas Cauchy</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00802">https://arxiv.org/abs/2510.00802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00802">https://arxiv.org/pdf/2510.00802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00802]] Guiding Evolutionary Molecular Design: Adding Reinforcement Learning for Mutation Selection(https://arxiv.org/abs/2510.00802)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The efficient exploration of chemical space remains a central challenge, as many generative models still produce unstable or non-synthesizable compounds. To address these limitations, we present EvoMol-RL, a significant extension of the EvoMol evolutionary algorithm that integrates reinforcement learning to guide molecular mutations based on local structural context. By leveraging Extended Connectivity Fingerprints (ECFPs), EvoMol-RL learns context-aware mutation policies that prioritize chemically plausible transformations. This approach significantly improves the generation of valid and realistic molecules, reducing the frequency of structural artifacts and enhancing optimization performance. The results demonstrate that EvoMol-RL consistently outperforms its baseline in molecular pre-filtering realism. These results emphasize the effectiveness of combining reinforcement learning with molecular fingerprints to generate chemically relevant molecular structures.</li>
<li><strong>摘要：</strong>化学空间的有效探索仍然是一个核心挑战，因为许多生成模型仍然产生不稳定或不可合理的化合物。为了解决这些局限性，我们提出了evomol-rl，这是evomol进化算法的显着扩展，该算法将增强学习整合到基于局部结构环境的分子突变。通过利用扩展的连通性指纹（ECFP），Evomol-RL学习了上下文感知的突变策略，这些突变策略优先考虑化学上合理的转换。这种方法可显着改善有效和逼真的分子的产生，从而降低结构伪影的频率并增强优化性能。结果表明，Evomol-RL在分子预过滤现实主义中始终优于其基线。这些结果强调了将增强学习与分子指纹结合起来产生化学相关的分子结构的有效性。</li>
</ul>

<h3>Title: MG2FlowNet: Accelerating High-Reward Sample Generation via Enhanced MCTS and Greediness Control</h3>
<ul>
<li><strong>Authors: </strong>Rui Zhu, Xuan Yu, Yudong Zhang, Chen Zhang, Xu Wang, Yang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00805">https://arxiv.org/abs/2510.00805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00805">https://arxiv.org/pdf/2510.00805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00805]] MG2FlowNet: Accelerating High-Reward Sample Generation via Enhanced MCTS and Greediness Control(https://arxiv.org/abs/2510.00805)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative Flow Networks (GFlowNets) have emerged as a powerful tool for generating diverse and high-reward structured objects by learning to sample from a distribution proportional to a given reward function. Unlike conventional reinforcement learning (RL) approaches that prioritize optimization of a single trajectory, GFlowNets seek to balance diversity and reward by modeling the entire trajectory distribution. This capability makes them especially suitable for domains such as molecular design and combinatorial optimization. However, existing GFlowNets sampling strategies tend to overexplore and struggle to consistently generate high-reward samples, particularly in large search spaces with sparse high-reward regions. Therefore, improving the probability of generating high-reward samples without sacrificing diversity remains a key challenge under this premise. In this work, we integrate an enhanced Monte Carlo Tree Search (MCTS) into the GFlowNets sampling process, using MCTS-based policy evaluation to guide the generation toward high-reward trajectories and Polynomial Upper Confidence Trees (PUCT) to balance exploration and exploitation adaptively, and we introduce a controllable mechanism to regulate the degree of greediness. Our method enhances exploitation without sacrificing diversity by dynamically balancing exploration and reward-driven guidance. The experimental results show that our method can not only accelerate the speed of discovering high-reward regions but also continuously generate high-reward samples, while preserving the diversity of the generative distribution. All implementations are available at this https URL.</li>
<li><strong>摘要：</strong>生成流动网络（Gflownets）已成为一种强大的工具，可以通过学习从分布与给定奖励功能成比例的分配中进行采样，从而生成多样化和高回报的结构化对象。与传统的增强学习（RL）方法相比，优先优化单个轨迹的方法不同，Gflownets试图通过建模整个轨迹分布来平衡多样性和奖励。这种能力使其特别适合分子设计和组合优化等领域。但是，现有的Gflownets抽样策略倾向于过度探索，并难以始终如一地产生高回报样本，尤其是在稀疏高回报区域的大型搜索空间中。因此，在此前提下，提高不牺牲多样性而无需牺牲多样性而产生高回报样本的概率仍然是一个关键挑战。在这项工作中，我们使用基于MCTS的策略评估将增强的蒙特卡洛树搜索（MCT）集成到GFLOWNETS取样过程中，以指导一代人朝着高回报轨迹和多项式上层置信树（Puct）迈进，以平衡探索和剥削，并对其进行适应性的平衡，我们引入了可控制的机制来调节贪婪程度。我们的方法通过动态平衡探索和奖励驱动的指导来增强剥削，而不会牺牲多样性。实验结果表明，我们的方法不仅可以加速发现高回报区域的速度，而且可以不断生成高回报样本，同时保留生成分布的多样性。所有实现都可以在此HTTPS URL上获得。</li>
</ul>

<h3>Title: From Seeing to Predicting: A Vision-Language Framework for Trajectory Forecasting and Controlled Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Fan Yang, Zhiyang Chen, Yousong Zhu, Xin Li, Jinqiao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00806">https://arxiv.org/abs/2510.00806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00806">https://arxiv.org/pdf/2510.00806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00806]] From Seeing to Predicting: A Vision-Language Framework for Trajectory Forecasting and Controlled Video Generation(https://arxiv.org/abs/2510.00806)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Current video generation models produce physically inconsistent motion that violates real-world dynamics. We propose TrajVLM-Gen, a two-stage framework for physics-aware image-to-video generation. First, we employ a Vision Language Model to predict coarse-grained motion trajectories that maintain consistency with real-world physics. Second, these trajectories guide video generation through attention-based mechanisms for fine-grained motion refinement. We build a trajectory prediction dataset based on video tracking data with realistic motion patterns. Experiments on UCF-101 and MSR-VTT demonstrate that TrajVLM-Gen outperforms existing methods, achieving competitive FVD scores of 545 on UCF-101 and 539 on MSR-VTT.</li>
<li><strong>摘要：</strong>当前的视频生成模型会产生身体上不一致的运动，违反了现实世界动态。我们提出了Trajvlm-gen，这是一个两阶段的物理意识到的图像与视频生成的框架。首先，我们采用视觉语言模型来预测与现实世界物理学保持一致性的粗粒度运动轨迹。其次，这些轨迹通过基于注意力的机制指导视频生成，以进行细粒度的运动精致。我们基于具有逼真的运动模式的视频跟踪数据构建轨迹预测数据集。在UCF-101和MSR-VTT上的实验表明，Trajvlm-Gen的表现优于现有方法，在UCF-101上达到545的竞争性FVD分数和MSR-VTT上的539。</li>
</ul>

<h3>Title: What You See is What You Ask: Evaluating Audio Descriptions</h3>
<ul>
<li><strong>Authors: </strong>Divy Kala, Eshika Khandelwal, Makarand Tapaswi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00808">https://arxiv.org/abs/2510.00808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00808">https://arxiv.org/pdf/2510.00808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00808]] What You See is What You Ask: Evaluating Audio Descriptions(https://arxiv.org/abs/2510.00808)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Audio descriptions (ADs) narrate important visual details in movies, enabling Blind and Low Vision (BLV) users to understand narratives and appreciate visual details. Existing works in automatic AD generation mostly focus on few-second trimmed clips, and evaluate them by comparing against a single ground-truth reference AD. However, writing ADs is inherently subjective. Through alignment and analysis of two independent AD tracks for the same movies, we quantify the subjectivity in when and whether to describe, and what and how to highlight. Thus, we show that working with trimmed clips is inadequate. We propose ADQA, a QA benchmark that evaluates ADs at the level of few-minute long, coherent video segments, testing whether they would help BLV users understand the story and appreciate visual details. ADQA features visual appreciation (VA) questions about visual facts and narrative understanding (NU) questions based on the plot. Through ADQA, we show that current AD generation methods lag far behind human-authored ADs. We conclude with several recommendations for future work and introduce a public leaderboard for benchmarking.</li>
<li><strong>摘要：</strong>音频描述（ADS）叙述了电影中的重要视觉细节，使盲目和低视力（BLV）用户能够理解叙事并欣赏视觉细节。自动广告生成中的现有作品主要集中在几个秒的修剪夹上，并通过与单个地面的参考广告进行比较来评估它们。但是，写广告本质上是主观的。通过对同一电影的两个独立广告轨道的对齐和分析，我们量化了何时以及是否描述以及如何突出显示的主观性。因此，我们表明使用修剪夹的工作不足。我们提出了ADQA，ADQA是一个质量检查基准，该基准评估了几分钟长，连贯的视频片段的广告，并测试他们是否会帮助BLV用户理解故事并欣赏视觉细节。 ADQA具有有关视觉事实和叙事理解（NU）问题的视觉欣赏（VA）问题。通过ADQA，我们表明当前的广告生成方法远远远远落后于人类作者的广告。我们最终提出了几项建议以后的工作，并介绍了公共排行榜进行基准测试。</li>
</ul>

<h3>Title: Learn to Guide Your Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Alexandre Galashov, Ashwini Pokle, Arnaud Doucet, Arthur Gretton, Mauricio Delbracio, Valentin De Bortoli</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00815">https://arxiv.org/abs/2510.00815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00815">https://arxiv.org/pdf/2510.00815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00815]] Learn to Guide Your Diffusion Model(https://arxiv.org/abs/2510.00815)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Classifier-free guidance (CFG) is a widely used technique for improving the perceptual quality of samples from conditional diffusion models. It operates by linearly combining conditional and unconditional score estimates using a guidance weight $\omega$. While a large, static weight can markedly improve visual results, this often comes at the cost of poorer distributional alignment. In order to better approximate the target conditional distribution, we instead learn guidance weights $\omega_{c,(s,t)}$, which are continuous functions of the conditioning $c$, the time $t$ from which we denoise, and the time $s$ towards which we denoise. We achieve this by minimizing the distributional mismatch between noised samples from the true conditional distribution and samples from the guided diffusion process. We extend our framework to reward guided sampling, enabling the model to target distributions tilted by a reward function $R(x_0,c)$, defined on clean data and a conditioning $c$. We demonstrate the effectiveness of our methodology on low-dimensional toy examples and high-dimensional image settings, where we observe improvements in Fréchet inception distance (FID) for image generation. In text-to-image applications, we observe that employing a reward function given by the CLIP score leads to guidance weights that improve image-prompt alignment.</li>
<li><strong>摘要：</strong>无分类器引导（CFG）是一种广泛使用的技术，用于提高条件扩散模型的样品的感知质量。它通过使用指导权重$ \ omega $线性结合条件和无条件得分估计来运行。尽管静态重量很大可以显着改善视觉效果，但这通常是以较差的分布对齐为代价。为了更好地近似目标条件分布，我们会学习指导权重$ \ omega_ {c，（s，t）} $，它是条件$ c $的连续函数，我们从中denoise的时间$ t $，以及我们denoise的时间$ s $。我们通过最大程度地减少来自真实条件分布的Nuises样品与来自引导扩散过程的样品之间的分布不匹配来实现这一目标。我们将框架扩展到奖励有指导的采样，使该模型能够以奖励功能$ r（x_0，c）$倾斜的目标分布，在干净的数据和条件$ c $上定义。我们证明了我们的方法学对低维玩具示例和高维图像设置的有效性，在此我们观察到FréchetInception距离（FID）的改善以生成图像。在文本到图像应用程序中，我们观察到，采用剪辑得分给出的奖励功能会导致指导权重，从而改善图像贡献。</li>
</ul>

<h3>Title: NSARM: Next-Scale Autoregressive Modeling for Robust Real-World Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Xiangtao Kong, Rongyuan Wu, Shuaizheng Liu, Lingchen Sun, Lei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00820">https://arxiv.org/abs/2510.00820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00820">https://arxiv.org/pdf/2510.00820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00820]] NSARM: Next-Scale Autoregressive Modeling for Robust Real-World Image Super-Resolution(https://arxiv.org/abs/2510.00820)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generation, generative</a></li>
<li><strong>Abstract: </strong>Most recent real-world image super-resolution (Real-ISR) methods employ pre-trained text-to-image (T2I) diffusion models to synthesize the high-quality image either from random Gaussian noise, which yields realistic results but is slow due to iterative denoising, or directly from the input low-quality image, which is efficient but at the price of lower output quality. These approaches train ControlNet or LoRA modules while keeping the pre-trained model fixed, which often introduces over-enhanced artifacts and hallucinations, suffering from the robustness to inputs of varying degradations. Recent visual autoregressive (AR) models, such as pre-trained Infinity, can provide strong T2I generation capabilities while offering superior efficiency by using the bitwise next-scale prediction strategy. Building upon next-scale prediction, we introduce a robust Real-ISR framework, namely Next-Scale Autoregressive Modeling (NSARM). Specifically, we train NSARM in two stages: a transformation network is first trained to map the input low-quality image to preliminary scales, followed by an end-to-end full-model fine-tuning. Such a comprehensive fine-tuning enhances the robustness of NSARM in Real-ISR tasks without compromising its generative capability. Extensive quantitative and qualitative evaluations demonstrate that as a pure AR model, NSARM achieves superior visual results over existing Real-ISR methods while maintaining a fast inference speed. Most importantly, it demonstrates much higher robustness to the quality of input images, showing stronger generalization performance. Project page: this https URL</li>
<li><strong>摘要：</strong>最新的现实世界图像超分辨率（Real-ISR）方法采用预先训练的文本对图像（T2I）扩散模型来合成随机高斯噪声的高质量图像，从而产生了现实的结果，但由于迭代性降解而产生较慢，由于迭代性降低，或直接从输入低质量图像中直接以较低的Quequielity图像，价格较低，但价格较低，但价格较低，但价格较低，但价格降低了。这些方法训练控制网或LORA模块，同时保持预先训练的模型固定，这通常会引入过度增强的人工制品和幻觉，遭受了稳健性和不同降级投入的投入。最近的视觉自我回归（AR）模型（例如预训练的无穷大）可以提供强大的T2I生成能力，同时通过使用位次级尺度预测策略，提供出色的效率。在下一尺度预测的基础上，我们引入了一个强大的现实ISR框架，即次尺度自回旋建模（NSARM）。具体来说，我们在两个阶段进行训练NSARM：首先对转换网络进行训练，以将输入低质量图像映射到初步尺度，然后进行端到端的全模型微调。如此全面的微调增强了NSARM在实施ISR任务中的鲁棒性，而不会损害其生成能力。广泛的定量和定性评估表明，作为纯AR模型，NSARM在保持快速推理速度的同时，在现有的实现ISR方法上取得了优越的视觉结果。最重要的是，它表现出对输入图像质量的更高鲁棒性，显示出更强的概括性能。项目页面：此HTTPS URL</li>
</ul>

<h3>Title: Can World Models Benefit VLMs for World Dynamics?</h3>
<ul>
<li><strong>Authors: </strong>Kevin Zhang, Kuangzhi Ge, Xiaowei Chi, Renrui Zhang, Shaojun Shi, Zhen Dong, Sirui Han, Shanghang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00855">https://arxiv.org/abs/2510.00855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00855">https://arxiv.org/pdf/2510.00855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00855]] Can World Models Benefit VLMs for World Dynamics?(https://arxiv.org/abs/2510.00855)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Trained on internet-scale video data, generative world models are increasingly recognized as powerful world simulators that can generate consistent and plausible dynamics over structure, motion, and physics. This raises a natural question: with the advent of strong video foundational models, might they supplant conventional vision encoder paradigms for general-purpose multimodal understanding? While recent studies have begun to explore the potential of world models on common vision tasks, these explorations typically lack a systematic investigation of generic, multimodal tasks. In this work, we strive to investigate the capabilities when world model priors are transferred into Vision-Language Models: we re-purpose a video diffusion model as a generative encoder to perform a single denoising step and treat the resulting latents as a set of visual embedding. We empirically investigate this class of models, which we refer to as World-Language Models (WorldLMs), and we find that generative encoders can capture latents useful for downstream understanding that show distinctions from conventional encoders. Naming our best-performing variant Dynamic Vision Aligner (DyVA), we further discover that this method significantly enhances spatial reasoning abilities and enables single-image models to perform multi-frame reasoning. Through the curation of a suite of visual reasoning tasks, we find DyVA to surpass both open-source and proprietary baselines, achieving state-of-the-art or comparable performance. We attribute these gains to WorldLM's inherited motion-consistency internalization from video pre-training. Finally, we systematically explore extensive model designs to highlight promising directions for future work. We hope our study can pave the way for a new family of VLMs that leverage priors from world models and are on a promising path towards generalist vision learners.</li>
<li><strong>摘要：</strong>经过互联网规模的视频数据培训，生成世界模型越来越被认为是强大的世界模拟器，可以在结构，运动和物理学上产生一致且合理的动态。这提出了一个自然的问题：随着强大的视频基础模型的出现，它们是否可以取代常规视觉编码器范式以通用多种模式理解？尽管最近的研究已经开始探索世界模型在共同的视觉任务上的潜力，但这些探索通常缺乏对通用的多模式任务的系统研究。在这项工作中，我们努力调查当世界模型先验转移到视觉模型中时的功能：我们将视频扩散模型重新使用作为生成编码器，以执行单个DeNoising步骤，并将所得的潜伏期视为一组视觉嵌入。我们从经验上研究了这类模型，我们将其称为世界语言模型（WorldLMS），我们发现生成型编码器可以捕获有助于下游理解的潜伏期，从而显示出与常规编码者的区别。命名我们表现最佳的变体动态视觉对准器（DYVA），我们进一步发现，这种方法显着增强了空间推理能力，并使单像模型能够执行多帧推理。通过一套视觉推理任务的策划，我们发现Dyva超过了开源和专有基准，从而实现了最先进的或可比的性能。我们将这些收益归因于WorldLM从视频预训练中的继承的运动抗性内在化。最后，我们系统地探索了广泛的模型设计，以突出未来工作的有希望的方向。我们希望我们的研究能为一个新的VLM家族铺平道路，该家族利用世界模型的先验，并正走向通用视力学习者的有前途的道路。</li>
</ul>

<h3>Title: Population Synthesis using Incomplete Information</h3>
<ul>
<li><strong>Authors: </strong>Tanay Rastogi, Daniel Jonsson, Anders Karlström</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00859">https://arxiv.org/abs/2510.00859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00859">https://arxiv.org/pdf/2510.00859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00859]] Population Synthesis using Incomplete Information(https://arxiv.org/abs/2510.00859)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper presents a population synthesis model that utilizes the Wasserstein Generative-Adversarial Network (WGAN) for training on incomplete microsamples. By using a mask matrix to represent missing values, the study proposes a WGAN training algorithm that lets the model learn from a training dataset that has some missing information. The proposed method aims to address the challenge of missing information in microsamples on one or more attributes due to privacy concerns or data collection constraints. The paper contrasts WGAN models trained on incomplete microsamples with those trained on complete microsamples, creating a synthetic population. We conducted a series of evaluations of the proposed method using a Swedish national travel survey. We validate the efficacy of the proposed method by generating synthetic populations from all the models and comparing them to the actual population dataset. The results from the experiments showed that the proposed methodology successfully generates synthetic data that closely resembles a model trained with complete data as well as the actual population. The paper contributes to the field by providing a robust solution for population synthesis with incomplete data, opening avenues for future research, and highlighting the potential of deep generative models in advancing population synthesis capabilities.</li>
<li><strong>摘要：</strong>本文提出了一种种群合成模型，该模型利用Wasserstein生成对抗网络（WGAN）进行不完整的微样本培训。通过使用蒙版矩阵来表示缺失值，该研究提出了一种智能培训算法，该算法使该模型可以从具有一些缺失信息的培训数据集中学习。提出的方法旨在解决由于隐私问题或数据收集约束，微型示例中缺少信息的挑战。该论文与对不完整的微型样本进行训练的WGAN模型与在完整的微型样本中训练的模型，从而创造了合成人群。我们使用瑞典国家旅行调查对拟议方法进行了一系列评估。我们通过从所有模型中生成合成种群并将其与实际的总体数据集进行比较来验证所提出的方法的功效。实验的结果表明，所提出的方法成功地生成了与经过完整数据以及实际人群训练的模型非常相似的合成数据。本文通过提供不完整的数据，为未来的研究开辟途径，并强调了深层生成模型在推进种群综合能力方面的潜力，从而为该领域做出了贡献。</li>
</ul>

<h3>Title: Gather-Scatter Mamba: Accelerating Propagation with Efficient State Space Model</h3>
<ul>
<li><strong>Authors: </strong>Hyun-kyu Ko, Youbin Kim, Jihyeon Park, Dongheok Park, Gyeongjin Kang, Wonjun Cho, Hyung Yi, Eunbyung Park</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00862">https://arxiv.org/abs/2510.00862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00862">https://arxiv.org/pdf/2510.00862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00862]] Gather-Scatter Mamba: Accelerating Propagation with Efficient State Space Model(https://arxiv.org/abs/2510.00862)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>State Space Models (SSMs)-most notably RNNs-have historically played a central role in sequential modeling. Although attention mechanisms such as Transformers have since dominated due to their ability to model global context, their quadratic complexity and limited scalability make them less suited for long sequences. Video super-resolution (VSR) methods have traditionally relied on recurrent architectures to propagate features across frames. However, such approaches suffer from well-known issues including vanishing gradients, lack of parallelism, and slow inference speed. Recent advances in selective SSMs like Mamba offer a compelling alternative: by enabling input-dependent state transitions with linear-time complexity, Mamba mitigates these issues while maintaining strong long-range modeling capabilities. Despite this potential, Mamba alone struggles to capture fine-grained spatial dependencies due to its causal nature and lack of explicit context aggregation. To address this, we propose a hybrid architecture that combines shifted window self-attention for spatial context aggregation with Mamba-based selective scanning for efficient temporal propagation. Furthermore, we introduce Gather-Scatter Mamba (GSM), an alignment-aware mechanism that warps features toward a center anchor frame within the temporal window before Mamba propagation and scatters them back afterward, effectively reducing occlusion artifacts and ensuring effective redistribution of aggregated information across all frames. The official implementation is provided at: this https URL.</li>
<li><strong>摘要：</strong>状态空间模型（SSM） - 最著名的是RNN-HAVE在历史上在顺序建模中起着核心作用。尽管此后的注意机制因其对全局上下文进行建模的能力而占主导地位，但它们的二次复杂性和有限的可扩展性使它们不适合长序列。传统上，视频超分辨率（VSR）方法依赖于复发体系结构来传播跨帧的功能。但是，这种方法遭受了众所周知的问题，包括消失的梯度，缺乏并行性和缓慢的推理速度。曼巴（Mamba）等选择性SSM的最新进展提供了一种引人注目的替代方案：通过启用具有线性时间复杂性的输入依赖性状态过渡，Mamba在保持强大的远程建模能力的同时减轻了这些问题。尽管有这种潜力，但仅Mamba由于其因果质性质和缺乏明确的上下文聚集而努力捕获细粒度的空间依赖性。为了解决这个问题，我们提出了一种混合体系结构，将窗口自我注意与空间上下文聚合与基于曼巴的选择性扫描结合在一起，以进行有效的时间传播。此外，我们引入了聚集碎片Mamba（GSM），这是一种对齐感知的机制，在Mamba繁殖之前向中心锚式扭曲的机制，并随后将它们散布回去，并有效地降低了闭合伪影并确保所有框架的聚合信息有效地分配了整个框架。提供官方实施，网址为：此HTTPS URL。</li>
</ul>

<h3>Title: Target Population Synthesis using CT-GAN</h3>
<ul>
<li><strong>Authors: </strong>Tanay Rastogi, Daniel Jonsson</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00871">https://arxiv.org/abs/2510.00871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00871">https://arxiv.org/pdf/2510.00871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00871]] Target Population Synthesis using CT-GAN(https://arxiv.org/abs/2510.00871)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Agent-based models used in scenario planning for transportation and urban planning usually require detailed population information from the base as well as target scenarios. These populations are usually provided by synthesizing fake agents through deterministic population synthesis methods. However, these deterministic population synthesis methods face several challenges, such as handling high-dimensional data, scalability, and zero-cell issues, particularly when generating populations for target scenarios. This research looks into how a deep generative model called Conditional Tabular Generative Adversarial Network (CT-GAN) can be used to create target populations either directly from a collection of marginal constraints or through a hybrid method that combines CT-GAN with Fitness-based Synthesis Combinatorial Optimization (FBS-CO). The research evaluates the proposed population synthesis models against travel survey and zonal-level aggregated population data. Results indicate that the stand-alone CT-GAN model performs the best when compared with FBS-CO and the hybrid model. CT-GAN by itself can create realistic-looking groups that match single-variable distributions, but it struggles to maintain relationships between multiple variables. However, the hybrid model demonstrates improved performance compared to FBS-CO by leveraging CT-GAN ability to generate a descriptive base population, which is then refined using FBS-CO to align with target-year marginals. This study demonstrates that CT-GAN represents an effective methodology for target populations and highlights how deep generative models can be successfully integrated with conventional synthesis techniques to enhance their performance.</li>
<li><strong>摘要：</strong>用于运输和城市规划的方案规划中使用的基于代理的模型通常需要从基地以及目标场景中进行详细的人群信息。这些人群通常是通过确定性种群综合方法合成假代理的。但是，这些确定性人群综合方法面临着几个挑战，例如处理高维数据，可伸缩性和零电池问题，尤其是在为目标情景产生人群时。这项研究探讨了如何使用称为条件表格生成对抗网络（CT-GAN）的深层生成模型，可以直接从边际约束的集合或通过将CT-GAN与基于适应性合成的组合组合优化（FBS-CO（FBS-CO）结合的混合方法直接创建目标种群。该研究评估了针对旅行调查和区域级汇总人群数据的拟议人群综合模型。结果表明，与FBS-CO和Hybrid模型相比，独立的CT-GAN模型表现最好。 ct-gan本身可以创建符合单变量分布的逼真的群体，但它努力维持多个变量之间的关系。然而，与FBS-CO相比，通过利用CT-GAN产生描述性基础种群的能力，混合模型表现出改善的性能，然后使用FBS-CO来完善与目标年度边缘相一致。这项研究表明，CT-GAN代表了目标人群的有效方法，并强调了如何成功地与常规合成技术集成以增强其性能。</li>
</ul>

<h3>Title: A Visual Diagnostics Framework for District Heating Data: Enhancing Data Quality for AI-Driven Heat Consumption Prediction</h3>
<ul>
<li><strong>Authors: </strong>Kristoffer Christensen, Bo Nørregaard Jørgensen, Zheng Grace Ma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00872">https://arxiv.org/abs/2510.00872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00872">https://arxiv.org/pdf/2510.00872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00872]] A Visual Diagnostics Framework for District Heating Data: Enhancing Data Quality for AI-Driven Heat Consumption Prediction(https://arxiv.org/abs/2510.00872)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>High-quality data is a prerequisite for training reliable Artificial Intelligence (AI) models in the energy domain. In district heating networks, sensor and metering data often suffer from noise, missing values, and temporal inconsistencies, which can significantly degrade model performance. This paper presents a systematic approach for evaluating and improving data quality using visual diagnostics, implemented through an interactive web-based dashboard. The dashboard employs Python-based visualization techniques, including time series plots, heatmaps, box plots, histograms, correlation matrices, and anomaly-sensitive KPIs such as skewness and anomaly detection based on the modified z-scores. These tools al-low human experts to inspect and interpret data anomalies, enabling a human-in-the-loop strategy for data quality assessment. The methodology is demonstrated on a real-world dataset from a Danish district heating provider, covering over four years of hourly data from nearly 7000 meters. The findings show how visual analytics can uncover systemic data issues and, in the future, guide data cleaning strategies that enhance the accuracy, stability, and generalizability of Long Short-Term Memory and Gated Recurrent Unit models for heat demand forecasting. The study contributes to a scalable, generalizable framework for visual data inspection and underlines the critical role of data quality in AI-driven energy management systems.</li>
<li><strong>摘要：</strong>高质量数据是能源领域中培训可靠人工智能（AI）模型的先决条件。在地区供暖网络中，传感器和计量数据通常会遇到噪音，缺失值和时间矛盾，这可能会大大降低模型性能。本文提出了一种系统的方法，用于使用基于交互式Web的仪表板实现的Visual Diagnostics评估和提高数据质量。仪表板采用基于Python的可视化技术，包括时间序列图，热图，盒子图，直方图，相关矩阵以及对基于修改的Z得分的偏度和异常检测的异常敏感KPI。这些工具al-low人类专家可以检查和解释数据异常，从而实现了人类的数据质量评估策略。该方法在来自丹麦地区供暖提供商的现实世界数据集上进行了证明，该数据集涵盖了近7000米的四年每小时数据。研究结果表明，视觉分析如何发现系统性数据问题，并在将来指导数据清洁策略，从而增强了长期短期内存和通路复发单位模型的准确性，稳定性和通用性，以预测热量需求。该研究为视觉数据检查的可扩展，可概括的框架做出了贡献，并强调了数据质量在AI驱动的能源管理系统中的关键作用。</li>
</ul>

<h3>Title: RiskPO: Risk-based Policy Optimization via Verifiable Reward for LLM Post-Training</h3>
<ul>
<li><strong>Authors: </strong>Tao Ren, Jinyang Jiang, Hui Yang, Wan Tian, Minhao Zou, Guanghao Li, Zishi Zhang, Qinghao Wang, Shentao Qin, Yanjun Zhao, Rui Tao, Hui Shao, Yijie Peng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00911">https://arxiv.org/abs/2510.00911</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00911">https://arxiv.org/pdf/2510.00911</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00911]] RiskPO: Risk-based Policy Optimization via Verifiable Reward for LLM Post-Training(https://arxiv.org/abs/2510.00911)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reinforcement learning with verifiable reward has recently emerged as a central paradigm for post-training large language models (LLMs); however, prevailing mean-based methods, such as Group Relative Policy Optimization (GRPO), suffer from entropy collapse and limited reasoning gains. We argue that these issues stem from overemphasizing high-probability output sequences while neglecting rare but informative reasoning paths. To address these challenges, we propose Risk-based Policy Optimization (RiskPO), which substitutes classical mean-based objectives with principled risk measures. Specifically, we introduce a Mixed Value-at-Risk objective that integrates weighted attention over multiple regions of the reward distribution, thereby amplifying gradient signals on challenging instances and preventing overconfident convergence. We further design a bundling scheme that aggregates multiple questions into bundles, thus enriching the feedback signal and yielding more stable and informative training dynamics. Theoretically, we prove that the risk-averse update alleviates entropy collapse and promotes exploration. Numerically, RiskPO achieves consistent and significant improvements in mathematical reasoning, multi-modal reasoning, and code generation benchmarks, surpassing GRPO and its variants on both Pass@1 and Pass@k metrics. Our results demonstrate that risk-based optimization provides a rigorous and effective paradigm for enhancing LLM reasoning capabilities.</li>
<li><strong>摘要：</strong>通过可验证的奖励的强化学习最近已成为培训后大语言模型（LLMS）的中心范式；但是，诸如小组相对政策优化（GRPO）之类的基于普通的基于均值的方法遭受熵崩溃和有限的推理收益。我们认为这些问题源于过度强调高概率的输出序列，同时忽略了罕见但有益的推理路径。为了应对这些挑战，我们提出了基于风险的政策优化（FISKPO），该挑战用原则性的风险措施代替了经典的基于平均值的目标。具体而言，我们引入了一个混合的价值风险目标，该目标在奖励分布的多个区域内整合了加权注意力，从而在挑战实例上放大了梯度信号并防止过度自信的融合。我们进一步设计了一个捆绑方案，该方案将多个问题汇总到捆绑中，从而丰富了反馈信号，并产生了更稳定和信息丰富的培训动态。从理论上讲，我们证明规避风险的更新减轻了熵的崩溃并促进了探索。从数值上讲，FiskPo在数学推理，多模式推理和代码生成基准测试方面取得了一致和显着改善，超过了GRPO及其在Pass@1和Pass@k Metrics上的变体。我们的结果表明，基于风险的优化为增强LLM推理功能提供了严格有效的范式。</li>
</ul>

<h3>Title: Looking Alike From Far to Near: Enhancing Cross-Resolution Re-Identification via Feature Vector Panning</h3>
<ul>
<li><strong>Authors: </strong>Zanwu Liu, Chao Yuan, Bo Li, Xiaowei Zhang, Guanglin Niu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00936">https://arxiv.org/abs/2510.00936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00936">https://arxiv.org/pdf/2510.00936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00936]] Looking Alike From Far to Near: Enhancing Cross-Resolution Re-Identification via Feature Vector Panning(https://arxiv.org/abs/2510.00936)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>In surveillance scenarios, varying camera distances cause significant differences among pedestrian image resolutions, making it hard to match low-resolution (LR) images with high-resolution (HR) counterparts, limiting the performance of Re-Identification (ReID) tasks. Most existing Cross-Resolution ReID (CR-ReID) methods rely on super-resolution (SR) or joint learning for feature compensation, which increases training and inference complexity and has reached a performance bottleneck in recent studies. Inspired by semantic directions in the word embedding space, we empirically discover that semantic directions implying resolution differences also emerge in the feature space of ReID, and we substantiate this finding from a statistical perspective using Canonical Correlation Analysis and Pearson Correlation Analysis. Based on this interesting finding, we propose a lightweight and effective Vector Panning Feature Alignment (VPFA) framework, which conducts CR-ReID from a novel perspective of modeling the resolution-specific feature discrepancy. Extensive experimental results on multiple CR-ReID benchmarks show that our method significantly outperforms previous state-of-the-art baseline models while obtaining higher efficiency, demonstrating the effectiveness and superiority of our model based on the new finding in this paper.</li>
<li><strong>摘要：</strong>在监视场景中，不同的摄像头距离在行人图像分辨率之间导致显着差异，因此很难与高分辨率（HR）对应物匹配低分辨率（LR）图像，从而限制了重新识别（REID）任务的性能。大多数现有的跨分辨率REID（CR-REID）方法依赖于超分辨率（SR）或联合学习来进行功能补偿，这增加了培训和推理的复杂性，并在最近的研究中达到了性能瓶颈。受嵌入空间词的语义方向的启发，我们从经验上发现，语义方向暗示分辨率差异在REID的特征空间中也出现了，我们从统计角度使用规范相关性分析和Pearson相关分析来证实这一发现。基于这一有趣的发现，我们提出了一个轻巧有效的矢量平移功能对齐（VPFA）框架，该框架从建模特定于分辨率特异性特征差异的新角度进行CR-REID。对多个CR-REID基准测试的广泛实验结果表明，我们的方法在获得更高效率的同时显着优于先前的最先进的基线模型，这表明了基于本文的新发现，我们的模型的有效性和优越性。</li>
</ul>

<h3>Title: InfVSR: Breaking Length Limits of Generic Video Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Ziqing Zhang, Kai Liu, Zheng Chen, Xi Li, Yucong Chen, Bingnan Duan, Linghe Kong, Yulun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00948">https://arxiv.org/abs/2510.00948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00948">https://arxiv.org/pdf/2510.00948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00948]] InfVSR: Breaking Length Limits of Generic Video Super-Resolution(https://arxiv.org/abs/2510.00948)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Real-world videos often extend over thousands of frames. Existing video super-resolution (VSR) approaches, however, face two persistent challenges when processing long sequences: (1) inefficiency due to the heavy cost of multi-step denoising for full-length sequences; and (2) poor scalability hindered by temporal decomposition that causes artifacts and discontinuities. To break these limits, we propose InfVSR, which novelly reformulates VSR as an autoregressive-one-step-diffusion paradigm. This enables streaming inference while fully leveraging pre-trained video diffusion priors. First, we adapt the pre-trained DiT into a causal structure, maintaining both local and global coherence via rolling KV-cache and joint visual guidance. Second, we distill the diffusion process into a single step efficiently, with patch-wise pixel supervision and cross-chunk distribution matching. Together, these designs enable efficient and scalable VSR for unbounded-length videos. To fill the gap in long-form video evaluation, we build a new benchmark tailored for extended sequences and further introduce semantic-level metrics to comprehensively assess temporal consistency. Our method pushes the frontier of long-form VSR, achieves state-of-the-art quality with enhanced semantic consistency, and delivers up to 58x speed-up over existing methods such as MGLD-VSR. Code will be available at this https URL.</li>
<li><strong>摘要：</strong>现实世界中的视频通常会延长数千个帧。但是，现有的视频超分辨率（VSR）方法在处理长序列时面临两个持续的挑战：（1）由于全长序列的多步降级成本繁重，效率低下； （2）可伸缩性差的可伸缩性受到导致伪影和不连续性的时间分解的阻碍。为了打破这些限制，我们提出了INFVSR，该INFVSR将VSR重新定义为自回归一步扩散范式。这可以使流媒体推断在完全利用预先训练的视频扩散先验的同时。首先，我们将预先训练的DIT调整为因果结构，通过滚动KV-CACHE和联合视觉指导保持局部和全球连贯性。其次，我们将扩散过程有效地提炼成单个步骤，并通过贴片的像素监督和跨块分布匹配。这些设计共同为无界长度视频提供了高效且可扩展的VSR。为了填补长期视频评估中的空白，我们构建了一个针对扩展序列的新基准测试，并进一步介绍了语义级别的指标，以全面评估时间一致性。我们的方法推动了长形VSR的前沿，具有提高语义一致性的最先进质量，并且比现有方法（例如MGLD-VSR）提供了高达58倍的速度。代码将在此HTTPS URL上可用。</li>
</ul>

<h3>Title: JEPA-T: Joint-Embedding Predictive Architecture with Text Fusion for Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Siheng Wan, Zhengtao Yao, Zhengdao Li, Junhao Dong, Yanshu Li, Yikai Li, Linshan Li, Haoyan Xu, Yijiang Li, Zhikang Dong, Huacan Wang, Jifeng Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00974">https://arxiv.org/abs/2510.00974</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00974">https://arxiv.org/pdf/2510.00974</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00974]] JEPA-T: Joint-Embedding Predictive Architecture with Text Fusion for Image Generation(https://arxiv.org/abs/2510.00974)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Modern Text-to-Image (T2I) generation increasingly relies on token-centric architectures that are trained with self-supervision, yet effectively fusing text with visual tokens remains a challenge. We propose \textbf{JEPA-T}, a unified multimodal framework that encodes images and captions into discrete visual and textual tokens, processed by a joint-embedding predictive Transformer. To enhance fusion, we incorporate cross-attention after the feature predictor for conditional denoising while maintaining a task-agnostic backbone. Additionally, raw texts embeddings are injected prior to the flow matching loss to improve alignment during training. During inference, the same network performs both class-conditional and free-text image generation by iteratively denoising visual tokens conditioned on text. Evaluations on ImageNet-1K demonstrate that JEPA-T achieves strong data efficiency, open-vocabulary generalization, and consistently outperforms non-fusion and late-fusion baselines. Our approach shows that late architectural fusion combined with objective-level alignment offers an effective balance between conditioning strength and backbone generality in token-based this http URL code is now available: this https URL</li>
<li><strong>摘要：</strong>现代文本对图像（T2I）的一代越来越依赖于以代币为中心的体系结构，这些体系结构接受了自学的训练，但有效地将文本与视觉令牌融合在一起仍然是一个挑战。我们提出了\ textbf {JEPA-T}，这是一个统一的多模式框架，将图像和字幕编码为离散的视觉和文本令牌，该框架由联合提出的预测变压器处理。为了增强融合，我们在特征预测器之后进行了跨注意，以进行有条件的denoing，同时保持任务不合时宜的主链。此外，在流量匹配损失之前注入了原始文本嵌入，以改善训练过程中的对齐。在推断期间，同一网络通过迭代地剥夺了以文本为条件的视觉令牌来执行类条件和自由文本图像的生成。对Imagenet-1K的评估表明，JEPA-T可以实现强大的数据效率，开放式允许概括，并且始终优于非融合和后期融合的基线。我们的方法表明，晚期的建筑融合与目标级别的对齐结合，在基于令牌的基于标记的此HTTP URL代码的条件强度和骨干通用性之间提供了有效的平衡：此HTTPS URL</li>
</ul>

<h3>Title: Riemannian Consistency Model</h3>
<ul>
<li><strong>Authors: </strong>Chaoran Cheng, Yusong Wang, Yuxin Chen, Xiangxin Zhou, Nanning Zheng, Ge Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00983">https://arxiv.org/abs/2510.00983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00983">https://arxiv.org/pdf/2510.00983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00983]] Riemannian Consistency Model(https://arxiv.org/abs/2510.00983)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Consistency models are a class of generative models that enable few-step generation for diffusion and flow matching models. While consistency models have achieved promising results on Euclidean domains like images, their applications to Riemannian manifolds remain challenging due to the curved geometry. In this work, we propose the Riemannian Consistency Model (RCM), which, for the first time, enables few-step consistency modeling while respecting the intrinsic manifold constraint imposed by the Riemannian geometry. Leveraging the covariant derivative and exponential-map-based parameterization, we derive the closed-form solutions for both discrete- and continuous-time training objectives for RCM. We then demonstrate theoretical equivalence between the two variants of RCM: Riemannian consistency distillation (RCD) that relies on a teacher model to approximate the marginal vector field, and Riemannian consistency training (RCT) that utilizes the conditional vector field for training. We further propose a simplified training objective that eliminates the need for the complicated differential calculation. Finally, we provide a unique kinematics perspective for interpreting the RCM objective, offering new theoretical angles. Through extensive experiments, we manifest the superior generative quality of RCM in few-step generation on various non-Euclidean manifolds, including flat-tori, spheres, and the 3D rotation group SO(3).</li>
<li><strong>摘要：</strong>一致性模型是一类生成模型，可为扩散和流匹配模型提供几步生成。尽管一致性模型在图像之类的欧几里得领域取得了有希望的结果，但由于弯曲的几何形状，它们在Riemannian歧管上的应用仍然具有挑战性。在这项工作中，我们提出了Riemannian一致性模型（RCM），该模型首次实现了很少的一致性建模，同时尊重Riemannian几何形状所施加的固有歧管约束。利用基于协方差的衍生衍生物和基于指数映射的参数化，我们为RCM的离散时间和连续时间训练目标提供了封闭形式的解决方案。然后，我们证明了RCM的两个变体之间的理论等效性：Riemannian一致性蒸馏（RCD），该变体依靠教师模型近似边缘矢量场，而Riemannian的一致性训练（RCT）利用条件矢量进行训练。我们进一步提出了一个简化的训练目标，以消除对复杂差异计算的需求。最后，我们提供了一种独特的运动学观点，用于解释RCM目标，并提供新的理论角度。通过广泛的实验，我们在几个步骤生成中表现出了RCM的出色生成质量，包括各种非欧盟歧管，包括平坦的甲虫，球体和3D旋转组SO（3）。</li>
</ul>

<h3>Title: Visual Self-Refinement for Autoregressive Models</h3>
<ul>
<li><strong>Authors: </strong>Jiamian Wang, Ziqi Zhou, Chaithanya Kumar Mummadi, Sohail Dianat, Majid Rabbani, Raghuveer Rao, Chen Qiu, Zhiqiang Tao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00993">https://arxiv.org/abs/2510.00993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00993">https://arxiv.org/pdf/2510.00993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00993]] Visual Self-Refinement for Autoregressive Models(https://arxiv.org/abs/2510.00993)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Autoregressive models excel in sequential modeling and have proven to be effective for vision-language data. However, the spatial nature of visual signals conflicts with the sequential dependencies of next-token prediction, leading to suboptimal results. This work proposes a plug-and-play refinement module to enhance the complex spatial correspondence modeling within the generated visual sequence. This module operates as a post-pretraining step to jointly refine all generated tokens of autoregressive model, enhancing vision-language modeling under a shared sequential prediction framework. By leveraging global context and relationship across the tokens, our method mitigates the error accumulation issue within the sequential generation. Experiments demonstrate that the proposed method improves the generation quality, enhancing the model's ability to produce semantically consistent results.</li>
<li><strong>摘要：</strong>自回归模型在顺序建模中表现出色，并且已被证明对视觉数据有效。然而，视觉信号的空间性质与下一句话预测的顺序依赖性冲突，从而导致次优结果。这项工作提出了一个插件的完善模块，以增强生成的视觉序列中的复杂空间对应建模。该模块作为后期的步骤运行，共同完善所有产生的自回归模型的令牌，从而在共享的顺序预测框架下增强视觉模型。通过利用整个代币的全局上下文和关系，我们的方法减轻了顺序生成中的错误积累问题。实验表明，所提出的方法提高了生成质量，增强了模型产生语义一致的结果的能力。</li>
</ul>

<h3>Title: SoftCFG: Uncertainty-guided Stable Guidance for Visual autoregressive Model</h3>
<ul>
<li><strong>Authors: </strong>Dongli Xu, Aleksei Tiulpin, Matthew B. Blaschko</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.00996">https://arxiv.org/abs/2510.00996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.00996">https://arxiv.org/pdf/2510.00996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.00996]] SoftCFG: Uncertainty-guided Stable Guidance for Visual autoregressive Model(https://arxiv.org/abs/2510.00996)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Autoregressive (AR) models have emerged as powerful tools for image generation by modeling images as sequences of discrete tokens. While Classifier-Free Guidance (CFG) has been adopted to improve conditional generation, its application in AR models faces two key issues: guidance diminishing, where the conditional-unconditional gap quickly vanishes as decoding progresses, and over-guidance, where strong conditions distort visual coherence. To address these challenges, we propose SoftCFG, an uncertainty-guided inference method that distributes adaptive perturbations across all tokens in the sequence. The key idea behind SoftCFG is to let each generated token contribute certainty-weighted guidance, ensuring that the signal persists across steps while resolving conflicts between text guidance and visual context. To further stabilize long-sequence generation, we introduce Step Normalization, which bounds cumulative perturbations of SoftCFG. Our method is training-free, model-agnostic, and seamlessly integrates with existing AR pipelines. Experiments show that SoftCFG significantly improves image quality over standard CFG and achieves state-of-the-art FID on ImageNet 256 among autoregressive models.</li>
<li><strong>摘要：</strong>自回归（AR）模型已通过将图像作为离散令牌的序列建模来成为图像生成的强大工具。尽管已采用了无分类器指导（CFG）来改善条件产生，但其在AR模型中的应用面临两个关键问题：指导减少，条件间隔的差距随着解码的进展而迅速消失，并且过度指导，在这种情况下，强烈的条件扭曲了视觉连贯性。为了应对这些挑战，我们提出了SoftCFG，这是一种不确定性引导的推理方法，该方法在序列中分布了所有令牌的适应性扰动。 SOFTCFG背后的关键思想是让每个生成的令牌都贡献确定性加权指导，以确保信号在跨步骤中持续存在，同时解决文本指导和视觉上下文之间的冲突。为了进一步稳定长期生成，我们引入了步骤归一化，从而界定了软核心的累积扰动。我们的方法是无训练的，模型不可替代的，并且与现有的AR管道无缝集成。实验表明，软核心可显着提高标准CFG的图像质量，并在自回归模型中实现Imagenet 256的最先进的FID。</li>
</ul>

<h3>Title: ImageDoctor: Diagnosing Text-to-Image Generation via Grounded Image Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yuxiang Guo, Jiang Liu, Ze Wang, Hao Chen, Ximeng Sun, Yang Zhao, Jialian Wu, Xiaodong Yu, Zicheng Liu, Emad Barsoum</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01010">https://arxiv.org/abs/2510.01010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01010">https://arxiv.org/pdf/2510.01010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01010]] ImageDoctor: Diagnosing Text-to-Image Generation via Grounded Image Reasoning(https://arxiv.org/abs/2510.01010)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The rapid advancement of text-to-image (T2I) models has increased the need for reliable human preference modeling, a demand further amplified by recent progress in reinforcement learning for preference alignment. However, existing approaches typically quantify the quality of a generated image using a single scalar, limiting their ability to provide comprehensive and interpretable feedback on image quality. To address this, we introduce ImageDoctor, a unified multi-aspect T2I model evaluation framework that assesses image quality across four complementary dimensions: plausibility, semantic alignment, aesthetics, and overall quality. ImageDoctor also provides pixel-level flaw indicators in the form of heatmaps, which highlight misaligned or implausible regions, and can be used as a dense reward for T2I model preference alignment. Inspired by the diagnostic process, we improve the detail sensitivity and reasoning capability of ImageDoctor by introducing a "look-think-predict" paradigm, where the model first localizes potential flaws, then generates reasoning, and finally concludes the evaluation with quantitative scores. Built on top of a vision-language model and trained through a combination of supervised fine-tuning and reinforcement learning, ImageDoctor demonstrates strong alignment with human preference across multiple datasets, establishing its effectiveness as an evaluation metric. Furthermore, when used as a reward model for preference tuning, ImageDoctor significantly improves generation quality -- achieving an improvement of 10% over scalar-based reward models.</li>
<li><strong>摘要：</strong>文本对图像（T2I）模型的快速发展增加了对可靠的人类偏好模型的需求，这一需求进一步扩大了增强学习对偏好一致性的进展。但是，现有方法通常使用单个标量量化生成图像的质量，从而限制了它们提供有关图像质量的全面和可解释的反馈的能力。为了解决这个问题，我们介绍了Imagedoctor，这是一个统一的多种多样的T2I模型评估框架，该框架评估了四个互补维度的图像质量：合理性，语义一致性，美学和整体质量。 Imagedoctor还以热图的形式提供像素级缺陷指示器，该指标突出显示了未对准或难以置信的区域，并可以用作对T2I模型偏好比对的密集奖励。受到诊断过程的启发，我们通过引入“观察思维预测”范式来提高iMagedoctor的细节灵敏度和推理能力，该模型首先将潜在的缺陷定位，然后产生推理，并最终以定量分数结束评估。 Imagedoctor建立在视觉模型的基础上，并通过受监督的微调和强化学习的结合进行训练，这表明了在多个数据集中与人类偏好的紧密相结合，确立了其作为评估指标的有效性。此外，当用作偏好调整的奖励模型时，Imagedoctor显着提高了发电质量 - 比基于标量的奖励模型提高了10％。</li>
</ul>

<h3>Title: Secure and reversible face anonymization with diffusion models</h3>
<ul>
<li><strong>Authors: </strong>Pol Labarbarie, Vincent Itier, William Puech</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01031">https://arxiv.org/abs/2510.01031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01031">https://arxiv.org/pdf/2510.01031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01031]] Secure and reversible face anonymization with diffusion models(https://arxiv.org/abs/2510.01031)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Face images processed by computer vision algorithms contain sensitive personal information that malicious actors can capture without consent. These privacy and security risks highlight the need for effective face anonymization methods. Current methods struggle to propose a good trade-off between a secure scheme with high-quality image generation and reversibility for later person authentication. Diffusion-based approaches produce high-quality anonymized images but lack the secret key mechanism to ensure that only authorized parties can reverse the process. In this paper, we introduce, to our knowledge, the first secure, high-quality reversible anonymization method based on a diffusion model. We propose to combine the secret key with the latent faces representation of the diffusion model. To preserve identity-irrelevant features, generation is constrained by a facial mask, maintaining high-quality images. By using a deterministic forward and backward diffusion process, our approach enforces that the original face can be recovered with the correct secret key. We also show that the proposed method produces anonymized faces that are less visually similar to the original faces, compared to other previous work.</li>
<li><strong>摘要：</strong>计算机视觉算法处理的面部图像包含敏感的个人信息，恶意演员可以在未经同意的情况下捕获。这些隐私和安全风险突出了有效面部匿名方法的需求。当前的方法努力提出具有高质量图像生成的安全计划与后来人身份验证的可逆性之间的良好权衡。基于扩散的方法产生高质量的匿名图像，但缺乏确保只有授权方可以扭转过程的秘密关键机制。在本文中，我们介绍了我们的知识，这是基于扩散模型的第一个安全，高质量的可逆匿名方法。我们建议将秘密钥匙与扩散模型的潜在面部表示相结合。为了保留身份 - 呈卵头特征，生成受到面膜的约束，并保持高质量的图像。通过使用确定性的向前和向后扩散过程，我们的方法可以使用正确的秘密键来恢复原始面。我们还表明，与其他以前的工作相比，所提出的方法产生的匿名面孔在视觉上与原始面孔相似。</li>
</ul>

<h3>Title: Authentic Discrete Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Xiao Li, Jiaqi Zhang, Shuxiang Zhang, Tianshui Chen, Liang Lin, Guangrun Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01047">https://arxiv.org/abs/2510.01047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01047">https://arxiv.org/pdf/2510.01047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01047]] Authentic Discrete Diffusion Model(https://arxiv.org/abs/2510.01047)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We propose an Authentic Discrete Diffusion (ADD) framework that fundamentally redefines prior pseudo-discrete approaches by preserving core diffusion characteristics directly in the one-hot space through a suite of coordinated mechanisms. Unlike conventional "pseudo" discrete diffusion (PDD) methods, ADD reformulates the diffusion input by directly using float-encoded one-hot class data, without relying on diffusing in the continuous latent spaces or masking policies. At its core, a timestep-conditioned cross-entropy loss is introduced between the diffusion model's outputs and the original one-hot labels. This synergistic design establishes a bridge between discriminative and generative learning. Our experiments demonstrate that ADD not only achieves superior performance on classification tasks compared to the baseline, but also exhibits excellent text generation capabilities on Image captioning. Extensive ablations validate the measurable gains of each component.</li>
<li><strong>摘要：</strong>我们提出了一个真实的离散扩散（ADD）框架，该框架从根本上从根本上重新定义了先前的伪折 - 污点方法，该方法通过通过一套协调的机制直接保留核心扩散特性，直接在单热空间中保存核心扩散特性。与常规的“伪”离散扩散（PDD）方法不同，添加可以通过直接使用浮点编码的一式式类别数据来重新构建扩散输入，而无需依赖于连续的潜在空间或掩盖策略的扩散。从本质上讲，在扩散模型的输出和原始的单热标签之间引入了时间步条件的横向渗透损失。这种协同设计在歧视性学习和生成性学习之间建立了桥梁。我们的实验表明，与基线相比，不仅可以在分类任务上实现出色的性能，而且在图像字幕上表现出出色的文本生成功能。广泛的消融验证了每个组件的可衡量收益。</li>
</ul>

<h3>Title: KeySG: Hierarchical Keyframe-Based 3D Scene Graphs</h3>
<ul>
<li><strong>Authors: </strong>Abdelrhman Werby, Dennis Rotondi, Fabio Scaparro, Kai O. Arras</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01049">https://arxiv.org/abs/2510.01049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01049">https://arxiv.org/pdf/2510.01049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01049]] KeySG: Hierarchical Keyframe-Based 3D Scene Graphs(https://arxiv.org/abs/2510.01049)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In recent years, 3D scene graphs have emerged as a powerful world representation, offering both geometric accuracy and semantic richness. Combining 3D scene graphs with large language models enables robots to reason, plan, and navigate in complex human-centered environments. However, current approaches for constructing 3D scene graphs are semantically limited to a predefined set of relationships, and their serialization in large environments can easily exceed an LLM's context window. We introduce KeySG, a framework that represents 3D scenes as a hierarchical graph consisting of floors, rooms, objects, and functional elements, where nodes are augmented with multi-modal information extracted from keyframes selected to optimize geometric and visual coverage. The keyframes allow us to efficiently leverage VLM to extract scene information, alleviating the need to explicitly model relationship edges between objects, enabling more general, task-agnostic reasoning and planning. Our approach can process complex and ambiguous queries while mitigating the scalability issues associated with large scene graphs by utilizing a hierarchical retrieval-augmented generation (RAG) pipeline to extract relevant context from the graph. Evaluated across four distinct benchmarks --including 3D object segmentation and complex query retrieval-- KeySG outperforms prior approaches on most metrics, demonstrating its superior semantic richness and efficiency.</li>
<li><strong>摘要：</strong>近年来，3D场景图成为了强大的世界代表，提供了几何准确性和语义丰富。将3D场景图与大语言模型相结合，使机器人能够在复杂的以人为中心的环境中进行推理，计划和导航。但是，当前构建3D场景图的方法在语义上仅限于预定义的关系集，并且它们在大环境中的序列化很容易超过LLM的上下文窗口。我们介绍了Keysg，该框架将3D场景表示为层次图，由地板，房间，对象和功能元素组成，其中节点被选择从选择从Keyframes提取的多模式信息增强，以优化几何和视觉覆盖。这些关键框架使我们能够有效利用VLM提取场景信息，减轻对象之间明确建模关系边缘的需求，从而实现更通用的，任务不可能的推理和计划。我们的方法可以通过利用层次检索 - 演示生成（RAG）管道来从图表中提取相关上下文，从而可以处理复杂和模棱两可的查询，同时减轻与大型场景图相关的可伸缩性问题。通过四个不同的基准进行评估 - 包括3D对象分割和复杂的查询检索 -  Keysg在大多数指标上的先验方法都优于先前的方法，这表明了其出色的语义丰富性和效率。</li>
</ul>

<h3>Title: Augmenting LLMs for General Time Series Understanding and Prediction</h3>
<ul>
<li><strong>Authors: </strong>Felix Parker, Nimeesha Chan, Chi Zhang, Kimia Ghobadi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01111">https://arxiv.org/abs/2510.01111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01111">https://arxiv.org/pdf/2510.01111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01111]] Augmenting LLMs for General Time Series Understanding and Prediction(https://arxiv.org/abs/2510.01111)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Time series data is fundamental to decision-making in many crucial domains including healthcare, finance, and environmental science. However, analyzing this data often requires incorporating unstructured contextual information, answering domain-specific questions, and generating natural language explanations -- capabilities that traditional time series models lack due to their inability to process text. While Large Language Models (LLMs) excel at contextual reasoning and knowledge integration, they struggle with numerical time series due to inefficient text-based representations and limited exposure to temporal data during pretraining. We address this gap by augmenting an LLM with specialized time series perception through a patch-based encoder-decoder architecture. We train this Time Series-augmented LLM (TsLLM) on a large corpus of over 2 million interleaved time series and text examples spanning diverse analysis tasks: forecasting with contextual information, time series question-answering, pattern explanation, classification with natural language outputs, and report generation. This training enables TsLLM to leverage both its language understanding and newly acquired temporal reasoning capabilities. While not designed to surpass specialized models on traditional benchmarks, TsLLM demonstrates strong performance on tasks requiring the integration of time series analysis with natural language -- capabilities that existing approaches cannot provide. Our work establishes a new paradigm for time series analysis that bridges numerical computation and natural language understanding, democratizing access to sophisticated temporal reasoning through natural language interaction.</li>
<li><strong>摘要：</strong>时间序列数据对于许多关键领域的决策至关重要，包括医疗保健，金融和环境科学。但是，分析这些数据通常需要合并非结构化的上下文信息，回答特定于领域的问题以及生成自然语言解释 - 传统时间序列模型由于无法处理文本而缺乏的功能。尽管大型语言模型（LLMS）在上下文推理和知识集成下脱颖而出，但由于基于文本的表示效率低下，并且在预处理过程中对时间数据的暴露量有限，因此它们在数值时间序列上挣扎。我们通过通过基于补丁的编码器架构来增强具有专门时间序列感知的LLM来解决这一差距。我们训练这个时间序列的LLM（TSLLM），这些LLM（TSLLM）在超过200万个交织的时间序列和文本示例中，涵盖了各种分析任务：预测有上下文信息，时间序列的提问，模式解释，与自然语言输出进行分类，并产生报告。该培训使TSLLM能够利用其语言理解和新获得的时间推理能力。 TSLLM并非旨在超越传统基准测试的专业模型，但在需要将时间序列分析与自然语言集成的任务上表现出强大的性能 - 现有方法无法提供的功能。我们的工作为时间序列分析建立了一个新的范式，该范式桥接了数字计算和自然语言理解，并通过自然语言互动使人们对复杂的时间推理进行民主化。</li>
</ul>

<h3>Title: Prompt Curriculum Learning for Efficient LLM Post-Training</h3>
<ul>
<li><strong>Authors: </strong>Zhaolin Gao, Joongwon Kim, Wen Sun, Thorsten Joachims, Sid Wang, Richard Yuanzhe Pang, Liang Tan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01135">https://arxiv.org/abs/2510.01135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01135">https://arxiv.org/pdf/2510.01135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01135]] Prompt Curriculum Learning for Efficient LLM Post-Training(https://arxiv.org/abs/2510.01135)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce Prompt Curriculum Learning (PCL), a lightweight reinforcement learning (RL) algorithm that selects intermediate-difficulty prompts using a learned value model to post-train language models. Since post-training LLMs via RL remains sensitive to batching and prompt selection strategies, we first conduct a series of systematic experiments where we (1) determine the optimal training batch size that balances generation efficiency and gradient quality and (2) establish the importance of focusing on prompts of intermediate difficulty for the policy. We build upon these results to design PCL, which identifies prompts of intermediate difficulty for the current policy in an on-policy manner by using a value model that is concurrently updated based on the current policy. By focusing on informative prompts that yield high effective ratios, PCL achieves either the highest performance or requires significantly less time to reach comparable performance to its counterparts. Compared to rollout-based filtering methods, PCL avoids costly rollouts and achieves $12.1\times$ and $16.9\times$ faster speed on identifying intermediate-difficulty prompts when training on MATH and DeepScaleR, respectively. We further demonstrate that our value model accurately predicts prompt difficulty and allows PCL to focus on progressively more challenging prompts during RL. Our results present a new methodology that delivers improved tradeoff between upper-bound performance and efficiency for reasoning-focused RL.</li>
<li><strong>摘要：</strong>我们介绍了提示课程学习（PCL），这是一种轻巧的增强学习（RL）算法，该算法使用学习的价值模型选择中间难题的提示来培训后语言模型。由于通过RL进行的培训后LLM仍然对分批和及时选择策略保持敏感，因此我们首先进行了一系列系统的实验，我们（1）我们（1）确定最佳的培训批量大小，以平衡发电效率和梯度质量以及（2）确定将重点放在中间难度的提示上的重要性。我们基于这些结果来设计PCL，该结果通过使用基于当前策略并同时更新的价值模型来确定当前策略的中间困难提示。通过关注提供高效比率的信息提示，PCL可以达到最高的性能或需要更少的时间才能达到与同行相当的表现。与基于推出的过滤方法相比，PCL避免了昂贵的推出，并获得$ 12.1 \ times $和$ 16.9 \ times $ $ $ $ $ $ $ $ $ $ $ $ $在识别数学和DeepScaler培训时识别中间难题提示的速度更快。我们进一步证明，我们的价值模型可以准确预测迅速的难度，并允许PCL专注于RL期间逐渐具有挑战性的提示。我们的结果提出了一种新的方法，该方法可以改善以推理为重点的RL的上限性能和效率之间的折衷。</li>
</ul>

<h3>Title: Prosperity before Collapse: How Far Can Off-Policy RL Reach with Stale Data on LLMs?</h3>
<ul>
<li><strong>Authors: </strong>Haizhong Zheng, Jiawei Zhao, Bedi Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01161">https://arxiv.org/abs/2510.01161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01161">https://arxiv.org/pdf/2510.01161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01161]] Prosperity before Collapse: How Far Can Off-Policy RL Reach with Stale Data on LLMs?(https://arxiv.org/abs/2510.01161)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reinforcement learning has been central to recent advances in large language model reasoning, but most algorithms rely on on-policy training that demands fresh rollouts at every update, limiting efficiency and scalability. Asynchronous RL systems alleviate this by decoupling rollout generation from training, yet their effectiveness hinges on tolerating large staleness in rollout data, a setting where existing methods either degrade in performance or collapse. We revisit this challenge and uncover a prosperity-before-collapse phenomenon: stale data can be as informative as on-policy data if exploited properly. Building on this insight, we introduce M2PO (Second-Moment Trust Policy Optimization), which constrains the second moment of importance weights to suppress only extreme outliers while preserving informative updates. Notably, M2PO sharply reduces the fraction of clipped tokens under high staleness (from 1.22% to 0.06% over training), precisely masking high-variance tokens while maintaining stable optimization. Extensive evaluation across six models (from 1.7B to 32B) and eight benchmarks shows that M2PO delivers stable off-policy training even with data stale by at least 256 model updates and matches on-policy performance.</li>
<li><strong>摘要：</strong>强化学习对于大型语言模型推理的最新进展至关重要，但是大多数算法都依赖于在每次更新时需要新鲜推出的政策培训，从而限制了效率和可扩展性。异步RL系统通过将推出的发行从培训中解除，但它们的有效性取决于耐受推广数据中的大稳定度，这种设置在这种情况下，现有方法要么降低了性能或崩溃。我们重新审查了这一挑战，并发现了繁荣之前的现象：如果得到适当的利用，陈旧的数据可能与政策数据一样丰富。在此洞察力的基础上，我们介绍了M2PO（第二阶段的信任政策优化），这限制了重要权重的第二时，以仅抑制极端异常值，同时保留信息丰富的更新。值得注意的是，M2PO急剧减少了高稳定度下的剪裁代币的比例（从训练中从1.22％到0.06％），精确地掩盖了高变化令牌，同时保持稳定的优化。六个模型（从1.7B到32B）和八个基准测试的广泛评估表明，即使在数据陈旧的情况下，M2PO至少通过256个模型更新和匹配匹配的派利性能也提供了稳定的货球培训。</li>
</ul>

<h3>Title: Fiaingen: A financial time series generative method matching real-world data quality</h3>
<ul>
<li><strong>Authors: </strong>Jože M. Rožanec, Tina Žezlin, Laurentiu Vasiliu, Dunja Mladenić, Radu Prodan, Dumitru Roman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01169">https://arxiv.org/abs/2510.01169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01169">https://arxiv.org/pdf/2510.01169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01169]] Fiaingen: A financial time series generative method matching real-world data quality(https://arxiv.org/abs/2510.01169)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Data is vital in enabling machine learning models to advance research and practical applications in finance, where accurate and robust models are essential for investment and trading decision-making. However, real-world data is limited despite its quantity, quality, and variety. The data shortage of various financial assets directly hinders the performance of machine learning models designed to trade and invest in these assets. Generative methods can mitigate this shortage. In this paper, we introduce a set of novel techniques for time series data generation (we name them Fiaingen) and assess their performance across three criteria: (a) overlap of real-world and synthetic data on a reduced dimensionality space, (b) performance on downstream machine learning tasks, and (c) runtime performance. Our experiments demonstrate that the methods achieve state-of-the-art performance across the three criteria listed above. Synthetic data generated with Fiaingen methods more closely mirrors the original time series data while keeping data generation time close to seconds - ensuring the scalability of the proposed approach. Furthermore, models trained on it achieve performance close to those trained with real-world data.</li>
<li><strong>摘要：</strong>数据对于使机器学习模型能够推进金融中的研究和实际应用至关重要，在该模型中，准确且健壮的模型对于投资和交易决策至关重要。但是，尽管其数量，质量和多样性，但现实世界中的数据仍有限。各种金融资产的数据短缺直接阻碍了旨在交易和投资这些资产的机器学习模型的性能。生成方法可以减轻这种短缺。在本文中，我们介绍了一套为时间序列数据生成的新技术（我们将其命名为fiaingen），并在三个标准中评估它们的性能：（a）关于降低维度空间的现实世界和合成数据的重叠，（（b）下游机器学习任务的性能，以及（c）运行时性能。我们的实验表明，这些方法在上面列出的三个标准中实现了最先进的性能。使用Fiaingen方法生成的合成数据更紧密地反映了原始时间序列数据，同时使数据生成时间接近秒，从而确保了所提出的方法的可扩展性。此外，受过训练的模型可以实现与经过现实世界数据训练的模型。</li>
</ul>

<h3>Title: Code2Video: A Code-centric Paradigm for Educational Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Yanzhe Chen, Kevin Qinghong Lin, Mike Zheng Shou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.HC, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01174">https://arxiv.org/abs/2510.01174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01174">https://arxiv.org/pdf/2510.01174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01174]] Code2Video: A Code-centric Paradigm for Educational Video Generation(https://arxiv.org/abs/2510.01174)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>While recent generative models advance pixel-space video synthesis, they remain limited in producing professional educational videos, which demand disciplinary knowledge, precise visual structures, and coherent transitions, limiting their applicability in educational scenarios. Intuitively, such requirements are better addressed through the manipulation of a renderable environment, which can be explicitly controlled via logical commands (e.g., code). In this work, we propose Code2Video, a code-centric agent framework for generating educational videos via executable Python code. The framework comprises three collaborative agents: (i) Planner, which structures lecture content into temporally coherent flows and prepares corresponding visual assets; (ii) Coder, which converts structured instructions into executable Python codes while incorporating scope-guided auto-fix to enhance efficiency; and (iii) Critic, which leverages vision-language models (VLM) with visual anchor prompts to refine spatial layout and ensure clarity. To support systematic evaluation, we build MMMC, a benchmark of professionally produced, discipline-specific educational videos. We evaluate MMMC across diverse dimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and particularly, TeachQuiz, a novel end-to-end metric that quantifies how well a VLM, after unlearning, can recover knowledge by watching the generated videos. Our results demonstrate the potential of Code2Video as a scalable, interpretable, and controllable approach, achieving 40% improvement over direct code generation and producing videos comparable to human-crafted tutorials. The code and datasets are available at this https URL.</li>
<li><strong>摘要：</strong>尽管最近的生成模型可以推进像素空间视频的综合，但它们仍在制作专业的教育视频方面有限，该视频需要学科知识，精确的视觉结构和连贯的过渡，从而限制了其在教育场景中的适用性。直观地，通过操纵可渲染的环境可以更好地解决此类要求，该环境可以通过逻辑命令（例如代码）明确控制。在这项工作中，我们提出了Code2Video，这是一个以代码为中心的代理框架，用于通过可执行的Python代码生成教育视频。该框架包括三个协作代理：（i）计划者，该计划将授课内容结构为时间连贯的流，并准备相应的视觉资产； （ii）编码器，该编码器将结构化指令转换为可执行的Python代码，同时结合了范围引导的自动固定以提高效率； （iii）评论家，它利用视觉锚点来利用视觉模型（VLM）提示提示空间布局并确保清晰度。为了支持系统的评估，我们构建了MMMC，这是专业生产的特定学科教育视频的基准。我们评估了MMMC跨不同维度的MMMC，包括VLM-AS-A-A-A-As-A-Gudge美学评分，代码效率，尤其是Teachquiz，这是一种新颖的端到端度量，可以量化VLM在未经学习后的能力，可以通过观看生成的视频来恢复知识。我们的结果表明，Code2Video是一种可扩展，可解释和可控制的方法，比直接代码生成的40％改进并制作了与人工制作的教程相当的视频。代码和数据集可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Wang, Luoxin Ye, TaiMing Lu, Junfei Xiao, Jiahan Zhang, Yuxiang Guo, Xijun Liu, Rama Chellappa, Cheng Peng, Alan Yuille, Jieneng Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01183">https://arxiv.org/abs/2510.01183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01183">https://arxiv.org/pdf/2510.01183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01183]] EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory(https://arxiv.org/abs/2510.01183)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Humans possess a remarkable ability to mentally explore and replay 3D environments they have previously experienced. Inspired by this mental process, we present EvoWorld: a world model that bridges panoramic video generation with evolving 3D memory to enable spatially consistent long-horizon exploration. Given a single panoramic image as input, EvoWorld first generates future video frames by leveraging a video generator with fine-grained view control, then evolves the scene's 3D reconstruction using a feedforward plug-and-play transformer, and finally synthesizes futures by conditioning on geometric reprojections from this evolving explicit 3D memory. Unlike prior state-of-the-arts that synthesize videos only, our key insight lies in exploiting this evolving 3D reconstruction as explicit spatial guidance for the video generation process, projecting the reconstructed geometry onto target viewpoints to provide rich spatial cues that significantly enhance both visual realism and geometric consistency. To evaluate long-range exploration capabilities, we introduce the first comprehensive benchmark spanning synthetic outdoor environments, Habitat indoor scenes, and challenging real-world scenarios, with particular emphasis on loop-closure detection and spatial coherence over extended trajectories. Extensive experiments demonstrate that our evolving 3D memory substantially improves visual fidelity and maintains spatial scene coherence compared to existing approaches, representing a significant advance toward long-horizon spatially consistent world modeling.</li>
<li><strong>摘要：</strong>人类具有出色的能力，可以在精神上探索和重播他们以前经历过的3D环境。受这个心理过程的启发，我们介绍了Evoworld：一种世界模型，它以不断发展的3D记忆将全景视频生成桥梁，以实现空间一致的长途探索。鉴于单个全景图像作为输入，Evoworld首先通过利用具有细粒度视图控件的视频生成器来生成未来的视频帧，然后使用馈电插件的插件变压器进化场景的3D重建，并最终通过从这种进化的Explicit Explicit Explicit 3D存储器中调节几何后期来合成期货。与仅综合视频的先前最新技术不同，我们的关键见解在于利用这种不断发展的3D重建为视频生成过程的明确空间指导，将重建的几何形状投射到目标观点上，以提供丰富的空间线索，从而显着增强可视化现象和几何现实感和几何相结合。为了评估远程探索能力，我们介绍了跨越合成的室外环境，室内场景以及具有挑战性的现实世界情景的第一个全面的基准测试，特别强调了循环闭合检测和空间相干性而不是扩展轨迹。广泛的实验表明，与现有方法相比，我们不断发展的3D记忆可改善视觉保真度，并保持空间场景连贯性，这代表了朝着空间上一致的世界建模方面的重大进展。</li>
</ul>

<h3>Title: Temporal Score Rescaling for Temperature Sampling in Diffusion and Flow Models</h3>
<ul>
<li><strong>Authors: </strong>Yanbo Xu, Yu Wu, Sungjae Park, Zhizhuo Zhou, Shubham Tulsiani</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01184">https://arxiv.org/abs/2510.01184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01184">https://arxiv.org/pdf/2510.01184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01184]] Temporal Score Rescaling for Temperature Sampling in Diffusion and Flow Models(https://arxiv.org/abs/2510.01184)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present a mechanism to steer the sampling diversity of denoising diffusion and flow matching models, allowing users to sample from a sharper or broader distribution than the training distribution. We build on the observation that these models leverage (learned) score functions of noisy data distributions for sampling and show that rescaling these allows one to effectively control a `local' sampling temperature. Notably, this approach does not require any finetuning or alterations to training strategy, and can be applied to any off-the-shelf model and is compatible with both deterministic and stochastic samplers. We first validate our framework on toy 2D data, and then demonstrate its application for diffusion models trained across five disparate tasks -- image generation, pose estimation, depth prediction, robot manipulation, and protein design. We find that across these tasks, our approach allows sampling from sharper (or flatter) distributions, yielding performance gains e.g., depth prediction models benefit from sampling more likely depth estimates, whereas image generation models perform better when sampling a slightly flatter distribution. Project page: this https URL</li>
<li><strong>摘要：</strong>我们提出了一种机制，可以指导DeNo扩散和流匹配模型的采样多样性，从而使用户可以从更清晰或更广泛的分布中取样。我们基于这样的观察结果，即这些模型利用噪声数据分布的得分函数用于采样，并表明重新制定这些分布可以有效地控制“局部”采样温度。值得注意的是，这种方法不需要对培训策略进行任何填充或更改，并且可以应用于任何现成模型，并且与确定性和随机采样器兼容。我们首先在玩具2D数据上验证框架，然后演示其针对跨越五个不同任务训练的扩散模型的应用 - 图像产生，姿势估计，深度预测，机器人操纵和蛋白质设计。我们发现，在这些任务中，我们的方法允许从更清晰（或扁平化）分布进行采样，从而产生性能提高，例如，深度预测模型受益于采样的可能性更可能的深度估计，而图像生成模型在采样稍微平淡的分布时性能更好。项目页面：此HTTPS URL</li>
</ul>

<h3>Title: IMAGEdit: Let Any Subject Transform</h3>
<ul>
<li><strong>Authors: </strong>Fei Shen, Weihao Xu, Rui Yan, Dong Zhang, Xiangbo Shu, Jinhui Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01186">https://arxiv.org/abs/2510.01186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01186">https://arxiv.org/pdf/2510.01186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01186]] IMAGEdit: Let Any Subject Transform(https://arxiv.org/abs/2510.01186)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In this paper, we present IMAGEdit, a training-free framework for any number of video subject editing that manipulates the appearances of multiple designated subjects while preserving non-target regions, without finetuning or retraining. We achieve this by providing robust multimodal conditioning and precise mask sequences through a prompt-guided multimodal alignment module and a prior-based mask retargeting module. We first leverage large models' understanding and generation capabilities to produce multimodal information and mask motion sequences for multiple subjects across various types. Then, the obtained prior mask sequences are fed into a pretrained mask-driven video generation model to synthesize the edited video. With strong generalization capability, IMAGEdit remedies insufficient prompt-side multimodal conditioning and overcomes mask boundary entanglement in videos with any number of subjects, thereby significantly expanding the applicability of video editing. More importantly, IMAGEdit is compatible with any mask-driven video generation model, significantly improving overall performance. Extensive experiments on our newly constructed multi-subject benchmark MSVBench verify that IMAGEdit consistently surpasses state-of-the-art methods. Code, models, and datasets are publicly available at this https URL.</li>
<li><strong>摘要：</strong>在本文中，我们介绍了Imagedit，这是用于任何数量的视频主题编辑的无培训框架，可以操纵多个指定主题的外观，同时保留非目标区域，而无需进行填充或再培训。我们通过通过及时引导的多模式对齐模块和先前的基于基于的掩码重新定位模块提供可靠的多模式调节和精确的面膜序列来实现这一目标。我们首先利用大型模型的理解和发电能力来为各种类型的多个受试者产生多模式信息和掩盖运动序列。然后，将获得的先前掩模序列馈入预验证的面具驱动的视频生成模型，以合成编辑的视频。具有强大的概括能力，ImageDIT疗法不足，及时的多模式调节，并克服了带有许多主题的视频中的掩盖边界纠缠，从而大大扩展了视频编辑的适用性。更重要的是，Imagedit与任何面具驱动的视频生成模型兼容，从而显着提高了整体性能。在我们新建的多主题基准MSVBench上进行的广泛实验验证ImageDit始终超过最新方法。代码，模型和数据集可在此HTTPS URL上公开可用。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
