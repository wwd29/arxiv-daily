<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-02-06</h1>
<h3>Title: Momentum Attention: The Physics of In-Context Learning and Spectral Forensics for Mechanistic Interpretability</h3>
<ul>
<li><strong>Authors: </strong>Kingsuk Maitra</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04902">https://arxiv.org/abs/2602.04902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04902">https://arxiv.org/pdf/2602.04902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04902]] Momentum Attention: The Physics of In-Context Learning and Spectral Forensics for Mechanistic Interpretability(https://arxiv.org/abs/2602.04902)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The Mechanistic Interpretability (MI) program has mapped the Transformer as a precise computational graph. We extend this graph with a conservation law and time-varying AC dynamics, viewing it as a physical circuit. We introduce Momentum Attention, a symplectic augmentation embedding physical priors via the kinematic difference operator $p_t = q_t - q_{t-1}$, implementing the symplectic shear $\hat{q}_t = q_t + \gamma p_t$ on queries and keys. We identify a fundamental Symplectic-Filter Duality: the physical shear is mathematically equivalent to a High-Pass Filter. This duality is our cornerstone contribution -- by injecting kinematic momentum, we sidestep the topological depth constraint ($L \geq 2$) for induction head formation. While standard architectures require two layers for induction from static positions, our extension grants direct access to velocity, enabling Single-Layer Induction and Spectral Forensics via Bode Plots. We formalize an Orthogonality Theorem proving that DC (semantic) and AC (mechanistic) signals segregate into orthogonal frequency bands when Low-Pass RoPE interacts with High-Pass Momentum. Validated through 5,100+ controlled experiments (documented in Supplementary Appendices A--R and 27 Jupyter notebooks), our 125M Momentum model exceeds expectations on induction-heavy tasks while tracking a 350M baseline within $\sim$2.9% validation loss. Dedicated associative recall experiments reveal a scaling law $\gamma^* = 4.17 \times N^{-0.74}$ establishing momentum-depth fungibility. We offer this framework as a complementary analytical toolkit connecting Generative AI, Hamiltonian Physics, and Signal Processing.</li>
<li><strong>摘要：</strong>机械可解释性 (MI) 程序已将 Transformer 映射为精确的计算图。我们用守恒定律和时变交流动力学扩展该图，将其视为物理电路。我们引入动量注意力，这是一种通过运动学差分算子 $p_t = q_t - q_{t-1}$ 嵌入物理先验的辛增强，在查询和键上实现辛剪切 $\hat{q}_t = q_t + \gamma p_t$。我们确定了基本的辛滤波器对偶性：物理剪切在数学上相当于高通滤波器。这种二元性是我们的基石贡献——通过注入运动动量，我们回避了感应头形成的拓扑深度约束（$L \geq 2$）。虽然标准架构需要两层从静态位置进行感应，但我们的扩展允许直接访问速度，从而通过波特图实现单层感应和光谱取证。我们形式化了正交性定理，证明当低通 RoPE 与高通动量相互作用时，直流（语义）和交流（机械）信号分离到正交频带。通过 5,100 多个对照实验（记录在补充附录 A--R 和 27 个 Jupyter 笔记本中）进行验证，我们的 125M Momentum 模型超出了归纳繁重任务的预期，同时在 $\sim$2.9% 验证损失内跟踪 350M 基线。专门的联想回忆实验揭示了建立动量深度可替代性的标度律 $\gamma^* = 4.17 \times N^{-0.74}$。我们提供这个框架作为连接生成人工智能、哈密顿物理和信号处理的补充分析工具包。</li>
</ul>

<h3>Title: Temporal Pair Consistency for Variance-Reduced Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Chika Maduabuchi, Jindong Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04908">https://arxiv.org/abs/2602.04908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04908">https://arxiv.org/pdf/2602.04908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04908]] Temporal Pair Consistency for Variance-Reduced Flow Matching(https://arxiv.org/abs/2602.04908)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Continuous-time generative models, such as diffusion models, flow matching, and rectified flow, learn time-dependent vector fields but are typically trained with objectives that treat timesteps independently, leading to high estimator variance and inefficient sampling. Prior approaches mitigate this via explicit smoothness penalties, trajectory regularization, or modified probability paths and solvers. We introduce Temporal Pair Consistency (TPC), a lightweight variance-reduction principle that couples velocity predictions at paired timesteps along the same probability path, operating entirely at the estimator level without modifying the model architecture, probability path, or solver. We provide a theoretical analysis showing that TPC induces a quadratic, trajectory-coupled regularization that provably reduces gradient variance while preserving the underlying flow-matching objective. Instantiated within flow matching, TPC improves sample quality and efficiency across CIFAR-10 and ImageNet at multiple resolutions, achieving lower FID at identical or lower computational cost than prior methods, and extends seamlessly to modern SOTA-style pipelines with noise-augmented training, score-based denoising, and rectified flow.</li>
<li><strong>摘要：</strong>连续时间生成模型（例如扩散模型、流匹配和校正流）学习时间相关向量场，但通常使用独立处理时间步长的目标进行训练，从而导致估计方差较高和采样效率低下。先前的方法通过显式的平滑度惩罚、轨迹正则化或修改的概率路径和求解器来缓解这一问题。我们引入了时间对一致性（TPC），这是一种轻量级方差减少原理，它将沿着相同概率路径的成对时间步长的速度预测耦合起来，完全在估计器级别上运行，而无需修改模型架构、概率路径或求解器。我们提供的理论分析表明，TPC 引入了二次、轨迹耦合正则化，可证明减少梯度方差，同时保留潜在的流匹配目标。 TPC 在流匹配中实例化，提高了 CIFAR-10 和 ImageNet 多种分辨率下的样本质量和效率，以与先前方法相同或更低的计算成本实现更低的 FID，并通过噪声增强训练、基于分数的去噪和校正流无缝扩展到现代 SOTA 式管道。</li>
</ul>

<h3>Title: A$^2$-LLM: An End-to-end Conversational Audio Avatar Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Xiaolin Hu, Hang Yuan, Xinzhu Sang, Binbin Yan, Zhou Yu, Cong Huang, Kai Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04913">https://arxiv.org/abs/2602.04913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04913">https://arxiv.org/pdf/2602.04913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04913]] A$^2$-LLM: An End-to-end Conversational Audio Avatar Large Language Model(https://arxiv.org/abs/2602.04913)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Developing expressive and responsive conversational digital humans is a cornerstone of next-generation human-computer interaction. While large language models (LLMs) have significantly enhanced dialogue capabilities, most current systems still rely on cascaded architectures that connect independent modules. These pipelines are often plagued by accumulated errors, high latency, and poor real-time performance. Lacking access to the underlying conversational context, these pipelines inherently prioritize rigid lip-sync over emotional depth. To address these challenges, we propose A$^2$-LLM, an end-to-end conversational audio avatar large language model that jointly reasons about language, audio prosody, and 3D facial motion within a unified framework. To facilitate training, we introduce FLAME-QA, a high-quality multimodal dataset designed to align semantic intent with expressive facial dynamics within a QA format. By leveraging deep semantic understanding, A$^2$-LLM generates emotionally rich facial movements beyond simple lip-synchronization. Experimental results demonstrate that our system achieves superior emotional expressiveness while maintaining real-time efficiency (500 ms latency, 0.7 RTF).</li>
<li><strong>摘要：</strong>开发富有表现力和响应能力的对话式数字人类是下一代人机交互的基石。虽然大型语言模型（LLM）显着增强了对话能力，但当前大多数系统仍然依赖于连接独立模块的级联架构。这些管道经常受到累积错误、高延迟和差实时性能的困扰。由于缺乏对潜在对话环境的访问，这些管道本质上优先考虑严格的口型同步而不是情感深度。为了应对这些挑战，我们提出了 A$^2$-LLM，一种端到端会话音频化身大型语言模型，可在统一框架内联合推理语言、音频韵律和 3D 面部动作。为了促进训练，我们引入了 FLAME-QA，这是一个高质量的多模态数据集，旨在将语义意图与 QA 格式中的表达性面部动态保持一致。通过利用深度语义理解，A$^2$-LLM 可以生成情感丰富的面部动作，而不仅仅是简单的嘴唇同步。实验结果表明，我们的系统在保持实时效率（500 毫秒延迟，0.7 RTF）的同时实现了卓越的情感表达。</li>
</ul>

<h3>Title: Gradually Compacting Large Language Models for Reasoning Like a Boiling Frog</h3>
<ul>
<li><strong>Authors: </strong>Yiran Zhao, Shengyang Zhou, Zijian Wu, Tongyan Hu, Yuhui Xu, Rengan Dou, Kenji Kawaguchi, Shafiq Joty, Junnan Li, Michael Qizhe Shieh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04919">https://arxiv.org/abs/2602.04919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04919">https://arxiv.org/pdf/2602.04919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04919]] Gradually Compacting Large Language Models for Reasoning Like a Boiling Frog(https://arxiv.org/abs/2602.04919)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, but their substantial size often demands significant computational resources. To reduce resource consumption and accelerate inference, it is essential to eliminate redundant parameters without compromising performance. However, conventional pruning methods that directly remove such parameters often lead to a dramatic drop in model performance in reasoning tasks, and require extensive post-training to recover the lost capabilities. In this work, we propose a gradual compacting method that divides the compression process into multiple fine-grained iterations, applying a Prune-Tune Loop (PTL) at each stage to incrementally reduce model size while restoring performance with finetuning. This iterative approach-reminiscent of the "boiling frog" effect-enables the model to be progressively compressed without abrupt performance loss. Experimental results show that PTL can compress LLMs to nearly half their original size with only lightweight post-training, while maintaining performance comparable to the original model on reasoning tasks. Moreover, PTL is flexible and can be applied to various pruning strategies, such as neuron pruning and layer pruning, as well as different post-training methods, including continual pre-training and reinforcement learning. Additionally, experimental results confirm the effectiveness of PTL on a variety of tasks beyond mathematical reasoning, such as code generation, demonstrating its broad applicability.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已展现出令人印象深刻的推理能力，但其庞大的规模通常需要大量的计算资源。为了减少资源消耗并加速推理，必​​须在不影响性能的情况下消除冗余参数。然而，直接去除此类参数的传统剪枝方法往往会导致模型在推理任务中的性能急剧下降，并且需要大量的后训练来恢复丢失的能力。在这项工作中，我们提出了一种渐进压缩方法，将压缩过程分为多个细粒度迭代，在每个阶段应用 Prune-Tune Loop (PTL) 来逐步减小模型大小，同时通过微调恢复性能。这种迭代方法让人想起“温水煮青蛙”效应，使模型能够逐步压缩，而不会突然造成性能损失。实验结果表明，PTL 只需轻量级的后训练即可将 LLM 压缩到原始大小的近一半，同时在推理任务上保持与原始模型相当的性能。此外，PTL非常灵活，可以应用于各种剪枝策略，例如神经元剪枝和层剪枝，以及不同的后训练方法，包括持续预训练和强化学习。此外，实验结果证实了 PTL 在数学推理之外的各种任务（例如代码生成）上的有效性，证明了其广泛的适用性。</li>
</ul>

<h3>Title: Internalizing LLM Reasoning via Discovery and Replay of Latent Actions</h3>
<ul>
<li><strong>Authors: </strong>Zhenning Shi, Yijia Zhu, Junhan Shi, Xun Zhang, Lei Wang, Congcong Miao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04925">https://arxiv.org/abs/2602.04925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04925">https://arxiv.org/pdf/2602.04925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04925]] Internalizing LLM Reasoning via Discovery and Replay of Latent Actions(https://arxiv.org/abs/2602.04925)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The internalization of chain-of-thought processes into hidden states has emerged as a highly efficient paradigm for scaling test-time compute. However, existing activation steering methods rely on static control vectors that fail to adapt to the non-stationary evolution of complex reasoning tasks. To address this limitation, we propose STIR (Self-Distilled Tools for Internal Reasoning), a framework that reformulates reasoning enhancement as a dynamic latent trajectory control problem. STIR introduces a synergistic three-stage pipeline: (1) differential intrinsic action induction harvests latent reasoning successes to crystallize steering primitives; (2) sparse control basis construction curates a compact, geometrically diverse tool library; and (3) value-modulated trajectory intervention dynamically injects context-specific impulses via anchor-based gating. Extensive experiments on six arithmetic and logical benchmarks across four representative models demonstrate that STIR improves average accuracy by 1.9% to 7.5% while reducing average token consumption by up to 35% compared to vanilla decoding. These findings demonstrate that the benefits of explicit chain-of-thought can be realized through dynamic latent trajectory control, internalizing the reasoning process to bypass the explicit generation while achieving superior fidelity. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>将思想过程内化为隐藏状态已成为扩展测试时计算的高效范例。然而，现有的激活引导方法依赖于静态控制向量，无法适应复杂推理任务的非平稳演化。为了解决这个限制，我们提出了 STIR（内部推理自蒸馏工具），这是一个将推理增强重新表述为动态潜在轨迹控制问题的框架。 STIR 引入了协同三阶段流程：（1）微分内在动作归纳收获潜在推理成功，以具体化转向原语； (2) 稀疏控制基础构建构建了一个紧凑的、几何多样化的工具库； (3)价值调制轨迹干预通过基于锚的门控动态注入上下文特定的脉冲。对四个代表性模型的六个算术和逻辑基准进行的广泛实验表明，与普通解码相比，STIR 将平均准确度提高了 1.9% 至 7.5%，同时将平均令牌消耗减少了高达 35%。这些发现表明，显式思维链的好处可以通过动态潜在轨迹控制来实现，将推理过程内在化以绕过显式生成，同时实现卓越的保真度。我们的代码可以在这个 https URL 上找到。</li>
</ul>

<h3>Title: Euphonium: Steering Video Flow Matching via Process Reward Gradient Guided Stochastic Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Ruizhe Zhong, Jiesong Lian, Xiaoyue Mi, Zixiang Zhou, Yuan Zhou, Qinglin Lu, Junchi Yan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04928">https://arxiv.org/abs/2602.04928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04928">https://arxiv.org/pdf/2602.04928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04928]] Euphonium: Steering Video Flow Matching via Process Reward Gradient Guided Stochastic Dynamics(https://arxiv.org/abs/2602.04928)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While online Reinforcement Learning has emerged as a crucial technique for aligning flow matching models with human preferences, current approaches are hindered by inefficient exploration during training rollouts. Relying on undirected stochasticity and sparse outcome rewards, these methods struggle to discover high-reward samples, resulting in data-inefficient and slow optimization. To address these limitations, we propose Euphonium, a novel framework that steers generation via process reward gradient guided dynamics. Our key insight is to formulate the sampling process as a theoretically principled Stochastic Differential Equation that explicitly incorporates the gradient of a Process Reward Model into the flow drift. This design enables dense, step-by-step steering toward high-reward regions, advancing beyond the unguided exploration in prior works, and theoretically encompasses existing sampling methods (e.g., Flow-GRPO, DanceGRPO) as special cases. We further derive a distillation objective that internalizes the guidance signal into the flow network, eliminating inference-time dependency on the reward model. We instantiate this framework with a Dual-Reward Group Relative Policy Optimization algorithm, combining latent process rewards for efficient credit assignment with pixel-level outcome rewards for final visual fidelity. Experiments on text-to-video generation show that Euphonium achieves better alignment compared to existing methods while accelerating training convergence by 1.66x.</li>
<li><strong>摘要：</strong>虽然在线强化学习已成为将流匹配模型与人类偏好相结合的关键技术，但当前的方法因训练推出期间的低效探索而受到阻碍。这些方法依赖于无向随机性和稀疏的结果奖励，很难发现高奖励样本，导致数据效率低下和优化缓慢。为了解决这些限制，我们提出了 Euphonium，这是一种通过过程奖励梯度引导动力学来引导生成的新颖框架。我们的主要见解是将采样过程表述为理论上有原理的随机微分方程，该方程明确地将过程奖励模型的梯度纳入流量漂移中。这种设计能够密集、逐步转向高回报区域，超越了先前作品中的无引导探索，并且理论上包含现有采样方法（例如 Flow-GRPO、DanceGRPO）作为特殊情况。我们进一步推导出一个蒸馏目标，将引导信号内化到流网络中，消除对奖励模型的推理时间依赖。我们使用双奖励组相对策略优化算法实例化该框架，将有效信用分配的潜在过程奖励与最终视觉保真度的像素级结果奖励相结合。文本到视频生成的实验表明，与现有方法相比，Euphium 实现了更好的对齐，同时将训练收敛速度加快了 1.66 倍。</li>
</ul>

<h3>Title: Position: Machine Learning for Heart Transplant Allocation Policy Optimization Should Account for Incentives</h3>
<ul>
<li><strong>Authors: </strong>Ioannis Anagnostides, Itai Zilberstein, Zachary W. Sollie, Arman Kilic, Tuomas Sandholm</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04990">https://arxiv.org/abs/2602.04990</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04990">https://arxiv.org/pdf/2602.04990</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04990]] Position: Machine Learning for Heart Transplant Allocation Policy Optimization Should Account for Incentives(https://arxiv.org/abs/2602.04990)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The allocation of scarce donor organs constitutes one of the most consequential algorithmic challenges in healthcare. While the field is rapidly transitioning from rigid, rule-based systems to machine learning and data-driven optimization, we argue that current approaches often overlook a fundamental barrier: incentives. In this position paper, we highlight that organ allocation is not merely a static optimization problem, but rather a complex game involving transplant centers, clinicians, and regulators. Focusing on US adult heart transplant allocation, we identify critical incentive misalignments across the decision-making pipeline, and present data showing that they are having adverse consequences today. Our main position is that the next generation of allocation policies should be incentive aware. We outline a research agenda for the machine learning community, calling for the integration of mechanism design, strategic classification, causal inference, and social choice to ensure robustness, efficiency, and fairness in the face of strategic behavior from the various constituent groups.</li>
<li><strong>摘要：</strong>稀缺供体器官的分配是医疗保健领域最重要的算法挑战之一。尽管该领域正在迅速从严格的、基于规则的系统过渡到机器学习和数据驱动的优化，但我们认为当前的方法往往忽视了一个根本障碍：激励措施。在这篇立场文件中，我们强调器官分配不仅仅是一个静态优化问题，而是一个涉及移植中心、临床医生和监管机构的复杂博弈。我们重点关注美国成人心脏移植分配，发现整个决策流程中关键的激励失调，并提供数据显示它们目前正在产生不利后果。我们的主要立场是下一代分配政策应该具有激励意识。我们为机器学习界制定了一个研究议程，呼吁整合机制设计、战略分类、因果推理和社会选择，以确保面对不同组成群体的战略行为时的稳健性、效率和公平性。</li>
</ul>

<h3>Title: SIDeR: Semantic Identity Decoupling for Unrestricted Face Privacy</h3>
<ul>
<li><strong>Authors: </strong>Zhuosen Bao, Xia Du, Zheng Lin, Jizhe Zhou, Zihan Fang, Jiening Wu, Yuxin Zhang, Zhe Chen, Chi-man Pun, Wei Ni, Jun Luo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04994">https://arxiv.org/abs/2602.04994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04994">https://arxiv.org/pdf/2602.04994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04994]] SIDeR: Semantic Identity Decoupling for Unrestricted Face Privacy(https://arxiv.org/abs/2602.04994)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>With the deep integration of facial recognition into online banking, identity verification, and other networked services, achieving effective decoupling of identity information from visual representations during image storage and transmission has become a critical challenge for privacy protection. To address this issue, we propose SIDeR, a Semantic decoupling-driven framework for unrestricted face privacy protection. SIDeR decomposes a facial image into a machine-recognizable identity feature vector and a visually perceptible semantic appearance component. By leveraging semantic-guided recomposition in the latent space of a diffusion model, it generates visually anonymous adversarial faces while maintaining machine-level identity consistency. The framework incorporates momentum-driven unrestricted perturbation optimization and a semantic-visual balancing factor to synthesize multiple visually diverse, highly natural adversarial samples. Furthermore, for authorized access, the protected image can be restored to its original form when the correct password is provided. Extensive experiments on the CelebA-HQ and FFHQ datasets demonstrate that SIDeR achieves a 99% attack success rate in black-box scenarios and outperforms baseline methods by 41.28% in PSNR-based restoration quality.</li>
<li><strong>摘要：</strong>随着人脸识别与网上银行、身份验证等网络服务的深度融合，在图像存储和传输过程中实现身份信息与视觉表征的有效解耦已成为隐私保护的关键挑战。为了解决这个问题，我们提出了 SIDeR，一种语义解耦驱动的框架，用于无限制的人脸隐私保护。 SIDeR 将面部图像分解为机器可识别的身份特征向量和视觉可感知的语义外观组件。通过在扩散模型的潜在空间中利用语义引导的重组，它可以生成视觉上匿名的对抗面孔，同时保持机器级身份的一致性。该框架结合了动量驱动的无限制扰动优化和语义视觉平衡因子来合成多个视觉多样化、高度自然的对抗样本。此外，对于授权访问，当提供正确的密码时，受保护的图像可以恢复到其原始形式。在 CelebA-HQ 和 FFHQ 数据集上进行的大量实验表明，SIDeR 在黑盒场景中实现了 99% 的攻击成功率，并且在基于 PSNR 的恢复质量方面优于基线方法 41.28%。</li>
</ul>

<h3>Title: Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Yu-Ang Lee, Ching-Yun Ko, Pin-Yu Chen, Mi-Yen Yeh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04998">https://arxiv.org/abs/2602.04998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04998">https://arxiv.org/pdf/2602.04998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04998]] Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning(https://arxiv.org/abs/2602.04998)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Low-Rank Adaptation (LoRA) is the prevailing approach for efficient large language model (LLM) fine-tuning. Building on this paradigm, recent studies have proposed alternative initialization strategies and architectural modifications, reporting substantial improvements over vanilla LoRA. However, these gains are often demonstrated under fixed or narrowly tuned hyperparameter settings, despite the known sensitivity of neural networks to training configurations. In this work, we systematically re-evaluate four representative LoRA variants alongside vanilla LoRA through extensive hyperparameter searches. Across mathematical and code generation tasks on diverse model scales, we find that different LoRA methods favor distinct learning rate ranges. Crucially, once learning rates are properly tuned, all methods achieve similar peak performance (within 1-2%), with only subtle rank-dependent behaviors. These results suggest that vanilla LoRA remains a competitive baseline and that improvements reported under single training configuration may not reflect consistent methodological advantages. Finally, a second-order analysis attributes the differing optimal learning rate ranges to variations in the largest Hessian eigenvalue, aligning with classical learning theories.</li>
<li><strong>摘要：</strong>低秩适应 (LoRA) 是高效大型语言模型 (LLM) 微调的流行方法。在此范式的基础上，最近的研究提出了替代初始化策略和架构修改，报告了相对于普通 LoRA 的重大改进。然而，尽管神经网络对训练配置的敏感性已知，但这些收益通常在固定或微调的超参数设置下得到证明。在这项工作中，我们通过广泛的超参数搜索，系统地重新评估了四种代表性 LoRA 变体以及普通 LoRA。在不同模型规模的数学和代码生成任务中，我们发现不同的 LoRA 方法有利于不同的学习率范围。至关重要的是，一旦学习率得到适当调整，所有方法都会达到相似的峰值性能（1-2% 以内），只有微妙的排名相关行为。这些结果表明，普通 LoRA 仍然是一个有竞争力的基线，并且在单一训练配置下报告的改进可能无法反映一致的方法优势。最后，二阶分析将不同的最佳学习率范围归因于最大 Hessian 特征值的变化，与经典学习理论保持一致。</li>
</ul>

<h3>Title: Private PoEtry: Private In-Context Learning via Product of Experts</h3>
<ul>
<li><strong>Authors: </strong>Rob Romijnders, Mohammad Mahdi Derakhshani, Jonathan Petit, Max Welling, Christos Louizos, Yuki M. Asano</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05012">https://arxiv.org/abs/2602.05012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05012">https://arxiv.org/pdf/2602.05012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05012]] Private PoEtry: Private In-Context Learning via Product of Experts(https://arxiv.org/abs/2602.05012)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) enables Large Language Models (LLMs) to adapt to new tasks with only a small set of examples at inference time, thereby avoiding task-specific fine-tuning. However, in-context examples may contain privacy-sensitive information that should not be revealed through model outputs. Existing differential privacy (DP) approaches to ICL are either computationally expensive or rely on heuristics with limited effectiveness, including context oversampling, synthetic data generation, or unnecessary thresholding. We reformulate private ICL through the lens of a Product-of-Experts model. This gives a theoretically grounded framework, and the algorithm can be trivially parallelized. We evaluate our method across five datasets in text classification, math, and vision-language. We find that our method improves accuracy by more than 30 percentage points on average compared to prior DP-ICL methods, while maintaining strong privacy guarantees.</li>
<li><strong>摘要：</strong>上下文学习 (ICL) 使大型语言模型 (LLM) 能够在推理时仅用少量示例来适应新任务，从而避免特定于任务的微调。但是，上下文示例可能包含不应通过模型输出泄露的隐私敏感信息。现有的 ICL 差分隐私 (DP) 方法要么计算成本昂贵，要么依赖有效性有限的启发式方法，包括上下文过采样、合成数据生成或不必要的阈值处理。我们通过专家产品模型的视角重新制定私人 ICL。这给出了一个理论上有基础的框架，并且该算法可以简单地并行化。我们在文本分类、数学和视觉语言的五个数据集上评估我们的方法。我们发现，与之前的 DP-ICL 方法相比，我们的方法的准确率平均提高了 30 个百分点以上，同时保持了强大的隐私保证。</li>
</ul>

<h3>Title: Rethinking Rubric Generation for Improving LLM Judge and Reward Modeling for Open-ended Tasks</h3>
<ul>
<li><strong>Authors: </strong>William F. Shen, Xinchi Qiu, Chenxi Whitehouse, Lisa Alazraki, Shashwat Goel, Francesco Barbieri, Timon Willi, Akhil Mathur, Ilias Leontiadis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05125">https://arxiv.org/abs/2602.05125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05125">https://arxiv.org/pdf/2602.05125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05125]] Rethinking Rubric Generation for Improving LLM Judge and Reward Modeling for Open-ended Tasks(https://arxiv.org/abs/2602.05125)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recently, rubrics have been used to guide LLM judges in capturing subjective, nuanced, multi-dimensional human preferences, and have been extended from evaluation to reward signals for reinforcement fine-tuning (RFT). However, rubric generation remains hard to control: rubrics often lack coverage, conflate dimensions, misalign preference direction, and contain redundant or highly correlated criteria, degrading judge accuracy and producing suboptimal rewards during RFT. We propose RRD, a principled framework for rubric refinement built on a recursive decompose-filter cycle. RRD decomposes coarse rubrics into fine-grained, discriminative criteria, expanding coverage while sharpening separation between responses. A complementary filtering mechanism removes misaligned and redundant rubrics, and a correlation-aware weighting scheme prevents over-representing highly correlated criteria, yielding rubric sets that are informative, comprehensive, and non-redundant. Empirically, RRD delivers large, consistent gains across both evaluation and training: it improves preference-judgment accuracy on JudgeBench and PPE for both GPT-4o and Llama3.1-405B judges, achieving top performance in all settings with up to +17.7 points on JudgeBench. When used as the reward source for RFT on WildChat, it yields substantially stronger and more stable learning signals, boosting reward by up to 160% (Qwen3-4B) and 60% (Llama3.1-8B) versus 10-20% for prior rubric baselines, with gains that transfer to HealthBench-Hard and BiGGen Bench. Overall, RRD establishes recursive rubric refinement as a scalable and interpretable foundation for LLM judging and reward modeling in open-ended domains.</li>
<li><strong>摘要：</strong>最近，评分细则已被用来指导法学硕士评委捕捉主观、细致、多维的人类偏好，并已从评估扩展到强化微调（RFT）的奖励信号。然而，评分标准的生成仍然难以控制：评分标准通常缺乏覆盖范围、维度混杂、偏好方向错位、包含冗余或高度相关的标准，从而降低了判断准确性并在 RFT 期间产生次优奖励。我们提出了 RRD，这是一个基于递归分解过滤循环的细化标题的原则框架。 RRD 将粗粒度的标准分解为细粒度的判别性标准，扩大了覆盖范围，同时加深了响应之间的分离。互补的过滤机制可以消除未对齐和冗余的评分标准，而相关性加权方案可以防止过度代表高度相关的标准，从而产生信息丰富、全面且非冗余的评分标准集。根据经验，RRD 在评估和训练方面都带来了巨大且一致的收益：它提高了 GPT-4o 和 Llama3.1-405B 法官在 JudgeBench 和 PPE 上的偏好判断准确性，在所有设置中实现了最佳表现，在 JudgeBench 上高达 +17.7 分。当用作 WildChat 上 RFT 的奖励源时，它会产生更强、更稳定的学习信号，将奖励提高高达 160% (Qwen3-4B) 和 60% (Llama3.1-8B)，而之前的评分标准基线为 10-20%，收益可转移到 HealthBench-Hard 和 BiGGen Bench。总体而言，RRD 建立了递归细化标准，作为开放领域中 LLM 评审和奖励建模的可扩展和可解释的基础。</li>
</ul>

<h3>Title: Fairness Under Group-Conditional Prior Probability Shift: Invariance, Drift, and Target-Aware Post-Processing</h3>
<ul>
<li><strong>Authors: </strong>Amir Asiaee, Kaveh Aryan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05144">https://arxiv.org/abs/2602.05144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05144">https://arxiv.org/pdf/2602.05144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05144]] Fairness Under Group-Conditional Prior Probability Shift: Invariance, Drift, and Target-Aware Post-Processing(https://arxiv.org/abs/2602.05144)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Machine learning systems are often trained and evaluated for fairness on historical data, yet deployed in environments where conditions have shifted. A particularly common form of shift occurs when the prevalence of positive outcomes changes differently across demographic groups--for example, when disease rates rise faster in one population than another, or when economic conditions affect loan default rates unequally. We study group-conditional prior probability shift (GPPS), where the label prevalence $P(Y=1\mid A=a)$ may change between training and deployment while the feature-generation process $P(X\mid Y,A)$ remains stable. Our analysis yields three main contributions. First, we prove a fundamental dichotomy: fairness criteria based on error rates (equalized odds) are structurally invariant under GPPS, while acceptance-rate criteria (demographic parity) can drift--and we prove this drift is unavoidable for non-trivial classifiers (shift-robust impossibility). Second, we show that target-domain risk and fairness metrics are identifiable without target labels: the invariance of ROC quantities under GPPS enables consistent estimation from source labels and unlabeled target data alone, with finite-sample guarantees. Third, we propose TAP-GPPS, a label-free post-processing algorithm that estimates prevalences from unlabeled data, corrects posteriors, and selects thresholds to satisfy demographic parity in the target domain. Experiments validate our theoretical predictions and demonstrate that TAP-GPPS achieves target fairness with minimal utility loss.</li>
<li><strong>摘要：</strong>机器学习系统通常会根据历史数据的公平性进行训练和评估，但会部署在条件发生变化的环境中。当不同人口群体的积极结果发生率发生不同变化时，就会出现一种特别常见的转变形式——例如，当一个人群的疾病发病率上升速度快于另一个人群时，或者当经济状况对贷款违约率的影响不同时。我们研究组条件先验概率转移（GPPS），其中标签流行度 $P(Y=1\mid A=a)$ 在训练和部署之间可能会发生变化，而特征生成过程 $P(X\mid Y,A)$ 保持稳定。我们的分析产生了三个主要贡献。首先，我们证明了一个基本的二分法：基于错误率（均等赔率）的公平标准在 GPPS 下在结构上是不变的，而接受率标准（人口均等）可能会发生漂移——并且我们证明这种漂移对于非平凡的分类器来说是不可避免的（移位鲁棒不可能性）。其次，我们证明了目标域风险和公平性指标无需目标标签即可识别：GPPS 下 ROC 数量的不变性使得能够仅根据源标签和未标记的目标数据进行一致的估计，并保证有限样本。第三，我们提出了 TAP-GPPS，一种无标签后处理算法，可根据未标记数据估计患病率，校正后验，并选择阈值以满足目标域中的人口统计平等。实验验证了我们的理论预测，并证明 TAP-GPPS 以最小的效用损失实现了目标公平性。</li>
</ul>

<h3>Title: CoSA: Compressed Sensing-Based Adaptation of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Songtao Wei, Yi Li, Bohan Zhang, Zhichun Guo, Ying Huang, Yuede Ji, Miao Yin, Guanpeng Li, Bingzhe Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05148">https://arxiv.org/abs/2602.05148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05148">https://arxiv.org/pdf/2602.05148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05148]] CoSA: Compressed Sensing-Based Adaptation of Large Language Models(https://arxiv.org/abs/2602.05148)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Parameter-Efficient Fine-Tuning (PEFT) has emerged as a practical paradigm for adapting large language models (LLMs) without updating all parameters. Most existing approaches, such as LoRA and PiSSA, rely on low-rank decompositions of weight updates. However, the low-rank assumption may restrict expressivity, particularly in task-specific adaptation scenarios where singular values are distributed relatively uniformly. To address this limitation, we propose CoSA (Compressed Sensing-Based Adaptation), a new PEFT method extended from compressed sensing theory. Instead of constraining weight updates to a low-rank subspace, CoSA expresses them through fixed random projection matrices and a compact learnable core. We provide a formal theoretical analysis of CoSA as a synthesis process, proving that weight updates can be compactly encoded into a low-dimensional space and mapped back through random projections. Extensive experimental results show that CoSA provides a principled perspective for efficient and expressive multi-scale model adaptation. Specifically, we evaluate CoSA on 10 diverse tasks, including natural language understanding and generation, employing 5 models of different scales from RoBERTa, Llama, and Qwen families. Across these settings, CoSA consistently matches or outperforms state-of-the-art PEFT methods.</li>
<li><strong>摘要：</strong>参数高效微调 (PEFT) 已成为一种在不更新所有参数的情况下适应大型语言模型 (LLM) 的实用范例。大多数现有方法，例如 LoRA 和 PiSSA，都依赖于权重更新的低秩分解。然而，低秩假设可能会限制表达能力，特别是在奇异值分布相对均匀的特定任务适应场景中。为了解决这个限制，我们提出了 CoSA（基于压缩感知的自适应），这是一种从压缩感知理论扩展而来的新 PEFT 方法。 CoSA 没有将权重更新限制在低秩子空间，而是通过固定的随机投影矩阵和紧凑的可学习核心来表达它们。我们提供了 CoSA 作为综合过程的正式理论分析，证明权重更新可以紧凑地编码到低维空间中，并通过随机投影映射回来。大量的实验结果表明，CoSA 为高效且富有表现力的多尺度模型适应提供了原则性的视角。具体来说，我们使用 RoBERTa、Llama 和 Qwen 系列的 5 个不同规模的模型，在 10 项不同的任务上评估 CoSA，包括自然语言理解和生成。在这些设置中，CoSA 始终匹配或优于最先进的 PEFT 方法。</li>
</ul>

<h3>Title: LOBSTgER-enhance: an underwater image enhancement pipeline</h3>
<ul>
<li><strong>Authors: </strong>Andreas Mentzelopoulos, Keith Ellenbogen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05163">https://arxiv.org/abs/2602.05163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05163">https://arxiv.org/pdf/2602.05163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05163]] LOBSTgER-enhance: an underwater image enhancement pipeline(https://arxiv.org/abs/2602.05163)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Underwater photography presents significant inherent challenges including reduced contrast, spatial blur, and wavelength-dependent color distortions. These effects can obscure the vibrancy of marine life and awareness photographers in particular are often challenged with heavy post-processing pipelines to correct for these distortions. We develop an image-to-image pipeline that learns to reverse underwater degradations by introducing a synthetic corruption pipeline and learning to reverse its effects with diffusion-based generation. Training and evaluation are performed on a small high-quality dataset of awareness photography images by Keith Ellenbogen. The proposed methodology achieves high perceptual consistency and strong generalization in synthesizing 512x768 images using a model of ~11M parameters after training from scratch on ~2.5k images.</li>
<li><strong>摘要：</strong>水下摄影面临着重大的固有挑战，包括对比度降低、空间模糊和与波长相关的颜色失真。这些效果可能会掩盖海洋生物的活力，特别是意识摄影师经常面临繁重的后处理流程来纠正这些扭曲的挑战。我们开发了一种图像到图像的管道，通过引入合成腐败管道并学习通过基于扩散的生成来扭转其影响，从而学习扭转水下退化。训练和评估是在 Keith Ellenbogen 的一个小型高质量意识摄影图像数据集上进行的。所提出的方法在从头开始对约 2.5k 图像进行训练后，使用约 11M 参数的模型合成 512x768 图像，实现了高感知一致性和强泛化性。</li>
</ul>

<h3>Title: GT-SVJ: Generative-Transformer-Based Self-Supervised Video Judge For Efficient Video Reward Modeling</h3>
<ul>
<li><strong>Authors: </strong>Shivanshu Shekhar, Uttaran Bhattacharya, Raghavendra Addanki, Mehrab Tanjim, Somdeb Sarkhel, Tong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05202">https://arxiv.org/abs/2602.05202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05202">https://arxiv.org/pdf/2602.05202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05202]] GT-SVJ: Generative-Transformer-Based Self-Supervised Video Judge For Efficient Video Reward Modeling(https://arxiv.org/abs/2602.05202)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Aligning video generative models with human preferences remains challenging: current approaches rely on Vision-Language Models (VLMs) for reward modeling, but these models struggle to capture subtle temporal dynamics. We propose a fundamentally different approach: repurposing video generative models, which are inherently designed to model temporal structure, as reward models. We present the Generative-Transformer-based Self-Supervised Video Judge (\modelname), a novel evaluation model that transforms state-of-the-art video generation models into powerful temporally-aware reward models. Our key insight is that generative models can be reformulated as energy-based models (EBMs) that assign low energy to high-quality videos and high energy to degraded ones, enabling them to discriminate video quality with remarkable precision when trained via contrastive objectives. To prevent the model from exploiting superficial differences between real and generated videos, we design challenging synthetic negative videos through controlled latent-space perturbations: temporal slicing, feature swapping, and frame shuffling, which simulate realistic but subtle visual degradations. This forces the model to learn meaningful spatiotemporal features rather than trivial artifacts. \modelname achieves state-of-the-art performance on GenAI-Bench and MonteBench using only 30K human-annotations: $6\times$ to $65\times$ fewer than existing VLM-based approaches.</li>
<li><strong>摘要：</strong>将视频生成模型与人类偏好保持一致仍然具有挑战性：当前的方法依赖视觉语言模型（VLM）进行奖励建模，但这些模型难以捕捉微妙的时间动态。我们提出了一种根本不同的方法：重新利用视频生成模型作为奖励模型，视频生成模型本质上是为了建模时间结构而设计的。我们提出了基于生成变压器的自监督视频法官（\modelname），这是一种新颖的评估模型，可将最先进的视频生成模型转换为强大的时间感知奖励模型。我们的主要见解是，生成模型可以重新表述为基于能量的模型（EBM），将低能量分配给高质量视频，将高能量分配给降级视频，从而使它们能够在通过对比目标进行训练时以极高的精度区分视频质量。为了防止模型利用真实视频和生成视频之间的表面差异，我们通过受控的潜在空间扰动设计了具有挑战性的合成负视频：时间切片、特征交换和帧洗牌，模拟真实但微妙的视觉退化。这迫使模型学习有意义的时空特征，而不是琐碎的伪影。 \modelname 仅使用 30K 人工注释即可在 GenAI-Bench 和 MonteBench 上实现最先进的性能：比现有的基于 VLM 的方法少 6\times$ 到 $65\times$。</li>
</ul>

<h3>Title: Extreme Weather Nowcasting via Local Precipitation Pattern Prediction</h3>
<ul>
<li><strong>Authors: </strong>Changhoon Song, Teng Yuan Chang, Youngjoon Hong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05204">https://arxiv.org/abs/2602.05204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05204">https://arxiv.org/pdf/2602.05204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05204]] Extreme Weather Nowcasting via Local Precipitation Pattern Prediction(https://arxiv.org/abs/2602.05204)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Accurate forecasting of extreme weather events such as heavy rainfall or storms is critical for risk management and disaster mitigation. Although high-resolution radar observations have spurred extensive research on nowcasting models, precipitation nowcasting remains particularly challenging due to pronounced spatial locality, intricate fine-scale rainfall structures, and variability in forecasting horizons. While recent diffusion-based generative ensembles show promising results, they are computationally expensive and unsuitable for real-time applications. In contrast, deterministic models are computationally efficient but remain biased toward normal rainfall. Furthermore, the benchmark datasets commonly used in prior studies are themselves skewed--either dominated by ordinary rainfall events or restricted to extreme rainfall episodes--thereby hindering general applicability in real-world settings. In this paper, we propose exPreCast, an efficient deterministic framework for generating finely detailed radar forecasts, and introduce a newly constructed balanced radar dataset from the Korea Meteorological Administration (KMA), which encompasses both ordinary precipitation and extreme events. Our model integrates local spatiotemporal attention, a texture-preserving cubic dual upsampling decoder, and a temporal extractor to flexibly adjust forecasting horizons. Experiments on established benchmarks (SEVIR and MeteoNet) as well as on the balanced KMA dataset demonstrate that our approach achieves state-of-the-art performance, delivering accurate and reliable nowcasts across both normal and extreme rainfall regimes.</li>
<li><strong>摘要：</strong>准确预报强降雨或风暴等极端天气事件对于风险管理和减灾至关重要。尽管高分辨率雷达观测刺激了对临近预报模型的广泛研究，但由于明显的空间局部性、复杂的细尺度降雨结构以及预报范围的变化，降水临近预报仍然特别具有挑战性。虽然最近基于扩散的生成集成显示出有希望的结果，但它们的计算成本很高并且不适合实时应用。相比之下，确定性模型计算效率较高，但仍然偏向正常降雨。此外，先前研究中常用的基准数据集本身就有偏差——要么以普通降雨事件为主，要么仅限于极端降雨事件——从而阻碍了在现实世界环境中的普遍适用性。在本文中，我们提出了 exPreCast，这是一种用于生成详细雷达预报的有效确定性框架，并介绍了韩国气象局（KMA）新构建的平衡雷达数据集，其中包括普通降水和极端事件。我们的模型集成了局部时空注意力、纹理保留立方双上采样解码器和时间提取器来灵活调整预测范围。在既定基准（SEVIR 和 MeteoNet）以及平衡 KMA 数据集上进行的实验表明，我们的方法实现了最先进的性能，在正常和极端降雨情况下都能提供准确可靠的临近预报。</li>
</ul>

<h3>Title: Dual-Representation Image Compression at Ultra-Low Bitrates via Explicit Semantics and Implicit Textures</h3>
<ul>
<li><strong>Authors: </strong>Chuqin Zhou, Xiaoyue Ling, Yunuo Chen, Jincheng Dai, Guo Lu, Wenjun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05213">https://arxiv.org/abs/2602.05213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05213">https://arxiv.org/pdf/2602.05213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05213]] Dual-Representation Image Compression at Ultra-Low Bitrates via Explicit Semantics and Implicit Textures(https://arxiv.org/abs/2602.05213)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While recent neural codecs achieve strong performance at low bitrates when optimized for perceptual quality, their effectiveness deteriorates significantly under ultra-low bitrate conditions. To mitigate this, generative compression methods leveraging semantic priors from pretrained models have emerged as a promising paradigm. However, existing approaches are fundamentally constrained by a tradeoff between semantic faithfulness and perceptual realism. Methods based on explicit representations preserve content structure but often lack fine-grained textures, whereas implicit methods can synthesize visually plausible details at the cost of semantic drift. In this work, we propose a unified framework that bridges this gap by coherently integrating explicit and implicit representations in a training-free manner. Specifically, We condition a diffusion model on explicit high-level semantics while employing reverse-channel coding to implicitly convey fine-grained details. Moreover, we introduce a plug-in encoder that enables flexible control of the distortion-perception tradeoff by modulating the implicit information. Extensive experiments demonstrate that the proposed framework achieves state-of-the-art rate-perception performance, outperforming existing methods and surpassing DiffC by 29.92%, 19.33%, and 20.89% in DISTS BD-Rate on the Kodak, DIV2K, and CLIC2020 datasets, respectively.</li>
<li><strong>摘要：</strong>虽然最近的神经编解码器在针对感知质量进行优化时在低比特率下实现了强大的性能，但它们的有效性在超低比特率条件下显着恶化。为了缓解这一问题，利用预训练模型的语义先验的生成压缩方法已成为一种有前景的范例。然而，现有的方法从根本上受到语义忠实度和感知现实主义之间的权衡的限制。基于显式表示的方法保留了内容结构，但通常缺乏细粒度的纹理，而隐式方法可以以语义漂移为代价合成视觉上合理的细节。在这项工作中，我们提出了一个统一的框架，通过以免训练的方式连贯地集成显式和隐式表示来弥合这一差距。具体来说，我们在显式高级语义上调节扩散模型，同时采用反向通道编码来隐式传达细粒度细节。此外，我们引入了一种插件编码器，可以通过调制隐式信息来灵活控制失真感知权衡。大量实验表明，所提出的框架实现了最先进的速率感知性能，优于现有方法，并在 Kodak、DIV2K 和 CLIC2020 数据集上的 DISTS BD-Rate 中分别超越 DiffC 29.92%、19.33% 和 20.89%。</li>
</ul>

<h3>Title: Disentangled Representation Learning via Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Jinjin Chi, Taoping Liu, Mengtao Yin, Ximing Li, Yongcheng Jing, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05214">https://arxiv.org/abs/2602.05214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05214">https://arxiv.org/pdf/2602.05214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05214]] Disentangled Representation Learning via Flow Matching(https://arxiv.org/abs/2602.05214)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Disentangled representation learning aims to capture the underlying explanatory factors of observed data, enabling a principled understanding of the data-generating process. Recent advances in generative modeling have introduced new paradigms for learning such representations. However, existing diffusion-based methods encourage factor independence via inductive biases, yet frequently lack strong semantic alignment. In this work, we propose a flow matching-based framework for disentangled representation learning, which casts disentanglement as learning factor-conditioned flows in a compact latent space. To enforce explicit semantic alignment, we introduce a non-overlap (orthogonality) regularizer that suppresses cross-factor interference and reduces information leakage between factors. Extensive experiments across multiple datasets demonstrate consistent improvements over representative baselines, yielding higher disentanglement scores as well as improved controllability and sample fidelity.</li>
<li><strong>摘要：</strong>解开表示学习旨在捕获观察数据的潜在解释因素，从而对数据生成过程有原则性的理解。生成建模的最新进展引入了学习此类表示的新范例。然而，现有的基于扩散的方法通过归纳偏差鼓励因素独立性，但经常缺乏强大的语义对齐。在这项工作中，我们提出了一种基于流匹配的解缠结表示学习框架，该框架将解缠结视为紧凑潜在空间中的学习因子条件流。为了强制执行显式语义对齐，我们引入了一种非重叠（正交）正则化器，可以抑制跨因素干扰并减少因素之间的信息泄漏。跨多个数据集的广泛实验表明，与代表性基线相比，得到了一致的改进，产生了更高的解缠结分数，并提高了可控性和样本保真度。</li>
</ul>

<h3>Title: Balanced Anomaly-guided Ego-graph Diffusion Model for Inductive Graph Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Chunyu Wei, Siyuan He, Yu Wang, Yueguo Chen, Yunhai Wang, Bing Bai, Yidong Zhang, Yong Xie, Shunming Zhang, Fei Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05232">https://arxiv.org/abs/2602.05232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05232">https://arxiv.org/pdf/2602.05232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05232]] Balanced Anomaly-guided Ego-graph Diffusion Model for Inductive Graph Anomaly Detection(https://arxiv.org/abs/2602.05232)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Graph anomaly detection (GAD) is crucial in applications like fraud detection and cybersecurity. Despite recent advancements using graph neural networks (GNNs), two major challenges persist. At the model level, most methods adopt a transductive learning paradigm, which assumes static graph structures, making them unsuitable for dynamic, evolving networks. At the data level, the extreme class imbalance, where anomalous nodes are rare, leads to biased models that fail to generalize to unseen anomalies. These challenges are interdependent: static transductive frameworks limit effective data augmentation, while imbalance exacerbates model distortion in inductive learning settings. To address these challenges, we propose a novel data-centric framework that integrates dynamic graph modeling with balanced anomaly synthesis. Our framework features: (1) a discrete ego-graph diffusion model, which captures the local topology of anomalies to generate ego-graphs aligned with anomalous structural distribution, and (2) a curriculum anomaly augmentation mechanism, which dynamically adjusts synthetic data generation during training, focusing on underrepresented anomaly patterns to improve detection and generalization. Experiments on five datasets demonstrate that the effectiveness of our framework.</li>
<li><strong>摘要：</strong>图异常检测 (GAD) 在欺诈检测和网络安全等应用中至关重要。尽管图神经网络（GNN）最近取得了进展，但仍然存在两个主要挑战。在模型层面，大多数方法采用转导学习范式，该范式假设静态图结构，这使得它们不适合动态的、不断发展的网络。在数据层面，极端的类别不平衡（异常节点很少见）会导致模型出现偏差，无法泛化到未见的异常。这些挑战是相互依赖的：静态转导框架限制了有效的数据增强，而不平衡加剧了归纳学习环境中的模型失真。为了应对这些挑战，我们提出了一种新颖的以数据为中心的框架，它将动态图建模与平衡异常合成相结合。我们的框架具有：（1）离散自我图扩散模型，捕获异常的局部拓扑，以生成与异常结构分布一致的自我图，以及（2）课程异常增强机制，在训练期间动态调整合成数据生成，重点关注代表性不足的异常模式，以改进检测和泛化。对五个数据集的实验证明了我们框架的有效性。</li>
</ul>

<h3>Title: TADS: Task-Aware Data Selection for Multi-Task Multimodal Pre-Training</h3>
<ul>
<li><strong>Authors: </strong>Guanjie Cheng, Boyi Li, Lingyu Sun, Mengying Zhu, Yangyang Wu, Xinkui Zhao, Shuiguang Deng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05251">https://arxiv.org/abs/2602.05251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05251">https://arxiv.org/pdf/2602.05251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05251]] TADS: Task-Aware Data Selection for Multi-Task Multimodal Pre-Training(https://arxiv.org/abs/2602.05251)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Large-scale multimodal pre-trained models like CLIP rely heavily on high-quality training data, yet raw web-crawled datasets are often noisy, misaligned, and redundant, leading to inefficient training and suboptimal generalization. Existing data selection methods are either heuristic-based, suffering from bias and limited diversity, or data-driven but task-agnostic, failing to optimize for multi-task scenarios. To address these gaps, we introduce TADS (Task-Aware Data Selection), a novel framework for multi-task multimodal pre-training that integrates Intrinsic Quality, Task Relevance, and Distributional Diversity into a learnable value function. TADS employs a comprehensive quality assessment system with unimodal and cross-modal operators, quantifies task relevance via interpretable similarity vectors, and optimizes diversity through cluster-based weighting. A feedback-driven meta-learning mechanism adaptively refines the selection strategy based on proxy model performance across multiple downstream tasks. Experiments on CC12M demonstrate that TADS achieves superior zero-shot performance on benchmarks like ImageNet, CIFAR-100, MS-COCO, and Flickr30K, using only 36% of the data while outperforming baselines by an average of 1.0%. This highlights that TADS significantly enhances data efficiency by curating a high-utility subset that yields a much higher performance ceiling within the same computational constraints.</li>
<li><strong>摘要：</strong>像 CLIP 这样的大规模多模态预训练模型在很大程度上依赖于高质量的训练数据，但原始的网络爬虫数据集通常充满噪音、错位和冗余，导致训练效率低下和泛化欠佳。现有的数据选择方法要么是基于启发式的，存在偏差和有限的多样性，要么是数据驱动的但与任务无关，无法针对多任务场景进行优化。为了解决这些差距，我们引入了 TADS（任务感知数据选择），这是一种用于多任务多模式预训练的新颖框架，它将内在质量、任务相关性和分布多样性集成到可学习的价值函数中。 TADS 采用具有单模态和跨模态算子的综合质量评估系统，通过可解释的相似性向量量化任务相关性，并通过基于聚类的加权优化多样性。反馈驱动的元学习机制根据跨多个下游任务的代理模型性能自适应地细化选择策略。 CC12M 上的实验表明，TADS 在 ImageNet、CIFAR-100、MS-COCO 和 Flickr30K 等基准测试中实现了卓越的零样本性能，仅使用 36% 的数据，同时平均优于基线 1.0%。这凸显了 TADS 通过管理一个高实用性子集来显着提高数据效率，该子集在相同的计算约束下产生更高的性能上限。</li>
</ul>

<h3>Title: RFM-Pose:Reinforcement-Guided Flow Matching for Fast Category-Level 6D Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Diya He, Qingchen Liu, Cong Zhang, Jiahu Qin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05257">https://arxiv.org/abs/2602.05257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05257">https://arxiv.org/pdf/2602.05257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05257]] RFM-Pose:Reinforcement-Guided Flow Matching for Fast Category-Level 6D Pose Estimation(https://arxiv.org/abs/2602.05257)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Object pose estimation is a fundamental problem in computer vision and plays a critical role in virtual reality and embodied intelligence, where agents must understand and interact with objects in 3D space. Recently, score based generative models have to some extent solved the rotational symmetry ambiguity problem in category level pose estimation, but their efficiency remains limited by the high sampling cost of score-based diffusion. In this work, we propose a new framework, RFM-Pose, that accelerates category-level 6D object pose generation while actively evaluating sampled hypotheses. To improve sampling efficiency, we adopt a flow-matching generative model and generate pose candidates along an optimal transport path from a simple prior to the pose distribution. To further refine these candidates, we cast the flow-matching sampling process as a Markov decision process and apply proximal policy optimization to fine-tune the sampling policy. In particular, we interpret the flow field as a learnable policy and map an estimator to a value network, enabling joint optimization of pose generation and hypothesis scoring within a reinforcement learning framework. Experiments on the REAL275 benchmark demonstrate that RFM-Pose achieves favorable performance while significantly reducing computational cost. Moreover, similar to prior work, our approach can be readily adapted to object pose tracking and attains competitive results in this setting.</li>
<li><strong>摘要：</strong>物体姿态估计是计算机视觉中的一个基本问题，在虚拟现实和体现智能中起着至关重要的作用，其中智能体必须理解 3D 空间中的物体并与之交互。最近，基于分数的生成模型在一定程度上解决了类别级姿态估计中的旋转对称模糊问题，但其效率仍然受到基于分数的扩散的高采样成本的限制。在这项工作中，我们提出了一个新的框架 RFM-Pose，它可以加速类别级 6D 对象姿势的生成，同时积极评估采样的假设。为了提高采样效率，我们采用流匹配生成模型，并从简单的先验姿势分布中沿着最佳传输路径生成候选姿势。为了进一步细化这些候选者，我们将流匹配采样过程转换为马尔可夫决策过程，并应用近端策略优化来微调采样策略。特别是，我们将流场解释为可学习的策略，并将估计器映射到价值网络，从而在强化学习框架内实现姿势生成和假设评分的联合优化。 REAL275 基准测试表明 RFM-Pose 实现了良好的性能，同时显着降低了计算成本。此外，与之前的工作类似，我们的方法可以很容易地适应对象姿态跟踪，并在此设置中获得有竞争力的结果。</li>
</ul>

<h3>Title: Magic-MM-Embedding: Towards Visual-Token-Efficient Universal Multimodal Embedding with MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Qi Li, Yanzhe Zhao, Yongxin Zhou, Yameng Wang, Yandong Yang, Yuanjia Zhou, Jue Wang, Zuojian Wang, Jinxiang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05275">https://arxiv.org/abs/2602.05275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05275">https://arxiv.org/pdf/2602.05275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05275]] Magic-MM-Embedding: Towards Visual-Token-Efficient Universal Multimodal Embedding with MLLMs(https://arxiv.org/abs/2602.05275)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have shown immense promise in universal multimodal retrieval, which aims to find relevant items of various modalities for a given query. But their practical application is often hindered by the substantial computational cost incurred from processing a large number of tokens from visual inputs. In this paper, we propose Magic-MM-Embedding, a series of novel models that achieve both high efficiency and state-of-the-art performance in universal multimodal embedding. Our approach is built on two synergistic pillars: (1) a highly efficient MLLM architecture incorporating visual token compression to drastically reduce inference latency and memory footprint, and (2) a multi-stage progressive training strategy designed to not only recover but significantly boost performance. This coarse-to-fine training paradigm begins with extensive continue pretraining to restore multimodal understanding and generation capabilities, progresses to large-scale contrastive pretraining and hard negative mining to enhance discriminative power, and culminates in a task-aware fine-tuning stage guided by an MLLM-as-a-Judge for precise data curation. Comprehensive experiments show that our model outperforms existing methods by a large margin while being more inference-efficient.</li>
<li><strong>摘要：</strong>多模态大语言模型（MLLM）在通用多模态检索方面显示出了巨大的前景，其目的是为给定的查询找到各种模态的相关项。但它们的实际应用常常受到处理来自视觉输入的大量标记所产生的大量计算成本的阻碍。在本文中，我们提出了 Magic-MM-Embedding，这是一系列新颖的模型，可在通用多模态嵌入中实现高效率和最先进的性能。我们的方法建立在两个协同支柱的基础上：(1) 高效的 MLLM 架构，结合了视觉令牌压缩，可大幅减少推理延迟和内存占用；(2) 多阶段渐进式训练策略，旨在不仅恢复性能，还能显着提升性能。这种从粗到精的训练范式从广泛的持续预训练开始，以恢复多模态理解和生成能力，然后进行大规模对比预训练和硬负挖掘，以增强判别力，最后达到由 MLLM 作为法官指导的任务感知微调阶段，以实现精确的数据管理。综合实验表明，我们的模型大幅优于现有方法，同时推理效率更高。</li>
</ul>

<h3>Title: Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities</h3>
<ul>
<li><strong>Authors: </strong>Pengyi Li, Elizaveta Goncharova, Andrey Kuznetsov, Ivan Oseledets</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05281">https://arxiv.org/abs/2602.05281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05281">https://arxiv.org/pdf/2602.05281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05281]] Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities(https://arxiv.org/abs/2602.05281)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispensable paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard policy optimization methods, such as Group Relative Policy Optimization (GRPO), often converge to low-entropy policies, leading to severe mode collapse and limited output diversity. We analyze this issue from the perspective of sampling probability dynamics, identifying that the standard objective disproportionately reinforces the highest-likelihood paths, thereby suppressing valid alternative reasoning chains. To address this, we propose a novel Advantage Re-weighting Mechanism (ARM) designed to equilibrate the confidence levels across all correct responses. By incorporating Prompt Perplexity and Answer Confidence into the advantage estimation, our method dynamically reshapes the reward signal to attenuate the gradient updates of over-confident reasoning paths, while redistributing probability mass toward under-explored correct solutions. Empirical results demonstrate that our approach significantly enhances generative diversity and response entropy while maintaining competitive accuracy, effectively achieving a superior trade-off between exploration and exploitation in reasoning tasks. Empirical results on Qwen2.5 and DeepSeek models across mathematical and coding benchmarks show that ProGRPO significantly mitigates entropy collapse. Specifically, on Qwen2.5-7B, our method outperforms GRPO by 5.7% in Pass@1 and, notably, by 13.9% in Pass@32, highlighting its superior capability in generating diverse correct reasoning paths.</li>
<li><strong>摘要：</strong>具有可验证奖励的强化学习（RLVR）已成为增强大型语言模型（LLM）推理的不可或缺的范例。然而，标准策略优化方法，例如组相对策略优化（GRPO），通常收敛于低熵策略，导致严重的模式崩溃和有限的输出多样性。我们从采样概率动态的角度分析这个问题，发现标准目标不成比例地强化了最高似然路径，从而抑制了有效的替代推理链。为了解决这个问题，我们提出了一种新颖的优势重新加权机制（ARM），旨在平衡所有正确响应的置信水平。通过将提示困惑度和答案置信度纳入优势估计中，我们的方法动态地重塑奖励信号，以减弱过度自信的推理路径的梯度更新，同时将概率质量重新分配给尚未探索的正确解决方案。实证结果表明，我们的方法显着增强了生成多样性和响应熵，同时保持了有竞争力的准确性，有效地实现了推理任务中探索和利用之间的优越权衡。 Qwen2.5 和 DeepSeek 模型在数学和编码基准上的实证结果表明，ProGRPO 显着减轻了熵崩溃。具体来说，在 Qwen2.5-7B 上，我们的方法在 Pass@1 中优于 GRPO 5.7%，尤其是在 Pass@32 中优于 GRPO 13.9%，凸显了其生成多种正确推理路径的卓越能力。</li>
</ul>

<h3>Title: Fast-SAM3D: 3Dfy Anything in Images but Faster</h3>
<ul>
<li><strong>Authors: </strong>Weilun Feng, Mingqiang Wu, Zhiliang Chen, Chuanguang Yang, Haotong Qin, Yuqi Li, Xiaokun Liu, Guoxin Fan, Zhulin An, Libo Huang, Yulun Zhang, Michele Magno, Yongjun Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05293">https://arxiv.org/abs/2602.05293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05293">https://arxiv.org/pdf/2602.05293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05293]] Fast-SAM3D: 3Dfy Anything in Images but Faster(https://arxiv.org/abs/2602.05293)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>SAM3D enables scalable, open-world 3D reconstruction from complex scenes, yet its deployment is hindered by prohibitive inference latency. In this work, we conduct the \textbf{first systematic investigation} into its inference dynamics, revealing that generic acceleration strategies are brittle in this context. We demonstrate that these failures stem from neglecting the pipeline's inherent multi-level \textbf{heterogeneity}: the kinematic distinctiveness between shape and layout, the intrinsic sparsity of texture refinement, and the spectral variance across geometries. To address this, we present \textbf{Fast-SAM3D}, a training-free framework that dynamically aligns computation with instantaneous generation complexity. Our approach integrates three heterogeneity-aware mechanisms: (1) \textit{Modality-Aware Step Caching} to decouple structural evolution from sensitive layout updates; (2) \textit{Joint Spatiotemporal Token Carving} to concentrate refinement on high-entropy regions; and (3) \textit{Spectral-Aware Token Aggregation} to adapt decoding resolution. Extensive experiments demonstrate that Fast-SAM3D delivers up to \textbf{2.67$\times$} end-to-end speedup with negligible fidelity loss, establishing a new Pareto frontier for efficient single-view 3D generation. Our code is released in this https URL.</li>
<li><strong>摘要：</strong>SAM3D 支持从复杂场景中进行可扩展的开放世界 3D 重建，但其部署却因推理延迟过高而受到阻碍。在这项工作中，我们对其推理动态进行了 \textbf{第一次系统研究}，揭示了通用加速策略在这种情况下是脆弱的。我们证明这些失败源于忽略管道固有的多级\textbf{异质性}：形状和布局之间的运动学独特性、纹理细化的内在稀疏性以及跨几何形状的光谱方差。为了解决这个问题，我们提出了 \textbf{Fast-SAM3D}，这是一个无需训练的框架，可以动态地将计算与瞬时生成复杂性保持一致。我们的方法集成了三种异质性感知机制：（1）\textit{模态感知步骤缓存}将结构演化与敏感布局更新分离； (2) \textit{联合时空令牌雕刻}，将细化集中在高熵区域； (3) \textit{频谱感知令牌聚合}以适应解码分辨率。大量实验表明，Fast-SAM3D 可提供高达 \textbf{2.67$\times$} 的端到端加速，而保真度损失可以忽略不计，为高效的单视图 3D 生成建立了新的帕累托前沿。我们的代码在此 https URL 中发布。</li>
</ul>

<h3>Title: FlashBlock: Attention Caching for Efficient Long-Context Block Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Zhuokun Chen, Jianfei Cai, Bohan Zhuang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05305">https://arxiv.org/abs/2602.05305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05305">https://arxiv.org/pdf/2602.05305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05305]] FlashBlock: Attention Caching for Efficient Long-Context Block Diffusion(https://arxiv.org/abs/2602.05305)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generating long-form content, such as minute-long videos and extended texts, is increasingly important for modern generative models. Block diffusion improves inference efficiency via KV caching and block-wise causal inference and has been widely adopted in diffusion language models and video generation. However, in long-context settings, block diffusion still incurs substantial overhead from repeatedly computing attention over a growing KV cache. We identify an underexplored property of block diffusion: cross-step redundancy of attention within a block. Our analysis shows that attention outputs from tokens outside the current block remain largely stable across diffusion steps, while block-internal attention varies significantly. Based on this observation, we propose FlashBlock, a cached block-external attention mechanism that reuses stable attention output, reducing attention computation and KV cache access without modifying the diffusion process. Moreover, FlashBlock is orthogonal to sparse attention and can be combined as a complementary residual reuse strategy, substantially improving model accuracy under aggressive sparsification. Experiments on diffusion language models and video generation demonstrate up to 1.44$\times$ higher token throughput and up to 1.6$\times$ reduction in attention time, with negligible impact on generation quality. Project page: this https URL.</li>
<li><strong>摘要：</strong>生成长格式内容，例如一分钟长的视频和扩展文本，对于现代生成模型越来越重要。块扩散通过 KV 缓存和块级因果推理提高推理效率，并已广泛应用于扩散语言模型和视频生成中。然而，在长上下文环境中，块扩散仍然会因在不断增长的 KV 缓存上重复计算注意力而产生大量开销。我们发现了块扩散的一个未被充分探索的属性：块内注意力的跨步骤冗余。我们的分析表明，当前区块外部代币的注意力输出在扩散步骤中基本保持稳定，而区块内部注意力则变化很大。基于这一观察，我们提出了FlashBlock，一种缓存的块外部注意力机制，它重用稳定的注意力输出，在不修改扩散过程的情况下减少注意力计算和KV缓存访问。此外，FlashBlock 与稀疏注意力正交，可以作为补充的残差重用策略组合起来，从而在激进的稀疏化下显着提高模型的准确性。扩散语言模型和视频生成的实验表明，令牌吞吐量提高了 1.44$\times$，注意力时间减少了 1.6$\times$，而对生成质量的影响可以忽略不计。项目页面：此 https URL。</li>
</ul>

<h3>Title: Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective</h3>
<ul>
<li><strong>Authors: </strong>Yinan Huang, Hans Hao-Hsun Hsu, Junran Wang, Bo Dai, Pan Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05319">https://arxiv.org/abs/2602.05319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05319">https://arxiv.org/pdf/2602.05319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05319]] Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective(https://arxiv.org/abs/2602.05319)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Sequential prediction from streaming observations is a fundamental problem in stochastic dynamical systems, where inherent uncertainty often leads to multiple plausible futures. While diffusion and flow-matching models are capable of modeling complex, multi-modal trajectories, their deployment in real-time streaming environments typically relies on repeated sampling from a non-informative initial distribution, incurring substantial inference latency and potential system backlogs. In this work, we introduce Sequential Flow Matching, a principled framework grounded in Bayesian filtering. By treating streaming inference as learning a probability flow that transports the predictive distribution from one time step to the next, our approach naturally aligns with the recursive structure of Bayesian belief updates. We provide theoretical justification that initializing generation from the previous posterior offers a principled warm start that can accelerate sampling compared to naïve re-sampling. Across a wide range of forecasting, decision-making and state estimation tasks, our method achieves performance competitive with full-step diffusion while requiring only one or very few sampling steps, therefore with faster sampling. It suggests that framing sequential inference via Bayesian filtering provides a new and principled perspective towards efficient real-time deployment of flow-based models.</li>
<li><strong>摘要：</strong>通过流观测进行顺序预测是随机动力系统中的一个基本问题，其中固有的不确定性通常会导致多个看似合理的未来。虽然扩散和流量匹配模型能够对复杂的多模态轨迹进行建模，但它们在实时流环境中的部署通常依赖于从无信息的初始分布中进行重复采样，从而导致大量的推理延迟和潜在的系统积压。在这项工作中，我们介绍了顺序流匹配，这是一个基于贝叶斯过滤的原则框架。通过将流推理视为学习一种将预测分布从一个时间步传输到下一个时间步的概率流，我们的方法自然地与贝叶斯信念更新的递归结构保持一致。我们提供了理论证明，从前一个后验中初始化生成提供了一个有原则的热启动，与简单的重采样相比，可以加速采样。在广泛的预测、决策和状态估计任务中，我们的方法实现了与全步扩散竞争的性能，同时只需要一个或很少的采样步骤，因此采样速度更快。它表明通过贝叶斯过滤构建顺序推理为基于流的模型的高效实时部署提供了新的原则性视角。</li>
</ul>

<h3>Title: GAS: Enhancing Reward-Cost Balance of Generative Model-assisted Offline Safe RL</h3>
<ul>
<li><strong>Authors: </strong>Zifan Liu, Xinran Li, Shibo Chen, Jun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05323">https://arxiv.org/abs/2602.05323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05323">https://arxiv.org/pdf/2602.05323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05323]] GAS: Enhancing Reward-Cost Balance of Generative Model-assisted Offline Safe RL(https://arxiv.org/abs/2602.05323)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Offline Safe Reinforcement Learning (OSRL) aims to learn a policy to achieve high performance in sequential decision-making while satisfying constraints, using only pre-collected datasets. Recent works, inspired by the strong capabilities of Generative Models (GMs), reformulate decision-making in OSRL as a conditional generative process, where GMs generate desirable actions conditioned on predefined reward and cost values. However, GM-assisted methods face two major challenges in OSRL: (1) lacking the ability to "stitch" optimal transitions from suboptimal trajectories within the dataset, and (2) struggling to balance reward targets with cost targets, particularly when they are conflict. To address these issues, we propose Goal-Assisted Stitching (GAS), a novel algorithm designed to enhance stitching capabilities while effectively balancing reward maximization and constraint satisfaction. To enhance the stitching ability, GAS first augments and relabels the dataset at the transition level, enabling the construction of high-quality trajectories from suboptimal ones. GAS also introduces novel goal functions, which estimate the optimal achievable reward and cost goals from the dataset. These goal functions, trained using expectile regression on the relabeled and augmented dataset, allow GAS to accommodate a broader range of reward-cost return pairs and achieve a better tradeoff between reward maximization and constraint satisfaction compared to human-specified values. The estimated goals then guide policy training, ensuring robust performance under constrained settings. Furthermore, to improve training stability and efficiency, we reshape the dataset to achieve a more uniform reward-cost return distribution. Empirical results validate the effectiveness of GAS, demonstrating superior performance in balancing reward maximization and constraint satisfaction compared to existing methods.</li>
<li><strong>摘要：</strong>离线安全强化学习（OSRL）旨在仅使用预先收集的数据集来学习一种策略，以在满足约束的同时在顺序决策中实现高性能。最近的工作受到生成模型 (GM) 强大功能的启发，将 OSRL 中的决策重新表述为条件生成过程，其中 GM 根据预定义的奖励和成本值生成所需的操作。然而，GM 辅助方法在 OSRL 中面临两个主要挑战：（1）缺乏从数据集中的次优轨迹“缝合”最优转换的能力，以及（2）难以平衡奖励目标与成本目标，特别是当它们发生冲突时。为了解决这些问题，我们提出了目标辅助缝合（GAS），这是一种新颖的算法，旨在增强缝合能力，同时有效平衡奖励最大化和约束满足。为了增强拼接能力，GAS 首先在过渡级别对数据集进行增强和重新标记，从而能够从次优轨迹构建高质量轨迹。 GAS 还引入了新颖的目标函数，可从数据集中估计最佳可实现的奖励和成本目标。这些目标函数在重新标记和增强的数据集上使用期望回归进行训练，使 GAS 能够适应更广泛的奖励成本回报对，并与人类指定的值相比，在奖励最大化和约束满足之间实现更好的权衡。然后，估计的目标指导政策培训，确保在受限环境下的稳健绩效。此外，为了提高训练稳定性和效率，我们重塑数据集以实现更均匀的奖励成本回报分布。实证结果验证了 GAS 的有效性，证明了与现有方法相比，GAS 在平衡奖励最大化和约束满足方面具有优越的性能。</li>
</ul>

<h3>Title: Consistency-Preserving Concept Erasure via Unsafe-Safe Pairing and Directional Fisher-weighted Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Yongwoo Kim, Sungmin Cha, Hyunsoo Kim, Jaewon Lee, Donghyun Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05339">https://arxiv.org/abs/2602.05339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05339">https://arxiv.org/pdf/2602.05339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05339]] Consistency-Preserving Concept Erasure via Unsafe-Safe Pairing and Directional Fisher-weighted Adaptation(https://arxiv.org/abs/2602.05339)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>With the increasing versatility of text-to-image diffusion models, the ability to selectively erase undesirable concepts (e.g., harmful content) has become indispensable. However, existing concept erasure approaches primarily focus on removing unsafe concepts without providing guidance toward corresponding safe alternatives, which often leads to failure in preserving the structural and semantic consistency between the original and erased generations. In this paper, we propose a novel framework, PAIRed Erasing (PAIR), which reframes concept erasure from simple removal to consistency-preserving semantic realignment using unsafe-safe pairs. We first generate safe counterparts from unsafe inputs while preserving structural and semantic fidelity, forming paired unsafe-safe multimodal data. Leveraging these pairs, we introduce two key components: (1) Paired Semantic Realignment, a guided objective that uses unsafe-safe pairs to explicitly map target concepts to semantically aligned safe anchors; and (2) Fisher-weighted Initialization for DoRA, which initializes parameter-efficient low-rank adaptation matrices using unsafe-safe pairs, encouraging the generation of safe alternatives while selectively suppressing unsafe concepts. Together, these components enable fine-grained erasure that removes only the targeted concepts while maintaining overall semantic consistency. Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art baselines, achieving effective concept erasure while preserving structural integrity, semantic coherence, and generation quality.</li>
<li><strong>摘要：</strong>随着文本到图像扩散模型的多功能性不断增加，选择性删除不需要的概念（例如有害内容）的能力已变得不可或缺。然而，现有的概念擦除方法主要侧重于删除不安全的概念，而不提供相应的安全替代方案的指导，这往往导致无法保持原始代和被擦除代之间的结构和语义一致性。在本文中，我们提出了一种新颖的框架，即成对擦除（PAIR），它将概念擦除从简单的删除重新定义为使用不安全-安全对来保持一致性的语义重新对齐。我们首先从不安全输入生成安全对应项，同时保留结构和语义保真度，形成配对的不安全-安全多模态数据。利用这些对，我们引入了两个关键组件：（1）配对语义重新对齐，这是一个引导目标，使用不安全-安全对来显式地将目标概念映射到语义对齐的安全锚； (2) DoRA 的 Fisher 加权初始化，它使用不安全-安全对初始化参数有效的低秩适应矩阵，鼓励安全替代方案的生成，同时选择性地抑制不安全概念。这些组件共同实现了细粒度擦除，仅删除目标概念，同时保持整体语义一致性。大量的实验表明，我们的方法明显优于最先进的基线，实现了有效的概念擦除，同时保持了结构完整性、语义一致性和生成质量。</li>
</ul>

<h3>Title: Multimodal Latent Reasoning via Hierarchical Visual Cues Injection</h3>
<ul>
<li><strong>Authors: </strong>Yiming Zhang, Qiangyu Yan, Borui Jiang, Kai Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05359">https://arxiv.org/abs/2602.05359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05359">https://arxiv.org/pdf/2602.05359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05359]] Multimodal Latent Reasoning via Hierarchical Visual Cues Injection(https://arxiv.org/abs/2602.05359)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The advancement of multimodal large language models (MLLMs) has enabled impressive perception capabilities. However, their reasoning process often remains a "fast thinking" paradigm, reliant on end-to-end generation or explicit, language-centric chains of thought (CoT), which can be inefficient, verbose, and prone to hallucination. This work posits that robust reasoning should evolve within a latent space, integrating multimodal signals seamlessly. We propose multimodal latent reasoning via HIerarchical Visual cuEs injection (\emph{HIVE}), a novel framework that instills deliberate, "slow thinking" without depending on superficial textual rationales. Our method recursively extends transformer blocks, creating an internal loop for iterative reasoning refinement. Crucially, it injectively grounds this process with hierarchical visual cues from global scene context to fine-grained regional details directly into the model's latent representations. This enables the model to perform grounded, multi-step inference entirely in the aligned latent space. Extensive evaluations demonstrate that test-time scaling is effective when incorporating vision knowledge, and that integrating hierarchical information significantly enhances the model's understanding of complex scenes.</li>
<li><strong>摘要：</strong>多模态大语言模型（MLLM）的进步实现了令人印象深刻的感知能力。然而，他们的推理过程通常仍然是“快速思维”范式，依赖于端到端生成或明确的、以语言为中心的思维链（CoT），这可能效率低下、冗长且容易产生幻觉。这项工作假设稳健的推理应该在潜在空间内发展，无缝集成多模态信号。我们通过分层视觉cuEs注入（\emph{HIVE}）提出多模式潜在推理，这是一种新颖的框架，可以灌输深思熟虑的“慢速思维”，而不依赖于肤浅的文本原理。我们的方法递归地扩展变压器块，创建一个用于迭代推理细化的内部循环。至关重要的是，它将这一过程与从全局场景上下文到细粒度区域细节的分层视觉线索直接注入到模型的潜在表示中。这使得模型能够完全在对齐的潜在空间中执行扎根的多步骤推理。广泛的评估表明，在合并视觉知识时，测试时间缩放是有效的，并且集成分层信息可以显着增强模型对复杂场景的理解。</li>
</ul>

<h3>Title: Imagine a City: CityGenAgent for Procedural 3D City Generation</h3>
<ul>
<li><strong>Authors: </strong>Zishan Liu, Zecong Tang, RuoCheng Wu, Xinzhe Zheng, Jingyu Hu, Ka-Hei Hui, Haoran Xie, Bo Dai, Zhengzhe Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05362">https://arxiv.org/abs/2602.05362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05362">https://arxiv.org/pdf/2602.05362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05362]] Imagine a City: CityGenAgent for Procedural 3D City Generation(https://arxiv.org/abs/2602.05362)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The automated generation of interactive 3D cities is a critical challenge with broad applications in autonomous driving, virtual reality, and embodied intelligence. While recent advances in generative models and procedural techniques have improved the realism of city generation, existing methods often struggle with high-fidelity asset creation, controllability, and manipulation. In this work, we introduce CityGenAgent, a natural language-driven framework for hierarchical procedural generation of high-quality 3D cities. Our approach decomposes city generation into two interpretable components, Block Program and Building Program. To ensure structural correctness and semantic alignment, we adopt a two-stage learning strategy: (1) Supervised Fine-Tuning (SFT). We train BlockGen and BuildingGen to generate valid programs that adhere to schema constraints, including non-self-intersecting polygons and complete fields; (2) Reinforcement Learning (RL). We design Spatial Alignment Reward to enhance spatial reasoning ability and Visual Consistency Reward to bridge the gap between textual descriptions and the visual modality. Benefiting from the programs and the models' generalization, CityGenAgent supports natural language editing and manipulation. Comprehensive evaluations demonstrate superior semantic alignment, visual quality, and controllability compared to existing methods, establishing a robust foundation for scalable 3D city generation.</li>
<li><strong>摘要：</strong>交互式 3D 城市的自动生成是自动驾驶、虚拟现实和实体智能领域广泛应用的一项严峻挑战。虽然生成模型和程序技术的最新进展提高了城市生成的现实性，但现有方法往往难以实现高保真资产创建、可控性和操纵。在这项工作中，我们介绍了 CityGenAgent，这是一种自然语言驱动的框架，用于分层程序生成高质量 3D 城市。我们的方法将城市生成分解为两个可解释的部分：街区计划和建筑计划。为了确保结构正确性和语义对齐，我们采用两阶段学习策略：（1）监督微调（SFT）。我们训练 BlockGen 和 BuildingGen 来生成符合模式约束的有效程序，包括非自相交多边形和完整字段； (2)强化学习(RL)。我们设计空间对齐奖励来增强空间推理能力，设计视觉一致性奖励来弥合文本描述和视觉形态之间的差距。受益于程序和模型的泛化，CityGenAgent 支持自然语言编辑和操作。与现有方法相比，综合评估显示出卓越的语义对齐、视觉质量和可控性，为可扩展的 3D 城市生成奠定了坚实的基础。</li>
</ul>

<h3>Title: Parallel Swin Transformer-Enhanced 3D MRI-to-CT Synthesis for MRI-Only Radiotherapy Planning</h3>
<ul>
<li><strong>Authors: </strong>Zolnamar Dorjsembe, Hung-Yi Chen, Furen Xiao, Hsing-Kuo Pao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05387">https://arxiv.org/abs/2602.05387</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05387">https://arxiv.org/pdf/2602.05387</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05387]] Parallel Swin Transformer-Enhanced 3D MRI-to-CT Synthesis for MRI-Only Radiotherapy Planning(https://arxiv.org/abs/2602.05387)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>MRI provides superior soft tissue contrast without ionizing radiation; however, the absence of electron density information limits its direct use for dose calculation. As a result, current radiotherapy workflows rely on combined MRI and CT acquisitions, increasing registration uncertainty and procedural complexity. Synthetic CT generation enables MRI only planning but remains challenging due to nonlinear MRI-CT relationships and anatomical variability. We propose Parallel Swin Transformer-Enhanced Med2Transformer, a 3D architecture that integrates convolutional encoding with dual Swin Transformer branches to model both local anatomical detail and long-range contextual dependencies. Multi-scale shifted window attention with hierarchical feature aggregation improves anatomical fidelity. Experiments on public and clinical datasets demonstrate higher image similarity and improved geometric accuracy compared with baseline methods. Dosimetric evaluation shows clinically acceptable performance, with a mean target dose error of 1.69%. Code is available at: this https URL.</li>
<li><strong>摘要：</strong>MRI 可提供卓越的软组织对比度，无需电离辐射；然而，电子密度信息的缺乏限制了其直接用于剂量计算。因此，当前的放射治疗工作流程依赖于 MRI 和 CT 的组合采集，增加了配准的不确定性和程序的复杂性。合成 CT 生成仅支持 MRI 规划，但由于非线性 MRI-CT 关系和解剖变异性，仍然具有挑战性。我们提出了 Parallel Swin Transformer-Enhanced Med2Transformer，这是一种 3D 架构，它将卷积编码与双 Swin Transformer 分支集成在一起，以对局部解剖细节和远程上下文依赖性进行建模。具有分层特征聚合的多尺度转移窗口注意力提高了解剖保真度。公共和临床数据集上的实验表明，与基线方法相比，图像相似度更高，几何精度更高。剂量测定评估显示临床可接受的性能，平均目标剂量误差为 1.69%。代码可在以下位置获得：此 https URL。</li>
</ul>

<h3>Title: LD-SLRO: Latent Diffusion Structured Light for 3-D Reconstruction of Highly Reflective Objects</h3>
<ul>
<li><strong>Authors: </strong>Sanghoon Jeon, Gihyun Jung, Suhyeon Ka, Jae-Sang Hyun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05434">https://arxiv.org/abs/2602.05434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05434">https://arxiv.org/pdf/2602.05434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05434]] LD-SLRO: Latent Diffusion Structured Light for 3-D Reconstruction of Highly Reflective Objects(https://arxiv.org/abs/2602.05434)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Fringe projection profilometry-based 3-D reconstruction of objects with high reflectivity and low surface roughness remains a significant challenge. When measuring such glossy surfaces, specular reflection and indirect illumination often lead to severe distortion or loss of the projected fringe patterns. To address these issues, we propose a latent diffusion-based structured light for reflective objects (LD-SLRO). Phase-shifted fringe images captured from highly reflective surfaces are first encoded to extract latent representations that capture surface reflectance characteristics. These latent features are then used as conditional inputs to a latent diffusion model, which probabilistically suppresses reflection-induced artifacts and recover lost fringe information, yielding high-quality fringe images. The proposed components, including the specular reflection encoder, time-variant channel affine layer, and attention modules, further improve fringe restoration quality. In addition, LD-SLRO provides high flexibility in configuring the input and output fringe sets. Experimental results demonstrate that the proposed method improves both fringe quality and 3-D reconstruction accuracy over state-of-the-art methods, reducing the average root-mean-squared error from 1.8176 mm to 0.9619 mm.</li>
<li><strong>摘要：</strong>基于条纹投影轮廓仪的高反射率和低表面粗糙度物体的 3D 重建仍然是一个重大挑战。在测量此类光泽表面时，镜面反射和间接照明通常会导致投影条纹图案严重变形或丢失。为了解决这些问题，我们提出了一种用于反射物体的基于潜在扩散的结构光（LD-SLRO）。首先对从高反射表面捕获的相移条纹图像进行编码，以提取捕获表面反射特性的潜在表示。然后将这些潜在特征用作潜在扩散模型的条件输入，该模型可以概率性地抑制反射引起的伪影并恢复丢失的条纹信息，从而产生高质量的条纹图像。所提出的组件，包括镜面反射编码器、时变通道仿射层和注意模块，进一步提高了条纹恢复质量。此外，LD-SLRO 在配置输入和输出边缘集方面提供了高度灵活性。实验结果表明，与最先进的方法相比，所提出的方法提高了条纹质量和 3D 重建精度，将平均均方根误差从 1.8176 mm 降低到 0.9619 mm。</li>
</ul>

<h3>Title: Synthetic Defect Geometries of Cast Metal Objects Modeled via 2d Voronoi Tessellations</h3>
<ul>
<li><strong>Authors: </strong>Natascha Jeziorski, Petra Gospodnetić, Claudia Redenbach</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05440">https://arxiv.org/abs/2602.05440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05440">https://arxiv.org/pdf/2602.05440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05440]] Synthetic Defect Geometries of Cast Metal Objects Modeled via 2d Voronoi Tessellations(https://arxiv.org/abs/2602.05440)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In industry, defect detection is crucial for quality control. Non-destructive testing (NDT) methods are preferred as they do not influence the functionality of the object while inspecting. Automated data evaluation for automated defect detection is a growing field of research. In particular, machine learning approaches show promising results. To provide training data in sufficient amount and quality, synthetic data can be used. Rule-based approaches enable synthetic data generation in a controllable environment. Therefore, a digital twin of the inspected object including synthetic defects is needed. We present parametric methods to model 3d mesh objects of various defect types that can then be added to the object geometry to obtain synthetic defective objects. The models are motivated by common defects in metal casting but can be transferred to other machining procedures that produce similar defect shapes. Synthetic data resembling the real inspection data can then be created by using a physically based Monte Carlo simulation of the respective testing method. Using our defect models, a variable and arbitrarily large synthetic data set can be generated with the possibility to include rarely occurring defects in sufficient quantity. Pixel-perfect annotation can be created in parallel. As an example, we will use visual surface inspection, but the procedure can be applied in combination with simulations for any other NDT method.</li>
<li><strong>摘要：</strong>在工业中，缺陷检测对于质量控制至关重要。首选无损检测 (NDT) 方法，因为它们在检查时不会影响物体的功能。用于自动缺陷检测的自动数据评估是一个不断发展的研究领域。特别是，机器学习方法显示出有希望的结果。为了提供足够数量和质量的训练数据，可以使用合成数据。基于规则的方法可以在可控环境中生成合成数据。因此，需要包含合成缺陷的被检物体的数字孪生。我们提出参数化方法来对各种缺陷类型的 3D 网格对象进行建模，然后将其添加到对象几何体中以获得合成缺陷对象。这些模型是由金属铸造中的常见缺陷激发的，但可以转移到产生类似缺陷形状的其他加工程序。然后可以通过使用相应测试方法的基于物理的蒙特卡罗模拟来创建类似于真实检查数据的合成数据。使用我们的缺陷模型，可以生成可变且任意大的合成数据集，并且可以包含足够数量的罕见缺陷。可以并行创建像素完美的注释。例如，我们将使用视觉表面检查，但该程序可以与任何其他 NDT 方法的模拟结合应用。</li>
</ul>

<h3>Title: BLITZRANK: Principled Zero-shot Ranking Agents with Tournament Graphs</h3>
<ul>
<li><strong>Authors: </strong>Sheshansh Agrawal, Thien Hang Nguyen, Douwe Kiela</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05448">https://arxiv.org/abs/2602.05448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05448">https://arxiv.org/pdf/2602.05448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05448]] BLITZRANK: Principled Zero-shot Ranking Agents with Tournament Graphs(https://arxiv.org/abs/2602.05448)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models have emerged as powerful zero-shot rerankers for retrieval-augmented generation, offering strong generalization without task-specific training. However, existing LLM reranking methods either rely on heuristics that fail to fully exploit the information revealed by each ranking decision or are inefficient when they do. We introduce a tournament graph framework that provides a principled foundation for $k$-wise reranking. Our key observation is that each $k$-document comparison reveals a complete tournament of $\binom{k}{2}$ pairwise preferences. These tournaments are aggregated into a global preference graph, whose transitive closure yields many additional orderings without further model invocations. We formalize when a candidate's rank is certifiably determined and design a query schedule that greedily maximizes information gain towards identifying the top-$m$ items. Our framework also gracefully handles non-transitive preferences - cycles induced by LLM judgments - by collapsing them into equivalence classes that yield principled tiered rankings. Empirically, across 14 benchmarks and 5 LLMs, our method achieves Pareto dominance over existing methods: matching or exceeding accuracy while requiring 25-40% fewer tokens than comparable approaches, and 7$\times$ fewer than pairwise methods at near-identical quality.</li>
<li><strong>摘要：</strong>大型语言模型已经成为强大的零样本重排序器，用于检索增强生成，无需特定任务的训练即可提供强大的泛化能力。然而，现有的法学硕士重新排名方法要么依赖于启发式方法，而无法充分利用每个排名决策所揭示的信息，要么在充分利用每个排名决策所揭示的信息时效率低下。我们引入了一个锦标赛图框架，为$k$明智的重新排名提供了原则基础。我们的主要观察结果是，每个 $k$ 文档比较揭示了 $\binom{k}{2}$ 成对偏好的完整锦标赛。这些锦标赛被聚合成一个全局偏好图，其传递闭包产生许多额外的排序，而无需进一步的模型调用。我们正式确定候选人的排名，并设计一个查询计划，贪婪地最大化信息增益，以识别前 $m$ 项。我们的框架还优雅地处理非传递性偏好（由 LLM 判断引起的循环），方法是将它们折叠成产生有原则的分层排名的等价类。根据经验，在 14 个基准和 5 个法学硕士中，我们的方法相对于现有方法实现了帕累托优势：匹配或超过准确性，同时比同类方法需要的标记少 25-40%，并且在几乎相同的质量下比成对方法少 7 倍。</li>
</ul>

<h3>Title: DisCa: Accelerating Video Diffusion Transformers with Distillation-Compatible Learnable Feature Caching</h3>
<ul>
<li><strong>Authors: </strong>Chang Zou, Changlin Li, Yang Li, Patrol Li, Jianbing Wu, Xiao He, Songtao Liu, Zhao Zhong, Kailin Huang, Linfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05449">https://arxiv.org/abs/2602.05449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05449">https://arxiv.org/pdf/2602.05449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05449]] DisCa: Accelerating Video Diffusion Transformers with Distillation-Compatible Learnable Feature Caching(https://arxiv.org/abs/2602.05449)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While diffusion models have achieved great success in the field of video generation, this progress is accompanied by a rapidly escalating computational burden. Among the existing acceleration methods, Feature Caching is popular due to its training-free property and considerable speedup performance, but it inevitably faces semantic and detail drop with further compression. Another widely adopted method, training-aware step-distillation, though successful in image generation, also faces drastic degradation in video generation with a few steps. Furthermore, the quality loss becomes more severe when simply applying training-free feature caching to the step-distilled models, due to the sparser sampling steps. This paper novelly introduces a distillation-compatible learnable feature caching mechanism for the first time. We employ a lightweight learnable neural predictor instead of traditional training-free heuristics for diffusion models, enabling a more accurate capture of the high-dimensional feature evolution process. Furthermore, we explore the challenges of highly compressed distillation on large-scale video models and propose a conservative Restricted MeanFlow approach to achieve more stable and lossless distillation. By undertaking these initiatives, we further push the acceleration boundaries to $11.8\times$ while preserving generation quality. Extensive experiments demonstrate the effectiveness of our method. The code is in the supplementary materials and will be publicly available.</li>
<li><strong>摘要：</strong>虽然扩散模型在视频生成领域取得了巨大成功，但这种进步伴随着计算负担的迅速增加。在现有的加速方法中，特征缓存因其免训练的特性和可观的加速性能而受到欢迎，但随着进一步压缩，它不可避免地面临语义和细节的下降。另一种广泛采用的方法是训练感知的逐步蒸馏，尽管在图像生成方面取得了成功，但在视频生成方面也面临着几个步骤的严重退化。此外，当简单地将免训练特征缓存应用于逐步蒸馏模型时，由于采样步骤稀疏，质量损失变得更加严重。本文首次新颖地引入了一种与蒸馏兼容的可学习特征缓存机制。我们采用轻量级可学习神经预测器来代替传统的免训练启发式扩散模型，从而能够更准确地捕获高维特征演化过程。此外，我们探索了大规模视频模型上高度压缩蒸馏的挑战，并提出了一种保守的受限平均流方法来实现更稳定和无损的蒸馏。通过采取这些举措，我们将加速边界进一步推至 11.8 美元\次$，同时保持发电质量。大量的实验证明了我们方法的有效性。该代码位于补充材料中，并将公开发布。</li>
</ul>

<h3>Title: SSG: Scaled Spatial Guidance for Multi-Scale Visual Autoregressive Generation</h3>
<ul>
<li><strong>Authors: </strong>Youngwoo Shin, Jiwan Hur, Junmo Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05534">https://arxiv.org/abs/2602.05534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05534">https://arxiv.org/pdf/2602.05534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05534]] SSG: Scaled Spatial Guidance for Multi-Scale Visual Autoregressive Generation(https://arxiv.org/abs/2602.05534)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Visual autoregressive (VAR) models generate images through next-scale prediction, naturally achieving coarse-to-fine, fast, high-fidelity synthesis mirroring human perception. In practice, this hierarchy can drift at inference time, as limited capacity and accumulated error cause the model to deviate from its coarse-to-fine nature. We revisit this limitation from an information-theoretic perspective and deduce that ensuring each scale contributes high-frequency content not explained by earlier scales mitigates the train-inference discrepancy. With this insight, we propose Scaled Spatial Guidance (SSG), training-free, inference-time guidance that steers generation toward the intended hierarchy while maintaining global coherence. SSG emphasizes target high-frequency signals, defined as the semantic residual, isolated from a coarser prior. To obtain this prior, we leverage a principled frequency-domain procedure, Discrete Spatial Enhancement (DSE), which is devised to sharpen and better isolate the semantic residual through frequency-aware construction. SSG applies broadly across VAR models leveraging discrete visual tokens, regardless of tokenization design or conditioning modality. Experiments demonstrate SSG yields consistent gains in fidelity and diversity while preserving low latency, revealing untapped efficiency in coarse-to-fine image generation. Code is available at this https URL.</li>
<li><strong>摘要：</strong>视觉自回归（VAR）模型通过下一尺度的预测生成图像，自然地实现从粗到细、快速、高保真的合成，反映人类的感知。在实践中，这种层次结构可能会在推理时发生漂移，因为有限的容量和累积的误差会导致模型偏离其从粗到细的性质。我们从信息论的角度重新审视这一限制，并推断确保每个尺度贡献早期尺度未解释的高频内容可以减轻训练推理差异。有了这种洞察力，我们提出了尺度空间指导（SSG），这是一种无需训练的推理时间指导，可以引导一代人朝着预期的层次结构发展，同时保持全局一致性。 SSG 强调目标高频信号，定义为语义残差，与较粗糙的先验隔离。为了获得这一先验，我们利用了原则性的频域过程，即离散空间增强（DSE），该过程旨在通过频率感知构造来锐化和更好地隔离语义残差。 SSG 广泛应用于利用离散视觉标记的 VAR 模型，无论标记化设计或调节模式如何。实验表明，SSG 在保持低延迟的同时，在保真度和多样性方面产生了一致的增益，揭示了从粗到细的图像生成中未开发的效率。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Detecting Misbehaviors of Large Vision-Language Models by Evidential Uncertainty Quantification</h3>
<ul>
<li><strong>Authors: </strong>Tao Huang, Rui Wang, Xiaofei Liu, Yi Qin, Li Duan, Liping Jing</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05535">https://arxiv.org/abs/2602.05535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05535">https://arxiv.org/pdf/2602.05535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05535]] Detecting Misbehaviors of Large Vision-Language Models by Evidential Uncertainty Quantification(https://arxiv.org/abs/2602.05535)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large vision-language models (LVLMs) have shown substantial advances in multimodal understanding and generation. However, when presented with incompetent or adversarial inputs, they frequently produce unreliable or even harmful content, such as fact hallucinations or dangerous instructions. This misalignment with human expectations, referred to as \emph{misbehaviors} of LVLMs, raises serious concerns for deployment in critical applications. These misbehaviors are found to stem from epistemic uncertainty, specifically either conflicting internal knowledge or the absence of supporting information. However, existing uncertainty quantification methods, which typically capture only overall epistemic uncertainty, have shown limited effectiveness in identifying such issues. To address this gap, we propose Evidential Uncertainty Quantification (EUQ), a fine-grained method that captures both information conflict and ignorance for effective detection of LVLM misbehaviors. In particular, we interpret features from the model output head as either supporting (positive) or opposing (negative) evidence. Leveraging Evidence Theory, we model and aggregate this evidence to quantify internal conflict and knowledge gaps within a single forward pass. We extensively evaluate our method across four categories of misbehavior, including hallucinations, jailbreaks, adversarial vulnerabilities, and out-of-distribution (OOD) failures, using state-of-the-art LVLMs, and find that EUQ consistently outperforms strong baselines, showing that hallucinations correspond to high internal conflict and OOD failures to high ignorance. Furthermore, layer-wise evidential uncertainty dynamics analysis helps interpret the evolution of internal representations from a new perspective. The source code is available at this https URL.</li>
<li><strong>摘要：</strong>大型视觉语言模型（LVLM）在多模态理解和生成方面取得了巨大进步。然而，当遇到不称职或对抗性的输入时，他们经常会产生不可靠甚至有害的内容，例如事实幻觉或危险的指令。这种与人类期望的不一致（称为 LVLM 的\emph{不当行为}）引起了对关键应用程序部署的严重担忧。这些不当行为被发现源于认知不确定性，特别是内部知识冲突或缺乏支持信息。然而，现有的不确定性量化方法通常仅捕获总体认知不确定性，在识别此类问题方面效果有限。为了解决这一差距，我们提出了证据不确定性量化（EUQ），这是一种细粒度的方法，可以捕获信息冲突和无知，从而有效检测 LVLM 不当行为。特别是，我们将模型输出头的特征解释为支持（正面）或反对（负面）证据。利用证据理论，我们对这些证据进行建模和聚合，以量化单个前向传递中的内部冲突和知识差距。我们使用最先进的 LVLM 对四类不当行为（包括幻觉、越狱、对抗性漏洞和分布外 (OOD) 失败）广泛评估我们的方法，并发现 EUQ 始终优于强基线，这表明幻觉对应于高度内部冲突，而 OOD 失败对应于高度无知。此外，分层证据不确定性动力学分析有助于从新的角度解释内部表征的演变。源代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Steering Large Reasoning Models towards Concise Reasoning via Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Yawei Li, Benjamin Bergner, Yinghan Zhao, Vihang Prakash Patil, Bei Chen, Cheng Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05539">https://arxiv.org/abs/2602.05539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05539">https://arxiv.org/pdf/2602.05539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05539]] Steering Large Reasoning Models towards Concise Reasoning via Flow Matching(https://arxiv.org/abs/2602.05539)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large Reasoning Models (LRMs) excel at complex reasoning tasks, but their efficiency is often hampered by overly verbose outputs. Prior steering methods attempt to address this issue by applying a single, global vector to hidden representations -- an approach grounded in the restrictive linear representation hypothesis. In this work, we introduce FlowSteer, a nonlinear steering method that goes beyond uniform linear shifts by learning a complete transformation between the distributions associated with verbose and concise reasoning. This transformation is learned via Flow Matching as a velocity field, enabling precise, input-dependent control over the model's reasoning process. By aligning steered representations with the distribution of concise-reasoning activations, FlowSteer yields more compact reasoning than the linear shifts. Across diverse reasoning benchmarks, FlowSteer demonstrates strong task performance and token efficiency compared to leading inference-time baselines. Our work demonstrates that modeling the full distributional transport with generative techniques offers a more effective and principled foundation for controlling LRMs.</li>
<li><strong>摘要：</strong>大型推理模型 (LRM) 擅长复杂的推理任务，但其效率往往因过于冗长的输出而受到阻碍。先前的引导方法试图通过将单个全局向量应用于隐藏表示来解决这个问题——这是一种基于限制性线性表示假设的方法。在这项工作中，我们介绍了 FlowSteer，这是一种非线性转向方法，它通过学习与详细推理和简洁推理相关的分布之间的完整转换，超越了均匀线性移位。这种转换是通过流匹配作为速度场来学习的，从而能够对模型的推理过程进行精确的、依赖于输入的控制。通过将引导表示与简洁推理激活的分布对齐，FlowSteer 产生比线性移位更紧凑的推理。在不同的推理基准中，与领先的推理时间基准相比，FlowSteer 表现出了强大的任务性能和令牌效率。我们的工作表明，使用生成技术对完整的分布式传输进行建模为控制 LRM 提供了更有效和更有原则的基础。</li>
</ul>

<h3>Title: Logical Guidance for the Exact Composition of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Francesco Alesiani, Jonathan Warrell, Tanja Bien, Henrik Christiansen, Matheus Ferraz, Mathias Niepert</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05549">https://arxiv.org/abs/2602.05549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05549">https://arxiv.org/pdf/2602.05549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05549]] Logical Guidance for the Exact Composition of Diffusion Models(https://arxiv.org/abs/2602.05549)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We propose LOGDIFF (Logical Guidance for the Exact Composition of Diffusion Models), a guidance framework for diffusion models that enables principled constrained generation with complex logical expressions at inference time. We study when exact score-based guidance for complex logical formulas can be obtained from guidance signals associated with atomic properties. First, we derive an exact Boolean calculus that provides a sufficient condition for exact logical guidance. Specifically, if a formula admits a circuit representation in which conjunctions combine conditionally independent subformulas and disjunctions combine subformulas that are either conditionally independent or mutually exclusive, exact logical guidance is achievable. In this case, the guidance signal can be computed exactly from atomic scores and posterior probabilities using an efficient recursive algorithm. Moreover, we show that, for commonly encountered classes of distributions, any desired Boolean formula is compilable into such a circuit representation. Second, by combining atomic guidance scores with posterior probability estimates, we introduce a hybrid guidance approach that bridges classifierguidance and classifier-free guidance, applicable to both compositional logical guidance and standard conditional generation. We demonstrate the effectiveness of our framework on multiple image and protein structure generation tasks.</li>
<li><strong>摘要：</strong>我们提出了 LOGDIFF（扩散模型精确组合的逻辑指导），这是一种扩散模型的指导框架，可以在推理时使用复杂的逻辑表达式进行有原则的约束生成。我们研究何时可以从与原子属性相关的指导信号中获得针对复杂逻辑公式的精确的基于分数的指导。首先，我们推导出精确的布尔演算，为精确的逻辑指导提供充分条件。具体来说，如果一个公式允许一种电路表示，其中连词组合条件独立的子公式，析取组合条件独立或互斥的子公式，则可以实现精确的逻辑指导。在这种情况下，可以使用高效的递归算法根据原子分数和后验概率准确地计算引导信号。此外，我们表明，对于常见的分布类别，任何所需的布尔公式都可以编译成这样的电路表示。其次，通过将原子指导分数与后验概率估计相结合，我们引入了一种混合指导方法，该方法将分类器指导和无分类器指导联系起来，适用于组合逻辑指导和标准条件生成。我们展示了我们的框架在多个图像和蛋白质结构生成任务上的有效性。</li>
</ul>

<h3>Title: A Hybrid CNN and ML Framework for Multi-modal Classification of Movement Disorders Using MRI and Brain Structural Features</h3>
<ul>
<li><strong>Authors: </strong>Mengyu Li, Ingibjörg Kristjánsdóttir, Thilo van Eimeren, Kathrin Giehl, Lotta M. Ellingsen, the ASAP Neuroimaging Initiative</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05574">https://arxiv.org/abs/2602.05574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05574">https://arxiv.org/pdf/2602.05574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05574]] A Hybrid CNN and ML Framework for Multi-modal Classification of Movement Disorders Using MRI and Brain Structural Features(https://arxiv.org/abs/2602.05574)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Atypical Parkinsonian Disorders (APD), also known as Parkinson-plus syndrome, are a group of neurodegenerative diseases that include progressive supranuclear palsy (PSP) and multiple system atrophy (MSA). In the early stages, overlapping clinical features often lead to misdiagnosis as Parkinson's disease (PD). Identifying reliable imaging biomarkers for early differential diagnosis remains a critical challenge. In this study, we propose a hybrid framework combining convolutional neural networks (CNNs) with machine learning (ML) techniques to classify APD subtypes versus PD and distinguish between the subtypes themselves: PSP vs. PD, MSA vs. PD, and PSP vs. MSA. The model leverages multi-modal input data, including T1-weighted magnetic resonance imaging (MRI), segmentation masks of 12 deep brain structures associated with APD, and their corresponding volumetric measurements. By integrating these complementary modalities, including image data, structural segmentation masks, and quantitative volume features, the hybrid approach achieved promising classification performance with area under the curve (AUC) scores of 0.95 for PSP vs. PD, 0.86 for MSA vs. PD, and 0.92 for PSP vs. MSA. These results highlight the potential of combining spatial and structural information for robust subtype differentiation. In conclusion, this study demonstrates that fusing CNN-based image features with volume-based ML inputs improves classification accuracy for APD subtypes. The proposed approach may contribute to more reliable early-stage diagnosis, facilitating timely and targeted interventions in clinical practice.</li>
<li><strong>摘要：</strong>非典型帕金森病 (APD)，也称为帕金森综合症，是一组神经退行性疾病，包括进行性核上性麻痹 (PSP) 和多系统萎缩 (MSA)。在早期阶段，重叠的临床特征常常导致误诊为帕金森病（PD）。识别用于早期鉴别诊断的可靠的成像生物标志物仍然是一个严峻的挑战。在本研究中，我们提出了一种将卷积神经网络 (CNN) 与机器学习 (ML) 技术相结合的混合框架，以对 APD 亚型与 PD 进行分类，并区分亚型本身：PSP 与 PD、MSA 与 PD 以及 PSP 与 MSA。该模型利用多模态输入数据，包括 T1 加权磁共振成像 (MRI)、与 APD 相关的 12 个深层大脑结构的分割掩模及其相应的体积测量。通过集成这些互补模式（包括图像数据、结构分割掩模和定量体积特征），混合方法实现了良好的分类性能，PSP 与 PD 的曲线下面积 (AUC) 得分为 0.95，MSA 与 PD 的得分为 0.86，PSP 与 MSA 的曲线下面积 (AUC) 得分为 0.92。这些结果凸显了结合空间和结构信息以实现稳健的亚型分化的潜力。总之，本研究表明，将基于 CNN 的图像特征与基于体积的 ML 输入融合可以提高 APD 子类型的分类准确性。所提出的方法可能有助于更可靠的早期诊断，促进临床实践中及时和有针对性的干预。</li>
</ul>

<h3>Title: Path-Guided Flow Matching for Dataset Distillation</h3>
<ul>
<li><strong>Authors: </strong>Xuhui Li, Zhengquan Luo, Xiwei Liu, Yongqiang Yu, Zhiqiang Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05616">https://arxiv.org/abs/2602.05616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05616">https://arxiv.org/pdf/2602.05616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05616]] Path-Guided Flow Matching for Dataset Distillation(https://arxiv.org/abs/2602.05616)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Dataset distillation compresses large datasets into compact synthetic sets with comparable performance in training models. Despite recent progress on diffusion-based distillation, this type of method typically depends on heuristic guidance or prototype assignment, which comes with time-consuming sampling and trajectory instability and thus hurts downstream generalization especially under strong control or low IPC. We propose \emph{Path-Guided Flow Matching (PGFM)}, the first flow matching-based framework for generative distillation, which enables fast deterministic synthesis by solving an ODE in a few steps. PGFM conducts flow matching in the latent space of a frozen VAE to learn class-conditional transport from Gaussian noise to data distribution. Particularly, we develop a continuous path-to-prototype guidance algorithm for ODE-consistent path control, which allows trajectories to reliably land on assigned prototypes while preserving diversity and efficiency. Extensive experiments across high-resolution benchmarks demonstrate that PGFM matches or surpasses prior diffusion-based distillation approaches with fewer steps of sampling while delivering competitive performance with remarkably improved efficiency, e.g., 7.6$\times$ more efficient than the diffusion-based counterparts with 78\% mode coverage.</li>
<li><strong>摘要：</strong>数据集蒸馏将大型数据集压缩为紧凑的合成集，在训练模型中具有可比的性能。尽管最近在基于扩散的蒸馏方面取得了进展，但这种方法通常依赖于启发式指导或原型分配，这会带来耗时的采样和轨迹不稳定，从而损害下游泛化，尤其是在强控制或低 IPC 的情况下。我们提出\emph{路径引导流匹配（PGFM）}，这是第一个基于流匹配的生成蒸馏框架，它通过几个步骤求解 ODE 来实现快速确定性合成。 PGFM 在冻结 VAE 的潜在空间中进行流匹配，以学习从高斯噪声到数据分布的类条件传输。特别是，我们开发了一种用于 ODE 一致路径控制的连续路径到原型引导算法，该算法允许轨迹可靠地降落在指定的原型上，同时保持多样性和效率。跨高分辨率基准的大量实验表明，PGFM 以更少的采样步骤匹配或超越了之前基于扩散的蒸馏方法，同时提供具有竞争力的性能和显着提高的效率，例如，比具有 78% 模式覆盖率的基于扩散的同行效率提高 7.6 倍。</li>
</ul>

<h3>Title: ROMAN: Reward-Orchestrated Multi-Head Attention Network for Autonomous Driving System Testing</h3>
<ul>
<li><strong>Authors: </strong>Jianlei Chi, Yuzhen Wu, Jiaxuan Hou, Xiaodong Zhang, Ming Fan, Suhui Sun, Weijun Dai, Bo Li, Jianguo Sun, Jun Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05629">https://arxiv.org/abs/2602.05629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05629">https://arxiv.org/pdf/2602.05629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05629]] ROMAN: Reward-Orchestrated Multi-Head Attention Network for Autonomous Driving System Testing(https://arxiv.org/abs/2602.05629)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Automated Driving System (ADS) acts as the brain of autonomous vehicles, responsible for their safety and efficiency. Safe deployment requires thorough testing in diverse real-world scenarios and compliance with traffic laws like speed limits, signal obedience, and right-of-way rules. Violations like running red lights or speeding pose severe safety risks. However, current testing approaches face significant challenges: limited ability to generate complex and high-risk law-breaking scenarios, and failing to account for complex interactions involving multiple vehicles and critical situations. To address these challenges, we propose ROMAN, a novel scenario generation approach for ADS testing that combines a multi-head attention network with a traffic law weighting mechanism. ROMAN is designed to generate high-risk violation scenarios to enable more thorough and targeted ADS evaluation. The multi-head attention mechanism models interactions among vehicles, traffic signals, and other factors. The traffic law weighting mechanism implements a workflow that leverages an LLM-based risk weighting module to evaluate violations based on the two dimensions of severity and occurrence. We have evaluated ROMAN by testing the Baidu Apollo ADS within the CARLA simulation platform and conducting extensive experiments to measure its performance. Experimental results demonstrate that ROMAN surpassed state-of-the-art tools ABLE and LawBreaker by achieving 7.91% higher average violation count than ABLE and 55.96% higher than LawBreaker, while also maintaining greater scenario diversity. In addition, only ROMAN successfully generated violation scenarios for every clause of the input traffic laws, enabling it to identify more high-risk violations than existing approaches.</li>
<li><strong>摘要：</strong>自动驾驶系统（ADS）充当自动驾驶汽车的大脑，负责其安全性和效率。安全部署需要在不同的现实场景中进行彻底的测试，并遵守速度限制、信号服从和通行权规则等交通法规。闯红灯、超速等违法行为构成严重的安全隐患。然而，当前的测试方法面临着重大挑战：生成复杂且高风险的违法场景的能力有限，并且无法考虑涉及多辆车和危急情况的复杂交互。为了应对这些挑战，我们提出了 ROMAN，一种用于 ADS 测试的新颖场景生成方法，它将多头注意力网络与交通法加权机制相结合。 ROMAN 旨在生成高风险违规场景，以实现更彻底、更有针对性的 ADS 评估。多头注意力机制模拟车辆、交通信号和其他因素之间的相互作用。交通法规权重机制实现了一个工作流程，利用基于法学硕士的风险权重模块，根据严重性和发生率两个维度来评估违规行为。我们通过在 CARLA 仿真平台上测试百度 Apollo ADS 并进行大量实验来衡量其性能来评估 ROMAN。实验结果表明，ROMAN 超越了最先进的工具 ABLE 和 LawBreaker，平均违规计数比 ABLE 高 7.91%，比 LawBreaker 高 55.96%，同时还保持了更大的场景多样性。此外，只有 ROMAN 成功地为输入交通法规的每个条款生成了违规场景，使其能够比现有方法识别更多的高风险违规行为。</li>
</ul>

<h3>Title: Probabilistic Multi-Regional Solar Power Forecasting with Any-Quantile Recurrent Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Slawek Smyl, Paweł Pełka, Grzegorz Dudek</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05660">https://arxiv.org/abs/2602.05660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05660">https://arxiv.org/pdf/2602.05660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05660]] Probabilistic Multi-Regional Solar Power Forecasting with Any-Quantile Recurrent Neural Networks(https://arxiv.org/abs/2602.05660)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The increasing penetration of photovoltaic (PV) generation introduces significant uncertainty into power system operation, necessitating forecasting approaches that extend beyond deterministic point predictions. This paper proposes an any-quantile probabilistic forecasting framework for multi-regional PV power generation based on the Any-Quantile Recurrent Neural Network (AQ-RNN). The model integrates an any-quantile forecasting paradigm with a dual-track recurrent architecture that jointly processes series-specific and cross-regional contextual information, supported by dilated recurrent cells, patch-based temporal modeling, and a dynamic ensemble mechanism. The proposed framework enables the estimation of calibrated conditional quantiles at arbitrary probability levels within a single trained model and effectively exploits spatial dependencies to enhance robustness at the system level. The approach is evaluated using 30 years of hourly PV generation data from 259 European regions and compared against established statistical and neural probabilistic baselines. The results demonstrate consistent improvements in forecast accuracy, calibration, and prediction interval quality, underscoring the suitability of the proposed method for uncertainty-aware energy management and operational decision-making in renewable-dominated power systems.</li>
<li><strong>摘要：</strong>光伏发电渗透率的不断提高给电力系统运行带来了巨大的不确定性，因此需要超越确定性点预测的预测方法。本文提出了一种基于任意分位数递归神经网络（AQ-RNN）的多区域光伏发电任意分位数概率预测框架。该模型将任意分位数预测范式与双轨循环架构相结合，联合处理系列特定和跨区域的上下文信息，并由扩张的循环单元、基于补丁的时间建模和动态集成机制支持。所提出的框架能够在单个训练模型内以任意概率水平估计校准条件分位数，并有效地利用空间依赖性来增强系统级别的鲁棒性。该方法使用来自 259 个欧洲地区 30 年的每小时光伏发电数据进行评估，并与既定的统计和神经概率基线进行比较。结果表明，预测精度、校准和预测区间质量得到了持续改进，强调了所提出的方法对于可再生能源主导的电力系统中不确定性感知能源管理和运营决策的适用性。</li>
</ul>

<h3>Title: ShapeUP: Scalable Image-Conditioned 3D Editing</h3>
<ul>
<li><strong>Authors: </strong>Inbar Gat, Dana Cohen-Bar, Guy Levy, Elad Richardson, Daniel Cohen-Or</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05676">https://arxiv.org/abs/2602.05676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05676">https://arxiv.org/pdf/2602.05676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05676]] ShapeUP: Scalable Image-Conditioned 3D Editing(https://arxiv.org/abs/2602.05676)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in 3D foundation models have enabled the generation of high-fidelity assets, yet precise 3D manipulation remains a significant challenge. Existing 3D editing frameworks often face a difficult trade-off between visual controllability, geometric consistency, and scalability. Specifically, optimization-based methods are prohibitively slow, multi-view 2D propagation techniques suffer from visual drift, and training-free latent manipulation methods are inherently bound by frozen priors and cannot directly benefit from scaling. In this work, we present ShapeUP, a scalable, image-conditioned 3D editing framework that formulates editing as a supervised latent-to-latent translation within a native 3D representation. This formulation allows ShapeUP to build on a pretrained 3D foundation model, leveraging its strong generative prior while adapting it to editing through supervised training. In practice, ShapeUP is trained on triplets consisting of a source 3D shape, an edited 2D image, and the corresponding edited 3D shape, and learns a direct mapping using a 3D Diffusion Transformer (DiT). This image-as-prompt approach enables fine-grained visual control over both local and global edits and achieves implicit, mask-free localization, while maintaining strict structural consistency with the original asset. Our extensive evaluations demonstrate that ShapeUP consistently outperforms current trained and training-free baselines in both identity preservation and edit fidelity, offering a robust and scalable paradigm for native 3D content creation.</li>
<li><strong>摘要：</strong>3D 基础模型的最新进展已经能够生成高保真资产，但精确的 3D 操作仍然是一个重大挑战。现有的 3D 编辑框架通常面临视觉可控性、几何一致性和可扩展性之间的艰难权衡。具体来说，基于优化的方法速度极其缓慢，多视图 2D 传播技术会受到视觉漂移的影响，而免训练的潜在操纵方法本质上受到冻结先验的限制，无法直接从缩放中受益。在这项工作中，我们提出了 ShapeUP，这是一个可扩展的、图像调节的 3D 编辑框架，它将编辑制定为原生 3D 表示中的有监督的潜在到潜在的转换。该公式允许 ShapeUP 在预训练的 3D 基础模型上构建，利用其强大的生成先验，同时通过监督训练使其适应编辑。在实践中，ShapeUP 在由源 3D 形状、编辑的 2D 图像和相应的编辑的 3D 形状组成的三元组上进行训练，并使用 3D 扩散变压器 (DiT) 学习直接映射。这种图像即提示方法可以对本地和全局编辑进行细粒度的视觉控制，并实现隐式、无掩模本地化，同时保持与原始资产的严格结构一致性。我们的广泛评估表明，ShapeUP 在身份保存和编辑保真度方面始终优于当前经过培训和免培训的基线，为原生 3D 内容创建提供了强大且可扩展的范例。</li>
</ul>

<h3>Title: FMPose3D: monocular 3D pose estimation via flow matching</h3>
<ul>
<li><strong>Authors: </strong>Ti Wang, Xiaohang Yu, Mackenzie Weygandt Mathis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05755">https://arxiv.org/abs/2602.05755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05755">https://arxiv.org/pdf/2602.05755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05755]] FMPose3D: monocular 3D pose estimation via flow matching(https://arxiv.org/abs/2602.05755)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Monocular 3D pose estimation is fundamentally ill-posed due to depth ambiguity and occlusions, thereby motivating probabilistic methods that generate multiple plausible 3D pose hypotheses. In particular, diffusion-based models have recently demonstrated strong performance, but their iterative denoising process typically requires many timesteps for each prediction, making inference computationally expensive. In contrast, we leverage Flow Matching (FM) to learn a velocity field defined by an Ordinary Differential Equation (ODE), enabling efficient generation of 3D pose samples with only a few integration steps. We propose a novel generative pose estimation framework, FMPose3D, that formulates 3D pose estimation as a conditional distribution transport problem. It continuously transports samples from a standard Gaussian prior to the distribution of plausible 3D poses conditioned only on 2D inputs. Although ODE trajectories are deterministic, FMPose3D naturally generates various pose hypotheses by sampling different noise seeds. To obtain a single accurate prediction from those hypotheses, we further introduce a Reprojection-based Posterior Expectation Aggregation (RPEA) module, which approximates the Bayesian posterior expectation over 3D hypotheses. FMPose3D surpasses existing methods on the widely used human pose estimation benchmarks Human3.6M and MPI-INF-3DHP, and further achieves state-of-the-art performance on the 3D animal pose datasets Animal3D and CtrlAni3D, demonstrating strong performance across both 3D pose domains. The code is available at this https URL.</li>
<li><strong>摘要：</strong>由于深度模糊和遮挡，单目 3D 姿态估计从根本上来说是不适定的，从而激发了生成多个看似合理的 3D 姿态假设的概率方法。特别是，基于扩散的模型最近表现出了强大的性能，但其迭代去噪过程通常需要每个预测许多时间步长，使得推理计算成本高昂。相比之下，我们利用流匹配 (FM) 来学习由常微分方程 (ODE) 定义的速度场，只需几个积分步骤即可高效生成 3D 位姿样本。我们提出了一种新颖的生成姿态估计框架 FMPose3D，它将 3D 姿态估计表述为条件分布传输问题。它在仅以 2D 输入为条件的合理 3D 位姿分布之前连续传输来自标准高斯的样本。尽管 ODE 轨迹是确定性的，但 FMPose3D 通过采样不同的噪声种子自然地生成各种姿态假设。为了从这些假设中获得单个准确的预测，我们进一步引入了基于重投影的后验期望聚合（RPEA）模块，该模块近似于 3D 假设的贝叶斯后验期望。 FMPose3D 在广泛使用的人体姿势估计基准 Human3.6M 和 MPI-INF-3DHP 上超越了现有方法，并进一步在 3D 动物姿势数据集 Animal3D 和 CtrlAni3D 上实现了最先进的性能，在两个 3D 姿势领域展示了强大的性能。该代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Principled Confidence Estimation for Deep Computed Tomography</h3>
<ul>
<li><strong>Authors: </strong>Matteo Gätzner, Johannes Kirschner</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05812">https://arxiv.org/abs/2602.05812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05812">https://arxiv.org/pdf/2602.05812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05812]] Principled Confidence Estimation for Deep Computed Tomography(https://arxiv.org/abs/2602.05812)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present a principled framework for confidence estimation in computed tomography (CT) reconstruction. Based on the sequential likelihood mixing framework (Kirschner et al., 2025), we establish confidence regions with theoretical coverage guarantees for deep-learning-based CT reconstructions. We consider a realistic forward model following the Beer-Lambert law, i.e., a log-linear forward model with Poisson noise, closely reflecting clinical and scientific imaging conditions. The framework is general and applies to both classical algorithms and deep learning reconstruction methods, including U-Nets, U-Net ensembles, and generative Diffusion models. Empirically, we demonstrate that deep reconstruction methods yield substantially tighter confidence regions than classical reconstructions, without sacrificing theoretical coverage guarantees. Our approach allows the detection of hallucinations in reconstructed images and provides interpretable visualizations of confidence regions. This establishes deep models not only as powerful estimators, but also as reliable tools for uncertainty-aware medical imaging.</li>
<li><strong>摘要：</strong>我们提出了计算机断层扫描（CT）重建中置信度估计的原则框架。基于顺序似然混合框架（Kirschner et al., 2025），我们为基于深度学习的 CT 重建建立了具有理论覆盖保证的置信区域。我们考虑遵循比尔-朗伯定律的现实正演模型，即具有泊松噪声的对数线性正演模型，密切反映临床和科学成像条件。该框架是通用的，适用于经典算法和深度学习重建方法，包括 U-Net、U-Net 集成和生成扩散模型。根据经验，我们证明深度重建方法产生比经典重建更严格的置信区域，而不牺牲理论覆盖保证。我们的方法可以检测重建图像中的幻觉，并提供可解释的置信区域可视化。这建立的深度模型不仅可以作为强大的估计器，而且可以作为不确定性感知医学成像的可靠工具。</li>
</ul>

<h3>Title: Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation</h3>
<ul>
<li><strong>Authors: </strong>Hai Zhang, Siqi Liang, Li Chen, Yuxian Li, Yukuan Xu, Yichao Zhong, Fu Zhang, Hongyang Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05827">https://arxiv.org/abs/2602.05827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05827">https://arxiv.org/pdf/2602.05827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05827]] Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation(https://arxiv.org/abs/2602.05827)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Why must vision-language navigation be bound to detailed and verbose language instructions? While such details ease decision-making, they fundamentally contradict the goal for navigation in the real-world. Ideally, agents should possess the autonomy to navigate in unknown environments guided solely by simple and high-level intents. Realizing this ambition introduces a formidable challenge: Beyond-the-View Navigation (BVN), where agents must locate distant, unseen targets without dense and step-by-step guidance. Existing large language model (LLM)-based methods, though adept at following dense instructions, often suffer from short-sighted behaviors due to their reliance on short-horimzon supervision. Simply extending the supervision horizon, however, destabilizes LLM training. In this work, we identify that video generation models inherently benefit from long-horizon supervision to align with language instructions, rendering them uniquely suitable for BVN tasks. Capitalizing on this insight, we propose introducing the video generation model into this field for the first time. Yet, the prohibitive latency for generating videos spanning tens of seconds makes real-world deployment impractical. To bridge this gap, we propose SparseVideoNav, achieving sub-second trajectory inference guided by a generated sparse future spanning a 20-second horizon. This yields a remarkable 27x speed-up compared to the unoptimized counterpart. Extensive real-world zero-shot experiments demonstrate that SparseVideoNav achieves 2.5x the success rate of state-of-the-art LLM baselines on BVN tasks and marks the first realization of such capability in challenging night scenes.</li>
<li><strong>摘要：</strong>为什么视觉语言导航必须与详细而冗长的语言指令绑定？虽然这些细节简化了决策，但它们从根本上与现实世界中的导航目标相矛盾。理想情况下，代理应该拥有在未知环境中仅在简单和高级意图的指导下导航的自主权。 Realizing this ambition introduces a formidable challenge: Beyond-the-View Navigation (BVN), where agents must locate distant, unseen targets without dense and step-by-step guidance. Existing large language model (LLM)-based methods, though adept at following dense instructions, often suffer from short-sighted behaviors due to their reliance on short-horimzon supervision.然而，简单地扩大监督范围会破坏法学硕士培训的稳定性。 In this work, we identify that video generation models inherently benefit from long-horizon supervision to align with language instructions, rendering them uniquely suitable for BVN tasks.利用这一见解，我们建议首次将视频生成模型引入该领域。然而，生成长达数十秒的视频的延迟令人望而却步，使得实际部署变得不切实际。 To bridge this gap, we propose SparseVideoNav, achieving sub-second trajectory inference guided by a generated sparse future spanning a 20-second horizon.与未优化的对应方案相比，这可实现 27 倍的显着加速。 Extensive real-world zero-shot experiments demonstrate that SparseVideoNav achieves 2.5x the success rate of state-of-the-art LLM baselines on BVN tasks and marks the first realization of such capability in challenging night scenes.</li>
</ul>

<h3>Title: Synthesizing Realistic Test Data without Breaking Privacy</h3>
<ul>
<li><strong>Authors: </strong>Laura Plein, Alexi Turcotte, Arina Hallemans, Andreas Zeller</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05833">https://arxiv.org/abs/2602.05833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05833">https://arxiv.org/pdf/2602.05833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05833]] Synthesizing Realistic Test Data without Breaking Privacy(https://arxiv.org/abs/2602.05833)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>There is a need for synthetic training and test datasets that replicate statistical distributions of original datasets without compromising their confidentiality. A lot of research has been done in leveraging Generative Adversarial Networks (GANs) for synthetic data generation. However, the resulting models are either not accurate enough or are still vulnerable to membership inference attacks (MIA) or dataset reconstruction attacks since the original data has been leveraged in the training process. In this paper, we explore the feasibility of producing a synthetic test dataset with the same statistical properties as the original one, with only indirectly leveraging the original data in the generation process. The approach is inspired by GANs, with a generation step and a discrimination step. However, in our approach, we use a test generator (a fuzzer) to produce test data from an input specification, preserving constraints set by the original data; a discriminator model determines how close we are to the original data. By evolving samples and determining "good samples" with the discriminator, we can generate privacy-preserving data that follows the same statistical distributions are the original dataset, leading to a similar utility as the original data. We evaluated our approach on four datasets that have been used to evaluate the state-of-the-art techniques. Our experiments highlight the potential of our approach towards generating synthetic datasets that have high utility while preserving privacy.</li>
<li><strong>摘要：</strong>需要合成训练和测试数据集来复制原始数据集的统计分布而不损害其机密性。在利用生成对抗网络（GAN）进行合成数据生成方面已经进行了大量研究。 However, the resulting models are either not accurate enough or are still vulnerable to membership inference attacks (MIA) or dataset reconstruction attacks since the original data has been leveraged in the training process. In this paper, we explore the feasibility of producing a synthetic test dataset with the same statistical properties as the original one, with only indirectly leveraging the original data in the generation process.该方法受到 GAN 的启发，具有生成步骤和判别步骤。 However, in our approach, we use a test generator (a fuzzer) to produce test data from an input specification, preserving constraints set by the original data; a discriminator model determines how close we are to the original data. By evolving samples and determining "good samples" with the discriminator, we can generate privacy-preserving data that follows the same statistical distributions are the original dataset, leading to a similar utility as the original data. We evaluated our approach on four datasets that have been used to evaluate the state-of-the-art techniques. Our experiments highlight the potential of our approach towards generating synthetic datasets that have high utility while preserving privacy.</li>
</ul>

<h3>Title: Pathwise Test-Time Correction for Autoregressive Long Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Xunzhi Xiang, Zixuan Duan, Guiyu Zhang, Haiyu Zhang, Zhe Gao, Junta Wu, Shaofeng Zhang, Tengfei Wang, Qi Fan, Chunchao Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05871">https://arxiv.org/abs/2602.05871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05871">https://arxiv.org/pdf/2602.05871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05871]] Pathwise Test-Time Correction for Autoregressive Long Video Generation(https://arxiv.org/abs/2602.05871)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Distilled autoregressive diffusion models facilitate real-time short video synthesis but suffer from severe error accumulation during long-sequence generation. While existing Test-Time Optimization (TTO) methods prove effective for images or short clips, we identify that they fail to mitigate drift in extended sequences due to unstable reward landscapes and the hypersensitivity of distilled parameters. To overcome these limitations, we introduce Test-Time Correction (TTC), a training-free alternative. Specifically, TTC utilizes the initial frame as a stable reference anchor to calibrate intermediate stochastic states along the sampling trajectory. Extensive experiments demonstrate that our method seamlessly integrates with various distilled models, extending generation lengths with negligible overhead while matching the quality of resource-intensive training-based methods on 30-second benchmarks.</li>
<li><strong>摘要：</strong>蒸馏自回归扩散模型有利于实时短视频合成，但在长序列生成过程中会遭受严重的错误积累。虽然现有的测试时间优化（TTO）方法被证明对图像或短片有效，但我们发现，由于不稳定的奖励景观和蒸馏参数的超敏性，它们无法减轻扩展序列中的漂移。为了克服这些限制，我们引入了测试时校正（TTC），这是一种免训练的替代方案。具体来说，TTC 利用初始帧作为稳定的参考锚点来校准沿采样轨迹的中间随机状态。大量实验表明，我们的方法与各种精炼模型无缝集成，以可忽略不计的开销延长了生成长度，同时在 30 秒基准上与基于资源密集型训练的方法的质量相匹配。</li>
</ul>

<h3>Title: Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations</h3>
<ul>
<li><strong>Authors: </strong>Wei Liu, Jiawei Xu, Yingru Li, Longtao Zheng, Tianjian Li, Qian Liu, Junxian He</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05885">https://arxiv.org/abs/2602.05885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05885">https://arxiv.org/pdf/2602.05885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05885]] Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations(https://arxiv.org/abs/2602.05885)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>High-quality kernel is critical for scalable AI systems, and enabling LLMs to generate such code would advance AI development. However, training LLMs for this task requires sufficient data, a robust environment, and the process is often vulnerable to reward hacking and lazy optimization. In these cases, models may hack training rewards and prioritize trivial correctness over meaningful speedup. In this paper, we systematically study reinforcement learning (RL) for kernel generation. We first design KernelGYM, a robust distributed GPU environment that supports reward hacking check, data collection from multi-turn interactions and long-term RL training. Building on KernelGYM, we investigate effective multi-turn RL methods and identify a biased policy gradient issue caused by self-inclusion in GRPO. To solve this, we propose Turn-level Reinforce-Leave-One-Out (TRLOO) to provide unbiased advantage estimation for multi-turn RL. To alleviate lazy optimization, we incorporate mismatch correction for training stability and introduce Profiling-based Rewards (PR) and Profiling-based Rejection Sampling (PRS) to overcome the issue. The trained model, this http URL-14B, reaches performance competitive with Claude-4.5-Sonnet in Kernelbench. Finally, we study sequential test-time scaling for this http URL-14B. On the KernelBench Level-2 subset, 31.6% of the generated kernels achieve at least a 1.2x speedup over the Torch reference, surpassing Claude-4.5-Sonnet (26.7%) and GPT-5 (28.6%). When selecting the best candidate across all turns, this 1.2x speedup rate further increases to 47.8%. All resources, including environment, training code, models, and dataset, are included in this https URL.</li>
<li><strong>摘要：</strong>高质量的内核对于可扩展的人工智能系统至关重要，使法学硕士能够生成此类代码将促进人工智能的发展。然而，为这项任务训练法学硕士需要足够的数据、强大的环境，而且这个过程往往容易受到奖励黑客和惰性优化的影响。在这些情况下，模型可能会破解训练奖励，并优先考虑琐碎的正确性而不是有意义的加速。在本文中，我们系统地研究了用于内核生成的强化学习（RL）。 We first design KernelGYM, a robust distributed GPU environment that supports reward hacking check, data collection from multi-turn interactions and long-term RL training. Building on KernelGYM, we investigate effective multi-turn RL methods and identify a biased policy gradient issue caused by self-inclusion in GRPO. To solve this, we propose Turn-level Reinforce-Leave-One-Out (TRLOO) to provide unbiased advantage estimation for multi-turn RL. To alleviate lazy optimization, we incorporate mismatch correction for training stability and introduce Profiling-based Rewards (PR) and Profiling-based Rejection Sampling (PRS) to overcome the issue. The trained model, this http URL-14B, reaches performance competitive with Claude-4.5-Sonnet in Kernelbench. Finally, we study sequential test-time scaling for this http URL-14B. On the KernelBench Level-2 subset, 31.6% of the generated kernels achieve at least a 1.2x speedup over the Torch reference, surpassing Claude-4.5-Sonnet (26.7%) and GPT-5 (28.6%). When selecting the best candidate across all turns, this 1.2x speedup rate further increases to 47.8%. All resources, including environment, training code, models, and dataset, are included in this https URL.</li>
</ul>

<h3>Title: Verification of the Implicit World Model in a Generative Model via Adversarial Sequences</h3>
<ul>
<li><strong>Authors: </strong>András Balogh, Márk Jelasity</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05903">https://arxiv.org/abs/2602.05903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05903">https://arxiv.org/pdf/2602.05903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05903]] Verification of the Implicit World Model in a Generative Model via Adversarial Sequences(https://arxiv.org/abs/2602.05903)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative sequence models are typically trained on sample sequences from natural or formal languages. It is a crucial question whether -- or to what extent -- sample-based training is able to capture the true structure of these languages, often referred to as the ``world model''. Theoretical results indicate that we can hope for soundness at best, that is, generating valid sequences, but not necessarily all of them. However, it is still important to have practical tools that are able to verify whether a given sequence model is sound. In this study, we focus on chess, as it is a domain that provides enough complexity while having a simple rule-based world model. We propose adversarial sequence generation for verifying the soundness of the sequence model. Our adversaries generate valid sequences so as to force the sequence model to generate an invalid next move prediction. Apart from the falsification of soundness, this method is also suitable for a more fine-grained analysis of the failure modes and the effects of different choices during training. To demonstrate this, we propose a number of methods for adversarial sequence generation and evaluate the approach on a large set of chess models. We train models on random as well as high-quality chess games, using several training recipes. We find that none of the models are sound, but some training techniques and dataset choices are able to improve soundness remarkably. We also investigate the potential application of board state probes in both our training and attack methods. Our findings indicate that the extracted board states have no causal role in next token prediction in most of the models.</li>
<li><strong>摘要：</strong>生成序列模型通常根据自然语言或形式语言的样本序列进行训练。 It is a crucial question whether -- or to what extent -- sample-based training is able to capture the true structure of these languages, often referred to as the ``world model''.理论结果表明，我们最多只能期望健全性，即生成有效的序列，但不一定是所有序列。然而，拥有能够验证给定序列模型是否合理的实用工具仍然很重要。在本研究中，我们关注国际象棋，因为它是一个提供足够复杂性同时具有简单的基于规则的世界模型的领域。我们提出对抗性序列生成来验证序列模型的健全性。我们的对手生成有效的序列，以迫使序列模型生成无效的下一步行动预测。 Apart from the falsification of soundness, this method is also suitable for a more fine-grained analysis of the failure modes and the effects of different choices during training. To demonstrate this, we propose a number of methods for adversarial sequence generation and evaluate the approach on a large set of chess models.我们使用多种训练方法在随机和高质量的国际象棋游戏上训练模型。我们发现没有一个模型是健全的，但一些训练技术和数据集选择能够显着提高健全性。我们还研究了董事会状态探测在我们的训练和攻击方法中的潜在应用。我们的研究结果表明，在大多数模型中，提取的董事会状态在下一个代币预测中没有因果作用。</li>
</ul>

<h3>Title: CLIP-Map: Structured Matrix Mapping for Parameter-Efficient CLIP Compression</h3>
<ul>
<li><strong>Authors: </strong>Kangjie Zhang, Wenxuan Huang, Xin Zhou, Boxiang Zhou, Dejia Song, Yuan Xie, Baochang Zhang, Lizhuang Ma, Nemo Chen, Xu Tang, Yao Hu, Shaohui Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05909">https://arxiv.org/abs/2602.05909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05909">https://arxiv.org/pdf/2602.05909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05909]] CLIP-Map: Structured Matrix Mapping for Parameter-Efficient CLIP Compression(https://arxiv.org/abs/2602.05909)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Contrastive Language-Image Pre-training (CLIP) has achieved widely applications in various computer vision tasks, e.g., text-to-image generation, Image-Text retrieval and Image captioning. However, CLIP suffers from high memory and computation cost, which prohibits its usage to the resource-limited application scenarios. Existing CLIP compression methods typically reduce the size of pre-trained CLIP weights by selecting their subset as weight inheritance for further retraining via mask optimization or important weight measurement. However, these select-based weight inheritance often compromises the feature presentation ability, especially on the extreme compression. In this paper, we propose a novel mapping-based CLIP compression framework, CLIP-Map. It leverages learnable matrices to map and combine pretrained weights by Full-Mapping with Kronecker Factorization, aiming to preserve as much information from the original weights as possible. To mitigate the optimization challenges introduced by the learnable mapping, we propose Diagonal Inheritance Initialization to reduce the distribution shifting problem for efficient and effective mapping learning. Extensive experimental results demonstrate that the proposed CLIP-Map outperforms select-based frameworks across various compression ratios, with particularly significant gains observed under high compression settings.</li>
<li><strong>摘要：</strong>Contrastive Language-Image Pre-training (CLIP) has achieved widely applications in various computer vision tasks, e.g., text-to-image generation, Image-Text retrieval and Image captioning.然而，CLIP的内存和计算成本较高，这限制了其在资源有限的应用场景中的使用。 Existing CLIP compression methods typically reduce the size of pre-trained CLIP weights by selecting their subset as weight inheritance for further retraining via mask optimization or important weight measurement.然而，这些基于选择的权重继承通常会损害特征呈现能力，尤其是在极端压缩时。在本文中，我们提出了一种新颖的基于映射的 CLIP 压缩框架，CLIP-Map。 It leverages learnable matrices to map and combine pretrained weights by Full-Mapping with Kronecker Factorization, aiming to preserve as much information from the original weights as possible. To mitigate the optimization challenges introduced by the learnable mapping, we propose Diagonal Inheritance Initialization to reduce the distribution shifting problem for efficient and effective mapping learning. Extensive experimental results demonstrate that the proposed CLIP-Map outperforms select-based frameworks across various compression ratios, with particularly significant gains observed under high compression settings.</li>
</ul>

<h3>Title: Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Junwan Kim, Jiho Park, Seonghu Jeon, Seungryong Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05951">https://arxiv.org/abs/2602.05951</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05951">https://arxiv.org/pdf/2602.05951</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05951]] Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching(https://arxiv.org/abs/2602.05951)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Flow matching has recently emerged as a promising alternative to diffusion-based generative models, particularly for text-to-image generation. Despite its flexibility in allowing arbitrary source distributions, most existing approaches rely on a standard Gaussian distribution, a choice inherited from diffusion models, and rarely consider the source distribution itself as an optimization target in such settings. In this work, we show that principled design of the source distribution is not only feasible but also beneficial at the scale of modern text-to-image systems. Specifically, we propose learning a condition-dependent source distribution under flow matching objective that better exploit rich conditioning signals. We identify key failure modes that arise when directly incorporating conditioning into the source, including distributional collapse and instability, and show that appropriate variance regularization and directional alignment between source and target are critical for stable and effective learning. We further analyze how the choice of target representation space impacts flow matching with structured sources, revealing regimes in which such designs are most effective. Extensive experiments across multiple text-to-image benchmarks demonstrate consistent and robust improvements, including up to a 3x faster convergence in FID, highlighting the practical benefits of a principled source distribution design for conditional flow matching.</li>
<li><strong>摘要：</strong>流匹配最近已成为基于扩散的生成模型的有前途的替代方案，特别是对于文本到图像的生成。尽管其在允许任意源分布方面具有灵活性，但大多数现有方法依赖于标准高斯分布，这是从扩散模型继承的选择，并且很少将源分布本身视为此类设置中的优化目标。在这项工作中，我们证明了源分布的原则性设计不仅是可行的，而且在现代文本到图像系统的规模上也是有益的。 Specifically, we propose learning a condition-dependent source distribution under flow matching objective that better exploit rich conditioning signals.我们确定了直接将条件合并到源中时出现的关键故障模式，包括分布崩溃和不稳定性，并表明适当的方差正则化和源与目标之间的方向对齐对于稳定有效的学习至关重要。我们进一步分析了目标表示空间的选择如何影响与结构化源的流匹配，揭示了此类设计最有效的机制。跨多个文本到图像基准的广泛实验证明了一致和稳健的改进，包括 FID 收敛速度提高了 3 倍，突出了用于条件流匹配的原则性源分布设计的实际优势。</li>
</ul>

<h3>Title: Discrete diffusion samplers and bridges: Off-policy algorithms and applications in latent spaces</h3>
<ul>
<li><strong>Authors: </strong>Arran Carter, Sanghyeok Choi, Kirill Tamogashev, Víctor Elvira, Nikolay Malkin</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05961">https://arxiv.org/abs/2602.05961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05961">https://arxiv.org/pdf/2602.05961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05961]] Discrete diffusion samplers and bridges: Off-policy algorithms and applications in latent spaces(https://arxiv.org/abs/2602.05961)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Sampling from a distribution $p(x) \propto e^{-\mathcal{E}(x)}$ known up to a normalising constant is an important and challenging problem in statistics. Recent years have seen the rise of a new family of amortised sampling algorithms, commonly referred to as diffusion samplers, that enable fast and efficient sampling from an unnormalised density. Such algorithms have been widely studied for continuous-space sampling tasks; however, their application to problems in discrete space remains largely unexplored. Although some progress has been made in this area, discrete diffusion samplers do not take full advantage of ideas commonly used for continuous-space sampling. In this paper, we propose to bridge this gap by introducing off-policy training techniques for discrete diffusion samplers. We show that these techniques improve the performance of discrete samplers on both established and new synthetic benchmarks. Next, we generalise discrete diffusion samplers to the task of bridging between two arbitrary distributions, introducing data-to-energy Schrödinger bridge training for the discrete domain for the first time. Lastly, we showcase the application of the proposed diffusion samplers to data-free posterior sampling in the discrete latent spaces of image generative models.</li>
<li><strong>摘要：</strong>Sampling from a distribution $p(x) \propto e^{-\mathcal{E}(x)}$ known up to a normalising constant is an important and challenging problem in statistics. Recent years have seen the rise of a new family of amortised sampling algorithms, commonly referred to as diffusion samplers, that enable fast and efficient sampling from an unnormalised density.此类算法已针对连续空间采样任务进行了广泛研究；然而，它们在离散空间问题中的应用在很大程度上仍未得到探索。 Although some progress has been made in this area, discrete diffusion samplers do not take full advantage of ideas commonly used for continuous-space sampling.在本文中，我们建议通过引入离散扩散采样器的离策略训练技术来弥补这一差距。我们表明，这些技术可以提高离散采样器在已建立的和新的综合基准上的性能。 Next, we generalise discrete diffusion samplers to the task of bridging between two arbitrary distributions, introducing data-to-energy Schrödinger bridge training for the discrete domain for the first time. Lastly, we showcase the application of the proposed diffusion samplers to data-free posterior sampling in the discrete latent spaces of image generative models.</li>
</ul>

<h3>Title: LSA: Localized Semantic Alignment for Enhancing Temporal Consistency in Traffic Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Mirlan Karimov, Teodora Spasojevic, Markus Braun, Julian Wiederer, Vasileios Belagiannis, Marc Pollefeys</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05966">https://arxiv.org/abs/2602.05966</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05966">https://arxiv.org/pdf/2602.05966</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05966]] LSA: Localized Semantic Alignment for Enhancing Temporal Consistency in Traffic Video Generation(https://arxiv.org/abs/2602.05966)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Controllable video generation has emerged as a versatile tool for autonomous driving, enabling realistic synthesis of traffic scenarios. However, existing methods depend on control signals at inference time to guide the generative model towards temporally consistent generation of dynamic objects, limiting their utility as scalable and generalizable data engines. In this work, we propose Localized Semantic Alignment (LSA), a simple yet effective framework for fine-tuning pre-trained video generation models. LSA enhances temporal consistency by aligning semantic features between ground-truth and generated video clips. Specifically, we compare the output of an off-the-shelf feature extraction model between the ground-truth and generated video clips localized around dynamic objects inducing a semantic feature consistency loss. We fine-tune the base model by combining this loss with the standard diffusion loss. The model fine-tuned for a single epoch with our novel loss outperforms the baselines in common video generation evaluation metrics. To further test the temporal consistency in generated videos we adapt two additional metrics from object detection task, namely mAP and mIoU. Extensive experiments on nuScenes and KITTI datasets show the effectiveness of our approach in enhancing temporal consistency in video generation without the need for external control signals during inference and any computational overheads.</li>
<li><strong>摘要：</strong>可控视频生成已成为自动驾驶的多功能工具，可实现交通场景的真实合成。然而，现有方法依赖于推理时的控制信号来引导生成模型实现动态对象的时间一致生成，限制了它们作为可扩展和可概括的数据引擎的实用性。在这项工作中，我们提出了局部语义对齐（LSA），这是一个简单而有效的框架，用于微调预训练的视频生成模型。 LSA enhances temporal consistency by aligning semantic features between ground-truth and generated video clips.具体来说，我们比较了现成特征提取模型的输出，即地面实况和围绕动态对象生成的视频剪辑，从而导致语义特征一致性损失。 We fine-tune the base model by combining this loss with the standard diffusion loss. The model fine-tuned for a single epoch with our novel loss outperforms the baselines in common video generation evaluation metrics.为了进一步测试生成视频的时间一致性，我们采用了对象检测任务中的两个附加指标，即 mAP 和 mIoU。 nuScenes 和 KITTI 数据集上的大量实验表明，我们的方法在增强视频生成的时间一致性方面的有效性，而无需在推理过程中使用外部控制信号和任何计算开销。</li>
</ul>

<h3>Title: A Hybrid Data-Driven Algorithm for Real-Time Friction Force Estimation in Hydraulic Cylinders</h3>
<ul>
<li><strong>Authors: </strong>Mohamad Amin Jamshidi, Mehrbod Zarifi, Zolfa Anvari, Hamed Ghafarirad, Mohammad Zareinejad</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05967">https://arxiv.org/abs/2602.05967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05967">https://arxiv.org/pdf/2602.05967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05967]] A Hybrid Data-Driven Algorithm for Real-Time Friction Force Estimation in Hydraulic Cylinders(https://arxiv.org/abs/2602.05967)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Hydraulic systems are widely utilized in industrial applications due to their high force generation, precise control, and ability to function in harsh environments. Hydraulic cylinders, as actuators in these systems, apply force and position through the displacement of hydraulic fluid, but their operation is significantly influenced by friction force. Achieving precision in hydraulic cylinders requires an accurate friction model under various operating conditions. Existing analytical models, often derived from experimental tests, necessitate the identification or estimation of influencing factors but are limited in adaptability and computational efficiency. This research introduces a data-driven, hybrid algorithm based on Long Short-Term Memory (LSTM) networks and Random Forests for nonlinear friction force estimation. The algorithm effectively combines feature detection and estimation processes using training data acquired from an experimental hydraulic test setup. It achieves a consistent and stable model error of less than 10% across diverse operating conditions and external load variations, ensuring robust performance in complex situations. The computational cost of the algorithm is 1.51 milliseconds per estimation, making it suitable for real-time applications. The proposed method addresses the limitations of analytical models by delivering high precision and computational efficiency. The algorithm's performance is validated through detailed analysis and experimental results, including direct comparisons with the LuGre model. The comparison highlights that while the LuGre model offers a theoretical foundation for friction modeling, its performance is limited by its inability to dynamically adjust to varying operational conditions of the hydraulic cylinder, further emphasizing the advantages of the proposed hybrid approach in real-time applications.</li>
<li><strong>摘要：</strong>液压系统因其产生的高力、精确的控制以及在恶劣环境下工作的能力而广泛应用于工业应用。液压缸作为这些系统中的执行器，通过液压油的位移来施加力和位置，但其操作受摩擦力的影响很大。要实现液压缸的精度，需要在各种操作条件下建立精确的摩擦模型。现有的分析模型通常来自实验测试，需要识别或估计影响因素，但在适应性和计算效率方面受到限制。本研究引入了一种基于长短期记忆 (LSTM) 网络和随机森林的数据驱动混合算法，用于非线性摩擦力估计。该算法使用从实验液压测试装置获得的训练数据有效地结合了特征检测和估计过程。它在不同的操作条件和外部负载变化下实现了小于 10% 的一致且稳定的模型误差，确保了复杂情况下的稳健性能。该算法每次估计的计算成本为 1.51 毫秒，适合实时应用。所提出的方法通过提供高精度和计算效率来解决分析模型的局限性。该算法的性能通过详细的分析和实验结果得到验证，包括与 LuGre 模型的直接比较。比较强调，虽然 LuGre 模型为摩擦建模提供了理论基础，但其性能因无法动态调整以适应液压缸不同的操作条件而受到限制，进一步强调了所提出的混合方法在实时应用中的优势。</li>
</ul>

<h3>Title: Clifford Kolmogorov-Arnold Networks</h3>
<ul>
<li><strong>Authors: </strong>Matthias Wolff, Francesco Alesiani, Christof Duhme, Xiaoyi Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05977">https://arxiv.org/abs/2602.05977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05977">https://arxiv.org/pdf/2602.05977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05977]] Clifford Kolmogorov-Arnold Networks(https://arxiv.org/abs/2602.05977)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce Clifford Kolmogorov-Arnold Network (ClKAN), a flexible and efficient architecture for function approximation in arbitrary Clifford algebra spaces. We propose the use of Randomized Quasi Monte Carlo grid generation as a solution to the exponential scaling associated with higher dimensional algebras. Our ClKAN also introduces new batch normalization strategies to deal with variable domain input. ClKAN finds application in scientific discovery and engineering, and is validated in synthetic and physics inspired tasks.</li>
<li><strong>摘要：</strong>我们介绍 Clifford Kolmogorov-Arnold Network (ClKAN)，这是一种灵活高效的架构，用于任意 Clifford 代数空间中的函数逼近。我们建议使用随机准蒙特卡罗网格生成作为与高维代数相关的指数缩放的解决方案。我们的 ClKAN 还引入了新的批量归一化策略来处理可变域输入。 ClKAN 在科学发现和工程中得到应用，并在合成和物理启发任务中得到验证。</li>
</ul>

<h3>Title: RISE-Video: Can Video Generators Decode Implicit World Rules?</h3>
<ul>
<li><strong>Authors: </strong>Mingxin Liu, Shuran Ma, Shibei Meng, Xiangyu Zhao, Zicheng Zhang, Shaofeng Zhang, Zhihang Zhong, Peixian Chen, Haoyu Cao, Xing Sun, Haodong Duan, Xue Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05986">https://arxiv.org/abs/2602.05986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05986">https://arxiv.org/pdf/2602.05986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05986]] RISE-Video: Can Video Generators Decode Implicit World Rules?(https://arxiv.org/abs/2602.05986)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While generative video models have achieved remarkable visual fidelity, their capacity to internalize and reason over implicit world rules remains a critical yet under-explored frontier. To bridge this gap, we present RISE-Video, a pioneering reasoning-oriented benchmark for Text-Image-to-Video (TI2V) synthesis that shifts the evaluative focus from surface-level aesthetics to deep cognitive reasoning. RISE-Video comprises 467 meticulously human-annotated samples spanning eight rigorous categories, providing a structured testbed for probing model intelligence across diverse dimensions, ranging from commonsense and spatial dynamics to specialized subject domains. Our framework introduces a multi-dimensional evaluation protocol consisting of four metrics: \textit{Reasoning Alignment}, \textit{Temporal Consistency}, \textit{Physical Rationality}, and \textit{Visual Quality}. To further support scalable evaluation, we propose an automated pipeline leveraging Large Multimodal Models (LMMs) to emulate human-centric assessment. Extensive experiments on 11 state-of-the-art TI2V models reveal pervasive deficiencies in simulating complex scenarios under implicit constraints, offering critical insights for the advancement of future world-simulating generative models.</li>
<li><strong>摘要：</strong>虽然生成视频模型已经实现了卓越的视觉保真度，但它们内化和推理隐含世界规则的能力仍然是一个关键但尚未充分探索的前沿领域。为了弥补这一差距，我们推出了 RISE-Video，这是一种面向文本图像到视频 (TI2V) 合成的开创性推理导向基准，它将评估焦点从表面美学转移到深层认知推理。 RISE-Video comprises 467 meticulously human-annotated samples spanning eight rigorous categories, providing a structured testbed for probing model intelligence across diverse dimensions, ranging from commonsense and spatial dynamics to specialized subject domains. Our framework introduces a multi-dimensional evaluation protocol consisting of four metrics: \textit{Reasoning Alignment}, \textit{Temporal Consistency}, \textit{Physical Rationality}, and \textit{Visual Quality}. To further support scalable evaluation, we propose an automated pipeline leveraging Large Multimodal Models (LMMs) to emulate human-centric assessment. Extensive experiments on 11 state-of-the-art TI2V models reveal pervasive deficiencies in simulating complex scenarios under implicit constraints, offering critical insights for the advancement of future world-simulating generative models.</li>
</ul>

<h3>Title: Diamond Maps: Efficient Reward Alignment via Stochastic Flow Maps</h3>
<ul>
<li><strong>Authors: </strong>Peter Holderrieth, Douglas Chen, Luca Eyring, Ishin Shah, Giri Anantharaman, Yutong He, Zeynep Akata, Tommi Jaakkola, Nicholas Matthew Boffi, Max Simchowitz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05993">https://arxiv.org/abs/2602.05993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05993">https://arxiv.org/pdf/2602.05993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05993]] Diamond Maps: Efficient Reward Alignment via Stochastic Flow Maps(https://arxiv.org/abs/2602.05993)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Flow and diffusion models produce high-quality samples, but adapting them to user preferences or constraints post-training remains costly and brittle, a challenge commonly called reward alignment. We argue that efficient reward alignment should be a property of the generative model itself, not an afterthought, and redesign the model for adaptability. We propose "Diamond Maps", stochastic flow map models that enable efficient and accurate alignment to arbitrary rewards at inference time. Diamond Maps amortize many simulation steps into a single-step sampler, like flow maps, while preserving the stochasticity required for optimal reward alignment. This design makes search, sequential Monte Carlo, and guidance scalable by enabling efficient and consistent estimation of the value function. Our experiments show that Diamond Maps can be learned efficiently via distillation from GLASS Flows, achieve stronger reward alignment performance, and scale better than existing methods. Our results point toward a practical route to generative models that can be rapidly adapted to arbitrary preferences and constraints at inference time.</li>
<li><strong>摘要：</strong>流动和扩散模型产生高质量的样本，但在训练后使它们适应用户偏好或约束仍然是昂贵且脆弱的，这是一个通常称为奖励调整的挑战。我们认为，有效的奖励调整应该是生成模型本身的属性，而不是事后的想法，并重新设计模型以实现适应性。我们提出“钻石图”，即随机流图模型，可以在推理时有效且准确地对齐任意奖励。钻石图将许多模拟步骤分摊到单步采样器中，如流程图，同时保留最佳奖励对齐所需的随机性。该设计通过实现价值函数的高效且一致的估计，使搜索、顺序蒙特卡罗和指导可扩展。我们的实验表明，钻石图可以通过从 GLASS Flows 中蒸馏来有效学习，实现更强的奖励对齐性能，并且比现有方法更好地扩展。我们的结果指出了生成模型的实用途径，该模型可以在推理时快速适应任意偏好和约束。</li>
</ul>

<h3>Title: VisRefiner: Learning from Visual Differences for Screenshot-to-Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Jie Deng, Kaichun Yao, Libo Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.05998">https://arxiv.org/abs/2602.05998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.05998">https://arxiv.org/pdf/2602.05998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.05998]] VisRefiner: Learning from Visual Differences for Screenshot-to-Code Generation(https://arxiv.org/abs/2602.05998)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Screenshot-to-code generation aims to translate user interface screenshots into executable frontend code that faithfully reproduces the target layout and style. Existing multimodal large language models perform this mapping directly from screenshots but are trained without observing the visual outcomes of their generated code. In contrast, human developers iteratively render their implementation, compare it with the design, and learn how visual differences relate to code changes. Inspired by this process, we propose VisRefiner, a training framework that enables models to learn from visual differences between rendered predictions and reference designs. We construct difference-aligned supervision that associates visual discrepancies with corresponding code edits, allowing the model to understand how appearance variations arise from implementation changes. Building on this, we introduce a reinforcement learning stage for self-refinement, where the model improves its generated code by observing both the rendered output and the target design, identifying their visual differences, and updating the code accordingly. Experiments show that VisRefiner substantially improves single-step generation quality and layout fidelity, while also endowing models with strong self-refinement ability. These results demonstrate the effectiveness of learning from visual differences for advancing screenshot-to-code generation.</li>
<li><strong>摘要：</strong>屏幕截图到代码生成旨在将用户界面屏幕截图转换为忠实再现目标布局和风格的可执行前端代码。现有的多模式大语言模型直接从屏幕截图执行此映射，但在不观察生成代码的视觉结果的情况下进行训练。相比之下，人类开发人员迭代地渲染他们的实现，将其与设计进行比较，并了解视觉差异与代码更改的关系。受此过程的启发，我们提出了 VisRefiner，这是一个训练框架，使模型能够从渲染预测和参考设计之间的视觉差异中学习。我们构建了差异对齐监督，将视觉差异与相应的代码编辑相关联，从而使模型能够理解实现更改如何引起外观变化。在此基础上，我们引入了用于自我改进的强化学习阶段，其中模型通过观察渲染的输出和目标设计、识别它们的视觉差异并相应地更新代码来改进其生成的代码。实验表明，VisRefiner 大幅提高了单步生成质量和布局保真度，同时赋予模型强大的自求精能力。这些结果证明了从视觉差异中学习对于推进屏幕截图到代码生成的有效性。</li>
</ul>

<h3>Title: GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?</h3>
<ul>
<li><strong>Authors: </strong>Ruihang Li, Leigang Qu, Jingxu Zhang, Dongnan Gui, Mengde Xu, Xiaosong Zhang, Han Hu, Wenjie Wang, Jiaqi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06013">https://arxiv.org/abs/2602.06013</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06013">https://arxiv.org/pdf/2602.06013</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06013]] GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?(https://arxiv.org/abs/2602.06013)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The rapid advancement of visual generation models has outpaced traditional evaluation approaches, necessitating the adoption of Vision-Language Models as surrogate judges. In this work, we systematically investigate the reliability of the prevailing absolute pointwise scoring standard, across a wide spectrum of visual generation tasks. Our analysis reveals that this paradigm is limited due to stochastic inconsistency and poor alignment with human perception. To resolve these limitations, we introduce GenArena, a unified evaluation framework that leverages a pairwise comparison paradigm to ensure stable and human-aligned evaluation. Crucially, our experiments uncover a transformative finding that simply adopting this pairwise protocol enables off-the-shelf open-source models to outperform top-tier proprietary models. Notably, our method boosts evaluation accuracy by over 20% and achieves a Spearman correlation of 0.86 with the authoritative LMArena leaderboard, drastically surpassing the 0.36 correlation of pointwise methods. Based on GenArena, we benchmark state-of-the-art visual generation models across diverse tasks, providing the community with a rigorous and automated evaluation standard for visual generation.</li>
<li><strong>摘要：</strong>视觉生成模型的快速发展已经超过了传统的评估方法，因此需要采用视觉语言模型作为代理评判。在这项工作中，我们系统地研究了广泛的视觉生成任务中流行的绝对逐点评分标准的可靠性。我们的分析表明，由于随机不一致和与人类感知的不一致，这种范式是有限的。为了解决这些限制，我们引入了 GenArena，这是一个统一的评估框架，它利用成对比较范式来确保稳定且符合人类的评估。至关重要的是，我们的实验揭示了一个变革性的发现，即简单地采用这种成对协议就可以使现成的开源模型优于顶级专有模型。值得注意的是，我们的方法将评估准确度提高了 20% 以上，并且与权威的 LMArena 排行榜的 Spearman 相关性达到了 0.86，大大超过了逐点方法的 0.36 相关性。基于 GenArena，我们在不同的任务中对最先进的视觉生成模型进行基准测试，为社区提供严格且自动化的视觉生成评估标准。</li>
</ul>

<h3>Title: Context Forcing: Consistent Autoregressive Video Generation with Long Context</h3>
<ul>
<li><strong>Authors: </strong>Shuo Chen, Cong Wei, Sun Sun, Ping Nie, Kai Zhou, Ge Zhang, Ming-Hsuan Yang, Wenhu Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06028">https://arxiv.org/abs/2602.06028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06028">https://arxiv.org/pdf/2602.06028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06028]] Context Forcing: Consistent Autoregressive Video Generation with Long Context(https://arxiv.org/abs/2602.06028)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent approaches to real-time long video generation typically employ streaming tuning strategies, attempting to train a long-context student using a short-context (memoryless) teacher. In these frameworks, the student performs long rollouts but receives supervision from a teacher limited to short 5-second windows. This structural discrepancy creates a critical \textbf{student-teacher mismatch}: the teacher's inability to access long-term history prevents it from guiding the student on global temporal dependencies, effectively capping the student's context length. To resolve this, we propose \textbf{Context Forcing}, a novel framework that trains a long-context student via a long-context teacher. By ensuring the teacher is aware of the full generation history, we eliminate the supervision mismatch, enabling the robust training of models capable of long-term consistency. To make this computationally feasible for extreme durations (e.g., 2 minutes), we introduce a context management system that transforms the linearly growing context into a \textbf{Slow-Fast Memory} architecture, significantly reducing visual redundancy. Extensive results demonstrate that our method enables effective context lengths exceeding 20 seconds -- 2 to 10 times longer than state-of-the-art methods like LongLive and Infinite-RoPE. By leveraging this extended context, Context Forcing preserves superior consistency across long durations, surpassing state-of-the-art baselines on various long video evaluation metrics.</li>
<li><strong>摘要：</strong>Recent approaches to real-time long video generation typically employ streaming tuning strategies, attempting to train a long-context student using a short-context (memoryless) teacher.在这些框架中，学生执行长时间的展示，但接受教师的监督，仅限于 5 秒的短暂窗口。 This structural discrepancy creates a critical \textbf{student-teacher mismatch}: the teacher's inability to access long-term history prevents it from guiding the student on global temporal dependencies, effectively capping the student's context length.为了解决这个问题，我们提出了 \textbf{Context Forcing}，这是一种通过长上下文教师训练长上下文学生的新颖框架。 By ensuring the teacher is aware of the full generation history, we eliminate the supervision mismatch, enabling the robust training of models capable of long-term consistency. To make this computationally feasible for extreme durations (e.g., 2 minutes), we introduce a context management system that transforms the linearly growing context into a \textbf{Slow-Fast Memory} architecture, significantly reducing visual redundancy. Extensive results demonstrate that our method enables effective context lengths exceeding 20 seconds -- 2 to 10 times longer than state-of-the-art methods like LongLive and Infinite-RoPE. By leveraging this extended context, Context Forcing preserves superior consistency across long durations, surpassing state-of-the-art baselines on various long video evaluation metrics.</li>
</ul>

<h3>Title: V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Dongyang Chen, Chaoyang Wang, Dezhao SU, Xi Xiao, Zeyu Zhang, Jing Xiong, Qing Li, Yuzhang Shang, Shichao Ka</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06034">https://arxiv.org/abs/2602.06034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06034">https://arxiv.org/pdf/2602.06034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06034]] V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval(https://arxiv.org/abs/2602.06034)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have recently been applied to universal multimodal retrieval, where Chain-of-Thought (CoT) reasoning improves candidate reranking. However, existing approaches remain largely language-driven, relying on static visual encodings and lacking the ability to actively verify fine-grained visual evidence, which often leads to speculative reasoning in visually ambiguous cases. We propose V-Retrver, an evidence-driven retrieval framework that reformulates multimodal retrieval as an agentic reasoning process grounded in visual inspection. V-Retrver enables an MLLM to selectively acquire visual evidence during reasoning via external visual tools, performing a multimodal interleaved reasoning process that alternates between hypothesis generation and targeted visual this http URL train such an evidence-gathering retrieval agent, we adopt a curriculum-based learning strategy combining supervised reasoning activation, rejection-based refinement, and reinforcement learning with an evidence-aligned objective. Experiments across multiple multimodal retrieval benchmarks demonstrate consistent improvements in retrieval accuracy (with 23.0% improvements on average), perception-driven reasoning reliability, and generalization.</li>
<li><strong>摘要：</strong></li>
</ul>

<h3>Title: InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions</h3>
<ul>
<li><strong>Authors: </strong>Sirui Xu, Samuel Schulter, Morteza Ziyadi, Xialin He, Xiaohan Fei, Yu-Xiong Wang, Liangyan Gui</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06035">https://arxiv.org/abs/2602.06035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06035">https://arxiv.org/pdf/2602.06035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06035]] InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions(https://arxiv.org/abs/2602.06035)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Humans rarely plan whole-body interactions with objects at the level of explicit whole-body movements. High-level intentions, such as affordance, define the goal, while coordinated balance, contact, and manipulation can emerge naturally from underlying physical and motor priors. Scaling such priors is key to enabling humanoids to compose and generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination. To this end, we introduce InterPrior, a scalable framework that learns a unified generative controller through large-scale imitation pretraining and post-training by reinforcement learning. InterPrior first distills a full-reference imitation expert into a versatile, goal-conditioned variational policy that reconstructs motion from multimodal observations and high-level intent. While the distilled policy reconstructs training behaviors, it does not generalize reliably due to the vast configuration space of large-scale human-object interactions. To address this, we apply data augmentation with physical perturbations, and then perform reinforcement learning finetuning to improve competence on unseen goals and initializations. Together, these steps consolidate the reconstructed latent skills into a valid manifold, yielding a motion prior that generalizes beyond the training data, e.g., it can incorporate new behaviors such as interactions with unseen objects. We further demonstrate its effectiveness for user-interactive control and its potential for real robot deployment.</li>
<li><strong>摘要：</strong>人类很少在明确的全身运动层面上计划与物体的全身交互。高层次的意图，例如可供性，定义了目标，而协调的平衡、接触和操纵可以从潜在的身体和运动先验中自然地出现。扩展此类先验是使类人机器人能够在不同环境中组合和概括局部操作技能，同时保持身体连贯的全身协调的关键。为此，我们引入了InterPrior，一个可扩展的框架，通过大规模模仿预训练和强化学习后训练来学习统一的生成控制器。 InterPrior 首先将全参考模仿专家提炼为多功能、目标条件变分策略，该策略根据多模态观察和高级意图重建运动。虽然蒸馏策略重建了训练行为，但由于大规模人机交互的巨大配置空间，它不能可靠地泛化。为了解决这个问题，我们应用物理扰动的数据增强，然后执行强化学习微调以提高看不见的目标和初始化的能力。总之，这些步骤将重建的潜在技能整合到一个有效的流形中，产生一个超越训练数据的先验运动，例如，它可以合并新的行为，例如与看不见的物体的交互。我们进一步证明了其在用户交互控制方面的有效性及其在实际机器人部署方面的潜力。</li>
</ul>

<h3>Title: Thinking with Geometry: Active Geometry Integration for Spatial Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Haoyuan Li, Qihang Cao, Tao Tang, Kun Xiang, Zihan Guo, Jianhua Han, Hang Xu, Xiaodan Liang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06037">https://arxiv.org/abs/2602.06037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06037">https://arxiv.org/pdf/2602.06037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06037]] Thinking with Geometry: Active Geometry Integration for Spatial Reasoning(https://arxiv.org/abs/2602.06037)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent progress in spatial reasoning with Multimodal Large Language Models (MLLMs) increasingly leverages geometric priors from 3D encoders. However, most existing integration strategies remain passive: geometry is exposed as a global stream and fused in an indiscriminate manner, which often induces semantic-geometry misalignment and redundant signals. We propose GeoThinker, a framework that shifts the paradigm from passive fusion to active perception. Instead of feature mixing, GeoThinker enables the model to selectively retrieve geometric evidence conditioned on its internal reasoning demands. GeoThinker achieves this through Spatial-Grounded Fusion applied at carefully selected VLM layers, where semantic visual priors selectively query and integrate task-relevant geometry via frame-strict cross-attention, further calibrated by Importance Gating that biases per-frame attention toward task-relevant structures. Comprehensive evaluation results show that GeoThinker sets a new state-of-the-art in spatial intelligence, achieving a peak score of 72.6 on the VSI-Bench. Furthermore, GeoThinker demonstrates robust generalization and significantly improved spatial perception across complex downstream scenarios, including embodied referring and autonomous driving. Our results indicate that the ability to actively integrate spatial structures is essential for next-generation spatial intelligence. Code can be found at this https URL.</li>
<li><strong>摘要：</strong>多模态大型语言模型 (MLLM) 空间推理的最新进展越来越多地利用 3D 编码器的几何先验。然而，大多数现有的集成策略仍然是被动的：几何图形被暴露为全局流并以不加区别的方式融合，这通常会导致语义-几何错位和冗余信号。我们提出了 GeoThinker，一个将范式从被动融合转变为主动感知的框架。 GeoThinker 不是特征混合，而是使模型能够根据其内部推理需求选择性地检索几何证据。 GeoThinker 通过在精心选择的 VLM 层上应用空间接地融合来实现这一目标，其中语义视觉先验通过帧严格交叉注意选择性地查询和集成任务相关几何图形，并通过重要性门控进一步校准，将每帧注意力偏向任务相关结构。综合评估结果表明，GeoThinker 在空间智能方面树立了新的最先进水平，在 VSI-Bench 上取得了 72.6 的最高分数。此外，GeoThinker 展示了强大的泛化能力，并显着改善了复杂下游场景（包括具体参考和自动驾驶）的空间感知。我们的结果表明，主动整合空间结构的能力对于下一代空间智能至关重要。可以在此 https URL 找到代码。</li>
</ul>

<h3>Title: Pseudo-Invertible Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Yamit Ehrlich, Nimrod Berman, Assaf Shocher</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06042">https://arxiv.org/abs/2602.06042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06042">https://arxiv.org/pdf/2602.06042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06042]] Pseudo-Invertible Neural Networks(https://arxiv.org/abs/2602.06042)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The Moore-Penrose Pseudo-inverse (PInv) serves as the fundamental solution for linear systems. In this paper, we propose a natural generalization of PInv to the nonlinear regime in general and to neural networks in particular. We introduce Surjective Pseudo-invertible Neural Networks (SPNN), a class of architectures explicitly designed to admit a tractable non-linear PInv. The proposed non-linear PInv and its implementation in SPNN satisfy fundamental geometric properties. One such property is null-space projection or "Back-Projection", $x' = x + A^\dagger(y-Ax)$, which moves a sample $x$ to its closest consistent state $x'$ satisfying $Ax=y$. We formalize Non-Linear Back-Projection (NLBP), a method that guarantees the same consistency constraint for non-linear mappings $f(x)=y$ via our defined PInv. We leverage SPNNs to expand the scope of zero-shot inverse problems. Diffusion-based null-space projection has revolutionized zero-shot solving for linear inverse problems by exploiting closed-form back-projection. We extend this method to non-linear degradations. Here, "degradation" is broadly generalized to include any non-linear loss of information, spanning from optical distortions to semantic abstractions like classification. This approach enables zero-shot inversion of complex degradations and allows precise semantic control over generative outputs without retraining the diffusion prior.</li>
<li><strong>摘要：</strong>Moore-Penrose 伪逆 (PInv) 是线性系统的基本解。在本文中，我们提出了将 PInv 自然推广到一般非线性机制，特别是神经网络的方法。我们引入了满射伪可逆神经网络（SPNN），这是一类明确设计用于接纳易处理的非线性 PInv 的架构。所提出的非线性 PInv 及其在 SPNN 中的实现满足基本几何特性。其中一个属性是零空间投影或“反向投影”，$x' = x + A^\dagger(y-Ax)$，它将样本 $x$ 移动到其最接近的一致状态 $x'$ 满足 $Ax=y$。我们形式化了非线性反投影（NLBP），这是一种通过我们定义的 PInv 保证非线性映射 $f(x)=y$ 具有相同一致性约束的方法。我们利用 SPNN 来扩大零样本逆问题的范围。基于扩散的零空间投影通过利用封闭形式的反投影彻底改变了线性逆问题的零样本求解。我们将此方法扩展到非线性退化。在这里，“退化”被广泛概括为包括任何非线性信息丢失，从光学扭曲到分类等语义抽象。这种方法能够实现复杂退化的零样本反演，并允许对生成输出进行精确的语义控制，而无需重新训练扩散先验。</li>
</ul>

<h3>Title: Shared LoRA Subspaces for almost Strict Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>Prakhar Kaushik, Ankit Vaidya, Shravan Chaudhari, Rama Chellappa, Alan Yuille</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06043">https://arxiv.org/abs/2602.06043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06043">https://arxiv.org/pdf/2602.06043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06043]] Shared LoRA Subspaces for almost Strict Continual Learning(https://arxiv.org/abs/2602.06043)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining. While parameter-efficient tuning methods like low rank adaptation (LoRA) reduce computational demands, they lack mechanisms for strict continual learning and knowledge integration, without relying on data replay, or multiple adapters. We propose Share, a novel approach to parameter efficient continual finetuning that learns and dynamically updates a single, shared low-rank subspace, enabling seamless adaptation across multiple tasks and modalities. Share constructs a foundational subspace that extracts core knowledge from past tasks and incrementally integrates new information by identifying essential subspace directions. Knowledge from each new task is incorporated into this evolving subspace, facilitating forward knowledge transfer, while minimizing catastrophic interference. This approach achieves up to 100x parameter reduction and 281x memory savings over traditional LoRA methods, maintaining performance comparable to jointly trained models. A single Share model can replace hundreds of task-specific LoRA adapters, supporting scalable, asynchronous continual learning. Experiments across image classification, natural language understanding, 3D pose estimation, and text-to-image generation validate its effectiveness, making Share a practical and scalable solution for lifelong learning in large-scale AI systems.</li>
<li><strong>摘要：</strong>高效、持续地使大型预训练模型适应新任务对于现实世界的部署至关重要，但由于灾难性遗忘和再训练的高昂成本，仍然具有挑战性。虽然像低阶自适应（LoRA）这样的参数高效调整方法可以减少计算需求，但它们缺乏严格的持续学习和知识集成的机制，而不依赖于数据重放或多个适配器。我们提出了 Share，这是一种参数高效持续微调的新颖方法，可以学习并动态更新单个共享的低秩子空间，从而实现跨多个任务和模式的无缝适应。 Share 构建了一个基础子空间，从过去的任务中提取核心知识，并通过识别基本的子空间方向逐步集成新信息。每个新任务的知识都被纳入这个不断发展的子空间中，促进向前的知识转移，同时最大限度地减少灾难性干扰。与传统 LoRA 方法相比，该方法可减少高达 100 倍的参数并节省 281 倍的内存，保持与联合训练模型相当的性能。单个共享模型可以取代数百个特定于任务的 LoRA 适配器，支持可扩展的异步持续学习。图像分类、自然语言理解、3D 姿态估计和文本到图像生成的实验验证了其有效性，使 Share 成为大规模 AI 系统中终身学习的实用且可扩展的解决方案。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
