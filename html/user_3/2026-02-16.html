<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-02-16</h1>
<h3>Title: LLaMo: Scaling Pretrained Language Models for Unified Motion Understanding and Generation with Continuous Autoregressive Tokens</h3>
<ul>
<li><strong>Authors: </strong>Zekun Li, Sizhe An, Chengcheng Tang, Chuan Guo, Ivan Shugurov, Linguang Zhang, Amy Zhao, Srinath Sridhar, Lingling Tao, Abhay Mittal</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12370">https://arxiv.org/abs/2602.12370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12370">https://arxiv.org/pdf/2602.12370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12370]] LLaMo: Scaling Pretrained Language Models for Unified Motion Understanding and Generation with Continuous Autoregressive Tokens(https://arxiv.org/abs/2602.12370)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent progress in large models has led to significant advances in unified multimodal generation and understanding. However, the development of models that unify motion-language generation and understanding remains largely underexplored. Existing approaches often fine-tune large language models (LLMs) on paired motion-text data, which can result in catastrophic forgetting of linguistic capabilities due to the limited scale of available text-motion pairs. Furthermore, prior methods typically convert motion into discrete representations via quantization to integrate with language models, introducing substantial jitter artifacts from discrete tokenization. To address these challenges, we propose LLaMo, a unified framework that extends pretrained LLMs through a modality-specific Mixture-of-Transformers (MoT) architecture. This design inherently preserves the language understanding of the base model while enabling scalable multimodal adaptation. We encode human motion into a causal continuous latent space and maintain the next-token prediction paradigm in the decoder-only backbone through a lightweight flow-matching head, allowing for streaming motion generation in real-time (>30 FPS). Leveraging the comprehensive language understanding of pretrained LLMs and large-scale motion-text pretraining, our experiments demonstrate that LLaMo achieves high-fidelity text-to-motion generation and motion-to-text captioning in general settings, especially zero-shot motion generation, marking a significant step towards a general unified motion-language large model.</li>
<li><strong>摘要：</strong>大型模型的最新进展导致统一多模态生成和理解方面取得了重大进展。然而，统一运动语言生成和理解的模型的开发在很大程度上仍未得到充分探索。现有的方法通常在成对的运动文本数据上微调大型语言模型（LLM），由于可用的文本运动对的规模有限，这可能会导致语言能力的灾难性遗忘。此外，现有方法通常通过量化将运动转换为离散表示以与语言模型集成，从而引入来自离散标记化的大量抖动伪影。为了应对这些挑战，我们提出了 LLaMo，这是一个统一的框架，通过特定模态的混合变压器 (MoT) 架构扩展预训练的 LLM。这种设计本质上保留了基本模型的语言理解，同时实现了可扩展的多模式适应。我们将人体运动编码到因果连续潜在空间中，并通过轻量级流匹配头在仅解码器主干中维护下一个令牌预测范例，从而允许实时流式运动生成（> 30 FPS）。利用预训练LLM的全面语言理解和大规模运动文本预训练，我们的实验表明LLaMo在一般设置下实现了高保真文本到运动生成和运动到文本字幕，特别是零样本运动生成，标志着向通用统一运动语言大型模型迈出了重要一步。</li>
</ul>

<h3>Title: Synthetic Image Detection with CLIP: Understanding and Assessing Predictive Cues</h3>
<ul>
<li><strong>Authors: </strong>Marco Willi, Melanie Mathys, Michael Graber</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12381">https://arxiv.org/abs/2602.12381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12381">https://arxiv.org/pdf/2602.12381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12381]] Synthetic Image Detection with CLIP: Understanding and Assessing Predictive Cues(https://arxiv.org/abs/2602.12381)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent generative models produce near-photorealistic images, challenging the trustworthiness of photographs. Synthetic image detection (SID) has thus become an important area of research. Prior work has highlighted how synthetic images differ from real photographs--unfortunately, SID methods often struggle to generalize to novel generative models and often perform poorly in practical settings. CLIP, a foundational vision-language model which yields semantically rich image-text embeddings, shows strong accuracy and generalization for SID. Yet, the underlying relevant cues embedded in CLIP-features remain unknown. It is unclear, whether CLIP-based detectors simply detect strong visual artifacts or exploit subtle semantic biases, both of which would render them useless in practical settings or on generative models of high quality. We introduce SynthCLIC, a paired dataset of real photographs and high-quality synthetic counterparts from recent diffusion models, designed to reduce semantic bias in SID. Using an interpretable linear head with de-correlated activations and a text-grounded concept-model, we analyze what CLIP-based detectors learn. CLIP-based linear detectors reach 0.96 mAP on a GAN-based benchmark but only 0.92 on our high-quality diffusion dataset SynthCLIC, and generalization across generator families drops to as low as 0.37 mAP. We find that the detectors primarily rely on high-level photographic attributes (e.g., minimalist style, lens flare, or depth layering), rather than overt generator-specific artifacts. CLIP-based detectors perform well overall but generalize unevenly across diverse generative architectures. This highlights the need for continual model updates and broader training exposure, while reinforcing CLIP-based approaches as a strong foundation for more universal, robust SID.</li>
<li><strong>摘要：</strong>最近的生成模型产生近乎真实的图像，挑战了照片的可信度。合成图像检测（SID）因此成为一个重要的研究领域。之前的工作强调了合成图像与真实照片的不同之处——不幸的是，SID 方法通常很难推广到新颖的生成模型，并且在实际环境中往往表现不佳。 CLIP 是一种基础视觉语言模型，可产生语义丰富的图像文本嵌入，显示出 SID 的强大准确性和泛化性。然而，CLIP 特征中嵌入的潜在相关线索仍然未知。目前尚不清楚基于 CLIP 的检测器是否只是简单地检测强烈的视觉伪影或利用微妙的语义偏差，这两者都会使它们在实际设置或高质量的生成模型中毫无用处。我们引入了 SynthCLIC，这是一个由真实照片和来自最新扩散模型的高质量合成对应物组成的配对数据集，旨在减少 SID 中的语义偏差。使用具有去相关激活的可解释线性头和基于文本的概念模型，我们分析了基于 CLIP 的检测器学习的内容。基于 CLIP 的线性检测器在基于 GAN 的基准上达到 0.96 mAP，但在我们的高质量扩散数据集 SynthCLIC 上仅为 0.92，并且跨生成器系列的泛化率降至低至 0.37 mAP。我们发现探测器主要依赖于高级摄影属性（例如，简约风格、镜头光晕或深度分层），而不是明显的特定于生成器的伪影。基于 CLIP 的检测器总体表现良好，但在不同的生成架构中泛化能力不均衡。这凸显了持续模型更新和更广泛的培训机会的需要，同时加强基于 CLIP 的方法作为更普遍、更强大的 SID 的坚实基础。</li>
</ul>

<h3>Title: Synthetic Interaction Data for Scalable Personalization in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Ma, Yue Huang, Wenjie Wang, Xiaonan Luo, Xiangliang Zhang, Stefan Feuerriegel</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12394">https://arxiv.org/abs/2602.12394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12394">https://arxiv.org/pdf/2602.12394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12394]] Synthetic Interaction Data for Scalable Personalization in Large Language Models(https://arxiv.org/abs/2602.12394)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Personalized prompting offers large opportunities for deploying large language models (LLMs) to diverse users, yet existing prompt optimization methods primarily focus on task-level optimization while largely overlooking user-specific preferences and latent constraints of individual users. This gap is primarily due to (i) the absence of high-quality, privacy-sensitive data that capture personalized user-LLM interactions at scale, and (ii) the lack of robust reward signals for individual preferences. To overcome existing data limitations, we introduce a high-fidelity synthetic data generation framework called PersonaGym. Unlike prior work that treats personalization as static persona-preference pairs, PersonaGym models a dynamic preference process via an agentic LLM system to simulate realistic preference behaviors and semantic-aware noise in order to generate personalized multi-turn interaction trajectories. Using PersonaGym, we release PersonaAtlas, a large-scale, high-quality, and diverse synthetic dataset of high-fidelity multi-turn personalized interaction trajectories that closely mirror real-world preference expression and noise patterns. We further propose Personalized Prompt Optimization (PPOpt), a scalable and model-agnostic framework that optimizes user prompts based on interaction histories without modifying the deployed LLM. PPOpt adopts a reason-then-optimize paradigm that infers an explicit user profile and conditions prompt rewriting on the user profile to avoid reward hacking. Our training procedure for PPOpt integrates a cold-start supervised prior with outcome-driven multi-objective reinforcement learning. We present extensive experiments to demonstrate consistent improvements over state-of-the-art baselines in terms of task performance, personalization quality, and robustness to noisy as well as to sparse preference signals.</li>
<li><strong>摘要：</strong>个性化提示为向不同用户部署大型语言模型（LLM）提供了巨大的机会，但现有的提示优化方法主要侧重于任务级优化，而在很大程度上忽视了用户特定的偏好和个人用户的潜在约束。这种差距主要是由于（i）缺乏大规模捕获个性化用户与法学硕士互动的高质量、隐私敏感数据，以及（ii）缺乏针对个人偏好的强大奖励信号。为了克服现有的数据限制，我们引入了一种名为 PersonaGym 的高保真合成数据生成框架。与之前将个性化视为静态角色偏好对的工作不同，PersonaGym 通过代理 LLM 系统对动态偏好过程进行建模，以模拟现实的偏好行为和语义感知噪声，从而生成个性化的多轮交互轨迹。使用 PersonaGym，我们发布了 PersonaAtlas，这是一个大规模、高质量和多样化的高保真多轮个性化交互轨迹合成数据集，密切反映了现实世界的偏好表达和噪声模式。我们进一步提出个性化提示优化（PPOpt），这是一个可扩展且与模型无关的框架，可根据交互历史优化用户提示，而无需修改已部署的 LLM。 PPOpt 采用先推理再优化的范例，推断出明确的用户配置文件，并在用户配置文件上提示重写以避免奖励黑客攻击。我们的 PPOpt 训练程序将冷启动监督先验与结果驱动的多目标强化学习相结合。我们进行了广泛的实验，以证明在任务性能、个性化质量以及对噪声和稀疏偏好信号的鲁棒性方面相对于最先进的基线的持续改进。</li>
</ul>

<h3>Title: ZeroDiff++: Substantial Unseen Visual-semantic Correlation in Zero-shot Learning</h3>
<ul>
<li><strong>Authors: </strong>Zihan Ye, Shreyank N Gowda, Kaile Du, Weijian Luo, Ling Shao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12401">https://arxiv.org/abs/2602.12401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12401">https://arxiv.org/pdf/2602.12401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12401]] ZeroDiff++: Substantial Unseen Visual-semantic Correlation in Zero-shot Learning(https://arxiv.org/abs/2602.12401)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Zero-shot Learning (ZSL) enables classifiers to recognize classes unseen during training, commonly via generative two stage methods: (1) learn visual semantic correlations from seen classes; (2) synthesize unseen class features from semantics to train classifiers. In this paper, we identify spurious visual semantic correlations in existing generative ZSL worsened by scarce seen class samples and introduce two metrics to quantify spuriousness for seen and unseen classes. Furthermore, we point out a more critical bottleneck: existing unadaptive fully noised generators produce features disconnected from real test samples, which also leads to the spurious correlation. To enhance the visual-semantic correlations on both seen and unseen classes, we propose ZeroDiff++, a diffusion-based generative framework. In training, ZeroDiff++ uses (i) diffusion augmentation to produce diverse noised samples, (ii) supervised contrastive (SC) representations for instance level semantics, and (iii) multi view discriminators with Wasserstein mutual learning to assess generated features. At generation time, we introduce (iv) Diffusion-based Test time Adaptation (DiffTTA) to adapt the generator using pseudo label reconstruction, and (v) Diffusion-based Test time Generation (DiffGen) to trace the diffusion denoising path and produce partially synthesized features that connect real and generated data, and mitigates data scarcity further. Extensive experiments on three ZSL benchmarks demonstrate that ZeroDiff++ not only achieves significant improvements over existing ZSL methods but also maintains robust performance even with scarce training data. Code would be available.</li>
<li><strong>摘要：</strong>零样本学习（ZSL）使分类器能够识别训练期间未见过的类，通常通过生成两阶段方法：（1）从已见的类中学习视觉语义相关性； (2)从语义上综合未见过的类特征来训练分类器。在本文中，我们识别了现有生成 ZSL 中因可见类样本稀缺而恶化的虚假视觉语义相关性，并引入了两个指标来量化可见类和未见类的虚假性。此外，我们指出了一个更关键的瓶颈：现有的非自适应全噪声生成器产生与真实测试样本脱节的特征，这也导致了虚假相关性。为了增强可见类和未见类的视觉语义相关性，我们提出了 ZeroDiff++，一种基于扩散的生成框架。在训练中，ZeroDiff++ 使用 (i) 扩散增强来生成不同的噪声样本，(ii) 用于实例级别语义的监督对比 (SC) 表示，以及 (iii) 具有 Wasserstein 相互学习的多视图鉴别器来评估生成的特征。在生成时，我们引入（iv）基于扩散的测试时间适应（DiffTTA）以使用伪标签重建来适应生成器，以及（v）基于扩散的测试时间生成（DiffGen）以跟踪扩散去噪路径并生成连接真实数据和生成数据的部分合成特征，并进一步缓解数据稀缺性。对三个 ZSL 基准的大量实验表明，ZeroDiff++ 不仅比现有 ZSL 方法取得了显着改进，而且即使在训练数据稀缺的情况下也能保持稳健的性能。代码将可用。</li>
</ul>

<h3>Title: AstRL: Analog and Mixed-Signal Circuit Synthesis with Deep Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Felicia B. Guo, Ken T. Ho, Andrei Vladimirescu, Borivoje Nikolic</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12402">https://arxiv.org/abs/2602.12402</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12402">https://arxiv.org/pdf/2602.12402</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12402]] AstRL: Analog and Mixed-Signal Circuit Synthesis with Deep Reinforcement Learning(https://arxiv.org/abs/2602.12402)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Analog and mixed-signal (AMS) integrated circuits (ICs) lie at the core of modern computing and communications systems. However, despite the continued rise in design complexity, advances in AMS automation remain limited. This reflects the central challenge in developing a generalized optimization method applicable across diverse circuit design spaces, many of which are distinct, constrained, and non-differentiable. To address this, our work casts circuit design as a graph generation problem and introduces a novel method of AMS synthesis driven by deep reinforcement learning (AstRL). Based on a policy-gradient approach, AstRL generates circuits directly optimized for user-specified targets within a simulator-embedded environment that provides ground-truth feedback during training. Through behavioral-cloning and discriminator-based similarity rewards, our method demonstrates, for the first time, an expert-aligned paradigm for generalized circuit generation validated in simulation. Importantly, the proposed approach operates at the level of individual transistors, enabling highly expressive, fine-grained topology generation. Strong inductive biases encoded in the action space and environment further drive structurally consistent and valid generation. Experimental results for three realistic design tasks illustrate substantial improvements in conventional design metrics over state-of-the-art baselines, with 100% of generated designs being structurally correct and over 90% demonstrating required functionality.</li>
<li><strong>摘要：</strong>模拟和混合信号 (AMS) 集成电路 (IC) 是现代计算和通信系统的核心。然而，尽管设计复杂性持续上升，AMS 自动化的进步仍然有限。这反映了开发适用于不同电路设计空间的通用优化方法的核心挑战，其中许多电路设计空间是独特的、受限的和不可微的。为了解决这个问题，我们的工作将电路设计视为图形生成问题，并引入了一种由深度强化学习（AstRL）驱动的 AMS 合成新方法。基于策略梯度方法，AstRL 在模拟器嵌入式环境中生成直接针对用户指定目标进行优化的电路，该环境在训练期间提供地面实况反馈。通过行为克隆和基于判别器的相似性奖励，我们的方法首次展示了在模拟中验证的广义电路生成的专家一致范式。重要的是，所提出的方法在单个晶体管的级别上运行，从而能够生成具有高度表现力的细粒度拓扑。行动空间和环境中编码的强归纳偏差进一步推动结构一致和有效的生成。三个实际设计任务的实验结果表明，传统设计指标相对于最先进的基准有显着改进，生成的设计 100% 结构正确，超过 90% 展示了所需的功能。</li>
</ul>

<h3>Title: Continuous Diffusion Models Can Obey Formal Syntax</h3>
<ul>
<li><strong>Authors: </strong>Jinwoo Kim, Taylor Berg-Kirkpatrick, Loris D'Antoni</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.FL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12468">https://arxiv.org/abs/2602.12468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12468">https://arxiv.org/pdf/2602.12468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12468]] Continuous Diffusion Models Can Obey Formal Syntax(https://arxiv.org/abs/2602.12468)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion language models offer a promising alternative to autoregressive models due to their global, non-causal generation process, but their continuous latent dynamics make discrete constraints -- e.g., the output should be a JSON file that matches a given schema -- difficult to impose. We introduce a training-free guidance method for steering continuous diffusion language models to satisfy formal syntactic constraints expressed using regular expressions. Our approach constructs an analytic score estimating the probability that a latent state decodes to a valid string accepted by a given regular expression, and uses its gradient to guide sampling, without training auxiliary classifiers. The denoising process targets the base model conditioned on syntactic validity. We implement our method in Diffinity on top of the PLAID diffusion model and evaluate it on 180 regular-expression constraints over JSON and natural-language benchmarks. Diffinity achieves 68-96\% constraint satisfaction while incurring only a small perplexity cost relative to unconstrained sampling, outperforming autoregressive constrained decoding in both constraint satisfaction and output quality.</li>
<li><strong>摘要：</strong>由于其全局、非因果生成过程，扩散语言模型为自回归模型提供了一种有前途的替代方案，但其连续的潜在动态使得离散约束（例如，输出应该是与给定模式匹配的 JSON 文件）难以施加。我们引入了一种免训练的引导方法，用于引导连续扩散语言模型，以满足使用正则表达式表达的形式语法约束。我们的方法构建了一个分析分数，估计潜在状态解码为给定正则表达式接受的有效字符串的概率，并使用其梯度来指导采样，而无需训练辅助分类器。去噪过程以句法有效性为条件的基础模型为目标。我们在 PLAID 扩散模型之上的 Diffinity 中实现我们的方法，并在 JSON 和自然语言基准的 180 个正则表达式约束上对其进行评估。 Diffinity 实现了 68-96% 的约束满足度，同时相对于无约束采样仅产生很小的困惑成本，在约束满足度和输出质量方面均优于自回归约束解码。</li>
</ul>

<h3>Title: Designing RNAs with Language Models</h3>
<ul>
<li><strong>Authors: </strong>Milan Gautam, Ning Dai, Tianshuo Zhou, Bowen Xie, David Mathews, Liang Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12470">https://arxiv.org/abs/2602.12470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12470">https://arxiv.org/pdf/2602.12470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12470]] Designing RNAs with Language Models(https://arxiv.org/abs/2602.12470)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>RNA design, the task of finding a sequence that folds into a target secondary structure, has broad biological and biomedical impact but remains computationally challenging due to the exponentially large sequence space and exponentially many competing folds. Traditional approaches treat it as an optimization problem, relying on per-instance heuristics or constraint-based search. We instead reframe RNA design as conditional sequence generation and introduce a reusable neural approximator, instantiated as an autoregressive language model (LM), that maps target structures directly to sequences. We first train our model in a supervised setting on random-induced structure-sequence pairs, and then use reinforcement learning (RL) to optimize end-to-end metrics. We also propose methods to select a small subset for RL that greatly improves RL efficiency and quality. Across four datasets, our approach outperforms state-of-the-art systems on key metrics such as Boltzmann probability while being 1.7x faster, establishing conditional LM generation as a scalable, task-agnostic alternative to per-instance optimization for RNA design. Our code and data are available at this https URL.</li>
<li><strong>摘要：</strong>RNA 设计是寻找折叠成目标二级结构的序列的任务，具有广泛的生物学和生物医学影响，但由于指数级大的序列空间和指数级许多竞争折叠，在计算上仍然具有挑战性。传统方法将其视为优化问题，依赖于每个实例的启发式或基于约束的搜索。相反，我们将 RNA 设计重新构建为条件序列生成，并引入可重用的神经逼近器，实例化为自回归语言模型 (LM)，将目标结构直接映射到序列。我们首先在随机诱导的结构序列对的监督环境中训练我们的模型，然后使用强化学习（RL）来优化端到端指标。我们还提出了为强化学习选择一个小子集的方法，这大大提高了强化学习的效率和质量。在四个数据集上，我们的方法在玻尔兹曼概率等关键指标上优于最先进的系统，同时速度提高了 1.7 倍，将条件 LM 生成建立为可扩展的、与任务无关的替代方案，以替代 RNA 设计的每个实例优化。我们的代码和数据可在此 https URL 中获取。</li>
</ul>

<h3>Title: AMPS: Adaptive Modality Preference Steering via Functional Entropy</h3>
<ul>
<li><strong>Authors: </strong>Zihan Huang, Xintong Li, Rohan Surana, Tong Yu, Rui Wang, Julian McAuley, Jingbo Shang, Junda Wu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12533">https://arxiv.org/abs/2602.12533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12533">https://arxiv.org/pdf/2602.12533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12533]] AMPS: Adaptive Modality Preference Steering via Functional Entropy(https://arxiv.org/abs/2602.12533)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) often exhibit significant modality preference, which is a tendency to favor one modality over another. Depending on the input, they may over-rely on linguistic priors relative to visual evidence, or conversely over-attend to visually salient but facts in textual contexts. Prior work has applied a uniform steering intensity to adjust the modality preference of MLLMs. However, strong steering can impair standard inference and increase error rates, whereas weak steering is often ineffective. In addition, because steering sensitivity varies substantially across multimodal instances, a single global strength is difficult to calibrate. To address this limitation with minimal disruption to inference, we introduce an instance-aware diagnostic metric that quantifies each modality's information contribution and reveals sample-specific susceptibility to steering. Building on these insights, we propose a scaling strategy that reduces steering for sensitive samples and a learnable module that infers scaling patterns, enabling instance-aware control of modality preference. Experimental results show that our instance-aware steering outperforms conventional steering in modulating modality preference, achieving effective adjustment while keeping generation error rates low.</li>
<li><strong>摘要：</strong>多模态大语言模型 (MLLM) 通常表现出显着的模态偏好，即倾向于一种模态而不是另一种模态。根据输入的不同，他们可能会过度依赖相对于视觉证据的语言先验，或者相反，过度关注视觉上显着但文本上下文中的事实。先前的工作应用了统一的转向强度来调整 MLLM 的模态偏好。然而，强指导会损害标准推理并增加错误率，而弱指导通常是无效的。此外，由于转向灵敏度在多模式实例中差异很大，因此很难校准单一的全局强度。为了以最小化对推理的干扰来解决这一限制，我们引入了一种实例感知诊断指标，该指标可以量化每种模态的信息贡献并揭示样本特定的转向敏感性。基于这些见解，我们提出了一种可减少敏感样本引导的扩展策略，以及一个可推断扩展模式的可学习模块，从而实现对模态偏好的实例感知控制。实验结果表明，我们的实例感知转向在调制模态偏好方面优于传统转向，实现了有效的调整，同时保持较低的生成错误率。</li>
</ul>

<h3>Title: The Constant Eye: Benchmarking and Bridging Appearance Robustness in Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Jiabao Wang, Hongyu Zhou, Yuanbo Yang, Jiahao Shao, Yiyi Liao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12563">https://arxiv.org/abs/2602.12563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12563">https://arxiv.org/pdf/2602.12563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12563]] The Constant Eye: Benchmarking and Bridging Appearance Robustness in Autonomous Driving(https://arxiv.org/abs/2602.12563)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Despite rapid progress, autonomous driving algorithms remain notoriously fragile under Out-of-Distribution (OOD) conditions. We identify a critical decoupling failure in current research: the lack of distinction between appearance-based shifts, such as weather and lighting, and structural scene changes. This leaves a fundamental question unanswered: Is the planner failing because of complex road geometry, or simply because it is raining? To resolve this, we establish navdream, a high-fidelity robustness benchmark leveraging generative pixel-aligned style transfer. By creating a visual stress test with negligible geometric deviation, we isolate the impact of appearance on driving performance. Our evaluation reveals that existing planning algorithms often show significant degradation under OOD appearance conditions, even when the underlying scene structure remains consistent. To bridge this gap, we propose a universal perception interface leveraging a frozen visual foundation model (DINOv3). By extracting appearance-invariant features as a stable interface for the planner, we achieve exceptional zero-shot generalization across diverse planning paradigms, including regression-based, diffusion-based, and scoring-based models. Our plug-and-play solution maintains consistent performance across extreme appearance shifts without requiring further fine-tuning. The benchmark and code will be made available.</li>
<li><strong>摘要：</strong>尽管进展迅速，但自动驾驶算法在分布外（OOD）条件下仍然脆弱。我们发现当前研究中一个严重的脱钩失败：基于外观的变化（例如天气和照明）与结构场景变化之间缺乏区别。这就留下了一个没有答案的基本问题：规划者的失败是因为复杂的道路几何形状，还是仅仅因为下雨？为了解决这个问题，我们建立了 navdream，一个利用生成像素对齐风格传输的高保真鲁棒性基准。通过创建几何偏差可忽略不计的视觉压力测试，我们隔离了外观对驾驶性能的影响。我们的评估表明，即使底层场景结构保持一致，现有的规划算法在 OOD 外观条件下通常也会表现出显着的退化。为了弥补这一差距，我们提出了一种利用冻结视觉基础模型（DINOv3）的通用感知界面。通过提取外观不变特征作为规划器的稳定接口，我们在不同的规划范式（包括基于回归、基于扩散和基于评分的模型）中实现了出色的零样本泛化。我们的即插即用解决方案可在极端的外观变化下保持一致的性能，无需进一步微调。基准测试和代码将可供使用。</li>
</ul>

<h3>Title: Formalizing the Sampling Design Space of Diffusion-Based Generative Models via Adaptive Solvers and Wasserstein-Bounded Timesteps</h3>
<ul>
<li><strong>Authors: </strong>Sangwoo Jo, Sungjoon Choi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12624">https://arxiv.org/abs/2602.12624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12624">https://arxiv.org/pdf/2602.12624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12624]] Formalizing the Sampling Design Space of Diffusion-Based Generative Models via Adaptive Solvers and Wasserstein-Bounded Timesteps(https://arxiv.org/abs/2602.12624)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion-based generative models have achieved remarkable performance across various domains, yet their practical deployment is often limited by high sampling costs. While prior work focuses on training objectives or individual solvers, the holistic design of sampling, specifically solver selection and scheduling, remains dominated by static heuristics. In this work, we revisit this challenge through a geometric lens, proposing SDM, a principled framework that aligns the numerical solver with the intrinsic properties of the diffusion trajectory. By analyzing the ODE dynamics, we show that efficient low-order solvers suffice in early high-noise stages while higher-order solvers can be progressively deployed to handle the increasing non-linearity of later stages. Furthermore, we formalize the scheduling by introducing a Wasserstein-bounded optimization framework. This method systematically derives adaptive timesteps that explicitly bound the local discretization error, ensuring the sampling process remains faithful to the underlying continuous dynamics. Without requiring additional training or architectural modifications, SDM achieves state-of-the-art performance across standard benchmarks, including an FID of 1.93 on CIFAR-10, 2.41 on FFHQ, and 1.98 on AFHQv2, with a reduced number of function evaluations compared to existing samplers. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>基于扩散的生成模型在各个领域都取得了显着的性能，但其实际部署往往受到高采样成本的限制。虽然之前的工作侧重于训练目标或单个求解器，但采样的整体设计，特别是求解器的选择和调度，仍然以静态启发式为主。在这项工作中，我们通过几何透镜重新审视这一挑战，提出了 SDM，这是一种将数值求解器与扩散轨迹的内在属性结合起来的原则框架。通过分析 ODE 动力学，我们表明高效的低阶求解器足以满足早期的高噪声阶段，而可以逐步部署高阶求解器来处理后期阶段不断增加的非线性。此外，我们通过引入 Wasserstein 有界优化框架来形式化调度。该方法系统地导出显式限制局部离散化误差的自适应时间步长，确保采样过程忠实于底层的连续动态。无需额外的培训或架构修改，SDM 即可在标准基准测试中实现最先进的性能，包括 CIFAR-10 上的 FID 为 1.93、FFHQ 上的 2.41 和 AFHQv2 上的 1.98，并且与现有采样器相比，功能评估数量减少。我们的代码可以在这个 https URL 上找到。</li>
</ul>

<h3>Title: Dual-Granularity Contrastive Reward via Generated Episodic Guidance for Efficient Embodied RL</h3>
<ul>
<li><strong>Authors: </strong>Xin Liu, Yixuan Li, Yuhui Chen, Yuxing Qin, Haoran Li, Dongbin Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12636">https://arxiv.org/abs/2602.12636</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12636">https://arxiv.org/pdf/2602.12636</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12636]] Dual-Granularity Contrastive Reward via Generated Episodic Guidance for Efficient Embodied RL(https://arxiv.org/abs/2602.12636)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Designing suitable rewards poses a significant challenge in reinforcement learning (RL), especially for embodied manipulation. Trajectory success rewards are suitable for human judges or model fitting, but the sparsity severely limits RL sample efficiency. While recent methods have effectively improved RL via dense rewards, they rely heavily on high-quality human-annotated data or abundant expert supervision. To tackle these issues, this paper proposes Dual-granularity contrastive reward via generated Episodic Guidance (DEG), a novel framework to seek sample-efficient dense rewards without requiring human annotations or extensive supervision. Leveraging the prior knowledge of large video generation models, DEG only needs a small number of expert videos for domain adaptation to generate dedicated task guidance for each RL episode. Then, the proposed dual-granularity reward that balances coarse-grained exploration and fine-grained matching, will guide the agent to efficiently approximate the generated guidance video sequentially in the contrastive self-supervised latent space, and finally complete the target task. Extensive experiments on 18 diverse tasks across both simulation and real-world settings show that DEG can not only serve as an efficient exploration stimulus to help the agent quickly discover sparse success rewards, but also guide effective RL and stable policy convergence independently.</li>
<li><strong>摘要：</strong>设计合适的奖励对强化学习（RL）提出了重大挑战，特别是对于具体操作而言。轨迹成功奖励适合人类判断或模型拟合，但稀疏性严重限制了 RL 样本效率。虽然最近的方法通过密集奖励有效地改进了强化学习，但它们在很大程度上依赖于高质量的人工注释数据或丰富的专家监督。为了解决这些问题，本文提出了通过生成情景指导（DEG）的双粒度对比奖励，这是一种新颖的框架，可以在不需要人工注释或广泛监督的情况下寻求样本高效的密集奖励。利用大型视频生成模型的先验知识，DEG 只需要少量的专家视频进行领域适应即可为每个 RL 片段生成专用的任务指导。然后，所提出的平衡粗粒度探索和细粒度匹配的双粒度奖励将指导代理在对比自监督潜在空间中顺序有效地逼近生成的指导视频，并最终完成目标任务。对模拟和现实环境中 18 种不同任务进行的广泛实验表明，DEG 不仅可以作为有效的探索刺激来帮助智能体快速发现稀疏的成功奖励，而且还可以独立指导有效的强化学习和稳定的策略收敛。</li>
</ul>

<h3>Title: ImageRAGTurbo: Towards One-step Text-to-Image Generation with Retrieval-Augmented Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Peijie Qiu, Hariharan Ramshankar, Arnau Ramisa, René Vidal, Amit Kumar K C, Vamsi Salaka, Rahul Bhagat</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12640">https://arxiv.org/abs/2602.12640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12640">https://arxiv.org/pdf/2602.12640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12640]] ImageRAGTurbo: Towards One-step Text-to-Image Generation with Retrieval-Augmented Diffusion Models(https://arxiv.org/abs/2602.12640)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as the leading approach for text-to-image generation. However, their iterative sampling process, which gradually morphs random noise into coherent images, introduces significant latency that limits their applicability. While recent few-step diffusion models reduce the number of sampling steps to as few as one to four steps, they often compromise image quality and prompt alignment, especially in one-step generation. Additionally, these models require computationally expensive training procedures. To address these limitations, we propose ImageRAGTurbo, a novel approach to efficiently finetune few-step diffusion models via retrieval augmentation. Given a text prompt, we retrieve relevant text-image pairs from a database and use them to condition the generation process. We argue that such retrieved examples provide rich contextual information to the UNet denoiser that helps reduce the number of denoising steps without compromising image quality. Indeed, our initial investigations show that using the retrieved content to edit the denoiser's latent space ($\mathcal{H}$-space) without additional finetuning already improves prompt fidelity. To further improve the quality of the generated images, we augment the UNet denoiser with a trainable adapter in the $\mathcal{H}$-space, which efficiently blends the retrieved content with the target prompt using a cross-attention mechanism. Experimental results on fast text-to-image generation demonstrate that our approach produces high-fidelity images without compromising latency compared to existing methods.</li>
<li><strong>摘要：</strong>扩散模型已成为文本到图像生成的主要方法。然而，它们的迭代采样过程逐渐将随机噪声转变为相干图像，引入了显着的延迟，限制了它们的适用性。虽然最近的几步扩散模型将采样步骤数量减少到一到四步，但它们通常会损害图像质量并提示对齐，尤其是在一步生成中。此外，这些模型需要计算成本高昂的训练过程。为了解决这些限制，我们提出了 ImageRAGTurbo，这是一种通过检索增强有效微调少步扩散模型的新方法。给定文本提示，我们从数据库中检索相关的文本图像对，并使用它们来调节生成过程。我们认为，此类检索到的示例为 UNet 降噪器提供了丰富的上下文信息，有助于减少降噪步骤的数量，而不会影响图像质量。事实上，我们的初步调查表明，使用检索到的内容来编辑降噪器的潜在空间（$\mathcal{H}$-space）而无需额外的微调，已经提高了提示保真度。为了进一步提高生成图像的质量，我们在 $\mathcal{H}$ 空间中使用可训练的适配器来增强 UNet 降噪器，该适配器使用交叉注意机制有效地将检索到的内容与目标提示混合在一起。快速文本到图像生成的实验结果表明，与现有方法相比，我们的方法可以生成高保真图像，而不会影响延迟。</li>
</ul>

<h3>Title: SLA2: Sparse-Linear Attention with Learnable Routing and QAT</h3>
<ul>
<li><strong>Authors: </strong>Jintao Zhang, Haoxu Wang, Kai Jiang, Kaiwen Zheng, Youhe Jiang, Ion Stoica, Jianfei Chen, Jun Zhu, Joseph E. Gonzalez</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12675">https://arxiv.org/abs/2602.12675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12675">https://arxiv.org/pdf/2602.12675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12675]] SLA2: Sparse-Linear Attention with Learnable Routing and QAT(https://arxiv.org/abs/2602.12675)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Sparse-Linear Attention (SLA) combines sparse and linear attention to accelerate diffusion models and has shown strong performance in video generation. However, (i) SLA relies on a heuristic split that assigns computations to the sparse or linear branch based on attention-weight magnitude, which can be suboptimal. Additionally, (ii) after formally analyzing the attention error in SLA, we identify a mismatch between SLA and a direct decomposition into sparse and linear attention. We propose SLA2, which introduces (I) a learnable router that dynamically selects whether each attention computation should use sparse or linear attention, (II) a more faithful and direct sparse-linear attention formulation that uses a learnable ratio to combine the sparse and linear attention branches, and (III) a sparse + low-bit attention design, where low-bit attention is introduced via quantization-aware fine-tuning to reduce quantization error. Experiments show that on video diffusion models, SLA2 can achieve 97% attention sparsity and deliver an 18.6x attention speedup while preserving generation quality.</li>
<li><strong>摘要：</strong>稀疏线性注意力（SLA）结合了稀疏和线性注意力来加速扩散模型，并在视频生成中表现出了强大的性能。然而，(i) SLA 依赖于启发式分割，该分割根据注意力权重大小将计算分配给稀疏或线性分支，这可能不是最优的。此外，(ii) 在正式分析 SLA 中的注意力误差后，我们发现 SLA 与直接分解为稀疏和线性注意力之间存在不匹配。我们提出SLA2，它引入了（I）一个可学习的路由器，动态选择每个注意力计算是否应该使用稀疏或线性注意力，（II）一种更忠实和直接的稀疏线性注意力公式，使用可学习的比率来组合稀疏和线性注意力分支，以及（III）稀疏+低位注意力设计，其中通过量化感知微调引入低位注意力以减少量化误差。实验表明，在视频扩散模型上，SLA2 可以实现 97% 的注意力稀疏度，并在保持生成质量的同时提供 18.6 倍的注意力加速。</li>
</ul>

<h3>Title: Motion Prior Distillation in Time Reversal Sampling for Generative Inbetweening</h3>
<ul>
<li><strong>Authors: </strong>Wooseok Jeon, Seunghyun Shin, Dongmin Shin, Hae-Gon Jeon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12679">https://arxiv.org/abs/2602.12679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12679">https://arxiv.org/pdf/2602.12679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12679]] Motion Prior Distillation in Time Reversal Sampling for Generative Inbetweening(https://arxiv.org/abs/2602.12679)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent progress in image-to-video (I2V) diffusion models has significantly advanced the field of generative inbetweening, which aims to generate semantically plausible frames between two keyframes. In particular, inference-time sampling strategies, which leverage the generative priors of large-scale pre-trained I2V models without additional training, have become increasingly popular. However, existing inference-time sampling, either fusing forward and backward paths in parallel or alternating them sequentially, often suffers from temporal discontinuities and undesirable visual artifacts due to the misalignment between the two generated paths. This is because each path follows the motion prior induced by its own conditioning frame. In this work, we propose Motion Prior Distillation (MPD), a simple yet effective inference-time distillation technique that suppresses bidirectional mismatch by distilling the motion residual of the forward path into the backward path. Our method can deliberately avoid denoising the end-conditioned path which causes the ambiguity of the path, and yield more temporally coherent inbetweening results with the forward motion prior. We not only perform quantitative evaluations on standard benchmarks, but also conduct extensive user studies to demonstrate the effectiveness of our approach in practical scenarios.</li>
<li><strong>摘要：</strong>图像到视频（I2V）扩散模型的最新进展显着推进了生成中间领域，其目的是在两个关键帧之间生成语义上合理的帧。特别是，推理时间采样策略，利用大规模预训练 I2V 模型的生成先验，无需额外训练，已经变得越来越流行。然而，现有的推理时间采样，无论是并行融合前向和后向路径还是顺序交替，通常会由于两个生成路径之间的未对准而遭受时间不连续性和不良视觉伪影的影响。这是因为每条路径都遵循由其自身的调节框架引起的先验运动。在这项工作中，我们提出了运动先验蒸馏（MPD），这是一种简单而有效的推理时间蒸馏技术，通过将前向路径的运动残差蒸馏到后向路径来抑制双向失配。我们的方法可以故意避免对最终条件路径进行去噪，这会导致路径的模糊性，并产生与前向运动先验在时间上更加一致的中间结果。我们不仅对标准基准进行定量评估，还进行广泛的用户研究，以证明我们的方法在实际场景中的有效性。</li>
</ul>

<h3>Title: Flow Matching from Viewpoint of Proximal Operators</h3>
<ul>
<li><strong>Authors: </strong>Kenji Fukumizu, Wei Huang, Han Bao, Shuntuo Xu, Nisha Chandramoothy</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12683">https://arxiv.org/abs/2602.12683</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12683">https://arxiv.org/pdf/2602.12683</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12683]] Flow Matching from Viewpoint of Proximal Operators(https://arxiv.org/abs/2602.12683)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We reformulate Optimal Transport Conditional Flow Matching (OT-CFM), a class of dynamical generative models, showing that it admits an exact proximal formulation via an extended Brenier potential, without assuming that the target distribution has a density. In particular, the mapping to recover the target point is exactly given by a proximal operator, which yields an explicit proximal expression of the vector field. We also discuss the convergence of minibatch OT-CFM to the population formulation as the batch size increases. Finally, using second epi-derivatives of convex potentials, we prove that, for manifold-supported targets, OT-CFM is terminally normally hyperbolic: after time rescaling, the dynamics contracts exponentially in directions normal to the data manifold while remaining neutral along tangential directions.</li>
<li><strong>摘要：</strong>我们重新表述了最优传输条件流匹配（OT-CFM），这是一类动态生成模型，表明它通过扩展的布雷尼尔势允许精确的近端公式，而不假设目标分布具有密度。特别是，恢复目标点的映射是由近端算子精确给出的，这会产生矢量场的显式近端表达式。我们还讨论了随着批量大小的增加，小批量 OT-CFM 与群体公式的收敛。最后，使用凸势的二阶外延导数，我们证明，对于流形支持的目标，OT-CFM 通常是终端双曲线：在时间重新缩放后，动力学在垂直于数据流形的方向上呈指数收缩，同时沿切线方向保持中性。</li>
</ul>

<h3>Title: QTabGAN: A Hybrid Quantum-Classical GAN for Tabular Data Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Subhangi Kumari, Rakesh Achutha, Vignesh Sivaraman</a></li>
<li><strong>Subjects: </strong>cs.LG, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12704">https://arxiv.org/abs/2602.12704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12704">https://arxiv.org/pdf/2602.12704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12704]] QTabGAN: A Hybrid Quantum-Classical GAN for Tabular Data Synthesis(https://arxiv.org/abs/2602.12704)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Synthesizing realistic tabular data is challenging due to heterogeneous feature types and high dimensionality. We introduce QTabGAN, a hybrid quantum-classical generative adversarial framework for tabular data synthesis. QTabGAN is especially designed for settings where real data are scarce or restricted by privacy constraints. The model exploits the expressive power of quantum circuits to learn complex data distributions, which are then mapped to tabular features using classical neural networks. We evaluate QTabGAN on multiple classification and regression datasets and benchmark it against leading state-of-the-art generative models. Experiments show that QTabGAN achieves up to 54.07% improvement across various classification datasets and evaluation metrics, thus establishing a scalable quantum approach to tabular data synthesis and highlighting its potential for quantum-assisted generative modelling.</li>
<li><strong>摘要：</strong>由于异构特征类型和高维度，合成真实的表格数据具有挑战性。我们介绍 QTabGAN，一种用于表格数据合成的混合量子经典生成对抗框架。 QTabGAN 专为真实数据稀缺或受隐私限制的环境而设计。该模型利用量子电路的表达能力来学习复杂的数据分布，然后使用经典神经网络将其映射到表格特征。我们在多个分类和回归数据集上评估 QTabGAN，并将其与领先的最先进的生成模型进行基准测试。实验表明，QTabGAN 在各种分类数据集和评估指标上实现了高达 54.07% 的改进，从而建立了一种可扩展的表格数据合成量子方法，并凸显了其在量子辅助生成建模方面的潜力。</li>
</ul>

<h3>Title: ADEPT: RL-Aligned Agentic Decoding of Emotion via Evidence Probing Tools -- From Consensus Learning to Ambiguity-Driven Emotion Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Esther Sun, Bo-Hao Su, Abinay Reddy Naini, Shinji Watanabe, Carlos Busso</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12714">https://arxiv.org/abs/2602.12714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12714">https://arxiv.org/pdf/2602.12714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12714]] ADEPT: RL-Aligned Agentic Decoding of Emotion via Evidence Probing Tools -- From Consensus Learning to Ambiguity-Driven Emotion Reasoning(https://arxiv.org/abs/2602.12714)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Speech Large Language Models (SLLMs) enable high-level emotion reasoning but often produce ungrounded, text-biased judgments without verifiable acoustic evidence. In contrast, self-supervised speech encoders such as WavLM provide strong acoustic representations yet remain opaque discriminative models with limited interpretability. To bridge this gap, we introduce ADEPT (Agentic Decoding of Emotion via Evidence Probing Tools), a framework that reframes emotion recognition as a multi-turn inquiry process rather than a single-pass prediction. ADEPT transforms an SLLM into an agent that maintains an evolving candidate emotion set and adaptively invokes dedicated semantic and acoustic probing tools within a structured pipeline of candidate generation, evidence collection, and adjudication. Crucially, ADEPT enables a paradigm shift from consensus learning to ambiguity-driven emotion reasoning. Since human affect exhibits inherent complexity and frequent co-occurrence of emotions, we treat minority annotations as informative perceptual signals rather than discarding them as noise. Finally, we integrate Group Relative Policy Optimization (GRPO) with an Evidence Trust Gate to explicitly couple tool-usage behaviors with prediction quality and enforce evidence-grounded reasoning. Experiments show that ADEPT improves primary emotion accuracy in most settings while substantially improving minor emotion characterization, producing explanations grounded in auditable acoustic and semantic evidence.</li>
<li><strong>摘要：</strong>语音大语言模型 (SLLM) 可以实现高级情感推理，但通常会在没有可验证的声学证据的情况下产生没有根据的、有文本偏见的判断。相比之下，WavLM 等自监督语音编码器提供了强大的声学表示，但仍然是不透明的判别模型，可解释性有限。为了弥补这一差距，我们引入了 ADEPT（通过证据探测工具对情绪进行代理解码），这是一个将情绪识别重新构建为多轮查询过程而不是单通道预测的框架。 ADEPT 将 SLLM 转变为一个代理，该代理维护不断变化的候选者情绪集，并在候选者生成、证据收集和裁决的结构化管道中自适应地调用专用语义和声学探测工具。至关重要的是，ADEPT 实现了从共识学习到模糊驱动的情感推理的范式转变。由于人类情感表现出固有的复杂性和情感的频繁共现，因此我们将少数注释视为信息丰富的感知信号，而不是将它们视为噪音而丢弃。最后，我们将组相对策略优化（GRPO）与证据信任门集成，以明确地将工具使用行为与预测质量结合起来，并强制执行基于证据的推理。实验表明，ADEPT 在大多数情况下提高了主要情绪的准确性，同时大大改善了次要情绪的表征，产生基于可审计的声学和语义证据的解释。</li>
</ul>

<h3>Title: VimRAG: Navigating Massive Visual Context in Retrieval-Augmented Generation via Multimodal Memory Graph</h3>
<ul>
<li><strong>Authors: </strong>Qiuchen Wang, Shihang Wang, Yu Zeng, Qiang Zhang, Fanrui Zhang, Zhuoning Guo, Bosi Zhang, Wenxuan Huang, Lin Chen, Zehui Chen, Pengjun Xie, Ruixue Ding</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12735">https://arxiv.org/abs/2602.12735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12735">https://arxiv.org/pdf/2602.12735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12735]] VimRAG: Navigating Massive Visual Context in Retrieval-Augmented Generation via Multimodal Memory Graph(https://arxiv.org/abs/2602.12735)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Effectively retrieving, reasoning, and understanding multimodal information remains a critical challenge for agentic systems. Traditional Retrieval-augmented Generation (RAG) methods rely on linear interaction histories, which struggle to handle long-context tasks, especially those involving information-sparse yet token-heavy visual data in iterative reasoning scenarios. To bridge this gap, we introduce VimRAG, a framework tailored for multimodal Retrieval-augmented Reasoning across text, images, and videos. Inspired by our systematic study, we model the reasoning process as a dynamic directed acyclic graph that structures the agent states and retrieved multimodal evidence. Building upon this structured memory, we introduce a Graph-Modulated Visual Memory Encoding mechanism, with which the significance of memory nodes is evaluated via their topological position, allowing the model to dynamically allocate high-resolution tokens to pivotal evidence while compressing or discarding trivial clues. To implement this paradigm, we propose a Graph-Guided Policy Optimization strategy. This strategy disentangles step-wise validity from trajectory-level rewards by pruning memory nodes associated with redundant actions, thereby facilitating fine-grained credit assignment. Extensive experiments demonstrate that VimRAG consistently achieves state-of-the-art performance on diverse multimodal RAG benchmarks. The code is available at this https URL.</li>
<li><strong>摘要：</strong>有效检索、推理和理解多模态信息仍然是代理系统面临的关键挑战。传统的检索增强生成（RAG）方法依赖于线性交互历史，这很难处理长上下文任务，特别是那些在迭代推理场景中涉及信息稀疏但标记较多的视觉数据的任务。为了弥补这一差距，我们引入了 VimRAG，这是一个专为跨文本、图像和视频的多模态检索增强推理而定制的框架。受我们系统研究的启发，我们将推理过程建模为动态有向无环图，该图构建代理状态并检索多模态证据。在这种结构化记忆的基础上，我们引入了一种图形调制视觉记忆编码机制，通过记忆节点的拓扑位置来评估其重要性，允许模型动态地将高分辨率标记分配给关键证据，同时压缩或丢弃琐碎的线索。为了实现这一范式，我们提出了一种图引导策略优化策略。该策略通过修剪与冗余动作相关的内存节点，将逐步有效性与轨迹级奖励分开，从而促进细粒度的信用分配。大量实验表明，VimRAG 在各种多模式 RAG 基准测试中始终如一地实现了最先进的性能。该代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Synthetic Craquelure Generation for Unsupervised Painting Restoration</h3>
<ul>
<li><strong>Authors: </strong>Jana Cuch-Guillén, Antonio Agudo, Raül Pérez-Gonzalo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12742">https://arxiv.org/abs/2602.12742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12742">https://arxiv.org/pdf/2602.12742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12742]] Synthetic Craquelure Generation for Unsupervised Painting Restoration(https://arxiv.org/abs/2602.12742)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generation</a></li>
<li><strong>Abstract: </strong>Cultural heritage preservation increasingly demands non-invasive digital methods for painting restoration, yet identifying and restoring fine craquelure patterns from complex brushstrokes remains challenging due to scarce pixel-level annotations. We propose a fully annotation-free framework driven by a domain-specific synthetic craquelure generator, which simulates realistic branching and tapered fissure geometry using Bézier trajectories. Our approach couples a classical morphological detector with a learning-based refinement module: a SegFormer backbone adapted via Low-Rank Adaptation (LoRA). Uniquely, we employ a detector-guided strategy, injecting the morphological map as an input spatial prior, while a masked hybrid loss and logit adjustment constrain the training to focus specifically on refining candidate crack regions. The refined masks subsequently guide an Anisotropic Diffusion inpainting stage to reconstruct missing content. Experimental results demonstrate that our pipeline significantly outperforms state-of-the-art photographic restoration models in zero-shot settings, while faithfully preserving the original paint brushwork.</li>
<li><strong>摘要：</strong>文化遗产保护越来越需要非侵入性的数字方法来进行绘画修复，但由于像素级注释稀缺，从复杂的笔触中识别和恢复精细的裂纹图案仍然具有挑战性。我们提出了一个由特定领域的合成裂纹生成器驱动的完全无注释的框架，该框架使用贝塞尔轨迹模拟真实的分支和锥形裂隙几何形状。我们的方法将经典形态检测器与基于学习的细化模块结合起来：通过低阶适应（LoRA）适应的 SegFormer 主干。独特的是，我们采用检测器引导策略，将形态图作为输入空间先验注入，而掩码混合损失和对数调整限制训练专门针对细化候选裂纹区域。精致的蒙版随后引导各向异性扩散修复阶段来重建缺失的内容。实验结果表明，我们的流程在零镜头设置下显着优于最先进的摄影修复模型，同时忠实地保留了原始的绘画笔触。</li>
</ul>

<h3>Title: Closing the Loop: A Control-Theoretic Framework for Provably Stable Time Series Forecasting with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xingyu Zhang, Hanyun Du, Zeen Song, Jianqi Zhang, Changwen Zheng, Wenwen Qiang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12756">https://arxiv.org/abs/2602.12756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12756">https://arxiv.org/pdf/2602.12756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12756]] Closing the Loop: A Control-Theoretic Framework for Provably Stable Time Series Forecasting with LLMs(https://arxiv.org/abs/2602.12756)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have recently shown exceptional potential in time series forecasting, leveraging their inherent sequential reasoning capabilities to model complex temporal dynamics. However, existing approaches typically employ a naive autoregressive generation strategy. We identify a critical theoretical flaw in this paradigm: during inference, the model operates in an open-loop manner, consuming its own generated outputs recursively. This leads to inevitable error accumulation (exposure bias), where minor early deviations cascade into significant trajectory drift over long horizons. In this paper, we reformulate autoregressive forecasting through the lens of control theory, proposing \textbf{F-LLM} (Feedback-driven LLM), a novel closed-loop framework. Unlike standard methods that passively propagate errors, F-LLM actively stabilizes the trajectory via a learnable residual estimator (Observer) and a feedback controller. Furthermore, we provide a theoretical guarantee that our closed-loop mechanism ensures uniformly bounded error, provided the base model satisfies a local Lipschitz constraint. Extensive experiments demonstrate that F-LLM significantly mitigates error propagation, achieving good performance on time series benchmarks.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 最近在时间序列预测方面显示出巨大的潜力，利用其固有的顺序推理功能来模拟复杂的时间动态。然而，现有方法通常采用朴素的自回归生成策略。我们发现了该范式中的一个关键理论缺陷：在推理过程中，模型以开环方式运行，递归地消耗其自己生成的输出。这会导致不可避免的误差积累（曝光偏差），其中较小的早期偏差会在长期范围内累积成显着的轨迹漂移。在本文中，我们通过控制理论的视角重新表述自回归预测，提出了 \textbf{F-LLM} （反馈驱动的 LLM），一种新颖的闭环框架。与被动传播误差的标准方法不同，F-LLM 通过可学习的残差估计器（观察者）和反馈控制器主动稳定轨迹。此外，我们提供了理论上的保证，只要基础模型满足局部 Lipschitz 约束，我们的闭环机制就能确保统一有界误差。大量实验表明，F-LLM 显着减轻了错误传播，在时间序列基准上实现了良好的性能。</li>
</ul>

<h3>Title: PixelRush: Ultra-Fast, Training-Free High-Resolution Image Generation via One-step Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Hong-Phuc Lai, Phong Nguyen, Anh Tran</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12769">https://arxiv.org/abs/2602.12769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12769">https://arxiv.org/pdf/2602.12769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12769]] PixelRush: Ultra-Fast, Training-Free High-Resolution Image Generation via One-step Diffusion(https://arxiv.org/abs/2602.12769)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Pre-trained diffusion models excel at generating high-quality images but remain inherently limited by their native training resolution. Recent training-free approaches have attempted to overcome this constraint by introducing interventions during the denoising process; however, these methods incur substantial computational overhead, often requiring more than five minutes to produce a single 4K image. In this paper, we present PixelRush, the first tuning-free framework for practical high-resolution text-to-image generation. Our method builds upon the established patch-based inference paradigm but eliminates the need for multiple inversion and regeneration cycles. Instead, PixelRush enables efficient patch-based denoising within a low-step regime. To address artifacts introduced by patch blending in few-step generation, we propose a seamless blending strategy. Furthermore, we mitigate over-smoothing effects through a noise injection mechanism. PixelRush delivers exceptional efficiency, generating 4K images in approximately 20 seconds representing a 10$\times$ to 35$\times$ speedup over state-of-the-art methods while maintaining superior visual fidelity. Extensive experiments validate both the performance gains and the quality of outputs achieved by our approach.</li>
<li><strong>摘要：</strong>预训练的扩散模型擅长生成高质量图像，但本质上仍受到其本机训练分辨率的限制。最近的免训练方法试图通过在去噪过程中引入干预措施来克服这一限制；然而，这些方法会产生大量的计算开销，通常需要五分钟以上才能生成单个 4K 图像。在本文中，我们介绍了 PixelRush，这是第一个用于实际高分辨率文本到图像生成的免调整框架。我们的方法建立在已建立的基于补丁的推理范式的基础上，但消除了多个反转和再生循环的需要。相反，PixelRush 可在低步长范围内实现高效的基于补丁的去噪。为了解决在几步生成中由补丁混合引入的伪像，我们提出了一种无缝混合策略。此外，我们通过噪声注入机制减轻过度平滑效应。 PixelRush 具有卓越的效率，可在大约 20 秒内生成 4K 图像，与最先进的方法相比，速度提高 10 美元到 35 美元，同时保持卓越的视觉保真度。大量的实验验证了我们的方法所实现的性能提升和输出质量。</li>
</ul>

<h3>Title: FLAC: Maximum Entropy RL via Kinetic Energy Regularized Bridge Matching</h3>
<ul>
<li><strong>Authors: </strong>Lei Lv, Yunfei Li, Yu Luo, Fuchun Sun, Xiao Ma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12829">https://arxiv.org/abs/2602.12829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12829">https://arxiv.org/pdf/2602.12829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12829]] FLAC: Maximum Entropy RL via Kinetic Energy Regularized Bridge Matching(https://arxiv.org/abs/2602.12829)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Iterative generative policies, such as diffusion models and flow matching, offer superior expressivity for continuous control but complicate Maximum Entropy Reinforcement Learning because their action log-densities are not directly accessible. To address this, we propose Field Least-Energy Actor-Critic (FLAC), a likelihood-free framework that regulates policy stochasticity by penalizing the kinetic energy of the velocity field. Our key insight is to formulate policy optimization as a Generalized Schrödinger Bridge (GSB) problem relative to a high-entropy reference process (e.g., uniform). Under this view, the maximum-entropy principle emerges naturally as staying close to a high-entropy reference while optimizing return, without requiring explicit action densities. In this framework, kinetic energy serves as a physically grounded proxy for divergence from the reference: minimizing path-space energy bounds the deviation of the induced terminal action distribution. Building on this view, we derive an energy-regularized policy iteration scheme and a practical off-policy algorithm that automatically tunes the kinetic energy via a Lagrangian dual mechanism. Empirically, FLAC achieves superior or comparable performance on high-dimensional benchmarks relative to strong baselines, while avoiding explicit density estimation.</li>
<li><strong>摘要：</strong>迭代生成策略，例如扩散模型和流匹配，为连续控制提供了卓越的表达能力，但使最大熵强化学习变得复杂，因为它们的动作日志密度无法直接访问。为了解决这个问题，我们提出了场最小能量演员批评家（FLAC），这是一种无似然框架，通过惩罚速度场的动能来调节政策随机性。我们的主要见解是将策略优化制定为相对于高熵参考过程（例如均匀过程）的广义薛定谔桥（GSB）问题。根据这种观点，最大熵原理自然出现，在优化回报的同时保持接近高熵参考，而不需要明确的动作密度。在此框架中，动能充当与参考偏差的物理接地代理：最小化路径空间能量限制了诱导的终端动作分布的偏差。基于这个观点，我们推导了能量正则化策略迭代方案和实用的离策略算法，该算法通过拉格朗日对偶机制自动调整动能。根据经验，FLAC 在高维基准上相对于强基线实现了优越或可比的性能，同时避免了显式的密度估计。</li>
</ul>

<h3>Title: Thinking Like a Radiologist: A Dataset for Anatomy-Guided Interleaved Vision Language Reasoning in Chest X-ray Interpretation</h3>
<ul>
<li><strong>Authors: </strong>Yichen Zhao, Zelin Peng, Piao Yang, Xiaokang Yang, Wei Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12843">https://arxiv.org/abs/2602.12843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12843">https://arxiv.org/pdf/2602.12843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12843]] Thinking Like a Radiologist: A Dataset for Anatomy-Guided Interleaved Vision Language Reasoning in Chest X-ray Interpretation(https://arxiv.org/abs/2602.12843)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Radiological diagnosis is a perceptual process in which careful visual inspection and language reasoning are repeatedly interleaved. Most medical large vision language models (LVLMs) perform visual inspection only once and then rely on text-only chain-of-thought (CoT) reasoning, which operates purely in the linguistic space and is prone to hallucination. Recent methods attempt to mitigate this issue by introducing visually related coordinates, such as bounding boxes. However, these remain a pseudo-visual solution: coordinates are still text and fail to preserve rich visual details like texture and density. Motivated by the interleaved nature of radiological diagnosis, we introduce MMRad-IVL-22K, the first large-scale dataset designed for natively interleaved visual language reasoning in chest X-ray interpretation. MMRad-IVL-22K reflects a repeated cycle of reasoning and visual inspection workflow of radiologists, in which visual rationales complement textual descriptions and ground each step of the reasoning process. MMRad-IVL-22K comprises 21,994 diagnostic traces, enabling systematic scanning across 35 anatomical regions. Experimental results on advanced closed-source LVLMs demonstrate that report generation guided by multimodal CoT significantly outperforms that guided by text-only CoT in clinical accuracy and report quality (e.g., 6\% increase in the RadGraph metric), confirming that high-fidelity interleaved vision language evidence is a non-substitutable component of reliable medical AI. Furthermore, benchmarking across seven state-of-the-art open-source LVLMs demonstrates that models fine-tuned on MMRad-IVL-22K achieve superior reasoning consistency and report quality compared with both general-purpose and medical-specific LVLMs. The project page is available at this https URL.</li>
<li><strong>摘要：</strong>放射诊断是一个仔细的视觉检查和语言推理反复交织的感知过程。大多数医学大视觉语言模型（LVLM）仅执行一次视觉检查，然后依赖于纯文本的思想链（CoT）推理，这种推理纯粹在语言空间中运行，并且容易产生幻觉。最近的方法试图通过引入视觉相关的坐标（例如边界框）来缓解这个问题。然而，这些仍然是伪视觉解决方案：坐标仍然是文本，无法保留丰富的视觉细节，例如纹理和密度。受放射诊断交错性质的启发，我们推出了 MMRad-IVL-22K，这是第一个为胸部 X 射线判读中的本机交错视觉语言推理而设计的大型数据集。 MMRad-IVL-22K 反映了放射科医生推理和目视检查工作流程的重复循环，其中视觉原理补充了文本描述，并为推理过程的每一步奠定了基础。 MMRad-IVL-22K 包含 21,994 条诊断轨迹，能够对 35 个解剖区域进行系统扫描。先进闭源 LVLM 的实验结果表明，多模态 CoT 引导的报告生成在临床准确性和报告质量方面显着优于纯文本 CoT 引导的报告生成（例如，RadGraph 指标提高 6%），证实高保真交错视觉语言证据是可靠医疗 AI 中不可替代的组成部分。此外，对七个最先进的开源 LVLM 进行基准测试表明，与通用和医疗专用 LVLM 相比，在 MMRad-IVL-22K 上微调的模型可实现卓越的推理一致性和报告质量。项目页面可通过此 https URL 获取。</li>
</ul>

<h3>Title: Amortized Reasoning Tree Search: Decoupling Proposal and Decision in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zesheng Hong, Jiadong Yu, Hui Pan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12846">https://arxiv.org/abs/2602.12846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12846">https://arxiv.org/pdf/2602.12846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12846]] Amortized Reasoning Tree Search: Decoupling Proposal and Decision in Large Language Models(https://arxiv.org/abs/2602.12846)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning with Verifiable Rewards (RLVR) has established itself as the dominant paradigm for instilling rigorous reasoning capabilities in Large Language Models. While effective at amplifying dominant behaviors, we identify a critical pathology in this alignment process: the systematic suppression of valid but rare (low-likelihood under the base model distribution) reasoning paths. We theoretically characterize this phenomenon as a "Normalization Squeeze," where the interplay between mode-seeking policy gradients and finite sampling acts as a high-pass likelihood filter, driving the probability of rare correct traces to statistical extinction. To counteract this collapse without discarding the base model's latent diversity, we propose Amortized Reasoning Tree Search (ARTS). Unlike standard approaches that force internalization via parameter updates, ARTS prioritizes deliberation by decoupling generation from verification. We introduce a Flow Matching objective that repurposes the verifier to estimate the conservation of probability flow, enabling robust navigation through sparse, high-entropy search spaces where traditional discriminative objectives fail. Extensive experiments on the MATH-500 benchmark demonstrate that ARTS achieves a performance of 74.6% (BoN@16), effectively matching fully fine-tuned policies (74.7%) without modifying the generative backbone. Crucially, on the long-tail subset where coupled RL optimization collapses to 0% pass@k, ARTS uniquely recovers significant performance, suggesting that disentangling verification from generation offers a more robust pathway for solving complex reasoning tasks.</li>
<li><strong>摘要：</strong>具有可验证奖励的强化学习（RLVR）已成为在大型语言模型中灌输严格推理能力的主导范例。虽然可以有效地放大主导行为，但我们在这个对齐过程中发现了一个关键的病态：对有效但罕见（基本模型分布下的低可能性）推理路径的系统性抑制。我们从理论上将这种现象描述为“归一化挤压”，其中模式寻求策略梯度和有限采样之间的相互作用充当高通似然滤波器，推动罕见正确轨迹的概率导致统计灭绝。为了在不放弃基础模型潜在多样性的情况下抵消这种崩溃，我们提出了摊销推理树搜索（ARTS）。与通过参数更新强制内化的标准方法不同，ARTS 通过将生成与验证分离来优先考虑深思熟虑。我们引入了一个流匹配目标，它重新利用验证器来估计概率流守恒，从而能够在传统判别目标失败的稀疏、高熵搜索空间中进行鲁棒导航。 MATH-500 基准的大量实验表明，ARTS 的性能达到 74.6% (BoN@16)，在不修改生成主干的情况下有效匹配完全微调的策略 (74.7%)。至关重要的是，在耦合 RL 优化崩溃至 0% pass@k 的长尾子集上，ARTS 独特地恢复了显着的性能，这表明将验证与生成分离为解决复杂推理任务提供了更稳健的途径。</li>
</ul>

<h3>Title: Drift-Aware Variational Autoencoder-based Anomaly Detection with Two-level Ensembling</h3>
<ul>
<li><strong>Authors: </strong>Jin Li, Kleanthis Malialis, Christos G. Panayiotou, Marios M. Polycarpou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12976">https://arxiv.org/abs/2602.12976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12976">https://arxiv.org/pdf/2602.12976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12976]] Drift-Aware Variational Autoencoder-based Anomaly Detection with Two-level Ensembling(https://arxiv.org/abs/2602.12976)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In today's digital world, the generation of vast amounts of streaming data in various domains has become ubiquitous. However, many of these data are unlabeled, making it challenging to identify events, particularly anomalies. This task becomes even more formidable in nonstationary environments where model performance can deteriorate over time due to concept drift. To address these challenges, this paper presents a novel method, VAE++ESDD, which employs incremental learning and two-level ensembling: an ensemble of Variational AutoEncoder(VAEs) for anomaly prediction, along with an ensemble of concept drift detectors. Each drift detector utilizes a statistical-based concept drift mechanism. To evaluate the effectiveness of VAE++ESDD, we conduct a comprehensive experimental study using real-world and synthetic datasets characterized by severely or extremely low anomalous rates and various drift characteristics. Our study reveals that the proposed method significantly outperforms both strong baselines and state-of-the-art methods.</li>
<li><strong>摘要：</strong>在当今的数字世界中，各个领域中大量流数据的生成已经变得无处不在。然而，其中许多数据都没有标记，这使得识别事件（尤其是异常）变得困难。在非平稳环境中，此任务变得更加艰巨，在非平稳环境中，由于概念漂移，模型性能可能会随着时间的推移而恶化。为了应对这些挑战，本文提出了一种新颖的方法 VAE++ESDD，该方法采用增量学习和两级集成：用于异常预测的变分自动编码器（VAE）集成以及概念漂移检测器集成。每个漂移检测器都采用基于统计的概念漂移机制。为了评估 VAE++ESDD 的有效性，我们使用真实世界和合成数据集进行了全面的实验研究，这些数据集具有严重或极低的异常率和各种漂移特征。我们的研究表明，所提出的方法显着优于强大的基线和最先进的方法。</li>
</ul>

<h3>Title: Probabilistic Wind Power Forecasting with Tree-Based Machine Learning and Weather Ensembles</h3>
<ul>
<li><strong>Authors: </strong>Max Bruninx, Diederik van Binsbergen, Timothy Verstraeten, Ann Nowé, Jan Helsen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.13010">https://arxiv.org/abs/2602.13010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.13010">https://arxiv.org/pdf/2602.13010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.13010]] Probabilistic Wind Power Forecasting with Tree-Based Machine Learning and Weather Ensembles(https://arxiv.org/abs/2602.13010)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Accurate production forecasts are essential to continue facilitating the integration of renewable energy sources into the power grid. This paper illustrates how to obtain probabilistic day-ahead forecasts of wind power generation via gradient boosting trees using an ensemble of weather forecasts. To this end, we perform a comparative analysis across three state-of-the-art probabilistic prediction methods-conformalised quantile regression, natural gradient boosting and conditional diffusion models-all of which can be combined with tree-based machine learning. The methods are validated using four years of data for all wind farms present within the Belgian offshore zone. Additionally, the point forecasts are benchmarked against deterministic engineering methods, using either the power curve or an advanced approach incorporating a calibrated analytical wake model. The experimental results show that the machine learning methods improve the mean absolute error by up to 53% and 33% compared to the power curve and the calibrated wake model. Considering the three probabilistic prediction methods, the conditional diffusion model is found to yield the best overall probabilistic and point estimate of wind power generation. Moreover, the findings suggest that the use of an ensemble of weather forecasts can improve point forecast accuracy by up to 23%.</li>
<li><strong>摘要：</strong>准确的产量预测对于继续促进可再生能源并入电网至关重要。本文说明了如何使用天气预报集合通过梯度提升树获得风力发电的概率性日前预报。为此，我们对三种最先进的概率预测方法（保形分位数回归、自然梯度提升和条件扩散模型）进行了比较分析，所有这些方法都可以与基于树的机器学习相结合。这些方法使用比利时近海区域内所有风电场四年的数据进行了验证。此外，点预测还以确定性工程方法为基准，使用功率曲线或结合校准分析尾流模型的高级方法。实验结果表明，与功率曲线和校准尾流模型相比，机器学习方法将平均绝对误差提高了高达 53% 和 33%。考虑到三种概率预测方法，发现条件扩散模型可以产生风力发电的最佳整体概率和点估计。此外，研究结果表明，使用天气预报集合可以将点预报精度提高高达 23%。</li>
</ul>

<h3>Title: Look Inward to Explore Outward: Learning Temperature Policy from LLM Internal States via Hierarchical RL</h3>
<ul>
<li><strong>Authors: </strong>Yixiao Zhou, Yang Li, Dongzhou Cheng, Hehe Fan, Yu Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.13035">https://arxiv.org/abs/2602.13035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.13035">https://arxiv.org/pdf/2602.13035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.13035]] Look Inward to Explore Outward: Learning Temperature Policy from LLM Internal States via Hierarchical RL(https://arxiv.org/abs/2602.13035)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Verifiable Rewards (RLVR) trains large language models (LLMs) from sampled trajectories, making decoding strategy a core component of learning rather than a purely inference-time choice. Sampling temperature directly controls the exploration--exploitation trade-off by modulating policy entropy, yet existing methods rely on static values or heuristic adaptations that are decoupled from task-level rewards. We propose Introspective LLM, a hierarchical reinforcement learning framework that learns to control sampling temperature during generation. At each decoding step, the model selects a temperature based on its hidden state and samples the next token from the resulting distribution. Temperature and token policies are jointly optimized from downstream rewards using a coordinate ascent scheme. Experiments on mathematical reasoning benchmarks show that learned temperature policies outperform fixed and heuristic baselines, while exhibiting interpretable exploration behaviors aligned with reasoning uncertainty.</li>
<li><strong>摘要：</strong>可验证奖励强化学习 (RLVR) 从采样轨迹训练大型语言模型 (LLM)，使解码策略成为学习的核心组成部分，而不是纯粹的推理时间选择。采样温度通过调节策略熵直接控制探索-利用权衡，但现有方法依赖于与任务级奖励脱钩的静态值或启发式适应。我们提出了 Introspective LLM，这是一种分层强化学习框架，可以学习在生成过程中控制采样温度。在每个解码步骤中，模型根据其隐藏状态选择一个温度，并从结果分布中采样下一个标记。使用坐标上升方案从下游奖励中联合优化温度和代币政策。数学推理基准实验表明，学习的温度策略优于固定和启发式基线，同时表现出与推理不确定性一致的可解释探索行为。</li>
</ul>

<h3>Title: Curriculum-DPO++: Direct Preference Optimization via Data and Model Curricula for Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Nicu Sebe, Mubarak Shah</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.13055">https://arxiv.org/abs/2602.13055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.13055">https://arxiv.org/pdf/2602.13055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.13055]] Curriculum-DPO++: Direct Preference Optimization via Data and Model Curricula for Text-to-Image Generation(https://arxiv.org/abs/2602.13055)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Direct Preference Optimization (DPO) has been proposed as an effective and efficient alternative to reinforcement learning from human feedback (RLHF). However, neither RLHF nor DPO take into account the fact that learning certain preferences is more difficult than learning other preferences, rendering the optimization process suboptimal. To address this gap in text-to-image generation, we recently proposed Curriculum-DPO, a method that organizes image pairs by difficulty. In this paper, we introduce Curriculum-DPO++, an enhanced method that combines the original data-level curriculum with a novel model-level curriculum. More precisely, we propose to dynamically increase the learning capacity of the denoising network as training advances. We implement this capacity increase via two mechanisms. First, we initialize the model with only a subset of the trainable layers used in the original Curriculum-DPO. As training progresses, we sequentially unfreeze layers until the configuration matches the full baseline architecture. Second, as the fine-tuning is based on Low-Rank Adaptation (LoRA), we implement a progressive schedule for the dimension of the low-rank matrices. Instead of maintaining a fixed capacity, we initialize the low-rank matrices with a dimension significantly smaller than that of the baseline. As training proceeds, we incrementally increase their rank, allowing the capacity to grow until it converges to the same rank value as in Curriculum-DPO. Furthermore, we propose an alternative ranking strategy to the one employed by Curriculum-DPO. Finally, we compare Curriculum-DPO++ against Curriculum-DPO and other state-of-the-art preference optimization approaches on nine benchmarks, outperforming the competing methods in terms of text alignment, aesthetics and human preference. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>直接偏好优化（DPO）已被提议作为人类反馈强化学习（RLHF）的有效且高效的替代方案。然而，RLHF 和 DPO 都没有考虑到学习某些偏好比学习其他偏好更困难的事实，导致优化过程不是最优的。为了解决文本到图像生成中的这一差距，我们最近提出了 Curriculum-DPO，一种按难度组织图像对的方法。在本文中，我们介绍了 Curriculum-DPO++，这是一种将原始数据级课程与新颖的模型级课程相结合的增强方法。更准确地说，我们建议随着训练的进展动态增加去噪网络的学习能力。我们通过两种机制实现容量增加。首先，我们仅使用原始 Curriculum-DPO 中使用的可训练层的子集来初始化模型。随着训练的进行，我们依次解冻层，直到配置与完整的基线架构匹配。其次，由于微调基于低秩适应（LoRA），因此我们对低秩矩阵的维度实施了渐进式调度。我们没有保持固定的容量，而是初始化维度明显小于基线的低秩矩阵。随着训练的进行，我们逐渐提高他们的等级，从而使容量不断增长，直到收敛到与 Curriculum-DPO 中相同的等级值。此外，我们提出了一种替代 Curriculum-DPO 采用的排名策略。最后，我们在九个基准上将 Curriculum-DPO++ 与 Curriculum-DPO 和其他最先进的偏好优化方法进行比较，在文本对齐、美观和人类偏好方面优于竞争方法。我们的代码可以在这个 https URL 上找到。</li>
</ul>

<h3>Title: Diverging Flows: Detecting Extrapolations in Conditional Generation</h3>
<ul>
<li><strong>Authors: </strong>Constantinos Tsakonas, Serena Ivaldi, Jean-Baptiste Mouret</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.13061">https://arxiv.org/abs/2602.13061</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.13061">https://arxiv.org/pdf/2602.13061</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.13061]] Diverging Flows: Detecting Extrapolations in Conditional Generation(https://arxiv.org/abs/2602.13061)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The ability of Flow Matching (FM) to model complex conditional distributions has established it as the state-of-the-art for prediction tasks (e.g., robotics, weather forecasting). However, deployment in safety-critical settings is hindered by a critical extrapolation hazard: driven by smoothness biases, flow models yield plausible outputs even for off-manifold conditions, resulting in silent failures indistinguishable from valid predictions. In this work, we introduce Diverging Flows, a novel approach that enables a single model to simultaneously perform conditional generation and native extrapolation detection by structurally enforcing inefficient transport for off-manifold inputs. We evaluate our method on synthetic manifolds, cross-domain style transfer, and weather temperature forecasting, demonstrating that it achieves effective detection of extrapolations without compromising predictive fidelity or inference latency. These results establish Diverging Flows as a robust solution for trustworthy flow models, paving the way for reliable deployment in domains such as medicine, robotics, and climate science.</li>
<li><strong>摘要：</strong>流匹配 (FM) 对复杂条件分布进行建模的能力使其成为预测任务（例如机器人、天气预报）的最先进技术。然而，在安全关键环境中的部署受到关键外推风险的阻碍：在平滑偏差的驱动下，流量模型即使在非歧管条件下也会产生合理的输出，从而导致与有效预测无法区分的无声故障。在这项工作中，我们引入了 Diverging Flows，这是一种新颖的方法，通过在结构上强制执行流形外输入的低效传输，使单个模型能够同时执行条件生成和本机外推检测。我们在合成流形、跨域风格转移和天气温度预测方面评估了我们的方法，证明它可以在不影响预测保真度或推理延迟的情况下实现外推的有效检测。这些结果使 Diverging Flows 成为值得信赖的流动模型的强大解决方案，为医学、机器人和气候科学等领域的可靠部署铺平了道路。</li>
</ul>

<h3>Title: A Calibrated Memorization Index (MI) for Detecting Training Data Leakage in Generative MRI Models</h3>
<ul>
<li><strong>Authors: </strong>Yash Deo, Yan Jia, Toni Lassila, Victoria J Hodge, Alejandro F Frang, Chenghao Qian, Siyuan Kang, Ibrahim Habli</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.13066">https://arxiv.org/abs/2602.13066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.13066">https://arxiv.org/pdf/2602.13066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.13066]] A Calibrated Memorization Index (MI) for Detecting Training Data Leakage in Generative MRI Models(https://arxiv.org/abs/2602.13066)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Image generative models are known to duplicate images from the training data as part of their outputs, which can lead to privacy concerns when used for medical image generation. We propose a calibrated per-sample metric for detecting memorization and duplication of training data. Our metric uses image features extracted using an MRI foundation model, aggregates multi-layer whitened nearest-neighbor similarities, and maps them to a bounded \emph{Overfit/Novelty Index} (ONI) and \emph{Memorization Index} (MI) scores. Across three MRI datasets with controlled duplication percentages and typical image augmentations, our metric robustly detects duplication and provides more consistent metric values across datasets. At the sample level, our metric achieves near-perfect detection of duplicates.</li>
<li><strong>摘要：</strong>众所周知，图像生成模型会复制训练数据中的图像作为其输出的一部分，这在用于医学图像生成时可能会导致隐私问题。我们提出了一种校准的每样本指标，用于检测训练数据的记忆和重复。我们的度量使用 MRI 基础模型提取的图像特征，聚合多层白化最近邻相似性，并将它们映射到有界的 \emph{Overfit/Novelty Index} (ONI) 和 \emph{Memorization Index} (MI) 分数。在具有受控重复百分比和典型图像增强的三个 MRI 数据集中，我们的指标可以稳健地检测重复并在数据集中提供更一致的指标值。在样本级别，我们的指标实现了近乎完美的重复检测。</li>
</ul>

<h3>Title: Bus-Conditioned Zero-Shot Trajectory Generation via Task Arithmetic</h3>
<ul>
<li><strong>Authors: </strong>Shuai Liu, Ning Cao, Yile Chen, Yue Jiang, Gao Cong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.13071">https://arxiv.org/abs/2602.13071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.13071">https://arxiv.org/pdf/2602.13071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.13071]] Bus-Conditioned Zero-Shot Trajectory Generation via Task Arithmetic(https://arxiv.org/abs/2602.13071)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Mobility trajectory data provide essential support for smart city applications. However, such data are often difficult to obtain. Meanwhile, most existing trajectory generation methods implicitly assume that at least a subset of real mobility data from target city is available, which limits their applicability in data-inaccessible scenarios. In this work, we propose a new problem setting, called bus-conditioned zero-shot trajectory generation, where no mobility trajectories from a target city are accessible. The generation process relies solely on source city mobility data and publicly available bus timetables from both cities. Under this setting, we propose MobTA, the first approach to introduce task arithmetic into trajectory generation. MobTA models the parameter shift from bus-timetable-based trajectory generation to mobility trajectory generation in source city, and applies this shift to target city through arithmetic operations on task vectors. This enables trajectory generation that reflects target-city mobility patterns without requiring any real mobility data from it. Furthermore, we theoretically analyze MobTA's stability across base and instruction-tuned LLMs. Extensive experiments show that MobTA significantly outperforms existing methods, and achieves performance close to models finetuned using target city mobility trajectories.</li>
<li><strong>摘要：</strong>移动轨迹数据为智慧城市应用提供重要支撑。然而，此类数据通常很难获得。同时，大多数现有的轨迹生成方法隐含地假设目标城市的真实移动数据的至少一个子集是可用的，这限制了它们在数据不可访问的场景中的适用性。在这项工作中，我们提出了一种新的问题设置，称为总线条件零样本轨迹生成，其中无法访问来自目标城市的移动轨迹。生成过程仅依赖于源城市交通数据和两个城市公开的公交车时刻表。在此背景下，我们提出了 MobTA，这是第一个将任务算法引入轨迹生成的方法。 MobTA 对从基于公交车时刻表的轨迹生成到源城市的移动轨迹生成的参数转变进行建模，并通过任务向量的算术运算将这种转变应用到目标城市。这使得轨迹生成能够反映目标城市的移动模式，而无需从中获取任何真实的移动数据。此外，我们从理论上分析了 MobTA 在基础和指令调整的 LLM 中的稳定性。大量实验表明，MobTA 的性能显着优于现有方法，并且其性能接近使用目标城市移动轨迹微调的模型。</li>
</ul>

<h3>Title: Order Matters in Retrosynthesis: Structure-aware Generation via Reaction-Center-Guided Discrete Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Chenguang Wang, Zihan Zhou, Lei Bai, Tianshu Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.13136">https://arxiv.org/abs/2602.13136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.13136">https://arxiv.org/pdf/2602.13136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.13136]] Order Matters in Retrosynthesis: Structure-aware Generation via Reaction-Center-Guided Discrete Flow Matching(https://arxiv.org/abs/2602.13136)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Template-free retrosynthesis methods treat the task as black-box sequence generation, limiting learning efficiency, while semi-template approaches rely on rigid reaction libraries that constrain generalization. We address this gap with a key insight: atom ordering in neural representations matters. Building on this insight, we propose a structure-aware template-free framework that encodes the two-stage nature of chemical reactions as a positional inductive bias. By placing reaction center atoms at the sequence head, our method transforms implicit chemical knowledge into explicit positional patterns that the model can readily capture. The proposed RetroDiT backbone, a graph transformer with rotary position embeddings, exploits this ordering to prioritize chemically critical regions. Combined with discrete flow matching, our approach decouples training from sampling and enables generation in 20--50 steps versus 500 for prior diffusion methods. Our method achieves state-of-the-art performance on both USPTO-50k (61.2% top-1) and the large-scale USPTO-Full (51.3% top-1) with predicted reaction centers. With oracle centers, performance reaches 71.1% and 63.4% respectively, surpassing foundation models trained on 10 billion reactions while using orders of magnitude less data. Ablation studies further reveal that structural priors outperform brute-force scaling: a 280K-parameter model with proper ordering matches a 65M-parameter model without it.</li>
<li><strong>摘要：</strong>无模板逆合成方法将任务视为黑盒序列生成，限制了学习效率，而半模板方法依赖于限制泛化的严格反应库。我们通过一个关键的见解来解决这一差距：神经表征中的原子排序很重要。基于这一见解，我们提出了一种结构感知的无模板框架，将化学反应的两阶段性质编码为位置归纳偏差。通过将反应中心原子放置在序列头，我们的方法将隐式化学知识转化为模型可以轻松捕获的显式位置模式。所提出的 RetroDiT 主干是一种具有旋转位置嵌入的图形转换器，利用这种顺序来优先考虑化学关键区域。与离散流匹配相结合，我们的方法将训练与采样分离，并且能够以 20--50 步生成，而之前的扩散方法需要 500 步。我们的方法在 USPTO-50k (61.2% top-1) 和具有预测反应中心的大规模 USPTO-Full (51.3% top-1) 上实现了最先进的性能。借助预言机中心，性能分别达到 71.1% 和 63.4%，超过了在使用少几个数量级的数据的情况下训练有 100 亿个反应的基础模型。消融研究进一步表明，结构先验优于强力缩放：具有正确排序的 280K 参数模型与没有正确排序的 65M 参数模型相匹配。</li>
</ul>

<h3>Title: FlexAM: Flexible Appearance-Motion Decomposition for Versatile Video Generation Control</h3>
<ul>
<li><strong>Authors: </strong>Mingzhi Sheng, Zekai Gu, Peng Li, Cheng Lin, Hao-Xiang Guo, Ying-Cong Chen, Yuan Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.13185">https://arxiv.org/abs/2602.13185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.13185">https://arxiv.org/pdf/2602.13185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.13185]] FlexAM: Flexible Appearance-Motion Decomposition for Versatile Video Generation Control(https://arxiv.org/abs/2602.13185)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Effective and generalizable control in video generation remains a significant challenge. While many methods rely on ambiguous or task-specific signals, we argue that a fundamental disentanglement of "appearance" and "motion" provides a more robust and scalable pathway. We propose FlexAM, a unified framework built upon a novel 3D control signal. This signal represents video dynamics as a point cloud, introducing three key enhancements: multi-frequency positional encoding to distinguish fine-grained motion, depth-aware positional encoding, and a flexible control signal for balancing precision and generative quality. This representation allows FlexAM to effectively disentangle appearance and motion, enabling a wide range of tasks including I2V/V2V editing, camera control, and spatial object editing. Extensive experiments demonstrate that FlexAM achieves superior performance across all evaluated tasks.</li>
<li><strong>摘要：</strong>视频生成的有效且通用的控制仍然是一个重大挑战。虽然许多方法依赖于模糊或特定于任务的信号，但我们认为“外观”和“运动”的根本分离提供了一条更稳健和可扩展的途径。我们提出了 FlexAM，这是一个基于新颖的 3D 控制信号构建的统一框架。该信号将视频动态表示为点云，引入了三个关键增强功能：用于区分细粒度运动的多频位置编码、深度感知位置编码以及用于平衡精度和生成质量的灵活控制信号。这种表示方式使 FlexAM 能够有效地理清外观和运动，从而实现广泛的任务，包括 I2V/V2V 编辑、相机控制和空间对象编辑。大量实验表明，FlexAM 在所有评估任务中均实现了卓越的性能。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
