<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-11-03</h1>
<h3>Title: BI-DCGAN: A Theoretically Grounded Bayesian Framework for Efficient and Diverse GANs</h3>
<ul>
<li><strong>Authors: </strong>Mahsa Valizadeh, Rui Tuo, James Caverlee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.26892">https://arxiv.org/abs/2510.26892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.26892">https://arxiv.org/pdf/2510.26892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.26892]] BI-DCGAN: A Theoretically Grounded Bayesian Framework for Efficient and Diverse GANs(https://arxiv.org/abs/2510.26892)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative Adversarial Networks (GANs) are proficient at generating synthetic data but continue to suffer from mode collapse, where the generator produces a narrow range of outputs that fool the discriminator but fail to capture the full data distribution. This limitation is particularly problematic, as generative models are increasingly deployed in real-world applications that demand both diversity and uncertainty awareness. In response, we introduce BI-DCGAN, a Bayesian extension of DCGAN that incorporates model uncertainty into the generative process while maintaining computational efficiency. BI-DCGAN integrates Bayes by Backprop to learn a distribution over network weights and employs mean-field variational inference to efficiently approximate the posterior distribution during GAN training. We establishes the first theoretical proof, based on covariance matrix analysis, that Bayesian modeling enhances sample diversity in GANs. We validate this theoretical result through extensive experiments on standard generative benchmarks, demonstrating that BI-DCGAN produces more diverse and robust outputs than conventional DCGANs, while maintaining training efficiency. These findings position BI-DCGAN as a scalable and timely solution for applications where both diversity and uncertainty are critical, and where modern alternatives like diffusion models remain too resource-intensive.</li>
<li><strong>摘要：</strong>生成对抗网络（GAN）擅长生成合成数据，但仍然遭受模式崩溃的困扰，生成器产生的输出范围很窄，可以欺骗鉴别器，但无法捕获完整的数据分布。这种限制尤其成问题，因为生成模型越来越多地部署在需要多样性和不确定性意识的现实世界应用中。为此，我们引入了 BI-DCGAN，这是 DCGAN 的贝叶斯扩展，它将模型不确定性纳入生成过程，同时保持计算效率。 BI-DCGAN 通过 Backprop 集成贝叶斯来学习网络权重的分布，并采用平均场变分推理在 GAN 训练期间有效地逼近后验分布。我们基于协方差矩阵分析建立了第一个理论证明，证明贝叶斯建模增强了 GAN 中的样本多样性。我们通过对标准生成基准的大量实验验证了这一理论结果，证明 BI-DCGAN 可以产生比传统 DCGAN 更多样化、更稳健的输出，同时保持训练效率。这些发现使 BI-DCGAN 成为一种可扩展且及时的解决方案，适用于多样性和不确定性都至关重要的应用，以及扩散模型等现代替代方案仍然过于资源密集型的应用。</li>
</ul>

<h3>Title: Integrating Ontologies with Large Language Models for Enhanced Control Systems in Chemical Engineering</h3>
<ul>
<li><strong>Authors: </strong>Crystal Su, Kuai Yu, Jingrui Zhang, Mingyuan Shao, Daniel Bauer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.26898">https://arxiv.org/abs/2510.26898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.26898">https://arxiv.org/pdf/2510.26898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.26898]] Integrating Ontologies with Large Language Models for Enhanced Control Systems in Chemical Engineering(https://arxiv.org/abs/2510.26898)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>This work presents an ontology-integrated large language model (LLM) framework for chemical engineering that unites structured domain knowledge with generative reasoning. The proposed pipeline aligns model training and inference with the COPE ontology through a sequence of data acquisition, semantic preprocessing, information extraction, and ontology mapping steps, producing templated question-answer pairs that guide fine-tuning. A control-focused decoding stage and citation gate enforce syntactic and factual grounding by constraining outputs to ontology-linked terms, while evaluation metrics quantify both linguistic quality and ontological accuracy. Feedback and future extensions, including semantic retrieval and iterative validation, further enhance the system's interpretability and reliability. This integration of symbolic structure and neural generation provides a transparent, auditable approach for applying LLMs to process control, safety analysis, and other critical engineering contexts.</li>
<li><strong>摘要：</strong>这项工作提出了一种用于化学工程的本体集成大语言模型（LLM）框架，它将结构化领域知识与生成推理结合起来。所提出的流程通过一系列数据采集、语义预处理、信息提取和本体映射步骤将模型训练和推理与 COPE 本体结合起来，生成指导微调的模板化问答对。以控制为中心的解码阶段和引用门通过将输出限制为与本体相关的术语来强制句法和事实基础，而评估指标则量化语言质量和本体准确性。反馈和未来的扩展，包括语义检索和迭代验证，进一步增强了系统的可解释性和可靠性。符号结构和神经生成的这种集成提供了一种透明的、可审计的方法，用于将法学硕士应用于过程控制、安全分析和其他关键工程环境。</li>
</ul>

<h3>Title: Semantic Frame Aggregation-based Transformer for Live Video Comment Generation</h3>
<ul>
<li><strong>Authors: </strong>Anam Fatima, Yi Yu, Janak Kapuriya, Julien Lalanne, Jainendra Shukla</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.26978">https://arxiv.org/abs/2510.26978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.26978">https://arxiv.org/pdf/2510.26978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.26978]] Semantic Frame Aggregation-based Transformer for Live Video Comment Generation(https://arxiv.org/abs/2510.26978)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Live commenting on video streams has surged in popularity on platforms like Twitch, enhancing viewer engagement through dynamic interactions. However, automatically generating contextually appropriate comments remains a challenging and exciting task. Video streams can contain a vast amount of data and extraneous content. Existing approaches tend to overlook an important aspect of prioritizing video frames that are most relevant to ongoing viewer interactions. This prioritization is crucial for producing contextually appropriate comments. To address this gap, we introduce a novel Semantic Frame Aggregation-based Transformer (SFAT) model for live video comment generation. This method not only leverages CLIP's visual-text multimodal knowledge to generate comments but also assigns weights to video frames based on their semantic relevance to ongoing viewer conversation. It employs an efficient weighted sum of frames technique to emphasize informative frames while focusing less on irrelevant ones. Finally, our comment decoder with a cross-attention mechanism that attends to each modality ensures that the generated comment reflects contextual cues from both chats and video. Furthermore, to address the limitations of existing datasets, which predominantly focus on Chinese-language content with limited video categories, we have constructed a large scale, diverse, multimodal English video comments dataset. Extracted from Twitch, this dataset covers 11 video categories, totaling 438 hours and 3.2 million comments. We demonstrate the effectiveness of our SFAT model by comparing it to existing methods for generating comments from live video and ongoing dialogue contexts.</li>
<li><strong>摘要：</strong>视频流实时评论在 Twitch 等平台上越来越受欢迎，通过动态互动提高观众参与度。然而，自动生成适合上下文的评论仍然是一项具有挑战性和令人兴奋的任务。视频流可能包含大量数据和无关内容。现有的方法往往忽视了对与正在进行的观看者交互最相关的视频帧进行优先级排序的一个重要方面。这种优先顺序对于生成适合上下文的评论至关重要。为了解决这一差距，我们引入了一种新颖的基于语义帧聚合的变压器（SFAT）模型，用于生成实时视频评论。该方法不仅利用 CLIP 的视觉文本多模态知识来生成评论，还根据视频帧与正在进行的观看者对话的语义相关性为视频帧分配权重。它采用有效的帧加权和技术来强调信息丰富的帧，同时较少关注不相关的帧。最后，我们的评论解码器具有关注每种模式的交叉注意机制，确保生成的评论反映来自聊天和视频的上下文线索。此外，为了解决现有数据集主要关注中文内容且视频类别有限的局限性，我们构建了一个大规模、多样化、多模态的英语视频评论数据集。该数据集摘自 Twitch，涵盖 11 个视频类别，总计 438 小时和 320 万条评论。我们通过将 SFAT 模型与现有的从实时视频和持续对话上下文中生成评论的方法进行比较，证明了 SFAT 模型的有效性。</li>
</ul>

<h3>Title: AI Agents in Drug Discovery</h3>
<ul>
<li><strong>Authors: </strong>Srijit Seal, Dinh Long Huynh, Moudather Chelbi, Sara Khosravi, Ankur Kumar, Mattson Thieme, Isaac Wilks, Mark Davies, Jessica Mustali, Yannick Sun, Nick Edwards, Daniil Boiko, Andrei Tyrin, Douglas W. Selinger, Ayaan Parikh, Rahul Vijayan, Shoman Kasbekar, Dylan Reid, Andreas Bender, Ola Spjuth</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.27130">https://arxiv.org/abs/2510.27130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.27130">https://arxiv.org/pdf/2510.27130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.27130]] AI Agents in Drug Discovery(https://arxiv.org/abs/2510.27130)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Artificial intelligence (AI) agents are emerging as transformative tools in drug discovery, with the ability to autonomously reason, act, and learn through complicated research workflows. Building on large language models (LLMs) coupled with perception, computation, action, and memory tools, these agentic AI systems could integrate diverse biomedical data, execute tasks, carry out experiments via robotic platforms, and iteratively refine hypotheses in closed loops. We provide a conceptual and technical overview of agentic AI architectures, ranging from ReAct and Reflection to Supervisor and Swarm systems, and illustrate their applications across key stages of drug discovery, including literature synthesis, toxicity prediction, automated protocol generation, small-molecule synthesis, drug repurposing, and end-to-end decision-making. To our knowledge, this represents the first comprehensive work to present real-world implementations and quantifiable impacts of agentic AI systems deployed in operational drug discovery settings. Early implementations demonstrate substantial gains in speed, reproducibility, and scalability, compressing workflows that once took months into hours while maintaining scientific traceability. We discuss the current challenges related to data heterogeneity, system reliability, privacy, and benchmarking, and outline future directions towards technology in support of science and translation.</li>
<li><strong>摘要：</strong>人工智能 (AI) 代理正在成为药物发现领域的变革性工具，能够通过复杂的研究工作流程自主推理、行动和学习。这些代理人工智能系统以大型语言模型 (LLM) 为基础，结合感知、计算、动作和记忆工具，可以集成不同的生物医学数据、执行任务、通过机器人平台进行实验，并在闭环中迭代地完善假设。我们提供了代理人工智能架构的概念和技术概述，范围从 ReAct 和 Reflection 到 Supervisor 和 Swarm 系统，并说明了它们在药物发现关键阶段的应用，包括文献合成、毒性预测、自动方案生成、小分子合成、药物再利用和端到端决策。据我们所知，这是第一个全面的工作，展示了在药物发现操作环境中部署的代理人工智能系统的实际实施和可量化影响。早期的实施展示了速度、可重复性和可扩展性方面的巨大进步，将曾经需要数月时间的工作流程压缩为数小时，同时保持了科学的可追溯性。我们讨论了当前与数据异构性、系统可靠性、隐私和基准相关的挑战，并概述了支持科学和翻译的技术的未来方向。</li>
</ul>

<h3>Title: E-MMDiT: Revisiting Multimodal Diffusion Transformer Design for Fast Image Synthesis under Limited Resources</h3>
<ul>
<li><strong>Authors: </strong>Tong Shen, Jingai Yu, Dong Zhou, Dong Li, Emad Barsoum</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.27135">https://arxiv.org/abs/2510.27135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.27135">https://arxiv.org/pdf/2510.27135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.27135]] E-MMDiT: Revisiting Multimodal Diffusion Transformer Design for Fast Image Synthesis under Limited Resources(https://arxiv.org/abs/2510.27135)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have shown strong capabilities in generating high-quality images from text prompts. However, these models often require large-scale training data and significant computational resources to train, or suffer from heavy structure with high latency. To this end, we propose Efficient Multimodal Diffusion Transformer (E-MMDiT), an efficient and lightweight multimodal diffusion model with only 304M parameters for fast image synthesis requiring low training resources. We provide an easily reproducible baseline with competitive results. Our model for 512px generation, trained with only 25M public data in 1.5 days on a single node of 8 AMD MI300X GPUs, achieves 0.66 on GenEval and easily reaches to 0.72 with some post-training techniques such as GRPO. Our design philosophy centers on token reduction as the computational cost scales significantly with the token count. We adopt a highly compressive visual tokenizer to produce a more compact representation and propose a novel multi-path compression module for further compression of tokens. To enhance our design, we introduce Position Reinforcement, which strengthens positional information to maintain spatial coherence, and Alternating Subregion Attention (ASA), which performs attention within subregions to further reduce computational cost. In addition, we propose AdaLN-affine, an efficient lightweight module for computing modulation parameters in transformer blocks. Our code is available at this https URL and we hope E-MMDiT serves as a strong and practical baseline for future research and contributes to democratization of generative AI models.</li>
<li><strong>摘要：</strong>扩散模型在根据文本提示生成高质量图像方面表现出了强大的能力。然而，这些模型通常需要大规模的训练数据和大量的计算资源来训练，或者遭受沉重的结构和高延迟的困扰。为此，我们提出了高效多模态扩散变压器（E-MMDiT），这是一种高效且轻量级的多模态扩散模型，仅具有 304M 参数，可实现需要较少训练资源的快速图像合成。我们提供易于重复的基线和具有竞争力的结果。我们的 512px 代模型在 8 个 AMD MI300X GPU 的单个节点上仅用 25M 公共数据在 1.5 天内进行了训练，在 GenEval 上达到了 0.66，并且通过一些训练后技术（例如 GRPO）轻松达到 0.72。我们的设计理念以减少代币为中心，因为计算成本随着代币数量的增加而显着增加。我们采用高度压缩的视觉标记器来产生更紧凑的表示，并提出一种新颖的多路径压缩模块来进一步压缩标记。为了增强我们的设计，我们引入了位置强化（Position Reinforcement）和交替子区域注意力（ASA），前者增强位置信息以保持空间一致性，后者在子区域内执行注意力以进一步降低计算成本。此外，我们提出了 AdaLN-affine，这是一种高效的轻量级模块，用于计算变压器块中的调制参数。我们的代码可以在这个 https URL 上找到，我们希望 E-MMDiT 成为未来研究的强大而实用的基线，并为生成人工智能模型的民主化做出贡献。</li>
</ul>

<h3>Title: HiGS: Hierarchical Generative Scene Framework for Multi-Step Associative Semantic Spatial Composition</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Hong, Kunzhen Wu, Mingrui Yu, Yichao Gu, Shengze Xue, Shuangjiu Xiao, Deli Dong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.27148">https://arxiv.org/abs/2510.27148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.27148">https://arxiv.org/pdf/2510.27148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.27148]] HiGS: Hierarchical Generative Scene Framework for Multi-Step Associative Semantic Spatial Composition(https://arxiv.org/abs/2510.27148)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Three-dimensional scene generation holds significant potential in gaming, film, and virtual reality. However, most existing methods adopt a single-step generation process, making it difficult to balance scene complexity with minimal user input. Inspired by the human cognitive process in scene modeling, which progresses from global to local, focuses on key elements, and completes the scene through semantic association, we propose HiGS, a hierarchical generative framework for multi-step associative semantic spatial composition. HiGS enables users to iteratively expand scenes by selecting key semantic objects, offering fine-grained control over regions of interest while the model completes peripheral areas automatically. To support structured and coherent generation, we introduce the Progressive Hierarchical Spatial-Semantic Graph (PHiSSG), which dynamically organizes spatial relationships and semantic dependencies across the evolving scene structure. PHiSSG ensures spatial and geometric consistency throughout the generation process by maintaining a one-to-one mapping between graph nodes and generated objects and supporting recursive layout optimization. Experiments demonstrate that HiGS outperforms single-stage methods in layout plausibility, style consistency, and user preference, offering a controllable and extensible paradigm for efficient 3D scene construction.</li>
<li><strong>摘要：</strong>三维场景生成在游戏、电影和虚拟现实中具有巨大的潜力。然而，大多数现有方法采用单步生成过程，很难在场景复杂性与最小用户输入之间取得平衡。受场景建模中人类认知过程的启发，从全局到局部，关注关键元素，并通过语义关联完成场景，我们提出了 HiGS，一种用于多步骤关联语义空间组成的分层生成框架。 HiGS 使用户能够通过选择关键语义对象来迭代扩展场景，提供对感兴趣区域的细粒度控制，同时模型自动完成外围区域。为了支持结构化和连贯的生成，我们引入了渐进层次空间语义图（PHiSSG），它动态地组织不断发展的场景结构中的空间关系和语义依赖性。 PHiSSG 通过维护图节点和生成对象之间的一对一映射并支持递归布局优化，确保整个生成过程中的空间和几何一致性。实验表明，HiGS 在布局合理性、风格一致性和用户偏好方面优于单阶段方法，为高效的 3D 场景构建提供了可控且可扩展的范例。</li>
</ul>

<h3>Title: DANCER: Dance ANimation via Condition Enhancement and Rendering with diffusion model</h3>
<ul>
<li><strong>Authors: </strong>Yucheng Xing, Jinxing Yin, Xiaodong Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.27169">https://arxiv.org/abs/2510.27169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.27169">https://arxiv.org/pdf/2510.27169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.27169]] DANCER: Dance ANimation via Condition Enhancement and Rendering with diffusion model(https://arxiv.org/abs/2510.27169)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recently, diffusion models have shown their impressive ability in visual generation tasks. Besides static images, more and more research attentions have been drawn to the generation of realistic videos. The video generation not only has a higher requirement for the quality, but also brings a challenge in ensuring the video continuity. Among all the video generation tasks, human-involved contents, such as human dancing, are even more difficult to generate due to the high degrees of freedom associated with human motions. In this paper, we propose a novel framework, named as DANCER (Dance ANimation via Condition Enhancement and Rendering with Diffusion Model), for realistic single-person dance synthesis based on the most recent stable video diffusion model. As the video generation is generally guided by a reference image and a video sequence, we introduce two important modules into our framework to fully benefit from the two inputs. More specifically, we design an Appearance Enhancement Module (AEM) to focus more on the details of the reference image during the generation, and extend the motion guidance through a Pose Rendering Module (PRM) to capture pose conditions from extra domains. To further improve the generation capability of our model, we also collect a large amount of video data from Internet, and generate a novel datasetTikTok-3K to enhance the model training. The effectiveness of the proposed model has been evaluated through extensive experiments on real-world datasets, where the performance of our model is superior to that of the state-of-the-art methods. All the data and codes will be released upon acceptance.</li>
<li><strong>摘要：</strong>最近，扩散模型在视觉生成任务中表现出了令人印象深刻的能力。除了静态图像之外，越来越多的研究注意力集中在真实视频的生成上。视频生成不仅对质量提出了更高的要求，也给保证视频的连续性带来了挑战。在所有视频生成任务中，由于与人体运动相关的高自由度，涉及人类的内容（例如人类舞蹈）的生成更加困难。在本文中，我们提出了一种新颖的框架，称为 DANCER（通过条件增强和扩散模型渲染的舞蹈动画），用于基于最新稳定视频扩散模型的逼真单人舞蹈合成。由于视频生成通常由参考图像和视频序列引导，因此我们在框架中引入了两个重要的模块，以充分受益于这两个输入。更具体地说，我们设计了一个外观增强模块（AEM），以在生成过程中更多地关注参考图像的细节，并通过姿势渲染模块（PRM）扩展运动指导，以从额外的域捕获姿势条件。为了进一步提高模型的生成能力，我们还从互联网上收集了大量的视频数据，并生成了一个新颖的数据集TikTok-3K来增强模型训练。所提出模型的有效性已经通过对现实世界数据集的广泛实验进行了评估，其中我们的模型的性能优于最先进的方法。所有数据和代码将在接受后发布。</li>
</ul>

<h3>Title: H2-Cache: A Novel Hierarchical Dual-Stage Cache for High-Performance Acceleration of Generative Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Mingyu Sung, Il-Min Kim, Sangseok Yun, Jae-Mo Kang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.27171">https://arxiv.org/abs/2510.27171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.27171">https://arxiv.org/pdf/2510.27171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.27171]] H2-Cache: A Novel Hierarchical Dual-Stage Cache for High-Performance Acceleration of Generative Diffusion Models(https://arxiv.org/abs/2510.27171)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as state-of-the-art in image generation, but their practical deployment is hindered by the significant computational cost of their iterative denoising process. While existing caching techniques can accelerate inference, they often create a challenging trade-off between speed and fidelity, suffering from quality degradation and high computational overhead. To address these limitations, we introduce H2-Cache, a novel hierarchical caching mechanism designed for modern generative diffusion model architectures. Our method is founded on the key insight that the denoising process can be functionally separated into a structure-defining stage and a detail-refining stage. H2-cache leverages this by employing a dual-threshold system, using independent thresholds to selectively cache each stage. To ensure the efficiency of our dual-check approach, we introduce pooled feature summarization (PFS), a lightweight technique for robust and fast similarity estimation. Extensive experiments on the Flux architecture demonstrate that H2-cache achieves significant acceleration (up to 5.08x) while maintaining image quality nearly identical to the baseline, quantitatively and qualitatively outperforming existing caching methods. Our work presents a robust and practical solution that effectively resolves the speed-quality dilemma, significantly lowering the barrier for the real-world application of high-fidelity diffusion models. Source code is available at this https URL.</li>
<li><strong>摘要：</strong>扩散模型已成为图像生成领域最先进的技术，但其实际部署却因其迭代去噪过程的巨大计算成本而受到阻碍。虽然现有的缓存技术可以加速推理，但它们通常会在速度和保真度之间进行具有挑战性的权衡，从而导致质量下降和高计算开销。为了解决这些限制，我们引入了 H2-Cache，这是一种专为现代生成扩散模型架构设计的新颖的分层缓存机制。我们的方法基于这样一个关键见解：去噪过程可以在功能上分为结构定义阶段和细节细化阶段。 H2-cache 通过采用双阈值系统来利用这一点，使用独立的阈值来选择性地缓存每个阶段。为了确保双重检查方法的效率，我们引入了池化特征摘要（PFS），这是一种用于稳健且快速相似性估计的轻量级技术。 Flux 架构上的大量实验表明，H2-cache 实现了显着的加速（高达 5.08 倍），同时保持图像质量几乎与基线相同，在数量和质量上都优于现有的缓存方法。我们的工作提出了一个强大而实用的解决方案，有效解决了速度与质量的困境，显着降低了高保真扩散模型在现实世界中应用的障碍。源代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Multi-Modal Feature Fusion for Spatial Morphology Analysis of Traditional Villages via Hierarchical Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Jiaxin Zhang, Zehong Zhu, Junye Deng, Yunqin Li, and Bowen Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.27208">https://arxiv.org/abs/2510.27208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.27208">https://arxiv.org/pdf/2510.27208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.27208]] Multi-Modal Feature Fusion for Spatial Morphology Analysis of Traditional Villages via Hierarchical Graph Neural Networks(https://arxiv.org/abs/2510.27208)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Villages areas hold significant importance in the study of human-land relationships. However, with the advancement of urbanization, the gradual disappearance of spatial characteristics and the homogenization of landscapes have emerged as prominent issues. Existing studies primarily adopt a single-disciplinary perspective to analyze villages spatial morphology and its influencing factors, relying heavily on qualitative analysis methods. These efforts are often constrained by the lack of digital infrastructure and insufficient data. To address the current research limitations, this paper proposes a Hierarchical Graph Neural Network (HGNN) model that integrates multi-source data to conduct an in-depth analysis of villages spatial morphology. The framework includes two types of nodes-input nodes and communication nodes-and two types of edges-static input edges and dynamic communication edges. By combining Graph Convolutional Networks (GCN) and Graph Attention Networks (GAT), the proposed model efficiently integrates multimodal features under a two-stage feature update mechanism. Additionally, based on existing principles for classifying villages spatial morphology, the paper introduces a relational pooling mechanism and implements a joint training strategy across 17 subtypes. Experimental results demonstrate that this method achieves significant performance improvements over existing approaches in multimodal fusion and classification tasks. Additionally, the proposed joint optimization of all sub-types lifts mean accuracy/F1 from 0.71/0.83 (independent models) to 0.82/0.90, driven by a 6% gain for parcel tasks. Our method provides scientific evidence for exploring villages spatial patterns and generative logic.</li>
<li><strong>摘要：</strong>村庄地区在人地关系研究中具有重要意义。然而，随着城市化进程的推进，空间特征逐渐消失、景观同质化等问题日益突出。现有研究主要采用单学科视角来分析村庄空间形态及其影响因素，严重依赖定性分析方法。这些努力往往受到数字基础设施缺乏和数据不足的限制。针对目前研究的局限性，本文提出了一种融合多源数据的层次图神经网络（HGNN）模型，对村庄空间形态进行深入分析。该框架包括两种类型的节点——输入节点和通信节点——以及两种类型的边——静态输入边和动态通信边。通过结合图卷积网络（GCN）和图注意网络（GAT），所提出的模型在两阶段特征更新机制下有效地集成多模态特征。此外，基于现有村庄空间形态分类原则，本文引入了关系池机制，并实施了跨17个子类型的联合训练策略。实验结果表明，该方法在多模态融合和分类任务中比现有方法取得了显着的性能改进。此外，所提出的所有子类型的联合优化将平均准确度/F1 从 0.71/0.83（独立模型）提升至 0.82/0.90，这得益于包裹任务 6% 的增益。我们的方法为探索村庄空间格局和生成逻辑提供了科学依据。</li>
</ul>

<h3>Title: Object-IR: Leveraging Object Consistency and Mesh Deformation for Self-Supervised Image Retargeting</h3>
<ul>
<li><strong>Authors: </strong>Tianli Liao, Ran Wang, Siqing Zhang, Lei Li, Guangen Liu, Chenyang Zhao, Heling Cao, Peng Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.27236">https://arxiv.org/abs/2510.27236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.27236">https://arxiv.org/pdf/2510.27236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.27236]] Object-IR: Leveraging Object Consistency and Mesh Deformation for Self-Supervised Image Retargeting(https://arxiv.org/abs/2510.27236)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Eliminating geometric distortion in semantically important regions remains an intractable challenge in image retargeting. This paper presents Object-IR, a self-supervised architecture that reformulates image retargeting as a learning-based mesh warping optimization problem, where the mesh deformation is guided by object appearance consistency and geometric-preserving constraints. Given an input image and a target aspect ratio, we initialize a uniform rigid mesh at the output resolution and use a convolutional neural network to predict the motion of each mesh grid and obtain the deformed mesh. The retargeted result is generated by warping the input image according to the rigid mesh in the input image and the deformed mesh in the output resolution. To mitigate geometric distortion, we design a comprehensive objective function incorporating a) object-consistent loss to ensure that the important semantic objects retain their appearance, b) geometric-preserving loss to constrain simple scale transform of the important meshes, and c) boundary loss to enforce a clean rectangular output. Notably, our self-supervised paradigm eliminates the need for manually annotated retargeting datasets by deriving supervision directly from the input's geometric and semantic properties. Extensive evaluations on the RetargetMe benchmark demonstrate that our Object-IR achieves state-of-the-art performance, outperforming existing methods in quantitative metrics and subjective visual quality assessments. The framework efficiently processes arbitrary input resolutions (average inference time: 0.009s for 1024x683 resolution) while maintaining real-time performance on consumer-grade GPUs. The source code will soon be available at this https URL.</li>
<li><strong>摘要：</strong>消除语义重要区域中的几何失真仍然是图像重定向中的一个棘手挑战。本文提出了 Object-IR，这是一种自监督架构，它将图像重定向重新表述为基于学习的网格变形优化问题，其中网格变形由对象外观一致性和几何保持约束引导。给定输入图像和目标纵横比，我们在输出分辨率下初始化均匀刚性网格，并使用卷积神经网络来预测每个网格的运动并获得变形网格。重定向结果是通过根据输入图像中的刚性网格和输出分辨率中的变形网格扭曲输入图像来生成的。为了减轻几何失真，我们设计了一个综合目标函数，其中包含a）对象一致损失以确保重要的语义对象保留其外观，b）几何保留损失以约束重要网格的简单尺度变换，以及c）边界损失以强制干净的矩形输出。值得注意的是，我们的自监督范式通过直接从输入的几何和语义属性派生监督，消除了手动注释重定向数据集的需要。对 RetargetMe 基准的广泛评估表明，我们的 Object-IR 实现了最先进的性能，在定量指标和主观视觉质量评估方面优于现有方法。该框架可有效处理任意输入分辨率（平均推理时间：1024x683 分辨率为 0.009 秒），同时保持消费级 GPU 的实时性能。源代码很快就会在此 https URL 上提供。</li>
</ul>

<h3>Title: RegionRAG: Region-level Retrieval-Augumented Generation for Visually-Rich Documents</h3>
<ul>
<li><strong>Authors: </strong>Yinglu Li, Zhiying Lu, Zhihang Liu, Chuanbin Liu, Hongtao Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.27261">https://arxiv.org/abs/2510.27261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.27261">https://arxiv.org/pdf/2510.27261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.27261]] RegionRAG: Region-level Retrieval-Augumented Generation for Visually-Rich Documents(https://arxiv.org/abs/2510.27261)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multi-modal Retrieval-Augmented Generation (RAG) has become a critical method for empowering LLMs by leveraging candidate visual documents. However, current methods consider the entire document as the basic retrieval unit, introducing substantial irrelevant visual content in two ways: 1) Relevant documents often contain large regions unrelated to the query, diluting the focus on salient information; 2) Retrieving multiple documents to increase recall further introduces redundant and irrelevant documents. These redundant contexts distract the model's attention and further degrade the performance. To address this challenge, we propose \modelname, a novel framework that shifts the retrieval paradigm from the document level to the region level. During training, we design a hybrid supervision strategy from both labeled data and unlabeled data to pinpoint relevant patches. During inference, we propose a dynamic pipeline that intelligently groups salient patches into complete semantic regions. By delegating the task of identifying relevant regions to the retriever, \modelname enables the generator to focus solely on concise visual content relevant to queries, improving both efficiency and accuracy. Experiments on six benchmarks demonstrate that RegionRAG achieves state-of-the-art performance. Improves retrieval accuracy by 10.02\% in R@1 on average and increases question answering accuracy by 3.56\% while using only 71.42\% visual tokens compared to prior methods. The code will be available at this https URL.</li>
<li><strong>摘要：</strong>多模态检索增强生成（RAG）已成为利用候选视觉文档增强法学硕士能力的关键方法。然而，当前的方法将整个文档视为基本检索单元，以两种方式引入大量不相关的视觉内容：1）相关文档通常包含与查询无关的大区域，削弱了对显着信息的关注； 2）检索多个文档以提高召回率进一步引入了冗余和不相关的文档。这些冗余上下文分散了模型的注意力并进一步降低了性能。为了应对这一挑战，我们提出了 \modelname，一个新颖的框架，它将检索范式从文档级别转移到区域级别。在训练过程中，我们根据标记数据和未标记数据设计了一种混合监督策略，以查明相关补丁。在推理过程中，我们提出了一种动态管道，可以将显着的补丁智能地分组为完整的语义区域。通过将识别相关区域的任务委托给检索器，\modelname 使生成器能够仅专注于与查询相关的简洁视觉内容，从而提高效率和准确性。六个基准测试的实验表明 RegionRAG 实现了最先进的性能。与之前的方法相比，在 R@1 中检索准确率平均提高了 10.02%，问答准确率提高了 3.56%，同时仅使用 71.42% 的视觉标记。该代码将在此 https URL 中提供。</li>
</ul>

<h3>Title: Rethinking Robust Adversarial Concept Erasure in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Qinghong Yin, Yu Tian, Yue Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.27285">https://arxiv.org/abs/2510.27285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.27285">https://arxiv.org/pdf/2510.27285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.27285]] Rethinking Robust Adversarial Concept Erasure in Diffusion Models(https://arxiv.org/abs/2510.27285)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Concept erasure aims to selectively unlearning undesirable content in diffusion models (DMs) to reduce the risk of sensitive content generation. As a novel paradigm in concept erasure, most existing methods employ adversarial training to identify and suppress target concepts, thus reducing the likelihood of sensitive outputs. However, these methods often neglect the specificity of adversarial training in DMs, resulting in only partial mitigation. In this work, we investigate and quantify this specificity from the perspective of concept space, i.e., can adversarial samples truly fit the target concept space? We observe that existing methods neglect the role of conceptual semantics when generating adversarial samples, resulting in ineffective fitting of concept spaces. This oversight leads to the following issues: 1) when there are few adversarial samples, they fail to comprehensively cover the object concept; 2) conversely, they will disrupt other target concept spaces. Motivated by the analysis of these findings, we introduce S-GRACE (Semantics-Guided Robust Adversarial Concept Erasure), which grace leveraging semantic guidance within the concept space to generate adversarial samples and perform erasure training. Experiments conducted with seven state-of-the-art methods and three adversarial prompt generation strategies across various DM unlearning scenarios demonstrate that S-GRACE significantly improves erasure performance 26%, better preserves non-target concepts, and reduces training time by 90%. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>概念擦除旨在选择性地消除扩散模型 (DM) 中不需要的内容，以降低敏感内容生成的风险。作为概念擦除的一种新颖范式，大多数现有方法采用对抗性训练来识别和抑制目标概念，从而降低敏感输出的可能性。然而，这些方法往往忽略了 DM 对抗训练的特殊性，导致只能得到部分缓解。在这项工作中，我们从概念空间的角度研究和量化这种特殊性，即对抗性样本能否真正适合目标概念空间？我们观察到现有方法在生成对抗性样本时忽略了概念语义的作用，导致概念空间拟合无效。这种疏忽导致了以下问题：1）当对抗性样本很少时，它们无法全面覆盖对象概念； 2）相反，它们会扰乱其他目标概念空间。在对这些发现的分析的推动下，我们引入了 S-GRACE（语义引导的鲁棒对抗性概念擦除），它利用概念空间内的语义指导来生成对抗性样本并执行擦除训练。使用七种最先进的方法和三种对抗性提示生成策略在各种 DM 遗忘场景中进行的实验表明，S-GRACE 将擦除性能显着提高了 26%，更好地保留了非目标概念，并将训练时间减少了 90%。我们的代码可以在这个 https URL 上找到。</li>
</ul>

<h3>Title: Can LLMs Help You at Work? A Sandbox for Evaluating LLM Agents in Enterprise Environments</h3>
<ul>
<li><strong>Authors: </strong>Harsh Vishwakarma, Ankush Agarwal, Ojas Patil, Chaitanya Devaguptapu, Mahesh Chandran</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.27287">https://arxiv.org/abs/2510.27287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.27287">https://arxiv.org/pdf/2510.27287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.27287]] Can LLMs Help You at Work? A Sandbox for Evaluating LLM Agents in Enterprise Environments(https://arxiv.org/abs/2510.27287)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Enterprise systems are crucial for enhancing productivity and decision-making among employees and customers. Integrating LLM based systems into enterprise systems enables intelligent automation, personalized experiences, and efficient information retrieval, driving operational efficiency and strategic growth. However, developing and evaluating such systems is challenging due to the inherent complexity of enterprise environments, where data is fragmented across multiple sources and governed by sophisticated access controls. We present EnterpriseBench, a comprehensive benchmark that simulates enterprise settings, featuring 500 diverse tasks across software engineering, HR, finance, and administrative domains. Our benchmark uniquely captures key enterprise characteristics including data source fragmentation, access control hierarchies, and cross-functional workflows. Additionally, we provide a novel data generation pipeline that creates internally consistent enterprise tasks from organizational metadata. Experiments with state-of-the-art LLM agents demonstrate that even the most capable models achieve only 41.8% task completion, highlighting significant opportunities for improvement in enterprise-focused AI systems.</li>
<li><strong>摘要：</strong>企业系统对于提高员工和客户的生产力和决策至关重要。将基于法学硕士的系统集成到企业系统中，可实现智能自动化、个性化体验和高效的信息检索，从而推动运营效率和战略增长。然而，由于企业环境固有的复杂性，开发和评估此类系统具有挑战性，企业环境中的数据分散在多个来源中，并受到复杂的访问控制的管理。我们推出 EnterpriseBench，这是一个模拟企业设置的综合基准测试，具有跨软件工程、人力资源、财务和管理领域的 500 项不同任务。我们的基准独特地捕捉了关键的企业特征，包括数据源碎片、访问控制层次结构和跨职能工作流程。此外，我们提供了一种新颖的数据生成管道，可以根据组织元数据创建内部一致的企业任务。使用最先进的 LLM 代理进行的实验表明，即使是最有能力的模型也只能完成 41.8% 的任务，这突显了以企业为中心的 AI 系统的重大改进机会。</li>
</ul>

<h3>Title: Versatile and Efficient Medical Image Super-Resolution Via Frequency-Gated Mamba</h3>
<ul>
<li><strong>Authors: </strong>Wenfeng Huang, Xiangyun Liao, Wei Cao, Wenjing Jia, Weixin Si</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.27296">https://arxiv.org/abs/2510.27296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.27296">https://arxiv.org/pdf/2510.27296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.27296]] Versatile and Efficient Medical Image Super-Resolution Via Frequency-Gated Mamba(https://arxiv.org/abs/2510.27296)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Medical image super-resolution (SR) is essential for enhancing diagnostic accuracy while reducing acquisition cost and scanning time. However, modeling both long-range anatomical structures and fine-grained frequency details with low computational overhead remains challenging. We propose FGMamba, a novel frequency-aware gated state-space model that unifies global dependency modeling and fine-detail enhancement into a lightweight architecture. Our method introduces two key innovations: a Gated Attention-enhanced State-Space Module (GASM) that integrates efficient state-space modeling with dual-branch spatial and channel attention, and a Pyramid Frequency Fusion Module (PFFM) that captures high-frequency details across multiple resolutions via FFT-guided fusion. Extensive evaluations across five medical imaging modalities (Ultrasound, OCT, MRI, CT, and Endoscopic) demonstrate that FGMamba achieves superior PSNR/SSIM while maintaining a compact parameter footprint ($<$0.75M), outperforming CNN-based and Transformer-based SOTAs. Our results validate the effectiveness of frequency-aware state-space modeling for scalable and accurate medical image enhancement.</li>
<li><strong>摘要：</strong>医学图像超分辨率 (SR) 对于提高诊断准确性、同时降低采集成本和扫描时间至关重要。然而，以低计算开销对远程解剖结构和细粒度频率细节进行建模仍然具有挑战性。我们提出了 FGMamba，一种新颖的频率感知门控状态空间模型，它将全局依赖建模和精细细节增强统一到轻量级架构中。我们的方法引入了两项关键创新：门控注意力增强状态空间模块（GASM），将高效的状态空间建模与双分支空间和通道注意力相结合；金字塔频率融合模块（PFFM），通过 FFT 引导融合捕获多个分辨率的高频细节。对五种医学成像模式（超声、OCT、MRI、CT 和内窥镜）的广泛评估表明，FGMamba 实现了卓越的 PSNR/SSIM，同时保持紧凑的参数足迹（$<75 万美元），优于基于 CNN 和基于 Transformer 的 SOTA。我们的结果验证了频率感知状态空间建模对于可扩展且准确的医学图像增强的有效性。</li>
</ul>

<h3>Title: Generative Semantic Coding for Ultra-Low Bitrate Visual Communication and Analysis</h3>
<ul>
<li><strong>Authors: </strong>Weiming Chen, Yijia Wang, Zhihan Zhu, Zhihai He</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.27324">https://arxiv.org/abs/2510.27324</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.27324">https://arxiv.org/pdf/2510.27324</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.27324]] Generative Semantic Coding for Ultra-Low Bitrate Visual Communication and Analysis(https://arxiv.org/abs/2510.27324)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We consider the problem of ultra-low bit rate visual communication for remote vision analysis, human interactions and control in challenging scenarios with very low communication bandwidth, such as deep space exploration, battlefield intelligence, and robot navigation in complex environments. In this paper, we ask the following important question: can we accurately reconstruct the visual scene using only a very small portion of the bit rate in existing coding methods while not sacrificing the accuracy of vision analysis and performance of human interactions? Existing text-to-image generation models offer a new approach for ultra-low bitrate image description. However, they can only achieve a semantic-level approximation of the visual scene, which is far insufficient for the purpose of visual communication and remote vision analysis and human interactions. To address this important issue, we propose to seamlessly integrate image generation with deep image compression, using joint text and coding latent to guide the rectified flow models for precise generation of the visual scene. The semantic text description and coding latent are both encoded and transmitted to the decoder at a very small bit rate. Experimental results demonstrate that our method can achieve the same image reconstruction quality and vision analysis accuracy as existing methods while using much less bandwidth. The code will be released upon paper acceptance.</li>
<li><strong>摘要：</strong>我们考虑超低比特率视觉通信问题，用于通信带宽极低的挑战性场景中的远程视觉分析、人类交互和控制，例如深空探索、战场情报和复杂环境中的机器人导航。在本文中，我们提出以下重要问题：我们能否仅使用现有编码方法中很小一部分的比特率来准确地重建视觉场景，同时不牺牲视觉分析的准确性和人类交互的性能？现有的文本到图像生成模型为超低比特率图像描述提供了一种新方法。然而，它们只能实现视觉场景的语义级近似，这对于视觉通信、远程视觉分析和人类交互的目的来说是远远不够的。为了解决这个重要问题，我们建议将图像生成与深度图像压缩无缝集成，使用联合文本和潜在编码来指导校正流模型以精确生成视觉场景。语义文本描述和潜在编码都被编码并以非常小的比特率传输到解码器。实验结果表明，我们的方法可以在使用更少的带宽的情况下实现与现有方法相同的图像重建质量和视觉分析精度。该代码将在论文被接受后发布。</li>
</ul>

<h3>Title: Fine-Tuning Open Video Generators for Cinematic Scene Synthesis: A Small-Data Pipeline with LoRA and Wan2.1 I2V</h3>
<ul>
<li><strong>Authors: </strong>Meftun Akarsu, Kerem Catay, Sedat Bin Vedat, Enes Kutay Yarkan, Ilke Senturk, Arda Sar, Dafne Eksioglu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.27364">https://arxiv.org/abs/2510.27364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.27364">https://arxiv.org/pdf/2510.27364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.27364]] Fine-Tuning Open Video Generators for Cinematic Scene Synthesis: A Small-Data Pipeline with LoRA and Wan2.1 I2V(https://arxiv.org/abs/2510.27364)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present a practical pipeline for fine-tuning open-source video diffusion transformers to synthesize cinematic scenes for television and film production from small datasets. The proposed two-stage process decouples visual style learning from motion generation. In the first stage, Low-Rank Adaptation (LoRA) modules are integrated into the cross-attention layers of the Wan2.1 I2V-14B model to adapt its visual representations using a compact dataset of short clips from Ay Yapim's historical television film El Turco. This enables efficient domain transfer within hours on a single GPU. In the second stage, the fine-tuned model produces stylistically consistent keyframes that preserve costume, lighting, and color grading, which are then temporally expanded into coherent 720p sequences through the model's video decoder. We further apply lightweight parallelization and sequence partitioning strategies to accelerate inference without quality degradation. Quantitative and qualitative evaluations using FVD, CLIP-SIM, and LPIPS metrics, supported by a small expert user study, demonstrate measurable improvements in cinematic fidelity and temporal stability over the base model. The complete training and inference pipeline is released to support reproducibility and adaptation across cinematic domains.</li>
<li><strong>摘要：</strong>我们提出了一个实用的管道，用于微调开源视频扩散变压器，以从小数据集合成电视和电影制作的电影场景。所提出的两阶段过程将视觉风格学习与运动生成分离。在第一阶段，低秩适应 (LoRA) 模块被集成到 Wan2.1 I2V-14B 模型的交叉注意层中，以使用 Ay Yapim 的历史电视电影 El Turco 中的简短剪辑数据集来调整其视觉表示。这使得单个 GPU 能够在数小时内实现高效的域传输。在第二阶段，经过微调的模型会生成风格一致的关键帧，保留服装、灯光和颜色分级，然后通过模型的视频解码器将其临时扩展为连贯的 720p 序列。我们进一步应用轻量级并行化和序列分区策略来加速推理而不降低质量。在小型专家用户研究的支持下，使用 FVD、CLIP-SIM 和 LPIPS 指标进行的定量和定性评估表明，与基本模型相比，电影保真度和时间稳定性有了可测量的改进。发布了完整的训练和推理管道，以支持跨电影领域的再现性和适应性。</li>
</ul>

<h3>Title: A Hybrid Deep Learning and Forensic Approach for Robust Deepfake Detection</h3>
<ul>
<li><strong>Authors: </strong>Sales Aribe Jr</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.27392">https://arxiv.org/abs/2510.27392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.27392">https://arxiv.org/pdf/2510.27392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.27392]] A Hybrid Deep Learning and Forensic Approach for Robust Deepfake Detection(https://arxiv.org/abs/2510.27392)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid evolution of generative adversarial networks (GANs) and diffusion models has made synthetic media increasingly realistic, raising societal concerns around misinformation, identity fraud, and digital trust. Existing deepfake detection methods either rely on deep learning, which suffers from poor generalization and vulnerability to distortions, or forensic analysis, which is interpretable but limited against new manipulation techniques. This study proposes a hybrid framework that fuses forensic features, including noise residuals, JPEG compression traces, and frequency-domain descriptors, with deep learning representations from convolutional neural networks (CNNs) and vision transformers (ViTs). Evaluated on benchmark datasets (FaceForensics++, Celeb-DF v2, DFDC), the proposed model consistently outperformed single-method baselines and demonstrated superior performance compared to existing state-of-the-art hybrid approaches, achieving F1-scores of 0.96, 0.82, and 0.77, respectively. Robustness tests demonstrated stable performance under compression (F1 = 0.87 at QF = 50), adversarial perturbations (AUC = 0.84), and unseen manipulations (F1 = 0.79). Importantly, explainability analysis showed that Grad-CAM and forensic heatmaps overlapped with ground-truth manipulated regions in 82 percent of cases, enhancing transparency and user trust. These findings confirm that hybrid approaches provide a balanced solution, combining the adaptability of deep models with the interpretability of forensic cues, to develop resilient and trustworthy deepfake detection systems.</li>
<li><strong>摘要：</strong>生成对抗网络 (GAN) 和扩散模型的快速发展使合成媒体变得越来越现实，引发了社会对错误信息、身份欺诈和数字信任的担忧。现有的深度造假检测方法要么依赖于深度学习，但其泛化性较差且容易扭曲；或者依赖于取证分析，后者是可解释的，但对新的操纵技术来说受到限制。这项研究提出了一种混合框架，它将取证特征（包括噪声残差、JPEG 压缩轨迹和频域描述符）与来自卷积神经网络 (CNN) 和视觉变换器 (ViT) 的深度学习表示融合在一起。在基准数据集（FaceForensics++、Celeb-DF v2、DFDC）上进行评估，所提出的模型始终优于单一方法基线，并且与现有最先进的混合方法相比表现出卓越的性能，分别实现了 0.96、0.82 和 0.77 的 F1 分数。鲁棒性测试证明了在压缩（F1 = 0.87，QF = 50）、对抗性扰动（AUC = 0.84）和看不见的操作（F1 = 0.79）下的稳定性能。重要的是，可解释性分析表明，Grad-CAM 和取证热图在 82% 的情况下与地面实况操纵区域重叠，从而提高了透明度和用户信任度。这些发现证实，混合方法提供了一种平衡的解决方案，将深度模型的适应性与取证线索的可解释性相结合，以开发有弹性且值得信赖的深度伪造检测系统。</li>
</ul>

<h3>Title: Atlas-Alignment: Making Interpretability Transferable Across Language Models</h3>
<ul>
<li><strong>Authors: </strong>Bruno Puri, Jim Berend, Sebastian Lapuschkin, Wojciech Samek</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.27413">https://arxiv.org/abs/2510.27413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.27413">https://arxiv.org/pdf/2510.27413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.27413]] Atlas-Alignment: Making Interpretability Transferable Across Language Models(https://arxiv.org/abs/2510.27413)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Interpretability is crucial for building safe, reliable, and controllable language models, yet existing interpretability pipelines remain costly and difficult to scale. Interpreting a new model typically requires costly training of model-specific sparse autoencoders, manual or semi-automated labeling of SAE components, and their subsequent validation. We introduce Atlas-Alignment, a framework for transferring interpretability across language models by aligning unknown latent spaces to a Concept Atlas - a labeled, human-interpretable latent space - using only shared inputs and lightweight representational alignment techniques. Once aligned, this enables two key capabilities in previously opaque models: (1) semantic feature search and retrieval, and (2) steering generation along human-interpretable atlas concepts. Through quantitative and qualitative evaluations, we show that simple representational alignment methods enable robust semantic retrieval and steerable generation without the need for labeled concept data. Atlas-Alignment thus amortizes the cost of explainable AI and mechanistic interpretability: by investing in one high-quality Concept Atlas, we can make many new models transparent and controllable at minimal marginal cost.</li>
<li><strong>摘要：</strong>可解释性对于构建安全、可靠和可控的语言模型至关重要，但现有的可解释性管道仍然成本高昂且难以扩展。解释新模型通常需要对特定于模型的稀疏自动编码器进行昂贵的训练、对 SAE 组件进行手动或半自动标记及其后续验证。我们引入了 Atlas-Alignment，这是一个跨语言模型转移可解释性的框架，通过将未知的潜在空间与概念图集（一个标记的、人类可解释的潜在空间）对齐，仅使用共享输入和轻量级表征对齐技术。一旦对齐，这将在以前不透明的模型中实现两个关键功能：（1）语义特征搜索和检索，以及（2）沿着人类可解释的图集概念引导生成。通过定量和定性评估，我们表明简单的表征对齐方法可以实现强大的语义检索和可操纵的生成，而无需标记的概念数据。因此，Atlas-Alignment 摊销了可解释的人工智能和机械可解释性的成本：通过投资一个高质量的概念图集，我们可以以最小的边际成本使许多新模型透明且可控。</li>
</ul>

<h3>Title: From Pixels to Paths: A Multi-Agent Framework for Editable Scientific Illustration</h3>
<ul>
<li><strong>Authors: </strong>Jianwen Sun, Fanrui Zhang, Yukang Feng, Chuanhao Li, Zizhen Li, Jiaxin Ai, Yifan Chang, Yu Dai, Kaipeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.27452">https://arxiv.org/abs/2510.27452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.27452">https://arxiv.org/pdf/2510.27452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.27452]] From Pixels to Paths: A Multi-Agent Framework for Editable Scientific Illustration(https://arxiv.org/abs/2510.27452)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Scientific illustrations demand both high information density and post-editability. However, current generative models have two major limitations: Frist, image generation models output rasterized images lacking semantic structure, making it impossible to access, edit, or rearrange independent visual components in the images. Second, code-based generation methods (TikZ or SVG), although providing element-level control, force users into the cumbersome cycle of "writing-compiling-reviewing" and lack the intuitiveness of manipulation. Neither of these two approaches can well meet the needs for efficiency, intuitiveness, and iterative modification in scientific creation. To bridge this gap, we introduce VisPainter, a multi-agent framework for scientific illustration built upon the model context protocol. VisPainter orchestrates three specialized modules-a Manager, a Designer, and a Toolbox-to collaboratively produce diagrams compatible with standard vector graphics software. This modular, role-based design allows each element to be explicitly represented and manipulated, enabling true element-level control and any element can be added and modified later. To systematically evaluate the quality of scientific illustrations, we introduce VisBench, a benchmark with seven-dimensional evaluation metrics. It assesses high-information-density scientific illustrations from four aspects: content, layout, visual perception, and interaction cost. To this end, we conducted extensive ablation experiments to verify the rationality of our architecture and the reliability of our evaluation methods. Finally, we evaluated various vision-language models, presenting fair and credible model rankings along with detailed comparisons of their respective capabilities. Additionally, we isolated and quantified the impacts of role division, step control,and description on the quality of illustrations.</li>
<li><strong>摘要：</strong>科学插图需要高信息密度和后期可编辑性。然而，当前的生成模型有两个主要局限性：首先，图像生成模型输出缺乏语义结构的光栅化图像，使得无法访问、编辑或重新排列图像中的独立视觉组件。其次，基于代码的生成方法（TikZ或SVG）虽然提供了元素级的控制，但迫使用户陷入“编写-编译-审阅”的繁琐循环，缺乏操作的直观性。这两种方式都不能很好地满足科学创作中对效率、直观性和迭代修改的需求。为了弥补这一差距，我们引入了 VisPainter，这是一个基于模型上下文协议的科学插图多代理框架。 VisPainter 协调三个专用模块（管理器、设计器和工具箱），以协作生成与标准矢量图形软件兼容的图表。这种模块化、基于角色的设计允许显式地表示和操作每个元素，从而实现真正的元素级控制，并且可以在以后添加和修改任何元素。为了系统地评估科学插图的质量，我们引入了 VisBench，一个具有七维评估指标的基准。它从内容、布局、视觉感知和交互成本四个方面评估高信息密度的科学插图。为此，我们进行了广泛的消融实验，以验证我们架构的合理性和评估方法的可靠性。最后，我们评估了各种视觉语言模型，给出了公平可信的模型排名以及它们各自能力的详细比较。此外，我们还隔离并量化了角色划分、步骤控制和描述对插图质量的影响。</li>
</ul>

<h3>Title: Referee: Reference-aware Audiovisual Deepfake Detection</h3>
<ul>
<li><strong>Authors: </strong>Hyemin Boo, Eunsang Lee, Jiyoung Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.27475">https://arxiv.org/abs/2510.27475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.27475">https://arxiv.org/pdf/2510.27475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.27475]] Referee: Reference-aware Audiovisual Deepfake Detection(https://arxiv.org/abs/2510.27475)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Since deepfakes generated by advanced generative models have rapidly posed serious threats, existing audiovisual deepfake detection approaches struggle to generalize to unseen forgeries. We propose a novel reference-aware audiovisual deepfake detection method, called Referee. Speaker-specific cues from only one-shot examples are leveraged to detect manipulations beyond spatiotemporal artifacts. By matching and aligning identity-related queries from reference and target content into cross-modal features, Referee jointly reasons about audiovisual synchrony and identity consistency. Extensive experiments on FakeAVCeleb, FaceForensics++, and KoDF demonstrate that Referee achieves state-of-the-art performance on cross-dataset and cross-language evaluation protocols. Experimental results highlight the importance of cross-modal identity verification for future deepfake detection. The code is available at this https URL.</li>
<li><strong>摘要：</strong>由于先进生成模型生成的深度伪造品已迅速构成严重威胁，现有的视听深度伪造品检测方法很难推广到看不见的伪造品。我们提出了一种新颖的参考感知视听深度伪造检测方法，称为 Referee。仅利用来自一次性示例的特定于说话人的线索来检测时空伪影之外的操作。通过将来自参考和目标内容的身份相关查询匹配和对齐到跨模式特征，裁判共同推理视听同步和身份一致性。在 FakeAVCeleb、FaceForensics++ 和 KoDF 上进行的大量实验表明，Referee 在跨数据集和跨语言评估协议上实现了最先进的性能。实验结果凸显了跨模式身份验证对于未来深度伪造检测的重要性。该代码可从此 https URL 获取。</li>
</ul>

<h3>Title: InertialAR: Autoregressive 3D Molecule Generation with Inertial Frames</h3>
<ul>
<li><strong>Authors: </strong>Haorui Li, Weitao Du, Yuqiang Li, Hongyu Guo, Shengchao Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.27497">https://arxiv.org/abs/2510.27497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.27497">https://arxiv.org/pdf/2510.27497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.27497]] InertialAR: Autoregressive 3D Molecule Generation with Inertial Frames(https://arxiv.org/abs/2510.27497)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Transformer-based autoregressive models have emerged as a unifying paradigm across modalities such as text and images, but their extension to 3D molecule generation remains underexplored. The gap stems from two fundamental challenges: (1) tokenizing molecules into a canonical 1D sequence of tokens that is invariant to both SE(3) transformations and atom index permutations, and (2) designing an architecture capable of modeling hybrid atom-based tokens that couple discrete atom types with continuous 3D coordinates. To address these challenges, we introduce InertialAR. InertialAR devises a canonical tokenization that aligns molecules to their inertial frames and reorders atoms to ensure SE(3) and permutation invariance. Moreover, InertialAR equips the attention mechanism with geometric awareness via geometric rotary positional encoding (GeoRoPE). In addition, it utilizes a hierarchical autoregressive paradigm to predict the next atom-based token, predicting the atom type first and then its 3D coordinates via Diffusion loss. Experimentally, InertialAR achieves state-of-the-art performance on 7 of the 10 evaluation metrics for unconditional molecule generation across QM9, GEOM-Drugs, and B3LYP. Moreover, it significantly outperforms strong baselines in controllable generation for targeted chemical functionality, attaining state-of-the-art results across all 5 metrics.</li>
<li><strong>摘要：</strong>基于 Transformer 的自回归模型已成为跨文本和图像等模式的统一范式，但其对 3D 分子生成的扩展仍未得到充分探索。这一差距源于两个基本挑战：(1) 将分子标记为规范的 1D 标记序列，该序列对于 SE(3) 变换和原子索引排列都是不变的；(2) 设计一种能够对基于混合原子的标记进行建模的架构，该架构将离散原子类型与连续 3D 坐标耦合起来。为了应对这些挑战，我们引入了 InertialAR。 InertialAR 设计了一种规范标记化，将分子与其惯性框架对齐并重新排序原子，以确保 SE(3) 和排列不变性。此外，InertialAR 通过几何旋转位置编码（GeoRoPE）为注意力机制配备几何感知。此外，它利用分层自回归范式来预测下一个基于原子的标记，首先预测原子类型，然后通过扩散损失预测其 3D 坐标。实验上，InertialAR 在 QM9、GEOM-Drugs 和 B3LYP 的无条件分子生成的 10 个评估指标中的 7 个上实现了最先进的性能。此外，它在目标化学功能的可控生成方面显着优于强大的基线，在所有 5 个指标上均取得了最先进的结果。</li>
</ul>

<h3>Title: AstuteRAG-FQA: Task-Aware Retrieval-Augmented Generation Framework for Proprietary Data Challenges in Financial Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Zahangir Alam, Khandoker Ashik Uz Zaman, Mahdi H. Miraz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.27537">https://arxiv.org/abs/2510.27537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.27537">https://arxiv.org/pdf/2510.27537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.27537]] AstuteRAG-FQA: Task-Aware Retrieval-Augmented Generation Framework for Proprietary Data Challenges in Financial Question Answering(https://arxiv.org/abs/2510.27537)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) shows significant promise in knowledge-intensive tasks by improving domain specificity, enhancing temporal relevance, and reducing hallucinations. However, applying RAG to finance encounters critical challenges: restricted access to proprietary datasets, limited retrieval accuracy, regulatory constraints, and sensitive data interpretation. We introduce AstuteRAG-FQA, an adaptive RAG framework tailored for Financial Question Answering (FQA), leveraging task-aware prompt engineering to address these challenges. The framework uses a hybrid retrieval strategy integrating both open-source and proprietary financial data while maintaining strict security protocols and regulatory compliance. A dynamic prompt framework adapts in real time to query complexity, improving precision and contextual relevance. To systematically address diverse financial queries, we propose a four-tier task classification: explicit factual, implicit factual, interpretable rationale, and hidden rationale involving implicit causal reasoning. For each category, we identify key challenges, datasets, and optimization techniques within the retrieval and generation process. The framework incorporates multi-layered security mechanisms including differential privacy, data anonymization, and role-based access controls to protect sensitive financial information. Additionally, AstuteRAG-FQA implements real-time compliance monitoring through automated regulatory validation systems that verify responses against industry standards and legal obligations. We evaluate three data integration techniques - contextual embedding, small model augmentation, and targeted fine-tuning - analyzing their efficiency and feasibility across varied financial environments.</li>
<li><strong>摘要：</strong>检索增强生成（RAG）通过提高领域特异性、增强时间相关性和减少幻觉，在知识密集型任务中显示出巨大的前景。然而，将 RAG 应用到金融领域遇到了严峻的挑战：专有数据集的访问受到限制、检索准确性有限、监管限制和敏感数据解释。我们推出 AstuteRAG-FQA，这是一种专为财务问答 (FQA) 量身定制的自适应 RAG 框架，利用任务感知提示工程来应对这些挑战。该框架采用混合检索策略，集成开源和专有财务数据，同时保持严格的安全协议和监管合规性。动态提示框架实时适应查询的复杂性，提高精度和上下文相关性。为了系统地解决不同的金融查询，我们提出了四层任务分类：显性事实、隐性事实、可解释的基本原理和涉及隐性因果推理的隐性基本原理。对于每个类别，我们都确定了检索和生成过程中的关键挑战、数据集和优化技术。该框架结合了多层安全机制，包括差异隐私、数据匿名化和基于角色的访问控制，以保护敏感的财务信息。此外，AstuteRAG-FQA 通过自动监管验证系统实施实时合规性监控，该系统根据行业标准和法律义务验证响应。我们评估了三种数据集成技术——上下文嵌入、小模型增强和有针对性的微调——分析它们在不同金融环境中的效率和可行性。</li>
</ul>

<h3>Title: Who Made This? Fake Detection and Source Attribution with Diffusion Features</h3>
<ul>
<li><strong>Authors: </strong>Simone Bonechi, Paolo Andreini, Barbara Toniella Corradini</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.27602">https://arxiv.org/abs/2510.27602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.27602">https://arxiv.org/pdf/2510.27602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.27602]] Who Made This? Fake Detection and Source Attribution with Diffusion Features(https://arxiv.org/abs/2510.27602)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid progress of generative diffusion models has enabled the creation of synthetic images that are increasingly difficult to distinguish from real ones, raising concerns about authenticity, copyright, and misinformation. Existing supervised detectors often struggle to generalize across unseen generators, requiring extensive labeled data and frequent retraining. We introduce FRIDA (Fake-image Recognition and source Identification via Diffusion-features Analysis), a lightweight framework that leverages internal activations from a pre-trained diffusion model for deepfake detection and source generator attribution. A k-nearest-neighbor classifier applied to diffusion features achieves state-of-the-art cross-generator performance without fine-tuning, while a compact neural model enables accurate source attribution. These results show that diffusion representations inherently encode generator-specific patterns, providing a simple and interpretable foundation for synthetic image forensics.</li>
<li><strong>摘要：</strong>生成扩散模型的快速进步使得合成图像的创建变得越来越难以与真实图像区分开来，引发了人们对真实性、版权和错误信息的担忧。现有的监督检测器通常很难在看不见的生成器上进行泛化，需要大量的标记数据和频繁的再训练。我们引入了 FRIDA（通过扩散特征分析进行假图像识别和源识别），这是一个轻量级框架，利用预训练扩散模型的内部激活进行深度伪造检测和源生成器归因。应用于扩散特征的 k 最近邻分类器无需微调即可实现最先进的跨生成器性能，而紧凑的神经模型可以实现准确的源归因。这些结果表明，扩散表示本质上编码了生成器特定的模式，为合成图像取证提供了简单且可解释的基础。</li>
</ul>

<h3>Title: Sketch-to-Layout: Sketch-Guided Multimodal Layout Generation</h3>
<ul>
<li><strong>Authors: </strong>Riccardo Brioschi, Aleksandr Alekseev, Emanuele Nevali, Berkay Döner, Omar El Malki, Blagoj Mitrevski, Leandro Kieliger, Mark Collier, Andrii Maksai, Jesse Berent, Claudiu Musat, Efi Kokiopoulou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.27632">https://arxiv.org/abs/2510.27632</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.27632">https://arxiv.org/pdf/2510.27632</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.27632]] Sketch-to-Layout: Sketch-Guided Multimodal Layout Generation(https://arxiv.org/abs/2510.27632)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Graphic layout generation is a growing research area focusing on generating aesthetically pleasing layouts ranging from poster designs to documents. While recent research has explored ways to incorporate user constraints to guide the layout generation, these constraints often require complex specifications which reduce usability. We introduce an innovative approach exploiting user-provided sketches as intuitive constraints and we demonstrate empirically the effectiveness of this new guidance method, establishing the sketch-to-layout problem as a promising research direction, which is currently under-explored. To tackle the sketch-to-layout problem, we propose a multimodal transformer-based solution using the sketch and the content assets as inputs to produce high quality layouts. Since collecting sketch training data from human annotators to train our model is very costly, we introduce a novel and efficient method to synthetically generate training sketches at scale. We train and evaluate our model on three publicly available datasets: PubLayNet, DocLayNet and SlidesVQA, demonstrating that it outperforms state-of-the-art constraint-based methods, while offering a more intuitive design experience. In order to facilitate future sketch-to-layout research, we release O(200k) synthetically-generated sketches for the public datasets above. The datasets are available at this https URL.</li>
<li><strong>摘要：</strong>图形布局生成是一个不断发展的研究领域，专注于生成从海报设计到文档的美观布局。虽然最近的研究已经探索了合并用户约束来指导布局生成的方法，但这些约束通常需要复杂的规范，从而降低了可用性。我们引入了一种创新方法，利用用户提供的草图作为直观约束，并通过实证证明了这种新指导方法的有效性，将草图到布局问题确立为一个有前途的研究方向，目前尚未得到充分探索。为了解决草图到布局的问题，我们提出了一种基于多模式变压器的解决方案，使用草图和内容资产作为输入来生成高质量的布局。由于从人类注释者那里收集草图训练数据来训练我们的模型非常昂贵，因此我们引入了一种新颖且有效的方法来大规模地综合生成训练草图。我们在三个公开可用的数据集上训练和评估我们的模型：PubLayNet、DocLayNet 和 SlidesVQA，证明它优于最先进的基于约束的方法，同时提供更直观的设计体验。为了促进未来的草图到布局研究，我们为上述公共数据集发布了 O(200k) 综合生成的草图。数据集可从此 https URL 获取。</li>
</ul>

<h3>Title: PETAR: Localized Findings Generation with Mask-Aware Vision-Language Modeling for PET Automated Reporting</h3>
<ul>
<li><strong>Authors: </strong>Danyal Maqbool, Changhee Lee, Zachary Huemann, Samuel D. Church, Matthew E. Larson, Scott B. Perlman, Tomas A. Romero, Joshua D. Warner, Meghan Lubner, Xin Tie, Jameson Merkow, Junjie Hu, Steve Y. Cho, Tyler J. Bradshaw</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.27680">https://arxiv.org/abs/2510.27680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.27680">https://arxiv.org/pdf/2510.27680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.27680]] PETAR: Localized Findings Generation with Mask-Aware Vision-Language Modeling for PET Automated Reporting(https://arxiv.org/abs/2510.27680)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in vision-language models (VLMs) have enabled impressive multimodal reasoning, yet most medical applications remain limited to 2D imaging. In this work, we extend VLMs to 3D positron emission tomography and computed tomography (PET/CT), a domain characterized by large volumetric data, small and dispersed lesions, and lengthy radiology reports. We introduce a large-scale dataset comprising over 11,000 lesion-level descriptions paired with 3D segmentations from more than 5,000 PET/CT exams, extracted via a hybrid rule-based and large language model (LLM) pipeline. Building upon this dataset, we propose PETAR-4B, a 3D mask-aware vision-language model that integrates PET, CT, and lesion contours for spatially grounded report generation. PETAR bridges global contextual reasoning with fine-grained lesion awareness, producing clinically coherent and localized findings. Comprehensive automated and human evaluations demonstrate that PETAR substantially improves PET/CT report generation quality, advancing 3D medical vision-language understanding.</li>
<li><strong>摘要：</strong>视觉语言模型 (VLM) 的最新进展实现了令人印象深刻的多模态推理，但大多数医学应用仍然仅限于 2D 成像。在这项工作中，我们将 VLM 扩展到 3D 正电子发射断层扫描和计算机断层扫描 (PET/CT)，这是一个以大体积数据、小而分散的病变以及冗长的放射学报告为特征的领域。我们引入了一个大规模数据集，其中包含 11,000 多个病变级别描述，以及来自 5,000 多个 PET/CT 检查的 3D 分割，这些数据是通过基于规则的混合和大型语言模型 (LLM) 管道提取的。在此数据集的基础上，我们提出了 PETAR-4B，这是一种 3D 掩模感知视觉语言模型，它集成了 PET、CT 和病变轮廓，用于生成基于空间的报告。 PETAR 将全局上下文推理与细粒度病变意识联系起来，产生临床连贯和局部的发现。全面的自动化和人工评估表明，PETAR 显着提高了 PET/CT 报告生成质量，促进了 3D 医学视觉语言理解。</li>
</ul>

<h3>Title: Phased DMD: Few-step Distribution Matching Distillation via Score Matching within Subintervals</h3>
<ul>
<li><strong>Authors: </strong>Xiangyu Fan, Zesong Qiu, Zhuguanyu Wu, Fanzhou Wang, Zhiqian Lin, Tianxiang Ren, Dahua Lin, Ruihao Gong, Lei Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.27684">https://arxiv.org/abs/2510.27684</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.27684">https://arxiv.org/pdf/2510.27684</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.27684]] Phased DMD: Few-step Distribution Matching Distillation via Score Matching within Subintervals(https://arxiv.org/abs/2510.27684)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Distribution Matching Distillation (DMD) distills score-based generative models into efficient one-step generators, without requiring a one-to-one correspondence with the sampling trajectories of their teachers. However, limited model capacity causes one-step distilled models underperform on complex generative tasks, e.g., synthesizing intricate object motions in text-to-video generation. Directly extending DMD to multi-step distillation increases memory usage and computational depth, leading to instability and reduced efficiency. While prior works propose stochastic gradient truncation as a potential solution, we observe that it substantially reduces the generation diversity of multi-step distilled models, bringing it down to the level of their one-step counterparts. To address these limitations, we propose Phased DMD, a multi-step distillation framework that bridges the idea of phase-wise distillation with Mixture-of-Experts (MoE), reducing learning difficulty while enhancing model capacity. Phased DMD is built upon two key ideas: progressive distribution matching and score matching within subintervals. First, our model divides the SNR range into subintervals, progressively refining the model to higher SNR levels, to better capture complex distributions. Next, to ensure the training objective within each subinterval is accurate, we have conducted rigorous mathematical derivations. We validate Phased DMD by distilling state-of-the-art image and video generation models, including Qwen-Image (20B parameters) and Wan2.2 (28B parameters). Experimental results demonstrate that Phased DMD preserves output diversity better than DMD while retaining key generative capabilities. We will release our code and models.</li>
<li><strong>摘要：</strong>分布匹配蒸馏（DMD）将基于分数的生成模型蒸馏为高效的一步生成器，而不需要与教师的采样轨迹一一对应。然而，有限的模型容量导致一步蒸馏模型在复杂的生成任务上表现不佳，例如在文本到视频生成中合成复杂的对象运动。直接将 DMD 扩展到多步蒸馏会增加内存使用量和计算深度，从而导致不稳定和效率降低。虽然之前的工作提出随机梯度截断作为一种潜在的解决方案，但我们观察到它大大降低了多步蒸馏模型的生成多样性，使其降至单步模型的水平。为了解决这些限制，我们提出了 Phased DMD，这是一种多步骤蒸馏框架，它将分相蒸馏的思想与专家混合 (MoE) 结合起来，降低学习难度，同时增强模型能力。阶段性 DMD 建立在两个关键思想之上：渐进分布匹配和子区间内的分数匹配。首先，我们的模型将 SNR 范围划分为子区间，逐步将模型细化到更高的 SNR 级别，以更好地捕获复杂的分布。接下来，为了确保每个子区间内的训练目标准确，我们进行了严格的数学推导。我们通过提炼最先进的图像和视频生成模型（包括 Qwen-Image（20B 参数）和 Wan2.2（28B 参数））来验证 Phased DMD。实验结果表明，Phased DMD 比 DMD 更好地保留了输出多样性，同时保留了关键的生成能力。我们将发布我们的代码和模型。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
