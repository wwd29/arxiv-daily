<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-11-20</h1>
<h3>Title: B-Rep Distance Functions (BR-DF): How to Represent a B-Rep Model by Volumetric Distance Functions?</h3>
<ul>
<li><strong>Authors: </strong>Fuyang Zhang, Pradeep Kumar Jayaraman, Xiang Xu, Yasutaka Furukawa</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14870">https://arxiv.org/abs/2511.14870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14870">https://arxiv.org/pdf/2511.14870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14870]] B-Rep Distance Functions (BR-DF): How to Represent a B-Rep Model by Volumetric Distance Functions?(https://arxiv.org/abs/2511.14870)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This paper presents a novel geometric representation for CAD Boundary Representation (B-Rep) based on volumetric distance functions, dubbed B-Rep Distance Functions (BR-DF). BR-DF encodes the surface mesh geometry of a CAD model as signed distance function (SDF). B-Rep vertices, edges, faces and their topology information are encoded as per-face unsigned distance functions (UDFs). An extension of the Marching Cubes algorithm converts BR-DF directly into watertight CAD B-Rep model (strictly speaking a faceted B-Rep model). A surprising characteristic of BR-DF is that this conversion process never fails. Leveraging the volumetric nature of BR-DF, we propose a multi-branch latent diffusion with 3D U-Net backbone for jointly generating the SDF and per-face UDFs of a BR-DF model. Our approach achieves comparable CAD generation performance against SOTA methods while reaching the unprecedented 100% success rate in producing (faceted) B-Rep models.</li>
<li><strong>摘要：</strong>本文提出了一种基于体积距离函数的 CAD 边界表示 (B-Rep) 的新颖几何表示，称为 B-Rep 距离函数 (BR-DF)。 BR-DF 将 CAD 模型的表面网格几何形状编码为有符号距离函数 (SDF)。 B-Rep 顶点、边、面及其拓扑信息被编码为每个面的无符号距离函数 (UDF)。 Marching Cubes 算法的扩展将 BR-DF 直接转换为无懈可击的 CAD B-Rep 模型（严格来说是多面 B-Rep 模型）。 BR-DF 的一个令人惊讶的特性是此转换过程永远不会失败。利用 BR-DF 的体积性质，我们提出了一种具有 3D U-Net 主干的多分支潜在扩散，用于联合生成 BR-DF 模型的 SDF 和每面 UDF。我们的方法实现了与 SOTA 方法相当的 CAD 生成性能，同时在生成（多面）B-Rep 模型方面达到了前所未有的 100% 成功率。</li>
</ul>

<h3>Title: GeoSceneGraph: Geometric Scene Graph Diffusion Model for Text-guided 3D Indoor Scene Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Antonio Ruiz, Tao Wu, Andrew Melnik, Qing Cheng, Xuqin Wang, Lu Liu, Yongliang Wang, Yanfeng Zhang, Helge Ritter</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14884">https://arxiv.org/abs/2511.14884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14884">https://arxiv.org/pdf/2511.14884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14884]] GeoSceneGraph: Geometric Scene Graph Diffusion Model for Text-guided 3D Indoor Scene Synthesis(https://arxiv.org/abs/2511.14884)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Methods that synthesize indoor 3D scenes from text prompts have wide-ranging applications in film production, interior design, video games, virtual reality, and synthetic data generation for training embodied agents. Existing approaches typically either train generative models from scratch or leverage vision-language models (VLMs). While VLMs achieve strong performance, particularly for complex or open-ended prompts, smaller task-specific models remain necessary for deployment on resource-constrained devices such as extended reality (XR) glasses or mobile phones. However, many generative approaches that train from scratch overlook the inherent graph structure of indoor scenes, which can limit scene coherence and realism. Conversely, methods that incorporate scene graphs either demand a user-provided semantic graph, which is generally inconvenient and restrictive, or rely on ground-truth relationship annotations, limiting their capacity to capture more varied object interactions. To address these challenges, we introduce GeoSceneGraph, a method that synthesizes 3D scenes from text prompts by leveraging the graph structure and geometric symmetries of 3D scenes, without relying on predefined relationship classes. Despite not using ground-truth relationships, GeoSceneGraph achieves performance comparable to methods that do. Our model is built on equivariant graph neural networks (EGNNs), but existing EGNN approaches are typically limited to low-dimensional conditioning and are not designed to handle complex modalities such as text. We propose a simple and effective strategy for conditioning EGNNs on text features, and we validate our design through ablation studies.</li>
<li><strong>摘要：</strong>根据文本提示合成室内 3D 场景的方法在电影制作、室内设计、视频游戏、虚拟现实和用于训练具体代理的合成数据生成方面有着广泛的应用。现有方法通常要么从头开始训练生成模型，要么利用视觉语言模型（VLM）。虽然 VLM 实现了强大的性能，特别是对于复杂或开放式提示，但在资源受限的设备（例如扩展现实 (XR) 眼镜或手机）上部署仍然需要较小的特定于任务的模型。然而，许多从头开始训练的生成方法忽略了室内场景固有的图形结构，这可能会限制场景的连贯性和真实感。相反，结合场景图的方法要么需要用户提供的语义图，这通常不方便且具有限制性，要么依赖真实关系注释，限制了它们捕获更多样化的对象交互的能力。为了解决这些挑战，我们引入了 GeoSceneGraph，这是一种通过利用 3D 场景的图形结构和几何对称性，从文本提示合成 3D 场景的方法，而不依赖于预定义的关系类。尽管没有使用真实关系，GeoSceneGraph 的性能与使用真实关系的方法相当。我们的模型建立在等变图神经网络（EGNN）的基础上，但现有的 EGNN 方法通常仅限于低维条件，并且不适用于处理文本等复杂模式。我们提出了一种简单而有效的策略，用于根据文本特征调节 EGNN，并通过消融研究验证我们的设计。</li>
</ul>

<h3>Title: HULFSynth : An INR based Super-Resolution and Ultra Low-Field MRI Synthesis via Contrast factor estimation</h3>
<ul>
<li><strong>Authors: </strong>Pranav Indrakanti, Ivor Simpson</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14897">https://arxiv.org/abs/2511.14897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14897">https://arxiv.org/pdf/2511.14897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14897]] HULFSynth : An INR based Super-Resolution and Ultra Low-Field MRI Synthesis via Contrast factor estimation(https://arxiv.org/abs/2511.14897)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>We present an unsupervised single image bidirectional Magnetic Resonance Image (MRI) synthesizer that synthesizes an Ultra-Low Field (ULF) like image from a High-Field (HF) magnitude image and vice-versa. Unlike existing MRI synthesis models, our approach is inspired by the physics that drives contrast changes between HF and ULF MRIs. Our forward model simulates a HF to ULF transformation by estimating the tissue-type Signal-to-Noise ratio (SNR) values based on target contrast values. For the Super-Resolution task, we used an Implicit Neural Representation (INR) network to synthesize HF image by simultaneously predicting tissue-type segmentations and image intensity without observed HF data. The proposed method is evaluated using synthetic ULF-like data from generated from standard 3T T$_1$-weighted images for qualitative assessments and paired 3T-64mT T$_1$-weighted images for validation experiments. WM-GM contrast improved by 52% in synthetic ULF-like images and 37% in 64mT images. Sensitivity experiments demonstrated the robustness of our forward model to variations in target contrast, noise and initial seeding.</li>
<li><strong>摘要：</strong>我们提出了一种无监督单图像双向磁共振图像（MRI）合成器，它可以从高场（HF）幅度图像合成超低场（ULF）类图像，反之亦然。与现有的 MRI 合成模型不同，我们的方法受到驱动 HF 和 ULF MRI 之间对比度变化的物理原理的启发。我们的正向模型通过根据目标对比度值估计组织类型信噪比 (SNR) 值来模拟 HF 到 ULF 的转换。对于超分辨率任务，我们使用隐式神经表示（INR）网络通过在没有观察到 HF 数据的情况下同时预测组织类型分割和图像强度来合成 HF 图像。使用从用于定性评估的标准 3T T$_1$ 加权图像生成的合成类 ULF 数据和用于验证实验的配对 3T-64mT T$_1$ 加权图像来评估所提出的方法。 WM-GM 对比度在合成类 ULF 图像中提高了 52%，在 64mT 图像中提高了 37%。灵敏度实验证明了我们的前向模型对目标对比度、噪声和初始种子变化的鲁棒性。</li>
</ul>

<h3>Title: InstructMix2Mix: Consistent Sparse-View Editing Through Multi-View Model Personalization</h3>
<ul>
<li><strong>Authors: </strong>Daniel Gilo, Or Litany</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14899">https://arxiv.org/abs/2511.14899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14899">https://arxiv.org/pdf/2511.14899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14899]] InstructMix2Mix: Consistent Sparse-View Editing Through Multi-View Model Personalization(https://arxiv.org/abs/2511.14899)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We address the task of multi-view image editing from sparse input views, where the inputs can be seen as a mix of images capturing the scene from different viewpoints. The goal is to modify the scene according to a textual instruction while preserving consistency across all views. Existing methods, based on per-scene neural fields or temporal attention mechanisms, struggle in this setting, often producing artifacts and incoherent edits. We propose InstructMix2Mix (I-Mix2Mix), a framework that distills the editing capabilities of a 2D diffusion model into a pretrained multi-view diffusion model, leveraging its data-driven 3D prior for cross-view consistency. A key contribution is replacing the conventional neural field consolidator in Score Distillation Sampling (SDS) with a multi-view diffusion student, which requires novel adaptations: incremental student updates across timesteps, a specialized teacher noise scheduler to prevent degeneration, and an attention modification that enhances cross-view coherence without additional cost. Experiments demonstrate that I-Mix2Mix significantly improves multi-view consistency while maintaining high per-frame edit quality.</li>
<li><strong>摘要：</strong>我们解决了从稀疏输入视图进行多视图图像编辑的任务，其中输入可以被视为从不同视点捕获场景的图像的混合。目标是根据文本指令修改场景，同时保持所有视图的一致性。基于每个场景的神经场或时间注意力机制的现有方法在这种情况下很困难，经常产生伪影和不连贯的编辑。我们提出了 InstructMix2Mix (I-Mix2Mix)，该框架将 2D 扩散模型的编辑功能提炼为预训练的多视图扩散模型，利用其数据驱动的 3D 先验来实现跨视图一致性。一个关键贡献是用多视图扩散学生取代分数蒸馏采样（SDS）中的传统神经场合并器，这需要新颖的适应：跨时间步长的增量学生更新、专门的教师噪声调度程序以防止退化，以及在不增加成本的情况下增强跨视图一致性的注意力修改。实验表明，I-Mix2Mix 显着提高了多视图一致性，同时保持了较高的每帧编辑质量。</li>
</ul>

<h3>Title: How to Train Private Clinical Language Models: A Comparative Study of Privacy-Preserving Pipelines for ICD-9 Coding</h3>
<ul>
<li><strong>Authors: </strong>Mathieu Dufour, Andrew Duncan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14936">https://arxiv.org/abs/2511.14936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14936">https://arxiv.org/pdf/2511.14936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14936]] How to Train Private Clinical Language Models: A Comparative Study of Privacy-Preserving Pipelines for ICD-9 Coding(https://arxiv.org/abs/2511.14936)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models trained on clinical text risk exposing sensitive patient information, yet differential privacy (DP) methods often severely degrade the diagnostic accuracy needed for deployment. Despite rapid progress in DP optimisation and text generation, it remains unclear which privacy-preserving strategy actually works best for clinical language tasks. We present the first systematic head-to-head comparison of four training pipelines for automated diagnostic coding from hospital discharge summaries. All pipelines use identical 1B-parameter models and matched privacy budgets to predict ICD-9 codes. At moderate and relaxed privacy budgets ($\varepsilon \in \{4, 6\}$), knowledge distillation from DP-trained teachers outperforms both direct DP-SGD and DP-synthetic data training, recovering up to 63\% of the non-private performance whilst maintaining strong empirical privacy (membership-inference AUC $\approx$ 0.5). These findings expose large differences in the privacy-utility trade-off across architectures and identify knowledge distillation as the most practical route to privacy-preserving clinical NLP.</li>
<li><strong>摘要：</strong>在临床文本上训练的大型语言模型存在暴露敏感患者信息的风险，而差分隐私 (DP) 方法通常会严重降低部署所需的诊断准确性。尽管 DP 优化和文本生成方面取得了快速进展，但仍不清楚哪种隐私保护策略实际上最适合临床语言任务。我们首次对四个训练管道进行系统性的头对头比较，用于从医院出院摘要中进行自动诊断编码。所有管道都使用相同的 1B 参数模型和匹配的隐私预算来预测 ICD-9 代码。在适度和宽松的隐私预算（$\varepsilon \in \{4, 6\}$）下，经过 DP 培训的教师的知识蒸馏优于直接 DP-SGD 和 DP 合成数据训练，恢复了高达 63\% 的非隐私性能，同时保持了强大的经验隐私（成员推理 AUC $\approx$ 0.5）。这些发现揭示了跨架构的隐私与效用权衡的巨大差异，并将知识蒸馏确定为保护隐私的临床 NLP 的最实用途径。</li>
</ul>

<h3>Title: Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Vladimir Arkhipkin, Vladimir Korviakov, Nikolai Gerasimenko, Denis Parkhomenko, Viacheslav Vasilev, Alexey Letunovskiy, Maria Kovaleva, Nikolai Vaulin, Ivan Kirillov, Lev Novitskiy, Denis Koposov, Nikita Kiselev, Alexander Varlamov, Dmitrii Mikhailov, Vladimir Polovnikov, Andrey Shutkin, Ilya Vasiliev, Julia Agafonova, Anastasiia Kargapoltseva, Anna Dmitrienko, Anastasia Maltseva, Anna Averchenkova, Olga Kim, Tatiana Nikulina, Denis Dimitrov</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14993">https://arxiv.org/abs/2511.14993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14993">https://arxiv.org/pdf/2511.14993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14993]] Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation(https://arxiv.org/abs/2511.14993)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.</li>
<li><strong>摘要：</strong>本报告介绍了 Kandinsky 5.0，这是一系列用于高分辨率图像和 10 秒视频合成的最先进的基础模型。该框架包括三个核心模型系列：Kandinsky 5.0 Image Lite - 一系列 6B 参数图像生成模型、Kandinsky 5.0 Video Lite - 快速、轻量级的 2B 参数文本到视频和图像到视频模型，以及 Kandinsky 5.0 Video Pro - 实现卓越视频生成质量的 19B 参数模型。我们对多阶段训练管道的数据管理生命周期（包括收集、处理、过滤和聚类）进行了全面审查，其中涉及广泛的预训练，并结合了质量增强技术，例如自监督微调 (SFT) 和基于强化学习 (RL) 的后期训练。我们还提出了新颖的架构、训练和推理优化，使 Kandinsky 5.0 能够在各种任务中实现高生成速度和最先进的性能，正如人类评估所证明的那样。作为一个大规模、公开可用的生成框架，Kandinsky 5.0 充分利用其预训练和后续阶段的潜力，以适应广泛的生成应用。我们希望这份报告，连同我们开源代码和培训检查点的发布，将大大促进研究界高质量生成模型的开发和可及性。</li>
</ul>

<h3>Title: Complex-Valued 2D Gaussian Representation for Computer-Generated Holography</h3>
<ul>
<li><strong>Authors: </strong>Yicheng Zhan, Xiangjun Gao, Long Quan, Kaan Akşit</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15022">https://arxiv.org/abs/2511.15022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15022">https://arxiv.org/pdf/2511.15022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15022]] Complex-Valued 2D Gaussian Representation for Computer-Generated Holography(https://arxiv.org/abs/2511.15022)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We propose a new hologram representation based on structured complex-valued 2D Gaussian primitives, which replaces per-pixel information storage and reduces the parameter search space by up to 10:1. To enable end-to-end training, we develop a differentiable rasterizer for our representation, integrated with a GPU-optimized light propagation kernel in free space. Our extensive experiments show that our method achieves up to 2.5x lower VRAM usage and 50% faster optimization while producing higher-fidelity reconstructions than existing methods. We further introduce a conversion procedure that adapts our representation to practical hologram formats, including smooth and random phase-only holograms. Our experiments show that this procedure can effectively suppress noise artifacts observed in previous methods. By reducing the hologram parameter search space, our representation enables a more scalable hologram estimation in the next-generation computer-generated holography systems.</li>
<li><strong>摘要：</strong>我们提出了一种基于结构化复值 2D 高斯基元的新全息图表示，它取代了每像素信息存储，并将参数搜索空间减少了 10:1。为了实现端到端训练，我们为我们的表示开发了一个可微分光栅器，与自由空间中的 GPU 优化的光传播内核集成。我们的大量实验表明，与现有方法相比，我们的方法可将 VRAM 使用率降低 2.5 倍，优化速度提高 50%，同时产生更高保真度的重建。我们进一步介绍了一种转换过程，使我们的表示适应实际的全息图格式，包括平滑和随机的纯相位全息图。我们的实验表明，该过程可以有效抑制以前方法中观察到的噪声伪影。通过减少全息图参数搜索空间，我们的表示可以在下一代计算机生成的全息系统中实现更具可扩展性的全息图估计。</li>
</ul>

<h3>Title: UniHOI: Unified Human-Object Interaction Understanding via Unified Token Space</h3>
<ul>
<li><strong>Authors: </strong>Panqi Yang, Haodong Jing, Nanning Zheng, Yongqiang Ma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15046">https://arxiv.org/abs/2511.15046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15046">https://arxiv.org/pdf/2511.15046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15046]] UniHOI: Unified Human-Object Interaction Understanding via Unified Token Space(https://arxiv.org/abs/2511.15046)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In the field of human-object interaction (HOI), detection and generation are two dual tasks that have traditionally been addressed separately, hindering the development of comprehensive interaction understanding. To address this, we propose UniHOI, which jointly models HOI detection and generation via a unified token space, thereby effectively promoting knowledge sharing and enhancing generalization. Specifically, we introduce a symmetric interaction-aware attention module and a unified semi-supervised learning paradigm, enabling effective bidirectional mapping between images and interaction semantics even under limited annotations. Extensive experiments demonstrate that UniHOI achieves state-of-the-art performance in both HOI detection and generation. Specifically, UniHOI improves accuracy by 4.9% on long-tailed HOI detection and boosts interaction metrics by 42.0% on open-vocabulary generation tasks.</li>
<li><strong>摘要：</strong>在人机交互（HOI）领域，检测和生成是两个传统上分开处理的双重任务，阻碍了全面交互理解的发展。为了解决这个问题，我们提出了 UniHOI，它通过统一的 token 空间对 HOI 检测和生成进行联合建模，从而有效促进知识共享并增强泛化能力。具体来说，我们引入了对称交互感知注意模块和统一的半监督学习范式，即使在有限的注释下也能实现图像和交互语义之间的有效双向映射。大量实验表明 UniHOI 在 HOI 检测和生成方面均实现了最先进的性能。具体来说，UniHOI 将长尾 HOI 检测的准确性提高了 4.9%，并将开放词汇生成任务的交互指标提高了 42.0%。</li>
</ul>

<h3>Title: Hyperspectral Super-Resolution with Inter-Image Variability via Degradation-based Low-Rank and Residual Fusion Method</h3>
<ul>
<li><strong>Authors: </strong>Yue Wen, Kunjing Yang, Minru Bai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15052">https://arxiv.org/abs/2511.15052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15052">https://arxiv.org/pdf/2511.15052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15052]] Hyperspectral Super-Resolution with Inter-Image Variability via Degradation-based Low-Rank and Residual Fusion Method(https://arxiv.org/abs/2511.15052)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>The fusion of hyperspectral image (HSI) with multispectral image (MSI) provides an effective way to enhance the spatial resolution of HSI. However, due to different acquisition conditions, there may exist spectral variability and spatially localized changes between HSI and MSI, referred to as inter-image variability, which can significantly affect the fusion performance. Existing methods typically handle inter-image variability by applying direct transformations to the images themselves, which can exacerbate the ill-posedness of the fusion model. To address this challenge, we propose a Degradation-based Low-Rank and Residual Fusion (DLRRF) model. First, we model the spectral variability as change in the spectral degradation operator. Second, to recover the lost spatial details caused by spatially localized changes, we decompose the target HSI into low rank and residual components, where the latter is used to capture the lost details. By exploiting the spectral correlation within the images, we perform dimensionality reduction on both components. Additionally, we introduce an implicit regularizer to utilize the spatial prior information from the images. The proposed DLRRF model is solved using the Proximal Alternating Optimization (PAO) algorithm within a Plug-and-Play (PnP) framework, where the subproblem regarding implicit regularizer is addressed by an external denoiser. We further provide a comprehensive convergence analysis of the algorithm. Finally, extensive numerical experiments demonstrate that DLRRF achieves superior performance in fusing HSI and MSI with inter-image variability.</li>
<li><strong>摘要：</strong>高光谱图像（HSI）与多光谱图像（MSI）的融合提供了增强HSI空间分辨率的有效方法。然而，由于采集条件不同，HSI和MSI之间可能存在光谱变异性和空间局部变化，称为图像间变异性，这会显着影响融合性能。现有方法通常通过对图像本身应用直接变换来处理图像间的变异性，这可能会加剧融合模型的不适定性。为了应对这一挑战，我们提出了一种基于退化的低阶残差融合（DLRRF）模型。首先，我们将光谱变异性建模为光谱退化算子的变化。其次，为了恢复由空间局部变化引起的丢失的空间细节，我们将目标 HSI 分解为低秩和残差分量，其中后者用于捕获丢失的细节。通过利用图像内的光谱相关性，我们对两个分量进行降维。此外，我们引入了隐式正则化器来利用图像中的空间先验信息。所提出的 DLRRF 模型使用即插即用 (PnP) 框架内的近端交替优化 (PAO) 算法来求解，其中有关隐式正则器的子问题由外部降噪器解决。我们进一步提供了算法的全面收敛分析。最后，大量的数值实验表明，DLRRF 在将 HSI 和 MSI 与图像间可变性融合方面取得了卓越的性能。</li>
</ul>

<h3>Title: Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks</h3>
<ul>
<li><strong>Authors: </strong>Cheng Yang, Haiyuan Wan, Yiran Peng, Xin Cheng, Zhaoyang Yu, Jiayi Zhang, Junchi Yu, Xinlei Yu, Xiawu Zheng, Dongzhan Zhou, Chenglin Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15065">https://arxiv.org/abs/2511.15065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15065">https://arxiv.org/pdf/2511.15065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15065]] Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks(https://arxiv.org/abs/2511.15065)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Video Models have achieved remarkable success in high-fidelity video generation with coherent motion dynamics. Analogous to the development from text generation to text-based reasoning in language modeling, the development of video models motivates us to ask: Can video models reason via video generation? Compared with the discrete text corpus, video grounds reasoning in explicit spatial layouts and temporal continuity, which serves as an ideal substrate for spatial reasoning. In this work, we explore the reasoning via video paradigm and introduce VR-Bench -- a comprehensive benchmark designed to systematically evaluate video models' reasoning capabilities. Grounded in maze-solving tasks that inherently require spatial planning and multi-step reasoning, VR-Bench contains 7,920 procedurally generated videos across five maze types and diverse visual styles. Our empirical analysis demonstrates that SFT can efficiently elicit the reasoning ability of video model. Video models exhibit stronger spatial perception during reasoning, outperforming leading VLMs and generalizing well across diverse scenarios, tasks, and levels of complexity. We further discover a test-time scaling effect, where diverse sampling during inference improves reasoning reliability by 10--20%. These findings highlight the unique potential and scalability of reasoning via video for spatial reasoning tasks.</li>
<li><strong>摘要：</strong>视频模型在具有连贯运动动力学的高保真视频生成方面取得了显着的成功。类似于语言建模中从文本生成到基于文本推理的发展，视频模型的发展促使我们问：视频模型可以通过视频生成进行推理吗？与离散文本语料库相比，视频推理具有明确的空间布局和时间连续性，是空间推理的理想基础。在这项工作中，我们通过视频范式探索推理，并介绍 VR-Bench——一个旨在系统评估视频模型推理能力的综合基准。 VR-Bench 基于本质上需要空间规划和多步骤推理的迷宫解决任务，包含 7,920 个程序生成的视频，涵盖五种迷宫类型和不同的视觉风格。我们的实证分析表明，SFT 可以有效地引出视频模型的推理能力。视频模型在推理过程中表现出更强的空间感知，优于领先的 VLM，并且在不同的场景、任务和复杂程度之间具有良好的泛化能力。我们进一步发现了测试时间缩放效应，推理过程中的多样化采样将推理可靠性提高了 10--20%。这些发现凸显了通过视频进行空间推理任务推理的独特潜力和可扩展性。</li>
</ul>

<h3>Title: BokehFlow: Depth-Free Controllable Bokeh Rendering via Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Yachuan Huang, Xianrui Luo, Qiwen Wang, Liao Shen, Jiaqi Li, Huiqiang Sun, Zihao Huang, Wei Jiang, Zhiguo Cao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15066">https://arxiv.org/abs/2511.15066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15066">https://arxiv.org/pdf/2511.15066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15066]] BokehFlow: Depth-Free Controllable Bokeh Rendering via Flow Matching(https://arxiv.org/abs/2511.15066)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Bokeh rendering simulates the shallow depth-of-field effect in photography, enhancing visual aesthetics and guiding viewer attention to regions of interest. Although recent approaches perform well, rendering controllable bokeh without additional depth inputs remains a significant challenge. Existing classical and neural controllable methods rely on accurate depth maps, while generative approaches often struggle with limited controllability and efficiency. In this paper, we propose BokehFlow, a depth-free framework for controllable bokeh rendering based on flow matching. BokehFlow directly synthesizes photorealistic bokeh effects from all-in-focus images, eliminating the need for depth inputs. It employs a cross-attention mechanism to enable semantic control over both focus regions and blur intensity via text prompts. To support training and evaluation, we collect and synthesize four datasets. Extensive experiments demonstrate that BokehFlow achieves visually compelling bokeh effects and offers precise control, outperforming existing depth-dependent and generative methods in both rendering quality and efficiency.</li>
<li><strong>摘要：</strong>散景渲染模拟摄影中的浅景深效果，增强视觉美感并引导观看者将注意力集中到感兴趣的区域。尽管最近的方法表现良好，但在没有额外深度输入的情况下渲染可控散景仍然是一个重大挑战。现有的经典和神经可控方法依赖于精确的深度图，而生成方法往往难以控制和效率有限。在本文中，我们提出了 BokehFlow，一种基于流匹配的可控散景渲染的无深度框架。 BokehFlow 直接从全焦图像合成逼真的散景效果，无需深度输​​入。它采用交叉注意机制，通过文本提示实现对焦点区域和模糊强度的语义控制。为了支持培训和评估，我们收集并综合了四个数据集。大量实验表明，BokehFlow 实现了视觉上引人注目的散景效果，并提供精确的控制，在渲染质量和效率方面均优于现有的依赖于深度的生成方法。</li>
</ul>

<h3>Title: Jointly Conditioned Diffusion Model for Multi-View Pose-Guided Person Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Chengyu Xie, Zhi Gong, Junchi Ren, Linkun Yu, Si Shen, Fei Shen, Xiaoyu Du</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15092">https://arxiv.org/abs/2511.15092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15092">https://arxiv.org/pdf/2511.15092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15092]] Jointly Conditioned Diffusion Model for Multi-View Pose-Guided Person Image Synthesis(https://arxiv.org/abs/2511.15092)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Pose-guided human image generation is limited by incomplete textures from single reference views and the absence of explicit cross-view interaction. We present jointly conditioned diffusion model (JCDM), a jointly conditioned diffusion framework that exploits multi-view priors. The appearance prior module (APM) infers a holistic identity preserving prior from incomplete references, and the joint conditional injection (JCI) mechanism fuses multi-view cues and injects shared conditioning into the denoising backbone to align identity, color, and texture across poses. JCDM supports a variable number of reference views and integrates with standard diffusion backbones with minimal and targeted architectural modifications. Experiments demonstrate state of the art fidelity and cross-view consistency.</li>
<li><strong>摘要：</strong>姿势引导的人类图像生成受到来自单个参考视图的不完整纹理以及缺乏明确的跨视图交互的限制。我们提出了联合条件扩散模型（JCDM），这是一种利用多视图先验的联合条件扩散框架。外观先验模块（APM）从不完整的参考中推断出整体身份保留先验，联合条件注入（JCI）机制融合多视图线索并将共享条件注入到去噪主干中，以跨姿势对齐身份、颜色和纹理。 JCDM 支持可变数量的参考视图，并通过最少且有针对性的架构修改与标准扩散骨干集成。实验证明了最先进的保真度和跨视图一致性。</li>
</ul>

<h3>Title: From Solving to Verifying: A Unified Objective for Robust Reasoning in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xiaoxuan Wang, Bo Liu, Song Jiang, Jingzhou Liu, Jingyuan Qi, Xia Chen, Baosheng He</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15137">https://arxiv.org/abs/2511.15137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15137">https://arxiv.org/pdf/2511.15137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15137]] From Solving to Verifying: A Unified Objective for Robust Reasoning in LLMs(https://arxiv.org/abs/2511.15137)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The reasoning capabilities of large language models (LLMs) have been significantly improved through reinforcement learning (RL). Nevertheless, LLMs still struggle to consistently verify their own reasoning traces. This raises the research question of how to enhance the self-verification ability of LLMs and whether such an ability can further improve reasoning performance. In this work, we propose GRPO-Verif, an algorithm that jointly optimizes solution generation and self-verification within a unified loss function, with an adjustable hyperparameter controlling the weight of the verification signal. Experimental results demonstrate that our method enhances self-verification capability while maintaining comparable performance in reasoning.</li>
<li><strong>摘要：</strong>通过强化学习（RL），大型语言模型（LLM）的推理能力得到了显着提高。尽管如此，法学硕士仍然难以一致地验证自己的推理轨迹。这就提出了如何增强法学硕士的自我验证能力以及这种能力是否能够进一步提高推理能力的研究问题。在这项工作中，我们提出了 GRPO-Verif，一种在统一损失函数内联合优化解决方案生成和自我验证的算法，并通过可调节的超参数控制验证信号的权重。实验结果表明，我们的方法增强了自我验证能力，同时保持了可比较的推理性能。</li>
</ul>

<h3>Title: Generating Natural-Language Surgical Feedback: From Structured Representation to Domain-Grounded Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Firdavs Nasriddinov, Rafal Kocielnik, Anima Anandkumar, Andrew J. Hung</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15159">https://arxiv.org/abs/2511.15159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15159">https://arxiv.org/pdf/2511.15159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15159]] Generating Natural-Language Surgical Feedback: From Structured Representation to Domain-Grounded Evaluation(https://arxiv.org/abs/2511.15159)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>High-quality intraoperative feedback from a surgical trainer is pivotal for improving trainee performance and long-term skill acquisition. Automating natural, trainer-style feedback promises timely, accessible, and consistent guidance at scale but requires models that understand clinically relevant representations. We present a structure-aware pipeline that learns a surgical action ontology from real trainer-to-trainee transcripts (33 surgeries) and uses it to condition feedback generation. We contribute by (1) mining Instrument-Action-Target (IAT) triplets from real-world feedback text and clustering surface forms into normalized categories, (2) fine-tuning a video-to-IAT model that leverages the surgical procedure and task contexts as well as fine-grained temporal instrument motion, and (3) demonstrating how to effectively use IAT triplet representations to guide GPT-4o in generating clinically grounded, trainer-style feedback. We show that, on Task 1: Video-to-IAT recognition, our context injection and temporal tracking deliver consistent AUC gains (Instrument: 0.67 to 0.74; Action: 0.60 to 0.63; Tissue: 0.74 to 0.79). For Task 2: feedback text generation (rated on a 1-5 fidelity rubric where 1 = opposite/unsafe, 3 = admissible, and 5 = perfect match to a human trainer), GPT-4o from video alone scores 2.17, while IAT conditioning reaches 2.44 (+12.4%), doubling the share of admissible generations with score >= 3 from 21% to 42%. Traditional text-similarity metrics also improve: word error rate decreases by 15-31% and ROUGE (phrase/substring overlap) increases by 9-64%. Grounding generation in explicit IAT structure improves fidelity and yields clinician-verifiable rationales, supporting auditable use in surgical training.</li>
<li><strong>摘要：</strong>外科培训师提供的高质量术中反馈对于提高学员表现和长期技能获取至关重要。自动化、培训师式的自然反馈有望提供及时、可访问且一致的大规模指导，但需要能够理解临床相关表征的模型。我们提出了一个结构感知管道，可以从真实的培训师到受训者的记录（33 次手术）中学习手术动作本体，并用它来调节反馈的生成。我们的贡献包括（1）从现实世界的反馈文本中挖掘仪器-动作-目标（IAT）三元组并将表面形式聚类为标准化类别，（2）微调视频到IAT模型，利用手术过程和任务上下文以及细粒度的时间仪器运动，以及（3）演示如何有效地使用IAT三元组表示来指导GPT-4o生成临床基础的培训师风格的反馈。我们表明，在任务 1：视频到 IAT 识别中，我们的上下文注入和时间跟踪提供了一致的 AUC 增益（仪器：0.67 至 0.74；动作：0.60 至 0.63；组织：0.74 至 0.79）。对于任务 2：反馈文本生成（按照 1-5 保真度评分标准进行评级，其中 1 = 相反/不安全，3 = 可接受，5 = 与人类训练师完美匹配），仅视频的 GPT-4o 得分为 2.17，而 IAT 调节达到 2.44 (+12.4%)，使得分 >= 3 的可接受代的比例翻倍，从 21% 增加到 42%。传统的文本相似性指标也得到了改善：单词错误率降低了 15-31%，ROUGE（短语/子串重叠）增加了 9-64%。显式 IAT 结构中的基础生成可提高保真度并产生临床医生可验证的基本原理，支持手术训练中的可审核使用。</li>
</ul>

<h3>Title: FaultDiffusion: Few-Shot Fault Time Series Generation with Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Yi Xu, Zhigang Chen, Rui Wang, Yangfan Li, Fengxiao Tang, Ming Zhao, Jiaqi Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15174">https://arxiv.org/abs/2511.15174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15174">https://arxiv.org/pdf/2511.15174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15174]] FaultDiffusion: Few-Shot Fault Time Series Generation with Diffusion Model(https://arxiv.org/abs/2511.15174)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In industrial equipment monitoring, fault diagnosis is critical for ensuring system reliability and enabling predictive maintenance. However, the scarcity of fault data, due to the rarity of fault events and the high cost of data annotation, significantly hinders data-driven approaches. Existing time-series generation models, optimized for abundant normal data, struggle to capture fault distributions in few-shot scenarios, producing samples that lack authenticity and diversity due to the large domain gap and high intra-class variability of faults. To address this, we propose a novel few-shot fault time-series generation framework based on diffusion models. Our approach employs a positive-negative difference adapter, leveraging pre-trained normal data distributions to model the discrepancies between normal and fault domains for accurate fault synthesis. Additionally, a diversity loss is introduced to prevent mode collapse, encouraging the generation of diverse fault samples through inter-sample difference regularization. Experimental results demonstrate that our model significantly outperforms traditional methods in authenticity and diversity, achieving state-of-the-art performance on key benchmarks.</li>
<li><strong>摘要：</strong>在工业设备监控中，故障诊断对于确保系统可靠性和实现预测性维护至关重要。然而，由于故障事件的稀有性和数据注释的高成本，故障数据的稀缺性极大地阻碍了数据驱动的方法。现有的时间序列生成模型针对丰富的正态数据进行了优化，难以捕获少量场景中的故障分布，由于域间隙较大和故障的类内变异性较高，生成的样本缺乏真实性和多样性。为了解决这个问题，我们提出了一种基于扩散模型的新颖的少样本故障时间序列生成框架。我们的方法采用正负差异适配器，利用预先训练的正态数据分布对正常域和故障域之间的差异进行建模，以实现准确的故障合成。此外，引入多样性损失来防止模式崩溃，通过样本间差异正则化鼓励生成多样化的故障样本。实验结果表明，我们的模型在真实性和多样性方面显着优于传统方法，在关键基准上实现了最先进的性能。</li>
</ul>

<h3>Title: BrainRotViT: Transformer-ResNet Hybrid for Explainable Modeling of Brain Aging from 3D sMRI</h3>
<ul>
<li><strong>Authors: </strong>Wasif Jalal, Md Nafiu Rahman, M.Sohel Rahman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15188">https://arxiv.org/abs/2511.15188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15188">https://arxiv.org/pdf/2511.15188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15188]] BrainRotViT: Transformer-ResNet Hybrid for Explainable Modeling of Brain Aging from 3D sMRI(https://arxiv.org/abs/2511.15188)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Accurate brain age estimation from structural MRI is a valuable biomarker for studying aging and neurodegeneration. Traditional regression and CNN-based methods face limitations such as manual feature engineering, limited receptive fields, and overfitting on heterogeneous data. Pure transformer models, while effective, require large datasets and high computational cost. We propose Brain ResNet over trained Vision Transformer (BrainRotViT), a hybrid architecture that combines the global context modeling of vision transformers (ViT) with the local refinement of residual CNNs. A ViT encoder is first trained on an auxiliary age and sex classification task to learn slice-level features. The frozen encoder is then applied to all sagittal slices to generate a 2D matrix of embedding vectors, which is fed into a residual CNN regressor that incorporates subject sex at the final fully-connected layer to estimate continuous brain age. Our method achieves an MAE of 3.34 years (Pearson $r=0.98$, Spearman $\rho=0.97$, $R^2=0.95$) on validation across 11 MRI datasets encompassing more than 130 acquisition sites, outperforming baseline and state-of-the-art models. It also generalizes well across 4 independent cohorts with MAEs between 3.77 and 5.04 years. Analyses on the brain age gap (the difference between the predicted age and actual age) show that aging patterns are associated with Alzheimer's disease, cognitive impairment, and autism spectrum disorder. Model attention maps highlight aging-associated regions of the brain, notably the cerebellar vermis, precentral and postcentral gyri, temporal lobes, and medial superior frontal gyrus. Our results demonstrate that this method provides an efficient, interpretable, and generalizable framework for brain-age prediction, bridging the gap between CNN- and transformer-based approaches while opening new avenues for aging and neurodegeneration research.</li>
<li><strong>摘要：</strong>通过结构 MRI 准确估计大脑年龄是研究衰老和神经退行性疾病的重要生物标志物。传统的回归和基于 CNN 的方法面临手动特征工程、有限的感受野以及对异构数据的过度拟合等限制。纯变压器模型虽然有效，但需要大量数据集和较高的计算成本。我们提出基于经过训练的 Vision Transformer (BrainRotViT) 的 Brain ResNet，这是一种混合架构，它将视觉 Transformer (ViT) 的全局上下文建模与残差 CNN 的局部细化相结合。 ViT 编码器首先接受辅助年龄和性别分类任务的训练，以学习切片级特征。然后，将冻结的编码器应用于所有矢状切片，以生成嵌入向量的 2D 矩阵，该矩阵被输入到残差 CNN 回归器中，该回归器在最终的全连接层中结合了受试者性别，以估计连续的大脑年龄。我们的方法在涵盖 130 多个采集站点的 11 个 MRI 数据集上进行验证，实现了 3.34 年的 MAE（Pearson $r=0.98$、Spearman $\rho=0.97$、$R^2=0.95$），优于基线模型和最先进的模型。它还可以很好地概括 4 个 MAE 介于 3.77 至 5.04 岁之间的独立队列。对大脑年龄差距（预测年龄与实际年龄之间的差异）的分析表明，衰老模式与阿尔茨海默病、认知障碍和自闭症谱系障碍有关。模型注意力图突出显示了大脑中与衰老相关的区域，特别是小脑蚓部、中央前回和中央后回、颞叶和内侧额上回。我们的结果表明，该方法为大脑年龄预测提供了一个高效、可解释和可推广的框架，弥合了基于 CNN 和基于 Transformer 的方法之间的差距，同时为衰老和神经退行性疾病研究开辟了新途径。</li>
</ul>

<h3>Title: Masked Auto-Regressive Variational Acceleration: Fast Inference Makes Practical Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Gu, Weimin Bai, Yifei Wang, Weijian Luo, He Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15190">https://arxiv.org/abs/2511.15190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15190">https://arxiv.org/pdf/2511.15190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15190]] Masked Auto-Regressive Variational Acceleration: Fast Inference Makes Practical Reinforcement Learning(https://arxiv.org/abs/2511.15190)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Masked auto-regressive diffusion models (MAR) benefit from the expressive modeling ability of diffusion models and the flexibility of masked auto-regressive ordering. However, vanilla MAR suffers from slow inference due to its hierarchical inference mechanism: an outer AR unmasking loop and an inner diffusion denoising chain. Such decoupled structure not only harm the generation efficiency but also hinder the practical use of MAR for reinforcement learning (RL), an increasingly critical paradigm for generative model this http URL address this fundamental issue, we introduce MARVAL (Masked Auto-regressive Variational Acceleration), a distillation-based framework that compresses the diffusion chain into a single AR generation step while preserving the flexible auto-regressive unmasking order. Such a distillation with MARVAL not only yields substantial inference acceleration but, crucially, makes RL post-training with verifiable rewards practical, resulting in scalable yet human-preferred fast generative models. Our contributions are twofold: (1) a novel score-based variational objective for distilling masked auto-regressive diffusion models into a single generation step without sacrificing sample quality; and (2) an efficient RL framework for masked auto-regressive models via MARVAL-RL. On ImageNet 256*256, MARVAL-Huge achieves an FID of 2.00 with more than 30 times speedup compared with MAR-diffusion, and MARVAL-RL yields consistent improvements in CLIP and image-reward scores on ImageNet datasets with entity names. In conclusion, MARVAL demonstrates the first practical path to distillation and RL of masked auto-regressive diffusion models, enabling fast sampling and better preference alignments.</li>
<li><strong>摘要：</strong>屏蔽自回归扩散模型（MAR）受益于扩散模型的表达建模能力和屏蔽自回归排序的灵活性。然而，vanilla MAR 由于其分层推理机制（外部 AR 揭露循环和内部扩散去噪链）而导致推理速度缓慢。这种解耦结构不仅损害了生成效率，而且阻碍了 MAR 在强化学习（RL）中的实际使用，这是生成模型日益重要的范式，这个 http URL 解决了这个基本问题，我们引入了 MARVAL（掩码自回归变分加速），一种基于蒸馏的框架，它将扩散链压缩为单个 AR 生成步骤，同时保留灵活的自回归揭露顺序。 MARVAL 的这种提炼不仅能产生显着的推理加速，而且最重要的是，使具有可验证奖励的 RL 后训练变得实用，从而产生可扩展且人类首选的快速生成模型。我们的贡献是双重的：（1）一种新颖的基于分数的变分目标，用于将掩码自回归扩散模型提炼为单个生成步骤，而不牺牲样本质量； (2) 通过 MARVAL-RL 构建用于屏蔽自回归模型的高效 RL 框架。在 ImageNet 256*256 上，MARVAL-Huge 实现了 2.00 的 FID，与 MAR-diffusion 相比，加速超过 30 倍，并且 MARVAL-RL 在具有实体名称的 ImageNet 数据集上的 CLIP 和图像奖励分数上产生了一致的改进。总之，MARVAL 展示了掩蔽自回归扩散模型的蒸馏和 RL 的第一个实用路径，从而实现快速采样和更好的偏好对齐。</li>
</ul>

<h3>Title: Insert In Style: A Zero-Shot Generative Framework for Harmonious Cross-Domain Object Composition</h3>
<ul>
<li><strong>Authors: </strong>Raghu Vamsi Chittersu, Yuvraj Singh Rathore, Pranav Adlinge, Kunal Swami</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15197">https://arxiv.org/abs/2511.15197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15197">https://arxiv.org/pdf/2511.15197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15197]] Insert In Style: A Zero-Shot Generative Framework for Harmonious Cross-Domain Object Composition(https://arxiv.org/abs/2511.15197)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Reference-based object composition methods fail when inserting real-world objects into stylized domains. This under-explored problem is currently split between practical "blenders" that lack generative fidelity and "generators" that require impractical, per-subject online finetuning. In this work, we introduce Insert In Style, the first zero-shot generative framework that is both practical and high-fidelity. Our core contribution is a unified framework with two key innovations: (i) a novel multi-stage training protocol that disentangles representations for identity, style, and composition, and (ii) a specialized masked-attention architecture that surgically enforces this disentanglement during generation. This approach prevents the concept interference common in general-purpose, unified-attention models. Our framework is trained on a new 100k sample dataset, curated from a novel data pipeline. This pipeline couples large-scale generation with a rigorous, two-stage filtering process to ensure both high-fidelity semantic identity and style coherence. Unlike prior work, our model is truly zero-shot and requires no text prompts. We also introduce a new public benchmark for stylized composition. We demonstrate state-of-the-art performance, significantly outperforming existing methods on both identity and style metrics, a result strongly corroborated by user studies.</li>
<li><strong>摘要：</strong>将现实世界的对象插入到风格化域中时，基于引用的对象组合方法会失败。这个尚未充分探索的问题目前分为缺乏生成保真度的实用“混合器”和需要不切实际的、每个主题在线微调的“生成器”。在这项工作中，我们介绍了 Insert In Style，这是第一个既实用又高保真的零样本生成框架。我们的核心贡献是一个具有两项关键创新的统一框架：（i）一种新颖的多阶段训练协议，可以解开身份、风格和组成的表示，以及（ii）一种专门的屏蔽注意力架构，可以在生成过程中通过外科手术强制执行这种解开。这种方法可以防止通用、统一注意力模型中常见的概念干扰。我们的框架是在一个新的 100k 样本数据集上进行训练的，这些数据集是从一个新颖的数据管道中策划的。该管道将​​大规模生成与严格的两阶段过滤过程结合起来，以确保高保真语义同一性和风格连贯性。与之前的工作不同，我们的模型是真正的零样本，不需要文本提示。我们还引入了一个新的风格化构图公共基准。我们展示了最先进的性能，在身份和风格指标方面显着优于现有方法，这一结果得到了用户研究的有力证实。</li>
</ul>

<h3>Title: SplitFlux: Learning to Decouple Content and Style from a Single Image</h3>
<ul>
<li><strong>Authors: </strong>Yitong Yang, Yinglin Wang, Changshuo Wang, Yongjun Zhang, Ziyang Chen, Shuting He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15258">https://arxiv.org/abs/2511.15258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15258">https://arxiv.org/pdf/2511.15258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15258]] SplitFlux: Learning to Decouple Content and Style from a Single Image(https://arxiv.org/abs/2511.15258)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Disentangling image content and style is essential for customized image generation. Existing SDXL-based methods struggle to achieve high-quality results, while the recently proposed Flux model fails to achieve effective content-style separation due to its underexplored characteristics. To address these challenges, we conduct a systematic analysis of Flux and make two key observations: (1) Single Dream Blocks are essential for image generation; and (2) Early single stream blocks mainly control content, whereas later blocks govern style. Based on these insights, we propose SplitFlux, which disentangles content and style by fine-tuning the single dream blocks via LoRA, enabling the disentangled content to be re-embedded into new contexts. It includes two key components: (1) Rank-Constrained Adaptation. To preserve content identity and structure, we compress the rank and amplify the magnitude of updates within specific blocks, preventing content leakage into style blocks. (2) Visual-Gated LoRA. We split the content LoRA into two branches with different ranks, guided by image saliency. The high-rank branch preserves primary subject information, while the low-rank branch encodes residual details, mitigating content overfitting and enabling seamless re-embedding. Extensive experiments demonstrate that SplitFlux consistently outperforms state-of-the-art methods, achieving superior content preservation and stylization quality across diverse scenarios.</li>
<li><strong>摘要：</strong>解开图像内容和风格对于定制图像生成至关重要。现有的基于 SDXL 的方法很难获得高质量的结果，而最近提出的 Flux 模型由于其尚未充分开发的特性而未能实现有效的内容风格分离。为了应对这些挑战，我们对 Flux 进行了系统分析，并得出两个关键观察结果：（1）单梦块对于图像生成至关重要； (2)早期的单流块主要控制内容，而后期的块控制风格。基于这些见解，我们提出了 SplitFlux，它通过 LoRA 微调单个梦想块来解开内容和风格，使解开的内容能够重新嵌入到新的上下文中。它包括两个关键组成部分：（1）等级约束适应。为了保持内容的身份和结构，我们压缩排名并放大特定块内的更新幅度，防止内容泄漏到样式块中。 (2) 视觉门控 LoRA。我们根据图像显着性将内容 LoRA 分为两个具有不同等级的分支。高等级分支保留主要主题信息，而低等级分支对残留细节进行编码，从而减轻内容过度拟合并实现无缝重新嵌入。大量实验表明，SplitFlux 始终优于最先进的方法，在不同场景下实现卓越的内容保存和风格化质量。</li>
</ul>

<h3>Title: Taming Generative Synthetic Data for X-ray Prohibited Item Detection</h3>
<ul>
<li><strong>Authors: </strong>Jialong Sun, Hongguang Zhu, Weizhe Liu, Yunda Sun, Renshuai Tao, Yunchao Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15299">https://arxiv.org/abs/2511.15299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15299">https://arxiv.org/pdf/2511.15299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15299]] Taming Generative Synthetic Data for X-ray Prohibited Item Detection(https://arxiv.org/abs/2511.15299)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Training prohibited item detection models requires a large amount of X-ray security images, but collecting and annotating these images is time-consuming and laborious. To address data insufficiency, X-ray security image synthesis methods composite images to scale up datasets. However, previous methods primarily follow a two-stage pipeline, where they implement labor-intensive foreground extraction in the first stage and then composite images in the second stage. Such a pipeline introduces inevitable extra labor cost and is not efficient. In this paper, we propose a one-stage X-ray security image synthesis pipeline (Xsyn) based on text-to-image generation, which incorporates two effective strategies to improve the usability of synthetic images. The Cross-Attention Refinement (CAR) strategy leverages the cross-attention map from the diffusion model to refine the bounding box annotation. The Background Occlusion Modeling (BOM) strategy explicitly models background occlusion in the latent space to enhance imaging complexity. To the best of our knowledge, compared with previous methods, Xsyn is the first to achieve high-quality X-ray security image synthesis without extra labor cost. Experiments demonstrate that our method outperforms all previous methods with 1.2% mAP improvement, and the synthetic images generated by our method are beneficial to improve prohibited item detection performance across various X-ray security datasets and detectors. Code is available at this https URL.</li>
<li><strong>摘要：</strong>训练违禁物品检测模型需要大量的X射线安全图像，但收集和注释这些图像既费时又费力。为了解决数据不足的问题，X 射线安全图像合成方法合成图像​​以扩大数据集。然而，以前的方法主要遵循两阶段流程，在第一阶段实现劳动密集型前景提取，然后在第二阶段合成图像。这样的管道不可避免地会带来额外的劳动力成本，而且效率不高。在本文中，我们提出了一种基于文本到图像生成的一级X射线安全图像合成管道（Xsyn），它结合了两种有效的策略来提高合成图像的可用性。交叉注意力细化（CAR）策略利用扩散模型中的交叉注意力图来细化边界框注释。背景遮挡建模 (BOM) 策略明确地模拟潜在空间中的背景遮挡，以提高成像复杂性。据我们所知，与之前的方法相比，Xsyn 是第一个在无需额外人工成本的情况下实现高质量 X 射线安全图像合成的方法。实验表明，我们的方法优于以前的所有方法，mAP 提高了 1.2%，并且我们的方法生成的合成图像有利于提高各种 X 射线安全数据集和探测器的违禁物品检测性能。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Breaking Expert Knowledge Limits: Self-Pruning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haidong Kang, Lihong Lin, Enneng Yang, Hongning Dai, Hao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15390">https://arxiv.org/abs/2511.15390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15390">https://arxiv.org/pdf/2511.15390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15390]] Breaking Expert Knowledge Limits: Self-Pruning for Large Language Models(https://arxiv.org/abs/2511.15390)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved remarkable performance on a wide range of tasks, hindering real-world deployment due to their massive size. Existing pruning methods (e.g., Wanda) tailored for LLMs rely heavily on manual design pruning algorithms, thereby leading to \textit{huge labor costs} and \textit{requires expert knowledge}. Furthermore, we are the first to identify the serious \textit{outlier value issue} behind dramatic performance degradation under high pruning ratios that are caused by uniform sparsity, raising an additional concern about how to design adaptive pruning sparsity ideal for LLMs. Can LLMs prune by themselves? In this work, we introduce an affirmative answer by proposing a novel pruning method called \textbf{AutoPrune}, which first overcomes expert knowledge limits by leveraging LLMs to design optimal pruning algorithms for themselves automatically without any expert knowledge. Specifically, to mitigate the black-box nature of LLMs, we propose a Graph-driven Chain-of-Thought (GCoT) to optimize prompts, significantly enhancing the reasoning process in learning the pruning algorithm and enabling us to generate pruning algorithms with superior performance and interpretability in the next generation. Finally, grounded in insights of outlier value issue, we introduce Skew-aware Dynamic Sparsity Allocation (SDSA) to overcome the outlier value issue, mitigating performance degradation under high pruning ratios. We conduct extensive experiments on mainstream LLMs benchmarks, demonstrating the superiority of AutoPrune, which consistently excels state-of-the-art competitors. The code is available at: this https URL.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在广泛的任务中取得了卓越的性能，但由于其庞大的规模阻碍了现实世界的部署。现有针对LLM量身定制的剪枝方法（例如Wanda）严重依赖于手动设计剪枝算法，从而导致\textit{巨大的劳动力成本}和\textit{需要专业知识}。此外，我们是第一个发现在高剪枝率下由均匀稀疏性引起的性能急剧下降背后的严重 \textit{异常值问题}，这引起了人们对如何设计适合 LLM 的自适应剪枝稀疏性的额外担忧。 LLM可以自行修剪吗？在这项工作中，我们通过提出一种名为 \textbf{AutoPrune} 的新颖剪枝方法来引入肯定的答案，该方法首先通过利用 LLM 在没有任何专业知识的情况下自动为自己设计最佳剪枝算法来克服专家知识限制。具体来说，为了减轻法学硕士的黑盒性质，我们提出了一种图驱动的思想链（GCoT）来优化提示，显着增强学习剪枝算法的推理过程，并使我们能够在下一代生成具有卓越性能和可解释性的剪枝算法。最后，基于对离群值问题的洞察，我们引入了倾斜感知动态稀疏分配（SDSA）来克服离群值问题，减轻高剪枝率下的性能下降。我们对主流法学硕士基准进行了广泛的实验，证明了 AutoPrune 的优越性，它始终优于最先进的竞争对手。该代码位于：此 https URL。</li>
</ul>

<h3>Title: D4C: Data-free Quantization for Contrastive Language-Image Pre-training Models</h3>
<ul>
<li><strong>Authors: </strong>Wenlun Zhang, Yunshan Zhong, Zihao Ding, Xinyu Li, Kentaro Yoshioka</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15411">https://arxiv.org/abs/2511.15411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15411">https://arxiv.org/pdf/2511.15411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15411]] D4C: Data-free Quantization for Contrastive Language-Image Pre-training Models(https://arxiv.org/abs/2511.15411)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Data-Free Quantization (DFQ) offers a practical solution for model compression without requiring access to real data, making it particularly attractive in privacy-sensitive scenarios. While DFQ has shown promise for unimodal models, its extension to Vision-Language Models such as Contrastive Language-Image Pre-training (CLIP) models remains underexplored. In this work, we reveal that directly applying existing DFQ techniques to CLIP results in substantial performance degradation due to two key limitations: insufficient semantic content and low intra-image diversity in synthesized samples. To tackle these challenges, we propose D4C, the first DFQ framework tailored for CLIP. D4C synthesizes semantically rich and structurally diverse pseudo images through three key components: (1) Prompt-Guided Semantic Injection aligns generated images with real-world semantics using text prompts; (2) Structural Contrastive Generation reproduces compositional structures of natural images by leveraging foreground-background contrastive synthesis; and (3) Perturbation-Aware Enhancement applies controlled perturbations to improve sample diversity and robustness. These components jointly empower D4C to synthesize images that are both semantically informative and structurally diverse, effectively bridging the performance gap of DFQ on CLIP. Extensive experiments validate the effectiveness of D4C, showing significant performance improvements on various bit-widths and models. For example, under the W4A8 setting with CLIP ResNet-50 and ViT-B/32, D4C achieves Top-1 accuracy improvement of 12.4% and 18.9% on CIFAR-10, 6.8% and 19.7% on CIFAR-100, and 1.4% and 5.7% on ImageNet-1K in zero-shot classification, respectively.</li>
<li><strong>摘要：</strong>无数据量化（DFQ）为模型压缩提供了一种实用的解决方案，无需访问真实数据，这使其在隐私敏感场景中特别有吸引力。虽然 DFQ 在单峰模型方面表现出了良好的前景，但其对视觉语言模型（例如对比语言图像预训练 (CLIP) 模型）的扩展仍未得到充分探索。在这项工作中，我们揭示了直接将现有的 DFQ 技术应用于 CLIP 会导致性能大幅下降，这是由于两个关键限制：语义内容不足和合成样本中图像内多样性较低。为了应对这些挑战，我们提出了 D4C，这是第一个为 CLIP 量身定制的 DFQ 框架。 D4C 通过三个关键组件合成语义丰富且结构多样的伪图像：（1）提示引导语义注入使用文本提示将生成的图像与现实世界的语义对齐； （2）结构对比生成利用前景-背景对比合成来再现自然图像的构图结构； (3) 扰动感知增强应用受控扰动来提高样本多样性和鲁棒性。这些组件共同使 D4C 能够合成语义信息丰富且结构多样的图像，有效弥补 DFQ 在 CLIP 上的性能差距。大量实验验证了 D4C 的有效性，显示了各种位宽和模型的显着性能改进。例如，在使用 CLIP ResNet-50 和 ViT-B/32 的 W4A8 设置下，D4C 在零样本分类中分别在 CIFAR-10 上实现了 12.4% 和 18.9%、在 CIFAR-100 上实现了 6.8% 和 19.7%、在 ImageNet-1K 上实现了 1.4% 和 5.7% 的 Top-1 精度提升。</li>
</ul>

<h3>Title: HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Linyin Luo, Yujuan Ding, Yunshan Ma, Wenqi Fan, Hanjiang Lai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15435">https://arxiv.org/abs/2511.15435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15435">https://arxiv.org/pdf/2511.15435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15435]] HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation(https://arxiv.org/abs/2511.15435)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Advanced multimodal Retrieval-Augmented Generation (MRAG) techniques have been widely applied to enhance the capabilities of Large Multimodal Models (LMMs), but they also bring along novel safety issues. Existing adversarial research has revealed the vulnerability of MRAG systems to knowledge poisoning attacks, which fool the retriever into recalling injected poisoned contents. However, our work considers a different setting: visual attack of MRAG by solely adding imperceptible perturbations at the image inputs of users, without manipulating any other components. This is challenging due to the robustness of fine-tuned retrievers and large-scale generators, and the effect of visual perturbation may be further weakened by propagation through the RAG chain. We propose a novel Hierarchical Visual Attack that misaligns and disrupts the two inputs (the multimodal query and the augmented knowledge) of MRAG's generator to confuse its generation. We further design a hierarchical two-stage strategy to obtain misaligned augmented knowledge. We disrupt the image input of the retriever to make it recall irrelevant knowledge from the original database, by optimizing the perturbation which first breaks the cross-modal alignment and then disrupts the multimodal semantic alignment. We conduct extensive experiments on two widely-used MRAG datasets: OK-VQA and InfoSeek. We use CLIP-based retrievers and two LMMs BLIP-2 and LLaVA as generators. Results demonstrate the effectiveness of our visual attack on MRAG through the significant decrease in both retrieval and generation performance.</li>
<li><strong>摘要：</strong>先进的多模态检索增强生成（MRAG）技术已被广泛应用于增强大型多模态模型（LMM）的能力，但它们也带来了新的安全问题。现有的对抗性研究揭示了 MRAG 系统容易遭受知识中毒攻击，这种攻击会欺骗检索器回忆注入的中毒内容。然而，我们的工作考虑了不同的设置：通过仅在用户的图像输入上添加难以察觉的扰动，而不操纵任何其他组件来对 MRAG 进行视觉攻击。由于微调检索器和大型生成器的鲁棒性，这一点具有挑战性，并且视觉扰动的影响可能会通过 RAG 链的传播进一步减弱。我们提出了一种新颖的分层视觉攻击，它会错位并破坏 MRAG 生成器的两个输入（多模式查询和增强知识），以混淆其生成。我们进一步设计了一个分层的两阶段策略来获取错位的增强知识。我们通过优化扰动来破坏检索器的图像输入，使其从原始数据库中回忆出不相关的知识，首先破坏跨模态对齐，然后破坏多模态语义对齐。我们对两个广泛使用的 MRAG 数据集进行了广泛的实验：OK-VQA 和 InfoSeek。我们使用基于 CLIP 的检索器和两个 LMM BLIP-2 和 LLaVA 作为生成器。结果证明了我们对 MRAG 的视觉攻击的有效性，检索和生成性能均显着下降。</li>
</ul>

<h3>Title: Learning to Expand Images for Efficient Visual Autoregressive Modeling</h3>
<ul>
<li><strong>Authors: </strong>Ruiqing Yang, Kaixin Zhang, Zheng Zhang, Shan You, Tao Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15499">https://arxiv.org/abs/2511.15499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15499">https://arxiv.org/pdf/2511.15499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15499]] Learning to Expand Images for Efficient Visual Autoregressive Modeling(https://arxiv.org/abs/2511.15499)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Autoregressive models have recently shown great promise in visual generation by leveraging discrete token sequences akin to language modeling. However, existing approaches often suffer from inefficiency, either due to token-by-token decoding or the complexity of multi-scale representations. In this work, we introduce Expanding Autoregressive Representation (EAR), a novel generation paradigm that emulates the human visual system's center-outward perception pattern. EAR unfolds image tokens in a spiral order from the center and progressively expands outward, preserving spatial continuity and enabling efficient parallel decoding. To further enhance flexibility and speed, we propose a length-adaptive decoding strategy that dynamically adjusts the number of tokens predicted at each step. This biologically inspired design not only reduces computational cost but also improves generation quality by aligning the generation order with perceptual relevance. Extensive experiments on ImageNet demonstrate that EAR achieves state-of-the-art trade-offs between fidelity and efficiency on single-scale autoregressive models, setting a new direction for scalable and cognitively aligned autoregressive image generation.</li>
<li><strong>摘要：</strong>自回归模型最近通过利用类似于语言建模的离散标记序列，在视觉生成方面显示出了巨大的前景。然而，由于逐个令牌解码或多尺度表示的复杂性，现有方法常常效率低下。在这项工作中，我们介绍了扩展自回归表示（EAR），这是一种模拟人类视觉系统的中心向外感知模式的新型生成范式。 EAR 从中心以螺旋顺序展开图像标记并逐渐向外扩展，保持空间连续性并实现高效的并行解码。为了进一步提高灵活性和速度，我们提出了一种长度自适应解码策略，可以动态调整每一步预测的令牌数量。这种受生物学启发的设计不仅降低了计算成本，而且通过将生成顺序与感知相关性对齐来提高生成质量。 ImageNet 上的大量实验表明，EAR 在单尺度自回归模型的保真度和效率之间实现了最先进的权衡，为可扩展且认知一致的自回归图像生成设定了新方向。</li>
</ul>

<h3>Title: A Hybrid CNN-ViT-GNN Framework with GAN-Based Augmentation for Intelligent Weed Detection in Precision Agriculture</h3>
<ul>
<li><strong>Authors: </strong>Pandiyaraju V, Abishek Karthik, Sreya Mynampati, Poovarasan L, D. Saraswathi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15535">https://arxiv.org/abs/2511.15535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15535">https://arxiv.org/pdf/2511.15535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15535]] A Hybrid CNN-ViT-GNN Framework with GAN-Based Augmentation for Intelligent Weed Detection in Precision Agriculture(https://arxiv.org/abs/2511.15535)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The task of weed detection is an essential element of precision agriculture since accurate species identification allows a farmer to selectively apply herbicides and fits into sustainable agriculture crop management. This paper proposes a hybrid deep learning framework recipe for weed detection that utilizes Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), and Graph Neural Networks (GNNs) to build robustness to multiple field conditions. A Generative Adversarial Network (GAN)-based augmentation method was imposed to balance class distributions and better generalize the model. Further, a self-supervised contrastive pre-training method helps to learn more features from limited annotated data. Experimental results yield superior results with 99.33% accuracy, precision, recall, and F1-score on multi-benchmark datasets. The proposed model architecture enables local, global, and relational feature representations and offers high interpretability and adaptability. Practically, the framework allows real-time, efficient deployment to edge devices for automated weed detecting, reducing over-reliance on herbicides and providing scalable, sustainable precision-farming options.</li>
<li><strong>摘要：</strong>杂草检测任务是精准农业的重要组成部分，因为准确的物种识别使农民能够有选择地使用除草剂并适应可持续农业作物管理。本文提出了一种用于杂草检测的混合深度学习框架配方，利用卷积神经网络 (CNN)、视觉变换器 (ViT) 和图神经网络 (GNN) 来构建对多种现场条件的鲁棒性。采用基于生成对抗网络（GAN）的增强方法来平衡类别分布并更好地推广模型。此外，自监督对比预训练方法有助于从有限的注释数据中学习更多特征。实验结果在多基准数据集上产生了 99.33% 的准确度、精确度、召回率和 F1 分数的优异结果。所提出的模型架构支持局部、全局和关系特征表示，并提供高可解释性和适应性。实际上，该框架允许实时、高效地部署到边缘设备以进行自动杂草检测，减少对除草剂的过度依赖，并提供可扩展、可持续的精准农业选项。</li>
</ul>

<h3>Title: Computer-Use Agents as Judges for Generative User Interface</h3>
<ul>
<li><strong>Authors: </strong>Kevin Qinghong Lin, Siyuan Hu, Linjie Li, Zhengyuan Yang, Lijuan Wang, Philip Torr, Mike Zheng Shou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15567">https://arxiv.org/abs/2511.15567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15567">https://arxiv.org/pdf/2511.15567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15567]] Computer-Use Agents as Judges for Generative User Interface(https://arxiv.org/abs/2511.15567)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Computer-Use Agents (CUA) are becoming increasingly capable of autonomously operating digital environments through Graphical User Interfaces (GUI). Yet, most GUI remain designed primarily for humans--prioritizing aesthetics and usability--forcing agents to adopt human-oriented behaviors that are unnecessary for efficient task execution. At the same time, rapid advances in coding-oriented language models (Coder) have transformed automatic GUI design. This raises a fundamental question: Can CUA as judges to assist Coder for automatic GUI design? To investigate, we introduce AUI-Gym, a benchmark for Automatic GUI development spanning 52 applications across diverse domains. Using language models, we synthesize 1560 tasks that simulate real-world scenarios. To ensure task reliability, we further develop a verifier that programmatically checks whether each task is executable within its environment. Building on this, we propose a Coder-CUA in Collaboration framework: the Coder acts as Designer, generating and revising websites, while the CUA serves as Judge, evaluating functionality and refining designs. Success is measured not by visual appearance, but by task solvability and CUA navigation success rate. To turn CUA feedback into usable guidance, we design a CUA Dashboard that compresses multi-step navigation histories into concise visual summaries, offering interpretable guidance for iterative redesign. By positioning agents as both designers and judges, our framework shifts interface design toward agent-native efficiency and reliability. Our work takes a step toward shifting agents from passive use toward active participation in digital environments. Our code and dataset are available at this https URL.</li>
<li><strong>摘要：</strong>计算机使用代理 (CUA) 通过图形用户界面 (GUI) 自主操作数字环境的能力越来越强。然而，大多数 GUI 仍然主要是为人类设计的——优先考虑美观和可用性——迫使智能体采用以人为本的行为，而这对于高效的任务执行来说是不必要的。与此同时，面向编码的语言模型 (Coder) 的快速发展已经改变了自动 GUI 设计。这就提出了一个根本性的问题：CUA作为评委能否辅助Coder进行自动GUI设计？为了进行调查，我们引入了 AUI-Gym，这是一个跨不同领域的 52 个应用程序的自动 GUI 开发基准。使用语言模型，我们合成了 1560 个模拟现实场景的任务。为了确保任务的可靠性，我们进一步开发了一个验证器，以编程方式检查每个任务是否在其环境中可执行。在此基础上，我们提出了一个 Coder-CUA 协作框架：Coder 充当设计师，生成和修改网站，而 CUA 充当法官，评估功能并完善设计。成功不是通过视觉外观来衡量的，而是通过任务解决能力和 CUA 导航成功率来衡量的。为了将 CUA 反馈转化为可用的指导，我们设计了一个 CUA 仪表板，将多步骤导航历史压缩为简洁的视觉摘要，为迭代重新设计提供可解释的指导。通过将代理定位为设计师和评委，我们的框架将界面设计转向代理原生的效率和可靠性。我们的工作朝着将代理从被动使用转向主动参与数字环境迈出了一步。我们的代码和数据集可在此 https URL 获取。</li>
</ul>

<h3>Title: Transferable Dual-Domain Feature Importance Attack against AI-Generated Image Detector</h3>
<ul>
<li><strong>Authors: </strong>Weiheng Zhu, Gang Cao, Jing Liu, Lifang Yu, Shaowei Weng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15571">https://arxiv.org/abs/2511.15571</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15571">https://arxiv.org/pdf/2511.15571</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15571]] Transferable Dual-Domain Feature Importance Attack against AI-Generated Image Detector(https://arxiv.org/abs/2511.15571)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent AI-generated image (AIGI) detectors achieve impressive accuracy under clean condition. In view of antiforensics, it is significant to develop advanced adversarial attacks for evaluating the security of such detectors, which remains unexplored sufficiently. This letter proposes a Dual-domain Feature Importance Attack (DuFIA) scheme to invalidate AIGI detectors to some extent. Forensically important features are captured by the spatially interpolated gradient and frequency-aware perturbation. The adversarial transferability is enhanced by jointly modeling spatial and frequency-domain feature importances, which are fused to guide the optimization-based adversarial example generation. Extensive experiments across various AIGI detectors verify the cross-model transferability, transparency and robustness of DuFIA.</li>
<li><strong>摘要：</strong>最近的人工智能生成图像（AIGI）探测器在清洁条件下实现了令人印象深刻的精度。考虑到反取证学，开发先进的对抗性攻击来评估此类探测器的安全性具有重要意义，而这一点尚未得到充分探索。这封信提出了一种双域特征重要性攻击（DuFIA）方案，以在一定程度上使 AIGI 检测器失效。通过空间插值梯度和频率感知扰动捕获法医学上的重要特征。通过对空间和频域特征重要性进行联合建模来增强对抗性可转移性，这些特征重要性被融合以指导基于优化的对抗性示例生成。在各种 AIGI 检测器上进行的大量实验验证了 DuFIA 的跨模型可转移性、透明度和鲁棒性。</li>
</ul>

<h3>Title: FlashMesh: Faster and Better Autoregressive Mesh Synthesis via Structured Speculation</h3>
<ul>
<li><strong>Authors: </strong>Tingrui Shen, Yiheng Zhang, Chen Tang, Chuan Ping, Zixing Zhao, Le Wan, Yuwang Wang, Ronggang Wang, Shengfeng He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15618">https://arxiv.org/abs/2511.15618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15618">https://arxiv.org/pdf/2511.15618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15618]] FlashMesh: Faster and Better Autoregressive Mesh Synthesis via Structured Speculation(https://arxiv.org/abs/2511.15618)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Autoregressive models can generate high-quality 3D meshes by sequentially producing vertices and faces, but their token-by-token decoding results in slow inference, limiting practical use in interactive and large-scale applications. We present FlashMesh, a fast and high-fidelity mesh generation framework that rethinks autoregressive decoding through a predict-correct-verify paradigm. The key insight is that mesh tokens exhibit strong structural and geometric correlations that enable confident multi-token speculation. FlashMesh leverages this by introducing a speculative decoding scheme tailored to the commonly used hourglass transformer architecture, enabling parallel prediction across face, point, and coordinate levels. Extensive experiments show that FlashMesh achieves up to a 2 x speedup over standard autoregressive models while also improving generation fidelity. Our results demonstrate that structural priors in mesh data can be systematically harnessed to accelerate and enhance autoregressive generation.</li>
<li><strong>摘要：</strong>自回归模型可以通过顺序生成顶点和面来生成高质量的 3D 网格，但其逐个令牌解码会导致推理缓慢，限制了交互式和大规模应用中的实际使用。我们提出了 FlashMesh，这是一种快速、高保真网格生成框架，它通过预测-正确-验证范例重新思考自回归解码。关键的见解是，网格代币表现出强大的结构和几何相关性，可以实现可靠的多代币投机。 FlashMesh 通过引入针对常用沙漏变压器架构量身定制的推测性解码方案来利用这一点，从而实现跨面、点和坐标级别的并行预测。大量实验表明，FlashMesh 的速度比标准自回归模型高出 2 倍，同时还提高了生成保真度。我们的结果表明，可以系统地利用网格数据中的结构先验来加速和增强自回归生成。</li>
</ul>

<h3>Title: First Frame Is the Place to Go for Video Content Customization</h3>
<ul>
<li><strong>Authors: </strong>Jingxi Chen, Zongxia Li, Zhichao Liu, Guangyao Shi, Xiyang Wu, Fuxiao Liu, Cornelia Fermuller, Brandon Y. Feng, Yiannis Aloimonos</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15700">https://arxiv.org/abs/2511.15700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15700">https://arxiv.org/pdf/2511.15700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15700]] First Frame Is the Place to Go for Video Content Customization(https://arxiv.org/abs/2511.15700)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>What role does the first frame play in video generation models? Traditionally, it's viewed as the spatial-temporal starting point of a video, merely a seed for subsequent animation. In this work, we reveal a fundamentally different perspective: video models implicitly treat the first frame as a conceptual memory buffer that stores visual entities for later reuse during generation. Leveraging this insight, we show that it's possible to achieve robust and generalized video content customization in diverse scenarios, using only 20-50 training examples without architectural changes or large-scale finetuning. This unveils a powerful, overlooked capability of video generation models for reference-based video customization.</li>
<li><strong>摘要：</strong>第一帧在视频生成模型中扮演什么角色？传统上，它被视为视频的时空起点，仅仅是后续动画的种子。在这项工作中，我们揭示了一个根本不同的观点：视频模型隐式地将第一帧视为概念性内存缓冲区，用于存储视觉实体以供以后在生成过程中重用。利用这种洞察力，我们表明，仅使用 20-50 个训练示例，无需架构更改或大规模微调，即可在不同场景中实现稳健且通用的视频内容定制。这揭示了用于基于参考的视频定制的视频生成模型的强大但被忽视的功能。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
