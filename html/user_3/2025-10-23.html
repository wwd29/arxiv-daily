<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-23</h1>
<h3>Title: NeuroAda: Activating Each Neuron's Potential for Parameter-Efficient Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Zhi Zhang, Yixian Shen, Congfeng Cao, Ekaterina Shutova</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18940">https://arxiv.org/abs/2510.18940</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18940">https://arxiv.org/pdf/2510.18940</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18940]] NeuroAda: Activating Each Neuron's Potential for Parameter-Efficient Fine-Tuning(https://arxiv.org/abs/2510.18940)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Existing parameter-efficient fine-tuning (PEFT) methods primarily fall into two categories: addition-based and selective in-situ adaptation. The former, such as LoRA, introduce additional modules to adapt the model to downstream tasks, offering strong memory efficiency. However, their representational capacity is often limited, making them less suitable for fine-grained adaptation. In contrast, the latter directly fine-tunes a carefully chosen subset of the original model parameters, allowing for more precise and effective adaptation, but at the cost of significantly increased memory consumption. To reconcile this trade-off, we propose NeuroAda, a novel PEFT method that enables fine-grained model finetuning while maintaining high memory efficiency. Our approach first identifies important parameters (i.e., connections within the network) as in selective adaptation, and then introduces bypass connections for these selected parameters. During finetuning, only the bypass connections are updated, leaving the original model parameters frozen. Empirical results on 23+ tasks spanning both natural language generation and understanding demonstrate that NeuroAda achieves state-of-the-art performance with as little as $\leq \textbf{0.02}\%$ trainable parameters, while reducing CUDA memory usage by up to 60%. We release our code here: this https URL.</li>
<li><strong>摘要：</strong>现有的参数高效微调（PEFT）方法主要分为两类：基于加法和选择性原位适应。前者，例如LoRA，引入了额外的模块来使模型适应下游任务，提供强大的内存效率。然而，它们的表征能力通常是有限的，使得它们不太适合细粒度的适应。相比之下，后者直接微调原始模型参数的精心选择的子集，允许更精确和有效的适应，但代价是显着增加内存消耗。为了协调这种权衡，我们提出了 NeuroAda，这是一种新颖的 PEFT 方法，可以在保持高内存效率的同时进行细粒度模型微调。我们的方法首先识别选择性适应中的重要参数（即网络内的连接），然后为这些选定的参数引入旁路连接。在微调期间，仅更新旁路连接，保持原始模型参数冻结。涵盖自然语言生成和理解的 23 多个任务的实证结果表明，NeuroAda 只需 $\leq \textbf{0.02}\%$ 可训练参数即可实现最先进的性能，同时将 CUDA 内存使用量减少高达 60%。我们在这里发布我们的代码：这个 https URL。</li>
</ul>

<h3>Title: Prior-informed optimization of treatment recommendation via bandit algorithms trained on large language model-processed historical records</h3>
<ul>
<li><strong>Authors: </strong>Saman Nessari, Ali Bozorgi-Amiri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19014">https://arxiv.org/abs/2510.19014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19014">https://arxiv.org/pdf/2510.19014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19014]] Prior-informed optimization of treatment recommendation via bandit algorithms trained on large language model-processed historical records(https://arxiv.org/abs/2510.19014)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Current medical practice depends on standardized treatment frameworks and empirical methodologies that neglect individual patient variations, leading to suboptimal health outcomes. We develop a comprehensive system integrating Large Language Models (LLMs), Conditional Tabular Generative Adversarial Networks (CTGAN), T-learner counterfactual models, and contextual bandit approaches to provide customized, data-informed clinical recommendations. The approach utilizes LLMs to process unstructured medical narratives into structured datasets (93.2% accuracy), uses CTGANs to produce realistic synthetic patient data (55% accuracy via two-sample verification), deploys T-learners to forecast patient-specific treatment responses (84.3% accuracy), and integrates prior-informed contextual bandits to enhance online therapeutic selection by effectively balancing exploration of new possibilities with exploitation of existing knowledge. Testing on stage III colon cancer datasets revealed that our KernelUCB approach obtained 0.60-0.61 average reward scores across 5,000 rounds, exceeding other reference methods. This comprehensive system overcomes cold-start limitations in online learning environments, improves computational effectiveness, and constitutes notable progress toward individualized medicine adapted to specific patient characteristics.</li>
<li><strong>摘要：</strong>当前的医疗实践依赖于标准化的治疗框架和经验方法，忽视了患者个体差异，导致健康结果不佳。我们开发了一个综合系统，集成了大型语言模型 (LLM)、条件表格生成对抗网络 (CTGAN)、T 学习器反事实模型和上下文老虎机方法，以提供定制的、基于数据的临床建议。该方法利用法学硕士将非结构化医学叙述处理为结构化数据集（准确度为 93.2%），使用 CTGAN 生成真实的合成患者数据（通过两个样本验证准确度为 55%），部署 T 学习器来预测患者特定的治疗反应（准确度为 84.3%），并集成事先知情的上下文强盗，通过有效平衡对新可能性的探索来增强在线治疗选择 利用现有知识。对 III 期结肠癌数据集的测试表明，我们的 KernelUCB 方法在 5,000 轮中获得了 0.60-0.61 的平均奖励分数，超过了其他参考方法。这个综合系统克服了在线学习环境中的冷启动限制，提高了计算效率，并在适应特定患者特征的个体化医疗方面取得了显着进展。</li>
</ul>

<h3>Title: MoAlign: Motion-Centric Representation Alignment for Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Aritra Bhowmik, Denis Korzhenkov, Cees G. M. Snoek, Amirhossein Habibian, Mohsen Ghafoorian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19022">https://arxiv.org/abs/2510.19022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19022">https://arxiv.org/pdf/2510.19022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19022]] MoAlign: Motion-Centric Representation Alignment for Video Diffusion Models(https://arxiv.org/abs/2510.19022)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Text-to-video diffusion models have enabled high-quality video synthesis, yet often fail to generate temporally coherent and physically plausible motion. A key reason is the models' insufficient understanding of complex motions that natural videos often entail. Recent works tackle this problem by aligning diffusion model features with those from pretrained video encoders. However, these encoders mix video appearance and dynamics into entangled features, limiting the benefit of such alignment. In this paper, we propose a motion-centric alignment framework that learns a disentangled motion subspace from a pretrained video encoder. This subspace is optimized to predict ground-truth optical flow, ensuring it captures true motion dynamics. We then align the latent features of a text-to-video diffusion model to this new subspace, enabling the generative model to internalize motion knowledge and generate more plausible videos. Our method improves the physical commonsense in a state-of-the-art video diffusion model, while preserving adherence to textual prompts, as evidenced by empirical evaluations on VideoPhy, VideoPhy2, VBench, and VBench-2.0, along with a user study.</li>
<li><strong>摘要：</strong>文本到视频的扩散模型已经实现了高质量的视频合成，但通常无法生成时间连贯且物理上合理的运动。一个关键原因是模型对自然视频通常需要的复杂运动理解不够。最近的工作通过将扩散模型特征与预训练视频编码器的特征对齐来解决这个问题。然而，这些编码器将视频外观和动态混合到纠缠特征中，限制了这种对齐的好处。在本文中，我们提出了一种以运动为中心的对齐框架，该框架从预训练的视频编码器中学习解缠结的运动子空间。该子空间经过优化，可预测真实光流，确保其捕获真实的运动动力学。然后，我们将文本到视频扩散模型的潜在特征与这个新的子空间对齐，使生成模型能够内化运动知识并生成更可信的视频。我们的方法改进了最先进的视频扩散模型中的物理常识，同时保留了对文本提示的遵守，对 VideoPhy、VideoPhy2、VBench 和 VBench-2.0 的实证评估以及用户研究证明了这一点。</li>
</ul>

<h3>Title: PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions</h3>
<ul>
<li><strong>Authors: </strong>Amith Ananthram, Elias Stengel-Eskin, Lorena A. Bradford, Julia Demarest, Adam Purvis, Keith Krut, Robert Stein, Rina Elster Pantalony, Mohit Bansal, Kathleen McKeown</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19060">https://arxiv.org/abs/2510.19060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19060">https://arxiv.org/pdf/2510.19060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19060]] PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions(https://arxiv.org/abs/2510.19060)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While vision-language models (VLMs) have advanced into detailed image description, evaluation remains a challenge. Standard metrics (e.g. CIDEr, SPICE) were designed for short texts and tuned to recognize errors that are now uncommon, such as object misidentification. In contrast, long texts require sensitivity to attribute and relation attachments and scores that localize errors to particular text spans. In this work, we introduce PoSh, a metric for detailed image description that uses scene graphs as structured rubrics to guide LLMs-as-a-Judge, producing aggregate scores grounded in fine-grained errors (e.g. mistakes in compositional understanding). PoSh is replicable, interpretable and a better proxy for human raters than existing metrics (including GPT4o-as-a-Judge). To validate PoSh, we introduce a challenging new dataset, DOCENT. This novel benchmark contains artwork, paired with expert-written references, and model-generated descriptions, augmented with granular and coarse judgments of their quality from art history students. Thus, DOCENT enables evaluating both detailed image description metrics and detailed image description itself in a challenging new domain. We show that PoSh achieves stronger correlations (+0.05 Spearman $\rho$) with the human judgments in DOCENT than the best open-weight alternatives, is robust to image type (using CapArena, an existing dataset of web imagery) and is a capable reward function, outperforming standard supervised fine-tuning. Then, using PoSh, we characterize the performance of open and closed models in describing the paintings, sketches and statues in DOCENT and find that foundation models struggle to achieve full, error-free coverage of images with rich scene dynamics, establishing a demanding new task to gauge VLM progress. Through both PoSh and DOCENT, we hope to enable advances in important areas such as assistive text generation.</li>
<li><strong>摘要：</strong>虽然视觉语言模型 (VLM) 已发展到详细的图像描述，但评估仍然是一个挑战。标准指标（例如 CIDEr、SPICE）专为短文本而设计，并经过调整以识别现在不常见的错误，例如对象错误识别。相反，长文本需要对属性和关系附件以及将错误定位到特定文本范围的分数敏感。在这项工作中，我们引入了 PoSh，这是一种用于详细图像描述的指标，它使用场景图作为结构化评分标准来指导法学硕士作为法官，产生基于细粒度错误（例如构图理解中的错误）的总分。 PoSh 是可复制、可解释的，并且比现有指标（包括 GPT4o-as-a-Judge）更能代表人类评分者。为了验证 PoSh，我们引入了一个具有挑战性的新数据集 DOCENT。这个新颖的基准包含艺术品，搭配专家撰写的参考文献和模型生成的描述，并通过艺术史学生对其质量的细致和粗略的判断进行增强。因此，DOCENT 能够在具有挑战性的新领域中评估详细图像描述指标和详细图像描述本身。我们表明，与最好的开放权重替代方案相比，PoSh 在 DOCENT 中与人类判断实现了更强的相关性（+0.05 Spearman $\rho$），对图像类型具有鲁棒性（使用 CapArena，现有的网络图像数据集），并且是一个强大的奖励函数，优于标准的监督微调。然后，我们使用 PoSh 描述了开放模型和封闭模型在描述 DOCENT 中的绘画、草图和雕像时的性能，并发现基础模型很难实现对具有丰富场景动态的图像的完整、无差错覆盖，从而建立了衡量 VLM 进度的艰巨新任务。通过 PoSh 和 DOCENT，我们希望能够在辅助文本生成等重要领域取得进展。</li>
</ul>

<h3>Title: UniHPR: Unified Human Pose Representation via Singular Value Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhongyu Jiang, Wenhao Chai, Lei Li, Zhuoran Zhou, Cheng-Yen Yang, Jenq-Neng Hwang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19078">https://arxiv.org/abs/2510.19078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19078">https://arxiv.org/pdf/2510.19078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19078]] UniHPR: Unified Human Pose Representation via Singular Value Contrastive Learning(https://arxiv.org/abs/2510.19078)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In recent years, there has been a growing interest in developing effective alignment pipelines to generate unified representations from different modalities for multi-modal fusion and generation. As an important component of Human-Centric applications, Human Pose representations are critical in many downstream tasks, such as Human Pose Estimation, Action Recognition, Human-Computer Interaction, Object tracking, etc. Human Pose representations or embeddings can be extracted from images, 2D keypoints, 3D skeletons, mesh models, and lots of other modalities. Yet, there are limited instances where the correlation among all of those representations has been clearly researched using a contrastive paradigm. In this paper, we propose UniHPR, a unified Human Pose Representation learning pipeline, which aligns Human Pose embeddings from images, 2D and 3D human poses. To align more than two data representations at the same time, we propose a novel singular value-based contrastive learning loss, which better aligns different modalities and further boosts performance. To evaluate the effectiveness of the aligned representation, we choose 2D and 3D Human Pose Estimation (HPE) as our evaluation tasks. In our evaluation, with a simple 3D human pose decoder, UniHPR achieves remarkable performance metrics: MPJPE 49.9mm on the Human3.6M dataset and PA-MPJPE 51.6mm on the 3DPW dataset with cross-domain evaluation. Meanwhile, we are able to achieve 2D and 3D pose retrieval with our unified human pose representations in Human3.6M dataset, where the retrieval error is 9.24mm in MPJPE.</li>
<li><strong>摘要：</strong>近年来，人们对开发有效的对齐管道以从不同模态生成统一表示以进行多模态融合和生成越来越感兴趣。作为以人为中心的应用程序的重要组成部分，人体姿势表示在许多下游任务中至关重要，例如人体姿势估计、动作识别、人机交互、对象跟踪等。人体姿势表示或嵌入可以从图像、2D 关键点、3D 骨架、网格模型和许多其他模态中提取。然而，在有限的情况下，所有这些表示之间的相关性已经使用对比范式进行了清楚的研究。在本文中，我们提出了 UniHPR，这是一个统一的人体姿势表示学习管道，它可以对齐来自图像、2D 和 3D 人体姿势的人体姿势嵌入。为了同时对齐两个以上的数据表示，我们提出了一种新颖的基于奇异值的对比学习损失，它可以更好地对齐不同的模式并进一步提高性能。为了评估对齐表示的有效性，我们选择 2D 和 3D 人体姿势估计 (HPE) 作为我们的评估任务。在我们的评估中，使用简单的 3D 人体姿势解码器，UniHPR 实现了卓越的性能指标：在 Human3.6M 数据集上的 MPJPE 49.9mm 和在 3DPW 数据集上的 PA-MPJPE 51.6mm（具有跨域评估）。同时，我们能够使用Human3.6M数据集中的统一人体姿势表示实现2D和3D姿势检索，其中MPJPE中的检索误差为9.24mm。</li>
</ul>

<h3>Title: Steering Autoregressive Music Generation with Recursive Feature Machines</h3>
<ul>
<li><strong>Authors: </strong>Daniel Zhao, Daniel Beaglehole, Taylor Berg-Kirkpatrick, Julian McAuley, Zachary Novack</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19127">https://arxiv.org/abs/2510.19127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19127">https://arxiv.org/pdf/2510.19127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19127]] Steering Autoregressive Music Generation with Recursive Feature Machines(https://arxiv.org/abs/2510.19127)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Controllable music generation remains a significant challenge, with existing methods often requiring model retraining or introducing audible artifacts. We introduce MusicRFM, a framework that adapts Recursive Feature Machines (RFMs) to enable fine-grained, interpretable control over frozen, pre-trained music models by directly steering their internal activations. RFMs analyze a model's internal gradients to produce interpretable "concept directions", or specific axes in the activation space that correspond to musical attributes like notes or chords. We first train lightweight RFM probes to discover these directions within MusicGen's hidden states; then, during inference, we inject them back into the model to guide the generation process in real-time without per-step optimization. We present advanced mechanisms for this control, including dynamic, time-varying schedules and methods for the simultaneous enforcement of multiple musical properties. Our method successfully navigates the trade-off between control and generation quality: we can increase the accuracy of generating a target musical note from 0.23 to 0.82, while text prompt adherence remains within approximately 0.02 of the unsteered baseline, demonstrating effective control with minimal impact on prompt fidelity. We release code to encourage further exploration on RFMs in the music domain.</li>
<li><strong>摘要：</strong>可控音乐生成仍然是一个重大挑战，现有方法通常需要模型重新训练或引入可听音损。我们引入了 MusicRFM，这是一个采用递归特征机 (RFM) 的框架，通过直接控制内部激活，对冻结的预训练音乐模型进行细粒度、可解释的控制。 RFM 分析模型的内部梯度，以产生可解释的“概念方向”，或激活空间中与音符或和弦等音乐属性相对应的特定轴。我们首先训练轻量级 RFM 探针来发现 MusicGen 隐藏状态中的这些方向；然后，在推理过程中，我们将它们注入模型中以实时指导生成过程，而无需每步优化。我们提出了这种控制的先进机制，包括动态的、随时间变化的时间表和同时执行多个音乐属性的方法。我们的方法成功地实现了控制和生成质量之间的权衡：我们可以将生成目标音符的准确度从 0.23 提高到 0.82，同时文本提示依从性保持在未控制基线的大约 0.02 范围内，这证明了有效的控制，对提示保真度的影响最小。我们发布代码以鼓励在音乐领域进一步探索 RFM。</li>
</ul>

<h3>Title: Video Consistency Distance: Enhancing Temporal Consistency for Image-to-Video Generation via Reward-Based Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Takehiro Aoshima, Yusuke Shinohara, Park Byeongseon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19193">https://arxiv.org/abs/2510.19193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19193">https://arxiv.org/pdf/2510.19193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19193]] Video Consistency Distance: Enhancing Temporal Consistency for Image-to-Video Generation via Reward-Based Fine-Tuning(https://arxiv.org/abs/2510.19193)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reward-based fine-tuning of video diffusion models is an effective approach to improve the quality of generated videos, as it can fine-tune models without requiring real-world video datasets. However, it can sometimes be limited to specific performances because conventional reward functions are mainly aimed at enhancing the quality across the whole generated video sequence, such as aesthetic appeal and overall consistency. Notably, the temporal consistency of the generated video often suffers when applying previous approaches to image-to-video (I2V) generation tasks. To address this limitation, we propose Video Consistency Distance (VCD), a novel metric designed to enhance temporal consistency, and fine-tune a model with the reward-based fine-tuning framework. To achieve coherent temporal consistency relative to a conditioning image, VCD is defined in the frequency space of video frame features to capture frame information effectively through frequency-domain analysis. Experimental results across multiple I2V datasets demonstrate that fine-tuning a video generation model with VCD significantly enhances temporal consistency without degrading other performance compared to the previous method.</li>
<li><strong>摘要：</strong>基于奖励的视频扩散模型微调是提高生成视频质量的有效方法，因为它可以在不需要真实视频数据集的情况下微调模型。然而，它有时可能仅限于特定的表现，因为传统的奖励功能主要旨在提高整个生成的视频序列的质量，例如审美吸引力和整体一致性。值得注意的是，当将以前的方法应用于图像到视频（I2V）生成任务时，生成视频的时间一致性通常会受到影响。为了解决这个限制，我们提出了视频一致性距离（VCD），这是一种旨在增强时间一致性的新颖指标，并使用基于奖励的微调框架来微调模型。为了实现相对于调节图像的相干时间一致性，在视频帧特征的频率空间中定义VCD，以通过频域分析有效地捕获帧信息。多个 I2V 数据集的实验结果表明，与之前的方法相比，使用 VCD 微调视频生成模型可显着增强时间一致性，而不会降低其他性能。</li>
</ul>

<h3>Title: Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks</h3>
<ul>
<li><strong>Authors: </strong>Kai Zeng, Zhanqian Wu, Kaixin Xiong, Xiaobao Wei, Xiangyu Guo, Zhenxin Zhu, Kalok Ho, Lijun Zhou, Bohan Zeng, Ming Lu, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye, Wentao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19195">https://arxiv.org/abs/2510.19195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19195">https://arxiv.org/pdf/2510.19195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19195]] Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks(https://arxiv.org/abs/2510.19195)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advancements in driving world models enable controllable generation of high-quality RGB videos or multimodal videos. Existing methods primarily focus on metrics related to generation quality and controllability. However, they often overlook the evaluation of downstream perception tasks, which are $\mathbf{really\ crucial}$ for the performance of autonomous driving. Existing methods usually leverage a training strategy that first pretrains on synthetic data and finetunes on real data, resulting in twice the epochs compared to the baseline (real data only). When we double the epochs in the baseline, the benefit of synthetic data becomes negligible. To thoroughly demonstrate the benefit of synthetic data, we introduce Dream4Drive, a novel synthetic data generation framework designed for enhancing the downstream perception tasks. Dream4Drive first decomposes the input video into several 3D-aware guidance maps and subsequently renders the 3D assets onto these guidance maps. Finally, the driving world model is fine-tuned to produce the edited, multi-view photorealistic videos, which can be used to train the downstream perception models. Dream4Drive enables unprecedented flexibility in generating multi-view corner cases at scale, significantly boosting corner case perception in autonomous driving. To facilitate future research, we also contribute a large-scale 3D asset dataset named DriveObj3D, covering the typical categories in driving scenarios and enabling diverse 3D-aware video editing. We conduct comprehensive experiments to show that Dream4Drive can effectively boost the performance of downstream perception models under various training epochs. Project: $\href{this https URL}{this\ https\ URL}$</li>
<li><strong>摘要：</strong>驾驶世界模型的最新进展使得能够可控地生成高质量 RGB 视频或多模态视频。现有方法主要关注与发电质量和可控性相关的指标。然而，他们经常忽视下游感知任务的评估，这对于自动驾驶的性能来说是$\mathbf{真正\至关重要}$。现有方法通常利用一种训练策略，首先对合成数据进行预训练，然后对真实数据进行微调，与基线（仅真实数据）相比，周期数是基线的两倍。当我们将基线的纪元加倍时，合成数据的好处就变得可以忽略不计。为了彻底展示合成数据的好处，我们引入了 Dream4Drive，这是一种新颖的合成数据生成框架，旨在增强下游感知任务。 Dream4Drive 首先将输入视频分解为多个 3D 感知引导图，然后将 3D 资源渲染到这些引导图上。最后，对驾驶世界模型进行微调以生成编辑后的多视图真实感视频，该视频可用于训练下游感知模型。 Dream4Drive 在大规模生成多视图极端情况方面提供了前所未有的灵活性，显着提高了自动驾驶中的极端情况感知。为了方便未来的研究，我们还贡献了一个名为DriveObj3D的大规模3D资产数据集，涵盖了驾驶场景中的典型类别，并支持多样化的3D感知视频编辑。我们进行了全面的实验，表明 Dream4Drive 可以有效提升下游感知模型在各种训练时期的性能。项目：$\href{这个https URL}{这个\ https\ URL}$</li>
</ul>

<h3>Title: Advances in 4D Representation: Geometry, Motion, and Interaction</h3>
<ul>
<li><strong>Authors: </strong>Mingrui Zhao, Sauradip Nag, Kai Wang, Aditya Vora, Guangda Ji, Peter Chun, Ali Mahdavi-Amiri, Hao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19255">https://arxiv.org/abs/2510.19255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19255">https://arxiv.org/pdf/2510.19255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19255]] Advances in 4D Representation: Geometry, Motion, and Interaction(https://arxiv.org/abs/2510.19255)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We present a survey on 4D generation and reconstruction, a fast-evolving subfield of computer graphics whose developments have been propelled by recent advances in neural fields, geometric and motion deep learning, as well 3D generative artificial intelligence (GenAI). While our survey is not the first of its kind, we build our coverage of the domain from a unique and distinctive perspective of 4D representations\/}, to model 3D geometry evolving over time while exhibiting motion and interaction. Specifically, instead of offering an exhaustive enumeration of many works, we take a more selective approach by focusing on representative works to highlight both the desirable properties and ensuing challenges of each representation under different computation, application, and data scenarios. The main take-away message we aim to convey to the readers is on how to select and then customize the appropriate 4D representations for their tasks. Organizationally, we separate the 4D representations based on three key pillars: geometry, motion, and interaction. Our discourse will not only encompass the most popular representations of today, such as neural radiance fields (NeRFs) and 3D Gaussian Splatting (3DGS), but also bring attention to relatively under-explored representations in the 4D context, such as structured models and long-range motions. Throughout our survey, we will reprise the role of large language models (LLMs) and video foundational models (VFMs) in a variety of 4D applications, while steering our discussion towards their current limitations and how they can be addressed. We also provide a dedicated coverage on what 4D datasets are currently available, as well as what is lacking, in driving the subfield forward. Project page:this https URL</li>
<li><strong>摘要：</strong>我们对 4D 生成和重建进行了一项调查，这是计算机图形学的一个快速发展的子领域，其发展受到神经领域、几何和运动深度学习以及 3D 生成人工智能 (GenAI) 的最新进展的推动。虽然我们的调查不是此类调查中的首次，但我们从 4D 表示的独特且独特的角度构建了该领域的覆盖范围，以对随时间演变的 3D 几何模型进行建模，同时展示运动和交互。具体来说，我们没有对许多作品进行详尽的列举，而是采取更有选择性的方法，重点关注代表性作品，以突出每种表示在不同计算、应用和数据场景下的理想属性和随之而来的挑战。我们旨在向读者传达的主要信息是如何选择并定制适合其任务的 4D 表示。在组织上，我们根据三个关键支柱来分离 4D 表示：几何、运动和交互。我们的讨论不仅涵盖当今最流行的表示形式，例如神经辐射场 (NeRF) 和 3D 高斯分布 (3DGS)，而且还将关注 4D 环境中相对未充分探索的表示形式，例如结构化模型和远程运动。在整个调查中，我们将重申大语言模型 (LLM) 和视频基础模型 (VFM) 在各种 4D 应用中的作用，同时引导我们讨论它们当前的局限性以及如何解决它们。我们还专门介绍了当前可用的 4D 数据集以及在推动该子领域向前发展方面所缺乏的数据集。项目页面：这个https URL</li>
</ul>

<h3>Title: SCEESR: Semantic-Control Edge Enhancement for Diffusion-Based Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Yun Kai Zhuang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19272">https://arxiv.org/abs/2510.19272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19272">https://arxiv.org/pdf/2510.19272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19272]] SCEESR: Semantic-Control Edge Enhancement for Diffusion-Based Super-Resolution(https://arxiv.org/abs/2510.19272)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generation, generative</a></li>
<li><strong>Abstract: </strong>Real-world image super-resolution (Real-ISR) must handle complex degradations and inherent reconstruction ambiguities. While generative models have improved perceptual quality, a key trade-off remains with computational cost. One-step diffusion models offer speed but often produce structural inaccuracies due to distillation artifacts. To address this, we propose a novel SR framework that enhances a one-step diffusion model using a ControlNet mechanism for semantic edge guidance. This integrates edge information to provide dynamic structural control during single-pass inference. We also introduce a hybrid loss combining L2, LPIPS, and an edge-aware AME loss to optimize for pixel accuracy, perceptual quality, and geometric precision. Experiments show our method effectively improves structural integrity and realism while maintaining the efficiency of one-step generation, achieving a superior balance between output quality and inference speed. The results of test datasets will be published at this https URL and the related code will be published at this https URL.</li>
<li><strong>摘要：</strong>真实世界图像超分辨率（Real-ISR）必须处理复杂的退化和固有的重建模糊性。虽然生成模型提高了感知质量，但计算成本仍然是一个关键的权衡。一步扩散模型提供了速度，但经常由于蒸馏伪影而产生结构误差。为了解决这个问题，我们提出了一种新颖的 SR 框架，该框架使用 ControlNet 机制进行语义边缘引导来增强一步扩散模型。这集成了边缘信息，以在单遍推理期间提供动态结构控制。我们还引入了一种结合了 L2、LPIPS 和边缘感知 AME 损失的混合损失，以优化像素精度、感知质量和几何精度。实验表明，我们的方法有效提高了结构完整性和真实性，同时保持了一步生成的效率，在输出质量和推理速度之间实现了良好的平衡。测试数据集的结果将在此 https URL 发布，相关代码将在此 https URL 发布。</li>
</ul>

<h3>Title: D2D: Detector-to-Differentiable Critic for Improved Numeracy in Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Nobline Yoo, Olga Russakovsky, Ye Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19278">https://arxiv.org/abs/2510.19278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19278">https://arxiv.org/pdf/2510.19278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19278]] D2D: Detector-to-Differentiable Critic for Improved Numeracy in Text-to-Image Generation(https://arxiv.org/abs/2510.19278)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) diffusion models have achieved strong performance in semantic alignment, yet they still struggle with generating the correct number of objects specified in prompts. Existing approaches typically incorporate auxiliary counting networks as external critics to enhance numeracy. However, since these critics must provide gradient guidance during generation, they are restricted to regression-based models that are inherently differentiable, thus excluding detector-based models with superior counting ability, whose count-via-enumeration nature is non-differentiable. To overcome this limitation, we propose Detector-to-Differentiable (D2D), a novel framework that transforms non-differentiable detection models into differentiable critics, thereby leveraging their superior counting ability to guide numeracy generation. Specifically, we design custom activation functions to convert detector logits into soft binary indicators, which are then used to optimize the noise prior at inference time with pre-trained T2I models. Our extensive experiments on SDXL-Turbo, SD-Turbo, and Pixart-DMD across four benchmarks of varying complexity (low-density, high-density, and multi-object scenarios) demonstrate consistent and substantial improvements in object counting accuracy (e.g., boosting up to 13.7% on D2D-Small, a 400-prompt, low-density benchmark), with minimal degradation in overall image quality and computational overhead.</li>
<li><strong>摘要：</strong>文本到图像（T2I）扩散模型在语义对齐方面取得了强大的性能，但它们仍然难以生成提示中指定的正确数量的对象。现有的方法通常将辅助计数网络作为外部批评器来增强计算能力。然而，由于这些批评者必须在生成过程中提供梯度指导，因此它们仅限于本质上可微的基于回归的模型，从而排除了具有卓越计数能力的基于检测器的模型，其通过枚举进行计数的性质是不可微的。为了克服这一限制，我们提出了 Detector-to-Differentiable (D2D)，这是一种新颖的框架，它将不可微分的检测模型转换为可微分的批评者，从而利用其卓越的计数能力来指导计算生成。具体来说，我们设计了自定义激活函数，将检测器 logits 转换为软二进制指标，然后使用预训练的 T2I 模型在推理时优化噪声先验。我们在 SDXL-Turbo、SD-Turbo 和 Pixart-DMD 上针对不同复杂度的四个基准（低密度、高密度和多对象场景）进行的广泛实验表明，对象计数精度得到了一致且显着的改进（例如，在 D2D-Small（一个 400 条提示的低密度基准）上提高了 13.7%），并且降级最小 整体图像质量和计算开销。</li>
</ul>

<h3>Title: QiMeng-SALV: Signal-Aware Learning for Verilog Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Yang Zhang, Rui Zhang, Jiaming Guo, Lei Huang, Di Huang, Yunpu Zhao, Shuyao Cheng, Pengwei Jin, Chongxiao Li, Zidong Du, Xing Hu, Qi Guo, Yunji Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19296">https://arxiv.org/abs/2510.19296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19296">https://arxiv.org/pdf/2510.19296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19296]] QiMeng-SALV: Signal-Aware Learning for Verilog Code Generation(https://arxiv.org/abs/2510.19296)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The remarkable progress of Large Language Models (LLMs) presents promising opportunities for Verilog code generation which is significantly important for automated circuit design. The lacking of meaningful functional rewards hinders the preference optimization based on Reinforcement Learning (RL) for producing functionally correct Verilog code. In this paper, we propose Signal-Aware Learning for Verilog code generation (QiMeng-SALV) by leveraging code segments of functionally correct output signal to optimize RL training. Considering Verilog code specifies the structural interconnection of hardware gates and wires so that different output signals are independent, the key insight of QiMeng-SALV is to extract verified signal-aware implementations in partially incorrect modules, so as to enhance the extraction of meaningful functional rewards. Roughly, we verify the functional correctness of signals in generated module by comparing with that of reference module in the training data. Then abstract syntax tree (AST) is employed to identify signal-aware code segments which can provide meaningful functional rewards from erroneous modules. Finally, we introduce signal-aware DPO which is optimized on the correct signal-level code segments, thereby preventing noise and interference from incorrect signals. The proposed QiMeng-SALV underscores the paradigm shift from conventional module-level to fine-grained signal-level optimization in Verilog code generation, addressing the issue of insufficient functional rewards. Experiments demonstrate that our method achieves state-of-the-art performance on VerilogEval and RTLLM, with a 7B parameter model matching the performance of the DeepSeek v3 671B model and significantly outperforming the leading open-source model CodeV trained on the same dataset. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的显着进步为 Verilog 代码生成提供了充满希望的机会，这对于自动化电路设计非常重要。缺乏有意义的功能奖励阻碍了基于强化学习 (RL) 的偏好优化，无法生成功能正确的 Verilog 代码。在本文中，我们提出了用于 Verilog 代码生成的信号感知学习 (QiMeng-SALV)，通过利用功能正确的输出信号的代码段来优化 RL 训练。考虑到Verilog代码指定了硬件门和线路的结构互连，使得不同的输出信号是独立的，QiMeng-SALV的关键洞察是在部分不正确的模块中提取经过验证的信号感知实现，从而增强有意义的功能奖励的提取。粗略地说，我们通过与训练数据中参考模块的信号进行比较来验证生成模块中信号的功能正确性。然后采用抽象语法树（AST）来识别信号感知代码段，这些代码段可以从错误模块中提供有意义的功能奖励。最后，我们引入了信号感知 DPO，它在正确的信号级代码段上进行了优化，从而防止了错误信号带来的噪声和干扰。所提出的 QiMeng-SALV 强调了 Verilog 代码生成中从传统模块级到细粒度信号级优化的范式转变，解决了功能奖励不足的问题。实验表明，我们的方法在 VerilogEval 和 RTLLM 上实现了最先进的性能，7B 参数模型与 DeepSeek v3 671B 模型的性能相匹配，并且显着优于在同一数据集上训练的领先开源模型 CodeV。我们的代码可以在这个 https URL 上找到。</li>
</ul>

<h3>Title: Loopholing Discrete Diffusion: Deterministic Bypass of the Sampling Wall</h3>
<ul>
<li><strong>Authors: </strong>Mingyu Jo, Jaesik Yoon, Justin Deschenaux, Caglar Gulcehre, Sungjin Ahn</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19304">https://arxiv.org/abs/2510.19304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19304">https://arxiv.org/pdf/2510.19304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19304]] Loopholing Discrete Diffusion: Deterministic Bypass of the Sampling Wall(https://arxiv.org/abs/2510.19304)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Discrete diffusion models offer a promising alternative to autoregressive generation through parallel decoding, but they suffer from a sampling wall: once categorical sampling occurs, rich distributional information collapses into one-hot vectors and cannot be propagated across steps, forcing subsequent steps to operate with limited information. To mitigate this problem, we introduce Loopholing, a novel and simple mechanism that preserves this information via a deterministic latent pathway, leading to Loopholing Discrete Diffusion Models (LDDMs). Trained efficiently with a self-conditioning strategy, LDDMs achieve substantial gains-reducing generative perplexity by up to 61% over prior baselines, closing (and in some cases surpassing) the gap with autoregressive models, and producing more coherent text. Applied to reasoning tasks, LDDMs also improve performance on arithmetic benchmarks such as Countdown and Game of 24. These results also indicate that loopholing mitigates idle steps and oscillations, providing a scalable path toward high-quality non-autoregressive text generation.</li>
<li><strong>摘要：</strong>离散扩散模型通过并行解码为自回归生成提供了一种有希望的替代方案，但它们受到采样墙的影响：一旦发生分类采样，丰富的分布信息就会崩溃为单热向量，并且无法跨步骤传播，迫使后续步骤使用有限的信息进行操作。为了缓解这个问题，我们引入了漏洞，这是一种新颖而简单的机制，它通过确定性的潜在路径保留这些信息，从而产生漏洞离散扩散模型（LDDM）。通过自我调节策略进行有效训练，LDDM 取得了显着的成果，将生成性困惑度比之前的基线降低了 61%，缩小了（在某些情况下甚至超越）与自回归模型的差距，并生成了更加连贯的文本。应用于推理任务时，LDDM 还可以提高算术基准（例如倒计时和 24 局游戏）的性能。这些结果还表明，漏洞可以减少空闲步骤和振荡，从而为高质量非自回归文本生成提供可扩展的路径。</li>
</ul>

<h3>Title: Unified Reinforcement and Imitation Learning for Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Byung-Kwan Lee, Ryo Hachiuma, Yong Man Ro, Yu-Chiang Frank Wang, Yueh-Hua Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19307">https://arxiv.org/abs/2510.19307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19307">https://arxiv.org/pdf/2510.19307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19307]] Unified Reinforcement and Imitation Learning for Vision-Language Models(https://arxiv.org/abs/2510.19307)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) have achieved remarkable progress, yet their large scale often renders them impractical for resource-constrained environments. This paper introduces Unified Reinforcement and Imitation Learning (RIL), a novel and efficient training algorithm designed to create powerful, lightweight VLMs. RIL distinctively combines the strengths of reinforcement learning with adversarial imitation learning. This enables smaller student VLMs not only to mimic the sophisticated text generation of large teacher models but also to systematically improve their generative capabilities through reinforcement signals. Key to our imitation framework is an LLM-based discriminator that adeptly distinguishes between student and teacher outputs, complemented by guidance from multiple large teacher VLMs to ensure diverse learning. This unified learning strategy, leveraging both reinforcement and imitation, empowers student models to achieve significant performance gains, making them competitive with leading closed-source VLMs. Extensive experiments on diverse vision-language benchmarks demonstrate that RIL significantly narrows the performance gap with state-of-the-art open- and closed-source VLMs and, in several instances, surpasses them.</li>
<li><strong>摘要：</strong>视觉语言模型（VLM）已经取得了显着的进步，但它们的大规模往往使其对于资源受限的环境来说不切实际。本文介绍了统一强化和模仿学习 (RIL)，这是一种新颖且高效的训练算法，旨在创建强大的轻量级 VLM。 RIL 独特地将强化学习与对抗性模仿学习的优势结合起来。这使得较小的学生 VLM 不仅能够模仿大型教师模型的复杂文本生成，而且还可以通过强化信号系统地提高其生成能力。我们模仿框架的关键是基于 LLM 的判别器，它能够熟练地区分学生和教师的输出，并辅以多个大型教师 VLM 的指导，以确保多样化的学习。这种统一的学习策略利用强化和模仿，使学生模型能够实现显着的性能提升，使其与领先的闭源 VLM 竞争。对各种视觉语言基准的大量实验表明，RIL 显着缩小了与最先进的开源和闭源 VLM 的性能差距，并且在某些情况下甚至超越了它们。</li>
</ul>

<h3>Title: Learning Noise-Resilient and Transferable Graph-Text Alignment via Dynamic Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Liu, Minglai Shao, Zengyi Wo, Yunlong Chu, Bing Hao, Shengzhong Liu, Ruijie Wang, Jianxin Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19384">https://arxiv.org/abs/2510.19384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19384">https://arxiv.org/pdf/2510.19384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19384]] Learning Noise-Resilient and Transferable Graph-Text Alignment via Dynamic Quality Assessment(https://arxiv.org/abs/2510.19384)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Pre-training Graph Foundation Models (GFMs) on text-attributed graphs (TAGs) is central to web-scale applications such as search, recommendation, and knowledge discovery. However, existing CLIP-style graph-text aligners face two key limitations: they assume strict one-to-one correspondences between nodes and texts, overlooking the inherent many-to-many relations in real-world graphs; and they rely on static alignment objectives that cannot adapt to varying data quality, making them brittle under noisy supervision. Together, these limitations expose a core dilemma: embracing expressive many-to-many alignment amplifies noise, while reverting to strict one-to-one strategies sacrifices semantic diversity and fails to handle inherently mismatched pairs. To address these challenges, we propose ADAligner, a dynamic, quality-aware graph-text alignment framework that dynamically adjusts between expressive many-to-many and conservative one-to-one objectives according to supervision quality. ADAligner estimates batch-level alignment reliability in real time and adapts its optimization accordingly, promoting soft, subgraph-level many-to-many alignment when supervision is clean, while emphasizing reliable one-to-one alignment by dynamically filtering low-confidence pairs under noise. Theoretically, we prove that this dynamic mechanism forms a stable negative feedback process, ensuring convergence and robustness. Comprehensive experiments on nine diverse TAG datasets demonstrate that ADAligner consistently outperforms prior graph-text aligners on zero-/few-shot node classification, link prediction and cross-modal retrieval tasks. It maintains strong robustness under noisy supervision and accelerates pre-training by approximately 2 to 3 times compared to multimodal baselines, establishing a scalable and reliable foundation for graph-text representation learning in real-world web environments.</li>
<li><strong>摘要：</strong>在文本属性图 (TAG) 上预训练图基础模型 (GFM) 是搜索、推荐和知识发现等网络规模应用的核心。然而，现有的 CLIP 风格的图文本对齐器面临两个关键限制：它们假设节点和文本之间严格的一对一对应关系，忽略了现实世界图中固有的多对多关系；它们依赖于静态对齐目标，无法适应不同的数据质量，这使得它们在嘈杂的监督下变得脆弱。这些限制共同暴露了一个核心困境：采用富有表现力的多对多对齐会放大噪音，而恢复​​严格的一对一策略会牺牲语义多样性，并且无法处理本质上不匹配的对。为了应对这些挑战，我们提出了 ADAligner，这是一种动态的、质量感知的图文本对齐框架，可以根据监督质量在富有表现力的多对多和保守的一对一目标之间进行动态调整。 ADAligner 实时估计批次级对齐可靠性并相应地调整其优化，在监督干净时促进软、子图级多对多对齐，同时通过在噪声下动态过滤低置信对来强调可靠的一对一对齐。从理论上讲，我们证明这种动态机制形成了稳定的负反馈过程，确保了收敛性和鲁棒性。对九个不同 TAG 数据集的综合实验表明，ADAaligner 在零/少样本节点分类、链接预测和跨模式检索任务方面始终优于先前的图文本对齐器。与多模态基线相比，它在噪声监督下保持了强大的鲁棒性，并将预训练速度提高了约 2 至 3 倍，为现实网络环境中的图文本表示学习奠定了可扩展且可靠的基础。</li>
</ul>

<h3>Title: LLM Unlearning with LLM Beliefs</h3>
<ul>
<li><strong>Authors: </strong>Kemou Li, Qizhou Wang, Yue Wang, Fengpeng Li, Jun Liu, Bo Han, Jiantao Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19422">https://arxiv.org/abs/2510.19422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19422">https://arxiv.org/pdf/2510.19422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19422]] LLM Unlearning with LLM Beliefs(https://arxiv.org/abs/2510.19422)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models trained on vast corpora inherently risk memorizing sensitive or harmful content, which may later resurface in their outputs. Prevailing unlearning methods generally rely on gradient ascent and its variants to lower the probability of specific target responses. However, we find that this strategy induces a critical side effect: probability mass is redistributed into high-likelihood regions, often corresponding to semantically related rephrasings of the targets. We refer to this as the squeezing effect, which explains why many methods yield merely spurious unlearning, a problem further obscured by automated metrics (e.g., ROUGE, truth ratio) that misreport actual success. To address this, we propose a bootstrapping (BS) framework that explicitly links the squeezing effect with the model's own high-confidence generations, namely its model beliefs. Since model beliefs inherently capture the very high-likelihood regions where probability mass is squeezed, incorporating them into the unlearning objective directly counters the squeezing effect. By jointly suppressing both target responses and model beliefs, BS-T (token) attenuates high-probability tokens, whereas BS-S (sequence) removes entire high-confidence generations, together achieving more thorough forgetting while preserving utility. Extensive experiments across diverse benchmarks with various model families confirm the effectiveness of our approach.</li>
<li><strong>摘要：</strong>在庞大的语料库上训练的大型语言模型本身就有记住敏感或有害内容的风险，这些内容可能会在以后的输出中重新出现。流行的遗忘方法通常依赖于梯度上升及其变体来降低特定目标响应的概率。然而，我们发现这种策略会产生一个严重的副作用：概率质量被重新分配到高似然区域，通常对应于目标的语义相关的重新措辞。我们将其称为挤压效应，这解释了为什么许多方法仅仅产生虚假的遗忘，这个问题被误报实际成功的自动化指标（例如 ROUGE、真值比）进一步掩盖。为了解决这个问题，我们提出了一个引导（BS）框架，该框架将挤压效应与模型自身的高置信度世代（即模型信念）明确联系起来。由于模型信念本质上捕获了概率质量被挤压的非常高似然区域，因此将它们合并到遗忘目标中可以直接抵消挤压效应。通过联合抑制目标响应和模型信念，BS-T（令牌）削弱了高概率令牌，而 BS-S（序列）则删除了整个高置信度代，共同实现了更彻底的遗忘，同时保留了效用。各种模型系列的不同基准的广泛实验证实了我们方法的有效性。</li>
</ul>

<h3>Title: PCP-GAN: Property-Constrained Pore-scale image reconstruction via conditional Generative Adversarial Networks</h3>
<ul>
<li><strong>Authors: </strong>Ali Sadeghkhani, Brandon Bennett, Masoud Babaei, Arash Rabbani</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, physics.geo-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19465">https://arxiv.org/abs/2510.19465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19465">https://arxiv.org/pdf/2510.19465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19465]] PCP-GAN: Property-Constrained Pore-scale image reconstruction via conditional Generative Adversarial Networks(https://arxiv.org/abs/2510.19465)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Obtaining truly representative pore-scale images that match bulk formation properties remains a fundamental challenge in subsurface characterization, as natural spatial heterogeneity causes extracted sub-images to deviate significantly from core-measured values. This challenge is compounded by data scarcity, where physical samples are only available at sparse well locations. This study presents a multi-conditional Generative Adversarial Network (cGAN) framework that generates representative pore-scale images with precisely controlled properties, addressing both the representativeness challenge and data availability constraints. The framework was trained on thin section samples from four depths (1879.50-1943.50 m) of a carbonate formation, simultaneously conditioning on porosity values and depth parameters within a single unified model. This approach captures both universal pore network principles and depth-specific geological characteristics, from grainstone fabrics with interparticle-intercrystalline porosity to crystalline textures with anhydrite inclusions. The model achieved exceptional porosity control (R^2=0.95) across all formations with mean absolute errors of 0.0099-0.0197. Morphological validation confirmed preservation of critical pore network characteristics including average pore radius, specific surface area, and tortuosity, with statistical differences remaining within acceptable geological tolerances. Most significantly, generated images demonstrated superior representativeness with dual-constraint errors of 1.9-11.3% compared to 36.4-578% for randomly extracted real sub-images. This capability provides transformative tools for subsurface characterization, particularly valuable for carbon storage, geothermal energy, and groundwater management applications where knowing the representative morphology of the pore space is critical for implementing digital rock physics.</li>
<li><strong>摘要：</strong>获得与整体地层特性相匹配的真正具有代表性的孔隙尺度图像仍然是地下表征中的一个基本挑战，因为自然空间异质性导致提取的子图像与岩心测量值显着偏离。数据稀缺加剧了这一挑战，因为物理样本只能在稀疏的井位获得。本研究提出了一种多条件生成对抗网络（cGAN）框架，该框架可生成具有精确控制属性的代表性孔隙尺度图像，解决代表性挑战和数据可用性限制。该框架在碳酸盐地层四个深度（1879.50-1943.50 m）的薄片样本上进行训练，同时在单个统一模型中调节孔隙度值和深度参数。这种方法捕获了通用的孔隙网络原理和特定深度的地质特征，从具有颗粒间-晶间孔隙度的颗粒岩织物到具有硬石膏包裹体的晶体结构。该模型在所有地层中实现了出色的孔隙度控制 (R^2=0.95)，平均绝对误差为 0.0099-0.0197。形态验证证实了关键孔隙网络特征的保留，包括平均孔隙半径、比表面积和弯曲度，统计差异保持在可接受的地质公差范围内。最重要的是，生成的图像表现出卓越的代表性，双约束误差为 1.9-11.3%，而随机提取的真实子图像的双约束误差为 36.4-578%。该功能为地下表征提供了变革性工具，对于碳储存、地热能和地下水管理应用特别有价值，在这些应用中，了解孔隙空间的代表性形态对于实施数字岩石物理至关重要。</li>
</ul>

<h3>Title: Predicting before Reconstruction: A generative prior framework for MRI acceleration</h3>
<ul>
<li><strong>Authors: </strong>Juhyung Park, Rokgi Hong, Roh-Eul Yoo, Jaehyeon Koo, Se Young Chun, Seung Hong Choi, Jongho Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19472">https://arxiv.org/abs/2510.19472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19472">https://arxiv.org/pdf/2510.19472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19472]] Predicting before Reconstruction: A generative prior framework for MRI acceleration(https://arxiv.org/abs/2510.19472)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in artificial intelligence have created transformative capabilities in image synthesis and generation, enabling diverse research fields to innovate at revolutionary speed and spectrum. In this study, we leverage this generative power to introduce a new paradigm for accelerating Magnetic Resonance Imaging (MRI), introducing a shift from image reconstruction to proactive predictive imaging. Despite being a cornerstone of modern patient care, MRI's lengthy acquisition times limit clinical throughput. Our novel framework addresses this challenge by first predicting a target contrast image, which then serves as a data-driven prior for reconstructing highly under-sampled data. This informative prior is predicted by a generative model conditioned on diverse data sources, such as other contrast images, previously scanned images, acquisition parameters, patient information. We demonstrate this approach with two key applications: (1) reconstructing FLAIR images using predictions from T1w and/or T2w scans, and (2) reconstructing T1w images using predictions from previously acquired T1w scans. The framework was evaluated on internal and multiple public datasets (total 14,921 scans; 1,051,904 slices), including multi-channel k-space data, for a range of high acceleration factors (x4, x8 and x12). The results demonstrate that our prediction-prior reconstruction method significantly outperforms other approaches, including those with alternative or no prior information. Through this framework we introduce a fundamental shift from image reconstruction towards a new paradigm of predictive imaging.</li>
<li><strong>摘要：</strong>人工智能的最新进展创造了图像合成和生成方面的变革能力，使不同的研究领域能够以革命性的速度和范围进行创新。在这项研究中，我们利用这种生成能力引入了一种加速磁共振成像（MRI）的新范例，实现了从图像重建到主动预测成像的转变。尽管 MRI 是现代患者护理的基石，但其较长的采集时间限制了临床吞吐量。我们的新颖框架通过首先预测目标对比度图像来解决这一挑战，然后将其用作数据驱动的先验，用于重建高度欠采样的数据。这种信息先验是通过以不同数据源为条件的生成模型来预测的，例如其他对比图像、先前扫描的图像、采集参数、患者信息。我们通过两个关键应用演示了这种方法：（1）使用来自 T1w 和/或 T2w 扫描的预测重建 FLAIR 图像，以及（2）使用来自先前采集的 T1w 扫描的预测重建 T1w 图像。该框架在内部和多个公共数据集（总共 14,921 次扫描；1,051,904 个切片）上进行了评估，包括多通道 k 空间数据，针对一系列高加速因子（x4、x8 和 x12）。结果表明，我们的预测先验重建方法显着优于其他方法，包括那些具有替代信息或没有先验信息的方法。通过这个框架，我们引入了从图像重建到预测成像新范式的根本转变。</li>
</ul>

<h3>Title: From Prototypes to Sparse ECG Explanations: SHAP-Driven Counterfactuals for Multivariate Time-Series Multi-class Classification</h3>
<ul>
<li><strong>Authors: </strong>Maciej Mozolewski, Betül Bayrak, Kerstin Bach, Grzegorz J. Nalepa</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19514">https://arxiv.org/abs/2510.19514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19514">https://arxiv.org/pdf/2510.19514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19514]] From Prototypes to Sparse ECG Explanations: SHAP-Driven Counterfactuals for Multivariate Time-Series Multi-class Classification(https://arxiv.org/abs/2510.19514)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In eXplainable Artificial Intelligence (XAI), instance-based explanations for time series have gained increasing attention due to their potential for actionable and interpretable insights in domains such as healthcare. Addressing the challenges of explainability of state-of-the-art models, we propose a prototype-driven framework for generating sparse counterfactual explanations tailored to 12-lead ECG classification models. Our method employs SHAP-based thresholds to identify critical signal segments and convert them into interval rules, uses Dynamic Time Warping (DTW) and medoid clustering to extract representative prototypes, and aligns these prototypes to query R-peaks for coherence with the sample being explained. The framework generates counterfactuals that modify only 78% of the original signal while maintaining 81.3% validity across all classes and achieving 43% improvement in temporal stability. We evaluate three variants of our approach, Original, Sparse, and Aligned Sparse, with class-specific performance ranging from 98.9% validity for myocardial infarction (MI) to challenges with hypertrophy (HYP) detection (13.2%). This approach supports near realtime generation (< 1 second) of clinically valid counterfactuals and provides a foundation for interactive explanation platforms. Our findings establish design principles for physiologically-aware counterfactual explanations in AI-based diagnosis systems and outline pathways toward user-controlled explanation interfaces for clinical deployment.</li>
<li><strong>摘要：</strong>在可解释人工智能（XAI）中，基于实例的时间序列解释因其在医疗保健等领域提供可操作和可解释的见解的潜力而受到越来越多的关注。为了解决最先进模型的可解释性挑战，我们提出了一个原型驱动的框架，用于生成适合 12 导联心电图分类模型的稀疏反事实解释。我们的方法采用基于 SHAP 的阈值来识别关键信号段并将其转换为间隔规则，使用动态时间规整 (DTW) 和中心点聚类来提取代表性原型，并对齐这些原型以查询 R 峰值与所解释的样本的一致性。该框架生成的反事实仅修改了 78% 的原始信号，同时在所有类别中保持 81.3% 的有效性，并在时间稳定性方面实现了 43% 的改进。我们评估了我们方法的三种变体：原始、稀疏和对齐稀疏，其特定类别的性能范围从心肌梗塞 (MI) 的 98.9% 有效性到肥厚 (HYP) 检测的挑战 (13.2%)。这种方法支持近乎实时地生成（< 1 秒）临床有效的反事实，并为交互式解释平台提供了基础。我们的研究结果为基于人工智能的诊断系统中的生理感知反事实解释建立了设计原则，并概述了用于临床部署的用户控制解释界面的路径。</li>
</ul>

<h3>Title: PoseCrafter: Extreme Pose Estimation with Hybrid Video Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Qing Mao, Tianxin Huang, Yu Zhu, Jinqiu Sun, Yanning Zhang, Gim Hee Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19527">https://arxiv.org/abs/2510.19527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19527">https://arxiv.org/pdf/2510.19527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19527]] PoseCrafter: Extreme Pose Estimation with Hybrid Video Synthesis(https://arxiv.org/abs/2510.19527)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Pairwise camera pose estimation from sparsely overlapping image pairs remains a critical and unsolved challenge in 3D vision. Most existing methods struggle with image pairs that have small or no overlap. Recent approaches attempt to address this by synthesizing intermediate frames using video interpolation and selecting key frames via a self-consistency score. However, the generated frames are often blurry due to small overlap inputs, and the selection strategies are slow and not explicitly aligned with pose estimation. To solve these cases, we propose Hybrid Video Generation (HVG) to synthesize clearer intermediate frames by coupling a video interpolation model with a pose-conditioned novel view synthesis model, where we also propose a Feature Matching Selector (FMS) based on feature correspondence to select intermediate frames appropriate for pose estimation from the synthesized results. Extensive experiments on Cambridge Landmarks, ScanNet, DL3DV-10K, and NAVI demonstrate that, compared to existing SOTA methods, PoseCrafter can obviously enhance the pose estimation performances, especially on examples with small or no overlap.</li>
<li><strong>摘要：</strong>从稀疏重叠的图像对中进行成对相机姿态估计仍然是 3D 视觉中一个关键且尚未解决的挑战。大多数现有方法都难以处理重叠较小或没有重叠的图像对。最近的方法试图通过使用视频插值合成中间帧并通过自一致性分数选择关键帧来解决这个问题。然而，由于输入重叠小，生成的帧通常是模糊的，并且选择策略很慢并且与姿态估计没有明确对齐。为了解决这些情况，我们提出混合视频生成（HVG），通过将视频插值模型与姿势条件新颖的视图合成模型相结合来合成更清晰的中间帧，其中我们还提出了基于特征对应的特征匹配选择器（FMS），以从合成结果中选择适合姿势估计的中间帧。在 Cambridge Landmarks、ScanNet、DL3DV-10K 和 NAVI 上进行的大量实验表明，与现有的 SOTA 方法相比，PoseCrafter 可以明显增强姿态估计性能，尤其是在重叠较小或没有重叠的示例上。</li>
</ul>

<h3>Title: The Intricate Dance of Prompt Complexity, Quality, Diversity, and Consistency in T2I Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaofeng Zhang, Aaron Courville, Michal Drozdzal, Adriana Romero-Soriano</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19557">https://arxiv.org/abs/2510.19557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19557">https://arxiv.org/pdf/2510.19557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19557]] The Intricate Dance of Prompt Complexity, Quality, Diversity, and Consistency in T2I Models(https://arxiv.org/abs/2510.19557)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) models offer great potential for creating virtually limitless synthetic data, a valuable resource compared to fixed and finite real datasets. Previous works evaluate the utility of synthetic data from T2I models on three key desiderata: quality, diversity, and consistency. While prompt engineering is the primary means of interacting with T2I models, the systematic impact of prompt complexity on these critical utility axes remains underexplored. In this paper, we first conduct synthetic experiments to motivate the difficulty of generalization w.r.t. prompt complexity and explain the observed difficulty with theoretical derivations. Then, we introduce a new evaluation framework that can compare the utility of real data and synthetic data, and present a comprehensive analysis of how prompt complexity influences the utility of synthetic data generated by commonly used T2I models. We conduct our study across diverse datasets, including CC12M, ImageNet-1k, and DCI, and evaluate different inference-time intervention methods. Our synthetic experiments show that generalizing to more general conditions is harder than the other way round, since the former needs an estimated likelihood that is not learned by diffusion models. Our large-scale empirical experiments reveal that increasing prompt complexity results in lower conditional diversity and prompt consistency, while reducing the synthetic-to-real distribution shift, which aligns with the synthetic experiments. Moreover, current inference-time interventions can augment the diversity of the generations at the expense of moving outside the support of real data. Among those interventions, prompt expansion, by deliberately using a pre-trained language model as a likelihood estimator, consistently achieves the highest performance in both image diversity and aesthetics, even higher than that of real data.</li>
<li><strong>摘要：</strong>文本到图像 (T2I) 模型为创建几乎无限的合成数据提供了巨大的潜力，与固定和有限的真实数据集相比，这是一种宝贵的资源。之前的工作根据三个关键需求评估了 T2I 模型合成数据的效用：质量、多样性和一致性。虽然即时工程是与 T2I 模型交互的主要手段，但即时复杂性对这些关键效用轴的系统影响仍未得到充分研究。在本文中，我们首先进行综合实验来激发泛化的难度。提示复杂性并用理论推导解释观察到的困难。然后，我们引入了一种新的评估框架，可以比较真实数据和合成数据的效用，并全面分析即时复杂性如何影响常用 T2I 模型生成的合成数据的效用。我们在不同的数据集（包括 CC12M、ImageNet-1k 和 DCI）上进行研究，并评估不同的推理时间干预方法。我们的综合实验表明，推广到更一般的条件比反之更难，因为前者需要估计的可能性，而扩散模型无法学习到这一点。我们的大规模实证实验表明，增加提示复杂性会导致条件多样性和提示一致性降低，同时减少合成到真实的分布变化，这与合成实验一致。此外，当前的推理时间干预可以增加世代的多样性，但代价是脱离真实数据的支持。在这些干预措施中，通过故意使用预先训练的语言模型作为似然估计器，即时扩展在图像多样性和美观性方面始终达到最高性能，甚至高于真实数据。</li>
</ul>

<h3>Title: Pragmatic Heterogeneous Collaborative Perception via Generative Communication Mechanism</h3>
<ul>
<li><strong>Authors: </strong>Junfei Zhou, Penglin Dai, Quanmin Wei, Bingyi Liu, Xiao Wu, Jianping Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19618">https://arxiv.org/abs/2510.19618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19618">https://arxiv.org/pdf/2510.19618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19618]] Pragmatic Heterogeneous Collaborative Perception via Generative Communication Mechanism(https://arxiv.org/abs/2510.19618)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Multi-agent collaboration enhances the perception capabilities of individual agents through information sharing. However, in real-world applications, differences in sensors and models across heterogeneous agents inevitably lead to domain gaps during collaboration. Existing approaches based on adaptation and reconstruction fail to support pragmatic heterogeneous collaboration due to two key limitations: (1) Intrusive retraining of the encoder or core modules disrupts the established semantic consistency among agents; and (2) accommodating new agents incurs high computational costs, limiting scalability. To address these challenges, we present a novel Generative Communication mechanism (GenComm) that facilitates seamless perception across heterogeneous multi-agent systems through feature generation, without altering the original network, and employs lightweight numerical alignment of spatial information to efficiently integrate new agents at minimal cost. Specifically, a tailored Deformable Message Extractor is designed to extract spatial message for each collaborator, which is then transmitted in place of intermediate features. The Spatial-Aware Feature Generator, utilizing a conditional diffusion model, generates features aligned with the ego agent's semantic space while preserving the spatial information of the collaborators. These generated features are further refined by a Channel Enhancer before fusion. Experiments conducted on the OPV2V-H, DAIR-V2X and V2X-Real datasets demonstrate that GenComm outperforms existing state-of-the-art methods, achieving an 81\% reduction in both computational cost and parameter count when incorporating new agents. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>多智能体协作通过信息共享增强个体智能体的感知能力。然而，在现实应用中，异构代理之间的传感器和模型的差异不可避免地导致协作过程中的领域差距。基于适应和重构的现有方法由于两个关键限制而无法支持务实的异构协作：（1）编码器或核心模块的侵入式再训练破坏了代理之间已建立的语义一致性； (2)容纳新代理会产生高昂的计算成本，限制了可扩展性。为了应对这些挑战，我们提出了一种新颖的生成通信机制（GenComm），该机制通过特征生成促进异构多智能体系统的无缝感知，而不改变原始网络，并采用空间信息的轻量级数字对齐，以最小的成本有效地集成新智能体。具体来说，定制的可变形消息提取器旨在为每个协作者提取空间消息，然后将其代替中间特征进行传输。空间感知特征生成器利用条件扩散模型，生成与自我代理的语义空间一致的特征，同时保留协作者的空间信息。这些生成的特征在融合之前由通道增强器进一步细化。在 OPV2V-H、DAIR-V2X 和 V2X-Real 数据集上进行的实验表明，GenComm 的性能优于现有的最先进方法，在合并新代理时，计算成本和参数数量减少了 81%。我们的代码可以在这个 https URL 上找到。</li>
</ul>

<h3>Title: Learning and Simulating Building Evacuation Patterns for Enhanced Safety Design Using Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Jin Han, Zhe Zheng, Yi Gu, Jia-Rui Lin, Xin-Zheng Lu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19623">https://arxiv.org/abs/2510.19623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19623">https://arxiv.org/pdf/2510.19623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19623]] Learning and Simulating Building Evacuation Patterns for Enhanced Safety Design Using Generative Models(https://arxiv.org/abs/2510.19623)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Evacuation simulation is essential for building safety design, ensuring properly planned evacuation routes. However, traditional evacuation simulation relies heavily on refined modeling with extensive parameters, making it challenging to adopt such methods in a rapid iteration process in early design stages. Thus, this study proposes DiffEvac, a novel method to learn building evacuation patterns based on Generative Models (GMs), for efficient evacuation simulation and enhanced safety design. Initially, a dataset of 399 diverse functional layouts and corresponding evacuation heatmaps of buildings was established. Then, a decoupled feature representation is proposed to embed physical features like layouts and occupant density for GMs. Finally, a diffusion model based on image prompts is proposed to learn evacuation patterns from simulated evacuation heatmaps. Compared to existing research using Conditional GANs with RGB representation, DiffEvac achieves up to a 37.6% improvement in SSIM, 142% in PSNR, and delivers results 16 times faster, thereby cutting simulation time to 2 minutes. Case studies further demonstrate that the proposed method not only significantly enhances the rapid design iteration and adjustment process with efficient evacuation simulation but also offers new insights and technical pathways for future safety optimization in intelligent building design. The research implication is that the approach lowers the modeling burden, enables large-scale what-if exploration, and facilitates coupling with multi-objective design tools.</li>
<li><strong>摘要：</strong>疏散模拟对于建筑安全设计至关重要，可确保正确规划疏散路线。然而，传统的疏散模拟严重依赖于具有广泛参数的精细建模，这使得在早期设计阶段的快速迭代过程中采用此类方法具有挑战性。因此，本研究提出了 DiffEvac，这是一种基于生成模型 (GM) 学习建筑疏散模式的新方法，用于高效的疏散模拟和增强的安全设计。最初，建立了 399 个不同功能布局和相应建筑物疏散热图的数据集。然后，提出了一种解耦的特征表示来嵌入 GM 的布局和占用密度等物理特征。最后，提出了一种基于图像提示的扩散模型，从模拟的疏散热图中学习疏散模式。与使用具有 RGB 表示的条件 GAN 的现有研究相比，DiffEvac 在 SSIM 方面提高了 37.6%，在 PSNR 方面提高了 142%，并且提供结果的速度提高了 16 倍，从而将模拟时间缩短至 2 分钟。案例研究进一步表明，该方法不仅通过高效的疏散模拟显着增强了快速设计迭代和调整过程，而且为未来智能建筑设计中的安全优化提供了新的见解和技术途径。研究意义在于，该方法降低了建模负担，实现了大规模假设探索，并有利于与多目标设计工具的耦合。</li>
</ul>

<h3>Title: Matrix-Free Least Squares Solvers: Values, Gradients, and What to Do With Them</h3>
<ul>
<li><strong>Authors: </strong>Hrittik Roy, Søren Hauberg, Nicholas Krämer</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19634">https://arxiv.org/abs/2510.19634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19634">https://arxiv.org/pdf/2510.19634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19634]] Matrix-Free Least Squares Solvers: Values, Gradients, and What to Do With Them(https://arxiv.org/abs/2510.19634)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper argues that the method of least squares has significant unfulfilled potential in modern machine learning, far beyond merely being a tool for fitting linear models. To release its potential, we derive custom gradients that transform the solver into a differentiable operator, like a neural network layer, enabling many diverse applications. Empirically, we demonstrate: (i) scalability by enforcing weight sparsity on a 50 million parameter model; (ii) imposing conservativeness constraints in score-based generative models; and (iii) hyperparameter tuning of Gaussian processes based on predictive performance. By doing this, our work represents the next iteration in developing differentiable linear-algebra tools and making them widely accessible to machine learning practitioners.</li>
<li><strong>摘要：</strong>本文认为，最小二乘法在现代机器学习中具有尚未实现的巨大潜力，而不仅仅是拟合线性模型的工具。为了释放其潜力，我们派生了自定义梯度，将求解器转换为可微分运算符，例如神经网络层，从而支持许多不同的应用。根据经验，我们证明：（i）通过在 5000 万参数模型上强制权重稀疏来实现可扩展性； (ii) 在基于分数的生成模型中施加保守性约束； (iii) 基于预测性能的高斯过程的超参数调整。通过这样做，我们的工作代表了开发可微分线性代数工具并使机器学习从业者广泛使用它们的下一次迭代。</li>
</ul>

<h3>Title: From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction</h3>
<ul>
<li><strong>Authors: </strong>Zhida Zhao, Talas Fu, Yifan Wang, Lijun Wang, Huchuan Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19654">https://arxiv.org/abs/2510.19654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19654">https://arxiv.org/pdf/2510.19654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19654]] From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction(https://arxiv.org/abs/2510.19654)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite remarkable progress in driving world models, their potential for autonomous systems remains largely untapped: the world models are mostly learned for world simulation and decoupled from trajectory planning. While recent efforts aim to unify world modeling and planning in a single framework, the synergistic facilitation mechanism of world modeling for planning still requires further exploration. In this work, we introduce a new driving paradigm named Policy World Model (PWM), which not only integrates world modeling and trajectory planning within a unified architecture, but is also able to benefit planning using the learned world knowledge through the proposed action-free future state forecasting scheme. Through collaborative state-action prediction, PWM can mimic the human-like anticipatory perception, yielding more reliable planning performance. To facilitate the efficiency of video forecasting, we further introduce a dynamically enhanced parallel token generation mechanism, equipped with a context-guided tokenizer and an adaptive dynamic focal loss. Despite utilizing only front camera input, our method matches or exceeds state-of-the-art approaches that rely on multi-view and multi-modal inputs. Code and model weights will be released at this https URL.</li>
<li><strong>摘要：</strong>尽管在驱动世界模型方面取得了显着进展，但它们在自主系统方面的潜力在很大程度上尚未开发：世界模型主要是为了世界模拟而学习的，并且与轨迹规划脱钩。虽然最近的努力旨在将世界建模和规划统一在一个框架中，但世界建模与规划的协同促进机制仍需要进一步探索。在这项工作中，我们引入了一种名为策略世界模型（PWM）的新驾驶范式，它不仅将世界建模和轨迹规划集成在统一的架构中，而且还能够通过提出的无行动未来状态预测方案，利用学到的世界知识来进行规划。通过协作状态动作预测，PWM 可以模仿类人的预期感知，从而产生更可靠的规划性能。为了提高视频预测的效率，我们进一步引入了动态增强的并行标记生成机制，配备了上下文引导标记器和自适应动态焦点损失。尽管仅利用前置摄像头输入，但我们的方法匹配或超过了依赖多视图和多模式输入的最先进方法。代码和模型权重将在此 https URL 发布。</li>
</ul>

<h3>Title: Fast Inference via Hierarchical Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Amir Globerson, Haim Kaplan, Yishay Mansour, Clara Mohri, Tal Schuster</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19705">https://arxiv.org/abs/2510.19705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19705">https://arxiv.org/pdf/2510.19705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19705]] Fast Inference via Hierarchical Speculative Decoding(https://arxiv.org/abs/2510.19705)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Transformer language models generate text autoregressively, making inference latency proportional to the number of tokens generated. Speculative decoding reduces this latency without sacrificing output quality, by leveraging a small draft model to propose tokens that the larger target model verifies in parallel. In practice, however, there may exist a set of potential draft models- ranging from faster but less inaccurate, to slower yet more reliable. We introduce Hierarchical Speculative Decoding (HSD), an algorithm that stacks these draft models into a hierarchy, where each model proposes tokens, and the next larger model verifies them in a single forward pass, until finally the target model verifies tokens. We derive an expression for the expected latency of any such hierarchy and show that selecting the latency-optimal hierarchy can be done in polynomial time. Empirically, HSD gives up to 1.2x speed-up over the best single-draft baseline, demonstrating the practicality of our algorithm in reducing generation latency beyond previous techniques.</li>
<li><strong>摘要：</strong>Transformer 语言模型自回归生成文本，使推理延迟与生成的标记数量成正比。推测性解码通过利用小型草稿模型来提出由较大目标模型并行验证的令牌，从而在不牺牲输出质量的情况下减少这种延迟。然而，在实践中，可能存在一组潜在的草案模型——从更快但不那么不准确，到更慢但更可靠。我们引入了分层推测解码（HSD），这是一种将这些草稿模型堆叠到一个层次结构中的算法，其中每个模型提出令牌，下一个更大的模型在单个前向传递中验证它们，直到最终目标模型验证令牌。我们推导了任何此类层次结构的预期延迟的表达式，并表明可以在多项式时间内完成选择延迟最优层次结构。根据经验，HSD 比最佳单草稿基线提高了 1.2 倍的速度，这证明了我们的算法在减少生成延迟方面比以前的技术更实用。</li>
</ul>

<h3>Title: CONFEX: Uncertainty-Aware Counterfactual Explanations with Conformal Guarantees</h3>
<ul>
<li><strong>Authors: </strong>Aman Bilkhoo (1), Milad Kazemi (1), Nicola Paoletti (1), Mehran Hosseini (1 and 2) ((1) Department of Informatics, King's College London, (2) Department of Computer Science, University of Manchester)</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19754">https://arxiv.org/abs/2510.19754</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19754">https://arxiv.org/pdf/2510.19754</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19754]] CONFEX: Uncertainty-Aware Counterfactual Explanations with Conformal Guarantees(https://arxiv.org/abs/2510.19754)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Counterfactual explanations (CFXs) provide human-understandable justifications for model predictions, enabling actionable recourse and enhancing interpretability. To be reliable, CFXs must avoid regions of high predictive uncertainty, where explanations may be misleading or inapplicable. However, existing methods often neglect uncertainty or lack principled mechanisms for incorporating it with formal guarantees. We propose CONFEX, a novel method for generating uncertainty-aware counterfactual explanations using Conformal Prediction (CP) and Mixed-Integer Linear Programming (MILP). CONFEX explanations are designed to provide local coverage guarantees, addressing the issue that CFX generation violates exchangeability. To do so, we develop a novel localised CP procedure that enjoys an efficient MILP encoding by leveraging an offline tree-based partitioning of the input space. This way, CONFEX generates CFXs with rigorous guarantees on both predictive uncertainty and optimality. We evaluate CONFEX against state-of-the-art methods across diverse benchmarks and metrics, demonstrating that our uncertainty-aware approach yields robust and plausible explanations.</li>
<li><strong>摘要：</strong>反事实解释 (CFX) 为模型预测提供人类可理解的理由，从而实现可操作的追索并增强可解释性。为了可靠，CFX 必须避开预测高度不确定的区域，这些区域的解释可能具有误导性或不适用。然而，现有方法往往忽视不确定性或缺乏将其与正式保证结合起来的原则性机制。我们提出了 CONFEX，这是一种使用保形预测（CP）和混合整数线性规划（MILP）生成不确定性反事实解释的新方法。 CONFEX解释旨在提供本地覆盖保证，解决CFX生成违反可交换性的问题。为此，我们开发了一种新颖的本地化 CP 程序，该程序通过利用输入空间的离线基于树的分区来实现高效的 MILP 编码。通过这种方式，CONFEX 生成的 CFX 可以严格保证预测的不确定性和最优性。我们根据不同基准和指标的最先进方法评估 CONFEX，证明我们的不确定性感知方法可以产生可靠且合理的解释。</li>
</ul>

<h3>Title: A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Liu, Xinyu Wang, Yuqi Lin, Zhikai Wang, Peiru Wang, Peiliang Cai, Qinming Zhou, Zhengan Yan, Zexuan Yan, Zhengyi Shi, Chang Zou, Yue Ma, Linfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19755">https://arxiv.org/abs/2510.19755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19755">https://arxiv.org/pdf/2510.19755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19755]] A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation(https://arxiv.org/abs/2510.19755)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion Models have become a cornerstone of modern generative AI for their exceptional generation quality and controllability. However, their inherent \textit{multi-step iterations} and \textit{complex backbone networks} lead to prohibitive computational overhead and generation latency, forming a major bottleneck for real-time applications. Although existing acceleration techniques have made progress, they still face challenges such as limited applicability, high training costs, or quality degradation. Against this backdrop, \textbf{Diffusion Caching} offers a promising training-free, architecture-agnostic, and efficient inference paradigm. Its core mechanism identifies and reuses intrinsic computational redundancies in the diffusion process. By enabling feature-level cross-step reuse and inter-layer scheduling, it reduces computation without modifying model parameters. This paper systematically reviews the theoretical foundations and evolution of Diffusion Caching and proposes a unified framework for its classification and analysis. Through comparative analysis of representative methods, we show that Diffusion Caching evolves from \textit{static reuse} to \textit{dynamic prediction}. This trend enhances caching flexibility across diverse tasks and enables integration with other acceleration techniques such as sampling optimization and model distillation, paving the way for a unified, efficient inference framework for future multimodal and interactive applications. We argue that this paradigm will become a key enabler of real-time and efficient generative AI, injecting new vitality into both theory and practice of \textit{Efficient Generative Intelligence}.</li>
<li><strong>摘要：</strong>扩散模型因其卓越的生成质量和可控性而成为现代生成人工智能的基石。然而，它们固有的\textit{多步迭代}和\textit{复杂骨干网络}导致过高的计算开销和生成延迟，形成实时应用的主要瓶颈。尽管现有的加速技术取得了进步，但它们仍然面临着适用性有限、训练成本高或质量下降等挑战。在此背景下，\textbf{扩散缓存}提供了一种有前景的免训练、与架构无关且高效的推理范式。其核心机制识别并重用扩散过程中固有的计算冗余。通过启用特征级跨步骤重用和层间调度，可以在不修改模型参数的情况下减少计算量。本文系统回顾了扩散缓存的理论基础和演变，并提出了统一的分类和分析框架。通过对代表性方法的比较分析，我们表明扩散缓存从\textit{静态重用}演变为\textit{动态预测}。这一趋势增强了跨不同任务的缓存灵活性，并能够与其他加速技术（例如采样优化和模型蒸馏）集成，为未来多模式和交互式应用程序的统一、高效的推理框架铺平了道路。我们认为，这种范式将成为实时高效生成人工智能的关键推动者，为 \textit{高效生成智能} 的理论和实践注入新的活力。</li>
</ul>

<h3>Title: OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation</h3>
<ul>
<li><strong>Authors: </strong>Guowei Xu, Yuxuan Bian, Ailing Zeng, Mingyi Shi, Shaoli Huang, Wen Li, Lixin Duan, Qiang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19789">https://arxiv.org/abs/2510.19789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19789">https://arxiv.org/pdf/2510.19789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19789]] OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation(https://arxiv.org/abs/2510.19789)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This paper introduces OmniMotion-X, a versatile multimodal framework for whole-body human motion generation, leveraging an autoregressive diffusion transformer in a unified sequence-to-sequence manner. OmniMotion-X efficiently supports diverse multimodal tasks, including text-to-motion, music-to-dance, speech-to-gesture, and global spatial-temporal control scenarios (e.g., motion prediction, in-betweening, completion, and joint/trajectory-guided synthesis), as well as flexible combinations of these tasks. Specifically, we propose the use of reference motion as a novel conditioning signal, substantially enhancing the consistency of generated content, style, and temporal dynamics crucial for realistic animations. To handle multimodal conflicts, we introduce a progressive weak-to-strong mixed-condition training strategy. To enable high-quality multimodal training, we construct OmniMoCap-X, the largest unified multimodal motion dataset to date, integrating 28 publicly available MoCap sources across 10 distinct tasks, standardized to the SMPL-X format at 30 fps. To ensure detailed and consistent annotations, we render sequences into videos and use GPT-4o to automatically generate structured and hierarchical captions, capturing both low-level actions and high-level semantics. Extensive experimental evaluations confirm that OmniMotion-X significantly surpasses existing methods, demonstrating state-of-the-art performance across multiple multimodal tasks and enabling the interactive generation of realistic, coherent, and controllable long-duration motions.</li>
<li><strong>摘要：</strong>本文介绍了 OmniMotion-X，这是一种用于全身人体运动生成的多功能多模态框架，以统一的序列到序列的方式利用自回归扩散变压器。 OmniMotion-X 有效支持各种多模态任务，包括文本到运动、音乐到舞蹈、语音到手势和全局时空控制场景（例如运动预测、中间、完成和关节/轨迹引导合成），以及这些任务的灵活组合。具体来说，我们建议使用参考运动作为一种新颖的调节信号，从而大大增强对逼真动画至关重要的生成内容、风格和时间动态的一致性。为了处理多模式冲突，我们引入了渐进的从弱到强的混合条件训练策略。为了实现高质量的多模态训练，我们构建了 OmniMoCap-X，这是迄今为止最大的统一多模态运动数据集，集成了 10 个不同任务的 28 个公开可用的 MoCap 源，标准化为 30 fps 的 SMPL-X 格式。为了确保详细且一致的注释，我们将序列渲染到视频中，并使用 GPT-4o 自动生成结构化和分层字幕，捕获低级动作和高级语义。广泛的实验评估证实，OmniMotion-X 显着超越了现有方法，在多个多模态任务中展示了最先进的性能，并能够交互式生成真实、连贯且可控的长时间运动。</li>
</ul>

<h3>Title: The Feasibility of Training Sovereign Language Models in the Global South: A Study of Brazil and Mexico</h3>
<ul>
<li><strong>Authors: </strong>Sandra Malagon (1 and 2), Monica A. Ulloa Ruiz (1 and 2), Tatiana Elizabeth Sandoval Plaza (1), Gabriel Rafael Rosario Bolívar (1), Valentina García Mesa (1), Ivanna Alvarado Morales (1) ((1) Carreras con Impacto, (2) AIxo)</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19801">https://arxiv.org/abs/2510.19801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19801">https://arxiv.org/pdf/2510.19801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19801]] The Feasibility of Training Sovereign Language Models in the Global South: A Study of Brazil and Mexico(https://arxiv.org/abs/2510.19801)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The rapid escalation of computational requirements for training large-scale language models has reinforced structural asymmetries between high-capacity jurisdictions and countries in the Global South. This paper examines the technical and fiscal feasibility of sovereign-scale language model training in Brazil and Mexico under conditions of constrained hardware access, energy availability, and fiscal ceilings. Using a dual-axis design that varies accelerator generation (NVIDIA H100 vs. A100) and training duration (90 vs. 150 days), we estimate compute demand, energy consumption, capital expenditures, and regulatory compatibility for the training of a 10-trillion-token model. Our findings show that while all configurations remain below export-control and electrical infrastructure thresholds, fiscal viability is determined by hardware efficiency. H100-based scenarios achieve training feasibility at a total cost of 8-14 million USD, while A100 deployments require 19-32 million USD due to higher energy and hardware demand. We argue that extending training timelines should be treated as a policy lever to mitigate hardware constraints, enabling the production of usable, auditable, and locally aligned models without competing at the global frontier. This study contributes to the discourse on AI compute governance and technological sovereignty by highlighting context-sensitive strategies that allow middle-income countries to establish sustainable and strategically sufficient AI capabilities.</li>
<li><strong>摘要：</strong>训练大规模语言模型的计算要求迅速上升，加剧了高容量管辖区和南半球国家之间的结构不对称。本文研究了在硬件访问、能源可用性和财政上限有限的条件下，巴西和墨西哥主权规模语言模型训练的技术和财政可行性。使用不同加速器代（NVIDIA H100 与 A100）和训练持续时间（90 天与 150 天）的双轴设计，我们估算了 10 万亿代币模型训练的计算需求、能源消耗、资本支出和监管兼容性。我们的研究结果表明，虽然所有配置仍低于出口管制和电力基础设施阈值，但财政可行性取决于硬件效率。基于H100的场景实现训练可行性的总成本为8-1400万美元，而A100部署由于更高的能源和硬件需求而需要19-3200万美元。我们认为，延长培训时间应被视为缓解硬件限制的政策杠杆，从而能够生产可用、可审计和本地一致的模型，而无需在全球前沿竞争。这项研究通过强调允许中等收入国家建立可持续且战略上充足的人工智能能力的环境敏感战略，为有关人工智能计算治理和技术主权的讨论做出了贡献。</li>
</ul>

<h3>Title: Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Yusu Qian, Eli Bocek-Rivele, Liangchen Song, Jialing Tong, Yinfei Yang, Jiasen Lu, Wenze Hu, Zhe Gan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19808">https://arxiv.org/abs/2510.19808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19808">https://arxiv.org/pdf/2510.19808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19808]] Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing(https://arxiv.org/abs/2510.19808)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in multimodal models have demonstrated remarkable text-guided image editing capabilities, with systems like GPT-4o and Nano-Banana setting new benchmarks. However, the research community's progress remains constrained by the absence of large-scale, high-quality, and openly accessible datasets built from real images. We introduce Pico-Banana-400K, a comprehensive 400K-image dataset for instruction-based image editing. Our dataset is constructed by leveraging Nano-Banana to generate diverse edit pairs from real photographs in the OpenImages collection. What distinguishes Pico-Banana-400K from previous synthetic datasets is our systematic approach to quality and diversity. We employ a fine-grained image editing taxonomy to ensure comprehensive coverage of edit types while maintaining precise content preservation and instruction faithfulness through MLLM-based quality scoring and careful curation. Beyond single turn editing, Pico-Banana-400K enables research into complex editing scenarios. The dataset includes three specialized subsets: (1) a 72K-example multi-turn collection for studying sequential editing, reasoning, and planning across consecutive modifications; (2) a 56K-example preference subset for alignment research and reward model training; and (3) paired long-short editing instructions for developing instruction rewriting and summarization capabilities. By providing this large-scale, high-quality, and task-rich resource, Pico-Banana-400K establishes a robust foundation for training and benchmarking the next generation of text-guided image editing models.</li>
<li><strong>摘要：</strong>多模态模型的最新进展展示了卓越的文本引导图像编辑功能，GPT-4o 和 Nano-Banana 等系统树立了新的基准。然而，由于缺乏根据真实图像构建的大规模、高质量且可公开访问的数据集，研究界的进展仍然受到限制。我们推出了 Pico-Banana-400K，这是一个用于基于指令的图像编辑的综合 400K 图像数据集。我们的数据集是通过利用 Nano-Banana 从 OpenImages 集合中的真实照片生成不同的编辑对来构建的。 Pico-Banana-400K 与以前的合成数据集的区别在于我们对质量和多样性的系统方法。我们采用细粒度的图像编辑分类法来确保编辑类型的全面覆盖，同时通过基于 MLLM 的质量评分和精心策划来保持精确的内容保存和指令忠实度。除了单轮编辑之外，Pico-Banana-400K 还可以研究复杂的编辑场景。该数据集包括三个专门的子集：（1）一个 72K 示例的多轮集合，用于研究连续修改的顺序编辑、推理和规划； (2) 用于一致性研究和奖励模型训练的 56K 示例偏好子集； (3) 成对的长短编辑指令，用于开发指令重写和摘要能力。通过提供这种大规模、高质量和任务丰富的资源，Pico-Banana-400K 为下一代文本引导图像编辑模型的训练和基准测试奠定了坚实的基础。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
