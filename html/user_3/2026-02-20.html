<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-02-20</h1>
<h3>Title: A Few-Shot LLM Framework for Extreme Day Classification in Electricity Markets</h3>
<ul>
<li><strong>Authors: </strong>Saud Alghumayjan, Ming Yi, Bolun Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16735">https://arxiv.org/abs/2602.16735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16735">https://arxiv.org/pdf/2602.16735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16735]] A Few-Shot LLM Framework for Extreme Day Classification in Electricity Markets(https://arxiv.org/abs/2602.16735)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This paper proposes a few-shot classification framework based on Large Language Models (LLMs) to predict whether the next day will have spikes in real-time electricity prices. The approach aggregates system state information, including electricity demand, renewable generation, weather forecasts, and recent electricity prices, into a set of statistical features that are formatted as natural-language prompts and fed to an LLM along with general instructions. The model then determines the likelihood that the next day would be a spike day and reports a confidence score. Using historical data from the Texas electricity market, we demonstrate that this few-shot approach achieves performance comparable to supervised machine learning models, such as Support Vector Machines and XGBoost, and outperforms the latter two when limited historical data are available. These findings highlight the potential of LLMs as a data-efficient tool for classifying electricity price spikes in settings with scarce data.</li>
<li><strong>摘要：</strong>本文提出了一种基于大型语言模型（LLM）的小样本分类框架，以预测第二天是否会出现实时电价峰值。该方法将系统状态信息（包括电力需求、可再生能源发电、天气预报和最近的电价）聚合成一组统计特征，这些特征被格式化为自然语言提示，并与一般说明一起提供给法学硕士。然后，该模型确定第二天是峰值日的可能性并报告置信度得分。使用德克萨斯州电力市场的历史数据，我们证明这种少量方法的性能可与支持向量机和 XGBoost 等监督机器学习模型相媲美，并且在可用历史数据有限时优于后两者。这些发现凸显了法学硕士作为一种数据高效工具的潜力，可以在数据稀缺的环境中对电价峰值进行分类。</li>
</ul>

<h3>Title: Efficient Tail-Aware Generative Optimization via Flow Model Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Zifan Wang, Riccardo De Santi, Xiaoyu Mo, Michael M. Zavlanos, Andreas Krause, Karl H. Johansson</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16796">https://arxiv.org/abs/2602.16796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16796">https://arxiv.org/pdf/2602.16796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16796]] Efficient Tail-Aware Generative Optimization via Flow Model Fine-Tuning(https://arxiv.org/abs/2602.16796)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Fine-tuning pre-trained diffusion and flow models to optimize downstream utilities is central to real-world deployment. Existing entropy-regularized methods primarily maximize expected reward, providing no mechanism to shape tail behavior. However, tail control is often essential: the lower tail determines reliability by limiting low-reward failures, while the upper tail enables discovery by prioritizing rare, high-reward outcomes. In this work, we present Tail-aware Flow Fine-Tuning (TFFT), a principled and efficient distributional fine-tuning algorithm based on the Conditional Value-at-Risk (CVaR). We address two distinct tail-shaping goals: right-CVaR for seeking novel samples in the high-reward tail and left-CVaR for controlling worst-case samples in the low-reward tail. Unlike prior approaches that rely on non-linear optimization, we leverage the variational dual formulation of CVaR to decompose it into a decoupled two-stage procedure: a lightweight one-dimensional threshold optimization step, and a single entropy-regularized fine-tuning process via a specific pseudo-reward. This decomposition achieves CVaR fine-tuning efficiently with computational cost comparable to standard expected fine-tuning methods. We demonstrate the effectiveness of TFFT across illustrative experiments, high-dimensional text-to-image generation, and molecular design.</li>
<li><strong>摘要：</strong>微调预先训练的扩散和流动模型以优化下游公用设施是实际部署的核心。现有的熵正则化方法主要是最大化预期奖励，没有提供塑造尾部行为的机制。然而，尾部控制通常是必不可少的：下尾部通过限制低回报失败来确定可靠性，而上尾部通过优先考虑罕见的高回报结果来实现发现。在这项工作中，我们提出了尾部感知流微调（TFFT），这是一种基于条件风险价值（CVaR）的原则性且高效的分布式微调算法。我们解决了两个不同的尾部整形目标：右CVaR用于在高奖励尾部寻找新样本，左CVaR用于控制低奖励尾部的最坏情况样本。与之前依赖非线性优化的方法不同，我们利用 CVaR 的变分对偶公式将其分解为解耦的两阶段过程：轻量级一维阈值优化步骤，以及通过特定伪奖励的单个熵正则化微调过程。这种分解有效地实现了 CVaR 微调，其计算成本与标准预期微调方法相当。我们通过说明性实验、高维文本到图像生成和分子设计证明了 TFFT 的有效性。</li>
</ul>

<h3>Title: Analytic Score Optimization for Multi Dimension Video Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Boda Lin, Yongjie Zhu, Wenyu Qin, Meng Wang, Pengfei Wan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16856">https://arxiv.org/abs/2602.16856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16856">https://arxiv.org/pdf/2602.16856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16856]] Analytic Score Optimization for Multi Dimension Video Quality Assessment(https://arxiv.org/abs/2602.16856)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Video Quality Assessment (VQA) is evolving beyond single-number mean opinion score toward richer, multi-faceted evaluations of video content. In this paper, we present a large-scale multi-dimensional VQA dataset UltraVQA that encompasses diverse User-Generated Content~(UGC) annotated across five key quality dimensions: Motion Quality, Motion Amplitude, Aesthetic Quality, Content Quality, and Clarity Quality. Each video in our dataset is scored by over 3 human raters on these dimensions, with fine-grained sub-attribute labels, and accompanied by an explanatory rationale generated by GPT based on the collective human judgments. To better leverage these rich annotations and improve discrete quality score assessment, we introduce Analytic Score Optimization (ASO), a theoretically grounded post-training objective derived for multi-dimensional VQA. By reframing quality assessment as a regularized decision-making process, we obtain a closed-form solution that naturally captures the ordinal nature of human ratings, ensuring alignment with human ranking preferences. In experiments, our method outperforms most baselines including closed-source APIs and open-source models, while also reducing mean absolute error (MAE) in quality prediction. Our work highlights the importance of multi-dimensional, interpretable annotations and reinforcement-based alignment in advancing video quality assessment.</li>
<li><strong>摘要：</strong>视频质量评估 (VQA) 正在超越单一数字平均意见评分，向更丰富、多方面的视频内容评估发展。在本文中，我们提出了一个大规模多维 VQA 数据集 UltraVQA，其中包含不同的用户生成内容（UGC），并在五个关键质量维度上进行注释：运动质量、运动幅度、审美质量、内容质量和清晰度质量。我们数据集中的每个视频均由超过 3 位人类评分者在这些维度上进行评分，并带有细粒度的子属性标签，并附有 GPT 基于人类集体判断生成的解释性原理。为了更好地利用这些丰富的注释并改进离散质量分数评估，我们引入了分析分数优化（ASO），这是一种基于多维 VQA 的理论基础训练后目标。通过将质量评估重新定义为规范化的决策过程，我们获得了一个封闭式解决方案，可以自然地捕捉人类评级的顺序性质，确保与人类排名偏好保持一致。在实验中，我们的方法优于大多数基线，包括闭源 API 和开源模型，同时还减少了质量预测中的平均绝对误差 (MAE)。我们的工作强调了多维、可解释的注释和基于强化的对齐在推进视频质量评估中的重要性。</li>
</ul>

<h3>Title: Position: Why a Dynamical Systems Perspective is Needed to Advance Time Series Modeling</h3>
<ul>
<li><strong>Authors: </strong>Daniel Durstewitz, Christoph Jürgen Hemmer, Florian Hess, Charlotte Ricarda Doll, Lukas Eisenmann</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16864">https://arxiv.org/abs/2602.16864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16864">https://arxiv.org/pdf/2602.16864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16864]] Position: Why a Dynamical Systems Perspective is Needed to Advance Time Series Modeling(https://arxiv.org/abs/2602.16864)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Time series (TS) modeling has come a long way from early statistical, mainly linear, approaches to the current trend in TS foundation models. With a lot of hype and industrial demand in this field, it is not always clear how much progress there really is. To advance TS forecasting and analysis to the next level, here we argue that the field needs a dynamical systems (DS) perspective. TS of observations from natural or engineered systems almost always originate from some underlying DS, and arguably access to its governing equations would yield theoretically optimal forecasts. This is the promise of DS reconstruction (DSR), a class of ML/AI approaches that aim to infer surrogate models of the underlying DS from data. But models based on DS principles offer other profound advantages: Beyond short-term forecasts, they enable to predict the long-term statistics of an observed system, which in many practical scenarios may be the more relevant quantities. DS theory furthermore provides domain-independent theoretical insight into mechanisms underlying TS generation, and thereby will inform us, e.g., about upper bounds on performance of any TS model, generalization into unseen regimes as in tipping points, or potential control strategies. After reviewing some of the central concepts, methods, measures, and models in DS theory and DSR, we will discuss how insights from this field can advance TS modeling in crucial ways, enabling better forecasting with much lower computational and memory footprints. We conclude with a number of specific suggestions for translating insights from DSR into TS modeling.</li>
<li><strong>摘要：</strong>时间序列 (TS) 建模从早期的统计（主要是线性）方法到 TS 基础模型的当前趋势，已经走过了很长的一段路。由于该领域有大量的炒作和工业需求，但并不总是清楚到底有多少进展。为了将 TS 预测和分析提升到一个新的水平，我们认为该领域需要动力系统 (DS) 的视角。 TS of observations from natural or engineered systems almost always originate from some underlying DS, and arguably access to its governing equations would yield theoretically optimal forecasts.这是 DS 重建 (DSR) 的承诺，DSR 是一类 ML/AI 方法，旨在从数据中推断出底层 DS 的替代模型。 But models based on DS principles offer other profound advantages: Beyond short-term forecasts, they enable to predict the long-term statistics of an observed system, which in many practical scenarios may be the more relevant quantities. DS theory furthermore provides domain-independent theoretical insight into mechanisms underlying TS generation, and thereby will inform us, e.g., about upper bounds on performance of any TS model, generalization into unseen regimes as in tipping points, or potential control strategies. After reviewing some of the central concepts, methods, measures, and models in DS theory and DSR, we will discuss how insights from this field can advance TS modeling in crucial ways, enabling better forecasting with much lower computational and memory footprints.最后，我们提出了一些将 DSR 见解转化为 TS 建模的具体建议。</li>
</ul>

<h3>Title: DODO: Discrete OCR Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Sean Man, Roy Ganz, Roi Ronen, Shahar Tsiper, Shai Mazor, Niv Nayman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16872">https://arxiv.org/abs/2602.16872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16872">https://arxiv.org/pdf/2602.16872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16872]] DODO: Discrete OCR Diffusion Models(https://arxiv.org/abs/2602.16872)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Optical Character Recognition (OCR) is a fundamental task for digitizing information, serving as a critical bridge between visual data and textual understanding. While modern Vision-Language Models (VLM) have achieved high accuracy in this domain, they predominantly rely on autoregressive decoding, which becomes computationally expensive and slow for long documents as it requires a sequential forward pass for every generated token. We identify a key opportunity to overcome this bottleneck: unlike open-ended generation, OCR is a highly deterministic task where the visual input strictly dictates a unique output sequence, theoretically enabling efficient, parallel decoding via diffusion models. However, we show that existing masked diffusion models fail to harness this potential; those introduce structural instabilities that are benign in flexible tasks, like captioning, but catastrophic for the rigid, exact-match requirements of OCR. To bridge this gap, we introduce DODO, the first VLM to utilize block discrete diffusion and unlock its speedup potential for OCR. By decomposing generation into blocks, DODO mitigates the synchronization errors of global diffusion. Empirically, our method achieves near state-of-the-art accuracy while enabling up to 3x faster inference compared to autoregressive baselines.</li>
<li><strong>摘要：</strong>光学字符识别 (OCR) 是信息数字化的一项基本任务，是视觉数据和文本理解之间的重要桥梁。虽然现代视觉语言模型 (VLM) 在此领域实现了高精度，但它们主要依赖于自回归解码，这对于长文档来说计算成本高昂且速度缓慢，因为它需要对每个生成的令牌进行顺序前向传递。我们找到了克服这一瓶颈的关键机会：与开放式生成不同，OCR 是一项高度确定性的任务，其中视觉输入严格规定唯一的输出序列，理论上可以通过扩散模型实现高效、并行的解码。然而，我们表明现有的掩蔽扩散模型无法利用这种潜力；这些引入的结构不稳定性对于字幕等灵活任务来说是良性的，但对于 OCR 严格的、精确匹配的要求来说却是灾难性的。为了弥补这一差距，我们引入了 DODO，这是第一个利用块离散扩散并释放其 OCR 加速潜力的 VLM。通过将生成分解为区块，DODO 减轻了全局扩散的同步错误。根据经验，我们的方法实现了接近最先进的精度，同时与自回归基线相比，推理速度提高了 3 倍。</li>
</ul>

<h3>Title: StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Ren, Xiang Li, Yiran Wang, Zeyu Zhang, Hao Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16915">https://arxiv.org/abs/2602.16915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16915">https://arxiv.org/pdf/2602.16915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16915]] StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation(https://arxiv.org/abs/2602.16915)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Stereo depth estimation is fundamental to underwater robotic perception, yet suffers from severe domain shifts caused by wavelength-dependent light attenuation, scattering, and refraction. Recent approaches leverage monocular foundation models with GRU-based iterative refinement for underwater adaptation; however, the sequential gating and local convolutional kernels in GRUs necessitate multiple iterations for long-range disparity propagation, limiting performance in large-disparity and textureless underwater regions. In this paper, we propose StereoAdapter-2, which replaces the conventional ConvGRU updater with a novel ConvSS2D operator based on selective state space models. The proposed operator employs a four-directional scanning strategy that naturally aligns with epipolar geometry while capturing vertical structural consistency, enabling efficient long-range spatial propagation within a single update step at linear computational complexity. Furthermore, we construct UW-StereoDepth-80K, a large-scale synthetic underwater stereo dataset featuring diverse baselines, attenuation coefficients, and scattering parameters through a two-stage generative pipeline combining semantic-aware style transfer and geometry-consistent novel view synthesis. Combined with dynamic LoRA adaptation inherited from StereoAdapter, our framework achieves state-of-the-art zero-shot performance on underwater benchmarks with 17% improvement on TartanAir-UW and 7.2% improvment on SQUID, with real-world validation on the BlueROV2 platform demonstrates the robustness of our approach. Code: this https URL. Website: this https URL.</li>
<li><strong>摘要：</strong>立体深度估计是水下机器人感知的基础，但由于波长相关的光衰减、散射和折射而导致严重的域偏移。最近的方法利用单目基础模型和基于 GRU 的迭代细化来实现水下适应；然而，GRU 中的顺序门控和局部卷积核需要多次迭代才能实现长距离视差传播，从而限制了大视差和无纹理水下区域的性能。在本文中，我们提出了 StereoAdapter-2，它用基于选择性状态空间模型的新型 ConvSS2D 算子取代了传统的 ConvGRU 更新器。所提出的算子采用四向扫描策略，该策略自然地与对极几何结构对齐，同时捕获垂直结构一致性，从而以线性计算复杂度在单个更新步骤内实现高效的远程空间传播。此外，我们通过结合语义感知风格传输和几何一致的新颖视图合成的两阶段生成管道构建了 UW-StereoDepth-80K，这是一个大规模合成水下立体数据集，具有不同的基线、衰减系数和散射参数。结合继承自 StereoAdapter 的动态 LoRA 自适应，我们的框架在水下基准测试中实现了最先进的零样本性能，在 TartanAir-UW 上提高了 17%，在 SQUID 上提高了 7.2%，在 BlueROV2 平台上的实际验证证明了我们方法的稳健性。代码：此 https URL。网站：此 https URL。</li>
</ul>

<h3>Title: Neural Proposals, Symbolic Guarantees: Neuro-Symbolic Graph Generation with Hard Constraints</h3>
<ul>
<li><strong>Authors: </strong>Chuqin Geng, Li Zhang, Mark Zhang, Haolin Ye, Ziyu Zhao, Xujie Si</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16954">https://arxiv.org/abs/2602.16954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16954">https://arxiv.org/pdf/2602.16954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16954]] Neural Proposals, Symbolic Guarantees: Neuro-Symbolic Graph Generation with Hard Constraints(https://arxiv.org/abs/2602.16954)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We challenge black-box purely deep neural approaches for molecules and graph generation, which are limited in controllability and lack formal guarantees. We introduce Neuro-Symbolic Graph Generative Modeling (NSGGM), a neurosymbolic framework that reapproaches molecule generation as a scaffold and interaction learning task with symbolic assembly. An autoregressive neural model proposes scaffolds and refines interaction signals, and a CPU-efficient SMT solver constructs full graphs while enforcing chemical validity, structural rules, and user-specific constraints, yielding molecules that are correct by construction and interpretable control that pure neural methods cannot provide. NSGGM delivers strong performance on both unconstrained generation and constrained generation tasks, demonstrating that neuro-symbolic modeling can match state-of-the-art generative performance while offering explicit controllability and guarantees. To evaluate more nuanced controllability, we also introduce a Logical-Constraint Molecular Benchmark, designed to test strict hard-rule satisfaction in workflows that require explicit, interpretable specifications together with verifiable compliance.</li>
<li><strong>摘要：</strong>我们挑战用于分子和图形生成的黑盒纯深度神经方法，这些方法的可控性有限并且缺乏正式的保证。我们介绍了神经符号图生成模型（NSGGM），这是一种神经符号框架，它重新将分子生成作为支架和与符号组装的交互学习任务。自回归神经模型提出支架并细化交互信号，CPU高效的SMT求解器构建完整的图，同时强制化学有效性、结构规则和用户特定的约束，产生通过构建和可解释控制正确的分子，这是纯神经方法无法提供的。 NSGGM 在无约束生成和约束生成任务上均提供了强大的性能，证明神经符号建模可以与最先进的生成性能相匹配，同时提供明确的可控性和保证。为了评估更细致的可控性，我们还引入了逻辑约束分子基准，旨在测试工作流程中严格的硬规则满意度，这些工作流程需要明确的、可解释的规范以及可验证的合规性。</li>
</ul>

<h3>Title: DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Dahye Kim, Deepti Ghadiyaram, Raghudeep Gadde</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16968">https://arxiv.org/abs/2602.16968</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16968">https://arxiv.org/pdf/2602.16968</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16968]] DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers(https://arxiv.org/abs/2602.16968)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion Transformers (DiTs) have achieved state-of-the-art performance in image and video generation, but their success comes at the cost of heavy computation. This inefficiency is largely due to the fixed tokenization process, which uses constant-sized patches throughout the entire denoising phase, regardless of the content's complexity. We propose dynamic tokenization, an efficient test-time strategy that varies patch sizes based on content complexity and the denoising timestep. Our key insight is that early timesteps only require coarser patches to model global structure, while later iterations demand finer (smaller-sized) patches to refine local details. During inference, our method dynamically reallocates patch sizes across denoising steps for image and video generation and substantially reduces cost while preserving perceptual generation quality. Extensive experiments demonstrate the effectiveness of our approach: it achieves up to $3.52\times$ and $3.2\times$ speedup on this http URL and Wan $2.1$, respectively, without compromising the generation quality and prompt adherence.</li>
<li><strong>摘要：</strong>扩散变压器 (DiT) 在图像和视频生成方面取得了最先进的性能，但它们的成功是以大量计算为代价的。这种低效率很大程度上是由于固定标记化过程造成的，该过程在整个去噪阶段都使用恒定大小的补丁，无论内容的复杂性如何。我们提出了动态标记化，这是一种有效的测试时间策略，可以根据内容复杂性和去噪时间步长来改变补丁大小。我们的主要见解是，早期的时间步长仅需要较粗糙的补丁来建模全局结构，而后期的迭代则需要更精细（较小尺寸）的补丁来细化局部细节。在推理过程中，我们的方法在图像和视频生成的去噪步骤中动态地重新分配补丁大小，并在保持感知生成质量的同时显着降低成本。大量的实验证明了我们方法的有效性：它在此 http URL 和 Wan 上分别实现了高达 $3.52\times$ 和 $3.2\times$ 的加速，而不会影响生成质量和提示依从性。</li>
</ul>

<h3>Title: Fail-Closed Alignment for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zachary Coalson, Beth Sohler, Aiden Gabriel, Sanghyun Hong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16977">https://arxiv.org/abs/2602.16977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16977">https://arxiv.org/pdf/2602.16977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16977]] Fail-Closed Alignment for Large Language Models(https://arxiv.org/abs/2602.16977)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We identify a structural weakness in current large language model (LLM) alignment: modern refusal mechanisms are fail-open. While existing approaches encode refusal behaviors across multiple latent features, suppressing a single dominant feature$-$via prompt-based jailbreaks$-$can cause alignment to collapse, leading to unsafe generation. Motivated by this, we propose fail-closed alignment as a design principle for robust LLM safety: refusal mechanisms should remain effective even under partial failures via redundant, independent causal pathways. We present a concrete instantiation of this principle: a progressive alignment framework that iteratively identifies and ablates previously learned refusal directions, forcing the model to reconstruct safety along new, independent subspaces. Across four jailbreak attacks, we achieve the strongest overall robustness while mitigating over-refusal and preserving generation quality, with small computational overhead. Our mechanistic analyses confirm that models trained with our method encode multiple, causally independent refusal directions that prompt-based jailbreaks cannot suppress simultaneously, providing empirical support for fail-closed alignment as a principled foundation for robust LLM safety.</li>
<li><strong>摘要：</strong>我们发现当前大语言模型（LLM）对齐中的一个结构性弱点：现代拒绝机制是故障开放的。虽然现有方法跨多个潜在特征对拒绝行为进行编码，但通过基于提示的越狱抑制单个主导特征$-$可能会导致对齐崩溃，从而导致不安全的生成。受此启发，我们提出故障封闭对齐作为稳健 LLM 安全性的设计原则：即使在部分故障下，拒绝机制也应通过冗余、独立的因果路径保持有效。我们提出了这一原则的具体实例：一个渐进的对齐框架，它迭代地识别和消除先前学习的拒绝方向，迫使模型沿着新的独立子空间重建安全性。在四次越狱攻击中，我们实现了最强的整体鲁棒性，同时减少了过度拒绝并保持了生成质量，并且计算开销很小。我们的机制分析证实，用我们的方法训练的模型编码了多个、因果独立的拒绝方向，而基于提示的越狱无法同时抑制这些方向，为失败关闭对齐提供了经验支持，作为稳健的 LLM 安全性的原则基础。</li>
</ul>

<h3>Title: Discovering Universal Activation Directions for PII Leakage in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Leo Marchyok, Zachary Coalson, Sungho Keum, Sooel Son, Sanghyun Hong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16980">https://arxiv.org/abs/2602.16980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16980">https://arxiv.org/pdf/2602.16980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16980]] Discovering Universal Activation Directions for PII Leakage in Language Models(https://arxiv.org/abs/2602.16980)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Modern language models exhibit rich internal structure, yet little is known about how privacy-sensitive behaviors, such as personally identifiable information (PII) leakage, are represented and modulated within their hidden states. We present UniLeak, a mechanistic-interpretability framework that identifies universal activation directions: latent directions in a model's residual stream whose linear addition at inference time consistently increases the likelihood of generating PII across prompts. These model-specific directions generalize across contexts and amplify PII generation probability, with minimal impact on generation quality. UniLeak recovers such directions without access to training data or groundtruth PII, relying only on self-generated text. Across multiple models and datasets, steering along these universal directions substantially increases PII leakage compared to existing prompt-based extraction methods. Our results offer a new perspective on PII leakage: the superposition of a latent signal in the model's representations, enabling both risk amplification and mitigation.</li>
<li><strong>摘要：</strong>现代语言模型展现了丰富的内部结构，但人们对隐私敏感行为（例如个人身份信息（PII）泄漏）如何在其隐藏状态中表示和调制知之甚少。我们提出了 UniLeak，这是一种机械可解释性框架，可识别通用激活方向：模型残差流中的潜在方向，其在推理时的线性相加一致地增加了跨提示生成 PII 的可能性。这些特定于模型的方向可以跨环境进行推广，并放大 PII 生成概率，同时对生成质量的影响最小。 UniLeak 在无需访问训练数据或真实 PII 的情况下，仅依靠自行生成的文本来恢复此类指示。与现有的基于提示的提取方法相比，在多个模型和数据集中，沿着这些通用方向进行引导会大大增加 PII 泄漏。我们的结果为 PII 泄漏提供了新的视角：模型表示中潜在信号的叠加，从而实现风险放大和缓解。</li>
</ul>

<h3>Title: PartRAG: Retrieval-Augmented Part-Level 3D Generation and Editing</h3>
<ul>
<li><strong>Authors: </strong>Peize Li, Zeyu Zhang, Hao Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.17033">https://arxiv.org/abs/2602.17033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.17033">https://arxiv.org/pdf/2602.17033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.17033]] PartRAG: Retrieval-Augmented Part-Level 3D Generation and Editing(https://arxiv.org/abs/2602.17033)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Single-image 3D generation with part-level structure remains challenging: learned priors struggle to cover the long tail of part geometries and maintain multi-view consistency, and existing systems provide limited support for precise, localized edits. We present PartRAG, a retrieval-augmented framework that integrates an external part database with a diffusion transformer to couple generation with an editable representation. To overcome the first challenge, we introduce a Hierarchical Contrastive Retrieval module that aligns dense image patches with 3D part latents at both part and object granularity, retrieving from a curated bank of 1,236 part-annotated assets to inject diverse, physically plausible exemplars into denoising. To overcome the second challenge, we add a masked, part-level editor that operates in a shared canonical space, enabling swaps, attribute refinements, and compositional updates without regenerating the whole object while preserving non-target parts and multi-view consistency. PartRAG achieves competitive results on Objaverse, ShapeNet, and ABO-reducing Chamfer Distance from 0.1726 to 0.1528 and raising F-Score from 0.7472 to 0.844 on Objaverse-with inference of 38s and interactive edits in 5-8s. Qualitatively, PartRAG produces sharper part boundaries, better thin-structure fidelity, and robust behavior on articulated objects. Code: this https URL. Website: this https URL.</li>
<li><strong>摘要：</strong>具有零件级结构的单图像 3D 生成仍然具有挑战性：学习先验很难覆盖零件几何形状的长尾并保持多视图一致性，而现有系统对精确的本地化编辑提供的支持有限。我们提出了 PartRAG，一个检索增强框架，它将外部零件数据库与扩散转换器集成在一起，以将生成与可编辑表示相结合。为了克服第一个挑战，我们引入了一个分层对比检索模块，该模块将密集图像块与零件和对象粒度上的 3D 零件潜在对齐，从包含 1,236 个零件注释资产的精选库中进行检索，以将多样化的、物理上合理的示例注入到去噪中。为了克服第二个挑战，我们添加了一个屏蔽的部件级编辑器，该编辑器在共享规范空间中运行，支持交换、属性细化和组合更新，而无需重新生成整个对象，同时保留非目标部件和多视图一致性。 PartRAG 在 Objaverse、ShapeNet 和 ABO 上取得了有竞争力的结果——将切角距离从 0.1726 减少到 0.1528，并将 Objaverse 上的 F 分数从 0.7472 提高到 0.844——推理 38 秒，交互式编辑 5-8 秒。定性地讲，PartRAG 可以在铰接对象上产生更清晰的零件边界、更好的薄结构保真度和稳健的行为。代码：此 https URL。网站：此 https URL。</li>
</ul>

<h3>Title: Amber-Image: Efficient Compression of Large-Scale Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Chaojie Yang, Tian Li, Yue Zhang, Jun Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.17047">https://arxiv.org/abs/2602.17047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.17047">https://arxiv.org/pdf/2602.17047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.17047]] Amber-Image: Efficient Compression of Large-Scale Diffusion Transformers(https://arxiv.org/abs/2602.17047)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion Transformer (DiT) architectures have significantly advanced Text-to-Image (T2I) generation but suffer from prohibitive computational costs and deployment barriers. To address these challenges, we propose an efficient compression framework that transforms the 60-layer dual-stream MMDiT-based Qwen-Image into lightweight models without training from scratch. Leveraging this framework, we introduce Amber-Image, a series of streamlined T2I models. We first derive Amber-Image-10B using a timestep-sensitive depth pruning strategy, where retained layers are reinitialized via local weight averaging and optimized through layer-wise distillation and full-parameter fine-tuning. Building on this, we develop Amber-Image-6B by introducing a hybrid-stream architecture that converts deep-layer dual streams into a single stream initialized from the image branch, further refined via progressive distillation and lightweight fine-tuning. Our approach reduces parameters by 70% and eliminates the need for large-scale data engineering. Notably, the entire compression and training pipeline-from the 10B to the 6B variant-requires fewer than 2,000 GPU hours, demonstrating exceptional cost-efficiency compared to training from scratch. Extensive evaluations on benchmarks like DPG-Bench and LongText-Bench show that Amber-Image achieves high-fidelity synthesis and superior text rendering, matching much larger models.</li>
<li><strong>摘要：</strong>Diffusion Transformer (DiT) 架构显着改进了文本到图像 (T2I) 的生成，但面临着过高的计算成本和部署障碍。为了应对这些挑战，我们提出了一种高效的压缩框架，可以将基于 MMDiT 的 60 层双流 Qwen-Image 转换为轻量级模型，而无需从头开始训练。利用这个框架，我们推出了 Amber-Image，一系列精简的 T2I 模型。我们首先使用时间步长敏感的深度修剪策略导出 Amber-Image-10B，其中保留层通过局部权重平均重新初始化，并通过分层蒸馏和全参数微调进行优化。在此基础上，我们通过引入混合流架构来开发 Amber-Image-6B，该架构将深层双流转换为从图像分支初始化的单流，并通过渐进式蒸馏和轻量级微调进一步细化。我们的方法将参数减少了 70%，并且消除了大规模数据工程的需要。值得注意的是，整个压缩和训练管道（从 10B 到 6B 变体）需要不到 2,000 个 GPU 小时，与从头开始训练相比，展现出卓越的成本效益。对 DPG-Bench 和 LongText-Bench 等基准测试的广泛评估表明，Amber-Image 实现了高保真合成和出色的文本渲染，可匹配更大的模型。</li>
</ul>

<h3>Title: AdvSynGNN: Structure-Adaptive Graph Neural Nets via Adversarial Synthesis and Self-Corrective Propagation</h3>
<ul>
<li><strong>Authors: </strong>Rong Fu, Muge Qi, Chunlei Meng, Shuo Yin, Kun Liu, Zhaolu Kang, Simon Fong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.17071">https://arxiv.org/abs/2602.17071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.17071">https://arxiv.org/pdf/2602.17071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.17071]] AdvSynGNN: Structure-Adaptive Graph Neural Nets via Adversarial Synthesis and Self-Corrective Propagation(https://arxiv.org/abs/2602.17071)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Graph neural networks frequently encounter significant performance degradation when confronted with structural noise or non-homophilous topologies. To address these systemic vulnerabilities, we present AdvSynGNN, a comprehensive architecture designed for resilient node-level representation learning. The proposed framework orchestrates multi-resolution structural synthesis alongside contrastive objectives to establish geometry-sensitive initializations. We develop a transformer backbone that adaptively accommodates heterophily by modulating attention mechanisms through learned topological signals. Central to our contribution is an integrated adversarial propagation engine, where a generative component identifies potential connectivity alterations while a discriminator enforces global coherence. Furthermore, label refinement is achieved through a residual correction scheme guided by per-node confidence metrics, which facilitates precise control over iterative stability. Empirical evaluations demonstrate that this synergistic approach effectively optimizes predictive accuracy across diverse graph distributions while maintaining computational efficiency. The study concludes with practical implementation protocols to ensure the robust deployment of the AdvSynGNN system in large-scale environments.</li>
<li><strong>摘要：</strong>当面对结构噪声或非同种拓扑时，图神经网络经常会遇到显着的性能下降。为了解决这些系统漏洞，我们提出了 AdvSynGNN，这是一种专为弹性节点级表示学习而设计的综合架构。所提出的框架协调多分辨率结构合成以及对比目标，以建立几何敏感的初始化。我们开发了一个变压器主干，通过学习的拓扑信号调节注意力机制，自适应地适应异质性。我们贡献的核心是集成的对抗性传播引擎，其中生成组件识别潜在的连接改变，而鉴别器则强制全局一致性。此外，标签细化是通过每个节点置信度指标指导的残差校正方案来实现的，这有助于精确控制迭代稳定性。实证评估表明，这种协同方法有效地优化了不同图形分布的预测准确性，同时保持了计算效率。该研究最后提出了实际的实施协议，以确保 AdvSynGNN 系统在大规模环境中的稳健部署。</li>
</ul>

<h3>Title: Synergizing Transport-Based Generative Models and Latent Geometry for Stochastic Closure Modeling</h3>
<ul>
<li><strong>Authors: </strong>Xinghao Dong, Huchen Yang, Jin-long Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DS, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.17089">https://arxiv.org/abs/2602.17089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.17089">https://arxiv.org/pdf/2602.17089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.17089]] Synergizing Transport-Based Generative Models and Latent Geometry for Stochastic Closure Modeling(https://arxiv.org/abs/2602.17089)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion models recently developed for generative AI tasks can produce high-quality samples while still maintaining diversity among samples to promote mode coverage, providing a promising path for learning stochastic closure models. Compared to other types of generative AI models, such as GANs and VAEs, the sampling speed is known as a key disadvantage of diffusion models. By systematically comparing transport-based generative models on a numerical example of 2D Kolmogorov flows, we show that flow matching in a lower-dimensional latent space is suited for fast sampling of stochastic closure models, enabling single-step sampling that is up to two orders of magnitude faster than iterative diffusion-based approaches. To control the latent space distortion and thus ensure the physical fidelity of the sampled closure term, we compare the implicit regularization offered by a joint training scheme against two explicit regularizers: metric-preserving (MP) and geometry-aware (GA) constraints. Besides offering a faster sampling speed, both explicitly and implicitly regularized latent spaces inherit the key topological information from the lower-dimensional manifold of the original complex dynamical system, which enables the learning of stochastic closure models without demanding a huge amount of training data.</li>
<li><strong>摘要：</strong>最近为生成人工智能任务开发的扩散模型可以产生高质量的样本，同时仍然保持样本之间的多样性以促进模式覆盖，为学习随机闭合模型提供了一条有前途的路径。与其他类型的生成式人工智能模型（例如 GAN 和 VAE）相比，采样速度被认为是扩散模型的一个关键缺点。通过在二维柯尔莫哥洛夫流的数值示例上系统地比较基于传输的生成模型，我们表明低维潜在空间中的流匹配适用于随机闭合模型的快速采样，使得单步采样比基于迭代扩散的方法快两个数量级。为了控制潜在空间失真，从而确保采样闭包项的物理保真度，我们将联合训练方案提供的隐式正则化与两个显式正则化器进行比较：度量保留（MP）和几何感知（GA）约束。除了提供更快的采样速度外，显式和隐式正则化潜在空间都继承了原始复杂动力系统的低维流形的关键拓扑信息，这使得无需大量训练数据即可学习随机闭包模型。</li>
</ul>

<h3>Title: VP-VAE: Rethinking Vector Quantization via Adaptive Vector Perturbation</h3>
<ul>
<li><strong>Authors: </strong>Linwei Zhai, Han Ding, Mingzhi Lin, Cui Zhao, Fei Wang, Ge Wang, Wang Zhi, Wei Xi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.17133">https://arxiv.org/abs/2602.17133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.17133">https://arxiv.org/pdf/2602.17133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.17133]] VP-VAE: Rethinking Vector Quantization via Adaptive Vector Perturbation(https://arxiv.org/abs/2602.17133)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Vector Quantized Variational Autoencoders (VQ-VAEs) are fundamental to modern generative modeling, yet they often suffer from training instability and "codebook collapse" due to the inherent coupling of representation learning and discrete codebook optimization. In this paper, we propose VP-VAE (Vector Perturbation VAE), a novel paradigm that decouples representation learning from discretization by eliminating the need for an explicit codebook during training. Our key insight is that, from the neural network's viewpoint, performing quantization primarily manifests as injecting a structured perturbation in latent space. Accordingly, VP-VAE replaces the non-differentiable quantizer with distribution-consistent and scale-adaptive latent perturbations generated via Metropolis--Hastings sampling. This design enables stable training without a codebook while making the model robust to inference-time quantization error. Moreover, under the assumption of approximately uniform latent variables, we derive FSP (Finite Scalar Perturbation), a lightweight variant of VP-VAE that provides a unified theoretical explanation and a practical improvement for FSQ-style fixed quantizers. Extensive experiments on image and audio benchmarks demonstrate that VP-VAE and FSP improve reconstruction fidelity and achieve substantially more balanced token usage, while avoiding the instability inherent to coupled codebook training.</li>
<li><strong>摘要：</strong>矢量量化变分自编码器 (VQ-VAE) 是现代生成建模的基础，但由于表示学习和离散码本优化的固有耦合，它们经常遭受训练不稳定和“码本崩溃”的困扰。在本文中，我们提出了 VP-VAE（矢量扰动 VAE），这是一种新颖的范式，通过消除训练期间对显式码本的需求，将表示学习与离散化解耦。我们的主要见解是，从神经网络的角度来看，执行量化主要表现为在潜在空间中注入结构化扰动。因此，VP-VAE 用通过 Metropolis-Hastings 采样生成的分布一致且尺度自适应的潜在扰动取代了不可微的量化器。这种设计无需密码本即可实现稳定的训练，同时使模型对推理时间量化误差具有鲁棒性。此外，在潜在变量近似一致的假设下，我们推导了FSP（有限标量扰动），这是VP-VAE的轻量级变体，为FSQ式固定量化器提供了统一的理论解释和实际改进。对图像和音频基准的大量实验表明，VP-VAE 和 FSP 提高了重建保真度，并实现了更加平衡的令牌使用，同时避免了耦合码本训练固有的不稳定性。</li>
</ul>

<h3>Title: TimeOmni-VL: Unified Models for Time Series Understanding and Generation</h3>
<ul>
<li><strong>Authors: </strong>Tong Guan, Sheng Pan, Johan Barthelemy, Zhao Li, Yujun Cai, Cesare Alippi, Ming Jin, Shirui Pan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.17149">https://arxiv.org/abs/2602.17149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.17149">https://arxiv.org/pdf/2602.17149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.17149]] TimeOmni-VL: Unified Models for Time Series Understanding and Generation(https://arxiv.org/abs/2602.17149)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent time series modeling faces a sharp divide between numerical generation and semantic understanding, with research showing that generation models often rely on superficial pattern matching, while understanding-oriented models struggle with high-fidelity numerical output. Although unified multimodal models (UMMs) have bridged this gap in vision, their potential for time series remains untapped. We propose TimeOmni-VL, the first vision-centric framework that unifies time series understanding and generation through two key innovations: (1) Fidelity-preserving bidirectional mapping between time series and images (Bi-TSI), which advances Time Series-to-Image (TS2I) and Image-to-Time Series (I2TS) conversions to ensure near-lossless transformations. (2) Understanding-guided generation. We introduce TSUMM-Suite, a novel dataset consists of six understanding tasks rooted in time series analytics that are coupled with two generation tasks. With a calibrated Chain-of-Thought, TimeOmni-VL is the first to leverage time series understanding as an explicit control signal for high-fidelity generation. Experiments confirm that this unified approach significantly improves both semantic understanding and numerical precision, establishing a new frontier for multimodal time series modeling.</li>
<li><strong>摘要：</strong>Recent time series modeling faces a sharp divide between numerical generation and semantic understanding, with research showing that generation models often rely on superficial pattern matching, while understanding-oriented models struggle with high-fidelity numerical output.尽管统一多模态模型 (UMM) 弥补了这一愿景差距，但其时间序列的潜力尚未开发。 We propose TimeOmni-VL, the first vision-centric framework that unifies time series understanding and generation through two key innovations: (1) Fidelity-preserving bidirectional mapping between time series and images (Bi-TSI), which advances Time Series-to-Image (TS2I) and Image-to-Time Series (I2TS) conversions to ensure near-lossless transformations. (2) Understanding-guided generation. We introduce TSUMM-Suite, a novel dataset consists of six understanding tasks rooted in time series analytics that are coupled with two generation tasks. With a calibrated Chain-of-Thought, TimeOmni-VL is the first to leverage time series understanding as an explicit control signal for high-fidelity generation. Experiments confirm that this unified approach significantly improves both semantic understanding and numerical precision, establishing a new frontier for multimodal time series modeling.</li>
</ul>

<h3>Title: GASS: Geometry-Aware Spherical Sampling for Disentangled Diversity Enhancement in Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Ye Zhu, Kaleb S. Newman, Johannes F. Lutzeyer, Adriana Romero-Soriano, Michal Drozdzal, Olga Russakovsky</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.17200">https://arxiv.org/abs/2602.17200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.17200">https://arxiv.org/pdf/2602.17200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.17200]] GASS: Geometry-Aware Spherical Sampling for Disentangled Diversity Enhancement in Text-to-Image Generation(https://arxiv.org/abs/2602.17200)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Despite high semantic alignment, modern text-to-image (T2I) generative models still struggle to synthesize diverse images from a given prompt. This lack of diversity not only restricts user choice, but also risks amplifying societal biases. In this work, we enhance the T2I diversity through a geometric lens. Unlike most existing methods that rely primarily on entropy-based guidance to increase sample dissimilarity, we introduce Geometry-Aware Spherical Sampling (GASS) to enhance diversity by explicitly controlling both prompt-dependent and prompt-independent sources of variation. Specifically, we decompose the diversity measure in CLIP embeddings using two orthogonal directions: the text embedding, which captures semantic variation related to the prompt, and an identified orthogonal direction that captures prompt-independent variation (e.g., backgrounds). Based on this decomposition, GASS increases the geometric projection spread of generated image embeddings along both axes and guides the T2I sampling process via expanded predictions along the generation trajectory. Our experiments on different frozen T2I backbones (U-Net and DiT, diffusion and flow) and benchmarks demonstrate the effectiveness of disentangled diversity enhancement with minimal impact on image fidelity and semantic alignment.</li>
<li><strong>摘要：</strong>尽管语义高度一致，现代文本到图像（T2I）生成模型仍然难以根据给定的提示合成不同的图像。这种缺乏多样性不仅限制了用户的选择，而且还有放大社会偏见的风险。在这项工作中，我们通过几何透镜增强了 T2I 多样性。与大多数主要依靠基于熵的指导来增加样本差异性的现有方法不同，我们引入了几何感知球形采样（GASS），通过明确控制提示相关和提示无关的变异源来增强多样性。具体来说，我们使用两个正交方向分解 CLIP 嵌入中的多样性度量：文本嵌入，捕获与提示相关的语义变化，以及识别的正交方向，捕获与提示无关的变化（例如背景）。基于这种分解，GASS 增加了生成的图像嵌入沿两个轴的几何投影扩展，并通过沿生成轨迹的扩展预测来指导 T2I 采样过程。我们对不同冻结 T2I 主干（U-Net 和 DiT、扩散和流）和基准的实验证明了解缠结多样性增强的有效性，同时对图像保真度和语义对齐的影响最小。</li>
</ul>

<h3>Title: CounterFlowNet: From Minimal Changes to Meaningful Counterfactual Explanations</h3>
<ul>
<li><strong>Authors: </strong>Oleksii Furman, Patryk Marszałek, Jan Masłowski, Piotr Gaiński, Maciej Zięba, Marek Śmieja</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.17244">https://arxiv.org/abs/2602.17244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.17244">https://arxiv.org/pdf/2602.17244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.17244]] CounterFlowNet: From Minimal Changes to Meaningful Counterfactual Explanations(https://arxiv.org/abs/2602.17244)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Counterfactual explanations (CFs) provide human-interpretable insights into model's predictions by identifying minimal changes to input features that would alter the model's output. However, existing methods struggle to generate multiple high-quality explanations that (1) affect only a small portion of the features, (2) can be applied to tabular data with heterogeneous features, and (3) are consistent with the user-defined constraints. We propose CounterFlowNet, a generative approach that formulates CF generation as sequential feature modification using conditional Generative Flow Networks (GFlowNet). CounterFlowNet is trained to sample CFs proportionally to a user-specified reward function that can encode key CF desiderata: validity, sparsity, proximity and plausibility, encouraging high-quality explanations. The sequential formulation yields highly sparse edits, while a unified action space seamlessly supports continuous and categorical features. Moreover, actionability constraints, such as immutability and monotonicity of features, can be enforced at inference time via action masking, without retraining. Experiments on eight datasets under two evaluation protocols demonstrate that CounterFlowNet achieves superior trade-offs between validity, sparsity, plausibility, and diversity with full satisfaction of the given constraints.</li>
<li><strong>摘要：</strong>反事实解释 (CF) 通过识别会改变模型输出的输入特征的最小变化，为模型的预测提供人类可解释的见解。然而，现有的方法很难生成多个高质量的解释，这些解释（1）仅影响一小部分特征，（2）可以应用于具有异构特征的表格数据，（3）与用户定义的约束一致。我们提出了 CounterFlowNet，这是一种使用条件生成流网络 (GFlowNet) 将 CF 生成制定为顺序特征修改的生成方法。 CounterFlowNet 经过训练，可以根据用户指定的奖励函数按比例对 CF 进行采样，该函数可以编码关键的 CF 需求：有效性、稀疏性、接近性和合理性，从而鼓励高质量的解释。顺序公式产生高度稀疏的编辑，而统一的动作空间无缝支持连续和分类特征。此外，可操作性约束（例如特征的不变性和单调性）可以在推理时通过动作屏蔽强制执行，而无需重新训练。在两种评估协议下对八个数据集进行的实验表明，CounterFlowNet 在有效性、稀疏性、合理性和多样性之间实现了卓越的权衡，并且完全满足给定的约束。</li>
</ul>

<h3>Title: Learning a Latent Pulse Shape Interface for Photoinjector Laser Systems</h3>
<ul>
<li><strong>Authors: </strong>Alexander Klemps, Denis Ilia, Pradeep Kr. Banerjee, Ye Chen, Henrik Tünnermann, Nihat Ay</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.17263">https://arxiv.org/abs/2602.17263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.17263">https://arxiv.org/pdf/2602.17263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.17263]] Learning a Latent Pulse Shape Interface for Photoinjector Laser Systems(https://arxiv.org/abs/2602.17263)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Controlling the longitudinal laser pulse shape in photoinjectors of Free-Electron Lasers is a powerful lever for optimizing electron beam quality, but systematic exploration of the vast design space is limited by the cost of brute-force pulse propagation simulations. We present a generative modeling framework based on Wasserstein Autoencoders to learn a differentiable latent interface between pulse shaping and downstream beam dynamics. Our empirical findings show that the learned latent space is continuous and interpretable while maintaining high-fidelity reconstructions. Pulse families such as higher-order Gaussians trace coherent trajectories, while standardizing the temporal pulse lengths shows a latent organization correlated with pulse energy. Analysis via principal components and Gaussian Mixture Models reveals a well behaved latent geometry, enabling smooth transitions between distinct pulse types via linear interpolation. The model generalizes from simulated data to real experimental pulse measurements, accurately reconstructing pulses and embedding them consistently into the learned manifold. Overall, the approach reduces reliance on expensive pulse-propagation simulations and facilitates downstream beam dynamics simulation and analysis.</li>
<li><strong>摘要：</strong>Controlling the longitudinal laser pulse shape in photoinjectors of Free-Electron Lasers is a powerful lever for optimizing electron beam quality, but systematic exploration of the vast design space is limited by the cost of brute-force pulse propagation simulations.我们提出了一个基于 Wasserstein 自动编码器的生成建模框架，用于学习脉冲整形和下游光束动力学之间的可微潜在接口。我们的实证研究结果表明，学习到的潜在空间是连续且可解释的，同时保持高保真重建。高阶高斯等脉冲族追踪相干轨迹，同时标准化时间脉冲长度显示与脉冲能量相关的潜在组织。通过主成分和高斯混合模型的分析揭示了表现良好的潜在几何结构，通过线性插值实现不同脉冲类型之间的平滑过渡。该模型从模拟数据推广到真实的实验脉冲测量，准确地重建脉冲并将它们一致地嵌入到学习的流形中。总体而言，该方法减少了对昂贵的脉冲传播模拟的依赖，并有利于下游光束动力学模拟和分析。</li>
</ul>

<h3>Title: Physics Encoded Spatial and Temporal Generative Adversarial Network for Tropical Cyclone Image Super-resolution</h3>
<ul>
<li><strong>Authors: </strong>Ruoyi Zhang, Jiawei Yuan, Lujia Ye, Runling Yu, Liling Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.17277">https://arxiv.org/abs/2602.17277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.17277">https://arxiv.org/pdf/2602.17277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.17277]] Physics Encoded Spatial and Temporal Generative Adversarial Network for Tropical Cyclone Image Super-resolution(https://arxiv.org/abs/2602.17277)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generative</a></li>
<li><strong>Abstract: </strong>High-resolution satellite imagery is indispensable for tracking the genesis, intensification, and trajectory of tropical cyclones (TCs). However, existing deep learning-based super-resolution (SR) methods often treat satellite image sequences as generic videos, neglecting the underlying atmospheric physical laws governing cloud motion. To address this, we propose a Physics Encoded Spatial and Temporal Generative Adversarial Network (PESTGAN) for TC image super-resolution. Specifically, we design a disentangled generator architecture incorporating a PhyCell module, which approximates the vorticity equation via constrained convolutions and encodes the resulting approximate physical dynamics as implicit latent representations to separate physical dynamics from visual textures. Furthermore, a dual-discriminator framework is introduced, employing a temporal discriminator to enforce motion consistency alongside spatial realism. Experiments on the Digital Typhoon dataset for 4$\times$ upscaling demonstrate that PESTGAN establishes a better performance in structural fidelity and perceptual quality. While maintaining competitive pixel-wise accuracy compared to existing approaches, our method significantly excels in reconstructing meteorologically plausible cloud structures with superior physical fidelity.</li>
<li><strong>摘要：</strong>高分辨率卫星图像对于追踪热带气旋 (TC) 的起源、增强和轨迹必不可少。然而，现有的基于深度学习的超分辨率（SR）方法通常将卫星图像序列视为通用视频，忽略了控制云运动的基本大气物理定律。为了解决这个问题，我们提出了一种用于TC图像超分辨率的物理编码时空生成对抗网络（PESTGAN）。具体来说，我们设计了一个包含 PhyCell 模块的解缠结生成器架构，该架构通过约束卷积来近似涡量方程，并将所得的近似物理动力学编码为隐式潜在表示，以将物理动力学与视觉纹理分开。此外，引入了双鉴别器框架，采用时间鉴别器来强制运动一致性和空间现实性。在 Digital Typhoon 数据集上进行 4$\times$ 放大的实验表明，PESTGAN 在结构保真度和感知质量方面建立了更好的性能。与现有方法相比，在保持具有竞争力的像素精度的同时，我们的方法在重建气象上合理的云结构方面表现出色，具有卓越的物理保真度。</li>
</ul>

<h3>Title: Leveraging Contrastive Learning for a Similarity-Guided Tampered Document Data Generation Pipeline</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Dhouib, Davide Buscaldi, Sonia Vanier, Aymen Shabou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.17322">https://arxiv.org/abs/2602.17322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.17322">https://arxiv.org/pdf/2602.17322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.17322]] Leveraging Contrastive Learning for a Similarity-Guided Tampered Document Data Generation Pipeline(https://arxiv.org/abs/2602.17322)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Detecting tampered text in document images is a challenging task due to data scarcity. To address this, previous work has attempted to generate tampered documents using rule-based methods. However, the resulting documents often suffer from limited variety and poor visual quality, typically leaving highly visible artifacts that are rarely observed in real-world manipulations. This undermines the model's ability to learn robust, generalizable features and results in poor performance on real-world data. Motivated by this discrepancy, we propose a novel method for generating high-quality tampered document images. We first train an auxiliary network to compare text crops, leveraging contrastive learning with a novel strategy for defining positive pairs and their corresponding negatives. We also train a second auxiliary network to evaluate whether a crop tightly encloses the intended characters, without cutting off parts of characters or including parts of adjacent ones. Using a carefully designed generation pipeline that leverages both networks, we introduce a framework capable of producing diverse, high-quality tampered document images. We assess the effectiveness of our data generation pipeline by training multiple models on datasets derived from the same source images, generated using our method and existing approaches, under identical training protocols. Evaluating these models on various open-source datasets shows that our pipeline yields consistent performance improvements across architectures and datasets.</li>
<li><strong>摘要：</strong>由于数据稀缺，检测文档图像中被篡改的文本是一项具有挑战性的任务。为了解决这个问题，之前的工作尝试使用基于规则的方法生成被篡改的文档。然而，生成的文档通常种类有限且视觉质量较差，通常会留下在现实世界的操作中很少观察到的高度可见的伪影。这削弱了模型学习稳健、可泛化特征的能力，并导致在实际数据上的表现不佳。受这种差异的启发，我们提出了一种生成高质量篡改文档图像的新方法。我们首先训练一个辅助网络来比较文本作物，利用对比学习和定义正对及其相应负数的新颖策略。我们还训练第二个辅助网络来评估裁剪是否紧密包围了预期的字符，而不切断字符的部分或包含相邻字符的部分。使用精心设计的利用两个网络的生成管道，我们引入了一个能够生成多样化、高质量的篡改文档图像的框架。我们通过在相同的训练协议下使用我们的方法和现有方法生成的来自相同源图像的数据集训练多个模型来评估数据生成管道的有效性。在各种开源数据集上评估这些模型表明，我们的管道在架构和数据集上产生了一致的性能改进。</li>
</ul>

<h3>Title: Variational Grey-Box Dynamics Matching</h3>
<ul>
<li><strong>Authors: </strong>Gurjeet Sangra Singh, Frantzeska Lavda, Giangiacomo Mercatali, Alexandros Kalousis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.17477">https://arxiv.org/abs/2602.17477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.17477">https://arxiv.org/pdf/2602.17477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.17477]] Variational Grey-Box Dynamics Matching(https://arxiv.org/abs/2602.17477)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep generative models such as flow matching and diffusion models have shown great potential in learning complex distributions and dynamical systems, but often act as black-boxes, neglecting underlying physics. In contrast, physics-based simulation models described by ODEs/PDEs remain interpretable, but may have missing or unknown terms, unable to fully describe real-world observations. We bridge this gap with a novel grey-box method that integrates incomplete physics models directly into generative models. Our approach learns dynamics from observational trajectories alone, without ground-truth physics parameters, in a simulation-free manner that avoids scalability and stability issues of Neural ODEs. The core of our method lies in modelling a structured variational distribution within the flow matching framework, by using two latent encodings: one to model the missing stochasticity and multi-modal velocity, and a second to encode physics parameters as a latent variable with a physics-informed prior. Furthermore, we present an adaptation of the framework to handle second-order dynamics. Our experiments on representative ODE/PDE problems show that our method performs on par with or superior to fully data-driven approaches and previous grey-box baselines, while preserving the interpretability of the physics model. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>诸如流匹配和扩散模型之类的深层生成模型在学习复杂分布和动力系统方面表现出了巨大的潜力，但通常充当黑匣子，忽略了基础物理。相比之下，由常微分方程/偏微分方程描述的基于物理的模拟模型仍然可以解释，但可能缺少或未知术语，无法完全描述现实世界的观察结果。我们通过一种新颖的灰盒方法弥补了这一差距，该方法将不完整的物理模型直接集成到生成模型中。我们的方法仅从观测轨迹中学习动力学，无需真实物理参数，以免模拟的方式避免了神经常微分方程的可扩展性和稳定性问题。我们方法的核心在于，通过使用两种潜在编码，在流匹配框架内对结构化变分分布进行建模：一种是对缺失的随机性和多模态速度进行建模，第二种是将物理参数编码为具有物理信息先验的潜在变量。此外，我们提出了该框架的改编方案来处理二阶动力学。我们对代表性 ODE/PDE 问题的实验表明，我们的方法的性能与完全数据驱动的方法和以前的灰盒基线相当或优于，同时保留了物理模型的可解释性。我们的代码可以在这个 https URL 上找到。</li>
</ul>

<h3>Title: A Theoretical Framework for Modular Learning of Robust Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Corinna Cortes, Mehryar Mohri, Yutao Zhong</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.17554">https://arxiv.org/abs/2602.17554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.17554">https://arxiv.org/pdf/2602.17554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.17554]] A Theoretical Framework for Modular Learning of Robust Generative Models(https://arxiv.org/abs/2602.17554)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Training large-scale generative models is resource-intensive and relies heavily on heuristic dataset weighting. We address two fundamental questions: Can we train Large Language Models (LLMs) modularly-combining small, domain-specific experts to match monolithic performance-and can we do so robustly for any data mixture, eliminating heuristic tuning? We present a theoretical framework for modular generative modeling where a set of pre-trained experts are combined via a gating mechanism. We define the space of normalized gating functions, $G_{1}$, and formulate the problem as a minimax game to find a single robust gate that minimizes divergence to the worst-case data mixture. We prove the existence of such a robust gate using Kakutani's fixed-point theorem and show that modularity acts as a strong regularizer, with generalization bounds scaling with the lightweight gate's complexity. Furthermore, we prove that this modular approach can theoretically outperform models retrained on aggregate data, with the gap characterized by the Jensen-Shannon Divergence. Finally, we introduce a scalable Stochastic Primal-Dual algorithm and a Structural Distillation method for efficient inference. Empirical results on synthetic and real-world datasets confirm that our modular architecture effectively mitigates gradient conflict and can robustly outperform monolithic baselines.</li>
<li><strong>摘要：</strong>训练大规模生成模型是资源密集型的，并且严重依赖启发式数据集加权。我们解决两个基本问题：我们能否以模块化方式训练大型语言模型（LLM）——将小型的、特定领域的专家组合起来以匹配整体性能——以及我们能否对任何数据混合稳健地进行训练，从而消除启发式调整？我们提出了模块化生成建模的理论框架，其中一组预先训练的专家通过门控机制组合在一起。我们定义归一化门函数的空间 $G_{1}$，并将问题表述为极小极大游戏，以找到单个鲁棒门，最大限度地减少最坏情况数据混合的分歧。我们使用角谷定理证明了这种鲁棒门的存在，并表明模块化充当了强大的正则化器，泛化边界随着轻量级门的复杂性而缩放。此外，我们证明这种模块化方法理论上可以优于在聚合数据上重新训练的模型，其差距以詹森-香农散度为特征。最后，我们介绍了一种可扩展的随机原始对偶算法和一种用于高效推理的结构蒸馏方法。合成和真实数据集的实证结果证实，我们的模块化架构有效地减轻了梯度冲突，并且可以稳健地优于整体基线。</li>
</ul>

<h3>Title: Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment</h3>
<ul>
<li><strong>Authors: </strong>Ivan Rinaldi, Matteo Mendula, Nicola Fanelli, Florence Levé, Matteo Testi, Giovanna Castellano, Gennaro Vessio</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.17599">https://arxiv.org/abs/2602.17599</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.17599">https://arxiv.org/pdf/2602.17599</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.17599]] Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment(https://arxiv.org/abs/2602.17599)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Music generation has advanced markedly through multimodal deep learning, enabling models to synthesize audio from text and, more recently, from images. However, existing image-conditioned systems suffer from two fundamental limitations: (i) they are typically trained on natural photographs, limiting their ability to capture the richer semantic, stylistic, and cultural content of artworks; and (ii) most rely on an image-to-text conversion stage, using language as a semantic shortcut that simplifies conditioning but prevents direct visual-to-audio learning. Motivated by these gaps, we introduce ArtSound, a large-scale multimodal dataset of 105,884 artwork-music pairs enriched with dual-modality captions, obtained by extending ArtGraph and the Free Music Archive. We further propose ArtToMus, the first framework explicitly designed for direct artwork-to-music generation, which maps digitized artworks to music without image-to-text translation or language-based semantic supervision. The framework projects visual embeddings into the conditioning space of a latent diffusion model, enabling music synthesis guided solely by visual information. Experimental results show that ArtToMus generates musically coherent and stylistically consistent outputs that reflect salient visual cues of the source artworks. While absolute alignment scores remain lower than those of text-conditioned systems-as expected given the substantially increased difficulty of removing linguistic supervision-ArtToMus achieves competitive perceptual quality and meaningful cross-modal correspondence. This work establishes direct visual-to-music generation as a distinct and challenging research direction, and provides resources that support applications in multimedia art, cultural heritage, and AI-assisted creative practice. Code and dataset will be publicly released upon acceptance.</li>
<li><strong>摘要：</strong>通过多模式深度学习，音乐生成取得了显着进步，使模型能够从文本以及最近的图像中合成音频。然而，现有的图像调节系统存在两个基本限制：（i）它们通常是在自然照片上进行训练的，限制了它们捕捉艺术品更丰富的语义、风格和文化内容的能力； (ii)大多数依赖于图像到文本的转换阶段，使用语言作为语义捷径，简化调节，但阻止直接的视觉到音频学习。受这些差距的启发，我们引入了 ArtSound，这是一个包含 105,884 个艺术作品-音乐对的大型多模态数据集，富含双模态字幕，是通过扩展 ArtGraph 和免费音乐档案库获得的。我们进一步提出了 ArtToMus，这是第一个专门为直接艺术品到音乐生成而设计的框架，它将数字化艺术品映射到音乐，而无需图像到文本的翻译或基于语言的语义监督。该框架将视觉嵌入投射到潜在扩散模型的条件空间中，从而实现仅由视觉信息引导的音乐合成。实验结果表明，ArtToMus 生成音乐连贯且风格一致的输出，反映源艺术品的显着视觉线索。虽然绝对对齐分数仍然低于文本条件系统的分数（考虑到消除语言监督的难度大幅增加，正如预期的那样），ArtToMus 实现了有竞争力的感知质量和有意义的跨模式对应。这项工作将直接视觉到音乐的生成确立为一个独特且具有挑战性的研究方向，并提供支持多媒体艺术、文化遗产和人工智能辅助创意实践应用的资源。代码和数据集将在接受后公开发布。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
