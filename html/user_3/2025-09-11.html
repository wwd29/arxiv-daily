<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-11</h1>
<h3>Title: Revisiting Deepfake Detection: Chronological Continual Learning and the Limits of Generalization</h3>
<ul>
<li><strong>Authors: </strong>Federico Fontana, Anxhelo Diko, Romeo Lanzino, Marco Raoul Marini, Bachir Kaddar, Gian Luca Foresti, Luigi Cinque</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07993">https://arxiv.org/abs/2509.07993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07993">https://arxiv.org/pdf/2509.07993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07993]] Revisiting Deepfake Detection: Chronological Continual Learning and the Limits of Generalization(https://arxiv.org/abs/2509.07993)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The rapid evolution of deepfake generation technologies poses critical challenges for detection systems, as non-continual learning methods demand frequent and expensive retraining. We reframe deepfake detection (DFD) as a Continual Learning (CL) problem, proposing an efficient framework that incrementally adapts to emerging visual manipulation techniques while retaining knowledge of past generators. Our framework, unlike prior approaches that rely on unreal simulation sequences, simulates the real-world chronological evolution of deepfake technologies in extended periods across 7 years. Simultaneously, our framework builds upon lightweight visual backbones to allow for the real-time performance of DFD systems. Additionally, we contribute two novel metrics: Continual AUC (C-AUC) for historical performance and Forward Transfer AUC (FWT-AUC) for future generalization. Through extensive experimentation (over 600 simulations), we empirically demonstrate that while efficient adaptation (+155 times faster than full retraining) and robust retention of historical knowledge is possible, the generalization of current approaches to future generators without additional training remains near-random (FWT-AUC $\approx$ 0.5) due to the unique imprint characterizing each existing generator. Such observations are the foundation of our newly proposed Non-Universal Deepfake Distribution Hypothesis. \textbf{Code will be released upon acceptance.}</li>
<li><strong>摘要：</strong>DeepFake生成技术的快速发展对检测系统构成了关键的挑战，因为非连续学习方法需要频繁且昂贵的再训练。我们将DeepFake检测（DFD）重新构架为一个持续学习（CL）问题，提出了一个有效的框架，该框架可以逐步适应新兴的视觉操纵技术，同时保留对过去发电机的知识。我们的框架与依靠虚幻仿真序列的先前方法不同，可以在7年的长时间内模拟Deepfake技术的现实时间表演化。同时，我们的框架建立在轻巧的视觉主机上，以实现DFD系统的实时性能。此外，我们贡献了两个新型指标：用于历史绩效的连续AUC（C-AUC）和前向转移AUC（FWT-AUC），以实现未来的概括。通过广泛的实验（超过600次模拟），我们从经验上证明，尽管有效的适应性（+155倍（+155倍，比全面的重新培训快），但由于历史知识的保留是可靠的，但由于每个现有的发电机都具有唯一的印记，因此对未来发电机的当前方法的推广仍然接近范围（FWT-AUC $ 0.5）。这种观察是我们新提出的非宇宙深层分布假设的基础。 \ textbf {将在接受后发布代码。}</li>
</ul>

<h3>Title: 3D and 4D World Modeling: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Lingdong Kong, Wesley Yang, Jianbiao Mei, Youquan Liu, Ao Liang, Dekai Zhu, Dongyue Lu, Wei Yin, Xiaotao Hu, Mingkai Jia, Junyuan Deng, Kaiwen Zhang, Yang Wu, Tianyi Yan, Shenyuan Gao, Song Wang, Linfeng Li, Liang Pan, Yong Liu, Jianke Zhu, Wei Tsang Ooi, Steven C.H. Hoi, Ziwei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.07996">https://arxiv.org/abs/2509.07996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.07996">https://arxiv.org/pdf/2509.07996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.07996]] 3D and 4D World Modeling: A Survey(https://arxiv.org/abs/2509.07996)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>World modeling has become a cornerstone in AI research, enabling agents to understand, represent, and predict the dynamic environments they inhabit. While prior work largely emphasizes generative methods for 2D image and video data, they overlook the rapidly growing body of work that leverages native 3D and 4D representations such as RGB-D imagery, occupancy grids, and LiDAR point clouds for large-scale scene modeling. At the same time, the absence of a standardized definition and taxonomy for ``world models'' has led to fragmented and sometimes inconsistent claims in the literature. This survey addresses these gaps by presenting the first comprehensive review explicitly dedicated to 3D and 4D world modeling and generation. We establish precise definitions, introduce a structured taxonomy spanning video-based (VideoGen), occupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and systematically summarize datasets and evaluation metrics tailored to 3D/4D settings. We further discuss practical applications, identify open challenges, and highlight promising research directions, aiming to provide a coherent and foundational reference for advancing the field. A systematic summary of existing literature is available at this https URL</li>
<li><strong>摘要：</strong>世界建模已成为AI研究的基石，使代理商能够理解，代表和预测他们所居住的动态环境。尽管先前的工作在很大程度上强调了2D图像和视频数据的生成方法，但它们忽略了利用本机3D和4D表示的快速增长的工作，例如RGB-D图像，占用网格和LIDAR点云用于大规模场景建模。同时，缺乏对``世界模型''的标准化定义和分类法，导致文献中的主张分散，有时甚至不一致。这项调查通过介绍第一个专门针对3D和4D世界建模和发电的首次全面审查来解决这些差距。我们建立精确的定义，介绍一个结构化分类法，该分类法（基于视频），基于占用率（OCCGEN）和基于激光雷达（Lidar）的方法（Lidargen）方法，并系统地总结了数据集和量身定制的针对3D/4D设置的评估指标。我们进一步讨论实用应用，确定开放挑战并突出有前途的研究方向，旨在为该领域提供连贯和基础的参考。此HTTPS URL可以提供现有文献的系统摘要</li>
</ul>

<h3>Title: MCTED: A Machine-Learning-Ready Dataset for Digital Elevation Model Generation From Mars Imagery</h3>
<ul>
<li><strong>Authors: </strong>Rafał Osadnik, Pablo Gómez, Eleni Bohacek, Rickbir Bahia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.08027">https://arxiv.org/abs/2509.08027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.08027">https://arxiv.org/pdf/2509.08027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.08027]] MCTED: A Machine-Learning-Ready Dataset for Digital Elevation Model Generation From Mars Imagery(https://arxiv.org/abs/2509.08027)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This work presents a new dataset for the Martian digital elevation model prediction task, ready for machine learning applications called MCTED. The dataset has been generated using a comprehensive pipeline designed to process high-resolution Mars orthoimage and DEM pairs from Day et al., yielding a dataset consisting of 80,898 data samples. The source images are data gathered by the Mars Reconnaissance Orbiter using the CTX instrument, providing a very diverse and comprehensive coverage of the Martian surface. Given the complexity of the processing pipelines used in large-scale DEMs, there are often artefacts and missing data points in the original data, for which we developed tools to solve or mitigate their impact. We divide the processed samples into training and validation splits, ensuring samples in both splits cover no mutual areas to avoid data leakage. Every sample in the dataset is represented by the optical image patch, DEM patch, and two mask patches, indicating values that were originally missing or were altered by us. This allows future users of the dataset to handle altered elevation regions as they please. We provide statistical insights of the generated dataset, including the spatial distribution of samples, the distributions of elevation values, slopes and more. Finally, we train a small U-Net architecture on the MCTED dataset and compare its performance to a monocular depth estimation foundation model, DepthAnythingV2, on the task of elevation prediction. We find that even a very small architecture trained on this dataset specifically, beats a zero-shot performance of a depth estimation foundation model like DepthAnythingV2. We make the dataset and code used for its generation completely open source in public repositories.</li>
<li><strong>摘要：</strong>这项工作为火星数字高程模型预测任务提供了一个新的数据集，准备用于机器学习应用程序，称为MCTED。该数据集是使用综合管道生成的，该管道旨在处理Day等人的高分辨率火星矫形图和DEM对，得出由80,898个数据样本组成的数据集。源图像是由火星侦察轨道仪使用CTX仪器收集的数据，提供了对火星表面的非常多样化和全面的覆盖范围。鉴于大规模DEM中使用的处理管道的复杂性，原始数据中通常会有伪像和缺少数据点，我们为其开发了来解决或减轻其影响的工具。我们将处理的样品分为训练和验证分割，确保两种拆分中的样品均不涵盖共同区域以避免数据泄漏。数据集中的每个示例都由光学图像补丁，DEM补丁和两个掩码补丁表示，这表明最初丢失或被我们更改的值表示。这使数据集的未来用户可以根据自己的意愿处理更改的高程区域。我们提供生成数据集的统计见解，包括样品的空间分布，高程值，斜率的分布等。最后，我们在MCTED数据集上训练一个小型的U-NET体系结构，并将其性能与单眼深度估计基础模型DepThanyThingv2进行比较，以提高高程预测的任务。我们发现，即使是在此数据集中训练的非常小的架构，也比DepThanyThingv2这样的深度估计基础模型击败了零拍性能。我们将用于生成的数据集和代码完全在公共存储库中完全开源。</li>
</ul>

<h3>Title: Performance Assessment Strategies for Generative AI Applications in Healthcare</h3>
<ul>
<li><strong>Authors: </strong>Victor Garcia, Mariia Sidulova, Aldo Badano</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.08087">https://arxiv.org/abs/2509.08087</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.08087">https://arxiv.org/pdf/2509.08087</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.08087]] Performance Assessment Strategies for Generative AI Applications in Healthcare(https://arxiv.org/abs/2509.08087)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative artificial intelligence (GenAI) represent an emerging paradigm within artificial intelligence, with applications throughout the medical enterprise. Assessing GenAI applications necessitates a comprehensive understanding of the clinical task and awareness of the variability in performance when implemented in actual clinical environments. Presently, a prevalent method for evaluating the performance of generative models relies on quantitative benchmarks. Such benchmarks have limitations and may suffer from train-to-the-test overfitting, optimizing performance for a specified test set at the cost of generalizability across other task and data distributions. Evaluation strategies leveraging human expertise and utilizing cost-effective computational models as evaluators are gaining interest. We discuss current state-of-the-art methodologies for assessing the performance of GenAI applications in healthcare and medical devices.</li>
<li><strong>摘要：</strong>生成人工智能（Genai）代表了人工智能中的新兴范式，并在整个医疗企业中进行了应用。评估Genai应用需要对在实际临床环境中实施时对临床任务的临床任务和对性能变异性的认识有全面的了解。目前，一种评估生成模型性能的普遍方法依赖于定量基准。这样的基准有局限性，并且可能会遭受火车到测试过度拟合的困扰，以优化指定测试集的性能，而这些测试集以跨其他任务和数据分布的普遍性为代价。利用人类专业知识并利用具有成本效益的计算模型作为评估者的评估策略引起了人们的兴趣。我们讨论了当前的最新方法，以评估Genai在医疗保健和医疗设备中的应用。</li>
</ul>

<h3>Title: APML: Adaptive Probabilistic Matching Loss for Robust 3D Point Cloud Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Sasan Sharifipour, Constantino Álvarez Casado, Mohammad Sabokrou, Miguel Bordallo López</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.08104">https://arxiv.org/abs/2509.08104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.08104">https://arxiv.org/pdf/2509.08104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.08104]] APML: Adaptive Probabilistic Matching Loss for Robust 3D Point Cloud Reconstruction(https://arxiv.org/abs/2509.08104)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Training deep learning models for point cloud prediction tasks such as shape completion and generation depends critically on loss functions that measure discrepancies between predicted and ground-truth point sets. Commonly used functions such as Chamfer Distance (CD), HyperCD, and InfoCD rely on nearest-neighbor assignments, which often induce many-to-one correspondences, leading to point congestion in dense regions and poor coverage in sparse regions. These losses also involve non-differentiable operations due to index selection, which may affect gradient-based optimization. Earth Mover Distance (EMD) enforces one-to-one correspondences and captures structural similarity more effectively, but its cubic computational complexity limits its practical use. We propose the Adaptive Probabilistic Matching Loss (APML), a fully differentiable approximation of one-to-one matching that leverages Sinkhorn iterations on a temperature-scaled similarity matrix derived from pairwise distances. We analytically compute the temperature to guarantee a minimum assignment probability, eliminating manual tuning. APML achieves near-quadratic runtime, comparable to Chamfer-based losses, and avoids non-differentiable operations. When integrated into state-of-the-art architectures (PoinTr, PCN, FoldingNet) on ShapeNet benchmarks and on a spatiotemporal Transformer (CSI2PC) that generates 3D human point clouds from WiFi CSI measurements, APM loss yields faster convergence, superior spatial distribution, especially in low-density regions, and improved or on-par quantitative performance without additional hyperparameter search. The code is available at: this https URL.</li>
<li><strong>摘要：</strong>训练对点云预测任务（例如完成和产生）的深度学习模型急取决于损失功能，这些损失功能衡量了预测点和地面真实点集之间的差异。常用的功能，例如倒角距离（CD），HyperCD和InfoCD依赖于最近的邻居分配，这些分配通常会诱导多一对一的对应关系，从而导致密集区域的点拥塞和稀疏区域的覆盖率不佳。这些损失还涉及由于指数选择而导致的非差异操作，这可能会影响基于梯度的优化。 Earth Mover距离（EMD）强制执行一对一的对应关系，并更有效地捕获结构相似性，但其立方计算复杂性限制了其实际使用。我们提出了自适应概率匹配损失（APML），这是一对一匹配的完全可区分的近似值，以在来自成对距离得出的温度相似性矩阵上利用sindhorn迭代。我们通过分析计算温度，以确保最低分配概率，从而消除手动调整。 APML达到了接近二次的运行时，与基于倒角的损失相当，并避免了非差异性操作。当在Shapenet基准测试和时空变压器（CSI2PC）上集成到最先进的体系结构（POINTR，PCN，FOLLINGNET）时，从WIFI CSI测量中产生3D人点云时，APM损失产生了更快的量子，尤其是在较低的空间范围内，尤其是在低密度的范围内，以及在低密度的范围内进行量子范围或改进的范围，或者在较低密度的范围内进行了改善或改善。该代码可用：此HTTPS URL。</li>
</ul>

<h3>Title: ArtifactGen: Benchmarking WGAN-GP vs Diffusion for Label-Aware EEG Artifact Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Hritik Arasu, Faisal R Jahangiri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.08188">https://arxiv.org/abs/2509.08188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.08188">https://arxiv.org/pdf/2509.08188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.08188]] ArtifactGen: Benchmarking WGAN-GP vs Diffusion for Label-Aware EEG Artifact Synthesis(https://arxiv.org/abs/2509.08188)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Artifacts in electroencephalography (EEG) -- muscle, eye movement, electrode, chewing, and shiver -- confound automated analysis yet are costly to label at scale. We study whether modern generative models can synthesize realistic, label-aware artifact segments suitable for augmentation and stress-testing. Using the TUH EEG Artifact (TUAR) corpus, we curate subject-wise splits and fixed-length multi-channel windows (e.g., 250 samples) with preprocessing tailored to each model (per-window min--max for adversarial training; per-recording/channel $z$-score for diffusion). We compare a conditional WGAN-GP with a projection discriminator to a 1D denoising diffusion model with classifier-free guidance, and evaluate along three axes: (i) fidelity via Welch band-power deltas ($\Delta\delta,\ \Delta\theta,\ \Delta\alpha,\ \Delta\beta$), channel-covariance Frobenius distance, autocorrelation $L_2$, and distributional metrics (MMD/PRD); (ii) specificity via class-conditional recovery with lightweight $k$NN/classifiers; and (iii) utility via augmentation effects on artifact recognition. In our setting, WGAN-GP achieves closer spectral alignment and lower MMD to real data, while both models exhibit weak class-conditional recovery, limiting immediate augmentation gains and revealing opportunities for stronger conditioning and coverage. We release a reproducible pipeline -- data manifests, training configurations, and evaluation scripts -- to establish a baseline for EEG artifact synthesis and to surface actionable failure modes for future work.</li>
<li><strong>摘要：</strong>脑电图（EEG）中的伪影 - 肌肉，眼动，电极，咀嚼和颤抖 - 混淆自动分析，但规模标记为昂贵。我们研究现代生成模型是否可以合成现实的，具有标签感的人工段，适合于增强和应力测试。使用TUH EEG伪影（TUAR）语料库，我们策划主题拆分和固定长度的多通道窗口（例如250个样本），并针对每个型号定制预处理（每种型号）（每次窗口最小训练；用于对抗性训练；每次记录/记录/通道/通道$ z $ -SSCORE，用于扩散）。我们将条件的WGAN-GP与投影区分器进行比较，并通过无分类器的指导进行1D DeNoising扩散模型，并沿三个轴进行评估：（i）通过welch band-power deltas（$ \ delta \ delta，\ \ \ \ \ \ \ \ \ delta \ \ \ \ \ \ \ \ \ alpha \ alpha，\ alpha，\ alpha，\ alpha，\ alpha，\ alpha，\ alpha，\ alpha，\ alpha \ alpha，频道协调FROBENIUS距离，自相关$ L_2 $和分销指标（MMD/PRD）； （ii）通过类条件恢复的特异性，带有轻量级$ k $ nn/分类器； （iii）通过增强对工件识别的影响。在我们的环境中，WGAN-GP实现了更接近的光谱对齐，并且MMD降低了实际数据，而两种模型均表现出较弱的类条件恢复，限制了立即增强的增强，并揭示了更强的调理和覆盖范围的机会。我们发布可再现的管道 - 数据表现，培训配置和评估脚本 - 以建立脑电图合成的基线，并为未来工作的表面可行的故障模式。</li>
</ul>

<h3>Title: GTA-Crime: A Synthetic Dataset and Generation Framework for Fatal Violence Detection with Adversarial Snippet-Level Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Seongho Kim, Sejong Ryu, Hyoukjun You, Je Hyeong Hong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.08232">https://arxiv.org/abs/2509.08232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.08232">https://arxiv.org/pdf/2509.08232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.08232]] GTA-Crime: A Synthetic Dataset and Generation Framework for Fatal Violence Detection with Adversarial Snippet-Level Domain Adaptation(https://arxiv.org/abs/2509.08232)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advancements in video anomaly detection (VAD) have enabled identification of various criminal activities in surveillance videos, but detecting fatal incidents such as shootings and stabbings remains difficult due to their rarity and ethical issues in data collection. Recognizing this limitation, we introduce GTA-Crime, a fatal video anomaly dataset and generation framework using Grand Theft Auto 5 (GTA5). Our dataset contains fatal situations such as shootings and stabbings, captured from CCTV multiview perspectives under diverse conditions including action types, weather, time of day, and viewpoints. To address the rarity of such scenarios, we also release a framework for generating these types of videos. Additionally, we propose a snippet-level domain adaptation strategy using Wasserstein adversarial training to bridge the gap between synthetic GTA-Crime features and real-world features like UCF-Crime. Experimental results validate our GTA-Crime dataset and demonstrate that incorporating GTA-Crime with our domain adaptation strategy consistently enhances real world fatal violence detection accuracy. Our dataset and the data generation framework are publicly available at this https URL.</li>
<li><strong>摘要：</strong>视频异常检测（VAD）的最新进展使监视视频中的各种犯罪活动都可以识别，但是由于数据收集中的稀有性和道德问题，发现诸如枪击和刺伤之类的致命事件（例如枪击和刺伤）仍然很困难。认识到这一限制，我们介绍了使用Grand Theft Auto 5（GTA5）的致命视频异常数据集和发电框架。我们的数据集包含致命情况，例如枪击和刺伤，在各种条件下从CCTV多维图角度捕获，包括动作类型，天气，一天中的时间和观点。为了解决此类情况的稀有性，我们还发布了一个用于生成此类视频的框架。此外，我们提出了使用Wasserstein对抗训练的段级适应策略，以弥合合成GTA犯罪功能与UCF-Crime（例如UCF-Crime）之间的差距。实验结果验证了我们的GTA犯罪数据集，并证明将GTA-Crime与我们的领域适应策略融入始终增强了现实世界的致命暴力检测准确性。我们的数据集和数据生成框架可在此HTTPS URL上公开获得。</li>
</ul>

<h3>Title: Bitrate-Controlled Diffusion for Disentangling Motion and Content in Video</h3>
<ul>
<li><strong>Authors: </strong>Xiao Li, Qi Chen, Xiulian Peng, Kai Yu, Xie Chen, Yan Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.08376">https://arxiv.org/abs/2509.08376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.08376">https://arxiv.org/pdf/2509.08376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.08376]] Bitrate-Controlled Diffusion for Disentangling Motion and Content in Video(https://arxiv.org/abs/2509.08376)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We propose a novel and general framework to disentangle video data into its dynamic motion and static content components. Our proposed method is a self-supervised pipeline with less assumptions and inductive biases than previous works: it utilizes a transformer-based architecture to jointly generate flexible implicit features for frame-wise motion and clip-wise content, and incorporates a low-bitrate vector quantization as an information bottleneck to promote disentanglement and form a meaningful discrete motion space. The bitrate-controlled latent motion and content are used as conditional inputs to a denoising diffusion model to facilitate self-supervised representation learning. We validate our disentangled representation learning framework on real-world talking head videos with motion transfer and auto-regressive motion generation tasks. Furthermore, we also show that our method can generalize to other types of video data, such as pixel sprites of 2D cartoon characters. Our work presents a new perspective on self-supervised learning of disentangled video representations, contributing to the broader field of video analysis and generation.</li>
<li><strong>摘要：</strong>我们提出了一个新颖的一般框架，将视频数据删除到其动态运动和静态内容组件中。我们提出的方法是一种自我审议的管道，其假设和感应性偏见比以前的作品较少：它利用基于变压器的架构来共同为框架运动和剪辑式的内容共同生成灵活的隐式特征，并结合了低比质矢量量化作为信息瓶颈，以促进脱离了限制性范围，并促进了无意义的散发运动空间。比特量控制的潜在运动和内容用作降级扩散模型的条件输入，以促进自我监督的表示。我们通过运动转移和自动回归运动生成任务来验证现实世界中的交谈主视频中的分离表示学习框架。此外，我们还表明，我们的方法可以推广到其他类型的视频数据，例如2D卡通字符的像素精灵。我们的作品介绍了关于解开视频表示的自学学习的新观点，这有助于更广泛的视频分析和发电领域。</li>
</ul>

<h3>Title: Efficient Decoding Methods for Language Models on Encrypted Data</h3>
<ul>
<li><strong>Authors: </strong>Matan Avitan, Moran Baruch, Nir Drucker, Itamar Zimerman, Yoav Goldberg</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.08383">https://arxiv.org/abs/2509.08383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.08383">https://arxiv.org/pdf/2509.08383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.08383]] Efficient Decoding Methods for Language Models on Encrypted Data(https://arxiv.org/abs/2509.08383)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) power modern AI applications, but processing sensitive data on untrusted servers raises privacy concerns. Homomorphic encryption (HE) enables computation on encrypted data for secure inference. However, neural text generation requires decoding methods like argmax and sampling, which are non-polynomial and thus computationally expensive under encryption, creating a significant performance bottleneck. We introduce cutmax, an HE-friendly argmax algorithm that reduces ciphertext operations compared to prior methods, enabling practical greedy decoding under encryption. We also propose the first HE-compatible nucleus (top-p) sampling method, leveraging cutmax for efficient stochastic decoding with provable privacy guarantees. Both techniques are polynomial, supporting efficient inference in privacy-preserving settings. Moreover, their differentiability facilitates gradient-based sequence-level optimization as a polynomial alternative to straight-through estimators. We further provide strong theoretical guarantees for cutmax, proving it converges globally to a unique two-level fixed point, independent of the input values beyond the identity of the maximizer, which explains its rapid convergence in just a few iterations. Evaluations on realistic LLM outputs show latency reductions of 24x-35x over baselines, advancing secure text generation.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）供电现代AI应用程序，但是对不受信任的服务器进行处理敏感数据引起了隐私问题。同态加密（HE）可以对加密数据进行计算以进行安全推断。但是，神经文本生成需要诸如Argmax和采样等解码方法，这些方法是非多项式的，因此在加密下计算昂贵，从而产生了重要的性能瓶颈。我们介绍了CutMax，这是一种与先前的方法相比，它可以减少密文操作，从而降低了加密的实际贪婪解码。我们还提出了第一个与He兼容的核（TOP-P）采样方法，利用Cutmax进行有效的随机解码，并提供可证明的隐私保证。这两种技术都是多项式的，可以在隐私设置中有效推断。此外，它们的可不同性促进了基于梯度的序列级优化，作为直通估计器的多项式替代方案。我们进一步为Cutmax提供了强大的理论保证，证明了它在全球范围内融合到独特的两级固定点，而与最大化器的身份相比，与输入值无关，这解释了其在几次迭代中的快速收敛。对现实的LLM输出的评估显示，基准的潜伏期减少了24 x-35x，从而推进了安全的文本生成。</li>
</ul>

<h3>Title: VRAE: Vertical Residual Autoencoder for License Plate Denoising and Deblurring</h3>
<ul>
<li><strong>Authors: </strong>Cuong Nguyen, Dung T. Tran, Hong Nguyen, Xuan-Vu Phan, Nam-Phong Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.08392">https://arxiv.org/abs/2509.08392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.08392">https://arxiv.org/pdf/2509.08392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.08392]] VRAE: Vertical Residual Autoencoder for License Plate Denoising and Deblurring(https://arxiv.org/abs/2509.08392)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In real-world traffic surveillance, vehicle images captured under adverse weather, poor lighting, or high-speed motion often suffer from severe noise and blur. Such degradations significantly reduce the accuracy of license plate recognition systems, especially when the plate occupies only a small region within the full vehicle image. Restoring these degraded images a fast realtime manner is thus a crucial pre-processing step to enhance recognition performance. In this work, we propose a Vertical Residual Autoencoder (VRAE) architecture designed for the image enhancement task in traffic surveillance. The method incorporates an enhancement strategy that employs an auxiliary block, which injects input-aware features at each encoding stage to guide the representation learning process, enabling better general information preservation throughout the network compared to conventional autoencoders. Experiments on a vehicle image dataset with visible license plates demonstrate that our method consistently outperforms Autoencoder (AE), Generative Adversarial Network (GAN), and Flow-Based (FB) approaches. Compared with AE at the same depth, it improves PSNR by about 20\%, reduces NMSE by around 50\%, and enhances SSIM by 1\%, while requiring only a marginal increase of roughly 1\% in parameters.</li>
<li><strong>摘要：</strong>在现实世界中的交通监视中，在不利天气，阳光差或高速运动下捕获的车辆图像通常会遭受严重的噪音和模糊。这种降解会大大降低车牌识别系统的准确性，尤其是当板在完整车辆图像中仅占据一小部分时。因此，恢复这些退化的图像快速实时方式是提高识别性能的关键预处理步骤。在这项工作中，我们提出了一个垂直剩余自动编码器（VRAE）架构，旨在在交通监视中进行图像增强任务。该方法结合了采用辅助块的增强策略，该策略在每个编码阶段都注入输入感知功能，以指导表示过程，与常规自动编码器相比，在整个网络中可以更好地保存一般信息。带有可见车牌的车辆图像数据集上的实验表明，我们的方法始终超过自动编码器（AE），生成对抗网络（GAN）和基于流（FB）的方法。与AE相同的深度相比，它可以将PSNR提高约20 \％，将NMSE降低约50 \％，并使SSIM提高1 \％，同时仅需要大约1 \％的参数增加。</li>
</ul>

<h3>Title: Spherical Brownian Bridge Diffusion Models for Conditional Cortical Thickness Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Ivan Stoyanov, Fabian Bongratz, Christian Wachinger</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.08442">https://arxiv.org/abs/2509.08442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.08442">https://arxiv.org/pdf/2509.08442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.08442]] Spherical Brownian Bridge Diffusion Models for Conditional Cortical Thickness Forecasting(https://arxiv.org/abs/2509.08442)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Accurate forecasting of individualized, high-resolution cortical thickness (CTh) trajectories is essential for detecting subtle cortical changes, providing invaluable insights into neurodegenerative processes and facilitating earlier and more precise intervention strategies. However, CTh forecasting is a challenging task due to the intricate non-Euclidean geometry of the cerebral cortex and the need to integrate multi-modal data for subject-specific predictions. To address these challenges, we introduce the Spherical Brownian Bridge Diffusion Model (SBDM). Specifically, we propose a bidirectional conditional Brownian bridge diffusion process to forecast CTh trajectories at the vertex level of registered cortical surfaces. Our technical contribution includes a new denoising model, the conditional spherical U-Net (CoS-UNet), which combines spherical convolutions and dense cross-attention to integrate cortical surfaces and tabular conditions seamlessly. Compared to previous approaches, SBDM achieves significantly reduced prediction errors, as demonstrated by our experiments based on longitudinal datasets from the ADNI and OASIS. Additionally, we demonstrate SBDM's ability to generate individual factual and counterfactual CTh trajectories, offering a novel framework for exploring hypothetical scenarios of cortical development.</li>
<li><strong>摘要：</strong>对个性化的高分辨率皮质厚度（CTH）轨迹的准确预测对于检测微妙的皮质变化至关重要，为神经退行性过程提供了宝贵的见解，并促进了更早，更精确的干预策略。但是，由于复杂的大脑皮层几何学几何形状，CTH预测是一项具有挑战性的任务，并且需要整合多模式数据以进行特定于主题的预测。为了应对这些挑战，我们介绍了球形布朗桥扩散模型（SBDM）。具体而言，我们提出了一个双向条件的布朗桥扩散过程，以预测注册皮层表面的顶点水平的CTH轨迹。我们的技术贡献包括一个新的Denoising模型，即条件的球形U-NET（COS-UNET），该模型结合了球形卷积和密集的跨注意，以整合皮质表面和表面条件。与以前的方法相比，SBDM显着减少了预测误差，这是我们基于ADNI和OASIS的纵向数据集的实验所证明的。此外，我们展示了SBDM产生个人事实和反事实CTH轨迹的能力，为探索皮质发展的假设情景提供了新的框架。</li>
</ul>

<h3>Title: First-order State Space Model for Lightweight Image Super-resolution</h3>
<ul>
<li><strong>Authors: </strong>Yujie Zhu, Xinyi Zhang, Yekai Lu, Guang Yang, Faming Fang, Guixu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.08458">https://arxiv.org/abs/2509.08458</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.08458">https://arxiv.org/pdf/2509.08458</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.08458]] First-order State Space Model for Lightweight Image Super-resolution(https://arxiv.org/abs/2509.08458)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>State space models (SSMs), particularly Mamba, have shown promise in NLP tasks and are increasingly applied to vision tasks. However, most Mamba-based vision models focus on network architecture and scan paths, with little attention to the SSM module. In order to explore the potential of SSMs, we modified the calculation process of SSM without increasing the number of parameters to improve the performance on lightweight super-resolution tasks. In this paper, we introduce the First-order State Space Model (FSSM) to improve the original Mamba module, enhancing performance by incorporating token correlations. We apply a first-order hold condition in SSMs, derive the new discretized form, and analyzed cumulative error. Extensive experimental results demonstrate that FSSM improves the performance of MambaIR on five benchmark datasets without additionally increasing the number of parameters, and surpasses current lightweight SR methods, achieving state-of-the-art results.</li>
<li><strong>摘要：</strong>国家空间模型（SSM），尤其是MAMBA，已在NLP任务中显示出希望，并越来越多地应用于视觉任务。但是，大多数基于MAMBA的视觉模型都集中在网络体系结构和扫描路径上，而很少关注SSM模块。为了探索SSM的潜力，我们修改了SSM的计算过程，而无需增加参数的数量以提高轻量级超级分辨率任务的性能。在本文中，我们介绍了一阶状态空间模型（FSSM），以改善原始的Mamba模块，从而通过结合令牌相关性来提高性能。我们在SSM中应用一阶保持条件，得出新的离散形式，并分析了累积错误。广泛的实验结果表明，FSSM在五个基准数据集上提高了Mambair的性能，而无需增加参数的数量，并超过了当前的轻质SR方法，从而实现了最新的结果。</li>
</ul>

<h3>Title: Prompt-Driven Image Analysis with Multimodal Generative AI: Detection, Segmentation, Inpainting, and Interpretation</h3>
<ul>
<li><strong>Authors: </strong>Kaleem Ahmad</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.08489">https://arxiv.org/abs/2509.08489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.08489">https://arxiv.org/pdf/2509.08489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.08489]] Prompt-Driven Image Analysis with Multimodal Generative AI: Detection, Segmentation, Inpainting, and Interpretation(https://arxiv.org/abs/2509.08489)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Prompt-driven image analysis converts a single natural-language instruction into multiple steps: locate, segment, edit, and describe. We present a practical case study of a unified pipeline that combines open-vocabulary detection, promptable segmentation, text-conditioned inpainting, and vision-language description into a single workflow. The system works end to end from a single prompt, retains intermediate artifacts for transparent debugging (such as detections, masks, overlays, edited images, and before and after composites), and provides the same functionality through an interactive UI and a scriptable CLI for consistent, repeatable runs. We highlight integration choices that reduce brittleness, including threshold adjustments, mask inspection with light morphology, and resource-aware defaults. In a small, single-word prompt segment, detection and segmentation produced usable masks in over 90% of cases with an accuracy above 85% based on our criteria. On a high-end GPU, inpainting makes up 60 to 75% of total runtime under typical guidance and sampling settings, which highlights the need for careful tuning. The study offers implementation-guided advice on thresholds, mask tightness, and diffusion parameters, and details version pinning, artifact logging, and seed control to support replay. Our contribution is a transparent, reliable pattern for assembling modern vision and multimodal models behind a single prompt, with clear guardrails and operational practices that improve reliability in object replacement, scene augmentation, and removal.</li>
<li><strong>摘要：</strong>及时驱动的图像分析将单个自然语言指令转换为多个步骤：定位，细分，编辑和描述。我们提出了一项统一管道的实用案例研究，该案例研究结合了开放式摄影检测，迅速的分割，文本条件的介入以及视觉语言描述为单个工作流程。该系统从单个提示下起作用，保留中间文物用于透明调试（例如检测，掩模，覆盖，覆盖，编辑的图像以及复合材料之前和之后），并通过交互式UI和可脚本的UI和可脚本的CLI提供相同的功能，以进行一致的重复运行。我们重点介绍了降低脆性的集成选择，包括阈值调整，带有光形态的掩盖检查和资源感知默认值。根据我们的标准，在一个较小的单词及时细分市场中，在90％的病例中产生了90％以上的案例，根据我们的标准，精度高于85％。在高端GPU上，在典型的指导和采样设置下占总运行时的60％至75％，这突出了需要仔细调整的需求。该研究提供了有关阈值，面膜紧密度和扩散参数的实施指导建议，以及详细信息版本固定，文物记录和种子控制以支持重播。我们的贡献是一种透明，可靠的模式，用于在单个提示背后组装现代愿景和多模型，并具有清晰的护栏和操作实践，可提高对象更换，场景增强和拆卸的可靠性。</li>
</ul>

<h3>Title: A Structured Review of Underwater Object Detection Challenges and Solutions: From Traditional to Large Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Edwine Nabahirwa, Wei Song, Minghua Zhang, Yi Fang, Zhou Ni</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.08490">https://arxiv.org/abs/2509.08490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.08490">https://arxiv.org/pdf/2509.08490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.08490]] A Structured Review of Underwater Object Detection Challenges and Solutions: From Traditional to Large Vision Language Models(https://arxiv.org/abs/2509.08490)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Underwater object detection (UOD) is vital to diverse marine applications, including oceanographic research, underwater robotics, and marine conservation. However, UOD faces numerous challenges that compromise its performance. Over the years, various methods have been proposed to address these issues, but they often fail to fully capture the complexities of underwater environments. This review systematically categorizes UOD challenges into five key areas: Image quality degradation, target-related issues, data-related challenges, computational and processing constraints, and limitations in detection methodologies. To address these challenges, we analyze the progression from traditional image processing and object detection techniques to modern approaches. Additionally, we explore the potential of large vision-language models (LVLMs) in UOD, leveraging their multi-modal capabilities demonstrated in other domains. We also present case studies, including synthetic dataset generation using DALL-E 3 and fine-tuning Florence-2 LVLM for UOD. This review identifies three key insights: (i) Current UOD methods are insufficient to fully address challenges like image degradation and small object detection in dynamic underwater environments. (ii) Synthetic data generation using LVLMs shows potential for augmenting datasets but requires further refinement to ensure realism and applicability. (iii) LVLMs hold significant promise for UOD, but their real-time application remains under-explored, requiring further research on optimization techniques.</li>
<li><strong>摘要：</strong>水下对象检测（UOD）对于包括海洋学研究，水下机器人技术和海洋保护在内的各种海洋应用至关重要。但是，UOD面临损害其性能的许多挑战。多年来，已经提出了各种方法来解决这些问题，但是它们通常无法完全捕捉水下环境的复杂性。这篇综述将UOD挑战分为五个关键领域：图像质量退化，与目标有关的问题，与数据相关的挑战，计算和处理限制以及检测方法的局限性。为了应对这些挑战，我们分析了从传统图像处理和对象检测技术到现代方法的发展。此外，我们探索了UOD中大型视觉模型（LVLM）的潜力，利用其在其他域中证明的多模式功能。我们还提出了案例研究，包括使用DALL-E 3的合成数据集生成，以及用于UOD的佛罗伦萨-2 LVLM。本综述确定了三个关键见解：（i）当前的UOD方法不足以完全解决诸如动态水下环境中图像降解和小物体检测之类的挑战。 （ii）使用LVLMS的合成数据生成显示了增加数据集的潜力，但需要进一步改进以确保现实主义和适用性。 （iii）LVLMS对UOD具有巨大的希望，但是它们的实时应用程序仍未探索，需要进一步研究优化技术。</li>
</ul>

<h3>Title: Variational Rank Reduction Autoencoders for Generative</h3>
<ul>
<li><strong>Authors: </strong>Alicia Tierz, Jad Mounayer, Beatriz Moya, Francisco Chinesta</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.08515">https://arxiv.org/abs/2509.08515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.08515">https://arxiv.org/pdf/2509.08515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.08515]] Variational Rank Reduction Autoencoders for Generative(https://arxiv.org/abs/2509.08515)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative thermal design for complex geometries is fundamental in many areas of engineering, yet it faces two main challenges: the high computational cost of high-fidelity simulations and the limitations of conventional generative models. Approaches such as autoencoders (AEs) and variational autoencoders (VAEs) often produce unstructured latent spaces with discontinuities, which restricts their capacity to explore designs and generate physically consistent solutions. To address these limitations, we propose a hybrid framework that combines Variational Rank-Reduction Autoencoders (VRRAEs) with Deep Operator Networks (DeepONets). The VRRAE introduces a truncated SVD within the latent space, leading to continuous, interpretable, and well-structured representations that mitigate posterior collapse and improve geometric reconstruction. The DeepONet then exploits this compact latent encoding in its branch network, together with spatial coordinates in the trunk network, to predict temperature gradients efficiently and accurately. This hybrid approach not only enhances the quality of generated geometries and the accuracy of gradient prediction, but also provides a substantial advantage in inference efficiency compared to traditional numerical solvers. Overall, the study underscores the importance of structured latent representations for operator learning and highlights the potential of combining generative models and operator networks in thermal design and broader engineering applications.</li>
<li><strong>摘要：</strong>复杂几何形状的生成热设计在工程的许多领域都是至关重要的，但是它面临两个主要挑战：高保真模拟的高计算成本和常规生成模型的局限性。诸如自动编码器（AES）和各种自动编码器（VAE）之类的方法通常会产生与不连续性的非结构化潜在空间，这限制了其探索设计和生成物理上一致的解决方案的能力。为了解决这些局限性，我们提出了一个混合框架，将各种排名降级自动编码器（VRRAES）与深操作员网络（DeepOnets）相结合。 VRRAE在潜在空间内引入了截断的SVD，导致连续，可解释且结构良好的表示形式减轻后层并改善几何重建。然后，deponet在其分支网络中利用了这种紧凑的潜在编码，以及中继网络中的空间坐标，以有效，准确地预测温度梯度。这种混合方法不仅提高了生成的几何形状质量和梯度预测的准确性，而且与传统的数值求解器相比，推断效率具有很大的优势。总体而言，该研究强调了结构化潜在表示对操作员学习的重要性，并突出了将生成模型和运营商网络组合到热设计和更广泛的工程应用中的潜力。</li>
</ul>

<h3>Title: HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning</h3>
<ul>
<li><strong>Authors: </strong>Liyang Chen, Tianxiang Ma, Jiawei Liu, Bingchuan Li, Zhuowei Chen, Lijie Liu, Xu He, Gen Li, Qian He, Zhiyong Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.08519">https://arxiv.org/abs/2509.08519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.08519">https://arxiv.org/pdf/2509.08519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.08519]] HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning(https://arxiv.org/abs/2509.08519)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Human-Centric Video Generation (HCVG) methods seek to synthesize human videos from multimodal inputs, including text, image, and audio. Existing methods struggle to effectively coordinate these heterogeneous modalities due to two challenges: the scarcity of training data with paired triplet conditions and the difficulty of collaborating the sub-tasks of subject preservation and audio-visual sync with multimodal inputs. In this work, we present HuMo, a unified HCVG framework for collaborative multimodal control. For the first challenge, we construct a high-quality dataset with diverse and paired text, reference images, and audio. For the second challenge, we propose a two-stage progressive multimodal training paradigm with task-specific strategies. For the subject preservation task, to maintain the prompt following and visual generation abilities of the foundation model, we adopt the minimal-invasive image injection strategy. For the audio-visual sync task, besides the commonly adopted audio cross-attention layer, we propose a focus-by-predicting strategy that implicitly guides the model to associate audio with facial regions. For joint learning of controllabilities across multimodal inputs, building on previously acquired capabilities, we progressively incorporate the audio-visual sync task. During inference, for flexible and fine-grained multimodal control, we design a time-adaptive Classifier-Free Guidance strategy that dynamically adjusts guidance weights across denoising steps. Extensive experimental results demonstrate that HuMo surpasses specialized state-of-the-art methods in sub-tasks, establishing a unified framework for collaborative multimodal-conditioned HCVG. Project Page: this https URL.</li>
<li><strong>摘要：</strong>以人为中心的视频生成（HCVG）方法旨在从多模式输入（包括文本，图像和音频）中综合人类视频。由于两个挑战，现有方法难以有效地协调这些异质方式：缺乏配对三重态条件的训练数据以及与多型输入的主题保存和视听同步的子任务的难度。在这项工作中，我们提出了Humo，这是一个统一的HCVG合作多模式控制的框架。对于第一个挑战，我们构建了一个具有多种和配对的文本，参考图像和音频的高质量数据集。对于第二个挑战，我们提出了具有特定于任务策略的两阶段渐进多模式训练范式。对于主题保存任务，要保持基础模型的及时关注和视觉产生能力，我们采用了最小的侵入性图像注入策略。对于视听同步任务，除了通常采用的音频跨注意层外，我们提出了一种逐个预测的策略，该策略隐含地指导该模型将音频与面部区域联系起来。为了在以前获得的功能为基础的基于多模式输入的控制能力的联合学习，我们逐步合并了视听同步任务。在推断期间，对于灵活且细粒度的多模式控制，我们设计了一种时间自适应分类器的指导策略，该策略会动态调整跨剥离步骤的指导权重。广泛的实验结果表明，Humo超过了子任务中的专业最先进方法，为协作多模式条件HCVG建立了统一的框架。项目页面：此HTTPS URL。</li>
</ul>

<h3>Title: Implicit Shape-Prior for Few-Shot Assisted 3D Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Mathilde Monvoisin, Louise Piecuch, Blanche Texier, Cédric Hémon, Anaïs Barateau, Jérémie Huet, Antoine Nordez, Anne-Sophie Boureau, Jean-Claude Nunes, Diana Mateus</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.08580">https://arxiv.org/abs/2509.08580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.08580">https://arxiv.org/pdf/2509.08580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.08580]] Implicit Shape-Prior for Few-Shot Assisted 3D Segmentation(https://arxiv.org/abs/2509.08580)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The objective of this paper is to significantly reduce the manual workload required from medical professionals in complex 3D segmentation tasks that cannot be yet fully automated. For instance, in radiotherapy planning, organs at risk must be accurately identified in computed tomography (CT) or magnetic resonance imaging (MRI) scans to ensure they are spared from harmful radiation. Similarly, diagnosing age-related degenerative diseases such as sarcopenia, which involve progressive muscle volume loss and strength, is commonly based on muscular mass measurements often obtained from manual segmentation of medical volumes. To alleviate the manual-segmentation burden, this paper introduces an implicit shape prior to segment volumes from sparse slice manual annotations generalized to the multi-organ case, along with a simple framework for automatically selecting the most informative slices to guide and minimize the next interactions. The experimental validation shows the method's effectiveness on two medical use cases: assisted segmentation in the context of at risks organs for brain cancer patients, and acceleration of the creation of a new database with unseen muscle shapes for patients with sarcopenia.</li>
<li><strong>摘要：</strong>本文的目的是显着减少复杂的3D分割任务中医疗专业人员所需的手动工作量，这些任务尚未完全自动化。例如，在放射疗法计划中，必须在计算机断层扫描（CT）或磁共振成像（MRI）扫描中准确识别有风险的器官，以确保免于有害辐射。同样，诊断与年龄相关的退行性疾病（例如肌肉减少症）涉及进行性肌肉体积损失和强度，通常基于通常是从手动分割医疗体积获得的肌肉质量测量值。为了减轻手动分割负担，本文在段落量中引入了隐式形状，该片段是从稀疏的切片手动注释中推广到多器官案例的，以及一个简单的框架，以自动选择最有用的切片来指导和最小化下一次相互作用。实验验证显示了该方法对两种医学用例的有效性：在脑癌患者的风险器官的背景下进行辅助分割，并加速了创建具有肌肉减少症患者的新数据库具有未看到的肌肉形状的新数据库。</li>
</ul>

<h3>Title: Generative Data Refinement: Just Ask for Better Data</h3>
<ul>
<li><strong>Authors: </strong>Minqi Jiang, João G. M. Araújo, Will Ellsworth, Sian Gooding, Edward Grefenstette</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.08653">https://arxiv.org/abs/2509.08653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.08653">https://arxiv.org/pdf/2509.08653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.08653]] Generative Data Refinement: Just Ask for Better Data(https://arxiv.org/abs/2509.08653)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>For a fixed parameter size, the capabilities of large models are primarily determined by the quality and quantity of its training data. Consequently, training datasets now grow faster than the rate at which new data is indexed on the web, leading to projected data exhaustion over the next decade. Much more data exists as user-generated content that is not publicly indexed, but incorporating such data comes with considerable risks, such as leaking private information and other undesirable content. We introduce a framework, Generative Data Refinement (GDR), for using pretrained generative models to transform a dataset with undesirable content into a refined dataset that is more suitable for training. Our experiments show that GDR can outperform industry-grade solutions for dataset anonymization, as well as enable direct detoxification of highly unsafe datasets. Moreover, we show that by generating synthetic data that is conditioned on each example in the real dataset, GDR's refined outputs naturally match the diversity of web scale datasets, and thereby avoid the often challenging task of generating diverse synthetic data via model prompting. The simplicity and effectiveness of GDR make it a powerful tool for scaling up the total stock of training data for frontier models.</li>
<li><strong>摘要：</strong>对于固定参数大小，大型模型的功能主要取决于其训练数据的质量和数量。因此，培训数据集现在增长的速度快于网络上索引的新数据的速度，这导致了未来十年的数据耗尽。作为用户生成的内容存在更多的数据，这些数据没有公开索引，但是包含此类数据的数据具有很大的风险，例如泄漏私人信息和其他不良内容。我们介绍了一个框架，生成数据改进（GDR），用于使用验证的生成模型将具有不良内容的数据集转换为更适合培训的精制数据集。我们的实验表明，GDR可以胜过数据集匿名化的行业级解决方案，并启用高度不安全数据集的直接排毒。此外，我们表明，通过生成在真实数据集中每个示例上进行条件的合成数据，GDR的精制输出自然与Web Scale数据集的多样性匹配，从而避免通过模型提示来生成多样化的合成数据。 GDR的简单性和有效性使其成为扩大Frontier模型的培训数据总库存的强大工具。</li>
</ul>

<h3>Title: Replicable Reinforcement Learning with Linear Function Approximation</h3>
<ul>
<li><strong>Authors: </strong>Eric Eaton, Marcel Hussing, Michael Kearns, Aaron Roth, Sikata Bela Sengupta, Jessica Sorrell</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.08660">https://arxiv.org/abs/2509.08660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.08660">https://arxiv.org/pdf/2509.08660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.08660]] Replicable Reinforcement Learning with Linear Function Approximation(https://arxiv.org/abs/2509.08660)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Replication of experimental results has been a challenge faced by many scientific disciplines, including the field of machine learning. Recent work on the theory of machine learning has formalized replicability as the demand that an algorithm produce identical outcomes when executed twice on different samples from the same distribution. Provably replicable algorithms are especially interesting for reinforcement learning (RL), where algorithms are known to be unstable in practice. While replicable algorithms exist for tabular RL settings, extending these guarantees to more practical function approximation settings has remained an open problem. In this work, we make progress by developing replicable methods for linear function approximation in RL. We first introduce two efficient algorithms for replicable random design regression and uncentered covariance estimation, each of independent interest. We then leverage these tools to provide the first provably efficient replicable RL algorithms for linear Markov decision processes in both the generative model and episodic settings. Finally, we evaluate our algorithms experimentally and show how they can inspire more consistent neural policies.</li>
<li><strong>摘要：</strong>许多科学学科（包括机器学习领域）面临的实验结果复制一直是一个挑战。关于机器学习理论的最新工作已将重复性形式化为要求算法在同一分布的不同样本上执行两次时，算法产生相同的结果。可证明可复制的算法对于增强学习（RL）特别有趣，在实践中已知算法是不稳定的。尽管存在用于表格RL设置的可复制算法，但将这些保证将这些保证扩展到更实际的函数近似设置仍然是一个开放的问题。在这项工作中，我们通过开发可复制的方法来实现RL线性函数近似的方法。我们首先引入了两种有效的算法，用于可复制的随机设计回归和无独立的协方差估计。然后，我们利用这些工具为生成模型和情节设置中的线性马尔可夫决策过程提供了第一个可证明有效的可复制RL算法。最后，我们通过实验评估我们的算法，并展示它们如何激发更一致的神经政策。</li>
</ul>

<h3>Title: Data-driven generative simulation of SDEs using diffusion models</h3>
<ul>
<li><strong>Authors: </strong>Xuefeng Gao, Jiale Zha, Xun Yu Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.08731">https://arxiv.org/abs/2509.08731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.08731">https://arxiv.org/pdf/2509.08731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.08731]] Data-driven generative simulation of SDEs using diffusion models(https://arxiv.org/abs/2509.08731)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper introduces a new approach to generating sample paths of unknown stochastic differential equations (SDEs) using diffusion models, a class of generative AI models commonly employed in image and video applications. Unlike the traditional Monte Carlo methods for simulating SDEs, which require explicit specifications of the drift and diffusion coefficients, our method takes a model-free, data-driven approach. Given a finite set of sample paths from an SDE, we utilize conditional diffusion models to generate new, synthetic paths of the same SDE. To demonstrate the effectiveness of our approach, we conduct a simulation experiment to compare our method with alternative benchmark ones including neural SDEs. Furthermore, in an empirical study we leverage these synthetically generated sample paths to enhance the performance of reinforcement learning algorithms for continuous-time mean-variance portfolio selection, hinting promising applications of diffusion models in financial analysis and decision-making.</li>
<li><strong>摘要：</strong>本文介绍了一种使用扩散模型（图像和视频应用程序中常用的一类生成AI模型）生成未知随机微分方程（SDE）的样本路径的新方法。与模拟SDE的传统蒙特卡洛方法不同，该方法需要显式的漂移和扩散系数规格，我们的方法采用了一种无模型，数据驱动的方法。给定来自SDE的有限样品路径集，我们利用条件扩散模型来生成同一SDE的新的合成路径。为了证明我们方法的有效性，我们进行了一个模拟实验，将我们的方法与包括神经SDE的替代基准测试方法进行了比较。此外，在一项实证研究中，我们利用这些合成生成的样本路径来增强增强学习算法的性能，以进行连续的时均值变化投资组合选择，并暗示了扩散模型在财务分析和决策中的有希望的应用。</li>
</ul>

<h3>Title: AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhiheng Xi, Jixuan Huang, Chenyang Liao, Baodai Huang, Honglin Guo, Jiaqi Liu, Rui Zheng, Junjie Ye, Jiazheng Zhang, Wenxiang Chen, Wei He, Yiwen Ding, Guanyu Li, Zehui Chen, Zhengyin Du, Xuesong Yao, Yufei Xu, Jiecao Chen, Tao Gui, Zuxuan Wu, Qi Zhang, Xuanjing Huang, Yu-Gang Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.08755">https://arxiv.org/abs/2509.08755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.08755">https://arxiv.org/pdf/2509.08755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.08755]] AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning(https://arxiv.org/abs/2509.08755)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Developing autonomous LLM agents capable of making a series of intelligent decisions to solve complex, real-world tasks is a fast-evolving frontier. Like human cognitive development, agents are expected to acquire knowledge and skills through exploration and interaction with the environment. Despite advances, the community still lacks a unified, interactive reinforcement learning (RL) framework that can effectively train such agents from scratch -- without relying on supervised fine-tuning (SFT) -- across diverse and realistic environments. To bridge this gap, we introduce AgentGym-RL, a new framework to train LLM agents for multi-turn interactive decision-making through RL. The framework features a modular and decoupled architecture, ensuring high flexibility and extensibility. It encompasses a wide variety of real-world scenarios, and supports mainstream RL algorithms. Furthermore, we propose ScalingInter-RL, a training approach designed for exploration-exploitation balance and stable RL optimization. In early stages, it emphasizes exploitation by restricting the number of interactions, and gradually shifts towards exploration with larger horizons to encourage diverse problem-solving strategies. In this way, the agent develops more diverse behaviors and is less prone to collapse under long horizons. We perform extensive experiments to validate the stability and effectiveness of both the AgentGym-RL framework and the ScalingInter-RL approach. Our agents match or surpass commercial models on 27 tasks across diverse environments. We offer key insights and will open-source the complete AgentGym-RL framework -- including code and datasets -- to empower the research community in developing the next generation of intelligent agents.</li>
<li><strong>摘要：</strong>开发能够做出一系列智能决策来解决复杂的现实世界任务的自主LLM代理是一个快速发展的前沿。像人类的认知发展一样，预计代理人将通过探索和与环境互动来获得知识和技能。尽管有进步，社区仍然缺乏统一的互动加强学习（RL）框架，该框架可以在不依赖各种环境的环境中有效地从头开始训练此类代理 - 而无需依靠监督的微调（SFT）。为了弥合这一差距，我们介绍了Agengym-RL，这是一个新的框架，用于通过RL培训LLM代理，以通过RL进行多转变的互动决策。该框架具有模块化和脱钩的体系结构，可确保高灵活性和可扩展性。它涵盖了各种各样的现实情况，并支持主流RL算法。此外，我们提出了缩放量-RL，这是一种训练方法，旨在探索探索平衡和稳定的RL优化。在早期阶段，它通过限制相互作用的数量来强调剥削，并逐渐向更大的视野转向探索，以鼓励各种各样的解决问题的策略。这样，代理会发展出更加多样化的行为，并且在远距离下不容易崩溃。我们执行广泛的实验，以验证AgensGyM-RL框架和ScaingInter-RL方法的稳定性和有效性。我们的代理商匹配或超过各种环境的27个任务的商业模型。我们提供关键的见解，并将开源完整的AgentGyM-RL框架（包括代码和数据集），以增强研究社区的能力，以开发下一代的智能代理。</li>
</ul>

<h3>Title: Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles</h3>
<ul>
<li><strong>Authors: </strong>Eric Slyman, Mehrab Tanjim, Kushal Kafle, Stefan Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.08777">https://arxiv.org/abs/2509.08777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.08777">https://arxiv.org/pdf/2509.08777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.08777]] Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles(https://arxiv.org/abs/2509.08777)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) are increasingly used to evaluate text-to-image (TTI) generation systems, providing automated judgments based on visual and textual context. However, these "judge" models often suffer from biases, overconfidence, and inconsistent performance across diverse image domains. While prompt ensembling has shown promise for mitigating these issues in unimodal, text-only settings, our experiments reveal that standard ensembling methods fail to generalize effectively for TTI tasks. To address these limitations, we propose a new multimodal-aware method called Multimodal Mixture-of-Bayesian Prompt Ensembles (MMB). Our method uses a Bayesian prompt ensemble approach augmented by image clustering, allowing the judge to dynamically assign prompt weights based on the visual characteristics of each sample. We show that MMB improves accuracy in pairwise preference judgments and greatly enhances calibration, making it easier to gauge the judge's true uncertainty. In evaluations on two TTI benchmarks, HPSv2 and MJBench, MMB outperforms existing baselines in alignment with human annotations and calibration across varied image content. Our findings highlight the importance of multimodal-specific strategies for judge calibration and suggest a promising path forward for reliable large-scale TTI evaluation.</li>
<li><strong>摘要：</strong>多模式大语模型（MLLM）越来越多地用于评估文本形象（TTI）生成系统，从而基于视觉和文本上下文提供自动判断。但是，这些“法官”模型经常遭受偏见，过度自信以及各种图像领域的性能不一致。虽然迅速的结合已显示出在单峰，仅文本设置中缓解这些问题的希望，但我们的实验表明，标准的结合方法无法有效地概括为TTI任务。为了解决这些限制，我们提出了一种新的多模式感知方法，称为多模式混合物及时及时提示合奏（MMB）。我们的方法使用图像群集增强的贝叶斯提示合奏方法，使法官可以根据每个样本的视觉特征动态分配及时的权重。我们表明，MMB提高了成对偏好判断的准确性，并大大增强了校准，从而更容易评估法官的真正不确定性。在对HPSV2和MJBENCH的两个TTI基准测试的评估中，MMB的表现优于现有基准，与人类注释和各种图像内容之间的校准相符。我们的发现突出了多模式特异性策略对法官校准的重要性，并提出了可靠的大规模TTI评估的有前途的前进道路。</li>
</ul>

<h3>Title: GeneVA: A Dataset of Human Annotations for Generative Text to Video Artifacts</h3>
<ul>
<li><strong>Authors: </strong>Jenna Kang, Maria Silva, Patsorn Sangkloy, Kenneth Chen, Niall Williams, Qi Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.08818">https://arxiv.org/abs/2509.08818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.08818">https://arxiv.org/pdf/2509.08818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.08818]] GeneVA: A Dataset of Human Annotations for Generative Text to Video Artifacts(https://arxiv.org/abs/2509.08818)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in probabilistic generative models have extended capabilities from static image synthesis to text-driven video generation. However, the inherent randomness of their generation process can lead to unpredictable artifacts, such as impossible physics and temporal inconsistency. Progress in addressing these challenges requires systematic benchmarks, yet existing datasets primarily focus on generative images due to the unique spatio-temporal complexities of videos. To bridge this gap, we introduce GeneVA, a large-scale artifact dataset with rich human annotations that focuses on spatio-temporal artifacts in videos generated from natural text prompts. We hope GeneVA can enable and assist critical applications, such as benchmarking model performance and improving generative video quality.</li>
<li><strong>摘要：</strong>概率生成模型的最新进展具有从静态图像综合到文本驱动的视频生成的扩展功能。但是，其生成过程的固有随机性可能导致不可预测的人工制品，例如不可能的物理学和时间不一致。解决这些挑战的进展需要系统的基准测试，但由于视频的独特时空复杂性，现有的数据集主要集中在生成图像上。为了弥合这一差距，我们介绍了Geneva，这是一个具有丰富人类注释的大规模文物数据集，重点是从自然文本提示产生的视频中的时空伪影。我们希望日内瓦能够启用和协助关键应用，例如基准测试模型性能和提高生成视频质量。</li>
</ul>

<h3>Title: RewardDance: Reward Scaling in Visual Generation</h3>
<ul>
<li><strong>Authors: </strong>Jie Wu, Yu Gao, Zilyu Ye, Ming Li, Liang Li, Hanzhong Guo, Jie Liu, Zeyue Xue, Xiaoxia Hou, Wei Liu, Yan Zeng, Weilin Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.08826">https://arxiv.org/abs/2509.08826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.08826">https://arxiv.org/pdf/2509.08826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.08826]] RewardDance: Reward Scaling in Visual Generation(https://arxiv.org/abs/2509.08826)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Reward Models (RMs) are critical for improving generation models via Reinforcement Learning (RL), yet the RM scaling paradigm in visual generation remains largely unexplored. It primarily due to fundamental limitations in existing approaches: CLIP-based RMs suffer from architectural and input modality constraints, while prevalent Bradley-Terry losses are fundamentally misaligned with the next-token prediction mechanism of Vision-Language Models (VLMs), hindering effective scaling. More critically, the RLHF optimization process is plagued by Reward Hacking issue, where models exploit flaws in the reward signal without improving true quality. To address these challenges, we introduce RewardDance, a scalable reward modeling framework that overcomes these barriers through a novel generative reward paradigm. By reformulating the reward score as the model's probability of predicting a "yes" token, indicating that the generated image outperforms a reference image according to specific criteria, RewardDance intrinsically aligns reward objectives with VLM architectures. This alignment unlocks scaling across two dimensions: (1) Model Scaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context Scaling: Integration of task-specific instructions, reference examples, and chain-of-thought (CoT) reasoning. Extensive experiments demonstrate that RewardDance significantly surpasses state-of-the-art methods in text-to-image, text-to-video, and image-to-video generation. Crucially, we resolve the persistent challenge of "reward hacking": Our large-scale RMs exhibit and maintain high reward variance during RL fine-tuning, proving their resistance to hacking and ability to produce diverse, high-quality outputs. It greatly relieves the mode collapse problem that plagues smaller models.</li>
<li><strong>摘要：</strong>奖励模型（RMS）对于通过增强学习（RL）改善生成模型至关重要，但是视觉生成中的RM缩放范式在很大程度上尚未开发。这主要是由于现有方法的基本限制：基于夹的RMS遭受了建筑和输入方式限制的影响，而普遍的Bradley-Terry-Terry-Terry损失从根本上与视觉模型（VLMS）的下一个预测机制（VLMS）的预测机制不一致，并阻碍了有效的尺度。更重要的是，RLHF优化过程受到奖励黑客问题的困扰，在奖励黑客问题上，模型在不提高真实质量的情况下利用奖励信号中的缺陷。为了应对这些挑战，我们引入了奖励，这是一个可扩展的奖励建模框架，通过新颖的生成奖励范式克服了这些障碍。通过将奖励分数重新定义为模型预测“是”令牌的可能性，表明生成的图像根据特定标准优于参考图像，奖励本质上将奖励目标与VLM架构保持一致。该对齐方式解锁了跨两个维度的缩放：（1）模型缩放：RMS的系统缩放率高达260亿个参数； （2）上下文缩放：特定于任务的指令，参考示例和思想链（COT）推理的集成。广泛的实验表明，奖励大大超过了文本对图像，文本对视频和图像到视频的最新方法。至关重要的是，我们解决了“奖励黑客”的持续挑战：我们的大规模RMS展示并在RL微调过程中保持了较高的奖励差异，证明了它们对黑客的抵抗力以及产生多样化的高质量产量的能力。它极大地缓解了困扰较小模型的模式崩溃问题。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
