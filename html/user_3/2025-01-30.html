<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-01-30</h1>
<h3>Title: Amplifier: Bringing Attention to Neglected Low-Energy Components in Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Jingru Fei, Kun Yi, Wei Fan, Qi Zhang, Zhendong Niu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.17216">https://arxiv.org/abs/2501.17216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.17216">https://arxiv.org/pdf/2501.17216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.17216]] Amplifier: Bringing Attention to Neglected Low-Energy Components in Time Series Forecasting(https://arxiv.org/abs/2501.17216)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>We propose an energy amplification technique to address the issue that existing models easily overlook low-energy components in time series forecasting. This technique comprises an energy amplification block and an energy restoration block. The energy amplification block enhances the energy of low-energy components to improve the model's learning efficiency for these components, while the energy restoration block returns the energy to its original level. Moreover, considering that the energy-amplified data typically displays two distinct energy peaks in the frequency spectrum, we integrate the energy amplification technique with a seasonal-trend forecaster to model the temporal relationships of these two peaks independently, serving as the backbone for our proposed model, Amplifier. Additionally, we propose a semi-channel interaction temporal relationship enhancement block for Amplifier, which enhances the model's ability to capture temporal relationships from the perspective of the commonality and specificity of each channel in the data. Extensive experiments on eight time series forecasting benchmarks consistently demonstrate our model's superiority in both effectiveness and efficiency compared to state-of-the-art methods.</li>
<li><strong>摘要：</strong>我们提出了一种能量放大技术来解决现有模型在时间序列预测中容易忽略低能量成分的问题。该技术包括能量放大块和能量恢复块。能量放大块增强低能量成分的能量，以提高模型对这些成分的学习效率，而能量恢复块将能量恢复到其原始水平。此外，考虑到能量放大数据通常在频谱中显示两个不同的能量峰，我们将能量放大技术与季节性趋势预测器相结合，以独立建模这两个峰值的时间关系，作为我们提出的模型 Amplifier 的骨干。此外，我们为 Amplifier 提出了一个半通道交互时间关系增强块，它从数据中每个通道的共性和特异性的角度增强了模型捕捉时间关系的能力。在八个时间序列预测基准上进行的大量实验一致证明了我们的模型在有效性和效率方面都优于最先进的方法。</li>
</ul>

<h3>Title: MDDM: A Molecular Dynamics Diffusion Model to Predict Particle Self-Assembly</h3>
<ul>
<li><strong>Authors: </strong>Kevin Ferguson, Yu-hsuan Chen, Levent Burak Kara</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.17319">https://arxiv.org/abs/2501.17319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.17319">https://arxiv.org/pdf/2501.17319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.17319]] MDDM: A Molecular Dynamics Diffusion Model to Predict Particle Self-Assembly(https://arxiv.org/abs/2501.17319)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The discovery and study of new material systems relies on molecular simulations that often come with significant computational expense. We propose MDDM, a Molecular Dynamics Diffusion Model, which is capable of predicting a valid output conformation for a given input pair potential function. After training MDDM on a large dataset of molecular dynamics self-assembly results, the proposed model can convert uniform noise into a meaningful output particle structure corresponding to an arbitrary input potential. The model's architecture has domain-specific properties built-in, such as satisfying periodic boundaries and being invariant to translation. The model significantly outperforms the baseline point-cloud diffusion model for both unconditional and conditional generation tasks.</li>
<li><strong>摘要：</strong>新材料系统的发现和研究依赖于分子模拟，而分子模拟通常需要大量的计算成本。我们提出了分子动力学扩散模型 MDDM，该模型能够预测给定输入对势函数的有效输出构象。在对大量分子动力学自组装结果数据集进行 MDDM 训练后，所提出的模型可以将均匀噪声转换为与任意输入势相对应的有意义的输出粒子结构。该模型的架构具有内置的领域特定属性，例如满足周期性边界和对平移不变。该模型在无条件和有条件生成任务中的表现都明显优于基线点云扩散模型。</li>
</ul>

<h3>Title: CardiCat: a Variational Autoencoder for High-Cardinality Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Lee Carlin, Yuval Benjamini</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.17324">https://arxiv.org/abs/2501.17324</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.17324">https://arxiv.org/pdf/2501.17324</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.17324]] CardiCat: a Variational Autoencoder for High-Cardinality Tabular Data(https://arxiv.org/abs/2501.17324)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>High-cardinality categorical features are a common characteristic of mixed-type tabular datasets. Existing generative model architectures struggle to learn the complexities of such data at scale, primarily due to the difficulty of parameterizing the categorical features. In this paper, we present a general variational autoencoder model, CardiCat, that can accurately fit imbalanced high-cardinality and heterogeneous tabular data. Our method substitutes one-hot encoding with regularized dual encoder-decoder embedding layers, which are jointly learned. This approach enables us to use embeddings that depend also on the other covariates, leading to a compact and homogenized parameterization of categorical features. Our model employs a considerably smaller trainable parameter space than competing methods, enabling learning at a large scale. CardiCat generates high-quality synthetic data that better represent high-cardinality and imbalanced features compared to competing VAE models for multiple real and simulated datasets.</li>
<li><strong>摘要：</strong>高基数分类特征是混合类型表格数据集的共同特征。现有的生成模型架构难以大规模学习此类数据的复杂性，这主要是由于难以参数化分类特征。在本文中，我们提出了一种通用变分自动编码器模型 CardiCat，它可以准确地拟合不平衡的高基数和异构表格数据。我们的方法用联合学习的正则化双编码器-解码器嵌入层代替独热编码。这种方法使我们能够使用也依赖于其他协变量的嵌入，从而实现紧凑且同质化的分类特征参数化。我们的模型采用的可训练参数空间比竞争方法小得多，从而能够大规模学习。与竞争 VAE 模型相比，CardiCat 为多个真实和模拟数据集生成了高质量的合成数据，可以更好地表示高基数和不平衡特征。</li>
</ul>

<h3>Title: Towards Making Flowchart Images Machine Interpretable</h3>
<ul>
<li><strong>Authors: </strong>Shreya Shukla, Prajwal Gatti, Yogesh Kumar, Vikash Yadav, Anand Mishra</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.DL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.17441">https://arxiv.org/abs/2501.17441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.17441">https://arxiv.org/pdf/2501.17441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.17441]] Towards Making Flowchart Images Machine Interpretable(https://arxiv.org/abs/2501.17441)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Computer programming textbooks and software documentations often contain flowcharts to illustrate the flow of an algorithm or procedure. Modern OCR engines often tag these flowcharts as graphics and ignore them in further processing. In this paper, we work towards making flowchart images machine-interpretable by converting them to executable Python codes. To this end, inspired by the recent success in natural language to code generation literature, we present a novel transformer-based framework, namely FloCo-T5. Our model is well-suited for this task,as it can effectively learn semantics, structure, and patterns of programming languages, which it leverages to generate syntactically correct code. We also used a task-specific pre-training objective to pre-train FloCo-T5 using a large number of logic-preserving augmented code samples. Further, to perform a rigorous study of this problem, we introduce theFloCo dataset that contains 11,884 flowchart images and their corresponding Python codes. Our experiments show promising results, and FloCo-T5 clearly outperforms related competitive baselines on code generation metrics. We make our dataset and implementation publicly available.</li>
<li><strong>摘要：</strong>计算机编程教科书和软件文档通常包含流程图，以说明算法或程序的流程。现代 OCR 引擎通常将这些流程图标记为图形，并在进一步处理中忽略它们。在本文中，我们致力于将流程图图像转换为可执行的 Python 代码，使其可被机器解释。为此，受最近自然语言到代码生成文献中成功的启发，我们提出了一种基于转换器的新型框架，即 FloCo-T5。我们的模型非常适合这项任务，因为它可以有效地学习编程语言的语义、结构和模式，并利用这些来生成语法正确的代码。我们还使用特定于任务的预训练目标，使用大量保留逻辑的增强代码样本对 FloCo-T5 进行预训练。此外，为了对这个问题进行严格的研究，我们引入了包含 11,884 张流程图图像及其对应的 Python 代码的 FloCo 数据集。我们的实验显示了令人鼓舞的结果，并且 FloCo-T5 在代码生成指标上明显优于相关的竞争基线。我们公开我们的数据集和实施情况。</li>
</ul>

<h3>Title: Gradual Domain Adaptation for Graph Learning</h3>
<ul>
<li><strong>Authors: </strong>Pui Ieng Lei, Ximing Chen, Yijun Sheng, Yanyan Liu, Jingzhi Guo, Zhiguo Gong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.17443">https://arxiv.org/abs/2501.17443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.17443">https://arxiv.org/pdf/2501.17443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.17443]] Gradual Domain Adaptation for Graph Learning(https://arxiv.org/abs/2501.17443)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Existing literature lacks a graph domain adaptation technique for handling large distribution shifts, primarily due to the difficulty in simulating an evolving path from source to target graph. To make a breakthrough, we present a graph gradual domain adaptation (GGDA) framework with the construction of a compact domain sequence that minimizes information loss in adaptations. Our approach starts with an efficient generation of knowledge-preserving intermediate graphs over the Fused Gromov-Wasserstein (FGW) metric. With the bridging data pool, GGDA domains are then constructed via a novel vertex-based domain progression, which comprises "close" vertex selections and adaptive domain advancement to enhance inter-domain information transferability. Theoretically, our framework concretizes the intractable inter-domain distance $W_p(\mu_t,\mu_{t+1})$ via implementable upper and lower bounds, enabling flexible adjustments of this metric for optimizing domain formation. Extensive experiments under various transfer scenarios validate the superior performance of our GGDA framework.</li>
<li><strong>摘要：</strong>现有文献缺乏用于处理大分布变化的图域自适应技术，这主要是因为难以模拟从源图到目标图的演变路径。为了取得突破，我们提出了一个图渐进域自适应 (GGDA) 框架，该框架构建了一个紧凑的域序列，以最大限度地减少自适应中的信息丢失。我们的方法首先在融合 Gromov-Wasserstein (FGW) 度量上高效生成知识保留的中间图。利用桥接数据池，然后通过一种新颖的基于顶点的域进展构建 GGDA 域，该进展包括“接近”顶点选择和自适应域推进，以增强域间信息可转移性。从理论上讲，我们的框架通过可实现的上限和下限具体化了难以处理的域间距离 $W_p(\mu_t,\mu_{t+1})$，从而可以灵活调整该度量以优化域形成。在各种传输场景下进行的大量实验验证了我们的 GGDA 框架的卓越性能。</li>
</ul>

<h3>Title: Towards Training-Free Open-World Classification with 3D Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Xinzhe Xia, Weiguang Zhao, Yuyao Yan, Guanyu Yang, Rui Zhang, Kaizhu Huang, Xi Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.17547">https://arxiv.org/abs/2501.17547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.17547">https://arxiv.org/pdf/2501.17547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.17547]] Towards Training-Free Open-World Classification with 3D Generative Models(https://arxiv.org/abs/2501.17547)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>3D open-world classification is a challenging yet essential task in dynamic and unstructured real-world scenarios, requiring both open-category and open-pose recognition. To address these challenges, recent wisdom often takes sophisticated 2D pre-trained models to provide enriched and stable representations. However, these methods largely rely on how 3D objects can be projected into 2D space, which is unfortunately not well solved, and thus significantly limits their performance. Unlike these present efforts, in this paper we make a pioneering exploration of 3D generative models for 3D open-world classification. Drawing on abundant prior knowledge from 3D generative models, we additionally craft a rotation-invariant feature extractor. This innovative synergy endows our pipeline with the advantages of being training-free, open-category, and pose-invariant, thus well suited to 3D open-world classification. Extensive experiments on benchmark datasets demonstrate the potential of generative models in 3D open-world classification, achieving state-of-the-art performance on ModelNet10 and McGill with 32.0% and 8.7% overall accuracy improvement, respectively.</li>
<li><strong>摘要：</strong>3D 开放世界分类是动态和非结构化现实世界场景中一项具有挑战性但又必不可少的任务，需要开放类别和开放姿势识别。为了应对这些挑战，最近的研究通常采用复杂的 2D 预训练模型来提供丰富而稳定的表示。然而，这些方法很大程度上依赖于如何将 3D 物体投影到 2D 空间，不幸的是，这个问题还没有得到很好的解决，因此大大限制了它们的性能。与目前的这些努力不同，在本文中，我们对用于 3D 开放世界分类的 3D 生成模型进行了开创性的探索。利用来自 3D 生成模型的丰富先验知识，我们还设计了一个旋转不变的特征提取器。这种创新的协同作用使我们的流程具有无需训练、开放类别和姿势不变的优势，因此非常适合 3D 开放世界分类。在基准数据集上进行的大量实验证明了生成模型在 3D 开放世界分类中的潜力，在 ModelNet10 和 McGill 上取得了最先进的性能，整体准确率分别提高了 32.0% 和 8.7%。</li>
</ul>

<h3>Title: Closing the Gap Between Synthetic and Ground Truth Time Series Distributions via Neural Mapping</h3>
<ul>
<li><strong>Authors: </strong>Daesoo Lee, Sara Malacarne, Erlend Aune</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.17553">https://arxiv.org/abs/2501.17553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.17553">https://arxiv.org/pdf/2501.17553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.17553]] Closing the Gap Between Synthetic and Ground Truth Time Series Distributions via Neural Mapping(https://arxiv.org/abs/2501.17553)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce Neural Mapper for Vector Quantized Time Series Generator (NM-VQTSG), a novel method aimed at addressing fidelity challenges in vector quantized (VQ) time series generation. VQ-based methods, such as TimeVQVAE, have demonstrated success in generating time series but are hindered by two critical bottlenecks: information loss during compression into discrete latent spaces and deviations in the learned prior distribution from the ground truth distribution. These challenges result in synthetic time series with compromised fidelity and distributional accuracy. To overcome these limitations, NM-VQTSG leverages a U-Net-based neural mapping model to bridge the distributional gap between synthetic and ground truth time series. To be more specific, the model refines synthetic data by addressing artifacts introduced during generation, effectively aligning the distributions of synthetic and real data. Importantly, NM-VQTSG can be used for synthetic time series generated by any VQ-based generative method. We evaluate NM-VQTSG across diverse datasets from the UCR Time Series Classification archive, demonstrating its capability to consistently enhance fidelity in both unconditional and conditional generation tasks. The improvements are evidenced by significant improvements in FID, IS, and conditional FID, additionally backed up by visual inspection in a data space and a latent space. Our findings establish NM-VQTSG as a new method to improve the quality of synthetic time series. Our implementation is available on \url{this https URL}.</li>
<li><strong>摘要：</strong>在本文中，我们介绍了用于矢量量化时间序列生成器的神经映射器 (NM-VQTSG)，这是一种旨在解决矢量量化 (VQ) 时间序列生成中的保真度挑战的新方法。基于 VQ 的方法（例如 TimeVQVAE）已证明在生成时间序列方面取得了成功，但受到两个关键瓶颈的阻碍：压缩到离散潜在空间期间的信息丢失以及学习到的先验分布与地面真实分布的偏差。这些挑战导致合成时间序列的保真度和分布准确性受损。为了克服这些限制，NM-VQTSG 利用基于 U-Net 的神经映射模型来弥合合成和地面真实时间序列之间的分布差距。更具体地说，该模型通过解决生成过程中引入的伪影来细化合成数据，从而有效地对齐合成数据和真实数据的分布。重要的是，NM-VQTSG 可用于任何基于 VQ 的生成方法生成的合成时间序列。我们对来自 UCR 时间序列分类档案的各种数据集中的 NM-VQTSG 进行了评估，证明了其在无条件和条件生成任务中持续提高保真度的能力。FID、IS 和条件 FID 的显著改进证明了这些改进，此外，数据空间和潜在空间中的视觉检查也支持了这些改进。我们的研究结果确立了 NM-VQTSG 是一种提高合成时间序列质量的新方法。我们的实现可在 \url{此 https URL} 上找到。</li>
</ul>

<h3>Title: RegionGCN: Spatial-Heterogeneity-Aware Graph Convolutional Networks</h3>
<ul>
<li><strong>Authors: </strong>Hao Guo, Han Wang, Di Zhu, Lun Wu, A. Stewart Fotheringham, Yu Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.17599">https://arxiv.org/abs/2501.17599</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.17599">https://arxiv.org/pdf/2501.17599</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.17599]] RegionGCN: Spatial-Heterogeneity-Aware Graph Convolutional Networks(https://arxiv.org/abs/2501.17599)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Modeling spatial heterogeneity in the data generation process is essential for understanding and predicting geographical phenomena. Despite their prevalence in geospatial tasks, neural network models usually assume spatial stationarity, which could limit their performance in the presence of spatial process heterogeneity. By allowing model parameters to vary over space, several approaches have been proposed to incorporate spatial heterogeneity into neural networks. However, current geographically weighting approaches are ineffective on graph neural networks, yielding no significant improvement in prediction accuracy. We assume the crux lies in the over-fitting risk brought by a large number of local parameters. Accordingly, we propose to model spatial process heterogeneity at the regional level rather than at the individual level, which largely reduces the number of spatially varying parameters. We further develop a heuristic optimization procedure to learn the region partition adaptively in the process of model training. Our proposed spatial-heterogeneity-aware graph convolutional network, named RegionGCN, is applied to the spatial prediction of county-level vote share in the 2016 US presidential election based on socioeconomic attributes. Results show that RegionGCN achieves significant improvement over the basic and geographically weighted GCNs. We also offer an exploratory analysis tool for the spatial variation of non-linear relationships through ensemble learning of regional partitions from RegionGCN. Our work contributes to the practice of Geospatial Artificial Intelligence (GeoAI) in tackling spatial heterogeneity.</li>
<li><strong>摘要：</strong>在数据生成过程中对空间异质性进行建模对于理解和预测地理现象至关重要。尽管神经网络模型在地理空间任务中很普遍，但它们通常假设空间平稳性，这可能会限制它们在存在空间过程异质性的情况下的性能。通过允许模型参数随空间变化，已经提出了几种将空间异质性纳入神经网络的方法。然而，当前的地理加权方法对图神经网络无效，预测精度没有显著提高。我们认为关键在于大量局部参数带来的过度拟合风险。因此，我们建议在区域层面而不是个人层面对空间过程异质性进行建模，这大大减少了空间变化参数的数量。我们进一步开发了一种启发式优化程序，以在模型训练过程中自适应地学习区域分区。我们提出的空间异质性感知图卷积网络 RegionGCN 应用于基于社会经济属性的 2016 年美国总统大选县级选票份额的空间预测。结果表明，RegionGCN 比基本和地理加权 GCN 取得了显著的改进。我们还通过对 RegionGCN 的区域分区进行集成学习，为非线性关系的空间变化提供了一种探索性分析工具。我们的工作为地理空间人工智能 (GeoAI) 在解决空间异质性方面的实践做出了贡献。</li>
</ul>

<h3>Title: Drivetrain simulation using variational autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Pallavi Sharma, Jorge-Humberto Urrea-Quintero, Bogdan Bogdan, Adrian-Dumitru Ciotec, Laura Vasilie, Henning Wessels, Matteo Skull</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.17653">https://arxiv.org/abs/2501.17653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.17653">https://arxiv.org/pdf/2501.17653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.17653]] Drivetrain simulation using variational autoencoders(https://arxiv.org/abs/2501.17653)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This work proposes variational autoencoders (VAEs) to predict a vehicle's jerk from a given torque demand, addressing the limitations of sparse real-world datasets. Specifically, we implement unconditional and conditional VAEs to generate jerk signals that integrate features from different drivetrain scenarios. The VAEs are trained on experimental data collected from two variants of a fully electric SUV, which differ in maximum torque delivery and drivetrain configuration. New meaningful jerk signals are generated within an engineering context through the interpretation of the VAE's latent space. A performance comparison with baseline physics-based and hybrid models confirms the effectiveness of the VAEs. We show that VAEs bypass the need for exhaustive manual system parametrization while maintaining physical plausibility by conditioning data generation on specific inputs.</li>
<li><strong>摘要：</strong>本研究提出变分自动编码器 (VAE) 来根据给定的扭矩需求预测车辆的抖动，从而解决稀疏现实世界数据集的局限性。具体而言，我们实现了无条件和条件 VAE 来生成集成了不同动力传动系统场景特征的抖动信号。VAE 是基于从两种全电动 SUV 变体收集的实验数据进行训练的，这两种变体在最大扭矩输出和动力传动系统配置方面有所不同。通过解释 VAE 的潜在空间，在工程环境中生成新的有意义的抖动信号。与基线物理模型和混合模型的性能比较证实了 VAE 的有效性。我们表明，VAE 绕过了详尽的手动系统参数化的需要，同时通过根据特定输入调节数据生成来保持物理合理性。</li>
</ul>

<h3>Title: Segmentation-Aware Generative Reinforcement Network (GRN) for Tissue Layer Segmentation in 3-D Ultrasound Images for Chronic Low-back Pain (cLBP) Assessment</h3>
<ul>
<li><strong>Authors: </strong>Zixue Zeng, Xiaoyan Zhao, Matthew Cartier, Tong Yu, Jing Wang, Xin Meng, Zhiyu Sheng, Maryam Satarpour, John M Cormack, Allison Bean, Ryan Nussbaum, Maya Maurer, Emily Landis-Walkenhorst, Dinesh Kumbhare, Kang Kim, Ajay Wasan, Jiantao Pu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.17690">https://arxiv.org/abs/2501.17690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.17690">https://arxiv.org/pdf/2501.17690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.17690]] Segmentation-Aware Generative Reinforcement Network (GRN) for Tissue Layer Segmentation in 3-D Ultrasound Images for Chronic Low-back Pain (cLBP) Assessment(https://arxiv.org/abs/2501.17690)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We introduce a novel segmentation-aware joint training framework called generative reinforcement network (GRN) that integrates segmentation loss feedback to optimize both image generation and segmentation performance in a single stage. An image enhancement technique called segmentation-guided enhancement (SGE) is also developed, where the generator produces images tailored specifically for the segmentation model. Two variants of GRN were also developed, including GRN for sample-efficient learning (GRN-SEL) and GRN for semi-supervised learning (GRN-SSL). GRN's performance was evaluated using a dataset of 69 fully annotated 3D ultrasound scans from 29 subjects. The annotations included six anatomical structures: dermis, superficial fat, superficial fascial membrane (SFM), deep fat, deep fascial membrane (DFM), and muscle. Our results show that GRN-SEL with SGE reduces labeling efforts by up to 70% while achieving a 1.98% improvement in the Dice Similarity Coefficient (DSC) compared to models trained on fully labeled datasets. GRN-SEL alone reduces labeling efforts by 60%, GRN-SSL with SGE decreases labeling requirements by 70%, and GRN-SSL alone by 60%, all while maintaining performance comparable to fully supervised models. These findings suggest the effectiveness of the GRN framework in optimizing segmentation performance with significantly less labeled data, offering a scalable and efficient solution for ultrasound image analysis and reducing the burdens associated with data annotation.</li>
<li><strong>摘要：</strong>我们引入了一种称为生成强化网络 (GRN) 的新型分割感知联合训练框架，该框架集成了分割损失反馈，可在单个阶段优化图像生成和分割性能。我们还开发了一种称为分割引导增强 (SGE) 的图像增强技术，其中生成器生成专门为分割模型定制的图像。还开发了两种 GRN 变体，包括用于样本高效学习的 GRN (GRN-SEL) 和用于半监督学习的 GRN (GRN-SSL)。使用来自 29 名受试者的 69 次完全注释的 3D 超声扫描数据集来评估 GRN 的性能。注释包括六种解剖结构：真皮、浅表脂肪、浅表筋膜 (SFM)、深层脂肪、深层筋膜 (DFM) 和肌肉。我们的结果表明，与在完全标记的数据集上训练的模型相比，带有 SGE 的 GRN-SEL 可将标记工作量减少高达 70%，同时 Dice 相似系数 (DSC) 提高 1.98%。单独使用 GRN-SEL 可将标记工作量减少 60%，带有 SGE 的 GRN-SSL 可将标记要求减少 70%，单独使用 GRN-SSL 可将标记要求减少 60%，同时保持与完全监督模型相当的性能。这些发现表明 GRN 框架在优化分割性能方面非常有效，标记数据明显较少，为超声图像分析提供了可扩展且高效的解决方案，并减轻了与数据注释相关的负担。</li>
</ul>

<h3>Title: VICCA: Visual Interpretation and Comprehension of Chest X-ray Anomalies in Generated Report Without Human Feedback</h3>
<ul>
<li><strong>Authors: </strong>Sayeh Gholipour Picha, Dawood Al Chanti, Alice Caplier</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.17726">https://arxiv.org/abs/2501.17726</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.17726">https://arxiv.org/pdf/2501.17726</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.17726]] VICCA: Visual Interpretation and Comprehension of Chest X-ray Anomalies in Generated Report Without Human Feedback(https://arxiv.org/abs/2501.17726)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>As artificial intelligence (AI) becomes increasingly central to healthcare, the demand for explainable and trustworthy models is paramount. Current report generation systems for chest X-rays (CXR) often lack mechanisms for validating outputs without expert oversight, raising concerns about reliability and interpretability. To address these challenges, we propose a novel multimodal framework designed to enhance the semantic alignment and localization accuracy of AI-generated medical reports. Our framework integrates two key modules: a Phrase Grounding Model, which identifies and localizes pathologies in CXR images based on textual prompts, and a Text-to-Image Diffusion Module, which generates synthetic CXR images from prompts while preserving anatomical fidelity. By comparing features between the original and generated images, we introduce a dual-scoring system: one score quantifies localization accuracy, while the other evaluates semantic consistency. This approach significantly outperforms existing methods, achieving state-of-the-art results in pathology localization and text-to-image alignment. The integration of phrase grounding with diffusion models, coupled with the dual-scoring evaluation system, provides a robust mechanism for validating report quality, paving the way for more trustworthy and transparent AI in medical imaging.</li>
<li><strong>摘要：</strong>随着人工智能 (AI) 在医疗保健领域变得越来越重要，对可解释和可信赖模型的需求也变得至关重要。目前的胸部 X 光 (CXR) 报告生成系统通常缺乏在没有专家监督的情况下验证输出的机制，这引发了人们对可靠性和可解释性的担忧。为了应对这些挑战，我们提出了一种新颖的多模态框架，旨在提高 AI 生成的医疗报告的语义对齐和定位准确性。我们的框架集成了两个关键模块：短语基础模型，它根据文本提示识别和定位 CXR 图像中的病理；以及文本到图像扩散模块，它根据提示生成合成 CXR 图像，同时保持解剖保真度。通过比较原始图像和生成的图像之间的特征，我们引入了一个双评分系统：一个分数量化定位准确性，而另一个分数评估语义一致性。这种方法明显优于现有方法，在病理定位和文本到图像对齐方面取得了最先进的结果。短语基础与扩散模型的结合，加上双重评分评估体系，提供了验证报告质量的强有力机制，为医学成像领域更值得信赖和透明的人工智能铺平了道路。</li>
</ul>

<h3>Title: Generative Unordered Flow for Set-Structured Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Yangming Li, Carola-Bibiane Schönlieb</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.17770">https://arxiv.org/abs/2501.17770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.17770">https://arxiv.org/pdf/2501.17770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.17770]] Generative Unordered Flow for Set-Structured Data Generation(https://arxiv.org/abs/2501.17770)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Flow-based generative models have demonstrated promising performance across a broad spectrum of data modalities (e.g., image and text). However, there are few works exploring their extension to unordered data (e.g., spatial point set), which is not trivial because previous models are mostly designed for vector data that are naturally ordered. In this paper, we present unordered flow, a type of flow-based generative model for set-structured data generation. Specifically, we convert unordered data into an appropriate function representation, and learn the probability measure of such representations through function-valued flow matching. For the inverse map from a function representation to unordered data, we propose a method similar to particle filtering, with Langevin dynamics to first warm-up the initial particles and gradient-based search to update them until convergence. We have conducted extensive experiments on multiple real-world datasets, showing that our unordered flow model is very effective in generating set-structured data and significantly outperforms previous baselines.</li>
<li><strong>摘要：</strong>基于流的生成模型已在广泛的数据模态（例如图像和文本）中表现出良好的性能。然而，很少有研究探索它们对无序数据（例如空间点集）的扩展，这并不容易，因为以前的模型大多是为自然有序的矢量数据设计的。在本文中，我们提出了一种基于流的生成模型，用于生成集合结构化数据。具体来说，我们将无序数据转换为适当的函数表示，并通过函数值流匹配来学习这种表示的概率测度。对于从函数表示到无序数据的逆映射，我们提出了一种类似于粒子过滤的方法，使用朗之万动力学首先预热初始粒子，然后使用基于梯度的搜索来更新它们直到收敛。我们在多个真实世界数据集上进行了广泛的实验，表明我们的无序流模型在生成集合结构化数据方面非常有效，并且明显优于以前的基线。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
