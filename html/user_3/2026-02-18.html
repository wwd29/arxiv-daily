<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-02-18</h1>
<h3>Title: Near-Optimal Sample Complexity for Online Constrained MDPs</h3>
<ul>
<li><strong>Authors: </strong>Chang Liu, Yunfan Li, Lin F. Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15076">https://arxiv.org/abs/2602.15076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15076">https://arxiv.org/pdf/2602.15076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15076]] Near-Optimal Sample Complexity for Online Constrained MDPs(https://arxiv.org/abs/2602.15076)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Safety is a fundamental challenge in reinforcement learning (RL), particularly in real-world applications such as autonomous driving, robotics, and healthcare. To address this, Constrained Markov Decision Processes (CMDPs) are commonly used to enforce safety constraints while optimizing performance. However, existing methods often suffer from significant safety violations or require a high sample complexity to generate near-optimal policies. We address two settings: relaxed feasibility, where small violations are allowed, and strict feasibility, where no violation is allowed. We propose a model-based primal-dual algorithm that balances regret and bounded constraint violations, drawing on techniques from online RL and constrained optimization. For relaxed feasibility, we prove that our algorithm returns an $\varepsilon$-optimal policy with $\varepsilon$-bounded violation with arbitrarily high probability, requiring $\tilde{O}\left(\frac{SAH^3}{\varepsilon^2}\right)$ learning episodes, matching the lower bound for unconstrained MDPs. For strict feasibility, we prove that our algorithm returns an $\varepsilon$-optimal policy with zero violation with arbitrarily high probability, requiring $\tilde{O}\left(\frac{SAH^5}{\varepsilon^2\zeta^2}\right)$ learning episodes, where $\zeta$ is the problem-dependent Slater constant characterizing the size of the feasible region. This result matches the lower bound for learning CMDPs with access to a generative model. Our results demonstrate that learning CMDPs in an online setting is as easy as learning with a generative model and is no more challenging than learning unconstrained MDPs when small violations are allowed.</li>
<li><strong>摘要：</strong>安全性是强化学习 (RL) 的一项基本挑战，尤其是在自动驾驶、机器人和医疗保健等现实应用中。为了解决这个问题，约束马尔可夫决策过程（CMDP）通常用于在优化性能的同时强制执行安全约束。然而，现有方法经常遭受严重的安全违规或需要高样本复杂性才能生成接近最优的策略。我们提出两种设置：宽松的可行性（允许轻微违规）和严格的可行性（不允许违规）。我们提出了一种基于模型的原始对偶算法，利用在线强化学习和约束优化技术来平衡遗憾和有界约束违规。为了宽松的可行性，我们证明我们的算法以任意高的概率返回 $\varepsilon$ 最优策略，并且 $\varepsilon$ 有界违规，需要 $\tilde{O}\left(\frac{SAH^3}{\varepsilon^2}\right)$ 学习片段，匹配无约束 MDP 的下限。为了严格的可行性，我们证明我们的算法以任意高的概率返回零违规的 $\varepsilon$ 最优策略，需要 $\tilde{O}\left(\frac{SAH^5}{\varepsilon^2\zeta^2}\right)$ 学习片段，其中 $\zeta$ 是表征可行区域大小的问题相关的 Slater 常数。该结果与学习 CMDP 并访问生成模型的下限相匹配。我们的结果表明，在在线环境中学习 CMDP 与使用生成模型学习一样简单，并且在允许小的违规情况下，并不比学习无约束 MDP 更具挑战性。</li>
</ul>

<h3>Title: Zero-shot HOI Detection with MLLM-based Detector-agnostic Interaction Recognition</h3>
<ul>
<li><strong>Authors: </strong>Shiyu Xuan, Dongkai Wang, Zechao Li, Jinhui Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15124">https://arxiv.org/abs/2602.15124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15124">https://arxiv.org/pdf/2602.15124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15124]] Zero-shot HOI Detection with MLLM-based Detector-agnostic Interaction Recognition(https://arxiv.org/abs/2602.15124)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Zero-shot Human-object interaction (HOI) detection aims to locate humans and objects in images and recognize their interactions. While advances in open-vocabulary object detection provide promising solutions for object localization, interaction recognition (IR) remains challenging due to the combinatorial diversity of interactions. Existing methods, including two-stage methods, tightly couple IR with a specific detector and rely on coarse-grained vision-language model (VLM) features, which limit generalization to unseen interactions. In this work, we propose a decoupled framework that separates object detection from IR and leverages multi-modal large language models (MLLMs) for zero-shot IR. We introduce a deterministic generation method that formulates IR as a visual question answering task and enforces deterministic outputs, enabling training-free zero-shot IR. To further enhance performance and efficiency by fine-tuning the model, we design a spatial-aware pooling module that integrates appearance and pairwise spatial cues, and a one-pass deterministic matching method that predicts all candidate interactions in a single forward pass. Extensive experiments on HICO-DET and V-COCO demonstrate that our method achieves superior zero-shot performance, strong cross-dataset generalization, and the flexibility to integrate with any object detectors without retraining. The codes are publicly available at this https URL.</li>
<li><strong>摘要：</strong>零样本人与物体交互（HOI）检测旨在定位图像中的人和物体并识别他们的交互。虽然开放词汇对象检测的进步为对象定位提供了有前途的解决方案，但由于交互的组合多样性，交互识别（IR）仍然具有挑战性。现有方法（包括两阶段方法）将 IR 与特定检测器紧密耦合，并依赖于粗粒度视觉语言模型 (VLM) 特征，这限制了对看不见的交互的泛化。在这项工作中，我们提出了一个解耦框架，将目标检测与 IR 分开，并利用多模态大语言模型 (MLLM) 进行零样本 IR。我们引入了一种确定性生成方法，将 IR 制定为视觉问答任务并强制执行确定性输出，从而实现免训练的零样本 IR。为了通过微调模型进一步提高性能和效率，我们设计了一个空间感知池模块，该模块集成了外观和成对空间线索，以及一种一次性确定性匹配方法，可以在一次前向传递中预测所有候选交互。在 HICO-DET 和 V-COCO 上进行的大量实验表明，我们的方法实现了卓越的零样本性能、强大的跨数据集泛化性以及无需重新训练即可与任何目标检测器集成的灵活性。这些代码可通过此 https URL 公开获取。</li>
</ul>

<h3>Title: Distributional Deep Learning for Super-Resolution of 4D Flow MRI under Domain Shift</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyi Wen, Fei Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV, stat.AP, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15167">https://arxiv.org/abs/2602.15167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15167">https://arxiv.org/pdf/2602.15167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15167]] Distributional Deep Learning for Super-Resolution of 4D Flow MRI under Domain Shift(https://arxiv.org/abs/2602.15167)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Super-resolution is widely used in medical imaging to enhance low-quality data, reducing scan time and improving abnormality detection. Conventional super-resolution approaches typically rely on paired datasets of downsampled and original high resolution images, training models to reconstruct high resolution images from their artificially degraded counterparts. However, in real-world clinical settings, low resolution data often arise from acquisition mechanisms that differ significantly from simple downsampling. As a result, these inputs may lie outside the domain of the training data, leading to poor model generalization due to domain shift. To address this limitation, we propose a distributional deep learning framework that improves model robustness and domain generalization. We develop this approch for enhancing the resolution of 4D Flow MRI (4DF). This is a novel imaging modality that captures hemodynamic flow velocity and clinically relevant metrics such as vessel wall stress. These metrics are critical for assessing aneurysm rupture risk. Our model is initially trained on high resolution computational fluid dynamics (CFD) simulations and their downsampled counterparts. It is then fine-tuned on a small, harmonized dataset of paired 4D Flow MRI and CFD samples. We derive the theoretical properties of our distributional estimators and demonstrate that our framework significantly outperforms traditional deep learning approaches through real data applications. This highlights the effectiveness of distributional learning in addressing domain shift and improving super-resolution performance in clinically realistic scenarios.</li>
<li><strong>摘要：</strong>超分辨率广泛应用于医学成像，以增强低质量数据、减少扫描时间并改善异常检测。传统的超分辨率方法通常依赖于下采样和原始高分辨率图像的配对数据集，训练模型从人工降级的对应图像中重建高分辨率图像。然而，在现实世界的临床环境中，低分辨率数据通常是由与简单下采样显着不同的采集机制产生的。因此，这些输入可能位于训练数据域之外，从而由于域转移而导致模型泛化不良。为了解决这个限制，我们提出了一种分布式深度学习框架，可以提高模型的鲁棒性和领域泛化能力。我们开发这种方法是为了提高 4D Flow MRI (4DF) 的分辨率。这是一种新颖的成像方式，可以捕获血流动力学流速和临床相关指标，例如血管壁应力。这些指标对于评估动脉瘤破裂风险至关重要。我们的模型最初是在高分辨率计算流体动力学 (CFD) 模拟及其下采样模拟上进行训练的。然后在配对 4D Flow MRI 和 CFD 样本的小型协调数据集上对其进行微调。我们推导出分布估计器的理论特性，并通过实际数据应用证明我们的框架显着优于传统的深度学习方法。这凸显了分布式学习在解决领域转移和提高临床现实场景中的超分辨率性能方面的有效性。</li>
</ul>

<h3>Title: Time-Archival Camera Virtualization for Sports and Visual Performances</h3>
<ul>
<li><strong>Authors: </strong>Yunxiao Zhang, William Stone, Suryansh Kumar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15181">https://arxiv.org/abs/2602.15181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15181">https://arxiv.org/pdf/2602.15181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15181]] Time-Archival Camera Virtualization for Sports and Visual Performances(https://arxiv.org/abs/2602.15181)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Camera virtualization -- an emerging solution to novel view synthesis -- holds transformative potential for visual entertainment, live performances, and sports broadcasting by enabling the generation of photorealistic images from novel viewpoints using images from a limited set of calibrated multiple static physical cameras. Despite recent advances, achieving spatially and temporally coherent and photorealistic rendering of dynamic scenes with efficient time-archival capabilities, particularly in fast-paced sports and stage performances, remains challenging for existing approaches. Recent methods based on 3D Gaussian Splatting (3DGS) for dynamic scenes could offer real-time view-synthesis results. Yet, they are hindered by their dependence on accurate 3D point clouds from the structure-from-motion method and their inability to handle large, non-rigid, rapid motions of different subjects (e.g., flips, jumps, articulations, sudden player-to-player transitions). Moreover, independent motions of multiple subjects can break the Gaussian-tracking assumptions commonly used in 4DGS, ST-GS, and other dynamic splatting variants. This paper advocates reconsidering a neural volume rendering formulation for camera virtualization and efficient time-archival capabilities, making it useful for sports broadcasting and related applications. By modeling a dynamic scene as rigid transformations across multiple synchronized camera views at a given time, our method performs neural representation learning, providing enhanced visual rendering quality at test time. A key contribution of our approach is its support for time-archival, i.e., users can revisit any past temporal instance of a dynamic scene and can perform novel view synthesis, enabling retrospective rendering for replay, analysis, and archival of live events, a functionality absent in existing neural rendering approaches and novel view synthesis...</li>
<li><strong>摘要：</strong>相机虚拟化是一种新兴的新颖视图合成解决方案，通过使用有限的一组已校准的多个静态物理相机的图像，从新颖的视角生成逼真的图像，为视觉娱乐、现场表演和体育广播带来了变革潜力。尽管最近取得了进展，但通过高效的时间存档功能实现动态场景的空间和时间连贯且逼真的渲染，特别是在快节奏的体育和舞台表演中，对现有方法来说仍然具有挑战性。最近基于 3D 高斯分布 (3DGS) 的动态场景方法可以提供实时视图合成结果。然而，它们依赖于运动结构方法中的精确 3D 点云，并且无法处理不同主体的大型、非刚性、快速运动（例如翻转、跳跃、关节、突然的玩家到玩家转换），从而阻碍了它们的发展。此外，多个主体的独立运动可以打破 4DGS、ST-GS 和其他动态泼溅变体中常用的高斯跟踪假设。本文主张重新考虑用于摄像机虚拟化和高效时间存档功能的神经体积渲染公式，使其可用于体育广播和相关应用。通过在给定时间将动态场景建模为跨多个同步摄像机视图的刚性变换，我们的方法执行神经表示学习，从而在测试时提供增强的视觉渲染质量。我们的方法的一个关键贡献是它对时间存档的支持，即用户可以重新访问动态场景的任何过去的时间实例，并且可以执行新颖的视图合成，从而实现对实时事件的重播、分析和存档的回顾性渲染，这是现有神经渲染方法和新颖的视图合成中不存在的功能......</li>
</ul>

<h3>Title: BindCLIP: A Unified Contrastive-Generative Representation Learning Framework for Virtual Screening</h3>
<ul>
<li><strong>Authors: </strong>Anjie Qiao, Zhen Wang, Yaliang Li, Jiahua Rao, Yuedong Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15236">https://arxiv.org/abs/2602.15236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15236">https://arxiv.org/pdf/2602.15236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15236]] BindCLIP: A Unified Contrastive-Generative Representation Learning Framework for Virtual Screening(https://arxiv.org/abs/2602.15236)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Virtual screening aims to efficiently identify active ligands from massive chemical libraries for a given target pocket. Recent CLIP-style models such as DrugCLIP enable scalable virtual screening by embedding pockets and ligands into a shared space. However, our analyses indicate that such representations can be insensitive to fine-grained binding interactions and may rely on shortcut correlations in training data, limiting their ability to rank ligands by true binding compatibility. To address these issues, we propose BindCLIP, a unified contrastive-generative representation learning framework for virtual screening. BindCLIP jointly trains pocket and ligand encoders using CLIP-style contrastive learning together with a pocket-conditioned diffusion objective for binding pose generation, so that pose-level supervision directly shapes the retrieval embedding space toward interaction-relevant features. To further mitigate shortcut reliance, we introduce hard-negative augmentation and a ligand-ligand anchoring regularizer that prevents representation collapse. Experiments on two public benchmarks demonstrate consistent improvements over strong baselines. BindCLIP achieves substantial gains on challenging out-of-distribution virtual screening and improves ligand-analogue ranking on the FEP+ benchmark. Together, these results indicate that integrating generative, pose-level supervision with contrastive learning yields more interaction-aware embeddings and improves generalization in realistic screening settings, bringing virtual screening closer to real-world applicability.</li>
<li><strong>摘要：</strong>虚拟筛选旨在从大量化学库中有效识别给定目标口袋的活性配体。最近的 CLIP 式模型（例如 DrugCLIP）通过将口袋和配体嵌入共享空间来实现可扩展的虚拟筛选。然而，我们的分析表明，这种表示可能对细粒度的结合相互作用不敏感，并且可能依赖于训练数据中的快捷相关性，从而限制了它们通过真实的结合兼容性对配体进行排序的能力。为了解决这些问题，我们提出了 BindCLIP，一个用于虚拟筛选的统一对比生成表示学习框架。 BindCLIP 使用 CLIP 式对比学习以及用于绑定姿势生成的口袋条件扩散目标来联合训练口袋和配体编码器，以便姿势级监督直接将检索嵌入空间塑造为交互相关特征。为了进一步减轻捷径依赖，我们引入了硬负增强和配体-配体锚定正则化器，以防止表示崩溃。对两个公共基准的实验表明，在强基准的基础上取得了持续的改进。 BindCLIP 在具有挑战性的分布外虚拟筛选方面取得了实质性进展，并提高了 FEP+ 基准上的配体类似物排名。总之，这些结果表明，将生成、姿势级监督与对比学习相结合，可以产生更多的交互感知嵌入，并提高现实筛选设置中的泛化能力，使虚拟筛选更接近现实世界的适用性。</li>
</ul>

<h3>Title: Closing the Distribution Gap in Adversarial Training for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Chengzhi Hu, Jonas Dornbusch, David Lüdke, Stephan Günnemann, Leo Schwinn</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15238">https://arxiv.org/abs/2602.15238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15238">https://arxiv.org/pdf/2602.15238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15238]] Closing the Distribution Gap in Adversarial Training for LLMs(https://arxiv.org/abs/2602.15238)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Adversarial training for LLMs is one of the most promising methods to reliably improve robustness against adversaries. However, despite significant progress, models remain vulnerable to simple in-distribution exploits, such as rewriting prompts in the past tense or translating them into other languages. We argue that this persistent fragility stems from a fundamental limitation in current adversarial training algorithms: they minimize adversarial loss on their training set but inadequately cover the data distribution, resulting in vulnerability to seemingly simple attacks. To bridge this gap, we propose Distributional Adversarial Training, DAT. We leverage Diffusion LLMs to approximate the true joint distribution of prompts and responses, enabling generation of diverse, high-likelihood samples that address generalization failures. By combining optimization over the data distribution provided by the diffusion model with continuous adversarial training, DAT achieves substantially higher adversarial robustness than previous methods.</li>
<li><strong>摘要：</strong>法学硕士的对抗性训练是可靠地提高对抗对手鲁棒性的最有前途的方法之一。然而，尽管取得了重大进展，模型仍然容易受到简单的分布漏洞的攻击，例如用过去时态重写提示或将其翻译成其他语言。我们认为，这种持续的脆弱性源于当前对抗性训练算法的一个基本限制：它们最大限度地减少了训练集上的对抗性损失，但不足以覆盖数据分布，导致容易受到看似简单的攻击。为了弥补这一差距，我们提出了分布式对抗训练（DAT）。我们利用 Diffusion LLM 来近似提示和响应的真实联合分布，从而能够生成解决泛化失败问题的多样化、高似然样本。通过将对扩散模型提供的数据分布的优化与连续对抗训练相结合，DAT 实现了比以前的方法更高的对抗鲁棒性。</li>
</ul>

<h3>Title: Visual Persuasion: What Influences Decisions of Vision-Language Models?</h3>
<ul>
<li><strong>Authors: </strong>Manuel Cherep, Pranav M R, Pattie Maes, Nikhil Singh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15278">https://arxiv.org/abs/2602.15278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15278">https://arxiv.org/pdf/2602.15278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15278]] Visual Persuasion: What Influences Decisions of Vision-Language Models?(https://arxiv.org/abs/2602.15278)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The web is littered with images, once created for human consumption and now increasingly interpreted by agents using vision-language models (VLMs). These agents make visual decisions at scale, deciding what to click, recommend, or buy. Yet, we know little about the structure of their visual preferences. We introduce a framework for studying this by placing VLMs in controlled image-based choice tasks and systematically perturbing their inputs. Our key idea is to treat the agent's decision function as a latent visual utility that can be inferred through revealed preference: choices between systematically edited images. Starting from common images, such as product photos, we propose methods for visual prompt optimization, adapting text optimization methods to iteratively propose and apply visually plausible modifications using an image generation model (such as in composition, lighting, or background). We then evaluate which edits increase selection probability. Through large-scale experiments on frontier VLMs, we demonstrate that optimized edits significantly shift choice probabilities in head-to-head comparisons. We develop an automatic interpretability pipeline to explain these preferences, identifying consistent visual themes that drive selection. We argue that this approach offers a practical and efficient way to surface visual vulnerabilities, safety concerns that might otherwise be discovered implicitly in the wild, supporting more proactive auditing and governance of image-based AI agents.</li>
<li><strong>摘要：</strong>网络上充斥着图像，这些图像曾经是为了人类消费而创建的，现在越来越多地由代理使用视觉语言模型（VLM）来解释。这些代理大规模地做出视觉决策，决定点击、推荐或购买什么。然而，我们对他们的视觉偏好的结构知之甚少。我们引入了一个研究框架，通过将 VLM 置于受控的基于图像的选择任务中并系统地扰动它们的输入。我们的关键思想是将智能体的决策功能视为一种潜在的视觉效用，可以通过显示的偏好来推断：系统编辑的图像之间的选择。从常见图像（例如产品照片）开始，我们提出了视觉提示优化方法，采用文本优化方法，使用图像生成模型（例如构图、照明或背景）迭代地提出和应用视觉上合理的修改。然后我们评估哪些编辑会增加选择概率。通过对前沿 VLM 的大规模实验，我们证明优化编辑可以显着改变头对头比较中的选择概率。我们开发了一个自动可解释性管道来解释这些偏好，识别驱动选择的一致视觉主题。我们认为，这种方法提供了一种实用且有效的方法来揭示视觉漏洞和安全问题，否则这些漏洞可能会在野外隐式地被发现，从而支持对基于图像的人工智能代理进行更主动的审计和治理。</li>
</ul>

<h3>Title: Consistency-Preserving Diverse Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Xinshuang Liu, Runfa Blark Li, Truong Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15287">https://arxiv.org/abs/2602.15287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15287">https://arxiv.org/pdf/2602.15287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15287]] Consistency-Preserving Diverse Video Generation(https://arxiv.org/abs/2602.15287)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-video generation is expensive, so only a few samples are typically produced per prompt. In this low-sample regime, maximizing the value of each batch requires high cross-video diversity. Recent methods improve diversity for image generation, but for videos they often degrade within-video temporal consistency and require costly backpropagation through a video decoder. We propose a joint-sampling framework for flow-matching video generators that improves batch diversity while preserving temporal consistency. Our approach applies diversity-driven updates and then removes only the components that would decrease a temporal-consistency objective. To avoid image-space gradients, we compute both objectives with lightweight latent-space models, avoiding video decoding and decoder backpropagation. Experiments on a state-of-the-art text-to-video flow-matching model show diversity comparable to strong joint-sampling baselines while substantially improving temporal consistency and color naturalness. Code will be released.</li>
<li><strong>摘要：</strong>文本到视频的生成成本很高，因此每个提示通常只生成几个样本。在这种低样本情况下，最大化每个批次的价值需要高度的跨视频多样性。最近的方法提高了图像生成的多样性，但对于视频来说，它们通常会降低视频内的时间一致性，并且需要通过视频解码器进行昂贵的反向传播。我们提出了一种用于流匹配视频生成器的联合采样框架，该框架可以提高批量多样性，同时保持时间一致性。我们的方法应用多样性驱动的更新，然后仅删除会降低时间一致性目标的组件。为了避免图像空间梯度，我们使用轻量级潜在空间模型计算两个目标，避免视频解码和解码器反向传播。最先进的文本到视频流匹配模型的实验显示出与强联合采样基线相当的多样性，同时显着提高了时间一致性和颜色自然度。代码将被发布。</li>
</ul>

<h3>Title: Prescriptive Scaling Reveals the Evolution of Language Model Capabilities</h3>
<ul>
<li><strong>Authors: </strong>Hanlin Zhang, Jikai Jin, Vasilis Syrgkanis, Sham Kakade</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15327">https://arxiv.org/abs/2602.15327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15327">https://arxiv.org/pdf/2602.15327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15327]] Prescriptive Scaling Reveals the Evolution of Language Model Capabilities(https://arxiv.org/abs/2602.15327)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>For deploying foundation models, practitioners increasingly need prescriptive scaling laws: given a pre training compute budget, what downstream accuracy is attainable with contemporary post training practice, and how stable is that mapping as the field evolves? Using large scale observational evaluations with 5k observational and 2k newly sampled data on model performance, we estimate capability boundaries, high conditional quantiles of benchmark scores as a function of log pre training FLOPs, via smoothed quantile regression with a monotone, saturating sigmoid parameterization. We validate the temporal reliability by fitting on earlier model generations and evaluating on later releases. Across various tasks, the estimated boundaries are mostly stable, with the exception of math reasoning that exhibits a consistently advancing boundary over time. We then extend our approach to analyze task dependent saturation and to probe contamination related shifts on math reasoning tasks. Finally, we introduce an efficient algorithm that recovers near full data frontiers using roughly 20% of evaluation budget. Together, our work releases the Proteus 2k, the latest model performance evaluation dataset, and introduces a practical methodology for translating compute budgets into reliable performance expectations and for monitoring when capability boundaries shift across time.</li>
<li><strong>摘要：</strong>为了部署基础模型，从业者越来越需要规定的缩放法则：给定训练前计算预算，当代训练后实践可以达到什么下游精度，以及随着领域的发展，映射的稳定性如何？使用关于模型性能的 5k 观察数据和 2k 新采样数据进行大规模观察评估，我们通过具有单调、饱和 sigmoid 参数化的平滑分位数回归来估计能力边界、基准分数的高条件分位数作为对数预训练 FLOP 的函数。我们通过拟合早期模型并评估后续版本来验证时间可靠性。在各种任务中，估计的边界大多是稳定的，但数学推理除外，它随着时间的推移呈现出持续推进的边界。然后，我们扩展我们的方法来分析任务相关的饱和度并探测数学推理任务中与污染相关的变化。最后，我们引入了一种有效的算法，该算法使用大约 20% 的评估预算来恢复接近完整的数据边界。我们共同发布了最新的模型性能评估数据集 Proteus 2k，并引入了一种实用方法，用于将计算预算转化为可靠的性能预期，并监控能力边界何时随时间变化。</li>
</ul>

<h3>Title: DAV-GSWT: Diffusion-Active-View Sampling for Data-Efficient Gaussian Splatting Wang Tiles</h3>
<ul>
<li><strong>Authors: </strong>Rong Fu, Jiekai Wu, Haiyun Wei, Yee Tan Jia, Wenxin Zhang, Yang Li, Xiaowen Ma, Wangyu Wu, Simon Fong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15355">https://arxiv.org/abs/2602.15355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15355">https://arxiv.org/pdf/2602.15355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15355]] DAV-GSWT: Diffusion-Active-View Sampling for Data-Efficient Gaussian Splatting Wang Tiles(https://arxiv.org/abs/2602.15355)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The emergence of 3D Gaussian Splatting has fundamentally redefined the capabilities of photorealistic neural rendering by enabling high-throughput synthesis of complex environments. While procedural methods like Wang Tiles have recently been integrated to facilitate the generation of expansive landscapes, these systems typically remain constrained by a reliance on densely sampled exemplar reconstructions. We present DAV-GSWT, a data-efficient framework that leverages diffusion priors and active view sampling to synthesize high-fidelity Gaussian Splatting Wang Tiles from minimal input observations. By integrating a hierarchical uncertainty quantification mechanism with generative diffusion models, our approach autonomously identifies the most informative viewpoints while hallucinating missing structural details to ensure seamless tile transitions. Experimental results indicate that our system significantly reduces the required data volume while maintaining the visual integrity and interactive performance necessary for large-scale virtual environments.</li>
<li><strong>摘要：</strong>The emergence of 3D Gaussian Splatting has fundamentally redefined the capabilities of photorealistic neural rendering by enabling high-throughput synthesis of complex environments.虽然像 Wang Tiles 这样的程序方法最近已被集成以促进广阔景观的生成，但这些系统通常仍然受到对密集采样样本重建的依赖的限制。我们提出了 DAV-GSWT，这是一种数据高效的框架，它利用扩散先验和主动视图采样，从最小的输入观察中合成高保真高斯 Splatting Wang Tiles。 By integrating a hierarchical uncertainty quantification mechanism with generative diffusion models, our approach autonomously identifies the most informative viewpoints while hallucinating missing structural details to ensure seamless tile transitions. Experimental results indicate that our system significantly reduces the required data volume while maintaining the visual integrity and interactive performance necessary for large-scale virtual environments.</li>
</ul>

<h3>Title: GMAIL: Generative Modality Alignment for generated Image Learning</h3>
<ul>
<li><strong>Authors: </strong>Shentong Mo, Sukmin Yun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15368">https://arxiv.org/abs/2602.15368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15368">https://arxiv.org/pdf/2602.15368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15368]] GMAIL: Generative Modality Alignment for generated Image Learning(https://arxiv.org/abs/2602.15368)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models have made it possible to synthesize highly realistic images, potentially providing an abundant data source for training machine learning models. Despite the advantages of these synthesizable data sources, the indiscriminate use of generated images as real images for training can even cause mode collapse due to modality discrepancies between real and synthetic domains. In this paper, we propose a novel framework for discriminative use of generated images, coined GMAIL, that explicitly treats generated images as a separate modality from real images. Instead of indiscriminately replacing real images with generated ones in the pixel space, our approach bridges the two distinct modalities in the same latent space through a multi-modal learning approach. To be specific, we first fine-tune a model exclusively on generated images using a cross-modality alignment loss and then employ this aligned model to further train various vision-language models with generated images. By aligning the two modalities, our approach effectively leverages the benefits of recent advances in generative models, thereby boosting the effectiveness of generated image learning across a range of vision-language tasks. Our framework can be easily incorporated with various vision-language models, and we demonstrate its efficacy throughout extensive experiments. For example, our framework significantly improves performance on image captioning, zero-shot image retrieval, zero-shot image classification, and long caption retrieval tasks. It also shows positive generated data scaling trends and notable enhancements in the captioning performance of the large multimodal model, LLaVA.</li>
<li><strong>摘要：</strong>生成模型使得合成高度逼真的图像成为可能，从而可能为训练机器学习模型提供丰富的数据源。尽管这些可合成数据源具有优势，但不加区别地使用生成的图像作为真实图像进行训练甚至可能由于真实域和合成域之间的模态差异而导致模式崩溃。在本文中，我们提出了一种用于区分使用生成图像的新颖框架，即 GMAIL，它明确地将生成图像视为与真实图像不同的模式。我们的方法不是不加区别地用像素空间中生成的图像替换真实图像，而是通过多模态学习方法在同一潜在空间中桥接两种不同的模态。具体来说，我们首先使用跨模态对齐损失专门在生成的图像上微调模型，然后利用该对齐模型进一步使用生成的图像训练各种视觉语言模型。通过调整这两种模式，我们的方法有效地利用了生成模型最新进展的优势，从而提高了一系列视觉语言任务中生成图像学习的有效性。我们的框架可以轻松地与各种视觉语言模型相结合，并且我们通过广泛的实验证明了其功效。例如，我们的框架显着提高了图像字幕、零样本图像检索、零样本图像分类和长字幕检索任务的性能。它还显示了积极的生成数据扩展趋势以及大型多模态模型 LLaVA 的字幕性能显着增强。</li>
</ul>

<h3>Title: Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schrödinger Bridge Matching</h3>
<ul>
<li><strong>Authors: </strong>Jeongwoo Shin, Jinhwan Sul, Joonseok Lee, Jaewong Choi, Jaemoo Choi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15396">https://arxiv.org/abs/2602.15396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15396">https://arxiv.org/pdf/2602.15396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15396]] Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schrödinger Bridge Matching(https://arxiv.org/abs/2602.15396)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models often yield highly curved trajectories and noisy score targets due to an uninformative, memoryless forward process that induces independent data-noise coupling. We propose Adjoint Schrödinger Bridge Matching (ASBM), a generative modeling framework that recovers optimal trajectories in high dimensions via two stages. First, we view the Schrödinger Bridge (SB) forward dynamic as a coupling construction problem and learn it through a data-to-energy sampling perspective that transports data to an energy-defined prior. Then, we learn the backward generative dynamic with a simple matching loss supervised by the induced optimal coupling. By operating in a non-memoryless regime, ASBM produces significantly straighter and more efficient sampling paths. Compared to prior works, ASBM scales to high-dimensional data with notably improved stability and efficiency. Extensive experiments on image generation show that ASBM improves fidelity with fewer sampling steps. We further showcase the effectiveness of our optimal trajectory via distillation to a one-step generator.</li>
<li><strong>摘要：</strong>由于无信息、无记忆的前向过程会引起独立的数据噪声耦合，扩散模型通常会产生高度弯曲的轨迹和嘈杂的分数目标。我们提出了伴随薛定谔桥匹配（ASBM），这是一种生成建模框架，可以通过两个阶段恢复高维度的最佳轨迹。首先，我们将薛定谔桥（SB）前向动力学视为一个耦合构造问题，并通过数据到能量采样的角度来学习它，将数据传输到能量定义的先验。然后，我们通过诱导最优耦合监督的简单匹配损失来学习反向生成动态。通过在非无记忆机制下运行，ASBM 产生明显更直、更高效的采样路径。与之前的工作相比，ASBM 可扩展到高维数据，并且稳定性和效率显着提高。关于图像生成的大量实验表明，ASBM 通过更少的采样步骤提高了保真度。我们进一步展示了通过蒸馏到一步生成器的最佳轨迹的有效性。</li>
</ul>

<h3>Title: RPT-SR: Regional Prior attention Transformer for infrared image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Youngwan Jin, Incheol Park, Yagiz Nalcakan, Hyeongjin Ju, Sanghyeop Yeo, Shiho Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15490">https://arxiv.org/abs/2602.15490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15490">https://arxiv.org/pdf/2602.15490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15490]] RPT-SR: Regional Prior attention Transformer for infrared image Super-Resolution(https://arxiv.org/abs/2602.15490)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>General-purpose super-resolution models, particularly Vision Transformers, have achieved remarkable success but exhibit fundamental inefficiencies in common infrared imaging scenarios like surveillance and autonomous driving, which operate from fixed or nearly-static viewpoints. These models fail to exploit the strong, persistent spatial priors inherent in such scenes, leading to redundant learning and suboptimal performance. To address this, we propose the Regional Prior attention Transformer for infrared image Super-Resolution (RPT-SR), a novel architecture that explicitly encodes scene layout information into the attention mechanism. Our core contribution is a dual-token framework that fuses (1) learnable, regional prior tokens, which act as a persistent memory for the scene's global structure, with (2) local tokens that capture the frame-specific content of the current input. By utilizing these tokens into an attention, our model allows the priors to dynamically modulate the local reconstruction process. Extensive experiments validate our approach. While most prior works focus on a single infrared band, we demonstrate the broad applicability and versatility of RPT-SR by establishing new state-of-the-art performance across diverse datasets covering both Long-Wave (LWIR) and Short-Wave (SWIR) spectra</li>
<li><strong>摘要：</strong>通用超分辨率模型，特别是 Vision Transformers，取得了显着的成功，但在监控和自动驾驶等常见红外成像场景中表现出根本性的低效率，这些场景从固定或近静态的视点进行操作。这些模型无法利用此类场景中固有的强大、持久的空间先验，导致冗余学习和次优性能。为了解决这个问题，我们提出了用于红外图像超分辨率的区域先验注意力变换器（RPT-SR），这是一种新颖的架构，它将场景布局信息显式编码到注意力机制中。我们的核心贡献是一个双令牌框架，它融合了（1）可学习的区域先验令牌（充当场景全局结构的持久存储器）和（2）捕获当前输入的特定于帧的内容的本地令牌。通过将这些标记用于注意力，我们的模型允许先验动态地调节局部重建过程。大量的实验验证了我们的方法。虽然大多数先前的工作都集中在单个红外波段，但我们通过在涵盖长波 (LWIR) 和短波 (SWIR) 光谱的不同数据集上建立新的最先进的性能，展示了 RPT-SR 的广泛适用性和多功能性</li>
</ul>

<h3>Title: Dynamic Training-Free Fusion of Subject and Style LoRAs</h3>
<ul>
<li><strong>Authors: </strong>Qinglong Cao, Yuntian Chen, Chao Ma, Xiaokang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.SC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15539">https://arxiv.org/abs/2602.15539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15539">https://arxiv.org/pdf/2602.15539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15539]] Dynamic Training-Free Fusion of Subject and Style LoRAs(https://arxiv.org/abs/2602.15539)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent studies have explored the combination of multiple LoRAs to simultaneously generate user-specified subjects and styles. However, most existing approaches fuse LoRA weights using static statistical heuristics that deviate from LoRA's original purpose of learning adaptive feature adjustments and ignore the randomness of sampled inputs. To address this, we propose a dynamic training-free fusion framework that operates throughout the generation process. During the forward pass, at each LoRA-applied layer, we dynamically compute the KL divergence between the base model's original features and those produced by subject and style LoRAs, respectively, and adaptively select the most appropriate weights for fusion. In the reverse denoising stage, we further refine the generation trajectory by dynamically applying gradient-based corrections derived from objective metrics such as CLIP and DINO scores, providing continuous semantic and stylistic guidance. By integrating these two complementary mechanisms-feature-level selection and metric-guided latent adjustment-across the entire diffusion timeline, our method dynamically achieves coherent subject-style synthesis without any retraining. Extensive experiments across diverse subject-style combinations demonstrate that our approach consistently outperforms state-of-the-art LoRA fusion methods both qualitatively and quantitatively.</li>
<li><strong>摘要：</strong>最近的研究探索了多个 LoRA 的组合，以同时生成用户指定的主题和风格。然而，大多数现有方法使用静态统计启发式融合 LoRA 权重，这偏离了 LoRA 学习自适应特征调整的原始目的，并且忽略了采样输入的随机性。为了解决这个问题，我们提出了一个动态的免训练融合框架，该框架在整个生成过程中运行。在前向传递过程中，在每个 LoRA 应用层，我们分别动态计算基础模型的原始特征与主题和风格 LoRA 生成的特征之间的 KL 散度，并自适应地选择最合适的权重进行融合。在反向去噪阶段，我们通过动态应用从 CLIP 和 DINO 分数等客观指标得出的基于梯度的校正来进一步细化生成轨迹，提供连续的语义和风格指导。通过在整个扩散时间线上集成这两种互补机制（特征级别选择和度量引导的潜在调整），我们的方法动态地实现了连贯的主题风格合成，而无需任何重新训练。跨不同主题风格组合的广泛实验表明，我们的方法在质量和数量上始终优于最先进的 LoRA 融合方法。</li>
</ul>

<h3>Title: 1-Bit Wonder: Improving QAT Performance in the Low-Bit Regime through K-Means Quantization</h3>
<ul>
<li><strong>Authors: </strong>Sohir Maskey, Constantin Eichenberg, Johannes Messner, Douglas Orr</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15563">https://arxiv.org/abs/2602.15563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15563">https://arxiv.org/pdf/2602.15563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15563]] 1-Bit Wonder: Improving QAT Performance in the Low-Bit Regime through K-Means Quantization(https://arxiv.org/abs/2602.15563)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Quantization-aware training (QAT) is an effective method to drastically reduce the memory footprint of LLMs while keeping performance degradation at an acceptable level. However, the optimal choice of quantization format and bit-width presents a challenge in practice. The full design space of quantization is not fully explored in the context of QAT, and the precise trade-off between quantization and downstream performance is poorly understood, as comparisons often rely solely on perplexity-based evaluations. In this work, we address these shortcomings with an empirical study of QAT in the low-bit regime. We show that k-means based weight quantization outperforms integer formats and can be implemented efficiently on standard hardware. Furthermore, we find that, under a fixed inference memory budget, the best performance on generative downstream tasks is achieved with $1$-bit quantized weights.</li>
<li><strong>摘要：</strong>量化感知训练 (QAT) 是一种有效方法，可以大幅减少 LLM 的内存占用，同时将性能下降保持在可接受的水平。然而，量化格式和位宽的最佳选择在实践中提出了挑战。在 QAT 的背景下，量化的完整设计空间尚未得到充分探索，并且量化和下游性能之间的精确权衡也知之甚少，因为比较通常仅依赖于基于困惑度的评估。在这项工作中，我们通过对低位机制中 QAT 的实证研究来解决这些缺点。我们证明基于 k 均值的权重量化优于整数格式，并且可以在标准硬件上有效实现。此外，我们发现，在固定的推理内存预算下，生成下游任务的最佳性能是通过 1 美元位量化权重实现的。</li>
</ul>

<h3>Title: Continuous-Time Piecewise-Linear Recurrent Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Alena Brändle, Lukas Eisenmann, Florian Götz, Daniel Durstewitz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15649">https://arxiv.org/abs/2602.15649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15649">https://arxiv.org/pdf/2602.15649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15649]] Continuous-Time Piecewise-Linear Recurrent Neural Networks(https://arxiv.org/abs/2602.15649)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In dynamical systems reconstruction (DSR) we aim to recover the dynamical system (DS) underlying observed time series. Specifically, we aim to learn a generative surrogate model which approximates the underlying, data-generating DS, and recreates its long-term properties (`climate statistics'). In scientific and medical areas, in particular, these models need to be mechanistically tractable -- through their mathematical analysis we would like to obtain insight into the recovered system's workings. Piecewise-linear (PL), ReLU-based RNNs (PLRNNs) have a strong track-record in this regard, representing SOTA DSR models while allowing mathematical insight by virtue of their PL design. However, all current PLRNN variants are discrete-time maps. This is in disaccord with the assumed continuous-time nature of most physical and biological processes, and makes it hard to accommodate data arriving at irregular temporal intervals. Neural ODEs are one solution, but they do not reach the DSR performance of PLRNNs and often lack their tractability. Here we develop theory for continuous-time PLRNNs (cPLRNNs): We present a novel algorithm for training and simulating such models, bypassing numerical integration by efficiently exploiting their PL structure. We further demonstrate how important topological objects like equilibria or limit cycles can be determined semi-analytically in trained models. We compare cPLRNNs to both their discrete-time cousins as well as Neural ODEs on DSR benchmarks, including systems with discontinuities which come with hard thresholds.</li>
<li><strong>摘要：</strong>在动力系统重建（DSR）中，我们的目标是恢复观察到的时间序列背后的动力系统（DS）。具体来说，我们的目标是学习一种生成代理模型，该模型近似于底层的数据生成 DS，并重新创建其长期属性（“气候统计”）。特别是在科学和医学领域，这些模型需要在机械上易于处理——通过它们的数学分析，我们希望深入了解恢复系统的工作原理。分段线性 (PL)、基于 ReLU 的 RNN (PLRNN) 在这方面拥有良好的记录，代表 SOTA DSR 模型，同时凭借其 PL 设计提供数学洞察力。然而，当前所有的 PLRNN 变体都是离散时间映射。这与大多数物理和生物过程假设的连续时间性质不一致，并且很难容纳以不规则时间间隔到达的数据。神经 ODE 是一种解决方案，但它们达不到 PLRNN 的 DSR 性能，并且通常缺乏可处理性。在这里，我们开发了连续时间 PLRNN (cPLRNN) 的理论：我们提出了一种用于训练和模拟此类模型的新算法，通过有效利用其 PL 结构来绕过数值积分。我们进一步证明了如何在训练模型中半解析地确定重要的拓扑对象（例如平衡或极限环）。我们将 cPLRNN 与其离散时间同类以及 DSR 基准上的神经常微分方程进行比较，包括具有硬阈值的不连续性系统。</li>
</ul>

<h3>Title: Concept-Enhanced Multimodal RAG: Towards Interpretable and Accurate Radiology Report Generation</h3>
<ul>
<li><strong>Authors: </strong>Marco Salmè, Federico Siciliano, Fabrizio Silvestri, Paolo Soda, Rosa Sicilia, Valerio Guarrasi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15650">https://arxiv.org/abs/2602.15650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15650">https://arxiv.org/pdf/2602.15650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15650]] Concept-Enhanced Multimodal RAG: Towards Interpretable and Accurate Radiology Report Generation(https://arxiv.org/abs/2602.15650)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Radiology Report Generation (RRG) through Vision-Language Models (VLMs) promises to reduce documentation burden, improve reporting consistency, and accelerate clinical workflows. However, their clinical adoption remains limited by the lack of interpretability and the tendency to hallucinate findings misaligned with imaging evidence. Existing research typically treats interpretability and accuracy as separate objectives, with concept-based explainability techniques focusing primarily on transparency, while Retrieval-Augmented Generation (RAG) methods targeting factual grounding through external retrieval. We present Concept-Enhanced Multimodal RAG (CEMRAG), a unified framework that decomposes visual representations into interpretable clinical concepts and integrates them with multimodal RAG. This approach exploits enriched contextual prompts for RRG, improving both interpretability and factual accuracy. Experiments on MIMIC-CXR and IU X-Ray across multiple VLM architectures, training regimes, and retrieval configurations demonstrate consistent improvements over both conventional RAG and concept-only baselines on clinical accuracy metrics and standard NLP measures. These results challenge the assumed trade-off between interpretability and performance, showing that transparent visual concepts can enhance rather than compromise diagnostic accuracy in medical VLMs. Our modular design decomposes interpretability into visual transparency and structured language model conditioning, providing a principled pathway toward clinically trustworthy AI-assisted radiology.</li>
<li><strong>摘要：</strong>通过视觉语言模型 (VLM) 生成放射学报告 (RRG) 有望减少文档负担、提高报告一致性并加速临床工作流程。然而，由于缺乏可解释性以及容易产生与影像学证据不一致的幻觉结果，它们的临床应用仍然受到限制。现有的研究通常将可解释性和准确性视为独立的目标，基于概念的可解释性技术主要关注透明度，而检索增强生成（RAG）方法则通过外部检索来瞄准事实基础。我们提出了概念增强型多模态 RAG (CEMRAG)，这是一个统一的框架，可将视觉表示分解为可解释的临床概念，并将其与多模态 RAG 集成。这种方法利用了 RRG 的丰富上下文提示，提高了可解释性和事实准确性。跨多个 VLM 架构、训练方案和检索配置的 MIMIC-CXR 和 IU X-Ray 实验表明，在临床准确性指标和标准 NLP 测量方面，与传统 RAG 和纯概念基线相比，有一致的改进。这些结果挑战了可解释性和性能之间假设的权衡，表明透明的视觉概念可以增强而不是损害医学 VLM 的诊断准确性。我们的模块化设计将可解释性分解为视觉透明度和结构化语言模型条件，为临床上值得信赖的人工智能辅助放射学提供了原则性途径。</li>
</ul>

<h3>Title: CAMEL: An ECG Language Model for Forecasting Cardiac Events</h3>
<ul>
<li><strong>Authors: </strong>Neelay Velingker, Alaia Solko-Breslin, Mayank Keoliya, Seewon Choi, Jiayi Xin, Anika Marathe, Alireza Oraii, Rajat Deo, Sameed Khatana, Rajeev Alur, Mayur Naik, Eric Wong</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15677">https://arxiv.org/abs/2602.15677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15677">https://arxiv.org/pdf/2602.15677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15677]] CAMEL: An ECG Language Model for Forecasting Cardiac Events(https://arxiv.org/abs/2602.15677)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Electrocardiograms (ECG) are electrical recordings of the heart that are critical for diagnosing cardiovascular conditions. ECG language models (ELMs) have recently emerged as a promising framework for ECG classification accompanied by report generation. However, current models cannot forecast future cardiac events despite the immense clinical value for planning earlier intervention. To address this gap, we propose CAMEL, the first ELM that is capable of inference over longer signal durations which enables its forecasting capability. Our key insight is a specialized ECG encoder which enables cross-understanding of ECG signals with text. We train CAMEL using established LLM training procedures, combining LoRA adaptation with a curriculum learning pipeline. Our curriculum includes ECG classification, metrics calculations, and multi-turn conversations to elicit reasoning. CAMEL demonstrates strong zero-shot performance across 6 tasks and 9 datasets, including ECGForecastBench, a new benchmark that we introduce for forecasting arrhythmias. CAMEL is on par with or surpasses ELMs and fully supervised baselines both in- and out-of-distribution, achieving SOTA results on ECGBench (+7.0% absolute average gain) as well as ECGForecastBench (+12.4% over fully supervised models and +21.1% over zero-shot ELMs).</li>
<li><strong>摘要：</strong>心电图 (ECG) 是心脏的电记录，对于诊断心血管疾病至关重要。心电图语言模型 (ELM) 最近已成为心电图分类和报告生成的一个有前途的框架。然而，尽管规划早期干预具有巨大的临床价值，但当前的模型无法预测未来的心脏事件。为了解决这一差距，我们提出了 CAMEL，这是第一个能够在较长信号持续时间上进行推理的 ELM，从而实现其预测能力。我们的主要见解是专门的心电图编码器，它可以实现心电图信号与文本的交叉理解。我们使用既定的法学硕士培训程序来培训 CAMEL，将 LoRA 适应与课程学习流程相结合。我们的课程包括心电图分类、指标计算和多轮对话以引发推理。 CAMEL 在 6 个任务和 9 个数据集上展示了强大的零样本性能，其中包括 ECGForecastBench，这是我们为预测心律失常引入的新基准。 CAMEL 与 ELM 相当或超越分布内和分布外的完全监督基线，在 ECGBench（+7.0% 绝对平均增益）和 ECGForecastBench（比完全监督模型+12.4%，比零样本 ELM+21.1%）上实现了 SOTA 结果。</li>
</ul>

<h3>Title: MRC-GAT: A Meta-Relational Copula-Based Graph Attention Network for Interpretable Multimodal Alzheimer's Disease Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Fatemeh Khalvandi, Saadat Izadi, Abdolah Chalechale</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15740">https://arxiv.org/abs/2602.15740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15740">https://arxiv.org/pdf/2602.15740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15740]] MRC-GAT: A Meta-Relational Copula-Based Graph Attention Network for Interpretable Multimodal Alzheimer's Disease Diagnosis(https://arxiv.org/abs/2602.15740)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Alzheimer's disease (AD) is a progressive neurodegenerative condition necessitating early and precise diagnosis to provide prompt clinical management. Given the paramount importance of early diagnosis, recent studies have increasingly focused on computer-aided diagnostic models to enhance precision and reliability. However, most graph-based approaches still rely on fixed structural designs, which restrict their flexibility and limit generalization across heterogeneous patient data. To overcome these limitations, the Meta-Relational Copula-Based Graph Attention Network (MRC-GAT) is proposed as an efficient multimodal model for AD classification tasks. The proposed architecture, copula-based similarity alignment, relational attention, and node fusion are integrated as the core components of episodic meta-learning, such that the multimodal features, including risk factors (RF), Cognitive test scores, and MRI attributes, are first aligned via a copula-based transformation in a common statistical space and then combined by a multi-relational attention mechanism. According to evaluations performed on the TADPOLE and NACC datasets, the MRC-GAT model achieved accuracies of 96.87% and 92.31%, respectively, demonstrating state-of-the-art performance compared to existing diagnostic models. Finally, the proposed model confirms the robustness and applicability of the proposed method by providing interpretability at various stages of disease diagnosis.</li>
<li><strong>摘要：</strong>阿尔茨海默病 (AD) 是一种进行性神经退行性疾病，需要早期准确的诊断以提供及时的临床治疗。鉴于早期诊断的重要性，最近的研究越来越关注计算机辅助诊断模型，以提高精度和可靠性。然而，大多数基于图的方法仍然依赖于固定的结构设计，这限制了它们的灵活性并限制了跨异构患者数据的泛化。为了克服这些限制，基于元关系 Copula 的图注意网络（MRC-GAT）被提出作为 AD 分类任务的有效多模态模型。所提出的架构、基于 copula 的相似性对齐、关系注意和节点融合被集成为情景元学习的核心组件，使得包括风险因素 (RF)、认知测试分数和 MRI 属性在内的多模态特征首先通过公共统计空间中基于 copula 的变换进行对齐，然后通过多关系注意机制进行组合。根据对 TADPOLE 和 NACC 数据集进行的评估，MRC-GAT 模型的准确率分别达到 96.87% 和 92.31%，与现有诊断模型相比，表现出最先进的性能。最后，所提出的模型通过在疾病诊断的各个阶段提供可解释性，证实了所提出方法的稳健性和适用性。</li>
</ul>

<h3>Title: GLM-5: from Vibe Coding to Agentic Engineering</h3>
<ul>
<li><strong>Authors: </strong>GLM-5 Team: Aohan Zeng, Xin Lv, Zhenyu Hou, Zhengxiao Du, Qinkai Zheng, Bin Chen, Da Yin, Chendi Ge, Chengxing Xie, Cunxiang Wang, Gengzheng Pan, Hao Zeng, Haoke Zhang, Haoran Wang, Huilong Chen, Jiajie Zhang, Jian Jiao, Jiaqi Guo, Jingsen Wang, Jingzhao Du, Jinzhu Wu, Kedong Wang, Lei Li, Lin Fan, Lucen Zhong, Mingdao Liu, Mingming Zhao, Pengfan Du, Qian Dong, Rui Lu, Shuang-Li, Shulin Cao, Song Liu, Ting Jiang, Xiaodong Chen, Xiaohan Zhang, Xuancheng Huang, Xuezhen Dong, Yabo Xu, Yao Wei, Yifan An, Yilin Niu, Yitong Zhu, Yuanhao Wen, Yukuo Cen, Yushi Bai, Zhongpei Qiao, Zihan Wang, Zikang Wang, Zilin Zhu, Ziqiang Liu, Zixuan Li, Bojie Wang, Bosi Wen, Can Huang, Changpeng Cai, Chao Yu, Chen Li, Chen Li, Chenghua Huang, Chengwei Hu, Chenhui Zhang, Chenzheng Zhu, Congfeng Yin, Daoyan Lin, Dayong Yang, Di Wang, Ding Ai, Erle Zhu, Fangzhou Yi, Feiyu Chen, Guohong Wen, Hailong Sun, Haisha Zhao, Haiyi Hu, Hanchen Zhang, Hanrui Liu, Hanyu Zhang, Hao Peng, Hao Tai, Haobo Zhang, He Liu, Hongwei Wang, Hongxi Yan, Hongyu Ge, Huan Liu, Huan Liu, Huanpeng Chu, Jia'ni Zhao, Jiachen Wang, Jiajing Zhao, Jiamin Ren, Jiapeng Wang, Jiaxin Zhang, Jiayi Gui, Jiayue Zhao, Jijie Li, Jing An, Jing Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15763">https://arxiv.org/abs/2602.15763</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15763">https://arxiv.org/pdf/2602.15763</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15763]] GLM-5: from Vibe Coding to Agentic Engineering(https://arxiv.org/abs/2602.15763)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks. Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at this https URL.</li>
<li><strong>摘要：</strong>我们提出了 GLM-5，这是一种下一代基础模型，旨在将振动编码范式转变为代理工程。 GLM-5 以其前身的代理、推理和编码 (ARC) 功能为基础，采用 DSA 来显着降低训练和推理成本，同时保持长上下文保真度。为了推进模型对齐和自治，我们实施了一个新的异步强化学习基础设施，通过将生成与训练解耦，极大地提高了训练后的效率。此外，我们提出了新颖的异步代理强化学习算法，可以进一步提高强化学习质量，使模型能够更有效地从复杂的长范围交互中学习。通过这些创新，GLM-5 在主要开放基准测试中实现了最先进的性能。最关键的是，GLM-5 在实际编码任务中展示了前所未有的能力，在处理端到端软件工程挑战方面超越了以前的基线。代码、模型和更多信息可从此 https URL 获取。</li>
</ul>

<h3>Title: Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models</h3>
<ul>
<li><strong>Authors: </strong>Sen Ye, Mengde Xu, Shuyang Gu, Di He, Liwei Wang, Han Hu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15772">https://arxiv.org/abs/2602.15772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15772">https://arxiv.org/pdf/2602.15772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15772]] Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models(https://arxiv.org/abs/2602.15772)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Current research in multimodal models faces a key challenge where enhancing generative capabilities often comes at the expense of understanding, and vice versa. We analyzed this trade-off and identify the primary cause might be the potential conflict between generation and understanding, which creates a competitive dynamic within the model. To address this, we propose the Reason-Reflect-Refine (R3) framework. This innovative algorithm re-frames the single-step generation task into a multi-step process of "generate-understand-regenerate". By explicitly leveraging the model's understanding capability during generation, we successfully mitigate the optimization dilemma, achieved stronger generation results and improved understanding ability which are related to the generation process. This offers valuable insights for designing next-generation unified multimodal models. Code is available at this https URL.</li>
<li><strong>摘要：</strong>当前多模态模型的研究面临着一个关键挑战，即增强生成能力往往以牺牲理解为代价，反之亦然。我们分析了这种权衡，并确定主要原因可能是生成和理解之间的潜在冲突，这在模型中产生了竞争动态。为了解决这个问题，我们提出了 Reason-Reflect-Refine (R3) 框架。这种创新算法将单步生成任务重新构建为“生成-理解-重新生成”的多步过程。通过在生成过程中显式地利用模型的理解能力，我们成功地缓解了优化困境，获得了更强的生成结果，并提高了与生成过程相关的理解能力。这为设计下一代统一多模式模型提供了宝贵的见解。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: VideoSketcher: Video Models Prior Enable Versatile Sequential Sketch Generation</h3>
<ul>
<li><strong>Authors: </strong>Hui Ren, Yuval Alaluf, Omer Bar Tal, Alexander Schwing, Antonio Torralba, Yael Vinker</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15819">https://arxiv.org/abs/2602.15819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15819">https://arxiv.org/pdf/2602.15819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15819]] VideoSketcher: Video Models Prior Enable Versatile Sequential Sketch Generation(https://arxiv.org/abs/2602.15819)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Sketching is inherently a sequential process, in which strokes are drawn in a meaningful order to explore and refine ideas. However, most generative models treat sketches as static images, overlooking the temporal structure that underlies creative drawing. We present a data-efficient approach for sequential sketch generation that adapts pretrained text-to-video diffusion models to generate sketching processes. Our key insight is that large language models and video diffusion models offer complementary strengths for this task: LLMs provide semantic planning and stroke ordering, while video diffusion models serve as strong renderers that produce high-quality, temporally coherent visuals. We leverage this by representing sketches as short videos in which strokes are progressively drawn on a blank canvas, guided by text-specified ordering instructions. We introduce a two-stage fine-tuning strategy that decouples the learning of stroke ordering from the learning of sketch appearance. Stroke ordering is learned using synthetic shape compositions with controlled temporal structure, while visual appearance is distilled from as few as seven manually authored sketching processes that capture both global drawing order and the continuous formation of individual strokes. Despite the extremely limited amount of human-drawn sketch data, our method generates high-quality sequential sketches that closely follow text-specified orderings while exhibiting rich visual detail. We further demonstrate the flexibility of our approach through extensions such as brush style conditioning and autoregressive sketch generation, enabling additional controllability and interactive, collaborative drawing.</li>
<li><strong>摘要：</strong>素描本质上是一个连续的过程，在这个过程中，笔画是按照有意义的顺序绘制的，以探索和完善想法。然而，大多数生成模型将草图视为静态图像，忽略了创意绘画背后的时间结构。我们提出了一种用于顺序草图生成的数据高效方法，该方法采用预先训练的文本到视频扩散模型来生成草图绘制过程。我们的主要见解是，大型语言模型和视频扩散模型为这项任务提供了互补的优势：法学硕士提供语义规划和笔画排序，而视频扩散模型作为强大的渲染器，产生高质量、时间连贯的视觉效果。我们通过将草图表示为短视频来利用这一点，其中在文本指定的排序指令的指导下，在空白画布上逐步绘制笔画。我们引入了一种两阶段微调策略，将笔划顺序的学习与草图外观的学习分离。笔画顺序是使用具有受控时间结构的合成形状组合来学习的，而视觉外观是从多达七个手动创作的草图过程中提取出来的，这些过程捕获了全局绘图顺序和单个笔画的连续形成。尽管人类绘制的草图数据数量极其有限，但我们的方法可以生成高质量的顺序草图，这些草图严格遵循文本指定的顺序，同时展示丰富的视觉细节。我们通过画笔样式调节和自回归草图生成等扩展进一步展示了我们方法的灵活性，从而实现了额外的可控性和交互式协作绘图。</li>
</ul>

<h3>Title: Stabilizing Test-Time Adaptation of High-Dimensional Simulation Surrogates via D-Optimal Statistics</h3>
<ul>
<li><strong>Authors: </strong>Anna Zimmel, Paul Setinek, Gianluca Galletti, Johannes Brandstetter, Werner Zellinger</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15820">https://arxiv.org/abs/2602.15820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15820">https://arxiv.org/pdf/2602.15820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15820]] Stabilizing Test-Time Adaptation of High-Dimensional Simulation Surrogates via D-Optimal Statistics(https://arxiv.org/abs/2602.15820)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Machine learning surrogates are increasingly used in engineering to accelerate costly simulations, yet distribution shifts between training and deployment often cause severe performance degradation (e.g., unseen geometries or configurations). Test-Time Adaptation (TTA) can mitigate such shifts, but existing methods are largely developed for lower-dimensional classification with structured outputs and visually aligned input-output relationships, making them unstable for the high-dimensional, unstructured and regression problems common in simulation. We address this challenge by proposing a TTA framework based on storing maximally informative (D-optimal) statistics, which jointly enables stable adaptation and principled parameter selection at test time. When applied to pretrained simulation surrogates, our method yields up to 7% out-of-distribution improvements at negligible computational cost. To the best of our knowledge, this is the first systematic demonstration of effective TTA for high-dimensional simulation regression and generative design optimization, validated on the SIMSHIFT and EngiBench benchmarks.</li>
<li><strong>摘要：</strong>机器学习代理越来越多地用于工程中，以加速昂贵的模拟，但训练和部署之间的分布变化通常会导致严重的性能下降（例如，看不见的几何形状或配置）。测试时间适应（TTA）可以减轻这种变化，但现有的方法主要是针对具有结构化输出和视觉上对齐的输入输出关系的低维分类而开发的，这使得它们对于模拟中常见的高维、非结构化和回归问题不稳定。我们通过提出一个基于存储最大信息量（D 最优）统计数据的 TTA 框架来应对这一挑战，该框架共同实现了测试时的稳定适应和原则性参数选择。当应用于预训练的模拟代理时，我们的方法以可忽略的计算成本实现了高达 7% 的分布外改进。据我们所知，这是针对高维模拟回归和生成设计优化的有效 TTA 的首次系统演示，并在 SIMSHIFT 和 EngiBench 基准测试上进行了验证。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
