<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-06-18</h1>
<h3>Title: Hidden Bias in the Machine: Stereotypes in Text-to-Image Models</h3>
<ul>
<li><strong>Authors: </strong>Sedat Porikli, Vedat Porikli</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13780">https://arxiv.org/abs/2506.13780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13780">https://arxiv.org/pdf/2506.13780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13780]] Hidden Bias in the Machine: Stereotypes in Text-to-Image Models(https://arxiv.org/abs/2506.13780)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Text-to-Image (T2I) models have transformed visual content creation, producing highly realistic images from natural language prompts. However, concerns persist around their potential to replicate and magnify existing societal biases. To investigate these issues, we curated a diverse set of prompts spanning thematic categories such as occupations, traits, actions, ideologies, emotions, family roles, place descriptions, spirituality, and life events. For each of the 160 unique topics, we crafted multiple prompt variations to reflect a wide range of meanings and perspectives. Using Stable Diffusion 1.5 (UNet-based) and Flux-1 (DiT-based) models with original checkpoints, we generated over 16,000 images under consistent settings. Additionally, we collected 8,000 comparison images from Google Image Search. All outputs were filtered to exclude abstract, distorted, or nonsensical results. Our analysis reveals significant disparities in the representation of gender, race, age, somatotype, and other human-centric factors across generated images. These disparities often mirror and reinforce harmful stereotypes embedded in societal narratives. We discuss the implications of these findings and emphasize the need for more inclusive datasets and development practices to foster fairness in generative visual systems.</li>
<li><strong>摘要：</strong>文本对图像（T2I）模型已改变了视觉内容的创建，从自然语言提示中产生了高度逼真的图像。但是，关注他们复制和放大现有社会偏见的潜力。为了调查这些问题，我们策划了一套涵盖主题类别的提示，例如职业，特质，行动，意识形态，情感，家庭角色，地点描述，灵性和生活事件。对于160个独特的主题中的每个主题，我们都制定了多个及时的变体，以反映出广泛的含义和观点。使用具有原始检查点的稳定扩散1.5（基于UNET）和Flux-1（基于DIT的）模型，我们在一致的设置下生成了16,000多个图像。此外，我们从Google图像搜索中收集了8,000张比较图像。对所有输出进行过滤以排除抽象，扭曲或荒谬的结果。我们的分析揭示了在产生的图像中的性别，种族，年龄，体型和其他以人为中心的因素的表示方面的显着差异。这些差异通常反映并加强了嵌入社会叙事中的有害刻板印象。我们讨论了这些发现的含义，并强调需要更具包容性的数据集和开发实践来促进生成视觉系统中的公平性。</li>
</ul>

<h3>Title: Evolvable Conditional Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Zhao Wei, Chin Chun Ooi, Abhishek Gupta, Jian Cheng Wong, Pao-Hsiung Chiu, Sheares Xue Wen Toh, Yew-Soon Ong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13834">https://arxiv.org/abs/2506.13834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13834">https://arxiv.org/pdf/2506.13834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13834]] Evolvable Conditional Diffusion(https://arxiv.org/abs/2506.13834)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper presents an evolvable conditional diffusion method such that black-box, non-differentiable multi-physics models, as are common in domains like computational fluid dynamics and electromagnetics, can be effectively used for guiding the generative process to facilitate autonomous scientific discovery. We formulate the guidance as an optimization problem where one optimizes for a desired fitness function through updates to the descriptive statistic for the denoising distribution, and derive an evolution-guided approach from first principles through the lens of probabilistic evolution. Interestingly, the final derived update algorithm is analogous to the update as per common gradient-based guided diffusion models, but without ever having to compute any derivatives. We validate our proposed evolvable diffusion algorithm in two AI for Science scenarios: the automated design of fluidic topology and meta-surface. Results demonstrate that this method effectively generates designs that better satisfy specific optimization objectives without reliance on differentiable proxies, providing an effective means of guidance-based diffusion that can capitalize on the wealth of black-box, non-differentiable multi-physics numerical models common across Science.</li>
<li><strong>摘要：</strong>本文提出了一种可增长的条件扩散方法，因此可以有效地使用黑盒，非差异性的多物理模型，例如计算流体动力学和电磁学等领域，用于指导生成过程以促进自主科学发现。我们将指导作为优化问题提出，其中一个人通过更新到脱索分布的描述性统计量来优化所需的健身函数，并通过概率进化的镜头从第一原理中得出了进化引导的方法。有趣的是，最终派生的更新算法类似于基于常见的基于梯度的引导扩散模型，但不必计算任何衍生物。我们在两个AI中验证了我们提出的可发展的扩散算法，以进行科学方案：流体拓扑和元表面的自动化设计。结果表明，该方法有效地生成了设计，可以更好地满足特定的优化目标，而无需依赖可微分的代理，从而提供了一种有效的基于指导的扩散手段，可以利用黑色框的财富，非差异性的多物理数值数字模型。</li>
</ul>

<h3>Title: Fake it till You Make it: Reward Modeling as Discriminative Prediction</h3>
<ul>
<li><strong>Authors: </strong>Runtao Liu, Jiahao Zhan, Yingqing He, Chen Wei, Alan Yuille, Qifeng Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13846">https://arxiv.org/abs/2506.13846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13846">https://arxiv.org/pdf/2506.13846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13846]] Fake it till You Make it: Reward Modeling as Discriminative Prediction(https://arxiv.org/abs/2506.13846)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>An effective reward model plays a pivotal role in reinforcement learning for post-training enhancement of visual generative models. However, current approaches of reward modeling suffer from implementation complexity due to their reliance on extensive human-annotated preference data or meticulously engineered quality dimensions that are often incomplete and engineering-intensive. Inspired by adversarial training in generative adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward modeling framework that eliminates manual preference annotation and explicit quality dimension engineering. Our method trains the reward model through discrimination between a small set of representative, unpaired target samples(denoted as Preference Proxy Data) and model-generated ordinary outputs, requiring only a few hundred target samples. Comprehensive experiments demonstrate our GAN-RM's effectiveness across multiple key applications including test-time scaling implemented as Best-of-N sample filtering, post-training approaches like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO).</li>
<li><strong>摘要：</strong>有效的奖励模型在增强视觉生成模型的训练后增强中起着关键作用。但是，当前的奖励建模方法由于依赖于广泛的人类宣布的偏好数据或精心设计的质量维度而遭受实施复杂性，而这些方法通常不完整且工程密集型。受生成对抗网络（GAN）的对抗培训的启发，本文提出了GAN-RM，这是一个有效的奖励建模框架，可消除手动偏好注释和明确的质量维度工程。我们的方法通过歧视奖励模型，通过歧视一组代表性的，未配对的目标样本（表示为偏好代理数据）和模型生成的普通产出，仅需要几百个目标样本。全面的实验证明了我们的GAN-RM在多个关键应用程序中的有效性，包括实施的测试时间缩放，以最佳N样本过滤，训练后的方法（例如监督微调（SFT）（SFT））和直接偏好优化（DPO）。</li>
</ul>

<h3>Title: GITO: Graph-Informed Transformer Operator for Learning Complex Partial Differential Equations</h3>
<ul>
<li><strong>Authors: </strong>Milad Ramezankhani, Janak M. Patel, Anirudh Deodhar, Dagnachew Birru</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13906">https://arxiv.org/abs/2506.13906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13906">https://arxiv.org/pdf/2506.13906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13906]] GITO: Graph-Informed Transformer Operator for Learning Complex Partial Differential Equations(https://arxiv.org/abs/2506.13906)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>We present a novel graph-informed transformer operator (GITO) architecture for learning complex partial differential equation systems defined on irregular geometries and non-uniform meshes. GITO consists of two main modules: a hybrid graph transformer (HGT) and a transformer neural operator (TNO). HGT leverages a graph neural network (GNN) to encode local spatial relationships and a transformer to capture long-range dependencies. A self-attention fusion layer integrates the outputs of the GNN and transformer to enable more expressive feature learning on graph-structured data. TNO module employs linear-complexity cross-attention and self-attention layers to map encoded input functions to predictions at arbitrary query locations, ensuring discretization invariance and enabling zero-shot super-resolution across any mesh. Empirical results on benchmark PDE tasks demonstrate that GITO outperforms existing transformer-based neural operators, paving the way for efficient, mesh-agnostic surrogate solvers in engineering applications.</li>
<li><strong>摘要：</strong>我们提出了一种新颖的图形变压器操作员（GITO）架构，用于学习在不规则几何和不均匀网格上定义的复杂部分微分方程系统。 GITO由两个主要模块组成：混合图变压器（HGT）和变压器神经操作员（TNO）。 HGT利用图形神经网络（GNN）编码局部空间关系和变压器来捕获远程依赖性。自我发项融合层集成了GNN和变压器的输出，以在图形结构化数据上实现更具表现力的特征学习。 TNO模块采用线性复杂性交叉注意力和自我注意事项层来映射编码的输入函数，以在任意查询位置的预测，确保离散化不变性并在任何网格中启用零击的超级分辨率。基准PDE任务的经验结果表明，GITO的表现优于现有的基于变压器的神经操作员，为工程应用中的高效，网状词汇替代求解器铺平了道路。</li>
</ul>

<h3>Title: AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science</h3>
<ul>
<li><strong>Authors: </strong>An Luo, Xun Xian, Jin Du, Fangqiao Tian, Ganghua Wang, Ming Zhong, Shengchun Zhao, Xuan Bi, Zirui Liu, Jiawei Zhou, Jayanth Srinivasa, Ashish Kundu, Charles Fleming, Mingyi Hong, Jie Ding</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13992">https://arxiv.org/abs/2506.13992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13992">https://arxiv.org/pdf/2506.13992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13992]] AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science(https://arxiv.org/abs/2506.13992)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have advanced the automation of data science workflows. Yet it remains unclear whether they can critically leverage external domain knowledge as human data scientists do in practice. To answer this question, we introduce AssistedDS (Assisted Data Science), a benchmark designed to systematically evaluate how LLMs handle domain knowledge in tabular prediction tasks. AssistedDS features both synthetic datasets with explicitly known generative mechanisms and real-world Kaggle competitions, each accompanied by curated bundles of helpful and adversarial documents. These documents provide domain-specific insights into data cleaning, feature engineering, and model selection. We assess state-of-the-art LLMs on their ability to discern and apply beneficial versus harmful domain knowledge, evaluating submission validity, information recall, and predictive performance. Our results demonstrate three key findings: (1) LLMs frequently exhibit an uncritical adoption of provided information, significantly impairing their predictive performance when adversarial content is introduced, (2) helpful guidance is often insufficient to counteract the negative influence of adversarial information, and (3) in Kaggle datasets, LLMs often make errors in handling time-series data, applying consistent feature engineering across different folds, and interpreting categorical variables correctly. These findings highlight a substantial gap in current models' ability to critically evaluate and leverage expert knowledge, underscoring an essential research direction for developing more robust, knowledge-aware automated data science systems.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）已提高了数据科学工作流的自动化。然而，尚不清楚他们是否可以像人类数据科学家在实践中那样批判性地利用外部领域知识。为了回答这个问题，我们介绍了辅助数据（辅助数据科学），这是一种基准测试，旨在系统地评估LLM在表格预测任务中如何处理域知识。 Assistedds具有综合数据集，具有明确的生成机制和现实世界中的Kaggle竞赛，每个竞争都伴随着精心策划的有用和对抗性文档的捆绑包。这些文档为数据清洁，功能工程和模型选择提供了特定领域的见解。我们评估了最先进的LLM，以识别和应用有益领域知识，评估提交有效性，信息回忆和预测性能的能力。我们的结果表明了三个关键发现：（1）LLM经常表现出对所提供的信息的不临能采用，在引入对抗内容时会严重损害其预测性能，（2）有用的指导通常不足以抵消对抗信息的负面影响，并且在Kaggle DataSet中使用折叠式数据，在Kaggle DataS中进行跨性别的功能，将其用于计时，而在Kaggle DataS中进行跨性别的特征，将其用于处理时间，而换句话说，则可以将其换成计时性数据。变量正确。这些发现突出了当前模型批判性评估和利用专家知识的能力，强调了开发更强大的知识自动化数据科学系统的重要研究方向。</li>
</ul>

<h3>Title: Mapping Farmed Landscapes from Remote Sensing</h3>
<ul>
<li><strong>Authors: </strong>Michelangelo Conserva, Alex Wilson, Charlotte Stanton, Vishal Batchu, Varun Gulshan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.13993">https://arxiv.org/abs/2506.13993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.13993">https://arxiv.org/pdf/2506.13993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.13993]] Mapping Farmed Landscapes from Remote Sensing(https://arxiv.org/abs/2506.13993)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Effective management of agricultural landscapes is critical for meeting global biodiversity targets, but efforts are hampered by the absence of detailed, large-scale ecological maps. To address this, we introduce Farmscapes, the first large-scale (covering most of England), high-resolution (25cm) map of rural landscape features, including ecologically vital elements like hedgerows, woodlands, and stone walls. This map was generated using a deep learning segmentation model trained on a novel, dataset of 942 manually annotated tiles derived from aerial imagery. Our model accurately identifies key habitats, achieving high f1-scores for woodland (96\%) and farmed land (95\%), and demonstrates strong capability in segmenting linear features, with an F1-score of 72\% for hedgerows. By releasing the England-wide map on Google Earth Engine, we provide a powerful, open-access tool for ecologists and policymakers. This work enables data-driven planning for habitat restoration, supports the monitoring of initiatives like the EU Biodiversity Strategy, and lays the foundation for advanced analysis of landscape connectivity.</li>
<li><strong>摘要：</strong>有效的农业景观管理对于满足全球生物多样性目标至关重要，但是由于缺乏详细的大规模生态图而妨碍了努力。为了解决这个问题，我们介绍了农场景观，这是第一个大规模（覆盖英格兰大部分地区），高分辨率（25厘米）的农村景观特征地图，包括生态上至关重要的元素，例如树篱，林地和石墙。该地图是使用深度学习分割模型生成的，该模型是在新颖的942个手动注释的瓷砖的数据集中训练的，该模型是从空中影像中衍生而来的。我们的模型准确地识别了关键栖息地，从而达到了林地（96 \％）和养殖土地（95 \％）的高F1分数，并且在分割线性特征方面表现出强大的能力，而Hedgerows的F1得分为72 \％。通过在Google Earth Engine上发布英格兰范围的地图，我们为生态学家和政策制定者提供了强大的开放式工具。这项工作使数据驱动的栖息地恢复计划可以进行数据驱动的计划，支持对欧盟生物多样性策略等计划的监控，并为景观连接的高级分析奠定了基础。</li>
</ul>

<h3>Title: Disentangling 3D from Large Vision-Language Models for Controlled Portrait Generation</h3>
<ul>
<li><strong>Authors: </strong>Nick Yiwen Huang, Akin Caliskan, Berkay Kicanaoglu, James Tompkin, Hyeongwoo Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14015">https://arxiv.org/abs/2506.14015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14015">https://arxiv.org/pdf/2506.14015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14015]] Disentangling 3D from Large Vision-Language Models for Controlled Portrait Generation(https://arxiv.org/abs/2506.14015)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We consider the problem of disentangling 3D from large vision-language models, which we show on generative 3D portraits. This allows free-form text control of appearance attributes like age, hair style, and glasses, and 3D geometry control of face expression and camera pose. In this setting, we assume we use a pre-trained large vision-language model (LVLM; CLIP) to generate from a smaller 2D dataset with no additional paired labels and with a pre-defined 3D morphable model (FLAME). First, we disentangle using canonicalization to a 2D reference frame from a deformable neural 3D triplane representation. But another form of entanglement arises from the significant noise in the LVLM's embedding space that describes irrelevant features. This damages output quality and diversity, but we overcome this with a Jacobian regularization that can be computed efficiently with a stochastic approximator. Compared to existing methods, our approach produces portraits with added text and 3D control, where portraits remain consistent when either control is changed. Broadly, this approach lets creators control 3D generators on their own 2D face data without needing resources to label large data or train large models.</li>
<li><strong>摘要：</strong>我们考虑了将3D从大型视觉模型中解开的问题，我们在生成3D肖像上展示了这一点。这允许对外观属性（如年龄，发型和眼镜）以及面部表情和摄像头姿势的3D几何控制等外观属性进行自由形式的文本控制。在这种情况下，我们假设我们使用预先训练的大型视觉模型（LVLM; Clip）从较小的2D数据集生成，没有其他配对标签，并且具有预定的3D形态模型（火焰）。首先，我们将规范化从可变形的神经3D三烷表示为2D参考框架上解开。但是另一种形式的纠缠是由LVLM嵌入空间中的明显噪声引起的，该噪声描述了无关紧要的特征。这会损害输出质量和多样性，但是我们可以通过雅各布式的正则化来克服这一点，该正则化可以通过随机近似器进行有效计算。与现有方法相比，我们的方法会产生带有添加文本和3D控制的肖像，在更改任一控制时，肖像保持一致。从广义上讲，这种方法使创建者可以通过自己的2D面部数据控制3D发电机，而无需资源来标记大型数据或训练大型模型。</li>
</ul>

<h3>Title: Bures-Wasserstein Flow Matching for Graph Generation</h3>
<ul>
<li><strong>Authors: </strong>Keyue Jiang, Jiahao Cui, Xiaowen Dong, Laura Toni</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14020">https://arxiv.org/abs/2506.14020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14020">https://arxiv.org/pdf/2506.14020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14020]] Bures-Wasserstein Flow Matching for Graph Generation(https://arxiv.org/abs/2506.14020)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Graph generation has emerged as a critical task in fields ranging from molecule design to drug discovery. Contemporary approaches, notably diffusion and flow-based models, have achieved solid graph generative performance through constructing a probability path that interpolates between a reference distribution and the data distribution. However, these methods typically model the evolution of individual nodes and edges independently and use linear interpolations to build the path assuming that the data lie in Euclidean space. We show that this is suboptimal given the intrinsic non-Euclidean structure and interconnected patterns of graphs, and it poses risks to the sampling convergence. To build a better probability path, we model the joint evolution of the nodes and edges by representing graphs as connected systems parameterized by Markov random fields (MRF). We then leverage the optimal transport displacement between MRF objects to design the probability path for graph generation. Based on this, we introduce BWFlow, a flow-matching framework for graph generation that respects the underlying geometry of graphs and provides smooth velocities in the probability path. The novel framework can be adapted to both continuous and discrete flow-matching algorithms. Experimental evaluations in plain graph generation and 2D/3D molecule generation validate the effectiveness of BWFlow in graph generation with competitive performance, stable training, and guaranteed sampling convergence.</li>
<li><strong>摘要：</strong>在从分子设计到药物发现等领域的一项关键任务中已经出现了图。当代方法，尤其是扩散和基于流的模型，通过构建概率路径在参考分布和数据分布之间插值来实现固体图生成性能。但是，这些方法通常会独立建模单个节点和边缘的演变，并使用线性插值来构建路径，假设数据位于欧几里得空间中。我们表明，考虑到图形的固有非欧盟结构和互连模式，这是次优的，并且对采样收敛构成了风险。为了构建更好的概率路径，我们通过表示由Markov随机字段（MRF）参数为参数的连接系统来对节点和边缘的关节演变进行建模。然后，我们利用MRF对象之间的最佳传输位移来设计图形生成的概率路径。基于此，我们介绍了BWFlow，这是图形生成的流程匹配框架，尊重图形的基础几何形状，并在概率路径中提供平滑的速度。新型框架可以适应连续和离散的流量匹配算法。纯图生成和2D/3D分子生成的实验评估验证了BWFLOF在图生成中的有效性，具有竞争性能，稳定的训练和保证的采样收敛。</li>
</ul>

<h3>Title: SimpleDoc: Multi-Modal Document Understanding with Dual-Cue Page Retrieval and Iterative Refinement</h3>
<ul>
<li><strong>Authors: </strong>Chelsi Jain, Yiran Wu, Yifan Zeng, Jiale Liu, S hengyu Dai, Zhenwen Shao, Qingyun Wu, Huazheng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14035">https://arxiv.org/abs/2506.14035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14035">https://arxiv.org/pdf/2506.14035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14035]] SimpleDoc: Multi-Modal Document Understanding with Dual-Cue Page Retrieval and Iterative Refinement(https://arxiv.org/abs/2506.14035)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Document Visual Question Answering (DocVQA) is a practical yet challenging task, which is to ask questions based on documents while referring to multiple pages and different modalities of information, e.g, images and tables. To handle multi-modality, recent methods follow a similar Retrieval Augmented Generation (RAG) pipeline, but utilize Visual Language Models (VLMs) based embedding model to embed and retrieve relevant pages as images, and generate answers with VLMs that can accept an image as input. In this paper, we introduce SimpleDoc, a lightweight yet powerful retrieval - augmented framework for DocVQA. It boosts evidence page gathering by first retrieving candidates through embedding similarity and then filtering and re-ranking these candidates based on page summaries. A single VLM-based reasoner agent repeatedly invokes this dual-cue retriever, iteratively pulling fresh pages into a working memory until the question is confidently answered. SimpleDoc outperforms previous baselines by 3.2% on average on 4 DocVQA datasets with much fewer pages retrieved. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>文档视觉问题回答（DOCVQA）是一项实用但又具有挑战性的任务，它是根据文档提出问题，同时参考多个页面和不同的信息方式，例如图像和表格。为了处理多模式，最近的方法遵循类似的检索增强生成（RAG）管道，但是利用基于视觉语言模型（VLMS）嵌入模型将相关页面嵌入和检索相关页面作为图像，并使用可以接受图像作为输入的VLMS生成答案。在本文中，我们介绍了SimpleDoc，这是一个轻巧而强大的检索 -  DOCVQA的增强框架。它通过首先通过嵌入相似性来检索候选人，然后根据页面摘要进行过滤和重新排列这些候选者来提高证据页的收集。单个基于VLM的推理器代理反复调用这位双提示回收器，迭代地将新页面拉入工作记忆中，直到对问题进行自信地回答。在4个DOCVQA数据集上，SimpleDoc的表现平均比以前的基线高3.2％，而书面的页面却少得多。我们的代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: A Regret Perspective on Online Selective Generation</h3>
<ul>
<li><strong>Authors: </strong>Minjae Lee, Yoonjae Jung, Sangdon Park</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14067">https://arxiv.org/abs/2506.14067</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14067">https://arxiv.org/pdf/2506.14067</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14067]] A Regret Perspective on Online Selective Generation(https://arxiv.org/abs/2506.14067)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Large language generative models increasingly interact with humans, while their falsified responses raise concerns. To address this hallucination effect, selectively abstaining from answering, called selective generation, provides an effective way for generators to control the hallucination when it is unsure of their answers. However, as selective generators are interacting under non-stochastic environments and having partial feedback from users on selective generation (e.g., thumbs up or down on the selected answer), learning methods for selective generation under such practical setups are crucial but currently missing. To address these limitations, we propose an online learning algorithm for selective generation under partial feedback. In particular, as learning under partial feedback is well-studied by multi-armed bandit problems, we reduce selective generation to bandits and provide a novel conversion lemma from bandits back to selective generation to leverage any known bandit algorithms and theoretical properties. This mainly connects regret guarantees of bandits to false discovery rate (FDR) guarantees of selective generation for controlling hallucination. However, naively exploiting known bandit algorithms and their regret bounds suffers from slow convergence speed in practice due the nature of partial feedback. To overcome this, we exploit a unique structure of arms in selective generation for feedback unlocking, i.e., unlocking unknown feedback from observed feedback. We theoretically and empirically evaluate the efficacy of the proposed online selective generation algorithm under partial feedback over diverse data environment setups, resulting in controlling a desired FDR, while maintaining reasonable selection efficiency, i.e., the ratio of non-abstaining answers, compared to baselines.</li>
<li><strong>摘要：</strong>大型语言生成模型越来越多地与人类互动，而他们伪造的反应引起了人们的关注。为了解决这种幻觉效应，选择性地弃权回答，称为选择性生成，为发电机不确定答案时可以控制幻觉。但是，随着选择性发电机在非传统环境下进行交互，并在选择性生成上获得了用户的部分反馈（例如，在所选答案上向上或向下竖起大拇指），在此类实用设置下的选择性生成的学习方法至关重要，但目前缺失。为了解决这些限制，我们为部分反馈下的选择性生成提出了一种在线学习算法。特别是，由于多臂匪徒问题对部分反馈的学习进行了充分研究，因此我们将选择性生成减少到土匪，并提供从土匪回到选择性生成的新型转换引理，以利用任何已知的Bandit算法和理论特性。这主要将遗憾保证与虚假发现率（FDR）保证选择性生成可控制幻觉。然而，由于部分反馈的性质，在实践中天真地利用已知的匪徒及其遗憾的界限在实践中的收敛速度缓慢。为了克服这一点，我们利用选择性生成中的独特的武器结构来解锁反馈解锁，即从观察到的反馈中解锁未知的反馈。我们从理论上和经验上评估了在各种数据环境设置的部分反馈下提出的在线选择性生成算法的疗效，从而控制了所需的FDR，同时保持合理的选择效率，即与基准相比，非放松答案的比率。</li>
</ul>

<h3>Title: Comprehensive Verilog Design Problems: A Next-Generation Benchmark Dataset for Evaluating Large Language Models and Agents on RTL Design and Verification</h3>
<ul>
<li><strong>Authors: </strong>Nathaniel Pinckney, Chenhui Deng, Chia-Tung Ho, Yun-Da Tsai, Mingjie Liu, Wenfei Zhou, Brucek Khailany, Haoxing Ren</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14074">https://arxiv.org/abs/2506.14074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14074">https://arxiv.org/pdf/2506.14074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14074]] Comprehensive Verilog Design Problems: A Next-Generation Benchmark Dataset for Evaluating Large Language Models and Agents on RTL Design and Verification(https://arxiv.org/abs/2506.14074)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present the Comprehensive Verilog Design Problems (CVDP) benchmark, a new dataset and infrastructure to advance LLM and agent research in hardware design and verification. CVDP includes 783 problems across 13 task categories, covering RTL generation, verification, debugging, specification alignment, and technical Q&A authored by experienced hardware engineers. Problems are offered in both non-agentic and agentic formats. The benchmark introduces more realistic and challenging contexts than prior work, with state-of-the-art models achieving no more than 34% pass@1 on code generation. Agentic tasks$\unicode{x2013}$especially those involving RTL reuse and verification$\unicode{x2013}$are particularly difficult. Evaluation uses open-source tools and model scoring infrastructure, with comprehension tasks assessed via BLEU and LLM-based judging. CVDP reveals substantial gaps in current model capabilities, underscoring the need for continued research toward robust, real-world hardware design automation.</li>
<li><strong>摘要：</strong>我们介绍了全面的Verilog设计问题（CVDP）基准，这是一种新的数据集和基础架构，可推进硬件设计和验证方面的LLM和代理研究。 CVDP包括13个任务类别的783个问题，涵盖RTL生成，验证，调试，规格对齐方式以及由经验丰富的硬件工程师撰写的技术问答。以非主张和代理格式提供问题。与先前的工作相比，基准标准引入更现实和具有挑战性的环境，而最先进的模型在代码生成上达到了不超过34％的通行证。代理任务$ \ unicode {x2013} $，尤其是涉及RTL重复使用和验证$ \ unicode {x2013} $的任务。评估使用开源工具和模型评分基础架构，并通过基于BLEU和LLM的评审来评估理解任务。 CVDP揭示了当前模型功能的巨大差距，强调了继续研究的必要性，以实现健壮的真实世界硬件设计自动化。</li>
</ul>

<h3>Title: Image Segmentation with Large Language Models: A Survey with Perspectives for Intelligent Transportation Systems</h3>
<ul>
<li><strong>Authors: </strong>Sanjeda Akter, Ibne Farabi Shihab, Anuj Sharma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14096">https://arxiv.org/abs/2506.14096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14096">https://arxiv.org/pdf/2506.14096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14096]] Image Segmentation with Large Language Models: A Survey with Perspectives for Intelligent Transportation Systems(https://arxiv.org/abs/2506.14096)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The integration of Large Language Models (LLMs) with computer vision is profoundly transforming perception tasks like image segmentation. For intelligent transportation systems (ITS), where accurate scene understanding is critical for safety and efficiency, this new paradigm offers unprecedented capabilities. This survey systematically reviews the emerging field of LLM-augmented image segmentation, focusing on its applications, challenges, and future directions within ITS. We provide a taxonomy of current approaches based on their prompting mechanisms and core architectures, and we highlight how these innovations can enhance road scene understanding for autonomous driving, traffic monitoring, and infrastructure maintenance. Finally, we identify key challenges, including real-time performance and safety-critical reliability, and outline a perspective centered on explainable, human-centric AI as a prerequisite for the successful deployment of this technology in next-generation transportation systems.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）与计算机视觉的集成正在深刻地改变了诸如图像分割之类的感知任务。对于智能运输系统（ITS），准确的场景理解对于安全和效率至关重要，这种新的范式提供了前所未有的功能。这项调查系统地回顾了LLM的图像分段的新兴领域，重点介绍其应用程序，挑战和未来方向。我们根据其促进机制和核心体系结构提供了当前方法的分类学，我们强调了这些创新如何增强自动驾驶，交通监控和基础设施维护的道路现场理解。最后，我们确定了关键挑战，包括实时绩效和关键安全性可靠性，并概述了以可解释的，以人为中心的AI的观点，这是成功部署该技术在下一代运输系统中的前提。</li>
</ul>

<h3>Title: FADPNet: Frequency-Aware Dual-Path Network for Face Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Siyu Xu, Wenjie Li, Guangwei Gao, Jian Yang, Guo-Jun Qi, Chia-Wen Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14121">https://arxiv.org/abs/2506.14121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14121">https://arxiv.org/pdf/2506.14121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14121]] FADPNet: Frequency-Aware Dual-Path Network for Face Super-Resolution(https://arxiv.org/abs/2506.14121)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Face super-resolution (FSR) under limited computational costs remains an open problem. Existing approaches typically treat all facial pixels equally, resulting in suboptimal allocation of computational resources and degraded FSR performance. CNN is relatively sensitive to high-frequency facial features, such as component contours and facial outlines. Meanwhile, Mamba excels at capturing low-frequency features like facial color and fine-grained texture, and does so with lower complexity than Transformers. Motivated by these observations, we propose FADPNet, a Frequency-Aware Dual-Path Network that decomposes facial features into low- and high-frequency components and processes them via dedicated branches. For low-frequency regions, we introduce a Mamba-based Low-Frequency Enhancement Block (LFEB), which combines state-space attention with squeeze-and-excitation operations to extract low-frequency global interactions and emphasize informative channels. For high-frequency regions, we design a CNN-based Deep Position-Aware Attention (DPA) module to enhance spatially-dependent structural details, complemented by a lightweight High-Frequency Refinement (HFR) module that further refines frequency-specific representations. Through the above designs, our method achieves an excellent balance between FSR quality and model efficiency, outperforming existing approaches.</li>
<li><strong>摘要：</strong>在有限的计算成本下，面对超分辨率（FSR）仍然是一个空旷的问题。现有方法通常平等地对待所有面部像素，从而导致计算资源的次优分配和FSR性能降解。 CNN对高频面部特征相对敏感，例如组件轮廓和面部轮廓。同时，曼巴（Mamba）擅长捕获诸如面部颜色和细粒度质地之类的低频特征，并且比变形金刚的复杂性较低。在这些观察结果的推动下，我们提出了FADPNET，这是一种频率吸引的双路径网络，将面部特征分解为低频和高频组件，并通过专用的分支对其进行处理。对于低频区域，我们引入了基于MAMBA的低频增强块（LFEB），该块将状态空间的关注与挤压和激发操作相结合，以提取低频全球相互作用并强调信息渠道。对于高频区域，我们设计了一个基于CNN的深层位置感知注意力（DPA）模块，以增强空间依赖的结构细节，并以轻质高频改进（HFR）模块的补充，进一步完善了频率特异性表示。通过上述设计，我们的方法在FSR质量和模型效率之间取得了良好的平衡，表现优于现有方法。</li>
</ul>

<h3>Title: Structured and Informed Probabilistic Modeling with the Thermodynamic Kolmogorov-Arnold Model</h3>
<ul>
<li><strong>Authors: </strong>Prithvi Raj</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14167">https://arxiv.org/abs/2506.14167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14167">https://arxiv.org/pdf/2506.14167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14167]] Structured and Informed Probabilistic Modeling with the Thermodynamic Kolmogorov-Arnold Model(https://arxiv.org/abs/2506.14167)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We adapt the Kolmogorov-Arnold Representation Theorem to generative modeling by reinterpreting its inner functions as a Markov Kernel between probability spaces via inverse transform sampling. We present a generative model that is interpretable, easy to design, and efficient. Our approach couples a Kolmogorov-Arnold Network generator with independent energy-based priors, trained via Maximum Likelihood. Inverse sampling enables fast inference, while prior knowledge can be incorporated before training to better align priors with posteriors, thereby improving learning efficiency and sample quality. The learned prior is also recoverable and visualizable post-training, offering an empirical Bayes perspective. To address inflexibility and mitigate prior-posterior mismatch, we introduce scalable extensions based on mixture distributions and Langevin Monte Carlo methods, admitting a trade-off between flexibility and training efficiency. Our contributions connect classical representation theorems with modern probabilistic modeling, while balancing training stability, inference speed, and the quality and diversity of generations.</li>
<li><strong>摘要：</strong>我们通过通过逆变换采样将其内部功能重新解释为Markov内核，将其内部函数重新解释为Markov内核，将Kolmogorov-Arnold表示定理调整为生成建模。我们提出了一种可解释，易于设计和高效的生成模型。我们的方法将Kolmogorov-Arnold网络生成器与独立的基于能量的先验融合在一起，并通过最大可能性进行了训练。逆采样可以快速推断，尽管可以在训练之前纳入先验知识以更好地与后者保持一致，从而提高学习效率和样本质量。博学的先验也是可回收且可可视化的训练后，提供了经验的贝叶斯观点。为了解决不灵活性并减轻先前的不匹配，我们基于混合物分布和Langevin Monte Carlo方法引入可扩展的扩展，并承认灵活性和训练效率之间的权衡。我们的贡献将古典代表定理与现代概率建模联系起来，同时平衡训练稳定性，推理速度以及世代的质量和多样性。</li>
</ul>

<h3>Title: VideoMAR: Autoregressive Video Generatio with Continuous Tokens</h3>
<ul>
<li><strong>Authors: </strong>Hu Yu, Biao Gong, Hangjie Yuan, DanDan Zheng, Weilong Chai, Jingdong Chen, Kecheng Zheng, Feng Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14168">https://arxiv.org/abs/2506.14168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14168">https://arxiv.org/pdf/2506.14168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14168]] VideoMAR: Autoregressive Video Generatio with Continuous Tokens(https://arxiv.org/abs/2506.14168)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Masked-based autoregressive models have demonstrated promising image generation capability in continuous space. However, their potential for video generation remains under-explored. In this paper, we propose \textbf{VideoMAR}, a concise and efficient decoder-only autoregressive image-to-video model with continuous tokens, composing temporal frame-by-frame and spatial masked generation. We first identify temporal causality and spatial bi-directionality as the first principle of video AR models, and propose the next-frame diffusion loss for the integration of mask and video generation. Besides, the huge cost and difficulty of long sequence autoregressive modeling is a basic but crucial issue. To this end, we propose the temporal short-to-long curriculum learning and spatial progressive resolution training, and employ progressive temperature strategy at inference time to mitigate the accumulation error. Furthermore, VideoMAR replicates several unique capacities of language models to video generation. It inherently bears high efficiency due to simultaneous temporal-wise KV cache and spatial-wise parallel generation, and presents the capacity of spatial and temporal extrapolation via 3D rotary embeddings. On the VBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos I2V) while requiring significantly fewer parameters ($9.3\%$), training data ($0.5\%$), and GPU resources ($0.2\%$).</li>
<li><strong>摘要：</strong>基于掩盖的自回旋模型已证明在连续空间中具有有希望的图像产生能力。但是，它们的视频生成潜力仍然不足。在本文中，我们提出了\ textbf {videoMar}，这是一种简洁有效的仅解码器自动回归图像到视频模型，具有连续令牌，构成了按框架和空间掩盖的生成的时间框架。我们首先将时间因果关系和空间双向性确定为视频AR模型的第一个原理，并提出了掩码和视频生成集成的下一框架扩散损失。此外，长序列自回归建模的巨大成本和困难是一个基本但至关重要的问题。为此，我们提出了短期到长时间的课程学习和空间渐进分辨率培训，并在推理时间采用渐进式温度策略来减轻累积误差。此外，VideoMar将语言模型的几种独特能力复制到视频生成。它固有地具有高效率，这是由于同时进行时间的KV缓存和空间平行的生成，并通过3D旋转嵌入呈现了空间和时间外推的能力。在VBENCH-I2V基准测试中，VideoMar超过了先前的最先进（COSMOS I2V），同时需要较少的参数（$ 9.3 \％$），培训数据（$ 0.5 \％\％$）和GPU资源（$ 0.2 \％$）。</li>
</ul>

<h3>Title: Meta-SurDiff: Classification Diffusion Model Optimized by Meta Learning is Reliable for Online Surgical Phase Recognition</h3>
<ul>
<li><strong>Authors: </strong>Yufei Li, Jirui Wu, Long Tian, Liming Wang, Xiaonan Liu, Zijun Liu, Xiyang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14181">https://arxiv.org/abs/2506.14181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14181">https://arxiv.org/pdf/2506.14181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14181]] Meta-SurDiff: Classification Diffusion Model Optimized by Meta Learning is Reliable for Online Surgical Phase Recognition(https://arxiv.org/abs/2506.14181)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Online surgical phase recognition has drawn great attention most recently due to its potential downstream applications closely related to human life and health. Despite deep models have made significant advances in capturing the discriminative long-term dependency of surgical videos to achieve improved recognition, they rarely account for exploring and modeling the uncertainty in surgical videos, which should be crucial for reliable online surgical phase recognition. We categorize the sources of uncertainty into two types, frame ambiguity in videos and unbalanced distribution among surgical phases, which are inevitable in surgical videos. To address this pivot issue, we introduce a meta-learning-optimized classification diffusion model (Meta-SurDiff), to take full advantage of the deep generative model and meta-learning in achieving precise frame-level distribution estimation for reliable online surgical phase recognition. For coarse recognition caused by ambiguous video frames, we employ a classification diffusion model to assess the confidence of recognition results at a finer-grained frame-level instance. For coarse recognition caused by unbalanced phase distribution, we use a meta-learning based objective to learn the diffusion model, thus enhancing the robustness of classification boundaries for different surgical this http URL establish effectiveness of Meta-SurDiff in online surgical phase recognition through extensive experiments on five widely used datasets using more than four practical metrics. The datasets include Cholec80, AutoLaparo, M2Cai16, OphNet, and NurViD, where OphNet comes from ophthalmic surgeries, NurViD is the daily care dataset, while the others come from laparoscopic surgeries. We will release the code upon acceptance.</li>
<li><strong>摘要：</strong>由于与人类的生活和健康密切相关的下游应用，在线手术阶段识别引起了人们的极大关注。尽管深层模型在捕获手术视频的歧视性长期依赖方面取得了重大进展，但他们很少考虑探索和建模手术视频中的不确定性，这对于可靠的在线手术阶段识别至关重要。我们将不确定性的来源分为两种类型，视频中的歧义以及手术阶段之间的分布不平衡，这在手术视频中是不可避免的。为了解决这个关键问题，我们引入了元学习优化的分类扩散模型（Meta-Surdiff），以充分利用深层生成模型和元学习，以实现可靠的在线手术相位识别的精确帧级分布估计。对于模棱两可的视频帧引起的粗略识别，我们采用分类扩散模型来评估在较细粒的帧级实例上识别结果的信心。为了通过不平衡的相位分布引起的粗略识别，我们使用基于元学习的目标来学习扩散模型，从而增强了不同外科手术的分类边界的鲁棒性。该HTTP URL在在线外科手术相位识别中通过在五个广泛使用的数据集上进行了超过四个实践计量的五个广泛使用的实验，从而在在线外科手术相位识别中建立了元过程的有效性。这些数据集包括Cholec80，Autolaparo，M2CAI16，Ophnet和Nurvid，Ophnet来自Ophthalmic手术，Nurvid是Daily Care数据集，而其他人则来自腹腔镜手术。我们将在接受后发布代码。</li>
</ul>

<h3>Title: DiffusionBlocks: Blockwise Training for Generative Models via Score-Based Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Makoto Shing, Takuya Akiba</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14202">https://arxiv.org/abs/2506.14202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14202">https://arxiv.org/pdf/2506.14202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14202]] DiffusionBlocks: Blockwise Training for Generative Models via Score-Based Diffusion(https://arxiv.org/abs/2506.14202)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Training large neural networks with end-to-end backpropagation creates significant memory bottlenecks, limiting accessibility to state-of-the-art AI research. We propose $\textit{DiffusionBlocks}$, a novel training framework that interprets neural network blocks as performing denoising operations in a continuous-time diffusion process. By partitioning the network into independently trainable blocks and optimizing noise level assignments based on equal cumulative probability mass, our approach achieves significant memory efficiency while maintaining competitive performance compared to traditional backpropagation in generative tasks. Experiments on image generation and language modeling tasks demonstrate memory reduction proportional to the number of blocks while achieving superior performance. DiffusionBlocks provides a promising pathway for democratizing access to large-scale neural network training with limited computational resources.</li>
<li><strong>摘要：</strong>通过端到端的反向传播培训大型神经网络会产生重要的内存瓶颈，从而限制了最新AI研究的可访问性。我们建议$ \ textit {diffusionBlocks} $，这是一个新颖的培训框架，将神经网络块解释为在连续的时间扩散过程中执行DeNo的操作。通过将网络划分为可独立训练的块并基于相同的累积概率质量来优化噪声水平分配，我们的方法可以达到显着的记忆效率，同时与传统的生成任务中的反向传播相比，保持了竞争性能。关于图像生成和语言建模任务的实验证明了与块数量成正比的记忆减少，同时实现了卓越的性能。 DiffusionBlocks为通过有限的计算资源民主化访问大规模神经网络培训提供了一种有希望的途径。</li>
</ul>

<h3>Title: RL-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?</h3>
<ul>
<li><strong>Authors: </strong>Rohan Gupta, Erik Jenner</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14261">https://arxiv.org/abs/2506.14261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14261">https://arxiv.org/pdf/2506.14261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14261]] RL-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?(https://arxiv.org/abs/2506.14261)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Latent-space monitors aim to detect undesirable behaviours in large language models by leveraging internal model representations rather than relying solely on black-box outputs. These methods have shown promise in identifying behaviours such as deception and unsafe completions, but a critical open question remains: can LLMs learn to evade such monitors? To study this, we introduce RL-Obfuscation, in which LLMs are finetuned via reinforcement learning to bypass latent-space monitors while maintaining coherent generations. We apply RL-Obfuscation to LLMs ranging from 7B to 14B parameters and evaluate evasion success against a suite of monitors. We find that token-level latent-space monitors are highly vulnerable to this attack. More holistic monitors, such as max-pooling or attention-based probes, remain robust. Moreover, we show that adversarial policies trained to evade a single static monitor generalise to unseen monitors of the same type. Finally, we study how the policy learned by RL bypasses these monitors and find that the model can also learn to repurpose tokens to mean something different internally.</li>
<li><strong>摘要：</strong>潜在空间监测器旨在通过利用内部模型表示，而不是仅依靠黑框输出来检测大语言模型中的不良行为。这些方法在识别诸如欺骗和不安全完成之类的行为方面显示出了希望，但是一个关键的开放问题仍然存在：LLMS可以学会逃避此类监视器吗？为了研究这一点，我们介绍了RL扑灭量，其中LLM通过加强学习对绕过潜在空间监视器的同时维持相干世代进行了训练。我们将RL抗掺杂应用于范围从7b到14b参数的LLM，并评估了一套监视器的逃避成功。我们发现令牌级的潜在空间监视器极易受到这一攻击的影响。更多的整体监控器，例如最大释放或基于注意力的探针，仍然坚固。此外，我们表明，受过训练的对抗性政策可以逃避单个静态监视器，从而推广了相同类型的显示器。最后，我们研究了RL学到的政策如何绕过这些监视器，并发现该模型还可以学会重新利用令牌，以内部的意思。</li>
</ul>

<h3>Title: Exploring Non-contrastive Self-supervised Representation Learning for Image-based Profiling</h3>
<ul>
<li><strong>Authors: </strong>Siran Dai, Qianqian Xu, Peisong Wen, Yang Liu, Qingming Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14265">https://arxiv.org/abs/2506.14265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14265">https://arxiv.org/pdf/2506.14265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14265]] Exploring Non-contrastive Self-supervised Representation Learning for Image-based Profiling(https://arxiv.org/abs/2506.14265)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Image-based cell profiling aims to create informative representations of cell images. This technique is critical in drug discovery and has greatly advanced with recent improvements in computer vision. Inspired by recent developments in non-contrastive Self-Supervised Learning (SSL), this paper provides an initial exploration into training a generalizable feature extractor for cell images using such methods. However, there are two major challenges: 1) There is a large difference between the distributions of cell images and natural images, causing the view-generation process in existing SSL methods to fail; and 2) Unlike typical scenarios where each representation is based on a single image, cell profiling often involves multiple input images, making it difficult to effectively combine all available information. To overcome these challenges, we propose SSLProfiler, a non-contrastive SSL framework specifically designed for cell profiling. We introduce specialized data augmentation and representation post-processing methods tailored to cell images, which effectively address the issues mentioned above and result in a robust feature extractor. With these improvements, SSLProfiler won the Cell Line Transferability challenge at CVPR 2025.</li>
<li><strong>摘要：</strong>基于图像的细胞分析旨在创建细胞图像的信息表示。该技术在药物发现中至关重要，并且随着计算机视觉的最新改进，这项技术已大大提高。受到非对抗性自我监督学习（SSL）的最新发展的启发，本文为培训使用这种方法提供了培训可概括的特征提取器的初步探索。但是，有两个主要的挑战：1）单元图像和自然图像的分布之间存在很大差异，从而导致现有SSL方法中的视图生成过程失败； 2）与典型的情况不同，每个表示都基于单个图像，单元格分析通常涉及多个输入图像，因此难以有效地结合所有可用信息。为了克服这些挑战，我们提出了SSL-Profiler，这是一种专门设计用于细胞分析的非对比度SSL框架。我们介绍了针对单元图像的专业数据增强和表示后处理方法，该方法有效地解决了上述问题，并导致了可靠的功能提取器。通过这些改进，SSL -Profiler在CVPR 2025赢得了细胞线的可传递性挑战。</li>
</ul>

<h3>Title: DGG-XNet: A Hybrid Deep Learning Framework for Multi-Class Brain Disease Classification with Explainable AI</h3>
<ul>
<li><strong>Authors: </strong>Sumshun Nahar Eity, Mahin Montasir Afif, Tanisha Fairooz, Md. Mortuza Ahmmed, Md Saef Ullah Miah</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14367">https://arxiv.org/abs/2506.14367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14367">https://arxiv.org/pdf/2506.14367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14367]] DGG-XNet: A Hybrid Deep Learning Framework for Multi-Class Brain Disease Classification with Explainable AI(https://arxiv.org/abs/2506.14367)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Accurate diagnosis of brain disorders such as Alzheimer's disease and brain tumors remains a critical challenge in medical imaging. Conventional methods based on manual MRI analysis are often inefficient and error-prone. To address this, we propose DGG-XNet, a hybrid deep learning model integrating VGG16 and DenseNet121 to enhance feature extraction and classification. DenseNet121 promotes feature reuse and efficient gradient flow through dense connectivity, while VGG16 contributes strong hierarchical spatial representations. Their fusion enables robust multiclass classification of neurological conditions. Grad-CAM is applied to visualize salient regions, enhancing model transparency. Trained on a combined dataset from BraTS 2021 and Kaggle, DGG-XNet achieved a test accuracy of 91.33\%, with precision, recall, and F1-score all exceeding 91\%. These results highlight DGG-XNet's potential as an effective and interpretable tool for computer-aided diagnosis (CAD) of neurodegenerative and oncological brain disorders.</li>
<li><strong>摘要：</strong>准确地诊断出诸如阿尔茨海默氏病和脑肿瘤之类的脑疾病仍然是医学成像中的关键挑战。基于手动MRI分析的常规方法通常效率低下且容易出错。为了解决这个问题，我们提出了DGG-XNET，这是一个集成VGG16和Densenet121的混合深度学习模型，以增强特征提取和分类。 Densenet121促进了通过密集连接的重复使用和有效梯度流的特征，而VGG16贡献了强大的层次空间表示。它们的融合可以使神经系统条件的强大多类分类。 Grad-CAM用于可视化明显区域，从而提高模型透明度。 DGG-XNET在Brats 2021和Kaggle的合并数据集上进行了培训，其测试精度为91.33 \％，精确，回忆和F1得分都超过91 \％。这些结果凸显了DGG-XNET作为神经退行性和肿瘤脑疾病的计算机辅助诊断（CAD）的有效且可解释的工具的潜力。</li>
</ul>

<h3>Title: Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Tian Xia, Fabio De Sousa Ribeiro, Rajat R Rasal, Avinash Kori, Raghav Mehta, Ben Glocker</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14399">https://arxiv.org/abs/2506.14399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14399">https://arxiv.org/pdf/2506.14399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14399]] Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models(https://arxiv.org/abs/2506.14399)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Counterfactual image generation aims to simulate realistic visual outcomes under specific causal interventions. Diffusion models have recently emerged as a powerful tool for this task, combining DDIM inversion with conditional generation via classifier-free guidance (CFG). However, standard CFG applies a single global weight across all conditioning variables, which can lead to poor identity preservation and spurious attribute changes - a phenomenon known as attribute amplification. To address this, we propose Decoupled Classifier-Free Guidance (DCFG), a flexible and model-agnostic framework that introduces group-wise conditioning control. DCFG builds on an attribute-split embedding strategy that disentangles semantic inputs, enabling selective guidance on user-defined attribute groups. For counterfactual generation, we partition attributes into intervened and invariant sets based on a causal graph and apply distinct guidance to each. Experiments on CelebA-HQ, MIMIC-CXR, and EMBED show that DCFG improves intervention fidelity, mitigates unintended changes, and enhances reversibility, enabling more faithful and interpretable counterfactual image generation.</li>
<li><strong>摘要：</strong>反事实图像生成旨在模拟特定因果干预措施下的现实视觉结果。扩散模型最近已成为该任务的强大工具，将DDIM倒置与有条件的无分类指导（CFG）相结合。但是，标准CFG在所有条件变量中都施加了单个全球重量，这可能导致身份保存和虚假属性变化差 - 一种称为属性扩增的现象。为了解决这个问题，我们提出了脱钩的无分类器指导（DCFG），这是一个灵活的模型敏捷框架框架，介绍了群体调节控制。 DCFG建立在一个属性 - 分式嵌入策略上，该策略会删除语义输入，从而为用户定义的属性组提供选择性指导。对于反事实生成，我们根据因果图将属性归因于中间和不变的集合，并对每个因素应用不同的指导。对Celeba-HQ，Mimic-CXR和Embed进行的实验表明，DCFG改善了干预保真度，减轻意外变化并增强了可逆性，从而实现了更忠实和更容易解释的反事实图像产生。</li>
</ul>

<h3>Title: Causally Steered Diffusion for Automated Video Counterfactual Generation</h3>
<ul>
<li><strong>Authors: </strong>Nikos Spyrou, Athanasios Vlontzos, Paraskevas Pegios, Thomas Melistas, Nefeli Gkouti, Yannis Panagakis, Giorgos Papanastasiou, Sotirios A. Tsaftaris</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14404">https://arxiv.org/abs/2506.14404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14404">https://arxiv.org/pdf/2506.14404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14404]] Causally Steered Diffusion for Automated Video Counterfactual Generation(https://arxiv.org/abs/2506.14404)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Adapting text-to-image (T2I) latent diffusion models for video editing has shown strong visual fidelity and controllability, but challenges remain in maintaining causal relationships in video content. Edits affecting causally dependent attributes risk generating unrealistic or misleading outcomes if these relationships are ignored. In this work, we propose a causally faithful framework for counterfactual video generation, guided by a vision-language model (VLM). Our method is agnostic to the underlying video editing system and does not require access to its internal mechanisms or finetuning. Instead, we guide the generation by optimizing text prompts based on an assumed causal graph, addressing the challenge of latent space control in LDMs. We evaluate our approach using standard video quality metrics and counterfactual-specific criteria, such as causal effectiveness and minimality. Our results demonstrate that causally faithful video counterfactuals can be effectively generated within the learned distribution of LDMs through prompt-based causal steering. With its compatibility with any black-box video editing system, our method holds significant potential for generating realistic "what-if" video scenarios in diverse areas such as healthcare and digital media.</li>
<li><strong>摘要：</strong>适应视频编辑的文本对图像（T2I）潜在扩散模型已显示出强烈的视觉保真度和可控性，但是在维持视频内容中的因果关系方面仍然存在挑战。如果忽略这些关系，影响因果关系依赖性属性的属性风险会产生不现实或误导性结果。在这项工作中，我们在视觉模型（VLM）的指导下为反事实视频生成的因果忠实框架提出了一个忠实的框架。我们的方法对基础视频编辑系统不可知，不需要访问其内部机制或填充。取而代之的是，我们通过根据假定的因果图优化文本提示来指导生成，从而解决了LDMS中潜在空间控制的挑战。我们使用标准的视频质量指标和反事实特定标准（例如因果有效性和最低限度）评估我们的方法。我们的结果表明，通过基于迅速的因果转向，可以在LDM的学识渊博的分布中有效地产生因果忠实的视频反事实。我们的方法凭借其与任何黑盒视频编辑系统的兼容性，具有在医疗保健和数字媒体等各个领域产生现实的“何种”视频场景的巨大潜力。</li>
</ul>

<h3>Title: Toward Rich Video Human-Motion2D Generation</h3>
<ul>
<li><strong>Authors: </strong>Ruihao Xi, Xuekuan Wang, Yongcheng Li, Shuhua Li, Zichen Wang, Yiwei Wang, Feng Wei, Cairong Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14428">https://arxiv.org/abs/2506.14428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14428">https://arxiv.org/pdf/2506.14428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14428]] Toward Rich Video Human-Motion2D Generation(https://arxiv.org/abs/2506.14428)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating realistic and controllable human motions, particularly those involving rich multi-character interactions, remains a significant challenge due to data scarcity and the complexities of modeling inter-personal dynamics. To address these limitations, we first introduce a new large-scale rich video human motion 2D dataset (Motion2D-Video-150K) comprising 150,000 video sequences. Motion2D-Video-150K features a balanced distribution of diverse single-character and, crucially, double-character interactive actions, each paired with detailed textual descriptions. Building upon this dataset, we propose a novel diffusion-based rich video human motion2D generation (RVHM2D) model. RVHM2D incorporates an enhanced textual conditioning mechanism utilizing either dual text encoders (CLIP-L/B) or T5-XXL with both global and local features. We devise a two-stage training strategy: the model is first trained with a standard diffusion objective, and then fine-tuned using reinforcement learning with an FID-based reward to further enhance motion realism and text alignment. Extensive experiments demonstrate that RVHM2D achieves leading performance on the Motion2D-Video-150K benchmark in generating both single and interactive double-character scenarios.</li>
<li><strong>摘要：</strong>由于数据稀缺性和建模人际动力学的复杂性，产生现实和可控的人类动作，尤其是涉及丰富多个特定相互作用的人的动作，仍然是一个重大挑战。为了解决这些限制，我们首先引入了一个新的大规模丰富视频人体运动2D数据集（Motion2d-Video-150k），其中包括150,000个视频序列。 Motion2D-Video-150k具有各种单字符和至关重要的双字符互动动作的平衡分布，每个动作都与详细的文本描述配对。在此数据集的基础上，我们提出了一种基于新颖的基于扩散的丰富视频Human Motion2D Generation（RVHM2D）模型。 RVHM2D使用双文本编码器（剪辑L/B）或T5-XXL具有增强的文本调节机制，并纳入全局和本地功能。我们制定了一个两阶段的训练策略：该模型首先是具有标准扩散目标的训练，然后使用强化学习以基于FID的奖励进行微调，以进一步增强运动现实主义和文本对准。广泛的实验表明，RVHM2D在产生单个和交互式双字符方案时在Motion2D-Video-150k基准中实现了领先的性能。</li>
</ul>

<h3>Title: Adapting Lightweight Vision Language Models for Radiological Visual Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Aditya Shourya, Michel Dumontier, Chang Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14451">https://arxiv.org/abs/2506.14451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14451">https://arxiv.org/pdf/2506.14451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14451]] Adapting Lightweight Vision Language Models for Radiological Visual Question Answering(https://arxiv.org/abs/2506.14451)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advancements in vision-language systems have improved the accuracy of Radiological Visual Question Answering (VQA) Models. However, some challenges remain across each stage of model development: limited expert-labeled images hinders data procurement at scale; the intricate and nuanced patterns of radiological images make modeling inherently difficult; and the lack of evaluation evaluation efforts makes it difficult to identify cases where the model might be ill-conditioned. In this study, we fine-tune a lightweight 3B parameter vision-language model for Radiological VQA, demonstrating that small models, when appropriately tuned with curated data, can achieve robust performance across both open- and closed-ended questions. We propose a cost-effective training pipeline from synthetic question-answer pair generation to multi-stage fine-tuning on specialised radiological domain-targeted datasets (e.g., ROCO v2.0, MedPix v2.0). Our results show that despite operating at a fraction of the scale of state-of-the-art models such as LLaVA-Med, our model achieves promising performance given its small parameter size and the limited scale of training data. We introduce a lightweight saliency-based diagnostic tool that enables domain experts to inspect VQA model performance and identify ill-conditioned failure modes through saliency analysis.</li>
<li><strong>摘要：</strong>视觉系统的最新进展提高了放射学视觉问题答案（VQA）模型的准确性。但是，模型开发的每个阶段仍然存在一些挑战：有限的专家标记图像阻碍了大规模的数据采购；放射学图像的复杂和细微的模式使建模本质上困难。而且缺乏评估评估工作使得难以确定模型可能存在不良条件的案例。在这项研究中，我们对放射学VQA进行了轻巧的3B参数视觉语言模型，这表明，在使用精选数据进行适当调整时，小型模型可以在开放式和封闭式问题上实现稳健的性能。我们提出了一条具有成本效益的培训管道，从合成问题 - 答案对生成到专门的放射线域数据集（例如Roco V2.0，Medpix V2.0）的多阶段微调。我们的结果表明，尽管以LLAVA-MED等最新模型的规模进行操作，但鉴于其较小的参数大小和有限的培训数据规模，我们的模型仍达到了有希望的性能。我们引入了一种基于轻巧的显着性诊断工具，该工具使域专家能够通过显着分析来检查VQA模型性能并确定未解决的故障模式。</li>
</ul>

<h3>Title: Automated Decision-Making on Networks with LLMs through Knowledge-Guided Evolution</h3>
<ul>
<li><strong>Authors: </strong>Xiaohan Zheng, Lanning Wei, Yong Li, Quanming Yao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14529">https://arxiv.org/abs/2506.14529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14529">https://arxiv.org/pdf/2506.14529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14529]] Automated Decision-Making on Networks with LLMs through Knowledge-Guided Evolution(https://arxiv.org/abs/2506.14529)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Effective decision-making on networks often relies on learning from graph-structured data, where Graph Neural Networks (GNNs) play a central role, but they take efforts to configure and tune. In this demo, we propose LLMNet, showing how to design GNN automated through Large Language Models. Our system develops a set of agents that construct graph-related knowlege bases and then leverages Retrieval-Augmented Generation (RAG) to support automated configuration and refinement of GNN models through a knowledge-guided evolution process. These agents, equipped with specialized knowledge bases, extract insights into tasks and graph structures by interacting with the knowledge bases. Empirical results show LLMNet excels in twelve datasets across three graph learning tasks, validating its effectiveness of GNN model designing.</li>
<li><strong>摘要：</strong>网络上的有效决策通常依赖于从图形结构化数据中学习，在图形结构化数据中，图形神经网络（GNN）起着核心作用，但它们正在努力配置和调整。在此演示中，我们提出了LLMNET，展示了如何通过大型语言模型设计GNN。我们的系统开发了一组代理，这些代理构建了与图形相关的知识库，然后利用检索功能增强的生成（RAG）来通过知识引导的进化过程来支持GNN模型的自动配置和改进。这些代理人配备了专门的知识库，可以通过与知识库互动来提取对任务和图形结构的见解。经验结果表明，LLMNET在三个图表学习任务中擅长十二个数据集，从而验证了其GNN模型设计的有效性。</li>
</ul>

<h3>Title: Exploring Diffusion with Test-Time Training on Efficient Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Rongchang Lu, Tianduo Luo, Yunzhi Zhang, Conghan Yue, Pei Yang, Guibao Liu, Changyang Gu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14541">https://arxiv.org/abs/2506.14541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14541">https://arxiv.org/pdf/2506.14541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14541]] Exploring Diffusion with Test-Time Training on Efficient Image Restoration(https://arxiv.org/abs/2506.14541)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, super-resolution</a></li>
<li><strong>Abstract: </strong>Image restoration faces challenges including ineffective feature fusion, computational bottlenecks and inefficient diffusion processes. To address these, we propose DiffRWKVIR, a novel framework unifying Test-Time Training (TTT) with efficient diffusion. Our approach introduces three key innovations: (1) Omni-Scale 2D State Evolution extends RWKV's location-dependent parameterization to hierarchical multi-directional 2D scanning, enabling global contextual awareness with linear complexity O(L); (2) Chunk-Optimized Flash Processing accelerates intra-chunk parallelism by 3.2x via contiguous chunk processing (O(LCd) complexity), reducing sequential dependencies and computational overhead; (3) Prior-Guided Efficient Diffusion extracts a compact Image Prior Representation (IPR) in only 5-20 steps, proving 45% faster training/inference than DiffIR while solving computational inefficiency in denoising. Evaluated across super-resolution and inpainting benchmarks (Set5, Set14, BSD100, Urban100, Places365), DiffRWKVIR outperforms SwinIR, HAT, and MambaIR/v2 in PSNR, SSIM, LPIPS, and efficiency metrics. Our method establishes a new paradigm for adaptive, high-efficiency image restoration with optimized hardware utilization.</li>
<li><strong>摘要：</strong>图像恢复面临挑战，包括无效的特征融合，计算瓶颈和效率低下的扩散过程。为了解决这些问题，我们提出了Diffrwkvir，这是一个具有有效扩散的新型框架，将测试时间训练（TTT）统一。我们的方法介绍了三个关键创新：（1）OMNI级2D状态进化将RWKV的位置依赖性参数化扩展到层次结构的多向2D扫描，从而使全球上下文意识具有线性复杂性o（l）； （2）块优化的闪光处理通过连续的块处理（O（LCD）复杂性）加速了3.2倍的丘一切并行性，从而降低了顺序依赖性和计算开销。 （3）提前引导的有效扩散仅以5-20步提取紧凑的图像先验表示（IPR），在求解deNoing中的计算效率低时，训练/推理的速度/推理速度要快45％。在PSNR，SSIM，SSIM，LPIPS和效率指标中，对超分辨率和内部基准（SET5，SET5，SET1，SET14，BSD100，URBAN100，PLACE365）进行了评估。我们的方法建立了一个新的范式，用于使用优化的硬件利用率进行自适应，高效图像修复。</li>
</ul>

<h3>Title: DreamLight: Towards Harmonious and Consistent Image Relighting</h3>
<ul>
<li><strong>Authors: </strong>Yong Liu, Wenpeng Xiao, Qianqian Wang, Junlin Chen, Shiyin Wang, Yitong Wang, Xinglong Wu, Yansong Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14549">https://arxiv.org/abs/2506.14549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14549">https://arxiv.org/pdf/2506.14549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14549]] DreamLight: Towards Harmonious and Consistent Image Relighting(https://arxiv.org/abs/2506.14549)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce a model named DreamLight for universal image relighting in this work, which can seamlessly composite subjects into a new background while maintaining aesthetic uniformity in terms of lighting and color tone. The background can be specified by natural images (image-based relighting) or generated from unlimited text prompts (text-based relighting). Existing studies primarily focus on image-based relighting, while with scant exploration into text-based scenarios. Some works employ intricate disentanglement pipeline designs relying on environment maps to provide relevant information, which grapples with the expensive data cost required for intrinsic decomposition and light source. Other methods take this task as an image translation problem and perform pixel-level transformation with autoencoder architecture. While these methods have achieved decent harmonization effects, they struggle to generate realistic and natural light interaction effects between the foreground and background. To alleviate these challenges, we reorganize the input data into a unified format and leverage the semantic prior provided by the pretrained diffusion model to facilitate the generation of natural results. Moreover, we propose a Position-Guided Light Adapter (PGLA) that condenses light information from different directions in the background into designed light query embeddings, and modulates the foreground with direction-biased masked attention. In addition, we present a post-processing module named Spectral Foreground Fixer (SFF) to adaptively reorganize different frequency components of subject and relighted background, which helps enhance the consistency of foreground appearance. Extensive comparisons and user study demonstrate that our DreamLight achieves remarkable relighting performance.</li>
<li><strong>摘要：</strong>我们介绍了一个名为Dreamlight的模型，用于在这项工作中重新保留通用图像，该模型可以将其无缝地复合主题为新的背景，同时在照明和色调方面保持美学统一性。背景可以通过自然图像（基于图像的重新计算）指定，也可以通过无限的文本提示（基于文本的重新计算）生成。现有的研究主要集中于基于图像的重新研究，而对基于文本的方案进行了少量探索。一些作品采用了复杂的解开管道设计，这些设计依靠环境图来提供相关信息，这些信息应对内在分解和光源所需的昂贵数据成本。其他方法将此任务作为图像翻译问题，并使用自动编码器体系结构执行像素级变换。尽管这些方法已经达到了不错的协调效应，但它们努力在前景和背景之间产生逼真的自然光相互作用。为了减轻这些挑战，我们将输入数据重组为统一的格式，并利用预验证的扩散模型提供的语义先验，以促进自然结果的产生。此外，我们提出了一个位置引导的轻型适配器（PGLA），该适配器（PGLA）将背景中不同方向的光信息凝结到设计的光查询嵌入中，并以方向偏见的掩盖注意力调节前景。此外，我们提出了一个名为Spectraws前景固定器（SFF）的后处理模块，以适应主题和重新背景的不同频率组件，这有助于增强前景外观的一致性。广泛的比较和用户研究表明，我们的Dreamlight取得了非凡的重新效果。</li>
</ul>

<h3>Title: Risk Estimation of Knee Osteoarthritis Progression via Predictive Multi-task Modelling from Efficient Diffusion Model using X-ray Images</h3>
<ul>
<li><strong>Authors: </strong>David Butler, Adrian Hilton, Gustavo Carneiro</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14560">https://arxiv.org/abs/2506.14560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14560">https://arxiv.org/pdf/2506.14560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14560]] Risk Estimation of Knee Osteoarthritis Progression via Predictive Multi-task Modelling from Efficient Diffusion Model using X-ray Images(https://arxiv.org/abs/2506.14560)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Medical imaging plays a crucial role in assessing knee osteoarthritis (OA) risk by enabling early detection and disease monitoring. Recent machine learning methods have improved risk estimation (i.e., predicting the likelihood of disease progression) and predictive modelling (i.e., the forecasting of future outcomes based on current data) using medical images, but clinical adoption remains limited due to their lack of interpretability. Existing approaches that generate future images for risk estimation are complex and impractical. Additionally, previous methods fail to localize anatomical knee landmarks, limiting interpretability. We address these gaps with a new interpretable machine learning method to estimate the risk of knee OA progression via multi-task predictive modelling that classifies future knee OA severity and predicts anatomical knee landmarks from efficiently generated high-quality future images. Such image generation is achieved by leveraging a diffusion model in a class-conditioned latent space to forecast disease progression, offering a visual representation of how particular health conditions may evolve. Applied to the Osteoarthritis Initiative dataset, our approach improves the state-of-the-art (SOTA) by 2\%, achieving an AUC of 0.71 in predicting knee OA progression while offering ~9% faster inference time.</li>
<li><strong>摘要：</strong>医学成像通过实现早期发现和疾病监测来评估膝盖骨关节炎（OA）风险至关重要。最近的机器学习方法改善了风险估计（即预测疾病进展的可能性）和预测性建模（即使用医学图像对未来结果的预测），但由于缺乏可解释性，临床采用仍然有限。产生未来图像以进行风险估计的现有方法是复杂且不切实际的。此外，以前的方法无法定位解剖膝关节标志，从而限制了解释性。我们通过一种新的可解释的机器学习方法来解决这些差距，以通过多任务预测建模来估计膝盖OA进展的风险，该模型对未来的膝盖OA严重性进行了分类，并通过有效产生的高质量的未来图像来预测解剖学的膝盖地标。这种图像产生是通过利用类带有的潜在空间中的扩散模型来预测疾病进展的，从而提供了视觉表示特定健康状况的发展。我们的方法应用于骨关节炎的倡议数据集，将最新的（SOTA）提高了2 \％，在预测膝关节进展的同时提供了约9％的推断时间，达到了0.71的AUC。</li>
</ul>

<h3>Title: Single-Example Learning in a Mixture of GPDMs with Latent Geometries</h3>
<ul>
<li><strong>Authors: </strong>Jesse St. Amand, Leonardo Gizzi, Martin A. Giese</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14563">https://arxiv.org/abs/2506.14563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14563">https://arxiv.org/pdf/2506.14563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14563]] Single-Example Learning in a Mixture of GPDMs with Latent Geometries(https://arxiv.org/abs/2506.14563)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We present the Gaussian process dynamical mixture model (GPDMM) and show its utility in single-example learning of human motion data. The Gaussian process dynamical model (GPDM) is a form of the Gaussian process latent variable model (GPLVM), but optimized with a hidden Markov model dynamical prior. The GPDMM combines multiple GPDMs in a probabilistic mixture-of-experts framework, utilizing embedded geometric features to allow for diverse sequences to be encoded in a single latent space, enabling the categorization and generation of each sequence class. GPDMs and our mixture model are particularly advantageous in addressing the challenges of modeling human movement in scenarios where data is limited and model interpretability is vital, such as in patient-specific medical applications like prosthesis control. We score the GPDMM on classification accuracy and generative ability in single-example learning, showcase model variations, and benchmark it against LSTMs, VAEs, and transformers.</li>
<li><strong>摘要：</strong>我们介绍了高斯工艺动力混合模型（GPDMM），并在人类运动数据的单个例子学习中显示了其效用。高斯工艺动力学模型（GPDM）是高斯过程潜在变量模型（GPLVM）的一种形式，但使用隐藏的Markov模型动态先验进行了优化。 GPDMM将多个gpdms结合在一起，在概率的专家框架中，利用嵌入式的几何特征允许在单个潜在空间中编码各种序列，从而使每个序列类别的分类和生成。 GPDM和我们的混合模型在解决数据有限且模型可解释性至关重要的情况下建模人类运动的挑战尤其有利，例如在特定于患者的医学应用中，例如假体控制。我们在单个示例学习，展示模型变化并根据LSTMS，VAE和变形金刚基准测试的分类精度和生成能力上的GPDMM评分。</li>
</ul>

<h3>Title: Align Your Flow: Scaling Continuous-Time Flow Map Distillation</h3>
<ul>
<li><strong>Authors: </strong>Amirmojtaba Sabour, Sanja Fidler, Karsten Kreis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14603">https://arxiv.org/abs/2506.14603</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14603">https://arxiv.org/pdf/2506.14603</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14603]] Align Your Flow: Scaling Continuous-Time Flow Map Distillation(https://arxiv.org/abs/2506.14603)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion- and flow-based models have emerged as state-of-the-art generative modeling approaches, but they require many sampling steps. Consistency models can distill these models into efficient one-step generators; however, unlike flow- and diffusion-based methods, their performance inevitably degrades when increasing the number of steps, which we show both analytically and empirically. Flow maps generalize these approaches by connecting any two noise levels in a single step and remain effective across all step counts. In this paper, we introduce two new continuous-time objectives for training flow maps, along with additional novel training techniques, generalizing existing consistency and flow matching objectives. We further demonstrate that autoguidance can improve performance, using a low-quality model for guidance during distillation, and an additional boost can be achieved by adversarial finetuning, with minimal loss in sample diversity. We extensively validate our flow map models, called Align Your Flow, on challenging image generation benchmarks and achieve state-of-the-art few-step generation performance on both ImageNet 64x64 and 512x512, using small and efficient neural networks. Finally, we show text-to-image flow map models that outperform all existing non-adversarially trained few-step samplers in text-conditioned synthesis.</li>
<li><strong>摘要：</strong>基于扩散和基于流的模型已成为最新的生成建模方法，但它们需要许多采样步骤。一致性模型可以将这些模型提炼成有效的一步生成器。但是，与基于流量和扩散的方法不同，它们的性能在增加步骤数时不可避免地会降低，我们在分析和经验上都显示出这些步骤。流图通过在一个步骤中连接任意两个噪声水平并在所有步骤计数中保持有效，从而概括了这些方法。在本文中，我们介绍了两个新的连续时间目标，用于训练流图，以及其他新颖的培训技术，从而推广了现有的一致性和流动匹配的目标。我们进一步证明，使用低质量模型在蒸馏过程中进行指导，自动化可以提高性能，并且可以通过对抗性填充来实现额外的提升，而样品多样性的损失最小。我们使用小而有效的神经网络，在Imagenet 64x64和512x512上广泛验证了我们的流图模型，称为“对齐您的流”，并在Imagenet 64x64和512x512上实现最新的几步生成性能。最后，我们显示了文本到图像流量图模型，这些模型在文本条件的合成中优于所有现有的非对抗性训练的几步采样器。</li>
</ul>

<h3>Title: Unsupervised Imaging Inverse Problems with Diffusion Distribution Matching</h3>
<ul>
<li><strong>Authors: </strong>Giacomo Meanti, Thomas Ryckeboer, Michael Arbel, Julien Mairal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14605">https://arxiv.org/abs/2506.14605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14605">https://arxiv.org/pdf/2506.14605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14605]] Unsupervised Imaging Inverse Problems with Diffusion Distribution Matching(https://arxiv.org/abs/2506.14605)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, super-resolution</a></li>
<li><strong>Abstract: </strong>This work addresses image restoration tasks through the lens of inverse problems using unpaired datasets. In contrast to traditional approaches -- which typically assume full knowledge of the forward model or access to paired degraded and ground-truth images -- the proposed method operates under minimal assumptions and relies only on small, unpaired datasets. This makes it particularly well-suited for real-world scenarios, where the forward model is often unknown or misspecified, and collecting paired data is costly or infeasible. The method leverages conditional flow matching to model the distribution of degraded observations, while simultaneously learning the forward model via a distribution-matching loss that arises naturally from the framework. Empirically, it outperforms both single-image blind and unsupervised approaches on deblurring and non-uniform point spread function (PSF) calibration tasks. It also matches state-of-the-art performance on blind super-resolution. We also showcase the effectiveness of our method with a proof of concept for lens calibration: a real-world application traditionally requiring time-consuming experiments and specialized equipment. In contrast, our approach achieves this with minimal data acquisition effort.</li>
<li><strong>摘要：</strong>这项工作通过使用未配对的数据集通过反向问题的镜头来解决图像恢复任务。与传统方法相反，该方法通常会完全了解向前模型或对配对的降解和地面真实图像的访问，该方法是在最小的假设下运行的，并且仅依赖于小的，不成对的数据集。这使其特别适合现实世界中的场景，在现实世界中，前向模型通常未知或误导，并且收集配对数据是昂贵或不可行的。该方法利用有条件的流量匹配来建模降解观测值的分布，同时通过自然而然地从框架上产生的分布匹配损失来学习前向模型。从经验上讲，它表现优于单位图像盲人和无监督的方法，而在脱胸和不均匀点扩散函数（PSF）校准任务上的表现。它还与盲目超级分辨率上的最新性能相匹配。我们还通过镜头校准的概念证明来展示我们的方法的有效性：传统上需要耗时的实验和专业设备的真实世界应用程序。相比之下，我们的方法通过最少的数据获取工作来实现这一目标。</li>
</ul>

<h3>Title: VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based Mosquito Breeding Site Detection and Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Md. Adnanul Islam, Md. Faiyaz Abdullah Sayeedi, Md. Asaduzzaman Shuvo, Muhammad Ziaur Rahman, Shahanur Rahman Bappy, Raiyan Rahman, Swakkhar Shatabda</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14629">https://arxiv.org/abs/2506.14629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14629">https://arxiv.org/pdf/2506.14629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14629]] VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based Mosquito Breeding Site Detection and Reasoning(https://arxiv.org/abs/2506.14629)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Mosquito-borne diseases pose a major global health risk, requiring early detection and proactive control of breeding sites to prevent outbreaks. In this paper, we present VisText-Mosquito, a multimodal dataset that integrates visual and textual data to support automated detection, segmentation, and reasoning for mosquito breeding site analysis. The dataset includes 1,828 annotated images for object detection, 142 images for water surface segmentation, and natural language reasoning texts linked to each image. The YOLOv9s model achieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object detection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and mAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves a final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and ROUGE-L of 0.87. This dataset and model framework emphasize the theme "Prevention is Better than Cure", showcasing how AI-based detection can proactively address mosquito-borne disease risks. The dataset and implementation code are publicly available at GitHub: this https URL</li>
<li><strong>摘要：</strong>蚊子传播的疾病构成了全球主要的健康风险，需要早期发现并积极控制繁殖地，以防止爆发。在本文中，我们提出了Vistext-Mosquito，这是一种多模式数据集，该数据集集成了视觉和文本数据，以支持自动检测，分割和蚊子育种站点分析的推理。该数据集包括1,828个带注释的图像，用于对象检测，142个用于水面分割的图像以及链接到每个图像的自然语言推理文本。 Yolov9S模型达到了0.92926的最高精度，并在0.92891中获得了0.92891的MAP@50，而Yolov11n-Seg达到了0.91587的分割精度，MAP@50 of 0.79795 of 0.79795。对于推理生成，我们的微调BLIP模型实现了0.0028的最终损失，BLEU得分为54.7，Bertscore为0.91，Rouge-L为0.87。该数据集和模型框架强调了主题“预防胜于治疗更好”，展示了基于AI的检测如何主动解决蚊子 - 传播疾病风险。数据集和实现代码可在GitHub上公开获得：此HTTPS URL</li>
</ul>

<h3>Title: 3DGS-IEval-15K: A Large-scale Image Quality Evaluation Database for 3D Gaussian-Splatting</h3>
<ul>
<li><strong>Authors: </strong>Yuke Xing, Jiarui Wang, Peizhi Niu, Wenjie Huang, Guangtao Zhai, Yiling Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14642">https://arxiv.org/abs/2506.14642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14642">https://arxiv.org/pdf/2506.14642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14642]] 3DGS-IEval-15K: A Large-scale Image Quality Evaluation Database for 3D Gaussian-Splatting(https://arxiv.org/abs/2506.14642)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>3D Gaussian Splatting (3DGS) has emerged as a promising approach for novel view synthesis, offering real-time rendering with high visual fidelity. However, its substantial storage requirements present significant challenges for practical applications. While recent state-of-the-art (SOTA) 3DGS methods increasingly incorporate dedicated compression modules, there is a lack of a comprehensive framework to evaluate their perceptual impact. Therefore we present 3DGS-IEval-15K, the first large-scale image quality assessment (IQA) dataset specifically designed for compressed 3DGS representations. Our dataset encompasses 15,200 images rendered from 10 real-world scenes through 6 representative 3DGS algorithms at 20 strategically selected viewpoints, with different compression levels leading to various distortion effects. Through controlled subjective experiments, we collect human perception data from 60 viewers. We validate dataset quality through scene diversity and MOS distribution analysis, and establish a comprehensive benchmark with 30 representative IQA metrics covering diverse types. As the largest-scale 3DGS quality assessment dataset to date, our work provides a foundation for developing 3DGS specialized IQA metrics, and offers essential data for investigating view-dependent quality distribution patterns unique to 3DGS. The database is publicly available at this https URL.</li>
<li><strong>摘要：</strong>3D高斯脱落（3DGS）已成为一种新型视图合成的有前途的方法，以高视觉保真度提供实时渲染。但是，其大量的存储要求对实际应用面临着重大挑战。尽管最近的最新最新（SOTA）3DGS方法越来越多地包含了专用的压缩模块，但缺乏评估其感知影响的全面框架。因此，我们提出了3DGS-IEVAL-15K，这是第一个专门为压缩3DG表示的大规模图像质量评估（IQA）数据集。我们的数据集包含15,200张图像，从10个具有战略性选择的观点的6种代表性3DGS算法从10个现实世界场景呈现，具有不同的压缩水平，从而导致各种失真效应。通过受控的主观实验，我们收集了来自60位观众的人类感知数据。我们通过场景多样性和MOS分配分析来验证数据集质量，并建立一个全面的基准测试，其中30个代表性的IQA指标涵盖了各种类型的类型。作为迄今为止最大的3DGS质量评估数据集，我们的工作为开发3DGS专业的IQA指标提供了基础，并提供了调查与3DGS独有的观点质量分布模式的重要数据。该数据库可在此HTTPS URL上公开获得。</li>
</ul>

<h3>Title: FocalClick-XL: Towards Unified and High-quality Interactive Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Xi Chen, Hengshuang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14686">https://arxiv.org/abs/2506.14686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14686">https://arxiv.org/pdf/2506.14686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14686]] FocalClick-XL: Towards Unified and High-quality Interactive Segmentation(https://arxiv.org/abs/2506.14686)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Interactive segmentation enables users to extract binary masks of target objects through simple interactions such as clicks, scribbles, and boxes. However, existing methods often support only limited interaction forms and struggle to capture fine details. In this paper, we revisit the classical coarse-to-fine design of FocalClick and introduce significant extensions. Inspired by its multi-stage strategy, we propose a novel pipeline, FocalClick-XL, to address these challenges simultaneously. Following the emerging trend of large-scale pretraining, we decompose interactive segmentation into meta-tasks that capture different levels of information -- context, object, and detail -- assigning a dedicated subnet to each this http URL decomposition allows each subnet to undergo scaled pretraining with independent data and supervision, maximizing its effectiveness. To enhance flexibility, we share context- and detail-level information across different interaction forms as common knowledge while introducing a prompting layer at the object level to encode specific interaction types. As a result, FocalClick-XL achieves state-of-the-art performance on click-based benchmarks and demonstrates remarkable adaptability to diverse interaction formats, including boxes, scribbles, and coarse masks. Beyond binary mask generation, it is also capable of predicting alpha mattes with fine-grained details, making it a versatile and powerful tool for interactive segmentation.</li>
<li><strong>摘要：</strong>交互式分割使用户可以通过单击，涂鸦和框等简单的交互来提取目标对象的二进制蒙版。但是，现有方法通常仅支持有限的互动形式，并难以捕获细节。在本文中，我们重新审视了焦点的经典粗到精细设计，并引入了重要的扩展。受其多阶段策略的启发，我们提出了一条新颖的管道focalClick-XL，以同时解决这些挑战。遵循大规模预处理的新兴趋势，我们将交互式分割分解为元任务，以捕获不同级别的信息（上下文，对象和细节），将专用子网分配给每个HTTP URL分解允许每个子网允许每个子网进行独立的数据并监督其有效性，从而实现缩放预处理。为了提高灵活性，我们在不同的交互形式上共享上下文和详细信息级别的信息作为常识，同时在对象级别引入提示层以编码特定的相互作用类型。结果，FocalClick-XL在基于点击的基准上实现了最先进的性能，并证明了对各种相互作用格式的显着适应性，包括盒子，涂鸦和粗蒙版。除了二进制掩码生成之外，它还能够预测具有细粒细节的α哑光，使其成为交互式分割的多功能和功能强大的工具。</li>
</ul>

<h3>Title: Towards Desiderata-Driven Design of Visual Counterfactual Explainers</h3>
<ul>
<li><strong>Authors: </strong>Sidney Bender, Jan Herrmann, Klaus-Robert Müller, Grégoire Montavon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14698">https://arxiv.org/abs/2506.14698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14698">https://arxiv.org/pdf/2506.14698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14698]] Towards Desiderata-Driven Design of Visual Counterfactual Explainers(https://arxiv.org/abs/2506.14698)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Visual counterfactual explainers (VCEs) are a straightforward and promising approach to enhancing the transparency of image classifiers. VCEs complement other types of explanations, such as feature attribution, by revealing the specific data transformations to which a machine learning model responds most strongly. In this paper, we argue that existing VCEs focus too narrowly on optimizing sample quality or change minimality; they fail to consider the more holistic desiderata for an explanation, such as fidelity, understandability, and sufficiency. To address this shortcoming, we explore new mechanisms for counterfactual generation and investigate how they can help fulfill these desiderata. We combine these mechanisms into a novel 'smooth counterfactual explorer' (SCE) algorithm and demonstrate its effectiveness through systematic evaluations on synthetic and real data.</li>
<li><strong>摘要：</strong>视觉反事实解释器（VCE）是提高图像分类器透明度的直接而有前途的方法。通过揭示机器学习模型对这些特定的数据转换，VCE可以补充其他类型的解释，例如特征归因。在本文中，我们认为现有的VCE过于狭窄地专注于优化样本质量或更改最小的。他们未能考虑更全面的解释，例如忠诚，可理解性和充分性。为了解决这一缺点，我们探索了反事实生成的新机制，并研究了它们如何帮助实现这些逃亡者。我们将这些机制结合到一种新颖的“平滑反事实探险家”（SCE）算法中，并通过对合成和真实数据进行系统评估来证明其有效性。</li>
</ul>

<h3>Title: Cost-Aware Routing for Efficient Text-To-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Qinchan (Wing)Li, Kenneth Chen, Changyue (Tina)Su, Wittawat Jitkrittum, Qi Sun, Patsorn Sangkloy</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14753">https://arxiv.org/abs/2506.14753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14753">https://arxiv.org/pdf/2506.14753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14753]] Cost-Aware Routing for Efficient Text-To-Image Generation(https://arxiv.org/abs/2506.14753)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models are well known for their ability to generate a high-fidelity image for an input prompt through an iterative denoising process. Unfortunately, the high fidelity also comes at a high computational cost due the inherently sequential generative process. In this work, we seek to optimally balance quality and computational cost, and propose a framework to allow the amount of computation to vary for each prompt, depending on its complexity. Each prompt is automatically routed to the most appropriate text-to-image generation function, which may correspond to a distinct number of denoising steps of a diffusion model, or a disparate, independent text-to-image model. Unlike uniform cost reduction techniques (e.g., distillation, model quantization), our approach achieves the optimal trade-off by learning to reserve expensive choices (e.g., 100+ denoising steps) only for a few complex prompts, and employ more economical choices (e.g., small distilled model) for less sophisticated prompts. We empirically demonstrate on COCO and DiffusionDB that by learning to route to nine already-trained text-to-image models, our approach is able to deliver an average quality that is higher than that achievable by any of these models alone.</li>
<li><strong>摘要：</strong>扩散模型以其通过迭代降解过程生成输入提示的高保真图像的能力而闻名。不幸的是，由于固有的顺序生成过程，高保真度也具有高计算成本。在这项工作中，我们试图最佳地平衡质量和计算成本，并提出一个框架，以使每个提示的计算量变化，具体取决于其复杂性。每个提示都会自动路由到最合适的文本到图像生成函数，这可能对应于扩散模型的不同数量的剥离步骤，或一个不同的独立文本对图像模型。与均匀的成本降低技术（例如，蒸馏，模型量化）不同，我们的方法通过学习保留昂贵的选择（例如100多个脱氧步骤）来实现最佳的权衡，仅用于几个复杂的提示，并采用更经济的选择（例如，小型蒸馏型模型）来获得较少的复杂提示。我们从经验上证明了可可和扩散性数据库，即通过学习途径到九种已经训练的文本对象模型，我们的方法能够提供的平均质量比仅这些模型中的任何一个都可以实现的质量高。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
