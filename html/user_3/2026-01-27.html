<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-01-27</h1>
<h3>Title: TelcoAI: Advancing 3GPP Technical Specification Search through Agentic Multi-Modal Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Rahul Ghosh, Chun-Hao Liu, Gaurav Rele, Vidya Sagar Ravipati, Hazar Aouad</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV, cs.IR, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16984">https://arxiv.org/abs/2601.16984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16984">https://arxiv.org/pdf/2601.16984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16984]] TelcoAI: Advancing 3GPP Technical Specification Search through Agentic Multi-Modal Retrieval-Augmented Generation(https://arxiv.org/abs/2601.16984)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The 3rd Generation Partnership Project (3GPP) produces complex technical specifications essential to global telecommunications, yet their hierarchical structure, dense formatting, and multi-modal content make them difficult to process. While Large Language Models (LLMs) show promise, existing approaches fall short in handling complex queries, visual information, and document interdependencies. We present TelcoAI, an agentic, multi-modal Retrieval-Augmented Generation (RAG) system tailored for 3GPP documentation. TelcoAI introduces section-aware chunking, structured query planning, metadata-guided retrieval, and multi-modal fusion of text and diagrams. Evaluated on multiple benchmarks-including expert-curated queries-our system achieves $87\%$ recall, $83\%$ claim recall, and $92\%$ faithfulness, representing a $16\%$ improvement over state-of-the-art baselines. These results demonstrate the effectiveness of agentic and multi-modal reasoning in technical document understanding, advancing practical solutions for real-world telecommunications research and engineering.</li>
<li><strong>摘要：</strong>第三代合作伙伴计划 (3GPP) 制定了对全球电信至关重要的复杂技术规范，但其层次结构、密集格式和多模式内容使其难以处理。虽然大型语言模型 (LLM) 展现出希望，但现有方法在处理复杂查询、视觉信息和文档相互依赖性方面存在不足。我们推出 TelcoAI，这是一种专为 3GPP 文档定制的代理、多模式检索增强生成 (RAG) 系统。 TelcoAI 引入了分段感知分块、结构化查询规划、元数据引导检索以及文本和图表的多模式融合。根据多个基准（包括专家策划的查询）进行评估，我们的系统实现了 $87\%$ 召回率、$83\%$ 索赔召回率和 $92\%$ 忠诚度，比最先进的基准提高了 $16\%$。这些结果证明了代理和多模态推理在技术文档理解方面的有效性，为现实世界的电信研究和工程提出了实用的解决方案。</li>
</ul>

<h3>Title: Analysis of voice recordings features for Classification of Parkinson's Disease</h3>
<ul>
<li><strong>Authors: </strong>Beatriz Pérez-Sánchez, Noelia Sánchez-Maroño, Miguel A. Díaz-Freire</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17007">https://arxiv.org/abs/2601.17007</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17007">https://arxiv.org/pdf/2601.17007</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17007]] Analysis of voice recordings features for Classification of Parkinson's Disease(https://arxiv.org/abs/2601.17007)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Parkinson's disease (PD) is a chronic neurodegenerative disease. Early diagnosis is essential to mitigate the progressive deterioration of patients' quality of life. The most characteristic motor symptoms are very mild in the early stages, making diagnosis difficult. Recent studies have shown that the use of patient voice recordings can aid in early diagnosis. Although the analysis of such recordings is costly from a clinical point of view, advances in machine learning techniques are making the processing of this type of data increasingly accurate and efficient. Vocal recordings contain many features, but it is not known whether all of them are relevant for diagnosing the disease. This paper proposes the use of different types of machine learning models combined with feature selection methods to detect the disease. The selection techniques allow to reduce the number of features used by the classifiers by determining which ones provide the most information about the problem. The results show that machine learning methods, in particular neural networks, are suitable for PD classification and that the number of features can be significantly reduced without affecting the performance of the models.</li>
<li><strong>摘要：</strong>帕金森病（PD）是一种慢性神经退行性疾病。早期诊断对于减轻患者生活质量的逐渐恶化至关重要。最典型的运动症状在早期阶段非常轻微，使得诊断变得困难。最近的研究表明，使用患者录音有助于早期诊断。尽管从临床角度来看，分析此类记录的成本很高，但机器学习技术的进步正在使此类数据的处理变得越来越准确和高效。声音录音包含许多特征，但尚不清楚所有这些特征是否都与诊断疾病相关。本文提出使用不同类型的机器学习模型结合特征选择方法来检测疾病。选择技术可以通过确定哪些特征提供有关问题的最多信息来减少分类器使用的特征数量。结果表明，机器学习方法，特别是神经网络，适合PD分类，并且可以显着减少特征数量而不影响模型的性能。</li>
</ul>

<h3>Title: Bayesian Robust Financial Trading with Adversarial Synthetic Market Data</h3>
<ul>
<li><strong>Authors: </strong>Haochong Xia, Simin Li, Ruixiao Xu, Zhixia Zhang, Hongxiang Wang, Zhiqian Liu, Teng Yao Long, Molei Qin, Chuqiao Zong, Bo An</a></li>
<li><strong>Subjects: </strong>cs.LG, q-fin.TR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17008">https://arxiv.org/abs/2601.17008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17008">https://arxiv.org/pdf/2601.17008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17008]] Bayesian Robust Financial Trading with Adversarial Synthetic Market Data(https://arxiv.org/abs/2601.17008)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Algorithmic trading relies on machine learning models to make trading decisions. Despite strong in-sample performance, these models often degrade when confronted with evolving real-world market regimes, which can shift dramatically due to macroeconomic changes-e.g., monetary policy updates or unanticipated fluctuations in participant behavior. We identify two challenges that perpetuate this mismatch: (1) insufficient robustness in existing policy against uncertainties in high-level market fluctuations, and (2) the absence of a realistic and diverse simulation environment for training, leading to policy overfitting. To address these issues, we propose a Bayesian Robust Framework that systematically integrates a macro-conditioned generative model with robust policy learning. On the data side, to generate realistic and diverse data, we propose a macro-conditioned GAN-based generator that leverages macroeconomic indicators as primary control variables, synthesizing data with faithful temporal, cross-instrument, and macro correlations. On the policy side, to learn robust policy against market fluctuations, we cast the trading process as a two-player zero-sum Bayesian Markov game, wherein an adversarial agent simulates shifting regimes by perturbing macroeconomic indicators in the macro-conditioned generator, while the trading agent-guided by a quantile belief network-maintains and updates its belief over hidden market states. The trading agent seeks a Robust Perfect Bayesian Equilibrium via Bayesian neural fictitious self-play, stabilizing learning under adversarial market perturbations. Extensive experiments on 9 financial instruments demonstrate that our framework outperforms 9 state-of-the-art baselines. In extreme events like the COVID, our method shows improved profitability and risk management, offering a reliable solution for trading under uncertain and shifting market dynamics.</li>
<li><strong>摘要：</strong>算法交易依靠机器学习模型来做出交易决策。尽管样本内表现强劲，但当面对不断变化的现实世界市场体制时，这些模型往往会退化，而现实市场体制可能会因宏观经济变化（例如货币政策更新或参与者行为的意外波动）而发生巨大变化。我们发现了导致这种不匹配现象长期存在的两个挑战：（1）现有政策对高水平市场波动的不确定性的鲁棒性不足；（2）缺乏现实且多样化的训练模拟环境，导致政策过度拟合。为了解决这些问题，我们提出了一个贝叶斯稳健框架，该框架系统地将宏观条件生成模型与稳健的政策学习相结合。在数据方面，为了生成真实且多样化的数据，我们提出了一种基于宏观条件的 GAN 生成器，它利用宏观经济指标作为主要控制变量，合成具有忠实时间、跨仪器和宏观相关性的数据。在政策方面，为了学习针对市场波动的稳健政策，我们将交易过程视为两人零和贝叶斯马尔可夫游戏，其中对抗代理通过扰乱宏观条件生成器中的宏观经济指标来模拟变化的制度，而交易代理则在分位数信念网络的指导下维持和更新其对隐藏市场状态的信念。交易代理通过贝叶斯神经虚拟自我博弈寻求鲁棒完美贝叶斯均衡，从而在对抗性市场扰动下稳定学习。对 9 种金融工具的广泛实验表明，我们的框架优于 9 种最先进的基线。在像新冠肺炎这样的极端事件中，我们的方法显示出盈利能力和风险管理的改善，为不确定和不断变化的市场动态下的交易提供了可靠的解决方案。</li>
</ul>

<h3>Title: Optimizing the Landscape of LLM Embeddings with Dynamic Exploratory Graph Analysis for Generative Psychometrics: A Monte Carlo Study</h3>
<ul>
<li><strong>Authors: </strong>Hudson Golino</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17010">https://arxiv.org/abs/2601.17010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17010">https://arxiv.org/pdf/2601.17010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17010]] Optimizing the Landscape of LLM Embeddings with Dynamic Exploratory Graph Analysis for Generative Psychometrics: A Monte Carlo Study(https://arxiv.org/abs/2601.17010)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) embeddings are increasingly used to estimate dimensional structure in psychological item pools prior to data collection, yet current applications treat embeddings as static, cross-sectional representations. This approach implicitly assumes uniform contribution across all embedding coordinates and overlooks the possibility that optimal structural information may be concentrated in specific regions of the embedding space. This study reframes embeddings as searchable landscapes and adapts Dynamic Exploratory Graph Analysis (DynEGA) to systematically traverse embedding coordinates, treating the dimension index as a pseudo-temporal ordering analogous to intensive longitudinal trajectories. A large-scale Monte Carlo simulation embedded items representing five dimensions of grandiose narcissism using OpenAI's text-embedding-3-small model, generating network estimations across systematically varied item pool sizes (3-40 items per dimension) and embedding depths (3-1,298 dimensions). Results reveal that Total Entropy Fit Index (TEFI) and Normalized Mutual Information (NMI) leads to competing optimization trajectories across the embedding landscape. TEFI achieves minima at deep embedding ranges (900--1,200 dimensions) where entropy-based organization is maximal but structural accuracy degrades, whereas NMI peaks at shallow depths where dimensional recovery is strongest but entropy-based fit remains suboptimal. Single-metric optimization produces structurally incoherent solutions, whereas a weighted composite criterion identifies embedding dimensions depth regions that jointly balance accuracy and organization. Optimal embedding depth scales systematically with item pool size. These findings establish embedding landscapes as non-uniform semantic spaces requiring principled optimization rather than default full-vector usage.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）嵌入越来越多地用于在数据收集之前估计心理项目池中的维度结构，但当前的应用程序将嵌入视为静态的横截面表示。这种方法隐含地假设所有嵌入坐标的贡献一致，并忽略了最佳结构信息可能集中在嵌入空间的特定区域的可能性。这项研究将嵌入重新构建为可搜索的景观，并采用动态探索图分析（DynEGA）来系统地遍历嵌入坐标，将维度索引视为类似于密集纵向轨迹的伪时间排序。使用 OpenAI 的 text-embedding-3-small 模型，大规模蒙特卡罗模拟嵌入了代表浮夸自恋的五个维度的项目，生成跨系统变化的项目池大小（每个维度 3-40 个项目）和嵌入深度（3-1,298 维度）的网络估计。结果表明，总熵拟合指数（TEFI）和归一化互信息（NMI）导致整个嵌入领域的竞争优化轨迹。 TEFI 在深嵌入范围（900--1,200 维）达到最小值，其中基于熵的组织最大，但结构精度下降，而 NMI 在浅深度达到峰值，其中尺寸恢复最强，但基于熵的拟合仍然次优。单度量优化产生结构上不连贯的解决方案，而加权复合标准则确定嵌入维度深度区域，从而共同平衡准确性和组织。最佳嵌入深度与项目池大小系统地缩放。这些发现将嵌入景观确定为非均匀语义空间，需要原则性优化，而不是默认的全向量使用。</li>
</ul>

<h3>Title: Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility</h3>
<ul>
<li><strong>Authors: </strong>Honglin Lin, Chonghan Qin, Zheng Liu, Qizhi Pei, Yu Li, Zhanping Zhong, Xin Gao, Yanfeng Wang, Conghui He, Lijun Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17027">https://arxiv.org/abs/2601.17027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17027">https://arxiv.org/pdf/2601.17027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17027]] Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility(https://arxiv.org/abs/2601.17027)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While synthetic data has proven effective for improving scientific reasoning in the text domain, multimodal reasoning remains constrained by the difficulty of synthesizing scientifically rigorous images. Existing Text-to-Image (T2I) models often produce outputs that are visually plausible yet scientifically incorrect, resulting in a persistent visual-logic divergence that limits their value for downstream reasoning. Motivated by recent advances in next-generation T2I models, we conduct a systematic study of scientific image synthesis across generation paradigms, evaluation, and downstream use. We analyze both direct pixel-based generation and programmatic synthesis, and propose ImgCoder, a logic-driven framework that follows an explicit "understand - plan - code" workflow to improve structural precision. To rigorously assess scientific correctness, we introduce SciGenBench, which evaluates generated images based on information utility and logical validity. Our evaluation reveals systematic failure modes in pixel-based models and highlights a fundamental expressiveness-precision trade-off. Finally, we show that fine-tuning Large Multimodal Models (LMMs) on rigorously verified synthetic scientific images yields consistent reasoning gains, with potential scaling trends analogous to the text domain, validating high-fidelity scientific synthesis as a viable path to unlocking massive multimodal reasoning capabilities.</li>
<li><strong>摘要：</strong>虽然合成数据已被证明可以有效改善文本领域的科学推理，但多模态推理仍然受到合成科学严谨图像的难度的限制。现有的文本到图像（T2I）模型通常会产生视觉上合理但科学上不正确的输出，从而导致持续的视觉逻辑分歧，限制了它们在下游推理中的价值。受下一代 T2I 模型最新进展的推动，我们对跨代范式、评估和下游使用的科学图像合成进行了系统研究。我们分析了基于像素的直接生成和编程合成，并提出了 ImgCoder，这是一种逻辑驱动框架，遵循明确的“理解 - 计划 - 编码”工作流程以提高结构精度。为了严格评估科学正确性，我们引入了 SciGenBench，它根据信息效用和逻辑有效性来评估生成的图像。我们的评估揭示了基于像素的模型中的系统故障模式，并强调了基本的表现力与精度的权衡。最后，我们表明，在经过严格验证的合成科学图像上微调大型多模态模型（LMM）可以产生一致的推理增益，具有类似于文本域的潜在缩放趋势，验证高保真科学合成是解锁大规模多模态推理能力的可行途径。</li>
</ul>

<h3>Title: Data-Efficient Meningioma Segmentation via Implicit Spatiotemporal Mixing and Sim2Real Semantic Injection</h3>
<ul>
<li><strong>Authors: </strong>Yunhao Xu, Fuquan Zong, Yexuan Xing, Chulong Zhang, Guang Yang, Shilong Yang, Xiaokun Liang, Juan Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17031">https://arxiv.org/abs/2601.17031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17031">https://arxiv.org/pdf/2601.17031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17031]] Data-Efficient Meningioma Segmentation via Implicit Spatiotemporal Mixing and Sim2Real Semantic Injection(https://arxiv.org/abs/2601.17031)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The performance of medical image segmentation is increasingly defined by the efficiency of data utilization rather than merely the volume of raw data. Accurate segmentation, particularly for complex pathologies like meningiomas, demands that models fully exploit the latent information within limited high-quality annotations. To maximize the value of existing datasets, we propose a novel dual-augmentation framework that synergistically integrates spatial manifold expansion and semantic object injection. Specifically, we leverage Implicit Neural Representations (INR) to model continuous velocity fields. Unlike previous methods, we perform linear mixing on the integrated deformation fields, enabling the efficient generation of anatomically plausible variations by interpolating within the deformation space. This approach allows for the extensive exploration of structural diversity from a small set of anchors. Furthermore, we introduce a Sim2Real lesion injection module. This module constructs a high-fidelity simulation domain by transplanting lesion textures into healthy anatomical backgrounds, effectively bridging the gap between synthetic augmentation and real-world pathology. Comprehensive experiments on a hybrid dataset demonstrate that our framework significantly enhances the data efficiency and robustness of state-of-the-art models, including nnU-Net and U-Mamba, offering a potent strategy for high-performance medical image analysis with limited annotation budgets.</li>
<li><strong>摘要：</strong>医学图像分割的性能越来越多地由数据利用效率而不仅仅是原始数据量来定义。准确的分割，特别是对于脑膜瘤等复杂的病理学，要求模型充分利用有限的高质量注释中的潜在信息。为了最大化现有数据集的价值，我们提出了一种新颖的双重增强框架，该框架协同集成了空间流形扩展和语义对象注入。具体来说，我们利用隐式神经表示（INR）来模拟连续速度场。与以前的方法不同，我们对集成变形场进行线性混合，通过在变形空间内插值来有效生成解剖学上合理的变化。这种方法允许从一小组锚点中广泛探索结构多样性。此外，我们引入了 Sim2Real 病变注入模块。该模块通过将病变纹理移植到健康的解剖背景中来构建高保真模拟域，有效地弥合了合成增强与现实世界病理学之间的差距。对混合数据集的综合实验表明，我们的框架显着提高了最先进模型（包括 nnU-Net 和 U-Mamba）的数据效率和稳健性，为在注释预算有限的情况下进行高性能医学图像分析提供了有效的策略。</li>
</ul>

<h3>Title: AMVICC: A Novel Benchmark for Cross-Modal Failure Mode Profiling for VLMs and IGMs</h3>
<ul>
<li><strong>Authors: </strong>Aahana Basappa, Pranay Goel, Anusri Karra, Anish Karra, Asa Gilmore, Kevin Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17037">https://arxiv.org/abs/2601.17037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17037">https://arxiv.org/pdf/2601.17037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17037]] AMVICC: A Novel Benchmark for Cross-Modal Failure Mode Profiling for VLMs and IGMs(https://arxiv.org/abs/2601.17037)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We investigated visual reasoning limitations of both multimodal large language models (MLLMs) and image generation models (IGMs) by creating a novel benchmark to systematically compare failure modes across image-to-text and text-to-image tasks, enabling cross-modal evaluation of visual understanding. Despite rapid growth in machine learning, vision language models (VLMs) still fail to understand or generate basic visual concepts such as object orientation, quantity, or spatial relationships, which highlighted gaps in elementary visual reasoning. By adapting MMVP benchmark questions into explicit and implicit prompts, we create \textit{AMVICC}, a novel benchmark for profiling failure modes across various modalities. After testing 11 MLLMs and 3 IGMs in nine categories of visual reasoning, our results show that failure modes are often shared between models and modalities, but certain failures are model-specific and modality-specific, and this can potentially be attributed to various factors. IGMs consistently struggled to manipulate specific visual components in response to prompts, especially in explicit prompts, suggesting poor control over fine-grained visual attributes. Our findings apply most directly to the evaluation of existing state-of-the-art models on structured visual reasoning tasks. This work lays the foundation for future cross-modal alignment studies, offering a framework to probe whether generation and interpretation failures stem from shared limitations to guide future improvements in unified vision-language modeling.</li>
<li><strong>摘要：</strong>我们通过创建一个新颖的基准来系统地比较图像到文本和文本到图像任务的失败模式，从而研究多模态大语言模型（MLLM）和图像生成模型（IGM）的视觉推理局限性，从而实现视觉理解的跨模态评估。尽管机器学习快速发展，视觉语言模型（VLM）仍然无法理解或生成基本的视觉概念，例如对象方向、数量或空间关系，这凸显了基本视觉推理的差距。通过将 MMVP 基准问题调整为显式和隐式提示，我们创建了 \textit{AMVICC}，这是一种跨各种模式分析故障模式的新颖基准。在对九个视觉推理类别中的 11 个 MLLM 和 3 个 IGM 进行测试后，我们的结果表明，故障模式通常在模型和模态之间共享，但某些故障是特定于模型和特定于模态的，这可能归因于各种因素。 IGM 始终难以操纵特定的视觉组件来响应提示，尤其是在明确的提示中，这表明对细粒度视觉属性的控制很差。我们的研究结果最直接地适用于对结构化视觉推理任务上现有最先进模型的评估。这项工作为未来的跨模式对齐研究奠定了基础，提供了一个框架来探讨生成和解释失败是否源于共同的限制，以指导统一视觉语言建模的未来改进。</li>
</ul>

<h3>Title: A Mechanistic View on Video Generation as World Models: State and Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Luozhou Wang, Zhifei Chen, Yihua Du, Dongyu Yan, Wenhang Ge, Guibao Shen, Xinli Xu, Leyi Wu, Man Chen, Tianshuo Xu, Peiran Ren, Xin Tao, Pengfei Wan, Ying-Cong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17067">https://arxiv.org/abs/2601.17067</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17067">https://arxiv.org/pdf/2601.17067</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17067]] A Mechanistic View on Video Generation as World Models: State and Dynamics(https://arxiv.org/abs/2601.17067)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large-scale video generation models have demonstrated emergent physical coherence, positioning them as potential world models. However, a gap remains between contemporary "stateless" video architectures and classic state-centric world model theories. This work bridges this gap by proposing a novel taxonomy centered on two pillars: State Construction and Dynamics Modeling. We categorize state construction into implicit paradigms (context management) and explicit paradigms (latent compression), while dynamics modeling is analyzed through knowledge integration and architectural reformulation. Furthermore, we advocate for a transition in evaluation from visual fidelity to functional benchmarks, testing physical persistence and causal reasoning. We conclude by identifying two critical frontiers: enhancing persistence via data-driven memory and compressed fidelity, and advancing causality through latent factor decoupling and reasoning-prior integration. By addressing these challenges, the field can evolve from generating visually plausible videos to building robust, general-purpose world simulators.</li>
<li><strong>摘要：</strong>大规模视频生成模型已经表现出新兴的物理一致性，将它们定位为潜在的世界模型。然而，当代“无状态”视频架构与经典的以状态为中心的世界模型理论之间仍然存在差距。这项工作通过提出一种以两个支柱为中心的新颖分类法来弥补这一差距：国家构建和动态建模。我们将状态构建分为隐式范式（上下文管理）和显式范式（潜在压缩），而动态建模则通过知识集成和架构重构进行分析。此外，我们主张评估从视觉保真度过渡到功能基准，测试物理持久性和因果推理。最后，我们确定了两个关键前沿：通过数据驱动的内存和压缩保真度来增强持久性，以及通过潜在因素解耦和推理先验集成来推进因果关系。通过解决这些挑战，该领域可以从生成视觉上合理的视频发展到构建强大的通用世界模拟器。</li>
</ul>

<h3>Title: GlassesGB: Controllable 2D GAN-Based Eyewear Personalization for 3D Gaussian Blendshapes Head Avatars</h3>
<ul>
<li><strong>Authors: </strong>Rui-Yang Ju, Jen-Shiun Chiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17088">https://arxiv.org/abs/2601.17088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17088">https://arxiv.org/pdf/2601.17088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17088]] GlassesGB: Controllable 2D GAN-Based Eyewear Personalization for 3D Gaussian Blendshapes Head Avatars(https://arxiv.org/abs/2601.17088)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Virtual try-on systems allow users to interactively try different products within VR scenarios. However, most existing VTON methods operate only on predefined eyewear templates and lack support for fine-grained, user-driven customization. While GlassesGAN enables personalized 2D eyewear design, its capability remains limited to 2D image generation. Motivated by the success of 3D Gaussian Blendshapes in head reconstruction, we integrate these two techniques and propose GlassesGB, a framework that supports customizable eyewear generation for 3D head avatars. GlassesGB effectively bridges 2D generative customization with 3D head avatar rendering, addressing the challenge in achieving personalized eyewear design for VR applications. The implementation code is available at this https URL.</li>
<li><strong>摘要：</strong>虚拟试穿系统允许用户在 VR 场景中交互式地尝试不同的产品。然而，大多数现有的 VTON 方法仅在预定义的眼镜模板上运行，缺乏对细粒度、用户驱动的定制的支持。虽然 GlassesGAN 可实现个性化 2D 眼镜设计，但其功能仍然仅限于 2D 图像生成。受到 3D Gaussian Blendshapes 在头部重建中成功的推动，我们集成了这两种技术并提出了 GlassesGB，这是一个支持 3D 头部头像可定制眼镜生成的框架。 GlassesGB 有效地将 2D 生成定制与 3D 头部头像渲染联系起来，解决了为 VR 应用实现个性化眼镜设计的挑战。实现代码可在此 https URL 获取。</li>
</ul>

<h3>Title: Boltzmann-GPT: Bridging Energy-Based World Models and Language Generation</h3>
<ul>
<li><strong>Authors: </strong>Junichiro Niimi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17094">https://arxiv.org/abs/2601.17094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17094">https://arxiv.org/pdf/2601.17094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17094]] Boltzmann-GPT: Bridging Energy-Based World Models and Language Generation(https://arxiv.org/abs/2601.17094)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) generate fluent text, yet whether they truly understand the world or merely produce plausible language about it remains contested. We propose an architectural principle, the mouth is not the brain, that explicitly separates world models from language models. Our architecture comprises three components: a Deep Boltzmann Machine (DBM) that captures domain structure as an energy-based world model, an adapter that projects latent belief states into embedding space, and a frozen GPT-2 that provides linguistic competence without domain knowledge. We instantiate this framework in the consumer review domain using Amazon smartphone reviews. Experiments demonstrate that (1) conditioning through the world model yields significantly higher sentiment correlation, lower perplexity, and greater semantic similarity compared to prompt-based generation alone; (2) the DBM's energy function distinguishes coherent from incoherent market configurations, assigning higher energy to implausible brand-price combinations; and (3) interventions on specific attributes propagate causally to generated text with intervened outputs exhibiting distributions statistically consistent with naturally occurring samples sharing the target configuration. These findings suggest that even small-scale language models can achieve consistent, controllable generation when connected to an appropriate world model, providing empirical support for separating linguistic competence from world understanding.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）生成流畅的文本，但它们是否真正理解世界或仅仅生成关于世界的合理语言仍然存在争议。我们提出了一个架构原则，“嘴不是大脑”，明确地将世界模型与语言模型分开。我们的架构由三个组件组成：深度玻尔兹曼机（DBM），它捕获领域结构作为基于能量的世界模型；适配器，将潜在信念状态投射到嵌入空间；以及冻结的 GPT-2，在没有领域知识的情况下提供语言能力。我们使用亚马逊智能手机评论在消费者评论领域实例化该框架。实验表明，（1）与单独基于提示的生成相比，通过世界模型进行调节可产生显着更高的情感相关性、更低的困惑度和更大的语义相似性； (2) DBM的能量函数区分一致和不一致的市场配置，将更高的能量分配给不合理的品牌价格组合； (3) 对特定属性的干预因果性地传播到生成的文本，干预输出的分布在统计上与共享目标配置的自然发生的样本一致。这些发现表明，即使是小规模的语言模型，当连接到适当的世界模型时，也可以实现一致、可控的生成，为将语言能力与世界理解分开提供经验支持。</li>
</ul>

<h3>Title: LoD Sketch Extraction from Architectural Models Using Generative AI: Dataset Construction for Multi-Level Architectural Design Generation</h3>
<ul>
<li><strong>Authors: </strong>Xusheng Du, Athiwat Kongkaeo, Ye Zhang, Haoran Xie</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17095">https://arxiv.org/abs/2601.17095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17095">https://arxiv.org/pdf/2601.17095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17095]] LoD Sketch Extraction from Architectural Models Using Generative AI: Dataset Construction for Multi-Level Architectural Design Generation(https://arxiv.org/abs/2601.17095)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>For architectural design, representation across multiple Levels of Details (LoD) is essential for achieving a smooth transition from conceptual massing to detailed modeling. However, traditional LoD modeling processes rely on manual operations that are time-consuming, labor-intensive, and prone to geometric inconsistencies. While the rapid advancement of generative artificial intelligence (AI) has opened new possibilities for generating multi-level architectural models from sketch inputs, its application remains limited by the lack of high-quality paired LoD training data. To address this issue, we propose an automatic LoD sketch extraction framework using generative AI models, which progressively simplifies high-detail architectural models to automatically generate geometrically consistent and hierarchically coherent multi-LoD representations. The proposed framework integrates computer vision techniques with generative AI methods to establish a progressive extraction pipeline that transitions from detailed representations to volumetric abstractions. Experimental results demonstrate that the method maintains strong geometric consistency across LoD levels, achieving SSIM values of 0.7319 and 0.7532 for the transitions from LoD3 to LoD2 and from LoD2 to LoD1, respectively, with corresponding normalized Hausdorff distances of 25.1% and 61.0% of the image diagonal, reflecting controlled geometric deviation during abstraction. These results verify that the proposed framework effectively preserves global structure while achieving progressive semantic simplification across different LoD levels, providing reliable data and technical support for AI-driven multi-level architectural generation and hierarchical modeling.</li>
<li><strong>摘要：</strong>对于建筑设计来说，跨多个细节层次 (LoD) 的表示对于实现从概念体量到详细建模的平滑过渡至关重要。然而，传统的LoD建模过程依赖于手动操作，耗时、耗力，并且容易出现几何不一致。虽然生成人工智能 (AI) 的快速发展为从草图输入生成多层建筑模型提供了新的可能性，但其应用仍然因缺乏高质量的配对 LoD 训练数据而受到限制。为了解决这个问题，我们提出了一种使用生成式 AI 模型的自动 LoD 草图提取框架，该框架逐步简化高细节建筑模型，以自动生成几何一致和层次一致的多 LoD 表示。所提出的框架将计算机视觉技术与生成式人工智能方法相结合，以建立一个渐进式提取管道，从详细表示过渡到体积抽象。实验结果表明，该方法在LoD级别上保持了很强的几何一致性，从LoD3到LoD2和从LoD2到LoD1的过渡的SSIM值分别为0.7319和0.7532，相应的归一化Hausdorff距离为图像对角线的25.1%和61.0%，反映了抽象过程中受控的几何偏差。这些结果验证了所提出的框架有效地保留了全局结构，同时实现了不同LoD级别的渐进语义简化，为AI驱动的多级架构生成和分层建模提供了可靠的数据和技术支持。</li>
</ul>

<h3>Title: StealthMark: Harmless and Stealthy Ownership Verification for Medical Segmentation via Uncertainty-Guided Backdoors</h3>
<ul>
<li><strong>Authors: </strong>Qinkai Yu, Chong Zhang, Gaojie Jin, Tianjin Huang, Wei Zhou, Wenhui Li, Xiaobo Jin, Bo Huang, Yitian Zhao, Guang Yang, Gregory Y.H. Lip, Yalin Zheng, Aline Villavicencio, Yanda Meng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17107">https://arxiv.org/abs/2601.17107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17107">https://arxiv.org/pdf/2601.17107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17107]] StealthMark: Harmless and Stealthy Ownership Verification for Medical Segmentation via Uncertainty-Guided Backdoors(https://arxiv.org/abs/2601.17107)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Annotating medical data for training AI models is often costly and limited due to the shortage of specialists with relevant clinical expertise. This challenge is further compounded by privacy and ethical concerns associated with sensitive patient information. As a result, well-trained medical segmentation models on private datasets constitute valuable intellectual property requiring robust protection mechanisms. Existing model protection techniques primarily focus on classification and generative tasks, while segmentation models-crucial to medical image analysis-remain largely underexplored. In this paper, we propose a novel, stealthy, and harmless method, StealthMark, for verifying the ownership of medical segmentation models under black-box conditions. Our approach subtly modulates model uncertainty without altering the final segmentation outputs, thereby preserving the model's performance. To enable ownership verification, we incorporate model-agnostic explanation methods, e.g. LIME, to extract feature attributions from the model outputs. Under specific triggering conditions, these explanations reveal a distinct and verifiable watermark. We further design the watermark as a QR code to facilitate robust and recognizable ownership claims. We conducted extensive experiments across four medical imaging datasets and five mainstream segmentation models. The results demonstrate the effectiveness, stealthiness, and harmlessness of our method on the original model's segmentation performance. For example, when applied to the SAM model, StealthMark consistently achieved ASR above 95% across various datasets while maintaining less than a 1% drop in Dice and AUC scores, significantly outperforming backdoor-based watermarking methods and highlighting its strong potential for practical deployment. Our implementation code is made available at: this https URL.</li>
<li><strong>摘要：</strong>由于缺乏具有相关临床专业知识的专家，注释用于训练人工智能模型的医疗数据通常成本高昂且受到限制。与敏感患者信息相关的隐私和道德问题进一步加剧了这一挑战。因此，私人数据集上训练有素的医疗分割模型构成了宝贵的知识产权，需要强大的保护机制。现有的模型保护技术主要侧重于分类和生成任务，而对医学图像分析至关重要的分割模型在很大程度上仍未得到充分探索。在本文中，我们提出了一种新颖、隐秘且无害的方法 StealthMark，用于在黑盒条件下验证医学分割模型的所有权。我们的方法巧妙地调节模型的不确定性，而不改变最终的分割输出，从而保持模型的性能。为了实现所有权验证，我们采用了与模型无关的解释方法，例如LIME，从模型输出中提取特征属性。在特定的触发条件下，这些解释揭示了独特且可验证的水印。我们进一步将水印设计为二维码，以促进稳健且可识别的所有权声明。我们对四个医学成像数据集和五个主流分割模型进行了广泛的实验。结果证明了我们的方法对原始模型分割性能的有效性、隐蔽性和无害性。例如，当应用于 SAM 模型时，StealthMark 在各种数据集上始终实现了 95% 以上的 ASR，同时保持 Dice 和 AUC 分数下降不到 1%，显着优于基于后门的水印方法，并凸显了其实际部署的强大潜力。我们的实现代码可在以下位置获取：此 https URL。</li>
</ul>

<h3>Title: iFSQ: Improving FSQ for Image Generation with 1 Line of Code</h3>
<ul>
<li><strong>Authors: </strong>Bin Lin, Zongjian Li, Yuwei Niu, Kaixiong Gong, Yunyang Ge, Yunlong Lin, Mingzhe Zheng, JianWei Zhang, Miles Yang, Zhao Zhong, Liefeng Bo, Li Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17124">https://arxiv.org/abs/2601.17124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17124">https://arxiv.org/pdf/2601.17124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17124]] iFSQ: Improving FSQ for Image Generation with 1 Line of Code(https://arxiv.org/abs/2601.17124)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The field of image generation is currently bifurcated into autoregressive (AR) models operating on discrete tokens and diffusion models utilizing continuous latents. This divide, rooted in the distinction between VQ-VAEs and VAEs, hinders unified modeling and fair benchmarking. Finite Scalar Quantization (FSQ) offers a theoretical bridge, yet vanilla FSQ suffers from a critical flaw: its equal-interval quantization can cause activation collapse. This mismatch forces a trade-off between reconstruction fidelity and information efficiency. In this work, we resolve this dilemma by simply replacing the activation function in original FSQ with a distribution-matching mapping to enforce a uniform prior. Termed iFSQ, this simple strategy requires just one line of code yet mathematically guarantees both optimal bin utilization and reconstruction precision. Leveraging iFSQ as a controlled benchmark, we uncover two key insights: (1) The optimal equilibrium between discrete and continuous representations lies at approximately 4 bits per dimension. (2) Under identical reconstruction constraints, AR models exhibit rapid initial convergence, whereas diffusion models achieve a superior performance ceiling, suggesting that strict sequential ordering may limit the upper bounds of generation quality. Finally, we extend our analysis by adapting Representation Alignment (REPA) to AR models, yielding LlamaGen-REPA. Codes is available at this https URL</li>
<li><strong>摘要：</strong>图像生成领域目前分为在离散标记上运行的自回归（AR）模型和利用连续潜伏的扩散模型。这种分歧源于 VQ-VAE 和 VAE 之间的区别，阻碍了统一建模和公平基准测试。有限标量量化 (FSQ) 提供了一个理论桥梁，但普通 FSQ 存在一个严重缺陷：其等间隔量化可能导致激活崩溃。这种不匹配迫使重建保真度和信息效率之间进行权衡。在这项工作中，我们通过简单地用分布匹配映射替换原始 FSQ 中的激活函数来解决这个困境，以强制执行统一的先验。这种简单的策略称为 iFSQ，仅需要一行代码，但在数学上保证了最佳的 bin 利用率和重建精度。利用 iFSQ 作为受控基准，我们发现了两个关键见解：(1) 离散表示和连续表示之间的最佳平衡在于每个维度大约 4 位。 （2）在相同的重建约束下，AR模型表现出快速的初始收敛，而扩散模型实现了优越的性能上限，这表明严格的顺序排序可能会限制生成质量的上限。最后，我们通过将表示对齐 (REPA) 应用于 AR 模型来扩展我们的分析，产生 LlamaGen-REPA。代码可在此 https URL 获取</li>
</ul>

<h3>Title: Learning to Collaborate: An Orchestrated-Decentralized Framework for Peer-to-Peer LLM Federation</h3>
<ul>
<li><strong>Authors: </strong>Inderjeet Singh, Eleonore Vissol-Gaudin, Andikan Otung, Motoyoshi Sekiya</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.DC, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17133">https://arxiv.org/abs/2601.17133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17133">https://arxiv.org/pdf/2601.17133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17133]] Learning to Collaborate: An Orchestrated-Decentralized Framework for Peer-to-Peer LLM Federation(https://arxiv.org/abs/2601.17133)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Fine-tuning Large Language Models (LLMs) for specialized domains is constrained by a fundamental challenge: the need for diverse, cross-organizational data conflicts with the principles of data privacy and sovereignty. While Federated Learning (FL) provides a framework for collaboration without raw data exchange, its classic centralized form introduces a single point of failure and remains vulnerable to model inversion attacks. Decentralized FL (DFL) mitigates this risk by removing the central aggregator but typically relies on inefficient, random peer-to-peer (P2P) pairings, forming a collaboration graph that is blind to agent heterogeneity and risks negative transfer. This paper introduces KNEXA-FL, a novel framework for orchestrated decentralization that resolves this trade-off. KNEXA-FL employs a non-aggregating Central Profiler/Matchmaker (CPM) that formulates P2P collaboration as a contextual bandit problem, using a LinUCB algorithm on abstract agent profiles to learn an optimal matchmaking policy. It orchestrates direct knowledge exchange between heterogeneous, PEFT-based LLM agents via secure distillation, without ever accessing the models themselves. Our comprehensive experiments on a challenging code generation task show that KNEXA-FL yields substantial gains, improving Pass@1 by approx. 50% relative to random P2P collaboration. Critically, our orchestrated approach demonstrates stable convergence, in stark contrast to a powerful centralized distillation baseline which suffers from catastrophic performance collapse. Our work establishes adaptive, learning-based orchestration as a foundational principle for building robust and effective decentralized AI ecosystems.</li>
<li><strong>摘要：</strong>针对专业领域的大型语言模型 (LLM) 的微调受到一个根本挑战的限制：对多样化、跨组织数据的需求与数据隐私和主权原则相冲突。虽然联邦学习（FL）提供了一个无需原始数据交换的协作框架，但其经典的集中式形式引入了单点故障，并且仍然容易受到模型反转攻击。去中心化 FL (DFL) 通过消除中央聚合器来减轻这种风险，但通常依赖于低效、随机的点对点 (P2P) 配对，形成一个对代理异质性视而不见的协作图，并存在负转移风险。本文介绍了 KNEXA-FL，这是一种精心策划的去中心化框架，可以解决这种权衡问题。 KNEXA-FL 采用非聚合中央分析器/匹配器 (CPM)，将 P2P 协作制定为上下文强盗问题，在抽象代理配置文件上使用 LinUCB 算法来学习最佳匹配策略。它通过安全蒸馏协调基于 PEFT 的异构 LLM 代理之间的直接知识交换，而无需访问模型本身。我们对具有挑战性的代码生成任务的综合实验表明，KNEXA-FL 产生了巨大的收益，将 Pass@1 提高了约 10%。 50% 相对于随机 P2P 协作。至关重要的是，我们精心策划的方法展示了稳定的收敛性，这与遭受灾难性性能崩溃的强大的集中式蒸馏基线形成鲜明对比。我们的工作将自适应的、基于学习的编排作为构建强大且有效的去中心化人工智能生态系统的基本原则。</li>
</ul>

<h3>Title: Scaling medical imaging report generation with multimodal reinforcement learning</h3>
<ul>
<li><strong>Authors: </strong>Qianchu Liu, Sheng Zhang, Guanghui Qin, Yu Gu, Ying Jin, Sam Preston, Yanbo Xu, Sid Kiblawi, Wen-wai Yim, Tim Ossowski, Tristan Naumann, Mu Wei, Hoifung Poon</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17151">https://arxiv.org/abs/2601.17151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17151">https://arxiv.org/pdf/2601.17151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17151]] Scaling medical imaging report generation with multimodal reinforcement learning(https://arxiv.org/abs/2601.17151)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Frontier models have demonstrated remarkable capabilities in understanding and reasoning with natural-language text, but they still exhibit major competency gaps in multimodal understanding and reasoning especially in high-value verticals such as biomedicine. Medical imaging report generation is a prominent example. Supervised fine-tuning can substantially improve performance, but they are prone to overfitting to superficial boilerplate patterns. In this paper, we introduce Universal Report Generation (UniRG) as a general framework for medical imaging report generation. By leveraging reinforcement learning as a unifying mechanism to directly optimize for evaluation metrics designed for end applications, UniRG can significantly improve upon supervised fine-tuning and attain durable generalization across diverse institutions and clinical practices. We trained UniRG-CXR on publicly available chest X-ray (CXR) data and conducted a thorough evaluation in CXR report generation with rigorous evaluation scenarios. On the authoritative ReXrank benchmark, UniRG-CXR sets new overall SOTA, outperforming prior state of the art by a wide margin.</li>
<li><strong>摘要：</strong>前沿模型在自然语言文本的理解和推理方面表现出了卓越的能力，但它们在多模态理解和推理方面仍然表现出重大的能力差距，特别是在生物医学等高价值垂直领域。医学成像报告生成就是一个突出的例子。有监督的微调可以显着提高性能，但它们很容易过度拟合表面的样板模式。在本文中，我们介绍通用报告生成（UniRG）作为医学成像报告生成的通用框架。通过利用强化学习作为统一机制来直接优化为最终应用设计的评估指标，UniRG 可以显着改进监督微调，并在不同机构和临床实践中实现持久的泛化。我们使用公开的胸部 X 射线 (CXR) 数据对 UniRG-CXR 进行了训练，并通过严格的评估场景对 CXR 报告生成进行了全面评估。在权威的 ReXrank 基准上，UniRG-CXR 设定了新的整体 SOTA，大幅优于先前的技术水平。</li>
</ul>

<h3>Title: SpecBridge: Bridging Mass Spectrometry and Molecular Representations via Cross-Modal Alignment</h3>
<ul>
<li><strong>Authors: </strong>Yinkai Wang, Yan Zhou Chen, Xiaohui Chen, Li-Ping Liu, Soha Hassoun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17204">https://arxiv.org/abs/2601.17204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17204">https://arxiv.org/pdf/2601.17204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17204]] SpecBridge: Bridging Mass Spectrometry and Molecular Representations via Cross-Modal Alignment(https://arxiv.org/abs/2601.17204)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Small-molecule identification from tandem mass spectrometry (MS/MS) remains a bottleneck in untargeted settings where spectral libraries are incomplete. While deep learning offers a solution, current approaches typically fall into two extremes: explicit generative models that construct molecular graphs atom-by-atom, or joint contrastive models that learn cross-modal subspaces from scratch. We introduce SpecBridge, a novel implicit alignment framework that treats structure identification as a geometric alignment problem. SpecBridge fine-tunes a self-supervised spectral encoder (DreaMS) to project directly into the latent space of a frozen molecular foundation model (ChemBERTa), and then performs retrieval by cosine similarity to a fixed bank of precomputed molecular embeddings. Across MassSpecGym, Spectraverse, and MSnLib benchmarks, SpecBridge improves top-1 retrieval accuracy by roughly 20-25% relative to strong neural baselines, while keeping the number of trainable parameters small. These results suggest that aligning to frozen foundation models is a practical, stable alternative to designing new architectures from scratch. The code for SpecBridge is released at this https URL.</li>
<li><strong>摘要：</strong>在谱库不完整的非目标环境中，串联质谱 (MS/MS) 的小分子鉴定仍然是一个瓶颈。虽然深度学习提供了一种解决方案，但当前的方法通常陷入两个极端：逐个原子构建分子图的显式生成模型，或从头开始学习跨模态子空间的联合对比模型。我们引入了 SpecBridge，一种新颖的隐式对齐框架，它将结构识别视为几何对齐问题。 SpecBridge 对自监督光谱编码器 (DreaMS) 进行微调，将其直接投影到冻结分子基础模型 (ChemBERTa) 的潜在空间中，然后通过与预先计算的固定分子嵌入库的余弦相似度来执行检索。在 MassSpecGym、Spectraverse 和 MSnLib 基准测试中，相对于强神经基线，SpecBridge 将 top-1 检索精度提高了大约 20-25%，同时保持可训练参数数量较少。这些结果表明，与从头开始设计新架构相比，与冻结基础模型保持一致是一种实用、稳定的替代方案。 SpecBridge 的代码在此 https URL 发布。</li>
</ul>

<h3>Title: Spatiotemporal Semantic V2X Framework for Cooperative Collision Prediction</h3>
<ul>
<li><strong>Authors: </strong>Murat Arda Onsu, Poonam Lohan, Burak Kantarci, Aisha Syed, Matthew Andrews, Sean Kennedy</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17216">https://arxiv.org/abs/2601.17216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17216">https://arxiv.org/pdf/2601.17216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17216]] Spatiotemporal Semantic V2X Framework for Cooperative Collision Prediction(https://arxiv.org/abs/2601.17216)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Intelligent Transportation Systems (ITS) demand real-time collision prediction to ensure road safety and reduce accident severity. Conventional approaches rely on transmitting raw video or high-dimensional sensory data from roadside units (RSUs) to vehicles, which is impractical under vehicular communication bandwidth and latency constraints. In this work, we propose a semantic V2X framework in which RSU-mounted cameras generate spatiotemporal semantic embeddings of future frames using the Video Joint Embedding Predictive Architecture (V-JEPA). To evaluate the system, we construct a digital twin of an urban traffic environment enabling the generation of d verse traffic scenarios with both safe and collision events. These embeddings of the future frame, extracted from V-JEPA, capture task-relevant traffic dynamics and are transmitted via V2X links to vehicles, where a lightweight attentive probe and classifier decode them to predict imminent collisions. By transmitting only semantic embeddings instead of raw frames, the proposed system significantly reduces communication overhead while maintaining predictive accuracy. Experimental results demonstrate that the framework with an appropriate processing method achieves a 10% F1-score improvement for collision prediction while reducing transmission requirements by four orders of magnitude compared to raw video. This validates the potential of semantic V2X communication to enable cooperative, real-time collision prediction in ITS.</li>
<li><strong>摘要：</strong>智能交通系统 (ITS) 需要实时碰撞预测，以确保道路安全并降低事故严重程度。传统方法依赖于将原始视频或高维传感数据从路边单元 (RSU) 传输到车辆，这在车辆通信带宽和延迟限制下是不切实际的。在这项工作中，我们提出了一种语义 V2X 框架，其中 RSU 安装的摄像机使用视频联合嵌入预测架构 (V-JEPA) 生成未来帧的时空语义嵌入。为了评估该系统，我们构建了城市交通环境的数字孪生，能够生成包含安全和碰撞事件的不同交通场景。这些未来框架的嵌入从 V-JEPA 中提取，捕获与任务相关的交通动态，并通过 V2X 链路传输到车辆，其中轻量级注意力探测器和分类器对其进行解码以预测即将发生的碰撞。通过仅传输语义嵌入而不是原始帧，所提出的系统显着减少了通信开销，同时保持了预测准确性。实验结果表明，采用适当处理方法的框架可将碰撞预测的 F1 分数提高 10%，同时与原始视频相比，传输要求降低了四个数量级。这验证了语义 V2X 通信在 ITS 中实现协作式实时碰撞预测的潜力。</li>
</ul>

<h3>Title: Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Lianlei Shan, Han Chen, Yixuan Wang, Zhenjie Liu, Wei Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17275">https://arxiv.org/abs/2601.17275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17275">https://arxiv.org/pdf/2601.17275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17275]] Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning(https://arxiv.org/abs/2601.17275)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting'' rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak'' paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs.</li>
<li><strong>摘要：</strong>虽然大型语言模型（LLM）在表面级文本生成方面表现出卓越的性能，但它们在处理复杂的多步骤推理任务时的本质通常仍然是“统计拟合”而不是系统的逻辑演绎。传统的强化学习（RL）试图通过引入“先思考后说话”范式来缓解这一问题。然而，直接在高维离散令牌空间中应用强化学习面临着三个固有的挑战：样本效率低下的推出、高梯度估计方差以及灾难性遗忘的风险。为了从根本上解决这些结构瓶颈，我们提出了 \textbf{DeepLatent Reasoning (DLR)}，一种潜在空间双向对比强化学习框架。该框架将试错成本从昂贵的令牌级全序列生成转移到连续潜在流形。具体来说，我们引入了一个轻量级辅助模型来有效地采样潜在空间内的 $K$ 推理链编码。这些编码通过基于正确性和格式的双重奖励机制进行过滤；只有高价值的潜在轨迹才会被输入到 \textbf{frozen main model} 中进行单通道解码。为了在保持连贯性的同时最大化推理多样性，我们设计了一个对比学习目标，以实现潜在空间内的定向探索。由于主要模型参数在优化过程中保持冻结状态，因此该方法在数学上消除了灾难性遗忘。实验表明，在相当的GPU计算预算下，DLR实现了更稳定的训练收敛，支持更长时限的推理链，并促进推理能力的可持续积累，为法学硕士实现可靠和可扩展的强化学习提供了可行的途径。</li>
</ul>

<h3>Title: PAR: Plausibility-aware Amortized Recourse Generation</h3>
<ul>
<li><strong>Authors: </strong>Anagha Sabu, Vidhya S, Narayanan C Krishnan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17309">https://arxiv.org/abs/2601.17309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17309">https://arxiv.org/pdf/2601.17309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17309]] PAR: Plausibility-aware Amortized Recourse Generation(https://arxiv.org/abs/2601.17309)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Algorithmic recourse aims to recommend actionable changes to a factual's attributes that flip an unfavorable model decision while remaining realistic and feasible. We formulate recourse as a Constrained Maximum A-Posteriori (MAP) inference problem under the accepted-class data distribution seeking counterfactuals with high likelihood while respecting other recourse constraints. We present PAR, an amortized approximate inference procedure that generates highly likely recourses efficiently. Recourse likelihood is estimated directly using tractable probabilistic models that admit exact likelihood evaluation and efficient gradient propagation that is useful during training. The recourse generator is trained with the objective of maximizing the likelihood under the accepted-class distribution while minimizing the likelihood under the denied-class distribution and other losses that encode recourse constraints. Furthermore, PAR includes a neighborhood-based conditioning mechanism to promote recourse generation that is customized to a factual. We validate PAR on widely used algorithmic recourse datasets and demonstrate its efficiency in generating recourses that are valid, similar to the factual, sparse, and highly plausible, yielding superior performance over existing state-of-the-art approaches.</li>
<li><strong>摘要：</strong>算法资源旨在建议对事实属性进行可行的更改，从而翻转不利的模型决策，同时保持现实和可行。我们将追索权表述为在接受类数据分布下的约束最大后验（MAP）推理问题，寻求高可能性的反事实，同时尊重其他追索权约束。我们提出了 PAR，一种摊销近似推理程序，可以有效地生成极有可能的资源。追索似然是使用易于处理的概率模型直接估计的，该模型允许精确的似然评估和在训练期间有用的有效梯度传播。资源生成器的训练目标是最大化接受类分布下的似然性，同时最小化拒绝类分布下的似然性和编码资源约束的其他损失。此外，PAR 还包括基于邻域的调节机制，以促进根据事实定制的资源生成。我们在广泛使用的算法资源数据集上验证 PAR，并证明其在生成有效资源方面的效率，这些资源与事实相似、稀疏且高度合理，与现有最先进的方法相比，具有优越的性能。</li>
</ul>

<h3>Title: ClinNet: Evidential Ordinal Regression with Bilateral Asymmetry and Prototype Memory for Knee Osteoarthritis Grading</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyang Li, Runni Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17315">https://arxiv.org/abs/2601.17315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17315">https://arxiv.org/pdf/2601.17315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17315]] ClinNet: Evidential Ordinal Regression with Bilateral Asymmetry and Prototype Memory for Knee Osteoarthritis Grading(https://arxiv.org/abs/2601.17315)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Knee osteoarthritis (KOA) grading based on radiographic images is a critical yet challenging task due to subtle inter-grade differences, annotation uncertainty, and the inherently ordinal nature of disease progression. Conventional deep learning approaches typically formulate this problem as deterministic multi-class classification, ignoring both the continuous progression of degeneration and the uncertainty in expert annotations. In this work, we propose ClinNet, a novel trustworthy framework that addresses KOA grading as an evidential ordinal regression problem. The proposed method integrates three key components: (1) a Bilateral Asymmetry Encoder (BAE) that explicitly models medial-lateral structural discrepancies; (2) a Diagnostic Memory Bank that maintains class-wise prototypes to stabilize feature representations; and (3) an Evidential Ordinal Head based on the Normal-Inverse-Gamma (NIG) distribution to jointly estimate continuous KL grades and epistemic uncertainty. Extensive experiments demonstrate that ClinNet achieves a Quadratic Weighted Kappa of 0.892 and Accuracy of 0.768, statistically outperforming state-of-the-art baselines (p < 0.001). Crucially, we demonstrate that the model's uncertainty estimates successfully flag out-of-distribution samples and potential misdiagnoses, paving the way for safe clinical deployment.</li>
<li><strong>摘要：</strong>由于级别间细微差异、注释不确定性以及疾病进展的固有顺序性质，基于放射线图像的膝骨关节炎 (KOA) 分级是一项关键但具有挑战性的任务。传统的深度学习方法通​​常将这个问题表述为确定性的多类分类，忽略退化的持续进展和专家注释的不确定性。在这项工作中，我们提出了 ClinNet，这是一种新颖的值得信赖的框架，它将 KOA 分级作为证据序数回归问题来解决。该方法集成了三个关键组件：（1）双边不对称编码器（BAE），明确模拟内侧-外侧结构差异； (2) 诊断内存库，维护分类原型以稳定特征表示； (3) 基于正态逆伽玛 (NIG) 分布的证据序数头，用于联合估计连续 KL 等级和认知不确定性。大量实验表明，ClinNet 的二次加权 Kappa 为 0.892，准确度为 0.768，在统计上优于最先进的基线 (p < 0.001)。至关重要的是，我们证明该模型的不确定性估计成功标记了分布外样本和潜在的误诊，为安全临床部署铺平了道路。</li>
</ul>

<h3>Title: SkyReels-V3 Technique Report</h3>
<ul>
<li><strong>Authors: </strong>Debang Li, Zhengcong Fei, Tuanhui Li, Yikun Dou, Zheng Chen, Jiangping Yang, Mingyuan Fan, Jingtao Xu, Jiahua Wang, Baoxuan Gu, Mingshan Chang, Yuqiang Xie, Binjie Mao, Youqiang Zhang, Nuo Pang, Hao Zhang, Yuzhe Jin, Zhiheng Xu, Dixuan Lin, Guibin Chen, Yahui Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17323">https://arxiv.org/abs/2601.17323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17323">https://arxiv.org/pdf/2601.17323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17323]] SkyReels-V3 Technique Report(https://arxiv.org/abs/2601.17323)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Video generation serves as a cornerstone for building world models, where multimodal contextual inference stands as the defining test of capability. In this end, we present SkyReels-V3, a conditional video generation model, built upon a unified multimodal in-context learning framework with diffusion Transformers. SkyReels-V3 model supports three core generative paradigms within a single architecture: reference images-to-video synthesis, video-to-video extension and audio-guided video generation. (i) reference images-to-video model is designed to produce high-fidelity videos with strong subject identity preservation, temporal coherence, and narrative consistency. To enhance reference adherence and compositional stability, we design a comprehensive data processing pipeline that leverages cross frame pairing, image editing, and semantic rewriting, effectively mitigating copy paste artifacts. During training, an image video hybrid strategy combined with multi-resolution joint optimization is employed to improve generalization and robustness across diverse scenarios. (ii) video extension model integrates spatio-temporal consistency modeling with large-scale video understanding, enabling both seamless single-shot continuation and intelligent multi-shot switching with professional cinematographic patterns. (iii) Talking avatar model supports minute-level audio-conditioned video generation by training first-and-last frame insertion patterns and reconstructing key-frame inference paradigms. On the basis of ensuring visual quality, synchronization of audio and videos has been optimized. Extensive evaluations demonstrate that SkyReels-V3 achieves state-of-the-art or near state-of-the-art performance on key metrics including visual quality, instruction following, and specific aspect metrics, approaching leading closed-source systems. Github: this https URL.</li>
<li><strong>摘要：</strong>视频生成是构建世界模型的基石，其中多模态上下文推理是能力的定义测试。最后，我们提出了 SkyReels-V3，这是一种条件视频生成模型，它建立在带有扩散 Transformer 的统一多模态上下文学习框架之上。 SkyReels-V3 模型在单一架构中支持三种核心生成范例：参考图像到视频合成、视频到视频扩展和音频引导视频生成。 (i) 参考图像到视频模型旨在生成具有强大的主体身份保留、时间连贯性和叙事一致性的高保真视频。为了增强参考依从性和构图稳定性，我们设计了一个全面的数据处理管道，利用跨帧配对、图像编辑和语义重写，有效减少复制粘贴伪影。在训练过程中，采用图像视频混合策略与多分辨率联合优化相结合，以提高跨不同场景的泛化性和鲁棒性。 (ii) 视频扩展模型将时空一致性建模与大规模视频理解相结合，实现无缝单镜头连续和具有专业电影模式的智能多镜头切换。 （iii）会说话的化身模型通过训练首尾帧插入模式和重建关键帧推理范例来支持分钟级音频调节视频生成。在保证视觉质量的基础上，优化了音视频的同步。广泛的评估表明，SkyReels-V3 在视觉质量、指令遵循和特定方面指标等关键指标上实现了最先进或接近最先进的性能，接近领先的闭源系统。 Github：此 https URL。</li>
</ul>

<h3>Title: TEXTS-Diff: TEXTS-Aware Diffusion Model for Real-World Text Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Haodong He, Xin Zhan, Yancheng Bai, Rui Lan, Lei Sun, Xiangxiang Chu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17340">https://arxiv.org/abs/2601.17340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17340">https://arxiv.org/pdf/2601.17340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17340]] TEXTS-Diff: TEXTS-Aware Diffusion Model for Real-World Text Image Super-Resolution(https://arxiv.org/abs/2601.17340)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, super-resolution, generation</a></li>
<li><strong>Abstract: </strong>Real-world text image super-resolution aims to restore overall visual quality and text legibility in images suffering from diverse degradations and text distortions. However, the scarcity of text image data in existing datasets results in poor performance on text regions. In addition, datasets consisting of isolated text samples limit the quality of background reconstruction. To address these limitations, we construct Real-Texts, a large-scale, high-quality dataset collected from real-world images, which covers diverse scenarios and contains natural text instances in both Chinese and English. Additionally, we propose the TEXTS-Aware Diffusion Model (TEXTS-Diff) to achieve high-quality generation in both background and textual regions. This approach leverages abstract concepts to improve the understanding of textual elements within visual scenes and concrete text regions to enhance textual details. It mitigates distortions and hallucination artifacts commonly observed in text regions, while preserving high-quality visual scene fidelity. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple evaluation metrics, exhibiting superior generalization ability and text restoration accuracy in complex scenarios. All the code, model, and dataset will be released.</li>
<li><strong>摘要：</strong>现实世界的文本图像超分辨率旨在恢复遭受各种退化和文本扭曲的图像的整体视觉质量和文本易读性。然而，现有数据集中文本图像数据的稀缺导致文本区域的性能较差。此外，由孤立的文本样本组成的数据集限制了背景重建的质量。为了解决这些限制，我们构建了真实文本，这是一个从真实世界图像收集的大规模、高质量的数据集，它涵盖了不同的场景，并包含中文和英文的自然文本实例。此外，我们提出了文本感知扩散模型（TEXTS-Diff），以实现背景和文本区域的高质量生成。这种方法利用抽象概念来提高对视觉场景中文本元素的理解，并利用具体文本区域来增强文本细节。它减轻了文本区域中常见的扭曲和幻觉伪影，同时保持高质量的视觉场景保真度。大量的实验表明，我们的方法在多个评估指标上实现了最先进的性能，在复杂场景中表现出卓越的泛化能力和文本恢复准确性。所有代码、模型和数据集都将被发布。</li>
</ul>

<h3>Title: NeRF-MIR: Towards High-Quality Restoration of Masked Images with Neural Radiance Fields</h3>
<ul>
<li><strong>Authors: </strong>Xianliang Huang, Zhizhou Zhong, Shuhang Chen, Yi Xu, Juhong Guan, Shuigeng Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17350">https://arxiv.org/abs/2601.17350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17350">https://arxiv.org/pdf/2601.17350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17350]] NeRF-MIR: Towards High-Quality Restoration of Masked Images with Neural Radiance Fields(https://arxiv.org/abs/2601.17350)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Neural Radiance Fields (NeRF) have demonstrated remarkable performance in novel view synthesis. However, there is much improvement room on restoring 3D scenes based on NeRF from corrupted images, which are common in natural scene captures and can significantly impact the effectiveness of NeRF. This paper introduces NeRF-MIR, a novel neural rendering approach specifically proposed for the restoration of masked images, demonstrating the potential of NeRF in this domain. Recognizing that randomly emitting rays to pixels in NeRF may not effectively learn intricate image textures, we propose a \textbf{P}atch-based \textbf{E}ntropy for \textbf{R}ay \textbf{E}mitting (\textbf{PERE}) strategy to distribute emitted rays properly. This enables NeRF-MIR to fuse comprehensive information from images of different views. Additionally, we introduce a \textbf{P}rogressively \textbf{I}terative \textbf{RE}storation (\textbf{PIRE}) mechanism to restore the masked regions in a self-training process. Furthermore, we design a dynamically-weighted loss function that automatically recalibrates the loss weights for masked regions. As existing datasets do not support NeRF-based masked image restoration, we construct three masked datasets to simulate corrupted scenarios. Extensive experiments on real data and constructed datasets demonstrate the superiority of NeRF-MIR over its counterparts in masked image restoration.</li>
<li><strong>摘要：</strong>神经辐射场（NeRF）在新颖的视图合成中表现出了卓越的性能。然而，基于 NeRF 从损坏的图像中恢复 3D 场景还有很大的改进空间，这在自然场景捕获中很常见，并且会显着影响 NeRF 的有效性。本文介绍了 NeRF-MIR，这是一种专门为修复蒙版图像而提出的新型神经渲染方法，展示了 NeRF 在该领域的潜力。认识到在 NeRF 中向像素随机发射射线可能无法有效地学习复杂的图像纹理，我们提出了一种基于 \textbf{P}atch 的 \textbf{E}ntropy，用于 \textbf{R}ay \textbf{E}mitting (\textbf{PERE}) 策略来正确分配发射的射线。这使得 NeRF-MIR 能够融合不同视图图像的综合信息。此外，我们引入了一种 \textbf{P}progressively \textbf{I}terative \textbf{RE}storation (\textbf{PIRE}) 机制来在自训练过程中恢复屏蔽区域。此外，我们设计了一个动态加权损失函数，可以自动重新校准屏蔽区域的损失权重。由于现有数据集不支持基于 NeRF 的蒙版图像恢复，我们构建了三个蒙版数据集来模拟损坏的场景。对真实数据和构建数据集的大量实验证明了 NeRF-MIR 在蒙版图像恢复方面优于同类产品。</li>
</ul>

<h3>Title: Diversified Scaling Inference in Time Series Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Ruijin Hua, Zichuan Liu, Kun Zhang, Yiyuan Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17376">https://arxiv.org/abs/2601.17376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17376">https://arxiv.org/pdf/2601.17376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17376]] Diversified Scaling Inference in Time Series Foundation Models(https://arxiv.org/abs/2601.17376)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The advancement of Time Series Foundation Models (TSFMs) has been driven primarily by large-scale pre-training, but inference-time compute potential remains largely untapped. This work systematically investigates two questions: how do TSFMs behave under standard sampling-based inference scaling, and can controlled sampling diversity enhance performance? We first examine the properties of TSFMs under standard sampling often fail to adhere to scaling laws due to insufficient exploration of the solution space. Building on this, we then delve into diversified inference scaling via tailored time series perturbations to expand the generative distribution's support. We theoretically analyze the diversity-fidelity trade-off and derive a critical sample threshold for diversified sampling to outperform standard sampling. Extensive experiments across various TSFMs and datasets show proper diversified inference scaling yields substantial performance gains without parameter updates, establishing inference design as a critical, compute-efficient dimension of TSFM optimization. As an application, we propose RobustMSE, a rigorous metric to quantify the headroom performance of TSFM under a fixed budget. Overall, our findings clarify these factor interactions, enabling reliable performance via diverse large-scale inference time series in parallel environments without re-training TSFMs.</li>
<li><strong>摘要：</strong>时间序列基础模型 (TSFM) 的进步主要是由大规模预训练推动的，但推理时间计算潜力在很大程度上尚未开发。这项工作系统地研究了两个问题：TSFM 在标准的基于采样的推理缩放下如何表现，以及受控的采样多样性能否提高性能？我们首先检查标准抽样下 TSFM 的属性，由于对解决方案空间的探索不充分，常常无法遵守缩放定律。在此基础上，我们通过定制的时间序列扰动深入研究多样化的推理扩展，以扩大生成分布的支持。我们从理论上分析了多样性与保真度的权衡，并得出了多样化抽样优于标准抽样的关键样本阈值。跨各种 TSFM 和数据集的大量实验表明，适当的多样化推理扩展无需参数更新即可带来显着的性能提升，从而将推理设计确立为 TSFM 优化的关键、计算高效维度。作为一个应用程序，我们提出了 RobustMSE，这是一个严格的指标，用于量化固定预算下 TSFM 的净空性能。总体而言，我们的研究结果阐明了这些因素的相互作用，从而通过并行环境中的各种大规模推理时间序列实现可靠的性能，而无需重新训练 TSFM。</li>
</ul>

<h3>Title: Coronary Artery Segmentation and Vessel-Type Classification in X-Ray Angiography</h3>
<ul>
<li><strong>Authors: </strong>Mehdi Yousefzadeh, Siavash Shirzadeh Barough, Ashkan Fakharifar, Yashar Tayyarazad, Narges Eghbali, Mohaddeseh Mozaffari, Hoda Taeb, Negar Sadat Rafiee Tabatabaee, Parsa Esfahanian, Ghazaleh Sadeghi Gohar, Amineh Safavirad, Saeideh Mazloomzadeh, Ehsan khalilipur, Armin Elahifar, Majid Maleki</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17429">https://arxiv.org/abs/2601.17429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17429">https://arxiv.org/pdf/2601.17429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17429]] Coronary Artery Segmentation and Vessel-Type Classification in X-Ray Angiography(https://arxiv.org/abs/2601.17429)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>X-ray coronary angiography (XCA) is the clinical reference standard for assessing coronary artery disease, yet quantitative analysis is limited by the difficulty of robust vessel segmentation in routine data. Low contrast, motion, foreshortening, overlap, and catheter confounding degrade segmentation and contribute to domain shift across centers. Reliable segmentation, together with vessel-type labeling, enables vessel-specific coronary analytics and downstream measurements that depend on anatomical localization. From 670 cine sequences (407 subjects), we select a best frame near peak opacification using a low-intensity histogram criterion and apply joint super-resolution and enhancement. We benchmark classical Meijering, Frangi, and Sato vesselness filters under per-image oracle tuning, a single global mean setting, and per-image parameter prediction via Support Vector Regression (SVR). Neural baselines include U-Net, FPN, and a Swin Transformer, trained with coronary-only and merged coronary+catheter supervision. A second stage assigns vessel identity (LAD, LCX, RCA). External evaluation uses the public DCA1 cohort. SVR per-image tuning improves Dice over global means for all classical filters (e.g., Frangi: 0.759 vs. 0.741). Among deep models, FPN attains 0.914+/-0.007 Dice (coronary-only), and merged coronary+catheter labels further improve to 0.931+/-0.006. On DCA1 as a strict external test, Dice drops to 0.798 (coronary-only) and 0.814 (merged), while light in-domain fine-tuning recovers to 0.881+/-0.014 and 0.882+/-0.015. Vessel-type labeling achieves 98.5% accuracy (Dice 0.844) for RCA, 95.4% (0.786) for LAD, and 96.2% (0.794) for LCX. Learned per-image tuning strengthens classical pipelines, while high-resolution FPN models and merged-label supervision improve stability and external transfer with modest adaptation.</li>
<li><strong>摘要：</strong>X射线冠状动脉造影（XCA）是评估冠状动脉疾病的临床参考标准，但定量分析由于常规数据中稳健的血管分割困难而受到限制。低对比度、运动、透视缩短、重叠和导管混杂会降低分割效果，并导致跨中心的域转移。可靠的分割与血管类型标记相结合，可以实现血管特定的冠状动脉分析和依赖于解剖定位的下游测量。从 670 个电影序列（407 个受试者）中，我们使用低强度直方图标准选择接近峰值不透明的最佳帧，并应用联合超分辨率和增强。我们在每图像预言调整、单一全局均值设置以及通过支持向量回归 (SVR) 的每图像参数预测下对经典 Meijering、Frangi 和 Sato 血管过滤器进行基准测试。神经基线包括 U-Net、FPN 和 Swin Transformer，仅使用冠状动脉监督和合并冠状动脉+导管监督进行训练。第二阶段分配船舶标识（LAD、LCX、RCA）。外部评估使用公共 DCA1 队列。 SVR 每图像调整提高了 Dice 的所有经典滤波器的全局平均值（例如，Frangi：0.759 与 0.741）。在深度模型中，FPN 达到 0.914+/-0.007 Dice（仅冠状动脉），合并冠状动脉+导管标签进一步提高至 0.931+/-0.006。在 DCA1 作为严格的外部测试时，Dice 下降至 0.798（仅冠状动脉）和 0.814（合并），而轻度域内微调则恢复至 0.881+/-0.014 和 0.882+/-0.015。容器类型标记的 RCA 准确率为 98.5%（Dice 0.844），LAD 为 95.4%（0.786），LCX 为 96.2%（0.794）。学习的每图像调整增强了经典管道，而高分辨率 FPN 模型和合并标签监督通过适度的适应提高了稳定性和外部传输。</li>
</ul>

<h3>Title: SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation</h3>
<ul>
<li><strong>Authors: </strong>Taewan Cho, Taeryang Kim, Andrew Jaeyong Choi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17657">https://arxiv.org/abs/2601.17657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17657">https://arxiv.org/pdf/2601.17657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17657]] SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation(https://arxiv.org/abs/2601.17657)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Contrastive Language-Image Pre-training (CLIP) has accomplished extraordinary success for semantic understanding but inherently struggles to perceive geometric structure. Existing methods attempt to bridge this gap by querying CLIP with textual prompts, a process that is often indirect and inefficient. This paper introduces a fundamentally different approach using a dual-pathway decoder. We present SPACE-CLIP, an architecture that unlocks and interprets latent geometric knowledge directly from a frozen CLIP vision encoder, completely bypassing the text encoder and its associated textual prompts. A semantic pathway interprets high-level features, dynamically conditioned on global context using feature-wise linear modulation (FiLM). In addition, a structural pathway extracts fine-grained spatial details from early layers. These complementary streams are hierarchically fused, enabling a robust synthesis of semantic context and precise geometry. Extensive experiments on the KITTI benchmark show that SPACE-CLIP dramatically outperforms previous CLIP-based methods. Our ablation studies validate that the synergistic fusion of our dual pathways is critical to this success. SPACE-CLIP offers a new, efficient, and architecturally elegant blueprint for repurposing large-scale vision models. The proposed method is not just a standalone depth estimator, but a readily integrable spatial perception module for the next generation of embodied AI systems, such as vision-language-action (VLA) models. Our model is available at this https URL</li>
<li><strong>摘要：</strong>对比语言-图像预训练（CLIP）在语义理解方面取得了非凡的成功，但本质上难以感知几何结构。现有方法试图通过使用文本提示查询 CLIP 来弥补这一差距，但这一过程通常是间接且低效的。本文介绍了一种使用双通道解码器的根本不同的方法。我们提出了 SPACE-CLIP，一种直接从冻结的 CLIP 视觉编码器解锁和解释潜在几何知识的架构，完全绕过文本编码器及其相关的文本提示。语义路径解释高级特征，并使用特征线性调制 (FiLM) 动态调节全局上下文。此外，结构路径从早期层中提取细粒度的空间细节。这些互补的流是分层融合的，能够实现语义上下文和精确几何的稳健综合。 KITTI 基准测试的大量实验表明，SPACE-CLIP 的性能显着优于以前基于 CLIP 的方法。我们的消融研究证实，我们的双重途径的协同融合对于这一成功至关重要。 SPACE-CLIP 为重新利用大型视觉模型提供了一个新的、高效的、结构优雅的蓝图。所提出的方法不仅仅是一个独立的深度估计器，而且是一个易于集成的空间感知模块，适用于下一代具体人工智能系统，例如视觉-语言-动作（VLA）模型。我们的模型可通过此 https URL 获取</li>
</ul>

<h3>Title: Training-Free Text-to-Image Compositional Food Generation via Prompt Grafting</h3>
<ul>
<li><strong>Authors: </strong>Xinyue Pan, Yuhao Chen, Fengqing Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17666">https://arxiv.org/abs/2601.17666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17666">https://arxiv.org/pdf/2601.17666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17666]] Training-Free Text-to-Image Compositional Food Generation via Prompt Grafting(https://arxiv.org/abs/2601.17666)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Real-world meal images often contain multiple food items, making reliable compositional food image generation important for applications such as image-based dietary assessment, where multi-food data augmentation is needed, and recipe visualization. However, modern text-to-image diffusion models struggle to generate accurate multi-food images due to object entanglement, where adjacent foods (e.g., rice and soup) fuse together because many foods do not have clear boundaries. To address this challenge, we introduce Prompt Grafting (PG), a training-free framework that combines explicit spatial cues in text with implicit layout guidance during sampling. PG runs a two-stage process where a layout prompt first establishes distinct regions and the target prompt is grafted once layout formation stabilizes. The framework enables food entanglement control: users can specify which food items should remain separated or be intentionally mixed by editing the arrangement of layouts. Across two food datasets, our method significantly improves the presence of target objects and provides qualitative evidence of controllable separation.</li>
<li><strong>摘要：</strong>现实世界的膳食图像通常包含多种食物，因此可靠的组合食物图像生成对于基于图像的饮食评估（需要多种食物数据增强）和食谱可视化等应用非常重要。然而，由于对象纠缠，现代文本到图像扩散模型很难生成准确的多食物图像，其中相邻食物（例如米饭和汤）融合在一起，因为许多食物没有明确的边界。为了应对这一挑战，我们引入了 Prompt Grafting (PG)，这是一种无需训练的框架，它将文本中的显式空间线索与采样过程中隐式的布局指导相结合。 PG 运行一个两阶段过程，其中布局提示首先建立不同的区域，一旦布局形成稳定，目标提示就会被嫁接。该框架支持食物纠缠控制：用户可以通过编辑布局的排列来指定哪些食物应保持分离或有意混合。在两个食物数据集中，我们的方法显着改善了目标对象的存在，并提供了可控分离的定性证据。</li>
</ul>

<h3>Title: Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing</h3>
<ul>
<li><strong>Authors: </strong>Weiyu Zhang, Yuan Hu, Yong Li, Yu Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17673">https://arxiv.org/abs/2601.17673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17673">https://arxiv.org/pdf/2601.17673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17673]] Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing(https://arxiv.org/abs/2601.17673)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks.</li>
<li><strong>摘要：</strong>统一的遥感多模态模型表现出明显的空间反转诅咒：虽然它们可以准确地识别和描述图像中的对象位置，但它们在文本到图像的生成过程中往往无法忠实地执行相同的空间关系，而这种关系构成了遥感的核心语义信息。受这一观察的启发，我们提出了 Uni-RS，这是第一个专为遥感定制的统一多模态模型，以明确解决理解和生成之间的空间不对称问题。具体来说，我们首先引入显式空间布局规划，将文本指令转换为空间布局规划，将几何规划与视觉合成解耦。然后，我们施加空间感知查询监督，将可学习查询偏向于指令中明确指定的空间关系。最后，我们开发了图像标题空间布局变化，使模型能够进行系统的几何一致的空间变换。跨多个基准的大量实验表明，我们的方法大大提高了文本到图像生成的空间忠实度，同时在图像字幕、视觉基础和 VQA 任务等多模态理解任务上保持强大的性能。</li>
</ul>

<h3>Title: Agentic reinforcement learning empowers next-generation chemical language models for molecular design and synthesis</h3>
<ul>
<li><strong>Authors: </strong>Hao Li, He Cao, Shenyao Peng, Zijing Liu, Bin Feng, Yu Wang, Zhiyuan Yan, Yonghong Tian, Yu Li, Li Yuan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17687">https://arxiv.org/abs/2601.17687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17687">https://arxiv.org/pdf/2601.17687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17687]] Agentic reinforcement learning empowers next-generation chemical language models for molecular design and synthesis(https://arxiv.org/abs/2601.17687)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Language models are revolutionizing the biochemistry domain, assisting scientists in drug design and chemical synthesis with high efficiency. Yet current approaches struggle between small language models prone to hallucination and limited knowledge retention, and large cloud-based language models plagued by privacy risks and high inference costs. To bridge this gap, we introduce ChemCRAFT, a novel framework leveraging agentic reinforcement learning to decouple chemical reasoning from knowledge storage. Instead of forcing the model to memorize vast chemical data, our approach empowers the language model to interact with a sandbox for precise information retrieval. This externalization of knowledge allows a locally deployable small model to achieve superior performance with minimal inference costs. To enable small language models for agent-calling ability, we build an agentic trajectory construction pipeline and a comprehensive chemical-agent sandbox. Based on sandbox interactions, we constructed ChemToolDataset, the first large-scale chemical tool trajectory dataset. Simultaneously, we propose SMILES-GRPO to build a dense chemical reward function, promoting the model's ability to call chemical agents. Evaluations across diverse aspects of drug design show that ChemCRAFT outperforms current cloud-based LLMs in molecular structure analysis, molecular optimization, and synthesis pathway prediction, demonstrating that scientific reasoning is not solely an emergent ability of model scale, but a learnable policy of tool orchestration. This work establishes a cost-effective and privacy-preserving paradigm for AI-aided chemistry, opening new avenues for accelerating molecular discovery with locally deployable agents.</li>
<li><strong>摘要：</strong>语言模型正在彻底改变生物化学领域，帮助科学家高效地进行药物设计和化学合成。然而，当前的方法在容易产生幻觉和有限的知识保留的小型语言模型与受隐私风险和高推理成本困扰的基于云的大型语言模型之间挣扎。为了弥补这一差距，我们引入了 ChemCRAFT，这是一种利用代理强化学习将化学推理与知识存储分离的新颖框架。我们的方法不是强迫模型记住大量的化学数据，而是使语言模型能够与沙箱交互以进行精确的信息检索。这种知识的外化使得可本地部署的小型模型能够以最小的推理成本实现卓越的性能。为了实现小语言模型的代理调用能力，我们构建了代理轨迹构建管道和全面的化学代理沙箱。基于沙箱交互，我们构建了第一个大规模化学工具轨迹数据集ChemToolDataset。同时，我们提出SMILES-GRPO来构建密集的化学奖励函数，提升模型调用化学制剂的能力。对药物设计各个方面的评估表明，ChemCRAFT 在分子结构分析、分子优化和合成途径预测方面优于当前基于云的法学硕士，这表明科学推理不仅是模型规模的新兴能力，而且是工具编排的可学习策略。这项工作为人工智能辅助化学建立了一种经济有效且保护隐私的范例，为利用本地可部署代理加速分子发现开辟了新途径。</li>
</ul>

<h3>Title: StyleDecoupler: Generalizable Artistic Style Disentanglement</h3>
<ul>
<li><strong>Authors: </strong>Zexi Jia, Jinchao Zhang, Jie Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17697">https://arxiv.org/abs/2601.17697</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17697">https://arxiv.org/pdf/2601.17697</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17697]] StyleDecoupler: Generalizable Artistic Style Disentanglement(https://arxiv.org/abs/2601.17697)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Representing artistic style is challenging due to its deep entanglement with semantic content. We propose StyleDecoupler, an information-theoretic framework that leverages a key insight: multi-modal vision models encode both style and content, while uni-modal models suppress style to focus on content-invariant features. By using uni-modal representations as content-only references, we isolate pure style features from multi-modal embeddings through mutual information minimization. StyleDecoupler operates as a plug-and-play module on frozen Vision-Language Models without fine-tuning. We also introduce WeART, a large-scale benchmark of 280K artworks across 152 styles and 1,556 artists. Experiments show state-of-the-art performance on style retrieval across WeART and WikiART, while enabling applications like style relationship mapping and generative model evaluation. We release our method and dataset at this url.</li>
<li><strong>摘要：</strong>由于艺术风格与语义内容的深度纠缠，表现艺术风格具有挑战性。我们提出了 StyleDe Coupler，这是一个利用关键见解的信息理论框架：多模态视觉模型对样式和内容进行编码，而单模态模型则抑制样式以专注于内容不变的特征。通过使用单模态表示作为仅内容参考，我们通过互信息最小化将纯风格特征与多模态嵌入隔离开来。 StyleDe Coupler 在冻结的视觉语言模型上作为即插即用模块运行，无需微调。我们还推出了 WeART，这是一个涵盖 152 种风格、1,556 名艺术家的 28 万件艺术品的大型基准。实验表明，WeART 和 WikiART 的风格检索具有最先进的性能，同时支持风格关系映射和生成模型评估等应用程序。我们在此网址发布我们的方法和数据集。</li>
</ul>

<h3>Title: Implicit Neural Representation-Based Continuous Single Image Super Resolution: An Empirical Study</h3>
<ul>
<li><strong>Authors: </strong>Tayyab Nasir, Daochang Liu, Ajmal Mian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17723">https://arxiv.org/abs/2601.17723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17723">https://arxiv.org/pdf/2601.17723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17723]] Implicit Neural Representation-Based Continuous Single Image Super Resolution: An Empirical Study(https://arxiv.org/abs/2601.17723)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Implicit neural representation (INR) has become the standard approach for arbitrary-scale image super-resolution (ASSR). To date, no empirical study has systematically examined the effectiveness of existing methods, nor investigated the effects of different training recipes, such as scaling laws, objective design, and optimization strategies. A rigorous empirical analysis is essential not only for benchmarking performance and revealing true gains but also for establishing the current state of ASSR, identifying saturation limits, and highlighting promising directions. We fill this gap by comparing existing techniques across diverse settings and presenting aggregated performance results on multiple image quality metrics. We contribute a unified framework and code repository to facilitate reproducible comparisons. Furthermore, we investigate the impact of carefully controlled training configurations on perceptual image quality and examine a new loss function that penalizes intensity variations while preserving edges, textures, and finer details during training. We conclude the following key insights that have been previously overlooked: (1) Recent, more complex INR methods provide only marginal improvements over earlier methods. (2) Model performance is strongly correlated to training configurations, a factor overlooked in prior works. (3) The proposed loss enhances texture fidelity across architectures, emphasizing the role of objective design for targeted perceptual gains. (4) Scaling laws apply to INR-based ASSR, confirming predictable gains with increased model complexity and data diversity.</li>
<li><strong>摘要：</strong>隐式神经表示（INR）已成为任意尺度图像超分辨率（ASSR）的标准方法。迄今为止，还没有实证研究系统地检验现有方法的有效性，也没有调查不同训练方案（例如缩放法则、目标设计和优化策略）的效果。严格的实证分析不仅对于基准性能和揭示真实收益至关重要，而且对于确定 ASSR 的当前状态、确定饱和限制和突出有前途的方向也至关重要。我们通过比较不同设置下的现有技术并呈现多个图像质量指标的汇总性能结果来填补这一空白。我们提供统一的框架和代码存储库，以促进可重复的比较。此外，我们研究了仔细控制的训练配置对感知图像质量的影响，并检查了一种新的损失函数，该函数在训练过程中保留边缘、纹理和更精细的细节的同时惩罚强度变化。我们总结了以下以前被忽视的关键见解：（1）最近更复杂的 INR 方法仅比早期方法提供了微小的改进。 (2) 模型性能与训练配置密切相关，这是先前工作中忽视的一个因素。 (3) 所提出的损失增强了跨架构的纹理保真度，强调了客观设计对于目标感知增益的作用。 (4) 缩放法则适用于基于 INR 的 ASSR，通过增加模型复杂性和数据多样性来确认可预测的增益。</li>
</ul>

<h3>Title: Flatten The Complex: Joint B-Rep Generation via Compositional $k$-Cell Particles</h3>
<ul>
<li><strong>Authors: </strong>Junran Lu, Yuanqi Li, Hengji Li, Jie Guo, Yanwen Guo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17733">https://arxiv.org/abs/2601.17733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17733">https://arxiv.org/pdf/2601.17733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17733]] Flatten The Complex: Joint B-Rep Generation via Compositional $k$-Cell Particles(https://arxiv.org/abs/2601.17733)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Boundary Representation (B-Rep) is the widely adopted standard in Computer-Aided Design (CAD) and manufacturing. However, generative modeling of B-Reps remains a formidable challenge due to their inherent heterogeneity as geometric cell complexes, which entangles topology with geometry across cells of varying orders (i.e., $k$-cells such as vertices, edges, faces). Previous methods typically rely on cascaded sequences to handle this hierarchy, which fails to fully exploit the geometric relationships between cells, such as adjacency and sharing, limiting context awareness and error recovery. To fill this gap, we introduce a novel paradigm that reformulates B-Reps into sets of compositional $k$-cell particles. Our approach encodes each topological entity as a composition of particles, where adjacent cells share identical latents at their interfaces, thereby promoting geometric coupling along shared boundaries. By decoupling the rigid hierarchy, our representation unifies vertices, edges, and faces, enabling the joint generation of topology and geometry with global context awareness. We synthesize these particle sets using a multi-modal flow matching framework to handle unconditional generation as well as precise conditional tasks, such as 3D reconstruction from single-view or point cloud. Furthermore, the explicit and localized nature of our representation naturally extends to downstream tasks like local in-painting and enables the direct synthesis of non-manifold structures (e.g., wireframes). Extensive experiments demonstrate that our method produces high-fidelity CAD models with superior validity and editability compared to state-of-the-art methods.</li>
<li><strong>摘要：</strong>边界表示 (B-Rep) 是计算机辅助设计 (CAD) 和制造领域广泛采用的标准。然而，B-Reps 的生成建模仍然是一个艰巨的挑战，因为它们作为几何单元复合体具有固有的异质性，它将拓扑与跨不同阶单元（即 $k$ 单元，如顶点、边、面）的几何结构纠缠在一起。以前的方法通常依赖级联序列来处理这种层次结构，这无法充分利用单元之间的几何关系，例如邻接和共享，限制了上下文感知和错误恢复。为了填补这一空白，我们引入了一种新的范式，将 B-Reps 重新表述为一组组合的 $k$-cell 粒子。我们的方法将每个拓扑实体编码为粒子的组合，其中相邻细胞在其界面上共享相同的潜伏，从而促进沿着共享边界的几何耦合。通过解耦严格的层次结构，我们的表示统一了顶点、边和面，从而能够联合生成具有全局上下文感知的拓扑和几何图形。我们使用多模态流匹配框架来合成这些粒子集，以处理无条件生成以及精确的条件任务，例如从单视图或点云进行 3D 重建。此外，我们表示的明确和局部性质自然延伸到下游任务，例如局部修复，并能够直接合成非流形结构（例如线框）。大量实验表明，与最先进的方法相比，我们的方法可以生成高保真 CAD 模型，具有卓越的有效性和可编辑性。</li>
</ul>

<h3>Title: The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Chenyu Mu, Xin He, Qu Yang, Wanshun Chen, Jiadi Yao, Huang Liu, Zihao Yi, Bo Zhao, Xingyu Chen, Ruotian Ma, Fanghua Ye, Erkun Yang, Cheng Deng, Zhaopeng Tu, Xiaolong Li, Linus</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17737">https://arxiv.org/abs/2601.17737</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17737">https://arxiv.org/pdf/2601.17737</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17737]] The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation(https://arxiv.org/abs/2601.17737)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.</li>
<li><strong>摘要：</strong>视频生成领域的最新进展已经产生了能够从简单的文本提示合成令人惊叹的视觉内容的模型。然而，这些模型很难从对话等高级概念中生成长篇连贯的叙述，揭示了创意与其电影执行之间的“语义差距”。为了弥补这一差距，我们引入了一种新颖的端到端代理框架，用于对话到电影视频的生成。我们框架的核心是 ScripterAgent，这是一个经过训练的模型，可以将粗略的对话转换为细粒度的、可执行的电影脚本。为了实现这一点，我们构建了 ScriptBench，这是一个具有丰富多模式上下文的新的大规模基准，通过专家指导的管道进行注释。然后生成的脚本指导DirectorAgent，它使用跨场景连续生成策略来编排最先进的视频模型，以确保长期一致性。我们的综合评估采用人工智能驱动的 CriticAgent 和新的视觉脚本对齐 (VSA) 指标，表明我们的框架显着提高了所有测试视频模型的脚本忠实度和时间保真度。此外，我们的分析揭示了当前 SOTA 模型中视觉奇观和严格脚本遵守之间的关键权衡，为自动化电影制作的未来提供了宝贵的见解。</li>
</ul>

<h3>Title: Learning Sewing Patterns via Latent Flow Matching of Implicit Fields</h3>
<ul>
<li><strong>Authors: </strong>Cong Cao, Ren Li, Corentin Dumery, Hao Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17740">https://arxiv.org/abs/2601.17740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17740">https://arxiv.org/pdf/2601.17740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17740]] Learning Sewing Patterns via Latent Flow Matching of Implicit Fields(https://arxiv.org/abs/2601.17740)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Sewing patterns define the structural foundation of garments and are essential for applications such as fashion design, fabrication, and physical simulation. Despite progress in automated pattern generation, accurately modeling sewing patterns remains difficult due to the broad variability in panel geometry and seam arrangements. In this work, we introduce a sewing pattern modeling method based on an implicit representation. We represent each panel using a signed distance field that defines its boundary and an unsigned distance field that identifies seam endpoints, and encode these fields into a continuous latent space that enables differentiable meshing. A latent flow matching model learns distributions over panel combinations in this representation, and a stitching prediction module recovers seam relations from extracted edge segments. This formulation allows accurate modeling and generation of sewing patterns with complex structures. We further show that it can be used to estimate sewing patterns from images with improved accuracy relative to existing approaches, and supports applications such as pattern completion and refitting, providing a practical tool for digital fashion design.</li>
<li><strong>摘要：</strong>缝纫图案定义了服装的结构基础，对于时装设计、制造和物理模拟等应用至关重要。尽管在自动图案生成方面取得了进展，但由于面板几何形状和接缝排列的广泛变化，准确地建模缝纫图案仍然很困难。在这项工作中，我们介绍了一种基于隐式表示的缝纫图案建模方法。我们使用定义其边界的有符号距离场和识别接缝端点的无符号距离场来表示每个面板，并将这些场编码到连续的潜在空间中，以实现可微分的网格划分。潜在流匹配模型学习该表示中面板组合的分布，并且拼接预测模块从提取的边缘片段恢复接缝关系。该公式允许精确建模和生成具有复杂结构的缝纫图案。我们进一步表明，它可用于从图像中估计缝纫图案，相对于现有方法，其准确性更高，并支持图案完成和改装等应用，为数字时装设计提供实用工具。</li>
</ul>

<h3>Title: MV-S2V: Multi-View Subject-Consistent Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Ziyang Song, Xinyu Gong, Bangya Liu, Zelin Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17756">https://arxiv.org/abs/2601.17756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17756">https://arxiv.org/pdf/2601.17756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17756]] MV-S2V: Multi-View Subject-Consistent Video Generation(https://arxiv.org/abs/2601.17756)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Existing Subject-to-Video Generation (S2V) methods have achieved high-fidelity and subject-consistent video generation, yet remain constrained to single-view subject references. This limitation renders the S2V task reducible to an S2I + I2V pipeline, failing to exploit the full potential of video subject control. In this work, we propose and address the challenging Multi-View S2V (MV-S2V) task, which synthesizes videos from multiple reference views to enforce 3D-level subject consistency. Regarding the scarcity of training data, we first develop a synthetic data curation pipeline to generate highly customized synthetic data, complemented by a small-scale real-world captured dataset to boost the training of MV-S2V. Another key issue lies in the potential confusion between cross-subject and cross-view references in conditional generation. To overcome this, we further introduce Temporally Shifted RoPE (TS-RoPE) to distinguish between different subjects and distinct views of the same subject in reference conditioning. Our framework achieves superior 3D subject consistency w.r.t. multi-view reference images and high-quality visual outputs, establishing a new meaningful direction for subject-driven video generation. Our project page is available at <a href="this https URL URL</a></li>
<li><strong>摘要：</strong>现有的主题到视频生成（S2V）方法已经实现了高保真度和主题一致的视频生成，但仍然受限于单视图主题参考。此限制使得 S2V 任务可简化为 S2I + I2V 管道，无法充分发挥视频主题控制的潜力。在这项工作中，我们提出并解决了具有挑战性的多视图 S2V (MV-S2V) 任务，该任务从多个参考视图合成视频以强制 3D 级别的主题一致性。关于训练数据的稀缺性，我们首先开发一个合成数据管理管道来生成高度定制的合成数据，并辅以小规模的真实世界捕获的数据集来促进 MV-S2V 的训练。另一个关键问题在于条件生成中跨主题和跨视图引用之间的潜在混淆。为了克服这个问题，我们进一步引入了时间平移 RoPE (TS-RoPE) 来区分参考调节中的不同主体和同一主体的不同视图。我们的框架实现了卓越的 3D 主题一致性。多视图参考图像和高质量的视觉输出，为主题驱动的视频生成建立了一个新的有意义的方向。我们的项目页面位于 <a href="this https URL URL</a></li>
</ul>

<h3>Title: AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation</h3>
<ul>
<li><strong>Authors: </strong>Dongjie Cheng, Ruifeng Yuan, Yongqi Li, Runyang You, Wenjie Wang, Liqiang Nie, Lei Zhang, Wenjie Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17761">https://arxiv.org/abs/2601.17761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17761">https://arxiv.org/pdf/2601.17761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17761]] AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation(https://arxiv.org/abs/2601.17761)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Real-world perception and interaction are inherently multimodal, encompassing not only language but also vision and speech, which motivates the development of "Omni" MLLMs that support both multimodal inputs and multimodal outputs. While a sequence of omni MLLMs has emerged, most existing systems still rely on additional expert components to achieve multimodal generation, limiting the simplicity of unified training and inference. Autoregressive (AR) modeling, with a single token stream, a single next-token objective, and a single decoder, is an elegant and scalable foundation in the text domain. Motivated by this, we present AR-Omni, a unified any-to-any model in the autoregressive paradigm without any expert decoders. AR-Omni supports autoregressive text and image generation, as well as streaming speech generation, all under a single Transformer decoder. We further address three practical issues in unified AR modeling: modality imbalance via task-aware loss reweighting, visual fidelity via a lightweight token-level perceptual alignment loss for image tokens, and stability-creativity trade-offs via a finite-state decoding mechanism. Empirically, AR-Omni achieves strong quality across three modalities while remaining real-time, achieving a 0.88 real-time factor for speech generation.</li>
<li><strong>摘要：</strong>现实世界的感知和交互本质上是多模态的，不仅包括语言，还包括视觉和语音，这激励了支持多模态输入和多模态输出的“Omni”MLLM的开发。虽然一系列全向 MLLM 已经出现，但大多数现有系统仍然依赖额外的专家组件来实现多模态生成，限制了统一训练和推理的简单性。自回归 (AR) 建模具有单个令牌流、单个下一个令牌目标和单个解码器，是文本领域中优雅且可扩展的基础。受此启发，我们提出了 AR-Omni，这是一种自回归范式中的统一任意对任意模型，无需任何专家解码器。 AR-Omni 支持自回归文本和图像生成，以及流式语音生成，所有这些都在单个 Transformer 解码器下进行。我们进一步解决了统一 A​​R 建模中的三个实际问题：通过任务感知损失重新加权来解决模态不平衡，通过图像令牌的轻量级令牌级感知对齐损失来实现视觉保真度，以及通过有限状态解码机制来进行稳定性与创造力的权衡。根据经验，AR-Omni 在保持实时性的同时，在三种模式中实现了出色的质量，实现了 0.88 的语音生成实时系数。</li>
</ul>

<h3>Title: Agreement-Driven Multi-View 3D Reconstruction for Live Cattle Weight Estimation</h3>
<ul>
<li><strong>Authors: </strong>Rabin Dulal, Wenfeng Jia, Lihong Zheng, Jane Quinn</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17791">https://arxiv.org/abs/2601.17791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17791">https://arxiv.org/pdf/2601.17791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17791]] Agreement-Driven Multi-View 3D Reconstruction for Live Cattle Weight Estimation(https://arxiv.org/abs/2601.17791)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Accurate cattle live weight estimation is vital for livestock management, welfare, and productivity. Traditional methods, such as manual weighing using a walk-over weighing system or proximate measurements using body condition scoring, involve manual handling of stock and can impact productivity from both a stock and economic perspective. To address these issues, this study investigated a cost-effective, non-contact method for live weight calculation in cattle using 3D reconstruction. The proposed pipeline utilized multi-view RGB images with SAM 3D-based agreement-guided fusion, followed by ensemble regression. Our approach generates a single 3D point cloud per animal and compares classical ensemble models with deep learning models under low-data conditions. Results show that SAM 3D with multi-view agreement fusion outperforms other 3D generation methods, while classical ensemble models provide the most consistent performance for practical farm scenarios (R$^2$ = 0.69 $\pm$ 0.10, MAPE = 2.22 $\pm$ 0.56 \%), making this practical for on-farm implementation. These findings demonstrate that improving reconstruction quality is more critical than increasing model complexity for scalable deployment on farms where producing a large volume of 3D data is challenging.</li>
<li><strong>摘要：</strong>准确的牛活重估算对于牲畜管理、福利和生产力至关重要。传统方法，例如使用步入式称重系统的手动称重或使用身体状况评分的近似测量，涉及库存的手动处理，并且从库存和经济角度都会影响生产率。为了解决这些问题，本研究研究了一种使用 3D 重建计算牛活重的经济有效的非接触方法。所提出的流程利用多视图 RGB 图像和基于 SAM 3D 的协议引导融合，然后进行集成回归。我们的方法为每只动物生成单个 3D 点云，并将经典集成模型与低数据条件下的深度学习模型进行比较。结果表明，具有多视图协议融合的 SAM 3D 优于其他 3D 生成方法，而经典集成模型为实际农场场景提供了最一致的性能（R$^2$ = 0.69 $\pm$ 0.10，MAPE = 2.22 $\pm$ 0.56 \%），这使得这对于农场实施来说非常实用。这些发现表明，在农场中进行可扩展部署时，提高重建质量比增加模型复杂性更为重要，因为在农场中生成大量 3D 数据具有挑战性。</li>
</ul>

<h3>Title: VAE-REPA: Variational Autoencoder Representation Alignment for Efficient Diffusion Training</h3>
<ul>
<li><strong>Authors: </strong>Mengmeng Wang, Dengyang Jiang, Liuzhuozheng Li, Yucheng Lin, Guojiang Shen, Xiangjie Kong, Yong Liu, Guang Dai, Jingdong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17830">https://arxiv.org/abs/2601.17830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17830">https://arxiv.org/pdf/2601.17830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17830]] VAE-REPA: Variational Autoencoder Representation Alignment for Efficient Diffusion Training(https://arxiv.org/abs/2601.17830)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Denoising-based diffusion transformers, despite their strong generation performance, suffer from inefficient training convergence. Existing methods addressing this issue, such as REPA (relying on external representation encoders) or SRA (requiring dual-model setups), inevitably incur heavy computational overhead during training due to external dependencies. To tackle these challenges, this paper proposes \textbf{\namex}, a lightweight intrinsic guidance framework for efficient diffusion training. \name leverages off-the-shelf pre-trained Variational Autoencoder (VAE) features: their reconstruction property ensures inherent encoding of visual priors like rich texture details, structural patterns, and basic semantic information. Specifically, \name aligns the intermediate latent features of diffusion transformers with VAE features via a lightweight projection layer, supervised by a feature alignment loss. This design accelerates training without extra representation encoders or dual-model maintenance, resulting in a simple yet effective pipeline. Extensive experiments demonstrate that \name improves both generation quality and training convergence speed compared to vanilla diffusion transformers, matches or outperforms state-of-the-art acceleration methods, and incurs merely 4\% extra GFLOPs with zero additional cost for external guidance models.</li>
<li><strong>摘要：</strong>基于去噪的扩散变压器尽管具有强大的生成性能，但仍面临训练收敛效率低下的问题。解决此问题的现有方法，例如 REPA（依赖于外部表示编码器）或 SRA（需要双模型设置），由于外部依赖性，在训练期间不可避免地会产生大量计算开销。为了应对这些挑战，本文提出了 \textbf{\namex}，一种用于高效扩散训练的轻量级内在指导框架。 \name 利用现成的预训练变分自动编码器（VAE）功能：它们的重建属性确保了视觉先验的固有编码，例如丰富的纹理细节、结构模式和基本语义信息。具体来说，\name 通过轻量级投影层将扩散变换器的中间潜在特征与 VAE 特征对齐，并由特征对齐损失进行监督。这种设计可以加速训练，无需额外的表示编码器或双模型维护，从而形成简单而有效的管道。大量实验表明，与普通扩散变压器相比，\name 提高了生成质量和训练收敛速度，匹配或优于最先进的加速方法，并且仅额外增加 4% 的 GFLOP，外部引导模型的额外成本为零。</li>
</ul>

<h3>Title: Feature-Space Generative Models for One-Shot Class-Incremental Learning</h3>
<ul>
<li><strong>Authors: </strong>Jack Foster, Kirill Paramonov, Mete Ozay, Umberto Michieli</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17905">https://arxiv.org/abs/2601.17905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17905">https://arxiv.org/pdf/2601.17905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17905]] Feature-Space Generative Models for One-Shot Class-Incremental Learning(https://arxiv.org/abs/2601.17905)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Few-shot class-incremental learning (FSCIL) is a paradigm where a model, initially trained on a dataset of base classes, must adapt to an expanding problem space by recognizing novel classes with limited data. We focus on the challenging FSCIL setup where a model receives only a single sample (1-shot) for each novel class and no further training or model alterations are allowed after the base training phase. This makes generalization to novel classes particularly difficult. We propose a novel approach predicated on the hypothesis that base and novel class embeddings have structural similarity. We map the original embedding space into a residual space by subtracting the class prototype (i.e., the average class embedding) of input samples. Then, we leverage generative modeling with VAE or diffusion models to learn the multi-modal distribution of residuals over the base classes, and we use this as a valuable structural prior to improve recognition of novel classes. Our approach, Gen1S, consistently improves novel class recognition over the state of the art across multiple benchmarks and backbone architectures.</li>
<li><strong>摘要：</strong>少样本类增量学习 (FSCIL) 是一种范式，其中最初在基类数据集上训练的模型必须通过识别具有有限数据的新类来适应不断扩展的问题空间。我们专注于具有挑战性的 FSCIL 设置，其中模型仅接收每个新类别的单个样本（1-shot），并且在基础训练阶段之后不允许进一步训练或模型更改。这使得泛化到新类特别困难。我们提出了一种基于基础类嵌入和新类嵌入具有结构相似性的假设的新颖方法。我们通过减去输入样本的类原型（即平均类嵌入）将原始嵌入空间映射到残差空间。然后，我们利用 VAE 或扩散模型的生成模型来学习基类上残差的多模态分布，并将其用作有价值的结构先验，以提高对新类别的识别。我们的方法 Gen1S 在多个基准和主干架构上持续改进新颖的类识别，超越现有技术。</li>
</ul>

<h3>Title: treaming-dLLM: Accelerating Diffusion LLMs via Suffix Pruning and Dynamic Decoding</h3>
<ul>
<li><strong>Authors: </strong>Zhongyu Xiao, Zhiwei Hao, Jianyuan Guo, Yong Luo, Jia Liu, Jie Xu, Han Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17917">https://arxiv.org/abs/2601.17917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17917">https://arxiv.org/pdf/2601.17917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17917]] treaming-dLLM: Accelerating Diffusion LLMs via Suffix Pruning and Dynamic Decoding(https://arxiv.org/abs/2601.17917)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion Large Language Models (dLLMs) offer a compelling paradigm for natural language generation, leveraging parallel decoding and bidirectional attention to achieve superior global coherence compared to autoregressive models. While recent works have accelerated inference via KV cache reuse or heuristic decoding, they overlook the intrinsic inefficiencies within the block-wise diffusion process. Specifically, they suffer from spatial redundancy by modeling informative-sparse suffix regions uniformly and temporal inefficiency by applying fixed denoising schedules across all the decoding process. To address this, we propose Streaming-dLLM, a training-free framework that streamlines inference across both spatial and temporal dimensions. Spatially, we introduce attenuation guided suffix modeling to approximate the full context by pruning redundant mask tokens. Temporally, we employ a dynamic confidence aware strategy with an early exit mechanism, allowing the model to skip unnecessary iterations for converged tokens. Extensive experiments show that Streaming-dLLM achieves up to 68.2X speedup while maintaining generation quality, highlighting its effectiveness in diffusion decoding. The code is available at this https URL.</li>
<li><strong>摘要：</strong>扩散大语言模型 (dLLM) 为自然语言生成提供了一个引人注目的范例，利用并行解码和双向注意力来实现与自回归模型相比卓越的全局一致性。虽然最近的工作通过 KV 缓存重用或启发式解码加速了推理，但它们忽视了逐块扩散过程中内在的低效率。具体来说，它们通过对信息稀疏的后缀区域进行统一建模而遭受空间冗余，并通过在所有解码过程中应用固定的去噪计划而遭受时间低效。为了解决这个问题，我们提出了 Streaming-dLLM，这是一种无需训练的框架，可以简化空间和时间维度上的推理。在空间上，我们引入衰减引导后缀建模，通过修剪冗余掩码标记来近似完整的上下文。暂时，我们采用具有早期退出机制的动态置信感知策略，允许模型跳过收敛令牌的不必要的迭代。大量实验表明，Streaming-dLLM 在保持生成质量的同时实现了高达 68.2 倍的加速，凸显了其在扩散解码方面的有效性。该代码可从此 https URL 获取。</li>
</ul>

<h3>Title: RemEdit: Efficient Diffusion Editing with Riemannian Geometry</h3>
<ul>
<li><strong>Authors: </strong>Eashan Adhikarla, Brian D. Davison</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17927">https://arxiv.org/abs/2601.17927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17927">https://arxiv.org/pdf/2601.17927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17927]] RemEdit: Efficient Diffusion Editing with Riemannian Geometry(https://arxiv.org/abs/2601.17927)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Controllable image generation is fundamental to the success of modern generative AI, yet it faces a critical trade-off between semantic fidelity and inference speed. The RemEdit diffusion-based framework addresses this trade-off with two synergistic innovations. First, for editing fidelity, we navigate the latent space as a Riemannian manifold. A mamba-based module efficiently learns the manifold's structure, enabling direct and accurate geodesic path computation for smooth semantic edits. This control is further refined by a dual-SLERP blending technique and a goal-aware prompt enrichment pass from a Vision-Language Model. Second, for additional acceleration, we introduce a novel task-specific attention pruning mechanism. A lightweight pruning head learns to retain tokens essential to the edit, enabling effective optimization without the semantic degradation common in content-agnostic approaches. RemEdit surpasses prior state-of-the-art editing frameworks while maintaining real-time performance under 50% pruning. Consequently, RemEdit establishes a new benchmark for practical and powerful image editing. Source code: this https URL.</li>
<li><strong>摘要：</strong>可控图像生成是现代生成人工智能成功的基础，但它面临着语义保真度和推理速度之间的关键权衡。基于 RemEdit 扩散的框架通过两项协同创新解决了这种权衡问题。首先，为了编辑保真度，我们将潜在空间作为黎曼流形进行导航。基于曼巴的模块可以有效地学习流形的结构，从而实现直接、准确的测地路径计算，以实现平滑的语义编辑。通过双 SLERP 混合技术和来自视觉语言模型的目标感知提示丰富传递进一步完善了这种控制。其次，为了进一步加速，我们引入了一种新颖的特定于任务的注意力修剪机制。轻量级修剪头学习保留编辑所必需的标记，从而实现有效的优化，而不会出现与内容无关的方法中常见的语义退化。 RemEdit 超越了之前最先进的编辑框架，同时在 50% 修剪的情况下保持实时性能。因此，RemEdit 为实用且强大的图像编辑树立了新的基准。源代码：这个 https URL。</li>
</ul>

<h3>Title: FedGraph-VASP: Privacy-Preserving Federated Graph Learning with Post-Quantum Security for Cross-Institutional Anti-Money Laundering</h3>
<ul>
<li><strong>Authors: </strong>Daniel Commey, Matilda Nkoom, Yousef Alsenani, Sena G. Hounsinou, Garth V. Crosby</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17935">https://arxiv.org/abs/2601.17935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17935">https://arxiv.org/pdf/2601.17935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17935]] FedGraph-VASP: Privacy-Preserving Federated Graph Learning with Post-Quantum Security for Cross-Institutional Anti-Money Laundering(https://arxiv.org/abs/2601.17935)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Virtual Asset Service Providers (VASPs) face a fundamental tension between regulatory compliance and user privacy when detecting cross-institutional money laundering. Current approaches require either sharing sensitive transaction data or operating in isolation, leaving critical cross-chain laundering patterns undetected. We present FedGraph-VASP, a privacy-preserving federated graph learning framework that enables collaborative anti-money laundering (AML) without exposing raw user data. Our key contribution is a Boundary Embedding Exchange protocol that shares only compressed, non-invertible graph neural network representations of boundary accounts. These exchanges are secured using post-quantum cryptography, specifically the NIST-standardized Kyber-512 key encapsulation mechanism combined with AES-256-GCM authenticated encryption. Experiments on the Elliptic Bitcoin dataset with realistic Louvain partitioning show that FedGraph-VASP achieves an F1-score of 0.508, outperforming the state-of-the-art generative baseline FedSage+ (F1 = 0.453) by 12.1 percent on binary fraud detection. We further show robustness under low-connectivity settings where generative imputation degrades performance, while approaching centralized performance (F1 = 0.620) in high-connectivity regimes. We additionally evaluate generalization on an Ethereum fraud detection dataset, where FedGraph-VASP (F1 = 0.635) is less effective under sparse cross-silo connectivity, while FedSage+ excels (F1 = 0.855), outperforming even local training (F1 = 0.785). These results highlight a topology-dependent trade-off: embedding exchange benefits connected transaction graphs, whereas generative imputation can dominate in highly modular sparse graphs. A privacy audit shows embeddings are only partially invertible (R^2 = 0.32), limiting exact feature recovery.</li>
<li><strong>摘要：</strong>在检测跨机构洗钱活动时，虚拟资产服务提供商 (VASP) 面临监管合规性和用户隐私之间的根本紧张关系。当前的方法需要共享敏感交易数据或隔离操作，从而无法检测到关键的跨链洗钱模式。我们推出了 FedGraph-VASP，这是一种保护隐私的联合图学习框架，可以在不暴露原始用户数据的情况下实现协作反洗钱 (AML)。我们的主要贡献是边界嵌入交换协议，该协议仅共享边界帐户的压缩的、不可逆的图神经网络表示。这些交换使用后量子加密技术进行保护，特别是 NIST 标准化的 Kyber-512 密钥封装机制与 AES-256-GCM 认证加密相结合。在具有真实 Louvain 分区的 Elliptic 比特币数据集上进行的实验表明，FedGraph-VASP 的 F1 分数为 0.508，在二进制欺诈检测方面比最先进的生成基线 FedSage+（F1 = 0.453）高出 12.1%。我们进一步展示了在低连接性设置下的鲁棒性，其中生成插补会降低性能，而在高连接性情况下接近集中式性能（F1 = 0.620）。我们还评估了以太坊欺诈检测数据集的泛化能力，其中 FedGraph-VASP (F1 = 0.635) 在稀疏的跨孤岛连接下效果较差，而 FedSage+ 表现出色 (F1 = 0.855)，甚至优于本地训练 (F1 = 0.785)。这些结果强调了依赖于拓扑的权衡：嵌入交换有利于连接的交易图，而生成插补可以在高度模块化的稀疏图中占主导地位。隐私审计显示嵌入仅部分可逆（R^2 = 0.32），限制了精确的特征恢复。</li>
</ul>

<h3>Title: UPLiFT: Efficient Pixel-Dense Feature Upsampling with Local Attenders</h3>
<ul>
<li><strong>Authors: </strong>Matthew Walmer, Saksham Suri, Anirud Aggarwal, Abhinav Shrivastava</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17950">https://arxiv.org/abs/2601.17950</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17950">https://arxiv.org/pdf/2601.17950</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17950]] UPLiFT: Efficient Pixel-Dense Feature Upsampling with Local Attenders(https://arxiv.org/abs/2601.17950)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The space of task-agnostic feature upsampling has emerged as a promising area of research to efficiently create denser features from pre-trained visual backbones. These methods act as a shortcut to achieve dense features for a fraction of the cost by learning to map low-resolution features to high-resolution versions. While early works in this space used iterative upsampling approaches, more recent works have switched to cross-attention-based methods, which risk falling into the same efficiency scaling problems of the backbones they are upsampling. In this work, we demonstrate that iterative upsampling methods can still compete with cross-attention-based methods; moreover, they can achieve state-of-the-art performance with lower inference costs. We propose UPLiFT, an architecture for Universal Pixel-dense Lightweight Feature Transforms. We also propose an efficient Local Attender operator to overcome the limitations of prior iterative feature upsampling methods. This operator uses an alternative attentional pooling formulation defined fully locally. We show that our Local Attender allows UPLiFT to maintain stable features throughout upsampling, enabling state-of-the-art performance with lower inference costs than existing pixel-dense feature upsamplers. In addition, we apply UPLiFT to generative downstream tasks and show that it achieves competitive performance with state-of-the-art Coupled Flow Matching models for VAE feature upsampling. Altogether, UPLiFT offers a versatile and efficient approach to creating denser features.</li>
<li><strong>摘要：</strong>与任务无关的特征上采样空间已经成为一个有前途的研究领域，可以从预先训练的视觉主干中有效地创建更密集的特征。这些方法是通过学习将低分辨率特征映射到高分辨率版本来以一小部分成本实现密集特征的捷径。虽然该领域的早期工作使用迭代上采样方法，但最近的工作已转向基于交叉注意力的方法，这可能会陷入与上采样主干网相同的效率扩展问题。在这项工作中，我们证明迭代上采样方法仍然可以与基于交叉注意力的方法竞争；此外，它们可以以较低的推理成本实现最先进的性能。我们提出了 UPLiFT，一种通用像素密集轻量级特征转换的架构。我们还提出了一种高效的本地参与算子来克服先前迭代特征上采样方法的局限性。该算子使用完全本地定义的替代注意力池公式。我们表明，我们的本地参与器允许 UPLiFT 在整个上采样过程中保持稳定的特征，从而实现最先进的性能，并且比现有的像素密集特征上采样器具有更低的推理成本。此外，我们将 UPLiFT 应用于生成下游任务，并表明它通过用于 VAE 特征上采样的最先进的耦合流匹配模型实现了具有竞争力的性能。总而言之，UPLiFT 提供了一种通用且高效的方法来创建更密集的特征。</li>
</ul>

<h3>Title: DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal</h3>
<ul>
<li><strong>Authors: </strong>Peixuan Han, Yingjie Yu, Jingjun Xu, Jiaxuan You</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18081">https://arxiv.org/abs/2601.18081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18081">https://arxiv.org/pdf/2601.18081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18081]] DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal(https://arxiv.org/abs/2601.18081)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite the growing adoption of large language models (LLMs) in scientific research workflows, automated support for academic rebuttal, a crucial step in academic communication and peer review, remains largely underexplored. Existing approaches typically rely on off-the-shelf LLMs or simple pipelines, which struggle with long-context understanding and often fail to produce targeted and persuasive responses. In this paper, we propose DRPG, an agentic framework for automatic academic rebuttal generation that operates through four steps: Decompose reviews into atomic concerns, Retrieve relevant evidence from the paper, Plan rebuttal strategies, and Generate responses accordingly. Notably, the Planner in DRPG reaches over 98% accuracy in identifying the most feasible rebuttal direction. Experiments on data from top-tier conferences demonstrate that DRPG significantly outperforms existing rebuttal pipelines and achieves performance beyond the average human level using only an 8B model. Our analysis further demonstrates the effectiveness of the planner design and its value in providing multi-perspective and explainable suggestions. We also showed that DRPG works well in a more complex multi-round setting. These results highlight the effectiveness of DRPG and its potential to provide high-quality rebuttal content and support the scaling of academic discussions. Codes for this work are available at this https URL.</li>
<li><strong>摘要：</strong>尽管在科学研究工作流程中越来越多地采用大型语言模型（LLM），但对学术反驳的自动化支持（学术交流和同行评审的关键步骤）仍然在很大程度上尚未得到充分探索。现有的方法通常依赖于现成的法学硕士或简单的流程，这些方法很难理解长上下文，并且往往无法产生有针对性和有说服力的响应。在本文中，我们提出了 DRPG，一种自动生成学术反驳的代理框架，它通过四个步骤进行操作：将评论分解为原子关注点，从论文中检索相关证据，计划反驳策略，并相应地生成响应。值得注意的是，DRPG 中的 Planner 在识别最可行的反驳方向方面达到了 98% 以上的准确率。对顶级会议数据的实验表明，DRPG 显着优于现有的反驳流程，仅使用 8B 模型即可实现超越人类平均水平的性能。我们的分析进一步证明了规划器设计的有效性及其在提供多视角和可解释建议方面的价值。我们还证明了 DRPG 在更复杂的多回合环境中也能发挥良好的作用。这些结果凸显了 DRPG 的有效性及其提供高质量反驳内容和支持学术讨论规模的潜力。这项工作的代码可在 https URL 中找到。</li>
</ul>

<h3>Title: From LLMs to LRMs: Rethinking Pruning for Reasoning-Centric Models</h3>
<ul>
<li><strong>Authors: </strong>Longwei Ding, Anhao Zhao, Fanghua Ye, Ziyang Chen, Xiaoyu Shen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18091">https://arxiv.org/abs/2601.18091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18091">https://arxiv.org/pdf/2601.18091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18091]] From LLMs to LRMs: Rethinking Pruning for Reasoning-Centric Models(https://arxiv.org/abs/2601.18091)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly costly to deploy, motivating extensive research on model pruning. However, most existing studies focus on instruction-following LLMs, leaving it unclear whether established pruning strategies transfer to reasoning-augmented models that explicitly generate long intermediate reasoning traces. In this work, we conduct a controlled study of pruning for both instruction-following ($\textbf{LLM-instruct}$) and reasoning-augmented ($\textbf{LLM-think}$) models. To isolate the effects of pruning, we align pruning calibration and post-pruning recovery data with each model's original training distribution, which we show yields more stable and reliable pruning behavior. We evaluate static depth pruning, static width pruning, and dynamic pruning across 17 tasks spanning classification, generation, and reasoning. Our results reveal clear paradigm-dependent differences: depth pruning outperforms width pruning on classification tasks, while width pruning is more robust for generation and reasoning. Moreover, static pruning better preserves reasoning performance, whereas dynamic pruning excels on classification and generation but remains challenging for long-chain reasoning. These findings underscore the need for pruning strategies that explicitly account for the distinct characteristics of reasoning-augmented LLMs. Our code is publicly available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的部署成本越来越高，这促使人们对模型剪枝进行广泛的研究。然而，大多数现有研究都集中在遵循指令的法学硕士上，尚不清楚已建立的修剪策略是否转移到显式生成长中间推理轨迹的推理增强模型。在这项工作中，我们对指令遵循 ($\textbf{LLM-instruct}$) 和推理增强 ($\textbf{LLM-think}$) 模型进行了修剪的对照研究。为了隔离剪枝的影响，我们将剪枝校准和剪枝后恢复数据与每个模型的原始训练分布对齐，我们证明这会产生更稳定和可靠的剪枝行为。我们在涵盖分类、生成和推理的 17 个任务中评估静态深度剪枝、静态宽度剪枝和动态剪枝。我们的结果揭示了明显的依赖于范式的差异：在分类任务上，深度剪枝优于宽度剪枝，而宽度剪枝对于生成和推理来说更加稳健。此外，静态剪枝更好地保留了推理性能，而动态剪枝在分类和生成方面表现出色，但对于长链推理仍然具有挑战性。这些发现强调需要明确考虑推理增强法学硕士的独特特征的修剪策略。我们的代码可通过此 https URL 公开获取。</li>
</ul>

<h3>Title: FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhaopeng Qiu, Shuang Yu, Jingqi Zhang, Shuai Zhang, Xue Huang, Jingyi Yang, Junjie Lai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18150">https://arxiv.org/abs/2601.18150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18150">https://arxiv.org/pdf/2601.18150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18150]] FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning(https://arxiv.org/abs/2601.18150)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) for large language models (LLMs) is increasingly bottlenecked by rollout (generation), where long output sequence lengths make attention and KV-cache memory dominate end-to-end step time. FP8 offers an attractive lever for accelerating RL by reducing compute cost and memory traffic during rollout, but applying FP8 in RL introduces unique engineering and algorithmic challenges: policy weights change every step (requiring repeated quantization and weight synchronization into the inference engine) and low-precision rollouts can deviate from the higher-precision policy assumed by the trainer, causing train-inference mismatch and potential instability. This report presents a practical FP8 rollout stack for LLM RL, implemented in the veRL ecosystem with support for common training backends (e.g., FSDP/Megatron-LM) and inference engines (e.g., vLLM/SGLang). We (i) enable FP8 W8A8 linear-layer rollout using blockwise FP8 quantization, (ii) extend FP8 to KV-cache to remove long-context memory bottlenecks via per-step QKV scale recalibration, and (iii) mitigate mismatch using importance-sampling-based rollout correction (token-level TIS/MIS variants). Across dense and MoE models, these techniques deliver up to 44% rollout throughput gains while preserving learning behavior comparable to BF16 baselines.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的强化学习 (RL) 越来越受到推出（生成）的瓶颈，其中长输出序列长度引起注意，而 KV 缓存内存主导端到端步骤时间。 FP8 通过降低推出期间的计算成本和内存流量，为加速 RL 提供了一个有吸引力的杠杆，但在 RL 中应用 FP8 带来了独特的工程和算法挑战：策略权重每一步都会改变（需要重复量化和权重同步到推理引擎中），低精度的推出可能会偏离训练器假设的更高精度策略，导致训练推理不匹配和潜在的不稳定。本报告介绍了 LLM RL 的实用 FP8 推出堆栈，在 veRL 生态系统中实施，支持常见训练后端（例如 FSDP/Megatron-LM）和推理引擎（例如 vLLM/SGLang）。我们 (i) 使用分块 FP8 量化启用 FP8 W8A8 线性层推出，(ii) 将 FP8 扩展到 KV 缓存，以通过每步 QKV 规模重新校准消除长上下文内存瓶颈，以及 (iii) 使用基于重要性采样的推出校正（令牌级 TIS/MIS 变体）减轻不匹配。在密集模型和 MoE 模型中，这些技术可实现高达 44% 的推出吞吐量增益，同时保持与 BF16 基线相当的学习行为。</li>
</ul>

<h3>Title: Agentic Very Long Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Aniket Rege, Arka Sadhu, Yuliang Li, Kejie Li, Ramya Korlakai Vinayak, Yuning Chai, Yong Jae Lee, Hyo Jin Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18157">https://arxiv.org/abs/2601.18157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18157">https://arxiv.org/pdf/2601.18157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18157]] Agentic Very Long Video Understanding(https://arxiv.org/abs/2601.18157)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands a new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips a planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.</li>
<li><strong>摘要：</strong></li>
</ul>

<h3>Title: QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding</h3>
<ul>
<li><strong>Authors: </strong>Linhan Cao, Wei Sun, Weixia Zhang, Xiangyang Zhu, Kaiwei Zhang, Jun Jia, Dandan Zhu, Guangtao Zhai, Xiongkuo Min</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18195">https://arxiv.org/abs/2601.18195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18195">https://arxiv.org/pdf/2601.18195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18195]] QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding(https://arxiv.org/abs/2601.18195)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, quality assessment</a></li>
<li><strong>Abstract: </strong>Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \textit{fine-grained spatiotemporal perception} and \textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \textbf{QualiRAG}, a \textit{training-free} \textbf{R}etrieval-\textbf{A}ugmented \textbf{G}eneration \textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \textit{visual metadata}, \textit{subject localization}, \textit{global quality summaries}, and \textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at this https URL.</li>
<li><strong>摘要：</strong>视觉质量评估（VQA）正逐渐从标量分数预测转向可解释的质量理解——这种范式需要 \textit{细粒度时空感知} 和 \textit{辅助上下文信息}。当前的方法依赖于对精选教学数据集的监督微调或强化学习，这涉及劳动密集型注释，并且容易出现数据集特定的偏差。为了应对这些挑战，我们提出了 \textbf{QualiRAG}，一个 \textit{training-free} \textbf{R}etrieval-\textbf{A}ugmented \textbf{G}eneration \textbf{(RAG)} 框架，系统地利用大型多模态模型（LMM）的潜在感知知识来实现​​视觉质量感知。与从静态语料库中检索的传统 RAG 不同，QualiRAG 通过将问题分解为结构化请求并构建四个互补的知识源来动态生成辅助知识：\textit{视觉元数据}、\textit{主题定位}、\textit{全局质量摘要}和 \textit{局部质量描述}，然后进行相关性感知检索以进行基于证据的推理。大量实验表明，QualiRAG 在视觉质量理解任务上比开源通用 LMM 和 VQA 微调 LMM 取得了实质性改进，并在视觉质量比较任务上提供了具有竞争力的性能，无需任何特定任务培训即可展示强大的质量评估能力。该代码将在此 https URL 上公开提供。</li>
</ul>

<h3>Title: HomoFM: Deep Homography Estimation with Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Mengfan He, Liangzheng Sun, Chunyu Li, Ziyang Meng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18222">https://arxiv.org/abs/2601.18222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18222">https://arxiv.org/pdf/2601.18222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18222]] HomoFM: Deep Homography Estimation with Flow Matching(https://arxiv.org/abs/2601.18222)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep homography estimation has broad applications in computer vision and robotics. Remarkable progresses have been achieved while the existing methods typically treat it as a direct regression or iterative refinement problem and often struggling to capture complex geometric transformations or generalize across different domains. In this work, we propose HomoFM, a new framework that introduces the flow matching technique from generative modeling into the homography estimation task for the first time. Unlike the existing methods, we formulate homography estimation problem as a velocity field learning problem. By modeling a continuous and point-wise velocity field that transforms noisy distributions into registered coordinates, the proposed network recovers high-precision transformations through a conditional flow trajectory. Furthermore, to address the challenge of domain shifts issue, e.g., the cases of multimodal matching or varying illumination scenarios, we integrate a gradient reversal layer (GRL) into the feature extraction backbone. This domain adaptation strategy explicitly constrains the encoder to learn domain-invariant representations, significantly enhancing the network's robustness. Extensive experiments demonstrate the effectiveness of the proposed method, showing that HomoFM outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks. Code and data resource are available at this https URL.</li>
<li><strong>摘要：</strong>深度单应性估计在计算机视觉和机器人技术中具有广泛的应用。已经取得了显着的进展，而现有的方法通常将其视为直接回归或迭代细化问题，并且常常难以捕获复杂的几何变换或跨不同领域进行概括。在这项工作中，我们提出了 HomoFM，这是一个新框架，首次将生成建模中的流匹配技术引入单应性估计任务中。与现有方法不同，我们将单应性估计问题表述为速度场学习问题。通过对连续的逐点速度场进行建模，将噪声分布转换为注册坐标，所提出的网络通过条件流轨迹恢复高精度转换。此外，为了解决域转移问题的挑战，例如多模态匹配或变化照明场景的情况，我们将梯度反转层（GRL）集成到特征提取主干中。这种域适应策略明确限制编码器学习域不变表示，从而显着增强网络的鲁棒性。大量实验证明了该方法的有效性，表明 HomoFM 在标准基准的估计精度和鲁棒性方面均优于最先进的方法。代码和数据资源可从此 https URL 获取。</li>
</ul>

<h3>Title: Beyond Retention: Orchestrating Structural Safety and Plasticity in Continual Learning for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Fei Meng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18255">https://arxiv.org/abs/2601.18255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18255">https://arxiv.org/pdf/2601.18255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18255]] Beyond Retention: Orchestrating Structural Safety and Plasticity in Continual Learning for LLMs(https://arxiv.org/abs/2601.18255)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Continual learning in Large Language Models (LLMs) faces the critical challenge of balancing stability (retaining old knowledge) and plasticity (learning new tasks). While Experience Replay (ER) is a standard countermeasure against catastrophic forgetting, its impact across diverse capabilities remains underexplored. In this work, we uncover a critical dichotomy in ER's behavior: while it induces positive backward transfer on robust, unstructured tasks (e.g., boosting performance on previous NLP classification tasks through repeated rehearsal), it causes severe negative transfer on fragile, structured domains like code generation (e.g., a significant relative drop in coding accuracy). This reveals that ER trades structural integrity for broad consolidation. To address this dilemma, we propose \textbf{Orthogonal Subspace Wake-up (OSW)}. OSW identifies essential parameter subspaces of previous tasks via a brief "wake-up" phase and enforces orthogonal updates for new tasks, providing a mathematically grounded "safety guarantee" for established knowledge structures. Empirical results across a diverse four-task sequence demonstrate that OSW uniquely succeeds in preserving fragile coding abilities where Replay fails, while simultaneously maintaining high plasticity for novel tasks. Our findings emphasize the necessity of evaluating structural safety alongside average retention in LLM continual learning.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的持续学习面临着平衡稳定性（保留旧知识）和可塑性（学习新任务）的严峻挑战。虽然体验重播 (ER) 是针对灾难性遗忘的标准对策，但其对不同功能的影响仍未得到充分探索。在这项工作中，我们发现了 ER 行为中的一个关键的二分法：虽然它在鲁棒的、非结构化的任务上引发了正向后迁移（例如，通过反复排练提高了先前 NLP 分类任务的性能），但它在代码生成等脆弱的结构化领域上导致了严重的负迁移（例如，编码准确性的相对显着下降）。这表明，ER 以结构完整性为代价换取了广泛的整合。为了解决这个困境，我们提出\textbf{正交子空间唤醒（OSW）}。 OSW通过短暂的“唤醒”阶段识别先前任务的基本参数子空间，并对新任务强制正交更新，为已建立的知识结构提供数学基础的“安全保证”。不同的四任务序列的实证结果表明，OSW 独特地成功地保留了重放失败时脆弱的编码能力，同时保持了新任务的高可塑性。我们的研究结果强调了在法学硕士持续学习中评估结构安全性和平均保留率的必要性。</li>
</ul>

<h3>Title: FGGM: Fisher-Guided Gradient Masking for Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>Chao-Hong Tan, Qian Chen, Wen Wang, Yukun Ma, Chong Zhang, Chong Deng, Qinglin Zhang, Xiangang Li, Jieping Ye</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18261">https://arxiv.org/abs/2601.18261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18261">https://arxiv.org/pdf/2601.18261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18261]] FGGM: Fisher-Guided Gradient Masking for Continual Learning(https://arxiv.org/abs/2601.18261)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Catastrophic forgetting impairs the continuous learning of large language models. We propose Fisher-Guided Gradient Masking (FGGM), a framework that mitigates this by strategically selecting parameters for updates using diagonal Fisher Information. FGGM dynamically generates binary masks with adaptive thresholds, preserving critical parameters to balance stability and plasticity without requiring historical data. Unlike magnitude-based methods such as MIGU, our approach offers a mathematically principled parameter importance estimation. On the TRACE benchmark, FGGM shows a 9.6% relative improvement in retaining general capabilities over supervised fine-tuning (SFT) and a 4.4% improvement over MIGU on TRACE tasks. Additional analysis on code generation tasks confirms FGGM's superior performance and reduced forgetting, establishing it as an effective solution.</li>
<li><strong>摘要：</strong>灾难性遗忘会损害大型语言模型的持续学习。我们提出费舍尔引导梯度掩蔽（FGGM），这是一个框架，通过使用对角费舍尔信息策略性地选择更新参数来缓解这一问题。 FGGM 动态生成具有自适应阈值的二进制掩模，保留关键参数以平衡稳定性和可塑性，而无需历史数据。与基于幅度的方法（例如 MIGU）不同，我们的方法提供了数学原理的参数重要性估计。在 TRACE 基准上，FGGM 在保留一般能力方面比监督微调 (SFT) 提高了 9.6%，在 TRACE 任务上比 MIGU 提高了 4.4%。对代码生成任务的额外分析证实了 FGGM 的卓越性能并减少了遗忘，使其成为一种有效的解决方案。</li>
</ul>

<h3>Title: TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment</h3>
<ul>
<li><strong>Authors: </strong>Zhewen Tan, Wenhan Yu, Jianfeng Si, Tongxin Liu, Kaiqi Guan, Huiyan Jin, Jiawen Tao, Xiaokun Yuan, Duohe Ma, Xiangzheng Zhang, Tong Yang, Lin Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18292">https://arxiv.org/abs/2601.18292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18292">https://arxiv.org/pdf/2601.18292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18292]] TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment(https://arxiv.org/abs/2601.18292)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In recent years, safety risks associated with large language models have become increasingly prominent, highlighting the urgent need to mitigate the generation of toxic and harmful content. The mainstream paradigm for LLM safety alignment typically adopts a collaborative framework involving three roles: an attacker for adversarial prompt generation, a defender for safety defense, and an evaluator for response assessment. In this paper, we propose a closed-loop reinforcement learning framework called TriPlay-RL that enables iterative and co-improving collaboration among three roles with near-zero manual annotation. Experimental results show that the attacker preserves high output diversity while achieving a 20%-50% improvement in adversarial effectiveness; the defender attains 10%-30% gains in safety performance without degrading general reasoning capability; and the evaluator continuously refines its fine-grained judgment ability through iterations, accurately distinguishing unsafe responses, simple refusals, and useful guidance. Overall, our framework establishes an efficient and scalable paradigm for LLM safety alignment, enabling continuous co-evolution within a unified learning loop.</li>
<li><strong>摘要：</strong>近年来，大型语言模型的安全风险日益凸显，迫切需要减少有毒有害内容的产生。 LLM安全一致性的主流范式通常采用协作框架，涉及三个角色：对抗性提示生成的攻击者、安全防御的防御者和响应评估的评估者。在本文中，我们提出了一种名为 TriPlay-RL 的闭环强化学习框架，该框架能够以接近零的手动注释实现三个角色之间的迭代和共同改进协作。实验结果表明，攻击者保留了较高的输出多样性，同时对抗有效性提高了20%-50%；防御者在不降低一般推理能力的情况下获得了 10%-30% 的安全性能提升；评估者通过迭代不断完善细粒度的判断能力，准确区分不安全的反应、简单的拒绝和有用的指导。总的来说，我们的框架为法学硕士安全调整建立了一个高效且可扩展的范式，从而在统一的学习循环中实现持续的共同进化。</li>
</ul>

<h3>Title: Making medical vision-language models think causally across modalities with retrieval-augmented cross-modal reasoning</h3>
<ul>
<li><strong>Authors: </strong>Weiqin Yang, Haowen Xue, Qingyi Peng, Hexuan Hu, Qian Huang, Tingbo Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18356">https://arxiv.org/abs/2601.18356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18356">https://arxiv.org/pdf/2601.18356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18356]] Making medical vision-language models think causally across modalities with retrieval-augmented cross-modal reasoning(https://arxiv.org/abs/2601.18356)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Medical vision-language models (VLMs) achieve strong performance in diagnostic reporting and image-text alignment, yet their underlying reasoning mechanisms remain fundamentally correlational, exhibiting reliance on superficial statistical associations that fail to capture the causal pathophysiological mechanisms central to clinical decision-making. This limitation makes them fragile, prone to hallucinations, and sensitive to dataset biases. Retrieval-augmented generation (RAG) offers a partial remedy by grounding predictions in external knowledge. However, conventional RAG depends on semantic similarity, introducing new spurious correlations. We propose Multimodal Causal Retrieval-Augmented Generation, a framework that integrates causal inference principles with multimodal retrieval. It retrieves clinically relevant exemplars and causal graphs from external sources, conditioning model reasoning on counterfactual and interventional evidence rather than correlations alone. Applied to radiology report generation, diagnosis prediction, and visual question answering, it improves factual accuracy, robustness to distribution shifts, and interpretability. Our results highlight causal retrieval as a scalable path toward medical VLMs that think beyond pattern matching, enabling trustworthy multimodal reasoning in high-stakes clinical settings.</li>
<li><strong>摘要：</strong>医学视觉语言模型（VLM）在诊断报告和图像文本对齐方面取得了出色的表现，但其潜在的推理机制仍然具有根本的相关性，表现出对表面统计关联的依赖，而这些关联无法捕获临床决策的核心因果病理生理机制。这种限制使它们变得脆弱，容易产生幻觉，并且对数据集偏差敏感。检索增强生成（RAG）通过将预测建立在外部知识的基础上提供了部分补救措施。然而，传统的 RAG 依赖于语义相似性，引入了新的虚假相关性。我们提出了多模态因果检索增强生成，这是一个将因果推理原理与多模态检索相结合的框架。它从外部来源检索临床相关的范例和因果图，根据反事实和干预证据进行条件模型推理，而不仅仅是相关性。应用于放射学报告生成、诊断预测和视觉问答，它提高了事实准确性、分布变化的鲁棒性和可解释性。我们的结果强调因果检索是通往医学 VLM 的可扩展路径，它超越模式匹配，在高风险的临床环境中实现值得信赖的多模态推理。</li>
</ul>

<h3>Title: GCFX: Generative Counterfactual Explanations for Deep Graph Models at the Model Level</h3>
<ul>
<li><strong>Authors: </strong>Jinlong Hu, Jiacheng Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18447">https://arxiv.org/abs/2601.18447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18447">https://arxiv.org/pdf/2601.18447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18447]] GCFX: Generative Counterfactual Explanations for Deep Graph Models at the Model Level(https://arxiv.org/abs/2601.18447)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Deep graph learning models have demonstrated remarkable capabilities in processing graph-structured data and have been widely applied across various fields. However, their complex internal architectures and lack of transparency make it difficult to explain their decisions, resulting in opaque models that users find hard to understand and trust. In this paper, we explore model-level explanation techniques for deep graph learning models, aiming to provide users with a comprehensive understanding of the models' overall decision-making processes and underlying mechanisms. Specifically, we address the problem of counterfactual explanations for deep graph learning models by introducing a generative model-level counterfactual explanation approach called GCFX, which is based on deep graph generation. This approach generates a set of high-quality counterfactual explanations that reflect the model's global predictive behavior by leveraging an enhanced deep graph generation framework and a global summarization algorithm. GCFX features an architecture that combines dual encoders, structure-aware taggers, and Message Passing Neural Network decoders, enabling it to accurately learn the true latent distribution of input data and generate high-quality, closely related counterfactual examples. Subsequently, a global counterfactual summarization algorithm selects the most representative and comprehensive explanations from numerous candidate counterfactuals, providing broad insights into the model's global predictive patterns. Experiments on a synthetic dataset and several real-world datasets demonstrate that GCFX outperforms existing methods in terms of counterfactual validity and coverage while maintaining low explanation costs, thereby offering crucial support for enhancing the practicality and trustworthiness of global counterfactual explanations.</li>
<li><strong>摘要：</strong>深度图学习模型在处理图结构数据方面表现出了卓越的能力，并已广泛应用于各个领域。然而，其复杂的内部架构和缺乏透明度使得很难解释他们的决策，导致用户难以理解和信任的不透明模型。在本文中，我们探索深度图学习模型的模型级解释技术，旨在为用户提供对模型整体决策过程和底层机制的全面理解。具体来说，我们通过引入一种称为 GCFX 的生成模型级反事实解释方法来解决深度图学习模型的反事实解释问题，该方法基于深度图生成。该方法利用增强的深度图生成框架和全局汇总算法生成一组高质量的反事实解释，反映模型的全局预测行为。 GCFX 的架构结合了双编码器、结构感知标记器和消息传递神经网络解码器，使其能够准确学习输入数据的真实潜在分布，并生成高质量、密切相关的反事实示例。随后，全局反事实总结算法从众多候选反事实中选择最具代表性和最全面的解释，为模型的全局预测模式提供广泛的见解。对合成数据集和多个真实世界数据集的实验表明，GCFX 在反事实有效性和覆盖范围方面优于现有方法，同时保持较低的解释成本，从而为增强全球反事实解释的实用性和可信度提供了关键支持。</li>
</ul>

<h3>Title: 3DGesPolicy: Phoneme-Aware Holistic Co-Speech Gesture Generation Based on Action Control</h3>
<ul>
<li><strong>Authors: </strong>Xuanmeng Sha, Liyun Zhang, Tomohiro Mashita, Naoya Chiba, Yuki Uranishi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.MM, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18451">https://arxiv.org/abs/2601.18451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18451">https://arxiv.org/pdf/2601.18451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18451]] 3DGesPolicy: Phoneme-Aware Holistic Co-Speech Gesture Generation Based on Action Control(https://arxiv.org/abs/2601.18451)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating holistic co-speech gestures that integrate full-body motion with facial expressions suffers from semantically incoherent coordination on body motion and spatially unstable meaningless movements due to existing part-decomposed or frame-level regression methods, We introduce 3DGesPolicy, a novel action-based framework that reformulates holistic gesture generation as a continuous trajectory control problem through diffusion policy from robotics. By modeling frame-to-frame variations as unified holistic actions, our method effectively learns inter-frame holistic gesture motion patterns and ensures both spatially and semantically coherent movement trajectories that adhere to realistic motion manifolds. To further bridge the gap in expressive alignment, we propose a Gesture-Audio-Phoneme (GAP) fusion module that can deeply integrate and refine multi-modal signals, ensuring structured and fine-grained alignment between speech semantics, body motion, and facial expressions. Extensive quantitative and qualitative experiments on the BEAT2 dataset demonstrate the effectiveness of our 3DGesPolicy across other state-of-the-art methods in generating natural, expressive, and highly speech-aligned holistic gestures.</li>
<li><strong>摘要：</strong>由于现有的部分分解或帧级回归方法，生成将全身运动与面部表情相结合的整体协同语音手势会受到身体运动语义不连贯的协调以及空间上不稳定的无意义运动的影响，我们引入了 3DGesPolicy，这是一种新颖的基于动作的框架，它通过机器人技术的扩散策略将整体手势生成重新表述为连续轨迹控制问题。通过将帧到帧的变化建模为统一的整体动作，我们的方法有效地学习帧间整体手势运动模式，并确保空间和语义上一致的运动轨迹，符合真实的运动流形。为了进一步弥合表达对齐方面的差距，我们提出了一种手势-音频-音素（GAP）融合模块，可以深度集成和细化多模态信号，确保语音语义、身体运动和面部表情之间的结构化和细粒度对齐。对 BEAT2 数据集进行的大量定量和定性实验证明了我们的 3DGesPolicy 相对于其他最先进的方法在生成自然、富有表现力和高度语音对齐的整体手势方面的有效性。</li>
</ul>

<h3>Title: DisasterInsight: A Multimodal Benchmark for Function-Aware and Grounded Disaster Assessment</h3>
<ul>
<li><strong>Authors: </strong>Sara Tehrani, Yonghao Xu, Leif Haglund, Amanda Berg, Michael Felsberg</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18493">https://arxiv.org/abs/2601.18493</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18493">https://arxiv.org/pdf/2601.18493</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18493]] DisasterInsight: A Multimodal Benchmark for Function-Aware and Grounded Disaster Assessment(https://arxiv.org/abs/2601.18493)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Timely interpretation of satellite imagery is critical for disaster response, yet existing vision-language benchmarks for remote sensing largely focus on coarse labels and image-level recognition, overlooking the functional understanding and instruction robustness required in real humanitarian workflows. We introduce DisasterInsight, a multimodal benchmark designed to evaluate vision-language models (VLMs) on realistic disaster analysis tasks. DisasterInsight restructures the xBD dataset into approximately 112K building-centered instances and supports instruction-diverse evaluation across multiple tasks, including building-function classification, damage-level and disaster-type classification, counting, and structured report generation aligned with humanitarian assessment guidelines. To establish domain-adapted baselines, we propose DI-Chat, obtained by fine-tuning existing VLM backbones on disaster-specific instruction data using parameter-efficient Low-Rank Adaptation (LoRA). Extensive experiments on state-of-the-art generic and remote-sensing VLMs reveal substantial performance gaps across tasks, particularly in damage understanding and structured report generation. DI-Chat achieves significant improvements on damage-level and disaster-type classification as well as report generation quality, while building-function classification remains challenging for all evaluated models. DisasterInsight provides a unified benchmark for studying grounded multimodal reasoning in disaster imagery.</li>
<li><strong>摘要：</strong>卫星图像的及时解释对于灾难响应至关重要，但现有的遥感视觉语言基准主要集中在粗略标签和图像级识别上，忽视了实际人道主义工作流程中所需的功能理解和指令稳健性。我们推出了 DisasterInsight，这是一个多模式基准测试，旨在评估现实灾难分析任务的视觉语言模型 (VLM)。 DisasterInsight 将 xBD 数据集重组为大约 112K 以建筑为中心的实例，并支持跨多个任务的指令多样化评估，包括建筑功能分类、损坏级别和灾难类型分类、计数以及符合人道主义评估指南的结构化报告生成。为了建立域适应基线，我们提出了 DI-Chat，它是通过使用参数高效的低阶适应 (LoRA) 对灾难特定指令数据上的现有 VLM 主干进行微调而获得的。对最先进的通用和遥感 VLM 进行的广泛实验揭示了任务之间的巨大性能差距，特别是在损伤理解和结构化报告生成方面。 DI-Chat 在损害级别和灾害类型分类以及报告生成质量方面取得了显着改进，而建筑功能分类对于所有评估模型来说仍然具有挑战性。 DisasterInsight 为研究灾难图像中的扎根多模态推理提供了统一的基准。</li>
</ul>

<h3>Title: GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Kaixun Jiang, Yuzheng Wang, Junjie Zhou, Pandeng Li, Zhihang Liu, Chen-Wei Xie, Zhaoyu Chen, Yun Zheng, Wenqiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18543">https://arxiv.org/abs/2601.18543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18543">https://arxiv.org/pdf/2601.18543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18543]] GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning(https://arxiv.org/abs/2601.18543)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\%) and WISE (+14\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \href{this https URL}{this url}.</li>
<li><strong>摘要：</strong>我们引入 GenAgent，通过代理多模态模型统一视觉理解和生成。与面临昂贵的训练成本和理解生成权衡的统一模型不同，GenAgent 通过代理框架将这些功能解耦：理解由多模态模型本身处理，而生成是通过将图像生成模型视为可调用工具来实现的。至关重要的是，与受静态管道约束的现有模块化系统不同，这种设计可以实现自主多轮交互，其中代理生成包含推理、工具调用、判断和反射的多模式思想链，以迭代地完善输出。我们采用两阶段训练策略：首先，冷启动，对高质量工具调用和反射数据进行监督微调，以引导代理行为；其次，端到端代理强化学习结合了点式奖励（最终图像质量）和成对奖励（反射准确性），并通过轨迹重采样来增强多轮探索。 GenAgent 显着提高了 GenEval++ (+23.6\%) 和 WISE (+14\%) 上的基本生成器 (FLUX.1-dev) 性能。除了性能提升之外，我们的框架还展示了三个关键属性：1）对具有不同功能的生成器的跨工具泛化，2）测试时间扩展，在交互轮次中具有一致的改进，以及3）自动调整以适应不同任务的任务自适应推理。我们的代码将在 \href{this https URL}{this url} 中提供。</li>
</ul>

<h3>Title: Generative Diffusion Augmentation with Quantum-Enhanced Discrimination for Medical Image Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Jingsong Xia, Siqi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18556">https://arxiv.org/abs/2601.18556</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18556">https://arxiv.org/pdf/2601.18556</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18556]] Generative Diffusion Augmentation with Quantum-Enhanced Discrimination for Medical Image Diagnosis(https://arxiv.org/abs/2601.18556)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In biomedical engineering, artificial intelligence has become a pivotal tool for enhancing medical diagnostics, particularly in medical image classification tasks such as detecting pneumonia from chest X-rays and breast cancer screening. However, real-world medical datasets frequently exhibit severe class imbalance, where positive samples substantially outnumber negative samples, leading to biased models with low recall rates for minority classes. This imbalance not only compromises diagnostic accuracy but also poses clinical misdiagnosis risks. To address this challenge, we propose SDA-QEC (Simplified Diffusion Augmentation with Quantum-Enhanced Classification), an innovative framework that integrates simplified diffusion-based data augmentation with quantum-enhanced feature discrimination. Our approach employs a lightweight diffusion augmentor to generate high-quality synthetic samples for minority classes, rebalancing the training distribution. Subsequently, a quantum feature layer embedded within MobileNetV2 architecture enhances the model's discriminative capability through high-dimensional feature mapping in Hilbert space. Comprehensive experiments on coronary angiography image classification demonstrate that SDA-QEC achieves 98.33% accuracy, 98.78% AUC, and 98.33% F1-score, significantly outperforming classical baselines including ResNet18, MobileNetV2, DenseNet121, and VGG16. Notably, our framework simultaneously attains 98.33% sensitivity and 98.33% specificity, achieving a balanced performance critical for clinical deployment. The proposed method validates the feasibility of integrating generative augmentation with quantum-enhanced modeling in real-world medical imaging tasks, offering a novel research pathway for developing highly reliable medical AI systems in small-sample, highly imbalanced, and high-risk diagnostic scenarios.</li>
<li><strong>摘要：</strong>在生物医学工程中，人工智能已成为增强医疗诊断的关键工具，特别是在医学图像分类任务中，例如通过胸部 X 光检测肺炎和乳腺癌筛查。然而，现实世界的医学数据集经常表现出严重的类别不平衡，其中正样本大大超过负样本，导致少数类别的召回率较低的有偏差模型。这种不平衡不仅会影响诊断准确性，还会带来临床误诊风险。为了应对这一挑战，我们提出了 SDA-QEC（具有量子增强分类的简化扩散增强），这是一种创新框架，它将基于简化扩散的数据增强与量子增强特征辨别相结合。我们的方法采用轻量级扩散增强器为少数类别生成高质量的合成样本，重新平衡训练分布。随后，MobileNetV2 架构中嵌入的量子特征层通过希尔伯特空间中的高维特征映射增强了模型的判别能力。冠状动脉造影图像分类的综合实验表明，SDA-QEC 的准确率达到 98.33%，AUC 达到 98.78%，F1 分数达到 98.33%，显着优于 ResNet18、MobileNetV2、DenseNet121 和 VGG16 等经典基线。值得注意的是，我们的框架同时达到了 98.33% 的敏感性和 98.33% 的特异性，实现了对临床部署至关重要的平衡性能。该方法验证了在现实世界的医学成像任务中将生成增强与量子增强建模相结合的可行性，为在小样本、高度不平衡和高风险的诊断场景中开发高可靠的医疗人工智能系统提供了一种新的研究途径。</li>
</ul>

<h3>Title: GimmBO: Interactive Generative Image Model Merging via Bayesian Optimization</h3>
<ul>
<li><strong>Authors: </strong>Chenxi Liu, Selena Ling, Alec Jacobson</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18585">https://arxiv.org/abs/2601.18585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18585">https://arxiv.org/pdf/2601.18585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18585]] GimmBO: Interactive Generative Image Model Merging via Bayesian Optimization(https://arxiv.org/abs/2601.18585)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Fine-tuning-based adaptation is widely used to customize diffusion-based image generation, leading to large collections of community-created adapters that capture diverse subjects and styles. Adapters derived from the same base model can be merged with weights, enabling the synthesis of new visual results within a vast and continuous design space. To explore this space, current workflows rely on manual slider-based tuning, an approach that scales poorly and makes weight selection difficult, even when the candidate set is limited to 20-30 adapters. We propose GimmBO to support interactive exploration of adapter merging for image generation through Preferential Bayesian Optimization (PBO). Motivated by observations from real-world usage, including sparsity and constrained weight ranges, we introduce a two-stage BO backend that improves sampling efficiency and convergence in high-dimensional spaces. We evaluate our approach with simulated users and a user study, demonstrating improved convergence, high success rates, and consistent gains over BO and line-search baselines, and further show the flexibility of the framework through several extensions.</li>
<li><strong>摘要：</strong>基于微调的适应广泛用于定制基于扩散的图像生成，从而产生了大量社区创建的适配器，可以捕捉不同的主题和风格。来自同一基本模型的适配器可以与权重合并，从而能够在广阔且连续的设计空间中合成新的视觉结果。为了探索这个空间，当前的工作流程依赖于基于滑块的手动调整，这种方法扩展性很差，并且使得权重选择变得困难，即使候选集仅限于 20-30 个适配器也是如此。我们建议 GimmBO 通过优先贝叶斯优化（PBO）支持适配器合并的交互式探索，以生成图像。受现实世界使用观察（包括稀疏性和约束权重范围）的启发，我们引入了两阶段 BO 后端，可以提高高维空间中的采样效率和收敛性。我们通过模拟用户和用户研究来评估我们的方法，证明了改进的收敛性、高成功率以及相对于 BO 和线搜索基线的一致增益，并通过多个扩展进一步展示了该框架的灵活性。</li>
</ul>

<h3>Title: Geometry-Free Conditional Diffusion Modeling for Solving the Inverse Electrocardiography Problem</h3>
<ul>
<li><strong>Authors: </strong>Ramiro Valdes Jara, Adam Meyers</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18615">https://arxiv.org/abs/2601.18615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18615">https://arxiv.org/pdf/2601.18615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18615]] Geometry-Free Conditional Diffusion Modeling for Solving the Inverse Electrocardiography Problem(https://arxiv.org/abs/2601.18615)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper proposes a data-driven model for solving the inverse problem of electrocardiography, the mathematical problem that forms the basis of electrocardiographic imaging (ECGI). We present a conditional diffusion framework that learns a probabilistic mapping from noisy body surface signals to heart surface electric potentials. The proposed approach leverages the generative nature of diffusion models to capture the non-unique and underdetermined nature of the ECGI inverse problem, enabling probabilistic sampling of multiple reconstructions rather than a single deterministic estimate. Unlike traditional methods, the proposed framework is geometry-free and purely data-driven, alleviating the need for patient-specific mesh construction. We evaluate the method on a real ECGI dataset and compare it against strong deterministic baselines, including a convolutional neural network, long short-term memory network, and transformer-based model. The results demonstrate that the proposed diffusion approach achieves improved reconstruction accuracy, highlighting the potential of diffusion models as a robust tool for noninvasive cardiac electrophysiology imaging.</li>
<li><strong>摘要：</strong>本文提出了一种数据驱动模型来解决心电图逆问题，即构成心电图成像（ECGI）基础的数学问题。我们提出了一个条件扩散框架，该框架学习从嘈杂的体表信号到心脏表面电位的概率映射。所提出的方法利用扩散模型的生成性质来捕获 ECGI 逆问题的非唯一性和不确定性，从而能够对多个重建进行概率采样，而不是单个确定性估计。与传统方法不同，所提出的框架是无几何形状且纯粹由数据驱动的，从而减轻了对患者特定网格构建的需求。我们在真实的 ECGI 数据集上评估该方法，并将其与强确定性基线进行比较，包括卷积神经网络、长短期记忆网络和基于变压器的模型。结果表明，所提出的扩散方法提高了重建精度，凸显了扩散模型作为无创心脏电生理学成像的强大工具的潜力。</li>
</ul>

<h3>Title: Adaptive Domain Shift in Diffusion Models for Cross-Modality Image Translation</h3>
<ul>
<li><strong>Authors: </strong>Zihao Wang, Yuzhou Chen, Shaogang Ren</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18623">https://arxiv.org/abs/2601.18623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18623">https://arxiv.org/pdf/2601.18623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18623]] Adaptive Domain Shift in Diffusion Models for Cross-Modality Image Translation(https://arxiv.org/abs/2601.18623)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generative</a></li>
<li><strong>Abstract: </strong>Cross-modal image translation remains brittle and inefficient. Standard diffusion approaches often rely on a single, global linear transfer between domains. We find that this shortcut forces the sampler to traverse off-manifold, high-cost regions, inflating the correction burden and inviting semantic drift. We refer to this shared failure mode as fixed-schedule domain transfer. In this paper, we embed domain-shift dynamics directly into the generative process. Our model predicts a spatially varying mixing field at every reverse step and injects an explicit, target-consistent restoration term into the drift. This in-step guidance keeps large updates on-manifold and shifts the model's role from global alignment to local residual correction. We provide a continuous-time formulation with an exact solution form and derive a practical first-order sampler that preserves marginal consistency. Empirically, across translation tasks in medical imaging, remote sensing, and electroluminescence semantic mapping, our framework improves structural fidelity and semantic consistency while converging in fewer denoising steps.</li>
<li><strong>摘要：</strong>跨模式图像翻译仍然脆弱且低效。标准扩散方法通常依赖于域之间的单一全局线性传输。我们发现这种捷径迫使采样器遍历非流形的高成本区域，增加了校正负担并引发语义漂移。我们将这种共享故障模式称为固定计划域传输。在本文中，我们将域转移动力学直接嵌入到生成过程中。我们的模型预测每个反向步骤中空间变化的混合场，并将一个明确的、目标一致的恢复项注入到漂移中。这种逐步指导使流形上保持大量更新，并将模型的角色从全局对齐转变为局部残差校正。我们提供了具有精确解形式的连续时间公式，并导出了保持边际一致性的实用一阶采样器。根据经验，在医学成像、遥感和电致发光语义映射的翻译任务中，我们的框架提高了结构保真度和语义一致性，同时以更少的去噪步骤收敛。</li>
</ul>

<h3>Title: Splat-Portrait: Generalizing Talking Heads with Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Tong Shi, Melonie de Almeida, Daniela Ivanova, Nicolas Pugeault, Paul Henderson</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18633">https://arxiv.org/abs/2601.18633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18633">https://arxiv.org/pdf/2601.18633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18633]] Splat-Portrait: Generalizing Talking Heads with Gaussian Splatting(https://arxiv.org/abs/2601.18633)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Talking Head Generation aims at synthesizing natural-looking talking videos from speech and a single portrait image. Previous 3D talking head generation methods have relied on domain-specific heuristics such as warping-based facial motion representation priors to animate talking motions, yet still produce inaccurate 3D avatar reconstructions, thus undermining the realism of generated animations. We introduce Splat-Portrait, a Gaussian-splatting-based method that addresses the challenges of 3D head reconstruction and lip motion synthesis. Our approach automatically learns to disentangle a single portrait image into a static 3D reconstruction represented as static Gaussian Splatting, and a predicted whole-image 2D background. It then generates natural lip motion conditioned on input audio, without any motion driven priors. Training is driven purely by 2D reconstruction and score-distillation losses, without 3D supervision nor landmarks. Experimental results demonstrate that Splat-Portrait exhibits superior performance on talking head generation and novel view synthesis, achieving better visual quality compared to previous works. Our project code and supplementary documents are public available at this https URL.</li>
<li><strong>摘要：</strong>Talking Head Generation 旨在从语音和单个肖像图像合成看起来自然的谈话视频。以前的 3D 头部说话生成方法依赖于特定领域的启发法，例如在动画说话动作之前基于扭曲的面部运动表示，但仍然会产生不准确的 3D 头像重建，从而破坏了生成动画的真实感。我们引入了 Splat-Portrait，这是一种基于高斯分布的方法，可解决 3D 头部重建和嘴唇运动合成的挑战。我们的方法自动学习将单个肖像图像分解为静态 3D 重建（表示为静态高斯分布）和预测的整个图像 2D 背景。然后，它会根据输入音频生成自然的嘴唇运动，而无需任何运动驱动的先验。训练纯粹由 2D 重建和分数蒸馏损失驱动，没有 3D 监督或地标。实验结果表明，Splat-Portrait 在头像生成和新颖的视图合成方面表现出优越的性能，与之前的作品相比，实现了更好的视觉质量。我们的项目代码和补充文档可通过此 https URL 公开获取。</li>
</ul>

<h3>Title: Quasi Monte Carlo methods enable extremely low-dimensional deep generative models</h3>
<ul>
<li><strong>Authors: </strong>Miles Martinez, Alex H. Williams</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18676">https://arxiv.org/abs/2601.18676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18676">https://arxiv.org/pdf/2601.18676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18676]] Quasi Monte Carlo methods enable extremely low-dimensional deep generative models(https://arxiv.org/abs/2601.18676)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper introduces quasi-Monte Carlo latent variable models (QLVMs): a class of deep generative models that are specialized for finding extremely low-dimensional and interpretable embeddings of high-dimensional datasets. Unlike standard approaches, which rely on a learned encoder and variational lower bounds, QLVMs directly approximate the marginal likelihood by randomized quasi-Monte Carlo integration. While this brute force approach has drawbacks in higher-dimensional spaces, we find that it excels in fitting one, two, and three dimensional deep latent variable models. Empirical results on a range of datasets show that QLVMs consistently outperform conventional variational autoencoders (VAEs) and importance weighted autoencoders (IWAEs) with matched latent dimensionality. The resulting embeddings enable transparent visualization and post hoc analyses such as nonparametric density estimation, clustering, and geodesic path computation, which are nontrivial to validate in higher-dimensional spaces. While our approach is compute-intensive and struggles to generate fine-scale details in complex datasets, it offers a compelling solution for applications prioritizing interpretability and latent space analysis.</li>
<li><strong>摘要：</strong>本文介绍了准蒙特卡罗潜变量模型（QLVM）：一类深度生成模型，专门用于查找高维数据集的极低维且可解释的嵌入。与依赖于学习编码器和变分下界的标准方法不同，QLVM 通过随机准蒙特卡罗积分直接近似边际似然。虽然这种强力方法在高维空间中存在缺点，但我们发现它在拟合一维、二维和三维深度潜变量模型方面表现出色。一系列数据集的实证结果表明，QLVM 始终优于具有匹配潜在维度的传统变分自动编码器 (VAE) 和重要性加权自动编码器 (IWAE)。由此产生的嵌入可以实现透明的可视化和事后分析，例如非参数密度估计、聚类和测地路径计算，这些在高维空间中进行验证非常重要。虽然我们的方法是计算密集型的，并且很难在复杂的数据集中生成精细的细节，但它为优先考虑可解释性和潜在空间分析的应用程序提供了一个引人注目的解决方案。</li>
</ul>

<h3>Title: Are Video Generation Models Geographically Fair? An Attraction-Centric Evaluation of Global Visual Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Xiao Liu, Jiawei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18698">https://arxiv.org/abs/2601.18698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18698">https://arxiv.org/pdf/2601.18698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18698]] Are Video Generation Models Geographically Fair? An Attraction-Centric Evaluation of Global Visual Knowledge(https://arxiv.org/abs/2601.18698)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in text-to-video generation have produced visually compelling results, yet it remains unclear whether these models encode geographically equitable visual knowledge. In this work, we investigate the geo-equity and geographically grounded visual knowledge of text-to-video models through an attraction-centric evaluation. We introduce Geo-Attraction Landmark Probing (GAP), a systematic framework for assessing how faithfully models synthesize tourist attractions from diverse regions, and construct GEOATTRACTION-500, a benchmark of 500 globally distributed attractions spanning varied regions and popularity levels. GAP integrates complementary metrics that disentangle overall video quality from attraction-specific knowledge, including global structural alignment, fine-grained keypoint-based alignment, and vision-language model judgments, all validated against human evaluation. Applying GAP to the state-of-the-art text-to-video model Sora 2, we find that, contrary to common assumptions of strong geographic bias, the model exhibits a relatively uniform level of geographically grounded visual knowledge across regions, development levels, and cultural groupings, with only weak dependence on attraction popularity. These results suggest that current text-to-video models express global visual knowledge more evenly than expected, highlighting both their promise for globally deployed applications and the need for continued evaluation as such systems evolve.</li>
<li><strong>摘要：</strong>文本到视频生成的最新进展已经产生了视觉上引人注目的结果，但仍不清楚这些模型是否编码了地理上公平的视觉知识。在这项工作中，我们通过以吸引力为中心的评估来研究文本到视频模型的地缘公平性和基于地理的视觉知识。我们引入了地理景点地标探测 (GAP)，这是一个系统框架，用于评估模型如何忠实地合成不同地区的旅游景点，并构建了 GEOATTRACTION-500，这是跨越不同地区和受欢迎程度的 500 个全球分布景点的基准。 GAP 集成了互补指标，将整体视频质量与特定景点的知识分开，包括全局结构对齐、基于细粒度关键点的对齐和视觉语言模型判断，所有这些都根据人类评估进行了验证。将 GAP 应用于最先进的文本到视频模型 Sora 2，我们发现，与强烈地理偏见的常见假设相反，该模型在不同地区、发展水平和文化群体中表现出相对统一的地理视觉知识水平，对景点受欢迎程度的依赖程度较弱。这些结果表明，当前的文本到视频模型比预期更均匀地表达了全球视觉知识，凸显了它们对全球部署应用程序的承诺，以及随着此类系统的发展需要持续评估的必要性。</li>
</ul>

<h3>Title: Riemannian AmbientFlow: Towards Simultaneous Manifold Learning and Generative Modeling from Corrupted Data</h3>
<ul>
<li><strong>Authors: </strong>Willem Diepeveen, Oscar Leong</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DG, math.OC, math.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18728">https://arxiv.org/abs/2601.18728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18728">https://arxiv.org/pdf/2601.18728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18728]] Riemannian AmbientFlow: Towards Simultaneous Manifold Learning and Generative Modeling from Corrupted Data(https://arxiv.org/abs/2601.18728)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Modern generative modeling methods have demonstrated strong performance in learning complex data distributions from clean samples. In many scientific and imaging applications, however, clean samples are unavailable, and only noisy or linearly corrupted measurements can be observed. Moreover, latent structures, such as manifold geometries, present in the data are important to extract for further downstream scientific analysis. In this work, we introduce Riemannian AmbientFlow, a framework for simultaneously learning a probabilistic generative model and the underlying, nonlinear data manifold directly from corrupted observations. Building on the variational inference framework of AmbientFlow, our approach incorporates data-driven Riemannian geometry induced by normalizing flows, enabling the extraction of manifold structure through pullback metrics and Riemannian Autoencoders. We establish theoretical guarantees showing that, under appropriate geometric regularization and measurement conditions, the learned model recovers the underlying data distribution up to a controllable error and yields a smooth, bi-Lipschitz manifold parametrization. We further show that the resulting smooth decoder can serve as a principled generative prior for inverse problems with recovery guarantees. We empirically validate our approach on low-dimensional synthetic manifolds and on MNIST.</li>
<li><strong>摘要：</strong>现代生成建模方法在从干净样本中学习复杂数据分布方面表现出了强大的性能。然而，在许多科学和成像应用中，无法获得干净的样本，只能观察到噪声或线性损坏的测量结果。此外，数据中存在的潜在结构（例如流形几何形状）对于提取以进行进一步的下游科学分析非常重要。在这项工作中，我们介绍了黎曼环境流（Riemannian AmbientFlow），这是一个直接从损坏的观测中同时学习概率生成模型和底层非线性数据流形的框架。基于 AmbientFlow 的变分推理框架，我们的方法结合了由归一化流引起的数据驱动的黎曼几何，从而能够通过回拉度量和黎曼自动编码器提取流形结构。我们建立了理论保证，表明在适当的几何正则化和测量条件下，学习模型将基础数据分布恢复到可控误差，并产生平滑的双利普希茨流形参数化。我们进一步表明，由此产生的平滑解码器可以作为具有恢复保证的逆问题的原则生成先验。我们在低维合成流形和 MNIST 上凭经验验证了我们的方法。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
