<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-12-08</h1>
<h3>Title: Breaking Scale Anchoring: Frequency Representation Learning for Accurate High-Resolution Inference from Low-Resolution Training</h3>
<ul>
<li><strong>Authors: </strong>Wenshuo Wang, Fan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05132">https://arxiv.org/abs/2512.05132</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05132">https://arxiv.org/pdf/2512.05132</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05132]] Breaking Scale Anchoring: Frequency Representation Learning for Accurate High-Resolution Inference from Low-Resolution Training(https://arxiv.org/abs/2512.05132)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Zero-Shot Super-Resolution Spatiotemporal Forecasting requires a deep learning model to be trained on low-resolution data and deployed for inference on high-resolution. Existing studies consider maintaining similar error across different resolutions as indicative of successful multi-resolution generalization. However, deep learning models serving as alternatives to numerical solvers should reduce error as resolution increases. The fundamental limitation is, the upper bound of physical law frequencies that low-resolution data can represent is constrained by its Nyquist frequency, making it difficult for models to process signals containing unseen frequency components during high-resolution inference. This results in errors being anchored at low resolution, incorrectly interpreted as successful generalization. We define this fundamental phenomenon as a new problem distinct from existing issues: Scale Anchoring. Therefore, we propose architecture-agnostic Frequency Representation Learning. It alleviates Scale Anchoring through resolution-aligned frequency representations and spectral consistency training: on grids with higher Nyquist frequencies, the frequency response in high-frequency bands of FRL-enhanced variants is more stable. This allows errors to decrease with resolution and significantly outperform baselines within our task and resolution range, while incurring only modest computational overhead.</li>
<li><strong>摘要：</strong>零样本超分辨率时空预测需要深度学习模型在低分辨率数据上进行训练并部署以进行高分辨率推理。现有研究认为在不同分辨率之间保持相似的误差表明多分辨率泛化成功。然而，作为数值求解器替代方案的深度学习模型应该会随着分辨率的提高而减少误差。根本的限制是，低分辨率数据所能表示的物理定律频率的上限受到其奈奎斯特频率的限制，使得模型在高分辨率推理过程中难以处理包含不可见频率分量的信号。这会导致错误被锚定在低分辨率，错误地解释为成功的泛化。我们将这一基本现象定义为一个不同于现有问题的新问题：尺度锚定。因此，我们提出了与架构无关的频率表示学习。它通过分辨率对齐的频率表示和频谱一致性训练来减轻尺度锚定：在具有较高奈奎斯特频率的网格上，FRL 增强型变体的高频带中的频率响应更加稳定。这使得错误随着分辨率的增加而减少，并且在我们的任务和分辨率范围内显着优于基线，同时仅产生适度的计算开销。</li>
</ul>

<h3>Title: Spatiotemporal Satellite Image Downscaling with Transfer Encoders and Autoregressive Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Yang Xiang, Jingwen Zhong, Yige Yan, Petros Koutrakis, Eric Garshick, Meredith Franklin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05139">https://arxiv.org/abs/2512.05139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05139">https://arxiv.org/pdf/2512.05139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05139]] Spatiotemporal Satellite Image Downscaling with Transfer Encoders and Autoregressive Generative Models(https://arxiv.org/abs/2512.05139)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present a transfer-learning generative downscaling framework to reconstruct fine resolution satellite images from coarse scale inputs. Our approach combines a lightweight U-Net transfer encoder with a diffusion-based generative model. The simpler U-Net is first pretrained on a long time series of coarse resolution data to learn spatiotemporal representations; its encoder is then frozen and transferred to a larger downscaling model as physically meaningful latent features. Our application uses NASA's MERRA-2 reanalysis as the low resolution source domain (50 km) and the GEOS-5 Nature Run (G5NR) as the high resolution target (7 km). Our study area included a large area in Asia, which was made computationally tractable by splitting into two subregions and four seasons. We conducted domain similarity analysis using Wasserstein distances confirmed minimal distributional shift between MERRA-2 and G5NR, validating the safety of parameter frozen transfer. Across seasonal regional splits, our model achieved excellent performance (R2 = 0.65 to 0.94), outperforming comparison models including deterministic U-Nets, variational autoencoders, and prior transfer learning baselines. Out of data evaluations using semivariograms, ACF/PACF, and lag-based RMSE/R2 demonstrated that the predicted downscaled images preserved physically consistent spatial variability and temporal autocorrelation, enabling stable autoregressive reconstruction beyond the G5NR record. These results show that transfer enhanced diffusion models provide a robust and physically coherent solution for downscaling a long time series of coarse resolution images with limited training periods. This advancement has significant implications for improving environmental exposure assessment and long term environmental monitoring.</li>
<li><strong>摘要：</strong>我们提出了一个迁移学习生成缩小框架，用于从粗尺度输入重建高分辨率卫星图像。我们的方法将轻量级 U-Net 传输编码器与基于扩散的生成模型相结合。更简单的 U-Net 首先在长时间序列的粗分辨率数据上进行预训练，以学习时空表示；然后它的编码器被冻结并转移到更大的缩小模型作为物理上有意义的潜在特征。我们的应用程序使用 NASA 的 MERRA-2 再分析作为低分辨率源域（50 公里），使用 GEOS-5 Nature Run (G5NR) 作为高分辨率目标域（7 公里）。我们的研究区域包括亚洲的一大片区域，通过划分为两个分区和四个季节，使其易于计算处理。我们使用 Wasserstein 距离进行域相似性分析，确认了 MERRA-2 和 G5NR 之间的最小分布偏移，验证了参数冻结转移的安全性。在季节性区域划分中，我们的模型取得了出色的性能（R2 = 0.65 至 0.94），优于包括确定性 U-Net、变分自动编码器和先前迁移学习基线在内的比较模型。使用半变异函数、ACF/PACF 和基于滞后的 RMSE/R2 进行的数据评估表明，预测的缩小图像保留了物理上一致的空间变异性和时间自相关性，从而实现了超出 G5NR 记录的稳定自回归重建。这些结果表明，传输增强扩散模型为在有限的训练周期内缩小长时间序列的粗分辨率图像提供了稳健且物理相干的解决方案。这一进展对于改善环境暴露评估和长期环境监测具有重大意义。</li>
</ul>

<h3>Title: FlowEO: Generative Unsupervised Domain Adaptation for Earth Observation</h3>
<ul>
<li><strong>Authors: </strong>Georges Le Bellier (CEDRIC - VERTIGO, Cnam), Nicolas Audebert (LaSTIG, IGN, CEDRIC - VERTIGO)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05140">https://arxiv.org/abs/2512.05140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05140">https://arxiv.org/pdf/2512.05140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05140]] FlowEO: Generative Unsupervised Domain Adaptation for Earth Observation(https://arxiv.org/abs/2512.05140)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The increasing availability of Earth observation data offers unprecedented opportunities for large-scale environmental monitoring and analysis. However, these datasets are inherently heterogeneous, stemming from diverse sensors, geographical regions, acquisition times, and atmospheric conditions. Distribution shifts between training and deployment domains severely limit the generalization of pretrained remote sensing models, making unsupervised domain adaptation (UDA) crucial for real-world applications. We introduce FlowEO, a novel framework that leverages generative models for image-space UDA in Earth observation. We leverage flow matching to learn a semantically preserving mapping that transports from the source to the target image distribution. This allows us to tackle challenging domain adaptation configurations for classification and semantic segmentation of Earth observation images. We conduct extensive experiments across four datasets covering adaptation scenarios such as SAR to optical translation and temporal and semantic shifts caused by natural disasters. Experimental results demonstrate that FlowEO outperforms existing image translation approaches for domain adaptation while achieving on-par or better perceptual image quality, highlighting the potential of flow-matching-based UDA for remote sensing.</li>
<li><strong>摘要：</strong>地球观测数据的不断增加为大规模环境监测和分析提供了前所未有的机会。然而，这些数据集本质上是异构的，源于不同的传感器、地理区域、采集时间和大气条件。训练域和部署域之间的分布变化严重限制了预训练遥感模型的泛化，使得无监督域适应（UDA）对于实际应用至关重要。我们介绍 FlowEO，这是一种新颖的框架，它利用地球观测中图像空间 UDA 的生成模型。我们利用流匹配来学习从源图像分布传输到目标图像分布的语义保留映射。这使我们能够解决具有挑战性的域适应配置，以进行地球观测图像的分类和语义分割。我们在四个数据集上进行了广泛的实验，涵盖了 SAR 到光学翻译以及自然灾害引起的时间和语义变化等适应场景。实验结果表明，FlowEO 在域适应方面优于现有的图像转换方法，同时实现了同等或更好的感知图像质量，凸显了基于流匹配的 UDA 在遥感方面的潜力。</li>
</ul>

<h3>Title: TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows</h3>
<ul>
<li><strong>Authors: </strong>Zhenglin Cheng, Peng Sun, Jianguo Li, Tao Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05150">https://arxiv.org/abs/2512.05150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05150">https://arxiv.org/pdf/2512.05150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05150]] TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows(https://arxiv.org/abs/2512.05150)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in large multi-modal generative models have demonstrated impressive capabilities in multi-modal generation, including image and video generation. These models are typically built upon multi-step frameworks like diffusion and flow matching, which inherently limits their inference efficiency (requiring 40-100 Number of Function Evaluations (NFEs)). While various few-step methods aim to accelerate the inference, existing solutions have clear limitations. Prominent distillation-based methods, such as progressive and consistency distillation, either require an iterative distillation procedure or show significant degradation at very few steps (< 4-NFE). Meanwhile, integrating adversarial training into distillation (e.g., DMD/DMD2 and SANA-Sprint) to enhance performance introduces training instability, added complexity, and high GPU memory overhead due to the auxiliary trained models. To this end, we propose TwinFlow, a simple yet effective framework for training 1-step generative models that bypasses the need of fixed pretrained teacher models and avoids standard adversarial networks during training, making it ideal for building large-scale, efficient models. On text-to-image tasks, our method achieves a GenEval score of 0.83 in 1-NFE, outperforming strong baselines like SANA-Sprint (a GAN loss-based framework) and RCGM (a consistency-based framework). Notably, we demonstrate the scalability of TwinFlow by full-parameter training on Qwen-Image-20B and transform it into an efficient few-step generator. With just 1-NFE, our approach matches the performance of the original 100-NFE model on both the GenEval and DPG-Bench benchmarks, reducing computational cost by $100\times$ with minor quality degradation. Project page is available at this https URL.</li>
<li><strong>摘要：</strong>大型多模态生成模型的最新进展展示了多模态生成（包括图像和视频生成）方面令人印象深刻的能力。这些模型通常建立在扩散和流匹配等多步骤框架之上，这本质上限制了它们的推理效率（需要 40-100 次功能评估 (NFE)）。虽然各种几步方法旨在加速推理，但现有的解决方案具有明显的局限性。著名的基于蒸馏的方法，例如渐进式蒸馏和稠度蒸馏，要么需要迭代蒸馏程序，要么在很少的步骤中表现出显着的降解（< 4-NFE）。同时，将对抗性训练集成到蒸馏中（例如 DMD/DMD2 和 SANA-Sprint）以增强性能，会由于辅助训练模型而导致训练不稳定、增加复杂性和高 GPU 内存开销。为此，我们提出了 TwinFlow，这是一种简单而有效的训练一步生成模型的框架，它绕过了固定的预训练教师模型的需要，并避免了训练期间的标准对抗网络，使其成为构建大规模、高效模型的理想选择。在文本到图像任务中，我们的方法在 1-NFE 中取得了 0.83 的 GenEval 分数，优于 SANA-Sprint（基于 GAN 损失的框架）和 RCGM（基于一致性的框架）等强基线。值得注意的是，我们通过在 Qwen-Image-20B 上进行全参数训练来展示 TwinFlow 的可扩展性，并将其转变为高效的少步生成器。仅使用 1-NFE，我们的方法就可以在 GenEval 和 DPG-Bench 基准测试中与原始 100-NFE 模型的性能相匹配，从而将计算成本降低 100 美元\倍，同时质量下降较小。项目页面可通过此 https URL 获取。</li>
</ul>

<h3>Title: EFDiT: Efficient Fine-grained Image Generation Using Diffusion Transformer Models</h3>
<ul>
<li><strong>Authors: </strong>Kun Wang, Donglin Di, Tonghua Su, Lei Fan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05152">https://arxiv.org/abs/2512.05152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05152">https://arxiv.org/pdf/2512.05152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05152]] EFDiT: Efficient Fine-grained Image Generation Using Diffusion Transformer Models(https://arxiv.org/abs/2512.05152)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generation</a></li>
<li><strong>Abstract: </strong>Diffusion models are highly regarded for their controllability and the diversity of images they generate. However, class-conditional generation methods based on diffusion models often focus on more common categories. In large-scale fine-grained image generation, issues of semantic information entanglement and insufficient detail in the generated images still persist. This paper attempts to introduce a concept of a tiered embedder in fine-grained image generation, which integrates semantic information from both super and child classes, allowing the diffusion model to better incorporate semantic information and address the issue of semantic entanglement. To address the issue of insufficient detail in fine-grained images, we introduce the concept of super-resolution during the perceptual information generation stage, enhancing the detailed features of fine-grained images through enhancement and degradation models. Furthermore, we propose an efficient ProAttention mechanism that can be effectively implemented in the diffusion model. We evaluate our method through extensive experiments on public benchmarks, demonstrating that our approach outperforms other state-of-the-art fine-tuning methods in terms of performance.</li>
<li><strong>摘要：</strong>扩散模型因其可控性和生成图像的多样性而受到高度重视。然而，基于扩散模型的类条件生成方法通常关注更常见的类别。在大规模细粒度图像生成中，生成的图像中语义信息纠缠和细节不足的问题仍然存在。本文试图在细粒度图像生成中引入分层嵌入器的概念，它集成了超类和子类的语义信息，使扩散模型能够更好地融合语义信息并解决语义纠缠问题。为了解决细粒度图像细节不足的问题，我们在感知信息生成阶段引入超分辨率的概念，通过增强和退化模型增强细粒度图像的细节特征。此外，我们提出了一种有效的 ProAttention 机制，可以在扩散模型中有效实现。我们通过对公共基准进行大量实验来评估我们的方法，证明我们的方法在性能方面优于其他最先进的微调方法。</li>
</ul>

<h3>Title: DEAR: Dataset for Evaluating the Aesthetics of RenderingDEAR: Dataset for Evaluating the Aesthetics of Rendering</h3>
<ul>
<li><strong>Authors: </strong>Vsevolod Plohotnuk, Artyom Panshin, Nikola Banić, Simone Bianco, Michael Freeman, Egor Ershov</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05209">https://arxiv.org/abs/2512.05209</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05209">https://arxiv.org/pdf/2512.05209</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05209]] DEAR: Dataset for Evaluating the Aesthetics of RenderingDEAR: Dataset for Evaluating the Aesthetics of Rendering(https://arxiv.org/abs/2512.05209)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Traditional Image Quality Assessment~(IQA) focuses on quantifying technical degradations such as noise, blur, or compression artifacts, using both full-reference and no-reference objective metrics. However, evaluation of rendering aesthetics, a growing domain relevant to photographic editing, content creation, and AI-generated imagery, remains underexplored due to the lack of datasets that reflect the inherently subjective nature of style preference. In this work, a novel benchmark dataset designed to model human aesthetic judgments of image rendering styles is introduced: the Dataset for Evaluating the Aesthetics of Rendering (DEAR). Built upon the MIT-Adobe FiveK dataset, DEAR incorporates pairwise human preference scores collected via large-scale crowdsourcing, with each image pair evaluated by 25 distinct human evaluators with a total of 13,648 of them participating overall. These annotations capture nuanced, context-sensitive aesthetic preferences, enabling the development and evaluation of models that go beyond traditional distortion-based IQA, focusing on a new task: Evaluation of Aesthetics of Rendering (EAR). The data collection pipeline is described, human voting patterns are analyzed, and multiple use cases are outlined, including style preference prediction, aesthetic benchmarking, and personalized aesthetic modeling. To the best of the authors' knowledge, DEAR is the first dataset to systematically address image aesthetics of rendering assessment grounded in subjective human preferences. A subset of 100 images with markup for them is published on HuggingFace (this http URL).</li>
<li><strong>摘要：</strong>传统的图像质量评估（IQA）侧重于使用全参考和无参考客观指标来量化技术退化，例如噪声、模糊或压缩伪影。然而，由于缺乏反映风格偏好固有的主观本质的数据集，渲染美学的评估是一个与摄影编辑、内容创建和人工智能生成图像相关的不断发展的领域，仍然没有得到充分探索。在这项工作中，引入了一个新颖的基准数据集，旨在模拟人类对图像渲染风格的审美判断：渲染美学评估数据集（DEAR）。 DEAR 建立在 MIT-Adobe FiveK 数据集的基础上，结合了通过大规模众包收集的成对人类偏好评分，每对图像均由 25 名不同的人类评估员进行评估，总共有 13,648 名评估员参与。这些注释捕获了细致入微、上下文敏感的审美偏好，从而能够开发和评估超越传统基于失真的 IQA 的模型，重点关注一项新任务：渲染美学评估 (EAR)。描述了数据收集管道，分析了人类投票模式，并概述了多个用例，包括风格偏好预测、审美基准测试和个性化审美建模。据作者所知，DEAR 是第一个系统地解决基于人类主观偏好的渲染评估图像美学的数据集。 HuggingFace（此 http URL）上发布了包含 100 张图像及其标记的子集。</li>
</ul>

<h3>Title: MAR-FL: A Communication Efficient Peer-to-Peer Federated Learning System</h3>
<ul>
<li><strong>Authors: </strong>Felix Mulitze, Herbert Woisetschläger, Hans Arno Jacobsen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05234">https://arxiv.org/abs/2512.05234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05234">https://arxiv.org/pdf/2512.05234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05234]] MAR-FL: A Communication Efficient Peer-to-Peer Federated Learning System(https://arxiv.org/abs/2512.05234)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The convergence of next-generation wireless systems and distributed Machine Learning (ML) demands Federated Learning (FL) methods that remain efficient and robust with wireless connected peers and under network churn. Peer-to-peer (P2P) FL removes the bottleneck of a central coordinator, but existing approaches suffer from excessive communication complexity, limiting their scalability in practice. We introduce MAR-FL, a novel P2P FL system that leverages iterative group-based aggregation to substantially reduce communication overhead while retaining resilience to churn. MAR-FL achieves communication costs that scale as O(N log N), contrasting with the O(N^2) complexity of previously existing baselines, and thereby maintains effectiveness especially as the number of peers in an aggregation round grows. The system is robust towards unreliable FL clients and can integrate private computing.</li>
<li><strong>摘要：</strong>下一代无线系统和分布式机器学习 (ML) 的融合要求联邦学习 (FL) 方法能够在无线连接的对等点和网络扰动的情况下保持高效和稳健。点对点（P2P）FL消除了中央协调器的瓶颈，但现有方法存在通信复杂性过高的问题，限制了它们在实践中的可扩展性。我们推出了 MAR-FL，这是一种新颖的 P2P FL 系统，它利用基于迭代组的聚合来大幅减少通信开销，同时保持对流失的弹性。与之前存在的基线的 O(N^2) 复杂性相比，MAR-FL 实现了 O(N log N) 的通信成本，从而保持了有效性，尤其是随着聚合轮中对等点数量的增长。该系统对于不可靠的 FL 客户端具有鲁棒性，并且可以集成私有计算。</li>
</ul>

<h3>Title: IE2Video: Adapting Pretrained Diffusion Models for Event-Based Video Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Dmitrii Torbunov, Onur Okuducu, Yi Huang, Odera Dim, Rebecca Coles, Yonggang Cui, Yihui Ren</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05240">https://arxiv.org/abs/2512.05240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05240">https://arxiv.org/pdf/2512.05240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05240]] IE2Video: Adapting Pretrained Diffusion Models for Event-Based Video Reconstruction(https://arxiv.org/abs/2512.05240)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Continuous video monitoring in surveillance, robotics, and wearable systems faces a fundamental power constraint: conventional RGB cameras consume substantial energy through fixed-rate capture. Event cameras offer sparse, motion-driven sensing with low power consumption, but produce asynchronous event streams rather than RGB video. We propose a hybrid capture paradigm that records sparse RGB keyframes alongside continuous event streams, then reconstructs full RGB video offline -- reducing capture power consumption while maintaining standard video output for downstream applications. We introduce the Image and Event to Video (IE2Video) task: reconstructing RGB video sequences from a single initial frame and subsequent event camera data. We investigate two architectural strategies: adapting an autoregressive model (HyperE2VID) for RGB generation, and injecting event representations into a pretrained text-to-video diffusion model (LTX) via learned encoders and low-rank adaptation. Our experiments demonstrate that the diffusion-based approach achieves 33\% better perceptual quality than the autoregressive baseline (0.283 vs 0.422 LPIPS). We validate our approach across three event camera datasets (BS-ERGB, HS-ERGB far/close) at varying sequence lengths (32-128 frames), demonstrating robust cross-dataset generalization with strong performance on unseen capture configurations.</li>
<li><strong>摘要：</strong>监控、机器人和可穿戴系统中的连续视频监控面临着基本的功耗限制：传统 RGB 摄像机通过固定速率捕获消耗大量能量。事件摄像机提供稀疏、运动驱动的传感且功耗低，但生成异步事件流而不是 RGB 视频。我们提出了一种混合捕获范例，可记录稀疏 RGB 关键帧和连续事件流，然后离线重建完整的 RGB 视频 - 降低捕获功耗，同时保持下游应用的标准视频输出。我们介绍图像和事件到视频 (IE2Video) 任务：从单个初始帧和后续事件摄像机数据重建 RGB 视频序列。我们研究了两种架构策略：采用自回归模型 (HyperE2VID) 来生成 RGB，以及通过学习编码器和低秩适应将事件表示注入到预训练的文本到视频扩散模型 (LTX) 中。我们的实验表明，基于扩散的方法比自回归基线的感知质量提高了 33%（0.283 vs 0.422 LPIPS）。我们在不同序列长度（32-128 帧）的三个事件相机数据集（BS-ERGB、HS-ERGB 远/近）上验证了我们的方法，展示了强大的跨数据集泛化能力以及在未见过的捕获配置上的强大性能。</li>
</ul>

<h3>Title: CARD: Correlation Aware Restoration with Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Niki Nezakati, Arnab Ghosh, Amit Roy-Chowdhury, Vishwanath Saragadam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05268">https://arxiv.org/abs/2512.05268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05268">https://arxiv.org/pdf/2512.05268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05268]] CARD: Correlation Aware Restoration with Diffusion(https://arxiv.org/abs/2512.05268)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, super-resolution</a></li>
<li><strong>Abstract: </strong>Denoising diffusion models have achieved state-of-the-art performance in image restoration by modeling the process as sequential denoising steps. However, most approaches assume independent and identically distributed (i.i.d.) Gaussian noise, while real-world sensors often exhibit spatially correlated noise due to readout mechanisms, limiting their practical effectiveness. We introduce Correlation Aware Restoration with Diffusion (CARD), a training-free extension of DDRM that explicitly handles correlated Gaussian noise. CARD first whitens the noisy observation, which converts the noise into an i.i.d. form. Then, the diffusion restoration steps are replaced with noise-whitened updates, which inherits DDRM's closed-form sampling efficiency while now being able to handle correlated noise. To emphasize the importance of addressing correlated noise, we contribute CIN-D, a novel correlated noise dataset captured across diverse illumination conditions to evaluate restoration methods on real rolling-shutter sensor noise. This dataset fills a critical gap in the literature for experimental evaluation with real-world correlated noise. Experiments on standard benchmarks with synthetic correlated noise and on CIN-D demonstrate that CARD consistently outperforms existing methods across denoising, deblurring, and super-resolution tasks.</li>
<li><strong>摘要：</strong>通过将过程建模为连续的去噪步骤，去噪扩散模型在图像恢复方面取得了最先进的性能。然而，大多数方法都假设独立同分布（i.i.d.）高斯噪声，而现实世界的传感器通常由于读出机制而表现出空间相关噪声，限制了它们的实际有效性。我们引入了相关感知扩散恢复 (CARD)，这是 DDRM 的免训练扩展，可显式处理相关高斯噪声。 CARD 首先对噪声观测值进行白化，将噪声转换为独立同分布。形式。然后，扩散恢复步骤被噪声白化更新所取代，这继承了 DDRM 的封闭式采样效率，同时现在能够处理相关噪声。为了强调解决相关噪声的重要性，我们贡献了 CIN-D，这是一种在不同照明条件下捕获的新型相关噪声数据集，用于评估真实卷帘快门传感器噪声的恢复方法。该数据集填补了文献中使用现实世界相关噪声进行实验评估的关键空白。在具有合成相关噪声的标准基准和 CIN-D 上进行的实验表明，CARD 在去噪、去模糊和超分辨率任务方面始终优于现有方法。</li>
</ul>

<h3>Title: When Forgetting Builds Reliability: LLM Unlearning for Reliable Hardware Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Yiwen Liang, Qiufeng Li, Shikai Wang, Weidong Cao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05341">https://arxiv.org/abs/2512.05341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05341">https://arxiv.org/pdf/2512.05341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05341]] When Forgetting Builds Reliability: LLM Unlearning for Reliable Hardware Code Generation(https://arxiv.org/abs/2512.05341)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown strong potential in accelerating digital hardware design through automated code generation. Yet, ensuring their reliability remains a critical challenge, as existing LLMs trained on massive heterogeneous datasets often exhibit problematic memorization of proprietary intellectual property (IP), contaminated benchmarks, and unsafe coding patterns. To mitigate these risks, we propose a novel unlearning framework tailored for LLM-based hardware code generation. Our method combines (i) a syntax-preserving unlearning strategy that safeguards the structural integrity of hardware code during forgetting, and (ii) a fine-grained floor-aware selective loss that enables precise and efficient removal of problematic knowledge. This integration achieves effective unlearning without degrading LLM code generation capabilities. Extensive experiments show that our framework supports forget sets up to 3x larger, typically requiring only a single training epoch, while preserving both syntactic correctness and functional integrity of register-transfer level (RTL) codes. Our work paves an avenue towards reliable LLM-assisted hardware design.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在通过自动代码生成加速数字硬件设计方面显示出强大的潜力。然而，确保其可靠性仍然是一个关键挑战，因为现有的法学硕士在大规模异构数据集上接受培训，经常表现出对专有知识产权 (IP) 的记忆有问题、基准受到污染以及不安全的编码模式。为了减轻这些风险，我们提出了一种为基于 LLM 的硬件代码生成量身定制的新颖的遗忘框架。我们的方法结合了（i）保留语法的遗忘策略，在遗忘过程中保护硬件代码的结构完整性，以及（ii）细粒度的地板感知选择性损失，能够精确有效地删除有问题的知识。这种集成实现了有效的忘却，而不会降低 LLM 代码生成能力。大量实验表明，我们的框架支持高达 3 倍大的遗忘集，通常只需要一个训练周期，同时保留寄存器传输级 (RTL) 代码的语法正确性和功能完整性。我们的工作为可靠的法学硕士辅助硬件设计铺平了道路。</li>
</ul>

<h3>Title: SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling</h3>
<ul>
<li><strong>Authors: </strong>Elisabetta Fedele, Francis Engelmann, Ian Huang, Or Litany, Marc Pollefeys, Leonidas Guibas</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05343">https://arxiv.org/abs/2512.05343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05343">https://arxiv.org/pdf/2512.05343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05343]] SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling(https://arxiv.org/abs/2512.05343)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative methods for 3D assets have recently achieved remarkable progress, yet providing intuitive and precise control over the object geometry remains a key challenge. Existing approaches predominantly rely on text or image prompts, which often fall short in geometric specificity: language can be ambiguous, and images are cumbersome to edit. In this work, we introduce SpaceControl, a training-free test-time method for explicit spatial control of 3D generation. Our approach accepts a wide range of geometric inputs, from coarse primitives to detailed meshes, and integrates seamlessly with modern pre-trained generative models without requiring any additional training. A controllable parameter lets users trade off between geometric fidelity and output realism. Extensive quantitative evaluation and user studies demonstrate that SpaceControl outperforms both training-based and optimization-based baselines in geometric faithfulness while preserving high visual quality. Finally, we present an interactive user interface that enables online editing of superquadrics for direct conversion into textured 3D assets, facilitating practical deployment in creative workflows. Find our project page at this https URL</li>
<li><strong>摘要：</strong>3D 资产的生成方法最近取得了显着的进展，但提供对对象几何形状的直观和精确的控制仍然是一个关键挑战。现有的方法主要依赖于文本或图像提示，而这些提示往往缺乏几何特异性：语言可能含糊不清，图像编辑起来也很麻烦。在这项工作中，我们介绍了 SpaceControl，这是一种无需训练的测试时间方法，用于 3D 生成的显式空间控制。我们的方法接受广泛的几何输入，从粗糙的基元到详细的网格，并与现代预训练的生成模型无缝集成，无需任何额外的训练。可控参数让用户可以在几何保真度和输出真实感之间进行权衡。广泛的定量评估和用户研究表明，SpaceControl 在几何忠实度方面优于基于训练和基于优化的基线，同时保持高视觉质量。最后，我们提出了一个交互式用户界面，可以在线编辑超二次曲面以直接转换为纹理 3D 资源，从而促进创意工作流程中的实际部署。在此 https URL 找到我们的项目页面</li>
</ul>

<h3>Title: China Regional 3km Downscaling Based on Residual Corrective Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Honglu Sun, Hao Jing, Zhixiang Dai, Sa Xiao, Wei Xue, Jian Sun, Qifeng Lu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.ao-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05377">https://arxiv.org/abs/2512.05377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05377">https://arxiv.org/pdf/2512.05377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05377]] China Regional 3km Downscaling Based on Residual Corrective Diffusion Model(https://arxiv.org/abs/2512.05377)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generative</a></li>
<li><strong>Abstract: </strong>A fundamental challenge in numerical weather prediction is to efficiently produce high-resolution forecasts. A common solution is applying downscaling methods, which include dynamical downscaling and statistical downscaling, to the outputs of global models. This work focuses on statistical downscaling, which establishes statistical relationships between low-resolution and high-resolution historical data using statistical models. Deep learning has emerged as a powerful tool for this task, giving rise to various high-performance super-resolution models, which can be directly applied for downscaling, such as diffusion models and Generative Adversarial Networks. This work relies on a diffusion-based downscaling framework named CorrDiff. In contrast to the original work of CorrDiff, the region considered in this work is nearly 20 times larger, and we not only consider surface variables as in the original work, but also encounter high-level variables (six pressure levels) as target downscaling variables. In addition, a global residual connection is added to improve accuracy. In order to generate the 3km forecasts for the China region, we apply our trained models to the 25km global grid forecasts of CMA-GFS, an operational global model of the China Meteorological Administration (CMA), and SFF, a data-driven deep learning-based weather model developed from Spherical Fourier Neural Operators (SFNO). CMA-MESO, a high-resolution regional model, is chosen as the baseline model. The experimental results demonstrate that the forecasts downscaled by our method generally outperform the direct forecasts of CMA-MESO in terms of MAE for the target variables. Our forecasts of radar composite reflectivity show that CorrDiff, as a generative model, can generate fine-scale details that lead to more realistic predictions compared to the corresponding deterministic regression models.</li>
<li><strong>摘要：</strong>数值天气预报的一个基本挑战是有效地生成高分辨率预报。常见的解决方案是将降尺度方法（包括动态降尺度和统计降尺度）应用于全局模型的输出。这项工作的重点是统计降尺度，即使用统计模型在低分辨率和高分辨率历史数据之间建立统计关系。深度学习已成为完成此任务的强大工具，催生了各种高性能超分辨率模型，可直接应用于降尺度，例如扩散模型和生成对抗网络。这项工作依赖于名为 CorrDiff 的基于扩散的缩小框架。与 CorrDiff 的原始工作相比，本工作考虑的区域大了近 20 倍，并且我们不仅像原始工作中那样考虑表面变量，而且还遇到高级变量（六个压力级别）作为目标降尺度变量。此外，还添加了全局残差连接以提高准确性。为了生成中国地区的 3 公里预报，我们将经过训练的模型应用于 CMA-GFS（中国气象局 (CMA) 的全球业务模型）和 SFF（一种由球面傅里叶神经算子 (SFNO) 开发的数据驱动的基于深度学习的天气模型）的 25 公里全球网格预报。选择高分辨率区域模型 CMA-MESO 作为基线模型。实验结果表明，就目标变量的 MAE 而言，通过我们的方法缩小的预测通常优于 CMA-MESO 的直接预测。我们对雷达复合反射率的预测表明，CorrDiff 作为一种生成模型，可以生成精细尺度的细节，与相应的确定性回归模型相比，这些细节可以带来更真实的预测。</li>
</ul>

<h3>Title: Delving into Latent Spectral Biasing of Video VAEs for Superior Diffusability</h3>
<ul>
<li><strong>Authors: </strong>Shizhan Liu, Xinran Deng, Zhuoyi Yang, Jiayan Teng, Xiaotao Gu, Jie Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05394">https://arxiv.org/abs/2512.05394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05394">https://arxiv.org/pdf/2512.05394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05394]] Delving into Latent Spectral Biasing of Video VAEs for Superior Diffusability(https://arxiv.org/abs/2512.05394)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Latent diffusion models pair VAEs with diffusion backbones, and the structure of VAE latents strongly influences the difficulty of diffusion training. However, existing video VAEs typically focus on reconstruction fidelity, overlooking latent structure. We present a statistical analysis of video VAE latent spaces and identify two spectral properties essential for diffusion training: a spatio-temporal frequency spectrum biased toward low frequencies, and a channel-wise eigenspectrum dominated by a few modes. To induce these properties, we propose two lightweight, backbone-agnostic regularizers: Local Correlation Regularization and Latent Masked Reconstruction. Experiments show that our Spectral-Structured VAE (SSVAE) achieves a $3\times$ speedup in text-to-video generation convergence and a 10\% gain in video reward, outperforming strong open-source VAEs. The code is available at this https URL.</li>
<li><strong>摘要：</strong>潜在扩散模型将 VAE 与扩散主干配对，并且 VAE 潜在结构的结构强烈影响扩散训练的难度。然而，现有的视频 VAE 通常侧重于重建保真度，而忽略了潜在结构。我们对视频 VAE 潜在空间进行了统计分析，并确定了扩散训练所必需的两个频谱特性：偏向低频的时空频谱，以及由几种模式主导的通道特征谱。为了引入这些属性，我们提出了两种轻量级的、与主干网络无关的正则化器：局部相关正则化和潜在掩模重建。实验表明，我们的谱结构 VAE (SSVAE) 在文本到视频生成收敛方面实现了 $3\times$ 加速，视频奖励提高了 10\%，优于强大的开源 VAE。该代码可从此 https URL 获取。</li>
</ul>

<h3>Title: RevoNAD: Reflective Evolutionary Exploration for Neural Architecture Design</h3>
<ul>
<li><strong>Authors: </strong>Gyusam Chang, Jeongyoon Yoon, Shin han yi, JaeHyeok Lee, Sujin Jang, Sangpil Kim</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05403">https://arxiv.org/abs/2512.05403</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05403">https://arxiv.org/pdf/2512.05403</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05403]] RevoNAD: Reflective Evolutionary Exploration for Neural Architecture Design(https://arxiv.org/abs/2512.05403)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent progress in leveraging large language models (LLMs) has enabled Neural Architecture Design (NAD) systems to generate new architecture not limited from manually predefined search space. Nevertheless, LLM-driven generation remains challenging: the token-level design loop is discrete and non-differentiable, preventing feedback from smoothly guiding architectural improvement. These methods, in turn, commonly suffer from mode collapse into redundant structures or drift toward infeasible designs when constructive reasoning is not well grounded. We introduce RevoNAD, a reflective evolutionary orchestrator that effectively bridges LLM-based reasoning with feedback-aligned architectural search. First, RevoNAD presents a Multi-round Multi-expert Consensus to transfer isolated design rules into meaningful architectural clues. Then, Adaptive Reflective Exploration adjusts the degree of exploration leveraging reward variance; it explores when feedback is uncertain and refines when stability is reached. Finally, Pareto-guided Evolutionary Selection effectively promotes architectures that jointly optimize accuracy, efficiency, latency, confidence, and structural diversity. Across CIFAR10, CIFAR100, ImageNet16-120, COCO-5K, and Cityscape, RevoNAD achieves state-of-the-art performance. Ablation and transfer studies further validate the effectiveness of RevoNAD in allowing practically reliable, and deployable neural architecture design.</li>
<li><strong>摘要：</strong>利用大型语言模型 (LLM) 的最新进展使神经架构设计 (NAD) 系统能够生成新的架构，而不受手动预定义搜索空间的限制。然而，LLM 驱动的生成仍然具有挑战性：代币级设计循环是离散且不可微的，导致反馈无法顺利指导架构改进。反过来，当构造性推理没有充分依据时，这些方法通常会遭受模式崩溃成冗余结构或转向不可行设计的问题。我们推出了 RevoNAD，这是一种反思性进化协调器，可以有效地将基于 LLM 的推理与反馈一致的架构搜索联系起来。首先，RevoNAD 提出了多轮多专家共识，将孤立的设计规则转化为有意义的架构线索。然后，自适应反思探索利用奖励方差调整探索程度；当反馈不确定时，它会进行探索；当反馈达到稳定时，它会进行细化。最后，帕累托引导的进化选择有效地促进了联合优化准确性、效率、延迟、置信度和结构多样性的架构。在 CIFAR10、CIFAR100、ImageNet16-120、COCO-5K 和 Cityscape 中，RevoNAD 实现了最先进的性能。消融和转移研究进一步验证了 RevoNAD 在实现实际可靠且可部署的神经架构设计方面的有效性。</li>
</ul>

<h3>Title: Genetic Algorithms For Parameter Optimization for Disparity Map Generation of Radiata Pine Branch Images</h3>
<ul>
<li><strong>Authors: </strong>Yida Lin, Bing Xue, Mengjie Zhang, Sam Schofield, Richard Green</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05410">https://arxiv.org/abs/2512.05410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05410">https://arxiv.org/pdf/2512.05410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05410]] Genetic Algorithms For Parameter Optimization for Disparity Map Generation of Radiata Pine Branch Images(https://arxiv.org/abs/2512.05410)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Traditional stereo matching algorithms like Semi-Global Block Matching (SGBM) with Weighted Least Squares (WLS) filtering offer speed advantages over neural networks for UAV applications, generating disparity maps in approximately 0.5 seconds per frame. However, these algorithms require meticulous parameter tuning. We propose a Genetic Algorithm (GA) based parameter optimization framework that systematically searches for optimal parameter configurations for SGBM and WLS, enabling UAVs to measure distances to tree branches with enhanced precision while maintaining processing efficiency. Our contributions include: (1) a novel GA-based parameter optimization framework that eliminates manual tuning; (2) a comprehensive evaluation methodology using multiple image quality metrics; and (3) a practical solution for resource-constrained UAV systems. Experimental results demonstrate that our GA-optimized approach reduces Mean Squared Error by 42.86% while increasing Peak Signal-to-Noise Ratio and Structural Similarity by 8.47% and 28.52%, respectively, compared with baseline configurations. Furthermore, our approach demonstrates superior generalization performance across varied imaging conditions, which is critcal for real-world forestry applications.</li>
<li><strong>摘要：</strong>具有加权最小二乘 (WLS) 滤波功能的半全局块匹配 (SGBM) 等传统立体匹配算法在无人机应用中比神经网络具有速度优势，每帧生成视差图大约需要 0.5 秒。然而，这些算法需要细致的参数调整。我们提出了一种基于遗传算法（GA）的参数优化框架，该框架系统地搜索 SGBM 和 WLS 的最佳参数配置，使无人机能够以更高的精度测量到树枝的距离，同时保持处理效率。我们的贡献包括：（1）一种新颖的基于遗传算法的参数优化框架，消除了手动调整； (2) 使用多种图像质量指标的综合评估方法； (3)针对资源受限的无人机系统的实用解决方案。实验结果表明，与基线配置相比，我们的 GA 优化方法将均方误差降低了 42.86%，同时峰值信噪比和结构相似度分别提高了 8.47% 和 28.52%。此外，我们的方法在不同的成像条件下表现出卓越的泛化性能，这对于现实世界的林业应用至关重要。</li>
</ul>

<h3>Title: ParaUni: Enhance Generation in Unified Multimodal Model with Reinforcement-driven Hierarchical Parallel Information Interaction</h3>
<ul>
<li><strong>Authors: </strong>Jiangtong Tan, Lin Liu, Jie Huanng, Xiaopeng Zhang, Qi Tian, Feng Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05422">https://arxiv.org/abs/2512.05422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05422">https://arxiv.org/pdf/2512.05422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05422]] ParaUni: Enhance Generation in Unified Multimodal Model with Reinforcement-driven Hierarchical Parallel Information Interaction(https://arxiv.org/abs/2512.05422)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Unified multimodal models significantly improve visual generation by combining vision-language models (VLMs) with diffusion models. However, existing methods struggle to fully balance sufficient interaction and flexible implementation due to vast representation difference. Considering abundant and hierarchical information in VLM's layers from low-level details to high-level semantics, we propose \textbf{ParaUni}. It extracts features from variants VLM's layers in a \textbf{Para}llel way for comprehensive information interaction and retains a flexible separation architecture to enhance generation in \textbf{Uni}fied multimodal model. Concretely, visual features from all VLM's layers are fed in parallel into a Layer Integration Module (LIM), which efficiently integrates fine-grained details and semantic abstractions and provides the fused representation as a condition to the diffusion model. To further enhance performance, we reveal that these hierarchical layers respond unequally to different rewards in Reinforcement Learning (RL). Crucially, we design a Layer-wise Dynamic Adjustment Mechanism (LDAM) to facilitate multiple reward improvements that aligns the hierarchical properties of these layers using RL. Extensive experiments show ParaUni leverages complementary multi-layer features to substantially improve generation quality and shows strong potential for multiple reward advances during RL stages. Code is available at this https URL.</li>
<li><strong>摘要：</strong>统一的多模态模型通过将视觉语言模型 (VLM) 与扩散模型相结合，显着改善视觉生成。然而，由于巨大的代表性差异，现有的方法很难充分平衡充分的交互和灵活的实施。考虑到VLM层中从低级细节到高级语义的丰富且分层的信息，我们提出\textbf{ParaUni}。它以并行方式从变体 VLM 层中提取特征以进行全面的信息交互，并保留灵活的分离架构以增强统一多模态模型的生成。具体来说，来自所有 VLM 层的视觉特征被并行输入层集成模块 (LIM)，该模块有效地集成细粒度细节和语义抽象，并提供融合表示作为扩散模型的条件。为了进一步提高性能，我们发现这些层次层对强化学习（RL）中不同奖励的响应不同。至关重要的是，我们设计了逐层动态调整机制 (LDAM)，以促进多重奖励改进，使用 RL 来调整这些层的层次属性。大量实验表明，ParaUni 利用互补的多层功能来大幅提高生成质量，并显示出在 RL 阶段实现多重奖励进步的强大潜力。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: EmoStyle: Emotion-Driven Image Stylization</h3>
<ul>
<li><strong>Authors: </strong>Jingyuan Yang, Zihuan Bai, Hui Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05478">https://arxiv.org/abs/2512.05478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05478">https://arxiv.org/pdf/2512.05478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05478]] EmoStyle: Emotion-Driven Image Stylization(https://arxiv.org/abs/2512.05478)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Art has long been a profound medium for expressing emotions. While existing image stylization methods effectively transform visual appearance, they often overlook the emotional impact carried by styles. To bridge this gap, we introduce Affective Image Stylization (AIS), a task that applies artistic styles to evoke specific emotions while preserving content. We present EmoStyle, a framework designed to address key challenges in AIS, including the lack of training data and the emotion-style mapping. First, we construct EmoStyleSet, a content-emotion-stylized image triplet dataset derived from ArtEmis to support AIS. We then propose an Emotion-Content Reasoner that adaptively integrates emotional cues with content to learn coherent style queries. Given the discrete nature of artistic styles, we further develop a Style Quantizer that converts continuous style features into emotion-related codebook entries. Extensive qualitative and quantitative evaluations, including user studies, demonstrate that EmoStyle enhances emotional expressiveness while maintaining content consistency. Moreover, the learned emotion-aware style dictionary is adaptable to other generative tasks, highlighting its potential for broader applications. Our work establishes a foundation for emotion-driven image stylization, expanding the creative potential of AI-generated art.</li>
<li><strong>摘要：</strong>艺术长期以来一直是表达情感的深刻媒介。虽然现有的图像风格化方法有效地改变了视觉外观，但它们往往忽视了风格所带来的情感影响。为了弥补这一差距，我们引入了情感图像风格化（AIS），这项任务应用艺术风格来唤起特定的情感，同时保留内容。我们提出了 EmoStyle，这是一个旨在解决 AIS 中的关键挑战的框架，包括缺乏训练数据和情感风格映射。首先，我们构建 EmoStyleSet，一个源自 ArtEmis 的内容-情感风格化图像三元组数据集，用于支持 AIS。然后，我们提出了一种情感内容推理器，它自适应地将情感线索与内容相结合，以学习连贯的风格查询。考虑到艺术风格的离散性，我们进一步开发了一种风格量化器，将连续的风格特征转换为与情感相关的密码本条目。广泛的定性和定量评估（包括用户研究）表明，EmoStyle 在保持内容一致性的同时增强了情感表达力。此外，学习到的情感感知风格词典适用于其他生成任务，凸显了其更广泛应用的潜力。我们的工作为情感驱动的图像风格化奠定了基础，扩大了人工智能生成艺术的创作潜力。</li>
</ul>

<h3>Title: VOST-SGG: VLM-Aided One-Stage Spatio-Temporal Scene Graph Generation</h3>
<ul>
<li><strong>Authors: </strong>Chinthani Sugandhika, Chen Li, Deepu Rajan, Basura Fernando</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05524">https://arxiv.org/abs/2512.05524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05524">https://arxiv.org/pdf/2512.05524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05524]] VOST-SGG: VLM-Aided One-Stage Spatio-Temporal Scene Graph Generation(https://arxiv.org/abs/2512.05524)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Spatio-temporal scene graph generation (ST-SGG) aims to model objects and their evolving relationships across video frames, enabling interpretable representations for downstream reasoning tasks such as video captioning and visual question answering. Despite recent advancements in DETR-style single-stage ST-SGG models, they still suffer from several key limitations. First, while these models rely on attention-based learnable queries as a core component, these learnable queries are semantically uninformed and instance-agnostically initialized. Second, these models rely exclusively on unimodal visual features for predicate classification. To address these challenges, we propose VOST-SGG, a VLM-aided one-stage ST-SGG framework that integrates the common sense reasoning capabilities of vision-language models (VLMs) into the ST-SGG pipeline. First, we introduce the dual-source query initialization strategy that disentangles what to attend to from where to attend, enabling semantically grounded what-where reasoning. Furthermore, we propose a multi-modal feature bank that fuses visual, textual, and spatial cues derived from VLMs for improved predicate classification. Extensive experiments on the Action Genome dataset demonstrate that our approach achieves state-of-the-art performance, validating the effectiveness of integrating VLM-aided semantic priors and multi-modal features for ST-SGG. We will release the code at this https URL.</li>
<li><strong>摘要：</strong>时空场景图生成（ST-SGG）旨在对视频帧中的对象及其演变关系进行建模，从而为下游推理任务（例如视频字幕和视觉问答）提供可解释的表示。尽管 DETR 式单级 ST-SGG 模型最近取得了进展，但它们仍然存在一些关键限制。首先，虽然这些模型依赖于基于注意力的可学习查询作为核心组件，但这些可学习查询在语义上是不知情的并且与实例无关的初始化。其次，这些模型完全依赖单峰视觉特征进行谓词分类。为了应对这些挑战，我们提出了 VOST-SGG，这是一种 VLM 辅助的单阶段 ST-SGG 框架，它将视觉语言模型 (VLM) 的常识推理功能集成到 ST-SGG 管道中。首先，我们引入双源查询初始化策略，该策略将关注内容与关注地点分开，从而实现基于语义的内容推理。此外，我们提出了一个多模态特征库，它融合了从 VLM 导出的视觉、文本和空间线索，以改进谓词分类。对 Action Genome 数据集的大量实验表明，我们的方法实现了最先进的性能，验证了集成 VLM 辅助语义先验和 ST-SGG 多模态特征的有效性。我们将在此 https URL 发布代码。</li>
</ul>

<h3>Title: Ideal Observer for Segmentation of Dead Leaves Images</h3>
<ul>
<li><strong>Authors: </strong>Swantje Mahncke, Malte Ott</a></li>
<li><strong>Subjects: </strong>cs.CV, math.ST, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05539">https://arxiv.org/abs/2512.05539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05539">https://arxiv.org/pdf/2512.05539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05539]] Ideal Observer for Segmentation of Dead Leaves Images(https://arxiv.org/abs/2512.05539)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The human visual environment is comprised of different surfaces that are distributed in space. The parts of a scene that are visible at any one time are governed by the occlusion of overlapping objects. In this work we consider "dead leaves" models, which replicate these occlusions when generating images by layering objects on top of each other. A dead leaves model is a generative model comprised of distributions for object position, shape, color and texture. An image is generated from a dead leaves model by sampling objects ("leaves") from these distributions until a stopping criterion is reached, usually when the image is fully covered or until a given number of leaves was sampled. Here, we describe a theoretical approach, based on previous work, to derive a Bayesian ideal observer for the partition of a given set of pixels based on independent dead leaves model distributions. Extending previous work, we provide step-by-step explanations for the computation of the posterior probability as well as describe factors that determine the feasibility of practically applying this computation. The dead leaves image model and the associated ideal observer can be applied to study segmentation decisions in a limited number of pixels, providing a principled upper-bound on performance, to which humans and vision algorithms could be compared.</li>
<li><strong>摘要：</strong>人类视觉环境由分布在空间中的不同表面组成。场景中任意时刻可见的部分受到重叠对象遮挡的控制。在这项工作中，我们考虑“死叶”模型，它在通过将对象分层放置在彼此之上来生成图像时复制这些遮挡。枯叶模型是一种生成模型，由对象位置、形状、颜色和纹理的分布组成。通过从这些分布中对对象（“叶子”）进行采样，直到达到停止标准（通常是在图像被完全覆盖时或直到对给定数量的叶子进行采样为止），从死叶模型生成图像。在这里，我们描述了一种基于先前工作的理论方法，用于基于独立的死叶模型分布导出给定像素集的划分的贝叶斯理想观察者。扩展以前的工作，我们为后验概率的计算提供了逐步解释，并描述了决定实际应用此计算的可行性的因素。枯叶图像模型和相关的理想观察者可用于研究有限数量像素的分割决策，提供原则上的性能上限，可以与人类和视觉算法进行比较。</li>
</ul>

<h3>Title: RoBoN: Routed Online Best-of-n for Test-Time Scaling with Multiple LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Geuter, Gregor Kornhardt</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05542">https://arxiv.org/abs/2512.05542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05542">https://arxiv.org/pdf/2512.05542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05542]] RoBoN: Routed Online Best-of-n for Test-Time Scaling with Multiple LLMs(https://arxiv.org/abs/2512.05542)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Best-of-$n$ is a widely used test-time scaling approach for LLM inference. Yet despite evidence that LLMs exhibit complementary strengths across tasks, traditionally best-of-$n$ relies on a single model to generate responses. We propose RoBoN (Routed Online Best-of-$n$), a sequential multi-LLM alternative to the prevailing single-model best-of-$n$. Given a suite of models $\{m_i\}_{i=1}^M$, RoBoN sequentially routes generations one-by-one across models, based on scores computed using a reward model and an agreement signal on the predicted responses. This online routing requires no additional training, keeps compute parity, and works with any plug-in reward model. Across reasoning benchmarks (MATH500, OlympiadBench, MinervaMath, GSM8K, MMLU), RoBoN consistently outperforms standard best-of-$n$ applied to each individual model for larger $n$, with gains of up to 3.4\% in absolute accuracy, and also improves over a uniform multi-model portfolio baseline. Our results indicate that diversity across models can be exploited at inference to improve best-of-$n$ performance over any constituent model alone, providing a simple, training-free path to test-time scaling with multiple LLMs.</li>
<li><strong>摘要：</strong>Best-of-$n$ 是一种广泛用于 LLM 推理的测试时间缩放方法。然而，尽管有证据表明法学硕士在跨任务方面表现出互补的优势，但传统上，best-of-n$ 依赖于单一模型来生成答案。我们提出 RoBoN（Roated Online Best-of-$n$），这是一种连续的多 LLM 替代流行的单一模型 best-of-$n$ 的方法。给定一组模型 $\{m_i\}_{i=1}^M$，RoBoN 根据使用奖励模型计算的分数和预测响应的一致信号，按顺序在模型之间逐一路由世代。这种在线路由不需要额外的训练，保持计算奇偶性，并且可以与任何插件奖励模型一起使用。在推理基准测试（MATH500、OlympiadBench、MinervaMath、GSM8K、MMLU）中，对于较大的 $n$，RoBoN 始终优于应用于每个单独模型的标准 best-of-$n$，绝对准确度提升高达 3.4\%，并且还比统一的多模型组合基线有所改进。我们的结果表明，可以在推理中利用模型之间的多样性来提高任何单独模型的最佳 $n$ 性能，从而为多个法学硕士的测试时间扩展提供简单、免训练的路径。</li>
</ul>

<h3>Title: ProPhy: Progressive Physical Alignment for Dynamic World Simulation</h3>
<ul>
<li><strong>Authors: </strong>Zijun Wang, Panwen Hu, Jing Wang, Terry Jingchen Zhang, Yuhao Cheng, Long Chen, Yiqiang Yan, Zutao Jiang, Hanhui Li, Xiaodan Liang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05564">https://arxiv.org/abs/2512.05564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05564">https://arxiv.org/pdf/2512.05564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05564]] ProPhy: Progressive Physical Alignment for Dynamic World Simulation(https://arxiv.org/abs/2512.05564)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in video generation have shown remarkable potential for constructing world simulators. However, current models still struggle to produce physically consistent results, particularly when handling large-scale or complex dynamics. This limitation arises primarily because existing approaches respond isotropically to physical prompts and neglect the fine-grained alignment between generated content and localized physical cues. To address these challenges, we propose ProPhy, a Progressive Physical Alignment Framework that enables explicit physics-aware conditioning and anisotropic generation. ProPhy employs a two-stage Mixture-of-Physics-Experts (MoPE) mechanism for discriminative physical prior extraction, where Semantic Experts infer semantic-level physical principles from textual descriptions, and Refinement Experts capture token-level physical dynamics. This mechanism allows the model to learn fine-grained, physics-aware video representations that better reflect underlying physical laws. Furthermore, we introduce a physical alignment strategy that transfers the physical reasoning capabilities of vision-language models (VLMs) into the Refinement Experts, facilitating a more accurate representation of dynamic physical phenomena. Extensive experiments on physics-aware video generation benchmarks demonstrate that ProPhy produces more realistic, dynamic, and physically coherent results than existing state-of-the-art methods.</li>
<li><strong>摘要：</strong>视频生成领域的最新进展显示出构建世界模拟器的巨大潜力。然而，当前的模型仍然难以产生物理上一致的结果，特别是在处理大规模或复杂的动力学时。出现这种限制的主要原因是现有方法对物理提示进行各向同性响应，而忽略了生成的内容和局部物理提示之间的细粒度对齐。为了应对这些挑战，我们提出了 ProPhy，一种渐进式物理对齐框架，可实现显式物理感知调节和各向异性生成。 ProPhy 采用两阶段物理专家混合 (MoPE) 机制进行判别性物理先验提取，其中语义专家从文本描述中推断语义级物理原理，而细化专家捕获令牌级物理动态。这种机制允许模型学习细粒度、物理感知的视频表示，从而更好地反映潜在的物理定律。此外，我们引入了一种物理对齐策略，将视觉语言模型（VLM）的物理推理能力转移到细化专家中，从而促进更准确地表示动态物理现象。对物理感知视频生成基准的大量实验表明，ProPhy 比现有最先进的方法产生更真实、动态和物理连贯的结果。</li>
</ul>

<h3>Title: Self-Supervised AI-Generated Image Detection: A Camera Metadata Perspective</h3>
<ul>
<li><strong>Authors: </strong>Nan Zhong, Mian Zou, Yiran Xu, Zhenxing Qian, Xinpeng Zhang, Baoyuan Wu, Kede Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05651">https://arxiv.org/abs/2512.05651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05651">https://arxiv.org/pdf/2512.05651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05651]] Self-Supervised AI-Generated Image Detection: A Camera Metadata Perspective(https://arxiv.org/abs/2512.05651)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The proliferation of AI-generated imagery poses escalating challenges for multimedia forensics, yet many existing detectors depend on assumptions about the internals of specific generative models, limiting their cross-model applicability. We introduce a self-supervised approach for detecting AI-generated images that leverages camera metadata -- specifically exchangeable image file format (EXIF) tags -- to learn features intrinsic to digital photography. Our pretext task trains a feature extractor solely on camera-captured photographs by classifying categorical EXIF tags (\eg, camera model and scene type) and pairwise-ranking ordinal and continuous EXIF tags (\eg, focal length and aperture value). Using these EXIF-induced features, we first perform one-class detection by modeling the distribution of photographic images with a Gaussian mixture model and flagging low-likelihood samples as AI-generated. We then extend to binary detection that treats the learned extractor as a strong regularizer for a classifier of the same architecture, operating on high-frequency residuals from spatially scrambled patches. Extensive experiments across various generative models demonstrate that our EXIF-induced detectors substantially advance the state of the art, delivering strong generalization to in-the-wild samples and robustness to common benign image perturbations.</li>
<li><strong>摘要：</strong>人工智能生成图像的激增给多媒体取证带来了不断升级的挑战，但许多现有检测器依赖于对特定生成模型内部结构的假设，限制了它们的跨模型适用性。我们引入了一种自我监督的方法来检测人工智能生成的图像，该方法利用相机元数据——特别是可交换图像文件格式（EXIF）标签——来学习数字摄影的固有特征。我们的借口任务通过对分类 EXIF 标签（例如，相机型号和场景类型）和成对排序有序和连续 EXIF 标签（例如，焦距和光圈值）进行分类，仅在相机拍摄的照片上训练特征提取器。使用这些 EXIF 诱导的特征，我们首先通过使用高斯混合模型对摄影图像的分布进行建模并将低似然样本标记为 AI 生成来执行一类检测。然后，我们扩展到二进制检测，将学习的提取器视为相同架构的分类器的强正则化器，对空间置乱补丁的高频残差进行操作。跨各种生成模型的大量实验表明，我们的 EXIF 诱导检测器极大地推进了现有技术的发展，为野外样本提供了强大的泛化能力，并对常见的良性图像扰动具有鲁棒性。</li>
</ul>

<h3>Title: Feasibility of AI-Assisted Programming for End-User Development</h3>
<ul>
<li><strong>Authors: </strong>Irene Weber</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05666">https://arxiv.org/abs/2512.05666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05666">https://arxiv.org/pdf/2512.05666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05666]] Feasibility of AI-Assisted Programming for End-User Development(https://arxiv.org/abs/2512.05666)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>End-user development,where non-programmers create or adapt their own digital tools, can play a key role in driving digital transformation within organizations. Currently, low-code/no-code platforms are widely used to enable end-user development through visual programming, minimizing the need for manual coding. Recent advancements in generative AI, particularly large language model-based assistants and "copilots", open new possibilities, as they may enable end users to generate and refine programming code and build apps directly from natural language prompts. This approach, here referred to as AI-assisted end-user coding, promises greater flexibility, broader applicability, faster development, improved reusability, and reduced vendor lock-in compared to the established visual LCNC platforms. This paper investigates whether AI-assisted end-user coding is a feasible paradigm for end-user development, which may complement or even replace the LCNC model in the future. To explore this, we conducted a case study in which non-programmers were asked to develop a basic web app through interaction with AI this http URL majority of study participants successfully completed the task in reasonable time and also expressed support for AI-assisted end-user coding as a viable approach for end-user development. The paper presents the study design, analyzes the outcomes, and discusses potential implications for practice, future research, and academic teaching.</li>
<li><strong>摘要：</strong>最终用户开发（即非程序员创建或改编自己的数字工具）可以在推动组织内部的数字化转型方面发挥关键作用。目前，低代码/无代码平台被广泛使用，以支持最终用户通过可视化编程进行开发，从而最大限度地减少手动编码的需要。生成式人工智能的最新进展，特别是基于大型语言模型的助手和“副驾驶”，开辟了新的可能性，因为它们可以使最终用户能够生成和完善编程代码，并直接根据自然语言提示构建应用程序。与现有的可视化 LCNC 平台相比，这种方法（此处称为人工智能辅助最终用户编码）具有更大的灵活性、更广泛的适用性、更快的开发速度、更高的可重用性并减少供应商锁定。本文研究了人工智能辅助最终用户编码是否是最终用户开发的可行范式，它可能在未来补充甚至取代 LCNC 模型。为了探索这一点，我们进行了一项案例研究，其中非程序员被要求通过与 AI 交互来开发一个基本的 Web 应用程序（该 http URL），大多数研究参与者在合理的时间内成功完成了任务，并表示支持 AI 辅助最终用户编码作为最终用户开发的可行方法。本文介绍了研究设计，分析了结果，并讨论了对实践、未来研究和学术教学的潜在影响。</li>
</ul>

<h3>Title: InverseCrafter: Efficient Video ReCapture as a Latent Domain Inverse Problem</h3>
<ul>
<li><strong>Authors: </strong>Yeobin Hong, Suhyeon Lee, Hyungjin Chung, Jong Chul Ye</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05672">https://arxiv.org/abs/2512.05672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05672">https://arxiv.org/pdf/2512.05672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05672]] InverseCrafter: Efficient Video ReCapture as a Latent Domain Inverse Problem(https://arxiv.org/abs/2512.05672)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent approaches to controllable 4D video generation often rely on fine-tuning pre-trained Video Diffusion Models (VDMs). This dominant paradigm is computationally expensive, requiring large-scale datasets and architectural modifications, and frequently suffers from catastrophic forgetting of the model's original generative priors. Here, we propose InverseCrafter, an efficient inpainting inverse solver that reformulates the 4D generation task as an inpainting problem solved in the latent space. The core of our method is a principled mechanism to encode the pixel space degradation operator into a continuous, multi-channel latent mask, thereby bypassing the costly bottleneck of repeated VAE operations and backpropagation. InverseCrafter not only achieves comparable novel view generation and superior measurement consistency in camera control tasks with near-zero computational overhead, but also excels at general-purpose video inpainting with editing. Code is available at this https URL.</li>
<li><strong>摘要：</strong>最近的可控 4D 视频生成方法通常依赖于微调预先训练的视频扩散模型 (VDM)。这种主导范式的计算成本很高，需要大规模数据集和架构修改，并且经常遭受模型原始生成先验的灾难性遗忘。在这里，我们提出了 InverseCrafter，一种高效的修复逆解算器，它将 4D 生成任务重新表述为在潜在空间中解决的修复问题。我们方法的核心是一种将像素空间退化算子编码为连续、多通道潜在掩模的原理机制，从而绕过重复 VAE 操作和反向传播的昂贵瓶颈。 InverseCrafter 不仅在相机控制任务中以接近零的计算开销实现了可比的新颖视图生成和卓越的测量一致性，而且在通用视频修复和编辑方面也表现出色。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Meta-Learning Multi-armed Bandits for Beam Tracking in 5G and 6G Networks</h3>
<ul>
<li><strong>Authors: </strong>Alexander Mattick, George Yammine, Georgios Kontes, Setareh Maghsudi, Christopher Mutschler</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05680">https://arxiv.org/abs/2512.05680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05680">https://arxiv.org/pdf/2512.05680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05680]] Meta-Learning Multi-armed Bandits for Beam Tracking in 5G and 6G Networks(https://arxiv.org/abs/2512.05680)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Beamforming-capable antenna arrays with many elements enable higher data rates in next generation 5G and 6G networks. In current practice, analog beamforming uses a codebook of pre-configured beams with each of them radiating towards a specific direction, and a beam management function continuously selects \textit{optimal} beams for moving user equipments (UEs). However, large codebooks and effects caused by reflections or blockages of beams make an optimal beam selection challenging. In contrast to previous work and standardization efforts that opt for supervised learning to train classifiers to predict the next best beam based on previously selected beams we formulate the problem as a partially observable Markov decision process (POMDP) and model the environment as the codebook itself. At each time step, we select a candidate beam conditioned on the belief state of the unobservable optimal beam and previously probed beams. This frames the beam selection problem as an online search procedure that locates the moving optimal beam. In contrast to previous work, our method handles new or unforeseen trajectories and changes in the physical environment, and outperforms previous work by orders of magnitude.</li>
<li><strong>摘要：</strong>具有多个元件的波束成形天线阵列可在下一代 5G 和 6G 网络中实现更高的数据速率。在当前实践中，模拟波束形成使用预先配置的波束的码本，每个波束都向特定方向辐射，并且波束管理功能不断地为移动用户设备（UE）选择\textit{最佳}波束。然而，大码本以及光束反射或阻挡造成的影响使得最佳光束选择具有挑战性。与之前的工作和标准化工作选择监督学习来训练分类器以根据先前选择的波束预测下一个最佳波束相比，我们将问题表述为部分可观察马尔可夫决策过程（POMDP），并将环境建模为码本本身。在每个时间步长，我们根据不可观察的最优光束和先前探测的光束的置信状态来选择候选光束。这将光束选择问题定义为定位移动最佳光束的在线搜索过程。与之前的工作相比，我们的方法可以处理物理环境中新的或不可预见的轨迹和变化，并且比之前的工作好几个数量级。</li>
</ul>

<h3>Title: Physics-Informed Graph Neural Network with Frequency-Aware Learning for Optical Aberration Correction</h3>
<ul>
<li><strong>Authors: </strong>Yong En Kok, Bowen Deng, Alexander Bentley, Andrew J. Parkes, Michael G. Somekh, Amanda J. Wright, Michael P. Pound</a></li>
<li><strong>Subjects: </strong>cs.CV, physics.optics</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05683">https://arxiv.org/abs/2512.05683</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05683">https://arxiv.org/pdf/2512.05683</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05683]] Physics-Informed Graph Neural Network with Frequency-Aware Learning for Optical Aberration Correction(https://arxiv.org/abs/2512.05683)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Optical aberrations significantly degrade image quality in microscopy, particularly when imaging deeper into samples. These aberrations arise from distortions in the optical wavefront and can be mathematically represented using Zernike polynomials. Existing methods often address only mild aberrations on limited sample types and modalities, typically treating the problem as a black-box mapping without leveraging the underlying optical physics of wavefront distortions. We propose ZRNet, a physics-informed framework that jointly performs Zernike coefficient prediction and optical image Restoration. We contribute a Zernike Graph module that explicitly models physical relationships between Zernike polynomials based on their azimuthal degrees-ensuring that learned corrections align with fundamental optical principles. To further enforce physical consistency between image restoration and Zernike prediction, we introduce a Frequency-Aware Alignment (FAA) loss, which better aligns Zernike coefficient prediction and image features in the Fourier domain. Extensive experiments on CytoImageNet demonstrates that our approach achieves state-of-the-art performance in both image restoration and Zernike coefficient prediction across diverse microscopy modalities and biological samples with complex, large-amplitude aberrations. Code is available at this https URL.</li>
<li><strong>摘要：</strong>光学像差会显着降低显微镜中的图像质量，特别是在对样品进行更深入的成像时。这些像差由光学波前的畸变引起，可以使用泽尼克多项式进行数学表示。现有方法通常仅解决有限样本类型和模态上的轻微像差，通常将问题视为黑盒映射，而不利用波前畸变的基础光学物理。我们提出了 ZRNet，一种物理信息框架，联合执行 Zernike 系数预测和光学图像恢复。我们贡献了一个 Zernike Graph 模块，该模块根据 Zernike 多项式的方位角度对 Zernike 多项式之间的物理关系进行显式建模，确保学习到的校正符合基本光学原理。为了进一步加强图像恢复和泽尼克预测之间的物理一致性，我们引入了频率感知对齐（FAA）损失，它可以更好地对齐傅立叶域中的泽尼克系数预测和图像特征。 CytoImageNet 上的大量实验表明，我们的方法在跨不同显微镜模式和具有复杂大振幅像差的生物样本的图像恢复和 Zernike 系数预测方面实现了最先进的性能。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: HQ-DM: Single Hadamard Transformation-Based Quantization-Aware Training for Low-Bit Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Shizhuo Mao, Hongtao Zou, Qihu Xie, Song Chen, Yi Kang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05746">https://arxiv.org/abs/2512.05746</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05746">https://arxiv.org/pdf/2512.05746</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05746]] HQ-DM: Single Hadamard Transformation-Based Quantization-Aware Training for Low-Bit Diffusion Models(https://arxiv.org/abs/2512.05746)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated significant applications in the field of image generation. However, their high computational and memory costs pose challenges for deployment. Model quantization has emerged as a promising solution to reduce storage overhead and accelerate inference. Nevertheless, existing quantization methods for diffusion models struggle to mitigate outliers in activation matrices during inference, leading to substantial performance degradation under low-bit quantization scenarios. To address this, we propose HQ-DM, a novel Quantization-Aware Training framework that applies Single Hadamard Transformation to activation matrices. This approach effectively reduces activation outliers while preserving model performance under quantization. Compared to traditional Double Hadamard Transformation, our proposed scheme offers distinct advantages by seamlessly supporting INT convolution operations while preventing the amplification of weight outliers. For conditional generation on the ImageNet 256x256 dataset using the LDM-4 model, our W4A4 and W4A3 quantization schemes improve the Inception Score by 12.8% and 467.73%, respectively, over the existing state-of-the-art method.</li>
<li><strong>摘要：</strong>扩散模型在图像生成领域已经展示了重要的应用。然而，它们的高计算和内存成本给部署带来了挑战。模型量化已成为减少存储开销和加速推理的有前景的解决方案。然而，现有的扩散模型量化方法很难在推理过程中减轻激活矩阵中的异常值，从而导致低位量化场景下的性能大幅下降。为了解决这个问题，我们提出了 HQ-DM，这是一种新颖的量化感知训练框架，它将单哈达玛变换应用于激活矩阵。这种方法有效地减少了激活异常值，同时在量化下保持模型性能。与传统的双哈达玛变换相比，我们提出的方案通过无缝支持 INT 卷积运算同时防止权重异常值的放大，提供了明显的优势。对于使用 LDM-4 模型在 ImageNet 256x256 数据集上进行条件生成，我们的 W4A4 和 W4A3 量化方案比现有最先进的方法分别将 Inception Score 提高了 12.8% 和 467.73%。</li>
</ul>

<h3>Title: USV: Unified Sparsification for Accelerating Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Xinjian Wu, Hongmei Wang, Yuan Zhou, Qinglin Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05754">https://arxiv.org/abs/2512.05754</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05754">https://arxiv.org/pdf/2512.05754</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05754]] USV: Unified Sparsification for Accelerating Video Diffusion Models(https://arxiv.org/abs/2512.05754)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The scalability of high-fidelity video diffusion models (VDMs) is constrained by two key sources of redundancy: the quadratic complexity of global spatio-temporal attention and the computational overhead of long iterative denoising trajectories. Existing accelerators -- such as sparse attention and step-distilled samplers -- typically target a single dimension in isolation and quickly encounter diminishing returns, as the remaining bottlenecks become dominant. In this work, we introduce USV (Unified Sparsification for Video diffusion models), an end-to-end trainable framework that overcomes this limitation by jointly orchestrating sparsification across both the model's internal computation and its sampling process. USV learns a dynamic, data- and timestep-dependent sparsification policy that prunes redundant attention connections, adaptively merges semantically similar tokens, and reduces denoising steps, treating them not as independent tricks but as coordinated actions within a single optimization objective. This multi-dimensional co-design enables strong mutual reinforcement among previously disjoint acceleration strategies. Extensive experiments on large-scale video generation benchmarks demonstrate that USV achieves up to 83.3% speedup in the denoising process and 22.7% end-to-end acceleration, while maintaining high visual fidelity. Our results highlight unified, dynamic sparsification as a practical path toward efficient, high-quality video generation.</li>
<li><strong>摘要：</strong>高保真视频扩散模型（VDM）的可扩展性受到两个关键冗余源的限制：全局时空注意力的二次复杂度和长迭代去噪轨迹的计算开销。现有的加速器（例如稀疏注意力和逐步蒸馏采样器）通常针对孤立的单一维度，并且随着剩余的瓶颈变得占主导地位，很快就会遇到收益递减的情况。在这项工作中，我们引入了 USV（视频扩散模型的统一稀疏化），这是一种端到端的可训练框架，它通过在模型的内部计算及其采样过程中联合协调稀疏化来克服这一限制。 USV 学习一种动态的、依赖于数据和时间步长的稀疏化策略，该策略可以修剪冗余的注意力连接，自适应地合并语义上相似的标记，并减少去噪步骤，将它们视为单个优化目标内的协调动作，而不是将它们视为独立的技巧。这种多维协同设计可以在之前不相交的加速策略之间实现强有力的相互强化。对大规模视频生成基准的大量实验表明，USV 在保持高视觉保真度的同时，在去噪过程中实现了高达 83.3% 的加速，以及 22.7% 的端到端加速。我们的结果强调了统一的动态稀疏化是实现高效、高质量视频生成的实用途径。</li>
</ul>

<h3>Title: Mechanistic Interpretability of Antibody Language Models Using SAEs</h3>
<ul>
<li><strong>Authors: </strong>Rebonto Haque, Oliver M. Turnbull, Anisha Parsan, Nithin Parsan, John J. Yang, Charlotte M. Deane</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05794">https://arxiv.org/abs/2512.05794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05794">https://arxiv.org/pdf/2512.05794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05794]] Mechanistic Interpretability of Antibody Language Models Using SAEs(https://arxiv.org/abs/2512.05794)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Sparse autoencoders (SAEs) are a mechanistic interpretability technique that have been used to provide insight into learned concepts within large protein language models. Here, we employ TopK and Ordered SAEs to investigate an autoregressive antibody language model, p-IgGen, and steer its generation. We show that TopK SAEs can reveal biologically meaningful latent features, but high feature concept correlation does not guarantee causal control over generation. In contrast, Ordered SAEs impose an hierarchical structure that reliably identifies steerable features, but at the expense of more complex and less interpretable activation patterns. These findings advance the mechanistic interpretability of domain-specific protein language models and suggest that, while TopK SAEs are sufficient for mapping latent features to concepts, Ordered SAEs are preferable when precise generative steering is required.</li>
<li><strong>摘要：</strong>稀疏自动编码器 (SAE) 是一种机械可解释性技术，已用于深入了解大型蛋白质语言模型中学到的概念。在这里，我们使用 TopK 和有序 SAE 来研究自回归抗体语言模型 p-IgGen，并引导其生成。我们表明，TopK SAE 可以揭示具有生物学意义的潜在特征，但高特征概念相关性并不能保证对生成的因果控制。相比之下，有序 SAE 强加了一种分层结构，可以可靠地识别可操纵的特征，但代价是更复杂且难以解释的激活模式。这些发现提高了特定领域蛋白质语言模型的机械解释性，并表明，虽然 TopK SAE 足以将潜在特征映射到概念，但当需要精确的生成控制时，有序 SAE 更可取。</li>
</ul>

<h3>Title: Bring Your Dreams to Life: Continual Text-to-Video Customization</h3>
<ul>
<li><strong>Authors: </strong>Jiahua Dong, Xudong Wang, Wenqi Liang, Zongyan Han, Meng Cao, Duzhen Zhang, Hanbin Zhao, Zhi Han, Salman Khan, Fahad Shahbaz Khan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05802">https://arxiv.org/abs/2512.05802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05802">https://arxiv.org/pdf/2512.05802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05802]] Bring Your Dreams to Life: Continual Text-to-Video Customization(https://arxiv.org/abs/2512.05802)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Customized text-to-video generation (CTVG) has recently witnessed great progress in generating tailored videos from user-specific text. However, most CTVG methods assume that personalized concepts remain static and do not expand incrementally over time. Additionally, they struggle with forgetting and concept neglect when continuously learning new concepts, including subjects and motions. To resolve the above challenges, we develop a novel Continual Customized Video Diffusion (CCVD) model, which can continuously learn new concepts to generate videos across various text-to-video generation tasks by tackling forgetting and concept neglect. To address catastrophic forgetting, we introduce a concept-specific attribute retention module and a task-aware concept aggregation strategy. They can capture the unique characteristics and identities of old concepts during training, while combining all subject and motion adapters of old concepts based on their relevance during testing. Besides, to tackle concept neglect, we develop a controllable conditional synthesis to enhance regional features and align video contexts with user conditions, by incorporating layer-specific region attention-guided noise estimation. Extensive experimental comparisons demonstrate that our CCVD outperforms existing CTVG models. The code is available at this https URL.</li>
<li><strong>摘要：</strong>定制文本到视频生成（CTVG）最近在根据用户特定文本生成定制视频方面取得了巨大进展。然而，大多数 CTVG 方法假设个性化概念保持静态，并且不会随着时间的推移逐渐扩展。此外，当他们不断学习新概念（包括主题和动作）时，他们会与遗忘和概念忽视作斗争。为了解决上述挑战，我们开发了一种新颖的持续定制视频扩散（CCVD）模型，该模型可以通过解决遗忘和概念忽略问题，在各种文本到视频生成任务中不断学习新概念来生成视频。为了解决灾难性遗忘，我们引入了特定于概念的属性保留模块和任务感知概念聚合策略。他们可以在训练期间捕获旧概念的独特特征和身份，同时根据测试期间的相关性将旧概念的所有主题和动作适配器组合起来。此外，为了解决概念忽略问题，我们开发了一种可控条件合成，通过结合特定层区域注意引导的噪声估计来增强区域特征并使视频上下文与用户条件保持一致。广泛的实验比较表明，我们的 CCVD 优于现有的 CTVG 模型。该代码可从此 https URL 获取。</li>
</ul>

<h3>Title: UG-FedDA: Uncertainty-Guided Federated Domain Adaptation for Multi-Center Alzheimer's Disease Detection</h3>
<ul>
<li><strong>Authors: </strong>Fubao Zhu, Zhanyuan Jia, Zhiguo Wang, Huan Huang, Danyang Sun, Chuang Han, Yanting Li, Jiaofen Nan, Chen Zhao, Weihua Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05814">https://arxiv.org/abs/2512.05814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05814">https://arxiv.org/pdf/2512.05814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05814]] UG-FedDA: Uncertainty-Guided Federated Domain Adaptation for Multi-Center Alzheimer's Disease Detection(https://arxiv.org/abs/2512.05814)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Alzheimer's disease (AD) is an irreversible neurodegenerative disorder, and early diagnosis is critical for timely intervention. However, most existing classification frameworks face challenges in multicenter studies, as they often neglect inter-site heterogeneity and lack mechanisms to quantify uncertainty, which limits their robustness and clinical applicability. To address these issues, we proposed Uncertainty-Guided Federated Domain Adaptation (UG-FedDA), a novel multicenter AD classification framework that integrates uncertainty quantification (UQ) with federated domain adaptation to handle cross-site structure magnetic resonance imaging (MRI) heterogeneity under privacy constraints. Our approach extracts multi-template region-of-interest (RoI) features using a self-attention transformer, capturing both regional representations and their interactions. UQ is integrated to guide feature alignment, mitigating source-target distribution shifts by down-weighting uncertain samples. Experiments are conducted on three public datasets: the Alzheimer's Disease Neuroimaging Initiative (ADNI), the Australian Imaging, Biomarkers and Lifestyle study (AIBL), and the Open Access Series of Imaging Studies (OASIS). UG-FedDA achieved consistent cross-domain improvements in accuracy, sensitivity, and area under the ROC curve across three classification tasks: AD vs. normal controls (NC), mild cognitive impairment (MCI) vs. AD, and NC vs. MCI. For NC vs. AD, UG-FedDA achieves accuracies of 90.54%, 89.04%, and 77.78% on ADNI, AIBL and OASIS datasets, respectively. For MCI vs. AD, accuracies are 80.20% (ADNI), 71.91% (AIBL), and 79.73% (OASIS). For NC vs. MCI, results are 76.87% (ADNI), 73.91% (AIBL), and 83.73% (OASIS). These results demonstrate that the proposed framework not only adapts efficiently across multiple sites but also preserves strict privacy.</li>
<li><strong>摘要：</strong>阿尔茨海默病（AD）是一种不可逆的神经退行性疾病，早期诊断对于及时干预至关重要。然而，大多数现有的分类框架在多中心研究中面临挑战，因为它们经常忽视站点间的异质性，并且缺乏量化不确定性的机制，这限制了它们的稳健性和临床适用性。为了解决这些问题，我们提出了不确定性引导联邦域适应（UG-FedDA），这是一种新颖的多中心AD分类框架，它将不确定性量化（UQ）与联邦域适应相结合，以处理隐私约束下的跨站点结构磁共振成像（MRI）异质性。我们的方法使用自注意力变换器提取多模板感兴趣区域（RoI）特征，捕获区域表示及其相互作用。 UQ 被集成来指导特征对齐，通过降低不确定样本的权重来减轻源-目标分布变化。实验在三个公共数据集上进行：阿尔茨海默病神经影像计划 (ADNI)、澳大利亚影像、生物标志物和生活方式研究 (AIBL) 以及影像研究开放获取系列 (OASIS)。 UG-FedDA 在三个分类任务中在准确性、灵敏度和 ROC 曲线下面积方面实现了一致的跨领域改进：AD 与正常对照 (NC)、轻度认知障碍 (MCI) 与 AD 以及 NC 与 MCI。对于 NC 与 AD，UG-FedDA 在 ADNI、AIBL 和 OASIS 数据集上的准确率分别为 90.54%、89.04% 和 77.78%。对于 MCI 与 AD，准确度分别为 80.20% (ADNI)、71.91% (AIBL) 和 79.73% (OASIS)。对于 NC 与 MCI，结果分别为 76.87% (ADNI)、73.91% (AIBL) 和 83.73% (OASIS)。这些结果表明，所提出的框架不仅可以有效地适应多个站点，而且还可以保护严格的隐私。</li>
</ul>

<h3>Title: NEAT: Neighborhood-Guided, Efficient, Autoregressive Set Transformer for 3D Molecular Generation</h3>
<ul>
<li><strong>Authors: </strong>Daniel Rose, Roxane Axel Jacob, Johannes Kirchmair, Thierry Langer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05844">https://arxiv.org/abs/2512.05844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05844">https://arxiv.org/pdf/2512.05844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05844]] NEAT: Neighborhood-Guided, Efficient, Autoregressive Set Transformer for 3D Molecular Generation(https://arxiv.org/abs/2512.05844)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Autoregressive models are a promising alternative to diffusion-based models for 3D molecular structure generation. However, a key limitation is the assumption of a token order: while text has a natural sequential order, the next token prediction given a molecular graph prefix should be invariant to atom permutations. Previous works sidestepped this mismatch by using canonical orders or focus atoms. We argue that this is unnecessary. We introduce NEAT, a Neighborhood-guided, Efficient, Autoregressive, Set Transformer that treats molecular graphs as sets of atoms and learns the order-agnostic distribution over admissible tokens at the graph boundary with an autoregressive flow model. NEAT approaches state-of-the-art performance in 3D molecular generation with high computational efficiency and atom-level permutation invariance, establishing a practical foundation for scalable molecular design.</li>
<li><strong>摘要：</strong>自回归模型是用于 3D 分子结构生成的基于扩散的模型的有前途的替代方案。然而，一个关键的限制是令牌顺序的假设：虽然文本具有自然的顺序顺序，但给定分子图前缀的下一个令牌预测应该对原子排列保持不变。以前的工作通过使用规范顺序或焦点原子来避免这种不匹配。我们认为这是不必要的。我们引入 NEAT，一种邻域引导、高效、自回归的集合转换器，它将分子图视为原子集合，并通过自回归流模型学习图边界处可接受标记的顺序无关分布。 NEAT 具有高计算效率和原子级排列不变性，在 3D 分子生成方面达到了最先进的性能，为可扩展的分子设计奠定了实用的基础。</li>
</ul>

<h3>Title: VRSA: Jailbreaking Multimodal Large Language Models through Visual Reasoning Sequential Attack</h3>
<ul>
<li><strong>Authors: </strong>Shiji Zhao, Shukun Xiong, Yao Huang, Yan Jin, Zhenyu Wu, Jiyang Guan, Ranjie Duan, Jialing Tao, Hui Xue, Xingxing Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05853">https://arxiv.org/abs/2512.05853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05853">https://arxiv.org/pdf/2512.05853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05853]] VRSA: Jailbreaking Multimodal Large Language Models through Visual Reasoning Sequential Attack(https://arxiv.org/abs/2512.05853)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) are widely used in various fields due to their powerful cross-modal comprehension and generation capabilities. However, more modalities bring more vulnerabilities to being utilized for jailbreak attacks, which induces MLLMs to output harmful content. Due to the strong reasoning ability of MLLMs, previous jailbreak attacks try to explore reasoning safety risk in text modal, while similar threats have been largely overlooked in the visual modal. To fully evaluate potential safety risks in the visual reasoning task, we propose Visual Reasoning Sequential Attack (VRSA), which induces MLLMs to gradually externalize and aggregate complete harmful intent by decomposing the original harmful text into several sequentially related sub-images. In particular, to enhance the rationality of the scene in the image sequence, we propose Adaptive Scene Refinement to optimize the scene most relevant to the original harmful query. To ensure the semantic continuity of the generated image, we propose Semantic Coherent Completion to iteratively rewrite each sub-text combined with contextual information in this scene. In addition, we propose Text-Image Consistency Alignment to keep the semantical consistency. A series of experiments demonstrates that the VRSA can achieve a higher attack success rate compared with the state-of-the-art jailbreak attack methods on both the open-source and closed-source MLLMs such as GPT-4o and Claude-4.5-Sonnet.</li>
<li><strong>摘要：</strong>多模态大语言模型（MLLM）因其强大的跨模态理解和生成能力而广泛应用于各个领域。然而，更多的模式会带来更多的漏洞用于越狱攻击，从而导致 MLLM 输出有害内容。由于MLLM具有强大的推理能力，之前的越狱攻击试图探索文本模式中的推理安全风险，而视觉模式中的类似威胁在很大程度上被忽视了。为了充分评估视觉推理任务中潜在的安全风险，我们提出了视觉推理序列攻击（VRSA），它通过将原始有害文本分解为几个顺序相关的子图像，诱导 MLLM 逐渐外化和聚合完整的有害意图。特别是，为了增强图像序列中场景的合理性，我们提出自适应场景细化来优化与原始有害查询最相关的场景。为了确保生成图像的语义连续性，我们提出语义连贯完成来结合该场景中的上下文信息迭代重写每个子文本。此外，我们提出文本图像一致性对齐来保持语义一致性。一系列实验表明，与GPT-4o和Claude-4.5-Sonnet等开源和闭源MLLM上最先进的越狱攻击方法相比，VRSA能够获得更高的攻击成功率。</li>
</ul>

<h3>Title: Underwater Image Reconstruction Using a Swin Transformer-Based Generator and PatchGAN Discriminator</h3>
<ul>
<li><strong>Authors: </strong>Md. Mahbub Hasan Akash, Aria Tasnim Mridula, Sheekar Banerjee, Ishtiak Al Mamoon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05866">https://arxiv.org/abs/2512.05866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05866">https://arxiv.org/pdf/2512.05866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05866]] Underwater Image Reconstruction Using a Swin Transformer-Based Generator and PatchGAN Discriminator(https://arxiv.org/abs/2512.05866)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generative</a></li>
<li><strong>Abstract: </strong>Underwater imaging is essential for marine exploration, environmental monitoring, and infrastructure inspection. However, water causes severe image degradation through wavelength-dependent absorption and scattering, resulting in color distortion, low contrast, and haze effects. Traditional reconstruction methods and convolutional neural network-based approaches often fail to adequately address these challenges due to limited receptive fields and inability to model global dependencies. This paper presented a novel deep learning framework that integrated a Swin Transformer architecture within a generative adversarial network (GAN) for underwater image reconstruction. Our generator employed a U-Net structure with Swin Transformer blocks to capture both local features and long-range dependencies crucial for color correction across entire images. A PatchGAN discriminator provided adversarial training to ensure high-frequency detail preservation. We trained and evaluated our model on the EUVP dataset, which contains paired underwater images of varying quality. Quantitative results demonstrate stateof-the-art performance with PSNR of 24.76 dB and SSIM of 0.89, representing significant improvements over existing methods. Visual results showed effective color balance restoration, contrast improvement, and haze reduction. An ablation study confirms the superiority of our Swin Transformer designed over convolutional alternatives. The proposed method offers robust underwater image reconstruction suitable for various marine applications.</li>
<li><strong>摘要：</strong>水下成像对于海洋勘探、环境监测和基础设施检查至关重要。然而，水会通过与波长相关的吸收和散射而导致严重的图像质量下降，从而导致颜色失真、低对比度和雾度效应。由于接受域有限且无法对全局依赖性进行建模，传统的重建方法和基于卷积神经网络的方法通常无法充分解决这些挑战。本文提出了一种新颖的深度学习框架，该框架将 Swin Transformer 架构集成到生成对抗网络 (GAN) 中，用于水下图像重建。我们的生成器采用带有 Swin Transformer 块的 U-Net 结构来捕获局部特征和远程依赖性，这对于整个图像的颜色校正至关重要。 PatchGAN 判别器提供对抗性训练，以确保高频细节保留。我们在 EUVP 数据集上训练和评估我们的模型，该数据集包含不同质量的配对水下图像。定量结果证明了最先进的性能，PSNR 为 24.76 dB，SSIM 为 0.89，比现有方法有了显着改进。视觉结果显示有效的色彩平衡恢复、对比度改善和雾度减少。消融研究证实了我们设计的 Swin Transformer 相对于卷积替代方案的优越性。所提出的方法提供了适合各种海洋应用的稳健的水下图像重建。</li>
</ul>

<h3>Title: World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty</h3>
<ul>
<li><strong>Authors: </strong>Zhiting Mei, Tenny Yin, Micah Baker, Ola Shorinwa, Anirudha Majumdar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05927">https://arxiv.org/abs/2512.05927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05927">https://arxiv.org/pdf/2512.05927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05927]] World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty(https://arxiv.org/abs/2512.05927)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection.</li>
<li><strong>摘要：</strong>生成视频模型的最新进展带来了高保真视频合成方面的重大突破，特别是在可控视频生成方面，其中生成的视频以文本和动作输入为条件，例如在机器人技术中的指令引导视频编辑和世界建模中。尽管具有这些卓越的功能，但可控视频模型经常会产生幻觉——生成与物理现实不相符的未来视频帧——这在机器人政策评估和规划等许多任务中引起了严重关注。然而，最先进的视频模型缺乏评估和表达信心的能力，从而阻碍了幻觉的缓解。为了严格应对这一挑战，我们提出了 C3，一种不确定性量化（UQ）方法，用于训练连续尺度校准的可控视频模型，以在子补丁级别进行密集置信度估计，精确定位每个生成视频帧中的不确定性。我们的昆士兰大学方法引入了三项核心创新，使视频模型能够估计其不确定性。首先，我们的方法开发了一个新颖的框架，通过严格正确的评分规则来训练视频模型的正确性和校准。其次，我们估计视频模型在潜在空间中的不确定性，避免与像素空间方法相关的训练不稳定和高昂的训练成本。第三，我们将密集的潜在空间不确定性映射到 RGB 空间中可解释的像素级不确定性，以进行直观可视化，提供识别不可信区域的高分辨率不确定性热图。通过对大规模机器人学习数据集（Bridge 和 DROID）和现实世界评估的广泛实验，我们证明我们的方法不仅提供训练分布内的校准不确定性估计，而且还能够实现有效的分布外检测。</li>
</ul>

<h3>Title: A Comparative Study on Synthetic Facial Data Generation Techniques for Face Recognition</h3>
<ul>
<li><strong>Authors: </strong>Pedro Vidal, Bernardo Biesseck, Luiz E. L. Coelho, Roger Granada, David Menotti</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05928">https://arxiv.org/abs/2512.05928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05928">https://arxiv.org/pdf/2512.05928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05928]] A Comparative Study on Synthetic Facial Data Generation Techniques for Face Recognition(https://arxiv.org/abs/2512.05928)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Facial recognition has become a widely used method for authentication and identification, with applications for secure access and locating missing persons. Its success is largely attributed to deep learning, which leverages large datasets and effective loss functions to learn discriminative features. Despite these advances, facial recognition still faces challenges in explainability, demographic bias, privacy, and robustness to aging, pose variations, lighting changes, occlusions, and facial expressions. Privacy regulations have also led to the degradation of several datasets, raising legal, ethical, and privacy concerns. Synthetic facial data generation has been proposed as a promising solution. It mitigates privacy issues, enables experimentation with controlled facial attributes, alleviates demographic bias, and provides supplementary data to improve models trained on real data. This study compares the effectiveness of synthetic facial datasets generated using different techniques in facial recognition tasks. We evaluate accuracy, rank-1, rank-5, and the true positive rate at a false positive rate of 0.01% on eight leading datasets, offering a comparative analysis not extensively explored in the literature. Results demonstrate the ability of synthetic data to capture realistic variations while emphasizing the need for further research to close the performance gap with real data. Techniques such as diffusion models, GANs, and 3D models show substantial progress; however, challenges remain.</li>
<li><strong>摘要：</strong>面部识别已成为一种广泛使用的身份验证和识别方法，可用于安全访问和定位失踪人员。它的成功很大程度上归功于深度学习，它利用大型数据集和有效的损失函数来学习判别性特征。尽管取得了这些进步，面部识别仍然面临可解释性、人口统计学偏见、隐私以及对衰老、姿势变化、光照变化、遮挡和面部表情的鲁棒性等方面的挑战。隐私法规还导致了一些数据集的退化，引发了法律、道德和隐私方面的担忧。合成面部数据生成已被认为是一种有前途的解决方案。它可以缓解隐私问题，支持对受控面部属性进行实验，减轻人口统计偏差，并提供补充数据来改进基于真实数据训练的模型。本研究比较了在面部识别任务中使用不同技术生成的合成面部数据集的有效性。我们在八个主要数据集上评估了准确率、rank-1、rank-5 和误报率为 0.01% 的真阳性率，提供了文献中未广泛探讨的比较分析。结果证明了合成数据捕获真实变化的能力，同时强调需要进一步研究以缩小与真实数据的性能差距。扩散模型、GAN 和 3D 模型等技术取得了实质性进展；然而，挑战依然存在。</li>
</ul>

<h3>Title: Synset Signset Germany: a Synthetic Dataset for German Traffic Sign Recognition</h3>
<ul>
<li><strong>Authors: </strong>Anne Sielemann, Lena Loercher, Max-Lion Schumacher, Stefan Wolf, Masoud Roschani, Jens Ziehn</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05936">https://arxiv.org/abs/2512.05936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05936">https://arxiv.org/pdf/2512.05936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05936]] Synset Signset Germany: a Synthetic Dataset for German Traffic Sign Recognition(https://arxiv.org/abs/2512.05936)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In this paper, we present a synthesis pipeline and dataset for training / testing data in the task of traffic sign recognition that combines the advantages of data-driven and analytical modeling: GAN-based texture generation enables data-driven dirt and wear artifacts, rendering unique and realistic traffic sign surfaces, while the analytical scene modulation achieves physically correct lighting and allows detailed parameterization. In particular, the latter opens up applications in the context of explainable AI (XAI) and robustness tests due to the possibility of evaluating the sensitivity to parameter changes, which we demonstrate with experiments. Our resulting synthetic traffic sign recognition dataset Synset Signset Germany contains a total of 105500 images of 211 different German traffic sign classes, including newly published (2020) and thus comparatively rare traffic signs. In addition to a mask and a segmentation image, we also provide extensive metadata including the stochastically selected environment and imaging effect parameters for each image. We evaluate the degree of realism of Synset Signset Germany on the real-world German Traffic Sign Recognition Benchmark (GTSRB) and in comparison to CATERED, a state-of-the-art synthetic traffic sign recognition dataset.</li>
<li><strong>摘要：</strong>在本文中，我们提出了一种用于交通标志识别任务中训练/测试数据的综合管道和数据集，它结合了数据驱动和分析建模的优点：基于 GAN 的纹理生成可以实现数据驱动的污垢和磨损伪影，渲染独特且真实的交通标志表面，而分析场景调制则实现物理上正确的照明并允许详细的参数化。特别是，后者由于可以评估对参数变化的敏感性而在可解释的人工智能（XAI）和鲁棒性测试中开辟了应用，我们通过实验证明了这一点。我们生成的合成交通标志识别数据集 Synset Signset 德国总共包含 211 个不同德国交通标志类别的 105500 张图像，包括新发布的（2020 年）因此相对罕见的交通标志。除了掩模和分割图像之外，我们还提供广泛的元数据，包括每个图像的随机选择的环境和成像效果参数。我们根据现实世界的德国交通标志识别基准 (GTSRB) 评估 Synset Signset 德国的真实度，并与最先进的合成交通标志识别数据集 CATERED 进行比较。</li>
</ul>

<h3>Title: Impugan: Learning Conditional Generative Models for Robust Data Imputation</h3>
<ul>
<li><strong>Authors: </strong>Zalish Mahmud, Anantaa Kotal, Aritran Piplai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05950">https://arxiv.org/abs/2512.05950</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05950">https://arxiv.org/pdf/2512.05950</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05950]] Impugan: Learning Conditional Generative Models for Robust Data Imputation(https://arxiv.org/abs/2512.05950)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Incomplete data are common in real-world applications. Sensors fail, records are inconsistent, and datasets collected from different sources often differ in scale, sampling rate, and quality. These differences create missing values that make it difficult to combine data and build reliable models. Standard imputation methods such as regression models, expectation-maximization, and multiple imputation rely on strong assumptions about linearity and independence. These assumptions rarely hold for complex or heterogeneous data, which can lead to biased or over-smoothed estimates. We propose Impugan, a conditional Generative Adversarial Network (cGAN) for imputing missing values and integrating heterogeneous datasets. The model is trained on complete samples to learn how missing variables depend on observed ones. During inference, the generator reconstructs missing entries from available features, and the discriminator enforces realism by distinguishing true from imputed data. This adversarial process allows Impugan to capture nonlinear and multimodal relationships that conventional methods cannot represent. In experiments on benchmark datasets and a multi-source integration task, Impugan achieves up to 82\% lower Earth Mover's Distance (EMD) and 70\% lower mutual-information deviation (MI) compared to leading baselines. These results show that adversarially trained generative models provide a scalable and principled approach for imputing and merging incomplete, heterogeneous data. Our model is available at: this http URL</li>
<li><strong>摘要：</strong>不完整的数据在实际应用中很常见。传感器发生故障、记录不一致，并且从不同来源收集的数据集在规模、采样率和质量方面通常存在差异。这些差异会造成缺失值，从而使组合数据和构建可靠模型变得困难。回归模型、期望最大化和多重插补等标准插补方法依赖于线性和独立性的强有力假设。这些假设很少适用于复杂或异构的数据，这可能导致估计有偏差或过度平滑。我们提出了 Impugan，一种条件生成对抗网络（cGAN），用于估算缺失值和集成异构数据集。该模型在完整样本上进行训练，以了解缺失变量如何依赖于观察到的变量。在推理过程中，生成器从可用特征中重建缺失的条目，鉴别器通过区分真实数据和估算数据来增强真实性。这种对抗过程使 Impugan 能够捕获传统方法无法表示的非线性和多模态关系。在基准数据集和多源集成任务的实验中，与领先基线相比，Impugan 的地球移动距离 (EMD) 降低了 82%，互信息偏差 (MI) 降低了 70%。这些结果表明，经过对抗性训练的生成模型为插补和合并不完整的异构数据提供了一种可扩展且有原则的方法。我们的模型可在以下位置获得：此 http URL</li>
</ul>

<h3>Title: MaxShapley: Towards Incentive-compatible Generative Search with Fair Context Attribution</h3>
<ul>
<li><strong>Authors: </strong>Sara Patel, Mingxun Zhou, Giulia Fanti</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05958">https://arxiv.org/abs/2512.05958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05958">https://arxiv.org/pdf/2512.05958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05958]] MaxShapley: Towards Incentive-compatible Generative Search with Fair Context Attribution(https://arxiv.org/abs/2512.05958)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative search engines based on large language models (LLMs) are replacing traditional search, fundamentally changing how information providers are compensated. To sustain this ecosystem, we need fair mechanisms to attribute and compensate content providers based on their contributions to generated answers. We introduce MaxShapley, an efficient algorithm for fair attribution in generative search pipelines that use retrieval-augmented generation (RAG). MaxShapley is a special case of the celebrated Shapley value; it leverages a decomposable max-sum utility function to compute attributions with linear computation in the number of documents, as opposed to the exponential cost of Shapley values. We evaluate MaxShapley on three multi-hop QA datasets (HotPotQA, MuSiQUE, MS MARCO); MaxShapley achieves comparable attribution quality to exact Shapley computation, while consuming a fraction of its tokens--for instance, it gives up to an 8x reduction in resource consumption over prior state-of-the-art methods at the same attribution accuracy.</li>
<li><strong>摘要：</strong>基于大语言模型 (LLM) 的生成搜索引擎正在取代传统搜索，从根本上改变信息提供者的报酬方式。为了维持这个生态系统，我们需要公平的机制来根据内容提供商对生成答案的贡献来归因和补偿内容提供商。我们引入了 MaxShapley，这是一种在使用检索增强生成 (RAG) 的生成搜索管道中进行公平归因的有效算法。 MaxShapley 是著名的 Shapley 值的一个特例；它利用可分解的最大和效用函数来通过文档数量的线性计算来计算归因，而不是 Shapley 值的指数成本。我们在三个多跳 QA 数据集（HotPotQA、MuSiQUE、MS MARCO）上评估 MaxShapley； MaxShapley 实现了与精确 Shapley 计算相当的归因质量，同时消耗了一小部分令牌——例如，在相同的归因精度下，它比之前最先进的方法减少了 8 倍的资源消耗。</li>
</ul>

<h3>Title: EditThinker: Unlocking Iterative Reasoning for Any Image Editor</h3>
<ul>
<li><strong>Authors: </strong>Hongyu Li, Manyuan Zhang, Dian Zheng, Ziyu Guo, Yimeng Jia, Kaituo Feng, Hao Yu, Yexin Liu, Yan Feng, Peng Pei, Xunliang Cai, Linjiang Huang, Hongsheng Li, Si Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.05965">https://arxiv.org/abs/2512.05965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.05965">https://arxiv.org/pdf/2512.05965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.05965]] EditThinker: Unlocking Iterative Reasoning for Any Image Editor(https://arxiv.org/abs/2512.05965)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Instruction-based image editing has emerged as a prominent research area, which, benefiting from image generation foundation models, have achieved high aesthetic quality, making instruction-following capability the primary challenge. Existing approaches improve instruction adherence via supervised or reinforcement learning, yet single-turn success rates remain limited due to inherent stochasticity and a lack of deliberation. In this work, we propose a deliberative editing framework to 'think' while they edit, which simulates the human cognitive loop by iteratively executing a Think-while-Edit cycle: Critiquing results and Refining instructions , followed by Repeating the generation until satisfactory. Specifically, we train a single MLLM, EditThinker, to act as the reasoning engine of this framework, which jointly produce the critique score, reasoning process, and refined instructions. We employ reinforcement learning to align the EditThinker's thinking with its editing, thereby generating more targeted instruction improvements. Extensive experiments on four benchmarks demonstrate that our approach significantly improves the instruction-following capability of any image editing model by a large margin. We will release our data construction framework, datasets, and models to benefit the community.</li>
<li><strong>摘要：</strong>基于指令的图像编辑已成为一个突出的研究领域，受益于图像生成基础模型，已经实现了很高的美学质量，使得指令跟踪能力成为主要挑战。现有方法通过监督学习或强化学习来提高指令依从性，但由于固有的随机性和缺乏深思熟虑，单轮成功率仍然有限。在这项工作中，我们提出了一个在编辑时“思考”的深思熟虑的编辑框架，它通过迭代执行边编辑边思考的循环来模拟人类的认知循环：批评结果和细化指令，然后重复生成直到满意为止。具体来说，我们训练一个 MLLM，EditThinker，作为该框架的推理引擎，共同产生批评分数、推理过程和细化指令。我们采用强化学习来使 EditThinker 的思维与其编辑保持一致，从而产生更有针对性的指令改进。对四个基准的广泛实验表明，我们的方法极大地提高了任何图像编辑模型的指令跟踪能力。我们将发布我们的数据构建框架、数据集和模型以造福社区。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
