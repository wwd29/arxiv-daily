<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-04-11</h1>
<h3>Title: Objaverse++: Curated 3D Object Dataset with Quality Annotations</h3>
<ul>
<li><strong>Authors: </strong>Chendi Lin, Heshan Liu, Qunshu Lin, Zachary Bright, Shitao Tang, Yihui He, Minghao Liu, Ling Zhu, Cindy Le</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07334">https://arxiv.org/abs/2504.07334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07334">https://arxiv.org/pdf/2504.07334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07334]] Objaverse++: Curated 3D Object Dataset with Quality Annotations(https://arxiv.org/abs/2504.07334)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>This paper presents Objaverse++, a curated subset of Objaverse enhanced with detailed attribute annotations by human experts. Recent advances in 3D content generation have been driven by large-scale datasets such as Objaverse, which contains over 800,000 3D objects collected from the Internet. Although Objaverse represents the largest available 3D asset collection, its utility is limited by the predominance of low-quality models. To address this limitation, we manually annotate 10,000 3D objects with detailed attributes, including aesthetic quality scores, texture color classifications, multi-object composition flags, transparency characteristics, etc. Then, we trained a neural network capable of annotating the tags for the rest of the Objaverse dataset. Through experiments and a user study on generation results, we demonstrate that models pre-trained on our quality-focused subset achieve better performance than those trained on the larger dataset of Objaverse in image-to-3D generation tasks. In addition, by comparing multiple subsets of training data filtered by our tags, our results show that the higher the data quality, the faster the training loss converges. These findings suggest that careful curation and rich annotation can compensate for the raw dataset size, potentially offering a more efficient path to develop 3D generative models. We release our enhanced dataset of approximately 500,000 curated 3D models to facilitate further research on various downstream tasks in 3D computer vision. In the near future, we aim to extend our annotations to cover the entire Objaverse dataset.</li>
<li><strong>摘要：</strong>本文介绍了Objaverse ++，这是人类专家的详细属性注释的策划的Objaverse的策划子集。 3D内容生成的最新进展是由大规模数据集（例如Objaverse）驱动的，objaverse包含从互联网收集的800,000多个3D对象。尽管Objaverse代表了最大的3D资产收集，但其效用受到低质量模型的优势的限制。为了解决此限制，我们手动注释10,000个具有详细属性的3D对象，包括美学质量得分，纹理色彩分类，多对象组合构，透明度特征等。通过实验和对生成结果的用户研究，我们证明了以质量为中心的子集进行预训练的模型比在图像到3D生成任务中较大的OBJAVERSE培训的模型获得了更好的性能。此外，通过比较由我们的标签过滤的多个训练数据的子集，我们的结果表明，数据质量越高，训练损失收敛的速度就越快。这些发现表明，仔细的策展和丰富的注释可以补偿原始数据集的大小，从而有可能提供更有效的途径来开发3D生成模型。我们发布了大约500,000个策划的3D模型的增强数据集，以促进3D计算机视觉中各种下游任务的进一步研究。在不久的将来，我们旨在扩展注释以涵盖整个Objaverse数据集。</li>
</ul>

<h3>Title: Zeus: Zero-shot LLM Instruction for Union Segmentation in Multimodal Medical Imaging</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Dai, Kai Ye, Guodong Liu, Haoteng Tang, Liang Zhan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07336">https://arxiv.org/abs/2504.07336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07336">https://arxiv.org/pdf/2504.07336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07336]] Zeus: Zero-shot LLM Instruction for Union Segmentation in Multimodal Medical Imaging(https://arxiv.org/abs/2504.07336)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Medical image segmentation has achieved remarkable success through the continuous advancement of UNet-based and Transformer-based foundation backbones. However, clinical diagnosis in the real world often requires integrating domain knowledge, especially textual information. Conducting multimodal learning involves visual and text modalities shown as a solution, but collecting paired vision-language datasets is expensive and time-consuming, posing significant challenges. Inspired by the superior ability in numerous cross-modal tasks for Large Language Models (LLMs), we proposed a novel Vision-LLM union framework to address the issues. Specifically, we introduce frozen LLMs for zero-shot instruction generation based on corresponding medical images, imitating the radiology scanning and report generation process. {To better approximate real-world diagnostic processes}, we generate more precise text instruction from multimodal radiology images (e.g., T1-w or T2-w MRI and CT). Based on the impressive ability of semantic understanding and rich knowledge of LLMs. This process emphasizes extracting special features from different modalities and reunion the information for the ultimate clinical diagnostic. With generated text instruction, our proposed union segmentation framework can handle multimodal segmentation without prior collected vision-language datasets. To evaluate our proposed method, we conduct comprehensive experiments with influential baselines, the statistical results and the visualized case study demonstrate the superiority of our novel method.}</li>
<li><strong>摘要：</strong>通过持续发展基于UNET和基于变压器的基础骨架，医疗图像细分取得了巨大的成功。但是，现实世界中的临床诊断通常需要整合域知识，尤其是文本信息。进行多模式学习涉及作为解决方案显示的视觉和文本方式，但是收集配对的视觉语言数据集是昂贵且耗时的，因此提出了重大挑战。受到大型语言模型（LLM）众多跨模式任务的卓越能力的启发，我们提出了一个新颖的Vision-Llm联合框架来解决这些问题。具体而言，我们基于相应的医学图像引入了冷冻的LLM，以生成零摄指令生成，并模仿放射学扫描和报告生成过程。 {为了更好地近似现实世界的诊断过程}，我们从多模式放射学图像（例如T1-W或T2-W MRI和CT）中生成更精确的文本指令。基于语义理解和对LLM的丰富知识的令人印象深刻的能力。该过程强调从不同方式中提取特殊特征，并使信息与最终临床诊断的信息团聚。通过生成的文本指令，我们提出的工会细分框架可以处理多模式分割，而无需事先收集的视觉语言数据集。为了评估我们提出的方法，我们对有影响力的基线进行了全面的实验，统计结果和可视化的案例研究证明了我们的新方法的优越性。}}</li>
</ul>

<h3>Title: Leveraging deep learning for plant disease identification: a bibliometric analysis in SCOPUS from 2018 to 2024</h3>
<ul>
<li><strong>Authors: </strong>Enow Takang Achuo Albert, Ngalle Hermine Bille, Ngonkeu Mangaptche Eddy Leonard</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07342">https://arxiv.org/abs/2504.07342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07342">https://arxiv.org/pdf/2504.07342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07342]] Leveraging deep learning for plant disease identification: a bibliometric analysis in SCOPUS from 2018 to 2024(https://arxiv.org/abs/2504.07342)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This work aimed to present a bibliometric analysis of deep learning research for plant disease identification, with a special focus on generative modeling. A thorough analysis of SCOPUS-sourced bibliometric data from 253 documents was performed. Key performance metrics such as accuracy, precision, recall, and F1-score were analyzed for generative modeling. The findings highlighted significant contributions from some authors Too and Arnal Barbedo, whose works had notable citation counts, suggesting their influence on the academic community. Co-authorship networks revealed strong collaborative clusters, while keyword analysis identified emerging research gaps. This study highlights the role of collaboration and citation metrics in shaping research directions and enhancing the impact of scholarly work in applications of deep learning to plant disease identification. Future research should explore the methodologies of highly cited studies to inform best practices and policy-making.</li>
<li><strong>摘要：</strong>这项工作旨在对植物疾病鉴定的深度学习研究进行文献计量分析，并特别关注生成建模。对253个文档的Scopus采购文献计量数据进行了彻底的分析。分析了关键性能指标，例如精度，精度，回忆和F1得分，以进行生成建模。这些发现也强调了一些作者和Arnal Barbedo的重大贡献，Arnal Barbedo的作品具有显着的引用，这表明它们对学术界的影响。共同创作网络揭示了强大的协作集群，而关键字分析确定了新兴的研究差距。这项研究强调了协作和引文指标在塑造研究方向的作用，并增强了学术工作在深度学习疾病鉴定的应用中的影响。未来的研究应探讨高度引用的研究的方法论，以告知最佳实践和决策。</li>
</ul>

<h3>Title: Model Discrepancy Learning: Synthetic Faces Detection Based on Multi-Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Qingchao Jiang, Zhishuo Xu, Zhiying Zhu, Ning Chen, Haoyue Wang, Zhongjie Ba</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07382">https://arxiv.org/abs/2504.07382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07382">https://arxiv.org/pdf/2504.07382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07382]] Model Discrepancy Learning: Synthetic Faces Detection Based on Multi-Reconstruction(https://arxiv.org/abs/2504.07382)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Advances in image generation enable hyper-realistic synthetic faces but also pose risks, thus making synthetic face detection crucial. Previous research focuses on the general differences between generated images and real images, often overlooking the discrepancies among various generative techniques. In this paper, we explore the intrinsic relationship between synthetic images and their corresponding generation technologies. We find that specific images exhibit significant reconstruction discrepancies across different generative methods and that matching generation techniques provide more accurate reconstructions. Based on this insight, we propose a Multi-Reconstruction-based detector. By reversing and reconstructing images using multiple generative models, we analyze the reconstruction differences among real, GAN-generated, and DM-generated images to facilitate effective differentiation. Additionally, we introduce the Asian Synthetic Face Dataset (ASFD), containing synthetic Asian faces generated with various GANs and DMs. This dataset complements existing synthetic face datasets. Experimental results demonstrate that our detector achieves exceptional performance, with strong generalization and robustness.</li>
<li><strong>摘要：</strong>图像产生的进展使超现实的合成面部面孔和构成风险，从而使合成面部检测至关重要。先前的研究重点是生成的图像和真实图像之间的一般差异，通常忽略了各种生成技术之间的差异。在本文中，我们探讨了合成图像及其相应发电技术之间的内在关系。我们发现，特定图像在不同的生成方法上表现出显着的重建差异，并且匹配的生成技术提供了更准确的重建。基于此洞察力，我们提出了一个基于多重构的检测器。通过使用多个生成模型逆转和重建图像，我们分析了真实，gan生成和DM生成的图像之间的重建差异，以促进有效的分化。此外，我们介绍了亚洲合成面数据集（ASFD），其中包含由各种gan和dms产生的合成亚洲面孔。该数据集补充了现有的合成面部数据集。实验结果表明，我们的检测器具有强大的概括和鲁棒性，可以实现出色的性能。</li>
</ul>

<h3>Title: ID-Booth: Identity-consistent Face Generation with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Darian Tomašević, Fadi Boutros, Chenhao Lin, Naser Damer, Vitomir Štruc, Peter Peer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07392">https://arxiv.org/abs/2504.07392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07392">https://arxiv.org/pdf/2504.07392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07392]] ID-Booth: Identity-consistent Face Generation with Diffusion Models(https://arxiv.org/abs/2504.07392)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative modeling have enabled the generation of high-quality synthetic data that is applicable in a variety of domains, including face recognition. Here, state-of-the-art generative models typically rely on conditioning and fine-tuning of powerful pretrained diffusion models to facilitate the synthesis of realistic images of a desired identity. Yet, these models often do not consider the identity of subjects during training, leading to poor consistency between generated and intended identities. In contrast, methods that employ identity-based training objectives tend to overfit on various aspects of the identity, and in turn, lower the diversity of images that can be generated. To address these issues, we present in this paper a novel generative diffusion-based framework, called ID-Booth. ID-Booth consists of a denoising network responsible for data generation, a variational auto-encoder for mapping images to and from a lower-dimensional latent space and a text encoder that allows for prompt-based control over the generation procedure. The framework utilizes a novel triplet identity training objective and enables identity-consistent image generation while retaining the synthesis capabilities of pretrained diffusion models. Experiments with a state-of-the-art latent diffusion model and diverse prompts reveal that our method facilitates better intra-identity consistency and inter-identity separability than competing methods, while achieving higher image diversity. In turn, the produced data allows for effective augmentation of small-scale datasets and training of better-performing recognition models in a privacy-preserving manner. The source code for the ID-Booth framework is publicly available at this https URL.</li>
<li><strong>摘要：</strong>生成建模的最新进展使得适用于各种领域（包括面部识别）的高质量合成数据。在这里，最先进的生成模型通常依赖于强大的预审预定扩散模型的调节和微调来促进综合所需身份的现实图像。但是，这些模型通常不会考虑训练期间受试者的身份，从而导致产生和预期身份之间的一致性差。相反，采用基于身份的培训目标的方法倾向于在身份的各个方面过度拟合，进而降低了可以生成的图像的多样性。为了解决这些问题，我们在本文中介绍了一种新颖的基于生成扩散的框架，称为ID-Booth。 ID-Booth由负责数据生成的剥落网络组成，该网络是一个用于绘制较低维的潜在空间和从映射图像的变异自动编码器以及一个允许对生成过程进行迅速控制的文本编码器。该框架利用了一个新颖的三重态身份训练目标​​，并实现了符合身份的图像产生，同时保留了经过预验扩散模型的合成能力。使用最先进的潜在扩散模型和不同提示的实验表明，与竞争方法相比，我们的方法促进了更好的身份一致性和确认性的可分离性，同时实现了更高的图像多样性。反过来，产生的数据允许以隐私性的方式有效地扩大小规模数据集并培训表现更好的识别模型。 ID-Booth框架的源代码可在此HTTPS URL上公开获得。</li>
</ul>

<h3>Title: ClimateBench-M: A Multi-Modal Climate Data Benchmark with a Simple Generative Method</h3>
<ul>
<li><strong>Authors: </strong>Dongqi Fu, Yada Zhu, Zhining Liu, Lecheng Zheng, Xiao Lin, Zihao Li, Liri Fang, Katherine Tieu, Onkar Bhardwaj, Kommy Weldemariam, Hanghang Tong, Hendrik Hamann, Jingrui He</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07394">https://arxiv.org/abs/2504.07394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07394">https://arxiv.org/pdf/2504.07394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07394]] ClimateBench-M: A Multi-Modal Climate Data Benchmark with a Simple Generative Method(https://arxiv.org/abs/2504.07394)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Climate science studies the structure and dynamics of Earth's climate system and seeks to understand how climate changes over time, where the data is usually stored in the format of time series, recording the climate features, geolocation, time attributes, etc. Recently, much research attention has been paid to the climate benchmarks. In addition to the most common task of weather forecasting, several pioneering benchmark works are proposed for extending the modality, such as domain-specific applications like tropical cyclone intensity prediction and flash flood damage estimation, or climate statement and confidence level in the format of natural language. To further motivate the artificial general intelligence development for climate science, in this paper, we first contribute a multi-modal climate benchmark, i.e., ClimateBench-M, which aligns (1) the time series climate data from ERA5, (2) extreme weather events data from NOAA, and (3) satellite image data from NASA HLS based on a unified spatial-temporal granularity. Second, under each data modality, we also propose a simple but strong generative method that could produce competitive performance in weather forecasting, thunderstorm alerts, and crop segmentation tasks in the proposed ClimateBench-M. The data and code of ClimateBench-M are publicly available at this https URL.</li>
<li><strong>摘要：</strong>气候科学研究地球气候系统的结构和动力学，并试图了解气候如何随着时间的流逝而变化，其中数据通常以时间序列的格式存储，记录气候特征，地理位置，时间属性等。最近，对气候基准的研究关注很多。除了最常见的天气预报任务外，还提出了一些开创性的基准作品来扩展方式，例如特定于领域的应用，例如热带旋风强度预测和山洪洪水损害估计，或自然语言形式的气候声明和信心水平。 To further motivate the artificial general intelligence development for climate science, in this paper, we first contribute a multi-modal climate benchmark, i.e., ClimateBench-M, which aligns (1) the time series climate data from ERA5, (2) extreme weather events data from NOAA, and (3) satellite image data from NASA HLS based on a unified spatial-temporal granularity.其次，在每种数据模式下，我们还提出了一种简单但强大的生成方法，该方法可以在拟议的ClimateBench-M中在天气预报，雷暴警报和作物细分任务中产生竞争性能。此HTTPS URL可在此HTTPS URL上公开获取ClimateBench-M的数据和代码。</li>
</ul>

<h3>Title: LauraTSE: Target Speaker Extraction using Auto-Regressive Decoder-Only Language Models</h3>
<ul>
<li><strong>Authors: </strong>Beilong Tang, Bang Zeng, Ming Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07402">https://arxiv.org/abs/2504.07402</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07402">https://arxiv.org/pdf/2504.07402</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07402]] LauraTSE: Target Speaker Extraction using Auto-Regressive Decoder-Only Language Models(https://arxiv.org/abs/2504.07402)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We propose LauraTSE, an Auto-Regressive Decoder-Only Language Model for Target Speaker Extraction (TSE) based on the LauraGPT backbone. It employs a small-scale auto-regressive decoder-only language model which takes the continuous representations for both the mixture and the reference speeches and produces the first few layers of the target speech's discrete codec representations. In addition, a one-step encoder-only language model reconstructs the sum of the predicted codec embeddings using both the mixture and the reference information. Our approach achieves superior or comparable performance to existing generative and discriminative TSE models. To the best of our knowledge, LauraTSE is the first single-task TSE model to leverage an auto-regressive decoder-only language model as the backbone.</li>
<li><strong>摘要：</strong>我们提出了Lauratse，这是一种基于月球骨干的自动回归解码器语言模型（TSE）。它采用小型自动回归解码器语言模型，该模型采用混合物和参考语音的连续表示，并产生目标语音的离散编解码器表示的前几层。此外，使用混合物和参考信息同时重建一个仅编码语言模型的单步编码模型。我们的方法比现有的生成和歧视性TSE模型实现了优越或可比的性能。据我们所知，劳拉特（Lauratse）是第一款单个任务TSE模型，它利用自动回归解码器的语言模型作为骨干。</li>
</ul>

<h3>Title: FlexIP: Dynamic Control of Preservation and Personality for Customized Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Linyan Huang, Haonan Lin, Yanning Zhou, Kaiwen Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07405">https://arxiv.org/abs/2504.07405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07405">https://arxiv.org/pdf/2504.07405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07405]] FlexIP: Dynamic Control of Preservation and Personality for Customized Image Generation(https://arxiv.org/abs/2504.07405)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of 2D generative models, preserving subject identity while enabling diverse editing has emerged as a critical research focus. Existing methods typically face inherent trade-offs between identity preservation and personalized manipulation. We introduce FlexIP, a novel framework that decouples these objectives through two dedicated components: a Personalization Adapter for stylistic manipulation and a Preservation Adapter for identity maintenance. By explicitly injecting both control mechanisms into the generative model, our framework enables flexible parameterized control during inference through dynamic tuning of the weight adapter. Experimental results demonstrate that our approach breaks through the performance limitations of conventional methods, achieving superior identity preservation while supporting more diverse personalized generation capabilities (Project Page: this https URL).</li>
<li><strong>摘要：</strong>随着2D生成模型的快速发展，在启用多样化编辑的同时，保留了主题身份，成为关键的研究重点。现有方法通常面临身份保存和个性化操作之间的固有权衡。我们介绍了Flexip，这是一个新颖的框架，可以通过两个专用组件来解除这些目标：一种用于风格操纵的个性化适配器，以及用于维护身份的保护适配器。通过将两种控制机制显式地注入生成模型，我们的框架可以通过重量适配器的动态调整在推理过程中进行灵活的参数化控制。实验结果表明，我们的方法突破了传统方法的性能限制，在支持更多样化的个性化生成能力的同时，获得了卓越的身份（项目页面：此HTTPS URL）。</li>
</ul>

<h3>Title: Leveraging LLMs for Multimodal Retrieval-Augmented Radiology Report Generation via Key Phrase Extraction</h3>
<ul>
<li><strong>Authors: </strong>Kyoyun Choi, Byungmu Yoon, Soobum Kim, Jonggwon Park</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07415">https://arxiv.org/abs/2504.07415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07415">https://arxiv.org/pdf/2504.07415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07415]] Leveraging LLMs for Multimodal Retrieval-Augmented Radiology Report Generation via Key Phrase Extraction(https://arxiv.org/abs/2504.07415)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Automated radiology report generation (RRG) holds potential to reduce radiologists' workload, especially as recent advancements in large language models (LLMs) enable the development of multimodal models for chest X-ray (CXR) report generation. However, multimodal LLMs (MLLMs) are resource-intensive, requiring vast datasets and substantial computational cost for training. To address these challenges, we propose a retrieval-augmented generation approach that leverages multimodal retrieval and LLMs to generate radiology reports while mitigating hallucinations and reducing computational demands. Our method uses LLMs to extract key phrases from radiology reports, effectively focusing on essential diagnostic information. Through exploring effective training strategies, including image encoder structure search, adding noise to text embeddings, and additional training objectives, we combine complementary pre-trained image encoders and adopt contrastive learning between text and semantic image embeddings. We evaluate our approach on MIMIC-CXR dataset, achieving state-of-the-art results on CheXbert metrics and competitive RadGraph F1 metric alongside MLLMs, without requiring LLM fine-tuning. Our method demonstrates robust generalization for multi-view RRG, making it suitable for comprehensive clinical applications.</li>
<li><strong>摘要：</strong>自动放射学报告生成（RRG）具有减少放射科医生的工作量的潜力，尤其是随着大语言模型（LLMS）的最新进展，可以开发用于胸部X射线（CXR）报告生成的多模式模型。但是，多模式LLM（MLLM）是资源密集的，需要庞大的数据集和大量的培训计算成本。为了应对这些挑战，我们提出了一种检索型生成方法，该方法利用多模式检索和LLMS生成放射学报告，同时减轻幻觉并减少计算需求。我们的方法使用LLM从放射学报告中提取关键短语，从而有效地关注基本诊断信息。通过探索有效的培训策略，包括图像编码器结构搜索，在文本嵌入中添加噪音以及其他培训目标，我们结合了互补的预训练的图像编码器，并在文本和语义图像嵌入之间采用对比度学习。我们评估了对模拟CXR数据集的方法，并在不需要LLM微调的情况下，在Chexbert指标和竞争性的Radgraph F1指标上获得了最新的结果。我们的方法证明了多视图RRG的鲁棒性概括，使其适用于全面的临床应用。</li>
</ul>

<h3>Title: Unifying and extending Diffusion Models through PDEs for solving Inverse Problems</h3>
<ul>
<li><strong>Authors: </strong>Agnimitra Dasgupta, Alexsander Marciano da Cunha, Ali Fardisi, Mehrnegar Aminy, Brianna Binder, Bryan Shaddy, Assad A Oberai</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.CO, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07437">https://arxiv.org/abs/2504.07437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07437">https://arxiv.org/pdf/2504.07437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07437]] Unifying and extending Diffusion Models through PDEs for solving Inverse Problems(https://arxiv.org/abs/2504.07437)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as powerful generative tools with applications in computer vision and scientific machine learning (SciML), where they have been used to solve large-scale probabilistic inverse problems. Traditionally, these models have been derived using principles of variational inference, denoising, statistical signal processing, and stochastic differential equations. In contrast to the conventional presentation, in this study we derive diffusion models using ideas from linear partial differential equations and demonstrate that this approach has several benefits that include a constructive derivation of the forward and reverse processes, a unified derivation of multiple formulations and sampling strategies, and the discovery of a new class of models. We also apply the conditional version of these models to solving canonical conditional density estimation problems and challenging inverse problems. These problems help establish benchmarks for systematically quantifying the performance of different formulations and sampling strategies in this study, and for future studies. Finally, we identify and implement a mechanism through which a single diffusion model can be applied to measurements obtained from multiple measurement operators. Taken together, the contents of this manuscript provide a new understanding and several new directions in the application of diffusion models to solving physics-based inverse problems.</li>
<li><strong>摘要：</strong>扩散模型已成为具有计算机视觉和科学机器学习（SCIML）应用的强大生成工具，在该工具中已被用来解决大规模的概率逆问题。传统上，这些模型是使用变异推断，降解，统计信号处理和随机微分方程的原理得出的。与常规介绍相反，在这项研究中，我们使用线性偏微分方程中的思想得出了扩散模型，并证明该方法具有多种好处，其中包括对正向和反向过程的建设性推导，统一的多种公式和采样策略的统一推导，以及对新模型的发现。我们还将这些模型的条件版本应用于解决规范的条件密度估计问题和具有挑战性的反问题。这些问题有助于建立基准，以系统地量化本研究中不同配方和采样策略的性能以及未来的研究。最后，我们确定并实施了一种机制，可以将单个扩散模型应用于从多个测量运算符获得的测量值中。综上所述，本手稿的内容为扩散模型应用于解决基于物理的逆问题的应用提供了新的理解和几个新方向。</li>
</ul>

<h3>Title: LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Juzheng Zhang, Jiacheng You, Ashwinee Panda, Tom Goldstein</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07448">https://arxiv.org/abs/2504.07448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07448">https://arxiv.org/pdf/2504.07448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07448]] LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation(https://arxiv.org/abs/2504.07448)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs notable overhead and suffers from parameter interference in multi-task scenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet effective approach that freezes the projection matrices $A$ as random projections and sparsifies the matrices $B$ using task-specific masks. This design substantially reduces the number of trainable parameters while maintaining strong task performance. Moreover, LoRI minimizes cross-task interference in adapter merging by leveraging the orthogonality between adapter subspaces, and supports continual learning by using sparsity to mitigate catastrophic forgetting. Extensive experiments across natural language understanding, mathematical reasoning, code generation, and safety alignment tasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT methods, while using up to 95% fewer trainable parameters than LoRA. In multi-task experiments, LoRI enables effective adapter merging and continual learning with reduced cross-task interference. Code is available at: this https URL</li>
<li><strong>摘要：</strong>低级适应性（LORA）已成为大型语言模型（LLMS）的流行参数效率微调（PEFT）方法，但它仍然引起了鲜明的开销，并且在多任务场景中受到参数干扰的影响。我们建议使用减少干扰（Lori）的洛拉（Lora），这是一种简单而有效的方法，可以将投影矩阵$ a $冻结，因为随机预测，并使用特定于任务的掩码对矩阵$ b $散布。这种设计大大减少了可训练参数的数量，同时保持了强大的任务性能。此外，Lori通过利用适配器子空间之间的正交性来最大程度地减少适配器合并中的交叉任务干扰，并通过使用稀疏性来减轻灾难性遗忘，从而支持持续学习。跨自然语言理解，数学推理，代码生成和安全对准任务的广泛实验表明，Lori的表现优于完整的微调和现有的PEFT方法，而使用的可训练参数比Lora少95％。在多任务实验中，Lori可以通过减少的交叉任务干扰来实现有效的适配器合并和持续学习。代码可用：此HTTPS URL</li>
</ul>

<h3>Title: Learning Universal Features for Generalizable Image Forgery Localization</h3>
<ul>
<li><strong>Authors: </strong>Hengrun Zhao, Yunzhi Zhuge, Yifan Wang, Lijun Wang, Huchuan Lu, Yu Zeng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07462">https://arxiv.org/abs/2504.07462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07462">https://arxiv.org/pdf/2504.07462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07462]] Learning Universal Features for Generalizable Image Forgery Localization(https://arxiv.org/abs/2504.07462)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In recent years, advanced image editing and generation methods have rapidly evolved, making detecting and locating forged image content increasingly challenging. Most existing image forgery detection methods rely on identifying the edited traces left in the image. However, because the traces of different forgeries are distinct, these methods can identify familiar forgeries included in the training data but struggle to handle unseen ones. In response, we present an approach for Generalizable Image Forgery Localization (GIFL). Once trained, our model can detect both seen and unseen forgeries, providing a more practical and efficient solution to counter false information in the era of generative AI. Our method focuses on learning general features from the pristine content rather than traces of specific forgeries, which are relatively consistent across different types of forgeries and therefore can be used as universal features to locate unseen forgeries. Additionally, as existing image forgery datasets are still dominated by traditional hand-crafted forgeries, we construct a new dataset consisting of images edited by various popular deep generative image editing methods to further encourage research in detecting images manipulated by deep generative models. Extensive experimental results show that the proposed approach outperforms state-of-the-art methods in the detection of unseen forgeries and also demonstrates competitive results for seen forgeries. The code and dataset are available at this https URL.</li>
<li><strong>摘要：</strong>近年来，先进的图像编辑和发电方法已经快速发展，使检测和定位伪造的图像内容越来越具有挑战性。大多数现有的图像伪造检测方法都依赖于识别图像中剩余的编辑痕迹。但是，由于不同伪造的痕迹是不同的，因此这些方法可以识别训练数据中包含的熟悉的伪造，但很难处理看不见的伪造。作为回应，我们提出了一种可推广图像伪造定位（GIFL）的方法。一旦受过培训，我们的模型就可以检测到可见的和看不见的伪造，从而提供了一种更实用，更有效的解决方案，以应对生成AI时代的虚假信息。我们的方法着重于从原始内容而不是特定伪造的痕迹中学习一般特征，这些特征在不同类型的伪造方面相对一致，因此可以用作通用特征来定位看不见的伪造。此外，由于现有的图像伪造数据集仍然由传统的手工制作的伪造所主导，因此我们构建了一个新的数据集，该数据集由由各种流行的深层生成图像编辑方法编辑的图像组成，以进一步鼓励研究通过深层生成模型操纵的图像进行研究。广泛的实验结果表明，所提出的方法在检测看不见的伪造方面优于最先进的方法，并且还证明了可见伪造的竞争结果。该代码和数据集可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Event Signal Filtering via Probability Flux Estimation</h3>
<ul>
<li><strong>Authors: </strong>Jinze Chen, Wei Zhai, Yang Cao, Bin Li, Zheng-Jun Zha</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07503">https://arxiv.org/abs/2504.07503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07503">https://arxiv.org/pdf/2504.07503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07503]] Event Signal Filtering via Probability Flux Estimation(https://arxiv.org/abs/2504.07503)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generation, generative</a></li>
<li><strong>Abstract: </strong>Events offer a novel paradigm for capturing scene dynamics via asynchronous sensing, but their inherent randomness often leads to degraded signal quality. Event signal filtering is thus essential for enhancing fidelity by reducing this internal randomness and ensuring consistent outputs across diverse acquisition conditions. Unlike traditional time series that rely on fixed temporal sampling to capture steady-state behaviors, events encode transient dynamics through polarity and event intervals, making signal modeling significantly more complex. To address this, the theoretical foundation of event generation is revisited through the lens of diffusion processes. The state and process information within events is modeled as continuous probability flux at threshold boundaries of the underlying irradiance diffusion. Building on this insight, a generative, online filtering framework called Event Density Flow Filter (EDFilter) is introduced. EDFilter estimates event correlation by reconstructing the continuous probability flux from discrete events using nonparametric kernel smoothing, and then resamples filtered events from this flux. To optimize fidelity over time, spatial and temporal kernels are employed in a time-varying optimization framework. A fast recursive solver with O(1) complexity is proposed, leveraging state-space models and lookup tables for efficient likelihood computation. Furthermore, a new real-world benchmark Rotary Event Dataset (RED) is released, offering microsecond-level ground truth irradiance for full-reference event filtering evaluation. Extensive experiments validate EDFilter's performance across tasks like event filtering, super-resolution, and direct event-based blob tracking. Significant gains in downstream applications such as SLAM and video reconstruction underscore its robustness and effectiveness.</li>
<li><strong>摘要：</strong>事件提供了一种新颖的范式，用于通过异步感应捕获场景动态，但是它们固有的随机性通常会导致信号质量下降。因此，事件信号滤波对于通过降低这种内部随机性并确保在各种采集条件下的输出一致的输出来提高忠诚度至关重要。与依靠固定时间采样来捕获稳态行为的传统时间序列不同，事件通过极性和事件间隔编码瞬态动力学，从而使信号建模变得更加复杂。为了解决这个问题，通过扩散过程的角度重新探讨了事件产生的理论基础。事件中的状态和过程信息被建模为在基础辐射扩散的阈值边界处的连续概率通量。在此洞察力的基础上，引入了一个生成的在线过滤框架，称为事件密度流过滤器（EDFILTER）。 Edfilter通过使用非参数内核平滑而从离散事件中重建连续概率通量来估计事件相关性，然后从此通量中重新示例过滤事件。为了优化忠诚度，随着时间的流逝，空间和时间内核被用于时变优化框架中。提出了具有O（1）复杂性的快速递归求解器，利用状态空间模型和查找表进行有效的可能性计算。此外，发布了一个新的现实基准旋转事件数据集（RED），为全参考事件过滤评估提供了微秒级地面真相辐照度。广泛的实验验证了Edfilter在事件过滤，超分辨率和基于事件的BLOB跟踪等任务中的性能。在诸如SLAM和视频重建之类的下游应用中的显着增长强调了其稳健性和有效性。</li>
</ul>

<h3>Title: VideoExpert: Augmented LLM for Temporal-Sensitive Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Henghao Zhao, Ge-Peng Ji, Rui Yan, Huan Xiong, Zechao Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07519">https://arxiv.org/abs/2504.07519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07519">https://arxiv.org/pdf/2504.07519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07519]] VideoExpert: Augmented LLM for Temporal-Sensitive Video Understanding(https://arxiv.org/abs/2504.07519)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The core challenge in video understanding lies in perceiving dynamic content changes over time. However, multimodal large language models struggle with temporal-sensitive video tasks, which requires generating timestamps to mark the occurrence of specific events. Existing strategies require MLLMs to generate absolute or relative timestamps directly. We have observed that those MLLMs tend to rely more on language patterns than visual cues when generating timestamps, affecting their performance. To address this problem, we propose VideoExpert, a general-purpose MLLM suitable for several temporal-sensitive video tasks. Inspired by the expert concept, VideoExpert integrates two parallel modules: the Temporal Expert and the Spatial Expert. The Temporal Expert is responsible for modeling time sequences and performing temporal grounding. It processes high-frame-rate yet compressed tokens to capture dynamic variations in videos and includes a lightweight prediction head for precise event localization. The Spatial Expert focuses on content detail analysis and instruction following. It handles specially designed spatial tokens and language input, aiming to generate content-related responses. These two experts collaborate seamlessly via a special token, ensuring coordinated temporal grounding and content generation. Notably, the Temporal and Spatial Experts maintain independent parameter sets. By offloading temporal grounding from content generation, VideoExpert prevents text pattern biases in timestamp predictions. Moreover, we introduce a Spatial Compress module to obtain spatial tokens. This module filters and compresses patch tokens while preserving key information, delivering compact yet detail-rich input for the Spatial Expert. Extensive experiments demonstrate the effectiveness and versatility of the VideoExpert.</li>
<li><strong>摘要：</strong>视频理解的核心挑战在于感知动态内容随时间变化。但是，多模式的大语言模型与时间敏感的视频任务斗争，这需要生成时间戳以标记特定事件的发生。现有策略要求MLLM直接生成绝对时间戳或相对时间戳。我们已经观察到，这些MLLM倾向于在产生时间戳时更多地依赖语言模式，从而影响其性能。为了解决这个问题，我们提出了VideoExpert，这是一种适用于几个时间敏感视频任务的通用MLLM。受到专家概念的启发，VideoExpert集成了两个并行模块：临时专家和空间专家。临时专家负责建模时间序列和执行时间基础。它处理高框架速率但压缩令牌以捕获视频中的动态变化，并包括一个轻巧的预测头，以进行精确的事件本地化。空间专家专注于内容细节分析和教学以下。它处理特殊设计的空间令牌和语言输入，旨在产生与内容相关的响应。这两位专家通过特殊的令牌无缝地合作，以确保暂时的基础和内容产生。值得注意的是，时间和空间专家保持独立的参数集。通过从内容产生的时间基础上卸载时间基础，VideoExpert可以防止时间戳预测中的文本模式偏见。此外，我们引入了一个空间压缩模块以获得空间令牌。该模块在保留关键信息的同时过滤和压缩补丁令牌，为空间专家提供紧凑而细节的输入。广泛的实验证明了视频专家的有效性和多功能性。</li>
</ul>

<h3>Title: Adversarial Subspace Generation for Outlier Detection in High-Dimensional Data</h3>
<ul>
<li><strong>Authors: </strong>Jose Cribeiro-Ramallo, Federico Matteucci, Paul Enciu, Alexander Jenke, Vadim Arzamasov, Thorsten Strufe, Klemens Böhm</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07522">https://arxiv.org/abs/2504.07522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07522">https://arxiv.org/pdf/2504.07522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07522]] Adversarial Subspace Generation for Outlier Detection in High-Dimensional Data(https://arxiv.org/abs/2504.07522)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Outlier detection in high-dimensional tabular data is challenging since data is often distributed across multiple lower-dimensional subspaces -- a phenomenon known as the Multiple Views effect (MV). This effect led to a large body of research focused on mining such subspaces, known as subspace selection. However, as the precise nature of the MV effect was not well understood, traditional methods had to rely on heuristic-driven search schemes that struggle to accurately capture the true structure of the data. Properly identifying these subspaces is critical for unsupervised tasks such as outlier detection or clustering, where misrepresenting the underlying data structure can hinder the performance. We introduce Myopic Subspace Theory (MST), a new theoretical framework that mathematically formulates the Multiple Views effect and writes subspace selection as a stochastic optimization problem. Based on MST, we introduce V-GAN, a generative method trained to solve such an optimization problem. This approach avoids any exhaustive search over the feature space while ensuring that the intrinsic data structure is preserved. Experiments on 42 real-world datasets show that using V-GAN subspaces to build ensemble methods leads to a significant increase in one-class classification performance -- compared to existing subspace selection, feature selection, and embedding methods. Further experiments on synthetic data show that V-GAN identifies subspaces more accurately while scaling better than other relevant subspace selection methods. These results confirm the theoretical guarantees of our approach and also highlight its practical viability in high-dimensional settings.</li>
<li><strong>摘要：</strong>高维表格数据中的离群值检测具有挑战性，因为数据通常分布在多个低维子空间中 - 这种现象称为多重视图效应（MV）。这种效果导致大量研究集中于采矿，称为子空间选择。但是，由于MV效应的确切性质尚未得到充分了解，因此，传统方法必须依靠启发式驱动的搜索方案，这些搜索方案难以准确捕获数据的真实结构。正确识别这些子空间对于无监督的任务（例如异常检测或聚类）至关重要，在这种任务中，歪曲基础数据结构可能会阻碍性能。我们介绍了近视子空间理论（MST），这是一个新的理论框架，在数学上提出了多个视图效应，并将子空间选择作为随机优化问题写入。基于MST，我们引入了V-GAN，这是一种用于解决此类优化问题的生成方法。这种方法可以避免在特征空间上进行任何详尽的搜索，同时确保保留固有的数据结构。与现有的子空间选择，特征选择和嵌入方法相比，使用V-GAN子空间构建集合方法的实验表明，使用V-GAN子空间构建集合方法的实验会显着增加一级分类性能。关于合成数据的进一步实验表明，V-GAN比其他相关子空间选择方法更准确地识别子空间。这些结果证实了我们方法的理论保证，也强调了其在高维环境中的实际生存能力。</li>
</ul>

<h3>Title: STeP: A General and Scalable Framework for Solving Video Inverse Problems with Spatiotemporal Diffusion Priors</h3>
<ul>
<li><strong>Authors: </strong>Bingliang Zhang, Zihui Wu, Berthy T. Feng, Yang Song, Yisong Yue, Katherine L. Bouman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07549">https://arxiv.org/abs/2504.07549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07549">https://arxiv.org/pdf/2504.07549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07549]] STeP: A General and Scalable Framework for Solving Video Inverse Problems with Spatiotemporal Diffusion Priors(https://arxiv.org/abs/2504.07549)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We study how to solve general Bayesian inverse problems involving videos using diffusion model priors. While it is desirable to use a video diffusion prior to effectively capture complex temporal relationships, due to the computational and data requirements of training such a model, prior work has instead relied on image diffusion priors on single frames combined with heuristics to enforce temporal consistency. However, these approaches struggle with faithfully recovering the underlying temporal relationships, particularly for tasks with high temporal uncertainty. In this paper, we demonstrate the feasibility of practical and accessible spatiotemporal diffusion priors by fine-tuning latent video diffusion models from pretrained image diffusion models using limited videos in specific domains. Leveraging this plug-and-play spatiotemporal diffusion prior, we introduce a general and scalable framework for solving video inverse problems. We then apply our framework to two challenging scientific video inverse problems--black hole imaging and dynamic MRI. Our framework enables the generation of diverse, high-fidelity video reconstructions that not only fit observations but also recover multi-modal solutions. By incorporating a spatiotemporal diffusion prior, we significantly improve our ability to capture complex temporal relationships in the data while also enhancing spatial fidelity.</li>
<li><strong>摘要：</strong>我们研究了如何使用扩散模型先验解决涉及视频的一般贝叶斯逆问题。虽然希望在有效捕获复杂的时间关系之前使用视频扩散，但由于训练这种模型的计算和数据要求，但先前的工作依赖于单帧与启发式方法上的图像扩散先验来实现时间一致性。但是，这些方法在忠实地恢复基本的时间关系方面挣扎，特别是对于时间不确定性高的任务。在本文中，我们通过使用特定域中的有限视频从预审计的图像扩散模型中微调潜在的视频扩散模型来证明实用和可访问的时空扩散率的可行性。先前利用此插件时空扩散的情况，我们引入了一个通用且可扩展的框架，以解决视频逆问题。然后，我们将框架应用于两个具有挑战性的科学视频逆问题 - 黑色孔成像和动态MRI。我们的框架可以产生各种高保真的视频重建，这些视频重建不仅适合观察值，而且还恢复了多模式解决方案。通过先验合并时空扩散，我们显着提高了在数据中捕获复杂时间关系的能力，同时还提高了空间忠诚度。</li>
</ul>

<h3>Title: TokenFocus-VQA: Enhancing Text-to-Image Alignment with Position-Aware Focus and Multi-Perspective Aggregations on LVLMs</h3>
<ul>
<li><strong>Authors: </strong>Zijian Zhang, Xuhui Zheng, Xuecheng Wu, Chong Peng, Xuezhi Cao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07556">https://arxiv.org/abs/2504.07556</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07556">https://arxiv.org/pdf/2504.07556</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07556]] TokenFocus-VQA: Enhancing Text-to-Image Alignment with Position-Aware Focus and Multi-Perspective Aggregations on LVLMs(https://arxiv.org/abs/2504.07556)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, quality assessment</a></li>
<li><strong>Abstract: </strong>While text-to-image (T2I) generation models have achieved remarkable progress in recent years, existing evaluation methodologies for vision-language alignment still struggle with the fine-grained semantic matching. Current approaches based on global similarity metrics often overlook critical token-level correspondences between textual descriptions and visual content. To this end, we present TokenFocus-VQA, a novel evaluation framework that leverages Large Vision-Language Models (LVLMs) through visual question answering (VQA) paradigm with position-specific probability optimization. Our key innovation lies in designing a token-aware loss function that selectively focuses on probability distributions at pre-defined vocabulary positions corresponding to crucial semantic elements, enabling precise measurement of fine-grained semantical alignment. The proposed framework further integrates ensemble learning techniques to aggregate multi-perspective assessments from diverse LVLMs architectures, thereby achieving further performance enhancement. Evaluated on the NTIRE 2025 T2I Quality Assessment Challenge Track 1, our TokenFocus-VQA ranks 2nd place (0.8445, only 0.0001 lower than the 1st method) on public evaluation and 2nd place (0.8426) on the official private test set, demonstrating superiority in capturing nuanced text-image correspondences compared to conventional evaluation methods.</li>
<li><strong>摘要：</strong>近年来，文本对图像（T2I）生成模型取得了显着的进步，但现有的视力语言一致性评估方法仍然在精细元素的语义匹配方面遇到困难。基于全球相似性指标的当前方法通常忽略文本描述和视觉内容之间的关键令牌级别的对应关系。为此，我们提出了TokenFocus-VQA，这是一个新颖的评估框架，该框架通过视觉问题答案（VQA）范式（VQA）范式利用大型视觉模型（LVLM），并具有特定于位置的概率优化。我们的关键创新在于设计令牌感知的损失函数，该功能有选择地关注与关键语义元素相对应的预定义词汇位置处的概率分布，从而可以精确地测量精细粒度的语义对准。拟议的框架进一步集成了集合学习技术，以汇总来自不同LVLMS架构的多观点评估，从而实现了进一步的绩效提高。在NTIRE 2025 T2I质量评估挑战赛1上进行了评估，我们的TokenFocus-VQA在公共评估中排名第二（0.8445，仅比第一个方法低0.0001），在官方私人测试集上排名第二（0.8426）在官方私人测试集上，在捕获文本图像对应量的优势方面表现出优势与公约评估相比。</li>
</ul>

<h3>Title: Diffusion Transformers for Tabular Data Time Series Generation</h3>
<ul>
<li><strong>Authors: </strong>Fabrizio Garuti, Enver Sangineto, Simone Luetto, Lorenzo Forni, Rita Cucchiara</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07566">https://arxiv.org/abs/2504.07566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07566">https://arxiv.org/pdf/2504.07566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07566]] Diffusion Transformers for Tabular Data Time Series Generation(https://arxiv.org/abs/2504.07566)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Tabular data generation has recently attracted a growing interest due to its different application scenarios. However, generating time series of tabular data, where each element of the series depends on the others, remains a largely unexplored domain. This gap is probably due to the difficulty of jointly solving different problems, the main of which are the heterogeneity of tabular data (a problem common to non-time-dependent approaches) and the variable length of a time series. In this paper, we propose a Diffusion Transformers (DiTs) based approach for tabular data series generation. Inspired by the recent success of DiTs in image and video generation, we extend this framework to deal with heterogeneous data and variable-length sequences. Using extensive experiments on six datasets, we show that the proposed approach outperforms previous work by a large margin.</li>
<li><strong>摘要：</strong>由于其不同的应用程序方案，表格数据生成最近引起了人们的兴趣。但是，生成表格数据的时间序列（该系列的每个元素都取决于其他元素）仍然是一个未开发的域。该差距可能是由于难以联合解决不同问题的困难，其中的主要问题是表格数据的异质性（非依赖于时间依赖性方法的问题）和时间序列的可变长度。在本文中，我们提出了基于表格数据系列生成的基于扩散变压器（DIT）的方法。受图像和视频生成的最新成功的启发，我们扩展了此框架以处理异质数据和可变长度序列。使用六个数据集上的大量实验，我们表明所提出的方法的表现优于先前的工作。</li>
</ul>

<h3>Title: PIDSR:ComplementaryPolarizedImageDemosaicingandSuper-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Shuangfan Zhou, Chu Zhou, Youwei Lyu, Heng Guo, Zhanyu Ma, Boxin Shi, Imari Sato</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07758">https://arxiv.org/abs/2504.07758</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07758">https://arxiv.org/pdf/2504.07758</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07758]] PIDSR:ComplementaryPolarizedImageDemosaicingandSuper-Resolution(https://arxiv.org/abs/2504.07758)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Polarization cameras can capture multiple polarized images with different polarizer angles in a single shot, bringing convenience to polarization-based downstream tasks. However, their direct outputs are color-polarization filter array (CPFA) raw images, requiring demosaicing to reconstruct full-resolution, full-color polarized images; unfortunately, this necessary step introduces artifacts that make polarization-related parameters such as the degree of polarization (DoP) and angle of polarization (AoP) prone to error. Besides, limited by the hardware design, the resolution of a polarization camera is often much lower than that of a conventional RGB camera. Existing polarized image demosaicing (PID) methods are limited in that they cannot enhance resolution, while polarized image super-resolution (PISR) methods, though designed to obtain high-resolution (HR) polarized images from the demosaicing results, tend to retain or even amplify errors in the DoP and AoP introduced by demosaicing artifacts. In this paper, we propose PIDSR, a joint framework that performs complementary Polarized Image Demosaicing and Super-Resolution, showing the ability to robustly obtain high-quality HR polarized images with more accurate DoP and AoP from a CPFA raw image in a direct manner. Experiments show our PIDSR not only achieves state-of-the-art performance on both synthetic and real data, but also facilitates downstream tasks.</li>
<li><strong>摘要：</strong>极化摄像机可以在单个镜头中捕获具有不同偏振器角度的多个偏光图像，从而为基于极化的下游任务带来便利。但是，它们的直接输出是色极化滤波器阵列（CPFA）原始图像，需要将示例化重建全分辨率，全彩色极化图像；不幸的是，这个必要的步骤引入了使极化相关参数（例如极化程度（DOP）和极化角度（AOP）容易出现误差的伪像。此外，受硬件设计的限制，极化相机的分辨率通常比传统的RGB摄像机低得多。现有的两极化图像表达式（PID）方法受到限制，因为它们无法增强分辨率，而极化图像超分辨率（PISR）方法虽然旨在从Demosaicing结果中获得高分辨率（HR）极化图像，但倾向于保留DEMOSAICAINICING ARTIFACTIFS的DOP和AOP中的dop和AOP中的ERPOR。在本文中，我们提出了PIDSR，PIDSR是一个执行互补极化图像表演和超分辨率的关节框架，显示了以直接方式从CPFA原始图像中稳健地获得具有更准确的DOP和AOP的高质量HR极化图像的能力。实验表明，我们的PIDSR不仅可以在合成和真实数据上实现最先进的性能，还可以促进下游任务。</li>
</ul>

<h3>Title: Revisiting Likelihood-Based Out-of-Distribution Detection by Modeling Representations</h3>
<ul>
<li><strong>Authors: </strong>Yifan Ding, Arturas Aleksandrauskas, Amirhossein Ahmadian, Jonas Unger, Fredrik Lindsten, Gabriel Eilertsen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07793">https://arxiv.org/abs/2504.07793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07793">https://arxiv.org/pdf/2504.07793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07793]] Revisiting Likelihood-Based Out-of-Distribution Detection by Modeling Representations(https://arxiv.org/abs/2504.07793)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Out-of-distribution (OOD) detection is critical for ensuring the reliability of deep learning systems, particularly in safety-critical applications. Likelihood-based deep generative models have historically faced criticism for their unsatisfactory performance in OOD detection, often assigning higher likelihood to OOD data than in-distribution samples when applied to image data. In this work, we demonstrate that likelihood is not inherently flawed. Rather, several properties in the images space prohibit likelihood as a valid detection score. Given a sufficiently good likelihood estimator, specifically using the probability flow formulation of a diffusion model, we show that likelihood-based methods can still perform on par with state-of-the-art methods when applied in the representation space of pre-trained encoders. The code of our work can be found at $\href{this https URL}{\texttt{this https URL}}$.</li>
<li><strong>摘要：</strong>分布（OOD）检测对于确保深度学习系统的可靠性，尤其是在安全至关重要的应用中至关重要。基于可能性的深层生成模型在历史上因其在OOD检测方面的性能不令人满意而受到批评，通常将比分布样本分配给图像数据时，通常为OOD数据分配了更高的可能性。在这项工作中，我们证明了可能性并非固有的缺陷。相反，图像空间中的几个属性禁止可能作为有效检测得分。考虑到足够好的似然估计器，特别是使用扩散模型的概率流式公式，我们表明，在预训练编码器的表示空间中应用，基于可能性的方法仍然可以与最新方法相同。我们的工作代码可以在$ \ href {此https url} {\ texttt {this https url}} $中找到。</li>
</ul>

<h3>Title: DG-STMTL: A Novel Graph Convolutional Network for Multi-Task Spatio-Temporal Traffic Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Wanna Cui, Peizheng Wang, Faliang Yin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07822">https://arxiv.org/abs/2504.07822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07822">https://arxiv.org/pdf/2504.07822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07822]] DG-STMTL: A Novel Graph Convolutional Network for Multi-Task Spatio-Temporal Traffic Forecasting(https://arxiv.org/abs/2504.07822)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Spatio-temporal traffic prediction is crucial in intelligent transportation systems. The key challenge of accurate prediction is how to model the complex spatio-temporal dependencies and adapt to the inherent dynamics in data. Traditional Graph Convolutional Networks (GCNs) often struggle with static adjacency matrices that introduce domain bias or learnable matrices that may be overfitting to specific patterns. This challenge becomes more complex when considering Multi-Task Learning (MTL). While MTL has the potential to enhance prediction accuracy through task synergies, it can also face significant hurdles due to task interference. To overcome these challenges, this study introduces a novel MTL framework, Dynamic Group-wise Spatio-Temporal Multi-Task Learning (DG-STMTL). DG-STMTL proposes a hybrid adjacency matrix generation module that combines static matrices with dynamic ones through a task-specific gating mechanism. We also introduce a group-wise GCN module to enhance the modelling capability of spatio-temporal dependencies. We conduct extensive experiments on two real-world datasets to evaluate our method. Results show that our method outperforms other state-of-the-arts, indicating its effectiveness and robustness.</li>
<li><strong>摘要：</strong>时空交通预测在智能运输系统中至关重要。准确预测的关键挑战是如何对复杂的时空依赖性建模并适应数据中的固有动力学。传统的图形卷积网络（GCN）经常在引入域偏见或可学习的矩阵的静态邻接矩阵中挣扎，这些矩阵可能过于适合特定模式。考虑多任务学习（MTL）时，这个挑战变得更加复杂。尽管MTL有可能通过任务协同增强预测准确性，但由于任务干扰，它也可能面临重大障碍。为了克服这些挑战，这项研究引入了一种新型的MTL框架，动态群体的时空多任务学习（DG-STMTL）。 DG-STMTL提出了一个混合邻接矩阵生成模块，该模块通过特定于任务的门控机制将静态矩阵与动态矩阵结合在一起。我们还引入了小组GCN模块，以增强时空依赖性的建模能力。我们对两个现实世界数据集进行了广泛的实验，以评估我们的方法。结果表明，我们的方法优于其他最先进的方法，表明其有效性和鲁棒性。</li>
</ul>

<h3>Title: Robust Hallucination Detection in LLMs via Adaptive Token Selection</h3>
<ul>
<li><strong>Authors: </strong>Mengjia Niu, Hamed Haddadi, Guansong Pang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07863">https://arxiv.org/abs/2504.07863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07863">https://arxiv.org/pdf/2504.07863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07863]] Robust Hallucination Detection in LLMs via Adaptive Token Selection(https://arxiv.org/abs/2504.07863)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Hallucinations in large language models (LLMs) pose significant safety concerns that impede their broader deployment. Recent research in hallucination detection has demonstrated that LLMs' internal representations contain truthfulness hints, which can be harnessed for detector training. However, the performance of these detectors is heavily dependent on the internal representations of predetermined tokens, fluctuating considerably when working on free-form generations with varying lengths and sparse distributions of hallucinated entities. To address this, we propose HaMI, a novel approach that enables robust detection of hallucinations through adaptive selection and learning of critical tokens that are most indicative of hallucinations. We achieve this robustness by an innovative formulation of the Hallucination detection task as Multiple Instance (HaMI) learning over token-level representations within a sequence, thereby facilitating a joint optimisation of token selection and hallucination detection on generation sequences of diverse forms. Comprehensive experimental results on four hallucination benchmarks show that HaMI significantly outperforms existing state-of-the-art approaches.</li>
<li><strong>摘要：</strong>大语言模型（LLMS）中的幻觉提出了阻碍其更广泛部署的重大安全问题。幻觉检测的最新研究表明，LLMS的内部表示包含真实的提示，可以利用这些暗示进行探测器训练。但是，这些探测器的性能在很大程度上取决于预定令牌的内部表示形式，当工作长度不同，幻觉实体的稀疏分布的自由形式时，会大大波动。为了解决这个问题，我们提出了一种新颖的方法，这是一种新颖的方法，可以通过自适应选择和学习最能表明幻觉的关键令牌来良好地检测幻觉。我们通过对序列内的多个实例（HAMI）学习对令牌级表示的多个实例（HAMI）学习的创新表述，从而实现了这种鲁棒性，从而促进了对代币选择和幻觉检测的联合优化。四个幻觉基准的全面实验结果表明，HAMI明显优于现有的最新方法。</li>
</ul>

<h3>Title: SAMJAM: Zero-Shot Video Scene Graph Generation for Egocentric Kitchen Videos</h3>
<ul>
<li><strong>Authors: </strong>Joshua Li, Fernando Jose Pena Cantu, Emily Yu, Alexander Wong, Yuchen Cui, Yuhao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07867">https://arxiv.org/abs/2504.07867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07867">https://arxiv.org/pdf/2504.07867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07867]] SAMJAM: Zero-Shot Video Scene Graph Generation for Egocentric Kitchen Videos(https://arxiv.org/abs/2504.07867)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Video Scene Graph Generation (VidSGG) is an important topic in understanding dynamic kitchen environments. Current models for VidSGG require extensive training to produce scene graphs. Recently, Vision Language Models (VLM) and Vision Foundation Models (VFM) have demonstrated impressive zero-shot capabilities in a variety of tasks. However, VLMs like Gemini struggle with the dynamics for VidSGG, failing to maintain stable object identities across frames. To overcome this limitation, we propose SAMJAM, a zero-shot pipeline that combines SAM2's temporal tracking with Gemini's semantic understanding. SAM2 also improves upon Gemini's object grounding by producing more accurate bounding boxes. In our method, we first prompt Gemini to generate a frame-level scene graph. Then, we employ a matching algorithm to map each object in the scene graph with a SAM2-generated or SAM2-propagated mask, producing a temporally-consistent scene graph in dynamic environments. Finally, we repeat this process again in each of the following frames. We empirically demonstrate that SAMJAM outperforms Gemini by 8.33% in mean recall on the EPIC-KITCHENS and EPIC-KITCHENS-100 datasets.</li>
<li><strong>摘要：</strong>视频场景图（Vidsgg）是理解动态厨房环境的重要主题。当前用于Vidsgg的模型需要大量的培训来制作场景图。最近，视觉语言模型（VLM）和视觉基础模型（VFM）在各种任务中都表现出令人印象深刻的零击功能。但是，像Gemini这样的VLM在Vidsgg的动力学方面挣扎，无法在框架上保持稳定的对象身份。为了克服这一局限性，我们提出了Samjam，这是一种零射管，将SAM2的时间跟踪与Gemini的语义理解相结合。 SAM2还通过产生更准确的边界框来改善双子座的对象接地。在我们的方法中，我们首先提示双子座生成帧级场景图。然后，我们使用匹配的算法在场景图中使用SAM2生成或SAM2传播的掩码映射每个对象，从而在动态环境中产生时间一致的场景图。最后，我们在以下每个帧中再次重复此过程。我们从经验上证明，在Epic-Kitchens和Epic-kitchens-100数据集中，Samjam在平均召回中的表现优于8.33％。</li>
</ul>

<h3>Title: DiverseFlow: Sample-Efficient Diverse Mode Coverage in Flows</h3>
<ul>
<li><strong>Authors: </strong>Mashrur M. Morshed, Vishnu Boddeti</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07894">https://arxiv.org/abs/2504.07894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07894">https://arxiv.org/pdf/2504.07894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07894]] DiverseFlow: Sample-Efficient Diverse Mode Coverage in Flows(https://arxiv.org/abs/2504.07894)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Many real-world applications of flow-based generative models desire a diverse set of samples that cover multiple modes of the target distribution. However, the predominant approach for obtaining diverse sets is not sample-efficient, as it involves independently obtaining many samples from the source distribution and mapping them through the flow until the desired mode coverage is achieved. As an alternative to repeated sampling, we introduce DiverseFlow: a training-free approach to improve the diversity of flow models. Our key idea is to employ a determinantal point process to induce a coupling between the samples that drives diversity under a fixed sampling budget. In essence, DiverseFlow allows exploration of more variations in a learned flow model with fewer samples. We demonstrate the efficacy of our method for tasks where sample-efficient diversity is desirable, such as text-guided image generation with polysemous words, inverse problems like large-hole inpainting, and class-conditional image synthesis.</li>
<li><strong>摘要：</strong>基于流量的生成模型的许多真实应用都希望一组涵盖目标分布的多种模式的样本。但是，获得多种集合的主要方法不是样品效率高效，因为它涉及独立从源分布中获取许多样品并通过流量绘制这些样品，直到达到所需的模式覆盖率为止。作为重复采样的替代方法，我们介绍了不同的流程：一种无训练的方法来改善流程模型的多样性。我们的关键思想是采用确定点过程来诱导样品之间的耦合，从而在固定采样预算下驱动多样性。从本质上讲，多样性允许探索较少样品的学习流模型中的更多变化。我们证明了我们方法对样品有效多样性的任务的疗效，例如带有多词的文本引导的图像生成，多义单词，逆问题，例如大孔介绍和班级条件图像合成。</li>
</ul>

<h3>Title: Beyond the Frame: Generating 360° Panoramic Videos from Perspective Videos</h3>
<ul>
<li><strong>Authors: </strong>Rundong Luo, Matthew Wallingford, Ali Farhadi, Noah Snavely, Wei-Chiu Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07940">https://arxiv.org/abs/2504.07940</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07940">https://arxiv.org/pdf/2504.07940</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07940]] Beyond the Frame: Generating 360° Panoramic Videos from Perspective Videos(https://arxiv.org/abs/2504.07940)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>360° videos have emerged as a promising medium to represent our dynamic visual world. Compared to the "tunnel vision" of standard cameras, their borderless field of view offers a more complete perspective of our surroundings. While existing video models excel at producing standard videos, their ability to generate full panoramic videos remains elusive. In this paper, we investigate the task of video-to-360° generation: given a perspective video as input, our goal is to generate a full panoramic video that is consistent with the original video. Unlike conventional video generation tasks, the output's field of view is significantly larger, and the model is required to have a deep understanding of both the spatial layout of the scene and the dynamics of objects to maintain spatio-temporal consistency. To address these challenges, we first leverage the abundant 360° videos available online and develop a high-quality data filtering pipeline to curate pairwise training data. We then carefully design a series of geometry- and motion-aware operations to facilitate the learning process and improve the quality of 360° video generation. Experimental results demonstrate that our model can generate realistic and coherent 360° videos from in-the-wild perspective video. In addition, we showcase its potential applications, including video stabilization, camera viewpoint control, and interactive visual question answering.</li>
<li><strong>摘要：</strong>360°视频已成为代表我们动态视觉世界的有希望的媒介。与标准摄像机的“隧道视觉”相比，它们的无边界视野为我们的周围环境提供了更完整的视野。尽管现有的视频模型在制作标准视频方面表现出色，但它们生成完整全景视频的能力仍然难以捉摸。在本文中，我们研究了视频到360°的任务：以视频为输入，我们的目标是生成与原始视频一致的完整全景视频。与传统的视频生成任务不同，输出的视野明显更大，并且该模型必须对场景的空间布局和对象的动力学有深入的了解，以维持时空的一致性。为了应对这些挑战，我们首先利用可在线提供的丰富360°视频，并开发高质量的数据过滤管道来策划成对训练数据。然后，我们仔细设计了一系列的几何学和运动感知操作，以促进学习过程并提高360°视频生成的质量。实验结果表明，我们的模型可以从野外视频中生成现实且连贯的360°视频。此外，我们还展示了其潜在的应用程序，包括视频稳定，摄像机观点控制和交互式视觉问题回答。</li>
</ul>

<h3>Title: HoloPart: Generative 3D Part Amodal Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yunhan Yang, Yuan-Chen Guo, Yukun Huang, Zi-Xin Zou, Zhipeng Yu, Yangguang Li, Yan-Pei Cao, Xihui Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07943">https://arxiv.org/abs/2504.07943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07943">https://arxiv.org/pdf/2504.07943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07943]] HoloPart: Generative 3D Part Amodal Segmentation(https://arxiv.org/abs/2504.07943)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>3D part amodal segmentation--decomposing a 3D shape into complete, semantically meaningful parts, even when occluded--is a challenging but crucial task for 3D content creation and understanding. Existing 3D part segmentation methods only identify visible surface patches, limiting their utility. Inspired by 2D amodal segmentation, we introduce this novel task to the 3D domain and propose a practical, two-stage approach, addressing the key challenges of inferring occluded 3D geometry, maintaining global shape consistency, and handling diverse shapes with limited training data. First, we leverage existing 3D part segmentation to obtain initial, incomplete part segments. Second, we introduce HoloPart, a novel diffusion-based model, to complete these segments into full 3D parts. HoloPart utilizes a specialized architecture with local attention to capture fine-grained part geometry and global shape context attention to ensure overall shape consistency. We introduce new benchmarks based on the ABO and PartObjaverse-Tiny datasets and demonstrate that HoloPart significantly outperforms state-of-the-art shape completion methods. By incorporating HoloPart with existing segmentation techniques, we achieve promising results on 3D part amodal segmentation, opening new avenues for applications in geometry editing, animation, and material assignment.</li>
<li><strong>摘要：</strong>3D部分Amodal分割 - 将3D形状分解为完整的，语义上有意义的部分，即使被遮挡 - 是3D内容创建和理解的具有挑战性但至关重要的任务。现有的3D零件分割方法仅识别可见的表面斑块，从而限制了它们的效用。受2D Amodal细分的启发，我们将这项新型任务介绍给了3D域，并提出了一种实用的，两阶段的方法，解决了推断闭塞3D几何形状的关键挑战，保持了全球形状的一致性，并通过有限的训练数据来处理多样的形状。首先，我们利用现有的3D零件细分来获得初始的，不完整的零件段。其次，我们介绍了一种新型基于扩散的模型Holopart，以将这些片段完成为完整的3D部分。 Holopart利用具有本地关注的专业体系结构来捕获细粒的零件几何形状和全球形状上下文的关注，以确保整体形状的一致性。我们基于ABO和PartoBjaverse微型数据集介绍了新的基准测试，并证明Holopart的表现明显超过了最先进的形状完成方法。通过将Holopart与现有的分割技术合并，我们可以在3D部分Amodal分割，为几何编辑，动画和材料分配的应用开辟新的途径。</li>
</ul>

<h3>Title: GenEAva: Generating Cartoon Avatars with Fine-Grained Facial Expressions from Realistic Diffusion-based Faces</h3>
<ul>
<li><strong>Authors: </strong>Hao Yu, Rupayan Mallick, Margrit Betke, Sarah Adel Bargal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07945">https://arxiv.org/abs/2504.07945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07945">https://arxiv.org/pdf/2504.07945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07945]] GenEAva: Generating Cartoon Avatars with Fine-Grained Facial Expressions from Realistic Diffusion-based Faces(https://arxiv.org/abs/2504.07945)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Cartoon avatars have been widely used in various applications, including social media, online tutoring, and gaming. However, existing cartoon avatar datasets and generation methods struggle to present highly expressive avatars with fine-grained facial expressions and are often inspired from real-world identities, raising privacy concerns. To address these challenges, we propose a novel framework, GenEAva, for generating high-quality cartoon avatars with fine-grained facial expressions. Our approach fine-tunes a state-of-the-art text-to-image diffusion model to synthesize highly detailed and expressive facial expressions. We then incorporate a stylization model that transforms these realistic faces into cartoon avatars while preserving both identity and expression. Leveraging this framework, we introduce the first expressive cartoon avatar dataset, GenEAva 1.0, specifically designed to capture 135 fine-grained facial expressions, featuring 13,230 expressive cartoon avatars with a balanced distribution across genders, racial groups, and age ranges. We demonstrate that our fine-tuned model generates more expressive faces than the state-of-the-art text-to-image diffusion model SDXL. We also verify that the cartoon avatars generated by our framework do not include memorized identities from fine-tuning data. The proposed framework and dataset provide a diverse and expressive benchmark for future research in cartoon avatar generation.</li>
<li><strong>摘要：</strong>卡通化身已被广泛用于各种应用程序，包括社交媒体，在线辅导和游戏。但是，现有的卡通头像数据集和一代方法难以在面部表情上呈现高度表现力的化身，并且通常受到现实身份的启发，从而引发了隐私问题。为了应对这些挑战，我们提出了一个新颖的框架Geneava，用于产生具有精细颗粒面部表情的高质量卡通化身。我们的方法微调是一种最先进的文本对图像扩散模型，以综合高度详细和表现力的面部表情。然后，我们合并了一种风格化模型，该模型将这些逼真的面孔转化为卡通头像，同时保留身份和表达。利用该框架，我们介绍了第一个富有表现力的卡通阿凡达数据集，Geneava 1.0，专门设计用于捕获135个细颗粒的面部表情，其中包含13,230个富有表现力的卡通化像，具有跨性别，种族群体和年龄范围的平衡分布。我们证明，与最先进的文本对图像扩散模型SDXL相比，微调模型产生的表现力更大。我们还验证了我们的框架生成的卡通化身不包括微调数据中的记忆身份。拟议的框架和数据集为卡通阿凡达（Avatar）的未来研究提供了多样化和表现力的基准。</li>
</ul>

<h3>Title: VisualCloze: A Universal Image Generation Framework via Visual In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhong-Yu Li, Ruoyi Du, Juncheng Yan, Le Zhuo, Zhen Li, Peng Gao, Zhanyu Ma, Ming-Ming Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07960">https://arxiv.org/abs/2504.07960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07960">https://arxiv.org/pdf/2504.07960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07960]] VisualCloze: A Universal Image Generation Framework via Visual In-Context Learning(https://arxiv.org/abs/2504.07960)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent progress in diffusion models significantly advances various image generation tasks. However, the current mainstream approach remains focused on building task-specific models, which have limited efficiency when supporting a wide range of different needs. While universal models attempt to address this limitation, they face critical challenges, including generalizable task instruction, appropriate task distributions, and unified architectural design. To tackle these challenges, we propose VisualCloze, a universal image generation framework, which supports a wide range of in-domain tasks, generalization to unseen ones, unseen unification of multiple tasks, and reverse generation. Unlike existing methods that rely on language-based task instruction, leading to task ambiguity and weak generalization, we integrate visual in-context learning, allowing models to identify tasks from visual demonstrations. Meanwhile, the inherent sparsity of visual task distributions hampers the learning of transferable knowledge across tasks. To this end, we introduce Graph200K, a graph-structured dataset that establishes various interrelated tasks, enhancing task density and transferable knowledge. Furthermore, we uncover that our unified image generation formulation shared a consistent objective with image infilling, enabling us to leverage the strong generative priors of pre-trained infilling models without modifying the architectures.</li>
<li><strong>摘要：</strong>扩散模型的最新进展大大提高了各种图像生成任务。但是，当前的主流方法仍然专注于构建特定于任务的模型，这些模型在支持广泛的不同需求时的效率有限。尽管通用模型试图解决这一限制，但他们面临着关键的挑战，包括可推广的任务​​指导，适当的任务分布和统一的建筑设计。为了应对这些挑战，我们提出了一个通用图像生成框架，它支持了广泛的内域任务，对看不见的任务的概括，看不见的多个任务统一以及反向产生。与依赖基于语言的任务指令的现有方法，导致任务歧义和弱化，我们整合了视觉上的内在学习学习，从而允许模型从视觉演示中识别任务。同时，视觉任务分布的固有稀疏性阻碍了跨任务转移知识的学习。为此，我们介绍了Graph200K，这是一个图形结构化数据集，该数据集建立了各种相互关联的任务，增强了任务密度和可转移的知识。此外，我们发现，我们的统一图像生成公式与图像填充有一个一致的目标，使我们能够在不修改架构的情况下利用预训练的填充模型的强生生成率。</li>
</ul>

<h3>Title: PixelFlow: Pixel-Space Generative Models with Flow</h3>
<ul>
<li><strong>Authors: </strong>Shoufa Chen, Chongjian Ge, Shilong Zhang, Peize Sun, Ping Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07963">https://arxiv.org/abs/2504.07963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07963">https://arxiv.org/pdf/2504.07963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07963]] PixelFlow: Pixel-Space Generative Models with Flow(https://arxiv.org/abs/2504.07963)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We present PixelFlow, a family of image generation models that operate directly in the raw pixel space, in contrast to the predominant latent-space models. This approach simplifies the image generation process by eliminating the need for a pre-trained Variational Autoencoder (VAE) and enabling the whole model end-to-end trainable. Through efficient cascade flow modeling, PixelFlow achieves affordable computation cost in pixel space. It achieves an FID of 1.98 on 256$\times$256 ImageNet class-conditional image generation benchmark. The qualitative text-to-image results demonstrate that PixelFlow excels in image quality, artistry, and semantic control. We hope this new paradigm will inspire and open up new opportunities for next-generation visual generation models. Code and models are available at this https URL.</li>
<li><strong>摘要：</strong>我们提出了Pixelflow，这是一个直接在原始像素空间中运行的图像生成模型家族，与主要的潜在空间模型相反。这种方法通过消除了对预训练的变分自动编码器（VAE）的需求，从而简化了图像生成过程，并启用了整个模型端到端训练。通过有效的级联流量建模，Pixelflow实现了像素空间中负担得起的计算成本。它在256 $ \ times $ 256 ImageNet类条件形象生成基准中实现了1.98的FID。定性文本对图像的结果表明，Pixelflow在图像质量，艺术和语义控制方面表现出色。我们希望这种新的范式能够激发下一代视觉生成模型的新机会。代码和型号可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Cat, Rat, Meow: On the Alignment of Language Model and Human Term-Similarity Judgments</h3>
<ul>
<li><strong>Authors: </strong>Lorenz Linhardt, Tom Neuhäuser, Lenka Tětková, Oliver Eberle</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07965">https://arxiv.org/abs/2504.07965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07965">https://arxiv.org/pdf/2504.07965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07965]] Cat, Rat, Meow: On the Alignment of Language Model and Human Term-Similarity Judgments(https://arxiv.org/abs/2504.07965)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Small and mid-sized generative language models have gained increasing attention. Their size and availability make them amenable to being analyzed at a behavioral as well as a representational level, allowing investigations of how these levels interact. We evaluate 32 publicly available language models for their representational and behavioral alignment with human similarity judgments on a word triplet task. This provides a novel evaluation setting to probe semantic associations in language beyond common pairwise comparisons. We find that (1) even the representations of small language models can achieve human-level alignment, (2) instruction-tuned model variants can exhibit substantially increased agreement, (3) the pattern of alignment across layers is highly model dependent, and (4) alignment based on models' behavioral responses is highly dependent on model size, matching their representational alignment only for the largest evaluated models.</li>
<li><strong>摘要：</strong>中小型生成语言模型引起了人们越来越多的关注。它们的规模和可用性使它们可以在行为和代表性层面上进行分析，从而可以研究这些水平的相互作用。我们评估了32种公开可用的语言模型，以与单词三重态任务上的人类相似性判断，以代表性和行为对齐。这提供了一个新颖的评估设置，以探测除了普通成对比较以外的语言中的语义关联。我们发现（1）即使小语言模型的表示也可以实现人类水平的对准，（2）指导调整的模型变体可以显示大幅度提高一致性，（3）跨层的对准模式高度依赖于模型，并且基于模型的行为响应的对齐方式高度依赖于模型大小，仅与模型相匹配，对模型进行了匹配。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
