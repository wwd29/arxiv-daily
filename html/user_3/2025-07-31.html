<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-07-31</h1>
<h3>Title: CIMR: Contextualized Iterative Multimodal Reasoning for Robust Instruction Following in LVLMs</h3>
<ul>
<li><strong>Authors: </strong>Yangshu Yuan, Heng Chen, Xinyi Jiang, Christian Ng, Kexin Qiu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22074">https://arxiv.org/abs/2507.22074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22074">https://arxiv.org/pdf/2507.22074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22074]] CIMR: Contextualized Iterative Multimodal Reasoning for Robust Instruction Following in LVLMs(https://arxiv.org/abs/2507.22074)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The rapid advancement of Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) has enhanced our ability to process and generate human language and visual information. However, these models often struggle with complex, multi-step multi-modal instructions that require logical reasoning, dynamic feedback integration, and iterative self-correction. To address this, we propose CIMR: Contextualized Iterative Multimodal Reasoning, a novel framework that introduces a context-aware iterative reasoning and self-correction module. CIMR operates in two stages: initial reasoning and response generation, followed by iterative refinement using parsed multi-modal feedback. A dynamic fusion module deeply integrates textual, visual, and contextual features at each step. We fine-tune LLaVA-1.5-7B on the Visual Instruction Tuning (VIT) dataset and evaluate CIMR on the newly introduced Multi-modal Action Planning (MAP) dataset. CIMR achieves 91.5% accuracy, outperforming state-of-the-art models such as GPT-4V (89.2%), LLaVA-1.5 (78.5%), MiniGPT-4 (75.3%), and InstructBLIP (72.8%), demonstrating the efficacy of its iterative reasoning and self-correction capabilities in complex tasks.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）和大型视觉语言模型（LVLM）的快速发展增强了我们处理和生成人类语言和视觉信息的能力。但是，这些模型通常会在需要逻辑推理，动态反馈集成和迭代自我纠正的复杂，多模式指令方面挣扎。为了解决这个问题，我们提出了CIMR：上下文化的迭代多模式推理，这是一个新颖的框架，引入了上下文意识到的迭代推理和自我纠正模块。 CIMR分为两个阶段：初始推理和响应产生，然后使用解析的多模式反馈进行迭代改进。动态融合模块在每个步骤都深入整合文本，视觉和上下文特征。我们在视觉说明调整（VIT）数据集中微调LLAVA-1.5-7B，并在新引入的多模式动作计划（MAP）数据集中评估CIMR。 CIMR achieves 91.5% accuracy, outperforming state-of-the-art models such as GPT-4V (89.2%), LLaVA-1.5 (78.5%), MiniGPT-4 (75.3%), and InstructBLIP (72.8%), demonstrating the efficacy of its iterative reasoning and self-correction capabilities in complex tasks.</li>
</ul>

<h3>Title: Test-time Prompt Refinement for Text-to-Image Models</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Abdul Hafeez Khan, Yash Jain, Siddhartha Bhattacharyya, Vibhav Vineet</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22076">https://arxiv.org/abs/2507.22076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22076">https://arxiv.org/pdf/2507.22076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22076]] Test-time Prompt Refinement for Text-to-Image Models(https://arxiv.org/abs/2507.22076)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) generation models have made significant strides but still struggle with prompt sensitivity: even minor changes in prompt wording can yield inconsistent or inaccurate outputs. To address this challenge, we introduce a closed-loop, test-time prompt refinement framework that requires no additional training of the underlying T2I model, termed TIR. In our approach, each generation step is followed by a refinement step, where a pretrained multimodal large language model (MLLM) analyzes the output image and the user's prompt. The MLLM detects misalignments (e.g., missing objects, incorrect attributes) and produces a refined and physically grounded prompt for the next round of image generation. By iteratively refining the prompt and verifying alignment between the prompt and the image, TIR corrects errors, mirroring the iterative refinement process of human artists. We demonstrate that this closed-loop strategy improves alignment and visual coherence across multiple benchmark datasets, all while maintaining plug-and-play integration with black-box T2I models.</li>
<li><strong>摘要：</strong>文本对图像（T2I）的生成模型取得了长足的进步，但仍会迅速敏感：即使迅速措辞的微小变化也会产生不一致或不准确的产出。为了应对这一挑战，我们引入了一个闭环，测试时间提示改进框架，该框架不需要对基础T2I模型进行额外的培训，该模型称为TIR。在我们的方法中，每个一代步骤之后是一个完善步骤，其中预处理的多模式模型（MLLM）分析了输出图像和用户的提示。 MLLM检测到未对准（例如缺失对象，不正确的属性），并为下一轮的图像生成产生精致且物理上的提示。通过迭代完善提示并验证提示和图像之间的对齐方式，TIR纠正了错误，反映了人类艺术家的迭代精炼过程。我们证明，这种闭环策略可以改善多个基准数据集的对齐和视觉连贯性，同时与黑盒T2I模型保持插件的集成。</li>
</ul>

<h3>Title: Shape Invariant 3D-Variational Autoencoder: Super Resolution in Turbulence flow</h3>
<ul>
<li><strong>Authors: </strong>Anuraj Maurya</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.flu-dyn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22082">https://arxiv.org/abs/2507.22082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22082">https://arxiv.org/pdf/2507.22082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22082]] Shape Invariant 3D-Variational Autoencoder: Super Resolution in Turbulence flow(https://arxiv.org/abs/2507.22082)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generative</a></li>
<li><strong>Abstract: </strong>Deep learning provides a versatile suite of methods for extracting structured information from complex datasets, enabling deeper understanding of underlying fluid dynamic phenomena. The field of turbulence modeling, in particular, benefits from the growing availability of high-dimensional data obtained through experiments, field observations, and large-scale simulations spanning multiple spatio-temporal scales. This report presents a concise overview of both classical and deep learningbased approaches to turbulence modeling. It further investigates two specific challenges at the intersection of fluid dynamics and machine learning: the integration of multiscale turbulence models with deep learning architectures, and the application of deep generative models for super-resolution reconstruction</li>
<li><strong>摘要：</strong>深度学习提供了一套多功能的方法，用于从复杂数据集中提取结构化信息，从而更深入地了解潜在的流体动态现象。特别是，通过实验，现场观测和跨越多个时空尺度的大规模模拟获得的高维数据的可用性，湍流建模的领域受益。该报告介绍了湍流建模的经典和深度学习方法的简洁概述。它进一步研究了流体动态与机器学习的交集的两个具体挑战：多尺度湍流模型与深度学习体系结构的整合，以及将深层生成模型应用于超分辨率重建</li>
</ul>

<h3>Title: Trade-offs in Image Generation: How Do Different Dimensions Interact?</h3>
<ul>
<li><strong>Authors: </strong>Sicheng Zhang, Binzhu Xie, Zhonghao Yan, Yuli Zhang, Donghao Zhou, Xiaofei Chen, Shi Qiu, Jiaqi Liu, Guoyang Xie, Zhichao Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22100">https://arxiv.org/abs/2507.22100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22100">https://arxiv.org/pdf/2507.22100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22100]] Trade-offs in Image Generation: How Do Different Dimensions Interact?(https://arxiv.org/abs/2507.22100)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Model performance in text-to-image (T2I) and image-to-image (I2I) generation often depends on multiple aspects, including quality, alignment, diversity, and robustness. However, models' complex trade-offs among these dimensions have rarely been explored due to (1) the lack of datasets that allow fine-grained quantification of these trade-offs, and (2) the use of a single metric for multiple dimensions. To bridge this gap, we introduce TRIG-Bench (Trade-offs in Image Generation), which spans 10 dimensions (Realism, Originality, Aesthetics, Content, Relation, Style, Knowledge, Ambiguity, Toxicity, and Bias), contains 40,200 samples, and covers 132 pairwise dimensional subsets. Furthermore, we develop TRIGScore, a VLM-as-judge metric that automatically adapts to various dimensions. Based on TRIG-Bench and TRIGScore, we evaluate 14 models across T2I and I2I tasks. In addition, we propose the Relation Recognition System to generate the Dimension Trade-off Map (DTM) that visualizes the trade-offs among model-specific capabilities. Our experiments demonstrate that DTM consistently provides a comprehensive understanding of the trade-offs between dimensions for each type of generative model. Notably, we show that the model's dimension-specific weaknesses can be mitigated through fine-tuning on DTM to enhance overall performance. Code is available at: this https URL</li>
<li><strong>摘要：</strong>文本对图像（T2I）和图像对图像（I2i）的生成中的模型性能通常取决于多个方面，包括质量，一致性，多样性和鲁棒性。但是，由于（1）缺乏允许对这些权衡的细粒度量化的数据集，并且（2）使用单个度量标准来用于多个维度，因此很少探索模型中模型的复杂权衡。为了弥合这一差距，我们介绍了Trig Bench（图像生成中的权衡），涵盖了10个维度（现实主义，独创性，美学，内容，关系，样式，知识，歧义，毒性和偏见），包含40,200个样品，并覆盖132个二重式下属。此外，我们开发了TrigScore，这是一种自动适应各种维度的VLM-AS法官指标。基于Trig Bench和Trigscore，我们评估了T2I和I2I任务的14个模型。此外，我们提出了关系识别系统，以生成维数权衡地图（DTM），以可视化模型特定功能之间的权衡。我们的实验表明，DTM始终对每种类型的生成模型之间的权衡方面提供了全面的理解。值得注意的是，我们表明该模型的特定弱点可以通过在DTM上进行微调来减轻以增强整体性能。代码可用：此HTTPS URL</li>
</ul>

<h3>Title: Weighted Conditional Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Sergio Calvo-Ordonez, Matthieu Meunier, Alvaro Cartea, Christoph Reisinger, Yarin Gal, Jose Miguel Hernandez-Lobato</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22270">https://arxiv.org/abs/2507.22270</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22270">https://arxiv.org/pdf/2507.22270</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22270]] Weighted Conditional Flow Matching(https://arxiv.org/abs/2507.22270)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Conditional flow matching (CFM) has emerged as a powerful framework for training continuous normalizing flows due to its computational efficiency and effectiveness. However, standard CFM often produces paths that deviate significantly from straight-line interpolations between prior and target distributions, making generation slower and less accurate due to the need for fine discretization at inference. Recent methods enhance CFM performance by inducing shorter and straighter trajectories but typically rely on computationally expensive mini-batch optimal transport (OT). Drawing insights from entropic optimal transport (EOT), we propose Weighted Conditional Flow Matching (W-CFM), a novel approach that modifies the classical CFM loss by weighting each training pair $(x, y)$ with a Gibbs kernel. We show that this weighting recovers the entropic OT coupling up to some bias in the marginals, and we provide the conditions under which the marginals remain nearly unchanged. Moreover, we establish an equivalence between W-CFM and the minibatch OT method in the large-batch limit, showing how our method overcomes computational and performance bottlenecks linked to batch size. Empirically, we test our method on unconditional generation on various synthetic and real datasets, confirming that W-CFM achieves comparable or superior sample quality, fidelity, and diversity to other alternative baselines while maintaining the computational efficiency of vanilla CFM.</li>
<li><strong>摘要：</strong>由于其计算效率和有效性，有条件的流量匹配（CFM）已成为训练连续归一化流的有力框架。但是，标准CFM通常会产生与先验分布和目标分布之间直线插值显着偏离的路径，从而使产生较慢，并且由于推断时需要进行良好的离散化而变得越来越准确。最近的方法通过诱导较短和更直的轨迹来增强CFM性能，但通常依赖于计算昂贵的迷你批量最佳运输（OT）。我们提出了加权条件流匹配（W-CFM）的熵最佳运输（EOT）的绘制见解，这是一种新颖的方法，通过将每个训练对$（x，y）$与吉布斯内核来修改经典的CFM损失。我们表明，这种权重恢复了边际偏见的熵ot，我们提供了边缘几乎保持不变的条件。此外，我们在大批量限制中建立了W-CFM和Minibatch OT方法之间的等效性，这表明我们的方法如何克服与批处理大小相关的计算和性能瓶颈。从经验上讲，我们测试了各种合成和真实数据集的无条件生成的方法，证实W-CFM可以在维持香草CFM的计算效率的同时，可以实现与其他替代基线的可比较或卓越的样本质量，保真度和多样性。</li>
</ul>

<h3>Title: HOG-CNN: Integrating Histogram of Oriented Gradients with Convolutional Neural Networks for Retinal Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Faisal Ahmed</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22274">https://arxiv.org/abs/2507.22274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22274">https://arxiv.org/pdf/2507.22274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22274]] HOG-CNN: Integrating Histogram of Oriented Gradients with Convolutional Neural Networks for Retinal Image Classification(https://arxiv.org/abs/2507.22274)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The analysis of fundus images is critical for the early detection and diagnosis of retinal diseases such as Diabetic Retinopathy (DR), Glaucoma, and Age-related Macular Degeneration (AMD). Traditional diagnostic workflows, however, often depend on manual interpretation and are both time- and resource-intensive. To address these limitations, we propose an automated and interpretable clinical decision support framework based on a hybrid feature extraction model called HOG-CNN. Our key contribution lies in the integration of handcrafted Histogram of Oriented Gradients (HOG) features with deep convolutional neural network (CNN) representations. This fusion enables our model to capture both local texture patterns and high-level semantic features from retinal fundus images. We evaluated our model on three public benchmark datasets: APTOS 2019 (for binary and multiclass DR classification), ORIGA (for Glaucoma detection), and IC-AMD (for AMD diagnosis); HOG-CNN demonstrates consistently high performance. It achieves 98.5\% accuracy and 99.2 AUC for binary DR classification, and 94.2 AUC for five-class DR classification. On the IC-AMD dataset, it attains 92.8\% accuracy, 94.8\% precision, and 94.5 AUC, outperforming several state-of-the-art models. For Glaucoma detection on ORIGA, our model achieves 83.9\% accuracy and 87.2 AUC, showing competitive performance despite dataset limitations. We show, through comprehensive appendix studies, the complementary strength of combining HOG and CNN features. The model's lightweight and interpretable design makes it particularly suitable for deployment in resource-constrained clinical environments. These results position HOG-CNN as a robust and scalable tool for automated retinal disease screening.</li>
<li><strong>摘要：</strong>基底图像的分析对于早期检测和诊断视网膜疾病（例如糖尿病性视网膜病（DR），青光眼和与年龄相关的黄斑变性（AMD））至关重要。但是，传统的诊断工作流程通常取决于手动解释，并且是时间和资源密集的。为了解决这些局限性，我们提出了一个基于混合特征提取模型的自动化且可解释的临床决策支持框架，称为Hog-CNN。我们的关键贡献在于与深度卷积神经网络（CNN）表示的定向梯度（HOG）特征的手工制作的直方图的整合。这种融合使我们的模型能够从视网膜眼镜图像中捕获本地纹理模式和高级语义特征。我们在三个公共基准数据集上评估了我们的模型：APTOS 2019（用于二进制和多类DR分类），Origa（用于青光眼检测）和IC-AMD（用于AMD诊断）； Hog-CNN表现出一贯的高性能。它实现了98.5 \％的精度和99.2 AUC，用于二进制DR分类，而94.2 AUC用于五级DR分类。在IC-AMD数据集上，它达到92.8 \％精度，94.8 \％精度和94.5 AUC，表现优于几个最新模型。对于Origa上的青光眼检测，我们的模型达到83.9 \％的准确性和87.2 AUC，尽管存在数据集限制，但表现出竞争性的性能。我们通过全面的附录研究展示了结合猪和CNN特征的互补强度。该模型的轻巧和可解释的设计使其特别适合在资源受限的临床环境中部署。这些结果将HOG-CNN定位为自动视网膜疾病筛查的强大而可扩展的工具。</li>
</ul>

<h3>Title: FaceGCD: Generalized Face Discovery via Dynamic Prefix Generation</h3>
<ul>
<li><strong>Authors: </strong>Yunseok Oh, Dong-Wan Choi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22353">https://arxiv.org/abs/2507.22353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22353">https://arxiv.org/pdf/2507.22353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22353]] FaceGCD: Generalized Face Discovery via Dynamic Prefix Generation(https://arxiv.org/abs/2507.22353)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recognizing and differentiating among both familiar and unfamiliar faces is a critical capability for face recognition systems and a key step toward artificial general intelligence (AGI). Motivated by this ability, this paper introduces generalized face discovery (GFD), a novel open-world face recognition task that unifies traditional face identification with generalized category discovery (GCD). GFD requires recognizing both labeled and unlabeled known identities (IDs) while simultaneously discovering new, previously unseen IDs. Unlike typical GCD settings, GFD poses unique challenges due to the high cardinality and fine-grained nature of face IDs, rendering existing GCD approaches ineffective. To tackle this problem, we propose FaceGCD, a method that dynamically constructs instance-specific feature extractors using lightweight, layer-wise prefixes. These prefixes are generated on the fly by a HyperNetwork, which adaptively outputs a set of prefix generators conditioned on each input image. This dynamic design enables FaceGCD to capture subtle identity-specific cues without relying on high-capacity static models. Extensive experiments demonstrate that FaceGCD significantly outperforms existing GCD methods and a strong face recognition baseline, ArcFace, achieving state-of-the-art results on the GFD task and advancing toward open-world face recognition.</li>
<li><strong>摘要：</strong>熟悉和陌生面孔的认识和区别是面部识别系统的关键能力，也是迈向人工通用智能（AGI）的关键一步。本文以这种能力的启发，引入了广义面部发现（GFD），这是一种新颖的开放世界识别任务，将传统的面部识别与广义类别发现（GCD）统一。 GFD需要识别标记和未标记的已知身份（ID），同时发现新的，以前看不见的ID。与典型的GCD设置不同，GFD由于面部ID的高基数和细粒度的性质而构成了独特的挑战，从而使现有的GCD方法无效。为了解决此问题，我们提出了faceGCD，该方法使用轻质的，层的前缀动态构建特定于实例的特征提取器。这些前缀是通过超网络生成的，该连接可以自适应地输出一组在每个输入图像上的前缀发电机。这种动态设计使FaceGCD能够捕获特定于特定身份的线索，而无需依赖大容量静态模型。广泛的实验表明，FaceGCD显着胜过现有的GCD方法和强大的面部识别基线，Arcface，在GFD任务上取得了最新的结果，并朝着开放世界的面部识别方向前进。</li>
</ul>

<h3>Title: GVD: Guiding Video Diffusion Model for Scalable Video Distillation</h3>
<ul>
<li><strong>Authors: </strong>Kunyang Li, Jeffrey A Chan Santiago, Sarinda Dhanesh Samarasinghe, Gaowen Liu, Mubarak Shah</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22360">https://arxiv.org/abs/2507.22360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22360">https://arxiv.org/pdf/2507.22360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22360]] GVD: Guiding Video Diffusion Model for Scalable Video Distillation(https://arxiv.org/abs/2507.22360)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>To address the larger computation and storage requirements associated with large video datasets, video dataset distillation aims to capture spatial and temporal information in a significantly smaller dataset, such that training on the distilled data has comparable performance to training on all of the data. We propose GVD: Guiding Video Diffusion, the first diffusion-based video distillation method. GVD jointly distills spatial and temporal features, ensuring high-fidelity video generation across diverse actions while capturing essential motion information. Our method's diverse yet representative distillations significantly outperform previous state-of-the-art approaches on the MiniUCF and HMDB51 datasets across 5, 10, and 20 Instances Per Class (IPC). Specifically, our method achieves 78.29 percent of the original dataset's performance using only 1.98 percent of the total number of frames in MiniUCF. Additionally, it reaches 73.83 percent of the performance with just 3.30 percent of the frames in HMDB51. Experimental results across benchmark video datasets demonstrate that GVD not only achieves state-of-the-art performance but can also generate higher resolution videos and higher IPC without significantly increasing computational cost.</li>
<li><strong>摘要：</strong>为了解决与大型视频数据集相关的较大的计算和存储要求，视频数据集蒸馏旨在捕获较小的数据集中的空间和时间信息，以便对蒸馏数据进行培训的培训与所有数据培训相当。我们提出了GVD：指导视频扩散，这是第一个基于扩散的视频蒸馏方法。 GVD共同提炼空间和时间特征，确保跨不同动作的高保真视频生成，同时捕获基本的运动信息。我们方法在MiniUCF和HMDB51数据集（IPC）上的MiniUCF和HMDB51数据集上的多样化且代表性的蒸馏量显着优于先前的先前最先进的方法（IPC）。具体而言，我们的方法仅使用MiniUCF中帧总数的1.98％实现了原始数据集的78.29％。此外，在HMDB51中仅3.30％的框架达到了73.83％的性能。基准视频数据集的实验结果表明，GVD不仅可以实现最新的性能，而且还可以生成更高的分辨率视频和更高的IPC，而不会显着增加计算成本。</li>
</ul>

<h3>Title: Moiré Zero: An Efficient and High-Performance Neural Architecture for Moiré Removal</h3>
<ul>
<li><strong>Authors: </strong>Seungryong Lee, Woojeong Baek, Younghyun Kim, Eunwoo Kim, Haru Moon, Donggon Yoo, Eunbyung Park</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22407">https://arxiv.org/abs/2507.22407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22407">https://arxiv.org/pdf/2507.22407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22407]] Moiré Zero: An Efficient and High-Performance Neural Architecture for Moiré Removal(https://arxiv.org/abs/2507.22407)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Moiré patterns, caused by frequency aliasing between fine repetitive structures and a camera sensor's sampling process, have been a significant obstacle in various real-world applications, such as consumer photography and industrial defect inspection. With the advancements in deep learning algorithms, numerous studies-predominantly based on convolutional neural networks-have suggested various solutions to address this issue. Despite these efforts, existing approaches still struggle to effectively eliminate artifacts due to the diverse scales, orientations, and color shifts of moiré patterns, primarily because the constrained receptive field of CNN-based architectures limits their ability to capture the complex characteristics of moiré patterns. In this paper, we propose MZNet, a U-shaped network designed to bring images closer to a 'Moire-Zero' state by effectively removing moiré patterns. It integrates three specialized components: Multi-Scale Dual Attention Block (MSDAB) for extracting and refining multi-scale features, Multi-Shape Large Kernel Convolution Block (MSLKB) for capturing diverse moiré structures, and Feature Fusion-Based Skip Connection for enhancing information flow. Together, these components enhance local texture restoration and large-scale artifact suppression. Experiments on benchmark datasets demonstrate that MZNet achieves state-of-the-art performance on high-resolution datasets and delivers competitive results on lower-resolution dataset, while maintaining a low computational cost, suggesting that it is an efficient and practical solution for real-world applications. Project page: this https URL</li>
<li><strong>摘要：</strong>Moiré模式是由精细的重复结构与摄像机传感器的采样过程之间的频率混乱引起的，在各种现实世界中，例如消费者摄影和工业缺陷检查，一直是一个重要的障碍。随着深度学习算法的进步，许多研究基于卷积神经网络，提出了解决此问题的各种解决方案。尽管做出了这些努力，但现有的方法仍然很难有效消除Moiré模式的不同尺度，方向和色转移而导致的人工制品，这主要是因为基于CNN的架构的受限制接收场限制了其捕获Moiré模式复杂特征的能力。在本文中，我们提出了MZNet，这是一个U形网络，旨在通过有效删除Moiré模式，使图像更接近“ Moire-Zero”状态。它集成了三个专业组件：用于提取和精炼多尺度功能的多尺度双关注块（MSDAB），多形大内核卷积块（MSLKB），用于捕获多样化的Moiré结构，以及用于增强信息流的基于融合的SKIP连接。这些成分共同增强了局部纹理的恢复和大规模的伪影抑制。基准数据集上的实验表明，MZNET在高分辨率数据集上实现最先进的性能，并在低分辨率数据集上提供竞争性结果，同时保持低计算成本，这表明它是现实世界应用程序的有效且实用的解决方案。项目页面：此HTTPS URL</li>
</ul>

<h3>Title: Aleatoric Uncertainty Medical Image Segmentation Estimation via Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Phi Van Nguyen, Ngoc Huynh Trinh, Duy Minh Lam Nguyen, Phu Loc Nguyen, Quoc Long Tran</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22418">https://arxiv.org/abs/2507.22418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22418">https://arxiv.org/pdf/2507.22418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22418]] Aleatoric Uncertainty Medical Image Segmentation Estimation via Flow Matching(https://arxiv.org/abs/2507.22418)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Quantifying aleatoric uncertainty in medical image segmentation is critical since it is a reflection of the natural variability observed among expert annotators. A conventional approach is to model the segmentation distribution using the generative model, but current methods limit the expression ability of generative models. While current diffusion-based approaches have demonstrated impressive performance in approximating the data distribution, their inherent stochastic sampling process and inability to model exact densities limit their effectiveness in accurately capturing uncertainty. In contrast, our proposed method leverages conditional flow matching, a simulation-free flow-based generative model that learns an exact density, to produce highly accurate segmentation results. By guiding the flow model on the input image and sampling multiple data points, our approach synthesizes segmentation samples whose pixel-wise variance reliably reflects the underlying data distribution. This sampling strategy captures uncertainties in regions with ambiguous boundaries, offering robust quantification that mirrors inter-annotator differences. Experimental results demonstrate that our method not only achieves competitive segmentation accuracy but also generates uncertainty maps that provide deeper insights into the reliability of the segmentation outcomes. The code for this paper is freely available at this https URL</li>
<li><strong>摘要：</strong>量化医学图像分割中的不确定性是至关重要的，因为它反映了专家注释者之间观察到的自然变异性。常规方法是使用生成模型对分割分布进行建模，但是当前方法限制了生成模型的表达能力。尽管当前基于扩散的方法在近似数据分布方面表现出了令人印象深刻的性能，但它们固有的随机抽样过程和无法建模精确的密度限制了它们在准确捕获不确定性方面的有效性。相比之下，我们提出的方法利用条件流匹配，这是一种基于无模拟的生成模型，该模型了解精确的密度，以产生高度准确的分割结果。通过引导输入图像上的流模型并对多个数据点进行采样，我们的方法合成了分割样品，其像素方差可靠地反映了基本数据分布。这种抽样策略捕获了具有模棱两可界限的地区的不确定性，提供了反映通道间差异的强大量化。实验结果表明，我们的方法不仅达到了竞争性分割精度，而且还产生了不确定性图，可以更深入地了解分割结果的可靠性。本文的代码可在此HTTPS URL上免费获得</li>
</ul>

<h3>Title: Spec-VLA: Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance</h3>
<ul>
<li><strong>Authors: </strong>Songsheng Wang, Rucheng Yu, Zhihang Yuan, Chao Yu, Feng Gao, Yu Wang, Derek F. Wong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22424">https://arxiv.org/abs/2507.22424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22424">https://arxiv.org/pdf/2507.22424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22424]] Spec-VLA: Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance(https://arxiv.org/abs/2507.22424)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision-Language-Action (VLA) models have made substantial progress by leveraging the robust capabilities of Visual Language Models (VLMs). However, VLMs' significant parameter size and autoregressive (AR) decoding nature impose considerable computational demands on VLA models. While Speculative Decoding (SD) has shown efficacy in accelerating Large Language Models (LLMs) by incorporating efficient drafting and parallel verification, allowing multiple tokens to be generated in one forward pass, its application to VLA models remains unexplored. This work introduces Spec-VLA, an SD framework designed to accelerate VLA models. Due to the difficulty of the action prediction task and the greedy decoding mechanism of the VLA models, the direct application of the advanced SD framework to the VLA prediction task yields a minor speed improvement. To boost the generation speed, we propose an effective mechanism to relax acceptance utilizing the relative distances represented by the action tokens of the VLA model. Empirical results across diverse test scenarios affirm the effectiveness of the Spec-VLA framework, and further analysis substantiates the impact of our proposed strategies, which enhance the acceptance length by 44%, achieving 1.42 times speedup compared with the OpenVLA baseline, without compromising the success rate. The success of the Spec-VLA framework highlights the potential for broader application of speculative execution in VLA prediction scenarios.</li>
<li><strong>摘要：</strong>视觉语言动作（VLA）模型通过利用视觉语言模型（VLMS）的强大功能来取得了重大进展。但是，VLMS的显着参数大小和自回旋（AR）解码性质对VLA模型施加了巨大的计算需求。虽然投机解码（SD）通过合并有效的制图和并行验证，在加速大语言模型（LLMS）方面表现出了功效，允许在一个正向通行证中生成多个代币，但其对VLA模型的应用仍未得到探索。这项工作介绍了Spec-VLA，这是一个旨在加速VLA模型的SD框架。由于动作预测任务的困难和VLA模型的贪婪解码机制，因此高级SD框架在VLA预测任务中的直接应用可略有提高。为了提高生成速度，我们提出了一种有效的机制，利用VLA模型的作用令牌表示的相对距离放松接受度。各种测试方案的经验结果肯定了Spec-VLA框架的有效性，并进一步分析证实了我们提出的策略的影响，这将接受长度提高了44％，与OpenVLA基线相比，达到了1.42倍的速度，而不会损害成功率。 Spec-VLA框架的成功突出了在VLA预测方案中更广泛应用投机执行的潜力。</li>
</ul>

<h3>Title: TopoLiDM: Topology-Aware LiDAR Diffusion Models for Interpretable and Realistic LiDAR Point Cloud Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiuming Liu, Zheng Huang, Mengmeng Liu, Tianchen Deng, Francesco Nex, Hao Cheng, Hesheng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22454">https://arxiv.org/abs/2507.22454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22454">https://arxiv.org/pdf/2507.22454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22454]] TopoLiDM: Topology-Aware LiDAR Diffusion Models for Interpretable and Realistic LiDAR Point Cloud Generation(https://arxiv.org/abs/2507.22454)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>LiDAR scene generation is critical for mitigating real-world LiDAR data collection costs and enhancing the robustness of downstream perception tasks in autonomous driving. However, existing methods commonly struggle to capture geometric realism and global topological consistency. Recent LiDAR Diffusion Models (LiDMs) predominantly embed LiDAR points into the latent space for improved generation efficiency, which limits their interpretable ability to model detailed geometric structures and preserve global topological consistency. To address these challenges, we propose TopoLiDM, a novel framework that integrates graph neural networks (GNNs) with diffusion models under topological regularization for high-fidelity LiDAR generation. Our approach first trains a topological-preserving VAE to extract latent graph representations by graph construction and multiple graph convolutional layers. Then we freeze the VAE and generate novel latent topological graphs through the latent diffusion models. We also introduce 0-dimensional persistent homology (PH) constraints, ensuring the generated LiDAR scenes adhere to real-world global topological structures. Extensive experiments on the KITTI-360 dataset demonstrate TopoLiDM's superiority over state-of-the-art methods, achieving improvements of 22.6% lower Frechet Range Image Distance (FRID) and 9.2% lower Minimum Matching Distance (MMD). Notably, our model also enables fast generation speed with an average inference time of 1.68 samples/s, showcasing its scalability for real-world applications. We will release the related codes at this https URL.</li>
<li><strong>摘要：</strong>激光雷达场景的产生对于减轻现实世界的LIDAR数据收集成本和增强自动驾驶中下游感知任务的鲁棒性至关重要。但是，现有方法通常难以捕获几何现实主义和全球拓扑一致性。最近的激光雷达扩散模型（lidms）主要将激光雷达点嵌入潜在空间中，以提高发电效率，这限制了其可解释的能力，以模拟详细的几何结构并保持全球拓扑结构。为了应对这些挑战，我们提出了Topolidm，这是一个新颖的框架，将图形神经网络（GNN）与拓扑正则化的扩散模型集成到高保真激光雷达的生成下。我们的方法首先训练拓扑维护VAE，以通过图形结构和多个图形卷积层提取潜在图表。然后，我们通过潜在扩散模型冻结VAE并生成新型的潜在拓扑图。我们还引入了0维持续的同源性（pH）约束，以确保生成的激光镜场景遵循现实世界的全球拓扑结构。在Kitti-360数据集上进行的广泛实验表明，高质体优于最先进的方法，可提高Frechet范围图像距离（FRID）22.6％，最小匹配距离（MMD）降低了9.2％。值得注意的是，我们的模型还可以以1.68个样本/s的平均推理时间来实现快速生成速度，从而展示了其对现实世界应用的可扩展性。我们将在此HTTPS URL上发布相关代码。</li>
</ul>

<h3>Title: Exploiting Diffusion Prior for Task-driven Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Jaeha Kim, Junghun Oh, Kyoung Mu Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22459">https://arxiv.org/abs/2507.22459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22459">https://arxiv.org/pdf/2507.22459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22459]] Exploiting Diffusion Prior for Task-driven Image Restoration(https://arxiv.org/abs/2507.22459)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generation</a></li>
<li><strong>Abstract: </strong>Task-driven image restoration (TDIR) has recently emerged to address performance drops in high-level vision tasks caused by low-quality (LQ) inputs. Previous TDIR methods struggle to handle practical scenarios in which images are degraded by multiple complex factors, leaving minimal clues for restoration. This motivates us to leverage the diffusion prior, one of the most powerful natural image priors. However, while the diffusion prior can help generate visually plausible results, using it to restore task-relevant details remains challenging, even when combined with recent TDIR methods. To address this, we propose EDTR, which effectively harnesses the power of diffusion prior to restore task-relevant details. Specifically, we propose directly leveraging useful clues from LQ images in the diffusion process by generating from pixel-error-based pre-restored LQ images with mild noise added. Moreover, we employ a small number of denoising steps to prevent the generation of redundant details that dilute crucial task-related information. We demonstrate that our method effectively utilizes diffusion prior for TDIR, significantly enhancing task performance and visual quality across diverse tasks with multiple complex degradations.</li>
<li><strong>摘要：</strong>最近出现了任务驱动的图像恢复（TDIR），以解决由低质量（LQ）输入引起的高级视觉任务的性能下降。以前的TDIR方法难以处理实际情况，其中图像被多个复杂因素降低，从而使恢复的线索最少。这促使我们利用先验的扩散，这是最强大的自然图像先验之一。但是，尽管扩散先验可以帮助产生视觉上合理的结果，但即使与最近的TDIR方法结合使用，使用它来恢复与任务相关的细节仍然具有挑战性。为了解决这个问题，我们提出了EDTR，该EDTR在恢复与任务相关的细节之前有效利用扩散的力量。具体而言，我们通过从基于Pixel-Error的预恢复的LQ图像产生并添加了轻度噪声来直接利用LQ图像中的有用线索。此外，我们采用了少量的降级步骤来防止生成稀释至关重要的任务相关信息的冗余细节。我们证明，我们的方法有效地利用了TDIR的扩散，从而显着提高了具有多个复杂降解的不同任务的任务性能和视觉质量。</li>
</ul>

<h3>Title: Robust Adverse Weather Removal via Spectral-based Spatial Grouping</h3>
<ul>
<li><strong>Authors: </strong>Yuhwan Jeong, Yunseo Yang, Youngjo Yoon, Kuk-Jin Yoon</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22498">https://arxiv.org/abs/2507.22498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22498">https://arxiv.org/pdf/2507.22498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22498]] Robust Adverse Weather Removal via Spectral-based Spatial Grouping(https://arxiv.org/abs/2507.22498)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Adverse weather conditions cause diverse and complex degradation patterns, driving the development of All-in-One (AiO) models. However, recent AiO solutions still struggle to capture diverse degradations, since global filtering methods like direct operations on the frequency domain fail to handle highly variable and localized distortions. To address these issue, we propose Spectral-based Spatial Grouping Transformer (SSGformer), a novel approach that leverages spectral decomposition and group-wise attention for multi-weather image restoration. SSGformer decomposes images into high-frequency edge features using conventional edge detection and low-frequency information via Singular Value Decomposition. We utilize multi-head linear attention to effectively model the relationship between these features. The fused features are integrated with the input to generate a grouping-mask that clusters regions based on the spatial similarity and image texture. To fully leverage this mask, we introduce a group-wise attention mechanism, enabling robust adverse weather removal and ensuring consistent performance across diverse weather conditions. We also propose a Spatial Grouping Transformer Block that uses both channel attention and spatial attention, effectively balancing feature-wise relationships and spatial dependencies. Extensive experiments show the superiority of our approach, validating its effectiveness in handling the varied and intricate adverse weather degradations.</li>
<li><strong>摘要：</strong>不利的天气条件会导致多种多样的降解模式，推动了多合一（AIO）模型的发展。但是，最近的AIO解决方案仍然难以捕获各种降解，因为诸如频域上的直接操作之类的全球过滤方法无法处理高度可变和局部变形。为了解决这些问题，我们提出了基于光谱的空间分组变压器（SSGFORMER），这是一种新型方法，利用光谱分解和群体的关注来恢复多天气图像。 SSGFormer使用传统的边缘检测和低频信息通过单数值分解将图像分解为高频边缘特征。我们利用多头线性关注来有效地建模这些特征之间的关系。融合功能与输入集成在一起，以生成基于空间相似性和图像纹理的区域的分组面罩。为了充分利用此面具，我们引入了一个小组的注意机制，从而实现了强大的不利天气，并确保在各种天气条件下保持持续的性能。我们还提出了一个空间分组变压器块，该块同时使用通道注意力和空间注意力，有效地平衡了特征关系和空间依赖性。广泛的实验显示了我们方法的优越性，验证了其在处理多样化和复杂的不良天气降解方面的有效性。</li>
</ul>

<h3>Title: LoReUn: Data Itself Implicitly Provides Cues to Improve Machine Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Xiang Li, Qianli Shen, Haonan Wang, Kenji Kawaguchi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22499">https://arxiv.org/abs/2507.22499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22499">https://arxiv.org/pdf/2507.22499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22499]] LoReUn: Data Itself Implicitly Provides Cues to Improve Machine Unlearning(https://arxiv.org/abs/2507.22499)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent generative models face significant risks of producing harmful content, which has underscored the importance of machine unlearning (MU) as a critical technique for eliminating the influence of undesired data. However, existing MU methods typically assign the same weight to all data to be forgotten, which makes it difficult to effectively forget certain data that is harder to unlearn than others. In this paper, we empirically demonstrate that the loss of data itself can implicitly reflect its varying difficulty. Building on this insight, we introduce Loss-based Reweighting Unlearning (LoReUn), a simple yet effective plug-and-play strategy that dynamically reweights data during the unlearning process with minimal additional computational overhead. Our approach significantly reduces the gap between existing MU methods and exact unlearning in both image classification and generation tasks, effectively enhancing the prevention of harmful content generation in text-to-image diffusion models.</li>
<li><strong>摘要：</strong>最近的生成模型面临生产有害含量的重大风险，这突显了机器未学习（MU）的重要性，作为消除不希望数据影响的关键技术。但是，现有的MU方法通常将相同的权重分配给所有要忘记的数据，这使得很难有效地忘记某些比其他数据更难学习的数据。在本文中，我们从经验上证明，数据本身的丢失可以隐式反映其不同的难度。在此洞察力的基础上，我们引入了基于损失的重新释放未学习（Loreun），这是一种简单而有效的插件策略，在未学习过程中动态重新加权数据，并以最小的其他计算开销。我们的方法大大减少了现有的MU方法与图像分类和发电任务中精确学习的差异，从而有效增强了在文本到图像扩散模型中预防有害内容的生成。</li>
</ul>

<h3>Title: DACA-Net: A Degradation-Aware Conditional Diffusion Network for Underwater Image Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Chang Huang, Jiahang Cao, Jun Ma, Kieren Yu, Cong Li, Huayong Yang, Kaishun Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22501">https://arxiv.org/abs/2507.22501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22501">https://arxiv.org/pdf/2507.22501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22501]] DACA-Net: A Degradation-Aware Conditional Diffusion Network for Underwater Image Enhancement(https://arxiv.org/abs/2507.22501)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Underwater images typically suffer from severe colour distortions, low visibility, and reduced structural clarity due to complex optical effects such as scattering and absorption, which greatly degrade their visual quality and limit the performance of downstream visual perception tasks. Existing enhancement methods often struggle to adaptively handle diverse degradation conditions and fail to leverage underwater-specific physical priors effectively. In this paper, we propose a degradation-aware conditional diffusion model to enhance underwater images adaptively and robustly. Given a degraded underwater image as input, we first predict its degradation level using a lightweight dual-stream convolutional network, generating a continuous degradation score as semantic guidance. Based on this score, we introduce a novel conditional diffusion-based restoration network with a Swin UNet backbone, enabling adaptive noise scheduling and hierarchical feature refinement. To incorporate underwater-specific physical priors, we further propose a degradation-guided adaptive feature fusion module and a hybrid loss function that combines perceptual consistency, histogram matching, and feature-level contrast. Comprehensive experiments on benchmark datasets demonstrate that our method effectively restores underwater images with superior colour fidelity, perceptual quality, and structural details. Compared with SOTA approaches, our framework achieves significant improvements in both quantitative metrics and qualitative visual assessments.</li>
<li><strong>摘要：</strong>水下图像通常会因复杂的光学效果（例如散射和吸收）而引起的严重颜色扭曲，低知识性以及降低的结构清晰度，从而极大地降低了它们的视觉质量并限制了下游视觉感知任务的性能。现有的增强方法通常很难适应地处理多种降解条件，并且无法有效利用特定于水下的身体先验。在本文中，我们提出了一个降解感知的条件扩散模型，以适应性和稳健地增强水下图像。鉴于水下图像作为输入，我们首先使用轻型双流卷积网络预测其降解水平，从而产生连续的退化评分作为语义指导。基于此分数，我们引入了一个具有SWIN UNET主链的新型基于条件扩散的恢复网络，从而实现了自适应噪声调度和分层功能的细化。为了纳入水下特定的物理先验，我们进一步提出了降解引导的自适应特征融合模块和一个结合感知一致性，直方图匹配和特征级对比度的混合损耗函数。基准数据集上的全面实验表明，我们的方法有效地恢复了具有优越的颜色保真度，感知质量和结构细节的水下图像。与SOTA方法相比，我们的框架在定量指标和定性视觉评估方面都取得了重大改进。</li>
</ul>

<h3>Title: Subtyping Breast Lesions via Generative Augmentation based Long-tailed Recognition in Ultrasound</h3>
<ul>
<li><strong>Authors: </strong>Shijing Chen, Xinrui Zhou, Yuhao Wang, Yuhao Huang, Ao Chang, Dong Ni, Ruobing Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22568">https://arxiv.org/abs/2507.22568</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22568">https://arxiv.org/pdf/2507.22568</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22568]] Subtyping Breast Lesions via Generative Augmentation based Long-tailed Recognition in Ultrasound(https://arxiv.org/abs/2507.22568)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Accurate identification of breast lesion subtypes can facilitate personalized treatment and interventions. Ultrasound (US), as a safe and accessible imaging modality, is extensively employed in breast abnormality screening and diagnosis. However, the incidence of different subtypes exhibits a skewed long-tailed distribution, posing significant challenges for automated recognition. Generative augmentation provides a promising solution to rectify data distribution. Inspired by this, we propose a dual-phase framework for long-tailed classification that mitigates distributional bias through high-fidelity data synthesis while avoiding overuse that corrupts holistic performance. The framework incorporates a reinforcement learning-driven adaptive sampler, dynamically calibrating synthetic-real data ratios by training a strategic multi-agent to compensate for scarcities of real data while ensuring stable discriminative capability. Furthermore, our class-controllable synthetic network integrates a sketch-grounded perception branch that harnesses anatomical priors to maintain distinctive class features while enabling annotation-free inference. Extensive experiments on an in-house long-tailed and a public imbalanced breast US datasets demonstrate that our method achieves promising performance compared to state-of-the-art approaches. More synthetic images can be found at this https URL.</li>
<li><strong>摘要：</strong>准确鉴定乳腺病变亚型可以促进个性化治疗和干预措施。超声（美国）作为一种安全且可访问的成像方式，广泛用于乳房异常筛查和诊断。但是，不同亚型的发生率表现出偏斜的长尾分布，对自动识别提出了重大挑战。生成增强为纠正数据分布提供了有希望的解决方案。受此启发，我们提出了一个双相框架，用于长尾分类，通过高保真数据综合来减轻分布偏见，同时避免过度使用会破坏整体性能的过度使用。该框架结合了增强学习驱动的自适应采样器，通过训练一种战略性的多机构来动态校准合成真实的数据比率，以补偿实际数据的稀缺性，同时确保稳定的判别能力。此外，我们的类可控制的合成网络集成了一个素描的感知分支，该分支利用解剖学先验来维持独特的类特征，同时启用无注释的推理。对内部长尾和公共不平衡的美国数据集进行的广泛实验表明，与最先进的方法相比，我们的方法实现了有希望的性能。在此HTTPS URL上可以找到更多的合成图像。</li>
</ul>

<h3>Title: Generative Active Learning for Long-tail Trajectory Prediction via Controllable Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Daehee Park, Monu Surana, Pranav Desai, Ashish Mehta, Reuben MV John, Kuk-Jin Yoon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22615">https://arxiv.org/abs/2507.22615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22615">https://arxiv.org/pdf/2507.22615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22615]] Generative Active Learning for Long-tail Trajectory Prediction via Controllable Diffusion Model(https://arxiv.org/abs/2507.22615)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>While data-driven trajectory prediction has enhanced the reliability of autonomous driving systems, it still struggles with rarely observed long-tail scenarios. Prior works addressed this by modifying model architectures, such as using hypernetworks. In contrast, we propose refining the training process to unlock each model's potential without altering its structure. We introduce Generative Active Learning for Trajectory prediction (GALTraj), the first method to successfully deploy generative active learning into trajectory prediction. It actively identifies rare tail samples where the model fails and augments these samples with a controllable diffusion model during training. In our framework, generating scenarios that are diverse, realistic, and preserve tail-case characteristics is paramount. Accordingly, we design a tail-aware generation method that applies tailored diffusion guidance to generate trajectories that both capture rare behaviors and respect traffic rules. Unlike prior simulation methods focused solely on scenario diversity, GALTraj is the first to show how simulator-driven augmentation benefits long-tail learning in trajectory prediction. Experiments on multiple trajectory datasets (WOMD, Argoverse2) with popular backbones (QCNet, MTR) confirm that our method significantly boosts performance on tail samples and also enhances accuracy on head samples.</li>
<li><strong>摘要：</strong>尽管数据驱动的轨迹预测提高了自主驾驶系统的可靠性，但它仍然很少在观察到的长尾方案。先前的工作通过修改模型体系结构（例如使用HyperNetworks）来解决此问题。相比之下，我们建议精炼训练过程，以解锁每个模型的潜力而不改变其结构。我们为轨迹预测（Galtraj）引入生成活性学习，这是成功将生成活性学习的第一种方法用于轨迹预测。它积极识别稀有的尾部样品，其中该模型在训练过程中以可控扩散模型而失败并增强了这些样品。在我们的框架中，生成多样化，现实和保存尾盘特征的场景至关重要。因此，我们设计了一种尾声的生成方法，该方法应用了量身定制的扩散指南，以生成既捕获稀有行为又尊重交通规则的轨迹。与仅关注场景多样性的先前仿真方法不同，Galtraj是第一个展示模拟器驱动的增强如何在轨迹预测中带来长尾学习的方法。具有流行骨干（QCNET，MTR）的多个轨迹数据集（WOMD，argoverse2）的实验证实，我们的方法显着提高了尾部样品的性能，并提高了头部样品的准确性。</li>
</ul>

<h3>Title: LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text Pairing</h3>
<ul>
<li><strong>Authors: </strong>Federico Girella, Davide Talon, Ziyue Liu, Zanxi Ruan, Yiming Wang, Marco Cristani</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22627">https://arxiv.org/abs/2507.22627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22627">https://arxiv.org/pdf/2507.22627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22627]] LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text Pairing(https://arxiv.org/abs/2507.22627)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Fashion design is a complex creative process that blends visual and textual expressions. Designers convey ideas through sketches, which define spatial structure and design elements, and textual descriptions, capturing material, texture, and stylistic details. In this paper, we present LOcalized Text and Sketch for fashion image generation (LOTS), an approach for compositional sketch-text based generation of complete fashion outlooks. LOTS leverages a global description with paired localized sketch + text information for conditioning and introduces a novel step-based merging strategy for diffusion adaptation. First, a Modularized Pair-Centric representation encodes sketches and text into a shared latent space while preserving independent localized features; then, a Diffusion Pair Guidance phase integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we build on Fashionpedia to release Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Quantitative results show LOTS achieves state-of-the-art image generation performance on both global and localized metrics, while qualitative examples and a human evaluation study highlight its unprecedented level of design customization.</li>
<li><strong>摘要：</strong>时装设计是一个复杂的创作过程，可以融合视觉和文本表达式。设计师通过草图传达思想，这些草图定义了空间结构和设计元素以及文本描述，捕获材料，纹理和风格细节。在本文中，我们介绍了时尚图像生成的本地化文本和草图，这是一种基于构图素描文本的方法的方法。 Loce利用配对的本地化草图 +文本信息来利用全球描述进行调理，并引入了一种基于步骤的新型合并策略以进行扩散适应。首先，模块化的配对表示形式将草图和文本编码为共享的潜在空间，同时保留独立的局部特征。然后，扩散对引导阶段通过基于注意力的指导在扩散模型的多步降解过程中都集成了局部和全局条件。为了验证我们的方法，我们以Fashionpedia为基础来释放Greatchy，这是第一个时尚数据集，每个图像提供了多个短信对。定量结果表明，大量在全球和局部指标上都取得了最先进的图像生成性能，而定性示例和人类评估研究突出了其前所未有的设计定制水平。</li>
</ul>

<h3>Title: Graph-Guided Dual-Level Augmentation for 3D Scene Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Hongbin Lin, Yifan Jiang, Juangui Xu, Jesse Jiaxi Xu, Yi Lu, Zhengyu Hu, Ying-Cong Chen, Hao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22668">https://arxiv.org/abs/2507.22668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22668">https://arxiv.org/pdf/2507.22668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22668]] Graph-Guided Dual-Level Augmentation for 3D Scene Segmentation(https://arxiv.org/abs/2507.22668)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>3D point cloud segmentation aims to assign semantic labels to individual points in a scene for fine-grained spatial understanding. Existing methods typically adopt data augmentation to alleviate the burden of large-scale annotation. However, most augmentation strategies only focus on local transformations or semantic recomposition, lacking the consideration of global structural dependencies within scenes. To address this limitation, we propose a graph-guided data augmentation framework with dual-level constraints for realistic 3D scene synthesis. Our method learns object relationship statistics from real-world data to construct guiding graphs for scene generation. Local-level constraints enforce geometric plausibility and semantic consistency between objects, while global-level constraints maintain the topological structure of the scene by aligning the generated layout with the guiding graph. Extensive experiments on indoor and outdoor datasets demonstrate that our framework generates diverse and high-quality augmented scenes, leading to consistent improvements in point cloud segmentation performance across various models.</li>
<li><strong>摘要：</strong>3D点云分段旨在将语义标签分配给场景中的各个点，以进行细粒度的空间理解。现有方法通常采用数据增强来减轻大规模注释的负担。但是，大多数增强策略仅着眼于局部转换或语义重构，而缺乏对场景内的全球结构依赖性的考虑。为了解决此限制，我们提出了一个图形引导的数据增强框架，并具有对现实的3D场景合成的双级约束。我们的方法从现实世界数据中学习对象关系统计信息，以构建场景生成的指导图。局部级别的约束在对象之间实现几何形状合理性和语义一致性，而全局级别的约束通过将生成的布局与引导图对齐来保持场景的拓扑结构。对室内和室外数据集进行的广泛实验表明，我们的框架会产生多样化和高质量的增强场景，从而导致各种模型的Point Cloud细分性能的一致改进。</li>
</ul>

<h3>Title: Zero-Shot Image Anomaly Detection Using Generative Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Lemar Abdi, Amaan Valiuddin, Francisco Caetano, Christiaan Viviers, Fons van der Sommen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22692">https://arxiv.org/abs/2507.22692</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22692">https://arxiv.org/pdf/2507.22692</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22692]] Zero-Shot Image Anomaly Detection Using Generative Foundation Models(https://arxiv.org/abs/2507.22692)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Detecting out-of-distribution (OOD) inputs is pivotal for deploying safe vision systems in open-world environments. We revisit diffusion models, not as generators, but as universal perceptual templates for OOD detection. This research explores the use of score-based generative models as foundational tools for semantic anomaly detection across unseen datasets. Specifically, we leverage the denoising trajectories of Denoising Diffusion Models (DDMs) as a rich source of texture and semantic information. By analyzing Stein score errors, amplified through the Structural Similarity Index Metric (SSIM), we introduce a novel method for identifying anomalous samples without requiring re-training on each target dataset. Our approach improves over state-of-the-art and relies on training a single model on one dataset -- CelebA -- which we find to be an effective base distribution, even outperforming more commonly used datasets like ImageNet in several settings. Experimental results show near-perfect performance on some benchmarks, with notable headroom on others, highlighting both the strength and future potential of generative foundation models in anomaly detection.</li>
<li><strong>摘要：</strong>检测到分布（OOD）输入对于在开放世界环境中部署安全视觉系统是关键的。我们重新审视扩散模型，而不是作为发电机，而是作为OOD检测的通用感知模板。这项研究探讨了基于分数的生成模型用作跨看不见数据集的语义异常检测的基础工具。具体而言，我们利用脱氧扩散模型（DDMS）作为质感和语义信息的丰富来源的脱氧轨迹。通过分析Stein分数误差，通过结构相似性指标（SSIM）放大，我们引入了一种新的方法来识别异常样品，而无需在每个目标数据集中重新训练。我们的方法改进了最先进的方法，并依靠在一个数据集中训练单个模型 -  Celeba-我们认为这是一个有效的基础分布，甚至在多种设置中都超过了更常用的数据集（例如Imagenet）。实验结果表明，在某些基准测试中的表现近乎完美，在其他基准上具有著名的净空，这既突出了在异常检测中生成基础模型的强度和未来潜力。</li>
</ul>

<h3>Title: HOLA: Enhancing Audio-visual Deepfake Detection via Hierarchical Contextual Aggregations and Efficient Pre-training</h3>
<ul>
<li><strong>Authors: </strong>Xuecheng Wu, Danlei Huang, Heli Sun, Xinyi Yin, Yifan Wang, Hao Wang, Jia Zhang, Fei Wang, Peihao Guo, Suyu Xing, Junxiao Xue, Liang He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22781">https://arxiv.org/abs/2507.22781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22781">https://arxiv.org/pdf/2507.22781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22781]] HOLA: Enhancing Audio-visual Deepfake Detection via Hierarchical Contextual Aggregations and Efficient Pre-training(https://arxiv.org/abs/2507.22781)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Advances in Generative AI have made video-level deepfake detection increasingly challenging, exposing the limitations of current detection techniques. In this paper, we present HOLA, our solution to the Video-Level Deepfake Detection track of 2025 1M-Deepfakes Detection Challenge. Inspired by the success of large-scale pre-training in the general domain, we first scale audio-visual self-supervised pre-training in the multimodal video-level deepfake detection, which leverages our self-built dataset of 1.81M samples, thereby leading to a unified two-stage framework. To be specific, HOLA features an iterative-aware cross-modal learning module for selective audio-visual interactions, hierarchical contextual modeling with gated aggregations under the local-global perspective, and a pyramid-like refiner for scale-aware cross-grained semantic enhancements. Moreover, we propose the pseudo supervised singal injection strategy to further boost model performance. Extensive experiments across expert models and MLLMs impressivly demonstrate the effectiveness of our proposed HOLA. We also conduct a series of ablation studies to explore the crucial design factors of our introduced components. Remarkably, our HOLA ranks 1st, outperforming the second by 0.0476 AUC on the TestA set.</li>
<li><strong>摘要：</strong>生成AI的进步使视频级别的DeepFake检测越来越具有挑战性，从而揭示了当前检测技术的局限性。在本文中，我们提出了Hola，这是我们对2025 1m深击蛋糕检测挑战的视频级深击检测轨道的解决方案。受到大规模预训练在一般领域的成功的启发，我们在多模式视频级的深层效果检测中首次缩放了视听自我监督的预训练，该预训练利用了我们的181万个样品的自我构建数据集，从而导致了一个统一的两步框架。要具体而言，Hola具有迭代感知的跨模式学习模块，用于选择性视听相互作用，层次上下文建模在局部 - 全球视角下与封闭式聚合以及用于尺度吸引的交叉粒度语义增强的金字化炼油机。此外，我们提出了伪监督的单次注射策略，以进一步提高模型性能。跨专家模型和MLLM的广泛实验令人印象深刻地证明了我们提出的HOLA的有效性。我们还进行了一系列消融研究，以探索我们引入的组件的关键设计因素。值得注意的是，我们的Hola排名第一，在Testa组上表现优于0.0476 AUC。</li>
</ul>

<h3>Title: DO-EM: Density Operator Expectation Maximization</h3>
<ul>
<li><strong>Authors: </strong>Adit Vishnu, Abhay Shastry, Dhruva Kashyap, Chiranjib Bhattacharyya</a></li>
<li><strong>Subjects: </strong>cs.LG, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22786">https://arxiv.org/abs/2507.22786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22786">https://arxiv.org/pdf/2507.22786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22786]] DO-EM: Density Operator Expectation Maximization(https://arxiv.org/abs/2507.22786)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Density operators, quantum generalizations of probability distributions, are gaining prominence in machine learning due to their foundational role in quantum computing. Generative modeling based on density operator models (\textbf{DOMs}) is an emerging field, but existing training algorithms -- such as those for the Quantum Boltzmann Machine -- do not scale to real-world data, such as the MNIST dataset. The Expectation-Maximization algorithm has played a fundamental role in enabling scalable training of probabilistic latent variable models on real-world datasets. \textit{In this paper, we develop an Expectation-Maximization framework to learn latent variable models defined through \textbf{DOMs} on classical hardware, with resources comparable to those used for probabilistic models, while scaling to real-world data.} However, designing such an algorithm is nontrivial due to the absence of a well-defined quantum analogue to conditional probability, which complicates the Expectation step. To overcome this, we reformulate the Expectation step as a quantum information projection (QIP) problem and show that the Petz Recovery Map provides a solution under sufficient conditions. Using this formulation, we introduce the Density Operator Expectation Maximization (DO-EM) algorithm -- an iterative Minorant-Maximization procedure that optimizes a quantum evidence lower bound. We show that the \textbf{DO-EM} algorithm ensures non-decreasing log-likelihood across iterations for a broad class of models. Finally, we present Quantum Interleaved Deep Boltzmann Machines (\textbf{QiDBMs}), a \textbf{DOM} that can be trained with the same resources as a DBM. When trained with \textbf{DO-EM} under Contrastive Divergence, a \textbf{QiDBM} outperforms larger classical DBMs in image generation on the MNIST dataset, achieving a 40--60\% reduction in the Fréchet Inception Distance.</li>
<li><strong>摘要：</strong>密度运算符，概率分布的量子概括，由于其在量子计算中的基础作用，机器学习变得突出。基于密度运算符模型（\ textbf {doms}）的生成建模是一个新兴领域，但是现有的训练算法（例如量子玻尔兹曼机器的机器）不扩展到现实世界数据，例如MNIST数据集。期望最大化算法在实现现实世界数据集的概率潜在变量模型的可扩展培训方面发挥了基本作用。 \ textIt {在本文中，我们开发了一个期望的最大化框架，以学习通过\ textbf {doms}定义的潜在变量模型，其资源与用于概率模型的资源可比，同时扩展到现实世界中的数据。}但是，由于构成量的构成量的构成，因此构成了良好的量子，该量子的变化是不合时宜的。 步。为了克服这一点，我们将期望步骤重新制定为量子信息投影（QIP）问题，并表明PETZ恢复图在充分条件下提供了解决方案。使用此公式，我们介绍了密度运算符期望最大化（DO-EM）算法 - 一种迭代性的次要最大化过程，可优化量子证据下限。我们表明，\ textbf {do-em}算法确保了一系列广泛模型的遍历遍历遍历的log-oikelihood。最后，我们提出了量子交织的深玻尔兹曼机器（\ textbf {qidbms}），\ textbf {dom}，可以用与DBM相同的资源训练。当在对比差异下接受\ textbf {do-em}培训时，A \ textbf {qidbm}在MNIST数据集上的图像生成中的较大经典dbms的表现，达到40--60 \％\％\％\％\％\％。</li>
</ul>

<h3>Title: G-Core: A Simple, Scalable and Balanced RLHF Trainer</h3>
<ul>
<li><strong>Authors: </strong>Junyu Wu, Weiming Chang, Xiaotao Liu, Guanyou He, Haoqiang Hong, Boqi Liu, Hongtao Tian, Tao Yang, Yunsheng Shi, Feng Lin, Ting Yao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22789">https://arxiv.org/abs/2507.22789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22789">https://arxiv.org/pdf/2507.22789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22789]] G-Core: A Simple, Scalable and Balanced RLHF Trainer(https://arxiv.org/abs/2507.22789)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) has become an increasingly popular paradigm for training large language models (LLMs) and diffusion models. While existing RLHF training systems have enabled significant progress, they often face challenges in scaling to multi-modal and diffusion workflows and adapting to dynamic workloads. In particular, current approaches may encounter limitations in controller scalability, flexible resource placement, and efficient orchestration when handling complex RLHF pipelines, especially in scenarios involving dynamic sampling or generative reward modeling. In this paper, we present \textbf{G-Core}, a simple, scalable, and balanced RLHF training framework designed to address these challenges. G-Core introduces a parallel controller programming model, enabling flexible and efficient orchestration of complex RLHF workflows without the bottlenecks of a single centralized controller. Furthermore, we propose a dynamic placement schema that adaptively partitions resources and schedules workloads, significantly reducing hardware idle time and improving utilization, even under highly variable training conditions. G-Core has successfully trained models that support WeChat product features serving a large-scale user base, demonstrating its effectiveness and robustness in real-world scenarios. Our results show that G-Core advances the state of the art in RLHF training, providing a solid foundation for future research and deployment of large-scale, human-aligned models.</li>
<li><strong>摘要：</strong>从人类反馈（RLHF）中学习的强化已成为训练大语模型（LLM）和扩散模型的越来越流行的范式。尽管现有的RLHF培训系统已取得了重大进展，但它们通常在扩展到多模式和扩散工作流程并适应动态工作量方面面临挑战。特别是，当处理复杂的RLHF管道时，当前的方法可能会遇到控制器可伸缩性，灵活的资源放置和有效的编排的限制，尤其是在涉及动态采样或生成奖励建模的情况下。在本文中，我们提出\ textbf {g-core}，这是一个简单，可扩展和平衡的RLHF培训框架，旨在应对这些挑战。 G-core引入了平行控制器编程模型，从而在没有单个集中式控制器的瓶颈的情况下可以灵活有效地编排复杂的RLHF工作流程。此外，我们提出了一个动态的放置模式，即使在高度可变的培训条件下，也可以自适应地分配资源和计划工作量，从而大大减少硬件空闲时间并改善利用率。 G-Core已成功训练了支持微信产品功能的模型，这些功能为大规模的用户群提供服务，并在现实世界中证明了其有效性和鲁棒性。我们的结果表明，G-Core在RLHF培训中的最新技术发展，为将来的研究和部署提供了稳固的基础。</li>
</ul>

<h3>Title: Advancing Fetal Ultrasound Image Quality Assessment in Low-Resource Settings</h3>
<ul>
<li><strong>Authors: </strong>Dongli He, Hu Wang, Mohammad Yaqub</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22802">https://arxiv.org/abs/2507.22802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22802">https://arxiv.org/pdf/2507.22802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22802]] Advancing Fetal Ultrasound Image Quality Assessment in Low-Resource Settings(https://arxiv.org/abs/2507.22802)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Accurate fetal biometric measurements, such as abdominal circumference, play a vital role in prenatal care. However, obtaining high-quality ultrasound images for these measurements heavily depends on the expertise of sonographers, posing a significant challenge in low-income countries due to the scarcity of trained personnel. To address this issue, we leverage FetalCLIP, a vision-language model pretrained on a curated dataset of over 210,000 fetal ultrasound image-caption pairs, to perform automated fetal ultrasound image quality assessment (IQA) on blind-sweep ultrasound data. We introduce FetalCLIP$_{CLS}$, an IQA model adapted from FetalCLIP using Low-Rank Adaptation (LoRA), and evaluate it on the ACOUSLIC-AI dataset against six CNN and Transformer baselines. FetalCLIP$_{CLS}$ achieves the highest F1 score of 0.757. Moreover, we show that an adapted segmentation model, when repurposed for classification, further improves performance, achieving an F1 score of 0.771. Our work demonstrates how parameter-efficient fine-tuning of fetal ultrasound foundation models can enable task-specific adaptations, advancing prenatal care in resource-limited settings. The experimental code is available at: this https URL.</li>
<li><strong>摘要：</strong>准确的胎儿生物识别测量（例如腹膜）在产前护理中起着至关重要的作用。但是，为这些测量值获得高质量的超声图像在很大程度上取决于超声检查员的专业知识，由于训练有素的人员缺乏，在低收入国家构成了重大挑战。为了解决这个问题，我们利用了胎儿限制，这是一种在超过210,000个胎儿超声图像限制对的策划数据集上预测的视觉模型，以对盲目超声数据进行自动胎儿超声图像质量评估（IQA）。我们介绍了使用低级适应（Lora）改编自胎儿clip的IQA模型$ _ {Cls} $，并在Acouslic-AI数据集上对六个CNN和变压器基线进行评估。 FetalClip $ _ {Cls} $达到0.757的最高F1分数。此外，我们表明，改编的分割模型在重新分类以进一步提高性能，达到0.771的F1分数。我们的工作展示了胎儿超声基础模型的参数有效微调如何实现特定于任务的适应，从而在资源有限的设置中推进产前护理。实验代码可获得：此HTTPS URL。</li>
</ul>

<h3>Title: DISTIL: Data-Free Inversion of Suspicious Trojan Inputs via Latent Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Hossein Mirzaei, Zeinab Taghavi, Sepehr Rezaee, Masoud Hadi, Moein Madadi, Mackenzie W. Mathis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22813">https://arxiv.org/abs/2507.22813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22813">https://arxiv.org/pdf/2507.22813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22813]] DISTIL: Data-Free Inversion of Suspicious Trojan Inputs via Latent Diffusion(https://arxiv.org/abs/2507.22813)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Deep neural networks have demonstrated remarkable success across numerous tasks, yet they remain vulnerable to Trojan (backdoor) attacks, raising serious concerns about their safety in real-world mission-critical applications. A common countermeasure is trigger inversion -- reconstructing malicious "shortcut" patterns (triggers) inserted by an adversary during training. Current trigger-inversion methods typically search the full pixel space under specific assumptions but offer no assurances that the estimated trigger is more than an adversarial perturbation that flips the model output. Here, we propose a data-free, zero-shot trigger-inversion strategy that restricts the search space while avoiding strong assumptions on trigger appearance. Specifically, we incorporate a diffusion-based generator guided by the target classifier; through iterative generation, we produce candidate triggers that align with the internal representations the model relies on for malicious behavior. Empirical evaluations, both quantitative and qualitative, show that our approach reconstructs triggers that effectively distinguish clean versus Trojaned models. DISTIL surpasses alternative methods by high margins, achieving up to 7.1% higher accuracy on the BackdoorBench dataset and a 9.4% improvement on trojaned object detection model scanning, offering a promising new direction for reliable backdoor defense without reliance on extensive data or strong prior assumptions about triggers. The code is available at this https URL.</li>
<li><strong>摘要：</strong>深层神经网络在众多任务中表现出了巨大的成功，但它们仍然容易受到特洛伊木马（后门）攻击的影响，从而引起了人们对他们在现实世界中关键任务应用中的安全性的严重关注。一个共同的对策是触发反演 - 重建训练期间对手插入的恶意“快捷方式”模式（触发器）。当前的触发插入方法通常在特定假设下搜索完整的像素空间，但没有保证估计的触发器不仅仅是对模型输出的对抗性扰动。在这里，我们提出了一个无数据的，零拍的触发策略，该策略限制了搜索空间，同时避免了对触发外观的强烈假设。具体而言，我们结合了一个基于扩散的发电机，以目标分类器为指导。通过迭代生成，我们产生了候选触发器，该触发者与模型依赖于恶意行为的内部表示形式保持一致。经验评估，无论是定量还是定性，都表明，我们的方法重建了有效区分清洁与木出模型的触发因素。 Distil超过了高边距的替代方法，在后门数据集上的准确性高达7.1％，在Trojaned对象检测模型扫描方面提高了9.4％的提高，为可靠的后门防御提供了有希望的新方向，而无需依赖于扩展数据或对触发器的强烈假设。该代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Bi-Level Optimization for Self-Supervised AI-Generated Face Detection</h3>
<ul>
<li><strong>Authors: </strong>Mian Zou, Nan Zhong, Baosheng Yu, Yibing Zhan, Kede Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22824">https://arxiv.org/abs/2507.22824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22824">https://arxiv.org/pdf/2507.22824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22824]] Bi-Level Optimization for Self-Supervised AI-Generated Face Detection(https://arxiv.org/abs/2507.22824)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>AI-generated face detectors trained via supervised learning typically rely on synthesized images from specific generators, limiting their generalization to emerging generative techniques. To overcome this limitation, we introduce a self-supervised method based on bi-level optimization. In the inner loop, we pretrain a vision encoder only on photographic face images using a set of linearly weighted pretext tasks: classification of categorical exchangeable image file format (EXIF) tags, ranking of ordinal EXIF tags, and detection of artificial face manipulations. The outer loop then optimizes the relative weights of these pretext tasks to enhance the coarse-grained detection of manipulated faces, serving as a proxy task for identifying AI-generated faces. In doing so, it aligns self-supervised learning more closely with the ultimate goal of AI-generated face detection. Once pretrained, the encoder remains fixed, and AI-generated faces are detected either as anomalies under a Gaussian mixture model fitted to photographic face features or by a lightweight two-layer perceptron serving as a binary classifier. Extensive experiments demonstrate that our detectors significantly outperform existing approaches in both one-class and binary classification settings, exhibiting strong generalization to unseen generators.</li>
<li><strong>摘要：</strong>通过监督学习训练的AI生成的面部探测器通常依赖于特定发电机的合成图像，从而将其泛化限制为新兴生成技术。为了克服这一限制，我们基于双层优化引入了一种自我监督的方法。在内部循环中，我们仅使用一组线性加权的借口任务在摄影面图像上预先编码：分类可交换图像文件格式（EXIF）标签的分类，序数Exif标签的排名以及对人造面部操作的检测。然后，外循环优化了这些借口任务的相对权重，以增强对操纵面的粗粒检测，作为识别AI生成的面孔的代理任务。这样一来，它与AI生成的面部检测的最终目标更加紧密地保持一致。预估计后，编码器保持固定，并在适合于照相脸部特征的高斯混合物模型下检测到AI生成的面部，或者是用轻量级的两层感知器作为二进制分类器。广泛的实验表明，我们的探测器在一级和二元分类设置中都显着优于现有的方法，对看不见的发电机表现出强烈的概括。</li>
</ul>

<h3>Title: ScreenCoder: Advancing Visual-to-Code Generation for Front-End Automation via Modular Multimodal Agents</h3>
<ul>
<li><strong>Authors: </strong>Yilei Jiang, Yaozhi Zheng, Yuxuan Wan, Jiaming Han, Qunzhong Wang, Michael R. Lyu, Xiangyu Yue</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22827">https://arxiv.org/abs/2507.22827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22827">https://arxiv.org/pdf/2507.22827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22827]] ScreenCoder: Advancing Visual-to-Code Generation for Front-End Automation via Modular Multimodal Agents(https://arxiv.org/abs/2507.22827)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Automating the transformation of user interface (UI) designs into front-end code holds significant promise for accelerating software development and democratizing design workflows. While recent large language models (LLMs) have demonstrated progress in text-to-code generation, many existing approaches rely solely on natural language prompts, limiting their effectiveness in capturing spatial layout and visual design intent. In contrast, UI development in practice is inherently multimodal, often starting from visual sketches or mockups. To address this gap, we introduce a modular multi-agent framework that performs UI-to-code generation in three interpretable stages: grounding, planning, and generation. The grounding agent uses a vision-language model to detect and label UI components, the planning agent constructs a hierarchical layout using front-end engineering priors, and the generation agent produces HTML/CSS code via adaptive prompt-based synthesis. This design improves robustness, interpretability, and fidelity over end-to-end black-box methods. Furthermore, we extend the framework into a scalable data engine that automatically produces large-scale image-code pairs. Using these synthetic examples, we fine-tune and reinforce an open-source VLM, yielding notable gains in UI understanding and code quality. Extensive experiments demonstrate that our approach achieves state-of-the-art performance in layout accuracy, structural coherence, and code correctness. Our code is made publicly available at this https URL.</li>
<li><strong>摘要：</strong>自动化用户界面（UI）设计到前端代码的转换具有巨大的希望，可以加速软件开发和民主化设计工作流程。尽管最近的大型语言模型（LLM）在文本到代码生成方面已经表现出进展，但许多现有方法仅依赖于自然语言提示，从而限制了它们在捕获空间布局和视觉设计意图方面的有效性。相比之下，实践中的UI开发本质上是多模式的，通常从视觉草图或模型开始。为了解决这一差距，我们引入了一个模块化的多代理框架，该框架在三个可解释的阶段执行UI到代码生成：接地，计划和生成。接地代理使用视觉模型来检测和标记UI组件，计划代理使用前端工程先验构建层次结构布局，而生成代理通过基于自适应及时的合成生产HTML/CSS代码。这种设计改善了端到端黑框方法的鲁棒性，可解释性和忠诚度。此外，我们将框架扩展到可扩展的数据引擎，该数据引擎会自动产生大规模的图像代码对。使用这些合成示例，我们对开源VLM进行微调和加强，从而在UI理解和代码质量方面取得了显着收益。广泛的实验表明，我们的方法在布局准确性，结构连贯性和代码正确性方面实现了最先进的性能。我们的代码在此HTTPS URL上公开可用。</li>
</ul>

<h3>Title: A Bit of Freedom Goes a Long Way: Classical and Quantum Algorithms for Reinforcement Learning under a Generative Model</h3>
<ul>
<li><strong>Authors: </strong>Andris Ambainis, Joao F. Doriguello, Debbie Lim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.OC, quant-ph, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22854">https://arxiv.org/abs/2507.22854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22854">https://arxiv.org/pdf/2507.22854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22854]] A Bit of Freedom Goes a Long Way: Classical and Quantum Algorithms for Reinforcement Learning under a Generative Model(https://arxiv.org/abs/2507.22854)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We propose novel classical and quantum online algorithms for learning finite-horizon and infinite-horizon average-reward Markov Decision Processes (MDPs). Our algorithms are based on a hybrid exploration-generative reinforcement learning (RL) model wherein the agent can, from time to time, freely interact with the environment in a generative sampling fashion, i.e., by having access to a "simulator". By employing known classical and new quantum algorithms for approximating optimal policies under a generative model within our learning algorithms, we show that it is possible to avoid several paradigms from RL like "optimism in the face of uncertainty" and "posterior sampling" and instead compute and use optimal policies directly, which yields better regret bounds compared to previous works. For finite-horizon MDPs, our quantum algorithms obtain regret bounds which only depend logarithmically on the number of time steps $T$, thus breaking the $O(\sqrt{T})$ classical barrier. This matches the time dependence of the prior quantum works of Ganguly et al. (arXiv'23) and Zhong et al. (ICML'24), but with improved dependence on other parameters like state space size $S$ and action space size $A$. For infinite-horizon MDPs, our classical and quantum bounds still maintain the $O(\sqrt{T})$ dependence but with better $S$ and $A$ factors. Nonetheless, we propose a novel measure of regret for infinite-horizon MDPs with respect to which our quantum algorithms have $\operatorname{poly}\log{T}$ regret, exponentially better compared to classical algorithms. Finally, we generalise all of our results to compact state spaces.</li>
<li><strong>摘要：</strong>我们提出了新的经典和量子在线算法，用于学习有限的 - 马和无限 - 霍尼平均奖励马尔可夫决策过程（MDPS）。我们的算法基于混合探索产生增强学习（RL）模型，其中代理可以不时以生成的采样方式自由地与环境自由互动，即通过访问“模拟器”。通过使用已知的古典和新量子算法来近似学习算法中的生成模型下的最佳政策，我们表明，可以避免使用RL的几种范式，例如“面对不确定性的乐观”和“后验样本”和“后验样本”，而是直接计算和使用最佳的政治，从而获得了与以前的遗憾相比的束缚。对于有限的Horizon MDP，我们的量子算法获得了遗憾界限，后者仅取决于时间步骤$ t $，从而破坏了$ o（\ sqrt {t}）$经典障碍。这与Ganguly等人先前的量子作品的时间依赖性相匹配。 （Arxiv'23）和Zhong等。 （ICML'24），但改善了对其他参数（例如状态空间尺寸$ s $ and Action Space size $ a $ a $）的依赖性。对于Infinite-Horizon MDP，我们的经典和量子界仍然维护$ O（\ sqrt {t}）$依赖性，但具有更好的$ s $和$ a $ ractor。尽管如此，我们提出了一种新颖的措施，以衡量无限 - 摩恩MDP的遗憾，与经典算法相比，我们的量子算法具有$ \ permatatorname {poly} \ log {t} $遗憾。最后，我们将所有结果推广到紧凑的状态空间。</li>
</ul>

<h3>Title: LCS: An AI-based Low-Complexity Scaler for Power-Efficient Super-Resolution of Game Content</h3>
<ul>
<li><strong>Authors: </strong>Simon Pochinda, Momen K. Tageldeen, Mark Thompson, Tony Rinaldi, Troy Giorshev, Keith Lee, Jie Zhou, Frederick Walls</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22873">https://arxiv.org/abs/2507.22873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22873">https://arxiv.org/pdf/2507.22873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22873]] LCS: An AI-based Low-Complexity Scaler for Power-Efficient Super-Resolution of Game Content(https://arxiv.org/abs/2507.22873)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>The increasing complexity of content rendering in modern games has led to a problematic growth in the workload of the GPU. In this paper, we propose an AI-based low-complexity scaler (LCS) inspired by state-of-the-art efficient super-resolution (ESR) models which could offload the workload on the GPU to a low-power device such as a neural processing unit (NPU). The LCS is trained on GameIR image pairs natively rendered at low and high resolution. We utilize adversarial training to encourage reconstruction of perceptually important details, and apply reparameterization and quantization techniques to reduce model complexity and size. In our comparative analysis we evaluate the LCS alongside the publicly available AMD hardware-based Edge Adaptive Scaling Function (EASF) and AMD FidelityFX Super Resolution 1 (FSR1) on five different metrics, and find that the LCS achieves better perceptual quality, demonstrating the potential of ESR models for upscaling on resource-constrained devices.</li>
<li><strong>摘要：</strong>现代游戏中内容渲染的复杂性日益增加，导致GPU工作量有问题。在本文中，我们提出了一个基于AI的低复杂性缩放器（LCS），灵感来自最先进的超级分辨率（ESR）模型，该模型可以将GPU上的工作量卸载到低功耗设备，例如神经处理单元（NPU）。 LCS通过Gameir图像对训练，以低分辨率和高分辨率为生渲染。我们利用对抗性训练来鼓励重建感知重要的细节，并应用重新聚集和量化技术来降低模型的复杂性和大小。在我们的比较分析中，我们将LCS与公开可用的AMD基于AMD硬件的边缘自适应缩放功能（EASF）和AMD FidelityFX超级分辨率1（FSR1）在五个不同的指标上评估，并发现LCS可以提高感知质量，以表明在资源受限设备上进行ESR模型的潜在模型。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
