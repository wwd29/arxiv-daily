<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-08-21</h1>
<h3>Title: Deep Learning for School Dropout Detection: A Comparison of Tabular and Graph-Based Models for Predicting At-Risk Students</h3>
<ul>
<li><strong>Authors: </strong>Pablo G. Almeida, Guilherme A. L. Silva, Valéria Santos, Gladston Moreira, Pedro Silva, Eduardo Luz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14057">https://arxiv.org/abs/2508.14057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14057">https://arxiv.org/pdf/2508.14057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14057]] Deep Learning for School Dropout Detection: A Comparison of Tabular and Graph-Based Models for Predicting At-Risk Students(https://arxiv.org/abs/2508.14057)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Student dropout is a significant challenge in educational systems worldwide, leading to substantial social and economic costs. Predicting students at risk of dropout allows for timely interventions. While traditional Machine Learning (ML) models operating on tabular data have shown promise, Graph Neural Networks (GNNs) offer a potential advantage by capturing complex relationships inherent in student data if structured as graphs. This paper investigates whether transforming tabular student data into graph structures, primarily using clustering techniques, enhances dropout prediction accuracy. We compare the performance of GNNs (a custom Graph Convolutional Network (GCN) and GraphSAGE) on these generated graphs against established tabular models (Random Forest (RF), XGBoost, and TabNet) using a real-world student dataset. Our experiments explore various graph construction strategies based on different clustering algorithms (K-Means, HDBSCAN) and dimensionality reduction techniques (Principal Component Analysis (PCA), Uniform Manifold Approximation and Projection (UMAP)). Our findings demonstrate that a specific GNN configuration, GraphSAGE on a graph derived from PCA-KMeans clustering, achieved superior performance, notably improving the macro F1-score by approximately 7 percentage points and accuracy by nearly 2 percentage points over the strongest tabular baseline (XGBoost). However, other GNN configurations and graph construction methods did not consistently surpass tabular models, emphasizing the critical role of the graph generation strategy and GNN architecture selection. This highlights both the potential of GNNs and the challenges in optimally transforming tabular data for graph-based learning in this domain.</li>
<li><strong>摘要：</strong>学生辍学是全球教育系统的重大挑战，导致了巨大的社会和经济成本。预测有辍学风险的学生可以及时进行干预。尽管在表格数据上运行的传统机器学习（ML）模型已显示出希望，但图形神经网络（GNN）通过捕获学生数据中固有的复杂关系（如果以图为图）来提供潜在的优势。本文研究了主要使用聚类技术将表格学生数据转换为图形结构，从而提高了辍学的预测准确性。我们使用现实世界中的学生数据集比较了这些生成的表格模型（Random Forest（RF），XGBoost和TabNet）上GNN（自定义图形卷积网络（GCN）和图形）的性能。我们的实验基于不同的聚类算法（K-均值，HDBSCAN）和降低降低技术（主要成分分析（PCA），统一的歧管近似和投影（UMAP））探索各种图形构造策略。我们的发现表明，特定的GNN配置，即从PCA-KMEANS聚类中得出的图表上的图形，实现了卓越的性能，尤其是将宏F1分数提高了约7个百分点，而精度比最强的表格基线（xgboost）相比将近2个百分点。但是，其他GNN配置和图形构造方法并未始终超过表格模型，从而强调了图生成策略和GNN体系结构选择的关键作用。这既突出了GNN的潜力，也强调了最佳转换该域中基于图的学​​习的表格数据的挑战。</li>
</ul>

<h3>Title: GEPD:GAN-Enhanced Generalizable Model for EEG-Based Detection of Parkinson's Disease</h3>
<ul>
<li><strong>Authors: </strong>Qian Zhang, Ruilin Zhang, Biaokai Zhu, Xun Han, Jun Xiao, Yifan Liu, Zhe Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14074">https://arxiv.org/abs/2508.14074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14074">https://arxiv.org/pdf/2508.14074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14074]] GEPD:GAN-Enhanced Generalizable Model for EEG-Based Detection of Parkinson's Disease(https://arxiv.org/abs/2508.14074)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, quality assessment</a></li>
<li><strong>Abstract: </strong>Electroencephalography has been established as an effective method for detecting Parkinson's disease, typically diagnosed this http URL Parkinson's disease detection methods have shown significant success within individual datasets, however, the variability in detection methods across different EEG datasets and the small size of each dataset pose challenges for training a generalizable model for cross-dataset scenarios. To address these issues, this paper proposes a GAN-enhanced generalizable model, named GEPD, specifically for EEG-based cross-dataset classification of Parkinson's this http URL, we design a generative network that creates fusion EEG data by controlling the distribution similarity between generated data and real this http URL addition, an EEG signal quality assessment model is designed to ensure the quality of generated data this http URL, we design a classification network that utilizes a combination of multiple convolutional neural networks to effectively capture the time-frequency characteristics of EEG signals, while maintaining a generalizable structure and ensuring easy this http URL work is dedicated to utilizing intelligent methods to study pathological manifestations, aiming to facilitate the diagnosis and monitoring of neurological this http URL evaluation results demonstrate that our model performs comparably to state-of-the-art models in cross-dataset settings, achieving an accuracy of 84.3% and an F1-score of 84.0%, showcasing the generalizability of the proposed model.</li>
<li><strong>摘要：</strong>脑电图已被确定为检测帕金森氏病的有效方法，通常诊断出该HTTP URL帕金森氏病检测方法在单个数据集中显示出很大的成功，但是，不同EEG数据集的检测方法的可变性以及每个数据集的较小尺寸构成了每个数据集的挑战，用于培训一个可培训的跨境场景的挑战。 To address these issues, this paper proposes a GAN-enhanced generalizable model, named GEPD, specifically for EEG-based cross-dataset classification of Parkinson's this http URL, we design a generative network that creates fusion EEG data by controlling the distribution similarity between generated data and real this http URL addition, an EEG signal quality assessment model is designed to ensure the quality of generated data this http URL, we design a classification network that utilizes a combination of multiple convolutional neural networks to effectively capture the time-frequency characteristics of EEG signals, while maintaining a generalizable structure and ensuring easy this http URL work is dedicated to utilizing intelligent methods to study pathological manifestations, aiming to facilitate the diagnosis and monitoring of neurological this http URL evaluation results demonstrate that our model performs comparably to state-of-the-art跨数据集设置中的模型达到了84.3％的精度和84.0％的F1得分，展示了所提出模型的普遍性。</li>
</ul>

<h3>Title: PersRM-R1: Enhance Personalized Reward Modeling with Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Mengdi Li, Guanqiao Chen, Xufeng Zhao, Haochen Wen, Shu Yang, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14076">https://arxiv.org/abs/2508.14076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14076">https://arxiv.org/pdf/2508.14076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14076]] PersRM-R1: Enhance Personalized Reward Modeling with Reinforcement Learning(https://arxiv.org/abs/2508.14076)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reward models (RMs), which are central to existing post-training methods, aim to align LLM outputs with human values by providing feedback signals during fine-tuning. However, existing RMs struggle to capture nuanced, user-specific preferences, especially under limited data and across diverse domains. Thus, we introduce PersRM-R1, the first reasoning-based reward modeling framework specifically designed to identify and represent personal factors from only one or a few personal exemplars. To address challenges including limited data availability and the requirement for robust generalization, our approach combines synthetic data generation with a two-stage training pipeline consisting of supervised fine-tuning followed by reinforcement fine-tuning. Experimental results demonstrate that PersRM-R1 outperforms existing models of similar size and matches the performance of much larger models in both accuracy and generalizability, paving the way for more effective personalized LLMs.</li>
<li><strong>摘要：</strong>奖励模型（RMS）是现有的训练后方法核心，旨在通过在微调过程中提供反馈信号来使LLM输出与人类价值保持一致。但是，现有的RMS难以捕获细微的，特定于用户的偏好，尤其是在有限的数据和跨不同领域的情况下。因此，我们介绍了PERSRM-R1，这是第一个基于推理的奖励建模框架，专门旨在识别和代表仅一个或几个个人示例中的个人因素。为了应对包括有限数据可用性的挑战和强大概括的要求，我们的方法将合成数据的生成与两阶段的训练管道结合在一起，该管道包括受监督的微调，然后加强微调。实验结果表明，PERSRM-R1的表现优于现有大小的现有模型，并匹配精确性和概括性的更大模型的性能，为更有效的个性化LLM铺平了道路。</li>
</ul>

<h3>Title: EEGDM: EEG Representation Learning via Generative Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Jia Hong Puah, Sim Kuan Goh, Ziwei Zhang, Zixuan Ye, Chow Khuen Chan, Kheng Seang Lim, Si Lei Fong, Kok Sin Woon</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14086">https://arxiv.org/abs/2508.14086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14086">https://arxiv.org/pdf/2508.14086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14086]] EEGDM: EEG Representation Learning via Generative Diffusion Model(https://arxiv.org/abs/2508.14086)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While electroencephalogram (EEG) has been a crucial tool for monitoring the brain and diagnosing neurological disorders (e.g., epilepsy), learning meaningful representations from raw EEG signals remains challenging due to limited annotations and high signal variability. Recently, EEG foundation models (FMs) have shown promising potential by adopting transformer architectures and self-supervised pre-training methods from large language models (e.g., masked prediction) to learn representations from diverse EEG data, followed by fine-tuning on specific EEG tasks. Nonetheless, these large models often incurred high computational costs during both training and inference, with only marginal performance improvements as model size increases. In this work, we proposed EEG representation learning framework building upon Generative Diffusion Model (EEGDM). Specifically, we developed structured state-space model for diffusion pretraining (SSMDP) to better capture the temporal dynamics of EEG signals and trained the architecture using a Denoising Diffusion Probabilistic Model. The resulting latent EEG representations were then used for downstream classification tasks via our proposed latent fusion transformer (LFT). To evaluate our method, we used the multi-event Temple University EEG Event Corpus and compared EEGDM with current state-of-the-art approaches, including EEG FMs. Empirical results showed that our method outperformed existing methods while being approximately 19x more lightweight. These findings suggested that EEGDM offered a promising alternative to current FMs. Our code is available at: this https URL.</li>
<li><strong>摘要：</strong>尽管脑电图（EEG）一直是监测大脑和诊断神经系统疾病的关键工具（例如癫痫），但由于注释有限和高信号变异性，从RAW EEG信号中学习有意义的表示有意义的表示仍然具有挑战性。最近，EEG基金会模型（FMS）通过采用变压器体系结构和从大型语言模型（例如，掩盖的预测）中的自我监管的预训练方法来显示出有希望的潜力，从而从不同的EEG数据中学习表示表示，然后对特定的EEG任务进行微调。尽管如此，这些大型模型通常在训练和推理期间都会产生高计算成本，并且随着模型大小的增加，边际性能的提高。在这项工作中，我们提出了基于生成扩散模型（EEGDM）建立的EEG表示框架。具体而言，我们开发了用于扩散预处理（SSMDP）的结构化状态空间模型，以更好地捕获EEG信号的时间动力学，并使用deo的扩散概率模型训练了体系结构。然后，通过我们提出的潜在融合变压器（LFT）将所得的潜在脑电图表示用于下游分类任务。为了评估我们的方法，我们使用了多事件神庙大学EEG活动语料库，并将EEGDM与包括EEG FMS在内的当前最新方法进行了比较。经验结果表明，我们的方法优于现有方法，而轻量级大约是19倍。这些发现表明，EEGDM提供了当前FMS的有前途的替代方案。我们的代码可用：此HTTPS URL。</li>
</ul>

<h3>Title: Physics-Informed Reward Machines</h3>
<ul>
<li><strong>Authors: </strong>Daniel Ajeleye, Ashutosh Trivedi, Majid Zamani</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14093">https://arxiv.org/abs/2508.14093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14093">https://arxiv.org/pdf/2508.14093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14093]] Physics-Informed Reward Machines(https://arxiv.org/abs/2508.14093)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reward machines (RMs) provide a structured way to specify non-Markovian rewards in reinforcement learning (RL), thereby improving both expressiveness and programmability. Viewed more broadly, they separate what is known about the environment, captured by the reward mechanism, from what remains unknown and must be discovered through sampling. This separation supports techniques such as counterfactual experience generation and reward shaping, which reduce sample complexity and speed up learning. We introduce physics-informed reward machines (pRMs), a symbolic machine designed to express complex learning objectives and reward structures for RL agents, thereby enabling more programmable, expressive, and efficient learning. We present RL algorithms capable of exploiting pRMs via counterfactual experiences and reward shaping. Our experimental results show that these techniques accelerate reward acquisition during the training phases of RL. We demonstrate the expressiveness and effectiveness of pRMs through experiments in both finite and continuous physical environments, illustrating that incorporating pRMs significantly improves learning efficiency across several control tasks.</li>
<li><strong>摘要：</strong>奖励机（RMS）提供了一种结构化的方法来指定强化学习（RL）的非马克维亚奖励，从而提高表现力和可编程性。更广泛地看，它们将奖励机制捕获的对环境的了解与未知的内容区分开，必须通过抽样发现。这种分离支持了反事实经验产生和奖励成型等技术，从而降低了样本的复杂性和加快学习的速度。我们介绍了物理知识的奖励机（PRM），这是一种符号机器，旨在为RL代理表达复杂的学习目标和奖励结构，从而实现更多可编程，表现力和高效的学习。我们提出了能够通过反事实经验来利用PRM的RL算法和奖励成型。我们的实验结果表明，这些技术在RL的训练阶段加速了奖励获取。我们通过在有限和连续的物理环境中进行实验来证明PRM的表现力和有效性，这说明了PRMS合并可显着提高多个控制任务的学习效率。</li>
</ul>

<h3>Title: From AI for Science to Agentic Science: A Survey on Autonomous Scientific Discovery</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Wei, Yuejin Yang, Xiang Zhang, Yuhan Chen, Xiang Zhuang, Zhangyang Gao, Dongzhan Zhou, Guangshuai Wang, Zhiqiang Gao, Juntai Cao, Zijie Qiu, Xuming He, Qiang Zhang, Chenyu You, Shuangjia Zheng, Ning Ding, Wanli Ouyang, Nanqing Dong, Yu Cheng, Siqi Sun, Lei Bai, Bowen Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14111">https://arxiv.org/abs/2508.14111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14111">https://arxiv.org/pdf/2508.14111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14111]] From AI for Science to Agentic Science: A Survey on Autonomous Scientific Discovery(https://arxiv.org/abs/2508.14111)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Artificial intelligence (AI) is reshaping scientific discovery, evolving from specialized computational tools into autonomous research partners. We position Agentic Science as a pivotal stage within the broader AI for Science paradigm, where AI systems progress from partial assistance to full scientific agency. Enabled by large language models (LLMs), multimodal systems, and integrated research platforms, agentic AI shows capabilities in hypothesis generation, experimental design, execution, analysis, and iterative refinement -- behaviors once regarded as uniquely human. This survey provides a domain-oriented review of autonomous scientific discovery across life sciences, chemistry, materials science, and physics. We unify three previously fragmented perspectives -- process-oriented, autonomy-oriented, and mechanism-oriented -- through a comprehensive framework that connects foundational capabilities, core processes, and domain-specific realizations. Building on this framework, we (i) trace the evolution of AI for Science, (ii) identify five core capabilities underpinning scientific agency, (iii) model discovery as a dynamic four-stage workflow, (iv) review applications across the above domains, and (v) synthesize key challenges and future opportunities. This work establishes a domain-oriented synthesis of autonomous scientific discovery and positions Agentic Science as a structured paradigm for advancing AI-driven research.</li>
<li><strong>摘要：</strong>人工智能（AI）正在重塑科学发现，从专门的计算工具发展成为自主研究伙伴。我们将代理科学定位为科学范式更广泛的AI中的关键阶段，在该范式中，AI系统从部分援助发展到完整的科学机构。代理AI由大语言模型（LLM），多模式系统和集成研究平台启用，显示了假设产生，实验设计，执行，分析和迭代精炼的能力 - 曾经被认为是独特的人类的行为。这项调查提供了面向领域的综述，对生命科学，化学，材料科学和物理学的自主科学发现。我们通过一个综合框架统一了三个先前零散的观点，即面向过程，面向自治的，面向自治的，面向机制，以机制为导向，该框架连接基础能力，核心过程和特定于领域的实现。在此框架的基础上，我们（i）追踪AI的科学演变，（ii）确定五个基于科学机构的核心能力，（iii）模型发现是动态的四阶段工作流程，（iv）审查上述范围内的应用程序，以及（v）合成关键挑战和未来的挑战和未来的机会。这项工作建立了以域为导向的自主科学发现的综合，并将代理科学定位为用于推进AI驱动研究的结构化范式。</li>
</ul>

<h3>Title: Tooth-Diffusion: Guided 3D CBCT Synthesis with Fine-Grained Tooth Conditioning</h3>
<ul>
<li><strong>Authors: </strong>Said Djafar Said, Torkan Gholamalizadeh, Mostafa Mehdipour Ghazi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14276">https://arxiv.org/abs/2508.14276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14276">https://arxiv.org/pdf/2508.14276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14276]] Tooth-Diffusion: Guided 3D CBCT Synthesis with Fine-Grained Tooth Conditioning(https://arxiv.org/abs/2508.14276)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite the growing importance of dental CBCT scans for diagnosis and treatment planning, generating anatomically realistic scans with fine-grained control remains a challenge in medical image synthesis. In this work, we propose a novel conditional diffusion framework for 3D dental volume generation, guided by tooth-level binary attributes that allow precise control over tooth presence and configuration. Our approach integrates wavelet-based denoising diffusion, FiLM conditioning, and masked loss functions to focus learning on relevant anatomical structures. We evaluate the model across diverse tasks, such as tooth addition, removal, and full dentition synthesis, using both paired and distributional similarity metrics. Results show strong fidelity and generalization with low FID scores, robust inpainting performance, and SSIM values above 0.91 even on unseen scans. By enabling realistic, localized modification of dentition without rescanning, this work opens opportunities for surgical planning, patient communication, and targeted data augmentation in dental AI workflows. The codes are available at: this https URL.</li>
<li><strong>摘要：</strong>尽管牙科CBCT扫描在诊断和治疗计划中的重要性越来越重要，但在医学图像合成中，产生具有细粒度控制的解剖学现实扫描仍然是一个挑战。在这项工作中，我们为3D牙科体积生成的新型条件扩散框架提供了新的条件扩散框架，并以牙齿级别的二进制属性为指导，该属性允许精确控制牙齿的存在和构型。我们的方法集成了基于小波的非授予扩散，膜调节和掩盖的损失功能，以将学习集中在相关的解剖结构上。我们使用配对和分布相似性指标评估了各种任务的模型，例如增加牙齿，去除和全牙合成。结果表明，即使在看不见的扫描时，也表现出强大的FID得分，稳健的介入性能以及超过0.91的SSIM值，表现出强烈的忠诚度和概括性。通过在不进行撤销的情况下实现现实的局部修改，这项工作为手术计划，患者沟通和牙科AI工作流程中有针对性的数据增强打开了机会。代码可用：此HTTPS URL。</li>
</ul>

<h3>Title: GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Amirmohsen Sattarifard, Sepehr Lavasani, Ehsan Imani, Kunlin Zhang, Hanlin Xu, Fengyu Sun, Negar Hassanpour, Chao Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14302">https://arxiv.org/abs/2508.14302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14302">https://arxiv.org/pdf/2508.14302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14302]] GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation(https://arxiv.org/abs/2508.14302)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Deploying Large Language Models (LLMs) on edge hardware demands aggressive, prompt-aware dynamic pruning to reduce computation without degrading quality. Static or predictor-based schemes either lock in a single sparsity pattern or incur extra runtime overhead, and recent zero-shot methods that rely on statistics from a single prompt fail on short prompt and/or long generation scenarios. We introduce A/I-GLASS: Activation- and Impact-based Global-Local neural importance Aggregation for feed-forward network SparSification, two training-free methods that dynamically select FFN units using a rank-aggregation of prompt local and model-intrinsic global neuron statistics. Empirical results across multiple LLMs and benchmarks demonstrate that GLASS significantly outperforms prior training-free methods, particularly in challenging long-form generation scenarios, without relying on auxiliary predictors or adding any inference overhead.</li>
<li><strong>摘要：</strong>在边缘硬件上部署大型语言模型（LLM）需要激进的，及时感知的动态修剪，以减少计算而不会降低质量。基于静态或预测指标的方案要么锁定单个稀疏模式，要么锁定额外的运行时开销，以及最近依赖于单个提示中统计信息的零射击方法在短时间和/或长期场景上失败。我们介绍了A/I-Glass：基于激活和影响力的全局 - 本地神经重要性聚集，以促进网络稀疏，这两种无训练的方法，使用迅速局部和模型 - 内部和模型全球神经元统计数据的等级聚集动态选择FFN单元。多个LLM和基准的经验结果表明，玻璃在不依赖辅助预测因子或添加任何推理开销的情况下，玻璃的表现明显优于先前的无训练方法，尤其是在挑战长期生成场景的情况下。</li>
</ul>

<h3>Title: Your Reward Function for RL is Your Best PRM for Search: Unifying RL and Search-Based TTS</h3>
<ul>
<li><strong>Authors: </strong>Can Jin, Yang Zhou, Qixin Zhang, Hongwu Peng, Di Zhang, Marco Pavone, Ligong Han, Zhang-Wei Hong, Tong Che, Dimitris N. Metaxas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14313">https://arxiv.org/abs/2508.14313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14313">https://arxiv.org/pdf/2508.14313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14313]] Your Reward Function for RL is Your Best PRM for Search: Unifying RL and Search-Based TTS(https://arxiv.org/abs/2508.14313)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Test-time scaling (TTS) for large language models (LLMs) has thus far fallen into two largely separate paradigms: (1) reinforcement learning (RL) methods that optimize sparse outcome-based rewards, yet suffer from instability and low sample efficiency; and (2) search-based techniques guided by independently trained, static process reward models (PRMs), which require expensive human- or LLM-generated labels and often degrade under distribution shifts. In this paper, we introduce AIRL-S, the first natural unification of RL-based and search-based TTS. Central to AIRL-S is the insight that the reward function learned during RL training inherently represents the ideal PRM for guiding downstream search. Specifically, we leverage adversarial inverse reinforcement learning (AIRL) combined with group relative policy optimization (GRPO) to learn a dense, dynamic PRM directly from correct reasoning traces, entirely eliminating the need for labeled intermediate process data. At inference, the resulting PRM simultaneously serves as the critic for RL rollouts and as a heuristic to effectively guide search procedures, facilitating robust reasoning chain extension, mitigating reward hacking, and enhancing cross-task generalization. Experimental results across eight benchmarks, including mathematics, scientific reasoning, and code generation, demonstrate that our unified approach improves performance by 9 % on average over the base model, matching GPT-4o. Furthermore, when integrated into multiple search algorithms, our PRM consistently outperforms all baseline PRMs trained with labeled data. These results underscore that, indeed, your reward function for RL is your best PRM for search, providing a robust and cost-effective solution to complex reasoning tasks in LLMs.</li>
<li><strong>摘要：</strong>到目前为止，大型语言模型（LLMS）的测试时间缩放（TTS）已陷入两个很大程度上独立的范式：（1）增强学习（RL）方法，可优化基于结果的奖励，但遭受不稳定和较低样本效率的影响； （2）以独立训练的静态过程奖励模型（PRM）为指导的基于搜索的技术，这些技术需要昂贵的人类或LLM生成的标签，并且经常在分配变化下退化。在本文中，我们介绍了Airl-S，这是第一个基于RL和基于搜索的TT的自然统一。 Airl-S的核心是，在RL培训期间学习的奖励功能本质上代表了指导下游搜索的理想PRM。具体而言，我们利用对抗性逆增强学习（AIRL）与小组相对策略优化（GRPO）结合使用，直接从正确的推理轨迹中学习了密集的，动态的PRM，完全消除了对标记的中间过程数据的需求。在推断时，由此产生的PRM同时充当RL推出的批评家，并作为有效指导搜索程序的启发式方法，促进了稳健的推理链扩展，减轻奖励黑客入侵以及增强交叉任务的概括。包括数学，科学推理和代码生成在内的八个基准测试的实验结果表明，我们的统一方法在基本模型中平均提高了9％的性能，与GPT-4O相匹配。此外，当集成到多个搜索算法中时，我们的PRM始终优于所有接受标记数据的基线PRM。这些结果强调了您的RL的奖励功能确实是您搜索的最佳PRM，为LLMS中的复杂推理任务提供了强大且具有成本效益的解决方案。</li>
</ul>

<h3>Title: MoVieDrive: Multi-Modal Multi-View Urban Scene Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Guile Wu, David Huang, Dongfeng Bai, Bingbing Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14327">https://arxiv.org/abs/2508.14327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14327">https://arxiv.org/pdf/2508.14327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14327]] MoVieDrive: Multi-Modal Multi-View Urban Scene Video Generation(https://arxiv.org/abs/2508.14327)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Video generation has recently shown superiority in urban scene synthesis for autonomous driving. Existing video generation approaches to autonomous driving primarily focus on RGB video generation and lack the ability to support multi-modal video generation. However, multi-modal data, such as depth maps and semantic maps, are crucial for holistic urban scene understanding in autonomous driving. Although it is feasible to use multiple models to generate different modalities, this increases the difficulty of model deployment and does not leverage complementary cues for multi-modal data generation. To address this problem, in this work, we propose a novel multi-modal multi-view video generation approach to autonomous driving. Specifically, we construct a unified diffusion transformer model composed of modal-shared components and modal-specific components. Then, we leverage diverse conditioning inputs to encode controllable scene structure and content cues into the unified diffusion model for multi-modal multi-view video generation. In this way, our approach is capable of generating multi-modal multi-view driving scene videos in a unified framework. Our experiments on the challenging real-world autonomous driving dataset, nuScenes, show that our approach can generate multi-modal multi-view urban scene videos with high fidelity and controllability, surpassing the state-of-the-art methods.</li>
<li><strong>摘要：</strong>视频生成最近显示出在自动驾驶中的城市场景合成中的优势。自动驾驶的现有视频生成方法主要关注RGB视频生成，并且缺乏支持多模式视频生成的能力。但是，多模式数据（例如深度图和语义图）对于自动驾驶中的整体城市场景理解至关重要。尽管使用多个模型生成不同的模式是可行的，但这增加了模型部署的困难，并且不利用多模式数据生成的互补提示。为了解决这个问题，在这项工作中，我们提出了一种新型的多模式多视频视频生成方法来自动驾驶。具体而言，我们构建了一个由模态共享组件和模态特异性组成的统一扩散变压器模型。然后，我们利用各种条件输入将可控场景结构和内容提示编码为多模式多视频视频生成的统一扩散模型。通过这种方式，我们的方法能够在统一框架中生成多模式的多模式驾驶场景视频。我们对具有挑战性的现实自主驾驶数据集Nuscenes的实验表明，我们的方法可以生成具有高忠诚度和可控性的多模式多视图城市场景视频，从而超过了最先进的方法。</li>
</ul>

<h3>Title: Generative AI Against Poaching: Latent Composite Flow Matching for Wildlife Conservation</h3>
<ul>
<li><strong>Authors: </strong>Lingkai Kong, Haichuan Wang, Charles A. Emogor, Vincent Börsch-Supan, Lily Xu, Milind Tambe</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14342">https://arxiv.org/abs/2508.14342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14342">https://arxiv.org/pdf/2508.14342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14342]] Generative AI Against Poaching: Latent Composite Flow Matching for Wildlife Conservation(https://arxiv.org/abs/2508.14342)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Poaching poses significant threats to wildlife and biodiversity. A valuable step in reducing poaching is to forecast poacher behavior, which can inform patrol planning and other conservation interventions. Existing poaching prediction methods based on linear models or decision trees lack the expressivity to capture complex, nonlinear spatiotemporal patterns. Recent advances in generative modeling, particularly flow matching, offer a more flexible alternative. However, training such models on real-world poaching data faces two central obstacles: imperfect detection of poaching events and limited data. To address imperfect detection, we integrate flow matching with an occupancy-based detection model and train the flow in latent space to infer the underlying occupancy state. To mitigate data scarcity, we adopt a composite flow initialized from a linear-model prediction rather than random noise which is the standard in diffusion models, injecting prior knowledge and improving generalization. Evaluations on datasets from two national parks in Uganda show consistent gains in predictive accuracy.</li>
<li><strong>摘要：</strong>偷猎对野生动植物和生物多样性构成了重大威胁。减少偷猎的宝贵步骤是预测偷猎行为，这可以为巡逻计划和其他保护干预提供信息。基于线性模型或决策树的现有偷猎预测方法缺乏捕获复杂的非线性时空模式的表现力。生成建模的最新进展，尤其是流量匹配，提供了更灵活的替代方案。但是，关于现实世界中的偷猎数据的培训模型面临两个核心障碍：偷猎事件的不完美检测和有限的数据。为了解决不完美的检测，我们将流量匹配与基于占用率的检测模型集成在一起，并在潜在空间中训练流量以推断基础的占用状态。为了减轻数据稀缺性，我们采用了从线性模型预测初始化的复合流，而不是随机噪声，而不是随机噪声，这是扩散模型中的标准，注入了先验知识并改善概括。来自乌干达两个国家公园的数据集的评估表明，预测准确性始终如一。</li>
</ul>

<h3>Title: HandCraft: Dynamic Sign Generation for Synthetic Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Gaston Gustavo Rios</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14345">https://arxiv.org/abs/2508.14345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14345">https://arxiv.org/pdf/2508.14345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14345]] HandCraft: Dynamic Sign Generation for Synthetic Data Augmentation(https://arxiv.org/abs/2508.14345)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Sign Language Recognition (SLR) models face significant performance limitations due to insufficient training data availability. In this article, we address the challenge of limited data in SLR by introducing a novel and lightweight sign generation model based on CMLPe. This model, coupled with a synthetic data pretraining approach, consistently improves recognition accuracy, establishing new state-of-the-art results for the LSFB and DiSPLaY datasets using our Mamba-SL and Transformer-SL classifiers. Our findings reveal that synthetic data pretraining outperforms traditional augmentation methods in some cases and yields complementary benefits when implemented alongside them. Our approach democratizes sign generation and synthetic data pretraining for SLR by providing computationally efficient methods that achieve significant performance improvements across diverse datasets.</li>
<li><strong>摘要：</strong>由于培训数据的可用性不足，手语识别（SLR）模型面临重大的性能限制。在本文中，我们通过引入基于CMLPE的新颖且轻巧的标志生成模型来应对SLR中有限数据的挑战。该模型，再加上合成数据预处理方法，始终提高识别精度，并使用我们的MAMBA-SL和Transferner-SL分类器为LSFB和显示数据集建立新的最新结果。我们的研究结果表明，在某些情况下，预一试的合成数据在某些情况下优于传统的增强方法，并在与之一起实施时会产生互补的好处。我们的方法通过提供具有计算有效的方法来实现各种数据集的绩效改进，使SLR的标志生成和合成数据对SLR进行了民主化。</li>
</ul>

<h3>Title: A Non-Asymptotic Convergent Analysis for Scored-Based Graph Generative Model via a System of Stochastic Differential Equations</h3>
<ul>
<li><strong>Authors: </strong>Junwei Su, Chuan Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14351">https://arxiv.org/abs/2508.14351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14351">https://arxiv.org/pdf/2508.14351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14351]] A Non-Asymptotic Convergent Analysis for Scored-Based Graph Generative Model via a System of Stochastic Differential Equations(https://arxiv.org/abs/2508.14351)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Score-based graph generative models (SGGMs) have proven effective in critical applications such as drug discovery and protein synthesis. However, their theoretical behavior, particularly regarding convergence, remains underexplored. Unlike common score-based generative models (SGMs), which are governed by a single stochastic differential equation (SDE), SGGMs involve a system of coupled SDEs. In SGGMs, the graph structure and node features are governed by separate but interdependent SDEs. This distinction makes existing convergence analyses from SGMs inapplicable for SGGMs. In this work, we present the first non-asymptotic convergence analysis for SGGMs, focusing on the convergence bound (the risk of generative error) across three key graph generation paradigms: (1) feature generation with a fixed graph structure, (2) graph structure generation with fixed node features, and (3) joint generation of both graph structure and node features. Our analysis reveals several unique factors specific to SGGMs (e.g., the topological properties of the graph structure) which affect the convergence bound. Additionally, we offer theoretical insights into the selection of hyperparameters (e.g., sampling steps and diffusion length) and advocate for techniques like normalization to improve convergence. To validate our theoretical findings, we conduct a controlled empirical study using synthetic graph models, and the results align with our theoretical predictions. This work deepens the theoretical understanding of SGGMs, demonstrates their applicability in critical domains, and provides practical guidance for designing effective models.</li>
<li><strong>摘要：</strong>基于得分的图生成模型（SGGM）已被证明在关键应用中有效，例如药物发现和蛋白质合成。但是，他们的理论行为，尤其是关于收敛性的行为，仍然没有得到充实的态度。与由单个随机微分方程（SDE）控制的基于共同分数的生成模型（SGM）不同，SGGM涉及耦合SDE系统。在SGGM中，图结构和节点特征由单独但相互依赖的SDE控制。这种区别使SGGM不适用的SGMS的现有收敛分析。在这项工作中，我们介绍了SGGMS的第一个非反应收敛分析，重点介绍了三个关键图生成范式的收敛结合（生成误差的风险）：（1）具有固定图结构的特征生成，（2）具有固定节点特征的图形结构生成，以及（3）图形结构和node特征的关节产生。我们的分析揭示了影响收敛结合的几个特定于SGGM的独特因素（例如，图结构的拓扑特性）。此外，我们为选择超参数（例如，采样步骤和扩散长度）的选择提供了理论见解，并倡导诸如归一化之类的技术以改善收敛性。为了验证我们的理论发现，我们使用合成图模型进行了受控的经验研究，结果与我们的理论预测一致。这项工作加深了对SGGM的理论理解，展示了它们在关键领域的适用性，并为设计有效模型提供了实用的指导。</li>
</ul>

<h3>Title: SBGD: Improving Graph Diffusion Generative Model via Stochastic Block Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Junwei Su, Shan Wu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14352">https://arxiv.org/abs/2508.14352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14352">https://arxiv.org/pdf/2508.14352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14352]] SBGD: Improving Graph Diffusion Generative Model via Stochastic Block Diffusion(https://arxiv.org/abs/2508.14352)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Graph diffusion generative models (GDGMs) have emerged as powerful tools for generating high-quality graphs. However, their broader adoption faces challenges in \emph{scalability and size generalization}. GDGMs struggle to scale to large graphs due to their high memory requirements, as they typically operate in the full graph space, requiring the entire graph to be stored in memory during training and inference. This constraint limits their feasibility for large-scale real-world graphs. GDGMs also exhibit poor size generalization, with limited ability to generate graphs of sizes different from those in the training data, restricting their adaptability across diverse applications. To address these challenges, we propose the stochastic block graph diffusion (SBGD) model, which refines graph representations into a block graph space. This space incorporates structural priors based on real-world graph patterns, significantly reducing memory complexity and enabling scalability to large graphs. The block representation also improves size generalization by capturing fundamental graph structures. Empirical results show that SBGD achieves significant memory improvements (up to 6$\times$) while maintaining comparable or even superior graph generation performance relative to state-of-the-art methods. Furthermore, experiments demonstrate that SBGD better generalizes to unseen graph sizes. The significance of SBGD extends beyond being a scalable and effective GDGM; it also exemplifies the principle of modularization in generative modeling, offering a new avenue for exploring generative models by decomposing complex tasks into more manageable components.</li>
<li><strong>摘要：</strong>图扩散生成模型（GDGM）已成为生成高质量图的强大工具。但是，他们更广泛的采用面临\ emph {可伸缩性和大小概括}的挑战。 GDGM由于其高内存需求而难以扩展到大图，因为它们通常在完整的图形空间中运行，因此需要在训练和推理期间将整个图存储在内存中。这种约束限制了它们对大型现实图形图的可行性。 GDGM也表现出较差的概括，其生成大小的图表的能力有限，与培训数据中的图形不同，从而限制了它们在不同应用程序中的适应性。为了应对这些挑战，我们提出了随机块图扩散（SBGD）模型，该模型将图形表示分为块图形空间。该空间结合了基于现实世界图模式的结构先验，可显着降低记忆复杂性，并使可伸缩性可扩展到大图。块表示还通过捕获基本图结构来改善大小的概括。经验结果表明，SBGD可实现显着的内存改进（高达6 $ \ times $），同时相对于最新方法，保持了可比甚至优越的生成性能。此外，实验表明SBGD更好地概括了看不见的图形大小。 SBGD的意义不仅仅是可扩展有效的GDGM；它还展示了生成建模中模块化的原理，从而通过将复杂的任务分解为更易于管理的组件来探索生成模型的新途径。</li>
</ul>

<h3>Title: Taming Transformer for Emotion-Controllable Talking Face Generation</h3>
<ul>
<li><strong>Authors: </strong>Ziqi Zhang, Cheng Deng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14359">https://arxiv.org/abs/2508.14359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14359">https://arxiv.org/pdf/2508.14359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14359]] Taming Transformer for Emotion-Controllable Talking Face Generation(https://arxiv.org/abs/2508.14359)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Talking face generation is a novel and challenging generation task, aiming at synthesizing a vivid speaking-face video given a specific audio. To fulfill emotion-controllable talking face generation, current methods need to overcome two challenges: One is how to effectively model the multimodal relationship related to the specific emotion, and the other is how to leverage this relationship to synthesize identity preserving emotional videos. In this paper, we propose a novel method to tackle the emotion-controllable talking face generation task discretely. Specifically, we employ two pre-training strategies to disentangle audio into independent components and quantize videos into combinations of visual tokens. Subsequently, we propose the emotion-anchor (EA) representation that integrates the emotional information into visual tokens. Finally, we introduce an autoregressive transformer to model the global distribution of the visual tokens under the given conditions and further predict the index sequence for synthesizing the manipulated videos. We conduct experiments on the MEAD dataset that controls the emotion of videos conditioned on multiple emotional audios. Extensive experiments demonstrate the superiorities of our method both qualitatively and quantitatively.</li>
<li><strong>摘要：</strong>说话的面部发电是一项新颖而又具有挑战性的一代任务，旨在综合一个生动的口语视频，给出了特定的音频。为了实现可控制的说话面孔的产生，当前的方法需要克服两个挑战：一个是如何有效地对与特定情感相关的多模式关系进行建模，而另一种是如何利用这种关系来综合保存情感视频的身份。在本文中，我们提出了一种新颖的方法，可以离散地解决情绪控制的谈话。具体而言，我们采用两种预训练策略将音频分解为独立的组件，并将视频量化为视觉令牌的组合。随后，我们提出了将情感信息整合到视觉令牌中的情感锚定（EA）表示。最后，我们引入了一个自回旋变压器，以模拟在给定条件下视觉令牌的全局分布，并进一步预测合成操纵视频的索引序列。我们在米德数据集上进行实验，以控制以多个情感音频为条件的视频的情感。广泛的实验证明了我们方法的优越性，既有定性和定量。</li>
</ul>

<h3>Title: Img2ST-Net: Efficient High-Resolution Spatial Omics Prediction from Whole Slide Histology Images via Fully Convolutional Image-to-Image Learning</h3>
<ul>
<li><strong>Authors: </strong>Junchao Zhu, Ruining Deng, Junlin Guo, Tianyuan Yao, Juming Xiong, Chongyu Qu, Mengmeng Yin, Yu Wang, Shilin Zhao, Haichun Yang, Daguang Xu, Yucheng Tang, Yuankai Huo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14393">https://arxiv.org/abs/2508.14393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14393">https://arxiv.org/pdf/2508.14393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14393]] Img2ST-Net: Efficient High-Resolution Spatial Omics Prediction from Whole Slide Histology Images via Fully Convolutional Image-to-Image Learning(https://arxiv.org/abs/2508.14393)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in multi-modal AI have demonstrated promising potential for generating the currently expensive spatial transcriptomics (ST) data directly from routine histology images, offering a means to reduce the high cost and time-intensive nature of ST data acquisition. However, the increasing resolution of ST, particularly with platforms such as Visium HD achieving 8um or finer, introduces significant computational and modeling challenges. Conventional spot-by-spot sequential regression frameworks become inefficient and unstable at this scale, while the inherent extreme sparsity and low expression levels of high-resolution ST further complicate both prediction and evaluation. To address these limitations, we propose Img2ST-Net, a novel histology-to-ST generation framework for efficient and parallel high-resolution ST prediction. Unlike conventional spot-by-spot inference methods, Img2ST-Net employs a fully convolutional architecture to generate dense, HD gene expression maps in a parallelized manner. By modeling HD ST data as super-pixel representations, the task is reformulated from image-to-omics inference into a super-content image generation problem with hundreds or thousands of output channels. This design not only improves computational efficiency but also better preserves the spatial organization intrinsic to spatial omics data. To enhance robustness under sparse expression patterns, we further introduce SSIM-ST, a structural-similarity-based evaluation metric tailored for high-resolution ST analysis. We present a scalable, biologically coherent framework for high-resolution ST prediction. Img2ST-Net offers a principled solution for efficient and accurate ST inference at scale. Our contributions lay the groundwork for next-generation ST modeling that is robust and resolution-aware. The source code has been made publicly available at this https URL.</li>
<li><strong>摘要：</strong>多模式AI的最新进展显示出直接从常规组织学图像中生成当前昂贵的空间转录组学（ST）数据的潜力，提供了一种减少ST数据获取的高成本和时间密集型性质的手段。但是，ST的分辨率越来越大，尤其是在诸如HD诸如达到8UM或更精细的平台之类的平台上引入了重大的计算和建模挑战。在此规模上，常规的逐点顺序回归框架在此规模上变得效率低下且不稳定，而固有的极端稀疏性和高分辨率ST的低表达水平进一步使预测和评估更加复杂。为了解决这些局限性，我们提出了IMG2ST-NET，这是一个新型的组织学到ST生成框架，以实现高效且平行的高分辨率ST预测。与传统的逐个点推理方法不同，IMG2st-NET采用完全卷积的结构来以并行的方式生成密集的HD基因表达图。通过将高清ST数据建模为超级像素表示，从图像到词素推断将任务重新构建为具有数百或数千个输出渠道的超及图像生成问题。这种设计不仅提高了计算效率，而且更好地保留了空间轨迹数据固有的空间组织。为了增强稀疏表达模式下的鲁棒性，我们进一步引入了SSIM-ST，这是一种基于结构相似的评估度量，用于高分辨率ST分析。我们为高分辨率ST预测提供了一个可扩展的，生物学上的连贯框架。 IMG2st-NET提供了一种原则性的解决方案，可在大规模上进行有效，准确的ST推理。我们的贡献为下一代ST建模奠定了基础，该建模具有强大的解决方案。源代码已在此HTTPS URL上公开可用。</li>
</ul>

<h3>Title: CTA-Flux: Integrating Chinese Cultural Semantics into High-Quality English Text-to-Image Communities</h3>
<ul>
<li><strong>Authors: </strong>Yue Gong, Shanyuan Liu, Liuzhuozheng Li, Jian Zhu, Bo Cheng, Liebucha Wu, Xiaoyu Wu, Yuhang Ma, Dawei Leng, Yuhui Yin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14405">https://arxiv.org/abs/2508.14405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14405">https://arxiv.org/pdf/2508.14405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14405]] CTA-Flux: Integrating Chinese Cultural Semantics into High-Quality English Text-to-Image Communities(https://arxiv.org/abs/2508.14405)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We proposed the Chinese Text Adapter-Flux (CTA-Flux). An adaptation method fits the Chinese text inputs to Flux, a powerful text-to-image (TTI) generative model initially trained on the English corpus. Despite the notable image generation ability conditioned on English text inputs, Flux performs poorly when processing non-English prompts, particularly due to linguistic and cultural biases inherent in predominantly English-centric training datasets. Existing approaches, such as translating non-English prompts into English or finetuning models for bilingual mappings, inadequately address culturally specific semantics, compromising image authenticity and quality. To address this issue, we introduce a novel method to bridge Chinese semantic understanding with compatibility in English-centric TTI model communities. Existing approaches relying on ControlNet-like architectures typically require a massive parameter scale and lack direct control over Chinese semantics. In comparison, CTA-flux leverages MultiModal Diffusion Transformer (MMDiT) to control the Flux backbone directly, significantly reducing the number of parameters while enhancing the model's understanding of Chinese semantics. This integration significantly improves the generation quality and cultural authenticity without extensive retraining of the entire model, thus maintaining compatibility with existing text-to-image plugins such as LoRA, IP-Adapter, and ControlNet. Empirical evaluations demonstrate that CTA-flux supports Chinese and English prompts and achieves superior image generation quality, visual realism, and faithful depiction of Chinese semantics.</li>
<li><strong>摘要：</strong>我们提出了中文文本适配器 - 频率（CTA-FLUX）。一种适应方法将中文文本输入拟合到Flux，这是一种最初在英语语料库上训练的强大文本对象（TTI）生成模型。尽管以英语文本输入为条件，但在处理非英语提示时的表现却很差，尤其是由于语言和文化偏见为主要以英语为中心的培训数据集，效果较差。现有的方法，例如将非英语提示转化为英语或双语映射模型，不足以解决文化特定的语义，损害图像的真实性和质量。为了解决这个问题，我们介绍了一种新颖的方法，可以在以英语为中心的TTI模型社区中桥接中国语义理解。依靠类似控制网的架构的现有方法通常需要大量的参数量表，并且缺乏对中国语义的直接控制。相比之下，CTA频率利用多模式扩散变压器（MMDIT）直接控制通量骨架，从而大大减少了参数的数量，同时增强了模型对中国语义的理解。这种集成显着提高了生成质量和文化真实性，而无需大量的整个模型培训，从而保持了与现有的文本到图像插件的兼容性，例如LORA，IP-ADAPTER和CONTROLNET。经验评估表明，CTA-Flux支持中文和英语提示，并实现了卓越的图像产生质量，视觉现实主义以及对中国语义的忠实描述。</li>
</ul>

<h3>Title: Disentanglement in T-space for Faster and Distributed Training of Diffusion Models with Fewer Latent-states</h3>
<ul>
<li><strong>Authors: </strong>Samarth Gupta, Raghudeep Gadde, Rui Chen, Aleix M. Martinez</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14413">https://arxiv.org/abs/2508.14413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14413">https://arxiv.org/pdf/2508.14413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14413]] Disentanglement in T-space for Faster and Distributed Training of Diffusion Models with Fewer Latent-states(https://arxiv.org/abs/2508.14413)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We challenge a fundamental assumption of diffusion models, namely, that a large number of latent-states or time-steps is required for training so that the reverse generative process is close to a Gaussian. We first show that with careful selection of a noise schedule, diffusion models trained over a small number of latent states (i.e. $T \sim 32$) match the performance of models trained over a much large number of latent states ($T \sim 1,000$). Second, we push this limit (on the minimum number of latent states required) to a single latent-state, which we refer to as complete disentanglement in T-space. We show that high quality samples can be easily generated by the disentangled model obtained by combining several independently trained single latent-state models. We provide extensive experiments to show that the proposed disentangled model provides 4-6$\times$ faster convergence measured across a variety of metrics on two different datasets.</li>
<li><strong>摘要：</strong>我们挑战了扩散模型的基本假设，即，训练需要大量的潜在状态或时间步骤，以便反向生成过程接近高斯。我们首先表明，通过仔细选择噪声时间表，通过少数潜在状态（即$ t \ sim 32 $）训练的扩散模型与在许多潜在状态（$ t \ sim 1,000 $）上训练的型号的性能相匹配。其次，我们将此限制（以最小的潜在状态数）推向一个潜在状态，我们将其称为t空间中的完全分离。我们表明，通过组合几个独立训练的单一潜在型模型获得的分离模型可以轻松生成高质量的样品。我们提供了广泛的实验，以表明所提出的分离模型提供了4-6 $ \ times $ $更快的收敛速度，这些收敛性在两个不同数据集上的各种指标上都测得。</li>
</ul>

<h3>Title: MoCHA-former: Moiré-Conditioned Hybrid Adaptive Transformer for Video Demoiréing</h3>
<ul>
<li><strong>Authors: </strong>Jeahun Sung, Changhyun Roh, Chanho Eom, Jihyong Oh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14423">https://arxiv.org/abs/2508.14423</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14423">https://arxiv.org/pdf/2508.14423</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14423]] MoCHA-former: Moiré-Conditioned Hybrid Adaptive Transformer for Video Demoiréing(https://arxiv.org/abs/2508.14423)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Recent advances in portable imaging have made camera-based screen capture ubiquitous. Unfortunately, frequency aliasing between the camera's color filter array (CFA) and the display's sub-pixels induces moiré patterns that severely degrade captured photos and videos. Although various demoiréing models have been proposed to remove such moiré patterns, these approaches still suffer from several limitations: (i) spatially varying artifact strength within a frame, (ii) large-scale and globally spreading structures, (iii) channel-dependent statistics and (iv) rapid temporal fluctuations across frames. We address these issues with the Moiré Conditioned Hybrid Adaptive Transformer (MoCHA-former), which comprises two key components: Decoupled Moiré Adaptive Demoiréing (DMAD) and Spatio-Temporal Adaptive Demoiréing (STAD). DMAD separates moiré and content via a Moiré Decoupling Block (MDB) and a Detail Decoupling Block (DDB), then produces moiré-adaptive features using a Moiré Conditioning Block (MCB) for targeted restoration. STAD introduces a Spatial Fusion Block (SFB) with window attention to capture large-scale structures, and a Feature Channel Attention (FCA) to model channel dependence in RAW frames. To ensure temporal consistency, MoCHA-former performs implicit frame alignment without any explicit alignment module. We analyze moiré characteristics through qualitative and quantitative studies, and evaluate on two video datasets covering RAW and sRGB domains. MoCHA-former consistently surpasses prior methods across PSNR, SSIM, and LPIPS.</li>
<li><strong>摘要：</strong>便携式成像的最新进展使基于相机的屏幕捕获无处不在。不幸的是，相机的颜色滤清器阵列（CFA）和显示器的子像素之间的频率混叠会引起Moiré模式，从而严重降低了捕获的照片和视频。尽管已经提出了各种演示模型来删除此类Moiré模式，但这些方法仍然遭受了几个局限性：（i）在框架内的空间变化的伪影强度，（ii）大规模和全球扩散的结构，（iii）通道依赖性统计量和（iv）（iv）（iv）（iv）跨框架之间的快速时间波动。我们通过Moiré条件的混合自适应变压器（Mocha-Former）解决了这些问题，该变压器包括两个关键组成部分：脱钩的Moiré自适应演示（DMAD）和时空适应性演示表（Stad）。 DMAD通过Moiré脱钩块（MDB）和细节解耦块（DDB）分开Moiré和内容，然后使用Moiré条件块（MCB）产生Moiré-Aptaptive特征，以实现目标修复。 Stad引入了一个空间融合块（SFB），并注意窗口以捕获大规模结构，并引入了特征通道注意（FCA），以模拟原始帧中的频道依赖性。为了确保时间一致性，摩卡摩卡形式执行隐式框架对齐，而无需任何显式比对模块。我们通过定性和定量研究来分析Moiré特征，并在涵盖RAW和SRGB域的两个视频数据集上进行评估。摩卡咖啡形成者始终超过PSNR，SSIM和LPIP的先前方法。</li>
</ul>

<h3>Title: MUSE: Multi-Subject Unified Synthesis via Explicit Layout Semantic Expansion</h3>
<ul>
<li><strong>Authors: </strong>Fei Peng, Junqiang Wu, Yan Li, Tingting Gao, Di Zhang, Huiyuan Fu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14440">https://arxiv.org/abs/2508.14440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14440">https://arxiv.org/pdf/2508.14440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14440]] MUSE: Multi-Subject Unified Synthesis via Explicit Layout Semantic Expansion(https://arxiv.org/abs/2508.14440)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Existing text-to-image diffusion models have demonstrated remarkable capabilities in generating high-quality images guided by textual prompts. However, achieving multi-subject compositional synthesis with precise spatial control remains a significant challenge. In this work, we address the task of layout-controllable multi-subject synthesis (LMS), which requires both faithful reconstruction of reference subjects and their accurate placement in specified regions within a unified image. While recent advancements have separately improved layout control and subject synthesis, existing approaches struggle to simultaneously satisfy the dual requirements of spatial precision and identity preservation in this composite task. To bridge this gap, we propose MUSE, a unified synthesis framework that employs concatenated cross-attention (CCA) to seamlessly integrate layout specifications with textual guidance through explicit semantic space expansion. The proposed CCA mechanism enables bidirectional modality alignment between spatial constraints and textual descriptions without interference. Furthermore, we design a progressive two-stage training strategy that decomposes the LMS task into learnable sub-objectives for effective optimization. Extensive experiments demonstrate that MUSE achieves zero-shot end-to-end generation with superior spatial accuracy and identity consistency compared to existing solutions, advancing the frontier of controllable image synthesis. Our code and model are available at this https URL.</li>
<li><strong>摘要：</strong>现有的文本对图像扩散模型表现出在产生以文本提示为指导的高质量图像中的显着功能。但是，通过精确的空间控制实现多主体组成合成仍然是一个重大挑战。在这项工作中，我们解决了可控制的多主体合成（LMS）的任务，该任务既需要忠实地重建参考主体及其在统一图像中指定区域中的准确位置。尽管最近的进步已分别改善了布局控制和主题综合，但现有的方法很难同时满足此综合任务中空间精度和身份保存的双重要求。为了弥合这一差距，我们提出了Muse，这是一个统一的合成框架，该统一的综合框架采用串联的交叉注意（CCA），通过显式的语义空间扩展将布局规格与文本指导无缝整合。所提出的CCA机制使空间约束和文本描述之间的双向模态对准而无需干扰。此外，我们设计了一种渐进式的两阶段培训策略，将LMS任务分解为可学习的子目标以进行有效优化。广泛的实验表明，与现有解决方案相比，Muse具有较高的空间准确性和身份一致性的端到端零镜头，从而推进了可控图像合成的前沿。我们的代码和模型可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: DreamSwapV: Mask-guided Subject Swapping for Any Customized Video Editing</h3>
<ul>
<li><strong>Authors: </strong>Weitao Wang, Zichen Wang, Hongdeng Shen, Yulei Lu, Xirui Fan, Suhui Wu, Jun Zhang, Haoqian Wang, Hao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14465">https://arxiv.org/abs/2508.14465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14465">https://arxiv.org/pdf/2508.14465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14465]] DreamSwapV: Mask-guided Subject Swapping for Any Customized Video Editing(https://arxiv.org/abs/2508.14465)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>With the rapid progress of video generation, demand for customized video editing is surging, where subject swapping constitutes a key component yet remains under-explored. Prevailing swapping approaches either specialize in narrow domains--such as human-body animation or hand-object interaction--or rely on some indirect editing paradigm or ambiguous text prompts that compromise final fidelity. In this paper, we propose DreamSwapV, a mask-guided, subject-agnostic, end-to-end framework that swaps any subject in any video for customization with a user-specified mask and reference image. To inject fine-grained guidance, we introduce multiple conditions and a dedicated condition fusion module that integrates them efficiently. In addition, an adaptive mask strategy is designed to accommodate subjects of varying scales and attributes, further improving interactions between the swapped subject and its surrounding context. Through our elaborate two-phase dataset construction and training scheme, our DreamSwapV outperforms existing methods, as validated by comprehensive experiments on VBench indicators and our first introduced DreamSwapV-Benchmark.</li>
<li><strong>摘要：</strong>随着视频生成的快速进展，对定制视频编辑的需求正在激增，主题交换构成关键组成部分，但仍未探索。盛行的交换方法要么专门研究狭窄的领域（例如人体动画或手动相互作用），要么依赖某些间接编辑范式或模棱两可的文本提示，这会损害最终的保真度。在本文中，我们提出了DreamSwapv，这是一种面具引导的，主题的端到端框架，该框架将任何视频中的任何主题与用户指定的掩码和参考图像进行自定义。为了注入细粒度的指导，我们引入了多种条件和专用条件融合模块，可以有效地整合它们。此外，自适应面具策略旨在适应不同尺度和属性的主题，从而进一步改善了交换主体及其周围环境之间的相互作用。通过我们精心设计的两相数据集构建和培训计划，我们的DreamSwapv优于现有方法，这是通过在VBENCH指标上的全面实验和我们的第一个介绍的Dreamswapv基准测试的。</li>
</ul>

<h3>Title: On the notion of missingness for path attribution explainability methods in medical settings: Guiding the selection of medically meaningful baselines</h3>
<ul>
<li><strong>Authors: </strong>Alexander Geiger, Lars Wagner, Daniel Rueckert, Dirk Wilhelm, Alissa Jell</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14482">https://arxiv.org/abs/2508.14482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14482">https://arxiv.org/pdf/2508.14482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14482]] On the notion of missingness for path attribution explainability methods in medical settings: Guiding the selection of medically meaningful baselines(https://arxiv.org/abs/2508.14482)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The explainability of deep learning models remains a significant challenge, particularly in the medical domain where interpretable outputs are critical for clinical trust and transparency. Path attribution methods such as Integrated Gradients rely on a baseline input representing the absence of relevant features ("missingness"). Commonly used baselines, such as all-zero inputs, are often semantically meaningless, especially in medical contexts where missingness can itself be informative. While alternative baseline choices have been explored, existing methods lack a principled approach to dynamically select baselines tailored to each input. In this work, we examine the notion of missingness in the medical setting, analyze its implications for baseline selection, and introduce a counterfactual-guided approach to address the limitations of conventional baselines. We argue that a clinically normal but input-close counterfactual represents a more accurate representation of a meaningful absence of features in medical data. To implement this, we use a Variational Autoencoder to generate counterfactual baselines, though our concept is generative-model-agnostic and can be applied with any suitable counterfactual method. We evaluate the approach on three distinct medical data sets and empirically demonstrate that counterfactual baselines yield more faithful and medically relevant attributions compared to standard baseline choices.</li>
<li><strong>摘要：</strong>深度学习模型的解释性仍然是一个重大挑战，尤其是在医学领域，可解释的产出对于临床信任和透明度至关重要。路径归因方法（例如集成梯度）依赖于代表缺乏相关特征的基线输入（“缺失”）。常用的基线，例如全零输入，通常在语义上是毫无意义的，尤其是在遗失本身可以提供信息的医学环境中。尽管已经探索了替代基线选择，但现有方法缺乏一种原则性的方法来动态选择针对每个输入量身定制的基准。在这项工作中，我们研究了医学环境中缺失的概念，分析其对基线选择的影响，并引入反事实引导的方法来解决常规基线的局限性。我们认为，临床正常但输入的反事实表现更准确地表示医学数据中有意义的特征。为了实现这一点，我们使用差异自动编码器来生成反事实基线，尽管我们的概念是生成模型 - 敏捷的，并且可以使用任何合适的反事实方法应用。我们在三个不同的医学数据集上评估了该方法，并从经验上表明，与标准基线选择相比，反事实基线产生了更忠实和医学相关的归因。</li>
</ul>

<h3>Title: Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer for Photorealistic Video Restoration</h3>
<ul>
<li><strong>Authors: </strong>Haoran Bai, Xiaoxu Chen, Canqian Yang, Zongyao He, Sibin Deng, Ying Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14483">https://arxiv.org/abs/2508.14483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14483">https://arxiv.org/pdf/2508.14483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14483]] Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer for Photorealistic Video Restoration(https://arxiv.org/abs/2508.14483)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generation, generative</a></li>
<li><strong>Abstract: </strong>We present Vivid-VR, a DiT-based generative video restoration method built upon an advanced T2V foundation model, where ControlNet is leveraged to control the generation process, ensuring content consistency. However, conventional fine-tuning of such controllable pipelines frequently suffers from distribution drift due to limitations in imperfect multimodal alignment, resulting in compromised texture realism and temporal coherence. To tackle this challenge, we propose a concept distillation training strategy that utilizes the pretrained T2V model to synthesize training samples with embedded textual concepts, thereby distilling its conceptual understanding to preserve texture and temporal quality. To enhance generation controllability, we redesign the control architecture with two key components: 1) a control feature projector that filters degradation artifacts from input video latents to minimize their propagation through the generation pipeline, and 2) a new ControlNet connector employing a dual-branch design. This connector synergistically combines MLP-based feature mapping with cross-attention mechanism for dynamic control feature retrieval, enabling both content preservation and adaptive control signal modulation. Extensive experiments show that Vivid-VR performs favorably against existing approaches on both synthetic and real-world benchmarks, as well as AIGC videos, achieving impressive texture realism, visual vividness, and temporal consistency. The codes and checkpoints are publicly available at this https URL.</li>
<li><strong>摘要：</strong>我们提出了Vivid-VR，这是一种基于DIT的生成视频修复方法，建立在高级T2V基础模型上，该方法利用ControlNet来控制生成过程，从而确保内容一致性。但是，由于不完善的多模式比对的局限性，这种可控管道的常规微调通常会因分布漂移而受到分布漂移，从而导致质地的现实主义和时间连贯性受损。为了应对这一挑战，我们提出了一种概念蒸馏培训策略，该培训策略利用验证的T2V模型将培训样品与嵌入式文本概念合成训练样本，从而将其概念理解提取以保持质地和时间质量。为了增强发电可控性，我们使用两个关键组件重新设计了控制架构：1）控制功能投影仪，该投影仪过滤了从输入视频潜在的降级伪像，以最大程度地减少其通过生成管道的传播，2）使用新的ControlNET连接器采用双支流设计。该连接器协同结合了基于MLP的功能映射与交叉注意机制，以进行动态控制功能检索，从而实现了内容保存和自适应控制信号调制。广泛的实验表明，Vivid-VR对合成和现实基准的现有方法以及AIGC视频的现有方法表现出色，实现了令人印象深刻的纹理现实主义，视觉生动和时间的一致性。这些https URL公开可用代码和检查点。</li>
</ul>

<h3>Title: SATURN: Autoregressive Image Generation Guided by Scene Graphs</h3>
<ul>
<li><strong>Authors: </strong>Thanh-Nhan Vo, Trong-Thuan Nguyen, Tam V. Nguyen, Minh-Triet Tran</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14502">https://arxiv.org/abs/2508.14502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14502">https://arxiv.org/pdf/2508.14502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14502]] SATURN: Autoregressive Image Generation Guided by Scene Graphs(https://arxiv.org/abs/2508.14502)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>State-of-the-art text-to-image models excel at photorealistic rendering but often struggle to capture the layout and object relationships implied by complex prompts. Scene graphs provide a natural structural prior, yet previous graph-guided approaches have typically relied on heavy GAN or diffusion pipelines, which lag behind modern autoregressive architectures in both speed and fidelity. We introduce SATURN (Structured Arrangement of Triplets for Unified Rendering Networks), a lightweight extension to VAR-CLIP that translates a scene graph into a salience-ordered token sequence, enabling a frozen CLIP-VQ-VAE backbone to interpret graph structure while fine-tuning only the VAR transformer. On the Visual Genome dataset, SATURN reduces FID from 56.45% to 21.62% and increases the Inception Score from 16.03 to 24.78, outperforming prior methods such as SG2IM and SGDiff without requiring extra modules or multi-stage training. Qualitative results further confirm improvements in object count fidelity and spatial relation accuracy, showing that SATURN effectively combines structural awareness with state-of-the-art autoregressive fidelity.</li>
<li><strong>摘要：</strong>最先进的文本对图像模型在逼真的渲染上表现出色，但通常很难捕获复杂提示所隐含的布局和对象关系。场景图提供了自然的结构性先验，但以前的图形指导方法通常依赖于重gan或扩散管道，该管道以速度和忠诚度落后于现代自动回归体系结构。我们介绍了土星（用于统一渲染网络的三重态的结构化布置），这是一种轻巧的扩展，可将场景图转换为一个可显着的令牌序列，从而使冻结的夹子VQ-VAE主链可以解释图形结构，同时仅通过微调Var Transformer来解释图形结构。在视觉基因组数据集上，土星将FID从56.45％降低到21.62％，并将其成立分数从16.03降低到24.78，在不需要额外的模块或多阶段训练的情况下，胜于先前的方法，例如SG2IM和SGDI​​FF。定性结果进一步证实了对象计数保真度和空间关系准确性的改善，这表明土星有效地结合了结构性意识与最新的自回归忠诚度。</li>
</ul>

<h3>Title: Adversarial Generation and Collaborative Evolution of Safety-Critical Scenarios for Autonomous Vehicles</h3>
<ul>
<li><strong>Authors: </strong>Jiangfan Liu, Yongkang Guo, Fangzhi Zhong, Tianyuan Zhang, Zonglei Jing, Siyuan Liang, Jiakai Wang, Mingchuan Zhang, Aishan Liu, Xianglong Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14527">https://arxiv.org/abs/2508.14527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14527">https://arxiv.org/pdf/2508.14527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14527]] Adversarial Generation and Collaborative Evolution of Safety-Critical Scenarios for Autonomous Vehicles(https://arxiv.org/abs/2508.14527)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The generation of safety-critical scenarios in simulation has become increasingly crucial for safety evaluation in autonomous vehicles prior to road deployment in society. However, current approaches largely rely on predefined threat patterns or rule-based strategies, which limit their ability to expose diverse and unforeseen failure modes. To overcome these, we propose ScenGE, a framework that can generate plentiful safety-critical scenarios by reasoning novel adversarial cases and then amplifying them with complex traffic flows. Given a simple prompt of a benign scene, it first performs Meta-Scenario Generation, where a large language model, grounded in structured driving knowledge, infers an adversarial agent whose behavior poses a threat that is both plausible and deliberately challenging. This meta-scenario is then specified in executable code for precise in-simulator control. Subsequently, Complex Scenario Evolution uses background vehicles to amplify the core threat introduced by Meta-Scenario. It builds an adversarial collaborator graph to identify key agent trajectories for optimization. These perturbations are designed to simultaneously reduce the ego vehicle's maneuvering space and create critical occlusions. Extensive experiments conducted on multiple reinforcement learning based AV models show that ScenGE uncovers more severe collision cases (+31.96%) on average than SoTA baselines. Additionally, our ScenGE can be applied to large model based AV systems and deployed on different simulators; we further observe that adversarial training on our scenarios improves the model robustness. Finally, we validate our framework through real-world vehicle tests and human evaluation, confirming that the generated scenarios are both plausible and critical. We hope our paper can build up a critical step towards building public trust and ensuring their safe deployment.</li>
<li><strong>摘要：</strong>模拟中的安全至关重要方案的产生对于在社会路线部署之前对自动驾驶汽车的安全评估变得越来越重要。但是，当前的方法在很大程度上依赖于预定义的威胁模式或基于规则的策略，从而限制了它们揭示多样化和不可预见的故障模式的能力。为了克服这些问题，我们提出了Spening，该框架可以通过推理新颖的对抗案例，然后通过复杂的交通流量来产生大量安全至关重要的情况。考虑到一个良性场景的简单提示，它首先执行了元塞纳里奥一代，其中一个基于结构化驾驶知识的大语言模型渗透了一种对抗性剂，其行为构成了一种既合理又有意挑战的威胁。然后，在可执行的代码中指定了此元塞纳里奥，以精确的模拟器控制。随后，复杂的方案进化使用背景车辆来扩大元塞纳里奥引入的核心威胁。它构建了一个对抗性合作者图，以识别以进行优化的关键代理轨迹。这些扰动旨在同时减少自我车辆的操纵空间并产生严重的闭合。在基于多个加强学习的AV模型上进行的广泛实验表明，Spegenge揭示了比SOTA基准平均更严重的碰撞病例（+31.96％）。此外，我们的scenge可以应用于大型基于模型的AV系统并部署在不同的模拟器上。我们进一步观察到，对我们方案的对抗训练可以提高模型的鲁棒性。最后，我们通过现实世界中的车辆测试和人类评估来验证我们的框架，证实生成的场景既合理又至关重要。我们希望我们的论文能够建立建立公众信任并确保其安全部署的关键一步。</li>
</ul>

<h3>Title: Making Pose Representations More Expressive and Disentangled via Residual Vector Quantization</h3>
<ul>
<li><strong>Authors: </strong>Sukhyun Jeong, Hong-Gi Shin, Yong-Hoon Choi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14561">https://arxiv.org/abs/2508.14561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14561">https://arxiv.org/pdf/2508.14561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14561]] Making Pose Representations More Expressive and Disentangled via Residual Vector Quantization(https://arxiv.org/abs/2508.14561)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent progress in text-to-motion has advanced both 3D human motion generation and text-based motion control. Controllable motion generation (CoMo), which enables intuitive control, typically relies on pose code representations, but discrete pose codes alone cannot capture fine-grained motion details, limiting expressiveness. To overcome this, we propose a method that augments pose code-based latent representations with continuous motion features using residual vector quantization (RVQ). This design preserves the interpretability and manipulability of pose codes while effectively capturing subtle motion characteristics such as high-frequency details. Experiments on the HumanML3D dataset show that our model reduces Frechet inception distance (FID) from 0.041 to 0.015 and improves Top-1 R-Precision from 0.508 to 0.510. Qualitative analysis of pairwise direction similarity between pose codes further confirms the model's controllability for motion editing.</li>
<li><strong>摘要：</strong>文本到动作的最新进展既提高了3D人类运动产生和基于文本的运动控制。可控运动（COMO）可以实现直观的控制，通常依赖于姿势代码表示，但是单独的离散姿势代码无法捕获细颗粒的运动细节，从而限制了表现力。为了克服这一点，我们提出了一种使用残留向量量化（RVQ）的连续运动特征构成基于代码的潜在表示的方法。该设计保留了姿势代码的可解释性和可操作性，同时有效地捕获了诸如高频细节之类的微妙运动特征。 HumanML3D数据集的实验表明，我们的模型将特雷希特的距离（FID）从0.041降低到0.015，并将Top-1 R-Precision从0.508提高到0.510。对姿势代码之间成对方向相似性的定性分析进一步证实了该模型的运动编辑可控性。</li>
</ul>

<h3>Title: Controllable Latent Space Augmentation for Digital Pathology</h3>
<ul>
<li><strong>Authors: </strong>Sofiène Boutaj, Marin Scalbert, Pierre Marza, Florent Couzinie-Devy, Maria Vakalopoulou, Stergios Christodoulidis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14588">https://arxiv.org/abs/2508.14588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14588">https://arxiv.org/pdf/2508.14588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14588]] Controllable Latent Space Augmentation for Digital Pathology(https://arxiv.org/abs/2508.14588)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Whole slide image (WSI) analysis in digital pathology presents unique challenges due to the gigapixel resolution of WSIs and the scarcity of dense supervision signals. While Multiple Instance Learning (MIL) is a natural fit for slide-level tasks, training robust models requires large and diverse datasets. Even though image augmentation techniques could be utilized to increase data variability and reduce overfitting, implementing them effectively is not a trivial task. Traditional patch-level augmentation is prohibitively expensive due to the large number of patches extracted from each WSI, and existing feature-level augmentation methods lack control over transformation semantics. We introduce HistAug, a fast and efficient generative model for controllable augmentations in the latent space for digital pathology. By conditioning on explicit patch-level transformations (e.g., hue, erosion), HistAug generates realistic augmented embeddings while preserving initial semantic information. Our method allows the processing of a large number of patches in a single forward pass efficiently, while at the same time consistently improving MIL model performance. Experiments across multiple slide-level tasks and diverse organs show that HistAug outperforms existing methods, particularly in low-data regimes. Ablation studies confirm the benefits of learned transformations over noise-based perturbations and highlight the importance of uniform WSI-wise augmentation. Code is available at this https URL.</li>
<li><strong>摘要：</strong>数字病理学中的整个幻灯片图像（WSI）分析提出了由于WSIS的千兆像素分辨率和密集的监督信号的稀缺而提出了独特的挑战。虽然多个实例学习（MIL）是幻灯片级任务的自然合适，但训练强大的模型需要大型多样的数据集。即使可以利用图像增强技术来增加数据可变性并减少过度拟合，但有效地实施它们并不是一项琐碎的任务。由于从每个WSI中提取了大量的补丁，并且现有的功能级增强方法缺乏对转换语义的控制，因此传统的补丁级增强非常昂贵。我们介绍了Histaug，这是一种快速有效的生成模型，用于在数字病理的潜在空间中可控增强。通过对显式补丁级转换（例如，色调，侵蚀）进行调节，Histaug在保留初始语义信息的同时生成了逼真的增强嵌入。我们的方法允许有效地处理大量贴片，同时始终提高MIL模型性能。跨多个幻灯片级任务和各种器官的实验表明，历史表优于现有方法，尤其是在低数据制度中。消融研究证实了学习转化对基于噪声的扰动的好处，并突出了统一的WSI增强的重要性。代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Towards PerSense++: Advancing Training-Free Personalized Instance Segmentation in Dense Images</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Ibraheem Siddiqui, Muhammad Umer Sheikh, Hassan Abid, Kevin Henry, Muhammad Haris Khan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14660">https://arxiv.org/abs/2508.14660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14660">https://arxiv.org/pdf/2508.14660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14660]] Towards PerSense++: Advancing Training-Free Personalized Instance Segmentation in Dense Images(https://arxiv.org/abs/2508.14660)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Segmentation in dense visual scenes poses significant challenges due to occlusions, background clutter, and scale variations. To address this, we introduce PerSense, an end-to-end, training-free, and model-agnostic one-shot framework for Personalized instance Segmentation in dense images. PerSense employs a novel Instance Detection Module (IDM) that leverages density maps (DMs) to generate instance-level candidate point prompts, followed by a Point Prompt Selection Module (PPSM) that filters false positives via adaptive thresholding and spatial gating. A feedback mechanism further enhances segmentation by automatically selecting effective exemplars to improve DM quality. We additionally present PerSense++, an enhanced variant that incorporates three additional components to improve robustness in cluttered scenes: (i) a diversity-aware exemplar selection strategy that leverages feature and scale diversity for better DM generation; (ii) a hybrid IDM combining contour and peak-based prompt generation for improved instance separation within complex density patterns; and (iii) an Irrelevant Mask Rejection Module (IMRM) that discards spatially inconsistent masks using outlier analysis. Finally, to support this underexplored task, we introduce PerSense-D, a dedicated benchmark for personalized segmentation in dense images. Extensive experiments across multiple benchmarks demonstrate that PerSense++ outperforms existing methods in dense settings.</li>
<li><strong>摘要：</strong>密集的视觉场景中的细分构成了由于阻塞，背景混乱和比例变化而引起的重大挑战。为了解决这个问题，我们介绍了Persense，一种端到端，无训练和模型不合时宜的一击框架，用于密集图像中的个性化实例细分。 Persense采用了一个新颖的实例检测模块（IDM），该模块利用密度图（DMS）生成实例级候选点提示，然后进行点提示选择模块（PPSM），该模块（PPSM）通过自适应阈值和空间门控过滤误报。反馈机制通过自动选择有效的示例以提高DM质量，从而进一步增强了细分。我们还提出了Persense ++，这是一种增强的变体，它结合了三个其他组件，以提高混乱的场景：（i）一种多样性感知到的示例选择策略，其利用和扩展多样性具有更好的DM生成； （ii）混合IDM结合了轮廓和基于峰的及时生成，以改善复杂密度模式内的实例分离； （iii）使用异常值分析丢弃空间上不一致的掩模的无关面膜排斥模块（IMRM）。最后，为了支持这项不受欢迎的任务，我们介绍了Persense-D，这是一个专门的基准，用于密集图像中的个性化细分。跨多个基准测试的广泛实验表明，Persense ++在密集设置中的现有方法优于现有方法。</li>
</ul>

<h3>Title: GeMS: Efficient Gaussian Splatting for Extreme Motion Blur</h3>
<ul>
<li><strong>Authors: </strong>Gopi Raju Matta, Trisha Reddypalli, Vemunuri Divya Madhuri, Kaushik Mitra</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14682">https://arxiv.org/abs/2508.14682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14682">https://arxiv.org/pdf/2508.14682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14682]] GeMS: Efficient Gaussian Splatting for Extreme Motion Blur(https://arxiv.org/abs/2508.14682)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce GeMS, a framework for 3D Gaussian Splatting (3DGS) designed to handle severely motion-blurred images. State-of-the-art deblurring methods for extreme blur, such as ExBluRF, as well as Gaussian Splatting-based approaches like Deblur-GS, typically assume access to sharp images for camera pose estimation and point cloud generation, an unrealistic assumption. Methods relying on COLMAP initialization, such as BAD-Gaussians, also fail due to unreliable feature correspondences under severe blur. To address these challenges, we propose GeMS, a 3DGS framework that reconstructs scenes directly from extremely blurred images. GeMS integrates: (1) VGGSfM, a deep learning-based Structure-from-Motion pipeline that estimates poses and generates point clouds directly from blurred inputs; (2) 3DGS-MCMC, which enables robust scene initialization by treating Gaussians as samples from a probability distribution, eliminating heuristic densification and pruning; and (3) joint optimization of camera trajectories and Gaussian parameters for stable reconstruction. While this pipeline produces strong results, inaccuracies may remain when all inputs are severely blurred. To mitigate this, we propose GeMS-E, which integrates a progressive refinement step using events: (4) Event-based Double Integral (EDI) deblurring restores sharper images that are then fed into GeMS, improving pose estimation, point cloud generation, and overall reconstruction. Both GeMS and GeMS-E achieve state-of-the-art performance on synthetic and real-world datasets. To our knowledge, this is the first framework to address extreme motion blur within 3DGS directly from severely blurred inputs.</li>
<li><strong>摘要：</strong>我们介绍了宝石，这是一个用于处理严重运动毛图像的3D高斯裂（3DG）的框架。极端模糊的最先进的脱毛方法，例如Exblurf，以及基于高斯脱布的方法，例如DeBlur-GS，通常会假设访问夏普图像以进行相机姿势估计和点云生成，这是一个不现实的假设。依靠Colmap初始化（例如坏高斯人）的方法，由于严重的模糊下的特征对应关系，也失败了。为了应对这些挑战，我们提出了GEMS，这是一个3DGS框架，可直接从极其模糊的图像中重建场景。 GEMS集成：（1）VGGSFM，这是一种基于深度学习的结构，从估计并从模糊的输入中产生点云； （2）3DGS-MCMC，可以通过将高斯作为概率分布的样本来实现强大的场景初始化，从而消除了启发式致密和修剪； （3）稳定重建的摄像机轨迹和高斯参数的联合优化。尽管该管道会产生强大的结果，但是当所有输入都严重模糊时，可能仍然存在不准确性。为了减轻这种情况，我们提出了GEMS-E，它使用事件集成了渐进的完善步骤：（4）基于事件的双重积分（EDI）deblurring恢复了较清晰的图像，然后将其馈入宝石，改善姿势估计，点云的产生和整体重建。 GEMS和GEMS-E都在合成和现实世界数据集上实现了最先进的性能。据我们所知，这是直接从严重模糊的输入中解决3DG中极端运动模糊的第一个框架。</li>
</ul>

<h3>Title: GSFix3D: Diffusion-Guided Repair of Novel Views in Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Jiaxin Wei, Stefan Leutenegger, Simon Schaefer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14717">https://arxiv.org/abs/2508.14717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14717">https://arxiv.org/pdf/2508.14717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14717]] GSFix3D: Diffusion-Guided Repair of Novel Views in Gaussian Splatting(https://arxiv.org/abs/2508.14717)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent developments in 3D Gaussian Splatting have significantly enhanced novel view synthesis, yet generating high-quality renderings from extreme novel viewpoints or partially observed regions remains challenging. Meanwhile, diffusion models exhibit strong generative capabilities, but their reliance on text prompts and lack of awareness of specific scene information hinder accurate 3D reconstruction tasks. To address these limitations, we introduce GSFix3D, a novel framework that improves the visual fidelity in under-constrained regions by distilling prior knowledge from diffusion models into 3D representations, while preserving consistency with observed scene details. At its core is GSFixer, a latent diffusion model obtained via our customized fine-tuning protocol that can leverage both mesh and 3D Gaussians to adapt pretrained generative models to a variety of environments and artifact types from different reconstruction methods, enabling robust novel view repair for unseen camera poses. Moreover, we propose a random mask augmentation strategy that empowers GSFixer to plausibly inpaint missing regions. Experiments on challenging benchmarks demonstrate that our GSFix3D and GSFixer achieve state-of-the-art performance, requiring only minimal scene-specific fine-tuning on captured data. Real-world test further confirms its resilience to potential pose errors. Our code and data will be made publicly available. Project page: this https URL.</li>
<li><strong>摘要：</strong>3D高斯脱落的最新发展显着增强了新型视图的综合，但从极端的新观点或部分观察到的区域产生了高质量的效果，仍然具有挑战性。同时，扩散模型具有强大的生成能力，但是它们对文本提示的依赖以及对特定场景信息的认识不足，阻碍了准确的3D重建任务。为了解决这些局限性，我们介绍了GSFIX3D，这是一个新颖的框架，通过将扩散模型从扩散模型提取到3D表示，同时与观察到的场景细节保持一致性，从而改善了不受约束区域的视觉保真度。 GSfixer的核心是通过我们定制的微调协议获得的潜在扩散模型GSFIXER，可以利用网格和3D高斯人将预审预周化的生成模型适应各种环境和不同的重建方法，从而使强大的新型视图修复可用于未见的相机姿势。此外，我们提出了一种随机的掩模增强策略，该策略使GSFIXER能够合理地涂有漆面区域。关于具有挑战性的基准测试的实验表明，我们的GSFIX3D和GSFIXER实现了最先进的性能，只需要在捕获的数据上进行最少的场景特定微调。实际测试进一步证实了其对潜在姿势错误的韧性。我们的代码和数据将公开可用。项目页面：此HTTPS URL。</li>
</ul>

<h3>Title: Cross-Modality Controlled Molecule Generation with Diffusion Language Model</h3>
<ul>
<li><strong>Authors: </strong>Yunzhe Zhang, Yifei Wang, Khanh Vinh Nguyen, Pengyu Hong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14748">https://arxiv.org/abs/2508.14748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14748">https://arxiv.org/pdf/2508.14748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14748]] Cross-Modality Controlled Molecule Generation with Diffusion Language Model(https://arxiv.org/abs/2508.14748)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Current SMILES-based diffusion models for molecule generation typically support only unimodal constraint. They inject conditioning signals at the start of the training process and require retraining a new model from scratch whenever the constraint changes. However, real-world applications often involve multiple constraints across different modalities, and additional constraints may emerge over the course of a study. This raises a challenge: how to extend a pre-trained diffusion model not only to support cross-modality constraints but also to incorporate new ones without retraining. To tackle this problem, we propose the Cross-Modality Controlled Molecule Generation with Diffusion Language Model (CMCM-DLM), demonstrated by two distinct cross modalities: molecular structure and chemical properties. Our approach builds upon a pre-trained diffusion model, incorporating two trainable modules, the Structure Control Module (SCM) and the Property Control Module (PCM), and operates in two distinct phases during the generation process. In Phase I, we employs the SCM to inject structural constraints during the early diffusion steps, effectively anchoring the molecular backbone. Phase II builds on this by further introducing PCM to guide the later stages of inference to refine the generated molecules, ensuring their chemical properties match the specified targets. Experimental results on multiple datasets demonstrate the efficiency and adaptability of our approach, highlighting CMCM-DLM's significant advancement in molecular generation for drug discovery applications.</li>
<li><strong>摘要：</strong>分子生成的当前基于微笑的扩散模型通常仅支持单峰约束。他们在培训过程开始时注入调节信号，并需要在约束变化时从头开始重新审查。但是，现实世界的应用通常涉及不同方式的多种限制，并且在整个研究过程中可能会出现其他约束。这引起了一个挑战：如何扩展预先训练的扩散模型不仅支持交叉模式约束，而且还可以在不重新培训的情况下纳入新的扩散模型。为了解决这个问题，我们提出了具有扩散语言模型（CMCM-DLM）的交叉模式控制分子的产生，这是通过两种不同的跨模态证明的：分子结构和化学特性。我们的方法基于预先训练的扩散模型，其中包括两个可训练的模块，结构控制模块（SCM）和属性控制模块（PCM），并在生成过程中以两个不同的阶段运行。在第一阶段，我们在早期扩散步骤中采用SCM来注入结构约束，从而有效地锚定了分子主链。 II阶段是通过进一步引入PCM来指导推理以完善产生分子的后期阶段建立的，从而确保其化学性质与指定的靶标匹配。多个数据集的实验结果证明了我们方法的效率和适应性，强调了CMCM-DLM在药物发现应用中的分子生成方面的显着进步。</li>
</ul>

<h3>Title: PepThink-R1: LLM for Interpretable Cyclic Peptide Optimization with CoT SFT and Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Ruheng Wang, Hang Zhang, Trieu Nguyen, Shasha Feng, Hao-Wei Pang, Xiang Yu, Li Xiao, Peter Zhiping Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14765">https://arxiv.org/abs/2508.14765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14765">https://arxiv.org/pdf/2508.14765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14765]] PepThink-R1: LLM for Interpretable Cyclic Peptide Optimization with CoT SFT and Reinforcement Learning(https://arxiv.org/abs/2508.14765)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Designing therapeutic peptides with tailored properties is hindered by the vastness of sequence space, limited experimental data, and poor interpretability of current generative models. To address these challenges, we introduce PepThink-R1, a generative framework that integrates large language models (LLMs) with chain-of-thought (CoT) supervised fine-tuning and reinforcement learning (RL). Unlike prior approaches, PepThink-R1 explicitly reasons about monomer-level modifications during sequence generation, enabling interpretable design choices while optimizing for multiple pharmacological properties. Guided by a tailored reward function balancing chemical validity and property improvements, the model autonomously explores diverse sequence variants. We demonstrate that PepThink-R1 generates cyclic peptides with significantly enhanced lipophilicity, stability, and exposure, outperforming existing general LLMs (e.g., GPT-5) and domain-specific baseline in both optimization success and interpretability. To our knowledge, this is the first LLM-based peptide design framework that combines explicit reasoning with RL-driven property control, marking a step toward reliable and transparent peptide optimization for therapeutic discovery.</li>
<li><strong>摘要：</strong>序列空间的广阔空间，有限的实验数据以及当前生成模型的可解释性差，可以设计具有量身定制特性的治疗性肽。为了应对这些挑战，我们介绍了Pepthink-R1，这是一个将大型语言模型（LLM）与经过思考链（COT）监督的微调和强化学习（RL）集成的生成框架。与先前的方法不同，在序列生成过程中，请明确地出现有关单体级修改的原因，从而实现了可解释的设计选择，同时为多种药理特性进行了优化。在量身定制的奖励功能平衡化学有效性和财产改善的指导下，该模型自主探索了各种序列变体。我们证明，Pepthink-R1生成具有显着增强的亲脂性，稳定性和暴露的环状肽，在优化成功和解释性方面表现优于现有的一般LLM（例如GPT-5）和域特异性基线。据我们所知，这是第一个将明确推理与RL驱动的属性控制相结合的基于LLM的肽设计框架，标志着朝着可靠且透明的肽优化进行治疗发现的一步。</li>
</ul>

<h3>Title: MF-LPR$^2$: Multi-Frame License Plate Image Restoration and Recognition using Optical Flow</h3>
<ul>
<li><strong>Authors: </strong>Kihyun Na, Junseok Oh, Youngkwan Cho, Bumjin Kim, Sungmin Cho, Jinyoung Choi, Injung Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14797">https://arxiv.org/abs/2508.14797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14797">https://arxiv.org/pdf/2508.14797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14797]] MF-LPR$^2$: Multi-Frame License Plate Image Restoration and Recognition using Optical Flow(https://arxiv.org/abs/2508.14797)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generative</a></li>
<li><strong>Abstract: </strong>License plate recognition (LPR) is important for traffic law enforcement, crime investigation, and surveillance. However, license plate areas in dash cam images often suffer from low resolution, motion blur, and glare, which make accurate recognition challenging. Existing generative models that rely on pretrained priors cannot reliably restore such poor-quality images, frequently introducing severe artifacts and distortions. To address this issue, we propose a novel multi-frame license plate restoration and recognition framework, MF-LPR$^2$, which addresses ambiguities in poor-quality images by aligning and aggregating neighboring frames instead of relying on pretrained knowledge. To achieve accurate frame alignment, we employ a state-of-the-art optical flow estimator in conjunction with carefully designed algorithms that detect and correct erroneous optical flow estimations by leveraging the spatio-temporal consistency inherent in license plate image sequences. Our approach enhances both image quality and recognition accuracy while preserving the evidential content of the input images. In addition, we constructed a novel Realistic LPR (RLPR) dataset to evaluate MF-LPR$^2$. The RLPR dataset contains 200 pairs of low-quality license plate image sequences and high-quality pseudo ground-truth images, reflecting the complexities of real-world scenarios. In experiments, MF-LPR$^2$ outperformed eight recent restoration models in terms of PSNR, SSIM, and LPIPS by significant margins. In recognition, MF-LPR$^2$ achieved an accuracy of 86.44%, outperforming both the best single-frame LPR (14.04%) and the multi-frame LPR (82.55%) among the eleven baseline models. The results of ablation studies confirm that our filtering and refinement algorithms significantly contribute to these improvements.</li>
<li><strong>摘要：</strong>车牌识别（LPR）对于交通执法，犯罪调查和监视很重要。但是，仪表板凸轮图像中的车牌区域通常会遭受低分辨率，运动模糊和眩光的困扰，这使得准确的识别挑战。依靠审慎的先验的现有生成模型无法可靠地恢复如此质量不佳的图像，经常引入严重的伪影和扭曲。为了解决这个问题，我们提出了一个新颖的多帧车牌修复和识别框架MF-LPR $^2 $，该框架通过对齐和汇总相邻框架而不是依靠预审核的知识来解决质量不佳图像的歧义。为了实现准确的框架对齐，我们与精心设计的算法结合使用了最新的光流估计器，通过利用车牌板图像序列中固有的时空一致性来检测和纠正错误的光流估计。我们的方法提高了图像质量和识别精度，同时保留了输入图像的证据内容。此外，我们构建了一个新颖的LPR（RLPR）数据集来评估MF-LPR $^2 $。 RLPR数据集包含200对低质量的车牌图像序列和高质量的伪基真实图像，反映了真实情况的复杂性。在实验中，从PSNR，SSIM和LPIPS上，MF-LPR $^2 $按大量利润率优于最近的八个恢复模型。为了认可，MF-LPR $^2 $的准确度为86.44％，在11个基线模型中，最佳单帧LPR（14.04％）和多帧LPR（82.55％）的表现都优于最佳单帧LPR（14.04％）。消融研究的结果证实，我们的过滤和改进算法显着有助于这些改进。</li>
</ul>

<h3>Title: Source-Guided Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Zifan Wang, Alice Harting, Matthieu Barreau, Michael M. Zavlanos, Karl H. Johansson</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14807">https://arxiv.org/abs/2508.14807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14807">https://arxiv.org/pdf/2508.14807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14807]] Source-Guided Flow Matching(https://arxiv.org/abs/2508.14807)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Guidance of generative models is typically achieved by modifying the probability flow vector field through the addition of a guidance field. In this paper, we instead propose the Source-Guided Flow Matching (SGFM) framework, which modifies the source distribution directly while keeping the pre-trained vector field intact. This reduces the guidance problem to a well-defined problem of sampling from the source distribution. We theoretically show that SGFM recovers the desired target distribution exactly. Furthermore, we provide bounds on the Wasserstein error for the generated distribution when using an approximate sampler of the source distribution and an approximate vector field. The key benefit of our approach is that it allows the user to flexibly choose the sampling method depending on their specific problem. To illustrate this, we systematically compare different sampling methods and discuss conditions for asymptotically exact guidance. Moreover, our framework integrates well with optimal flow matching models since the straight transport map generated by the vector field is preserved. Experimental results on synthetic 2D benchmarks, image datasets, and physics-informed generative tasks demonstrate the effectiveness and flexibility of the proposed framework.</li>
<li><strong>摘要：</strong>通常通过增加指导场来修改概率流量矢量场来实现生成模型的指导。在本文中，我们提出了源引导的流量匹配（SGFM）框架，该框架直接修改源分布，同时保持预训练的矢量场完整。这将指导问题减少到了从源分布中进行采样明确的问题。从理论上讲，我们表明SGFM精确地恢复了所需的目标分布。此外，当使用源分布的近似采样器和近似向量场时，我们为生成的分布提供了瓦斯汀误差的界限。我们方法的关键好处是，它允许用户根据其特定问题灵活选择采样方法。为了说明这一点，我们从系统地比较了不同的抽样方法，并讨论了渐近确切指导的条件。此外，我们的框架与最佳流量匹配模型很好地集成在一起，因为保留了矢量场产生的直​​接传输图。合成2D基准，图像数据集和物理信息的生成任务的实验结果证明了所提出的框架的有效性和灵活性。</li>
</ul>

<h3>Title: Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From Sparse Inputs without Per-Scene Optimization</h3>
<ul>
<li><strong>Authors: </strong>Canyu Zhao, Xiaoman Li, Tianjian Feng, Zhiyue Zhao, Hao Chen, Chunhua Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14811">https://arxiv.org/abs/2508.14811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14811">https://arxiv.org/pdf/2508.14811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14811]] Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From Sparse Inputs without Per-Scene Optimization(https://arxiv.org/abs/2508.14811)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce Tinker, a versatile framework for high-fidelity 3D editing that operates in both one-shot and few-shot regimes without any per-scene finetuning. Unlike prior techniques that demand extensive per-scene optimization to ensure multi-view consistency or to produce dozens of consistent edited input views, Tinker delivers robust, multi-view consistent edits from as few as one or two images. This capability stems from repurposing pretrained diffusion models, which unlocks their latent 3D awareness. To drive research in this space, we curate the first large-scale multi-view editing dataset and data pipeline, spanning diverse scenes and styles. Building on this dataset, we develop our framework capable of generating multi-view consistent edited views without per-scene training, which consists of two novel components: (1) Referring multi-view editor: Enables precise, reference-driven edits that remain coherent across all viewpoints. (2) Any-view-to-video synthesizer: Leverages spatial-temporal priors from video diffusion to perform high-quality scene completion and novel-view generation even from sparse inputs. Through extensive experiments, Tinker significantly reduces the barrier to generalizable 3D content creation, achieving state-of-the-art performance on editing, novel-view synthesis, and rendering enhancement tasks. We believe that Tinker represents a key step towards truly scalable, zero-shot 3D editing. Project webpage: this https URL</li>
<li><strong>摘要：</strong>我们介绍了Tinker，这是一种用于高保真3D编辑的多功能框架，在一声和少数弹药方面都可以运行，而无需进行任何单场填充。与以前需要广泛的人均优化的技术不同，以确保多视图的一致性或产生数十个一致的编辑输入视图，Tinker提供了可靠的多视图一致的一致编辑，从一两个图像。这种能力源于重新修复预处理的扩散模型，从而释放了其潜在的3D意识。为了推动该领域的研究，我们策划了第一个大规模的多视图编辑数据集和数据管道，从而涵盖了各种场景和样式。在此数据集的基础上，我们开发了能够生成一致的多视图编辑视图的框架，而无需每场训练，该视图由两个新颖的组件组成：（1）参考多视图编辑器：启用精确的，参考驱动的编辑，这些编辑在所有观点中保持一致。 （2）任何视图到视频合成器：从视频扩散到实现高质量的场景完成和新颖的视图，甚至从稀疏输入中，利用时空先验。通过广泛的实验，修补匠大大降低了可推广的3D内容创建的障碍，在编辑，小说视图合成和渲染增强任务方面实现了最新性能。我们认为，Tinker代表了迈出真正可扩展，零击3D编辑的关键步骤。项目网页：此HTTPS URL</li>
</ul>

<h3>Title: TransLight: Image-Guided Customized Lighting Control with Generative Decoupling</h3>
<ul>
<li><strong>Authors: </strong>Zongming Li, Lianghui Zhu, Haocheng Shen, Longjin Ran, Wenyu Liu, Xinggang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14814">https://arxiv.org/abs/2508.14814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14814">https://arxiv.org/pdf/2508.14814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14814]] TransLight: Image-Guided Customized Lighting Control with Generative Decoupling(https://arxiv.org/abs/2508.14814)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Most existing illumination-editing approaches fail to simultaneously provide customized control of light effects and preserve content integrity. This makes them less effective for practical lighting stylization requirements, especially in the challenging task of transferring complex light effects from a reference image to a user-specified target image. To address this problem, we propose TransLight, a novel framework that enables high-fidelity and high-freedom transfer of light effects. Extracting the light effect from the reference image is the most critical and challenging step in our method. The difficulty lies in the complex geometric structure features embedded in light effects that are highly coupled with content in real-world scenarios. To achieve this, we first present Generative Decoupling, where two fine-tuned diffusion models are used to accurately separate image content and light effects, generating a newly curated, million-scale dataset of image-content-light triplets. Then, we employ IC-Light as the generative model and train our model with our triplets, injecting the reference lighting image as an additional conditioning signal. The resulting TransLight model enables customized and natural transfer of diverse light effects. Notably, by thoroughly disentangling light effects from reference images, our generative decoupling strategy endows TransLight with highly flexible illumination control. Experimental results establish TransLight as the first method to successfully transfer light effects across disparate images, delivering more customized illumination control than existing techniques and charting new directions for research in illumination harmonization and editing.</li>
<li><strong>摘要：</strong>大多数现有的照明方法无法同时提供对光效应的自定义控制并保留内容完整性。这使得它们对实践照明样式化要求的有效性降低了，尤其是在将复杂的光效果从参考图像转移到用户指定的目标图像的挑战性任务中。为了解决这个问题，我们提出了Translight，这是一个新颖的框架，可以使光效应的高保真和高自由度转移。从参考图像中提取光效应是我们方法中最重要，最具挑战性的步骤。困难在于复杂的几何结构特征，这些特征嵌入了光效应中，这些效应与现实世界中的内容高度结合。为了实现这一目标，我们首先提出生成脱钩，其中两个微调的扩散模型用于准确地分离图像内容和光效应，从而生成一个新策划的百万个尺度图像符合图的数据集。然后，我们将IC-Light用作生成模型，并用三胞胎训练我们的模型，将参考照明图像作为附加条件信号注入。最终的晶体光模型可实现自定义和自然转移的不同光效应。值得注意的是，通过完全将光线效应与参考图像彻底删除，我们的生成脱钩策略赋予了具有高灵活的照明控制的Translight。实验结果将跨灯确定为成功传递不同图像的光效应的第一种方法，比现有技术提供了更多的定制照明控制，并绘制了用于照明协调和编辑的研究新方向。</li>
</ul>

<h3>Title: Squeezed Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Jyotirmai Singh, Samar Khanna, James Burgess</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14871">https://arxiv.org/abs/2508.14871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14871">https://arxiv.org/pdf/2508.14871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14871]] Squeezed Diffusion Models(https://arxiv.org/abs/2508.14871)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion models typically inject isotropic Gaussian noise, disregarding structure in the data. Motivated by the way quantum squeezed states redistribute uncertainty according to the Heisenberg uncertainty principle, we introduce Squeezed Diffusion Models (SDM), which scale noise anisotropically along the principal component of the training distribution. As squeezing enhances the signal-to-noise ratio in physics, we hypothesize that scaling noise in a data-dependent manner can better assist diffusion models in learning important data features. We study two configurations: (i) a Heisenberg diffusion model that compensates the scaling on the principal axis with inverse scaling on orthogonal directions and (ii) a standard SDM variant that scales only the principal axis. Counterintuitively, on CIFAR-10/100 and CelebA-64, mild antisqueezing - i.e. increasing variance on the principal axis - consistently improves FID by up to 15% and shifts the precision-recall frontier toward higher recall. Our results demonstrate that simple, data-aware noise shaping can deliver robust generative gains without architectural changes.</li>
<li><strong>摘要：</strong>扩散模型通常注入各向同性高斯噪声，无视数据中的结构。通过量子挤压状态根据海森伯格不确定性原则重新分布不确定性的方式，我们引入了挤压扩散模型（SDM），该模型（SDM）沿训练分布的主要成分进行缩放噪声。随着挤压提高物理的信噪比，我们假设以数据依赖性方式扩展噪声可以更好地帮助扩散模型学习重要的数据特征。我们研究了两种配置：（i）Heisenberg扩散模型，该模型在正交方向上对主轴上的缩放进行补偿，以及（ii）仅缩放主轴的标准SDM变体。在违反直觉上，在CIFAR-10/100和Celeba-64上，轻度的反queezing-即增加主轴的差异 - 始终提高FID高达15％，并将Precision-Recall Recall Frontier转移到更高的回忆中。我们的结果表明，简单的，数据感知的噪声构成可以在没有架构变化的情况下带来强大的生成增长。</li>
</ul>

<h3>Title: Virtual Community: An Open World for Humans, Robots, and Society</h3>
<ul>
<li><strong>Authors: </strong>Qinhong Zhou, Hongxin Zhang, Xiangye Lin, Zheyuan Zhang, Yutian Chen, Wenjun Liu, Zunzhe Zhang, Sunli Chen, Lixing Fang, Qiushi Lyu, Xinyu Sun, Jincheng Yang, Zeyuan Wang, Bao Chi Dang, Zhehuan Chen, Daksha Ladia, Jiageng Liu, Chuang Gan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14893">https://arxiv.org/abs/2508.14893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14893">https://arxiv.org/pdf/2508.14893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14893]] Virtual Community: An Open World for Humans, Robots, and Society(https://arxiv.org/abs/2508.14893)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The rapid progress in AI and Robotics may lead to a profound societal transformation, as humans and robots begin to coexist within shared communities, introducing both opportunities and challenges. To explore this future, we present Virtual Community-an open-world platform for humans, robots, and society-built on a universal physics engine and grounded in real-world 3D scenes. With Virtual Community, we aim to study embodied social intelligence at scale: 1) How robots can intelligently cooperate or compete; 2) How humans develop social relations and build community; 3) More importantly, how intelligent robots and humans can co-exist in an open world. To support these, Virtual Community features: 1) An open-source multi-agent physics simulator that supports robots, humans, and their interactions within a society; 2) A large-scale, real-world aligned community generation pipeline, including vast outdoor space, diverse indoor scenes, and a community of grounded agents with rich characters and appearances. Leveraging Virtual Community, we propose two novel challenges. The Community Planning Challenge evaluates multi-agent reasoning and planning ability in open-world settings, such as cooperating to help agents with daily activities and efficiently connecting other agents. The Community Robot Challenge requires multiple heterogeneous robots to collaborate in solving complex open-world tasks. We evaluate various baselines on these tasks and demonstrate the challenges in both high-level open-world task planning and low-level cooperation controls. We hope that Virtual Community will unlock further study of human-robot coexistence within open-world environments.</li>
<li><strong>摘要：</strong>随着人类和机器人在共享社区内共存，引入机遇和挑战时，AI和机器人技术的快速进步可能会导致深刻的社会转变。为了探索这一未来，我们介绍了虚拟社区 - 一个在通用物理引擎上为人类，机器人和社会建造的开放世界平台，并以现实世界中的3D场景为基础。对于虚拟社区，我们的目标是按大规模研究体现的社会情报：1）机器人如何智能合作或竞争； 2）人类如何发展社会关系并建立社区； 3）更重要的是，聪明的机器人和人类如何在开放世界中共存。为了支持这些虚拟社区特征：1）支持机器人，人类及其在社会中的互动的开源多代理物理模拟器； 2）一个大规模的现实世界一致的社区一代管道，包括宽敞的室外空间，各种各样的室内场景以及一个具有丰富人物和外表的地面代理商。利用虚拟社区，我们提出了两个新颖的挑战。社区规划挑战挑战评估了开放世界环境中的多代理推理和计划能力，例如合作以帮助代理商进行日常活动并有效地连接其他代理。社区机器人挑战需要多个异构机器人来解决复杂的开放世界任务。我们在这些任务上评估了各种基准，并证明了高级开放世界任务计划和低级合作控制中的挑战。我们希望虚拟社区能够在开放世界环境中释放对人类机器人共存的进一步研究。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
