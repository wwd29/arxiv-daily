<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-13</h1>
<h3>Title: Adjusting Initial Noise to Mitigate Memorization in Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Hyeonggeun Han, Sehwan Kim, Hyungjun Joo, Sangwoo Hong, Jungwoo Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.08625">https://arxiv.org/abs/2510.08625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.08625">https://arxiv.org/pdf/2510.08625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.08625]] Adjusting Initial Noise to Mitigate Memorization in Text-to-Image Diffusion Models(https://arxiv.org/abs/2510.08625)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Despite their impressive generative capabilities, text-to-image diffusion models often memorize and replicate training data, prompting serious concerns over privacy and copyright. Recent work has attributed this memorization to an attraction basin-a region where applying classifier-free guidance (CFG) steers the denoising trajectory toward memorized outputs-and has proposed deferring CFG application until the denoising trajectory escapes this basin. However, such delays often result in non-memorized images that are poorly aligned with the input prompts, highlighting the need to promote earlier escape so that CFG can be applied sooner in the denoising process. In this work, we show that the initial noise sample plays a crucial role in determining when this escape occurs. We empirically observe that different initial samples lead to varying escape times. Building on this insight, we propose two mitigation strategies that adjust the initial noise-either collectively or individually-to find and utilize initial samples that encourage earlier basin escape. These approaches significantly reduce memorization while preserving image-text alignment.</li>
<li><strong>摘要：</strong>尽管文本到图像的扩散模型具有令人印象深刻的生成能力，但它们通常会记住和复制训练数据，从而引发对隐私和版权的严重担忧。最近的工作将这种记忆归因于吸引盆地——在该区域应用无分类器引导（CFG）将去噪轨迹转向记忆的输出——并建议推迟 CFG 的应用，直到去噪轨迹逃离该盆地。然而，这种延迟通常会导致未记忆的图像与输入提示不一致，这凸显了促进早期转义的必要性，以便 CFG 可以更快地应用于去噪过程。在这项工作中，我们表明初始噪声样本在确定这种逃逸何时发生方面起着至关重要的作用。我们凭经验观察到不同的初始样本会导致不同的逃逸时间。基于这一见解，我们提出了两种缓解策略，集体或单独调整初始噪声，以找到并利用鼓励早期盆地逃逸的初始样本。这些方法显着减少记忆，同时保持图像文本对齐。</li>
</ul>

<h3>Title: The Digital Mirror: Gender Bias and Occupational Stereotypes in AI-Generated Images</h3>
<ul>
<li><strong>Authors: </strong>Siiri Leppälampi, Sonja M. Hyrynsalmi, Erno Vanhala</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.08628">https://arxiv.org/abs/2510.08628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.08628">https://arxiv.org/pdf/2510.08628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.08628]] The Digital Mirror: Gender Bias and Occupational Stereotypes in AI-Generated Images(https://arxiv.org/abs/2510.08628)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative AI offers vast opportunities for creating visualisations, such as graphics, videos, and images. However, recent studies around AI-generated visualisations have primarily focused on the creation process and image quality, overlooking representational biases. This study addresses this gap by testing representation biases in AI-generated pictures in an occupational setting and evaluating how two AI image generator tools, DALL-E 3 and Ideogram, compare. Additionally, the study discusses topics such as ageing and emotions in AI-generated images. As AI image tools are becoming more widely used, addressing and mitigating harmful gender biases becomes essential to ensure diverse representation in media and professional settings. In this study, over 750 AI-generated images of occupations were prompted. The thematic analysis results revealed that both DALL-E 3 and Ideogram reinforce traditional gender stereotypes in AI-generated images, although to varying degrees. These findings emphasise that AI visualisation tools risk reinforcing narrow representations. In our discussion section, we propose suggestions for practitioners, individuals and researchers to increase representation when generating images with visible genders.</li>
<li><strong>摘要：</strong>生成式人工智能为创建图形、视频和图像等可视化提供了巨大的机会。然而，最近围绕人工智能生成的可视化的研究主要集中在创作过程和图像质量上，忽视了代表性偏差。本研究通过测试职业环境中人工智能生成的图片的表征偏差并评估两种人工智能图像生成工具 DALL-E 3 和 Ideogram 的比较来解决这一差距。此外，该研究还讨论了人工智能生成图像中的衰老和情绪等主题。随着人工智能图像工具的使用越来越广泛，解决和减轻有害的性别偏见对于确保媒体和专业环境中的多元化代表性至关重要。在这项研究中，提示了超过 750 张人工智能生成的职业图像。主题分析结果显示，DALL-E 3 和 Ideogram 都强化了人工智能生成图像中的传统性别刻板印象，尽管程度不同。这些发现强调人工智能可视化工具有强化狭隘表述的风险。在我们的讨论部分，我们为从业者、个人和研究人员提出建议，以在生成具有明显性别的图像时增加代表性。</li>
</ul>

<h3>Title: Dynamic Mixture-of-Experts for Visual Autoregressive Model</h3>
<ul>
<li><strong>Authors: </strong>Jort Vincenti, Metod Jazbec, Guoxuan Xia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.08629">https://arxiv.org/abs/2510.08629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.08629">https://arxiv.org/pdf/2510.08629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.08629]] Dynamic Mixture-of-Experts for Visual Autoregressive Model(https://arxiv.org/abs/2510.08629)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Visual Autoregressive Models (VAR) offer efficient and high-quality image generation but suffer from computational redundancy due to repeated Transformer calls at increasing resolutions. We introduce a dynamic Mixture-of-Experts router integrated into VAR. The new architecture allows to trade compute for quality through scale-aware thresholding. This thresholding strategy balances expert selection based on token complexity and resolution, without requiring additional training. As a result, we achieve 20% fewer FLOPs, 11% faster inference and match the image quality achieved by the dense baseline.</li>
<li><strong>摘要：</strong>视觉自回归模型 (VAR) 提供高效、高质量的图像生成，但由于分辨率不断提高而重复调用 Transformer，因此会产生计算冗余。我们引入了集成到 VAR 中的动态专家混合路由器。新架构允许通过规模感知阈值以计算换取质量。这种阈值策略根据令牌复杂性和分辨率平衡专家选择，而不需要额外的培训。结果，我们的 FLOP 次数减少了 20%，推理速度提高了 11%，并且与密集基线实现的图像质量相匹配。</li>
</ul>

<h3>Title: PhyDAE: Physics-Guided Degradation-Adaptive Experts for All-in-One Remote Sensing Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Zhe Dong, Yuzhe Sun, Haochen Jiang, Tianzhu Liu, Yanfeng Gu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.08653">https://arxiv.org/abs/2510.08653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.08653">https://arxiv.org/pdf/2510.08653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.08653]] PhyDAE: Physics-Guided Degradation-Adaptive Experts for All-in-One Remote Sensing Image Restoration(https://arxiv.org/abs/2510.08653)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Remote sensing images inevitably suffer from various degradation factors during acquisition, including atmospheric interference, sensor limitations, and imaging conditions. These complex and heterogeneous degradations pose severe challenges to image quality and downstream interpretation tasks. Addressing limitations of existing all-in-one restoration methods that overly rely on implicit feature representations and lack explicit modeling of degradation physics, this paper proposes Physics-Guided Degradation-Adaptive Experts (PhyDAE). The method employs a two-stage cascaded architecture transforming degradation information from implicit features into explicit decision signals, enabling precise identification and differentiated processing of multiple heterogeneous degradations including haze, noise, blur, and low-light conditions. The model incorporates progressive degradation mining and exploitation mechanisms, where the Residual Manifold Projector (RMP) and Frequency-Aware Degradation Decomposer (FADD) comprehensively analyze degradation characteristics from manifold geometry and frequency perspectives. Physics-aware expert modules and temperature-controlled sparse activation strategies are introduced to enhance computational efficiency while ensuring imaging physics consistency. Extensive experiments on three benchmark datasets (MD-RSID, MD-RRSHID, and MDRS-Landsat) demonstrate that PhyDAE achieves superior performance across all four restoration tasks, comprehensively outperforming state-of-the-art methods. Notably, PhyDAE substantially improves restoration quality while achieving significant reductions in parameter count and computational complexity, resulting in remarkable efficiency gains compared to mainstream approaches and achieving optimal balance between performance and efficiency. Code is available at this https URL.</li>
<li><strong>摘要：</strong>遥感图像在获取过程中不可避免地会受到各种退化因素的影响，包括大气干扰、传感器限制和成像条件等。这些复杂且异构的退化对图像质量和下游解释任务提出了严峻的挑战。针对现有一体化恢复方法过度依赖隐式特征表示且缺乏退化物理的显式建模的局限性，本文提出了物理引导退化自适应专家（PhyDAE）。该方法采用两级级联架构，将隐式特征的退化信息转换为显式决策信号，从而能够精确识别和区分处理包括雾霾、噪声、模糊和弱光条件在内的多种异构退化。该模型结合了渐进退化挖掘和利用机制，其中残余流形投影仪（RMP）和频率感知退化分解器（FADD）从流形几何和频率角度全面分析退化特征。引入物理感知专家模块和温控稀疏激活策略，以提高计算效率，同时确保成像物理一致性。对三个基准数据集（MD-RSID、MD-RRSHID 和 MDRS-Landsat）的大量实验表明，PhyDAE 在所有四个恢复任务中均实现了卓越的性能，全面优于最先进的方法。值得注意的是，PhyDAE 大幅提高了恢复质量，同时显着降低了参数数量和计算复杂性，与主流方法相比，效率显着提升，并实现了性能和效率之间的最佳平衡。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Hulu-Med: A Transparent Generalist Model towards Holistic Medical Vision-Language Understanding</h3>
<ul>
<li><strong>Authors: </strong>Songtao Jiang, Yuan Wang, Sibo Song, Tianxiang Hu, Chenyi Zhou, Bin Pu, Yan Zhang, Zhibo Yang, Yang Feng, Joey Tianyi Zhou, Jin Hao, Zijian Chen, Ruijia Wu, Tao Tang, Junhui Lv, Hongxia Xu, Hongwei Wang, Jun Xiao, Bin Feng, Fudong Zhu, Kenli Li, Weidi Xie, Jimeng Sun, Jian Wu, Zuozhu Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.08668">https://arxiv.org/abs/2510.08668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.08668">https://arxiv.org/pdf/2510.08668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.08668]] Hulu-Med: A Transparent Generalist Model towards Holistic Medical Vision-Language Understanding(https://arxiv.org/abs/2510.08668)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Real-world clinical decision-making grapples with integrating information from diverse data modalities, including medical text, 2D/3D images, and video, leading to inefficiencies and potential diagnostic oversights. While generalist vision-language models (VLMs) offer promise, their medical development faces challenges of opaque pipelines, data scarcity, and architectural inflexibility. Here we present Hulu-Med, a transparent medical VLM that unifies understanding across all these modalities. Built upon a unified patch-based vision encoder and an LLM decoder, Hulu-Med was progressively trained on 16.7 million (M) samples to scale from 2D to 3D and video comprehension. The medical-aware token reduction enables efficient training, requiring only 4,000 to 40,000 GPU hours for 7B to 32B parameter variants. Extensive evaluation across 30 benchmarks exhibits state-of-the-art performance, surpassing leading open-source models and competing with proprietary systems in tasks spanning visual question-answering, medical report generation, and complex reasoning in multilingual and rare disease scenarios. By open-sourcing our complete pipeline, we establish that high-performance medical VLM can be achieved transparently, providing a foundational tool for accessible and impactful clinical AI. Code is released on \href{this https URL}{this https URL}.</li>
<li><strong>摘要：</strong>现实世界的临床决策需要整合来自不同数据模式（包括医学文本、2D/3D 图像和视频）的信息，从而导致效率低下和潜在的诊断疏忽。虽然通用视觉语言模型 (VLM) 带来了希望，但其医疗发展面临着管道不透明、数据稀缺和架构不灵活的挑战。在这里，我们推出 Hulu-Med，这是一种透明的医疗 VLM，它统一了对所有这些模式的理解。 Hulu-Med 基于统一的基于补丁的视觉编码器和 LLM 解码器构建，在 1670 万 (M) 个样本上逐步进行训练，以从 2D 扩展到 3D 和视频理解。医疗感知令牌减少可实现高效训练，7B 至 32B 参数变体仅需要 4,000 至 40,000 个 GPU 小时。对 30 个基准的广泛评估展示了最先进的性能，超越了领先的开源模型，并在视觉问答、医疗报告生成以及多语言和罕见疾病场景中的复杂推理等任务中与专有系统竞争。通过开源我们的完整管道，我们确定可以透明地实现高性能医疗 VLM，为可访问且有影响力的临床 AI 提供基础工具。代码发布于 \href{此 https URL}{此 https URL}。</li>
</ul>

<h3>Title: FreqCa: Accelerating Diffusion Models via Frequency-Aware Caching</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Liu, Peiliang Cai, Qinming Zhou, Yuqi Lin, Deyang Kong, Benhao Huang, Yupei Pan, Haowen Xu, Chang Zou, Junshu Tang, Shikang Zheng, Linfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.08669">https://arxiv.org/abs/2510.08669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.08669">https://arxiv.org/pdf/2510.08669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.08669]] FreqCa: Accelerating Diffusion Models via Frequency-Aware Caching(https://arxiv.org/abs/2510.08669)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The application of diffusion transformers is suffering from their significant inference costs. Recently, feature caching has been proposed to solve this problem by reusing features from previous timesteps, thereby skipping computation in future timesteps. However, previous feature caching assumes that features in adjacent timesteps are similar or continuous, which does not always hold in all settings. To investigate this, this paper begins with an analysis from the frequency domain, which reveal that different frequency bands in the features of diffusion models exhibit different dynamics across timesteps. Concretely, low-frequency components, which decide the structure of images, exhibit higher similarity but poor continuity. In contrast, the high-frequency bands, which decode the details of images, show significant continuity but poor similarity. These interesting observations motivate us to propose Frequency-aware Caching (FreqCa) which directly reuses features of low-frequency components based on their similarity, while using a second-order Hermite interpolator to predict the volatile high-frequency ones based on its continuity. Besides, we further propose to cache Cumulative Residual Feature (CRF) instead of the features in all the layers, which reduces the memory footprint of feature caching by 99%. Extensive experiments on FLUX.1-dev, FLUX.1-Kontext-dev, Qwen-Image, and Qwen-Image-Edit demonstrate its effectiveness in both generation and editing. Codes are available in the supplementary materials and will be released on GitHub.</li>
<li><strong>摘要：</strong>扩散变压器的应用受到其巨大推理成本的影响。最近，提出了特征缓存来通过重用先前时间步的特征来解决这个问题，从而跳过未来时间步的计算。然而，以前的特征缓存假设相邻时间步中的特征相似或连续，这并不总是适用于所有设置。为了研究这一点，本文从频域分析开始，揭示了扩散模型特征中的不同频带在时间步长上表现出不同的动态。具体来说，决定图像结构的低频分量表现出较高的相似性，但连续性较差。相比之下，解码图像细节的高频段表现出显着的连续性，但相似性较差。这些有趣的观察促使我们提出频率感知缓存（FreqCa），它根据低频分量的相似性直接重用特征，同时使用二阶 Hermite 插值器根据其连续性来预测不稳定的高频分量。此外，我们进一步提出缓存累积残差特征（CRF）而不是所有层中的特征，这将特征缓存的内存占用减少了99%。在 FLUX.1-dev、FLUX.1-Kontext-dev、Qwen-Image 和 Qwen-Image-Edit 上进行的大量实验证明了其在生成和编辑方面的有效性。代码可在补充材料中找到，并将在 GitHub 上发布。</li>
</ul>

<h3>Title: Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation</h3>
<ul>
<li><strong>Authors: </strong>Kang Liao, Size Wu, Zhonghua Wu, Linyi Jin, Chao Wang, Yikai Wang, Fei Wang, Wei Li, Chen Change Loy</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.08673">https://arxiv.org/abs/2510.08673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.08673">https://arxiv.org/pdf/2510.08673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.08673]] Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation(https://arxiv.org/abs/2510.08673)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Camera-centric understanding and generation are two cornerstones of spatial intelligence, yet they are typically studied in isolation. We present Puffin, a unified camera-centric multimodal model that extends spatial awareness along the camera dimension. Puffin integrates language regression and diffusion-based generation to interpret and create scenes from arbitrary viewpoints. To bridge the modality gap between cameras and vision-language, we introduce a novel paradigm that treats camera as language, enabling thinking with camera. This guides the model to align spatially grounded visual cues with photographic terminology while reasoning across geometric context. Puffin is trained on Puffin-4M, a large-scale dataset of 4 million vision-language-camera triplets. We incorporate both global camera parameters and pixel-wise camera maps, yielding flexible and reliable spatial generation. Experiments demonstrate Puffin superior performance over specialized models for camera-centric generation and understanding. With instruction tuning, Puffin generalizes to diverse cross-view tasks such as spatial imagination, world exploration, and photography guidance. We will release the code, models, dataset pipeline, and benchmark to advance multimodal spatial intelligence research.</li>
<li><strong>摘要：</strong>以相机为中心的理解和生成是空间智能的两个基石，但它们通常是孤立研究的。我们推出了 Puffin，这是一种以相机为中心的统一多模态模型，可沿相机维度扩展空间感知。 Puffin 集成了语言回归和基于扩散的生成，可以从任意视角解释和创建场景。为了弥合相机和视觉语言之间的模态差距，我们引入了一种新颖的范式，将相机视为语言，从而能够用相机进行思考。这引导模型将空间基础的视觉线索与摄影术语结合起来，同时跨几何上下文进行推理。 Puffin 在 Puffin-4M 上进行训练，Puffin-4M 是一个包含 400 万个视觉-语言-相机三元组的大型数据集。我们结合了全局相机参数和逐像素相机地图，产生灵活可靠的空间生成。实验证明 Puffin 在以相机为中心的生成和理解方面比专用模型具有优越的性能。通过指令调整，Puffin 可以推广到各种跨视图任务，例如空间想象、世界探索和摄影指导。我们将发布代码、模型、数据集管道和基准，以推进多模式空间智能研究。</li>
</ul>

<h3>Title: Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence Reweighting</h3>
<ul>
<li><strong>Authors: </strong>Yunzhen Feng, Parag Jain, Anthony Hartshorn, Yaqi Duan, Julia Kempe</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.08696">https://arxiv.org/abs/2510.08696</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.08696">https://arxiv.org/pdf/2510.08696</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.08696]] Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence Reweighting(https://arxiv.org/abs/2510.08696)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reinforcement learning with verifiable rewards (RLVR) has become a standard recipe for improving large language models (LLMs) on reasoning tasks, with Group Relative Policy Optimization (GRPO) widely used in practice. Yet GRPO wastes substantial compute on negative groups: groups in which no sampled response is correct yield zero advantage and thus no gradient. We ask whether negative groups can be leveraged without extra supervision. Starting from a maximum-likelihood (MLE) objective in reward modeling, we show that the MLE gradient is equivalent to a policy gradient for a modified value function. This value function adds a confidence-weighted penalty on incorrect responses, imposing larger penalties on more confident mistakes. We refer to this as \textbf{L}ikelihood \textbf{E}stimation with \textbf{N}egative \textbf{S}amples (\textbf{LENS}). LENS modifies GRPO to assign non-zero, confidence-dependent rewards to incorrect generations, making negative groups informative and converting previously wasted samples into useful gradient updates. On the MATH benchmark with Llama-3.1-8B and Qwen-2.5-3B, the proposed variant consistently outperforms GRPO baseline, with significant gains on harder items. These results demonstrate a principled and practical way to "rescue" negative groups, improving efficiency and performance in RLVR.</li>
<li><strong>摘要：</strong>具有可验证奖励的强化学习（RLVR）已成为改进大型语言模型（LLM）推理任务的标准方法，组相对策略优化（GRPO）在实践中得到广泛应用。然而，GRPO 在负组上浪费了大量计算：没有正确采样响应的组产生零优势，因此没有梯度。我们询问是否可以在没有额外监督的情况下利用负面群体。从奖励建模中的最大似然（MLE）目标开始，我们证明 MLE 梯度相当于修改后的价值函数的策略梯度。该价值函数增加了对错误响应的置信度加权惩罚，对更自信的错误施加更大的惩罚。我们将其称为 \textbf{L}ikelihood \textbf{E}stimation with \textbf{N}egative \textbf{S}amples (\textbf{LENS})。 LENS 修改了 GRPO，将非零、依赖于置信度的奖励分配给不正确的世代，从而使负组信息丰富，并将之前浪费的样本转换为有用的梯度更新。在 Llama-3.1-8B 和 Qwen-2.5-3B 的 MATH 基准上，所提出的变体始终优于 GRPO 基准，在较难的项目上有显着的提升。这些结果展示了一种“拯救”消极群体、提高 RLVR 效率和绩效的原则性且实用的方法。</li>
</ul>

<h3>Title: In-Context Learning for Non-Stationary MIMO Equalization</h3>
<ul>
<li><strong>Authors: </strong>Jiachen Jiang, Zhen Qin, Zhihui Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.08711">https://arxiv.org/abs/2510.08711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.08711">https://arxiv.org/pdf/2510.08711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.08711]] In-Context Learning for Non-Stationary MIMO Equalization(https://arxiv.org/abs/2510.08711)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Channel equalization is fundamental for mitigating distortions such as frequency-selective fading and inter-symbol interference. Unlike standard supervised learning approaches that require costly retraining or fine-tuning for each new task, in-context learning (ICL) adapts to new channels at inference time with only a few examples. However, existing ICL-based equalizers are primarily developed for and evaluated on static channels within the context window. Indeed, to our knowledge, prior principled analyses and theoretical studies of ICL focus exclusively on the stationary setting, where the function remains fixed within the context. In this paper, we investigate the ability of ICL to address non-stationary problems through the lens of time-varying channel equalization. We employ a principled framework for designing efficient attention mechanisms with improved adaptivity in non-stationary tasks, leveraging algorithms from adaptive signal processing to guide better designs. For example, new attention variants can be derived from the Least Mean Square (LMS) adaptive algorithm, a Least Root Mean Square (LRMS) formulation for enhanced robustness, or multi-step gradient updates for improved long-term tracking. Experimental results demonstrate that ICL holds strong promise for non-stationary MIMO equalization, and that attention mechanisms inspired by classical adaptive algorithms can substantially enhance adaptability and performance in dynamic environments. Our findings may provide critical insights for developing next-generation wireless foundation models with stronger adaptability and robustness.</li>
<li><strong>摘要：</strong>信道均衡对于减轻频率选择性衰落和符号间干扰等失真至关重要。与需要对每个新任务进行昂贵的再训练或微调的标准监督学习方法不同，上下文学习（ICL）只需几个示例即可在推理时适应新的通道。然而，现有的基于 ICL 的均衡器主要是针对上下文窗口内的静态通道开发和评估的。事实上，据我们所知，先前的 ICL 原理分析和理论研究仅集中于静止环境，即功能在环境中保持固定。在本文中，我们从时变通道均衡的角度研究了 ICL 解决非平稳问题的能力。我们采用一个原则框架来设计有效的注意力机制，提高非平稳任务的适应性，利用自适应信号处理的算法来指导更好的设计。例如，新的注意力变体可以从最小均方（LMS）自适应算法、用于增强鲁棒性的最小均方根（LRMS）公式或多步梯度更新以改进长期跟踪中导出。实验结果表明，ICL 在非平稳 MIMO 均衡方面具有强大的前景，并且受经典自适应算法启发的注意力机制可以显着增强动态环境中的适应性和性能。我们的研究结果可能为开发具有更强适应性和鲁棒性的下一代无线基础模型提供重要见解。</li>
</ul>

<h3>Title: LinearSR: Unlocking Linear Attention for Stable and Efficient Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Xiaohui Li, Shaobin Zhuang, Shuo Cao, Yang Yang, Yuandong Pu, Qi Qin, Siqi Luo, Bin Fu, Yihao Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.08771">https://arxiv.org/abs/2510.08771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.08771">https://arxiv.org/pdf/2510.08771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.08771]] LinearSR: Unlocking Linear Attention for Stable and Efficient Image Super-Resolution(https://arxiv.org/abs/2510.08771)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generative</a></li>
<li><strong>Abstract: </strong>Generative models for Image Super-Resolution (SR) are increasingly powerful, yet their reliance on self-attention's quadratic complexity (O(N^2)) creates a major computational bottleneck. Linear Attention offers an O(N) solution, but its promise for photorealistic SR has remained largely untapped, historically hindered by a cascade of interrelated and previously unsolved challenges. This paper introduces LinearSR, a holistic framework that, for the first time, systematically overcomes these critical hurdles. Specifically, we resolve a fundamental, training instability that causes catastrophic model divergence using our novel "knee point"-based Early-Stopping Guided Fine-tuning (ESGF) strategy. Furthermore, we mitigate the classic perception-distortion trade-off with a dedicated SNR-based Mixture of Experts (MoE) architecture. Finally, we establish an effective and lightweight guidance paradigm, TAG, derived from our "precision-over-volume" principle. Our resulting LinearSR model simultaneously delivers state-of-the-art perceptual quality with exceptional efficiency. Its core diffusion forward pass (1-NFE) achieves SOTA-level speed, while its overall multi-step inference time remains highly competitive. This work provides the first robust methodology for applying Linear Attention in the photorealistic SR domain, establishing a foundational paradigm for future research in efficient generative super-resolution.</li>
<li><strong>摘要：</strong>图像超分辨率 (SR) 的生成模型越来越强大，但它们对自注意力的二次复杂度 (O(N^2)) 的依赖造成了主要的计算瓶颈。线性注意力机制提供了一种 O(N) 解决方案，但其对真实感 SR 的承诺在很大程度上仍未得到开发，历史上一直受到一系列相互关联且先前未解决的挑战的阻碍。本文介绍了 LinearSR，这是一个整体框架，首次系统地克服了这些关键障碍。具体来说，我们使用我们新颖的基于“拐点”的早期停止引导微调（ESGF）策略解决了导致灾难性模型发散的根本训练不稳定性。此外，我们通过专用的基于 SNR 的专家混合 (MoE) 架构减轻了经典的感知失真权衡。最后，我们建立了一个有效且轻量级的制导范式，TAG，源自我们的“精度超过体积”原则。我们生成的 LinearSR 模型同时提供最先进的感知质量和卓越的效率。其核心扩散前向传递（1-NFE）实现了SOTA级别的速度，而其整体多步推理时间仍然具有很强的竞争力。这项工作提供了第一个在真实感 SR 领域应用线性注意力的稳健方法，为未来高效生成超分辨率的研究奠定了基础范例。</li>
</ul>

<h3>Title: Q-Router: Agentic Video Quality Assessment with Expert Model Routing and Artifact Localization</h3>
<ul>
<li><strong>Authors: </strong>Shuo Xing, Soumik Dey, Mingyang Wu, Ashirbad Mishra, Hansi Wu, Binbin Li, Zhengzhong Tu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.08789">https://arxiv.org/abs/2510.08789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.08789">https://arxiv.org/pdf/2510.08789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.08789]] Q-Router: Agentic Video Quality Assessment with Expert Model Routing and Artifact Localization(https://arxiv.org/abs/2510.08789)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, quality assessment</a></li>
<li><strong>Abstract: </strong>Video quality assessment (VQA) is a fundamental computer vision task that aims to predict the perceptual quality of a given video in alignment with human judgments. Existing performant VQA models trained with direct score supervision suffer from (1) poor generalization across diverse content and tasks, ranging from user-generated content (UGC), short-form videos, to AI-generated content (AIGC), (2) limited interpretability, and (3) lack of extensibility to novel use cases or content types. We propose Q-Router, an agentic framework for universal VQA with a multi-tier model routing system. Q-Router integrates a diverse set of expert models and employs vision--language models (VLMs) as real-time routers that dynamically reason and then ensemble the most appropriate experts conditioned on the input video semantics. We build a multi-tiered routing system based on the computing budget, with the heaviest tier involving a specific spatiotemporal artifacts localization for interpretability. This agentic design enables Q-Router to combine the complementary strengths of specialized experts, achieving both flexibility and robustness in delivering consistent performance across heterogeneous video sources and tasks. Extensive experiments demonstrate that Q-Router matches or surpasses state-of-the-art VQA models on a variety of benchmarks, while substantially improving generalization and interpretability. Moreover, Q-Router excels on the quality-based question answering benchmark, Q-Bench-Video, highlighting its promise as a foundation for next-generation VQA systems. Finally, we show that Q-Router capably localizes spatiotemporal artifacts, showing potential as a reward function for post-training video generation models.</li>
<li><strong>摘要：</strong>视频质量评估（VQA）是一项基本的计算机视觉任务，旨在根据人类判断来预测给定视频的感知质量。现有的通过直接评分监督训练的高性能 VQA 模型存在以下问题：(1) 对不同内容和任务的泛化性较差，从用户生成内容 (UGC)、短格式视频到人工智能生成内容 (AIGC)；(2) 可解释性有限；(3) 缺乏对新用例或内容类型的可扩展性。我们提出了 Q-Router，一种具有多层模型路由系统的通用 VQA 代理框架。 Q-Router 集成了一组不同的专家模型，并采用视觉语言模型 (VLM) 作为实时路由器，动态推理，然后根据输入视频语义集成最合适的专家。我们根据计算预算构建了一个多层路由系统，其中最重的一层涉及特定的时空工件本地化以实现可解释性。这种代理设计使 Q-Router 能够结合专业专家的互补优势，实现跨异构视频源和任务提供一致性能的灵活性和稳健性。大量实验表明，Q-Router 在各种基准测试中均匹配或超越了最先进的 VQA 模型，同时大幅提高了泛化性和可解释性。此外，Q-Router 在基于质量的问答基准 Q-Bench-Video 上表现出色，凸显了其作为下一代 VQA 系统基础的承诺。最后，我们证明 Q-Router 能够定位时空伪像，显示出作为训练后视频生成模型的奖励函数的潜力。</li>
</ul>

<h3>Title: SkipSR: Faster Super Resolution with Token Skipping</h3>
<ul>
<li><strong>Authors: </strong>Rohan Choudhury, Shanchuan Lin, Jianyi Wang, Hao Chen, Qi Zhao, Feng Cheng, Lu Jiang, Kris Kitani, Laszlo A. Jeni</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.08799">https://arxiv.org/abs/2510.08799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.08799">https://arxiv.org/pdf/2510.08799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.08799]] SkipSR: Faster Super Resolution with Token Skipping(https://arxiv.org/abs/2510.08799)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, super-resolution, generation</a></li>
<li><strong>Abstract: </strong>Diffusion-based super-resolution (SR) is a key component in video generation and video restoration, but is slow and expensive, limiting scalability to higher resolutions and longer videos. Our key insight is that many regions in video are inherently low-detail and gain little from refinement, yet current methods process all pixels uniformly. To take advantage of this, we propose SkipSR, a simple framework for accelerating video SR by identifying low-detail regions directly from low-resolution input, then skipping computation on them entirely, only super-resolving the areas that require refinement. This simple yet effective strategy preserves perceptual quality in both standard and one-step diffusion SR models while significantly reducing computation. In standard SR benchmarks, our method achieves up to 60% faster end-to-end latency than prior models on 720p videos with no perceptible loss in quality. Video demos are available at this https URL</li>
<li><strong>摘要：</strong>基于扩散的超分辨率 (SR) 是视频生成和视频恢复的关键组成部分，但速度慢且成本高，限制了更高分辨率和更长视频的可扩展性。我们的主要见解是，视频中的许多区域本质上细节较少，并且从细化中获得的收益很少，但当前的方法统一处理所有像素。为了利用这一点，我们提出了 SkipSR，这是一个简单的框架，用于通过直接从低分辨率输入识别低细节区域来加速视频 SR，然后完全跳过对它们的计算，仅对需要细化的区域进行超分辨率。这一简单而有效的策略在标准和一步扩散 SR 模型中保留了感知质量，同时显着减少了计算量。在标准 SR 基准测试中，我们的方法在 720p 视频上实现了比之前模型快 60% 的端到端延迟，并且没有明显的质量损失。视频演示可在此 https URL 获取</li>
</ul>

<h3>Title: A Frequency-Domain Analysis of the Multi-Armed Bandit Problem: A New Perspective on the Exploration-Exploitation Trade-off</h3>
<ul>
<li><strong>Authors: </strong>Di Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IT, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.08908">https://arxiv.org/abs/2510.08908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.08908">https://arxiv.org/pdf/2510.08908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.08908]] A Frequency-Domain Analysis of the Multi-Armed Bandit Problem: A New Perspective on the Exploration-Exploitation Trade-off(https://arxiv.org/abs/2510.08908)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The stochastic multi-armed bandit (MAB) problem is one of the most fundamental models in sequential decision-making, with the core challenge being the trade-off between exploration and exploitation. Although algorithms such as Upper Confidence Bound (UCB) and Thompson Sampling, along with their regret theories, are well-established, existing analyses primarily operate from a time-domain and cumulative regret perspective, struggling to characterize the dynamic nature of the learning process. This paper proposes a novel frequency-domain analysis framework, reformulating the bandit process as a signal processing problem. Within this framework, the reward estimate of each arm is viewed as a spectral component, with its uncertainty corresponding to the component's frequency, and the bandit algorithm is interpreted as an adaptive filter. We construct a formal Frequency-Domain Bandit Model and prove the main theorem: the confidence bound term in the UCB algorithm is equivalent in the frequency domain to a time-varying gain applied to uncertain spectral components, a gain inversely proportional to the square root of the visit count. Based on this, we further derive finite-time dynamic bounds concerning the exploration rate decay. This theory not only provides a novel and intuitive physical interpretation for classical algorithms but also lays a rigorous theoretical foundation for designing next-generation algorithms with adaptive parameter adjustment.</li>
<li><strong>摘要：</strong>随机多臂老虎机（MAB）问题是顺序决策中最基本的模型之一，其核心挑战是探索和利用之间的权衡。尽管上置信界（UCB）和汤普森采样等算法及其后悔理论已经很成熟，但现有的分析主要从时域和累积后悔的角度进行操作，难以描述学习过程的动态性质。本文提出了一种新颖的频域分析框架，将老虎机过程重新表述为信号处理问题。在此框架内，每个臂的奖励估计被视为频谱分量，其不确定性对应于该分量的频率，而老虎机算法被解释为自适应滤波器。我们构建了一个正式的频域强盗模型并证明了主要定理：UCB 算法中的置信界项在频域中相当于应用于不确定频谱分量的时变增益，该增益与访问计数的平方根成反比。在此基础上，我们进一步推导了有关探索率衰减的有限时间动态界限。该理论不仅为经典算法提供了新颖、直观的物理解释，而且为设计下一代自适应参数调整算法奠定了严格的理论基础。</li>
</ul>

<h3>Title: Defense against Unauthorized Distillation in Image Restoration via Feature Space Perturbation</h3>
<ul>
<li><strong>Authors: </strong>Han Hu, Zhuoran Zheng, Chen Lyu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.08925">https://arxiv.org/abs/2510.08925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.08925">https://arxiv.org/pdf/2510.08925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.08925]] Defense against Unauthorized Distillation in Image Restoration via Feature Space Perturbation(https://arxiv.org/abs/2510.08925)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, super-resolution, generative</a></li>
<li><strong>Abstract: </strong>Knowledge distillation (KD) attacks pose a significant threat to deep model intellectual property by enabling adversaries to train student networks using a teacher model's outputs. While recent defenses in image classification have successfully disrupted KD by perturbing output probabilities, extending these methods to image restoration is difficult. Unlike classification, restoration is a generative task with continuous, high-dimensional outputs that depend on spatial coherence and fine details. Minor perturbations are often insufficient, as students can still learn the underlying this http URL address this, we propose Adaptive Singular Value Perturbation (ASVP), a runtime defense tailored for image restoration models. ASVP operates on internal feature maps of the teacher using singular value decomposition (SVD). It amplifies the topk singular values to inject structured, high-frequency perturbations, disrupting the alignment needed for distillation. This hinders student learning while preserving the teacher's output this http URL evaluate ASVP across five image restoration tasks: super-resolution, low-light enhancement, underwater enhancement, dehazing, and deraining. Experiments show ASVP reduces student PSNR by up to 4 dB and SSIM by 60-75%, with negligible impact on the teacher's performance. Compared to prior methods, ASVP offers a stronger and more consistent this http URL approach provides a practical solution to protect open-source restoration models from unauthorized knowledge distillation.</li>
<li><strong>摘要：</strong>知识蒸馏（KD）攻击使对手能够使用教师模型的输出来训练学生网络，从而对深度模型知识产权构成重大威胁。虽然最近图像分类中的防御措施已经通过扰动输出概率成功地破坏了 KD，但将这些方法扩展到图像恢复是很困难的。与分类不同，恢复是一项生成任务，具有连续的高维输出，取决于空间相干性和精细细节。微小的扰动通常是不够的，因为学生仍然可以了解该 http URL 地址的底层，因此我们提出了自适应奇异值扰动（ASVP），这是一种为图像恢复模型量身定制的运行时防御。 ASVP 使用奇异值分解 (SVD) 对教师的内部特征图进行操作。它放大 topk 奇异值以注入结构化的高频扰动，破坏蒸馏所需的对齐。这会阻碍学生的学习，同时保留教师的输出。此 http URL 在五个图像恢复任务中评估 ASVP：超分辨率、低光增强、水下增强、去雾和去雨。实验表明，ASVP 可将学生 PSNR 降低多达 4 dB，将 SSIM 降低 60-75%，而对教师表现的影响可以忽略不计。与之前的方法相比，ASVP 提供了更强大、更一致的 http URL 方法，为保护开源恢复模型免受未经授权的知识蒸馏提供了实用的解决方案。</li>
</ul>

<h3>Title: MATT-CTR: Unleashing a Model-Agnostic Test-Time Paradigm for CTR Prediction with Confidence-Guided Inference Paths</h3>
<ul>
<li><strong>Authors: </strong>Moyu Zhang, Yun Chen, Yujun Jin, Jinxin Hu, Yu Zhang, Xiaoyi Zeng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.08932">https://arxiv.org/abs/2510.08932</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.08932">https://arxiv.org/pdf/2510.08932</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.08932]] MATT-CTR: Unleashing a Model-Agnostic Test-Time Paradigm for CTR Prediction with Confidence-Guided Inference Paths(https://arxiv.org/abs/2510.08932)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recently, a growing body of research has focused on either optimizing CTR model architectures to better model feature interactions or refining training objectives to aid parameter learning, thereby achieving better predictive performance. However, previous efforts have primarily focused on the training phase, largely neglecting opportunities for optimization during the inference phase. Infrequently occurring feature combinations, in particular, can degrade prediction performance, leading to unreliable or low-confidence outputs. To unlock the predictive potential of trained CTR models, we propose a Model-Agnostic Test-Time paradigm (MATT), which leverages the confidence scores of feature combinations to guide the generation of multiple inference paths, thereby mitigating the influence of low-confidence features on the final prediction. Specifically, to quantify the confidence of feature combinations, we introduce a hierarchical probabilistic hashing method to estimate the occurrence frequencies of feature combinations at various orders, which serve as their corresponding confidence scores. Then, using the confidence scores as sampling probabilities, we generate multiple instance-specific inference paths through iterative sampling and subsequently aggregate the prediction scores from multiple paths to conduct robust predictions. Finally, extensive offline experiments and online A/B tests strongly validate the compatibility and effectiveness of MATT across existing CTR models.</li>
<li><strong>摘要：</strong>最近，越来越多的研究集中在优化 CTR 模型架构以更好地建模特征交互或细化训练目标以帮助参数学习，从而实现更好的预测性能。然而，之前的努力主要集中在训练阶段，很大程度上忽略了推理阶段的优化机会。特别是不经常出现的特征组合可能会降低预测性能，导致输出不可靠或置信度低。为了释放经过训练的 CTR 模型的预测潜力，我们提出了一种与模型无关的测试时间范式（MATT），它利用特征组合的置信度分数来指导多个推理路径的生成，从而减轻低置信度特征对最终预测的影响。具体来说，为了量化特征组合的置信度，我们引入了一种分层概率哈希方法来估计不同顺序的特征组合的出现频率，作为它们相应的置信度得分。然后，使用置信度分数作为采样概率，通过迭代采样生成多个特定于实例的推理路径，并随后聚合来自多个路径的预测分数以进行稳健的预测。最后，大量的离线实验和在线 A/B 测试有力地验证了 MATT 在现有 CTR 模型中的兼容性和有效性。</li>
</ul>

<h3>Title: Hierarchical Scheduling for Multi-Vector Image Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Maoliang Li, Ke Li, Yaoyang Liu, Jiayu Chen, Zihao Zheng, Yinjun Wu, Xiang Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.DC, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.08976">https://arxiv.org/abs/2510.08976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.08976">https://arxiv.org/pdf/2510.08976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.08976]] Hierarchical Scheduling for Multi-Vector Image Retrieval(https://arxiv.org/abs/2510.08976)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>To effectively leverage user-specific data, retrieval augmented generation (RAG) is employed in multimodal large language model (MLLM) applications. However, conventional retrieval approaches often suffer from limited retrieval accuracy. Recent advances in multi-vector retrieval (MVR) improve accuracy by decomposing queries and matching against segmented images. They still suffer from sub-optimal accuracy and efficiency, overlooking alignment between the query and varying image objects and redundant fine-grained image segments. In this work, we present an efficient scheduling framework for image retrieval - HiMIR. First, we introduce a novel hierarchical paradigm, employing multiple intermediate granularities for varying image objects to enhance alignment. Second, we minimize redundancy in retrieval by leveraging cross-hierarchy similarity consistency and hierarchy sparsity to minimize unnecessary matching computation. Furthermore, we configure parameters for each dataset automatically for practicality across diverse scenarios. Our empirical study shows that, HiMIR not only achieves substantial accuracy improvements but also reduces computation by up to 3.5 times over the existing MVR system.</li>
<li><strong>摘要：</strong>为了有效地利用用户特定的数据，在多模式大语言模型 (MLLM) 应用程序中采用了检索增强生成 (RAG)。然而，传统的检索方法往往检索精度有限。多向量检索 (MVR) 的最新进展通过分解查询和匹配分段图像来提高准确性。它们仍然面临着次优的准确性和效率，忽视了查询与变化的图像对象以及冗余的细粒度图像片段之间的对齐。在这项工作中，我们提出了一种高效的图像检索调度框架 - HiMIR。首先，我们引入了一种新颖的分层范例，对不同的图像对象采用多个中间粒度来增强对齐。其次，我们通过利用跨层次相似性一致性和层次稀疏性来最小化检索中的冗余，以最小化不必要的匹配计算。此外，我们自动为每个数据集配置参数，以实现跨不同场景的实用性。我们的实证研究表明，HiMIR 不仅实现了精度的大幅提升，而且比现有 MVR 系统减少了高达 3.5 倍的计算量。</li>
</ul>

<h3>Title: HandEval: Taking the First Step Towards Hand Quality Evaluation in Generated Images</h3>
<ul>
<li><strong>Authors: </strong>Zichuan Wang, Bo Peng, Songlin Yang, Zhenchen Tang, Jing Dong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.08978">https://arxiv.org/abs/2510.08978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.08978">https://arxiv.org/pdf/2510.08978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.08978]] HandEval: Taking the First Step Towards Hand Quality Evaluation in Generated Images(https://arxiv.org/abs/2510.08978)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, quality assessment</a></li>
<li><strong>Abstract: </strong>Although recent text-to-image (T2I) models have significantly improved the overall visual quality of generated images, they still struggle in the generation of accurate details in complex local regions, especially human hands. Generated hands often exhibit structural distortions and unrealistic textures, which can be very noticeable even when the rest of the body is well-generated. However, the quality assessment of hand regions remains largely neglected, limiting downstream task performance like human-centric generation quality optimization and AIGC detection. To address this, we propose the first quality assessment task targeting generated hand regions and showcase its abundant downstream applications. We first introduce the HandPair dataset for training hand quality assessment models. It consists of 48k images formed by high- and low-quality hand pairs, enabling low-cost, efficient supervision without manual annotation. Based on it, we develop HandEval, a carefully designed hand-specific quality assessment model. It leverages the powerful visual understanding capability of Multimodal Large Language Model (MLLM) and incorporates prior knowledge of hand keypoints, gaining strong perception of hand quality. We further construct a human-annotated test set with hand images from various state-of-the-art (SOTA) T2I models to validate its quality evaluation capability. Results show that HandEval aligns better with human judgments than existing SOTA methods. Furthermore, we integrate HandEval into image generation and AIGC detection pipelines, prominently enhancing generated hand realism and detection accuracy, respectively, confirming its universal effectiveness in downstream applications. Code and dataset will be available.</li>
<li><strong>摘要：</strong>尽管最近的文本到图像（T2I）模型显着提高了生成图像的整体视觉质量，但它们仍然难以在复杂的局部区域（尤其是人手）生成准确的细节。生成的手通常会表现出结构扭曲和不切实际的纹理，即使身体的其他部分生成良好，这也可能非常明显。然而，手部区域的质量评估仍然在很大程度上被忽视，限制了下游任务的性能，例如以人为中心的生成质量优化和 AIGC 检测。为了解决这个问题，我们提出了第一个针对生成的手部区域的质量评估任务，并展示了其丰富的下游应用。我们首先介绍用于训练手部质量评估模型的 HandPair 数据集。它由高质量和低质量的手对形成的 48k 图像组成，无需手动注释即可实现低成本、高效的监督。在此基础上，我们开发了HandEval，一个精心设计的针对手部的质量评估模型。它利用多模态大语言模型（MLLM）强大的视觉理解能力，并结合手部关键点的先验知识，获得对手部质量的强烈感知。我们进一步使用来自各种最先进（SOTA）T2I模型的手部图像构建了人类注释的测试集，以验证其质量评估能力。结果表明，与现有的 SOTA 方法相比，HandEval 更符合人类判断。此外，我们将 HandEval 集成到图像生成和 AIGC 检测管道中，分别显着增强了生成的手的真实感和检测精度，证实了其在下游应用中的普遍有效性。代码和数据集将可用。</li>
</ul>

<h3>Title: Uncolorable Examples: Preventing Unauthorized AI Colorization via Perception-Aware Chroma-Restrictive Perturbation</h3>
<ul>
<li><strong>Authors: </strong>Yuki Nii, Futa Waseda, Ching-Chun Chang, Isao Echizen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.08979">https://arxiv.org/abs/2510.08979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.08979">https://arxiv.org/pdf/2510.08979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.08979]] Uncolorable Examples: Preventing Unauthorized AI Colorization via Perception-Aware Chroma-Restrictive Perturbation(https://arxiv.org/abs/2510.08979)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>AI-based colorization has shown remarkable capability in generating realistic color images from grayscale inputs. However, it poses risks of copyright infringement -- for example, the unauthorized colorization and resale of monochrome manga and films. Despite these concerns, no effective method currently exists to prevent such misuse. To address this, we introduce the first defensive paradigm, Uncolorable Examples, which embed imperceptible perturbations into grayscale images to invalidate unauthorized colorization. To ensure real-world applicability, we establish four criteria: effectiveness, imperceptibility, transferability, and robustness. Our method, Perception-Aware Chroma-Restrictive Perturbation (PAChroma), generates Uncolorable Examples that meet these four criteria by optimizing imperceptible perturbations with a Laplacian filter to preserve perceptual quality, and applying diverse input transformations during optimization to enhance transferability across models and robustness against common post-processing (e.g., compression). Experiments on ImageNet and Danbooru datasets demonstrate that PAChroma effectively degrades colorization quality while maintaining the visual appearance. This work marks the first step toward protecting visual content from illegitimate AI colorization, paving the way for copyright-aware defenses in generative media.</li>
<li><strong>摘要：</strong>基于人工智能的着色在从灰度输入生成逼真的彩色图像方面表现出了卓越的能力。然而，它带来了版权侵权的风险——例如，未经授权的彩色化和转售单色漫画和电影。尽管存在这些担忧，但目前还没有有效的方法来防止这种滥用。为了解决这个问题，我们引入了第一个防御范例，不可着色示例，它将难以察觉的扰动嵌入到灰度图像中，以使未经授权的着色无效。为了确保现实世界的适用性，我们建立了四个标准：有效性、不可察觉性、可转移性和稳健性。我们的方法，感知感知色度限制性扰动（PAChroma），通过使用拉普拉斯滤波器优化难以察觉的扰动以保持感知质量，并在优化过程中应用不同的输入转换来增强跨模型的可转移性和针对常见后处理（例如压缩）的鲁棒性，从而生成满足这四个标准的不可着色示例。在 ImageNet 和 Danbooru 数据集上进行的实验表明，PAChroma 在保持视觉外观的同时有效降低了着色质量。这项工作标志着保护视觉内容免受非法人工智能着色的第一步，为生成媒体中的版权意识防御铺平了道路。</li>
</ul>

<h3>Title: Constraints-of-Thought: A Framework for Constrained Reasoning in Language-Model-Guided Search</h3>
<ul>
<li><strong>Authors: </strong>Kamel Alrashedy, Vriksha Srihari, Zulfiqar Zaidi, Ridam Srivastava, Pradyumna Tambwekar, Matthew Gombolay</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.08992">https://arxiv.org/abs/2510.08992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.08992">https://arxiv.org/pdf/2510.08992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.08992]] Constraints-of-Thought: A Framework for Constrained Reasoning in Language-Model-Guided Search(https://arxiv.org/abs/2510.08992)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While researchers have made significant progress in enabling large language models (LLMs) to perform multi-step planning, LLMs struggle to ensure that those plans align with high-level user intent and satisfy symbolic constraints, especially in complex, multi-step domains. Existing reasoning approaches such as Chain-of-Thought (CoT), Tree-of-Thought (ToT), and verifier-augmented methods, expand the search space but often yield infeasible actions or hallucinated steps. To overcome these limitations, we propose Constraints-of-Thought (Const-o-T), a framework that provides a structured prior that enables Monte Carlo Tree Search (MCTS) focus search on semantically meaningful paths. Each reasoning step is represented as an (intent, constraint) pair, which serves both to compress the search space and enforce validity. Unlike prior methods that merely generate reasoning traces or validate outputs post hoc, Const-o-T uses (intent, constraint)pairs to actively focus the search toward feasible and meaningful plans. We integrate Const-o-T into MCTS using a structured representation of intent-constraint pairs constraints prune infeasible branches and guide exploration toward semantically valid actions, improving planning efficiency and verifiable decision-making. We demonstrate across three domains Risk game, CAD code generation, and arithmetic reasoning that our approach outperforms baselines, yielding higher accuracy and stronger structural alignment. Our contribution is to demonstrate that Const-of-T offers a generalizable foundation for constraint-guided reasoning, enabling more efficient, constraint-aligned, and domain-adaptable planning with LLMs.</li>
<li><strong>摘要：</strong>尽管研究人员在使大型语言模型 (LLM) 执行多步骤规划方面取得了重大进展，但 LLM 仍难以确保这些计划符合高级用户意图并满足符号约束，尤其是在复杂的多步骤领域。现有的推理方法，例如思想链（CoT）、思想树（ToT）和验证者增强方法，扩大了搜索空间，但经常产生不可行的动作或幻觉步骤。为了克服这些限制，我们提出了 Constraints-of-Thought (Const-o-T)，这是一个提供结构化先验的框架，使蒙特卡洛树搜索 (MCTS) 能够将搜索集中在语义上有意义的路径上。每个推理步骤都表示为一个（意图，约束）对，它既可以压缩搜索空间，又可以增强有效性。与仅生成推理轨迹或事后验证输出的先前方法不同，Const-o-T 使用（意图、约束）对主动将搜索集中于可行且有意义的计划。我们使用意图约束对的结构化表示将 Const-o-T 集成到 MCTS 中，修剪不可行的分支并指导探索语义上有效的操作，提高规划效率和可验证的决策。我们在风险博弈、CAD 代码生成和算术推理三个领域证明，我们的方法优于基线，产生更高的准确性和更强的结构对齐。我们的贡献是证明 Const-of-T 为约束引导推理提供了通用的基础，从而使法学硕士能够实现更高效、约束对齐和领域适应性的规划。</li>
</ul>

<h3>Title: Speculative Jacobi-Denoising Decoding for Accelerating Autoregressive Text-to-image Generation</h3>
<ul>
<li><strong>Authors: </strong>Yao Teng, Fuyun Wang, Xian Liu, Zhekai Chen, Han Shi, Yu Wang, Zhenguo Li, Weiyang Liu, Difan Zou, Xihui Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.08994">https://arxiv.org/abs/2510.08994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.08994">https://arxiv.org/pdf/2510.08994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.08994]] Speculative Jacobi-Denoising Decoding for Accelerating Autoregressive Text-to-image Generation(https://arxiv.org/abs/2510.08994)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>As a new paradigm of visual content generation, autoregressive text-to-image models suffer from slow inference due to their sequential token-by-token decoding process, often requiring thousands of model forward passes to generate a single image. To address this inefficiency, we propose Speculative Jacobi-Denoising Decoding (SJD2), a framework that incorporates the denoising process into Jacobi iterations to enable parallel token generation in autoregressive models. Our method introduces a next-clean-token prediction paradigm that enables the pre-trained autoregressive models to accept noise-perturbed token embeddings and predict the next clean tokens through low-cost fine-tuning. This denoising paradigm guides the model towards more stable Jacobi trajectories. During inference, our method initializes token sequences with Gaussian noise and performs iterative next-clean-token-prediction in the embedding space. We employ a probabilistic criterion to verify and accept multiple tokens in parallel, and refine the unaccepted tokens for the next iteration with the denoising trajectory. Experiments show that our method can accelerate generation by reducing model forward passes while maintaining the visual quality of generated images.</li>
<li><strong>摘要：</strong>作为视觉内容生成的新范式，自回归文本到图像模型由于其顺序逐个令牌解码过程而受到推理缓慢的困扰，通常需要数千次模型前向传递才能生成单个图像。为了解决这种低效率问题，我们提出了推测雅可比去噪解码（SJD2），这是一个将去噪过程合并到雅可比迭代中的框架，以在自回归模型中实现并行标记生成。我们的方法引入了下一个干净令牌预测范例，使预训练的自回归模型能够接受噪声扰动的令牌嵌入，并通过低成本微调来预测下一个干净令牌。这种去噪范式引导模型走向更稳定的雅可比轨迹。在推理过程中，我们的方法使用高斯噪声初始化令牌序列，并在嵌入空间中执行迭代的下一个干净令牌预测。我们采用概率标准来并行验证和接受多个令牌，并使用去噪轨迹为下一次迭代细化未接受的令牌。实验表明，我们的方法可以通过减少模型前向传递来加速生成，同时保持生成图像的视觉质量。</li>
</ul>

<h3>Title: LLM Unlearning on Noisy Forget Sets: A Study of Incomplete, Rewritten, and Watermarked Data</h3>
<ul>
<li><strong>Authors: </strong>Changsheng Wang, Yihua Zhang, Dennis Wei, Jinghan Jia, Pin-Yu Chen, Sijia Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09007">https://arxiv.org/abs/2510.09007</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09007">https://arxiv.org/pdf/2510.09007</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09007]] LLM Unlearning on Noisy Forget Sets: A Study of Incomplete, Rewritten, and Watermarked Data(https://arxiv.org/abs/2510.09007)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) exhibit remarkable generative capabilities but raise ethical and security concerns by memorizing sensitive data, reinforcing biases, and producing harmful content. These risks have spurred interest in LLM unlearning, the task of removing knowledge associated with undesirable data from pre-trained models. However, most existing methods assume access to clean, well-defined forget data samples, whereas real-world forget data could often be low-quality, synthetically rewritten, or watermarked, casting doubt on the reliability of unlearning. This work presents the first study of unlearning under perturbed or low-fidelity forget data, referred to as noisy forget sets. By systematically benchmarking state-of-the-art LLM unlearning methods, RMU and NPO, on such noisy forget sets, we find that unlearning remains surprisingly robust to perturbations, provided that core semantic signals are preserved. To explain this robustness, we propose a saliency-based interpretation: key semantic components that drive forgetting remain consistently influential despite substantial variation in surface form. This suggests that unlearning algorithms are primarily guided by deep semantic cues rather than shallow lexical patterns.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 表现出卓越的生成能力，但通过记忆敏感数据、强化偏见和生成有害内容，引发了道德和安全问题。这些风险激发了人们对法学硕士取消学习的兴趣，即从预先训练的模型中删除与不需要的数据相关的知识的任务。然而，大多数现有方法都假设可以访问干净、定义明确的遗忘数据样本，而现实世界的遗忘数据通常可能是低质量的、综合重写的或带有水印的，这使人对遗忘的可靠性产生怀疑。这项工作提出了第一个在扰动或低保真度遗忘数据（称为噪声遗忘集）下遗忘的研究。通过在这种嘈杂的遗忘集上系统地对最先进的 LLM 忘却方法、RMU 和 NPO 进行基准测试，我们发现，只要保留核心语义信号，忘却对于扰动仍然具有惊人的鲁棒性。为了解释这种鲁棒性，我们提出了一种基于显着性的解释：尽管表面形式存在很大变化，但驱动遗忘的关键语义成分仍然具有持续的影响力。这表明遗忘算法主要受深层语义线索而非浅层词汇模式的指导。</li>
</ul>

<h3>Title: Towards Better & Faster Autoregressive Image Generation: From the Perspective of Entropy</h3>
<ul>
<li><strong>Authors: </strong>Xiaoxiao Ma, Feng Zhao, Pengyang Ling, Haibo Qiu, Zhixiang Wei, Hu Yu, Jie Huang, Zhixiong Zeng, Lin Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09012">https://arxiv.org/abs/2510.09012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09012">https://arxiv.org/pdf/2510.09012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09012]] Towards Better & Faster Autoregressive Image Generation: From the Perspective of Entropy(https://arxiv.org/abs/2510.09012)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In this work, we first revisit the sampling issues in current autoregressive (AR) image generation models and identify that image tokens, unlike text tokens, exhibit lower information density and non-uniform spatial distribution. Accordingly, we present an entropy-informed decoding strategy that facilitates higher autoregressive generation quality with faster synthesis speed. Specifically, the proposed method introduces two main innovations: 1) dynamic temperature control guided by spatial entropy of token distributions, enhancing the balance between content diversity, alignment accuracy, and structural coherence in both mask-based and scale-wise models, without extra computational overhead, and 2) entropy-aware acceptance rules in speculative decoding, achieving near-lossless generation at about 85\% of the inference cost of conventional acceleration methods. Extensive experiments across multiple benchmarks using diverse AR image generation models demonstrate the effectiveness and generalizability of our approach in enhancing both generation quality and sampling speed.</li>
<li><strong>摘要：</strong>在这项工作中，我们首先重新审视当前自回归（AR）图像生成模型中的采样问题，并确定图像标记与文本标记不同，表现出较低的信息密度和不均匀的空间分布。因此，我们提出了一种基于熵的解码策略，该策略有助于提高自回归生成质量和更快的合成速度。具体来说，该方法引入了两个主要创新：1）由令牌分布的空间熵引导的动态温度控制，增强了基于掩模和尺度模型中内容多样性、对齐精度和结构一致性之间的平衡，而无需额外的计算开销；2）推测解码中的熵感知接受规则，以传统推理成本的约 85% 实现近乎无损的生成。 加速方法。使用不同的 AR 图像生成模型在多个基准上进行的广泛实验证明了我们的方法在提高生成质量和采样速度方面的有效性和通用性。</li>
</ul>

<h3>Title: MagicDock: Toward Docking-oriented De Novo Ligand Design via Gradient Inversion</h3>
<ul>
<li><strong>Authors: </strong>Zekai Chen, Xunkai Li, Sirui Zhang, Henan Sun, Jia Li, Zhenjun Li, Bing Zhou, Rong-Hua Li, Guoren Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09020">https://arxiv.org/abs/2510.09020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09020">https://arxiv.org/pdf/2510.09020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09020]] MagicDock: Toward Docking-oriented De Novo Ligand Design via Gradient Inversion(https://arxiv.org/abs/2510.09020)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>De novo ligand design is a fundamental task that seeks to generate protein or molecule candidates that can effectively dock with protein receptors and achieve strong binding affinity entirely from scratch. It holds paramount significance for a wide spectrum of biomedical applications. However, most existing studies are constrained by the \textbf{Pseudo De Novo}, \textbf{Limited Docking Modeling}, and \textbf{Inflexible Ligand Type}. To address these issues, we propose MagicDock, a forward-looking framework grounded in the progressive pipeline and differentiable surface modeling. (1) We adopt a well-designed gradient inversion framework. To begin with, general docking knowledge of receptors and ligands is incorporated into the backbone model. Subsequently, the docking knowledge is instantiated as reverse gradient flows by binding prediction, which iteratively guide the de novo generation of ligands. (2) We emphasize differentiable surface modeling in the docking process, leveraging learnable 3D point-cloud representations to precisely capture binding details, thereby ensuring that the generated ligands preserve docking validity through direct and interpretable spatial fingerprints. (3) We introduce customized designs for different ligand types and integrate them into a unified gradient inversion framework with flexible triggers, thereby ensuring broad applicability. Moreover, we provide rigorous theoretical guarantees for each component of MagicDock. Extensive experiments across 9 scenarios demonstrate that MagicDock achieves average improvements of 27.1\% and 11.7\% over SOTA baselines specialized for protein or molecule ligand design, respectively.</li>
<li><strong>摘要：</strong>从头配体设计是一项基本任务，旨在产生能够有效地与蛋白质受体对接并完全从头开始实现强结合亲和力的蛋白质或分子候选物。它对于广泛的生物医学应用具有至关重要的意义。然而，大多数现有研究受到\textbf{Pseudo De Novo}、\textbf{有限对接模型}和\textbf{不灵活配体类型}的限制。为了解决这些问题，我们提出了 MagicDock，这是一个基于渐进式管道和可微表面建模的前瞻性框架。 （1）我们采用精心设计的梯度反演框架。首先，将受体和配体的一般对接知识纳入主干模型中。随后，对接知识通过结合预测被实例化为反向梯度流，迭代地指导配体的从头生成。 (2)我们强调对接过程中的可微分表面建模，利用可学习的3D点云表示来精确捕获结合细节，从而确保生成的配体通过直接和可解释的空间指纹保持对接有效性。 （3）我们针对不同配体类型引入定制设计，并将其集成到具有灵活触发器的统一梯度反演框架中，从而确保广泛的适用性。而且，我们为MagicDock的每个组件提供了严格的理论保证。跨 9 个场景的广泛实验表明，MagicDock 比专门用于蛋白质或分子配体设计的 SOTA 基线分别实现了 27.1% 和 11.7% 的平均改进。</li>
</ul>

<h3>Title: Lesion-Aware Post-Training of Latent Diffusion Models for Synthesizing Diffusion MRI from CT Perfusion</h3>
<ul>
<li><strong>Authors: </strong>Junhyeok Lee, Hyunwoong Kim, Hyungjin Chung, Heeseong Eom, Joon Jang, Chul-Ho Sohn, Kyu Sung Choi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09056">https://arxiv.org/abs/2510.09056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09056">https://arxiv.org/pdf/2510.09056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09056]] Lesion-Aware Post-Training of Latent Diffusion Models for Synthesizing Diffusion MRI from CT Perfusion(https://arxiv.org/abs/2510.09056)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Image-to-Image translation models can help mitigate various challenges inherent to medical image acquisition. Latent diffusion models (LDMs) leverage efficient learning in compressed latent space and constitute the core of state-of-the-art generative image models. However, this efficiency comes with a trade-off, potentially compromising crucial pixel-level detail essential for high-fidelity medical images. This limitation becomes particularly critical when generating clinically significant structures, such as lesions, which often occupy only a small portion of the image. Failure to accurately reconstruct these regions can severely impact diagnostic reliability and clinical decision-making. To overcome this limitation, we propose a novel post-training framework for LDMs in medical image-to-image translation by incorporating lesion-aware medical pixel space objectives. This approach is essential, as it not only enhances overall image quality but also improves the precision of lesion delineation. We evaluate our framework on brain CT-to-MRI translation in acute ischemic stroke patients, where early and accurate diagnosis is critical for optimal treatment selection and improved patient outcomes. While diffusion MRI is the gold standard for stroke diagnosis, its clinical utility is often constrained by high costs and low accessibility. Using a dataset of 817 patients, we demonstrate that our framework improves overall image quality and enhances lesion delineation when synthesizing DWI and ADC images from CT perfusion scans, outperforming existing image-to-image translation models. Furthermore, our post-training strategy is easily adaptable to pre-trained LDMs and exhibits substantial potential for broader applications across diverse medical image translation tasks.</li>
<li><strong>摘要：</strong>图像到图像转换模型可以帮助缓解医学图像采集固有的各种挑战。潜在扩散模型（LDM）利用压缩潜在空间中的高效学习，构成了最先进的生成图像模型的核心。然而，这种效率是有代价的，可能会损害高保真医学图像所必需的关键像素级细节。当生成临床上重要的结构（例如通常仅占据图像的一小部分的病变）时，这种限制变得尤其重要。未能准确重建这些区域可能会严重影响诊断可靠性和临床决策。为了克服这一限制，我们通过结合病变感知的医学像素空间目标，为医学图像到图像转换中的 LDM 提出了一种新颖的后训练框架。这种方法至关重要，因为它不仅提高了整体图像质量，而且提高了病灶勾画的精度。我们评估了急性缺血性中风患者脑部 CT 到 MRI 转换的框架，其中早期准确的诊断对于最佳治疗选择和改善患者预后至关重要。虽然扩散 MRI 是中风诊断的金标准，但其临床实用性往往受到高成本和低可及性的限制。使用 817 名患者的数据集，我们证明了我们的框架在合成来自 CT 灌注扫描的 DWI 和 ADC 图像时提高了整体图像质量并增强了病灶勾画，优于现有的图像到图像转换模型。此外，我们的训练后策略可以轻松适应预先训练的 LDM，并在各种医学图像翻译任务中展现出更广泛应用的巨大潜力。</li>
</ul>

<h3>Title: Dense2MoE: Restructuring Diffusion Transformer to MoE for Efficient Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Youwei Zheng, Yuxi Ren, Xin Xia, Xuefeng Xiao, Xiaohua Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09094">https://arxiv.org/abs/2510.09094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09094">https://arxiv.org/pdf/2510.09094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09094]] Dense2MoE: Restructuring Diffusion Transformer to MoE for Efficient Text-to-Image Generation(https://arxiv.org/abs/2510.09094)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion Transformer (DiT) has demonstrated remarkable performance in text-to-image generation; however, its large parameter size results in substantial inference overhead. Existing parameter compression methods primarily focus on pruning, but aggressive pruning often leads to severe performance degradation due to reduced model capacity. To address this limitation, we pioneer the transformation of a dense DiT into a Mixture of Experts (MoE) for structured sparsification, reducing the number of activated parameters while preserving model capacity. Specifically, we replace the Feed-Forward Networks (FFNs) in DiT Blocks with MoE layers, reducing the number of activated parameters in the FFNs by 62.5\%. Furthermore, we propose the Mixture of Blocks (MoB) to selectively activate DiT blocks, thereby further enhancing sparsity. To ensure an effective dense-to-MoE conversion, we design a multi-step distillation pipeline, incorporating Taylor metric-based expert initialization, knowledge distillation with load balancing, and group feature loss for MoB optimization. We transform large diffusion transformers (e.g., FLUX.1 [dev]) into an MoE structure, reducing activated parameters by 60\% while maintaining original performance and surpassing pruning-based approaches in extensive experiments. Overall, Dense2MoE establishes a new paradigm for efficient text-to-image generation.</li>
<li><strong>摘要：</strong>Diffusion Transformer (DiT) 在文本到图像生成方面表现出了卓越的性能；然而，其较大的参数大小会导致大量的推理开销。现有的参数压缩方法主要集中在剪枝上，但激进的剪枝往往会因模型容量减少而导致性能严重下降。为了解决这一限制，我们率先将密集 DiT 转换为专家混合 (MoE) 以进行结构化稀疏化，减少激活参数的数量，同时保留模型容量。具体来说，我们用 MoE 层替换 DiT 块中的前馈网络（FFN），将 FFN 中激活参数的数量减少 62.5%。此外，我们提出了块混合（MoB）来选择性地激活 DiT 块，从而进一步增强稀疏性。为了确保有效的密集到 MoE 转换，我们设计了一个多步骤蒸馏管道，结合了基于泰勒度量的专家初始化、具有负载平衡的知识蒸馏以及用于 MoB 优化的组特征损失。我们将大型扩散变压器（例如 FLUX.1 [dev]）转换为 MoE 结构，将激活参数减少 60%，同时保持原始性能并在大量实验中超越基于剪枝的方法。总体而言，Dense2MoE 建立了高效文本到图像生成的新范例。</li>
</ul>

<h3>Title: SOS: Synthetic Object Segments Improve Detection, Segmentation, and Grounding</h3>
<ul>
<li><strong>Authors: </strong>Weikai Huang, Jieyu Zhang, Taoyang Jia, Chenhao Zheng, Ziqi Gao, Jae Sung Park, Ranjay Krishna</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09110">https://arxiv.org/abs/2510.09110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09110">https://arxiv.org/pdf/2510.09110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09110]] SOS: Synthetic Object Segments Improve Detection, Segmentation, and Grounding(https://arxiv.org/abs/2510.09110)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Visual grouping -- operationalized via instance segmentation, visual grounding, and object detection -- underpins applications from robotic perception to photo editing. Large annotated datasets are costly, biased in coverage, and hard to scale. Synthetic data are promising but often lack flexibility, accuracy, and compositional diversity. We present SOS, a simple and scalable data synthesis pipeline based on an object-centric composition strategy. It pastes high-quality synthetic object segments into new images using structured layout priors and generative relighting, producing accurate and diverse masks, boxes, and referring expressions. Models trained on 100000 synthetic images from SOS outperform those trained on larger real-image datasets such as GRIT (20M) and V3Det (200K) on detection and grounding tasks, achieving +10.9 AP on LVIS detection and +8.4 $N_{\text{Acc}}$ on gRefCOCO grounding. SOS enables controllable dataset construction and improves generalization in both low-data and closed-vocabulary settings. Augmenting LVIS and COCO with synthetic object segments yields strong performance across real-data scales and even larger gains under extremely limited real data (for example, +3.83 $AP_{\text{rare}}$ on LVIS instance segmentation and +6.59 AP with a 1 percent COCO setup). This controllability also supports targeted data generation for challenging intra-class referring in visual grounding.</li>
<li><strong>摘要：</strong>视觉分组——通过实例分割、视觉基础和对象检测进行操作——支撑着从机器人感知到照片编辑的应用。大型带注释数据集成本高昂、覆盖范围存在偏差且难以扩展。合成数据很有前途，但往往缺乏灵活性、准确性和组成多样性。我们提出了 SOS，一个基于以对象为中心的组合策略的简单且可扩展的数据合成管道。它使用结构化布局先验和生成重新照明将高质量的合成对象片段粘贴到新图像中，从而生成准确且多样化的蒙版、框和引用表达式。在 SOS 的 100000 张合成图像上训练的模型在检测和接地任务上优于在更大的真实图像数据集（例如 GRIT (20M) 和 V3Det (200K)）上训练的模型，在 LVIS 检测上获得 +10.9 AP，在 gRefCOCO 接地上获得 +8.4 $N_{\text{Acc}}$。 SOS 支持可控的数据集构建，并提高低数据和封闭词汇设置中的泛化能力。使用合成对象分段增强 LVIS 和 COCO 可在实际数据规模上产生强大的性能，并且在极其有限的实际数据下产生更大的增益（例如，LVIS 实例分段上的 +3.83 $AP_{\text{rare}}$ 和使用 1% COCO 设置的 +6.59 AP）。这种可控性还支持有针对性的数据生成，以挑战视觉基础中的类内参考。</li>
</ul>

<h3>Title: MSDM: Generating Task-Specific Pathology Images with a Multimodal Conditioned Diffusion Model for Cell and Nuclei Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Dominik Winter, Mai Bui, Monica Azqueta Gavaldon, Nicolas Triltsch, Marco Rosati, Nicolas Brieu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09121">https://arxiv.org/abs/2510.09121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09121">https://arxiv.org/pdf/2510.09121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09121]] MSDM: Generating Task-Specific Pathology Images with a Multimodal Conditioned Diffusion Model for Cell and Nuclei Segmentation(https://arxiv.org/abs/2510.09121)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Scarcity of annotated data, particularly for rare or atypical morphologies, present significant challenges for cell and nuclei segmentation in computational pathology. While manual annotation is labor-intensive and costly, synthetic data offers a cost-effective alternative. We introduce a Multimodal Semantic Diffusion Model (MSDM) for generating realistic pixel-precise image-mask pairs for cell and nuclei segmentation. By conditioning the generative process with cellular/nuclear morphologies (using horizontal and vertical maps), RGB color characteristics, and BERT-encoded assay/indication metadata, MSDM generates datasests with desired morphological properties. These heterogeneous modalities are integrated via multi-head cross-attention, enabling fine-grained control over the generated images. Quantitative analysis demonstrates that synthetic images closely match real data, with low Wasserstein distances between embeddings of generated and real images under matching biological conditions. The incorporation of these synthetic samples, exemplified by columnar cells, significantly improves segmentation model accuracy on columnar cells. This strategy systematically enriches data sets, directly targeting model deficiencies. We highlight the effectiveness of multimodal diffusion-based augmentation for advancing the robustness and generalizability of cell and nuclei segmentation models. Thereby, we pave the way for broader application of generative models in computational pathology.</li>
<li><strong>摘要：</strong>带注释数据的稀缺，特别是罕见或非典型形态的数据，给计算病理学中的细胞和细胞核分割带来了重大挑战。虽然手动注释是劳动密集型且成本高昂的，但合成数据提供了一种经济高效的替代方案。我们引入了多模态语义扩散模型（MSDM），用于生成用于细胞和细胞核分割的逼真的像素精确图像掩模对。通过使用细胞/核形态（使用水平和垂直图）、RGB 颜色特征和 BERT 编码的分析/指示元数据来调节生成过程，MSDM 生成具有所需形态特性的数据集。这些异构模式通过多头交叉注意力进行集成，从而能够对生成的图像进行细粒度控制。定量分析表明，合成图像与真实数据紧密匹配，在匹配的生物条件下生成的图像和真实图像的嵌入之间的 Wasserstein 距离较小。这些合成样本（以柱状细胞为例）的结合显着提高了柱状细胞的分割模型准确性。该策略系统地丰富了数据集，直接针对模型缺陷。我们强调基于多模态扩散的增强对于提高细胞和细胞核分割模型的鲁棒性和通用性的有效性。因此，我们为生成模型在计算病理学中的更广泛应用铺平了道路。</li>
</ul>

<h3>Title: Instance-Level Generation for Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Yankun Wu, Zakaria Laskar, Giorgos Kordopatis-Zilos, Noa Garcia, Giorgos Tolias</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09171">https://arxiv.org/abs/2510.09171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09171">https://arxiv.org/pdf/2510.09171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09171]] Instance-Level Generation for Representation Learning(https://arxiv.org/abs/2510.09171)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Instance-level recognition (ILR) focuses on identifying individual objects rather than broad categories, offering the highest granularity in image classification. However, this fine-grained nature makes creating large-scale annotated datasets challenging, limiting ILR's real-world applicability across domains. To overcome this, we introduce a novel approach that synthetically generates diverse object instances from multiple domains under varied conditions and backgrounds, forming a large-scale training set. Unlike prior work on automatic data synthesis, our method is the first to address ILR-specific challenges without relying on any real images. Fine-tuning foundation vision models on the generated data significantly improves retrieval performance across seven ILR benchmarks spanning multiple domains. Our approach offers a new, efficient, and effective alternative to extensive data collection and curation, introducing a new ILR paradigm where the only input is the names of the target domains, unlocking a wide range of real-world applications.</li>
<li><strong>摘要：</strong>实例级识别 (ILR) 侧重于识别单个对象而不是广泛的类别，提供图像分类的最高粒度。然而，这种细粒度的性质使得创建大规模带注释的数据集具有挑战性，限制了 ILR 跨领域的实际适用性。为了克服这个问题，我们引入了一种新颖的方法，可以在不同的条件和背景下从多个领域综合生成不同的对象实例，形成大规模的训练集。与之前的自动数据合成工作不同，我们的方法是第一个在不依赖任何真实图像的情况下解决 ILR 特定挑战的方法。根据生成的数据微调基础视觉模型，可显着提高跨多个领域的七个 ILR 基准的检索性能。我们的方法为广泛的数据收集和管理提供了一种新的、高效的、有效的替代方案，引入了新的 ILR 范例，其中唯一的输入是目标域的名称，从而解锁了广泛的实际应用程序。</li>
</ul>

<h3>Title: RepDL: Bit-level Reproducible Deep Learning Training and Inference</h3>
<ul>
<li><strong>Authors: </strong>Peichen Xie, Xian Zhang, Shuo Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09180">https://arxiv.org/abs/2510.09180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09180">https://arxiv.org/pdf/2510.09180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09180]] RepDL: Bit-level Reproducible Deep Learning Training and Inference(https://arxiv.org/abs/2510.09180)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Non-determinism and non-reproducibility present significant challenges in deep learning, leading to inconsistent results across runs and platforms. These issues stem from two origins: random number generation and floating-point computation. While randomness can be controlled through deterministic configurations, floating-point inconsistencies remain largely unresolved. To address this, we introduce RepDL, an open-source library that ensures deterministic and bitwise-reproducible deep learning training and inference across diverse computing environments. RepDL achieves this by enforcing correct rounding and order invariance in floating-point computation. The source code is available at this https URL .</li>
<li><strong>摘要：</strong>不确定性和不可重复性给深度学习带来了重大挑战，导致运行和平台之间的结果不一致。这些问题源于两个根源：随机数生成和浮点计算。虽然可以通过确定性配置来控制随机性，但浮点不一致问题在很大程度上仍未得到解决。为了解决这个问题，我们引入了 RepDL，这是一个开源库，可确保跨不同计算环境进行确定性和按位可再现的深度学习训练和推理。 RepDL 通过在浮点计算中强制执行正确的舍入和顺序不变性来实现这一点。源代码可在此 https URL 获取。</li>
</ul>

<h3>Title: Stable Video Infinity: Infinite-Length Video Generation with Error Recycling</h3>
<ul>
<li><strong>Authors: </strong>Wuyang Li, Wentao Pan, Po-Chien Luan, Yang Gao, Alexandre Alahi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09212">https://arxiv.org/abs/2510.09212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09212">https://arxiv.org/pdf/2510.09212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09212]] Stable Video Infinity: Infinite-Length Video Generation with Error Recycling(https://arxiv.org/abs/2510.09212)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We propose Stable Video Infinity (SVI) that is able to generate infinite-length videos with high temporal consistency, plausible scene transitions, and controllable streaming storylines. While existing long-video methods attempt to mitigate accumulated errors via handcrafted anti-drifting (e.g., modified noise scheduler, frame anchoring), they remain limited to single-prompt extrapolation, producing homogeneous scenes with repetitive motions. We identify that the fundamental challenge extends beyond error accumulation to a critical discrepancy between the training assumption (seeing clean data) and the test-time autoregressive reality (conditioning on self-generated, error-prone outputs). To bridge this hypothesis gap, SVI incorporates Error-Recycling Fine-Tuning, a new type of efficient training that recycles the Diffusion Transformer (DiT)'s self-generated errors into supervisory prompts, thereby encouraging DiT to actively identify and correct its own errors. This is achieved by injecting, collecting, and banking errors through closed-loop recycling, autoregressively learning from error-injected feedback. Specifically, we (i) inject historical errors made by DiT to intervene on clean inputs, simulating error-accumulated trajectories in flow matching; (ii) efficiently approximate predictions with one-step bidirectional integration and calculate errors with residuals; (iii) dynamically bank errors into replay memory across discretized timesteps, which are resampled for new input. SVI is able to scale videos from seconds to infinite durations with no additional inference cost, while remaining compatible with diverse conditions (e.g., audio, skeleton, and text streams). We evaluate SVI on three benchmarks, including consistent, creative, and conditional settings, thoroughly verifying its versatility and state-of-the-art role.</li>
<li><strong>摘要：</strong>我们提出了稳定视频无限（SVI），它能够生成具有高时间一致性、合理的场景转换和可控流式故事情节的无限长度视频。虽然现有的长视频方法试图通过手工设计的抗漂移（例如，修改的噪声调度程序、帧锚定）来减轻累积的错误，但它们仍然仅限于单提示外推，产生具有重复运动的均匀场景。我们发现，根本性的挑战不仅限于错误累积，还包括训练假设（查看干净的数据）和测试时自回归现实（以自我生成的、容易出错的输出为条件）之间的关键差异。为了弥补这一假设差距，SVI 采用了错误回收微调（Error-Recycling Fine-Tuning），这是一种新型的高效训练，可将扩散变压器（DiT）自身产生的错误回收到监督提示中，从而鼓励 DiT 主动识别并纠正自己的错误。这是通过闭环回收注入、收集和存储错误、从错误注入反馈中进行自回归学习来实现的。具体来说，我们 (i) 注入 DiT 产生的历史错误来干预干净的输入，模拟流量匹配中的错误累积轨迹； (ii) 通过一步双向积分有效地近似预测并用残差计算误差； (iii) 跨离散时间步长将错误动态存储到重放内存中，并针对新输入进行重新采样。 SVI 能够将视频从几秒扩展到无限持续时间，无需额外的推理成本，同时保持与各种条件（例如音频、骨架和文本流）的兼容。我们根据一致性、创造性和条件设置三个基准对 SVI 进行评估，彻底验证了其多功能性和最先进的作用。</li>
</ul>

<h3>Title: Tag-Enriched Multi-Attention with Large Language Models for Cross-Domain Sequential Recommendation</h3>
<ul>
<li><strong>Authors: </strong>Wangyu Wu, Xuhang Chen, Zhenhong Chen, Jing-En Jiang, Kim-Fung Tsang, Xiaowei Huang, Fei Ma, Jimin Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09224">https://arxiv.org/abs/2510.09224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09224">https://arxiv.org/pdf/2510.09224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09224]] Tag-Enriched Multi-Attention with Large Language Models for Cross-Domain Sequential Recommendation(https://arxiv.org/abs/2510.09224)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Cross-Domain Sequential Recommendation (CDSR) plays a crucial role in modern consumer electronics and e-commerce platforms, where users interact with diverse services such as books, movies, and online retail products. These systems must accurately capture both domain-specific and cross-domain behavioral patterns to provide personalized and seamless consumer experiences. To address this challenge, we propose \textbf{TEMA-LLM} (\textit{Tag-Enriched Multi-Attention with Large Language Models}), a practical and effective framework that integrates \textit{Large Language Models (LLMs)} for semantic tag generation and enrichment. Specifically, TEMA-LLM employs LLMs to assign domain-aware prompts and generate descriptive tags from item titles and descriptions. The resulting tag embeddings are fused with item identifiers as well as textual and visual features to construct enhanced item representations. A \textit{Tag-Enriched Multi-Attention} mechanism is then introduced to jointly model user preferences within and across domains, enabling the system to capture complex and evolving consumer interests. Extensive experiments on four large-scale e-commerce datasets demonstrate that TEMA-LLM consistently outperforms state-of-the-art baselines, underscoring the benefits of LLM-based semantic tagging and multi-attention integration for consumer-facing recommendation systems. The proposed approach highlights the potential of LLMs to advance intelligent, user-centric services in the field of consumer electronics.</li>
<li><strong>摘要：</strong>跨域顺序推荐（CDSR）在现代消费电子和电子商务平台中发挥着至关重要的作用，用户在这些平台上与书籍、电影和在线零售产品等多种服务进行交互。这些系统必须准确捕获特定领域和跨领域的行为模式，以提供个性化和无缝的消费者体验。为了应对这一挑战，我们提出了 \textbf{TEMA-LLM} (\textit{Tag-Enriched Multi-Attention with Large Language Models})，这是一个实用且有效的框架，集成了 \textit{Large Language Models (LLM)} 用于语义标签生成和丰富。具体来说，TEMA-LLM 使用 LLM 来分配领域感知提示并根据项目标题和描述生成描述性标签。生成的标签嵌入与项目标识符以及文本和视觉特征融合，以构建增强的项目表示。然后引入 \textit{Tag-Enriched Multi-Attention} 机制来联合建模域内和跨域的用户偏好，使系统能够捕获复杂且不断变化的消费者兴趣。对四个大型电子商务数据集的广泛实验表明，TEMA-LLM 始终优于最先进的基线，强调了基于 LLM 的语义标记和多注意力集成对于面向消费者的推荐系统的好处。所提出的方法凸显了法学硕士在消费电子领域推进智能、以用户为中心的服务的潜力。</li>
</ul>

<h3>Title: Clear Roads, Clear Vision: Advancements in Multi-Weather Restoration for Smart Transportation</h3>
<ul>
<li><strong>Authors: </strong>Vijay M. Galshetwar, Praful Hambarde, Prashant W. Patil, Akshay Dudhane, Sachin Chaudhary, Santosh Kumar Vipparathi, Subrahmanyam Murala</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09228">https://arxiv.org/abs/2510.09228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09228">https://arxiv.org/pdf/2510.09228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09228]] Clear Roads, Clear Vision: Advancements in Multi-Weather Restoration for Smart Transportation(https://arxiv.org/abs/2510.09228)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Adverse weather conditions such as haze, rain, and snow significantly degrade the quality of images and videos, posing serious challenges to intelligent transportation systems (ITS) that rely on visual input. These degradations affect critical applications including autonomous driving, traffic monitoring, and surveillance. This survey presents a comprehensive review of image and video restoration techniques developed to mitigate weather-induced visual impairments. We categorize existing approaches into traditional prior-based methods and modern data-driven models, including CNNs, transformers, diffusion models, and emerging vision-language models (VLMs). Restoration strategies are further classified based on their scope: single-task models, multi-task/multi-weather systems, and all-in-one frameworks capable of handling diverse degradations. In addition, we discuss day and night time restoration challenges, benchmark datasets, and evaluation protocols. The survey concludes with an in-depth discussion on limitations in current research and outlines future directions such as mixed/compound-degradation restoration, real-time deployment, and agentic AI frameworks. This work aims to serve as a valuable reference for advancing weather-resilient vision systems in smart transportation environments. Lastly, to stay current with rapid advancements in this field, we will maintain regular updates of the latest relevant papers and their open-source implementations at this https URL</li>
<li><strong>摘要：</strong>雾霾、雨雪等恶劣天气条件会显着降低图像和视频的质量，给依赖视觉输入的智能交通系统（ITS）带来严峻挑战。这些性能下降会影响自动驾驶、交通监控和监视等关键应用。这项调查对为减轻天气引起的视觉障碍而开发的图像和视频恢复技术进行了全面回顾。我们将现有方法分为传统的基于先验的方法和现代数据驱动模型，包括 CNN、变压器、扩散模型和新兴的视觉语言模型 (VLM)。恢复策略根据其范围进一步分类：单任务模型、多任务/多天气系统以及能够处理各种退化的一体化框架。此外，我们还讨论了白天和夜间恢复挑战、基准数据集和评估协议。该调查最后深入讨论了当前研究的局限性，并概述了未来的方向，例如混合/复合降解恢复、实时部署和代理人工智能框架。这项工作旨在为智能交通环境中先进的耐候视觉系统提供有价值的参考。最后，为了跟上该领域的快速发展，我们将定期更新最新的相关论文及其开源实现，网址为 https URL</li>
</ul>

<h3>Title: Foraging with the Eyes: Dynamics in Human Visual Gaze and Deep Predictive Modeling</h3>
<ul>
<li><strong>Authors: </strong>Tejaswi V. Panchagnula</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09299">https://arxiv.org/abs/2510.09299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09299">https://arxiv.org/pdf/2510.09299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09299]] Foraging with the Eyes: Dynamics in Human Visual Gaze and Deep Predictive Modeling(https://arxiv.org/abs/2510.09299)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Animals often forage via Levy walks stochastic trajectories with heavy tailed step lengths optimized for sparse resource environments. We show that human visual gaze follows similar dynamics when scanning images. While traditional models emphasize image based saliency, the underlying spatiotemporal statistics of eye movements remain underexplored. Understanding these dynamics has broad applications in attention modeling and vision-based interfaces. In this study, we conducted a large scale human subject experiment involving 40 participants viewing 50 diverse images under unconstrained conditions, recording over 4 million gaze points using a high speed eye tracker. Analysis of these data shows that the gaze trajectory of the human eye also follows a Levy walk akin to animal foraging. This suggests that the human eye forages for visual information in an optimally efficient manner. Further, we trained a convolutional neural network (CNN) to predict fixation heatmaps from image input alone. The model accurately reproduced salient fixation regions across novel images, demonstrating that key components of gaze behavior are learnable from visual structure alone. Our findings present new evidence that human visual exploration obeys statistical laws analogous to natural foraging and open avenues for modeling gaze through generative and predictive frameworks.</li>
<li><strong>摘要：</strong>动物通常通过 Levy 行走随机轨迹来觅食，该轨迹具有针对稀疏资源环境优化的重尾步长。我们表明，在扫描图像时，人类视觉凝视遵循类似的动态。虽然传统模型强调基于图像的显着性，但眼球运动的潜在时空统计数据仍未得到充分探索。了解这些动态在注意力建模和基于视觉的界面中具有广泛的应用。在这项研究中，我们进行了一项大规模的人体实验，40 名参与者在不受约束的条件下观看 50 张不同的图像，使用高速眼动仪记录了超过 400 万个注视点。对这些数据的分析表明，人眼的注视轨迹也遵循类似于动物觅食的利维行走。这表明人眼以最有效的方式寻找视觉信息。此外，我们训练了一个卷积神经网络（CNN）来仅根据图像输入来预测注视热图。该模型准确地再现了新颖图像中的显着注视区域，证明注视行为的关键组成部分可以仅从视觉结构中学习。我们的研究结果提供了新的证据，表明人类视觉探索遵循类似于自然觅食的统计规律，并通过生成和预测框架为凝视建模提供了开放途径。</li>
</ul>

<h3>Title: RadioFlow: Efficient Radio Map Construction Framework with Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Haozhe Jia, Wenshuo Chen, Xiucheng Wang, Nan Cheng, Hongbo Zhang, Kuimou Yu, Songning Lai, Nanjian Jia, Bowen Tian, Hongru Xiao, Yutao Yue</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09314">https://arxiv.org/abs/2510.09314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09314">https://arxiv.org/pdf/2510.09314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09314]] RadioFlow: Efficient Radio Map Construction Framework with Flow Matching(https://arxiv.org/abs/2510.09314)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Accurate and real-time radio map (RM) generation is crucial for next-generation wireless systems, yet diffusion-based approaches often suffer from large model sizes, slow iterative denoising, and high inference latency, which hinder practical deployment. To overcome these limitations, we propose \textbf{RadioFlow}, a novel flow-matching-based generative framework that achieves high-fidelity RM generation through single-step efficient sampling. Unlike conventional diffusion models, RadioFlow learns continuous transport trajectories between noise and data, enabling both training and inference to be significantly accelerated while preserving reconstruction accuracy. Comprehensive experiments demonstrate that RadioFlow achieves state-of-the-art performance with \textbf{up to 8$\times$ fewer parameters} and \textbf{over 4$\times$ faster inference} compared to the leading diffusion-based baseline (RadioDiff). This advancement provides a promising pathway toward scalable, energy-efficient, and real-time electromagnetic digital twins for future 6G networks. We release the code at \href{this https URL}{GitHub}.</li>
<li><strong>摘要：</strong>准确、实时的无线电地图（RM）生成对于下一代无线系统至关重要，但基于扩散的方法通常会遇到模型尺寸大、迭代去噪速度慢和推理延迟高的问题，这阻碍了实际部署。为了克服这些限制，我们提出了 \textbf{RadioFlow}，一种新颖的基于流匹配的生成框架，通过单步高效采样实现高保真 RM 生成。与传统的扩散模型不同，RadioFlow 学习噪声和数据之间的连续传输轨迹，从而在保持重建精度的同时显着加速训练和推理。综合实验表明，与领先的基于扩散的基线 (RadioDiff) 相比，RadioFlow 实现了最先进的性能，\textbf{参数减少多达 8$\time$}，并且 \textbf{推理速度加快 4$\time$ 以上}。这一进步为未来 6G 网络提供了一条实现可扩展、节能和实时电磁数字孪生的有前途的途径。我们在 \href{此 https URL}{GitHub} 发布代码。</li>
</ul>

<h3>Title: Efficient Bayesian Inference from Noisy Pairwise Comparisons</h3>
<ul>
<li><strong>Authors: </strong>Till Aczel, Lucas Theis, Wattenhofer Roger</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09333">https://arxiv.org/abs/2510.09333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09333">https://arxiv.org/pdf/2510.09333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09333]] Efficient Bayesian Inference from Noisy Pairwise Comparisons(https://arxiv.org/abs/2510.09333)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Evaluating generative models is challenging because standard metrics often fail to reflect human preferences. Human evaluations are more reliable but costly and noisy, as participants vary in expertise, attention, and diligence. Pairwise comparisons improve consistency, yet aggregating them into overall quality scores requires careful modeling. Bradley-Terry-based methods update item scores from comparisons, but existing approaches either ignore rater variability or lack convergence guarantees, limiting robustness and interpretability. We introduce BBQ, a Bayesian Bradley-Terry variant that explicitly models rater quality, downweighting or removing unreliable participants, and provides guaranteed monotonic likelihood convergence through an Expectation-Maximization algorithm. Empirical results show that BBQ achieves faster convergence, well-calibrated uncertainty estimates, and more robust, interpretable rankings compared to baseline Bradley-Terry models, even with noisy or crowdsourced raters. This framework enables more reliable and cost-effective human evaluation of generative models.</li>
<li><strong>摘要：</strong>评估生成模型具有挑战性，因为标准指标通常无法反映人类偏好。人工评估更可靠，但成本高昂且噪音较大，因为参与者的专业知识、注意力和勤奋程度各不相同。成对比较可以提高一致性，但将它们汇总到整体质量分数中需要仔细建模。基于 Bradley-Terry 的方法通过比较来更新项目分数，但现有方法要么忽略评分者的变异性，要么缺乏收敛保证，从而限制了鲁棒性和可解释性。我们引入了 BBQ，一种贝叶斯 Bradley-Terry 变体，它显式地对评估者质量进行建模，降低权重或删除不可靠的参与者，并通过期望最大化算法提供有保证的单调似然收敛。实证结果表明，与基线 Bradley-Terry 模型相比，BBQ 能够实现更快的收敛、经过良好校准的不确定性估计以及更稳健、可解释的排名，即使使用嘈杂或众包的评估者也是如此。该框架可以对生成模型进行更可靠且更具成本效益的人类评估。</li>
</ul>

<h3>Title: Visibility-Aware Densification for 3D Gaussian Splatting in Dynamic Urban Scenes</h3>
<ul>
<li><strong>Authors: </strong>Yikang Zhang, Rui Fan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09364">https://arxiv.org/abs/2510.09364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09364">https://arxiv.org/pdf/2510.09364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09364]] Visibility-Aware Densification for 3D Gaussian Splatting in Dynamic Urban Scenes(https://arxiv.org/abs/2510.09364)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>3D Gaussian splatting (3DGS) has demonstrated impressive performance in synthesizing high-fidelity novel views. Nonetheless, its effectiveness critically depends on the quality of the initialized point cloud. Specifically, achieving uniform and complete point coverage over the underlying scene structure requires overlapping observation frustums, an assumption that is often violated in unbounded, dynamic urban environments. Training Gaussian models with partially initialized point clouds often leads to distortions and artifacts, as camera rays may fail to intersect valid surfaces, resulting in incorrect gradient propagation to Gaussian primitives associated with occluded or invisible geometry. Additionally, existing densification strategies simply clone and split Gaussian primitives from existing ones, incapable of reconstructing missing structures. To address these limitations, we propose VAD-GS, a 3DGS framework tailored for geometry recovery in challenging urban scenes. Our method identifies unreliable geometry structures via voxel-based visibility reasoning, selects informative supporting views through diversity-aware view selection, and recovers missing structures via patch matching-based multi-view stereo reconstruction. This design enables the generation of new Gaussian primitives guided by reliable geometric priors, even in regions lacking initial points. Extensive experiments on the Waymo and nuScenes datasets demonstrate that VAD-GS outperforms state-of-the-art 3DGS approaches and significantly improves the quality of reconstructed geometry for both static and dynamic objects. Source code will be released upon publication.</li>
<li><strong>摘要：</strong>3D 高斯泼溅 (3DGS) 在合成高保真新颖视图方面表现出了令人印象深刻的性能。尽管如此，其有效性关键取决于初始化点云的质量。具体来说，在底层场景结构上实现统一且完整的点覆盖需要重叠的观察截锥体，这一假设在无界、动态的城市环境中经常被违反。使用部分初始化的点云训练高斯模型通常会导致扭曲和伪影，因为相机光线可能无法与有效表面相交，从而导致错误的梯度传播到与遮挡或不可见几何体相关的高斯基元。此外，现有的致密化策略只是从现有的高斯基元中克隆和分离高斯基元，无法重建丢失的结构。为了解决这些限制，我们提出了 VAD-GS，这是一种专为具有挑战性的城市场景中的几何恢复而定制的 3DGS 框架。我们的方法通过基于体素的可见性推理来识别不可靠的几何结构，通过多样性感知视图选择来选择信息丰富的支持视图，并通过基于补丁匹配的多视图立体重建来恢复丢失的结构。这种设计使得能够在可靠的几何先验的指导下生成新的高斯基元，即使在缺乏初始点的区域也是如此。在 Waymo 和 nuScenes 数据集上进行的大量实验表明，VAD-GS 的性能优于最先进的 3DGS 方法，并显着提高了静态和动态对象的重建几何形状的质量。源代码将在发布后发布。</li>
</ul>

<h3>Title: Few-shot multi-token DreamBooth with LoRa for style-consistent character generation</h3>
<ul>
<li><strong>Authors: </strong>Ruben Pascual, Mikel Sesma-Sara, Aranzazu Jurio, Daniel Paternain, Mikel Galar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09475">https://arxiv.org/abs/2510.09475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09475">https://arxiv.org/pdf/2510.09475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09475]] Few-shot multi-token DreamBooth with LoRa for style-consistent character generation(https://arxiv.org/abs/2510.09475)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The audiovisual industry is undergoing a profound transformation as it is integrating AI developments not only to automate routine tasks but also to inspire new forms of art. This paper addresses the problem of producing a virtually unlimited number of novel characters that preserve the artistic style and shared visual traits of a small set of human-designed reference characters, thus broadening creative possibilities in animation, gaming, and related domains. Our solution builds upon DreamBooth, a well-established fine-tuning technique for text-to-image diffusion models, and adapts it to tackle two core challenges: capturing intricate character details beyond textual prompts and the few-shot nature of the training data. To achieve this, we propose a multi-token strategy, using clustering to assign separate tokens to individual characters and their collective style, combined with LoRA-based parameter-efficient fine-tuning. By removing the class-specific regularization set and introducing random tokens and embeddings during generation, our approach allows for unlimited character creation while preserving the learned style. We evaluate our method on five small specialized datasets, comparing it to relevant baselines using both quantitative metrics and a human evaluation study. Our results demonstrate that our approach produces high-quality, diverse characters while preserving the distinctive aesthetic features of the reference characters, with human evaluation further reinforcing its effectiveness and highlighting the potential of our method.</li>
<li><strong>摘要：</strong>视听行业正在经历一场深刻的变革，因为它正在整合人工智能的发展，不仅使日常任务自动化，而且激发新的艺术形式。本文解决了产生几乎无限数量的新颖角色的问题，这些角色保留了一小部分人类设计的参考角色的艺术风格和共享视觉特征，从而拓宽了动画、游戏和相关领域的创作可能性。我们的解决方案建立在 DreamBooth（一种行之有效的文本到图像扩散模型微调技术）的基础上，并对其进行调整以解决两个核心挑战：捕获文本提示之外的复杂字符细节和训练数据的少数镜头性质。为了实现这一目标，我们提出了一种多令牌策略，使用聚类将单独的令牌分配给各个角色及其集体风格，并结合基于 LoRA 的参数高效微调。通过删除特定于类的正则化集并在生成过程中引入随机标记和嵌入，我们的方法允许无限的角色创建，同时保留学习的风格。我们在五个小型专业数据集上评估我们的方法，并使用定量指标和人类评估研究将其与相关基线进行比较。我们的结果表明，我们的方法产生了高质量、多样化的字符，同时保留了参考字符独特的美学特征，人类评估进一步增强了其有效性并凸显了我们方法的潜力。</li>
</ul>

<h3>Title: TC-LoRA: Temporally Modulated Conditional LoRA for Adaptive Diffusion Control</h3>
<ul>
<li><strong>Authors: </strong>Minkyoung Cho, Ruben Ohana, Christian Jacobsen, Adityan Jothi, Min-Hung Chen, Z. Morley Mao, Ethem Can</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09561">https://arxiv.org/abs/2510.09561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09561">https://arxiv.org/pdf/2510.09561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09561]] TC-LoRA: Temporally Modulated Conditional LoRA for Adaptive Diffusion Control(https://arxiv.org/abs/2510.09561)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Current controllable diffusion models typically rely on fixed architectures that modify intermediate activations to inject guidance conditioned on a new modality. This approach uses a static conditioning strategy for a dynamic, multi-stage denoising process, limiting the model's ability to adapt its response as the generation evolves from coarse structure to fine detail. We introduce TC-LoRA (Temporally Modulated Conditional LoRA), a new paradigm that enables dynamic, context-aware control by conditioning the model's weights directly. Our framework uses a hypernetwork to generate LoRA adapters on-the-fly, tailoring weight modifications for the frozen backbone at each diffusion step based on time and the user's condition. This mechanism enables the model to learn and execute an explicit, adaptive strategy for applying conditional guidance throughout the entire generation process. Through experiments on various data domains, we demonstrate that this dynamic, parametric control significantly enhances generative fidelity and adherence to spatial conditions compared to static, activation-based methods. TC-LoRA establishes an alternative approach in which the model's conditioning strategy is modified through a deeper functional adaptation of its weights, allowing control to align with the dynamic demands of the task and generative stage.</li>
<li><strong>摘要：</strong>当前的可控扩散模型通常依赖于固定的架构，该架构修改中间激活以注入以新模态为条件的引导。这种方法对动态、多阶段去噪过程使用静态调节策略，限制了模型在生成从粗糙结构发展到精细细节时调整其响应的能力。我们引入了 TC-LoRA（时间调制条件 LoRA），这是一种新范例，可通过直接调节模型的权重来实现动态、上下文感知控制。我们的框架使用超网络动态生成 LoRA 适配器，根据时间和用户状况在每个扩散步骤中为冻结主干调整权重修改。这种机制使模型能够学习并执行明确的自适应策略，以便在整个生成过程中应用条件指导。通过对各种数据域的实验，我们证明，与静态的基于激活的方法相比，这种动态的参数控制显着增强了生成保真度和对空间条件的遵守。 TC-LoRA 建立了一种替代方法，通过对其权重进行更深入的功能调整来修改模型的调节策略，从而使控制能够与任务和生成阶段的动态需求保持一致。</li>
</ul>

<h3>Title: Vision Language Models: A Survey of 26K Papers</h3>
<ul>
<li><strong>Authors: </strong>Fengming Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09586">https://arxiv.org/abs/2510.09586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09586">https://arxiv.org/pdf/2510.09586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09586]] Vision Language Models: A Survey of 26K Papers(https://arxiv.org/abs/2510.09586)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present a transparent, reproducible measurement of research trends across 26,104 accepted papers from CVPR, ICLR, and NeurIPS spanning 2023-2025. Titles and abstracts are normalized, phrase-protected, and matched against a hand-crafted lexicon to assign up to 35 topical labels and mine fine-grained cues about tasks, architectures, training regimes, objectives, datasets, and co-mentioned modalities. The analysis quantifies three macro shifts: (1) a sharp rise of multimodal vision-language-LLM work, which increasingly reframes classic perception as instruction following and multi-step reasoning; (2) steady expansion of generative methods, with diffusion research consolidating around controllability, distillation, and speed; and (3) resilient 3D and video activity, with composition moving from NeRFs to Gaussian splatting and a growing emphasis on human- and agent-centric understanding. Within VLMs, parameter-efficient adaptation like prompting/adapters/LoRA and lightweight vision-language bridges dominate; training practice shifts from building encoders from scratch to instruction tuning and finetuning strong backbones; contrastive objectives recede relative to cross-entropy/ranking and distillation. Cross-venue comparisons show CVPR has a stronger 3D footprint and ICLR the highest VLM share, while reliability themes such as efficiency or robustness diffuse across areas. We release the lexicon and methodology to enable auditing and extension. Limitations include lexicon recall and abstract-only scope, but the longitudinal signals are consistent across venues and years.</li>
<li><strong>摘要：</strong>我们对 2023 年至 2025 年 CVPR、ICLR 和 NeurIPS 接收的 26,104 篇论文的研究趋势进行了透明、可重复的衡量。标题和摘要经过标准化、短语保护，并与手工制作的词典进行匹配，以分配多达 35 个主题标签，并挖掘有关任务、架构、训练制度、目标、数据集和共同提及的模式的细粒度线索。该分析量化了三个宏观转变：（1）多模式视觉-语言-法学硕士工作的急剧上升，它越来越多地将经典感知重新构建为指令遵循和多步骤推理； (2) 生成方法的稳步扩展，扩散研究围绕可控性、蒸馏和速度进行巩固； (3) 弹性 3D 和视频活动，构图从 NeRF 转向高斯泼溅，并且越来越强调以人类和代理为中心的理解。在 VLM 中，参数高效的自适应（如提示/适配器/LoRA 和轻量级视觉语言桥）占主导地位；培训实践从从头开始构建编码器转向指令调整和微调强大的骨干网；相对于交叉熵/排序和蒸馏，对比目标逐渐减弱。跨场地比较显示 CVPR 具有更强的 3D 覆盖范围，ICLR 具有最高的 VLM 份额，而效率或稳健性等可靠性主题则在各个领域中扩散。我们发布词典和方法以实现审核和扩展。局限性包括词汇回忆和仅限摘要范围，但纵向信号在不同地点和年份之间是一致的。</li>
</ul>

<h3>Title: STaTS: Structure-Aware Temporal Sequence Summarization via Statistical Window Merging</h3>
<ul>
<li><strong>Authors: </strong>Disharee Bhowmick, Ranjith Ramanathan, Sathyanarayanan N. Aakur</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09593">https://arxiv.org/abs/2510.09593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09593">https://arxiv.org/pdf/2510.09593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09593]] STaTS: Structure-Aware Temporal Sequence Summarization via Statistical Window Merging(https://arxiv.org/abs/2510.09593)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Time series data often contain latent temporal structure, transitions between locally stationary regimes, repeated motifs, and bursts of variability, that are rarely leveraged in standard representation learning pipelines. Existing models typically operate on raw or fixed-window sequences, treating all time steps as equally informative, which leads to inefficiencies, poor robustness, and limited scalability in long or noisy sequences. We propose STaTS, a lightweight, unsupervised framework for Structure-Aware Temporal Summarization that adaptively compresses both univariate and multivariate time series into compact, information-preserving token sequences. STaTS detects change points across multiple temporal resolutions using a BIC-based statistical divergence criterion, then summarizes each segment using simple functions like the mean or generative models such as GMMs. This process achieves up to 30x sequence compression while retaining core temporal dynamics. STaTS operates as a model-agnostic preprocessor and can be integrated with existing unsupervised time series encoders without retraining. Extensive experiments on 150+ datasets, including classification tasks on the UCR-85, UCR-128, and UEA-30 archives, and forecasting on ETTh1 and ETTh2, ETTm1, and Electricity, demonstrate that STaTS enables 85-90\% of the full-model performance while offering dramatic reductions in computational cost. Moreover, STaTS improves robustness under noise and preserves discriminative structure, outperforming uniform and clustering-based compression baselines. These results position STaTS as a principled, general-purpose solution for efficient, structure-aware time series modeling.</li>
<li><strong>摘要：</strong>时间序列数据通常包含潜在的时间结构、局部固定状态之间的转换、重复的主题和变异性的爆发，这些在标准表示学习管道中很少被利用。现有模型通常在原始或固定窗口序列上运行，将所有时间步视为同等信息，这会导致长序列或噪声序列中的效率低下、鲁棒性差以及可扩展性有限。我们提出了 STaTS，一种轻量级、无监督的结构感知时间摘要框架，它自适应地将单变量和多元时间序列压缩为紧凑的、信息保留的标记序列。 STaTS 使用基于 BIC 的统计散度标准检测多个时间分辨率的变化点，然后使用简单函数（如均值或生成模型（如 GMM））总结每个片段。此过程可实现高达 30 倍的序列压缩，同时保留核心时间动态。 STaTS 作为与模型无关的预处理器运行，可以与现有的无监督时间序列编码器集成，而无需重新训练。对 150 多个数据集的广泛实验（包括 UCR-85、UCR-128 和 UEA-30 档案的分类任务，以及 ETTh1 和 ETTh2、ETTm1 和电力的预测）表明，STaTS 可以实现 85-90% 的全模型性能，同时大幅降低计算成本。此外，STaTS 提高了噪声下的鲁棒性并保留了判别性结构，优于均匀和基于聚类的压缩基线。这些结果将 STaTS 定位为一种有原则的通用解决方案，用于高效、结构感知的时间序列建模。</li>
</ul>

<h3>Title: BaNEL: Exploration Posteriors for Generative Modeling Using Only Negative Rewards</h3>
<ul>
<li><strong>Authors: </strong>Sangyun Lee, Brandon Amos, Giulia Fanti</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09596">https://arxiv.org/abs/2510.09596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09596">https://arxiv.org/pdf/2510.09596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09596]] BaNEL: Exploration Posteriors for Generative Modeling Using Only Negative Rewards(https://arxiv.org/abs/2510.09596)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Today's generative models thrive with large amounts of supervised data and informative reward functions characterizing the quality of the generation. They work under the assumptions that the supervised data provides knowledge to pre-train the model, and the reward function provides dense information about how to further improve the generation quality and correctness. However, in the hardest instances of important problems, two problems arise: (1) the base generative model attains a near-zero reward signal, and (2) calls to the reward oracle are expensive. This setting poses a fundamentally different learning challenge than standard reward-based post-training. To address this, we propose BaNEL (Bayesian Negative Evidence Learning), an algorithm that post-trains the model using failed attempts only, while minimizing the number of reward evaluations (NREs). Our method is based on the idea that the problem of learning regularities underlying failures can be cast as another, in-loop generative modeling problem. We then leverage this model to assess whether new data resembles previously seen failures and steer the generation away from them. We show that BaNEL can improve model performance without observing a single successful sample on several sparse-reward tasks, outperforming existing novelty-bonus approaches by up to several orders of magnitude in success rate, while using fewer reward evaluations.</li>
<li><strong>摘要：</strong>当今的生成模型凭借大量的监督数据和表征生成质量的信息奖励函数而蓬勃发展。他们的工作假设是监督数据提供了预训练模型的知识，而奖励函数提供了有关如何进一步提高生成质量和正确性的密集信息。然而，在重要问题的最困难的情况下，会出现两个问题：（1）基本生成模型获得接近于零的奖励信号，（2）对奖励预言机的调用成本很高。与标准的基于奖励的后期培训相比，这种设置提出了根本不同的学习挑战。为了解决这个问题，我们提出了 BaNEL（贝叶斯负证据学习），这是一种仅使用失败的尝试对模型进行后训练的算法，同时最大限度地减少奖励评估 (NRE) 的数量。我们的方法基于这样的想法：失败背后的学习规律问题可以转化为另一个循环内生成建模问题。然后，我们利用这个模型来评估新数据是否类似于以前见过的失败，并引导一代人远离它们。我们证明，BaNEL 可以在多个稀疏奖励任务上观察到单个成功样本的情况下提高模型性能，在成功率上比现有的新颖性奖励方法高出几个数量级，同时使用更少的奖励评估。</li>
</ul>

<h3>Title: VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation</h3>
<ul>
<li><strong>Authors: </strong>Shaoqi Dong, Chaoyou Fu, Haihan Gao, Yi-Fan Zhang, Chi Yan, Chu Wu, Xiaoyu Liu, Yunhang Shen, Jing Huo, Deqiang Jiang, Haoyu Cao, Yang Gao, Xing Sun, Ran He, Caifeng Shan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09607">https://arxiv.org/abs/2510.09607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09607">https://arxiv.org/pdf/2510.09607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09607]] VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation(https://arxiv.org/abs/2510.09607)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision-Language Action (VLA) models significantly advance robotic manipulation by leveraging the strong perception capabilities of pretrained vision-language models (VLMs). By integrating action modules into these pretrained models, VLA methods exhibit improved generalization. However, training them from scratch is costly. In this work, we propose a simple yet effective distillation-based framework that equips VLMs with action-execution capability by transferring knowledge from pretrained small action models. Our architecture retains the original VLM structure, adding only an action token and a state encoder to incorporate physical inputs. To distill action knowledge, we adopt a two-stage training strategy. First, we perform lightweight alignment by mapping VLM hidden states into the action space of the small action model, enabling effective reuse of its pretrained action decoder and avoiding expensive pretraining. Second, we selectively fine-tune the language model, state encoder, and action modules, enabling the system to integrate multimodal inputs with precise action generation. Specifically, the action token provides the VLM with a direct handle for predicting future actions, while the state encoder allows the model to incorporate robot dynamics not captured by vision alone. This design yields substantial efficiency gains over training large VLA models from scratch. Compared with previous state-of-the-art methods, our method achieves 97.3% average success rate on LIBERO (11.8% improvement) and 93.5% on LIBERO-LONG (24.5% improvement). In real-world experiments across five manipulation tasks, our method consistently outperforms the teacher model, achieving 82.0% success rate (17% improvement), which demonstrate that action distillation effectively enables VLMs to generate precise actions while substantially reducing training costs.</li>
<li><strong>摘要：</strong>视觉语言动作 (VLA) 模型利用预训练视觉语言模型 (VLM) 的强大感知能力，显着推进机器人操作。通过将动作模块集成到这些预训练模型中，VLA 方法表现出改进的泛化能力。然而，从头开始培训他们的成本很高。在这项工作中，我们提出了一个简单而有效的基于蒸馏的框架，通过从预先训练的小动作模型中转移知识，为 VLM 配备动作执行能力。我们的架构保留了原始的 VLM 结构，仅添加了一个动作令牌和一个状态编码器来合并物理输入。为了提炼动作知识，我们采用两阶段训练策略。首先，我们通过将 VLM 隐藏状态映射到小动作模型的动作空间来执行轻量级对齐，从而能够有效地重用其预训练的动作解码器并避免昂贵的预训练。其次，我们有选择地微调语言模型、状态编码器和动作模块，使系统能够将多模态输入与精确的动作生成相结合。具体来说，动作令牌为 VLM 提供了预测未来动作的直接句柄，而状态编码器允许模型结合仅由视觉捕获的机器人动力学。与从头开始训练大型 VLA 模型相比，这种设计可显着提高效率。与之前最先进的方法相比，我们的方法在 LIBERO 上实现了 97.3% 的平均成功率（提高了 11.8%），在 LIBERO-LONG 上实现了 93.5% 的平均成功率（提高了 24.5%）。在五个操作任务的真实实验中，我们的方法始终优于教师模型，成功率达到 82.0%（提高了 17%），这表明动作蒸馏有效地使 VLM 能够生成精确的动作，同时大幅降低训练成本。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
