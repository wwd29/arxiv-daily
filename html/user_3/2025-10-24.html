<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-24</h1>
<h3>Title: Fourier-Based GAN Fingerprint Detection using ResNet50</h3>
<ul>
<li><strong>Authors: </strong>Sai Teja Erukude, Viswa Chaitanya Marella, Suhasnadh Reddy Veluru</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19840">https://arxiv.org/abs/2510.19840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19840">https://arxiv.org/pdf/2510.19840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19840]] Fourier-Based GAN Fingerprint Detection using ResNet50(https://arxiv.org/abs/2510.19840)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid rise of photorealistic images produced from Generative Adversarial Networks (GANs) poses a serious challenge for image forensics and industrial systems requiring reliable content authenticity. This paper uses frequency-domain analysis combined with deep learning to solve the problem of distinguishing StyleGAN-generated images from real ones. Specifically, a two-dimensional Discrete Fourier Transform (2D DFT) was applied to transform images into the Fourier domain, where subtle periodic artifacts become detectable. A ResNet50 neural network is trained on these transformed images to differentiate between real and synthetic ones. The experiments demonstrate that the frequency-domain model achieves a 92.8 percent and an AUC of 0.95, significantly outperforming the equivalent model trained on raw spatial-domain images. These results indicate that the GAN-generated images have unique frequency-domain signatures or "fingerprints". The method proposed highlights the industrial potential of combining signal processing techniques and deep learning to enhance digital forensics and strengthen the trustworthiness of industrial AI systems.</li>
<li><strong>摘要：</strong>生成对抗网络 (GAN) 生成的真实感图像的快速增长给需要可靠内容真实性的图像取证和工业系统带来了严峻的挑战。本文利用频域分析结合深度学习来解决区分 StyleGAN 生成的图像与真实图像的问题。具体来说，应用二维离散傅里叶变换（2D DFT）将图像变换到傅里叶域，在傅里叶域中可以检测到细微的周期性伪影。 ResNet50 神经网络在这些转换后的图像上进行训练，以区分真实图像和合成图像。实验表明，频域模型达到了 92.8%，AUC 为 0.95，显着优于在原始空间域图像上训练的等效模型。这些结果表明 GAN 生成的图像具有独特的频域特征或“指纹”。所提出的方法凸显了将信号处理技术和深度学习相结合的工业潜力，以增强数字取证并增强工业人工智能系统的可信度。</li>
</ul>

<h3>Title: Some Attention is All You Need for Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Felix Michalak, Steven Abreu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19861">https://arxiv.org/abs/2510.19861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19861">https://arxiv.org/pdf/2510.19861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19861]] Some Attention is All You Need for Retrieval(https://arxiv.org/abs/2510.19861)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We demonstrate complete functional segregation in hybrid SSM-Transformer architectures: retrieval depends exclusively on self-attention layers. Across RecurrentGemma-2B/9B and Jamba-Mini-1.6, attention ablation causes catastrophic retrieval failure (0% accuracy), while SSM layers show no compensatory mechanisms even with improved prompting. Conversely, sparsifying attention to just 15% of heads maintains near-perfect retrieval while preserving 84% MMLU performance, suggesting self-attention specializes primarily for retrieval tasks. We identify precise mechanistic requirements for retrieval: needle tokens must be exposed during generation and sufficient context must be available during prefill or generation. This strict functional specialization challenges assumptions about redundancy in hybrid architectures and suggests these models operate as specialized modules rather than integrated systems, with immediate implications for architecture optimization and interpretability.</li>
<li><strong>摘要：</strong>我们在混合 SSM-Transformer 架构中展示了完整的功能隔离：检索完全依赖于自注意力层。在 RecurrentGemma-2B/9B 和 Jamba-Mini-1.6 中，注意力消融会导致灾难性的检索失败（0% 准确率），而 SSM 层即使改进了提示，也没有表现出补偿机制。相反，将注意力稀疏到 15% 的头部即可保持近乎完美的检索，同时保留 84% 的 MMLU 性能，这表明自注意力主要专门用于检索任务。我们确定了检索的精确机械要求：针标记必须在生成过程中暴露，并且在预填充或生成过程中必须提供足够的上下文。这种严格的功能专业化挑战了关于混合架构中冗余的假设，并表明这些模型作为专用模块而不是集成系统运行，这对架构优化和可解释性具有直接影响。</li>
</ul>

<h3>Title: From Large to Small: Transferring CUDA Optimization Expertise via Reasoning Graph</h3>
<ul>
<li><strong>Authors: </strong>Junfeng Gong, Zhiyi Wei, Junying Chen, Cheng Liu, Huawei Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19873">https://arxiv.org/abs/2510.19873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19873">https://arxiv.org/pdf/2510.19873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19873]] From Large to Small: Transferring CUDA Optimization Expertise via Reasoning Graph(https://arxiv.org/abs/2510.19873)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite significant evolution of CUDA programming and domain-specific libraries, effectively utilizing GPUs with massively parallel engines remains difficult. Large language models (LLMs) show strong potential in generating optimized CUDA code from sequential code. However, using LLMs in practice faces two major challenges: cloud-based APIs pose risks of code leakage, and local deployment is often computationally expensive and inefficient. These drawbacks have spurred interest in small language models (SLMs), which are more lightweight and privacy-friendly. Encouragingly, recent studies show that SLMs can achieve performance comparable to LLMs on specific tasks. While SLMs can match LLMs on domain-specific tasks, their limited reasoning abilities lead to suboptimal performance in complex CUDA generation according to our experiments. To bridge this gap, we propose ReGraphT, a training-free, retrieval-augmented generation framework that transfers LLM-level reasoning to smaller models. ReGraphT organizes CUDA optimization trajectories into a structured reasoning graph, modeling the combined CUDA optimizations as state transitions, and leverages Monte Carlo Graph Search (MCGS) for efficient exploration. We also present a CUDA-specific benchmark with difficulty tiers defined by reasoning complexity to evaluate models more comprehensively. Experiments show that ReGraphT outperforms HPC-specific fine-tuned models and other retrieval-augmented approaches, achieving an average 2.33X speedup on CUDAEval and ParEval. When paired with DeepSeek-Coder-V2-Lite-Instruct and Qwen2.5-Coder-7B-Instruct, ReGraphT enables SLMs to approach LLM-level performance without the associated privacy risks or excessive computing overhead.</li>
<li><strong>摘要：</strong>尽管 CUDA 编程和特定领域库取得了重大发展，但有效利用具有大规模并行引擎的 GPU 仍然很困难。大型语言模型 (LLM) 在从顺序代码生成优化的 CUDA 代码方面显示出强大的潜力。然而，在实践中使用 LLM 面临两大挑战：基于云的 API 带来代码泄漏的风险，而本地部署通常计算成本昂贵且效率低下。这些缺点激发了人们对小语言模型（SLM）的兴趣，它更轻量级且对隐私友好。令人鼓舞的是，最近的研究表明，在特定任务上，SLM 可以取得与 LLM 相当的表现。虽然 SLM 可以在特定领域的任务上与 LLM 相媲美，但根据我们的实验，它们有限的推理能力导致复杂 CUDA 生成中的性能不佳。为了弥补这一差距，我们提出了 ReGraphT，这是一种免训练、检索增强的生成框架，可将 LLM 级别的推理转移到更小的模型。 ReGraphT 将 CUDA 优化轨迹组织成结构化推理图，将组合的 CUDA 优化建模为状态转换，并利用蒙特卡罗图搜索 (MCGS) 进行高效探索。我们还提出了一个特定于 CUDA 的基准，其难度等级由推理复杂性定义，以更全面地评估模型。实验表明，ReGraphT 的性能优于 HPC 特定的微调模型和其他检索增强方法，在 CUDAEval 和 ParEval 上实现平均 2.33 倍的加速。当与 DeepSeek-Coder-V2-Lite-Instruct 和 Qwen2.5-Coder-7B-Instruct 配合使用时，ReGraphT 使 SLM 能够接近 LLM 级别的性能，而不会带来相关的隐私风险或过多的计算开销。</li>
</ul>

<h3>Title: Extreme Views: 3DGS Filter for Novel View Synthesis from Out-of-Distribution Camera Poses</h3>
<ul>
<li><strong>Authors: </strong>Damian Bowness, Charalambos Poullis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20027">https://arxiv.org/abs/2510.20027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20027">https://arxiv.org/pdf/2510.20027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20027]] Extreme Views: 3DGS Filter for Novel View Synthesis from Out-of-Distribution Camera Poses(https://arxiv.org/abs/2510.20027)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>When viewing a 3D Gaussian Splatting (3DGS) model from camera positions significantly outside the training data distribution, substantial visual noise commonly occurs. These artifacts result from the lack of training data in these extrapolated regions, leading to uncertain density, color, and geometry predictions from the model. To address this issue, we propose a novel real-time render-aware filtering method. Our approach leverages sensitivity scores derived from intermediate gradients, explicitly targeting instabilities caused by anisotropic orientations rather than isotropic variance. This filtering method directly addresses the core issue of generative uncertainty, allowing 3D reconstruction systems to maintain high visual fidelity even when users freely navigate outside the original training viewpoints. Experimental evaluation demonstrates that our method substantially improves visual quality, realism, and consistency compared to existing Neural Radiance Field (NeRF)-based approaches such as BayesRays. Critically, our filter seamlessly integrates into existing 3DGS rendering pipelines in real-time, unlike methods that require extensive post-hoc retraining or fine-tuning. Code and results at this https URL</li>
<li><strong>摘要：</strong>当从明显超出训练数据分布的相机位置查看 3D 高斯泼溅 (3DGS) 模型时，通常会出现大量视觉噪声。这些伪像是由于这些外推区域中缺乏训练数据造成的，导致模型的密度、颜色和几何预测不确定。为了解决这个问题，我们提出了一种新颖的实时渲染感知过滤方法。我们的方法利用中间梯度得出的敏感度分数，明确针对由各向异性方向而不是各向同性方差引起的不稳定性。这种过滤方法直接解决了生成不确定性的核心问题，即使用户在原始训练视点之外自由导航，3D 重建系统也能保持高视觉保真度。实验评估表明，与现有的基于神经辐射场 (NeRF) 的方法（例如 BayesRays）相比，我们的方法大大提高了视觉质量、真实感和一致性。至关重要的是，我们的过滤器可以实时无缝集成到现有的 3DGS 渲染管道中，这与需要大量事后重新训练或微调的方法不同。代码和结果位于此 https URL</li>
</ul>

<h3>Title: BrainPuzzle: Hybrid Physics and Data-Driven Reconstruction for Transcranial Ultrasound Tomography</h3>
<ul>
<li><strong>Authors: </strong>Shengyu Chen, Shihang Feng, Yi Luo, Xiaowei Jia, Youzuo Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20029">https://arxiv.org/abs/2510.20029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20029">https://arxiv.org/pdf/2510.20029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20029]] BrainPuzzle: Hybrid Physics and Data-Driven Reconstruction for Transcranial Ultrasound Tomography(https://arxiv.org/abs/2510.20029)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Ultrasound brain imaging remains challenging due to the large difference in sound speed between the skull and brain tissues and the difficulty of coupling large probes to the skull. This work aims to achieve quantitative transcranial ultrasound by reconstructing an accurate speed-of-sound (SoS) map of the brain. Traditional physics-based full-waveform inversion (FWI) is limited by weak signals caused by skull-induced attenuation, mode conversion, and phase aberration, as well as incomplete spatial coverage since full-aperture arrays are clinically impractical. In contrast, purely data-driven methods that learn directly from raw ultrasound data often fail to model the complex nonlinear and nonlocal wave propagation through bone, leading to anatomically plausible but quantitatively biased SoS maps under low signal-to-noise and sparse-aperture conditions. To address these issues, we propose BrainPuzzle, a hybrid two-stage framework that combines physical modeling with machine learning. In the first stage, reverse time migration (time-reversal acoustics) is applied to multi-angle acquisitions to produce migration fragments that preserve structural details even under low SNR. In the second stage, a transformer-based super-resolution encoder-decoder with a graph-based attention unit (GAU) fuses these fragments into a coherent and quantitatively accurate SoS image. A partial-array acquisition strategy using a movable low-count transducer set improves feasibility and coupling, while the hybrid algorithm compensates for the missing aperture. Experiments on two synthetic datasets show that BrainPuzzle achieves superior SoS reconstruction accuracy and image completeness, demonstrating its potential for advancing quantitative ultrasound brain imaging.</li>
<li><strong>摘要：</strong>由于颅骨和脑组织之间的声速差异很大，并且将大型探头耦合到颅骨上很困难，超声脑成像仍然具有挑战性。这项工作旨在通过重建精确的大脑声速（SoS）图来实现定量经颅超声。传统的基于物理的全波形反演（FWI）受到颅骨引起的衰减、模式转换和相位像差引起的微弱信号的限制，以及由于全孔径阵列在临床上不切实际而导致的不完整的空间覆盖。相比之下，直接从原始超声数据中学习的纯数据驱动方法通常无法对通过骨骼的复杂非线性和非局部波传播进行建模，导致在低信噪比和稀疏孔径条件下产生解剖学上合理但定量偏差的 SoS 图。为了解决这些问题，我们提出了 BrainPuzzle，这是一种将物理建模与机器学习相结合的混合两阶段框架。在第一阶段，将逆时偏移（时间反转声学）应用于多角度采集，以产生即使在低信噪比下也能保留结构细节的偏移片段。在第二阶段，基于变压器的超分辨率编码器-解码器和基于图的注意力单元（GAU）将这些片段融合成连贯且定量准确的 SoS 图像。使用可移动低计数传感器组的部分阵列采集策略提高了可行性和耦合性，而混合算法则补偿了缺失的孔径。对两个合成数据集的实验表明，BrainPuzzle 实现了卓越的 SoS 重建精度和图像完整性，展示了其推进定量超声脑成像的潜力。</li>
</ul>

<h3>Title: Speculative Sampling for Parametric Temporal Point Processes</h3>
<ul>
<li><strong>Authors: </strong>Marin Biloš, Anderson Schneider, Yuriy Nevmyvaka</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20031">https://arxiv.org/abs/2510.20031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20031">https://arxiv.org/pdf/2510.20031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20031]] Speculative Sampling for Parametric Temporal Point Processes(https://arxiv.org/abs/2510.20031)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Temporal point processes are powerful generative models for event sequences that capture complex dependencies in time-series data. They are commonly specified using autoregressive models that learn the distribution of the next event from the previous events. This makes sampling inherently sequential, limiting efficiency. In this paper, we propose a novel algorithm based on rejection sampling that enables exact sampling of multiple future values from existing TPP models, in parallel, and without requiring any architectural changes or retraining. Besides theoretical guarantees, our method demonstrates empirical speedups on real-world datasets, bridging the gap between expressive modeling and efficient parallel generation for large-scale TPP applications.</li>
<li><strong>摘要：</strong>时间点过程是事件序列的强大生成模型，可捕获时间序列数据中的复杂依赖性。它们通常使用自回归模型来指定，该模型从先前的事件中学习下一个事件的分布。这使得采样本质上是连续的，从而限制了效率。在本文中，我们提出了一种基于拒绝采样的新颖算法，该算法能够并行地从现有 TPP 模型中精确采样多个未来值，并且不需要任何架构更改或重新训练。除了理论保证之外，我们的方法还展示了对现实世界数据集的经验加速，缩小了大规模 TPP 应用的表达建模和高效并行生成之间的差距。</li>
</ul>

<h3>Title: Exposing Blindspots: Cultural Bias Evaluation in Generative Image Models</h3>
<ul>
<li><strong>Authors: </strong>Huichan Seo, Sieun Choi, Minki Hong, Yi Zhou, Junseo Kim, Lukman Ismaila, Naome Etori, Mehul Agarwal, Zhixuan Liu, Jihie Kim, Jean Oh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20042">https://arxiv.org/abs/2510.20042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20042">https://arxiv.org/pdf/2510.20042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20042]] Exposing Blindspots: Cultural Bias Evaluation in Generative Image Models(https://arxiv.org/abs/2510.20042)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative image models produce striking visuals yet often misrepresent culture. Prior work has examined cultural bias mainly in text-to-image (T2I) systems, leaving image-to-image (I2I) editors underexplored. We bridge this gap with a unified evaluation across six countries, an 8-category/36-subcategory schema, and era-aware prompts, auditing both T2I generation and I2I editing under a standardized protocol that yields comparable diagnostics. Using open models with fixed settings, we derive cross-country, cross-era, and cross-category evaluations. Our framework combines standard automatic metrics, a culture-aware retrieval-augmented VQA, and expert human judgments collected from native reviewers. To enable reproducibility, we release the complete image corpus, prompts, and configurations. Our study reveals three findings: (1) under country-agnostic prompts, models default to Global-North, modern-leaning depictions that flatten cross-country distinctions; (2) iterative I2I editing erodes cultural fidelity even when conventional metrics remain flat or improve; and (3) I2I models apply superficial cues (palette shifts, generic props) rather than era-consistent, context-aware changes, often retaining source identity for Global-South targets. These results highlight that culture-sensitive edits remain unreliable in current systems. By releasing standardized data, prompts, and human evaluation protocols, we provide a reproducible, culture-centered benchmark for diagnosing and tracking cultural bias in generative image models.</li>
<li><strong>摘要：</strong>生成图像模型可以产生引人注目的视觉效果，但常常歪曲文化。之前的工作主要研究了文本到图像（T2I）系统中的文化偏见，而图像到图像（I2I）编辑器的研究还不够。我们通过跨六个国家的统一评估、8 个类别/36 个子类别模式和时代感知提示来弥补这一差距，并根据标准化协议审核 T2I 生成和 I2I 编辑，从而产生可比的诊断结果。使用固定设置的开放模型，我们得出跨国家、跨时代、跨品类的评价。我们的框架结合了标准自动指标、文化感知检索增强的 VQA 以及从本地审阅者收集的专家人类判断。为了实现可重复性，我们发布了完整的图像语料库、提示和配置。我们的研究揭示了三个发现：（1）在与国家无关的提示下，模型默认为北半球、现代倾向的描述，从而消除了跨国差异； (2) 即使传统指标保持不变或有所改善，迭代的 I2I 编辑也会侵蚀文化保真度； (3) I2I 模型应用表面线索（调色板转换、通用道具），而不是时代一致的、情境感知的变化，通常保留全球南方目标的来源身份。这些结果凸显出文化敏感编辑在当前系统中仍然不可靠。通过发布标准化数据、提示和人类评估协议，我们提供了一个可重复的、以文化为中心的基准，用于诊断和跟踪生成图像模型中的文化偏见。</li>
</ul>

<h3>Title: Attentive Convolution: Unifying the Expressivity of Self-Attention with Convolutional Efficiency</h3>
<ul>
<li><strong>Authors: </strong>Hao Yu, Haoyu Chen, Yan Jiang, Wei Peng, Zhaodong Sun, Samuel Kaski, Guoying Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20092">https://arxiv.org/abs/2510.20092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20092">https://arxiv.org/pdf/2510.20092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20092]] Attentive Convolution: Unifying the Expressivity of Self-Attention with Convolutional Efficiency(https://arxiv.org/abs/2510.20092)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Self-attention (SA) has become the cornerstone of modern vision backbones for its powerful expressivity over traditional Convolutions (Conv). However, its quadratic complexity remains a critical bottleneck for practical applications. Given that Conv offers linear complexity and strong visual priors, continuing efforts have been made to promote the renaissance of Conv. However, a persistent performance chasm remains, highlighting that these modernizations have not yet captured the intrinsic expressivity that defines SA. In this paper, we re-examine the design of the CNNs, directed by a key question: what principles give SA its edge over Conv? As a result, we reveal two fundamental insights that challenge the long-standing design intuitions in prior research (e.g., Receptive field). The two findings are: (1) \textit{Adaptive routing}: SA dynamically regulates positional information flow according to semantic content, whereas Conv employs static kernels uniformly across all positions. (2) \textit{Lateral inhibition}: SA induces score competition among token weighting, effectively suppressing redundancy and sharpening representations, whereas Conv filters lack such inhibitory dynamics and exhibit considerable redundancy. Based on this, we propose \textit{Attentive Convolution} (ATConv), a principled reformulation of the convolutional operator that intrinsically injects these principles. Interestingly, with only $3\times3$ kernels, ATConv consistently outperforms various SA mechanisms in fundamental vision tasks. Building on ATConv, we introduce AttNet, a CNN family that can attain \textbf{84.4\%} ImageNet-1K Top-1 accuracy with only 27M parameters. In diffusion-based image generation, replacing all SA with the proposed $3\times 3$ ATConv in SiT-XL/2 reduces ImageNet FID by 0.15 in 400k steps with faster sampling. Code is available at: this http URL.</li>
<li><strong>摘要：</strong>自注意力（SA）因其比传统卷积（Conv）强大的表达能力而成为现代视觉骨干的基石。然而，其二次复杂度仍然是实际应用的关键瓶颈。鉴于 Conv 提供线性复杂性和强大的视觉先验，人们不断努力促进 Conv 的复兴。然而，持续存在的性能差距仍然存在，这凸显出这些现代化尚未捕捉到定义 SA 的内在表现力。在本文中，我们以一个关键问题为导向，重新审视了 CNN 的设计：哪些原则使 SA 优于 Conv？因此，我们揭示了两个基本见解，挑战了先前研究中长期存在的设计直觉（例如，感受野）。这两个发现是：（1）\textit{自适应路由}：SA根据语义内容动态调节位置信息流，而Conv在所有位置上统一采用静态内核。 (2) \textit{横向抑制}：SA 引起 token 权重之间的分数竞争，有效抑制冗余和锐化表示，而 Conv 过滤器缺乏这种抑制动态并表现出相当大的冗余。基于此，我们提出了 \textit{Attentive Convolution} (ATConv)，这是对卷积算子的原则性重新表述，本质上注入了这些原理。有趣的是，仅使用 3\times3$ 内核，ATConv 在基本视觉任务中始终优于各种 SA 机制。在 ATConv 的基础上，我们引入了 AttNet，这是一个 CNN 系列，只需 27M 个参数即可达到 \textbf{84.4\%} ImageNet-1K Top-1 精度。在基于扩散的图像生成中，用 SiT-XL/2 中提议的 $3\times 3$ ATConv 替换所有 SA，可以在 400k 步中将 ImageNet FID 减少 0.15，并且采样速度更快。代码可在以下位置获得：此 http URL。</li>
</ul>

<h3>Title: StableSketcher: Enhancing Diffusion Model for Pixel-based Sketch Generation via Visual Question Answering Feedback</h3>
<ul>
<li><strong>Authors: </strong>Jiho Park, Sieun Choi, Jaeyoon Seo, Jihie Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20093">https://arxiv.org/abs/2510.20093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20093">https://arxiv.org/pdf/2510.20093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20093]] StableSketcher: Enhancing Diffusion Model for Pixel-based Sketch Generation via Visual Question Answering Feedback(https://arxiv.org/abs/2510.20093)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Although recent advancements in diffusion models have significantly enriched the quality of generated images, challenges remain in synthesizing pixel-based human-drawn sketches, a representative example of abstract expression. To combat these challenges, we propose StableSketcher, a novel framework that empowers diffusion models to generate hand-drawn sketches with high prompt fidelity. Within this framework, we fine-tune the variational autoencoder to optimize latent decoding, enabling it to better capture the characteristics of sketches. In parallel, we integrate a new reward function for reinforcement learning based on visual question answering, which improves text-image alignment and semantic consistency. Extensive experiments demonstrate that StableSketcher generates sketches with improved stylistic fidelity, achieving better alignment with prompts compared to the Stable Diffusion baseline. Additionally, we introduce SketchDUO, to the best of our knowledge, the first dataset comprising instance-level sketches paired with captions and question-answer pairs, thereby addressing the limitations of existing datasets that rely on image-label pairs. Our code and dataset will be made publicly available upon acceptance.</li>
<li><strong>摘要：</strong>尽管扩散模型的最新进展显着提高了生成图像的质量，但合成基于像素的人体绘制草图（抽象表达的代表性示例）仍然存在挑战。为了应对这些挑战，我们提出了 StableSketcher，这是一个新颖的框架，使扩散模型能够生成具有高保真度的手绘草图。在此框架内，我们微调变分自动编码器以优化潜在解码，使其能够更好地捕获草图的特征。与此同时，我们集成了基于视觉问答的强化学习的新奖励函数，这提高了文本图像对齐和语义一致性。大量实验表明，与稳定扩散基线相比，StableSketcher 生成的草图具有更高的风格保真度，能够更好地与提示对齐。此外，据我们所知，我们引入了 SketchDUO，这是第一个数据集，包含实例级草图以及标题和问答对，从而解决了依赖图像标签对的现有数据集的局限性。我们的代码和数据集将在接受后公开发布。</li>
</ul>

<h3>Title: Inverse Image-Based Rendering for Light Field Generation from Single Images</h3>
<ul>
<li><strong>Authors: </strong>Hyunjun Jung, Hae-Gon Jeon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20132">https://arxiv.org/abs/2510.20132</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20132">https://arxiv.org/pdf/2510.20132</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20132]] Inverse Image-Based Rendering for Light Field Generation from Single Images(https://arxiv.org/abs/2510.20132)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>A concept of light-fields computed from multiple view images on regular grids has proven its benefit for scene representations, and supported realistic renderings of novel views and photographic effects such as refocusing and shallow depth of field. In spite of its effectiveness of light flow computations, obtaining light fields requires either computational costs or specialized devices like a bulky camera setup and a specialized microlens array. In an effort to broaden its benefit and applicability, in this paper, we propose a novel view synthesis method for light field generation from only single images, named inverse image-based rendering. Unlike previous attempts to implicitly rebuild 3D geometry or to explicitly represent objective scenes, our method reconstructs light flows in a space from image pixels, which behaves in the opposite way to image-based rendering. To accomplish this, we design a neural rendering pipeline to render a target ray in an arbitrary viewpoint. Our neural renderer first stores the light flow of source rays from the input image, then computes the relationships among them through cross-attention, and finally predicts the color of the target ray based on these relationships. After the rendering pipeline generates the first novel view from a single input image, the generated out-of-view contents are updated to the set of source rays. This procedure is iteratively performed while ensuring the consistent generation of occluded contents. We demonstrate that our inverse image-based rendering works well with various challenging datasets without any retraining or finetuning after once trained on synthetic dataset, and outperforms relevant state-of-the-art novel view synthesis methods.</li>
<li><strong>摘要：</strong>根据规则网格上的多视图图像计算的光场概念已证明其对于场景表示的好处，并支持新视图和摄影效果（例如重新聚焦和浅景深）的真实渲染。尽管光流计算非常有效，但获得光场需要计算成本或专用设备，例如笨重的相机设置和专用的微透镜阵列。为了扩大其优势和适用性，在本文中，我们提出了一种仅从单个图像生成光场的新颖视图合成方法，称为基于逆图像的渲染。与之前隐式重建 3D 几何或显式表示客观场景的尝试不同，我们的方法从图像像素重建空间中的光流，其行为与基于图像的渲染相反。为了实现这一目标，我们设计了一个神经渲染管道来在任意视点渲染目标光线。我们的神经渲染器首先存储来自输入图像的源光线的光流，然后通过交叉注意力计算它们之间的关系，最后根据这些关系预测目标光线的颜色。在渲染管线从单个输入图像生成第一个新视图之后，生成的视图外内容被更新为源光线集。该过程是迭代执行的，同时确保遮挡内容的生成一致。我们证明，我们的基于逆向图像的渲染可以很好地处理各种具有挑战性的数据集，在合成数据集上训练后无需任何重新训练或微调，并且优于相关的最先进的新颖视图合成方法。</li>
</ul>

<h3>Title: PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D Part Understanding</h3>
<ul>
<li><strong>Authors: </strong>Penghao Wang, Yiyang He, Xin Lv, Yukai Zhou, Lan Xu, Jingyi Yu, Jiayuan Gu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20155">https://arxiv.org/abs/2510.20155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20155">https://arxiv.org/pdf/2510.20155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20155]] PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D Part Understanding(https://arxiv.org/abs/2510.20155)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Understanding objects at the level of their constituent parts is fundamental to advancing computer vision, graphics, and robotics. While datasets like PartNet have driven progress in 3D part understanding, their reliance on untextured geometries and expert-dependent annotation limits scalability and usability. We introduce PartNeXt, a next-generation dataset addressing these gaps with over 23,000 high-quality, textured 3D models annotated with fine-grained, hierarchical part labels across 50 categories. We benchmark PartNeXt on two tasks: (1) class-agnostic part segmentation, where state-of-the-art methods (e.g., PartField, SAMPart3D) struggle with fine-grained and leaf-level parts, and (2) 3D part-centric question answering, a new benchmark for 3D-LLMs that reveals significant gaps in open-vocabulary part grounding. Additionally, training Point-SAM on PartNeXt yields substantial gains over PartNet, underscoring the dataset's superior quality and diversity. By combining scalable annotation, texture-aware labels, and multi-task evaluation, PartNeXt opens new avenues for research in structured 3D understanding.</li>
<li><strong>摘要：</strong>在其组成部分的层面上理解对象是推进计算机视觉、图形和机器人技术的基础。虽然像 PartNet 这样的数据集推动了 3D 零件理解的进步，但它们对无纹理几何形状和依赖于专家的注释的依赖限制了可扩展性和可用性。我们推出了 PartNeXt，这是一个下一代数据集，它通过 23,000 多个高质量、有纹理的 3D 模型来解决这些差距，并用 50 个类别的细粒度、分层零件标签进行注释。我们在两项任务上对 PartNeXt 进行基准测试：(1) 与类无关的零件分割，其中最先进的方法（例如 PartField、SAMPart3D）与细粒度和叶级零件作斗争，以及 (2) 以 3D 零件为中心的问答，这是 3D-LLM 的新基准，揭示了开放词汇零件基础方面的显着差距。此外，在 PartNeXt 上训练 Point-SAM 比 PartNet 获得了显着的收益，凸显了数据集的卓越质量和多样性。通过结合可扩展注释、纹理感知标签和多任务评估，PartNeXt 为结构化 3D 理解的研究开辟了新途径。</li>
</ul>

<h3>Title: IB-GAN: Disentangled Representation Learning with Information Bottleneck Generative Adversarial Networks</h3>
<ul>
<li><strong>Authors: </strong>Insu Jeon, Wonkwang Lee, Myeongjang Pyeon, Gunhee Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20165">https://arxiv.org/abs/2510.20165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20165">https://arxiv.org/pdf/2510.20165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20165]] IB-GAN: Disentangled Representation Learning with Information Bottleneck Generative Adversarial Networks(https://arxiv.org/abs/2510.20165)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We propose a new GAN-based unsupervised model for disentangled representation learning. The new model is discovered in an attempt to utilize the Information Bottleneck (IB) framework to the optimization of GAN, thereby named IB-GAN. The architecture of IB-GAN is partially similar to that of InfoGAN but has a critical difference; an intermediate layer of the generator is leveraged to constrain the mutual information between the input and the generated output. The intermediate stochastic layer can serve as a learnable latent distribution that is trained with the generator jointly in an end-to-end fashion. As a result, the generator of IB-GAN can harness the latent space in a disentangled and interpretable manner. With the experiments on dSprites and Color-dSprites dataset, we demonstrate that IB-GAN achieves competitive disentanglement scores to those of state-of-the-art \b{eta}-VAEs and outperforms InfoGAN. Moreover, the visual quality and the diversity of samples generated by IB-GAN are often better than those by \b{eta}-VAEs and Info-GAN in terms of FID score on CelebA and 3D Chairs dataset.</li>
<li><strong>摘要：</strong>我们提出了一种新的基于 GAN 的无监督模型，用于解缠结表示学习。新模型的发现是为了尝试利用信息瓶颈（IB）框架来优化 GAN，因此命名为 IB-GAN。 IB-GAN 的架构与 InfoGAN 部分相似，但有一个关键的区别；利用生成器的中间层来约束输入和生成的输出之间的互信息。中间随机层可以作为可学习的潜在分布，以端到端的方式与生成器联合训练。因此，IB-GAN 的生成器可以以一种解开且可解释的方式利用潜在空间。通过在 dSprites 和 Color-dSprites 数据集上的实验，我们证明 IB-GAN 实现了与最先进的 \b{eta}-VAE 相媲美的解缠分数，并且性能优于 InfoGAN。此外，就 CelebA 和 3D Chairs 数据集上的 FID 得分而言，IB-GAN 生成的样本的视觉质量和多样性通常优于 eta-VAE 和 Info-GAN。</li>
</ul>

<h3>Title: Empowering Targeted Neighborhood Search via Hyper Tour for Large-Scale TSP</h3>
<ul>
<li><strong>Authors: </strong>Tongkai Lu, Shuai Ma, Chongyang Tao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20169">https://arxiv.org/abs/2510.20169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20169">https://arxiv.org/pdf/2510.20169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20169]] Empowering Targeted Neighborhood Search via Hyper Tour for Large-Scale TSP(https://arxiv.org/abs/2510.20169)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Traveling Salesman Problem (TSP) is a classic NP-hard problem that has garnered significant attention from both academia and industry. While neural-based methods have shown promise for solving TSPs, they still face challenges in scaling to larger instances, particularly in memory constraints associated with global heatmaps, edge weights, or access matrices, as well as in generating high-quality initial solutions and insufficient global guidance for efficiently navigating vast search spaces. To address these challenges, we propose a Hyper Tour Guided Neighborhood Search (HyperNS) method for large-scale TSP instances. Inspired by the ``clustering first, route second" strategy, our approach initially divides the TSP instance into clusters using a sparse heatmap graph and abstracts them as supernodes, followed by the generation of a hyper tour to guide both the initialization and optimization processes. This method reduces the search space by focusing on edges relevant to the hyper tour, leading to more efficient and effective optimization. Experimental results on both synthetic and real-world datasets demonstrate that our approach outperforms existing neural-based methods, particularly in handling larger-scale instances, offering a significant reduction in the gap to the optimal solution.</li>
<li><strong>摘要：</strong>旅行商问题（TSP）是一个经典的 NP 难问题，引起了学术界和工业界的广泛关注。虽然基于神经的方法在解决 TSP 方面表现出了希望，但它们在扩展到更大的实例方面仍然面临挑战，特别是在与全局热图、边缘权重或访问矩阵相关的内存限制方面，以及在生成高质量的初始解决方案和有效导航巨大搜索空间的全局指导不足方面。为了应对这些挑战，我们提出了一种针对大规模 TSP 实例的超级漫游引导邻域搜索 (HyperNS) 方法。受“聚类第一，路线第二”策略的启发，我们的方法首先使用稀疏热图将 TSP 实例划分为集群，并将它们抽象为超级节点，然后生成超级游览来指导初始化和优化过程。该方法通过关注与超级游览相关的边来减少搜索空间，从而实现更高效和有效的优化。在合成数据集和真实数据集上的实验结果 证明我们的方法优于现有的基于神经的方法，特别是在处理更大规模的实例时，显着缩小了与最佳解决方案的差距。</li>
</ul>

<h3>Title: Evaluating Video Models as Simulators of Multi-Person Pedestrian Trajectories</h3>
<ul>
<li><strong>Authors: </strong>Aaron Appelle, Jerome P. Lynch</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20182">https://arxiv.org/abs/2510.20182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20182">https://arxiv.org/pdf/2510.20182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20182]] Evaluating Video Models as Simulators of Multi-Person Pedestrian Trajectories(https://arxiv.org/abs/2510.20182)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large-scale video generation models have demonstrated high visual realism in diverse contexts, spurring interest in their potential as general-purpose world simulators. Existing benchmarks focus on individual subjects rather than scenes with multiple interacting people. However, the plausibility of multi-agent dynamics in generated videos remains unverified. We propose a rigorous evaluation protocol to benchmark text-to-video (T2V) and image-to-video (I2V) models as implicit simulators of pedestrian dynamics. For I2V, we leverage start frames from established datasets to enable comparison with a ground truth video dataset. For T2V, we develop a prompt suite to explore diverse pedestrian densities and interactions. A key component is a method to reconstruct 2D bird's-eye view trajectories from pixel-space without known camera parameters. Our analysis reveals that leading models have learned surprisingly effective priors for plausible multi-agent behavior. However, failure modes like merging and disappearing people highlight areas for future improvement.</li>
<li><strong>摘要：</strong>大规模视频生成模型在不同的环境中表现出了高度的视觉真实感，激发了人们对其作为通用世界模拟器的潜力的兴趣。现有的基准侧重于单个主题，而不是多人交互的场景。然而，生成视频中多智能体动态的合理性仍未得到验证。我们提出了严格的评估协议，以将文本到视频（T2V）和图像到视频（I2V）模型作为行人动力学的隐式模拟器进行基准测试。对于 I2V，我们利用已建立的数据集的起始帧来与地面真实视频数据集进行比较。对于 T2V，我们开发了一套提示套件来探索不同的行人密度和互动。一个关键组成部分是一种在没有已知相机参数的情况下从像素空间重建 2D 鸟瞰视图轨迹的方法。我们的分析表明，领先的模型已经学习了关于合理的多智能体行为的令人惊讶的有效先验。然而，人员合并和消失等失败模式凸显了未来需要改进的领域。</li>
</ul>

<h3>Title: RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling</h3>
<ul>
<li><strong>Authors: </strong>Bingjie Gao, Qianli Ma, Xiaoxue Wu, Shuai Yang, Guanzhou Lan, Haonan Zhao, Jiaxuan Chen, Qingyang Liu, Yu Qiao, Xinyuan Chen, Yaohui Wang, Li Niu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20206">https://arxiv.org/abs/2510.20206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20206">https://arxiv.org/pdf/2510.20206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20206]] RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling(https://arxiv.org/abs/2510.20206)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Prompt design plays a crucial role in text-to-video (T2V) generation, yet user-provided prompts are often short, unstructured, and misaligned with training data, limiting the generative potential of diffusion-based T2V models. We present \textbf{RAPO++}, a cross-stage prompt optimization framework that unifies training-data--aligned refinement, test-time iterative scaling, and large language model (LLM) fine-tuning to substantially improve T2V generation without modifying the underlying generative backbone. In \textbf{Stage 1}, Retrieval-Augmented Prompt Optimization (RAPO) enriches user prompts with semantically relevant modifiers retrieved from a relation graph and refactors them to match training distributions, enhancing compositionality and multi-object fidelity. \textbf{Stage 2} introduces Sample-Specific Prompt Optimization (SSPO), a closed-loop mechanism that iteratively refines prompts using multi-source feedback -- including semantic alignment, spatial fidelity, temporal coherence, and task-specific signals such as optical flow -- yielding progressively improved video generation quality. \textbf{Stage 3} leverages optimized prompt pairs from SSPO to fine-tune the rewriter LLM, internalizing task-specific optimization patterns and enabling efficient, high-quality prompt generation even before inference. Extensive experiments across five state-of-the-art T2V models and five benchmarks demonstrate that RAPO++ achieves significant gains in semantic alignment, compositional reasoning, temporal stability, and physical plausibility, outperforming existing methods by large margins. Our results highlight RAPO++ as a model-agnostic, cost-efficient, and scalable solution that sets a new standard for prompt optimization in T2V generation. The code is available at this https URL.</li>
<li><strong>摘要：</strong>提示设计在文本到视频 (T2V) 生成中起着至关重要的作用，但用户提供的提示通常很短、非结构化且与训练数据不一致，限制了基于扩散的 T2V 模型的生成潜力。我们提出了 \textbf{RAPO++}，一个跨阶段的提示优化框架，它统一了训练数据对齐细化、测试时迭代缩放和大语言模型 (LLM) 微调，以在不修改底层生成主干的情况下大幅改进 T2V 生成。在第 1 阶段，检索增强提示优化（RAPO）通过从关系图中检索到的语义相关修饰符来丰富用户提示，并重构它们以匹配训练分布，从而增强组合性和多对象保真度。第 2 阶段引入了特定于样本的提示优化（SSPO），这是一种闭环机制，可使用多源反馈迭代地细化提示，包括语义对齐、空间保真度、时间连贯性和特定于任务的信号（例如光流），从而逐步提高视频生成质量。 \textbf{第 3 阶段}利用 SSPO 的优化提示对来微调重写器 LLM，内化特定于任务的优化模式，并在推理之前实现高效、高质量的提示生成。跨越五个最先进的 T2V 模型和五个基准的广泛实验表明，RAPO++ 在语义对齐、组合推理、时间稳定性和物理合理性方面取得了显着的进步，大大优于现有方法。我们的结果凸显了 RAPO++ 作为一种与模型无关、经济高效且可扩展的解决方案，为 T2V 生成的快速优化设立了新标准。该代码可从此 https URL 获取。</li>
</ul>

<h3>Title: FlowCycle: Pursuing Cycle-Consistent Flows for Text-based Editing</h3>
<ul>
<li><strong>Authors: </strong>Yanghao Wang, Zhen Wang, Long Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20212">https://arxiv.org/abs/2510.20212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20212">https://arxiv.org/pdf/2510.20212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20212]] FlowCycle: Pursuing Cycle-Consistent Flows for Text-based Editing(https://arxiv.org/abs/2510.20212)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Recent advances in pre-trained text-to-image flow models have enabled remarkable progress in text-based image editing. Mainstream approaches always adopt a corruption-then-restoration paradigm, where the source image is first corrupted into an ``intermediate state'' and then restored to the target image under the prompt guidance. However, current methods construct this intermediate state in a target-agnostic manner, i.e., they primarily focus on realizing source image reconstruction while neglecting the semantic gaps towards the specific editing target. This design inherently results in limited editability or inconsistency when the desired modifications substantially deviate from the source. In this paper, we argue that the intermediate state should be target-aware, i.e., selectively corrupting editing-relevant contents while preserving editing-irrelevant ones. To this end, we propose FlowCycle, a novel inversion-free and flow-based editing framework that parameterizes corruption with learnable noises and optimizes them through a cycle-consistent process. By iteratively editing the source to the target and recovering back to the source with dual consistency constraints, FlowCycle learns to produce a target-aware intermediate state, enabling faithful modifications while preserving source consistency. Extensive ablations have demonstrated that FlowCycle achieves superior editing quality and consistency over state-of-the-art methods.</li>
<li><strong>摘要：</strong>预训练文本到图像流模型的最新进展使得基于文本的图像编辑取得了显着进展。主流方法总是采用先损坏后恢复的范例，其中源图像首先被损坏到“中间状态”，然后在提示指导下恢复到目标图像。然而，当前的方法以与目标无关的方式构建这种中间状态，即它们主要关注于实现源图像重建，而忽略了与特定编辑目标的语义差距。当所需的修改实质上偏离源时，这种设计本质上会导致有限的可编辑性或不一致。在本文中，我们认为中间状态应该是目标感知的，即选择性地破坏与编辑相关的内容，同时保留与编辑无关的内容。为此，我们提出了 FlowCycle，一种新颖的无反转和基于流的编辑框架，它用可学习的噪声参数化损坏，并通过循环一致的过程对其进行优化。通过迭代地将源编辑到目标并使用双重一致性约束恢复回源，FlowCycle 学会生成目标感知的中间状态，从而在保持源一致性的同时实现忠实的修改。广泛的消融表明 FlowCycle 比最先进的方法实现了卓越的编辑质量和一致性。</li>
</ul>

<h3>Title: EditInfinity: Image Editing with Binary-Quantized Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Jiahuan Wang, Yuxin Chen, Jun Yu, Guangming Lu, Wenjie Pei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20217">https://arxiv.org/abs/2510.20217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20217">https://arxiv.org/pdf/2510.20217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20217]] EditInfinity: Image Editing with Binary-Quantized Generative Models(https://arxiv.org/abs/2510.20217)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Adapting pretrained diffusion-based generative models for text-driven image editing with negligible tuning overhead has demonstrated remarkable potential. A classical adaptation paradigm, as followed by these methods, first infers the generative trajectory inversely for a given source image by image inversion, then performs image editing along the inferred trajectory guided by the target text prompts. However, the performance of image editing is heavily limited by the approximation errors introduced during image inversion by diffusion models, which arise from the absence of exact supervision in the intermediate generative steps. To circumvent this issue, we investigate the parameter-efficient adaptation of VQ-based generative models for image editing, and leverage their inherent characteristic that the exact intermediate quantized representations of a source image are attainable, enabling more effective supervision for precise image inversion. Specifically, we propose \emph{EditInfinity}, which adapts \emph{Infinity}, a binary-quantized generative model, for image editing. We propose an efficient yet effective image inversion mechanism that integrates text prompting rectification and image style preservation, enabling precise image inversion. Furthermore, we devise a holistic smoothing strategy which allows our \emph{EditInfinity} to perform image editing with high fidelity to source images and precise semantic alignment to the text prompts. Extensive experiments on the PIE-Bench benchmark across "add", "change", and "delete" editing operations, demonstrate the superior performance of our model compared to state-of-the-art diffusion-based baselines. Code available at: this https URL.</li>
<li><strong>摘要：</strong>采用预训练的基于扩散的生成模型进行文本驱动的图像编辑，并且调整开销可以忽略不计，已经展现出巨大的潜力。这些方法遵循的经典适应范式首先通过图像反转来逆向推断给定源图像的生成轨迹，然后沿着由目标文本提示引导的推断轨迹进行图像编辑。然而，图像编辑的性能受到扩散模型图像反演过程中引入的近似误差的严重限制，这些误差是由于中间生成步骤中缺乏精确监督而产生的。为了解决这个问题，我们研究了用于图像编辑的基于 VQ 的生成模型的参数有效适应，并利用其固有特性，即可以获得源图像的精确中间量化表示，从而能够更有效地监督精确的图像反演。具体来说，我们提出 \emph{EditInfinity}，它采用 \emph{Infinity}（一种二进制量化生成模型）来进行图像编辑。我们提出了一种高效且有效的图像反转机制，将文本提示校正和图像风格保留相结合，从而实现精确的图像反转。此外，我们设计了一种整体平滑策略，使我们的 \emph{EditInfinity} 能够以对源图像的高保真度和与文本提示的精确语义对齐来执行图像编辑。在“添加”、“更改”和“删除”编辑操作的 PIE-Bench 基准上进行的广泛实验表明，与最先进的基于扩散的基线相比，我们的模型具有卓越的性能。代码位于：此 https URL。</li>
</ul>

<h3>Title: Calibrating Multimodal Consensus for Emotion Recognition</h3>
<ul>
<li><strong>Authors: </strong>Guowei Zhong, Junjie Li, Huaiyu Zhu, Ruohong Huan, Yun Pan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20256">https://arxiv.org/abs/2510.20256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20256">https://arxiv.org/pdf/2510.20256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20256]] Calibrating Multimodal Consensus for Emotion Recognition(https://arxiv.org/abs/2510.20256)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In recent years, Multimodal Emotion Recognition (MER) has made substantial progress. Nevertheless, most existing approaches neglect the semantic inconsistencies that may arise across modalities, such as conflicting emotional cues between text and visual inputs. Besides, current methods are often dominated by the text modality due to its strong representational capacity, which can compromise recognition accuracy. To address these challenges, we propose a model termed Calibrated Multimodal Consensus (CMC). CMC introduces a Pseudo Label Generation Module (PLGM) to produce pseudo unimodal labels, enabling unimodal pretraining in a self-supervised fashion. It then employs a Parameter-free Fusion Module (PFM) and a Multimodal Consensus Router (MCR) for multimodal finetuning, thereby mitigating text dominance and guiding the fusion process toward a more reliable consensus. Experimental results demonstrate that CMC achieves performance on par with or superior to state-of-the-art methods across four datasets, CH-SIMS, CH-SIMS v2, CMU-MOSI, and CMU-MOSEI, and exhibits notable advantages in scenarios with semantic inconsistencies on CH-SIMS and CH-SIMS v2. The implementation of this work is publicly accessible at this https URL.</li>
<li><strong>摘要：</strong>近年来，多模态情感识别（MER）取得了长足的进步。然而，大多数现有方法忽略了跨模态可能出现的语义不一致，例如文本和视觉输入之间冲突的情感线索。此外，由于文本模态具有很强的表示能力，当前的方法往往以文本模态为主，这会损害识别的准确性。为了应对这些挑战，我们提出了一种称为校准多模式共识（CMC）的模型。 CMC 引入了伪标签生成模块 (PLGM) 来生成伪单峰标签，从而以自我监督的方式实现单峰预训练。然后，它采用无参数融合模块（PFM）和多模态共识路由器（MCR）进行多模态微调，从而减轻文本主导并引导融合过程达成更可靠的共识。实验结果表明，CMC 在 CH-SIMS、CH-SIMS v2、CMU-MOSI 和 CMU-MOSEI 四个数据集上实现了与最先进方法相当或更好的性能，并且在 CH-SIMS 和 CH-SIMS v2 语义不一致的场景中表现出显着的优势。这项工作的实现可通过此 https URL 公开访问。</li>
</ul>

<h3>Title: ResearchGPT: Benchmarking and Training LLMs for End-to-End Computer Science Research Workflows</h3>
<ul>
<li><strong>Authors: </strong>Penghao Wang, Yuhao Zhou, Mengxuan Wu, Ziheng Qin, Bangyuan Zhu, Shengbin Huang, Xuanlei Zhao, Panpan Zhang, Xiaojiang Peng, Yuzhang Shang, Jianfei Yang, Zheng Zhu, Tianlong Chen, Zhangyang Wang, Kai Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20279">https://arxiv.org/abs/2510.20279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20279">https://arxiv.org/pdf/2510.20279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20279]] ResearchGPT: Benchmarking and Training LLMs for End-to-End Computer Science Research Workflows(https://arxiv.org/abs/2510.20279)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) advance, the ultimate vision for their role in science is emerging: we could build an AI collaborator to effectively assist human beings throughout the entire scientific research process. We refer to this envisioned system as ResearchGPT. Given that scientific research progresses through multiple interdependent phases, achieving this vision requires rigorous benchmarks that evaluate the end-to-end workflow rather than isolated sub-tasks. To this end, we contribute CS-54k, a high-quality corpus of scientific Q&A pairs in computer science, built from 14k CC-licensed papers. It is constructed through a scalable, paper-grounded pipeline that combines retrieval-augmented generation (RAG) with multi-stage quality control to ensure factual grounding. From this unified corpus, we derive two complementary subsets: CS-4k, a carefully curated benchmark for evaluating AI's ability to assist scientific research, and CS-50k, a large-scale training dataset. Extensive experiments demonstrate that CS-4k stratifies state-of-the-art LLMs into distinct capability tiers. Open models trained on CS-50k with supervised training and reinforcement learning demonstrate substantial improvements. Even 7B-scale models, when properly trained, outperform many larger proprietary systems, such as GPT-4.1, GPT-4o, and Gemini 2.5 Pro. This indicates that making AI models better research assistants relies more on domain-aligned training with high-quality data than on pretraining scale or general benchmark performance. We release CS-4k and CS-50k in the hope of fostering AI systems as reliable collaborators in CS research.</li>
<li><strong>摘要：</strong>随着大型语言模型（LLM）的进步，它们在科学中的作用的最终愿景正在浮现：我们可以建立一个人工智能协作者，在整个科学研究过程中有效地帮助人类。我们将这个设想的系统称为 ResearchGPT。鉴于科学研究要经历多个相互依赖的阶段，实现这一愿景需要严格的基准来评估端到端工作流程而不是孤立的子任务。为此，我们贡献了 CS-54k，这是一个高质量的计算机科学科学问答对语料库，由 14k CC 许可的论文构建而成。它是通过可扩展的纸质管道构建的，该管道将检索增强生成（RAG）与多阶段质量控制相结合，以确保事实基础。从这个统一的语料库中，我们得出了两个互补的子集：CS-4k（一个精心策划的基准，用于评估人工智能协助科学研究的能力）和 CS-50k（一个大规模训练数据集）。大量实验表明，CS-4k 将最先进的法学硕士分为不同的能力层级。在 CS-50k 上经过监督训练和强化学习训练的开放模型显示出显着的改进。即使是 7B 规模的模型，如果经过适当的训练，其性能也优于许多较大的专有系统，例如 GPT-4.1、GPT-4o 和 Gemini 2.5 Pro。这表明，让人工智能模型成为更好的研究助理，更多地依赖于高质量数据的领域对齐训练，而不是预训练规模或一般基准性能。我们发布 CS-4k 和 CS-50k，希望将 AI 系统培养成 CS 研究的可靠合作者。</li>
</ul>

<h3>Title: Breakdance Video classification in the age of Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Sauptik Dhar, Naveen Ramakrishnan, Michelle Munson</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20287">https://arxiv.org/abs/2510.20287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20287">https://arxiv.org/pdf/2510.20287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20287]] Breakdance Video classification in the age of Generative AI(https://arxiv.org/abs/2510.20287)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Large Vision Language models have seen huge application in several sports use-cases recently. Most of these works have been targeted towards a limited subset of popular sports like soccer, cricket, basketball etc; focusing on generative tasks like visual question answering, highlight generation. This work analyzes the applicability of the modern video foundation models (both encoder and decoder) for a very niche but hugely popular dance sports - breakdance. Our results show that Video Encoder models continue to outperform state-of-the-art Video Language Models for prediction tasks. We provide insights on how to choose the encoder model and provide a thorough analysis into the workings of a finetuned decoder model for breakdance video classification.</li>
<li><strong>摘要：</strong>大视觉语言模型最近在几个体育用例中得到了广泛的应用。这些作品中的大多数都针对有限的流行运动，如足球、板球、篮球等；专注于生成任务，如视觉问答、亮点生成。这项工作分析了现代视频基础模型（编码器和解码器）对于非常小众但非常流行的舞蹈运动——霹雳舞的适用性。我们的结果表明，视频编码器模型在预测任务方面继续优于最先进的视频语言模型。我们提供了有关如何选择编码器模型的见解，并对用于霹雳舞视频分类的微调解码器模型的工作原理进行了全面分析。</li>
</ul>

<h3>Title: Synthetic Data for Robust Runway Detection</h3>
<ul>
<li><strong>Authors: </strong>Estelle Chigot, Dennis G. Wilson, Meriem Ghrib, Fabrice Jimenez, Thomas Oberlin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20349">https://arxiv.org/abs/2510.20349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20349">https://arxiv.org/pdf/2510.20349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20349]] Synthetic Data for Robust Runway Detection(https://arxiv.org/abs/2510.20349)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Deep vision models are now mature enough to be integrated in industrial and possibly critical applications such as autonomous navigation. Yet, data collection and labeling to train such models requires too much efforts and costs for a single company or product. This drawback is more significant in critical applications, where training data must include all possible conditions including rare scenarios. In this perspective, generating synthetic images is an appealing solution, since it allows a cheap yet reliable covering of all the conditions and environments, if the impact of the synthetic-to-real distribution shift is mitigated. In this article, we consider the case of runway detection that is a critical part in autonomous landing systems developed by aircraft manufacturers. We propose an image generation approach based on a commercial flight simulator that complements a few annotated real images. By controlling the image generation and the integration of real and synthetic data, we show that standard object detection models can achieve accurate prediction. We also evaluate their robustness with respect to adverse conditions, in our case nighttime images, that were not represented in the real data, and show the interest of using a customized domain adaptation strategy.</li>
<li><strong>摘要：</strong>深度视觉模型现在已经足够成熟，可以集成到工业和可能的关键应用（例如自主导航）中。然而，对于单个公司或产品来说，训练此类模型的数据收集和标记需要太多的努力和成本。这个缺点在关键应用中更为重要，因为训练数据必须包括所有可能的条件，包括罕见的场景。从这个角度来看，生成合成图像是一个有吸引力的解决方案，因为如果合成到真实分布转变的影响得到减轻，它可以廉价而可靠地覆盖所有条件和环境。在本文中，我们考虑跑道检测的情况，这是飞机制造商开发的自主着陆系统的关键部分。我们提出了一种基于商业飞行模拟器的图像生成方法，该方法补充了一些带注释的真实图像。通过控制图像生成以及真实数据和合成数据的集成，我们表明标准目标检测模型可以实现准确的预测。我们还评估了它们在不利条件下的鲁棒性，在我们的例子中是夜间图像，这些图像在真实数据中没有体现，并显示了使用定制域适应策略的兴趣。</li>
</ul>

<h3>Title: Positional Encoding Field</h3>
<ul>
<li><strong>Authors: </strong>Yunpeng Bai, Haoxiang Li, Qixing Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20385">https://arxiv.org/abs/2510.20385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20385">https://arxiv.org/pdf/2510.20385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20385]] Positional Encoding Field(https://arxiv.org/abs/2510.20385)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion Transformers (DiTs) have emerged as the dominant architecture for visual generation, powering state-of-the-art image and video models. By representing images as patch tokens with positional encodings (PEs), DiTs combine Transformer scalability with spatial and temporal inductive biases. In this work, we revisit how DiTs organize visual content and discover that patch tokens exhibit a surprising degree of independence: even when PEs are perturbed, DiTs still produce globally coherent outputs, indicating that spatial coherence is primarily governed by PEs. Motivated by this finding, we introduce the Positional Encoding Field (PE-Field), which extends positional encodings from the 2D plane to a structured 3D field. PE-Field incorporates depth-aware encodings for volumetric reasoning and hierarchical encodings for fine-grained sub-patch control, enabling DiTs to model geometry directly in 3D space. Our PE-Field-augmented DiT achieves state-of-the-art performance on single-image novel view synthesis and generalizes to controllable spatial image editing.</li>
<li><strong>摘要：</strong>扩散变压器 (DiT) 已成为视觉生成的主导架构，为最先进的图像和视频模型提供动力。通过将图像表示为具有位置编码 (PE) 的补丁标记，DiT 将 Transformer 可扩展性与空间和时间归纳偏差相结合。在这项工作中，我们重新审视 DiT 如何组织视觉内容，并发现补丁标记表现出令人惊讶的独立性：即使 PE 受到干扰，DiT 仍然产生全局一致的输出，表明空间一致性主要由 PE 控制。受这一发现的启发，我们引入了位置编码场（PE-Field），它将位置编码从 2D 平面扩展到结构化 3D 场。 PE-Field 结合了用于体积推理的深度感知编码和用于细粒度子块控制的分层编码，使 DiT 能够直接在 3D 空间中对几何进行建模。我们的 PE 场增强 DiT 在单图像新颖视图合成方面实现了最先进的性能，并推广到可控空间图像编辑。</li>
</ul>

<h3>Title: An Empirical Study of Sample Selection Strategies for Large Language Model Repair</h3>
<ul>
<li><strong>Authors: </strong>Xuran Li, Jingyi Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20428">https://arxiv.org/abs/2510.20428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20428">https://arxiv.org/pdf/2510.20428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20428]] An Empirical Study of Sample Selection Strategies for Large Language Model Repair(https://arxiv.org/abs/2510.20428)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly deployed in real-world systems, yet they can produce toxic or biased outputs that undermine safety and trust. Post-hoc model repair provides a practical remedy, but the high cost of parameter updates motivates selective use of repair data. Despite extensive prior work on data selection for model training, it remains unclear which sampling criteria are most effective and efficient when applied specifically to behavioral repair of large generative models. Our study presents a systematic analysis of sample prioritization strategies for LLM repair. We evaluate five representative selection methods, including random sampling, K-Center, gradient-norm-based selection(GraNd), stratified coverage (CCS), and a Semantic-Aware Prioritized Sampling (SAPS) approach we proposed. Repair effectiveness and trade-offs are assessed through toxicity reduction, perplexity on WikiText-2 and LAMBADA, and three composite metrics: the Repair Proximity Score (RPS), the Overall Performance Score (OPS), and the Repair Efficiency Score (RES). Experimental results show that SAPS achieves the best balance between detoxification, utility preservation, and efficiency, delivering comparable or superior repair outcomes with substantially less data. Random sampling remains effective for large or robust models, while high-overhead methods such as CCS and GraNd provide limited benefit. The optimal data proportion depends on model scale and repair method, indicating that sample selection should be regarded as a tunable component of repair pipelines. Overall, these findings establish selection-based repair as an efficient and scalable paradigm for maintaining LLM reliability.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 越来越多地部署在现实世界的系统中，但它们可能会产生有毒或有偏见的输出，从而破坏安全性和信任。事后模型修复提供了一种实用的补救措施，但参数更新的高成本促使选择性地使用修复数据。尽管先前在模型训练的数据选择方面进行了大量工作，但仍不清楚在专门应用于大型生成模型的行为修复时哪种采样标准最有效和高效。我们的研究对法学硕士修复的样本优先策略进行了系统分析。我们评估了五种代表性的选择方法，包括随机抽样、K-Center、基于梯度范数的选择（GraNd）、分层覆盖（CCS）和我们提出的语义感知优先抽样（SAPS）方法。修复有效性和权衡通过毒性降低、WikiText-2 和 LAMBADA 的困惑度以及三个复合指标进行评估：修复邻近得分 (RPS)、总体性能得分 (OPS) 和修复效率得分 (RES)。实验结果表明，SAPS 在解毒、效用保存和效率之间实现了最佳平衡，以更少的数据提供可比或更好的修复结果。随机采样对于大型或稳健的模型仍然有效，而 CCS 和 GraNd 等高开销方法的好处有限。最佳数据比例取决于模型规模和修复方法，这表明样本选择应被视为修复管道的可调组成部分。总体而言，这些发现将基于选择的修复确立为维持法学硕士可靠性的有效且可扩展的范例。</li>
</ul>

<h3>Title: Transferable Black-Box One-Shot Forging of Watermarks via Image Preference Models</h3>
<ul>
<li><strong>Authors: </strong>Tomáš Souček, Sylvestre-Alvise Rebuffi, Pierre Fernandez, Nikola Jovanović, Hady Elsahar, Valeriu Lacatusu, Tuan Tran, Alexandre Mourachko</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20468">https://arxiv.org/abs/2510.20468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20468">https://arxiv.org/pdf/2510.20468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20468]] Transferable Black-Box One-Shot Forging of Watermarks via Image Preference Models(https://arxiv.org/abs/2510.20468)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent years have seen a surge in interest in digital content watermarking techniques, driven by the proliferation of generative models and increased legal pressure. With an ever-growing percentage of AI-generated content available online, watermarking plays an increasingly important role in ensuring content authenticity and attribution at scale. There have been many works assessing the robustness of watermarking to removal attacks, yet, watermark forging, the scenario when a watermark is stolen from genuine content and applied to malicious content, remains underexplored. In this work, we investigate watermark forging in the context of widely used post-hoc image watermarking. Our contributions are as follows. First, we introduce a preference model to assess whether an image is watermarked. The model is trained using a ranking loss on purely procedurally generated images without any need for real watermarks. Second, we demonstrate the model's capability to remove and forge watermarks by optimizing the input image through backpropagation. This technique requires only a single watermarked image and works without knowledge of the watermarking model, making our attack much simpler and more practical than attacks introduced in related work. Third, we evaluate our proposed method on a variety of post-hoc image watermarking models, demonstrating that our approach can effectively forge watermarks, questioning the security of current watermarking approaches. Our code and further resources are publicly available.</li>
<li><strong>摘要：</strong>近年来，由于生成模型的激增和法律压力的增加，人们对数字内容水印技术的兴趣激增。随着人工智能生成的在线内容比例不断增加，水印在确保大规模内容的真实性和归属方面发挥着越来越重要的作用。已经有许多工作评估水印对删除攻击的鲁棒性，然而，水印伪造，即从真实内容中窃取水印并将其应用于恶意内容的情况，仍然没有得到充分研究。在这项工作中，我们在广泛使用的事后图像水印的背景下研究水印伪造。我们的贡献如下。首先，我们引入一个偏好模型来评估图像是否带有水印。该模型使用纯粹程序生成的图像上的排名损失进行训练，不需要真正的水印。其次，我们通过反向传播优化输入图像来演示模型去除和伪造水印的能力。该技术仅需要单个水印图像，并且无需了解水印模型即可工作，使得我们的攻击比相关工作中引入的攻击更简单、更实用。第三，我们在各种事后图像水印模型上评估我们提出的方法，证明我们的方法可以有效地伪造水印，质疑当前水印方法的安全性。我们的代码和更多资源是公开的。</li>
</ul>

<h3>Title: Hurdle-IMDL: An Imbalanced Learning Framework for Infrared Rainfall Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Fangjian Zhang, Xiaoyong Zhuge, Wenlan Wang, Haixia Xiao, Yuying Zhu, Siyang Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.ao-ph, physics.geo-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20486">https://arxiv.org/abs/2510.20486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20486">https://arxiv.org/pdf/2510.20486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20486]] Hurdle-IMDL: An Imbalanced Learning Framework for Infrared Rainfall Retrieval(https://arxiv.org/abs/2510.20486)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Artificial intelligence has advanced quantitative remote sensing, yet its effectiveness is constrained by imbalanced label distribution. This imbalance leads conventionally trained models to favor common samples, which in turn degrades retrieval performance for rare ones. Rainfall retrieval exemplifies this issue, with performance particularly compromised for heavy rain. This study proposes Hurdle-Inversion Model Debiasing Learning (IMDL) framework. Following a divide-and-conquer strategy, imbalance in the rain distribution is decomposed into two components: zero inflation, defined by the predominance of non-rain samples; and long tail, defined by the disproportionate abundance of light-rain samples relative to heavy-rain samples. A hurdle model is adopted to handle the zero inflation, while IMDL is proposed to address the long tail by transforming the learning object into an unbiased ideal inverse model. Comprehensive evaluation via statistical metrics and case studies investigating rainy weather in eastern China confirms Hurdle-IMDL's superiority over conventional, cost-sensitive, generative, and multi-task learning methods. Its key advancements include effective mitigation of systematic underestimation and a marked improvement in the retrieval of heavy-to-extreme rain. IMDL offers a generalizable approach for addressing imbalance in distributions of environmental variables, enabling enhanced retrieval of rare yet high-impact events.</li>
<li><strong>摘要：</strong>人工智能具有先进的定量遥感，但其有效性受到标签分布不平衡的限制。这种不平衡导致传统训练的模型偏向于常见样本，从而降低了稀有样本的检索性能。降雨量检索就是这个问题的例证，大雨时性能尤其受到影响。本研究提出了跨栏反转模型去偏学习（IMDL）框架。按照分而治之的策略，降雨分布的不平衡被分解为两个组成部分：零通货膨胀，由非降雨样本的优势定义；长尾，由小雨样本相对于大雨样本的丰度不成比例来定义。采用障碍模型来处理零通胀，而 IMDL 则通过将学习对象转换为无偏理想逆模型来解决长尾问题。通过统计指标和调查中国东部阴雨天气的案例研究进行的综合评估证实了 Hurdle-IMDL 相对于传统的、成本敏感的、生成式和多任务学习方法的优越性。其主要进步包括有效缓解系统性低估，以及在大雨到极端降雨的反演方面取得显着改进。 IMDL 提供了一种通用方法来解决环境变量分布不平衡的问题，从而增强对罕见但影响大的事件的检索。</li>
</ul>

<h3>Title: EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion Personalization</h3>
<ul>
<li><strong>Authors: </strong>Yixiong Yang, Tao Wu, Senmao Li, Shiqi Yang, Yaxing Wang, Joost van de Weijer, Kai Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20512">https://arxiv.org/abs/2510.20512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20512">https://arxiv.org/pdf/2510.20512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20512]] EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion Personalization(https://arxiv.org/abs/2510.20512)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in accelerating text-to-image (T2I) diffusion models have enabled the synthesis of high-fidelity images even in a single step. However, personalizing these models to incorporate novel concepts remains a challenge due to the limited capacity of one-step models to capture new concept distributions effectively. We propose a bidirectional concept distillation framework, EchoDistill, to enable one-step diffusion personalization (1-SDP). Our approach involves an end-to-end training process where a multi-step diffusion model (teacher) and a one-step diffusion model (student) are trained simultaneously. The concept is first distilled from the teacher model to the student, and then echoed back from the student to the teacher. During the EchoDistill, we share the text encoder between the two models to ensure consistent semantic understanding. Following this, the student model is optimized with adversarial losses to align with the real image distribution and with alignment losses to maintain consistency with the teacher's output. Furthermore, we introduce the bidirectional echoing refinement strategy, wherein the student model leverages its faster generation capability to feedback to the teacher model. This bidirectional concept distillation mechanism not only enhances the student ability to personalize novel concepts but also improves the generative quality of the teacher model. Our experiments demonstrate that this collaborative framework significantly outperforms existing personalization methods over the 1-SDP setup, establishing a novel paradigm for rapid and effective personalization in T2I diffusion models.</li>
<li><strong>摘要：</strong>加速文本到图像 (T2I) 扩散模型的最新进展使得即使在单个步骤中也能合成高保真图像。然而，由于一步模型有效捕获新概念分布的能力有限，个性化这些模型以纳入新概念仍然是一项挑战。我们提出了一种双向概念蒸馏框架 EchoDistill，以实现一步扩散个性化（1-SDP）。我们的方法涉及端到端的训练过程，其中同时训练多步扩散模型（教师）和一步扩散模型（学生）。这个概念首先从教师模型提炼到学生，然后从学生回响到教师。在 EchoDistill 期间，我们在两个模型之间共享文本编码器，以确保一致的语义理解。此后，学生模型通过对抗性损失进行优化，以与真实图像分布对齐，并通过对齐损失进行优化，以保持与教师输出的一致性。此外，我们引入了双向回显细化策略，其中学生模型利用其更快的生成能力来反馈给教师模型。这种双向的概念蒸馏机制不仅增强了学生个性化新概念的能力，而且提高了教师模型的生成质量。我们的实验表明，与 1-SDP 设置相比，该协作框架显着优于现有的个性化方法，为 T2I 扩散模型中快速有效的个性化建立了一种新颖的范例。</li>
</ul>

<h3>Title: Blur2seq: Blind Deblurring and Camera Trajectory Estimation from a Single Camera Motion-blurred Image</h3>
<ul>
<li><strong>Authors: </strong>Guillermo Carbajal, Andrés Almansa, Pablo Musé</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20539">https://arxiv.org/abs/2510.20539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20539">https://arxiv.org/pdf/2510.20539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20539]] Blur2seq: Blind Deblurring and Camera Trajectory Estimation from a Single Camera Motion-blurred Image(https://arxiv.org/abs/2510.20539)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Motion blur caused by camera shake, particularly under large or rotational movements, remains a major challenge in image restoration. We propose a deep learning framework that jointly estimates the latent sharp image and the underlying camera motion trajectory from a single blurry image. Our method leverages the Projective Motion Blur Model (PMBM), implemented efficiently using a differentiable blur creation module compatible with modern networks. A neural network predicts a full 3D rotation trajectory, which guides a model-based restoration network trained end-to-end. This modular architecture provides interpretability by revealing the camera motion that produced the blur. Moreover, this trajectory enables the reconstruction of the sequence of sharp images that generated the observed blurry image. To further refine results, we optimize the trajectory post-inference via a reblur loss, improving consistency between the blurry input and the restored output. Extensive experiments show that our method achieves state-of-the-art performance on both synthetic and real datasets, particularly in cases with severe or spatially variant blur, where end-to-end deblurring networks struggle. Code and trained models are available at this https URL</li>
<li><strong>摘要：</strong>由相机抖动引起的运动模糊，特别是在大幅度或旋转运动下，仍然是图像恢复的主要挑战。我们提出了一种深度学习框架，可以从单个模糊图像中联合估计潜在的清晰图像和底层相机运动轨迹。我们的方法利用投影运动模糊模型（PMBM），使用与现代网络兼容的可微模糊创建模块有效实现。神经网络预测完整的 3D 旋转轨迹，指导端到端训练的基于模型的恢复网络。这种模块化架构通过揭示产生模糊的相机运动来提供可解释性。此外，该轨迹能够重建生成观察到的模糊图像的清晰图像序列。为了进一步细化结果，我们通过重新模糊损失优化轨迹后推理，提高模糊输入和恢复输出之间的一致性。大量的实验表明，我们的方法在合成数据集和真实数据集上都实现了最先进的性能，特别是在严重或空间变异模糊的情况下，端到端去模糊网络陷入困境。代码和经过训练的模型可在此 https URL 获取</li>
</ul>

<h3>Title: EmbodiedBrain: Expanding Performance Boundaries of Task Planning for Embodied Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Ding Zou, Feifan Wang, Mengyu Ge, Siyuan Fan, Zongbing Zhang, Wei Chen, Lingfeng Wang, Zhongyou Hu, Wenrui Yan, Zhengwei Gao, Hao Wang, Weizhao Jin, Yu Zhang, Hainan Zhao, Mingliang Zhang, Xianxian Xi, Yaru Zhang, Wenyuan Li, Zhengguang Gao, Yurui Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20578">https://arxiv.org/abs/2510.20578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20578">https://arxiv.org/pdf/2510.20578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20578]] EmbodiedBrain: Expanding Performance Boundaries of Task Planning for Embodied Intelligence(https://arxiv.org/abs/2510.20578)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The realization of Artificial General Intelligence (AGI) necessitates Embodied AI agents capable of robust spatial perception, effective task planning, and adaptive execution in physical environments. However, current large language models (LLMs) and multimodal LLMs (MLLMs) for embodied tasks suffer from key limitations, including a significant gap between model design and agent requirements, an unavoidable trade-off between real-time latency and performance, and the use of unauthentic, offline evaluation metrics. To address these challenges, we propose EmbodiedBrain, a novel vision-language foundation model available in both 7B and 32B parameter sizes. Our framework features an agent-aligned data structure and employs a powerful training methodology that integrates large-scale Supervised Fine-Tuning (SFT) with Step-Augumented Group Relative Policy Optimization (Step-GRPO), which boosts long-horizon task success by integrating preceding steps as Guided Precursors. Furthermore, we incorporate a comprehensive reward system, including a Generative Reward Model (GRM) accelerated at the infrastructure level, to improve training efficiency. For enable thorough validation, we establish a three-part evaluation system encompassing General, Planning, and End-to-End Simulation Benchmarks, highlighted by the proposal and open-sourcing of a novel, challenging simulation environment. Experimental results demonstrate that EmbodiedBrain achieves superior performance across all metrics, establishing a new state-of-the-art for embodied foundation models. Towards paving the way for the next generation of generalist embodied agents, we open-source all of our data, model weight, and evaluating methods, which are available at this https URL.</li>
<li><strong>摘要：</strong>通用人工智能 (AGI) 的实现需要具有强大的空间感知能力、有效的任务规划能力以及在物理环境中的自适应执行能力的嵌入式人工智能代理。然而，当前用于具体任务的大型语言模型（LLM）和多模态 LLM（MLLM）受到关键限制，包括模型设计和代理需求之间的巨大差距、实时延迟和性能之间不可避免的权衡，以及使用不真实的离线评估指标。为了应对这些挑战，我们提出了 EmbodiedBrain，这是一种新颖的视觉语言基础模型，可提供 7B 和 32B 参数大小。我们的框架采用与智能体对齐的数据结构，并采用强大的训练方法，将大规模监督微调（SFT）与逐步增强组相对策略优化（Step-GRPO）相结合，通过将前面的步骤集成为引导前体来提高长期任务的成功率。此外，我们还采用了全面的奖励系统，包括在基础设施层面加速的生成奖励模型（GRM），以提高培训效率。为了实现彻底的验证，我们建立了一个由三部分组成的评估系统，包括总体、规划和端到端仿真基准，并强调了新颖的、具有挑战性的仿真环境的提案和开源。实验结果表明，EmbodiedBrain 在所有指标上都实现了卓越的性能，为体现基础模型建立了新的最先进技术。为了为下一代多面手实体智能体铺平道路，我们开源了所有数据、模型权重和评估方法，这些数据可通过 https URL 获取。</li>
</ul>

<h3>Title: GenColorBench: A Color Evaluation Benchmark for Text-to-Image Generation Models</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Atif Butt, Alexandra Gomez-Villa, Tao Wu, Javier Vazquez-Corral, Joost Van De Weijer, Kai Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20586">https://arxiv.org/abs/2510.20586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20586">https://arxiv.org/pdf/2510.20586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20586]] GenColorBench: A Color Evaluation Benchmark for Text-to-Image Generation Models(https://arxiv.org/abs/2510.20586)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent years have seen impressive advances in text-to-image generation, with image generative or unified models producing high-quality images from text. Yet these models still struggle with fine-grained color controllability, often failing to accurately match colors specified in text prompts. While existing benchmarks evaluate compositional reasoning and prompt adherence, none systematically assess color precision. Color is fundamental to human visual perception and communication, critical for applications from art to design workflows requiring brand consistency. However, current benchmarks either neglect color or rely on coarse assessments, missing key capabilities such as interpreting RGB values or aligning with human expectations. To this end, we propose GenColorBench, the first comprehensive benchmark for text-to-image color generation, grounded in color systems like ISCC-NBS and CSS3/X11, including numerical colors which are absent elsewhere. With 44K color-focused prompts covering 400+ colors, it reveals models' true capabilities via perceptual and automated assessments. Evaluations of popular text-to-image models using GenColorBench show performance variations, highlighting which color conventions models understand best and identifying failure modes. Our GenColorBench assessments will guide improvements in precise color generation. The benchmark will be made public upon acceptance.</li>
<li><strong>摘要：</strong>近年来，文本到图像生成领域取得了令人瞩目的进步，图像生成或统一模型可以从文本生成高质量图像。然而，这些模型仍然难以实现细粒度的颜色可控性，通常无法准确匹配文本提示中指定的颜色。虽然现有的基准评估构图推理和提示遵守情况，但没有一个基准测试系统地评估颜色精度。颜色是人类视觉感知和交流的基础，对于需要品牌一致性的艺术到设计工作流程的应用至关重要。然而，当前的基准要么忽略颜色，要么依赖粗略评估，缺少解释 RGB 值或与人类期望保持一致等关键功能。为此，我们提出了 GenColorBench，这是第一个文本到图像颜色生成的综合基准，它基于 ISCC-NBS 和 CSS3/X11 等颜色系统，包括其他地方所没有的数字颜色。凭借涵盖 400 多种颜色的 44K 颜色提示，它通过感知和自动评估揭示模型的真实能力。使用 GenColorBench 对流行的文本到图像模型的评估显示了性能变化，突出显示了哪些颜色约定模型理解最好并识别故障模式。我们的 GenColorBench 评估将指导精确颜色生成的改进。该基准将在接受后公布。</li>
</ul>

<h3>Title: Practical Code RAG at Scale: Task-Aware Retrieval Design Choices under Compute Budgets</h3>
<ul>
<li><strong>Authors: </strong>Timur Galimzyanov, Olga Kolomyttseva, Egor Bogomolov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20609">https://arxiv.org/abs/2510.20609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20609">https://arxiv.org/pdf/2510.20609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20609]] Practical Code RAG at Scale: Task-Aware Retrieval Design Choices under Compute Budgets(https://arxiv.org/abs/2510.20609)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We study retrieval design for code-focused generation tasks under realistic compute budgets. Using two complementary tasks from Long Code Arena -- code completion and bug localization -- we systematically compare retrieval configurations across various context window sizes along three axes: (i) chunking strategy, (ii) similarity scoring, and (iii) splitting granularity. (1) For PL-PL, sparse BM25 with word-level splitting is the most effective and practical, significantly outperforming dense alternatives while being an order of magnitude faster. (2) For NL-PL, proprietary dense encoders (Voyager-3 family) consistently beat sparse retrievers, however requiring 100x larger latency. (3) Optimal chunk size scales with available context: 32-64 line chunks work best at small budgets, and whole-file retrieval becomes competitive at 16000 tokens. (4) Simple line-based chunking matches syntax-aware splitting across budgets. (5) Retrieval latency varies by up to 200x across configurations; BPE-based splitting is needlessly slow, and BM25 + word splitting offers the best quality-latency trade-off. Thus, we provide evidence-based recommendations for implementing effective code-oriented RAG systems based on task requirements, model constraints, and computational efficiency.</li>
<li><strong>摘要：</strong>我们研究在实际计算预算下以代码为中心的生成任务的检索设计。使用长代码竞技场中的两个互补任务——代码补全和错误定位——我们沿着三个轴系统地比较不同上下文窗口大小的检索配置：(i) 分块策略，(ii) 相似性评分，以及 (iii) 分割粒度。 (1) 对于 PL-PL，具有字级分割的稀疏 BM25 是最有效和实用的，显着优于密集替代方案，同时速度快一个数量级。 (2) 对于 NL-PL，专有的密集编码器（Voyager-3 系列）始终击败稀疏检索器，但需要 100 倍以上的延迟。 (3) 最佳块大小随可用上下文而变化：32-64 行块在小预算下效果最好，并且整个文件检索在 16000 个令牌时变得具有竞争力。 (4) 简单的基于行的分块与跨预算的语法感知分割相匹配。 (5) 不同配置的检索延迟差异高达 200 倍；基于 BPE 的拆分速度不必要地慢，而 BM25 + 字拆分提供了最佳的质量与延迟权衡。因此，我们根据任务要求、模型约束和计算效率提供基于证据的建议，用于实施有效的面向代码的 RAG 系统。</li>
</ul>

<h3>Title: Large Multimodal Models-Empowered Task-Oriented Autonomous Communications: Design Methodology and Implementation Challenges</h3>
<ul>
<li><strong>Authors: </strong>Hyun Jong Yang, Hyunsoo Kim, Hyeonho Noh, Seungnyun Kim, Byonghyo Shim</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20637">https://arxiv.org/abs/2510.20637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20637">https://arxiv.org/pdf/2510.20637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20637]] Large Multimodal Models-Empowered Task-Oriented Autonomous Communications: Design Methodology and Implementation Challenges(https://arxiv.org/abs/2510.20637)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) and large multimodal models (LMMs) have achieved unprecedented breakthrough, showcasing remarkable capabilities in natural language understanding, generation, and complex reasoning. This transformative potential has positioned them as key enablers for 6G autonomous communications among machines, vehicles, and humanoids. In this article, we provide an overview of task-oriented autonomous communications with LLMs/LMMs, focusing on multimodal sensing integration, adaptive reconfiguration, and prompt/fine-tuning strategies for wireless tasks. We demonstrate the framework through three case studies: LMM-based traffic control, LLM-based robot scheduling, and LMM-based environment-aware channel estimation. From experimental results, we show that the proposed LLM/LMM-aided autonomous systems significantly outperform conventional and discriminative deep learning (DL) model-based techniques, maintaining robustness under dynamic objectives, varying input parameters, and heterogeneous multimodal conditions where conventional static optimization degrades.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）和大型多模态模型（LMM）取得了前所未有的突破，在自然语言理解、生成和复杂推理方面展现了卓越的能力。这种变革潜力使它们成为机器、车辆和人形机器人之间 6G 自主通信的关键推动者。在本文中，我们概述了与 LLM/LMM 的面向任务的自主通信，重点关注多模态传感集成、自适应重新配置以及无线任务的提示/微调策略。我们通过三个案例研究演示了该框架：基于 LMM 的流量控制、基于 LMM 的机器人调度和基于 LMM 的环境感知信道估计。从实验结果来看，我们表明，所提出的 LLM/LMM 辅助自主系统显着优于传统的基于判别性深度学习 (DL) 模型的技术，在动态目标、变化的输入参数和传统静态优化退化的异构多模态条件下保持鲁棒性。</li>
</ul>

<h3>Title: Better Tokens for Better 3D: Advancing Vision-Language Modeling in 3D Medical Imaging</h3>
<ul>
<li><strong>Authors: </strong>Ibrahim Ethem Hamamci, Sezgin Er, Suprosanna Shit, Hadrien Reynaud, Dong Yang, Pengfei Guo, Marc Edgar, Daguang Xu, Bernhard Kainz, Bjoern Menze</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20639">https://arxiv.org/abs/2510.20639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20639">https://arxiv.org/pdf/2510.20639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20639]] Better Tokens for Better 3D: Advancing Vision-Language Modeling in 3D Medical Imaging(https://arxiv.org/abs/2510.20639)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent progress in vision-language modeling for 3D medical imaging has been fueled by large-scale computed tomography (CT) corpora with paired free-text reports, stronger architectures, and powerful pretrained models. This has enabled applications such as automated report generation and text-conditioned 3D image synthesis. Yet, current approaches struggle with high-resolution, long-sequence volumes: contrastive pretraining often yields vision encoders that are misaligned with clinical language, and slice-wise tokenization blurs fine anatomy, reducing diagnostic performance on downstream tasks. We introduce BTB3D (Better Tokens for Better 3D), a causal convolutional encoder-decoder that unifies 2D and 3D training and inference while producing compact, frequency-aware volumetric tokens. A three-stage training curriculum enables (i) local reconstruction, (ii) overlapping-window tiling, and (iii) long-context decoder refinement, during which the model learns from short slice excerpts yet generalizes to scans exceeding 300 slices without additional memory overhead. BTB3D sets a new state-of-the-art on two key tasks: it improves BLEU scores and increases clinical F1 by 40% over CT2Rep, CT-CHAT, and Merlin for report generation; and it reduces FID by 75% and halves FVD compared to GenerateCT and MedSyn for text-to-CT synthesis, producing anatomically consistent 512*512*241 volumes. These results confirm that precise three-dimensional tokenization, rather than larger language backbones alone, is essential for scalable vision-language modeling in 3D medical imaging. The codebase is available at: this https URL</li>
<li><strong>摘要：</strong>大规模计算机断层扫描 (CT) 语料库以及配对的自由文本报告、更强大的架构和强大的预训练模型推动了 3D 医学成像视觉语言建模的最新进展。这使得自动报告生成和文本条件 3D 图像合成等应用成为可能。然而，当前的方法难以应对高分辨率、长序列的体积：对比预训练通常会产生与临床语言不一致的视觉编码器，并且切片标记化模糊了精细的解剖结构，降低了下游任务的诊断性能。我们引入了 BTB3D（Better Tokens for Better 3D），这是一种因果卷积编码器-解码器，它统一了 2D 和 3D 训练和推理，同时生成紧凑的、频率感知的体积标记。三阶段训练课程可实现 (i) 局部重建、(ii) 重叠窗口平铺和 (iii) 长上下文解码器细化，在此期间，模型从短切片摘录中学习，但可推广到超过 300 个切片的扫描，而无需额外的内存开销。 BTB3D 在两项关键任务上树立了新的最先进水平：与 CT2Rep、CT-CHAT 和 Merlin 相比，它提高了 BLEU 分数并将临床 F1 在报告生成方面提高了 40%；与用于文本到 CT 合成的 GenerateCT 和 MedSyn 相比，它可将 FID 减少 75%，并将 FVD 减半，从而生成解剖学上一致的 512*512*241 体积。这些结果证实，精确的三维标记化（而不仅仅是更大的语言主干）对于 3D 医学成像中的可扩展视觉语言建模至关重要。代码库位于：此 https URL</li>
</ul>

<h3>Title: UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset</h3>
<ul>
<li><strong>Authors: </strong>Chen Zhao, En Ci, Yunzhe Xu, Tiehan Fan, Shanyan Guan, Yanhao Ge, Jian Yang, Ying Tai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20661">https://arxiv.org/abs/2510.20661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20661">https://arxiv.org/pdf/2510.20661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20661]] UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset(https://arxiv.org/abs/2510.20661)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Ultra-high-resolution (UHR) text-to-image (T2I) generation has seen notable progress. However, two key challenges remain : 1) the absence of a large-scale high-quality UHR T2I dataset, and (2) the neglect of tailored training strategies for fine-grained detail synthesis in UHR scenarios. To tackle the first challenge, we introduce \textbf{UltraHR-100K}, a high-quality dataset of 100K UHR images with rich captions, offering diverse content and strong visual fidelity. Each image exceeds 3K resolution and is rigorously curated based on detail richness, content complexity, and aesthetic quality. To tackle the second challenge, we propose a frequency-aware post-training method that enhances fine-detail generation in T2I diffusion models. Specifically, we design (i) \textit{Detail-Oriented Timestep Sampling (DOTS)} to focus learning on detail-critical denoising steps, and (ii) \textit{Soft-Weighting Frequency Regularization (SWFR)}, which leverages Discrete Fourier Transform (DFT) to softly constrain frequency components, encouraging high-frequency detail preservation. Extensive experiments on our proposed UltraHR-eval4K benchmarks demonstrate that our approach significantly improves the fine-grained detail quality and overall fidelity of UHR image generation. The code is available at \href{this https URL}{here}.</li>
<li><strong>摘要：</strong>超高分辨率 (UHR) 文本到图像 (T2I) 生成已取得显着进展。然而，仍然存在两个关键挑战：1）缺乏大规模高质量的 UHR T2I 数据集，以及（2）忽视了 UHR 场景中细粒度细节合成的定制训练策略。为了解决第一个挑战，我们引入了 \textbf{UltraHR-100K}，这是一个包含 100K UHR 图像的高质量数据集，具有丰富的标题，提供多样化的内容和强大的视觉保真度。每张图像的分辨率都超过 3K，并根据细节丰富度、内容复杂性和审美质量进行严格策划。为了解决第二个挑战，我们提出了一种频率感知的后训练方法，可以增强 T2I 扩散模型中的精细细节生成。具体来说，我们设计了（i）\textit{面向细节的时间步采样（DOTS）}来重点学习细节关键的去噪步骤，以及（ii）\textit{软加权频率正则化（SWFR）}，它利用离散傅立叶变换（DFT）来软约束频率分量，鼓励高频细节保留。对我们提出的 UltraHR-eval4K 基准进行的大量实验表明，我们的方法显着提高了 UHR 图像生成的细粒度细节质量和整体保真度。该代码位于 \href{此 https URL}{此处}。</li>
</ul>

<h3>Title: From Masks to Worlds: A Hitchhiker's Guide to World Models</h3>
<ul>
<li><strong>Authors: </strong>Jinbin Bai, Yu Lei, Hecong Wu, Yuchen Zhu, Shufan Li, Yi Xin, Xiangtai Li, Molei Tao, Aditya Grover, Ming-Hsuan Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20668">https://arxiv.org/abs/2510.20668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20668">https://arxiv.org/pdf/2510.20668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20668]] From Masks to Worlds: A Hitchhiker's Guide to World Models(https://arxiv.org/abs/2510.20668)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This is not a typical survey of world models; it is a guide for those who want to build worlds. We do not aim to catalog every paper that has ever mentioned a ``world model". Instead, we follow one clear road: from early masked models that unified representation learning across modalities, to unified architectures that share a single paradigm, then to interactive generative models that close the action-perception loop, and finally to memory-augmented systems that sustain consistent worlds over time. We bypass loosely related branches to focus on the core: the generative heart, the interactive loop, and the memory system. We show that this is the most promising path towards true world models.</li>
<li><strong>摘要：</strong>这不是对世界模式的典型调查；它是那些想要构建世界的人的指南。我们的目的并不是对每一篇提到“世界模型”的论文进行分类。相反，我们遵循一条清晰的道路：从早期统一跨模态表示学习的屏蔽模型，到共享单一范式的统一架构，然后到关闭动作感知循环的交互式生成模型，最后到随着时间的推移维持一致世界的记忆增强系统。我们绕过松散相关的分支，专注于核心：生成 心脏、交互循环和记忆系统。我们证明这是通往真实世界模型的最有希望的道路。</li>
</ul>

<h3>Title: ALICE-LRI: A General Method for Lossless Range Image Generation for Spinning LiDAR Sensors without Calibration Metadata</h3>
<ul>
<li><strong>Authors: </strong>Samuel Soutullo, Miguel Yermo, David L. Vilariño, Óscar G. Lorenzo, José C. Cabaleiro, Francisco F. Rivera</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20708">https://arxiv.org/abs/2510.20708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20708">https://arxiv.org/pdf/2510.20708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20708]] ALICE-LRI: A General Method for Lossless Range Image Generation for Spinning LiDAR Sensors without Calibration Metadata(https://arxiv.org/abs/2510.20708)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>3D LiDAR sensors are essential for autonomous navigation, environmental monitoring, and precision mapping in remote sensing applications. To efficiently process the massive point clouds generated by these sensors, LiDAR data is often projected into 2D range images that organize points by their angular positions and distances. While these range image representations enable efficient processing, conventional projection methods suffer from fundamental geometric inconsistencies that cause irreversible information loss, compromising high-fidelity applications. We present ALICE-LRI (Automatic LiDAR Intrinsic Calibration Estimation for Lossless Range Images), the first general, sensor-agnostic method that achieves lossless range image generation from spinning LiDAR point clouds without requiring manufacturer metadata or calibration files. Our algorithm automatically reverse-engineers the intrinsic geometry of any spinning LiDAR sensor by inferring critical parameters including laser beam configuration, angular distributions, and per-beam calibration corrections, enabling lossless projection and complete point cloud reconstruction with zero point loss. Comprehensive evaluation across the complete KITTI and DurLAR datasets demonstrates that ALICE-LRI achieves perfect point preservation, with zero points lost across all point clouds. Geometric accuracy is maintained well within sensor precision limits, establishing geometric losslessness with real-time performance. We also present a compression case study that validates substantial downstream benefits, demonstrating significant quality improvements in practical applications. This paradigm shift from approximate to lossless LiDAR projections opens new possibilities for high-precision remote sensing applications requiring complete geometric preservation.</li>
<li><strong>摘要：</strong>3D LiDAR 传感器对于遥感应用中的自主导航、环境监测和精密测绘至关重要。为了有效处理这些传感器生成的大量点云，LiDAR 数据通常被投影到 2D 范围图像中，这些图像按角度位置和距离组织点。虽然这些距离图像表示可以实现高效处理，但传统的投影方法存在基本的几何不一致问题，会导致不可逆的信息丢失，从而损害高保真应用。我们提出了 ALICE-LRI（无损距离图像的自动 LiDAR 固有校准估计），这是第一个与传感器无关的通用方法，可通过旋转 LiDAR 点云实现无损距离图像生成，而不需要制造商元数据或校准文件。我们的算法通过推断激光束配置、角度分布和每束校准校正等关键参数，自动对任何旋转 LiDAR 传感器的固有几何结构进行逆向工程，从而实现无损投影和零点损失的完整点云重建。对完整 KITTI 和 DurLAR 数据集的综合评估表明，ALICE-LRI 实现了完美的点保留，所有点云中的点丢失为零。几何精度很好地保持在传感器精度范围内，从而建立了具有实时性能的几何无损性。我们还提出了一个压缩案例研究，验证了下游的巨大优势，展示了实际应用中的显着质量改进。这种从近似激光雷达投影到无损激光雷达投影的范式转变为需要完整几何保留的高精度遥感应用开辟了新的可能性。</li>
</ul>

<h3>Title: Separating the what and how of compositional computation to enable reuse and continual learning</h3>
<ul>
<li><strong>Authors: </strong>Haozhe Shan, Sun Minni, Lea Duncker</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20709">https://arxiv.org/abs/2510.20709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20709">https://arxiv.org/pdf/2510.20709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20709]] Separating the what and how of compositional computation to enable reuse and continual learning(https://arxiv.org/abs/2510.20709)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The ability to continually learn, retain and deploy skills to accomplish goals is a key feature of intelligent and efficient behavior. However, the neural mechanisms facilitating the continual learning and flexible (re-)composition of skills remain elusive. Here, we study continual learning and the compositional reuse of learned computations in recurrent neural network (RNN) models using a novel two-system approach: one system that infers what computation to perform, and one that implements how to perform it. We focus on a set of compositional cognitive tasks commonly studied in neuroscience. To construct the what system, we first show that a large family of tasks can be systematically described by a probabilistic generative model, where compositionality stems from a shared underlying vocabulary of discrete task epochs. The shared epoch structure makes these tasks inherently compositional. We first show that this compositionality can be systematically described by a probabilistic generative model. Furthermore, We develop an unsupervised online learning approach that can learn this model on a single-trial basis, building its vocabulary incrementally as it is exposed to new tasks, and inferring the latent epoch structure as a time-varying computational context within a trial. We implement the how system as an RNN whose low-rank components are composed according to the context inferred by the what system. Contextual inference facilitates the creation, learning, and reuse of low-rank RNN components as new tasks are introduced sequentially, enabling continual learning without catastrophic forgetting. Using an example task set, we demonstrate the efficacy and competitive performance of this two-system learning framework, its potential for forward and backward transfer, as well as fast compositional generalization to unseen tasks.</li>
<li><strong>摘要：</strong>不断学习、保留和运用技能来实现目标的能力是智能和高效行为的关键特征。然而，促进持续学习和灵活（重新）组合技能的神经机制仍然难以捉摸。在这里，我们使用一种新颖的双系统方法研究循环神经网络（RNN）模型中的持续学习和学习计算的组合重用：一个系统推断要执行什么计算，另一个系统实现如何执行计算。我们专注于神经科学中常见的一组组合认知任务。为了构建“什么”系统，我们首先证明可以通过概率生成模型系统地描述一大堆任务，其中组合性源于离散任务时期的共享底层词汇。共享的纪元结构使得这些任务本质上是组合的。我们首先证明这种组合性可以通过概率生成模型系统地描述。此外，我们开发了一种无监督的在线学习方法，可以在单次试验的基础上学习该模型，在接触新任务时逐步构建其词汇表，并将潜在的纪元结构推断为试验中随时间变化的计算上下文。我们将 how 系统实现为一个 RNN，其低秩组件是根据 What 系统推断的上下文组成的。当新任务按顺序引入时，上下文推理有助于低秩 RNN 组件的创建、学习和重用，从而实现持续学习，而不会发生灾难性遗忘。使用示例任务集，我们展示了这个双系统学习框架的功效和竞争性能、其前向和后向迁移的潜力，以及对未见过的任务的快速组合泛化。</li>
</ul>

<h3>Title: AutoScape: Geometry-Consistent Long-Horizon Scene Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Chen, Ziyu Jiang, Mingfu Liang, Bingbing Zhuang, Jong-Chyi Su, Sparsh Garg, Ying Wu, Manmohan Chandraker</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20726">https://arxiv.org/abs/2510.20726</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20726">https://arxiv.org/pdf/2510.20726</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20726]] AutoScape: Geometry-Consistent Long-Horizon Scene Generation(https://arxiv.org/abs/2510.20726)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This paper proposes AutoScape, a long-horizon driving scene generation framework. At its core is a novel RGB-D diffusion model that iteratively generates sparse, geometrically consistent keyframes, serving as reliable anchors for the scene's appearance and geometry. To maintain long-range geometric consistency, the model 1) jointly handles image and depth in a shared latent space, 2) explicitly conditions on the existing scene geometry (i.e., rendered point clouds) from previously generated keyframes, and 3) steers the sampling process with a warp-consistent guidance. Given high-quality RGB-D keyframes, a video diffusion model then interpolates between them to produce dense and coherent video frames. AutoScape generates realistic and geometrically consistent driving videos of over 20 seconds, improving the long-horizon FID and FVD scores over the prior state-of-the-art by 48.6\% and 43.0\%, respectively.</li>
<li><strong>摘要：</strong>本文提出了 AutoScape，一种长视野驾驶场景生成框架。其核心是一种新颖的 RGB-D 扩散模型，可迭代生成稀疏、几何一致的关键帧，作为场景外观和几何形状的可靠锚点。为了保持远程几何一致性，该模型 1) 联合处理共享潜在空间中的图像和深度，2) 根据先前生成的关键帧对现有场景几何图形（即渲染的点云）进行明确条件，3) 使用扭曲一致的指导来引导采样过程。给定高质量的 RGB-D 关键帧，视频扩散模型会在它们之间进行插值，以生成密集且连贯的视频帧。 AutoScape 生成超过 20 秒的真实且几何一致的驾驶视频，与之前最先进的技术相比，长视距 FID 和 FVD 分数分别提高了 48.6% 和 43.0%。</li>
</ul>

<h3>Title: Thought Communication in Multiagent Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Yujia Zheng, Zhuokai Zhao, Zijian Li, Yaqi Xie, Mingze Gao, Lizhu Zhang, Kun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20733">https://arxiv.org/abs/2510.20733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20733">https://arxiv.org/pdf/2510.20733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20733]] Thought Communication in Multiagent Collaboration(https://arxiv.org/abs/2510.20733)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Natural language has long enabled human cooperation, but its lossy, ambiguous, and indirect nature limits the potential of collective intelligence. While machines are not subject to these constraints, most LLM-based multi-agent systems still rely solely on natural language, exchanging tokens or their embeddings. To go beyond language, we introduce a new paradigm, thought communication, which enables agents to interact directly mind-to-mind, akin to telepathy. To uncover these latent thoughts in a principled way, we formalize the process as a general latent variable model, where agent states are generated by an unknown function of underlying thoughts. We prove that, in a nonparametric setting without auxiliary information, both shared and private latent thoughts between any pair of agents can be identified. Moreover, the global structure of thought sharing, including which agents share which thoughts and how these relationships are structured, can also be recovered with theoretical guarantees. Guided by the established theory, we develop a framework that extracts latent thoughts from all agents prior to communication and assigns each agent the relevant thoughts, along with their sharing patterns. This paradigm naturally extends beyond LLMs to all modalities, as most observational data arise from hidden generative processes. Experiments on both synthetic and real-world benchmarks validate the theory and demonstrate the collaborative advantages of thought communication. We hope this work illuminates the potential of leveraging the hidden world, as many challenges remain unsolvable through surface-level observation alone, regardless of compute or data scale.</li>
<li><strong>摘要：</strong>自然语言长期以来一直使人类合作成为可能，但其有损、模糊和间接的性质限制了集体智慧的潜力。虽然机器不受这些限制，但大多数基于 LLM 的多智能体系统仍然仅依赖于自然语言、交换令牌或其嵌入。为了超越语言，我们引入了一种新的范式，即思想交流，它使智能体能够直接进行心灵互动，类似于心灵感应。为了以原则性的方式揭示这些潜在的想法，我们将这个过程形式化为一般的潜在变量模型，其中代理状态是由潜在想法的未知函数生成的。我们证明，在没有辅助信息的非参数设置中，可以识别任何一对代理之间的共享和私人潜在想法。此外，思想共享的全局结构，包括哪些主体共享哪些思想以及这些关系如何构建，也可以通过理论保证来恢复。在既定理论的指导下，我们开发了一个框架，在通信之前从所有代理中提取潜在的想法，并为每个代理分配相关的想法及其共享模式。这种范式自然地扩展到法学硕士之外的所有模式，因为大多数观察数据都来自隐藏的生成过程。综合基准​​和现实基准的实验验证了该理论并证明了思想交流的协作优势。我们希望这项工作能够阐明利用隐藏世界的潜力，因为无论计算或数据规模如何，仅通过表面观察仍然无法解决许多挑战。</li>
</ul>

<h3>Title: DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Noam Issachar, Guy Yariv, Sagie Benaim, Yossi Adi, Dani Lischinski, Raanan Fattal</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20766">https://arxiv.org/abs/2510.20766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20766">https://arxiv.org/pdf/2510.20766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20766]] DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion(https://arxiv.org/abs/2510.20766)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion Transformer models can generate images with remarkable fidelity and detail, yet training them at ultra-high resolutions remains extremely costly due to the self-attention mechanism's quadratic scaling with the number of image tokens. In this paper, we introduce Dynamic Position Extrapolation (DyPE), a novel, training-free method that enables pre-trained diffusion transformers to synthesize images at resolutions far beyond their training data, with no additional sampling cost. DyPE takes advantage of the spectral progression inherent to the diffusion process, where low-frequency structures converge early, while high-frequencies take more steps to resolve. Specifically, DyPE dynamically adjusts the model's positional encoding at each diffusion step, matching their frequency spectrum with the current stage of the generative process. This approach allows us to generate images at resolutions that exceed the training resolution dramatically, e.g., 16 million pixels using FLUX. On multiple benchmarks, DyPE consistently improves performance and achieves state-of-the-art fidelity in ultra-high-resolution image generation, with gains becoming even more pronounced at higher resolutions. Project page is available at this https URL.</li>
<li><strong>摘要：</strong>Diffusion Transformer 模型可以生成具有出色保真度和细节的图像，但由于自注意力机制与图像标记数量的二次缩放，以超高分辨率训练它们仍然非常昂贵。在本文中，我们介绍了动态位置外推法（DyPE），这是一种新颖的免训练方法，使预先训练的扩散变换器能够以远远超出其训练数据的分辨率合成图像，而无需额外的采样成本。 DyPE 利用了扩散过程固有的频谱级数，其中低频结构较早收敛，而高频则需要更多步骤来解析。具体来说，DyPE 在每个扩散步骤动态调整模型的位置编码，将其频谱与生成过程的当前阶段相匹配。这种方法使我们能够生成分辨率大大超过训练分辨率的图像，例如使用 FLUX 生成 1600 万像素。在多个基准测试中，DyPE 不断提高性能，并在超高分辨率图像生成中实现最先进的保真度，并且在更高分辨率下，增益变得更加明显。项目页面可通过此 https URL 获取。</li>
</ul>

<h3>Title: AlphaFlow: Understanding and Improving MeanFlow Models</h3>
<ul>
<li><strong>Authors: </strong>Huijie Zhang, Aliaksandr Siarohin, Willi Menapace, Michael Vasilkovsky, Sergey Tulyakov, Qing Qu, Ivan Skorokhodov</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20771">https://arxiv.org/abs/2510.20771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20771">https://arxiv.org/pdf/2510.20771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20771]] AlphaFlow: Understanding and Improving MeanFlow Models(https://arxiv.org/abs/2510.20771)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>MeanFlow has recently emerged as a powerful framework for few-step generative modeling trained from scratch, but its success is not yet fully understood. In this work, we show that the MeanFlow objective naturally decomposes into two parts: trajectory flow matching and trajectory consistency. Through gradient analysis, we find that these terms are strongly negatively correlated, causing optimization conflict and slow convergence. Motivated by these insights, we introduce $\alpha$-Flow, a broad family of objectives that unifies trajectory flow matching, Shortcut Model, and MeanFlow under one formulation. By adopting a curriculum strategy that smoothly anneals from trajectory flow matching to MeanFlow, $\alpha$-Flow disentangles the conflicting objectives, and achieves better convergence. When trained from scratch on class-conditional ImageNet-1K 256x256 with vanilla DiT backbones, $\alpha$-Flow consistently outperforms MeanFlow across scales and settings. Our largest $\alpha$-Flow-XL/2+ model achieves new state-of-the-art results using vanilla DiT backbones, with FID scores of 2.58 (1-NFE) and 2.15 (2-NFE).</li>
<li><strong>摘要：</strong>MeanFlow 最近已经成为一个强大的框架，用于从头开始训练的几个步骤生成建模，但它的成功尚未得到充分理解。在这项工作中，我们表明 MeanFlow 目标自然地分解为两部分：轨迹流匹配和轨迹一致性。通过梯度分析，我们发现这些项是强负相关的，导致优化冲突和收敛缓慢。受这些见解的启发，我们引入了 $\alpha$-Flow，这是一个广泛的目标系列，它将轨迹流匹配、快捷模型和 MeanFlow 统一在一个公式下。通过采用从轨迹流匹配平滑退火到 MeanFlow 的课程策略，$\alpha$-Flow 消除了相互冲突的目标，并实现了更好的收敛。当在具有普通 DiT 主干的类条件 ImageNet-1K 256x256 上从头开始训练时，$\alpha$-Flow 在各个尺度和设置上始终优于 MeanFlow。我们最大的 $\alpha$-Flow-XL/2+ 模型使用普通 DiT 主干网实现了新的最先进结果，FID 分数为 2.58 (1-NFE) 和 2.15 (2-NFE)。</li>
</ul>

<h3>Title: CUPID: Pose-Grounded Generative 3D Reconstruction from a Single Image</h3>
<ul>
<li><strong>Authors: </strong>Binbin Huang, Haobin Duan, Yiqun Zhao, Zibo Zhao, Yi Ma, Shenghua Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20776">https://arxiv.org/abs/2510.20776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20776">https://arxiv.org/pdf/2510.20776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20776]] CUPID: Pose-Grounded Generative 3D Reconstruction from a Single Image(https://arxiv.org/abs/2510.20776)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>This work proposes a new generation-based 3D reconstruction method, named Cupid, that accurately infers the camera pose, 3D shape, and texture of an object from a single 2D image. Cupid casts 3D reconstruction as a conditional sampling process from a learned distribution of 3D objects, and it jointly generates voxels and pixel-voxel correspondences, enabling robust pose and shape estimation under a unified generative framework. By representing both input camera poses and 3D shape as a distribution in a shared 3D latent space, Cupid adopts a two-stage flow matching pipeline: (1) a coarse stage that produces initial 3D geometry with associated 2D projections for pose recovery; and (2) a refinement stage that integrates pose-aligned image features to enhance structural fidelity and appearance details. Extensive experiments demonstrate Cupid outperforms leading 3D reconstruction methods with an over 3 dB PSNR gain and an over 10% Chamfer Distance reduction, while matching monocular estimators on pose accuracy and delivering superior visual fidelity over baseline 3D generative models. For an immersive view of the 3D results generated by Cupid, please visit this http URL.</li>
<li><strong>摘要：</strong>这项工作提出了一种名为 Cupid 的新一代 3D 重建方法，该方法可以从单个 2D 图像中准确推断出相机姿态、3D 形状和物体的纹理。 Cupid 将 3D 重建作为从学习的 3D 对象分布中进行条件采样的过程，并联合生成体素和像素-体素对应关系，从而在统一的生成框架下实现稳健的姿态和形状估计。通过将输入相机姿势和 3D 形状表示为共享 3D 潜在空间中的分布，Cupid 采用两阶段流匹配管道：(1) 粗略阶段，生成初始 3D 几何图形以及用于姿势恢复的相关 2D 投影； (2) 细化阶段，集成姿势对齐图像特征以增强结构保真度和外观细节。大量实验表明，Cupid 的性能优于领先的 3D 重建方法，PSNR 增益超过 3 dB，倒角距离减少超过 10%，同时在姿势精度方面与单目估计器相匹配，并提供优于基线 3D 生成模型的视觉保真度。要获得 Cupid 生成的 3D 结果的沉浸式视图，请访问此 http URL。</li>
</ul>

<h3>Title: BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation</h3>
<ul>
<li><strong>Authors: </strong>Liang Ye, Shengqin Chen, Jiazhu Dai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20792">https://arxiv.org/abs/2510.20792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20792">https://arxiv.org/pdf/2510.20792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20792]] BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation(https://arxiv.org/abs/2510.20792)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The rapid progress of graph generation has raised new security concerns, particularly regarding backdoor vulnerabilities. While prior work has explored backdoor attacks in image diffusion and unconditional graph generation, conditional, especially text-guided graph generation remains largely unexamined. This paper proposes BadGraph, a backdoor attack method targeting latent diffusion models for text-guided graph generation. BadGraph leverages textual triggers to poison training data, covertly implanting backdoors that induce attacker-specified subgraphs during inference when triggers appear, while preserving normal performance on clean inputs. Extensive experiments on four benchmark datasets (PubChem, ChEBI-20, PCDes, MoMu) demonstrate the effectiveness and stealth of the attack: less than 10% poisoning rate can achieves 50% attack success rate, while 24% suffices for over 80% success rate, with negligible performance degradation on benign samples. Ablation studies further reveal that the backdoor is implanted during VAE and diffusion training rather than pretraining. These findings reveal the security vulnerabilities in latent diffusion models of text-guided graph generation, highlight the serious risks in models' applications such as drug discovery and underscore the need for robust defenses against the backdoor attack in such diffusion models.</li>
<li><strong>摘要：</strong>图生成的快速进展引发了新的安全问题，特别是关于后门漏洞。虽然之前的工作已经探索了图像扩散和无条件图生成中的后门攻击，但条件图生成，尤其是文本引导图生成仍然很大程度上未经检查。本文提出了 BadGraph，一种针对文本引导图生成的潜在扩散模型的后门攻击方法。 BadGraph 利用文本触发器来毒害训练数据，秘密植入后门，在触发器出现时在推理过程中引发攻击者指定的子图，同时保持干净输入的正常性能。在四个基准数据集（PubChem、ChEBI-20、PCDes、MoMu）上进行的大量实验证明了该攻击的有效性和隐蔽性：低于 10% 的中毒率可以实现 50% 的攻击成功率，而 24% 的中毒率足以达到 80% 以上的成功率，而良性样本的性能下降可以忽略不计。消融研究进一步表明，后门是在 VAE 和扩散训练期间而不是预训练期间植入的。这些发现揭示了文本引导图生成的潜在扩散模型中的安全漏洞，突出了药物发现等模型应用中的严重风险，并强调了在此类扩散模型中针对后门攻击进行强有力的防御的必要性。</li>
</ul>

<h3>Title: ARGenSeg: Image Segmentation with Autoregressive Image Generation Model</h3>
<ul>
<li><strong>Authors: </strong>Xiaolong Wang, Lixiang Ru, Ziyuan Huang, Kaixiang Ji, Dandan Zheng, Jingdong Chen, Jun Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20803">https://arxiv.org/abs/2510.20803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20803">https://arxiv.org/pdf/2510.20803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20803]] ARGenSeg: Image Segmentation with Autoregressive Image Generation Model(https://arxiv.org/abs/2510.20803)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We propose a novel AutoRegressive Generation-based paradigm for image Segmentation (ARGenSeg), achieving multimodal understanding and pixel-level perception within a unified framework. Prior works integrating image segmentation into multimodal large language models (MLLMs) typically employ either boundary points representation or dedicated segmentation heads. These methods rely on discrete representations or semantic prompts fed into task-specific decoders, which limits the ability of the MLLM to capture fine-grained visual details. To address these challenges, we introduce a segmentation framework for MLLM based on image generation, which naturally produces dense masks for target objects. We leverage MLLM to output visual tokens and detokenize them into images using an universal VQ-VAE, making the segmentation fully dependent on the pixel-level understanding of the MLLM. To reduce inference latency, we employ a next-scale-prediction strategy to generate required visual tokens in parallel. Extensive experiments demonstrate that our method surpasses prior state-of-the-art approaches on multiple segmentation datasets with a remarkable boost in inference speed, while maintaining strong understanding capabilities.</li>
<li><strong>摘要：</strong>我们提出了一种新颖的基于自回归生成的图像分割范例（ARGenSeg），在统一的框架内实现多模态理解和像素级感知。先前将图像分割集成到多模态大语言模型（MLLM）中的工作通常采用边界点表示或专用分割头。这些方法依赖于输入特定于任务的解码器的离散表示或语义提示，这限制了 MLLM 捕获细粒度视觉细节的能力。为了解决这些挑战，我们引入了基于图像生成的 MLLM 分割框架，它自然地为目标对象生成密集的掩模。我们利用 MLLM 输出视觉标记，并使用通用 VQ-VAE 将它们去标记为图像，从而使分割完全依赖于对 MLLM 的像素级理解。为了减少推理延迟，我们采用下一个规模预测策略来并行生成所需的视觉标记。大量的实验表明，我们的方法在多个分割数据集上超越了先前最先进的方法，显着提高了推理速度，同时保持了强大的理解能力。</li>
</ul>

<h3>Title: Video Prediction of Dynamic Physical Simulations With Pixel-Space Spatiotemporal Transformers</h3>
<ul>
<li><strong>Authors: </strong>Dean L Slack, G Thomas Hudson, Thomas Winterbottom, Noura Al Moubayed</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20807">https://arxiv.org/abs/2510.20807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20807">https://arxiv.org/pdf/2510.20807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20807]] Video Prediction of Dynamic Physical Simulations With Pixel-Space Spatiotemporal Transformers(https://arxiv.org/abs/2510.20807)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Inspired by the performance and scalability of autoregressive large language models (LLMs), transformer-based models have seen recent success in the visual domain. This study investigates a transformer adaptation for video prediction with a simple end-to-end approach, comparing various spatiotemporal self-attention layouts. Focusing on causal modeling of physical simulations over time; a common shortcoming of existing video-generative approaches, we attempt to isolate spatiotemporal reasoning via physical object tracking metrics and unsupervised training on physical simulation datasets. We introduce a simple yet effective pure transformer model for autoregressive video prediction, utilizing continuous pixel-space representations for video prediction. Without the need for complex training strategies or latent feature-learning components, our approach significantly extends the time horizon for physically accurate predictions by up to 50% when compared with existing latent-space approaches, while maintaining comparable performance on common video quality metrics. In addition, we conduct interpretability experiments to identify network regions that encode information useful to perform accurate estimations of PDE simulation parameters via probing models, and find that this generalizes to the estimation of out-of-distribution simulation parameters. This work serves as a platform for further attention-based spatiotemporal modeling of videos via a simple, parameter efficient, and interpretable approach.</li>
<li><strong>摘要：</strong>受到自回归大语言模型 (LLM) 的性能和可扩展性的启发，基于 Transformer 的模型最近在视觉领域取得了成功。本研究通过简单的端到端方法研究了视频预测的变压器适应，比较了各种时空自注意力布局。专注于随时间推移的物理模拟的因果建模；现有视频生成方法的一个常见缺点是，我们尝试通过物理对象跟踪指标和对物理模拟数据集的无监督训练来隔离时空推理。我们引入了一种简单而有效的纯 Transformer 模型，用于自回归视频预测，利用连续像素空间表示进行视频预测。与现有潜在空间方法相比，我们的方法无需复杂的训练策略或潜在特征学习组件，即可将物理准确预测的时间范围显着延长高达 50%，同时在常见视频质量指标上保持可比较的性能。此外，我们进行了可解释性实验，以识别编码信息的网络区域，这些信息有助于通过探测模型执行偏微分方程模拟参数的精确估计，并发现这可以推广到分布外模拟参数的估计。这项工作通过简单、参数高效且可解释的方法作为进一步基于注意力的视频时空建模的平台。</li>
</ul>

<h3>Title: SpectraMorph: Structured Latent Learning for Self-Supervised Hyperspectral Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Ritik Shah, Marco F Duarte</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20814">https://arxiv.org/abs/2510.20814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20814">https://arxiv.org/pdf/2510.20814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20814]] SpectraMorph: Structured Latent Learning for Self-Supervised Hyperspectral Super-Resolution(https://arxiv.org/abs/2510.20814)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Hyperspectral sensors capture dense spectra per pixel but suffer from low spatial resolution, causing blurred boundaries and mixed-pixel effects. Co-registered companion sensors such as multispectral, RGB, or panchromatic cameras provide high-resolution spatial detail, motivating hyperspectral super-resolution through the fusion of hyperspectral and multispectral images (HSI-MSI). Existing deep learning based methods achieve strong performance but rely on opaque regressors that lack interpretability and often fail when the MSI has very few bands. We propose SpectraMorph, a physics-guided self-supervised fusion framework with a structured latent space. Instead of direct regression, SpectraMorph enforces an unmixing bottleneck: endmember signatures are extracted from the low-resolution HSI, and a compact multilayer perceptron predicts abundance-like maps from the MSI. Spectra are reconstructed by linear mixing, with training performed in a self-supervised manner via the MSI sensor's spectral response function. SpectraMorph produces interpretable intermediates, trains in under a minute, and remains robust even with a single-band (pan-chromatic) MSI. Experiments on synthetic and real-world datasets show SpectraMorph consistently outperforming state-of-the-art unsupervised/self-supervised baselines while remaining very competitive against supervised baselines.</li>
<li><strong>摘要：</strong>高光谱传感器捕获每个像素的密集光谱，但空间分辨率较低，导致边界模糊和混合像素效应。多光谱、RGB 或全色相机等共同配准的配套传感器可提供高分辨率空间细节，通过高光谱和多光谱图像 (HSI-MSI) 的融合来激发高光谱超分辨率。现有的基于深度学习的方法实现了强大的性能，但依赖于缺乏可解释性的不透明回归器，并且当 MSI 的频带非常少时通常会失败。我们提出了 SpectraMorph，一种具有结构化潜在空间的物理引导的自监督融合框架。 SpectraMorph 不是直接回归，而是强制执行分解瓶颈：从低分辨率 HSI 中提取端元签名，并且紧凑的多层感知器从 MSI 预测类似丰度的图。光谱通过线性混合重建，并通过 MSI 传感器的光谱响应函数以自我监督的方式进行训练。 SpectraMorph 生成可解释的中间体，在一分钟内完成训练，即使使用单波段（全色）MSI 也能保持稳健。对合成数据集和真实世界数据集的实验表明，SpectraMorph 始终优于最先进的无监督/自监督基线，同时与监督基线相比仍然非常有竞争力。</li>
</ul>

<h3>Title: Towards General Modality Translation with Contrastive and Predictive Latent Diffusion Bridge</h3>
<ul>
<li><strong>Authors: </strong>Nimrod Berman, Omkar Joglekar, Eitan Kosman, Dotan Di Castro, Omri Azencot</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20819">https://arxiv.org/abs/2510.20819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20819">https://arxiv.org/pdf/2510.20819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20819]] Towards General Modality Translation with Contrastive and Predictive Latent Diffusion Bridge(https://arxiv.org/abs/2510.20819)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative modeling have positioned diffusion models as state-of-the-art tools for sampling from complex data distributions. While these models have shown remarkable success across single-modality domains such as images and audio, extending their capabilities to Modality Translation (MT), translating information across different sensory modalities, remains an open challenge. Existing approaches often rely on restrictive assumptions, including shared dimensionality, Gaussian source priors, and modality-specific architectures, which limit their generality and theoretical grounding. In this work, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), a general-purpose framework for modality translation based on a latent-variable extension of Denoising Diffusion Bridge Models. By operating in a shared latent space, our method learns a bridge between arbitrary modalities without requiring aligned dimensions. We introduce a contrastive alignment loss to enforce semantic consistency between paired samples and design a domain-agnostic encoder-decoder architecture tailored for noise prediction in latent space. Additionally, we propose a predictive loss to guide training toward accurate cross-domain translation and explore several training strategies to improve stability. Our approach supports arbitrary modality pairs and performs strongly on diverse MT tasks, including multi-view to 3D shape generation, image super-resolution, and multi-view scene synthesis. Comprehensive experiments and ablations validate the effectiveness of our framework, establishing a new strong baseline in general modality translation. For more information, see our project page: this https URL.</li>
<li><strong>摘要：</strong>生成建模的最新进展已将扩散模型定位为从复杂数据分布中进行采样的最先进工具。虽然这些模型在图像和音频等单一模态领域取得了显着的成功，但将其功能扩展到模态翻译（MT），即跨不同感官模态翻译信息，仍然是一个开放的挑战。现有的方法通常依赖于限制性假设，包括共享维度、高斯源先验和特定模态的架构，这限制了它们的通用性和理论基础。在这项工作中，我们提出了潜在去噪扩散桥模型（LDDBM），这是一种基于去噪扩散桥模型的潜在变量扩展的模态转换通用框架。通过在共享的潜在空间中操作，我们的方法无需对齐维度即可学习任意模态之间的桥梁。我们引入了对比对齐损失来强制配对样本之间的语义一致性，并设计了一个专为潜在空间中的噪声预测而设计的与域无关的编码器-解码器架构。此外，我们提出了预测损失来指导训练进行准确的跨域翻译，并探索几种训练策略来提高稳定性。我们的方法支持任意模态对，并且在各种 MT 任务上表现出色，包括多视图到 3D 形状生成、图像超分辨率和多视图场景合成。全面的实验和消融验证了我们框架的有效性，为一般模态翻译建立了新的强大基线。有关更多信息，请参阅我们的项目页面：此 https URL。</li>
</ul>

<h3>Title: LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered Canvas</h3>
<ul>
<li><strong>Authors: </strong>Guocheng Gordon Qian, Ruihang Zhang, Tsai-Shien Chen, Yusuf Dalva, Anujraaj Argo Goyal, Willi Menapace, Ivan Skorokhodov, Meng Dong, Arpit Sahni, Daniil Ostashev, Ju Hu, Sergey Tulyakov, Kuan-Chieh Jackson Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20820">https://arxiv.org/abs/2510.20820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20820">https://arxiv.org/pdf/2510.20820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20820]] LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered Canvas(https://arxiv.org/abs/2510.20820)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Despite their impressive visual fidelity, existing personalized generative models lack interactive control over spatial composition and scale poorly to multiple subjects. To address these limitations, we present LayerComposer, an interactive framework for personalized, multi-subject text-to-image generation. Our approach introduces two main contributions: (1) a layered canvas, a novel representation in which each subject is placed on a distinct layer, enabling occlusion-free composition; and (2) a locking mechanism that preserves selected layers with high fidelity while allowing the remaining layers to adapt flexibly to the surrounding context. Similar to professional image-editing software, the proposed layered canvas allows users to place, resize, or lock input subjects through intuitive layer manipulation. Our versatile locking mechanism requires no architectural changes, relying instead on inherent positional embeddings combined with a new complementary data sampling strategy. Extensive experiments demonstrate that LayerComposer achieves superior spatial control and identity preservation compared to the state-of-the-art methods in multi-subject personalized image generation.</li>
<li><strong>摘要：</strong>尽管视觉保真度令人印象深刻，但现有的个性化生成模型缺乏对空间构成的交互控制，并且很难扩展到多个主题。为了解决这些限制，我们提出了 LayerComposer，这是一个用于个性化、多主题文本到图像生成的交互式框架。我们的方法引入了两个主要贡献：（1）分层画布，一种新颖的表示形式，其中每个主题都放置在不同的层上，从而实现无遮挡构图； （2）锁定机制，以高保真度保留选定的层，同时允许其余层灵活地适应周围的环境。与专业图像编辑软件类似，所提出的分层画布允许用户通过直观的图层操作来放置、调整大小或锁定输入主题。我们的多功能锁定机制不需要进行架构更改，而是依赖于固有的位置嵌入与新的互补数据采样策略相结合。大量实验表明，与多主体个性化图像生成中最先进的方法相比，LayerComposer 实现了卓越的空间控制和身份保存。</li>
</ul>

<h3>Title: HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives</h3>
<ul>
<li><strong>Authors: </strong>Yihao Meng, Hao Ouyang, Yue Yu, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Hanlin Wang, Yixuan Li, Cheng Chen, Yanhong Zeng, Yujun Shen, Huamin Qu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.20822">https://arxiv.org/abs/2510.20822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.20822">https://arxiv.org/pdf/2510.20822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.20822]] HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives(https://arxiv.org/abs/2510.20822)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>State-of-the-art text-to-video models excel at generating isolated clips but fall short of creating the coherent, multi-shot narratives, which are the essence of storytelling. We bridge this "narrative gap" with HoloCine, a model that generates entire scenes holistically to ensure global consistency from the first shot to the last. Our architecture achieves precise directorial control through a Window Cross-Attention mechanism that localizes text prompts to specific shots, while a Sparse Inter-Shot Self-Attention pattern (dense within shots but sparse between them) ensures the efficiency required for minute-scale generation. Beyond setting a new state-of-the-art in narrative coherence, HoloCine develops remarkable emergent abilities: a persistent memory for characters and scenes, and an intuitive grasp of cinematic techniques. Our work marks a pivotal shift from clip synthesis towards automated filmmaking, making end-to-end cinematic creation a tangible future. Our code is available at: this https URL.</li>
<li><strong>摘要：</strong>最先进的文本到视频模型擅长生成孤立的剪辑，但无法创建连贯的多镜头叙事，而这正是讲故事的本质。我们通过 HoloCine 弥补了这一“叙事差距”，HoloCine 是一种整体生成整个场景的模型，以确保从第一个镜头到最后一个镜头的全局一致性。我们的架构通过窗口交叉注意机制实现精确的方向控制，将文本提示本地化到特定镜头，而稀疏镜头间自注意模式（镜头内密集但镜头之间稀疏）确保了分钟级生成所需的效率。除了在叙事连贯性方面创造了新的最先进水平之外，HoloCine 还开发了非凡的新兴能力：对角色和场景的持久记忆，以及对电影技术的直观掌握。我们的工作标志着从剪辑合成到自动化电影制作的关键转变，使端到端的电影创作成为一个切实的未来。我们的代码位于：此 https URL。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
