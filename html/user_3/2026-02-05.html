<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-02-05</h1>
<h3>Title: Reversible Deep Learning for 13C NMR in Chemoinformatics: On Structures and Spectra</h3>
<ul>
<li><strong>Authors: </strong>Stefan Kuhn, Vandana Dwarka, Przemyslaw Karol Grenda, Eero Vainikko</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03875">https://arxiv.org/abs/2602.03875</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03875">https://arxiv.org/pdf/2602.03875</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03875]] Reversible Deep Learning for 13C NMR in Chemoinformatics: On Structures and Spectra(https://arxiv.org/abs/2602.03875)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce a reversible deep learning model for 13C NMR that uses a single conditional invertible neural network for both directions between molecular structures and spectra. The network is built from i-RevNet style bijective blocks, so the forward map and its inverse are available by construction. We train the model to predict a 128-bit binned spectrum code from a graph-based structure encoding, while the remaining latent dimensions capture residual variability. At inference time, we invert the same trained network to generate structure candidates from a spectrum code, which explicitly represents the one-to-many nature of spectrum-to-structure inference. On a filtered subset, the model is numerically invertible on trained examples, achieves spectrum-code prediction above chance, and produces coarse but meaningful structural signals when inverted on validation spectra. These results demonstrate that invertible architectures can unify spectrum prediction and uncertainty-aware candidate generation within one end-to-end model.</li>
<li><strong>摘要：</strong>我们引入了 13C NMR 的可逆深度学习模型，该模型对分子结构和光谱之间的两个方向使用单个条件可逆神经网络。该网络是由 i-RevNet 风格的双射块构建的，因此前向映射及其逆映射可通过构造获得。我们训练模型从基于图的结构编码中预测 128 位分箱频谱代码，而其余潜在维度则捕获残余变异性。在推理时，我们反转相同的训练网络以从频谱代码生成结构候选，这明确地表示了频谱到结构推理的一对多性质。在过滤子集上，该模型在训练样本上可进行数值可逆，实现高于概率的频谱代码预测，并在验证频谱上反转时产生粗略但有意义的结构信号。这些结果表明，可逆架构可以在一个端到端模型中统一频谱预测和不确定性感知候选生成。</li>
</ul>

<h3>Title: Intellectual Property Protection for 3D Gaussian Splatting Assets: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Longjie Zhao, Ziming Hong, Jiaxin Huang, Runnan Chen, Mingming Gong, Tongliang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03878">https://arxiv.org/abs/2602.03878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03878">https://arxiv.org/pdf/2602.03878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03878]] Intellectual Property Protection for 3D Gaussian Splatting Assets: A Survey(https://arxiv.org/abs/2602.03878)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>3D Gaussian Splatting (3DGS) has become a mainstream representation for real-time 3D scene synthesis, enabling applications in virtual and augmented reality, robotics, and 3D content creation. Its rising commercial value and explicit parametric structure raise emerging intellectual property (IP) protection concerns, prompting a surge of research on 3DGS IP protection. However, current progress remains fragmented, lacking a unified view of the underlying mechanisms, protection paradigms, and robustness challenges. To address this gap, we present the first systematic survey on 3DGS IP protection and introduce a bottom-up framework that examines (i) underlying Gaussian-based perturbation mechanisms, (ii) passive and active protection paradigms, and (iii) robustness threats under emerging generative AI era, revealing gaps in technical foundations and robustness characterization and indicating opportunities for deeper investigation. Finally, we outline six research directions across robustness, efficiency, and protection paradigms, offering a roadmap toward reliable and trustworthy IP protection for 3DGS assets.</li>
<li><strong>摘要：</strong>3D 高斯溅射 (3DGS) 已成为实时 3D 场景合成的主流表示形式，支持虚拟现实和增强现实、机器人技术以及 3D 内容创建中的应用。其不断上升的商业价值和明确的参数结构引发了新兴的知识产权 (IP) 保护问题，促使 3DGS 知识产权保护的研究激增。然而，目前的进展仍然支离破碎，缺乏对底层机制、保护范式和稳健性挑战的统一看法。为了解决这一差距，我们提出了第一个关于 3DGS 知识产权保护的系统调查，并引入了一个自下而上的框架，该框架检查（i）基于高斯的扰动机制，（ii）被动和主动保护范式，以及（iii）新兴生成人工智能时代的稳健性威胁，揭示技术基础和稳健性特征方面的差距，并指出进行更深入研究的机会。最后，我们概述了涵盖稳健性、效率和保护范式的六个研究方向，为 3DGS 资产提供可靠且值得信赖的 IP 保护的路线图。</li>
</ul>

<h3>Title: Audit After Segmentation: Reference-Free Mask Quality Assessment for Language-Referred Audio-Visual Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jinxing Zhou, Yanghao Zhou, Yaoting Wang, Zongyan Han, Jiaqi Ma, Henghui Ding, Rao Muhammad Anwer, Hisham Cholakkal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.MM, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03892">https://arxiv.org/abs/2602.03892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03892">https://arxiv.org/pdf/2602.03892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03892]] Audit After Segmentation: Reference-Free Mask Quality Assessment for Language-Referred Audio-Visual Segmentation(https://arxiv.org/abs/2602.03892)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Language-referred audio-visual segmentation (Ref-AVS) aims to segment target objects described by natural language by jointly reasoning over video, audio, and text. Beyond generating segmentation masks, providing rich and interpretable diagnoses of mask quality remains largely underexplored. In this work, we introduce Mask Quality Assessment in the Ref-AVS context (MQA-RefAVS), a new task that evaluates the quality of candidate segmentation masks without relying on ground-truth annotations as references at inference time. Given audio-visual-language inputs and each provided segmentation mask, the task requires estimating its IoU with the unobserved ground truth, identifying the corresponding error type, and recommending an actionable quality-control decision. To support this task, we construct MQ-RAVSBench, a benchmark featuring diverse and representative mask error modes that span both geometric and semantic issues. We further propose MQ-Auditor, a multimodal large language model (MLLM)-based auditor that explicitly reasons over multimodal cues and mask information to produce quantitative and qualitative mask quality assessments. Extensive experiments demonstrate that MQ-Auditor outperforms strong open-source and commercial MLLMs and can be integrated with existing Ref-AVS systems to detect segmentation failures and support downstream segmentation improvement. Data and codes will be released at this https URL.</li>
<li><strong>摘要：</strong>语言参考视听分割（Ref-AVS）旨在通过视频、音频和文本的联合推理来分割自然语言描述的目标对象。除了生成分割掩模之外，提供丰富且可解释的掩模质量诊断在很大程度上仍未得到充分探索。在这项工作中，我们在 Ref-AVS 上下文中引入了掩模质量评估 (MQA-RefAVS)，这是一项新任务，可评估候选分割掩模的质量，而无需依赖真实注释作为推理时的参考。给定视听语言输入和每个提供的分割掩码，该任务需要使用未观察到的真实情况估计其 IoU，识别相应的错误类型，并推荐可行的质量控制决策。为了支持这项任务，我们构建了 MQ-RAVSBench，这是一个基准测试，具有跨越几何和语义问题的多样化且具有代表性的掩模错误模式。我们进一步提出了 MQ-Auditor，这是一种基于多模态大语言模型 (MLLM) 的审计器，它可以对多模态线索和掩码信息进行明确推理，以产生定量和定性的掩码质量评估。大量实验表明，MQ-Auditor 的性能优于强大的开源和商业 MLLM，并且可以与现有 Ref-AVS 系统集成，以检测分段故障并支持下游分段改进。数据和代码将在此 https URL 发布。</li>
</ul>

<h3>Title: NeuroPareto: Calibrated Acquisition for Costly Many-Goal Search in Vast Parameter Spaces</h3>
<ul>
<li><strong>Authors: </strong>Rong Fu, Wenxin Zhang, Chunlei Meng, Youjin Wang, Haoyu Zhao, Jiaxuan Lu, Kun Liu, JiaBao Dou, Simon James Fong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03901">https://arxiv.org/abs/2602.03901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03901">https://arxiv.org/pdf/2602.03901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03901]] NeuroPareto: Calibrated Acquisition for Costly Many-Goal Search in Vast Parameter Spaces(https://arxiv.org/abs/2602.03901)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The pursuit of optimal trade-offs in high-dimensional search spaces under stringent computational constraints poses a fundamental challenge for contemporary multi-objective optimization. We develop NeuroPareto, a cohesive architecture that integrates rank-centric filtering, uncertainty disentanglement, and history-conditioned acquisition strategies to navigate complex objective landscapes. A calibrated Bayesian classifier estimates epistemic uncertainty across non-domination tiers, enabling rapid generation of high-quality candidates with minimal evaluation cost. Deep Gaussian Process surrogates further separate predictive uncertainty into reducible and irreducible components, providing refined predictive means and risk-aware signals for downstream selection. A lightweight acquisition network, trained online from historical hypervolume improvements, guides expensive evaluations toward regions balancing convergence and diversity. With hierarchical screening and amortized surrogate updates, the method maintains accuracy while keeping computational overhead low. Experiments on DTLZ and ZDT suites and a subsurface energy extraction task show that NeuroPareto consistently outperforms classifier-enhanced and surrogate-assisted baselines in Pareto proximity and hypervolume.</li>
<li><strong>摘要：</strong>在严格的计算约束下追求高维搜索空间中的最优权衡对当代多目标优化提出了根本性挑战。我们开发了 NeuroPareto，这是一种内聚架构，集成了以排名为中心的过滤、不确定性解缠和历史条件获取策略，以导航复杂的客观环境。经过校准的贝叶斯分类器可估计非支配层的认知不确定性，从而能够以最小的评估成本快速生成高质量的候选者。深度高斯过程代理将预测不确定性进一步分离为可约和不可约分量，为下游选择提供精细的预测手段和风险意识信号。一个轻量级采集网络，通过历史超容量改进进行在线训练，指导对平衡收敛性和多样性的区域进行昂贵的评估。通过分层筛选和摊销替代更新，该方法可以保持准确性，同时保持较低的计算开销。 DTLZ 和 ZDT 套件以及地下能量提取任务的实验表明，NeuroPareto 在 Pareto 邻近性和超体积方面始终优于分类器增强和替代辅助基线。</li>
</ul>

<h3>Title: HY3D-Bench: Generation of 3D Assets</h3>
<ul>
<li><strong>Authors: </strong>Team Hunyuan3D: Bowen Zhang, Chunchao Guo, Dongyuan Guo, Haolin Liu, Hongyu Yan, Huiwen Shi, Jiaao Yu, Jiachen Xu, Jingwei Huang, Kunhong Li, Lifu Wang, Linus, Penghao Wang, Qingxiang Lin, Ruining Tang, Xianghui Yang, Yang Li, Yirui Guan, Yunfei Zhao, Yunhan Yang, Zeqiang Lai, Zhihao Liang, Zibo Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03907">https://arxiv.org/abs/2602.03907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03907">https://arxiv.org/pdf/2602.03907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03907]] HY3D-Bench: Generation of 3D Assets(https://arxiv.org/abs/2602.03907)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>While recent advances in neural representations and generative models have revolutionized 3D content creation, the field remains constrained by significant data processing bottlenecks. To address this, we introduce HY3D-Bench, an open-source ecosystem designed to establish a unified, high-quality foundation for 3D generation. Our contributions are threefold: (1) We curate a library of 250k high-fidelity 3D objects distilled from large-scale repositories, employing a rigorous pipeline to deliver training-ready artifacts, including watertight meshes and multi-view renderings; (2) We introduce structured part-level decomposition, providing the granularity essential for fine-grained perception and controllable editing; and (3) We bridge real-world distribution gaps via a scalable AIGC synthesis pipeline, contributing 125k synthetic assets to enhance diversity in long-tail categories. Validated empirically through the training of Hunyuan3D-2.1-Small, HY3D-Bench democratizes access to robust data resources, aiming to catalyze innovation across 3D perception, robotics, and digital content creation.</li>
<li><strong>摘要：</strong>虽然神经表示和生成模型的最新进展彻底改变了 3D 内容创建，但该领域仍然受到重大数据处理瓶颈的限制。为了解决这个问题，我们推出了 HY3D-Bench，这是一个开源生态系统，旨在为 3D 生成建立统一、高质量的基础。我们的贡（2）引入结构化的零件级分解，为细粒度感知和可控编辑提供必要的粒度； (3) 我们通过可扩展的 AIGC 合成管道弥合现实世界的分配差距，贡献 125,000 种合成资产，以增强长尾类别的多样性。通过 Hunyuan3D-2.1-Small 的训练进行了实证验证，HY3D-Bench 实现了对强大数据资源的民主化访问，旨在促进 3D 感知、机器人技术和数字内容创建方面的创新。</li>
</ul>

<h3>Title: Phaedra: Learning High-Fidelity Discrete Tokenization for the Physical Science</h3>
<ul>
<li><strong>Authors: </strong>Levi Lingsch, Georgios Kissas, Johannes Jakubik, Siddhartha Mishra</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CE, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03915">https://arxiv.org/abs/2602.03915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03915">https://arxiv.org/pdf/2602.03915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03915]] Phaedra: Learning High-Fidelity Discrete Tokenization for the Physical Science(https://arxiv.org/abs/2602.03915)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Tokens are discrete representations that allow modern deep learning to scale by transforming high-dimensional data into sequences that can be efficiently learned, generated, and generalized to new tasks. These have become foundational for image and video generation and, more recently, physical simulation. As existing tokenizers are designed for the explicit requirements of realistic visual perception of images, it is necessary to ask whether these approaches are optimal for scientific images, which exhibit a large dynamic range and require token embeddings to retain physical and spectral properties. In this work, we investigate the accuracy of a suite of image tokenizers across a range of metrics designed to measure the fidelity of PDE properties in both physical and spectral space. Based on the observation that these struggle to capture both fine details and precise magnitudes, we propose Phaedra, inspired by classical shape-gain quantization and proper orthogonal decomposition. We demonstrate that Phaedra consistently improves reconstruction across a range of PDE datasets. Additionally, our results show strong out-of-distribution generalization capabilities to three tasks of increasing complexity, namely known PDEs with different conditions, unknown PDEs, and real-world Earth observation and weather data.</li>
<li><strong>摘要：</strong>令牌是离散表示，允许现代深度学习通过将高维数据转换为可以有效学习、生成和推广到新任务的序列来扩展。这些已成为图像和视频生成以及最近的物理模拟的基础。由于现有的标记器是为图像的真实视觉感知的明确要求而设计的，因此有必要询问这些方法是否最适合科学图像，因为科学图像表现出很大的动态范围，并且需要标记嵌入来保留物理和光谱属性。在这项工作中，我们研究了一系列图像标记器的准确性，这些指标旨在测量物理和光谱空间中偏微分方程属性的保真度。基于对这些难以捕捉精细细节和精确幅度的观察，我们提出了 Phaedra，其灵感来自于经典的形状增益量化和适当的正交分解。我们证明了 Phaedra 持续改进了一系列偏微分方程数据集的重建。此外，我们的结果显示了对三个日益复杂的任务的强大的分布外泛化能力，即不同条件下的已知偏微分方程、未知偏微分方程以及真实世界的地球观测和天气数据。</li>
</ul>

<h3>Title: WIND: Weather Inverse Diffusion for Zero-Shot Atmospheric Modeling</h3>
<ul>
<li><strong>Authors: </strong>Michael Aich, Andreas Fürst, Florian Sestak, Carlos Ruiz-Gonzalez, Niklas Boers, Johannes Brandstetter</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.ao-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03924">https://arxiv.org/abs/2602.03924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03924">https://arxiv.org/pdf/2602.03924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03924]] WIND: Weather Inverse Diffusion for Zero-Shot Atmospheric Modeling(https://arxiv.org/abs/2602.03924)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep learning has revolutionized weather and climate modeling, yet the current landscape remains fragmented: highly specialized models are typically trained individually for distinct tasks. To unify this landscape, we introduce WIND, a single pre-trained foundation model capable of replacing specialized baselines across a vast array of tasks. Crucially, in contrast to previous atmospheric foundation models, we achieve this without any task-specific fine-tuning. To learn a robust, task-agnostic prior of the atmosphere, we pre-train WIND with a self-supervised video reconstruction objective, utilizing an unconditional video diffusion model to iteratively reconstruct atmospheric dynamics from a noisy state. At inference, we frame diverse domain-specific problems strictly as inverse problems and solve them via posterior sampling. This unified approach allows us to tackle highly relevant weather and climate problems, including probabilistic forecasting, spatial and temporal downscaling, sparse reconstruction and enforcing conservation laws purely with our pre-trained model. We further demonstrate the model's capacity to generate physically consistent counterfactual storylines of extreme weather events under global warming scenarios. By combining generative video modeling with inverse problem solving, WIND offers a computationally efficient paradigm shift in AI-based atmospheric modeling.</li>
<li><strong>摘要：</strong>深度学习彻底改变了天气和气候建模，但目前的情况仍然支离破碎：高度专业化的模型通常针对不同的任务进行单独训练。为了统一这一格局，我们引入了 WIND，这是一个单一的预训练基础模型，能够替换大量任务中的专用基线。至关重要的是，与之前的大气基础模型相比，我们无需任何特定于任务的微调即可实现这一目标。为了学习稳健的、与任务无关的大气先验，我们使用自监督视频重建目标对 WIND 进行预训练，利用无条件视频扩散模型从噪声状态迭代重建大气动态。在推理时，我们将不同的特定领域问题严格定义为逆问题，并通过后验采样来解决它们。这种统一的方法使我们能够解决高度相关的天气和气候问题，包括概率预测、空间和时间降尺度、稀疏重建以及纯粹使用我们预先训练的模型执行守恒定律。我们进一步证明了该模型在全球变暖情景下生成物理上一致的极端天气事件反事实故事情节的能力。通过将生成视频建模与逆向问题解决相结合，WIND 为基于 AI 的大气建模提供了计算高效的范式转变。</li>
</ul>

<h3>Title: PromptSplit: Revealing Prompt-Level Disagreement in Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Mehdi Lotfian, Mohammad Jalali, Farzan Farnia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04009">https://arxiv.org/abs/2602.04009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04009">https://arxiv.org/pdf/2602.04009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04009]] PromptSplit: Revealing Prompt-Level Disagreement in Generative Models(https://arxiv.org/abs/2602.04009)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Prompt-guided generative AI models have rapidly expanded across vision and language domains, producing realistic and diverse outputs from textual inputs. The growing variety of such models, trained with different data and architectures, calls for principled methods to identify which types of prompts lead to distinct model behaviors. In this work, we propose PromptSplit, a kernel-based framework for detecting and analyzing prompt-dependent disagreement between generative models. For each compared model pair, PromptSplit constructs a joint prompt--output representation by forming tensor-product embeddings of the prompt and image (or text) features, and then computes the corresponding kernel covariance matrix. We utilize the eigenspace of the weighted difference between these matrices to identify the main directions of behavioral difference across prompts. To ensure scalability, we employ a random-projection approximation that reduces computational complexity to $O(nr^2 + r^3)$ for projection dimension $r$. We further provide a theoretical analysis showing that this approximation yields an eigenstructure estimate whose expected deviation from the full-dimensional result is bounded by $O(1/r^2)$. Experiments across text-to-image, text-to-text, and image-captioning settings demonstrate that PromptSplit accurately detects ground-truth behavioral differences and isolates the prompts responsible, offering an interpretable tool for detecting where generative models disagree.</li>
<li><strong>摘要：</strong>即时引导的生成人工智能模型已迅速扩展到视觉和语言领域，从文本输入产生现实且多样化的输出。此类模型的种类越来越多，并使用不同的数据和架构进行训练，需要有原则的方法来识别哪些类型的提示会导致不同的模型行为。在这项工作中，我们提出了 PromptSplit，一个基于内核的框架，用于检测和分析生成模型之间与提示相关的分歧。对于每个比较的模型对，PromptSplit 通过形成提示和图像（或文本）特征的张量积嵌入来构造联合提示-输出表示，然后计算相应的核协方差矩阵。我们利用这些矩阵之间的加权差异的特征空间来识别提示之间行为差异的主要方向。为了确保可扩展性，我们采用随机投影近似，将投影维度 $r$ 的计算复杂度降低至 $O(nr^2 + r^3)$。我们进一步提供了理论分析，表明这种近似产生了一个特征结构估计，其与全维结果的预期偏差受到 $O(1/r^2)$ 的限制。跨文本到图像、文本到文本和图像字幕设置的实验表明，PromptSplit 可以准确检测真实行为差异并隔离负责的提示，从而提供可解释的工具来检测生成模型不一致的地方。</li>
</ul>

<h3>Title: DADP: Domain Adaptive Diffusion Policy</h3>
<ul>
<li><strong>Authors: </strong>Pengcheng Wang, Qinghang Liu, Haotian Lin, Yiheng Li, Guojian Zhan, Masayoshi Tomizuka, Yixiao Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04037">https://arxiv.org/abs/2602.04037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04037">https://arxiv.org/pdf/2602.04037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04037]] DADP: Domain Adaptive Diffusion Policy(https://arxiv.org/abs/2602.04037)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Learning domain adaptive policies that can generalize to unseen transition dynamics, remains a fundamental challenge in learning-based control. Substantial progress has been made through domain representation learning to capture domain-specific information, thus enabling domain-aware decision making. We analyze the process of learning domain representations through dynamical prediction and find that selecting contexts adjacent to the current step causes the learned representations to entangle static domain information with varying dynamical properties. Such mixture can confuse the conditioned policy, thereby constraining zero-shot adaptation. To tackle the challenge, we propose DADP (Domain Adaptive Diffusion Policy), which achieves robust adaptation through unsupervised disentanglement and domain-aware diffusion injection. First, we introduce Lagged Context Dynamical Prediction, a strategy that conditions future state estimation on a historical offset context; by increasing this temporal gap, we unsupervisedly disentangle static domain representations by filtering out transient properties. Second, we integrate the learned domain representations directly into the generative process by biasing the prior distribution and reformulating the diffusion target. Extensive experiments on challenging benchmarks across locomotion and manipulation demonstrate the superior performance, and the generalizability of DADP over prior methods. More visualization results are available on the this https URL.</li>
<li><strong>摘要：</strong>可以推广到看不见的过渡动态的学习域自适应策略仍然是基于学习的控制的基本挑战。通过领域表示学习来捕获特定领域的信息，从而实现领域感知决策，已经取得了实质性进展。我们分析了通过动态预测学习域表示的过程，发现选择与当前步骤相邻的上下文会导致学习到的表示将静态域信息与不同的动态属性纠缠在一起。这种混合可能会混淆条件策略，从而限制零样本适应。为了应对这一挑战，我们提出了 DADP（域自适应扩散策略），它通过无监督解缠和域感知扩散注入实现稳健的自适应。首先，我们介绍滞后上下文动态预测，这是一种根据历史偏移上下文来估计未来状态的策略；通过增加这种时间间隙，我们可以通过过滤瞬态属性来无监督地解开静态域表示。其次，我们通过偏置先验分布并重新制定扩散目标，将学习到的域表示直接集成到生成过程中。在运动和操作方面具有挑战性的基准上进行的广泛实验证明了 DADP 相对于先前方法的优越性能和通用性。此 https URL 上提供了更多可视化结果。</li>
</ul>

<h3>Title: Fast, Unsupervised Framework for Registration Quality Assessment of Multi-stain Histological Whole Slide Pairs</h3>
<ul>
<li><strong>Authors: </strong>Shikha Dubey, Patricia Raciti, Kristopher Standish, Albert Juan Ramon, Erik Ames Burlingame</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04046">https://arxiv.org/abs/2602.04046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04046">https://arxiv.org/pdf/2602.04046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04046]] Fast, Unsupervised Framework for Registration Quality Assessment of Multi-stain Histological Whole Slide Pairs(https://arxiv.org/abs/2602.04046)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>High-fidelity registration of histopathological whole slide images (WSIs), such as hematoxylin & eosin (H&E) and immunohistochemistry (IHC), is vital for integrated molecular analysis but challenging to evaluate without ground-truth (GT) annotations. Existing WSI-level assessments -- using annotated landmarks or intensity-based similarity metrics -- are often time-consuming, unreliable, and computationally intensive, limiting large-scale applicability. This study proposes a fast, unsupervised framework that jointly employs down-sampled tissue masks- and deformations-based metrics for registration quality assessment (RQA) of registered H&E and IHC WSI pairs. The masks-based metrics measure global structural correspondence, while the deformations-based metrics evaluate local smoothness, continuity, and transformation realism. Validation across multiple IHC markers and multi-expert assessments demonstrate a strong correlation between automated metrics and human evaluations. In the absence of GT, this framework offers reliable, real-time RQA with high fidelity and minimal computational resources, making it suitable for large-scale quality control in digital pathology.</li>
<li><strong>摘要：</strong>组织病理学全切片图像 (WSI)（例如苏木精和伊红 (H&E) 和免疫组织化学 (IHC)）的高保真配准对于综合分子分析至关重要，但在没有真实 (GT) 注释的情况下很难进行评估。现有的 WSI 级别评估（使用带注释的地标或基于强度的相似性度量）通常非常耗时、不可靠且计算量大，限制了大规模适用性。本研究提出了一种快速、无监督的框架，联合采用基于下采样组织掩模和变形的指标来对注册的 H&E 和 IHC WSI 对进行注册质量评估 (RQA)。基于掩模的指标测量全局结构对应性，而基于变形的指标评估局部平滑度、连续性和变换真实性。多个 IHC 标记物和多专家评估的验证表明，自动化指标和人工评估之间存在很强的相关性。在没有 GT 的情况下，该框架提供可靠、实时的 RQA，具有高保真度和最少的计算资源，使其适合数字病理学中的大规模质量控制。</li>
</ul>

<h3>Title: Artifact Removal and Image Restoration in AFM:A Structured Mask-Guided Directional Inpainting Approach</h3>
<ul>
<li><strong>Authors: </strong>Juntao Zhang, Angona Biswas, Jaydeep Rade, Charchit Shukla, Juan Ren, Anwesha Sarkar, Adarsh Krishnamurthy, Aditya Balu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04051">https://arxiv.org/abs/2602.04051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04051">https://arxiv.org/pdf/2602.04051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04051]] Artifact Removal and Image Restoration in AFM:A Structured Mask-Guided Directional Inpainting Approach(https://arxiv.org/abs/2602.04051)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Atomic Force Microscopy (AFM) enables high-resolution surface imaging at the nanoscale, yet the output is often degraded by artifacts introduced by environmental noise, scanning imperfections, and tip-sample interactions. To address this challenge, a lightweight and fully automated framework for artifact detection and restoration in AFM image analysis is presented. The pipeline begins with a classification model that determines whether an AFM image contains artifacts. If necessary, a lightweight semantic segmentation network, custom-designed and trained on AFM data, is applied to generate precise artifact masks. These masks are adaptively expanded based on their structural orientation and then inpainted using a directional neighbor-based interpolation strategy to preserve 3D surface continuity. A localized Gaussian smoothing operation is then applied for seamless restoration. The system is integrated into a user-friendly GUI that supports real-time parameter adjustments and batch processing. Experimental results demonstrate the effective artifact removal while preserving nanoscale structural details, providing a robust, geometry-aware solution for high-fidelity AFM data interpretation.</li>
<li><strong>摘要：</strong>原子力显微镜 (AFM) 可实现纳米级的高分辨率表面成像，但输出常常因环境噪声、扫描缺陷和尖端与样品相互作用引入的伪影而降低。为了应对这一挑战，提出了一种用于 AFM 图像分析中的伪影检测和恢复的轻量级全自动框架。该流程从一个分类模型开始，该模型确定 AFM 图像是否包含伪影。如有必要，可以应用根据 AFM 数据定制设计和训练的轻量级语义分割网络来生成精确的伪影掩模。这些掩模根据其结构方向自适应扩展，然后使用基于方向邻居的插值策略进行修复，以保持 3D 表面连续性。然后应用局部高斯平滑操作进行无缝恢复。该系统集成到用户友好的GUI中，支持实时参数调整和批量处理。实验结果表明，可以有效去除伪影，同时保留纳米级结构细节，为高保真 AFM 数据解释提供强大的几何感知解决方案。</li>
</ul>

<h3>Title: Agentic AI-Empowered Dynamic Survey Framework</h3>
<ul>
<li><strong>Authors: </strong>Furkan Mumcu, Lokman Bekit, Michael J. Jones, Anoop Cherian, Yasin Yilmaz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04071">https://arxiv.org/abs/2602.04071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04071">https://arxiv.org/pdf/2602.04071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04071]] Agentic AI-Empowered Dynamic Survey Framework(https://arxiv.org/abs/2602.04071)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Survey papers play a central role in synthesizing and organizing scientific knowledge, yet they are increasingly strained by the rapid growth of research output. As new work continues to appear after publication, surveys quickly become outdated, contributing to redundancy and fragmentation in the literature. We reframe survey writing as a long-horizon maintenance problem rather than a one-time generation task, treating surveys as living documents that evolve alongside the research they describe. We propose an agentic Dynamic Survey Framework that supports the continuous updating of existing survey papers by incrementally integrating new work while preserving survey structure and minimizing unnecessary disruption. Using a retrospective experimental setup, we demonstrate that the proposed framework effectively identifies and incorporates emerging research while preserving the coherence and structure of existing surveys.</li>
<li><strong>摘要：</strong>调查论文在综合和组织科学知识方面发挥着核心作用，但由于研究成果的快速增长，调查论文的压力越来越大。随着新作品在出版后不断出现，调查很快就会过时，导致文献的冗余和碎片化。我们将调查写作重新定义为一个长期维护问题，而不是一次性生成任务，将调查视为随其描述的研究一起发展的活文件。我们提出了一个代理动态调查框架，通过增量集成新工作来支持现有调查论文的持续更新，同时保留调查结构并最大限度地减少不必要的干扰。使用回顾性实验设置，我们证明所提出的框架有效地识别和纳入新兴研究，同时保留现有调查的连贯性和结构。</li>
</ul>

<h3>Title: CoRe: Context-Robust Remasking for Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kevin Zhai, Sabbir Mollah, Zhenyi Wang, Mubarak Shah</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04096">https://arxiv.org/abs/2602.04096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04096">https://arxiv.org/pdf/2602.04096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04096]] CoRe: Context-Robust Remasking for Diffusion Language Models(https://arxiv.org/abs/2602.04096)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Standard decoding in Masked Diffusion Models (MDMs) is hindered by context rigidity: tokens are retained based on transient high confidence, often ignoring that early predictions lack full context. This creates cascade effects where initial inconsistencies misguide the remaining generation. Existing revision strategies attempt to mitigate this by relying on static confidence scores, but these signals are inherently myopic; inconsistent tokens can appear confident to the model itself. We propose Context-Robust Remasking (CoRe), a training-free framework for inference-time revision. Rather than trusting static token probabilities, CoRe identifies context-brittle tokens by probing their sensitivity to targeted masked-context perturbations. We formalize revision as a robust optimization objective over context shifts and efficiently approximate this objective to prioritize unstable tokens for revision. On LLaDA-8B-Base, CoRe delivers consistent improvements across reasoning and code benchmarks, outperforming compute-matched baselines and improving MBPP by up to 9.2 percentage points.</li>
<li><strong>摘要：</strong>掩码扩散模型 (MDM) 中的标准解码受到上下文刚性的阻碍：基于瞬态高置信度保留标记，通常忽略早期预测缺乏完整的上下文。这会产生级联效应，最初的不一致会误导剩下的一代。现有的修订策略试图通过依赖静态置信度分数来缓解这一问题，但这些信号本质上是短视的；不一致的标记可能会让模型本身显得自信。我们提出了上下文鲁棒重掩码（CoRe），这是一种用于推理时间修订的免训练框架。 CoRe 不是信任静态令牌概率，而是通过探测它们对目标屏蔽上下文扰动的敏感性来识别上下文脆弱令牌。我们将修订形式化为针对上下文变化的稳健优化目标，并有效地近似该目标，以优先考虑不稳定的标记进行修订。在 LLaDA-8B-Base 上，CoRe 在推理和代码基准测试中提供了一致的改进，超越了计算匹配的基准，并将 MBPP 提高了高达 9.2 个百分点。</li>
</ul>

<h3>Title: Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach</h3>
<ul>
<li><strong>Authors: </strong>Sicheng Liu, Xunkai Li, Daohan Su, Ru Zhang, Hongchao Qin, Ronghua Li, Guoren Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04116">https://arxiv.org/abs/2602.04116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04116">https://arxiv.org/pdf/2602.04116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04116]] Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach(https://arxiv.org/abs/2602.04116)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Graph Foundation Models (GFMs) have achieved remarkable success in generalizing across diverse domains. However, they mainly focus on Text-Attributed Graphs (TAGs), leaving Multimodal-Attributed Graphs (MAGs) largely untapped. Developing Multimodal Graph Foundation Models (MGFMs) allows for leveraging the rich multimodal information in MAGs, and extends applicability to broader types of downstream tasks. While recent MGFMs integrate diverse modality information, our empirical investigation reveals two fundamental limitations of existing MGFMs: (1)they fail to explicitly model modality interaction, essential for capturing intricate cross-modal semantics beyond simple aggregation, and (2)they exhibit sub-optimal modality alignment, which is critical for bridging the significant semantic disparity between distinct modal spaces. To address these challenges, we propose PLANET (graPh topoLogy-aware modAlity iNteraction and alignmEnT), a novel framework employing a Divide-and-Conquer strategy to decouple modality interaction and alignment across distinct granularities. At the embedding granularity, (1)Embedding-wise Domain Gating (EDG) performs local semantic enrichment by adaptively infusing topology-aware cross-modal context, achieving modality interaction. At the node granularity, (2)Node-wise Discretization Retrieval (NDR) ensures global modality alignment by constructing a Discretized Semantic Representation Space (DSRS) to bridge modality gaps. Extensive experiments demonstrate that PLANET significantly outperforms state-of-the-art baselines across diverse graph-centric and multimodal generative tasks.</li>
<li><strong>摘要：</strong>图基础模型（GFM）在跨不同领域的泛化方面取得了显着的成功。然而，他们主要关注文本属性图（TAG），而多模式属性图（MAG）在很大程度上尚未开发。开发多模态图基础模型 (MGFM) 可以利用 MAG 中丰富的多模态信息，并将适用性扩展到更广泛类型的下游任务。虽然最近的 MGFM 集成了不同的模态信息，但我们的实证研究揭示了现有 MGFM 的两个基本局限性：（1）它们无法显式地建模模态交互，这对于捕获简单聚合之外的复杂的跨模态语义至关重要；（2）它们表现出次优的模态对齐，这对于弥合不同模态空间之间的显着语义差异至关重要。为了应对这些挑战，我们提出了 PLANET（图拓扑感知模态交互和对齐），这是一种采用分而治之策略来解耦跨不同粒度的模态交互和对齐的新颖框架。在嵌入粒度上，（1）嵌入智能域门控（EDG）通过自适应地注入拓扑感知的跨模态上下文来执行局部语义丰富，实现模态交互。在节点粒度上，（2）节点离散化检索（NDR）通过构建离散语义表示空间（DSRS）来弥合模态间隙，从而确保全局模态对齐。大量实验表明，PLANET 在各种以图为中心的多模态生成任务中显着优于最先进的基线。</li>
</ul>

<h3>Title: Synthesizable Molecular Generation via Soft-constrained GFlowNets with Rich Chemical Priors</h3>
<ul>
<li><strong>Authors: </strong>Hyeonah Kim, Minsu Kim, Celine Roget, Dionessa Biton, Louis Vaillancourt, Yves V. Brun, Yoshua Bengio, Alex Hernandez-Garcia</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04119">https://arxiv.org/abs/2602.04119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04119">https://arxiv.org/pdf/2602.04119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04119]] Synthesizable Molecular Generation via Soft-constrained GFlowNets with Rich Chemical Priors(https://arxiv.org/abs/2602.04119)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The application of generative models for experimental drug discovery campaigns is severely limited by the difficulty of designing molecules de novo that can be synthesized in practice. Previous works have leveraged Generative Flow Networks (GFlowNets) to impose hard synthesizability constraints through the design of state and action spaces based on predefined reaction templates and building blocks. Despite the promising prospects of this approach, it currently lacks flexibility and scalability. As an alternative, we propose S3-GFN, which generates synthesizable SMILES molecules via simple soft regularization of a sequence-based GFlowNet. Our approach leverages rich molecular priors learned from large-scale SMILES corpora to steer molecular generation towards high-reward, synthesizable chemical spaces. The model induces constraints through off-policy replay training with a contrastive learning signal based on separate buffers of synthesizable and unsynthesizable samples. Our experiments show that S3-GFN learns to generate synthesizable molecules ($\geq 95\%$) with higher rewards in diverse tasks.</li>
<li><strong>摘要：</strong>由于从头设计可在实践中合成的分子的难度，生成模型在实验药物发现活动中的应用受到严重限制。之前的工作利用生成流网络（GFlowNets）通过基于预定义的反应模板和构建块设计状态和动作空间来施加硬可合成性约束。尽管这种方法前景广阔，但目前缺乏灵活性和可扩展性。作为替代方案，我们提出了 S3-GFN，它通过基于序列的 GFlowNet 的简单软正则化生成可合成的 SMILES 分子。我们的方法利用从大规模 SMILES 语料库中学到的丰富的分子先验来引导分子生成走向高回报、可合成的化学空间。该模型通过离策略重放训练和基于可合成和不可合成样本的单独缓冲区的对比学习信号来引入约束。我们的实验表明，S3-GFN 学习生成可合成的分子 ($\geq 95\%$)，在不同的任务中获得更高的奖励。</li>
</ul>

<h3>Title: Scalable Explainability-as-a-Service (XaaS) for Edge AI Systems</h3>
<ul>
<li><strong>Authors: </strong>Samaresh Kumar Singh, Joyjit Roy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04120">https://arxiv.org/abs/2602.04120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04120">https://arxiv.org/pdf/2602.04120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04120]] Scalable Explainability-as-a-Service (XaaS) for Edge AI Systems(https://arxiv.org/abs/2602.04120)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Though Explainable AI (XAI) has made significant advancements, its inclusion in edge and IoT systems is typically ad-hoc and inefficient. Most current methods are "coupled" in such a way that they generate explanations simultaneously with model inferences. As a result, these approaches incur redundant computation, high latency and poor scalability when deployed across heterogeneous sets of edge devices. In this work we propose Explainability-as-a-Service (XaaS), a distributed architecture for treating explainability as a first-class system service (as opposed to a model-specific feature). The key innovation in our proposed XaaS architecture is that it decouples inference from explanation generation allowing edge devices to request, cache and verify explanations subject to resource and latency constraints. To achieve this, we introduce three main innovations: (1) A distributed explanation cache with a semantic similarity based explanation retrieval method which significantly reduces redundant computation; (2) A lightweight verification protocol that ensures the fidelity of both cached and newly generated explanations; and (3) An adaptive explanation engine that chooses explanation methods based upon device capability and user requirement. We evaluated the performance of XaaS on three real-world edge-AI use cases: (i) manufacturing quality control; (ii) autonomous vehicle perception; and (iii) healthcare diagnostics. Experimental results show that XaaS reduces latency by 38\% while maintaining high explanation quality across three real-world deployments. Overall, this work enables the deployment of transparent and accountable AI across large scale, heterogeneous IoT systems, and bridges the gap between XAI research and edge-practicality.</li>
<li><strong>摘要：</strong>尽管可解释人工智能 (XAI) 取得了重大进步，但它在边缘和物联网系统中的包含通常是临时的且效率低下。当前的大多数方法都是“耦合”的，即它们与模型推理同时生成解释。因此，当跨异构边缘设备集部署时，这些方法会导致冗余计算、高延迟和可扩展性差。在这项工作中，我们提出了可解释性即服务（XaaS），这是一种分布式架构，用于将可解释性视为一流的系统服务（而不是特定于模型的功能）。我们提出的 XaaS 架构的关键创新在于，它将推理与解释生成分离，允许边​​缘设备在资源和延迟限制的情况下请求、缓存和验证解释。为了实现这一目标，我们引入了三个主要创新：（1）分布式解释缓存，具有基于语义相似性的解释检索方法，可显着减少冗余计算； (2) 轻量级验证协议，确保缓存和新生成的解释的保真度； (3)自适应解释引擎，根据设备能力和用户需求选择解释方法。我们评估了 XaaS 在三个现实世界边缘 AI 用例中的性能：(i) 制造质量控制； (ii) 自动驾驶车辆感知； (iii) 医疗保健诊断。实验结果表明，XaaS 将延迟减少了 38%，同时在三个实际部署中保持了较高的解释质量。总体而言，这项工作能够在大规模、异构物联网系统中部署透明且负责任的人工智能，并弥合 XAI 研究与边缘实用性之间的差距。</li>
</ul>

<h3>Title: Generative Neural Operators through Diffusion Last Layer</h3>
<ul>
<li><strong>Authors: </strong>Sungwon Park, Anthony Zhou, Hongjoong Kim, Amir Barati Farimani</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04139">https://arxiv.org/abs/2602.04139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04139">https://arxiv.org/pdf/2602.04139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04139]] Generative Neural Operators through Diffusion Last Layer(https://arxiv.org/abs/2602.04139)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Neural operators have emerged as a powerful paradigm for learning discretization-invariant function-to-function mappings in scientific computing. However, many practical systems are inherently stochastic, making principled uncertainty quantification essential for reliable deployment. To address this, we introduce a simple add-on, the diffusion last layer (DLL), a lightweight probabilistic head that can be attached to arbitrary neural operator backbones to model predictive uncertainty. Motivated by the relative smoothness and low-dimensional structure often exhibited by PDE solution distributions, DLL parameterizes the conditional output distribution directly in function space through a low-rank Karhunen-Loève expansion, enabling efficient and expressive uncertainty modeling. Across stochastic PDE operator learning benchmarks, DLL improves generalization and uncertainty-aware prediction. Moreover, even in deterministic long-horizon rollout settings, DLL enhances rollout stability and provides meaningful estimates of epistemic uncertainty for backbone neural operators.</li>
<li><strong>摘要：</strong>神经算子已成为学习科学计算中离散不变函数到函数映射的强大范例。然而，许多实际系统本质上是随机的，因此原则上的不确定性量化对于可靠部署至关重要。为了解决这个问题，我们引入了一个简单的附加组件，即扩散最后层（DLL），这是一个轻量级概率头，可以附加到任意神经算子主干上以对预测不确定性进行建模。受偏微分方程解分布经常表现出的相对平滑度和低维结构的启发，DLL 通过低秩 Karhunen-Loève 展开直接在函数空间中参数化条件输出分布，从而实现高效且富有表现力的不确定性建模。在随机 PDE 算子学习基准中，DLL 改进了泛化和不确定性感知预测。此外，即使在确定性的长范围推出设置中，DLL 也可以增强推出稳定性，并为骨干神经算子提供有意义的认知不确定性估计。</li>
</ul>

<h3>Title: Natural Language Instructions for Scene-Responsive Human-in-the-Loop Motion Planning in Autonomous Driving using Vision-Language-Action Models</h3>
<ul>
<li><strong>Authors: </strong>Angel Martinez-Sanchez, Parthib Roy, Ross Greer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04184">https://arxiv.org/abs/2602.04184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04184">https://arxiv.org/pdf/2602.04184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04184]] Natural Language Instructions for Scene-Responsive Human-in-the-Loop Motion Planning in Autonomous Driving using Vision-Language-Action Models(https://arxiv.org/abs/2602.04184)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Instruction-grounded driving, where passenger language guides trajectory planning, requires vehicles to understand intent before motion. However, most prior instruction-following planners rely on simulation or fixed command vocabularies, limiting real-world generalization. doScenes, the first real-world dataset linking free-form instructions (with referentiality) to nuScenes ground-truth motion, enables instruction-conditioned planning. In this work, we adapt OpenEMMA, an open-source MLLM-based end-to-end driving framework that ingests front-camera views and ego-state and outputs 10-step speed-curvature trajectories, to this setting, presenting a reproducible instruction-conditioned baseline on doScenes and investigate the effects of human instruction prompts on predicted driving behavior. We integrate doScenes directives as passenger-style prompts within OpenEMMA's vision-language interface, enabling linguistic conditioning before trajectory generation. Evaluated on 849 annotated scenes using ADE, we observe that instruction conditioning substantially improves robustness by preventing extreme baseline failures, yielding a 98.7% reduction in mean ADE. When such outliers are removed, instructions still influence trajectory alignment, with well-phrased prompts improving ADE by up to 5.1%. We use this analysis to discuss what makes a "good" instruction for the OpenEMMA framework. We release the evaluation prompts and scripts to establish a reproducible baseline for instruction-aware planning. GitHub: this https URL</li>
<li><strong>摘要：</strong>基于指令的驾驶，即乘客语言指导轨迹规划，要求车辆在运动前理解意图。然而，大多数先前的指令遵循规划器依赖于模拟或固定命令词汇，限制了现实世界的概括。 doScenes 是第一个将自由格式指令（具有参考性）与 nuScenes 地面实况运动链接起来的现实世界数据集，可实现指令条件规划。在这项工作中，我们将 OpenEMMA（一种基于开源 MLLM 的端到端驾驶框架，摄取前置摄像头视图和自我状态并输出 10 步速度曲率轨迹）应用于此设置，在 doScenes 上呈现可重复的指令条件基线，并研究人类指令提示对预测驾驶行为的影响。我们将 doScenes 指令作为乘客式提示集成到 OpenEMMA 的视觉语言界面中，从而在轨迹生成之前实现语言调节。使用 ADE 对 849 个带注释的场景进行评估，我们观察到指令调节通过防止极端基线故障显着提高了鲁棒性，使平均 ADE 降低了 98.7%。当此类异常值被移除后，指令仍然会影响轨迹对齐，措辞恰当的提示可将 ADE 提高高达 5.1%。我们使用此分析来讨论什么才是 OpenEMMA 框架的“好”指令。我们发布评估提示和脚本，为指令感知规划建立可重复的基线。 GitHub：此 https URL</li>
</ul>

<h3>Title: DiMo: Discrete Diffusion Modeling for Motion Generation and Understanding</h3>
<ul>
<li><strong>Authors: </strong>Ning Zhang, Zhengyu Li, Kwong Weng Loh, Mingxi Xu, Qi Wang, Zhengyu Wen, Xiaoyu He, Wei Zhao, Kehong Gong, Mingyuan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04188">https://arxiv.org/abs/2602.04188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04188">https://arxiv.org/pdf/2602.04188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04188]] DiMo: Discrete Diffusion Modeling for Motion Generation and Understanding(https://arxiv.org/abs/2602.04188)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Prior masked modeling motion generation methods predominantly study text-to-motion. We present DiMo, a discrete diffusion-style framework, which extends masked modeling to bidirectional text--motion understanding and generation. Unlike GPT-style autoregressive approaches that tokenize motion and decode sequentially, DiMo performs iterative masked token refinement, unifying Text-to-Motion (T2M), Motion-to-Text (M2T), and text-free Motion-to-Motion (M2M) within a single model. This decoding paradigm naturally enables a quality-latency trade-off at inference via the number of refinement this http URL further improve motion token fidelity with residual vector quantization (RVQ) and enhance alignment and controllability with Group Relative Policy Optimization (GRPO). Experiments on HumanML3D and KIT-ML show strong motion quality and competitive bidirectional understanding under a unified framework. In addition, we demonstrate model ability in text-free motion completion, text-guided motion prediction and motion caption correction without architectural this http URL qualitative results are available on our project page: this https URL.</li>
<li><strong>摘要：</strong>先前的掩蔽建模运动生成方法主要研究文本到运动。我们提出了 DiMo，一种离散扩散式框架，它将掩模建模扩展到双向文本运动理解和生成。与按顺序标记运动和解码的 GPT 式自回归方法不同，DiMo 执行迭代屏蔽标记细化，将文本到运动 (T2M)、运动到文本 (M2T) 和无文本运动到运动 (M2M) 统一在单个模型中。这种解码范例自然可以通过此 http URL 的细化次数实现推理时的质量延迟权衡，通过残差矢量量化 (RVQ) 进一步提高运动令牌保真度，并通过组相对策略优化 (GRPO) 增强对齐和可控性。 HumanML3D 和 KIT-ML 上的实验显示出统一框架下强大的运动质量和有竞争力的双向理解。此外，我们还展示了无文本运动完成、文本引导运动预测和运动字幕校正方面的模型能力，无需架构此 http URL 定性结果可在我们的项目页面上找到：此 https URL。</li>
</ul>

<h3>Title: Continuous Degradation Modeling via Latent Flow Matching for Real-World Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Hyeonjae Kim, Dongjin Kim, Eugene Jin, Tae Hyun Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04193">https://arxiv.org/abs/2602.04193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04193">https://arxiv.org/pdf/2602.04193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04193]] Continuous Degradation Modeling via Latent Flow Matching for Real-World Super-Resolution(https://arxiv.org/abs/2602.04193)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>While deep learning-based super-resolution (SR) methods have shown impressive outcomes with synthetic degradation scenarios such as bicubic downsampling, they frequently struggle to perform well on real-world images that feature complex, nonlinear degradations like noise, blur, and compression artifacts. Recent efforts to address this issue have involved the painstaking compilation of real low-resolution (LR) and high-resolution (HR) image pairs, usually limited to several specific downscaling factors. To address these challenges, our work introduces a novel framework capable of synthesizing authentic LR images from a single HR image by leveraging the latent degradation space with flow matching. Our approach generates LR images with realistic artifacts at unseen degradation levels, which facilitates the creation of large-scale, real-world SR training datasets. Comprehensive quantitative and qualitative assessments verify that our synthetic LR images accurately replicate real-world degradations. Furthermore, both traditional and arbitrary-scale SR models trained using our datasets consistently yield much better HR outcomes.</li>
<li><strong>摘要：</strong>虽然基于深度学习的超分辨率 (SR) 方法在双三次下采样等合成退化场景中显示出令人印象深刻的结果，但它们经常难以在具有复杂、非线性退化（如噪声、模糊和压缩伪影）的现实世界图像上表现良好。最近解决这个问题的努力涉及真实的低分辨率（LR）和高分辨率（HR）图像对的艰苦编译，通常仅限于几个特定的​​缩小因素。为了应对这些挑战，我们的工作引入了一种新颖的框架，能够通过利用流匹配的潜在退化空间从单个 HR 图像合成真实的 LR 图像。我们的方法生成具有看不见的退化水平的真实伪影的 LR 图像，这有助于创建大规模的、真实世界的 SR 训练数据集。全面的定量和定性评估验证了我们的合成 LR 图像准确地复制了现实世界的退化情况。此外，使用我们的数据集训练的传统和任意规模的 SR 模型始终能产生更好的 HR 结果。</li>
</ul>

<h3>Title: From Sparse Sensors to Continuous Fields: STRIDE for Spatiotemporal Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Yanjie Tong, Peng Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04201">https://arxiv.org/abs/2602.04201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04201">https://arxiv.org/pdf/2602.04201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04201]] From Sparse Sensors to Continuous Fields: STRIDE for Spatiotemporal Reconstruction(https://arxiv.org/abs/2602.04201)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Reconstructing high-dimensional spatiotemporal fields from sparse point-sensor measurements is a central challenge in learning parametric PDE dynamics. Existing approaches often struggle to generalize across trajectories and parameter settings, or rely on discretization-tied decoders that do not naturally transfer across meshes and resolutions. We propose STRIDE (Spatio-Temporal Recurrent Implicit DEcoder), a two-stage framework that maps a short window of sensor measurements to a latent state with a temporal encoder and reconstructs the field at arbitrary query locations with a modulated implicit neural representation (INR) decoder. Using the Fourier Multi-Component and Multi-Layer Neural Network (FMMNN) as the INR backbone improves representation of complex spatial fields and yields more stable optimization than sine-based INRs. We provide a conditional theoretical justification: under stable delay observability of point measurements on a low-dimensional parametric invariant set, the reconstruction operator factors through a finite-dimensional embedding, making STRIDE-type architectures natural approximators. Experiments on four challenging benchmarks spanning chaotic dynamics and wave propagation show that STRIDE outperforms strong baselines under extremely sparse sensing, supports super-resolution, and remains robust to noise.</li>
<li><strong>摘要：</strong>从稀疏点传感器测量重建高维时空场是学习参数偏微分方程动力学的核心挑战。现有的方法通常很难在轨迹和参数设置之间进行泛化，或者依赖于离散化相关的解码器，而这些解码器不能自然地跨网格和分辨率进行传输。我们提出了 STRIDE（时空循环隐式解码器），这是一个两阶段框架，它使用时间编码器将传感器测量的短窗口映射到潜在状态，并使用调制隐式神经表示（INR）解码器在任意查询位置重建字段。使用傅里叶多分量多层神经网络 (FMMNN) 作为 INR 主干，可以改善复杂空间场的表示，并产生比基于正弦的 INR 更稳定的优化。我们提供了一个有条件的理论论证：在低维参数不变集上点测量的稳定延迟可观测性下，重建算子通过有限维嵌入进行因子分解，使 STRIDE 型架构成为自然逼近器。对涵盖混沌动力学和波传播的四个具有挑战性的基准进行的实验表明，STRIDE 在极其稀疏的传感下优于强基线，支持超分辨率，并且对噪声保持鲁棒性。</li>
</ul>

<h3>Title: VTok: A Unified Video Tokenizer with Decoupled Spatial-Temporal Latents</h3>
<ul>
<li><strong>Authors: </strong>Feng Wang, Yichun Shi, Ceyuan Yang, Qiushan Guo, Jingxiang Sun, Alan Yuille, Peng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04202">https://arxiv.org/abs/2602.04202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04202">https://arxiv.org/pdf/2602.04202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04202]] VTok: A Unified Video Tokenizer with Decoupled Spatial-Temporal Latents(https://arxiv.org/abs/2602.04202)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This work presents VTok, a unified video tokenization framework that can be used for both generation and understanding tasks. Unlike the leading vision-language systems that tokenize videos through a naive frame-sampling strategy, we propose to decouple the spatial and temporal representations of videos by retaining the spatial features of a single key frame while encoding each subsequent frame into a single residual token, achieving compact yet expressive video tokenization. Our experiments suggest that VTok effectively reduces the complexity of video representation from the product of frame count and per-frame token count to their sum, while the residual tokens sufficiently capture viewpoint and motion changes relative to the key frame. Extensive evaluations demonstrate the efficacy and efficiency of VTok: it achieves notably higher performance on a range of video understanding and text-to-video generation benchmarks compared with baselines using naive tokenization, all with shorter token sequences per video (e.g., 3.4% higher accuracy on our TV-Align benchmark and 1.9% higher VBench score). Remarkably, VTok produces more coherent motion and stronger guidance following in text-to-video generation, owing to its more consistent temporal encoding. We hope VTok can serve as a standardized video tokenization paradigm for future research in video understanding and generation.</li>
<li><strong>摘要：</strong>这项工作提出了 VTok，一个统一的视频标记化框架，可用于生成和理解任务。与通过简单的帧采样策略对视频进行标记的领先视觉语言系统不同，我们建议通过保留单个关键帧的空间特征，同时将每个后续帧编码为单个残差标记来解耦视频的空间和时间表示，从而实现紧凑而富有表现力的视频标记化。我们的实验表明，VTok 有效地降低了视频表示的复杂性，从帧计数和每帧令牌计数的乘积到它们的总和，而剩余令牌充分捕获相对于关键帧的视点和运动变化。广泛的评估证明了 VTok 的功效和效率：与使用朴素标记化的基线相比，它在一系列视频理解和文本到视频生成基准测试中实现了显着更高的性能，并且每个视频的标记序列更短（例如，我们的 TV-Align 基准测试的准确度提高了 3.4%，VBench 得分提高了 1.9%）。值得注意的是，由于其更一致的时间编码，VTok 在文本到视频生成过程中产生了更连贯的运动和更强的指导。我们希望 VTok 能够作为未来视频理解和生成研究的标准化视频标记化范式。</li>
</ul>

<h3>Title: Adaptive 1D Video Diffusion Autoencoder</h3>
<ul>
<li><strong>Authors: </strong>Yao Teng, Minxuan Lin, Xian Liu, Shuai Wang, Xiao Yang, Xihui Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04220">https://arxiv.org/abs/2602.04220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04220">https://arxiv.org/pdf/2602.04220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04220]] Adaptive 1D Video Diffusion Autoencoder(https://arxiv.org/abs/2602.04220)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent video generation models largely rely on video autoencoders that compress pixel-space videos into latent representations. However, existing video autoencoders suffer from three major limitations: (1) fixed-rate compression that wastes tokens on simple videos, (2) inflexible CNN architectures that prevent variable-length latent modeling, and (3) deterministic decoders that struggle to recover appropriate details from compressed latents. To address these issues, we propose One-Dimensional Diffusion Video Autoencoder (One-DVA), a transformer-based framework for adaptive 1D encoding and diffusion-based decoding. The encoder employs query-based vision transformers to extract spatiotemporal features and produce latent representations, while a variable-length dropout mechanism dynamically adjusts the latent length. The decoder is a pixel-space diffusion transformer that reconstructs videos with the latents as input conditions. With a two-stage training strategy, One-DVA achieves performance comparable to 3D-CNN VAEs on reconstruction metrics at identical compression ratios. More importantly, it supports adaptive compression and thus can achieve higher compression ratios. To better support downstream latent generation, we further regularize the One-DVA latent distribution for generative modeling and fine-tune its decoder to mitigate artifacts caused by the generation process.</li>
<li><strong>摘要：</strong>最近的视频生成模型很大程度上依赖于视频自动编码器，将像素空间视频压缩为潜在表示。然而，现有的视频自动编码器面临三个主要限制：(1) 固定速率压缩会浪费简单视频上的标记，(2) 不灵活的 CNN 架构会阻碍可变长度潜在模型的建模，(3) 确定性解码器很难从压缩的潜在模型中恢复适当的细节。为了解决这些问题，我们提出了一维扩散视频自动编码器（One-DVA），这是一种基于变压器的框架，用于自适应一维编码和基于扩散的解码。编码器采用基于查询的视觉变换器来提取时空特征并产生潜在表示，而可变长度丢失机制动态调整潜在长度。解码器是一个像素空间扩散变换器，它以潜在特征作为输入条件来重建视频。通过两阶段训练策略，One-DVA 在相同压缩比的重建指标上实现了与 3D-CNN VAE 相当的性能。更重要的是，它支持自适应压缩，从而可以获得更高的压缩比。为了更好地支持下游潜在生成，我们进一步规范生成建模的 One-DVA 潜在分布，并微调其解码器以减轻生成过程造成的伪影。</li>
</ul>

<h3>Title: KVSmooth: Mitigating Hallucination in Multi-modal Large Language Models through Key-Value Smoothing</h3>
<ul>
<li><strong>Authors: </strong>Siyu Jiang, Feiyang Chen, Xiaojin Zhang, Kun He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04268">https://arxiv.org/abs/2602.04268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04268">https://arxiv.org/pdf/2602.04268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04268]] KVSmooth: Mitigating Hallucination in Multi-modal Large Language Models through Key-Value Smoothing(https://arxiv.org/abs/2602.04268)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite the significant progress of Multimodal Large Language Models (MLLMs) across diverse tasks, hallucination -- corresponding to the generation of visually inconsistent objects, attributes, or relations -- remains a major obstacle to their reliable deployment. Unlike pure language models, MLLMs must ground their generation process in visual inputs. However, existing models often suffer from semantic drift during decoding, causing outputs to diverge from visual facts as the sequence length increases. To address this issue, we propose KVSmooth, a training-free and plug-and-play method that mitigates hallucination by performing attention-entropy-guided adaptive smoothing on hidden states. Specifically, KVSmooth applies an exponential moving average (EMA) to both keys and values in the KV-Cache, while dynamically quantifying the sink degree of each token through the entropy of its attention distribution to adaptively adjust the smoothing strength. Unlike computationally expensive retraining or contrastive decoding methods, KVSmooth operates efficiently during inference without additional training or model modification. Extensive experiments demonstrate that KVSmooth significantly reduces hallucination ($\mathit{CHAIR}_{S}$ from $41.8 \rightarrow 18.2$) while improving overall performance ($F_1$ score from $77.5 \rightarrow 79.2$), achieving higher precision and recall simultaneously. In contrast, prior methods often improve one at the expense of the other, validating the effectiveness and generality of our approach.</li>
<li><strong>摘要：</strong>尽管多模态大语言模型（MLLM）在不同的任务中取得了重大进展，但幻觉（对应于视觉上不一致的对象、属性或关系的生成）仍然是其可靠部署的主要障碍。与纯语言模型不同，MLLM 的生成过程必须基于视觉输入。然而，现有模型在解码过程中经常会出现语义漂移，导致随着序列长度的增加，输出与视觉事实出现偏差。为了解决这个问题，我们提出了 KVSmooth，这是一种免训练、即插即用的方法，通过对隐藏状态执行注意力熵引导的自适应平滑来减轻幻觉。具体来说，KVSmooth 对 KV-Cache 中的键和值应用指数移动平均（EMA），同时通过其注意力分布的熵动态量化每个令牌的下沉程度，以自适应调整平滑强度。与计算成本高昂的重新训练或对比解码方法不同，KVSmooth 在推理过程中高效运行，无需额外训练或模型修改。大量实验表明，KVSmooth 显着减少了幻觉（$\mathit{CHAIR}_{S}$ 从 $41.8 \rightarrow 18.2$ 起），同时提高了整体性能（$F_1$ 得分从 $77.5 \rightarrow 79.2$ 起），同时实现了更高的精确度和召回率。相比之下，现有方法通常会以牺牲另一种方法为代价来改进一种方法，从而验证了我们方法的有效性和通用性。</li>
</ul>

<h3>Title: SkeletonGaussian: Editable 4D Generation through Gaussian Skeletonization</h3>
<ul>
<li><strong>Authors: </strong>Lifan Wu, Ruijie Zhu, Yubo Ai, Tianzhu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04271">https://arxiv.org/abs/2602.04271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04271">https://arxiv.org/pdf/2602.04271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04271]] SkeletonGaussian: Editable 4D Generation through Gaussian Skeletonization(https://arxiv.org/abs/2602.04271)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>4D generation has made remarkable progress in synthesizing dynamic 3D objects from input text, images, or videos. However, existing methods often represent motion as an implicit deformation field, which limits direct control and editability. To address this issue, we propose SkeletonGaussian, a novel framework for generating editable dynamic 3D Gaussians from monocular video input. Our approach introduces a hierarchical articulated representation that decomposes motion into sparse rigid motion explicitly driven by a skeleton and fine-grained non-rigid motion. Concretely, we extract a robust skeleton and drive rigid motion via linear blend skinning, followed by a hexplane-based refinement for non-rigid deformations, enhancing interpretability and editability. Experimental results demonstrate that SkeletonGaussian surpasses existing methods in generation quality while enabling intuitive motion editing, establishing a new paradigm for editable 4D generation. Project page: this https URL</li>
<li><strong>摘要：</strong>4D 生成在从输入文本、图像或视频合成动态 3D 对象方面取得了显着进展。然而，现有方法通常将运动表示为隐式变形场，这限制了直接控制和可编辑性。为了解决这个问题，我们提出了 SkeletonGaussian，这是一种从单目视频输入生成可编辑动态 3D 高斯的新颖框架。我们的方法引入了分层铰接表示，将运动分解为由骨架显式驱动的稀疏刚性运动和细粒度非刚性运动。具体来说，我们提取稳健的骨架并通过线性混合蒙皮驱动刚性运动，然后对非刚性变形进行基于六角平面的细化，从而增强可解释性和可编辑性。实验结果表明，SkeletonGaussian 在生成质量上超越了现有方法，同时实现了直观的运动编辑，为可编辑 4D 生成建立了新的范例。项目页面：此 https URL</li>
</ul>

<h3>Title: Multi Objective Design Optimization of Non Pneumatic Passenger Car Tires Using Finite Element Modeling, Machine Learning, and Particle swarm Optimization and Bayesian Optimization Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Priyankkumar Dhrangdhariya, Soumyadipta Maiti, Venkataramana Runkana</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04277">https://arxiv.org/abs/2602.04277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04277">https://arxiv.org/pdf/2602.04277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04277]] Multi Objective Design Optimization of Non Pneumatic Passenger Car Tires Using Finite Element Modeling, Machine Learning, and Particle swarm Optimization and Bayesian Optimization Algorithms(https://arxiv.org/abs/2602.04277)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Non Pneumatic tires offer a promising alternative to pneumatic tires. However, their discontinuous spoke structures present challenges in stiffness tuning, durability, and high speed vibration. This study introduces an integrated generative design and machine learning driven framework to optimize UPTIS type spoke geometries for passenger vehicles. Upper and lower spoke profiles were parameterized using high order polynomial representations, enabling the creation of approximately 250 generative designs through PCHIP based geometric variation. Machine learning models like KRR for stiffness and XGBoost for durability and vibration achieved strong predictive accuracy, reducing the reliance on computationally intensive FEM simulations. Optimization using Particle Swarm Optimization and Bayesian Optimization further enabled extensive performance refinement. The resulting designs demonstrate 53% stiffness tunability, up to 50% durability improvement, and 43% reduction in vibration compared to the baseline. PSO provided fast, targeted convergence, while Bayesian Optimization effectively explored multi objective tradeoffs. Overall, the proposed framework enables systematic development of high performance, next generation UPTIS spoke structures.</li>
<li><strong>摘要：</strong>非充气轮胎为充气轮胎提供了一种有前景的替代品。然而，它们的不连续辐条结构在刚度调节、耐用性和高速振动方面提出了挑战。本研究引入了集成生成设计和机器学习驱动框架，以优化乘用车的 UPTIS 型辐条几何形状。使用高阶多项式表示对上下辐条轮廓进行参数化，从而能够通过基于 PCHIP 的几何变化创建大约 250 种生成设计。用于刚度的 KRR 和用于耐用性和振动的 XGBoost 等机器学习模型实现了强大的预测精度，减少了对计算密集型 FEM 模拟的依赖。使用粒子群优化和贝叶斯优化的优化进一步实现了广泛的性能改进。与基线相比，最终的设计显示出 53% 的刚度可调性、高达 50% 的耐用性改进以及 43% 的振动减少。 PSO 提供快速、有针对性的收敛，而贝叶斯优化有效地探索了多目标权衡。总体而言，所提出的框架能够系统地开发高性能、下一代 UPTIS 辐条结构。</li>
</ul>

<h3>Title: Disentangling Causal Importance from Emergent Structure in Multi-Expert Orchestration</h3>
<ul>
<li><strong>Authors: </strong>Sudipto Ghosh, Sujoy Nath, Sunny Manchanda, Tanmoy Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04291">https://arxiv.org/abs/2602.04291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04291">https://arxiv.org/pdf/2602.04291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04291]] Disentangling Causal Importance from Emergent Structure in Multi-Expert Orchestration(https://arxiv.org/abs/2602.04291)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multi-expert systems, where multiple Large Language Models (LLMs) collaborate to solve complex tasks, are increasingly adopted for high-performance reasoning and generation. However, the orchestration policies governing expert interaction and sequencing remain largely opaque. We introduce INFORM, an interpretability analysis that treats orchestration as an explicit, analyzable computation, enabling the decoupling of expert interaction structure, execution order, and causal attribution. We use INFORM to evaluate an orchestrator on GSM8K, HumanEval, and MMLU using a homogeneous consortium of ten instruction-tuned experts drawn from LLaMA-3.1 8B, Qwen-3 8B, and DeepSeek-R1 8B, with controlled decoding-temperature variation, and a secondary heterogeneous consortium spanning 1B-7B parameter models. Across tasks, routing dominance is a poor proxy for functional necessity. We reveal a divergence between relational importance, captured by routing mass and interaction topology, and intrinsic importance, measured via gradient-based causal attribution: frequently selected experts often act as interaction hubs with limited causal influence, while sparsely routed experts can be structurally critical. Orchestration behaviors emerge asynchronously, with expert centralization preceding stable routing confidence and expert ordering remaining non-deterministic. Targeted ablations show that masking intrinsically important experts induces disproportionate collapse in interaction structure compared to masking frequent peers, confirming that INFORM exposes causal and structural dependencies beyond accuracy metrics alone.</li>
<li><strong>摘要：</strong>多个大型语言模型 (LLM) 协作解决复杂任务的多专家系统越来越多地用于高性能推理和生成。然而，管理专家交互和排序的编排策略在很大程度上仍然不透明。我们引入了 INFORM，一种可解释性分析，它将编排视为显式的、可分析的计算，从而实现专家交互结构、执行顺序和因果归因的解耦。我们使用 INFORM 来评估 GSM8K、HumanEval 和 MMLU 上的协调器，使用由来自 LLaMA-3.1 8B、Qwen-3 8B 和 DeepSeek-R1 8B 的 10 名指令调整专家组成的同构联盟，具有受控的解码温度变化，以及跨越 1B-7B 参数模型的辅助异构联盟。在不同的任务中，路由优势并不能很好地体现功能的必要性。我们揭示了通过路由质量和交互拓扑捕获的关系重要性与通过基于梯度的因果归因测量的内在重要性之间的差异：频繁选择的专家通常充当因果影响有限的交互中心，而稀疏路由的专家可能在结构上至关重要。编排行为是异步出现的，专家集中化先于稳定的路由信心，而专家排序仍然是不确定的。有针对性的消融表明，与掩盖频繁的同行相比，掩盖本质上重要的专家会导致交互结构不成比例的崩溃，这证实了 INFORM 暴露了超出准确性指标的因果和结构依赖性。</li>
</ul>

<h3>Title: UnMaskFork: Test-Time Scaling for Masked Diffusion via Deterministic Action Branching</h3>
<ul>
<li><strong>Authors: </strong>Kou Misaki, Takuya Akiba</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04344">https://arxiv.org/abs/2602.04344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04344">https://arxiv.org/pdf/2602.04344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04344]] UnMaskFork: Test-Time Scaling for Masked Diffusion via Deterministic Action Branching(https://arxiv.org/abs/2602.04344)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Test-time scaling strategies have effectively leveraged inference-time compute to enhance the reasoning abilities of Autoregressive Large Language Models. In this work, we demonstrate that Masked Diffusion Language Models (MDLMs) are inherently amenable to advanced search strategies, owing to their iterative and non-autoregressive generation process. To leverage this, we propose UnMaskFork (UMF), a framework that formulates the unmasking trajectory as a search tree and employs Monte Carlo Tree Search to optimize the generation path. In contrast to standard scaling methods relying on stochastic sampling, UMF explores the search space through deterministic partial unmasking actions performed by multiple MDLMs. Our empirical evaluation demonstrates that UMF consistently outperforms existing test-time scaling baselines on complex coding benchmarks, while also exhibiting strong scalability on mathematical reasoning tasks.</li>
<li><strong>摘要：</strong>测试时间扩展策略有效地利用推理时间计算来增强自回归大型语言模型的推理能力。在这项工作中，我们证明了掩蔽扩散语言模型（MDLM）由于其迭代和非自回归生成过程，本质上适合高级搜索策略。为了利用这一点，我们提出了 UnMaskFork (UMF)，这是一个框架，它将解锁轨迹制定为搜索树，并采用蒙特卡罗树搜索来优化生成路径。与依赖随机采样的标准缩放方法相比，UMF 通过多个 MDLM 执行的确定性部分揭露操作来探索搜索空间。我们的实证评估表明，UMF 在复杂编码基准上始终优于现有的测试时间扩展基线，同时在数学推理任务上也表现出强大的可扩展性。</li>
</ul>

<h3>Title: SparVAR: Exploring Sparsity in Visual AutoRegressive Modeling for Training-Free Acceleration</h3>
<ul>
<li><strong>Authors: </strong>Zekun Li, Ning Wang, Tongxin Bai, Changwang Mei, Peisong Wang, Shuang Qiu, Jian Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04361">https://arxiv.org/abs/2602.04361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04361">https://arxiv.org/pdf/2602.04361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04361]] SparVAR: Exploring Sparsity in Visual AutoRegressive Modeling for Training-Free Acceleration(https://arxiv.org/abs/2602.04361)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Visual AutoRegressive (VAR) modeling has garnered significant attention for its innovative next-scale prediction paradigm. However, mainstream VAR paradigms attend to all tokens across historical scales at each autoregressive step. As the next scale resolution grows, the computational complexity of attention increases quartically with resolution, causing substantial latency. Prior accelerations often skip high-resolution scales, which speeds up inference but discards high-frequency details and harms image quality. To address these problems, we present SparVAR, a training-free acceleration framework that exploits three properties of VAR attention: (i) strong attention sinks, (ii) cross-scale activation similarity, and (iii) pronounced locality. Specifically, we dynamically predict the sparse attention pattern of later high-resolution scales from a sparse decision scale, and construct scale self-similar sparse attention via an efficient index-mapping mechanism, enabling high-efficiency sparse attention computation at large scales. Furthermore, we propose cross-scale local sparse attention and implement an efficient block-wise sparse kernel, which achieves $\mathbf{> 5\times}$ faster forward speed than FlashAttention. Extensive experiments demonstrate that the proposed SparseVAR can reduce the generation time of an 8B model producing $1024\times1024$ high-resolution images to the 1s, without skipping the last scales. Compared with the VAR baseline accelerated by FlashAttention, our method achieves a $\mathbf{1.57\times}$ speed-up while preserving almost all high-frequency details. When combined with existing scale-skipping strategies, SparseVAR attains up to a $\mathbf{2.28\times}$ acceleration, while maintaining competitive visual generation quality. Code is available at this https URL.</li>
<li><strong>摘要：</strong>视觉自回归（VAR）建模因其创新的下一代预测范式而受到广泛关注。然而，主流 VAR 范式在每个自回归步骤中都考虑到历史尺度上的所有代币。随着下一个尺度分辨率的增长，注意力的计算复杂性随着分辨率的增加呈四倍增长，从而导致大量的延迟。先前的加速通常会跳过高分辨率尺度，这会加快推理速度，但会丢弃高频细节并损害图像质量。为了解决这些问题，我们提出了 SparVAR，这是一种无需训练的加速框架，它利用了 VAR 注意力的三个属性：（i）强大的注意力池，（ii）跨尺度激活相似性，以及（iii）明显的局部性。具体来说，我们从稀疏决策尺度动态预测后续高分辨率尺度的稀疏注意力模式，并通过有效的索引映射机制构造尺度自相似稀疏注意力，从而实现大规模的高效稀疏注意力计算。此外，我们提出了跨尺度局部稀疏注意力并实现了高效的分块稀疏内核，其前进速度比 FlashAttention 快 $\mathbf{> 5\times}$。大量实验表明，所提出的 SparseVAR 可以将 8B 模型生成 $1024\times1024$ 高分辨率图像的生成时间减少到 1 秒，而无需跳过最后一个尺度。与 FlashAttention 加速的 VAR 基线相比，我们的方法实现了 $\mathbf{1.57\times}$ 加速，同时保留了几乎所有高频细节。当与现有的尺度跳跃策略相结合时，SparseVAR 可以获得高达 $\mathbf{2.28\times}$ 的加速，同时保持具有竞争力的视觉生成质量。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Beyond KL Divergence: Policy Optimization with Flexible Bregman Divergences for LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Rui Yuan, Mykola Khandoga, Vinay Kumar Sankarapu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04380">https://arxiv.org/abs/2602.04380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04380">https://arxiv.org/pdf/2602.04380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04380]] Beyond KL Divergence: Policy Optimization with Flexible Bregman Divergences for LLM Reasoning(https://arxiv.org/abs/2602.04380)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Policy optimization methods like Group Relative Policy Optimization (GRPO) and its variants have achieved strong results on mathematical reasoning and code generation tasks. Despite extensive exploration of reward processing strategies and training dynamics, all existing group-based methods exclusively use KL divergence for policy regularization, leaving the choice of divergence function unexplored. We introduce Group-Based Mirror Policy Optimization (GBMPO), a framework that extends group-based policy optimization to flexible Bregman divergences, including hand-designed alternatives (L2 in probability space) and learned neural mirror maps. On GSM8K mathematical reasoning, hand-designed ProbL2-GRPO achieves 86.7% accuracy, improving +5.5 points over the Dr. GRPO baseline. On MBPP code generation, neural mirror maps reach 60.1-60.8% pass@1, with random initialization already capturing most of the benefit. While evolutionary strategies meta-learning provides marginal accuracy improvements, its primary value lies in variance reduction ($\pm$0.2 versus $\pm$0.6) and efficiency gains (15% shorter responses on MBPP), suggesting that random initialization of neural mirror maps is sufficient for most practical applications. These results establish divergence choice as a critical, previously unexplored design dimension in group-based policy optimization for LLM reasoning.</li>
<li><strong>摘要：</strong>组相对策略优化（GRPO）及其变体等策略优化方法在数学推理和代码生成任务上取得了很好的成果。尽管对奖励处理策略和训练动态进行了广泛的探索，但所有现有的基于群体的方法都专门使用 KL 散度进行策略正则化，而散度函数的选择尚未得到探索。我们引入了基于组的镜像策略优化（GBMPO），这是一个将基于组的策略优化扩展到灵活的 Bregman 散度的框架，包括手工设计的替代方案（概率空间中的 L2）和学习的神经镜像映射。在 GSM8K 数学推理上，手工设计的 ProbL2-GRPO 达到了 86.7% 的准确率，比 Dr. GRPO 基线提高了 5.5 分。在 MBPP 代码生成中，神经镜像映射达到 60.1-60.8% pass@1，随机初始化已经获得了大部分好处。虽然进化策略元学习提供了边际精度的提高，但其主要价值在于方差减少（$\pm$0.2 vs $\pm$0.6）和效率增益（MBPP 上的响应缩短了 15%），这表明神经镜像映射的随机初始化足以满足大多数实际应用。这些结果将发散选择确定为 LLM 推理的基于组的策略优化中关键的、先前未探索的设计维度。</li>
</ul>

<h3>Title: Theory of Speciation Transitions in Diffusion Models with General Class Structure</h3>
<ul>
<li><strong>Authors: </strong>Beatrice Achilli, Marco Benedetti, Giulio Biroli, Marc Mézard</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.dis-nn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04404">https://arxiv.org/abs/2602.04404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04404">https://arxiv.org/pdf/2602.04404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04404]] Theory of Speciation Transitions in Diffusion Models with General Class Structure(https://arxiv.org/abs/2602.04404)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion Models generate data by reversing a stochastic diffusion process, progressively transforming noise into structured samples drawn from a target distribution. Recent theoretical work has shown that this backward dynamics can undergo sharp qualitative transitions, known as speciation transitions, during which trajectories become dynamically committed to data classes. Existing theoretical analyses, however, are limited to settings where classes are identifiable through first moments, such as mixtures of Gaussians with well-separated means. In this work, we develop a general theory of speciation in diffusion models that applies to arbitrary target distributions admitting well-defined classes. We formalize the notion of class structure through Bayes classification and characterize speciation times in terms of free-entropy difference between classes. This criterion recovers known results in previously studied Gaussian-mixture models, while extending to situations in which classes are not distinguishable by first moments and may instead differ through higher-order or collective features. Our framework also accommodates multiple classes and predicts the existence of successive speciation times associated with increasingly fine-grained class commitment. We illustrate the theory on two analytically tractable examples: mixtures of one-dimensional Ising models at different temperatures and mixtures of zero-mean Gaussians with distinct covariance structures. In the Ising case, we obtain explicit expressions for speciation times by mapping the problem onto a random-field Ising model and solving it via the replica method. Our results provide a unified and broadly applicable description of speciation transitions in diffusion-based generative models.</li>
<li><strong>摘要：</strong>扩散模型通过反转随机扩散过程来生成数据，逐步将噪声转换为从目标分布中提取的结构化样本。最近的理论工作表明，这种向后动态可以经历急剧的定性转变，称为物种形成转变，在此期间轨迹动态地致力于数据类。然而，现有的理论分析仅限于可通过一阶矩识别类别的设置，例如具有良好分离均值的高斯混合。在这项工作中，我们开发了扩散模型中物种形成的一般理论，该理论适用于允许明确定义的类别的任意目标分布。我们通过贝叶斯分类形式化了类结构的概念，并根据类之间的自由熵差来表征物种形成时间。该标准恢复了先前研究的高斯混合模型中的已知结果，同时扩展到无法通过一阶矩区分类别并且可能通过高阶或集体特征进行区分的情况。我们的框架还适应多个类别，并预测与日益细粒度的类别承诺相关的连续物种形成时间的存在。我们用两个分析上易于处理的例子来说明该理论：不同温度下的一维伊辛模型的混合和具有不同协方差结构的零均值高斯模型的混合。在伊辛案例中，我们通过将问题映射到随机场伊辛模型并通过复制方法求解来获得物种形成时间的显式表达式。我们的结果为基于扩散的生成模型中的形态转变提供了统一且广泛适用的描述。</li>
</ul>

<h3>Title: LCUDiff: Latent Capacity Upgrade Diffusion for Faithful Human Body Restoration</h3>
<ul>
<li><strong>Authors: </strong>Jue Gong, Zihan Zhou, Jingkai Wang, Shu Li, Libo Liu, Jianliang Lan, Yulun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04406">https://arxiv.org/abs/2602.04406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04406">https://arxiv.org/pdf/2602.04406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04406]] LCUDiff: Latent Capacity Upgrade Diffusion for Faithful Human Body Restoration(https://arxiv.org/abs/2602.04406)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Existing methods for restoring degraded human-centric images often struggle with insufficient fidelity, particularly in human body restoration (HBR). Recent diffusion-based restoration methods commonly adapt pre-trained text-to-image diffusion models, where the variational autoencoder (VAE) can significantly bottleneck restoration fidelity. We propose LCUDiff, a stable one-step framework that upgrades a pre-trained latent diffusion model from the 4-channel latent space to the 16-channel latent space. For VAE fine-tuning, channel splitting distillation (CSD) is used to keep the first four channels aligned with pre-trained priors while allocating the additional channels to effectively encode high-frequency details. We further design prior-preserving adaptation (PPA) to smoothly bridge the mismatch between 4-channel diffusion backbones and the higher-dimensional 16-channel latent. In addition, we propose a decoder router (DeR) for per-sample decoder routing using restoration-quality score annotations, which improves visual quality across diverse conditions. Experiments on synthetic and real-world datasets show competitive results with higher fidelity and fewer artifacts under mild degradations, while preserving one-step efficiency. The code and model will be at this https URL.</li>
<li><strong>摘要：</strong>现有的用于恢复退化的以人为中心的图像的方法常常因保真度不足而苦苦挣扎，特别是在人体恢复（HBR）方面。最近基于扩散的恢复方法通常采用预先训练的文本到图像扩散模型，其中变分自动编码器（VAE）会严重阻碍恢复保真度。我们提出了 LCUDiff，一种稳定的一步框架，将预训练的潜在扩散模型从 ​​4 通道潜在空间升级到 16 通道潜在空间。对于 VAE 微调，通道分割蒸馏 (CSD) 用于保持前四个通道与预训练先验保持一致，同时分配附加通道以有效编码高频细节。我们进一步设计了先验保留自适应（PPA），以平滑地弥合 4 通道扩散主干和高维 16 通道潜在网络之间的不匹配。此外，我们提出了一种解码器路由器（DeR），用于使用恢复质量分数注释进行每个样本解码器路由，这提高了不同条件下的视觉质量。对合成数据集和真实世界数据集的实验显示出具有竞争力的结果，在轻度退化的情况下具有更高的保真度和更少的伪影，同时保持一步效率。代码和模型将位于此 https URL。</li>
</ul>

<h3>Title: Mixture of Masters: Sparse Chess Language Models with Player Routing</h3>
<ul>
<li><strong>Authors: </strong>Giacomo Frisoni, Lorenzo Molfetta, Davide Freddi, Gianluca Moro</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04447">https://arxiv.org/abs/2602.04447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04447">https://arxiv.org/pdf/2602.04447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04447]] Mixture of Masters: Sparse Chess Language Models with Player Routing(https://arxiv.org/abs/2602.04447)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Modern chess language models are dense transformers trained on millions of games played by thousands of high-rated individuals. However, these monolithic networks tend to collapse into mode-averaged behavior, where stylistic boundaries are blurred, and rare but effective strategies are suppressed. To counteract homogenization, we introduce Mixture-of-Masters (MoM), the first chess mixture-of-experts model with small-sized GPT experts emulating world-class grandmasters. Each expert is trained with a combination of self-supervised learning and reinforcement learning guided by chess-specific rewards. For each move, a post-hoc learnable gating network selects the most appropriate persona to channel depending on the game state, allowing MoM to switch its style dynamically$--$e.g., Tal's offensive vocation or Petrosian's defensive solidity. When evaluated against Stockfish on unseen standard games, MoM outperforms both dense individual expert networks and popular GPT baselines trained on aggregated data, while ensuring generation variety, control, and interpretability.</li>
<li><strong>摘要：</strong>现代国际象棋语言模型是密集的变压器，经过数千名高评价个人玩的数百万场比赛的训练。然而，这些整体网络往往会崩溃为模式平均行为，其中风格界限变得模糊，罕见但有效的策略受到抑制。为了对抗同质化，我们引入了 Mixture-of-Masters (MoM)，这是第一个国际象棋专家混合模型，其中小型 GPT 专家模仿世界级大师。每位专家都接受了以国际象棋特定奖励为指导的自我监督学习和强化学习相结合的培训。对于每个动作，事后可学习的门控网络会根据游戏状态选择最合适的角色进行引导，从而允许 MoM 动态切换其风格$--$例如，Tal 的进攻职业或 Petrosian 的防守稳定性。当在看不见的标准游戏上与 Stockfish 进行评估时，MoM 的性能优于密集的个人专家网络和基于聚合数据训练的流行 GPT 基线，同时确保生成多样性、控制和可解释性。</li>
</ul>

<h3>Title: Vision-aligned Latent Reasoning for Multi-modal Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Byungwoo Jeon, Yoonwoo Jeong, Hyunseok Lee, Minsu Cho, Jinwoo Shin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04476">https://arxiv.org/abs/2602.04476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04476">https://arxiv.org/pdf/2602.04476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04476]] Vision-aligned Latent Reasoning for Multi-modal Large Language Model(https://arxiv.org/abs/2602.04476)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite recent advancements in Multi-modal Large Language Models (MLLMs) on diverse understanding tasks, these models struggle to solve problems which require extensive multi-step reasoning. This is primarily due to the progressive dilution of visual information during long-context generation, which hinders their ability to fully exploit test-time scaling. To address this issue, we introduce Vision-aligned Latent Reasoning (VaLR), a simple, yet effective reasoning framework that dynamically generates vision-aligned latent tokens before each Chain of Thought reasoning step, guiding the model to reason based on perceptual cues in the latent space. Specifically, VaLR is trained to preserve visual knowledge during reasoning by aligning intermediate embeddings of MLLM with those from vision encoders. Empirical results demonstrate that VaLR consistently outperforms existing approaches across a wide range of benchmarks requiring long-context understanding or precise visual perception, while exhibiting test-time scaling behavior not observed in prior MLLMs. In particular, VaLR improves the performance significantly from 33.0% to 52.9% on VSI-Bench, achieving a 19.9%p gain over Qwen2.5-VL.</li>
<li><strong>摘要：</strong>尽管多模态大语言模型（MLLM）最近在不同的理解任务上取得了进展，但这些模型很难解决需要广泛的多步骤推理的问题。这主要是由于长上下文生成过程中视觉信息的逐渐稀释，这阻碍了他们充分利用测试时间缩放的能力。为了解决这个问题，我们引入了视觉对齐潜在推理（VaLR），这是一个简单而有效的推理框架，它在每个思想链推理步骤之前动态生成视觉对齐潜在标记，指导模型根据潜在空间中的感知线索进行推理。具体来说，VaLR 经过训练，通过将 MLLM 的中间嵌入与视觉编码器的中间嵌入对齐来在推理过程中保留视觉知识。实证结果表明，VaLR 在需要长上下文理解或精确视觉感知的各种基准测试中始终优于现有方法，同时表现出先前 MLLM 中未观察到的测试时间缩放行为。特别是，VaLR 在 VSI-Bench 上将性能显着提高，从 33.0% 提高到 52.9%，比 Qwen2.5-VL 提高了 19.9%p。</li>
</ul>

<h3>Title: SLUM-i: Semi-supervised Learning for Urban Mapping of Informal Settlements and Data Quality Benchmarking</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Taha Mukhtar (1 and 2), Syed Musa Ali Kazmi (1), Khola Naseem (2), Muhammad Ali Chattha (2), Andreas Dengel (2), Sheraz Ahmed (2), Muhammad Naseer Bajwa (1), Muhammad Imran Malik (1) ((1) National University of Sciences and Technology (NUST), Islamabad, Pakistan, (2) German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04525">https://arxiv.org/abs/2602.04525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04525">https://arxiv.org/pdf/2602.04525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04525]] SLUM-i: Semi-supervised Learning for Urban Mapping of Informal Settlements and Data Quality Benchmarking(https://arxiv.org/abs/2602.04525)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Rapid urban expansion has fueled the growth of informal settlements in major cities of low- and middle-income countries, with Lahore and Karachi in Pakistan and Mumbai in India serving as prominent examples. However, large-scale mapping of these settlements is severely constrained not only by the scarcity of annotations but by inherent data quality challenges, specifically high spectral ambiguity between formal and informal structures and significant annotation noise. We address this by introducing a benchmark dataset for Lahore, constructed from scratch, along with companion datasets for Karachi and Mumbai, which were derived from verified administrative boundaries, totaling 1,869 $\text{km}^2$ of area. To evaluate the global robustness of our framework, we extend our experiments to five additional established benchmarks, encompassing eight cities across three continents, and provide comprehensive data quality assessments of all datasets. We also propose a new semi-supervised segmentation framework designed to mitigate the class imbalance and feature degradation inherent in standard semi-supervised learning pipelines. Our method integrates a Class-Aware Adaptive Thresholding mechanism that dynamically adjusts confidence thresholds to prevent minority class suppression and a Prototype Bank System that enforces semantic consistency by anchoring predictions to historically learned high-fidelity feature representations. Extensive experiments across a total of eight cities spanning three continents demonstrate that our approach outperforms state-of-the-art semi-supervised baselines. Most notably, our method demonstrates superior domain transfer capability whereby a model trained on only 10% of source labels reaches a 0.461 mIoU on unseen geographies and outperforms the zero-shot generalization of fully supervised models.</li>
<li><strong>摘要：</strong>快速的城市扩张推动了低收入和中等收入国家主要城市非正规住区的增长，巴基斯坦的拉合尔和卡拉奇以及印度的孟买就是典型的例子。然而，这些聚落的大规模测绘不仅受到注释稀缺的严重限制，而且还受到固有的数据质量挑战的限制，特别是正式和非正式结构之间的高光谱模糊性以及显着的注释噪声。我们通过引入从头开始构建的拉合尔基准数据集以及卡拉奇和孟买的配套数据集来解决这个问题，这些数据集源自经过验证的行政边界，总面积为 1,869 $\text{km}^2$。为了评估我们框架的全球稳健性，我们将实验扩展到另外五个已建立的基准，涵盖三大洲的八个城市，并提供所有数据集的全面数据质量评估。我们还提出了一种新的半监督分割框架，旨在减轻标准半监督学习流程中固有的类别不平衡和特征退化。我们的方法集成了一个类感知自适应阈值机制，该机制动态调整置信度阈值以防止少数类抑制，以及一个原型银行系统，该系统通过将预测锚定到历史学习的高保真特征表示来强制语义一致性。在横跨三大洲的八个城市进行的广泛实验表明，我们的方法优于最先进的半监督基线。最值得注意的是，我们的方法展示了卓越的域转移能力，仅在 10% 的源标签上训练的模型在不可见的地理区域上达到 0.461 mIoU，并且优于完全监督模型的零样本泛化。</li>
</ul>

<h3>Title: Forget to Generalize: Iterative Adaptation for Generalization in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Abdulrahman Alotaibi, Irene Tenison, Miriam Kim, Isaac Lee, Lalana Kagal</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04536">https://arxiv.org/abs/2602.04536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04536">https://arxiv.org/pdf/2602.04536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04536]] Forget to Generalize: Iterative Adaptation for Generalization in Federated Learning(https://arxiv.org/abs/2602.04536)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The Web is naturally heterogeneous with user devices, geographic regions, browsing patterns, and contexts all leading to highly diverse, unique datasets. Federated Learning (FL) is an important paradigm for the Web because it enables privacy-preserving, collaborative machine learning across diverse user devices, web services and clients without needing to centralize sensitive data. However, its performance degrades severely under non-IID client distributions that is prevalent in real-world web systems. In this work, we propose a new training paradigm - Iterative Federated Adaptation (IFA) - that enhances generalization in heterogeneous federated settings through generation-wise forget and evolve strategy. Specifically, we divide training into multiple generations and, at the end of each, select a fraction of model parameters (a) randomly or (b) from the later layers of the model and reinitialize them. This iterative forget and evolve schedule allows the model to escape local minima and preserve globally relevant representations. Extensive experiments on CIFAR-10, MIT-Indoors, and Stanford Dogs datasets show that the proposed approach improves global accuracy, especially when the data cross clients are Non-IID. This method can be implemented on top any federated algorithm to improve its generalization performance. We observe an average of 21.5%improvement across datasets. This work advances the vision of scalable, privacy-preserving intelligence for real-world heterogeneous and distributed web systems.</li>
<li><strong>摘要：</strong>Web 本质上是异构的，用户设备、地理区域、浏览模式和上下文都导致高度多样化、独特的数据集。联合学习 (FL) 是 Web 的一个重要范例，因为它可以跨不同的用户设备、Web 服务和客户端实现隐私保护、协作机器学习，而无需集中敏感数据。然而，在现实 Web 系统中普遍存在的非 IID 客户端发行版下，其性能会严重下降。在这项工作中，我们提出了一种新的训练范式——迭代联邦适应（IFA）——它通过逐代遗忘和进化策略来增强异构联邦环境中的泛化。具体来说，我们将训练分为多代，并在每代结束时（a）随机选择一部分模型参数或（b）从模型的后面层中选择一部分模型参数并重新初始化它们。这种迭代忘记和演化的时间表使模型能够摆脱局部极小值并保留全局相关的表示。在 CIFAR-10、MIT-Indoors 和 Stanley Dogs 数据集上进行的大量实验表明，所提出的方法提高了全局准确性，特别是当跨客户端的数据是非 IID 时。该方法可以在任何联邦算法之上实现，以提高其泛化性能。我们观察到整个数据集平均提高了 21.5%。这项工作推进了现实世界异构和分布式网络系统的可扩展、隐私保护智能的愿景。</li>
</ul>

<h3>Title: Nix and Fix: Targeting 1000x Compression of 3D Gaussian Splatting with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Cem Eteke, Enzo Tartaglione</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04549">https://arxiv.org/abs/2602.04549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04549">https://arxiv.org/pdf/2602.04549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04549]] Nix and Fix: Targeting 1000x Compression of 3D Gaussian Splatting with Diffusion Models(https://arxiv.org/abs/2602.04549)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>3D Gaussian Splatting (3DGS) revolutionized novel view rendering. Instead of inferring from dense spatial points, as implicit representations do, 3DGS uses sparse Gaussians. This enables real-time performance but increases space requirements, hindering applications such as immersive communication. 3DGS compression emerged as a field aimed at alleviating this issue. While impressive progress has been made, at low rates, compression introduces artifacts that degrade visual quality significantly. We introduce NiFi, a method for extreme 3DGS compression through restoration via artifact-aware, diffusion-based one-step distillation. We show that our method achieves state-of-the-art perceptual quality at extremely low rates, down to 0.1 MB, and towards 1000x rate improvement over 3DGS at comparable perceptual performance. The code will be open-sourced upon acceptance.</li>
<li><strong>摘要：</strong>3D 高斯喷射 (3DGS) 彻底改变了新颖的视图渲染。 3DGS 不像隐式表示那样从密集空间点进行推断，而是使用稀疏高斯分布。这可以实现实时性能，但会增加空间需求，阻碍沉浸式通信等应用。 3DGS 压缩作为一个旨在缓解这一问题的领域而应运而生。虽然已经取得了令人印象深刻的进展，但在低速率下，压缩会引入伪影，从而显着降低视觉质量。我们介绍 NiFi，这是一种通过基于伪影的、基于扩散的一步蒸馏进行恢复来实现极端 3DGS 压缩的方法。我们证明，我们的方法以极低的速率（低至 0.1 MB）实现了最先进的感知质量，并且在可比较的感知性能下，速率比 3DGS 提高了 1000 倍。该代码将在接受后开源。</li>
</ul>

<h3>Title: Understanding Degradation with Vision Language Model</h3>
<ul>
<li><strong>Authors: </strong>Guanzhou Lan, Chenyi Liao, Yuqi Yang, Qianli Ma, Zhigang Wang, Dong Wang, Bin Zhao, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04565">https://arxiv.org/abs/2602.04565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04565">https://arxiv.org/pdf/2602.04565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04565]] Understanding Degradation with Vision Language Model(https://arxiv.org/abs/2602.04565)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generative</a></li>
<li><strong>Abstract: </strong>Understanding visual degradations is a critical yet challenging problem in computer vision. While recent Vision-Language Models (VLMs) excel at qualitative description, they often fall short in understanding the parametric physics underlying image degradations. In this work, we redefine degradation understanding as a hierarchical structured prediction task, necessitating the concurrent estimation of degradation types, parameter keys, and their continuous physical values. Although these sub-tasks operate in disparate spaces, we prove that they can be unified under one autoregressive next-token prediction paradigm, whose error is bounded by the value-space quantization grid. Building on this insight, we introduce DU-VLM, a multimodal chain-of-thought model trained with supervised fine-tuning and reinforcement learning using structured rewards. Furthermore, we show that DU-VLM can serve as a zero-shot controller for pre-trained diffusion models, enabling high-fidelity image restoration without fine-tuning the generative backbone. We also introduce \textbf{DU-110k}, a large-scale dataset comprising 110,000 clean-degraded pairs with grounded physical annotations. Extensive experiments demonstrate that our approach significantly outperforms generalist baselines in both accuracy and robustness, exhibiting generalization to unseen distributions.</li>
<li><strong>摘要：</strong>理解视觉退化是计算机视觉中一个关键但具有挑战性的问题。虽然最近的视觉语言模型 (VLM) 擅长定性描述，但它们通常无法理解图像退化背后的参数物理原理。在这项工作中，我们将退化理解重新定义为分层结构化预测任务，需要同时估计退化类型、参数键及其连续物理值。尽管这些子任务在不同的空间中运行，但我们证明它们可以在一个自回归下一个标记预测范式下统一，其误差受到值空间量化网格的限制。基于这一见解，我们引入了 DU-VLM，这是一种多模式思想链模型，通过使用结构化奖励的监督微调和强化学习进行训练。此外，我们还表明 DU-VLM 可以用作预训练扩散模型的零样本控制器，无需微调生成主干即可实现高保真图像恢复。我们还引入了 \textbf{DU-110k}，这是一个大型数据集，包含 110,000 个带有基础物理注释的干净退化对。大量的实验表明，我们的方法在准确性和鲁棒性方面都显着优于通才基线，表现出对看不见的分布的泛化能力。</li>
</ul>

<h3>Title: Rethinking the Design Space of Reinforcement Learning for Diffusion Models: On the Importance of Likelihood Estimation Beyond Loss Design</h3>
<ul>
<li><strong>Authors: </strong>Jaemoo Choi, Yuchen Zhu, Wei Guo, Petr Molodyk, Bo Yuan, Jinbin Bai, Yi Xin, Molei Tao, Yongxin Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04663">https://arxiv.org/abs/2602.04663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04663">https://arxiv.org/pdf/2602.04663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04663]] Rethinking the Design Space of Reinforcement Learning for Diffusion Models: On the Importance of Likelihood Estimation Beyond Loss Design(https://arxiv.org/abs/2602.04663)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reinforcement learning has been widely applied to diffusion and flow models for visual tasks such as text-to-image generation. However, these tasks remain challenging because diffusion models have intractable likelihoods, which creates a barrier for directly applying popular policy-gradient type methods. Existing approaches primarily focus on crafting new objectives built on already heavily engineered LLM objectives, using ad hoc estimators for likelihood, without a thorough investigation into how such estimation affects overall algorithmic performance. In this work, we provide a systematic analysis of the RL design space by disentangling three factors: i) policy-gradient objectives, ii) likelihood estimators, and iii) rollout sampling schemes. We show that adopting an evidence lower bound (ELBO) based model likelihood estimator, computed only from the final generated sample, is the dominant factor enabling effective, efficient, and stable RL optimization, outweighing the impact of the specific policy-gradient loss functional. We validate our findings across multiple reward benchmarks using SD 3.5 Medium, and observe consistent trends across all tasks. Our method improves the GenEval score from 0.24 to 0.95 in 90 GPU hours, which is $4.6\times$ more efficient than FlowGRPO and $2\times$ more efficient than the SOTA method DiffusionNFT without reward hacking.</li>
<li><strong>摘要：</strong>强化学习已广泛应用于视觉任务（例如文本到图像生成）的扩散和流模型。然而，这些任务仍然具有挑战性，因为扩散模型具有棘手的可能性，这为直接应用流行的策略梯度类型方法造成了障碍。现有方法主要侧重于在已经精心设计的 LLM 目标的基础上制定新目标，使用临时估计器来估计可能性，而没有彻底调查这种估计如何影响整体算法性能。在这项工作中，我们通过分解三个因素来对 RL 设计空间进行系统分析：i）策略梯度目标，ii）似然估计器，以及 iii）推出抽样方案。我们表明，采用基于证据下界（ELBO）的模型似然估计（仅根据最终生成的样本计算）是实现有效、高效和稳定的 RL 优化的主导因素，超过了特定策略梯度损失函数的影响。我们使用 SD 3.5 Medium 在多个奖励基准上验证我们的发现，并观察所有任务的一致趋势。我们的方法在 90 个 GPU 小时内将 GenEval 分数从 0.24 提高到 0.95，比 FlowGRPO 效率高 4.6 美元，比没有奖励黑客的 SOTA 方法 DiffusionNFT 效率高 2 倍。</li>
</ul>

<h3>Title: AGILE: Hand-Object Interaction Reconstruction from Video via Agentic Generation</h3>
<ul>
<li><strong>Authors: </strong>Jin-Chuan Shi, Binhong Ye, Tao Liu, Junzhe He, Yangjinhui Xu, Xiaoyang Liu, Zeju Li, Hao Chen, Chunhua Shen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04672">https://arxiv.org/abs/2602.04672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04672">https://arxiv.org/pdf/2602.04672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04672]] AGILE: Hand-Object Interaction Reconstruction from Video via Agentic Generation(https://arxiv.org/abs/2602.04672)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Reconstructing dynamic hand-object interactions from monocular videos is critical for dexterous manipulation data collection and creating realistic digital twins for robotics and VR. However, current methods face two prohibitive barriers: (1) reliance on neural rendering often yields fragmented, non-simulation-ready geometries under heavy occlusion, and (2) dependence on brittle Structure-from-Motion (SfM) initialization leads to frequent failures on in-the-wild footage. To overcome these limitations, we introduce AGILE, a robust framework that shifts the paradigm from reconstruction to agentic generation for interaction learning. First, we employ an agentic pipeline where a Vision-Language Model (VLM) guides a generative model to synthesize a complete, watertight object mesh with high-fidelity texture, independent of video occlusions. Second, bypassing fragile SfM entirely, we propose a robust anchor-and-track strategy. We initialize the object pose at a single interaction onset frame using a foundation model and propagate it temporally by leveraging the strong visual similarity between our generated asset and video observations. Finally, a contact-aware optimization integrates semantic, geometric, and interaction stability constraints to enforce physical plausibility. Extensive experiments on HO3D, DexYCB, and in-the-wild videos reveal that AGILE outperforms baselines in global geometric accuracy while demonstrating exceptional robustness on challenging sequences where prior art frequently collapses. By prioritizing physical validity, our method produces simulation-ready assets validated via real-to-sim retargeting for robotic applications.</li>
<li><strong>摘要：</strong>从单眼视频中重建动态手部物体交互对于灵巧的操作数据收集以及为机器人和 VR 创建逼真的数字孪生至关重要。然而，当前的方法面临两个令人望而却步的障碍：（1）对神经渲染的依赖通常会在严重遮挡下产生支离破碎的、无法模拟的几何形状，（2）对脆弱的运动结构（SfM）初始化的依赖会导致野外镜头频繁失败。为了克服这些限制，我们引入了 AGILE，这是一个强大的框架，它将交互学习的范式从重构转变为代理生成。首先，我们采用代理管道，其中视觉语言模型 (VLM) 引导生成模型合成具有高保真纹理的完整、无懈可击的对象网格，且与视频遮挡无关。其次，我们完全绕过脆弱的 SfM，提出了一种强大的锚定和跟踪策略。我们使用基础模型在单个交互开始帧初始化对象姿势，并利用我们生成的资产和视频观察之间的强烈视觉相似性来临时传播它。最后，接触感知优化集成了语义、几何和交互稳定性约束，以增强物理合理性。对 HO3D、DexYCB 和野外​​视频进行的大量实验表明，AGILE 在全局几何精度方面优于基线，同时在现有技术经常崩溃的挑战性序列上表现出卓越的鲁棒性。通过优先考虑物理有效性，我们的方法可以生成模拟就绪的资产，并通过机器人应用程序的真实到模拟重定向进行验证。</li>
</ul>

<h3>Title: SAR-RAG: ATR Visual Question Answering by Semantic Search, Retrieval, and MLLM Generation</h3>
<ul>
<li><strong>Authors: </strong>David F. Ramirez, Tim Overman, Kristen Jaskie, Joe Marvin, Andreas Spanias</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04712">https://arxiv.org/abs/2602.04712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04712">https://arxiv.org/pdf/2602.04712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04712]] SAR-RAG: ATR Visual Question Answering by Semantic Search, Retrieval, and MLLM Generation(https://arxiv.org/abs/2602.04712)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present a visual-context image retrieval-augmented generation (ImageRAG) assisted AI agent for automatic target recognition (ATR) of synthetic aperture radar (SAR). SAR is a remote sensing method used in defense and security applications to detect and monitor the positions of military vehicles, which may appear indistinguishable in images. Researchers have extensively studied SAR ATR to improve the differentiation and identification of vehicle types, characteristics, and measurements. Test examples can be compared with known vehicle target types to improve recognition tasks. New methods enhance the capabilities of neural networks, transformer attention, and multimodal large language models. An agentic AI method may be developed to utilize a defined set of tools, such as searching through a library of similar examples. Our proposed method, SAR Retrieval-Augmented Generation (SAR-RAG), combines a multimodal large language model (MLLM) with a vector database of semantic embeddings to support contextual search for image exemplars with known qualities. By recovering past image examples with known true target types, our SAR-RAG system can compare similar vehicle categories, achieving improved ATR prediction accuracy. We evaluate this through search and retrieval metrics, categorical classification accuracy, and numeric regression of vehicle dimensions. These metrics all show improvements when SAR-RAG is added to an MLLM baseline method as an attached ATR memory bank.</li>
<li><strong>摘要：</strong>我们提出了一种视觉上下文图像检索增强生成（ImageRAG）辅助人工智能代理，用于合成孔径雷达（SAR）的自动目标识别（ATR）。 SAR 是一种用于国防和安全应用的遥感方法，用于检测和监控军用车辆的位置，这些车辆在图像中可能无法区分。研究人员广泛研究了 SAR ATR，以提高车辆类型、特性和测量值的区分和识别。可以将测试示例与已知的车辆目标类型进行比较，以改进识别任务。新方法增强了神经网络、变压器注意力和多模态大语言模型的能力。可以开发代理人工智能方法来利用一组定义的工具，例如搜索类似示例的库。我们提出的方法 SAR 检索增强生成 (SAR-RAG) 将多模态大语言模型 (MLLM) 与语义嵌入向量数据库相结合，以支持对具有已知质量的图像样本的上下文搜索。通过恢复具有已知真实目标类型的过去图像示例，我们的 SAR-RAG 系统可以比较相似的车辆类别，从而提高 ATR 预测精度。我们通过搜索和检索指标、分类准确度以及车辆尺寸的数值回归来评估这一点。当 SAR-RAG 作为附加 ATR 内存库添加到 MLLM 基线方法中时，这些指标都显示出改进。</li>
</ul>

<h3>Title: DMFlow: Disordered Materials Generation by Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Liming Wu, Rui Jiao, Qi Li, Mingze Li, Songyou Li, Shifeng Jin, Wenbing Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04734">https://arxiv.org/abs/2602.04734</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04734">https://arxiv.org/pdf/2602.04734</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04734]] DMFlow: Disordered Materials Generation by Flow Matching(https://arxiv.org/abs/2602.04734)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The design of materials with tailored properties is crucial for technological progress. However, most deep generative models focus exclusively on perfectly ordered crystals, neglecting the important class of disordered materials. To address this gap, we introduce DMFlow, a generative framework specifically designed for disordered crystals. Our approach introduces a unified representation for ordered, Substitutionally Disordered (SD), and Positionally Disordered (PD) crystals, and employs a flow matching model to jointly generate all structural components. A key innovation is a Riemannian flow matching framework with spherical reparameterization, which ensures physically valid disorder weights on the probability simplex. The vector field is learned by a novel Graph Neural Network (GNN) that incorporates physical symmetries and a specialized message-passing scheme. Finally, a two-stage discretization procedure converts the continuous weights into multi-hot atomic assignments. To support research in this area, we release a benchmark containing SD, PD, and mixed structures curated from the Crystallography Open Database. Experiments on Crystal Structure Prediction (CSP) and De Novo Generation (DNG) tasks demonstrate that DMFlow significantly outperforms state-of-the-art baselines adapted from ordered crystal generation. We hope our work provides a foundation for the AI-driven discovery of disordered materials.</li>
<li><strong>摘要：</strong>设计具有定制特性的材料对于技术进步至关重要。然而，大多数深层生成模型只关注完美有序的晶体，忽略了一类重要的无序材料。为了解决这一差距，我们引入了 DMFlow，这是一种专为无序晶体设计的生成框架。我们的方法引入了有序、取代无序（SD）和位置无序（PD）晶体的统一表示，并采用流匹配模型来共同生成所有结构组件。一个关键的创新是具有球形重新参数化的黎曼流匹配框架，它确保了概率单纯形上物理上有效的无序权重。矢量场是通过一种新颖的图神经网络（GNN）来学习的，该网络结合了物理对称性和专门的消息传递方案。最后，两阶段离散化过程将连续权重转换为多热原子分配。为了支持该领域的研究，我们发布了一个基准，其中包含从晶体学开放数据库中精选的 SD、PD 和混合结构。晶体结构预测 (CSP) 和从头生成 (DNG) 任务的实验表明，DMFlow 的性能显着优于根据有序晶体生成改编的最先进基线。我们希望我们的工作为人工智能驱动的无序材料发现奠定基础。</li>
</ul>

<h3>Title: Generative Modeling via Drifting</h3>
<ul>
<li><strong>Authors: </strong>Mingyang Deng, He Li, Tianhong Li, Yilun Du, Kaiming He</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04770">https://arxiv.org/abs/2602.04770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04770">https://arxiv.org/pdf/2602.04770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04770]] Generative Modeling via Drifting(https://arxiv.org/abs/2602.04770)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative modeling can be formulated as learning a mapping f such that its pushforward distribution matches the data distribution. The pushforward behavior can be carried out iteratively at inference time, for example in diffusion and flow-based models. In this paper, we propose a new paradigm called Drifting Models, which evolve the pushforward distribution during training and naturally admit one-step inference. We introduce a drifting field that governs the sample movement and achieves equilibrium when the distributions match. This leads to a training objective that allows the neural network optimizer to evolve the distribution. In experiments, our one-step generator achieves state-of-the-art results on ImageNet at 256 x 256 resolution, with an FID of 1.54 in latent space and 1.61 in pixel space. We hope that our work opens up new opportunities for high-quality one-step generation.</li>
<li><strong>摘要：</strong>生成建模可以表述为学习映射 f，使其前推分布与数据分布相匹配。前推行为可以在推理时迭代地执行，例如在扩散和基于流的模型中。在本文中，我们提出了一种称为漂移模型的新范式，它在训练过程中演化了前推分布，并自然地允许一步推理。我们引入了一个漂移场来控制样本运动并在分布匹配时达到平衡。这导致了一个训练目标，允许神经网络优化器进化分布。在实验中，我们的一步生成器在 ImageNet 上以 256 x 256 分辨率实现了最先进的结果，潜在空间中的 FID 为 1.54，像素空间中的 FID 为 1.61。我们希望我们的工作为高质量的一步生成开辟新的机遇。</li>
</ul>

<h3>Title: Dynamical Regimes of Multimodal Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Emil Albrychiewicz, Andrés Franco Valiente, Li-Ching Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04780">https://arxiv.org/abs/2602.04780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04780">https://arxiv.org/pdf/2602.04780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04780]] Dynamical Regimes of Multimodal Diffusion Models(https://arxiv.org/abs/2602.04780)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion based generative models have achieved unprecedented fidelity in synthesizing high dimensional data, yet the theoretical mechanisms governing multimodal generation remain poorly understood. Here, we present a theoretical framework for coupled diffusion models, using coupled Ornstein-Uhlenbeck processes as a tractable model. By using the nonequilibrium statistical physics of dynamical phase transitions, we demonstrate that multimodal generation is governed by a spectral hierarchy of interaction timescales rather than simultaneous resolution. A key prediction is the ``synchronization gap'', a temporal window during the reverse generative process where distinct eigenmodes stabilize at different rates, providing a theoretical explanation for common desynchronization artifacts. We derive analytical conditions for speciation and collapse times under both symmetric and anisotropic coupling regimes, establishing strict bounds for coupling strength to avoid unstable symmetry breaking. We show that the coupling strength acts as a spectral filter that enforces a tunable temporal hierarchy on generation. We support these predictions through controlled experiments with diffusion models trained on MNIST datasets and exact score samplers. These results motivate time dependent coupling schedules that target mode specific timescales, offering a potential alternative to ad hoc guidance tuning.</li>
<li><strong>摘要：</strong>基于扩散的生成模型在合成高维数据方面实现了前所未有的保真度，但控制多模态生成的理论机制仍然知之甚少。在这里，我们提出了耦合扩散模型的理论框架，使用耦合 Ornstein-Uhlenbeck 过程作为易于处理的模型。通过使用动态相变的非平衡统计物理学，我们证明多模态生成是由相互作用时间尺度的谱层次而不是同时分辨率控制的。一个关键的预测是“同步间隙”，这是反向生成过程中的一个时间窗口，其中不同的本征模以不同的速率稳定，为常见的去同步伪影提供了理论解释。我们推导了对称和各向异性耦合状态下形态和塌缩时间的分析条件，为耦合强度建立了严格的界限，以避免不稳定的对称性破缺。我们证明耦合强度充当光谱滤波器，在生成时强制执行可调时间层次结构。我们通过使用在 MNIST 数据集和精确分数采样器上训练的扩散模型进行受控实验来支持这些预测。这些结果激发了针对特定模式时间尺度的时间相关耦合计划，为临时指导调整提供了潜在的替代方案。</li>
</ul>

<h3>Title: Team, Then Trim: An Assembly-Line LLM Framework for High-Quality Tabular Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Congjing Zhang, Ryan Feng Lin, Ruoxuan Bao, Shuai Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04785">https://arxiv.org/abs/2602.04785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04785">https://arxiv.org/pdf/2602.04785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04785]] Team, Then Trim: An Assembly-Line LLM Framework for High-Quality Tabular Data Generation(https://arxiv.org/abs/2602.04785)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While tabular data is fundamental to many real-world machine learning (ML) applications, acquiring high-quality tabular data is usually labor-intensive and expensive. Limited by the scarcity of observations, tabular datasets often exhibit critical deficiencies, such as class imbalance, selection bias, and low fidelity. To address these challenges, building on recent advances in Large Language Models (LLMs), this paper introduces Team-then-Trim (T$^2$), a framework that synthesizes high-quality tabular data through a collaborative team of LLMs, followed by a rigorous three-stage plug-in data quality control (QC) pipeline. In T$^2$, tabular data generation is conceptualized as a manufacturing process: specialized LLMs, guided by domain knowledge, are tasked with generating different data components sequentially, and the resulting products, i.e., the synthetic data, are systematically evaluated across multiple dimensions of QC. Empirical results on both simulated and real-world datasets demonstrate that T$^2$ outperforms state-of-the-art methods in producing high-quality tabular data, highlighting its potential to support downstream models when direct data collection is practically infeasible.</li>
<li><strong>摘要：</strong>虽然表格数据是许多现实世界机器学习 (ML) 应用程序的基础，但获取高质量表格数据通常需要大量人力且成本高昂。由于观察数据的稀缺性，表格数据集经常表现出严重的缺陷，例如类别不平衡、选择偏差和低保真度。为了应对这些挑战，本文以大型语言模型 (LLM) 的最新进展为基础，引入了 Team-then-Trim (T$^2$) 框架，该框架通过 LLM 协作团队合成高质量的表格数据，然后是严格的三阶段插件数据质量控制 (QC) 管道。在 T$^2$ 中，表格数据生成被概念化为一个制造过程：专门的法学硕士在领域知识的指导下，负责顺序生成不同的数据组件，并在质量控制的多个维度上系统地评估生成的产品，即合成数据。模拟和现实数据集的实证结果表明，T$^2$ 在生成高质量表格数据方面优于最先进的方法，突显了当直接数据收集实际上不可行时它支持下游模型的潜力。</li>
</ul>

<h3>Title: Light Forcing: Accelerating Autoregressive Video Diffusion via Sparse Attention</h3>
<ul>
<li><strong>Authors: </strong>Chengtao Lv, Yumeng Shi, Yushi Huang, Ruihao Gong, Shen Ren, Wenya Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04789">https://arxiv.org/abs/2602.04789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04789">https://arxiv.org/pdf/2602.04789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04789]] Light Forcing: Accelerating Autoregressive Video Diffusion via Sparse Attention(https://arxiv.org/abs/2602.04789)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Advanced autoregressive (AR) video generation models have improved visual fidelity and interactivity, but the quadratic complexity of attention remains a primary bottleneck for efficient deployment. While existing sparse attention solutions have shown promise on bidirectional models, we identify that applying these solutions to AR models leads to considerable performance degradation for two reasons: isolated consideration of chunk generation and insufficient utilization of past informative context. Motivated by these observations, we propose \textsc{Light Forcing}, the \textit{first} sparse attention solution tailored for AR video generation models. It incorporates a \textit{Chunk-Aware Growth} mechanism to quantitatively estimate the contribution of each chunk, which determines their sparsity allocation. This progressive sparsity increase strategy enables the current chunk to inherit prior knowledge in earlier chunks during generation. Additionally, we introduce a \textit{Hierarchical Sparse Attention} to capture informative historical and local context in a coarse-to-fine manner. Such two-level mask selection strategy (\ie, frame and block level) can adaptively handle diverse attention patterns. Extensive experiments demonstrate that our method outperforms existing sparse attention in quality (\eg, 84.5 on VBench) and efficiency (\eg, $1.2{\sim}1.3\times$ end-to-end speedup). Combined with FP8 quantization and LightVAE, \textsc{Light Forcing} further achieves a $2.3\times$ speedup and 19.7\,FPS on an RTX~5090 GPU. Code will be released at \href{this https URL}{this https URL}.</li>
<li><strong>摘要：</strong>先进的自回归 (AR) 视频生成模型提高了视觉保真度和交互性，但注意力的二次复杂度仍然是高效部署的主要瓶颈。虽然现有的稀疏注意力解决方案在双向模型上显示出了希望，但我们发现，将这些解决方案应用于 AR 模型会导致性能显着下降，原因有两个：单独考虑块生成以及对过去信息上下文的利用不足。受这些观察的启发，我们提出了 \textsc{Light Forcing}，即为 AR 视频生成模型量身定制的 \textit{first} 稀疏注意力解决方案。它采用了 \textit{Chunk-Aware Growth} 机制来定量估计每个块的贡献，这决定了它们的稀疏性分配。这种渐进式稀疏性增加策略使当前块能够在生成过程中继承早期块中的先验知识。此外，我们引入了 \textit{Hierarchical Sparse Attention} 以从粗到细的方式捕获信息丰富的历史和本地上下文。这种两级掩码选择策略（即帧级和块级）可以自适应地处理不同的注意力模式。大量实验表明，我们的方法在质量（例如 VBench 上的 84.5）和效率（例如 $1.2{\sim}1.3\times$ 端到端加速）方面优于现有的稀疏注意力。结合 FP8 量化和 LightVAE，\textsc{Light Forcing} 在 RTX~5090 GPU 上进一步实现了 $2.3\times$ 加速和 19.7\,FPS。代码将在 \href{此 https URL}{此 https URL} 发布。</li>
</ul>

<h3>Title: X2HDR: HDR Image Generation in a Perceptually Uniform Space</h3>
<ul>
<li><strong>Authors: </strong>Ronghuan Wu, Wanchao Su, Kede Ma, Jing Liao, Rafał K. Mantiuk</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04814">https://arxiv.org/abs/2602.04814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04814">https://arxiv.org/pdf/2602.04814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04814]] X2HDR: HDR Image Generation in a Perceptually Uniform Space(https://arxiv.org/abs/2602.04814)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>High-dynamic-range (HDR) formats and displays are becoming increasingly prevalent, yet state-of-the-art image generators (e.g., Stable Diffusion and FLUX) typically remain limited to low-dynamic-range (LDR) output due to the lack of large-scale HDR training data. In this work, we show that existing pretrained diffusion models can be easily adapted to HDR generation without retraining from scratch. A key challenge is that HDR images are natively represented in linear RGB, whose intensity and color statistics differ substantially from those of sRGB-encoded LDR images. This gap, however, can be effectively bridged by converting HDR inputs into perceptually uniform encodings (e.g., using PU21 or PQ). Empirically, we find that LDR-pretrained variational autoencoders (VAEs) reconstruct PU21-encoded HDR inputs with fidelity comparable to LDR data, whereas linear RGB inputs cause severe degradations. Motivated by this finding, we describe an efficient adaptation strategy that freezes the VAE and finetunes only the denoiser via low-rank adaptation in a perceptually uniform space. This results in a unified computational method that supports both text-to-HDR synthesis and single-image RAW-to-HDR reconstruction. Experiments demonstrate that our perceptually encoded adaptation consistently improves perceptual fidelity, text-image alignment, and effective dynamic range, relative to previous techniques.</li>
<li><strong>摘要：</strong>高动态范围 (HDR) 格式和显示器变得越来越普遍，但由于缺乏大规模 HDR 训练数据，最先进的图像生成器（例如稳定扩散和通量）通常仍仅限于低动态范围 (LDR) 输出。在这项工作中，我们表明现有的预训练扩散模型可以轻松适应 HDR 生成，而无需从头开始重新训练。一个关键的挑战是 HDR 图像本身以线性 RGB 表示，其强度和颜色统计数据与 sRGB 编码的 LDR 图像有很大不同。然而，可以通过将 HDR 输入转换为感知统一的编码（例如使用 PU21 或 PQ）来有效弥补这一差距。根据经验，我们发现 LDR 预训练的变分自动编码器 (VAE) 重建 PU21 编码的 HDR 输入，其保真度与 LDR 数据相当，而线性 RGB 输入会导致严重的退化。受这一发现的启发，我们描述了一种有效的适应策略，该策略冻结 VAE 并通过感知均匀空间中的低秩适应仅微调降噪器。这产生了统一的计算方法，支持文本到 HDR 合成和单图像 RAW 到 HDR 重建。实验表明，相对于以前的技术，我们的感知编码适应持续提高了感知保真度、文本图像对齐和有效动态范围。</li>
</ul>

<h3>Title: PerpetualWonder: Long-Horizon Action-Conditioned 4D Scene Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Zhan, Zizhang Li, Hong-Xing Yu, Jiajun Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04876">https://arxiv.org/abs/2602.04876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04876">https://arxiv.org/pdf/2602.04876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04876]] PerpetualWonder: Long-Horizon Action-Conditioned 4D Scene Generation(https://arxiv.org/abs/2602.04876)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We introduce PerpetualWonder, a hybrid generative simulator that enables long-horizon, action-conditioned 4D scene generation from a single image. Current works fail at this task because their physical state is decoupled from their visual representation, which prevents generative refinements to update the underlying physics for subsequent interactions. PerpetualWonder solves this by introducing the first true closed-loop system. It features a novel unified representation that creates a bidirectional link between the physical state and visual primitives, allowing generative refinements to correct both the dynamics and appearance. It also introduces a robust update mechanism that gathers supervision from multiple viewpoints to resolve optimization ambiguity. Experiments demonstrate that from a single image, PerpetualWonder can successfully simulate complex, multi-step interactions from long-horizon actions, maintaining physical plausibility and visual consistency.</li>
<li><strong>摘要：</strong>我们推出了 PerpetualWonder，这是一种混合生成模拟器，可以从单个图像生成长视野、动作条件的 4D 场景。目前的工作在这项任务上失败了，因为它们的物理状态与视觉表示脱钩，这阻止了生成性细化来更新后续交互的底层物理。 PerpetualWonder 通过引入第一个真正的闭环系统解决了这个问题。它具有新颖的统一表示形式，可在物理状态和视觉基元之间创建双向链接，从而允许生成细化来纠正动态和外观。它还引入了强大的更新机制，从多个角度收集监督以解决优化模糊性。实验表明，PerpetualWonder 可以通过单张图像成功模拟长视距动作的复杂、多步骤交互，保持物理合理性和视觉一致性。</li>
</ul>

<h3>Title: Protein Autoregressive Modeling via Multiscale Structure Generation</h3>
<ul>
<li><strong>Authors: </strong>Yanru Qu, Cheng-Yen Hsieh, Zaixiang Zheng, Ge Liu, Quanquan Gu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.BM, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.04883">https://arxiv.org/abs/2602.04883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.04883">https://arxiv.org/pdf/2602.04883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.04883]] Protein Autoregressive Modeling via Multiscale Structure Generation(https://arxiv.org/abs/2602.04883)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales. To achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces conditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that generates backbone atoms conditioned on these embeddings. Moreover, autoregressive models suffer from exposure bias, caused by the training and the generation procedure mismatch, and substantially degrades structure generation quality. We effectively alleviate this issue by adopting noisy context learning and scheduled sampling, enabling robust backbone generation. Notably, PAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional generation and motif scaffolding without requiring fine-tuning. On the unconditional generation benchmark, PAR effectively learns protein distributions and produces backbones of high design quality, and exhibits favorable scaling behavior. Together, these properties establish PAR as a promising framework for protein structure generation.</li>
<li><strong>摘要：</strong>我们提出了蛋白质自回归模型（PAR），这是第一个通过从粗到细的下一尺度预测来生成蛋白质主干的多尺度自回归框架。利用蛋白质的层次性质，PAR 生成模仿雕刻雕像的结构，形成粗略的拓扑结构并在尺度上细化结构细节。为了实现这一目标，PAR 由三个关键部分组成：（i）多尺度下采样操作，在训练过程中表示跨多个尺度的蛋白质结构； (ii) 一个自回归变压器，用于编码多尺度信息并产生条件嵌入来指导结构生成； （iii）基于流的主干解码器，生成以这些嵌入为条件的主干原子。此外，自回归模型会因训练和生成过程不匹配而遭受暴露偏差，并显着降低结构生成质量。我们通过采用嘈杂的上下文学习和计划采样来有效缓解这个问题，从而实现强大的骨干网生成。值得注意的是，PAR 表现出强大的零样本泛化能力，支持灵活的人类提示条件生成和主题支架，无需微调。在无条件生成基准上，PAR 有效地学习蛋白质分布并生成高设计质量的主干，并表现出良好的缩放行为。这些特性共同使 PAR 成为蛋白质结构生成的有前途的框架。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
