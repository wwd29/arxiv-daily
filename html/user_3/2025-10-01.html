<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-01</h1>
<h3>Title: Spectral Logit Sculpting: Adaptive Low-Rank Logit Transformation for Controlled Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Jin Li, Zhebo Wang, Tianliang Lu, Mohan Li, Wenpeng Xing, Meng Han</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25204">https://arxiv.org/abs/2509.25204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25204">https://arxiv.org/pdf/2509.25204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25204]] Spectral Logit Sculpting: Adaptive Low-Rank Logit Transformation for Controlled Text Generation(https://arxiv.org/abs/2509.25204)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Entropy-based inference methods have gained traction for improving the reliability of Large Language Models (LLMs). However, many existing approaches, such as entropy minimization techniques, suffer from high computational overhead and fail to leverage historical token context effectively. To address these limitations, we propose Spectral Logit Sculpting (SLS), a lightweight inference-time optimization method that dynamically modulates token distributions using spectral and entropic properties of recent logits. SLS maintains a sliding buffer of top-K logits, performs on-the-fly Singular Value Decomposition (SVD) to identify dominant spectral directions, and adaptively rescales logits based on both entropy and logit gap statistics--only activating when uncertainty is high. Without updating any model parameters, SLS effectively sharpens the output distribution while preserving contextual consistency. Experimental results on multiple public benchmarks demonstrate that SLS consistently outperforms existing baseline methods, achieving superior accuracy in mathematical, coding, and scientific reasoning tasks.</li>
<li><strong>摘要：</strong>基于熵的推理方法已获得了提高大语言模型（LLM）的可靠性的吸引力。但是，许多现有的方法，例如熵最小化技术，都遭受了高度计算开销的困扰，并且无法有效利用历史令牌环境。为了解决这些局限性，我们提出了光谱logit雕刻（SLS），这是一种轻巧的推理时间优化方法，该方法使用最近逻辑的光谱和熵属性动态调节令牌分布。 SLS保持顶级逻辑的滑动缓冲区，执行直接的奇异值分解（SVD），以识别主要的光谱方向，并且基于熵和logit Gap Gap统计量的自适应重新划分逻辑 - 仅在不确定的情况下就可以激活。在不更新任何模型参数的情况下，SLS可以有效地提高输出分布，同时保持上下文一致性。对多个公共基准测试的实验结果表明，SLS始终优于现有的基线方法，在数学，编码和科学推理任务方面达到了卓越的准确性。</li>
</ul>

<h3>Title: Hyperbolic Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yanke Wang, Kyriakos Flouris</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25206">https://arxiv.org/abs/2509.25206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25206">https://arxiv.org/pdf/2509.25206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25206]] Hyperbolic Optimization(https://arxiv.org/abs/2509.25206)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This work explores optimization methods on hyperbolic manifolds. Building on Riemannian optimization principles, we extend the Hyperbolic Stochastic Gradient Descent (a specialization of Riemannian SGD) to a Hyperbolic Adam optimizer. While these methods are particularly relevant for learning on the Poincaré ball, they may also provide benefits in Euclidean and other non-Euclidean settings, as the chosen optimization encourages the learning of Poincaré embeddings. This representation, in turn, accelerates convergence in the early stages of training, when parameters are far from the optimum. As a case study, we train diffusion models using the hyperbolic optimization methods with hyperbolic time-discretization of the Langevin dynamics, and show that they achieve faster convergence on certain datasets without sacrificing generative quality.</li>
<li><strong>摘要：</strong>这项工作探讨了双曲线歧管上的优化方法。在基于Riemannian优化原则的基础上，我们将双曲线随机梯度下降（Riemannian SGD的专业化）扩展到双曲线Adam优化器。尽管这些方法与庞加莱球上的学习特别相关，但它们也可能在欧几里得和其他非欧几里得环境中提供好处，因为所选优化鼓励学习庞加莱嵌入。反过来，当参数远非最佳时，这种表示在训练的早期阶段就可以加速收敛。作为一个案例研究，我们使用双曲线优化方法训练扩散模型，该模型具有双曲线的时间限制，并表明它们在不牺牲生成质量的情况下在某些数据集上实现了更快的收敛速度。</li>
</ul>

<h3>Title: Six Sigma For Neural Networks: Taguchi-based optimization</h3>
<ul>
<li><strong>Authors: </strong>Sai Varun Kodathala</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25213">https://arxiv.org/abs/2509.25213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25213">https://arxiv.org/pdf/2509.25213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25213]] Six Sigma For Neural Networks: Taguchi-based optimization(https://arxiv.org/abs/2509.25213)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>The optimization of hyperparameters in convolutional neural networks (CNNs) remains a challenging and computationally expensive process, often requiring extensive trial-and-error approaches or exhaustive grid searches. This study introduces the application of Taguchi Design of Experiments methodology, a statistical optimization technique traditionally used in quality engineering, to systematically optimize CNN hyperparameters for professional boxing action recognition. Using an L12(211) orthogonal array, eight hyperparameters including image size, color mode, activation function, learning rate, rescaling, shuffling, vertical flip, and horizontal flip were systematically evaluated across twelve experimental configurations. To address the multi-objective nature of machine learning optimization, five different approaches were developed to simultaneously optimize training accuracy, validation accuracy, training loss, and validation loss using Signal-to-Noise ratio analysis. The study employed a novel logarithmic scaling technique to unify conflicting metrics and enable comprehensive multi-quality assessment within the Taguchi framework. Results demonstrate that Approach 3, combining weighted accuracy metrics with logarithmically transformed loss functions, achieved optimal performance with 98.84% training accuracy and 86.25% validation accuracy while maintaining minimal loss values. The Taguchi analysis revealed that learning rate emerged as the most influential parameter, followed by image size and activation function, providing clear guidance for hyperparameter prioritization in CNN optimization.</li>
<li><strong>摘要：</strong>在卷积神经网络（CNN）中的高参数优化仍然是一个具有挑战性且计算昂贵的过程，通常需要广泛的试验方法或详尽的网格搜索。这项研究介绍了Taguchi实验方法的应用，这是一种传统上用于质量工程的统计优化技术，以系统地优化CNN超参数用于专业拳击行动识别。使用L12（211）正交阵列，包括图像大小，颜色模式，激活函数，学习率，重新缩放，改组，垂直翻转和水平翻转的八个超参数。为了解决机器学习优化的多目标性质，开发了五种不同的方法来同时优化训练准确性，验证精度，训练损失和使用信噪比分析的验证损失。该研究采用了一种新颖的对数缩放技术来统一冲突的指标，并在Taguchi框架内实现了全面的多质量评估。结果表明，将加权精度指标与对数转换的损失功能相结合，以98.84％的训练精度和86.25％的验证精度，在保持最小的损失值的同时，实现了最佳性能。 Taguchi分析表明，学习率是最有影响力的参数，其次是图像大小和激活函数，为CNN优化中的超参数优先级提供了明确的指导。</li>
</ul>

<h3>Title: Learning to Condition: A Neural Heuristic for Scalable MPE Inference</h3>
<ul>
<li><strong>Authors: </strong>Brij Malhotra, Shivvrat Arya, Tahrima Rahman, Vibhav Giridhar Gogate</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25217">https://arxiv.org/abs/2509.25217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25217">https://arxiv.org/pdf/2509.25217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25217]] Learning to Condition: A Neural Heuristic for Scalable MPE Inference(https://arxiv.org/abs/2509.25217)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce learning to condition (L2C), a scalable, data-driven framework for accelerating Most Probable Explanation (MPE) inference in Probabilistic Graphical Models (PGMs), a fundamentally intractable problem. L2C trains a neural network to score variable-value assignments based on their utility for conditioning, given observed evidence. To facilitate supervised learning, we develop a scalable data generation pipeline that extracts training signals from the search traces of existing MPE solvers. The trained network serves as a heuristic that integrates with search algorithms, acting as a conditioning strategy prior to exact inference or as a branching and node selection policy within branch-and-bound solvers. We evaluate L2C on challenging MPE queries involving high-treewidth PGMs. Experiments show that our learned heuristic significantly reduces the search space while maintaining or improving solution quality over state-of-the-art methods.</li>
<li><strong>摘要：</strong>我们将学习介绍到条件（L2C），这是一个可扩展的，数据驱动的框架，用于加速概率图形模型（PGMS）中最可能的解释（MPE）推断，这是一个根本上棘手的问题。鉴于观察到的证据，L2C根据其效用来训练神经网络，以根据其效用来评分可变值分配。为了促进监督学习，我们开发了可扩展的数据生成管道，该管道从现有MPE求解器的搜索轨迹中提取培训信号。训练有素的网络是一种与搜索算法集成在一起的启发式，在精确推理之前充当条件策略，或者在分支机构和结合求解器中充当分支和节点选择策略。我们评估L2C关于涉及高营PGM的具有挑战性的MPE查询。实验表明，我们所学到的启发式大大降低了搜索空间，同时维持或提高了与最先进方法的解决方案质量。</li>
</ul>

<h3>Title: MSCoD: An Enhanced Bayesian Updating Framework with Multi-Scale Information Bottleneck and Cooperative Attention for Structure-Based Drug Design</h3>
<ul>
<li><strong>Authors: </strong>Long Xu, Yongcai Chen, Fengshuo Liu, Yuzhong Peng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25225">https://arxiv.org/abs/2509.25225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25225">https://arxiv.org/pdf/2509.25225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25225]] MSCoD: An Enhanced Bayesian Updating Framework with Multi-Scale Information Bottleneck and Cooperative Attention for Structure-Based Drug Design(https://arxiv.org/abs/2509.25225)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Structure-Based Drug Design (SBDD) is a powerful strategy in computational drug discovery, utilizing three-dimensional protein structures to guide the design of molecules with improved binding affinity. However, capturing complex protein-ligand interactions across multiple scales remains challenging, as current methods often overlook the hierarchical organization and intrinsic asymmetry of these interactions. To address these limitations, we propose MSCoD, a novel Bayesian updating-based generative framework for structure-based drug design. In our MSCoD, Multi-Scale Information Bottleneck (MSIB) was developed, which enables semantic compression at multiple abstraction levels for efficient hierarchical feature extraction. Furthermore, a multi-head cooperative attention (MHCA) mechanism was developed, which employs asymmetric protein-to-ligand attention to capture diverse interaction types while addressing the dimensionality disparity between proteins and ligands. Empirical studies showed that MSCoD outperforms state-of-the-art methods on the benchmark dataset. Case studies on challenging targets such as KRAS G12D further demonstrate its applicability in real-world scenarios. The code and data underlying this article are freely available at this https URL.</li>
<li><strong>摘要：</strong>基于结构的药物设计（SBDD）是计算药物发现的强大策略，利用三维蛋白质结构来指导具有改善结合亲和力的分子的设计。但是，捕获多个尺度的复杂蛋白质 - 配体相互作用仍然具有挑战性，因为当前方法通常忽略了这些相互作用的层次组织和内在的不对称性。为了解决这些限制，我们提出了MSCOD，这是一种基于结构的药物设计的新型贝叶斯更新生成框架。在我们的MSCOD中，开发了多尺度信息瓶颈（MSIB），该信息可以在多个抽象水平下进行语义压缩，以实现有效的分层特征提取。此外，开发了一种多头合作关注（MHCA）机制，该机制采用了不对称的蛋白质到配体的关注来捕获各种相互作用类型，同时解决蛋白质与配体之间的维度差异。实证研究表明，MSCOD在基准数据集上的最先进方法优于最先进的方法。关于诸如KRAS G12D的具有挑战性目标的案例研究进一步证明了其在现实情况下的适用性。本文的代码和数据可在此HTTPS URL上免费获得。</li>
</ul>

<h3>Title: Integrated Forecasting of Marine Renewable Power: An Adaptively Bayesian-Optimized MVMD-LSTM Framework for Wind-Solar-Wave Energy</h3>
<ul>
<li><strong>Authors: </strong>Baoyi Xie, Shuiling Shi, Wenqi Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25226">https://arxiv.org/abs/2509.25226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25226">https://arxiv.org/pdf/2509.25226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25226]] Integrated Forecasting of Marine Renewable Power: An Adaptively Bayesian-Optimized MVMD-LSTM Framework for Wind-Solar-Wave Energy(https://arxiv.org/abs/2509.25226)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Integrated wind-solar-wave marine energy systems hold broad promise for supplying clean electricity in offshore and coastal regions. By leveraging the spatiotemporal complementarity of multiple resources, such systems can effectively mitigate the intermittency and volatility of single-source outputs, thereby substantially improving overall power-generation efficiency and resource utilization. Accurate ultra-short-term forecasting is crucial for ensuring secure operation and optimizing proactive dispatch. However, most existing forecasting methods construct separate models for each energy source, insufficiently account for the complex couplings among multiple energies, struggle to capture the system's nonlinear and nonstationary dynamics, and typically depend on extensive manual parameter tuning-limitations that constrain both predictive performance and practicality. We address this issue using a Bayesian-optimized Multivariate Variational Mode Decomposition-Long Short-Term Memory (MVMD-LSTM) framework. The framework first applies MVMD to jointly decompose wind, solar and wave power series so as to preserve cross-source couplings; it uses Bayesian optimization to automatically search the number of modes and the penalty parameter in the MVMD process to obtain intrinsic mode functions (IMFs); finally, an LSTM models the resulting IMFs to achieve ultra-short-term power forecasting for the integrated system. Experiments based on field measurements from an offshore integrated energy platform in China show that the proposed framework significantly outperforms benchmark models in terms of MAPE, RMSE and MAE. The results demonstrate superior predictive accuracy, robustness, and degree of automation.</li>
<li><strong>摘要：</strong>综合风能海洋能源系统在近海和沿海地区提供清洁电力具有广阔的希望。通过利用多种资源的时空互补性，这种系统可以有效地降低单源输出的间歇性和波动性，从而显着提高整体功率产生效率和资源利用率。准确的超短期预测对于确保安全操作和优化主动调度至关重要。但是，大多数现有的预测方法为每个能源构建了单独的模型，不足以说明多个能量之间的复杂耦合，难以捕获系统的非线性和非平稳动态，并且通常取决于限制预测性能和实用性的广泛手动参数调谐限制。我们使用贝叶斯优化的多元变分模式分解长期内存（MVMD-LSTM）框架来解决此问题。该框架首先将MVMD应用于共同分解风，太阳能和波力序列，以保留跨源耦合。它使用贝叶斯优化来自动搜索MVMD过程中的模式数量和惩罚参数，以获得内在模式函数（IMFS）；最后，LSTM对所得IMF进行建模，以实现集成系统的超短期功率预测。基于中国离岸集成能量平台的现场测量值的实验表明，该框架在MAPE，RMSE和MAE方面大大优于基准模型。结果表明了卓越的预测准确性，鲁棒性和自动化程度。</li>
</ul>

<h3>Title: Simple, Fast and Efficient Injective Manifold Density Estimation with Random Projections</h3>
<ul>
<li><strong>Authors: </strong>Ahmad Ayaz Amin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25228">https://arxiv.org/abs/2509.25228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25228">https://arxiv.org/pdf/2509.25228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25228]] Simple, Fast and Efficient Injective Manifold Density Estimation with Random Projections(https://arxiv.org/abs/2509.25228)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce Random Projection Flows (RPFs), a principled framework for injective normalizing flows that leverages tools from random matrix theory and the geometry of random projections. RPFs employ random semi-orthogonal matrices, drawn from Haar-distributed orthogonal ensembles via QR decomposition of Gaussian matrices, to project data into lower-dimensional latent spaces for the base distribution. Unlike PCA-based flows or learned injective maps, RPFs are plug-and-play, efficient, and yield closed-form expressions for the Riemannian volume correction term. We demonstrate that RPFs are both theoretically grounded and practically effective, providing a strong baseline for generative modeling and a bridge between random projection theory and normalizing flows.</li>
<li><strong>摘要：</strong>我们介绍了随机投影流（RPF），这是一个原则性的框架，用于将其归一化流量归一化，从而利用随机矩阵理论和随机投影的几何形状来利用工具。 RPFS采用随机的半正交矩阵，通过HAAR分布的正交组合通过Gaussian矩阵分解，将数据数据投影到较低维度的潜在空间中，以进行基本分布。与基于PCA的流量或学到的注射映射不同，RPF是Riemannian体积校正项的插件，高效和产量闭合形式表达式。我们证明了RPF既是理论上的扎根又是实际有效的，为生成建模和随机投影理论和正常流量之间的桥梁提供了强大的基线。</li>
</ul>

<h3>Title: A Weather Foundation Model for the Power Grid</h3>
<ul>
<li><strong>Authors: </strong>Cristian Bodnar, Raphaël Rousseau-Rizzi, Nikhil Shankar, James Merleau, Stylianos Flampouris, Guillem Candille, Slavica Antic, François Miralles, Jayesh K. Gupta</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.ao-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25268">https://arxiv.org/abs/2509.25268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25268">https://arxiv.org/pdf/2509.25268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25268]] A Weather Foundation Model for the Power Grid(https://arxiv.org/abs/2509.25268)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Weather foundation models (WFMs) have recently set new benchmarks in global forecast skill, yet their concrete value for the weather-sensitive infrastructure that powers modern society remains largely unexplored. In this study, we fine-tune Silurian AI's 1.5B-parameter WFM, Generative Forecasting Transformer (GFT), on a rich archive of Hydro-Québec asset observations--including transmission-line weather stations, wind-farm met-mast streams, and icing sensors--to deliver hyper-local, asset-level forecasts for five grid-critical variables: surface temperature, precipitation, hub-height wind speed, wind-turbine icing risk, and rime-ice accretion on overhead conductors. Across 6-72 h lead times, the tailored model surpasses state-of-the-art NWP benchmarks, trimming temperature mean absolute error (MAE) by 15%, total-precipitation MAE by 35%, and lowering wind speed MAE by 15%. Most importantly, it attains an average precision score of 0.72 for day-ahead rime-ice detection, a capability absent from existing operational systems, which affords several hours of actionable warning for potentially catastrophic outage events. These results show that WFMs, when post-trained with small amounts of high-fidelity, can serve as a practical foundation for next-generation grid-resilience intelligence.</li>
<li><strong>摘要：</strong>天气基金会模型（WFMS）最近在全球预测技能上树立了新的基准，但是它们对维护现代社会的天气敏感基础设施的具体价值仍然很大程度上尚未得到探索。在这项研究中，我们在丰富的Hydro-Québec资产观察档案中微调了Silurian AI的1.5B参数WFM，生成预测变压器（GFT），包括传输线天气站，包括风网启示式启动和固定传感器 - 到交付超级元素，五个型号的差异，以获取五粒子的呈五颗粒，以获取五颗粒的定量，以获取五颗粒的变化，以获取五颗粒的变化，以获取5颗粒的差异，以获取五颗粒的差异，以弥补五粒子综合综合综合综合综合综合综合综合因式疾病，轮毂高风速，风涡轮糖霜风险和架空导体上的rime-ice积聚。在6-72小时的交货时间内，量身定制的模型超过了最先进的NWP基准，修剪温度的绝对误差（MAE）降低15％，总质量MAE降低35％，并将风速MAE降低15％。最重要的是，它的平均精度得分为0.72的rime-Ice检测，这是现有操作系统缺乏的能力，该系统对潜在的灾难性停电事件提供了数小时可行的警告。这些结果表明，在接受少量高保真性训练后，WFM可以成为下一代网格弥补智能的实践基础。</li>
</ul>

<h3>Title: LUMA: Low-Dimension Unified Motion Alignment with Dual-Path Anchoring for Text-to-Motion Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Haozhe Jia, Wenshuo Chen, Yuqi Lin, Yang Yang, Lei Wang, Mang Ning, Bowen Tian, Songning Lai, Nanqian Jia, Yifan Chen, Yutao Yue</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25304">https://arxiv.org/abs/2509.25304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25304">https://arxiv.org/pdf/2509.25304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25304]] LUMA: Low-Dimension Unified Motion Alignment with Dual-Path Anchoring for Text-to-Motion Diffusion Model(https://arxiv.org/abs/2509.25304)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While current diffusion-based models, typically built on U-Net architectures, have shown promising results on the text-to-motion generation task, they still suffer from semantic misalignment and kinematic artifacts. Through analysis, we identify severe gradient attenuation in the deep layers of the network as a key bottleneck, leading to insufficient learning of high-level features. To address this issue, we propose \textbf{LUMA} (\textit{\textbf{L}ow-dimension \textbf{U}nified \textbf{M}otion \textbf{A}lignment}), a text-to-motion diffusion model that incorporates dual-path anchoring to enhance semantic alignment. The first path incorporates a lightweight MoCLIP model trained via contrastive learning without relying on external data, offering semantic supervision in the temporal domain. The second path introduces complementary alignment signals in the frequency domain, extracted from low-frequency DCT components known for their rich semantic content. These two anchors are adaptively fused through a temporal modulation mechanism, allowing the model to progressively transition from coarse alignment to fine-grained semantic refinement throughout the denoising process. Experimental results on HumanML3D and KIT-ML demonstrate that LUMA achieves state-of-the-art performance, with FID scores of 0.035 and 0.123, respectively. Furthermore, LUMA accelerates convergence by 1.4$\times$ compared to the baseline, making it an efficient and scalable solution for high-fidelity text-to-motion generation.</li>
<li><strong>摘要：</strong>尽管当前基于扩散的模型通常建立在U-NET体系结构上，但在文本到动作生成任务上显示出令人鼓舞的结果，但它们仍然遭受语义不一致和运动型文物的困扰。通过分析，我们将网络深层的严重梯度衰减视为关键瓶颈，导致对高级特征的学习不足。 To address this issue, we propose \textbf{LUMA} (\textit{\textbf{L}ow-dimension \textbf{U}nified \textbf{M}otion \textbf{A}lignment}), a text-to-motion diffusion model that incorporates dual-path anchoring to enhance semantic alignment.第一条路径结合了通过对比度学习训练的轻量级移动模型，而无需依赖外部数据，并在时间域中提供语义监督。第二个路径在频域中引入互补的对齐信号，该信号从低频DCT组件中提取，以其丰富的语义含量而闻名。这两个锚通过时间调制机制自适应地融合，从而使模型可以在整个剥离过程中逐步过渡到细粒度的语义细化。对HumanML3D和Kit-ML的实验结果表明，Luma可以达到最先进的性能，而FID得分分别为0.035和0.123。此外，Luma与基线相比，Luma的收敛量增加了1.4 $ \ times $，这使其成为高保真文本到动作生成的有效且可扩展的解决方案。</li>
</ul>

<h3>Title: Uncertainty-Aware Generative Oversampling Using an Entropy-Guided Conditional Variational Autoencoder</h3>
<ul>
<li><strong>Authors: </strong>Amirhossein Zare, Amirhessam Zare, Parmida Sadat Pezeshki, Herlock (SeyedAbolfazl)Rahimi, Ali Ebrahimi, Ignacio Vázquez-García, Leo Anthony Celi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25334">https://arxiv.org/abs/2509.25334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25334">https://arxiv.org/pdf/2509.25334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25334]] Uncertainty-Aware Generative Oversampling Using an Entropy-Guided Conditional Variational Autoencoder(https://arxiv.org/abs/2509.25334)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Class imbalance remains a major challenge in machine learning, especially for high-dimensional biomedical data where nonlinear manifold structures dominate. Traditional oversampling methods such as SMOTE rely on local linear interpolation, often producing implausible synthetic samples. Deep generative models like Conditional Variational Autoencoders (CVAEs) better capture nonlinear distributions, but standard variants treat all minority samples equally, neglecting the importance of uncertain, boundary-region examples emphasized by heuristic methods like Borderline-SMOTE and ADASYN. We propose Local Entropy-Guided Oversampling with a CVAE (LEO-CVAE), a generative oversampling framework that explicitly incorporates local uncertainty into both representation learning and data generation. To quantify uncertainty, we compute Shannon entropy over the class distribution in a sample's neighborhood: high entropy indicates greater class overlap, serving as a proxy for uncertainty. LEO-CVAE leverages this signal through two mechanisms: (i) a Local Entropy-Weighted Loss (LEWL) that emphasizes robust learning in uncertain regions, and (ii) an entropy-guided sampling strategy that concentrates generation in these informative, class-overlapping areas. Applied to clinical genomics datasets (ADNI and TCGA lung cancer), LEO-CVAE consistently improves classifier performance, outperforming both traditional oversampling and generative baselines. These results highlight the value of uncertainty-aware generative oversampling for imbalanced learning in domains governed by complex nonlinear structures, such as omics data.</li>
<li><strong>摘要：</strong>阶级失衡仍然是机器学习的主要挑战，尤其是对于非线性歧管结构占主导地位的高维生物医学数据。传统的过采样方法（例如SMOTE）依赖于局部线性插值，通常会产生难以置信的合成样本。深层生成模型，例如条件变异自动编码器（CVAE）更好地捕获非线性分布，但是标准变体平均对所有少数族裔样本进行了处理，从而忽略了不确定的，边界区域示例的重要性。我们建议使用CVAE（LEO-CVAE）的本地熵引导的过采样，这是一个生成的过采样框架，将局部不确定性明确地纳入表示形式学习和数据生成。为了量化不确定性，我们计算样本邻居中类分布的香农熵：高熵表示较大的类重叠，是不确定性的代理。 LEO-CVAE通过两种机制利用该信号：（i）局部熵加权损失（LEWL）强调在不确定区域的健壮学习，以及（ii）熵引导的采样策略，该策略集中在这些信息性的，多次交流的集体批评区中。 LEO-CVAE应用于临床基因组学数据集（ADNI和TCGA肺癌），始终提高分类器的性能，表现优于传统的过度采样和生成基线。这些结果突出了不确定性感知的生成过度采样的价值，该域中的学习不平衡学习，该域受复杂的非线性结构（例如OMICS数据）的影响。</li>
</ul>

<h3>Title: Let Physics Guide Your Protein Flows: Topology-aware Unfolding and Generation</h3>
<ul>
<li><strong>Authors: </strong>Yogesh Verma, Markus Heinonen, Vikas Garg</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25379">https://arxiv.org/abs/2509.25379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25379">https://arxiv.org/pdf/2509.25379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25379]] Let Physics Guide Your Protein Flows: Topology-aware Unfolding and Generation(https://arxiv.org/abs/2509.25379)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Protein structure prediction and folding are fundamental to understanding biology, with recent deep learning advances reshaping the field. Diffusion-based generative models have revolutionized protein design, enabling the creation of novel proteins. However, these methods often neglect the intrinsic physical realism of proteins, driven by noising dynamics that lack grounding in physical principles. To address this, we first introduce a physically motivated non-linear noising process, grounded in classical physics, that unfolds proteins into secondary structures (e.g., alpha helices, linear beta sheets) while preserving topological integrity--maintaining bonds, and preventing collisions. We then integrate this process with the flow-matching paradigm on SE(3) to model the invariant distribution of protein backbones with high fidelity, incorporating sequence information to enable sequence-conditioned folding and expand the generative capabilities of our model. Experimental results demonstrate that the proposed method achieves state-of-the-art performance in unconditional protein generation, producing more designable and novel protein structures while accurately folding monomer sequences into precise protein conformations.</li>
<li><strong>摘要：</strong>蛋白质结构的预测和折叠对于理解生物学至关重要，而最近的深度学习进展可以重塑该领域。基于扩散的生成模型已彻底改变了蛋白质设计，从而实现了新型蛋白质的创造。但是，这些方法通常忽略了蛋白质的内在物理现实主义，这是由于缺乏物理原理基础的尖锐动力学的驱动。为了解决这个问题，我们首先引入了基于古典物理学的物理动机的非线性尖锐过程，该过程将蛋白质展开为二级结构（例如，α螺旋，线性β床单），同时保持拓扑完整性 - 维护拓扑完整性 - 维护拓扑键 - 并预防碰撞。然后，我们将此过程与SE（3）上的流匹配范式集成在一起，以建模具有高保真度的蛋白质骨架的不变分布，并结合序列信息以启用序列条件条件的折叠并扩展我们模型的生成能力。实验结果表明，所提出的方法在无条件蛋白质的产生中实现了最先进的性能，产生了更具设计和新颖的蛋白质结构，同时将单体序列准确地折叠成精确的蛋白质构象。</li>
</ul>

<h3>Title: On the Shape of Latent Variables in a Denoising VAE-MoG: A Posterior Sampling-Based Study</h3>
<ul>
<li><strong>Authors: </strong>Fernanda Zapata Bascuñán</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25382">https://arxiv.org/abs/2509.25382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25382">https://arxiv.org/pdf/2509.25382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25382]] On the Shape of Latent Variables in a Denoising VAE-MoG: A Posterior Sampling-Based Study(https://arxiv.org/abs/2509.25382)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this work, we explore the latent space of a denoising variational autoencoder with a mixture-of-Gaussians prior (VAE-MoG), trained on gravitational wave data from event GW150914. To evaluate how well the model captures the underlying structure, we use Hamiltonian Monte Carlo (HMC) to draw posterior samples conditioned on clean inputs, and compare them to the encoder's outputs from noisy data. Although the model reconstructs signals accurately, statistical comparisons reveal a clear mismatch in the latent space. This shows that strong denoising performance doesn't necessarily mean the latent representations are reliable highlighting the importance of using posterior-based validation when evaluating generative models.</li>
<li><strong>摘要：</strong>在这项工作中，我们探索了deno的变异自动编码器的潜在空间，并通过gaussians先验（vae-mog）进行了训练，该空间对事件GW150914的重力波数据进行了训练。为了评估模型捕获基础结构的效果，我们使用汉密尔顿蒙特卡洛（HMC）绘制以干净输入为条件的后样品，并将它们与噪音数据中的编码器输出进行比较。尽管该模型可以准确地重建信号，但统计比较揭示了潜在空间中明显的不匹配。这表明，强大的降级性能并不一定意味着潜在表示可靠地强调了在评估生成模型时使用基于后验验证的重要性。</li>
</ul>

<h3>Title: Polychromic Objectives for Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Jubayer Ibn Hamid, Ifdita Hasan Orney, Ellen Xu, Chelsea Finn, Dorsa Sadigh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25424">https://arxiv.org/abs/2509.25424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25424">https://arxiv.org/pdf/2509.25424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25424]] Polychromic Objectives for Reinforcement Learning(https://arxiv.org/abs/2509.25424)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reinforcement learning fine-tuning (RLFT) is a dominant paradigm for improving pretrained policies for downstream tasks. These pretrained policies, trained on large datasets, produce generations with a broad range of promising but unrefined behaviors. Often, a critical failure mode of RLFT arises when policies lose this diversity and collapse into a handful of easily exploitable outputs. This convergence hinders exploration, which is essential for expanding the capabilities of the pretrained policy and for amplifying the benefits of test-time compute scaling. To address this, we introduce an objective for policy gradient methods that explicitly enforces the exploration and refinement of diverse generations, which we call a polychromic objective. We then show how proximal policy optimization (PPO) can be adapted to optimize this objective. Our method (1) employs vine sampling to collect on-policy rollouts and (2) modifies the advantage function to reflect the advantage under our new objective. Experiments on BabyAI, Minigrid, and Algorithmic Creativity show that our method improves success rates by reliably solving a larger set of environment configurations and generalizes better under large perturbations. Moreover, when given multiple attempts in pass@$k$ experiments, the policy achieves substantially higher coverage, demonstrating its ability to maintain and exploit a diverse repertoire of strategies.</li>
<li><strong>摘要：</strong>增强学习微调（RLFT）是改善审计政策的下游任务的主要范式。这些预处理的政策，在大型数据集上训练，生产具有广泛有希望但未精制的行为的几代人。通常，当政策失去这种多样性并崩溃成少数易于利用的输出时，会产生重大的RLFT故障模式。这种融合阻碍了探索，这对于扩大验证策略的能力以及扩大测试时间计算缩放的好处至关重要。为了解决这个问题，我们为政策梯度方法介绍了一个目标，该方法明确执行了对不同世代的探索和改进，我们称之为多色粒子目标。然后，我们展示如何对近端策略优化（PPO）进行调整以优化此目标。我们的方法（1）采用藤蔓采样来收集额外的推广，（2）修改优势功能，以反映我们新目标下的优势。关于Babyai，Minigrid和算法创造力的实验表明，我们的方法通过可靠地求解较大的环境配置来提高成功率，并在大型扰动下更好地概括。此外，当在Pass@$ K $实验中进行多次尝试时，该政策实现了更高的覆盖范围，这表明了其维护和利用各种策略的能力。</li>
</ul>

<h3>Title: Norm-Q: Effective Compression Method for Hidden Markov Models in Neuro-Symbolic Applications</h3>
<ul>
<li><strong>Authors: </strong>Hanyuan Gao, Xiaoxuan Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25439">https://arxiv.org/abs/2509.25439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25439">https://arxiv.org/pdf/2509.25439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25439]] Norm-Q: Effective Compression Method for Hidden Markov Models in Neuro-Symbolic Applications(https://arxiv.org/abs/2509.25439)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Hidden Markov models (HMM) are commonly used in generation tasks and have demonstrated strong capabilities in neuro-symbolic applications for the Markov property. These applications leverage the strengths of neural networks and symbolic reasoning to create robust and interpretable AI systems. However, they may inherit and amplify the shortcomings of both approaches. Both components require dense computation and data transfer, and their communication further hinders performance. This paper proposes Norm-Q, a normalized linear quantization approach for compressing probabilistic symbolic models, such as HMMs. We reduce the bit width of the data with minimal impact, thereby alleviating memory and bandwidth stress and enabling deployment on potential custom hardware. Our method introduces a normalized quantization-aware expectation maximization process for probabilistic model training. The experimental results show that Norm-Q achieves a higher compression rate with reasonable score loss compared to traditional quantization methods. In the case of the constrained generation task of large language models, we successfully quantize an HMM of 4096 hidden states to 8 bits without loss and, at most, 3 bits with acceptable loss. Notably, the Norm-Q method can achieve a compression rate of 99% for the weights of the HMM. The code is open source at this https URL.</li>
<li><strong>摘要：</strong>隐藏的马尔可夫模型（HMM）通常用于生成任务，并在Markov属性的神经符号应用中表现出很强的功能。这些应用利用神经网络和象征性推理的优势来创建强大而可解释的AI系统。但是，它们可能会继承和扩大两种方法的缺点。这两个组件都需要密集的计算和数据传输，并且它们的通信进一步阻碍了性能。本文提出了Norm-Q，这是一种用于压缩概率符号模型（例如HMMS）的归一化线性量化方法。我们会以最小的影响减少数据的位宽度，从而减轻记忆力和带宽压力，并在潜在的自定义硬件上部署。我们的方法引入了概率模型训练的归一化量化期望最大化过程。实验结果表明，与传统量化方法相比，Norm-Q具有更高的压缩率，得分损失合理。在大型语言模型的限制生成任务的情况下，我们将4096个隐藏状态的HMM成功量化为8位而无需损失，最多只能接受3位具有可接受的损失。值得注意的是，Norm-Q方法可以达到HMM重量的压缩率为99％。该代码是此HTTPS URL上的开源。</li>
</ul>

<h3>Title: Infrastructure Sensor-enabled Vehicle Data Generation using Multi-Sensor Fusion for Proactive Safety Applications at Work Zone</h3>
<ul>
<li><strong>Authors: </strong>Suhala Rabab Saba, Sakib Khan, Minhaj Uddin Ahmad, Jiahe Cao, Mizanur Rahman, Li Zhao, Nathan Huynh, Eren Erman Ozguven</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25452">https://arxiv.org/abs/2509.25452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25452">https://arxiv.org/pdf/2509.25452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25452]] Infrastructure Sensor-enabled Vehicle Data Generation using Multi-Sensor Fusion for Proactive Safety Applications at Work Zone(https://arxiv.org/abs/2509.25452)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Infrastructure-based sensing and real-time trajectory generation show promise for improving safety in high-risk roadway segments such as work zones, yet practical deployments are hindered by perspective distortion, complex geometry, occlusions, and costs. This study tackles these barriers by integrating roadside camera and LiDAR sensors into a cosimulation environment to develop a scalable, cost-effective vehicle detection and localization framework, and employing a Kalman Filter-based late fusion strategy to enhance trajectory consistency and accuracy. In simulation, the fusion algorithm reduced longitudinal error by up to 70 percent compared to individual sensors while preserving lateral accuracy within 1 to 3 meters. Field validation in an active work zone, using LiDAR, a radar-camera rig, and RTK-GPS as ground truth, demonstrated that the fused trajectories closely match real vehicle paths, even when single-sensor data are intermittent or degraded. These results confirm that KF based sensor fusion can reliably compensate for individual sensor limitations, providing precise and robust vehicle tracking capabilities. Our approach thus offers a practical pathway to deploy infrastructure-enabled multi-sensor systems for proactive safety measures in complex traffic environments.</li>
<li><strong>摘要：</strong>基于基础设施的感应和实时轨迹产生表明有望改善高风险道路领域的安全性，例如工作区，但实际部署受到透视扭曲，复杂的几何形状，遮挡和成本的阻碍。这项研究通过将路边摄像头和激光镜传感器集成到共同模拟环境中，以开发可扩展，具有成本效益的车辆检测和本地化框架，并采用基于卡尔曼过滤器的晚期融合策略来提高轨迹一致性和准确性，从而解决了这些障碍。在模拟中，与单个传感器相比，融合算法将纵向误差降低了70％，同时保留了1至3米以内的横向精度。使用LIDAR，雷达相机钻机和RTK-GP作为地面真相，在活跃工作区中的现场验证表明，即使单个传感器数据是间歇性数据或降级，熔融轨迹与真实的车辆路径紧密匹配。这些结果证实，基于KF的传感器融合可以可靠地补偿各个传感器的限制，从而提供精确且健壮的车辆跟踪功能。因此，我们的方法提供了一种实用的途径，可以在复杂的交通环境中部署支持基础架构的多传感器系统来主动安全措施。</li>
</ul>

<h3>Title: Translation from Wearable PPG to 12-Lead ECG</h3>
<ul>
<li><strong>Authors: </strong>Hui Ji, Wei Gao, Pengfei Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25480">https://arxiv.org/abs/2509.25480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25480">https://arxiv.org/pdf/2509.25480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25480]] Translation from Wearable PPG to 12-Lead ECG(https://arxiv.org/abs/2509.25480)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The 12-lead electrocardiogram (ECG) is the gold standard for cardiovascular monitoring, offering superior diagnostic granularity and specificity compared to photoplethysmography (PPG). However, existing 12-lead ECG systems rely on cumbersome multi-electrode setups, limiting sustained monitoring in ambulatory settings, while current PPG-based methods fail to reconstruct multi-lead ECG due to the absence of inter-lead constraints and insufficient modeling of spatial-temporal dependencies across leads. To bridge this gap, we introduce P2Es, an innovative demographic-aware diffusion framework designed to generate clinically valid 12-lead ECG from PPG signals via three key innovations. Specifically, in the forward process, we introduce frequency-domain blurring followed by temporal noise interference to simulate real-world signal distortions. In the reverse process, we design a temporal multi-scale generation module followed by frequency deblurring. In particular, we leverage KNN-based clustering combined with contrastive learning to assign affinity matrices for the reverse process, enabling demographic-specific ECG translation. Extensive experimental results show that P2Es outperforms baseline models in 12-lead ECG reconstruction.</li>
<li><strong>摘要：</strong>12导潜在的心电图（ECG）是心血管监测的金标准，与光摄影学（PPG）相比，具有出色的诊断粒度和特异性。但是，现有的12铅ECG系统依赖于繁琐的多电极设置，从而限制了卧床环境中的持续监控，而当前基于PPG的方法由于缺乏跨线索的空间依赖性的不足而无法重建多铅ECG。为了弥合这一差距，我们介绍了P2ES，这是一个创新的人口统计学扩散框架，旨在通过三个关键创新从PPG信号生成临床上有效的12铅ECG。具体而言，在正向过程中，我们引入了频域模糊，然后进行时间噪声干扰以模拟现实世界的信号失真。在反向过程中，我们设计了一个时间多尺度生成模块，然后是频率去膨胀。特别是，我们利用基于KNN的聚类与对比度学习相结合，为反向过程分配亲和力矩阵，从而实现人口统计学特定的ECG翻译。广泛的实验结果表明，P2ES在12铅ECG重建中的表现优于基线模型。</li>
</ul>

<h3>Title: Flow Matching with Semidiscrete Couplings</h3>
<ul>
<li><strong>Authors: </strong>Alireza Mousavi-Hosseini, Stephen Y. Zhang, Michal Klein, Marco Cuturi</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25519">https://arxiv.org/abs/2509.25519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25519">https://arxiv.org/pdf/2509.25519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25519]] Flow Matching with Semidiscrete Couplings(https://arxiv.org/abs/2509.25519)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Flow models parameterized as time-dependent velocity fields can generate data from noise by integrating an ODE. These models are often trained using flow matching, i.e. by sampling random pairs of noise and target points $(\mathbf{x}_0,\mathbf{x}_1)$ and ensuring that the velocity field is aligned, on average, with $\mathbf{x}_1-\mathbf{x}_0$ when evaluated along a segment linking $\mathbf{x}_0$ to $\mathbf{x}_1$. While these pairs are sampled independently by default, they can also be selected more carefully by matching batches of $n$ noise to $n$ target points using an optimal transport (OT) solver. Although promising in theory, the OT flow matching (OT-FM) approach is not widely used in practice. Zhang et al. (2025) pointed out recently that OT-FM truly starts paying off when the batch size $n$ grows significantly, which only a multi-GPU implementation of the Sinkhorn algorithm can handle. Unfortunately, the costs of running Sinkhorn can quickly balloon, requiring $O(n^2/\varepsilon^2)$ operations for every $n$ pairs used to fit the velocity field, where $\varepsilon$ is a regularization parameter that should be typically small to yield better results. To fulfill the theoretical promises of OT-FM, we propose to move away from batch-OT and rely instead on a semidiscrete formulation that leverages the fact that the target dataset distribution is usually of finite size $N$. The SD-OT problem is solved by estimating a dual potential vector using SGD; using that vector, freshly sampled noise vectors at train time can then be matched with data points at the cost of a maximum inner product search (MIPS). Semidiscrete FM (SD-FM) removes the quadratic dependency on $n/\varepsilon$ that bottlenecks OT-FM. SD-FM beats both FM and OT-FM on all training metrics and inference budget constraints, across multiple datasets, on unconditional/conditional generation, or when using mean-flow models.</li>
<li><strong>摘要：</strong>流量模型参数为时间依赖性速度字段可以通过集成ode来从噪声中生成数据。这些模型通常是使用流量匹配的训练，即通过对随机的噪声对和目标点$（\ Mathbf {x} _0，\ Mathbf {x} _1）$进行培训，并确保速度字段平均与$ \ \ \ \ \ \ \ _1- \ _1- \ mathbf {$ sement Blink contect $ \ mathbf {x} _0 $ to $ \ mathbf {x} _1 $。虽然默认情况下对这些对进行独立采样，但也可以通过使用最佳传输（OT）求解器将$ n $噪声与$ n $目标点匹配到$ n $目标点的批次来更仔细地选择它们。尽管从理论上讲有希望，但OT流量匹配（OT-FM）方法并未在实践中广泛使用。张等。 （2025）最近指出，当批处理大小$ n $显着增长时，OT-FM真正开始还清，这只有Sinkhorn算法的多GPU实现才能处理。不幸的是，每条用于适合速度场的$ n $ pairs的$ o（n^2/\ varepsilon^2）$操作的成本可以快速气球，其中$ \ varepsilon $是一个正则化参数，通常应该很小，以产生更好的结果。为了履行OT-FM的理论承诺，我们建议您远离批处理，而要依靠半零散的公式，该公式利用了目标数据集分布通常为有限的尺寸$ n $的事实。通过使用SGD估算双重电位向量来解决SD-OT问题。使用该矢量，可以在火车时间进行新鲜采样的噪声向量与数据点匹配，以最大的内部产品搜索（MIPS）为代价。 Semidiscrete FM（SD-FM）删除了对瓶装ot-fm的$ n/\ varepsilon $的二次依赖。 SD-FM在所有培训指标和推理预算约束，多个数据集，无条件/条件生成或使用均值流模型时都击败FM和OT-FM。</li>
</ul>

<h3>Title: Steering an Active Learning Workflow Towards Novel Materials Discovery via Queue Prioritization</h3>
<ul>
<li><strong>Authors: </strong>Marcus Schwarting, Logan Ward, Nathaniel Hudson, Xiaoli Yan, Ben Blaiszik, Santanu Chaudhuri, Eliu Huerta, Ian Foster</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25538">https://arxiv.org/abs/2509.25538</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25538">https://arxiv.org/pdf/2509.25538</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25538]] Steering an Active Learning Workflow Towards Novel Materials Discovery via Queue Prioritization(https://arxiv.org/abs/2509.25538)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative AI poses both opportunities and risks for solving inverse design problems in the sciences. Generative tools provide the ability to expand and refine a search space autonomously, but do so at the cost of exploring low-quality regions until sufficiently fine tuned. Here, we propose a queue prioritization algorithm that combines generative modeling and active learning in the context of a distributed workflow for exploring complex design spaces. We find that incorporating an active learning model to prioritize top design candidates can prevent a generative AI workflow from expending resources on nonsensical candidates and halt potential generative model decay. For an existing generative AI workflow for discovering novel molecular structure candidates for carbon capture, our active learning approach significantly increases the number of high-quality candidates identified by the generative model. We find that, out of 1000 novel candidates, our workflow without active learning can generate an average of 281 high-performing candidates, while our proposed prioritization with active learning can generate an average 604 high-performing candidates.</li>
<li><strong>摘要：</strong>生成的AI既带来了解决科学中的逆设计问题的机会和风险。生成工具提供了自主扩展和完善搜索空间的能力，但要以探索低质量区域为代价，直到进行充分调整。在这里，我们提出了一种队列优先级算法，该算法在分布式工作流程的背景下结合了生成建模和主动学习，以探索复杂的设计空间。我们发现，将主动学习模型合并以优先考虑顶级设计候选者，可以防止生成的AI工作流程在荒谬的候选者上花费资源，并停止潜在的生成模型衰减。对于现有的生成AI工作流，用于发现碳捕获的新分子结构候选物，我们的主动学习方法显着增加了生成模型确定的高质量候选者的数量。我们发现，在1000名新型候选人中，我们没有主动学习的工作流程平均可以产生281个高性能的候选人，而我们提出的主动学习优先级可以产生平均604个高性能的候选人。</li>
</ul>

<h3>Title: Machine Learning Algorithms for Improving Black Box Optimization Solvers</h3>
<ul>
<li><strong>Authors: </strong>Morteza Kimiaei, Vyacheslav Kungurtsev</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25592">https://arxiv.org/abs/2509.25592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25592">https://arxiv.org/pdf/2509.25592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25592]] Machine Learning Algorithms for Improving Black Box Optimization Solvers(https://arxiv.org/abs/2509.25592)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Black-box optimization (BBO) addresses problems where objectives are accessible only through costly queries without gradients or explicit structure. Classical derivative-free methods -- line search, direct search, and model-based solvers such as Bayesian optimization -- form the backbone of BBO, yet often struggle in high-dimensional, noisy, or mixed-integer settings. Recent advances use machine learning (ML) and reinforcement learning (RL) to enhance BBO: ML provides expressive surrogates, adaptive updates, meta-learning portfolios, and generative models, while RL enables dynamic operator configuration, robustness, and meta-optimization across tasks. This paper surveys these developments, covering representative algorithms such as NNs with the modular model-based optimization framework (mlrMBO), zeroth-order adaptive momentum methods (ZO-AdaMM), automated BBO (ABBO), distributed block-wise optimization (DiBB), partition-based Bayesian optimization (SPBOpt), the transformer-based optimizer (B2Opt), diffusion-model-based BBO, surrogate-assisted RL for differential evolution (Surr-RLDE), robust BBO (RBO), coordinate-ascent model-based optimization with relative entropy (CAS-MORE), log-barrier stochastic gradient descent (LB-SGD), policy improvement with black-box (PIBB), and offline Q-learning with Mamba backbones (Q-Mamba). We also review benchmark efforts such as the NeurIPS 2020 BBO Challenge and the MetaBox framework. Overall, we highlight how ML and RL transform classical inexact solvers into more scalable, robust, and adaptive frameworks for real-world optimization.</li>
<li><strong>摘要：</strong>黑盒优化（BBO）解决了仅通过无梯度或明确结构的昂贵查询才能访问目标的问题。经典的无衍生化方法 - 线搜索，直接搜索和基于模型的求解器，例如贝叶斯优化 - 构成了BBO的骨干，但经常在高维，嘈杂或混合构成设置中挣扎。最近的进步使用机器学习（ML）和增强学习（RL）来增强BBO：ML提供表达的替代物，自适应更新，元学习组合和生成模型，而RL可以启用动态操作员的配置，稳健性和跨任务的元优化。本文调查了这些发展，涵盖了代表性的算法，例如具有基于模块模型的优化框架（MLRMBO），零阶自适应动量方法（ZO-ADAMM），自动化BBO（ABBO），基于分布的基于贝尼斯的贝叶斯优化（b2基于扩散模型的BBO，用于差异进化的替代辅助RL（SURS-RLDE），强大的BBO（RBO），基于坐标的基于坐标的模型优化，具有相对熵（CAS-MORE），对数伴侣随机梯度下降（LB-SGD），策略改进，Black-Box（PIBB）（PIBB）和Offline Q-Mambs Q-Mambning（Q-Mamb）（QMAMBNING）（QMAMB）（QMAMBANNING）（QMAMBA）（QMAMBA）（QMAMA）（QMABA）（QMA）（QMA）（QMA）。我们还审查了基准努力，例如2020年神经BBO挑战和Metabox框架。总体而言，我们强调了ML和RL如何将经典的不精确求解器转化为实现现实世界优化的更可扩展，健壮和自适应框架。</li>
</ul>

<h3>Title: Unsupervised Detection of Spatiotemporal Anomalies in PMU Data Using Transformer-Based BiGAN</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Imran Hossain, Jignesh Solanki, Sarika Khushlani Solanki</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25612">https://arxiv.org/abs/2509.25612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25612">https://arxiv.org/pdf/2509.25612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25612]] Unsupervised Detection of Spatiotemporal Anomalies in PMU Data Using Transformer-Based BiGAN(https://arxiv.org/abs/2509.25612)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Ensuring power grid resilience requires the timely and unsupervised detection of anomalies in synchrophasor data streams. We introduce T-BiGAN, a novel framework that integrates window-attention Transformers within a bidirectional Generative Adversarial Network (BiGAN) to address this challenge. Its self-attention encoder-decoder architecture captures complex spatio-temporal dependencies across the grid, while a joint discriminator enforces cycle consistency to align the learned latent space with the true data distribution. Anomalies are flagged in real-time using an adaptive score that combines reconstruction error, latent space drift, and discriminator confidence. Evaluated on a realistic hardware-in-the-loop PMU benchmark, T-BiGAN achieves an ROC-AUC of 0.95 and an average precision of 0.996, significantly outperforming leading supervised and unsupervised methods. It shows particular strength in detecting subtle frequency and voltage deviations, demonstrating its practical value for live, wide-area monitoring without relying on manually labeled fault data.</li>
<li><strong>摘要：</strong>确保电网弹性需要及时，无监督的检测，对同步声数据流中的异常。我们介绍了T-Bigan，这是一个新颖的框架，将窗口发入变压器集成到双向生成对抗网络（BIGAN）中以应对这一挑战。它的自我发作编码器架构结构捕获了整个网格上的复杂时空依赖性，而联合歧视者则强制执行循环一致性，以使学习的潜在空间与真实的数据分布保持一致。使用自适应分数将重构误差，潜在空间漂移和歧视器信心的自适应分数实时标记。 T-bigan在现实的硬件PMU基准测试中进行了评估，其ROC-AUC的ROC-AUC为0.95，平均精度为0.996，表现明显优于领先的监督和无人监督的方法。它显示出在检测微妙的频率和电压偏差方面的特殊强度，证明了其实用值的实用值，而无需依赖手动标记的故障数据。</li>
</ul>

<h3>Title: LMOD+: A Comprehensive Multimodal Dataset and Benchmark for Developing and Evaluating Multimodal Large Language Models in Ophthalmology</h3>
<ul>
<li><strong>Authors: </strong>Zhenyue Qin, Yang Liu, Yu Yin, Jinyu Ding, Haoran Zhang, Anran Li, Dylan Campbell, Xuansheng Wu, Ke Zou, Tiarnan D. L. Keenan, Emily Y. Chew, Zhiyong Lu, Yih-Chung Tham, Ninghao Liu, Xiuzhen Zhang, Qingyu Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25620">https://arxiv.org/abs/2509.25620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25620">https://arxiv.org/pdf/2509.25620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25620]] LMOD+: A Comprehensive Multimodal Dataset and Benchmark for Developing and Evaluating Multimodal Large Language Models in Ophthalmology(https://arxiv.org/abs/2509.25620)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Vision-threatening eye diseases pose a major global health burden, with timely diagnosis limited by workforce shortages and restricted access to specialized care. While multimodal large language models (MLLMs) show promise for medical image interpretation, advancing MLLMs for ophthalmology is hindered by the lack of comprehensive benchmark datasets suitable for evaluating generative models. We present a large-scale multimodal ophthalmology benchmark comprising 32,633 instances with multi-granular annotations across 12 common ophthalmic conditions and 5 imaging modalities. The dataset integrates imaging, anatomical structures, demographics, and free-text annotations, supporting anatomical structure recognition, disease screening, disease staging, and demographic prediction for bias evaluation. This work extends our preliminary LMOD benchmark with three major enhancements: (1) nearly 50% dataset expansion with substantial enlargement of color fundus photography; (2) broadened task coverage including binary disease diagnosis, multi-class diagnosis, severity classification with international grading standards, and demographic prediction; and (3) systematic evaluation of 24 state-of-the-art MLLMs. Our evaluations reveal both promise and limitations. Top-performing models achieved ~58% accuracy in disease screening under zero-shot settings, and performance remained suboptimal for challenging tasks like disease staging. We will publicly release the dataset, curation pipeline, and leaderboard to potentially advance ophthalmic AI applications and reduce the global burden of vision-threatening diseases.</li>
<li><strong>摘要：</strong>威胁性的眼病造成了重大的全球健康负担，及时诊断受到劳动力短缺的限制和受到专业护理的限制。虽然多模式大语模型（MLLM）显示出医学图像解释的希望，但由于缺乏适合评估生成模型的全面基准数据集，因此阻碍了眼科的MLLM。我们提出了一个大规模的多模式眼科基准，其中包含32,633个实例，并在12个常见的眼科条件和5种成像方式中具有多种晶体注释。数据集整合了成像，解剖结构，人口统计和自由文本注释，以支持解剖结构识别，疾病筛查，疾病分期和人口统计预测，以进行偏见评估。这项工作扩大了我们的初步LMOD基准，并具有三个主要的增强功能：（1）近50％的数据集扩展，并大量扩大了颜色的粉底摄影； （2）扩大任务覆盖范围，包括二元疾病诊断，多类诊断，具有国际评分标准的严重性分类和人口预测； （3）对24个最先进的MLLM的系统评估。我们的评估揭示了诺言和局限性。在零射门设置下疾病筛查中，表现最佳模型的精度约为58％，并且在诸如疾病分期之类的具有挑战性的任务中，表现仍然是最佳的。我们将公开发布数据集，策展管道和排行榜，以潜在地推进眼科AI应用程序，并减轻威胁性疾病的全球负担。</li>
</ul>

<h3>Title: OmniDFA: A Unified Framework for Open Set Synthesis Image Detection and Few-Shot Attribution</h3>
<ul>
<li><strong>Authors: </strong>Shiyu Wu, Shuyan Li, Jing Li, Jing Liu, Yequan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25682">https://arxiv.org/abs/2509.25682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25682">https://arxiv.org/pdf/2509.25682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25682]] OmniDFA: A Unified Framework for Open Set Synthesis Image Detection and Few-Shot Attribution(https://arxiv.org/abs/2509.25682)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>AI-generated image (AIGI) detection and source model attribution remain central challenges in combating deepfake abuses, primarily due to the structural diversity of generative models. Current detection methods are prone to overfitting specific forgery traits, whereas source attribution offers a robust alternative through fine-grained feature discrimination. However, synthetic image attribution remains constrained by the scarcity of large-scale, well-categorized synthetic datasets, limiting its practicality and compatibility with detection systems. In this work, we propose a new paradigm for image attribution called open-set, few-shot source identification. This paradigm is designed to reliably identify unseen generators using only limited samples, making it highly suitable for real-world application. To this end, we introduce OmniDFA (Omni Detector and Few-shot Attributor), a novel framework for AIGI that not only assesses the authenticity of images, but also determines the synthesis origins in a few-shot manner. To facilitate this work, we construct OmniFake, a large class-aware synthetic image dataset that curates $1.17$ M images from $45$ distinct generative models, substantially enriching the foundational resources for research on both AIGI detection and attribution. Experiments demonstrate that OmniDFA exhibits excellent capability in open-set attribution and achieves state-of-the-art generalization performance on AIGI detection. Our dataset and code will be made available.</li>
<li><strong>摘要：</strong>AI生成的图像（AIGI）检测和源模型归因仍然是对抗深层滥用的核心挑战，这主要是由于生成模型的结构多样性。当前的检测方法容易适合特定的伪造性状，而源归因通过细粒度的特征歧视提供了强大的替代方案。但是，合成图像归因仍然受到大规模，良好分类合成数据集的稀缺性的限制，从而限制了其实用性和与检测系统的兼容性。在这项工作中，我们为图像归因提出了一个新的范式，称为开放设置，几乎没有弹药源标识。该范式旨在仅使用有限的样本可靠地识别看不见的发电机，使其非常适合现实世界应用。为此，我们介绍了Omnidfa（Omni检测器和少量归因），这是AIGI的新型框架，不仅评估了图像的真实性，而且还以几种方式确定了合成起源。为了促进这项工作，我们构建了Omnifake，这是一种大型的班级合成图像数据集，它从$ 45 $ $ 45 $不同的生成模型中策划了$ 1.17 $ M的图像，从而实质上丰富了有关AIGI检测和归因的研究基础资源。实验表明，Omnidfa在开放式归因方面具有出色的能力，并在AIGI检测中实现了最新的概括性能。我们的数据集和代码将提供。</li>
</ul>

<h3>Title: Minimalist Explanation Generation and Circuit Discovery</h3>
<ul>
<li><strong>Authors: </strong>Pirzada Suhail, Aditya Anand, Amit Sethi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25686">https://arxiv.org/abs/2509.25686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25686">https://arxiv.org/pdf/2509.25686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25686]] Minimalist Explanation Generation and Circuit Discovery(https://arxiv.org/abs/2509.25686)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Machine learning models, by virtue of training, learn a large repertoire of decision rules for any given input, and any one of these may suffice to justify a prediction. However, in high-dimensional input spaces, such rules are difficult to identify and interpret. In this paper, we introduce an activation-matching based approach to generate minimal and faithful explanations for the decisions of pre-trained image classifiers. We aim to identify minimal explanations that not only preserve the model's decision but are also concise and human-readable. To achieve this, we train a lightweight autoencoder to produce binary masks that learns to highlight the decision-wise critical regions of an image while discarding irrelevant background. The training objective integrates activation alignment across multiple layers, consistency at the output label, priors that encourage sparsity, and compactness, along with a robustness constraint that enforces faithfulness. The minimal explanations so generated also lead us to mechanistically interpreting the model internals. In this regard we also introduce a circuit readout procedure wherein using the explanation's forward pass and gradients, we identify active channels and construct a channel-level graph, scoring inter-layer edges by ingress weight magnitude times source activation and feature-to-class links by classifier weight magnitude times feature activation. Together, these contributions provide a practical bridge between minimal input-level explanations and a mechanistic understanding of the internal computations driving model decisions.</li>
<li><strong>摘要：</strong>通过培训，机器学习模型为任何给定输入学习了大量的决策规则曲目，其中任何一个都足以证明预测是合理的。但是，在高维输入空间中，这种规则很难识别和解释。在本文中，我们介绍了一种基于激活匹配的方法，以为预训练的图像分类器的决策生成最小和忠实的解释。我们的目的是确定最小的解释，不仅保留了模型的决定，而且是简洁而可读的。为了实现这一目标，我们训练轻巧的自动编码器生产二进制面具，以学会突出图像的决策关键区域，同时丢弃无关紧要的背景。训练目标整合了多个层之间的激活对齐，输出标签的一致性，鼓励稀疏性和紧凑性的先验以及强大的限制，从而实现忠诚。如此生成的最小解释也使我们能够机械地解释模型内部。在这方面，我们还引入了一个电路读数过程，其中使用说明的前向通过和梯度，我们识别有效的通道并构建通道级别的图，并通过Ingress重量幅度量源激活和分类器重量量量尺度特征时间激活来得分层间层。这些贡献共同提供了最小的输入级解释与对内部计算驱动模型决策的机械理解之间的实用桥梁。</li>
</ul>

<h3>Title: A Unified Probabilistic Framework for Dictionary Learning with Parsimonious Activation</h3>
<ul>
<li><strong>Authors: </strong>Zihui Zhao, Yuanbo Tang, Jieyu Ren, Xiaoping Zhang, Yang Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25690">https://arxiv.org/abs/2509.25690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25690">https://arxiv.org/pdf/2509.25690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25690]] A Unified Probabilistic Framework for Dictionary Learning with Parsimonious Activation(https://arxiv.org/abs/2509.25690)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Dictionary learning is traditionally formulated as an $L_1$-regularized signal reconstruction problem. While recent developments have incorporated discriminative, hierarchical, or generative structures, most approaches rely on encouraging representation sparsity over individual samples that overlook how atoms are shared across samples, resulting in redundant and sub-optimal dictionaries. We introduce a parsimony promoting regularizer based on the row-wise $L_\infty$ norm of the coefficient matrix. This additional penalty encourages entire rows of the coefficient matrix to vanish, thereby reducing the number of dictionary atoms activated across the dataset. We derive the formulation from a probabilistic model with Beta-Bernoulli priors, which provides a Bayesian interpretation linking the regularization parameters to prior distributions. We further establish theoretical calculation for optimal hyperparameter selection and connect our formulation to both Minimum Description Length, Bayesian model selection and pathlet learning. Extensive experiments on benchmark datasets demonstrate that our method achieves substantially improved reconstruction quality (with a 20\% reduction in RMSE) and enhanced representation sparsity, utilizing fewer than one-tenth of the available dictionary atoms, while empirically validating our theoretical analysis.</li>
<li><strong>摘要：</strong>传统上，词典学习是作为$ L_1 $登记的信号重建问题的表述。尽管最近的发展已结合了歧视性，分层或生成结构，但大多数方法都依赖于鼓励对单个样本的稀疏性稀疏性，这些样本忽略了忽略原子在样本中如何共享的原子，从而导致冗余和亚最佳词典。我们基于系数矩阵的行$ l_ \ infty $ norm介绍了一个简约的促进正常化程序。这种额外的惩罚鼓励系数矩阵的整行消失，从而减少了整个数据集激活的字典原子的数量。我们从beta-bernoulli先验的概率模型中得出公式，该模型提供了贝叶斯解释，将正则化参数与先前的分布联系起来。我们进一步建立了理论计算，用于最佳的高参数选择，并将我们的公式连接到最小描述长度，贝叶斯模型选择和Pathlet学习。基准数据集的广泛实验表明，我们的方法可实现重大改善的重建质量（RMSE降低了20 \％）和增强的表示稀疏性，使用了少于可用的词典原子的十分之一，而经验上可以验证我们的理论分析。</li>
</ul>

<h3>Title: AIMCoT: Active Information-driven Multimodal Chain-of-Thought for Vision-Language Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Xiping Li, Jianghong Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25699">https://arxiv.org/abs/2509.25699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25699">https://arxiv.org/pdf/2509.25699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25699]] AIMCoT: Active Information-driven Multimodal Chain-of-Thought for Vision-Language Reasoning(https://arxiv.org/abs/2509.25699)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal Chain-of-Thought (CoT) has emerged as a powerful technique for enhancing the vision-language reasoning with interleaved information. However, existing methods often rely on simplistic heuristics for constructing interleaved CoT, typically depending on attention maps, which our empirical analysis reveals can be unreliable. What's more, the shortcomings of their passive and purposeless selection strategies and their arbitrary triggering mechanisms in capturing the model's cognitive need for information are further amplified. In this paper, we propose \textbf{AIMCoT}, an \textbf{A}ctive \textbf{I}nformation-driven \textbf{M}ulti-modal \textbf{C}hain-\textbf{o}f-\textbf{T}hought framework that addresses these fundamental limitations. AIMCoT introduces three synergistic components: (1) \textbf{Context-enhanced Attention-map Generation (CAG)}, which mitigates the text-vision granularity imbalance, thereby producing more reliable attention maps as a foundation. (2) \textbf{Active Visual Probing (AVP)}, which replaces passive selection with a proactive, goal-oriented strategy grounded in information theory to select image regions that help answer the questions maximally. (3) \textbf{Dynamic Attention-shifting Trigger (DAT)}, which intelligently determines the optimal moments to insert visual information by monitoring the model's text-to-vision attention shifts. Extensive experiments on three challenging benchmarks demonstrate that AIMCoT significantly outperforms state-of-the-art methods across different settings. By actively foraging for information and dynamically structuring its reasoning process, AIMCoT represents a critical step towards more robust, effective, and human-like multimodal reasoning. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>多模式链（COT）已成为一种强大的技术，可以通过交织信息来增强视觉推理。但是，现有方法通常依赖于简单的启发式方法来构建交织的COT，通常取决于注意图，我们的经验分析表明，这可能是不可靠的。更重要的是，他们的被动和无目的的选择策略的缺点及其在捕获模型对信息认知需求时的任意触发机制得到了进一步扩大。在本文中，我们提出了\ textbf {aimcot}，一个\ textbf {a} ctive \ textbf {i} nformation-driend-drive \ textbf {m} ulti-modal \ textbf {c} c} hain- \ textbf { AimCot引入了三个协同组件：（1）\ textbf {上下文增强注意映射生成（CAG）}，这减轻了文本视觉粒度不平衡，从而产生了更多可靠的注意力图作为基础。 （2）\ textbf {活动视觉探测（AVP）}，它用积极的，面向目标的策略代替了以信息理论为基础的，以目标为目标的策略，以选择有助于最大程度地回答问题的图像区域。 （3）\ textbf {动态注意力转移触发器（dat）}，它可以智能地确定最佳矩，以通过监视模型的文本到视觉注意力转移来插入视觉信息。对三个具有挑战性的基准测试的广泛实验表明，Aimcot在不同环境中的最先进方法明显优于最先进的方法。通过积极觅食信息并动态构建其推理过程，Aimcot代表了朝着更健壮，有效和类似人类的多模式推理迈出的关键一步。我们的代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: How Diffusion Models Memorize</h3>
<ul>
<li><strong>Authors: </strong>Juyeop Kim, Songkuk Kim, Jong-Seok Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25705">https://arxiv.org/abs/2509.25705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25705">https://arxiv.org/pdf/2509.25705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25705]] How Diffusion Models Memorize(https://arxiv.org/abs/2509.25705)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite their success in image generation, diffusion models can memorize training data, raising serious privacy and copyright concerns. Although prior work has sought to characterize, detect, and mitigate memorization, the fundamental question of why and how it occurs remains unresolved. In this paper, we revisit the diffusion and denoising process and analyze latent space dynamics to address the question: "How do diffusion models memorize?" We show that memorization is driven by the overestimation of training samples during early denoising, which reduces diversity, collapses denoising trajectories, and accelerates convergence toward the memorized image. Specifically: (i) memorization cannot be explained by overfitting alone, as training loss is larger under memorization due to classifier-free guidance amplifying predictions and inducing overestimation; (ii) memorized prompts inject training images into noise predictions, forcing latent trajectories to converge and steering denoising toward their paired samples; and (iii) a decomposition of intermediate latents reveals how initial randomness is quickly suppressed and replaced by memorized content, with deviations from the theoretical denoising schedule correlating almost perfectly with memorization severity. Together, these results identify early overestimation as the central underlying mechanism of memorization in diffusion models.</li>
<li><strong>摘要：</strong>尽管它们在图像产生方面取得了成功，但扩散模型仍可以记住培训数据，从而提出严重的隐私和版权问题。尽管先前的工作试图表征，检测和减轻记忆，但其为什么和如何发生的基本问题仍未解决。在本文中，我们重新审视了扩散和降解过程，并分析了潜在空间动力学，以解决：“扩散模型如何记住？”我们表明，记忆是由早期denoising期间训练样本高估的驱动，从而降低了多样性，崩溃了轨迹，并加速了融合到记忆的图像。具体来说：（i）不能单独使用过度拟合来解释记忆，因为由于无分类器的指导，训练损失在记忆中较大，从而放大了预测并引起高估； （ii）记住的提示将训练图像注入噪声预测中，迫使潜在轨迹融合并转向将其转向其配对样品； （iii）中级潜在潜在的分解揭示了如何迅速抑制和替换记忆内容的最初随机性，并且与理论剥离时间表的偏差几乎与记忆的严重性完全相关。总之，这些结果将早期高估是扩散模型中记忆的中心基础机制。</li>
</ul>

<h3>Title: Reweighted Flow Matching via Unbalanced OT for Label-free Long-tailed Generation</h3>
<ul>
<li><strong>Authors: </strong>Hyunsoo Song, Minjung Gim, Jaewoong Choi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25713">https://arxiv.org/abs/2509.25713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25713">https://arxiv.org/pdf/2509.25713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25713]] Reweighted Flow Matching via Unbalanced OT for Label-free Long-tailed Generation(https://arxiv.org/abs/2509.25713)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Flow matching has recently emerged as a powerful framework for continuous-time generative modeling. However, when applied to long-tailed distributions, standard flow matching suffers from majority bias, producing minority modes with low fidelity and failing to match the true class proportions. In this work, we propose Unbalanced Optimal Transport Reweighted Flow Matching (UOT-RFM), a novel framework for generative modeling under class-imbalanced (long-tailed) distributions that operates without any class label information. Our method constructs the conditional vector field using mini-batch Unbalanced Optimal Transport (UOT) and mitigates majority bias through a principled inverse reweighting strategy. The reweighting relies on a label-free majority score, defined as the density ratio between the target distribution and the UOT marginal. This score quantifies the degree of majority based on the geometric structure of the data, without requiring class labels. By incorporating this score into the training objective, UOT-RFM theoretically recovers the target distribution with first-order correction ($k=1$) and empirically improves tail-class generation through higher-order corrections ($k > 1$). Our model outperforms existing flow matching baselines on long-tailed benchmarks, while maintaining competitive performance on balanced datasets.</li>
<li><strong>摘要：</strong>流量匹配最近已成为连续时间生成建模的强大框架。但是，当应用于长尾分布时，标准流匹配遭受了多数偏差，产生低忠诚度的少数族裔模式，并且无法匹配真实的类比例。在这项工作中，我们提出了不平衡的最佳运输重新加权流量匹配（UOT-RFM），这是一个新颖的框架，用于在类别不平衡（长尾）分布下进行生成建模，无需任何类标签信息即可运行。我们的方法使用微型批次不平衡的最佳传输（UOT）构建条件矢量场，并通过原则上的持续重量重量策略来减轻多数偏差。重新权依赖于无标签的多数得分，该得分定义为目标分布和UOT边缘之间的密度比。此分数根据数据的几何结构量化了多数的程度，而无需类标签。通过将此分数纳入训练目标，UOT-RFM理论上通过一阶校正（$ k = 1 $）恢复目标分布，并通过高阶校正（$ k> 1 $）进行经验改善尾巴的生成。我们的模型在长尾基准测试上优于现有的流匹配基线，同时保持平衡数据集的竞争性能。</li>
</ul>

<h3>Title: Boundary-to-Region Supervision for Offline Safe Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Huikang Su, Dengyun Peng, Zifeng Zhuang, YuHan Liu, Qiguang Chen, Donglin Wang, Qinghe Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25727">https://arxiv.org/abs/2509.25727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25727">https://arxiv.org/pdf/2509.25727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25727]] Boundary-to-Region Supervision for Offline Safe Reinforcement Learning(https://arxiv.org/abs/2509.25727)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Offline safe reinforcement learning aims to learn policies that satisfy predefined safety constraints from static datasets. Existing sequence-model-based methods condition action generation on symmetric input tokens for return-to-go and cost-to-go, neglecting their intrinsic asymmetry: return-to-go (RTG) serves as a flexible performance target, while cost-to-go (CTG) should represent a rigid safety boundary. This symmetric conditioning leads to unreliable constraint satisfaction, especially when encountering out-of-distribution cost trajectories. To address this, we propose Boundary-to-Region (B2R), a framework that enables asymmetric conditioning through cost signal realignment . B2R redefines CTG as a boundary constraint under a fixed safety budget, unifying the cost distribution of all feasible trajectories while preserving reward structures. Combined with rotary positional embeddings , it enhances exploration within the safe region. Experimental results show that B2R satisfies safety constraints in 35 out of 38 safety-critical tasks while achieving superior reward performance over baseline methods. This work highlights the limitations of symmetric token conditioning and establishes a new theoretical and practical approach for applying sequence models to safe RL. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>离线安全加强学习旨在学习满足静态数据集预定义安全约束的政策。现有的基于序列模型的方法在对称输入令牌上生成动作动作，用于返回和到达成本，忽略了其内在的不对称性：返回到GO（RTG）是一个灵活的性能目标，而成本为GO（CTG）应代表刚性安全边界。这种对称条件会导致不可靠的约束满意度，尤其是在遇到分布外的成本轨迹时。为了解决这个问题，我们提出了边界对区域（B2R），该框架可以通过成本信号重新调整实现不对称调节。 B2R将CTG重新定义为固定安全预算下的边界约束，统一了所有可行轨迹的成本分布，同时保留奖励结构。结合旋转位置嵌入，它增强了安全区域内的勘探。实验结果表明，在38个安全至关重要的任务中，B2R满足了35个安全限制，同时达到了优于基线方法的卓越奖励性能。这项工作突出了对称令牌调理的局限性，并建立了一种将序列模型应用于安全RL的新理论和实用方法。我们的代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: LieHMR: Autoregressive Human Mesh Recovery with $SO(3)$ Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Donghwan Kim, Tae-Kyun Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25739">https://arxiv.org/abs/2509.25739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25739">https://arxiv.org/pdf/2509.25739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25739]] LieHMR: Autoregressive Human Mesh Recovery with $SO(3)$ Diffusion(https://arxiv.org/abs/2509.25739)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We tackle the problem of Human Mesh Recovery (HMR) from a single RGB image, formulating it as an image-conditioned human pose and shape generation. While recovering 3D human pose from 2D observations is inherently ambiguous, most existing approaches have regressed a single deterministic output. Probabilistic methods attempt to address this by generating multiple plausible outputs to model the ambiguity. However, these methods often exhibit a trade-off between accuracy and sample diversity, and their single predictions are not competitive with state-of-the-art deterministic models. To overcome these limitations, we propose a novel approach that models well-aligned distribution to 2D observations. In particular, we introduce $SO(3)$ diffusion model, which generates the distribution of pose parameters represented as 3D rotations unconditional and conditional to image observations via conditioning dropout. Our model learns the hierarchical structure of human body joints using the transformer. Instead of using transformer as a denoising model, the time-independent transformer extracts latent vectors for the joints and a small MLP-based denoising model learns the per-joint distribution conditioned on the latent vector. We experimentally demonstrate and analyze that our model predicts accurate pose probability distribution effectively.</li>
<li><strong>摘要：</strong>我们从单个RGB图像中解决了人网恢复（HMR）的问题，将其作为图像条件的人姿势和形状产生。尽管从2D观测中恢复3D人的姿势本质上是模棱两可的，但大多数现有方法都会回归单个确定性输出。概率方法试图通过生成多个合理的输出来模拟歧义来解决此问题。但是，这些方法通常在准确性和样本多样性之间表现出权衡，而它们的单一预测与最先进的确定性模型没有竞争力。为了克服这些局限性，我们提出了一种新颖的方法，该方法将良好的分布对2D观察结果进行了建模。特别是，我们引入了$ SO（3）$扩散模型，该模型生成了姿势参数的分布，该姿势参数无条件地表示为3D旋转，并通过条件辍学来实现图像观测。我们的模型使用变压器了解人体关节的分层结构。无独立的变压器并没有将变压器用作denoising模型，而是为关节提取潜在向量和基于MLP的小型DeNoising模型，而是学习在潜在矢量上的每连接点分布。我们通过实验证明并分析我们的模型是否有效地预测了准确的姿势概率分布。</li>
</ul>

<h3>Title: Dolphin v1.0 Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Taohan Weng, Chi zhang, Chaoran Yan, Siya Liu, Xiaoyang Liu, Yalun Wu, Boyang Wang, Boyan Wang, Jiren Ren, Kaiwen Yan, Jinze Yu, Kaibing Hu, Henan Liu, Haoyun zheng, Anjie Le, Hongcheng Guo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25748">https://arxiv.org/abs/2509.25748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25748">https://arxiv.org/pdf/2509.25748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25748]] Dolphin v1.0 Technical Report(https://arxiv.org/abs/2509.25748)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Ultrasound is crucial in modern medicine but faces challenges like operator dependence, image noise, and real-time scanning, hindering AI integration. While large multimodal models excel in other medical imaging areas, they struggle with ultrasound's complexities. To address this, we introduce Dolphin v1.0 (V1) and its reasoning-augmented version, Dolphin R1-the first large-scale multimodal ultrasound foundation models unifying diverse clinical tasks in a single vision-language this http URL tackle ultrasound variability and noise, we curated a 2-million-scale multimodal dataset, combining textbook knowledge, public data, synthetic samples, and general corpora. This ensures robust perception, generalization, and clinical this http URL Dolphin series employs a three-stage training strategy: domain-specialized pretraining, instruction-driven alignment, and reinforcement-based refinement. Dolphin v1.0 delivers reliable performance in classification, detection, regression, and report generation. Dolphin R1 enhances diagnostic inference, reasoning transparency, and interpretability through reinforcement learning with ultrasound-specific this http URL on U2-Bench across eight ultrasound tasks, Dolphin R1 achieves a U2-score of 0.5835-over twice the second-best model (0.2968) setting a new state of the art. Dolphin v1.0 also performs competitively, validating the unified framework. Comparisons show reasoning-enhanced training significantly improves diagnostic accuracy, consistency, and interpretability, highlighting its importance for high-stakes medical AI.</li>
<li><strong>摘要：</strong>超声波在现代医学中至关重要，但是面临诸如操作员依赖，图像噪声和实时扫描以及阻碍AI集成的挑战。尽管大型多模型在其他医学成像区域中都表现出色，但它们在超声的复杂性方面挣扎。为了解决这个问题，我们介绍了海豚V1.0（V1）及其推理的增强版本，Dolphin R1-第一个大规模多模式超声超声基础模型，在单个视觉上统一了多种临床任务。在单个视觉上，此HTTP URL解决了超声波变异性和噪声一般语料库。这确保了强大的感知，概括和临床，该HTTP URL Dolphin系列采用了三阶段的训练策略：领域专题化的预训练，指导驱动的对准和基于增强的改进。 Dolphin V1.0在分类，检测，回归和报告生成方面提供可靠的性能。 Dolphin R1通过使用超声特异性的该HTTP URL的增强学习来增强诊断推理，推理透明度和可解释性，在八个超声波任务中，在U2板凳上进行了HTTP URL，Dolphin R1实现了第二好的模型（0.2968）的U2尺寸为0.5835，这是0.5835的二次，设置了新的Art and Art or Art of Art of Art of Art of Art of Art of Art of Art of Art of Art of Art of Art of Art of Art。 Dolphin V1.0还竞争性能，验证了统一的框架。比较表明，推理增强培训可显着提高诊断准确性，一致性和解释性，从而强调了其对高风险医学AI的重要性。</li>
</ul>

<h3>Title: ART-VITON: Measurement-Guided Latent Diffusion for Artifact-Free Virtual Try-On</h3>
<ul>
<li><strong>Authors: </strong>Junseo Park, Hyeryung Jang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25749">https://arxiv.org/abs/2509.25749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25749">https://arxiv.org/pdf/2509.25749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25749]] ART-VITON: Measurement-Guided Latent Diffusion for Artifact-Free Virtual Try-On(https://arxiv.org/abs/2509.25749)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Virtual try-on (VITON) aims to generate realistic images of a person wearing a target garment, requiring precise garment alignment in try-on regions and faithful preservation of identity and background in non-try-on regions. While latent diffusion models (LDMs) have advanced alignment and detail synthesis, preserving non-try-on regions remains challenging. A common post-hoc strategy directly replaces these regions with original content, but abrupt transitions often produce boundary artifacts. To overcome this, we reformulate VITON as a linear inverse problem and adopt trajectory-aligned solvers that progressively enforce measurement consistency, reducing abrupt changes in non-try-on regions. However, existing solvers still suffer from semantic drift during generation, leading to artifacts. We propose ART-VITON, a measurement-guided diffusion framework that ensures measurement adherence while maintaining artifact-free synthesis. Our method integrates residual prior-based initialization to mitigate training-inference mismatch and artifact-free measurement-guided sampling that combines data consistency, frequency-level correction, and periodic standard denoising. Experiments on VITON-HD, DressCode, and SHHQ-1.0 demonstrate that ART-VITON effectively preserves identity and background, eliminates boundary artifacts, and consistently improves visual fidelity and robustness over state-of-the-art baselines.</li>
<li><strong>摘要：</strong>虚拟试验（Viton）旨在产生一个穿着目标服装的人的逼真的图像，需要在试用区域进行精确的衣服对齐，并忠实地保留非try型地区的身份和背景。尽管潜在扩散模型（LDMS）具有高级对齐和细节综合，但保留非尝试区域仍然具有挑战性。一种常见的事后策略直接用原始内容取代了这些区域，但是突然的过渡通常会产生边界伪像。为了克服这一点，我们将Viton重新制定为一个线性反问题，并采用轨迹一致的求解器，逐步执行测量一致性，从而减少了非尝试区域的突然变化。但是，现有的求解器在世代相传仍遭受语义漂移，导致工件。我们提出了Art-Viton，这是一种测量引导的扩散框架，可确保在保持无伪影合成的同时测量粘附。我们的方法将基于剩余的先验初始化整合在一起，以减轻训练 - 推导不匹配和无伪影测量引导的采样，从而结合了数据一致性，频率级校正和定期标准标准。对Viton-HD，DressCode和SHHQ-1.0的实验表明，Art-Viton有效地保留了身份和背景，消除了边界伪像，并始终提高了对最新基准的视觉保真度和稳健性。</li>
</ul>

<h3>Title: OPPO: Accelerating PPO-based RLHF via Pipeline Overlap</h3>
<ul>
<li><strong>Authors: </strong>Kaizhuo Yan (1), Yingjie Yu (1), Yifan Yu (1), Haizhong Zheng (2), Fan Lai (1) ((1) University of Illinois Urbana-Champaign, (2) Carnegie Mellon University)</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25762">https://arxiv.org/abs/2509.25762</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25762">https://arxiv.org/pdf/2509.25762</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25762]] OPPO: Accelerating PPO-based RLHF via Pipeline Overlap(https://arxiv.org/abs/2509.25762)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Proximal Policy Optimization (PPO)-based reinforcement learning from human feedback (RLHF) is a widely adopted paradigm for aligning large language models (LLMs) with human preferences. However, its training pipeline suffers from substantial inefficiencies due to sequential multi-model dependencies (e.g., reward model depends on actor outputs) and long-tail response lengths, where a few long responses straggle the stage completion. We present OPPO, a novel, lightweight, and model-agnostic PPO-based RLHF framework that improves training efficiency by overlapping pipeline execution. OPPO introduces two novel techniques: (1) Intra-step overlap, which streams upstream model outputs (e.g., actor model) in right-sized chunks, enabling the downstream model (e.g., reward) to begin prefill while the upstream continues decoding; and (2) Inter-step overlap, which adaptively overcommits a few prompts and defers long generations to future steps, mitigating tail latency without discarding partial work. OPPO integrates easily with existing PPO implementations with a few lines of code change. Extensive evaluations show that OPPO accelerates PPO-based RLHF training by $1.8 \times-2.8 \times$ and improves GPU utilization by $1.4 \times-2.1 \times$ without compromising training convergence.</li>
<li><strong>摘要：</strong>基于人类反馈（RLHF）的基于近端政策优化（PPO）的强化学习是将大语言模型（LLMS）与人类偏好保持一致的广泛采用的范式。但是，由于顺序多模型依赖性（例如，奖励模型取决于参与者的输出）和长尾响应长度，其训练管线遭受了巨大的效率低下，其中一些长期的响应陷入了阶段的完成。我们提出了Oppo，这是一种基于新颖，轻巧和模型不合时宜的RLHF框架，可通过重叠的管道执行来提高训练效率。 OPPO介绍了两种新型技术：（1）阶段重叠，该重叠在右尺寸的块中流式传输上游模型输出（例如Actor模型），从而使下游模型（例如奖励）可以开始预填充，而上游继续进行解码； （2）阶层重叠，它可以适应一些提示，并为将来的步骤辩护，从而减轻尾巴潜伏期而不丢弃部分工作。 OPPO可以轻松地与现有的PPO实现进行集成，并进行几行代码更改。广泛的评估表明，OPPO基于PPO的RLHF培训$ 1.8 \ times-2.8 \ times $，并将GPU利用率提高了$ 1.4 \ times-2.1 \ times $，而无需损害培训融合。</li>
</ul>

<h3>Title: PCPO: Proportionate Credit Policy Optimization for Aligning Image Generation Models</h3>
<ul>
<li><strong>Authors: </strong>Jeongjae Lee, Jong Chul Ye</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25774">https://arxiv.org/abs/2509.25774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25774">https://arxiv.org/pdf/2509.25774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25774]] PCPO: Proportionate Credit Policy Optimization for Aligning Image Generation Models(https://arxiv.org/abs/2509.25774)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>While reinforcement learning has advanced the alignment of text-to-image (T2I) models, state-of-the-art policy gradient methods are still hampered by training instability and high variance, hindering convergence speed and compromising image quality. Our analysis identifies a key cause of this instability: disproportionate credit assignment, in which the mathematical structure of the generative sampler produces volatile and non-proportional feedback across timesteps. To address this, we introduce Proportionate Credit Policy Optimization (PCPO), a framework that enforces proportional credit assignment through a stable objective reformulation and a principled reweighting of timesteps. This correction stabilizes the training process, leading to significantly accelerated convergence and superior image quality. The improvement in quality is a direct result of mitigating model collapse, a common failure mode in recursive training. PCPO substantially outperforms existing policy gradient baselines on all fronts, including the state-of-the-art DanceGRPO.</li>
<li><strong>摘要：</strong>尽管强化学习已经提高了文本对图像（T2I）模型的对准，但最先进的策略梯度方法仍然受到训练不稳定性和高方差，阻碍收敛速度和损害图像质量的阻碍。我们的分析确定了这种不稳定性的关键原因：信用分配不成比例，其中生成采样器的数学结构会产生跨时间段的挥发性和非比例反馈。为了解决这个问题，我们引入了按比例的信用政策优化（PCPO），该框架通过稳定的客观重新重新制定和对时间段的原则重新持续重新持续来执行比例的信用分配。这种校正稳定了训练过程，导致了显着加速的收敛和出色的图像质量。质量的改善是减轻模型崩溃的直接结果，这是递归训练中常见的故障模式。 PCPO在包括最先进的DanceGrpo在内的所有方面都大大优于现有的策略梯度基线。</li>
</ul>

<h3>Title: Online Decision Making with Generative Action Sets</h3>
<ul>
<li><strong>Authors: </strong>Jianyu Xu, Vidhi Jain, Bryan Wilder, Aarti Singh</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25777">https://arxiv.org/abs/2509.25777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25777">https://arxiv.org/pdf/2509.25777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25777]] Online Decision Making with Generative Action Sets(https://arxiv.org/abs/2509.25777)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>With advances in generative AI, decision-making agents can now dynamically create new actions during online learning, but action generation typically incurs costs that must be balanced against potential benefits. We study an online learning problem where an agent can generate new actions at any time step by paying a one-time cost, with these actions becoming permanently available for future use. The challenge lies in learning the optimal sequence of two-fold decisions: which action to take and when to generate new ones, further complicated by the triangular tradeoffs among exploitation, exploration and $\textit{creation}$. To solve this problem, we propose a doubly-optimistic algorithm that employs Lower Confidence Bounds (LCB) for action selection and Upper Confidence Bounds (UCB) for action generation. Empirical evaluation on healthcare question-answering datasets demonstrates that our approach achieves favorable generation-quality tradeoffs compared to baseline strategies. From theoretical perspectives, we prove that our algorithm achieves the optimal regret of $O(T^{\frac{d}{d+2}}d^{\frac{d}{d+2}} + d\sqrt{T\log T})$, providing the first sublinear regret bound for online learning with expanding action spaces.</li>
<li><strong>摘要：</strong>随着生成AI的进步，决策代理现在可以在在线学习期间动态创建新的动作，但是行动产生通常会产生必须与潜在收益保持平衡的成本。我们研究一个在线学习问题，代理可以通过支付一次性成本来在任何时间步骤中生成新的动作，这些动作将永久使用以供将来使用。挑战在于学习两倍决策的最佳顺序：要采取的行动以及何时产生新的决定，这是由于剥削，勘探和$ \ textit {creation} $之间三角折衷而更加复杂的。为了解决这个问题，我们提出了一种双重的算法，该算法采用较低的置信界（LCB）进行动作选择和上限置信界（UCB）进行动作生成。对医疗保健问题的经验评估，避开问题数据集表明，与基线策略相比，我们的方法实现了有利的一代质量权衡。从理论角度来看，我们证明我们的算法实现了$ O（t^{\ frac {d} {d+2}}} {d+2}} d^{\ frac {d} {d+2} {d+2}}}+d \ d \ sqrt {t \ log t} {t \ log t} $ forine for forinfore for First for Founce forning founding的行动。</li>
</ul>

<h3>Title: Self-Evolving Vision-Language Models for Image Quality Assessment via Voting and Ranking</h3>
<ul>
<li><strong>Authors: </strong>Wen Wen, Tianwu Zhi, Kanglong Fan, Yang Li, Xinge Peng, Yabin Zhang, Yiting Liao, Junlin Li, Li Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25787">https://arxiv.org/abs/2509.25787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25787">https://arxiv.org/pdf/2509.25787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25787]] Self-Evolving Vision-Language Models for Image Quality Assessment via Voting and Ranking(https://arxiv.org/abs/2509.25787)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Improving vision-language models (VLMs) in the post-training stage typically relies on supervised fine-tuning or reinforcement learning, methods that necessitate costly, human-annotated data. While self-supervised techniques such as self-consistency have proven effective for enhancing reasoning capabilities, their application to perceptual domains such as image quality assessment (IQA) remains largely unexplored. In this work, we introduce EvoQuality, a novel framework that enables a VLM to autonomously refine its quality perception capabilities without any ground-truth labels. EvoQuality adapts the principle of self-consistency to the ranking-based nature of IQA. It generates pseudo-labels by performing pairwise majority voting on the VLM's own outputs to establish a consensus on relative quality. These pseudo-rankings are then formulated into a fidelity reward that guides the model's iterative evolution through group relative policy optimization (GRPO). By iteratively leveraging its own predictions, EvoQuality progressively refines the VLM's perceptual capability. Extensive experiments show that EvoQuality boosts the base VLM's zero-shot performance by 31.8\% on PLCC across diverse IQA benchmarks. Remarkably, despite being entirely self-supervised, EvoQuality achieves performance that is competitive with, or even surpasses, state-of-the-art supervised VLM-based IQA models, outperforming these models on 5 out of 7 IQA benchmarks.</li>
<li><strong>摘要：</strong>在训练后阶段改善视觉模型（VLM）通常依赖于监督的微调或加强学习，这些方法是需要昂贵，人为认可的数据的方法。虽然自我监督的技术（例如自矛盾）已被证明可以有效地增强推理能力，但它们在图像质量评估（IQA）等知觉领域的应用仍然很大程度上尚未得到探索。在这项工作中，我们介绍了一种新颖的框架，它使VLM能够自主地完善其质量感知能力，而无需任何地面真实标签。闻质将自矛盾的原则适应IQA的基于排名的本质。它通过对VLM自己的输出进行成对的多数投票来产生伪标签，以建立对相对质量的共识。然后将这些伪级列出为忠诚度奖励，该奖励通过小组相对政策优化（GRPO）指导模型的迭代演变。通过迭代地利用自己的预测，回避逐步完善了VLM的感知能力。广泛的实验表明，在不同的IQA基准的PLCC上，卓越质量使基本VLM的零击性能提高了31.8％。值得注意的是，尽管完全是自我监督的，但令人回味质量取得了与最先进的基于VLM的IQA模型竞争甚至超过最先进的性能，在7个IQA基准中的5个模型上都优于这些模型。</li>
</ul>

<h3>Title: Learning to Reason as Action Abstractions with Scalable Mid-Training RL</h3>
<ul>
<li><strong>Authors: </strong>Shenao Zhang, Donghan Yu, Yihao Feng, Bowen Jin, Zhaoran Wang, John Peebles, Zirui Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25810">https://arxiv.org/abs/2509.25810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25810">https://arxiv.org/pdf/2509.25810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25810]] Learning to Reason as Action Abstractions with Scalable Mid-Training RL(https://arxiv.org/abs/2509.25810)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models excel with reinforcement learning (RL), but fully unlocking this potential requires a mid-training stage. An effective mid-training phase should identify a compact set of useful actions and enable fast selection among them through online RL. We formalize this intuition by presenting the first theoretical result on how mid-training shapes post-training: it characterizes an action subspace that minimizes both the value approximation error from pruning and the RL error during subsequent planning. Our analysis reveals two key determinants of mid-training effectiveness: pruning efficiency, which shapes the prior of the initial RL policy, and its impact on RL convergence, which governs the extent to which that policy can be improved via online interactions. These results suggest that mid-training is most effective when the decision space is compact and the effective horizon is short, highlighting the importance of operating in the space of action abstractions rather than primitive actions. Building on these insights, we propose Reasoning as Action Abstractions (RA3), a scalable mid-training algorithm. Specifically, we derive a sequential variational lower bound and optimize it by iteratively discovering temporally-consistent latent structures via RL, followed by fine-tuning on the bootstrapped data. Experiments on code generation tasks demonstrate the effectiveness of our approach. Across multiple base models, RA3 improves the average performance on HumanEval and MBPP by 8 and 4 points over the base model and the next-token prediction baseline. Furthermore, RA3 achieves faster convergence and higher asymptotic performance in RLVR on HumanEval+, MBPP+, LiveCodeBench, and Codeforces.</li>
<li><strong>摘要：</strong>大型语言模型在加强学习（RL）方面表现出色，但充分解锁这种潜力需要中期训练阶段。有效的中期训练阶段应确定一套紧凑的有用动作，并通过在线RL进行快速选择。我们通过在训练后进行中期训练的第一个理论结果来形式化这种直觉：它表征了一个动作子空间，该子空间可以最大程度地减少修剪的值近似误差和随后计划中的RL误差。我们的分析揭示了中期培训有效性的两个关键决定因素：修剪效率，它塑造了初始RL策略的先验及其对RL融合的影响，这控制了通过在线互动可以改善该策略的程度。这些结果表明，当决策空间紧凑并且有效地平线很短时，中期训练最有效，这突出了在动作空间抽象而不是原始动作中操作的重要性。在这些见解的基础上，我们将推理作为动作抽象（RA3），这是一种可扩展的中训练算法。具体而言，我们通过迭代通过RL迭代地发现了暂时性的潜在结构，从而得出了一个顺序的变异下限，然后对其进行了对引导数据进行微调。关于代码生成任务的实验证明了我们方法的有效性。在多个基本模型中，RA3在基本模型和下一步的预测基线上将HOMANEVAL和MBPP的平均性能提高了8和4点。此外，RA3在HumaneVal+，MBPP+，LiveCodeBench和Codeforces上的RLVR中实现了更快的收敛性和较高的渐近性能。</li>
</ul>

<h3>Title: Training-Free Reward-Guided Image Editing via Trajectory Optimal Control</h3>
<ul>
<li><strong>Authors: </strong>Jinho Chang, Jaemin Kim, Jong Chul Ye</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25845">https://arxiv.org/abs/2509.25845</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25845">https://arxiv.org/pdf/2509.25845</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25845]] Training-Free Reward-Guided Image Editing via Trajectory Optimal Control(https://arxiv.org/abs/2509.25845)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advancements in diffusion and flow-matching models have demonstrated remarkable capabilities in high-fidelity image synthesis. A prominent line of research involves reward-guided guidance, which steers the generation process during inference to align with specific objectives. However, leveraging this reward-guided approach to the task of image editing, which requires preserving the semantic content of the source image while enhancing a target reward, is largely unexplored. In this work, we introduce a novel framework for training-free, reward-guided image editing. We formulate the editing process as a trajectory optimal control problem where the reverse process of a diffusion model is treated as a controllable trajectory originating from the source image, and the adjoint states are iteratively updated to steer the editing process. Through extensive experiments across distinct editing tasks, we demonstrate that our approach significantly outperforms existing inversion-based training-free guidance baselines, achieving a superior balance between reward maximization and fidelity to the source image without reward hacking.</li>
<li><strong>摘要：</strong>扩散和流程匹配模型的最新进步表明，在高保真图像合成中具有显着的功能。一项著名的研究涉及奖励指导的指导，该指导在推断过程中引导生成过程以与特定目标保持一致。但是，将这种奖励指导的方法用于图像编辑任务，该方法需要保留源图像的语义内容，同时增强目标奖励，这在很大程度上没有探索。在这项工作中，我们介绍了一个新颖的框架，用于无训练，奖励指导的图像编辑。我们将编辑过程提出为轨迹最佳控制问题，其中扩散模型的反向过程被视为源自源图像的可控轨迹，并且迭代地进行了更新以引导编辑过程。通过跨不同编辑任务进行的广泛实验，我们证明我们的方法显着优于现有的基于反演的无训练指导基线，在没有奖励黑客的情况下，在奖励最大化和忠诚度与源图像之间取得了卓越的平衡。</li>
</ul>

<h3>Title: More Thought, Less Accuracy? On the Dual Nature of Reasoning in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Tian, Shu Zou, Zhaoyuan Yang, Mengqi He, Fabian Waschkowski, Lukas Wesemann, Peter Tu, Jing Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25848">https://arxiv.org/abs/2509.25848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25848">https://arxiv.org/pdf/2509.25848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25848]] More Thought, Less Accuracy? On the Dual Nature of Reasoning in Vision-Language Models(https://arxiv.org/abs/2509.25848)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reasoning has emerged as a pivotal capability in Large Language Models (LLMs). Through Reinforcement Learning (RL), typically Group Relative Policy Optimization (GRPO), these models are able to solve complex tasks such as mathematics and code generation. Building on these advances, recent research has sought to extend reasoning to Vision-Language Models (VLMs), yielding promising results across diverse visual tasks. Despite this progress, our study uncovers the dual nature of multimodal reasoning: while it substantially enhances logical inference and facilitates performance on challenging problems, it may gradually impair perceptual grounding, leading to recognition failures on otherwise basic visual questions. Through further analysis, we attribute this phenomenon to visual forgetting, wherein prolonged reasoning causes the model to increasingly disregard visual input. To address this, we propose Vision-Anchored Policy Optimization (VAPO), a simple yet effective method that explicitly steers the reasoning process toward visually grounded trajectories. Our result model, VAPO-Thinker-7B, significantly strengthens the model's reliance on visual information and achieves new state-of-the-art results on a wide range of established benchmarks. Project page: this https URL</li>
<li><strong>摘要：</strong>推理已成为大语言模型（LLMS）中的关键能力。通过加强学习（RL），通常是组相对策略优化（GRPO），这些模型能够解决复杂的任务，例如数学和代码生成。在这些进步的基础上，最近的研究试图将推理扩展到视觉模型（VLMS），从而在各种视觉任务中产生了有希望的结果。尽管取得了这种进步，但我们的研究仍揭示了多模式推理的双重性质：虽然它显着增强了逻辑推理并促进了在挑战性问题上的性能，但它可能会逐渐损害知觉基础，从而导致对其他基本视觉问题的识别失败。通过进一步的分析，我们将这种现象归因于视觉遗忘，其中延长推理导致模型越来越无视视觉输入。为了解决这个问题，我们提出了视觉锚定的策略优化（VAPO），这是一种简单而有效的方法，可以明确地将推理过程转向视觉扎根的轨迹。我们的结果模型Vapo-Thinker-7B显着增强了该模型对视觉信息的依赖，并在广泛的既定基准上实现了新的最新结果。项目页面：此HTTPS URL</li>
</ul>

<h3>Title: VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained Perception in VLMs</h3>
<ul>
<li><strong>Authors: </strong>Peng Liu, Haozhan Shen, Chunxin Fang, Zhicheng Sun, Jiajia Liao, Tiancheng Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25916">https://arxiv.org/abs/2509.25916</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25916">https://arxiv.org/pdf/2509.25916</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25916]] VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained Perception in VLMs(https://arxiv.org/abs/2509.25916)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) excel at high-level scene understanding but falter on fine-grained perception tasks requiring precise localization. This failure stems from a fundamental mismatch, as generating exact numerical coordinates is a challenging task for language-centric architectures. In this paper, we introduce VLM-FO1, a novel framework that overcomes this limitation by reframing object-centric perception from a brittle coordinate generation problem into a robust feature retrieval task. Our method operates as a plug-and-play module that integrates with any pre-trained VLM. It leverages a Hybrid Fine-grained Region Encoder (HFRE), featuring a dual vision encoder, to generate powerful region tokens rich in both semantic and spatial detail. A token-based referencing system then enables the LLM to seamlessly reason about and ground language in these specific visual regions. Experiments show that VLM-FO1 achieves state-of-the-art performance across a diverse suite of benchmarks, demonstrating exceptional capabilities in object grounding, region generational understanding, and visual region reasoning. Crucially, our two-stage training strategy ensures that these perception gains are achieved without compromising the base model's general visual understanding capabilities. VLM-FO1 establishes an effective and flexible paradigm for building perception-aware VLMs, bridging the gap between high-level reasoning and fine-grained visual grounding.</li>
<li><strong>摘要：</strong>视觉语言模型（VLM）在高级场景中表现出色，但在需要精确本地化的细粒感知任务上步履蹒跚。这种故障源于根本的不匹配，因为生成精确的数值坐标是以语言为中心的架构的挑战性任务。在本文中，我们引入了VLM-FO1，这是一个新颖的框架，通过将以对象为中心的感知从脆性坐标生成问题重新定义为强大的特征检索任务来克服这种限制。我们的方法是与任何预训练的VLM集成的插件模块。它利用了带有双视觉编码器的混合细粒区编码器（HFRE）来生成富含语义和空间细节的强大区域令牌。然后，基于令牌的引用系统使LLM能够在这些特定的视觉区域中无缝地理论和地面语言。实验表明，VLM-FO1在各种基准测试套件中实现了最先进的性能，证明了对象接地，区域世代理解和视觉区域推理的非凡功能。至关重要的是，我们的两阶段训练策略可确保在不损害基本模型的一般视觉理解能力的情况下实现这些感知的增长。 VLM-FO1建立了一个有效且灵活的范式，用于构建感知感知的VLM，从而弥合了高级推理和细粒度的视觉接地之间的差距。</li>
</ul>

<h3>Title: Self-Supervised Anatomical Consistency Learning for Vision-Grounded Medical Report Generation</h3>
<ul>
<li><strong>Authors: </strong>Longzhen Yang, Zhangkai Ni, Ying Wen, Yihang Liu, Lianghua He, Heng Tao Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25963">https://arxiv.org/abs/2509.25963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25963">https://arxiv.org/pdf/2509.25963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25963]] Self-Supervised Anatomical Consistency Learning for Vision-Grounded Medical Report Generation(https://arxiv.org/abs/2509.25963)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision-grounded medical report generation aims to produce clinically accurate descriptions of medical images, anchored in explicit visual evidence to improve interpretability and facilitate integration into clinical workflows. However, existing methods often rely on separately trained detection modules that require extensive expert annotations, introducing high labeling costs and limiting generalizability due to pathology distribution bias across datasets. To address these challenges, we propose Self-Supervised Anatomical Consistency Learning (SS-ACL) -- a novel and annotation-free framework that aligns generated reports with corresponding anatomical regions using simple textual prompts. SS-ACL constructs a hierarchical anatomical graph inspired by the invariant top-down inclusion structure of human anatomy, organizing entities by spatial location. It recursively reconstructs fine-grained anatomical regions to enforce intra-sample spatial alignment, inherently guiding attention maps toward visually relevant areas prompted by text. To further enhance inter-sample semantic alignment for abnormality recognition, SS-ACL introduces a region-level contrastive learning based on anatomical consistency. These aligned embeddings serve as priors for report generation, enabling attention maps to provide interpretable visual evidence. Extensive experiments demonstrate that SS-ACL, without relying on expert annotations, (i) generates accurate and visually grounded reports -- outperforming state-of-the-art methods by 10\% in lexical accuracy and 25\% in clinical efficacy, and (ii) achieves competitive performance on various downstream visual tasks, surpassing current leading visual foundation models by 8\% in zero-shot visual grounding.</li>
<li><strong>摘要：</strong>视觉接地的医学报告的生成旨在对医学图像进行临床准确的描述，并以明确的视觉证据为基础，以提高可解释性并促进整合到临床工作流程中。但是，现有方法通常依赖于需要广泛的专家注释，引入高标签成本并限制由于跨数据集的病理分布偏差引起的可推广性的单独训练的检测模块。为了应对这些挑战，我们提出了自我监督的解剖一致性学习（SS-ACL） - 一种新颖且无注释的框架，使用简单的文本提示将生成的报告与相应的解剖区域保持一致。 SS-ACL构建了一个由人类解剖学的自上而下包含结构启发的层次解剖图，并通过空间位置组织实体。它递归地重建细颗粒的解剖区域来执行样本内的空间对准，固有地指导注意力图，指向文本引起的视觉相关区域。为了进一步增强样本间的语义对准异常识别，SS-ACL基于解剖学一致性引入了区域级的对比度学习。这些对齐的嵌入是报告生成的先验，从而使注意图能够提供可解释的视觉证据。广泛的实验表明，SS-ACL在不依赖专家注释的情况下（i）生成准确且视觉上的报告 - 在词汇准确性上优于10 \％的最先进方法，而在临床效率上的25％则超过了25％，并且（II）在各种下降视觉范围内实现了竞争性的竞争性，以实现当前的视觉效果，以8岁的型号为基础。</li>
</ul>

<h3>Title: Reevaluating Convolutional Neural Networks for Spectral Analysis: A Focus on Raman Spectroscopy</h3>
<ul>
<li><strong>Authors: </strong>Deniz Soysal, Xabier García-Andrade, Laura E. Rodriguez, Pablo Sobron, Laura M. Barge, Renaud Detry</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25964">https://arxiv.org/abs/2509.25964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25964">https://arxiv.org/pdf/2509.25964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25964]] Reevaluating Convolutional Neural Networks for Spectral Analysis: A Focus on Raman Spectroscopy(https://arxiv.org/abs/2509.25964)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Autonomous Raman instruments on Mars rovers, deep-sea landers, and field robots must interpret raw spectra distorted by fluorescence baselines, peak shifts, and limited ground-truth labels. Using curated subsets of the RRUFF database, we evaluate one-dimensional convolutional neural networks (CNNs) and report four advances: (i) Baseline-independent classification: compact CNNs surpass $k$-nearest-neighbors and support-vector machines on handcrafted features, removing background-correction and peak-picking stages while ensuring reproducibility through released data splits and scripts. (ii) Pooling-controlled robustness: tuning a single pooling parameter accommodates Raman shifts up to $30 \,\mathrm{cm}^{-1}$, balancing translational invariance with spectral resolution. (iii) Label-efficient learning: semi-supervised generative adversarial networks and contrastive pretraining raise accuracy by up to $11\%$ with only $10\%$ labels, valuable for autonomous deployments with scarce annotation. (iv) Constant-time adaptation: freezing the CNN backbone and retraining only the softmax layer transfers models to unseen minerals at $\mathcal{O}(1)$ cost, outperforming Siamese networks on resource-limited processors. This workflow, which involves training on raw spectra, tuning pooling, adding semi-supervision when labels are scarce, and fine-tuning lightly for new targets, provides a practical path toward robust, low-footprint Raman classification in autonomous exploration.</li>
<li><strong>摘要：</strong>火星漫游者，深海着陆器和现场机器人上的自主拉曼仪器必须解释荧光基线，峰值偏移和有限的地面真实标签的原始光谱。 Using curated subsets of the RRUFF database, we evaluate one-dimensional convolutional neural networks (CNNs) and report four advances: (i) Baseline-independent classification: compact CNNs surpass $k$-nearest-neighbors and support-vector machines on handcrafted features, removing background-correction and peak-picking stages while ensuring reproducibility through released data splits and scripts. （ii）合并控制的鲁棒性：调整单个池参数可容纳拉曼（Raman）可容纳$ 30 \，\ mathrm {cm}^{ -  1} $，平衡翻译不变性与光谱分辨率。 （iii）标签效率学习：半监督的生成对抗网络和对比预处理的提高准确性高达$ 11 \％$，只有$ 10 \％$ $标签，对于稀缺注释的自主部署来说很有价值。 （iv）恒定时间适应：冻结CNN主链并仅重新训练SoftMax层将模型转移到以$ \ Mathcal {O}（1）$成本上的看不见的矿物质，以优于资源限制的处理器的暹罗网络。该工作流程涉及在原始光谱上进行培训，调整合并，在标签稀缺时增加半佩维斯，并轻轻地对新目标进行微调，为自主探索中的强大，低英尺的拉曼分类提供了一种实用的途径。</li>
</ul>

<h3>Title: Data-Free Continual Learning of Server Models in Model-Heterogeneous Federated learning</h3>
<ul>
<li><strong>Authors: </strong>Xiao Zhang, Zengzhe Chen, Yuan Yuan, Yifei Zou, Fuzhen Zhuang, Wenyu Jiao, Yuke Wang, Dongxiao Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25977">https://arxiv.org/abs/2509.25977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25977">https://arxiv.org/pdf/2509.25977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25977]] Data-Free Continual Learning of Server Models in Model-Heterogeneous Federated learning(https://arxiv.org/abs/2509.25977)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) is a distributed learning paradigm across multiple entities while preserving data privacy. However, with the continuous emergence of new data and increasing model diversity, traditional federated learning faces significant challenges, including inherent issues of data heterogeneity, model heterogeneity and catastrophic forgetting, along with new challenge of knowledge misalignment. In this study, we introduce FedDCL, a novel framework designed to enable data-free continual learning of the server model in a model-heterogeneous federated setting. We leverage pre-trained diffusion models to extract lightweight class-specific prototypes, which confer a threefold data-free advantage, enabling: (1) generation of synthetic data for the current task to augment training and counteract non-IID data distributions; (2) exemplar-free generative replay for retaining knowledge from previous tasks; and (3) data-free dynamic knowledge transfer from heterogeneous clients to the server. Experimental results on various datasets demonstrate the effectiveness of FedDCL, showcasing its potential to enhance the generalizability and practical applicability of federated learning in dynamic settings.</li>
<li><strong>摘要：</strong>联合学习（FL）是跨多个实体的分布式学习范式，同时保留数据隐私。但是，随着新数据的持续出现和增加模型多样性，传统的联邦学习面临着重大挑战，包括固有的数据异质性问题，模型异质性和灾难性遗忘，以及知识未对准的新挑战。在这项研究中，我们介绍了FedDCL，这是一个新颖的框架，旨在在模型异质联合设置中启用服务器模型的无数据持续学习。我们利用预先训练的扩散模型来提取轻量级类特异性的原型，该原型赋予了三倍的无数据优势，可以：（1）（1）为当前任务生成合成数据以增强培训并抵消非IID数据分布； （2）无示例性生成重播，用于保留以前任务的知识； （3）无数据的动态知识转移从异质客户端转移到服务器。各种数据集的实验结果证明了FedDCL的有效性，展示了其在动态环境中联合学习的普遍性和实际适用性的潜力。</li>
</ul>

<h3>Title: Exact Solutions to the Quantum Schrödinger Bridge Problem</h3>
<ul>
<li><strong>Authors: </strong>Mykola Bordyuh, Djork-Arné Clevert, Marco Bertolini</a></li>
<li><strong>Subjects: </strong>cs.LG, math-ph, math.PR, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.25980">https://arxiv.org/abs/2509.25980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.25980">https://arxiv.org/pdf/2509.25980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.25980]] Exact Solutions to the Quantum Schrödinger Bridge Problem(https://arxiv.org/abs/2509.25980)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The Quantum Schrödinger Bridge Problem (QSBP) describes the evolution of a stochastic process between two arbitrary probability distributions, where the dynamics are governed by the Schrödinger equation rather than by the traditional real-valued wave equation. Although the QSBP is known in the mathematical literature, we formulate it here from a Lagrangian perspective and derive its main features in a way that is particularly suited to generative modeling. We show that the resulting evolution equations involve the so-called Bohm (quantum) potential, representing a notion of non-locality in the stochastic process. This distinguishes the QSBP from classical stochastic dynamics and reflects a key characteristic typical of quantum mechanical systems. In this work, we derive exact closed-form solutions for the QSBP between Gaussian distributions. Our derivation is based on solving the Fokker-Planck Equation (FPE) and the Hamilton-Jacobi Equation (HJE) arising from the Lagrangian formulation of dynamical Optimal Transport. We find that, similar to the classical Schrödinger Bridge Problem, the solution to the QSBP between Gaussians is again a Gaussian process; however, the evolution of the covariance differs due to quantum effects. Leveraging these explicit solutions, we present a modified algorithm based on a Gaussian Mixture Model framework, and demonstrate its effectiveness across several experimental settings, including single-cell evolution data, image generation, molecular translation and applications in Mean-Field Games.</li>
<li><strong>摘要：</strong>量子Schrödinger桥问题（QSBP）描述了两个任意概率分布之间随机过程的演变，其中动力学由schrödinger方程来控制，而不是传统的实数波方程。尽管QSBP在数学文献中是众所周知的，但我们从拉格朗日的角度将其提出，并以特别适合生成建模的方式得出其主要特征。我们表明，所得的进化方程涉及所谓的BOHM（量子）电位，代表了随机过程中非本地性的概念。这将QSBP与经典的随机动力学区分开，并反映了量子机械系统的典型特征。在这项工作中，我们为高斯分布之间的QSBP得出了精确的封闭式解决方案。我们的推导基于求解Fokker-Planck方程（FPE）和汉密尔顿 - 雅各比方程（HJE），该方程（HJE）是由Lagrangian的动力学最佳传输配方引起的。我们发现，类似于古典Schrödinger桥问题，高斯人之间QSBP的解决方案再次是高斯过程。但是，由于量子效应，协方差的演变不同。利用这些明确的解决方案，我们基于高斯混合模型框架提出了一种修改的算法，并在几种实验环境中证明了其有效性，包括单细胞进化数据，图像产生，分子翻译和平均场游戏中的应用。</li>
</ul>

<h3>Title: AgenticIQA: An Agentic Framework for Adaptive and Interpretable Image Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Hanwei Zhu, Yu Tian, Keyan Ding, Baoliang Chen, Bolin Chen, Shiqi Wang, Weisi Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26006">https://arxiv.org/abs/2509.26006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26006">https://arxiv.org/pdf/2509.26006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26006]] AgenticIQA: An Agentic Framework for Adaptive and Interpretable Image Quality Assessment(https://arxiv.org/abs/2509.26006)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Image quality assessment (IQA) is inherently complex, as it reflects both the quantification and interpretation of perceptual quality rooted in the human visual system. Conventional approaches typically rely on fixed models to output scalar scores, limiting their adaptability to diverse distortions, user-specific queries, and interpretability needs. Furthermore, scoring and interpretation are often treated as independent processes, despite their interdependence: interpretation identifies perceptual degradations, while scoring abstracts them into a compact metric. To address these limitations, we propose AgenticIQA, a modular agentic framework that integrates vision-language models (VLMs) with traditional IQA tools in a dynamic, query-aware manner. AgenticIQA decomposes IQA into four subtasks -- distortion detection, distortion analysis, tool selection, and tool execution -- coordinated by a planner, executor, and summarizer. The planner formulates task-specific strategies, the executor collects perceptual evidence via tool invocation, and the summarizer integrates this evidence to produce accurate scores with human-aligned explanations. To support training and evaluation, we introduce AgenticIQA-200K, a large-scale instruction dataset tailored for IQA agents, and AgenticIQA-Eval, the first benchmark for assessing the planning, execution, and summarization capabilities of VLM-based IQA agents. Extensive experiments across diverse IQA datasets demonstrate that AgenticIQA consistently surpasses strong baselines in both scoring accuracy and explanatory alignment.</li>
<li><strong>摘要：</strong>图像质量评估（IQA）本质上是复杂的，因为它反映了植根于人类视觉系统的知觉质量的量化和解释。常规方法通常依靠固定模型来输出标量分数，将其适应性限制为各种扭曲，特定于用户特定的查询和解释性需求。此外，尽管它们相互依存关系，但经常将评分和解释视为独立过程：解释确定了感知降解，同时将它们抽象为紧凑的度量标准。为了解决这些局限性，我们提出了AgenticiQA，这是一个模块化的代理框架，以动态的，感知的方式将视觉语言模型（VLM）与传统的IQA工具集成在一起。 AgenticiQA将IQA分解为四个子任务 - 失真检测，失真分析，工具选择和工具执行 - 由计划者，执行者和摘要协调。计划者制定了特定于任务的策略，执行人通过工具调用收集感知证据，汇总器将此证据整合在一起，以与人类一致的解释产生准确的分数。为了支持培训和评估，我们介绍了针对IQA代理量身定制的大规模指令数据集的Agenticiqa-200K，以及Agenticiqa-eval，这是评估基于VLM的IQA代理的计划，执行和摘要功能的第一个基准。跨不同IQA数据集的广泛实验表明，AgeniCiQA在评分精度和解释性一致性方面始终超过强基础。</li>
</ul>

<h3>Title: PatchVSR: Breaking Video Diffusion Resolution Limits with Patch-wise Video Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Shian Du, Menghan Xia, Chang Liu, Xintao Wang, Jing Wang, Pengfei Wan, Di Zhang, Xiangyang Ji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26025">https://arxiv.org/abs/2509.26025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26025">https://arxiv.org/pdf/2509.26025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26025]] PatchVSR: Breaking Video Diffusion Resolution Limits with Patch-wise Video Super-Resolution(https://arxiv.org/abs/2509.26025)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generation, generative</a></li>
<li><strong>Abstract: </strong>Pre-trained video generation models hold great potential for generative video super-resolution (VSR). However, adapting them for full-size VSR, as most existing methods do, suffers from unnecessary intensive full-attention computation and fixed output resolution. To overcome these limitations, we make the first exploration into utilizing video diffusion priors for patch-wise VSR. This is non-trivial because pre-trained video diffusion models are not native for patch-level detail generation. To mitigate this challenge, we propose an innovative approach, called PatchVSR, which integrates a dual-stream adapter for conditional guidance. The patch branch extracts features from input patches to maintain content fidelity while the global branch extracts context features from the resized full video to bridge the generation gap caused by incomplete semantics of patches. Particularly, we also inject the patch's location information into the model to better contextualize patch synthesis within the global video frame. Experiments demonstrate that our method can synthesize high-fidelity, high-resolution details at the patch level. A tailor-made multi-patch joint modulation is proposed to ensure visual consistency across individually enhanced patches. Due to the flexibility of our patch-based paradigm, we can achieve highly competitive 4K VSR based on a 512x512 resolution base model, with extremely high efficiency.</li>
<li><strong>摘要：</strong>预训练的视频生成模型具有生成视频超分辨率（VSR）的巨大潜力。但是，像大多数现有方法一样，将它们调整为全尺寸VSR，遭受了不必要的密集全注意计算和固定输出分辨率的困扰。为了克服这些局限性，我们首次探索了通过贴片VSR的视频扩散先验。这是非平凡的，因为预训练的视频扩散模型不是贴片级详细信息生成的本地。为了缓解这一挑战，我们提出了一种创新的方法，称为PatchVSR，该方法集成了双流适配器以进行有条件的指导。补丁分支从输入补丁中提取功能，以维持内容保真度，而全局分支从调整大小的完整视频中提取上下文功能，以弥合由于补丁的语义不完整而引起的一代差距。特别是，我们还将补丁的位置信息注入模型中，以更好地将整个视频框架中的补丁合成。实验表明，我们的方法可以在斑块级别综合高保真性，高分辨率的细节。提出了量身定制的多块接头调制，以确保跨个别增强的斑块的视觉一致性。由于基于贴片的范式的灵活性，我们可以基于512x512分辨率基本模型实现高竞争力的4K VSR，该模型具有极高的效率。</li>
</ul>

<h3>Title: Causally Guided Gaussian Perturbations for Out-Of-Distribution Generalization in Medical Imaging</h3>
<ul>
<li><strong>Authors: </strong>Haoran Pei, Yuguang Yang, Kexin Liu, Baochang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26027">https://arxiv.org/abs/2509.26027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26027">https://arxiv.org/pdf/2509.26027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26027]] Causally Guided Gaussian Perturbations for Out-Of-Distribution Generalization in Medical Imaging(https://arxiv.org/abs/2509.26027)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Out-of-distribution (OOD) generalization remains a central challenge in deploying deep learning models to real-world scenarios, particularly in domains such as biomedical images, where distribution shifts are both subtle and pervasive. While existing methods often pursue domain invariance through complex generative models or adversarial training, these approaches may overlook the underlying causal mechanisms of this http URL this work, we propose Causally-Guided Gaussian Perturbations (CGP)-a lightweight framework that enhances OOD generalization by injecting spatially varying noise into input images, guided by soft causal masks derived from Vision Transformers. By applying stronger perturbations to background regions and weaker ones to foreground areas, CGP encourages the model to rely on causally relevant features rather than spurious this http URL results on the challenging WILDS benchmark Camelyon17 demonstrate consistent performance gains over state-of-the-art OOD baselines, highlighting the potential of causal perturbation as a tool for reliable and interpretable generalization.</li>
<li><strong>摘要：</strong>分布（OOD）的概括仍然是将深度学习模型部署到现实情况的核心挑战，尤其是在诸如生物医学图像之类的领域，在诸如生物医学图像之类的领域中，分布既微妙又普遍。虽然现有方法通常通过复杂的生成模型或对抗训练来追求领域不变性，但这些方法可能会忽略这项工作的基本因果机制，我们提出，我们提出了因果关系引导的高斯扰动（CGP）的因果关系，从而通过在柔软的范围内降低了ood危险，从而使散发出的散发效率降低，从而使散发出散布的范围内的散发性，从而使散发出的范围降低，从而使散发出散布的变化，从而使散发出噪声，从而掩盖噪声，从而掩盖噪声，从而使图像散发出噪声，从而在噪声中散发出噪声，从而在噪声中散发出噪声，从而在噪声中散发出噪声，从而在噪声中散发出噪声，从而在图像中散发出来变压器。通过将更强的扰动应用于背景区域和较弱的扰动到前景区域，CGP鼓励模型依靠因果关系特征，而不是杂乱无章的http URL导致挑战性的野生基准相机17的一致性表现出一致的绩效表现出一致的稳定的效果，从而超过了先进的OOD基础，从而可以肯定的是可靠地进行了可靠的工具。</li>
</ul>

<h3>Title: DGM4+: Dataset Extension for Global Scene Inconsistency</h3>
<ul>
<li><strong>Authors: </strong>Gagandeep Singh, Samudi Amarsinghe, Priyanka Singh, Xue Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26047">https://arxiv.org/abs/2509.26047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26047">https://arxiv.org/pdf/2509.26047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26047]] DGM4+: Dataset Extension for Global Scene Inconsistency(https://arxiv.org/abs/2509.26047)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The rapid advances in generative models have significantly lowered the barrier to producing convincing multimodal disinformation. Fabricated images and manipulated captions increasingly co-occur to create persuasive false narratives. While the Detecting and Grounding Multi-Modal Media Manipulation (DGM4) dataset established a foundation for research in this area, it is restricted to local manipulations such as face swaps, attribute edits, and caption changes. This leaves a critical gap: global inconsistencies, such as mismatched foregrounds and backgrounds, which are now prevalent in real-world forgeries. To address this, we extend DGM4 with 5,000 high-quality samples that introduce Foreground-Background (FG-BG) mismatches and their hybrids with text manipulations. Using OpenAI's gpt-image-1 and carefully designed prompts, we generate human-centric news-style images where authentic figures are placed into absurd or impossible backdrops (e.g., a teacher calmly addressing students on the surface of Mars). Captions are produced under three conditions: literal, text attribute, and text split, yielding three new manipulation categories: FG-BG, FG-BG+TA, and FG-BG+TS. Quality control pipelines enforce one-to-three visible faces, perceptual hash deduplication, OCR-based text scrubbing, and realistic headline length. By introducing global manipulations, our extension complements existing datasets, creating a benchmark DGM4+ that tests detectors on both local and global reasoning. This resource is intended to strengthen evaluation of multimodal models such as HAMMER, which currently struggle with FG-BG inconsistencies. We release our DGM4+ dataset and generation script at this https URL</li>
<li><strong>摘要：</strong>生成模型的快速进步显着降低了产生令人信服的多模式虚假信息的障碍。制作的图像和操纵字幕越来越同时发生，以创建有说服力的虚假叙述。尽管检测和接地多模式媒体操纵（DGM4）数据集在该领域建立了研究基础，但它仅限于当地操作，例如面部掉期，属性编辑和字幕变化。这留下了一个关键的差距：全球矛盾之处，例如不匹配的前景和背景，现在在现实世界中很普遍。为了解决这个问题，我们使用5,000个高质量的样品扩展了DGM4，这些样本引入了前后背景（FG-BG）不匹配及其混合动力车和文本操作。使用OpenAI的GPT-Image-1和精心设计的提示，我们生成以人为本的新闻型图像，将真实的人物放在荒谬或不可能的背景中（例如，老师平静地向Mars表面的学生讲话）。字幕是在三个条件下产生的：文字，文本属性和文本拆分，产生了三个新的操纵类别：FG-BG，FG-BG+TA和FG-BG+TS。质量控制管道强制执行一到三个可见的面孔，感知性障碍，基于OCR的文本擦洗和现实的标题长度。通过引入全局操作，我们的扩展程序补充了现有数据集，创建了基准DGM4+，该数据集在本地和全球推理上测试检测器。该资源旨在加强对目前与FG-BG不一致的HAMMER等多模型模型的评估。我们在此HTTPS URL上发布DGM4+数据集和生成脚本</li>
</ul>

<h3>Title: Text-to-Scene with Large Reasoning Models</h3>
<ul>
<li><strong>Authors: </strong>Frédéric Berdoz, Luca A. Lanzendörfer, Nick Tuninga, Roger Wattenhofer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26091">https://arxiv.org/abs/2509.26091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26091">https://arxiv.org/pdf/2509.26091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26091]] Text-to-Scene with Large Reasoning Models(https://arxiv.org/abs/2509.26091)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Prompt-driven scene synthesis allows users to generate complete 3D environments from textual descriptions. Current text-to-scene methods often struggle with complex geometries and object transformations, and tend to show weak adherence to complex instructions. We address these limitations by introducing Reason-3D, a text-to-scene model powered by large reasoning models (LRMs). Reason-3D integrates object retrieval using captions covering physical, functional, and contextual attributes. Reason-3D then places the selected objects based on implicit and explicit layout constraints, and refines their positions with collision-aware spatial reasoning. Evaluated on instructions ranging from simple to complex indoor configurations, Reason-3D significantly outperforms previous methods in human-rated visual fidelity, adherence to constraints, and asset retrieval quality. Beyond its contribution to the field of text-to-scene generation, our work showcases the advanced spatial reasoning abilities of modern LRMs. Additionally, we release the codebase to further the research in object retrieval and placement with LRMs.</li>
<li><strong>摘要：</strong>及时驱动的场景综合允许用户从文本描述中生成完整的3D环境。当前的文本到现实方法通常会在复杂的几何形状和对象转换中挣扎，并倾向于表现出对复杂指令的依从性较弱。我们通过引入推理3D（由大型推理模型（LRMS）提供动力的文本到现场模型）来解决这些限制。 Reason-3D使用涵盖物理，功能和上下文属性的标题集成对象检索。 Reason-3D然后根据隐式和明确的布局约束放置所选对象，并通过相撞感知的空间推理来完善其位置。根据说明，从简单到复杂的室内配置进行了评估，原因3D显着超过了人级视觉保真度，遵守约束和资产检索质量的先前方法。除了它对文本到现场一代领域的贡献外，我们的工作还展示了现代LRM的先进空间推理能力。此外，我们发布了代码库，以进一步研究对象检索和使用LRMS的位置。</li>
</ul>

<h3>Title: EVODiff: Entropy-aware Variance Optimized Diffusion Inference</h3>
<ul>
<li><strong>Authors: </strong>Shigui Li, Wei Chen, Delu Zeng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IT, cs.LG, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26096">https://arxiv.org/abs/2509.26096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26096">https://arxiv.org/pdf/2509.26096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26096]] EVODiff: Entropy-aware Variance Optimized Diffusion Inference(https://arxiv.org/abs/2509.26096)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models (DMs) excel in image generation, but suffer from slow inference and the training-inference discrepancies. Although gradient-based solvers like DPM-Solver accelerate the denoising inference, they lack theoretical foundations in information transmission efficiency. In this work, we introduce an information-theoretic perspective on the inference processes of DMs, revealing that successful denoising fundamentally reduces conditional entropy in reverse transitions. This principle leads to our key insights into the inference processes: (1) data prediction parameterization outperforms its noise counterpart, and (2) optimizing conditional variance offers a reference-free way to minimize both transition and reconstruction errors. Based on these insights, we propose an entropy-aware variance optimized method for the generative process of DMs, called EVODiff, which systematically reduces uncertainty by optimizing conditional entropy during denoising. Extensive experiments on DMs validate our insights and demonstrate that our method significantly and consistently outperforms state-of-the-art (SOTA) gradient-based solvers. For example, compared to the DPM-Solver++, EVODiff reduces the reconstruction error by up to 45.5\% (FID improves from 5.10 to 2.78) at 10 function evaluations (NFE) on CIFAR-10, cuts the NFE cost by 25\% (from 20 to 15 NFE) for high-quality samples on ImageNet-256, and improves text-to-image generation while reducing artifacts. Code is available at this https URL.</li>
<li><strong>摘要：</strong>扩散模型（DMS）在图像产生中脱颖而出，但遭受了缓慢的推理和训练推断差异的困扰。尽管基于梯度的求解器（例如DPM溶剂）加速了降解推断，但它们缺乏信息传输效率的理论基础。在这项工作中，我们介绍了关于DMS推理过程的信息理论观点，表明成功的deNOCON从根本上减少了反向过渡中的条件熵。该原理导致我们对推理过程的关键见解：（1）数据预测参数化优于其噪声对应物，并且（2）优化条件差异提供了一种无参考的方法来最大程度地减少过渡和重建错误。基于这些见解，我们为DMS的生成过程（称为Evodiff）提出了一种熵感知的方差优化方法，该方法通过优化DeNosing期间的条件熵来系统地降低不确定性。关于DMS的广泛实验验证了我们的见解，并证明我们的方法显着，始终如一地超过了最先进的（SOTA）基于梯度的求解器。例如，与DPM-Solver ++相比，Evodiff在10个功能评估（NFE）（NFE）中最多将重建误差降低到45.5％（FID从5.10提高到2.78），将NFE成本降低了25 \％（从20到15 NFE），而在Imagenet-25上的较高量将其改进，并改善了ARTIF spect-25，并将其改进。代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: EchoGen: Generating Visual Echoes in Any Scene via Feed-Forward Subject-Driven Auto-Regressive Model</h3>
<ul>
<li><strong>Authors: </strong>Ruixiao Dong, Zhendong Wang, Keli Liu, Li Li, Ying Chen, Kai Li, Daowen Li, Houqiang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26127">https://arxiv.org/abs/2509.26127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26127">https://arxiv.org/pdf/2509.26127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26127]] EchoGen: Generating Visual Echoes in Any Scene via Feed-Forward Subject-Driven Auto-Regressive Model(https://arxiv.org/abs/2509.26127)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Subject-driven generation is a critical task in creative AI; yet current state-of-the-art methods present a stark trade-off. They either rely on computationally expensive, per-subject fine-tuning, sacrificing efficiency and zero-shot capability, or employ feed-forward architectures built on diffusion models, which are inherently plagued by slow inference speeds. Visual Auto-Regressive (VAR) models are renowned for their rapid sampling speeds and strong generative quality, making them an ideal yet underexplored foundation for resolving this tension. To bridge this gap, we introduce EchoGen, a pioneering framework that empowers VAR models with subject-driven generation capabilities. The core design of EchoGen is an effective dual-path injection strategy that disentangles a subject's high-level semantic identity from its low-level fine-grained details, enabling enhanced controllability and fidelity. We employ a semantic encoder to extract the subject's abstract identity, which is injected through decoupled cross-attention to guide the overall composition. Concurrently, a content encoder captures intricate visual details, which are integrated via a multi-modal attention mechanism to ensure high-fidelity texture and structural preservation. To the best of our knowledge, EchoGen is the first feed-forward subject-driven framework built upon VAR models. Both quantitative and qualitative results substantiate our design, demonstrating that EchoGen achieves subject fidelity and image quality comparable to state-of-the-art diffusion-based methods with significantly lower sampling latency. Code and models will be released soon.</li>
<li><strong>摘要：</strong>主题驱动的一代是Creative AI的关键任务；然而，当前的最新方法呈现出鲜明的权衡。他们要么依赖计算昂贵的，每个受试者的微调，牺牲效率和零发功能，要么采用基于扩散模型的馈送前架构，这些模型固有地受到缓慢的推理速度困扰。视觉自动回归（VAR）型号以其快速采样速度和强大的生成质量而闻名，使其成为解决这种张力的理想但毫无疑问的基础。为了弥合这一差距，我们引入了Echogen，这是一个开创性的框架，该框架使VAR模型具有主题驱动的生成能力。 Echogen的核心设计是一种有效的双路注射策略，它使受试者的高级语义身份从其低级细粒细节中解散，从而增强了可控性和忠诚度。我们采用语义编码器来提取受试者的抽象身份，该标志是通过脱钩的交叉注意来注入的，以指导整体组成。同时，内容编码器捕获了复杂的视觉细节，这些细节是通过多模式注意机制集成的，以确保高保真性纹理和结构保存。据我们所知，Echogen是第一个以VAR模型为基础的前馈受试者驱动的框架。定量和定性结果都证实了我们的设计，这表明Echogen实现了与最先进的基于扩散的方法相当的标志忠诚度和图像质量，其采样延迟显着降低。代码和模型将很快发布。</li>
</ul>

<h3>Title: IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Guo, Chuanhao Yan, Xingqian Xu, Yulin Wang, Kai Wang, Gao Huang, Humphrey Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26231">https://arxiv.org/abs/2509.26231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26231">https://arxiv.org/pdf/2509.26231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26231]] IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance(https://arxiv.org/abs/2509.26231)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Ensuring precise multimodal alignment between diffusion-generated images and input prompts has been a long-standing challenge. Earlier works finetune diffusion weight using high-quality preference data, which tends to be limited and difficult to scale up. Recent editing-based methods further refine local regions of generated images but may compromise overall image quality. In this work, we propose Implicit Multimodal Guidance (IMG), a novel re-generation-based multimodal alignment framework that requires no extra data or editing operations. Specifically, given a generated image and its prompt, IMG a) utilizes a multimodal large language model (MLLM) to identify misalignments; b) introduces an Implicit Aligner that manipulates diffusion conditioning features to reduce misalignments and enable re-generation; and c) formulates the re-alignment goal into a trainable objective, namely Iteratively Updated Preference Objective. Extensive qualitative and quantitative evaluations on SDXL, SDXL-DPO, and FLUX show that IMG outperforms existing alignment methods. Furthermore, IMG acts as a flexible plug-and-play adapter, seamlessly enhancing prior finetuning-based alignment methods. Our code will be available at this https URL.</li>
<li><strong>摘要：</strong>确保扩散生成图像和输入提示之间的精确多模式对齐是一个长期的挑战。较早的作品使用高质量的偏好数据进行了芬日扩散权重，这些数据往往受到限制且难以扩大。最近的基于编辑的方法进一步完善了生成图像的本地区域，但可能会损害整体图像质量。在这项工作中，我们提出了隐式多模式指导（IMG），这是一种基于重新生成的新型多模式对齐框架，不需要额外的数据或编辑操作。具体而言，鉴于生成的图像及其提示，IMG a）使用多模式大语模型（MLLM）来识别未对准； b）引入一个隐式对准器，该对准器操纵扩散条件特征以减少未对准并实现重新生成； c）将重新调整目标制定为一个可训练的目标，即迭代更新的偏好目标。对SDXL，SDXL-DPO和Flux的广泛定性和定量评估表明，IMG的表现优于现有的对齐方法。此外，IMG充当灵活的插件适配器，无缝增强了先前的基于Finetuning的对准方法。我们的代码将在此HTTPS URL上可用。</li>
</ul>

<h3>Title: ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency Estimation</h3>
<ul>
<li><strong>Authors: </strong>Edoardo Bianchi, Jacopo Staiano, Antonio Liotta</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26278">https://arxiv.org/abs/2509.26278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26278">https://arxiv.org/pdf/2509.26278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26278]] ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency Estimation(https://arxiv.org/abs/2509.26278)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Existing approaches to skill proficiency estimation often rely on black-box video classifiers, ignoring multi-view context and lacking explainability. We present ProfVLM, a compact vision-language model that reformulates this task as generative reasoning: it jointly predicts skill level and generates expert-like feedback from egocentric and exocentric videos. Central to our method is an AttentiveGatedProjector that dynamically fuses multi-view features, projected from a frozen TimeSformer backbone into a language model tuned for feedback generation. Trained on EgoExo4D with expert commentaries, ProfVLM surpasses state-of-the-art methods while using up to 20x fewer parameters and reducing training time by up to 60%. Our approach not only achieves superior accuracy across diverse activities, but also outputs natural language critiques aligned with performance, offering transparent reasoning. These results highlight generative vision-language modeling as a powerful new direction for skill assessment.</li>
<li><strong>摘要：</strong>现有的技能能力估计方法通常依赖于黑框视频分类器，忽略了多视图上下文，缺乏解释性。我们提出了PROFVLM，这是一种紧凑的视觉语言模型，将这项任务重新设计为生成推理：它共同预测技能水平并产生来自以自我为中心和外向视频的专家式反馈。我们方法的核心是一个细心的项目，它动态融合了多视图功能，该功能从冷冻时代形式的主链投影到一个用于反馈生成的语言模型中。 Profvlm在Egoexo4d接受了Egoexo4d的培训，超过了最先进的方法，同时使用了多达20倍的参数，并将培训时间减少了60％。我们的方法不仅在各种活动中达到了卓越的准确性，而且还输出了与性能一致的自然语言批评，并提供了透明的推理。这些结果强调了生成视觉模型作为技能评估的强大新方向。</li>
</ul>

<h3>Title: Reframing Generative Models for Physical Systems using Stochastic Interpolants</h3>
<ul>
<li><strong>Authors: </strong>Anthony Zhou, Alexander Wikner, Amaury Lancelin, Pedram Hassanzadeh, Amir Barati Farimani</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26282">https://arxiv.org/abs/2509.26282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26282">https://arxiv.org/pdf/2509.26282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26282]] Reframing Generative Models for Physical Systems using Stochastic Interpolants(https://arxiv.org/abs/2509.26282)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models have recently emerged as powerful surrogates for physical systems, demonstrating increased accuracy, stability, and/or statistical fidelity. Most approaches rely on iteratively denoising a Gaussian, a choice that may not be the most effective for autoregressive prediction tasks in PDEs and dynamical systems such as climate. In this work, we benchmark generative models across diverse physical domains and tasks, and highlight the role of stochastic interpolants. By directly learning a stochastic process between current and future states, stochastic interpolants can leverage the proximity of successive physical distributions. This allows for generative models that can use fewer sampling steps and produce more accurate predictions than models relying on transporting Gaussian noise. Our experiments suggest that generative models need to balance deterministic accuracy, spectral consistency, and probabilistic calibration, and that stochastic interpolants can potentially fulfill these requirements by adjusting their sampling. This study establishes stochastic interpolants as a competitive baseline for physical emulation and gives insight into the abilities of different generative modeling frameworks.</li>
<li><strong>摘要：</strong>生成模型最近已成为物理系统的强大替代物，证明了提高的准确性，稳定性和/或统计保真度。大多数方法都依赖于迭代地降级高斯，这是对PDE和动态系统（例如气候）中自回归预测任务最有效的选择。在这项工作中，我们基准了各种物理领域和任务的生成模型，并强调了随机插值的作用。通过直接学习当前状态和未来状态之间的随机过程，随机插值可以利用连续的物理分布的接近性。与依靠运输高斯噪声相比，这允许生成模型可以使用更少的采样步骤并产生更准确的预测。我们的实验表明，生成模型需要平衡确定性的准确性，光谱一致性和概率校准，并且随机插值剂可以通过调整其采样来潜在地满足这些要求。这项研究将随机插入剂建立为物理仿真的竞争基线，并深入了解不同生成建模框架的能力。</li>
</ul>

<h3>Title: FLOWER: A Flow-Matching Solver for Inverse Problems</h3>
<ul>
<li><strong>Authors: </strong>Mehrsa Pourya, Bassam El Rawas, Michael Unser</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26287">https://arxiv.org/abs/2509.26287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26287">https://arxiv.org/pdf/2509.26287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26287]] FLOWER: A Flow-Matching Solver for Inverse Problems(https://arxiv.org/abs/2509.26287)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce Flower, a solver for inverse problems. It leverages a pre-trained flow model to produce reconstructions that are consistent with the observed measurements. Flower operates through an iterative procedure over three steps: (i) a flow-consistent destination estimation, where the velocity network predicts a denoised target; (ii) a refinement step that projects the estimated destination onto a feasible set defined by the forward operator; and (iii) a time-progression step that re-projects the refined destination along the flow trajectory. We provide a theoretical analysis that demonstrates how Flower approximates Bayesian posterior sampling, thereby unifying perspectives from plug-and-play methods and generative inverse solvers. On the practical side, Flower achieves state-of-the-art reconstruction quality while using nearly identical hyperparameters across various inverse problems.</li>
<li><strong>摘要：</strong>我们介绍了Flower，这是一个逆问题的求解器。它利用预先训练的流程模型来产生与观察到的测量结果一致的重建。花通过三个步骤的迭代过程运行：（i）流动矛盾的目的地估计，其中速度网络预测了一个被剥落的目标； （ii）将估计目的地投射到前向操作员定义的可行集合的改进步骤； （iii）沿流动轨迹重新投影精制的目的地的时间产生步骤。我们提供了理论分析，该分析表明花如何近似贝叶斯后方采样，从而从插件方法和生成的逆求器中统一了视角。在实用方面，花朵可以达到最新的重建质量，同时在各种反问题上使用几乎相同的超参数。</li>
</ul>

<h3>Title: Attribution-Guided Decoding</h3>
<ul>
<li><strong>Authors: </strong>Piotr Komorowski, Elena Golimblevskaia, Reduan Achtibat, Thomas Wiegand, Sebastian Lapuschkin, Wojciech Samek</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26307">https://arxiv.org/abs/2509.26307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26307">https://arxiv.org/pdf/2509.26307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26307]] Attribution-Guided Decoding(https://arxiv.org/abs/2509.26307)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The capacity of Large Language Models (LLMs) to follow complex instructions and generate factually accurate text is critical for their real-world application. However, standard decoding methods often fail to robustly satisfy these requirements, while existing control techniques frequently degrade general output quality. In this work, we introduce Attribution-Guided Decoding (AGD), an interpretability-based decoding strategy. Instead of directly manipulating model activations, AGD considers a set of high-probability output token candidates and selects the one that exhibits the highest attribution to a user-defined Region of Interest (ROI). This ROI can be flexibly defined over different parts of the model's input or internal components, allowing AGD to steer generation towards various desirable behaviors. We demonstrate AGD's efficacy across three challenging domains. For instruction following, we show that AGD significantly boosts adherence (e.g., improving the overall success rate on Llama 3.1 from 66.0% to 79.1%). For knowledge-intensive tasks, we show that guiding generation towards usage of internal knowledge components or contextual sources can reduce hallucinations and improve factual accuracy in both closed-book and open-book settings. Furthermore, we propose an adaptive, entropy-based variant of AGD that mitigates quality degradation and reduces computational overhead by applying guidance only when the model is uncertain. Our work presents a versatile, more interpretable, and effective method for enhancing the reliability of modern LLMs.</li>
<li><strong>摘要：</strong>大语言模型（LLMS）遵循复杂说明并生成事实准确的文本的能力对于其现实世界应用至关重要。但是，标准解码方法通常无法牢固地满足这些要求，而现有的控制技术经常降低一般输出质量。在这项工作中，我们介绍了归因引导的解码（AGD），这是一种基于解释性的解码策略。 AGD没有直接操纵模型激活，而是考虑了一组高概率的输出令牌令牌候选人，并选择了对用户定义的感兴趣区域（ROI）表现出最高归因的候选者。该ROI可以在模型输入或内部组件的不同部分中灵活地定义，从而使AGD可以转向各种理想的行为。我们证明了AGD在三个具有挑战性的领域中的功效。对于以下教学，我们表明AGD显着提高了依从性（例如，将Llama 3.1的总体成功率从66.0％提高到79.1％）。对于知识密集的任务，我们表明，指导生成用于使用内部知识组件或上下文来源可以降低幻觉并提高闭和开书设置的事实准确性。此外，我们提出了一种基于自适应的，基于熵的AGD变体，可减少质量降解，并仅在模型不确定时应用指导来减少计算开销。我们的工作提出了一种增强现代LLM的可靠性的多功能，更容易解释和有效的方法。</li>
</ul>

<h3>Title: Continuous Space-Time Video Super-Resolution with 3D Fourier Fields</h3>
<ul>
<li><strong>Authors: </strong>Alexander Becker, Julius Erbach, Dominik Narnhofer, Konrad Schindler</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26325">https://arxiv.org/abs/2509.26325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26325">https://arxiv.org/pdf/2509.26325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26325]] Continuous Space-Time Video Super-Resolution with 3D Fourier Fields(https://arxiv.org/abs/2509.26325)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>We introduce a novel formulation for continuous space-time video super-resolution. Instead of decoupling the representation of a video sequence into separate spatial and temporal components and relying on brittle, explicit frame warping for motion compensation, we encode video as a continuous, spatio-temporally coherent 3D Video Fourier Field (VFF). That representation offers three key advantages: (1) it enables cheap, flexible sampling at arbitrary locations in space and time; (2) it is able to simultaneously capture fine spatial detail and smooth temporal dynamics; and (3) it offers the possibility to include an analytical, Gaussian point spread function in the sampling to ensure aliasing-free reconstruction at arbitrary scale. The coefficients of the proposed, Fourier-like sinusoidal basis are predicted with a neural encoder with a large spatio-temporal receptive field, conditioned on the low-resolution input video. Through extensive experiments, we show that our joint modeling substantially improves both spatial and temporal super-resolution and sets a new state of the art for multiple benchmarks: across a wide range of upscaling factors, it delivers sharper and temporally more consistent reconstructions than existing baselines, while being computationally more efficient. Project page: this https URL.</li>
<li><strong>摘要：</strong>我们介绍了一种新颖的配方，用于连续时空视频超分辨率。我们没有将视频序列表示为单独的空间和时间组件并依靠脆弱的框架翘曲以进行运动补偿，而是将视频编码为连续的，时空上的3D视频傅立叶（VFF）。该表示形式提供了三个关键优势：（1）它可以在时空的任意位置进行廉价，灵活的抽样； （2）它能够同时捕获精细的空间细节和光滑的时间动力学； （3）它提供了在抽样中包含分析性高斯点扩散函数的可能性，以确保以任意规模的无均匀重建。通过具有较大时空接收场的神经编码器预测所提出的傅立叶样正弦基础的系数，以低分辨率输入视频为条件。通过广泛的实验，我们表明我们的联合建模基本上改善了空间和时间超级分辨率，并为多个基准设置了新的艺术状态：在广泛的高尺度因素上，它比现有基础线更敏捷，并且在时间上更加一致，同时更加一致，同时计算高效。项目页面：此HTTPS URL。</li>
</ul>

<h3>Title: Data-to-Energy Stochastic Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Kirill Tamogashev, Nikolay Malkin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26364">https://arxiv.org/abs/2509.26364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26364">https://arxiv.org/pdf/2509.26364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26364]] Data-to-Energy Stochastic Dynamics(https://arxiv.org/abs/2509.26364)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The Schrödinger bridge problem is concerned with finding a stochastic dynamical system bridging two marginal distributions that minimises a certain transportation cost. This problem, which represents a generalisation of optimal transport to the stochastic case, has received attention due to its connections to diffusion models and flow matching, as well as its applications in the natural sciences. However, all existing algorithms allow to infer such dynamics only for cases where samples from both distributions are available. In this paper, we propose the first general method for modelling Schrödinger bridges when one (or both) distributions are given by their unnormalised densities, with no access to data samples. Our algorithm relies on a generalisation of the iterative proportional fitting (IPF) procedure to the data-free case, inspired by recent developments in off-policy reinforcement learning for training of diffusion samplers. We demonstrate the efficacy of the proposed data-to-energy IPF on synthetic problems, finding that it can successfully learn transports between multimodal distributions. As a secondary consequence of our reinforcement learning formulation, which assumes a fixed time discretisation scheme for the dynamics, we find that existing data-to-data Schrödinger bridge algorithms can be substantially improved by learning the diffusion coefficient of the dynamics. Finally, we apply the newly developed algorithm to the problem of sampling posterior distributions in latent spaces of generative models, thus creating a data-free image-to-image translation method. Code: this https URL</li>
<li><strong>摘要：</strong>Schrödinger桥的问题涉及找到一个随机动力系统，该系统桥接了两个边缘分布，从而最大程度地减少了一定的运输成本。这个问题代表了最佳运输到随机情况的概括，由于其与扩散模型和流动匹配的联系以及其在自然科学中的应用，因此受到了关注。但是，所有现有的算法允许仅在可用的两个分布的样本的情况下推断这种动态。在本文中，我们提出了第一个对Schrödinger桥建模的通用方法，当一个（或两个）分布由其无标准密度给出，而无需访问数据样本。我们的算法依赖于对无数据案例的迭代比例拟合程序（IPF）程序的概括，这是受到近期在政策外增强学习中的发展启发的，用于培训扩散样本者的培训。我们证明了所提出的数据到能源IPF对合成问题的功效，发现它可以成功地学习多模式分布之间的运输。作为我们的强化学习公式的次要结果，该公式采用了动力学的固定时间离散方案，我们发现可以通过学习动态的扩散系数来显着改善现有的数据对数据之间的数据之间。最后，我们将新开发的算法应用于生成模型的潜在空间中的后验分布的问题，从而创建了一种无数据的图像到图像转换方法。代码：此HTTPS URL</li>
</ul>

<h3>Title: Go with Your Gut: Scaling Confidence for Autoregressive Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Harold Haodong Chen, Xianfeng Wu, Wen-Jie Shu, Rongjin Guo, Disen Lan, Harry Yang, Ying-Cong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26376">https://arxiv.org/abs/2509.26376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26376">https://arxiv.org/pdf/2509.26376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26376]] Go with Your Gut: Scaling Confidence for Autoregressive Image Generation(https://arxiv.org/abs/2509.26376)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Test-time scaling (TTS) has demonstrated remarkable success in enhancing large language models, yet its application to next-token prediction (NTP) autoregressive (AR) image generation remains largely uncharted. Existing TTS approaches for visual AR (VAR), which rely on frequent partial decoding and external reward models, are ill-suited for NTP-based image generation due to the inherent incompleteness of intermediate decoding results. To bridge this gap, we introduce ScalingAR, the first TTS framework specifically designed for NTP-based AR image generation that eliminates the need for early decoding or auxiliary rewards. ScalingAR leverages token entropy as a novel signal in visual token generation and operates at two complementary scaling levels: (i) Profile Level, which streams a calibrated confidence state by fusing intrinsic and conditional signals; and (ii) Policy Level, which utilizes this state to adaptively terminate low-confidence trajectories and dynamically schedule guidance for phase-appropriate conditioning strength. Experiments on both general and compositional benchmarks show that ScalingAR (1) improves base models by 12.5% on GenEval and 15.2% on TIIF-Bench, (2) efficiently reduces visual token consumption by 62.0% while outperforming baselines, and (3) successfully enhances robustness, mitigating performance drops by 26.0% in challenging scenarios.</li>
<li><strong>摘要：</strong>测试时间缩放（TTS）在增强大型语言模型方面取得了巨大的成功，但其应用于下一步的预测（NTP）自动回归（AR）图像生成仍然很大程度上尚未大脑。依赖于频繁的部分解码和外部奖励模型的Visual AR（VAR）的现有TTS方法不适合基于NTP的图像生成，因为中间解码结果的固有不完整。为了弥合这一差距，我们介绍了Scalingar，这是专门为基于NTP的AR图像生成设计的第一个TTS框架，它消除了对早期解码或辅助奖励的需求。 ScaLingar在视觉令牌生成中杠杆化的熵是一种新的信号，并在两个互补的缩放水平上运行：（i）轮廓级别，通过融合固有和条件信号来流式传输校准的置信度状态； （ii）政策水平，利用该状态适应性地终止低信心轨迹，并动态地安排指南以适合相适应的条件强度。对一般和组成基准的实验都表明，缩放级（1）将基本模型提高了12.5％，在TIIF板凳上提高了15.2％，（2）有效地将视觉令牌的消耗降低了62.0％，而质量超过了质量，而（3）成功增强了稳健性，可减轻稳健性的性能下降，而降低了26.0.0％的挑战。</li>
</ul>

<h3>Title: MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Chenhui Zhu, Yilu Wu, Shuai Wang, Gangshan Wu, Limin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26391">https://arxiv.org/abs/2509.26391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26391">https://arxiv.org/pdf/2509.26391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26391]] MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation(https://arxiv.org/abs/2509.26391)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Image-to-video generation has made remarkable progress with the advancements in diffusion models, yet generating videos with realistic motion remains highly challenging. This difficulty arises from the complexity of accurately modeling motion, which involves capturing physical constraints, object interactions, and domain-specific dynamics that are not easily generalized across diverse scenarios. To address this, we propose MotionRAG, a retrieval-augmented framework that enhances motion realism by adapting motion priors from relevant reference videos through Context-Aware Motion Adaptation (CAMA). The key technical innovations include: (i) a retrieval-based pipeline extracting high-level motion features using video encoder and specialized resamplers to distill semantic motion representations; (ii) an in-context learning approach for motion adaptation implemented through a causal transformer architecture; (iii) an attention-based motion injection adapter that seamlessly integrates transferred motion features into pretrained video diffusion models. Extensive experiments demonstrate that our method achieves significant improvements across multiple domains and various base models, all with negligible computational overhead during inference. Furthermore, our modular design enables zero-shot generalization to new domains by simply updating the retrieval database without retraining any components. This research enhances the core capability of video generation systems by enabling the effective retrieval and transfer of motion priors, facilitating the synthesis of realistic motion dynamics.</li>
<li><strong>摘要：</strong>图像到视频的生成在扩散模型的进步中取得了显着的进步，但是以现实的运动生成视频仍然是高度挑战性的。这个困难源于准确建模运动的复杂性，涉及捕获物理约束，对象相互作用和特定领域特定的动态，这些动力不容易在各种情况下概括。为了解决这个问题，我们提出了MotionRag，这是一个检索框架的框架，通过通过上下文感知运动适应（CAMA）从相关参考视频中调整运动先验，从而增强运动现实主义。关键的技术创新包括：（i）使用视频编码器和专门的重采样器提取高级运动功能的基于检索的管道来提取语义运动表示； （ii）通过因果变压器体系结构实施的一种运动适应性的内在学习方法； （iii）基于注意力的运动注射适配器，将传递的运动特征无缝整合到预验证的视频扩散模型中。广泛的实验表明，我们的方法在推断过程中均具有可忽略的计算开销，从而在多个领域和各种基本模型之间取得了重大改进。此外，我们的模块化设计可以通过简单地更新检索数据库而无需重新培训任何组件，从而使对新域的零弹性概括。这项研究通过实现有效检索和转移运动先验，从而增强了视频生成系统的核心能力，从而促进了现实运动动力学的综合。</li>
</ul>

<h3>Title: Image-Difficulty-Aware Evaluation of Super-Resolution Models</h3>
<ul>
<li><strong>Authors: </strong>Atakan Topaloglu, Ahmet Bilican, Cansu Korkmaz, A. Murat Tekalp</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26398">https://arxiv.org/abs/2509.26398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26398">https://arxiv.org/pdf/2509.26398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26398]] Image-Difficulty-Aware Evaluation of Super-Resolution Models(https://arxiv.org/abs/2509.26398)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Image super-resolution models are commonly evaluated by average scores (over some benchmark test sets), which fail to reflect the performance of these models on images of varying difficulty and that some models generate artifacts on certain difficult images, which is not reflected by the average scores. We propose difficulty-aware performance evaluation procedures to better differentiate between SISR models that produce visually different results on some images but yield close average performance scores over the entire test set. In particular, we propose two image-difficulty measures, the high-frequency index and rotation-invariant edge index, to predict those test images, where a model would yield significantly better visual results over another model, and an evaluation method where these visual differences are reflected on objective measures. Experimental results demonstrate the effectiveness of the proposed image-difficulty measures and evaluation methodology.</li>
<li><strong>摘要：</strong>图像超分辨率模型通常通过平均得分（在某些基准测试集上）进行评估，这些分数无法反映这些模型在不同难度的图像上的性能，并且某些模型在某些困难图像上产生伪影，这并不反映平均得分。我们提出了难以感知的性能评估程序，以更好地区分某些图像上的视觉结果但在整个测试集中产生的平均性能得分。特别是，我们提出了两项​​图像缺陷措施，即高频指数和旋转不变的边缘指数，以预测那些测试图像，其中模型将与另一个模型相比，在这些模型中产生更好的视觉结果，以及在客观措施上反映这些视觉差异的评估方法。实验结果证明了所提出的图像缺陷措施和评估方法的有效性。</li>
</ul>

<h3>Title: Refine Drugs, Don't Complete Them: Uniform-Source Discrete Flows for Fragment-Based Drug Discovery</h3>
<ul>
<li><strong>Authors: </strong>Benno Kaech, Luis Wyss, Karsten Borgwardt, Gianvito Grasso</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26405">https://arxiv.org/abs/2509.26405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26405">https://arxiv.org/pdf/2509.26405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26405]] Refine Drugs, Don't Complete Them: Uniform-Source Discrete Flows for Fragment-Based Drug Discovery(https://arxiv.org/abs/2509.26405)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We introduce InVirtuoGen, a discrete flow generative model for fragmented SMILES for de novo and fragment-constrained generation, and target-property/lead optimization of small molecules. The model learns to transform a uniform source over all possible tokens into the data distribution. Unlike masked models, its training loss accounts for predictions on all sequence positions at every denoising step, shifting the generation paradigm from completion to refinement, and decoupling the number of sampling steps from the sequence length. For \textit{de novo} generation, InVirtuoGen achieves a stronger quality-diversity pareto frontier than prior fragment-based models and competitive performance on fragment-constrained tasks. For property and lead optimization, we propose a hybrid scheme that combines a genetic algorithm with a Proximal Property Optimization fine-tuning strategy adapted to discrete flows. Our approach sets a new state-of-the-art on the Practical Molecular Optimization benchmark, measured by top-10 AUC across tasks, and yields higher docking scores in lead optimization than previous baselines. InVirtuoGen thus establishes a versatile generative foundation for drug discovery, from early hit finding to multi-objective lead optimization. We further contribute to open science by releasing pretrained checkpoints and code, making our results fully reproducible\footnote{this https URL}.</li>
<li><strong>摘要：</strong>我们引入了Invirtuen，这是一种用于从头开始和碎片约束的产生的碎片微笑的离散流生成模型，以及小分子的靶标/铅/铅优化。该模型学会将所有可能的令牌上的统一源转换为数据分布。与蒙版模型不同，其训练损失是每个降级步骤中所有序列位置的预测，将生成范式从完成转移到细化，并将采样步骤的数量与序列长度解耦。对于\ textit {de novo}生成，Invirtuogen比以前的基于碎片的模型和在碎片约束任务上的竞争性能相比，获得了更强的质量多样性帕累托前沿。对于属性和铅优化，我们提出了一种混合方案，该方案将遗传算法与适合离散流的近端属性优化策略相结合。我们的方法为实用的分子优化基准设置了一个新的最先进的方法，该基准通过TOP-10 AUC跨任务衡量，并且比以前的基线相比，铅优化的对接得分更高。因此，Invirtuogen从早期发现到多目标铅优化建立了一种多功能的药物发现基础。我们通过释放验证的检查点和代码进一步为开放科学做出了贡献，使我们的结果完全可重复\ footNote {this HTTPS url}。</li>
</ul>

<h3>Title: Post-Training Quantization via Residual Truncation and Zero Suppression for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Donghoon Kim, Dongyoung Lee, Ik Joon Chang, Sung-Ho Bae</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26436">https://arxiv.org/abs/2509.26436</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26436">https://arxiv.org/pdf/2509.26436</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26436]] Post-Training Quantization via Residual Truncation and Zero Suppression for Diffusion Models(https://arxiv.org/abs/2509.26436)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models achieve high-quality image generation but face deployment challenges due to their high computational requirements. Although 8-bit outlier-aware post-training quantization (PTQ) matches full-precision performance, extending PTQ to 4 bits remains challenging. Larger step sizes in 4-bit quantization amplify rounding errors in dense, low-magnitude activations, leading to the loss of fine-grained textures. We hypothesize that not only outliers but also small activations are critical for texture fidelity. To this end, we propose Quantization via Residual Truncation and Zero Suppression (QuaRTZ), a 4-bit PTQ scheme for diffusion models. QuaRTZ applies 8-bit min-max quantization for outlier handling and compresses to 4 bits via leading-zero suppression to retain LSBs, thereby preserving texture details. Our approach reduces rounding errors and improves quantization efficiency by balancing outlier preservation and LSB precision. Both theoretical derivations and empirical evaluations demonstrate the generalizability of QuaRTZ across diverse activation distributions. Notably, 4-bit QuaRTZ achieves an FID of 6.98 on FLUX.1-schnell, outperforming SVDQuant that requires auxiliary FP16 branches.</li>
<li><strong>摘要：</strong>扩散模型可实现高质量的图像生成，但由于其高计算要求而面临部署挑战。尽管8位离群值的训练后量化（PTQ）与完整精确的性能相匹配，但将PTQ扩展到4位仍然具有挑战性。 4位量化中的较大步骤大小扩大了密集的低稳定性激活中的圆形误差，从而导致细粒纹理的损失。我们假设不仅离群值，而且小型激活对于纹理保真度至关重要。为此，我们建议通过残留截断和零抑制（QUARTZ）进行量化，这是一种用于扩散模型的4位PTQ方案。石英通过领先的零抑制作用将8位最小值量化用于离群处理，并将其压缩到4位以保留LSB，从而保留纹理细节。我们的方法可通过平衡离群保护和LSB精度来降低舍入错误并提高量化效率。理论推导和经验评估都证明了石英在各种激活分布中的普遍性。值得注意的是，4位石英在Flux上达到6.98的FID。1-SCHNELL，优于需要辅助FP16分支的SVDQuant。</li>
</ul>

<h3>Title: DiVeQ: Differentiable Vector Quantization Using the Reparameterization Trick</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Hassan Vali, Tom Bäckström, Arno Solin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26469">https://arxiv.org/abs/2509.26469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26469">https://arxiv.org/pdf/2509.26469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26469]] DiVeQ: Differentiable Vector Quantization Using the Reparameterization Trick(https://arxiv.org/abs/2509.26469)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vector quantization is common in deep models, yet its hard assignments block gradients and hinder end-to-end training. We propose DiVeQ, which treats quantization as adding an error vector that mimics the quantization distortion, keeping the forward pass hard while letting gradients flow. We also present a space-filling variant (SF-DiVeQ) that assigns to a curve constructed by the lines connecting codewords, resulting in less quantization error and full codebook usage. Both methods train end-to-end without requiring auxiliary losses or temperature schedules. On VQ-VAE compression and VQGAN generation across various data sets, they improve reconstruction and sample quality over alternative quantization approaches.</li>
<li><strong>摘要：</strong>矢量量化在深层模型中很常见，但是其艰苦的分配阻止了梯度和阻碍端到端训练。我们提出了DIVEQ，它将量化视为添加一个误差向量，该误差向量模仿量化失真，使前向通过，同时让梯度流动。我们还提出了一个空间填充变体（SF-DIVEQ），该变体分配给连接代码字的行构建的曲线，从而导致量化错误较少和使用完整代码簿的使用情况。两种方法都端到端训练，而无需辅助损失或温度时间表。在各种数据集中的VQ-VAE压缩和VQGAN生成上，它们改善了替代量化方法的重建和样品质量。</li>
</ul>

<h3>Title: Contrastive Diffusion Guidance for Spatial Inverse Problems</h3>
<ul>
<li><strong>Authors: </strong>Sattwik Basu, Chaitanya Amballa, Zhongweiyang Xu, Jorge Vančo Sampedro, Srihari Nelakuditi, Romit Roy Choudhury</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26489">https://arxiv.org/abs/2509.26489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26489">https://arxiv.org/pdf/2509.26489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26489]] Contrastive Diffusion Guidance for Spatial Inverse Problems(https://arxiv.org/abs/2509.26489)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We consider the inverse problem of reconstructing the spatial layout of a place, a home floorplan for example, from a user`s movements inside that layout. Direct inversion is ill-posed since many floorplans can explain the same movement trajectories. We adopt a diffusion-based posterior sampler to generate layouts consistent with the measurements. While active research is in progress on generative inverse solvers, we find that the forward operator in our problem poses new challenges. The path-planning process inside a floorplan is a non-invertible, non-differentiable function, and causes instability while optimizing using the likelihood score. We break-away from existing approaches and reformulate the likelihood score in a smoother embedding space. The embedding space is trained with a contrastive loss which brings compatible floorplans and trajectories close to each other, while pushing mismatched pairs far apart. We show that a surrogate form of the likelihood score in this embedding space is a valid approximation of the true likelihood score, making it possible to steer the denoising process towards the posterior. Across extensive experiments, our model CoGuide produces more consistent floorplans from trajectories, and is more robust than differentiable-planner baselines and guided-diffusion methods.</li>
<li><strong>摘要：</strong>我们考虑重建位置的空间布局的逆问题，例如，从该布局内部的用户动作中，家庭平面图。直接反转是错误的，因为许多平面图可以解释相同的运动轨迹。我们采用基于扩散的后验采样器来生成与测量结果一致的布局。尽管积极的研究正在进行生成反向求解器，但我们发现我们问题的远期操作员带来了新的挑战。平面图内的路径规划过程是不可逆转的，不可差异的功能，在使用似然得分进行优化时会导致不稳定。我们从现有的方法中脱颖而出，并在更光滑的嵌入空间中重新制定了可能性得分。嵌入空间经过对比损失的训练，可以使兼容的平面图和轨迹彼此接近，同时将错配对的成对截然不同。我们表明，在这个嵌入空间中，可能性得分的替代形式是真实似然得分的有效近似，使得可以将剥离过程转向后部。在广泛的实验中，我们的模型Coguide从轨迹产生了更一致的平面图，并且比可区分的型号基准和引导扩散方法更健壮。</li>
</ul>

<h3>Title: Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Agneet Chatterjee, Rahim Entezari, Maksym Zhuravinskyi, Maksim Lapin, Reshinth Adithyan, Amit Raj, Chitta Baral, Yezhou Yang, Varun Jampani</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26555">https://arxiv.org/abs/2509.26555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26555">https://arxiv.org/pdf/2509.26555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26555]] Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation(https://arxiv.org/abs/2509.26555)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in video generation have enabled high-fidelity video synthesis from user provided prompts. However, existing models and benchmarks fail to capture the complexity and requirements of professional video generation. Towards that goal, we introduce Stable Cinemetrics, a structured evaluation framework that formalizes filmmaking controls into four disentangled, hierarchical taxonomies: Setup, Event, Lighting, and Camera. Together, these taxonomies define 76 fine-grained control nodes grounded in industry practices. Using these taxonomies, we construct a benchmark of prompts aligned with professional use cases and develop an automated pipeline for prompt categorization and question generation, enabling independent evaluation of each control dimension. We conduct a large-scale human study spanning 10+ models and 20K videos, annotated by a pool of 80+ film professionals. Our analysis, both coarse and fine-grained reveal that even the strongest current models exhibit significant gaps, particularly in Events and Camera-related controls. To enable scalable evaluation, we train an automatic evaluator, a vision-language model aligned with expert annotations that outperforms existing zero-shot baselines. SCINE is the first approach to situate professional video generation within the landscape of video generative models, introducing taxonomies centered around cinematic controls and supporting them with structured evaluation pipelines and detailed analyses to guide future research.</li>
<li><strong>摘要：</strong>视频生成的最新进展使用户提供了提示的高保真视频综合。但是，现有的模型和基准无法捕获专业视频生成的复杂性和要求。为了实现这一目标，我们介绍了稳定的Cinemetrics，这是一个结构化的评估框架，将电影制作控件正式化为四个分散的，分层分类法：设置，事件，照明和相机。这些分类法共同定义了以行业实践为基础的76个细粒控制节点。使用这些分类法，我们构建了与专业用例保持一致的提示的基准，并开发自动化管道以及时分类和问题产生，从而可以独立评估每个控制维度。我们进行了一项大规模的人类研究，涵盖了10多个模型和20K视频，并由80多个电影专业人士注释。我们的分析是粗粒和细粒度的，即使当前最强的电流模型也会显示出明显的差距，尤其是在事件和摄像机相关的控制中。为了启用可扩展评估，我们训练一个自动评估器，这是一种与专家注释相一致的视觉模型，该模型优于现有的零击基线。 SCINE是在视频生成模型的景观中置于专业视频生成的第一种方法，引入了围绕电影控制的分类法，并通过结构化的评估管道和详细的分析来指导未来的研究。</li>
</ul>

<h3>Title: DiffCamera: Arbitrary Refocusing on Images</h3>
<ul>
<li><strong>Authors: </strong>Yiyang Wang, Xi Chen, Xiaogang Xu, Yu Liu, Hengshuang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26599">https://arxiv.org/abs/2509.26599</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26599">https://arxiv.org/pdf/2509.26599</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26599]] DiffCamera: Arbitrary Refocusing on Images(https://arxiv.org/abs/2509.26599)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The depth-of-field (DoF) effect, which introduces aesthetically pleasing blur, enhances photographic quality but is fixed and difficult to modify once the image has been created. This becomes problematic when the applied blur is undesirable~(e.g., the subject is out of focus). To address this, we propose DiffCamera, a model that enables flexible refocusing of a created image conditioned on an arbitrary new focus point and a blur level. Specifically, we design a diffusion transformer framework for refocusing learning. However, the training requires pairs of data with different focus planes and bokeh levels in the same scene, which are hard to acquire. To overcome this limitation, we develop a simulation-based pipeline to generate large-scale image pairs with varying focus planes and bokeh levels. With the simulated data, we find that training with only a vanilla diffusion objective often leads to incorrect DoF behaviors due to the complexity of the task. This requires a stronger constraint during training. Inspired by the photographic principle that photos of different focus planes can be linearly blended into a multi-focus image, we propose a stacking constraint during training to enforce precise DoF manipulation. This constraint enhances model training by imposing physically grounded refocusing behavior that the focusing results should be faithfully aligned with the scene structure and the camera conditions so that they can be combined into the correct multi-focus image. We also construct a benchmark to evaluate the effectiveness of our refocusing model. Extensive experiments demonstrate that DiffCamera supports stable refocusing across a wide range of scenes, providing unprecedented control over DoF adjustments for photography and generative AI applications.</li>
<li><strong>摘要：</strong>引入美学上令人愉悦的模糊的视野（DOF）效应增强了摄影质量，但一旦创建图像就固定且难以修改。当应用模糊不良〜时，这将变得有问题（例如，受试者不在焦点中）。为了解决这个问题，我们提出了Diffcamera，该模型可以灵活地重新关注以任意新的焦点点和模糊级别为条件的创建图像。具体而言，我们设计了一个扩散变压器框架，用于重新集中学习。但是，培训需要在同一场景中具有不同焦点平面和散景水平的一对数据，这很难获得。为了克服这一限制，我们开发了基于模拟的管道，以生成具有不同焦点平面和散景级别的大规模图像对。通过模拟数据，我们发现仅使用香草扩散目标的训练通常会导致由于任务的复杂性而导致DOF行为不正确。这需要在培训期间更加限制。受摄影原则的启发，即不同的焦点平面的照片可以线性融合到多聚焦图像中，我们在训练过程中提出了堆叠约束，以实施精确的DOF操纵。该约束通过强加身体接地的重新聚焦行为来增强模型训练，以使聚焦结果应忠实地与场景结构和相机条件保持一致，以便可以将它们组合到正确的多聚焦图像中。我们还构建了一个基准来评估我们重新关注模型的有效性。广泛的实验表明，Diffcamera支持在各种场景中稳定重新聚焦，从而为摄影和生成AI应用提供了对DOF调整的前所未有的控制。</li>
</ul>

<h3>Title: Video Object Segmentation-Aware Audio Generation</h3>
<ul>
<li><strong>Authors: </strong>Ilpo Viertola, Vladimir Iashin, Esa Rahtu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26604">https://arxiv.org/abs/2509.26604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26604">https://arxiv.org/pdf/2509.26604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26604]] Video Object Segmentation-Aware Audio Generation(https://arxiv.org/abs/2509.26604)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Existing multimodal audio generation models often lack precise user control, which limits their applicability in professional Foley workflows. In particular, these models focus on the entire video and do not provide precise methods for prioritizing a specific object within a scene, generating unnecessary background sounds, or focusing on the wrong objects. To address this gap, we introduce the novel task of video object segmentation-aware audio generation, which explicitly conditions sound synthesis on object-level segmentation maps. We present SAGANet, a new multimodal generative model that enables controllable audio generation by leveraging visual segmentation masks along with video and textual cues. Our model provides users with fine-grained and visually localized control over audio generation. To support this task and further research on segmentation-aware Foley, we propose Segmented Music Solos, a benchmark dataset of musical instrument performance videos with segmentation information. Our method demonstrates substantial improvements over current state-of-the-art methods and sets a new standard for controllable, high-fidelity Foley synthesis. Code, samples, and Segmented Music Solos are available at this https URL</li>
<li><strong>摘要：</strong>现有的多模式音频生成模型通常缺乏精确的用户控制，这限制了其在专业Foley工作流中的适用性。特别是，这些模型集中在整个视频上，并且没有提供精确的方法来确定场景中的特定对象，生成不必要的背景声音或专注于错误的对象。为了解决这一差距，我们介绍了视频对象分割 - 感知音频生成的新任务，该任务明确地在对象级分割图上明确条件综合。我们提出了Saganet，这是一种新的多模式生成模型，它通过利用视觉分割掩码以及视频和文本提示来实现可控的音频生成。我们的模型为用户提供了对音频生成的细粒度和视觉上局部的控制。为了支持这项任务并进一步研究分割感知的Foley，我们提出了分割音乐独奏，这是一个带有分段信息的乐器性能视频的基准数据集。我们的方法证明了对当前最新方法的实质性改进，并为可控制的高保真foley合成设置了新标准。代码，样本和分段音乐独奏可在此HTTPS URL上找到</li>
</ul>

<h3>Title: Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training</h3>
<ul>
<li><strong>Authors: </strong>Junlin Han, Shengbang Tong, David Fan, Yufan Ren, Koustuv Sinha, Philip Torr, Filippos Kokkinos</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26625">https://arxiv.org/abs/2509.26625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26625">https://arxiv.org/pdf/2509.26625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26625]] Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training(https://arxiv.org/abs/2509.26625)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), despite being trained on text alone, surprisingly develop rich visual priors. These priors allow latent visual capabilities to be unlocked for vision tasks with a relatively small amount of multimodal data, and in some cases, to perform visual tasks without ever having seen an image. Through systematic analysis, we reveal that visual priors-the implicit, emergent knowledge about the visual world acquired during language pre-training-are composed of separable perception and reasoning priors with unique scaling trends and origins. We show that an LLM's latent visual reasoning ability is predominantly developed by pre-training on reasoning-centric data (e.g., code, math, academia) and scales progressively. This reasoning prior acquired from language pre-training is transferable and universally applicable to visual reasoning. In contrast, a perception prior emerges more diffusely from broad corpora, and perception ability is more sensitive to the vision encoder and visual instruction tuning data. In parallel, text describing the visual world proves crucial, though its performance impact saturates rapidly. Leveraging these insights, we propose a data-centric recipe for pre-training vision-aware LLMs and verify it in 1T token scale pre-training. Our findings are grounded in over 100 controlled experiments consuming 500,000 GPU-hours, spanning the full MLLM construction pipeline-from LLM pre-training to visual alignment and supervised multimodal fine-tuning-across five model scales, a wide range of data categories and mixtures, and multiple adaptation setups. Along with our main findings, we propose and investigate several hypotheses, and introduce the Multi-Level Existence Bench (MLE-Bench). Together, this work provides a new way of deliberately cultivating visual priors from language pre-training, paving the way for the next generation of multimodal LLMs.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）尽管单独接受了文本培训，但出人意料地发展了丰富的视觉先验。这些先验允许使用相对较少数量的多模式数据的视觉任务解锁潜在的视觉功能，在某些情况下可以执行视觉任务，而没有看过图像。通过系统的分析，我们揭示了视觉先验 - 关于在语言预训练中获得的视觉世界的隐性，新兴的知识 - 由可分离的感知和具有独特缩放趋势和起源的推理先验组成。我们表明，LLM的潜在视觉推理能力主要是通过对以推理为中心的数据（例如代码，数学，学术界）进行预训练的，并且逐渐逐步缩放。从语言预训练中获得的这种推理可转移，并且普遍适用于视觉推理。相比之下，先验的感知与广泛的语料库更具传播，而感知能力对视觉编码器和视觉指导调整数据更敏感。同时，描述视觉世界的文本证明了至关重要的，尽管其性能迅速饱和。利用这些见解，我们提出了一个以数据为中心的培训培训式视觉感知LLM的配方，并在1T代币量表预训练中验证它。我们的发现基于100多个受控实验，这些实验消耗了500,000个GPU小时，跨越了整个MLLM施工管道，从LLM预先培训进行视觉对齐和有监督的多模式微调五个型号量表，广泛的数据类别和混合物以及多种适应设置。除了我们的主要发现外，我们提出并研究了几种假设，并介绍了多级存在基准（MLE基础）。这项工作共同提供了一种新的方式，可以故意从语言预训练中培养视觉先验，为下一代多模式LLM铺平道路。</li>
</ul>

<h3>Title: SPATA: Systematic Pattern Analysis for Detailed and Transparent Data Cards</h3>
<ul>
<li><strong>Authors: </strong>João Vitorino, Eva Maia, Isabel Praça, Carlos Soares</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26640">https://arxiv.org/abs/2509.26640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26640">https://arxiv.org/pdf/2509.26640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26640]] SPATA: Systematic Pattern Analysis for Detailed and Transparent Data Cards(https://arxiv.org/abs/2509.26640)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Due to the susceptibility of Artificial Intelligence (AI) to data perturbations and adversarial examples, it is crucial to perform a thorough robustness evaluation before any Machine Learning (ML) model is deployed. However, examining a model's decision boundaries and identifying potential vulnerabilities typically requires access to the training and testing datasets, which may pose risks to data privacy and confidentiality. To improve transparency in organizations that handle confidential data or manage critical infrastructure, it is essential to allow external verification and validation of AI without the disclosure of private datasets. This paper presents Systematic Pattern Analysis (SPATA), a deterministic method that converts any tabular dataset to a domain-independent representation of its statistical patterns, to provide more detailed and transparent data cards. SPATA computes the projection of each data instance into a discrete space where they can be analyzed and compared, without risking data leakage. These projected datasets can be reliably used for the evaluation of how different features affect ML model robustness and for the generation of interpretable explanations of their behavior, contributing to more trustworthy AI.</li>
<li><strong>摘要：</strong>由于人工智能（AI）对数据扰动和对抗性示例的敏感性，在部署任何机器学习（ML）模型之前进行彻底的鲁棒性评估至关重要。但是，检查模型的决策边界并确定潜在的漏洞通常需要访问培训和测试数据集，这可能会对数据隐私和机密性构成风险。为了提高处理机密数据或管理关键基础架构的组织的透明度，必须在不披露私人数据集的情况下允许外部验证和验证AI。本文介绍了系统模式分析（SPATA），这是一种确定性方法，可将任何表格数据集转换为与域无关的统计模式表示，以提供更详细且透明的数据卡。 SPATA将每个数据实例的投影计算为一个离散的空间，可以在其中分析和比较它们，而不会冒险泄漏。这些投影的数据集可以可靠地用于评估不同特征如何影响ML模型的鲁棒性以及对其行为的可解释解释，从而有助于更值得信赖的AI。</li>
</ul>

<h3>Title: Query-Kontext: An Unified Multimodal Model for Image Generation and Editing</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Song, Wenkai Dong, Shizun Wang, Qi Zhang, Song Xue, Tao Yuan, Hu Yang, Haocheng Feng, Hang Zhou, Xinyan Xiao, Jingdong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26641">https://arxiv.org/abs/2509.26641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26641">https://arxiv.org/pdf/2509.26641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26641]] Query-Kontext: An Unified Multimodal Model for Image Generation and Editing(https://arxiv.org/abs/2509.26641)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Unified Multimodal Models (UMMs) have demonstrated remarkable performance in text-to-image generation (T2I) and editing (TI2I), whether instantiated as assembled unified frameworks which couple powerful vision-language model (VLM) with diffusion-based generator, or as naive Unified Multimodal Models with an early fusion of understanding and generation modalities. We contend that in current unified frameworks, the crucial capability of multimodal generative reasoning which encompasses instruction understanding, grounding, and image referring for identity preservation and faithful reconstruction, is intrinsically entangled with high-fidelity synthesis. In this work, we introduce Query-Kontext, a novel approach that bridges the VLM and diffusion model via a multimodal ``kontext'' composed of semantic cues and coarse-grained image conditions encoded from multimodal inputs. This design delegates the complex ability of multimodal generative reasoning to powerful VLM while reserving diffusion model's role for high-quality visual synthesis. To achieve this, we propose a three-stage progressive training strategy. First, we connect the VLM to a lightweight diffusion head via multimodal kontext tokens to unleash the VLM's generative reasoning ability. Second, we scale this head to a large, pre-trained diffusion model to enhance visual detail and realism. Finally, we introduce a low-level image encoder to improve image fidelity and perform instruction tuning on downstream tasks. Furthermore, we build a comprehensive data pipeline integrating real, synthetic, and open-source datasets, covering diverse multimodal reference-to-image scenarios, including image generation, instruction-driven editing, customized generation, and multi-subject composition. Experiments show that our approach matches strong unified baselines and even outperforms task-specific state-of-the-art methods in several cases.</li>
<li><strong>摘要：</strong>统一的多模型模型（UMMS）在文本到图像生成（T2I）和编辑（TI2I）中表现出色，无论是将其与基于扩散基于扩散的生成器的强大视觉模型（VLM）相结合的统一统一框架，还是将基于扩散的生成器或天真的统一多态模型与对理解和生成模式的早期融合。我们认为，在当前的统一框架中，多模式生成推理的关键能力包括指导理解，接地和图像参考身份保存和忠实的重建，这本质上与高保真合成。在这项工作中，我们介绍了Query-Kontext，这是一种新颖的方法，该方法通过多模式``kontext''桥接VLM和扩散模型，该模型由语义提示和由多模态输入编码的粗粒图像条件组成。该设计将多模式生成推理的复杂能力委托给功能强大的VLM，同时保留扩散模型在高质量的视觉合成中的作用。为了实现这一目标，我们提出了三阶段的渐进培训策略。首先，我们将VLM通过多模式Kontext代币连接到轻质扩散头，以释放VLM的生成推理能力。其次，我们将此头扩展到一个大型的预训练扩散模型，以增强视觉细节和现实主义。最后，我们介绍了一个低级图像编码器，以改善图像保真度并在下游任务上进行指令调整。此外，我们构建了一条全面的数据管道，集成了真实，合成和开源数据集，涵盖了多样化的多模式参考对图像的方案，包括图像生成，讲义驱动的编辑，自定义生成和多对象组合。实验表明，在某些情况下，我们的方法与强大的统一基线相匹配，甚至均优于特定于任务的最先进方法。</li>
</ul>

<h3>Title: Stitch: Training-Free Position Control in Multimodal Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Jessica Bader, Mateusz Pach, Maria A. Bravo, Serge Belongie, Zeynep Akata</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.26644">https://arxiv.org/abs/2509.26644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.26644">https://arxiv.org/pdf/2509.26644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.26644]] Stitch: Training-Free Position Control in Multimodal Diffusion Transformers(https://arxiv.org/abs/2509.26644)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-Image (T2I) generation models have advanced rapidly in recent years, but accurately capturing spatial relationships like "above" or "to the right of" poses a persistent challenge. Earlier methods improved spatial relationship following with external position control. However, as architectures evolved to enhance image quality, these techniques became incompatible with modern models. We propose Stitch, a training-free method for incorporating external position control into Multi-Modal Diffusion Transformers (MMDiT) via automatically-generated bounding boxes. Stitch produces images that are both spatially accurate and visually appealing by generating individual objects within designated bounding boxes and seamlessly stitching them together. We find that targeted attention heads capture the information necessary to isolate and cut out individual objects mid-generation, without needing to fully complete the image. We evaluate Stitch on PosEval, our benchmark for position-based T2I generation. Featuring five new tasks that extend the concept of Position beyond the basic GenEval task, PosEval demonstrates that even top models still have significant room for improvement in position-based generation. Tested on Qwen-Image, FLUX, and SD3.5, Stitch consistently enhances base models, even improving FLUX by 218% on GenEval's Position task and by 206% on PosEval. Stitch achieves state-of-the-art results with Qwen-Image on PosEval, improving over previous models by 54%, all accomplished while integrating position control into leading models training-free. Code is available at this https URL.</li>
<li><strong>摘要：</strong>近年来，文本对图像（T2I）的生成模型已迅速发展，但准确地捕获了像“上方”或“在“权利”中的“构成”的空间关系“构成了持续的挑战。早期方法改善了与外部位置控制之后的空间关系。但是，随着体系结构的发展以提高图像质量，这些技术与现代模型不相容。我们提出了针迹，这是一种通过自动生成的边界框，将外部位置控制纳入多模式扩散变压器（MMDIT）的无训练方法。针迹通过在指定的边界框中产生各个对象并将它们无缝缝合在一起，从而产生了空间准确和视觉上吸引人的图像。我们发现，有针对性的注意力头捕获了在生成中期隔离和切除各个对象所需的信息，而无需完全完成图像。我们评估了基于位置T2I生成的基准Poseval上的针迹。 Poseval具有五个新任务，这些任务将位置概念扩展到基本的遗传任务之外，这表明，即使是顶级模型仍然有很大的改善基于位置的生成的空间。在QWEN图像，通量和SD3.5上进行了测试，缝合一致地增强了基本模型，甚至在Geneval的位置任务上甚至可以提高218％的通量，而Poseval的速度也提高了206％。 Stitch在Poseval上以QWEN形象的形式实现了最先进的结果，使以前的型号提高了54％，所有这些都完成了，同时将位置控制纳入无领先模型的无训练。代码可在此HTTPS URL上找到。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
