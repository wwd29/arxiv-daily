<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-11-21</h1>
<h3>Title: Connecting the Dots: A Machine Learning Ready Dataset for Ionospheric Forecasting Models</h3>
<ul>
<li><strong>Authors: </strong>Linnea M. Wolniewicz, Halil S. Kelebek, Simone Mestici, Michael D. Vergalla, Giacomo Acciarini, Bala Poduval, Olga Verkhoglyadova, Madhulika Guhathakurta, Thomas E. Berger, Atılım Güneş Baydin, Frank Soboczenski</a></li>
<li><strong>Subjects: </strong>cs.LG, astro-ph.EP, astro-ph.IM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15743">https://arxiv.org/abs/2511.15743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15743">https://arxiv.org/pdf/2511.15743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15743]] Connecting the Dots: A Machine Learning Ready Dataset for Ionospheric Forecasting Models(https://arxiv.org/abs/2511.15743)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Operational forecasting of the ionosphere remains a critical space weather challenge due to sparse observations, complex coupling across geospatial layers, and a growing need for timely, accurate predictions that support Global Navigation Satellite System (GNSS), communications, aviation safety, as well as satellite operations. As part of the 2025 NASA Heliolab, we present a curated, open-access dataset that integrates diverse ionospheric and heliospheric measurements into a coherent, machine learning-ready structure, designed specifically to support next-generation forecasting models and address gaps in current operational frameworks. Our workflow integrates a large selection of data sources comprising Solar Dynamic Observatory data, solar irradiance indices (F10.7), solar wind parameters (velocity and interplanetary magnetic field), geomagnetic activity indices (Kp, AE, SYM-H), and NASA JPL's Global Ionospheric Maps of Total Electron Content (GIM-TEC). We also implement geospatially sparse data such as the TEC derived from the World-Wide GNSS Receiver Network and crowdsourced Android smartphone measurements. This novel heterogeneous dataset is temporally and spatially aligned into a single, modular data structure that supports both physical and data-driven modeling. Leveraging this dataset, we train and benchmark several spatiotemporal machine learning architectures for forecasting vertical TEC under both quiet and geomagnetically active conditions. This work presents an extensive dataset and modeling pipeline that enables exploration of not only ionospheric dynamics but also broader Sun-Earth interactions, supporting both scientific inquiry and operational forecasting efforts.</li>
<li><strong>摘要：</strong>由于观测稀疏、跨地理空间层的复杂耦合以及对支持全球导航卫星系统 (GNSS)、通信、航空安全以及卫星运行的及时、准确预测的需求日益增长，电离层的业务预报仍然是一个关键的空间天气挑战。作为 2025 年 NASA Heliolab 的一部分，我们提出了一个精心策划的开放访问数据集，它将不同的电离层和日光层测量集成到一个连贯的、机器学习就绪的结构中，专门设计用于支持下一代预测模型并解决当前操作框架中的差距。我们的工作流程集成了大量的数据源，包括太阳动态观测站数据、太阳辐照度指数 (F10.7)、太阳风参数（速度和行星际磁场）、地磁活动指数（Kp、AE、SYM-H）和 NASA JPL 的全球电离层总电子含量图 (GIM-TEC)。我们还实施地理空间稀疏数据，例如源自全球 GNSS 接收器网络和众包 Android 智能手机测量的 TEC。这种新颖的异构数据集在时间和空间上对齐为单个模块化数据结构，支持物理和数据驱动建模。利用该数据集，我们训练和基准测试了几种时空机器学习架构，用于预测安静和地磁活跃条件下的垂直 TEC。这项工作提供了广泛的数据集和建模流程，不仅可以探索电离层动力学，还可以探索更广泛的日地相互作用，支持科学探究和业务预测工作。</li>
</ul>

<h3>Title: TB or Not TB: Coverage-Driven Direct Preference Optimization for Verilog Stimulus Generation</h3>
<ul>
<li><strong>Authors: </strong>Bardia Nadimi, Khashayar Filom, Deming Chen, Hao Zheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15767">https://arxiv.org/abs/2511.15767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15767">https://arxiv.org/pdf/2511.15767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15767]] TB or Not TB: Coverage-Driven Direct Preference Optimization for Verilog Stimulus Generation(https://arxiv.org/abs/2511.15767)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of Large Language Models (LLMs), there is growing interest in applying them to hardware design and verification. Among these stages, design verification remains the most time-consuming and resource-intensive phase, where generating effective stimuli for the design under test (DUT) is both critical and labor-intensive. We present {\it TB or not TB}, a framework for automated stimulus generation using LLMs fine-tuned through Coverage-Driven Direct Preference Optimization (CD-DPO). To enable preference-based training, we introduce PairaNet, a dataset derived from PyraNet that pairs high- and low-quality testbenches labeled using simulation-derived coverage metrics. The proposed CD-DPO method integrates quantitative coverage feedback directly into the optimization objective, guiding the model toward generating stimuli that maximize verification coverage. Experiments on the CVDP CID12 benchmark show that {\it TB or not TB} outperforms both open-source and commercial baselines, achieving up to 77.27\% improvement in code coverage, demonstrating the effectiveness of Coverage-driven preference optimization for LLM-based hardware verification.</li>
<li><strong>摘要：</strong>随着大型语言模型（LLM）的快速发展，人们越来越有兴趣将其应用于硬件设计和验证。在这些阶段中，设计验证仍然是最耗时和资源密集的阶段，其中为被测设计（DUT）生成有效的激励既关键又耗费人力。我们提出了 {\it TB or not TB}，这是一个使用通过覆盖驱动直接偏好优化 (CD-DPO) 进行微调的 LLM 进行自动刺激生成的框架。为了实现基于偏好的训练，我们引入了 PairaNet，这是一个源自 PyraNet 的数据集，它对使用模拟衍生的覆盖率指标标记的高质量和低质量测试平台进行配对。所提出的 CD-DPO 方法将定量覆盖率反馈直接集成到优化目标中，指导模型生成最大化验证覆盖率的刺激。 CVDP CID12 基准测试表明，{\it TB or not TB} 的性能优于开源和商业基准，代码覆盖率提高了 77.27\%，证明了覆盖率驱动的偏好优化对于基于 LLM 的硬件验证的有效性。</li>
</ul>

<h3>Title: UniFit: Towards Universal Virtual Try-on with MLLM-Guided Semantic Alignment</h3>
<ul>
<li><strong>Authors: </strong>Wei Zhang, Yeying Jin, Xin Li, Yan Zhang, Xiaofeng Cong, Cong Wang, Fengcai Qiao, zhichao Lian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15831">https://arxiv.org/abs/2511.15831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15831">https://arxiv.org/pdf/2511.15831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15831]] UniFit: Towards Universal Virtual Try-on with MLLM-Guided Semantic Alignment(https://arxiv.org/abs/2511.15831)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Image-based virtual try-on (VTON) aims to synthesize photorealistic images of a person wearing specified garments. Despite significant progress, building a universal VTON framework that can flexibly handle diverse and complex tasks remains a major challenge. Recent methods explore multi-task VTON frameworks guided by textual instructions, yet they still face two key limitations: (1) semantic gap between text instructions and reference images, and (2) data scarcity in complex scenarios. To address these challenges, we propose UniFit, a universal VTON framework driven by a Multimodal Large Language Model (MLLM). Specifically, we introduce an MLLM-Guided Semantic Alignment Module (MGSA), which integrates multimodal inputs using an MLLM and a set of learnable queries. By imposing a semantic alignment loss, MGSA captures cross-modal semantic relationships and provides coherent and explicit semantic guidance for the generative process, thereby reducing the semantic gap. Moreover, by devising a two-stage progressive training strategy with a self-synthesis pipeline, UniFit is able to learn complex tasks from limited data. Extensive experiments show that UniFit not only supports a wide range of VTON tasks, including multi-garment and model-to-model try-on, but also achieves state-of-the-art performance. The source code and pretrained models are available at this https URL.</li>
<li><strong>摘要：</strong>基于图像的虚拟试穿（VTON）旨在合成穿着指定服装的人的逼真图像。尽管取得了重大进展，但构建一个能够灵活处理多样化和复杂任务的通用 VTON 框架仍然是一项重大挑战。最近的方法探索了由文本指令引导的多任务 VTON 框架，但它们仍然面临两个关键限制：（1）文本指令和参考图像之间的语义差距，以及（2）复杂场景中的数据稀缺。为了应对这些挑战，我们提出了 UniFit，这是一个由多模态大语言模型 (MLLM) 驱动的通用 VTON 框架。具体来说，我们引入了 MLLM 引导语义对齐模块 (MGSA)，它使用 MLLM 和一组可学习查询集成多模态输入。通过施加语义对齐损失，MGSA 捕获跨模态语义关系，并为生成过程提供连贯且明确的语义指导，从而减少语义差距。此外，通过设计具有自合成管道的两阶段渐进训练策略，UniFit 能够从有限的数据中学习复杂的任务。大量实验表明，UniFit 不仅支持广泛的 VTON 任务，包括多种服装和模特试穿，而且还实现了最先进的性能。源代码和预训练模型可从此 https URL 获取。</li>
</ul>

<h3>Title: Automatic Uncertainty-Aware Synthetic Data Bootstrapping for Historical Map Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Lukas Arzoumanidis, Julius Knechtel, Jan-Henrik Haunert, Youness Dehbi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15875">https://arxiv.org/abs/2511.15875</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15875">https://arxiv.org/pdf/2511.15875</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15875]] Automatic Uncertainty-Aware Synthetic Data Bootstrapping for Historical Map Segmentation(https://arxiv.org/abs/2511.15875)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The automated analysis of historical documents, particularly maps, has drastically benefited from advances in deep learning and its success across various computer vision applications. However, most deep learning-based methods heavily rely on large amounts of annotated training data, which are typically unavailable for historical maps, especially for those belonging to specific, homogeneous cartographic domains, also known as corpora. Creating high-quality training data suitable for machine learning often takes a significant amount of time and involves extensive manual effort. While synthetic training data can alleviate the scarcity of real-world samples, it often lacks the affinity (realism) and diversity (variation) necessary for effective learning. By transferring the cartographic style of an original historical map corpus onto vector data, we bootstrap an effectively unlimited number of synthetic historical maps suitable for tasks such as land-cover interpretation of a homogeneous historical map corpus. We propose an automatic deep generative approach and a alternative manual stochastic degradation technique to emulate the visual uncertainty and noise, also known as data-dependent uncertainty, commonly observed in historical map scans. To quantitatively evaluate the effectiveness and applicability of our approach, the generated training datasets were employed for domain-adaptive semantic segmentation on a homogeneous map corpus using a Self-Constructing Graph Convolutional Network, enabling a comprehensive assessment of the impact of our data bootstrapping methods.</li>
<li><strong>摘要：</strong>历史文档（尤其是地图）的自动分析极大地受益于深度学习的进步及其在各种计算机视觉应用中的成功。然而，大多数基于深度学习的方法严重依赖于大量带注释的训练数据，这些数据通常不适用于历史地图，尤其是那些属于特定的同质制图领域（也称为语料库）的地图。创建适合机器学习的高质量训练数据通常需要大量时间并涉及大量手动工作。虽然合成训练数据可以缓解现实世界样本的稀缺性，但它通常缺乏有效学习所需的亲和力（真实性）和多样性（变化）。通过将原始历史地图语料库的制图风格转移到矢量数据上，我们有效地引导了无限数量的合成历史地图，适用于诸如同质历史地图语料库的土地覆盖解释等任务。我们提出了一种自动深度生成方法和另一种手动随机退化技术来模拟视觉不确定性和噪声，也称为数据依赖的不确定性，通常在历史地图扫描中观察到。为了定量评估我们方法的有效性和适用性，生成的训练数据集被用于使用自构建图卷积网络在同质地图语料库上进行领域自适应语义分割，从而能够全面评估我们的数据引导方法的影响。</li>
</ul>

<h3>Title: Unified all-atom molecule generation with neural fields</h3>
<ul>
<li><strong>Authors: </strong>Matthieu Kirchmeyer, Pedro O. Pinheiro, Emma Willett, Karolis Martinkus, Joseph Kleinhenz, Emily K. Makowski, Andrew M. Watkins, Vladimir Gligorijevic, Richard Bonneau, Saeed Saremi</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15906">https://arxiv.org/abs/2511.15906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15906">https://arxiv.org/pdf/2511.15906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15906]] Unified all-atom molecule generation with neural fields(https://arxiv.org/abs/2511.15906)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative models for structure-based drug design are often limited to a specific modality, restricting their broader applicability. To address this challenge, we introduce FuncBind, a framework based on computer vision to generate target-conditioned, all-atom molecules across atomic systems. FuncBind uses neural fields to represent molecules as continuous atomic densities and employs score-based generative models with modern architectures adapted from the computer vision literature. This modality-agnostic representation allows a single unified model to be trained on diverse atomic systems, from small to large molecules, and handle variable atom/residue counts, including non-canonical amino acids. FuncBind achieves competitive in silico performance in generating small molecules, macrocyclic peptides, and antibody complementarity-determining region loops, conditioned on target structures. FuncBind also generated in vitro novel antibody binders via de novo redesign of the complementarity-determining region H3 loop of two chosen co-crystal structures. As a final contribution, we introduce a new dataset and benchmark for structure-conditioned macrocyclic peptide generation. The code is available at this https URL.</li>
<li><strong>摘要：</strong>基于结构的药物设计的生成模型通常仅限于特定的模式，限制了其更广泛的适用性。为了应对这一挑战，我们引入了 FuncBind，这是一个基于计算机视觉的框架，用于跨原子系统生成目标条件的全原子分子。 FuncBind 使用神经场将分子表示为连续原子密度，并采用基于分数的生成模型以及改编自计算机视觉文献的现代架构。这种与模态无关的表示允许在从小分子到大分子的不同原子系统上训练单个统一模型，并处理可变原子/残基计数，包括非规范氨基酸。 FuncBind 在生成小分子、大环肽和抗体互补决定区域环方面实现了具有竞争力的计算机性能，以目标结构为条件。 FuncBind 还通过从头重新设计两个选定共晶结构的互补决定区 H3 环，在体外生成新型抗体结合物。作为最后的贡献，我们引入了用于结构条件大环肽生成的新数据集和基准。该代码可从此 https URL 获取。</li>
</ul>

<h3>Title: AccelOpt: A Self-Improving LLM Agentic System for AI Accelerator Kernel Optimization</h3>
<ul>
<li><strong>Authors: </strong>Genghan Zhang, Shaowei Zhu, Anjiang Wei, Zhenyu Song, Allen Nie, Zhen Jia, Nandita Vijaykumar, Yida Wang, Kunle Olukotun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15915">https://arxiv.org/abs/2511.15915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15915">https://arxiv.org/pdf/2511.15915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15915]] AccelOpt: A Self-Improving LLM Agentic System for AI Accelerator Kernel Optimization(https://arxiv.org/abs/2511.15915)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present AccelOpt, a self-improving large language model (LLM) agentic system that autonomously optimizes kernels for emerging AI acclerators, eliminating the need for expert-provided hardware-specific optimization knowledge. AccelOpt explores the kernel optimization space through iterative generation, informed by an optimization memory that curates experiences and insights from previously encountered slow-fast kernel pairs. We build NKIBench, a new benchmark suite of AWS Trainium accelerator kernels with varying complexity extracted from real-world LLM workloads to evaluate the effectiveness of AccelOpt. Our evaluation confirms that AccelOpt's capability improves over time, boosting the average percentage of peak throughput from $49\%$ to $61\%$ on Trainium 1 and from $45\%$ to $59\%$ on Trainium 2 for NKIBench kernels. Moreover, AccelOpt is highly cost-effective: using open-source models, it matches the kernel improvements of Claude Sonnet 4 while being $26\times$ cheaper.</li>
<li><strong>摘要：</strong>我们推出了 AccelOpt，这是一种自我改进的大型语言模型 (LLM) 代理系统，可以自动优化新兴 AI 加速器的内核，从而无需专家提供特定于硬件的优化知识。 AccelOpt 通过迭代生成探索内核优化空间，优化内存提供信息，该内存从以前遇到的慢速内核对中收集经验和见解。我们构建了 NKIBench，这是一个新的 AWS Trainium 加速器内核基准测试套件，具有从现实世界的 LLM 工作负载中提取的不同复杂性，以评估 AccelOpt 的有效性。我们的评估证实，AccelOpt 的功能随着时间的推移而不断提高，对于 NKIBench 内核，Trainium 1 上的平均峰值吞吐量百分比从 $49\%$ 提高到 $61\%$，Trainium 2 上的峰值吞吐量从 $45\%$ 提高到 $59\%$。此外，AccelOpt 具有很高的成本效益：使用开源模型，它与 Claude Sonnet 4 的内核改进相匹配，同时便宜 26 美元\倍$。</li>
</ul>

<h3>Title: Breaking the Bottleneck with DiffuApriel: High-Throughput Diffusion LMs with Mamba Backbone</h3>
<ul>
<li><strong>Authors: </strong>Vaibhav Singh, Oleksiy Ostapenko, Pierre-André Noël, Torsten Scholak</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15927">https://arxiv.org/abs/2511.15927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15927">https://arxiv.org/pdf/2511.15927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15927]] Breaking the Bottleneck with DiffuApriel: High-Throughput Diffusion LMs with Mamba Backbone(https://arxiv.org/abs/2511.15927)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion-based language models have recently emerged as a promising alternative to autoregressive generation, yet their reliance on Transformer backbones limits inference efficiency due to quadratic attention and KV-cache overhead. In this work, we introduce DiffuApriel, a masked diffusion language model built on a bidirectional Mamba backbone that combines the diffusion objective with linear-time sequence modeling. DiffuApriel matches the performance of Transformer-based diffusion models while achieving up to 4.4x higher inference throughput for long sequences with a 1.3B model. We further propose DiffuApriel-H, a hybrid variant that interleaves attention and mamba layers, offering up to 2.6x throughput improvement with balanced global and local context modeling. Our results demonstrate that bidirectional state-space architectures serve as strong denoisers in masked diffusion LMs, providing a practical and scalable foundation for faster, memory-efficient text generation.</li>
<li><strong>摘要：</strong>基于扩散的语言模型最近已成为自回归生成的一种有前景的替代方案，但由于二次注意力和 KV 缓存开销，它们对 Transformer 主干的依赖限制了推理效率。在这项工作中，我们介绍了 DiffuApriel，这是一种基于双向 Mamba 主干的屏蔽扩散语言模型，它将扩散目标与线性时间序列建模相结合。 DiffuApriel 与基于 Transformer 的扩散模型的性能相匹配，同时使用 1.3B 模型将长序列的推理吞吐量提高了 4.4 倍。我们进一步提出了 DiffuApriel-H，一种将注意力层和曼巴层交错的混合变体，通过平衡的全局和局部上下文建模提供高达 2.6 倍的吞吐量提升。我们的结果表明，双向状态空间架构可作为掩蔽扩散 LM 中的强大降噪器，为更快、节省内存的文本生成提供实用且可扩展的基础。</li>
</ul>

<h3>Title: Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click</h3>
<ul>
<li><strong>Authors: </strong>Raphael Ruschel, Hardikkumar Prajapati, Awsafur Rahman, B.S. Manjunath</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15948">https://arxiv.org/abs/2511.15948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15948">https://arxiv.org/pdf/2511.15948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15948]] Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click(https://arxiv.org/abs/2511.15948)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>State-of-the-art Video Scene Graph Generation (VSGG) systems provide structured visual understanding but operate as closed, feed-forward pipelines with no ability to incorporate human guidance. In contrast, promptable segmentation models such as SAM2 enable precise user interaction but lack semantic or relational reasoning. We introduce Click2Graph, the first interactive framework for Panoptic Video Scene Graph Generation (PVSG) that unifies visual prompting with spatial, temporal, and semantic understanding. From a single user cue, such as a click or bounding box, Click2Graph segments and tracks the subject across time, autonomously discovers interacting objects, and predicts <subject, object, predicate> triplets to form a temporally consistent scene graph. Our framework introduces two key components: a Dynamic Interaction Discovery Module that generates subject-conditioned object prompts, and a Semantic Classification Head that performs joint entity and predicate reasoning. Experiments on the OpenPVSG benchmark demonstrate that Click2Graph establishes a strong foundation for user-guided PVSG, showing how human prompting can be combined with panoptic grounding and relational inference to enable controllable and interpretable video scene understanding.</li>
<li><strong>摘要：</strong>最先进的视频场景图生成（VSGG）系统提供结构化视觉理解，但作为封闭的前馈管道运行，无法结合人类指导。相比之下，诸如 SAM2 之类的提示分割模型可以实现精确的用户交互，但缺乏语义或关系推理。我们介绍 Click2Graph，这是第一个全景视频场景图生成 (PVSG) 的交互式框架，它将视觉提示与空间、时间和语义理解结合起来。根据单个用户提示（例如单击或边界框），Click2Graph 可以跨时间分段和跟踪主题，自动发现交互对象，并预测 <主题、对象、谓词> 三元组以形成时间一致的场景图。我们的框架引入了两个关键组件：生成主语条件对象提示的动态交互发现模块，以及执行联合实体和谓词推理的语义分类头。 OpenPVSG 基准测试表明 Click2Graph 为用户引导的 PVSG 奠定了坚实的基础，展示了人类提示如何与全景基础和关系推理相结合，以实现可控和可解释的视频场景理解。</li>
</ul>

<h3>Title: UniDGF: A Unified Detection-to-Generation Framework for Hierarchical Object Visual Recognition</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Nan, Lingtao Mao, Huangyu Dai, Zexin Zheng, Xinyu Sun, Zihan Liang, Ben Chen, Yuqing Ding, Chenyi Lei, Wenwu Ou, Han Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.15984">https://arxiv.org/abs/2511.15984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.15984">https://arxiv.org/pdf/2511.15984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.15984]] UniDGF: A Unified Detection-to-Generation Framework for Hierarchical Object Visual Recognition(https://arxiv.org/abs/2511.15984)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Achieving visual semantic understanding requires a unified framework that simultaneously handles object detection, category prediction, and attribute recognition. However, current advanced approaches rely on global similarity and struggle to capture fine-grained category distinctions and category-specific attribute diversity, especially in large-scale e-commerce scenarios. To overcome these challenges, we introduce a detection-guided generative framework that predicts hierarchical category and attribute tokens. For each detected object, we extract refined ROI-level features and employ a BART-based generator to produce semantic tokens in a coarse-to-fine sequence covering category hierarchies and property-value pairs, with support for property-conditioned attribute recognition. Experiments on both large-scale proprietary e-commerce datasets and open-source datasets demonstrate that our approach significantly outperforms existing similarity-based pipelines and multi-stage classification systems, achieving stronger fine-grained recognition and more coherent unified inference.</li>
<li><strong>摘要：</strong>实现视觉语义理解需要一个统一的框架，同时处理对象检测、类别预测和属性识别。然而，当前的先进方法依赖于全局相似性，难以捕获细粒度的类别区别和类别特定的属性多样性，特别是在大规模电子商务场景中。为了克服这些挑战，我们引入了一个检测引导的生成框架，可以预测层次类别和属性标记。对于每个检测到的对象，我们提取精炼的 ROI 级特征，并使用基于 BART 的生成器以从粗到细的顺序生成涵盖类别层次结构和属性值对的语义标记，并支持属性条件属性识别。在大规模专有电子商务数据集和开源数据集上的实验表明，我们的方法显着优于现有的基于相似性的管道和多级分类系统，实现了更强的细粒度识别和更连贯的统一推理。</li>
</ul>

<h3>Title: CARE: Turning LLMs Into Causal Reasoning Expert</h3>
<ul>
<li><strong>Authors: </strong>Juncheng Dong, Yiling Liu, Ahmed Aloui, Vahid Tarokh, David Carlson</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16016">https://arxiv.org/abs/2511.16016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16016">https://arxiv.org/pdf/2511.16016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16016]] CARE: Turning LLMs Into Causal Reasoning Expert(https://arxiv.org/abs/2511.16016)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have recently demonstrated impressive capabilities across a range of reasoning and generation tasks. However, research studies have shown that LLMs lack the ability to identify causal relationships, a fundamental cornerstone of human intelligence. We first conduct an exploratory investigation of LLMs' behavior when asked to perform a causal-discovery task and find that they mostly rely on the semantic meaning of variable names, ignoring the observation data. This is unsurprising, given that LLMs were never trained to process structural datasets. To first tackle this challenge, we prompt the LLMs with the outputs of established causal discovery algorithms designed for observational datasets. These algorithm outputs effectively serve as the sufficient statistics of the observation data. However, quite surprisingly, we find that prompting the LLMs with these sufficient statistics decreases the LLMs' performance in causal discovery. To address this current limitation, we propose CARE, a framework that enhances LLMs' causal-reasoning ability by teaching them to effectively utilize the outputs of established causal-discovery algorithms through supervised fine-tuning. Experimental results show that a finetuned Qwen2.5-1.5B model produced by CARE significantly outperforms both traditional causal-discovery algorithms and state-of-the-art LLMs with over a thousand times more parameters, demonstrating effective utilization of its own knowledge and the external algorithmic clues.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 最近在一系列推理和生成任务中展示了令人印象深刻的功能。然而，研究表明，法学硕士缺乏识别因果关系的能力，而因果关系是人类智力的基本基石。我们首先对法学硕士在执行因果发现任务时的行为进行了探索性调查，发现他们大多依赖于变量名称的语义，而忽略了观察数据。鉴于法学硕士从未接受过处理结构数据集的培训，这并不奇怪。为了首先应对这一挑战，我们向法学硕士提供专为观察数据集设计的已建立因果发现算法的输出。这些算法输出有效地充当了观测数据的充分统计。然而，令人惊讶的是，我们发现向法学硕士提供这些足够的统计数据会降低法学硕士在因果发现方面的表现。为了解决当前的这一限制，我们提出了 CARE，这是一个框架，通过教导法学硕士通过监督微调有效利用已建立的因果发现算法的输出，从而增强法学硕士的因果推理能力。实验结果表明，CARE 生成的微调 Qwen2.5-1.5B 模型显着优于传统的因果发现算法和最先进的 LLM，其参数数量增加了一千倍以上，证明了其自身知识和外部算法线索的有效利用。</li>
</ul>

<h3>Title: Mixture of Ranks with Degradation-Aware Routing for One-Step Real-World Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Xiao He, Zhijun Tu, Kun Cheng, Mingrui Zhu, Jie Hu, Nannan Wang, Xinbo Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16024">https://arxiv.org/abs/2511.16024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16024">https://arxiv.org/pdf/2511.16024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16024]] Mixture of Ranks with Degradation-Aware Routing for One-Step Real-World Image Super-Resolution(https://arxiv.org/abs/2511.16024)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>The demonstrated success of sparsely-gated Mixture-of-Experts (MoE) architectures, exemplified by models such as DeepSeek and Grok, has motivated researchers to investigate their adaptation to diverse domains. In real-world image super-resolution (Real-ISR), existing approaches mainly rely on fine-tuning pre-trained diffusion models through Low-Rank Adaptation (LoRA) module to reconstruct high-resolution (HR) images. However, these dense Real-ISR models are limited in their ability to adaptively capture the heterogeneous characteristics of complex real-world degraded samples or enable knowledge sharing between inputs under equivalent computational budgets. To address this, we investigate the integration of sparse MoE into Real-ISR and propose a Mixture-of-Ranks (MoR) architecture for single-step image super-resolution. We introduce a fine-grained expert partitioning strategy that treats each rank in LoRA as an independent expert. This design enables flexible knowledge recombination while isolating fixed-position ranks as shared experts to preserve common-sense features and minimize routing redundancy. Furthermore, we develop a degradation estimation module leveraging CLIP embeddings and predefined positive-negative text pairs to compute relative degradation scores, dynamically guiding expert activation. To better accommodate varying sample complexities, we incorporate zero-expert slots and propose a degradation-aware load-balancing loss, which dynamically adjusts the number of active experts based on degradation severity, ensuring optimal computational resource allocation. Comprehensive experiments validate our framework's effectiveness and state-of-the-art performance.</li>
<li><strong>摘要：</strong>稀疏门控专家混合 (MoE) 架构（以 DeepSeek 和 Grok 等模型为例）所取得的成功促使研究人员研究它们对不同领域的适应。在现实世界图像超分辨率（Real-ISR）中，现有方法主要依靠通过低秩适应（LoRA）模块微调预训练的扩散模型来重建高分辨率（HR）图像。然而，这些密集的 Real-ISR 模型在自适应捕获复杂现实世界退化样本的异构特征或在等效计算预算下实现输入之间的知识共享方面的能力受到限制。为了解决这个问题，我们研究了将稀疏 MoE 集成到 Real-ISR 中，并提出了一种用于单步图像超分辨率的混合等级 (MoR) 架构。我们引入了细粒度的专家划分策略，将 LoRA 中的每个等级视为独立专家。这种设计可以实现灵活的知识重组，同时将固定位置的等级隔离为共享专家，以保留常识性特征并最大限度地减少路由冗余。此外，我们开发了一个退化估计模块，利用 CLIP 嵌入和预定义的正负文本对来计算相对退化分数，动态指导专家激活。为了更好地适应不同的样本复杂性，我们结合了零专家槽，并提出了一种退化感知负载平衡损失，它根据退化严重程度动态调整活跃专家的数量，确保最佳的计算资源分配。全面的实验验证了我们框架的有效性和最先进的性能。</li>
</ul>

<h3>Title: AMS-KV: Adaptive KV Caching in Multi-Scale Visual Autoregressive Transformers</h3>
<ul>
<li><strong>Authors: </strong>Boxun Xu, Yu Wang, Zihu Wang, Peng Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16047">https://arxiv.org/abs/2511.16047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16047">https://arxiv.org/pdf/2511.16047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16047]] AMS-KV: Adaptive KV Caching in Multi-Scale Visual Autoregressive Transformers(https://arxiv.org/abs/2511.16047)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Visual autoregressive modeling (VAR) via next-scale prediction has emerged as a scalable image generation paradigm. While Key and Value (KV) caching in large language models (LLMs) has been extensively studied, next-scale prediction presents unique challenges, and KV caching design for next-scale based VAR transformers remains largely unexplored. A major bottleneck is the excessive KV memory growth with the increasing number of scales-severely limiting scalability. Our systematic investigation reveals that: (1) Attending to tokens from local scales significantly contributes to generation quality (2) Allocating a small amount of memory for the coarsest scales, termed as condensed scales, stabilizes multi-scale image generation (3) Strong KV similarity across finer scales is predominantly observed in cache-efficient layers, whereas cache-demanding layers exhibit weaker inter-scale similarity. Based on the observations, we introduce AMS-KV, a scale-adaptive KV caching policy for next-scale prediction in VAR models. AMS-KV prioritizes storing KVs from condensed and local scales, preserving the most relevant tokens to maintain generation quality. It further optimizes KV cache utilization and computational efficiency identifying cache-demanding layers through inter-scale similarity analysis. Compared to the vanilla next-scale prediction-based VAR models, AMS-KV reduces KV cache usage by up to 84.83% and self-attention latency by 60.48%. Moreover, when the baseline VAR-d30 model encounters out-of-memory failures at a batch size of 128, AMS-KV enables stable scaling to a batch size of 256 with improved throughput.</li>
<li><strong>摘要：</strong>通过下一尺度预测的视觉自回归建模（VAR）已成为可扩展的图像生成范例。虽然大型语言模型 (LLM) 中的键和值 (KV) 缓存已得到广泛研究，但下一代预测提出了独特的挑战，并且基于下一代的 VAR 转换器的 KV 缓存设计在很大程度上仍未得到探索。一个主要瓶颈是随着尺度数量的增加，KV内存过度增长——严重限制了可扩展性。我们的系统调查表明：（1）关注局部尺度的标记对生成质量有显着贡献（2）为最粗糙的尺度（称为压缩尺度）分配少量内存，可以稳定多尺度图像的生成（3）跨精细尺度的强KV相似性主要在缓存有效层中观察到，而缓存需求层表现出较弱的尺度间相似性。基于观察结果，我们引入了 AMS-KV，一种用于 VAR 模型中下一个尺度预测的尺度自适应 KV 缓存策略。 AMS-KV 优先存储压缩和局部尺度的 KV，保留最相关的令牌以维持生成质量。它通过尺度间相似性分析识别缓存需求层，进一步优化 KV 缓存利用率和计算效率。与普通的基于下一代预测的 VAR 模型相比，AMS-KV 将 KV 缓存使用率降低了 84.83%，并将 self-attention 延迟降低了 60.48%。此外，当基线 VAR-d30 模型在批量大小为 128 时遇到内存不足故障时，AMS-KV 可以稳定扩展到批量大小为 256，并提高吞吐量。</li>
</ul>

<h3>Title: LiSTAR: Ray-Centric World Models for 4D LiDAR Sequences in Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Pei Liu, Songtao Wang, Lang Zhang, Xingyue Peng, Yuandong Lyu, Jiaxin Deng, Songxin Lu, Weiliang Ma, Xueyang Zhang, Yifei Zhan, XianPeng Lang, Jun Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16049">https://arxiv.org/abs/2511.16049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16049">https://arxiv.org/pdf/2511.16049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16049]] LiSTAR: Ray-Centric World Models for 4D LiDAR Sequences in Autonomous Driving(https://arxiv.org/abs/2511.16049)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Synthesizing high-fidelity and controllable 4D LiDAR data is crucial for creating scalable simulation environments for autonomous driving. This task is inherently challenging due to the sensor's unique spherical geometry, the temporal sparsity of point clouds, and the complexity of dynamic scenes. To address these challenges, we present LiSTAR, a novel generative world model that operates directly on the sensor's native geometry. LiSTAR introduces a Hybrid-Cylindrical-Spherical (HCS) representation to preserve data fidelity by mitigating quantization artifacts common in Cartesian grids. To capture complex dynamics from sparse temporal data, it utilizes a Spatio-Temporal Attention with Ray-Centric Transformer (START) that explicitly models feature evolution along individual sensor rays for robust temporal coherence. Furthermore, for controllable synthesis, we propose a novel 4D point cloud-aligned voxel layout for conditioning and a corresponding discrete Masked Generative START (MaskSTART) framework, which learns a compact, tokenized representation of the scene, enabling efficient, high-resolution, and layout-guided compositional generation. Comprehensive experiments validate LiSTAR's state-of-the-art performance across 4D LiDAR reconstruction, prediction, and conditional generation, with substantial quantitative gains: reducing generation MMD by a massive 76%, improving reconstruction IoU by 32%, and lowering prediction L1 Med by 50%. This level of performance provides a powerful new foundation for creating realistic and controllable autonomous systems simulations. Project link: this https URL.</li>
<li><strong>摘要：</strong>合成高保真且可控的 4D LiDAR 数据对于创建可扩展的自动驾驶模拟环境至关重要。由于传感器独特的球形几何形状、点云的时间稀疏性以及动态场景的复杂性，这项任务本质上具有挑战性。为了应对这些挑战，我们推出了 LiSTAR，这是一种直接在传感器的原生几何结构上运行的新颖的生成世界模型。 LiSTAR 引入了混合柱球 (HCS) 表示，通过减轻笛卡尔网格中常见的量化伪影来保持数据保真度。为了从稀疏的时间数据中捕获复杂的动态，它利用带有射线中心变压器（START）的时空注意力，明确地模拟沿单个传感器射线的特征演化，以实现鲁棒的时间相干性。此外，为了实现可控合成，我们提出了一种用于调节的新型 4D 点云对齐体素布局和相应的离散掩模生成起始（MaskSTART）框架，该框架学习场景的紧凑、标记化表示，从而实现高效、高分辨率和布局引导的合成生成。综合实验验证了 LiSTAR 在 4D LiDAR 重建、预测和条件生成方面的最先进性能，并具有显着的定量收益：将生成 MMD 大幅减少 76%，将重建 IoU 提高 32%，并将预测 L1 Med 降低 50%。这种性能水平为创建真实且可控的自主系统模拟提供了强大的新基础。项目链接：此 https URL。</li>
</ul>

<h3>Title: Pathlet Variational Auto-Encoder for Robust Trajectory Generation</h3>
<ul>
<li><strong>Authors: </strong>Yuanbo Tang, Yan Tang, Zixuan Zhang, Zihui Zhao, Yang Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16105">https://arxiv.org/abs/2511.16105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16105">https://arxiv.org/pdf/2511.16105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16105]] Pathlet Variational Auto-Encoder for Robust Trajectory Generation(https://arxiv.org/abs/2511.16105)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Trajectory generation has recently drawn growing interest in privacy-preserving urban mobility studies and location-based service applications. Although many studies have used deep learning or generative AI methods to model trajectories and have achieved promising results, the robustness and interpretability of such models are largely unexplored. This limits the application of trajectory generation algorithms on noisy real-world data and their trustworthiness in downstream tasks. To address this issue, we exploit the regular structure in urban trajectories and propose a deep generative model based on the pathlet representation, which encode trajectories with binary vectors associated with a learned dictionary of trajectory segments. Specifically, we introduce a probabilistic graphical model to describe the trajectory generation process, which includes a Variational Autoencoder (VAE) component and a linear decoder component. During training, the model can simultaneously learn the latent embedding of pathlet representations and the pathlet dictionary that captures mobility patterns in the trajectory dataset. The conditional version of our model can also be used to generate customized trajectories based on temporal and spatial constraints. Our model can effectively learn data distribution even using noisy data, achieving relative improvements of $35.4\%$ and $26.3\%$ over strong baselines on two real-world trajectory datasets. Moreover, the generated trajectories can be conveniently utilized for multiple downstream tasks, including trajectory prediction and data denoising. Lastly, the framework design offers a significant efficiency advantage, saving $64.8\%$ of the time and $56.5\%$ of GPU memory compared to previous approaches.</li>
<li><strong>摘要：</strong>轨迹生成最近引起了人们对保护隐私的城市交通研究和基于位置的服务应用的越来越浓厚的兴趣。尽管许多研究使用深度学习或生成人工智能方法来对轨迹进行建模并取得了有希望的结果，但此类模型的稳健性和可解释性在很大程度上尚未得到探索。这限制了轨迹生成算法在嘈杂的现实世界数据上的应用及其在下游任务中的可信度。为了解决这个问题，我们利用城市轨迹中的规则结构，并提出了一种基于路径表示的深度生成模型，该模型使用与轨迹段的学习字典相关联的二进制向量对轨迹进行编码。具体来说，我们引入了概率图形模型来描述轨迹生成过程，其中包括变分自动编码器（VAE）组件和线性解码器组件。在训练过程中，模型可以同时学习路径表示的潜在嵌入和捕获轨迹数据集中的移动模式的路径字典。我们模型的条件版本还可以用于根据时间和空间约束生成定制轨迹。即使使用噪声数据，我们的模型也可以有效地学习数据分布，在两个现实世界轨迹数据集的强基线上实现了 35.4\%$ 和 26.3\%$ 的相对改进。此外，生成的轨迹可以方便地用于多种下游任务，包括轨迹预测和数据去噪。最后，该框架设计提供了显着的效率优势，与以前的方法相比，节省了 64.8\%$ 的时间和 56.5\%$ 的 GPU 内存。</li>
</ul>

<h3>Title: Decoupling Complexity from Scale in Latent Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Tianxiong Zhong, Xingye Tian, Xuebo Wang, Boyuan Jiang, Xin Tao, Pengfei Wan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16117">https://arxiv.org/abs/2511.16117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16117">https://arxiv.org/pdf/2511.16117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16117]] Decoupling Complexity from Scale in Latent Diffusion Model(https://arxiv.org/abs/2511.16117)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Existing latent diffusion models typically couple scale with content complexity, using more latent tokens to represent higher-resolution images or higher-frame rate videos. However, the latent capacity required to represent visual data primarily depends on content complexity, with scale serving only as an upper bound. Motivated by this observation, we propose DCS-LDM, a novel paradigm for visual generation that decouples information complexity from scale. DCS-LDM constructs a hierarchical, scale-independent latent space that models sample complexity through multi-level tokens and supports decoding to arbitrary resolutions and frame rates within a fixed latent representation. This latent space enables DCS-LDM to achieve a flexible computation-quality tradeoff. Furthermore, by decomposing structural and detailed information across levels, DCS-LDM supports a progressive coarse-to-fine generation paradigm. Experimental results show that DCS-LDM delivers performance comparable to state-of-the-art methods while offering flexible generation across diverse scales and visual qualities.</li>
<li><strong>摘要：</strong>现有的潜在扩散模型通常将规模与内容复杂性结合起来，使用更多的潜在标记来表示更高分辨率的图像或更高帧速率的视频。然而，表示视觉数据所需的潜在容量主要取决于内容复杂性，规模仅作为上限。受这一观察的启发，我们提出了 DCS-LDM，这是一种新颖的视觉生成范例，可将信息复杂性与规模解耦。 DCS-LDM 构建了一个分层的、与尺度无关的潜在空间，通过多级标记对样本复杂性进行建模，并支持在固定潜在表示内解码到任意分辨率和帧速率。这个潜在空间使 DCS-LDM 能够实现灵活的计算质量权衡。此外，通过跨级别分解结构和详细信息，DCS-LDM 支持渐进的从粗到细的生成范例。实验结果表明，DCS-LDM 的性能可与最先进的方法相媲美，同时提供跨不同尺度和视觉质量的灵活生成。</li>
</ul>

<h3>Title: An Interpretability-Guided Framework for Responsible Synthetic Data Generation in Emotional Text</h3>
<ul>
<li><strong>Authors: </strong>Paula Joy B. Martinez, Jose Marie Antonio Miñoza, Sebastian C. Ibañez</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16132">https://arxiv.org/abs/2511.16132</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16132">https://arxiv.org/pdf/2511.16132</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16132]] An Interpretability-Guided Framework for Responsible Synthetic Data Generation in Emotional Text(https://arxiv.org/abs/2511.16132)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Emotion recognition from social media is critical for understanding public sentiment, but accessing training data has become prohibitively expensive due to escalating API costs and platform restrictions. We introduce an interpretability-guided framework where Shapley Additive Explanations (SHAP) provide principled guidance for LLM-based synthetic data generation. With sufficient seed data, SHAP-guided approach matches real data performance, significantly outperforms naïve generation, and substantially improves classification for underrepresented emotion classes. However, our linguistic analysis reveals that synthetic text exhibits reduced vocabulary richness and fewer personal or temporally complex expressions than authentic posts. This work provides both a practical framework for responsible synthetic data generation and a critical perspective on its limitations, underscoring that the future of trustworthy AI depends on navigating the trade-offs between synthetic utility and real-world authenticity.</li>
<li><strong>摘要：</strong>来自社交媒体的情绪识别对于理解公众情绪至关重要，但由于 API 成本不断上升和平台限制，访问训练数据变得异常昂贵。我们引入了一个可解释性引导的框架，其中 Shapley Additive Explanations (SHAP) 为基于 LLM 的合成数据生成提供原则性指导。有了足够的种子数据，SHAP 引导的方法可以匹配真实的数据性能，显着优于朴素生成，并大大改进了代表性不足的情绪类别的分类。然而，我们的语言分析表明，与真实帖子相比，合成文本的词汇丰富度较低，个人或时间复杂的表达也较少。这项工作为负责任的合成数据生成提供了一个实用的框架，并对其局限性提供了批判性的视角，强调值得信赖的人工智能的未来取决于在合成效用和现实世界真实性之间进行权衡。</li>
</ul>

<h3>Title: How Noise Benefits AI-generated Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Jiazhen Yan, Ziqiang Li, Fan Wang, Kai Zeng, Zhangjie Fu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16136">https://arxiv.org/abs/2511.16136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16136">https://arxiv.org/pdf/2511.16136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16136]] How Noise Benefits AI-generated Image Detection(https://arxiv.org/abs/2511.16136)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of generative models has made real and synthetic images increasingly indistinguishable. Although extensive efforts have been devoted to detecting AI-generated images, out-of-distribution generalization remains a persistent challenge. We trace this weakness to spurious shortcuts exploited during training and we also observe that small feature-space perturbations can mitigate shortcut dominance. To address this problem in a more controllable manner, we propose the Positive-Incentive Noise for CLIP (PiN-CLIP), which jointly trains a noise generator and a detection network under a variational positive-incentive principle. Specifically, we construct positive-incentive noise in the feature space via cross-attention fusion of visual and categorical semantic features. During optimization, the noise is injected into the feature space to fine-tune the visual encoder, suppressing shortcut-sensitive directions while amplifying stable forensic cues, thereby enabling the extraction of more robust and generalized artifact representations. Comparative experiments are conducted on an open-world dataset comprising synthetic images generated by 42 distinct generative models. Our method achieves new state-of-the-art performance, with notable improvements of 5.4 in average accuracy over existing approaches.</li>
<li><strong>摘要：</strong>生成模型的快速发展使得真实图像和合成图像变得越来越难以区分。尽管人们在检测人工智能生成的图像方面做出了大量努力，但分布外泛化仍然是一个持续存在的挑战。我们将这一弱点追溯到训练期间利用的虚假快捷方式，并且我们还观察到小的特征空间扰动可以减轻快捷方式的优势。为了以更可控的方式解决这个问题，我们提出了正激励噪声CLIP（PiN-CLIP），它在变分正激励原理下联合训练噪声生成器和检测网络。具体来说，我们通过视觉和分类语义特征的交叉注意融合在特征空间中构建正激励噪声。在优化过程中，噪声被注入特征空间以微调视觉编码器，抑制捷径敏感方向，同时放大稳定的取证线索，从而能够提取更稳健和更通用的伪影表示。在包含 42 个不同生成模型生成的合成图像的开放世界数据集上进行了比较实验。我们的方法实现了新的最先进的性能，与现有方法相比，平均准确度显着提高了 5.4。</li>
</ul>

<h3>Title: Reasoning Guided Embeddings: Leveraging MLLM Reasoning for Improved Multimodal Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Chunxu Liu, Jiyuan Yang, Ruopeng Gao, Yuhan Zhu, Feng Zhu, Rui Zhao, Limin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16150">https://arxiv.org/abs/2511.16150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16150">https://arxiv.org/pdf/2511.16150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16150]] Reasoning Guided Embeddings: Leveraging MLLM Reasoning for Improved Multimodal Retrieval(https://arxiv.org/abs/2511.16150)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Multimodal embeddings are widely used in downstream tasks such as multimodal retrieval, enabling alignment of interleaved modalities in a shared representation space. While recent studies show that Multimodal Large Language Models (MLLMs) can serve as strong embedding extractors, existing approaches treat embedding extraction as a direct encoding step, overlooking the fact that MLLMs possess the generative capability for reasoning that could be leveraged to enhance representation quality. In this work, we explore how to explicitly incorporate reasoning into the embedding process. To this end, we propose Reasoning Guided Embeddings (RGE), which preserves the generative rationale process of MLLMs and couples it with contrastive training. Our method first enables the model to perform structured rationale generation conditioned on the instruction, and then extracts representations after reasoning has unfolded. This simple design enhances the context-conditional inference signals within the embedding, leading to improved multimodal representation quality. Experiments on the MMEB benchmark show that reasoning-guided conditioning improves multimodal retrieval performance by 4.9% over the non-reasoning baseline, confirming that explicit reasoning can effectively enhance embedding quality.</li>
<li><strong>摘要：</strong>多模态嵌入广泛用于下游任务，例如多模态检索，可以在共享表示空间中对齐交错模态。虽然最近的研究表明多模态大型语言模型 (MLLM) 可以作为强大的嵌入提取器，但现有方法将嵌入提取视为直接编码步骤，忽视了 MLLM 拥有可用于提高表示质量的推理生成能力这一事实。在这项工作中，我们探索如何将推理明确地纳入嵌入过程中。为此，我们提出推理引导嵌入（RGE），它保留了 MLLM 的生成原理过程并将其与对比训练结合起来。我们的方法首先使模型能够根据指令执行结构化基本原理生成，然后在推理展开后提取表示。这种简单的设计增强了嵌入内的上下文条件推理信号，从而提高了多模态表示质量。 MMEB 基准上的实验表明，推理引导调节比非推理基线将多模态检索性能提高了 4.9%，证实显式推理可以有效提高嵌入质量。</li>
</ul>

<h3>Title: Pluggable Pruning with Contiguous Layer Distillation for Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Jian Ma, Qirong Peng, Xujie Zhu, Peixing Xie, Chen Chen, Haonan Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16156">https://arxiv.org/abs/2511.16156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16156">https://arxiv.org/pdf/2511.16156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16156]] Pluggable Pruning with Contiguous Layer Distillation for Diffusion Transformers(https://arxiv.org/abs/2511.16156)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion Transformers (DiTs) have shown exceptional performance in image generation, yet their large parameter counts incur high computational costs, impeding deployment in resource-constrained settings. To address this, we propose Pluggable Pruning with Contiguous Layer Distillation (PPCL), a flexible structured pruning framework specifically designed for DiT architectures. First, we identify redundant layer intervals through a linear probing mechanism combined with the first-order differential trend analysis of similarity metrics. Subsequently, we propose a plug-and-play teacher-student alternating distillation scheme tailored to integrate depth-wise and width-wise pruning within a single training phase. This distillation framework enables flexible knowledge transfer across diverse pruning ratios, eliminating the need for per-configuration retraining. Extensive experiments on multiple Multi-Modal Diffusion Transformer architecture models demonstrate that PPCL achieves a 50\% reduction in parameter count compared to the full model, with less than 3\% degradation in key objective metrics. Notably, our method maintains high-quality image generation capabilities while achieving higher compression ratios, rendering it well-suited for resource-constrained environments. The open-source code, checkpoints for PPCL can be found at the following link: this https URL.</li>
<li><strong>摘要：</strong>扩散变压器 (DiT) 在图像生成方面表现出了卓越的性能，但其大量参数会带来高昂的计算成本，阻碍了在资源有限的环境中的部署。为了解决这个问题，我们提出了带有连续层蒸馏（PPCL）的可插入剪枝，这是一种专为 DiT 架构设计的灵活的结构化剪枝框架。首先，我们通过线性探测机制结合相似性度量的一阶微分趋势分析来识别冗余层区间。随后，我们提出了一种即插即用的师生交替蒸馏方案，旨在将深度修剪和宽度修剪整合到单个训练阶段中。这种蒸馏框架可以实现跨不同修剪比率的灵活知识转移，从而消除了对每个配置进行重新训练的需要。对多个多模态扩散变压器架构模型的大量实验表明，与完整模型相比，PPCL 的参数数量减少了 50%，而关键目标指标的下降幅度不到 3%。值得注意的是，我们的方法保持了高质量的图像生成能力，同时实现了更高的压缩比，使其非常适合资源受限的环境。 PPCL 的开源代码和检查点可以在以下链接中找到：此 https URL。</li>
</ul>

<h3>Title: Simba: Towards High-Fidelity and Geometrically-Consistent Point Cloud Completion via Transformation Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Lirui Zhang, Zhengkai Zhao, Zhi Zuo, Pan Gao, Jie Qin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16161">https://arxiv.org/abs/2511.16161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16161">https://arxiv.org/pdf/2511.16161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16161]] Simba: Towards High-Fidelity and Geometrically-Consistent Point Cloud Completion via Transformation Diffusion(https://arxiv.org/abs/2511.16161)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Point cloud completion is a fundamental task in 3D vision. A persistent challenge in this field is simultaneously preserving fine-grained details present in the input while ensuring the global structural integrity of the completed shape. While recent works leveraging local symmetry transformations via direct regression have significantly improved the preservation of geometric structure details, these methods suffer from two major limitations: (1) These regression-based methods are prone to overfitting which tend to memorize instant-specific transformations instead of learning a generalizable geometric prior. (2) Their reliance on point-wise transformation regression lead to high sensitivity to input noise, severely degrading their robustness and generalization. To address these challenges, we introduce Simba, a novel framework that reformulates point-wise transformation regression as a distribution learning problem. Our approach integrates symmetry priors with the powerful generative capabilities of diffusion models, avoiding instance-specific memorization while capturing robust geometric structures. Additionally, we introduce a hierarchical Mamba-based architecture to achieve high-fidelity upsampling. Extensive experiments across the PCN, ShapeNet, and KITTI benchmarks validate our method's state-of-the-art (SOTA) performance.</li>
<li><strong>摘要：</strong>点云补全是 3D 视觉中的一项基本任务。该领域持续存在的挑战是同时保留输入中存在的细粒度细节，同时确保完成形状的全局结构完整性。虽然最近通过直接回归利用局部对称变换的工作显着改善了几何结构细节的保存，但这些方法存在两个主要局限性：（1）这些基于回归的方法容易过度拟合，往往会记住特定于瞬时的变换，而不是学习可概括的几何先验。 (2)它们对逐点变换回归的依赖导致对输入噪声高度敏感，严重降低了它们的鲁棒性和泛化性。为了应对这些挑战，我们引入了 Simba，这是一种新颖的框架，它将逐点变换回归重新表述为分布学习问题。我们的方法将对称先验与扩散模型的强大生成能力相结合，避免特定于实例的记忆，同时捕获稳健的几何结构。此外，我们引入了基于 Mamba 的分层架构来实现高保真上采样。跨 PCN、ShapeNet 和 KITTI 基准的大量实验验证了我们的方法的最先进 (SOTA) 性能。</li>
</ul>

<h3>Title: An Image Is Worth Ten Thousand Words: Verbose-Text Induction Attacks on VLMs</h3>
<ul>
<li><strong>Authors: </strong>Zhi Luo, Zenghui Yuan, Wenqi Wei, Daizong Liu, Pan Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16163">https://arxiv.org/abs/2511.16163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16163">https://arxiv.org/pdf/2511.16163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16163]] An Image Is Worth Ten Thousand Words: Verbose-Text Induction Attacks on VLMs(https://arxiv.org/abs/2511.16163)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>With the remarkable success of Vision-Language Models (VLMs) on multimodal tasks, concerns regarding their deployment efficiency have become increasingly prominent. In particular, the number of tokens consumed during the generation process has emerged as a key evaluation this http URL studies have shown that specific inputs can induce VLMs to generate lengthy outputs with low information density, which significantly increases energy consumption, latency, and token costs. However, existing methods simply delay the occurrence of the EOS token to implicitly prolong output, and fail to directly maximize the output token length as an explicit optimization objective, lacking stability and this http URL address these limitations, this paper proposes a novel verbose-text induction attack (VTIA) to inject imperceptible adversarial perturbations into benign images via a two-stage framework, which identifies the most malicious prompt embeddings for optimizing and maximizing the output token of the perturbed this http URL, we first perform adversarial prompt search, employing reinforcement learning strategies to automatically identify adversarial prompts capable of inducing the LLM component within VLMs to produce verbose outputs. We then conduct vision-aligned perturbation optimization to craft adversarial examples on input images, maximizing the similarity between the perturbed image's visual embeddings and those of the adversarial prompt, thereby constructing malicious images that trigger verbose text generation. Comprehensive experiments on four popular VLMs demonstrate that our method achieves significant advantages in terms of effectiveness, efficiency, and generalization capability.</li>
<li><strong>摘要：</strong>随着视觉语言模型（VLM）在多模式任务上取得巨大成功，对其部署效率的担忧变得越来越突出。特别是，生成过程中消耗的代币数量已成为一个关键评估指标。http URL 研究表明，特定的输入可能会导致 VLM 生成信息密度低的冗长输出，从而显着增加能耗、延迟和代币成本。然而，现有的方法只是延迟 EOS 令牌的出现以隐式延长输出，并且无法直接最大化输出令牌长度作为显式优化目标，缺乏稳定性，而此 http URL 解决了这些限制，本文提出了一种新颖的详细文本归纳攻击（VTIA），通过两阶段框架将难以察觉的对抗性扰动注入良性图像，该攻击识别最恶意的提示嵌入，以优化和最大化受扰动此 http URL 的输出令牌，我们首先执行对抗性提示搜索，采用强化学习策略自动识别能够诱导 VLM 内的 LLM 组件产生详细输出的对抗性提示。然后，我们进行视觉对齐扰动优化，以在输入图像上制作对抗性示例，最大化扰动图像的视觉嵌入与对抗性提示之间的相似性，从而构建触发详细文本生成的恶意图像。对四种流行的 VLM 的综合实验表明，我们的方法在有效性、效率和泛化能力方面具有显着的优势。</li>
</ul>

<h3>Title: Causal Synthetic Data Generation in Recruitment</h3>
<ul>
<li><strong>Authors: </strong>Andrea Iommi, Antonio Mastropietro, Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16204">https://arxiv.org/abs/2511.16204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16204">https://arxiv.org/pdf/2511.16204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16204]] Causal Synthetic Data Generation in Recruitment(https://arxiv.org/abs/2511.16204)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The importance of Synthetic Data Generation (SDG) has increased significantly in domains where data quality is poor or access is limited due to privacy and regulatory constraints. One such domain is recruitment, where publicly available datasets are scarce due to the sensitive nature of information typically found in curricula vitae, such as gender, disability status, or age. % This lack of accessible, representative data presents a significant obstacle to the development of fair and transparent machine learning models, particularly ranking algorithms that require large volumes of data to effectively learn how to recommend candidates. In the absence of such data, these models are prone to poor generalisation and may fail to perform reliably in real-world scenarios. % Recent advances in Causal Generative Models (CGMs) offer a promising solution. CGMs enable the generation of synthetic datasets that preserve the underlying causal relationships within the data, providing greater control over fairness and interpretability in the data generation process. % In this study, we present a specialised SDG method involving two CGMs: one modelling job offers and the other modelling curricula. Each model is structured according to a causal graph informed by domain expertise. We use these models to generate synthetic datasets and evaluate the fairness of candidate rankings under controlled scenarios that introduce specific biases.</li>
<li><strong>摘要：</strong>在数据质量较差或由于隐私和监管限制而导致访问受限的领域，合成数据生成 (SDG) 的重要性显着增加。其中一个领域是招聘，由于简历中通常存在的信息（例如性别、残疾状况或年龄）的敏感性，公开可用的数据集很少。 % 缺乏可访问的、有代表性的数据对公平和透明的机器学习模型的开发构成了重大障碍，特别是需要大量数据才能有效学习如何推荐候选人的排名算法。如果缺乏此类数据，这些模型很容易泛化不良，并且可能无法在现实场景中可靠地执行。 % 因果生成模型 (CGM) 的最新进展提供了一个有前途的解决方案。 CGM 能够生成合成数据集，保留数据内的潜在因果关系，从而更好地控制数据生成过程中的公平性和可解释性。 % 在这项研究中，我们提出了一种专门的 SDG 方法，涉及两种 CGM：一个是建模工作机会，另一个是建模课程。每个模型都是根据领域专业知识提供的因果图构建的。我们使用这些模型来生成合成数据集，并在引入特定偏差的受控场景下评估候选排名的公平性。</li>
</ul>

<h3>Title: Towards Overcoming Data Scarcity in Nuclear Energy: A Study on Critical Heat Flux with Physics-consistent Conditional Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Farah Alsafadi, Alexandra Akins, Xu Wu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16207">https://arxiv.org/abs/2511.16207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16207">https://arxiv.org/pdf/2511.16207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16207]] Towards Overcoming Data Scarcity in Nuclear Energy: A Study on Critical Heat Flux with Physics-consistent Conditional Diffusion Model(https://arxiv.org/abs/2511.16207)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Deep generative modeling provides a powerful pathway to overcome data scarcity in energy-related applications where experimental data are often limited, costly, or difficult to obtain. By learning the underlying probability distribution of the training dataset, deep generative models, such as the diffusion model (DM), can generate high-fidelity synthetic samples that statistically resemble the training data. Such synthetic data generation can significantly enrich the size and diversity of the available training data, and more importantly, improve the robustness of downstream machine learning models in predictive tasks. The objective of this paper is to investigate the effectiveness of DM for overcoming data scarcity in nuclear energy applications. By leveraging a public dataset on critical heat flux (CHF) that cover a wide range of commercial nuclear reactor operational conditions, we developed a DM that can generate an arbitrary amount of synthetic samples for augmenting of the CHF dataset. Since a vanilla DM can only generate samples randomly, we also developed a conditional DM capable of generating targeted CHF data under user-specified thermal-hydraulic conditions. The performance of the DM was evaluated based on their ability to capture empirical feature distributions and pair-wise correlations, as well as to maintain physical consistency. The results showed that both the DM and conditional DM can successfully generate realistic and physics-consistent CHF data. Furthermore, uncertainty quantification was performed to establish confidence in the generated data. The results demonstrated that the conditional DM is highly effective in augmenting CHF data while maintaining acceptable levels of uncertainty.</li>
<li><strong>摘要：</strong>深度生成建模提供了一种强大的途径来克服能源相关应用中的数据稀缺问题，在这些应用中，实验数据通常有限、成本高昂或难以获取。通过学习训练数据集的潜在概率分布，深度生成模型（例如扩散模型 (DM)）可以生成在统计上与训练数据相似的高保真合成样本。这种合成数据生成可以显着丰富可用训练数据的规模和多样性，更重要的是，提高下游机器学习模型在预测任务中的稳健性。本文的目的是研究 DM 在克服核能应用中数据稀缺方面的有效性。通过利用涵盖广泛商业核反应堆运行条件的临界热通量 (CHF) 公共数据集，我们开发了一种 DM，可以生成任意数量的合成样本以增强 CHF 数据集。由于普通 DM 只能随机生成样本，因此我们还开发了一种条件 DM，能够在用户指定的热水力条件下生成目标 CHF 数据。 DM 的性能根据其捕获经验特征分布和成对相关性以及保持物理一致性的能力进行评估。结果表明，DM 和条件 DM 都可以成功生成真实且物理一致的 CHF 数据。此外，还进行了不确定性量化，以建立对生成数据的信心。结果表明，条件 DM 在增强 CHF 数据方面非常有效，同时保持可接受的不确定性水平。</li>
</ul>

<h3>Title: Mem-MLP: Real-Time 3D Human Motion Generation from Sparse Inputs</h3>
<ul>
<li><strong>Authors: </strong>Sinan Mutlu, Georgios F. Angelis, Savas Ozkan, Paul Wisbey, Anastasios Drosou, Mete Ozay</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16264">https://arxiv.org/abs/2511.16264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16264">https://arxiv.org/pdf/2511.16264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16264]] Mem-MLP: Real-Time 3D Human Motion Generation from Sparse Inputs(https://arxiv.org/abs/2511.16264)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Realistic and smooth full-body tracking is crucial for immersive AR/VR applications. Existing systems primarily track head and hands via Head Mounted Devices (HMDs) and controllers, making the 3D full-body reconstruction in-complete. One potential approach is to generate the full-body motions from sparse inputs collected from limited sensors using a Neural Network (NN) model. In this paper, we propose a novel method based on a multi-layer perceptron (MLP) backbone that is enhanced with residual connections and a novel NN-component called Memory-Block. In particular, Memory-Block represents missing sensor data with trainable code-vectors, which are combined with the sparse signals from previous time instances to improve the temporal consistency. Furthermore, we formulate our solution as a multi-task learning problem, allowing our MLP-backbone to learn robust representations that boost accuracy. Our experiments show that our method outperforms state-of-the-art baselines by substantially reducing prediction errors. Moreover, it achieves 72 FPS on mobile HMDs that ultimately improves the accuracy-running time tradeoff.</li>
<li><strong>摘要：</strong>真实流畅的全身跟踪对于沉浸式 AR/VR 应用至关重要。现有系统主要通过头戴式设备 (HMD) 和控制器来跟踪头部和手部，使得 3D 全身重建不完整。一种潜在的方法是使用神经网络 (NN) 模型根据从有限传感器收集的稀疏输入生成全身运动。在本文中，我们提出了一种基于多层感知器（MLP）主干的新方法，该主干通过残差连接和称为记忆块的新型神经网络组件进行增强。特别是，内存块用可训练的代码向量表示丢失的传感器数据，这些数据与先前时间实例的稀疏信号相结合以提高时间一致性。此外，我们将我们的解决方案制定为多任务学习问题，使我们的 MLP 骨干能够学习鲁棒的表示，从而提高准确性。我们的实验表明，我们的方法通过大幅减少预测误差，优于最先进的基线。此外，它在移动 HMD 上实现了 72 FPS，最终改善了准确性与运行时间的权衡。</li>
</ul>

<h3>Title: Building temporally coherent 3D maps with VGGT for memory-efficient Semantic SLAM</h3>
<ul>
<li><strong>Authors: </strong>Gergely Dinya, Péter Halász, András Lőrincz, Kristóf Karacs, Anna Gelencsér-Horváth</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16282">https://arxiv.org/abs/2511.16282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16282">https://arxiv.org/pdf/2511.16282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16282]] Building temporally coherent 3D maps with VGGT for memory-efficient Semantic SLAM(https://arxiv.org/abs/2511.16282)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present a fast, spatio-temporal scene understanding framework based on Vision Gated Generative Transformers (VGGT). The proposed pipeline is designed to enable efficient, close to real-time performance, supporting applications including assistive navigation. To achieve continuous updates of the 3D scene representation, we process the image flow with a sliding window, aligning submaps, thereby overcoming VGGT's high memory demands. We exploit the VGGT tracking head to aggregate 2D semantic instance masks into 3D objects. To allow for temporal consistency and richer contextual reasoning the system stores timestamps and instance-level identities, thereby enabling the detection of changes in the environment. We evaluate the approach on well-known benchmarks and custom datasets specifically designed for assistive navigation scenarios. The results demonstrate the applicability of the framework to real-world scenarios.</li>
<li><strong>摘要：</strong>我们提出了一种基于视觉门控生成变压器（VGGT）的快速时空场景理解框架。拟议的管道旨在实现高效、接近实时的性能，支持包括辅助导航在内的应用程序。为了实现 3D 场景表示的持续更新，我们使用滑动窗口处理图像流，对齐子图，从而克服 VGGT 的高内存需求。我们利用 VGGT 跟踪头将 2D 语义实例掩码聚合为 3D 对象。为了实现时间一致性和更丰富的上下文推理，系统存储时间戳和实例级身份，从而能够检测环境中的变化。我们在专门为辅助导航场景设计的知名基准和自定义数据集上评估该方法。结果证明了该框架对现实场景的适用性。</li>
</ul>

<h3>Title: Explainable AI for Diabetic Retinopathy Detection Using Deep Learning with Attention Mechanisms and Fuzzy Logic-Based Interpretability</h3>
<ul>
<li><strong>Authors: </strong>Abishek Karthik, Pandiyaraju V, Sreya Mynampati</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16294">https://arxiv.org/abs/2511.16294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16294">https://arxiv.org/pdf/2511.16294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16294]] Explainable AI for Diabetic Retinopathy Detection Using Deep Learning with Attention Mechanisms and Fuzzy Logic-Based Interpretability(https://arxiv.org/abs/2511.16294)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The task of weed detection is an essential element of precision agriculture since accurate species identification allows a farmer to selectively apply herbicides and fits into sustainable agriculture crop management. This paper proposes a hybrid deep learning framework recipe for weed detection that utilizes Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), and Graph Neural Networks (GNNs) to build robustness to multiple field conditions. A Generative Adversarial Network (GAN)-based augmentation method was imposed to balance class distributions and better generalize the model. Further, a self-supervised contrastive pre-training method helps to learn more features from limited annotated data. Experimental results yield superior results with 99.33% accuracy, precision, recall, and F1-score on multi-benchmark datasets. The proposed model architecture enables local, global, and relational feature representations and offers high interpretability and adaptability. Practically, the framework allows real-time, efficient deployment of edge devices for automated weed detecting, reducing over-reliance on herbicides and providing scalable, sustainable precision-farming options.</li>
<li><strong>摘要：</strong>杂草检测任务是精准农业的重要组成部分，因为准确的物种识别使农民能够有选择地使用除草剂并适应可持续农业作物管理。本文提出了一种用于杂草检测的混合深度学习框架配方，利用卷积神经网络 (CNN)、视觉变换器 (ViT) 和图神经网络 (GNN) 来构建对多种现场条件的鲁棒性。采用基于生成对抗网络（GAN）的增强方法来平衡类别分布并更好地推广模型。此外，自监督对比预训练方法有助于从有限的注释数据中学习更多特征。实验结果在多基准数据集上产生了 99.33% 的准确度、精确度、召回率和 F1 分数的优异结果。所提出的模型架构支持局部、全局和关系特征表示，并提供高可解释性和适应性。实际上，该框架允许实时、高效地部署边缘设备以进行自动杂草检测，减少对除草剂的过度依赖，并提供可扩展、可持续的精准农业选项。</li>
</ul>

<h3>Title: NaTex: Seamless Texture Generation as Latent Color Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Zeqiang Lai, Yunfei Zhao, Zibo Zhao, Xin Yang, Xin Huang, Jingwei Huang, Xiangyu Yue, Chunchao Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16317">https://arxiv.org/abs/2511.16317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16317">https://arxiv.org/pdf/2511.16317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16317]] NaTex: Seamless Texture Generation as Latent Color Diffusion(https://arxiv.org/abs/2511.16317)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present NaTex, a native texture generation framework that predicts texture color directly in 3D space. In contrast to previous approaches that rely on baking 2D multi-view images synthesized by geometry-conditioned Multi-View Diffusion models (MVDs), NaTex avoids several inherent limitations of the MVD pipeline. These include difficulties in handling occluded regions that require inpainting, achieving precise mesh-texture alignment along boundaries, and maintaining cross-view consistency and coherence in both content and color intensity. NaTex features a novel paradigm that addresses the aforementioned issues by viewing texture as a dense color point cloud. Driven by this idea, we propose latent color diffusion, which comprises a geometry-awared color point cloud VAE and a multi-control diffusion transformer (DiT), entirely trained from scratch using 3D data, for texture reconstruction and generation. To enable precise alignment, we introduce native geometry control that conditions the DiT on direct 3D spatial information via positional embeddings and geometry latents. We co-design the VAE-DiT architecture, where the geometry latents are extracted via a dedicated geometry branch tightly coupled with the color VAE, providing fine-grained surface guidance that maintains strong correspondence with the texture. With these designs, NaTex demonstrates strong performance, significantly outperforming previous methods in texture coherence and alignment. Moreover, NaTex also exhibits strong generalization capabilities, either training-free or with simple tuning, for various downstream applications, e.g., material generation, texture refinement, and part segmentation and texturing.</li>
<li><strong>摘要：</strong>我们推出了 NaTex，一种原生纹理生成框架，可以直接在 3D 空间中预测纹理颜色。与之前依赖于烘焙由几何条件多视图扩散模型 (MVD) 合成的 2D 多视图图像的方法相比，NaTex 避免了 MVD 管道的几个固有限制。其中包括处理需要修复的遮挡区域、沿边界实现精确的网格纹理对齐以及保持内容和颜色强度的跨视图一致性和连贯性方面的困难。 NaTex 采用了一种新颖的范例，通过将纹理视为密集的彩色点云来解决上述问题。在这个想法的驱动下，我们提出了潜在颜色扩散，其中包括几何感知的颜色点云 VAE 和多控制扩散变压器 (DiT)，完全使用 3D 数据从头开始训练，用于纹理重建和生成。为了实现精确对齐，我们引入了原生几何控制，通过位置嵌入和几何潜在信息将 DiT 调节为直接 3D 空间信息。我们共同设计了 VAE-DiT 架构，其中通过与颜色 VAE 紧密耦合的专用几何分支提取几何潜伏，提供细粒度的表面引导，与纹理保持强对应关系。通过这些设计，NaTex 展示了强大的性能，在纹理连贯性和对齐方面显着优于以前的方法。此外，NaTex 还表现出强大的泛化能力，无论是免训练还是简单调整，适用于各种下游应用，例如材料生成、纹理细化以及零件分割和纹理化。</li>
</ul>

<h3>Title: WWE-UIE: A Wavelet & White Balance Efficient Network for Underwater Image Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Ching-Heng Cheng, Jen-Wei Lee, Chia-Ming Lee, Chih-Chung Hsu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16321">https://arxiv.org/abs/2511.16321</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16321">https://arxiv.org/pdf/2511.16321</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16321]] WWE-UIE: A Wavelet & White Balance Efficient Network for Underwater Image Enhancement(https://arxiv.org/abs/2511.16321)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Underwater Image Enhancement (UIE) aims to restore visibility and correct color distortions caused by wavelength-dependent absorption and scattering. Recent hybrid approaches, which couple domain priors with modern deep neural architectures, have achieved strong performance but incur high computational cost, limiting their practicality in real-time scenarios. In this work, we propose WWE-UIE, a compact and efficient enhancement network that integrates three interpretable priors. First, adaptive white balance alleviates the strong wavelength-dependent color attenuation, particularly the dominance of blue-green tones. Second, a wavelet-based enhancement block (WEB) performs multi-band decomposition, enabling the network to capture both global structures and fine textures, which are critical for underwater restoration. Third, a gradient-aware module (SGFB) leverages Sobel operators with learnable gating to explicitly preserve edge structures degraded by scattering. Extensive experiments on benchmark datasets demonstrate that WWE-UIE achieves competitive restoration quality with substantially fewer parameters and FLOPs, enabling real-time inference on resource-limited platforms. Ablation studies and visualizations further validate the contribution of each component. The source code is available at this https URL.</li>
<li><strong>摘要：</strong>水下图像增强 (UIE) 旨在恢复可见度并纠正由波长相关吸收和散射引起的颜色失真。最近的混合方法将领域先验与现代深度神经架构结合起来，取得了强大的性能，但会产生高昂的计算成本，限制了它们在实时场景中的实用性。在这项工作中，我们提出了 WWE-UIE，一种紧凑而高效的增强网络，集成了三个可解释的先验。首先，自适应白平衡减轻了与波长相关的强烈颜色衰减，特别是蓝绿色色调的主导地位。其次，基于小波的增强块（WEB）执行多频带分解，使网络能够捕获全局结构和精细纹理，这对于水下恢复至关重要。第三，梯度感知模块（SGFB）利用具有可学习门控的 Sobel 算子来显式保留因散射而退化的边缘结构。对基准数据集的大量实验表明，WWE-UIE 通过大幅减少的参数和 FLOP 实现了具有竞争力的恢复质量，从而能够在资源有限的平台上进行实时推理。消融研究和可视化进一步验证了每个组件的贡献。源代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Beyond Generative AI: World Models for Clinical Prediction, Counterfactuals, and Planning</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Areeb Qazi, Maryam Nadeem, Mohammad Yaqub</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16333">https://arxiv.org/abs/2511.16333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16333">https://arxiv.org/pdf/2511.16333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16333]] Beyond Generative AI: World Models for Clinical Prediction, Counterfactuals, and Planning(https://arxiv.org/abs/2511.16333)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Healthcare requires AI that is predictive, reliable, and data-efficient. However, recent generative models lack physical foundation and temporal reasoning required for clinical decision support. As scaling language models show diminishing returns for grounded clinical reasoning, world models are gaining traction because they learn multimodal, temporally coherent, and action-conditioned representations that reflect the physical and causal structure of care. This paper reviews World Models for healthcare systems that learn predictive dynamics to enable multistep rollouts, counterfactual evaluation and planning. We survey recent work across three domains: (i) medical imaging and diagnostics (e.g., longitudinal tumor simulation, projection-transition modeling, and Joint Embedding Predictive Architecture i.e., JEPA-style predictive representation learning), (ii) disease progression modeling from electronic health records (generative event forecasting at scale), and (iii) robotic surgery and surgical planning (action-conditioned guidance and control). We also introduce a capability rubric: L1 temporal prediction, L2 action-conditioned prediction, L3 counterfactual rollouts for decision support, and L4 planning/control. Most reviewed systems achieve L1--L2, with fewer instances of L3 and rare L4. We identify cross-cutting gaps that limit clinical reliability; under-specified action spaces and safety constraints, weak interventional validation, incomplete multimodal state construction, and limited trajectory-level uncertainty calibration. This review outlines a research agenda for clinically robust prediction-first world models that integrate generative backbones (transformers, diffusion, VAE) with causal/mechanical foundation for safe decision support in healthcare.</li>
<li><strong>摘要：</strong>医疗保健需要具有预测性、可靠且数据高效的人工智能。然而，最近的生成模型缺乏临床决策支持所需的物理基础和时间推理。随着缩放语言模型显示出扎根临床推理的回报递减，世界模型正在获得关注，因为它们学习多模态、时间连贯和动作条件的表示，这些表示反映了护理的物理和因果结构。本文回顾了医疗保健系统的世界模型，这些模型学习预测动态以实现多步部署、反事实评估和规划。我们调查了三个领域的最新工作：（i）医学成像和诊​​断（例如，纵向肿瘤模拟、投影转换建模和联合嵌入预测架构，即 JEPA 式预测表示学习），（ii）电子健康记录中的疾病进展建模（大规模生成事件预测），以及（iii）机器人手术和手术计划（动作条件指导和控制）。我们还引入了一个能力标题：L1 时间预测、L2 动作条件预测、用于决策支持的 L3 反事实推出以及 L4 规划/控制。大多数经过审查的系统都实现了 L1--L2，L3 的实例较少，L4 的实例也很少。我们确定了限制临床可靠性的跨领域差距；指定的动作空间和安全约束不足、干预验证较弱、多模态状态构建不完整以及轨迹级不确定性校准有限。本综述概述了临床稳健的预测优先世界模型的研究议程，该模型将生成主干（变压器、扩散、VAE）与因果/机械基础相结合，以提供医疗保健中的安全决策支持。</li>
</ul>

<h3>Title: Arbitrary-Resolution and Arbitrary-Scale Face Super-Resolution with Implicit Representation Networks</h3>
<ul>
<li><strong>Authors: </strong>Yi Ting Tsai, Yu Wei Chen, Hong-Han Shuai, Ching-Chun Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16341">https://arxiv.org/abs/2511.16341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16341">https://arxiv.org/pdf/2511.16341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16341]] Arbitrary-Resolution and Arbitrary-Scale Face Super-Resolution with Implicit Representation Networks(https://arxiv.org/abs/2511.16341)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Face super-resolution (FSR) is a critical technique for enhancing low-resolution facial images and has significant implications for face-related tasks. However, existing FSR methods are limited by fixed up-sampling scales and sensitivity to input size variations. To address these limitations, this paper introduces an Arbitrary-Resolution and Arbitrary-Scale FSR method with implicit representation networks (ARASFSR), featuring three novel designs. First, ARASFSR employs 2D deep features, local relative coordinates, and up-sampling scale ratios to predict RGB values for each target pixel, allowing super-resolution at any up-sampling scale. Second, a local frequency estimation module captures high-frequency facial texture information to reduce the spectral bias effect. Lastly, a global coordinate modulation module guides FSR to leverage prior facial structure knowledge and achieve resolution adaptation effectively. Quantitative and qualitative evaluations demonstrate the robustness of ARASFSR over existing state-of-the-art methods while super-resolving facial images across various input sizes and up-sampling scales.</li>
<li><strong>摘要：</strong>面部超分辨率（FSR）是增强低分辨率面部图像的关键技术，对面部相关任务具有重要意义。然而，现有的 FSR 方法受到固定上采样尺度和对输入大小变化的敏感性的限制。为了解决这些限制，本文介绍了一种具有隐式表示网络的任意分辨率和任意规模 FSR 方法（ARASFSR），具有三种新颖的设计。首先，ARASFSR 采用 2D 深度特征、局部相对坐标和上采样比例来预测每个目标像素的 RGB 值，从而允许在任何上采样比例下实现超分辨率。其次，局部频率估计模块捕获高频面部纹理信息以减少频谱偏差效应。最后，全局坐标调制模块引导 FSR 利用先前的面部结构知识并有效实现分辨率适应。定量和定性评估证明了 ARASFSR 相对于现有最先进方法的鲁棒性，同时在各种输入尺寸和上采样尺度上实现超分辨率面部图像。</li>
</ul>

<h3>Title: Multi-Order Matching Network for Alignment-Free Depth Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Zhengxue Wang, Zhiqiang Yan, Yuan Wu, Guangwei Gao, Xiang Li, Jian Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16361">https://arxiv.org/abs/2511.16361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16361">https://arxiv.org/pdf/2511.16361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16361]] Multi-Order Matching Network for Alignment-Free Depth Super-Resolution(https://arxiv.org/abs/2511.16361)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Recent guided depth super-resolution methods are premised on the assumption of strictly spatial alignment between depth and RGB, achieving high-quality depth reconstruction. However, in real-world scenarios, the acquisition of strictly aligned RGB-D is hindered by inherent hardware limitations (e.g., physically separate RGB-D sensors) and unavoidable calibration drift induced by mechanical vibrations or temperature variations. Consequently, existing approaches often suffer inevitable performance degradation when applied to misaligned real-world scenes. In this paper, we propose the Multi-Order Matching Network (MOMNet), a novel alignment-free framework that adaptively retrieves and selects the most relevant information from misaligned RGB. Specifically, our method begins with a multi-order matching mechanism, which jointly performs zero-order, first-order, and second-order matching to comprehensively identify RGB information consistent with depth across multi-order feature spaces. To effectively integrate the retrieved RGB and depth, we further introduce a multi-order aggregation composed of multiple structure detectors. This strategy uses multi-order priors as prompts to facilitate the selective feature transfer from RGB to depth. Extensive experiments demonstrate that MOMNet achieves state-of-the-art performance and exhibits outstanding robustness.</li>
<li><strong>摘要：</strong>最近的引导深度超分辨率方法的前提是深度和 RGB 之间严格空间对齐的假设，从而实现高质量的深度重建。然而，在现实场景中，严格对齐的 RGB-D 的获取受到固有硬件限制（例如物理上独立的 RGB-D 传感器）以及机械振动或温度变化引起的不可避免的校准漂移的阻碍。因此，当应用于未对齐的现实场景时，现有方法通常会不可避免地遭受性能下降。在本文中，我们提出了多阶匹配网络（MOMNet），这是一种新颖的免对齐框架，可以从未对齐的 RGB 中自适应地检索和选择最相关的信息。具体来说，我们的方法从多阶匹配机制开始，联合执行零阶、一阶和二阶匹配，以跨多阶特征空间全面识别与深度一致的RGB信息。为了有效地整合检索到的 RGB 和深度，我们进一步引入了由多个结构检测器组成的多阶聚合。该策略使用多阶先验作为提示，以促进从 RGB 到深度的选择性特征转移。大量实验表明 MOMNet 实现了最先进的性能并表现出出色的鲁棒性。</li>
</ul>

<h3>Title: FreqFlow: Long-term forecasting using lightweight flow matching</h3>
<ul>
<li><strong>Authors: </strong>Seyed Mohamad Moghadas, Bruno Cornelis, Adrian Munteanu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16426">https://arxiv.org/abs/2511.16426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16426">https://arxiv.org/pdf/2511.16426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16426]] FreqFlow: Long-term forecasting using lightweight flow matching(https://arxiv.org/abs/2511.16426)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Multivariate time-series (MTS) forecasting is fundamental to applications ranging from urban mobility and resource management to climate modeling. While recent generative models based on denoising diffusion have advanced state-of-the-art performance in capturing complex data distributions, they suffer from significant computational overhead due to iterative stochastic sampling procedures that limit real-time deployment. Moreover, these models can be brittle when handling high-dimensional, non-stationary, and multi-scale periodic patterns characteristic of real-world sensor networks. We introduce FreqFlow, a novel framework that leverages conditional flow matching in the frequency domain for deterministic MTS forecasting. Unlike conventional approaches that operate in the time domain, FreqFlow transforms the forecasting problem into the spectral domain, where it learns to model amplitude and phase shifts through a single complex-valued linear layer. This frequency-domain formulation enables the model to efficiently capture temporal dynamics via complex multiplication, corresponding to scaling and temporal translations. The resulting architecture is exceptionally lightweight with only 89k parameters - an order of magnitude smaller than competing diffusion-based models-while enabling single-pass deterministic sampling through ordinary differential equation (ODE) integration. Our approach decomposes MTS signals into trend, seasonal, and residual components, with the flow matching mechanism specifically designed for residual learning to enhance long-term forecasting accuracy. Extensive experiments on real-world traffic speed, volume, and flow datasets demonstrate that FreqFlow achieves state-of-the-art forecasting performance, on average 7\% RMSE improvements, while being significantly faster and more parameter-efficient than existing methods</li>
<li><strong>摘要：</strong>多元时间序列 (MTS) 预测是城市交通、资源管理和气候建模等应用的基础。虽然最近基于去噪扩散的生成模型在捕获复杂数据分布方面具有最先进的性能，但由于限制实时部署的迭代随机采样过程，它们承受着巨大的计算开销。此外，在处理现实世界传感器网络的高维、非平稳和多尺度周期性模式特征时，这些模型可能很脆弱。我们介绍 FreqFlow，这是一种利用频域条件流匹配进行确定性 MTS 预测的新颖框架。与在时域中运行的传统方法不同，FreqFlow 将预测问题转换到谱域中，在谱域中它学习通过单个复值线性层对幅度和相移进行建模。这种频域公式使模型能够通过复杂的乘法有效地捕获时间动态，对应于缩放和平移。由此产生的架构非常轻量级，只有 89k 个参数，比竞争的基于扩散的模型小一个数量级，同时通过常微分方程 (ODE) 积分实现单通道确定性采样。我们的方法将 MTS 信号分解为趋势、季节和残差分量，并采用专为残差学习设计的流量匹配机制，以提高长期预测准确性。对现实世界交通速度、流量和流量数据集的大量实验表明，FreqFlow 实现了最先进的预测性能，平均 RMSE 提高了 7\%，同时比现有方法速度更快、参数效率更高</li>
</ul>

<h3>Title: Generative Modeling of Clinical Time Series via Latent Stochastic Differential Equations</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Aslanimoghanloo, Ahmed ElGazzar, Marcel van Gerven</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16427">https://arxiv.org/abs/2511.16427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16427">https://arxiv.org/pdf/2511.16427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16427]] Generative Modeling of Clinical Time Series via Latent Stochastic Differential Equations(https://arxiv.org/abs/2511.16427)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Clinical time series data from electronic health records and medical registries offer unprecedented opportunities to understand patient trajectories and inform medical decision-making. However, leveraging such data presents significant challenges due to irregular sampling, complex latent physiology, and inherent uncertainties in both measurements and disease progression. To address these challenges, we propose a generative modeling framework based on latent neural stochastic differential equations (SDEs) that views clinical time series as discrete-time partial observations of an underlying controlled stochastic dynamical system. Our approach models latent dynamics via neural SDEs with modality-dependent emission models, while performing state estimation and parameter learning through variational inference. This formulation naturally handles irregularly sampled observations, learns complex non-linear interactions, and captures the stochasticity of disease progression and measurement noise within a unified scalable probabilistic framework. We validate the framework on two complementary tasks: (i) individual treatment effect estimation using a simulated pharmacokinetic-pharmacodynamic (PKPD) model of lung cancer, and (ii) probabilistic forecasting of physiological signals using real-world intensive care unit (ICU) data from 12,000 patients. Results show that our framework outperforms ordinary differential equation and long short-term memory baseline models in accuracy and uncertainty estimation. These results highlight its potential for enabling precise, uncertainty-aware predictions to support clinical decision-making.</li>
<li><strong>摘要：</strong>来自电子健康记录和医疗登记的临床时间序列数据为了解患者轨迹和为医疗决策提供信息提供了前所未有的机会。然而，由于采样不规则、潜在生理学复杂以及测量和疾病进展固有的不确定性，利用这些数据带来了巨大的挑战。为了应对这些挑战，我们提出了一种基于潜在神经随机微分方程（SDE）的生成建模框架，该框架将临床时间序列视为底层受控随机动力系统的离散时间部分观察。我们的方法通过具有模态相关发射模型的神经 SDE 来模拟潜在动态，同时通过变分推理执行状态估计和参数学习。该公式自然地处理不规则采样的观察结果，学习复杂的非线性相互作用，并在统一的可扩展概率框架内捕获疾病进展的随机性和测量噪声。我们在两项补充任务上验证了该框架：(i) 使用肺癌的模拟药代动力学-药效 (PKPD) 模型估计个体治疗效果，以及 (ii) 使用来自 12,000 名患者的真实重症监护病房 (ICU) 数据对生理信号进行概率预测。结果表明，我们的框架在准确性和不确定性估计方面优于常微分方程和长短期记忆基线模型。这些结果凸显了其实现精确、不确定性预测以支持临床决策的潜力。</li>
</ul>

<h3>Title: VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference</h3>
<ul>
<li><strong>Authors: </strong>Ziyan Liu, Yeqiu Chen, Hongyi Cai, Tao Lin, Shuo Yang, Zheng Liu, Bo Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16449">https://arxiv.org/abs/2511.16449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16449">https://arxiv.org/pdf/2511.16449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16449]] VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference(https://arxiv.org/abs/2511.16449)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision-Language-Action (VLA) models have shown great promise for embodied AI, yet the heavy computational cost of processing continuous visual streams severely limits their real-time deployment. Token pruning (keeping salient visual tokens and dropping redundant ones) has emerged as an effective approach for accelerating Vision-Language Models (VLMs), offering a solution for efficient VLA. However, these VLM-specific token pruning methods select tokens based solely on semantic salience metrics (e.g., prefill attention), while overlooking the VLA's intrinsic dual-system nature of high-level semantic understanding and low-level action execution. Consequently, these methods bias token retention toward semantic cues, discard critical information for action generation, and significantly degrade VLA performance. To bridge this gap, we propose VLA-Pruner, a versatile plug-and-play VLA-specific token prune method that aligns with the dual-system nature of VLA models and exploits the temporal continuity in robot manipulation. Specifically, VLA-Pruner adopts a dual-level importance criterion for visual token retention: vision-language prefill attention for semantic-level relevance and action decode attention, estimated via temporal smoothing, for action-level importance. Based on this criterion, VLA-Pruner proposes a novel dual-level token selection strategy that adaptively preserves a compact, informative set of visual tokens for both semantic understanding and action execution under given compute budget. Experiments show that VLA-Pruner achieves state-of-the-art performance across multiple VLA architectures and diverse robotic tasks.</li>
<li><strong>摘要：</strong>视觉-语言-动作（VLA）模型在嵌入式人工智能方面展现出了巨大的前景，但处理连续视觉流的繁重计算成本严重限制了它们的实时部署。令牌修剪（保留显着的视觉令牌并删除冗余的令牌）已成为加速视觉语言模型 (VLM) 的有效方法，为高效的 VLA 提供了解决方案。然而，这些特定于 VLM 的标记修剪方法仅基于语义显着性指标（例如预填充注意力）来选择标记，而忽略了 VLA 的高级语义理解和低级动作执行的内在双系统性质。因此，这些方法将令牌保留偏向于语义线索，丢弃了动作生成的关键信息，并显着降低了 VLA 性能。为了弥补这一差距，我们提出了 VLA-Pruner，这是一种多功能的即插即用 VLA 特定令牌修剪方法，它符合 VLA 模型的双系统性质，并利用机器人操作的时间连续性。具体来说，VLA-Pruner 采用双级重要性标准来保留视觉标记：用于语义级相关性的视觉语言预填充注意力和通过时间平滑估计的动作解码注意力，用于动作级重要性。基于这一标准，VLA-Pruner 提出了一种新颖的双层令牌选择策略，该策略自适应地保留一组紧凑的、信息丰富的视觉令牌，以便在给定的计算预算下实现语义理解和动作执行。实验表明，VLA-Pruner 在多个 VLA 架构和不同的机器人任务中实现了最先进的性能。</li>
</ul>

<h3>Title: Physics-Informed Machine Learning for Efficient Sim-to-Real Data Augmentation in Micro-Object Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Zongcai Tan, Lan Wei, Dandan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16494">https://arxiv.org/abs/2511.16494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16494">https://arxiv.org/pdf/2511.16494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16494]] Physics-Informed Machine Learning for Efficient Sim-to-Real Data Augmentation in Micro-Object Pose Estimation(https://arxiv.org/abs/2511.16494)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Precise pose estimation of optical microrobots is essential for enabling high-precision object tracking and autonomous biological studies. However, current methods rely heavily on large, high-quality microscope image datasets, which are difficult and costly to acquire due to the complexity of microrobot fabrication and the labour-intensive labelling. Digital twin systems offer a promising path for sim-to-real data augmentation, yet existing techniques struggle to replicate complex optical microscopy phenomena, such as diffraction artifacts and depth-dependent this http URL work proposes a novel physics-informed deep generative learning framework that, for the first time, integrates wave optics-based physical rendering and depth alignment into a generative adversarial network (GAN), to synthesise high-fidelity microscope images for microrobot pose estimation efficiently. Our method improves the structural similarity index (SSIM) by 35.6% compared to purely AI-driven methods, while maintaining real-time rendering speeds (0.022 s/frame).The pose estimator (CNN backbone) trained on our synthetic data achieves 93.9%/91.9% (pitch/roll) accuracy, just 5.0%/5.4% (pitch/roll) below that of an estimator trained exclusively on real data. Furthermore, our framework generalises to unseen poses, enabling data augmentation and robust pose estimation for novel microrobot configurations without additional training data.</li>
<li><strong>摘要：</strong>光学微型机器人的精确姿态估计对于实现高精度目标跟踪和自主生物学研究至关重要。然而，当前的方法严重依赖于大型、高质量的显微镜图像数据集，由于微型机器人制造的复杂性和劳动密集型标签，获取这些数据集非常困难且成本高昂。数字孪生系统为模拟到真实的数据增强提供了一条有前途的道路，但现有技术难以复制复杂的光学显微镜现象，例如衍射伪影和深度依赖性。这项http URL工作提出了一种新颖的基于物理的深度生成学习框架，该框架首次将基于波动光学的物理渲染和深度对齐集成到生成对抗网络（GAN）中，以有效地合成用于微型机器人姿态估计的高保真显微镜图像。与纯人工智能驱动的方法相比，我们的方法将结构相似性指数 (SSIM) 提高了 35.6%，同时保持实时渲染速度（0.022 秒/帧）。在我们的合成数据上训练的姿态估计器（CNN 主干）实现了 93.9%/91.9%（俯仰/滚动）精度，仅比仅在真实数据上训练的估计器低 5.0%/5.4%（俯仰/滚动）。此外，我们的框架泛化到看不见的姿势，无需额外的训练数据即可对新颖的微型机器人配置进行数据增强和稳健的姿势估计。</li>
</ul>

<h3>Title: Saving Foundation Flow-Matching Priors for Inverse Problems</h3>
<ul>
<li><strong>Authors: </strong>Yuxiang Wan, Ryan Devera, Wenjie Zhang, Ju Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16520">https://arxiv.org/abs/2511.16520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16520">https://arxiv.org/pdf/2511.16520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16520]] Saving Foundation Flow-Matching Priors for Inverse Problems(https://arxiv.org/abs/2511.16520)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Foundation flow-matching (FM) models promise a universal prior for solving inverse problems (IPs), yet today they trail behind domain-specific or even untrained priors. How can we unlock their potential? We introduce FMPlug, a plug-in framework that redefines how foundation FMs are used in IPs. FMPlug combines an instance-guided, time-dependent warm-start strategy with a sharp Gaussianity regularization, adding problem-specific guidance while preserving the Gaussian structures. This leads to a significant performance boost across image restoration and scientific IPs. Our results point to a path for making foundation FM models practical, reusable priors for IP solving.</li>
<li><strong>摘要：</strong>基础流匹配（FM）模型承诺为解决逆问题（IP）提供通用先验，但如今它们落后于特定领域甚至未经训练的先验。我们如何才能释放他们的潜力？我们推出了 FMPlug，这是一个插件框架，它重新定义了基础 FM 在 IP 中的使用方式。 FMPlug 将实例引导、时间相关的热启动策略与尖锐的高斯正则化相结合，在保留高斯结构的同时添加针对特定问题的指导。这使得图像恢复和科学 IP 的性能显着提升。我们的结果指出了一条使基础 FM 模型实用、可重用先验用于 IP 求解的途径。</li>
</ul>

<h3>Title: Dynamic Participation in Federated Learning: Benchmarks and a Knowledge Pool Plugin</h3>
<ul>
<li><strong>Authors: </strong>Ming-Lun Lee, Fu-Shiang Yang, Cheng-Kuan Lin, Yan-Ann Chen, Chih-Yu Lin, Yu-Chee Tseng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16523">https://arxiv.org/abs/2511.16523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16523">https://arxiv.org/pdf/2511.16523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16523]] Dynamic Participation in Federated Learning: Benchmarks and a Knowledge Pool Plugin(https://arxiv.org/abs/2511.16523)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) enables clients to collaboratively train a shared model in a distributed manner, setting it apart from traditional deep learning paradigms. However, most existing FL research assumes consistent client participation, overlooking the practical scenario of dynamic participation (DPFL), where clients may intermittently join or leave during training. Moreover, no existing benchmarking framework systematically supports the study of DPFL-specific challenges. In this work, we present the first open-source framework explicitly designed for benchmarking FL models under dynamic client participation. Our framework provides configurable data distributions, participation patterns, and evaluation metrics tailored to DPFL scenarios. Using this platform, we benchmark four major categories of widely adopted FL models and uncover substantial performance degradation under dynamic participation. To address these challenges, we further propose Knowledge-Pool Federated Learning (KPFL), a generic plugin that maintains a shared knowledge pool across both active and idle clients. KPFL leverages dual-age and data-bias weighting, combined with generative knowledge distillation, to mitigate instability and prevent knowledge loss. Extensive experiments demonstrate the significant impact of dynamic participation on FL performance and the effectiveness of KPFL in improving model robustness and generalization.</li>
<li><strong>摘要：</strong>联邦学习 (FL) 使客户能够以分布式方式协作训练共享模型，使其有别于传统的深度学习范例。然而，大多数现有的 FL 研究假设客户参与一致，忽视了动态参与 (DPFL) 的实际场景，即客户可能在培训期间间歇性地加入或离开。此外，没有现有的基准测试框架系统地支持 DPFL 特定挑战的研究。在这项工作中，我们提出了第一个专门为动态客户参与下的 FL 模型基准测试而设计的开源框架。我们的框架提供了针对 DPFL 场景定制的可配置数据分布、参与模式和评估指标。使用这个平台，我们对广泛采用的 FL 模型的四大类进行了基准测试，并发现动态参与下的显着性能下降。为了应对这些挑战，我们进一步提出知识池联邦学习（KPFL），这是一个通用插件，可以在活跃和空闲客户端之间维护共享知识池。 KPFL 利用双年龄和数据偏差加权，结合生成知识蒸馏，来减轻不稳定性并防止知识丢失。大量实验证明了动态参与对 FL 性能的显着影响以及 KPFL 在提高模型鲁棒性和泛化性方面的有效性。</li>
</ul>

<h3>Title: Supervised Contrastive Learning for Few-Shot AI-Generated Image Detection and Attribution</h3>
<ul>
<li><strong>Authors: </strong>Jaime Álvarez Urueña, David Camacho, Javier Huertas Tato</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16541">https://arxiv.org/abs/2511.16541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16541">https://arxiv.org/pdf/2511.16541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16541]] Supervised Contrastive Learning for Few-Shot AI-Generated Image Detection and Attribution(https://arxiv.org/abs/2511.16541)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of generative artificial intelligence has enabled the creation of synthetic images that are increasingly indistinguishable from authentic content, posing significant challenges for digital media integrity. This problem is compounded by the accelerated release cycle of novel generative models, which renders traditional detection approaches (reliant on periodic retraining) computationally infeasible and operationally impractical. This work proposes a novel two-stage detection framework designed to address the generalization challenge inherent in synthetic image detection. The first stage employs a vision deep learning model trained via supervised contrastive learning to extract discriminative embeddings from input imagery. Critically, this model was trained on a strategically partitioned subset of available generators, with specific architectures withheld from training to rigorously ablate cross-generator generalization capabilities. The second stage utilizes a k-nearest neighbors (k-NN) classifier operating on the learned embedding space, trained in a few-shot learning paradigm incorporating limited samples from previously unseen test generators. With merely 150 images per class in the few-shot learning regime, which are easily obtainable from current generation models, the proposed framework achieves an average detection accuracy of 91.3\%, representing a 5.2 percentage point improvement over existing approaches . For the source attribution task, the proposed approach obtains improvements of of 14.70\% and 4.27\% in AUC and OSCR respectively on an open set classification context, marking a significant advancement toward robust, scalable forensic attribution systems capable of adapting to the evolving generative AI landscape without requiring exhaustive retraining protocols.</li>
<li><strong>摘要：</strong>生成人工智能的快速发展使得合成图像的创建与真实内容越来越难以区分，这对数字媒体的完整性提出了重大挑战。这个问题因新型生成模型的加速发布周期而变得更加复杂，这使得传统的检测方法（依赖于定期再训练）在计算上不可行且在操作上不切实际。这项工作提出了一种新颖的两阶段检测框架，旨在解决合成图像检测中固有的泛化挑战。第一阶段采用通过监督对比学习训练的视觉深度学习模型，从输入图像中提取有区别的嵌入。至关重要的是，该模型是在可用生成器的战略分区子集上进行训练的，并且不进行特定架构的训练，以严格消除跨生成器泛化能力。第二阶段利用在学习的嵌入空间上运行的 k-近邻 (k-NN) 分类器，在几次学习范式中进行训练，其中包含来自以前未见过的测试生成器的有限样本。在少镜头学习机制中，每类仅 150 个图像（很容易从当前生成的模型中获得），所提出的框架实现了 91.3% 的平均检测精度，比现有方法提高了 5.2 个百分点。对于源归因任务，所提出的方法在开放集分类上下文中的 AUC 和 OSCR 分别获得了 14.70\% 和 4.27\% 的改进，标志着向稳健、可扩展的取证归因系统迈出了重大进步，该系统能够适应不断发展的生成 AI 环境，而无需详尽的再训练协议。</li>
</ul>

<h3>Title: Progressive Supernet Training for Efficient Visual Autoregressive Modeling</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyue Chen, Yuling Shi, Kaiyuan Li, Huandong Wang, Yong Li, Xiaodong Gu, Xinlei Chen, Mingbao Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16546">https://arxiv.org/abs/2511.16546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16546">https://arxiv.org/pdf/2511.16546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16546]] Progressive Supernet Training for Efficient Visual Autoregressive Modeling(https://arxiv.org/abs/2511.16546)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Visual Auto-Regressive (VAR) models significantly reduce inference steps through the "next-scale" prediction paradigm. However, progressive multi-scale generation incurs substantial memory overhead due to cumulative KV caching, limiting practical deployment. We observe a scale-depth asymmetric dependency in VAR: early scales exhibit extreme sensitivity to network depth, while later scales remain robust to depth reduction. Inspired by this, we propose VARiant: by equidistant sampling, we select multiple subnets ranging from 16 to 2 layers from the original 30-layer VAR-d30 network. Early scales are processed by the full network, while later scales utilize subnet. Subnet and the full network share weights, enabling flexible depth adjustment within a single model. However, weight sharing between subnet and the entire network can lead to optimization conflicts. To address this, we propose a progressive training strategy that breaks through the Pareto frontier of generation quality for both subnets and the full network under fixed-ratio training, achieving joint optimality. Experiments on ImageNet demonstrate that, compared to the pretrained VAR-d30 (FID 1.95), VARiant-d16 and VARiant-d8 achieve nearly equivalent quality (FID 2.05/2.12) while reducing memory consumption by 40-65%. VARiant-d2 achieves 3.5 times speedup and 80% memory reduction at moderate quality cost (FID 2.97). In terms of deployment, VARiant's single-model architecture supports zero-cost runtime depth switching and provides flexible deployment options from high quality to extreme efficiency, catering to diverse application scenarios.</li>
<li><strong>摘要：</strong>视觉自回归 (VAR) 模型通过“下一个规模”预测范式显着减少推理步骤。然而，由于累积的 KV 缓存，渐进式多尺度生成会产生大量内存开销，限制了实际部署。我们观察到 VAR 中的尺度深度不对称依赖性：早期尺度对网络深度表现出极其敏感，而后期尺度对深度缩减仍然保持鲁棒性。受此启发，我们提出VARiant：通过等距采样，从原始30层VAR-d30网络中选择16至2层的多个子网。早期的规模由整个网络处理，而后来的规模则利用子网。子网和全网络共享权重，从而在单个模型内实现灵活的深度调整。然而，子网和整个网络之间的权重共享可能会导致优化冲突。为了解决这个问题，我们提出了一种渐进式训练策略，在固定比率训练下突破子网和全网络生成质量的帕累托前沿，实现联合最优。 ImageNet 上的实验表明，与预训练的 VAR-d30 (FID 1.95) 相比，VARiant-d16 和 VARiant-d8 实现了几乎相同的质量 (FID 2.05/2.12)，同时减少了 40-65% 的内存消耗。 VARiant-d2 以中等质量成本实现了 3.5 倍的加速和 80% 的内存减少 (FID 2.97)。在部署方面，VARiant的单模型架构支持零成本运行时深度切换，并提供从高质量到极致效率的灵活部署选项，满足多样化的应用场景。</li>
</ul>

<h3>Title: Toward Valid Generative Clinical Trial Data with Survival Endpoints</h3>
<ul>
<li><strong>Authors: </strong>Perrine Chassat, Van Tuan Nguyen, Lucas Ducrot, Emilie Lanoy, Agathe Guilloux</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP, stat.ME, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16551">https://arxiv.org/abs/2511.16551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16551">https://arxiv.org/pdf/2511.16551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16551]] Toward Valid Generative Clinical Trial Data with Survival Endpoints(https://arxiv.org/abs/2511.16551)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Clinical trials face mounting challenges: fragmented patient populations, slow enrollment, and unsustainable costs, particularly for late phase trials in oncology and rare diseases. While external control arms built from real-world data have been explored, a promising alternative is the generation of synthetic control arms using generative AI. A central challenge is the generation of time-to-event outcomes, which constitute primary endpoints in oncology and rare disease trials, but are difficult to model under censoring and small sample sizes. Existing generative approaches, largely GAN-based, are data-hungry, unstable, and rely on strong assumptions such as independent censoring. We introduce a variational autoencoder (VAE) that jointly generates mixed-type covariates and survival outcomes within a unified latent variable framework, without assuming independent censoring. Across synthetic and real trial datasets, we evaluate our model in two realistic scenarios: (i) data sharing under privacy constraints, where synthetic controls substitute for original data, and (ii) control-arm augmentation, where synthetic patients mitigate imbalances between treated and control groups. Our method outperforms GAN baselines on fidelity, utility, and privacy metrics, while revealing systematic miscalibration of type I error and power. We propose a post-generation selection procedure that improves calibration, highlighting both progress and open challenges for generative survival modeling.</li>
<li><strong>摘要：</strong>临床试验面临着越来越多的挑战：患者群体分散、入组缓慢和成本不可持续，特别是对于肿瘤学和罕见疾病的后期试验。虽然已经探索了根据现实世界数据构建的外部控制臂，但一个有前途的替代方案是使用生成人工智能生成合成控制臂。一个核心挑战是生成事件发生时间结果，它构成肿瘤学和罕见疾病试验的主要终点，但在审查和小样本量下很难建模。现有的生成方法主要基于 GAN，需要大量数据、不稳定，并且依赖于独立审查等强有力的假设。我们引入了一种变分自动编码器（VAE），它在统一的潜在变量框架内联合生成混合类型的协变量和生存结果，而不假设独立审查。在合成和真实试验数据集中，我们在两个现实场景中评估我们的模型：（i）隐私约束下的数据共享，其中合成控制替代原始数据，以及（ii）控制臂增强，其中合成患者减轻治疗组和对照组之间的不平衡。我们的方法在保真度、效用和隐私指标方面优于 GAN 基线，同时揭示了 I 类错误和功率的系统性错误校准。我们提出了一种改进校准的后生成选择程序，突出了生成生存模型的进展和开放挑战。</li>
</ul>

<h3>Title: Boosting Predictive Performance on Tabular Data through Data Augmentation with Latent-Space Flow-Based Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Md. Tawfique Ihsan, Md. Rakibul Hasan Rafi, Ahmed Shoyeb Raihan, Imtiaz Ahmed, Abdullahil Azeem</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16571">https://arxiv.org/abs/2511.16571</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16571">https://arxiv.org/pdf/2511.16571</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16571]] Boosting Predictive Performance on Tabular Data through Data Augmentation with Latent-Space Flow-Based Diffusion(https://arxiv.org/abs/2511.16571)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Severe class imbalance is common in real-world tabular learning, where rare but important minority classes are essential for reliable prediction. Existing generative oversampling methods such as GANs, VAEs, and diffusion models can improve minority-class performance, but they often struggle with tabular heterogeneity, training stability, and privacy concerns. We propose a family of latent-space, tree-driven diffusion methods for minority oversampling that use conditional flow matching with gradient-boosted trees as the vector-field learner. The models operate in compact latent spaces to preserve tabular structure and reduce computation. We introduce three variants: PCAForest, which uses linear PCA embedding; EmbedForest, which uses a learned nonlinear embedding; and AttentionForest, which uses an attention-augmented embedding. Each method couples a GBT-based flow with a decoder back to the original feature space. Across 11 datasets from healthcare, finance, and manufacturing, AttentionForest achieves the best average minority recall while maintaining competitive precision, calibration, and distributional similarity. PCAForest and EmbedForest reach similar utility with much faster generation, offering favorable accuracy-efficiency trade-offs. Privacy evaluated with nearest-neighbor distance ratio and distance-to-closest-record is comparable to or better than the ForestDiffusion baseline. Ablation studies show that smaller embeddings tend to improve minority recall, while aggressive learning rates harm stability. Overall, latent-space, tree-driven diffusion provides an efficient and privacy-aware approach to high-fidelity tabular data augmentation under severe class imbalance.</li>
<li><strong>摘要：</strong>严重的类别不平衡在现实世界的表格学习中很常见，其中罕见但重要的少数类别对于可靠的预测至关重要。现有的生成式过采样方法（例如 GAN、VAE 和扩散模型）可以提高少数类别的性能，但它们经常会遇到表格异质性、训练稳定性和隐私问题。我们提出了一系列用于少数过采样的潜在空间、树驱动的扩散方法，该方法使用条件流匹配和梯度增强树作为矢量场学习器。该模型在紧凑的潜在空间中运行，以保留表格结构并减少计算。我们介绍三种变体：PCAForest，它使用线性 PCA 嵌入； EmbedForest，它使用学习的非线性嵌入；和 AttentionForest，它使用注意力增强嵌入。每种方法都将基于 GBT 的流与解码器耦合回原始特征空间。在来自医疗保健、金融和制造的 11 个数据集中，AttentionForest 实现了最佳的平均少数族裔召回率，同时保持了有竞争力的精度、校准和分布相似性。 PCAForest 和 EmbedForest 以更快的生成速度达到类似的效用，提供有利的准确性与效率权衡。使用最近邻距离比和最近记录距离评估的隐私与 ForestDiffusion 基线相当或更好。消融研究表明，较小的嵌入往往会提高少数人的召回率，而激进的学习率会损害稳定性。总体而言，潜在空间、树驱动的扩散为严重类别不平衡下的高保真表格数据增强提供了一种有效且具有隐私意识的方法。</li>
</ul>

<h3>Title: Erase to Retain: Low Rank Adaptation Guided Selective Unlearning in Medical Segmentation Networks</h3>
<ul>
<li><strong>Authors: </strong>Nirjhor Datta, Md. Golam Rabiul Alam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16574">https://arxiv.org/abs/2511.16574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16574">https://arxiv.org/pdf/2511.16574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16574]] Erase to Retain: Low Rank Adaptation Guided Selective Unlearning in Medical Segmentation Networks(https://arxiv.org/abs/2511.16574)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>The ability to selectively remove knowledge from medical segmentation networks is increasingly important for privacy compliance, ethical deployment, and continual dataset revision. We introduce Erase to Retain, a controllable unlearning framework for medical image segmentation that achieves targeted forgetting without full retraining. Our method uses a teacher-student distillation paradigm with Low-Rank Adaptation (LoRA) constrained subspace updates, enabling the student network to erase lesion-specific or class-specific representations in low-rank decoder spaces while preserving global anatomical understanding. During the strong unlearning phase, LoRA modules are adversarially optimized to contradict the teacher's confident predictions on a designated forget subset, enforcing semantic removal. This is followed by a gentle restoration phase that recovers generalization on retained data through head-only supervised refinement. For ISIC segmentation, the student reduces forget-set IoU from 0.875 to 0.509 while maintaining competitive performance on the retain and validation splits (0.647 to 0.677 IoU). On the cross-domain CHASE dataset, Erase to Retain consistently lowers forget-set IoU while preserving utility on retain and validation sets. For ISIC classification, our method decreases accuracy on the forget subset from 87.0 percent to 64.1 percent while improving retain accuracy from 83.9 percent to 90.6 percent. These results demonstrate that LoRA-based subspace unlearning provides a practical pathway toward responsible, controllable, and reversible unlearning in medical image analysis, enabling models to forget sensitive samples or structures while preserving performance where it matters most.</li>
<li><strong>摘要：</strong>有选择地从医学分割网络中删除知识的能力对于隐私合规性、道德部署和持续的数据集修订变得越来越重要。我们引入了 Erase to Retain，这是一种用于医学图像分割的可控遗忘框架，无需完全重新训练即可实现有针对性的遗忘。我们的方法使用具有低秩适应（LoRA）约束子空间更新的师生蒸馏范式，使学生网络能够擦除低秩解码器空间中特定于病变或特定类别的表示，同时保留全局解剖学理解。在强遗忘阶段，LoRA 模块进行对抗性优化，以与教师对指定遗忘子集的置信预测相矛盾，从而强制删除语义。接下来是温和的恢复阶段，通过仅头部的监督细化恢复保留数据的泛化。对于 ISIC 分割，学生将遗忘集 IoU 从 0.875 减少到 0.509，同时保持保留和验证分割的竞争性能（0.647 到 0.677 IoU）。在跨域 CHASE 数据集上，Erase to Retain 始终降低忘记集 IoU，同时保留保留集和验证集的实用性。对于 ISIC 分类，我们的方法将遗忘子集的准确率从 87.0% 降低到 64.1%，同时将保留准确率从 83.9% 提高到 90.6%。这些结果表明，基于 LoRA 的子空间遗忘提供了一条在医学图像分析中实现负责任、可控和可逆遗忘的实用途径，使模型能够忘记敏感样本或结构，同时保留最重要的性能。</li>
</ul>

<h3>Title: gfnx: Fast and Scalable Library for Generative Flow Networks in JAX</h3>
<ul>
<li><strong>Authors: </strong>Daniil Tiapkin, Artem Agarkov, Nikita Morozov, Ian Maksimov, Askar Tsyganov, Timofei Gritsaev, Sergey Samsonov</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16592">https://arxiv.org/abs/2511.16592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16592">https://arxiv.org/pdf/2511.16592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16592]] gfnx: Fast and Scalable Library for Generative Flow Networks in JAX(https://arxiv.org/abs/2511.16592)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In this paper, we present gfnx, a fast and scalable package for training and evaluating Generative Flow Networks (GFlowNets) written in JAX. gfnx provides an extensive set of environments and metrics for benchmarking, accompanied with single-file implementations of core objectives for training GFlowNets. We include synthetic hypergrids, multiple sequence generation environments with various editing regimes and particular reward designs for molecular generation, phylogenetic tree construction, Bayesian structure learning, and sampling from the Ising model energy. Across different tasks, gfnx achieves significant wall-clock speedups compared to Pytorch-based benchmarks (such as torchgfn library) and author implementations. For example, gfnx achieves up to 55 times speedup on CPU-based sequence generation environments, and up to 80 times speedup with the GPU-based Bayesian network structure learning setup. Our package provides a diverse set of benchmarks and aims to standardize empirical evaluation and accelerate research and applications of GFlowNets. The library is available on GitHub (this https URL) and on pypi (this https URL). Documentation is available on this https URL.</li>
<li><strong>摘要：</strong>在本文中，我们提出了 gfnx，这是一个快速且可扩展的包，用于训练和评估用 JAX 编写的生成流网络 (GFlowNets)。 gfnx 提供了一组广泛的基准测试环境和指标，以及用于训练 GFlowNet 的核心目标的单文件实现。我们包括合成超网格、具有各种编辑机制的多序列生成环境以及针对分子生成、系统发育树构建、贝叶斯结构学习和伊辛模型能量采样的特殊奖励设计。在不同的任务中，与基于 Pytorch 的基准测试（例如 torchgfn 库）和作者实现相比，gfnx 实现了显着的挂钟加速。例如，gfnx 在基于 CPU 的序列生成环境中实现了高达 55 倍的加速，在基于 GPU 的贝叶斯网络结构学习设置中实现了高达 80 倍的加速。我们的软件包提供了一组多样化的基准，旨在标准化实证评估并加速 GFlowNets 的研究和应用。该库可在 GitHub（此 https URL）和 pypi（此 https URL）上获取。此 https URL 上提供了文档。</li>
</ul>

<h3>Title: Generative AI for Enhanced Wildfire Detection: Bridging the Synthetic-Real Domain Gap</h3>
<ul>
<li><strong>Authors: </strong>Satyam Gaba</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16617">https://arxiv.org/abs/2511.16617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16617">https://arxiv.org/pdf/2511.16617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16617]] Generative AI for Enhanced Wildfire Detection: Bridging the Synthetic-Real Domain Gap(https://arxiv.org/abs/2511.16617)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The early detection of wildfires is a critical environmental challenge, with timely identification of smoke plumes being key to mitigating large-scale damage. While deep neural networks have proven highly effective for localization tasks, the scarcity of large, annotated datasets for smoke detection limits their potential. In response, we leverage generative AI techniques to address this data limitation by synthesizing a comprehensive, annotated smoke dataset. We then explore unsupervised domain adaptation methods for smoke plume segmentation, analyzing their effectiveness in closing the gap between synthetic and real-world data. To further refine performance, we integrate advanced generative approaches such as style transfer, Generative Adversarial Networks (GANs), and image matting. These methods aim to enhance the realism of synthetic data and bridge the domain disparity, paving the way for more accurate and scalable wildfire detection models.</li>
<li><strong>摘要：</strong>及早发现野火是一项严峻的环境挑战，及时识别烟羽是减轻大规模损失的关键。虽然深度神经网络已被证明对于定位任务非常有效，但用于烟雾检测的大型带注释数据集的稀缺限制了它们的潜力。作为回应，我们利用生成式人工智能技术，通过合成一个全面的、带注释的烟雾数据集来解决这一数据限制。然后，我们探索用于烟雾羽流分割的无监督域适应方法，分析其在缩小合成数据与真实数据之间差距方面的有效性。为了进一步提高性能，我们集成了先进的生成方法，例如风格迁移、生成对抗网络 (GAN) 和图像抠图。这些方法旨在增强合成数据的真实性并弥合领域差异，为更准确和可扩展的野火检测模型铺平道路。</li>
</ul>

<h3>Title: SAM 3D: 3Dfy Anything in Images</h3>
<ul>
<li><strong>Authors: </strong>SAM 3D Team, Xingyu Chen, Fu-Jen Chu, Pierre Gleize, Kevin J Liang, Alexander Sax, Hao Tang, Weiyao Wang, Michelle Guo, Thibaut Hardin, Xiang Li, Aohan Lin, Jiawei Liu, Ziqi Ma, Anushka Sagar, Bowen Song, Xiaodong Wang, Jianing Yang, Bowen Zhang, Piotr Dollár, Georgia Gkioxari, Matt Feiszli, Jitendra Malik</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16624">https://arxiv.org/abs/2511.16624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16624">https://arxiv.org/pdf/2511.16624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16624]] SAM 3D: 3Dfy Anything in Images(https://arxiv.org/abs/2511.16624)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting geometry, texture, and layout from a single image. SAM 3D excels in natural images, where occlusion and scene clutter are common and visual recognition cues from context play a larger role. We achieve this with a human- and model-in-the-loop pipeline for annotating object shape, texture, and pose, providing visually grounded 3D reconstruction data at unprecedented scale. We learn from this data in a modern, multi-stage training framework that combines synthetic pretraining with real-world alignment, breaking the 3D "data barrier". We obtain significant gains over recent work, with at least a 5:1 win rate in human preference tests on real-world objects and scenes. We will release our code and model weights, an online demo, and a new challenging benchmark for in-the-wild 3D object reconstruction.</li>
<li><strong>摘要：</strong>我们提出了 SAM 3D，这是一种用于基于视觉的 3D 对象重建的生成模型，可从单个图像预测几何形状、纹理和布局。 SAM 3D 擅长处理自然图像，其中遮挡和场景混乱很常见，来自上下文的视觉识别线索发挥着更大的作用。我们通过人类和模型在环管道来实现这一目标，用于注释对象形状、纹理和姿势，以前所未有的规模提供基于视觉的 3D 重建数据。我们在现代多阶段训练框架中从这些数据中学习，该框架将综合预训练与现实世界对齐相结合，打破了 3D“数据障碍”。与最近的工作相比，我们取得了显着的进展，在对现实世界物体和场景的人类偏好测试中，胜率至少为 5:1。我们将发布代码和模型权重、在线演示以及用于野外 3D 对象重建的新的具有挑战性的基准。</li>
</ul>

<h3>Title: TRIM: Scalable 3D Gaussian Diffusion Inference with Temporal and Spatial Trimming</h3>
<ul>
<li><strong>Authors: </strong>Zeyuan Yin, Xiaoming Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16642">https://arxiv.org/abs/2511.16642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16642">https://arxiv.org/pdf/2511.16642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16642]] TRIM: Scalable 3D Gaussian Diffusion Inference with Temporal and Spatial Trimming(https://arxiv.org/abs/2511.16642)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in 3D Gaussian diffusion models suffer from time-intensive denoising and post-denoising processing due to the massive number of Gaussian primitives, resulting in slow generation and limited scalability along sampling trajectories. To improve the efficiency of 3D diffusion models, we propose $\textbf{TRIM}$ ($\textbf{T}$rajectory $\textbf{R}$eduction and $\textbf{I}$nstance $\textbf{M}$ask denoising), a post-training approach that incorporates both temporal and spatial trimming strategies, to accelerate inference without compromising output quality while supporting the inference-time scaling for Gaussian diffusion models. Instead of scaling denoising trajectories in a costly end-to-end manner, we develop a lightweight selector model to evaluate latent Gaussian primitives derived from multiple sampled noises, enabling early trajectory reduction by selecting candidates with high-quality potential. Furthermore, we introduce instance mask denoising to prune learnable Gaussian primitives by filtering out redundant background regions, reducing inference computation at each denoising step. Extensive experiments and analysis demonstrate that TRIM significantly improves both the efficiency and quality of 3D generation. Source code is available at $\href{this https URL}{link}$.</li>
<li><strong>摘要：</strong>3D 高斯扩散模型的最新进展由于大量高斯基元而受到时间密集型去噪和去噪后处理的影响，导致沿采样轨迹生成缓慢且可扩展性有限。为了提高3D扩散模型的效率，我们提出了$\textbf{TRIM}$（$\textbf{T}$rajectory $\textbf{R}$eduction和$\textbf{I}$nstance $\textbf{M}$ask去噪），这是一种结合时间和空间修剪策略的训练后方法，在不影响输出质量的情况下加速推理，同时支持高斯的推理时间缩放扩散模型。我们没有以昂贵的端到端方式缩放去噪轨迹，而是开发了一种轻量级选择器模型来评估从多个采样噪声导出的潜在高斯原语，通过选择具有高质量潜力的候选者来实现早期轨迹减少。此外，我们引入实例掩模去噪，通过过滤掉冗余背景区域来修剪可学习的高斯基元，减少每个去噪步骤的推理计算。大量的实验和分析表明，TRIM 显着提高了 3D 生成的效率和质量。源代码可在 $\href{此 https URL}{link}$ 获取。</li>
</ul>

<h3>Title: TriDiff-4D: Fast 4D Generation through Diffusion-based Triplane Re-posing</h3>
<ul>
<li><strong>Authors: </strong>Eddie Pokming Sheung, Qihao Liu, Wufei Ma, Prakhar Kaushik, Jianwen Xie, Alan Yuille</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16662">https://arxiv.org/abs/2511.16662</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16662">https://arxiv.org/pdf/2511.16662</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16662]] TriDiff-4D: Fast 4D Generation through Diffusion-based Triplane Re-posing(https://arxiv.org/abs/2511.16662)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>With the increasing demand for 3D animation, generating high-fidelity, controllable 4D avatars from textual descriptions remains a significant challenge. Despite notable efforts in 4D generative modeling, existing methods exhibit fundamental limitations that impede their broader applicability, including temporal and geometric inconsistencies, perceptual artifacts, motion irregularities, high computational costs, and limited control over dynamics. To address these challenges, we propose TriDiff-4D, a novel 4D generative pipeline that employs diffusion-based triplane re-posing to produce high-quality, temporally coherent 4D avatars. Our model adopts an auto-regressive strategy to generate 4D sequences of arbitrary length, synthesizing each 3D frame with a single diffusion process. By explicitly learning 3D structure and motion priors from large-scale 3D and motion datasets, TriDiff-4D enables skeleton-driven 4D generation that excels in temporal consistency, motion accuracy, computational efficiency, and visual fidelity. Specifically, TriDiff-4D first generates a canonical 3D avatar and a corresponding motion sequence from a text prompt, then uses a second diffusion model to animate the avatar according to the motion sequence, supporting arbitrarily long 4D generation. Experimental results demonstrate that TriDiff-4D significantly outperforms existing methods, reducing generation time from hours to seconds by eliminating the optimization process, while substantially improving the generation of complex motions with high-fidelity appearance and accurate 3D geometry.</li>
<li><strong>摘要：</strong>随着对 3D 动画的需求不断增加，从文本描述生成高保真、可控的 4D 头像仍然是一个重大挑战。尽管在 4D 生成建模方面做出了显着的努力，但现有方法表现出根本性的局限性，阻碍了其更广泛的适用性，包括时间和几何不一致、感知伪影、运动不规则、高计算成本以及对动力学的有限控制。为了应对这些挑战，我们提出了 TriDiff-4D，这是一种新颖的 4D 生成管道，它采用基于扩散的三平面重新构成来生成高质量、时间相干的 4D 化身。我们的模型采用自回归策略来生成任意长度的 4D 序列，通过单个扩散过程合成每个 3D 帧。通过从大规模 3D 和运动数据集中显式学习 3D 结构和运动先验，TriDiff-4D 能够实现骨架驱动的 4D 生成，在时间一致性、运动准确性、计算效率和视觉保真度方面表现出色。具体来说，TriDiff-4D首先根据文本提示生成规范的3D头像和相应的运动序列，然后使用第二个扩散模型根据运动序列对头像进行动画处理，支持任意长的4D生成。实验结果表明，TriDiff-4D 的性能显着优于现有方法，通过消除优化过程，将生成时间从几小时缩短至几秒，同时大幅改进具有高保真外观和精确 3D 几何形状的复杂运动的生成。</li>
</ul>

<h3>Title: Taming the Long-Tail: Efficient Reasoning RL Training with Adaptive Drafter</h3>
<ul>
<li><strong>Authors: </strong>Qinghao Hu, Shang Yang, Junxian Guo, Xiaozhe Yao, Yujun Lin, Yuxian Gu, Han Cai, Chuang Gan, Ana Klimovic, Song Han</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16665">https://arxiv.org/abs/2511.16665</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16665">https://arxiv.org/pdf/2511.16665</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16665]] Taming the Long-Tail: Efficient Reasoning RL Training with Adaptive Drafter(https://arxiv.org/abs/2511.16665)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The emergence of Large Language Models (LLMs) with strong reasoning capabilities marks a significant milestone, unlocking new frontiers in complex problem-solving. However, training these reasoning models, typically using Reinforcement Learning (RL), encounters critical efficiency bottlenecks: response generation during RL training exhibits a persistent long-tail distribution, where a few very long responses dominate execution time, wasting resources and inflating costs. To address this, we propose TLT, a system that accelerates reasoning RL training losslessly by integrating adaptive speculative decoding. Applying speculative decoding in RL is challenging due to the dynamic workloads, evolving target model, and draft model training overhead. TLT overcomes these obstacles with two synergistic components: (1) Adaptive Drafter, a lightweight draft model trained continuously on idle GPUs during long-tail generation to maintain alignment with the target model at no extra cost; and (2) Adaptive Rollout Engine, which maintains a memory-efficient pool of pre-captured CUDAGraphs and adaptively select suitable SD strategies for each input batch. Evaluations demonstrate that TLT achieves over 1.7x end-to-end RL training speedup over state-of-the-art systems, preserves the model accuracy, and yields a high-quality draft model as a free byproduct suitable for efficient deployment. Code is released at this https URL.</li>
<li><strong>摘要：</strong>具有强大推理能力的大型语言模型（LLM）的出现标志着一个重要的里程碑，开启了解决复杂问题的新领域。然而，通常使用强化学习 (RL) 来训练这些推理模型会遇到关键的效率瓶颈：RL 训练期间的响应生成呈现出持久的长尾分布，其中一些非常长的响应占据了执行时间，浪费了资源并增加了成本。为了解决这个问题，我们提出了 TLT，这是一种通过集成自适应推测解码来无损加速推理 RL 训练的系统。由于动态工作负载、不断变化的目标模型和草稿模型训练开销，在强化学习中应用推测性解码具有挑战性。 TLT 通过两个协同组件克服了这些障碍：(1) Adaptive Drafter，一种轻量级草稿模型，在长尾生成过程中在空闲 GPU 上持续训练，以保持与目标模型的一致性，无需额外成本； (2) 自适应推出引擎，它维护预先捕获的 CUDAGraph 的内存高效池，并为每个输入批次自适应地选择合适的 SD 策略。评估表明，与最先进的系统相比，TLT 实现了超过 1.7 倍的端到端 RL 训练加速，保持了模型准确性，并生成了高质量的草稿模型，作为适合高效部署的免费副产品。代码在此 https URL 发布。</li>
</ul>

<h3>Title: SceneDesigner: Controllable Multi-Object Image Generation with 9-DoF Pose Manipulation</h3>
<ul>
<li><strong>Authors: </strong>Zhenyuan Qin, Xincheng Shuai, Henghui Ding</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16666">https://arxiv.org/abs/2511.16666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16666">https://arxiv.org/pdf/2511.16666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16666]] SceneDesigner: Controllable Multi-Object Image Generation with 9-DoF Pose Manipulation(https://arxiv.org/abs/2511.16666)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Controllable image generation has attracted increasing attention in recent years, enabling users to manipulate visual content such as identity and style. However, achieving simultaneous control over the 9D poses (location, size, and orientation) of multiple objects remains an open challenge. Despite recent progress, existing methods often suffer from limited controllability and degraded quality, falling short of comprehensive multi-object 9D pose control. To address these limitations, we propose SceneDesigner, a method for accurate and flexible multi-object 9-DoF pose manipulation. SceneDesigner incorporates a branched network to the pre-trained base model and leverages a new representation, CNOCS map, which encodes 9D pose information from the camera view. This representation exhibits strong geometric interpretation properties, leading to more efficient and stable training. To support training, we construct a new dataset, ObjectPose9D, which aggregates images from diverse sources along with 9D pose annotations. To further address data imbalance issues, particularly performance degradation on low-frequency poses, we introduce a two-stage training strategy with reinforcement learning, where the second stage fine-tunes the model using a reward-based objective on rebalanced data. At inference time, we propose Disentangled Object Sampling, a technique that mitigates insufficient object generation and concept confusion in complex multi-object scenes. Moreover, by integrating user-specific personalization weights, SceneDesigner enables customized pose control for reference subjects. Extensive qualitative and quantitative experiments demonstrate that SceneDesigner significantly outperforms existing approaches in both controllability and quality. Code is publicly available at this https URL.</li>
<li><strong>摘要：</strong>近年来，可控图像生成引起了越来越多的关注，使用户能够操纵身份和风格等视觉内容。然而，实现对多个物体的 9D 姿态（位置、大小和方向）的同时控制仍然是一个开放的挑战。尽管最近取得了进展，但现有方法常常受到可控性有限和质量下降的影响，无法实现全面的多目标 9D 位姿控制。为了解决这些限制，我们提出了 SceneDesigner，一种用于精确且灵活的多对象 9-DoF 姿势操纵的方法。 SceneDesigner 将分支网络合并到预训练的基础模型中，并利用新的表示形式 CNOCS 地图，该地图对来自摄像机视图的 9D 位姿信息进行编码。这种表示表现出强大的几何解释特性，从而实现更高效、更稳定的训练。为了支持训练，我们构建了一个新的数据集 ObjectPose9D，它聚合了来自不同来源的图像以及 9D 姿势注释。为了进一步解决数据不平衡问题，特别是低频姿势的性能下降，我们引入了带有强化学习的两阶段训练策略，其中第二阶段使用基于奖励的目标对重新平衡的数据进行微调模型。在推理时，我们提出了解缠结对象采样，这是一种可以缓解复杂多对象场景中对象生成不足和概念混淆的技术。此外，通过集成用户特定的个性化权重，SceneDesigner 可以对参考对象进行自定义姿势控制。广泛的定性和定量实验表明，SceneDesigner 在可控性和质量方面均明显优于现有方法。代码可通过此 https URL 公开获取。</li>
</ul>

<h3>Title: V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models</h3>
<ul>
<li><strong>Authors: </strong>Yang Luo, Xuanlei Zhao, Baijiong Lin, Lingting Zhu, Liyao Tang, Yuqi Liu, Ying-Cong Chen, Shengju Qian, Xin Wang, Yang You</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16668">https://arxiv.org/abs/2511.16668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16668">https://arxiv.org/pdf/2511.16668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16668]] V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models(https://arxiv.org/abs/2511.16668)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent progress in generative video models, such as Veo-3, has shown surprising zero-shot reasoning abilities, creating a growing need for systematic and reliable evaluation. We introduce V-ReasonBench, a benchmark designed to assess video reasoning across four key dimensions: structured problem-solving, spatial cognition, pattern-based inference, and physical dynamics. The benchmark is built from both synthetic and real-world image sequences and provides a diverse set of answer-verifiable tasks that are reproducible, scalable, and unambiguous. Evaluations of six state-of-the-art video models reveal clear dimension-wise differences, with strong variation in structured, spatial, pattern-based, and physical reasoning. We further compare video models with strong image models, analyze common hallucination behaviors, and study how video duration affects Chain-of-Frames reasoning. Overall, V-ReasonBench offers a unified and reproducible framework for measuring video reasoning and aims to support the development of models with more reliable, human-aligned reasoning skills.</li>
<li><strong>摘要：</strong>Veo-3 等生成视频模型的最新进展显示出令人惊讶的零样本推理能力，从而对系统性和可靠的评估产生了日益增长的需求。我们推出了 V-ReasonBench，这是一个旨在评估四个关键维度的视频推理的基准：结构化问题解决、空间认知、基于模式的推理和物理动力学。该基准测试是根据合成图像序列和真实世界图像序列构建的，并提供了一组多样化的可验证答案的任务，这些任务是可重复的、可扩展的且明确的。对六种最先进的视频模型的评估揭示了明显的维度差异，在结构、空间、基于模式和物理推理方面存在巨大差异。我们进一步将视频模型与强大的图像模型进行比较，分析常见的幻觉行为，并研究视频持续时间如何影响帧链推理。总体而言，V-ReasonBench 为测量视频推理提供了一个统一且可重复的框架，旨在支持开发具有更可靠、更符合人类推理技能的模型。</li>
</ul>

<h3>Title: Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO</h3>
<ul>
<li><strong>Authors: </strong>Junhao Cheng, Liang Hou, Xin Tao, Jing Liao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16669">https://arxiv.org/abs/2511.16669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16669">https://arxiv.org/pdf/2511.16669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16669]] Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO(https://arxiv.org/abs/2511.16669)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While language models have become impactful in many real-world applications, video generation remains largely confined to entertainment. Motivated by video's inherent capacity to demonstrate physical-world information that is difficult to convey through language alone (e.g., imagine teaching someone to tie a tie using only text), we identify an underutilized opportunity to extend video as a new answer modality for Next-Event Prediction (NEP), formalized as Video-Next-Event Prediction (VNEP). While the established NEP task takes a video with a procedural or predictive question as input to predict the next event in text, VNEP requires dynamic video responses. This shift from telling to showing unlocks more intuitive and customized answers for procedural learning and creative exploration. However, this task remains challenging for existing models, as it demands an understanding of multimodal input, instruction-conditioned reasoning, and the generation of video with visual and semantic consistency. To address this, we introduce VANS, a model that leverages reinforcement learning to align a Vision-Language Model (VLM) with a Video Diffusion Model (VDM) for VNEP. The core of VANS is our proposed Joint-GRPO that orchestrates the VLM and VDM to function as a unit. Driven by a shared reward on their respective output, it optimizes the VLM to produce captions that are both accurate and friendly to visualize, while guiding the VDM to generate videos that are faithful to these captions and the input visual context. To enable this learning, we craft VANS-Data-100K, a dedicated dataset for the VNEP task. Experiments on procedural and predictive benchmarks demonstrate that VANS achieves state-of-the-art performance in both video event prediction and visualization. Codes are released in this https URL.</li>
<li><strong>摘要：</strong>尽管语言模型在许多现实应用中已经产生了影响，但视频生成仍然主要局限于娱乐。受视频固有的展示物理世界信息的能力的启发，这些信息很难仅通过语言来传达（例如，想象一下仅使用文本教某人打领带），我们发现了一个未充分利用的机会，将视频扩展为下一事件预测（NEP）的新答案模式，形式化为视频下一事件预测（VNEP）。虽然已建立的 NEP 任务将带有程序性或预测性问题的视频作为输入来预测文本中的下一个事件，但 VNEP 需要动态视频响应。这种从讲述到展示的转变为程序性学习和创造性探索提供了更直观和定制的答案。然而，这项任务对于现有模型来说仍然具有挑战性，因为它需要理解多模态输入、指令条件推理以及生成具有视觉和语义一致性的视频。为了解决这个问题，我们引入了 VANS，这是一种利用强化学习将视觉语言模型 (VLM) 与 VNEP 的视频扩散模型 (VDM) 结合起来的模型。 VANS 的核心是我们提出的 Joint-GRPO，它协调 VLM 和 VDM 作为一个单元发挥作用。在各自输出的共享奖励的驱动下，它优化了 VLM，以生成准确且易于可视化的字幕，同时指导 VDM 生成忠实于这些字幕和输入视觉上下文的视频。为了实现这种学习，我们制作了 VANS-Data-100K，这是一个用于 VNEP 任务的专用数据集。程序和预测基准实验表明，VANS 在视频事件预测和可视化方面均实现了最先进的性能。代码在此 https URL 中发布。</li>
</ul>

<h3>Title: Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation</h3>
<ul>
<li><strong>Authors: </strong>Ziyu Guo, Renrui Zhang, Hongyu Li, Manyuan Zhang, Xinyan Chen, Sifan Wang, Yan Feng, Peng Pei, Pheng-Ann Heng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16671">https://arxiv.org/abs/2511.16671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16671">https://arxiv.org/pdf/2511.16671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16671]] Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation(https://arxiv.org/abs/2511.16671)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: this https URL.</li>
<li><strong>摘要：</strong>视觉生成的最新进展越来越多地探索推理能力的整合。它们结合了文本推理，即在生成过程之前（作为预先规划）或之后（作为后细化）进行思考，但它们在生成过程中缺乏即时的多模式交互。在这项初步研究中，我们介绍了生成时思考（TwiG），这是第一个交错框架，可以在整个视觉生成过程中共同进化文本推理。随着视觉内容的逐渐生成，文本推理被交织在一起，以指导即将到来的局部区域并反思之前合成的区域。这种动态的相互作用产生了更多的上下文感知和语义丰富的视觉输出。为了揭示该框架的潜力，我们研究了三种候选策略：零样本提示、在我们策划的 TwiG-50K 数据集上进行监督微调 (SFT)，以及通过定制的 TwiG-GRPO 策略进行强化学习 (RL)，每种策略都提供了对交错推理动态的独特见解。我们希望这项工作能够激发对交错文本推理以增强视觉生成的进一步研究。代码将发布在：此 https URL。</li>
</ul>

<h3>Title: EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards</h3>
<ul>
<li><strong>Authors: </strong>Omkat Thawakar, Shravan Venkatraman, Ritesh Thawkar, Abdelrahman Shaker, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, Fahad Khan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.16672">https://arxiv.org/abs/2511.16672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.16672">https://arxiv.org/pdf/2511.16672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.16672]] EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards(https://arxiv.org/abs/2511.16672)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in large multimodal models (LMMs) have enabled impressive reasoning and perception abilities, yet most existing training pipelines still depend on human-curated data or externally verified reward models, limiting their autonomy and scalability. In this work, we strive to improve LMM reasoning capabilities in a purely unsupervised fashion (without any annotated data or reward distillation). To this end, we propose a self-evolving framework, named EvoLMM, that instantiates two cooperative agents from a single backbone model: a Proposer, which generates diverse, image-grounded questions, and a Solver, which solves them through internal consistency, where learning proceeds through a continuous self-rewarding process. This dynamic feedback encourages both the generation of informative queries and the refinement of structured reasoning without relying on ground-truth or human judgments. When using the popular Qwen2.5-VL as the base model, our EvoLMM yields consistent gains upto $\sim$3\% on multimodal math-reasoning benchmarks, including ChartQA, MathVista, and MathVision, using only raw training images. We hope our simple yet effective approach will serve as a solid baseline easing future research in self-improving LMMs in a fully-unsupervised fashion. Our code and models are available at this https URL.</li>
<li><strong>摘要：</strong>大型多模态模型 (LMM) 的最新进展实现了令人印象深刻的推理和感知能力，但大多数现有的训练流程仍然依赖于人工管理的数据或外部验证的奖励模型，限制了它们的自主性和可扩展性。在这项工作中，我们努力以纯粹无监督的方式提高 LMM 推理能力（没有任何注释数据或奖励蒸馏）。为此，我们提出了一个名为 EvoLMM 的自我进化框架，它从一个主干模型实例化两个合作代理：一个提议者，它生成多样化的、基于图像的问题，以及一个求解器，它通过内部一致性来解决这些问题，其中学习通过连续的自我奖励过程进行。这种动态反馈鼓励信息查询的生成和结构化推理的细化，而不依赖于事实真相或人类判断。当使用流行的 Qwen2.5-VL 作为基本模型时，我们的 EvoLMM 仅使用原始训练图像，在多模态数学推理基准（包括 ChartQA、MathVista 和 MathVision）上产生高达 $\sim$3\% 的一致增益。我们希望我们简单而有效的方法能够作为一个坚实的基线，方便未来以完全无监督的方式进行自我改进 LMM 的研究。我们的代码和模型可从此 https URL 获取。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
