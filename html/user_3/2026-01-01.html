<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-01-01</h1>
<h3>Title: A Review of Diffusion-based Simulation-Based Inference: Foundations and Applications in Non-Ideal Data Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Haley Rosso, Talea Mayo</a></li>
<li><strong>Subjects: </strong>cs.LG, math.PR, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.23748">https://arxiv.org/abs/2512.23748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.23748">https://arxiv.org/pdf/2512.23748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.23748]] A Review of Diffusion-based Simulation-Based Inference: Foundations and Applications in Non-Ideal Data Scenarios(https://arxiv.org/abs/2512.23748)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>For complex simulation problems, inferring parameters of scientific interest often precludes the use of classical likelihood-based techniques due to intractable likelihood functions. Simulation-based inference (SBI) methods forego the need for explicit likelihoods by directly utilizing samples from the simulator to learn posterior distributions over parameters $\mathbf{\theta}$ given observed data $\mathbf{x}_{\text{o}}$. Recent work has brought attention to diffusion models -- a type of generative model rooted in score matching and reverse-time stochastic dynamics -- as a flexible framework SBI tasks. This article reviews diffusion-based SBI from first principles to applications in practice. We first recall the mathematical foundations of diffusion modeling (forward noising, reverse-time SDE/ODE, probability flow, and denoising score matching) and explain how conditional scores enable likelihood-free posterior sampling. We then examine where diffusion models address pain points of normalizing flows in neural posterior/likelihood estimation and where they introduce new trade-offs (e.g., iterative sampling costs). The key theme of this review is robustness of diffusion-based SBI in non-ideal conditions common to scientific data: misspecification (mismatch between simulated training data and reality), unstructured or infinite-dimensional observations, and missingness. We synthesize methods spanning foundations drawing from Schrodinger-bridge formulations, conditional and sequential posterior samplers, amortized architectures for unstructured data, and inference-time prior adaptation. Throughout, we adopt consistent notation and emphasize conditions and caveats required for accurate posteriors. The review closes with a discussion of open problems with an eye toward applications of uncertainty quantification for probabilistic geophysical models that may benefit from diffusion-based SBI.</li>
<li><strong>摘要：</strong>对于复杂的模拟问题，由于难以处理的似然函数，推断具有科学意义的参数通常会妨碍使用经典的基于似然的技术。基于模拟的推理 (SBI) 方法无需显式似然，直接利用模拟器中的样本来学习给定观测数据 $\mathbf{x}_{\text{o}}$ 的参数 $\mathbf{\theta}$ 的后验分布。最近的工作引起了人们对扩散模型的关注——一种植根于分数匹配和逆时随机动力学的生成模型——作为 SBI 任务的灵活框架。本文回顾了基于扩散的 SBI，从基本原理到实践中的应用。我们首先回顾扩散建模的数学基础（前向噪声、逆时 SDE/ODE、概率流和去噪分数匹配），并解释条件分数如何实现无似然后验采样。然后，我们检查扩散模型在哪里解决了神经后验/似然估计中标准化流的痛点，以及它们在哪里引入了新的权衡（例如，迭代采样成本）。本次综述的主题是基于扩散的 SBI 在科学数据常见的非理想条件下的鲁棒性：错误指定（模拟训练数据与现实之间的不匹配）、非结构化或无限维观察以及缺失。我们综合了从薛定谔桥公式、条件和顺序后验采样器、非结构化数据的摊销架构以及推理时间先验适应中汲取的基础方法。在整个过程中，我们采用一致的符号，并强调准确后验所需的条件和注意事项。综述最后讨论了悬而未决的问题，着眼于可能受益于基于扩散的 SBI 的概率地球物理模型的不确定性量化应用。</li>
</ul>

<h3>Title: Generalized Regularized Evidential Deep Learning Models: Theory and Comprehensive Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Deep Shankar Pandey, Hyomin Choi, Qi Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.23753">https://arxiv.org/abs/2512.23753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.23753">https://arxiv.org/pdf/2512.23753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.23753]] Generalized Regularized Evidential Deep Learning Models: Theory and Comprehensive Evaluation(https://arxiv.org/abs/2512.23753)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Evidential deep learning (EDL) models, based on Subjective Logic, introduce a principled and computationally efficient way to make deterministic neural networks uncertainty-aware. The resulting evidential models can quantify fine-grained uncertainty using learned evidence. However, the Subjective-Logic framework constrains evidence to be non-negative, requiring specific activation functions whose geometric properties can induce activation-dependent learning-freeze behavior: a regime where gradients become extremely small for samples mapped into low-evidence regions. We theoretically characterize this behavior and analyze how different evidential activations influence learning dynamics. Building on this analysis, we design a general family of activation functions and corresponding evidential regularizers that provide an alternative pathway for consistent evidence updates across activation regimes. Extensive experiments on four benchmark classification problems (MNIST, CIFAR-10, CIFAR-100, and Tiny-ImageNet), two few-shot classification problems, and blind face restoration problem empirically validate the developed theory and demonstrate the effectiveness of the proposed generalized regularized evidential models.</li>
<li><strong>摘要：</strong>基于主观逻辑的证据深度学习 (EDL) 模型引入了一种原理性且计算高效的方法，使确定性神经网络具有不确定性感知能力。由此产生的证据模型可以使用学到的证据来量化细粒度的不确定性。然而，主观逻辑框架将证据限制为非负的，需要特定的激活函数，其几何特性可以引发依赖于激活的学习冻结行为：对于映射到低证据区域的样本，梯度变得极小。我们从理论上描述了这种行为，并分析了不同的证据激活如何影响学习动态。在此分析的基础上，我们设计了一个通用的激活函数族和相应的证据正则化器，为跨激活机制的一致证据更新提供了替代途径。对四个基准分类问题（MNIST、CIFAR-10、CIFAR-100 和 Tiny-ImageNet）、两个小样本分类问题和盲人脸恢复问题的广泛实验验证了所发展的理论，并证明了所提出的广义正则化证据模型的有效性。</li>
</ul>

<h3>Title: Exploiting the Prior of Generative Time Series Imputation</h3>
<ul>
<li><strong>Authors: </strong>YuYang Miao, Chang Li, Zehua Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.23832">https://arxiv.org/abs/2512.23832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.23832">https://arxiv.org/pdf/2512.23832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.23832]] Exploiting the Prior of Generative Time Series Imputation(https://arxiv.org/abs/2512.23832)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Time series imputation, i.e., filling the missing values of a time recording, finds various applications in electricity, finance, and weather modelling. Previous methods have introduced generative models such as diffusion probabilistic models and Schrodinger bridge models to conditionally generate the missing values from Gaussian noise or directly from linear interpolation results. However, as their prior is not informative to the ground-truth target, their generation process inevitably suffer increased burden and limited imputation accuracy. In this work, we present Bridge-TS, building a data-to-data generation process for generative time series imputation and exploiting the design of prior with two novel designs. Firstly, we propose expert prior, leveraging a pretrained transformer-based module as an expert to fill the missing values with a deterministic estimation, and then taking the results as the prior of ground truth target. Secondly, we explore compositional priors, utilizing several pretrained models to provide different estimation results, and then combining them in the data-to-data generation process to achieve a compositional priors-to-target imputation process. Experiments conducted on several benchmark datasets such as ETT, Exchange, and Weather show that Bridge-TS reaches a new record of imputation accuracy in terms of mean square error and mean absolute error, demonstrating the superiority of improving prior for generative time series imputation.</li>
<li><strong>摘要：</strong>时间序列插补，即填充时间记录的缺失值，在电力、金融和天气建模中有多种应用。以前的方法引入了生成模型，例如扩散概率模型和薛定谔桥模型，以有条件地从高斯噪声或直接从线性插值结果生成缺失值。然而，由于它们的先验对于地面实况目标来说并不提供信息，因此它们的生成过程不可避免地会承受更大的负担和有限的插补准确性。在这项工作中，我们提出了 Bridge-TS，构建了用于生成时间序列插补的数据到数据生成过程，并利用两种新颖的设计来利用先验设计。首先，我们提出专家先验，利用预训练的基于变压器的模块作为专家，通过确定性估计来填充缺失值，然后将结果作为地面真实目标的先验。其次，我们探索组合先验，利用多个预训练模型提供不同的估计结果，然后在数据到数据的生成过程中将它们组合起来，以实现组合先验到目标的插补过程。在ETT、Exchange、Weather等多个基准数据集上进行的实验表明，Bridge-TS在均方误差和平均绝对误差方面达到了插补精度的新纪录，证明了改进先验对于生成时间序列插补的优越性。</li>
</ul>

<h3>Title: Flow Matching Neural Processes</h3>
<ul>
<li><strong>Authors: </strong>Hussen Abu Hamad, Dan Rosenbaum</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.23853">https://arxiv.org/abs/2512.23853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.23853">https://arxiv.org/pdf/2512.23853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.23853]] Flow Matching Neural Processes(https://arxiv.org/abs/2512.23853)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Neural processes (NPs) are a class of models that learn stochastic processes directly from data and can be used for inference, sampling and conditional sampling. We introduce a new NP model based on flow matching, a generative modeling paradigm that has demonstrated strong performance on various data modalities. Following the NP training framework, the model provides amortized predictions of conditional distributions over any arbitrary points in the data. Compared to previous NP models, our model is simple to implement and can be used to sample from conditional distributions using an ODE solver, without requiring auxiliary conditioning methods. In addition, the model provides a controllable tradeoff between accuracy and running time via the number of steps in the ODE solver. We show that our model outperforms previous state-of-the-art neural process methods on various benchmarks including synthetic 1D Gaussian processes data, 2D images, and real-world weather data.</li>
<li><strong>摘要：</strong>神经过程（NP）是一类直接从数据中学习随机过程的模型，可用于推理、采样和条件采样。我们引入了一种基于流匹配的新 NP 模型，这是一种生成建模范式，在各种数据模态上都表现出了强大的性能。遵循 NP 训练框架，该模型提供数据中任意点的条件分布的摊销预测。与之前的 NP 模型相比，我们的模型实现起来很简单，并且可以使用 ODE 求解器从条件分布中进行采样，而不需要辅助条件方法。此外，该模型通过 ODE 求解器中的步数提供了精度和运行时间之间的可控权衡。我们证明，我们的模型在各种基准测试上都优于以前最先进的神经处理方法，包括合成一维高斯处理数据、二维图像和真实世界天气数据。</li>
</ul>

<h3>Title: Lifelong Domain Adaptive 3D Human Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Qucheng Peng, Hongfei Xue, Pu Wang, Chen Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.23860">https://arxiv.org/abs/2512.23860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.23860">https://arxiv.org/pdf/2512.23860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.23860]] Lifelong Domain Adaptive 3D Human Pose Estimation(https://arxiv.org/abs/2512.23860)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>3D Human Pose Estimation (3D HPE) is vital in various applications, from person re-identification and action recognition to virtual reality. However, the reliance on annotated 3D data collected in controlled environments poses challenges for generalization to diverse in-the-wild scenarios. Existing domain adaptation (DA) paradigms like general DA and source-free DA for 3D HPE overlook the issues of non-stationary target pose datasets. To address these challenges, we propose a novel task named lifelong domain adaptive 3D HPE. To our knowledge, we are the first to introduce the lifelong domain adaptation to the 3D HPE task. In this lifelong DA setting, the pose estimator is pretrained on the source domain and subsequently adapted to distinct target domains. Moreover, during adaptation to the current target domain, the pose estimator cannot access the source and all the previous target domains. The lifelong DA for 3D HPE involves overcoming challenges in adapting to current domain poses and preserving knowledge from previous domains, particularly combating catastrophic forgetting. We present an innovative Generative Adversarial Network (GAN) framework, which incorporates 3D pose generators, a 2D pose discriminator, and a 3D pose estimator. This framework effectively mitigates domain shifts and aligns original and augmented poses. Moreover, we construct a novel 3D pose generator paradigm, integrating pose-aware, temporal-aware, and domain-aware knowledge to enhance the current domain's adaptation and alleviate catastrophic forgetting on previous domains. Our method demonstrates superior performance through extensive experiments on diverse domain adaptive 3D HPE datasets.</li>
<li><strong>摘要：</strong>3D 人体姿势估计 (3D HPE) 在从人员重新识别和动作识别到虚拟现实的各种应用中都至关重要。然而，对在受控环境中收集的带注释 3D 数据的依赖对泛化到各种野外场景提出了挑战。现有的域适应 (DA) 范式（例如通用 DA 和 3D HPE 的无源 DA）忽略了非平稳目标姿态数据集的问题。为了应对这些挑战，我们提出了一项名为终身域自适应 3D HPE 的新任务。据我们所知，我们是第一个将终身域适应引入 3D HPE 任务的人。在这种终身 DA 设置中，姿态估计器在源域上进行预训练，然后适应不同的目标域。此外，在适应当前目标域期间，位姿估计器无法访问源域和所有先前的目标域。 3D HPE 的终身 DA 涉及克服适应当前领域姿势和保留先前领域知识的挑战，特别是对抗灾难性遗忘。我们提出了一种创新的生成对抗网络 (GAN) 框架，其中包含 3D 姿势生成器、2D 姿势鉴别器和 3D 姿势估计器。该框架有效地减轻了域变化并对齐原始姿势和增强姿势。此外，我们构建了一种新颖的 3D 姿势生成器范例，集成了姿势感知、时间感知和领域感知知识，以增强当前领域的适应能力并减轻对先前领域的灾难性遗忘。我们的方法通过对不同域自适应 3D HPE 数据集进行大量实验，展示了卓越的性能。</li>
</ul>

<h3>Title: Scaling Remote Sensing Foundation Models: Data Domain Tradeoffs at the Peta-Scale</h3>
<ul>
<li><strong>Authors: </strong>Charith Wickrema, Eliza Mace, Hunter Brown, Heidys Cabrera, Nick Krall, Matthew O'Neill, Shivangi Sarkar, Lowell Weissman, Eric Hughes, Guido Zarrella</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.23903">https://arxiv.org/abs/2512.23903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.23903">https://arxiv.org/pdf/2512.23903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.23903]] Scaling Remote Sensing Foundation Models: Data Domain Tradeoffs at the Peta-Scale(https://arxiv.org/abs/2512.23903)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We explore the scaling behaviors of artificial intelligence to establish practical techniques for training foundation models on high-resolution electro-optical (EO) datasets that exceed the current state-of-the-art scale by orders of magnitude. Modern multimodal machine learning (ML) applications, such as generative artificial intelligence (GenAI) systems for image captioning, search, and reasoning, depend on robust, domain-specialized encoders for non-text modalities. In natural-image domains where internet-scale data is plentiful, well-established scaling laws help optimize the joint scaling of model capacity, training compute, and dataset size. Unfortunately, these relationships are much less well-understood in high-value domains like remote sensing (RS). Using over a quadrillion pixels of commercial satellite EO data and the MITRE Federal AI Sandbox, we train progressively larger vision transformer (ViT) backbones, report success and failure modes observed at petascale, and analyze implications for bridging domain gaps across additional RS modalities. We observe that even at this scale, performance is consistent with a data limited regime rather than a model parameter-limited one. These practical insights are intended to inform data-collection strategies, compute budgets, and optimization schedules that advance the future development of frontier-scale RS foundation models.</li>
<li><strong>摘要：</strong>我们探索人工智能的扩展行为，以建立在高分辨率电光（EO）数据集上训练基础模型的实用技术，该数据集超出了当前最先进的规模几个数量级。现代多模态机器学习 (ML) 应用，例如用于图像字幕、搜索和推理的生成人工智能 (GenAI) 系统，依赖于用于非文本模态的强大的、领域专用的编码器。在互联网规模数据丰富的自然图像领域，完善的缩放法则有助于优化模型容量、训练计算和数据集大小的联合缩放。不幸的是，在遥感 (RS) 等高价值领域，人们对这些关系的理解要少得多。使用超过千万亿像素的商业卫星 EO 数据和 MITRE 联邦 AI 沙箱，我们训练逐渐增大的视觉变换器 (ViT) 主干网，报告在千万级规模观察到的成功和失败模式，并分析跨其他 RS 模式弥合领域差距的影响。我们观察到，即使在这种规模下，性能也与数据有限的状态一致，而不是模型参数限制的状态。这些实用见解旨在为数据收集策略、计算预算和优化计划提供信息，从而推动前沿规模遥感基础模型的未来发展。</li>
</ul>

<h3>Title: Assured Autonomy: How Operations Research Powers and Orchestrates Generative AI Systems</h3>
<ul>
<li><strong>Authors: </strong>Tinglong Dai, David Simchi-Levi, Michelle Xiao Wu, Yao Xie</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.23978">https://arxiv.org/abs/2512.23978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.23978">https://arxiv.org/pdf/2512.23978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.23978]] Assured Autonomy: How Operations Research Powers and Orchestrates Generative AI Systems(https://arxiv.org/abs/2512.23978)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative artificial intelligence (GenAI) is shifting from conversational assistants toward agentic systems -- autonomous decision-making systems that sense, decide, and act within operational workflows. This shift creates an autonomy paradox: as GenAI systems are granted greater operational autonomy, they should, by design, embody more formal structure, more explicit constraints, and stronger tail-risk discipline. We argue stochastic generative models can be fragile in operational domains unless paired with mechanisms that provide verifiable feasibility, robustness to distribution shift, and stress testing under high-consequence scenarios. To address this challenge, we develop a conceptual framework for assured autonomy grounded in operations research (OR), built on two complementary approaches. First, flow-based generative models frame generation as deterministic transport characterized by an ordinary differential equation, enabling auditability, constraint-aware generation, and connections to optimal transport, robust optimization, and sequential decision control. Second, operational safety is formulated through an adversarial robustness lens: decision rules are evaluated against worst-case perturbations within uncertainty or ambiguity sets, making unmodeled risks part of the design. This framework clarifies how increasing autonomy shifts OR's role from solver to guardrail to system architect, with responsibility for control logic, incentive protocols, monitoring regimes, and safety boundaries. These elements define a research agenda for assured autonomy in safety-critical, reliability-sensitive operational domains.</li>
<li><strong>摘要：</strong>生成人工智能 (GenAI) 正在从对话助理转向代理系统，即在操作工作流程中感知、决策和行动的自主决策系统。这种转变造成了一个自治悖论：随着 GenAI 系统被授予更大的操作自主权，它们在设计上应该体现更正式的结构、更明确的约束和更强的尾部风险纪律。我们认为，随机生成模型在操作领域可能很脆弱，除非与提供可验证的可行性、分布变化的鲁棒性以及高后果场景下的压力测试的机制相结合。为了应对这一挑战，我们开发了一个以运筹学 (OR) 为基础、建立在两种互补方法基础上的有保证的自主性概念框架。首先，基于流的生成模型将生成框架定义为以常微分方程为特征的确定性传输，从而实现可审核性、约束感知生成以及与最佳传输、鲁棒优化和顺序决策控制的连接。其次，操作安全是通过对抗性稳健性镜头来制定的：根据不确定性或模糊性集中的最坏情况扰动来评估决策规则，从而使未建模的风险成为设计的一部分。该框架阐明了增加自主权如何将 OR 的角色从求解器转变为护栏，再转变为系统架构师，负责控制逻辑、激励协议、监控机制和安全边界。这些要素定义了一个研究议程，以确保安全关键、可靠性敏感的操作领域的自主性。</li>
</ul>

<h3>Title: DriveExplorer: Images-Only Decoupled 4D Reconstruction with Progressive Restoration for Driving View Extrapolation</h3>
<ul>
<li><strong>Authors: </strong>Yuang Jia, Jinlong Wang, Jiayi Zhao, Chunlam Li, Shunzhou Wang, Wei Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.23983">https://arxiv.org/abs/2512.23983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.23983">https://arxiv.org/pdf/2512.23983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.23983]] DriveExplorer: Images-Only Decoupled 4D Reconstruction with Progressive Restoration for Driving View Extrapolation(https://arxiv.org/abs/2512.23983)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>This paper presents an effective solution for view extrapolation in autonomous driving scenarios. Recent approaches focus on generating shifted novel view images from given viewpoints using diffusion models. However, these methods heavily rely on priors such as LiDAR point clouds, 3D bounding boxes, and lane annotations, which demand expensive sensors or labor-intensive labeling, limiting applicability in real-world deployment. In this work, with only images and optional camera poses, we first estimate a global static point cloud and per-frame dynamic point clouds, fusing them into a unified representation. We then employ a deformable 4D Gaussian framework to reconstruct the scene. The initially trained 4D Gaussian model renders degraded and pseudo-images to train a video diffusion model. Subsequently, progressively shifted Gaussian renderings are iteratively refined by the diffusion model,and the enhanced results are incorporated back as training data for 4DGS. This process continues until extrapolation reaches the target viewpoints. Compared with baselines, our method produces higher-quality images at novel extrapolated viewpoints.</li>
<li><strong>摘要：</strong>本文提出了自动驾驶场景中视图外推的有效解决方案。最近的方法侧重于使用扩散模型从给定的视点生成移位的新颖视图图像。然而，这些方法严重依赖于 LiDAR 点云、3D 边界框和车道注释等先验知识，这些先验知识需要昂贵的传感器或劳动密集型标签，限制了在实际部署中的适用性。在这项工作中，仅使用图像和可选的相机姿势，我们首先估计全局静态点云和每帧动态点云，将它们融合成统一的表示。然后，我们采用可变形 4D 高斯框架来重建场景。最初训练的 4D 高斯模型渲染退化图像和伪图像来训练视频扩散模型。随后，通过扩散模型迭代地改进逐渐平移的高斯渲染，并将增强的结果合并回作为 4DGS 的训练数据。这个过程一直持续到外推达到目标视点为止。与基线相比，我们的方法在新颖的外推视点产生更高质量的图像。</li>
</ul>

<h3>Title: FitControler: Toward Fit-Aware Virtual Try-On</h3>
<ul>
<li><strong>Authors: </strong>Lu Yang, Yicheng Liu, Yanan Li, Xiang Bai, Hao Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24016">https://arxiv.org/abs/2512.24016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24016">https://arxiv.org/pdf/2512.24016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24016]] FitControler: Toward Fit-Aware Virtual Try-On(https://arxiv.org/abs/2512.24016)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Realistic virtual try-on (VTON) concerns not only faithful rendering of garment details but also coordination of the style. Prior art typically pursues the former, but neglects a key factor that shapes the holistic style -- garment fit. Garment fit delineates how a garment aligns with the body of a wearer and is a fundamental element in fashion design. In this work, we introduce fit-aware VTON and present FitControler, a learnable plug-in that can seamlessly integrate into modern VTON models to enable customized fit control. To achieve this, we highlight two challenges: i) how to delineate layouts of different fits and ii) how to render the garment that matches the layout. FitControler first features a fit-aware layout generator to redraw the body-garment layout conditioned on a set of delicately processed garment-agnostic representations, and a multi-scale fit injector is then used to deliver layout cues to enable layout-driven VTON. In particular, we build a fit-aware VTON dataset termed Fit4Men, including 13,000 body-garment pairs of different fits, covering both tops and bottoms, and featuring varying camera distances and body poses. Two fit consistency metrics are also introduced to assess the fitness of generations. Extensive experiments show that FitControler can work with various VTON models and achieve accurate fit control. Code and data will be released.</li>
<li><strong>摘要：</strong>逼真的虚拟试穿（VTON）不仅涉及服装细节的忠实呈现，还涉及风格的协调。现有技术通常追求前者，但忽略了塑造整体风格的关键因素——服装合身性。服装合身性描述了服装如何与穿着者的身体保持一致，是时装设计的基本元素。在这项工作中，我们介绍了适合感知的 VTON 并展示了 FitControler，这是一个可学习的插件，可以无缝集成到现代 VTON 模型中以实现定制的适合控制。为了实现这一目标，我们强调两个挑战：i）如何描绘不同版型的布局，以及 ii）如何呈现与布局相匹配的服装。 FitControler 首先具有一个合身感知布局生成器，用于根据一组精心处理的与服装无关的表示来重新绘制人体服装布局，然后使用多尺度合身注入器来提供布局提示，以启用布局驱动的 VTON。特别是，我们构建了一个名为 Fit4Men 的合身感知 VTON 数据集，其中包括 13,000 对不同合身的紧身衣，涵盖上衣和下装，并具有不同的相机距离和身体姿势。还引入了两个适合一致性指标来评估各代人的适合度。大量实验表明FitControler可以与各种VTON模型配合使用并实现精确的拟合控制。代码和数据将被发布。</li>
</ul>

<h3>Title: FUSE-RSVLM: Feature Fusion Vision-Language Model for Remote Sensing</h3>
<ul>
<li><strong>Authors: </strong>Yunkai Dang, Donghao Wang, Jiacheng Yang, Yifan Jiang, Meiyi Zhu, Yuekun Yang, Cong Wang, Qi Fan, Wenbin Li, Yang Gao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24022">https://arxiv.org/abs/2512.24022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24022">https://arxiv.org/pdf/2512.24022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24022]] FUSE-RSVLM: Feature Fusion Vision-Language Model for Remote Sensing(https://arxiv.org/abs/2512.24022)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large vision-language models (VLMs) exhibit strong performance across various tasks. However, these VLMs encounter significant challenges when applied to the remote sensing domain due to the inherent differences between remote sensing images and natural images. Existing remote sensing VLMs often fail to extract fine-grained visual features and suffer from visual forgetting during deep language processing. To address this, we introduce MF-RSVLM, a Multi-Feature Fusion Remote Sensing Vision--Language Model that effectively extracts and fuses visual features for RS understanding. MF-RSVLM learns multi-scale visual representations and combines global context with local details, improving the capture of small and complex structures in RS scenes. A recurrent visual feature injection scheme ensures the language model remains grounded in visual evidence and reduces visual forgetting during generation. Extensive experiments on diverse RS benchmarks show that MF-RSVLM achieves state-of-the-art or highly competitive performance across remote sensing classification, image captioning, and VQA tasks. Our code is publicly available at this https URL.</li>
<li><strong>摘要：</strong>大型视觉语言模型 (VLM) 在各种任务中表现出强大的性能。然而，由于遥感图像和自然图像之间的固有差异，这些 VLM 在应用于遥感领域时遇到了重大挑战。现有的遥感 VLM 通常无法提取细粒度的视觉特征，并且在深度语言处理过程中会出现视觉遗忘。为了解决这个问题，我们引入了 MF-RSVLM，一种多功能融合遥感视觉语言模型，可以有效地提取和融合视觉特征以促进遥感理解。 MF-RSVLM 学习多尺度视觉表示，并将全局上下文与局部细节相结合，改进 RS 场景中小型复杂结构的捕获。循环视觉特征注入方案确保语言模型始终以视觉证据为基础，并减少生成过程中的视觉遗忘。对各种 RS 基准的大量实验表明，MF-RSVLM 在遥感分类、图像字幕和 VQA 任务中实现了最先进的或极具竞争力的性能。我们的代码可通过此 https URL 公开获取。</li>
</ul>

<h3>Title: RainFusion2.0: Temporal-Spatial Awareness and Hardware-Efficient Block-wise Sparse Attention</h3>
<ul>
<li><strong>Authors: </strong>Aiyue Chen, Yaofu Liu, Junjian Huang, Guang Lian, Yiwu Yao, Wangli Lan, Jing Lin, Zhixin Ma, Tingting Zhou, Harry Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24086">https://arxiv.org/abs/2512.24086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24086">https://arxiv.org/pdf/2512.24086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24086]] RainFusion2.0: Temporal-Spatial Awareness and Hardware-Efficient Block-wise Sparse Attention(https://arxiv.org/abs/2512.24086)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In video and image generation tasks, Diffusion Transformer (DiT) models incur extremely high computational costs due to attention mechanisms, which limits their practical applications. Furthermore, with hardware advancements, a wide range of devices besides graphics processing unit (GPU), such as application-specific integrated circuit (ASIC), have been increasingly adopted for model inference. Sparse attention, which leverages the inherent sparsity of attention by skipping computations for insignificant tokens, is an effective approach to mitigate computational costs. However, existing sparse attention methods have two critical limitations: the overhead of sparse pattern prediction and the lack of hardware generality, as most of these methods are designed for GPU. To address these challenges, this study proposes RainFusion2.0, which aims to develop an online adaptive, hardware-efficient, and low-overhead sparse attention mechanism to accelerate both video and image generative models, with robust performance across diverse hardware platforms. Key technical insights include: (1) leveraging block-wise mean values as representative tokens for sparse mask prediction; (2) implementing spatiotemporal-aware token permutation; and (3) introducing a first-frame sink mechanism specifically designed for video generation scenarios. Experimental results demonstrate that RainFusion2.0 can achieve 80% sparsity while achieving an end-to-end speedup of 1.5~1.8x without compromising video quality. Moreover, RainFusion2.0 demonstrates effectiveness across various generative models and validates its generalization across diverse hardware platforms.</li>
<li><strong>摘要：</strong>在视频和图像生成任务中，扩散变压器（DiT）模型由于注意力机制而产生极高的计算成本，这限制了其实际应用。此外，随着硬件的进步，除图形处理单元（GPU）之外的各种设备，例如专用集成电路（ASIC），已越来越多地用于模型推理。稀疏注意力通过跳过对无关紧要标记的计算来利用注意力固有的稀疏性，是减轻计算成本的有效方法。然而，现有的稀疏注意力方法有两个关键限制：稀疏模式预测的开销和缺乏硬件通用性，因为这些方法大多数都是为 GPU 设计的。为了应对这些挑战，本研究提出了 RainFusion2.0，旨在开发一种在线自适应、硬件高效且低开销的稀疏注意力机制，以加速视频和图像生成模型，并在不同的硬件平台上具有强大的性能。关键技术见解包括：（1）利用逐块平均值作为稀疏掩模预测的代表标记； (2) 实现时空感知的令牌排列； (3)引入专门针对视频生成场景设计的首帧接收机制。实验结果表明，RainFusion2.0可以实现80%的稀疏度，同时在不影响视频质量的情况下实现1.5~1.8倍的端到端加速。此外，RainFusion2.0 展示了跨各种生成模型的有效性，并验证了其跨不同硬件平台的泛化能力。</li>
</ul>

<h3>Title: Think Before You Move: Latent Motion Reasoning for Text-to-Motion Generation</h3>
<ul>
<li><strong>Authors: </strong>Yijie Qian, Juncheng Wang, Yuxiang Feng, Chao Xu, Wang Lu, Yang Liu, Baigui Sun, Yiqiang Chen, Yong Liu, Shujun Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24100">https://arxiv.org/abs/2512.24100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24100">https://arxiv.org/pdf/2512.24100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24100]] Think Before You Move: Latent Motion Reasoning for Text-to-Motion Generation(https://arxiv.org/abs/2512.24100)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Current state-of-the-art paradigms predominantly treat Text-to-Motion (T2M) generation as a direct translation problem, mapping symbolic language directly to continuous poses. While effective for simple actions, this System 1 approach faces a fundamental theoretical bottleneck we identify as the Semantic-Kinematic Impedance Mismatch: the inherent difficulty of grounding semantically dense, discrete linguistic intent into kinematically dense, high-frequency motion data in a single shot. In this paper, we argue that the solution lies in an architectural shift towards Latent System 2 Reasoning. Drawing inspiration from Hierarchical Motor Control in cognitive science, we propose Latent Motion Reasoning (LMR) that reformulates generation as a two-stage Think-then-Act decision process. Central to LMR is a novel Dual-Granularity Tokenizer that disentangles motion into two distinct manifolds: a compressed, semantically rich Reasoning Latent for planning global topology, and a high-frequency Execution Latent for preserving physical fidelity. By forcing the model to autoregressively reason (plan the coarse trajectory) before it moves (instantiates the frames), we effectively bridge the ineffability gap between language and physics. We demonstrate LMR's versatility by implementing it for two representative baselines: T2M-GPT (discrete) and MotionStreamer (continuous). Extensive experiments show that LMR yields non-trivial improvements in both semantic alignment and physical plausibility, validating that the optimal substrate for motion planning is not natural language, but a learned, motion-aligned concept space. Codes and demos can be found in \hyperlink{this https URL}{this https URL}</li>
<li><strong>摘要：</strong>当前最先进的范式主要将文本到动作（T2M）生成视为直接翻译问题，将符号语言直接映射到连续姿势。虽然对于简单的动作有效，但这种系统 1 方法面临着一个基本的理论瓶颈，我们将其称为语义-运动学阻抗失配：将语义密集、离散的语言意图在单次拍摄中转化为运动学密集、高频运动数据的固有困难。在本文中，我们认为解决方案在于向潜在系统 2 推理的架构转变。从认知科学中的分层运动控制中汲取灵感，我们提出了潜在运动推理（LMR），它将生成重新表述为一个两阶段“思考然后行动”的决策过程。 LMR 的核心是一种新颖的双粒度分词器，它将运动分解为两个不同的流形：用于规划全局拓扑的压缩的、语义丰富的推理潜伏，以及用于保持物理保真度的高频执行潜伏。通过强制模型在移动（实例化框架）之前进行自回归推理（规划粗略轨迹），我们有效地弥合了语言和物理之间不可言喻的鸿沟。我们通过在两个代表性基线上实现 LMR 来展示 LMR 的多功能性：T2M-GPT（离散）和 MotionStreamer（连续）。大量实验表明，LMR 在语义对齐和物理合理性方面都取得了重大改进，验证了运动规划的最佳基础不是自然语言，而是学习的、运动对齐的概念空间。代码和演示可以在\hyperlink{这个https URL}{这个https URL}中找到</li>
</ul>

<h3>Title: Guided Diffusion-based Generation of Adversarial Objects for Real-World Monocular Depth Estimation Attacks</h3>
<ul>
<li><strong>Authors: </strong>Yongtao Chen, Yanbo Wang, Wentao Zhao, Guole Shen, Tianchen Deng, Jingchuan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24111">https://arxiv.org/abs/2512.24111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24111">https://arxiv.org/pdf/2512.24111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24111]] Guided Diffusion-based Generation of Adversarial Objects for Real-World Monocular Depth Estimation Attacks(https://arxiv.org/abs/2512.24111)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Monocular Depth Estimation (MDE) serves as a core perception module in autonomous driving systems, but it remains highly susceptible to adversarial attacks. Errors in depth estimation may propagate through downstream decision making and influence overall traffic safety. Existing physical attacks primarily rely on texture-based patches, which impose strict placement constraints and exhibit limited realism, thereby reducing their effectiveness in complex driving environments. To overcome these limitations, this work introduces a training-free generative adversarial attack framework that generates naturalistic, scene-consistent adversarial objects via a diffusion-based conditional generation process. The framework incorporates a Salient Region Selection module that identifies regions most influential to MDE and a Jacobian Vector Product Guidance mechanism that steers adversarial gradients toward update directions supported by the pre-trained diffusion model. This formulation enables the generation of physically plausible adversarial objects capable of inducing substantial adversarial depth shifts. Extensive digital and physical experiments demonstrate that our method significantly outperforms existing attacks in effectiveness, stealthiness, and physical deployability, underscoring its strong practical implications for autonomous driving safety assessment.</li>
<li><strong>摘要：</strong>单目深度估计（MDE）是自动驾驶系统中的核心感知模块，但它仍然非常容易受到对抗性攻击。深度估计中的误差可能会通过下游决策传播并影响整体交通安全。现有的物理攻击主要依赖于基于纹理的补丁，这会施加严格的放置限制并表现出有限的真实性，从而降低了其在复杂驾驶环境中的有效性。为了克服这些限制，这项工作引入了一种免训练的生成对抗攻击框架，该框架通过基于扩散的条件生成过程生成自然的、场景一致的对抗对象。该框架包含一个显着区域选择模块，用于识别对 MDE 最有影响的区域；以及一个雅可比向量积引导机制，用于将对抗性梯度引导至预训练扩散模型支持的更新方向。这种公式能够生成物理上合理的对抗性物体，能够引起实质性的对抗性深度变化。广泛的数字和物理实验表明，我们的方法在有效性、隐蔽性和物理可部署性方面显着优于现有的攻击，强调了其对自动驾驶安全评估的强大实际意义。</li>
</ul>

<h3>Title: Enhancing LLM-Based Neural Network Generation: Few-Shot Prompting and Efficient Validation for Automated Architecture Design</h3>
<ul>
<li><strong>Authors: </strong>Chandini Vysyaraju, Raghuvir Duvvuri, Avi Goyal, Dmitry Ignatov, Radu Timofte</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24120">https://arxiv.org/abs/2512.24120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24120">https://arxiv.org/pdf/2512.24120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24120]] Enhancing LLM-Based Neural Network Generation: Few-Shot Prompting and Efficient Validation for Automated Architecture Design(https://arxiv.org/abs/2512.24120)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Automated neural network architecture design remains a significant challenge in computer vision. Task diversity and computational constraints require both effective architectures and efficient search methods. Large Language Models (LLMs) present a promising alternative to computationally intensive Neural Architecture Search (NAS), but their application to architecture generation in computer vision has not been systematically studied, particularly regarding prompt engineering and validation strategies. Building on the task-agnostic NNGPT/LEMUR framework, this work introduces and validates two key contributions for computer vision. First, we present Few-Shot Architecture Prompting (FSAP), the first systematic study of the number of supporting examples (n = 1, 2, 3, 4, 5, 6) for LLM-based architecture generation. We find that using n = 3 examples best balances architectural diversity and context focus for vision tasks. Second, we introduce Whitespace-Normalized Hash Validation, a lightweight deduplication method (less than 1 ms) that provides a 100x speedup over AST parsing and prevents redundant training of duplicate computer vision architectures. In large-scale experiments across seven computer vision benchmarks (MNIST, CIFAR-10, CIFAR-100, CelebA, ImageNette, SVHN, Places365), we generated 1,900 unique architectures. We also introduce a dataset-balanced evaluation methodology to address the challenge of comparing architectures across heterogeneous vision tasks. These contributions provide actionable guidelines for LLM-based architecture search in computer vision and establish rigorous evaluation practices, making automated design more accessible to researchers with limited computational resources.</li>
<li><strong>摘要：</strong>自动神经网络架构设计仍然是计算机视觉领域的重大挑战。任务多样性和计算限制需要有效的架构和高效的搜索方法。大型语言模型（LLM）为计算密集型神经架构搜索（NAS）提供了一种有前途的替代方案，但它们在计算机视觉架构生成中的应用尚未得到系统研究，特别是在即时工程和验证策略方面。这项工作建立在与任务无关的 NNGPT/LEMUR 框架之上，介绍并验证了计算机视觉的两个关键贡献。首先，我们提出少样本架构提示（FSAP），这是对基于 LLM 的架构生成的支持示例数量（n = 1,2,3,4,5,6）的第一个系统研究。我们发现，使用 n = 3 个示例可以最好地平衡视觉任务的架构多样性和上下文焦点。其次，我们引入了空白标准化哈希验证，这是一种轻量级重复数据删除方法（小于 1 毫秒），它比 AST 解析速度提高了 100 倍，并防止重复计算机视觉架构的冗余训练。在七个计算机视觉基准（MNIST、CIFAR-10、CIFAR-100、CelebA、ImageNette、SVHN、Places365）的大规模实验中，我们生成了 1,900 个独特的架构。我们还引入了数据集平衡的评估方法，以解决跨异构视觉任务比较架构的挑战。这些贡献为计算机视觉中基于法学硕士的架构搜索提供了可行的指南，并建立了严格的评估实践，使计算资源有限的研究人员更容易进行自动化设计。</li>
</ul>

<h3>Title: GARDO: Reinforcing Diffusion Models without Reward Hacking</h3>
<ul>
<li><strong>Authors: </strong>Haoran He, Yuxiao Ye, Jie Liu, Jiajun Liang, Zhiyong Wang, Ziyang Yuan, Xintao Wang, Hangyu Mao, Pengfei Wan, Ling Pan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24138">https://arxiv.org/abs/2512.24138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24138">https://arxiv.org/pdf/2512.24138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24138]] GARDO: Reinforcing Diffusion Models without Reward Hacking(https://arxiv.org/abs/2512.24138)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Fine-tuning diffusion models via online reinforcement learning (RL) has shown great potential for enhancing text-to-image alignment. However, since precisely specifying a ground-truth objective for visual tasks remains challenging, the models are often optimized using a proxy reward that only partially captures the true goal. This mismatch often leads to reward hacking, where proxy scores increase while real image quality deteriorates and generation diversity collapses. While common solutions add regularization against the reference policy to prevent reward hacking, they compromise sample efficiency and impede the exploration of novel, high-reward regions, as the reference policy is usually sub-optimal. To address the competing demands of sample efficiency, effective exploration, and mitigation of reward hacking, we propose Gated and Adaptive Regularization with Diversity-aware Optimization (GARDO), a versatile framework compatible with various RL algorithms. Our key insight is that regularization need not be applied universally; instead, it is highly effective to selectively penalize a subset of samples that exhibit high uncertainty. To address the exploration challenge, GARDO introduces an adaptive regularization mechanism wherein the reference model is periodically updated to match the capabilities of the online policy, ensuring a relevant regularization target. To address the mode collapse issue in RL, GARDO amplifies the rewards for high-quality samples that also exhibit high diversity, encouraging mode coverage without destabilizing the optimization process. Extensive experiments across diverse proxy rewards and hold-out unseen metrics consistently show that GARDO mitigates reward hacking and enhances generation diversity without sacrificing sample efficiency or exploration, highlighting its effectiveness and robustness.</li>
<li><strong>摘要：</strong>通过在线强化学习（RL）微调扩散模型已显示出增强文本到图像对齐的巨大潜力。然而，由于为视觉任务精确指定真实目标仍然具有挑战性，因此通常使用仅部分捕获真实目标的代理奖励来优化模型。这种不匹配通常会导致奖励黑客攻击，即代理分数增加，而真实图像质量恶化，世代多样性崩溃。虽然常见的解决方案增加了针对参考策略的正则化以防止奖励黑客攻击，但它们会损害样本效率并阻碍对新颖的高奖励区域的探索，因为参考策略通常不是最优的。为了解决样本效率、有效探索和缓解奖励黑客攻击的竞争需求，我们提出了具有多样性感知优化的门控和自适应正则化（GARDO），这是一个与各种 RL 算法兼容的多功能框架。我们的主要见解是，正则化不需要普遍应用；相反，有选择地惩罚表现出高度不确定性的样本子集是非常有效的。为了应对探索挑战，GARDO 引入了自适应正则化机制，其中参考模型会定期更新以匹配在线策略的功能，从而确保相关的正则化目标。为了解决 RL 中的模式崩溃问题，GARDO 扩大了对也表现出高度多样性的高质量样本的奖励，鼓励模式覆盖而不破坏优化过程的稳定性。对各种代理奖励和保留的看不见的指标进行的广泛实验一致表明，GARDO 在不牺牲样本效率或探索的情况下减轻了奖励黑客行为并增强了生成多样性，突出了其有效性和鲁棒性。</li>
</ul>

<h3>Title: Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Chubin Chen, Sujie Hu, Jiashu Zhu, Meiqi Wu, Jintao Chen, Yanxun Li, Nisha Huang, Chengyu Fang, Jiahong Wu, Xiangxiang Chu, Xiu Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24146">https://arxiv.org/abs/2512.24146</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24146">https://arxiv.org/pdf/2512.24146</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24146]] Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning(https://arxiv.org/abs/2512.24146)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent studies have demonstrated significant progress in aligning text-to-image diffusion models with human preference via Reinforcement Learning from Human Feedback. However, while existing methods achieve high scores on automated reward metrics, they often lead to Preference Mode Collapse (PMC)-a specific form of reward hacking where models converge on narrow, high-scoring outputs (e.g., images with monolithic styles or pervasive overexposure), severely degrading generative diversity. In this work, we introduce and quantify this phenomenon, proposing DivGenBench, a novel benchmark designed to measure the extent of PMC. We posit that this collapse is driven by over-optimization along the reward model's inherent biases. Building on this analysis, we propose Directional Decoupling Alignment (D$^2$-Align), a novel framework that mitigates PMC by directionally correcting the reward signal. Specifically, our method first learns a directional correction within the reward model's embedding space while keeping the model frozen. This correction is then applied to the reward signal during the optimization process, preventing the model from collapsing into specific modes and thereby maintaining diversity. Our comprehensive evaluation, combining qualitative analysis with quantitative metrics for both quality and diversity, reveals that D$^2$-Align achieves superior alignment with human preference.</li>
<li><strong>摘要：</strong>最近的研究表明，通过人类反馈的强化学习，在将文本到图像的扩散模型与人类偏好保持一致方面取得了重大进展。然而，虽然现有方法在自动奖励指标上取得了高分，但它们通常会导致偏好模式崩溃（PMC）——一种奖励黑客的特定形式，其中模型收敛于狭窄的高分输出（例如，具有单一风格或普遍过度曝光的图像）​​，严重降低了生成多样性。在这项工作中，我们介绍并量化了这种现象，提出了 DivGenBench，这是一种旨在衡量 PMC 程度的新颖基准。我们认为这种崩溃是由于奖励模型固有偏差的过度优化造成的。在此分析的基础上，我们提出了定向解耦对齐（D$^2$-Align），这是一种通过定向纠正奖励信号来减轻 PMC 的新颖框架。具体来说，我们的方法首先在奖励模型的嵌入空间内学习方向校正，同时保持模型冻结。然后在优化过程中将该校正应用于奖励信号，防止模型崩溃为特定模式，从而保持多样性。我们的综合评估将定性分析与质量和多样性的定量指标相结合，表明 D$^2$-Align 实现了与人类偏好的高度一致。</li>
</ul>

<h3>Title: Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset</h3>
<ul>
<li><strong>Authors: </strong>TsaiChing Ni, ZhenQi Chen, YuanFu Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24160">https://arxiv.org/abs/2512.24160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24160">https://arxiv.org/pdf/2512.24160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24160]] Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset(https://arxiv.org/abs/2512.24160)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.</li>
<li><strong>摘要：</strong>我们推出了 IMDD-1M，这是第一个大规模工业多模态缺陷数据集，包含 1,000,000 个对齐的图像-文本对，旨在推进制造和质量检测的多模态学习。 IMDD-1M 包含跨越 60 多种材料类别和 400 多种缺陷类型的高分辨率现实世界缺陷，每种缺陷都附有经过专家验证的注释和细粒度的文本描述，详细说明了缺陷位置、严重性和上下文属性。该数据集支持广泛的应用，包括分类、分割、检索、字幕和生成建模。在 IMDD-1M 的基础上，我们从头开始训练基于扩散的视觉语言基础模型，专为工业场景量身定制。该模型作为一个通用的基础，可以通过轻量级微调有效地适应专门领域。它只需要不到专用专家模型 5% 所需的特定任务数据，就可以实现相当的性能，凸显了数据高效的基础模型适应工业检查和生成的潜力，为可扩展、领域自适应和基于知识的制造智能铺平了道路。</li>
</ul>

<h3>Title: DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Zefeng He, Xiaoye Qu, Yafu Li, Tong Zhu, Siyuan Huang, Yu Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24165">https://arxiv.org/abs/2512.24165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24165">https://arxiv.org/pdf/2512.24165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24165]] DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models(https://arxiv.org/abs/2512.24165)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While recent Multimodal Large Language Models (MLLMs) have attained significant strides in multimodal reasoning, their reasoning processes remain predominantly text-centric, leading to suboptimal performance in complex long-horizon, vision-centric tasks. In this paper, we establish a novel Generative Multimodal Reasoning paradigm and introduce DiffThinker, a diffusion-based reasoning framework. Conceptually, DiffThinker reformulates multimodal reasoning as a native generative image-to-image task, achieving superior logical consistency and spatial precision in vision-centric tasks. We perform a systematic comparison between DiffThinker and MLLMs, providing the first in-depth investigation into the intrinsic characteristics of this paradigm, revealing four core properties: efficiency, controllability, native parallelism, and collaboration. Extensive experiments across four domains (sequential planning, combinatorial optimization, constraint satisfaction, and spatial configuration) demonstrate that DiffThinker significantly outperforms leading closed source models including GPT-5 (+314.2\%) and Gemini-3-Flash (+111.6\%), as well as the fine-tuned Qwen3-VL-32B baseline (+39.0\%), highlighting generative multimodal reasoning as a promising approach for vision-centric reasoning.</li>
<li><strong>摘要：</strong>虽然最近的多模态大型语言模型（MLLM）在多模态推理方面取得了显着的进步，但它们的推理过程仍然主要以文本为中心，导致在复杂的长期、以视觉为中心的任务中表现不佳。在本文中，我们建立了一种新颖的生成多模态推理范式，并介绍了 DiffThinker，一种基于扩散的推理框架。从概念上讲，DiffThinker 将多模态推理重新表述为本地生成图像到图像任务，在以视觉为中心的任务中实现卓越的逻辑一致性和空间精度。我们对 DiffThinker 和 MLLM 进行了系统比较，首次深入研究了该范式的内在特征，揭示了四个核心属性：效率、可控性、本机并行性和协作。跨四个领域（顺序规划、组合优化、约束满足和空间配置）的广泛实验表明，DiffThinker 的性能显着优于领先的闭源模型，包括 GPT-5 (+314.2\%) 和 Gemini-3-Flash (+111.6\%)，以及微调的 Qwen3-VL-32B 基线 (+39.0\%)，凸显生成式多模态推理是一种有前途的方法。以视觉为中心的推理。</li>
</ul>

<h3>Title: Guiding a Diffusion Transformer with the Internal Dynamics of Itself</h3>
<ul>
<li><strong>Authors: </strong>Xingyu Zhou, Qifan Li, Xiaobin Hu, Hai Chen, Shuhang Gu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24176">https://arxiv.org/abs/2512.24176</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24176">https://arxiv.org/pdf/2512.24176</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24176]] Guiding a Diffusion Transformer with the Internal Dynamics of Itself(https://arxiv.org/abs/2512.24176)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The diffusion model presents a powerful ability to capture the entire (conditional) data distribution. However, due to the lack of sufficient training and data to learn to cover low-probability areas, the model will be penalized for failing to generate high-quality images corresponding to these areas. To achieve better generation quality, guidance strategies such as classifier free guidance (CFG) can guide the samples to the high-probability areas during the sampling stage. However, the standard CFG often leads to over-simplified or distorted samples. On the other hand, the alternative line of guiding diffusion model with its bad version is limited by carefully designed degradation strategies, extra training and additional sampling steps. In this paper, we proposed a simple yet effective strategy Internal Guidance (IG), which introduces an auxiliary supervision on the intermediate layer during training process and extrapolates the intermediate and deep layer's outputs to obtain generative results during sampling process. This simple strategy yields significant improvements in both training efficiency and generation quality on various baselines. On ImageNet 256x256, SiT-XL/2+IG achieves FID=5.31 and FID=1.75 at 80 and 800 epochs. More impressively, LightningDiT-XL/1+IG achieves FID=1.34 which achieves a large margin between all of these methods. Combined with CFG, LightningDiT-XL/1+IG achieves the current state-of-the-art FID of 1.19.</li>
<li><strong>摘要：</strong>扩散模型提供了捕获整个（条件）数据分布的强大能力。然而，由于缺乏足够的训练和数据来学习覆盖低概率区域，模型将因无法生成与这些区域相对应的高质量图像而受到惩罚。为了获得更好的生成质量，无分类器引导（CFG）等引导策略可以在采样阶段将样本引导到高概率区域。然而，标准CFG常常会导致样本过于简化或扭曲。另一方面，引导扩散模型的替代路线及其糟糕的版本受到精心设计的退化策略、额外的训练和额外的采样步骤的限制。在本文中，我们提出了一种简单而有效的策略内部指导（IG），它在训练过程中对中间层引入辅助监督，并在采样过程中外推中间层和深层的输出以获得生成结果。这种简单的策略在各种基线上显着提高了训练效率和生成质量。在 ImageNet 256x256 上，SiT-XL/2+IG 在 80 和 800 epoch 时实现 FID=5.31 和 FID=1.75。更令人印象深刻的是，LightningDiT-XL/1+IG 实现了 FID=1.34，这在所有这些方法之间实现了很大的差距。与 CFG 相结合，LightningDiT-XL/1+IG 实现了当前最先进的 FID 1.19。</li>
</ul>

<h3>Title: CorGi: Contribution-Guided Block-Wise Interval Caching for Training-Free Acceleration of Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Yonglak Son, Suhyeok Kim, Seungryong Kim, Young Geun Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24195">https://arxiv.org/abs/2512.24195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24195">https://arxiv.org/pdf/2512.24195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24195]] CorGi: Contribution-Guided Block-Wise Interval Caching for Training-Free Acceleration of Diffusion Transformers(https://arxiv.org/abs/2512.24195)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion transformer (DiT) achieves remarkable performance in visual generation, but its iterative denoising process combined with larger capacity leads to a high inference cost. Recent works have demonstrated that the iterative denoising process of DiT models involves substantial redundant computation across steps. To effectively reduce the redundant computation in DiT, we propose CorGi (Contribution-Guided Block-Wise Interval Caching), training-free DiT inference acceleration framework that selectively reuses the outputs of transformer blocks in DiT across denoising steps. CorGi caches low-contribution blocks and reuses them in later steps within each interval to reduce redundant computation while preserving generation quality. For text-to-image tasks, we further propose CorGi+, which leverages per-block cross-attention maps to identify salient tokens and applies partial attention updates to protect important object details. Evaluation on the state-of-the-art DiT models demonstrates that CorGi and CorGi+ achieve up to 2.0x speedup on average, while preserving high generation quality.</li>
<li><strong>摘要：</strong>扩散变压器（DiT）在视觉生成方面取得了显着的性能，但其迭代去噪过程与较大的容量相结合导致了较高的推理成本。最近的工作表明，DiT 模型的迭代去噪过程涉及大量跨步骤的冗余计算。为了有效减少 DiT 中的冗余计算，我们提出了 CorGi（Contribution-Guided Block-Wise Interval Caching），这是一种免训练的 DiT 推理加速框架，可以在去噪步骤中选择性地重用 DiT 中变压器块的输出。 CorGi 缓存低贡献块，并在每个间隔内的后续步骤中重用它们，以减少冗余计算，同时保持生成质量。对于文本到图像任务，我们进一步提出 CorGi+，它利用每个块的交叉注意力图来识别显着标记，并应用部分注意力更新来保护重要的对象细节。对最先进的 DiT 模型的评估表明，CorGi 和 CorGi+ 平均实现高达 2.0 倍的加速，同时保持高生成质量。</li>
</ul>

<h3>Title: Medical Image Classification on Imbalanced Data Using ProGAN and SMA-Optimized ResNet: Application to COVID-19</h3>
<ul>
<li><strong>Authors: </strong>Sina Jahromi, Farshid Hajati, Alireza Rezaee, Javaher Nourian</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24214">https://arxiv.org/abs/2512.24214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24214">https://arxiv.org/pdf/2512.24214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24214]] Medical Image Classification on Imbalanced Data Using ProGAN and SMA-Optimized ResNet: Application to COVID-19(https://arxiv.org/abs/2512.24214)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The challenge of imbalanced data is prominent in medical image classification. This challenge arises when there is a significant disparity in the number of images belonging to a particular class, such as the presence or absence of a specific disease, as compared to the number of images belonging to other classes. This issue is especially notable during pandemics, which may result in an even more significant imbalance in the dataset. Researchers have employed various approaches in recent years to detect COVID-19 infected individuals accurately and quickly, with artificial intelligence and machine learning algorithms at the forefront. However, the lack of sufficient and balanced data remains a significant obstacle to these methods. This study addresses the challenge by proposing a progressive generative adversarial network to generate synthetic data to supplement the real ones. The proposed method suggests a weighted approach to combine synthetic data with real ones before inputting it into a deep network classifier. A multi-objective meta-heuristic population-based optimization algorithm is employed to optimize the hyper-parameters of the classifier. The proposed model exhibits superior cross-validated metrics compared to existing methods when applied to a large and imbalanced chest X-ray image dataset of COVID-19. The proposed model achieves 95.5% and 98.5% accuracy for 4-class and 2-class imbalanced classification problems, respectively. The successful experimental outcomes demonstrate the effectiveness of the proposed model in classifying medical images using imbalanced data during pandemics.</li>
<li><strong>摘要：</strong>不平衡数据的挑战在医学图像分类中非常突出。当属于特定类别的图像数量与属于其他类别的图像数量相比存在显着差异时（例如是否存在特定疾病），就会出现这一挑战。这个问题在大流行期间尤其值得注意，这可能会导致数据集更加严重的不平衡。近年来，研究人员采用了各种方法来准确、快速地检测出 COVID-19 感染者，其中人工智能和机器学习算法处于最前沿。然而，缺乏足够和平衡的数据仍然是这些方法的重大障碍。这项研究通过提出一种渐进式生成对抗网络来生成合成数据来补充真实数据来解决这一挑战。所提出的方法提出了一种加权方法，在将合成数据输入深度网络分类器之前将其与真实数据相结合。采用多目标元启发式基于群体的优化算法来优化分类器的超参数。当应用于大型且不平衡的 COVID-19 胸部 X 射线图像数据集时，与现有方法相比，所提出的模型表现出优越的交叉验证指标。该模型对于 4 类和 2 类不平衡分类问题分别达到 95.5% 和 98.5% 的准确率。成功的实验结果证明了所提出的模型在大流行期间使用不平衡数据对医学图像进行分类的有效性。</li>
</ul>

<h3>Title: Physically-Grounded Manifold Projection with Foundation Priors for Metal Artifact Reduction in Dental CBCT</h3>
<ul>
<li><strong>Authors: </strong>Zhi Li, Yaqi Wang, Bingtao Ma, Yifan Zhang, Huiyu Zhou, Shuai Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24260">https://arxiv.org/abs/2512.24260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24260">https://arxiv.org/pdf/2512.24260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24260]] Physically-Grounded Manifold Projection with Foundation Priors for Metal Artifact Reduction in Dental CBCT(https://arxiv.org/abs/2512.24260)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Metal artifacts in Dental CBCT severely obscure anatomical structures, hindering diagnosis. Current deep learning for Metal Artifact Reduction (MAR) faces limitations: supervised methods suffer from spectral blurring due to "regression-to-the-mean", while unsupervised ones risk structural hallucinations. Denoising Diffusion Models (DDPMs) offer realism but rely on slow, stochastic iterative sampling, unsuitable for clinical use. To resolve this, we propose the Physically-Grounded Manifold Projection (PGMP) framework. First, our Anatomically-Adaptive Physics Simulation (AAPS) pipeline synthesizes high-fidelity training pairs via Monte Carlo spectral modeling and patient-specific digital twins, bridging the synthetic-to-real gap. Second, our DMP-Former adapts the Direct x-Prediction paradigm, reformulating restoration as a deterministic manifold projection to recover clean anatomy in a single forward pass, eliminating stochastic sampling. Finally, a Semantic-Structural Alignment (SSA) module anchors the solution using priors from medical foundation models (MedDINOv3), ensuring clinical plausibility. Experiments on synthetic and multi-center clinical datasets show PGMP outperforms state-of-the-art methods on unseen anatomy, setting new benchmarks in efficiency and diagnostic reliability. Code and data: this https URL</li>
<li><strong>摘要：</strong>牙科 CBCT 中的金属伪影严重掩盖了解剖结构，阻碍了诊断。当前金属伪影减少（MAR）的深度学习面临局限性：监督方法由于“均值回归”而遭受光谱模糊，而无监督方法则面临结构性幻觉的风险。去噪扩散模型 (DDPM) 具有真实感，但依赖于缓慢的随机迭代采样，不适合临床使用。为了解决这个问题，我们提出了物理接地流形投影（PGMP）框架。首先，我们的解剖自适应物理模拟 (AAPS) 管道通过蒙特卡罗光谱建模和患者特定的数字双胞胎合成高保真训练对，从而弥合了合成与真实的差距。其次，我们的 DMP-Former 采用直接 x 预测范式，将恢复重新表述为确定性流形投影，以在单次前向传递中恢复干净的解剖结构，从而消除随机采样。最后，语义结构对齐 (SSA) 模块使用医学基础模型 (MedDINOv3) 的先验知识来锚定解决方案，确保临床合理性。合成和多中心临床数据集的实验表明，PGMP 在看不见的解剖学方面优于最先进的方法，在效率和诊断可靠性方面树立了新的基准。代码和数据：这个https URL</li>
</ul>

<h3>Title: Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhe Huang, Hao Wen, Aiming Hao, Bingze Song, Meiqi Wu, Jiahong Wu, Xiangxiang Chu, Sheng Lu, Haoqian Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24271">https://arxiv.org/abs/2512.24271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24271">https://arxiv.org/pdf/2512.24271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24271]] Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation(https://arxiv.org/abs/2512.24271)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have made remarkable progress in video understanding. However, they suffer from a critical vulnerability: an over-reliance on language priors, which can lead to visual ungrounded hallucinations, especially when processing counterfactual videos that defy common sense. This limitation, stemming from the intrinsic data imbalance between text and video, is challenging to address due to the substantial cost of collecting and annotating counterfactual data. To address this, we introduce DualityForge, a novel counterfactual data synthesis framework that employs controllable, diffusion-based video editing to transform real-world videos into counterfactual scenarios. By embedding structured contextual information into the video editing and QA generation processes, the framework automatically produces high-quality QA pairs together with original-edited video pairs for contrastive training. Based on this, we build DualityVidQA, a large-scale video dataset designed to reduce MLLM hallucinations. In addition, to fully exploit the contrastive nature of our paired data, we propose Duality-Normalized Advantage Training (DNA-Train), a two-stage SFT-RL training regime where the RL phase applies pair-wise $\ell_1$ advantage normalization, thereby enabling a more stable and efficient policy optimization. Experiments on DualityVidQA-Test demonstrate that our method substantially reduces model hallucinations on counterfactual videos, yielding a relative improvement of 24.0% over the Qwen2.5-VL-7B baseline. Moreover, our approach achieves significant gains across both hallucination and general-purpose benchmarks, indicating strong generalization capability. We will open-source our dataset and code.</li>
<li><strong>摘要：</strong>多模态大语言模型（MLLM）在视频理解方面取得了显着进展。然而，它们存在一个严重的弱点：过度依赖语言先验，这可能会导致视觉上的无根据的幻觉，特别是在处理违背常识的反事实视频时。这种限制源于文本和视频之间固有的数据不平衡，由于收集和注释反事实数据的成本高昂，因此很难解决。为了解决这个问题，我们引入了 DualityForge，这是一种新颖的反事实数据合成框架，它采用可控的、基于扩散的视频编辑将现实世界的视频转换为反事实场景。通过将结构化上下文信息嵌入到视频编辑和 QA 生成过程中，该框架会自动生成高质量的 QA 对以及原始编辑的视频对，以进行对比训练。基于此，我们构建了 DualityVidQA，一个旨在减少 MLLM 幻觉的大规模视频数据集。此外，为了充分利用配对数据的对比性质，我们提出了对偶归一化优势训练（DNA-Train），这是一种两阶段 SFT-RL 训练机制，其中 RL 阶段应用成对 $\ell_1$ 优势归一化，从而实现更稳定、更高效的策略优化。 DualityVidQA-Test 的实验表明，我们的方法大大减少了反事实视频上的模型幻觉，比 Qwen2.5-VL-7B 基线相对提高了 24.0%。此外，我们的方法在幻觉和通用基准方面都取得了显着的进步，表明了强大的泛化能力。我们将开源我们的数据集和代码。</li>
</ul>

<h3>Title: One-shot synthesis of rare gastrointestinal lesions improves diagnostic accuracy and clinical training</h3>
<ul>
<li><strong>Authors: </strong>Jia Yu, Yan Zhu, Peiyao Fu, Tianyi Chen, Zhihua Wang, Fei Wu, Quanlin Li, Pinghong Zhou, Shuo Wang, Xian Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24278">https://arxiv.org/abs/2512.24278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24278">https://arxiv.org/pdf/2512.24278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24278]] One-shot synthesis of rare gastrointestinal lesions improves diagnostic accuracy and clinical training(https://arxiv.org/abs/2512.24278)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Rare gastrointestinal lesions are infrequently encountered in routine endoscopy, restricting the data available for developing reliable artificial intelligence (AI) models and training novice clinicians. Here we present EndoRare, a one-shot, retraining-free generative framework that synthesizes diverse, high-fidelity lesion exemplars from a single reference image. By leveraging language-guided concept disentanglement, EndoRare separates pathognomonic lesion features from non-diagnostic attributes, encoding the former into a learnable prototype embedding while varying the latter to ensure diversity. We validated the framework across four rare pathologies (calcifying fibrous tumor, juvenile polyposis syndrome, familial adenomatous polyposis, and Peutz-Jeghers syndrome). Synthetic images were judged clinically plausible by experts and, when used for data augmentation, significantly enhanced downstream AI classifiers, improving the true positive rate at low false-positive rates. Crucially, a blinded reader study demonstrated that novice endoscopists exposed to EndoRare-generated cases achieved a 0.400 increase in recall and a 0.267 increase in precision. These results establish a practical, data-efficient pathway to bridge the rare-disease gap in both computer-aided diagnostics and clinical education.</li>
<li><strong>摘要：</strong>常规内窥镜检查中很少遇到罕见的胃肠道病变，这限制了可用于开发可靠的人工智能（AI）模型和培训新手临床医生的数据。在这里，我们提出了 EndoRare，这是一种一次性、无需再训练的生成框架，可以从单个参考图像合成多样化的高保真病变样本。通过利用语言引导的概念解开，EndoRare 将病理性病变特征与非诊断属性分开，将前者编码为可学习的原型嵌入，同时改变后者以确保多样性。我们针对四种罕见的病理学（钙化性纤维瘤、幼年性息肉病综合征、家族性腺瘤性息肉病和黑斑息肉病综合征）验证了该框架。合成图像经专家判断在临床上合理，当用于数据增强时，显着增强了下游人工智能分类器，在低假阳性率的情况下提高了真阳性率。至关重要的是，一项盲法读者研究表明，接触 EndoRare 生成病例的新手内窥镜医师的召回率提高了 0.400，精度提高了 0.267。这些结果建立了一条实用、数据高效的途径，以弥合计算机辅助诊断和临床教育方面的罕见疾病差距。</li>
</ul>

<h3>Title: UniAct: Unified Motion Generation and Action Streaming for Humanoid Robots</h3>
<ul>
<li><strong>Authors: </strong>Nan Jiang, Zimo He, Wanhe Yu, Lexi Pang, Yunhao Li, Hongjie Li, Jieming Cui, Yuhan Li, Yizhou Wang, Yixin Zhu, Siyuan Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24321">https://arxiv.org/abs/2512.24321</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24321">https://arxiv.org/pdf/2512.24321</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24321]] UniAct: Unified Motion Generation and Action Streaming for Humanoid Robots(https://arxiv.org/abs/2512.24321)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>A long-standing objective in humanoid robotics is the realization of versatile agents capable of following diverse multimodal instructions with human-level flexibility. Despite advances in humanoid control, bridging high-level multimodal perception with whole-body execution remains a significant bottleneck. Existing methods often struggle to translate heterogeneous instructions -- such as language, music, and trajectories -- into stable, real-time actions. Here we show that UniAct, a two-stage framework integrating a fine-tuned MLLM with a causal streaming pipeline, enables humanoid robots to execute multimodal instructions with sub-500 ms latency. By unifying inputs through a shared discrete codebook via FSQ, UniAct ensures cross-modal alignment while constraining motions to a physically grounded manifold. This approach yields a 19% improvement in the success rate of zero-shot tracking of imperfect reference motions. We validate UniAct on UniMoCap, our 20-hour humanoid motion benchmark, demonstrating robust generalization across diverse real-world scenarios. Our results mark a critical step toward responsive, general-purpose humanoid assistants capable of seamless interaction through unified perception and control.</li>
<li><strong>摘要：</strong>人形机器人技术的一个长期目标是实现能够遵循多种多模式指令并具有人类水平灵活性的多功能代理。尽管人形控制取得了进步，但将高级多模式感知与全身执行联系起来仍然是一个重大瓶颈。现有的方法常常难以将异构指令（例如语言、音乐和轨迹）转化为稳定的实时动作。在这里，我们展示了 UniAct，这是一个将微调的 MLLM 与因果流管道相集成的两阶段框架，使人形机器人能够以低于 500 毫秒的延迟执行多模式指令。通过 FSQ 通过共享离散码本统一输入，UniAct 确保跨模式对齐，同时将运动限制到物理接地流形。这种方法将不完美参考运动的零样本跟踪的成功率提高了 19%。我们在 UniMoCap（我们的 20 小时人形运动基准）上验证了 UniAct，展示了跨不同现实场景的强大泛化能力。我们的结果标志着向响应式通用人形助手迈出了关键一步，能够通过统一的感知和控制实现无缝交互。</li>
</ul>

<h3>Title: DyStream: Streaming Dyadic Talking Heads Generation via Flow Matching-based Autoregressive Model</h3>
<ul>
<li><strong>Authors: </strong>Bohong Chen, Haiyang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24408">https://arxiv.org/abs/2512.24408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24408">https://arxiv.org/pdf/2512.24408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24408]] DyStream: Streaming Dyadic Talking Heads Generation via Flow Matching-based Autoregressive Model(https://arxiv.org/abs/2512.24408)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generating realistic, dyadic talking head video requires ultra-low latency. Existing chunk-based methods require full non-causal context windows, introducing significant delays. This high latency critically prevents the immediate, non-verbal feedback required for a realistic listener. To address this, we present DyStream, a flow matching-based autoregressive model that could generate video in real-time from both speaker and listener audio. Our method contains two key designs: (1) we adopt a stream-friendly autoregressive framework with flow-matching heads for probabilistic modeling, and (2) We propose a causal encoder enhanced by a lookahead module to incorporate short future context (e.g., 60 ms) to improve quality while maintaining low latency. Our analysis shows this simple-and-effective method significantly surpass alternative causal strategies, including distillation and generative encoder. Extensive experiments show that DyStream could generate video within 34 ms per frame, guaranteeing the entire system latency remains under 100 ms. Besides, it achieves state-of-the-art lip-sync quality, with offline and online LipSync Confidence scores of 8.13 and 7.61 on HDTF, respectively. The model, weights and codes are available.</li>
<li><strong>摘要：</strong>生成逼真的二元头像视频需要超低延迟。现有的基于块的方法需要完整的非因果上下文窗口，从而引入显着的延迟。这种高延迟严重阻碍了现实听众所需的即时、非语言反馈。为了解决这个问题，我们提出了 DyStream，一种基于流匹配的自回归模型，可以从说话者和听众的音频中实时生成视频。我们的方法包含两个关键设计：（1）我们采用带有流匹配头的流友好自回归框架进行概率建模，（2）我们提出了一种由前瞻模块增强的因果编码器，以合并短的未来上下文（例如 60 ms），以提高质量，同时保持低延迟。我们的分析表明，这种简单有效的方法明显优于其他因果策略，包括蒸馏和生成编码器。大量实验表明，DyStream可以在每帧34毫秒内生成视频，保证整个系统延迟保持在100毫秒以下。此外，它还实现了最先进的口型同步质量，HDTF 上的离线和在线口型同步置信度分数分别为 8.13 和 7.61。型号、重量和代码均可提供。</li>
</ul>

<h3>Title: Generative forecasting with joint probability models</h3>
<ul>
<li><strong>Authors: </strong>Patrick Wyrod, Ashesh Chattopadhyay, Daniele Venturi</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24446">https://arxiv.org/abs/2512.24446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24446">https://arxiv.org/pdf/2512.24446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24446]] Generative forecasting with joint probability models(https://arxiv.org/abs/2512.24446)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Chaotic dynamical systems exhibit strong sensitivity to initial conditions and often contain unresolved multiscale processes, making deterministic forecasting fundamentally limited. Generative models offer an appealing alternative by learning distributions over plausible system evolutions; yet, most existing approaches focus on next-step conditional prediction rather than the structure of the underlying dynamics. In this work, we reframe forecasting as a fully generative problem by learning the joint probability distribution of lagged system states over short temporal windows and obtaining forecasts through marginalization. This new perspective allows the model to capture nonlinear temporal dependencies, represent multistep trajectory segments, and produce next-step predictions consistent with the learned joint distribution. We also introduce a general, model-agnostic training and inference framework for joint generative forecasting and show how it enables assessment of forecast robustness and reliability using three complementary uncertainty quantification metrics (ensemble variance, short-horizon autocorrelation, and cumulative Wasserstein drift), without access to ground truth. We evaluate the performance of the proposed method on two canonical chaotic dynamical systems, the Lorenz-63 system and the Kuramoto-Sivashinsky equation, and show that joint generative models yield improved short-term predictive skill, preserve attractor geometry, and achieve substantially more accurate long-range statistical behaviour than conventional conditional next-step models.</li>
<li><strong>摘要：</strong>混沌动力系统对初始条件表现出很强的敏感性，并且通常包含未解决的多尺度过程，使得确定性预测从根本上受到限制。生成模型通过学习合理的系统演化分布，提供了一种有吸引力的替代方案；然而，大多数现有方法侧重于下一步条件预测，而不是潜在动态的结构。在这项工作中，我们通过学习短时间窗口内滞后系统状态的联合概率分布并通过边缘化获得预测，将预测重新构建为一个完全生成的问题。这种新的视角允许模型捕获非线性时间依赖性，表示多步轨迹段，并生成与学习的联合分布一致的下一步预测。我们还介绍了一种用于联合生成预测的通用的、与模型无关的训练和推理框架，并展示了它如何使用三个互补的不确定性量化指标（集合方差、短视野自相关和累积 Wasserstein 漂移）来评估预测的稳健性和可靠性，而无需访问地面实况。我们评估了所提出的方法在两个规范混沌动力系统（Lorenz-63 系统和 Kuramoto-Sivashinsky 方程）上的性能，结果表明联合生成模型可以提高短期预测能力，保留吸引子几何形状，并实现比传统条件下一步模型更准确的长期统计行为。</li>
</ul>

<h3>Title: F2IDiff: Real-world Image Super-resolution using Feature to Image Diffusion Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Devendra K. Jangid, Ripon K. Saha, Dilshan Godaliyadda, Jing Li, Seok-Jun Lee, Hamid R. Sheikh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24473">https://arxiv.org/abs/2512.24473</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24473">https://arxiv.org/pdf/2512.24473</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24473]] F2IDiff: Real-world Image Super-resolution using Feature to Image Diffusion Foundation Model(https://arxiv.org/abs/2512.24473)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generation, generative</a></li>
<li><strong>Abstract: </strong>With the advent of Generative AI, Single Image Super-Resolution (SISR) quality has seen substantial improvement, as the strong priors learned by Text-2-Image Diffusion (T2IDiff) Foundation Models (FM) can bridge the gap between High-Resolution (HR) and Low-Resolution (LR) images. However, flagship smartphone cameras have been slow to adopt generative models because strong generation can lead to undesirable hallucinations. For substantially degraded LR images, as seen in academia, strong generation is required and hallucinations are more tolerable because of the wide gap between LR and HR images. In contrast, in consumer photography, the LR image has substantially higher fidelity, requiring only minimal hallucination-free generation. We hypothesize that generation in SISR is controlled by the stringency and richness of the FM's conditioning feature. First, text features are high level features, which often cannot describe subtle textures in an image. Additionally, Smartphone LR images are at least $12MP$, whereas SISR networks built on T2IDiff FM are designed to perform inference on much smaller images ($<1MP$). As a result, SISR inference has to be performed on small patches, which often cannot be accurately described by text feature. To address these shortcomings, we introduce an SISR network built on a FM with lower-level feature conditioning, specifically DINOv2 features, which we call a Feature-to-Image Diffusion (F2IDiff) Foundation Model (FM). Lower level features provide stricter conditioning while being rich descriptors of even small patches.</li>
<li><strong>摘要：</strong>随着生成式 AI 的出现，单图像超分辨率 (SISR) 质量得到了显着改善，因为通过文本 2-图像扩散 (T2IDiff) 基础模型 (FM) 学习到的强先验可以弥合高分辨率 (HR) 和低分辨率 (LR) 图像之间的差距。然而，旗舰智能手机相机在采用生成模型方面进展缓慢，因为强大的生成可能会导致不良的幻觉。对于严重退化的 LR 图像，如学术界所见，需要强大的生成能力，并且由于 LR 和 HR 图像之间的巨大差距，幻觉更容易被容忍。相比之下，在消费者摄影中，LR 图像的保真度要高得多，只需要最少的无幻觉生成。我们假设 SISR 的生成是由 FM 调节功能的严格性和丰富性控制的。首先，文本特征是高级特征，通常无法描述图像中的微妙纹理。此外，智能手机 LR 图像至少为 12MP$，而基于 T2IDiff FM 构建的 SISR 网络旨在对更小的图像执行推理（$<1MP$）。因此，SISR 推理必须在小块上进行，而这些小块通常无法通过文本特征准确描述。为了解决这些缺点，我们引入了一个基于 FM 构建的 SISR 网络，具有较低级别的特征调节，特别是 DINOv2 特征，我们将其称为特征到图像扩散 (F2IDiff) 基础模型 (FM)。较低级别的特征提供更严格的调节，同时甚至是小补丁的丰富描述符。</li>
</ul>

<h3>Title: PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Yuanhao Cai, Kunpeng Li, Menglin Jia, Jialiang Wang, Junzhe Sun, Feng Liang, Weifeng Chen, Felix Juefei-Xu, Chu Wang, Ali Thabet, Xiaoliang Dai, Xuan Ju, Alan Yuille, Ji Hou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24551">https://arxiv.org/abs/2512.24551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24551">https://arxiv.org/pdf/2512.24551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24551]] PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation(https://arxiv.org/abs/2512.24551)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in text-to-video (T2V) generation have achieved good visual quality, yet synthesizing videos that faithfully follow physical laws remains an open challenge. Existing methods mainly based on graphics or prompt extension struggle to generalize beyond simple simulated environments or learn implicit physical reasoning. The scarcity of training data with rich physics interactions and phenomena is also a problem. In this paper, we first introduce a Physics-Augmented video data construction Pipeline, PhyAugPipe, that leverages a vision-language model (VLM) with chain-of-thought reasoning to collect a large-scale training dataset, PhyVidGen-135K. Then we formulate a principled Physics-aware Groupwise Direct Preference Optimization, PhyGDPO, framework that builds upon the groupwise Plackett-Luce probabilistic model to capture holistic preferences beyond pairwise comparisons. In PhyGDPO, we design a Physics-Guided Rewarding (PGR) scheme that embeds VLM-based physics rewards to steer optimization toward physical consistency. We also propose a LoRA-Switch Reference (LoRA-SR) scheme that eliminates memory-heavy reference duplication for efficient training. Experiments show that our method significantly outperforms state-of-the-art open-source methods on PhyGenBench and VideoPhy2. Please check our project page at this https URL for more video results. Our code, models, and data will be released at this https URL</li>
<li><strong>摘要：</strong>文本到视频（T2V）生成的最新进展已经实现了良好的视觉质量，但合成忠实遵循物理定律的视频仍然是一个开放的挑战。现有的主要基于图形或提示扩展的方法很难推广到简单的模拟环境之外或学习隐式物理推理。缺乏具有丰富物理相互作用和现象的训练数据也是一个问题。在本文中，我们首先介绍了一种物理增强视频数据构建管道 PhyAugPipe，它利用具有思维链推理的视觉语言模型 (VLM) 来收集大规模训练数据集 PhyVidGen-135K。然后，我们制定了一个有原则的物理感知分组直接偏好优化（PhyGDPO）框架，该框架建立在分组 Plackett-Luce 概率模型的基础上，以捕获超越成对比较的整体偏好。在 PhyGDPO 中，我们设计了一种物理引导奖励 (PGR) 方案，该方案嵌入基于 VLM 的物理奖励，以引导优化实现物理一致性。我们还提出了一种 LoRA-Switch Reference (LoRA-SR) 方案，该方案消除了内存繁重的参考重复，以实现高效训练。实验表明，我们的方法在 PhyGenBench 和 VideoPhy2 上显着优于最先进的开源方法。请通过此 https URL 查看我们的项目页面以获取更多视频结果。我们的代码、模型和数据将在此 https URL 发布</li>
</ul>

<h3>Title: From Perception to Punchline: Empowering VLM with the Art of In-the-wild Meme</h3>
<ul>
<li><strong>Authors: </strong>Xueyan Li, Yingyi Xue, Mengjie Jiang, Qingzi Zhu, Yazhe Niu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24555">https://arxiv.org/abs/2512.24555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24555">https://arxiv.org/pdf/2512.24555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24555]] From Perception to Punchline: Empowering VLM with the Art of In-the-wild Meme(https://arxiv.org/abs/2512.24555)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating humorous memes is a challenging multimodal task that moves beyond direct image-to-caption supervision. It requires a nuanced reasoning over visual content, contextual cues, and subjective humor. To bridge this gap between visual perception and humorous punchline creation, we propose HUMOR}, a novel framework that guides VLMs through hierarchical reasoning and aligns them with group-wise human preferences. First, HUMOR employs a hierarchical, multi-path Chain-of-Thought (CoT): the model begins by identifying a template-level intent, then explores diverse reasoning paths under different contexts, and finally anchors onto a high-quality, context-specific path. This CoT supervision, which traces back from ground-truth captions, enhances reasoning diversity. We further analyze that this multi-path exploration with anchoring maintains a high expected humor quality, under the practical condition that high-quality paths retain significant probability mass. Second, to capture subjective humor, we train a pairwise reward model that operates within groups of memes sharing the same template. Following established theory, this approach ensures a consistent and robust proxy for human preference, even with subjective and noisy labels. The reward model then enables a group-wise reinforcement learning optimization, guaranteeing providing a theoretical guarantee for monotonic improvement within the trust region. Extensive experiments show that HUMOR empowers various VLMs with superior reasoning diversity, more reliable preference alignment, and higher overall meme quality. Beyond memes, our work presents a general training paradigm for open-ended, human-aligned multimodal generation, where success is guided by comparative judgment within coherent output group.</li>
<li><strong>摘要：</strong>生成幽默模因是一项具有挑战性的多模式任务，它超越了直接图像到标题的监督。它需要对视觉内容、上下文线索和主观幽默进行细致入微的推理。为了弥合视觉感知和幽默妙语创作之间的差距，我们提出了 HUMOR}，这是一种新颖的框架，可以指导 VLM 进行分层推理，并使它们与按群体的人类偏好保持一致。首先，HUMOR 采用分层、多路径思想链 (CoT)：模型首先识别模板级意图，然后探索不同上下文下的不同推理路径，最后锚定到高质量、特定于上下文的路径。这种 CoT 监督可以追溯到真实的字幕，增强了推理的多样性。我们进一步分析，在高质量路径保留显着概率质量的实际条件下，这种带有锚定的多路径探索保持了较高的预期幽默质量。其次，为了捕捉主观幽默，我们训练了一个成对奖励模型，该模型在共享相同模板的模因组中运行。遵循既定的理论，这种方法确保了人类偏好的一致和稳健的代理，即使有主观和嘈杂的标签。然后，奖励模型可以实现分组强化学习优化，从而为信任区域内的单调改进提供理论保证。大量实验表明，HUMOR 使各种 VLM 具有卓越的推理多样性、更可靠的偏好对齐和更高的整体模因质量。除了模因之外，我们的工作还提出了一种开放式、以人为本的多模式生成的通用培训​​范式，其中成功是由连贯输出组内的比较判断来指导的。</li>
</ul>

<h3>Title: CPR: Causal Physiological Representation Learning for Robust ECG Analysis under Distribution Shifts</h3>
<ul>
<li><strong>Authors: </strong>Shunbo Jia, Caizhi Liao</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24564">https://arxiv.org/abs/2512.24564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24564">https://arxiv.org/pdf/2512.24564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24564]] CPR: Causal Physiological Representation Learning for Robust ECG Analysis under Distribution Shifts(https://arxiv.org/abs/2512.24564)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Deep learning models for Electrocardiogram (ECG) diagnosis have achieved remarkable accuracy but exhibit fragility against adversarial perturbations, particularly Smooth Adversarial Perturbations (SAP) that mimic biological morphology. Existing defenses face a critical dilemma: Adversarial Training (AT) provides robustness but incurs a prohibitive computational burden, while certified methods like Randomized Smoothing (RS) introduce significant inference latency, rendering them impractical for real-time clinical monitoring. We posit that this vulnerability stems from the models' reliance on non-robust spurious correlations rather than invariant pathological features. To address this, we propose Causal Physiological Representation Learning (CPR). Unlike standard denoising approaches that operate without semantic constraints, CPR incorporates a Physiological Structural Prior within a causal disentanglement framework. By modeling ECG generation via a Structural Causal Model (SCM), CPR enforces a structural intervention that strictly separates invariant pathological morphology (P-QRS-T complex) from non-causal artifacts. Empirical results on PTB-XL demonstrate that CPR significantly outperforms standard clinical preprocessing methods. Specifically, under SAP attacks, CPR achieves an F1 score of 0.632, surpassing Median Smoothing (0.541 F1) by 9.1%. Crucially, CPR matches the certified robustness of Randomized Smoothing while maintaining single-pass inference efficiency, offering a superior trade-off between robustness, efficiency, and clinical interpretability.</li>
<li><strong>摘要：</strong>用于心电图 (ECG) 诊断的深度学习模型已经取得了显着的准确性，但在对抗对抗性扰动方面表现出脆弱性，特别是模仿生物形态的平滑对抗性扰动 (SAP)。现有的防御措施面临着严峻的困境：对抗训练（AT）提供了鲁棒性，但会产生令人望而却步的计算负担，而随机平滑（RS）等经过认证的方法会引入显着的推理延迟，使得它们对于实时临床监测来说不切实际。我们认为这种脆弱性源于模型对非鲁棒虚假相关性的依赖，而不是不变的病理特征。为了解决这个问题，我们提出因果生理表征学习（CPR）。与不受语义限制的标准去噪方法不同，CPR 在因果解开框架内结合了生理结构先验。通过结构因果模型 (SCM) 对心电图生成进行建模，CPR 强制实施结构干预，将不变的病理形态（P-QRS-T 复合波）与非因果伪影严格分开。 PTB-XL 的经验结果表明，CPR 明显优于标准临床预处理方法。具体来说，在 SAP 攻击下，CPR 的 F1 得分为 0.632，超过 Median Smoothing (0.541 F1) 9.1%。至关重要的是，CPR 与随机平滑经过认证的稳健性相匹配，同时保持单遍推理效率，在稳健性、效率和临床可解释性之间提供卓越的权衡。</li>
</ul>

<h3>Title: From Sequential to Spatial: Reordering Autoregression for Efficient Visual Generation</h3>
<ul>
<li><strong>Authors: </strong>Siyang Wang, Hanting Li, Wei Li, Jie Hu, Xinghao Chen, Feng Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24639">https://arxiv.org/abs/2512.24639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24639">https://arxiv.org/pdf/2512.24639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24639]] From Sequential to Spatial: Reordering Autoregression for Efficient Visual Generation(https://arxiv.org/abs/2512.24639)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Inspired by the remarkable success of autoregressive models in language modeling, this paradigm has been widely adopted in visual generation. However, the sequential token-by-token decoding mechanism inherent in traditional autoregressive models leads to low inference this http URL this paper, we propose RadAR, an efficient and parallelizable framework designed to accelerate autoregressive visual generation while preserving its representational capacity. Our approach is motivated by the observation that visual tokens exhibit strong local dependencies and spatial correlations with their neighbors--a property not fully exploited in standard raster-scan decoding orders. Specifically, we organize the generation process around a radial topology: an initial token is selected as the starting point, and all other tokens are systematically grouped into multiple concentric rings according to their spatial distances from this center. Generation then proceeds in a ring-wise manner, from inner to outer regions, enabling the parallel prediction of all tokens within the same ring. This design not only preserves the structural locality and spatial coherence of visual scenes but also substantially increases parallelization. Furthermore, to address the risk of inconsistent predictions arising from simultaneous token generation with limited context, we introduce a nested attention mechanism. This mechanism dynamically refines implausible outputs during the forward pass, thereby mitigating error accumulation and preventing model collapse. By integrating radial parallel prediction with dynamic output correction, RadAR significantly improves generation efficiency.</li>
<li><strong>摘要：</strong>受到自回归模型在语言建模中取得的巨大成功的启发，这种范式已在视觉生成中被广泛采用。然而，传统自回归模型中固有的逐个令牌解码机制导致该 http URL 的推理能力较低。本文提出了 RadAR，这是一种高效且可并行的框架，旨在加速自回归视觉生成，同时保留其表示能力。我们的方法的动机是观察到视觉标记与它们的邻居表现出强烈的局部依赖性和空间相关性——这一特性在标准光栅扫描解码顺序中没有得到充分利用。具体来说，我们围绕径向拓扑组织生成过程：选择一个初始令牌作为起点，所有其他令牌根据距该中心的空间距离系统地分组为多个同心环。然后，生成从内部区域到外部区域以环方式进行，从而能够并行预测同一环内的所有令牌。这种设计不仅保留了视觉场景的结构局部性和空间连贯性，而且还大大提高了并行性。此外，为了解决在有限的上下文中同时生成令牌而导致预测不一致的风险，我们引入了嵌套注意机制。该机制在前向传递过程中动态地细化不可信的输出，从而减少错误累积并防止模型崩溃。通过将径向并行预测与动态输出校正相结合，RadAR 显着提高了发电效率。</li>
</ul>

<h3>Title: HeteroHBA: A Generative Structure-Manipulating Backdoor Attack on Heterogeneous Graphs</h3>
<ul>
<li><strong>Authors: </strong>Honglin Gao, Lan Zhao, Junhao Ren, Xiang Li, Gaoxi Xiao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24665">https://arxiv.org/abs/2512.24665</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24665">https://arxiv.org/pdf/2512.24665</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24665]] HeteroHBA: A Generative Structure-Manipulating Backdoor Attack on Heterogeneous Graphs(https://arxiv.org/abs/2512.24665)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Heterogeneous graph neural networks (HGNNs) have achieved strong performance in many real-world applications, yet targeted backdoor poisoning on heterogeneous graphs remains less studied. We consider backdoor attacks for heterogeneous node classification, where an adversary injects a small set of trigger nodes and connections during training to force specific victim nodes to be misclassified into an attacker-chosen label at test time while preserving clean performance. We propose HeteroHBA, a generative backdoor framework that selects influential auxiliary neighbors for trigger attachment via saliency-based screening and synthesizes diverse trigger features and connection patterns to better match the local heterogeneous context. To improve stealthiness, we combine Adaptive Instance Normalization (AdaIN) with a Maximum Mean Discrepancy (MMD) loss to align the trigger feature distribution with benign statistics, thereby reducing detectability, and we optimize the attack with a bilevel objective that jointly promotes attack success and maintains clean accuracy. Experiments on multiple real-world heterogeneous graphs with representative HGNN architectures show that HeteroHBA consistently achieves higher attack success than prior backdoor baselines with comparable or smaller impact on clean accuracy; moreover, the attack remains effective under our heterogeneity-aware structural defense, CSD. These results highlight practical backdoor risks in heterogeneous graph learning and motivate the development of stronger defenses.</li>
<li><strong>摘要：</strong>异构图神经网络（HGNN）在许多实际应用中取得了强大的性能，但异构图上有针对性的后门投毒的研究仍然较少。我们考虑针对异构节点分类的后门攻击，其中攻击者在训练期间注入一小组触发节点和连接，以迫使特定的受害者节点在测试时被错误分类到攻击者选择的标签中，同时保持干净的性能。我们提出了 HeteroHBA，一种生成式后门框架，通过基于显着性的筛选来选择有影响力的辅助邻居进行触发附加，并综合不同的触发特征和连接模式以更好地匹配本地异构上下文。为了提高隐蔽性，我们将自适应实例归一化 (AdaIN) 与最大平均差异 (MMD) 损失相结合，使触发特征分布与良性统计数据保持一致，从而降低可检测性，并且我们以双层目标优化攻击，共同促进攻击成功并保持干净的准确性。对具有代表性 HGNN 架构的多个真实世界异构图进行的实验表明，HeteroHBA 始终比之前的后门基线取得更高的攻击成功率，并且对干净精度的影响相当或更小；此外，在我们的异质性感知结构防御（CSD）下，攻击仍然有效。这些结果凸显了异构图学习中的实际后门风险，并激励了更强大的防御措施的开发。</li>
</ul>

<h3>Title: Mobility-Assisted Decentralized Federated Learning: Convergence Analysis and A Data-Driven Approach</h3>
<ul>
<li><strong>Authors: </strong>Reza Jahani, Md Farhamdur Reza, Richeng Jin, Huaiyu Dai</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24694">https://arxiv.org/abs/2512.24694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24694">https://arxiv.org/pdf/2512.24694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24694]] Mobility-Assisted Decentralized Federated Learning: Convergence Analysis and A Data-Driven Approach(https://arxiv.org/abs/2512.24694)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Decentralized Federated Learning (DFL) has emerged as a privacy-preserving machine learning paradigm that enables collaborative training among users without relying on a central server. However, its performance often degrades significantly due to limited connectivity and data heterogeneity. As we move toward the next generation of wireless networks, mobility is increasingly embedded in many real-world applications. The user mobility, either natural or induced, enables clients to act as relays or bridges, thus enhancing information flow in sparse networks; however, its impact on DFL has been largely overlooked despite its potential. In this work, we systematically investigate the role of mobility in improving DFL performance. We first establish the convergence of DFL in sparse networks under user mobility and theoretically demonstrate that even random movement of a fraction of users can significantly boost performance. Building upon this insight, we propose a DFL framework that utilizes mobile users with induced mobility patterns, allowing them to exploit the knowledge of data distribution to determine their trajectories to enhance information propagation through the network. Through extensive experiments, we empirically confirm our theoretical findings, validate the superiority of our approach over baselines, and provide a comprehensive analysis of how various network parameters influence DFL performance in mobile networks.</li>
<li><strong>摘要：</strong>去中心化联合学习 (DFL) 已成为一种保护隐私的机器学习范例，可以在不依赖中央服务器的情况下实现用户之间的协作训练。然而，由于连接性和数据异构性有限，其性能通常会显着下降。随着我们迈向下一代无线网络，移动性越来越多地嵌入到许多实际应用中。用户移动性，无论是自然的还是诱导的，使客户端能够充当中继或桥梁，从而增强稀疏网络中的信息流；然而，尽管它具有潜力，但它对 DFL 的影响却在很大程度上被忽视了。在这项工作中，我们系统地研究了流动性在提高 DFL 性能中的作用。我们首先在用户移动性的稀疏网络中建立 DFL 的收敛性，并从理论上证明，即使一小部分用户的随机移动也可以显着提高性能。基于这一见解，我们提出了一个 DFL 框架，该框架利用具有诱导移动模式的移动用户，使他们能够利用数据分布知识来确定其轨迹，从而增强通过网络的信息传播。通过大量的实验，我们凭经验证实了我们的理论发现，验证了我们的方法相对于基线的优越性，并对各种网络参数如何影响移动网络中的 DFL 性能进行了全面分析。</li>
</ul>

<h3>Title: FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Jibin Song, Mingi Kwon, Jaeseok Jeong, Youngjung Uh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24724">https://arxiv.org/abs/2512.24724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24724">https://arxiv.org/pdf/2512.24724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24724]] FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation(https://arxiv.org/abs/2512.24724)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In this work, we show that the impact of model capacity varies across timesteps: it is crucial for the early and late stages but largely negligible during the intermediate stage. Accordingly, we propose FlowBlending, a stage-aware multi-model sampling strategy that employs a large model and a small model at capacity-sensitive stages and intermediate stages, respectively. We further introduce simple criteria to choose stage boundaries and provide a velocity-divergence analysis as an effective proxy for identifying capacity-sensitive regions. Across LTX-Video (2B/13B) and WAN 2.1 (1.3B/14B), FlowBlending achieves up to 1.65x faster inference with 57.35% fewer FLOPs, while maintaining the visual fidelity, temporal coherence, and semantic alignment of the large models. FlowBlending is also compatible with existing sampling-acceleration techniques, enabling up to 2x additional speedup. Project page is available at: this https URL.</li>
<li><strong>摘要：</strong>在这项工作中，我们表明模型容量的影响随时间步长的不同而变化：它对于早期和晚期阶段至关重要，但在中间阶段基本上可以忽略不计。因此，我们提出了 FlowBlending，一种阶段感知的多模型采样策略，分别在容量敏感阶段和中间阶段采用大模型和小模型。我们进一步引入简单的标准来选择阶段边界，并提供速度发散分析作为识别容量敏感区域的有效代理。在 LTX-Video (2B/13B) 和 WAN 2.1 (1.3B/14B) 中，FlowBlending 的推理速度提高了 1.65 倍，失败次数减少了 57.35%，同时保持了大型模型的视觉保真度、时间连贯性和语义对齐。 FlowBlending 还与现有的采样加速技术兼容，可实现高达 2 倍的额外加速。项目页面位于：此 https URL。</li>
</ul>

<h3>Title: EchoFoley: Event-Centric Hierarchical Control for Video Grounded Creative Sound Generation</h3>
<ul>
<li><strong>Authors: </strong>Bingxuan Li, Yiming Cui, Yicheng He, Yiwei Wang, Shu Zhang, Longyin Wen, Yulei Niu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24731">https://arxiv.org/abs/2512.24731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24731">https://arxiv.org/pdf/2512.24731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24731]] EchoFoley: Event-Centric Hierarchical Control for Video Grounded Creative Sound Generation(https://arxiv.org/abs/2512.24731)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Sound effects build an essential layer of multimodal storytelling, shaping the emotional atmosphere and the narrative semantics of videos. Despite recent advancement in video-text-to-audio (VT2A), the current formulation faces three key limitations: First, an imbalance between visual and textual conditioning that leads to visual dominance; Second, the absence of a concrete definition for fine-grained controllable generation; Third, weak instruction understanding and following, as existing datasets rely on brief categorical tags. To address these limitations, we introduce EchoFoley, a new task designed for video-grounded sound generation with both event level local control and hierarchical semantic control. Our symbolic representation for sounding events specifies when, what, and how each sound is produced within a video or instruction, enabling fine-grained controls like sound generation, insertion, and editing. To support this task, we construct EchoFoley-6k, a large-scale, expert-curated benchmark containing over 6,000 video-instruction-annotation triplets. Building upon this foundation, we propose EchoVidia a sounding-event-centric agentic generation framework with slow-fast thinking strategy. Experiments show that EchoVidia surpasses recent VT2A models by 40.7% in controllability and 12.5% in perceptual quality.</li>
<li><strong>摘要：</strong>音效构建了多模式叙事的重要层，塑造了视频的情感氛围和叙事语义。尽管视频文本到音频（VT2A）最近取得了进展，但当前的表述面临三个关键限制：首先，视觉和文本调节之间的不平衡导致视觉优势；其次，细粒度可控发电缺乏具体定义；第三，指令理解和遵循能力薄弱，因为现有数据集依赖于简短的分类标签。为了解决这些限制，我们引入了 EchoFoley，这是一项专为基于视频的声音生成而设计的新任务，具有事件级本地控制和分层语义控制。我们对发声事件的符号表示指定了视频或指令中每个声音产生的时间、内容和方式，从而实现声音生成、插入和编辑等细粒度控制。为了支持这项任务，我们构建了 EchoFoley-6k，这是一个由专家策划的大型基准测试，包含 6,000 多个视频-指令-注释三元组。在此基础上，我们提出了 EchoVidia 一个以声音事件为中心的代理生成框架，具有慢速思维策略。实验表明，EchoVidia 在可控性方面超越了最新的 VT2A 模型 40.7%，在感知质量方面超越了 12.5%。</li>
</ul>

<h3>Title: AODDiff: Probabilistic Reconstruction of Aerosol Optical Depth via Diffusion-based Bayesian Inference</h3>
<ul>
<li><strong>Authors: </strong>Linhao Fan, Hongqiang Fang, Jingyang Dai, Yong Jiang, Qixing Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.ao-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24847">https://arxiv.org/abs/2512.24847</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24847">https://arxiv.org/pdf/2512.24847</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24847]] AODDiff: Probabilistic Reconstruction of Aerosol Optical Depth via Diffusion-based Bayesian Inference(https://arxiv.org/abs/2512.24847)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>High-quality reconstruction of Aerosol Optical Depth (AOD) fields is critical for Atmosphere monitoring, yet current models remain constrained by the scarcity of complete training data and a lack of uncertainty this http URL address these limitations, we propose AODDiff, a probabilistic reconstruction framework based on diffusion-based Bayesian inference. By leveraging the learned spatiotemporal probability distribution of the AOD field as a generative prior, this framework can be flexibly adapted to various reconstruction tasks without requiring task-specific retraining. We first introduce a corruption-aware training strategy to learns a spatiotemporal AOD prior solely from naturally incomplete data. Subsequently, we employ a decoupled annealing posterior sampling strategy that enables the more effective and integration of heterogeneous observations as constraints to guide the generation process. We validate the proposed framework through extensive experiments on Reanalysis data. Results across downscaling and inpainting tasks confirm the efficacy and robustness of AODDiff, specifically demonstrating its advantage in maintaining high spatial spectral fidelity. Furthermore, as a generative model, AODDiff inherently enables uncertainty quantification via multiple sampling, offering critical confidence metrics for downstream applications.</li>
<li><strong>摘要：</strong>气溶胶光学深度 (AOD) 场的高质量重建对于大气监测至关重要，但当前的模型仍然受到完整训练数据的稀缺性和缺乏不确定性的限制。此 http URL 解决了这些限制，我们提出了 AODDiff，一种基于扩散贝叶斯推理的概率重建框架。通过利用学习到的 AOD 场的时空概率分布作为生成先验，该框架可以灵活地适应各种重建任务，而不需要特定于任务的重新训练。我们首先引入一种腐败感知训练策略，仅从自然不完整的数据中学习时空 AOD 先验。随后，我们采用解耦退火后验采样策略，使异质观测结果更有效和集成，作为指导生成过程的约束。我们通过对再分析数据进行大量实验来验证所提出的框架。缩小尺度和修复任务的结果证实了 AODDiff 的有效性和鲁棒性，特别证明了其在保持高空间光谱保真度方面的优势。此外，作为一种生成模型，AODDiff 本质上可以通过多次采样实现不确定性量化，为下游应用程序提供关键的置信度指标。</li>
</ul>

<h3>Title: FinMMDocR: Benchmarking Financial Multimodal Reasoning with Scenario Awareness, Document Understanding, and Multi-Step Computation</h3>
<ul>
<li><strong>Authors: </strong>Zichen Tang, Haihong E, Rongjin Li, Jiacheng Liu, Linwei Jia, Zhuodi Hao, Zhongjun Yang, Yuanze Li, Haolin Tian, Xinyi Hu, Peizhi Zhao, Yuan Liu, Zhengyu Wang, Xianghe Wang, Yiling Huang, Xueyuan Lin, Ruofei Bai, Zijian Xie, Qian Huang, Ruining Cao, Haocheng Gao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24903">https://arxiv.org/abs/2512.24903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24903">https://arxiv.org/pdf/2512.24903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24903]] FinMMDocR: Benchmarking Financial Multimodal Reasoning with Scenario Awareness, Document Understanding, and Multi-Step Computation(https://arxiv.org/abs/2512.24903)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce FinMMDocR, a novel bilingual multimodal benchmark for evaluating multimodal large language models (MLLMs) on real-world financial numerical reasoning. Compared to existing benchmarks, our work delivers three major advancements. (1) Scenario Awareness: 57.9% of 1,200 expert-annotated problems incorporate 12 types of implicit financial scenarios (e.g., Portfolio Management), challenging models to perform expert-level reasoning based on assumptions; (2) Document Understanding: 837 Chinese/English documents spanning 9 types (e.g., Company Research) average 50.8 pages with rich visual elements, significantly surpassing existing benchmarks in both breadth and depth of financial documents; (3) Multi-Step Computation: Problems demand 11-step reasoning on average (5.3 extraction + 5.7 calculation steps), with 65.0% requiring cross-page evidence (2.4 pages average). The best-performing MLLM achieves only 58.0% accuracy, and different retrieval-augmented generation (RAG) methods show significant performance variations on this task. We expect FinMMDocR to drive improvements in MLLMs and reasoning-enhanced methods on complex multimodal reasoning tasks in real-world scenarios.</li>
<li><strong>摘要：</strong>我们推出 FinMMDocR，这是一种新颖的双语多模态基准，用于评估现实世界金融数值推理的多模态大语言模型 (MLLM)。与现有基准相比，我们的工作实现了三项重大进步。 （1）场景意识：1200个专家注释问题中，57.9%包含12种隐含金融场景（例如投资组合管理），挑战模型基于假设进行专家级推理； （2）文档理解：涵盖公司研究等9个类型的837篇中英文文档，平均50.8页，视觉元素丰富，在广度和深度上均大幅超越现有财务文档基准； （3）多步计算：问题平均需要11步推理（5.3步提取+5.7步计算），其中65.0%需要跨页证据（平均2.4页）。性能最好的 MLLM 仅达到 58.0% 的准确率，并且不同的检索增强生成 (RAG) 方法在此任务上显示出显着的性能差异。我们期望 FinMMDocR 能够推动 MLLM 和现实场景中复杂多模态推理任务的推理增强方法的改进。</li>
</ul>

<h3>Title: HaineiFRDM: Explore Diffusion to Restore Defects in Fast-Movement Films</h3>
<ul>
<li><strong>Authors: </strong>Rongji Xun, Junjie Yuan, Zhongjie Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24946">https://arxiv.org/abs/2512.24946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24946">https://arxiv.org/pdf/2512.24946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24946]] HaineiFRDM: Explore Diffusion to Restore Defects in Fast-Movement Films(https://arxiv.org/abs/2512.24946)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Existing open-source film restoration methods show limited performance compared to commercial methods due to training with low-quality synthetic data and employing noisy optical flows. In addition, high-resolution films have not been explored by the open-source this http URL propose HaineiFRDM(Film Restoration Diffusion Model), a film restoration framework, to explore diffusion model's powerful content-understanding ability to help human expert better restore indistinguishable film this http URL, we employ a patch-wise training and testing strategy to make restoring high-resolution films on one 24GB-VRAMR GPU possible and design a position-aware Global Prompt and Frame Fusion this http URL, we introduce a global-local frequency module to reconstruct consistent textures among different patches. Besides, we firstly restore a low-resolution result and use it as global residual to mitigate blocky artifacts caused by patching this http URL, we construct a film restoration dataset that contains restored real-degraded films and realistic synthetic this http URL experimental results conclusively demonstrate the superiority of our model in defect restoration ability over existing open-source methods. Code and the dataset will be released.</li>
<li><strong>摘要：</strong>由于使用低质量的合成数据进行训练并采用噪声光流，现有的开源胶片修复方法与商业方法相比表现出有限的性能。此外，开源这个http URL还没有探索过高分辨率电影，提出了HaineiFRDM(Film Restoration Diffusion Model)，一个电影修复框架，为了探索扩散模型强大的内容理解能力，帮助人类专家更好地恢复难以区分的电影这个http URL，我们采用了补丁式的训练和测试策略，使得在一个24GB-VRAMR GPU上恢复高分辨率电影成为可能，并设计了一个位置感知的全局提示和帧融合这个http URL URL，我们引入了全局局部频率模块来重建不同补丁之间一致的纹理。此外，我们首先恢复低分辨率结果并将其用作全局残差，以减轻因修补此http URL而导致的块状伪影，我们构建了包含恢复的真实退化胶片的胶片恢复数据集，并真实合成此http URL实验结果最终证明了我们的模型在缺陷恢复能力方面优于现有开源方法。代码和数据集将被发布。</li>
</ul>

<h3>Title: ProDM: Synthetic Reality-driven Property-aware Progressive Diffusion Model for Coronary Calcium Motion Correction in Non-gated Chest CT</h3>
<ul>
<li><strong>Authors: </strong>Xinran Gong, Gorkem Durak, Halil Ertugrul Aktas, Vedat Cicek, Jinkui Hao, Ulas Bagci, Nilay S. Shah, Bo Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24948">https://arxiv.org/abs/2512.24948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24948">https://arxiv.org/pdf/2512.24948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24948]] ProDM: Synthetic Reality-driven Property-aware Progressive Diffusion Model for Coronary Calcium Motion Correction in Non-gated Chest CT(https://arxiv.org/abs/2512.24948)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Coronary artery calcium (CAC) scoring from chest CT is a well-established tool to stratify and refine clinical cardiovascular disease risk estimation. CAC quantification relies on the accurate delineation of calcified lesions, but is oftentimes affected by artifacts introduced by cardiac and respiratory motion. ECG-gated cardiac CTs substantially reduce motion artifacts, but their use in population screening and routine imaging remains limited due to gating requirements and lack of insurance coverage. Although identification of incidental CAC from non-gated chest CT is increasingly considered for it offers an accessible and widely available alternative, this modality is limited by more severe motion artifacts. We present ProDM (Property-aware Progressive Correction Diffusion Model), a generative diffusion framework that restores motion-free calcified lesions from non-gated CTs. ProDM introduces three key components: (1) a CAC motion simulation data engine that synthesizes realistic non-gated acquisitions with diverse motion trajectories directly from cardiac-gated CTs, enabling supervised training without paired data; (2) a property-aware learning strategy incorporating calcium-specific priors through a differentiable calcium consistency loss to preserve lesion integrity; and (3) a progressive correction scheme that reduces artifacts gradually across diffusion steps to enhance stability and calcium fidelity. Experiments on real patient datasets show that ProDM significantly improves CAC scoring accuracy, spatial lesion fidelity, and risk stratification performance compared with several baselines. A reader study on real non-gated scans further confirms that ProDM suppresses motion artifacts and improves clinical usability. These findings highlight the potential of progressive, property-aware frameworks for reliable CAC quantification from routine chest CT imaging.</li>
<li><strong>摘要：</strong>胸部 CT 冠状动脉钙 (CAC) 评分是分层和完善临床心血管疾病风险评估的成熟工具。 CAC 定量依赖于钙化病变的准确描绘，但常常受到心脏和呼吸运动引入的伪影的影响。心电图门控心脏 CT 大大减少了运动伪影，但由于门控要求和缺乏保险覆盖范围，其在人群筛查和常规成像中的使用仍然受到限制。尽管越来越多地考虑从非门控胸部 CT 中识别偶然的 CAC，因为它提供了一种易于使用且广泛使用的替代方案，但这种方式受到更严重的运动伪影的限制。我们提出了 ProDM（属性感知渐进校正扩散模型），这是一种生成扩散框架，可从非门控 CT 恢复无运动钙化病变。 ProDM 引入了三个关键组件：(1) CAC 运动模拟数据引擎，可直接从心脏门控 CT 合成真实的非门控采集数据和不同的运动轨迹，从而无需配对数据即可进行监督训练； （2）属性感知学习策略，通过可微的钙一致性损失结合钙特定先验，以保持病变完整性； (3) 渐进式校正方案，逐渐减少扩散步骤中的伪影，以增强稳定性和钙保真度。对真实患者数据集的实验表明，与多个基线相比，ProDM 显着提高了 CAC 评分准确性、空间病变保真度和风险分层性能。对真实非门控扫描的读者研究进一步证实 ProDM 可以抑制运动伪影并提高临床可用性。这些发现凸显了渐进式、属性感知框架在常规胸部 CT 成像中可靠量化 CAC 的潜力。</li>
</ul>

<h3>Title: VIPER: Process-aware Evaluation for Generative Video Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yifan Li, Yukai Gu, Yingqian Min, Zikang Liu, Yifan Du, Kun Zhou, Min Yang, Wayne Xin Zhao, Minghui Qiu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24952">https://arxiv.org/abs/2512.24952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24952">https://arxiv.org/pdf/2512.24952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24952]] VIPER: Process-aware Evaluation for Generative Video Reasoning(https://arxiv.org/abs/2512.24952)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent breakthroughs in video generation have demonstrated an emerging capability termed Chain-of-Frames (CoF) reasoning, where models resolve complex tasks through the generation of continuous frames. While these models show promise for Generative Video Reasoning (GVR), existing evaluation frameworks often rely on single-frame assessments, which can lead to outcome-hacking, where a model reaches a correct conclusion through an erroneous process. To address this, we propose a process-aware evaluation paradigm. We introduce VIPER, a comprehensive benchmark spanning 16 tasks across temporal, structural, symbolic, spatial, physics, and planning reasoning. Furthermore, we propose Process-outcome Consistency (POC@r), a new metric that utilizes VLM-as-Judge with a hierarchical rubric to evaluate both the validity of the intermediate steps and the final result. Our experiments reveal that state-of-the-art video models achieve only about 20% POC@1.0 and exhibit a significant outcome-hacking. We further explore the impact of test-time scaling and sampling robustness, highlighting a substantial gap between current video generation and true generalized visual reasoning. Our benchmark will be publicly released.</li>
<li><strong>摘要：</strong>视频生成领域的最新突破展示了一种称为帧链 (CoF) 推理的新兴功能，其中模型通过生成连续帧来解决复杂的任务。虽然这些模型显示出生成视频推理 (GVR) 的前景，但现有的评估框架通常依赖于单帧评估，这可能会导致结果黑客攻击，即模型通过错误的过程得出正确的结论。为了解决这个问题，我们提出了一种流程感知的评估范式。我们推出了 VIPER，这是一个涵盖时间、结构、符号、空间、物理和规划推理等 16 项任务的综合基准测试。此外，我们提出了过程结果一致性（POC@r），这是一种新的指标，利用带有分层标题的 VLM-as-Judge 来评估中间步骤和最终结果的有效性。我们的实验表明，最先进的视频模型仅实现了约 20% POC@1.0，并且表现出显着的结果黑客攻击。我们进一步探讨了测试时间缩放和采样鲁棒性的影响，强调了当前视频生成和真正的广义视觉推理之间的巨大差距。我们的基准将公开发布。</li>
</ul>

<h3>Title: ShowUI-$π$: Flow-based Generative Models as GUI Dexterous Hands</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Hu, Kevin Qinghong Lin, Mike Zheng Shou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.24965">https://arxiv.org/abs/2512.24965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.24965">https://arxiv.org/pdf/2512.24965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.24965]] ShowUI-$π$: Flow-based Generative Models as GUI Dexterous Hands(https://arxiv.org/abs/2512.24965)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Building intelligent agents capable of dexterous manipulation is essential for achieving human-like automation in both robotics and digital environments. However, existing GUI agents rely on discrete click predictions (x,y), which prohibits free-form, closed-loop trajectories (e.g. dragging a progress bar) that require continuous, on-the-fly perception and adjustment. In this work, we develop ShowUI-$\pi$, the first flow-based generative model as GUI dexterous hand, featuring the following designs: (i) Unified Discrete-Continuous Actions, integrating discrete clicks and continuous drags within a shared model, enabling flexible adaptation across diverse interaction modes; (ii) Flow-based Action Generation for drag modeling, which predicts incremental cursor adjustments from continuous visual observations via a lightweight action expert, ensuring smooth and stable trajectories; (iii) Drag Training data and Benchmark, where we manually collect and synthesize 20K drag trajectories across five domains (e.g. PowerPoint, Adobe Premiere Pro), and introduce ScreenDrag, a benchmark with comprehensive online and offline evaluation protocols for assessing GUI agents' drag capabilities. Our experiments show that proprietary GUI agents still struggle on ScreenDrag (e.g. Operator scores 13.27, and the best Gemini-2.5-CUA reaches 22.18). In contrast, ShowUI-$\pi$ achieves 26.98 with only 450M parameters, underscoring both the difficulty of the task and the effectiveness of our approach. We hope this work advances GUI agents toward human-like dexterous control in digital world. The code is available at this https URL.</li>
<li><strong>摘要：</strong>构建能够灵巧操作的智能代理对于在机器人和数字环境中实现类人自动化至关重要。然而，现有的 GUI 代理依赖于离散点击预测 (x,y)，这禁止了需要连续、即时感知和调整的自由形式、闭环轨迹（例如拖动进度条）。在这项工作中，我们开发了ShowUI-$\pi$，第一个基于流的GUI灵​​巧手生成模型，具有以下设计：（i）统一离散连续操作，将离散点击和连续拖动集成在共享模型中，实现跨不同交互模式的灵活适应； (ii) 用于拖动建模的基于流的动作生成，它通过轻量级动作专家从连续视觉观察中预测增量光标调整，确保平滑和稳定的轨迹； (iii) 拖动训练数据和基准，我们手动收集和合成跨五个领域（例如 PowerPoint、Adobe Premiere Pro）的 20K 拖动轨迹，并引入 ScreenDrag，这是一个具有全面的在线和离线评估协议的基准，用于评估 GUI 代理的拖动能力。我们的实验表明，专有的 GUI 代理在 ScreenDrag 上仍然表现不佳（例如 Operator 得分 13.27，最好的 Gemini-2.5-CUA 达到 22.18）。相比之下，ShowUI-$\pi$ 仅用 450M 参数就达到了 26.98，这凸显了任务的难度和我们方法的有效性。我们希望这项工作能够推动 GUI 智能体在数字世界中实现类似人类的灵巧控制。该代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Diffusion Language Models are Provably Optimal Parallel Samplers</h3>
<ul>
<li><strong>Authors: </strong>Haozhe Jiang, Nika Haghtalab, Lijie Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.25014">https://arxiv.org/abs/2512.25014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.25014">https://arxiv.org/pdf/2512.25014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.25014]] Diffusion Language Models are Provably Optimal Parallel Samplers(https://arxiv.org/abs/2512.25014)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion language models (DLMs) have emerged as a promising alternative to autoregressive models for faster inference via parallel token generation. We provide a rigorous foundation for this advantage by formalizing a model of parallel sampling and showing that DLMs augmented with polynomial-length chain-of-thought (CoT) can simulate any parallel sampling algorithm using an optimal number of sequential steps. Consequently, whenever a target distribution can be generated using a small number of sequential steps, a DLM can be used to generate the distribution using the same number of optimal sequential steps. However, without the ability to modify previously revealed tokens, DLMs with CoT can still incur large intermediate footprints. We prove that enabling remasking (converting unmasked tokens to masks) or revision (converting unmasked tokens to other unmasked tokens) together with CoT further allows DLMs to simulate any parallel sampling algorithm with optimal space complexity. We further justify the advantage of revision by establishing a strict expressivity gap: DLMs with revision or remasking are strictly more expressive than those without. Our results not only provide a theoretical justification for the promise of DLMs as the most efficient parallel sampler, but also advocate for enabling revision in DLMs.</li>
<li><strong>摘要：</strong>扩散语言模型 (DLM) 已成为自回归模型的一种有前景的替代方案，可通过并行标记生成实现更快的推理。我们通过形式化并行采样模型并证明用多项式长度思想链 (CoT) 增强的 DLM 可以使用最佳数量的连续步骤来模拟任何并行采样算法，从而为这一优势提供了严格的基础。因此，只要可以使用少量连续步骤生成目标分布，就可以使用 DLM 来使用相同数量的最佳连续步骤生成分布。然而，如果无法修改之前公开的代币，带有 CoT 的 DLM 仍然会产生大量的中间足迹。我们证明，启用重新屏蔽（将未屏蔽的令牌转换为屏蔽）或修订（将未屏蔽的令牌转换为其他未屏蔽的令牌）与 CoT 一起进一步允许 DLM 模拟具有最佳空间复杂度的任何并行采样算法。我们通过建立严格的表现力差距进一步证明修订的优势：经过修订或重新屏蔽的 DLM 严格来说比没有修订或重新屏蔽的 DLM 更具表现力。我们的结果不仅为 DLM 作为最高效的并行采样器的承诺提供了理论依据，而且还提倡对 DLM 进行修订。</li>
</ul>

<h3>Title: Generative Classifiers Avoid Shortcut Solutions</h3>
<ul>
<li><strong>Authors: </strong>Alexander C. Li, Ananya Kumar, Deepak Pathak</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.25034">https://arxiv.org/abs/2512.25034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.25034">https://arxiv.org/pdf/2512.25034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.25034]] Generative Classifiers Avoid Shortcut Solutions(https://arxiv.org/abs/2512.25034)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Discriminative approaches to classification often learn shortcuts that hold in-distribution but fail even under minor distribution shift. This failure mode stems from an overreliance on features that are spuriously correlated with the label. We show that generative classifiers, which use class-conditional generative models, can avoid this issue by modeling all features, both core and spurious, instead of mainly spurious ones. These generative classifiers are simple to train, avoiding the need for specialized augmentations, strong regularization, extra hyperparameters, or knowledge of the specific spurious correlations to avoid. We find that diffusion-based and autoregressive generative classifiers achieve state-of-the-art performance on five standard image and text distribution shift benchmarks and reduce the impact of spurious correlations in realistic applications, such as medical or satellite datasets. Finally, we carefully analyze a Gaussian toy setting to understand the inductive biases of generative classifiers, as well as the data properties that determine when generative classifiers outperform discriminative ones.</li>
<li><strong>摘要：</strong>区分性分类方法通常会学习保持分布内的捷径，但即使在较小的分布变化下也会失败。这种故障模式源于对与标签虚假相关的特征的过度依赖。我们证明，使用类条件生成模型的生成分类器可以通过对所有特征（包括核心特征和虚假特征）而不是主要是虚假特征进行建模来避免这个问题。这些生成分类器易于训练，无需专门的增强、强正则化、额外的超参数或了解要避免的特定虚假相关性。我们发现基于扩散和自回归生成分类器在五个标准图像和文本分布变化基准上实现了最先进的性能，并减少了实际应用（例如医疗或卫星数据集）中虚假相关性的影响。最后，我们仔细分析高斯玩具设置，以了解生成分类器的归纳偏差，以及确定生成分类器何时优于判别分类器的数据属性。</li>
</ul>

<h3>Title: Many Minds from One Model: Bayesian Transformers for Population Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Diji Yang, Yi Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.25063">https://arxiv.org/abs/2512.25063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.25063">https://arxiv.org/pdf/2512.25063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.25063]] Many Minds from One Model: Bayesian Transformers for Population Intelligence(https://arxiv.org/abs/2512.25063)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite their scale and success, modern transformers are almost universally trained as single-minded systems: optimization produces one deterministic set of parameters, representing a single functional hypothesis about the data. Motivated by the idea that intelligence emerge from many minds, we propose Population Bayesian Transformers (B-Trans), which transform a standard Large Language Model into a Bayesian Transformer model to supports sampling diverse yet coherent model instances from a single set of pre-trained weights. B-Trans introduces a Bayesian-motivated posterior proxy by treating the bias-like offsets in normalization layers as stochastic variables with a Gaussian variational approximation, inducing a distribution over model behavior without the cost of training full Bayesian neural networks. Sampling from this proxy yields a set of model instances with diverse behaviors while maintaining general competence. To preserve coherence within each generation, we freeze the sampled noise at the sequence level, enforcing temporal consistency across tokens. B-Trans allows for population-level decision-making, where aggregating predictions across sampled individuals significantly enhances exploration. Experiments across zero-shot generation, Reinforcement Learning with Verifiable Rewards (RLVR), and RL without explicit labels demonstrate that B-Trans effectively leverage the wisdom of crowds, yielding superior semantic diversity while achieving better task performance compared to deterministic baselines.</li>
<li><strong>摘要：</strong>尽管现代变压器规模庞大且成功，但它们几乎普遍被训练为单一系统：优化产生一组确定性参数，代表关于数据的单一功能假设。受智力来自于许多头脑的想法的启发，我们提出了群体贝叶斯变换器（B-Trans），它将标准大型语言模型转换为贝叶斯变换器模型，以支持从一组预训练权重中采样不同但连贯的模型实例。 B-Trans 通过将归一化层中的类似偏差的偏移视为具有高斯变分近似的随机变量，引入了贝叶斯驱动的后验代理，从而在无需训练完整贝叶斯神经网络的成本的情况下诱导模型行为的分布。从该代理中采样会产生一组具有不同行为的模型实例，同时保持一般能力。为了保持每一代内的一致性，我们在序列级别冻结采样噪声，从而强制跨令牌的时间一致性。 B-Trans 允许进行群体层面的决策，其中汇总样本个体的预测可以显着增强探索。零样本生成、可验证奖励的强化学习（RLVR）和无显式标签的强化学习的实验表明，B-Trans 有效地利用了群体的智慧，产生了卓越的语义多样性，同时与确定性基线相比实现了更好的任务性能。</li>
</ul>

<h3>Title: Scaling Open-Ended Reasoning to Predict the Future</h3>
<ul>
<li><strong>Authors: </strong>Nikhil Chandak, Shashwat Goel, Ameya Prabhu, Moritz Hardt, Jonas Geiping</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.25070">https://arxiv.org/abs/2512.25070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.25070">https://arxiv.org/pdf/2512.25070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.25070]] Scaling Open-Ended Reasoning to Predict the Future(https://arxiv.org/abs/2512.25070)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster 8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible.</li>
<li><strong>摘要：</strong>高风险决策涉及在未来不确定的情况下进行推理。在这项工作中，我们训练语言模型来对开放式预测问题进行预测。为了扩大训练数据的规模，我们使用全自动、精心的管理方法，从每日新闻报道的全球事件中综合出新颖的预测问题。我们在我们的数据集 OpenForesight 上训练 Qwen3 思维模型。为了防止在训练和评估过程中泄露未来信息，我们使用离线新闻语料库，用于预测系统中的数据生成和检索。在一个小型验证集的指导下，我们展示了检索的好处以及强化学习（RL）的改进奖励函数。一旦我们获得最终的预测系统，我们就会在 2025 年 5 月至 8 月之间进行持续测试。我们的专业模型 OpenForecaster 8B 与更大的专有模型相匹配，我们的培训提高了预测的准确性、校准和一致性。我们发现预测训练的校准改进可以推广到流行的基准测试中。我们开源所有模型、代码和数据，以使语言模型预测的研究能够被广泛使用。</li>
</ul>

<h3>Title: SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time</h3>
<ul>
<li><strong>Authors: </strong>Zhening Huang, Hyeonho Jeong, Xuelin Chen, Yulia Gryaditskaya, Tuanfeng Y. Wang, Joan Lasenby, Chun-Hao Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.25075">https://arxiv.org/abs/2512.25075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.25075">https://arxiv.org/pdf/2512.25075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.25075]] SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time(https://arxiv.org/abs/2512.25075)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present SpaceTimePilot, a video diffusion model that disentangles space and time for controllable generative rendering. Given a monocular video, SpaceTimePilot can independently alter the camera viewpoint and the motion sequence within the generative process, re-rendering the scene for continuous and arbitrary exploration across space and time. To achieve this, we introduce an effective animation time-embedding mechanism in the diffusion process, allowing explicit control of the output video's motion sequence with respect to that of the source video. As no datasets provide paired videos of the same dynamic scene with continuous temporal variations, we propose a simple yet effective temporal-warping training scheme that repurposes existing multi-view datasets to mimic temporal differences. This strategy effectively supervises the model to learn temporal control and achieve robust space-time disentanglement. To further enhance the precision of dual control, we introduce two additional components: an improved camera-conditioning mechanism that allows altering the camera from the first frame, and CamxTime, the first synthetic space-and-time full-coverage rendering dataset that provides fully free space-time video trajectories within a scene. Joint training on the temporal-warping scheme and the CamxTime dataset yields more precise temporal control. We evaluate SpaceTimePilot on both real-world and synthetic data, demonstrating clear space-time disentanglement and strong results compared to prior work. Project page: this https URL Code: this https URL</li>
<li><strong>摘要：</strong>我们提出了 SpaceTimePilot，这是一种视频扩散模型，可以解开空间和时间以实现可控的生成渲染。给定单目视频，SpaceTimePilot 可以在生成过程中独立改变摄像机视点和运动序列，重新渲染场景，以实现跨空间和时间的连续和任意探索。为了实现这一目标，我们在扩散过程中引入了一种有效的动画时间嵌入机制，允许对输出视频相对于源视频的运动序列进行显式控制。由于没有数据集提供具有连续时间变化的同一动态场景的配对视频，我们提出了一种简单而有效的时间扭曲训练方案，该方案重新利用现有的多视图数据集来模拟时间差异。该策略有效地监督模型学习时间控制并实现鲁棒的时空解缠结。为了进一步提高双重控制的精度，我们引入了两个额外的组件：改进的相机调节机制，允许从第一帧开始改变相机，以及CamxTime，第一个合成时空全覆盖渲染数据集，可在场景内提供完全自由的时空视频轨迹。对时间扭曲方案和 CamxTime 数据集的联合训练可以产生更精确的时间控制。我们在现实世界和合成数据上评估了 SpaceTimePilot，与之前的工作相比，展示了清晰的时空解离和强大的结果。项目页面：此 https URL 代码：此 https URL</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
