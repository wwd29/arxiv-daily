<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-07-10</h1>
<h3>Title: Advancing Offline Handwritten Text Recognition: A Systematic Review of Data Augmentation and Generation Techniques</h3>
<ul>
<li><strong>Authors: </strong>Yassin Hussein Rassul, Aram M. Ahmed, Polla Fattah, Bryar A. Hassan, Arwaa W. Abdulkareem, Tarik A. Rashid, Joan Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06275">https://arxiv.org/abs/2507.06275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06275">https://arxiv.org/pdf/2507.06275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06275]] Advancing Offline Handwritten Text Recognition: A Systematic Review of Data Augmentation and Generation Techniques(https://arxiv.org/abs/2507.06275)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Offline Handwritten Text Recognition (HTR) systems play a crucial role in applications such as historical document digitization, automatic form processing, and biometric authentication. However, their performance is often hindered by the limited availability of annotated training data, particularly for low-resource languages and complex scripts. This paper presents a comprehensive survey of offline handwritten data augmentation and generation techniques designed to improve the accuracy and robustness of HTR systems. We systematically examine traditional augmentation methods alongside recent advances in deep learning, including Generative Adversarial Networks (GANs), diffusion models, and transformer-based approaches. Furthermore, we explore the challenges associated with generating diverse and realistic handwriting samples, particularly in preserving script authenticity and addressing data scarcity. This survey follows the PRISMA methodology, ensuring a structured and rigorous selection process. Our analysis began with 1,302 primary studies, which were filtered down to 848 after removing duplicates, drawing from key academic sources such as IEEE Digital Library, Springer Link, Science Direct, and ACM Digital Library. By evaluating existing datasets, assessment metrics, and state-of-the-art methodologies, this survey identifies key research gaps and proposes future directions to advance the field of handwritten text generation across diverse linguistic and stylistic landscapes.</li>
<li><strong>摘要：</strong>离线手写文本识别（HTR）系统在诸如历史文档数字化，自动形式处理和生物识别身份验证等应用中起着至关重要的作用。但是，注释培训数据的可用性有限，特别是对于低资源语言和复杂的脚本，通常会阻碍它们的性能。本文介绍了旨在提高HTR系统的准确性和鲁棒性的离线手写数据增强和发电技术的全面调查。我们系统地检查了传统的增强方法以及深度学习的最新进展，包括生成的对抗网络（GAN），扩散模型和基于变压器的方法。此外，我们探讨了与生成多样化和现实的手写样本相关的挑战，尤其是在保留脚本真实性和解决数据稀缺性方面。该调查遵循Prisma方法论，确保了结构化和严格的选择过程。我们的分析始于1,302项主要研究，这些研究在删除重复项后被过滤至848，从IEEE数字图书馆，Springer Link，Science Direct和ACM Digital Library等关键学术来源借鉴。通过评估现有数据集，评估指标和最先进的方法，该调查确定了关键的研究差距，并提出了未来的方向，以推动各种语言和风格景观的手写文本生成领域。</li>
</ul>

<h3>Title: Secure and Storage-Efficient Deep Learning Models for Edge AI Using Automatic Weight Generation</h3>
<ul>
<li><strong>Authors: </strong>Habibur Rahaman, Atri Chatterjee, Swarup Bhunia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06380">https://arxiv.org/abs/2507.06380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06380">https://arxiv.org/pdf/2507.06380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06380]] Secure and Storage-Efficient Deep Learning Models for Edge AI Using Automatic Weight Generation(https://arxiv.org/abs/2507.06380)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Complex neural networks require substantial memory to store a large number of synaptic weights. This work introduces WINGs (Automatic Weight Generator for Secure and Storage-Efficient Deep Learning Models), a novel framework that dynamically generates layer weights in a fully connected neural network (FC) and compresses the weights in convolutional neural networks (CNNs) during inference, significantly reducing memory requirements without sacrificing accuracy. WINGs framework uses principal component analysis (PCA) for dimensionality reduction and lightweight support vector regression (SVR) models to predict layer weights in the FC networks, removing the need for storing full-weight matrices and achieving substantial memory savings. It also preferentially compresses the weights in low-sensitivity layers of CNNs using PCA and SVR with sensitivity analysis. The sensitivity-aware design also offers an added level of security, as any bit-flip attack with weights in compressed layers has an amplified and readily detectable effect on accuracy. WINGs achieves 53x compression for the FC layers and 28x for AlexNet with MNIST dataset, and 18x for Alexnet with CIFAR-10 dataset with 1-2% accuracy loss. This significant reduction in memory results in higher throughput and lower energy for DNN inference, making it attractive for resource-constrained edge applications.</li>
<li><strong>摘要：</strong>复杂的神经网络需要大量记忆才能存储大量突触权重。这项工作介绍了机翼（用于安全和存储有效的深度学习模型的自动重量产生器），该新型框架在推理过程中动态生成完全连接的神经网络（FC）中的层重量，并在推理过程中压缩卷积神经网络（CNN）中的权重，可大大降低内存需求而无需牺牲准确性。 WINGS框架使用主组件分析（PCA）来缩小维度和轻质支持向量回归（SVR）模型来预测FC网络中的层权重，从而消除了存储全重量矩阵并实现大量内存节省的需求。它还优先使用PCA和SVR和SVR和SVR和灵敏度分析来压缩CNN低敏感层的权重。灵敏度感知的设计还提供了一个增加的安全性，因为压缩层中重量的任何flip攻击都具有放大且易于检测到准确性的影响。 Wings具有FC层的53倍压缩，使用MNIST数据集可实现Alexnet的28倍，而使用CIFAR-10数据集则达到了Alexnet的18倍，精度损失为1-2％。记忆的显着降低导致DNN推断的较高吞吐量和较低的能量，从而使其对资源受限的边缘应用具有吸引力。</li>
</ul>

<h3>Title: SImpHAR: Advancing impedance-based human activity recognition using 3D simulation and text-to-motion models</h3>
<ul>
<li><strong>Authors: </strong>Lala Shakti Swarup Ray, Mengxi Liu, Deepika Gurung, Bo Zhou, Sungho Suh, Paul Lukowicz</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06405">https://arxiv.org/abs/2507.06405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06405">https://arxiv.org/pdf/2507.06405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06405]] SImpHAR: Advancing impedance-based human activity recognition using 3D simulation and text-to-motion models(https://arxiv.org/abs/2507.06405)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Human Activity Recognition (HAR) with wearable sensors is essential for applications in healthcare, fitness, and human-computer interaction. Bio-impedance sensing offers unique advantages for fine-grained motion capture but remains underutilized due to the scarcity of labeled data. We introduce SImpHAR, a novel framework addressing this limitation through two core contributions. First, we propose a simulation pipeline that generates realistic bio-impedance signals from 3D human meshes using shortest-path estimation, soft-body physics, and text-to-motion generation serving as a digital twin for data augmentation. Second, we design a two-stage training strategy with decoupled approach that enables broader activity coverage without requiring label-aligned synthetic data. We evaluate SImpHAR on our collected ImpAct dataset and two public benchmarks, showing consistent improvements over state-of-the-art methods, with gains of up to 22.3% and 21.8%, in terms of accuracy and macro F1 score, respectively. Our results highlight the promise of simulation-driven augmentation and modular training for impedance-based HAR.</li>
<li><strong>摘要：</strong>具有可穿戴传感器的人类活动识别（HAR）对于在医疗保健，健身和人类计算机相互作用中的应用至关重要。生物阻抗传感为细粒运动捕获提供了独特的优势，但由于标记的数据的稀缺性，仍未得到充分利用。我们介绍了Simphar，这是一个新颖的框架，通过两个核心贡献来解决这一限制。首先，我们提出了一个模拟管道，该管道使用最短的路径估计，软体体物理学和文本到动作生成，从而从3D人网格中生成现实的生物阻抗信号，可作为数据增强的数字双胞胎。其次，我们设计了一种两阶段的训练策略，采用脱钩方法，可实现更广泛的活动覆盖范围，而无需标记的合成数据。我们在收集到的影响数据集和两个公共基准测试中评估了Simphar，在准确性和宏F1得分方面，分别对最先进的方法进行了一致的改进，高达22.3％和21.8％。我们的结果突出了模拟驱动的增强和模块化训练的希望，以基于阻抗的HAR。</li>
</ul>

<h3>Title: Automated Neuron Labelling Enables Generative Steering and Interpretability in Protein Language Models</h3>
<ul>
<li><strong>Authors: </strong>Arjun Banerjee, David Martinez, Camille Dang, Ethan Tam</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06458">https://arxiv.org/abs/2507.06458</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06458">https://arxiv.org/pdf/2507.06458</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06458]] Automated Neuron Labelling Enables Generative Steering and Interpretability in Protein Language Models(https://arxiv.org/abs/2507.06458)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Protein language models (PLMs) encode rich biological information, yet their internal neuron representations are poorly understood. We introduce the first automated framework for labeling every neuron in a PLM with biologically grounded natural language descriptions. Unlike prior approaches relying on sparse autoencoders or manual annotation, our method scales to hundreds of thousands of neurons, revealing individual neurons are selectively sensitive to diverse biochemical and structural properties. We then develop a novel neuron activation-guided steering method to generate proteins with desired traits, enabling convergence to target biochemical properties like molecular weight and instability index as well as secondary and tertiary structural motifs, including alpha helices and canonical Zinc Fingers. We finally show that analysis of labeled neurons in different model sizes reveals PLM scaling laws and a structured neuron space distribution.</li>
<li><strong>摘要：</strong>蛋白质语言模型（PLM）编码丰富的生物学信息，但其内部神经元表示却鲜为人知。我们介绍了第一个自动化框架，用于标记具有生物扎根的自然语言描述的PLM中的每个神经元。与先前依靠稀疏自动编码器或手动注释的方法不同，我们的方法缩放到数十万个神经元，揭示单个神经元对多种生化和结构特性有选择性地敏感。然后，我们开发了一种新型的神经元激活引导的转向方法，以产生具有所需性状的蛋白质，从而使收敛到靶向生物化学特性，例如分子量和不稳定性指数，以及二级和第三结构基序，包括α螺旋螺旋和规范的锌指。我们最终表明，对不同模型大小的标记神经元的分析揭示了PLM缩放定律和结构化神经元空间分布。</li>
</ul>

<h3>Title: Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Aaron Dharna, Cong Lu, Jeff Clune</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06466">https://arxiv.org/abs/2507.06466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06466">https://arxiv.org/pdf/2507.06466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06466]] Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models(https://arxiv.org/abs/2507.06466)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multi-agent interactions have long fueled innovation, from natural predator-prey dynamics to the space race. Self-play (SP) algorithms try to harness these dynamics by pitting agents against ever-improving opponents, thereby creating an implicit curriculum toward learning high-quality solutions. However, SP often fails to produce diverse solutions and can get stuck in locally optimal behaviors. We introduce Foundation-Model Self-Play (FMSP), a new direction that leverages the code-generation capabilities and vast knowledge of foundation models (FMs) to overcome these challenges by leaping across local optima in policy space. We propose a family of approaches: (1) \textbf{Vanilla Foundation-Model Self-Play (vFMSP)} continually refines agent policies via competitive self-play; (2) \textbf{Novelty-Search Self-Play (NSSP)} builds a diverse population of strategies, ignoring performance; and (3) the most promising variant, \textbf{Quality-Diveristy Self-Play (QDSP)}, creates a diverse set of high-quality policies by combining the diversity of NSSP and refinement of vFMSP. We evaluate FMSPs in Car Tag, a continuous-control pursuer-evader setting, and in Gandalf, a simple AI safety simulation in which an attacker tries to jailbreak an LLM's defenses. In Car Tag, FMSPs explore a wide variety of reinforcement learning, tree search, and heuristic-based methods, to name just a few. In terms of discovered policy quality, \ouralgo and vFMSP surpass strong human-designed strategies. In Gandalf, FMSPs can successfully automatically red-team an LLM, breaking through and jailbreaking six different, progressively stronger levels of defense. Furthermore, FMSPs can automatically proceed to patch the discovered vulnerabilities. Overall, FMSPs represent a promising new research frontier of improving self-play with foundation models, opening fresh paths toward more creative and open-ended strategy discovery</li>
<li><strong>摘要：</strong>从天然捕食者 - 捕食动力学到太空竞赛，多代理互动长期以来一直助长创新。自我播放（SP）算法试图通过使代理不断改善对手来利用这些动态，从而为学习高质量的解决方案创造了隐性的课程。但是，SP通常无法产生各种解决方案，并且可能陷入本地最佳行为。我们介绍了基金会模型自我播放（FMSP），这是一个新的方向，利用代码生成能力和对基础模型（FMS）的广泛了解来克服这些挑战，通过在政策领域中跨越本地最佳优点来克服这些挑战。我们提出了一种方法：（1）\ textbf {Vanilla Foundation-Model自我播放（VFMSP）}不断地通过竞争性自我播放来完善代理政策； （2）\ textbf {Novelty-Search selfplay（NSSP）}建立了多种策略，无视绩效； （3）最有前途的变体，\ textbf {质量动机自我播放（QDSP）}，通过结合NSSP的多样性和VFMSP的完善来创建一套多样化的高质量政策。我们评估了FMSP的汽车标签，这是一个连续控制的追捕者环境，在Gandalf中评估了一个简单的AI安全模拟，其中攻击者试图越狱LLM的防御力。在汽车标签中，FMSP探索了各种各样的强化学习，树木搜索和基于启发式的方法，仅举几例。就发现的政策质量而言，\ Ouralgo和VFMSP超过了强大的人为设计的策略。在甘道夫（Gandalf），FMSP可以成功地自动红色的LLM红色团队，闯入并越狱六个不同的，逐渐强大的防御水平。此外，FMSP可以自动继续修补发现的漏洞。总体而言，FMSP代表了通过基础模型改善自我竞争的有希望的新研究领域，开辟了通往更具创造力和开放式策略发现</li>
</ul>

<h3>Title: FIFA: Unified Faithfulness Evaluation Framework for Text-to-Video and Video-to-Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Liqiang Jing, Viet Lai, Seunghyun Yoon, Trung Bui, Xinya Du</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06523">https://arxiv.org/abs/2507.06523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06523">https://arxiv.org/pdf/2507.06523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06523]] FIFA: Unified Faithfulness Evaluation Framework for Text-to-Video and Video-to-Text Generation(https://arxiv.org/abs/2507.06523)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Video Multimodal Large Language Models (VideoMLLMs) have achieved remarkable progress in both Video-to-Text and Text-to-Video tasks. However, they often suffer fro hallucinations, generating content that contradicts the visual input. Existing evaluation methods are limited to one task (e.g., V2T) and also fail to assess hallucinations in open-ended, free-form responses. To address this gap, we propose FIFA, a unified FaIthFulness evAluation framework that extracts comprehensive descriptive facts, models their semantic dependencies via a Spatio-Temporal Semantic Dependency Graph, and verifies them using VideoQA models. We further introduce Post-Correction, a tool-based correction framework that revises hallucinated content. Extensive experiments demonstrate that FIFA aligns more closely with human judgment than existing evaluation methods, and that Post-Correction effectively improves factual consistency in both text and video generation.</li>
<li><strong>摘要：</strong>视频多模式大型语言模型（videomllms）在视频到文本和文本对象任务中都取得了显着的进步。但是，他们经常遭受幻觉，产生与视觉输入相矛盾的内容。现有的评估方法仅限于一项任务（例如V2T），并且也无法评估开放式，自由形式响应中的幻觉。为了解决这一差距，我们提出了FIFA，这是一个统一的忠诚评估框架，提取了全面的描述性事实，通过时空的语义依赖性图对其语义依赖性进行建模，并使用VideoQA模型验证它们。我们进一步介绍了后校正，这是一个基于工具的校正框架，可修订幻觉内容。广泛的实验表明，与现有评估方法相比，FIFA与人类判断更加一致，并且后校正有效地提高了文本和视频生成的事实一致性。</li>
</ul>

<h3>Title: Concept Unlearning by Modeling Key Steps of Diffusion Process</h3>
<ul>
<li><strong>Authors: </strong>Chaoshuo Zhang, Chenhao Lin, Zhengyu Zhao, Le Yang, Qian Wang, Chao Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06526">https://arxiv.org/abs/2507.06526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06526">https://arxiv.org/pdf/2507.06526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06526]] Concept Unlearning by Modeling Key Steps of Diffusion Process(https://arxiv.org/abs/2507.06526)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models (T2I DMs), represented by Stable Diffusion, which generate highly realistic images based on textual input, have been widely used. However, their misuse poses serious security risks. While existing concept unlearning methods aim to mitigate these risks, they struggle to balance unlearning effectiveness with generative this http URL overcome this limitation, we innovatively propose the Key Step Concept Unlearning (KSCU) method, which ingeniously capitalizes on the unique stepwise sampling characteristic inherent in diffusion models during the image generation process. Unlike conventional approaches that treat all denoising steps equally, KSCU strategically focuses on pivotal steps with the most influence over the final outcome by dividing key steps for different concept unlearning tasks and fine-tuning the model only at those steps. This targeted approach reduces the number of parameter updates needed for effective unlearning, while maximizing the retention of the model's generative this http URL extensive benchmark experiments, we demonstrate that KSCU effectively prevents T2I DMs from generating undesirable images while better retaining the model's generative this http URL code will be released.</li>
<li><strong>摘要：</strong>以稳定扩散为代表的文本到图像扩散模型（T2I DMS）已被广泛使用，该模型基于文本输入而生成高度逼真的图像。但是，他们的滥用构成了严重的安全风险。尽管现有的概念未学习的方法旨在减轻这些风险，但它们努力平衡未学习的效率与生成性的HTTP URL克服了这一限制，我们创新地提出了关键步骤概念（KSCU）方法，该方法在图像生成过程中巧妙地利用了独特的逐步采样在扩散模型中的既定阶段采样特征性的固有。与传统的方法相同地对待所有脱氧步骤，KSCU从战略上关注关注的步骤，通过为不同概念学习任务的关键步骤分配关键步骤，并仅在这些步骤中对模型进行微调。这种有针对性的方法减少了有效学习所需的参数更新数量，同时最大程度地保留了该模型的生成性此HTTP URL广泛的基准实验，我们证明KSCU可以有效地防止T2I DMS有效地生成不良图像，同时更好地保留该模型的HTTP URL代码。</li>
</ul>

<h3>Title: Concept-TRAK: Understanding how diffusion models learn concepts through concept-level attribution</h3>
<ul>
<li><strong>Authors: </strong>Yonghyun Park, Chieh-Hsin Lai, Satoshi Hayakawa, Yuhta Takida, Naoki Murata, Wei-Hsiang Liao, Woosung Choi, Kin Wai Cheuk, Junghyun Koo, Yuki Mitsufuji</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06547">https://arxiv.org/abs/2507.06547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06547">https://arxiv.org/pdf/2507.06547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06547]] Concept-TRAK: Understanding how diffusion models learn concepts through concept-level attribution(https://arxiv.org/abs/2507.06547)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>While diffusion models excel at image generation, their growing adoption raises critical concerns around copyright issues and model transparency. Existing attribution methods identify training examples influencing an entire image, but fall short in isolating contributions to specific elements, such as styles or objects, that matter most to stakeholders. To bridge this gap, we introduce \emph{concept-level attribution} via a novel method called \emph{Concept-TRAK}. Concept-TRAK extends influence functions with two key innovations: (1) a reformulated diffusion training loss based on diffusion posterior sampling, enabling robust, sample-specific attribution; and (2) a concept-aware reward function that emphasizes semantic relevance. We evaluate Concept-TRAK on the AbC benchmark, showing substantial improvements over prior methods. Through diverse case studies--ranging from identifying IP-protected and unsafe content to analyzing prompt engineering and compositional learning--we demonstrate how concept-level attribution yields actionable insights for responsible generative AI development and governance.</li>
<li><strong>摘要：</strong>尽管扩散模型在图像生成方面表现出色，但它们日益增长的采用引起了对版权问题和模型透明度的关键关注。现有的归因方法确定了影响整个图像的培训示例，但缺乏隔离对特定元素（例如样式或对象）对利益相关者最重要的贡献。为了弥合这一差距，我们通过一种称为\ emph {concept-trak}的新方法引入\ emph {概念级属性}。 Concept-Trak通过两个关键创新扩展了影响功能：（1）基于扩散后采样的重新分配扩散训练损失，实现了可靠的，特定于样本的归因； （2）强调语义相关性的概念感知奖励函数。我们在ABC基准上评估了概念传播，显示了对先前方法的实质性改进。通过各种案例研究 - 从识别受IP保护和不安全的内容到分析及时的工程和组成学习 - 我们证明了概念级归因如何为负责任的生成AI开发和治理提供可行的见解。</li>
</ul>

<h3>Title: MOST: Motion Diffusion Model for Rare Text via Temporal Clip Banzhaf Interaction</h3>
<ul>
<li><strong>Authors: </strong>Yin Wang, Mu li, Zhiying Leng, Frederick W. B. Li, Xiaohui Liang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06590">https://arxiv.org/abs/2507.06590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06590">https://arxiv.org/pdf/2507.06590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06590]] MOST: Motion Diffusion Model for Rare Text via Temporal Clip Banzhaf Interaction(https://arxiv.org/abs/2507.06590)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce MOST, a novel motion diffusion model via temporal clip Banzhaf interaction, aimed at addressing the persistent challenge of generating human motion from rare language prompts. While previous approaches struggle with coarse-grained matching and overlook important semantic cues due to motion redundancy, our key insight lies in leveraging fine-grained clip relationships to mitigate these issues. MOST's retrieval stage presents the first formulation of its kind - temporal clip Banzhaf interaction - which precisely quantifies textual-motion coherence at the clip level. This facilitates direct, fine-grained text-to-motion clip matching and eliminates prevalent redundancy. In the generation stage, a motion prompt module effectively utilizes retrieved motion clips to produce semantically consistent movements. Extensive evaluations confirm that MOST achieves state-of-the-art text-to-motion retrieval and generation performance by comprehensively addressing previous challenges, as demonstrated through quantitative and qualitative results highlighting its effectiveness, especially for rare prompts.</li>
<li><strong>摘要：</strong>我们最多通过暂时的剪辑Banzhaf互动引入了一种新型运动扩散模型，旨在解决从稀有语言提示中产生人类运动的持续挑战。尽管以前的方法因运动冗余而与粗粒匹配和忽略重要的语义提示而奋斗，但我们的主要见解在于利用细粒度的夹子关系来减轻这些问题。大多数的检索阶段提出了同类的第一个表述 - 时间剪辑Banzhaf相互作用 - 精确地量化了剪辑级别的文本运动连贯性。这有助于直接，细粒度的文本对动作夹匹配，并消除普遍的冗余。在生成阶段，运动提示模块有效地利用检索到的运动夹来产生语义一致的运动。广泛的评估证实，大多数人通过全面应对先前的挑战来实现最先进的文本到动作检索和发电绩效，这是通过定量和定性结果强调其有效性的，尤其是对于稀有提示而言。</li>
</ul>

<h3>Title: Generalization in Reinforcement Learning for Radio Access Networks</h3>
<ul>
<li><strong>Authors: </strong>Burak Demirel, Yu Wang, Cristian Tatino, Pablo Soldati</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06602">https://arxiv.org/abs/2507.06602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06602">https://arxiv.org/pdf/2507.06602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06602]] Generalization in Reinforcement Learning for Radio Access Networks(https://arxiv.org/abs/2507.06602)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Modern RAN operate in highly dynamic and heterogeneous environments, where hand-tuned, rule-based RRM algorithms often underperform. While RL can surpass such heuristics in constrained settings, the diversity of deployments and unpredictable radio conditions introduce major generalization challenges. Data-driven policies frequently overfit to training conditions, degrading performance in unseen scenarios. To address this, we propose a generalization-centered RL framework for RAN control that: (i) encodes cell topology and node attributes via attention-based graph representations; (ii) applies domain randomization to broaden the training distribution; and (iii) distributes data generation across multiple actors while centralizing training in a cloud-compatible architecture aligned with O-RAN principles. Although generalization increases computational and data-management complexity, our distributed design mitigates this by scaling data collection and training across diverse network conditions. Applied to downlink link adaptation in five 5G benchmarks, our policy improves average throughput and spectral efficiency by ~10% over an OLLA baseline (10% BLER target) in full-buffer MIMO/mMIMO and by >20% under high mobility. It matches specialized RL in full-buffer traffic and achieves up to 4- and 2-fold gains in eMBB and mixed-traffic benchmarks, respectively. In nine-cell deployments, GAT models offer 30% higher throughput over MLP baselines. These results, combined with our scalable architecture, offer a path toward AI-native 6G RAN using a single, generalizable RL agent.</li>
<li><strong>摘要：</strong>现代运行在高度动态和异构环境中运行，在该环境中，基于规则的RRM算法通常表现不佳。尽管RL可以在受限的设置中超越这种启发式方法，但部署和不可预测的无线电条件的多样性引入了重大的泛化挑战。数据驱动的政策经常过度地适合培训条件，在看不见的情况下降低表现。为了解决这个问题，我们提出了一个以概括为中心的RL框架进行控制：（i）通过基于注意的图表表示细胞拓扑和节点属性； （ii）应用域随机化以扩大训练分布； （iii）在多个参与者中分发数据生成，同时将培训集中在与O-RAN原则一致的云兼容体系结构中。尽管概括增加了计算和数据管理的复杂性，但我们的分布式设计通过在不同的网络条件上扩展数据收集和培训来减轻这种情况。我们的政策应用于五个5G基准的下行链路适应，在全班级MIMO/MMIMO中，我们的政策将平均吞吐量和光谱效率提高了约10％（BLER目标10％），在高迁移率下> 20％。它与专门的RL相匹配，在全班级流量中，分别在Embb和混合人流基准中获得了4倍和2倍的收益。在九个细胞部署中，GAT模型比MLP基准可提供30％的吞吐量。这些结果与我们的可扩展体系结构相结合，使用单个可推广的RL代理提供了通往AI-Native 6G的途径。</li>
</ul>

<h3>Title: Denoising Multi-Beta VAE: Representation Learning for Disentanglement and Generation</h3>
<ul>
<li><strong>Authors: </strong>Anshuk Uppal, Yuhta Takida, Chieh-Hsin Lai, Yuki Mitsufuji</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06613">https://arxiv.org/abs/2507.06613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06613">https://arxiv.org/pdf/2507.06613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06613]] Denoising Multi-Beta VAE: Representation Learning for Disentanglement and Generation(https://arxiv.org/abs/2507.06613)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Disentangled and interpretable latent representations in generative models typically come at the cost of generation quality. The $\beta$-VAE framework introduces a hyperparameter $\beta$ to balance disentanglement and reconstruction quality, where setting $\beta > 1$ introduces an information bottleneck that favors disentanglement over sharp, accurate reconstructions. To address this trade-off, we propose a novel generative modeling framework that leverages a range of $\beta$ values to learn multiple corresponding latent representations. First, we obtain a slew of representations by training a single variational autoencoder (VAE), with a new loss function that controls the information retained in each latent representation such that the higher $\beta$ value prioritize disentanglement over reconstruction fidelity. We then, introduce a non-linear diffusion model that smoothly transitions latent representations corresponding to different $\beta$ values. This model denoises towards less disentangled and more informative representations, ultimately leading to (almost) lossless representations, enabling sharp reconstructions. Furthermore, our model supports sample generation without input images, functioning as a standalone generative model. We evaluate our framework in terms of both disentanglement and generation quality. Additionally, we observe smooth transitions in the latent spaces with respect to changes in $\beta$, facilitating consistent manipulation of generated outputs.</li>
<li><strong>摘要：</strong>生成模型中的解释和可解释的潜在表示通常以发电质量为代价。 $ \ beta $ -VAE框架引入了一个超参数$ \ beta $，以平衡分离和重建质量，其中设置$ \ beta> 1 $引入了一个信息瓶颈，该信息瓶颈比锐利，准确的重建有利于脱节。为了解决这个权衡，我们提出了一个新颖的生成建模框架，该框架利用$ \ beta $值的范围来学习多个相应的潜在表示。首先，我们通过训练单个变异自动编码器（VAE）获得了一系列表示形式，并具有新的损失函数，可以控制每个潜在表示中保留的信息，从而使较高的$ \ beta $ value优先确定重建忠诚度上的删除。然后，我们引入一个非线性扩散模型，该模型平滑地过渡了与不同$ \ beta $值相对应的潜在表示。该模型降低了较少的分解和更有信息的表示，最终导致了（几乎）无损代表，从而实现了尖锐的重建。此外，我们的模型支持没有输入图像的样品生成，作为独立生成模型的作用。我们根据分离和发电质量评估我们的框架。此外，我们观察到潜在空间中有关$ \ beta $的变化的平稳过渡，从而有助于对产生的输出的一致操纵。</li>
</ul>

<h3>Title: Deep Disentangled Representation Network for Treatment Effect Estimation</h3>
<ul>
<li><strong>Authors: </strong>Hui Meng, Keping Yang, Xuyu Peng, Bo Zheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06650">https://arxiv.org/abs/2507.06650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06650">https://arxiv.org/pdf/2507.06650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06650]] Deep Disentangled Representation Network for Treatment Effect Estimation(https://arxiv.org/abs/2507.06650)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Estimating individual-level treatment effect from observational data is a fundamental problem in causal inference and has attracted increasing attention in the fields of education, healthcare, and public this http URL this work, we concentrate on the study of disentangled representation methods that have shown promising outcomes by decomposing observed covariates into instrumental, confounding, and adjustment factors. However, most of the previous work has primarily revolved around generative models or hard decomposition methods for covariates, which often struggle to guarantee the attainment of precisely disentangled factors. In order to effectively model different causal relationships, we propose a novel treatment effect estimation algorithm that incorporates a mixture of experts with multi-head attention and a linear orthogonal regularizer to softly decompose the pre-treatment variables, and simultaneously eliminates selection bias via importance sampling re-weighting techniques. We conduct extensive experiments on both public semi-synthetic and real-world production datasets. The experimental results clearly demonstrate that our algorithm outperforms the state-of-the-art methods focused on individual treatment effects.</li>
<li><strong>摘要：</strong>从观察数据中估算个人水平的治疗效果是因果推理中的一个基本问题，并且在教育，医疗保健和公众领域引起了越来越多的关注，这项工作的研究集中在研究中，这些方法的研究通过将观测的协方差分解为工具，并调整了构造，并调整了构造，这些方法表现出了有希望的货币。但是，以前的大多数工作主要围绕着生成模型或协变量的硬分解方法旋转，这些方法通常很难确保达到精确分解的因素。为了有效地模拟不同的因果关系，我们提出了一种新型的治疗效果估计算法，该算法结合了具有多头关注的专家的混合物和线性正交正规剂，以软分分解预处理变量，并同时消除选择偏见的选择偏见，以采样采样重新启动重新接触技术。我们对公共半合成和现实生产数据集进行了广泛的实验。实验结果清楚地表明，我们的算法优于针对个体治疗效应的最新方法。</li>
</ul>

<h3>Title: Diff$^2$I2P: Differentiable Image-to-Point Cloud Registration with Diffusion Prior</h3>
<ul>
<li><strong>Authors: </strong>Juncheng Mu, Chengwei Ren, Weixiang Zhang, Liang Pan, Xiao-Ping Zhang, Yue Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06651">https://arxiv.org/abs/2507.06651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06651">https://arxiv.org/pdf/2507.06651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06651]] Diff$^2$I2P: Differentiable Image-to-Point Cloud Registration with Diffusion Prior(https://arxiv.org/abs/2507.06651)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Learning cross-modal correspondences is essential for image-to-point cloud (I2P) registration. Existing methods achieve this mostly by utilizing metric learning to enforce feature alignment across modalities, disregarding the inherent modality gap between image and point data. Consequently, this paradigm struggles to ensure accurate cross-modal correspondences. To this end, inspired by the cross-modal generation success of recent large diffusion models, we propose Diff$^2$I2P, a fully Differentiable I2P registration framework, leveraging a novel and effective Diffusion prior for bridging the modality gap. Specifically, we propose a Control-Side Score Distillation (CSD) technique to distill knowledge from a depth-conditioned diffusion model to directly optimize the predicted transformation. However, the gradients on the transformation fail to backpropagate onto the cross-modal features due to the non-differentiability of correspondence retrieval and PnP solver. To this end, we further propose a Deformable Correspondence Tuning (DCT) module to estimate the correspondences in a differentiable way, followed by the transformation estimation using a differentiable PnP solver. With these two designs, the Diffusion model serves as a strong prior to guide the cross-modal feature learning of image and point cloud for forming robust correspondences, which significantly improves the registration. Extensive experimental results demonstrate that Diff$^2$I2P consistently outperforms SoTA I2P registration methods, achieving over 7% improvement in registration recall on the 7-Scenes benchmark.</li>
<li><strong>摘要：</strong>学习跨模式对应关系对于图像到点云（I2P）注册至关重要。现有方法主要是通过使用公制学习来实现跨模态的特征对齐，而忽略了图像和点数据之间的固有模态差距。因此，该范式努力确保准确的跨模式对应关系。为此，我们受到最近大型扩散模型的跨模式生成成功的启发，我们提出了Diff $^2 $ i2p，这是一个完全可区分的I2P注册框架，利用了弥合模态差距的新颖有效的扩散。具体而言，我们提出了一种对照端评分蒸馏（CSD）技术来将知识从深度条件扩散模型中提炼出来，以直接优化预测的转换。但是，由于对应检索和PNP求解器的非差异性，转换上的梯度无法将其倒流到交叉模式特征上。为此，我们进一步提出了一个可变形的对应关系（DCT）模块，以以可区分的方式估算对应关系，然后使用可区分的PNP求解器进行转换估计。借助这两种设计，扩散模型在引导图像和点云的跨模式特征学习以形成强大的对应关系之前充当强大，从而显着改善了注册。广泛的实验结果表明，Diff $^2 $ I2P始终优于SOTA I2P注册方法，在7片基准测试中的注册召回率提高了7％以上。</li>
</ul>

<h3>Title: Enhancing Diffusion Model Stability for Image Restoration via Gradient Management</h3>
<ul>
<li><strong>Authors: </strong>Hongjie Wu, Mingqin Zhang, Linchao He, Ji-Zhe Zhou, Jiancheng Lv</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06656">https://arxiv.org/abs/2507.06656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06656">https://arxiv.org/pdf/2507.06656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06656]] Enhancing Diffusion Model Stability for Image Restoration via Gradient Management(https://arxiv.org/abs/2507.06656)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have shown remarkable promise for image restoration by leveraging powerful priors. Prominent methods typically frame the restoration problem within a Bayesian inference framework, which iteratively combines a denoising step with a likelihood guidance step. However, the interactions between these two components in the generation process remain underexplored. In this paper, we analyze the underlying gradient dynamics of these components and identify significant instabilities. Specifically, we demonstrate conflicts between the prior and likelihood gradient directions, alongside temporal fluctuations in the likelihood gradient itself. We show that these instabilities disrupt the generative process and compromise restoration performance. To address these issues, we propose Stabilized Progressive Gradient Diffusion (SPGD), a novel gradient management technique. SPGD integrates two synergistic components: (1) a progressive likelihood warm-up strategy to mitigate gradient conflicts; and (2) adaptive directional momentum (ADM) smoothing to reduce fluctuations in the likelihood gradient. Extensive experiments across diverse restoration tasks demonstrate that SPGD significantly enhances generation stability, leading to state-of-the-art performance in quantitative metrics and visually superior results. Code is available at \href{this https URL}{here}.</li>
<li><strong>摘要：</strong>扩散模型通过利用强大的先验来显示出对图像恢复的巨大希望。突出的方法通常会在贝叶斯推理框架内构建恢复问题，该框架将其与可能性指导步骤结合在一起。但是，生成过程中这两个组件之间的相互作用仍然没有被逐渐倍增。在本文中，我们分析了这些组件的潜在梯度动力学，并确定了重要的不稳定性。具体而言，我们证明了先前和似然梯度方向之间的冲突，以及似然梯度本身的时间波动。我们表明，这些不稳定性破坏了生成过程并妥协恢复性能。为了解决这些问题，我们提出了一种稳定的渐进梯度扩散（SPGD），这是一种新颖的梯度管理技术。 SPGD整合了两个协同组件：（1）一种缓解梯度冲突的渐进式可能性热身策略； （2）自适应定向动量（ADM）平滑以减少似然梯度中的波动。跨不同恢复任务的广泛实验表明，SPGD显着提高了发电稳定性，从而导致定量指标的最新性能和视觉上出色的结果。代码可在\ href {此https url} {there}中获得。</li>
</ul>

<h3>Title: MK-Pose: Category-Level Object Pose Estimation via Multimodal-Based Keypoint Learning</h3>
<ul>
<li><strong>Authors: </strong>Yifan Yang, Peili Song, Enfan Lan, Dong Liu, Jingtai Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06662">https://arxiv.org/abs/2507.06662</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06662">https://arxiv.org/pdf/2507.06662</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06662]] MK-Pose: Category-Level Object Pose Estimation via Multimodal-Based Keypoint Learning(https://arxiv.org/abs/2507.06662)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Category-level object pose estimation, which predicts the pose of objects within a known category without prior knowledge of individual instances, is essential in applications like warehouse automation and manufacturing. Existing methods relying on RGB images or point cloud data often struggle with object occlusion and generalization across different instances and categories. This paper proposes a multimodal-based keypoint learning framework (MK-Pose) that integrates RGB images, point clouds, and category-level textual descriptions. The model uses a self-supervised keypoint detection module enhanced with attention-based query generation, soft heatmap matching and graph-based relational modeling. Additionally, a graph-enhanced feature fusion module is designed to integrate local geometric information and global context. MK-Pose is evaluated on CAMERA25 and REAL275 dataset, and is further tested for cross-dataset capability on HouseCat6D dataset. The results demonstrate that MK-Pose outperforms existing state-of-the-art methods in both IoU and average precision without shape priors. Codes will be released at \href{this https URL}{this https URL}.</li>
<li><strong>摘要：</strong>类别级别的对象姿势估计可以预测已知类别中对象的姿势，而无需事先了解单个实例，这对于仓库自动化和制造等应用至关重要。依靠RGB图像或点云数据的现有方法通常在不同实例和类别之间在对象阻塞和概括方面遇到困难。本文提出了一个基于多模式的关键点学习框架（MK置），该框架集成了RGB图像，点云和类别级文本描述。该模型使用基于注意的查询生成，软热图匹配和基于图的关系建模增强了自我监督的关键点检测模块。此外，设计图形的特征​​融合模块旨在集成本地几何信息和全局上下文。在Camera25和Real275数据集上评估了MK置孔，并进一步测试了HouseCat6D数据集上的交叉数据集功能。结果表明，MK置端在IOU和平均精度没有形状先验的情况下都优于现有的最新方法。代码将在\ href {this HTTPS url} {this HTTPS url}中发布。</li>
</ul>

<h3>Title: PromptTea: Let Prompts Tell TeaCache the Optimal Threshold</h3>
<ul>
<li><strong>Authors: </strong>Zishen Huang, Chunyu Yang, Mengyuan Ren</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06739">https://arxiv.org/abs/2507.06739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06739">https://arxiv.org/pdf/2507.06739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06739]] PromptTea: Let Prompts Tell TeaCache the Optimal Threshold(https://arxiv.org/abs/2507.06739)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite recent progress in video generation, inference speed remains a major bottleneck. A common acceleration strategy involves reusing model outputs via caching mechanisms at fixed intervals. However, we find that such fixed-frequency reuse significantly degrades quality in complex scenes, while manually tuning reuse thresholds is inefficient and lacks robustness. To address this, we propose Prompt-Complexity-Aware (PCA) caching, a method that automatically adjusts reuse thresholds based on scene complexity estimated directly from the input prompt. By incorporating prompt-derived semantic cues, PCA enables more adaptive and informed reuse decisions than conventional caching methods. We also revisit the assumptions behind TeaCache and identify a key limitation: it suffers from poor input-output relationship modeling due to an oversimplified prior. To overcome this, we decouple the noisy input, enhance the contribution of meaningful textual information, and improve the model's predictive accuracy through multivariate polynomial feature expansion. To further reduce computational cost, we replace the static CFGCache with DynCFGCache, a dynamic mechanism that selectively reuses classifier-free guidance (CFG) outputs based on estimated output variations. This allows for more flexible reuse without compromising output quality. Extensive experiments demonstrate that our approach achieves significant acceleration-for example, 2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across a range of scenes.</li>
<li><strong>摘要：</strong>尽管视频产生最近进展，但推理速度仍然是主要的瓶颈。一种常见的加速策略涉及通过固定间隔的缓存机制重复使用模型输出。但是，我们发现这种固定频率的重复使用会在复杂的场景中显着降低质量，而手动调整重用阈值效率低下，缺乏稳健性。为了解决这个问题，我们提出了提示复杂性感知（PCA）缓存，该方法会根据直接从输入提示符直接估算的场景复杂性自动调整重复使用阈值。通过合并及时的语义提示，PCA比传统的缓存方法实现了更多的自适应和知情的重复使用决策。我们还重新审视了Teacache背后的假设并确定一个关键限制：由于先验过多的先验而导致的输入输出关系建模差。为了克服这一点，我们将嘈杂的输入脱致，增强有意义的文本信息的贡献，并通过多元多项式特征扩展提高模型的预测精度。为了进一步降低计算成本，我们用dyncfgcache替换静态CFGCACHE，这是一种动态机制，它根据估计的输出变化有选择地重用无分类器指导（CFG）输出。这允许在不损害输出质量的情况下进行更灵活的重复使用。广泛的实验表明，我们的方法实现了显着的加速度示例，在WAN2.1模型上进行了2.79倍的速度，同时在一系列场景中保持了高视觉保真度。</li>
</ul>

<h3>Title: FOLC-Net: A Federated-Optimized Lightweight Architecture for Enhanced MRI Disease Diagnosis across Axial, Coronal, and Sagittal Views</h3>
<ul>
<li><strong>Authors: </strong>Saif Ur Rehman Khan, Muhammad Nabeel Asim, Sebastian Vollmer, Andreas Dengel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06763">https://arxiv.org/abs/2507.06763</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06763">https://arxiv.org/pdf/2507.06763</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06763]] FOLC-Net: A Federated-Optimized Lightweight Architecture for Enhanced MRI Disease Diagnosis across Axial, Coronal, and Sagittal Views(https://arxiv.org/abs/2507.06763)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The framework is designed to improve performance in the analysis of combined as well as single anatomical perspectives for MRI disease diagnosis. It specifically addresses the performance degradation observed in state-of-the-art (SOTA) models, particularly when processing axial, coronal, and sagittal anatomical planes. The paper introduces the FOLC-Net framework, which incorporates a novel federated-optimized lightweight architecture with approximately 1.217 million parameters and a storage requirement of only 0.9 MB. FOLC-Net integrates Manta-ray foraging optimization (MRFO) mechanisms for efficient model structure generation, global model cloning for scalable training, and ConvNeXt for enhanced client adaptability. The model was evaluated on combined multi-view data as well as individual views, such as axial, coronal, and sagittal, to assess its robustness in various medical imaging scenarios. Moreover, FOLC-Net tests a ShallowFed model on different data to evaluate its ability to generalize beyond the training dataset. The results show that FOLC-Net outperforms existing models, particularly in the challenging sagittal view. For instance, FOLC-Net achieved an accuracy of 92.44% on the sagittal view, significantly higher than the 88.37% accuracy of study method (DL + Residual Learning) and 88.95% of DL models. Additionally, FOLC-Net demonstrated improved accuracy across all individual views, providing a more reliable and robust solution for medical image analysis in decentralized environments. FOLC-Net addresses the limitations of existing SOTA models by providing a framework that ensures better adaptability to individual views while maintaining strong performance in multi-view settings. The incorporation of MRFO, global model cloning, and ConvNeXt ensures that FOLC-Net performs better in real-world medical applications.</li>
<li><strong>摘要：</strong>该框架旨在提高在MRI疾病诊断的组合分析以及单个解剖学观点中的性能。它专门解决了在最新模型（SOTA）模型中观察到的性能降解，尤其是在处理轴向，冠状和矢状解剖平面时。该论文介绍了FOLC-NET框架，该框架结合了一种新型联邦优化的轻型体系结构，其中约有121.7万参数，并且仅存储需求仅为0.9 MB。 FOLC-NET集成了有效的模型结构生成，可扩展训练的全球模型克隆以及Convnext的麦芽射线觅食优化（MRFO）机制，以增强客户的适应性。该模型在组合的多视图数据以及轴向，冠状和矢状等各个观点上进行了评估，以评估其在各种医学成像方案中的鲁棒性。此外，FOLC-NET测试了不同数据的浅层模型，以评估其在培训数据集之外的推广能力。结果表明，FOLC-NET的表现优于现有模型，尤其是在具有挑战性的矢状视图中。例如，FOLC-NET在矢状视图上达到了92.44％的精度，显着高于研究方法的88.37％精度（DL +残留学习）和88.95％的DL模型。此外，FOLC-NET在所有个别视图中都表现出了提高的准确性，为分散环境中的医学图像分析提供了更可靠，更强大的解决方案。 FOLC-NET通过提供一个框架来确保对单个视图的适应性更好，同时在多视图设置中保持强劲的性能，从而解决了现有SOTA模型的局限性。 MRFO，全球模型克隆和Convnext的结合确保FOLC-NET在实际医疗应用中的表现更好。</li>
</ul>

<h3>Title: Democratizing High-Fidelity Co-Speech Gesture Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Xu Yang, Shaoli Huang, Shenbo Xie, Xuelin Chen, Yifei Liu, Changxing Ding</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06812">https://arxiv.org/abs/2507.06812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06812">https://arxiv.org/pdf/2507.06812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06812]] Democratizing High-Fidelity Co-Speech Gesture Video Generation(https://arxiv.org/abs/2507.06812)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Co-speech gesture video generation aims to synthesize realistic, audio-aligned videos of speakers, complete with synchronized facial expressions and body gestures. This task presents challenges due to the significant one-to-many mapping between audio and visual content, further complicated by the scarcity of large-scale public datasets and high computational demands. We propose a lightweight framework that utilizes 2D full-body skeletons as an efficient auxiliary condition to bridge audio signals with visual outputs. Our approach introduces a diffusion model conditioned on fine-grained audio segments and a skeleton extracted from the speaker's reference image, predicting skeletal motions through skeleton-audio feature fusion to ensure strict audio coordination and body shape consistency. The generated skeletons are then fed into an off-the-shelf human video generation model with the speaker's reference image to synthesize high-fidelity videos. To democratize research, we present CSG-405-the first public dataset with 405 hours of high-resolution videos across 71 speech types, annotated with 2D skeletons and diverse speaker demographics. Experiments show that our method exceeds state-of-the-art approaches in visual quality and synchronization while generalizing across speakers and contexts.</li>
<li><strong>摘要：</strong>共同语音的手势视频生成旨在综合扬声器的现实，音频一致的视频，并配有同步的面部表情和身体手势。由于音频和视觉内容之间的一对多映射，这项任务提出了挑战，这使大规模公共数据集的稀缺和高计算需求更加复杂。我们提出了一个轻巧的框架，该框架利用2D全身骨骼作为有效的辅助条件，以启动视觉输出的音频信号。我们的方法引入了一个以细粒度音频段为条件的扩散模型，并从扬声器的参考图像中提取的骨骼进行了构成，通过骨骼审计特征融合来预测骨骼运动，以确保严格的音频配位和身体形状的一致性。然后使用扬声器的参考图像将生成的骨骼送入现成的人类视频生成模型中，以合成高保真视频。为了使研究民主化，我们介绍了CSG-405-第一个公共数据集，其中有405个小时的71种语音类型的高分辨率视频，并注明了2D骨架和不同的演讲者人口统计。实验表明，我们的方法超过了视觉质量和同步的最新方法，同时跨越了扬声器和环境。</li>
</ul>

<h3>Title: HVI-CIDNet+: Beyond Extreme Darkness for Low-Light Image Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Qingsen Yan, Kangbiao Shi, Yixu Feng, Tao Hu, Peng Wu, Guansong Pang, Yanning Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06814">https://arxiv.org/abs/2507.06814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06814">https://arxiv.org/pdf/2507.06814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06814]] HVI-CIDNet+: Beyond Extreme Darkness for Low-Light Image Enhancement(https://arxiv.org/abs/2507.06814)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Low-Light Image Enhancement (LLIE) aims to restore vivid content and details from corrupted low-light images. However, existing standard RGB (sRGB) color space-based LLIE methods often produce color bias and brightness artifacts due to the inherent high color sensitivity. While Hue, Saturation, and Value (HSV) color space can decouple brightness and color, it introduces significant red and black noise artifacts. To address this problem, we propose a new color space for LLIE, namely Horizontal/Vertical-Intensity (HVI), defined by the HV color map and learnable intensity. The HV color map enforces small distances for the red coordinates to remove red noise artifacts, while the learnable intensity compresses the low-light regions to remove black noise artifacts. Additionally, we introduce the Color and Intensity Decoupling Network+ (HVI-CIDNet+), built upon the HVI color space, to restore damaged content and mitigate color distortion in extremely dark regions. Specifically, HVI-CIDNet+ leverages abundant contextual and degraded knowledge extracted from low-light images using pre-trained vision-language models, integrated via a novel Prior-guided Attention Block (PAB). Within the PAB, latent semantic priors can promote content restoration, while degraded representations guide precise color correction, both particularly in extremely dark regions through the meticulously designed cross-attention fusion mechanism. Furthermore, we construct a Region Refinement Block that employs convolution for information-rich regions and self-attention for information-scarce regions, ensuring accurate brightness adjustments. Comprehensive results from benchmark experiments demonstrate that the proposed HVI-CIDNet+ outperforms the state-of-the-art methods on 10 datasets.</li>
<li><strong>摘要：</strong>弱光图像增强（LLIE）旨在恢复损坏的低光图像中生动的内容和细节。但是，由于固有的高色彩灵敏度，现有的标准RGB（SRGB）颜色基于空间的LLIE方法通常会产生颜色偏差和亮度伪像。虽然色相，饱和度和价值（HSV）色彩空间可以使亮度和颜色脱致，但它引入了明显的红色和黑色噪声伪像。为了解决这个问题，我们为LLIE提出了一个新的色彩空间，即水平/垂直强度（HVI），由HV颜色图和可学习的强度定义。 HV颜色地图为红色坐标执行了较小的距离以去除红色噪声伪影，而可学习的强度则压缩了低光区以去除黑色噪声伪像。此外，我们介绍了建立在HVI颜色空间上的颜色和强度解耦网络+（HVI-CIDNET+），以恢复损坏的内容并减轻极度黑暗区域的颜色失真。具体而言，HVI-CIDNET+利用了使用预先训练的视觉模型从低光图像中提取的丰富上下文和退化的知识，这些模型通过新颖的先前引入的注意块（PAB）集成。在PAB中，潜在的语义先验可以促进内容恢复，而退化的表示指导指导精确的色彩校正，尤其是在精心设计的跨注意融合机制中，尤其是在极度黑暗的区域中。此外，我们构建了一个区域改进块，该区域采用卷积用于信息丰富的区域和信息筛分区域的自我注意力，从而确保了准确的亮度调整。基准实验的全面结果表明，所提出的HVI-CIDNET+优于10个数据集上的最新方法。</li>
</ul>

<h3>Title: Physics-Grounded Motion Forecasting via Equation Discovery for Trajectory-Guided Image-to-Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Tao Feng, Xianbing Zhao, Zhenhua Chen, Tien Tsin Wong, Hamid Rezatofighi, Gholamreza Haffari, Lizhen Qu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06830">https://arxiv.org/abs/2507.06830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06830">https://arxiv.org/pdf/2507.06830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06830]] Physics-Grounded Motion Forecasting via Equation Discovery for Trajectory-Guided Image-to-Video Generation(https://arxiv.org/abs/2507.06830)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion-based and autoregressive video generation models have achieved remarkable visual realism. However, these models typically lack accurate physical alignment, failing to replicate real-world dynamics in object motion. This limitation arises primarily from their reliance on learned statistical correlations rather than capturing mechanisms adhering to physical laws. To address this issue, we introduce a novel framework that integrates symbolic regression (SR) and trajectory-guided image-to-video (I2V) models for physics-grounded video forecasting. Our approach extracts motion trajectories from input videos, uses a retrieval-based pre-training mechanism to enhance symbolic regression, and discovers equations of motion to forecast physically accurate future trajectories. These trajectories then guide video generation without requiring fine-tuning of existing models. Evaluated on scenarios in Classical Mechanics, including spring-mass, pendulums, and projectile motions, our method successfully recovers ground-truth analytical equations and improves the physical alignment of generated videos over baseline methods.</li>
<li><strong>摘要：</strong>基于扩散和自回归的视频生成模型的最新进展已取得了显着的视觉现实主义。但是，这些模型通常缺乏准确的物理对准，无法在对象运动中复制现实世界动态。这种局限性主要源于它们依赖于学习的统计相关性，而不是捕获遵守物理定律的机制。为了解决这个问题，我们介绍了一个新颖的框架，该框架集成了符号回归（SR）和轨迹引导的图像到视频（I2V）模型，以用于物理界面的视频预测。我们的方法从输入视频中提取运动轨迹，使用基于检索的预训练机制来增强符号回归，并发现运动方程式以预测物理上准确的未来轨迹。然后，这些轨迹指导视频生成，而无需对现有模型进行微调。在经典力学的方案中进行了评估，包括弹簧质量，摆和弹丸动作，我们的方法成功地恢复了基地真实分析方程，并改善了基线方法上生成的视频的物理对齐。</li>
</ul>

<h3>Title: DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Liang Wang, Yu Rong, Tingyang Xu, Zhenyi Zhong, Zhiyuan Liu, Pengju Wang, Deli Zhao, Qiang Liu, Shu Wu, Liang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE, physics.chem-ph, q-bio.MN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06853">https://arxiv.org/abs/2507.06853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06853">https://arxiv.org/pdf/2507.06853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06853]] DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models(https://arxiv.org/abs/2507.06853)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Molecular structure elucidation from spectra is a foundational problem in chemistry, with profound implications for compound identification, synthesis, and drug development. Traditional methods rely heavily on expert interpretation and lack scalability. Pioneering machine learning methods have introduced retrieval-based strategies, but their reliance on finite libraries limits generalization to novel molecules. Generative models offer a promising alternative, yet most adopt autoregressive SMILES-based architectures that overlook 3D geometry and struggle to integrate diverse spectral modalities. In this work, we present DiffSpectra, a generative framework that directly infers both 2D and 3D molecular structures from multi-modal spectral data using diffusion models. DiffSpectra formulates structure elucidation as a conditional generation process. Its denoising network is parameterized by Diffusion Molecule Transformer, an SE(3)-equivariant architecture that integrates topological and geometric information. Conditioning is provided by SpecFormer, a transformer-based spectral encoder that captures intra- and inter-spectral dependencies from multi-modal spectra. Extensive experiments demonstrate that DiffSpectra achieves high accuracy in structure elucidation, recovering exact structures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through sampling. The model benefits significantly from 3D geometric modeling, SpecFormer pre-training, and multi-modal conditioning. These results highlight the effectiveness of spectrum-conditioned diffusion modeling in addressing the challenge of molecular structure elucidation. To our knowledge, DiffSpectra is the first framework to unify multi-modal spectral reasoning and joint 2D/3D generative modeling for de novo molecular structure elucidation.</li>
<li><strong>摘要：</strong>光谱中的分子结构阐明是化学中的基础问题，对化合物鉴定，合成和药物开发产生了深远的影响。传统方法在很大程度上依赖于专家解释和缺乏可扩展性。开创性的机器学习方法引入了基于检索的策略，但它们对有限库的依赖限制了对新分子的概括。生成模型提供了一种有希望的替代方案，但大多数采用自回归的基于微笑的体系结构，这些体系结构忽略了3D几何形状，并难以整合各种光谱模式。在这项工作中，我们提出了Diffspectra，这是一种生成框架，使用扩散模型直接从多模式光谱数据中直接渗透2D和3D分子结构。 Diffspectra将结构阐明作为条件生成过程。它的去核网络通过扩散分子变压器（3） - 等级结构的参数化，该结构集成了拓扑和几何信息。条件由SpecFormer提供，SpecFormer是一种基于变压器的光谱编码器，可捕获来自多模式光谱的偏光依赖性。广泛的实验表明，Diffspectra在结构阐明方面具有很高的精度，通过抽样恢复了16.01％Top-1精确度和96.86％TOP-20的精确结构。该模型从3D几何建模，SpecFormer预训练和多模式调节中显着受益。这些结果突出了光谱条件扩散建模在解决分子结构阐明的挑战方面的有效性。据我们所知，Diffspectra是统一多模式光谱推理的第一个框架和从头分子结构阐明的连接2D/3D生成建模。</li>
</ul>

<h3>Title: Hallucinating 360°: Panoramic Street-View Generation via Local Scenes Diffusion and Probabilistic Prompting</h3>
<ul>
<li><strong>Authors: </strong>Fei Teng, Kai Luo, Sheng Wu, Siyu Li, Pujun Guo, Jiale Wei, Kunyu Peng, Jiaming Zhang, Kailun Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06971">https://arxiv.org/abs/2507.06971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06971">https://arxiv.org/pdf/2507.06971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06971]] Hallucinating 360°: Panoramic Street-View Generation via Local Scenes Diffusion and Probabilistic Prompting(https://arxiv.org/abs/2507.06971)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, quality assessment</a></li>
<li><strong>Abstract: </strong>Panoramic perception holds significant potential for autonomous driving, enabling vehicles to acquire a comprehensive 360° surround view in a single shot. However, autonomous driving is a data-driven task. Complete panoramic data acquisition requires complex sampling systems and annotation pipelines, which are time-consuming and labor-intensive. Although existing street view generation models have demonstrated strong data regeneration capabilities, they can only learn from the fixed data distribution of existing datasets and cannot achieve high-quality, controllable panoramic generation. In this paper, we propose the first panoramic generation method Percep360 for autonomous driving. Percep360 enables coherent generation of panoramic data with control signals based on the stitched panoramic data. Percep360 focuses on two key aspects: coherence and controllability. Specifically, to overcome the inherent information loss caused by the pinhole sampling process, we propose the Local Scenes Diffusion Method (LSDM). LSDM reformulates the panorama generation as a spatially continuous diffusion process, bridging the gaps between different data distributions. Additionally, to achieve the controllable generation of panoramic images, we propose a Probabilistic Prompting Method (PPM). PPM dynamically selects the most relevant control cues, enabling controllable panoramic image generation. We evaluate the effectiveness of the generated images from three perspectives: image quality assessment (i.e., no-reference and with reference), controllability, and their utility in real-world Bird's Eye View (BEV) segmentation. Notably, the generated data consistently outperforms the original stitched images in no-reference quality metrics and enhances downstream perception models. The source code will be publicly available at this https URL.</li>
<li><strong>摘要：</strong>全景知觉具有自动驾驶的巨大潜力，使车辆能够单镜头获得全面的360°环绕景观。但是，自动驾驶是一项数据驱动的任务。完整的全景数据采集需要复杂的采样系统和注释管道，这是耗时且劳动力密集的。尽管现有的街道视图生成模型已经证明了强大的数据再生能力，但它们只能从现有数据集的固定数据分布中学习，并且无法获得高质量，可控的全景生成。在本文中，我们提出了第一种用于自动驾驶的全景生成方法。 percep360基于缝合的全景数据，可以与控制信号相干生成全景数据。 PERCEP360专注于两个关键方面：连贯性和可控性。具体而言，为了克服针孔采样过程引起的固有信息损失，我们提出了本地场景扩散方法（LSDM）。 LSDM将全景生成重新定义为空间连续的扩散过程，从而弥合了不同数据分布之间的差距。此外，为了获得可控的全景图像，我们提出了一种概率提示方法（PPM）。 PPM动态选择最相关的控制线索，从而实现可控制的全景图像生成。我们从三个角度评估了生成的图像的有效性：图像质量评估（即无参考和参考），可控性及其在现实世界中的鸟类视图（BEV）细分中的效用。值得注意的是，生成的数据始终在无参考质量指标中胜过原始缝合图像，并增强了下游感知模型。源代码将在此HTTPS URL上公开可用。</li>
</ul>

<h3>Title: MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology Report Generation</h3>
<ul>
<li><strong>Authors: </strong>Qilong Xing, Zikai Song, Youjia Zhang, Na Feng, Junqing Yu, Wei Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06992">https://arxiv.org/abs/2507.06992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06992">https://arxiv.org/pdf/2507.06992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06992]] MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology Report Generation(https://arxiv.org/abs/2507.06992)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite significant advancements in adapting Large Language Models (LLMs) for radiology report generation (RRG), clinical adoption remains challenging due to difficulties in accurately mapping pathological and anatomical features to their corresponding text descriptions. Additionally, semantic agnostic feature extraction further hampers the generation of accurate diagnostic reports. To address these challenges, we introduce Medical Concept Aligned Radiology Report Generation (MCA-RG), a knowledge-driven framework that explicitly aligns visual features with distinct medical concepts to enhance the report generation process. MCA-RG utilizes two curated concept banks: a pathology bank containing lesion-related knowledge, and an anatomy bank with anatomical descriptions. The visual features are aligned with these medical concepts and undergo tailored enhancement. We further propose an anatomy-based contrastive learning procedure to improve the generalization of anatomical features, coupled with a matching loss for pathological features to prioritize clinically relevant regions. Additionally, a feature gating mechanism is employed to filter out low-quality concept features. Finally, the visual features are corresponding to individual medical concepts, and are leveraged to guide the report generation process. Experiments on two public benchmarks (MIMIC-CXR and CheXpert Plus) demonstrate that MCA-RG achieves superior performance, highlighting its effectiveness in radiology report generation.</li>
<li><strong>摘要：</strong>尽管在适应放射学报告生成（RRG）的大型语言模型（LLM）方面取得了重大进步，但由于难以将病理学和解剖学特征映射到相应的文本描述中，临床采用仍然具有挑战性。此外，语义不可知的特征提取进一步阻碍了准确的诊断报告的产生。为了应对这些挑战，我们引入了医学概念一致性放射学报告生成（MCA-RG），这是一个知识驱动的框架，将视觉特征与不同的医学概念相结合以增强报告生成过程。 MCA-RG利用了两个策划的概念库：一家含有与病变相关知识的病理库，以及具有解剖学描述的解剖库。视觉特征与这些医学概念保持一致，并进行了量身定制的增强功能。我们进一步提出了一种基于解剖学的对比学习程序，以改善解剖学特征的概括，再加上病理特征的匹配损失，以优先考虑临床相关的区域。此外，还采用特征门控机制来滤除低质量概念特征。最后，视觉特征与单个医学概念相对应，并利用以指导报告生成过程。对两个公共基准测试（MIMIC-CXR和CHEXPERT PLUS）进行的实验表明，MCA-RG取得了出色的性能，突出了其在放射学报告生成中的有效性。</li>
</ul>

<h3>Title: Generating Multi-Table Time Series EHR from Latent Space with Minimal Preprocessing</h3>
<ul>
<li><strong>Authors: </strong>Eunbyeol Cho, Jiyoun Kim, Minjae Lee, Sungjin Park, Edward Choi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06996">https://arxiv.org/abs/2507.06996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06996">https://arxiv.org/pdf/2507.06996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06996]] Generating Multi-Table Time Series EHR from Latent Space with Minimal Preprocessing(https://arxiv.org/abs/2507.06996)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Electronic Health Records (EHR) are time-series relational databases that record patient interactions and medical events over time, serving as a critical resource for healthcare research and applications. However, privacy concerns and regulatory restrictions limit the sharing and utilization of such sensitive data, necessitating the generation of synthetic EHR datasets. Unlike previous EHR synthesis methods, which typically generate medical records consisting of expert-chosen features (e.g. a few vital signs or structured codes only), we introduce RawMed, the first framework to synthesize multi-table, time-series EHR data that closely resembles raw EHRs. Using text-based representation and compression techniques, RawMed captures complex structures and temporal dynamics with minimal preprocessing. We also propose a new evaluation framework for multi-table time-series synthetic EHRs, assessing distributional similarity, inter-table relationships, temporal dynamics, and privacy. Validated on two open-source EHR datasets, RawMed outperforms baseline models in fidelity and utility. The code is available at this https URL.</li>
<li><strong>摘要：</strong>电子健康记录（EHR）是时间序列关系数据库，随着时间的流逝，记录患者互动和医疗事件，是医疗保健研究和应用的关键资源。但是，隐私问题和监管限制限制了这种敏感数据的共享和利用，因此需要生成合成EHR数据集。与以前的EHR合成方法不同，通常生成由专家选择的特征（例如一些生命体征或结构化代码）组成的医疗记录，我们介绍了RAWMED，这是合成多表序列的EHR EHR EHR数据的第一个框架，这些框架与原始EHR密切相似。使用基于文本的表示和压缩技术，RAWMed捕获了复杂的结构和时间动力学，并以最小的预处理捕获。我们还提出了一个新的评估框架，用于多桌时间序列综合EHR，评估分布相似性，餐桌间关系，时间动态和隐私。在两个开源EHR数据集上验证了，RAWMED在Fidelity and Utility中的基线模型优于基线模型。该代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: GNN-ViTCap: GNN-Enhanced Multiple Instance Learning with Vision Transformers for Whole Slide Image Classification and Captioning</h3>
<ul>
<li><strong>Authors: </strong>S M Taslim Uddin Raju, Md. Milon Islam, Md Rezwanul Haque, Hamdi Altaheri, Fakhri Karray</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07006">https://arxiv.org/abs/2507.07006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07006">https://arxiv.org/pdf/2507.07006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07006]] GNN-ViTCap: GNN-Enhanced Multiple Instance Learning with Vision Transformers for Whole Slide Image Classification and Captioning(https://arxiv.org/abs/2507.07006)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Microscopic assessment of histopathology images is vital for accurate cancer diagnosis and treatment. Whole Slide Image (WSI) classification and captioning have become crucial tasks in computer-aided pathology. However, microscopic WSI face challenges such as redundant patches and unknown patch positions due to subjective pathologist captures. Moreover, generating automatic pathology captions remains a significant challenge. To address these issues, we introduce a novel GNN-ViTCap framework for classification and caption generation from histopathological microscopic images. First, a visual feature extractor generates patch embeddings. Redundant patches are then removed by dynamically clustering these embeddings using deep embedded clustering and selecting representative patches via a scalar dot attention mechanism. We build a graph by connecting each node to its nearest neighbors in the similarity matrix and apply a graph neural network to capture both local and global context. The aggregated image embeddings are projected into the language model's input space through a linear layer and combined with caption tokens to fine-tune a large language model. We validate our method on the BreakHis and PatchGastric datasets. GNN-ViTCap achieves an F1 score of 0.934 and an AUC of 0.963 for classification, along with a BLEU-4 score of 0.811 and a METEOR score of 0.569 for captioning. Experimental results demonstrate that GNN-ViTCap outperforms state of the art approaches, offering a reliable and efficient solution for microscopy based patient diagnosis.</li>
<li><strong>摘要：</strong>组织病理学图像的微观评估对于准确的癌症诊断和治疗至关重要。整个幻灯片图像（WSI）分类和字幕已成为计算机辅助病理学的关键任务。然而，由于主观病理学家的捕获，微观WSI面临诸如冗余斑块和未知斑块位置之类的挑战。此外，生成自动病理标题仍然是一个重大挑战。为了解决这些问题，我们引入了一个新型的GNN-VitCap框架，用于从组织病理学显微镜图像中分类和字幕产生。首先，视觉功能提取器会生成补丁嵌入。然后，通过使用嵌入式聚类进行动态聚类，通过标量点注意机制将这些嵌入方式动态聚类并选择代表性贴片来消除冗余贴片。我们通过将每个节点连接到相似性矩阵中的最近邻居并应用图形神经网络来捕获本地和全局上下文来构建图形。汇总的图像嵌入通过线性层投影到语言模型的输入空间中，并与标题令牌结合使用，以微调大型语言模型。我们在Breakhis和PatchGastric数据集上验证我们的方法。 GNN-VITCAP的分类为0.934，AUC为0.963，BLEU-4分数为0.811，字幕的流星得分为0.569。实验结果表明，GNN-VitCap的表现优于艺术方法，为基于显微镜的患者诊断提供了可靠，有效的解决方案。</li>
</ul>

<h3>Title: Exact Evaluation of the Accuracy of Diffusion Models for Inverse Problems with Gaussian Data Distributions</h3>
<ul>
<li><strong>Authors: </strong>Emile Pierret, Bruno Galerne</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07008">https://arxiv.org/abs/2507.07008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07008">https://arxiv.org/pdf/2507.07008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07008]] Exact Evaluation of the Accuracy of Diffusion Models for Inverse Problems with Gaussian Data Distributions(https://arxiv.org/abs/2507.07008)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Used as priors for Bayesian inverse problems, diffusion models have recently attracted considerable attention in the literature. Their flexibility and high variance enable them to generate multiple solutions for a given task, such as inpainting, super-resolution, and deblurring. However, several unresolved questions remain about how well they perform. In this article, we investigate the accuracy of these models when applied to a Gaussian data distribution for deblurring. Within this constrained context, we are able to precisely analyze the discrepancy between the theoretical resolution of inverse problems and their resolution obtained using diffusion models by computing the exact Wasserstein distance between the distribution of the diffusion model sampler and the ideal distribution of solutions to the inverse problem. Our findings allow for the comparison of different algorithms from the literature.</li>
<li><strong>摘要：</strong>扩散模型用作贝叶斯逆问题的先验，最近在文献中引起了很大的关注。它们的灵活性和较高的差异使他们能够为给定任务（例如介入，超分辨率和脱布）生成多个解决方案。但是，关于它们的性能仍然存在一些未解决的问题。在本文中，我们研究了这些模型的准确性，用于用于DeBlurring的高斯数据分布。在这种受约束的环境中，我们能够通过计算扩散模型采样器的分布与逆问题的理想分布之间的确切WASSERSTEIN距离来精确分析反相问题的理论解决方案与使用扩散模型获得的分辨率之间的差异。我们的发现可以比较文献中不同算法。</li>
</ul>

<h3>Title: PLAME: Leveraging Pretrained Language Models to Generate Enhanced Protein Multiple Sequence Alignments</h3>
<ul>
<li><strong>Authors: </strong>Hanqun Cao, Xinyi Zhou, Zijun Gao, Chenyu Wang, Xin Gao, Zhi Zhang, Chunbin Gu, Ge Liu, Pheng-Ann Heng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07032">https://arxiv.org/abs/2507.07032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07032">https://arxiv.org/pdf/2507.07032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07032]] PLAME: Leveraging Pretrained Language Models to Generate Enhanced Protein Multiple Sequence Alignments(https://arxiv.org/abs/2507.07032)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, quality assessment</a></li>
<li><strong>Abstract: </strong>Protein structure prediction is essential for drug discovery and understanding biological functions. While recent advancements like AlphaFold have achieved remarkable accuracy, most folding models rely heavily on multiple sequence alignments (MSAs) to boost prediction performance. This dependency limits their effectiveness on low-homology proteins and orphan proteins, where MSA information is sparse or unavailable. To address this limitation, we propose PLAME, a novel MSA design model that leverages evolutionary embeddings from pretrained protein language models. Unlike existing methods, PLAME introduces pretrained representations to enhance evolutionary information and employs a conservation-diversity loss to enhance generation quality. Additionally, we propose a novel MSA selection method to effectively screen high-quality MSAs and improve folding performance. We also propose a sequence quality assessment metric that provides an orthogonal perspective to evaluate MSA quality. On the AlphaFold2 benchmark of low-homology and orphan proteins, PLAME achieves state-of-the-art performance in folding enhancement and sequence quality assessment, with consistent improvements demonstrated on AlphaFold3. Ablation studies validate the effectiveness of the MSA selection method, while extensive case studies on various protein types provide insights into the relationship between AlphaFold's prediction quality and MSA characteristics. Furthermore, we demonstrate that PLAME can serve as an adapter achieving AlphaFold2-level accuracy with the ESMFold's inference speed.</li>
<li><strong>摘要：</strong>蛋白质结构预测对于药物发现和理解生物学功能至关重要。尽管诸如Alphafold之类的最新进步已经达到了显着的准确性，但大多数折叠模型在很大程度上依赖于多个序列比对（MSA）来提高预测性能。这种依赖性将其有效性限制在低体现蛋白和孤儿蛋白上，其中MSA信息稀疏或不可用。为了解决这一限制，我们提出了Plame，这是一种新型的MSA设计模型，利用验证的蛋白质语言模型的进化嵌入。与现有的方法不同，Plame引入了预验证的表示以增强进化信息并采用保护多样性损失以提高发电质量。此外，我们提出了一种新型的MSA选择方法，以有效筛选高质量的MSA并提高折叠性能。我们还提出了一个序列质量评估指标，该指标提供了评估MSA质量的正交视角。在低血压和孤儿蛋白的Alphafold2基准上，Plame在折叠增强和序列质量评估方面取得了最先进的性能，并且在Alphafold3上证明了一致的改进。消融研究验证了MSA选择方法的有效性，而各种蛋白质类型的广泛案例研究为Alphafold的预测质量与MSA特征之间的关系提供了见解。此外，我们证明，铅可以用作Esmfold的推理速度来实现AlphaFold2级准确性的适配器。</li>
</ul>

<h3>Title: Reading a Ruler in the Wild</h3>
<ul>
<li><strong>Authors: </strong>Yimu Pan, Manas Mehta, Gwen Sincerbeaux, Jeffery A. Goldstein, Alison D. Gernand, James Z. Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07077">https://arxiv.org/abs/2507.07077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07077">https://arxiv.org/pdf/2507.07077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07077]] Reading a Ruler in the Wild(https://arxiv.org/abs/2507.07077)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Accurately converting pixel measurements into absolute real-world dimensions remains a fundamental challenge in computer vision and limits progress in key applications such as biomedicine, forensics, nutritional analysis, and e-commerce. We introduce RulerNet, a deep learning framework that robustly infers scale "in the wild" by reformulating ruler reading as a unified keypoint-detection problem and by representing the ruler with geometric-progression parameters that are invariant to perspective transformations. Unlike traditional methods that rely on handcrafted thresholds or rigid, ruler-specific pipelines, RulerNet directly localizes centimeter marks using a distortion-invariant annotation and training strategy, enabling strong generalization across diverse ruler types and imaging conditions while mitigating data scarcity. We also present a scalable synthetic-data pipeline that combines graphics-based ruler generation with ControlNet to add photorealistic context, greatly increasing training diversity and improving performance. To further enhance robustness and efficiency, we propose DeepGP, a lightweight feed-forward network that regresses geometric-progression parameters from noisy marks and eliminates iterative optimization, enabling real-time scale estimation on mobile or edge devices. Experiments show that RulerNet delivers accurate, consistent, and efficient scale estimates under challenging real-world conditions. These results underscore its utility as a generalizable measurement tool and its potential for integration with other vision components for automated, scale-aware analysis in high-impact domains. A live demo is available at this https URL.</li>
<li><strong>摘要：</strong>准确地将像素测量值转换为绝对现实世界的维度仍然是计算机视觉中的一个基本挑战，并限制了诸如生物医学，取证，营养分析和电子商务等关键应用程序的进展。我们介绍了一个深度学习框架，它通过将统治者阅读作为一个统一的关键点检测问题，并通过代表统治者的几何形式 - 促进参数来强烈地“在野外”，从而“野外”中的规模。与依靠手工阈值或刚性的传统方法不同，统治者特定的管道不同，Larernet使用变形不变的注释和培训策略直接定位了厘米标记，从而在缓解数据稀缺的同时，可以在不同的统治者类型和成像条件上进行强有力的概括。我们还提出了可扩展的合成数据管道，该管道将基于图形的标尺生成与ControlNet结合在一起，以增加逼真的环境，大大提高训练多样性并提高性能。为了进一步提高鲁棒性和效率，我们提出了DEEPGP，这是一个轻巧的进料前向网络，可从嘈杂的标记中回归几何形状产生参数，并消除迭代优化，从而对移动设备进行实时尺度估计。实验表明，在挑战性的现实情况下，Larernet可以提供准确，一致和有效的规模估计。这些结果强调了其作为可推广的测量工具的实用性及其与其他视觉组件集成的潜力，以在高影响力域中自动化，比例意识分析。此HTTPS URL可用实时演示。</li>
</ul>

<h3>Title: Evaluating Attribute Confusion in Fashion Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Ziyue Liu, Federico Girella, Yiming Wang, Davide Talon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07079">https://arxiv.org/abs/2507.07079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07079">https://arxiv.org/pdf/2507.07079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07079]] Evaluating Attribute Confusion in Fashion Text-to-Image Generation(https://arxiv.org/abs/2507.07079)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite the rapid advances in Text-to-Image (T2I) generation models, their evaluation remains challenging in domains like fashion, involving complex compositional generation. Recent automated T2I evaluation methods leverage pre-trained vision-language models to measure cross-modal alignment. However, our preliminary study reveals that they are still limited in assessing rich entity-attribute semantics, facing challenges in attribute confusion, i.e., when attributes are correctly depicted but associated to the wrong entities. To address this, we build on a Visual Question Answering (VQA) localization strategy targeting one single entity at a time across both visual and textual modalities. We propose a localized human evaluation protocol and introduce a novel automatic metric, Localized VQAScore (L-VQAScore), that combines visual localization with VQA probing both correct (reflection) and miss-localized (leakage) attribute generation. On a newly curated dataset featuring challenging compositional alignment scenarios, L-VQAScore outperforms state-of-the-art T2I evaluation methods in terms of correlation with human judgments, demonstrating its strength in capturing fine-grained entity-attribute associations. We believe L-VQAScore can be a reliable and scalable alternative to subjective evaluations.</li>
<li><strong>摘要：</strong>尽管文本到图像（T2i）生成模型取得了迅速的进步，但在诸如时尚之类的领域，涉及复杂的构图生成，它们的评估仍然具有挑战性。最近的自动化T2I评估方法利用预训练的视力语言模型来测量跨模式比对。但是，我们的初步研究表明，它们在评估丰富的实体 - 属性语义方面仍然受到限制，在属性混乱中面临挑战，即当正确描绘属性但与错误的实体相关联时。为了解决这个问题，我们以视觉问题回答（VQA）本地化策略为基础，一次针对一个单个实体，一次针对一个视觉和文本方式。我们提出了一个局部的人类评估协议，并引入了一种新型的自动指标，局部VQASCORE（L-VQASCORE），该指标将视觉定位与VQA探测正确（反射）和未定位（泄漏）属性的产生相结合。在新策划的数据集中，具有具有挑战性的组成对齐场景，L-VQASCORE在与人类判断的相关性方面优于最先进的T2I评估方法，这表明了其在捕获细粒度的实体 - 属性关联方面的实力。我们认为L-VQASCORE可以是主观评估的可靠且可扩展的替代方法。</li>
</ul>

<h3>Title: Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data</h3>
<ul>
<li><strong>Authors: </strong>Ke Fan, Shunlin Lu, Minyue Dai, Runyi Yu, Lixing Xiao, Zhiyang Dou, Junting Dong, Lizhuang Ma, Jingbo Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07095">https://arxiv.org/abs/2507.07095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07095">https://arxiv.org/pdf/2507.07095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07095]] Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data(https://arxiv.org/abs/2507.07095)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating diverse and natural human motion sequences based on textual descriptions constitutes a fundamental and challenging research area within the domains of computer vision, graphics, and robotics. Despite significant advancements in this field, current methodologies often face challenges regarding zero-shot generalization capabilities, largely attributable to the limited size of training datasets. Moreover, the lack of a comprehensive evaluation framework impedes the advancement of this task by failing to identify directions for improvement. In this work, we aim to push text-to-motion into a new era, that is, to achieve the generalization ability of zero-shot. To this end, firstly, we develop an efficient annotation pipeline and introduce MotionMillion-the largest human motion dataset to date, featuring over 2,000 hours and 2 million high-quality motion sequences. Additionally, we propose MotionMillion-Eval, the most comprehensive benchmark for evaluating zero-shot motion generation. Leveraging a scalable architecture, we scale our model to 7B parameters and validate its performance on MotionMillion-Eval. Our results demonstrate strong generalization to out-of-domain and complex compositional motions, marking a significant step toward zero-shot human motion generation. The code is available at this https URL.</li>
<li><strong>摘要：</strong>基于文本描述生成多样化和自然的人类运动序列构成了计算机视觉，图形和机器人技术领域内的基本和挑战性的研究领域。尽管在该领域取得了重大进步，但当前的方法论经常在零击泛化能力方面面临挑战，这在很大程度上归因于训练数据集的规模有限。此外，缺乏全面的评估框架通过未能识别改进的方向来阻碍此任务的进步。在这项工作中，我们旨在将文本到动作推向一个新时代，即实现零击的概括能力。为此，首先，我们开发了一个有效的注释管道，并引入了迄今为止最大的人类运动数据集，其中包括2000多个小时和200万个高质量的运动序列。此外，我们提出了动态数字，这是评估零弹性运动产生的最全面的基准。利用可扩展的体系结构，我们将模型扩展到7b参数，并验证其在动感数字上的性能。我们的结果表明，对不域外和复杂组成运动的强烈概括，这标志着朝着零拍的人类运动产生迈出的重要一步。该代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: 4KAgent: Agentic Any Image to 4K Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Yushen Zuo, Qi Zheng, Mingyang Wu, Xinrui Jiang, Renjie Li, Jian Wang, Yide Zhang, Gengchen Mai, Lihong V. Wang, James Zou, Xiaoyu Wang, Ming-Hsuan Yang, Zhengzhong Tu</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07105">https://arxiv.org/abs/2507.07105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07105">https://arxiv.org/pdf/2507.07105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07105]] 4KAgent: Agentic Any Image to 4K Super-Resolution(https://arxiv.org/abs/2507.07105)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, super-resolution, quality assessment</a></li>
<li><strong>Abstract: </strong>We present 4KAgent, a unified agentic super-resolution generalist system designed to universally upscale any image to 4K resolution (and even higher, if applied iteratively). Our system can transform images from extremely low resolutions with severe degradations, for example, highly distorted inputs at 256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three core components: (1) Profiling, a module that customizes the 4KAgent pipeline based on bespoke use cases; (2) A Perception Agent, which leverages vision-language models alongside image quality assessment experts to analyze the input image and make a tailored restoration plan; and (3) A Restoration Agent, which executes the plan, following a recursive execution-reflection paradigm, guided by a quality-driven mixture-of-expert policy to select the optimal output for each step. Additionally, 4KAgent embeds a specialized face restoration pipeline, significantly enhancing facial details in portrait and selfie photos. We rigorously evaluate our 4KAgent across 11 distinct task categories encompassing a total of 26 diverse benchmarks, setting new state-of-the-art on a broad spectrum of imaging domains. Our evaluations cover natural images, portrait photos, AI-generated content, satellite imagery, fluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and X-ray, demonstrating superior performance in terms of both perceptual (e.g., NIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic paradigm for low-level vision tasks, we aim to catalyze broader interest and innovation within vision-centric autonomous agents across diverse research communities. We will release all the code, models, and results at: this https URL.</li>
<li><strong>摘要：</strong>我们提出了4Kagent，这是一种统一的代理超级分辨率的通才系统，旨在普遍地将任何图像提高到4K分辨率（如果迭代地应用甚至更高）。我们的系统可以将图像从具有严重降解的极低分辨率转化，例如，在256x256的高度扭曲的输入中，将图像转变为晶体清晰的，逼真的4K输出。 4Kagent包括三个核心组件：（1）分析，一个基于定制用例的4Kagent管道的模块； （2）一种感知剂，它利用视觉模型与图像质量评估专家一起分析输入图像并制定量身定制的恢复计划； （3）按照递归执行反射范式执行计划的恢复代理，并在质量驱动的专家策略中指导，以选择每个步骤的最佳输出。此外，4Kagent嵌入了专门的面部修复管道，可显着增强肖像和自拍照片中的面部细节。我们在11个不同的任务类别中严格评估了我们的4Kagent，其中包括总共26种不同的基准测试，从而在广泛的成像域中设定了新的最新时间。我们的评估涵盖了自然图像，肖像照片，AI生成的内容，卫星图像，荧光显微镜以及医学成像，例如基础镜检查，超声和X射线，在感知方面（例如NIQE，MUSIQ）和Fidelity（例如，PSNR）表示，在感知方面表现出了出色的表现。通过建立用于低级视觉任务的新型代理范式，我们旨在促进以各种研究社区为中心的以视觉为中心的自主代理的更广泛的兴趣和创新。我们将在以下位置发布所有代码，模型和结果：此HTTPS URL。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
