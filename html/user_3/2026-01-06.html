<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-01-06</h1>
<h3>Title: Can Generative Models Actually Forge Realistic Identity Documents?</h3>
<ul>
<li><strong>Authors: </strong>Alexander Vinogradov</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00829">https://arxiv.org/abs/2601.00829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00829">https://arxiv.org/pdf/2601.00829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00829]] Can Generative Models Actually Forge Realistic Identity Documents?(https://arxiv.org/abs/2601.00829)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative image models have recently shown significant progress in image realism, leading to public concerns about their potential misuse for document forgery. This paper explores whether contemporary open-source and publicly accessible diffusion-based generative models can produce identity document forgeries that could realistically bypass human or automated verification systems. We evaluate text-to-image and image-to-image generation pipelines using multiple publicly available generative model families, including Stable Diffusion, Qwen, Flux, Nano-Banana, and others. The findings indicate that while current generative models can simulate surface-level document aesthetics, they fail to reproduce structural and forensic authenticity. Consequently, the risk of generative identity document deepfakes achieving forensic-level authenticity may be overestimated, underscoring the value of collaboration between machine learning practitioners and document-forensics experts in realistic risk assessment.</li>
<li><strong>摘要：</strong>生成图像模型最近在图像真实感方面取得了重大进展，导致公众担心它们可能被滥用于文件伪造。本文探讨了当代开源和可公开访问的基于扩散的生成模型是否可以产生实际上可以绕过人类或自动验证系统的身份证件伪造品。我们使用多个公开可用的生成模型系列（包括 Stable Diffusion、Qwen、Flux、Nano-Banana 等）评估文本到图像和图像到图像的生成流程。研究结果表明，虽然当前的生成模型可以模拟表面级别的文档美学，但它们无法再现结构和法医真实性。因此，生成身份文档深度伪造实现取证级别真实性的风险可能被高估，这凸显了机器学习从业者和文档取证专家之间在实际风险评估中合作的价值。</li>
</ul>

<h3>Title: Intrinsic-Metric Physics-Informed Neural Networks (IM-PINN) for Reaction-Diffusion Dynamics on Complex Riemannian Manifolds</h3>
<ul>
<li><strong>Authors: </strong>Julian Evan Chrisnanto, Salsabila Rahma Alia, Nurfauzi Fadillah, Yulison Herry Chrisnanto</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00834">https://arxiv.org/abs/2601.00834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00834">https://arxiv.org/pdf/2601.00834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00834]] Intrinsic-Metric Physics-Informed Neural Networks (IM-PINN) for Reaction-Diffusion Dynamics on Complex Riemannian Manifolds(https://arxiv.org/abs/2601.00834)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Simulating nonlinear reaction-diffusion dynamics on complex, non-Euclidean manifolds remains a fundamental challenge in computational morphogenesis, constrained by high-fidelity mesh generation costs and symplectic drift in discrete time-stepping schemes. This study introduces the Intrinsic-Metric Physics-Informed Neural Network (IM-PINN), a mesh-free geometric deep learning framework that solves partial differential equations directly in the continuous parametric domain. By embedding the Riemannian metric tensor into the automatic differentiation graph, our architecture analytically reconstructs the Laplace-Beltrami operator, decoupling solution complexity from geometric discretization. We validate the framework on a "Stochastic Cloth" manifold with extreme Gaussian curvature fluctuations ($K \in [-2489, 3580]$), where traditional adaptive refinement fails to resolve anisotropic Turing instabilities. Using a dual-stream architecture with Fourier feature embeddings to mitigate spectral bias, the IM-PINN recovers the "splitting spot" and "labyrinthine" regimes of the Gray-Scott model. Benchmarking against the Surface Finite Element Method (SFEM) reveals superior physical rigor: the IM-PINN achieves global mass conservation error of $\mathcal{E}_{mass} \approx 0.157$ versus SFEM's $0.258$, acting as a thermodynamically consistent global solver that eliminates mass drift inherent in semi-implicit integration. The framework offers a memory-efficient, resolution-independent paradigm for simulating biological pattern formation on evolving surfaces, bridging differential geometry and physics-informed machine learning.</li>
<li><strong>摘要：</strong>在复杂的非欧几里得流形上模拟非线性反应扩散动力学仍然是计算形态发生中的一个基本挑战，受到高保真网格生成成本和离散时间步进方案中辛漂移的限制。本研究引入了本征度量物理信息神经网络 (IM-PINN)，这是一种无网格几何深度学习框架，可直接在连续参数域中求解偏微分方程。通过将黎曼度量张量嵌入到自动微分图中，我们的架构分析性地重建了 Laplace-Beltrami 算子，将解决方案的复杂性与几何离散化解耦。我们在具有极端高斯曲率波动（$K \in [-2489, 3580]$）的“随机布料”流形上验证了该框架，其中传统的自适应细化无法解决各向异性图灵不稳定性。 IM-PINN 使用具有傅立叶特征嵌入的双流架构来减轻频谱偏差，恢复了 Gray-Scott 模型的“分裂点”和“迷宫”状态。表面有限元法 (SFEM) 的基准测试揭示了卓越的物理严谨性：IM-PINN 实现的全局质量守恒误差为 $\mathcal{E}_{mass} \approx 0.157$，而 SFEM 为 $0.258$，充当热力学一致的全局求解器，消除了半隐式积分中固有的质量漂移。该框架提供了一种内存高效、与分辨率无关的范例，用于模拟不断变化的表面上的生物模式形成，桥接微分几何和物理信息机器学习。</li>
</ul>

<h3>Title: SLO-Conditioned Action Routing for Retrieval-Augmented Generation: Objective Ablation and Failure Modes</h3>
<ul>
<li><strong>Authors: </strong>Bharath Nunepalli</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00841">https://arxiv.org/abs/2601.00841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00841">https://arxiv.org/pdf/2601.00841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00841]] SLO-Conditioned Action Routing for Retrieval-Augmented Generation: Objective Ablation and Failure Modes(https://arxiv.org/abs/2601.00841)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) introduces a practical control problem: retrieval depth and generation behavior must be chosen per query to satisfy service-level objectives (SLOs) such as cost, refusal rate, and hallucination risk. This work models per-query control as a small discrete action: choose a retrieval depth and a generation mode (guarded vs. auto), or refuse. An offline logged dataset is constructed from SQuAD 2.0 by executing each action and recording accuracy, token cost, hallucination/refusal indicators, and an SLO-weighted reward. Two simple policy-learning objectives are evaluated: supervised classification of the per-state best action (Argmax-CE) and a reward-weighted variant (Argmax-CE-WT). Across the evaluated settings, a strong fixed baseline (low k, guarded prompting) performs competitively; learned policies mainly provide additional cost savings under a quality-focused SLO and can exhibit refusal collapse under a cheap SLO when refusal is heavily rewarded. The contribution is a reproducible case study of SLO-aware control for RAG pipelines, emphasizing failure modes and reporting conventions rather than proposing a new retriever or language model.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 引入了一个实际控制问题：必须根据查询选择检索深度和生成行为，以满足服务级别目标 (SLO)，例如成本、拒绝率和幻觉风险。这项工作将每个查询控制建模为一个小的离散操作：选择检索深度和生成模式（保护与自动），或者拒绝。离线记录的数据集是根据 SQuAD 2.0 构建的，通过执行每个操作并记录准确性、代币成本、幻觉/拒绝指标和 SLO 加权奖励。评估两个简单的策略学习目标：每个状态最佳操作的监督分类（Argmax-CE）和奖励加权变体（Argmax-CE-WT）。在评估的设置中，强大的固定基线（低 k、受保护的提示）具有竞争力；学习到的策略主要在注重质量的 SLO 下提供额外的成本节省，并且在廉价的 SLO 下，当拒绝获得丰厚回报时，可能会出现拒绝崩溃。该贡献是对 RAG 管道的 SLO 感知控制的可重复案例研究，强调故障模式和报告约定，而不是提出新的检索器或语言模型。</li>
</ul>

<h3>Title: EdgeJury: Cross-Reviewed Small-Model Ensembles for Truthful Question Answering on Serverless Edge Inference</h3>
<ul>
<li><strong>Authors: </strong>Aayush Kumar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00850">https://arxiv.org/abs/2601.00850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00850">https://arxiv.org/pdf/2601.00850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00850]] EdgeJury: Cross-Reviewed Small-Model Ensembles for Truthful Question Answering on Serverless Edge Inference(https://arxiv.org/abs/2601.00850)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Hallucinations hinder reliable question answering, especially in resource-constrained deployments where frontier-scale models or retrieval pipelines may be impractical. We present EdgeJury, a lightweight ensemble framework that improves truthfulness and robustness using only small instruction-tuned language models (3B-8B) suitable for serverless edge inference. EdgeJury orchestrates four stages: (1) parallel role-specialized generation, (2) anonymized cross-review with structured critiques and rankings, (3) chairman synthesis that integrates the strongest content while addressing flagged issues, and (4) claim-level consistency labeling based on inter-model agreement. On TruthfulQA (MC1), EdgeJury achieves 76.2% accuracy (95% CI: 72.8-79.6%), a +21.4% relative improvement over a single 8B baseline (62.8%), and outperforms standard baselines including self-consistency and majority voting under transparent compute accounting (total tokens and platform cost reported). On a 200-question adversarial EdgeCases set, EdgeJury yields +48.2% relative gains (95% CI: 44.0-52.4%). Manual analysis on 100 incorrect answers shows an approximately 55% reduction in factual hallucination errors versus the single-model baseline. Deployed on Cloudflare Workers AI, EdgeJury achieves 8.4 s median end-to-end latency, demonstrating that coordinated small-model ensembles can improve truthfulness on misconception-heavy QA benchmarks without external retrieval or proprietary large-model APIs.</li>
<li><strong>摘要：</strong>幻觉阻碍了可靠的问题回答，特别是在资源有限的部署中，前沿规模的模型或检索管道可能不切实际。我们推出了 EdgeJury，这是一个轻量级集成框架，仅使用适合无服务器边缘推理的小型指令调整语言模型 (3B-8B) 即可提高真实性和鲁棒性。 EdgeJury 精心策划了四个阶段：(1) 并行角色专业化生成，(2) 具有结构化评论和排名的匿名交叉评审，(3) 主席综合，在解决标记问题的同时整合最强的内容，以及 (4) 基于模型间协议的声明级一致性标签。在 TruthfulQA (MC1) 上，EdgeJury 实现了 76.2% 的准确率（95% CI：72.8-79.6%），比单个 8B 基线（62.8%）相对提高了 21.4%，并且优于标准基线，包括透明计算会计下的自我一致性和多数投票（报告的总代币和平台成本）。在 200 个问题的对抗性 EdgeCases 集上，EdgeJury 的相对收益为 +48.2%（95% CI：44.0-52.4%）。对 100 个错误答案的手动分析表明，与单一模型基线相比，事实幻觉错误减少了约 55%。 EdgeJury 部署在 Cloudflare Workers AI 上，实现了 8.4 秒的平均端到端延迟，这表明协调的小模型集成可以提高误解严重的 QA 基准的真实性，而无需外部检索或专有的大模型 API。</li>
</ul>

<h3>Title: Path Integral Solution for Dissipative Generative Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Xidi Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.app-ph, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00860">https://arxiv.org/abs/2601.00860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00860">https://arxiv.org/pdf/2601.00860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00860]] Path Integral Solution for Dissipative Generative Dynamics(https://arxiv.org/abs/2601.00860)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Can purely mechanical systems generate intelligent language? We prove that dissipative quantum dynamics with analytically tractable non-local context aggregation produce coherent text generation, while conservation laws cause fundamental failure. Employing Koopman operators with closed-form path integral propagators, we show irreversible computation fundamentally requires both controlled information dissipation and causal context aggregation. Spectral analysis reveals emergent eigenvalue structure, separating into decay modes (forgetting), growth modes (amplification), and neutral modes (preservation) -- the essential ingredients for directed information flow. Hamiltonian constraints force the elimination of these dissipative modes and degrading performance despite unchanged model capacity. This establishes language generation as dissipative quantum field theory, proving mechanical systems acquire intelligence through the combination of dissipation and non-locality, not through conservation.</li>
<li><strong>摘要：</strong>纯机械系统能产生智能语言吗？我们证明，具有可分析处理的非局部上下文聚合的耗散量子动力学可以产生连贯的文本生成，而守恒定律会导致根本性的失败。使用具有封闭形式路径积分传播器的库普曼算子，我们表明不可逆计算从根本上需要受控信息耗散和因果上下文聚合。谱分析揭示了新兴的特征值结构，分为衰减模式（遗忘）、增长模式（放大）和中性模式（保存）——定向信息流的基本要素。尽管模型容量不变，但哈密顿约束迫使消除这些耗散模式并降低性能。这将语言生成确立为耗散量子场论，证明机械系统通过耗散和非定域性的结合而不是通过守恒来获得智能。</li>
</ul>

<h3>Title: Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery</h3>
<ul>
<li><strong>Authors: </strong>Markus J. Buehler</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.dis-nn, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00863">https://arxiv.org/abs/2601.00863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00863">https://arxiv.org/pdf/2601.00863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00863]] Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery(https://arxiv.org/abs/2601.00863)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce materiomusic as a generative framework linking the hierarchical structures of matter with the compositional logic of music. Across proteins, spider webs and flame dynamics, vibrational and architectural principles recur as tonal hierarchies, harmonic progressions, and long-range musical form. Using reversible mappings, from molecular spectra to musical tones and from three-dimensional networks to playable instruments, we show how sound functions as a scientific probe, an epistemic inversion where listening becomes a mode of seeing and musical composition becomes a blueprint for matter. These mappings excavate deep time: patterns originating in femtosecond molecular vibrations or billion-year evolutionary histories become audible. We posit that novelty in science and art emerges when constraints cannot be satisfied within existing degrees of freedom, forcing expansion of the space of viable configurations. Selective imperfection provides the mechanism restoring balance between coherence and adaptability. Quantitative support comes from exhaustive enumeration of all 2^12 musical scales, revealing that culturally significant systems cluster in a mid-entropy, mid-defect corridor, directly paralleling the Hall-Petch optimum where intermediate defect densities maximize material strength. Iterating these mappings creates productive collisions between human creativity and physics, generating new information as musical structures encounter evolutionary constraints. We show how swarm-based AI models compose music exhibiting human-like structural signatures such as small-world connectivity, modular integration, long-range coherence, suggesting a route beyond interpolation toward invention. We show that science and art are generative acts of world-building under constraint, with vibration as a shared grammar organizing structure across scales.</li>
<li><strong>摘要：</strong>我们引入物质音乐作为一种生成框架，将物质的层次结构与音乐的作曲逻辑联系起来。跨越蛋白质、蜘蛛网和火焰动力学，振动和建筑原理以音调层次、和声进行和长音域音乐形式的形式重现。使用可逆映射，从分子光谱到音乐音调，从三维网络到可演奏的乐器，我们展示了声音如何作为科学探针发挥作用，这是一种认知倒转，其中聆听成为一种观看方式，音乐作品成为物质的蓝图。这些映射挖掘了深层时间：源自飞秒分子振动或十亿年进化历史的模式变得清晰可闻。我们假设，当现有自由度内无法满足约束时，科学和艺术中的新颖性就会出现，从而迫使可行配置的空间扩大。选择性缺陷提供了恢复一致性和适应性之间平衡的机制。定量支持来自对所有 2^12 音阶的详尽枚举，揭示了具有文化意义的系统聚集在中熵、中缺陷走廊中，直接平行于 Hall-Petch 最佳值，其中中间缺陷密度最大化材料强度。迭代这些映射会在人类创造力和物理学之间产生富有成效的碰撞，当音乐结构遇到进化限制时生成新信息。我们展示了基于群体的人工智能模型如何创作具有类人结构特征的音乐，例如小世界连接性、模块化集成、远程一致性，这表明了一条超越插值的发明之路。我们表明，科学和艺术是在约束下构建世界的生成行为，振动作为跨尺度的共享语法组织结构。</li>
</ul>

<h3>Title: Quantum Machine Learning Approaches for Coordinated Stealth Attack Detection in Distributed Generation Systems</h3>
<ul>
<li><strong>Authors: </strong>Osasumwen Cedric Ogiesoba-Eguakun, Suman Rath</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00873">https://arxiv.org/abs/2601.00873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00873">https://arxiv.org/pdf/2601.00873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00873]] Quantum Machine Learning Approaches for Coordinated Stealth Attack Detection in Distributed Generation Systems(https://arxiv.org/abs/2601.00873)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Coordinated stealth attacks are a serious cybersecurity threat to distributed generation systems because they modify control and measurement signals while remaining close to normal behavior, making them difficult to detect using standard intrusion detection methods. This study investigates quantum machine learning approaches for detecting coordinated stealth attacks on a distributed generation unit in a microgrid. High-quality simulated measurements were used to create a balanced binary classification dataset using three features: reactive power at DG1, frequency deviation relative to the nominal value, and terminal voltage magnitude. Classical machine learning baselines, fully quantum variational classifiers, and hybrid quantum classical models were evaluated. The results show that a hybrid quantum classical model combining quantum feature embeddings with a classical RBF support vector machine achieves the best overall performance on this low dimensional dataset, with a modest improvement in accuracy and F1 score over a strong classical SVM baseline. Fully quantum models perform worse due to training instability and limitations of current NISQ hardware. In contrast, hybrid models train more reliably and demonstrate that quantum feature mapping can enhance intrusion detection even when fully quantum learning is not yet practical.</li>
<li><strong>摘要：</strong>协同隐形攻击对分布式发电系统构成严重的网络安全威胁，因为它们会修改控制和测量信号，同时保持接近正常行为，从而很难使用标准入侵检测方法进行检测。这项研究研究了量子机器学习方法，用于检测对微电网中分布式发电单元的协调隐形攻击。使用高质量模拟测量来创建平衡的二元分类数据集，该数据集使用三个特征：DG1 的无功功率、相对于标称值的频率偏差以及端电压幅度。评估了经典机器学习基线、全量子变分分类器和混合量子经典模型。结果表明，将量子特征嵌入与经典 RBF 支持向量机相结合的混合量子经典模型在此低维数据集上实现了最佳整体性能，与强大的经典 SVM 基线相比，准确度和 F1 分数略有提高。由于训练不稳定和当前 NISQ 硬件的限制，全量子模型的性能较差。相比之下，混合模型训练更可靠，并证明即使完全量子学习尚不可行，量子特征映射也可以增强入侵检测。</li>
</ul>

<h3>Title: VideoCuRL: Video Curriculum Reinforcement Learning with Orthogonal Difficulty Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Hongbo Jin, Kuanwei Lin, Wenhao Zhang, Yichen Jin, Ge Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00887">https://arxiv.org/abs/2601.00887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00887">https://arxiv.org/pdf/2601.00887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00887]] VideoCuRL: Video Curriculum Reinforcement Learning with Orthogonal Difficulty Decomposition(https://arxiv.org/abs/2601.00887)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) is crucial for empowering VideoLLMs with complex spatiotemporal reasoning. However, current RL paradigms predominantly rely on random data shuffling or naive curriculum strategies based on scalar difficulty metrics. We argue that scalar metrics fail to disentangle two orthogonal challenges in video understanding: Visual Temporal Perception Load and Cognitive Reasoning Depth. To address this, we propose VideoCuRL, a novel framework that decomposes difficulty into these two axes. We employ efficient, training-free proxies, optical flow and keyframe entropy for visual complexity, Calibrated Surprisal for cognitive complexity, to map data onto a 2D curriculum grid. A competence aware Diagonal Wavefront strategy then schedules training from base alignment to complex reasoning. Furthermore, we introduce Dynamic Sparse KL and Structured Revisiting to stabilize training against reward collapse and catastrophic forgetting. Extensive experiments show that VideoCuRL surpasses strong RL baselines on reasoning (+2.5 on VSI-Bench) and perception (+2.9 on VideoMME) tasks. Notably, VideoCuRL eliminates the prohibitive inference overhead of generation-based curricula, offering a scalable solution for robust video post-training.</li>
<li><strong>摘要：</strong>强化学习 (RL) 对于赋予 VideoLLM 复杂的时空推理能力至关重要。然而，当前的强化学习范式主要依赖于随机数据洗牌或基于标量难度指标的朴素课程策略。我们认为标量指标无法解决视频理解中的两个正交挑战：视觉时间感知负载和认知推理深度。为了解决这个问题，我们提出了 VideoCuRL，这是一种将难度分解为这两个轴的新颖框架。我们采用高效、免训练的代理、光流和关键帧熵来解决视觉复杂性，使用校准惊喜来解决认知复杂性，将数据映射到 2D 课程网格上。然后，能力意识对角波前策略安排从基本对齐到复杂推理的培训。此外，我们引入了动态稀疏 KL 和结构化重访来稳定训练，防止奖励崩溃和灾难性遗忘。大量实验表明，VideoCuRL 在推理（VSI-Bench 上+2.5）和感知（VideoMME 上+2.9）任务上超越了强大的 RL 基线。值得注意的是，VideoCuRL 消除了基于生成的课程的令人望而却步的推理开销，为强大的视频后期训练提供了可扩展的解决方案。</li>
</ul>

<h3>Title: Comparative Evaluation of CNN Architectures for Neural Style Transfer in Indonesian Batik Motif Generation: A Comprehensive Study</h3>
<ul>
<li><strong>Authors: </strong>Happy Gery Pangestu, Andi Prademon Yunus, Siti Khomsah</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00888">https://arxiv.org/abs/2601.00888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00888">https://arxiv.org/pdf/2601.00888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00888]] Comparative Evaluation of CNN Architectures for Neural Style Transfer in Indonesian Batik Motif Generation: A Comprehensive Study(https://arxiv.org/abs/2601.00888)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Neural Style Transfer (NST) provides a computational framework for the digital preservation and generative exploration of Indonesian batik motifs; however, existing approaches remain largely centered on VGG-based architectures whose strong stylistic expressiveness comes at the cost of high computational and memory demands, that limits practical deployment in resource-limited environments. This study presents a systematic comparative analysis of five widely used CNN backbones, namely VGG16, VGG19, Inception V3, ResNet50, and ResNet101, based on 245 controlled experiments combining quantitative metrics, qualitative assessment, and statistical analysis to examine the trade-off between structural preservation, stylistic behavior, and computational efficiency. The results show that backbone selection does not yield statistically significant differences in structural similarity, as confirmed by ANOVA on SSIM (p= 0.83), indicating comparable levels of structural preservation rather than equivalent stylistic quality. Within this context, ResNet-based architectures achieve approximately 5-6x faster convergence than VGG models while maintaining similar perceptual similarity (LPIPS = 0.53) and requiring over 16x fewer FLOPs (0.63 vs 10.12 GFLOPs). Qualitative analysis reveals consistent stylistic trade-offs, with VGG producing denser painterly textures, ResNet favoring geometric stability and canting stroke preservation with milder stylization, and Inception V3 exhibiting intermediate but noisier behavior. These findings reposition architectural choice in NST from maximizing stylistic intensity toward efficiency-aware and structure-preserving deployment, highlighting ResNet-based backbones as a practical foundation for scalable, industry-oriented batik generation.</li>
<li><strong>摘要：</strong>神经风格迁移（NST）为印度尼西亚蜡染图案的数字保存和生成探索提供了计算框架；然而，现有的方法仍然主要集中在基于 VGG 的架构上，其强大的风格表现力是以高计算和内存需求为代价的，这限制了资源有限环境中的实际部署。本研究基于 245 个对照实验，结合定量指标、定性评估和统计分析，对五种广泛使用的 CNN 主干网络（即 VGG16、VGG19、Inception V3、ResNet50 和 ResNet101）进行了系统比较分析，以检验结构保存、风格行为和计算效率之间的权衡。结果表明，正如 SSIM 上的方差分析所证实的那样，主干选择不会产生统计上显着的结构相似性差异（p = 0.83），表明结构保留水平相当，而不是同等的风格质量。在此背景下，基于 ResNet 的架构的收敛速度比 VGG 模型快约 5-6 倍，同时保持相似的感知相似性 (LPIPS = 0.53)，并且需要的 FLOP 减少 16 倍以上（0.63 vs 10.12 GFLOP）。定性分析揭示了一致的风格权衡，VGG 产生更密集的绘画纹理，ResNet 有利于几何稳定性和倾斜笔画保留，具有更温和的风格，而 Inception V3 则表现出中等但噪音更大的行为。这些发现重新定位了 NST 中的架构选择，从最大限度地提高风格强度转向注重效率和保留结构的部署，强调基于 ResNet 的骨干网作为可扩展、面向行业的蜡染生成的实用基础。</li>
</ul>

<h3>Title: When to Ponder: Adaptive Compute Allocation for Code Generation via Test-Time Training</h3>
<ul>
<li><strong>Authors: </strong>Gihyeon Sim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00894">https://arxiv.org/abs/2601.00894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00894">https://arxiv.org/pdf/2601.00894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00894]] When to Ponder: Adaptive Compute Allocation for Code Generation via Test-Time Training(https://arxiv.org/abs/2601.00894)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models apply uniform computation to all inputs, regardless of difficulty. We propose PonderTTT, a gating strategy using the TTT layer's self-supervised reconstruction loss to selectively trigger Test-Time Training (TTT) updates. The gating decision itself is training-free--requiring no learned classifier or auxiliary networks; only a single scalar threshold is initially calibrated on unlabeled data and continuously adapted via EMA to maintain target update rates. Our experiments with GPT-2 models (124M to 1.5B) on code language modeling (The Stack v2, teacher-forced perplexity) demonstrate that this signal is inference-compatible, requiring no ground-truth labels. Our Reconstruction Gating achieves 82-89% Oracle Recovery while being fully training-free, significantly outperforming Random Skip baselines (up to 16% lower loss on OOD languages).</li>
<li><strong>摘要：</strong>大型语言模型对所有输入应用统一计算，无论难度如何。我们提出了 PonderTTT，这是一种门控策略，使用 TTT 层的自监督重建损失来选择性地触发测试时训练（TTT）更新。门控决策本身是免训练的——不需要学习分类器或辅助网络；最初仅在未标记数据上校准单个标量阈值，并通过 EMA 不断调整以维持目标更新率。我们在代码语言建模（The Stack v2，教师强制困惑）上使用 GPT-2 模型（124M 到 1.5B）进行的实验表明，该信号是推理兼容的，不需要真实标签。我们的重建门控实现了 82-89% 的 Oracle 恢复率，同时完全无需训练，显着优于随机跳过基线（OOD 语言的损失降低高达 16%）。</li>
</ul>

<h3>Title: CornViT: A Multi-Stage Convolutional Vision Transformer Framework for Hierarchical Corn Kernel Analysis</h3>
<ul>
<li><strong>Authors: </strong>Sai Teja Erukude, Jane Mascarenhas, Lior Shamir</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00897">https://arxiv.org/abs/2601.00897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00897">https://arxiv.org/pdf/2601.00897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00897]] CornViT: A Multi-Stage Convolutional Vision Transformer Framework for Hierarchical Corn Kernel Analysis(https://arxiv.org/abs/2601.00897)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Accurate grading of corn kernels is critical for seed certification, directional seeding, and breeding, yet it is still predominantly performed by manual inspection. This work introduces CornViT, a three-stage Convolutional Vision Transformer (CvT) framework that emulates the hierarchical reasoning of human seed analysts for single-kernel evaluation. Three sequential CvT-13 classifiers operate on 384x384 RGB images: Stage 1 distinguishes pure from impure kernels; Stage 2 categorizes pure kernels into flat and round morphologies; and Stage 3 determines the embryo orientation (up vs. down) for pure, flat kernels. Starting from a public corn seed image collection, we manually relabeled and filtered images to construct three stage-specific datasets: 7265 kernels for purity, 3859 pure kernels for morphology, and 1960 pure-flat kernels for embryo orientation, all released as benchmarks. Head-only fine-tuning of ImageNet-22k pretrained CvT-13 backbones yields test accuracies of 93.76% for purity, 94.11% for shape, and 91.12% for embryo-orientation detection. Under identical training conditions, ResNet-50 reaches only 76.56 to 81.02 percent, whereas DenseNet-121 attains 86.56 to 89.38 percent accuracy. These results highlight the advantages of convolution-augmented self-attention for kernel analysis. To facilitate adoption, we deploy CornViT in a Flask-based web application that performs stage-wise inference and exposes interpretable outputs through a browser interface. Together, the CornViT framework, curated datasets, and web application provide a deployable solution for automated corn kernel quality assessment in seed quality workflows. Source code and data are publicly available.</li>
<li><strong>摘要：</strong>玉米粒的准确分级对于种子认证、定向播种和育种至关重要，但仍主要通过人工检查进行。这项工作引入了 CornViT，这是一个三阶段卷积视觉变换器 (CvT) 框架，可模拟人类种子分析师的分层推理以进行单内核评估。三个连续的 CvT-13 分类器对 384x384 RGB 图像进行操作：第 1 阶段区分纯内核和不纯内核；第 2 阶段将纯籽粒分为扁平和圆形两种形态；第 3 阶段确定纯扁平籽粒的胚胎方向（向上或向下）。从公共玉米种子图像集合开始，我们手动重新标记和过滤图像，以构建三个特定阶段的数据集：7265 个用于纯度的颗粒、3859 个用于形态的纯颗粒和 1960 个用于胚胎方向的纯扁平颗粒，所有这些都作为基准发布。对 ImageNet-22k 预训练的 CvT-13 主干进行仅头部微调，纯度测试精度为 93.76%，形状测试精度为 94.11%，胚胎方向检测测试精度为 91.12%。在相同的训练条件下，ResNet-50 的准确率仅为 76.56% 至 81.02%，而 DenseNet-121 的准确率达到 86.56% 至 89.38%。这些结果凸显了卷积增强自注意力在核分析中的优势。为了促进采用，我们将 CornViT 部署在基于 Flask 的 Web 应用程序中，该应用程序执行分阶段推理并通过浏览器界面公开可解释的输出。 CornViT 框架、精选数据集和 Web 应用程序共同为种子质量工作流程中的自动化玉米粒质量评估提供了可部署的解决方案。源代码和数据是公开的。</li>
</ul>

<h3>Title: Dichotomous Diffusion Policy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Ruiming Liang, Yinan Zheng, Kexin Zheng, Tianyi Tan, Jianxiong Li, Liyuan Mao, Zhihao Wang, Guang Chen, Hangjun Ye, Jingjing Liu, Jinqiao Wang, Xianyuan Zhan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00898">https://arxiv.org/abs/2601.00898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00898">https://arxiv.org/pdf/2601.00898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00898]] Dichotomous Diffusion Policy Optimization(https://arxiv.org/abs/2601.00898)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion-based policies have gained growing popularity in solving a wide range of decision-making tasks due to their superior expressiveness and controllable generation during inference. However, effectively training large diffusion policies using reinforcement learning (RL) remains challenging. Existing methods either suffer from unstable training due to directly maximizing value objectives, or face computational issues due to relying on crude Gaussian likelihood approximation, which requires a large amount of sufficiently small denoising steps. In this work, we propose DIPOLE (Dichotomous diffusion Policy improvement), a novel RL algorithm designed for stable and controllable diffusion policy optimization. We begin by revisiting the KL-regularized objective in RL, which offers a desirable weighted regression objective for diffusion policy extraction, but often struggles to balance greediness and stability. We then formulate a greedified policy regularization scheme, which naturally enables decomposing the optimal policy into a pair of stably learned dichotomous policies: one aims at reward maximization, and the other focuses on reward minimization. Under such a design, optimized actions can be generated by linearly combining the scores of dichotomous policies during inference, thereby enabling flexible control over the level of this http URL in offline and offline-to-online RL settings on ExORL and OGBench demonstrate the effectiveness of our approach. We also use DIPOLE to train a large vision-language-action (VLA) model for end-to-end autonomous driving (AD) and evaluate it on the large-scale real-world AD benchmark NAVSIM, highlighting its potential for complex real-world applications.</li>
<li><strong>摘要：</strong>基于扩散的策略由于其卓越的表达能力和推理过程中的可控生成而在解决各种决策任务中越来越受欢迎。然而，使用强化学习（RL）有效地训练大规模扩散策略仍然具有挑战性。现有方法要么由于直接最大化价值目标而遭受不稳定的训练，要么由于依赖粗略的高斯似然近似而面临计算问题，这需要大量足够小的去噪步骤。在这项工作中，我们提出了 DIPOLE（二分扩散策略改进），这是一种专为稳定可控扩散策略优化而设计的新型强化学习算法。我们首先重新审视 RL 中的 KL 正则化目标，它为扩散策略提取提供了理想的加权回归目标，但常常难以平衡贪婪和稳定性。然后，我们制定了一个贪婪的策略正则化方案，它自然地能够将最优策略分解为一对稳定学习的二分策略：一个旨在奖励最大化，另一个专注于奖励最小化。在这样的设计下，可以通过在推理过程中线性组合二分策略的分数来生成优化的动作，从而能够在 ExORL 和 OGBench 上的离线和离线到在线 RL 设置中灵活控制此 http URL 的级别，这证明了我们方法的有效性。我们还使用 DIPOLE 训练用于端到端自动驾驶 (AD) 的大型视觉语言动作 (VLA) 模型，并在大规模现实世界 AD 基准 NAVSIM 上对其进行评估，突出其在复杂现实世界应用中的潜力。</li>
</ul>

<h3>Title: Latent-Constrained Conditional VAEs for Augmenting Large-Scale Climate Ensembles</h3>
<ul>
<li><strong>Authors: </strong>Jacquelyn Shelton, Przemyslaw Polewski, Alexander Robel, Matthew Hoffman, Stephen Price</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00915">https://arxiv.org/abs/2601.00915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00915">https://arxiv.org/pdf/2601.00915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00915]] Latent-Constrained Conditional VAEs for Augmenting Large-Scale Climate Ensembles(https://arxiv.org/abs/2601.00915)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large climate-model ensembles are computationally expensive; yet many downstream analyses would benefit from additional, statistically consistent realizations of spatiotemporal climate variables. We study a generative modeling approach for producing new realizations from a limited set of available runs by transferring structure learned across an ensemble. Using monthly near-surface temperature time series from ten independent reanalysis realizations (ERA5), we find that a vanilla conditional variational autoencoder (CVAE) trained jointly across realizations yields a fragmented latent space that fails to generalize to unseen ensemble members. To address this, we introduce a latent-constrained CVAE (LC-CVAE) that enforces cross-realization homogeneity of latent embeddings at a small set of shared geographic 'anchor' locations. We then use multi-output Gaussian process regression in the latent space to predict latent coordinates at unsampled locations in a new realization, followed by decoding to generate full time series fields. Experiments and ablations demonstrate (i) instability when training on a single realization, (ii) diminishing returns after incorporating roughly five realizations, and (iii) a trade-off between spatial coverage and reconstruction quality that is closely linked to the average neighbor distance in latent space.</li>
<li><strong>摘要：</strong>大型气候模型集合的计算成本很高；然而，许多下游分析将受益于时空气候变量的统计上一致的额外实现。我们研究了一种生成建模方法，通过在整个集成中转移学习的结构，从有限的一组可用运行中产生新的实现。使用来自十个独立再分析实现（ERA5）的每月近地表温度时间序列，我们发现跨实现联合训练的普通条件变分自动编码器（CVAE）产生了一个碎片化的潜在空间，无法泛化到看不见的集合成员。为了解决这个问题，我们引入了一种潜在约束的 CVAE (LC-CVAE)，它强制在一小组共享地理“锚”位置处的潜在嵌入的交叉实现同质性。然后，我们在潜在空间中使用多输出高斯过程回归来预测新实现中未采样位置的潜在坐标，然后进行解码以生成完整的时间序列场。实验和消融证明了（i）在单一实现上训练时的不稳定性，（ii）合并大约五个实现后收益递减，以及（iii）空间覆盖和重建质量之间的权衡，这与潜在空间中的平均邻居距离密切相关。</li>
</ul>

<h3>Title: Reliability Under Randomness: An Empirical Analysis of Sparse and Dense Language Models Across Decoding Temperatures</h3>
<ul>
<li><strong>Authors: </strong>Kabir Grover</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00942">https://arxiv.org/abs/2601.00942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00942">https://arxiv.org/pdf/2601.00942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00942]] Reliability Under Randomness: An Empirical Analysis of Sparse and Dense Language Models Across Decoding Temperatures(https://arxiv.org/abs/2601.00942)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The increasing prevalence of sparse Mixture-of-Experts (MoE) architectures in large language models raises important questions regarding their reliability under stochastic decoding. While conditional computation enables substantial gains in computational efficiency, it remains unclear whether the interaction between sparse routing and temperature-based sampling compromises output stability relative to dense architectures. This work investigates whether conditional computation in MoE models amplifies decoding-induced randomness, leading to reduced reliability as temperature increases. We evaluate three representative models: OLMoE-7B (sparse base), Mixtral-8x7B (sparse instruction-tuned), and Qwen2.5-3B (dense instruction-tuned) on deterministic arithmetic reasoning tasks with objectively verifiable answers. Experiments span four decoding configurations, ranging from greedy decoding to T=1.0. Our evaluation encompasses accuracy, format compliance, output consistency across repeated generations, and confidence metrics, totaling 9,360 model generations. Results demonstrate that the sparse instruction-tuned model exhibits stability comparable to the dense instruction-tuned model across all decoding temperatures, while the sparse base model shows systematic degradation as temperature increases. These findings indicate that instruction tuning, rather than architectural sparsity, is the primary determinant of robustness to decoding randomness on deterministic tasks. We discuss the implications of these results for deploying sparse language models in reliability-critical applications, highlighting scenarios in which sparse architectures can be safely adopted without sacrificing output stability.</li>
<li><strong>摘要：</strong>大型语言模型中稀疏专家混合 (MoE) 架构的日益普及引发了有关其在随机解码下可靠性的重要问题。虽然条件计算可以大幅提高计算效率，但仍不清楚稀疏路由和基于温度的采样之间的相互作用是否会损害相对于密集架构的输出稳定性。这项工作研究了 MoE 模型中的条件计算是否会放大解码引起的随机性，从而导致随着温度升高而降低可靠性。我们在具有客观可验证答案的确定性算术推理任务上评估了三种代表性模型：OLMoE-7B（稀疏基础）、Mixtral-8x7B（稀疏指令调整）和 Qwen2.5-3B（密集指令调整）。实验涵盖四种解码配置，范围从贪婪解码到 T=1.0。我们的评估包括准确性、格式合规性、重复生成的输出一致性以及置信度指标，总共 9,360 个模型生成。结果表明，稀疏指令调整模型在所有解码温度下表现出与密集指令调整模型相当的稳定性，而稀疏基础模型随着温度升高而表现出系统退化。这些发现表明，指令调优，而不是架构稀疏性，是确定性任务中解码随机性的鲁棒性的主要决定因素。我们讨论了这些结果对于在可靠性关键的应用程序中部署稀疏语言模型的影响，强调了可以安全地采用稀疏架构而不牺牲输出稳定性的场景。</li>
</ul>

<h3>Title: PhyEduVideo: A Benchmark for Evaluating Text-to-Video Models for Physics Education</h3>
<ul>
<li><strong>Authors: </strong>Megha Mariam K.M, Aditya Arun, Zakaria Laskar, C.V. Jawahar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00943">https://arxiv.org/abs/2601.00943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00943">https://arxiv.org/pdf/2601.00943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00943]] PhyEduVideo: A Benchmark for Evaluating Text-to-Video Models for Physics Education(https://arxiv.org/abs/2601.00943)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative AI models, particularly Text-to-Video (T2V) systems, offer a promising avenue for transforming science education by automating the creation of engaging and intuitive visual explanations. In this work, we take a first step toward evaluating their potential in physics education by introducing a dedicated benchmark for explanatory video generation. The benchmark is designed to assess how well T2V models can convey core physics concepts through visual illustrations. Each physics concept in our benchmark is decomposed into granular teaching points, with each point accompanied by a carefully crafted prompt intended for visual explanation of the teaching point. T2V models are evaluated on their ability to generate accurate videos in response to these prompts. Our aim is to systematically explore the feasibility of using T2V models to generate high-quality, curriculum-aligned educational content-paving the way toward scalable, accessible, and personalized learning experiences powered by AI. Our evaluation reveals that current models produce visually coherent videos with smooth motion and minimal flickering, yet their conceptual accuracy is less reliable. Performance in areas such as mechanics, fluids, and optics is encouraging, but models struggle with electromagnetism and thermodynamics, where abstract interactions are harder to depict. These findings underscore the gap between visual quality and conceptual correctness in educational video generation. We hope this benchmark helps the community close that gap and move toward T2V systems that can deliver accurate, curriculum-aligned physics content at scale. The benchmark and accompanying codebase are publicly available at this https URL.</li>
<li><strong>摘要：</strong>生成式人工智能模型，特别是文本到视频（T2V）系统，通过自动创建引人入胜且直观的视觉解释，为改变科学教育提供了一条有前途的途径。在这项工作中，我们通过引入解释性视频生成的专用基准，迈出了评估其在物理教育中的潜力的第一步。该基准旨在评估 T2V 模型通过视觉插图传达核心物理概念的能力。我们基准测试中的每个物理概念都被分解为细粒度的教学点，每个点都附有精心设计的提示，旨在对教学点进行视觉解释。 T2V 模型根据其根据这些提示生成准确视频的能力进行评估。我们的目标是系统地探索使用 T2V 模型生成高质量、符合课程的教育内容的可行性，为实现由人工智能驱动的可扩展、可访问和个性化的学习体验铺平道路。我们的评估表明，当前的模型可以生成视觉上连贯的视频，具有平滑的运动和最小的闪烁，但其概念准确性不太可靠。力学、流体和光学等领域的表现令人鼓舞，但模型在电磁学和热力学方面遇到了困难，这些领域的抽象相互作用很难描述。这些发现强调了教育视频生成中视觉质量和概念正确性之间的差距。我们希望这个基准测试能够帮助社区缩小这一差距，并转向能够大规模提供准确、符合课程的物理内容的 T2V 系统。基准测试和随附的代码库可通过此 https URL 公开获取。</li>
</ul>

<h3>Title: Zero-shot Forecasting by Simulation Alone</h3>
<ul>
<li><strong>Authors: </strong>Boris N. Oreshkin, Mayank Jauhari, Ravi Kiran Selvam, Malcolm Wolff, Wenhao Pan, Shankar Ramasubramanian, Kin G. Olivares, Tatiana Konstantinova, Andres Potapczynski, Mengfei Cao, Dmitry Efimov, Michael W. Mahoney, Andrew G. Wilson</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00970">https://arxiv.org/abs/2601.00970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00970">https://arxiv.org/pdf/2601.00970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00970]] Zero-shot Forecasting by Simulation Alone(https://arxiv.org/abs/2601.00970)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Zero-shot time-series forecasting holds great promise, but is still in its infancy, hindered by limited and biased data corpora, leakage-prone evaluation, and privacy and licensing constraints. Motivated by these challenges, we propose the first practical univariate time series simulation pipeline which is simultaneously fast enough for on-the-fly data generation and enables notable zero-shot forecasting performance on M-Series and GiftEval benchmarks that capture trend/seasonality/intermittency patterns, typical of industrial forecasting applications across a variety of domains. Our simulator, which we call SarSim0 (SARIMA Simulator for Zero-Shot Forecasting), is based off of a seasonal autoregressive integrated moving average (SARIMA) model as its core data source. Due to instability in the autoregressive component, naive SARIMA simulation often leads to unusable paths. Instead, we follow a three-step procedure: (1) we sample well-behaved trajectories from its characteristic polynomial stability region; (2) we introduce a superposition scheme that combines multiple paths into rich multi-seasonality traces; and (3) we add rate-based heavy-tailed noise models to capture burstiness and intermittency alongside seasonalities and trends. SarSim0 is orders of magnitude faster than kernel-based generators, and it enables training on circa 1B unique purely simulated series, generated on the fly; after which well-established neural network backbones exhibit strong zero-shot generalization, surpassing strong statistical forecasters and recent foundation baselines, while operating under strict zero-shot protocol. Notably, on GiftEval we observe a "student-beats-teacher" effect: models trained on our simulations exceed the forecasting accuracy of the AutoARIMA generating processes.</li>
<li><strong>摘要：</strong>零样本时间序列预测前景广阔，但仍处于起步阶段，受到有限且有偏见的数据语料库、易于泄漏的评估以及隐私和许可限制的阻碍。受这些挑战的推动，我们提出了第一个实用的单变量时间序列模拟管道，该管道的速度足够快，可以实时生成数据，并在 M 系列和 GiftEval 基准上实现显着的零样本预测性能，捕获趋势/季节性/间歇性模式，这是跨多个领域的工业预测应用的典型特征。我们的模拟器称为 SarSim0（零样本预测的 SARIMA 模拟器），基于季节性自回归积分移动平均 (SARIMA) 模型作为其核心数据源。由于自回归分量的不稳定，简单的 SARIMA 模拟通常会导致不可用的路径。相反，我们遵循三个步骤：（1）我们从其特征多项式稳定区域中采样表现良好的轨迹； （2）我们引入了一种叠加方案，将多条路径组合成丰富的多季节性轨迹； (3)我们添加基于速率的重尾噪声模型来捕获突发性和间歇性以及季节性和趋势。 SarSim0 比基于内核的生成器快几个数量级，并且它可以对动态生成的约 1B 个独特的纯模拟序列进行训练；之后，完善的神经网络主干表现出强大的零样本泛化能力，超越了强大的统计预测器和最近的基础基线，同时在严格的零样本协议下运行。值得注意的是，在 GiftEval 上，我们观察到“学生击败老师”效应：在我们的模拟中训练的模型超过了 AutoARIMA 生成过程的预测准确性。</li>
</ul>

<h3>Title: UnrealPose: Leveraging Game Engine Kinematics for Large-Scale Synthetic Human Pose Data</h3>
<ul>
<li><strong>Authors: </strong>Joshua Kawaguchi, Saad Manzur, Emily Gao Wang, Maitreyi Sinha, Bryan Vela, Yunxi Wang, Brandon Vela, Wayne B. Hayes</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.00991">https://arxiv.org/abs/2601.00991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.00991">https://arxiv.org/pdf/2601.00991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.00991]] UnrealPose: Leveraging Game Engine Kinematics for Large-Scale Synthetic Human Pose Data(https://arxiv.org/abs/2601.00991)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diverse, accurately labeled 3D human pose data is expensive and studio-bound, while in-the-wild datasets lack known ground truth. We introduce UnrealPose-Gen, an Unreal Engine 5 pipeline built on Movie Render Queue for high-quality offline rendering. Our generated frames include: (i) 3D joints in world and camera coordinates, (ii) 2D projections and COCO-style keypoints with occlusion and joint-visibility flags, (iii) person bounding boxes, and (iv) camera intrinsics and extrinsics. We use UnrealPose-Gen to present UnrealPose-1M, an approximately one million frame corpus comprising eight sequences: five scripted "coherent" sequences spanning five scenes, approximately 40 actions, and five subjects; and three randomized sequences across three scenes, approximately 100 actions, and five subjects, all captured from diverse camera trajectories for broad viewpoint coverage. As a fidelity check, we report real-to-synthetic results on four tasks: image-to-3D pose, 2D keypoint detection, 2D-to-3D lifting, and person detection/segmentation. Though time and resources constrain us from an unlimited dataset, we release the UnrealPose-1M dataset, as well as the UnrealPose-Gen pipeline to support third-party generation of human pose data.</li>
<li><strong>摘要：</strong>多样化、准确标记的 3D 人体姿势数据价格昂贵且受工作室限制，而野外数据集缺乏已知的基本事实。我们介绍 UnrealPose-Gen，这是一个基于 Movie Render Queue 构建的虚幻引擎 5 管道，用于高质量离线渲染。我们生成的帧包括：(i) 世界和相机坐标中的 3D 关节，(ii) 2D 投影和带有遮挡和关节可见性标志的 COCO 式关键点，(iii) 人物边界框，以及 (iv) 相机内在和外在。我们使用 UnrealPose-Gen 来呈现 UnrealPose-1M，这是一个大约一百万帧的语料库，包含八个序列：五个脚本化的“连贯”序列，跨越五个场景、大约 40 个动作和五个主题；三个场景的三个随机序列、大约 100 个动作和五个主题，全部从不同的摄像机轨迹捕获，以实现广泛的视点覆盖。作为保真度检查，我们报告了四项任务的真实到合成结果：图像到 3D 姿势、2D 关键点检测、2D 到 3D 提升以及人物检测/分割。尽管时间和资源限制我们无法获得无限的数据集，但我们还是发布了 UnrealPose-1M 数据集以及 UnrealPose-Gen 管道来支持第三方生成人体姿势数据。</li>
</ul>

<h3>Title: Contractive Diffusion Policies: Robust Action Diffusion via Contractive Score-Based Sampling with Differential Equations</h3>
<ul>
<li><strong>Authors: </strong>Amin Abyaneh, Charlotte Morissette, Mohamad H. Danesh, Anas El Houssaini, David Meger, Gregory Dudek, Hsiu-Chin Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01003">https://arxiv.org/abs/2601.01003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01003">https://arxiv.org/pdf/2601.01003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01003]] Contractive Diffusion Policies: Robust Action Diffusion via Contractive Score-Based Sampling with Differential Equations(https://arxiv.org/abs/2601.01003)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion policies have emerged as powerful generative models for offline policy learning, whose sampling process can be rigorously characterized by a score function guiding a Stochastic Differential Equation (SDE). However, the same score-based SDE modeling that grants diffusion policies the flexibility to learn diverse behavior also incurs solver and score-matching errors, large data requirements, and inconsistencies in action generation. While less critical in image generation, these inaccuracies compound and lead to failure in continuous control settings. We introduce Contractive Diffusion Policies (CDPs) to induce contractive behavior in the diffusion sampling dynamics. Contraction pulls nearby flows closer to enhance robustness against solver and score-matching errors while reducing unwanted action variance. We develop an in-depth theoretical analysis along with a practical implementation recipe to incorporate CDPs into existing diffusion policy architectures with minimal modification and computational cost. We evaluate CDPs for offline learning by conducting extensive experiments in simulation and real-world settings. Across benchmarks, CDPs often outperform baseline policies, with pronounced benefits under data scarcity.</li>
<li><strong>摘要：</strong>扩散策略已成为离线策略学习的强大生成模型，其采样过程可以通过指导随机微分方程（SDE）的评分函数来严格表征。然而，相同的基于分数的 SDE 模型赋予扩散策略学习不同行为的灵活性，但也会导致求解器和分数匹配错误、大数据要求以及动作生成的不一致。虽然在图像生成中不太重要，但这些不准确性会加剧并导致连续控制设置失败。我们引入收缩扩散策略（CDP）来诱导扩散采样动力学中的收缩行为。收缩将附近的流拉得更近，以增强针对求解器和分数匹配错误的鲁棒性，同时减少不需要的动作方差。我们开发了深入的理论分析以及实际的实施方案，以最小的修改和计算成本将 CDP 纳入现有的扩散政策架构中。我们通过在模拟和现实环境中进行广泛的实验来评估 CDP 的离线学习能力。在各个基准中，CDP 的表现通常优于基准政策，在数据稀缺的情况下具有明显的优势。</li>
</ul>

<h3>Title: Wireless Dataset Similarity: Measuring Distances in Supervised and Unsupervised Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>João Morais, Sadjad Alikhani, Akshay Malhotra, Shahab Hamidi-Rad, Ahmed Alkhateeb</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01023">https://arxiv.org/abs/2601.01023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01023">https://arxiv.org/pdf/2601.01023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01023]] Wireless Dataset Similarity: Measuring Distances in Supervised and Unsupervised Machine Learning(https://arxiv.org/abs/2601.01023)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This paper introduces a task- and model-aware framework for measuring similarity between wireless datasets, enabling applications such as dataset selection/augmentation, simulation-to-real (sim2real) comparison, task-specific synthetic data generation, and informing decisions on model training/adaptation to new deployments. We evaluate candidate dataset distance metrics by how well they predict cross-dataset transferability: if two datasets have a small distance, a model trained on one should perform well on the other. We apply the framework on an unsupervised task, channel state information (CSI) compression, using autoencoders. Using metrics based on UMAP embeddings, combined with Wasserstein and Euclidean distances, we achieve Pearson correlations exceeding 0.85 between dataset distances and train-on-one/test-on-another task performance. We also apply the framework to a supervised beam prediction in the downlink using convolutional neural networks. For this task, we derive a label-aware distance by integrating supervised UMAP and penalties for dataset imbalance. Across both tasks, the resulting distances outperform traditional baselines and consistently exhibit stronger correlations with model transferability, supporting task-relevant comparisons between wireless datasets.</li>
<li><strong>摘要：</strong>本文介绍了一个任务和模型感知框架，用于测量无线数据集之间的相似性，支持数据集选择/增强、模拟与真实 (sim2real) 比较、特定于任务的合成数据生成等应用，并为模型训练/适应新部署的决策提供信息。我们通过预测跨数据集可迁移性的程度来评估候选数据集距离指标：如果两个数据集距离较小，则在一个数据集上训练的模型应该在另一个数据集上表现良好。我们使用自动编码器将该框架应用于无监督任务，即通道状态信息（CSI）压缩。使用基于 UMAP 嵌入的指标，结合 Wasserstein 和欧几里德距离，我们实现了数据集距离与训练/测试另一个任务性能之间超过 0.85 的 Pearson 相关性。我们还将该框架应用于使用卷积神经网络的下行链路中的监督波束预测。对于此任务，我们通过集成监督 UMAP 和数据集不平衡的惩罚来得出标签感知距离。在这两项任务中，所得距离优于传统基线，并且始终表现出与模型可转移性更强的相关性，支持无线数据集之间与任务相关的比较。</li>
</ul>

<h3>Title: Coarse-Grained Kullback--Leibler Control of Diffusion-Based Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Tatsuaki Tsuruyama</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01045">https://arxiv.org/abs/2601.01045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01045">https://arxiv.org/pdf/2601.01045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01045]] Coarse-Grained Kullback--Leibler Control of Diffusion-Based Generative AI(https://arxiv.org/abs/2601.01045)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion models and score-based generative models provide a powerful framework for synthesizing high-quality images from noise. However, there is still no satisfactory theory that describes how coarse-grained quantities, such as blockwise intensity or class proportions after partitioning an image into spatial blocks, are preserved and evolve along the reverse diffusion dynamics. In previous work, the author introduced an information-theoretic Lyapunov function V for non-ergodic Markov processes on a state space partitioned into blocks, defined as the minimal Kullback-Leibler divergence to the set of stationary distributions reachable from a given initial condition, and showed that a leak-tolerant potential V-delta with a prescribed tolerance for block masses admits a closed-form expression as a scaling-and-clipping operation on block masses. In this paper, I transplant this framework to the reverse diffusion process in generative models and propose a reverse diffusion scheme that is projected by the potential V-delta (referred to as the V-delta projected reverse diffusion). I extend the monotonicity of V to time-inhomogeneous block-preserving Markov kernels and show that, under small leakage and the V-delta projection, V-delta acts as an approximate Lyapunov function. Furthermore, using a toy model consisting of block-constant images and a simplified reverse kernel, I numerically demonstrate that the proposed method keeps the block-mass error and the leak-tolerant potential within the prescribed tolerance, while achieving pixel-wise accuracy and visual quality comparable to the non-projected dynamics. This study reinterprets generative sampling as a decrease of an information potential from noise to data, and provides a design principle for reverse diffusion processes with explicit control of coarse-grained quantities.</li>
<li><strong>摘要：</strong>扩散模型和基于分数的生成模型为从噪声中合成高质量图像提供了强大的框架。然而，仍然没有令人满意的理论来描述粗粒度数量（例如将图像划分为空间块后的块强度或类比例）如何保留并沿着反向扩散动力学演化。在之前的工作中，作者为划分为块的状态空间上的非遍历马尔可夫过程引入了信息论 Lyapunov 函数 V，定义为从给定初始条件可达的平稳分布集的最小 Kullback-Leibler 散度，并表明具有规定的块质量容差的泄漏容错潜在 V-delta 允许封闭形式表达式作为块质量的缩放和裁剪操作。在本文中，我将该框架移植到生成模型中的反向扩散过程中，并提出了一种由潜在V-delta投影的反向扩散方案（简称V-delta投影反向扩散）。我将 V 的单调性扩展到时间不均匀的块保持马尔可夫核，并表明，在小泄漏和 V-delta 投影下，V-delta 充当近似 Lyapunov 函数。此外，使用由块恒定图像和简化的反向内核组成的玩具模型，我以数值方式证明了所提出的方法将块质量误差和泄漏容忍潜力保持在规定的容差范围内，同时实现了与非投影动态相当的像素精度和视觉质量。这项研究将生成采样重新解释为从噪声到数据的信息潜力的减少，并为具有粗粒度数量的显式控制的反向扩散过程提供了设计原理。</li>
</ul>

<h3>Title: SPoRC-VIST: A Benchmark for Evaluating Generative Natural Narrative in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yunlin Zeng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01062">https://arxiv.org/abs/2601.01062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01062">https://arxiv.org/pdf/2601.01062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01062]] SPoRC-VIST: A Benchmark for Evaluating Generative Natural Narrative in Vision-Language Models(https://arxiv.org/abs/2601.01062)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) have achieved remarkable success in descriptive tasks such as image captioning and visual question answering (VQA). However, their ability to generate engaging, long-form narratives -- specifically multi-speaker podcast dialogues -- remains under-explored and difficult to evaluate. Standard metrics like BLEU and ROUGE fail to capture the nuances of conversational naturalness, personality, and narrative flow, often rewarding safe, repetitive outputs over engaging storytelling. In this work, we present a novel pipeline for end-to-end visual podcast generation, and fine-tune a Qwen3-VL-32B model on a curated dataset of 4,000 image-dialogue pairs. Crucially, we use a synthetic-to-real training strategy: we train on high-quality podcast dialogues from the Structured Podcast Research Corpus (SPoRC) paired with synthetically generated imagery, and evaluate on real-world photo sequences from the Visual Storytelling Dataset (VIST). This rigorous setup tests the model's ability to generalize from synthetic training data to real-world visual domains. We propose a comprehensive evaluation framework that moves beyond textual overlap, and use AI-as-a-judge (Gemini 3 Pro, Claude Opus 4.5, GPT 5.2) and novel style metrics (average turn length, speaker switch rate) to assess quality. Our experiments demonstrate that our fine-tuned 32B model significantly outperforms a 235B base model in conversational naturalness ($>$80\% win rate) and narrative depth (+50\% turn length), while maintaining identical visual grounding capabilities (CLIPScore: 20.39).</li>
<li><strong>摘要：</strong>视觉语言模型 (VLM) 在图像字幕和视觉问答 (VQA) 等描述性任务中取得了显着的成功。然而，它们生成引人入胜的长篇叙事的能力——特别是多发言者播客对话——仍然未被充分探索且难以评估。 BLEU 和 ROUGE 等标准指标无法捕捉对话自然性、个性和叙事流程的细微差别，通常会奖励安全、重复的输出，而不是引人入胜的故事讲述。在这项工作中，我们提出了一种用于端到端视觉播客生成的新颖管道，并在 4,000 个图像对话对的精选数据集上微调 Qwen3-VL-32B 模型。至关重要的是，我们使用合成到真实的训练策略：我们对来自结构化播客研究语料库（SPoRC）的高质量播客对话与合成生成的图像进行训练，并评估来自视觉讲故事数据集（VIST）的真实世界照片序列。这种严格的设置测试了模型从合成训练数据泛化到现实世界视觉领域的能力。我们提出了一个超越文本重叠的综合评估框架，并使用人工智能作为法官（Gemini 3 Pro、Claude Opus 4.5、GPT 5.2）和新颖的风格指标（平均转弯长度、发言者切换率）来评估质量。我们的实验表明，经过微调的 32B 模型在对话自然度（​​$>$80\% 获胜率）和叙事深度（+50\% 回合长度）方面显着优于 235B 基本模型，同时保持相同的视觉基础能力（CLIPScore：20.39）。</li>
</ul>

<h3>Title: Luminark: Training-free, Probabilistically-Certified Watermarking for General Vision Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Xu, Zhang Zhang, Yuanrui Zhang, Ruitao Chen, Yixian Xu, Tianyu He, Di He</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01085">https://arxiv.org/abs/2601.01085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01085">https://arxiv.org/pdf/2601.01085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01085]] Luminark: Training-free, Probabilistically-Certified Watermarking for General Vision Generative Models(https://arxiv.org/abs/2601.01085)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce \emph{Luminark}, a training-free and probabilistically-certified watermarking method for general vision generative models. Our approach is built upon a novel watermark definition that leverages patch-level luminance statistics. Specifically, the service provider predefines a binary pattern together with corresponding patch-level thresholds. To detect a watermark in a given image, we evaluate whether the luminance of each patch surpasses its threshold and then verify whether the resulting binary pattern aligns with the target one. A simple statistical analysis demonstrates that the false positive rate of the proposed method can be effectively controlled, thereby ensuring certified detection. To enable seamless watermark injection across different paradigms, we leverage the widely adopted guidance technique as a plug-and-play mechanism and develop the \emph{watermark guidance}. This design enables Luminark to achieve generality across state-of-the-art generative models without compromising image quality. Empirically, we evaluate our approach on nine models spanning diffusion, autoregressive, and hybrid frameworks. Across all evaluations, Luminark consistently demonstrates high detection accuracy, strong robustness against common image transformations, and good performance on visual quality.</li>
<li><strong>摘要：</strong>在本文中，我们介绍了 \emph{Luminark}，一种用于通用视觉生成模型的免训练且经过概率认证的水印方法。我们的方法建立在一种新颖的水印定义之上，该定义利用了块级亮度统计数据。具体来说，服务提供商预定义二进制模式以及相应的补丁级别阈值。为了检测给定图像中的水印，我们评估每个补丁的亮度是否超过其阈值，然后验证生成的二进制模式是否与目标模式对齐。简单的统计分析表明，该方法的误报率可以得到有效控制，从而保证检测的合格性。为了实现跨不同范式的无缝水印注入，我们利用广泛采用的引导技术作为即插即用机制，并开发了\emph{水印引导}。这种设计使 Luminark 能够在不影响图像质量的情况下实现最先进的生成模型的通用性。根据经验，我们在扩散、自回归和混合框架的九个模型上评估了我们的方法。在所有评估中，Luminark 始终表现出较高的检测精度、针对常见图像转换的强大鲁棒性以及良好的视觉质量性能。</li>
</ul>

<h3>Title: 600k-ks-ocr: a large-scale synthetic dataset for optical character recognition in kashmiri script</h3>
<ul>
<li><strong>Authors: </strong>Haq Nawaz Malik</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01088">https://arxiv.org/abs/2601.01088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01088">https://arxiv.org/pdf/2601.01088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01088]] 600k-ks-ocr: a large-scale synthetic dataset for optical character recognition in kashmiri script(https://arxiv.org/abs/2601.01088)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This technical report presents the 600K-KS-OCR Dataset, a large-scale synthetic corpus comprising approximately 602,000 word-level segmented images designed for training and evaluating optical character recognition systems targeting Kashmiri script. The dataset addresses a critical resource gap for Kashmiri, an endangered Dardic language utilizing a modified Perso-Arabic writing system spoken by approximately seven million people. Each image is rendered at 256x64 pixels with corresponding ground-truth transcriptions provided in multiple formats compatible with CRNN, TrOCR, and generalpurpose machine learning pipelines. The generation methodology incorporates three traditional Kashmiri typefaces, comprehensive data augmentation simulating real-world document degradation, and diverse background textures to enhance model robustness. The dataset is distributed across ten partitioned archives totaling approximately 10.6 GB and is released under the CC-BY-4.0 license to facilitate research in low-resource language optical character recognition.</li>
<li><strong>摘要：</strong>该技术报告介绍了 600K-KS-OCR 数据集，这是一个大型合成语料库，包含约 602,000 个单词级分割图像，旨在训练和评估针对克什米尔文字的光学字符识别系统。该数据集解决了克什米尔语的关键资源缺口，克什米尔语是一种濒临灭绝的达尔克语言，采用修改后的波斯阿拉伯语书写系统，约有 700 万人使用。每张图像均以 256x64 像素渲染，并以与 CRNN、TrOCR 和通用机器学习管道兼容的多种格式提供相应的真实转录。生成方法结合了三种传统的克什米尔字体、模拟真实世界文档退化的综合数据增强以及多种背景纹理以增强模型的鲁棒性。该数据集分布在 10 个分区档案中，总计约 10.6 GB，并根据 CC-BY-4.0 许可证发布，以促进低资源语言光学字符识别的研究。</li>
</ul>

<h3>Title: Histogram Assisted Quality Aware Generative Model for Resolution Invariant NIR Image Colorization</h3>
<ul>
<li><strong>Authors: </strong>Abhinav Attri, Rajeev Ranjan Dwivedi, Samiran Das, Vinod Kumar Kurmi</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01103">https://arxiv.org/abs/2601.01103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01103">https://arxiv.org/pdf/2601.01103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01103]] Histogram Assisted Quality Aware Generative Model for Resolution Invariant NIR Image Colorization(https://arxiv.org/abs/2601.01103)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present HAQAGen, a unified generative model for resolution-invariant NIR-to-RGB colorization that balances chromatic realism with structural fidelity. The proposed model introduces (i) a combined loss term aligning the global color statistics through differentiable histogram matching, perceptual image quality measure, and feature based similarity to preserve texture information, (ii) local hue-saturation priors injected via Spatially Adaptive Denormalization (SPADE) to stabilize chromatic reconstruction, and (iii) texture-aware supervision within a Mamba backbone to preserve fine details. We introduce an adaptive-resolution inference engine that further enables high-resolution translation without sacrificing quality. Our proposed NIR-to-RGB translation model simultaneously enforces global color statistics and local chromatic consistency, while scaling to native resolutions without compromising texture fidelity or generalization. Extensive evaluations on FANVID, OMSIV, VCIP2020, and RGB2NIR using different evaluation metrics demonstrate consistent improvements over state-of-the-art baseline methods. HAQAGen produces images with sharper textures, natural colors, attaining significant gains as per perceptual metrics. These results position HAQAGen as a scalable and effective solution for NIR-to-RGB translation across diverse imaging scenarios. Project Page: this https URL</li>
<li><strong>摘要：</strong>我们提出了 HAQAGen，这是一种用于分辨率不变的 NIR-to-RGB 着色的统一生成模型，可平衡色彩真实性与结构保真度。所提出的模型引入了（i）通过可微直方图匹配、感知图像质量测量和基于特征的相似性来对齐全局颜色统计的组合损失项以保留纹理信息，（ii）通过空间自适应去规范化（SPADE）注入局部色调饱和度先验以稳定色彩重建，以及（iii）Mamba骨干内的纹理感知监督以保留精细细节。我们引入了自适应分辨率推理引擎，可以在不牺牲质量的情况下进一步实现高分辨率翻译。我们提出的 NIR-to-RGB 转换模型同时强制执行全局颜色统计和局部色彩一致性，同时缩放到原始分辨率，而不会影响纹理保真度或泛化。使用不同的评估指标对 FANVID、OMSIV、VCIP2020 和 RGB2NIR 进行的广泛评估表明，与最先进的基线方法相比具有一致的改进。 HAQAGen 生成的图像具有更清晰的纹理、自然的色彩，根据感知指标获得了显着的收益。这些结果使 HAQAGen 成为跨不同成像场景的 NIR 到 RGB 转换的可扩展且有效的解决方案。项目页面：此 https URL</li>
</ul>

<h3>Title: Evo-TFS: Evolutionary Time-Frequency Domain-Based Synthetic Minority Oversampling Approach to Imbalanced Time Series Classification</h3>
<ul>
<li><strong>Authors: </strong>Wenbin Pei, Ruohao Dai, Bing Xue, Mengjie Zhang, Qiang Zhang, Yiu-Ming Cheung</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01150">https://arxiv.org/abs/2601.01150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01150">https://arxiv.org/pdf/2601.01150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01150]] Evo-TFS: Evolutionary Time-Frequency Domain-Based Synthetic Minority Oversampling Approach to Imbalanced Time Series Classification(https://arxiv.org/abs/2601.01150)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Time series classification is a fundamental machine learning task with broad real-world applications. Although many deep learning methods have proven effective in learning time-series data for classification, they were originally developed under the assumption of balanced data distributions. Once data distribution is uneven, these methods tend to ignore the minority class that is typically of higher practical significance. Oversampling methods have been designed to address this by generating minority-class samples, but their reliance on linear interpolation often hampers the preservation of temporal dynamics and the generation of diverse samples. Therefore, in this paper, we propose Evo-TFS, a novel evolutionary oversampling method that integrates both time- and frequency-domain characteristics. In Evo-TFS, strongly typed genetic programming is employed to evolve diverse, high-quality time series, guided by a fitness function that incorporates both time-domain and frequency-domain characteristics. Experiments conducted on imbalanced time series datasets demonstrate that Evo-TFS outperforms existing oversampling methods, significantly enhancing the performance of time-domain and frequency-domain classifiers.</li>
<li><strong>摘要：</strong>时间序列分类是一项基本的机器学习任务，具有广泛的实际应用。尽管许多深度学习方法已被证明在学习时间序列数据以进行分类方面是有效的，但它们最初是在平衡数据分布的假设下开发的。一旦数据分布不均匀，这些方法往往会忽略通常具有较高实际意义的少数类。过采样方法旨在通过生成少数类样本来解决这个问题，但它们对线性插值的依赖通常会妨碍时间动态的保存和不同样本的生成。因此，在本文中，我们提出了Evo-TFS，一种融合时域和频域特征的新型进化过采样方法。在 Evo-TFS 中，在结合时域和频域特征的适应度函数的指导下，采用强类型遗传编程来演化多样化的高质量时间序列。在不平衡时间序列数据集上进行的实验表明，Evo-TFS 优于现有的过采样方法，显着增强了时域和频域分类器的性能。</li>
</ul>

<h3>Title: GenCAMO: Scene-Graph Contextual Decoupling for Environment-aware and Mask-free Camouflage Image-Dense Annotation Generation</h3>
<ul>
<li><strong>Authors: </strong>Chenglizhao Chen, Shaojiang Yuan, Xiaoxue Lu, Mengke Song, Jia Song, Zhenyu Wu, Wenfeng Song, Shuai Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01181">https://arxiv.org/abs/2601.01181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01181">https://arxiv.org/pdf/2601.01181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01181]] GenCAMO: Scene-Graph Contextual Decoupling for Environment-aware and Mask-free Camouflage Image-Dense Annotation Generation(https://arxiv.org/abs/2601.01181)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Conceal dense prediction (CDP), especially RGB-D camouflage object detection and open-vocabulary camouflage object segmentation, plays a crucial role in advancing the understanding and reasoning of complex camouflage scenes. However, high-quality and large-scale camouflage datasets with dense annotation remain scarce due to expensive data collection and labeling costs. To address this challenge, we explore leveraging generative models to synthesize realistic camouflage image-dense data for training CDP models with fine-grained representations, prior knowledge, and auxiliary reasoning. Concretely, our contributions are threefold: (i) we introduce GenCAMO-DB, a large-scale camouflage dataset with multi-modal annotations, including depth maps, scene graphs, attribute descriptions, and text prompts; (ii) we present GenCAMO, an environment-aware and mask-free generative framework that produces high-fidelity camouflage image-dense annotations; (iii) extensive experiments across multiple modalities demonstrate that GenCAMO significantly improves dense prediction performance on complex camouflage scenes by providing high-quality synthetic data. The code and datasets will be released after paper acceptance.</li>
<li><strong>摘要：</strong>隐藏密集预测（CDP），特别是RGB-D迷彩目标检测和开放词汇迷彩目标分割，在推进复杂迷彩场景的理解和推理方面发挥着至关重要的作用。然而，由于昂贵的数据收集和标记成本，具有密集注释的高质量、大规模迷彩数据集仍然稀缺。为了应对这一挑战，我们探索利用生成模型来合成真实的迷彩图像密集数据，以训练具有细粒度表示、先验知识和辅助推理的 CDP 模型。具体来说，我们的贡献有三个方面：(i)我们引入了GenCAMO-DB，一个具有多模态注释的大规模迷彩数据集，包括深度图、场景图、属性描述和文本提示； (ii) 我们提出了 GenCAMO，一种环境感知且无掩模的生成框架，可生成高保真迷彩图像密集注释； (iii) 跨多种模式的广泛实验表明，GenCAMO 通过提供高质量的合成数据，显着提高了复杂迷彩场景的密集预测性能。代码和数据集将在论文接受后发布。</li>
</ul>

<h3>Title: MS-ISSM: Objective Quality Assessment of Point Clouds Using Multi-scale Implicit Structural Similarity</h3>
<ul>
<li><strong>Authors: </strong>Zhang Chen, Shuai Wan, Yuezhe Zhang, Siyu Ren, Fuzheng Yang, Junhui Hou</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01200">https://arxiv.org/abs/2601.01200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01200">https://arxiv.org/pdf/2601.01200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01200]] MS-ISSM: Objective Quality Assessment of Point Clouds Using Multi-scale Implicit Structural Similarity(https://arxiv.org/abs/2601.01200)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>The unstructured and irregular nature of point clouds poses a significant challenge for objective quality assessment (PCQA), particularly in establishing accurate perceptual feature correspondence. To tackle this, we propose the Multi-scale Implicit Structural Similarity Measurement (MS-ISSM). Unlike traditional point-to-point matching, MS-ISSM utilizes Radial Basis Functions (RBF) to represent local features continuously, transforming distortion measurement into a comparison of implicit function coefficients. This approach effectively circumvents matching errors inherent in irregular data. Additionally, we propose a ResGrouped-MLP quality assessment network, which robustly maps multi-scale feature differences to perceptual scores. The network architecture departs from traditional flat MLPs by adopting a grouped encoding strategy integrated with Residual Blocks and Channel-wise Attention mechanisms. This hierarchical design allows the model to preserve the distinct physical semantics of luma, chroma, and geometry while adaptively focusing on the most salient distortion features across High, Medium, and Low scales. Experimental results on multiple benchmarks demonstrate that MS-ISSM outperforms state-of-the-art metrics in both reliability and generalization. The source code is available at: this https URL.</li>
<li><strong>摘要：</strong>点云的非结构化和不规则性质对客观质量评估（PCQA）提出了重大挑战，特别是在建立准确的感知特征对应方面。为了解决这个问题，我们提出了多尺度隐式结构相似性测量（MS-ISSM）。与传统的点对点匹配不同，MS-ISSM利用径向基函数（RBF）连续表示局部特征，将失真测量转化为隐函数系数的比较。这种方法有效地避免了不规则数据固有的匹配错误。此外，我们提出了一个 ResGrouped-MLP 质量评估网络，它将多尺度特征差异稳健地映射到感知分数。该网络架构与传统的平面 MLP 不同，采用了与残差块和通道注意机制集成的分组编码策略。这种分层设计允许模型保留亮度、色度和几何形状的独特物理语义，同时自适应地关注高、中和低尺度上最显着的失真特征。多个基准的实验结果表明，MS-ISSM 在可靠性和泛化性方面均优于最先进的指标。源代码位于：此 https URL。</li>
</ul>

<h3>Title: RefSR-Adv: Adversarial Attack on Reference-based Image Super-Resolution Models</h3>
<ul>
<li><strong>Authors: </strong>Jiazhu Dai, Huihui Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01202">https://arxiv.org/abs/2601.01202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01202">https://arxiv.org/pdf/2601.01202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01202]] RefSR-Adv: Adversarial Attack on Reference-based Image Super-Resolution Models(https://arxiv.org/abs/2601.01202)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Single Image Super-Resolution (SISR) aims to recover high-resolution images from low-resolution inputs. Unlike SISR, Reference-based Super-Resolution (RefSR) leverages an additional high-resolution reference image to facilitate the recovery of high-frequency textures. However, existing research mainly focuses on backdoor attacks targeting RefSR, while the vulnerability of the adversarial attacks targeting RefSR has not been fully explored. To fill this research gap, we propose RefSR-Adv, an adversarial attack that degrades SR outputs by perturbing only the reference image. By maximizing the difference between adversarial and clean outputs, RefSR-Adv induces significant performance degradation and generates severe artifacts across CNN, Transformer, and Mamba architectures on the CUFED5, WR-SR, and DRefSR datasets. Importantly, experiments confirm a positive correlation between the similarity of the low-resolution input and the reference image and attack effectiveness, revealing that the model's over-reliance on reference features is a key security flaw. This study reveals a security vulnerability in RefSR systems, aiming to urge researchers to pay attention to the robustness of RefSR.</li>
<li><strong>摘要：</strong>单图像超分辨率（SISR）旨在从低分辨率输入中恢复高分辨率图像。与 SISR 不同，基于参考的超分辨率 (RefSR) 利用额外的高分辨率参考图像来促进高频纹理的恢复。然而，现有的研究主要集中在针对 RefSR 的后门攻击，而针对 RefSR 的对抗性攻击的脆弱性尚未得到充分探讨。为了填补这一研究空白，我们提出了 RefSR-Adv，这是一种对抗性攻击，通过仅扰乱参考图像来降低 SR 输出。通过最大化对抗性输出和干净输出之间的差异，RefSR-Adv 会导致性能显着下降，并在 CUFED5、WR-SR 和 DRefSR 数据集上的 CNN、Transformer 和 Mamba 架构中生成严重的伪影。重要的是，实验证实了低分辨率输入与参考图像的相似度与攻击有效性之间存在正相关关系，揭示了模型对参考特征的过度依赖是一个关键的安全缺陷。本研究揭示了 RefSR 系统中的一个安全漏洞，旨在敦促研究人员关注 RefSR 的鲁棒性。</li>
</ul>

<h3>Title: Improved Object-Centric Diffusion Learning with Registers and Contrastive Alignment</h3>
<ul>
<li><strong>Authors: </strong>Bac Nguyen, Yuhta Takida, Naoki Murata, Chieh-Hsin Lai, Toshimitsu Uesaka, Stefano Ermon, Yuki Mitsufuji</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01224">https://arxiv.org/abs/2601.01224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01224">https://arxiv.org/pdf/2601.01224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01224]] Improved Object-Centric Diffusion Learning with Registers and Contrastive Alignment(https://arxiv.org/abs/2601.01224)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Slot Attention (SA) with pretrained diffusion models has recently shown promise for object-centric learning (OCL), but suffers from slot entanglement and weak alignment between object slots and image content. We propose Contrastive Object-centric Diffusion Alignment (CODA), a simple extension that (i) employs register slots to absorb residual attention and reduce interference between object slots, and (ii) applies a contrastive alignment loss to explicitly encourage slot-image correspondence. The resulting training objective serves as a tractable surrogate for maximizing mutual information (MI) between slots and inputs, strengthening slot representation quality. On both synthetic (MOVi-C/E) and real-world datasets (VOC, COCO), CODA improves object discovery (e.g., +6.1% FG-ARI on COCO), property prediction, and compositional image generation over strong baselines. Register slots add negligible overhead, keeping CODA efficient and scalable. These results indicate potential applications of CODA as an effective framework for robust OCL in complex, real-world scenes.</li>
<li><strong>摘要：</strong>带有预训练扩散模型的槽位注意力（SA）最近在以对象为中心的学习（OCL）中展现出了前景，但受到槽位纠缠以及对象槽位和图像内容之间对齐较弱的困扰。我们提出了以对象为中心的对比扩散对齐（CODA），这是一个简单的扩展，（i）使用寄存器槽来吸收残余注意力并减少对象槽之间的干扰，以及（ii）应用对比对齐损失来明确鼓励槽图像对应。由此产生的训练目标可作为一个易于处理的替代品，用于最大化时隙和输入之间的互信息 (MI)，从而增强时隙表示质量。在合成 (MOVi-C/E) 和真实世界数据集（VOC、COCO）上，CODA 改进了对象发现（例如，COCO 上 +6.1% FG-ARI）、属性预测和组合图像生成（在强基线上）。寄存器槽增加的开销可以忽略不计，从而保持 CODA 的高效性和可扩展性。这些结果表明 CODA 作为复杂现实场景中稳健 OCL 的有效框架的潜在应用。</li>
</ul>

<h3>Title: Warp-Cortex: An Asynchronous, Memory-Efficient Architecture for Million-Agent Cognitive Scaling on Consumer Hardware</h3>
<ul>
<li><strong>Authors: </strong>Jorge L. Ruiz Williams</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.AR, cs.DC, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01298">https://arxiv.org/abs/2601.01298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01298">https://arxiv.org/pdf/2601.01298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01298]] Warp-Cortex: An Asynchronous, Memory-Efficient Architecture for Million-Agent Cognitive Scaling on Consumer Hardware(https://arxiv.org/abs/2601.01298)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Current multi-agent Large Language Model (LLM) frameworks suffer from linear memory scaling, rendering "System 2" parallel reasoning impractical on consumer hardware. We present Warp Cortex, an asynchronous architecture that theoretically enables million-agent cognitive scaling by decoupling agent logic from physical memory. Through Singleton Weight Sharing and a novel Topological Synapse--inspired by hybrid landmarking techniques from Topological Data Analysis (TDA)--we reduce memory complexity from O(N * L) to O(1) for weights and O(N * k) for context, where k << L. By treating the KV-cache as a point cloud in latent space, we apply witness-complex-inspired sparsification to preserve persistent homological features of the context manifold. On a single NVIDIA RTX 4090, we empirically demonstrate 100 concurrent agents at 2.2 GB total VRAM, with theoretical capacity exceeding 1,000 agents before compute latency becomes the bottleneck. We further introduce Referential Injection, a non-intrusive KV-cache update mechanism that allows asynchronous sub-agents to influence primary generation without stream disruption.</li>
<li><strong>摘要：</strong>当前的多智能体大型语言模型 (LLM) 框架受到线性内存扩展的影响，导致“系统 2”并行推理在消费类硬件上不切实际。我们提出了 Warp Cortex，这是一种异步架构，理论上可以通过将代理逻辑与物理内存解耦来实现百万代理认知扩展。通过单例权重共享和新颖的拓扑突触（受到拓扑数据分析 (TDA) 的混合标志技术的启发），我们将权重的内存复杂度从 O(N * L) 降低到 O(1)，上下文的 O(N * k)，其中 k << L。通过将 KV 缓存视为潜在空间中的点云，我们应用见证复合体启发的稀疏化来保留上下文流形的持久同源特征。在单个 NVIDIA RTX 4090 上，我们凭经验演示了 2.2 GB 总 VRAM 下的 100 个并发代理，在计算延迟成为瓶颈之前理论容量超过 1,000 个代理。我们进一步介绍了引用注入，这是一种非侵入式 KV 缓存更新机制，允许异步子代理在不中断流的情况下影响主生成。</li>
</ul>

<h3>Title: Achieving Fine-grained Cross-modal Understanding through Brain-inspired Hierarchical Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Weihang You, Hanqi Jiang, Yi Pan, Junhao Chen, Tianming Liu, Fei Dou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01339">https://arxiv.org/abs/2601.01339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01339">https://arxiv.org/pdf/2601.01339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01339]] Achieving Fine-grained Cross-modal Understanding through Brain-inspired Hierarchical Representation Learning(https://arxiv.org/abs/2601.01339)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Understanding neural responses to visual stimuli remains challenging due to the inherent complexity of brain representations and the modality gap between neural data and visual inputs. Existing methods, mainly based on reducing neural decoding to generation tasks or simple correlations, fail to reflect the hierarchical and temporal processes of visual processing in the brain. To address these limitations, we present NeuroAlign, a novel framework for fine-grained fMRI-video alignment inspired by the hierarchical organization of the human visual system. Our framework implements a two-stage mechanism that mirrors biological visual pathways: global semantic understanding through Neural-Temporal Contrastive Learning (NTCL) and fine-grained pattern matching through enhanced vector quantization. NTCL explicitly models temporal dynamics through bidirectional prediction between modalities, while our DynaSyncMM-EMA approach enables dynamic multi-modal fusion with adaptive weighting. Experiments demonstrate that NeuroAlign significantly outperforms existing methods in cross-modal retrieval tasks, establishing a new paradigm for understanding visual cognitive mechanisms.</li>
<li><strong>摘要：</strong>由于大脑表征固有的复杂性以及神经数据和视觉输入之间的模态差距，理解神经对视觉刺激的反应仍然具有挑战性。现有的方法主要基于将神经解码简化为生成任务或简单的相关性，无法反映大脑中视觉处理的层次和时间过程。为了解决这些限制，我们提出了 NeuroAlign，这是一种受人类视觉系统的分层组织启发的细粒度 fMRI-视频对齐的新颖框架。我们的框架实现了反映生物视觉路径的两阶段机制：通过神经时间对比学习（NTCL）进行全局语义理解，以及通过增强矢量量化进行细粒度模式匹配。 NTCL 通过模态之间的双向预测显式模拟时间动态，而我们的 DynaSyncMM-EMA 方法可以通过自适应权重实现动态多模态融合。实验表明，NeuroAlign 在跨模态检索任务中显着优于现有方法，为理解视觉认知机制建立了新的范式。</li>
</ul>

<h3>Title: From Classification to Generation: An Open-Ended Paradigm for Adverse Drug Reaction Prediction Based on Graph-Motif Feature Fusion</h3>
<ul>
<li><strong>Authors: </strong>Yuyan Pi, Min Jin, Wentao Xie, Xinhua Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01347">https://arxiv.org/abs/2601.01347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01347">https://arxiv.org/pdf/2601.01347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01347]] From Classification to Generation: An Open-Ended Paradigm for Adverse Drug Reaction Prediction Based on Graph-Motif Feature Fusion(https://arxiv.org/abs/2601.01347)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Computational biology offers immense potential for reducing the high costs and protracted cycles of new drug development through adverse drug reaction (ADR) prediction. However, current methods remain impeded by drug data scarcity-induced cold-start challenge, closed label sets, and inadequate modeling of label dependencies. Here we propose an open-ended ADR prediction paradigm based on Graph-Motif feature fusion and Multi-Label Generation (GM-MLG). Leveraging molecular structure as an intrinsic and inherent feature, GM-MLG constructs a dual-graph representation architecture spanning the atomic level, the local molecular level (utilizing fine-grained motifs dynamically extracted via the BRICS algorithm combined with additional fragmentation rules), and the global molecular level. Uniquely, GM-MLG pioneers transforming ADR prediction from multi-label classification into Transformer Decoder-based multi-label generation. By treating ADR labels as discrete token sequences, it employs positional embeddings to explicitly capture dependencies and co-occurrence relationships within large-scale label spaces, generating predictions via autoregressive decoding to dynamically expand the prediction space. Experiments demonstrate GM-MLG achieves up to 38% improvement and an average gain of 20%, expanding the prediction space from 200 to over 10,000 types. Furthermore, it elucidates non-linear structure-activity relationships between ADRs and motifs via retrosynthetic motif analysis, providing interpretable and innovative support for systematic risk reduction in drug safety.</li>
<li><strong>摘要：</strong>计算生物学通过药物不良反应（ADR）预测为降低新药开发的高成本和延长周期提供了巨大的潜力。然而，当前的方法仍然受到药物数据稀缺引起的冷启动挑战、封闭的标签集以及标签依赖性建模不足的阻碍。在这里，我们提出了一种基于图基序特征融合和多标签生成（GM-MLG）的开放式 ADR 预测范例。利用分子结构作为内在和固有的特征，GM-MLG构建了一个跨越原子水平、局部分子水平（利用通过金砖四国算法动态提取的细粒度图案并结合额外的碎片规则）和全局分子水平的双图表示架构。独特的是，GM-MLG 率先将 ADR 预测从多标签分类转变为基于 Transformer Decoder 的多标签生成。通过将 ADR 标签视为离散标记序列，它采用位置嵌入来显式捕获大规模标签空间内的依赖性和共现关系，通过自回归解码生成预测以动态扩展预测空间。实验表明，GM-MLG 实现了高达 38% 的改进和 20% 的平均增益，将预测空间从 200 种扩展至 10,000 多种类型。此外，它通过逆合成基序分析阐明了 ADR 和基序之间的非线性构效关系，为系统性降低药物安全风险提供可解释和创新的支持。</li>
</ul>

<h3>Title: Slot-ID: Identity-Preserving Video Generation from Reference Videos via Slot-Based Temporal Identity Encoding</h3>
<ul>
<li><strong>Authors: </strong>Yixuan Lai, He Wang, Kun Zhou, Tianjia Shao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01352">https://arxiv.org/abs/2601.01352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01352">https://arxiv.org/pdf/2601.01352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01352]] Slot-ID: Identity-Preserving Video Generation from Reference Videos via Slot-Based Temporal Identity Encoding(https://arxiv.org/abs/2601.01352)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Producing prompt-faithful videos that preserve a user-specified identity remains challenging: models need to extrapolate facial dynamics from sparse reference while balancing the tension between identity preservation and motion naturalness. Conditioning on a single image completely ignores the temporal signature, which leads to pose-locked motions, unnatural warping, and "average" faces when viewpoints and expressions change. To this end, we introduce an identity-conditioned variant of a diffusion-transformer video generator which uses a short reference video rather than a single portrait. Our key idea is to incorporate the dynamics in the reference. A short clip reveals subject-specific patterns, e.g., how smiles form, across poses and lighting. From this clip, a Sinkhorn-routed encoder learns compact identity tokens that capture characteristic dynamics while remaining pretrained backbone-compatible. Despite adding only lightweight conditioning, the approach consistently improves identity retention under large pose changes and expressive facial behavior, while maintaining prompt faithfulness and visual realism across diverse subjects and prompts.</li>
<li><strong>摘要：</strong>制作保留用户指定身份的即时忠实视频仍然具有挑战性：模型需要从稀疏参考中推断面部动态，同时平衡身份保留和运动自然度之间的紧张关系。对单个图像进行调节完全忽略了时间特征，这会导致姿势锁定运动、不自然的扭曲以及当视点和表情发生变化时出现“平均”面孔。为此，我们引入了扩散变换器视频生成器的身份条件变体，它使用短参考视频而不是单个肖像。我们的关键想法是将动态纳入参考文献中。一个短片揭示了特定主题的模式，例如微笑如何形成、姿势和灯光。从这个片段中，Sinkhorn 路由编码器学习紧凑的身份令牌，捕获特征动态，同时保持预训练的骨干网兼容。尽管仅添加了轻量级调节，但该方法始终可以提高大姿势变化和富有表现力的面部行为下的身份保留，同时在不同的主题和提示中保持即时的忠实性和视觉真实感。</li>
</ul>

<h3>Title: SwinIFS: Landmark Guided Swin Transformer For Identity Preserving Face Super Resolution</h3>
<ul>
<li><strong>Authors: </strong>Habiba Kausar, Saeed Anwar, Omar Jamal Hammad, Abdul Bais</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01406">https://arxiv.org/abs/2601.01406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01406">https://arxiv.org/pdf/2601.01406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01406]] SwinIFS: Landmark Guided Swin Transformer For Identity Preserving Face Super Resolution(https://arxiv.org/abs/2601.01406)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, super-resolution</a></li>
<li><strong>Abstract: </strong>Face super-resolution aims to recover high-quality facial images from severely degraded low-resolution inputs, but remains challenging due to the loss of fine structural details and identity-specific features. This work introduces SwinIFS, a landmark-guided super-resolution framework that integrates structural priors with hierarchical attention mechanisms to achieve identity-preserving reconstruction at both moderate and extreme upscaling factors. The method incorporates dense Gaussian heatmaps of key facial landmarks into the input representation, enabling the network to focus on semantically important facial regions from the earliest stages of processing. A compact Swin Transformer backbone is employed to capture long-range contextual information while preserving local geometry, allowing the model to restore subtle facial textures and maintain global structural consistency. Extensive experiments on the CelebA benchmark demonstrate that SwinIFS achieves superior perceptual quality, sharper reconstructions, and improved identity retention; it consistently produces more photorealistic results and exhibits strong performance even under 8x magnification, where most methods fail to recover meaningful structure. SwinIFS also provides an advantageous balance between reconstruction accuracy and computational efficiency, making it suitable for real-world applications in facial enhancement, surveillance, and digital restoration. Our code, model weights, and results are available at this https URL.</li>
<li><strong>摘要：</strong>面部超分辨率旨在从严重退化的低分辨率输入中恢复高质量的面部图像，但由于精细结构细节和特定身份特征的丢失，仍然具有挑战性。这项工作引入了 SwinIFS，这是一种地标引导的超分辨率框架，它将结构先验与分层注意机制相结合，以在中等和极端放大因子下实现身份保留重建。该方法将关键面部标志的密集高斯热图合并到输入表示中，使网络能够从处理的最早阶段就专注于语义上重要的面部区域。采用紧凑的 Swin Transformer 主干来捕获远程上下文信息，同时保留局部几何形状，使模型能够恢复微妙的面部纹理并保持全局结构一致性。 CelebA 基准上的大量实验表明，SwinIFS 实现了卓越的感知质量、更清晰的重建和改进的身份保留；它始终能产生更加逼真的结果，即使在 8 倍放大倍率下也能表现出强大的性能，而在这种情况下，大多数方法都无法恢复有意义的结构。 SwinIFS 还提供了重建精度和计算效率之间的有利平衡，使其适合面部增强、监控和数字修复等实际应用。我们的代码、模型权重和结果可从此 https URL 获取。</li>
</ul>

<h3>Title: Domain Adaptation of Carotid Ultrasound Images using Generative Adversarial Network</h3>
<ul>
<li><strong>Authors: </strong>Mohd Usama, Belal Ahmad, Christer Gronlund, Faleh Menawer R Althiyabi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01460">https://arxiv.org/abs/2601.01460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01460">https://arxiv.org/pdf/2601.01460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01460]] Domain Adaptation of Carotid Ultrasound Images using Generative Adversarial Network(https://arxiv.org/abs/2601.01460)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep learning has been extensively used in medical imaging applications, assuming that the test and training datasets belong to the same probability distribution. However, a common challenge arises when working with medical images generated by different systems or even the same system with different parameter settings. Such images contain diverse textures and reverberation noise that violate the aforementioned assumption. Consequently, models trained on data from one device or setting often struggle to perform effectively with data from other devices or settings. In addition, retraining models for each specific device or setting is labor-intensive and costly. To address these issues in ultrasound images, we propose a novel Generative Adversarial Network (GAN)-based model. We formulated the domain adaptation tasks as an image-to-image translation task, in which we modified the texture patterns and removed reverberation noise in the test data images from the source domain to align with those in the target domain images while keeping the image content unchanged. We applied the proposed method to two datasets containing carotid ultrasound images from three different domains. The experimental results demonstrate that the model successfully translated the texture pattern of images and removed reverberation noise from the ultrasound images. Furthermore, we evaluated the CycleGAN approaches for a comparative study with the proposed model. The experimental findings conclusively demonstrated that the proposed model achieved domain adaptation (histogram correlation (0.960 (0.019), & 0.920 (0.043) and bhattacharya distance (0.040 (0.020), & 0.085 (0.048)), compared to no adaptation (0.916 (0.062) & 0.890 (0.077), 0.090 (0.070) & 0.121 (0.095)) for both datasets.</li>
<li><strong>摘要：</strong>深度学习已广泛应用于医学成像应用，假设测试数据集和训练数据集属于相同的概率分布。然而，当处理由不同系统甚至具有不同参数设置的同一系统生成的医学图像时，会出现一个常见的挑战。这些图像包含违反上述假设的不同纹理和混响噪声。因此，根据来自一种设备或设置的数据训练的模型通常难以有效地处理来自其他设备或设置的数据。此外，针对每种特定设备或设置重新训练模型是劳动密集型且成本高昂的。为了解决超声图像中的这些问题，我们提出了一种新颖的基于生成对抗网络（GAN）的模型。我们将域适应任务制定为图像到图像的转换任务，其中我们修改了纹理模式并删除了源域中测试数据图像中的混响噪声，以与目标域图像中的混响噪声保持一致，同时保持图像内容不变。我们将所提出的方法应用于包含来自三个不同领域的颈动脉超声图像的两个数据集。实验结果表明，该模型成功地转换了图像的纹理图案并消除了超声图像中的混响噪声。此外，我们评估了 CycleGAN 方法，以便与所提出的模型进行比较研究。实验结果最终证明，与没有适应（0.916（0.062）和0.890（0.077）相比，所提出的模型实现了域适应（直方图相关性（0.960（0.019）和0.920（0.043）和bhattacharya距离（0.040（0.020）和0.085（0.048））））两个数据集均为 0.090 (0.070) 和 0.121 (0.095))。</li>
</ul>

<h3>Title: Multi-Subspace Multi-Modal Modeling for Diffusion Models: Estimation, Convergence and Mixture of Experts</h3>
<ul>
<li><strong>Authors: </strong>Ruofeng Yang, Yongcan Li, Bo Jiang, Cheng Chen, Shuai Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01475">https://arxiv.org/abs/2601.01475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01475">https://arxiv.org/pdf/2601.01475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01475]] Multi-Subspace Multi-Modal Modeling for Diffusion Models: Estimation, Convergence and Mixture of Experts(https://arxiv.org/abs/2601.01475)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recently, diffusion models have achieved a great performance with a small dataset of size $n$ and a fast optimization process. However, the estimation error of diffusion models suffers from the curse of dimensionality $n^{-1/D}$ with the data dimension $D$. Since images are usually a union of low-dimensional manifolds, current works model the data as a union of linear subspaces with Gaussian latent and achieve a $1/\sqrt{n}$ bound. Though this modeling reflects the multi-manifold property, the Gaussian latent can not capture the multi-modal property of the latent manifold. To bridge this gap, we propose the mixture subspace of low-rank mixture of Gaussian (MoLR-MoG) modeling, which models the target data as a union of $K$ linear subspaces, and each subspace admits a mixture of Gaussian latent ($n_k$ modals with dimension $d_k$). With this modeling, the corresponding score function naturally has a mixture of expert (MoE) structure, captures the multi-modal information, and contains nonlinear property. We first conduct real-world experiments to show that the generation results of MoE-latent MoG NN are much better than MoE-latent Gaussian score. Furthermore, MoE-latent MoG NN achieves a comparable performance with MoE-latent Unet with $10 \times$ parameters. These results indicate that the MoLR-MoG modeling is reasonable and suitable for real-world data. After that, based on such MoE-latent MoG score, we provide a $R^4\sqrt{\Sigma_{k=1}^Kn_k}\sqrt{\Sigma_{k=1}^Kn_kd_k}/\sqrt{n}$ estimation error, which escapes the curse of dimensionality by using data structure. Finally, we study the optimization process and prove the convergence guarantee under the MoLR-MoG modeling. Combined with these results, under a setting close to real-world data, this work explains why diffusion models only require a small training sample and enjoy a fast optimization process to achieve a great performance.</li>
<li><strong>摘要：</strong>最近，扩散模型凭借 $n$ 大小的小数据集和快速优化过程取得了出色的性能。然而，扩散模型的估计误差受到维度$n^{-1/D}$与数据维度$D$的诅咒。由于图像通常是低维流形的并集，因此当前的工作将数据建模为具有高斯潜伏的线性子空间的并集，并实现 $1/\sqrt{n}$ 界限。尽管该模型反映了多流形特性，但高斯潜流形无法捕捉潜流形的多模态特性。为了弥补这一差距，我们提出了低秩混合高斯（MoLR-MoG）建模的混合子空间，它将目标数据建模为 $K$ 线性子空间的并集，并且每个子空间允许高斯潜在的混合（维度为 $d_k$ 的 $n_k$ 模态）。通过这种建模，相应的评分函数自然具有混合专家（MoE）结构，捕获多模态信息，并包含非线性属性。我们首先进行了真实世界的实验，表明 MoE-latent MoG NN 的生成结果比 MoE-latent Gaussian 分数要好得多。此外，MoE-latent MoG NN 的性能与具有 10 美元×参数的 MoE-latent Unet 相当。这些结果表明MoLR-MoG建模是合理的并且适合真实世界的数据。之后，基于这样的MoE-latent MoG分数，我们提供了$R^4\sqrt{\Sigma_{k=1}^Kn_k}\sqrt{\Sigma_{k=1}^Kn_kd_k}/\sqrt{n}$估计误差，通过使用数据结构逃脱了维度的诅咒。最后，我们研究了优化过程并证明了MoLR-MoG模型下的收敛保证。结合这些结果，在接近真实世界数据的设置下，这项工作解释了为什么扩散模型只需要少量的训练样本并享受快速的优化过程即可实现出色的性能。</li>
</ul>

<h3>Title: Unified Generation and Self-Verification for Vision-Language Models via Advantage Decoupled Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Qiu, Heng Jia, Zhengwen Zeng, Shuheng Shen, Changhua Meng, Yi Yang, Linchao Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01483">https://arxiv.org/abs/2601.01483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01483">https://arxiv.org/pdf/2601.01483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01483]] Unified Generation and Self-Verification for Vision-Language Models via Advantage Decoupled Preference Optimization(https://arxiv.org/abs/2601.01483)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Parallel test-time scaling typically trains separate generation and verification models, incurring high training and inference costs. We propose Advantage Decoupled Preference Optimization (ADPO), a unified reinforcement learning framework that jointly learns answer generation and self-verification within a single policy. ADPO introduces two innovations: a preference verification reward improving verification capability and a decoupled optimization mechanism enabling synergistic optimization of generation and verification. Specifically, the preference verification reward computes mean verification scores from positive and negative samples as decision thresholds, providing positive feedback when prediction correctness aligns with answer correctness. Meanwhile, the advantage decoupled optimization computes separate advantages for generation and verification, applies token masks to isolate gradients, and combines masked GRPO objectives, preserving generation quality while calibrating verification scores. ADPO achieves up to +34.1% higher verification AUC and -53.5% lower inference time, with significant gains of +2.8%/+1.4% accuracy on MathVista/MMMU, +1.9 cIoU on ReasonSeg, and +1.7%/+1.0% step success rate on AndroidControl/GUI Odyssey.</li>
<li><strong>摘要：</strong>并行测试时间扩展通常会训练单独的生成和验证模型，从而产生高昂的训练和推理成本。我们提出了优势解耦偏好优化（ADPO），这是一个统一的强化学习框架，可以在单个策略中共同学习答案生成和自我验证。 ADPO引入了两项创新：提高验证能力的偏好验证奖励和实现生成和验证协同优化的解耦优化机制。具体来说，偏好验证奖励计算正样本和负样本的平均验证分数作为决策阈值，当预测正确性与答案正确性一致时提供正反馈。同时，优势解耦优化计算生成和验证的单独优势，应用令牌掩码来隔离梯度，并结合掩码GRPO目标，在校准验证分数的同时保持生成质量。 ADPO 的验证 AUC 提高了 34.1%，推理时间缩短了 -53.5%，在 MathVista/MMMU 上的准确率显着提高了 +2.8%/+1.4%，在 ReasonSeg 上显着提高了 +1.9 cIoU，在 AndroidControl/GUI Odyssey 上的步骤成功率提高了 +1.7%/+1.0%。</li>
</ul>

<h3>Title: A Novel Deep Learning Method for Segmenting the Left Ventricle in Cardiac Cine MRI</h3>
<ul>
<li><strong>Authors: </strong>Wenhui Chu, Aobo Jin, Hardik A. Gohel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01512">https://arxiv.org/abs/2601.01512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01512">https://arxiv.org/pdf/2601.01512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01512]] A Novel Deep Learning Method for Segmenting the Left Ventricle in Cardiac Cine MRI(https://arxiv.org/abs/2601.01512)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>This research aims to develop a novel deep learning network, GBU-Net, utilizing a group-batch-normalized U-Net framework, specifically designed for the precise semantic segmentation of the left ventricle in short-axis cine MRI scans. The methodology includes a down-sampling pathway for feature extraction and an up-sampling pathway for detail restoration, enhanced for medical imaging. Key modifications include techniques for better contextual understanding crucial in cardiac MRI segmentation. The dataset consists of 805 left ventricular MRI scans from 45 patients, with comparative analysis using established metrics such as the dice coefficient and mean perpendicular distance. GBU-Net significantly improves the accuracy of left ventricle segmentation in cine MRI scans. Its innovative design outperforms existing methods in tests, surpassing standard metrics like the dice coefficient and mean perpendicular distance. The approach is unique in its ability to capture contextual information, often missed in traditional CNN-based segmentation. An ensemble of the GBU-Net attains a 97% dice score on the SunnyBrook testing dataset. GBU-Net offers enhanced precision and contextual understanding in left ventricle segmentation for surgical robotics and medical analysis.</li>
<li><strong>摘要：</strong>本研究旨在开发一种新型深度学习网络 GBU-Net，利用群批量归一化 U-Net 框架，专为短轴电影 MRI 扫描中左心室的精确语义分割而设计。该方法包括用于特征提取的下采样路径和用于细节恢复的上采样路径，并针对医学成像进行了增强。关键修改包括更好地理解背景的技术，这对于心脏 MRI 分割至关重要。该数据集由 45 名患者的 805 幅左心室 MRI 扫描组成，并使用骰子系数和平均垂直距离等既定指标进行比较分析。 GBU-Net 显着提高了电影 MRI 扫描中左心室分割的准确性。其创新设计在测试中优于现有方法，超越了骰子系数和平均垂直距离等标准指标。该方法的独特之处在于它能够捕获上下文信息，而这在传统的基于 CNN 的分割中经常被忽略。 GBU-Net 的整体在 SunnyBrook 测试数据集上获得了 97% 的骰子分数。 GBU-Net 为手术机器人和医学分析的左心室分割提供了更高的精度和上下文理解。</li>
</ul>

<h3>Title: FastV-RAG: Towards Fast and Fine-Grained Video QA with Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Gen Li, Peiyu Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01513">https://arxiv.org/abs/2601.01513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01513">https://arxiv.org/pdf/2601.01513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01513]] FastV-RAG: Towards Fast and Fine-Grained Video QA with Retrieval-Augmented Generation(https://arxiv.org/abs/2601.01513)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) excel at visual reasoning but still struggle with integrating external knowledge. Retrieval-Augmented Generation (RAG) is a promising solution, but current methods remain inefficient and often fail to maintain high answer quality. To address these challenges, we propose VideoSpeculateRAG, an efficient VLM-based RAG framework built on two key ideas. First, we introduce a speculative decoding pipeline: a lightweight draft model quickly generates multiple answer candidates, which are then verified and refined by a more accurate heavyweight model, substantially reducing inference latency without sacrificing correctness. Second, we identify a major source of error - incorrect entity recognition in retrieved knowledge - and mitigate it with a simple yet effective similarity-based filtering strategy that improves entity alignment and boosts overall answer accuracy. Experiments demonstrate that VideoSpeculateRAG achieves comparable or higher accuracy than standard RAG approaches while accelerating inference by approximately 2x. Our framework highlights the potential of combining speculative decoding with retrieval-augmented reasoning to enhance efficiency and reliability in complex, knowledge-intensive multimodal tasks.</li>
<li><strong>摘要：</strong>视觉语言模型 (VLM) 擅长视觉推理，但在整合外部知识方面仍然存在困难。检索增强生成（RAG）是一种很有前途的解决方案，但当前的方法仍然效率低下，并且常常无法保持较高的答案质量。为了应对这些挑战，我们提出了 VideoSpeculateRAG，这是一种基于 VLM 的高效 RAG 框架，建立在两个关键思想的基础上。首先，我们引入推测性解码管道：轻量级草稿模型快速生成多个候选答案，然后由更准确的重量级模型进行验证和细化，从而在不牺牲正确性的情况下大幅减少推理延迟。其次，我们确定了错误的主要来源 - 检索到的知识中的实体识别不正确 - 并通过简单而有效的基于相似性的过滤策略来缓解它，该策略可以改善实体对齐并提高整体答案准确性。实验表明，VideoSpeculateRAG 的准确度与标准 RAG 方法相当或更高，同时推理速度提高了约 2 倍。我们的框架强调了将推测性解码与检索增强推理相结合的潜力，以提高复杂、知识密集型多模态任务的效率和可靠性。</li>
</ul>

<h3>Title: DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Yang Zhou, Hao Shao, Letian Wang, Zhuofan Zong, Hongsheng Li, Steven L. Waslander</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01528">https://arxiv.org/abs/2601.01528</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01528">https://arxiv.org/pdf/2601.01528</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01528]] DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving(https://arxiv.org/abs/2601.01528)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Video generation models, as one form of world models, have emerged as one of the most exciting frontiers in AI, promising agents the ability to imagine the future by modeling the temporal evolution of complex scenes. In autonomous driving, this vision gives rise to driving world models: generative simulators that imagine ego and agent futures, enabling scalable simulation, safe testing of corner cases, and rich synthetic data generation. Yet, despite fast-growing research activity, the field lacks a rigorous benchmark to measure progress and guide priorities. Existing evaluations remain limited: generic video metrics overlook safety-critical imaging factors; trajectory plausibility is rarely quantified; temporal and agent-level consistency is neglected; and controllability with respect to ego conditioning is ignored. Moreover, current datasets fail to cover the diversity of conditions required for real-world deployment. To address these gaps, we present DrivingGen, the first comprehensive benchmark for generative driving world models. DrivingGen combines a diverse evaluation dataset curated from both driving datasets and internet-scale video sources, spanning varied weather, time of day, geographic regions, and complex maneuvers, with a suite of new metrics that jointly assess visual realism, trajectory plausibility, temporal coherence, and controllability. Benchmarking 14 state-of-the-art models reveals clear trade-offs: general models look better but break physics, while driving-specific ones capture motion realistically but lag in visual quality. DrivingGen offers a unified evaluation framework to foster reliable, controllable, and deployable driving world models, enabling scalable simulation, planning, and data-driven decision-making.</li>
<li><strong>摘要：</strong>视频生成模型作为世界模型的一种形式，已成为人工智能领域最令人兴奋的前沿之一，它让智能体能够通过对复杂场景的时间演化进行建模来想象未来。在自动驾驶中，这一愿景催生了驾驶世界模型：想象自我和代理未来的生成模拟器，实现可扩展的模拟、极端情况的安全测试以及丰富的合成数据生成。然而，尽管研究活动快速增长，但该领域缺乏严格的基准来衡量进展和指导优先事项。现有的评估仍然有限：通用视频指标忽视了安全关键的成像因素；轨迹的合理性很少被量化；时间和代理级别的一致性被忽略；自我调节的可控性被忽略。此外，当前的数据集无法涵盖现实世界部署所需的条件的多样性。为了解决这些差距，我们推出了 DrivingGen，这是第一个生成驾驶世界模型的综合基准测试。 DrivingGen 将来自驾驶数据集和互联网规模视频源的多样化评估数据集（涵盖不同的天气、一天中的时间、地理区域和复杂的操作）与一套新指标相结合，共同评估视觉真实性、轨迹合理性、时间一致性和可控性。对 14 个最先进模型进行基准测试揭示了明显的权衡：通用模型看起来更好，但违反物理原理，而驾驶专用模型可以真实地捕捉运动，但视觉质量落后。 DrivingGen 提供统一的评估框架，以培育可靠、可控和可部署的驾驶世界模型，从而实现可扩展的模拟、规划和数据驱动的决策。</li>
</ul>

<h3>Title: Improving Flexible Image Tokenizers for Autoregressive Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Zixuan Fu, Lanqing Guo, Chong Wang, Binbin Song, Ding Liu, Bihan Wen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01535">https://arxiv.org/abs/2601.01535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01535">https://arxiv.org/pdf/2601.01535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01535]] Improving Flexible Image Tokenizers for Autoregressive Image Generation(https://arxiv.org/abs/2601.01535)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Flexible image tokenizers aim to represent an image using an ordered 1D variable-length token sequence. This flexible tokenization is typically achieved through nested dropout, where a portion of trailing tokens is randomly truncated during training, and the image is reconstructed using the remaining preceding sequence. However, this tail-truncation strategy inherently concentrates the image information in the early tokens, limiting the effectiveness of downstream AutoRegressive (AR) image generation as the token length increases. To overcome these limitations, we propose \textbf{ReToK}, a flexible tokenizer with \underline{Re}dundant \underline{Tok}en Padding and Hierarchical Semantic Regularization, designed to fully exploit all tokens for enhanced latent modeling. Specifically, we introduce \textbf{Redundant Token Padding} to activate tail tokens more frequently, thereby alleviating information over-concentration in the early tokens. In addition, we apply \textbf{Hierarchical Semantic Regularization} to align the decoding features of earlier tokens with those from a pre-trained vision foundation model, while progressively reducing the regularization strength toward the tail to allow finer low-level detail reconstruction. Extensive experiments demonstrate the effectiveness of ReTok: on ImageNet 256$\times$256, our method achieves superior generation performance compared with both flexible and fixed-length tokenizers. Code will be available at: \href{this https URL}{this https URL}</li>
<li><strong>摘要：</strong>灵活的图像标记器旨在使用有序的一维可变长度标记序列来表示图像。这种灵活的标记化通常通过嵌套 dropout 来实现，其中部分尾随标记在训练期间被随机截断，并使用剩余的先前序列重建图像。然而，这种尾部截断策略本质上将图像信息集中在早期令牌中，随着令牌长度的增加，限制了下游自回归（AR）图像生成的有效性。为了克服这些限制，我们提出了 \textbf{ReToK}，一种灵活的标记器，具有 \underline{Re}dundant \underline{Tok}en 填充和分层语义正则化，旨在充分利用所有标记来增强潜在建模。具体来说，我们引入 \textbf{Redundant Token Padding} 来更频繁地激活尾部令牌，从而缓解早期令牌中的信息过度集中。此外，我们应用 \textbf{层次语义正则化} 将早期标记的解码特征与预先训练的视觉基础模型的解码特征对齐，同时逐渐降低尾部的正则化强度，以允许更精细的低级细节重建。大量实验证明了 ReTok 的有效性：在 ImageNet 256$\times$256 上，与灵活和固定长度分词器相比，我们的方法实现了卓越的生成性能。代码位于：\href{此 https URL}{此 https URL}</li>
</ul>

<h3>Title: The Two-Stage Decision-Sampling Hypothesis: Understanding the Emergence of Self-Reflection in RL-Trained LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zibo Zhao (1), Yuanting Zha (2), Haipeng Zhang (2), Xingcheng Xu (3) ((1) Arizona State University, (2) ShanghaiTech University, (3) Shanghai Artificial Intelligence Laboratory)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01580">https://arxiv.org/abs/2601.01580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01580">https://arxiv.org/pdf/2601.01580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01580]] The Two-Stage Decision-Sampling Hypothesis: Understanding the Emergence of Self-Reflection in RL-Trained LLMs(https://arxiv.org/abs/2601.01580)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Self-reflection capabilities emerge in Large Language Models after RL post-training, with multi-turn RL achieving substantial gains over SFT counterparts. Yet the mechanism of how a unified optimization objective gives rise to functionally distinct capabilities of generating solutions and evaluating when to revise them remains opaque. To address this question, we introduce the Gradient Attribution Property to characterize how reward gradients distribute across policy components, formalized through the Two-Stage Decision-Sampling (DS) Hypothesis, which decomposes the policy into sampling ($\pi_{sample}$) for generation and decision ($\pi_{d}$) for verification. We prove that surrogate rewards exhibit Balanced Gradient Attribution, while SFT and KL penalties exhibit Unbalanced Gradient Attribution, with length-weighting creating asymmetric regularization that constrains $\pi_{sample}$ while leaving $\pi_{d}$ under-optimized, providing an theoretical explanation of why RL succeeds where SFT fails. We also empirically validate our theoretical predictions on arithmetic reasoning demonstrates that RL's superior generalization stems primarily from improved decision-making ($\pi_{d}$) rather than sampling capabilities, providing a first-principles mechanistic explanation for self-correction in thinking models.</li>
<li><strong>摘要：</strong>经过 RL 后训练后，大型语言模型中出现了自我反思能力，多轮 RL 比 SFT 同行取得了显着的进步。然而，统一的优化目标如何产生功能上不同的生成解决方案和评估何时修改解决方案的能力的机制仍然不透明。为了解决这个问题，我们引入梯度归因属性来描述奖励梯度如何在策略组件之间分布，通过两阶段决策采样（DS）假设形式化，该假设将策略分解为用于生成的采样（$\pi_{sample}$）和用于验证的决策（$\pi_{d}$）。我们证明代理奖励表现出平衡梯度归因，而 SFT 和 KL 惩罚表现出不平衡梯度归因，长度加权创建不对称正则化，限制 $\pi_{sample}$，同时使 $\pi_{d}$ 优化不足，这为为什么 RL 成功而 SFT 失败提供了理论解释。我们还通过实证验证了我们对算术推理的理论预测，表明强化学习的卓越泛化能力主要源于改进的决策（$\pi_{d}$）而不是采样能力，为思维模型中的自我纠正提供了第一性原理的机械解释。</li>
</ul>

<h3>Title: Beyond Patches: Global-aware Autoregressive Model for Multimodal Few-Shot Font Generation</h3>
<ul>
<li><strong>Authors: </strong>Haonan Cai, Yuxuan Luo, Zhouhui Lian</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01593">https://arxiv.org/abs/2601.01593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01593">https://arxiv.org/pdf/2601.01593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01593]] Beyond Patches: Global-aware Autoregressive Model for Multimodal Few-Shot Font Generation(https://arxiv.org/abs/2601.01593)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Manual font design is an intricate process that transforms a stylistic visual concept into a coherent glyph set. This challenge persists in automated Few-shot Font Generation (FFG), where models often struggle to preserve both the structural integrity and stylistic fidelity from limited references. While autoregressive (AR) models have demonstrated impressive generative capabilities, their application to FFG is constrained by conventional patch-level tokenization, which neglects global dependencies crucial for coherent font synthesis. Moreover, existing FFG methods remain within the image-to-image paradigm, relying solely on visual references and overlooking the role of language in conveying stylistic intent during font design. To address these limitations, we propose GAR-Font, a novel AR framework for multimodal few-shot font generation. GAR-Font introduces a global-aware tokenizer that effectively captures both local structures and global stylistic patterns, a multimodal style encoder offering flexible style control through a lightweight language-style adapter without requiring intensive multimodal pretraining, and a post-refinement pipeline that further enhances structural fidelity and style coherence. Extensive experiments show that GAR-Font outperforms existing FFG methods, excelling in maintaining global style faithfulness and achieving higher-quality results with textual stylistic guidance.</li>
<li><strong>摘要：</strong>手动字体设计是一个复杂的过程，它将风格视觉概念转化为连贯的字形集。这一挑战在自动化少样本字体生成 (FFG) 中依然存在，其中模型常常难以在有限的参考中保持结构完整性和风格保真度。虽然自回归 (AR) 模型已经展示了令人印象深刻的生成能力，但它们在 FFG 中的应用受到传统补丁级标记化的限制，这种标记化忽略了对于连贯字体合成至关重要的全局依赖性。此外，现有的 FFG 方法仍然停留在图像到图像的范式内，仅依赖于视觉参考，而忽略了语言在字体设计过程中传达风格意图方面的作用。为了解决这些限制，我们提出了 GAR-Font，这是一种用于多模式少样本字体生成的新颖 AR 框架。 GAR-Font 引入了一个全局感知分词器，可有效捕获局部结构和全局风格模式；多模式风格编码器，通过轻量级语言风格适配器提供灵活的风格控制，无需密集的多模式预训练；以及后细化管道，可进一步增强结构保真度和风格连贯性。大量实验表明，GAR-Font 的性能优于现有的 FFG 方法，在保持全局风格忠实性方面表现出色，并通过文本风格指导实现更高质量的结果。</li>
</ul>

<h3>Title: CAP-IQA: Context-Aware Prompt-Guided CT Image Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Kazi Ramisa Rifa, Jie Zhang, Abdullah Imran</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01613">https://arxiv.org/abs/2601.01613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01613">https://arxiv.org/pdf/2601.01613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01613]] CAP-IQA: Context-Aware Prompt-Guided CT Image Quality Assessment(https://arxiv.org/abs/2601.01613)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Prompt-based methods, which encode medical priors through descriptive text, have been only minimally explored for CT Image Quality Assessment (IQA). While such prompts can embed prior knowledge about diagnostic quality, they often introduce bias by reflecting idealized definitions that may not hold under real-world degradations such as noise, motion artifacts, or scanner variability. To address this, we propose the Context-Aware Prompt-guided Image Quality Assessment (CAP-IQA) framework, which integrates text-level priors with instance-level context prompts and applies causal debiasing to separate idealized knowledge from factual, image-specific degradations. Our framework combines a CNN-based visual encoder with a domain-specific text encoder to assess diagnostic visibility, anatomical clarity, and noise perception in abdominal CT images. The model leverages radiology-style prompts and context-aware fusion to align semantic and perceptual representations. On the 2023 LDCTIQA challenge benchmark, CAP-IQA achieves an overall correlation score of 2.8590 (sum of PLCC, SROCC, and KROCC), surpassing the top-ranked leaderboard team (2.7427) by 4.24%. Moreover, our comprehensive ablation experiments confirm that prompt-guided fusion and the simplified encoder-only design jointly enhance feature alignment and interpretability. Furthermore, evaluation on an in-house dataset of 91,514 pediatric CT images demonstrates the true generalizability of CAP-IQA in assessing perceptual fidelity in a different patient population.</li>
<li><strong>摘要：</strong>基于提示的方法通过描述性文本对医学先验进行编码，在 CT 图像质量评估 (IQA) 中仅得到了最低程度的探索。虽然此类提示可以嵌入有关诊断质量的先验知识，但它们通常会通过反映理想化的定义而引入偏差，而这些理想化的定义在现实世界的退化（例如噪声、运动伪影或扫描仪变异性）下可能不成立。为了解决这个问题，我们提出了上下文感知提示引导的图像质量评估（CAP-IQA）框架，该框架将文本级先验与实例级上下文提示相结合，并应用因果去偏将理想化的知识与实际的、图像特定的退化分开。我们的框架将基于 CNN 的视觉编码器与特定领域的文本编码器相结合，以评估腹部 CT 图像中的诊断可视性、解剖清晰度和噪声感知。该模型利用放射学风格的提示和上下文感知融合来协调语义和感知表示。在 2023 年 LDCTIQA 挑战基准上，CAP-IQA 的整体相关性得分为 2.8590（PLCC、SROCC 和 KROCC 之和），比排名第一的排行榜团队（2.7427）高出了 4.24%。此外，我们的综合消融实验证实，提示引导融合和简化的仅编码器设计共同增强了特征对齐和可解释性。此外，对 91,514 张儿科 CT 图像的内部数据集的评估证明了 CAP-IQA 在评估不同患者群体的感知保真度方面的真正普遍性。</li>
</ul>

<h3>Title: Length-Aware Adversarial Training for Variable-Length Trajectories: Digital Twins for Mall Shopper Paths</h3>
<ul>
<li><strong>Authors: </strong>He Sun, Jiwoong Shin, Ravi Dhar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01663">https://arxiv.org/abs/2601.01663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01663">https://arxiv.org/pdf/2601.01663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01663]] Length-Aware Adversarial Training for Variable-Length Trajectories: Digital Twins for Mall Shopper Paths(https://arxiv.org/abs/2601.01663)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We study generative modeling of \emph{variable-length trajectories} -- sequences of visited locations/items with associated timestamps -- for downstream simulation and counterfactual analysis. A recurring practical issue is that standard mini-batch training can be unstable when trajectory lengths are highly heterogeneous, which in turn degrades \emph{distribution matching} for trajectory-derived statistics. We propose \textbf{length-aware sampling (LAS)}, a simple batching strategy that groups trajectories by length and samples batches from a single length bucket, reducing within-batch length heterogeneity (and making updates more consistent) without changing the model class. We integrate LAS into a conditional trajectory GAN with auxiliary time-alignment losses and provide (i) a distribution-level guarantee for derived variables under mild boundedness assumptions, and (ii) an IPM/Wasserstein mechanism explaining why LAS improves distribution matching by removing length-only shortcut critics and targeting within-bucket discrepancies. Empirically, LAS consistently improves matching of derived-variable distributions on a multi-mall dataset of shopper trajectories and on diverse public sequence datasets (GPS, education, e-commerce, and movies), outperforming random sampling across dataset-specific metrics.</li>
<li><strong>摘要：</strong>我们研究 \emph{可变长度轨迹}（具有相关时间戳的访问位置/项目序列）的生成模型，用于下游模拟和反事实分析。一个反复出现的实际问题是，当轨迹长度高度异构时，标准小批量训练可能不稳定，这反过来又降低了轨迹导出统计数据的\emph{分布匹配}。我们提出 \textbf{长度感知采样（LAS）}，这是一种简单的批处理策略，可以按长度对轨迹进行分组，并从单个长度桶中对批次进行采样，从而在不更改模型类的情况下减少批次内长度异质性（并使更新更加一致）。我们将 LAS 集成到具有辅助时间对齐损失的条件轨迹 GAN 中，并提供 (i) 在温和有界假设下为派生变量提供分布级别保证，以及 (ii) IPM/Wasserstein 机制解释了为什么 LAS 通过删除仅限长度的快捷批评和针对桶内差异来改进分布匹配。根据经验，LAS 持续改进了购物者轨迹的多个购物中心数据集和各种公共序列数据集（GPS、教育、电子商务和电影）上派生变量分布的匹配，优于跨数据集特定指标的随机采样。</li>
</ul>

<h3>Title: Adversarial Instance Generation and Robust Training for Neural Combinatorial Optimization with Multiple Objectives</h3>
<ul>
<li><strong>Authors: </strong>Wei Liu, Yaoxin Wu, Yingqian Zhang, Thomas Bäck, Yingjie Fan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01665">https://arxiv.org/abs/2601.01665</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01665">https://arxiv.org/pdf/2601.01665</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01665]] Adversarial Instance Generation and Robust Training for Neural Combinatorial Optimization with Multiple Objectives(https://arxiv.org/abs/2601.01665)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Deep reinforcement learning (DRL) has shown great promise in addressing multi-objective combinatorial optimization problems (MOCOPs). Nevertheless, the robustness of these learning-based solvers has remained insufficiently explored, especially across diverse and complex problem distributions. In this paper, we propose a unified robustness-oriented framework for preference-conditioned DRL solvers for MOCOPs. Within this framework, we develop a preference-based adversarial attack to generate hard instances that expose solver weaknesses, and quantify the attack impact by the resulting degradation on Pareto-front quality. We further introduce a defense strategy that integrates hardness-aware preference selection into adversarial training to reduce overfitting to restricted preference regions and improve out-of-distribution performance. The experimental results on multi-objective traveling salesman problem (MOTSP), multi-objective capacitated vehicle routing problem (MOCVRP), and multi-objective knapsack problem (MOKP) verify that our attack method successfully learns hard instances for different solvers. Furthermore, our defense method significantly strengthens the robustness and generalizability of neural solvers, delivering superior performance on hard or out-of-distribution instances.</li>
<li><strong>摘要：</strong>深度强化学习（DRL）在解决多目标组合优化问题（MOCOP）方面显示出巨大的前景。然而，这些基于学习的求解器的鲁棒性仍未得到充分探索，特别是在多样化和复杂的问题分布中。在本文中，我们为 MOCOP 的偏好条件 DRL 求解器提出了一个统一的面向鲁棒性的框架。在此框架内，我们开发了一种基于偏好的对抗性攻击，以生成暴露求解器弱点的硬实例，并通过由此导致的帕累托前沿质量下降来量化攻击影响。我们进一步引入了一种防御策略，将硬度感知偏好选择集成到对抗性训练中，以减少对受限偏好区域的过度拟合并提高分布外性能。多目标旅行商问题（MOTSP）、多目标容量车辆路径问题（MOCVRP）和多目标背包问题（MOKP）的实验结果验证了我们的攻击方法成功地学习了不同求解器的困难实例。此外，我们的防御方法显着增强了神经求解器的鲁棒性和泛化性，在硬实例或分布外实例上提供卓越的性能。</li>
</ul>

<h3>Title: HeurekaBench: A Benchmarking Framework for AI Co-scientist</h3>
<ul>
<li><strong>Authors: </strong>Siba Smarak Panigrahi, Jovana Videnović, Maria Brbić</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01678">https://arxiv.org/abs/2601.01678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01678">https://arxiv.org/pdf/2601.01678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01678]] HeurekaBench: A Benchmarking Framework for AI Co-scientist(https://arxiv.org/abs/2601.01678)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>LLM-based reasoning models have enabled the development of agentic systems that act as co-scientists, assisting in multi-step scientific analysis. However, evaluating these systems is challenging, as it requires realistic, end-to-end research scenarios that integrate data analysis, interpretation, and the generation of new insights from the experimental data. To address this limitation, we introduce HeurekaBench, a framework to create benchmarks with exploratory, open-ended research questions for experimental datasets. Each such question is grounded in a scientific study and its corresponding code repository, and is created using a semi-automated pipeline that leverages multiple LLMs to extract insights and generate candidate workflows, which are then verified against reported findings. We instantiate the framework in single-cell biology to obtain sc-HeurekaBench benchmark and use it to compare state-of-the-art single-cell agents. We further showcase the benefits of our benchmark for quantitatively analyzing current design choices in agentic systems. We find that the addition of a critic module can improve ill-formed responses for open-source LLM-based agents by up to 22% and close the gap with their closed-source counterparts. Overall, HeurekaBench sets a path toward rigorous, end-to-end evaluation of scientific agents, grounding benchmark construction in real scientific workflows.</li>
<li><strong>摘要：</strong>基于法学硕士的推理模型使得代理系统的开发成为可能，该系统充当联合科学家，协助多步骤的科学分析。然而，评估这些系统具有挑战性，因为它需要现实的、端到端的研究场景，整合数据分析、解释以及从实验数据中生成新的见解。为了解决这一限制，我们引入了 HeurekaBench，这是一个为实验数据集创建具有探索性、开放式研究问题的基准的框架。每个此类问题都以科学研究及其相应的代码存储库为基础，并使用半自动化管道创建，该管道利用多个法学硕士来提取见解并生成候选工作流程，然后根据报告的结果进行验证。我们在单细胞生物学中实例化该框架以获得 sc-HeurekaBench 基准，并用它来比较最先进的单细胞试剂。我们进一步展示了我们的基准对定量分析代理系统中当前设计选择的好处。我们发现，添加批评家模块可以将基于开源 LLM 的代理的格式错误响应提高高达 22%，并缩小与闭源代理的差距。总体而言，HeurekaBench 为科学代理的严格、端到端评估开辟了一条道路，为真正的科学工作流程奠定了基准建设的基础。</li>
</ul>

<h3>Title: Mitigating Longitudinal Performance Degradation in Child Face Recognition Using Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Afzal Hossain, Stephanie Schuckers</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01689">https://arxiv.org/abs/2601.01689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01689">https://arxiv.org/pdf/2601.01689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01689]] Mitigating Longitudinal Performance Degradation in Child Face Recognition Using Synthetic Data(https://arxiv.org/abs/2601.01689)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Longitudinal face recognition in children remains challenging due to rapid and nonlinear facial growth, which causes template drift and increasing verification errors over time. This work investigates whether synthetic face data can act as a longitudinal stabilizer by improving temporal robustness of child face recognition models. Using an identity disjoint protocol on the Young Face Aging (YFA) dataset, we evaluate three settings: (i) pretrained MagFace embeddings without dataset specific fine-tuning, (ii) MagFace fine-tuned using authentic training faces only, and (iii) MagFace fine-tuned using a combination of authentic and synthetically generated training faces. Synthetic data is generated using StyleGAN2 ADA and incorporated exclusively within the training identities; a post generation filtering step is applied to mitigate identity leakage and remove artifact affected samples. Experimental results across enrollment verification gaps from 6 to 36 months show that synthetic-augmented fine tuning substantially reduces error rates relative to both the pretrained baseline and real only fine tuning. These findings provide a risk aware assessment of synthetic augmentation for improving identity persistence in pediatric face recognition.</li>
<li><strong>摘要：</strong>由于快速且非线性的面部生长，儿童的纵向面部识别仍然具有挑战性，这会导致模板漂移并随着时间的推移增加验证错误。这项工作研究了合成人脸数据是否可以通过提高儿童人脸识别模型的时间鲁棒性来充当纵向稳定器。在 Young Face Aging (YFA) 数据集上使用身份不相交协议，我们评估三种设置：(i) 没有数据集特定微调的预训练 MagFace 嵌入，(ii) 仅使用真实的训练面孔进行微调，以及 (iii) 使用真实和合成生成的训练面孔的组合进行 MagFace 微调。合成数据是使用 StyleGAN2 ADA 生成的，并专门纳入训练身份中；应用生成后过滤步骤来减轻身份泄漏并删除受伪影影响的样本。 6 至 36 个月的注册验证间隔的实验结果表明，相对于预训练基线和真实微调，合成增强微调大大降低了错误率。这些发现提供了对合成增强的风险意识评估，以提高儿科面部识别中的身份持久性。</li>
</ul>

<h3>Title: Entropy-Aligned Decoding of LMs for Better Writing and Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Kareem Ahmed, Sameer Singh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01714">https://arxiv.org/abs/2601.01714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01714">https://arxiv.org/pdf/2601.01714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01714]] Entropy-Aligned Decoding of LMs for Better Writing and Reasoning(https://arxiv.org/abs/2601.01714)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Language models (LMs) are trained on billions of tokens in an attempt to recover the true language distribution. Still, vanilla random sampling from LMs yields low quality generations. Decoding algorithms attempt to restrict the LM distribution to a set of high-probability continuations, but rely on greedy heuristics that introduce myopic distortions, yielding sentences that are homogeneous, repetitive and incoherent. In this paper, we introduce EPIC, a hyperparameter-free decoding approach that incorporates the entropy of future trajectories into LM decoding. EPIC explicitly regulates the amount of uncertainty expressed at every step of generation, aligning the sampling distribution's entropy to the aleatoric (data) uncertainty. Through Entropy-Aware Lazy Gumbel-Max sampling, EPIC manages to be exact, while also being efficient, requiring only a sublinear number of entropy evaluations per step. Unlike current baselines, EPIC yields sampling distributions that are empirically well-aligned with the entropy of the underlying data distribution. Across creative writing and summarization tasks, EPIC consistently improves LM-as-judge preference win-rates over widely used decoding strategies. These preference gains are complemented by automatic metrics, showing that EPIC produces more diverse generations and more faithful summaries. We also evaluate EPIC on mathematical reasoning, where it outperforms all baselines.</li>
<li><strong>摘要：</strong>语言模型 (LM) 经过数十亿个标记的训练，试图恢复真实的语言分布。尽管如此，从 LM 中进行普通随机抽样会产生低质量的生成。解码算法试图将 LM 分布限制为一组高概率的延续，但依赖于引入短视扭曲的贪婪启发法，产生同质、重复和不连贯的句子。在本文中，我们介绍了 EPIC，一种无超参数解码方法，它将未来轨迹的熵纳入 LM 解码中。 EPIC 明确规定了生成的每个步骤所表达的不确定性量，使采样分布的熵与任意（数据）不确定性保持一致。通过熵感知惰性 Gumbel-Max 采样，EPIC 能够做到精确且高效，每步仅需要亚线性数量的熵评估。与当前基线不同，EPIC 生成的采样分布在经验上与基础数据分布的熵非常一致。在创意写作和摘要任务中，与广泛使用的解码策略相比，EPIC 持续提高了 LM 作为法官的偏好胜率。这些偏好增益得到了自动指标的补充，表明 EPIC 生成了更多样化的代和更忠实的摘要。我们还对 EPIC 的数学推理进行了评估，它的表现优于所有基线。</li>
</ul>

<h3>Title: MANGO:Natural Multi-speaker 3D Talking Head Generation via 2D-Lifted Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Lei Zhu, Lijian Lin, Ye Zhu, Jiahao Wu, Xuehan Hou, Yu Li, Yunfei Liu, Jie Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01749">https://arxiv.org/abs/2601.01749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01749">https://arxiv.org/pdf/2601.01749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01749]] MANGO:Natural Multi-speaker 3D Talking Head Generation via 2D-Lifted Enhancement(https://arxiv.org/abs/2601.01749)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Current audio-driven 3D head generation methods mainly focus on single-speaker scenarios, lacking natural, bidirectional listen-and-speak interaction. Achieving seamless conversational behavior, where speaking and listening states transition fluidly remains a key challenge. Existing 3D conversational avatar approaches rely on error-prone pseudo-3D labels that fail to capture fine-grained facial dynamics. To address these limitations, we introduce a novel two-stage framework MANGO, which leveraging pure image-level supervision by alternately training to mitigate the noise introduced by pseudo-3D labels, thereby achieving better alignment with real-world conversational behaviors. Specifically, in the first stage, a diffusion-based transformer with a dual-audio interaction module models natural 3D motion from multi-speaker audio. In the second stage, we use a fast 3D Gaussian Renderer to generate high-fidelity images and provide 2D-level photometric supervision for the 3D motions through alternate training. Additionally, we introduce MANGO-Dialog, a high-quality dataset with over 50 hours of aligned 2D-3D conversational data across 500+ identities. Extensive experiments demonstrate that our method achieves exceptional accuracy and realism in modeling two-person 3D dialogue motion, significantly advancing the fidelity and controllability of audio-driven talking heads.</li>
<li><strong>摘要：</strong>目前音频驱动的3D头部生成方法主要针对单扬声器场景，缺乏自然、双向的听和说交互。实现无缝对话行为（即说和听状态的流畅转换）仍然是一个关键挑战。现有的 3D 对话化身方法依赖于容易出错的伪 3D 标签，无法捕捉细粒度的面部动态。为了解决这些限制，我们引入了一种新颖的两阶段框架 MANGO，它通过交替训练来利用纯图像级监督来减轻伪 3D 标签引入的噪声，从而实现与现实世界对话行为的更好对齐。具体来说，在第一阶段，带有双音频交互模块的基于扩散的变压器对多扬声器音频的自然 3D 运动进行建模。在第二阶段，我们使用快速 3D 高斯渲染器生成高保真图像，并通过交替训练为 3D 运动提供 2D 级光度监控。此外，我们还推出了 MANGO-Dialog，这是一个高质量的数据集，包含 500 多个身份的超过 50 小时的对齐 2D-3D 对话数据。大量实验表明，我们的方法在模拟两人 3D 对话运动方面实现了卓越的准确性和真实性，显着提高了音频驱动头部说话的保真度和可控性。</li>
</ul>

<h3>Title: HyperCLOVA X 8B Omni</h3>
<ul>
<li><strong>Authors: </strong>NAVER Cloud HyperCLOVA X Team</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01792">https://arxiv.org/abs/2601.01792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01792">https://arxiv.org/pdf/2601.01792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01792]] HyperCLOVA X 8B Omni(https://arxiv.org/abs/2601.01792)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In this report, we present HyperCLOVA X 8B Omni, the first any-to-any omnimodal model in the HyperCLOVA X family that supports text, audio, and vision as both inputs and outputs. By consolidating multimodal understanding and generation into a single model rather than separate modality-specific pipelines, HyperCLOVA X 8B Omni serves as an 8B-scale omni-pathfinding point toward practical any-to-any omni assistants. At a high level, the model unifies modalities through a shared next-token prediction interface over an interleaved multimodal sequence, while vision and audio encoders inject continuous embeddings for fine-grained understanding and grounding. Empirical evaluations demonstrate competitive performance against comparably sized models across diverse input-output combinations spanning text, audio, and vision, in both Korean and English. We anticipate that the open-weight release of HyperCLOVA X 8B Omni will support a wide range of research and deployment scenarios.</li>
<li><strong>摘要：</strong>在本报告中，我们介绍了 HyperCLOVA X 8B Omni，这是 HyperCLOVA X 系列中第一个任意对任意全模式模型，支持文本、音频和视觉作为输入和输出。通过将多模态理解和生成整合到单个模型中，而不是单独的特定模态管道，HyperCLOVA X 8B Omni 可作为 8B 规模的全向寻路点，实现实用的任意到任意全向助手。在高层次上，该模型通过交错多模态序列上的共享下一个令牌预测接口来统一模态，而视觉和音频编码器则注入连续嵌入以实现细粒度的理解和基础。实证评估表明，在韩语和英语的文本、音频和视觉等多种输入输出组合中，与同等规模的模型相比，其性能具有竞争力。我们预计 HyperCLOVA X 8B Omni 的开放重量版本将支持广泛的研究和部署场景。</li>
</ul>

<h3>Title: ESGaussianFace: Emotional and Stylized Audio-Driven Facial Animation via 3D Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Chuhang Ma, Shuai Tan, Ye Pan, Jiaolong Yang, Xin Tong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01847">https://arxiv.org/abs/2601.01847</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01847">https://arxiv.org/pdf/2601.01847</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01847]] ESGaussianFace: Emotional and Stylized Audio-Driven Facial Animation via 3D Gaussian Splatting(https://arxiv.org/abs/2601.01847)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Most current audio-driven facial animation research primarily focuses on generating videos with neutral emotions. While some studies have addressed the generation of facial videos driven by emotional audio, efficiently generating high-quality talking head videos that integrate both emotional expressions and style features remains a significant challenge. In this paper, we propose ESGaussianFace, an innovative framework for emotional and stylized audio-driven facial animation. Our approach leverages 3D Gaussian Splatting to reconstruct 3D scenes and render videos, ensuring efficient generation of 3D consistent results. We propose an emotion-audio-guided spatial attention method that effectively integrates emotion features with audio content features. Through emotion-guided attention, the model is able to reconstruct facial details across different emotional states more accurately. To achieve emotional and stylized deformations of the 3D Gaussian points through emotion and style features, we introduce two 3D Gaussian deformation predictors. Futhermore, we propose a multi-stage training strategy, enabling the step-by-step learning of the character's lip movements, emotional variations, and style features. Our generated results exhibit high efficiency, high quality, and 3D consistency. Extensive experimental results demonstrate that our method outperforms existing state-of-the-art techniques in terms of lip movement accuracy, expression variation, and style feature expressiveness.</li>
<li><strong>摘要：</strong>当前大多数音频驱动的面部动画研究主要集中于生成具有中性情感的视频。虽然一些研究已经解决了由情感音频驱动的面部视频的生成问题，但有效生成融合情感表达和风格特征的高质量头部特写视频仍然是一个重大挑战。在本文中，我们提出了 ESGaussianFace，这是一种用于情感和风格化音频驱动面部动画的创新框架。我们的方法利用 3D Gaussian Splatting 来重建 3D 场景并渲染视频，确保高效生成 3D 一致的结果。我们提出了一种情感音频引导的空间注意方法，该方法有效地将情感特征与音频内容特征结合起来。通过情绪引导的注意力，该模型能够更准确地重建不同情绪状态下的面部细节。为了通过情感和风格特征实现 3D 高斯点的情感和风格化变形，我们引入了两个 3D 高斯变形预测器。此外，我们提出了一种多阶段训练策略，能够逐步学习角色的嘴唇动作、情绪变化和风格特征。我们生成的结果表现出高效率、高质量和 3D 一致性。大量的实验结果表明，我们的方法在嘴唇运动准确性、表情变化和风格特征表现力方面优于现有的最先进技术。</li>
</ul>

<h3>Title: RRNet: Configurable Real-Time Video Enhancement with Arbitrary Local Lighting Variations</h3>
<ul>
<li><strong>Authors: </strong>Wenlong Yang, Canran Jin, Weihang Yuan, Chao Wang, Lifeng Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01865">https://arxiv.org/abs/2601.01865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01865">https://arxiv.org/pdf/2601.01865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01865]] RRNet: Configurable Real-Time Video Enhancement with Arbitrary Local Lighting Variations(https://arxiv.org/abs/2601.01865)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the growing demand for real-time video enhancement in live applications, existing methods often struggle to balance speed and effective exposure control, particularly under uneven lighting. We introduce RRNet (Rendering Relighting Network), a lightweight and configurable framework that achieves a state-of-the-art tradeoff between visual quality and efficiency. By estimating parameters for a minimal set of virtual light sources, RRNet enables localized relighting through a depth-aware rendering module without requiring pixel-aligned training data. This object-aware formulation preserves facial identity and supports real-time, high-resolution performance using a streamlined encoder and lightweight prediction head. To facilitate training, we propose a generative AI-based dataset creation pipeline that synthesizes diverse lighting conditions at low cost. With its interpretable lighting control and efficient architecture, RRNet is well suited for practical applications such as video conferencing, AR-based portrait enhancement, and mobile photography. Experiments show that RRNet consistently outperforms prior methods in low-light enhancement, localized illumination adjustment, and glare removal.</li>
<li><strong>摘要：</strong>随着现场应用中对实时视频增强的需求不断增长，现有方法往往难以平衡速度和有效的曝光控制，特别是在照明不均匀的情况下。我们引入了 RRNet（渲染重新照明网络），这是一个轻量级且可配置的框架，可实现视觉质量和效率之间最先进的权衡。通过估计最小虚拟光源集的参数，RRNet 通过深度感知渲染模块实现局部重新照明，而不需要像素对齐的训练数据。这种对象感知配方保留了面部身份，并使用简化的编码器和轻量级预测头支持实时、高分辨率性能。为了促进训练，我们提出了一种基于人工智能的生成数据集创建管道，以低成本综合不同的照明条件。凭借其可解释的照明控制和高效的架构，RRNet 非常适合视频会议、基于 AR 的肖像增强和移动摄影等实际应用。实验表明，RRNet 在低光增强、局部照明调整和眩光消除方面始终优于现有方法。</li>
</ul>

<h3>Title: Agentic AI in Remote Sensing: Foundations, Taxonomy, and Emerging Systems</h3>
<ul>
<li><strong>Authors: </strong>Niloufar Alipour Talemi, Julia Boone, Fatemeh Afghah</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01891">https://arxiv.org/abs/2601.01891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01891">https://arxiv.org/pdf/2601.01891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01891]] Agentic AI in Remote Sensing: Foundations, Taxonomy, and Emerging Systems(https://arxiv.org/abs/2601.01891)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The paradigm of Earth Observation analysis is shifting from static deep learning models to autonomous agentic AI. Although recent vision foundation models and multimodal large language models advance representation learning, they often lack the sequential planning and active tool orchestration required for complex geospatial workflows. This survey presents the first comprehensive review of agentic AI in remote sensing. We introduce a unified taxonomy distinguishing between single-agent copilots and multi-agent systems while analyzing architectural foundations such as planning mechanisms, retrieval-augmented generation, and memory structures. Furthermore, we review emerging benchmarks that move the evaluation from pixel-level accuracy to trajectory-aware reasoning correctness. By critically examining limitations in grounding, safety, and orchestration, this work outlines a strategic roadmap for the development of robust, autonomous geospatial intelligence.</li>
<li><strong>摘要：</strong>地球观测分析的范式正在从静态深度学习模型转向自主代理人工智能。尽管最近的视觉基础模型和多模态大语言模型促进了表示学习，但它们通常缺乏复杂地理空间工作流程所需的顺序规划和主动工具编排。这项调查首次对遥感领域的代理人工智能进行了全面审查。我们引入了区分单代理副驾驶和多代理系统的统一分类法，同时分析了规划机制、检索增强生成和内存结构等架构基础。此外，我们回顾了新兴的基准，将评估从像素级精度转移到轨迹感知推理正确性。通过严格审查基础、安全和编排方面的局限性，这项工作概述了发展强大的自主地理空间智能的战略路线图。</li>
</ul>

<h3>Title: Forget Less by Learning from Parents Through Hierarchical Relationships</h3>
<ul>
<li><strong>Authors: </strong>Arjun Ramesh Kaushik, Naresh Kumar Devulapally, Vishnu Suresh Lokhande, Nalini K. Ratha, Venu Govindaraju</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01892">https://arxiv.org/abs/2601.01892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01892">https://arxiv.org/pdf/2601.01892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01892]] Forget Less by Learning from Parents Through Hierarchical Relationships(https://arxiv.org/abs/2601.01892)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Custom Diffusion Models (CDMs) offer impressive capabilities for personalization in generative modeling, yet they remain vulnerable to catastrophic forgetting when learning new concepts sequentially. Existing approaches primarily focus on minimizing interference between concepts, often neglecting the potential for positive inter-concept interactions. In this work, we present Forget Less by Learning from Parents (FLLP), a novel framework that introduces a parent-child inter-concept learning mechanism in hyperbolic space to mitigate forgetting. By embedding concept representations within a Lorentzian manifold, naturally suited to modeling tree-like hierarchies, we define parent-child relationships in which previously learned concepts serve as guidance for adapting to new ones. Our method not only preserves prior knowledge but also supports continual integration of new concepts. We validate FLLP on three public datasets and one synthetic benchmark, showing consistent improvements in both robustness and generalization.</li>
<li><strong>摘要：</strong>自定义扩散模型 (CDM) 在生成建模中提供了令人印象深刻的个性化功能，但在顺序学习新概念时，它们仍然容易遭受灾难性遗忘。现有的方法主要侧重于最大限度地减少概念之间的干扰，常常忽略概念间积极相互作用的潜力。在这项工作中，我们提出了通过向父母学习来减少遗忘（FLLP），这是一个新颖的框架，它在双曲空间中引入了亲子间概念学习机制以减轻遗忘。通过将概念表示嵌入自然适合树状层次结构建模的洛伦兹流形中，我们定义了父子关系，其中先前学习的概念可以作为适应新概念的指导。我们的方法不仅保留了先验知识，而且支持新概念的持续集成。我们在三个公共数据集和一个综合基准上验证 FLLP，显示出鲁棒性和泛化性方面的持续改进。</li>
</ul>

<h3>Title: AR-MOT: Autoregressive Multi-object Tracking</h3>
<ul>
<li><strong>Authors: </strong>Lianjie Jia, Yuhan Wu, Binghao Ran, Yifan Wang, Lijun Wang, Huchuan Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01925">https://arxiv.org/abs/2601.01925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01925">https://arxiv.org/pdf/2601.01925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01925]] AR-MOT: Autoregressive Multi-object Tracking(https://arxiv.org/abs/2601.01925)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>As multi-object tracking (MOT) tasks continue to evolve toward more general and multi-modal scenarios, the rigid and task-specific architectures of existing MOT methods increasingly hinder their applicability across diverse tasks and limit flexibility in adapting to new tracking formulations. Most approaches rely on fixed output heads and bespoke tracking pipelines, making them difficult to extend to more complex or instruction-driven tasks. To address these limitations, we propose AR-MOT, a novel autoregressive paradigm that formulates MOT as a sequence generation task within a large language model (LLM) framework. This design enables the model to output structured results through flexible sequence construction, without requiring any task-specific heads. To enhance region-level visual perception, we introduce an Object Tokenizer based on a pretrained detector. To mitigate the misalignment between global and regional features, we propose a Region-Aware Alignment (RAA) module, and to support long-term tracking, we design a Temporal Memory Fusion (TMF) module that caches historical object tokens. AR-MOT offers strong potential for extensibility, as new modalities or instructions can be integrated by simply modifying the output sequence format without altering the model architecture. Extensive experiments on MOT17 and DanceTrack validate the feasibility of our approach, achieving performance comparable to state-of-the-art methods while laying the foundation for more general and flexible MOT systems.</li>
<li><strong>摘要：</strong>随着多目标跟踪（MOT）任务继续向更通用和多模式场景发展，现有 MOT 方法的僵化和特定于任务的架构越来越阻碍其在不同任务中的适用性，并限制了适应新跟踪公式的灵活性。大多数方法依赖于固定输出头和定制跟踪管道，这使得它们难以扩展到更复杂或指令驱动的任务。为了解决这些限制，我们提出了 AR-MOT，一种新颖的自回归范式，它将 MOT 表述为大型语言模型 (LLM) 框架内的序列生成任务。这种设计使模型能够通过灵活的序列构造输出结构化结果，而不需要任何特定于任务的头。为了增强区域级视觉感知，我们引入了基于预训练检测器的对象标记器。为了减轻全局和区域特征之间的不一致，我们提出了区域感知对齐（RAA）模块，为了支持长期跟踪，我们设计了一个缓存历史对象标记的临时内存融合（TMF）模块。 AR-MOT 具有强大的可扩展性潜力，因为只需修改输出序列格式即可集成新的模式或指令，而无需更改模型架构。 MOT17 和 DanceTrack 上的大量实验验证了我们方法的可行性，实现了与最先进方法相当的性能，同时为更通用和灵活的 MOT 系统奠定了基础。</li>
</ul>

<h3>Title: SynRXN: An Open Benchmark and Curated Dataset for Computational Reaction Modeling</h3>
<ul>
<li><strong>Authors: </strong>Tieu-Long Phan, Nhu-Ngoc Nguyen Song, Peter F. Stadler</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01943">https://arxiv.org/abs/2601.01943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01943">https://arxiv.org/pdf/2601.01943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01943]] SynRXN: An Open Benchmark and Curated Dataset for Computational Reaction Modeling(https://arxiv.org/abs/2601.01943)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present SynRXN, a unified benchmarking framework and open-data resource for computer-aided synthesis planning (CASP). SynRXN decomposes end-to-end synthesis planning into five task families, covering reaction rebalancing, atom-to-atom mapping, reaction classification, reaction property prediction, and synthesis route design. Curated, provenance-tracked reaction corpora are assembled from heterogeneous public sources into a harmonized representation and packaged as versioned datasets for each task family, with explicit source metadata, licence tags, and machine-readable manifests that record checksums, and row counts. For every task, SynRXN provides transparent splitting functions that generate leakage-aware train, validation, and test partitions, together with standardized evaluation workflows and metric suites tailored to classification, regression, and structured prediction settings. For sensitive benchmarking, we combine public training and validation data with held-out gold-standard test sets, and contamination-prone tasks such as reaction rebalancing and atom-to-atom mapping are distributed only as evaluation sets and are explicitly not intended for model training. Scripted build recipes enable bitwise-reproducible regeneration of all corpora across machines and over time, and the entire resource is released under permissive open licences to support reuse and extension. By removing dataset heterogeneity and packaging transparent, reusable evaluation scaffolding, SynRXN enables fair longitudinal comparison of CASP methods, supports rigorous ablations and stress tests along the full reaction-informatics pipeline, and lowers the barrier for practitioners who seek robust and comparable performance estimates for real-world synthesis planning workloads.</li>
<li><strong>摘要：</strong>我们推出 SynRXN，这是一个用于计算机辅助综合规划 (CASP) 的统一基准测试框架和开放数据资源。 SynRXN将端到端合成规划分解为五个任务族，涵盖反应再平衡、原子到原子映射、反应分类、反应性质预测和合成路线设计。精心策划、追踪出处的反应语料库是从异构公共来源组装成统一的表示形式，并打包为每个任务系列的版本化数据集，并具有显式源元数据、许可证标签和记录校验和和行计数的机器可读清单。对于每项任务，SynRXN 都提供透明的拆分功能，可生成泄漏感知训练、验证和测试分区，以及针对分类、回归和结构化预测设置量身定制的标准化评估工作流程和指标套件。对于敏感的基准测试，我们将公共训练和验证数据与保留的黄金标准测试集相结合，而容易污染的任务（例如反应重新平衡和原子到原子映射）仅作为评估集分发，并且明确不用于模型训练。脚本化的构建配方可以跨机器、随着时间的推移对所有语料库进行按位再现的重新生成，并且整个资源在宽松的开放许可证下发布，以支持重用和扩展。通过消除数据集异质性和打包透明、可重用的评估支架，SynRXN 能够对 CASP 方法进行公平的纵向比较，支持整个反应信息学管道中严格的消融和压力测试，并为寻求对现实世界合成规划工作负载进行可靠且可比较的性能估计的从业者降低障碍。</li>
</ul>

<h3>Title: Forget Less by Learning Together through Concept Consolidation</h3>
<ul>
<li><strong>Authors: </strong>Arjun Ramesh Kaushik, Naresh Kumar Devulapally, Vishnu Suresh Lokhande, Nalini Ratha, Venu Govindaraju</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01963">https://arxiv.org/abs/2601.01963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01963">https://arxiv.org/pdf/2601.01963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01963]] Forget Less by Learning Together through Concept Consolidation(https://arxiv.org/abs/2601.01963)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Custom Diffusion Models (CDMs) have gained significant attention due to their remarkable ability to personalize generative processes. However, existing CDMs suffer from catastrophic forgetting when continuously learning new concepts. Most prior works attempt to mitigate this issue under the sequential learning setting with a fixed order of concept inflow and neglect inter-concept interactions. In this paper, we propose a novel framework - Forget Less by Learning Together (FL2T) - that enables concurrent and order-agnostic concept learning while addressing catastrophic forgetting. Specifically, we introduce a set-invariant inter-concept learning module where proxies guide feature selection across concepts, facilitating improved knowledge retention and transfer. By leveraging inter-concept guidance, our approach preserves old concepts while efficiently incorporating new ones. Extensive experiments, across three datasets, demonstrates that our method significantly improves concept retention and mitigates catastrophic forgetting, highlighting the effectiveness of inter-concept catalytic behavior in incremental concept learning of ten tasks with at least 2% gain on average CLIP Image Alignment scores.</li>
<li><strong>摘要：</strong>定制扩散模型（CDM）由于其个性化生成过程的卓越能力而受到广泛关注。然而，现有的 CDM 在不断学习新概念时会遭受灾难性的遗忘。大多数先前的工作试图在顺序学习环境下通过固定的概念流入顺序来缓解这个问题，并忽略概念间的相互作用。在本文中，我们提出了一个新颖的框架 - 通过共同学习减少遗忘（FL2T） - 该框架能够实现并发且与顺序无关的概念学习，同时解决灾难性遗忘问题。具体来说，我们引入了一个集合不变的概念间学习模块，其中代理指导跨概念的特征选择，促进改进的知识保留和迁移。通过利用概念间指导，我们的方法保留了旧概念，同时有效地融入了新概念。跨越三个数据集的大量实验表明，我们的方法显着提高了概念保留并减轻了灾难性遗忘，突出了十个任务的增量概念学习中概念间催化行为的有效性，平均 CLIP 图像对齐分数至少提高了 2%。</li>
</ul>

<h3>Title: SerpentFlow: Generative Unpaired Domain Alignment via Shared-Structure Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Julie Keisler (ARCHES), Anastase Alexandre Charantonis (ARCHES), Yannig Goude (EDF R\&amp;D OSIRIS, LMO), Boutheina Oueslati (EDF R\&amp;D OSIRIS), Claire Monteleoni (ARCHES)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01979">https://arxiv.org/abs/2601.01979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01979">https://arxiv.org/pdf/2601.01979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01979]] SerpentFlow: Generative Unpaired Domain Alignment via Shared-Structure Decomposition(https://arxiv.org/abs/2601.01979)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generative</a></li>
<li><strong>Abstract: </strong>Domain alignment refers broadly to learning correspondences between data distributions from distinct domains. In this work, we focus on a setting where domains share underlying structural patterns despite differences in their specific realizations. The task is particularly challenging in the absence of paired observations, which removes direct supervision across domains. We introduce a generative framework, called SerpentFlow (SharEd-structuRe decomPosition for gEnerative domaiN adapTation), for unpaired domain alignment. SerpentFlow decomposes data within a latent space into a shared component common to both domains and a domain-specific one. By isolating the shared structure and replacing the domain-specific component with stochastic noise, we construct synthetic training pairs between shared representations and target-domain samples, thereby enabling the use of conditional generative models that are traditionally restricted to paired settings. We apply this approach to super-resolution tasks, where the shared component naturally corresponds to low-frequency content while high-frequency details capture domain-specific variability. The cutoff frequency separating low- and high-frequency components is determined automatically using a classifier-based criterion, ensuring a data-driven and domain-adaptive decomposition. By generating pseudo-pairs that preserve low-frequency structures while injecting stochastic high-frequency realizations, we learn the conditional distribution of the target domain given the shared representation. We implement SerpentFlow using Flow Matching as the generative pipeline, although the framework is compatible with other conditional generative approaches. Experiments on synthetic images, physical process simulations, and a climate downscaling task demonstrate that the method effectively reconstructs high-frequency structures consistent with underlying low-frequency patterns, supporting shared-structure decomposition as an effective strategy for unpaired domain alignment.</li>
<li><strong>摘要：</strong>域对齐广泛地指学习来自不同域的数据分布之间的对应关系。在这项工作中，我们重点关注域共享底层结构模式的设置，尽管它们的具体实现存在差异。在缺乏配对观察的情况下，这项任务尤其具有挑战性，这消除了跨领域的直接监督。我们引入了一个生成框架，称为 SerpentFlow（用于生成域适应的共享结构分解），用于不配对域对齐。 SerpentFlow 将潜在空间内的数据分解为两个域和特定于域的共享组件。通过隔离共享结构并用随机噪声替换特定于域的组件，我们在共享表示和目标域样本之间构建合成训练对，从而能够使用传统上仅限于配对设置的条件生成模型。我们将这种方法应用于超分辨率任务，其中共享组件自然对应于低频内容，而高频细节捕获特定于域的可变性。使用基于分类器的标准自动确定分离低频和高频分量的截止频率，确保数据驱动和域自适应分解。通过生成保留低频结构的伪对，同时注入随机高频实现，我们学习了给定共享表示的目标域的条件分布。我们使用流匹配作为生成管道来实现 SerpentFlow，尽管该框架与其他条件生成方法兼容。合成图像、物理过程模拟和气候降尺度任务的实验表明，该方法有效地重建了与底层低频模式一致的高频结构，支持共享结构分解作为不成对域对齐的有效策略。</li>
</ul>

<h3>Title: API: Empowering Generalizable Real-World Image Dehazing via Adaptive Patch Importance Learning</h3>
<ul>
<li><strong>Authors: </strong>Chen Zhu, Huiwen Zhang, Yujie Li, Mu He, Xiaotian Qiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01992">https://arxiv.org/abs/2601.01992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01992">https://arxiv.org/pdf/2601.01992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01992]] API: Empowering Generalizable Real-World Image Dehazing via Adaptive Patch Importance Learning(https://arxiv.org/abs/2601.01992)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Real-world image dehazing is a fundamental yet challenging task in low-level vision. Existing learning-based methods often suffer from significant performance degradation when applied to complex real-world hazy scenes, primarily due to limited training data and the intrinsic complexity of haze density this http URL address these challenges, we introduce a novel Adaptive Patch Importance-aware (API) framework for generalizable real-world image dehazing. Specifically, our framework consists of an Automatic Haze Generation (AHG) module and a Density-aware Haze Removal (DHR) module. AHG provides a hybrid data augmentation strategy by generating realistic and diverse hazy images as additional high-quality training data. DHR considers hazy regions with varying haze density distributions for generalizable real-world image dehazing in an adaptive patch importance-aware manner. To alleviate the ambiguity of the dehazed image details, we further introduce a new Multi-Negative Contrastive Dehazing (MNCD) loss, which fully utilizes information from multiple negative samples across both spatial and frequency domains. Extensive experiments demonstrate that our framework achieves state-of-the-art performance across multiple real-world benchmarks, delivering strong results in both quantitative metrics and qualitative visual quality, and exhibiting robust generalization across diverse haze distributions.</li>
<li><strong>摘要：</strong>现实世界的图像去雾是低级视觉中一项基本但具有挑战性的任务。现有的基于学习的方法在应用于复杂的现实世界雾霾场景时通常会遭受显着的性能下降，这主要是由于有限的训练数据和雾霾密度的内在复杂性。这个http URL解决了这些挑战，我们引入了一种新颖的自适应补丁重要性感知（API）框架，用于通用的现实世界图像去雾。具体来说，我们的框架由自动雾霾生成（AHG）模块和密度感知雾霾去除（DHR）模块组成。 AHG 通过生成真实且多样化的模糊图像作为额外的高质量训练数据，提供了混合数据增强策略。 DHR 考虑具有不同雾度密度分布的雾度区域，以自适应补丁重要性感知方式进行通用的现实世界图像去雾。为了减轻去雾图像细节的模糊性，我们进一步引入了一种新的多负对比去雾（MNCD）损失，它充分利用了跨空间和频域的多个负样本的信息。大量的实验表明，我们的框架在多个现实世界基准中实现了最先进的性能，在定量指标和定性视觉质量方面均提供了强劲的结果，并在不同的雾霾分布中表现出强大的泛化能力。</li>
</ul>

<h3>Title: Nighttime Hazy Image Enhancement via Progressively and Mutually Reinforcing Night-Haze Priors</h3>
<ul>
<li><strong>Authors: </strong>Chen Zhu, Huiwen Zhang, Mu He, Yujie Li, Xiaotian Qiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.01998">https://arxiv.org/abs/2601.01998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.01998">https://arxiv.org/pdf/2601.01998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.01998]] Nighttime Hazy Image Enhancement via Progressively and Mutually Reinforcing Night-Haze Priors(https://arxiv.org/abs/2601.01998)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Enhancing the visibility of nighttime hazy images is challenging due to the complex degradation distributions. Existing methods mainly address a single type of degradation (e.g., haze or low-light) at a time, ignoring the interplay of different degradation types and resulting in limited visibility improvement. We observe that the domain knowledge shared between low-light and haze priors can be reinforced mutually for better visibility. Based on this key insight, in this paper, we propose a novel framework that enhances visibility in nighttime hazy images by reinforcing the intrinsic consistency between haze and low-light priors mutually and progressively. In particular, our model utilizes image-, patch-, and pixel-level experts that operate across visual and frequency domains to recover global scene structure, regional patterns, and fine-grained details progressively. A frequency-aware router is further introduced to adaptively guide the contribution of each expert, ensuring robust image restoration. Extensive experiments demonstrate the superior performance of our model on nighttime dehazing benchmarks both quantitatively and qualitatively. Moreover, we showcase the generalizability of our model in daytime dehazing and low-light enhancement tasks.</li>
<li><strong>摘要：</strong>由于复杂的退化分布，增强夜间模糊图像的可见度具有挑战性。现有方法主要一次解决单一类型的退化（例如雾霾或低光），忽略了不同退化类型的相互作用，导致能见度改善有限。我们观察到，低光和雾霾先验之间共享的领域知识可以相互加强，以获得更好的可见性。基于这一关键见解，在本文中，我们提出了一种新颖的框架，通过逐步增强雾霾和低光先验之间的内在一致性来增强夜间雾霾图像的可见度。特别是，我们的模型利用跨视觉和频域操作的图像、补丁和像素级专家来逐步恢复全局场景结构、区域模式和细粒度细节。进一步引入频率感知路由器来自适应地指导每个专家的贡献，确保稳健的图像恢复。大量的实验在定量和定性方面证明了我们的模型在夜间除雾基准上的卓越性能。此外，我们展示了我们的模型在白天除雾和弱光增强任务中的通用性。</li>
</ul>

<h3>Title: Towards Any-Quality Image Segmentation via Generative and Adaptive Latent Space Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Guangqian Guo, Aixi Ren, Yong Guo, Xuehui Yu, Jiacheng Tian, Wenli Li, Yaoxing Wang, Shan Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02018">https://arxiv.org/abs/2601.02018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02018">https://arxiv.org/pdf/2601.02018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02018]] Towards Any-Quality Image Segmentation via Generative and Adaptive Latent Space Enhancement(https://arxiv.org/abs/2601.02018)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Segment Anything Models (SAMs), known for their exceptional zero-shot segmentation performance, have garnered significant attention in the research community. Nevertheless, their performance drops significantly on severely degraded, low-quality images, limiting their effectiveness in real-world scenarios. To address this, we propose GleSAM++, which utilizes Generative Latent space Enhancement to boost robustness on low-quality images, thus enabling generalization across various image qualities. Additionally, to improve compatibility between the pre-trained diffusion model and the segmentation framework, we introduce two techniques, i.e., Feature Distribution Alignment (FDA) and Channel Replication and Expansion (CRE). However, the above components lack explicit guidance regarding the degree of degradation. The model is forced to implicitly fit a complex noise distribution that spans conditions from mild noise to severe artifacts, which substantially increases the learning burden and leads to suboptimal reconstructions. To address this issue, we further introduce a Degradation-aware Adaptive Enhancement (DAE) mechanism. The key principle of DAE is to decouple the reconstruction process for arbitrary-quality features into two stages: degradation-level prediction and degradation-aware reconstruction. Our method can be applied to pre-trained SAM and SAM2 with only minimal additional learnable parameters, allowing for efficient optimization. Extensive experiments demonstrate that GleSAM++ significantly improves segmentation robustness on complex degradations while maintaining generalization to clear images. Furthermore, GleSAM++ also performs well on unseen degradations, underscoring the versatility of our approach and dataset.</li>
<li><strong>摘要：</strong>分段任意模型 (SAM) 以其卓越的零样本分割性能而闻名，已引起研究界的广泛关注。然而，它们的性能在严重退化、低质量的图像上显着下降，限制了它们在现实场景中的有效性。为了解决这个问题，我们提出了 GleSAM++，它利用生成潜在空间增强来提高低质量图像的鲁棒性，从而实现各种图像质量的泛化。此外，为了提高预训练扩散模型和分割框架之间的兼容性，我们引入了两种技术，即特征分布对齐（FDA）和通道复制和扩展（CRE）。然而，上述组件缺乏关于降解程度的明确指导。该模型被迫隐式拟合复杂的噪声分布，涵盖从轻微噪声到严重伪影的各种条件，这大大增加了学习负担并导致次优重建。为了解决这个问题，我们进一步引入了降级感知自适应增强（DAE）机制。 DAE 的关键原理是将任意质量特征的重建过程解耦为两个阶段：退化级别预测和退化感知重建。我们的方法可以应用于预训练的 SAM 和 SAM2，只需最少的额外可学习参数，从而实现高效优化。大量实验表明，GleSAM++ 显着提高了复杂退化的分割鲁棒性，同时保持了对清晰图像的泛化。此外，GleSAM++ 在看不见的退化方面也表现良好，强调了我们的方法和数据集的多功能性。</li>
</ul>

<h3>Title: AlignVTOFF: Texture-Spatial Feature Alignment for High-Fidelity Virtual Try-Off</h3>
<ul>
<li><strong>Authors: </strong>Yihan Zhu, Mengying Ge</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02038">https://arxiv.org/abs/2601.02038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02038">https://arxiv.org/pdf/2601.02038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02038]] AlignVTOFF: Texture-Spatial Feature Alignment for High-Fidelity Virtual Try-Off(https://arxiv.org/abs/2601.02038)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Virtual Try-Off (VTOFF) is a challenging multimodal image generation task that aims to synthesize high-fidelity flat-lay garments under complex geometric deformation and rich high-frequency textures. Existing methods often rely on lightweight modules for fast feature extraction, which struggles to preserve structured patterns and fine-grained details, leading to texture attenuation during this http URL address these issues, we propose AlignVTOFF, a novel parallel U-Net framework built upon a Reference U-Net and Texture-Spatial Feature Alignment (TSFA). The Reference U-Net performs multi-scale feature extraction and enhances geometric fidelity, enabling robust modeling of deformation while retaining complex structured patterns. TSFA then injects the reference garment features into a frozen denoising U-Net via a hybrid attention design, consisting of a trainable cross-attention module and a frozen self-attention module. This design explicitly aligns texture and spatial cues and alleviates the loss of high-frequency information during the denoising this http URL experiments across multiple settings demonstrate that AlignVTOFF consistently outperforms state-of-the-art methods, producing flat-lay garment results with improved structural realism and high-frequency detail fidelity.</li>
<li><strong>摘要：</strong>虚拟试穿（VTOFF）是一项具有挑战性的多模态图像生成任务，旨在在复杂的几何变形和丰富的高频纹理下合成高保真平铺服装。现有方法通常依赖于轻量级模块来进行快速特征提取，这很难保留结构化模式和细粒度细节，从而导致在此 http URL 期间纹理衰减。为了解决这些问题，我们提出了 AlignVTOFF，这是一种基于参考 U-Net 和纹理空间特征对齐 (TSFA) 构建的新型并行 U-Net 框架。 Reference U-Net 执行多尺度特征提取并增强几何保真度，实现稳健的变形建模，同时保留复杂的结构化模式。然后，TSFA 通过混合注意力设计将参考服装特征注入冻结去噪 U-Net，该设计由可训练的交叉注意力模块和冻结自注意力模块组成。这种设计明确地对齐纹理和空间线索，并减少了去噪过程​​中高频信息的丢失。跨多个设置的 http URL 实验表明，AlignVTOFF 始终优于最先进的方法，产生具有改进的结构真实性和高频细节保真度的平铺服装结果。</li>
</ul>

<h3>Title: Agentic Retoucher for Text-To-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Shaocheng Shen, Jianfeng Liang. Chunlei Cai, Cong Geng, Huiyu Duan, Xiaoyun Zhang, Qiang Hu, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02046">https://arxiv.org/abs/2601.02046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02046">https://arxiv.org/pdf/2601.02046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02046]] Agentic Retoucher for Text-To-Image Generation(https://arxiv.org/abs/2601.02046)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) diffusion models such as SDXL and FLUX have achieved impressive photorealism, yet small-scale distortions remain pervasive in limbs, face, text and so on. Existing refinement approaches either perform costly iterative re-generation or rely on vision-language models (VLMs) with weak spatial grounding, leading to semantic drift and unreliable local edits. To close this gap, we propose Agentic Retoucher, a hierarchical decision-driven framework that reformulates post-generation correction as a human-like perception-reasoning-action loop. Specifically, we design (1) a perception agent that learns contextual saliency for fine-grained distortion localization under text-image consistency cues, (2) a reasoning agent that performs human-aligned inferential diagnosis via progressive preference alignment, and (3) an action agent that adaptively plans localized inpainting guided by user preference. This design integrates perceptual evidence, linguistic reasoning, and controllable correction into a unified, self-corrective decision process. To enable fine-grained supervision and quantitative evaluation, we further construct GenBlemish-27K, a dataset of 6K T2I images with 27K annotated artifact regions across 12 categories. Extensive experiments demonstrate that Agentic Retoucher consistently outperforms state-of-the-art methods in perceptual quality, distortion localization and human preference alignment, establishing a new paradigm for self-corrective and perceptually reliable T2I generation.</li>
<li><strong>摘要：</strong>SDXL 和 FLUX 等文本到图像 (T2I) 扩散模型已经实现了令人印象深刻的照片级真实感，但四肢、面部、文本等中仍然普遍存在小规模扭曲。现有的细化方法要么执行成本高昂的迭代重新生成，要么依赖空间基础薄弱的视觉语言模型（VLM），导致语义漂移和不可靠的本地编辑。为了缩小这一差距，我们提出了 Agentic Retoucher，这是一种分层决策驱动框架，它将生成后校正重新表述为类人感知-推理-行动循环。具体来说，我们设计了（1）一个感知代理，它在文本图像一致性提示下学习上下文显着性，以实现细粒度的失真定位；（2）一个推理代理，通过渐进式偏好对齐执行与人类一致的推理诊断；（3）一个动作代理，根据用户偏好指导自适应地规划局部修复。这种设计将感知证据、语言推理和可控纠正整合到一个统一的、自我纠正的决策过程中。为了实现细粒度监督和定量评估，我们进一步构建了 GenBlemish-27K，这是一个 6K T2I 图像的数据集，其中包含 12 个类别的 27K 个带注释的伪影区域。大量实验表明，Agentic Retoucher 在感知质量、失真定位和人类偏好调整方面始终优于最先进的方法，为自我校正和感知可靠的 T2I 生成建立了新的范例。</li>
</ul>

<h3>Title: HeadLighter: Disentangling Illumination in Generative 3D Gaussian Heads via Lightstage Captures</h3>
<ul>
<li><strong>Authors: </strong>Yating Wang, Yuan Sun, Xuan Wang, Ran Yi, Boyao Zhou, Yipengjing Sun, Hongyu Liu, Yinuo Wang, Lizhuang Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02103">https://arxiv.org/abs/2601.02103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02103">https://arxiv.org/pdf/2601.02103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02103]] HeadLighter: Disentangling Illumination in Generative 3D Gaussian Heads via Lightstage Captures(https://arxiv.org/abs/2601.02103)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent 3D-aware head generative models based on 3D Gaussian Splatting achieve real-time, photorealistic and view-consistent head synthesis. However, a fundamental limitation persists: the deep entanglement of illumination and intrinsic appearance prevents controllable relighting. Existing disentanglement methods rely on strong assumptions to enable weakly supervised learning, which restricts their capacity for complex illumination. To address this challenge, we introduce HeadLighter, a novel supervised framework that learns a physically plausible decomposition of appearance and illumination in head generative models. Specifically, we design a dual-branch architecture that separately models lighting-invariant head attributes and physically grounded rendering components. A progressive disentanglement training is employed to gradually inject head appearance priors into the generative architecture, supervised by multi-view images captured under controlled light conditions with a light stage setup. We further introduce a distillation strategy to generate high-quality normals for realistic rendering. Experiments demonstrate that our method preserves high-quality generation and real-time rendering, while simultaneously supporting explicit lighting and viewpoint editing. We will publicly release our code and dataset.</li>
<li><strong>摘要：</strong>最近基于 3D Gaussian Splatting 的 3D 感知头部生成模型实现了实时、真实感和视图一致的头部合成。然而，一个根本的限制仍然存在：照明和内在外观的深度纠缠阻碍了可控的重新照明。现有的解缠结方法依赖于强假设来实现弱监督学习，这限制了它们复杂照明的能力。为了应对这一挑战，我们引入了 HeadLighter，这是一种新颖的监督框架，可以学习头部生成模型中外观和照明的物理合理分解。具体来说，我们设计了一个双分支架构，分别对光照不变的头部属性和物理接地的渲染组件进行建模。采用渐进式解缠结训练将头部外观先验逐渐注入生成架构中，并通过在受控光照条件下使用光舞台设置捕获的多视图图像进行监督。我们进一步引入了一种蒸馏策略来生成用于真实渲染的高质量法线。实验表明，我们的方法保留了高质量的生成和实时渲染，同时支持显式照明和视点编辑。我们将公开发布我们的代码和数据集。</li>
</ul>

<h3>Title: MagicFight: Personalized Martial Arts Combat Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiancheng Huang, Mingfu Yan, Songyan Chen, Yi Huang, Shifeng Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02107">https://arxiv.org/abs/2601.02107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02107">https://arxiv.org/pdf/2601.02107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02107]] MagicFight: Personalized Martial Arts Combat Video Generation(https://arxiv.org/abs/2601.02107)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Amid the surge in generic text-to-video generation, the field of personalized human video generation has witnessed notable advancements, primarily concentrated on single-person scenarios. However, to our knowledge, the domain of two-person interactions, particularly in the context of martial arts combat, remains uncharted. We identify a significant gap: existing models for single-person dancing generation prove insufficient for capturing the subtleties and complexities of two engaged fighters, resulting in challenges such as identity confusion, anomalous limbs, and action mismatches. To address this, we introduce a pioneering new task, Personalized Martial Arts Combat Video Generation. Our approach, MagicFight, is specifically crafted to overcome these hurdles. Given this pioneering task, we face a lack of appropriate datasets. Thus, we generate a bespoke dataset using the game physics engine Unity, meticulously crafting a multitude of 3D characters, martial arts moves, and scenes designed to represent the diversity of combat. MagicFight refines and adapts existing models and strategies to generate high-fidelity two-person combat videos that maintain individual identities and ensure seamless, coherent action sequences, thereby laying the groundwork for future innovations in the realm of interactive video content creation. Website: this https URL Dataset: this https URL</li>
<li><strong>摘要：</strong>在通用文本到视频生成的激增中，个性化人类视频生成领域取得了显着的进步，主要集中在单人场景上。然而，据我们所知，两人互动的领域，特别是在武术格斗的背景下，仍然是未知的。我们发现了一个重大差距：现有的单人舞蹈生成模型不足以捕捉两名交战战士的微妙性和复杂性，从而导致身份混乱、肢体异常和动作不匹配等挑战。为了解决这个问题，我们引入了一项开创性的新任务：个性化武术战斗视频生成。我们的方法 MagicFight 是专门为克服这些障碍而设计的。鉴于这项开创性任务，我们面临着缺乏适当的数据集的问题。因此，我们使用游戏物理引擎 Unity 生成定制数据集，精心制作大量 3D 角色、武术动作和场景，旨在表现战斗的多样性。 MagicFight 改进并调整了现有的模型和策略，以生成高保真两人战斗视频，保持个人身份并确保无缝、连贯的动作序列，从而为交互式视频内容创建领域的未来创新奠定基础。网站：此 https URL 数据集：此 https URL</li>
</ul>

<h3>Title: Remote Sensing Change Detection via Weak Temporal Supervision</h3>
<ul>
<li><strong>Authors: </strong>Xavier Bou, Elliot Vincent, Gabriele Facciolo, Rafael Grompone von Gioi, Jean-Michel Morel, Thibaud Ehret</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02126">https://arxiv.org/abs/2601.02126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02126">https://arxiv.org/pdf/2601.02126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02126]] Remote Sensing Change Detection via Weak Temporal Supervision(https://arxiv.org/abs/2601.02126)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Semantic change detection in remote sensing aims to identify land cover changes between bi-temporal image pairs. Progress in this area has been limited by the scarcity of annotated datasets, as pixel-level annotation is costly and time-consuming. To address this, recent methods leverage synthetic data or generate artificial change pairs, but out-of-domain generalization remains limited. In this work, we introduce a weak temporal supervision strategy that leverages additional temporal observations of existing single-temporal datasets, without requiring any new annotations. Specifically, we extend single-date remote sensing datasets with new observations acquired at different times and train a change detection model by assuming that real bi-temporal pairs mostly contain no change, while pairing images from different locations to generate change examples. To handle the inherent noise in these weak labels, we employ an object-aware change map generation and an iterative refinement process. We validate our approach on extended versions of the FLAIR and IAILD aerial datasets, achieving strong zero-shot and low-data regime performance across different benchmarks. Lastly, we showcase results over large areas in France, highlighting the scalability potential of our method.</li>
<li><strong>摘要：</strong>遥感中的语义变化检测旨在识别双时态图像对之间的土地覆盖变化。该领域的进展受到注释数据集稀缺的限制，因为像素级注释成本高昂且耗时。为了解决这个问题，最近的方法利用合成数据或生成人工变化对，但域外泛化仍然有限。在这项工作中，我们引入了一种弱时间监督策略，该策略利用现有单时间数据集的额外时间观察，而不需要任何新的注释。具体来说，我们用不同时间获取的新观测值扩展单日期遥感数据集，并通过假设真实的双时态对大多不包含变化来训练变化检测模型，同时将来自不同位置的图像配对以生成变化示例。为了处理这些弱标签中的固有噪声，我们采用了对象感知的变化图生成和迭代细化过程。我们在 FLAIR 和 IAILD 航空数据集的扩展版本上验证了我们的方法，在不同的基准上实现了强大的零样本和低数据状态性能。最后，我们展示了法国大片地区的结果，强调了我们方法的可扩展性潜力。</li>
</ul>

<h3>Title: Parameter-Efficient Domain Adaption for CSI Crowd-Counting via Self-Supervised Learning with Adapter Modules</h3>
<ul>
<li><strong>Authors: </strong>Oliver Custance, Saad Khan, Simon Parkinson, Quan Z. Sheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02203">https://arxiv.org/abs/2601.02203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02203">https://arxiv.org/pdf/2601.02203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02203]] Parameter-Efficient Domain Adaption for CSI Crowd-Counting via Self-Supervised Learning with Adapter Modules(https://arxiv.org/abs/2601.02203)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Device-free crowd-counting using WiFi Channel State Information (CSI) is a key enabling technology for a new generation of privacy-preserving Internet of Things (IoT) applications. However, practical deployment is severely hampered by the domain shift problem, where models trained in one environment fail to generalise to another. To overcome this, we propose a novel two-stage framework centred on a CSI-ResNet-A architecture. This model is pre-trained via self-supervised contrastive learning to learn domain-invariant representations and leverages lightweight Adapter modules for highly efficient fine-tuning. The resulting event sequence is then processed by a stateful counting machine to produce a final, stable occupancy estimate. We validate our framework extensively. On our WiFlow dataset, our unsupervised approach excels in a 10-shot learning scenario, achieving a final Mean Absolute Error (MAE) of just 0.44--a task where supervised baselines fail. To formally quantify robustness, we introduce the Generalisation Index (GI), on which our model scores near-perfectly, confirming its ability to generalise. Furthermore, our framework sets a new state-of-the-art public WiAR benchmark with 98.8\% accuracy. Our ablation studies reveal the core strength of our design: adapter-based fine-tuning achieves performance within 1\% of a full fine-tune (98.84\% vs. 99.67\%) while training 97.2\% fewer parameters. Our work provides a practical and scalable solution for developing robust sensing systems ready for real-world IoT deployments.</li>
<li><strong>摘要：</strong>使用 WiFi 信道状态信息 (CSI) 进行无设备人群计数是新一代隐私保护物联网 (IoT) 应用的关键支持技术。然而，实际部署受到域转移问题的严重阻碍，在一种环境中训练的模型无法推广到另一种环境。为了克服这个问题，我们提出了一种以 CSI-ResNet-A 架构为中心的新型两阶段框架。该模型通过自监督对比学习进行预训练，以学习域不变表示，并利用轻量级适配器模块进行高效微调。然后，产生的事件序列由状态计数机处理，以产生最终的、稳定的占用率估计。我们广泛验证我们的框架。在我们的 WiFlow 数据集上，我们的无监督方法在 10 次学习场景中表现出色，最终平均绝对误差 (MAE) 仅为 0.44，这是监督基线失败的任务。为了正式量化稳健性，我们引入了泛化指数（GI），我们的模型在该指数上得分近乎完美，证实了其泛化能力。此外，我们的框架设定了新的最先进的公共 WiAR 基准，准确度为 98.8%。我们的消融研究揭示了我们设计的核心优势：基于适配器的微调实现了完全微调 1% 以内的性能（98.84% 与 99.67%），同时训练的参数减少了 97.2%。我们的工作提供了实用且可扩展的解决方案，用于开发强大的传感系统，为现实世界的物联网部署做好准备。</li>
</ul>

<h3>Title: NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation</h3>
<ul>
<li><strong>Authors: </strong>Huichao Zhang, Liao Qu, Yiheng Liu, Hang Chen, Yangyang Song, Yongsheng Dong, Shikun Sun, Xian Li, Xu Wang, Yi Jiang, Hu Ye, Bo Chen, Yiming Gao, Peng Liu, Akide Liu, Zhipeng Yang, Qili Deng, Linjie Xing, Jiyang Liu, Zhao Wang, Yang Zhou, Mingcong Liu, Yi Zhang, Qian He, Xiwei Hu, Zhongqi Qi, Jie Shao, Zhiye Fu, Shuai Wang, Fangmin Chen, Xuezhi Chai, Zhihua Wu, Yitong Wang, Zehuan Yuan, Daniel K. Du, Xinglong Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02204">https://arxiv.org/abs/2601.02204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02204">https://arxiv.org/pdf/2601.02204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02204]] NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation(https://arxiv.org/abs/2601.02204)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.</li>
<li><strong>摘要：</strong>我们提出了 NextFlow，一个统一的仅解码器自回归变压器，在 6 万亿个交错的文本图像离散标记上进行训练。通过利用统一自回归架构中的统一视觉表示，NextFlow 原生激活多模态理解和生成功能，解锁图像编辑、交错内容和视频生成的能力。受模态独特性质的启发（其中文本是严格顺序的，而图像本质上是分层的），我们保留文本的下一个标记预测，但采用下一个尺度预测进行视觉生成。这与传统的光栅扫描方法不同，只需 5 秒即可生成 1024x1024 图像，比同类 AR 模型快几个数量级。我们通过强大的训练方法解决多尺度生成的不稳定性。此外，我们引入了强化学习的前缀调整策略。实验表明，NextFlow 在统一模型中实现了最先进的性能，并且在视觉质量方面可与专门的扩散基线相媲美。</li>
</ul>

<h3>Title: Seeing the Unseen: Zooming in the Dark with Event Cameras</h3>
<ul>
<li><strong>Authors: </strong>Dachun Kai, Zeyu Xiao, Huyue Zhu, Jiaxiao Wang, Yueyi Zhang, Xiaoyan Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02206">https://arxiv.org/abs/2601.02206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02206">https://arxiv.org/pdf/2601.02206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02206]] Seeing the Unseen: Zooming in the Dark with Event Cameras(https://arxiv.org/abs/2601.02206)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>This paper addresses low-light video super-resolution (LVSR), aiming to restore high-resolution videos from low-light, low-resolution (LR) inputs. Existing LVSR methods often struggle to recover fine details due to limited contrast and insufficient high-frequency information. To overcome these challenges, we present RetinexEVSR, the first event-driven LVSR framework that leverages high-contrast event signals and Retinex-inspired priors to enhance video quality under low-light scenarios. Unlike previous approaches that directly fuse degraded signals, RetinexEVSR introduces a novel bidirectional cross-modal fusion strategy to extract and integrate meaningful cues from noisy event data and degraded RGB frames. Specifically, an illumination-guided event enhancement module is designed to progressively refine event features using illumination maps derived from the Retinex model, thereby suppressing low-light artifacts while preserving high-contrast details. Furthermore, we propose an event-guided reflectance enhancement module that utilizes the enhanced event features to dynamically recover reflectance details via a multi-scale fusion mechanism. Experimental results show that our RetinexEVSR achieves state-of-the-art performance on three datasets. Notably, on the SDSD benchmark, our method can get up to 2.95 dB gain while reducing runtime by 65% compared to prior event-based methods. Code: this https URL.</li>
<li><strong>摘要：</strong>本文讨论低光视频超分辨率 (LVSR)，旨在从低光、低分辨率 (LR) 输入恢复高分辨率视频。由于对比度有限和高频信息不足，现有的 LVSR 方法通常难以恢复精细细节。为了克服这些挑战，我们推出了 RetinexEVSR，这是第一个事件驱动的 LVSR 框架，它利用高对比度事件信号和受 Retinex 启发的先验来提高弱光场景下的视频质量。与之前直接融合降级信号的方法不同，RetinexEVSR 引入了一种新颖的双向跨模态融合策略，从噪声事件数据和降级 RGB 帧中提取并集成有意义的线索。具体来说，照明引导事件增强模块旨在使用从 Retinex 模型导出的照明图逐步细化事件特征，从而抑制低光伪影，同时保留高对比度细节。此外，我们提出了一种事件引导的反射率增强模块，该模块利用增强的事件特征通过多尺度融合机制动态恢复反射率细节。实验结果表明，我们的 RetinexEVSR 在三个数据集上实现了最先进的性能。值得注意的是，在 SDSD 基准上，与之前基于事件的方法相比，我们的方法可以获得高达 2.95 dB 的增益，同时将运行时间缩短 65%。代码：此 https URL。</li>
</ul>

<h3>Title: Unraveling MMDiT Blocks: Training-free Analysis and Enhancement of Text-conditioned Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Binglei Li, Mengping Yang, Zhiyu Tan, Junping Zhang, Hao Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02211">https://arxiv.org/abs/2601.02211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02211">https://arxiv.org/pdf/2601.02211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02211]] Unraveling MMDiT Blocks: Training-free Analysis and Enhancement of Text-conditioned Diffusion(https://arxiv.org/abs/2601.02211)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent breakthroughs of transformer-based diffusion models, particularly with Multimodal Diffusion Transformers (MMDiT) driven models like FLUX and Qwen Image, have facilitated thrilling experiences in text-to-image generation and editing. To understand the internal mechanism of MMDiT-based models, existing methods tried to analyze the effect of specific components like positional encoding and attention layers. Yet, a comprehensive understanding of how different blocks and their interactions with textual conditions contribute to the synthesis process remains elusive. In this paper, we first develop a systematic pipeline to comprehensively investigate each block's functionality by removing, disabling and enhancing textual hidden-states at corresponding blocks. Our analysis reveals that 1) semantic information appears in earlier blocks and finer details are rendered in later blocks, 2) removing specific blocks is usually less disruptive than disabling text conditions, and 3) enhancing textual conditions in selective blocks improves semantic attributes. Building on these observations, we further propose novel training-free strategies for improved text alignment, precise editing, and acceleration. Extensive experiments demonstrated that our method outperforms various baselines and remains flexible across text-to-image generation, image editing, and inference acceleration. Our method improves T2I-Combench++ from 56.92% to 63.00% and GenEval from 66.42% to 71.63% on SD3.5, without sacrificing synthesis quality. These results advance understanding of MMDiT models and provide valuable insights to unlock new possibilities for further improvements.</li>
<li><strong>摘要：</strong>基于变压器的扩散模型的最新突破，特别是多模态扩散变压器 (MMDiT) 驱动的模型（如 FLUX 和 Qwen Image），促进了文本到图像生成和编辑方面令人兴奋的体验。为了理解基于 MMDiT 的模型的内部机制，现有方法试图分析位置编码和注意力层等特定组件的效果。然而，对不同块及其与文本条件的相互作用如何促进合成过程的全面理解仍然难以实现。在本文中，我们首先开发一个系统管道，通过删除、禁用和增强相应块的文本隐藏状态来全面研究每个块的功能。我们的分析表明，1）语义信息出现在较早的块中，更精细的细节在后面的块中呈现，2）删除特定块通常比禁用文本条件的破坏性更小，3）增强选择性块中的文本条件可以改善语义属性。基于这些观察，我们进一步提出了新颖的免训练策略，以改进文本对齐、精确编辑和加速。大量的实验表明，我们的方法优于各种基线，并且在文本到图像生成、图像编辑和推理加速方面保持灵活性。我们的方法在 SD3.5 上将 T2I-Combench++ 从 56.92% 提高到 63.00%，将 GenEval 从 66.42% 提高到 71.63%，而不会牺牲合成质量。这些结果促进了对 MMDiT 模型的理解，并提供了宝贵的见解，以释放进一步改进的新可能性。</li>
</ul>

<h3>Title: VIBE: Visual Instruction Based Editor</h3>
<ul>
<li><strong>Authors: </strong>Grigorii Alekseenko, Aleksandr Gordeev, Irina Tolstykh, Bulat Suleimanov, Vladimir Dokholyan, Georgii Fedorov, Sergey Yakubson, Aleksandra Tsybina, Mikhail Chernyshov, Maksim Kuprashevich</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02242">https://arxiv.org/abs/2601.02242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02242">https://arxiv.org/pdf/2601.02242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02242]] VIBE: Visual Instruction Based Editor(https://arxiv.org/abs/2601.02242)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation.</li>
<li><strong>摘要：</strong>基于指令的图像编辑是生成人工智能中发展最快的领域之一。在过去的一年里，该领域已经达到了一个新的水平，数十个开源模型与高性能的商业系统一起发布。然而，目前只有有限数量的开源方法达到了现实世界的质量。此外，扩散主干网（这些管道的主要选择）对于许多部署和研究环境来说通常很大且计算成本昂贵，广泛使用的变体通常包含 6B 到 20B 参数。本文提出了一种紧凑、高吞吐量的基于指令的图像编辑管道，该管道使用现代 2B 参数 Qwen3-VL 模型来指导编辑过程，并使用 1.6B 参数扩散模型 Sana1.5 进行图像生成。我们在架构、数据处理、训练配置和评估方面的设计决策以低成本推理和严格的源一致性为目标，同时在这种规模下可行的主要编辑类别中保持高质量。在 ImgEdit 和 GEdit 基准上进行评估，所提出的方法匹配或超过了更重的基线的性能，包括具有数倍参数和更高推理成本的模型，并且在需要保留输入图像的编辑方面特别强大，例如属性调整、对象删除、背景编辑和有针对性的替换。该模型适合 24 GB GPU 内存，在 BF16 的 NVIDIA H100 上大约 4 秒内生成分辨率高达 2K 的编辑图像，无需额外的推理优化或蒸馏。</li>
</ul>

<h3>Title: Neuro-Channel Networks: A Multiplication-Free Architecture by Biological Signal Transmission</h3>
<ul>
<li><strong>Authors: </strong>Emrah Mete, Emin Erkan Korkmaz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02253">https://arxiv.org/abs/2601.02253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02253">https://arxiv.org/pdf/2601.02253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02253]] Neuro-Channel Networks: A Multiplication-Free Architecture by Biological Signal Transmission(https://arxiv.org/abs/2601.02253)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The rapid proliferation of Deep Learning is increasingly constrained by its heavy reliance on high-performance hardware, particularly Graphics Processing Units (GPUs). These specialized accelerators are not only prohibitively expensive and energy-intensive but also suffer from significant supply scarcity, limiting the ubiquity of Artificial Intelligence (AI) deployment on edge devices. The core of this inefficiency stems from the standard artificial perceptron's dependence on intensive matrix multiplications. However, biological nervous systems achieve unparalleled efficiency without such arithmetic intensity; synaptic signal transmission is regulated by physical ion channel limits and chemical neurotransmitter levels rather than a process that can be analogous to arithmetic multiplication. Inspired by this biological mechanism, we propose Neuro-Channel Networks (NCN), a novel multiplication-free architecture designed to decouple AI from expensive hardware dependencies. In our model, weights are replaced with Channel Widths that physically limit the signal magnitude, while a secondary parameter acts as a Neurotransmitter to regulate Signal Transmission based on sign logic. The forward pass relies exclusively on addition, subtraction, and bitwise operations (minimum, sign), eliminating floating-point multiplication entirely. In this proof-of-concept study, we demonstrate that NCNs can solve non-linearly separable problems like XOR and the Majority function with 100% accuracy using standard backpropagation, proving their capability to form complex decision boundaries without multiplicative weights. This architecture offers a highly efficient alternative for next-generation neuromorphic hardware, paving the way for running complex models on commodity CPUs or ultra-low-power chips without relying on costly GPU clusters.</li>
<li><strong>摘要：</strong>深度学习的快速发展越来越受到其对高性能硬件，特别是图形处理单元（GPU）的严重依赖的限制。这些专用加速器不仅价格昂贵、能源密集，而且供应严重短缺，限制了人工智能 (AI) 在边缘设备上的普遍部署。这种低效率的核心源于标准人工感知器对密集矩阵乘法的依赖。然而，生物神经系统无需如此高的运算强度即可实现无与伦比的效率；突触信号传输由物理离子通道限制和化学神经递质水平调节，而不是类似于算术乘法的过程。受这种生物机制的启发，我们提出了神经通道网络（NCN），这是一种新颖的无乘法架构，旨在将人工智能与昂贵的硬件依赖性分离。在我们的模型中，权重被物理上限制信号幅度的通道宽度取代，而辅助参数则充当神经递质，根据符号逻辑调节信号传输。前向传递完全依赖于加法、减法和按位运算（最小值、符号），完全消除了浮点乘法。在这项概念验证研究中，我们证明 NCN 可以使用标准反向传播以 100% 的精度解决 XOR 和多数函数等非线性可分离问题，证明它们无需乘法权重即可形成复杂决策边界的能力。该架构为下一代神经形态硬件提供了高效的替代方案，为在商用 CPU 或超低功耗芯片上运行复杂模型而无需依赖昂贵的 GPU 集群铺平了道路。</li>
</ul>

<h3>Title: VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation</h3>
<ul>
<li><strong>Authors: </strong>Shikun Sun, Liao Qu, Huichao Zhang, Yiheng Liu, Yangyang Song, Xian Li, Xu Wang, Yi Jiang, Daniel K. Du, Xinglong Wu, Jia Jia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02256">https://arxiv.org/abs/2601.02256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02256">https://arxiv.org/pdf/2601.02256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02256]] VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation(https://arxiv.org/abs/2601.02256)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Visual generation is dominated by three paradigms: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR) models. Unlike AR and diffusion, VARs operate on heterogeneous input structures across their generation steps, which creates severe asynchronous policy conflicts. This issue becomes particularly acute in reinforcement learning (RL) scenarios, leading to unstable training and suboptimal alignment. To resolve this, we propose a novel framework to enhance Group Relative Policy Optimization (GRPO) by explicitly managing these conflicts. Our method integrates three synergistic components: 1) a stabilizing intermediate reward to guide early-stage generation; 2) a dynamic time-step reweighting scheme for precise credit assignment; and 3) a novel mask propagation algorithm, derived from principles of Reward Feedback Learning (ReFL), designed to isolate optimization effects both spatially and temporally. Our approach demonstrates significant improvements in sample quality and objective alignment over the vanilla GRPO baseline, enabling robust and effective optimization for VAR models.</li>
<li><strong>摘要：</strong>视觉生成主要由三种范式主导：自回归（AR）模型、扩散模型和视觉自回归（VAR）模型。与 AR 和扩散不同，VAR 在其生成步骤中在异构输入结构上运行，这会产生严重的异步政策冲突。这个问题在强化学习 (RL) 场景中变得尤为严重，导致训练不稳定和对齐不理想。为了解决这个问题，我们提出了一个新的框架，通过明确管理这些冲突来增强组相对策略优化（GRPO）。我们的方法整合了三个协同组成部分：1）稳定的中间奖励来指导早期生成； 2）用于精确学分分配的动态时间步长重新加权方案； 3）一种新颖的掩模传播算法，源自奖励反馈学习（ReFL）原理，旨在隔离空间和时间上的优化效果。我们的方法表明，与普通 GRPO 基线相比，样本质量和目标一致性有了显着改进，从而能够对 VAR 模型进行稳健且有效的优化。</li>
</ul>

<h3>Title: POSEIDON: Physics-Optimized Seismic Energy Inference and Detection Operating Network</h3>
<ul>
<li><strong>Authors: </strong>Boris Kriuk, Fedor Kriuk</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02264">https://arxiv.org/abs/2601.02264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02264">https://arxiv.org/pdf/2601.02264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02264]] POSEIDON: Physics-Optimized Seismic Energy Inference and Detection Operating Network(https://arxiv.org/abs/2601.02264)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Earthquake prediction and seismic hazard assessment remain fundamental challenges in geophysics, with existing machine learning approaches often operating as black boxes that ignore established physical laws. We introduce POSEIDON (Physics-Optimized Seismic Energy Inference and Detection Operating Network), a physics-informed energy-based model for unified multi-task seismic event prediction, alongside the Poseidon dataset -- the largest open-source global earthquake catalog comprising 2.8 million events spanning 30 years. POSEIDON embeds fundamental seismological principles, including the Gutenberg-Richter magnitude-frequency relationship and Omori-Utsu aftershock decay law, as learnable constraints within an energy-based modeling framework. The architecture simultaneously addresses three interconnected prediction tasks: aftershock sequence identification, tsunami generation potential, and foreshock detection. Extensive experiments demonstrate that POSEIDON achieves state-of-the-art performance across all tasks, outperforming gradient boosting, random forest, and CNN baselines with the highest average F1 score among all compared methods. Crucially, the learned physics parameters converge to scientifically interpretable values -- Gutenberg-Richter b-value of 0.752 and Omori-Utsu parameters p=0.835, c=0.1948 days -- falling within established seismological ranges while enhancing rather than compromising predictive accuracy. The Poseidon dataset is publicly available at this https URL, providing pre-computed energy features, spatial grid indices, and standardized quality metrics to advance physics-informed seismic research.</li>
<li><strong>摘要：</strong>地震预测和地震灾害评估仍然是地球物理学的基本挑战，现有的机器学习方法通​​常作为黑匣子运行，忽略既定的物理定律。我们介绍了 POSEIDON（物理优化的地震能量推断和检测操作网络），这是一种基于物理的能量模型，用于统一多任务地震事件预测，以及 Poseidon 数据集——最大的开源全球地震目录，包含 30 年期间的 280 万个事件。 POSEIDON 嵌入了基本的地震学原理，包括古腾堡-里希特震级频率关系和大森-宇津余震衰减定律，作为基于能量的建模框架内的可学习约束。该架构同时解决三个相互关联的预测任务：余震序列识别、海啸发生潜力和前震检测。大量实验表明，POSEIDON 在所有任务中都实现了最先进的性能，优于梯度提升、随机森林和 CNN 基线，在所有比较方法中平均 F1 得分最高。至关重要的是，学习到的物理参数收敛到科学上可解释的值——Gutenberg-Richter b 值为 0.752，Omori-Utsu 参数 p=0.835，c=0.1948 天——落在既定的地震范围内，同时增强而不是损害预测准确性。 Poseidon 数据集可在此 https URL 上公开获取，提供预先计算的能量特征、空间网格索引和标准化质量指标，以推进基于物理的地震研究。</li>
</ul>

<h3>Title: DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies</h3>
<ul>
<li><strong>Authors: </strong>Renke Wang, Zhenyu Zhang, Ying Tai, Jian Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02267">https://arxiv.org/abs/2601.02267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02267">https://arxiv.org/pdf/2601.02267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02267]] DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies(https://arxiv.org/abs/2601.02267)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Human mesh recovery from multi-view images faces a fundamental challenge: real-world datasets contain imperfect ground-truth annotations that bias the models' training, while synthetic data with precise supervision suffers from domain gap. In this paper, we propose DiffProxy, a novel framework that generates multi-view consistent human proxies for mesh recovery. Central to DiffProxy is leveraging the diffusion-based generative priors to bridge the synthetic training and real-world generalization. Its key innovations include: (1) a multi-conditional mechanism for generating multi-view consistent, pixel-aligned human proxies; (2) a hand refinement module that incorporates flexible visual prompts to enhance local details; and (3) an uncertainty-aware test-time scaling method that increases robustness to challenging cases during optimization. These designs ensure that the mesh recovery process effectively benefits from the precise synthetic ground truth and generative advantages of the diffusion-based pipeline. Trained entirely on synthetic data, DiffProxy achieves state-of-the-art performance across five real-world benchmarks, demonstrating strong zero-shot generalization particularly on challenging scenarios with occlusions and partial views. Project page: this https URL</li>
<li><strong>摘要：</strong>从多视图图像中恢复人体网格面临着一个根本性的挑战：现实世界的数据集包含不完美的真实注释，这会影响模型的训练，而具有精确监督的合成数据则受到域差距的影响。在本文中，我们提出了 DiffProxy，这是一种新颖的框架，可以生成用于网格恢复的多视图一致的人类代理。 DiffProxy 的核心是利用基于扩散的生成先验来连接综合训练和现实世界的泛化。其主要创新包括：（1）用于生成多视图一致、像素对齐的人体代理的多条件机制； （2）手部细化模块，结合灵活的视觉提示，增强局部细节； (3) 一种不确定性感知测试时间缩放方法，可提高优化过程中对挑战性情况的鲁棒性。这些设计确保网格恢复过程有效地受益于基于扩散的管道的精确合成地面实况和生成优势。 DiffProxy 完全基于合成数据进行训练，在五个现实世界基准测试中实现了最先进的性能，展示了强大的零样本泛化能力，特别是在具有遮挡和部分视图的挑战性场景上。项目页面：此 https URL</li>
</ul>

<h3>Title: DatBench: Discriminative, Faithful, and Efficient VLM Evaluations</h3>
<ul>
<li><strong>Authors: </strong>Siddharth Joshi, Haoli Yin, Rishabh Adiga, Ricardo Monti, Aldo Carranza, Alex Fang, Alvin Deng, Amro Abbas, Brett Larsen, Cody Blakeney, Darren Teh, David Schwab, Fan Pan, Haakon Mongstad, Jack Urbanek, Jason Lee, Jason Telanoff, Josh Wills, Kaleigh Mentzer, Luke Merrick, Parth Doshi, Paul Burstein, Pratyush Maini, Scott Loftin, Spandan Das, Tony Jiang, Vineeth Dorna, Zhengping Wang, Bogdan Gaza, Ari Morcos, Matthew Leavitt</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02316">https://arxiv.org/abs/2601.02316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02316">https://arxiv.org/pdf/2601.02316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02316]] DatBench: Discriminative, Faithful, and Efficient VLM Evaluations(https://arxiv.org/abs/2601.02316)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Empirical evaluation serves as the primary compass guiding research progress in foundation models. Despite a large body of work focused on training frontier vision-language models (VLMs), approaches to their evaluation remain nascent. To guide their maturation, we propose three desiderata that evaluations should satisfy: (1) faithfulness to the modality and application, (2) discriminability between models of varying quality, and (3) efficiency in compute. Through this lens, we identify critical failure modes that violate faithfulness and discriminability, misrepresenting model capabilities: (i) multiple-choice formats reward guessing, poorly reflect downstream use cases, and saturate early as models improve; (ii) blindly solvable questions, which can be answered without images, constitute up to 70% of some evaluations; and (iii) mislabeled or ambiguous samples compromise up to 42% of examples in certain datasets. Regarding efficiency, the computational burden of evaluating frontier models has become prohibitive: by some accounts, nearly 20% of development compute is devoted to evaluation alone. Rather than discarding existing benchmarks, we curate them via transformation and filtering to maximize fidelity and discriminability. We find that converting multiple-choice questions to generative tasks reveals sharp capability drops of up to 35%. In addition, filtering blindly solvable and mislabeled samples improves discriminative power while simultaneously reducing computational cost. We release DatBench-Full, a cleaned evaluation suite of 33 datasets spanning nine VLM capabilities, and DatBench, a discriminative subset that achieves 13x average speedup (up to 50x) while closely matching the discriminative power of the original datasets. Our work outlines a path toward evaluation practices that are both rigorous and sustainable as VLMs continue to scale.</li>
<li><strong>摘要：</strong>实证评价是指导基础模型研究进展的主要指南针。尽管大量工作集中在训练前沿视觉语言模型（VLM）上，但对其评估的方法仍然处于萌芽状态。为了指导它们的成熟，我们提出了评估应满足的三个需求：（1）忠实于模式和应用，（2）不同质量模型之间的可区分性，以及（3）计算效率。通过这个镜头，我们识别出违反忠实性和可辨别性、歪曲模型能力的关键故障模式：（i）多项选择格式奖励猜测，不能很好地反映下游用例，并且随着模型的改进而提前饱和； (ii) 盲目解决的问题，即无需图像即可回答的问题，在某些评估中所占比例高达 70%； (iii) 某些数据集中高达 42% 的示例中，标签错误或模糊的样本受到影响。就效率而言，评估前沿模型的计算负担已变得令人望而却步：据一些统计，近 20% 的开发计算仅用于评估。我们没有放弃现有的基准，而是通过转换和过滤来管理它们，以最大限度地提高保真度和可辨别性。我们发现，将多项选择题转换为生成性任务后，能力会急剧下降高达 35%。此外，过滤盲目可解和错误标记的样本可以提高判别能力，同时降低计算成本。我们发布了 DatBench-Full，这是一个包含 33 个数据集、涵盖 9 个 VLM 功能的清理评估套件，以及 DatBench，这是一个判别子集，可实现 13 倍平均加速（高达 50 倍），同时与原始数据集的判别能力非常匹配。随着 VLM 的不断扩展，我们的工作概述了一条既严格又可持续的评估实践之路。</li>
</ul>

<h3>Title: Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes</h3>
<ul>
<li><strong>Authors: </strong>Jing Tan, Zhaoyang Zhang, Yantao Shen, Jiarui Cai, Shuo Yang, Jiajun Wu, Wei Xia, Zhuowen Tu, Stefano Soatto</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02356">https://arxiv.org/abs/2601.02356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02356">https://arxiv.org/pdf/2601.02356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02356]] Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes(https://arxiv.org/abs/2601.02356)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial transformation of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for multimodal generation systems. While existing text-based manipulation methods can adjust appearance or style, they struggle to perform object-level geometric transformations-such as translating, rotating, or resizing objects-due to scarce paired supervision and pixel-level optimization limits. Talk2Move employs Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts generated from input images and lightweight textual variations, removing the need for costly paired data. A spatial reward guided model aligns geometric transformations with linguistic description, while off-policy step evaluation and active step sampling improve learning efficiency by focusing on informative transformation stages. Furthermore, we design object-centric spatial rewards that evaluate displacement, rotation, and scaling behaviors directly, enabling interpretable and coherent transformations. Experiments on curated benchmarks demonstrate that Talk2Move achieves precise, consistent, and semantically faithful object transformations, outperforming existing text-guided editing approaches in both spatial accuracy and scene coherence.</li>
<li><strong>摘要：</strong>我们介绍 Talk2Move，这是一种基于强化学习 (RL) 的扩散框架，用于场景内对象的文本指导空间变换。通过自然语言对场景中的对象进行空间操作对多模态生成系统提出了挑战。虽然现有的基于文本的操作方法可以调整外观或风格，但由于缺乏配对监督和像素级优化限制，它们难以执行对象级几何变换（例如平移、旋转或调整对象大小）。 Talk2Move 采用组相对策略优化 (GRPO)，通过输入图像和轻量级文本变化生成的各种卷展来探索几何动作，从而无需昂贵的配对数据。空间奖励引导模型将几何变换与语言描述结合起来，而离策略步骤评估和主动步骤采样则通过关注信息转换阶段来提高学习效率。此外，我们设计了以对象为中心的空间奖励，直接评估位移、旋转和缩放行为，从而实现可解释和连贯的转换。对策划基准的实验表明，Talk2Move 实现了精确、一致且语义忠实的对象转换，在空间准确性和场景连贯性方面优于现有的文本引导编辑方法。</li>
</ul>

<h3>Title: VINO: A Unified Visual Generator with Interleaved OmniModal Context</h3>
<ul>
<li><strong>Authors: </strong>Junyi Chen, Tong He, Zhoujie Fu, Pengfei Wan, Kun Gai, Weicai Ye</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.02358">https://arxiv.org/abs/2601.02358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.02358">https://arxiv.org/pdf/2601.02358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.02358]] VINO: A Unified Visual Generator with Interleaved OmniModal Context(https://arxiv.org/abs/2601.02358)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.</li>
<li><strong>摘要：</strong>我们推出了 VINO，一个统一的视觉生成器，可以在单个框架内执行图像和视频生成和编辑。 VINO 不依赖特定于任务的模型或每种模态的独立模块，而是使用以文本、图像和视频为条件的共享扩散主干，从而在一个模型下实现广泛的视觉创建和编辑任务。具体来说，VINO 将视觉语言模型 (VLM) 与多模态扩散变压器 (MMDiT) 结合起来，其中多模态输入被编码为交错条件标记，然后用于指导扩散过程。该设计支持多参考基础、长格式指令遵循以及跨静态和动态内容的一致身份保留，同时避免特定于模态的架构组件。为了训练这样一个统一的系统，我们引入了一个多阶段训练管道，该管道逐步将视频生成基础模型扩展为能够进行图像和视频输入和输出的统一的多任务生成器。在不同的生成和编辑基准中，VINO 展示了强大的视觉质量、忠实的指令遵循、改进的参考和属性保留以及更可控的多身份编辑。我们的结果强调了实现可扩展的统一视觉生成的实用路径，以及交错的上下文计算作为通用视觉创建基础的前景。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
