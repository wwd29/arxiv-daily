<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-03</h1>
<h3>Title: ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning Models</h3>
<ul>
<li><strong>Authors: </strong>Akshat Ramachandran, Marina Neseem, Charbel Sakr, Rangharajan Venkatesan, Brucek Khailany, Tushar Krishna</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01290">https://arxiv.org/abs/2510.01290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01290">https://arxiv.org/pdf/2510.01290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01290]] ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning Models(https://arxiv.org/abs/2510.01290)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The long-output context generation of large reasoning models enables extended chain of thought (CoT) but also drives rapid growth of the key-value (KV) cache, quickly overwhelming GPU memory. To address this challenge, we propose ThinKV, a thought-adaptive KV cache compression framework. ThinKV is based on the observation that attention sparsity reveals distinct thought types with varying importance within the CoT. It applies a hybrid quantization-eviction strategy, assigning token precision by thought importance and progressively evicting tokens from less critical thoughts as reasoning trajectories evolve. Furthermore, to implement ThinKV, we design a kernel that extends PagedAttention to enable efficient reuse of evicted tokens' memory slots, eliminating compaction overheads. Extensive experiments on DeepSeek-R1-Distill, GPT-OSS, and NVIDIA AceReason across mathematics and coding benchmarks show that ThinKV achieves near-lossless accuracy with less than 5% of the original KV cache, while improving performance with up to 5.8x higher inference throughput over state-of-the-art baselines.</li>
<li><strong>摘要：</strong>大型推理模型的长输出上下文生成可以实现扩展的思想链（COT），但也可以驱动钥匙值（KV）缓存的快速增长，从而迅速压倒了GPU内存。为了应对这一挑战，我们提出了一个思想自适应的KV缓存压缩框架ThinkV。 ThinkV基于这样的观察，即注意力稀疏揭示了在COT中具有不同重要性的不同思想类型。它采用了混合量化 - 差异策略，通过思想重要性分配令牌精度，并逐渐驱逐出代币，因为推理轨迹的发展而不是批判性较低的思想。此外，为了实现ThinkV，我们设计了一个扩展pageding的内核，以有效地重用被驱逐的令牌记忆插槽，从而消除了压实开销。对DeepSeek-R1-Distill，GPT-Oss和Nvidia AceReason进行了广泛的实验，跨数学和编码基准表明，ThinkV可实现近乎无情的精度，而不到原始的KV高速缓存的5％，同时提高了性能，高达5.8倍，高达5.8倍，高达5.8倍，高达5.8倍的跨度。</li>
</ul>

<h3>Title: LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration</h3>
<ul>
<li><strong>Authors: </strong>Alessio Spagnoletti, Andrés Almansa, Marcelo Pereyra</a></li>
<li><strong>Subjects: </strong>cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01339">https://arxiv.org/abs/2510.01339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01339">https://arxiv.org/pdf/2510.01339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01339]] LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration(https://arxiv.org/abs/2510.01339)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generative</a></li>
<li><strong>Abstract: </strong>Computational imaging methods increasingly rely on powerful generative diffusion models to tackle challenging image restoration tasks. In particular, state-of-the-art zero-shot image inverse solvers leverage distilled text-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy and perceptual quality with high computational efficiency. However, extending these advances to high-definition video restoration remains a significant challenge, due to the need to recover fine spatial detail while capturing subtle temporal dependencies. Consequently, methods that naively apply image-based LDM priors on a frame-by-frame basis often result in temporally inconsistent reconstructions. We address this challenge by leveraging recent advances in Video Consistency Models (VCMs), which distill video latent diffusion models into fast generators that explicitly capture temporal causality. Building on this foundation, we propose LVTINO, the first zero-shot or plug-and-play inverse solver for high definition video restoration with priors encoded by VCMs. Our conditioning mechanism bypasses the need for automatic differentiation and achieves state-of-the-art video reconstruction quality with only a few neural function evaluations, while ensuring strong measurement consistency and smooth temporal transitions across frames. Extensive experiments on a diverse set of video inverse problems show significant perceptual improvements over current state-of-the-art methods that apply image LDMs frame by frame, establishing a new benchmark in both reconstruction fidelity and computational efficiency.</li>
<li><strong>摘要：</strong>计算成像方法越来越依赖强大的生成扩散模型来解决具有挑战性的图像恢复任务。特别是，最新的零摄像图像反求器利用蒸馏的文本对图像潜扩散模型（LDMS）以高计算效率实现前所未有的准确性和感知质量。但是，将这些进步扩展到高清视频恢复仍然是一个重大挑战，因为需要在捕获微妙的时间依赖性的同时恢复精细的空间细节。因此，逐框基础上天真地应用基于图像的LDM先验的方法通常会导致时间上不一致的重建。我们通过利用视频一致性模型（VCM）的最新进展来应对这一挑战，该模型将视频潜在扩散模型提炼为明确捕获时间因果关系的快速发电机。在此基础的基础上，我们提出了LVTINO，这是第一个零射击或插件的倒数求解器，用于使用VCMS编码的PRIORS进行高清视频修复。我们的条件机制绕过了对自动分化的需求，并仅通过少量神经功能评估来实现最先进的视频重建质量，同时确保了强大的测量一致性和跨帧的平稳时间过渡。对各种视频逆问题的广泛实验表明，与当前的最新方法相比，通过框架应用图像LDMS框架，在重建保真度和计算效率中建立了新的基准。</li>
</ul>

<h3>Title: Image Generation Based on Image Style Extraction</h3>
<ul>
<li><strong>Authors: </strong>Shuochen Chang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01347">https://arxiv.org/abs/2510.01347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01347">https://arxiv.org/pdf/2510.01347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01347]] Image Generation Based on Image Style Extraction(https://arxiv.org/abs/2510.01347)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Image generation based on text-to-image generation models is a task with practical application scenarios that fine-grained styles cannot be precisely described and controlled in natural language, while the guidance information of stylized reference images is difficult to be directly aligned with the textual conditions of traditional textual guidance generation. This study focuses on how to maximize the generative capability of the pretrained generative model, by obtaining fine-grained stylistic representations from a single given stylistic reference image, and injecting the stylistic representations into the generative body without changing the structural framework of the downstream generative model, so as to achieve fine-grained controlled stylized image generation. In this study, we propose a three-stage training style extraction-based image generation method, which uses a style encoder and a style projection layer to align the style representations with the textual representations to realize fine-grained textual cue-based style guide generation. In addition, this study constructs the Style30k-captions dataset, whose samples contain a triad of images, style labels, and text descriptions, to train the style encoder and style projection layer in this experiment.</li>
<li><strong>摘要：</strong>基于文本到图像生成模型的图像生成是一项任务，具有实用的应用方案，无法精确描述和控制自然语言，而风格化参考图像的指导信息很难与传统文本指导生成的文本条件直接保持一致。这项研究的重点是如何通过从单个给定的风格参考图像中获得细粒度的样式表示，并将风格表现形式注入生成物体，而无需更改下游生成模型的结构框架，以实现良好的受控样式图像生成。在这项研究中，我们提出了一种基于三阶段训练样式提取样式的图像生成方法，该方法使用样式编码器和样式投影层将样式表示与文本表示形式保持一致，以实现基于细粒的基于文本提示的样式指南的生成。此外，这项研究构建了Style30k捕获数据集，其样品包含三大图像，样式标签和文本说明，以训练该实验中的样式编码器和样式投影层。</li>
</ul>

<h3>Title: RheOFormer: A generative transformer model for simulation of complex fluids and flows</h3>
<ul>
<li><strong>Authors: </strong>Maedeh Saberi, Amir Barati Farimani, Safa Jamali</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.flu-dyn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01365">https://arxiv.org/abs/2510.01365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01365">https://arxiv.org/pdf/2510.01365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01365]] RheOFormer: A generative transformer model for simulation of complex fluids and flows(https://arxiv.org/abs/2510.01365)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The ability to model mechanics of soft materials under flowing conditions is key in designing and engineering processes and materials with targeted properties. This generally requires solution of internal stress tensor, related to the deformation tensor through nonlinear and history-dependent constitutive models. Traditional numerical methods for non-Newtonian fluid dynamics often suffer from prohibitive computational demands and poor scalability to new problem instances. Developments in data-driven methods have mitigated some limitations but still require retraining across varied physical conditions. In this work, we introduce Rheological Operator Transformer (RheOFormer), a generative operator learning method leveraging self-attention to efficiently learn different spatial interactions and features of complex fluid flows. We benchmark RheOFormer across a range of different viscometric and non-viscometric flows with different types of viscoelastic and elastoviscoplastic mechanics in complex domains against ground truth solutions. Our results demonstrate that RheOFormer can accurately learn both scalar and tensorial nonlinear mechanics of different complex fluids and predict the spatio-temporal evolution of their flows, even when trained on limited datasets. Its strong generalization capabilities and computational efficiency establish RheOFormer as a robust neural surrogate for accelerating predictive complex fluid simulations, advancing data-driven experimentation, and enabling real-time process optimization across a wide range of applications.</li>
<li><strong>摘要：</strong>在流动条件下模拟软材料力学的能力是设计和工程过程以及具有目标特性的材料的关键。这通常需要解决与非线性和历史依赖性本构模型相关的内部应力张量解决方案。非牛顿流体动力学的传统数值方法通常会遭受超出的计算需求和对新问题实例的可扩展性差。数据驱动方法的发展减轻了一些局限性，但仍需要在各种物理条件下进行重新培训。在这项工作中，我们介绍了流变操作器变压器（Rheoformer），这是一种生成操作员的学习方法，利用自我注意力来有效学习不同的空间相互作用和复杂流体流的特征。我们在复杂域中针对地面真实解决方案的复杂域中的不同类型的粘弹性和弹性机械力学基准，具有不同类型的粘弹性和弹性塑性力学的不同类型的粘度和非viscometric流。我们的结果表明，即使在有限的数据集中接受培训，也可以准确地学习不同复杂流体的标量和张力非线性力学，并预测其流量的时空演化。其强大的概括能力和计算效率将风湿性化合物作为强大的神经替代物，用于加速预测复杂的流体模拟，进行数据驱动的实验，并在广泛的应用中实现实时过程优化。</li>
</ul>

<h3>Title: Selective Underfitting in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Kiwhan Song, Jaeyeon Kim, Sitan Chen, Yilun Du, Sham Kakade, Vincent Sitzmann</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01378">https://arxiv.org/abs/2510.01378</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01378">https://arxiv.org/pdf/2510.01378</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01378]] Selective Underfitting in Diffusion Models(https://arxiv.org/abs/2510.01378)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as the principal paradigm for generative modeling across various domains. During training, they learn the score function, which in turn is used to generate samples at inference. They raise a basic yet unsolved question: which score do they actually learn? In principle, a diffusion model that matches the empirical score in the entire data space would simply reproduce the training data, failing to generate novel samples. Recent work addresses this question by arguing that diffusion models underfit the empirical score due to training-time inductive biases. In this work, we refine this perspective, introducing the notion of selective underfitting: instead of underfitting the score everywhere, better diffusion models more accurately approximate the score in certain regions of input space, while underfitting it in others. We characterize these regions and design empirical interventions to validate our perspective. Our results establish that selective underfitting is essential for understanding diffusion models, yielding new, testable insights into their generalization and generative performance.</li>
<li><strong>摘要：</strong>扩散模型已成为跨各个领域的生成建模的主要范式。在训练过程中，他们学习了分数功能，而得分函数又用于推理时生成样品。他们提出了一个基本但未解决的问题：他们实际学到了哪个分数？原则上，与整个数据空间中的经验得分相匹配的扩散模型将简单地重现训练数据，无法生成新的样本。最近的工作通过争辩说，由于训练时间电感偏见而导致的经验得分低于经验得分来解决这个问题。在这项工作中，我们完善了这种观点，引入了选择性不足的概念：与其在任何地方不及时拟合分数，不如更准确地近似于输入空间的某些区域中的分数，同时在其他方面不适合。我们表征这些区域并设计经验干预措施以验证我们的观点。我们的结果表明，选择性不足对于理解扩散模型至关重要，为其概括和生成性能提供了新的，可测试的见解。</li>
</ul>

<h3>Title: Fine-Tuning Masked Diffusion for Provable Self-Correction</h3>
<ul>
<li><strong>Authors: </strong>Jaeyeon Kim, Seunggeun Kim, Taekyun Lee, David Z. Pan, Hyeji Kim, Sham Kakade, Sitan Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01384">https://arxiv.org/abs/2510.01384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01384">https://arxiv.org/pdf/2510.01384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01384]] Fine-Tuning Masked Diffusion for Provable Self-Correction(https://arxiv.org/abs/2510.01384)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>A natural desideratum for generative models is self-correction--detecting and revising low-quality tokens at inference. While Masked Diffusion Models (MDMs) have emerged as a promising approach for generative modeling in discrete spaces, their capacity for self-correction remains poorly understood. Prior attempts to incorporate self-correction into MDMs either require overhauling MDM architectures/training or rely on imprecise proxies for token quality, limiting their applicability. Motivated by this, we introduce PRISM--Plug-in Remasking for Inference-time Self-correction of Masked Diffusions--a lightweight, model-agnostic approach that applies to any pretrained MDM. Theoretically, PRISM defines a self-correction loss that provably learns per-token quality scores, without RL or a verifier. These quality scores are computed in the same forward pass with MDM and used to detect low-quality tokens. Empirically, PRISM advances MDM inference across domains and scales: Sudoku; unconditional text (170M); and code with LLaDA (8B).</li>
<li><strong>摘要：</strong>生成模型的天然欲望是自我纠正 - 检测并在推理时修改低质量的代币。虽然蒙版扩散模型（MDM）已成为离散空间中生成建模的一种有希望的方法，但它们的自我校正能力仍然很少了解。事先尝试将自我纠正纳入MDMS需要对MDM架构/培训进行大修，或者依靠不精确的代理来代表令牌质量，从而限制其适用性。在此激励的情况下，我们引入了Prism-插入掩盖扩散的推理时间自我纠正 - 一种适用于任何预审预定的MDM的轻巧，模型 - 不合Snostic方法。从理论上讲，棱镜定义了自我纠正损失，该损失可以证明可以学习无RL或验证者的质量得分。这些质量得分是用MDM在同一正向通过中计算的，用于检测低质量令牌。从经验上讲，棱镜推进了跨领域和尺度的MDM推论：sudoku;无条件文字（170m）；和Llada（8b）的代码。</li>
</ul>

<h3>Title: Optimal Stopping vs Best-of-$N$ for Inference Time Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yusuf Kalayci, Vinod Raman, Shaddin Dughmi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01394">https://arxiv.org/abs/2510.01394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01394">https://arxiv.org/pdf/2510.01394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01394]] Optimal Stopping vs Best-of-$N$ for Inference Time Optimization(https://arxiv.org/abs/2510.01394)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) generation often requires balancing output quality against inference cost, especially when using multiple generations. We introduce a new framework for inference-time optimization based on the classical Pandora's Box problem. Viewing each generation as opening a costly "box" with random reward, we develop algorithms that decide when to stop generating without knowing the underlying reward distribution. Our first contribution is a UCB-style Pandora's Box algorithm, which achieves performance that is provably close to Weitzman's algorithm, the optimal strategy when the distribution is known. We further adapt this method to practical LLM settings by addressing reward scaling across prompts via a Bradley-Terry inspired transformation. This leads to an adaptive inference-time optimization method that normalizes rewards and learns stopping thresholds on the fly. Experiments on the AlpacaFarm and HH-RLHF datasets, using multiple LLM-reward model pairs, show that our adaptive strategy can obtain the same performance as non-adaptive Best-of-N sampling while requiring 15-35 percent fewer generations on average. Our results establish a principled bridge between optimal stopping theory and inference-time scaling, providing both theoretical performance bounds and practical efficiency gains for LLM deployment.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的生成通常需要平衡输出质量与推理成本，尤其是在使用多代时。我们根据经典的Pandora的盒子问题介绍了用于推理时间优化的新框架。我们将每一代人视为以随机奖励打开昂贵的“盒子”，我们开发了算法，这些算法决定何时停止生成而不知道基础奖励分布。我们的第一个贡献是UCB风格的Pandora的盒子算法，该算法实现了与Weitzman的算法相近的性能，这是知道分布时的最佳策略。我们通过通过Bradley-Terry启发的转换来解决跨提示的奖励缩放，进一步适应了实用的LLM设置。这导致了一种自适应推理时间优化方法，该方法使奖励归一化并学习了停止阈值。使用多个LLM奖励模型对的Alpacafarm和HH-RLHF数据集进行的实验表明，我们的自适应策略可以获得与非自适应最佳最佳采样的相同性能，同时平均需要少15-35％的人。我们的结果在最佳停止理论和推理时间缩放之间建立了原则上的桥梁，从而为LLM部署提供了理论性能界限和实践效率提高。</li>
</ul>

<h3>Title: DisCo: Reinforcement with Diversity Constraints for Multi-Human Generation</h3>
<ul>
<li><strong>Authors: </strong>Shubhankar Borse, Farzad Farhadzadeh, Munawar Hayat, Fatih Porikli</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01399">https://arxiv.org/abs/2510.01399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01399">https://arxiv.org/pdf/2510.01399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01399]] DisCo: Reinforcement with Diversity Constraints for Multi-Human Generation(https://arxiv.org/abs/2510.01399)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>State-of-the-art text-to-image models excel at realism but collapse on multi-human prompts - duplicating faces, merging identities, and miscounting individuals. We introduce DisCo (Reinforcement with Diversity Constraints), the first RL-based framework to directly optimize identity diversity in multi-human generation. DisCo fine-tunes flow-matching models via Group-Relative Policy Optimization (GRPO) with a compositional reward that (i) penalizes intra-image facial similarity, (ii) discourages cross-sample identity repetition, (iii) enforces accurate person counts, and (iv) preserves visual fidelity through human preference scores. A single-stage curriculum stabilizes training as complexity scales, requiring no extra annotations. On the DiverseHumans Testset, DisCo achieves 98.6 Unique Face Accuracy and near-perfect Global Identity Spread - surpassing both open-source and proprietary methods (e.g., Gemini, GPT-Image) while maintaining competitive perceptual quality. Our results establish DisCo as a scalable, annotation-free solution that resolves the long-standing identity crisis in generative models and sets a new benchmark for compositional multi-human generation.</li>
<li><strong>摘要：</strong>最先进的文本到图像模型在现实主义方面表现出色，但在多人类提示上崩溃 - 复制面孔，合并身份和错误的个人。我们介绍了迪斯科舞厅（具有多样性约束的增强），这是第一个基于RL的框架，可直接优化多人类生成中的身份多样性。通过组成策略优化（GRPO），迪斯科微型匹配模型具有组成奖励，（i）对图像内的面部相似性进行惩罚，（（ii）不建议通过人类的优先审查来维护准确的人数，（iii）强制执行准确的人计数，（iiv）可以保留视觉效果。单级课程将培训稳定为复杂性量表，不需要额外的注释。在不同人类测试集上，迪斯科达到98.6独特的面部精度和近乎完美的全球身份传播 - 超过了开源和专有方法（例如，Gemini，GPT-Image），同时保持竞争感知质量。我们的结果将迪斯科舞厅确定为可扩展的，无注释的解决方案，它可以解决生成模型中长期存在的身份危机，并为构图多人类生成设定了新的基准。</li>
</ul>

<h3>Title: Purrception: Variational Flow Matching for Vector-Quantized Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Răzvan-Andrei Matişan, Vincent Tao Hu, Grigory Bartosh, Björn Ommer, Cees G. M. Snoek, Max Welling, Jan-Willem van de Meent, Mohammad Mahdi Derakhshani, Floor Eijkelboom</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01478">https://arxiv.org/abs/2510.01478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01478">https://arxiv.org/pdf/2510.01478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01478]] Purrception: Variational Flow Matching for Vector-Quantized Image Generation(https://arxiv.org/abs/2510.01478)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce Purrception, a variational flow matching approach for vector-quantized image generation that provides explicit categorical supervision while maintaining continuous transport dynamics. Our method adapts Variational Flow Matching to vector-quantized latents by learning categorical posteriors over codebook indices while computing velocity fields in the continuous embedding space. This combines the geometric awareness of continuous methods with the discrete supervision of categorical approaches, enabling uncertainty quantification over plausible codes and temperature-controlled generation. We evaluate Purrception on ImageNet-1k 256x256 generation. Training converges faster than both continuous flow matching and discrete flow matching baselines while achieving competitive FID scores with state-of-the-art models. This demonstrates that Variational Flow Matching can effectively bridge continuous transport and discrete supervision for improved training efficiency in image generation.</li>
<li><strong>摘要：</strong>我们介绍了Purrception，这是一种用于矢量定量图像生成的变分流匹配方法，可提供明确的分类监督，同时保持连续的运输动力学。我们的方法通过在计算连续嵌入空间中计算速度字段时，通过学习分类后代来调整各种流量匹配到矢量定量的潜在潜在的潜在潜在的潜在。这将连续方法的几何意识与分类方法的离散监督结合在一起，从而可以对合理的代码和温度控制的生成进行不确定性定量。我们评估了Imagenet-1K 256x256生成上的purrotction。训练的收敛速度比连续流匹配和离散流匹配基线的收敛速度快，同时通过最先进的模型获得了竞争性的FID得分。这表明，变分流匹配可以有效地弥合连续运输和离散监督，以提高图像产生的训练效率。</li>
</ul>

<h3>Title: NVIDIA AI Aerial: AI-Native Wireless Communications</h3>
<ul>
<li><strong>Authors: </strong>Kobi Cohen-Arazi, Michael Roe, Zhen Hu, Rohan Chavan, Anna Ptasznik, Joanna Lin, Joao Morais, Joseph Boccuzzi, Tommaso Balercia</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01533">https://arxiv.org/abs/2510.01533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01533">https://arxiv.org/pdf/2510.01533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01533]] NVIDIA AI Aerial: AI-Native Wireless Communications(https://arxiv.org/abs/2510.01533)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>6G brings a paradigm shift towards AI-native wireless systems, necessitating the seamless integration of digital signal processing (DSP) and machine learning (ML) within the software stacks of cellular networks. This transformation brings the life cycle of modern networks closer to AI systems, where models and algorithms are iteratively trained, simulated, and deployed across adjacent environments. In this work, we propose a robust framework that compiles Python-based algorithms into GPU-runnable blobs. The result is a unified approach that ensures efficiency, flexibility, and the highest possible performance on NVIDIA GPUs. As an example of the capabilities of the framework, we demonstrate the efficacy of performing the channel estimation function in the PUSCH receiver through a convolutional neural network (CNN) trained in Python. This is done in a digital twin first, and subsequently in a real-time testbed. Our proposed methodology, realized in the NVIDIA AI Aerial platform, lays the foundation for scalable integration of AI/ML models into next-generation cellular systems, and is essential for realizing the vision of natively intelligent 6G networks.</li>
<li><strong>摘要：</strong>6G将范式转向AI本地的无线系统，因此需要在蜂窝网络的软件堆栈中进行数字信号处理（DSP）和机器学习（ML）的无缝集成。这种转变使现代网络的生命周期更靠近AI系统，在该系统中，模型和算法在相邻环境中进行了迭代训练，模拟和部署。在这项工作中，我们提出了一个强大的框架，该框架将基于Python的算法编译为GPU可敲击的斑点。结果是一种统一的方法，可确保NVIDIA GPU上的效率，灵活性和最高性能。作为框架功能的一个例子，我们证明了通过在Python中训练的卷积神经网络（CNN）在PUSCH接收器中执行通道估计功能的功效。这是在数字双胞胎中进行的，随后在实时测试台上完成。我们提出的方法是在NVIDIA AI空中平台中实现的，为将AI/ML模型的可扩展整合到下一代蜂窝系统中奠定了基础，对于实现本地智能6G网络的愿景至关重要。</li>
</ul>

<h3>Title: Towards Better Optimization For Listwise Preference in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Jiamu Bai, Xin Yu, Meilong Xu, Weitao Lu, Xin Pan, Kiwan Maeng, Daniel Kifer, Jian Wang, Yu Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01540">https://arxiv.org/abs/2510.01540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01540">https://arxiv.org/pdf/2510.01540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01540]] Towards Better Optimization For Listwise Preference in Diffusion Models(https://arxiv.org/abs/2510.01540)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reinforcement learning from human feedback (RLHF) has proven effectiveness for aligning text-to-image (T2I) diffusion models with human preferences. Although Direct Preference Optimization (DPO) is widely adopted for its computational efficiency and avoidance of explicit reward modeling, its applications to diffusion models have primarily relied on pairwise preferences. The precise optimization of listwise preferences remains largely unaddressed. In practice, human feedback on image preferences often contains implicit ranked information, which conveys more precise human preferences than pairwise comparisons. In this work, we propose Diffusion-LPO, a simple and effective framework for Listwise Preference Optimization in diffusion models with listwise data. Given a caption, we aggregate user feedback into a ranked list of images and derive a listwise extension of the DPO objective under the Plackett-Luce model. Diffusion-LPO enforces consistency across the entire ranking by encouraging each sample to be preferred over all of its lower-ranked alternatives. We empirically demonstrate the effectiveness of Diffusion-LPO across various tasks, including text-to-image generation, image editing, and personalized preference alignment. Diffusion-LPO consistently outperforms pairwise DPO baselines on visual quality and preference alignment.</li>
<li><strong>摘要：</strong>从人类反馈（RLHF）中学习的强化学习对将文本对象（T2I）扩散模型与人类偏好保持一致。尽管直接偏好优化（DPO）被广泛用于其计算效率和避免显式奖励建模，但其对扩散模型的应用主要依赖于成对的偏好。列表偏好的精确优化在很大程度上仍未得到解决。在实践中，人类对图像偏好的反馈通常包含隐式排名信息，这比成对比较传达了更精确的人类偏好。在这项工作中，我们提出了扩散LPO，这是一个简单有效的框架，用于使用ListWise数据的扩散模型中列表偏好优化。给定标题，我们将用户反馈汇总到排名的图像列表中，并在Plackett-luce模型下得出DPO目标的列表扩展。通过鼓励每个样本优先于其所有较低级别的替代方案，扩散LPO可以在整个排名中执行一致性。我们从经验上证明了扩散LPO在各种任务中的有效性，包括文本对图像生成，图像编辑和个性化的偏好对齐。扩散LPO始终在视觉质量和偏好比对上的成对DPO基准。</li>
</ul>

<h3>Title: Growing Visual Generative Capacity for Pre-Trained MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Hanyu Wang, Jiaming Han, Ziyan Yang, Qi Zhao, Shanchuan Lin, Xiangyu Yue, Abhinav Shrivastava, Zhenheng Yang, Hao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01546">https://arxiv.org/abs/2510.01546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01546">https://arxiv.org/pdf/2510.01546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01546]] Growing Visual Generative Capacity for Pre-Trained MLLMs(https://arxiv.org/abs/2510.01546)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) extend the success of language models to visual understanding, and recent efforts have sought to build unified MLLMs that support both understanding and generation. However, constructing such models remains challenging: hybrid approaches combine continuous embeddings with diffusion or flow-based objectives, producing high-quality images but breaking the autoregressive paradigm, while pure autoregressive approaches unify text and image prediction over discrete visual tokens but often face trade-offs between semantic alignment and pixel-level fidelity. In this work, we present Bridge, a pure autoregressive unified MLLM that augments pre-trained visual understanding models with generative ability through a Mixture-of-Transformers architecture, enabling both image understanding and generation within a single next-token prediction framework. To further improve visual generation fidelity, we propose a semantic-to-pixel discrete representation that integrates compact semantic tokens with fine-grained pixel tokens, achieving strong language alignment and precise description of visual details with only a 7.9% increase in sequence length. Extensive experiments across diverse multimodal benchmarks demonstrate that Bridge achieves competitive or superior results in both understanding and generation benchmarks, while requiring less training data and reduced training time compared to prior unified MLLMs.</li>
<li><strong>摘要：</strong>多模式的大语言模型（MLLM）将语言模型的成功扩展到视觉理解，最近的努力试图建立支持理解和产生的统一MLLM。但是，构建此类模型仍然具有挑战性：混合方法将连续的嵌入与扩散或基于流程的目标结合，产生高质量的图像，但会破坏自动性范式的范式，而纯净的自动化方法统一方法和图像预测超过了离散的视觉令牌，但通常面临着语义偏见，但面临着语义上的折衷和Pixelle fidelity之间的权衡。在这项工作中，我们提出了桥梁，这是一种纯粹的自动回归统一的MLLM，它通过转换器架构的混合物增强了具有生成能力的预训练的视觉理解模型，从而在单个下一步的预测框架内实现了图像理解和生成。为了进一步提高视觉产生的保真度，我们提出了一种语义到像素离散表示，将紧凑的语义令牌与细粒度的像素令牌集成在一起，实现强烈的语言对齐和对视觉细节的精确描述，仅序列长度增加7.9％。各种多模式基准的广泛实验表明，与先前的统一MLLM相比，桥梁在理解和生成基准测试方面取得了竞争或优越的结果，同时需要更少的训练数据和减少的训练时间。</li>
</ul>

<h3>Title: TetriServe: Efficient DiT Serving for Heterogeneous Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Runyu Lu, Shiqi He, Wenxuan Tan, Shenggui Li, Ruofan Wu, Jeff J. Ma, Ang Chen, Mosharaf Chowdhury</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01565">https://arxiv.org/abs/2510.01565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01565">https://arxiv.org/pdf/2510.01565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01565]] TetriServe: Efficient DiT Serving for Heterogeneous Image Generation(https://arxiv.org/abs/2510.01565)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion Transformer (DiT) models excel at generating highquality images through iterative denoising steps, but serving them under strict Service Level Objectives (SLOs) is challenging due to their high computational cost, particularly at large resolutions. Existing serving systems use fixed degree sequence parallelism, which is inefficient for heterogeneous workloads with mixed resolutions and deadlines, leading to poor GPU utilization and low SLO attainment. In this paper, we propose step-level sequence parallelism to dynamically adjust the parallel degree of individual requests according to their deadlines. We present TetriServe, a DiT serving system that implements this strategy for highly efficient image generation. Specifically, TetriServe introduces a novel round-based scheduling mechanism that improves SLO attainment: (1) discretizing time into fixed rounds to make deadline-aware scheduling tractable, (2) adapting parallelism at the step level and minimize GPU hour consumption, and (3) jointly packing requests to minimize late completions. Extensive evaluation on state-of-the-art DiT models shows that TetriServe achieves up to 32% higher SLO attainment compared to existing solutions without degrading image quality.</li>
<li><strong>摘要：</strong>扩散变压器（DIT）模型在通过迭代授权步骤生成高质量图像方面出色，但是在严格的服务水平目标（SLO）下为它们服务，由于其高计算成本，尤其是在大量分辨率下，因此具有挑战性。现有的服务系统使用固定学位序列并行性，这对于具有混合分辨率和截止日期的异质工作负载效率低下，导致GPU利用率差和SLO的较低成就。在本文中，我们提出了阶梯序列并行性，以根据其截止日期动态调整单个请求的平行程度。我们提出了Tetriserve，这是一种DIT服务系统，可实现此策略，以产生高效的图像产生。具体而言，Tetriserve引入了一种新型的基于圆形的调度机制，该机制可以改善SLO的达到：（1）将时间分配到固定的圆圈中，以使截止日期的时间表可以易于处理，（2）在步骤级别适应并行性，并最大程度地减少GPU小时消耗，以及（3）共同的包装请求，以最小化最小的后期完成。对最先进的DIT模型的广泛评估表明，与现有的解决方案相比，Tetriserve的SLO成就高达32％，而不会降低图像质量。</li>
</ul>

<h3>Title: Enhancing Noise Robustness of Parkinson's Disease Telemonitoring via Contrastive Feature Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Ziming Tang, Chengbin Hou, Tianyu Zhang, Bangxu Tian, Jinbao Wang, Hairong Lv</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01588">https://arxiv.org/abs/2510.01588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01588">https://arxiv.org/pdf/2510.01588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01588]] Enhancing Noise Robustness of Parkinson's Disease Telemonitoring via Contrastive Feature Augmentation(https://arxiv.org/abs/2510.01588)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Parkinson's disease (PD) is one of the most common neurodegenerative disorder. PD telemonitoring emerges as a novel assessment modality enabling self-administered at-home tests of Unified Parkinson's Disease Rating Scale (UPDRS) scores, enhancing accessibility for PD patients. However, three types of noise would occur during measurements: (1) patient-induced measurement inaccuracies, (2) environmental noise, and (3) data packet loss during transmission, resulting in higher prediction errors. To address these challenges, NoRo, a noise-robust UPDRS prediction framework is proposed. First, the original speech features are grouped into ordered bins, based on the continuous values of a selected feature, to construct contrastive pairs. Second, the contrastive pairs are employed to train a multilayer perceptron encoder for generating noise-robust features. Finally, these features are concatenated with the original features as the augmented features, which are then fed into the UPDRS prediction models. Notably, we further introduces a novel evaluation approach with customizable noise injection module, and extensive experiments show that NoRo can successfully enhance the noise robustness of UPDRS prediction across various downstream prediction models under different noisy environments.</li>
<li><strong>摘要：</strong>帕金森氏病（PD）是最常见的神经退行性疾病之一。 PD远程监控是一种新颖的评估方式，从而实现了统一帕金森氏病评级量表（UPDRS）评分的自助式在家测试，从而增强了PD患者的可及性。但是，在测量过程中将发生三种类型的噪声：（1）患者诱导的测量不准确性，（2）环境噪声和（3）传输过程中数据包丢失，从而导致更高的预测错误。为了应对这些挑战，NORO提出了一个努力的UPDRS预测框架。首先，根据所选特征的连续值将原始的语音特征分为有序的垃圾箱，以构造对比对。其次，对比度对训练多层感知器编码器，以生成噪声功能。最后，这些功能与原始功能作为增强功能串联，然后将其馈入UPDRS预测模型。值得注意的是，我们进一步引入了一种新的评估方法，该方法具有可自定义的噪声注入模块，广泛的实验表明，NORO可以成功地增强在不同噪声环境下各种下游预测模型中UPDRS预测的噪声稳健性。</li>
</ul>

<h3>Title: Securing generative artificial intelligence with parallel magnetic tunnel junction true randomness</h3>
<ul>
<li><strong>Authors: </strong>Youwei Bao, Shuhan Yang, Hyunsoo Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci, physics.data-an</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01598">https://arxiv.org/abs/2510.01598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01598">https://arxiv.org/pdf/2510.01598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01598]] Securing generative artificial intelligence with parallel magnetic tunnel junction true randomness(https://arxiv.org/abs/2510.01598)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Deterministic pseudo random number generators (PRNGs) used in generative artificial intelligence (GAI) models produce predictable patterns vulnerable to exploitation by attackers. Conventional defences against the vulnerabilities often come with significant energy and latency overhead. Here, we embed hardware-generated true random bits from spin-transfer torque magnetic tunnel junctions (STT-MTJs) to address the challenges. A highly parallel, FPGA-assisted prototype computing system delivers megabit-per-second true random numbers, passing NIST randomness tests after in-situ operations with minimal overhead. Integrating the hardware random bits into a generative adversarial network (GAN) trained on CIFAR-10 reduces insecure outputs by up to 18.6 times compared to the low-quality random number generators (RNG) baseline. With nanosecond switching speed, high energy efficiency, and established scalability, our STT-MTJ-based system holds the potential to scale beyond 106 parallel cells, achieving gigabit-per-second throughput suitable for large language model sampling. This advancement highlights spintronic RNGs as practical security components for next-generation GAI systems.</li>
<li><strong>摘要：</strong>生成人工智能（GAI）模型中使用的确定性伪随机数发生器（PRNG）产生了容易受到攻击者剥削的可预测模式。防御漏洞的常规防御通常会带有巨大的能量和潜伏的开销。在这里，我们从自旋转移扭矩磁性隧道连接（STT-MTJ）中嵌入了硬件生成的真实随机位，以应对挑战。高度平行的，FPGA辅助的原型计算系统可提供每秒的真实随机数，并在原位操作中通过最小开销后通过NIST随机测试。与低质量的随机数生成器（RNG）基线相比，在CIFAR-10训练的硬件随机位中，在CIFAR-10训练的生成对抗网络（GAN）中，不安全的输出量最多减少了18.6倍。纳米秒开关速度，高能效率和已建立的可扩展性，我们的基于STT-MTJ的系统具有超过106个并行细胞的扩展，从而实现适合大型语言模型采样的每秒千兆位吞吐量。这一进步强调了Spintronic RNG作为下一代GAI系统的实际安全组件。</li>
</ul>

<h3>Title: NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems</h3>
<ul>
<li><strong>Authors: </strong>Roman Jacome, Romario Gualdrón-Hurtado, Leon Suarez, Henry Arguello</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.SP, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01608">https://arxiv.org/abs/2510.01608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01608">https://arxiv.org/pdf/2510.01608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01608]] NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems(https://arxiv.org/abs/2510.01608)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Imaging inverse problems aims to recover high-dimensional signals from undersampled, noisy measurements, a fundamentally ill-posed task with infinite solutions in the null-space of the sensing operator. To resolve this ambiguity, prior information is typically incorporated through handcrafted regularizers or learned models that constrain the solution space. However, these priors typically ignore the task-specific structure of that null-space. In this work, we propose \textit{Non-Linear Projections of the Null-Space} (NPN), a novel class of regularization that, instead of enforcing structural constraints in the image domain, promotes solutions that lie in a low-dimensional projection of the sensing matrix's null-space with a neural network. Our approach has two key advantages: (1) Interpretability: by focusing on the structure of the null-space, we design sensing-matrix-specific priors that capture information orthogonal to the signal components that are fundamentally blind to the sensing process. (2) Flexibility: NPN is adaptable to various inverse problems, compatible with existing reconstruction frameworks, and complementary to conventional image-domain priors. We provide theoretical guarantees on convergence and reconstruction accuracy when used within plug-and-play methods. Empirical results across diverse sensing matrices demonstrate that NPN priors consistently enhance reconstruction fidelity in various imaging inverse problems, such as compressive sensing, deblurring, super-resolution, computed tomography, and magnetic resonance imaging, with plug-and-play methods, unrolling networks, deep image prior, and diffusion models.</li>
<li><strong>摘要：</strong>成像逆问题旨在从不足的嘈杂测量值中恢复高维信号，这是一种在传感操作员的空空间中提供无限溶液的根本性不足的任务。为了解决这种歧义，通常通过手工制作的正规化器或限制解决方案空间的学识渊博的模型来合并先前的信息。但是，这些先验通常会忽略该空空间的特定任务结构。在这项工作中，我们提出了\ textIt {Null空间}（NPN）的\ textIt {非线性投影，这是一种新颖的正则化类别，该类别不是在图像域中强制执行结构约束，而是促进了在Sensing Matrix的Null Space带有Neural网络的低维投影中的解决方案。我们的方法具有两个关键的优势：（1）解释性：通过关注零空间的结构，我们设计了传感 - 矩阵特异性先验，这些先验捕获信息与信号成分的正交信息，这些信号成分从根本上蒙蔽了感应过程。 （2）灵活性：NPN适应各种反问题，与现有的重建框架兼容，并与常规的图像域较先验互补。当在插件方法中使用时，我们提供有关收敛和重建精度的理论保证。各种传感矩阵的经验结果表明，NPN先验始终增强各种成像逆问题的重建保真度，例如压缩感应，脱张，超分辨率，超分辨率，计算机断层扫描和磁共振成像，以及使用插件的方法，插件方法，外观网络，深层图像，先验和扩散模型。</li>
</ul>

<h3>Title: Posterior Collapse as a Phase Transition in Variational Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Zhen Li, Fan Zhang, Zheng Zhang, Yu Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01621">https://arxiv.org/abs/2510.01621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01621">https://arxiv.org/pdf/2510.01621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01621]] Posterior Collapse as a Phase Transition in Variational Autoencoders(https://arxiv.org/abs/2510.01621)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We investigate the phenomenon of posterior collapse in variational autoencoders (VAEs) from the perspective of statistical physics, and reveal that it constitutes a phase transition governed jointly by data structure and model hyper-parameters. By analyzing the stability of the trivial solution associated with posterior collapse, we identify a critical hyper-parameter threshold. This critical boundary, separating meaningful latent inference from collapse, is characterized by a discontinuity in the KL divergence between the approximate posterior and the prior distribution. We validate this critical behavior on both synthetic and real-world datasets, confirming the existence of a phase transition. Our results demonstrate that posterior collapse is not merely an optimization failure, but rather an emerging phase transition arising from the interplay between data structure and variational constraints. This perspective offers new insights into the trainability and representational capacity of deep generative models.</li>
<li><strong>摘要：</strong>我们从统计物理学的角度研究了变异自动编码器（VAE）的后验崩溃现象，并揭示其构成了由数据结构和模型超参数共同控制的相变。通过分析与后塌陷相关的琐碎溶液的稳定性，我们确定了关键的高参数阈值。将有意义的潜在推断与塌陷区分开的临界边界的特征是近似后部和先前分布之间的KL差异不连续。我们在合成和现实世界数据集上验证了这种关键行为，证实了相变的存在。我们的结果表明，后塌陷不仅是优化失败，而且是由数据结构和变异约束之间的相互作用引起的新出现的相变。这种观点为深层生成模型的训练性和表示能力提供了新的见解。</li>
</ul>

<h3>Title: VLA-R1: Enhancing Reasoning in Vision-Language-Action Models</h3>
<ul>
<li><strong>Authors: </strong>Angen Ye, Zeyu Zhang, Boyuan Wang, Xiaofeng Wang, Dapeng Zhang, Zheng Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01623">https://arxiv.org/abs/2510.01623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01623">https://arxiv.org/pdf/2510.01623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01623]] VLA-R1: Enhancing Reasoning in Vision-Language-Action Models(https://arxiv.org/abs/2510.01623)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision-Language-Action (VLA) models aim to unify perception, language understanding, and action generation, offering strong cross-task and cross-scene generalization with broad impact on embodied AI. However, current VLA models often lack explicit step-by-step reasoning, instead emitting final actions without considering affordance constraints or geometric relations. Their post-training pipelines also rarely reinforce reasoning quality, relying primarily on supervised fine-tuning with weak reward design. To address these challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative Policy Optimization (GRPO) to systematically optimize both reasoning and execution. Specifically, we design an RLVR-based post-training strategy with verifiable rewards for region alignment, trajectory consistency, and output formatting, thereby strengthening reasoning robustness and execution accuracy. Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides chain-of-thought supervision explicitly aligned with affordance and trajectory annotations. Furthermore, extensive evaluations on in-domain, out-of-domain, simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior generalization and real-world performance compared to prior VLA methods. We plan to release the model, code, and dataset following the publication of this work. Code: this https URL. Website: this https URL.</li>
<li><strong>摘要：</strong>视觉语言动作（VLA）模型旨在统一感知，语言理解和行动产生，提供强大的交叉任务和跨场概括，并对体现的AI产生广泛的影响。但是，当前的VLA模型通常缺乏明确的逐步推理，而是在不考虑负担限制或几何关系的情况下发出最终动作。他们的训练后管道也很少加强推理质量，主要依靠较弱的奖励设计受到监督的微调。为了应对这些挑战，我们提出了VLA-R1，这是一种推理增强的VLA，将增强奖励从可验证的奖励（RLVR）与组相对策略优化（GRPO）集成在一起，以系统地优化推理和执行。具体而言，我们设计了一个基于RLVR的训练后策略，并具有可验证的奖励，可用于区域对齐，轨迹一致性和输出格式，从而增强了推理的鲁棒性和执行精度。此外，我们开发了VLA-COT-13K，这是一种高质量的数据集，可显式地提供与负担能力和轨迹注释相结合的经过经过经过思考的监督。此外，对内域，室外，仿真和现实机器人平台的广泛评估表明，与先前的VLA方法相比，VLA-R1可以实现卓越的概括和现实世界的性能。我们计划在此工作发布后发布模型，代码和数据集。代码：此HTTPS URL。网站：此HTTPS URL。</li>
</ul>

<h3>Title: Detecting Post-generation Edits to Watermarked LLM Outputs via Combinatorial Watermarking</h3>
<ul>
<li><strong>Authors: </strong>Liyan Xie, Muhammad Siddeek, Mohamed Seif, Andrea J. Goldsmith, Mengdi Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01637">https://arxiv.org/abs/2510.01637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01637">https://arxiv.org/pdf/2510.01637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01637]] Detecting Post-generation Edits to Watermarked LLM Outputs via Combinatorial Watermarking(https://arxiv.org/abs/2510.01637)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Watermarking has become a key technique for proprietary language models, enabling the distinction between AI-generated and human-written text. However, in many real-world scenarios, LLM-generated content may undergo post-generation edits, such as human revisions or even spoofing attacks, making it critical to detect and localize such modifications. In this work, we introduce a new task: detecting post-generation edits locally made to watermarked LLM outputs. To this end, we propose a combinatorial pattern-based watermarking framework, which partitions the vocabulary into disjoint subsets and embeds the watermark by enforcing a deterministic combinatorial pattern over these subsets during generation. We accompany the combinatorial watermark with a global statistic that can be used to detect the watermark. Furthermore, we design lightweight local statistics to flag and localize potential edits. We introduce two task-specific evaluation metrics, Type-I error rate and detection accuracy, and evaluate our method on open-source LLMs across a variety of editing scenarios, demonstrating strong empirical performance in edit localization.</li>
<li><strong>摘要：</strong>水印已成为专有语言模型的关键技术，从而实现了AI生成和人工写的文本之间的区别。但是，在许多实际情况下，LLM生成的内容可能会经过后期的编辑，例如人类修订甚至欺骗攻击，因此检测和本地化此类修改至关重要。在这项工作中，我们介绍了一项新任务：检测对以水印的LLM输出为当地进行的后生成编辑。为此，我们提出了一个基于组合图案的水印框架，该框架将词汇分配为不相交的子集，并通过在发电期间对这些子集进行确定性的组合模式来嵌入水印。我们伴随组合水印具有可用于检测水印的全球统计数据。此外，我们设计了轻巧的本地统计数据，以标记和本地化潜在的编辑。我们介绍了两个特定于任务的评估指标，I型错误率和检测准确性，并在各种编辑方案中评估了我们在开源LLMS上的方法，这表明在编辑本地化中表现出强烈的经验性能。</li>
</ul>

<h3>Title: FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion Deblurring</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyang Liu, Zhengyan Zhou, Zihang Xu, Jiezhang Cao, Zheng Chen, Yulun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01641">https://arxiv.org/abs/2510.01641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01641">https://arxiv.org/pdf/2510.01641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01641]] FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion Deblurring(https://arxiv.org/abs/2510.01641)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in image motion deblurring, driven by CNNs and transformers, have made significant progress. Large-scale pre-trained diffusion models, which are rich in true-world modeling, have shown great promise for high-quality image restoration tasks such as deblurring, demonstrating stronger generative capabilities than CNN and transformer-based methods. However, challenges such as unbearable inference time and compromised fidelity still limit the full potential of the diffusion models. To address this, we introduce FideDiff, a novel single-step diffusion model designed for high-fidelity deblurring. We reformulate motion deblurring as a diffusion-like process where each timestep represents a progressively blurred image, and we train a consistency model that aligns all timesteps to the same clean image. By reconstructing training data with matched blur trajectories, the model learns temporal consistency, enabling accurate one-step deblurring. We further enhance model performance by integrating Kernel ControlNet for blur kernel estimation and introducing adaptive timestep prediction. Our model achieves superior performance on full-reference metrics, surpassing previous diffusion-based methods and matching the performance of other state-of-the-art models. FideDiff offers a new direction for applying pre-trained diffusion models to high-fidelity image restoration tasks, establishing a robust baseline for further advancing diffusion models in real-world industrial applications. Our dataset and code will be available at this https URL.</li>
<li><strong>摘要：</strong>由CNN和变形金刚驱动的图像运动去缩合的最新进展取得了重大进展。富含真实世界建模的大规模预训练的扩散模型对高质量的图像恢复任务（例如DeBlurring）表现出了巨大的希望，比CNN和基于变压器的方法表现出更强的生成能力。但是，诸如无法忍受的推理时间和损害忠诚之类的挑战仍然限制了扩散模型的全部潜力。为了解决这个问题，我们介绍了Fidediff，这是一种新型的单步扩散模型，专为高保真脱张。我们将运动脱毛重新制定为一个类似扩散的过程，每个时间步长代表逐渐模糊的图像，并且我们训练一个一致性模型，该模型将所有时间段与相同的干净图像保持一致。通过使用匹配的模糊轨迹重建训练数据，该模型可以学习时间一致性，从而实现准确的一步脱毛。我们通过集成内核控制网络来进一步增强模型性能，以进行模糊内核估计并引入自适应时间段预测。我们的模型在全参考指标上实现了卓越的性能，超过了以前的基于扩散的方法并匹配其他最先进模型的性能。 Fidediff提供了一个新的方向，用于将预训练的扩散模型应用于高保真图像恢复任务，从而建立了可靠的基线，以进一步推进现实世界中的工业应用中的扩散模型。我们的数据集和代码将在此HTTPS URL上可用。</li>
</ul>

<h3>Title: UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Jin Cao, Hongrui Wu, Ziyong Feng, Hujun Bao, Xiaowei Zhou, Sida Peng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01669">https://arxiv.org/abs/2510.01669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01669">https://arxiv.org/pdf/2510.01669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01669]] UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction(https://arxiv.org/abs/2510.01669)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>This paper tackles the challenge of robust reconstruction, i.e., the task of reconstructing a 3D scene from a set of inconsistent multi-view images. Some recent works have attempted to simultaneously remove image inconsistencies and perform reconstruction by integrating image degradation modeling into neural 3D scene this http URL, these methods rely heavily on dense observations for robustly optimizing model this http URL address this issue, we propose to decouple robust reconstruction into two subtasks: restoration and reconstruction, which naturally simplifies the optimization this http URL this end, we introduce UniVerse, a unified framework for robust reconstruction based on a video diffusion model. Specifically, UniVerse first converts inconsistent images into initial videos, then uses a specially designed video diffusion model to restore them into consistent images, and finally reconstructs the 3D scenes from these restored this http URL with case-by-case per-view degradation modeling, the diffusion model learns a general scene prior from large-scale data, making it applicable to diverse image this http URL experiments on both synthetic and real-world datasets demonstrate the strong generalization capability and superior performance of our method in robust reconstruction. Moreover, UniVerse can control the style of the reconstructed 3D scene. Project page: this https URL</li>
<li><strong>摘要：</strong>本文解决了可靠重建的挑战，即，从一组不一致的多视图图像中重建3D场景的任务。 Some recent works have attempted to simultaneously remove image inconsistencies and perform reconstruction by integrating image degradation modeling into neural 3D scene this http URL, these methods rely heavily on dense observations for robustly optimizing model this http URL address this issue, we propose to decouple robust reconstruction into two subtasks: restoration and reconstruction, which naturally simplifies the optimization this HTTP URL此目的，我们介绍了Universe，这是基于视频扩散模型的稳健重建统一框架。具体而言，宇宙首先将不一致的图像转换为初始视频，然后使用特殊设计的视频扩散模型将它们恢复为一致的图像，最后重建了这些图像，并重建了这些图像，并重建了这些图像，这些场景恢复了此HTTP URL，并通过逐案例的每个观看案例逐步降级降级模型，该模型在大型现场中学习了这一范围，使其与大型图像进行了多样化，以使其与该模型进行了多样化，以使其与众不同。现实世界的数据集证明了我们方法在鲁棒重建中的强大概括能力和出色的性能。此外，Universe可以控制重建的3D场景的样式。项目页面：此HTTPS URL</li>
</ul>

<h3>Title: MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment Abilities in MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Jiyao Liu, Jinjie Wei, Wanying Qu, Chenglong Ma, Junzhi Ning, Yunheng Li, Ying Chen, Xinzhe Luo, Pengcheng Chen, Xin Gao, Ming Hu, Huihui Xu, Xin Wang, Shujian Gao, Dingkang Yang, Zhongying Deng, Jin Ye, Lihao Liu, Junjun He, Ningsheng Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01691">https://arxiv.org/abs/2510.01691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01691">https://arxiv.org/pdf/2510.01691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01691]] MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment Abilities in MLLMs(https://arxiv.org/abs/2510.01691)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Medical Image Quality Assessment (IQA) serves as the first-mile safety gate for clinical AI, yet existing approaches remain constrained by scalar, score-based metrics and fail to reflect the descriptive, human-like reasoning process central to expert evaluation. To address this gap, we introduce MedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning paradigm for language-based evaluation of medical image quality with Multi-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary tasks: (1) MedQ-Perception, which probes low-level perceptual capability via human-curated questions on fundamental visual attributes; and (2) MedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks, aligning model evaluation with human-like reasoning on image quality. The benchmark spans five imaging modalities and over forty quality attributes, totaling 2,600 perceptual queries and 708 reasoning assessments, covering diverse image sources including authentic clinical acquisitions, images with simulated degradations via physics-based reconstructions, and AI-generated images. To evaluate reasoning ability, we propose a multi-dimensional judging protocol that assesses model outputs along four complementary axes. We further conduct rigorous human-AI alignment validation by comparing LLM-based judgement with radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates that models exhibit preliminary but unstable perceptual and reasoning skills, with insufficient accuracy for reliable clinical use. These findings highlight the need for targeted optimization of MLLMs in medical IQA. We hope that MedQ-Bench will catalyze further exploration and unlock the untapped potential of MLLMs for medical image quality evaluation.</li>
<li><strong>摘要：</strong>医疗图像质量评估（IQA）是临床AI的第一英里安全门，但现有方法仍受到标量，基于得分的指标的约束，并且无法反映出专家评估中心的描述性，类似人类的推理过程。为了解决这一差距，我们介绍了MEDQ Bench，这是一个全面的基准，该基准建立了具有多模式大语模型（MLLM）的基于语言的医学图像质量评估的感知范式。 MEDQ BENCH定义了两个互补任务：（1）MEDQ-pecception，它通过针对基本视觉属性的人类策划的问题探讨低级感知能力； （2）MEDQ-REOMING，包括无参考和比较推理任务，将模型评估与图像质量的类似人类的推理保持一致。基准测试涵盖了五种成像方式和40多种质量属性，总共有2,600个感知查询和708次推理评估，涵盖了包括真实的临床获取，包括真实的临床获取，通过物理基于物理的重建进行模拟降解的图像，以及AI生成的图像。为了评估推理能力，我们提出了一种多维判断协议，该协议评估沿四个互补轴的模型输出。我们通过将基于LLM的判断与放射科医生进行比较，进一步进行了严格的人类对准验证。我们对14个最先进的MLLM的评估表明，模型表现出初步但不稳定的感知和推理能力，并且无法获得可靠的临床使用精度。这些发现凸显了在医学IQA中有针对性优化MLLM的需求。我们希望MEDQ板凳能够促进进一步的探索并解锁MLLM在医疗图像质量评估中的未开发潜力。</li>
</ul>

<h3>Title: Finite-Time Bounds for Distributionally Robust TD Learning with Linear Function Approximation</h3>
<ul>
<li><strong>Authors: </strong>Saptarshi Mandal, Yashaswini Murthy, R. Srikant</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01721">https://arxiv.org/abs/2510.01721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01721">https://arxiv.org/pdf/2510.01721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01721]] Finite-Time Bounds for Distributionally Robust TD Learning with Linear Function Approximation(https://arxiv.org/abs/2510.01721)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Distributionally robust reinforcement learning (DRRL) focuses on designing policies that achieve good performance under model uncertainties. In particular, we are interested in maximizing the worst-case long-term discounted reward, where the data for RL comes from a nominal model while the deployed environment can deviate from the nominal model within a prescribed uncertainty set. Existing convergence guarantees for robust temporal-difference (TD) learning for policy evaluation are limited to tabular MDPs or are dependent on restrictive discount-factor assumptions when function approximation is used. We present the first robust TD learning with linear function approximation, where robustness is measured with respect to the total-variation distance and Wasserstein-l distance uncertainty set. Additionally, our algorithm is both model-free and does not require generative access to the MDP. Our algorithm combines a two-time-scale stochastic-approximation update with an outer-loop target-network update. We establish an $\tilde{O}(1/\epsilon^2)$ sample complexity to obtain an $\epsilon$-accurate value estimate. Our results close a key gap between the empirical success of robust RL algorithms and the non-asymptotic guarantees enjoyed by their non-robust counterparts. The key ideas in the paper also extend in a relatively straightforward fashion to robust Q-learning with function approximation.</li>
<li><strong>摘要：</strong>分配强大的增强学习（DRRL）的重点是设计在模型不确定性下实现良好性能的政策。特别是，我们有兴趣最大化最差的长期折扣奖励，其中RL的数据来自名义模型，而部署的环境可能会偏离规定的不确定性集中的名义模型。用于策略评估的鲁棒时间差异（TD）学习的现有收敛保证仅限于表格MDP，或者在使用功能近似时取决于限制性折现因子假设。我们介绍了第一个具有线性函数近似的鲁棒性TD学习，其中根据总变化距离进行了鲁棒性和Wasserstein-L距离不确定性集。此外，我们的算法既无模型，又不需要生成对MDP的访问。我们的算法结合了两次尺度随机交易的更新与外环目标网络更新。我们建立了一个$ \ tilde {o}（1/\ epsilon^2）$样本复杂性，以获得$ \ epsilon $ -Carcurate值估算。我们的结果缩小了强大的RL算法的经验成功与其非持姿势对应物所享有的非反应保证之间的重要差距。本文中的关键思想还以相对简单的方式扩展到功能近似的稳健Q学习。</li>
</ul>

<h3>Title: Unsupervised Dynamic Feature Selection for Robust Latent Spaces in Vision Tasks</h3>
<ul>
<li><strong>Authors: </strong>Bruno Corcuera, Carlos Eiras-Franco, Brais Cancela</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01758">https://arxiv.org/abs/2510.01758</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01758">https://arxiv.org/pdf/2510.01758</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01758]] Unsupervised Dynamic Feature Selection for Robust Latent Spaces in Vision Tasks(https://arxiv.org/abs/2510.01758)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Latent representations are critical for the performance and robustness of machine learning models, as they encode the essential features of data in a compact and informative manner. However, in vision tasks, these representations are often affected by noisy or irrelevant features, which can degrade the model's performance and generalization capabilities. This paper presents a novel approach for enhancing latent representations using unsupervised Dynamic Feature Selection (DFS). For each instance, the proposed method identifies and removes misleading or redundant information in images, ensuring that only the most relevant features contribute to the latent space. By leveraging an unsupervised framework, our approach avoids reliance on labeled data, making it broadly applicable across various domains and datasets. Experiments conducted on image datasets demonstrate that models equipped with unsupervised DFS achieve significant improvements in generalization performance across various tasks, including clustering and image generation, while incurring a minimal increase in the computational cost.</li>
<li><strong>摘要：</strong>潜在表示对机器学习模型的性能和鲁棒性至关重要，因为它们以紧凑而有益的方式编码数据的基本特征。但是，在视觉任务中，这些表示通常会受到嘈杂或无关的特征的影响，这可能会降低模型的性能和概括能力。本文提出了一种使用无监督的动态特征选择（DFS）来增强潜在表示的新方法。对于每个实例，所提出的方法都会标识并消除图像中的误导或冗余信息，从而确保只有最相关的功能有助于潜在空间。通过利用无监督的框架，我们的方法避免了对标记数据的依赖，从而广泛适用于各个域和数据集。在图像数据集上进行的实验表明，配备了无监督DFS的模型在各种任务（包括聚类和图像生成）上的泛化性能方面取得了重大改进，同时导致计算成本的最小增加。</li>
</ul>

<h3>Title: Pack and Force Your Memory: Long-form and Consistent Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiaofei Wu, Guozhen Zhang, Zhiyong Xu, Yuan Zhou, Qinglin Lu, Xuming He</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01784">https://arxiv.org/abs/2510.01784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01784">https://arxiv.org/pdf/2510.01784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01784]] Pack and Force Your Memory: Long-form and Consistent Video Generation(https://arxiv.org/abs/2510.01784)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Long-form video generation presents a dual challenge: models must capture long-range dependencies while preventing the error accumulation inherent in autoregressive decoding. To address these challenges, we make two contributions. First, for dynamic context modeling, we propose MemoryPack, a learnable context-retrieval mechanism that leverages both textual and image information as global guidance to jointly model short- and long-term dependencies, achieving minute-level temporal consistency. This design scales gracefully with video length, preserves computational efficiency, and maintains linear complexity. Second, to mitigate error accumulation, we introduce Direct Forcing, an efficient single-step approximating strategy that improves training-inference alignment and thereby curtails error propagation during inference. Together, MemoryPack and Direct Forcing substantially enhance the context consistency and reliability of long-form video generation, advancing the practical usability of autoregressive video models.</li>
<li><strong>摘要：</strong>长格式视频生成提出了双重挑战：模型必须捕获长距离依赖性，同时防止自回归解码固有的错误积累。为了应对这些挑战，我们做出了两项贡献。首先，对于动态上下文建模，我们提出了MemoryPack，MemoryPack是一种可学习的上下文 - 回归机制，它利用文本和图像信息作为全局指导，以共同对短期和长期依赖性建模，实现分钟级的时间一致性。该设计以视频长度优雅地缩放，保留计算效率并保持线性复杂性。其次，为了减轻错误积累，我们引入了直接强迫，这是一种有效的单步近似策略，可改善训练 - 推导对准，从而减少推理过程中的错误传播。 Memory Pack和Direct强迫共同提高了长期视频生成的上下文一致性和可靠性，从而提高了自动回归视频模型的实际可用性。</li>
</ul>

<h3>Title: Sensitivity, Specificity, and Consistency: A Tripartite Evaluation of Privacy Filters for Synthetic Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Adil Koeken, Alexander Ziller, Moritz Knolle, Daniel Rueckert</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01793">https://arxiv.org/abs/2510.01793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01793">https://arxiv.org/pdf/2510.01793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01793]] Sensitivity, Specificity, and Consistency: A Tripartite Evaluation of Privacy Filters for Synthetic Data Generation(https://arxiv.org/abs/2510.01793)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The generation of privacy-preserving synthetic datasets is a promising avenue for overcoming data scarcity in medical AI research. Post-hoc privacy filtering techniques, designed to remove samples containing personally identifiable information, have recently been proposed as a solution. However, their effectiveness remains largely unverified. This work presents a rigorous evaluation of a filtering pipeline applied to chest X-ray synthesis. Contrary to claims from the original publications, our results demonstrate that current filters exhibit limited specificity and consistency, achieving high sensitivity only for real images while failing to reliably detect near-duplicates generated from training data. These results demonstrate a critical limitation of post-hoc filtering: rather than effectively safeguarding patient privacy, these methods may provide a false sense of security while leaving unacceptable levels of patient information exposed. We conclude that substantial advances in filter design are needed before these methods can be confidently deployed in sensitive applications.</li>
<li><strong>摘要：</strong>保护隐私的合成数据集的产生是克服医学AI研究中数据稀缺的有前途的途径。 HOC后隐私过滤技术旨在删除包含个人身份信息的样本，最近被提议作为解决方案。但是，它们的有效性在很大程度上没有得到验证。这项工作对应用于胸部X射线合成的过滤管道进行了严格的评估。与原始出版物的主张相反，我们的结果表明，当前的过滤器具有有限的特异性和一致性，仅对真实图像实现了高灵敏度，同时未能可靠地检测到训练数据产生的近乎解说。这些结果证明了事后过滤的关键局限性：这些方法不是有效地保护患者的隐私，而是可以提供错误的安全感，同时留下无法接受的患者信息。我们得出的结论是，在敏感应用中，这些方法可以确定地部署到滤波器设计中，需要实质性的进步。</li>
</ul>

<h3>Title: Rethinking the shape convention of an MLP</h3>
<ul>
<li><strong>Authors: </strong>Meng-Hsi Chen, Yu-Ang Lee, Feng-Ting Liao, Da-shan Shiu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01796">https://arxiv.org/abs/2510.01796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01796">https://arxiv.org/pdf/2510.01796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01796]] Rethinking the shape convention of an MLP(https://arxiv.org/abs/2510.01796)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Multi-layer perceptrons (MLPs) conventionally follow a narrow-wide-narrow design where skip connections operate at the input/output dimensions while processing occurs in expanded hidden spaces. We challenge this convention by proposing wide-narrow-wide (Hourglass) MLP blocks where skip connections operate at expanded dimensions while residual computation flows through narrow bottlenecks. This inversion leverages higher-dimensional spaces for incremental refinement while maintaining computational efficiency through parameter-matched designs. Implementing Hourglass MLPs requires an initial projection to lift input signals to expanded dimensions. We propose that this projection can remain fixed at random initialization throughout training, enabling efficient training and inference implementations. We evaluate both architectures on generative tasks over popular image datasets, characterizing performance-parameter Pareto frontiers through systematic architectural search. Results show that Hourglass architectures consistently achieve superior Pareto frontiers compared to conventional designs. As parameter budgets increase, optimal Hourglass configurations favor deeper networks with wider skip connections and narrower bottlenecks-a scaling pattern distinct from conventional MLPs. Our findings suggest reconsidering skip connection placement in modern architectures, with potential applications extending to Transformers and other residual networks.</li>
<li><strong>摘要：</strong>多层感知器（MLP）常规遵循狭窄的纳罗设计，其中跳过连接在输入/输出尺寸上运行，而处理在扩展的隐藏空间中进行。我们通过提出在整个Narrow（沙漏）MLP块中提出挑战，其中跳过连接在扩展的尺寸下运行，而残留的计算流经狭窄的瓶颈。这种反演利用较高维的空间进行增量，同时通过参数匹配的设计保持计算效率。实施沙漏MLP需要初始投影，以提高输入信号到扩展的尺寸。我们建议在整个培训过程中，该预测可以保持在随机初始化，从而实现有效的培训和推理实施。我们通过系统的体系结构搜索来表征性能 - 参数帕累托边界，从而在流行的图像数据集上评估了两种架构。结果表明，与传统设计相比，沙漏体系结构始终达到了优越的帕累托边界。随着参数预算的增加，最佳的沙漏配置有利于更深的跳过连接和较窄的瓶颈 -  A缩放模式与常规MLP不同。我们的发现表明，重新考虑了现代体系结构中的跳过连接位置，潜在的应用程序扩展到了变压器和其他残留网络。</li>
</ul>

<h3>Title: Black-Box Combinatorial Optimization with Order-Invariant Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Olivier Goudet, Quentin Suire, Adrien Goëffon, Frédéric Saubion, Sylvain Lamprier</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01824">https://arxiv.org/abs/2510.01824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01824">https://arxiv.org/pdf/2510.01824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01824]] Black-Box Combinatorial Optimization with Order-Invariant Reinforcement Learning(https://arxiv.org/abs/2510.01824)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We introduce an order-invariant reinforcement learning framework for black-box combinatorial optimization. Classical estimation-of-distribution algorithms (EDAs) often rely on learning explicit variable dependency graphs, which can be costly and fail to capture complex interactions efficiently. In contrast, we parameterize a multivariate autoregressive generative model trained without a fixed variable ordering. By sampling random generation orders during training - a form of information-preserving dropout - the model is encouraged to be invariant to variable order, promoting search-space diversity and shaping the model to focus on the most relevant variable dependencies, improving sample efficiency. We adapt Generalized Reinforcement Policy Optimization (GRPO) to this setting, providing stable policy-gradient updates from scale-invariant advantages. Across a wide range of benchmark algorithms and problem instances of varying sizes, our method frequently achieves the best performance and consistently avoids catastrophic failures.</li>
<li><strong>摘要：</strong>我们引入了一个订单不变的增强学习框架，以实现黑框组合优化。经典分布算法（EDA）通常依赖于学习显式变量依赖图，这可能是昂贵的，并且无法有效地捕获复杂的相互作用。相比之下，我们将未经固定变量排序的多元自回归生成模型进行参数化。通过在训练过程中抽样随机生成订单 - 一种信息保护辍学的形式，该模型被鼓励是可变顺序不变的，促进搜索空间的多样性并塑造模型以专注于最相关的可变依赖性，从而提高了样本效率。我们将广义增强政策优化（GRPO）调整为此设置，从规模不变的优势提供稳定的政策梯度更新。在各种基准算法和不同大小的问题实例中，我们的方法经常达到最佳性能，并始终避免灾难性的失败。</li>
</ul>

<h3>Title: Compositional meta-learning through probabilistic task inference</h3>
<ul>
<li><strong>Authors: </strong>Jacob J. W. Bakermans, Pablo Tano, Reidar Riveland, Charles Findling, Alexandre Pouget</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01858">https://arxiv.org/abs/2510.01858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01858">https://arxiv.org/pdf/2510.01858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01858]] Compositional meta-learning through probabilistic task inference(https://arxiv.org/abs/2510.01858)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>To solve a new task from minimal experience, it is essential to effectively reuse knowledge from previous tasks, a problem known as meta-learning. Compositional solutions, where common elements of computation are flexibly recombined into new configurations, are particularly well-suited for meta-learning. Here, we propose a compositional meta-learning model that explicitly represents tasks as structured combinations of reusable computations. We achieve this by learning a generative model that captures the underlying components and their statistics shared across a family of tasks. This approach transforms learning a new task into a probabilistic inference problem, which allows for finding solutions without parameter updates through highly constrained hypothesis testing. Our model successfully recovers ground truth components and statistics in rule learning and motor learning tasks. We then demonstrate its ability to quickly infer new solutions from just single examples. Together, our framework joins the expressivity of neural networks with the data-efficiency of probabilistic inference to achieve rapid compositional meta-learning.</li>
<li><strong>摘要：</strong>为了从最少的经验中解决新任务，从以前的任务（一个称为元学习的问题）中有效地重复使用知识至关重要。组成溶液（在新的配置中都可以灵活地重新融合到新的配置中，组成溶液特别适合元学习。在这里，我们提出了一个组成元学习模型，该模型将任务明确表示为可重复使用的计算的结构化组合。我们通过学习一种生成模型来实现这一目标，该模型捕获了一系列任务中共享的基本组成部分及其统计数据。这种方法将学习新任务转换为概率推理问题，该问题允许通过高度约束的假设检验来查找无参数更新的解决方案。我们的模型成功地恢复了规则学习和运动学习任务中的地面真相组成部分和统计数据。然后，我们证明了其仅从单个示例中快速推断新解决方案的能力。我们的框架共同将神经网络的表达性与概率推断的数据效率相结合，以实现快速的组成元学习。</li>
</ul>

<h3>Title: Multi-marginal temporal Schrödinger Bridge Matching for video generation from unpaired data</h3>
<ul>
<li><strong>Authors: </strong>Thomas Gravier, Thomas Boyer, Auguste Genovesio</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01894">https://arxiv.org/abs/2510.01894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01894">https://arxiv.org/pdf/2510.01894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01894]] Multi-marginal temporal Schrödinger Bridge Matching for video generation from unpaired data(https://arxiv.org/abs/2510.01894)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Many natural dynamic processes -- such as in vivo cellular differentiation or disease progression -- can only be observed through the lens of static sample snapshots. While challenging, reconstructing their temporal evolution to decipher underlying dynamic properties is of major interest to scientific research. Existing approaches enable data transport along a temporal axis but are poorly scalable in high dimension and require restrictive assumptions to be met. To address these issues, we propose \textit{\textbf{Multi-Marginal temporal Schrödinger Bridge Matching}} (\textbf{MMtSBM}) \textit{for video generation from unpaired data}, extending the theoretical guarantees and empirical efficiency of Diffusion Schrödinger Bridge Matching (arXiv:archive/2303.16852) by deriving the Iterative Markovian Fitting algorithm to multiple marginals in a novel factorized fashion. Experiments show that MMtSBM retains theoretical properties on toy examples, achieves state-of-the-art performance on real world datasets such as transcriptomic trajectory inference in 100 dimensions, and for the first time recovers couplings and dynamics in very high dimensional image settings. Our work establishes multi-marginal Schrödinger bridges as a practical and principled approach for recovering hidden dynamics from static data.</li>
<li><strong>摘要：</strong>只能通过静态样品快照的晶状体观察到许多自然动态过程，例如体内细胞分化或疾病进展。在具有挑战性的同时，重建其时间演变以破译潜在的动态特性是科学研究的主要兴趣。现有方法可以沿时间轴沿着数据传输，但在高维度上的可扩展性较差，需要满足限制性假设。为了解决这些问题，我们提出\ textIt {\ textbf {多边缘时间schrödinger桥匹配}}}}}}（\ textbf {mmtsbm}）\ textit {从不合规数据中生成视频}，从未支配的数据}，扩展了理论和经验的档案库，以扩展了diffusion schrudfusion schrudfusion schrudder cridge/diffusion cridge 33.通过以新颖的分解方式将迭代的马尔可夫拟合算法推导到多个边缘。实验表明，MMTSBM在玩具示例上保留理论属性，在现实世界数据集上实现最新性能，例如100个维度的转录组轨迹推断，并且首次在非常高的尺寸图像设置中恢复耦合和动态。我们的工作建立了多 - 边界的Schrödinger桥，是一种从静态数据中恢复隐藏动态的实用和原则方法。</li>
</ul>

<h3>Title: Are LLMs Better GNN Helpers? Rethinking Robust Graph Learning under Deficiencies with Iterative Refinement</h3>
<ul>
<li><strong>Authors: </strong>Zhaoyan Wang, Zheng Gao, Arogya Kharel, In-Young Ko</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01910">https://arxiv.org/abs/2510.01910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01910">https://arxiv.org/pdf/2510.01910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01910]] Are LLMs Better GNN Helpers? Rethinking Robust Graph Learning under Deficiencies with Iterative Refinement(https://arxiv.org/abs/2510.01910)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) are widely adopted in Web-related applications, serving as a core technique for learning from graph-structured data, such as text-attributed graphs. Yet in real-world scenarios, such graphs exhibit deficiencies that substantially undermine GNN performance. While prior GNN-based augmentation studies have explored robustness against individual imperfections, a systematic understanding of how graph-native and Large Language Models (LLMs) enhanced methods behave under compound deficiencies is still missing. Specifically, there has been no comprehensive investigation comparing conventional approaches and recent LLM-on-graph frameworks, leaving their merits unclear. To fill this gap, we conduct the first empirical study that benchmarks these two lines of methods across diverse graph deficiencies, revealing overlooked vulnerabilities and challenging the assumption that LLM augmentation is consistently superior. Building on empirical findings, we propose Robust Graph Learning via Retrieval-Augmented Contrastive Refinement (RoGRAD) framework. Unlike prior one-shot LLM-as-Enhancer designs, RoGRAD is the first iterative paradigm that leverages Retrieval-Augmented Generation (RAG) to inject retrieval-grounded augmentations by supplying class-consistent, diverse augmentations and enforcing discriminative representations through iterative graph contrastive learning. It transforms LLM augmentation for graphs from static signal injection into dynamic refinement. Extensive experiments demonstrate RoGRAD's superiority over both conventional GNN- and LLM-enhanced baselines, achieving up to 82.43% average improvement.</li>
<li><strong>摘要：</strong>图形神经网络（GNN）在与Web相关的应用程序中广泛采用，是从图形结构化数据（例如文本属性图）中学习的核心技术。然而，在实际情况下，这些图表表现出严重破坏GNN性能的缺陷。虽然先前基于GNN的增强研究探索了针对单个缺陷的鲁棒性，但对图形本文和大型语言模型（LLMS）如何增强的方法在复合缺陷下仍缺失。具体而言，没有进行全面的调查，比较了传统方法和最近的llm-on-graph框架，因此他们的优点不清楚。为了填补这一空白，我们进行了首次实证研究，该研究基准了各种图形缺陷的这两条方法，揭示了被忽视的脆弱性，并挑战了LLM增强始终优势的假设。在经验发现的基础上，我们通过检索对比度改进（Rograd）框架提出了强大的图形学习。与先前的一杆LLM-As-As-Enhancer设计不同，Rograd是第一个迭代范式，它利用检索结果（RAG）来注入检索取回的增强，通过提供课堂持续的，多样的增强和通过迭代图形违规学习来提供持续的，多样化的增强和强制性歧视性表示。它将图形从静态信号注入的图形转化为动态改进。广泛的实验表明，Rograd在常规GNN-和LLM增强基础线方面的优势，达到平均改善高达82.43％。</li>
</ul>

<h3>Title: Flow-Matching Guided Deep Unfolding for Hyperspectral Image Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Yi Ai, Yuanhao Cai, Yulun Zhang, Xiaokang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01912">https://arxiv.org/abs/2510.01912</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01912">https://arxiv.org/pdf/2510.01912</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01912]] Flow-Matching Guided Deep Unfolding for Hyperspectral Image Reconstruction(https://arxiv.org/abs/2510.01912)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Hyperspectral imaging (HSI) provides rich spatial-spectral information but remains costly to acquire due to hardware limitations and the difficulty of reconstructing three-dimensional data from compressed measurements. Although compressive sensing systems such as CASSI improve efficiency, accurate reconstruction is still challenged by severe degradation and loss of fine spectral details. We propose the Flow-Matching-guided Unfolding network (FMU), which, to our knowledge, is the first to integrate flow matching into HSI reconstruction by embedding its generative prior within a deep unfolding framework. To further strengthen the learned dynamics, we introduce a mean velocity loss that enforces global consistency of the flow, leading to a more robust and accurate reconstruction. This hybrid design leverages the interpretability of optimization-based methods and the generative capacity of flow matching. Extensive experiments on both simulated and real datasets show that FMU significantly outperforms existing approaches in reconstruction quality. Code and models will be available at this https URL.</li>
<li><strong>摘要：</strong>高光谱成像（HSI）提供了丰富的空间光谱信息，但由于硬件限制以及从压缩度测量中重建三维数据的难度而获得的成本仍然很高。尽管诸如CASSI之类的压缩传感系统提高了效率，但准确的重建仍然受到严重降解和损失精细光谱细节的挑战。我们提出了流量匹配引导的展开网络（FMU），据我们所知，该网络是第一个将流量匹配到HSI重建中的流程，通过将其生成的先验嵌入到深层展开的框架中。为了进一步加强学习的动力学，我们引入了平均速度损失，从而实现了流动的全球一致性，从而导致了更强大，更准确的重建。这种混合设计利用了基于优化的方法的解释性和流量匹配的生成能力。对模拟数据集和真实数据集进行的广泛实验表明，FMU在重建质量方面的表现明显优于现有方法。代码和型号将在此HTTPS URL上可用。</li>
</ul>

<h3>Title: StelLA: Subspace Learning in Low-rank Adaptation using Stiefel Manifold</h3>
<ul>
<li><strong>Authors: </strong>Zhizhong Li, Sina Sajadmanesh, Jingtao Li, Lingjuan Lyu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01938">https://arxiv.org/abs/2510.01938</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01938">https://arxiv.org/pdf/2510.01938</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01938]] StelLA: Subspace Learning in Low-rank Adaptation using Stiefel Manifold(https://arxiv.org/abs/2510.01938)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Low-rank adaptation (LoRA) has been widely adopted as a parameter-efficient technique for fine-tuning large-scale pre-trained models. However, it still lags behind full fine-tuning in performance, partly due to its insufficient exploitation of the geometric structure underlying low-rank manifolds. In this paper, we propose a geometry-aware extension of LoRA that uses a three-factor decomposition $U\!SV^\top$. Analogous to the structure of singular value decomposition (SVD), it separates the adapter's input and output subspaces, $V$ and $U$, from the scaling factor $S$. Our method constrains $U$ and $V$ to lie on the Stiefel manifold, ensuring their orthonormality throughout the training. To optimize on the Stiefel manifold, we employ a flexible and modular geometric optimization design that converts any Euclidean optimizer to a Riemannian one. It enables efficient subspace learning while remaining compatible with existing fine-tuning pipelines. Empirical results across a wide range of downstream tasks, including commonsense reasoning, math and code generation, image classification, and image generation, demonstrate the superior performance of our approach against the recent state-of-the-art variants of LoRA. Code is available at this https URL.</li>
<li><strong>摘要：</strong>低级适应性（LORA）已被广泛用作用于微调大规模预训练模型的参数有效技术。但是，它仍然落后于全面的性能，部分原因是由于其对低级歧管基础的几何结构的开发不足。在本文中，我们提出了使用三因素分解$ u \！sv^\ top $的洛拉的几何感知扩展。类似于单数值分解（SVD）的结构，它将适配器的输入和输出子空间（$ v $和$ u $）与缩放因子$ s $分开。我们的方法限制了$ u $和$ v $，以便在Stiefel歧管上躺着，以确保其在整个培训过程中的正常情况。为了在Stiefel歧管上进行优化，我们采用了灵活而模块化的几何优化设计，将任何欧几里得优化器转换为Riemannian。它可以实现有效的子空间学习，同时与现有的微调管道保持兼容。跨多种下游任务的经验结果，包括常识性推理，数学和代码生成，图像分类和图像生成，证明了我们的方法与洛拉最近最新的变体相比。代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: $\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models</h3>
<ul>
<li><strong>Authors: </strong>Yujie Zhou, Pengyang Ling, Jiazi Bu, Yibin Wang, Yuhang Zang, Jiaqi Wang, Li Niu, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01982">https://arxiv.org/abs/2510.01982</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01982">https://arxiv.org/pdf/2510.01982</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01982]] $\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models(https://arxiv.org/abs/2510.01982)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The integration of online reinforcement learning (RL) into diffusion and flow models has recently emerged as a promising approach for aligning generative models with human preferences. Stochastic sampling via Stochastic Differential Equations (SDE) is employed during the denoising process to generate diverse denoising directions for RL exploration. While existing methods effectively explore potential high-value samples, they suffer from sub-optimal preference alignment due to sparse and narrow reward signals. To address these challenges, we propose a novel Granular-GRPO ($\text{G}^2$RPO ) framework that achieves precise and comprehensive reward assessments of sampling directions in reinforcement learning of flow models. Specifically, a Singular Stochastic Sampling strategy is introduced to support step-wise stochastic exploration while enforcing a high correlation between the reward and the injected noise, thereby facilitating a faithful reward for each SDE perturbation. Concurrently, to eliminate the bias inherent in fixed-granularity denoising, we introduce a Multi-Granularity Advantage Integration module that aggregates advantages computed at multiple diffusion scales, producing a more comprehensive and robust evaluation of the sampling directions. Experiments conducted on various reward models, including both in-domain and out-of-domain evaluations, demonstrate that our $\text{G}^2$RPO significantly outperforms existing flow-based GRPO baselines,highlighting its effectiveness and robustness.</li>
<li><strong>摘要：</strong>在线增强学习（RL）纳入扩散和流程模型的整合已成为将生成模型与人类偏好相结合的一种有希望的方法。通过随机微分方程（SDE）进行随机采样，在deoising过程中采用了用于RL探索的多种降解方向。尽管现有方法有效地探索了潜在的高价值样本，但由于稀疏和狭窄的奖励信号，它们遭受了次优的比对。为了应对这些挑战，我们提出了一个新颖的粒状grpo（$ \ text {g}^2 $ rpo）框架，该框架可以实现对流程模型增强学习中采样方向的精确而全面的奖励评估。具体而言，引入了一种奇异的随机抽样策略，以支持逐步的随机探索，同时实施奖励与注入噪声之间的高度相关性，从而促进每个SDE扰动的忠实奖励。同时，为了消除固定粒度denoising中固有的偏差，我们引入了一个多粒度优势集成模块，该模块集合以多个扩散量表计算出的优势，从而对采样方向产生更全面，更强大的评估。在包括内域和室外评估在内的各种奖励模型上进行的实验表明，我们的$ \ text {g}^2 $ rpo显着胜过现有的基于流动的GRPO基线，突出了其有效性和鲁棒性。</li>
</ul>

<h3>Title: PepCompass: Navigating peptide embedding spaces using Riemannian Geometry</h3>
<ul>
<li><strong>Authors: </strong>Marcin Możejko (1), Adam Bielecki (1), Jurand Prądzyński (1), Marcin Traskowski (1), Antoni Janowski (1), Karol Jurasz (1), Michał Kucharczyk (1), Hyun-Su Lee (2), Marcelo Der Torossian Torres (2), Cesar de la Fuente-Nunez (2), Paulina Szymczak (3), Michał Kmicikiewicz (3), Ewa Szczurek (1 and 3) ((1) University of Warsaw, (2) University of Pennsylvania, (3) Hemholtz Center Munich)</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01988">https://arxiv.org/abs/2510.01988</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01988">https://arxiv.org/pdf/2510.01988</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01988]] PepCompass: Navigating peptide embedding spaces using Riemannian Geometry(https://arxiv.org/abs/2510.01988)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Antimicrobial peptide discovery is challenged by the astronomical size of peptide space and the relative scarcity of active peptides. Generative models provide continuous latent "maps" of peptide space, but conventionally ignore decoder-induced geometry and rely on flat Euclidean metrics, rendering exploration and optimization distorted and inefficient. Prior manifold-based remedies assume fixed intrinsic dimensionality, which critically fails in practice for peptide data. Here, we introduce PepCompass, a geometry-aware framework for peptide exploration and optimization. At its core, we define a Union of $\kappa$-Stable Riemannian Manifolds $\mathbb{M}^{\kappa}$, a family of decoder-induced manifolds that captures local geometry while ensuring computational stability. We propose two local exploration methods: Second-Order Riemannian Brownian Efficient Sampling, which provides a convergent second-order approximation to Riemannian Brownian motion, and Mutation Enumeration in Tangent Space, which reinterprets tangent directions as discrete amino-acid substitutions. Combining these yields Local Enumeration Bayesian Optimization (LE-BO), an efficient algorithm for local activity optimization. Finally, we introduce Potential-minimizing Geodesic Search (PoGS), which interpolates between prototype embeddings along property-enriched geodesics, biasing discovery toward seeds, i.e. peptides with favorable activity. In-vitro validation confirms the effectiveness of PepCompass: PoGS yields four novel seeds, and subsequent optimization with LE-BO discovers 25 highly active peptides with broad-spectrum activity, including against resistant bacterial strains. These results demonstrate that geometry-informed exploration provides a powerful new paradigm for antimicrobial peptide design.</li>
<li><strong>摘要：</strong>抗菌肽的发现受到肽空间的天文大小和活性肽的相对稀缺性的挑战。生成模型提供了连续的肽空间的潜在“地图”，但通常忽略了解码器诱导的几何形状，并依靠平坦的欧几里得指标，使探索和优化扭曲和效率低下。先前基于歧管的补救措施假设固定的内在维度，这在肽数据的实践中严重失败。在这里，我们介绍了Pepcompass，这是用于肽探索和优化的几何感知框架。从本质上讲，我们定义了$ \ kappa $  - 稳定的riemannian歧管$ \ mathbb {m}^{\ kappa} $的结合，这是一个解码器诱导的歧管一家，可以在确保计算稳定性的同时捕获本地几何形状。我们提出了两种局部探索方法：二阶Riemannian Brownian有效抽样，该采样为Riemannian Brownian运动提供了收敛的二阶近似，并在切线空间中枚举了突变，将切线切换为离散的氨基酸替代方案。结合这些产生局部枚举贝叶斯优化（LE-BO），这是一种有效的局部活动优化算法。最后，我们引入了潜在的最小化测量搜索（POGS），该搜索（POGS）在沿富含特性的大地测量学沿原型嵌入的原型嵌入之间，将发现偏向种子，即具有良好活性的肽。体外验证证实了Pepcompass的有效性：POGS产生四个新种子，随后使用Le-Bo进行优化，发现25种具有广谱活性的高度活性肽，包括抵抗抗性细菌菌株。这些结果表明，几何形状的探索为抗菌肽设计提供了强大的新范式。</li>
</ul>

<h3>Title: Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing Routing in Lightweight Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Junyu Wu, Jie Tang, Jie Liu, Gangshan Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01997">https://arxiv.org/abs/2510.01997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01997">https://arxiv.org/pdf/2510.01997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01997]] Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing Routing in Lightweight Image Super-Resolution(https://arxiv.org/abs/2510.01997)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Image Super-Resolution (SR) aims to reconstruct high-resolution images from low-resolution counterparts, but the computational complexity of deep learning-based methods often hinders practical deployment. CAMixer is the pioneering work to integrate the advantages of existing lightweight SR methods and proposes a content-aware mixer to route token mixers of varied complexities according to the difficulty of content recovery. However, several limitations remain, such as poor adaptability, coarse-grained masking and spatial inflexibility, among others. We propose Pure-Pass (PP), a pixel-level masking mechanism that identifies pure pixels and exempts them from expensive computations. PP utilizes fixed color center points to classify pixels into distinct categories, enabling fine-grained, spatially flexible masking while maintaining adaptive flexibility. Integrated into the state-of-the-art ATD-light model, PP-ATD-light achieves superior SR performance with minimal overhead, outperforming CAMixer-ATD-light in reconstruction quality and parameter efficiency when saving a similar amount of computation.</li>
<li><strong>摘要：</strong>图像超分辨率（SR）旨在重建低分辨率对应物的高分辨率图像，但是基于深度学习的方法的计算复杂性通常会阻碍实际部署。 Camixer是整合现有轻质SR方法的优势的开创性工作，并建议使用内容感知的混合器，以根据内容恢复的难度来路由各种复杂性的令牌混音器。但是，仍然存在一些局限性，例如适应性差，粗粒掩盖和空间僵化性等。我们提出了Pure-Pass（PP），这是一种像素级掩蔽机制，可以识别纯像素并免除它们的昂贵计算。 PP利用固定的颜色中心点将像素分为不同的类别，在保持自适应柔韧性的同时，可以实现细粒度，空间柔性的遮罩。 PP-ATD-Light集成到最先进的ATD-Light模型中，在节省相似的计算量时，在重建质量和参数效率方面的表现最小，在重建质量和参数效率方面表现出色。</li>
</ul>

<h3>Title: Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output (SLSO) Framework</h3>
<ul>
<li><strong>Authors: </strong>Nanaka Hosokawa, Ryo Takahashi, Tomoya Kitano, Yukihiro Iida, Chisako Muramatsu, Tatsuro Hayashi, Yuta Seino, Xiangrong Zhou, Takeshi Hara, Akitoshi Katsumata, Hiroshi Fujita</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02001">https://arxiv.org/abs/2510.02001</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02001">https://arxiv.org/pdf/2510.02001</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02001]] Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output (SLSO) Framework(https://arxiv.org/abs/2510.02001)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In this study, we utilized the multimodal capabilities of OpenAI GPT-4o to automatically generate jaw cyst findings on dental panoramic radiographs. To improve accuracy, we constructed a Self-correction Loop with Structured Output (SLSO) framework and verified its effectiveness. A 10-step process was implemented for 22 cases of jaw cysts, including image input and analysis, structured data generation, tooth number extraction and consistency checking, iterative regeneration when inconsistencies were detected, and finding generation with subsequent restructuring and consistency verification. A comparative experiment was conducted using the conventional Chain-of-Thought (CoT) method across seven evaluation items: transparency, internal structure, borders, root resorption, tooth movement, relationships with other structures, and tooth number. The results showed that the proposed SLSO framework improved output accuracy for many items, with 66.9%, 33.3%, and 28.6% improvement rates for tooth number, tooth movement, and root resorption, respectively. In the successful cases, a consistently structured output was achieved after up to five regenerations. Although statistical significance was not reached because of the small size of the dataset, the overall SLSO framework enforced negative finding descriptions, suppressed hallucinations, and improved tooth number identification accuracy. However, the accurate identification of extensive lesions spanning multiple teeth is limited. Nevertheless, further refinement is required to enhance overall performance and move toward a practical finding generation system.</li>
<li><strong>摘要：</strong>在这项研究中，我们利用OpenAI GPT-4O的多模式能力自动在牙科全景X光片上产生下颌囊肿发现。为了提高准确性，我们构建了一个具有结构化输出（SLSO）框架的自校正环，并验证了其有效性。针对22例下颌囊肿实施了10步过程，包括图像输入和分析，结构化数据生成，牙齿数量提取和一致性检查，迭代后再生，当检测到不一致时，并发现了随后的重组和一致性验证的产生。在七个评估项目中，使用常规思考链（COT）方法进行了比较实验：透明度，内部结构，边界，根部吸收，牙齿运动，与其他结构的关系以及牙齿数量。结果表明，所提出的SLSO框架提高了许多项目的产出精度，牙齿数，牙齿运动和根吸收分别提高了66.9％，33.3％和28.6％。在成功的情况下，最多五个再生后，达到了一致的结构化输出。尽管由于数据集的尺寸较小，但未达到统计学意义，但总体SLSO框架执行了负面发现描述，抑制幻觉和提高的牙齿数识别精度。但是，跨越多牙的广泛病变的准确鉴定是有限的。然而，需要进一步的改进来增强整体性能并朝着实用的发现生成系统迈进。</li>
</ul>

<h3>Title: Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers</h3>
<ul>
<li><strong>Authors: </strong>Sahil Bhandary Karnoor, Romit Roy Choudhury</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02043">https://arxiv.org/abs/2510.02043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02043">https://arxiv.org/pdf/2510.02043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02043]] Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers(https://arxiv.org/abs/2510.02043)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Pose estimation refers to tracking a human's full body posture, including their head, torso, arms, and legs. The problem is challenging in practical settings where the number of body sensors are limited. Past work has shown promising results using conditional diffusion models, where the pose prediction is conditioned on both <location, rotation> measurements from the sensors. Unfortunately, nearly all these approaches generalize poorly across users, primarly because location measurements are highly influenced by the body size of the user. In this paper, we formulate pose estimation as an inverse problem and design an algorithm capable of zero-shot generalization. Our idea utilizes a pre-trained diffusion model and conditions it on rotational measurements alone; the priors from this model are then guided by a likelihood term, derived from the measured locations. Thus, given any user, our proposed InPose method generatively estimates the highly likely sequence of poses that best explains the sparse on-body measurements.</li>
<li><strong>摘要：</strong>姿势估计是指追踪人的全身姿势，包括他们的头，躯干，手臂和腿。在车身传感器数量有限的实际环境中，这个问题具有挑战性。过去的工作显示了使用条件扩散模型的有希望的结果，其中姿势预测在<位置，旋转>测量传感器的测量方面。不幸的是，几乎所有这些方法在整个用户之间概括不多，这主要是因为位置测量受到用户体型的高度影响。在本文中，我们将姿势估计作为一个反问题，并设计了能够零弹性化的算法。我们的想法利用了预先训练的扩散模型，并仅在旋转测量中对其进行调节。然后，该模型的先验者以源自测量位置得出的可能性术语为指导。因此，鉴于任何用户，我们提出的Inpose方法通常估计了极有可能的姿势序列，最能解释稀疏的体内测量值。</li>
</ul>

<h3>Title: Fine-Tuning Flow Matching via Maximum Likelihood Estimation of Reconstructions</h3>
<ul>
<li><strong>Authors: </strong>Zhaoyi Li, Jingtao Ding, Yong Li, Shihua Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02081">https://arxiv.org/abs/2510.02081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02081">https://arxiv.org/pdf/2510.02081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02081]] Fine-Tuning Flow Matching via Maximum Likelihood Estimation of Reconstructions(https://arxiv.org/abs/2510.02081)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Flow Matching (FM) algorithm achieves remarkable results in generative tasks especially in robotic manipulation. Building upon the foundations of diffusion models, the simulation-free paradigm of FM enables simple and efficient training, but inherently introduces a train-inference gap. Specifically, we cannot assess the model's output during the training phase. In contrast, other generative models including Variational Autoencoder (VAE), Normalizing Flow and Generative Adversarial Networks (GANs) directly optimize on the reconstruction loss. Such a gap is particularly evident in scenarios that demand high precision, such as robotic manipulation. Moreover, we show that FM's over-pursuit of straight predefined paths may introduce some serious problems such as stiffness into the system. These motivate us to fine-tune FM via Maximum Likelihood Estimation of reconstructions - an approach made feasible by FM's underlying smooth ODE formulation, in contrast to the stochastic differential equations (SDEs) used in diffusion models. This paper first theoretically analyzes the relation between training loss and inference error in FM. Then we propose a method of fine-tuning FM via Maximum Likelihood Estimation of reconstructions, which includes both straightforward fine-tuning and residual-based fine-tuning approaches. Furthermore, through specifically designed architectures, the residual-based fine-tuning can incorporate the contraction property into the model, which is crucial for the model's robustness and interpretability. Experimental results in image generation and robotic manipulation verify that our method reliably improves the inference performance of FM.</li>
<li><strong>摘要：</strong>流量匹配（FM）算法在生成任务中尤其是在机器人操作中取得了显着的结果。在扩散模型的基础上，FM的无模拟范式可实现简单有效的训练，但本质上引入了火车 - 介绍差距。具体来说，我们无法在训练阶段评估模型的输出。相反，其他生成模型，包括变异自动编码器（VAE），正常流量和生成对抗网络（GAN），直接对重建损失进行了优化。在需要高精度（例如机器人操纵）的情况下，这种差距尤其明显。此外，我们表明FM的直接义途径的超越可能会引入一些严重的问题，例如刚度进入系统。这些激励我们通过对重建的最大似然估计来微调FM  - 与扩散模型中使用的随机微分方程（SDE）相反，FM的基本平滑ODE配方可行的方法。本文理论上首先分析了FM中训练损失与推理误差之间的关系。然后，我们通过对重建的最大似然估计进行微调FM进行微调FM的方法，其中包括直接的微调和基于残留的微调方法。此外，通过专门设计的体系结构，基于残差的微调可以将收缩属性纳入模型，这对于模型的鲁棒性和可解释性至关重要。图像产生和机器人操纵的实验结果证明了我们的方法可靠地改善了FM的推理性能。</li>
</ul>

<h3>Title: Catalyst GFlowNet for electrocatalyst design: A hydrogen evolution reaction case study</h3>
<ul>
<li><strong>Authors: </strong>Lena Podina, Christina Humer, Alexandre Duval, Victor Schmidt, Ali Ramlaoui, Shahana Chatterjee, Yoshua Bengio, Alex Hernandez-Garcia, David Rolnick, Félix Therrien</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02142">https://arxiv.org/abs/2510.02142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02142">https://arxiv.org/pdf/2510.02142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02142]] Catalyst GFlowNet for electrocatalyst design: A hydrogen evolution reaction case study(https://arxiv.org/abs/2510.02142)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Efficient and inexpensive energy storage is essential for accelerating the adoption of renewable energy and ensuring a stable supply, despite fluctuations in sources such as wind and solar. Electrocatalysts play a key role in hydrogen energy storage (HES), allowing the energy to be stored as hydrogen. However, the development of affordable and high-performance catalysts for this process remains a significant challenge. We introduce Catalyst GFlowNet, a generative model that leverages machine learning-based predictors of formation and adsorption energy to design crystal surfaces that act as efficient catalysts. We demonstrate the performance of the model through a proof-of-concept application to the hydrogen evolution reaction, a key reaction in HES, for which we successfully identified platinum as the most efficient known catalyst. In future work, we aim to extend this approach to the oxygen evolution reaction, where current optimal catalysts are expensive metal oxides, and open the search space to discover new materials. This generative modeling framework offers a promising pathway for accelerating the search for novel and efficient catalysts.</li>
<li><strong>摘要：</strong>尽管风和太阳能等来源发生了波动，但有效且廉价的能源存储对于加速采用可再生能源并确保稳定供应至关重要。电催化剂在氢能（HES）中起关键作用，从而使能量作为氢气储存。但是，在此过程中开发负担得起的高性能催化剂仍然是一个重大挑战。我们介绍了Catalyst Gflownet，这是一种生成模型，该模型利用基于机器学习的形成和吸附能的预测指标来设计充当有效催化剂的晶体表面。我们通过对氢进化反应的概念验证施加了模型的性能，这是HES中的关键反应，我们成功地将铂鉴定为最有效的已知催化剂。在将来的工作中，我们旨在将这种方法扩展到氧气演化反应，当前最佳催化剂是昂贵的金属氧化物，并打开搜索空间以发现新材料。这个生成的建模框架为加速搜索新颖有效的催化剂提供了有希望的途径。</li>
</ul>

<h3>Title: Diffusion Transformers for Imputation: Statistical Efficiency and Uncertainty Quantification</h3>
<ul>
<li><strong>Authors: </strong>Zeqi Ye, Minshuo Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02216">https://arxiv.org/abs/2510.02216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02216">https://arxiv.org/pdf/2510.02216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02216]] Diffusion Transformers for Imputation: Statistical Efficiency and Uncertainty Quantification(https://arxiv.org/abs/2510.02216)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Imputation methods play a critical role in enhancing the quality of practical time-series data, which often suffer from pervasive missing values. Recently, diffusion-based generative imputation methods have demonstrated remarkable success compared to autoregressive and conventional statistical approaches. Despite their empirical success, the theoretical understanding of how well diffusion-based models capture complex spatial and temporal dependencies between the missing values and observed ones remains limited. Our work addresses this gap by investigating the statistical efficiency of conditional diffusion transformers for imputation and quantifying the uncertainty in missing values. Specifically, we derive statistical sample complexity bounds based on a novel approximation theory for conditional score functions using transformers, and, through this, construct tight confidence regions for missing values. Our findings also reveal that the efficiency and accuracy of imputation are significantly influenced by the missing patterns. Furthermore, we validate these theoretical insights through simulation and propose a mixed-masking training strategy to enhance the imputation performance.</li>
<li><strong>摘要：</strong>插补方法在增强实际时间序列数据的质量方面起着至关重要的作用，该数据通常会遭受普遍缺失的价值。最近，与自回归和常规统计方法相比，基于扩散的生成插补方法表现出了显着的成功。尽管取得了经验成功，但对基于扩散的模型如何捕获缺失值和观测值之间的复杂空间和时间依赖性的理论理解仍然有限。我们的工作通过研究条件扩散变压器的统计效率来解决这一差距，以插补并量化缺失值的不确定性。具体而言，我们基于使用变压器的条件分数函数的新型近似理论得出统计样本复杂性界限，并通过此得分构建了损失值的紧密置信区。我们的发现还表明，插补的效率和准确性受到丢失模式的显着影响。此外，我们通过模拟来验证这些理论见解，并提出一种混合掩蔽培训策略来增强归合性能。</li>
</ul>

<h3>Title: TempoControl: Temporal Attention Guidance for Text-to-Video Models</h3>
<ul>
<li><strong>Authors: </strong>Shira Schiber, Ofir Lindenbaum, Idan Schwartz</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02226">https://arxiv.org/abs/2510.02226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02226">https://arxiv.org/pdf/2510.02226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02226]] TempoControl: Temporal Attention Guidance for Text-to-Video Models(https://arxiv.org/abs/2510.02226)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative video models have enabled the creation of high-quality videos based on natural language prompts. However, these models frequently lack fine-grained temporal control, meaning they do not allow users to specify when particular visual elements should appear within a generated sequence. In this work, we introduce TempoControl, a method that allows for temporal alignment of visual concepts during inference, without requiring retraining or additional supervision. TempoControl utilizes cross-attention maps, a key component of text-to-video diffusion models, to guide the timing of concepts through a novel optimization approach. Our method steers attention using three complementary principles: aligning its temporal shape with a control signal (via correlation), amplifying it where visibility is needed (via energy), and maintaining spatial focus (via entropy). TempoControl allows precise control over timing while ensuring high video quality and diversity. We demonstrate its effectiveness across various video generation applications, including temporal reordering for single and multiple objects, as well as action and audio-aligned generation.</li>
<li><strong>摘要：</strong>生成视频模型的最新进展使基于自然语言提示的高质量视频创建了高质量的视频。但是，这些模型经常缺乏细粒度的时间控制，这意味着它们不允许用户指定何时应在生成的序列中出现特定的视觉元素。在这项工作中，我们介绍了Tempocontrol，这种方法允许在推理过程中进行时间对齐，而无需进行重新训练或其他监督。 Tempocontrol利用跨意义图（文本对视频扩散模型的关键组成部分）通过新颖的优化方法来指导概念的时机。我们的方法使用三个互补原理引导注意力：将其时间形状与控制信号（通过相关性）对齐，在需要（通过能量）（通过能量）的地方放大它，并保持空间焦点（通过熵）。 Tempocontrol可以精确控制时间，同时确保高视频质量和多样性。我们证明了其在各种视频生成应用程序中的有效性，包括单个和多个对象的时间重新排序，以及动作和音频一致的生成。</li>
</ul>

<h3>Title: PUL-Inter-slice Defender: An Anomaly Detection Solution for Distributed Slice Mobility Attacks</h3>
<ul>
<li><strong>Authors: </strong>Ricardo Misael Ayala Molina, Hyame Assem Alameddine, Makan Pourzandi, Chadi Assi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02236">https://arxiv.org/abs/2510.02236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02236">https://arxiv.org/pdf/2510.02236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02236]] PUL-Inter-slice Defender: An Anomaly Detection Solution for Distributed Slice Mobility Attacks(https://arxiv.org/abs/2510.02236)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Network Slices (NSs) are virtual networks operating over a shared physical infrastructure, each designed to meet specific application requirements while maintaining consistent Quality of Service (QoS). In Fifth Generation (5G) networks, User Equipment (UE) can connect to and seamlessly switch between multiple NSs to access diverse services. However, this flexibility, known as Inter-Slice Switching (ISS), introduces a potential vulnerability that can be exploited to launch Distributed Slice Mobility (DSM) attacks, a form of Distributed Denial of Service (DDoS) attack. To secure 5G networks and their NSs against DSM attacks, we present in this work, PUL-Inter-Slice Defender; an anomaly detection solution that leverages Positive Unlabeled Learning (PUL) and incorporates a combination of Long Short-Term Memory Autoencoders and K-Means clustering. PUL-Inter-Slice Defender leverages the Third Generation Partnership Project (3GPP) key performance indicators and performance measurement counters as features for its machine learning models to detect DSM attack variants while maintaining robustness in the presence of contaminated training data. When evaluated on data collected from our 5G testbed based on the open-source free5GC and UERANSIM, a UE/ Radio Access Network (RAN) simulator; PUL-Inter-Slice Defender achieved F1-scores exceeding 98.50% on training datasets with 10% to 40% attack contamination, consistently outperforming its counterpart Inter-Slice Defender and other PUL based solutions combining One-Class Support Vector Machine (OCSVM) with Random Forest and XGBoost.</li>
<li><strong>摘要：</strong>网络切片（NSS）是通过共享物理基础结构运行的虚拟网络，每个网络旨在满足特定的应用程序要求，同时保持一致的服务质量（QOS）。在第五代（5G）网络中，用户设备（UE）可以连接并在多个NSS之间无缝切换以访问多种服务。但是，这种灵活性被称为切片间切换（ISS），引入了一个潜在的漏洞，可以利用该漏洞来启动分布式切片移动性（DSM）攻击，这是一种分布式拒绝服务（DDOS）攻击的形式。为了确保5G网络及其NSS免受DSM攻击，我们在这项工作中提出了Pul-Inter-Slice Defender；一种利用积极的未标记学习（PUL）的异常检测解决方案，并结合了长期短期记忆自动编码器和K-均值聚类的组合。 Pul-Inter-Slice Defender利用第三代合作伙伴项目（3GPP）关键绩效指标和性能测量计数器作为其机器学习模型的功能，以检测DSM攻击变体，同时在存在受污染的培训数据的情况下保持稳健性。根据开源Free5GC和Ueransim从我们的5G测试床中收集的数据进行评估时，UE/无线电访问网络（RAN）模拟器；在10％至40％的攻击污染的训练数据集中，Pul-Inter-Slice防守者的F1得分超过98.50％，始终优于其对应机间辩护人和其他基于PUL的解决方案，结合了一个级别的支撑矢量机（OCSVM）与随机森林和Xgboost。</li>
</ul>

<h3>Title: DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing</h3>
<ul>
<li><strong>Authors: </strong>Zihan Zhou, Shilin Lu, Shuli Leng, Shaocong Zhang, Zhuming Lian, Xinlei Yu, Adams Wai-Kin Kong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02253">https://arxiv.org/abs/2510.02253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02253">https://arxiv.org/pdf/2510.02253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02253]] DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing(https://arxiv.org/abs/2510.02253)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Drag-based image editing has long suffered from distortions in the target region, largely because the priors of earlier base models, Stable Diffusion, are insufficient to project optimized latents back onto the natural image manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow matching (e.g., SD3.5, FLUX), generative priors have become significantly stronger, enabling advances across diverse editing tasks. However, drag-based editing has yet to benefit from these stronger priors. This work proposes the first framework to effectively harness FLUX's rich prior for drag-based editing, dubbed DragFlow, achieving substantial gains over baselines. We first show that directly applying point-based drag editing to DiTs performs poorly: unlike the highly compressed features of UNets, DiT features are insufficiently structured to provide reliable guidance for point-wise motion supervision. To overcome this limitation, DragFlow introduces a region-based editing paradigm, where affine transformations enable richer and more consistent feature supervision. Additionally, we integrate pretrained open-domain personalization adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving background fidelity through gradient mask-based hard constraints. Multimodal large language models (MLLMs) are further employed to resolve task ambiguities. For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench) featuring region-level dragging instructions. Extensive experiments on DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and region-based baselines, setting a new state-of-the-art in drag-based image editing. Code and datasets will be publicly available upon publication.</li>
<li><strong>摘要：</strong>基于阻力的图像编辑长期以来一直遭受目标区域的扭曲，这主要是因为早期基本模型的先验稳定扩散，不足以将优化潜在的潜伏期投影回自然图像歧管。随着从基于UNET的DDPM转变为具有流量匹配（例如SD3.5，通量）的更可扩展的DIT，生成的先验变得更加强大，从而在各种编辑任务中取得了进步。但是，基于阻力的编辑尚未从这些更强大的先验中受益。这项工作提出了第一个框架，以有效利用基于阻力的编辑（称为Dragflow）的富裕先验，从而在基线方面取得了可观的收益。我们首先表明，直接将基于点的阻力编辑应用于DIT的表现较差：与UNET的高度压缩功能不同，DIT功能的结构不足以为点的运动监督提供可靠的指导。为了克服这一限制，DragFlow引入了基于区域的编辑范式，在该范式中，仿射转换使更丰富，更一致的功能监督。此外，我们集成了经过预定的开放域个性化适配器（例如IP-Apapter），以增强主题一致性，同时通过基于梯度掩盖的硬约束来保留背景忠诚度。多模式大语模型（MLLM）进一步用于解决任务歧义。为了进行评估，我们策划了一个新型的基于区域的拖放基准（红色长凳），其中包含区域级拖动说明。在Dragbench-DR和Red Bench上进行的广泛实验表明，Dragflow超过了基于点和基于区域的基线，在基于阻力的图像编辑中设置了新的最新艺术。出版物后，代码和数据集将公开使用。</li>
</ul>

<h3>Title: NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual Reconstruction of Complex Scenes</h3>
<ul>
<li><strong>Authors: </strong>Shiyi Zhang, Dong Liang, Yihang Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02266">https://arxiv.org/abs/2510.02266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02266">https://arxiv.org/pdf/2510.02266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02266]] NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual Reconstruction of Complex Scenes(https://arxiv.org/abs/2510.02266)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Reconstructing visual information from brain activity via computer vision technology provides an intuitive understanding of visual neural mechanisms. Despite progress in decoding fMRI data with generative models, achieving accurate cross-subject reconstruction of visual stimuli remains challenging and computationally demanding. This difficulty arises from inter-subject variability in neural representations and the brain's abstract encoding of core semantic features in complex visual inputs. To address these challenges, we propose NeuroSwift, which integrates complementary adapters via diffusion: AutoKL for low-level features and CLIP for semantics. NeuroSwift's CLIP Adapter is trained on Stable Diffusion generated images paired with COCO captions to emulate higher visual cortex encoding. For cross-subject generalization, we pretrain on one subject and then fine-tune only 17 percent of parameters (fully connected layers) for new subjects, while freezing other components. This enables state-of-the-art performance with only one hour of training per subject on lightweight GPUs (three RTX 4090), and it outperforms existing methods.</li>
<li><strong>摘要：</strong>通过计算机视觉技术从大脑活动中重建视觉信息提供了对视觉神经机制的直观理解。尽管在用生成模型中解码fMRI数据方面取得了进展，但实现了视觉刺激的准确的跨主题重建仍然具有挑战性和计算要求。这个困难来自神经表示的受试者间变异性以及大脑在复杂的视觉输入中对核心语义特征的抽象编码。为了应对这些挑战，我们提出了Neuroswift，该神经自动链通过扩散来整合互补的适配器：低水平特征和语义剪辑的AutoKl。 Neuroswift的夹夹适配器经过稳定的扩散产生的图像和可可字幕的培训，以模仿较高的视觉皮层编码。为了进行跨受试者的概括，我们在一个受试者上预处理，然后仅对新受试者进行微调参数（完全连接的层），同时冻结其他组件。这可以使最先进的性能，每个受试者对轻质GPU（三个RTX 4090）的培训仅一小时，并且优于现有方法。</li>
</ul>

<h3>Title: Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods for Natural Language Generation</h3>
<ul>
<li><strong>Authors: </strong>Mykyta Ielanskyi, Kajetan Schweighofer, Lukas Aichberger, Sepp Hochreiter</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02279">https://arxiv.org/abs/2510.02279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02279">https://arxiv.org/pdf/2510.02279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02279]] Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods for Natural Language Generation(https://arxiv.org/abs/2510.02279)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Hallucinations are a common issue that undermine the reliability of large language models (LLMs). Recent studies have identified a specific subset of hallucinations, known as confabulations, which arise due to predictive uncertainty of LLMs. To detect confabulations, various methods for estimating predictive uncertainty in natural language generation (NLG) have been developed. These methods are typically evaluated by correlating uncertainty estimates with the correctness of generated text, with question-answering (QA) datasets serving as the standard benchmark. However, commonly used approximate correctness functions have substantial disagreement between each other and, consequently, in the ranking of the uncertainty estimation methods. This allows one to inflate the apparent performance of uncertainty estimation methods. We propose using several alternative risk indicators for risk correlation experiments that improve robustness of empirical assessment of UE algorithms for NLG. For QA tasks, we show that marginalizing over multiple LLM-as-a-judge variants leads to reducing the evaluation biases. Furthermore, we explore structured tasks as well as out of distribution and perturbation detection tasks which provide robust and controllable risk indicators. Finally, we propose to use an Elo rating of uncertainty estimation methods to give an objective summarization over extensive evaluation settings.</li>
<li><strong>摘要：</strong>幻觉是破坏大语言模型（LLM）可靠性的常见问题。最近的研究已经确定了幻觉的特定子集，称为综合，这是由于LLM的预测不确定性而引起的。为了检测综合，已经开发了各种用于估计自然语言产生（NLG）的预测不确定性的方法。这些方法通常是通过将不确定性估计与生成的文本的正确性以及问题解答（QA）数据集相关联的评估来评估的。但是，通常使用的近似正确性函数彼此之间存在实质性分歧，因此，在不确定性估计方法的排名中。这使人们可以夸大不确定性估计方法的明显性能。我们建议使用几种替代风险指标进行风险相关实验，以改善NLG的UE算法经验评估的鲁棒性。对于QA任务，我们表明，在多个LLM-AS-A-A-Gudge变体上进行边缘化会导致减少评估偏见。此外，我们探索结构化的任务以及分配和扰动检测任务，这些任务提供了可靠，可控制的风险指标。最后，我们建议使用不确定性估计方法的ELO等级，以对广泛的评估设置进行客观摘要。</li>
</ul>

<h3>Title: VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL</h3>
<ul>
<li><strong>Authors: </strong>Kyoungjun Park, Yifan Yang, Juheon Yi, Shicheng Zheng, Yifei Shen, Dongqi Han, Caihua Shan, Muhammad Muaz, Lili Qiu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02282">https://arxiv.org/abs/2510.02282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02282">https://arxiv.org/pdf/2510.02282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02282]] VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL(https://arxiv.org/abs/2510.02282)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of AI-generated videos, there is an urgent need for effective detection tools to mitigate societal risks such as misinformation and reputational harm. In addition to accurate classification, it is essential that detection models provide interpretable explanations to ensure transparency for regulators and end users. To address these challenges, we introduce VidGuard-R1, the first video authenticity detector that fine-tunes a multi-modal large language model (MLLM) using group relative policy optimization (GRPO). Our model delivers both highly accurate judgments and insightful reasoning. We curate a challenging dataset of 140k real and AI-generated videos produced by state-of-the-art generation models, carefully designing the generation process to maximize discrimination difficulty. We then fine-tune Qwen-VL using GRPO with two specialized reward models that target temporal artifacts and generation complexity. Extensive experiments demonstrate that VidGuard-R1 achieves state-of-the-art zero-shot performance on existing benchmarks, with additional training pushing accuracy above 95%. Case studies further show that VidGuard-R1 produces precise and interpretable rationales behind its predictions. The code is publicly available at this https URL.</li>
<li><strong>摘要：</strong>随着AI生成的视频的迅速发展，迫切需要有效的检测工具来减轻社会风险，例如错误的信息和声誉危害。除了准确的分类外，必须提供可解释的解释以确保监管机构和最终用户的透明度。为了应对这些挑战，我们介绍了Vidguard-R1，这是第一个使用组相对策略优化（GRPO）微调多模式大语言模型（MLLM）微调的视频真实性检测器。我们的模型提供了高度准确的判断和有见地的推理。我们策划了一个具有挑战性的数据集，该数据集由最先进的一代模型制作的140K真实和AI生成的视频，仔细设计生成过程，以最大程度地提高歧视难度。然后，我们使用GRPO和两个针对时间伪像和产生复杂性的专门奖励模型对QWEN-VL进行微调。广泛的实验表明，Vidguard-R1在现有基准测试中实现了最新的零球性能，额外的训练将精度提高到95％以上。案例研究进一步表明，Vidguard-R1在预测背后产生精确的可解释原理。该代码在此HTTPS URL上公开可用。</li>
</ul>

<h3>Title: Self-Forcing++: Towards Minute-Scale High-Quality Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li, Rui Wang, Andrew Bai, Yuanhao Ban, Cho-Jui Hsieh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02283">https://arxiv.org/abs/2510.02283</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02283">https://arxiv.org/pdf/2510.02283</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02283]] Self-Forcing++: Towards Minute-Scale High-Quality Video Generation(https://arxiv.org/abs/2510.02283)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models have revolutionized image and video generation, achieving unprecedented visual quality. However, their reliance on transformer architectures incurs prohibitively high computational costs, particularly when extending generation to long videos. Recent work has explored autoregressive formulations for long video generation, typically by distilling from short-horizon bidirectional teachers. Nevertheless, given that teacher models cannot synthesize long videos, the extrapolation of student models beyond their training horizon often leads to pronounced quality degradation, arising from the compounding of errors within the continuous latent space. In this paper, we propose a simple yet effective approach to mitigate quality degradation in long-horizon video generation without requiring supervision from long-video teachers or retraining on long video datasets. Our approach centers on exploiting the rich knowledge of teacher models to provide guidance for the student model through sampled segments drawn from self-generated long videos. Our method maintains temporal consistency while scaling video length by up to 20x beyond teacher's capability, avoiding common issues such as over-exposure and error-accumulation without recomputing overlapping frames like previous methods. When scaling up the computation, our method shows the capability of generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the maximum span supported by our base model's position embedding and more than 50x longer than that of our baseline model. Experiments on standard benchmarks and our proposed improved benchmark demonstrate that our approach substantially outperforms baseline methods in both fidelity and consistency. Our long-horizon videos demo can be found at this https URL</li>
<li><strong>摘要：</strong>扩散模型已彻底改变了图像和视频的产生，从而达到了前所未有的视觉质量。但是，他们对变压器体系结构的依赖会导致高昂的计算成本，尤其是在将一代延伸到长视频时。最近的工作探索了长期视频的自回旋配方，通常是通过从短距离双向教师中提取的。然而，鉴于教师模型无法综合长时间的视频，因此推断学生模型超出了他们的训练范围，通常会导致明显的质量降级，这是由于连续的潜在空间中错误的复杂性而引起的。在本文中，我们提出了一种简单而有效的方法，以减轻长途视频的质量退化，而无需长期视频老师的监督或在长视频数据集中进行重新培训。我们的方法集中在利用教师模型的丰富知识中，通过从自我生成的长视频中得出的采样段为学生模型提供指导。我们的方法保持时间一致性，同时将视频长度扩展到教师能力之外的20倍，避免了常见问题，例如过度曝光和错误蓄能，而无需重新计算以前的方法（如先前的方法）。在扩大计算时，我们的方法显示了生成最多4分15秒的视频的能力，相当于基本模型的位置嵌入的最大跨度的99.9％，并且比基线模型长50倍以上。对标准基准和我们提出的改进基准的实验表明，我们的方法在忠诚度和一致性方面基本上都优于基线方法。可以在此HTTPS URL上找到我们的长音视频演示</li>
</ul>

<h3>Title: Learning to Generate Object Interactions with Physics-Guided Video Diffusion</h3>
<ul>
<li><strong>Authors: </strong>David Romero, Ariana Bermudez, Hao Li, Fabio Pizzati, Ivan Laptev</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02284">https://arxiv.org/abs/2510.02284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02284">https://arxiv.org/pdf/2510.02284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02284]] Learning to Generate Object Interactions with Physics-Guided Video Diffusion(https://arxiv.org/abs/2510.02284)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent models for video generation have achieved remarkable progress and are now deployed in film, social media production, and advertising. Beyond their creative potential, such models also hold promise as world simulators for robotics and embodied decision making. Despite strong advances, however, current approaches still struggle to generate physically plausible object interactions and lack physics-grounded control mechanisms. To address this limitation, we introduce KineMask, an approach for physics-guided video generation that enables realistic rigid body control, interactions, and effects. Given a single image and a specified object velocity, our method generates videos with inferred motions and future object interactions. We propose a two-stage training strategy that gradually removes future motion supervision via object masks. Using this strategy we train video diffusion models (VDMs) on synthetic scenes of simple interactions and demonstrate significant improvements of object interactions in real scenes. Furthermore, KineMask integrates low-level motion control with high-level textual conditioning via predictive scene descriptions, leading to effective support for synthesis of complex dynamical phenomena. Extensive experiments show that KineMask achieves strong improvements over recent models of comparable size. Ablation studies further highlight the complementary roles of low- and high-level conditioning in VDMs. Our code, model, and data will be made publicly available.</li>
<li><strong>摘要：</strong>视频生成的最新模型取得了显着的进步，现在已在电影，社交媒体制作和广告中部署。除了创造性的潜力之外，这种模型还具有成为世界模拟者的机器人技术和具体决策的希望。然而，尽管进步很强，但目前的方法仍在难以产生物理上合理的对象相互作用并缺乏物理基础的控制机制。为了解决这一限制，我们介绍了Kinemask，这是一种物理引导的视频生成方法，可实现逼真的僵化身体控制，相互作用和效果。给定单个图像和指定的对象速度，我们的方法生成具有推断动作和未来对象相互作用的视频。我们提出了一种两阶段的培训策略，该策略逐渐通过对象面罩逐渐消除未来的运动监督。使用此策略，我们在简单相互作用的合成场景上训练视频扩散模型（VDM），并在真实场景中显示出对象相互作用的显着改善。此外，Kinemask通过预测场景描述将低级运动控制与高级文本调节整合，从而有效地支持了复杂动力学现象的综合。广泛的实验表明，Kinemask比最近大小的模型实现了强大的改进。消融研究进一步强调了VDM中低和高级条件的互补作用。我们的代码，模型和数据将公开可用。</li>
</ul>

<h3>Title: MultiModal Action Conditioned Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Yichen Li, Antonio Torralba</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02287">https://arxiv.org/abs/2510.02287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02287">https://arxiv.org/pdf/2510.02287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02287]] MultiModal Action Conditioned Video Generation(https://arxiv.org/abs/2510.02287)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Current video models fail as world model as they lack fine-graiend control. General-purpose household robots require real-time fine motor control to handle delicate tasks and urgent situations. In this work, we introduce fine-grained multimodal actions to capture such precise control. We consider senses of proprioception, kinesthesia, force haptics, and muscle activation. Such multimodal senses naturally enables fine-grained interactions that are difficult to simulate with text-conditioned generative models. To effectively simulate fine-grained multisensory actions, we develop a feature learning paradigm that aligns these modalities while preserving the unique information each modality provides. We further propose a regularization scheme to enhance causality of the action trajectory features in representing intricate interaction dynamics. Experiments show that incorporating multimodal senses improves simulation accuracy and reduces temporal drift. Extensive ablation studies and downstream applications demonstrate the effectiveness and practicality of our work.</li>
<li><strong>摘要：</strong>当前的视频模型失败了世界模型，因为它们缺乏善良的控制。通用家用机器人需要实时精细的运动控制，以应对精致的任务和紧急情况。在这项工作中，我们引入了细粒度的多模式动作，以捕获这种精确的控制。我们考虑了本体感受的感觉，动力学，力触觉和肌肉激活。这种多模式的感觉自然可以实现细粒的相互作用，这些相互作用很难使用文本条件的生成模型进行模拟。为了有效地模拟细粒度的多感官动作，我们开发了一个特征学习范式，该范式可以使这些模式保持一致，同时保留每种模式提供的独特信息。我们进一步提出了一种正则化方案，以增强代表复杂相互作用动力学的动作轨迹特征的因果关系。实验表明，结合多模式感官可提高模拟精度并降低时间漂移。广泛的消融研究和下游应用证明了我们工作的有效性和实用性。</li>
</ul>

<h3>Title: Test-Time Anchoring for Discrete Diffusion Posterior Sampling</h3>
<ul>
<li><strong>Authors: </strong>Litu Rout, Andreas Lugmayr, Yasamin Jafarian, Srivatsan Varadharajan, Constantine Caramanis, Sanjay Shakkottai, Ira Kemelmacher-Shlizerman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02291">https://arxiv.org/abs/2510.02291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02291">https://arxiv.org/pdf/2510.02291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02291]] Test-Time Anchoring for Discrete Diffusion Posterior Sampling(https://arxiv.org/abs/2510.02291)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We study the problem of posterior sampling using pretrained discrete diffusion foundation models, aiming to recover images from noisy measurements without retraining task-specific models. While diffusion models have achieved remarkable success in generative modeling, most advances rely on continuous Gaussian diffusion. In contrast, discrete diffusion offers a unified framework for jointly modeling categorical data such as text and images. Beyond unification, discrete diffusion provides faster inference, finer control, and principled training-free Bayesian inference, making it particularly well-suited for posterior sampling. However, existing approaches to discrete diffusion posterior sampling face severe challenges: derivative-free guidance yields sparse signals, continuous relaxations limit applicability, and split Gibbs samplers suffer from the curse of dimensionality. To overcome these limitations, we introduce Anchored Posterior Sampling (APS) for masked diffusion foundation models, built on two key innovations -- quantized expectation for gradient-like guidance in discrete embedding space, and anchored remasking for adaptive decoding. Our approach achieves state-of-the-art performance among discrete diffusion samplers across linear and nonlinear inverse problems on the standard benchmarks. We further demonstrate the benefits of our approach in training-free stylization and text-guided editing.</li>
<li><strong>摘要：</strong>我们使用预验证的离散扩散基础模型研究后取样的问题，旨在在不重新训练特定于任务的模型的情况下从嘈杂的测量中恢复图像。尽管扩散模型在生成建模方面取得了显着成功，但大多数进步都取决于连续的高斯扩散。相比之下，离散扩散为共同建模的分类数据（例如文本和图像）提供了统一的框架。除了统一外，离散的扩散提供了更快的推断，更精细的控制和无训练的贝叶斯推断，使其特别适合后采样。但是，现有的离散扩散后抽样的方法面临严重的挑战：无衍生的指导产生稀疏信号，连续的放松限制了适用性，而分裂的吉布斯采样器则遭受了维度的诅咒。为了克服这些局限性，我们为掩盖的扩散基础模型引入了锚定的后验采样（AP），该模型建立在两个关键的创新上 - 量化了对离散嵌入空间中类似梯度的指导的期望，并锚定对自适应解码进行重新启动。我们的方法在标准基准上的线性和非线性逆问题跨离散扩散采样器中实现了最先进的性能。我们进一步证明了方法在无训练的风格化和文本指导编辑方面的好处。</li>
</ul>

<h3>Title: Continual Personalization for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yu-Chien Liao, Jr-Jen Chen, Chi-Pin Huang, Ci-Siang Lin, Meng-Lin Wu, Yu-Chiang Frank Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02296">https://arxiv.org/abs/2510.02296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02296">https://arxiv.org/pdf/2510.02296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02296]] Continual Personalization for Diffusion Models(https://arxiv.org/abs/2510.02296)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Updating diffusion models in an incremental setting would be practical in real-world applications yet computationally challenging. We present a novel learning strategy of Concept Neuron Selection (CNS), a simple yet effective approach to perform personalization in a continual learning scheme. CNS uniquely identifies neurons in diffusion models that are closely related to the target concepts. In order to mitigate catastrophic forgetting problems while preserving zero-shot text-to-image generation ability, CNS finetunes concept neurons in an incremental manner and jointly preserves knowledge learned of previous concepts. Evaluation of real-world datasets demonstrates that CNS achieves state-of-the-art performance with minimal parameter adjustments, outperforming previous methods in both single and multi-concept personalization works. CNS also achieves fusion-free operation, reducing memory storage and processing time for continual personalization.</li>
<li><strong>摘要：</strong>在实际应用程序中，在渐进设置中更新扩散模型将是实用的，但在计算方面具有挑战性。我们提出了一种新颖的概念神经元选择学习策略（CNS），这是一种在持续学习方案中执行个性化的简单而有效的方法。中枢神经系统独特地识别与目标概念密切相关的扩散模型中的神经元。为了减轻灾难性遗忘问题，同时保留零拍的文本对图像生成能力，以渐进的方式CNS Finetunes概念神经元，并共同保留对先前概念的知识。对现实世界数据集的评估表明，CNS通过最小的参数调整实现最先进的性能，在单个和多概念个性化的工作中都表现优于以前的方法。中枢神经系统还可以实现无融合操作，减少内存存储和连续个性化的处理时间。</li>
</ul>

<h3>Title: Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models</h3>
<ul>
<li><strong>Authors: </strong>Runqian Wang, Yilun Du</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02300">https://arxiv.org/abs/2510.02300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02300">https://arxiv.org/pdf/2510.02300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02300]] Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models(https://arxiv.org/abs/2510.02300)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We introduce Equilibrium Matching (EqM), a generative modeling framework built from an equilibrium dynamics perspective. EqM discards the non-equilibrium, time-conditional dynamics in traditional diffusion and flow-based generative models and instead learns the equilibrium gradient of an implicit energy landscape. Through this approach, we can adopt an optimization-based sampling process at inference time, where samples are obtained by gradient descent on the learned landscape with adjustable step sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation performance of diffusion/flow models empirically, achieving an FID of 1.90 on ImageNet 256$\times$256. EqM is also theoretically justified to learn and sample from the data manifold. Beyond generation, EqM is a flexible framework that naturally handles tasks including partially noised image denoising, OOD detection, and image composition. By replacing time-conditional velocities with a unified equilibrium landscape, EqM offers a tighter bridge between flow and energy-based models and a simple route to optimization-driven inference.</li>
<li><strong>摘要：</strong>我们引入了平衡匹配（EQM），这是一种从平衡动力学角度构建的生成建模框架。 EQM丢弃了传统扩散和基于流量的生成模型中的非平衡，时间条件动力学，而是学习隐式能量景观的平衡梯度。通过这种方法，我们可以在推理时间采用基于优化的采样过程，其中通过可调节的台阶尺寸，自适应优化器和自适应计算在学习的景观上通过梯度下降获得样本。 EQM从经验上超过了扩散/流模型的发电性能，在Imagenet 256 $ \ times $ 256上达到了1.90的FID。从理论上讲，EQM也有理由从数据歧管学习和采样。除了生成之外，EQM是一个灵活的框架，它自然地处理任务，包括部分噪声图像DeNoising，OOD检测和图像组成。通过用统一的平衡景观代替时间条件速度，EQM在流量和基于能量的模型之间提供了更紧密的桥梁，以及一种简单的优化推理途径。</li>
</ul>

<h3>Title: Knowledge Distillation Detection for Open-weights Models</h3>
<ul>
<li><strong>Authors: </strong>Qin Shi, Amber Yijia Zheng, Qifan Song, Raymond A. Yeh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02302">https://arxiv.org/abs/2510.02302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02302">https://arxiv.org/pdf/2510.02302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02302]] Knowledge Distillation Detection for Open-weights Models(https://arxiv.org/abs/2510.02302)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We propose the task of knowledge distillation detection, which aims to determine whether a student model has been distilled from a given teacher, under a practical setting where only the student's weights and the teacher's API are available. This problem is motivated by growing concerns about model provenance and unauthorized replication through distillation. To address this task, we introduce a model-agnostic framework that combines data-free input synthesis and statistical score computation for detecting distillation. Our approach is applicable to both classification and generative models. Experiments on diverse architectures for image classification and text-to-image generation show that our method improves detection accuracy over the strongest baselines by 59.6% on CIFAR-10, 71.2% on ImageNet, and 20.0% for text-to-image generation. The code is available at this https URL.</li>
<li><strong>摘要：</strong>我们提出了知识蒸馏检测的任务，该任务旨在确定在只有学生的体重和教师的API的实用环境下，是否可以从给定的老师那里提炼学生模型。对模型出处和未经蒸馏的未经授权复制的担忧日益严重的原因。为了解决此任务，我们引入了一个模型 - 反应框架，该框架结合了无数据输入合成和检测蒸馏的统计得分计算。我们的方法适用于分类和生成模型。关于图像分类和文本对图像生成的各种体系结构的实验表明，我们的方法在CIFAR-10上提高了最强基线的检测准确性59.6％，ImageNet的71.2％，文本对图像生成的检测准确性为20.0％。该代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Ruozhen He, Moayed Haji-Ali, Ziyan Yang, Vicente Ordonez</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02307">https://arxiv.org/abs/2510.02307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02307">https://arxiv.org/pdf/2510.02307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02307]] NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation(https://arxiv.org/abs/2510.02307)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models trained on a fixed set of resolutions often fail to generalize, even when asked to generate images at lower resolutions than those seen during training. High-resolution text-to-image generators are currently unable to easily offer an out-of-the-box budget-efficient alternative to their users who might not need high-resolution images. We identify a key technical insight in diffusion models that when addressed can help tackle this limitation: Noise schedulers have unequal perceptual effects across resolutions. The same level of noise removes disproportionately more signal from lower-resolution images than from high-resolution images, leading to a train-test mismatch. We propose NoiseShift, a training-free method that recalibrates the noise level of the denoiser conditioned on resolution size. NoiseShift requires no changes to model architecture or sampling schedule and is compatible with existing models. When applied to Stable Diffusion 3, Stable Diffusion 3.5, and Flux-Dev, quality at low resolutions is significantly improved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and Flux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by 10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results demonstrate the effectiveness of NoiseShift in mitigating resolution-dependent artifacts and enhancing the quality of low-resolution image generation.</li>
<li><strong>摘要：</strong>在固定分辨率上训练的文本到图像扩散模型即使被要求在较低的分辨率上生成图像比在训练中看到的图像时也无法概括。高分辨率的文本到图像发电机目前无法轻松地为可能不需要高分辨率图像的用户提供开箱即用的预算替代品。我们在扩散模型中确定了一个关键的技术见解，该模型可以帮助解决此限制：噪声调度程序在分辨率之间具有不平等的感知效果。与高分辨率图像相比，相同水平的噪声从低分辨率图像中消除了更多的信号，从而导致火车测试不匹配。我们提出了NoiseShift，这是一种无训练的方法，可重新校准以分辨率大小为条件的DeNoiser的噪声水平。 NoiseShift不需要更改模型架构或采样时间表，并且与现有模型兼容。当应用于稳定的扩散3，稳定的扩散3.5和通量-DEV时，低分辨率下的质量将显着提高。在Laion-Coco上，Noiseshift将SD3.5提高15.89％，SD3提高了8.56％，而Flux-DEV平均为2.44％。在Celeba上，Noiseshift将SD3.5提高10.36％，SD3提高了5.19％，而Flux-DEV平均将FID提高3.02％。这些结果证明了Noiseshift在缓解分辨率依赖性伪影并提高低分辨率图像产生质量方面的有效性。</li>
</ul>

<h3>Title: Inferring Dynamic Physical Properties from Video Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Guanqi Zhan, Xianzheng Ma, Weidi Xie, Andrew Zisserman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02311">https://arxiv.org/abs/2510.02311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02311">https://arxiv.org/pdf/2510.02311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02311]] Inferring Dynamic Physical Properties from Video Foundation Models(https://arxiv.org/abs/2510.02311)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We study the task of predicting dynamic physical properties from videos. More specifically, we consider physical properties that require temporal information to be inferred: elasticity of a bouncing object, viscosity of a flowing liquid, and dynamic friction of an object sliding on a surface. To this end, we make the following contributions: (i) We collect a new video dataset for each physical property, consisting of synthetic training and testing splits, as well as a real split for real world evaluation. (ii) We explore three ways to infer the physical property from videos: (a) an oracle method where we supply the visual cues that intrinsically reflect the property using classical computer vision techniques; (b) a simple read out mechanism using a visual prompt and trainable prompt vector for cross-attention on pre-trained video generative and self-supervised models; and (c) prompt strategies for Multi-modal Large Language Models (MLLMs). (iii) We show that video foundation models trained in a generative or self-supervised manner achieve a similar performance, though behind that of the oracle, and MLLMs are currently inferior to the other models, though their performance can be improved through suitable prompting.</li>
<li><strong>摘要：</strong>我们研究了视频中预测动态物理特性的任务。更具体地说，我们考虑需要推断时间信息的物理特性：弹跳对象的弹性，流动液体的粘度以及对物体在表面上滑动的动态摩擦。为此，我们做出以下贡献：（i）我们为每个物理属性收集一个新的视频数据集，包括合成训练和测试拆分，以及对现实世界评估的真实拆分。 （ii）我们探索从视频中推断物理属性的三种方法：（a）一种甲骨文方法，在其中我们提供了使用经典的计算机视觉技术来本质地反映属性的视觉提示； （b）使用视觉提示和可训练的提示向量进行简单读取机制，以在预先训练的视频生成和自我监督模型上进行交叉注意； （c）促使多模式大语言模型（MLLM）提示策略。 （iii）我们表明，以生成或自我监督的方式训练的视频基础模型达到了类似的性能，尽管在甲骨文的后面，而MLLM当前不如其他模型，尽管可以通过合适的提示来提高其性能。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
