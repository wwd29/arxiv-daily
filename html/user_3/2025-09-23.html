<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-23</h1>
<h3>Title: From Canopy to Ground via ForestGen3D: Learning Cross-Domain Generation of 3D Forest Structure from Aerial-to-Terrestrial LiDAR</h3>
<ul>
<li><strong>Authors: </strong>Juan Castorena, E. Louise Loudermilk, Scott Pokswinski, Rodman Linn</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16346">https://arxiv.org/abs/2509.16346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16346">https://arxiv.org/pdf/2509.16346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16346]] From Canopy to Ground via ForestGen3D: Learning Cross-Domain Generation of 3D Forest Structure from Aerial-to-Terrestrial LiDAR(https://arxiv.org/abs/2509.16346)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The 3D structure of living and non-living components in ecosystems plays a critical role in determining ecological processes and feedbacks from both natural and human-driven disturbances. Anticipating the effects of wildfire, drought, disease, or atmospheric deposition depends on accurate characterization of 3D vegetation structure, yet widespread measurement remains prohibitively expensive and often infeasible. We introduce ForestGen3D, a novel generative modeling framework that synthesizes high-fidelity 3D forest structure using only aerial LiDAR (ALS) inputs. ForestGen3D is based on conditional denoising diffusion probabilistic models (DDPMs) trained on co-registered ALS/TLS (terrestrial LiDAR) data. The model learns to generate TLS-like 3D point clouds conditioned on sparse ALS observations, effectively reconstructing occluded sub-canopy detail at scale. To ensure ecological plausibility, we introduce a geometric containment prior based on the convex hull of ALS observations and provide theoretical and empirical guarantees that generated structures remain spatially consistent. We evaluate ForestGen3D at tree, plot, and landscape scales using real-world data from mixed conifer ecosystems, and show that it produces high-fidelity reconstructions that closely match TLS references in terms of geometric similarity and biophysical metrics, such as tree height, DBH, crown diameter and crown volume. Additionally, we demonstrate that the containment property can serve as a practical proxy for generation quality in settings where TLS ground truth is unavailable. Our results position ForestGen3D as a scalable tool for ecological modeling, wildfire simulation, and structural fuel characterization in ALS-only environments.</li>
<li><strong>摘要：</strong>生态系统中生存和非生存组成部分的3D结构在确定自然和人驱动干扰的生态过程和反馈方面起着关键作用。预期野火，干旱，疾病或大气沉积的影响取决于3D植被结构的准确表征，但是广泛的测量仍然过于昂贵，而且通常不可行。我们介绍了ForestGen3D，这是一种新型的生成建模框架，仅使用空中激光雷达（ALS）输入来综合3D森林结构。 ForestGen3D基于对共同注册的ALS/TLS（陆地LIDAR）数据训练的条件降解扩散概率模型（DDPM）。该模型学会生成基于稀疏ALS观测的条件的类似TLS的3D点云，从而有效地重建了遮挡的子囊遗迹细节。为了确保生态上的合理性，我们根据ALS观测的凸面引入了几何遏制，并提供了理论和经验保证，即生成的结构在空间上保持一致。我们使用来自混合针叶树生态系统的现实世界数据在树，情节和景观量表上评估Forestgen3d，并表明它会根据几何相似性和生物物理指标（例如树高，DBH，DBH，DBH，DBH，DBH，DBH，DBH，DBH，DBH，DBH，DBH，皇冠量和冠冠量）产生高保真重建。此外，我们证明，在TLS地面真理不可用的环境中，遏制属性可以作为生成质量的实际代理。我们的结果将ForestGen3D定位为可扩展的工具，用于仅在ALS环境中进行生态建模，野火模拟和结构性燃料表征。</li>
</ul>

<h3>Title: Guided Sequence-Structure Generative Modeling for Iterative Antibody Optimization</h3>
<ul>
<li><strong>Authors: </strong>Aniruddh Raghu, Sebastian Ober, Maxwell Kazman, Hunter Elliott</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16357">https://arxiv.org/abs/2509.16357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16357">https://arxiv.org/pdf/2509.16357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16357]] Guided Sequence-Structure Generative Modeling for Iterative Antibody Optimization(https://arxiv.org/abs/2509.16357)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Therapeutic antibody candidates often require extensive engineering to improve key functional and developability properties before clinical development. This can be achieved through iterative design, where starting molecules are optimized over several rounds of in vitro experiments. While protein structure can provide a strong inductive bias, it is rarely used in iterative design due to the lack of structural data for continually evolving lead molecules over the course of optimization. In this work, we propose a strategy for iterative antibody optimization that leverages both sequence and structure as well as accumulating lab measurements of binding and developability. Building on prior work, we first train a sequence-structure diffusion generative model that operates on antibody-antigen complexes. We then outline an approach to use this model, together with carefully predicted antibody-antigen complexes, to optimize lead candidates throughout the iterative design process. Further, we describe a guided sampling approach that biases generation toward desirable properties by integrating models trained on experimental data from iterative design. We evaluate our approach in multiple in silico and in vitro experiments, demonstrating that it produces high-affinity binders at multiple stages of an active antibody optimization campaign.</li>
<li><strong>摘要：</strong>候选治疗抗体通常需要广泛的工程，以在临床开发之前提高关键功能和发展性能。这可以通过迭代设计来实现，在该设计中，在几轮体外实验中优化了启动分子。尽管蛋白质结构可以提供强烈的感应偏置，但由于缺乏在优化过程中不断发展的铅分子的结构数据，因此很少在迭代设计中使用它。在这项工作中，我们提出了一种迭代抗体优化的策略，该策略既利用序列和结构，又积累了实验室的结合和发展性。在先前工作的基础上，我们首先训练在抗体 - 抗原复合物上运行的序列结构扩散生成模型。然后，我们概述了一种使用该模型的方法，以及精心预测的抗体 - 抗原复合物，以在整个迭代设计过程中优化铅候选。此外，我们描述了一种有导的抽样方法，该方法通过整合了对迭代设计实验数据训练的模型，从而使生成偏向理想的特性。我们在硅和体外实验中评估了我们的方法，表明它在主动抗体优化运动的多个阶段产生了高亲和力粘合剂。</li>
</ul>

<h3>Title: Introducing Resizable Region Packing Problem in Image Generation, with a Heuristic Solution</h3>
<ul>
<li><strong>Authors: </strong>Hrishikesh Sharma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16363">https://arxiv.org/abs/2509.16363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16363">https://arxiv.org/pdf/2509.16363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16363]] Introducing Resizable Region Packing Problem in Image Generation, with a Heuristic Solution(https://arxiv.org/abs/2509.16363)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The problem of image data generation in computer vision has traditionally been a harder problem to solve, than discriminative problems. Such data generation entails placing relevant objects of appropriate sizes each, at meaningful location in a scene canvas. There have been two classes of popular approaches to such generation: graphics based, and generative models-based. Optimization problems are known to lurk in the background for both these classes of approaches. In this paper, we introduce a novel, practically useful manifestation of the classical Bin Packing problem in the context of generation of synthetic image data. We conjecture that the newly introduced problem, Resizable Anchored Region Packing(RARP) Problem, is NP-hard, and provide detailed arguments about our conjecture. As a first solution, we present a novel heuristic algorithm that is generic enough and therefore scales and packs arbitrary number of arbitrary-shaped regions at arbitrary locations, into an image canvas. The algorithm follows greedy approach to iteratively pack region pairs in a careful way, while obeying the optimization constraints. The algorithm is validated by an implementation that was used to generate a large-scale synthetic anomaly detection dataset, with highly varying degree of bin packing parameters per image sample i.e. RARP instance. Visual inspection of such data and checking of the correctness of each solution proves the effectiveness of our algorithm. With generative modeling being on rise in deep learning, and synthetic data generation poised to become mainstream, we expect that the newly introduced problem will be valued in the imaging scientific community.</li>
<li><strong>摘要：</strong>传统上，计算机视觉中图像数据生成的问题比歧视性问题更难以解决。这样的数据生成需要在场景画布中的有意义的位置放置每个相关的对象。有两类流行的方法：基于图形和基于生成的模型。这两种方法都潜伏在背景中已知优化问题。在本文中，我们在生成合成图像数据的背景下介绍了经典垃圾箱问题的新颖，实际上有用的表现。我们猜想新引入的问题，可分解的锚定区域包装（RARP）问题是NP-HARD，并提供了有关我们猜想的详细论点。作为第一个解决方案，我们提出了一种新颖的启发式算法，该算法足够通用，因此在任意位置的任意形状区域缩放和包装到图像画布中。该算法遵循贪婪的方法，以仔细的方式仔细填充区域对，同时遵守优化约束。该算法通过用于生成大规模合成异常检测数据集的实现来验证，每个图像样本（即RARP实例）具有高度变化的bin填料参数。目视检查此类数据并检查每个溶液的正确性证明了我们算法的有效性。随着生成建模在深度学习中的增长，合成数据生成有望成为主流，我们希望新引入的问题将在成像科学界中得到重视。</li>
</ul>

<h3>Title: Cross-Corpus and Cross-domain Handwriting Assessment of NeuroDegenerative Diseases via Time-Series-to-Image Conversion</h3>
<ul>
<li><strong>Authors: </strong>Gabrielle Chavez, Laureano Moro-Velazquez, Ankur Butala, Najim Dehak, Thomas Thebaud</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16474">https://arxiv.org/abs/2509.16474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16474">https://arxiv.org/pdf/2509.16474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16474]] Cross-Corpus and Cross-domain Handwriting Assessment of NeuroDegenerative Diseases via Time-Series-to-Image Conversion(https://arxiv.org/abs/2509.16474)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Handwriting is significantly affected by neurological disorders (ND) such as Parkinson's disease (PD) and Alzheimer's disease (AD). Prior works have analyzed handwriting tasks using feature-based approaches or computer-vision techniques, but these methods have struggled to generalize across multiple datasets, particularly between temporal features represented as time-series and images. We propose a framework that leverages both time-series and images of handwriting through a joint classifier, based on a ResNet50 pretrained on ImageNet-1k. Binary classification experiments demonstrate state-of-the-art performances on existing time-series and image datasets, with significant improvement on specific drawing and writing tasks from the NeuroLogical Signals (NLS) dataset. In particular, the proposed model demonstrates improved performance on Draw Clock and Spiral tasks. Additionally, cross-dataset and multi-dataset experiments were consistently able to achieve high F1 scores, up to 98 for PD detection, highlighting the potential of the proposed model to generalize over different forms of handwriting signals, and enhance the detection of motor deficits in ND.</li>
<li><strong>摘要：</strong>手写受到神经系统疾病（ND）的显着影响，例如帕金森氏病（PD）和阿尔茨海默氏病（AD）。先前的工作已经使用基于功能的方法或计算机视觉技术分析了手写任务，但是这些方法一直在努力跨多个数据集进行概括，尤其是在表示为时间序列和图像的时间功能之间。我们提出了一个基于在Imagenet-1K上预先预测的RESNET50的框架，该框架通过联合分类器来利用时间序列和手写图像。二进制分类实验证明了现有时间序列和图像数据集的最新性能，从神经信号（NLS）数据集的特定绘图和写作任务上进行了重大改进。尤其是，提出的模型表明了在拉动时钟和螺旋任务上的性能提高。此外，跨数据库和多数据集实验始终能够达到高F1分数，最高为98，用于PD检测，突出了所提出的模型在不同形式的手写信号上概括的潜力，并增强了ND中运动缺陷的检测。</li>
</ul>

<h3>Title: Towards Universal Debiasing for Language Models-based Tabular Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Tianchun Li, Tianci Liu, Xingchen Wang, Rongzhe Wei, Pan Li, Lu Su, Jing Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16475">https://arxiv.org/abs/2509.16475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16475">https://arxiv.org/pdf/2509.16475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16475]] Towards Universal Debiasing for Language Models-based Tabular Data Generation(https://arxiv.org/abs/2509.16475)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved promising results in tabular data generation. However, inherent historical biases in tabular datasets often cause LLMs to exacerbate fairness issues, particularly when multiple advantaged and protected features are involved. In this work, we introduce a universal debiasing framework that minimizes group-level dependencies by simultaneously reducing the mutual information between advantaged and protected attributes. By leveraging the autoregressive structure and analytic sampling distributions of LLM-based tabular data generators, our approach efficiently computes mutual information, reducing the need for cumbersome numerical estimations. Building on this foundation, we propose two complementary methods: a direct preference optimization (DPO)-based strategy, namely UDF-DPO, that integrates seamlessly with existing models, and a targeted debiasing technique, namely UDF-MIX, that achieves debiasing without tuning the parameters of LLMs. Extensive experiments demonstrate that our framework effectively balances fairness and utility, offering a scalable and practical solution for debiasing in high-stakes applications.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）在表格数据生成中取得了有希望的结果。但是，表格数据集中固有的历史偏见通常会导致LLMS加剧公平性问题，尤其是在涉及多个优势和受保护的特征时。在这项工作中，我们介绍了一个通用的偏见框架，该框架通过同时减少优先属性和受保护属性之间的相互信息来最大程度地降低群体级别的依赖性。通过利用基于LLM的表格数据生成器的自回旋结构和分析采样分布，我们的方法有效地计算了相互信息，从而减少了对繁琐的数值估计的需求。在此基础的基础上，我们提出了两种互补方法：基于直接的优化优化（DPO）策略，即UDF-DPO，与现有模型无缝集成，而有针对性的偏见技术，即UDF-MIX，即无需调谐LLM的参数而实现了偏见。广泛的实验表明，我们的框架有效地平衡了公平和效用，为在高风险应用中提供了可扩展且实用的解决方案。</li>
</ul>

<h3>Title: Octree Latent Diffusion for Semantic 3D Scene Generation and Completion</h3>
<ul>
<li><strong>Authors: </strong>Xujia Zhang, Brendan Crowe, Christoffer Heckman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16483">https://arxiv.org/abs/2509.16483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16483">https://arxiv.org/pdf/2509.16483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16483]] Octree Latent Diffusion for Semantic 3D Scene Generation and Completion(https://arxiv.org/abs/2509.16483)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The completion, extension, and generation of 3D semantic scenes are an interrelated set of capabilities that are useful for robotic navigation and exploration. Existing approaches seek to decouple these problems and solve them oneoff. Additionally, these approaches are often domain-specific, requiring separate models for different data distributions, e.g. indoor vs. outdoor scenes. To unify these techniques and provide cross-domain compatibility, we develop a single framework that can perform scene completion, extension, and generation in both indoor and outdoor scenes, which we term Octree Latent Semantic Diffusion. Our approach operates directly on an efficient dual octree graph latent representation: a hierarchical, sparse, and memory-efficient occupancy structure. This technique disentangles synthesis into two stages: (i) structure diffusion, which predicts binary split signals to construct a coarse occupancy octree, and (ii) latent semantic diffusion, which generates semantic embeddings decoded by a graph VAE into voxellevel semantic labels. To perform semantic scene completion or extension, our model leverages inference-time latent inpainting, or outpainting respectively. These inference-time methods use partial LiDAR scans or maps to condition generation, without the need for retraining or finetuning. We demonstrate highquality structure, coherent semantics, and robust completion from single LiDAR scans, as well as zero-shot generalization to out-of-distribution LiDAR data. These results indicate that completion-through-generation in a dual octree graph latent space is a practical and scalable alternative to regression-based pipelines for real-world robotic perception tasks.</li>
<li><strong>摘要：</strong>3D语义场景的完成，扩展和生成是一组相互关联的功能，可用于机器人导航和探索。现有的方法试图将这些问题解散并解决它们。此外，这些方法通常是特定于域的，需要单独的模型来用于不同的数据分布，例如室内与室外场景。为了统一这些技术并提供跨域的兼容性，我们开发了一个单个框架，可以在室内和室外场景中执行场景完成，扩展和发电，我们将其称为OCTREE潜在的语义扩散。我们的方法直接在有效的双重OCTREE图潜在表示上运行：层次，稀疏和记忆有效的占用结构。该技术将合成分为两个阶段：（i）结构扩散，该结构扩散，该结构预测二进制拆分信号构建了粗大的占用OCTREE，以及（ii）潜在的语义扩散，该语义扩散产生了由图VAE解码到Voxellevel语义标签中的语义嵌入。要执行语义场景的完成或扩展，我们的模型分别利用推理时间潜在的镶嵌或支出。这些推理时间方法使用部分激光扫描或地图进行条件生成，而无需重新训练或填充。我们展示了单个LIDAR扫描的高质量结构，连贯的语义和鲁棒完成，以及零射门到分布激光雷达数据的零概括。这些结果表明，在双OCTREE图潜在空间中完成的直通生成是实用且可扩展的替代基于回归的机器人机器人感知任务的替代方案。</li>
</ul>

<h3>Title: A Closer Look at Model Collapse: From a Generalization-to-Memorization Perspective</h3>
<ul>
<li><strong>Authors: </strong>Lianghe Shi, Meng Wu, Huijie Zhang, Zekai Zhang, Molei Tao, Qing Qu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16499">https://arxiv.org/abs/2509.16499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16499">https://arxiv.org/pdf/2509.16499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16499]] A Closer Look at Model Collapse: From a Generalization-to-Memorization Perspective(https://arxiv.org/abs/2509.16499)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The widespread use of diffusion models has led to an abundance of AI-generated data, raising concerns about model collapse -- a phenomenon in which recursive iterations of training on synthetic data lead to performance degradation. Prior work primarily characterizes this collapse via variance shrinkage or distribution shift, but these perspectives miss practical manifestations of model collapse. This paper identifies a transition from generalization to memorization during model collapse in diffusion models, where models increasingly replicate training data instead of generating novel content during iterative training on synthetic samples. This transition is directly driven by the declining entropy of the synthetic training data produced in each training cycle, which serves as a clear indicator of model degradation. Motivated by this insight, we propose an entropy-based data selection strategy to mitigate the transition from generalization to memorization and alleviate model collapse. Empirical results show that our approach significantly enhances visual quality and diversity in recursive generation, effectively preventing collapse.</li>
<li><strong>摘要：</strong>扩散模型的广泛使用导致了大量的AI生成数据，引起了人们对模型崩溃的担忧 - 这种现象中，合成数据的递归迭代会导致性能降解。先前的工作主要是通过方差收缩或分布变化来表征这种崩溃的，但是这些观点错过了模型崩溃的实际表现。本文确定了扩散模型中模型崩溃期间从概括到记忆的过渡，其中模型越来越复制训练数据，而不是在合成样品的迭代培训期间生成新的内容。这种过渡是由每个训练周期中产生的合成训练数据的熵直接驱动的，这是模型退化的明显指标。在这种见解的促进下，我们提出了一种基于熵的数据选择策略，以减轻从概括到记忆的过渡并减轻模型崩溃。经验结果表明，我们的方法显着提高了递归产生的视觉质量和多样性，从而有效地防止了崩溃。</li>
</ul>

<h3>Title: RLGF: Reinforcement Learning with Geometric Feedback for Autonomous Driving Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Tianyi Yan, Wencheng Han, Xia Zhou, Xueyang Zhang, Kun Zhan, Cheng-zhong Xu, Jianbing Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16500">https://arxiv.org/abs/2509.16500</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16500">https://arxiv.org/pdf/2509.16500</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16500]] RLGF: Reinforcement Learning with Geometric Feedback for Autonomous Driving Video Generation(https://arxiv.org/abs/2509.16500)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Synthetic data is crucial for advancing autonomous driving (AD) systems, yet current state-of-the-art video generation models, despite their visual realism, suffer from subtle geometric distortions that limit their utility for downstream perception tasks. We identify and quantify this critical issue, demonstrating a significant performance gap in 3D object detection when using synthetic versus real data. To address this, we introduce Reinforcement Learning with Geometric Feedback (RLGF), RLGF uniquely refines video diffusion models by incorporating rewards from specialized latent-space AD perception models. Its core components include an efficient Latent-Space Windowing Optimization technique for targeted feedback during diffusion, and a Hierarchical Geometric Reward (HGR) system providing multi-level rewards for point-line-plane alignment, and scene occupancy coherence. To quantify these distortions, we propose GeoScores. Applied to models like DiVE on nuScenes, RLGF substantially reduces geometric errors (e.g., VP error by 21\%, Depth error by 57\%) and dramatically improves 3D object detection mAP by 12.7\%, narrowing the gap to real-data performance. RLGF offers a plug-and-play solution for generating geometrically sound and reliable synthetic videos for AD development.</li>
<li><strong>摘要：</strong>合成数据对于推进自动驾驶（AD）系统至关重要，但是当前最新的视频生成模型尽管具有视觉现实主义，但仍遭受了微妙的几何扭曲，这些扭曲限制了其对下游感知任务的效用。我们识别并量化了这个关键问题，在使用合成数据与实际数据时，证明了3D对象检测的显着性能差距。为了解决这个问题，我们通过几何反馈（RLGF）介绍了增强学习，RLGF通过纳入专门的潜在空间广告感知模型的奖励来独特地完善视频扩散模型。它的核心组件包括在扩散过程中针对目标反馈的有效的潜在空间窗口优化技术，以及一个分层的几何奖励（HGR）系统，为点线平面比对提供多级奖励以及场景占用连贯性。为了量化这些扭曲，我们提出了地质。 RLGF应用于潜水的模型，将几何误差（例如，VP误差降低21 \％，深度误差，57 \％）将3D对象检测图提高到12.7 \％，从而将差距缩小到实时数据范围。 RLGF提供了一种插件解决方案，用于生成几何声音和可靠的合成视频，以进行广告开发。</li>
</ul>

<h3>Title: GRIL: Knowledge Graph Retrieval-Integrated Learning with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jialin Chen, Houyu Zhang, Seongjun Yun, Alejandro Mottini, Rex Ying, Xiang Song, Vassilis N. Ioannidis, Zheng Li, Qingjun Cui</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16502">https://arxiv.org/abs/2509.16502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16502">https://arxiv.org/pdf/2509.16502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16502]] GRIL: Knowledge Graph Retrieval-Integrated Learning with Large Language Models(https://arxiv.org/abs/2509.16502)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) has significantly mitigated the hallucinations of Large Language Models (LLMs) by grounding the generation with external knowledge. Recent extensions of RAG to graph-based retrieval offer a promising direction, leveraging the structural knowledge for multi-hop reasoning. However, existing graph RAG typically decouples retrieval and reasoning processes, which prevents the retriever from adapting to the reasoning needs of the LLM. They also struggle with scalability when performing multi-hop expansion over large-scale graphs, or depend heavily on annotated ground-truth entities, which are often unavailable in open-domain settings. To address these challenges, we propose a novel graph retriever trained end-to-end with LLM, which features an attention-based growing and pruning mechanism, adaptively navigating multi-hop relevant entities while filtering out noise. Within the extracted subgraph, structural knowledge and semantic features are encoded via soft tokens and the verbalized graph, respectively, which are infused into the LLM together, thereby enhancing its reasoning capability and facilitating interactive joint training of the graph retriever and the LLM reasoner. Experimental results across three QA benchmarks show that our approach consistently achieves state-of-the-art performance, validating the strength of joint graph-LLM optimization for complex reasoning tasks. Notably, our framework eliminates the need for predefined ground-truth entities by directly optimizing the retriever using LLM logits as implicit feedback, making it especially effective in open-domain settings.</li>
<li><strong>摘要：</strong>检索增强的生成（RAG）通过用外部知识将一代人扎根，从而大大减轻了大语言模型（LLMS）的幻觉。抹布到基于图的检索的最新扩展提供了一个有希望的方向，利用结构知识进行多跳上的推理。但是，现有的图形抹​​布通常会剥离检索和推理过程，从而阻止了猎犬适应LLM的推理需求。在大规模图上进行多跳扩展时，他们还具有可扩展性的努力，或者在很大程度上取决于注释的地面真实实体，在开放域设置中通常无法使用。为了应对这些挑战，我们提出了一个新型的图形检索器，该挑战是通过LLM训练有素的端到端，该挑战具有基于注意力的生长和修剪机制，在过滤噪声的同时适应了多跳的相关实体。在提取的子图中，结构知识和语义特征分别通过软令牌和言语图编码，这些图被一起注入LLM中，从而增强了其推理能力，并促进了图捕获器和LLM推理器的交互性关节训练。三个质量检查基准的实验结果表明，我们的方法始终达到最先进的性能，从而验证了对复杂推理任务的联合图形-LLM优化的强度。值得注意的是，我们的框架通过使用LLM Logits作为隐式反馈来直接优化检索器，从而消除了对预定义的基地真实实体的需求，从而使其在开放域设置中特别有效。</li>
</ul>

<h3>Title: OS-DiffVSR: Towards One-step Latent Diffusion Model for High-detailed Real-world Video Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Hanting Li, Huaao Tang, Jianhong Han, Tianxiong Zhou, Jiulong Cui, Haizhen Xie, Yan Chen, Jie Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16507">https://arxiv.org/abs/2509.16507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16507">https://arxiv.org/pdf/2509.16507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16507]] OS-DiffVSR: Towards One-step Latent Diffusion Model for High-detailed Real-world Video Super-Resolution(https://arxiv.org/abs/2509.16507)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Recently, latent diffusion models has demonstrated promising performance in real-world video super-resolution (VSR) task, which can reconstruct high-quality videos from distorted low-resolution input through multiple diffusion steps. Compared to image super-resolution (ISR), VSR methods needs to process each frame in a video, which poses challenges to its inference efficiency. However, video quality and inference efficiency have always been a trade-off for the diffusion-based VSR methods. In this work, we propose One-Step Diffusion model for real-world Video Super-Resolution, namely OS-DiffVSR. Specifically, we devise a novel adjacent frame adversarial training paradigm, which can significantly improve the quality of synthetic videos. Besides, we devise a multi-frame fusion mechanism to maintain inter-frame temporal consistency and reduce the flicker in video. Extensive experiments on several popular VSR benchmarks demonstrate that OS-DiffVSR can even achieve better quality than existing diffusion-based VSR methods that require dozens of sampling steps.</li>
<li><strong>摘要：</strong>最近，潜在扩散模型在现实世界视频超分辨率（VSR）任务中表现出了令人鼓舞的性能，该任务可以通过多个扩散步骤从扭曲的低分辨率输入中重建高质量的视频。与图像超分辨率（ISR）相比，VSR方法需要处理视频中的每个帧，这对其推理效率构成了挑战。但是，视频质量和推理效率一直是基于扩散的VSR方法的权衡。在这项工作中，我们为现实世界视频超分辨率（即OS-DIFFVSR）提出了一步扩散模型。具体来说，我们设计了一种新颖的相邻框架对抗训练范式，可以显着提高合成视频的质量。此外，我们设计了一种多帧融合机制来保持框架间的时间一致性并减少视频中的闪烁。对几个流行的VSR基准测试的广泛实验表明，OS-DIFFVSR甚至可以比需要数十个采样步骤的现有基于扩散的VSR方法获得更好的质量。</li>
</ul>

<h3>Title: Federated Learning with Ad-hoc Adapter Insertions: The Case of Soft-Embeddings for Training Classifier-as-Retriever</h3>
<ul>
<li><strong>Authors: </strong>Marijan Fofonjka, Shahryar Zehtabi, Alireza Behtash, Tyler Mauer, David Stout</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16508">https://arxiv.org/abs/2509.16508</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16508">https://arxiv.org/pdf/2509.16508</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16508]] Federated Learning with Ad-hoc Adapter Insertions: The Case of Soft-Embeddings for Training Classifier-as-Retriever(https://arxiv.org/abs/2509.16508)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>When existing retrieval-augmented generation (RAG) solutions are intended to be used for new knowledge domains, it is necessary to update their encoders, which are taken to be pretrained large language models (LLMs). However, fully finetuning these large models is compute- and memory-intensive, and even infeasible when deployed on resource-constrained edge devices. We propose a novel encoder architecture in this work that addresses this limitation by using a frozen small language model (SLM), which satisfies the memory constraints of edge devices, and inserting a small adapter network before the transformer blocks of the SLM. The trainable adapter takes the token embeddings of the new corpus and learns to produce enhanced soft embeddings for it, while requiring significantly less compute power to update than full fine-tuning. We further propose a novel retrieval mechanism by attaching a classifier head to the SLM encoder, which is trained to learn a similarity mapping of the input embeddings to their corresponding documents. Finally, to enable the online fine-tuning of both (i) the encoder soft embeddings and (ii) the classifier-as-retriever on edge devices, we adopt federated learning (FL) and differential privacy (DP) to achieve an efficient, privacy-preserving, and product-grade training solution. We conduct a theoretical analysis of our methodology, establishing convergence guarantees under mild assumptions on gradient variance when deployed for general smooth nonconvex loss functions. Through extensive numerical experiments, we demonstrate (i) the efficacy of obtaining soft embeddings to enhance the encoder, (ii) training a classifier to improve the retriever, and (iii) the role of FL in achieving speedup.</li>
<li><strong>摘要：</strong>如果将现有的检索功能生成（RAG）解决方案用于新的知识领域，则有必要更新其编码器，这些编码器被认为是预验证的大型语言模型（LLMS）。但是，这些大型模型充分填充是计算和内存密集的，甚至在资源受限的边缘设备上部署时甚至不可行。我们在这项工作中提出了一种新颖的编码器体系结构，该架构通过使用冷冻的小语言模型（SLM）来解决此限制，该模型满足边缘设备的内存约束，并在SLM的变压器块之前插入一个小型适配器网络。可训练的适配器采用新语料库的令牌嵌入，并学会为其生成增强的软嵌入，同时需要更新的计算功率要比完整的微调较少。我们进一步提出了一种新颖的检索机制，通过将分类器头连接到SLM编码器，该机构经过训练，可以学习将输入嵌入的相似性映射到其相应的文档中。最后，为了启用（i）（i）编码器软嵌入以及（ii）边缘设备上的分类器 - 回程的在线微调，我们采用联合学习（FL）和差异隐私（DP）来实现有效的，隐私保护和产品级培训解决方案。我们对我们的方法进行了理论分析，在部署一般平滑的非凸损失函数时，在对梯度方差的轻度假设下建立收敛保证。通过广泛的数值实验，我们证明了（i）获得软嵌入以增强编码器的功效，（ii）训练分类器以改善撤回器，以及（iii）FL在实现加速方面的作用。</li>
</ul>

<h3>Title: mmExpert: Integrating Large Language Models for Comprehensive mmWave Data Synthesis and Understanding</h3>
<ul>
<li><strong>Authors: </strong>Yifan Yan, Shuai Yang, Xiuzhen Guo, Xiangguang Wang, Wei Chow, Yuanchao Shu, Shibo He</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16521">https://arxiv.org/abs/2509.16521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16521">https://arxiv.org/pdf/2509.16521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16521]] mmExpert: Integrating Large Language Models for Comprehensive mmWave Data Synthesis and Understanding(https://arxiv.org/abs/2509.16521)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Millimeter-wave (mmWave) sensing technology holds significant value in human-centric applications, yet the high costs associated with data acquisition and annotation limit its widespread adoption in our daily lives. Concurrently, the rapid evolution of large language models (LLMs) has opened up opportunities for addressing complex human needs. This paper presents mmExpert, an innovative mmWave understanding framework consisting of a data generation flywheel that leverages LLMs to automate the generation of synthetic mmWave radar datasets for specific application scenarios, thereby training models capable of zero-shot generalization in real-world environments. Extensive experiments demonstrate that the data synthesized by mmExpert significantly enhances the performance of downstream models and facilitates the successful deployment of large models for mmWave understanding.</li>
<li><strong>摘要：</strong>毫米波（MMWAVE）传感技术在以人为本的应用中具有重要的价值，但是与数据获取和注释相关的高成本将其在我们日常生活中广泛采用。同时，大型语言模型（LLM）的快速发展为满足复杂人类需求的机会打开了机会。本文介绍了MMExpert，这是一种创新的MMWave理解框架，该框架由数据生成飞轮组成，该框架利用LLMS来自动化合成MMWave Radar数据集的生成，以实现特定的应用程序场景，从而在现实世界中能够在现实世界中零震动的培训模型。广泛的实验表明，MMExpert合成的数据显着提高了下游模型的性能，并促进了成功部署大型模型以了解MMWave的理解。</li>
</ul>

<h3>Title: ViTCAE: ViT-based Class-conditioned Autoencoder</h3>
<ul>
<li><strong>Authors: </strong>Vahid Jebraeeli, Hamid Krim, Derya Cansever</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16554">https://arxiv.org/abs/2509.16554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16554">https://arxiv.org/pdf/2509.16554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16554]] ViTCAE: ViT-based Class-conditioned Autoencoder(https://arxiv.org/abs/2509.16554)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Vision Transformer (ViT) based autoencoders often underutilize the global Class token and employ static attention mechanisms, limiting both generative control and optimization efficiency. This paper introduces ViTCAE, a framework that addresses these issues by re-purposing the Class token into a generative linchpin. In our architecture, the encoder maps the Class token to a global latent variable that dictates the prior distribution for local, patch-level latent variables, establishing a robust dependency where global semantics directly inform the synthesis of local details. Drawing inspiration from opinion dynamics, we treat each attention head as a dynamical system of interacting tokens seeking consensus. This perspective motivates a convergence-aware temperature scheduler that adaptively anneals each head's influence function based on its distributional stability. This process enables a principled head-freezing mechanism, guided by theoretically-grounded diagnostics like an attention evolution distance and a consensus/cluster functional. This technique prunes converged heads during training to significantly improve computational efficiency without sacrificing fidelity. By unifying a generative Class token with an adaptive attention mechanism rooted in multi-agent consensus theory, ViTCAE offers a more efficient and controllable approach to transformer-based generation.</li>
<li><strong>摘要：</strong>基于视觉变压器（VIT）的自动编码器通常不足以将全球类代币充分利用并采用静态注意机制，从而限制了生成控制和优化效率。本文介绍了Vitcae，该框架是通过将类令牌重新定位为生成的Linchpin来解决这些问题的框架。在我们的体系结构中，编码器将类令牌映射到一个全局潜在变量，该变量决定了本地补丁级的潜在变量的先前分布，从而建立了一个可靠的依赖性，其中全局语义直接直接告知本地细节的综合。从意见动力学中汲取灵感，我们将每个注意力头视为寻求共识的动力学系统的动态系统。这种观点激发了收敛感的温度调度程序，该调度调度仪根据其分布稳定性自适应地将每个头部的影响函数退火。该过程实现了一种有原则的头部冻结机制，以理论上的诊断为指导，例如注意力演化距离和共识/群集功能。该技术在训练过程中汇总了头部，以显着提高计算效率而不牺牲忠诚度。通过统一具有植根于多代理共识理论的自适应注意机制的生成类代币，Vitcae为基于变压器的发电提供了一种更有效，更可控制的方法。</li>
</ul>

<h3>Title: Captioning for Text-Video Retrieval via Dual-Group Direct Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Ji Soo Lee, Byungoh Ko, Jaewon Cho, Howoong Lee, Jaewoon Byun, Hyunwoo J. Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16560">https://arxiv.org/abs/2509.16560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16560">https://arxiv.org/pdf/2509.16560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16560]] Captioning for Text-Video Retrieval via Dual-Group Direct Preference Optimization(https://arxiv.org/abs/2509.16560)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In text-video retrieval, auxiliary captions are often used to enhance video understanding, bridging the gap between the modalities. While recent advances in multi-modal large language models (MLLMs) have enabled strong zero-shot caption generation, we observe that such captions tend to be generic and indistinguishable across visually similar videos, limiting their utility for fine-grained retrieval. Moreover, conventional captioning approaches are typically evaluated using language generation metrics, such as BLEU, which are not typically tailored for retrieval tasks that require making discriminative distinctions between candidates. To address this, we propose $\textbf{CaRe-DPO}$, a retrieval framework that directly optimizes caption generation using retrieval relevance scores. At its core is Dual-Group Direct Preference Optimization (DG-DPO), a novel learning strategy that supervises captioning by modeling preferences across groups of distinct video and caption pairs. In addition, we present an MLLM-based retrieval model that incorporates role-embeddings to better distinguish between textual inputs with different functional roles, such as an auxiliary caption and a text query. Through extensive experiments, we demonstrate that CaRe-DPO significantly enhances retrieval performance by effectively leveraging auxiliary knowledge to generate fine-grained captions for retrieval. Code is available at this https URL.</li>
<li><strong>摘要：</strong>在文本视频检索中，通常使用辅助字幕来增强视频理解，从而弥合了方式之间的差距。尽管多模式大语言模型（MLLM）的最新进展使得强烈的零射门字幕，但我们观察到，这种字幕在视觉上相似的视频中往往是通用和无法区分的，从而限制了它们的实用性，以获得精细粒度的检索。此外，通常使用语言产生指标（例如BLEU）评估常规字幕方法，而BLEU通常不适合检索任务，这些任务需要在候选人之间进行区分区别。为了解决这个问题，我们提出了$ \ textbf {Care-dpo} $，这是一个检索框架，可直接使用检索相关性分数直接优化字幕生成。其核心是双基团直接偏好优化（DG-DPO），这是一种新颖的学习策略，该策略通过对不同视频和标题对的组进行建模偏好来监督字幕。此外，我们提出了一个基于MLLM的检索模型，该模型结合了角色插件，以更好地区分具有不同功能角色的文本输入，例如辅助字幕和文本查询。通过广泛的实验，我们证明了Care-DPO通过有效利用辅助知识来生成细粒的标题进行检索，从而显着提高了检索性能。代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: V-CECE: Visual Counterfactual Explanations via Conceptual Edits</h3>
<ul>
<li><strong>Authors: </strong>Nikolaos Spanos, Maria Lymperaiou, Giorgos Filandrianos, Konstantinos Thomas, Athanasios Voulodimos, Giorgos Stamou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16567">https://arxiv.org/abs/2509.16567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16567">https://arxiv.org/pdf/2509.16567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16567]] V-CECE: Visual Counterfactual Explanations via Conceptual Edits(https://arxiv.org/abs/2509.16567)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent black-box counterfactual generation frameworks fail to take into account the semantic content of the proposed edits, while relying heavily on training to guide the generation process. We propose a novel, plug-and-play black-box counterfactual generation framework, which suggests step-by-step edits based on theoretical guarantees of optimal edits to produce human-level counterfactual explanations with zero training. Our framework utilizes a pre-trained image editing diffusion model, and operates without access to the internals of the classifier, leading to an explainable counterfactual generation process. Throughout our experimentation, we showcase the explanatory gap between human reasoning and neural model behavior by utilizing both Convolutional Neural Network (CNN), Vision Transformer (ViT) and Large Vision Language Model (LVLM) classifiers, substantiated through a comprehensive human evaluation.</li>
<li><strong>摘要：</strong>最近的Black-Box反事实生成框架未能考虑到拟议的编辑的语义内容，同时又依靠培训来指导生成过程。我们提出了一个新颖的，即插即用的黑框反事实生成框架，该框架建议基于理论保证的最佳编辑的理论保证，以通过零培训产生人级的反事实解释。我们的框架采用了预先训练的图像编辑扩散模型，并且无法访问分类器的内部，从而导致可解释的反事实生成过程。在整个实验中，我们通过利用卷积神经网络（CNN），Vision Transformer（VIT）和大型视觉语言模型（LVLM）分类器来展示人类推理和神经模型行为之间的解释差距，这是通过全面的人类评估证实的。</li>
</ul>

<h3>Title: A Novel Metric for Detecting Memorization in Generative Models for Brain MRI Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Antonio Scardace, Lemuel Puglisi, Francesco Guarnera, Sebastiano Battiato, Daniele Ravì</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16582">https://arxiv.org/abs/2509.16582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16582">https://arxiv.org/pdf/2509.16582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16582]] A Novel Metric for Detecting Memorization in Generative Models for Brain MRI Synthesis(https://arxiv.org/abs/2509.16582)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Deep generative models have emerged as a transformative tool in medical imaging, offering substantial potential for synthetic data generation. However, recent empirical studies highlight a critical vulnerability: these models can memorize sensitive training data, posing significant risks of unauthorized patient information disclosure. Detecting memorization in generative models remains particularly challenging, necessitating scalable methods capable of identifying training data leakage across large sets of generated samples. In this work, we propose DeepSSIM, a novel self-supervised metric for quantifying memorization in generative models. DeepSSIM is trained to: i) project images into a learned embedding space and ii) force the cosine similarity between embeddings to match the ground-truth SSIM (Structural Similarity Index) scores computed in the image space. To capture domain-specific anatomical features, training incorporates structure-preserving augmentations, allowing DeepSSIM to estimate similarity reliably without requiring precise spatial alignment. We evaluate DeepSSIM in a case study involving synthetic brain MRI data generated by a Latent Diffusion Model (LDM) trained under memorization-prone conditions, using 2,195 MRI scans from two publicly available datasets (IXI and CoRR). Compared to state-of-the-art memorization metrics, DeepSSIM achieves superior performance, improving F1 scores by an average of +52.03% over the best existing method. Code and data of our approach are publicly available at the following link: this https URL.</li>
<li><strong>摘要：</strong>深层生成模型已成为医学成像中的一种变革性工具，为合成数据的生成提供了巨大的潜力。但是，最近的实证研究突出了一个关键的脆弱性：这些模型可以记住敏感的培训数据，从而带来了未经授权的患者信息披露的重大风险。在生成模型中检测记忆仍然特别具有挑战性，需要可扩展的方法，能够识别大量生成样品的训练数据泄漏。在这项工作中，我们提出了DeepsSim，这是一种新型的自我监督指标，用于量化生成模型中的记忆。 Deepssim经过训练：i）将图像投影到学习的嵌入空间中，ii）迫使嵌入之间的余弦相似性匹配图像空间中计算的地面真相SSIM（结构相似性指数）。为了捕获特定于域的解剖特征，训练结合了结构扩展的增强，使深sim可以可靠地估计相似性，而无需精确的空间比对。我们在一个涉及在易于记忆条件下训练的潜在扩散模型（LDM）产生的涉及合成脑MRI数据的案例研究中评估了DeepSSIM，并使用了两个公开可用数据集（IXI和Corr）的2,195 MRI扫描。与最先进的记忆指标相比，Deepssim取得了出色的性能，比最佳现有方法平均提高了F1分数 +52.03％。我们的方法的代码和数据在以下链接上公开可用：此HTTPS URL。</li>
</ul>

<h3>Title: Near-Optimal Sample Complexity Bounds for Constrained Average-Reward MDPs</h3>
<ul>
<li><strong>Authors: </strong>Yukuan Wei, Xudong Li, Lin F. Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16586">https://arxiv.org/abs/2509.16586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16586">https://arxiv.org/pdf/2509.16586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16586]] Near-Optimal Sample Complexity Bounds for Constrained Average-Reward MDPs(https://arxiv.org/abs/2509.16586)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances have significantly improved our understanding of the sample complexity of learning in average-reward Markov decision processes (AMDPs) under the generative model. However, much less is known about the constrained average-reward MDP (CAMDP), where policies must satisfy long-run average constraints. In this work, we address this gap by studying the sample complexity of learning an $\epsilon$-optimal policy in CAMDPs under a generative model. We propose a model-based algorithm that operates under two settings: (i) relaxed feasibility, which allows small constraint violations, and (ii) strict feasibility, where the output policy satisfies the constraint. We show that our algorithm achieves sample complexities of $\tilde{O}\left(\frac{S A (B+H)}{ \epsilon^2}\right)$ and $\tilde{O} \left(\frac{S A (B+H)}{\epsilon^2 \zeta^2} \right)$ under the relaxed and strict feasibility settings, respectively. Here, $\zeta$ is the Slater constant indicating the size of the feasible region, $H$ is the span bound of the bias function, and $B$ is the transient time bound. Moreover, a matching lower bound of $\tilde{\Omega}\left(\frac{S A (B+H)}{ \epsilon^2\zeta^2}\right)$ for the strict feasibility case is established, thus providing the first minimax-optimal bounds for CAMDPs. Our results close the theoretical gap in understanding the complexity of constrained average-reward MDPs.</li>
<li><strong>摘要：</strong>最近的进步显着改善了我们对生成模型下平均奖励马尔可夫决策过程（AMDP）学习样本复杂性的理解。但是，关于受约束的平均奖励MDP（CAMDP）的了解，策略必须满足长期平均约束。在这项工作中，我们通过研究在生成模型下学习$ \ epsilon $最佳政策的样本复杂性来解决这一差距。我们提出了一种基于模型的算法，该算法在两个设置下运行：（i）宽松的可行性，允许违反小的约束，以及（ii）严格的可行性，在输出策略满足约束的情况下。我们表明，我们的算法达到了$ \ tilde {o} \ left的样本复杂性（\ frac {s a（b+h）} {\ epsilon^2} \ right）$ and $ \ tilde {o} {o} \ left（在放松和严格的可行性设置下。在这里，$ \ zeta $是指示可行区域大小的slater常数，$ h $是偏差函数的跨度，$ b $是瞬态时间绑定。此外，$ \ tilde {\ omega} \ left（\ frac {s a（b+h）} {\ epsilon^2 \ zeta^2} \ right）$ for camdps for camdps for Camdps。我们的结果缩小了理论差距，以理解约束平均奖励MDP的复杂性。</li>
</ul>

<h3>Title: FakeChain: Exposing Shallow Cues in Multi-Step Deepfake Detection</h3>
<ul>
<li><strong>Authors: </strong>Minji Heo, Simon S. Woo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16602">https://arxiv.org/abs/2509.16602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16602">https://arxiv.org/pdf/2509.16602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16602]] FakeChain: Exposing Shallow Cues in Multi-Step Deepfake Detection(https://arxiv.org/abs/2509.16602)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multi-step or hybrid deepfakes, created by sequentially applying different deepfake creation methods such as Face-Swapping, GAN-based generation, and Diffusion methods, can pose an emerging and unforseen technical challenge for detection models trained on single-step forgeries. While prior studies have mainly focused on detecting isolated single manipulation, little is known about the detection model behavior under such compositional, hybrid, and complex manipulation pipelines. In this work, we introduce \textbf{FakeChain}, a large-scale benchmark comprising 1-, 2-, and 3-Step forgeries synthesized using five state-of-the-art representative generators. Using this approach, we analyze detection performance and spectral properties across hybrid manipulation at different step, along with varying generator combinations and quality settings. Surprisingly, our findings reveal that detection performance highly depends on the final manipulation type, with F1-score dropping by up to \textbf{58.83\%} when it differs from training distribution. This clearly demonstrates that detectors rely on last-stage artifacts rather than cumulative manipulation traces, limiting generalization. Such findings highlight the need for detection models to explicitly consider manipulation history and sequences. Our results highlight the importance of benchmarks such as FakeChain, reflecting growing synthesis complexity and diversity in real-world scenarios. Our sample code is available here\footnote{this https URL}.</li>
<li><strong>摘要：</strong>多步或混合深层蛋糕是通过依次应用不同的深层创建方法（例如面部汇总，基于GAN的生成和扩散方法）创建的，可以对在单步伪造进行训练的检测模型构成新兴和不预定的技术挑战。虽然先前的研究主要集中于检测孤立的单一操纵，但对于这种组成，混合和复杂的操纵管道下的检测模型行为知之甚少。在这项工作中，我们介绍了\ textbf {fakechain}，这是一种大规模的基准，该基准包括使用五个最先进的代表生成器合成的1-、2-和3步伪造。使用这种方法，我们在不同步骤中分析了混合操纵的检测性能和光谱特性，以及不同的发电机组合和质量设置。令人惊讶的是，我们的发现表明，检测性能在很大程度上取决于最终的操作类型，而当它与训练分布不同时，F1得分下降到\ textbf {58.83 \％}。这清楚地表明，探测器依赖于最后一阶段的伪像，而不是累积的操纵痕迹，从而限制了概括。这些发现突出了检测模型的需求，以明确考虑操纵历史记录和序列。我们的结果强调了基准（例如Fakechain）的重要性，这反映了现实情况下的综合复杂性和多样性的日益增长。我们的示例代码可在此处提供\ footNote {this https url}。</li>
</ul>

<h3>Title: Describe-to-Score: Text-Guided Efficient Image Complexity Assessment</h3>
<ul>
<li><strong>Authors: </strong>Shipeng Liu, Zhonglin Zhang, Dengfeng Chen, Liang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16609">https://arxiv.org/abs/2509.16609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16609">https://arxiv.org/pdf/2509.16609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16609]] Describe-to-Score: Text-Guided Efficient Image Complexity Assessment(https://arxiv.org/abs/2509.16609)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Accurately assessing image complexity (IC) is critical for computer vision, yet most existing methods rely solely on visual features and often neglect high-level semantic information, limiting their accuracy and generalization. We introduce vision-text fusion for IC modeling. This approach integrates visual and textual semantic features, increasing representational diversity. It also reduces the complexity of the hypothesis space, which enhances both accuracy and generalization in complexity assessment. We propose the D2S (Describe-to-Score) framework, which generates image captions with a pre-trained vision-language model. We propose the feature alignment and entropy distribution alignment mechanisms, D2S guides semantic information to inform complexity assessment while bridging the gap between vision and text modalities. D2S utilizes multi-modal information during training but requires only the vision branch during inference, thereby avoiding multi-modal computational overhead and enabling efficient assessment. Experimental results demonstrate that D2S outperforms existing methods on the IC9600 dataset and maintains competitiveness on no-reference image quality assessment (NR-IQA) benchmark, validating the effectiveness and efficiency of multi-modal fusion in complexity-related tasks. Code is available at: this https URL</li>
<li><strong>摘要：</strong>准确评估图像复杂性（IC）对于计算机视觉至关重要，但是大多数现有方法仅依赖于视觉特征，并且经常忽略高级语义信息，从而限制了它们的准确性和概括。我们介绍了视觉文本融合以进行IC建模。这种方法整合了视觉和文本语义特征，增加了表示多样性。它还降低了假设空间的复杂性，从而增强了复杂性评估的准确性和概括。我们提出了D2S（描述得分）框架，该框架以预先训练的视觉语言模型生成图像字幕。我们提出了特征对齐和熵分布对准机制，D2S指导语义信息以告知复杂性评估，同时弥合视觉和文本方式之间的差距。 D2在训练过程中利用多模式信息，但在推理过程中仅需要视觉分支，从而避免了多模式的计算开销并实现有效评估。实验结果表明，D2在IC9600数据集上的现有方法优于现有方法，并在无参考图像质量评估（NR-IQA）基准上保持竞争力，从而验证了与复杂性相关任务中多模式融合的有效性和效率。代码可用：此HTTPS URL</li>
</ul>

<h3>Title: Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation</h3>
<ul>
<li><strong>Authors: </strong>Yue Ma, Zexuan Yan, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Zhifeng Li, Wei Liu, Linfeng Zhang, Qifeng Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16630">https://arxiv.org/abs/2509.16630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16630">https://arxiv.org/pdf/2509.16630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16630]] Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation(https://arxiv.org/abs/2509.16630)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework for freestyle portrait animation driven by facial landmarks. The main challenges in this task are preserving the identity of the reference portrait, accurately transferring target expressions, and maintaining long-term temporal consistency while ensuring generation efficiency. To address identity preservation and accurate expression retargeting, we enhance Stable Diffusion with two key components: a expression-aware landmarks as explicit motion signals, which improve motion alignment, support exaggerated expressions, and reduce identity leakage; and a fine-grained facial loss that leverages both expression and facial masks to better capture subtle expressions and faithfully preserve the reference appearance. With these components, our model supports controllable and expressive animation across diverse portrait types, including real faces, cartoons, sculptures, and animals. However, diffusion-based frameworks typically struggle to efficiently generate long-term stable animation results, which remains a core challenge in this task. To address this, we propose a progressive generation strategy for stable long-term animation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless acceleration. These two strategies ensure that our method produces high-quality results efficiently, making it user-friendly and accessible. Finally, we introduce EmojiBench++, a more comprehensive benchmark comprising diverse portraits, driving videos, and landmark sequences. Extensive evaluations on EmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior performance in both animation quality and controllability. The code, training dataset and benchmark will be found in this https URL.</li>
<li><strong>摘要：</strong>我们提出了跟随您的emoji-faster，这是一个由面部地标驱动的自由式肖像动画的有效基于扩散的框架。此任务的主要挑战是保留参考肖像的身份，准确地传递目标表达式，并保持长期的时间一致性，同时确保产生效率。为了解决身份保存和准确的表达重新定位，我们通过两个关键组成部分增强了稳定的扩散：表达感知的地标作为显式运动信号，可以改善运动比对，支持夸张的表达式和减少身份泄漏；以及一种利用表达和面膜的细粒面部损失，以更好地捕捉微妙的表情并忠实地保留参考外观。借助这些组件，我们的模型支持各种肖像类型的可控和表达动画，包括真实面孔，卡通，雕塑和动物。但是，基于扩散的框架通常很难有效地产生长期稳定的动画结果，这仍然是该任务的核心挑战。为了解决这个问题，我们为稳定的长期动画提出了渐进的生成策略，并引入了泰勒交互的缓存，达到了2.6倍的无损加速。这两种策略确保我们的方法可以有效地产生高质量的结果，从而使其用户友好且易于访问。最后，我们介绍了Emojibench ++，这是一种更全面的基准，包括各种肖像，驾驶视频和地标序列。对表情符号++的广泛评估表明，跟随您的emoji-faster速度在动画质量和可控性方面都能达到卓越的性能。代码，培训数据集和基准将在此HTTPS URL中找到。</li>
</ul>

<h3>Title: DA-Font: Few-Shot Font Generation via Dual-Attention Hybrid Integration</h3>
<ul>
<li><strong>Authors: </strong>Weiran Chen, Guiqian Zhu, Ying Li, Yi Ji, Chunping Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16632">https://arxiv.org/abs/2509.16632</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16632">https://arxiv.org/pdf/2509.16632</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16632]] DA-Font: Few-Shot Font Generation via Dual-Attention Hybrid Integration(https://arxiv.org/abs/2509.16632)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Few-shot font generation aims to create new fonts with a limited number of glyph references. It can be used to significantly reduce the labor cost of manual font design. However, due to the variety and complexity of font styles, the results generated by existing methods often suffer from visible defects, such as stroke errors, artifacts and blurriness. To address these issues, we propose DA-Font, a novel framework which integrates a Dual-Attention Hybrid Module (DAHM). Specifically, we introduce two synergistic attention blocks: the component attention block that leverages component information from content images to guide the style transfer process, and the relation attention block that further refines spatial relationships through interacting the content feature with both original and stylized component-wise representations. These two blocks collaborate to preserve accurate character shapes and stylistic textures. Moreover, we also design a corner consistency loss and an elastic mesh feature loss to better improve geometric alignment. Extensive experiments show that our DA-Font outperforms the state-of-the-art methods across diverse font styles and characters, demonstrating its effectiveness in enhancing structural integrity and local fidelity. The source code can be found at \href{this https URL}{\textit{this https URL}}.</li>
<li><strong>摘要：</strong>很少有字体生成旨在创建具有数量有限的字形参考的新字体。它可用于大大降低手动字体设计的人工成本。但是，由于字体样式的多样性和复杂性，现有方法产生的结果通常会遭受可见的缺陷，例如中风错误，人工制品和模糊。为了解决这些问题，我们提出了DA-FONT，这是一个集成了双重注意混合模块（DAHM）的新型框架。具体而言，我们介绍了两个协同的注意块：从内容图像中利用组件信息来指导样式传输过程的组件注意块，以及通过将内容特征与原始和风格化的组件通过相互交互，从而进一步完善空间关系的关系块。这两个块协作以保持准确的角色形状和风格纹理。此外，我们还设计了一个角一致性损失和弹性网格特征损失，以更好地改善几何对齐。广泛的实验表明，我们的DA-FONT胜过各种字体样式和角色的最先进方法，证明了其在增强结构完整性和本地忠诚度方面的有效性。源代码可以在\ href {this Https url} {\ textit {this Https url}}}上找到。</li>
</ul>

<h3>Title: InstanceAssemble: Layout-Aware Image Generation via Instance Assembling Attention</h3>
<ul>
<li><strong>Authors: </strong>Qiang Xiang, Shuang Sun, Binglei Li, Dejia Song, Huaxia Li, Nemo Chen, Xu Tang, Yao Hu, Junping Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16691">https://arxiv.org/abs/2509.16691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16691">https://arxiv.org/pdf/2509.16691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16691]] InstanceAssemble: Layout-Aware Image Generation via Instance Assembling Attention(https://arxiv.org/abs/2509.16691)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated remarkable capabilities in generating high-quality images. Recent advancements in Layout-to-Image (L2I) generation have leveraged positional conditions and textual descriptions to facilitate precise and controllable image synthesis. Despite overall progress, current L2I methods still exhibit suboptimal performance. Therefore, we propose InstanceAssemble, a novel architecture that incorporates layout conditions via instance-assembling attention, enabling position control with bounding boxes (bbox) and multimodal content control including texts and additional visual content. Our method achieves flexible adaption to existing DiT-based T2I models through light-weighted LoRA modules. Additionally, we propose a Layout-to-Image benchmark, Denselayout, a comprehensive benchmark for layout-to-image generation, containing 5k images with 90k instances in total. We further introduce Layout Grounding Score (LGS), an interpretable evaluation metric to more precisely assess the accuracy of L2I generation. Experiments demonstrate that our InstanceAssemble method achieves state-of-the-art performance under complex layout conditions, while exhibiting strong compatibility with diverse style LoRA modules.</li>
<li><strong>摘要：</strong>扩散模型在产生高质量图像方面表现出了显着的功能。布局到图像（L2i）生成的最新进展具有杠杆位置条件和文本描述，以促进精确且可控的图像合成。尽管总体进展，但当前的L2I方法仍表现出次优性能。因此，我们提出了InstanceAssemble，这是一种新颖的体系结构，通过实例组装的注意力结合了布局条件，从而使位置控制具有边界框（Bbox）和多模式内容控制，包括文本和其他视觉内容。我们的方法通过轻加权的LORA模块实现了对现有基于DIT的T2I模型的灵活适应。此外，我们提出了一个布局至图像基准，Denselayout，这是一个用于布局到图像生成的综合基准，其中包含5K图像，总共具有90K实例。我们进一步介绍了布局接地评分（LGS），这是一种可解释的评估度量标准，可以更精确地评​​估L2I生成的准确性。实验表明，我们的InstanceSsemled方法在复杂的布局条件下实现了最先进的性能，同时表现出与各种风格的Lora模块的强烈兼容性。</li>
</ul>

<h3>Title: Animalbooth: multimodal feature enhancement for animal subject personalization</h3>
<ul>
<li><strong>Authors: </strong>Chen Liu, Haitao Wu, Kafeng Wang, Xiaowang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16702">https://arxiv.org/abs/2509.16702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16702">https://arxiv.org/pdf/2509.16702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16702]] Animalbooth: multimodal feature enhancement for animal subject personalization(https://arxiv.org/abs/2509.16702)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Personalized animal image generation is challenging due to rich appearance cues and large morphological variability. Existing approaches often exhibit feature misalignment across domains, which leads to identity drift. We present AnimalBooth, a framework that strengthens identity preservation with an Animal Net and an adaptive attention module, mitigating cross domain alignment errors. We further introduce a frequency controlled feature integration module that applies Discrete Cosine Transform filtering in the latent space to guide the diffusion process, enabling a coarse to fine progression from global structure to detailed texture. To advance research in this area, we curate AnimalBench, a high resolution dataset for animal personalization. Extensive experiments show that AnimalBooth consistently outperforms strong baselines on multiple benchmarks and improves both identity fidelity and perceptual quality.</li>
<li><strong>摘要：</strong>个性化的动物形象产生由于丰富的外观提示和较大的形态变异性而具有挑战性。现有的方法通常在跨域上表现出特征错位，从而导致身份漂移。我们提出了动物靴，该框架可以通过动物网和适应性注意模块加强身份保存，从而减轻跨域的对齐误差。我们进一步介绍了一个频率控制的特征集成模块，该模块在潜在空间中应用离散的余弦变换过滤以指导扩散过程，从而使从全局结构到详细纹理的粗略进展。为了推进该领域的研究，我们策划了动物个性化的高分辨率数据集Animal Bench。广泛的实验表明，动物布斯在多个基准上始终优于强大的基准，并提高了身份保真度和知觉质量。</li>
</ul>

<h3>Title: When Confidence Fails: Revisiting Pseudo-Label Selection in Semi-supervised Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Pan Liu, Jinshi Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16704">https://arxiv.org/abs/2509.16704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16704">https://arxiv.org/pdf/2509.16704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16704]] When Confidence Fails: Revisiting Pseudo-Label Selection in Semi-supervised Semantic Segmentation(https://arxiv.org/abs/2509.16704)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While significant advances exist in pseudo-label generation for semi-supervised semantic segmentation, pseudo-label selection remains understudied. Existing methods typically use fixed confidence thresholds to retain high-confidence predictions as pseudo-labels. However, these methods cannot cope with network overconfidence tendency, where correct and incorrect predictions overlap significantly in high-confidence regions, making separation challenging and amplifying model cognitive bias. Meanwhile, the direct discarding of low-confidence predictions disrupts spatial-semantic continuity, causing critical context loss. We propose Confidence Separable Learning (CSL) to address these limitations. CSL formulates pseudo-label selection as a convex optimization problem within the confidence distribution feature space, establishing sample-specific decision boundaries to distinguish reliable from unreliable predictions. Additionally, CSL introduces random masking of reliable pixels to guide the network in learning contextual relationships from low-reliability regions, thereby mitigating the adverse effects of discarding uncertain predictions. Extensive experimental results on the Pascal, Cityscapes, and COCO benchmarks show that CSL performs favorably against state-of-the-art methods. Code and model weights are available at this https URL.</li>
<li><strong>摘要：</strong>尽管对于半监督语义分割的伪标签生成中存在显着进步，但伪标签的选择仍在研究中。现有方法通常使用固定的置信阈值来保留高信心预测作为伪标签。但是，这些方法无法应对网络过度的趋势，在高信区域中正确和错误的预测显着重叠，从而使分离具有挑战性和放大模型认知偏差。同时，低信仰预测的直接丢弃会破坏空间语义的连续性，从而导致关键上下文损失。我们建议信心可分离学习（CSL）来解决这些局限性。 CSL将伪标签选择作为置信分布特征空间内的凸优化问题，建立了特定于样本的决策范围，以区分可靠的预测和不可靠的预测。此外，CSL还引入了可靠像素的随机掩盖，以指导网络从低可元性区域学习上下文关系中，从而减轻了丢弃不确定预测的不利影响。对Pascal，CityScapes和Coco基准测试的广泛实验结果表明，CSL对最新方法的表现良好。代码和型号权重可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Pain in 3D: Generating Controllable Synthetic Faces for Automated Pain Assessment</h3>
<ul>
<li><strong>Authors: </strong>Xin Lei Lin, Soroush Mehraban, Abhishek Moturu, Babak Taati</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16727">https://arxiv.org/abs/2509.16727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16727">https://arxiv.org/pdf/2509.16727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16727]] Pain in 3D: Generating Controllable Synthetic Faces for Automated Pain Assessment(https://arxiv.org/abs/2509.16727)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Automated pain assessment from facial expressions is crucial for non-communicative patients, such as those with dementia. Progress has been limited by two challenges: (i) existing datasets exhibit severe demographic and label imbalance due to ethical constraints, and (ii) current generative models cannot precisely control facial action units (AUs), facial structure, or clinically validated pain levels. We present 3DPain, a large-scale synthetic dataset specifically designed for automated pain assessment, featuring unprecedented annotation richness and demographic diversity. Our three-stage framework generates diverse 3D meshes, textures them with diffusion models, and applies AU-driven face rigging to synthesize multi-view faces with paired neutral and pain images, AU configurations, PSPI scores, and the first dataset-level annotations of pain-region heatmaps. The dataset comprises 82,500 samples across 25,000 pain expression heatmaps and 2,500 synthetic identities balanced by age, gender, and ethnicity. We further introduce ViTPain, a Vision Transformer based cross-modal distillation framework in which a heatmap-trained teacher guides a student trained on RGB images, enhancing accuracy, interpretability, and clinical reliability. Together, 3DPain and ViTPain establish a controllable, diverse, and clinically grounded foundation for generalizable automated pain assessment.</li>
<li><strong>摘要：</strong>面部表情的自动疼痛评估对于非交流性患者（例如患有痴呆症患者）至关重要。进展受到两个挑战的限制：（i）由于道德限制而导致的现有数据集表现出严重的人口统计学和标签失衡，并且（ii）当前的生成模型无法精确控制面部动作单位（AUS），面部结构或经过临床验证的疼痛水平。我们提出3DPAIN，这是一种专门为自动疼痛评估而设计的大规模合成数据集，具有前所未有的注释丰富性和人口统计学多样性。我们的三阶段框架生成了不同的3D网眼，用扩散模型进行纹理，并应用Au驱动的面部索具可通过配对中性和疼痛图像，AU配置，PSPI得分以及对疼痛区域热图的第一个数据集级别的注释来合成多视图面。该数据集包含25,000个疼痛表达热图中的82,500个样品，以及按年龄，性别和种族平衡的2,500个合成身份。我们进一步介绍了VITPAIN，这是一个基于视觉变压器的跨模式蒸馏框架，其中由热图训练的教师指导了接受RGB图像培训的学生，从而提高了准确性，可解释性和临床可靠性。 3DPAIN和VITPAIN共同为可控，多样化且临床上的基础建立了可概括的自动疼痛评估的基础。</li>
</ul>

<h3>Title: DiffEye: Diffusion-Based Continuous Eye-Tracking Data Generation Conditioned on Natural Images</h3>
<ul>
<li><strong>Authors: </strong>Ozgur Kara, Harris Nisar, James M. Rehg</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16767">https://arxiv.org/abs/2509.16767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16767">https://arxiv.org/pdf/2509.16767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16767]] DiffEye: Diffusion-Based Continuous Eye-Tracking Data Generation Conditioned on Natural Images(https://arxiv.org/abs/2509.16767)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Numerous models have been developed for scanpath and saliency prediction, which are typically trained on scanpaths, which model eye movement as a sequence of discrete fixation points connected by saccades, while the rich information contained in the raw trajectories is often discarded. Moreover, most existing approaches fail to capture the variability observed among human subjects viewing the same image. They generally predict a single scanpath of fixed, pre-defined length, which conflicts with the inherent diversity and stochastic nature of real-world visual attention. To address these challenges, we propose DiffEye, a diffusion-based training framework designed to model continuous and diverse eye movement trajectories during free viewing of natural images. Our method builds on a diffusion model conditioned on visual stimuli and introduces a novel component, namely Corresponding Positional Embedding (CPE), which aligns spatial gaze information with the patch-based semantic features of the visual input. By leveraging raw eye-tracking trajectories rather than relying on scanpaths, DiffEye captures the inherent variability in human gaze behavior and generates high-quality, realistic eye movement patterns, despite being trained on a comparatively small dataset. The generated trajectories can also be converted into scanpaths and saliency maps, resulting in outputs that more accurately reflect the distribution of human visual attention. DiffEye is the first method to tackle this task on natural images using a diffusion model while fully leveraging the richness of raw eye-tracking data. Our extensive evaluation shows that DiffEye not only achieves state-of-the-art performance in scanpath generation but also enables, for the first time, the generation of continuous eye movement trajectories. Project webpage: this https URL</li>
<li><strong>摘要：</strong>已经开发了许多用于扫描和显着性预测的模型，这些模型通常是在扫描路径上训练的，这些模型是通过扫视连接的一系列离散固定点的序列，而原始轨迹中包含的丰富信息通常被丢弃。此外，大多数现有方法无法捕获观看相同图像的人类受试者中观察到的可变性。他们通常可以预测固定的预定义长度的单个扫描路径，这与现实世界视觉关注的固有多样性和随机性质冲突。为了应对这些挑战，我们提出了Difeye，这是一个基于扩散的训练框架，旨在在自然图像的自由观看过程中对连续和多样的眼动轨迹进行建模。我们的方法建立在以视觉刺激为条件的扩散模型的基础上，并引入了一种新颖的组件，即相应的位置嵌入（CPE），该组件将空间凝视信息与视觉输入的基于贴片的语义特征保持一致。通过利用原始的眼睛追踪轨迹而不是依靠扫描路径，DiFeye捕获了人类凝视行为的固有变异性，并产生了高质量的，现实的眼动模式，尽管在相对小的数据集中接受了训练。生成的轨迹也可以转换为扫描路径和显着图，从而更准确地反映了人类视觉注意力的分布。 DiFeye是使用扩散模型在自然图像上解决此任务的第一种方法，同时充分利用了原始眼睛跟踪数据的丰富性。我们的广泛评估表明，DiFeye不仅在扫描Pathen生成中实现了最先进的性能，而且还可以使连续的眼动轨迹产生产生。项目网页：此HTTPS URL</li>
</ul>

<h3>Title: MMPart: Harnessing Multi-Modal Large Language Models for Part-Aware 3D Generation</h3>
<ul>
<li><strong>Authors: </strong>Omid Bonakdar, Nasser Mozayani</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16768">https://arxiv.org/abs/2509.16768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16768">https://arxiv.org/pdf/2509.16768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16768]] MMPart: Harnessing Multi-Modal Large Language Models for Part-Aware 3D Generation(https://arxiv.org/abs/2509.16768)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative 3D modeling has advanced rapidly, driven by applications in VR/AR, metaverse, and robotics. However, most methods represent the target object as a closed mesh devoid of any structural information, limiting editing, animation, and semantic understanding. Part-aware 3D generation addresses this problem by decomposing objects into meaningful components, but existing pipelines face challenges: in existing methods, the user has no control over which objects are separated and how model imagine the occluded parts in isolation phase. In this paper, we introduce MMPart, an innovative framework for generating part-aware 3D models from a single image. We first use a VLM to generate a set of prompts based on the input image and user descriptions. In the next step, a generative model generates isolated images of each object based on the initial image and the previous step's prompts as supervisor (which control the pose and guide model how imagine previously occluded areas). Each of those images then enters the multi-view generation stage, where a number of consistent images from different views are generated. Finally, a reconstruction model converts each of these multi-view images into a 3D model.</li>
<li><strong>摘要：</strong>在VR/AR，Metaverse和Robotics中的应用中，生成3D建模迅速发展。但是，大多数方法将目标对象表示为没有任何结构信息的封闭网格，限制了编辑，动画和语义理解。部分感知的3D生成通过将对象分解为有意义的组件来解决此问题，但是现有管道面临挑战：在现有方法中，用户无法控制哪些对象分离，并且模型如何想象隔离阶段的封闭零件。在本文中，我们介绍了MMPart，这是一个创新的框架，用于从单个图像中生成零件感知的3D模型。我们首先使用VLM根据输入映像和用户描述生成一组提示。在下一步中，生成模型根据初始图像和上一个步骤的提示（控制姿势和指南模型如何想象先前遮挡的区域）生成每个对象的孤立图像。然后，每个图像都进入多视图生成阶段，其中生成了许多一致的图像。最后，重建模型将这些多视图图像中的每一个都转换为3D模型。</li>
</ul>

<h3>Title: Looking in the mirror: A faithful counterfactual explanation method for interpreting deep image classification models</h3>
<ul>
<li><strong>Authors: </strong>Townim Faisal Chowdhury, Vu Minh Hieu Phan, Kewen Liao, Nanyu Dong, Minh-Son To, Anton Hengel, Johan Verjans, Zhibin Liao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16822">https://arxiv.org/abs/2509.16822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16822">https://arxiv.org/pdf/2509.16822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16822]] Looking in the mirror: A faithful counterfactual explanation method for interpreting deep image classification models(https://arxiv.org/abs/2509.16822)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Counterfactual explanations (CFE) for deep image classifiers aim to reveal how minimal input changes lead to different model decisions, providing critical insights for model interpretation and improvement. However, existing CFE methods often rely on additional image encoders and generative models to create plausible images, neglecting the classifier's own feature space and decision boundaries. As such, they do not explain the intrinsic feature space and decision boundaries learned by the classifier. To address this limitation, we propose Mirror-CFE, a novel method that generates faithful counterfactual explanations by operating directly in the classifier's feature space, treating decision boundaries as mirrors that ``reflect'' feature representations in the mirror. Mirror-CFE learns a mapping function from feature space to image space while preserving distance relationships, enabling smooth transitions between source images and their counterfactuals. Through extensive experiments on four image datasets, we demonstrate that Mirror-CFE achieves superior performance in validity while maintaining input resemblance compared to state-of-the-art explanation methods. Finally, mirror-CFE provides interpretable visualization of the classifier's decision process by generating step-wise transitions that reveal how features evolve as classification confidence changes.</li>
<li><strong>摘要：</strong>对深图像分类器的反事实解释（CFE）旨在揭示最小的输入变化如何导致不同的模型决策，从而为模型解释和改进提供了关键的见解。但是，现有的CFE方法通常依靠其他图像编码器和生成模型来创建合理的图像，忽略了分类器自己的特征空间和决策边界。因此，他们不能解释分类器学到的内在特征空间和决策界限。为了解决这个限制，我们提出了一种新颖的方法，这是一种新颖的方法，该方法通过直接在分类器的特征空间中操作来产生忠实的反事实解释，将决策边界视为镜子中``反映''特征表示的镜像。 Mirror-CFE在保留距离关系的同时，学习了从特征空间到图像空间的映射功能，从而在源图像及其反事实之间进行平滑过渡。通过在四个图像数据集上进行的大量实验，我们证明了Mirror-CFE与最新的解释方法相比，在保持输入相似之处的同时，在保持有效性方面具有出色的性能。最后，Mirror-CFE通过生成逐步过渡来揭示特征如何随着分类置信度的变化而发展的逐步过渡，从而提供了可解释的分类器决策过程。</li>
</ul>

<h3>Title: $\mathtt{M^3VIR}$: A Large-Scale Multi-Modality Multi-View Synthesized Benchmark Dataset for Image Restoration and Content Creation</h3>
<ul>
<li><strong>Authors: </strong>Yuanzhi Li, Lebin Zhou, Nam Ling, Zhenghao Chen, Wei Wang, Wei Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16873">https://arxiv.org/abs/2509.16873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16873">https://arxiv.org/pdf/2509.16873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16873]] $\mathtt{M^3VIR}$: A Large-Scale Multi-Modality Multi-View Synthesized Benchmark Dataset for Image Restoration and Content Creation(https://arxiv.org/abs/2509.16873)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, super-resolution, generation, generative</a></li>
<li><strong>Abstract: </strong>The gaming and entertainment industry is rapidly evolving, driven by immersive experiences and the integration of generative AI (GAI) technologies. Training such models effectively requires large-scale datasets that capture the diversity and context of gaming environments. However, existing datasets are often limited to specific domains or rely on artificial degradations, which do not accurately capture the unique characteristics of gaming content. Moreover, benchmarks for controllable video generation remain absent. To address these limitations, we introduce $\mathtt{M^3VIR}$, a large-scale, multi-modal, multi-view dataset specifically designed to overcome the shortcomings of current resources. Unlike existing datasets, $\mathtt{M^3VIR}$ provides diverse, high-fidelity gaming content rendered with Unreal Engine 5, offering authentic ground-truth LR-HR paired and multi-view frames across 80 scenes in 8 categories. It includes $\mathtt{M^3VIR\_MR}$ for super-resolution (SR), novel view synthesis (NVS), and combined NVS+SR tasks, and $\mathtt{M^3VIR\_{MS}}$, the first multi-style, object-level ground-truth set enabling research on controlled video generation. Additionally, we benchmark several state-of-the-art SR and NVS methods to establish performance baselines. While no existing approaches directly handle controlled video generation, $\mathtt{M^3VIR}$ provides a benchmark for advancing this area. By releasing the dataset, we aim to facilitate research in AI-powered restoration, compression, and controllable content generation for next-generation cloud gaming and entertainment.</li>
<li><strong>摘要：</strong>游戏和娱乐业正在迅速发展，这是由沉浸式体验和生成AI（GAI）技术的整合驱动的。有效地培训此类模型需要大规模数据集来捕获游戏环境的多样性和背景。但是，现有数据集通常仅限于特定域或依赖人工降解，而人工降解并不能准确捕获游戏内容的独特特征。此外，仍缺乏可控视频的基准。为了解决这些限制，我们介绍了$ \ mathtt {m^3vir} $，这是一种大规模的多模式，多视图数据集，专门设计用于克服当前资源的缺点。与现有数据集不同，$ \ mathtt {m^3vir} $提供了多样的，高保真的游戏内容，并用虚幻引擎5呈现，在8个类别的80个场景中提供了真实的地面LR-HR配对和多视图框架。它包括$ \ Mathtt {M^3vir \ _mr} $用于超分辨率（SR），新颖的视图合成（NVS）和组合的NVS+SR任务，以及$ \ Mathtt {M^3vir \ _ {MS}} $，首次进行多型式的“启用”启用，以下设置了启动的研究。此外，我们基于建立性能基准的几种最先进的SR和NVS方法。尽管没有现有的方法直接处理受控视频生成，但$ \ mathtt {m^3vir} $为推进此区域提供了基准。通过释放数据集，我们旨在促进下一代云游戏和娱乐的AI驱动恢复，压缩和可控内容的研究。</li>
</ul>

<h3>Title: PRISM: Precision-Recall Informed Data-Free Knowledge Distillation via Generative Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Xuewan He, Jielei Wang, Zihan Cheng, Yuchen Su, Shiyue Huang, Guoming Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16897">https://arxiv.org/abs/2509.16897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16897">https://arxiv.org/pdf/2509.16897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16897]] PRISM: Precision-Recall Informed Data-Free Knowledge Distillation via Generative Diffusion(https://arxiv.org/abs/2509.16897)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Data-free knowledge distillation (DFKD) transfers knowledge from a teacher to a student without access to the real in-distribution (ID) data. While existing methods perform well on small-scale images, they suffer from mode collapse when synthesizing large-scale images, resulting in limited knowledge transfer. Recently, leveraging advanced generative models to synthesize photorealistic images has emerged as a promising alternative. Nevertheless, directly using off-the-shelf diffusion to generate datasets faces the precision-recall challenges: 1) ensuring synthetic data aligns with the real distribution, and 2) ensuring coverage of the real ID manifold. In response, we propose PRISM, a precision-recall informed synthesis method. Specifically, we introduce Energy-guided Distribution Alignment to avoid the generation of out-of-distribution samples, and design the Diversified Prompt Engineering to enhance coverage of the real ID manifold. Extensive experiments on various large-scale image datasets demonstrate the superiority of PRISM. Moreover, we demonstrate that models trained with PRISM exhibit strong domain generalization.</li>
<li><strong>摘要：</strong>无数据知识蒸馏（DFKD）将知识从教师转移到学生，而无需访问实际分布（ID）数据。尽管现有方法在小规模图像上表现良好，但在合成大型图像时，它们会遭受模式崩溃，从而导致知识传递有限。最近，利用先进的生成模型综合影像现实主义图像已成为有希望的替代方法。然而，直接使用现成的扩散来生成数据集面临着精确核心挑战：1）确保合成数据与真实分布对齐，以及2）确保对真实ID歧管的覆盖范围。作为回应，我们提出了Prism，一种Precision-Recall知情合成方法。具体而言，我们引入了能源引导的分配对齐方式，以避免产生分发样本，并设计多元化的及时工程以增强对真实ID歧管的覆盖范围。在各种大型图像数据集上进行的广泛实验证明了棱镜的优势。此外，我们证明了经过棱镜训练的模型表现出强大的领域概括。</li>
</ul>

<h3>Title: Prototype-Based Pseudo-Label Denoising for Source-Free Domain Adaptation in Remote Sensing Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Bin Wang, Fei Deng, Zeyu Chen, Zhicheng Yu, Yiguang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16942">https://arxiv.org/abs/2509.16942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16942">https://arxiv.org/pdf/2509.16942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16942]] Prototype-Based Pseudo-Label Denoising for Source-Free Domain Adaptation in Remote Sensing Semantic Segmentation(https://arxiv.org/abs/2509.16942)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Source-Free Domain Adaptation (SFDA) enables domain adaptation for semantic segmentation of Remote Sensing Images (RSIs) using only a well-trained source model and unlabeled target domain data. However, the lack of ground-truth labels in the target domain often leads to the generation of noisy pseudo-labels. Such noise impedes the effective mitigation of domain shift (DS). To address this challenge, we propose ProSFDA, a prototype-guided SFDA framework. It employs prototype-weighted pseudo-labels to facilitate reliable self-training (ST) under pseudo-labels noise. We, in addition, introduce a prototype-contrast strategy that encourages the aggregation of features belonging to the same class, enabling the model to learn discriminative target domain representations without relying on ground-truth supervision. Extensive experiments show that our approach substantially outperforms existing methods.</li>
<li><strong>摘要：</strong>无源域的适应（SFDA）仅使用训练有素的源模型和未标记的目标域数据，可以适应遥感图像（RSIS）的语义分割域的适应性。但是，目标结构域缺乏地面标签通常会导致嘈杂的伪标签的产生。这种噪声阻碍了域移动（DS）的有效缓解。为了应对这一挑战，我们提出了一个原型引导的SFDA框架Prosfda。它采用原型加权伪标签来促进伪标签噪声下可靠的自我训练（ST）。此外，我们还引入了一种原型对比策略，该策略鼓励属于同一类的特征的汇总，从而使模型能够学习歧视性目标域表示，而无需依靠地面真实的监督。广泛的实验表明，我们的方法基本上要优于现有方法。</li>
</ul>

<h3>Title: Catching the Details: Self-Distilled RoI Predictors for Fine-Grained MLLM Perception</h3>
<ul>
<li><strong>Authors: </strong>Yuheng Shi, Xiaohuan Pei, Minjing Dong, Chang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16944">https://arxiv.org/abs/2509.16944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16944">https://arxiv.org/pdf/2509.16944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16944]] Catching the Details: Self-Distilled RoI Predictors for Fine-Grained MLLM Perception(https://arxiv.org/abs/2509.16944)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) require high-resolution visual information to perform fine-grained perception, yet processing entire high-resolution images is computationally prohibitive. While recent methods leverage a Region-of-Interest (RoI) mechanism to focus on salient areas, they typically present a difficult trade-off: training-based approaches depend on large-scale annotated datasets, while training-free methods that utilize the model's internal attention are computationally inefficient and less accurate, requiring either multi-pass prefill stages or reliance on the slow auto-regressive decoding process. In this paper, we propose an efficient, annotation-free Self-Distilled Region Proposal Network (SD-RPN) that resolves this trade-off. The SD-RPN is built around a pipeline that transforms the noisy attention maps from the MLLM's middle layers into high-quality pseudo-RoI labels by explicitly denoising the signal and resolving ambiguity. We use these labels to train a lightweight Region Proposal Network (RPN) that learns a more precise localization. This RPN is also highly efficient, predicting the RoI in a single forward pass using features from the MLLM's middle layers, decoupling RoI identification from the auto-regressive generation and avoiding costly multi-pass this http URL validate our approach, we integrate the framework into the LLaVA-1.5 architecture. Despite being trained on only a few (e.g. 10K) question-answer pairs, our method demonstrates exceptional data efficiency and generalization, achieving over a 10% absolute accuracy improvement on unseen benchmarks, including TextVQA, DocVQA, and V-Star. Our work presents a practical and scalable solution for enhancing the fine-grained perception of MLLMs without requiring costly supervision or full model fine-tuning. Code is available at this https URL.</li>
<li><strong>摘要：</strong>多模式大型语言模型（MLLM）需要高分辨率的视觉信息来执行细粒度的感知，但是处理整个高分辨率图像在计算上都是过于刺激的。尽管最近的方法利用了利益区域（ROI）机制专注于显着领域，但它们通常会带来一个困难的权衡：基于培训的方法取决于大规模注释的数据集，而利用模型内部注意力的训练无训练方法在计算上却效率低下且准确性较低，需要较低的速度较高的速度较高的较高的效果阶段，从而缓慢自动编码。在本文中，我们提出了解决这一权衡的高效，无注释的自缩合区域提案网络（SD-RPN）。 SD-RPN围绕一条管道构建，该管道将嘈杂的注意力图从MLLM的中层转化为高质量的伪ROI标签，通过明确降低信号并解决歧义。我们使用这些标签来培训轻量级区域提案网络（RPN），该网络学习更精确的本地化。该RPN也非常有效，可以使用MLLM中间层的功能来预测单个正向通行证的ROI，从而将自动回归生成的ROI识别解耦，并避免了这种HTTP URL验证我们的方法的代价高昂的多通，我们将框架集成到LLAVA-1.5体系结构中。尽管仅接受了少数（例如10K）的提问对，但我们的方法表现出了出色的数据效率和概括，在未看到的基准测试的绝对准确性提高了10％的绝对准确性上，包括TextVQA，DOCVQA和V-Star。我们的工作提出了一种实用且可扩展的解决方案，用于增强MLLM的细粒度感知，而无需昂贵的监督或完整的模型进行微调。代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Leveraging RGB Images for Pre-Training of Event-Based Hand Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Ruicong Liu, Takehiko Ohkawa, Tze Ho Elden Tse, Mingfang Zhang, Angela Yao, Yoichi Sato</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16949">https://arxiv.org/abs/2509.16949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16949">https://arxiv.org/pdf/2509.16949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16949]] Leveraging RGB Images for Pre-Training of Event-Based Hand Pose Estimation(https://arxiv.org/abs/2509.16949)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This paper presents RPEP, the first pre-training method for event-based 3D hand pose estimation using labeled RGB images and unpaired, unlabeled event data. Event data offer significant benefits such as high temporal resolution and low latency, but their application to hand pose estimation is still limited by the scarcity of labeled training data. To address this, we repurpose real RGB datasets to train event-based estimators. This is done by constructing pseudo-event-RGB pairs, where event data is generated and aligned with the ground-truth poses of RGB images. Unfortunately, existing pseudo-event generation techniques assume stationary objects, thus struggling to handle non-stationary, dynamically moving hands. To overcome this, RPEP introduces a novel generation strategy that decomposes hand movements into smaller, step-by-step motions. This decomposition allows our method to capture temporal changes in articulation, constructing more realistic event data for a moving hand. Additionally, RPEP imposes a motion reversal constraint, regularizing event generation using reversed motion. Extensive experiments show that our pre-trained model significantly outperforms state-of-the-art methods on real event data, achieving up to 24% improvement on EvRealHands. Moreover, it delivers strong performance with minimal labeled samples for fine-tuning, making it well-suited for practical deployment.</li>
<li><strong>摘要：</strong>本文介绍了RPEP，这是使用标记的RGB图像和未配对的，未标记的事件数据的第一种基于事件的3D手姿势估算的预训练方法。事件数据提供了很大的好处，例如高时间分辨率和低潜伏期，但是它们的手工姿势估计的应用仍受到标记培训数据的稀缺的限制。为了解决这个问题，我们将真实的RGB数据集重新利用为训练基于事件的估计器。这是通过构建伪事件RGB对来完成的，其中生成事件数据并与RGB图像的地面真相姿势对齐。不幸的是，现有的伪事实生成技术假设固定物体，因此努力处理非平稳的，动态移动的手。为了克服这一点，RPEP引入了一种新颖的一代策略，该策略将手动运动分解为较小的逐步运动。这种分解使我们的方法可以捕获发音的时间变化，从而为移动的手构建更现实的事件数据。此外，RPEP施加了运动逆转约束，使用反向运动正规化事件的生成。广泛的实验表明，我们的预训练模型对真实事件数据的最先进方法显着超过了evrealhands的24％改善。此外，它通过最小的标签样品进行微调提供了强大的性能，使其非常适合实践部署。</li>
</ul>

<h3>Title: VidCLearn: A Continual Learning Approach for Text-to-Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Luca Zanchetta, Lorenzo Papa, Luca Maiano, Irene Amerini</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16956">https://arxiv.org/abs/2509.16956</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16956">https://arxiv.org/pdf/2509.16956</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16956]] VidCLearn: A Continual Learning Approach for Text-to-Video Generation(https://arxiv.org/abs/2509.16956)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Text-to-video generation is an emerging field in generative AI, enabling the creation of realistic, semantically accurate videos from text prompts. While current models achieve impressive visual quality and alignment with input text, they typically rely on static knowledge, making it difficult to incorporate new data without retraining from scratch. To address this limitation, we propose VidCLearn, a continual learning framework for diffusion-based text-to-video generation. VidCLearn features a student-teacher architecture where the student model is incrementally updated with new text-video pairs, and the teacher model helps preserve previously learned knowledge through generative replay. Additionally, we introduce a novel temporal consistency loss to enhance motion smoothness and a video retrieval module to provide structural guidance at inference. Our architecture is also designed to be more computationally efficient than existing models while retaining satisfactory generation performance. Experimental results show VidCLearn's superiority over baseline methods in terms of visual quality, semantic alignment, and temporal coherence.</li>
<li><strong>摘要：</strong>文本到视频生成是生成AI的新兴领域，从而从文本提示中创建了现实的，语义上准确的视频。尽管当前模型具有令人印象深刻的视觉质量和与输入文本的对齐，但它们通常依赖于静态知识，因此很难在不从头开始重新培训的新数据。为了解决这一限制，我们提出了Vidclearn，这是一个基于扩散的文本到视频生成的持续学习框架。 Vidclearn具有学生教师的体系结构，其中学生模型通过新的文本视频对进行了逐步更新，而教师模型可以通过生成重播来维护以前学习的知识。此外，我们引入了一种新型的时间一致性损失，以增强运动平滑度和视频检索模块，以提供推理结构指导。我们的体系结构还旨在在保持令人满意的生成性能的同时，比现有模型更有效地计算效率。实验结果表明，在视觉质量，语义比对和时间连贯性方面，Vidclearn优于基线方法。</li>
</ul>

<h3>Title: Penalizing Boundary Activation for Object Completeness in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Haoyang Xu, Tianhao Zhao, Sibei Yang, Yutian Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16968">https://arxiv.org/abs/2509.16968</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16968">https://arxiv.org/pdf/2509.16968</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16968]] Penalizing Boundary Activation for Object Completeness in Diffusion Models(https://arxiv.org/abs/2509.16968)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as a powerful technique for text-to-image (T2I) generation, creating high-quality, diverse images across various domains. However, a common limitation in these models is the incomplete display of objects, where fragments or missing parts undermine the model's performance in downstream applications. In this study, we conduct an in-depth analysis of the incompleteness issue and reveal that the primary factor behind incomplete object generation is the usage of RandomCrop during model training. This widely used data augmentation method, though enhances model generalization ability, disrupts object continuity during training. To address this, we propose a training-free solution that penalizes activation values at image boundaries during the early denoising steps. Our method is easily applicable to pre-trained Stable Diffusion models with minimal modifications and negligible computational overhead. Extensive experiments demonstrate the effectiveness of our method, showing substantial improvements in object integrity and image quality.</li>
<li><strong>摘要：</strong>扩散模型已成为文本对图像（T2I）生成的强大技术，创建了各个领域的高质量，多样的图像。但是，这些模型中的一个共同限制是对象的不完整显示，其中片段或丢失的部分破坏了模型在下游应用程序中的性能。在这项研究中，我们对不完整问题进行了深入的分析，并揭示了不完整的物体产生背后的主要因素是在模型训练过程中使用Random Crop。这种广泛使用的数据增强方法，尽管增强了模型泛化能力，但仍破坏了训练期间的对象连续性。为了解决这个问题，我们提出了一个无训练解决方案，该解决方案在早期的剥离步骤中对图像边界处的激活值进行了惩罚。我们的方法很容易适用于具有最小修改和可忽略不计的计算开销的预训练的稳定扩散模型。广泛的实验证明了我们方法的有效性，显示了对象完整性和图像质量的实质性改善。</li>
</ul>

<h3>Title: VCE: Safe Autoregressive Image Generation via Visual Contrast Exploitation</h3>
<ul>
<li><strong>Authors: </strong>Feng Han, Chao Gong, Zhipeng Wei, Jingjing Chen, Yu-Gang Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.16986">https://arxiv.org/abs/2509.16986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.16986">https://arxiv.org/pdf/2509.16986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.16986]] VCE: Safe Autoregressive Image Generation via Visual Contrast Exploitation(https://arxiv.org/abs/2509.16986)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recently, autoregressive image generation models have wowed audiences with their remarkable capability in creating surprisingly realistic images. Models such as GPT-4o and LlamaGen can not only produce images that faithfully mimic renowned artistic styles like Ghibli, Van Gogh, or Picasso, but also potentially generate Not-Safe-For-Work (NSFW) content, raising significant concerns regarding copyright infringement and ethical use. Despite these concerns, methods to safeguard autoregressive text-to-image models remain underexplored. Previous concept erasure methods, primarily designed for diffusion models that operate in denoising latent space, are not directly applicable to autoregressive models that generate images token by token. To address this critical gap, we propose Visual Contrast Exploitation (VCE), a novel framework comprising: (1) an innovative contrastive image pair construction paradigm that precisely decouples unsafe concepts from their associated content semantics, and (2) a sophisticated DPO-based training approach that enhances the model's ability to identify and leverage visual contrastive features from image pairs, enabling precise concept erasure. Our comprehensive experiments across three challenging tasks-artist style erasure, explicit content erasure, and object removal-demonstrate that our method effectively secures the model, achieving state-of-the-art results while erasing unsafe concepts and maintaining the integrity of unrelated safe concepts. The code and models are available at this https URL.</li>
<li><strong>摘要：</strong>最近，自回归图像生成模型以其出色的创建现实图像的出色能力使观众惊叹不已。诸如GPT-4O和Llamagen之类的模型不仅可以忠实地模仿吉卜力，梵高或毕加索等著名的艺术风格，而且还可以产生不可能的工作（NSFW）内容，从而引起了对版权侵权和道德使用的重要关注。尽管有这些担忧，但仍未充分利用自动回归文本到图像模型的方法。以前的概念擦除方法主要是为在降级潜在空间中运行的扩散模型而设计的，并不直接适用于产生令牌图像的自回旋模型。为了解决这一关键差距，我们提出了视觉对比度开发（VCE），这是一个新的框架，包括：（1）一种创新的对比图像对构造范式，该构建范式精确地将不安全的概念从其相关内容语义上解脱出来，（2）基于DPO的训练方法，可增强模型的能力来识别和构成视觉对比的构图，以实现图像的构图。我们在三个具有挑战性的艺术家风格擦除，明确的内容擦除和对象去除方面进行的全面实验表明，我们的方法有效地确保了模型，在擦除不安全概念并保持不相关的安全概念的完整性的同时，取得了最新的结果。代码和模型可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: DocIQ: A Benchmark Dataset and Feature Fusion Network for Document Image Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Zhichao Ma, Fan Huang, Lu Zhao, Fengjun Guo, Guangtao Zhai, Xiongkuo Min</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17012">https://arxiv.org/abs/2509.17012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17012">https://arxiv.org/pdf/2509.17012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17012]] DocIQ: A Benchmark Dataset and Feature Fusion Network for Document Image Quality Assessment(https://arxiv.org/abs/2509.17012)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, quality assessment</a></li>
<li><strong>Abstract: </strong>Document image quality assessment (DIQA) is an important component for various applications, including optical character recognition (OCR), document restoration, and the evaluation of document image processing systems. In this paper, we introduce a subjective DIQA dataset DIQA-5000. The DIQA-5000 dataset comprises 5,000 document images, generated by applying multiple document enhancement techniques to 500 real-world images with diverse distortions. Each enhanced image was rated by 15 subjects across three rating dimensions: overall quality, sharpness, and color fidelity. Furthermore, we propose a specialized no-reference DIQA model that exploits document layout features to maintain quality perception at reduced resolutions to lower computational cost. Recognizing that image quality is influenced by both low-level and high-level visual features, we designed a feature fusion module to extract and integrate multi-level features from document images. To generate multi-dimensional scores, our model employs independent quality heads for each dimension to predict score distributions, allowing it to learn distinct aspects of document image quality. Experimental results demonstrate that our method outperforms current state-of-the-art general-purpose IQA models on both DIQA-5000 and an additional document image dataset focused on OCR accuracy.</li>
<li><strong>摘要：</strong>文档图像质量评估（DIQA）是各种应用程序的重要组成部分，包括光学特征识别（OCR），文档修复以及文档图像处理系统的评估。在本文中，我们介绍了一个主观的DIQA数据集DIQA-5000。 DIQA-5000数据集包含5,000张文档图像，通过将多个文档增强技术应用于500个具有多种失真的现实世界图像而生成。每个增强的图像均由三个评级维度的15个受试者进行评分：整体质量，清晰度和颜色保真度。此外，我们提出了一种专门的NO-NO-REFERIOND DIQA模型，该模型利用文档布局功能来维持质量感知，以减少分辨率以降低计算成本。认识到图像质量受到低级和高级视觉特征的影响，我们设计了一个功能融合模块，以从文档图像中提取和整合多层次功能。为了生成多维分数，我们的模型为每个维度采用独立的质量头来预测得分分布，从而可以学习文档图像质量的不同方面。实验结果表明，我们的方法优于DIQA-5000上的当前最新通用IQA模型，而侧重于OCR准确性的附加文档图像数据集。</li>
</ul>

<h3>Title: When Color-Space Decoupling Meets Diffusion for Adverse-Weather Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Wenxuan Fang, Jili Fan, Chao Wang, Xiantao Hu, Jiangwei Weng, Ying Tai, Jian Yang, Jun Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17024">https://arxiv.org/abs/2509.17024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17024">https://arxiv.org/pdf/2509.17024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17024]] When Color-Space Decoupling Meets Diffusion for Adverse-Weather Image Restoration(https://arxiv.org/abs/2509.17024)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Adverse Weather Image Restoration (AWIR) is a highly challenging task due to the unpredictable and dynamic nature of weather-related degradations. Traditional task-specific methods often fail to generalize to unseen or complex degradation types, while recent prompt-learning approaches depend heavily on the degradation estimation capabilities of vision-language models, resulting in inconsistent restorations. In this paper, we propose \textbf{LCDiff}, a novel framework comprising two key components: \textit{Lumina-Chroma Decomposition Network} (LCDN) and \textit{Lumina-Guided Diffusion Model} (LGDM). LCDN processes degraded images in the YCbCr color space, separately handling degradation-related luminance and degradation-invariant chrominance components. This decomposition effectively mitigates weather-induced degradation while preserving color fidelity. To further enhance restoration quality, LGDM leverages degradation-related luminance information as a guiding condition, eliminating the need for explicit degradation prompts. Additionally, LGDM incorporates a \textit{Dynamic Time Step Loss} to optimize the denoising network, ensuring a balanced recovery of both low- and high-frequency features in the image. Finally, we present DriveWeather, a comprehensive all-weather driving dataset designed to enable robust evaluation. Extensive experiments demonstrate that our approach surpasses state-of-the-art methods, setting a new benchmark in AWIR. The dataset and code are available at: this https URL.</li>
<li><strong>摘要：</strong>由于天气相关的降解的不可预测性和动态性质，不利天气图像恢复（AWIR）是一项高度挑战的任务。传统特定于任务的方法通常无法概括地看不见或复杂的降解类型，而最近的及时学习方法在很大程度上取决于视觉模型的降解估计能力，从而导致修复不一致。在本文中，我们建议\ textbf {lcdiff}，这是一个包含两个关键组件的新型框架：\ textIt {lumina-chroma demomposition网络}（lcdn）和\ textit {lumina引导的扩散模型}（LGDM）。 LCDN在YCBCR颜色空间中处理降解图像，分别处理与降解相关的亮度和降解不变的色度成分。这种分解有效地减轻了天气引起的降解，同时保留了颜色保真度。为了进一步提高恢复质量，LGDM利用与降解相关的亮度信息作为指导条件，从而消除了明确降解提示的需求。此外，LGDM结合了一个\ textIt {动态时间步长}，以优化denoising网络，从而确保图像中低频和高频功能的平衡恢复。最后，我们介绍了DriveWeather，这是一个全天候的全天候驾驶数据集，旨在实现强大的评估。广泛的实验表明，我们的方法超过了最先进的方法，在AWIR中设定了新的基准。数据集和代码可在以下网址提供：此HTTPS URL。</li>
</ul>

<h3>Title: AlignedGen: Aligning Style Across Generated Images</h3>
<ul>
<li><strong>Authors: </strong>Jiexuan Zhang, Yiheng Du, Qian Wang, Weiqi Li, Yu Gu, Jian Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17088">https://arxiv.org/abs/2509.17088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17088">https://arxiv.org/pdf/2509.17088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17088]] AlignedGen: Aligning Style Across Generated Images(https://arxiv.org/abs/2509.17088)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Despite their generative power, diffusion models struggle to maintain style consistency across images conditioned on the same style prompt, hindering their practical deployment in creative workflows. While several training-free methods attempt to solve this, they are constrained to the U-Net architecture, which not only leads to low-quality results and artifacts like object repetition but also renders them incompatible with superior Diffusion Transformer (DiT). To address these issues, we introduce AlignedGen, a novel training-free framework that enhances style consistency across images generated by DiT models. Our work first reveals a critical insight: naive attention sharing fails in DiT due to conflicting positional signals from improper position embeddings. We introduce Shifted Position Embedding (ShiftPE), an effective solution that resolves this conflict by allocating a non-overlapping set of positional indices to each image. Building on this foundation, we develop Advanced Attention Sharing (AAS), a suite of three techniques meticulously designed to fully unleash the potential of attention sharing within the DiT. Furthermore, to broaden the applicability of our method, we present an efficient query, key, and value feature extraction algorithm, enabling our method to seamlessly incorporate external images as style references. Extensive experimental results validate that our method effectively enhances style consistency across generated images while maintaining precise text-to-image alignment.</li>
<li><strong>摘要：</strong>尽管具有产生的力量，但扩散模型仍在以相同风格提示为条件的图像中保持风格一致性，从而阻碍了他们在创意工作流程中的实际部署。尽管几种无培训方法试图解决此问题，但它们被限制在U-NET体系结构上，这不仅导致质量低下的结果和诸如对象重复之类的人工制品，而且使它们与上级扩散变压器（DIT）不相容。为了解决这些问题，我们介绍了AlignedGen，这是一个新颖的无培训框架，可增强DIT模型生成的图像之间的样式一致性。我们的工作首先揭示了一个关键的见解：由于位置嵌入不当的位置信号冲突，DIT的幼稚注意力共享失败。我们引入了移位位置嵌入（ShiftPe），这是一种有效的解决方案，可以通过将一组非重叠的位置指数集分配给每个图像来解决这一冲突。在这个基础的基础上，我们开发了高级注意力共享（AAS），这是一套精心设计的三种技术，旨在完全释放DIT中的注意力共享的潜力。此外，为了扩大我们方法的适用性，我们提出了一个有效的查询，键和值特征提取算法，使我们的方法能够无缝地将外部图像作为样式参考。广泛的实验结果验证了我们的方法可以有效地增强生成的图像的样式一致性，同时保持精确的文本对齐对齐。</li>
</ul>

<h3>Title: Ultra-short-term solar power forecasting by deep learning and data reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Jinbao Wang, Jun Liu, Shiliang Zhang, Xuehui Ma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17095">https://arxiv.org/abs/2509.17095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17095">https://arxiv.org/pdf/2509.17095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17095]] Ultra-short-term solar power forecasting by deep learning and data reconstruction(https://arxiv.org/abs/2509.17095)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The integration of solar power has been increasing as the green energy transition rolls out. The penetration of solar power challenges the grid stability and energy scheduling, due to its intermittent energy generation. Accurate and near real-time solar power prediction is of critical importance to tolerant and support the permeation of distributed and volatile solar power production in the energy system. In this paper, we propose a deep-learning based ultra-short-term solar power prediction with data reconstruction. We decompose the data for the prediction to facilitate extensive exploration of the spatial and temporal dependencies within the data. Particularly, we reconstruct the data into low- and high-frequency components, using ensemble empirical model decomposition with adaptive noise (CEEMDAN). We integrate meteorological data with those two components, and employ deep-learning models to capture long- and short-term dependencies towards the target prediction period. In this way, we excessively exploit the features in historical data in predicting a ultra-short-term solar power production. Furthermore, as ultra-short-term prediction is vulnerable to local optima, we modify the optimization in our deep-learning training by penalizing long prediction intervals. Numerical experiments with diverse settings demonstrate that, compared to baseline models, the proposed method achieves improved generalization in data reconstruction and higher prediction accuracy for ultra-short-term solar power production.</li>
<li><strong>摘要：</strong>随着绿色能源过渡的推出，太阳能的整合一直在增加。太阳能的渗透挑战网格稳定性和能源调度，这是由于其间歇性能量产生。准确且近乎实时的太阳能预测对于能源系统中分布式和挥发性太阳能生产的渗透至关重要。在本文中，我们提出了一个基于深度学习的超短期太阳能预测，并通过数据重建。我们将数据分解为预测，以促进数据中空间和时间依赖性的广泛探索。特别是，我们使用合奏经验模型分解（Ceemdan）将数据重建为低频和高频组件。我们将气象数据与这两个组成部分整合在一起，并采用深度学习模型来捕获目标预测期的长期和短期依赖性。这样，我们过度利用了历史数据中的特征，以预测超短期的太阳能产生。此外，由于超短期预测容易受到局部优势的影响，我们通过惩罚长期预测间隔来修改我们深入学习培训的优化。具有不同设置的数值实验表明，与基线模型相比，所提出的方法可以改善数据重建的概括，并且超短期太阳能生产的较高预测准确性。</li>
</ul>

<h3>Title: The SAGES Critical View of Safety Challenge: A Global Benchmark for AI-Assisted Surgical Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Deepak Alapatt, Jennifer Eckhoff, Zhiliang Lyu, Yutong Ban, Jean-Paul Mazellier, Sarah Choksi, Kunyi Yang, 2024 CVS Challenge Consortium, Quanzheng Li, Filippo Filicori, Xiang Li, Pietro Mascagni, Daniel A. Hashimoto, Guy Rosman, Ozanan Meireles, Nicolas Padoy</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17100">https://arxiv.org/abs/2509.17100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17100">https://arxiv.org/pdf/2509.17100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17100]] The SAGES Critical View of Safety Challenge: A Global Benchmark for AI-Assisted Surgical Quality Assessment(https://arxiv.org/abs/2509.17100)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Advances in artificial intelligence (AI) for surgical quality assessment promise to democratize access to expertise, with applications in training, guidance, and accreditation. This study presents the SAGES Critical View of Safety (CVS) Challenge, the first AI competition organized by a surgical society, using the CVS in laparoscopic cholecystectomy, a universally recommended yet inconsistently performed safety step, as an exemplar of surgical quality assessment. A global collaboration across 54 institutions in 24 countries engaged hundreds of clinicians and engineers to curate 1,000 videos annotated by 20 surgical experts according to a consensus-validated protocol. The challenge addressed key barriers to real-world deployment in surgery, including achieving high performance, capturing uncertainty in subjective assessment, and ensuring robustness to clinical variability. To enable this scale of effort, we developed EndoGlacier, a framework for managing large, heterogeneous surgical video and multi-annotator workflows. Thirteen international teams participated, achieving up to a 17\% relative gain in assessment performance, over 80\% reduction in calibration error, and a 17\% relative improvement in robustness over the state-of-the-art. Analysis of results highlighted methodological trends linked to model performance, providing guidance for future research toward robust, clinically deployable AI for surgical quality assessment.</li>
<li><strong>摘要：</strong>手术质量评估的人工智能（AI）的进步有望在培训，指导和认证中应用专业知识的访问权限。这项研究介绍了Sage对安全性（CVS）挑战的批判性观点，这是手术社会组织的第一次AI竞赛，使用腹腔镜胆囊切除术中的CVS组织，这是一个普遍建议但不一致的安全步骤，作为外科手术质量评估的典范。根据一项共识验证的协议，在24个国家 /地区的54个机构进行了全球合作，邀请数百名临床医生和工程师策划20个手术专家注释的视频。这项挑战涉及手术中现实世界部署的关键障碍，包括达到高性能，捕获主观评估的不确定性以及确保对临床变异性的稳健性。为了实现这一规模的努力，我们开发了内射线器，这是一个用于管理大型，异构手术视频和多通道工作流程的框架。 13个国际团队参加了评估绩效的相对增长17％，校准误差降低了80 \％，并且在最先进的面前的鲁棒性相对相对提高了17 \％。对结果的分析强调了与模型性能相关的方法论趋势，为未来的研究提供了指导，以进行临床上可部署的AI进行手术质量评估。</li>
</ul>

<h3>Title: ScenGAN: Attention-Intensive Generative Model for Uncertainty-Aware Renewable Scenario Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Yifei Wu, Bo Wang, Jingshi Cui, Pei-chun Lin, Junzo Watada</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17119">https://arxiv.org/abs/2509.17119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17119">https://arxiv.org/pdf/2509.17119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17119]] ScenGAN: Attention-Intensive Generative Model for Uncertainty-Aware Renewable Scenario Forecasting(https://arxiv.org/abs/2509.17119)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>To address the intermittency of renewable energy source (RES) generation, scenario forecasting offers a series of stochastic realizations for predictive objects with superior flexibility and direct views. Based on a long time-series perspective, this paper explores uncertainties in the realms of renewable power and deep learning. Then, an uncertainty-aware model is meticulously designed for renewable scenario forecasting, which leverages an attention mechanism and generative adversarial networks (GANs) to precisely capture complex spatial-temporal dynamics. To improve the interpretability of uncertain behavior in RES generation, Bayesian deep learning and adaptive instance normalization (AdaIN) are incorporated to simulate typical patterns and variations. Additionally, the integration of meteorological information, forecasts, and historical trajectories in the processing layer improves the synergistic forecasting capability for multiscale periodic regularities. Numerical experiments and case analyses demonstrate that the proposed approach provides an appropriate interpretation for renewable uncertainty representation, including both aleatoric and epistemic uncertainties, and shows superior performance over state-of-the-art methods.</li>
<li><strong>摘要：</strong>为了解决可再生能源（RES）生成的间歇性，方案预测为具有较高灵活性和直接视图的预测对象提供了一系列随机实现。根据长时间的观点，本文探讨了可再生能力和深度学习领域的不确定性。然后，不确定性感知模型是针对可再生方案预测的精心设计的，该预测利用注意机制和生成性对抗网络（GAN）来精确捕获复杂的时空动力学。为了提高RES生成中不确定行为的解释性，贝叶斯深度学习和自适应实例归一化（ADAIN）被合并以模拟典型的模式和变化。此外，在处理层中的气象信息，预测和历史轨迹的整合增强了多尺度周期性规律性的协同预测能力。数值实验和案例分析表明，所提出的方法为可再生不确定性表示（包括核心和认知不确定性）提供了适当的解释，并且比最先进的方法表现出了优越的性能。</li>
</ul>

<h3>Title: Stencil: Subject-Driven Generation with Context Guidance</h3>
<ul>
<li><strong>Authors: </strong>Gordon Chen, Ziqi Huang, Cheston Tan, Ziwei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17120">https://arxiv.org/abs/2509.17120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17120">https://arxiv.org/pdf/2509.17120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17120]] Stencil: Subject-Driven Generation with Context Guidance(https://arxiv.org/abs/2509.17120)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent text-to-image diffusion models can generate striking visuals from text prompts, but they often fail to maintain subject consistency across generations and contexts. One major limitation of current fine-tuning approaches is the inherent trade-off between quality and efficiency. Fine-tuning large models improves fidelity but is computationally expensive, while fine-tuning lightweight models improves efficiency but compromises image fidelity. Moreover, fine-tuning pre-trained models on a small set of images of the subject can damage the existing priors, resulting in suboptimal results. To this end, we present Stencil, a novel framework that jointly employs two diffusion models during inference. Stencil efficiently fine-tunes a lightweight model on images of the subject, while a large frozen pre-trained model provides contextual guidance during inference, injecting rich priors to enhance generation with minimal overhead. Stencil excels at generating high-fidelity, novel renditions of the subject in less than a minute, delivering state-of-the-art performance and setting a new benchmark in subject-driven generation.</li>
<li><strong>摘要：</strong>最近的文本到图像扩散模型可以从文本提示中产生引人注目的视觉效果，但是它们通常无法在世代和上下文中保持主题一致性。当前微调方法的一个主要局限性是质量和效率之间的固有权衡。微调大型模型可提高忠诚度，但在计算上昂贵，而微调轻巧模型则提高了效率，但会损害图像保真度。此外，在受试者的一小部分图像上进行的微调预训练模型会损害现有先验，从而造成次优的结果。为此，我们提出了模具，这是一个新型框架，在推断期间共同采用了两个扩散模型。模板有效地对受试者的图像进行了轻巧的模型，而大型冷冻预训练的模型在推断过程中提供了上下文指导，并注入了丰富的先验，以最少的开销来增强产生。模板在不到一分钟的时间内就产生了高保真性，对主题的新颖性，提供最先进的性能，并在主题驱动的一代中树立新的基准。</li>
</ul>

<h3>Title: SynergyNet: Fusing Generative Priors and State-Space Models for Facial Beauty Prediction</h3>
<ul>
<li><strong>Authors: </strong>Djamel Eddine Boukhari</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17172">https://arxiv.org/abs/2509.17172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17172">https://arxiv.org/pdf/2509.17172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17172]] SynergyNet: Fusing Generative Priors and State-Space Models for Facial Beauty Prediction(https://arxiv.org/abs/2509.17172)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The automated prediction of facial beauty is a benchmark task in affective computing that requires a sophisticated understanding of both local aesthetic details (e.g., skin texture) and global facial harmony (e.g., symmetry, proportions). Existing models, based on either Convolutional Neural Networks (CNNs) or Vision Transformers (ViTs), exhibit inherent architectural biases that limit their performance; CNNs excel at local feature extraction but struggle with long-range dependencies, while ViTs model global relationships at a significant computational cost. This paper introduces the \textbf{Mamba-Diffusion Network (MD-Net)}, a novel dual-stream architecture that resolves this trade-off by delegating specialized roles to state-of-the-art models. The first stream leverages a frozen U-Net encoder from a pre-trained latent diffusion model, providing a powerful generative prior for fine-grained aesthetic qualities. The second stream employs a Vision Mamba (Vim), a modern state-space model, to efficiently capture global facial structure with linear-time complexity. By synergistically integrating these complementary representations through a cross-attention mechanism, MD-Net creates a holistic and nuanced feature space for prediction. Evaluated on the SCUT-FBP5500 benchmark, MD-Net sets a new state-of-the-art, achieving a Pearson Correlation of \textbf{0.9235} and demonstrating the significant potential of hybrid architectures that fuse generative and sequential modeling paradigms for complex visual assessment tasks.</li>
<li><strong>摘要：</strong>对面部美的自动预测是情感计算中的一项基准任务，需要对局部美学细节（例如皮肤纹理）和全球面部和谐（例如对称性，比例）进行复杂的了解。基于卷积神经网络（CNN）或视觉变压器（VIT）的现有模型表现出固有的建筑偏见，从而限制其性能； CNN在本地功能提取方面表现出色，但要与长期依赖关系挣扎，而VITS以大量的计算成本为模型。本文介绍了\ textbf {mamba-diffusion网络（MD-net）}，这是一种新颖的双流式体系结构，通过将专业角色委派给最新模型来解决这种权衡。第一流利用预先训练的潜扩散模型的冷冻U-NET编码器，为细粒度美学品质提供了强大的生成性生成性。第二个流采用了现代状态空间模型的视觉Mamba（VIM），以有效地捕获具有线性时间复杂性的全球面部结构。通过通过交叉注意机制协同整合这些互补表示，MD-NET创造了一个整体和细微的特征空间以进行预测。 MD-NET在SCUT-FBP5500基准测试中进行了评估，设置了一种新的最先进的艺术品，达到了\ textbf {0.9235}的Pearson相关性，并展示了混合体系结构的重要潜力，这些混合体系结构融合了生成生成和顺序的建模范式，以实现复杂的视觉评估任务。</li>
</ul>

<h3>Title: Echo-Path: Pathology-Conditioned Echo Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Kabir Hamzah Muhammad, Marawan Elbatel, Yi Qin, Xiaomeng Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17190">https://arxiv.org/abs/2509.17190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17190">https://arxiv.org/pdf/2509.17190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17190]] Echo-Path: Pathology-Conditioned Echo Video Generation(https://arxiv.org/abs/2509.17190)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Cardiovascular diseases (CVDs) remain the leading cause of mortality globally, and echocardiography is critical for diagnosis of both common and congenital cardiac conditions. However, echocardiographic data for certain pathologies are scarce, hindering the development of robust automated diagnosis models. In this work, we propose Echo-Path, a novel generative framework to produce echocardiogram videos conditioned on specific cardiac pathologies. Echo-Path can synthesize realistic ultrasound video sequences that exhibit targeted abnormalities, focusing here on atrial septal defect (ASD) and pulmonary arterial hypertension (PAH). Our approach introduces a pathology-conditioning mechanism into a state-of-the-art echo video generator, allowing the model to learn and control disease-specific structural and motion patterns in the heart. Quantitative evaluation demonstrates that the synthetic videos achieve low distribution distances, indicating high visual fidelity. Clinically, the generated echoes exhibit plausible pathology markers. Furthermore, classifiers trained on our synthetic data generalize well to real data and, when used to augment real training sets, it improves downstream diagnosis of ASD and PAH by 7\% and 8\% respectively. Code, weights and dataset are available here this https URL</li>
<li><strong>摘要：</strong>心血管疾病（CVD）仍然是全球死亡率的主要原因，超声心动图对于诊断常见和先天性心脏状况至关重要。但是，某些病理的超声心动图数据稀缺，阻碍了强大的自动诊断模型的发展。在这项工作中，我们提出了Echo-Path，这是一种新型的生成框架，以生成以特定心脏病理为条件的超声心动图视频。 Echo-Path可以合成具有靶向异常的现实超声视频序列，重点是心房间隔缺陷（ASD）和肺动脉高压（PAH）。我们的方法将病理条件的机制引入了最新的回声视频发生器，从而使模型可以在心脏中学习和控制特定于疾病的结构和运动模式。定量评估表明，合成视频达到了低分布距离，表明视觉效果很高。在临床上，产生的回声表现出合理的病理标记。此外，经过培训的合成数据的分类器可以很好地推广到真实数据，并且在用于增强实际训练集的情况下，它将ASD和PAH的下游诊断分别提高了7 \％和8 \％。代码，权重和数据集可在此提供此HTTPS URL</li>
</ul>

<h3>Title: SignalLLM: A General-Purpose LLM Agent Framework for Automated Signal Processing</h3>
<ul>
<li><strong>Authors: </strong>Junlong Ke, Qiying Hu, Shenghai Yuan, Yuecong Xu, Jianfei Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17197">https://arxiv.org/abs/2509.17197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17197">https://arxiv.org/pdf/2509.17197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17197]] SignalLLM: A General-Purpose LLM Agent Framework for Automated Signal Processing(https://arxiv.org/abs/2509.17197)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Modern signal processing (SP) pipelines, whether model-based or data-driven, often constrained by complex and fragmented workflow, rely heavily on expert knowledge and manual engineering, and struggle with adaptability and generalization under limited data. In contrast, Large Language Models (LLMs) offer strong reasoning capabilities, broad general-purpose knowledge, in-context learning, and cross-modal transfer abilities, positioning them as powerful tools for automating and generalizing SP workflows. Motivated by these potentials, we introduce SignalLLM, the first general-purpose LLM-based agent framework for general SP tasks. Unlike prior LLM-based SP approaches that are limited to narrow applications or tricky prompting, SignalLLM introduces a principled, modular architecture. It decomposes high-level SP goals into structured subtasks via in-context learning and domain-specific retrieval, followed by hierarchical planning through adaptive retrieval-augmented generation (RAG) and refinement; these subtasks are then executed through prompt-based reasoning, cross-modal reasoning, code synthesis, model invocation, or data-driven LLM-assisted modeling. Its generalizable design enables the flexible selection of problem solving strategies across different signal modalities, task types, and data conditions. We demonstrate the versatility and effectiveness of SignalLLM through five representative tasks in communication and sensing, such as radar target detection, human activity recognition, and text compression. Experimental results show superior performance over traditional and existing LLM-based methods, particularly in few-shot and zero-shot settings.</li>
<li><strong>摘要：</strong>现代信号处理（SP）管道，无论是基于模型还是数据驱动的管道，通常受复杂且分散的工作流程约束，在很大程度上依赖于专家知识和手动工程，并在有限的数据下与适应性和概括斗争。相比之下，大型语言模型（LLMS）具有强大的推理能力，广泛的通用知识，内在学习和跨模式转移能力，将它们定位为自动化和推广SP工作流的强大工具。在这些潜力的驱动下，我们引入了Insigalllm，这是第一个用于通用SP任务的基于LLM的通用代理框架。与以前的基于LLM的SP方法仅限于狭窄的应用程序或棘手的提示，Signalllm引入了原则上的模块化体系结构。它通过内在的学习和特定领域的检索将高级SP目标分解为结构化的子任务，然后通过自适应检索效果（RAG）和改进来分层计划；然后，这些子任务是通过基于及时的推理，跨模式推理，代码合成，模型调用或数据驱动的LLM辅助建模来执行的。它的可推广设计可以灵活地选择不同信号，任务类型和数据条件的问题解决策略。我们通过五个代表性任务在交流和传感中（例如雷达目标检测，人类活动识别和文本压缩）中证明了信号的多功能性和有效性。实验结果表明，与传统和现有的基于LLM的方法相比，表现出色，尤其是在少量和零弹位设置中。</li>
</ul>

<h3>Title: Conditional Policy Generator for Dynamic Constraint Satisfaction and Optimization</h3>
<ul>
<li><strong>Authors: </strong>Wook Lee, Frans A. Oliehoek</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17205">https://arxiv.org/abs/2509.17205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17205">https://arxiv.org/pdf/2509.17205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17205]] Conditional Policy Generator for Dynamic Constraint Satisfaction and Optimization(https://arxiv.org/abs/2509.17205)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Leveraging machine learning methods to solve constraint satisfaction problems has shown promising, but they are mostly limited to a static situation where the problem description is completely known and fixed from the beginning. In this work we present a new approach to constraint satisfaction and optimization in dynamically changing environments, particularly when variables in the problem are statistically independent. We frame it as a reinforcement learning problem and introduce a conditional policy generator by borrowing the idea of class conditional generative adversarial networks (GANs). Assuming that the problem includes both static and dynamic constraints, the former are used in a reward formulation to guide the policy training such that it learns to map to a probabilistic distribution of solutions satisfying static constraints from a noise prior, which is similar to a generator in GANs. On the other hand, dynamic constraints in the problem are encoded to different class labels and fed with the input noise. The policy is then simultaneously updated for maximum likelihood of correctly classifying given the dynamic conditions in a supervised manner. We empirically demonstrate a proof-of-principle experiment with a multi-modal constraint satisfaction problem and compare between unconditional and conditional cases.</li>
<li><strong>摘要：</strong>利用机器学习方法来解决约束满意度问题已显示出很有希望的问题，但是它们主要限于静态情况，在这种情况下，问题描述是完全已知并从一开始就固定的。在这项工作中，我们提出了一种在动态变化的环境中限制满意度和优化的新方法，尤其是当问题中的变量在统计上是独立的时。我们将其视为一个强化学习问题，并通过借用有条件的生成对抗网络（GAN）来引入条件策略生成器。假设该问题包括静态和动态约束，前者用于奖励公式中，以指导政策培训，以便学会映射到满足噪声先验的静态约束的解决方案的概率分布，这类似于gan中的发电机。另一方面，问题中的动态约束被编码为不同的类标签，并用输入噪声喂食。然后，同时更新该策略，以最大程度的可能以监督的方式正确分类。我们从经验上展示了具有多模式约束满意度问题的原则证明实验，并在无条件和条件案例之间进行比较。</li>
</ul>

<h3>Title: Guided and Unguided Conditional Diffusion Mechanisms for Structured and Semantically-Aware 3D Point Cloud Generation</h3>
<ul>
<li><strong>Authors: </strong>Gunner Stone, Sushmita Sarker, Alireza Tavakkoli</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17206">https://arxiv.org/abs/2509.17206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17206">https://arxiv.org/pdf/2509.17206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17206]] Guided and Unguided Conditional Diffusion Mechanisms for Structured and Semantically-Aware 3D Point Cloud Generation(https://arxiv.org/abs/2509.17206)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generating realistic 3D point clouds is a fundamental problem in computer vision with applications in remote sensing, robotics, and digital object modeling. Existing generative approaches primarily capture geometry, and when semantics are considered, they are typically imposed post hoc through external segmentation or clustering rather than integrated into the generative process itself. We propose a diffusion-based framework that embeds per-point semantic conditioning directly within generation. Each point is associated with a conditional variable corresponding to its semantic label, which guides the diffusion dynamics and enables the joint synthesis of geometry and semantics. This design produces point clouds that are both structurally coherent and segmentation-aware, with object parts explicitly represented during synthesis. Through a comparative analysis of guided and unguided diffusion processes, we demonstrate the significant impact of conditional variables on diffusion dynamics and generation quality. Extensive experiments validate the efficacy of our approach, producing detailed and accurate 3D point clouds tailored to specific parts and features.</li>
<li><strong>摘要：</strong>生成现实的3D点云是计算机视觉中的一个基本问题，具有遥感，机器人和数字对象建模中的应用。现有的生成方法主要捕获几何形状，当考虑语义时，通常通过外部分割或聚类施加了事后，而不是集成到生成过程本身中。我们提出了一个基于扩散的框架，该框架将直接嵌入生成中的每个点语义调节。每个点都与与其语义标签相对应的条件变量相关联，该变量指导扩散动力学并实现几何和语义的关节合成。该设计产生的点云在结构上是连贯的，又是分割的，并且在合成过程中明确表示对象部分。通过对引导和未指导的扩散过程的比较分析，我们证明了条件变量对扩散动力学和发电质量的显着影响。广泛的实验验证了我们方法的功效，从而产生了针对特定零件和特征的详细而准确的3D点云。</li>
</ul>

<h3>Title: MirrorSAM2: Segment Mirror in Videos with Depth Perception</h3>
<ul>
<li><strong>Authors: </strong>Mingchen Xu, Yukun Lai, Ze Ji, Jing Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17220">https://arxiv.org/abs/2509.17220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17220">https://arxiv.org/pdf/2509.17220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17220]] MirrorSAM2: Segment Mirror in Videos with Depth Perception(https://arxiv.org/abs/2509.17220)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This paper presents MirrorSAM2, the first framework that adapts Segment Anything Model 2 (SAM2) to the task of RGB-D video mirror segmentation. MirrorSAM2 addresses key challenges in mirror detection, such as reflection ambiguity and texture confusion, by introducing four tailored modules: a Depth Warping Module for RGB and depth alignment, a Depth-guided Multi-Scale Point Prompt Generator for automatic prompt generation, a Frequency Detail Attention Fusion Module to enhance structural boundaries, and a Mirror Mask Decoder with a learnable mirror token for refined segmentation. By fully leveraging the complementarity between RGB and depth, MirrorSAM2 extends SAM2's capabilities to the prompt-free setting. To our knowledge, this is the first work to enable SAM2 for automatic video mirror segmentation. Experiments on the VMD and DVMD benchmark demonstrate that MirrorSAM2 achieves SOTA performance, even under challenging conditions such as small mirrors, weak boundaries, and strong reflections.</li>
<li><strong>摘要：</strong>本文介绍了MirrorSam2，这是第一个将任何模型2（SAM2）调整为RGB-D视频镜面细分任务的框架。 MirrorSAM2 addresses key challenges in mirror detection, such as reflection ambiguity and texture confusion, by introducing four tailored modules: a Depth Warping Module for RGB and depth alignment, a Depth-guided Multi-Scale Point Prompt Generator for automatic prompt generation, a Frequency Detail Attention Fusion Module to enhance structural boundaries, and a Mirror Mask Decoder with a learnable mirror token for refined segmentation.通过完全利用RGB和深度之间的互补性，MirrorSam2将SAM2的功能扩展到了及时的设置。据我们所知，这是启用SAM2进行自动视频镜面细分的第一项工作。在VMD和DVMD基准上进行的实验表明，即使在诸如小镜子，较弱的边界和强烈的反射之类的具有挑战性的条件下，MirrorSam2也达到了SOTA性能。</li>
</ul>

<h3>Title: DT-NeRF: A Diffusion and Transformer-Based Optimization Approach for Neural Radiance Fields in 3D Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Bo Liu, Runlong Li, Li Zhou, Yan Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17232">https://arxiv.org/abs/2509.17232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17232">https://arxiv.org/pdf/2509.17232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17232]] DT-NeRF: A Diffusion and Transformer-Based Optimization Approach for Neural Radiance Fields in 3D Reconstruction(https://arxiv.org/abs/2509.17232)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper proposes a Diffusion Model-Optimized Neural Radiance Field (DT-NeRF) method, aimed at enhancing detail recovery and multi-view consistency in 3D scene reconstruction. By combining diffusion models with Transformers, DT-NeRF effectively restores details under sparse viewpoints and maintains high accuracy in complex geometric scenes. Experimental results demonstrate that DT-NeRF significantly outperforms traditional NeRF and other state-of-the-art methods on the Matterport3D and ShapeNet datasets, particularly in metrics such as PSNR, SSIM, Chamfer Distance, and Fidelity. Ablation experiments further confirm the critical role of the diffusion and Transformer modules in the model's performance, with the removal of either module leading to a decline in performance. The design of DT-NeRF showcases the synergistic effect between modules, providing an efficient and accurate solution for 3D scene reconstruction. Future research may focus on further optimizing the model, exploring more advanced generative models and network architectures to enhance its performance in large-scale dynamic scenes.</li>
<li><strong>摘要：</strong>本文提出了一种扩散模型优化的神经辐射场（DT-NERF）方法，旨在增强3D场景重建中的细节恢复和多视图一致性。通过将扩散模型与变压器相结合，DT-NERF在稀疏观点下有效地恢复了细节，并在复杂的几何场景中保持了高精度。实验结果表明，DT-NERF在MatterPort3D和Shapenet数据集上的表现明显优于传统的NERF和其他最先进的方法，尤其是在PSNR，SSIM，Chamfer距离和保真度等指标中。消融实验进一步证实了扩散和变压器模块在模型性能中的关键作用，并消除了任何一个模块导致性能下降。 DT-NERF的设计展示了模块之间的协同效果，为3D场景重建提供了有效而准确的解决方案。未来的研究可能着重于进一步优化该模型，探索更先进的生成模型和网络体系结构，以增强其在大规模动态场景中的性能。</li>
</ul>

<h3>Title: Graph Signal Generative Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yigit Berkay Uslu, Samar Hadou, Sergio Rozada, Shirin Saeedi Bidokhti, Alejandro Ribeiro</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17250">https://arxiv.org/abs/2509.17250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17250">https://arxiv.org/pdf/2509.17250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17250]] Graph Signal Generative Diffusion Models(https://arxiv.org/abs/2509.17250)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We introduce U-shaped encoder-decoder graph neural networks (U-GNNs) for stochastic graph signal generation using denoising diffusion processes. The architecture learns node features at different resolutions with skip connections between the encoder and decoder paths, analogous to the convolutional U-Net for image generation. The U-GNN is prominent for a pooling operation that leverages zero-padding and avoids arbitrary graph coarsening, with graph convolutions layered on top to capture local dependencies. This technique permits learning feature embeddings for sampled nodes at deeper levels of the architecture that remain convolutional with respect to the original graph. Applied to stock price prediction -- where deterministic forecasts struggle to capture uncertainties and tail events that are paramount -- we demonstrate the effectiveness of the diffusion model in probabilistic forecasting of stock prices.</li>
<li><strong>摘要：</strong>我们介绍了使用deno的扩散过程引入U形编码器数据神经网络（U-GNN），以生成随机图信号。该体系结构在不同的分辨率上学习节点特征，并在编码器和解码器路径之间进行跳过连接，类似于卷积U-net的图像生成。 U-GNN对于利用零盖并避免任意图形的合并操作是突出的，并且图形卷积在顶部分层以捕获局部依赖性。该技术允许学习特征在更深层次的架构中进行采样节点的嵌入，这些节点相对于原始图而保持卷积。应用于股票价格预测 - 确定性的预测难以捕获至关重要的不确定性和尾巴事件 - 我们证明了扩散模型在概率预测股票价格中的有效性。</li>
</ul>

<h3>Title: Training the next generation of physicians for artificial intelligence-assisted clinical neuroradiology: ASNR MICCAI Brain Tumor Segmentation (BraTS) 2025 Lighthouse Challenge education platform</h3>
<ul>
<li><strong>Authors: </strong>Raisa Amiruddin, Nikolay Y. Yordanov, Nazanin Maleki, Pascal Fehringer, Athanasios Gkampenis, Anastasia Janas, Kiril Krantchev, Ahmed Moawad, Fabian Umeh, Salma Abosabie, Sara Abosabie, Albara Alotaibi, Mohamed Ghonim, Mohanad Ghonim, Sedra Abou Ali Mhana, Nathan Page, Marko Jakovljevic, Yasaman Sharifi, Prisha Bhatia, Amirreza Manteghinejad, Melisa Guelen, Michael Veronesi, Virginia Hill, Tiffany So, Mark Krycia, Bojan Petrovic, Fatima Memon, Justin Cramer, Elizabeth Schrickel, Vilma Kosovic, Lorenna Vidal, Gerard Thompson, Ichiro Ikuta, Basimah Albalooshy, Ali Nabavizadeh, Nourel Hoda Tahon, Karuna Shekdar, Aashim Bhatia, Claudia Kirsch, Gennaro D'Anna, Philipp Lohmann, Amal Saleh Nour, Andriy Myronenko, Adam Goldman-Yassen, Janet R. Reid, Sanjay Aneja, Spyridon Bakas, Mariam Aboian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17281">https://arxiv.org/abs/2509.17281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17281">https://arxiv.org/pdf/2509.17281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17281]] Training the next generation of physicians for artificial intelligence-assisted clinical neuroradiology: ASNR MICCAI Brain Tumor Segmentation (BraTS) 2025 Lighthouse Challenge education platform(https://arxiv.org/abs/2509.17281)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>High-quality reference standard image data creation by neuroradiology experts for automated clinical tools can be a powerful tool for neuroradiology & artificial intelligence education. We developed a multimodal educational approach for students and trainees during the MICCAI Brain Tumor Segmentation Lighthouse Challenge 2025, a landmark initiative to develop accurate brain tumor segmentation algorithms. Fifty-six medical students & radiology trainees volunteered to annotate brain tumor MR images for the BraTS challenges of 2023 & 2024, guided by faculty-led didactics on neuropathology MRI. Among the 56 annotators, 14 select volunteers were then paired with neuroradiology faculty for guided one-on-one annotation sessions for BraTS 2025. Lectures on neuroanatomy, pathology & AI, journal clubs & data scientist-led workshops were organized online. Annotators & audience members completed surveys on their perceived knowledge before & after annotations & lectures respectively. Fourteen coordinators, each paired with a neuroradiologist, completed the data annotation process, averaging 1322.9+/-760.7 hours per dataset per pair and 1200 segmentations in total. On a scale of 1-10, annotation coordinators reported significant increase in familiarity with image segmentation software pre- and post-annotation, moving from initial average of 6+/-2.9 to final average of 8.9+/-1.1, and significant increase in familiarity with brain tumor features pre- and post-annotation, moving from initial average of 6.2+/-2.4 to final average of 8.1+/-1.2. We demonstrate an innovative offering for providing neuroradiology & AI education through an image segmentation challenge to enhance understanding of algorithm development, reinforce the concept of data reference standard, and diversify opportunities for AI-driven image analysis among future physicians.</li>
<li><strong>摘要：</strong>神经放射学专家为自动化临床工具创建的高质量参考标准图像数据创建可以成为神经放射学和人工智能教育的强大工具。在Miccai脑肿瘤分割灯塔挑战2025年，我们为学生和学员开发了一种多模式教育方法，这是一项具有里程碑意义的计划，旨在开发准确的脑肿瘤分割算法。 56名医学生和放射学学员自愿注释了脑肿瘤MR图像，以针对2023年和2024年的Brats挑战，由教师主导的Dodactics在神经病理学MRI方面进行指导。在56个注释者中，14名选定的志愿者与神经放射学系配对，以指导Brats 2025的一对一注释会议。关于神经解剖学，病理学和AI的讲座，期刊俱乐部和数据科学家主导的工作室在线组织。注释者和听众成员分别在注释和讲座之前和之后就完成了对他们所感知的知识的调查。 14个协调员与神经放射科医生配对，完成了数据注释过程，平均每对数据集的1322.9 +/- 760.7小时，总共1200个分段。在1-10的范围内，注释协调员报告说，对图像分割软件前后通量的熟悉程度显着增加，从初始平均值6 +/- 2.9转变为8.9 +/- 1.1的最终平均水平，并且对脑肿瘤的熟悉程度显着增加，对脑肿瘤的熟悉和后通道的特征和后平均水平从初始平均水平转移到6.2 +/- 2.4平均2.4平均平均值为8.1 +/- 8.1 +/-- 1.2+/-1。2。我们展示了通过图像分割挑战提供神经放射学和AI教育的创新产品，以增强对算法开发的理解，增强数据参考标准的概念，并在未来医生中为AI驱动的图像分析提供多元化的机会。</li>
</ul>

<h3>Title: GraphWeave: Interpretable and Robust Graph Generation via Random Walk Trajectories</h3>
<ul>
<li><strong>Authors: </strong>Rahul Nandakumar, Deepayan Chakrabarti</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17291">https://arxiv.org/abs/2509.17291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17291">https://arxiv.org/pdf/2509.17291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17291]] GraphWeave: Interpretable and Robust Graph Generation via Random Walk Trajectories(https://arxiv.org/abs/2509.17291)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Given a set of graphs from some unknown family, we want to generate new graphs from that family. Recent methods use diffusion on either graph embeddings or the discrete space of nodes and edges. However, simple changes to embeddings (say, adding noise) can mean uninterpretable changes in the graph. In discrete-space diffusion, each step may add or remove many nodes/edges. It is hard to predict what graph patterns we will observe after many diffusion steps. Our proposed method, called GraphWeave, takes a different approach. We separate pattern generation and graph construction. To find patterns in the training graphs, we see how they transform vectors during random walks. We then generate new graphs in two steps. First, we generate realistic random walk "trajectories" which match the learned patterns. Then, we find the optimal graph that fits these trajectories. The optimization infers all edges jointly, which improves robustness to errors. On four simulated and five real-world benchmark datasets, GraphWeave outperforms existing methods. The most significant differences are on large-scale graph structures such as PageRank, cuts, communities, degree distributions, and flows. GraphWeave is also 10x faster than its closest competitor. Finally, GraphWeave is simple, needing only a transformer and standard optimizers.</li>
<li><strong>摘要：</strong>鉴于来自一些未知家庭的一组图表，我们希望从该家族中生成新的图表。最近的方法在图形嵌入或节点和边缘的离散空间上使用扩散。但是，对嵌入的简单更改（例如，添加噪声）可能意味着图表中无法解释的更改。在离散空间扩散中，每个步骤都可以添加或删除许多节点/边缘。在许多扩散步骤之后，我们将很难预测我们将观察到的图形模式。我们提出的称为GraphWeave的方法采用了不同的方法。我们将图案生成和图形结构分开。为了在训练图中找到图案，我们看到它们在随机步行过程中如何转换向量。然后，我们分为两个步骤生成新图形。首先，我们生成与学习模式相匹配的逼真的随机步行“轨迹”。然后，我们找到适合这些轨迹的最佳图。优化将所有边缘共同渗透，从而提高了误差的鲁棒性。在四个模拟和五个实际基准数据集上，GraphWeave的表现优于现有方法。最重要的差异在于大规模的图形结构，例如Pagerank，削减，社区，学位分布和流。 GraphWeave的速度也比其最接近的竞争对手快10倍。最后，GraphWeave很简单，只需要变压器和标准优化器。</li>
</ul>

<h3>Title: SmokeSeer: 3D Gaussian Splatting for Smoke Removal and Scene Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Neham Jain, Andrew Jong, Sebastian Scherer, Ioannis Gkioulekas</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17329">https://arxiv.org/abs/2509.17329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17329">https://arxiv.org/pdf/2509.17329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17329]] SmokeSeer: 3D Gaussian Splatting for Smoke Removal and Scene Reconstruction(https://arxiv.org/abs/2509.17329)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Smoke in real-world scenes can severely degrade the quality of images and hamper visibility. Recent methods for image restoration either rely on data-driven priors that are susceptible to hallucinations, or are limited to static low-density smoke. We introduce SmokeSeer, a method for simultaneous 3D scene reconstruction and smoke removal from a video capturing multiple views of a scene. Our method uses thermal and RGB images, leveraging the fact that the reduced scattering in thermal images enables us to see through the smoke. We build upon 3D Gaussian splatting to fuse information from the two image modalities, and decompose the scene explicitly into smoke and non-smoke components. Unlike prior approaches, SmokeSeer handles a broad range of smoke densities and can adapt to temporally varying smoke. We validate our approach on synthetic data and introduce a real-world multi-view smoke dataset with RGB and thermal images. We provide open-source code and data at the project website.</li>
<li><strong>摘要：</strong>在现实世界中的烟雾会严重降低图像的质量和障碍物的可见性。最新的图像恢复方法依赖于易受幻觉感染的数据驱动的先验，或者仅限于静态低密度烟雾。我们介绍了Smokeseer，这是一种从捕获场景的多个视频中的视频中同时进行3D场景重建和烟雾清除的方法。我们的方法使用热图像和RGB图像，利用了热图像中减少的散射使我们能够看到烟雾的事实。我们基于3D高斯碎片，从两种图像方式中融合信息，并将场景明确分解为烟雾和非烟民组件。与先前的方法不同，Smokeseer可以处理广泛的烟雾密度，并且可以适应时间变化的烟雾。我们验证了合成数据的方法，并引入了带有RGB和热图像的真实世界多视频烟数据集。我们在项目网站上提供开源代码和数据。</li>
</ul>

<h3>Title: Pre-Trained CNN Architecture for Transformer-Based Image Caption Generation Model</h3>
<ul>
<li><strong>Authors: </strong>Amanuel Tafese Dufera</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17365">https://arxiv.org/abs/2509.17365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17365">https://arxiv.org/pdf/2509.17365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17365]] Pre-Trained CNN Architecture for Transformer-Based Image Caption Generation Model(https://arxiv.org/abs/2509.17365)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Automatic image captioning, a multifaceted task bridging computer vision and natural lan- guage processing, aims to generate descriptive textual content from visual input. While Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks have achieved significant advancements, they present limitations. The inherent sequential nature of RNNs leads to sluggish training and inference times. LSTMs further struggle with retaining information from earlier sequence elements when dealing with very long se- quences. This project presents a comprehensive guide to constructing and comprehending transformer models for image captioning. Transformers employ self-attention mechanisms, capturing both short- and long-range dependencies within the data. This facilitates efficient parallelization during both training and inference phases. We leverage the well-established Transformer architecture, recognized for its effectiveness in managing sequential data, and present a meticulous methodology. Utilizing the Flickr30k dataset, we conduct data pre- processing, construct a model architecture that integrates an EfficientNetB0 CNN for fea- ture extraction, and train the model with attention mechanisms incorporated. Our approach exemplifies the utilization of parallelization for efficient training and inference. You can find the project on GitHub.</li>
<li><strong>摘要：</strong>自动图像字幕是一个多方面的任务桥接计算机视觉和自然语言处理，旨在从视觉输入中生成描述性文本内容。尽管卷积神经网络（CNN）和长期短期记忆（LSTM）网络已取得了重大进步，但它们提出了局限性。 RNN的固有顺序性质导致训练和推理时间缓慢。 LSTMS在处理很长时间时会进一步努力保留早期序列元素的信息。该项目介绍了构建和理解用于图像字幕的变压器模型的综合指南。变形金刚采用自我注意的机制，在数据中捕获短期和远程依赖性。这促进了在训练和推理阶段的有效并行化。我们利用良好的变压器体系结构，以其在管理顺序数据方面的有效性而认可，并提出了细致的方法。利用FlickR30K数据集，我们进行了数据预处理，构建了一个模型体系结构，该模型架构集成了有效的NETB0 CNN，以提取feletractire提取，并使用已加入的注意机制来训练模型。我们的方法体现了对有效训练和推理的并行化利用。您可以在Github上找到该项目。</li>
</ul>

<h3>Title: Revisiting Vision Language Foundations for No-Reference Image Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Ankit Yadav, Ta Duc Huy, Lingqiao Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17374">https://arxiv.org/abs/2509.17374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17374">https://arxiv.org/pdf/2509.17374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17374]] Revisiting Vision Language Foundations for No-Reference Image Quality Assessment(https://arxiv.org/abs/2509.17374)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Large-scale vision language pre-training has recently shown promise for no-reference image-quality assessment (NR-IQA), yet the relative merits of modern Vision Transformer foundations remain poorly understood. In this work, we present the first systematic evaluation of six prominent pretrained backbones, CLIP, SigLIP2, DINOv2, DINOv3, Perception, and ResNet, for the task of No-Reference Image Quality Assessment (NR-IQA), each finetuned using an identical lightweight MLP head. Our study uncovers two previously overlooked factors: (1) SigLIP2 consistently achieves strong performance; and (2) the choice of activation function plays a surprisingly crucial role, particularly for enhancing the generalization ability of image quality assessment models. Notably, we find that simple sigmoid activations outperform commonly used ReLU and GELU on several benchmarks. Motivated by this finding, we introduce a learnable activation selection mechanism that adaptively determines the nonlinearity for each channel, eliminating the need for manual activation design, and achieving new state-of-the-art SRCC on CLIVE, KADID10K, and AGIQA3K. Extensive ablations confirm the benefits across architectures and regimes, establishing strong, resource-efficient NR-IQA baselines.</li>
<li><strong>摘要：</strong>大规模的视力语言预训练最近显示出对无参考图像质量评估（NR-IQA）的希望，但是现代视觉变形金刚基础的相对优点仍然知之甚少。在这项工作中，我们介绍了六个突出预估计的骨架，夹子，siglip2，dinov2，dinov3，感知和重新连接，以进行无参考图像质量评估（NR-IQA），每种都使用相同的轻量级MLP MLP头进行了列式。我们的研究发现了两个以前被忽视的因素：（1）Siglip2始终达到强大的性能； （2）激活函数的选择起着令人惊讶的至关重要的作用，特别是在增强图像质量评估模型的概括能力方面。值得注意的是，我们发现简单的Sigmoid激活在几个基准上超过了常用的relu和Gelu。在这一发现的激励下，我们引入了一种可学习的激活选择机制，该机制可以自适应地确定每个通道的非线性，消除了对手动激活设计的需求，并在Clive，Kadid10k和Agiqa3k上实现了新的最先进的SRCC。广泛的消融证实了建筑和政权之间的好处，建立了强大的资源NR-IQA基准。</li>
</ul>

<h3>Title: Diff-GNSS: Diffusion-based Pseudorange Error Estimation</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Zhu, Shouyi Lu, Ziyao Li, Guirong Zhuo, Lu Xiong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17397">https://arxiv.org/abs/2509.17397</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17397">https://arxiv.org/pdf/2509.17397</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17397]] Diff-GNSS: Diffusion-based Pseudorange Error Estimation(https://arxiv.org/abs/2509.17397)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Global Navigation Satellite Systems (GNSS) are vital for reliable urban positioning. However, multipath and non-line-of-sight reception often introduce large measurement errors that degrade accuracy. Learning-based methods for predicting and compensating pseudorange errors have gained traction, but their performance is limited by complex error distributions. To address this challenge, we propose Diff-GNSS, a coarse-to-fine GNSS measurement (pseudorange) error estimation framework that leverages a conditional diffusion model to capture such complex distributions. Firstly, a Mamba-based module performs coarse estimation to provide an initial prediction with appropriate scale and trend. Then, a conditional denoising diffusion layer refines the estimate, enabling fine-grained modeling of pseudorange errors. To suppress uncontrolled generative diversity and achieve controllable synthesis, three key features related to GNSS measurement quality are used as conditions to precisely guide the reverse denoising process. We further incorporate per-satellite uncertainty modeling within the diffusion stage to assess the reliability of the predicted errors. We have collected and publicly released a real-world dataset covering various scenes. Experiments on public and self-collected datasets show that DiffGNSS consistently outperforms state-of-the-art baselines across multiple metrics. To the best of our knowledge, this is the first application of diffusion models to pseudorange error estimation. The proposed diffusion-based refinement module is plug-and-play and can be readily integrated into existing networks to markedly improve estimation accuracy.</li>
<li><strong>摘要：</strong>全球导航卫星系统（GNSS）对于可靠的城市定位至关重要。但是，多径和非视线接收通常会引入较大的测量误差，从而降低精度。基于学习的方法来预测和补偿伪造错误，已经获得了吸引力，但是它们的性能受到复杂的错误分布的限制。为了应对这一挑战，我们提出了DIFF-GNSS，粗到最细的GNSS测量（伪键）误差估计框架，该框架利用条件扩散模型来捕获此类复杂的分布。首先，基于曼巴的模块执行粗略的估计，以适当的规模和趋势提供初始预测。然后，有条件的降级扩散层完善了估计值，从而实现了伪造误差的细粒建模。为了抑制不受控制的生成多样性并实现可控制的综合，将与GNSS测量质量相关的三个关键特征用作准确指导反向剥离过程的条件。我们进一步在扩散阶段中纳入了人均不确定性建模，以评估预测误差的可靠性。我们已经收集并公开发布了一个涵盖各种场景的现实世界数据集。公共和自收集数据集的实验表明，跨多个指标的差异始终优于最先进的基线。据我们所知，这是扩散模型在伪造误差估计中的首次应用。所提出的基于扩散的改进模块是插件播放的，并且可以轻松地集成到现有网络中，以显着提高估计精度。</li>
</ul>

<h3>Title: Efficient Sliced Wasserstein Distance Computation via Adaptive Bayesian Optimization</h3>
<ul>
<li><strong>Authors: </strong>Manish Acharya, David Hyde</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17405">https://arxiv.org/abs/2509.17405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17405">https://arxiv.org/pdf/2509.17405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17405]] Efficient Sliced Wasserstein Distance Computation via Adaptive Bayesian Optimization(https://arxiv.org/abs/2509.17405)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The sliced Wasserstein distance (SW) reduces optimal transport on $\mathbb{R}^d$ to a sum of one-dimensional projections, and thanks to this efficiency, it is widely used in geometry, generative modeling, and registration tasks. Recent work shows that quasi-Monte Carlo constructions for computing SW (QSW) yield direction sets with excellent approximation error. This paper presents an alternate, novel approach: learning directions with Bayesian optimization (BO), particularly in settings where SW appears inside an optimization loop (e.g., gradient flows). We introduce a family of drop-in selectors for projection directions: BOSW, a one-shot BO scheme on the unit sphere; RBOSW, a periodic-refresh variant; ABOSW, an adaptive hybrid that seeds from competitive QSW sets and performs a few lightweight BO refinements; and ARBOSW, a restarted hybrid that periodically relearns directions during optimization. Our BO approaches can be composed with QSW and its variants (demonstrated by ABOSW/ARBOSW) and require no changes to downstream losses or gradients. We provide numerical experiments where our methods achieve state-of-the-art performance, and on the experimental suite of the original QSW paper, we find that ABOSW and ARBOSW can achieve convergence comparable to the best QSW variants with modest runtime overhead.</li>
<li><strong>摘要：</strong>切成薄片的Wasserstein距离（SW）将$ \ Mathbb {r}^d $上的最佳传输降低到一维投影的总和，并且由于这种效率，它被广泛用于几何，生成建模和注册任务。最近的工作表明，用于计算SW（QSW）产量方向的准蒙特卡洛结构具有出色的近似误差。本文提出了一种替代的新方法：贝叶斯优化（BO）的学习方向，尤其是在SW出现在优化循环内（例如梯度流）内的设置中。我们介绍了一个投影方向的滴入选择器家族：BOSW，单位球上的单发bo方案； rbosw，一种定期反复的变体； Abosw是一种自适应杂种，它是根据竞争性QSW播种并进行一些轻巧的精炼的；和Arbosw，这是一种重新启动的混合体，在优化过程中定期重新学习方向。我们的BO方法可以由QSW及其变体组成（由ABOSW/ARBOSW展示），并且不需要更改下游损失或梯度。我们提供了数值实验，我们的方法可以实现最先进的性能，并且在原始QSW论文的实验套件中，我们发现Abosw和Arbosw可以实现与具有适度运行时开销的最佳QSW变体相当的收敛性。</li>
</ul>

<h3>Title: Multi-scale Temporal Prediction via Incremental Generation and Multi-agent Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Zhitao Zeng, Guojian Yuan, Junyuan Mao, Yuxuan Wang, Xiaoshuang Jia, Yueming Jin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17429">https://arxiv.org/abs/2509.17429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17429">https://arxiv.org/pdf/2509.17429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17429]] Multi-scale Temporal Prediction via Incremental Generation and Multi-agent Collaboration(https://arxiv.org/abs/2509.17429)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Accurate temporal prediction is the bridge between comprehensive scene understanding and embodied artificial intelligence. However, predicting multiple fine-grained states of a scene at multiple temporal scales is difficult for vision-language models. We formalize the Multi-Scale Temporal Prediction (MSTP) task in general and surgical scenes by decomposing multi-scale into two orthogonal dimensions: the temporal scale, forecasting states of humans and surgery at varying look-ahead intervals, and the state scale, modeling a hierarchy of states in general and surgical scenes. For example, in general scenes, states of contact relationships are finer-grained than states of spatial relationships. In surgical scenes, medium-level steps are finer-grained than high-level phases yet remain constrained by their encompassing phase. To support this unified task, we introduce the first MSTP Benchmark, featuring synchronized annotations across multiple state scales and temporal scales. We further propose a method, Incremental Generation and Multi-agent Collaboration (IG-MC), which integrates two key innovations. First, we present a plug-and-play incremental generation module that continuously synthesizes up-to-date visual previews at expanding temporal scales to inform multiple decision-making agents, keeping decisions and generated visuals synchronized and preventing performance degradation as look-ahead intervals lengthen. Second, we present a decision-driven multi-agent collaboration framework for multi-state prediction, comprising generation, initiation, and multi-state assessment agents that dynamically trigger and evaluate prediction cycles to balance global coherence and local fidelity.</li>
<li><strong>摘要：</strong>准确的时间预测是全面的场景理解与体现人工智能之间的桥梁。但是，对于视觉模型，很难在多个时间尺度上预测场景的多个细粒状态。我们将多尺度的时间预测（MSTP）任务正式化，并通过将多尺度分解为两个正交维度：时间尺度，人类的预测状态和以各种外观范围的间隔以及状态规模的状态，并在一般情况下建模状态和一般状态的状态。例如，在一般场景中，接触关系的状态比空间关系的状态更细。在外科手术场景中，中级步骤比高级阶段更细粒，但仍受其包含阶段的限制。为了支持这项统一的任务，我们介绍了第一个MSTP基准，该基准具有多个状态量表和时间尺度的同步注释。我们进一步提出了一种方法，增量生成和多代理协作（IG-MC），该方法集成了两个关键创新。首先，我们提出了一个插件增量生成模块，该模块在扩展时间尺度上连续合成最新的视觉预览，以告知多个决策代理，保留决策并产生的视觉效果同步并防止性能降解，以延长。其次，我们提出了一个决策驱动的多代理协作框架，用于多州预测，包括生成，启动和多状态评估代理，该框架动态触发和评估预测周期以平衡全球连贯性和局部忠诚度。</li>
</ul>

<h3>Title: Emergent 3D Correspondence from Neural Shape Representation</h3>
<ul>
<li><strong>Authors: </strong>Keyu Du, Jingyu Hu, Haipeng Li, Hao Xu, Haibing Huang, Chi-Wing Fu, Shuaicheng Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17431">https://arxiv.org/abs/2509.17431</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17431">https://arxiv.org/pdf/2509.17431</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17431]] Emergent 3D Correspondence from Neural Shape Representation(https://arxiv.org/abs/2509.17431)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper presents a new approach to estimate accurate and robust 3D semantic correspondence with the hierarchical neural semantic representation. Our work has three key contributions. First, we design the hierarchical neural semantic representation (HNSR), which consists of a global semantic feature to capture high-level structure and multi-resolution local geometric features to preserve fine details, by carefully harnessing 3D priors from pre-trained 3D generative models. Second, we design a progressive global-to-local matching strategy, which establishes coarse semantic correspondence using the global semantic feature, then iteratively refines it with local geometric features, yielding accurate and semantically-consistent mappings. Third, our framework is training-free and broadly compatible with various pre-trained 3D generative backbones, demonstrating strong generalization across diverse shape categories. Our method also supports various applications, such as shape co-segmentation, keypoint matching, and texture transfer, and generalizes well to structurally diverse shapes, with promising results even in cross-category scenarios. Both qualitative and quantitative evaluations show that our method outperforms previous state-of-the-art techniques.</li>
<li><strong>摘要：</strong>本文提出了一种新的方法，可以通过分层神经语义表示估计准确，鲁棒的3D语义对应。我们的工作有三个关键的贡献。首先，我们设计了层次神经语义表示（HNSR），它由一个全球语义特征组成，可通过仔细利用预先训练的3D生成模型的3D先验来捕获高级结构和多分辨率的局部几何特征。其次，我们设计了一种渐进式的全部本地匹配策略，该策略使用全球语义特征建立了粗糙的语义对应关系，然后以局部几何特征进行迭代完善，从而得出准确和语义上的逐一映射。第三，我们的框架是无训练的，并且与各种预训练的3D生成式骨架具有广泛兼容，表明在各种形状类别中进行了强烈的概括。我们的方法还支持各种应用程序，例如形状共进行分割，关键点匹配和纹理传输，并概括为结构上多样的形状，即使在跨类别方案中也具有令人鼓舞的结果。定性和定量评估都表明，我们的方法的表现优于先前的最先进技术。</li>
</ul>

<h3>Title: Training-Free Label Space Alignment for Universal Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Dujin Lee, Sojung An, Jungmyung Wi, Kuniaki Saito, Donghyun Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17452">https://arxiv.org/abs/2509.17452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17452">https://arxiv.org/pdf/2509.17452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17452]] Training-Free Label Space Alignment for Universal Domain Adaptation(https://arxiv.org/abs/2509.17452)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Universal domain adaptation (UniDA) transfers knowledge from a labeled source domain to an unlabeled target domain, where label spaces may differ and the target domain may contain private classes. Previous UniDA methods primarily focused on visual space alignment but often struggled with visual ambiguities due to content differences, which limited their robustness and generalizability. To overcome this, we introduce a novel approach that leverages the strong \textit{zero-shot capabilities} of recent vision-language foundation models (VLMs) like CLIP, concentrating solely on label space alignment to enhance adaptation stability. CLIP can generate task-specific classifiers based only on label names. However, adapting CLIP to UniDA is challenging because the label space is not fully known in advance. In this study, we first utilize generative vision-language models to identify unknown categories in the target domain. Noise and semantic ambiguities in the discovered labels -- such as those similar to source labels (e.g., synonyms, hypernyms, hyponyms) -- complicate label alignment. To address this, we propose a training-free label-space alignment method for UniDA (\ours). Our method aligns label spaces instead of visual spaces by filtering and refining noisy labels between the domains. We then construct a \textit{universal classifier} that integrates both shared knowledge and target-private class information, thereby improving generalizability under domain shifts. The results reveal that the proposed method considerably outperforms existing UniDA techniques across key DomainBed benchmarks, delivering an average improvement of \textcolor{blue}{+7.9\%}in H-score and \textcolor{blue}{+6.1\%} in H$^3$-score. Furthermore, incorporating self-training further enhances performance and achieves an additional (\textcolor{blue}{+1.6\%}) increment in both H- and H$^3$-scores.</li>
<li><strong>摘要：</strong>通用域的适应性（UNIDA）将知识从标记的源域转移到未标记的目标域，其中标签空间可能有所不同，目标域可能包含私有类。以前的UNIDA方法主要集中于视觉空间对齐，但由于内容差异而经常在视觉歧义上挣扎，从而限制了它们的鲁棒性和可推广性。为了克服这一点，我们介绍了一种新的方法，该方法利用了最近的视觉语言基础模型（VLM）（例如剪辑）的强\ textit {零射击功能}，仅集中在标签空间对齐方式上以增强适应稳定性。剪辑只能基于标签名称生成特定于任务的分类器。但是，将剪辑适应UNIDA是具有挑战性的，因为标签空间并未得到预先完全知道。在这项研究中，我们首先利用生成视觉模型来识别目标域中未知类别。发现的标签中的噪声和语义模棱两可 - 例如类似于源标签的标签（例如，同义词，超核，假说） - 使标签对齐变得复杂。为了解决这个问题，我们为UNIDA（\ ous）提出了一种无训练的标签空间对准方法。我们的方法通过过滤和完善域之间的嘈杂标签来对准标签空间，而不是视觉空间。然后，我们构建一个\ textIt {通用分类器}，该{通用分类器}集成了共享知识和目标私有类信息，从而改善了域移动下的通用性。结果表明，所提出的方法在关键域基准测试中的现有UNIDA技术的表现大大优于现有的UNIDA技术，从而在H-SCORE和\ TextColor {blue} {blue} {+6.1 \％}中平均改善了\ textColor {blue} {+7.9 \％}。此外，合并自训练进一步提高了性能，并在H $^3 $ -SCORES中获得了附加的（\ textColor {blue} {+1.6 \％}）。</li>
</ul>

<h3>Title: SAMSON: 3rd Place Solution of LSVOS 2025 VOS Challenge</h3>
<ul>
<li><strong>Authors: </strong>Yujie Xie, Hongyang Zhang, Zhihui Liu, Shihai Ruan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17500">https://arxiv.org/abs/2509.17500</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17500">https://arxiv.org/pdf/2509.17500</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17500]] SAMSON: 3rd Place Solution of LSVOS 2025 VOS Challenge(https://arxiv.org/abs/2509.17500)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large-scale Video Object Segmentation (LSVOS) addresses the challenge of accurately tracking and segmenting objects in long video sequences, where difficulties stem from object reappearance, small-scale targets, heavy occlusions, and crowded scenes. Existing approaches predominantly adopt SAM2-based frameworks with various memory mechanisms for complex video mask generation. In this report, we proposed Segment Anything with Memory Strengthened Object Navigation (SAMSON), the 3rd place solution in the MOSE track of ICCV 2025, which integrates the strengths of stateof-the-art VOS models into an effective paradigm. To handle visually similar instances and long-term object disappearance in MOSE, we incorporate a long-term memorymodule for reliable object re-identification. Additionly, we adopt SAM2Long as a post-processing strategy to reduce error accumulation and enhance segmentation stability in long video sequences. Our method achieved a final performance of 0.8427 in terms of J &F in the test-set leaderboard.</li>
<li><strong>摘要：</strong>大规模视频对象细分（LSVOS）解决了在长时间视频序列中准确跟踪和分割对象的挑战，在这种序列中，困难源于对象重新出现，小规模目标，重型遮挡和拥挤的场景。现有方法主要采用具有各种内存机制的基于SAM2的框架，以生成复杂的视频掩码。在本报告中，我们提出了任何具有内存加强对象导航（Samson）的段，这是ICCV 2025的Mose Track中的第三名解决方案，该解决方案将现状VOS模型的优势整合到有效的范式中。为了处理摩西的视觉上相似的实例和长期对象消失，我们合并了一个长期内存模块，以重新识别可靠的对象。此外，我们采用SAM2LONG作为后处理策略，以减少误差积累并增强长时间视频序列中的细分稳定性。我们的方法在测试集排行榜中的J＆F方面实现了0.8427的最终性能。</li>
</ul>

<h3>Title: 4DGCPro: Efficient Hierarchical 4D Gaussian Compression for Progressive Volumetric Video Streaming</h3>
<ul>
<li><strong>Authors: </strong>Zihan Zheng, Zhenlong Wu, Houqiang Zhong, Yuan Tian, Ning Cao, Lan Xu, Jiangchao Yao, Xiaoyun Zhang, Qiang Hu, Wenjun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17513">https://arxiv.org/abs/2509.17513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17513">https://arxiv.org/pdf/2509.17513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17513]] 4DGCPro: Efficient Hierarchical 4D Gaussian Compression for Progressive Volumetric Video Streaming(https://arxiv.org/abs/2509.17513)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Achieving seamless viewing of high-fidelity volumetric video, comparable to 2D video experiences, remains an open challenge. Existing volumetric video compression methods either lack the flexibility to adjust quality and bitrate within a single model for efficient streaming across diverse networks and devices, or struggle with real-time decoding and rendering on lightweight mobile platforms. To address these challenges, we introduce 4DGCPro, a novel hierarchical 4D Gaussian compression framework that facilitates real-time mobile decoding and high-quality rendering via progressive volumetric video streaming in a single bitstream. Specifically, we propose a perceptually-weighted and compression-friendly hierarchical 4D Gaussian representation with motion-aware adaptive grouping to reduce temporal redundancy, preserve coherence, and enable scalable multi-level detail streaming. Furthermore, we present an end-to-end entropy-optimized training scheme, which incorporates layer-wise rate-distortion (RD) supervision and attribute-specific entropy modeling for efficient bitstream generation. Extensive experiments show that 4DGCPro enables flexible quality and multiple bitrate within a single model, achieving real-time decoding and rendering on mobile devices while outperforming existing methods in RD performance across multiple datasets. Project Page: this https URL</li>
<li><strong>摘要：</strong>与2D视频体验相当的高保真体积视频的无缝观看仍然是一个开放的挑战。现有的体积视频压缩方法要么缺乏在单个模型中调整质量和比特率的灵活性，以便在不同的网络和设备上有效流式传输，要么在轻量级移动平台上进行实时解码和渲染而挣扎。为了应对这些挑战，我们介绍了4DGCPRO，这是一种新型的分层4D高斯压缩框架，可通过单个Bitstream中的渐进式体积视频流进行实时移动解码和高质量渲染。具体而言，我们提出了一种具有感知加权和压缩友好的层次4D高斯表示，并具有运动感知的自适应组，以降低时间冗余，保持连贯性并启用可扩展的多级细节流。此外，我们提出了一种端到端熵优化的训练方案，该方案结合了层次的率降低（RD）监督（RD）监督和属性特定的熵建模，以进行有效的bitstream生成。广泛的实验表明，4DGCPRO可以在单个模型中具有灵活的质量和多重比特率，从而在移动设备上实现了实时解码和渲染，同时在多个数据集中胜过RD性能中的现有方法。项目页面：此HTTPS URL</li>
</ul>

<h3>Title: Visual Instruction Pretraining for Domain-Specific Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Li, Yicheng Zhang, Wenhao Tang, Yimian Dai, Ming-Ming Cheng, Xiang Li, Jian Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17562">https://arxiv.org/abs/2509.17562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17562">https://arxiv.org/pdf/2509.17562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17562]] Visual Instruction Pretraining for Domain-Specific Foundation Models(https://arxiv.org/abs/2509.17562)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Modern computer vision is converging on a closed loop in which perception, reasoning and generation mutually reinforce each other. However, this loop remains incomplete: the top-down influence of high-level reasoning on the foundational learning of low-level perceptual features is not yet underexplored. This paper addresses this gap by proposing a new paradigm for pretraining foundation models in downstream domains. We introduce Visual insTruction Pretraining (ViTP), a novel approach that directly leverages reasoning to enhance perception. ViTP embeds a Vision Transformer (ViT) backbone within a Vision-Language Model and pretrains it end-to-end using a rich corpus of visual instruction data curated from target downstream domains. ViTP is powered by our proposed Visual Robustness Learning (VRL), which compels the ViT to learn robust and domain-relevant features from a sparse set of visual tokens. Extensive experiments on 16 challenging remote sensing and medical imaging benchmarks demonstrate that ViTP establishes new state-of-the-art performance across a diverse range of downstream tasks. The code is available at this http URL.</li>
<li><strong>摘要：</strong>现代计算机视觉正在汇聚在一个封闭的循环中，在这种循环中，感知，推理和一代相互加强。但是，此循环仍然不完整：高级推理对低级感知特征的基础学习的自上而下的影响尚未得到充分展望。本文通过提出一个新的范式来解决该差距，用于在下游域中预处理基础模型。我们介绍了视觉指导预处理（VITP），这是一种新型的方法，直接利用推理来增强感知。 VITP将视觉变压器（VIT）骨架嵌入视觉模型中，并使用来自目标下游域策划的丰富视觉指导数据端到端预端。 VITP由我们提出的视觉鲁棒性学习（VRL）提供动力，该学习迫使VIT从一组稀疏的视觉令牌中学习鲁棒和与域相关的特征。对16种挑战的遥感和医学成像基准进行的广泛实验表明，VITP在各种下游任务中建立了新的最先进的性能。该代码可在此HTTP URL上找到。</li>
</ul>

<h3>Title: Clothing agnostic Pre-inpainting Virtual Try-ON</h3>
<ul>
<li><strong>Authors: </strong>Sehyun Kim, Hye Jun Lee, Jiwoo Lee, Taemin Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17654">https://arxiv.org/abs/2509.17654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17654">https://arxiv.org/pdf/2509.17654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17654]] Clothing agnostic Pre-inpainting Virtual Try-ON(https://arxiv.org/abs/2509.17654)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>With the development of deep learning technology, virtual try-on technology has become an important application value in the fields of e-commerce, fashion, and entertainment. The recently proposed Leffa has improved the texture distortion problem of diffu-sion-based models, but there are limitations in that the bottom detection inaccuracy and the existing clothing silhouette remain in the synthesis results. To solve this problem, this study proposes CaP-VTON (Clothing agnostic Pre-inpainting Virtual Try-ON). CaP-VTON has improved the naturalness and consistency of whole-body clothing syn-thesis by integrating multi-category masking based on Dress Code and skin inpainting based on Stable Diffusion. In particular, a generate skin module was introduced to solve the skin restoration problem that occurs when long-sleeved images are converted into short-sleeved or sleeveless ones, and high-quality restoration was implemented consider-ing the human body posture and color. As a result, CaP-VTON recorded 92.5\%, which is 15.4\% better than Leffa in short-sleeved synthesis accuracy, and showed the performance of consistently reproducing the style and shape of reference clothing in visual evaluation. These structures maintain model-agnostic properties and are applicable to various diffu-sion-based virtual inspection systems, and can contribute to applications that require high-precision virtual wearing, such as e-commerce, custom styling, and avatar creation.</li>
<li><strong>摘要：</strong>随着深度学习技术的发展，虚拟的尝试技术已成为电子商务，时尚和娱乐领域的重要应用价值。最近提出的Leffa改善了基于Diffu-sion模型的质地失真问题，但是存在局限性，因为底部检测不准确性，现有的服装轮廓仍然存在于合成结果中。为了解决这个问题，这项研究提出了Cap-Vton（服装不可知的预先侵害虚拟试验）。 Cap-Vton通过基于着装要求和基于稳定的扩散的皮肤涂层整合多类掩蔽，改善了全身服装合成论文的自然性和一致性。特别是，引入了一个生成的皮肤模块，以解决长袖图像转化为短袖或无袖的皮肤恢复问题，并实施了高质量的恢复。结果，Cap-Vton的记录为92.5 \％，在短袖合成精度中比Leffa好15.4％，并显示了视觉评估中始终重现参考服装的样式和形状的性能。这些结构维护模型不足的属性，适用于各种基于差异的虚拟检查系统，并且可以促进需要高精度虚拟磨损的应用，例如电子商务，自定义样式和avatar创建。</li>
</ul>

<h3>Title: FROQ: Observing Face Recognition Models for Efficient Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Žiga Babnik, Deepak Kumar Jain, Peter Peer, Vitomir Štruc</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17689">https://arxiv.org/abs/2509.17689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17689">https://arxiv.org/pdf/2509.17689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17689]] FROQ: Observing Face Recognition Models for Efficient Quality Assessment(https://arxiv.org/abs/2509.17689)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Face Recognition (FR) plays a crucial role in many critical (high-stakes) applications, where errors in the recognition process can lead to serious consequences. Face Image Quality Assessment (FIQA) techniques enhance FR systems by providing quality estimates of face samples, enabling the systems to discard samples that are unsuitable for reliable recognition or lead to low-confidence recognition decisions. Most state-of-the-art FIQA techniques rely on extensive supervised training to achieve accurate quality estimation. In contrast, unsupervised techniques eliminate the need for additional training but tend to be slower and typically exhibit lower performance. In this paper, we introduce FROQ (Face Recognition Observer of Quality), a semi-supervised, training-free approach that leverages specific intermediate representations within a given FR model to estimate face-image quality, and combines the efficiency of supervised FIQA models with the training-free approach of unsupervised methods. A simple calibration step based on pseudo-quality labels allows FROQ to uncover specific representations, useful for quality assessment, in any modern FR model. To generate these pseudo-labels, we propose a novel unsupervised FIQA technique based on sample perturbations. Comprehensive experiments with four state-of-the-art FR models and eight benchmark datasets show that FROQ leads to highly competitive results compared to the state-of-the-art, achieving both strong performance and efficient runtime, without requiring explicit training.</li>
<li><strong>摘要：</strong>面部识别（FR）在许多关键（高风险）应用中起着至关重要的作用，在许多关键（高风险）应用中，识别过程中的错误可能导致严重的后果。面部图像质量评估（FIQA）技术通过提供面部样本的质量估计来增强FR系统，从而使系统能够丢弃不适合可靠识别或导致低信心识别决策的样品。大多数最先进的FIQA技术都依赖于广泛的监督培训来实现准确的质量估计。相比之下，无监督的技术消除了对额外训练的需求，但往往较慢，通常表现出较低的性能。在本文中，我们介绍了FROQ（质量的面部识别观察者），这是一种半监督，无训练的方法，利用给定的FR模型中的特定中间表示来估计面部图像质量，并将监督FIQA模型的效率与无需培训方法的无训练方法结合在一起。基于伪质量标签的简单校准步骤使FROQ可以在任何现代的FR模型中发现特定的表示，可用于质量评估。为了生成这些伪标签，我们提出了一种基于样品扰动的新型无监督的FIQA技术。使用四种最先进的FR模型和八个基准数据集进行的全面实验表明，与最先进的面前相比，FROQ导致了高度竞争的结果，在不需要明确培训的情况下达到了强大的性能和有效的运行时。</li>
</ul>

<h3>Title: Neurodynamics-Driven Coupled Neural P Systems for Multi-Focus Image Fusion</h3>
<ul>
<li><strong>Authors: </strong>Bo Li, Yunkuo Lei, Tingting Bao, Yaxian Wang, Lingling Zhang, Jun Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17704">https://arxiv.org/abs/2509.17704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17704">https://arxiv.org/pdf/2509.17704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17704]] Neurodynamics-Driven Coupled Neural P Systems for Multi-Focus Image Fusion(https://arxiv.org/abs/2509.17704)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multi-focus image fusion (MFIF) is a crucial technique in image processing, with a key challenge being the generation of decision maps with precise boundaries. However, traditional methods based on heuristic rules and deep learning methods with black-box mechanisms are difficult to generate high-quality decision maps. To overcome this challenge, we introduce neurodynamics-driven coupled neural P (CNP) systems, which are third-generation neural computation models inspired by spiking mechanisms, to enhance the accuracy of decision maps. Specifically, we first conduct an in-depth analysis of the model's neurodynamics to identify the constraints between the network parameters and the input signals. This solid analysis avoids abnormal continuous firing of neurons and ensures the model accurately distinguishes between focused and unfocused regions, generating high-quality decision maps for MFIF. Based on this analysis, we propose a \textbf{N}eurodynamics-\textbf{D}riven \textbf{CNP} \textbf{F}usion model (\textbf{ND-CNPFuse}) tailored for the challenging MFIF task. Unlike current ideas of decision map generation, ND-CNPFuse distinguishes between focused and unfocused regions by mapping the source image into interpretable spike matrices. By comparing the number of spikes, an accurate decision map can be generated directly without any post-processing. Extensive experimental results show that ND-CNPFuse achieves new state-of-the-art performance on four classical MFIF datasets, including Lytro, MFFW, MFI-WHU, and Real-MFF. The code is available at this https URL.</li>
<li><strong>摘要：</strong>多聚焦图像融合（MFIF）是图像处理中的一种关键技术，关键的挑战是生成具有精确边界的决策图。但是，基于启发式规则和带有黑盒机制的深度学习方法的传统方法很难产生高质量的决策图。为了克服这一挑战，我们引入了神经动力学驱动的耦合神经P（CNP）系统，该系统是受尖峰机制启发的第三代神经计算模型，以提高决策图的准确性。具体而言，我们首先对模型的神经动力学进行了深入的分析，以确定网络参数和输入信号之间的约束。这种扎实的分析避免了神经元异常的连续触发，并确保模型准确区分了聚焦和未关注的区域，从而为MFIF生成了高质量的决策图。基于此分析，我们提出了一个\ textbf {n} eurodroymics- \ textbf {d} riven \ textbf {cnp} \ textbf {f} usion model（\ textbf {nd-cnpfuse}）量身定制的，用于具有挑战性的MFIF任务。与当前的决策图生成思想不同，ND-CNPFUSE通过将源图像映射到可解释的尖峰矩阵来区分集中和未关注的区域。通过比较尖峰的数量，可以直接生成准确的决策图，而无需任何后处理。广泛的实验结果表明，ND-CNPFUSE在四个经典的MFIF数据集上实现了新的最新性能，包括Lytro，MFFW，MFI-WHU和RealMFF。该代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: A Generative Conditional Distribution Equality Testing Framework and Its Minimax Analysis</h3>
<ul>
<li><strong>Authors: </strong>Siming Zheng, Meifang Lan, Tong Wang, Yuanyuan Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, math.ST, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17729">https://arxiv.org/abs/2509.17729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17729">https://arxiv.org/pdf/2509.17729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17729]] A Generative Conditional Distribution Equality Testing Framework and Its Minimax Analysis(https://arxiv.org/abs/2509.17729)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a general framework for testing the equality of the conditional distributions in a two-sample problem. This problem is most relevant to transfer learning under covariate shift. Our framework is built on neural network-based generative methods and sample splitting techniques by transforming the conditional distribution testing problem into an unconditional one. We introduce two special tests: the generative permutation-based conditional distribution equality test and the generative classification accuracy-based conditional distribution equality test. Theoretically, we establish a minimax lower bound for statistical inference in testing the equality of two conditional distributions under certain smoothness conditions. We demonstrate that the generative permutation-based conditional distribution equality test and its modified version can attain this lower bound precisely or up to some iterated logarithmic factor. Moreover, we prove the testing consistency of the generative classification accuracy-based conditional distribution equality test. We also establish the convergence rate for the learned conditional generator by deriving new results related to the recently-developed offset Rademacher complexity and approximation properties using neural networks. Empirically, we conduct numerical studies including synthetic datasets and two real-world datasets, demonstrating the effectiveness of our approach.</li>
<li><strong>摘要：</strong>在本文中，我们提出了一个一般框架，用于在两个样本问题中测试条件分布的平等。这个问题与协变量转移下的学习最相关。我们的框架是基于基于神经网络的生成方法和样品分裂技术，通过将条件分布测试问题转换为无条件的框架。我们介绍了两个特殊测试：基于生成置换的条件分布平等测试和基于生成的有条件分布平等测试。从理论上讲，我们在某些平滑度条件下建立了一个最小值下限，用于测试两个条件分布的平等。我们证明，基于生成置换的条件分布平等测试及其修改版本可以精确或达到某些迭代的对数因子。此外，我们证明了基于生成分类精度的条件分配平等测试的测试一致性。我们还通过得出与最近开发的偏移rademacher复杂性和使用神经网络相关的新结果来建立学习条件发生器的收敛速率。从经验上讲，我们进行了数值研究，包括合成数据集和两个现实世界数据集，证明了我们方法的有效性。</li>
</ul>

<h3>Title: WISE: Weak-Supervision-Guided Step-by-Step Explanations for Multimodal LLMs in Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Yiwen Jiang, Deval Mehta, Siyuan Yan, Yaling Shen, Zimu Wang, Zongyuan Ge</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17740">https://arxiv.org/abs/2509.17740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17740">https://arxiv.org/pdf/2509.17740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17740]] WISE: Weak-Supervision-Guided Step-by-Step Explanations for Multimodal LLMs in Image Classification(https://arxiv.org/abs/2509.17740)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have shown promise in visual-textual reasoning, with Multimodal Chain-of-Thought (MCoT) prompting significantly enhancing interpretability. However, existing MCoT methods rely on rationale-rich datasets and largely focus on inter-object reasoning, overlooking the intra-object understanding crucial for image classification. To address this gap, we propose WISE, a Weak-supervision-guided Step-by-step Explanation method that augments any image classification dataset with MCoTs by reformulating the concept-based representations from Concept Bottleneck Models (CBMs) into concise, interpretable reasoning chains under weak supervision. Experiments across ten datasets show that our generated MCoTs not only improve interpretability by 37% but also lead to gains in classification accuracy when used to fine-tune MLLMs. Our work bridges concept-based interpretability and generative MCoT reasoning, providing a generalizable framework for enhancing MLLMs in fine-grained visual understanding.</li>
<li><strong>摘要：</strong>多模式的大语言模型（MLLM）在视觉文本推理中表现出了希望，多模式链（MCOT）促使人们显着增强了可解释性。但是，现有的MCOT方法依赖于富含基本原理的数据集，并在很大程度上集中于对象间推理，从而忽略了对图像分类至关重要的对象内的理解。为了解决这一差距，我们提出了Wise，这是一种弱者指导的逐步解释方法，通过将基于概念的瓶颈模型（CBMS）从弱体监督下的简洁，可解释的推理链中重新定义为简洁，可解释的理由链来增强任何图像分类数据集。十个数据集的实验表明，我们生成的MCOT不仅提高了37％的可解释性，而且还会在用于微调MLLM时提高分类精度。我们的工作桥梁基于概念的解释性和生成的MCOT推理，提供了一个可推广的框架，以增强精细元素视觉理解中的MLLM。</li>
</ul>

<h3>Title: GEM-T: Generative Tabular Data via Fitting Moments</h3>
<ul>
<li><strong>Authors: </strong>Miao Li, Phuc Nguyen, Christopher Tam, Alexandra Morgan, Kenneth Ge, Rahul Bansal, Linzi Yu, Rima Arnaout, Ramy Arnaout</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17752">https://arxiv.org/abs/2509.17752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17752">https://arxiv.org/pdf/2509.17752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17752]] GEM-T: Generative Tabular Data via Fitting Moments(https://arxiv.org/abs/2509.17752)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Tabular data dominates data science but poses challenges for generative models, especially when the data is limited or sensitive. We present a novel approach to generating synthetic tabular data based on the principle of maximum entropy -- MaxEnt -- called GEM-T, for ``generative entropy maximization for tables.'' GEM-T directly captures nth-order interactions -- pairwise, third-order, etc. -- among columns of training data. In extensive testing, GEM-T matches or exceeds deep neural network approaches previously regarded as state-of-the-art in 23 of 34 publicly available datasets representing diverse subject domains (68\%). Notably, GEM-T involves orders-of-magnitude fewer trainable parameters, demonstrating that much of the information in real-world data resides in low-dimensional, potentially human-interpretable correlations, provided that the input data is appropriately transformed first. Furthermore, MaxEnt better handles heterogeneous data types (continuous vs. discrete vs. categorical), lack of local structure, and other features of tabular data. GEM-T represents a promising direction for light-weight high-performance generative models for structured data.</li>
<li><strong>摘要：</strong>表格数据主导了数据科学，但对生成模型构成了挑战，尤其是当数据有限或敏感时。我们提出了一种基于最大熵的原理生成合成表格数据的新方法 - 最大 - 称为GEM-T，用于````'''''''''''''''Gem-t直接捕获训练数据列之间的Nth-order相互作用 - 成对，三阶，三阶等。在广泛的测试中，GEM-T匹配或超过了先前被认为是代表各种主题域的34个公开可用数据集中的23种（68 \％）的深度神经网络方法。值得注意的是，GEM-T涉及较少的可训练参数，表明现实世界数据中的许多信息都存在于低维，潜在的人性化相关性的情况下，只要输入数据首先是正确转换的。此外，Maxent更好地处理异质数据类型（连续与离散与分类），缺乏本地结构以及表格数据的其他特征。 GEM-T代表了用于结构化数据的轻质高性能生成模型的有希望的方向。</li>
</ul>

<h3>Title: Multi-Agent Amodal Completion: Direct Synthesis with Fine-Grained Semantic Guidance</h3>
<ul>
<li><strong>Authors: </strong>Hongxing Fan, Lipeng Wang, Haohua Chen, Zehuan Huang, Jiangtao Wu, Lu Sheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17757">https://arxiv.org/abs/2509.17757</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17757">https://arxiv.org/pdf/2509.17757</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17757]] Multi-Agent Amodal Completion: Direct Synthesis with Fine-Grained Semantic Guidance(https://arxiv.org/abs/2509.17757)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Amodal completion, generating invisible parts of occluded objects, is vital for applications like image editing and AR. Prior methods face challenges with data needs, generalization, or error accumulation in progressive pipelines. We propose a Collaborative Multi-Agent Reasoning Framework based on upfront collaborative reasoning to overcome these issues. Our framework uses multiple agents to collaboratively analyze occlusion relationships and determine necessary boundary expansion, yielding a precise mask for inpainting. Concurrently, an agent generates fine-grained textual descriptions, enabling Fine-Grained Semantic Guidance. This ensures accurate object synthesis and prevents the regeneration of occluders or other unwanted elements, especially within large inpainting areas. Furthermore, our method directly produces layered RGBA outputs guided by visible masks and attention maps from a Diffusion Transformer, eliminating extra segmentation. Extensive evaluations demonstrate our framework achieves state-of-the-art visual quality.</li>
<li><strong>摘要：</strong>Amodal完成，生成遮挡物体的隐形部分，对于图像编辑和AR等应用至关重要。先前的方法在渐进管道中面临数据需求，概括或错误积累的挑战。我们提出了一个基于预先协作推理的协作多代理推理框架，以克服这些问题。我们的框架使用多种代理来协作分析遮挡关系并确定必要的边界扩展，从而获得精确的掩盖掩护。同时，代理会生成细粒的文本描述，从而实现精细的语义指导。这样可以确保准确的物体综合，并防止阻塞或其他不需要的元素的再生，尤其是在大型填充区域内。此外，我们的方法直接产生以可见的掩模和扩散变压器的注意图为引导的分层RGBA输出，从而消除了额外的分割。广泛的评估表明我们的框架可实现最先进的视觉质量。</li>
</ul>

<h3>Title: Incorporating the Refractory Period into Spiking Neural Networks through Spike-Triggered Threshold Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Yang Li, Xinyi Zeng, Zhe Xue, Pinxian Zeng, Zikai Zhang, Yan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17769">https://arxiv.org/abs/2509.17769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17769">https://arxiv.org/pdf/2509.17769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17769]] Incorporating the Refractory Period into Spiking Neural Networks through Spike-Triggered Threshold Dynamics(https://arxiv.org/abs/2509.17769)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>As the third generation of neural networks, spiking neural networks (SNNs) have recently gained widespread attention for their biological plausibility, energy efficiency, and effectiveness in processing neuromorphic datasets. To better emulate biological neurons, various models such as Integrate-and-Fire (IF) and Leaky Integrate-and-Fire (LIF) have been widely adopted in SNNs. However, these neuron models overlook the refractory period, a fundamental characteristic of biological neurons. Research on excitable neurons reveal that after firing, neurons enter a refractory period during which they are temporarily unresponsive to subsequent stimuli. This mechanism is critical for preventing over-excitation and mitigating interference from aberrant signals. Therefore, we propose a simple yet effective method to incorporate the refractory period into spiking LIF neurons through spike-triggered threshold dynamics, termed RPLIF. Our method ensures that each spike accurately encodes neural information, effectively preventing neuron over-excitation under continuous inputs and interference from anomalous inputs. Incorporating the refractory period into LIF neurons is seamless and computationally efficient, enhancing robustness and efficiency while yielding better performance with negligible overhead. To the best of our knowledge, RPLIF achieves state-of-the-art performance on Cifar10-DVS(82.40%) and N-Caltech101(83.35%) with fewer timesteps and demonstrates superior performance on DVS128 Gesture(97.22%) at low latency.</li>
<li><strong>摘要：</strong>作为第三代神经网络，尖峰神经网络（SNN）最近因其在处理神经形态数据集中的生物学合理性，能量效率和有效性而引起了广泛关注。为了更好地模拟生物神经元，在SNN中广泛采用了各种模型（例如集成和泄漏）（如果）和泄漏的集成和开火（LIF）。但是，这些神经元模型忽略了难治时期，这是生物神经元的基本特征。对兴奋性神经元的研究表明，开火后，神经元进入难治时期，在此期间它们对随后的刺激无反应。这种机制对于防止过度兴奋和减轻异常信号的干扰至关重要。因此，我们提出了一种简单而有效的方法，将耐火度周期纳入尖峰LIF神经元中，通过尖峰触发的阈值动力学，称为RPLIF。我们的方法可确保每个尖峰能够准确地编码神经信息，从而有效防止在连续输入和异常输入中干扰下神经元过度兴奋。将难治性周期纳入LIF神经元是无缝的，并且在计算上是有效的，可以提高鲁棒性和效率，同时通过可忽略不计的开销，从而产生更好的性能。据我们所知，RPLIF在CIFAR10-DVS（82.40％）和N-Caltech101（83.35％）上实现了最先进的性能，时间段较少，并且在低延迟时表现出了DVS128手势（97.22％）的卓越性能。</li>
</ul>

<h3>Title: I2VWM: Robust Watermarking for Image to Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Guanjie Wang, Zehua Ma, Han Fang, Weiming Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17773">https://arxiv.org/abs/2509.17773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17773">https://arxiv.org/pdf/2509.17773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17773]] I2VWM: Robust Watermarking for Image to Video Generation(https://arxiv.org/abs/2509.17773)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The rapid progress of image-guided video generation (I2V) has raised concerns about its potential misuse in misinformation and fraud, underscoring the urgent need for effective digital watermarking. While existing watermarking methods demonstrate robustness within a single modality, they fail to trace source images in I2V settings. To address this gap, we introduce the concept of Robust Diffusion Distance, which measures the temporal persistence of watermark signals in generated videos. Building on this, we propose I2VWM, a cross-modal watermarking framework designed to enhance watermark robustness across time. I2VWM leverages a video-simulation noise layer during training and employs an optical-flow-based alignment module during inference. Experiments on both open-source and commercial I2V models demonstrate that I2VWM significantly improves robustness while maintaining imperceptibility, establishing a new paradigm for cross-modal watermarking in the era of generative video. \href{this https URL}{Code Released.}</li>
<li><strong>摘要：</strong>图像引导的视频生成（I2V）的快速进步引起了人们对其在错误信息和欺诈方面的潜在滥用的担忧，强调了迫切需要有效的数字水印。尽管现有的水印方法证明了单个模态内的鲁棒性，但它们无法在I2V设置中追踪源图像。为了解决这一差距，我们介绍了稳健的扩散距离的概念，该距离衡量了生成的视频中水印信号的时间持久性。在此基础上，我们提出了I2VWM，这是一种跨模式水印框架，旨在增强随时间的水印稳健性。 I2VWM在训练过程中利用视频模拟噪声层，并在推理过程中采用基于光学的对准模块。开源和商业I2V模型的实验表明，I2VWM在保持不可识别的同时显着提高了鲁棒性，在生成视频时代建立了新的跨模式水印范式。 \ href {此https url} {代码发布。}</li>
</ul>

<h3>Title: From Restoration to Reconstruction: Rethinking 3D Gaussian Splatting for Underwater Scenes</h3>
<ul>
<li><strong>Authors: </strong>Guoxi Huang, Haoran Wang, Zipeng Qi, Wenjun Lu, David Bull, Nantheera Anantrasirichai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17789">https://arxiv.org/abs/2509.17789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17789">https://arxiv.org/pdf/2509.17789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17789]] From Restoration to Reconstruction: Rethinking 3D Gaussian Splatting for Underwater Scenes(https://arxiv.org/abs/2509.17789)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Underwater image degradation poses significant challenges for 3D reconstruction, where simplified physical models often fail in complex scenes. We propose \textbf{R-Splatting}, a unified framework that bridges underwater image restoration (UIR) with 3D Gaussian Splatting (3DGS) to improve both rendering quality and geometric fidelity. Our method integrates multiple enhanced views produced by diverse UIR models into a single reconstruction pipeline. During inference, a lightweight illumination generator samples latent codes to support diverse yet coherent renderings, while a contrastive loss ensures disentangled and stable illumination representations. Furthermore, we propose \textit{Uncertainty-Aware Opacity Optimization (UAOO)}, which models opacity as a stochastic function to regularize training. This suppresses abrupt gradient responses triggered by illumination variation and mitigates overfitting to noisy or view-specific artifacts. Experiments on Seathru-NeRF and our new BlueCoral3D dataset demonstrate that R-Splatting outperforms strong baselines in both rendering quality and geometric accuracy.</li>
<li><strong>摘要：</strong>水下图像退化对3D重建构成了重大挑战，在复杂的场景中，简化的物理模型通常会失败。我们提出了\ textbf {r-Splatting}，这是一个统一的框架，它在水下图像恢复（UIR）中与3D高斯分裂（3DGS）架起，以改善渲染质量和几何忠诚度。我们的方法将各种UIR模型产生的多种增强视图集成到单个重建管道中。在推断期间，轻巧的照明发电机采样了潜在的代码以支持多样化但连贯的渲染，而对比度损失则确保了散布且稳定的照明表示。此外，我们提出\ textIt {不确定性意识不透明度优化（uaoo）}，它将不透明度模拟为随机函数以正规化训练。这抑制了由照明变化触发的突然梯度响应，并减轻过度拟合到嘈杂或特定的伪影。 Seathru-nerf和我们新的BlueCoral3D数据集的实验表明，R-Splatting在渲染质量和几何准确性方面的表现都优于强大的基准。</li>
</ul>

<h3>Title: Degradation-Aware All-in-One Image Restoration via Latent Prior Encoding</h3>
<ul>
<li><strong>Authors: </strong>S M A Sharif, Abdur Rehman, Fayaz Ali Dharejo, Radu Timofte, Rizwan Ali Naqvi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17792">https://arxiv.org/abs/2509.17792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17792">https://arxiv.org/pdf/2509.17792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17792]] Degradation-Aware All-in-One Image Restoration via Latent Prior Encoding(https://arxiv.org/abs/2509.17792)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Real-world images often suffer from spatially diverse degradations such as haze, rain, snow, and low-light, significantly impacting visual quality and downstream vision tasks. Existing all-in-one restoration (AIR) approaches either depend on external text prompts or embed hand-crafted architectural priors (e.g., frequency heuristics); both impose discrete, brittle assumptions that weaken generalization to unseen or mixed degradations. To address this limitation, we propose to reframe AIR as learned latent prior inference, where degradation-aware representations are automatically inferred from the input without explicit task cues. Based on latent priors, we formulate AIR as a structured reasoning paradigm: (1) which features to route (adaptive feature selection), (2) where to restore (spatial localization), and (3) what to restore (degradation semantics). We design a lightweight decoding module that efficiently leverages these latent encoded cues for spatially-adaptive restoration. Extensive experiments across six common degradation tasks, five compound settings, and previously unseen degradations demonstrate that our method outperforms state-of-the-art (SOTA) approaches, achieving an average PSNR improvement of 1.68 dB while being three times more efficient.</li>
<li><strong>摘要：</strong>现实世界中的图像通常会遭受空间上不同的降解，例如阴霾，雨水，雪和弱光，从而显着影响视觉质量和下游视觉任务。现有的全合一修复（空气）方法取决于外部文本提示或嵌入手工制作的建筑先验（例如频率启发式方法）；两者都施加了离散的，脆弱的假设，削弱了对看不见或混合降解的概括。为了解决这一限制，我们建议将空气重新构架为潜在的先前推断，在这种情况下，从输入中自动推断出降解感知表示，而无需明确的任务提示。基于潜在的先验，我们将空气作为结构化推理范例：（1）哪些特征路线（自适应特征选择），（2）在哪里恢复（空间定位），以及（3）要恢复的内容（降级语义）。我们设计了一个轻巧的解码模块，可有效利用这些潜在编码的提示进行空间自适应的修复。跨越六项常见的降解任务，五个复合设置以及以前看不见的降解的广泛实验表明，我们的方法的表现优于最先进的方法（SOTA）方法，实现了1.68 dB的平均PSNR改善，而效率提高了三倍。</li>
</ul>

<h3>Title: TS-P$^2$CL: Plug-and-Play Dual Contrastive Learning for Vision-Guided Medical Time Series Classification</h3>
<ul>
<li><strong>Authors: </strong>Qi'ao Xu, Pengfei Wang, Bo Zhong, Tianwen Qian, Xiaoling Wang, Ye Wang, Hong Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17802">https://arxiv.org/abs/2509.17802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17802">https://arxiv.org/pdf/2509.17802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17802]] TS-P$^2$CL: Plug-and-Play Dual Contrastive Learning for Vision-Guided Medical Time Series Classification(https://arxiv.org/abs/2509.17802)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Medical time series (MedTS) classification is pivotal for intelligent healthcare, yet its efficacy is severely limited by poor cross-subject generation due to the profound cross-individual heterogeneity. Despite advances in architectural innovations and transfer learning techniques, current methods remain constrained by modality-specific inductive biases that limit their ability to learn universally invariant representations. To overcome this, we propose TS-P$^2$CL, a novel plug-and-play framework that leverages the universal pattern recognition capabilities of pre-trained vision models. We introduce a vision-guided paradigm that transforms 1D physiological signals into 2D pseudo-images, establishing a bridge to the visual domain. This transformation enables implicit access to rich semantic priors learned from natural images. Within this unified space, we employ a dual-contrastive learning strategy: intra-modal consistency enforces temporal coherence, while cross-modal alignment aligns time-series dynamics with visual semantics, thereby mitigating individual-specific biases and learning robust, domain-invariant features. Extensive experiments on six MedTS datasets demonstrate that TS-P$^2$CL consistently outperforms fourteen methods in both subject-dependent and subject-independent settings.</li>
<li><strong>摘要：</strong>医疗时间序列（MEDTS）分类对于智能医疗保健至关重要，但由于深刻的跨个体异质性，其功效受到跨主题不良的严重限制。尽管建筑创新和转移学习技术取得了进步，但当前的方法仍受到特定于模态的归纳偏见的限制，这些偏见限制了他们学习普遍不变的表示的能力。为了克服这一点，我们提出了TS-P $^2 $ CL，这是一个新颖的插件框架，利用了预训练的视觉模型的通用模式识别能力。我们引入了视觉引导的范式，该范式将一维生理信号转换为2D伪图像，建立了通往视觉域的桥梁。这种转变使人们能够隐含从自然图像中学到的丰富语义先验的访问。在这个统一的空间中，我们采用了双对对比的学习策略：模式内一致性强制执行时间连贯性，而跨模式的对齐将时间序列动态与视觉语义相结合，从而减轻了个体特定的偏见和学习鲁棒的稳健，域名，域名的特征。在六个MEDTS数据集上进行的广泛实验表明，TS-P $^2 $ Cl始终超过受试者独立的和主题独立的设置中的14种方法。</li>
</ul>

<h3>Title: Semantic and Visual Crop-Guided Diffusion Models for Heterogeneous Tissue Synthesis in Histopathology</h3>
<ul>
<li><strong>Authors: </strong>Saghir Alfasly, Wataru Uegami, MD Enamul Hoq, Ghazal Alabtah, H.R. Tizhoosh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17847">https://arxiv.org/abs/2509.17847</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17847">https://arxiv.org/pdf/2509.17847</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17847]] Semantic and Visual Crop-Guided Diffusion Models for Heterogeneous Tissue Synthesis in Histopathology(https://arxiv.org/abs/2509.17847)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Synthetic data generation in histopathology faces unique challenges: preserving tissue heterogeneity, capturing subtle morphological features, and scaling to unannotated datasets. We present a latent diffusion model that generates realistic heterogeneous histopathology images through a novel dual-conditioning approach combining semantic segmentation maps with tissue-specific visual crops. Unlike existing methods that rely on text prompts or abstract visual embeddings, our approach preserves critical morphological details by directly incorporating raw tissue crops from corresponding semantic regions. For annotated datasets (i.e., Camelyon16, Panda), we extract patches ensuring 20-80% tissue heterogeneity. For unannotated data (i.e., TCGA), we introduce a self-supervised extension that clusters whole-slide images into 100 tissue types using foundation model embeddings, automatically generating pseudo-semantic maps for training. Our method synthesizes high-fidelity images with precise region-wise annotations, achieving superior performance on downstream segmentation tasks. When evaluated on annotated datasets, models trained on our synthetic data show competitive performance to those trained on real data, demonstrating the utility of controlled heterogeneous tissue generation. In quantitative evaluation, prompt-guided synthesis reduces Frechet Distance by up to 6X on Camelyon16 (from 430.1 to 72.0) and yields 2-3x lower FD across Panda and TCGA. Downstream DeepLabv3+ models trained solely on synthetic data attain test IoU of 0.71 and 0.95 on Camelyon16 and Panda, within 1-2% of real-data baselines (0.72 and 0.96). By scaling to 11,765 TCGA whole-slide images without manual annotations, our framework offers a practical solution for an urgent need for generating diverse, annotated histopathology data, addressing a critical bottleneck in computational pathology.</li>
<li><strong>摘要：</strong>组织病理学中的合成数据生成面临独特的挑战：保留组织异质性，捕获微妙的形态特征以及对未注释的数据集进行缩放。我们提出了一种潜在扩散模型，该模型通过一种新型的双条件方法与组织特定的视觉作物结合了新型的双条件方法来生成现实的异质组织病理学图像。与依赖文本提示或抽象视觉嵌入的现有方法不同，我们的方法通过直接纳入相应语义区域的原始组织作物来保留关键的形态学细节。对于注释的数据集（即Camelyon16，Panda），我们提取斑块，确保20-80％的组织异质性。对于未经通知的数据（即TCGA），我们引入了一个自制的扩展，该扩展将整个扫描图像簇插入100种组织类型中，使用基础模型嵌入，自动生成伪声音地图进行训练。我们的方法通过精确的区域注释综合了高保真图像，从而在下游分割任务上实现了卓越的性能。当对注释数据集进行评估时，对我们的合成数据训练的模型显示了对实际数据培训的模型，这表明了受控异质组织产生的实用性。在定量评估中，迅速引导的合成可在Camelyon16上（从430.1到72.0）将特雷切特的距离降低到6倍，并且在panda和TCGA中的FD降低了2-3倍。仅在Camelyon16和Panda上的合成数据达到0.71和0.95的下游DEEPLABV3+模型，在Real-Data基准的1-2％以内（0.72和0.96）。通过在没有手动注释的情况下扩展到11,765个TCGA全扫描图像，我们的框架为迫切需要生成多样化的注释的组织病理学数据提供了一种实用的解决方案，并解决了计算病理学中的关键瓶颈。</li>
</ul>

<h3>Title: Deep Hierarchical Learning with Nested Subspace Networks</h3>
<ul>
<li><strong>Authors: </strong>Paulius Rauba, Mihaela van der Schaar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17874">https://arxiv.org/abs/2509.17874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17874">https://arxiv.org/pdf/2509.17874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17874]] Deep Hierarchical Learning with Nested Subspace Networks(https://arxiv.org/abs/2509.17874)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large neural networks are typically trained for a fixed computational budget, creating a rigid trade-off between performance and efficiency that is ill-suited for deployment in resource-constrained or dynamic environments. Existing approaches to this problem present a difficult choice: training a discrete collection of specialist models is computationally prohibitive, while dynamic methods like slimmable networks often lack the flexibility to be applied to large, pre-trained foundation models. In this work, we propose Nested Subspace Networks (NSNs), a novel architectural paradigm that enables a single model to be dynamically and granularly adjusted across a continuous spectrum of compute budgets at inference time. The core of our approach is to re-parameterize linear layers to satisfy a nested subspace property, such that the function computed at a given rank is a strict subspace of the function at any higher rank. We show that this entire hierarchy of models can be optimized jointly via an uncertainty-aware objective that learns to balance the contributions of different ranks based on their intrinsic difficulty. We demonstrate empirically that NSNs can be surgically applied to pre-trained LLMs and unlock a smooth and predictable compute-performance frontier. For example, a single NSN-adapted model can achieve a 50% reduction in inference FLOPs with only a 5 percentage point loss in accuracy. Our findings establish NSNs as a powerful framework for creating the next generation of adaptive foundation models.</li>
<li><strong>摘要：</strong>大型神经网络通常接受固定计算预算的培训，从而在绩效和效率之间创造了严格的权衡，这不适合在资源受限或动态环境中部署。现有的方法是一个困难的选择：培训专家模型的离散集合在计算上是过时的，而诸如微小的网络之类的动态方法通常缺乏适用于大型，预训练的基础模型的灵活性。在这项工作中，我们提出了嵌套子空间网络（NSN），这是一种新型的体系结构范式，使单个模型可以在推理时在计算预算的连续频谱中进行动态和颗粒状的调整。我们方法的核心是重新参数化线性层以满足嵌套子空间属性，以便以给定等级计算的函数是该函数在任何较高等级处的严格子空间。我们表明，模型的整个层次结构可以通过一个不确定性意识的目标共同优化，该目标学会根据不同等级的固有难度来平衡不同等级的贡献。我们从经验上证明，NSN可以手术应用于预先训练的LLMS并解锁光滑且可预测的计算绩效边界。例如，单个NSN适应的模型可以使推理拖失板降低50％，而准确性仅5个百分点损失。我们的发现建立了NSN作为创建下一代自适应基础模型的有力框架。</li>
</ul>

<h3>Title: Optimizing Inference in Transformer-Based Models: A Multi-Method Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Siu Hang Ho, Prasad Ganesan, Nguyen Duong, Daniel Schlabig</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17894">https://arxiv.org/abs/2509.17894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17894">https://arxiv.org/pdf/2509.17894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17894]] Optimizing Inference in Transformer-Based Models: A Multi-Method Benchmark(https://arxiv.org/abs/2509.17894)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Efficient inference is a critical challenge in deep generative modeling, particularly as diffusion models grow in capacity and complexity. While increased complexity often improves accuracy, it raises compute costs, latency, and memory requirements. This work investigates techniques such as pruning, quantization, knowledge distillation, and simplified attention to reduce computational overhead without impacting performance. The study also explores the Mixture of Experts (MoE) approach to further enhance efficiency. These experiments provide insights into optimizing inference for the state-of-the-art Fast Diffusion Transformer (fast-DiT) model.</li>
<li><strong>摘要：</strong>在深层生成建模中，有效的推断是一个关键的挑战，尤其是随着扩散模型的能力和复杂性的增长。尽管增加的复杂性通常会提高准确性，但它提高了计算成本，延迟和内存要求。这项工作研究了诸如修剪，量化，知识蒸馏和简化注意力之类的技术，以减少计算开销而不会影响性能。该研究还探讨了专家（MOE）方法的混合，以进一步提高效率。这些实验为优化最新的快速扩散变压器（快速dit）模型的推断提供了见解。</li>
</ul>

<h3>Title: StefaLand: An Efficient Geoscience Foundation Model That Improves Dynamic Land-Surface Predictions</h3>
<ul>
<li><strong>Authors: </strong>Nicholas Kraabel, Jiangtao Liu, Yuchen Bian, Daniel Kifer, Chaopeng Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17942">https://arxiv.org/abs/2509.17942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17942">https://arxiv.org/pdf/2509.17942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17942]] StefaLand: An Efficient Geoscience Foundation Model That Improves Dynamic Land-Surface Predictions(https://arxiv.org/abs/2509.17942)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Stewarding natural resources, mitigating floods, droughts, wildfires, and landslides, and meeting growing demands require models that can predict climate-driven land-surface responses and human feedback with high accuracy. Traditional impact models, whether process-based, statistical, or machine learning, struggle with spatial generalization due to limited observations and concept drift. Recently proposed vision foundation models trained on satellite imagery demand massive compute and are ill-suited for dynamic land-surface prediction. We introduce StefaLand, a generative spatiotemporal earth foundation model centered on landscape interactions. StefaLand improves predictions on three tasks and four datasets: streamflow, soil moisture, and soil composition, compared to prior state-of-the-art. Results highlight its ability to generalize across diverse, data-scarce regions and support broad land-surface applications. The model builds on a masked autoencoder backbone that learns deep joint representations of landscape attributes, with a location-aware architecture fusing static and time-series inputs, attribute-based representations that drastically reduce compute, and residual fine-tuning adapters that enhance transfer. While inspired by prior methods, their alignment with geoscience and integration in one model enables robust performance on dynamic land-surface tasks. StefaLand can be pretrained and finetuned on academic compute yet outperforms state-of-the-art baselines and even fine-tuned vision foundation models. To our knowledge, this is the first geoscience land-surface foundation model that demonstrably improves dynamic land-surface interaction predictions and supports diverse downstream applications.</li>
<li><strong>摘要：</strong>管理自然资源，减轻洪水，干旱，野火和滑坡，并满足不断增长的需求，需要模型，以高度准确地预测气候驱动的土地表面反应和人类反馈。传统影响模型，无论是基于过程，统计还是机器学习，由于观察到有限和概念漂移而与空间泛化作斗争。最近提出的视觉基础模型在卫星图像需求需求大规模计算中训练，并且不适合动态的土地表面预测。我们介绍了Stefaland，这是一种以景观相互作用为中心的生成时空地球基础模型。 Stefaland改善了对三个任务和四个数据集的预测：与先前的最新时间相比，水流，土壤水分和土壤成分。结果突出了其在各种，数据堆积区域概括并支持广泛的土地表面应用的能力。该模型建立在屏蔽的自动编码器主链基础上，该主型主链了解景观属性的深度联合表示，具有位置感知的体系结构融合了静态和时间序列输入，基于属性的表示，可大大减少计算，以及残留的微调适配器，以增强传输。尽管受到先前方法的启发，但它们与一个模型中的地球科学和集成的一致性可以在动态的土地表面任务上进行出色的性能。 Stefaland可以在学术计算方面进行审议和填充，但表现优于最先进的基线，甚至是精心调整的视觉基础模型。据我们所知，这是第一个地球科学土地表面基础模型，它可以明显地改善动态的土地表面相互作用预测并支持各种下游应用。</li>
</ul>

<h3>Title: StableGuard: Towards Unified Copyright Protection and Tamper Localization in Latent Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Haoxin Yang, Bangzhen Liu, Xuemiao Xu, Cheng Xu, Yuyang Yu, Zikai Huang, Yi Wang, Shengfeng He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.17993">https://arxiv.org/abs/2509.17993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.17993">https://arxiv.org/pdf/2509.17993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.17993]] StableGuard: Towards Unified Copyright Protection and Tamper Localization in Latent Diffusion Models(https://arxiv.org/abs/2509.17993)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The advancement of diffusion models has enhanced the realism of AI-generated content but also raised concerns about misuse, necessitating robust copyright protection and tampering localization. Although recent methods have made progress toward unified solutions, their reliance on post hoc processing introduces considerable application inconvenience and compromises forensic reliability. We propose StableGuard, a novel framework that seamlessly integrates a binary watermark into the diffusion generation process, ensuring copyright protection and tampering localization in Latent Diffusion Models through an end-to-end design. We develop a Multiplexing Watermark VAE (MPW-VAE) by equipping a pretrained Variational Autoencoder (VAE) with a lightweight latent residual-based adapter, enabling the generation of paired watermarked and watermark-free images. These pairs, fused via random masks, create a diverse dataset for training a tampering-agnostic forensic network. To further enhance forensic synergy, we introduce a Mixture-of-Experts Guided Forensic Network (MoE-GFN) that dynamically integrates holistic watermark patterns, local tampering traces, and frequency-domain cues for precise watermark verification and tampered region detection. The MPW-VAE and MoE-GFN are jointly optimized in a self-supervised, end-to-end manner, fostering a reciprocal training between watermark embedding and forensic accuracy. Extensive experiments demonstrate that StableGuard consistently outperforms state-of-the-art methods in image fidelity, watermark verification, and tampering localization.</li>
<li><strong>摘要：</strong>扩散模型的进步增强了AI生成的内容的现实主义，但也引起了人们对滥用的担忧，需要强大的版权保护和篡改本地化。尽管最近的方法已取得了统一解决方案的进步，但他们对事后处理的依赖却带来了相当大的应用不便，并妥协了法医的可靠性。我们提出了StableGuard，这是一个新颖的框架，将二进制水印无缝地集成到扩散生成过程中，从而确保通过端到端设计在潜在扩散模型中进行版权保护并篡改本地化。我们通过配备预处理的变异自动编码器（VAE）以及轻质潜在的基于残留的剩余适配器，从而开发多发性水印VAE（MPW-VAE），从而使配对的水印和无水印图像能够生成。这些对通过随机掩码融合的，创建了一个多样化的数据集，用于培训篡改 - 静态法医网络。为了进一步增强法医协同作用，我们引入了由指导的法医网络（MOE-GFN）的混合物，该混合物通过动态整合整体水印模式，局部篡改痕迹和频域提示，以进行精确的水印验证和篡改区域检测。 MPW-VAE和MOE-GFN以一种自制的，端到端的方式共同优化，从而促进了水印嵌入和法医精度之间的相互训练。广泛的实验表明，稳定的守卫在图像保真度，水印验证和篡改定位方面始终超过最先进的方法。</li>
</ul>

<h3>Title: Reinforced Generation of Combinatorial Structures: Applications to Complexity Theory</h3>
<ul>
<li><strong>Authors: </strong>Ansh Nagda, Prabhakar Raghavan, Abhradeep Thakurta</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CC, math.CO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.18057">https://arxiv.org/abs/2509.18057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.18057">https://arxiv.org/pdf/2509.18057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.18057]] Reinforced Generation of Combinatorial Structures: Applications to Complexity Theory(https://arxiv.org/abs/2509.18057)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We explore whether techniques from AI can help discover new combinatorial structures that improve provable limits on efficient algorithms. Specifically, we use AlphaEvolve (an LLM coding agent) to study two settings: a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a recent result of Kunisky and Yu to obtain near-optimal upper and (conditional) lower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on random 3- and 4-regular graphs. Our improved lower bounds are obtained by constructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using AlphaEvolve. Additionally, via analytical arguments we strengthen the upper bounds to settle the computational hardness of these questions up to an error in the third decimal place. b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new inapproximability results, proving that it is NP-hard to approximate MAX-4-CUT and MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using AlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves upon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current best gadget-based inapproximability result of $0.9853$, but falls short of improving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget reduction from "standard" Håstad-style PCPs. A key technical challenge we faced: verifying a candidate construction produced by AlphaEvolve is costly (often requiring exponential time). In both settings above, our results were enabled by using AlphaEvolve itself to evolve the verification procedure to be faster (sometimes by $10,000\times$). We conclude with a discussion of norms by which to assess the assistance from AI in developing proofs.</li>
<li><strong>摘要：</strong>我们探索来自AI的技术是否可以帮助发现新的组合结构，以改善有效算法的可证明限制。具体而言，我们使用alphaevolve（LLM编码剂）研究两个设置：a）用于最大切割和最大不依赖性集合的平均值硬度：我们改善了Kunisky和Yu的最新结果，以获得最接近的上层和（条件）下界的较低限制，以在最大限制和最大依赖性的3--和4- regular 3-regrigard和4- regriment regriment 3-- regular regrimindriment regriment 3-- regriment和4- regriment regriment regriment regriment regriment cornerification算法上。我们的改进的下限是通过使用Alphaevolve构建多达$ 163 $节点的几乎极端Ramanujan图形来获得的。此外，通过分析论证，我们加强了上限，以解决这些问题的计算硬度，以在第三个小数位置到误差。 b）最大近似值的近似值最差的硬度：我们获得了新的不Xibibibility结果，证明在分别在$ 0.987 $和$ 0.9649 $的因素内，使用Alphaevolve发现Alphaevolve发现新的GADGET减少是NP-HARD。 Our MAX-4-CUT result improves upon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current best gadget-based inapproximability result of $0.9853$, but falls short of improving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget reduction from "standard" Håstad-style PCPs.我们面临的关键技术挑战是：验证Alphaevolve生产的候选建筑的昂贵（通常需要指数时间）。在上面的两个设置中，我们通过使用Alphaevolve自身发展验证过程（有时为$ 10,000 \ times $）来启用我们的结果。最后，我们讨论了在制定证据方面评估AI援助的规范。</li>
</ul>

<h3>Title: Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Sudhanshu Agrawal, Risheek Garrepalli, Raghavv Goel, Mingu Lee, Christopher Lott, Fatih Porikli</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.18085">https://arxiv.org/abs/2509.18085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.18085">https://arxiv.org/pdf/2509.18085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.18085]] Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding(https://arxiv.org/abs/2509.18085)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs (AR-LLMs) with the potential to operate at significantly higher token generation rates. However, currently available open-source dLLMs often generate at much lower rates, typically decoding only a single token at every denoising timestep in order to maximize output quality. We present Spiffy, a speculative decoding algorithm that accelerates dLLM inference by $\mathbf{2.8{-}3.1\times}$ while provably preserving the model's output distribution. This work addresses the unique challenges involved in applying ideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes draft states by leveraging the dLLM's distribution itself in an auto-speculative manner. This approach is efficient and effective, and eliminates the overheads of training and running an independent draft model. To structure the candidate draft states, we propose a novel directed draft graph which is uniquely designed to take advantage of the bidirectional, block-wise nature of dLLM generation and can be verified in parallel by the dLLM. To further optimize the structure of these draft graphs, we introduce an efficient, offline calibration algorithm that procedurally determines high-quality graph configurations. These optimized draft graphs, enabling increased acceptance rates, lead to a significant boost in the overall speedup achieved by the system. Crucially, Spiffy is also complementary to other recent innovations in improving dLLM generation speeds such as KV-caching and multi-token unmasking. We demonstrate that when combined with such parallel decoding algorithms, Spiffy is able to effectively multiply the benefits of these methods leading to total speedups of up to $\mathbf{7.9\times}$.</li>
<li><strong>摘要：</strong>扩散LLM（DLLM）最近已成为自回归LLM（AR-LLM）的强大替代方法，其潜力有明显更高的令牌生成率。但是，目前可用的开源DLLM通常以较低的速率产生，通常在每个DeNoising Timestep上仅解码一个令牌，以最大程度地提高输出质量。我们提出Spiffy是一种投机解码算法，该算法通过$ \ Mathbf {2.8 { - } 3.1 \ times} $加速DLLM推断，而可以证明可以保留模型的输出分布。这项工作解决了将AR-LLLS投机解码应用到DLLM设置的想法所涉及的独特挑战。 Spiffy通过以自动规范方式利用DLLM的发行本身，提出了草案。这种方法是有效和有效的，并消除了培训的间接费用和运行独立的草稿模型。为了构建候选草稿状态，我们提出了一个新颖的定向草稿图，该图旨在利用DLLM生成的双向性，稳定性的性质，并可以通过DLLM并行验证。为了进一步优化这些草稿图的结构，我们引入了一种有效的，离线校准算法，该算法在过程中确定高质量的图形配置。这些优化的草稿图以提高了接受率，从而大大提高了系统的总体加速。至关重要的是，Spiffy也与最近在提高DLLM生成速度（例如KV-CACHING和MULTI-TOKE DOMENS UNSEAKS）的其他创新方面进行了补充。我们证明，当与这种并行解码算法结合使用时，Spiffy能够有效地乘以这些方法的好处，从而导致总速度最高为$ \ MATHBF {7.9 \ times} $。</li>
</ul>

<h3>Title: ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Guocheng Gordon Qian, Daniil Ostashev, Egor Nemchinov, Avihay Assouline, Sergey Tulyakov, Kuan-Chieh Jackson Wang, Kfir Aberman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.18092">https://arxiv.org/abs/2509.18092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.18092">https://arxiv.org/pdf/2509.18092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.18092]] ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image Generation(https://arxiv.org/abs/2509.18092)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating high-fidelity images of humans with fine-grained control over attributes such as hairstyle and clothing remains a core challenge in personalized text-to-image synthesis. While prior methods emphasize identity preservation from a reference image, they lack modularity and fail to provide disentangled control over specific visual attributes. We introduce a new paradigm for attribute-specific image prompting, in which distinct sets of reference images are used to guide the generation of individual aspects of human appearance, such as hair, clothing, and identity. Our method encodes these inputs into attribute-specific tokens, which are injected into a pre-trained text-to-image diffusion model. This enables compositional and disentangled control over multiple visual factors, even across multiple people within a single image. To promote natural composition and robust disentanglement, we curate a cross-reference training dataset featuring subjects in diverse poses and expressions, and propose a multi-attribute cross-reference training strategy that encourages the model to generate faithful outputs from misaligned attribute inputs while adhering to both identity and textual conditioning. Extensive experiments show that our method achieves state-of-the-art performance in accurately following both visual and textual prompts. Our framework paves the way for more configurable human image synthesis by combining visual prompting with text-driven generation. Webpage is available at: this https URL.</li>
<li><strong>摘要：</strong>对人类的高保真图像具有对诸如发型和服装等属性的细粒度控制的高保真图像仍然是个性化文本对图像综合的核心挑战。虽然先前的方法强调了从参考图像中保存身份，但它们缺乏模块化，并且无法对特定的视觉属性进行分离的控制。我们引入了一种针对特定于属性的图像提示的新范式，其中使用了不同的参考图像集来指导人类外观各个方面的产生，例如头发，衣服和身份。我们的方法将这些输入编码为属性特异性令牌，这些输入被注入预训练的文本对图像扩散模型中。这使得即使在单个图像中的多个人之间，也可以对多个视觉因素进行组成和分离的控制。为了促进自然构图和鲁棒的分解，我们策划了一个跨参考训练数据集，其中包含各种姿势和表达方式的主题，并提出了一种多属性的跨参考培训策略，该策略鼓励该模型从未对身份和文本条件依附的属性属性输入中产生忠实的输出。广泛的实验表明，我们的方法可以准确地遵循视觉和文字提示，从而实现了最先进的性能。我们的框架通过将视觉提示与文本驱动的生成相结合，为更多可配置的人类图像综合铺平了道路。网页可在以下网页上找到：此HTTPS URL。</li>
</ul>

<h3>Title: Seg4Diff: Unveiling Open-Vocabulary Segmentation in Text-to-Image Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Chaehyun Kim, Heeseong Shin, Eunbeen Hong, Heeji Yoon, Anurag Arnab, Paul Hongsuck Seo, Sunghwan Hong, Seungryong Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.18096">https://arxiv.org/abs/2509.18096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.18096">https://arxiv.org/pdf/2509.18096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.18096]] Seg4Diff: Unveiling Open-Vocabulary Segmentation in Text-to-Image Diffusion Transformers(https://arxiv.org/abs/2509.18096)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models excel at translating language prompts into photorealistic images by implicitly grounding textual concepts through their cross-modal attention mechanisms. Recent multi-modal diffusion transformers extend this by introducing joint self-attention over concatenated image and text tokens, enabling richer and more scalable cross-modal alignment. However, a detailed understanding of how and where these attention maps contribute to image generation remains limited. In this paper, we introduce Seg4Diff (Segmentation for Diffusion), a systematic framework for analyzing the attention structures of MM-DiT, with a focus on how specific layers propagate semantic information from text to image. Through comprehensive analysis, we identify a semantic grounding expert layer, a specific MM-DiT block that consistently aligns text tokens with spatially coherent image regions, naturally producing high-quality semantic segmentation masks. We further demonstrate that applying a lightweight fine-tuning scheme with mask-annotated image data enhances the semantic grouping capabilities of these layers and thereby improves both segmentation performance and generated image fidelity. Our findings demonstrate that semantic grouping is an emergent property of diffusion transformers and can be selectively amplified to advance both segmentation and generation performance, paving the way for unified models that bridge visual perception and generation.</li>
<li><strong>摘要：</strong>文本对图像扩散模型在将语言转化为逼真的图像中表现出色，通过通过其交叉模式的注意机制隐式接地文本概念。最近的多模式扩散变压器通过对串联的图像和文本令牌引入关节自我注意，从而实现了更丰富，更可扩展的跨模式对齐方式，从而扩展了这一点。但是，对这些注意力图有助于图像生成的方式和何处的详细理解仍然有限。在本文中，我们介绍了SEG4DIFF（扩散的分割），这是一个系统的框架，用于分析MM-DIT的注意力结构，重点是特定层如何将语义信息从文本传播到图像。通过全面的分析，我们确定了一个语义基础专家层，这是一个特定的MM-DIT块，该块始终将文本令牌与空间相干图像区域保持一致，自然会产生高质量的语义分割掩码。我们进一步证明，使用蒙版意识的图像数据应用轻巧的微调方案增强了这些层的语义分组功能，从而提高了分割性能和生成的图像保真度。我们的发现表明，语义分组是扩散变压器的新兴特性，可以选择性地放大以提高分割和生成性能，为桥接视觉感知和生成的统一模型铺平了道路。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
