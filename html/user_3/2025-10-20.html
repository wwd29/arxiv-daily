<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-20</h1>
<h3>Title: Constantly Improving Image Models Need Constantly Improving Benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Jiaxin Ge, Grace Luo, Heekyung Lee, Nishant Malpani, Long Lian, XuDong Wang, Aleksander Holynski, Trevor Darrell, Sewon Min, David M. Chan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15021">https://arxiv.org/abs/2510.15021</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15021">https://arxiv.org/pdf/2510.15021</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15021]] Constantly Improving Image Models Need Constantly Improving Benchmarks(https://arxiv.org/abs/2510.15021)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in image generation, often driven by proprietary systems like GPT-4o Image Gen, regularly introduce new capabilities that reshape how users interact with these models. Existing benchmarks often lag behind and fail to capture these emerging use cases, leaving a gap between community perceptions of progress and formal evaluation. To address this, we present ECHO, a framework for constructing benchmarks directly from real-world evidence of model use: social media posts that showcase novel prompts and qualitative user judgments. Applying this framework to GPT-4o Image Gen, we construct a dataset of over 31,000 prompts curated from such posts. Our analysis shows that ECHO (1) discovers creative and complex tasks absent from existing benchmarks, such as re-rendering product labels across languages or generating receipts with specified totals, (2) more clearly distinguishes state-of-the-art models from alternatives, and (3) surfaces community feedback that we use to inform the design of metrics for model quality (e.g., measuring observed shifts in color, identity, and structure). Our website is at this https URL.</li>
<li><strong>摘要：</strong>图像生成方面的最新进展通常由 GPT-4o Image Gen 等专有系统驱动，定期引入新功能，重塑用户与这些模型的交互方式。现有的基准往往落后并且无法捕捉这些新兴用例，从而在社区对进展的看法与正式评估之间留下了差距。为了解决这个问题，我们提出了 ECHO，一个直接根据模型使用的现实证据构建基准的框架：展示新颖提示和定性用户判断的社交媒体帖子。将此框架应用于 GPT-4o Image Gen，我们构建了一个包含超过 31,000 个从此类帖子中精选的提示的数据集。我们的分析表明，ECHO (1) 发现了现有基准中缺少的创造性和复杂的任务，例如跨语言重新渲染产品标签或生成指定总数的收据，(2) 更清楚地区分最先进的模型和替代模型，以及 (3) 提供社区反馈，我们用这些反馈来设计模型质量指标（例如，测量观察到的颜色、身份和结构的变化）。我们的网站位于此 https URL。</li>
</ul>

<h3>Title: LoRAverse: A Submodular Framework to Retrieve Diverse Adapters for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Mert Sonmezer, Matthew Zheng, Pinar Yanardag</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15022">https://arxiv.org/abs/2510.15022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15022">https://arxiv.org/pdf/2510.15022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15022]] LoRAverse: A Submodular Framework to Retrieve Diverse Adapters for Diffusion Models(https://arxiv.org/abs/2510.15022)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Low-rank Adaptation (LoRA) models have revolutionized the personalization of pre-trained diffusion models by enabling fine-tuning through low-rank, factorized weight matrices specifically optimized for attention layers. These models facilitate the generation of highly customized content across a variety of objects, individuals, and artistic styles without the need for extensive retraining. Despite the availability of over 100K LoRA adapters on platforms like this http URL, users often face challenges in navigating, selecting, and effectively utilizing the most suitable adapters due to their sheer volume, diversity, and lack of structured organization. This paper addresses the problem of selecting the most relevant and diverse LoRA models from this vast database by framing the task as a combinatorial optimization problem and proposing a novel submodular framework. Our quantitative and qualitative experiments demonstrate that our method generates diverse outputs across a wide range of domains.</li>
<li><strong>摘要：</strong>低秩适应 (LoRA) 模型通过专门针对注意力层优化的低秩分解权重矩阵进行微调，彻底改变了预训练扩散模型的个性化。这些模型有助于生成跨各种对象、个人和艺术风格的高度定制的内容，而无需进行大量的再培训。尽管在像这样的 http URL 的平台上有超过 100K LoRA 适配器可用，但由于适配器数量庞大、多样性且缺乏结构化组织，用户在导航、选择和有效利用最合适的适配器时经常面临挑战。本文通过将任务构建为组合优化问题并提出一种新颖的子模块框架，解决了从这个庞大的数据库中选择最相关和最多样化的 LoRA 模型的问题。我们的定量和定性实验表明，我们的方法在广泛的领域产生了不同的输出。</li>
</ul>

<h3>Title: AlignFlow: Improving Flow-based Generative Models with Semi-Discrete Optimal Transport</h3>
<ul>
<li><strong>Authors: </strong>Lingkai Kong, Molei Tao, Yang Liu, Bryan Wang, Jinmiao Fu, Chien-Chih Wang, Huidong Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15038">https://arxiv.org/abs/2510.15038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15038">https://arxiv.org/pdf/2510.15038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15038]] AlignFlow: Improving Flow-based Generative Models with Semi-Discrete Optimal Transport(https://arxiv.org/abs/2510.15038)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Flow-based Generative Models (FGMs) effectively transform noise into complex data distributions. Incorporating Optimal Transport (OT) to couple noise and data during FGM training has been shown to improve the straightness of flow trajectories, enabling more effective inference. However, existing OT-based methods estimate the OT plan using (mini-)batches of sampled noise and data points, which limits their scalability to large and high-dimensional datasets in FGMs. This paper introduces AlignFlow, a novel approach that leverages Semi-Discrete Optimal Transport (SDOT) to enhance the training of FGMs by establishing an explicit, optimal alignment between noise distribution and data points with guaranteed convergence. SDOT computes a transport map by partitioning the noise space into Laguerre cells, each mapped to a corresponding data point. During FGM training, i.i.d. noise samples are paired with data points via the SDOT map. AlignFlow scales well to large datasets and model architectures with negligible computational overhead. Experimental results show that AlignFlow improves the performance of a wide range of state-of-the-art FGM algorithms and can be integrated as a plug-and-play component. Code is available at: this https URL.</li>
<li><strong>摘要：</strong>基于流的生成模型（FGM）有效地将噪声转化为复杂的数据分布。事实证明，在 FGM 训练期间结合最佳传输 (OT) 来耦合噪声和数据可以提高流动轨迹的直线度，从而实现更有效的推理。然而，现有的基于 OT 的方法使用（小）批量采样噪声和数据点来估计 OT 计划，这限制了它们对 FGM 中大型和高维数据集的可扩展性。本文介绍了 AlignFlow，这是一种利用半离散最优传输 (SDOT) 的新颖方法，通过在噪声分布和数据点之间建立明确的最优对齐来增强 FGM 的训练，并保证收敛。 SDOT 通过将噪声空间划分为拉盖尔单元来计算传输图，每个拉盖尔单元映射到相应的数据点。在 FGM 培训期间，i.i.d.噪声样本通过 SDOT 图与数据点配对。 AlignFlow 可以很好地扩展到大型数据集和模型架构，而计算开销可以忽略不计。实验结果表明，AlignFlow 提高了各种最先进的 FGM 算法的性能，并且可以集成为即插即用组件。代码可在以下位置获得：此 https URL。</li>
</ul>

<h3>Title: Generalized Dynamics Generation towards Scannable Physical World Model</h3>
<ul>
<li><strong>Authors: </strong>Yichen Li, Zhiyi Li, Brandon Feng, Dinghuai Zhang, Antonio Torralba</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15041">https://arxiv.org/abs/2510.15041</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15041">https://arxiv.org/pdf/2510.15041</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15041]] Generalized Dynamics Generation towards Scannable Physical World Model(https://arxiv.org/abs/2510.15041)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Digital twin worlds with realistic interactive dynamics presents a new opportunity to develop generalist embodied agents in scannable environments with complex physical behaviors. To this end, we present GDGen (Generalized Representation for Generalized Dynamics Generation), a framework that takes a potential energy perspective to seamlessly integrate rigid body, articulated body, and soft body dynamics into a unified, geometry-agnostic system. GDGen operates from the governing principle that the potential energy for any stable physical system should be low. This fresh perspective allows us to treat the world as one holistic entity and infer underlying physical properties from simple motion observations. We extend classic elastodynamics by introducing directional stiffness to capture a broad spectrum of physical behaviors, covering soft elastic, articulated, and rigid body systems. We propose a specialized network to model the extended material property and employ a neural field to represent deformation in a geometry-agnostic manner. Extensive experiments demonstrate that GDGen robustly unifies diverse simulation paradigms, offering a versatile foundation for creating interactive virtual environments and training robotic agents in complex, dynamically rich scenarios.</li>
<li><strong>摘要：</strong>具有真实交互动态的数字孪生世界为在具有复杂物理行为的可扫描环境中开发通用实体代理提供了新的机会。为此，我们提出了 GDGen（广义动力学生成的广义表示），这是一个框架，它采用势能视角，将刚体、关节体和软体动力学无缝集成到一个统一的、与几何无关的系统中。 GDGen 的运行遵循以下控制原则：任何稳定的物理系统的势能都应该较低。这种新鲜的视角使我们能够将世界视为一个整体实体，并从简单的运动观察中推断出潜在的物理特性。我们通过引入方向刚度来扩展经典弹性动力学，以捕获广泛的物理行为，涵盖软弹性、铰接式和刚体系统。我们提出了一个专门的网络来模拟扩展的材料属性，并采用神经场以与几何无关的方式表示变形。大量实验表明，GDGen 稳健地统一了不同的模拟范例，为创建交互式虚拟环境和在复杂、动态丰富的场景中训练机器人代理提供了通用基础。</li>
</ul>

<h3>Title: Comprehensive language-image pre-training for 3D medical image understanding</h3>
<ul>
<li><strong>Authors: </strong>Tassilo Wald, Ibrahim Ethem Hamamci, Yuan Gao, Sam Bond-Taylor, Harshita Sharma, Maximilian Ilse, Cynthia Lo, Olesya Melnichenko, Noel C. F. Codella, Maria Teodora Wetscherek, Klaus H. Maier-Hein, Panagiotis Korfiatis, Valentina Salvatelli, Javier Alvarez-Valle, Fernando Pérez-García</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15042">https://arxiv.org/abs/2510.15042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15042">https://arxiv.org/pdf/2510.15042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15042]] Comprehensive language-image pre-training for 3D medical image understanding(https://arxiv.org/abs/2510.15042)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision-language pre-training, i.e., aligning images with paired text, is a powerful paradigm to create encoders that can be directly used for tasks such as classification and retrieval, and for downstream tasks such as segmentation and report generation. In the 3D medical image domain, these capabilities allow vision-language encoders (VLEs) to support radiologists by retrieving patients with similar abnormalities or predicting likelihoods of abnormality. While the methodology holds promise, data availability limits the capabilities of current 3D VLEs. In this paper, we alleviate the lack of data by injecting additional inductive biases: introducing a report generation objective and pairing vision-language pre-training with vision-only pre-training. This allows us to leverage both image-only and paired image-text 3D datasets, increasing the total amount of data to which our model is exposed. Through these additional inductive biases, paired with best practices of the 3D medical imaging domain, we develop the Comprehensive Language-image Pre-training (COLIPRI) encoder family. Our COLIPRI encoders achieve state-of-the-art performance in report generation, classification probing, and zero-shot classification, and remain competitive for semantic segmentation.</li>
<li><strong>摘要：</strong>视觉语言预训练，即将图像与配对文本对齐，是创建可直接用于分类和检索等任务以及分割和报告生成等下游任务的编码器的强大范例。在 3D 医学图像领域，这些功能允许视觉语言编码器 (VLE) 通过检索具有类似异常的患者或预测异常的可能性来为放射科医生提供支持。虽然该方法很有前景，但数据可用性限制了当前 3D VLE 的功能。在本文中，我们通过注入额外的归纳偏差来缓解数据的缺乏：引入报告生成目标并将视觉语言预训练与仅视觉预训练配对。这使我们能够利用纯图像和配对图像文本 3D 数据集，增加模型所接触的数据总量。通过这些额外的归纳偏差，结合 3D 医学成像领域的最佳实践，我们开发了综合语言图像预训练 (COLIPRI) 编码器系列。我们的 COLIPRI 编码器在报告生成、分类探测和零样本分类方面实现了最先进的性能，并在语义分割方面保持竞争力。</li>
</ul>

<h3>Title: Operator Flow Matching for Timeseries Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Yolanne Yi Ran Lee, Kyriakos Flouris</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15101">https://arxiv.org/abs/2510.15101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15101">https://arxiv.org/pdf/2510.15101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15101]] Operator Flow Matching for Timeseries Forecasting(https://arxiv.org/abs/2510.15101)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Forecasting high-dimensional, PDE-governed dynamics remains a core challenge for generative modeling. Existing autoregressive and diffusion-based approaches often suffer cumulative errors and discretisation artifacts that limit long, physically consistent forecasts. Flow matching offers a natural alternative, enabling efficient, deterministic sampling. We prove an upper bound on FNO approximation error and propose TempO, a latent flow matching model leveraging sparse conditioning with channel folding to efficiently process 3D spatiotemporal fields using time-conditioned Fourier layers to capture multi-scale modes with high fidelity. TempO outperforms state-of-the-art baselines across three benchmark PDE datasets, and spectral analysis further demonstrates superior recovery of multi-scale dynamics, while efficiency studies highlight its parameter- and memory-light design compared to attention-based or convolutional regressors.</li>
<li><strong>摘要：</strong>预测高维、偏微分方程控制的动力学仍然是生成建模的核心挑战。现有的自回归和基于扩散的方法经常遭受累积误差和离散伪影的影响，从而限制了长期、物理一致的预测。流量匹配提供了一种自然的替代方案，可实现高效、确定性采样。我们证明了 FNO 近似误差的上限，并提出了 TempO，这是一种潜在流匹配模型，利用稀疏条件和通道折叠来有效处理 3D 时空场，使用时间条件傅立叶层来捕获高保真度的多尺度模式。 TempO 在三个基准偏微分方程数据集上的表现优于最先进的基线，谱分析进一步证明了多尺度动力学的卓越恢复，而效率研究则强调了与基于注意力或卷积回归器相比，其参数和记忆光设计。</li>
</ul>

<h3>Title: TGT: Text-Grounded Trajectories for Locally Controlled Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Guofeng Zhang, Angtian Wang, Jacob Zhiyuan Fang, Liming Jiang, Haotian Yang, Bo Liu, Yiding Yang, Guang Chen, Longyin Wen, Alan Yuille, Chongyang Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15104">https://arxiv.org/abs/2510.15104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15104">https://arxiv.org/pdf/2510.15104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15104]] TGT: Text-Grounded Trajectories for Locally Controlled Video Generation(https://arxiv.org/abs/2510.15104)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-video generation has advanced rapidly in visual fidelity, whereas standard methods still have limited ability to control the subject composition of generated scenes. Prior work shows that adding localized text control signals, such as bounding boxes or segmentation masks, can help. However, these methods struggle in complex scenarios and degrade in multi-object settings, offering limited precision and lacking a clear correspondence between individual trajectories and visual entities as the number of controllable objects increases. We introduce Text-Grounded Trajectories (TGT), a framework that conditions video generation on trajectories paired with localized text descriptions. We propose Location-Aware Cross-Attention (LACA) to integrate these signals and adopt a dual-CFG scheme to separately modulate local and global text guidance. In addition, we develop a data processing pipeline that produces trajectories with localized descriptions of tracked entities, and we annotate two million high quality video clips to train TGT. Together, these components enable TGT to use point trajectories as intuitive motion handles, pairing each trajectory with text to control both appearance and motion. Extensive experiments show that TGT achieves higher visual quality, more accurate text alignment, and improved motion controllability compared with prior approaches. Website: this https URL.</li>
<li><strong>摘要：</strong>文本到视频的生成在视觉保真度方面取得了快速进步，而标准方法控制生成场景的主题构成的能力仍然有限。先前的工作表明，添加本地化文本控制信号（例如边界框或分段掩码）会有所帮助。然而，这些方法在复杂场景中举步维艰，在多对象设置中性能下降，随着可控对象数量的增加，精度有限，并且个体轨迹和视觉实体之间缺乏清晰的对应关系。我们引入了基于文本的轨迹（TGT），这是一个框架，可以根据与本地化文本描述配对的轨迹来生成视频。我们提出位置感知交叉注意（LACA）来整合这些信号，并采用双 CFG 方案来分别调制本地和全局文本指导。此外，我们开发了一个数据处理管道，可生成具有跟踪实体的本地化描述的轨迹，并注释 200 万个高质量视频剪辑来训练 TGT。这些组件共同使 TGT 能够使用点轨迹作为直观的运动手柄，将每个轨迹与文本配对以控制外观和运动。大量实验表明，与之前的方法相比，TGT 实现了更高的视觉质量、更准确的文本对齐以及改进的运动可控性。网站：此 https URL。</li>
</ul>

<h3>Title: Deep generative priors for 3D brain analysis</h3>
<ul>
<li><strong>Authors: </strong>Ana Lawry Aguila, Dina Zemlyanker, You Cheng, Sudeshna Das, Daniel C. Alexander, Oula Puonti, Annabel Sorby-Adams, W. Taylor Kimberly, Juan Eugenio Iglesias</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15119">https://arxiv.org/abs/2510.15119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15119">https://arxiv.org/pdf/2510.15119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15119]] Deep generative priors for 3D brain analysis(https://arxiv.org/abs/2510.15119)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have recently emerged as powerful generative models in medical imaging. However, it remains a major challenge to combine these data-driven models with domain knowledge to guide brain imaging problems. In neuroimaging, Bayesian inverse problems have long provided a successful framework for inference tasks, where incorporating domain knowledge of the imaging process enables robust performance without requiring extensive training data. However, the anatomical modeling component of these approaches typically relies on classical mathematical priors that often fail to capture the complex structure of brain anatomy. In this work, we present the first general-purpose application of diffusion models as priors for solving a wide range of medical imaging inverse problems. Our approach leverages a score-based diffusion prior trained extensively on diverse brain MRI data, paired with flexible forward models that capture common image processing tasks such as super-resolution, bias field correction, inpainting, and combinations thereof. We further demonstrate how our framework can refine outputs from existing deep learning methods to improve anatomical fidelity. Experiments on heterogeneous clinical and research MRI data show that our method achieves state-of-the-art performance producing consistent, high-quality solutions without requiring paired training datasets. These results highlight the potential of diffusion priors as versatile tools for brain MRI analysis.</li>
<li><strong>摘要：</strong>扩散模型最近已成为医学成像领域强大的生成模型。然而，将这些数据驱动模型与领域知识相结合来指导大脑成像问题仍然是一个重大挑战。在神经成像中，贝叶斯逆问题长期以来为推理任务提供了一个成功的框架，其中结合成像过程的领域知识可以实现强大的性能，而无需大量的训练数据。然而，这些方法的解剖建模部分通常依赖于经典数学先验，而这些先验往往无法捕捉大脑解剖学的复杂结构。在这项工作中，我们提出了扩散模型的第一个通用应用，作为解决各种医学成像逆问题的先验。我们的方法利用基于分数的扩散，事先对不同的大脑 MRI 数据进行了广泛的训练，并与灵活的前向模型配对，该模型捕获常见的图像处理任务，例如超分辨率、偏差场校正、修复及其组合。我们进一步展示了我们的框架如何改进现有深度学习方法的输出以提高解剖保真度。对异构临床和研究 MRI 数据的实验表明，我们的方法实现了最先进的性能，无需配对训练数据集即可产生一致的高质量解决方案。这些结果凸显了扩散先验作为脑 MRI 分析多功能工具的潜力。</li>
</ul>

<h3>Title: Salient Concept-Aware Generative Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Tianchen Zhao, Xuanbai Chen, Zhihua Li, Jun Fang, Dongsheng An, Xiang Xu, Zhuowen Tu, Yifan Xing</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15194">https://arxiv.org/abs/2510.15194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15194">https://arxiv.org/pdf/2510.15194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15194]] Salient Concept-Aware Generative Data Augmentation(https://arxiv.org/abs/2510.15194)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent generative data augmentation methods conditioned on both image and text prompts struggle to balance between fidelity and diversity, as it is challenging to preserve essential image details while aligning with varied text prompts. This challenge arises because representations in the synthesis process often become entangled with non-essential input image attributes such as environmental contexts, creating conflicts with text prompts intended to modify these elements. To address this, we propose a personalized image generation framework that uses a salient concept-aware image embedding model to reduce the influence of irrelevant visual details during the synthesis process, thereby maintaining intuitive alignment between image and text inputs. By generating images that better preserve class-discriminative features with additional controlled variations, our framework effectively enhances the diversity of training datasets and thereby improves the robustness of downstream models. Our approach demonstrates superior performance across eight fine-grained vision datasets, outperforming state-of-the-art augmentation methods with averaged classification accuracy improvements by 0.73% and 6.5% under conventional and long-tail settings, respectively.</li>
<li><strong>摘要：</strong>最近以图像和文本提示为条件的生成数据增强方法很难在保真度和多样性之间取得平衡，因为在与不同的文本提示对齐的同时保留基本的图像细节具有挑战性。出现这一挑战是因为合成过程中的表示经常与非必要的输入图像属性（例如环境上下文）纠缠在一起，从而与旨在修改这些元素的文本提示产生冲突。为了解决这个问题，我们提出了一种个性化图像生成框架，该框架使用显着概念感知图像嵌入模型来减少合成过程中不相关视觉细节的影响，从而保持图像和文本输入之间的直观对齐。通过生成能够更好地保留具有额外受控变化的类判别特征的图像，我们的框架有效地增强了训练数据集的多样性，从而提高了下游模型的鲁棒性。我们的方法在八个细粒度视觉数据集上展示了卓越的性能，优于最先进的增强方法，在传统和长尾设置下平均分类精度分别提高了 0.73% 和 6.5%。</li>
</ul>

<h3>Title: Dual-Weighted Reinforcement Learning for Generative Preference Modeling</h3>
<ul>
<li><strong>Authors: </strong>Shengyu Feng, Yun He, Shuang Ma, Beibin Li, Yuanhao Xiong, Vincent Li, Karishma Mandyam, Julian Katz-Samuels, Shengjie Bi, Licheng Yu, Hejia Zhang, Karthik Abinav Sankararaman, Han Fang, Riham Mansour, Yiming Yang, Manaal Faruqui</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15242">https://arxiv.org/abs/2510.15242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15242">https://arxiv.org/pdf/2510.15242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15242]] Dual-Weighted Reinforcement Learning for Generative Preference Modeling(https://arxiv.org/abs/2510.15242)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has recently proven effective at scaling chain-of-thought (CoT) reasoning in large language models on tasks with verifiable answers. However, extending RL to more general non-verifiable tasks, typically in the format of human preference pairs, remains both challenging and underexplored. In this work, we propose Dual-Weighted Reinforcement Learning (DWRL), a new framework for preference modeling that integrates CoT reasoning with the Bradley-Terry (BT) model via a dual-weighted RL objective that preserves preference-modeling inductive bias. DWRL approximates the maximum-likelihood objective of the BT model with two complementary weights: an instance-wise misalignment weight, which emphasizes under-trained pairs misaligned with human preference, and a group-wise (self-normalized) conditional preference score, which promotes promising thoughts. In this paper, we apply DWRL to preference modeling by training generative preference models (GPMs) to first generate a thought and then predict the human preference score. Across multiple benchmarks and model scales (Llama3 and Qwen2.5), DWRL consistently outperforms both GPM baselines and scalar models, while producing coherent, interpretable thoughts. In summary, our results position DWRL as a general framework for reasoning-enhanced preference learning beyond verifiable tasks.</li>
<li><strong>摘要：</strong>最近，强化学习 (RL) 被证明可以有效地扩展大型语言模型中针对具有可验证答案的任务的思想链 (CoT) 推理。然而，将强化学习扩展到更一般的不可验证的任务（通常以人类偏好对的形式）仍然具有挑战性且尚未得到充分探索。在这项工作中，我们提出了双权重强化学习 (DWRL)，这是一种新的偏好建模框架，通过双权重 RL 目标将 CoT 推理与 Bradley-Terry (BT) 模型集成在一起，保留了偏好建模归纳偏差。 DWRL 使用两个互补权重来近似 BT 模型的最大似然目标：一个是实例方面的错位权重，它强调与人类偏好不一致的训练不足的配对；另一个是分组方面（自我标准化）条件偏好得分，它促进有希望的想法。在本文中，我们将 DWRL 应用于偏好建模，通过训练生成偏好模型 (GPM) 首先生成想法，然后预测人类偏好得分。在多个基准和模型尺度（Llama3 和 Qwen2.5）中，DWRL 始终优于 GPM 基线和标量模型，同时产生连贯的、可解释的想法。总之，我们的结果将 DWRL 定位为超越可验证任务的推理增强偏好学习的通用框架。</li>
</ul>

<h3>Title: DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Weijie Wang, Jiagang Zhu, Zeyu Zhang, Xiaofeng Wang, Zheng Zhu, Guosheng Zhao, Chaojun Ni, Haoxiao Wang, Guan Huang, Xinze Chen, Yukun Zhou, Wenkang Qin, Duochao Shi, Haoyun Li, Guanghong Jia, Jiwen Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15264">https://arxiv.org/abs/2510.15264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15264">https://arxiv.org/pdf/2510.15264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15264]] DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion(https://arxiv.org/abs/2510.15264)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present DriveGen3D, a novel framework for generating high-quality and highly controllable dynamic 3D driving scenes that addresses critical limitations in existing methodologies. Current approaches to driving scene synthesis either suffer from prohibitive computational demands for extended temporal generation, focus exclusively on prolonged video synthesis without 3D representation, or restrict themselves to static single-scene reconstruction. Our work bridges this methodological gap by integrating accelerated long-term video generation with large-scale dynamic scene reconstruction through multimodal conditional control. DriveGen3D introduces a unified pipeline consisting of two specialized components: FastDrive-DiT, an efficient video diffusion transformer for high-resolution, temporally coherent video synthesis under text and Bird's-Eye-View (BEV) layout guidance; and FastRecon3D, a feed-forward reconstruction module that rapidly builds 3D Gaussian representations across time, ensuring spatial-temporal consistency. Together, these components enable real-time generation of extended driving videos (up to $424\times800$ at 12 FPS) and corresponding dynamic 3D scenes, achieving SSIM of 0.811 and PSNR of 22.84 on novel view synthesis, all while maintaining parameter efficiency.</li>
<li><strong>摘要：</strong>我们推出了 DriveGen3D，这是一种用于生成高质量且高度可控的动态 3D 驾驶场景的新颖框架，可解决现有方法中的关键限制。当前驱动场景合成的方法要么受到扩展时间生成的过高计算需求的困扰，要么专门专注于没有 3D 表示的长时间视频合成，要么仅限于静态单场景重建。我们的工作通过多模态条件控制将加速的长期视频生成与大规模动态场景重建相结合，从而弥补了这一方法上的差距。 DriveGen3D 引入了一个由两个专用组件组成的统一管道：FastDrive-DiT，一种高效的视频扩散变压器，用于在文本和鸟瞰图 (BEV) 布局指导下进行高分辨率、时间连贯的视频合成； FastRecon3D，一个前馈重建模块，可以跨时间快速构建 3D 高斯表示，确保时空一致性。这些组件共同支持实时生成扩展驾驶视频（12 FPS 时高达 424×800 美元）和相应的动态 3D 场景，在新颖的视图合成上实现 0.811 的 SSIM 和 22.84 的 PSNR，同时保持参数效率。</li>
</ul>

<h3>Title: Foundation Models for Scientific Discovery: From Paradigm Enhancement to Paradigm Transition</h3>
<ul>
<li><strong>Authors: </strong>Fan Liu, Jindong Han, Tengfei Lyu, Weijia Zhang, Zhe-Rui Yang, Lu Dai, Cancheng Liu, Hao Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15280">https://arxiv.org/abs/2510.15280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15280">https://arxiv.org/pdf/2510.15280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15280]] Foundation Models for Scientific Discovery: From Paradigm Enhancement to Paradigm Transition(https://arxiv.org/abs/2510.15280)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Foundation models (FMs), such as GPT-4 and AlphaFold, are reshaping the landscape of scientific research. Beyond accelerating tasks such as hypothesis generation, experimental design, and result interpretation, they prompt a more fundamental question: Are FMs merely enhancing existing scientific methodologies, or are they redefining the way science is conducted? In this paper, we argue that FMs are catalyzing a transition toward a new scientific paradigm. We introduce a three-stage framework to describe this evolution: (1) Meta-Scientific Integration, where FMs enhance workflows within traditional paradigms; (2) Hybrid Human-AI Co-Creation, where FMs become active collaborators in problem formulation, reasoning, and discovery; and (3) Autonomous Scientific Discovery, where FMs operate as independent agents capable of generating new scientific knowledge with minimal human intervention. Through this lens, we review current applications and emerging capabilities of FMs across existing scientific paradigms. We further identify risks and future directions for FM-enabled scientific discovery. This position paper aims to support the scientific community in understanding the transformative role of FMs and to foster reflection on the future of scientific discovery. Our project is available at this https URL.</li>
<li><strong>摘要：</strong>GPT-4 和 AlphaFold 等基础模型 (FM) 正在重塑科学研究的格局。除了加速假设生成、实验设计和结果解释等任务之外，它们还提出了一个更基本的问题：FM 仅仅是增强现有的科学方法，还是重新定义了科学的开展方式？在本文中，我们认为 FM 正在催化向新的科学范式的转变。我们引入一个三阶段框架来描述这一演变：（1）元科学集成，FM 增强传统范式内的工作流程； (2) 人类与人工智能混合共创，FM 成为问题表述、推理和发现的积极合作者； (3) 自主科学发现，其中 FM 作为独立代理运行，能够以最少的人为干预生成新的科学知识。通过这个视角，我们回顾了 FM 在现有科学范式中的当前应用和新兴功能。我们进一步确定基于 FM 的科学发现的风险和未来方向。本立场文件旨在支持科学界理解 FM 的变革作用，并促进对科学发现的未来的反思。我们的项目可以通过此 https URL 获取。</li>
</ul>

<h3>Title: Latent Diffusion Model without Variational Autoencoder</h3>
<ul>
<li><strong>Authors: </strong>Minglei Shi, Haolin Wang, Wenzhao Zheng, Ziyang Yuan, Xiaoshi Wu, Xintao Wang, Pengfei Wan, Jie Zhou, Jiwen Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15301">https://arxiv.org/abs/2510.15301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15301">https://arxiv.org/pdf/2510.15301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15301]] Latent Diffusion Model without Variational Autoencoder(https://arxiv.org/abs/2510.15301)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent progress in diffusion-based visual generation has largely relied on latent diffusion models with variational autoencoders (VAEs). While effective for high-fidelity synthesis, this VAE+diffusion paradigm suffers from limited training efficiency, slow inference, and poor transferability to broader vision tasks. These issues stem from a key limitation of VAE latent spaces: the lack of clear semantic separation and strong discriminative structure. Our analysis confirms that these properties are crucial not only for perception and understanding tasks, but also for the stable and efficient training of latent diffusion models. Motivated by this insight, we introduce SVG, a novel latent diffusion model without variational autoencoders, which leverages self-supervised representations for visual generation. SVG constructs a feature space with clear semantic discriminability by leveraging frozen DINO features, while a lightweight residual branch captures fine-grained details for high-fidelity reconstruction. Diffusion models are trained directly on this semantically structured latent space to facilitate more efficient learning. As a result, SVG enables accelerated diffusion training, supports few-step sampling, and improves generative quality. Experimental results further show that SVG preserves the semantic and discriminative capabilities of the underlying self-supervised representations, providing a principled pathway toward task-general, high-quality visual representations.</li>
<li><strong>摘要：</strong>基于扩散的视觉生成的最新进展很大程度上依赖于具有变分自动编码器（VAE）的潜在扩散模型。虽然这种 VAE+扩散范式对于高保真合成有效，但其训练效率有限、推理速度慢以及向更广泛的视觉任务的可迁移性较差。这些问题源于 VAE 潜在空间的一个关键限制：缺乏清晰的语义分离和强判别结构。我们的分析证实，这些属性不仅对于感知和理解任务至关重要，而且对于潜在扩散模型的稳定有效的训练也至关重要。受这种洞察力的启发，我们引入了 SVG，这是一种没有变分自动编码器的新型潜在扩散模型，它利用自监督表示来进行视觉生成。 SVG 通过利用冻结的 DINO 特征构建具有清晰语义辨别性的特征空间，而轻量级残差分支捕获细粒度细节以进行高保真重建。扩散模型直接在这个语义结构化的潜在空间上进行训练，以促进更有效的学习。因此，SVG 能够加速扩散训练，支持少步采样，并提高生成质量。实验结果进一步表明，SVG 保留了底层自我监督表示的语义和判别能力，为实现任务通用、高质量视觉表示提供了原则性途径。</li>
</ul>

<h3>Title: Sequence Modeling with Spectral Mean Flows</h3>
<ul>
<li><strong>Authors: </strong>Jinwoo Kim, Max Beier, Petar Bevanda, Nayun Kim, Seunghoon Hong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15366">https://arxiv.org/abs/2510.15366</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15366">https://arxiv.org/pdf/2510.15366</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15366]] Sequence Modeling with Spectral Mean Flows(https://arxiv.org/abs/2510.15366)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>A key question in sequence modeling with neural networks is how to represent and learn highly nonlinear and probabilistic state dynamics. Operator theory views such dynamics as linear maps on Hilbert spaces containing mean embedding vectors of distributions, offering an appealing but currently overlooked perspective. We propose a new approach to sequence modeling based on an operator-theoretic view of a hidden Markov model (HMM). Instead of materializing stochastic recurrence, we embed the full sequence distribution as a tensor in the product Hilbert space. A generative process is then defined as maximum mean discrepancy (MMD) gradient flow in the space of sequences. To overcome challenges with large tensors and slow sampling convergence, we introduce spectral mean flows, a novel tractable algorithm integrating two core concepts. First, we propose a new neural architecture by leveraging spectral decomposition of linear operators to derive a scalable tensor network decomposition of sequence mean embeddings. Second, we extend MMD gradient flows to time-dependent Hilbert spaces and connect them to flow matching via the continuity equation, enabling simulation-free learning and faster sampling. We demonstrate competitive results on a range of time-series modeling datasets. Code is available at this https URL.</li>
<li><strong>摘要：</strong>神经网络序列建模的一个关键问题是如何表示和学习高度非线性和概率状态动力学。算子理论将这种动态视为希尔伯特空间上的线性映射，其中包含分布的平均嵌入向量，提供了一个吸引人但目前被忽视的观点。我们提出了一种基于隐马尔可夫模型（HMM）的算子理论视图的序列建模新方法。我们没有具体化随机递归，而是将完整序列分布作为张量嵌入到乘积希尔伯特空间中。然后，生成过程被定义为序列空间中的最大平均差异（MMD）梯度流。为了克服大张量和采样收敛速度慢的挑战，我们引入了谱平均流，这是一种集成了两个核心概念的新型易处理算法。首先，我们提出了一种新的神经架构，通过利用线性算子的谱分解来导出序列均值嵌入的可扩展张量网络分解。其次，我们将 MMD 梯度流扩展到时间相关的希尔伯特空间，并通过连续性方程将它们连接到流匹配，从而实现免模拟学习和更快的采样。我们在一系列时间序列建模数据集上展示了具有竞争力的结果。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Towards Robust Zero-Shot Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Kexin Zheng, Lauriane Teyssier, Yinan Zheng, Yu Luo, Xiayuan Zhan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15382">https://arxiv.org/abs/2510.15382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15382">https://arxiv.org/pdf/2510.15382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15382]] Towards Robust Zero-Shot Reinforcement Learning(https://arxiv.org/abs/2510.15382)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The recent development of zero-shot reinforcement learning (RL) has opened a new avenue for learning pre-trained generalist policies that can adapt to arbitrary new tasks in a zero-shot manner. While the popular Forward-Backward representations (FB) and related methods have shown promise in zero-shot RL, we empirically found that their modeling lacks expressivity and that extrapolation errors caused by out-of-distribution (OOD) actions during offline learning sometimes lead to biased representations, ultimately resulting in suboptimal performance. To address these issues, we propose Behavior-REgularizEd Zero-shot RL with Expressivity enhancement (BREEZE), an upgraded FB-based framework that simultaneously enhances learning stability, policy extraction capability, and representation learning quality. BREEZE introduces behavioral regularization in zero-shot RL policy learning, transforming policy optimization into a stable in-sample learning paradigm. Additionally, BREEZE extracts the policy using a task-conditioned diffusion model, enabling the generation of high-quality and multimodal action distributions in zero-shot RL settings. Moreover, BREEZE employs expressive attention-based architectures for representation modeling to capture the complex relationships between environmental dynamics. Extensive experiments on ExORL and D4RL Kitchen demonstrate that BREEZE achieves the best or near-the-best performance while exhibiting superior robustness compared to prior offline zero-shot RL methods. The official implementation is available at: this https URL.</li>
<li><strong>摘要：</strong>零样本强化学习（RL）的最新发展为学习预训练的通才策略开辟了一条新途径，这些策略可以以零样本的方式适应任意新任务。虽然流行的前向-后向表示（FB）和相关方法在零样本强化学习中表现出了希望，但我们凭经验发现它们的模型缺乏表达性，并且离线学习期间由分布外（OOD）行为引起的外推错误有时会导致表示有偏差，最终导致性能不佳。为了解决这些问题，我们提出了具有表现力增强的Behavior-REgularizEd零样本强化学习（BREEZE），这是一种基于FB的升级框架，可同时增强学习稳定性、策略提取能力和表示学习质量。 BREEZE 在零样本强化学习策略学习中引入了行为正则化，将策略优化转变为稳定的样本内学习范式。此外，BREEZE 使用任务条件扩散模型提取策略，从而能够在零样本 RL 设置中生成高质量的多模态动作分布。此外，BREEZE 采用基于表达注意力的架构进行表示建模，以捕获环境动态之间的复杂关系。在 ExORL 和 D4RL Kitchen 上进行的大量实验表明，与之前的离线零样本 RL 方法相比，BREEZE 实现了最佳或接近最佳的性能，同时表现出卓越的鲁棒性。官方实现位于：此 https URL。</li>
</ul>

<h3>Title: Safe, Efficient, and Robust Reinforcement Learning for Ranking and Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Shashank Gupta</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15429">https://arxiv.org/abs/2510.15429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15429">https://arxiv.org/pdf/2510.15429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15429]] Safe, Efficient, and Robust Reinforcement Learning for Ranking and Diffusion Models(https://arxiv.org/abs/2510.15429)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>This dissertation investigates how reinforcement learning (RL) methods can be designed to be safe, sample-efficient, and robust. Framed through the unifying perspective of contextual-bandit RL, the work addresses two major application domains - ranking and recommendation, and text-to-image diffusion models. The first part of the thesis develops theory and algorithms for safe deployment in ranking systems. An exposure-based generalisation bound is derived, leading to a counterfactual risk-minimisation objective whose solution is guaranteed not to underperform the logging policy, even with sparse feedback. This guarantee is extended to doubly robust estimators, enabling safety even under adversarial or misspecified user models and offering practitioners explicit control over permissible utility loss. The second part turns to single-action bandits, where various off-policy estimators are unified within a baseline-correction framework. A closed-form optimal baseline is proposed and shown to minimise both evaluation and policy-gradient variance, thereby improving off-policy learning reliability. The final part examines the trade-offs between efficiency and effectiveness in generative RL. A systematic study of PPO and REINFORCE motivates the Leave-One-Out PPO (LOOP) algorithm, which combines multiple diffusion trajectories with a REINFORCE-style baseline inside PPO's clipped objective. LOOP achieves PPO-level sample efficiency while producing generations that align more faithfully with textual attributes.</li>
<li><strong>摘要：</strong>本论文研究了如何设计安全、样本高效且稳健的强化学习 (RL) 方法。该工作以上下文强盗强化学习的统一视角为框架，解决了两个主要应用领域——排名和推荐，以及文本到图像的扩散模型。论文的第一部分开发了排名系统中安全部署的理论和算法。推导出基于暴露的泛化界限，从而产生反事实的风险最小化目标，即使反馈稀疏，其解决方案也保证不会低于日志记录策略。这种保证扩展到双重稳健的估计器，即使在对抗性或错误指定的用户模型下也能保证安全，并为从业者提供对允许的效用损失的明确控制。第二部分转向单动作老虎机，其中各种离策略估计器统一在基线校正框架内。提出并证明了封闭形式的最佳基线可以最小化评估和策略梯度方差，从而提高离策略学习的可靠性。最后一部分研究生成强化学习中效率和有效性之间的权衡。对 PPO 和 REINFORCE 的系统研究激发了留一法 PPO (LOOP) 算法，该算法将多个扩散轨迹与 PPO 裁剪目标内的 REINFORCE 式基线相结合。 LOOP 实现了 PPO 级别的样本效率，同时生成更忠实于文本属性的生成。</li>
</ul>

<h3>Title: Particle Dynamics for Latent-Variable Energy-Based Models</h3>
<ul>
<li><strong>Authors: </strong>Shiqin Tang, Shuxin Zhuang, Rong Feng, Runsheng Yu, Hongzong Li, Youzhi Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15447">https://arxiv.org/abs/2510.15447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15447">https://arxiv.org/pdf/2510.15447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15447]] Particle Dynamics for Latent-Variable Energy-Based Models(https://arxiv.org/abs/2510.15447)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Latent-variable energy-based models (LVEBMs) assign a single normalized energy to joint pairs of observed data and latent variables, offering expressive generative modeling while capturing hidden structure. We recast maximum-likelihood training as a saddle problem over distributions on the latent and joint manifolds and view the inner updates as coupled Wasserstein gradient flows. The resulting algorithm alternates overdamped Langevin updates for a joint negative pool and for conditional latent particles with stochastic parameter ascent, requiring no discriminator or auxiliary networks. We prove existence and convergence under standard smoothness and dissipativity assumptions, with decay rates in KL divergence and Wasserstein-2 distance. The saddle-point view further yields an ELBO strictly tighter than bounds obtained with restricted amortized posteriors. Our method is evaluated on numerical approximations of physical systems and performs competitively against comparable approaches.</li>
<li><strong>摘要：</strong>基于潜变量能量的模型 (LVEBM) 将单个归一化能量分配给观测数据和潜变量的联合对，在捕获隐藏结构的同时提供富有表现力的生成模型。我们将最大似然训练重新定义为潜在流形和联合流形上分布的鞍问题，并将内部更新视为耦合的 Wasserstein 梯度流。由此产生的算法交替对联合负池和具有随机参数上升的条件潜在粒子进行过阻尼朗之万更新，不需要判别器或辅助网络。我们证明了在标准平滑度和耗散性假设下的存在性和收敛性，以及 KL 散度和 Wasserstein-2 距离的衰减率。鞍点视图进一步产生的 ELBO 比通过限制摊销后验获得的边界更严格。我们的方法是根据物理系统的数值近似进行评估的，并且与同类方法相比具有竞争力。</li>
</ul>

<h3>Title: Compressive Modeling and Visualization of Multivariate Scientific Data using Implicit Neural Representation</h3>
<ul>
<li><strong>Authors: </strong>Abhay Kumar Dwivedi, Shanu Saklani, Soumya Dutta</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15535">https://arxiv.org/abs/2510.15535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15535">https://arxiv.org/pdf/2510.15535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15535]] Compressive Modeling and Visualization of Multivariate Scientific Data using Implicit Neural Representation(https://arxiv.org/abs/2510.15535)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>The extensive adoption of Deep Neural Networks has led to their increased utilization in challenging scientific visualization tasks. Recent advancements in building compressed data models using implicit neural representations have shown promising results for tasks like spatiotemporal volume visualization and super-resolution. Inspired by these successes, we develop compressed neural representations for multivariate datasets containing tens to hundreds of variables. Our approach utilizes a single network to learn representations for all data variables simultaneously through parameter sharing. This allows us to achieve state-of-the-art data compression. Through comprehensive evaluations, we demonstrate superior performance in terms of reconstructed data quality, rendering and visualization quality, preservation of dependency information among variables, and storage efficiency.</li>
<li><strong>摘要：</strong>深度神经网络的广泛采用导致其在具有挑战性的科学可视化任务中的利用率不断提高。使用隐式神经表示构建压缩数据模型的最新进展已经在时空体积可视化和超分辨率等任务中显示出有希望的结果。受这些成功的启发，我们为包含数十到数百个变量的多元数据集开发了压缩神经表示。我们的方法利用单个网络通过参数共享同时学习所有数据变量的表示。这使我们能够实现最先进的数据压缩。通过综合评估，我们在重建数据质量、渲染和可视化质量、变量之间依赖信息的保存以及存储效率方面表现出了优越的性能。</li>
</ul>

<h3>Title: Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiaoming Zhu, Xu Huang, Qinghongbing Xie, Zhi Deng, Junsheng Yu, Yirui Guan, Zhongyuan Liu, Lin Zhu, Qijun Zhao, Ligang Liu, Long Zeng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15564">https://arxiv.org/abs/2510.15564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15564">https://arxiv.org/pdf/2510.15564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15564]] Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation(https://arxiv.org/abs/2510.15564)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generating artistic and coherent 3D scene layouts is crucial in digital content creation. Traditional optimization-based methods are often constrained by cumbersome manual rules, while deep generative models face challenges in producing content with richness and diversity. Furthermore, approaches that utilize large language models frequently lack robustness and fail to accurately capture complex spatial relationships. To address these challenges, this paper presents a novel vision-guided 3D layout generation system. We first construct a high-quality asset library containing 2,037 scene assets and 147 3D scene layouts. Subsequently, we employ an image generation model to expand prompt representations into images, fine-tuning it to align with our asset library. We then develop a robust image parsing module to recover the 3D layout of scenes based on visual semantics and geometric information. Finally, we optimize the scene layout using scene graphs and overall visual semantics to ensure logical coherence and alignment with the images. Extensive user testing demonstrates that our algorithm significantly outperforms existing methods in terms of layout richness and quality. The code and dataset will be available at this https URL.</li>
<li><strong>摘要：</strong>生成艺术且连贯的 3D 场景布局对于数字内容创作至关重要。传统的基于优化的方法往往受到繁琐的手动规则的限制，而深度生成模型在生成丰富性和多样性的内容方面面临挑战。此外，利用大型语言模型的方法通常缺乏鲁棒性，并且无法准确捕获复杂的空间关系。为了应对这些挑战，本文提出了一种新颖的视觉引导 3D 布局生成系统。我们首先构建了一个高质量的资源库，包含 2,037 个场景资源和 147 个 3D 场景布局。随后，我们采用图像生成模型将提示表示扩展为图像，并对其进行微调以与我们的资源库保持一致。然后，我们开发了一个强大的图像解析模块，以根据视觉语义和几何信息恢复场景的 3D 布局。最后，我们使用场景图和整体视觉语义优化场景布局，以确保逻辑连贯性和与图像的对齐。广泛的用户测试表明，我们的算法在布局丰富度和质量方面显着优于现有方法。代码和数据集将在此 https URL 中提供。</li>
</ul>

<h3>Title: Lightweight CycleGAN Models for Cross-Modality Image Transformation and Experimental Quality Assessment in Fluorescence Microscopy</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Soltaninezhad, Yashar Rouzbahani, Jhonatan Contreras, Rohan Chippalkatti, Daniel Kwaku Abankwa, Christian Eggeling, Thomas Bocklitz</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15579">https://arxiv.org/abs/2510.15579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15579">https://arxiv.org/pdf/2510.15579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15579]] Lightweight CycleGAN Models for Cross-Modality Image Transformation and Experimental Quality Assessment in Fluorescence Microscopy(https://arxiv.org/abs/2510.15579)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, quality assessment</a></li>
<li><strong>Abstract: </strong>Lightweight deep learning models offer substantial reductions in computational cost and environmental impact, making them crucial for scientific applications. We present a lightweight CycleGAN for modality transfer in fluorescence microscopy (confocal to super-resolution STED/deconvolved STED), addressing the common challenge of unpaired datasets. By replacing the traditional channel-doubling strategy in the U-Net-based generator with a fixed channel approach, we drastically reduce trainable parameters from 41.8 million to approximately nine thousand, achieving superior performance with faster training and lower memory usage. We also introduce the GAN as a diagnostic tool for experimental and labeling quality. When trained on high-quality images, the GAN learns the characteristics of optimal imaging; deviations between its generated outputs and new experimental images can reveal issues such as photobleaching, artifacts, or inaccurate labeling. This establishes the model as a practical tool for validating experimental accuracy and image fidelity in microscopy workflows.</li>
<li><strong>摘要：</strong>轻量级深度学习模型可大幅降低计算成本和环境影响，这使得它们对于科学应用至关重要。我们提出了一种轻量级的 CycleGAN，用于荧光显微镜中的模态转移（共焦到超分辨率 STED/去卷积 STED），解决了不配对数据集的常见挑战。通过用固定通道方法替换基于 U-Net 的生成器中的传统通道加倍策略，我们将可训练参数从 4180 万个大幅减少到大约 9000 个，通过更快的训练和更低的内存使用实现了卓越的性能。我们还引入 GAN 作为实验和标记质量的诊断工具。当使用高质量图像进行训练时，GAN 会学习最佳成像的特征；其生成的输出与新的实验图像之间的偏差可能会揭示诸如光漂白、伪影或不准确标记等问题。这使该模型成为验证显微镜工作流程中的实验准确性和图像保真度的实用工具。</li>
</ul>

<h3>Title: Lightweight Data-Free Denoising for Detail-Preserving Biomedical Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Tomáš Chobola, Julia A. Schnabel, Tingying Peng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15611">https://arxiv.org/abs/2510.15611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15611">https://arxiv.org/pdf/2510.15611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15611]] Lightweight Data-Free Denoising for Detail-Preserving Biomedical Image Restoration(https://arxiv.org/abs/2510.15611)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Current self-supervised denoising techniques achieve impressive results, yet their real-world application is frequently constrained by substantial computational and memory demands, necessitating a compromise between inference speed and reconstruction quality. In this paper, we present an ultra-lightweight model that addresses this challenge, achieving both fast denoising and high quality image restoration. Built upon the Noise2Noise training framework-which removes the reliance on clean reference images or explicit noise modeling-we introduce an innovative multistage denoising pipeline named Noise2Detail (N2D). During inference, this approach disrupts the spatial correlations of noise patterns to produce intermediate smooth structures, which are subsequently refined to recapture fine details directly from the noisy input. Extensive testing reveals that Noise2Detail surpasses existing dataset-free techniques in performance, while requiring only a fraction of the computational resources. This combination of efficiency, low computational cost, and data-free approach make it a valuable tool for biomedical imaging, overcoming the challenges of scarce clean training data-due to rare and complex imaging modalities-while enabling fast inference for practical use.</li>
<li><strong>摘要：</strong>当前的自监督去噪技术取得了令人印象深刻的结果，但它们的实际应用经常受到大量计算和内存需求的限制，因此需要在推理速度和重建质量之间进行折衷。在本文中，我们提出了一种超轻量级模型来解决这一挑战，实现快速去噪和高质量图像恢复。基于 Noise2Noise 训练框架（消除了对干净参考图像或显式噪声建模的依赖），我们引入了一种名为 Noise2Detail (N2D) 的创新多级去噪管道。在推理过程中，这种方法会破坏噪声模式的空间相关性，以产生中间平滑结构，随后对其进行细化，以直接从噪声输入中重新捕获精细细节。广泛的测试表明，Noise2Detail 在性能上超越了现有的无数据集技术，同时只需要一小部分计算资源。这种效率、低计算成本和无数据方法的结合使其成为生物医学成像的宝贵工具，克服了由于罕见且复杂的成像模式而导致的清洁训练数据稀缺的挑战，同时实现了实际应用的快速推理。</li>
</ul>

<h3>Title: GRATING: Low-Latency and Memory-Efficient Semantic Selection on Device</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Zhou, Chengliang Lin, Dingji Li, Mingkai Dong, Haibo Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15620">https://arxiv.org/abs/2510.15620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15620">https://arxiv.org/pdf/2510.15620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15620]] GRATING: Low-Latency and Memory-Efficient Semantic Selection on Device(https://arxiv.org/abs/2510.15620)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Semantic top-K selection with cross-encoder rerankers underpins of on-device AI services, such as retrieval-augmented generation, agent memory, and personalized recommendation. However, its latency and memory demands dominate end-to-end budgets on edge hardware. Revisiting the objective of top-K selection, we reveal that only relative rankings matter, not exact per-candidate scores. We further observe sequence-level sparsity: relative rankings stabilize early in intermediate layers, allowing pruning opportunities prior to completing full inference. Building on this insight, we propose monolithic forwarding and develop a training-free inference system, GRATING. By maintaining a global view of all candidates, it reduces latency through progressive cluster pruning. It also bounds peak memory usage by strategically overlapping I/O with computation via dual-layer sliding window and chunked execution. We evaluate GRATING against state-of-the-art baselines on rerankers from 0.6B to 8B parameters across Apple M2 and RTX 5070. GRATING consistently reduces latency by up to 89.0% and peak memory by up to 94.9% in microbenchmarks, without any loss in precision. Across three real-world on-device AI applications, GRATING lowers latency by 11.6%-51.0% and peak memory by 18.6%-77.8%, demonstrating substantial improvements in efficiency and deployability.</li>
<li><strong>摘要：</strong>具有跨编码器重排序功能的语义 top-K 选择是设备上 AI 服务的基础，例如检索增强生成、代理记忆和个性化推荐。然而，其延迟和内存需求主导了边缘硬件的端到端预算。重新审视前 K 选择的目标，我们发现只有相对排名才重要，而不是每个候选人的确切分数。我们进一步观察序列级稀疏性：相对排名在中间层的早期稳定，从而在完成完整推理之前提供修剪机会。基于这一见解，我们提出了整体转发并开发了一个免训练的推理系统 GRATING。通过维护所有候选者的全局视图，它可以通过渐进式集群修剪来减少延迟。它还通过双层滑动窗口和分块执行策略性地将 I/O 与计算重叠，从而限制峰值内存使用。我们根据 Apple M2 和 RTX 5070 上从 0.6B 到 8B 参数的重新排序器上的最先进基准来评估 GRATING。在微基准测试中，GRATING 始终将延迟降低高达 89.0%，将峰值内存降低高达 94.9%，而没有任何精度损失。在三个真实的设备端人工智能应用程序中，GRATING 将延迟降低了 11.6%-51.0%，将峰值内存降低了 18.6%-77.8%，显示出效率和可部署性的显着提高。</li>
</ul>

<h3>Title: CarBoN: Calibrated Best-of-N Sampling Improves Test-time Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yung-Chen Tang, Pin-Yu Chen, Andrea Cavallaro</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15674">https://arxiv.org/abs/2510.15674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15674">https://arxiv.org/pdf/2510.15674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15674]] CarBoN: Calibrated Best-of-N Sampling Improves Test-time Reasoning(https://arxiv.org/abs/2510.15674)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Allocating more computation during inference time (test-time scaling) improves language model performance, especially for reasoning tasks. However, popular methods like Best-of-$N$ sampling often show diminishing returns as $N$ increases. To address this inefficiency, we introduce a general test-time calibration framework that adaptively modifies the model toward high-reward reasoning paths, with theoretical guarantees of improving the lower bound of expected reward under finite sampling, all without large language model (LLM) retraining. Within this framework, we propose CarBoN (Calibrated Best-of-$N$), a two-phase method that first explores the solution space and then learns a calibration of the logits via an input-specific temperature $T$ and additive shift vector $\delta$, guiding generation toward more reliable reasoning. Experiments on MATH-500 and AIME-2024 show that CarBoN improves efficiency, with up to $4\times$ fewer rollouts to reach the same accuracy, while often achieving higher accuracy under fixed budgets. We also analyze the complementary roles of $T$ and $\delta$ in balancing output diversity and correctness, and demonstrate that the framework also generalizes to step-level sampling strategies such as beam search. For more information, please refer to our project page at this http URL.</li>
<li><strong>摘要：</strong>在推理时间（测试时间缩放）期间分配更多计算可以提高语言模型性能，特别是对于推理任务。然而，像 Best-of-$N$ 抽样这样的流行方法通常会随着 $N$ 的增加而显示出收益递减。为了解决这种低效率问题，我们引入了一种通用的测试时间校准框架，该框架可以自适应地将模型修改为高奖励推理路径，并从理论上保证在有限采样下提高预期奖励的下限，所有这些都不需要大型语言模型（LLM）再训练。在此框架内，我们提出了 CarBoN（校准最佳 $N$），这是一种两阶段方法，首先探索解决方案空间，然后通过输入特定温度 $T$ 和加性位移向量 $\delta$ 学习 logits 校准，指导生成更可靠的推理。 MATH-500 和 AIME-2024 上的实验表明，CarBoN 提高了效率，只需花费高达 4 倍的成本即可达到相同的精度，同时在固定预算下通常可以实现更高的精度。我们还分析了 $T$ 和 $\delta$ 在平衡输出多样性和正确性方面的互补作用，并证明该框架还可以推广到波束搜索等步进级采样策略。欲了解更多信息，请参阅我们的项目页面（http URL）。</li>
</ul>

<h3>Title: Unimedvl: Unifying Medical Multimodal Understanding And Generation Through Observation-Knowledge-Analysis</h3>
<ul>
<li><strong>Authors: </strong>Junzhi Ning, Wei Li, Cheng Tang, Jiashi Lin, Chenglong Ma, Chaoyang Zhang, Jiyao Liu, Ying Chen, Shujian Gao, Lihao Liu, Yuandong Pu, Huihui Xu, Chenhui Gou, Ziyan Huang, Yi Xin, Qi Qin, Zhongying Deng, Diping Song, Bin Fu, Guang Yang, Yuanfeng Ji, Tianbin Li, Yanzhou Su, Jin Ye, Shixiang Tang, Ming Hu, Junjun He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15710">https://arxiv.org/abs/2510.15710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15710">https://arxiv.org/pdf/2510.15710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15710]] Unimedvl: Unifying Medical Multimodal Understanding And Generation Through Observation-Knowledge-Analysis(https://arxiv.org/abs/2510.15710)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Medical diagnostic applications require models that can process multimodal medical inputs (images, patient histories, lab results) and generate diverse outputs including both textual reports and visual content (annotations, segmentation masks, and images). Despite this need, existing medical AI systems disrupt this unified process: medical image understanding models interpret images but cannot generate visual outputs, while medical image generation models synthesize images but cannot provide textual explanations. This leads to gaps in data representation, feature integration, and task-level multimodal capabilities. To this end, we propose a multi-level framework that draws inspiration from diagnostic workflows through the Observation-Knowledge-Analysis (OKA) paradigm. Specifically, at the observation level, we construct UniMed-5M, a dataset comprising over 5.6M samples that reformat diverse unimodal data into multimodal pairs for foundational observation. At the knowledge level, we propose Progressive Curriculum Learning that systematically introduces medical multimodal knowledge. At the analysis level, we introduce UniMedVL, the first medical unified multimodal model for the simultaneous analysis of image understanding and generation tasks within a single architecture. UniMedVL achieves superior performance on five medical image understanding benchmarks, while matching specialized models in generation quality across eight medical imaging modalities. Crucially, our unified architecture enables bidirectional knowledge sharing: generation tasks enhance visual understanding features, demonstrating that integrating traditionally separate capabilities within a single medical framework unlocks improvements across diverse medical vision-language tasks. Code is available at this https URL.</li>
<li><strong>摘要：</strong>医疗诊断应用程序需要能够处理多模式医疗输入（图像、患者病史、实验室结果）并生成不同输出的模型，包括文本报告和视觉内容（注释、分割掩模和图像）。尽管有这种需求，现有的医疗人工智能系统却破坏了这个统一的过程：医学图像理解模型解释图像但无法生成视觉输出，而医学图像生成模型合成图像但无法提供文本解释。这导致了数据表示、特征集成和任务级多模式能力方面的差距。为此，我们提出了一个多层次框架，通过观察-知识-分析（OKA）范式从诊断工作流程中汲取灵感。具体来说，在观察层面，我们构建了 UniMed-5M，这是一个包含超过 560 万个样本的数据集，可将各种单峰数据重新格式化为多峰对以进行基础观察。在知识层面，我们提出渐进式课程学习，系统地引入医学多模态知识。在分析层面，我们引入了 UniMedVL，这是第一个医学统一多模态模型，用于在单一架构中同时分析图像理解和生成任务。 UniMedVL 在五种医学图像理解基准上实现了卓越的性能，同时在八种医学成像模式的生成质量方面与专业模型相匹配。至关重要的是，我们的统一架构实现了双向知识共享：生成任务增强了视觉理解功能，证明将传统上独立的功能集成到单个医疗框架中可以实现跨不同医学视觉语言任务的改进。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset</h3>
<ul>
<li><strong>Authors: </strong>Qingyan Bai, Qiuyu Wang, Hao Ouyang, Yue Yu, Hanlin Wang, Wen Wang, Ka Leong Cheng, Shuailei Ma, Yanhong Zeng, Zichen Liu, Yinghao Xu, Yujun Shen, Qifeng Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15742">https://arxiv.org/abs/2510.15742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15742">https://arxiv.org/pdf/2510.15742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15742]] Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset(https://arxiv.org/abs/2510.15742)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Instruction-based video editing promises to democratize content creation, yet its progress is severely hampered by the scarcity of large-scale, high-quality training data. We introduce Ditto, a holistic framework designed to tackle this fundamental challenge. At its heart, Ditto features a novel data generation pipeline that fuses the creative diversity of a leading image editor with an in-context video generator, overcoming the limited scope of existing models. To make this process viable, our framework resolves the prohibitive cost-quality trade-off by employing an efficient, distilled model architecture augmented by a temporal enhancer, which simultaneously reduces computational overhead and improves temporal coherence. Finally, to achieve full scalability, this entire pipeline is driven by an intelligent agent that crafts diverse instructions and rigorously filters the output, ensuring quality control at scale. Using this framework, we invested over 12,000 GPU-days to build Ditto-1M, a new dataset of one million high-fidelity video editing examples. We trained our model, Editto, on Ditto-1M with a curriculum learning strategy. The results demonstrate superior instruction-following ability and establish a new state-of-the-art in instruction-based video editing.</li>
<li><strong>摘要：</strong>基于指令的视频编辑有望使内容创作民主化，但其进展却因大规模、高质量训练数据的稀缺而受到严重阻碍。我们推出 Ditto，一个旨在应对这一基本挑战的整体框架。 Ditto 的核心是一个新颖的数据生成管道，它将领先图像编辑器的创意多样性与上下文视频生成器融合在一起，克服了现有模型的有限范围。为了使这个过程可行，我们的框架通过采用由时间增强器增强的高效、精炼的模型架构来解决令人望而却步的成本质量权衡，同时减少计算开销并提高时间一致性。最后，为了实现完全的可扩展性，整个管道由智能代理驱动，该智能代理可以制作不同的指令并严格过滤输出，从而确保大规模的质量控制。使用该框架，我们投入了超过 12,000 个 GPU 天来构建 Ditto-1M，这是一个包含 100 万个高保真视频编辑示例的新数据集。我们使用课程学习策略在 Ditto-1M 上训练我们的模型 Editto。结果展示了卓越的指令遵循能力，并在基于指令的视频编辑中建立了新的最先进技术。</li>
</ul>

<h3>Title: SEGA: A Stepwise Evolution Paradigm for Content-Aware Layout Generation with Design Prior</h3>
<ul>
<li><strong>Authors: </strong>Haoran Wang, Bo Zhao, Jinghui Wang, Hanzhang Wang, Huan Yang, Wei Ji, Hao Liu, Xinyan Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15749">https://arxiv.org/abs/2510.15749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15749">https://arxiv.org/pdf/2510.15749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15749]] SEGA: A Stepwise Evolution Paradigm for Content-Aware Layout Generation with Design Prior(https://arxiv.org/abs/2510.15749)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In this paper, we study the content-aware layout generation problem, which aims to automatically generate layouts that are harmonious with a given background image. Existing methods usually deal with this task with a single-step reasoning framework. The lack of a feedback-based self-correction mechanism leads to their failure rates significantly increasing when faced with complex element layout planning. To address this challenge, we introduce SEGA, a novel Stepwise Evolution Paradigm for Content-Aware Layout Generation. Inspired by the systematic mode of human thinking, SEGA employs a hierarchical reasoning framework with a coarse-to-fine strategy: first, a coarse-level module roughly estimates the layout planning results; then, another refining module performs fine-level reasoning regarding the coarse planning results. Furthermore, we incorporate layout design principles as prior knowledge into the model to enhance its layout planning ability. Besides, we present GenPoster-100K that is a new large-scale poster dataset with rich meta-information annotation. The experiments demonstrate the effectiveness of our approach by achieving the state-of-the-art results on multiple benchmark datasets. Our project page is at: this https URL</li>
<li><strong>摘要：</strong>在本文中，我们研究内容感知布局生成问题，旨在自动生成与给定背景图像和谐的布局。现有方法通常使用单步推理框架来处理此任务。缺乏基于反馈的自我修正机制导致在面对复杂的元素布局规划时失败率显着增加。为了应对这一挑战，我们引入了 SEGA，一种用于内容感知布局生成的新颖的逐步进化范式。受人类思维系统模式的启发，SEGA采用了由粗到细策略的分层推理框架：首先，粗级模块粗略估计布局规划结果；然后，另一个细化模块对粗略规划结果进行精细推理。此外，我们将布局设计原理作为先验知识纳入模型中，以增强其布局规划能力。此外，我们还提出了 GenPoster-100K，这是一个具有丰富元信息注释的新型大型海报数据集。实验通过在多个基准数据集上取得最先进的结果，证明了我们方法的有效性。我们的项目页面位于：此 https URL</li>
</ul>

<h3>Title: NDM: A Noise-driven Detection and Mitigation Framework against Implicit Sexual Intentions in Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Yitong Sun, Yao Huang, Ruochen Zhang, Huanran Chen, Shouwei Ruan, Ranjie Duan, Xingxing Wei</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15752">https://arxiv.org/abs/2510.15752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15752">https://arxiv.org/pdf/2510.15752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15752]] NDM: A Noise-driven Detection and Mitigation Framework against Implicit Sexual Intentions in Text-to-Image Generation(https://arxiv.org/abs/2510.15752)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Despite the impressive generative capabilities of text-to-image (T2I) diffusion models, they remain vulnerable to generating inappropriate content, especially when confronted with implicit sexual prompts. Unlike explicit harmful prompts, these subtle cues, often disguised as seemingly benign terms, can unexpectedly trigger sexual content due to underlying model biases, raising significant ethical concerns. However, existing detection methods are primarily designed to identify explicit sexual content and therefore struggle to detect these implicit cues. Fine-tuning approaches, while effective to some extent, risk degrading the model's generative quality, creating an undesirable trade-off. To address this, we propose NDM, the first noise-driven detection and mitigation framework, which could detect and mitigate implicit malicious intention in T2I generation while preserving the model's original generative capabilities. Specifically, we introduce two key innovations: first, we leverage the separability of early-stage predicted noise to develop a noise-based detection method that could identify malicious content with high accuracy and efficiency; second, we propose a noise-enhanced adaptive negative guidance mechanism that could optimize the initial noise by suppressing the prominent region's attention, thereby enhancing the effectiveness of adaptive negative guidance for sexual mitigation. Experimentally, we validate NDM on both natural and adversarial datasets, demonstrating its superior performance over existing SOTA methods, including SLD, UCE, and RECE, etc. Code and resources are available at this https URL.</li>
<li><strong>摘要：</strong>尽管文本到图像（T2I）扩散模型的生成能力令人印象深刻，但它们仍然容易生成不适当的内容，特别是在面对隐含的性提示时。与明确的有害提示不同，这些微妙的提示通常伪装成看似良性的术语，但由于潜在的模型偏差，可能会意外地触发性内容，引发重大的道德问题。然而，现有的检测方法主要旨在识别明确的性内容，因此很难检测这些隐含的线索。微调方法虽然在一定程度上有效，但可能会降低模型的生成质量，从而产生不良的权衡。为了解决这个问题，我们提出了 NDM，这是第一个噪声驱动的检测和缓解框架，它可以检测和缓解 T2I 生成中隐含的恶意意图，同时保留模型的原始生成能力。具体来说，我们引入了两个关键创新：首先，我们利用早期预测噪声的可分离性，开发了一种基于噪声的检测方法，可以高精度和高效地识别恶意内容；其次，我们提出了一种噪声增强的自适应负引导机制，可以通过抑制突出区域的注意力来优化初始噪声，从而增强自适应负引导对性缓解的有效性。通过实验，我们在自然数据集和对抗数据集上验证了 NDM，证明了其优于现有 SOTA 方法（包括 SLD、UCE 和 RECE 等）的性能。代码和资源可在此 https URL 中获取。</li>
</ul>

<h3>Title: Controlling the image generation process with parametric activation functions</h3>
<ul>
<li><strong>Authors: </strong>Ilia Pavlov</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15778">https://arxiv.org/abs/2510.15778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15778">https://arxiv.org/pdf/2510.15778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15778]] Controlling the image generation process with parametric activation functions(https://arxiv.org/abs/2510.15778)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>As image generative models continue to increase not only in their fidelity but also in their ubiquity the development of tools that leverage direct interaction with their internal mechanisms in an interpretable way has received little attention In this work we introduce a system that allows users to develop a better understanding of the model through interaction and experimentation By giving users the ability to replace activation functions of a generative network with parametric ones and a way to set the parameters of these functions we introduce an alternative approach to control the networks output We demonstrate the use of our method on StyleGAN2 and BigGAN networks trained on FFHQ and ImageNet respectively.</li>
<li><strong>摘要：</strong>随着图像生成模型不仅保真度不断提高，而且普遍存在，以可解释的方式利用与其内部机制直接交互的工具的开发很少受到关注。在这项工作中，我们介绍了一个系统，允许用户通过交互和实验更好地理解模型。通过让用户能够用参数化的激活函数替换生成网络的激活函数，以及设置这些函数的参数的方法，我们引入了一种替代方法来控制 网络输出 我们演示了我们的方法在分别在 FFHQ 和 ImageNet 上训练的 StyleGAN2 和 BigGAN 网络上的使用。</li>
</ul>

<h3>Title: ReCon: Region-Controllable Data Augmentation with Rectification and Alignment for Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Haowei Zhu, Tianxiang Pan, Rui Qin, Jun-Hai Yong, Bin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15783">https://arxiv.org/abs/2510.15783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15783">https://arxiv.org/pdf/2510.15783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15783]] ReCon: Region-Controllable Data Augmentation with Rectification and Alignment for Object Detection(https://arxiv.org/abs/2510.15783)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The scale and quality of datasets are crucial for training robust perception models. However, obtaining large-scale annotated data is both costly and time-consuming. Generative models have emerged as a powerful tool for data augmentation by synthesizing samples that adhere to desired distributions. However, current generative approaches often rely on complex post-processing or extensive fine-tuning on massive datasets to achieve satisfactory results, and they remain prone to content-position mismatches and semantic leakage. To overcome these limitations, we introduce ReCon, a novel augmentation framework that enhances the capacity of structure-controllable generative models for object detection. ReCon integrates region-guided rectification into the diffusion sampling process, using feedback from a pre-trained perception model to rectify misgenerated regions within diffusion sampling process. We further propose region-aligned cross-attention to enforce spatial-semantic alignment between image regions and their textual cues, thereby improving both semantic consistency and overall image fidelity. Extensive experiments demonstrate that ReCon substantially improve the quality and trainability of generated data, achieving consistent performance gains across various datasets, backbone architectures, and data scales. Our code is available at this https URL .</li>
<li><strong>摘要：</strong>数据集的规模和质量对于训练稳健的感知模型至关重要。然而，获取大规模注释数据既昂贵又耗时。生成模型已成为通过合成符合所需分布的样本来增强数据的强大工具。然而，当前的生成方法通常依赖于复杂的后处理或对大量数据集的广泛微调来获得满意的结果，并且它们仍然容易出现内容位置不匹配和语义泄漏。为了克服这些限制，我们引入了 ReCon，这是一种新颖的增强框架，可以增强结构可控生成模型的对象检测能力。 ReCon 将区域引导校正集成到扩散采样过程中，使用预先训练的感知模型的反馈来校正扩散采样过程中错误生成的区域。我们进一步提出区域对齐交叉注意来强制图像区域及其文本线索之间的空间语义对齐，从而提高语义一致性和整体图像保真度。大量实验表明，ReCon 显着提高了生成数据的质量和可训练性，在各种数据集、骨干架构和数据规模上实现了一致的性能增益。我们的代码可在此 https URL 获取。</li>
</ul>

<h3>Title: AB-UPT for Automotive and Aerospace Applications</h3>
<ul>
<li><strong>Authors: </strong>Benedikt Alkin, Richard Kurle, Louis Serrano, Dennis Just, Johannes Brandstetter</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15808">https://arxiv.org/abs/2510.15808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15808">https://arxiv.org/pdf/2510.15808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15808]] AB-UPT for Automotive and Aerospace Applications(https://arxiv.org/abs/2510.15808)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The recently proposed Anchored-Branched Universal Physics Transformers (AB-UPT) shows strong capabilities to replicate automotive computational fluid dynamics simulations requiring orders of magnitudes less compute than traditional numerical solvers. In this technical report, we add two new datasets to the body of empirically evaluated use-cases of AB-UPT, combining high-quality data generation with state-of-the-art neural surrogates. Both datasets were generated with the Luminary Cloud platform containing automotives (SHIFT-SUV) and aircrafts (SHIFT-Wing). We start by detailing the data generation. Next, we show favorable performances of AB-UPT against previous state-of-the-art transformer-based baselines on both datasets, followed by extensive qualitative and quantitative evaluations of our best AB-UPT model. AB-UPT shows strong performances across the board. Notably, it obtains near perfect prediction of integrated aerodynamic forces within seconds from a simple isotopically tesselate geometry representation and is trainable within a day on a single GPU, paving the way for industry-scale applications.</li>
<li><strong>摘要：</strong>最近提出的锚定分支通用物理变压器（AB-UPT）显示出复制汽车计算流体动力学模拟的强大能力，所需的计算量比传统数值求解器少几个数量级。在这份技术报告中，我们在 AB-UPT 的实证评估用例主体中添加了两个新数据集，将高质量数据生成与最先进的神经代理相结合。这两个数据集都是通过包含汽车 (SHIFT-SUV) 和飞机 (SHIFT-Wing) 的 Luminary Cloud 平台生成的。我们首先详细介绍数据生成。接下来，我们在两个数据集上展示了 AB-UPT 相对于之前最先进的基于 Transformer 的基线的良好性能，然后对我们的最佳 AB-UPT 模型进行了广泛的定性和定量评估。 AB-UPT 全面展现出强劲的表现。值得注意的是，它可以在几秒钟内从简单的同位素镶嵌几何表示中获得近乎完美的综合空气动力预测，并且可以在一天内在单个 GPU 上进行训练，为行业规模的应用铺平了道路。</li>
</ul>

<h3>Title: VISTA: A Test-Time Self-Improving Video Generation Agent</h3>
<ul>
<li><strong>Authors: </strong>Do Xuan Long, Xingchen Wan, Hootan Nakhost, Chen-Yu Lee, Tomas Pfister, Sercan Ö. Arık</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15831">https://arxiv.org/abs/2510.15831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15831">https://arxiv.org/pdf/2510.15831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15831]] VISTA: A Test-Time Self-Improving Video Generation Agent(https://arxiv.org/abs/2510.15831)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite rapid advances in text-to-video synthesis, generated video quality remains critically dependent on precise user prompts. Existing test-time optimization methods, successful in other domains, struggle with the multi-faceted nature of video. In this work, we introduce VISTA (Video Iterative Self-improvemenT Agent), a novel multi-agent system that autonomously improves video generation through refining prompts in an iterative loop. VISTA first decomposes a user idea into a structured temporal plan. After generation, the best video is identified through a robust pairwise tournament. This winning video is then critiqued by a trio of specialized agents focusing on visual, audio, and contextual fidelity. Finally, a reasoning agent synthesizes this feedback to introspectively rewrite and enhance the prompt for the next generation cycle. Experiments on single- and multi-scene video generation scenarios show that while prior methods yield inconsistent gains, VISTA consistently improves video quality and alignment with user intent, achieving up to 60% pairwise win rate against state-of-the-art baselines. Human evaluators concur, preferring VISTA outputs in 66.4% of comparisons.</li>
<li><strong>摘要：</strong>尽管文本到视频合成技术取得了快速进步，但生成的视频质量仍然严重依赖于精确的用户提示。现有的测试时优化方法在其他领域取得了成功，但与视频的多面性相矛盾。在这项工作中，我们介绍了 VISTA（视频迭代自我改进代理），这是一种新颖的多代理系统，可以通过迭代循环中的细化提示来自主改进视频生成。 VISTA 首先将用户想法分解为结构化的时间计划。生成后，通过强大的配对锦标赛来确定最佳视频。然后，三位专门针对视觉、音频和上下文保真度的专业代理人对这段获奖视频进行了评论。最后，推理代理综合此反馈，以内省方式重写和增强下一代循环的提示。单场景和多场景视频生成场景的实验表明，虽然之前的方法产生的增​​益不一致，但 VISTA 始终如一地提高视频质量并与用户意图保持一致，与最先进的基线相比，实现了高达 60% 的成对获胜率。人类评估者同意，在 66.4% 的比较中更喜欢 VISTA 输出。</li>
</ul>

<h3>Title: 3DPR: Single Image 3D Portrait Relight using Generative Priors</h3>
<ul>
<li><strong>Authors: </strong>Pramod Rao, Abhimitra Meka, Xilong Zhou, Gereon Fox, Mallikarjun B R, Fangneng Zhan, Tim Weyrich, Bernd Bickel, Hanspeter Pfister, Wojciech Matusik, Thabo Beeler, Mohamed Elgharib, Marc Habermann, Christian Theobalt</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15846">https://arxiv.org/abs/2510.15846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15846">https://arxiv.org/pdf/2510.15846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15846]] 3DPR: Single Image 3D Portrait Relight using Generative Priors(https://arxiv.org/abs/2510.15846)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Rendering novel, relit views of a human head, given a monocular portrait image as input, is an inherently underconstrained problem. The traditional graphics solution is to explicitly decompose the input image into geometry, material and lighting via differentiable rendering; but this is constrained by the multiple assumptions and approximations of the underlying models and parameterizations of these scene components. We propose 3DPR, an image-based relighting model that leverages generative priors learnt from multi-view One-Light-at-A-Time (OLAT) images captured in a light stage. We introduce a new diverse and large-scale multi-view 4K OLAT dataset of 139 subjects to learn a high-quality prior over the distribution of high-frequency face reflectance. We leverage the latent space of a pre-trained generative head model that provides a rich prior over face geometry learnt from in-the-wild image datasets. The input portrait is first embedded in the latent manifold of such a model through an encoder-based inversion process. Then a novel triplane-based reflectance network trained on our lightstage data is used to synthesize high-fidelity OLAT images to enable image-based relighting. Our reflectance network operates in the latent space of the generative head model, crucially enabling a relatively small number of lightstage images to train the reflectance model. Combining the generated OLATs according to a given HDRI environment maps yields physically accurate environmental relighting results. Through quantitative and qualitative evaluations, we demonstrate that 3DPR outperforms previous methods, particularly in preserving identity and in capturing lighting effects such as specularities, self-shadows, and subsurface scattering. Project Page: this https URL</li>
<li><strong>摘要：</strong>在给定单眼肖像图像作为输入的情况下，渲染新颖的、重新照亮的人头视图本质上是一个受约束不足的问题。传统的图形解决方案是通过可微分渲染将输入图像显式分解为几何体、材质和光照；但这受到底层模型的多重假设和近似以及这些场景组件的参数化的限制。我们提出了 3DPR，这是一种基于图像的重新照明模型，它利用从光阶段捕获的多视图一次一光 (OLAT) 图像中学习到的生成先验。我们引入了一个包含 139 个受试者的新的多样化、大规模多视图 4K OLAT 数据集，以学习高频人脸反射率分布的高质量先验。我们利用预训练的生成头部模型的潜在空间，该模型提供了从野外图像数据集中学习到的丰富的先验脸部几何形状。输入肖像首先通过基于编码器的反演过程嵌入到此类模型的潜在流形中。然后，使用在 LightStage 数据上训练的新颖的基于三平面的反射网络来合成高保真 OLAT 图像，以实现基于图像的重新照明。我们的反射网络在生成头部模型的潜在空间中运行，至关重要的是能够使用相对少量的光阶段图像来训练反射模型。根据给定的 HDRI 环境图组合生成的 OLAT 可产生物理上准确的环境重新照明结果。通过定量和定性评估，我们证明 3DPR 优于以前的方法，特别是在保留身份和捕捉光照效果（例如镜面反射、自阴影和次表面散射）方面。项目页面：此 https URL</li>
</ul>

<h3>Title: BLIP3o-NEXT: Next Frontier of Native Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiuhai Chen, Le Xue, Zhiyang Xu, Xichen Pan, Shusheng Yang, Can Qin, An Yan, Honglu Zhou, Zeyuan Chen, Lifu Huang, Tianyi Zhou, Junnan Li, Silvio Savarese, Caiming Xiong, Ran Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15857">https://arxiv.org/abs/2510.15857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15857">https://arxiv.org/pdf/2510.15857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15857]] BLIP3o-NEXT: Next Frontier of Native Image Generation(https://arxiv.org/abs/2510.15857)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3 series that advances the next frontier of native image generation. BLIP3o-NEXT unifies text-to-image generation and image editing within a single architecture, demonstrating strong image generation and image editing capabilities. In developing the state-of-the-art native image generation model, we identify four key insights: (1) Most architectural choices yield comparable performance; an architecture can be deemed effective provided it scales efficiently and supports fast inference; (2) The successful application of reinforcement learning can further push the frontier of native image generation; (3) Image editing still remains a challenging task, yet instruction following and the consistency between generated and reference images can be significantly enhanced through post-training and data engine; (4) Data quality and scale continue to be decisive factors that determine the upper bound of model performance. Building upon these insights, BLIP3o-NEXT leverages an Autoregressive + Diffusion architecture in which an autoregressive model first generates discrete image tokens conditioned on multimodal inputs, whose hidden states are then used as conditioning signals for a diffusion model to generate high-fidelity images. This architecture integrates the reasoning strength and instruction following of autoregressive models with the fine-detail rendering ability of diffusion models, achieving a new level of coherence and realism. Extensive evaluations of various text-to-image and image-editing benchmarks show that BLIP3o-NEXT achieves superior performance over existing models.</li>
<li><strong>摘要：</strong>我们推出 BLIP3o-NEXT，这是 BLIP3 系列中的一个完全开源的基础模型，它推动了原生图像生成的下一个前沿。 BLIP3o-NEXT 将文本到图像生成和图像编辑统一在一个架构内，展示了强大的图像生成和图像编辑功能。在开发最先进的本机图像生成模型时，我们确定了四个关键见解：（1）大多数架构选择都会产生可比的性能；只要架构能够有效扩展并支持快速推理，就可以被认为是有效的； （2）强化学习的成功应用可以进一步推动原生图像生成的前沿； （3）图像编辑仍然是一项具有挑战性的任务，但通过后训练和数据引擎可以显着增强指令跟踪以及生成图像和参考图像之间的一致性； （4）数据质量和规模仍然是决定模型性能上限的决定性因素。基于这些见解，BLIP3o-NEXT 利用自回归 + 扩散架构，其中自回归模型首先生成以多模态输入为条件的离散图像标记，然后将其隐藏状态用作扩散模型的条件信号以生成高保真图像。该架构将自回归模型的推理能力和指令跟踪能力与扩散模型的精细细节渲染能力相结合，实现了连贯性和真实性的新水平。对各种文本到图像和图像编辑基准的广泛评估表明，BLIP3o-NEXT 实现了优于现有模型的性能。</li>
</ul>

<h3>Title: Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite Imagery</h3>
<ul>
<li><strong>Authors: </strong>Jie-Ying Lee, Yi-Ruei Liu, Shr-Ruei Tsai, Wei-Cheng Chang, Chung-Ho Wu, Jiewen Chan, Zhenjun Zhao, Chieh Hubert Lin, Yu-Lun Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15869">https://arxiv.org/abs/2510.15869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15869">https://arxiv.org/pdf/2510.15869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15869]] Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite Imagery(https://arxiv.org/abs/2510.15869)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Synthesizing large-scale, explorable, and geometrically accurate 3D urban scenes is a challenging yet valuable task in providing immersive and embodied applications. The challenges lie in the lack of large-scale and high-quality real-world 3D scans for training generalizable generative models. In this paper, we take an alternative route to create large-scale 3D scenes by synergizing the readily available satellite imagery that supplies realistic coarse geometry and the open-domain diffusion model for creating high-quality close-up appearances. We propose \textbf{Skyfall-GS}, the first city-block scale 3D scene creation framework without costly 3D annotations, also featuring real-time, immersive 3D exploration. We tailor a curriculum-driven iterative refinement strategy to progressively enhance geometric completeness and photorealistic textures. Extensive experiments demonstrate that Skyfall-GS provides improved cross-view consistent geometry and more realistic textures compared to state-of-the-art approaches. Project page: this https URL</li>
<li><strong>摘要：</strong>合成大规模、可探索且几何精确的 3D 城市场景是提供沉浸式和具体化应用程序的一项具有挑战性但有价值的任务。挑战在于缺乏大规模、高质量的现实世界 3D 扫描来训练可推广的生成模型。在本文中，我们采用另一种方法来创建大规模 3D 场景，即通过协同提供逼真的粗糙几何形状的现成卫星图像和用于创建高质量特写外观的开放域扩散模型。我们提出 \textbf{Skyfall-GS}，这是第一个城市街区规模的 3D 场景创建框架，无需昂贵的 3D 注释，还具有实时、沉浸式 3D 探索功能。我们定制了课程驱动的迭代细化策略，以逐步增强几何完整性和逼真的纹理。大量实验表明，与最先进的方法相比，Skyfall-GS 提供了改进的跨视图一致几何形状和更真实的纹理。项目页面：此 https URL</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
