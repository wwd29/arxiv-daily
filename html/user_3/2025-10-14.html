<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-14</h1>
<h3>Title: TinyViT-Batten: Few-Shot Vision Transformer with Explainable Attention for Early Batten-Disease Detection on Pediatric MRI</h3>
<ul>
<li><strong>Authors: </strong>Khartik Uppalapati, Bora Yimenicioglu, Shakeel Abdulkareem, Adan Eftekhari, Bhavya Uppalapati, Viraj Kamath</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09649">https://arxiv.org/abs/2510.09649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09649">https://arxiv.org/pdf/2510.09649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09649]] TinyViT-Batten: Few-Shot Vision Transformer with Explainable Attention for Early Batten-Disease Detection on Pediatric MRI(https://arxiv.org/abs/2510.09649)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Batten disease (neuronal ceroid lipofuscinosis) is a rare pediatric neurodegenerative disorder whose early MRI signs are subtle and often missed. We propose TinyViT-Batten, a few-shot Vision Transformer (ViT) framework to detect early Batten disease from pediatric brain MRI with limited training cases. We distill a large teacher ViT into a 5 M-parameter TinyViT and fine-tune it using metric-based few-shot learning (prototypical loss with 5-shot episodes). Our model achieves high accuracy (approximately 91%) and area under ROC of at least 0.95 on a multi-site dataset of 79 genetically confirmed Batten-disease MRIs (27 CLN3 from the Hochstein natural-history study, 32 CLN2 from an international longitudinal cohort, 12 early-manifestation CLN2 cases reported by Cokal et al., and 8 public Radiopaedia scans) together with 90 age-matched controls, outperforming a 3D-ResNet and Swin-Tiny baseline. We further integrate Gradient-weighted Class Activation Mapping (Grad-CAM) to highlight disease-relevant brain regions, enabling explainable predictions. The model's small size and strong performance (sensitivity greater than 90%, specificity approximately 90%) demonstrates a practical AI solution for early Batten disease detection.</li>
<li><strong>摘要：</strong>Batten 病（神经元蜡样质脂褐质沉积症）是一种罕见的儿科神经退行性疾病，其早期 MRI 症状很微妙，经常被遗漏。我们提出了 TinyViT-Batten，这是一个少量的 Vision Transformer (ViT) 框架，用于通过有限的训练案例从儿科脑 MRI 中检测早期 Batten 疾病。我们将大型教师 ViT 提炼为 5 M 参数的 TinyViT，并使用基于度量的少样本学习（5 样本片段的典型损失）对其进行微调。我们的模型在 79 个基因证实的巴顿病 MRI 的多站点数据集上实现了高精度（约 91%）和至少 0.95 的 ROC 面积（27 个 CLN3 来自 Hochstein 自然历史研究，32 个 CLN2 来自国际纵向队列，12 个由 Cokal 等人报告的早期表现 CLN2 病例，以及 8 个公共放射百科全书扫描）与 90 个年龄匹配的对照，优于 3D-ResNet 和 Swin-Tiny 基线。我们进一步整合梯度加权类激活映射（Grad-CAM）来突出与疾病相关的大脑区域，从而实现可解释的预测。该模型体积小、性能强（灵敏度大于 90%，特异性约为 90%），展示了用于早期巴顿病检测的实用 AI 解决方案。</li>
</ul>

<h3>Title: Generative Models for Helmholtz Equation Solutions: A Dataset of Acoustic Materials</h3>
<ul>
<li><strong>Authors: </strong>Riccardo Fosco Gramaccioni, Christian Marinoni, Fabrizio Frezza, Aurelio Uncini, Danilo Comminiello</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09657">https://arxiv.org/abs/2510.09657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09657">https://arxiv.org/pdf/2510.09657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09657]] Generative Models for Helmholtz Equation Solutions: A Dataset of Acoustic Materials(https://arxiv.org/abs/2510.09657)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Accurate simulation of wave propagation in complex acoustic materials is crucial for applications in sound design, noise control, and material engineering. Traditional numerical solvers, such as finite element methods, are computationally expensive, especially when dealing with large-scale or real-time scenarios. In this work, we introduce a dataset of 31,000 acoustic materials, named HA30K, designed and simulated solving the Helmholtz equations. For each material, we provide the geometric configuration and the corresponding pressure field solution, enabling data-driven approaches to learn Helmholtz equation solutions. As a baseline, we explore a deep learning approach based on Stable Diffusion with ControlNet, a state-of-the-art model for image generation. Unlike classical solvers, our approach leverages GPU parallelization to process multiple simulations simultaneously, drastically reducing computation time. By representing solutions as images, we bypass the need for complex simulation software and explicit equation-solving. Additionally, the number of diffusion steps can be adjusted at inference time, balancing speed and quality. We aim to demonstrate that deep learning-based methods are particularly useful in early-stage research, where rapid exploration is more critical than absolute accuracy.</li>
<li><strong>摘要：</strong>复杂声学材料中波传播的精确模拟对于声音设计、噪声控制和材料工程中的应用至关重要。传统的数值求解器（例如有限元方法）的计算成本很高，尤其是在处理大规模或实时场景时。在这项工作中，我们引入了一个包含 31,000 种声学材料的数据集，名为 HA30K，设计并模拟求解亥姆霍兹方程。对于每种材料，我们提供几何配置和相应的压力场解，使数据驱动的方法能够学习亥姆霍兹方程解。作为基线，我们探索了一种基于稳定扩散和 ControlNet 的深度学习方法，ControlNet 是一种最先进的图像生成模型。与经典求解器不同，我们的方法利用 GPU 并行化同时处理多个模拟，从而大大减少计算时间。通过将解决方案表示为图像，我们无需复杂的模拟软件和显式方程求解。此外，可以在推理时调整扩散步骤的数量，以平衡速度和质量。我们的目标是证明基于深度学习的方法在早期研究中特别有用，在早期研究中，快速探索比绝对准确性更重要。</li>
</ul>

<h3>Title: Learning What Matters: Steering Diffusion via Spectrally Anisotropic Forward Noise</h3>
<ul>
<li><strong>Authors: </strong>Luca Scimeca, Thomas Jiralerspong, Berton Earnshaw, Jason Hartford, Yoshua Bengio</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09660">https://arxiv.org/abs/2510.09660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09660">https://arxiv.org/pdf/2510.09660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09660]] Learning What Matters: Steering Diffusion via Spectrally Anisotropic Forward Noise(https://arxiv.org/abs/2510.09660)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion Probabilistic Models (DPMs) have achieved strong generative performance, yet their inductive biases remain largely implicit. In this work, we aim to build inductive biases into the training and sampling of diffusion models to better accommodate the target distribution of the data to model. We introduce an anisotropic noise operator that shapes these biases by replacing the isotropic forward covariance with a structured, frequency-diagonal covariance. This operator unifies band-pass masks and power-law weightings, allowing us to emphasize or suppress designated frequency bands, while keeping the forward process Gaussian. We refer to this as spectrally anisotropic Gaussian diffusion (SAGD). In this work, we derive the score relation for anisotropic covariances and show that, under full support, the learned score converges to the true data score as $t\!\to\!0$, while anisotropy reshapes the probability-flow path from noise to data. Empirically, we show the induced anisotropy outperforms standard diffusion across several vision datasets, and enables selective omission: learning while ignoring known corruptions confined to specific bands. Together, these results demonstrate that carefully designed anisotropic forward noise provides a simple, yet principled, handle to tailor inductive bias in DPMs.</li>
<li><strong>摘要：</strong>扩散概率模型（DPM）已经取得了强大的生成性能，但它们的归纳偏差在很大程度上仍然是隐性的。在这项工作中，我们的目标是在扩散模型的训练和采样中建立归纳偏差，以更好地适应模型数据的目标分布。我们引入了一种各向异性噪声算子，它通过用结构化的频率对角协方差替换各向同性前向协方差来塑造这些偏差。该算子统一了带通掩模和幂律权重，使我们能够强调或抑制指定的频带，同时保持前向过程高斯分布。我们将其称为光谱各向异性高斯扩散 (SAGD)。在这项工作中，我们推导了各向异性协方差的得分关系，并表明，在完全支持下，学习得分收敛到真实数据得分 $t\!\to\!0$，而各向异性重塑了从噪声到数据的概率流路径。根据经验，我们表明诱导各向异性在多个视觉数据集中优于标准扩散，并实现选择性省略：学习同时忽略仅限于特定频带的已知损坏。总之，这些结果表明，精心设计的各向异性前向噪声提供了一种简单但有原则的处理方法来定制 DPM 中的电感偏置。</li>
</ul>

<h3>Title: NNDM: NN_UNet Diffusion Model for Brain Tumor Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Sashank Makanaboyina</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09681">https://arxiv.org/abs/2510.09681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09681">https://arxiv.org/pdf/2510.09681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09681]] NNDM: NN_UNet Diffusion Model for Brain Tumor Segmentation(https://arxiv.org/abs/2510.09681)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Accurate detection and segmentation of brain tumors in magnetic resonance imaging (MRI) are critical for effective diagnosis and treatment planning. Despite advances in convolutional neural networks (CNNs) such as U-Net, existing models often struggle with generalization, boundary precision, and limited data diversity. To address these challenges, we propose NNDM (NN\_UNet Diffusion Model)a hybrid framework that integrates the robust feature extraction of NN-UNet with the generative capabilities of diffusion probabilistic models. In our approach, the diffusion model progressively refines the segmentation masks generated by NN-UNet by learning the residual error distribution between predicted and ground-truth masks. This iterative denoising process enables the model to correct fine structural inconsistencies and enhance tumor boundary delineation. Experiments conducted on the BraTS 2021 datasets demonstrate that NNDM achieves superior performance compared to conventional U-Net and transformer-based baselines, yielding improvements in Dice coefficient and Hausdorff distance metrics. Moreover, the diffusion-guided refinement enhances robustness across modalities and tumor subregions. The proposed NNDM establishes a new direction for combining deterministic segmentation networks with stochastic diffusion models, advancing the state of the art in automated brain tumor analysis.</li>
<li><strong>摘要：</strong>磁共振成像（MRI）中脑肿瘤的准确检测和分割对于有效的诊断和治疗计划至关重要。尽管 U-Net 等卷积神经网络 (CNN) 取得了进步，但现有模型常常在泛化、边界精度和有限的数据多样性方面遇到困难。为了应对这些挑战，我们提出了 NNDM (NN\_UNet Diffusion Model) 一种混合框架，它将 NN-UNet 的鲁棒特征提取与扩散概率模型的生成能力相结合。在我们的方法中，扩散模型通过学习预测掩模和真实掩模之间的残差分布，逐步细化由 NN-UNet 生成的分割掩模。这种迭代去噪过程使模型能够纠正精细结构的不一致并增强肿瘤边界的描绘。在 BraTS 2021 数据集上进行的实验表明，与传统的 U-Net 和基于 Transformer 的基线相比，NNDM 实现了卓越的性能，从而在 Dice 系数和 Hausdorff 距离度量方面取得了改进。此外，扩散引导的细化增强了跨模式和肿瘤亚区域的鲁棒性。所提出的 NNDM 确立了将确定性分割网络与随机扩散模型相结合的新方向，从而推进了自动脑肿瘤分析的最新技术。</li>
</ul>

<h3>Title: Using LLMs to Directly Guess Conditional Expectations Can Improve Efficiency in Causal Estimation</h3>
<ul>
<li><strong>Authors: </strong>Chris Engh, P. M. Aronow</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09684">https://arxiv.org/abs/2510.09684</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09684">https://arxiv.org/pdf/2510.09684</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09684]] Using LLMs to Directly Guess Conditional Expectations Can Improve Efficiency in Causal Estimation(https://arxiv.org/abs/2510.09684)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We propose a simple yet effective use of LLM-powered AI tools to improve causal estimation. In double machine learning, the accuracy of causal estimates of the effect of a treatment on an outcome in the presence of a high-dimensional confounder depends on the performance of estimators of conditional expectation functions. We show that predictions made by generative models trained on historical data can be used to improve the performance of these estimators relative to approaches that solely rely on adjusting for embeddings extracted from these models. We argue that the historical knowledge and reasoning capacities associated with these generative models can help overcome curse-of-dimensionality problems in causal inference problems. We consider a case study using a small dataset of online jewelry auctions, and demonstrate that inclusion of LLM-generated guesses as predictors can improve efficiency in estimation.</li>
<li><strong>摘要：</strong>我们提出了一种简单而有效的使用法学硕士驱动的人工智能工具来改进因果估计。在双机器学习中，在存在高维混杂因素的情况下，治疗对结果影响的因果估计的准确性取决于条件期望函数估计器的性能。我们表明，相对于仅依赖于调整从这些模型中提取的嵌入的方法，通过历史数据训练的生成模型做出的预测可用于提高这些估计器的性能。我们认为，与这些生成模型相关的历史知识和推理能力可以帮助克服因果推理问题中的维数灾难问题。我们考虑使用在线珠宝拍卖的小型数据集进行案例研究，并证明将法学硕士生成的猜测作为预测变量可以提高估计效率。</li>
</ul>

<h3>Title: Kelp: A Streaming Safeguard for Large Models via Latent Dynamics-Guided Risk Detection</h3>
<ul>
<li><strong>Authors: </strong>Xiaodan Li, Mengjie Wu, Yao Zhu, Yunna Lv, YueFeng Chen, Cen Chen, Jianmei Guo, Hui Xue</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09694">https://arxiv.org/abs/2510.09694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09694">https://arxiv.org/pdf/2510.09694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09694]] Kelp: A Streaming Safeguard for Large Models via Latent Dynamics-Guided Risk Detection(https://arxiv.org/abs/2510.09694)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large models (LMs) are powerful content generators, yet their open-ended nature can also introduce potential risks, such as generating harmful or biased content. Existing guardrails mostly perform post-hoc detection that may expose unsafe content before it is caught, and the latency constraints further push them toward lightweight models, limiting detection accuracy. In this work, we propose Kelp, a novel plug-in framework that enables streaming risk detection within the LM generation pipeline. Kelp leverages intermediate LM hidden states through a Streaming Latent Dynamics Head (SLD), which models the temporal evolution of risk across the generated sequence for more accurate real-time risk detection. To ensure reliable streaming moderation in real applications, we introduce an Anchored Temporal Consistency (ATC) loss to enforce monotonic harm predictions by embedding a benign-then-harmful temporal prior. Besides, for a rigorous evaluation of streaming guardrails, we also present StreamGuardBench-a model-grounded benchmark featuring on-the-fly responses from each protected model, reflecting real-world streaming scenarios in both text and vision-language tasks. Across diverse models and datasets, Kelp consistently outperforms state-of-the-art post-hoc guardrails and prior plug-in probes (15.61% higher average F1), while using only 20M parameters and adding less than 0.5 ms of per-token latency.</li>
<li><strong>摘要：</strong>大型模型 (LM) 是强大的内容生成器，但其开放式性质也可能带来潜在风险，例如生成有害或有偏见的内容。现有的护栏大多执行事后检测，可能会在不安全内容被捕获之前将其暴露出来，而延迟限制进一步将它们推向轻量级模型，限制了检测的准确性。在这项工作中，我们提出了 Kelp，这是一种新颖的插件框架，可以在 LM 生成管道中实现流式风险检测。 Kelp 通过流式潜在动态头 (SLD) 利用中间 LM 隐藏状态，该头对生成序列中风险的时间演化进行建模，以实现更准确的实时风险检测。为了确保实际应用中可靠的流调节，我们引入了锚定时间一致性（ATC）损失，通过嵌入良性然后有害的时间先验来强制执行单调伤害预测。此外，为了对流护栏进行严格评估，我们还推出了 StreamGuardBench——一个基于模型的基准测试，具有每个受保护模型的即时响应，反映了文本和视觉语言任务中的真实流场景。在不同的模型和数据集中，Kelp 始终优于最先进的事后护栏和先前的插件探针（平均 F1 高出 15.61%），同时仅使用 20M 参数，并且每个令牌的延迟增加不到 0.5 毫秒。</li>
</ul>

<h3>Title: Operator Learning for Power Systems Simulation</h3>
<ul>
<li><strong>Authors: </strong>Matthew Schlegel, Matthew E. Taylor, Mostafa Farrokhabadi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09704">https://arxiv.org/abs/2510.09704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09704">https://arxiv.org/pdf/2510.09704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09704]] Operator Learning for Power Systems Simulation(https://arxiv.org/abs/2510.09704)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Time domain simulation, i.e., modeling the system's evolution over time, is a crucial tool for studying and enhancing power system stability and dynamic performance. However, these simulations become computationally intractable for renewable-penetrated grids, due to the small simulation time step required to capture renewable energy resources' ultra-fast dynamic phenomena in the range of 1-50 microseconds. This creates a critical need for solutions that are both fast and scalable, posing a major barrier for the stable integration of renewable energy resources and thus climate change mitigation. This paper explores operator learning, a family of machine learning methods that learn mappings between functions, as a surrogate model for these costly simulations. The paper investigates, for the first time, the fundamental concept of simulation time step-invariance, which enables models trained on coarse time steps to generalize to fine-resolution dynamics. Three operator learning methods are benchmarked on a simple test system that, while not incorporating practical complexities of renewable-penetrated grids, serves as a first proof-of-concept to demonstrate the viability of time step-invariance. Models are evaluated on (i) zero-shot super-resolution, where training is performed on a coarse simulation time step and inference is performed at super-resolution, and (ii) generalization between stable and unstable dynamic regimes. This work addresses a key challenge in the integration of renewable energy for the mitigation of climate change by benchmarking operator learning methods to model physical systems.</li>
<li><strong>摘要：</strong>时域仿真，即对系统随时间的演化进行建模，是研究和增强电力系统稳定性和动态性能的重要工具。然而，由于捕获可再生能源在 1-50 微秒范围内的超快动态现象所需的模拟时间步长较小，这些模拟对于可再生能源渗透电网来说在计算上变得难以处理。这就迫切需要快速且可扩展的解决方案，这对可再生能源资源的稳定整合以及减缓气候变化构成了主要障碍。本文探讨了算子学习，这是一系列学习函数之间映射的机器学习方法，作为这些昂贵的模拟的替代模型。该论文首次研究了模拟时间步长不变性的基本概念，该概念使得在粗时间步长上训练的模型能够推广到精细分辨率动力学。三种算子学习方法在一个简单的测试系统上进行了基准测试，虽然没有考虑可再生能源渗透电网的实际复杂性，但可以作为第一个概念验证来证明时间步不变性的可行性。模型在（i）零样本超分辨率上进行评估，其中在粗略模拟时间步长上进行训练，并在超分辨率下进行推理，以及（ii）稳定和不稳定动态状态之间的泛化。这项工作通过对物理系统建模的操作员学习方法进行基准测试，解决了整合可再生能源以缓解气候变化的关键挑战。</li>
</ul>

<h3>Title: Multi Camera Connected Vision System with Multi View Analytics: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Munsif, Waqas Ahmad, Amjid Ali, Mohib Ullah, Adnan Hussain, Sung Wook Baik</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09731">https://arxiv.org/abs/2510.09731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09731">https://arxiv.org/pdf/2510.09731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09731]] Multi Camera Connected Vision System with Multi View Analytics: A Comprehensive Survey(https://arxiv.org/abs/2510.09731)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Connected Vision Systems (CVS) are transforming a variety of applications, including autonomous vehicles, smart cities, surveillance, and human-robot interaction. These systems harness multi-view multi-camera (MVMC) data to provide enhanced situational awareness through the integration of MVMC tracking, re-identification (Re-ID), and action understanding (AU). However, deploying CVS in real-world, dynamic environments presents a number of challenges, particularly in addressing occlusions, diverse viewpoints, and environmental variability. Existing surveys have focused primarily on isolated tasks such as tracking, Re-ID, and AU, often neglecting their integration into a cohesive system. These reviews typically emphasize single-view setups, overlooking the complexities and opportunities provided by multi-camera collaboration and multi-view data analysis. To the best of our knowledge, this survey is the first to offer a comprehensive and integrated review of MVMC that unifies MVMC tracking, Re-ID, and AU into a single framework. We propose a unique taxonomy to better understand the critical components of CVS, dividing it into four key parts: MVMC tracking, Re-ID, AU, and combined methods. We systematically arrange and summarize the state-of-the-art datasets, methodologies, results, and evaluation metrics, providing a structured view of the field's progression. Furthermore, we identify and discuss the open research questions and challenges, along with emerging technologies such as lifelong learning, privacy, and federated learning, that need to be addressed for future advancements. The paper concludes by outlining key research directions for enhancing the robustness, efficiency, and adaptability of CVS in complex, real-world applications. We hope this survey will inspire innovative solutions and guide future research toward the next generation of intelligent and adaptive CVS.</li>
<li><strong>摘要：</strong>互联视觉系统 (CVS) 正在改变各种应用，包括自动驾驶汽车、智能城市、监控和人机交互。这些系统利用多视图多摄像头 (MVMC) 数据，通过集成 MVMC 跟踪、重新识别 (Re-ID) 和动作理解 (AU) 来提供增强的态势感知。然而，在现实世界的动态环境中部署 CVS 会带来许多挑战，特别是在解决遮挡、不同观点和环境变化方面。现有的调查主要集中在跟踪、重新识别和 AU 等孤立的任务上，往往忽略了将它们集成到一个有凝聚力的系统中。这些评论通常强调单视图设置，忽视多摄像机协作和多视图数据分析提供的复杂性和机会。据我们所知，这项调查首次对 MVMC 进行了全面综合的审查，将 MVMC 跟踪、Re-ID 和 AU 统一到一个框架中。我们提出了一种独特的分类法，以更好地理解 CVS 的关键组成部分，将其分为四个关键部分：MVMC 跟踪、Re-ID、AU 和组合方法。我们系统地整理和总结了最先进的数据集、方法、结果和评估指标，提供了该领域进展的结构化视图。此外，我们还确定并讨论了开放的研究问题和挑战，以及未来发展需要解决的新兴技术，例如终身学习、隐私和联合学习。本文最后概述了在复杂的现实应用中增强 CVS 的鲁棒性、效率和适应性的关键研究方向。我们希望这项调查能够激发创新解决方案，并指导未来对下一代智能和自适应 CVS 的研究。</li>
</ul>

<h3>Title: Large Language Models for Imbalanced Classification: Diversity makes the difference</h3>
<ul>
<li><strong>Authors: </strong>Dang Nguyen, Sunil Gupta, Kien Do, Thin Nguyen, Taylor Braund, Alexis Whitton, Svetha Venkatesh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09783">https://arxiv.org/abs/2510.09783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09783">https://arxiv.org/pdf/2510.09783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09783]] Large Language Models for Imbalanced Classification: Diversity makes the difference(https://arxiv.org/abs/2510.09783)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Oversampling is one of the most widely used approaches for addressing imbalanced classification. The core idea is to generate additional minority samples to rebalance the dataset. Most existing methods, such as SMOTE, require converting categorical variables into numerical vectors, which often leads to information loss. Recently, large language model (LLM)-based methods have been introduced to overcome this limitation. However, current LLM-based approaches typically generate minority samples with limited diversity, reducing robustness and generalizability in downstream classification tasks. To address this gap, we propose a novel LLM-based oversampling method designed to enhance diversity. First, we introduce a sampling strategy that conditions synthetic sample generation on both minority labels and features. Second, we develop a new permutation strategy for fine-tuning pre-trained LLMs. Third, we fine-tune the LLM not only on minority samples but also on interpolated samples to further enrich variability. Extensive experiments on 10 tabular datasets demonstrate that our method significantly outperforms eight SOTA baselines. The generated synthetic samples are both realistic and diverse. Moreover, we provide theoretical analysis through an entropy-based perspective, proving that our method encourages diversity in the generated samples.</li>
<li><strong>摘要：</strong>过采样是解决不平衡分类问题最广泛使用的方法之一。核心思想是生成额外的少数样本来重新平衡数据集。大多数现有方法（例如 SMOTE）需要将分类变量转换为数值向量，这通常会导致信息丢失。最近，基于大语言模型（LLM）的方法被引入来克服这一限制。然而，当前基于 LLM 的方法通常生成多样性有限的少数样本，从而降低了下游分类任务的稳健性和泛化性。为了解决这一差距，我们提出了一种新颖的基于 LLM 的过采样方法，旨在增强多样性。首先，我们引入了一种采样策略，该策略可以根据少数标签和特征生成合成样本。其次，我们开发了一种新的排列策略来微调预训练的法学硕士。第三，我们不仅对少数样本进行 LLM 微调，还对插值样本进行微调，以进一步丰富变异性。对 10 个表格数据集的广泛实验表明，我们的方法显着优于 8 个 SOTA 基线。生成的合成样本既真实又多样。此外，我们通过基于熵的角度提供理论分析，证明我们的方法鼓励生成样本的多样性。</li>
</ul>

<h3>Title: Combined Representation and Generation with Diffusive State Predictive Information Bottleneck</h3>
<ul>
<li><strong>Authors: </strong>Richard John, Yunrui Qiu, Lukas Herron, Pratyush Tiwary</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.stat-mech, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09784">https://arxiv.org/abs/2510.09784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09784">https://arxiv.org/pdf/2510.09784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09784]] Combined Representation and Generation with Diffusive State Predictive Information Bottleneck(https://arxiv.org/abs/2510.09784)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative modeling becomes increasingly data-intensive in high-dimensional spaces. In molecular science, where data collection is expensive and important events are rare, compression to lower-dimensional manifolds is especially important for various downstream tasks, including generation. We combine a time-lagged information bottleneck designed to characterize molecular important representations and a diffusion model in one joint training objective. The resulting protocol, which we term Diffusive State Predictive Information Bottleneck (D-SPIB), enables the balancing of representation learning and generation aims in one flexible architecture. Additionally, the model is capable of combining temperature information from different molecular simulation trajectories to learn a coherent and useful internal representation of thermodynamics. We benchmark D-SPIB on multiple molecular tasks and showcase its potential for exploring physical conditions outside the training set.</li>
<li><strong>摘要：</strong>在高维空间中，生成建模变得越来越数据密集。在分子科学中，数据收集成本高昂且重要事件很少，因此压缩到低维流形对于各种下游任务（包括生成）尤其重要。我们将旨在表征分子重要表征的时滞信息瓶颈和扩散模型结合到一个联合训练目标中。由此产生的协议，我们称之为扩散状态预测信息瓶颈（D-SPIB），能够在一个灵活的架构中平衡表示学习和生成目标。此外，该模型能够结合来自不同分子模拟轨迹的温度信息，以学习热力学的连贯且有用的内部表示。我们在多个分子任务上对 D-SPIB 进行基准测试，并展示其探索训练集之外的身体条件的潜力。</li>
</ul>

<h3>Title: CHUG: Crowdsourced User-Generated HDR Video Quality Dataset</h3>
<ul>
<li><strong>Authors: </strong>Shreshth Saini, Alan C. Bovik, Neil Birkbeck, Yilin Wang, Balu Adsumilli</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09879">https://arxiv.org/abs/2510.09879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09879">https://arxiv.org/pdf/2510.09879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09879]] CHUG: Crowdsourced User-Generated HDR Video Quality Dataset(https://arxiv.org/abs/2510.09879)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>High Dynamic Range (HDR) videos enhance visual experiences with superior brightness, contrast, and color depth. The surge of User-Generated Content (UGC) on platforms like YouTube and TikTok introduces unique challenges for HDR video quality assessment (VQA) due to diverse capture conditions, editing artifacts, and compression distortions. Existing HDR-VQA datasets primarily focus on professionally generated content (PGC), leaving a gap in understanding real-world UGC-HDR degradations. To address this, we introduce CHUG: Crowdsourced User-Generated HDR Video Quality Dataset, the first large-scale subjective study on UGC-HDR quality. CHUG comprises 856 UGC-HDR source videos, transcoded across multiple resolutions and bitrates to simulate real-world scenarios, totaling 5,992 videos. A large-scale study via Amazon Mechanical Turk collected 211,848 perceptual ratings. CHUG provides a benchmark for analyzing UGC-specific distortions in HDR videos. We anticipate CHUG will advance No-Reference (NR) HDR-VQA research by offering a large-scale, diverse, and real-world UGC dataset. The dataset is publicly available at: this https URL.</li>
<li><strong>摘要：</strong>高动态范围 (HDR) 视频通过卓越的亮度、对比度和颜色深度增强视觉体验。由于不同的捕捉条件、编辑伪影和压缩失真，YouTube 和 TikTok 等平台上用户生成内容 (UGC) 的激增给 HDR 视频质量评估 (VQA) 带来了独特的挑战。现有的 HDR-VQA 数据集主要关注专业生成的内容 (PGC)，在理解现实世界的 UGC-HDR 退化方面存在差距。为了解决这个问题，我们引入了 CHUG：众包用户生成的 HDR 视频质量数据集，这是第一个关于 UGC-HDR 质量的大规模主观研究。 CHUG 包含 856 个 UGC-HDR 源视频，经过多种分辨率和比特率转码以模拟真实场景，总共 5,992 个视频。通过 Amazon Mechanical Turk 进行的一项大规模研究收集了 211,848 个感知评分。 CHUG 提供了用于分析 HDR 视频中 UGC 特定失真的基准。我们预计 CHUG 将通过提供大规模、多样化且真实的 UGC 数据集来推进无参考 (NR) HDR-VQA 研究。该数据集可在以下网址公开获取：此 https URL。</li>
</ul>

<h3>Title: SpectralCA: Bi-Directional Cross-Attention for Next-Generation UAV Hyperspectral Vision</h3>
<ul>
<li><strong>Authors: </strong>D.V. Brovko</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09912">https://arxiv.org/abs/2510.09912</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09912">https://arxiv.org/pdf/2510.09912</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09912]] SpectralCA: Bi-Directional Cross-Attention for Next-Generation UAV Hyperspectral Vision(https://arxiv.org/abs/2510.09912)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The relevance of this research lies in the growing demand for unmanned aerial vehicles (UAVs) capable of operating reliably in complex environments where conventional navigation becomes unreliable due to interference, poor visibility, or camouflage. Hyperspectral imaging (HSI) provides unique opportunities for UAV-based computer vision by enabling fine-grained material recognition and object differentiation, which are critical for navigation, surveillance, agriculture, and environmental monitoring. The aim of this work is to develop a deep learning architecture integrating HSI into UAV perception for navigation, object detection, and terrain classification. Objectives include: reviewing existing HSI methods, designing a hybrid 2D/3D convolutional architecture with spectral-spatial cross-attention, training, and benchmarking. The methodology is based on the modification of the Mobile 3D Vision Transformer (MDvT) by introducing the proposed SpectralCA block. This block employs bi-directional cross-attention to fuse spectral and spatial features, enhancing accuracy while reducing parameters and inference time. Experimental evaluation was conducted on the WHU-Hi-HongHu dataset, with results assessed using Overall Accuracy, Average Accuracy, and the Kappa coefficient. The findings confirm that the proposed architecture improves UAV perception efficiency, enabling real-time operation for navigation, object recognition, and environmental monitoring tasks. Keywords: SpectralCA, deep learning, computer vision, hyperspectral imaging, unmanned aerial vehicle, object detection, semi-supervised learning.</li>
<li><strong>摘要：</strong>这项研究的意义在于对能够在复杂环境中可靠运行的无人机（UAV）的需求不断增长，在这些环境中，传统导航由于干扰、能见度差或伪装而变得不可靠。高光谱成像 (HSI) 通过实现细粒度材料识别和对象区分，为基于无人机的计算机视觉提供了独特的机会，这对于导航、监视、农业和环境监测至关重要。这项工作的目的是开发一种将 HSI 集成到无人机感知中的深度学习架构，用于导航、物体检测和地形分类。目标包括：回顾现有的 HSI 方法，设计具有光谱空间交叉注意力、训练和基准测试的混合 2D/3D 卷积架构。该方法基于通过引入提议的 SpectralCA 块对移动 3D 视觉转换器 (MDvT) 进行的修改。该块采用双向交叉注意力来融合光谱和空间特征，提高准确性，同时减少参数和推理时间。在WHU-Hi-HongHu数据集上进行实验评估，使用总体精度、平均精度和Kappa系数评估结果。研究结果证实，所提出的架构提高了无人机感知效率，实现导航、物体识别和环境监测任务的实时操作。关键词：SpectralCA、深度学习、计算机视觉、高光谱成像、无人机、目标检测、半监督学习。</li>
</ul>

<h3>Title: Augmenting generative models with biomedical knowledge graphs improves targeted drug discovery</h3>
<ul>
<li><strong>Authors: </strong>Aditya Malusare, Vineet Punyamoorty, Vaneet Aggarwal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09914">https://arxiv.org/abs/2510.09914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09914">https://arxiv.org/pdf/2510.09914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09914]] Augmenting generative models with biomedical knowledge graphs improves targeted drug discovery(https://arxiv.org/abs/2510.09914)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent breakthroughs in generative modeling have demonstrated remarkable capabilities in molecular generation, yet the integration of comprehensive biomedical knowledge into these models has remained an untapped frontier. In this study, we introduce K-DREAM (Knowledge-Driven Embedding-Augmented Model), a novel framework that leverages knowledge graphs to augment diffusion-based generative models for drug discovery. By embedding structured information from large-scale knowledge graphs, K-DREAM directs molecular generation toward candidates with higher biological relevance and therapeutic suitability. This integration ensures that the generated molecules are aligned with specific therapeutic targets, moving beyond traditional heuristic-driven approaches. In targeted drug design tasks, K-DREAM generates drug candidates with improved binding affinities and predicted efficacy, surpassing current state-of-the-art generative models. It also demonstrates flexibility by producing molecules designed for multiple targets, enabling applications to complex disease mechanisms. These results highlight the utility of knowledge-enhanced generative models in rational drug design and their relevance to practical therapeutic development.</li>
<li><strong>摘要：</strong>生成模型的最新突破已经证明了分子生成的卓越能力，但将全面的生物医学知识整合到这些模型中仍然是一个尚未开发的前沿。在这项研究中，我们介绍了 K-DREAM（知识驱动嵌入增强模型），这是一种利用知识图来增强基于扩散的药物发现生成模型的新颖框架。通过嵌入来自大规模知识图谱的结构化信息，K-DREAM 将分子生成引导至具有更高生物学相关性和治疗适用性的候选者。这种整合确保生成的分子与特定的治疗靶标相一致，超越了传统的启发式驱动方法。在靶向药物设计任务中，K-DREAM 生成的候选药物具有改进的结合亲和力和预测功效，超越了当前最先进的生成模型。它还通过生产针对多个靶点设计的分子来展示灵活性，从而能够应用于复杂的疾病机制。这些结果强调了知识增强生成模型在合理药物设计中的实用性及其与实际治疗开发的相关性。</li>
</ul>

<h3>Title: HeadsUp! High-Fidelity Portrait Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Renjie Li, Zihao Zhu, Xiaoyu Wang, Zhengzhong Tu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09924">https://arxiv.org/abs/2510.09924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09924">https://arxiv.org/pdf/2510.09924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09924]] HeadsUp! High-Fidelity Portrait Image Super-Resolution(https://arxiv.org/abs/2510.09924)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, super-resolution</a></li>
<li><strong>Abstract: </strong>Portrait pictures, which typically feature both human subjects and natural backgrounds, are one of the most prevalent forms of photography on social media. Existing image super-resolution (ISR) techniques generally focus either on generic real-world images or strictly aligned facial images (i.e., face super-resolution). In practice, separate models are blended to handle portrait photos: the face specialist model handles the face region, and the general model processes the rest. However, these blending approaches inevitably introduce blending or boundary artifacts around the facial regions due to different model training recipes, while human perception is particularly sensitive to facial fidelity. To overcome these limitations, we study the portrait image supersolution (PortraitISR) problem, and propose HeadsUp, a single-step diffusion model that is capable of seamlessly restoring and upscaling portrait images in an end-to-end manner. Specifically, we build our model on top of a single-step diffusion model and develop a face supervision mechanism to guide the model in focusing on the facial region. We then integrate a reference-based mechanism to help with identity restoration, reducing face ambiguity in low-quality face restoration. Additionally, we have built a high-quality 4K portrait image ISR dataset dubbed PortraitSR-4K, to support model training and benchmarking for portrait images. Extensive experiments show that HeadsUp achieves state-of-the-art performance on the PortraitISR task while maintaining comparable or higher performance on both general image and aligned face datasets.</li>
<li><strong>摘要：</strong>肖像照片通常同时具有人物主题和自然背景，是社交媒体上最流行的摄影形式之一。现有的图像超分辨率（ISR）技术通常关注通用的现实世界图像或严格对齐的面部图像（即面部超分辨率）。在实践中，单独的模型被混合来处理肖像照片：面部专家模型处理面部区域，通用模型处理其余部分。然而，由于不同的模型训练方案，这些混合方法不可避免地会在面部区域周围引入混合或边界伪影，而人类感知对面部保真度特别敏感。为了克服这些限制，我们研究了肖像图像超解（PortraitISR）问题，并提出了 HeadsUp，这是一种单步扩散模型，能够以端到端的方式无缝恢复和放大肖像图像。具体来说，我们在单步扩散模型的基础上构建模型，并开发面部监督机制来指导模型关注面部区域。然后，我们集成基于参考的机制来帮助身份恢复，减少低质量面部恢复中的面部模糊性。此外，我们还构建了一个名为 PortraitSR-4K 的高质量 4K 人像图像 ISR 数据集，以支持人像图像的模型训练和基准测试。大量实验表明，HeadsUp 在 PortraitISR 任务上实现了最先进的性能，同时在一般图像和对齐的人脸数据集上保持了相当或更高的性能。</li>
</ul>

<h3>Title: FlareX: A Physics-Informed Dataset for Lens Flare Removal via 2D Synthesis and 3D Rendering</h3>
<ul>
<li><strong>Authors: </strong>Lishen Qu, Zhihao Liu, Jinshan Pan, Shihao Zhou, Jinglei Shi, Duosheng Chen, Jufeng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09995">https://arxiv.org/abs/2510.09995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09995">https://arxiv.org/pdf/2510.09995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09995]] FlareX: A Physics-Informed Dataset for Lens Flare Removal via 2D Synthesis and 3D Rendering(https://arxiv.org/abs/2510.09995)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Lens flare occurs when shooting towards strong light sources, significantly degrading the visual quality of images. Due to the difficulty in capturing flare-corrupted and flare-free image pairs in the real world, existing datasets are typically synthesized in 2D by overlaying artificial flare templates onto background images. However, the lack of flare diversity in templates and the neglect of physical principles in the synthesis process hinder models trained on these datasets from generalizing well to real-world scenarios. To address these challenges, we propose a new physics-informed method for flare data generation, which consists of three stages: parameterized template creation, the laws of illumination-aware 2D synthesis, and physical engine-based 3D rendering, which finally gives us a mixed flare dataset that incorporates both 2D and 3D perspectives, namely FlareX. This dataset offers 9,500 2D templates derived from 95 flare patterns and 3,000 flare image pairs rendered from 60 3D scenes. Furthermore, we design a masking approach to obtain real-world flare-free images from their corrupted counterparts to measure the performance of the model on real-world images. Extensive experiments demonstrate the effectiveness of our method and dataset.</li>
<li><strong>摘要：</strong>向强光源拍摄时会出现镜头眩光，从而显着降低图像的视觉质量。由于在现实世界中捕获耀斑损坏和无耀斑图像对的困难，现有数据集通常通过将人造耀斑模板叠加到背景图像上来以二维方式合成。然而，模板中缺乏耀斑多样性以及合成过程中对物理原理的忽视阻碍了在这些数据集上训练的模型无法很好地推广到现实世界的场景。为了应对这些挑战，我们提出了一种新的基于物理的耀斑数据生成方法，该方法由三个阶段组成：参数化模板创建、照明感知 2D 合成法则以及基于物理引擎的 3D 渲染，最终为我们提供了一个包含 2D 和 3D 视角的混合耀斑数据集，即 FlareX。该数据集提供了源自 95 个耀斑图案的 9,500 个 2D 模板以及从 60 个 3D 场景渲染的 3,000 个耀斑图像对。此外，我们设计了一种掩蔽方法，从损坏的对应图像中获取真实世界的无耀斑图像，以测量模型在真实世界图像上的性能。大量的实验证明了我们的方法和数据集的有效性。</li>
</ul>

<h3>Title: BurstDeflicker: A Benchmark Dataset for Flicker Removal in Dynamic Scenes</h3>
<ul>
<li><strong>Authors: </strong>Lishen Qu, Zhihao Liu, Shihao Zhou, Yaqi Luo, Jie Liang, Hui Zeng, Lei Zhang, Jufeng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09996">https://arxiv.org/abs/2510.09996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09996">https://arxiv.org/pdf/2510.09996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09996]] BurstDeflicker: A Benchmark Dataset for Flicker Removal in Dynamic Scenes(https://arxiv.org/abs/2510.09996)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Flicker artifacts in short-exposure images are caused by the interplay between the row-wise exposure mechanism of rolling shutter cameras and the temporal intensity variations of alternating current (AC)-powered lighting. These artifacts typically appear as uneven brightness distribution across the image, forming noticeable dark bands. Beyond compromising image quality, this structured noise also affects high-level tasks, such as object detection and tracking, where reliable lighting is crucial. Despite the prevalence of flicker, the lack of a large-scale, realistic dataset has been a significant barrier to advancing research in flicker removal. To address this issue, we present BurstDeflicker, a scalable benchmark constructed using three complementary data acquisition strategies. First, we develop a Retinex-based synthesis pipeline that redefines the goal of flicker removal and enables controllable manipulation of key flicker-related attributes (e.g., intensity, area, and frequency), thereby facilitating the generation of diverse flicker patterns. Second, we capture 4,000 real-world flicker images from different scenes, which help the model better understand the spatial and temporal characteristics of real flicker artifacts and generalize more effectively to wild scenarios. Finally, due to the non-repeatable nature of dynamic scenes, we propose a green-screen method to incorporate motion into image pairs while preserving real flicker degradation. Comprehensive experiments demonstrate the effectiveness of our dataset and its potential to advance research in flicker removal.</li>
<li><strong>摘要：</strong>短曝光图像中的闪烁伪像是由卷帘快门相机的行式曝光机制与交流 (AC) 供电照明的时间强度变化之间的相互作用引起的。这些伪影通常表现为图像中亮度分布不均匀，形成明显的暗带。除了影响图像质量之外，这种结构化噪声还会影响高级任务，例如对象检测和跟踪，其中可靠的照明至关重要。尽管闪烁很普遍，但缺乏大规模、真实的数据集一直是推进消除闪烁研究的重大障碍。为了解决这个问题，我们提出了 BurstDeflicker，这是一个使用三种互补的数据采集策略构建的可扩展基准。首先，我们开发了一种基于 Retinex 的合成管道，它重新定义了闪烁消除的目标，并实现了对关键闪烁相关属性（例如强度、面积和频率）的可控操作，从而促进了不同闪烁模式的生成。其次，我们捕获了来自不同场景的 4,000 张真实世界闪烁图像，这有助于模型更好地理解真实闪烁伪影的空间和时间特征，并更有效地推广到野外场景。最后，由于动态场景的不可重复性，我们提出了一种绿屏方法，将运动合并到图像对中，同时保留真实的闪烁衰减。综合实验证明了我们数据集的有效性及其推进闪烁消除研究的潜力。</li>
</ul>

<h3>Title: DREAM: A Benchmark Study for Deepfake REalism AssessMent</h3>
<ul>
<li><strong>Authors: </strong>Bo Peng, Zichuan Wang, Sheng Yu, Xiaochuan Jin, Wei Wang, Jing Dong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10053">https://arxiv.org/abs/2510.10053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10053">https://arxiv.org/pdf/2510.10053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10053]] DREAM: A Benchmark Study for Deepfake REalism AssessMent(https://arxiv.org/abs/2510.10053)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Deep learning based face-swap videos, widely known as deepfakes, have drawn wide attention due to their threat to information credibility. Recent works mainly focus on the problem of deepfake detection that aims to reliably tell deepfakes apart from real ones, in an objective way. On the other hand, the subjective perception of deepfakes, especially its computational modeling and imitation, is also a significant problem but lacks adequate study. In this paper, we focus on the visual realism assessment of deepfakes, which is defined as the automatic assessment of deepfake visual realism that approximates human perception of deepfakes. It is important for evaluating the quality and deceptiveness of deepfakes which can be used for predicting the influence of deepfakes on Internet, and it also has potentials in improving the deepfake generation process by serving as a critic. This paper prompts this new direction by presenting a comprehensive benchmark called DREAM, which stands for Deepfake REalism AssessMent. It is comprised of a deepfake video dataset of diverse quality, a large scale annotation that includes 140,000 realism scores and textual descriptions obtained from 3,500 human annotators, and a comprehensive evaluation and analysis of 16 representative realism assessment methods, including recent large vision language model based methods and a newly proposed description-aligned CLIP method. The benchmark and insights included in this study can lay the foundation for future research in this direction and other related areas.</li>
<li><strong>摘要：</strong>基于深度学习的换脸视频，即众所周知的 Deepfakes，由于对信息可信度的威胁而引起了广泛关注。最近的工作主要集中在深度伪造品检测问题上，旨在以客观的方式可靠地区分深度伪造品和真实的伪造品。另一方面，深度伪造品的主观感知，尤其是其计算建模和模仿，也是一个重要问题，但缺乏足够的研究。在本文中，我们重点关注 Deepfakes 的视觉真实感评估，其定义为对 Deepfakes 视觉真实感的自动评估，接近人类对 Deepfakes 的感知。它对于评估 Deepfakes 的质量和欺骗性具有重要意义，可用于预测 Deepfakes 对互联网的影响，并且还具有作为批评者改进 Deepfakes 生成过程的潜力。本文通过提出一个名为 DREAM（代表 Deepfake REalism AssessMent）的综合基准来推动这一新方向。它由不同质量的 Deepfake 视频数据集、包括从 3,500 名人类注释者获得的 140,000 个真实性分数和文本描述的大规模注释，以及对 16 种代表性真实性评估方法的综合评估和分析，包括最近基于大型视觉语言模型的方法和新提出的描述对齐 CLIP 方法。本研究中包含的基准和见解可以为该方向和其他相关领域的未来研究奠定基础。</li>
</ul>

<h3>Title: PANTHER: Generative Pretraining Beyond Language for Sequential User Behavior Modeling</h3>
<ul>
<li><strong>Authors: </strong>Guilin Li, Yun Zhang, Xiuyuan Chen, Chengqi Li, Bo Wang, Linghe Kong, Wenjia Wang, Weiran Huang, Matthias Hwai Yong Tan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10102">https://arxiv.org/abs/2510.10102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10102">https://arxiv.org/pdf/2510.10102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10102]] PANTHER: Generative Pretraining Beyond Language for Sequential User Behavior Modeling(https://arxiv.org/abs/2510.10102)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown that generative pretraining can distill vast world knowledge into compact token representations. While LLMs encapsulate extensive world knowledge, they remain limited in modeling the behavioral knowledge contained within user interaction histories. User behavior forms a distinct modality, where each action, defined by multi-dimensional attributes such as time, context, and transaction type, constitutes a behavioral token. Modeling these high-cardinality sequences is challenging, and discriminative models often falter under limited supervision. To bridge this gap, we extend generative pretraining to user behavior, learning transferable representations from unlabeled behavioral data analogous to how LLMs learn from text. We present PANTHER, a hybrid generative-discriminative framework that unifies user behavior pretraining and downstream adaptation, enabling large-scale sequential user representation learning and real-time inference. PANTHER introduces: (1) Structured Tokenization to compress multi-dimensional transaction attributes into an interpretable vocabulary; (2) Sequence Pattern Recognition Module (SPRM) for modeling periodic transaction motifs; (3) a Unified User-Profile Embedding that fuses static demographics with dynamic transaction histories; and (4) Real-time scalability enabled by offline caching of pretrained embeddings for millisecond-level inference. Fully deployed and operational online at WeChat Pay, PANTHER delivers a 25.6 percent boost in next-transaction prediction HitRate@1 and a 38.6 percent relative improvement in fraud detection recall over baselines. Cross-domain evaluations on public benchmarks show strong generalization, achieving up to 21 percent HitRate@1 gains over transformer baselines, establishing PANTHER as a scalable, high-performance framework for industrial sequential user behavior modeling.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）表明，生成式预训练可以将大量的世界知识提炼为紧凑的标记表示。虽然法学硕士囊括了广泛的世界知识，但它们在对用户交互历史中包含的行为知识进行建模方面仍然受到限制。用户行为形成了独特的模态，其中每个动作由时间、上下文、交易类型等多维属性定义，构成行为令牌。对这些高基数序列进行建模具有挑战性，并且判别模型在有限的监督下常常会动摇。为了弥补这一差距，我们将生成预训练扩展到用户行为，从未标记的行为数据中学习可转移的表示，类似于法学硕士从文本中学习的方式。我们提出了 PANTHER，一种混合​​生成判别框架，它将用户行为预训练和下游适应相结合，从而实现大规模顺序用户表示学习和实时推理。 PANTHER介绍：（1）结构化Tokenization，将多维交易属性压缩为可解释的词汇表； (2) 序列模式识别模块（SPRM），用于对周期性交易主题进行建模； (3) 统一的用户配置文件嵌入，将静态人口统计数据与动态交易历史融为一体； (4) 通过离线缓存预训练嵌入来实现毫秒级推理的实时可扩展性。 PANTHER 在微信支付上全面部署并在线运行，与基线相比，下一笔交易预测 HitRate@1 提高了 25.6%，欺诈检测召回率相对提高了 38.6%。对公共基准的跨域评估显示出很强的通用性，与 Transformer 基准相比，HitRate@1 增益高达 21%，将 PANTHER 确立为工业顺序用户行为建模的可扩展、高性能框架。</li>
</ul>

<h3>Title: DeepFusionNet: Autoencoder-Based Low-Light Image Enhancement and Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Halil Hüseyin Çalışkan, Talha Koruk</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10122">https://arxiv.org/abs/2510.10122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10122">https://arxiv.org/pdf/2510.10122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10122]] DeepFusionNet: Autoencoder-Based Low-Light Image Enhancement and Super-Resolution(https://arxiv.org/abs/2510.10122)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Computer vision and image processing applications suffer from dark and low-light images, particularly during real-time image transmission. Currently, low light and dark images are converted to bright and colored forms using autoencoders; however, these methods often achieve low SSIM and PSNR scores and require high computational power due to their large number of parameters. To address these challenges, the DeepFusionNet architecture has been developed. According to the results obtained with the LOL-v1 dataset, DeepFusionNet achieved an SSIM of 92.8% and a PSNR score of 26.30, while containing only approximately 2.5 million parameters. On the other hand, conversion of blurry and low-resolution images into high-resolution and blur-free images has gained importance in image processing applications. Unlike GAN-based super-resolution methods, an autoencoder-based super resolution model has been developed that contains approximately 100 thousand parameters and uses the DeepFusionNet architecture. According to the results of the tests, the DeepFusionNet based super-resolution method achieved a PSNR of 25.30 and a SSIM score of 80.7 percent according to the validation set.</li>
<li><strong>摘要：</strong>计算机视觉和图像处理应用会受到黑暗和低光图像的影响，特别是在实时图像传输期间。目前，使用自动编码器将低光和暗图像转换为明亮和彩色的形式；然而，这些方法通常获得较低的 SSIM 和 PSNR 分数，并且由于参数数量较多而需要较高的计算能力。为了应对这些挑战，开发了 DeepFusionNet 架构。根据 LOL-v1 数据集获得的结果，DeepFusionNet 实现了 92.8% 的 SSIM 和 26.30 的 PSNR 分数，同时仅包含约 250 万个参数。另一方面，将模糊和低分辨率图像转换为高分辨率和无模糊图像在图像处理应用中变得越来越重要。与基于 GAN 的超分辨率方法不同，我们开发了基于自动编码器的超分辨率模型，包含约 10 万个参数并使用 DeepFusionNet 架构。测试结果显示，基于 DeepFusionNet 的超分辨率方法在验证集上的 PSNR 为 25.30，SSIM 得分为 80.7%。</li>
</ul>

<h3>Title: CacheClip: Accelerating RAG with Effective KV Cache Reuse</h3>
<ul>
<li><strong>Authors: </strong>Bin Yang, Qiuyu Leng, Jun Zeng, Zhenhua Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10129">https://arxiv.org/abs/2510.10129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10129">https://arxiv.org/pdf/2510.10129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10129]] CacheClip: Accelerating RAG with Effective KV Cache Reuse(https://arxiv.org/abs/2510.10129)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) systems suffer from severe time-to-first-token (TTFT) bottlenecks due to long input sequences. Existing KV cache reuse methods face a fundamental trade-off: prefix caching requires identical prefixes that rarely occur in RAG scenarios, while direct precomputation sacrifices quality due to missing inter-chunk attention and repeated attention sinks. Recent methods like APE and CacheBlend partially address these issues but remain inadequate for robust RAG applications. This paper presents CacheClip, a novel framework that achieves both fast TTFT and high generation quality. Our key insight is that small auxiliary LLMs exhibit similar last-layer attention distributions to primary LLMs (the target model for generation), enabling efficient identification of tokens critical for restoring inter-chunk attention, thereby significantly improving response quality on cross-chunk reasoning tasks. CacheClip integrates three techniques: (1) auxiliary-model-guided token selection for selective KV cache recomputation, where the auxiliary model is finetuned to improve selection accuracy, (2) shared prefixes to eliminate redundant attention sinks, and (3) grouping strategy to maintain local coherence during partial KV cache updates. Experiments show CacheClip retains up to 94.8% and 85.0% of full-attention performance on NIAH and LongBench, outperforming APE and CacheBlend by 25.2% and 35.1% on NIAH (with reomp% = 20%). Meanwhile, CacheClip accelerates LLM inference by up to 1.92x in prefill time, providing a practical solution to the efficiency-quality trade-off in RAG systems.</li>
<li><strong>摘要：</strong>由于输入序列较长，检索增强生成 (RAG) 系统面临严重的首次令牌时间 (TTFT) 瓶颈。现有的 KV 缓存重用方法面临一个根本性的权衡：前缀缓存需要相同的前缀，而这在 RAG 场景中很少出现，而直接预计算由于缺少块间注意力和重复的注意力池而牺牲了质量。最近的方法（例如 APE 和 CacheBlend）部分解决了这些问题，但对于强大的 RAG 应用程序来说仍然不够。本文介绍了 CacheClip，这是一种既实现快速 TTFT 又实现高生成质量的新颖框架。我们的主要见解是，小型辅助 LLM 表现出与主 LLM（生成的目标模型）类似的最后一层注意力分布，从而能够有效识别对于恢复块间注意力至关重要的标记，从而显着提高跨块推理任务的响应质量。 CacheClip 集成了三种技术：（1）用于选择性 KV 缓存重新计算的辅助模型引导标记选择，其中辅助模型经过微调以提高选择准确性，（2）共享前缀以消除冗余注意力池，以及（3）分组策略以在部分 KV 缓存更新期间保持局部一致性。实验表明，CacheClip 在 NIAH 和 LongBench 上保留了高达 94.8% 和 85.0% 的全注意力性能，在 NIAH 上比 APE 和 CacheBlend 分别高出 25.2% 和 35.1%（reomp% = 20%）。同时，CacheClip 将预填充时间的 LLM 推理速度提高了 1.92 倍，为 RAG 系统中的效率与质量权衡提供了实用的解决方案。</li>
</ul>

<h3>Title: Robust Learning of Diffusion Models with Extremely Noisy Conditions</h3>
<ul>
<li><strong>Authors: </strong>Xin Chen, Gillian Dobbie, Xinyu Wang, Feng Liu, Di Wang, Jingfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10149">https://arxiv.org/abs/2510.10149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10149">https://arxiv.org/pdf/2510.10149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10149]] Robust Learning of Diffusion Models with Extremely Noisy Conditions(https://arxiv.org/abs/2510.10149)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Conditional diffusion models have the generative controllability by incorporating external conditions. However, their performance significantly degrades with noisy conditions, such as corrupted labels in the image generation or unreliable observations or states in the control policy generation. This paper introduces a robust learning framework to address extremely noisy conditions in conditional diffusion models. We empirically demonstrate that existing noise-robust methods fail when the noise level is high. To overcome this, we propose learning pseudo conditions as surrogates for clean conditions and refining pseudo ones progressively via the technique of temporal ensembling. Additionally, we develop a Reverse-time Diffusion Condition (RDC) technique, which diffuses pseudo conditions to reinforce the memorization effect and further facilitate the refinement of the pseudo conditions. Experimentally, our approach achieves state-of-the-art performance across a range of noise levels on both class-conditional image generation and visuomotor policy generation this http URL code can be accessible via the project page this https URL</li>
<li><strong>摘要：</strong>条件扩散模型通过结合外部条件而具有生成可控性。然而，它们的性能会因噪声条件而显着降低，例如图像生成中的标签损坏或控制策略生成中的观察或状态不可靠。本文介绍了一个强大的学习框架来解决条件扩散模型中极其嘈杂的条件。我们凭经验证明，当噪声水平较高时，现有的抗噪声方法会失败。为了克服这个问题，我们建议学习伪条件作为干净条件的替代，并通过时间集成技术逐步完善伪条件。此外，我们还开发了一种逆时扩散条件（RDC）技术，该技术可以扩散伪条件以增强记忆效果并进一步促进伪条件的细化。实验上，我们的方法在类条件图像生成和视觉运动策略生成方面在一系列噪声水平上实现了最先进的性能，可以通过项目页面访问此 https URL</li>
</ul>

<h3>Title: Rethinking Entropy Interventions in RLVR: An Entropy Change Perspective</h3>
<ul>
<li><strong>Authors: </strong>Zhezheng Hao, Hong Wang, Haoyang Liu, Jian Luo, Jiarui Yu, Hande Dong, Qiang Lin, Can Wang, Jiawei Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10150">https://arxiv.org/abs/2510.10150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10150">https://arxiv.org/pdf/2510.10150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10150]] Rethinking Entropy Interventions in RLVR: An Entropy Change Perspective(https://arxiv.org/abs/2510.10150)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While Reinforcement Learning with Verifiable Rewards (RLVR) can enhance LLM reasoning, its training process poses a critical risk: entropy collapse. This phenomenon is a rapid loss of policy diversity, stemming from the exploration-exploitation imbalance and leading to a lack of generalization. Recent entropy-intervention methods aim to prevent \coloredtext{entropy collapse}, yet their underlying mechanisms remain unclear. In this paper, we conduct a quantitative analysis to reveal token-level entropy changes and how existing entropy intervention methods help avoid entropy collapse. Our findings point out a fundamental limitation of existing methods: they attempt to control entropy dynamics indirectly. By only affecting related factors, such as the advantage signal and generation probability, their effectiveness is inherently limited and could potentially fail. To address this limitation, we introduce an entropy-change-aware reweighting scheme, namely Stabilizing Token-level Entropy-changE via Reweighting (STEER), that adaptively stabilizes entropy dynamics through fine-grained token-level adjustments. Our approach mitigates over-exploitation while fostering robust exploration. Extensive experiments demonstrate that STEER significantly mitigates entropy collapse, stabilizes entropy dynamics, and achieves stronger downstream performance across various mathematical reasoning benchmarks \footnote{Our code is available at this https URL.</li>
<li><strong>摘要：</strong>虽然带有可验证奖励的强化学习（RLVR）可以增强 LLM 推理，但其训练过程带来了一个严重的风险：熵崩溃。这种现象是政策多样性的迅速丧失，源于勘探与开发的不平衡，并导致缺乏普遍性。最近的熵干预方法旨在防止\coloredtext{熵崩溃}，但其潜在机制仍不清楚。在本文中，我们进行了定量分析，以揭示代币级别的熵变化以及现有的熵干预方法如何帮助避免熵崩溃。我们的研究结果指出了现有方法的根本局限性：它们试图间接控制熵动力学。由于仅影响相关因素，例如优势信号和生成概率，其有效性本质上是有限的，并且可能会失败。为了解决这个限制，我们引入了一种熵变化感知的重新加权方案，即通过重新加权稳定令牌级别的熵变化（STEER），该方案通过细粒度的令牌级别调整自适应地稳定熵动态。我们的方法减少了过度开发，同时促进了强有力的探索。大量实验表明，STEER 显着减轻了熵崩溃，稳定了熵动态，并在各种数学推理基准中实现了更强的下游性能 \footnote{我们的代码可在此 https URL 中获取。</li>
</ul>

<h3>Title: Stroke Locus Net: Occluded Vessel Localization from MRI Modalities</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Hamad, Muhammad Khan, Tamer Khattab, Mohamed Mabrok</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10155">https://arxiv.org/abs/2510.10155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10155">https://arxiv.org/pdf/2510.10155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10155]] Stroke Locus Net: Occluded Vessel Localization from MRI Modalities(https://arxiv.org/abs/2510.10155)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>A key challenge in ischemic stroke diagnosis using medical imaging is the accurate localization of the occluded vessel. Current machine learning methods in focus primarily on lesion segmentation, with limited work on vessel localization. In this study, we introduce Stroke Locus Net, an end-to-end deep learning pipeline for detection, segmentation, and occluded vessel localization using only MRI scans. The proposed system combines a segmentation branch using nnUNet for lesion detection with an arterial atlas for vessel mapping and identification, and a generation branch using pGAN to synthesize MRA images from MRI. Our implementation demonstrates promising results in localizing occluded vessels on stroke-affected T1 MRI scans, with potential for faster and more informed stroke diagnosis.</li>
<li><strong>摘要：</strong>使用医学成像诊断缺血性中风的一个关键挑战是闭塞血管的准确定位。当前的机器学习方法主要关注病变分割，在血管定位方面的工作有限。在这项研究中，我们介绍了 Stroke Locus Net，这是一种仅使用 MRI 扫描即可进行检测、分割和闭塞血管定位的端到端深度学习管道。该系统将使用 nnUNet 进行病变检测的分割分支与用于血管映射和识别的动脉图集相结合，以及使用 pGAN 合成来自 MRI 的 MRA 图像的生成分支。我们的实施表明，在受中风影响的 T1 MRI 扫描中定位闭塞血管方面取得了有希望的结果，有可能实现更快、更明智的中风诊断。</li>
</ul>

<h3>Title: ReMix: Towards a Unified View of Consistent Character Generation and Editing</h3>
<ul>
<li><strong>Authors: </strong>Benjia Zhou, Bin Fu, Pei Cheng, Yanru Wang, Jiayuan Fan, Tao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10156">https://arxiv.org/abs/2510.10156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10156">https://arxiv.org/pdf/2510.10156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10156]] ReMix: Towards a Unified View of Consistent Character Generation and Editing(https://arxiv.org/abs/2510.10156)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in large-scale text-to-image diffusion models (e.g., FLUX.1) have greatly improved visual fidelity in consistent character generation and editing. However, existing methods rarely unify these tasks within a single framework. Generation-based approaches struggle with fine-grained identity consistency across instances, while editing-based methods often lose spatial controllability and instruction alignment. To bridge this gap, we propose ReMix, a unified framework for character-consistent generation and editing. It constitutes two core components: the ReMix Module and IP-ControlNet. The ReMix Module leverages the multimodal reasoning ability of MLLMs to edit semantic features of input images and adapt instruction embeddings to the native DiT backbone without fine-tuning. While this ensures coherent semantic layouts, pixel-level consistency and pose controllability remain challenging. To address this, IP-ControlNet extends ControlNet to decouple semantic and layout cues from reference images and introduces an {\epsilon}-equivariant latent space that jointly denoises the reference and target images within a shared noise space. Inspired by convergent evolution and quantum decoherence,i.e., where environmental noise drives state convergence, this design promotes feature alignment in the hidden space, enabling consistent object generation while preserving identity. ReMix supports a wide range of tasks, including personalized generation, image editing, style transfer, and multi-condition synthesis. Extensive experiments validate its effectiveness and efficiency as a unified framework for character-consistent image generation and editing.</li>
<li><strong>摘要：</strong>大规模文本到图像扩散模型（例如 FLUX.1）的最新进展极大地提高了一致的字符生成和编辑的视觉保真度。然而，现有的方法很少将这些任务统一在一个框架内。基于生成的方法在实例之间难以实现细粒度的身份一致性，而基于编辑的方法通常会失去空间可控性和指令对齐。为了弥补这一差距，我们提出了 ReMix，这是一个用于角色一致生成和编辑的统一框架。它由两个核心组件组成：ReMix Module 和 IP-ControlNet。 ReMix 模块利用 MLLM 的多模态推理能力来编辑输入图像的语义特征，并使指令嵌入适应本机 DiT 主干，而无需进行微调。虽然这确保了连贯的语义布局，但像素级一致性和姿势可控性仍然具有挑战性。为了解决这个问题，IP-ControlNet 扩展了 ControlNet，将语义和布局线索与参考图像解耦，并引入了一个 {\epsilon} 等变潜在空间，该空间在共享噪声空间内联合对参考图像和目标图像进行去噪。受收敛进化和量子退相干（即环境噪声驱动状态收敛）的启发，该设计促进隐藏空间中的特征对齐，从而在保持身份的同时实现一致的对象生成。 ReMix 支持广泛的任务，包括个性化生成、图像编辑、风格转移和多条件合成。大量的实验验证了其作为字符一致图像生成和编辑的统一框架的有效性和效率。</li>
</ul>

<h3>Title: Hierarchical Bayesian Flow Networks for Molecular Graph Generation</h3>
<ul>
<li><strong>Authors: </strong>Yida Xiong, Jiameng Chen, Kun Li, Hongzhi Zhang, Xiantao Cai, Wenbin Hu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10211">https://arxiv.org/abs/2510.10211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10211">https://arxiv.org/pdf/2510.10211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10211]] Hierarchical Bayesian Flow Networks for Molecular Graph Generation(https://arxiv.org/abs/2510.10211)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Molecular graph generation is essentially a classification generation problem, aimed at predicting categories of atoms and bonds. Currently, prevailing paradigms such as continuous diffusion models are trained to predict continuous numerical values, treating the training process as a regression task. However, the final generation necessitates a rounding step to convert these predictions back into discrete classification categories, which is intrinsically a classification operation. Given that the rounding operation is not incorporated during training, there exists a significant discrepancy between the model's training objective and its inference procedure. As a consequence, an excessive emphasis on point-wise precision can lead to overfitting and inefficient learning. This occurs because considerable efforts are devoted to capturing intra-bin variations that are ultimately irrelevant to the discrete nature of the task at hand. Such a flaw results in diminished molecular diversity and constrains the model's generalization capabilities. To address this fundamental limitation, we propose GraphBFN, a novel hierarchical coarse-to-fine framework based on Bayesian Flow Networks that operates on the parameters of distributions. By innovatively introducing Cumulative Distribution Function, GraphBFN is capable of calculating the probability of selecting the correct category, thereby unifying the training objective with the sampling rounding operation. We demonstrate that our method achieves superior performance and faster generation, setting new state-of-the-art results on the QM9 and ZINC250k molecular graph generation benchmarks.</li>
<li><strong>摘要：</strong>分子图生成本质上是一个分类生成问题，旨在预测原子和键的类别。目前，流行的范式（例如连续扩散模型）被训练来预测连续数值，将训练过程视为回归任务。然而，最终生成需要舍入步骤将这些预测转换回离散分类类别，这本质上是分类操作。由于训练过程中没有加入舍入操作，因此模型的训练目标与其推理过程之间存在显着差异。因此，过度强调逐点精度可能会导致过度拟合和低效学习。发生这种情况是因为我们投入了大量精力来捕获箱内变化，而这些变化最终与当前任务的离散性质无关。这种缺陷会导致分子多样性减少并限制模型的泛化能力。为了解决这一基本限制，我们提出了 GraphBFN，这是一种基于贝叶斯流网络的新型分层粗到细框架，可对分布参数进行操作。通过创新性地引入累积分布函数，GraphBFN能够计算选择正确类别的概率，从而将训练目标与采样舍入操作统一起来。我们证明了我们的方法实现了卓越的性能和更快的生成速度，在 QM9 和 ZINC250k 分子图生成基准上设定了新的最先进的结果。</li>
</ul>

<h3>Title: Are Video Models Emerging as Zero-Shot Learners and Reasoners in Medical Imaging?</h3>
<ul>
<li><strong>Authors: </strong>Yuxiang Lai, Jike Zhong, Ming Li, Yuheng Li, Xiaofeng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10254">https://arxiv.org/abs/2510.10254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10254">https://arxiv.org/pdf/2510.10254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10254]] Are Video Models Emerging as Zero-Shot Learners and Reasoners in Medical Imaging?(https://arxiv.org/abs/2510.10254)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in large generative models have shown that simple autoregressive formulations, when scaled appropriately, can exhibit strong zero-shot generalization across domains. Motivated by this trend, we investigate whether autoregressive video modeling principles can be directly applied to medical imaging tasks, despite the model never being trained on medical data. Specifically, we evaluate a large vision model (LVM) in a zero-shot setting across four representative tasks: organ segmentation, denoising, super-resolution, and motion prediction. Remarkably, even without domain-specific fine-tuning, the LVM can delineate anatomical structures in CT scans and achieve competitive performance on segmentation, denoising, and super-resolution. Most notably, in radiotherapy motion prediction, the model forecasts future 3D CT phases directly from prior phases of a 4D CT scan, producing anatomically consistent predictions that capture patient-specific respiratory dynamics with realistic temporal coherence. We evaluate the LVM on 4D CT data from 122 patients, totaling over 1,820 3D CT volumes. Despite no prior exposure to medical data, the model achieves strong performance across all tasks and surpasses specialized DVF-based and generative baselines in motion prediction, achieving state-of-the-art spatial accuracy. These findings reveal the emergence of zero-shot capabilities in medical video modeling and highlight the potential of general-purpose video models to serve as unified learners and reasoners laying the groundwork for future medical foundation models built on video models.</li>
<li><strong>摘要：</strong>大型生成模型的最新进展表明，简单的自回归公式在适当缩放时可以表现出跨领域的强大零样本泛化能力。受这一趋势的推动，我们研究了自回归视频建模原理是否可以直接应用于医学成像任务，尽管该模型从未接受过医学数据的训练。具体来说，我们在零样本设置下评估了四个代表性任务的大型视觉模型（LVM）：器官分割、去噪、超分辨率和运动预测。值得注意的是，即使没有特定领域的微调，LVM 也可以在 CT 扫描中描绘解剖结构，并在分割、去噪和超分辨率方面实现具有竞争力的性能。最值得注意的是，在放射治疗运动预测中，该模型直接根据 4D CT 扫描的先前阶段预测未来的 3D CT 阶段，产生解剖学上一致的预测，以现实的时间相干性捕获患者特定的呼吸动态。我们根据 122 名患者的 4D CT 数据评估 LVM，总计超过 1,820 个 3D CT 体积。尽管之前没有接触过医疗数据，但该模型在所有任务中都实现了强大的性能，并超越了运动预测中基于 DVF 的专门生成基线，实现了最先进的空间精度。这些发现揭示了医学视频建模中零样本功能的出现，并强调了通用视频模型作为统一学习器和推理器的潜力，为基于视频模型的未来医学基础模型奠定了基础。</li>
</ul>

<h3>Title: From Programs to Poses: Factored Real-World Scene Generation via Learned Program Libraries</h3>
<ul>
<li><strong>Authors: </strong>Joy Hsu, Emily Jin, Jiajun Wu, Niloy J. Mitra</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10292">https://arxiv.org/abs/2510.10292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10292">https://arxiv.org/pdf/2510.10292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10292]] From Programs to Poses: Factored Real-World Scene Generation via Learned Program Libraries(https://arxiv.org/abs/2510.10292)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Real-world scenes, such as those in ScanNet, are difficult to capture, with highly limited data available. Generating realistic scenes with varied object poses remains an open and challenging task. In this work, we propose FactoredScenes, a framework that synthesizes realistic 3D scenes by leveraging the underlying structure of rooms while learning the variation of object poses from lived-in scenes. We introduce a factored representation that decomposes scenes into hierarchically organized concepts of room programs and object poses. To encode structure, FactoredScenes learns a library of functions capturing reusable layout patterns from which scenes are drawn, then uses large language models to generate high-level programs, regularized by the learned library. To represent scene variations, FactoredScenes learns a program-conditioned model to hierarchically predict object poses, and retrieves and places 3D objects in a scene. We show that FactoredScenes generates realistic, real-world rooms that are difficult to distinguish from real ScanNet scenes.</li>
<li><strong>摘要：</strong>现实世界的场景（例如 ScanNet 中的场景）很难捕捉，可用数据非常有限。生成具有不同物体姿势的真实场景仍然是一项开放且具有挑战性的任务。在这项工作中，我们提出了 FactoredScenes，这是一个框架，它通过利用房间的底层结构来合成逼真的 3D 场景，同时学习居住场景中物体姿势的变化。我们引入了一种分解表示，将场景分解为房间程序和对象姿势的分层组织概念。为了对结构进行编码，FactoredScenes 学习了一个函数库，该函数库捕获可重用的布局模式，从中绘制场景，然后使用大型语言模型生成高级程序，并由所学习的库进行规范化。为了表示场景变化，FactoredScenes 学习程序条件模型来分层预测对象姿势，并检索 3D 对象并将其放置在场景中。我们证明 FactoredScenes 生成真实的、真实世界的房间，很难与真实的 ScanNet 场景区分开来。</li>
</ul>

<h3>Title: Ortho-Fuse: Orthomosaic Generation for Sparse High-Resolution Crop Health Maps Through Intermediate Optical Flow Estimation</h3>
<ul>
<li><strong>Authors: </strong>Rugved Katole, Christopher Stewart</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10360">https://arxiv.org/abs/2510.10360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10360">https://arxiv.org/pdf/2510.10360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10360]] Ortho-Fuse: Orthomosaic Generation for Sparse High-Resolution Crop Health Maps Through Intermediate Optical Flow Estimation(https://arxiv.org/abs/2510.10360)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>AI-driven crop health mapping systems offer substantial advantages over conventional monitoring approaches through accelerated data acquisition and cost reduction. However, widespread farmer adoption remains constrained by technical limitations in orthomosaic generation from sparse aerial imagery datasets. Traditional photogrammetric reconstruction requires 70-80\% inter-image overlap to establish sufficient feature correspondences for accurate geometric registration. AI-driven systems operating under resource-constrained conditions cannot consistently achieve these overlap thresholds, resulting in degraded reconstruction quality that undermines user confidence in autonomous monitoring technologies. In this paper, we present Ortho-Fuse, an optical flow-based framework that enables the generation of a reliable orthomosaic with reduced overlap requirements. Our approach employs intermediate flow estimation to synthesize transitional imagery between consecutive aerial frames, artificially augmenting feature correspondences for improved geometric reconstruction. Experimental validation demonstrates a 20\% reduction in minimum overlap requirements. We further analyze adoption barriers in precision agriculture to identify pathways for enhanced integration of AI-driven monitoring systems.</li>
<li><strong>摘要：</strong>人工智能驱动的作物健康绘图系统通过加速数据采集和降低成本，比传统监测方法具有显着优势。然而，农民的广泛采用仍然受到从稀疏航空图像数据集生成正射影像的技术限制的限制。传统的摄影测量重建需要 70-80% 的图像间重叠才能建立足够的特征对应关系以实现精确的几何配准。在资源有限的条件下运行的人工智能驱动系统无法始终达到这些重叠阈值，从而导致重建质量下降，从而削弱用户对自主监测技术的信心。在本文中，我们提出了 Ortho-Fuse，这是一种基于光流的框架，可以生成可靠的正射马赛克，并减少重叠要求。我们的方法采用中间流估计来合成连续航空帧之间的过渡图像，人为地增强特征对应以改进几何重建。实验验证表明，最小重叠要求减少了 20%。我们进一步分析精准农业的采用障碍，以确定加强人工智能驱动的监测系统集成的途径。</li>
</ul>

<h3>Title: AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration</h3>
<ul>
<li><strong>Authors: </strong>Xinlong Chen, Yue Ding, Weihong Lin, Jingyun Hua, Linli Yao, Yang Shi, Bozhou Li, Yuanxing Zhang, Qiang Liu, Pengfei Wan, Liang Wang, Tieniu Tan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10395">https://arxiv.org/abs/2510.10395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10395">https://arxiv.org/pdf/2510.10395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10395]] AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration(https://arxiv.org/abs/2510.10395)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Audiovisual video captioning aims to generate semantically rich descriptions with temporal alignment between visual and auditory events, thereby benefiting both video understanding and generation. In this paper, we present AVoCaDO, a powerful audiovisual video captioner driven by the temporal orchestration between audio and visual modalities. We propose a two-stage post-training pipeline: (1) AVoCaDO SFT, which fine-tunes the model on a newly curated dataset of 107K high-quality, temporally-aligned audiovisual captions; and (2) AVoCaDO GRPO, which leverages tailored reward functions to further enhance temporal coherence and dialogue accuracy while regularizing caption length and reducing collapse. Experimental results demonstrate that AVoCaDO significantly outperforms existing open-source models across four audiovisual video captioning benchmarks, and also achieves competitive performance on the VDC and DREAM-1K benchmark under visual-only settings.</li>
<li><strong>摘要：</strong>视听视频字幕旨在生成语义丰富的描述，并在视觉和听觉事件之间进行时间对齐，从而有利于视频理解和生成。在本文中，我们提出了 AVoCaDO，这是一种强大的视听视频字幕生成器，由音频和视觉模式之间的时间编排驱动。我们提出了一个两阶段的后训练流程：（1）AVoCaDO SFT，它在新策划的 107K 高质量、时间对齐的视听字幕数据集上微调模型； (2) AVoCaDO GRPO，它利用定制的奖励函数来进一步增强时间连贯性和对话准确性，同时规范字幕长度并减少崩溃。实验结果表明，AVoCaDO 在四个视听视频字幕基准测试中显着优于现有的开源模型，并且在仅视觉设置下的 VDC 和 DREAM-1K 基准测试中也实现了具有竞争力的性能。</li>
</ul>

<h3>Title: Controllable Graph Generation with Diffusion Models via Inference-Time Tree Search Guidance</h3>
<ul>
<li><strong>Authors: </strong>Jiachi Zhao, Zehong Wang, Yamei Liao, Chuxu Zhang, Yanfang Ye</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10402">https://arxiv.org/abs/2510.10402</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10402">https://arxiv.org/pdf/2510.10402</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10402]] Controllable Graph Generation with Diffusion Models via Inference-Time Tree Search Guidance(https://arxiv.org/abs/2510.10402)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Graph generation is a fundamental problem in graph learning with broad applications across Web-scale systems, knowledge graphs, and scientific domains such as drug and material discovery. Recent approaches leverage diffusion models for step-by-step generation, yet unconditional diffusion offers little control over desired properties, often leading to unstable quality and difficulty in incorporating new objectives. Inference-time guidance methods mitigate these issues by adjusting the sampling process without retraining, but they remain inherently local, heuristic, and limited in controllability. To overcome these limitations, we propose TreeDiff, a Monte Carlo Tree Search (MCTS) guided dual-space diffusion framework for controllable graph generation. TreeDiff is a plug-and-play inference-time method that expands the search space while keeping computation tractable. Specifically, TreeDiff introduces three key designs to make it practical and scalable: (1) a macro-step expansion strategy that groups multiple denoising updates into a single transition, reducing tree depth and enabling long-horizon exploration; (2) a dual-space denoising mechanism that couples efficient latent-space denoising with lightweight discrete correction in graph space, ensuring both scalability and structural fidelity; and (3) a dual-space verifier that predicts long-term rewards from partially denoised graphs, enabling early value estimation and removing the need for full rollouts. Extensive experiments on 2D and 3D molecular generation benchmarks, under both unconditional and conditional settings, demonstrate that TreeDiff achieves state-of-the-art performance. Notably, TreeDiff exhibits favorable inference-time scaling: it continues to improve with additional computation, while existing inference-time methods plateau early under limited resources.</li>
<li><strong>摘要：</strong>图生成是图学习中的一个基本问题，在网络规模系统、知识图以及药物和材料发现等科学领域有着广泛的应用。最近的方法利用扩散模型来逐步生成，但无条件扩散几乎无法控制所需的属性，通常会导致质量不稳定并且难以纳入新目标。推理时间指导方法通过调整采样过程而无需重新训练来缓解这些问题，但它们本质上仍然是局部的、启发式的，并且可控性有限。为了克服这些限制，我们提出了 TreeDiff，一种用于可控图生成的蒙特卡罗树搜索（MCTS）引导的双空间扩散框架。 TreeDiff 是一种即插即用的推理时间方法，可扩展搜索空间，同时保持计算易于处理。具体来说，TreeDiff 引入了三个关键设计，使其实用且可扩展：（1）宏步扩展策略，将多个去噪更新分组为单个转换，减少树深度并实现长范围探索； （2）双空间去噪机制，将高效的潜在空间去噪与图空间中的轻量级离散校正相结合，确保可扩展性和结构保真度； (3) 双空间验证器，可从部分去噪图预测长期奖励，从而实现早期价值估计并消除全面推出的需要。在无条件和条件设置下对 2D 和 3D 分子生成基准进行的大量实验表明，TreeDiff 实现了最先进的性能。值得注意的是，TreeDiff 表现出了良好的推理时间扩展：它通过额外的计算而不断改进，而现有的推理时间方法在资源有限的情况下很早就陷入了停滞状态。</li>
</ul>

<h3>Title: Taming a Retrieval Framework to Read Images in Humanlike Manner for Augmenting Generation of MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Suyang Xi, Chenxi Yang, Hong Ding, Yiqing Ni, Catherine C. Liu, Yunhao Liu, Chengqi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10426">https://arxiv.org/abs/2510.10426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10426">https://arxiv.org/pdf/2510.10426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10426]] Taming a Retrieval Framework to Read Images in Humanlike Manner for Augmenting Generation of MLLMs(https://arxiv.org/abs/2510.10426)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) often fail in fine-grained visual question answering, producing hallucinations about object identities, positions, and relations because textual queries are not explicitly anchored to visual referents. Retrieval-augmented generation (RAG) alleviates some errors, but it fails to align with human-like processing at both the retrieval and augmentation levels. Specifically, it focuses only on global-level image information but lacks local detail and limits reasoning about fine-grained interactions. To overcome this limitation, we present Human-Like Retrieval-Augmented Generation (HuLiRAG), a framework that stages multimodal reasoning as a ``what--where--reweight'' cascade. Queries are first anchored to candidate referents via open-vocabulary detection (what), then spatially resolved with SAM-derived masks to recover fine-grained precision (where), and adaptively prioritized through the trade-off between local and global alignment (reweight). Mask-guided fine-tuning further injects spatial evidence into the generation process, transforming grounding from a passive bias into an explicit constraint on answer formulation. Extensive experiments demonstrate that this human-like cascade improves grounding fidelity and factual consistency while reducing hallucinations, advancing multimodal question answering toward trustworthy reasoning.</li>
<li><strong>摘要：</strong>多模态大语言模型 (MLLM) 通常无法进行细粒度的视觉问答，从而产生有关对象身份、位置和关系的幻觉，因为文本查询没有明确锚定到视觉所指对象。检索增强生成（RAG）减少了一些错误，但它无法在检索和增强级别上与类人处理保持一致。具体来说，它仅关注全局级别的图像信息，但缺乏局部细节并限制了细粒度交互的推理。为了克服这一限制，我们提出了类人检索增强生成（HuLiRAG），这是一个将多模态推理阶段为“什么-哪里-重新加权”级联的框架。查询首先通过开放词汇检测（what）锚定到候选所指对象，然后使用 SAM 派生的掩码进行空间解析以恢复细粒度精度（where），并通过局部和全局对齐之间的权衡（重新加权）自适应地确定优先级。掩模引导的微调进一步将空间证据注入生成过程，将被动偏见的基础转变为对答案制定的明确约束。大量实验表明，这种类人级联可以提高基础保真度和事实一致性，同时减少幻觉，推动多模态问答迈向可信推理。</li>
</ul>

<h3>Title: Multi-Task Learning with Feature-Similarity Laplacian Graphs for Predicting Alzheimer's Disease Progression</h3>
<ul>
<li><strong>Authors: </strong>Zixiang Xu, Menghui Zhou, Jun Qi, Xuanhan Fan, Yun Yang, Po Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10433">https://arxiv.org/abs/2510.10433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10433">https://arxiv.org/pdf/2510.10433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10433]] Multi-Task Learning with Feature-Similarity Laplacian Graphs for Predicting Alzheimer's Disease Progression(https://arxiv.org/abs/2510.10433)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Alzheimer's Disease (AD) is the most prevalent neurodegenerative disorder in aging populations, posing a significant and escalating burden on global healthcare systems. While Multi-Tusk Learning (MTL) has emerged as a powerful computational paradigm for modeling longitudinal AD data, existing frameworks do not account for the time-varying nature of feature correlations. To address this limitation, we propose a novel MTL framework, named Feature Similarity Laplacian graph Multi-Task Learning (MTL-FSL). Our framework introduces a novel Feature Similarity Laplacian (FSL) penalty that explicitly models the time-varying relationships between features. By simultaneously considering temporal smoothness among tasks and the dynamic correlations among features, our model enhances both predictive accuracy and biological interpretability. To solve the non-smooth optimization problem arising from our proposed penalty terms, we adopt the Alternating Direction Method of Multipliers (ADMM) algorithm. Experiments conducted on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset demonstrate that our proposed MTL-FSL framework achieves state-of-the-art performance, outperforming various baseline methods. The implementation source can be found at this https URL.</li>
<li><strong>摘要：</strong>阿尔茨海默病（AD）是老龄化人群中最常见的神经退行性疾病，给全球医疗保健系统带来了巨大且不断升级的负担。虽然多象牙学习 (MTL) 已成为用于建模纵向 AD 数据的强大计算范例，但现有框架并未考虑特征相关性的时变性质。为了解决这个限制，我们提出了一种新颖的 MTL 框架，称为特征相似性拉普拉斯图多任务学习（MTL-FSL）。我们的框架引入了一种新颖的特征相似性拉普拉斯（FSL）惩罚，它显式地模拟了特征之间随时间变化的关系。通过同时考虑任务之间的时间平滑性和特征之间的动态相关性，我们的模型提高了预测准确性和生物可解释性。为了解决由我们提出的惩罚项引起的非平滑优化问题，我们采用交替方向乘子法（ADMM）算法。在阿尔茨海默病神经影像计划 (ADNI) 数据集上进行的实验表明，我们提出的 MTL-FSL 框架实现了最先进的性能，优于各种基线方法。可以在此 https URL 找到实现源。</li>
</ul>

<h3>Title: Reverse Supervision at Scale: Exponential Search Meets the Economics of Annotation</h3>
<ul>
<li><strong>Authors: </strong>Masoud Makrehchi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10446">https://arxiv.org/abs/2510.10446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10446">https://arxiv.org/pdf/2510.10446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10446]] Reverse Supervision at Scale: Exponential Search Meets the Economics of Annotation(https://arxiv.org/abs/2510.10446)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We analyze a reversed-supervision strategy that searches over labelings of a large unlabeled set \(B\) to minimize error on a small labeled set \(A\). The search space is \(2^n\), and the resulting complexity remains exponential even under large constant-factor speedups (e.g., quantum or massively parallel hardware). Consequently, arbitrarily fast -- but not exponentially faster -- computation does not obviate the need for informative labels or priors. In practice, the machine learning pipeline still requires an initial human contribution: specifying the objective, defining classes, and providing a seed set of representative annotations that inject inductive bias and align models with task semantics. Synthetic labels from generative AI can partially substitute provided their quality is human-grade and anchored by a human-specified objective, seed supervision, and validation. In this view, generative models function as \emph{label amplifiers}, leveraging small human-curated cores via active, semi-supervised, and self-training loops, while humans retain oversight for calibration, drift detection, and failure auditing. Thus, extreme computational speed reduces wall-clock time but not the fundamental supervision needs of learning; initial human (or human-grade) input remains necessary to ground the system in the intended task.</li>
<li><strong>摘要：</strong>我们分析了一种反向监督策略，该策略搜索大型未标记集 \(B\) 的标签，以最大限度地减少小型标记集 \(A\) 上的错误。搜索空间为 \(2^n\)，即使在较大的常数因子加速（例如量子或大规模并行硬件）下，最终的复杂性仍然呈指数级增长。因此，任意快速（但不是指数级更快）的计算并不能消除对信息标签或先验的需要。在实践中，机器学习管道仍然需要最初的人类贡献：指定目标、定义类别并提供一组代表性注释，以注入归纳偏差并使模型与任务语义保持一致。来自生成人工智能的合成标签可以部分替代，只要它们的质量达到人类级别，并以人类指定的目标、种子监督和验证为基础。从这个角度来看，生成模型的功能就像\emph{标签放大器}，通过主动、半监督和自我训练循环利用小型人工核心，而人类则保留对校准、漂移检测和故障审核的监督。因此，极高的计算速度会减少挂钟时间，但不会减少学习的基本监督需求；初始的人类（或人类级）输入仍然是系统执行预期任务所必需的。</li>
</ul>

<h3>Title: Data-driven simulator of multi-animal behavior with unknown dynamics via offline and online reinforcement learning</h3>
<ul>
<li><strong>Authors: </strong>Keisuke Fujii, Kazushi Tsutsui, Yu Teshima, Makoto Itoh, Naoya Takeishi, Nozomi Nishiumi, Ryoya Tanaka, Shunsuke Shigaki, Yoshinobu Kawahara</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10451">https://arxiv.org/abs/2510.10451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10451">https://arxiv.org/pdf/2510.10451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10451]] Data-driven simulator of multi-animal behavior with unknown dynamics via offline and online reinforcement learning(https://arxiv.org/abs/2510.10451)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Simulators of animal movements play a valuable role in studying behavior. Advances in imitation learning for robotics have expanded possibilities for reproducing human and animal movements. A key challenge for realistic multi-animal simulation in biology is bridging the gap between unknown real-world transition models and their simulated counterparts. Because locomotion dynamics are seldom known, relying solely on mathematical models is insufficient; constructing a simulator that both reproduces real trajectories and supports reward-driven optimization remains an open problem. We introduce a data-driven simulator for multi-animal behavior based on deep reinforcement learning and counterfactual simulation. We address the ill-posed nature of the problem caused by high degrees of freedom in locomotion by estimating movement variables of an incomplete transition model as actions within an RL framework. We also employ a distance-based pseudo-reward to align and compare states between cyber and physical spaces. Validated on artificial agents, flies, newts, and silkmoth, our approach achieves higher reproducibility of species-specific behaviors and improved reward acquisition compared with standard imitation and RL methods. Moreover, it enables counterfactual behavior prediction in novel experimental settings and supports multi-individual modeling for flexible what-if trajectory generation, suggesting its potential to simulate and elucidate complex multi-animal behaviors.</li>
<li><strong>摘要：</strong>动物运动模拟器在行为研究中发挥着重要作用。机器人模仿学习的进步扩大了复制人类和动物运动的可能性。生物学中真实多动物模拟的一个关键挑战是弥合未知的现实世界转换模型与其模拟模型之间的差距。由于运动动力学鲜为人知，仅依靠数学模型是不够的。构建一个既能重现真实轨迹又能支持奖励驱动优化的模拟器仍然是一个悬而未决的问题。我们引入了一种基于深度强化学习和反事实模拟的数据驱动的多动物行为模拟器。我们通过将不完全转换模型的运动变量估计为 RL 框架内的动作来解决由运动的高自由度引起的问题的不适定性质。我们还采用基于距离的伪奖励来对齐和比较网络空间和物理空间之间的状态。我们的方法在人工代理、苍蝇、蝾螈和蚕身上进行了验证，与标准模仿和强化学习方法相比，我们的方法实现了物种特定行为的更高再现性，并改善了奖励获取。此外，它能够在新颖的实验环境中进行反事实行为预测，并支持多个体建模以实现灵活的假设轨迹生成，这表明它具有模拟和阐明复杂的多动物行为的潜力。</li>
</ul>

<h3>Title: Latent Retrieval Augmented Generation of Cross-Domain Protein Binders</h3>
<ul>
<li><strong>Authors: </strong>Zishen Zhang, Xiangzhe Kong, Wenbing Huang, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10480">https://arxiv.org/abs/2510.10480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10480">https://arxiv.org/pdf/2510.10480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10480]] Latent Retrieval Augmented Generation of Cross-Domain Protein Binders(https://arxiv.org/abs/2510.10480)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Designing protein binders targeting specific sites, which requires to generate realistic and functional interaction patterns, is a fundamental challenge in drug discovery. Current structure-based generative models are limited in generating nterfaces with sufficient rationality and interpretability. In this paper, we propose Retrieval-Augmented Diffusion for Aligned interface (RADiAnce), a new framework that leverages known interfaces to guide the design of novel binders. By unifying retrieval and generation in a shared contrastive latent space, our model efficiently identifies relevant interfaces for a given binding site and seamlessly integrates them through a conditional latent diffusion generator, enabling cross-domain interface transfer. Extensive exeriments show that RADiAnce significantly outperforms baseline models across multiple metrics, including binding affinity and recovery of geometries and interactions. Additional experimental results validate cross-domain generalization, demonstrating that retrieving interfaces from diverse domains, such as peptides, antibodies, and protein fragments, enhances the generation performance of binders for other domains. Our work establishes a new paradigm for protein binder design that successfully bridges retrieval-based knowledge and generative AI, opening new possibilities for drug discovery.</li>
<li><strong>摘要：</strong>设计针对特定位点的蛋白质结合剂需要生成现实且功能性的相互作用模式，这是药物发现中的一个基本挑战。当前基于结构的生成模型在生成具有足够合理性和可解释性的界面方面受到限制。在本文中，我们提出了对齐界面的检索增强扩散（RADiAnce），这是一个利用已知界面来指导新型绑定器设计的新框架。通过在共享的对比潜在空间中统一检索和生成，我们的模型有效地识别给定结合位点的相关接口，并通过条件潜在扩散生成器将它们无缝集成，从而实现跨域接口传输。广泛的实验表明，RADiAnce 在多个指标上显着优于基线模型，包括结合亲和力以及几何形状和相互作用的恢复。其他实验结果验证了跨域泛化，证明从不同域（例如肽、抗体和蛋白质片段）检索接口可以增强其他域的结合物的生成性能。我们的工作为蛋白质结合剂设计建立了一个新的范例，成功地将基于检索的知识和生成人工智能联系起来，为药物发现开辟了新的可能性。</li>
</ul>

<h3>Title: Towards Self-Refinement of Vision-Language Models with Triangular Consistency</h3>
<ul>
<li><strong>Authors: </strong>Yunlong Deng, Guangyi Chen, Tianpei Gu, Lingjing Kong, Yan Li, Zeyu Tang, Kun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10487">https://arxiv.org/abs/2510.10487</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10487">https://arxiv.org/pdf/2510.10487</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10487]] Towards Self-Refinement of Vision-Language Models with Triangular Consistency(https://arxiv.org/abs/2510.10487)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) integrate visual knowledge with the analytical capabilities of Large Language Models (LLMs) through supervised visual instruction tuning, using image-question-answer triplets. However, the potential of VLMs trained without supervised instruction remains largely unexplored. This study validates that VLMs possess inherent self-refinement capabilities, enabling them to generate high-quality supervised data without external inputs and thereby learn autonomously. Specifically, to stimulate the self-refinement ability of VLMs, we propose a self-refinement framework based on a Triangular Consistency principle: within the image-query-answer triangle, any masked elements should be consistently and accurately reconstructed. The framework involves three steps: (1) We enable the instruction generation ability of VLMs by adding multi-task instruction tuning like image$\rightarrow$question-answer or image-answer$\rightarrow$question. (2) We generate image-query-answer triplets from unlabeled images and use the Triangular Consistency principle for filtering. (3) The model is further updated using the filtered synthetic data. To investigate the underlying mechanisms behind this self-refinement capability, we conduct a theoretical analysis from a causal perspective. Using the widely recognized LLaVA-1.5 as our baseline, our experiments reveal that the model can autonomously achieve consistent, though deliberately modest, improvements across multiple benchmarks without any external supervision, such as human annotations or environmental feedback. We expect that the insights of this study on the self-refinement ability of VLMs can inspire future research on the learning mechanism of VLMs. Code is available at this https URL.</li>
<li><strong>摘要：</strong>视觉语言模型 (VLM) 通过使用图像-问题-答案三元组的监督视觉指令调整，将视觉知识与大型语言模型 (LLM) 的分析能力集成在一起。然而，在没有监督指导的情况下训练 VLM 的潜力在很大程度上仍未得到开发。这项研究验证了VLM具有固有的自我完善能力，使它们能够在没有外部输入的情况下生成高质量的监督数据，从而自主学习。具体来说，为了激发VLM的自我细化能力，我们提出了一种基于三角一致性原则的自我细化框架：在图像-查询-答案三角形内，任何被屏蔽的元素都应该一致且准确地重建。该框架涉及三个步骤：（1）我们通过添加多任务指令调整（如 image$\rightarrow$question-answer 或 image-answer$\rightarrow$question）来启用 VLM 的指令生成能力。 (2)我们从未标记的图像生成图像-查询-答案三元组，并使用三角一致性原则进行过滤。 (3)使用过滤后的合成数据进一步更新模型。为了研究这种自我完善能力背后的潜在机制，我们从因果角度进行了理论分析。使用广泛认可的 LLaVA-1.5 作为我们的基线，我们的实验表明，该模型可以在多个基准上自主地实现一致但故意适度的改进，而无需任何外部监督，例如人工注释或环境反馈。我们期望这项研究对 VLM 自我完善能力的见解能够启发未来对 VLM 学习机制的研究。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Head-wise Adaptive Rotary Positional Encoding for Fine-Grained Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiaye Li, Baoyou Chen, Hui Li, Zilong Dong, Jingdong Wang, Siyu Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10489">https://arxiv.org/abs/2510.10489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10489">https://arxiv.org/pdf/2510.10489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10489]] Head-wise Adaptive Rotary Positional Encoding for Fine-Grained Image Generation(https://arxiv.org/abs/2510.10489)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Transformers rely on explicit positional encoding to model structure in data. While Rotary Position Embedding (RoPE) excels in 1D domains, its application to image generation reveals significant limitations such as fine-grained spatial relation modeling, color cues, and object counting. This paper identifies key limitations of standard multi-dimensional RoPE-rigid frequency allocation, axis-wise independence, and uniform head treatment-in capturing the complex structural biases required for fine-grained image generation. We propose HARoPE, a head-wise adaptive extension that inserts a learnable linear transformation parameterized via singular value decomposition (SVD) before the rotary mapping. This lightweight modification enables dynamic frequency reallocation, semantic alignment of rotary planes, and head-specific positional receptive fields while rigorously preserving RoPE's relative-position property. Extensive experiments on class-conditional ImageNet and text-to-image generation (Flux and MMDiT) demonstrate that HARoPE consistently improves performance over strong RoPE baselines and other extensions. The method serves as an effective drop-in replacement, offering a principled and adaptable solution for enhancing positional awareness in transformer-based image generative models.</li>
<li><strong>摘要：</strong>Transformer 依靠显式位置编码来建模数据结构。虽然旋转位置嵌入 (RoPE) 在一维领域表现出色，但其在图像生成中的应用却暴露出显着的局限性，例如细粒度空间关系建模、颜色提示和对象计数。本文指出了标准多维 RoPE（刚性频率分配、轴方向独立性和统一头部处理）在捕获细粒度图像生成所需的复杂结构偏差方面的关键局限性。我们提出了 HARoPE，这是一种头向自适应扩展，它在旋转映射之前插入通过奇异值分解（SVD）参数化的可学习线性变换。这种轻量级修改可以实现动态频率重新分配、旋转平面的语义对齐和头部特定的位置感受野，同时严格保留 RoPE 的相对位置属性。关于类条件 ImageNet 和文本到图像生成（Flux 和 MMDiT）的大量实验表明，HARoPE 相对于强大的 RoPE 基线和其他扩展不断提高性能。该方法作为一种有效的直接替代方法，为增强基于变压器的图像生成模型中的位置感知提供了原则性且适应性强的解决方案。</li>
</ul>

<h3>Title: Jigsaw3D: Disentangled 3D Style Transfer via Patch Shuffling and Masking</h3>
<ul>
<li><strong>Authors: </strong>Yuteng Ye, Zheng Zhang, Qinchuan Zhang, Di Wang, Youjia Zhang, Wenxiao Zhang, Wei Yang, Yuan Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10497">https://arxiv.org/abs/2510.10497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10497">https://arxiv.org/pdf/2510.10497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10497]] Jigsaw3D: Disentangled 3D Style Transfer via Patch Shuffling and Masking(https://arxiv.org/abs/2510.10497)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Controllable 3D style transfer seeks to restyle a 3D asset so that its textures match a reference image while preserving the integrity and multi-view consistency. The prevalent methods either rely on direct reference style token injection or score-distillation from 2D diffusion models, which incurs heavy per-scene optimization and often entangles style with semantic content. We introduce Jigsaw3D, a multi-view diffusion based pipeline that decouples style from content and enables fast, view-consistent stylization. Our key idea is to leverage the jigsaw operation - spatial shuffling and random masking of reference patches - to suppress object semantics and isolate stylistic statistics (color palettes, strokes, textures). We integrate these style cues into a multi-view diffusion model via reference-to-view cross-attention, producing view-consistent stylized renderings conditioned on the input mesh. The renders are then style-baked onto the surface to yield seamless textures. Across standard 3D stylization benchmarks, Jigsaw3D achieves high style fidelity and multi-view consistency with substantially lower latency, and generalizes to masked partial reference stylization, multi-object scene styling, and tileable texture generation. Project page is available at: this https URL</li>
<li><strong>摘要：</strong>可控 3D 样式传输旨在重新设计 3D 资源的样式，使其纹理与参考图像匹配，同时保持完整性和多视图一致性。流行的方法要么依赖于直接引用样式标记注入，要么依赖于 2D 扩散模型的分数蒸馏，这会导致大量的每个场景优化，并且常常将样式与语义内容纠缠在一起。我们推出了 Jigsaw3D，这是一种基于多视图扩散的管道，可将风格与内容分离，并实现快速、视图一致的风格化。我们的关键思想是利用拼图操作（空间改组和参考补丁的随机屏蔽）来抑制对象语义并隔离风格统计数据（调色板、笔划、纹理）。我们通过参考视图交叉注意将这些风格提示集成到多视图扩散模型中，从而生成以输入网格为条件的视图一致的风格化渲染。然后将渲染样式烘焙到表面上以产生无缝纹理。在标准 3D 风格化基准中，Jigsaw3D 实现了高风格保真度和多视图一致性，同时显着降低了延迟，并推广到蒙版部分参考风格化、多对象场景风格化和可平铺纹理生成。项目页面位于：此 https URL</li>
</ul>

<h3>Title: A Hybrid Machine Learning Approach for Synthetic Data Generation with Post Hoc Calibration for Clinical Tabular Datasets</h3>
<ul>
<li><strong>Authors: </strong>Md Ibrahim Shikder Mahin, Md Shamsul Arefin, Md Tanvir Hasan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10513">https://arxiv.org/abs/2510.10513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10513">https://arxiv.org/pdf/2510.10513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10513]] A Hybrid Machine Learning Approach for Synthetic Data Generation with Post Hoc Calibration for Clinical Tabular Datasets(https://arxiv.org/abs/2510.10513)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Healthcare research and development face significant obstacles due to data scarcity and stringent privacy regulations, such as HIPAA and the GDPR, restricting access to essential real-world medical data. These limitations impede innovation, delay robust AI model creation, and hinder advancements in patient-centered care. Synthetic data generation offers a transformative solution by producing artificial datasets that emulate real data statistics while safeguarding patient privacy. We introduce a novel hybrid framework for high-fidelity healthcare data synthesis integrating five augmentation methods: noise injection, interpolation, Gaussian Mixture Model (GMM) sampling, Conditional Variational Autoencoder (CVAE) sampling, and SMOTE, combined via a reinforcement learning-based dynamic weight selection mechanism. Its key innovations include advanced calibration techniques -- moment matching, full histogram matching, soft and adaptive soft histogram matching, and iterative refinement -- that align marginal distributions and preserve joint feature dependencies. Evaluated on the Breast Cancer Wisconsin (UCI Repository) and Khulna Medical College cardiology datasets, our calibrated hybrid achieves Wasserstein distances as low as 0.001 and Kolmogorov-Smirnov statistics around 0.01, demonstrating near-zero marginal discrepancy. Pairwise trend scores surpass 90%, and Nearest Neighbor Adversarial Accuracy approaches 50%, confirming robust privacy protection. Downstream classifiers trained on synthetic data achieve up to 94% accuracy and F1 scores above 93%, comparable to models trained on real data. This scalable, privacy-preserving approach matches state-of-the-art methods, sets new benchmarks for joint-distribution fidelity in healthcare, and supports sensitive AI applications.</li>
<li><strong>摘要：</strong>由于数据稀缺和严格的隐私法规（例如 HIPAA 和 GDPR）限制了对真实世界重要医疗数据的访问，医疗保健研究和开发面临着重大障碍。这些限制阻碍了创新，延迟了强大的人工智能模型的创建，并阻碍了以患者为中心的护理的进步。合成数据生成通过生成模拟真实数据统计数据的人工数据集，同时保护患者隐私，提供了一种变革性的解决方案。我们引入了一种用于高保真医疗数据合成的新型混合框架，集成了五种增强方法：噪声注入、插值、高斯混合模型（GMM）采样、条件变分自动编码器（CVAE）采样和 SMOTE，并通过基于强化学习的动态权重选择机制进行组合。其关键创新包括先进的校准技术——矩匹配、完整直方图匹配、软和自适应软直方图匹配以及迭代细化——对齐边缘分布并保留联合特征依赖性。在威斯康星州乳腺癌（UCI 存储库）和库尔纳医学院心脏病学数据集上进行评估，我们的校准混合实现了低至 0.001 的 Wasserstein 距离和大约 0.01 的 Kolmogorov-Smirnov 统计数据，显示出接近于零的边际差异。配对趋势得分超过 90%，最近邻对抗准确率接近 50%，证实了强大的隐私保护。在合成数据上训练的下游分类器的准确度高达 94%，F1 分数超过 93%，与在真实数据上训练的模型相当。这种可扩展的隐私保护方法与最先进的方法相匹配，为医疗保健领域的联合分配保真度设定了新的基准，并支持敏感的人工智能应用程序。</li>
</ul>

<h3>Title: VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Qunzhong Wang, Jie Liu, Jiajun Liang, Yilei Jiang, Yuanxing Zhang, Jinyuan Chen, Yaozhi Zheng, Xintao Wang, Pengfei Wan, Xiangyu Yue, Jiaheng Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10518">https://arxiv.org/abs/2510.10518</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10518">https://arxiv.org/pdf/2510.10518</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10518]] VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning(https://arxiv.org/abs/2510.10518)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in multimodal reward models (RMs) have substantially improved post-training for visual generative models. However, current RMs face inherent limitations: (1) visual inputs consume large context budgets, forcing fewer frames and causing loss of fine-grained details; and (2) all visual information is packed into the initial prompt, exacerbating hallucination and forgetting during chain-of-thought reasoning. To overcome these issues, we introduce VideoReward Thinker (VR-Thinker), a thinking-with-image framework that equips the RM with visual reasoning operations (e.g., select frame) and a configurable visual memory window. This allows the RM to actively acquire and update visual evidence within context limits, improving reasoning fidelity and reliability. We activate visual reasoning via a reinforcement fine-tuning pipeline: (i) Cold Start with curated visual chain-of-thought data to distill basic reasoning skills and operation formatting; (ii) select samples whose per-dimension and overall judgments are all correct, then conduct Rejection sampling Fine-Tuning on these high-quality traces to further enhance reasoning; and (iii) apply Group Relative Policy Optimization (GRPO) to strengthen reasoning. Our approach delivers state-of-the-art accuracy among open-source models on video preference benchmarks, especially for longer videos: a 7B VR-Thinker achieves 80.5% on VideoGen Reward, 82.3% on GenAI-Bench, and 75.6% on MJ-Bench-Video. These results validate the effectiveness and promise of thinking-with-image multimodal reward modeling.</li>
<li><strong>摘要：</strong>多模式奖励模型（RM）的最新进展极大地改善了视觉生成模型的后训练。然而，当前的 RM 面临着固有的局限性：（1）视觉输入消耗大量的上下文预算，迫使更少的帧并导致细粒度细节的丢失； （2）所有视觉信息都被塞进最初的提示中，加剧了链式推理过程中的幻觉和遗忘。为了克服这些问题，我们引入了 VideoReward Thinker (VR-Thinker)，这是一种图像思维框架，为 RM 配备了视觉推理操作（例如，选择帧）和可配置的视觉记忆窗口。这使得 RM 能够在上下文限制内主动获取和更新视觉证据，从而提高推理保真度和可靠性。我们通过强化微调管道激活视觉推理：（i）使用精心策划的视觉思维链数据进行冷启动，以提炼基本推理技能和操作格式； (ii) 选择每维度和整体判断都正确的样本，然后对这些高质量迹线进行拒绝采样Fine-Tuning，以进一步增强推理； (iii) 应用组相对策略优化 (GRPO) 来加强推理。我们的方法在视频偏好基准测试中提供了开源模型中最先进的准确性，特别是对于较长的视频：7B VR-Thinker 在 VideoGen Reward 上达到 80.5%，在 GenAI-Bench 上达到 82.3%，在 MJ-Bench-Video 上达到 75.6%。这些结果验证了图像思维多模式奖励模型的有效性和前景。</li>
</ul>

<h3>Title: UniFlow: A Unified Pixel Flow Tokenizer for Visual Understanding and Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhengrong Yue, Haiyu Zhang, Xiangyu Zeng, Boyu Chen, Chenting Wang, Shaobin Zhuang, Lu Dong, KunPeng Du, Yi Wang, Limin Wang, Yali Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10575">https://arxiv.org/abs/2510.10575</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10575">https://arxiv.org/pdf/2510.10575</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10575]] UniFlow: A Unified Pixel Flow Tokenizer for Visual Understanding and Generation(https://arxiv.org/abs/2510.10575)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Tokenizer is a crucial component for both visual understanding and generation. To advance toward the ultimate goal of universal modeling, recent research has focused on developing a unified tokenizer. However, existing tokenizers face a significant performance trade-off between understanding and generation, stemming from the inherent conflict between high-level semantic abstraction and low-level pixel reconstruction. To tackle this challenge, we propose a generic and unified tokenizer, namely UniFlow, by flexibly adapting any visual encoder with a concise reconstruction decoder. Specifically, we introduce layer-wise adaptive self-distillation applied to the well-pretrained visual encoders, which enables UniFlow to simultaneously inherit the strong semantic features for visual understanding and flexibly adapt to model fine-grained details for visual generation. Moreover, we propose a lightweight patch-wise pixel flow decoder, which efficiently achieves high-fidelity pixel reconstruction by modeling a conditional flow from the noisy state back to the patch-wise pixel domain. By leveraging the semantic features as visual conditions for the decoder, we effectively alleviate the training conflicts between understanding and generation. Furthermore, the patch-wise learning strategy simplifies the data distribution, thereby improving training efficiency. Extensive experiments across 13 challenging benchmarks spanning 7 widely studied visual understanding and generation tasks demonstrate that UniFlow achieves a win-win outcome. For instance, our 7B UniFlow-XL not only surpasses the 14B TokenFlow-XL by 7.75% on average understanding benchmarks, but also achieves competitive results in both visual reconstruction and generation, surpassing UniTok by 0.15 in rFID and 0.09 in gFID (without guidance), respectively.</li>
<li><strong>摘要：</strong>分词器是视觉理解和生成的关键组件。为了实现通用建模的最终目标，最近的研究重点是开发统一的分词器。然而，由于高级语义抽象和低级像素重建之间的固有冲突，现有的分词器面临着理解和生成之间的显着性能权衡。为了应对这一挑战，我们提出了一种通用且统一的分词器，即 UniFlow，通过使用简洁的重建解码器灵活地适应任何视觉编码器。具体来说，我们引入了应用于经过良好预训练的视觉编码器的逐层自适应自蒸馏，这使得 UniFlow 能够同时继承用于视觉理解的强语义特征，并灵活地适应模型细粒度细节以进行视觉生成。此外，我们提出了一种轻量级的逐块像素流解码器，它通过建模从噪声状态回到逐块像素域的条件流来有效地实现高保真像素重建。通过利用语义特征作为解码器的视觉条件，我们有效地缓解了理解和生成之间的训练冲突。此外，patch-wise学习策略简化了数据分布，从而提高了训练效率。涵盖 7 个广泛研究的视觉理解和生成任务的 13 个具有挑战性的基准的广泛实验表明 UniFlow 实现了双赢的结果。例如，我们的 7B UniFlow-XL 不仅在平均理解基准上超过 14B TokenFlow-XL 7.75%，而且在视觉重建和生成方面都取得了有竞争力的结果，在 rFID 中超过 UniTok 0.15，在 gFID 中超过 UniTok 0.09（无引导）。</li>
</ul>

<h3>Title: Injecting Frame-Event Complementary Fusion into Diffusion for Optical Flow in Challenging Scenes</h3>
<ul>
<li><strong>Authors: </strong>Haonan Wang, Hanyu Zhou, Haoyue Liu, Luxin Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10577">https://arxiv.org/abs/2510.10577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10577">https://arxiv.org/pdf/2510.10577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10577]] Injecting Frame-Event Complementary Fusion into Diffusion for Optical Flow in Challenging Scenes(https://arxiv.org/abs/2510.10577)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Optical flow estimation has achieved promising results in conventional scenes but faces challenges in high-speed and low-light scenes, which suffer from motion blur and insufficient illumination. These conditions lead to weakened texture and amplified noise and deteriorate the appearance saturation and boundary completeness of frame cameras, which are necessary for motion feature matching. In degraded scenes, the frame camera provides dense appearance saturation but sparse boundary completeness due to its long imaging time and low dynamic range. In contrast, the event camera offers sparse appearance saturation, while its short imaging time and high dynamic range gives rise to dense boundary completeness. Traditionally, existing methods utilize feature fusion or domain adaptation to introduce event to improve boundary completeness. However, the appearance features are still deteriorated, which severely affects the mostly adopted discriminative models that learn the mapping from visual features to motion fields and generative models that generate motion fields based on given visual features. So we introduce diffusion models that learn the mapping from noising flow to clear flow, which is not affected by the deteriorated visual features. Therefore, we propose a novel optical flow estimation framework Diff-ABFlow based on diffusion models with frame-event appearance-boundary fusion.</li>
<li><strong>摘要：</strong>光流估计在传统场景中取得了可喜的结果，但在高速和弱光场景中面临着运动模糊和照明不足的挑战。这些条件导致纹理减弱和噪声放大，并恶化帧相机的外观饱和度和边界完整性，而这是运动特征匹配所必需的。在退化场景中，帧相机由于其长成像时间和低动态范围而提供密集的外观饱和度但稀疏的边界完整性。相比之下，事件相机提供稀疏的外观饱和度，而其较短的成像时间和高动态范围则带来密集的边界完整性。传统上，现有方法利用特征融合或域适应来引入事件以提高边界完整性。然而，外观特征仍然恶化，这严重影响了大多数采用的学习从视觉特征到运动场的映射的判别模型以及基于给定视觉特征生成运动场的生成模型。因此，我们引入了扩散模型，该模型可以学习从噪声流到清晰流的映射，而不受视觉特征恶化的影响。因此，我们提出了一种基于帧-事件外观-边界融合的扩散模型的新型光流估计框架Diff-ABFlow。</li>
</ul>

<h3>Title: Compositional Symmetry as Compression: Lie Pseudogroup Structure in Algorithmic Agents</h3>
<ul>
<li><strong>Authors: </strong>Giulio Ruffini</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IT, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10586">https://arxiv.org/abs/2510.10586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10586">https://arxiv.org/pdf/2510.10586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10586]] Compositional Symmetry as Compression: Lie Pseudogroup Structure in Algorithmic Agents(https://arxiv.org/abs/2510.10586)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In the algorithmic (Kolmogorov) view, agents are programs that track and compress sensory streams using generative programs. We propose a framework where the relevant structural prior is simplicity (Solomonoff) understood as \emph{compositional symmetry}: natural streams are well described by (local) actions of finite-parameter Lie pseudogroups on geometrically and topologically complex low-dimensional configuration manifolds (latent spaces). Modeling the agent as a generic neural dynamical system coupled to such streams, we show that accurate world-tracking imposes (i) \emph{structural constraints} -- equivariance of the agent's constitutive equations and readouts -- and (ii) \emph{dynamical constraints}: under static inputs, symmetry induces conserved quantities (Noether-style labels) in the agent dynamics and confines trajectories to reduced invariant manifolds; under slow drift, these manifolds move but remain low-dimensional. This yields a hierarchy of reduced manifolds aligned with the compositional factorization of the pseudogroup, providing a geometric account of the ``blessing of compositionality'' in deep models. We connect these ideas to the Spencer formalism for Lie pseudogroups and formulate a symmetry-based, self-contained version of predictive coding in which higher layers receive only \emph{coarse-grained residual transformations} (prediction-error coordinates) along symmetry directions unresolved at lower layers.</li>
<li><strong>摘要：</strong>在算法（柯尔莫哥洛夫）观点中，代理是使用生成程序跟踪和压缩感知流的程序。我们提出了一个框架，其中相关的结构先验是简单性（所罗门诺夫），被理解为 \emph{组合对称性}：自然流可以通过有限参数李伪群在几何和拓扑复杂的低维配置流形（潜在空间）上的（局部）行为来很好地描述。将智能体建模为与此类流耦合的通用神经动力学系统，我们表明准确的世界跟踪会施加（i）\emph{结构约束}——智能体本构方程和读数的等变性——以及（ii）\emph{动态约束}：在静态输入下，对称性会在智能体动力学中引入守恒量（诺特式标签），并且 将轨迹限制为简化的不变流形；在缓慢漂移的情况下，这些流形会移动但仍保持低维。这产生了与伪群的组合因式分解一致的约化流形的层次结构，提供了深度模型中“组合性的祝福”的几何解释。我们将这些想法与李伪群的斯宾塞形式主义联系起来，并制定了一种基于对称性的、独立的预测编码版本，其中较高层仅沿较低层未解析的对称方向接收 \emph{粗粒度残差变换}（预测误差坐标）。</li>
</ul>

<h3>Title: FusionGen: Feature Fusion-Based Few-Shot EEG Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Yuheng Chen, Dingkun Liu, Xinyao Yang, Xinping Xu, Baicheng Chen, Dongrui Wu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10604">https://arxiv.org/abs/2510.10604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10604">https://arxiv.org/pdf/2510.10604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10604]] FusionGen: Feature Fusion-Based Few-Shot EEG Data Generation(https://arxiv.org/abs/2510.10604)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Brain-computer interfaces (BCIs) provide potential for applications ranging from medical rehabilitation to cognitive state assessment by establishing direct communication pathways between the brain and external devices via electroencephalography (EEG). However, EEG-based BCIs are severely constrained by data scarcity and significant inter-subject variability, which hinder the generalization and applicability of EEG decoding models in practical settings. To address these challenges, we propose FusionGen, a novel EEG data generation framework based on disentangled representation learning and feature fusion. By integrating features across trials through a feature matching fusion module and combining them with a lightweight feature extraction and reconstruction pipeline, FusionGen ensures both data diversity and trainability under limited data constraints. Extensive experiments on multiple publicly available EEG datasets demonstrate that FusionGen significantly outperforms existing augmentation techniques, yielding notable improvements in classification accuracy.</li>
<li><strong>摘要：</strong>脑机接口（BCI）通过脑电图（EEG）在大脑和外部设备之间建立直接通信路径，为从医疗康复到认知状态评估等应用提供了潜力。然而，基于脑电图的脑机接口受到数据稀缺和显着的受试者间变异性的严重限制，这阻碍了脑电图解码模型在实际环境中的泛化和适用性。为了应对这些挑战，我们提出了 FusionGen，一种基于解缠表示学习和特征融合的新型脑电图数据生成框架。通过特征匹配融合模块集成跨试验的特征，并将其与轻量级特征提取和重建管道相结合，FusionGen 确保了有限数据约束下的数据多样性和可训练性。对多个公开可用的脑电图数据集进行的广泛实验表明，FusionGen 显着优于现有的增强技术，在分类准确性方面产生了显着提高。</li>
</ul>

<h3>Title: OmniQuality-R: Advancing Reward Models Through All-Encompassing Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Yiting Lu, Fengbin Guan, Yixin Gao, Yan Zhong, Xinge Peng, Jiakang Yuan, Yihao Liu, Bo Zhang, Xin Li, Zhibo Chen, Weisi Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10609">https://arxiv.org/abs/2510.10609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10609">https://arxiv.org/pdf/2510.10609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10609]] OmniQuality-R: Advancing Reward Models Through All-Encompassing Quality Assessment(https://arxiv.org/abs/2510.10609)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Current visual evaluation approaches are typically constrained to a single task. To address this, we propose OmniQuality-R, a unified reward modeling framework that transforms multi-task quality reasoning into continuous and interpretable reward signals for policy optimization. Inspired by subjective experiments, where participants are given task-specific instructions outlining distinct assessment principles prior to evaluation, we propose OmniQuality-R, a structured reward modeling framework that transforms multi-dimensional reasoning into continuous and interpretable reward signals. To enable this, we construct a reasoning-enhanced reward modeling dataset by sampling informative plan-reason trajectories via rejection sampling, forming a reliable chain-of-thought (CoT) dataset for supervised fine-tuning (SFT). Building on this, we apply Group Relative Policy Optimization (GRPO) for post-training, using a Gaussian-based reward to support continuous score prediction. To further stabilize the training and improve downstream generalization, we incorporate standard deviation (STD) filtering and entropy gating mechanisms during reinforcement learning. These techniques suppress unstable updates and reduce variance in policy optimization. We evaluate OmniQuality-R on three key IQA tasks: aesthetic quality assessment, technical quality evaluation, and text-image alignment.</li>
<li><strong>摘要：</strong>当前的视觉评估方法通常仅限于单个任务。为了解决这个问题，我们提出了 OmniQuality-R，这是一个统一的奖励建模框架，可将多任务质量推理转换为连续且可解释的奖励信号以进行策略优化。受主观实验的启发，参与者在评估之前会收到概述不同评估原则的特定任务指令，我们提出了 OmniQuality-R，这是一种结构化奖励建模框架，可将多维推理转换为连续且可解释的奖励信号。为了实现这一点，我们通过拒绝采样对信息丰富的计划推理轨迹进行采样，构建了推理增强奖励建模数据集，形成了用于监督微调（SFT）的可靠的思想链（CoT）数据集。在此基础上，我们应用组相对策略优化（GRPO）进行后训练，使用基于高斯的奖励来支持连续分数预测。为了进一步稳定训练并提高下游泛化能力，我们在强化学习期间结合了标准差（STD）过滤和熵门机制。这些技术抑制不稳定的更新并减少策略优化中的方差。我们在三个关键 IQA 任务上评估 OmniQuality-R：美学质量评估、技术质量评估和文本图像对齐。</li>
</ul>

<h3>Title: Encoder Decoder Generative Adversarial Network Model for Stock Market Prediction</h3>
<ul>
<li><strong>Authors: </strong>Bahadur Yadav, Sanjay Kumar Mohanty</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10617">https://arxiv.org/abs/2510.10617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10617">https://arxiv.org/pdf/2510.10617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10617]] Encoder Decoder Generative Adversarial Network Model for Stock Market Prediction(https://arxiv.org/abs/2510.10617)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Forecasting stock prices remains challenging due to the volatile and non-linear nature of financial markets. Despite the promise of deep learning, issues such as mode collapse, unstable training, and difficulty in capturing temporal and feature level correlations have limited the applications of GANs in this domain. We propose a GRU-based Encoder-Decoder GAN (EDGAN) model that strikes a balance between expressive power and simplicity. The model introduces key innovations such as a temporal decoder with residual connections for precise reconstruction, conditioning on static and dynamic covariates for contextual learning, and a windowing mechanism to capture temporal dynamics. Here, the generator uses a dense encoder-decoder framework with residual GRU blocks. Extensive experiments on diverse stock datasets demonstrate that EDGAN achieves superior forecasting accuracy and training stability, even in volatile markets. It consistently outperforms traditional GAN variants in forecasting accuracy and convergence stability under market conditions.</li>
<li><strong>摘要：</strong>由于金融市场的波动性和非线性性质，预测股票价格仍然具有挑战性。尽管深度学习前景广阔，但模式崩溃、训练不稳定以及难以捕获时间和特征级别相关性等问题限制了 GAN 在该领域的应用。我们提出了一种基于 GRU 的编码器-解码器 GAN (EDGAN) 模型，该模型在表达能力和简单性之间取得了平衡。该模型引入了关键创新，例如具有用于精确重建的残差连接的时间解码器、用于上下文学习的静态和动态协变量的调节，以及用于捕获时间动态的窗口机制。这里，生成器使用带有残差 GRU 块的密集编码器-解码器框架。对不同股票数据集的大量实验表明，即使在波动的市场中，EDGAN 也能实现卓越的预测准确性和训练稳定性。它在市场条件下的预测准确性和收敛稳定性方面始终优于传统的 GAN 变体。</li>
</ul>

<h3>Title: ProteinAE: Protein Diffusion Autoencoders for Structure Encoding</h3>
<ul>
<li><strong>Authors: </strong>Shaoning Li, Le Zhuo, Yusong Wang, Mingyu Li, Xinheng He, Fandi Wu, Hongsheng Li, Pheng-Ann Heng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10634">https://arxiv.org/abs/2510.10634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10634">https://arxiv.org/pdf/2510.10634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10634]] ProteinAE: Protein Diffusion Autoencoders for Structure Encoding(https://arxiv.org/abs/2510.10634)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Developing effective representations of protein structures is essential for advancing protein science, particularly for protein generative modeling. Current approaches often grapple with the complexities of the SE(3) manifold, rely on discrete tokenization, or the need for multiple training objectives, all of which can hinder the model optimization and generalization. We introduce ProteinAE, a novel and streamlined protein diffusion autoencoder designed to overcome these challenges by directly mapping protein backbone coordinates from E(3) into a continuous, compact latent space. ProteinAE employs a non-equivariant Diffusion Transformer with a bottleneck design for efficient compression and is trained end-to-end with a single flow matching objective, substantially simplifying the optimization pipeline. We demonstrate that ProteinAE achieves state-of-the-art reconstruction quality, outperforming existing autoencoders. The resulting latent space serves as a powerful foundation for a latent diffusion model that bypasses the need for explicit equivariance. This enables efficient, high-quality structure generation that is competitive with leading structure-based approaches and significantly outperforms prior latent-based methods. Code is available at this https URL.</li>
<li><strong>摘要：</strong>开发蛋白质结构的有效表示对于推进蛋白质科学，特别是蛋白质生成模型至关重要。当前的方法通常会解决 SE(3) 流形的复杂性，依赖离散标记化或需要多个训练目标，所有这些都会阻碍模型优化和泛化。我们引入 ProteinAE，这是一种新颖且简化的蛋白质扩散自动编码器，旨在通过直接将 E(3) 中的蛋白质主链坐标映射到连续、紧凑的潜在空间来克服这些挑战。 ProteinAE 采用具有瓶颈设计的非等变扩散变压器以实现高效压缩，并使用单个流匹配目标进行端到端训练，从而大大简化了优化流程。我们证明 ProteinAE 实现了最先进的重建质量，优于现有的自动编码器。由此产生的潜在空间作为潜在扩散模型的强大基础，绕过了显式等方差的需要。这使得高效、高质量的结构生成能够与领先的基于结构的方法竞争，并显着优于先前的基于潜在的方法。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Trustworthy Retrosynthesis: Eliminating Hallucinations with a Diverse Ensemble of Reaction Scorers</h3>
<ul>
<li><strong>Authors: </strong>Michal Sadowski, Maria Wyrzykowska, Lukasz Sztukiewicz, Tadija Radusinović, Jan Rzymkowski, Paweł Włodarczyk-Pruszyński, Mikołaj Sacha, Piotr Kozakowski, Ruard van Workum, Stanislaw Kamil Jastrzebski</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10645">https://arxiv.org/abs/2510.10645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10645">https://arxiv.org/pdf/2510.10645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10645]] Trustworthy Retrosynthesis: Eliminating Hallucinations with a Diverse Ensemble of Reaction Scorers(https://arxiv.org/abs/2510.10645)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Retrosynthesis is one of the domains transformed by the rise of generative models, and it is one where the problem of nonsensical or erroneous outputs (hallucinations) is particularly insidious: reliable assessment of synthetic plans is time-consuming, with automatic methods lacking. In this work, we present RetroTrim, a retrosynthesis system that successfully avoids nonsensical plans on a set of challenging drug-like targets. Compared to common baselines in the field, our system is not only the sole method that succeeds in filtering out hallucinated reactions, but it also results in the highest number of high-quality paths overall. The key insight behind RetroTrim is the combination of diverse reaction scoring strategies, based on machine learning models and existing chemical databases. We show that our scoring strategies capture different classes of hallucinations by analyzing them on a dataset of labeled retrosynthetic intermediates. To measure the performance of retrosynthesis systems, we propose a novel evaluation protocol for reactions and synthetic paths based on a structured review by expert chemists. Using this protocol, we compare systems on a set of 32 novel targets, curated to reflect recent trends in drug structures. While the insights behind our methodology are broadly applicable to retrosynthesis, our focus is on targets in the drug-like domain. By releasing our benchmark targets and the details of our evaluation protocol, we hope to inspire further research into reliable retrosynthesis.</li>
<li><strong>摘要：</strong>逆合成是生成模型兴起所改变的领域之一，也是其中无意义或错误输出（幻觉）问题特别隐蔽的领域：对合成计划的可靠评估非常耗时，而且缺乏自动方法。在这项工作中，我们提出了 RetroTrim，一种逆合成系统，它成功地避免了对一组具有挑战性的类药物靶标的无意义计划。与该领域的常见基线相比，我们的系统不仅是成功过滤幻觉反应的唯一方法，而且总体上还产生了最高数量的高质量路径。 RetroTrim 背后的关键见解是基于机器学习模型和现有化学数据库的多种反应评分策略的组合。我们表明，我们的评分策略通过在标记的逆合成中间体数据集上进行分析来捕获不同类别的幻觉。为了衡量逆合成系统的性能，我们根据化学专家的结构化审查提出了一种新颖的反应和合成路径评估方案。使用该协议，我们比较了一组 32 个新靶标的系统，这些靶标旨在反映药物结构的最新趋势。虽然我们的方法背后的见解广泛适用于逆合成，但我们的重点是类药物领域的靶标。通过发布我们的基准目标和评估方案的详细信息，我们希望激发对可靠逆合成的进一步研究。</li>
</ul>

<h3>Title: DEMO: Disentangled Motion Latent Flow Matching for Fine-Grained Controllable Talking Portrait Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Peiyin Chen, Zhuowei Yang, Hui Feng, Sheng Jiang, Rui Yan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10650">https://arxiv.org/abs/2510.10650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10650">https://arxiv.org/pdf/2510.10650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10650]] DEMO: Disentangled Motion Latent Flow Matching for Fine-Grained Controllable Talking Portrait Synthesis(https://arxiv.org/abs/2510.10650)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Audio-driven talking-head generation has advanced rapidly with diffusion-based generative models, yet producing temporally coherent videos with fine-grained motion control remains challenging. We propose DEMO, a flow-matching generative framework for audio-driven talking-portrait video synthesis that delivers disentangled, high-fidelity control of lip motion, head pose, and eye gaze. The core contribution is a motion auto-encoder that builds a structured latent space in which motion factors are independently represented and approximately orthogonalized. On this disentangled motion space, we apply optimal-transport-based flow matching with a transformer predictor to generate temporally smooth motion trajectories conditioned on audio. Extensive experiments across multiple benchmarks show that DEMO outperforms prior methods in video realism, lip-audio synchronization, and motion fidelity. These results demonstrate that combining fine-grained motion disentanglement with flow-based generative modeling provides a powerful new paradigm for controllable talking-head video synthesis.</li>
<li><strong>摘要：</strong>音频驱动的头部说话的生成通过基于扩散的生成模型迅速发展，但通过细粒度运动控制生成时间连贯的视频仍然具有挑战性。我们提出了 DEMO，这是一种用于音频驱动的谈话肖像视频合成的流程匹配生成框架，可以对嘴唇运动、头部姿势和眼睛注视提供解开的高保真控制。核心贡献是运动自动编码器，它构建了一个结构化的潜在空间，其中运动因子被独立表示并近似正交。在这个解开的运动空间上，我们应用基于最佳传输的流匹配与变压器预测器来生成以音频为条件的时间平滑的运动轨迹。跨多个基准的大量实验表明，DEMO 在视频真实感、唇音同步和运动保真度方面优于先前的方法。这些结果表明，将细粒度运动解缠与基于流的生成建模相结合，为可控头部说话视频合成提供了强大的新范例。</li>
</ul>

<h3>Title: AdaViewPlanner: Adapting Video Diffusion Models for Viewpoint Planning in 4D Scenes</h3>
<ul>
<li><strong>Authors: </strong>Yu Li, Menghan Xia, Gongye Liu, Jianhong Bai, Xintao Wang, Conglang Zhang, Yuxuan Lin, Ruihang Chu, Pengfei Wan, Yujiu Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10670">https://arxiv.org/abs/2510.10670</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10670">https://arxiv.org/pdf/2510.10670</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10670]] AdaViewPlanner: Adapting Video Diffusion Models for Viewpoint Planning in 4D Scenes(https://arxiv.org/abs/2510.10670)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent Text-to-Video (T2V) models have demonstrated powerful capability in visual simulation of real-world geometry and physical laws, indicating its potential as implicit world models. Inspired by this, we explore the feasibility of leveraging the video generation prior for viewpoint planning from given 4D scenes, since videos internally accompany dynamic scenes with natural viewpoints. To this end, we propose a two-stage paradigm to adapt pre-trained T2V models for viewpoint prediction, in a compatible manner. First, we inject the 4D scene representation into the pre-trained T2V model via an adaptive learning branch, where the 4D scene is viewpoint-agnostic and the conditional generated video embeds the viewpoints visually. Then, we formulate viewpoint extraction as a hybrid-condition guided camera extrinsic denoising process. Specifically, a camera extrinsic diffusion branch is further introduced onto the pre-trained T2V model, by taking the generated video and 4D scene as input. Experimental results show the superiority of our proposed method over existing competitors, and ablation studies validate the effectiveness of our key technical designs. To some extent, this work proves the potential of video generation models toward 4D interaction in real world.</li>
<li><strong>摘要：</strong>最近的文本到视频（T2V）模型在现实世界几何和物理定律的视觉模拟方面表现出了强大的能力，表明其作为隐式世界模型的潜力。受此启发，我们探索了利用视频生成之前从给定 4D 场景进行视点规划的可行性，因为视频内部伴随着具有自然视点的动态场景。为此，我们提出了一种两阶段范例，以兼容的方式调整预训练的 T2V 模型以进行视点预测。首先，我们通过自适应学习分支将 4D 场景表示注入到预先训练的 T2V 模型中，其中 4D 场景与视点无关，而条件生成的视频以视觉方式嵌入视点。然后，我们将视点提取公式化为混合条件引导的相机外在降噪过程。具体来说，通过将生成的视频和 4D 场景作为输入，将相机外在扩散分支进一步引入到预训练的 T2V 模型中。实验结果表明我们提出的方法相对于现有竞争对手的优越性，消融研究验证了我们关键技术设计的有效性。在某种程度上，这项工作证明了视频生成模型在现实世界中实现 4D 交互的潜力。</li>
</ul>

<h3>Title: Image-to-Video Transfer Learning based on Image-Language Foundation Models: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Jinxuan Li, Chaolei Tan, Haoxuan Chen, Jianxin Ma, Jian-Fang Hu, Wei-Shi Zheng, Jianhuang Lai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10671">https://arxiv.org/abs/2510.10671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10671">https://arxiv.org/pdf/2510.10671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10671]] Image-to-Video Transfer Learning based on Image-Language Foundation Models: A Comprehensive Survey(https://arxiv.org/abs/2510.10671)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Image-Language Foundation Models (ILFM) have demonstrated remarkable success in image-text understanding/generation tasks, providing transferable multimodal representations that generalize across diverse downstream image-based tasks. The advancement of video-text research has spurred growing interest in extending image-based models to the video domain. This paradigm, known as image-to-video transfer learning, succeeds in alleviating the substantial data and computational requirements associated with training video-language foundation models from scratch for video-text learning. This survey provides the first comprehensive review of this emerging field, which begins by summarizing the widely used ILFM and their capabilities. We then systematically classify existing image-to-video transfer learning strategies into two categories: frozen features and modified features, depending on whether the original representations from ILFM are preserved or undergo modifications. Building upon the task-specific nature of image-to-video transfer, this survey methodically elaborates these strategies and details their applications across a spectrum of video-text learning tasks, ranging from fine-grained (e.g., spatio-temporal video grounding) to coarse-grained (e.g., video question answering). We further present a detailed experimental analysis to investigate the efficacy of different image-to-video transfer learning paradigms on a range of downstream video understanding tasks. Finally, we identify prevailing challenges and highlight promising directions for future research. By offering a comprehensive and structured overview, this survey aims to establish a structured roadmap for advancing video-text learning based on existing ILFM, and to inspire future research directions in this rapidly evolving domain.</li>
<li><strong>摘要：</strong>图像语言基础模型（ILFM）在图像文本理解/生成任务中取得了显着的成功，提供了可迁移的多模态表示，可以泛化到不同的下游基于图像的任务。视频文本研究的进步激发了人们对将基于图像的模型扩展到视频领域的兴趣。这种范例被称为图像到视频的迁移学习，成功地减轻了与从头开始训练视频语言基础模型以进行视频文本学习相关的大量数据和计算要求。这项调查首次全面回顾了这一新兴领域，首先总结了广泛使用的 ILFM 及其功能。然后，我们将现有的图像到视频迁移学习策略系统地分为两类：冻结特征和修改特征，具体取决于 ILFM 的原始表示是否被保留或进行修改。基于图像到视频传输的特定任务性质，本次调查系统地阐述了这些策略，并详细介绍了它们在一系列视频文本学习任务中的应用，从细粒度（例如，时空视频基础）到粗粒度（例如，视频问答）。我们进一步提出了详细的实验分析，以研究不同图像到视频迁移学习范例在一系列下游视频理解任务上的功效。最后，我们确定了当前的挑战并强调了未来研究的有希望的方向。通过提供全面和结构化的概述，本调查旨在建立一个基于现有 ILFM 的推进视频文本学习的结构化路线图，并激发这个快速发展领域的未来研究方向。</li>
</ul>

<h3>Title: Digital Twin-enabled Multi-generation Control Co-Design with Deep Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Ying-Kuan Tsai, Vispi Karkaria, Yi-Ping Chen, Wei Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10694">https://arxiv.org/abs/2510.10694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10694">https://arxiv.org/pdf/2510.10694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10694]] Digital Twin-enabled Multi-generation Control Co-Design with Deep Reinforcement Learning(https://arxiv.org/abs/2510.10694)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Control Co-Design (CCD) integrates physical and control system design to improve the performance of dynamic and autonomous systems. Despite advances in uncertainty-aware CCD methods, real-world uncertainties remain highly unpredictable. Multi-generation design addresses this challenge by considering the full lifecycle of a product: data collected from each generation informs the design of subsequent generations, enabling progressive improvements in robustness and efficiency. Digital Twin (DT) technology further strengthens this paradigm by creating virtual representations that evolve over the lifecycle through real-time sensing, model updating, and adaptive re-optimization. This paper presents a DT-enabled CCD framework that integrates Deep Reinforcement Learning (DRL) to jointly optimize physical design and controller. DRL accelerates real-time decision-making by allowing controllers to continuously learn from data and adapt to uncertain environments. Extending this approach, the framework employs a multi-generation paradigm, where each cycle of deployment, operation, and redesign uses collected data to refine DT models, improve uncertainty quantification through quantile regression, and inform next-generation designs of both physical components and controllers. The framework is demonstrated on an active suspension system, where DT-enabled learning from road conditions and driving behaviors yields smoother and more stable control trajectories. Results show that the method significantly enhances dynamic performance, robustness, and efficiency. Contributions of this work include: (1) extending CCD into a lifecycle-oriented multi-generation framework, (2) leveraging DTs for continuous model updating and informed design, and (3) employing DRL to accelerate adaptive real-time decision-making.</li>
<li><strong>摘要：</strong>控制协同设计 (CCD) 集成了物理和控制系统设计，以提高动态和自主系统的性能。尽管不确定性感知 CCD 方法取得了进步，但现实世界的不确定性仍然高度不可预测。多代设计通过考虑产品的整个生命周期来解决这一挑战：从每一代收集的数据为后续几代的设计提供信息，从而逐步提高稳健性和效率。数字孪生 (DT) 技术通过实时传感、模型更新和自适应重新优化创建在生命周期中不断演变的虚拟表示，进一步强化了这一范式。本文提出了一种支持 DT 的 CCD 框架，该框架集成了深度强化学习 (DRL)，以联合优化物理设计和控制器。 DRL 通过允许控制器不断地从数据中学习并适应不确定的环境来加速实时决策。该框架扩展了这种方法，采用了多代范式，其中部署、操作和重新设计的每个周期都使用收集的数据来完善DT模型，通过分位数回归改进不确定性量化，并为物理组件和控制器的下一代设计提供信息。该框架在主动悬架系统上进行了演示，其中基于 DT 的路况和驾驶行为学习可产生更平滑、更稳定的控制轨迹。结果表明，该方法显着提高了动态性能、鲁棒性和效率。这项工作的贡献包括：（1）将 CCD 扩展到面向生命周期的多代框架，（2）利用 DT 进行持续模型更新和知情设计，以及（3）利用 DRL 加速自适应实时决策。</li>
</ul>

<h3>Title: Designing ReLU Generative Networks to Enumerate Trees with a Given Tree Edit Distance</h3>
<ul>
<li><strong>Authors: </strong>Mamoona Ghafoor, Tatsuya Akutsu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10706">https://arxiv.org/abs/2510.10706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10706">https://arxiv.org/pdf/2510.10706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10706]] Designing ReLU Generative Networks to Enumerate Trees with a Given Tree Edit Distance(https://arxiv.org/abs/2510.10706)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The generation of trees with a specified tree edit distance has significant applications across various fields, including computational biology, structured data analysis, and image processing. Recently, generative networks have been increasingly employed to synthesize new data that closely resembles the original datasets. However, the appropriate size and depth of generative networks required to generate data with a specified tree edit distance remain unclear. In this paper, we theoretically establish the existence and construction of generative networks capable of producing trees similar to a given tree with respect to the tree edit distance. Specifically, for a given rooted, ordered, and vertex-labeled tree T of size n + 1 with labels from an alphabet \Sigma, and a non-negative integer d, we prove that all rooted, ordered, and vertex-labeled trees over \Sigma with tree edit distance at most d from T can be generated using a ReLU-based generative network with size O(n^3 ) and constant depth. The proposed networks were implemented and evaluated for generating trees with up to 21 nodes. Due to their deterministic architecture, the networks successfully generated all valid trees within the specified tree edit distance. In contrast, state-of-the-art graph generative models GraphRNN and GraphGDP, which rely on non-deterministic mechanisms, produced significantly fewer valid trees, achieving validation rates of only up to 35% and 48%, respectively. These findings provide a theoretical foundation towards construction of compact generative models and open new directions for exact and valid tree-structured data generation. An implementation of the proposed networks is available at this https URL.</li>
<li><strong>摘要：</strong>具有指定树编辑距离的树的生成在各个领域具有重要的应用，包括计算生物学、结构化数据分析和图像处理。最近，生成网络越来越多地被用来合成与原始数据集非常相似的新数据。然而，生成具有指定树编辑距离的数据所需的生成网络的适当大小和深度仍不清楚。在本文中，我们从理论上建立了生成网络的存在和构造，该生成网络能够生成与给定树在树编辑距离方面相似的树。具体来说，对于给定的大小为 n + 1 的有根、有序和顶点标记树 T，其标签来自字母表 Sigma 和非负整数 d，我们证明所有在 Sigma 上且距 T 的树编辑距离最大为 d 的有根、有序和顶点标记树可以使用大小为 O(n^3 ) 和恒定深度的基于 ReLU 的生成网络生成。所提出的网络已实现并评估用于生成最多 21 个节点的树。由于其确定性架构，网络成功生成了指定树编辑距离内的所有有效树。相比之下，最先进的图生成模型 GraphRNN 和 GraphGDP 依赖于非确定性机制，生成的有效树明显较少，验证率分别仅为 35% 和 48%。这些发现为构建紧凑的生成模型提供了理论基础，并为精确有效的树结构数据生成开辟了新的方向。所提议网络的实现可在此 https URL 上找到。</li>
</ul>

<h3>Title: A Stochastic Differential Equation Framework for Multi-Objective LLM Interactions: Dynamical Systems Analysis with Code Generation Applications</h3>
<ul>
<li><strong>Authors: </strong>Shivani Shukla, Himanshu Joshi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10739">https://arxiv.org/abs/2510.10739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10739">https://arxiv.org/pdf/2510.10739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10739]] A Stochastic Differential Equation Framework for Multi-Objective LLM Interactions: Dynamical Systems Analysis with Code Generation Applications(https://arxiv.org/abs/2510.10739)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce a general stochastic differential equation framework for modelling multiobjective optimization dynamics in iterative Large Language Model (LLM) interactions. Our framework captures the inherent stochasticity of LLM responses through explicit diffusion terms and reveals systematic interference patterns between competing objectives via an interference matrix formulation. We validate our theoretical framework using iterative code generation as a proof-of-concept application, analyzing 400 sessions across security, efficiency, and functionality objectives. Our results demonstrate strategy-dependent convergence behaviors with rates ranging from 0.33 to 1.29, and predictive accuracy achieving R2 = 0.74 for balanced approaches. This work proposes the feasibility of dynamical systems analysis for multi-objective LLM interactions, with code generation serving as an initial validation domain.</li>
<li><strong>摘要：</strong>我们引入了一个通用随机微分方程框架，用于在迭代大语言模型（LLM）交互中对多目标优化动力学进行建模。我们的框架通过显式扩散项捕获了 LLM 响应的固有随机性，并通过干扰矩阵公式揭示了竞争目标之间的系统干扰模式。我们使用迭代代码生成作为概念验证应用程序来验证我们的理论框架，分析跨安全性、效率和功能目标的 400 个会话。我们的结果表明，策略相关的收敛行为的收敛速度范围为 0.33 至 1.29，平衡方法的预测精度达到 R2 = 0.74。这项工作提出了多目标 LLM 交互的动态系统分析的可行性，其中代码生成作为初始验证域。</li>
</ul>

<h3>Title: Structured Spectral Graph Learning for Multi-label Abnormality Classification in 3D Chest CT Scans</h3>
<ul>
<li><strong>Authors: </strong>Theo Di Piazza, Carole Lazarus, Olivier Nempont, Loic Boussel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10779">https://arxiv.org/abs/2510.10779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10779">https://arxiv.org/pdf/2510.10779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10779]] Structured Spectral Graph Learning for Multi-label Abnormality Classification in 3D Chest CT Scans(https://arxiv.org/abs/2510.10779)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>With the growing volume of CT examinations, there is an increasing demand for automated tools such as organ segmentation, abnormality detection, and report generation to support radiologists in managing their clinical workload. Multi-label classification of 3D Chest CT scans remains a critical yet challenging problem due to the complex spatial relationships inherent in volumetric data and the wide variability of abnormalities. Existing methods based on 3D convolutional neural networks struggle to capture long-range dependencies, while Vision Transformers often require extensive pre-training on large-scale, domain-specific datasets to perform competitively. In this work, we propose a 2.5D alternative by introducing a new graph-based framework that represents 3D CT volumes as structured graphs, where axial slice triplets serve as nodes processed through spectral graph convolution, enabling the model to reason over inter-slice dependencies while maintaining complexity compatible with clinical deployment. Our method, trained and evaluated on 3 datasets from independent institutions, achieves strong cross-dataset generalization, and shows competitive performance compared to state-of-the-art visual encoders. We further conduct comprehensive ablation studies to evaluate the impact of various aggregation strategies, edge-weighting schemes, and graph connectivity patterns. Additionally, we demonstrate the broader applicability of our approach through transfer experiments on automated radiology report generation and abdominal CT data.\\ This work extends our previous contribution presented at the MICCAI 2025 EMERGE Workshop.</li>
<li><strong>摘要：</strong>随着 CT 检查量的不断增加，对器官分割、异常检测和报告生成等自动化工具的需求不断增加，以支持放射科医生管理其临床工作量。由于体积数据固有的复杂空间关系和异常的广泛变异性，3D 胸部 CT 扫描的多标签分类仍然是一个关键但具有挑战性的问题。基于 3D 卷积神经网络的现有方法很难捕获远程依赖性，而 Vision Transformers 通常需要对大规模、特定领域的数据集进行广泛的预训练才能具有竞争力。在这项工作中，我们提出了一种 2.5D 替代方案，引入了一种新的基于图的框架，该框架将 3D CT 体积表示为结构化图，其中轴向切片三元组充当通过谱图卷积处理的节点，使模型能够推理切片间依赖性，同时保持与临床部署兼容的复杂性。我们的方法在来自独立机构的 3 个数据集上进行了训练和评估，实现了强大的跨数据集泛化，并且与最先进的视觉编码器相比表现出了具有竞争力的性能。我们进一步进行全面的消融研究，以评估各种聚合策略、边缘加权方案和图连接模式的影响。此外，我们通过自动放射学报告生成和腹部 CT 数据的传输实验证明了我们的方法具有更广泛的适用性。\\这项工作扩展了我们之前在 MICCAI 2025 EMERGE 研讨会上提出的贡献。</li>
</ul>

<h3>Title: DISC-GAN: Disentangling Style and Content for Cluster-Specific Synthetic Underwater Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Sneha Varur, Anirudh R Hanchinamani, Tarun S Bagewadi, Uma Mudenagudi, Chaitra D Desai, Sujata C, Padmashree Desai, Sumit Meharwade</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10782">https://arxiv.org/abs/2510.10782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10782">https://arxiv.org/pdf/2510.10782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10782]] DISC-GAN: Disentangling Style and Content for Cluster-Specific Synthetic Underwater Image Generation(https://arxiv.org/abs/2510.10782)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a novel framework, Disentangled Style-Content GAN (DISC-GAN), which integrates style-content disentanglement with a cluster-specific training strategy towards photorealistic underwater image synthesis. The quality of synthetic underwater images is challenged by optical due to phenomena such as color attenuation and turbidity. These phenomena are represented by distinct stylistic variations across different waterbodies, such as changes in tint and haze. While generative models are well-suited to capture complex patterns, they often lack the ability to model the non-uniform conditions of diverse underwater environments. To address these challenges, we employ K-means clustering to partition a dataset into style-specific domains. We use separate encoders to get latent spaces for style and content; we further integrate these latent representations via Adaptive Instance Normalization (AdaIN) and decode the result to produce the final synthetic image. The model is trained independently on each style cluster to preserve domain-specific characteristics. Our framework demonstrates state-of-the-art performance, obtaining a Structural Similarity Index (SSIM) of 0.9012, an average Peak Signal-to-Noise Ratio (PSNR) of 32.5118 dB, and a Frechet Inception Distance (FID) of 13.3728.</li>
<li><strong>摘要：</strong>在本文中，我们提出了一种新颖的框架，即解开风格内容 GAN (DISC-GAN)，它将风格内容解开与针对真实感水下图像合成的特定于集群的训练策略相结合。由于颜色衰减和浑浊等现象，合成水下图像的质量受到光学挑战。这些现象表现为不同水体的独特风格变化，例如色调和雾度的变化。虽然生成模型非常适合捕获复杂的模式，但它们通常缺乏对不同水下环境的不均匀条件进行建模的能力。为了应对这些挑战，我们采用 K 均值聚类将数据集划分为特定于风格的域。我们使用单独的编码器来获取样式和内容的潜在空间；我们通过自适应实例归一化（AdaIN）进一步整合这些潜在表示，并对结果进行解码以生成最终的合成图像。该模型在每个风格集群上独立训练，以保留特定领域的特征。我们的框架展示了最先进的性能，获得了 0.9012 的结构相似性指数 (SSIM)、32.5118 dB 的平均峰值信噪比 (PSNR) 和 13.3728 的 Frechet 起始距离 (FID)。</li>
</ul>

<h3>Title: Crisis-Aware Regime-Conditioned Diffusion with CVaR Allocation</h3>
<ul>
<li><strong>Authors: </strong>Ali Atiah Alzahrani</a></li>
<li><strong>Subjects: </strong>cs.LG, q-fin.CP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10807">https://arxiv.org/abs/2510.10807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10807">https://arxiv.org/pdf/2510.10807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10807]] Crisis-Aware Regime-Conditioned Diffusion with CVaR Allocation(https://arxiv.org/abs/2510.10807)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We study whether regime-conditioned generative scenarios, coupled with a convex CVaR allocator, improve portfolio decisions under regime shifts. We introduce Multi-Agent Regime-Conditioned Diffusion (MARCD), which (i) infers latent regimes via a Gaussian HMM, (ii) trains a diffusion model with a tail-weighted objective and a regime-specialized mixture-of-experts (MoE) denoiser to enrich crisis co-movements, and (iii) feeds the generated scenarios into a turnover-aware CVaR epigraph quadratic program with explicit governance. In strict walk-forward tests on liquid multi-asset ETFs (2005-2025), MARCD outperforms standard allocators and improves calibration relative to popular generators. Over 2020-2025 out-of-sample (monthly; 10 bps), MARCD attains Sharpe 1.23 (BL 1.02) and MaxDD 9.3 percent (BL 14.1 percent), a 34 percent reduction, at comparable turnover; stationary block-bootstrap intervals indicate the Sharpe uplift is significant at 5 percent. We provide theory linking tail-weighted diffusion to spectral-risk control of the decision-relevant CVaR gap, oracle/consistency results for the regime-MoE denoiser, and Lipschitz/regret guarantees for the allocator. Together, MARCD offers a reproducible bridge from tail-faithful scenario modeling to governed portfolio decisions with materially improved drawdown control.</li>
<li><strong>摘要：</strong>我们研究了制度条件生成场景与凸 CVaR 分配器相结合是否可以改善制度转变下的投资组合决策。我们引入了多智能体制度条件扩散（MARCD），它（i）通过高斯 HMM 推断潜在制度，（ii）训练具有尾部加权目标和制度专用专家混合（MoE）降噪器的扩散模型，以丰富危机联动，以及（iii）将生成的场景输入到周转感知的 CVaR 铭文中 具有明确治理的二次规划。在对流动性多资产 ETF（2005-2025）进行严格的前瞻性测试中，MARCD 的表现优于标准分配器，并相对于流行的生成器改进了校准。 2020-2025年样本外（每月；10个基点），MARCD达到夏普1.23（BL 1.02）和MaxDD 9.3%（BL 14.1%），在可比营业额下下降34%；固定块自举区间表明夏普抬升幅度为 5%，非常显着。我们提供了将尾部加权扩散与决策相关 CVaR 差距的谱风险控制、机制 MoE 降噪器的预言/一致性结果以及分配器的 Lipschitz/regret 保证联系起来的理论。总之，MARCD 提供了一个可重复的桥梁，从尾部忠实场景建模到受管理的投资组合决策，并显着改进了回撤控制。</li>
</ul>

<h3>Title: Towards Distribution-Shift Uncertainty Estimation for Inverse Problems with Generative Priors</h3>
<ul>
<li><strong>Authors: </strong>Namhoon Kim, Sara Fridovich-Keil</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10947">https://arxiv.org/abs/2510.10947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10947">https://arxiv.org/pdf/2510.10947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10947]] Towards Distribution-Shift Uncertainty Estimation for Inverse Problems with Generative Priors(https://arxiv.org/abs/2510.10947)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models have shown strong potential as data-driven priors for solving inverse problems such as reconstructing medical images from undersampled measurements. While these priors improve reconstruction quality with fewer measurements, they risk hallucinating features when test images lie outside the training distribution. Existing uncertainty quantification methods in this setting (i) require an in-distribution calibration dataset, which may not be available, (ii) provide heuristic rather than statistical estimates, or (iii) quantify uncertainty from model capacity or limited measurements rather than distribution shift. We propose an instance-level, calibration-free uncertainty indicator that is sensitive to distribution shift, requires no knowledge of the training distribution, and incurs no retraining cost. Our key hypothesis is that reconstructions of in-distribution images remain stable under random measurement variations, while reconstructions of out-of-distribution (OOD) images exhibit greater instability. We use this stability as a proxy for detecting distribution shift. Our proposed OOD indicator is efficiently computable for any computational imaging inverse problem; we demonstrate it on tomographic reconstruction of MNIST digits, where a learned proximal network trained only on digit "0" is evaluated on all ten digits. Reconstructions of OOD digits show higher variability and correspondingly higher reconstruction error, validating this indicator. These results suggest a deployment strategy that pairs generative priors with lightweight guardrails, enabling aggressive measurement reduction for in-distribution cases while automatically warning when priors are applied out of distribution.</li>
<li><strong>摘要：</strong>生成模型作为数据驱动的先验，在解决逆问题（例如根据欠采样测量重建医学图像）方面显示出强大的潜力。虽然这些先验通过较少的测量提高了重建质量，但当测试图像位于训练分布之外时，它们可能会产生幻觉特征。这种情况下现有的不确定性量化方法（i）需要分布内校准数据集，但可能无法获得，（ii）提供启发式而非统计估计，或（iii）根据模型容量或有限测量而不是分布偏移来量化不确定性。我们提出了一种实例级、免校准的不确定性指标，该指标对分布变化敏感，不需要了解训练分布，并且不会产生再训练成本。我们的关键假设是，分布内图像的重建在随机测量变化下保持稳定，而分布外（OOD）图像的重建表现出更大的不稳定性。我们使用这种稳定性作为检测分布变化的代理。我们提出的 OOD 指标对于任何计算成像逆问题都是可有效计算的；我们在 MNIST 数字的断层扫描重建中演示了它，其中仅在数字“0”上训练的学习近端网络在所有十个数字上进行评估。 OOD 数字的重建显示出更高的变异性和相应更高的重建误差，验证了该指标。这些结果提出了一种部署策略，将生成先验与轻量级护栏配对，从而能够积极减少分布内情况的测量，同时在先验应用于分布外时自动发出警告。</li>
</ul>

<h3>Title: Interpretable Machine Learning for Cognitive Aging: Handling Missing Data and Uncovering Social Determinant</h3>
<ul>
<li><strong>Authors: </strong>Xi Mao, Zhendong Wang, Jingyu Li, Lingchao Mao, Utibe Essien, Hairong Wang, Xuelei Sherry Ni</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10952">https://arxiv.org/abs/2510.10952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10952">https://arxiv.org/pdf/2510.10952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10952]] Interpretable Machine Learning for Cognitive Aging: Handling Missing Data and Uncovering Social Determinant(https://arxiv.org/abs/2510.10952)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Early detection of Alzheimer's disease (AD) is crucial because its neurodegenerative effects are irreversible, and neuropathologic and social-behavioral risk factors accumulate years before diagnosis. Identifying higher-risk individuals earlier enables prevention, timely care, and equitable resource allocation. We predict cognitive performance from social determinants of health (SDOH) using the NIH NIA-supported PREPARE Challenge Phase 2 dataset derived from the nationally representative Mex-Cog cohort of the 2003 and 2012 Mexican Health and Aging Study (MHAS). Data: The target is a validated composite cognitive score across seven domains-orientation, memory, attention, language, constructional praxis, and executive function-derived from the 2016 and 2021 MHAS waves. Predictors span demographic, socioeconomic, health, lifestyle, psychosocial, and healthcare access factors. Methodology: Missingness was addressed with a singular value decomposition (SVD)-based imputation pipeline treating continuous and categorical variables separately. This approach leverages latent feature correlations to recover missing values while balancing reliability and scalability. After evaluating multiple methods, XGBoost was chosen for its superior predictive performance. Results and Discussion: The framework outperformed existing methods and the data challenge leaderboard, demonstrating high accuracy, robustness, and interpretability. SHAP-based post hoc analysis identified top contributing SDOH factors and age-specific feature patterns. Notably, flooring material emerged as a strong predictor, reflecting socioeconomic and environmental disparities. Other influential factors, age, SES, lifestyle, social interaction, sleep, stress, and BMI, underscore the multifactorial nature of cognitive aging and the value of interpretable, data-driven SDOH modeling.</li>
<li><strong>摘要：</strong>阿尔茨海默病 (AD) 的早期发现至关重要，因为其神经退行性影响是不可逆转的，并且神经病理学和社会行为危险因素在诊断前数年就已积累。尽早识别高风险个体可以实现预防、及时护理和公平的资源分配。我们使用 NIH NIA 支持的 PREPARE Challenge 第 2 阶段数据集来预测健康社会决定因素 (SDOH) 的认知表现，该数据集源自 2003 年和 2012 年墨西哥健康与老龄化研究 (MHAS) 的全国代表性 Mex-Cog 队列。数据：目标是从 2016 年和 2021 年 MHAS 浪潮中得出的跨七个领域（定向、记忆、注意力、语言、结构实践和执行功能）的经过验证的综合认知得分。预测因素涵盖人口、社会经济、健康、生活方式、社会心理和医疗保健获取因素。方法：通过基于奇异值分解 (SVD) 的插补流程分别处理连续变量和分类变量来解决缺失问题。这种方法利用潜在特征相关性来恢复缺失值，同时平衡可靠性和可扩展性。在评估多种方法后，XGBoost 因其卓越的预测性能而被选中。结果和讨论：该框架优于现有方法和数据挑战排行榜，表现出高精度、稳健性和可解释性。基于 SHAP 的事后分析确定了最主要的 SDOH 因素和特定年龄的特征模式。值得注意的是，地板材料成为强有力的预测因素，反映了社会经济和环境差异。其他影响因素，年龄、社会经济地位、生活方式、社交互动、睡眠、压力和体重指数，强调了认知衰老的多因素性质以及可解释的、数据驱动的 SDOH 模型的价值。</li>
</ul>

<h3>Title: Not All Bits Are Equal: Scale-Dependent Memory Optimization Strategies for Reasoning Models</h3>
<ul>
<li><strong>Authors: </strong>Junhyuck Kim, Ethan Ewer, Taehong Moon, Jongho Park, Dimitris Papailiopoulos</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10964">https://arxiv.org/abs/2510.10964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10964">https://arxiv.org/pdf/2510.10964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10964]] Not All Bits Are Equal: Scale-Dependent Memory Optimization Strategies for Reasoning Models(https://arxiv.org/abs/2510.10964)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While 4-bit quantization has emerged as a memory-optimal choice for non-reasoning models and zero-shot tasks across scales, we show that this universal prescription fails for reasoning models, where the KV cache rather than model size can dominate memory. Through systematic experiments across 1,700 inference scenarios on AIME25 and GPQA-Diamond, we find a scale-dependent trade-off: models with an effective size below 8-bit 4B parameters achieve better accuracy by allocating memory to more weights rather than longer generation, while larger models achieve better accuracy by allocating memory to longer generations. This scale threshold also determines when parallel scaling becomes memory-efficient and whether KV cache eviction outperforms KV quantization. Our findings show that memory optimization for LLMs cannot be scale-agnostic, while providing principled guidelines: for small reasoning models, prioritize model capacity over test-time compute, while for larger ones, maximize test-time compute. Our results suggest that optimizing reasoning models for deployment requires fundamentally different strategies from those established for non-reasoning models.</li>
<li><strong>摘要：</strong>虽然 4 位量化已成为非推理模型和跨尺度零样本任务的内存最佳选择，但我们表明，这种通用处方对于推理模型来说是失败的，在推理模型中，KV 缓存而不是模型大小可以主导内存。通过 AIME25 和 GPQA-Diamond 上 1,700 个推理场景的系统实验，我们发现了一个与规模相关的权衡：有效大小低于 8 位 4B 参数的模型通过将内存分配给更多权重而不是更长的代来实现更好的精度，而较大的模型通过将内存分配给更长的代来实现更好的精度。此缩放阈值还确定并行缩放何时变得内存高效以及 KV 缓存逐出是否优于 KV 量化。我们的研究结果表明，法学硕士的内存优化不能与规模无关，同时提供原则性指导原则：对于小型推理模型，优先考虑模型容量而不是测试时计算，而对于较大的模型，则最大化测试时计算。我们的结果表明，优化部署推理模型需要与为非推理模型建立的策略完全不同的策略。</li>
</ul>

<h3>Title: IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Zeteng Lin, Xingxing Li, Wen You, Xiaoyang Li, Zehan Lu, Yujun Cai, Jing Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10969">https://arxiv.org/abs/2510.10969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10969">https://arxiv.org/pdf/2510.10969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10969]] IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation(https://arxiv.org/abs/2510.10969)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Existing vision language models (VLMs), including GPT-4 and DALL-E, often struggle to preserve logic, object identity, and style in multimodal image-text generation. This limitation significantly hinders the generalization capability of VLMs in complex image-text input-output scenarios. To address this issue, we propose IUT-Plug, a module grounded in an Image Understanding Tree (IUT), which enhances existing interleaved VLMs through explicit structured reasoning, thereby mitigating context drift in logic, entity identity, and style. The proposed framework operates in two stages. (1) A dynamic IUT-Plug extraction module parses visual scenes into hierarchical symbolic structures. (2) A coordinated narrative-flow and image synthesis mechanism ensures cross-modal consistency. To evaluate our approach, we construct a novel benchmark based on 3,000 real human-generated question-answer pairs over fine-tuned large models, introducing a dynamic evaluation protocol for quantifying context drift in interleaved VLMs. Experimental results demonstrate that IUT-Plug not only improves accuracy on established benchmarks but also effectively alleviates the three critical forms of context drift across diverse multimodal question answering (QA) scenarios.</li>
<li><strong>摘要：</strong>现有的视觉语言模型 (VLM)，包括 GPT-4 和 DALL-E，通常很难在多模式图像文本生成中保留逻辑、对象身份和风格。这种限制极大地阻碍了 VLM 在复杂的图像文本输入输出场景中的泛化能力。为了解决这个问题，我们提出了 IUT-Plug，这是一个基于图像理解树 (IUT) 的模块，它通过显式结构化推理增强现有的交错 VLM，从而减轻逻辑、实体身份和风格中的上下文漂移。拟议的框架分两个阶段运行。 (1)动态IUT-Plug提取模块将视觉场景解析为分层符号结构。 （2）协调的叙事流和图像合成机制确保跨模态一致性。为了评估我们的方法，我们基于经过微调的大型模型的 3,000 个真实的人类生成的问答对构建了一个新颖的基准，引入了一种动态评估协议来量化交错 VLM 中的上下文漂移。实验结果表明，IUT-Plug 不仅提高了既定基准的准确性，而且还有效缓解了不同多模态问答 (QA) 场景中上下文漂移的三种关键形式。</li>
</ul>

<h3>Title: ContextGen: Contextual Layout Anchoring for Identity-Consistent Multi-Instance Generation</h3>
<ul>
<li><strong>Authors: </strong>Ruihang Xu, Dewei Zhou, Fan Ma, Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11000">https://arxiv.org/abs/2510.11000</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11000">https://arxiv.org/pdf/2510.11000</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11000]] ContextGen: Contextual Layout Anchoring for Identity-Consistent Multi-Instance Generation(https://arxiv.org/abs/2510.11000)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multi-instance image generation (MIG) remains a significant challenge for modern diffusion models due to key limitations in achieving precise control over object layout and preserving the identity of multiple distinct subjects. To address these limitations, we introduce ContextGen, a novel Diffusion Transformer framework for multi-instance generation that is guided by both layout and reference images. Our approach integrates two key technical contributions: a Contextual Layout Anchoring (CLA) mechanism that incorporates the composite layout image into the generation context to robustly anchor the objects in their desired positions, and Identity Consistency Attention (ICA), an innovative attention mechanism that leverages contextual reference images to ensure the identity consistency of multiple instances. Recognizing the lack of large-scale, hierarchically-structured datasets for this task, we introduce IMIG-100K, the first dataset with detailed layout and identity annotations. Extensive experiments demonstrate that ContextGen sets a new state-of-the-art, outperforming existing methods in control precision, identity fidelity, and overall visual quality.</li>
<li><strong>摘要：</strong>由于在实现对象布局的精确控制和保留多个不同主体的身份方面存在关键限制，多实例图像生成（MIG）仍然是现代扩散模型的重大挑战。为了解决这些限制，我们引入了 ContextGen，这是一种新颖的 Diffusion Transformer 框架，用于由布局和参考图像引导的多实例生成。我们的方法集成了两个关键的技术贡献：上下文布局锚定（CLA）机制，将复合布局图像合并到生成上下文中，以将对象牢固地锚定在所需位置；以及身份一致性注意（ICA），这是一种创新的注意机制，利用上下文参考图像来确保多个实例的身份一致性。认识到此任务缺乏大规模、分层结构的数据集，我们引入了 IMIG-100K，这是第一个具有详细布局和身份注释的数据集。大量实验表明，ContextGen 树立了新的最先进技术，在控制精度、身份保真度和整体视觉质量方面优于现有方法。</li>
</ul>

<h3>Title: GIR-Bench: Versatile Benchmark for Generating Images with Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Hongxiang Li, Yaowei Li, Bin Lin, Yuwei Niu, Yuhang Yang, Xiaoshuang Huang, Jiayin Cai, Xiaolong Jiang, Yao Hu, Long Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11026">https://arxiv.org/abs/2510.11026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11026">https://arxiv.org/pdf/2510.11026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11026]] GIR-Bench: Versatile Benchmark for Generating Images with Reasoning(https://arxiv.org/abs/2510.11026)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Unified multimodal models integrate the reasoning capacity of large language models with both image understanding and generation, showing great promise for advanced multimodal intelligence. However, the community still lacks a rigorous reasoning-centric benchmark to systematically evaluate the alignment between understanding and generation, and their generalization potential in complex visual tasks. To this end, we introduce \textbf{GIR-Bench}, a comprehensive benchmark that evaluates unified models across three complementary perspectives. Firstly, we investigate understanding-generation consistency (GIR-Bench-UGC), asking whether models can consistently leverage the same knowledge in both understanding and generation tasks. Secondly, we investigate whether models can perform reasoning-centric text-to-image generation that requires applying logical constraints and implicit knowledge to generate faithful visual content (GIR-Bench-T2I). Thirdly, we evaluate whether models can handle multi-step reasoning in editing (GIR-Bench-Edit). For each subset, we carefully design different task-specific evaluation pipelines tailored for each task. This enables fine-grained and interpretable evaluation while mitigating biases from the prevalent MLLM-as-a-Judge paradigm. Extensive ablations over various unified models and generation-only systems have shown that: Although unified models are more capable of reasoning-driven visual tasks, they still exhibit a persistent gap between understanding and generation. The data and code for GIR-Bench are available at \href{this https URL}{this https URL}.</li>
<li><strong>摘要：</strong>统一多模态模型将大型语言模型的推理能力与图像理解和生成相结合，为高级多模态智能展现了巨大的前景。然而，社区仍然缺乏严格的以推理为中心的基准来系统地评估理解和生成之间的一致性，以及它们在复杂视觉任务中的泛化潜力。为此，我们引入了 \textbf{GIR-Bench}，这是一个综合基准，可以从三个互补的角度评估统一模型。首先，我们研究理解-生成一致性（GIR-Bench-UGC），询问模型是否能够在理解和生成任务中一致地利用相同的知识。其次，我们研究模型是否可以执行以推理为中心的文本到图像生成，这需要应用逻辑约束和隐式知识来生成忠实的视觉内容（GIR-Bench-T2I）。第三，我们评估模型是否可以处理编辑中的多步推理（GIR-Bench-Edit）。对于每个子集，我们精心设计了针对每个任务量身定制的不同特定任务评估流程。这使得细粒度和可解释的评估成为可能，同时减少流行的 MLLM 作为法官范式的偏见。对各种统一模型和仅生成系统的广泛消融表明：尽管统一模型更有能力执行推理驱动的视觉任务，但它们仍然在理解和生成之间表现出持续的差距。 GIR-Bench 的数据和代码可在 \href{此 https URL}{此 https URL} 中找到。</li>
</ul>

<h3>Title: Enhancing Zero-Shot Anomaly Detection: CLIP-SAM Collaboration with Cascaded Prompts</h3>
<ul>
<li><strong>Authors: </strong>Yanning Hou, Ke Xu, Junfa Li, Yanran Ruan, Jianfeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11028">https://arxiv.org/abs/2510.11028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11028">https://arxiv.org/pdf/2510.11028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11028]] Enhancing Zero-Shot Anomaly Detection: CLIP-SAM Collaboration with Cascaded Prompts(https://arxiv.org/abs/2510.11028)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recently, the powerful generalization ability exhibited by foundation models has brought forth new solutions for zero-shot anomaly segmentation tasks. However, guiding these foundation models correctly to address downstream tasks remains a challenge. This paper proposes a novel two-stage framework, for zero-shot anomaly segmentation tasks in industrial anomaly detection. This framework excellently leverages the powerful anomaly localization capability of CLIP and the boundary perception ability of SAM.(1) To mitigate SAM's inclination towards object segmentation, we propose the Co-Feature Point Prompt Generation (PPG) module. This module collaboratively utilizes CLIP and SAM to generate positive and negative point prompts, guiding SAM to focus on segmenting anomalous regions rather than the entire object. (2) To further optimize SAM's segmentation results and mitigate rough boundaries and isolated noise, we introduce the Cascaded Prompts for SAM (CPS) module. This module employs hybrid prompts cascaded with a lightweight decoder of SAM, achieving precise segmentation of anomalous regions. Across multiple datasets, consistent experimental validation demonstrates that our approach achieves state-of-the-art zero-shot anomaly segmentation results. Particularly noteworthy is our performance on the Visa dataset, where we outperform the state-of-the-art methods by 10.3\% and 7.7\% in terms of {$F_1$-max} and AP metrics, respectively.</li>
<li><strong>摘要：</strong>最近，基础模型所展现出的强大泛化能力为零样本异常分割任务带来了新的解决方案。然而，正确指导这些基础模型来解决下游任务仍然是一个挑战。本文提出了一种新颖的两阶段框架，用于工业异常检测中的零样本异常分割任务。该框架很好地利用了CLIP强大的异常定位能力和SAM的边界感知能力。（1）为了减轻SAM对对象分割的倾向，我们提出了联合特征点提示生成（PPG）模块。该模块协同利用CLIP和SAM生成正负点提示，引导SAM专注于分割异常区域而不是整个物体。 (2) 为了进一步优化 SAM 的分割结果并减轻粗糙边界和孤立噪声，我们引入了 Cascaded Prompts for SAM (CPS) 模块。该模块采用混合提示与轻量级SAM解码器级联，实现异常区域的精确分割。在多个数据集中，一致的实验验证表明我们的方法实现了最先进的零样本异常分割结果。特别值得注意的是我们在 Visa 数据集上的表现，在 {$F_1$-max} 和 AP 指标方面，我们分别比最先进的方法高出 10.3% 和 7.7%。</li>
</ul>

<h3>Title: Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Youngrok Park, Hojung Jung, Sangmin Bae, Se-Young Yun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11057">https://arxiv.org/abs/2510.11057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11057">https://arxiv.org/pdf/2510.11057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11057]] Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models(https://arxiv.org/abs/2510.11057)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved remarkable success as generative models. However, even a well-trained model can accumulate errors throughout the generation process. These errors become particularly problematic when arbitrary guidance is applied to steer samples toward desired properties, which often breaks sample fidelity. In this paper, we propose a general solution to address the off-manifold phenomenon observed in diffusion models. Our approach leverages a time predictor to estimate deviations from the desired data manifold at each timestep, identifying that a larger time gap is associated with reduced generation quality. We then design a novel guidance mechanism, `Temporal Alignment Guidance' (TAG), attracting the samples back to the desired manifold at every timestep during generation. Through extensive experiments, we demonstrate that TAG consistently produces samples closely aligned with the desired manifold at each timestep, leading to significant improvements in generation quality across various downstream tasks.</li>
<li><strong>摘要：</strong>扩散模型作为生成模型取得了巨大的成功。然而，即使训练有素的模型也会在整个生成过程中积累错误。当应用任意引导来引导样品走向所需的特性时，这些错误变得尤其成问题，这通常会破坏样品的保真度。在本文中，我们提出了一种通用解决方案来解决扩散模型中观察到的偏离流形现象。我们的方法利用时间预测器来估计每个时间步长与所需数据流形的偏差，确定较大的时间间隙与降低的发电质量相关。然后，我们设计了一种新颖的引导机制，“时间对齐引导”（TAG），在生成过程中的每个时间步将样本吸引回所需的流形。通过大量的实验，我们证明 TAG 在每个时间步上始终如一地生成与所需流形紧密一致的样本，从而显着提高了各种下游任务的生成质量。</li>
</ul>

<h3>Title: Future-Aware End-to-End Driving: Bidirectional Modeling of Trajectory Planning and Scene Evolution</h3>
<ul>
<li><strong>Authors: </strong>Bozhou Zhang, Nan Song, Jingyu Li, Xiatian Zhu, Jiankang Deng, Li Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11092">https://arxiv.org/abs/2510.11092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11092">https://arxiv.org/pdf/2510.11092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11092]] Future-Aware End-to-End Driving: Bidirectional Modeling of Trajectory Planning and Scene Evolution(https://arxiv.org/abs/2510.11092)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>End-to-end autonomous driving methods aim to directly map raw sensor inputs to future driving actions such as planned trajectories, bypassing traditional modular pipelines. While these approaches have shown promise, they often operate under a one-shot paradigm that relies heavily on the current scene context, potentially underestimating the importance of scene dynamics and their temporal evolution. This limitation restricts the model's ability to make informed and adaptive decisions in complex driving scenarios. We propose a new perspective: the future trajectory of an autonomous vehicle is closely intertwined with the evolving dynamics of its environment, and conversely, the vehicle's own future states can influence how the surrounding scene unfolds. Motivated by this bidirectional relationship, we introduce SeerDrive, a novel end-to-end framework that jointly models future scene evolution and trajectory planning in a closed-loop manner. Our method first predicts future bird's-eye view (BEV) representations to anticipate the dynamics of the surrounding scene, then leverages this foresight to generate future-context-aware trajectories. Two key components enable this: (1) future-aware planning, which injects predicted BEV features into the trajectory planner, and (2) iterative scene modeling and vehicle planning, which refines both future scene prediction and trajectory generation through collaborative optimization. Extensive experiments on the NAVSIM and nuScenes benchmarks show that SeerDrive significantly outperforms existing state-of-the-art methods.</li>
<li><strong>摘要：</strong>端到端自动驾驶方法旨在绕过传统的模块化管道，将原始传感器输入直接映射到未来的驾驶行为（例如计划轨迹）。虽然这些方法已经显示出希望，但它们通常在严重依赖于当前场景上下文的一次性范式下运行，可能低估了场景动态及其时间演化的重要性。这种限制限制了模型在复杂驾驶场景中做出明智和自适应决策的能力。我们提出了一个新的视角：自动驾驶汽车的未来轨迹与其环境的动态变化密切相关，相反，汽车自身的未来状态可以影响周围场景的发展。受这种双向关系的推动，我们引入了 SeerDrive，这是一种新颖的端到端框架，它以闭环方式联合模拟未来场景演化和轨迹规划。我们的方法首先预测未来的鸟瞰图（BEV）表示，以预测周围场景的动态，然后利用这种预见来生成未来上下文感知的轨迹。实现这一目标的两个关键组件是：(1) 未来感知规划，将预测的 BEV 特征注入轨迹规划器；(2) 迭代场景建模和车辆规划，通过协作优化完善未来场景预测和轨迹生成。对 NAVSIM 和 nuScenes 基准的大量实验表明，SeerDrive 的性能显着优于现有的最先进方法。</li>
</ul>

<h3>Title: MoMaps: Semantics-Aware Scene Motion Generation with Motion Maps</h3>
<ul>
<li><strong>Authors: </strong>Jiahui Lei, Kyle Genova, George Kopanas, Noah Snavely, Leonidas Guibas</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11107">https://arxiv.org/abs/2510.11107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11107">https://arxiv.org/pdf/2510.11107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11107]] MoMaps: Semantics-Aware Scene Motion Generation with Motion Maps(https://arxiv.org/abs/2510.11107)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>This paper addresses the challenge of learning semantically and functionally meaningful 3D motion priors from real-world videos, in order to enable prediction of future 3D scene motion from a single input image. We propose a novel pixel-aligned Motion Map (MoMap) representation for 3D scene motion, which can be generated from existing generative image models to facilitate efficient and effective motion prediction. To learn meaningful distributions over motion, we create a large-scale database of MoMaps from over 50,000 real videos and train a diffusion model on these representations. Our motion generation not only synthesizes trajectories in 3D but also suggests a new pipeline for 2D video synthesis: first generate a MoMap, then warp an image accordingly and complete the warped point-based renderings. Experimental results demonstrate that our approach generates plausible and semantically consistent 3D scene motion.</li>
<li><strong>摘要：</strong>本文解决了从现实世界视频中学习语义和功能上有意义的 3D 运动先验的挑战，以便能够从单个输入图像预测未来的 3D 场景运动。我们提出了一种新颖的用于 3D 场景运动的像素对齐运动图 (MoMap) 表示，它可以从现有的生成图像模型生成，以促进高效且有效的运动预测。为了学习有意义的运动分布，我们根据 50,000 多个真实视频创建了一个大型 MoMap 数据库，并根据这些表示训练了扩散模型。我们的运动生成不仅合成 3D 轨迹，还提出了一种用于 2D 视频合成的新管道：首先生成 MoMap，然后相应地扭曲图像并完成基于扭曲点的渲染。实验结果表明，我们的方法可以生成合理且语义一致的 3D 场景运动。</li>
</ul>

<h3>Title: PhysioME: A Robust Multimodal Self-Supervised Framework for Physiological Signals with Missing Modalities</h3>
<ul>
<li><strong>Authors: </strong>Cheol-Hui Lee, Hwa-Yeon Lee, Min-Kyung Jung, Dong-Joo Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11110">https://arxiv.org/abs/2510.11110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11110">https://arxiv.org/pdf/2510.11110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11110]] PhysioME: A Robust Multimodal Self-Supervised Framework for Physiological Signals with Missing Modalities(https://arxiv.org/abs/2510.11110)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Missing or corrupted modalities are common in physiological signal-based medical applications owing to hardware constraints or motion artifacts. However, most existing methods assume the availability of all modalities, resulting in substantial performance degradation in the absence of any modality. To overcome this limitation, this study proposes PhysioME, a robust framework designed to ensure reliable performance under missing modality conditions. PhysioME adopts: (1) a multimodal self-supervised learning approach that combines contrastive learning with masked prediction; (2) a Dual-PathNeuroNet backbone tailored to capture the temporal dynamics of each physiological signal modality; and (3) a restoration decoder that reconstructs missing modality tokens, enabling flexible processing of incomplete inputs. The experimental results show that PhysioME achieves high consistency and generalization performance across various missing modality scenarios. These findings highlight the potential of PhysioME as a reliable tool for supporting clinical decision-making in real-world settings with imperfect data availability.</li>
<li><strong>摘要：</strong>由于硬件限制或运动伪影，在基于生理信号的医疗应用中，丢失或损坏的模式很常见。然而，大多数现有方法假设所有模态都可用，导致在没有任何模态的情况下性能大幅下降。为了克服这一限制，本研究提出了 PhysioME，这是一个强大的框架，旨在确保在缺失模态条件下的可靠性能。 PhysioME采用：（1）将对比学习与掩模预测相结合的多模态自监督学习方法； (2) 专为捕获每种生理信号模态的时间动态而定制的 Dual-PathNeuroNet 主干网； (3) 恢复解码器，可重建丢失的模态标记，从而能够灵活处理不完整的输入。实验结果表明，PhysioME 在各种缺失模态场景中实现了高度的一致性和泛化性能。这些发现凸显了 PhysioME 作为可靠工具的潜力，可在数据可用性不完善的现实环境中支持临床决策。</li>
</ul>

<h3>Title: Demystifying Numerosity in Diffusion Models -- Limitations and Remedies</h3>
<ul>
<li><strong>Authors: </strong>Yaqi Zhao, Xiaochen Wang, Li Dong, Wentao Zhang, Yuhui Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11117">https://arxiv.org/abs/2510.11117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11117">https://arxiv.org/pdf/2510.11117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11117]] Demystifying Numerosity in Diffusion Models -- Limitations and Remedies(https://arxiv.org/abs/2510.11117)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Numerosity remains a challenge for state-of-the-art text-to-image generation models like FLUX and GPT-4o, which often fail to accurately follow counting instructions in text prompts. In this paper, we aim to study a fundamental yet often overlooked question: Can diffusion models inherently generate the correct number of objects specified by a textual prompt simply by scaling up the dataset and model size? To enable rigorous and reproducible evaluation, we construct a clean synthetic numerosity benchmark comprising two complementary datasets: GrayCount250 for controlled scaling studies, and NaturalCount6 featuring complex naturalistic scenes. Second, we empirically show that the scaling hypothesis does not hold: larger models and datasets alone fail to improve counting accuracy on our benchmark. Our analysis identifies a key reason: diffusion models tend to rely heavily on the noise initialization rather than the explicit numerosity specified in the prompt. We observe that noise priors exhibit biases toward specific object counts. In addition, we propose an effective strategy for controlling numerosity by injecting count-aware layout information into the noise prior. Our method achieves significant gains, improving accuracy on GrayCount250 from 20.0\% to 85.3\% and on NaturalCount6 from 74.8\% to 86.3\%, demonstrating effective generalization across settings.</li>
<li><strong>摘要：</strong>对于 FLUX 和 GPT-4o 等最先进的文本到图像生成模型来说，数量仍然是一个挑战，这些模型通常无法准确遵循文本提示中的计数指令。在本文中，我们旨在研究一个基本但经常被忽视的问题：扩散模型是否可以通过扩大数据集和模型大小来固有地生成由文本提示指定的正确数量的对象？为了实现严格且可重复的评估，我们构建了一个干净的综合数值基准，其中包含两个互补的数据集：用于受控缩放研究的 GrayCount250 和具有复杂自然场景的 NaturalCount6。其次，我们凭经验证明缩放假设不成立：仅较大的模型和数据集无法提高我们基准的计数准确性。我们的分析确定了一个关键原因：扩散模型往往严重依赖噪声初始化，而不是提示中指定的显式数量。我们观察到噪声先验对特定对象计数表现出偏差。此外，我们提出了一种通过将计数感知布局信息注入噪声先验来控制数量的有效策略。我们的方法取得了显着的成果，将 GrayCount250 的准确率从 20.0% 提高到 85.3%，将 NaturalCount6 的准确率从 74.8% 提高到 86.3%，证明了跨设置的有效泛化。</li>
</ul>

<h3>Title: EAGER: Entropy-Aware GEneRation for Adaptive Inference-Time Scaling</h3>
<ul>
<li><strong>Authors: </strong>Daniel Scalena, Leonidas Zotos, Elisabetta Fersini, Malvina Nissim, Ahmet Üstün</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11170">https://arxiv.org/abs/2510.11170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11170">https://arxiv.org/pdf/2510.11170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11170]] EAGER: Entropy-Aware GEneRation for Adaptive Inference-Time Scaling(https://arxiv.org/abs/2510.11170)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>With the rise of reasoning language models and test-time scaling methods as a paradigm for improving model performance, substantial computation is often required to generate multiple candidate sequences from the same prompt. This enables exploration of different reasoning paths toward the correct solution, however, allocates the same compute budget for each prompt. Grounded on the assumption that different prompts carry different degrees of complexity, and thus different computation needs, we propose EAGer, a training-free generation method that leverages model uncertainty through token-wise entropy distribution to reduce redundant computation and concurrently improve overall performance. EAGer allows branching to multiple reasoning paths only in the presence of high-entropy tokens, and then reallocates the saved compute budget to the instances where exploration of alternative paths is most needed. We find that across multiple open-source models on complex reasoning benchmarks such as AIME 2025, EAGer can reallocate the budget without accessing target labels, achieving the best efficiency-performance trade-off in terms of reasoning length and Pass@k. When target labels are accessible, EAGer generates up to 65% fewer tokens (hence saving compute) and achieves up to 37% improvement in Pass@k compared to the Full Parallel Sampling.</li>
<li><strong>摘要：</strong>随着推理语言模型和测试时间缩放方法作为提高模型性能范例的兴起，通常需要大量计算才能从同一提示生成多个候选序列。这使得能够探索不同的推理路径以获得正确的解决方案，但是，为每个提示分配相同的计算预算。基于不同提示具有不同复杂程度，因此计算需求不同的假设，我们提出了 EAGer，一种免训练生成方法，通过 token-wise 熵分布利用模型不确定性来减少冗余计算，同时提高整体性能。 EAGer 仅在存在高熵令牌的情况下才允许分支到多个推理路径，然后将节省的计算预算重新分配到最需要探索替代路径的实例。我们发现，在 AIME 2025 等复杂推理基准上的多个开源模型中，EAGer 可以在不访问目标标签的情况下重新分配预算，从而在推理长度和 Pass@k 方面实现最佳效率与性能权衡。当目标标签可访问时，EAGer 生成的令牌最多可减少 65%（从而节省计算量），并且与完全并行采样相比，Pass@k 的性能提高最多 37%。</li>
</ul>

<h3>Title: CoPRS: Learning Positional Prior from Chain-of-Thought for Reasoning Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Zhenyu Lu, Liupeng Li, Jinpeng Wang, Yan Feng, Bin Chen, Ke Chen, Yaowei Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11173">https://arxiv.org/abs/2510.11173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11173">https://arxiv.org/pdf/2510.11173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11173]] CoPRS: Learning Positional Prior from Chain-of-Thought for Reasoning Segmentation(https://arxiv.org/abs/2510.11173)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Existing works on reasoning segmentation either connect hidden features from a language model directly to a mask decoder or represent positions in text, which limits interpretability and semantic detail. To solve this, we present CoPRS, a Multi-modal Chain-of-Thought (MCoT)-based positional perception model that bridges language reasoning to segmentation through a differentiable and interpretable positional prior instantiated as a heatmap. By making the reasoning process clear via MCoT and expressing it as a dense, differentiable heatmap, this interface enhances interpretability and diagnostic analysis and yields more concentrated evidence on the target. A learnable concentration token aggregates features of the image and reasoning text to generate this positional prior, which is decoded to precise masks through a lightweight decoder, providing a direct connection between reasoning and segmentation. Across the RefCOCO series and ReasonSeg, CoPRS matches or surpasses the best reported metrics on each standard split under comparable protocols, with performance at or above prior state of the art across both validation and test partitions. Extensive experiments reveal that the quality of the heatmap strongly influences the resulting mask quality, supporting a consistent association between the reasoning output and downstream mask generation. Collectively, these findings support the utility of this paradigm in bridging reasoning and segmentation and show advantages in concentration driven by reasoning and predicting masks more precisely. Code, checkpoints and logs are released at this https URL.</li>
<li><strong>摘要：</strong>现有的推理分割工作要么将语言模型中的隐藏特征直接连接到掩码解码器，要么表示文本中的位置，这限制了可解释性和语义细节。为了解决这个问题，我们提出了 CoPRS，一种基于多模态思想链 (MCoT) 的位置感知模型，通过实例化为热图的可微分和可解释的位置先验，将语言推理与分割联系起来。通过 MCoT 使推理过程变得清晰并将其表达为密集的、可微的热图，该界面增强了可解释性和诊断分析，并产生了关于目标的更集中的证据。可学习的集中标记聚合图像和推理文本的特征以生成该位置先验，该先验通过轻量级解码器解码为精确的掩模，从而提供推理和分割之间的直接连接。在 RefCOCO 系列和 ReasonSeg 中，CoPRS 匹配或超过了可比较协议下每个标准分割的最佳报告指标，在验证和测试分区方面的性能达到或高于现有技术水平。大量实验表明，热图的质量强烈影响生成的掩模质量，支持推理输出和下游掩模生成之间的一致关联。总的来说，这些发现支持了这种范式在桥接推理和分割方面的实用性，并显示出由推理和更精确地预测掩模驱动的注意力集中的优势。代码、检查点和日志在此 https URL 发布。</li>
</ul>

<h3>Title: A Large-Language-Model Assisted Automated Scale Bar Detection and Extraction Framework for Scanning Electron Microscopic Images</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Chen, Ruotong Yang, Zhengyang Zhang, Mehreen Ahmed, Yanming Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cond-mat.mtrl-sci, cs.AI, physics.data-an</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11260">https://arxiv.org/abs/2510.11260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11260">https://arxiv.org/pdf/2510.11260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11260]] A Large-Language-Model Assisted Automated Scale Bar Detection and Extraction Framework for Scanning Electron Microscopic Images(https://arxiv.org/abs/2510.11260)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Microscopic characterizations, such as Scanning Electron Microscopy (SEM), are widely used in scientific research for visualizing and analyzing microstructures. Determining the scale bars is an important first step of accurate SEM analysis; however, currently, it mainly relies on manual operations, which is both time-consuming and prone to errors. To address this issue, we propose a multi-modal and automated scale bar detection and extraction framework that provides concurrent object detection, text detection and text recognition with a Large Language Model (LLM) agent. The proposed framework operates in four phases; i) Automatic Dataset Generation (Auto-DG) model to synthesize a diverse dataset of SEM images ensuring robust training and high generalizability of the model, ii) scale bar object detection, iii) information extraction using a hybrid Optical Character Recognition (OCR) system with DenseNet and Convolutional Recurrent Neural Network (CRNN) based algorithms, iv) an LLM agent to analyze and verify accuracy of the results. The proposed model demonstrates a strong performance in object detection and accurate localization with a precision of 100%, recall of 95.8%, and a mean Average Precision (mAP) of 99.2% at IoU=0.5 and 69.1% at IoU=0.5:0.95. The hybrid OCR system achieved 89% precision, 65% recall, and a 75% F1 score on the Auto-DG dataset, significantly outperforming several mainstream standalone engines, highlighting its reliability for scientific image analysis. The LLM is introduced as a reasoning engine as well as an intelligent assistant that suggests follow-up steps and verifies the results. This automated method powered by an LLM agent significantly enhances the efficiency and accuracy of scale bar detection and extraction in SEM images, providing a valuable tool for microscopic analysis and advancing the field of scientific imaging.</li>
<li><strong>摘要：</strong>微观表征，例如扫描电子显微镜 (SEM)，广泛应用于科学研究中，用于可视化和分析微观结构。确定比例尺是精确 SEM 分析的重要第一步；但目前主要依靠人工操作，既费时又容易出错。为了解决这个问题，我们提出了一种多模式和自动化比例尺检测和提取框架，该框架通过大型语言模型（LLM）代理提供并发对象检测、文本检测和文本识别。拟议框架分四个阶段运作； i) 自动数据集生成 (Auto-DG) 模型，用于合成 SEM 图像的多样化数据集，确保模型的稳健训练和高通用性，ii) 比例尺对象检测，iii) 使用混合光学字符识别 (OCR) 系统与基于 DenseNet 和卷积循环神经网络 (CRNN) 的算法进行信息提取，iv) LLM 代理来分析和验证模型的准确性 结果。所提出的模型在目标检测和准确定位方面表现出强大的性能，精度为 100%，召回率为 95.8%，平均精度（mAP）在 IoU=0.5 时为 99.2%，在 IoU=0.5:0.95 时为 69.1%。混合OCR系统在Auto-DG数据集上实现了89%的准确率、65%的召回率和75%的F1分数，显着优于几种主流的独立引擎，凸显了其科学图像分析的可靠性。法学硕士作为推理引擎和智能助手被引入，可以建议后续步骤并验证结果。这种由 LLM 代理支持的自动化方法显着提高了 SEM 图像中比例尺检测和提取的效率和准确性，为显微分析和推进科学成像领域提供了宝贵的工具。</li>
</ul>

<h3>Title: EEMS: Edge-Prompt Enhanced Medical Image Segmentation Based on Learnable Gating Mechanism</h3>
<ul>
<li><strong>Authors: </strong>Han Xia, Quanjun Li, Qian Li, Zimeng Li, Hongbin Ye, Yupeng Liu, Haolun Li, Xuhang Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11287">https://arxiv.org/abs/2510.11287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11287">https://arxiv.org/pdf/2510.11287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11287]] EEMS: Edge-Prompt Enhanced Medical Image Segmentation Based on Learnable Gating Mechanism(https://arxiv.org/abs/2510.11287)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Medical image segmentation is vital for diagnosis, treatment planning, and disease monitoring but is challenged by complex factors like ambiguous edges and background noise. We introduce EEMS, a new model for segmentation, combining an Edge-Aware Enhancement Unit (EAEU) and a Multi-scale Prompt Generation Unit (MSPGU). EAEU enhances edge perception via multi-frequency feature extraction, accurately defining boundaries. MSPGU integrates high-level semantic and low-level spatial features using a prompt-guided approach, ensuring precise target localization. The Dual-Source Adaptive Gated Fusion Unit (DAGFU) merges edge features from EAEU with semantic features from MSPGU, enhancing segmentation accuracy and robustness. Tests on datasets like ISIC2018 confirm EEMS's superior performance and reliability as a clinical tool.</li>
<li><strong>摘要：</strong>医学图像分割对于诊断、治疗计划和疾病监测至关重要，但受到模糊边缘和背景噪声等复杂因素的挑战。我们引入了 EEMS，这是一种新的分割模型，结合了边缘感知增强单元 (EAEU) 和多尺度提示生成单元 (MSPGU)。 EAEU 通过多频特征提取增强边缘感知，准确定义边界。 MSPGU 使用提示引导方法集成高级语义和低级空间特征，确保精确的目标定位。双源自适应门控融合单元 (DAGFU) 将 EAEU 的边缘特征与 MSPGU 的语义特征合并，提高了分割精度和鲁棒性。对 ISIC2018 等数据集的测试证实了 EEMS 作为临床工具的卓越性能和可靠性。</li>
</ul>

<h3>Title: sketch2symm: Symmetry-aware sketch-to-shape generation via semantic bridging</h3>
<ul>
<li><strong>Authors: </strong>Yan Zhou (1), Mingji Li (2), Xiantao Zeng (2), Jie Lin (1), Yuexia Zhou (1) ((1) School of Electronic Information Engineering, Foshan University, Guangdong, China, (2) School of Computer Science and Artificial Intelligence, Foshan University, Guangdong, China)</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11303">https://arxiv.org/abs/2510.11303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11303">https://arxiv.org/pdf/2510.11303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11303]] sketch2symm: Symmetry-aware sketch-to-shape generation via semantic bridging(https://arxiv.org/abs/2510.11303)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Sketch-based 3D reconstruction remains a challenging task due to the abstract and sparse nature of sketch inputs, which often lack sufficient semantic and geometric information. To address this, we propose Sketch2Symm, a two-stage generation method that produces geometrically consistent 3D shapes from sketches. Our approach introduces semantic bridging via sketch-to-image translation to enrich sparse sketch representations, and incorporates symmetry constraints as geometric priors to leverage the structural regularity commonly found in everyday objects. Experiments on mainstream sketch datasets demonstrate that our method achieves superior performance compared to existing sketch-based reconstruction methods in terms of Chamfer Distance, Earth Mover's Distance, and F-Score, verifying the effectiveness of the proposed semantic bridging and symmetry-aware design.</li>
<li><strong>摘要：</strong>由于草图输入的抽象性和稀疏性，通常缺乏足够的语义和几何信息，基于草图的 3D 重建仍然是一项具有挑战性的任务。为了解决这个问题，我们提出了 Sketch2Symm，这是一种两阶段生成方法，可以从草图生成几何一致的 3D 形状。我们的方法通过草图到图像的转换引入语义桥接，以丰富稀疏的草图表示，并将对称约束作为几何先验，以利用日常物体中常见的结构规律。在主流草图数据集上的实验表明，与现有的基于草图的重建方法相比，我们的方法在倒角距离、地球移动器距离和 F-Score 方面实现了优越的性能，验证了所提出的语义桥接和对称感知设计的有效性。</li>
</ul>

<h3>Title: DiffStyleTS: Diffusion Model for Style Transfer in Time Series</h3>
<ul>
<li><strong>Authors: </strong>Mayank Nagda, Phil Ostheimer, Justus Arweiler, Indra Jungjohann, Jennifer Werner, Dennis Wagner, Aparna Muraleedharan, Pouya Jafari, Jochen Schmid, Fabian Jirasek, Jakob Burger, Michael Bortz, Hans Hasse, Stephan Mandt, Marius Kloft, Sophie Fellenz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11335">https://arxiv.org/abs/2510.11335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11335">https://arxiv.org/pdf/2510.11335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11335]] DiffStyleTS: Diffusion Model for Style Transfer in Time Series(https://arxiv.org/abs/2510.11335)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Style transfer combines the content of one signal with the style of another. It supports applications such as data augmentation and scenario simulation, helping machine learning models generalize in data-scarce domains. While well developed in vision and language, style transfer methods for time series data remain limited. We introduce DiffTSST, a diffusion-based framework that disentangles a time series into content and style representations via convolutional encoders and recombines them through a self-supervised attention-based diffusion process. At inference, encoders extract content and style from two distinct series, enabling conditional generation of novel samples to achieve style transfer. We demonstrate both qualitatively and quantitatively that DiffTSST achieves effective style transfer. We further validate its real-world utility by showing that data augmentation with DiffTSST improves anomaly detection in data-scarce regimes.</li>
<li><strong>摘要：</strong>风格转换将一种信号的内容与另一种信号的风格结合起来。它支持数据增强和场景模拟等应用，帮助机器学习模型在数据稀缺领域进行泛化。尽管视觉和语言已得到很好的发展，但时间序列数据的风格迁移方法仍然有限。我们引入了 DiffTSST，这是一种基于扩散的框架，它通过卷积编码器将时间序列分解为内容和风格表示，并通过自我监督的基于注意力的扩散过程将它们重新组合。在推理时，编码器从两个不同的系列中提取内容和风格，从而能够有条件地生成新样本以实现风格迁移。我们定性和定量地证明了 DiffTSST 实现了有效的风格迁移。我们通过展示 DiffTSST 的数据增强改进了数据稀缺情况下的异常检测，进一步验证了其在现实世界中的实用性。</li>
</ul>

<h3>Title: REACT3D: Recovering Articulations for Interactive Physical 3D Scenes</h3>
<ul>
<li><strong>Authors: </strong>Zhao Huang, Boyang Sun, Alexandros Delitzas, Jiaqi Chen, Marc Pollefeys</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11340">https://arxiv.org/abs/2510.11340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11340">https://arxiv.org/pdf/2510.11340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11340]] REACT3D: Recovering Articulations for Interactive Physical 3D Scenes(https://arxiv.org/abs/2510.11340)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Interactive 3D scenes are increasingly vital for embodied intelligence, yet existing datasets remain limited due to the labor-intensive process of annotating part segmentation, kinematic types, and motion trajectories. We present REACT3D, a scalable zero-shot framework that converts static 3D scenes into simulation-ready interactive replicas with consistent geometry, enabling direct use in diverse downstream tasks. Our contributions include: (i) openable-object detection and segmentation to extract candidate movable parts from static scenes, (ii) articulation estimation that infers joint types and motion parameters, (iii) hidden-geometry completion followed by interactive object assembly, and (iv) interactive scene integration in widely supported formats to ensure compatibility with standard simulation platforms. We achieve state-of-the-art performance on detection/segmentation and articulation metrics across diverse indoor scenes, demonstrating the effectiveness of our framework and providing a practical foundation for scalable interactive scene generation, thereby lowering the barrier to large-scale research on articulated scene understanding. Our project page is \textit{\hypersetup{urlcolor=black}\href{this https URL}{this http URL}}.</li>
<li><strong>摘要：</strong>交互式 3D 场景对于体现智能越来越重要，但由于注释零件分割、运动学类型和运动轨迹的劳动密集型过程，现有数据集仍然有限。我们推出了 REACT3D，这是一种可扩展的零镜头框架，可将静态 3D 场景转换为具有一致几何形状的模拟就绪交互式副本，从而能够直接用于各种下游任务。我们的贡献包括：（i）可打开对象检测和分割，从静态场景中提取候选可移动部件，（ii）推断关节类型和运动参数的关节估计，（iii）隐藏几何完成，然后进行交互式对象组装，以及（iv）以广泛支持的格式进行交互式场景集成，以确保与标准仿真平台的兼容性。我们在不同室内场景的检测/分割和清晰度指标方面实现了最先进的性能，展示了我们框架的有效性，并为可扩展的交互式场景生成提供了实用基础，从而降低了大规模研究清晰度场景理解的障碍。我们的项目页面是 \textit{\hypersetup{urlcolor=black}\href{这个 https URL}{这个 http URL}}。</li>
</ul>

<h3>Title: InternSVG: Towards Unified SVG Tasks with Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haomin Wang, Jinhui Yin, Qi Wei, Wenguang Zeng, Lixin Gu, Shenglong Ye, Zhangwei Gao, Yaohui Wang, Yanting Zhang, Yuanqi Li, Yanwen Guo, Wenhai Wang, Kai Chen, Yu Qiao, Hongjie Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11341">https://arxiv.org/abs/2510.11341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11341">https://arxiv.org/pdf/2510.11341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11341]] InternSVG: Towards Unified SVG Tasks with Multimodal Large Language Models(https://arxiv.org/abs/2510.11341)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>General SVG modeling remains challenging due to fragmented datasets, limited transferability of methods across tasks, and the difficulty of handling structural complexity. In response, we leverage the strong transfer and generalization capabilities of multimodal large language models (MLLMs) to achieve unified modeling for SVG understanding, editing, and generation. We present the InternSVG family, an integrated data-benchmark-model suite. At its core is SAgoge, the largest and most comprehensive multimodal dataset for SVG tasks, encompassing both static graphics and dynamic animations. It covers icons, long-sequence illustrations, scientific diagrams, and dynamic animations, supporting tasks of varied difficulty levels and providing deeper hierarchies with richer attributes compared to previous datasets. Based on this resource, we introduce SArena, a companion benchmark with comprehensive task definitions and standardized evaluation that aligns with the domains and difficulty spectrum covered by SAgoge. Building on these foundations, we propose InternSVG, a unified MLLM for SVG understanding, editing, and generation with SVG-specific special tokens, subword-based embedding initialization, and a two-stage training strategy that progresses from short static SVGs to long-sequence illustrations and complex animations. This unified formulation induces positive transfer and improves overall performance. Experiments on SArena and prior benchmark confirm that InternSVG achieves substantial gains and consistently outperforms leading open and proprietary counterparts.</li>
<li><strong>摘要：</strong>由于数据集分散、跨任务方法的可移植性有限以及处理结构复杂性的困难，通用 SVG 建模仍然具有挑战性。为此，我们利用多模态大语言模型（MLLM）强大的迁移和泛化能力来实现 SVG 理解、编辑和生成的统一建模。我们推出了 InternSVG 系列，这是一个集成的数据基准模型套件。其核心是 SAgoge，这是最大、最全面的 SVG 任务多模式数据集，涵盖静态图形和动态动画。它涵盖了图标、长序列插图、科学图表和动态动画，支持不同难度级别的任务，并提供比以前的数据集更深的层次结构和更丰富的属性。基于此资源，我们推出了 SArena，这是一个配套基准测试，具有全面的任务定义和标准化评估，与 SAgoge 涵盖的领域和难度范围相一致。在此基础上，我们提出了 InternSVG，这是一个用于 SVG 理解、编辑和生成的统一 MLLM，具有 SVG 特定的特殊标记、基于子字的嵌入初始化以及从短静态 SVG 到长序列插图和复杂动画的两阶段训练策略。这种统一的配方可诱导正向迁移并提高整体性能。 SArena 和先前基准测试的实验证实，InternSVG 取得了巨大的进步，并且始终优于领先的开放和专有同行。</li>
</ul>

<h3>Title: Uncertainty-Aware ControlNet: Bridging Domain Gaps with Synthetic Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Joshua Niemeijer, Jan Ehrhardt, Heinz Handels, Hristina Uzunova</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11346">https://arxiv.org/abs/2510.11346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11346">https://arxiv.org/pdf/2510.11346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11346]] Uncertainty-Aware ControlNet: Bridging Domain Gaps with Synthetic Image Generation(https://arxiv.org/abs/2510.11346)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative Models are a valuable tool for the controlled creation of high-quality image data. Controlled diffusion models like the ControlNet have allowed the creation of labeled distributions. Such synthetic datasets can augment the original training distribution when discriminative models, like semantic segmentation, are trained. However, this augmentation effect is limited since ControlNets tend to reproduce the original training distribution. This work introduces a method to utilize data from unlabeled domains to train ControlNets by introducing the concept of uncertainty into the control mechanism. The uncertainty indicates that a given image was not part of the training distribution of a downstream task, e.g., segmentation. Thus, two types of control are engaged in the final network: an uncertainty control from an unlabeled dataset and a semantic control from the labeled dataset. The resulting ControlNet allows us to create annotated data with high uncertainty from the target domain, i.e., synthetic data from the unlabeled distribution with labels. In our scenario, we consider retinal OCTs, where typically high-quality Spectralis images are available with given ground truth segmentations, enabling the training of segmentation networks. The recent development in Home-OCT devices, however, yields retinal OCTs with lower quality and a large domain shift, such that out-of-the-pocket segmentation networks cannot be applied for this type of data. Synthesizing annotated images from the Home-OCT domain using the proposed approach closes this gap and leads to significantly improved segmentation results without adding any further supervision. The advantage of uncertainty-guidance becomes obvious when compared to style transfer: it enables arbitrary domain shifts without any strict learning of an image style. This is also demonstrated in a traffic scene experiment.</li>
<li><strong>摘要：</strong>生成模型是用于受控创建高质量图像数据的宝贵工具。像 ControlNet 这样的受控扩散模型已经允许创建标记分布。当训练判别模型（如语义分割）时，此类合成数据集可以增强原始训练分布。然而，这种增强效果是有限的，因为 ControlNet 倾向于重现原始训练分布。这项工作介绍了一种通过将不确定性概念引入控制机制来利用未标记域的数据来训练 ControlNet 的方法。不确定性表明给定图像不是下游任务（例如分割）训练分布的一部分。因此，最终网络中涉及两种类型的控制：来自未标记数据集的不确定性控制和来自标记数据集的语义控制。由此产生的 ControlNet 允许我们从目标域创建具有高度不确定性的注释数据，即来自带有标签的未标记分布的合成数据。在我们的场景中，我们考虑视网膜 OCT，其中通常可以使用给定的地面真实分割提供高质量的 Spectralis 图像，从而能够训练分割网络。然而，Home-OCT 设备的最新发展产生了质量较低且域偏移较大的视网膜 OCT，因此现成的分割网络无法应用于此类数据。使用所提出的方法合成来自 Home-OCT 域的带注释图像可以弥补这一差距，并在无需添加任何进一步监督的情况下显着改善分割结果。与风格迁移相比，不确定性指导的优势变得显而易见：它可以实现任意域变换，而无需对图像风格进行任何严格的学习。这也在交通场景实验中得到了证明。</li>
</ul>

<h3>Title: Reasoning as Representation: Rethinking Visual Reinforcement Learning in Image Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Shijie Zhao, Xuanyu Zhang, Weiqi Li, Junlin Li, Li Zhang, Tianfan Xue, Jian Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11369">https://arxiv.org/abs/2510.11369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11369">https://arxiv.org/pdf/2510.11369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11369]] Reasoning as Representation: Rethinking Visual Reinforcement Learning in Image Quality Assessment(https://arxiv.org/abs/2510.11369)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Reasoning-based image quality assessment (IQA) models trained through reinforcement learning (RL) exhibit exceptional generalization, yet the underlying mechanisms and critical factors driving this capability remain underexplored in current research. Moreover, despite their superior performance, these models incur inference energy usage and latency orders of magnitude higher than their earlier counterparts, restricting their deployment in specific scenarios. Through extensive experiments, this paper verifies and elaborates that through RL training, MLLMs leverage their reasoning capability to convert redundant visual representations into compact, cross-domain aligned text representations. This conversion is precisely the source of the generalization exhibited by these reasoning-based IQA models. Building on this fundamental insight, we propose a novel algorithm, RALI, which employs contrastive learning to directly align images with these generalizable text representations learned by RL. This approach eliminates the reliance on reasoning processes and even obviates the need to load an LLM. For the quality scoring task, this framework achieves generalization performance comparable to reasoning-based models while requiring less than 5% of their model parameters and inference time.</li>
<li><strong>摘要：</strong>通过强化学习 (RL) 训练的基于推理的图像质量评估 (IQA) 模型表现出卓越的泛化能力，但目前的研究尚未充分探索驱动这种能力的潜在机制和关键因素。此外，尽管性能优越，但这些模型的推理能耗和延迟时间比早期模型高出几个数量级，从而限制了它们在特定场景中的部署。通过大量的实验，本文验证并阐述了通过 RL 训练，MLLM 利用其推理能力将冗余的视觉表示转换为紧凑的、跨域对齐的文本表示。这种转换正是这些基于推理的 IQA 模型所表现出的泛化能力的来源。基于这一基本见解，我们提出了一种新颖的算法 RALI，它采用对比学习来直接将图像与 RL 学习到的通用文本表示进行对齐。这种方法消除了对推理过程的依赖，甚至不需要加载法学硕士。对于质量评分任务，该框架实现了与基于推理的模型相当的泛化性能，同时需要不到 5% 的模型参数和推理时间。</li>
</ul>

<h3>Title: DocReward: A Document Reward Model for Structuring and Stylizing</h3>
<ul>
<li><strong>Authors: </strong>Junpeng Liu, Yuzhong Zhao, Bowen Cao, Jiayu Ding, Yilin Jia, Tengchao Lv, Yupan Huang, Shaohan Huang, Nan Yang, Li Dong, Lei Cui, Tao Ge, Xun Wang, Huitian Jiao, Sun Mao, FNU Kartik, Si-Qing Chen, Wai Lam, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11391">https://arxiv.org/abs/2510.11391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11391">https://arxiv.org/pdf/2510.11391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11391]] DocReward: A Document Reward Model for Structuring and Stylizing(https://arxiv.org/abs/2510.11391)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in agentic workflows have enabled the automation of tasks such as professional document generation. However, they primarily focus on textual quality, neglecting visual structure and style, which are crucial for readability and engagement. This gap arises mainly from the absence of suitable reward models to guide agentic workflows toward producing documents with stronger structural and stylistic quality. To address this, we propose DocReward, a document reward model that evaluates documents based on their structure and style. We construct a multi-domain dataset DocPair of 117K paired documents, covering 32 domains and 267 document types, each including a high- and low-professionalism document with identical content but different structure and style. This enables the model to evaluate professionalism comprehensively, and in a textual-quality-agnostic way. DocReward is trained using the Bradley-Terry loss to score documents, penalizing predictions that contradict the annotated ranking. To assess the performance of reward models, we create a test dataset containing document bundles ranked by well-educated human evaluators. Notably, DocReward outperforms GPT-4o and GPT-5 in accuracy by 30.6 and 19.4 percentage points, respectively, demonstrating its superiority over baselines. In an extrinsic evaluation of document generation, DocReward achieves a significantly higher win rate of 60.8%, compared to GPT-5's 37.7% win rate, demonstrating its utility in guiding generation agents toward producing human-preferred documents.</li>
<li><strong>摘要：</strong>代理工作流程的最新进展实现了专业文档生成等任务的自动化。然而，他们主要关注文本质量，忽略了视觉结构和风格，而这对于可读性和参与度至关重要。这种差距主要是由于缺乏合适的奖励模型来指导代理工作流程生成具有更强结构和风格质量的文档。为了解决这个问题，我们提出了 DocReward，这是一种根据文档结构和风格评估文档的文档奖励模型。我们构建了一个包含 117K 配对文档的多领域数据集 DocPair，涵盖 32 个领域和 267 个文档类型，每个文档都包含内容相同但结构和风格不同的高专业度和低专业度文档。这使得模型能够以与文本质量无关的方式全面评估专业水平。 DocReward 使用 Bradley-Terry 损失对文档进行训练，对与注释排名相矛盾的预测进行惩罚。为了评估奖励模型的性能，我们创建了一个测试数据集，其中包含由受过良好教育的人类评估者排名的文档包。值得注意的是，DocReward 的准确率比 GPT-4o 和 GPT-5 分别高出 30.6 和 19.4 个百分点，证明了其相对于基线的优越性。在文档生成的外部评估中，DocReward 实现了 60.8% 的胜率，远高于 GPT-5 37.7% 的胜率，这证明了它在指导生成代理生成人类首选文档方面的实用性。</li>
</ul>

<h3>Title: Reconstructing 12-Lead ECG from 3-Lead ECG using Variational Autoencoder to Improve Cardiac Disease Detection of Wearable ECG Devices</h3>
<ul>
<li><strong>Authors: </strong>Xinyan Guan, Yongfan Lai, Jiarui Jin, Jun Li, Haoyu Wang, Qinghao Zhao, Deyun Zhang, Shijia Geng, Shenda Hong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11442">https://arxiv.org/abs/2510.11442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11442">https://arxiv.org/pdf/2510.11442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11442]] Reconstructing 12-Lead ECG from 3-Lead ECG using Variational Autoencoder to Improve Cardiac Disease Detection of Wearable ECG Devices(https://arxiv.org/abs/2510.11442)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Twelve-lead electrocardiograms (ECGs) are the clinical gold standard for cardiac diagnosis, providing comprehensive spatial coverage of the heart necessary to detect conditions such as myocardial infarction (MI). However, their lack of portability limits continuous and large-scale use. Three-lead ECG systems are widely used in wearable devices due to their simplicity and mobility, but they often fail to capture pathologies in unmeasured regions. To address this, we propose WearECG, a Variational Autoencoder (VAE) method that reconstructs twelve-lead ECGs from three leads: II, V1, and V5. Our model includes architectural improvements to better capture temporal and spatial dependencies in ECG signals. We evaluate generation quality using MSE, MAE, and Frechet Inception Distance (FID), and assess clinical validity via a Turing test with expert cardiologists. To further validate diagnostic utility, we fine-tune ECGFounder, a large-scale pretrained ECG model, on a multi-label classification task involving over 40 cardiac conditions, including six different myocardial infarction locations, using both real and generated signals. Experiments on the MIMIC dataset show that our method produces physiologically realistic and diagnostically informative signals, with robust performance in downstream tasks. This work demonstrates the potential of generative modeling for ECG reconstruction and its implications for scalable, low-cost cardiac screening.</li>
<li><strong>摘要：</strong>十二导联心电图 (ECG) 是心脏诊断的临床金标准，可提供检测心肌梗塞 (MI) 等疾病所需的全面的心脏空间覆盖。然而，它们缺乏便携性限制了连续和大规模的使用。三导联心电图系统因其简单性和移动性而广泛应用于可穿戴设备，但它们通常无法捕获未测量区域的病理情况。为了解决这个问题，我们提出了 WearECG，一种变分自动编码器 (VAE) 方法，可以从三个导联重建十二导联心电图：II、V1 和 V5。我们的模型包括架构改进，以更好地捕获心电图信号中的时间和空间依赖性。我们使用 MSE、MAE 和 Frechet 起始距离 (FID) 评估生成质量，并通过心脏病专家的图灵测试评估临床有效性。为了进一步验证诊断效用，我们使用真实信号和生成信号，在涉及 40 多种心脏疾病（包括六种不同的心肌梗塞位置）的多标签分类任务中对 ECGFounder（一种大规模预训练心电图模型）进行了微调。 MIMIC 数据集上的实验表明，我们的方法产生了生理上真实且具有诊断信息的信号，并且在下游任务中具有强大的性能。这项工作展示了心电图重建生成模型的潜力及其对可扩展、低成本心脏筛查的影响。</li>
</ul>

<h3>Title: ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web Coding</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Li, Chenchen Zhang, Ruilin Lv, Ao Liu, Ken Deng, Yuanxing Zhang, Jiaheng Liu, Wiggin Zhou, Bo Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11498">https://arxiv.org/abs/2510.11498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11498">https://arxiv.org/pdf/2510.11498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11498]] ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web Coding(https://arxiv.org/abs/2510.11498)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) excel at algorithmic code generation, they struggle with front-end development, where correctness is judged on rendered pixels and interaction. We present ReLook, an agentic, vision-grounded reinforcement learning framework that empowers an agent to close a robust generate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool. During training, the agent uses the MLLM-in-the-loop both as a visual critic--scoring code with screenshots--and as a source of actionable, vision-grounded feedback; a strict zero-reward rule for invalid renders anchors renderability and prevents reward hacking. To prevent behavioral collapse, we introduce Forced Optimization, a strict acceptance rule that admits only improving revisions, yielding monotonically better trajectories. At inference, we decouple the critic and run a lightweight, critic-free self-edit cycle, keeping latency comparable to base decoding while retaining most of the gains. Across three widely used benchmarks, ReLook consistently outperforms strong baselines in vision-grounded front-end code generation, highlighting the benefits of agentic perception, visual rewards, and training-inference decoupling.</li>
<li><strong>摘要：</strong>虽然大型语言模型 (LLM) 擅长算法代码生成，但它们在前端开发方面遇到了困难，前端开发的正确性是根据渲染的像素和交互来判断的。我们提出了 ReLook，这是一个基于视觉的代理强化学习框架，它使代理能够通过调用多模态 LLM (MLLM) 作为工具来关闭强大的生成-诊断-优化循环。在训练期间，代理使用 MLLM-in-the-loop 既作为视觉评论家（用屏幕截图对代码进行评分），又作为可操作的、基于视觉的反馈来源；针对无效渲染的严格的零奖励规则可以锚定渲染能力并防止奖励黑客攻击。为了防止行为崩溃，我们引入了强制优化，这是一种严格的接受规则，只允许改进修订，产生单调更好的轨迹。在推理时，我们将批评家解耦并运行一个轻量级、无批评家的自编辑周期，使延迟与基本解码相当，同时保留大部分收益。在三个广泛使用的基准测试中，ReLook 在基于视觉的前端代码生成方面始终优于强大的基线，突出了代理感知、视觉奖励和训练推理解耦的好处。</li>
</ul>

<h3>Title: Offline Reinforcement Learning with Generative Trajectory Policies</h3>
<ul>
<li><strong>Authors: </strong>Xinsong Feng, Leshu Tang, Chenan Wang, Haipeng Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11499">https://arxiv.org/abs/2510.11499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11499">https://arxiv.org/pdf/2510.11499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11499]] Offline Reinforcement Learning with Generative Trajectory Policies(https://arxiv.org/abs/2510.11499)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models have emerged as a powerful class of policies for offline reinforcement learning (RL) due to their ability to capture complex, multi-modal behaviors. However, existing methods face a stark trade-off: slow, iterative models like diffusion policies are computationally expensive, while fast, single-step models like consistency policies often suffer from degraded performance. In this paper, we demonstrate that it is possible to bridge this gap. The key to moving beyond the limitations of individual methods, we argue, lies in a unifying perspective that views modern generative models, including diffusion, flow matching, and consistency models, as specific instances of learning a continuous-time generative trajectory governed by an Ordinary Differential Equation (ODE). This principled foundation provides a clearer design space for generative policies in RL and allows us to propose Generative Trajectory Policies (GTPs), a new and more general policy paradigm that learns the entire solution map of the underlying ODE. To make this paradigm practical for offline RL, we further introduce two key theoretically principled adaptations. Empirical results demonstrate that GTP achieves state-of-the-art performance on D4RL benchmarks - it significantly outperforms prior generative policies, achieving perfect scores on several notoriously hard AntMaze tasks.</li>
<li><strong>摘要：</strong>由于生成模型能够捕获复杂的多模式行为，因此它已成为离线强化学习 (RL) 的一类强大策略。然而，现有的方法面临着一个严峻的权衡：像扩散策略这样的缓慢的迭代模型在计算上是昂贵的，而像一致性策略这样的快速单步模型通常会出现性能下降的问题。在本文中，我们证明了弥合这一差距是可能的。我们认为，超越个别方法局限性的关键在于一个统一的视角，将现代生成模型（包括扩散、流动匹配和一致性模型）视为学习由常微分方程（ODE）控制的连续时间生成轨迹的具体实例。这个原则性的基础为强化学习中的生成策略提供了更清晰的设计空间，并使我们能够提出生成轨迹策略（GTP），这是一种新的、更通用的策略范式，可以学习底层 ODE 的整个解决方案图。为了使这种范式适用于离线强化学习，我们进一步介绍了两个关键的理论原理改编。实证结果表明，GTP 在 D4RL 基准上实现了最先进的性能 - 它显着优于先前的生成策略，在几个众所周知的困难 AntMaze 任务上取得了满分。</li>
</ul>

<h3>Title: Learning to Make MISTAKEs: Modeling Incorrect Student Thinking And Key Errors</h3>
<ul>
<li><strong>Authors: </strong>Alexis Ross, Jacob Andreas</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11502">https://arxiv.org/abs/2510.11502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11502">https://arxiv.org/pdf/2510.11502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11502]] Learning to Make MISTAKEs: Modeling Incorrect Student Thinking And Key Errors(https://arxiv.org/abs/2510.11502)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Research on reasoning in language models (LMs) predominantly focuses on improving the correctness of their outputs. But some important applications require modeling reasoning patterns that are incorrect. For example, automated systems that can reason about and simulate student errors are useful for providing real-time feedback in the classroom or offline practice for educators-in-training. This paper presents a new method, MISTAKE, that (1) constructs high-quality synthetic examples of reasoning errors by leveraging cycle consistency between incorrect answers and latent misconceptions; and (2) uses the generated data to learn models for student simulation, misconception classification, and answer generation. We evaluate MISTAKE on three educational tasks and find that it results in (1) higher accuracy when simulating incorrect student answers based on specific misconceptions, (2) increased performance inferring latent misconceptions from observed incorrect answers, and (3) higher alignment with expert-written distractor answers when generating incorrect answers (e.g., for multiple-choice tests).</li>
<li><strong>摘要：</strong>语言模型（LM）推理研究主要集中于提高其输出的正确性。但一些重要的应用程序需要对不正确的推理模式进行建模。例如，可以推理和模拟学生错误的自动化系统对于在课堂上或为受训教育者的离线练习中提供实时反馈非常有用。本文提出了一种新方法“MISTAKE”，该方法（1）通过利用错误答案和潜在误解之间的循环一致性来构造推理错误的高质量综合示例； (2) 使用生成的数据来学习用于学生模拟、误解分类和答案生成的模型。我们评估了三项教育任务上的 MISTAKE，发现它可以（1）在根​​据特定的误解模拟不正确的学生答案时提高准确性，（2）提高从观察到的错误答案推断潜在误解的性能，以及（3）在生成错误答案时（例如，多项选择测试）与专家编写的干扰答案的一致性更高。</li>
</ul>

<h3>Title: LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion Models via Likelihood Preference</h3>
<ul>
<li><strong>Authors: </strong>Jianhao Yuan, Fabio Pizzati, Francesco Pinto, Lars Kunze, Ivan Laptev, Paul Newman, Philip Torr, Daniele De Martini</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11512">https://arxiv.org/abs/2510.11512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11512">https://arxiv.org/pdf/2510.11512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11512]] LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion Models via Likelihood Preference(https://arxiv.org/abs/2510.11512)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Intuitive physics understanding in video diffusion models plays an essential role in building general-purpose physically plausible world simulators, yet accurately evaluating such capacity remains a challenging task due to the difficulty in disentangling physics correctness from visual appearance in generation. To the end, we introduce LikePhys, a training-free method that evaluates intuitive physics in video diffusion models by distinguishing physically valid and impossible videos using the denoising objective as an ELBO-based likelihood surrogate on a curated dataset of valid-invalid pairs. By testing on our constructed benchmark of twelve scenarios spanning over four physics domains, we show that our evaluation metric, Plausibility Preference Error (PPE), demonstrates strong alignment with human preference, outperforming state-of-the-art evaluator baselines. We then systematically benchmark intuitive physics understanding in current video diffusion models. Our study further analyses how model design and inference settings affect intuitive physics understanding and highlights domain-specific capacity variations across physical laws. Empirical results show that, despite current models struggling with complex and chaotic dynamics, there is a clear trend of improvement in physics understanding as model capacity and inference settings scale.</li>
<li><strong>摘要：</strong>视频扩散模型中直观的物理理解在构建通用的物理合理的世界模拟器中起着至关重要的作用，但准确评估这种能力仍然是一项具有挑战性的任务，因为在生成过程中很难将物理正确性与视觉外观分开。最后，我们介绍 LikePhys，这是一种免训练方法，通过使用去噪目标作为有效-无效对的精选数据集上基于 ELBO 的似然替代来区分物理上有效和不可能的视频，从而评估视频扩散模型中的直观物理现象。通过对我们构建的跨越四个物理领域的十二个场景的基准进行测试，我们表明我们的评估指标，合理性偏好误差（PPE），表现出与人类偏好的强烈一致性，优于最先进的评估器基线。然后，我们系统地对当前视频扩散模型中直观的物理理解进行基准测试。我们的研究进一步分析了模型设计和推理设置如何影响直观的物理理解，并强调了跨物理定律的特定领域的能力变化。实证结果表明，尽管当前模型难以应对复杂和混沌的动力学，但随着模型容量和推理​​设置规模的扩大，物理理解有明显的改善趋势。</li>
</ul>

<h3>Title: Massive Activations are the Key to Local Detail Synthesis in Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Chaofan Gan, Zicheng Zhao, Yuanpeng Tu, Xi Chen, Ziran Qin, Tieyuan Chen, Mehrtash Harandi, Weiyao Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11538">https://arxiv.org/abs/2510.11538</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11538">https://arxiv.org/pdf/2510.11538</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11538]] Massive Activations are the Key to Local Detail Synthesis in Diffusion Transformers(https://arxiv.org/abs/2510.11538)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion Transformers (DiTs) have recently emerged as a powerful backbone for visual generation. Recent observations reveal \emph{Massive Activations} (MAs) in their internal feature maps, yet their function remains poorly understood. In this work, we systematically investigate these activations to elucidate their role in visual generation. We found that these massive activations occur across all spatial tokens, and their distribution is modulated by the input timestep embeddings. Importantly, our investigations further demonstrate that these massive activations play a key role in local detail synthesis, while having minimal impact on the overall semantic content of output. Building on these insights, we propose \textbf{D}etail \textbf{G}uidance (\textbf{DG}), a MAs-driven, training-free self-guidance strategy to explicitly enhance local detail fidelity for DiTs. Specifically, DG constructs a degraded ``detail-deficient'' model by disrupting MAs and leverages it to guide the original network toward higher-quality detail synthesis. Our DG can seamlessly integrate with Classifier-Free Guidance (CFG), enabling further refinements of fine-grained details. Extensive experiments demonstrate that our DG consistently improves fine-grained detail quality across various pre-trained DiTs (\eg, SD3, SD3.5, and Flux).</li>
<li><strong>摘要：</strong>扩散变压器（DiT）最近已成为视觉生成的强大支柱。最近的观察揭示了它们内部特征图中的\emph{大规模激活}（MA），但它们的功能仍然知之甚少。在这项工作中，我们系统地研究这些激活，以阐明它们在视觉生成中的作用。我们发现这些大规模激活发生在所有空间标记中，并且它们的分布由输入时间步嵌入进行调制。重要的是，我们的研究进一步表明，这些大量激活在局部细节合成中发挥着关键作用，同时对输出的整体语义内容影响最小。基于这些见解，我们提出了 \textbf{D}etail \textbf{G}uidance (\textbf{DG})，这是一种 MA 驱动的、免训练的自我指导策略，可显式增强 DiT 的局部细节保真度。具体来说，DG 通过破坏 MA 构建了一个降级的“细节缺乏”模型，并利用它来引导原始网络进行更高质量的细节合成。我们的 DG 可以与无分类器指导 (CFG) 无缝集成，从而进一步细化细粒度细节。大量实验表明，我们的 DG 能够持续提高各种预训练 DiT（例如 SD3、SD3.5 和 Flux）的细粒度细节质量。</li>
</ul>

<h3>Title: Query-Specific GNN: A Comprehensive Graph Representation Learning Method for Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Yan, Zhihua Liu, Hao Wang, Weiming Li, Xiaoshuai Hao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11541">https://arxiv.org/abs/2510.11541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11541">https://arxiv.org/pdf/2510.11541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11541]] Query-Specific GNN: A Comprehensive Graph Representation Learning Method for Retrieval Augmented Generation(https://arxiv.org/abs/2510.11541)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) has demonstrated its ability to enhance Large Language Models (LLMs) by integrating external knowledge sources. However, multi-hop questions, which require the identification of multiple knowledge targets to form a synthesized answer, raise new challenges for RAG systems. Under the multi-hop settings, existing methods often struggle to fully understand the questions with complex semantic structures and are susceptible to irrelevant noise during the retrieval of multiple information targets. To address these limitations, we propose a novel graph representation learning framework for multi-hop question retrieval. We first introduce a Multi-information Level Knowledge Graph (Multi-L KG) to model various information levels for a more comprehensive understanding of multi-hop questions. Based on this, we design a Query-Specific Graph Neural Network (QSGNN) for representation learning on the Multi-L KG. QSGNN employs intra/inter-level message passing mechanisms, and in each message passing the information aggregation is guided by the query, which not only facilitates multi-granular information aggregation but also significantly reduces the impact of noise. To enhance its ability to learn robust representations, we further propose two synthesized data generation strategies for pre-training the QSGNN. Extensive experimental results demonstrate the effectiveness of our framework in multi-hop scenarios, especially in high-hop questions the improvement can reach 33.8\%. The code is available at: this https URL.</li>
<li><strong>摘要：</strong>检索增强生成（RAG）已经证明了其通过集成外部知识源来增强大型语言模型（LLM）的能力。然而，多跳问题需要识别多个知识目标以形成综合答案，这给 RAG 系统带来了新的挑战。在多跳设置下，现有方法往往难以完全理解具有复杂语义结构的问题，并且在检索多个信息目标时容易受到不相关噪声的影响。为了解决这些限制，我们提出了一种用于多跳问题检索的新颖的图表示学习框架。我们首先引入多信息级别知识图（Multi-L KG）来对各种信息级别进行建模，以便更全面地理解多跳问题。基于此，我们设计了一个查询特定图神经网络（QSGNN），用于 Multi-L KG 上的表示学习。 QSGNN采用层内/层间消息传递机制，在每次消息传递中都以查询为指导进行信息聚合，不仅有利于多粒度信息聚合，而且显着降低了噪声的影响。为了增强其学习鲁棒表示的能力，我们进一步提出了两种用于预训练 QSGNN 的综合数据生成策略。大量的实验结果证明了我们的框架在多跳场景中的有效性，特别是在高跳问题中，改进可以达到 33.8%。该代码位于：此 https URL。</li>
</ul>

<h3>Title: A Framework for Low-Effort Training Data Generation for Urban Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Denis Zavadski, Damjan Kalšan, Tim Küchler, Haebom Lee, Stefan Roth, Carsten Rother</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11567">https://arxiv.org/abs/2510.11567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11567">https://arxiv.org/pdf/2510.11567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11567]] A Framework for Low-Effort Training Data Generation for Urban Semantic Segmentation(https://arxiv.org/abs/2510.11567)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Synthetic datasets are widely used for training urban scene recognition models, but even highly realistic renderings show a noticeable gap to real imagery. This gap is particularly pronounced when adapting to a specific target domain, such as Cityscapes, where differences in architecture, vegetation, object appearance, and camera characteristics limit downstream performance. Closing this gap with more detailed 3D modelling would require expensive asset and scene design, defeating the purpose of low-cost labelled data. To address this, we present a new framework that adapts an off-the-shelf diffusion model to a target domain using only imperfect pseudo-labels. Once trained, it generates high-fidelity, target-aligned images from semantic maps of any synthetic dataset, including low-effort sources created in hours rather than months. The method filters suboptimal generations, rectifies image-label misalignments, and standardises semantics across datasets, transforming weak synthetic data into competitive real-domain training sets. Experiments on five synthetic datasets and two real target datasets show segmentation gains of up to +8.0%pt. mIoU over state-of-the-art translation methods, making rapidly constructed synthetic datasets as effective as high-effort, time-intensive synthetic datasets requiring extensive manual design. This work highlights a valuable collaborative paradigm where fast semantic prototyping, combined with generative models, enables scalable, high-quality training data creation for urban scene understanding.</li>
<li><strong>摘要：</strong>合成数据集广泛用于训练城市场景识别模型，但即使是高度逼真的渲染也与真实图像存在明显差距。当适应特定目标领域（例如城市景观）时，这种差距尤其明显，其中建筑、植被、物体外观和相机特征的差异限制了下游性能。通过更详细的 3D 建模来缩小这一差距将需要昂贵的资产和场景设计，从而违背了低成本标记数据的目的。为了解决这个问题，我们提出了一个新的框架，该框架仅使用不完美的伪标签将现成的扩散模型适应目标域。经过训练后，它可以根据任何合成数据集的语义图生成高保真、目标对齐的图像，包括在数小时而不是数月内创建的省力源。该方法过滤次优生成，纠正图像标签错位，并标准化数据集的语义，将弱合成数据转换为有竞争力的实域训练集。对五个合成数据集和两个真实目标数据集的实验显示分割增益高达 +8.0%pt。 mIoU 优于最先进的翻译方法，使快速构建的合成数据集与需要大量手动设计的费力、耗时的合成数据集一样有效。这项工作突出了一种有价值的协作范例，其中快速语义原型与生成模型相结合，可以为城市场景理解创建可扩展的高质量训练数据。</li>
</ul>

<h3>Title: ExpVid: A Benchmark for Experiment Video Understanding & Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yicheng Xu, Yue Wu, Jiashuo Yu, Ziang Yan, Tianxiang Jiang, Yinan He, Qingsong Zhao, Kai Chen, Yu Qiao, Limin Wang, Manabu Okumura, Yi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11606">https://arxiv.org/abs/2510.11606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11606">https://arxiv.org/pdf/2510.11606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11606]] ExpVid: A Benchmark for Experiment Video Understanding & Reasoning(https://arxiv.org/abs/2510.11606)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) hold promise for accelerating scientific discovery by interpreting complex experimental procedures. However, their true capabilities are poorly understood, as existing benchmarks neglect the fine-grained and long-horizon nature of authentic laboratory work, especially in wet-lab settings. To bridge this gap, we introduce ExpVid, the first benchmark designed to systematically evaluate MLLMs on scientific experiment videos. Curated from peer-reviewed video publications, ExpVid features a new three-level task hierarchy that mirrors the scientific process: (1) Fine-grained Perception of tools, materials, and actions; (2) Procedural Understanding of step order and completeness; and (3) Scientific Reasoning that connects the full experiment to its published conclusions. Our vision-centric annotation pipeline, combining automated generation with multi-disciplinary expert validation, ensures that tasks require visual grounding. We evaluate 19 leading MLLMs on ExpVid and find that while they excel at coarse-grained recognition, they struggle with disambiguating fine details, tracking state changes over time, and linking experimental procedures to scientific outcomes. Our results reveal a notable performance gap between proprietary and open-source models, particularly in high-order reasoning. ExpVid not only provides a diagnostic tool but also charts a roadmap for developing MLLMs capable of becoming trustworthy partners in scientific experimentation.</li>
<li><strong>摘要：</strong>多模态大语言模型 (MLLM) 有望通过解释复杂的实验过程来加速科学发现。然而，人们对它们的真正能力知之甚少，因为现有的基准忽略了真实实验室工作的细粒度和长期性，尤其是在湿实验室环境中。为了弥补这一差距，我们引入了 ExpVid，这是第一个旨在系统评估科学实验视频上的 MLLM 的基准。 ExpVid 根据同行评审的视频出版物进行策划，具有反映科学过程的新的三级任务层次结构：（1）对工具、材料和行动的细粒度感知； (2) 对步骤顺序和完整性的程序性理解； (3) 将完整的实验与其已发表的结论联系起来的科学推理。我们以视觉为中心的注释管道将自动生成与多学科专家验证相结合，确保任务需要视觉基础。我们在 ExpVid 上评估了 19 个领先的 MLLM，发现虽然它们擅长粗粒度识别，但它们在消除细节歧义、跟踪状态随时间的变化以及将实验程序与科学结果联系起来方面遇到困难。我们的结果揭示了专有模型和开源模型之间存在显着的性能差距，特别是在高阶推理方面。 ExpVid 不仅提供了诊断工具，还绘制了开发 MLLM 的路线图，使其能够成为科学实验中值得信赖的合作伙伴。</li>
</ul>

<h3>Title: EvoCAD: Evolutionary CAD Code Generation with Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tobias Preintner, Weixuan Yuan, Adrian König, Thomas Bäck, Elena Raponi, Niki van Stein</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11631">https://arxiv.org/abs/2510.11631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11631">https://arxiv.org/pdf/2510.11631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11631]] EvoCAD: Evolutionary CAD Code Generation with Vision Language Models(https://arxiv.org/abs/2510.11631)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Combining large language models with evolutionary computation algorithms represents a promising research direction leveraging the remarkable generative and in-context learning capabilities of LLMs with the strengths of evolutionary algorithms. In this work, we present EvoCAD, a method for generating computer-aided design (CAD) objects through their symbolic representations using vision language models and evolutionary optimization. Our method samples multiple CAD objects, which are then optimized using an evolutionary approach with vision language and reasoning language models. We assess our method using GPT-4V and GPT-4o, evaluating it on the CADPrompt benchmark dataset and comparing it to prior methods. Additionally, we introduce two new metrics based on topological properties defined by the Euler characteristic, which capture a form of semantic similarity between 3D objects. Our results demonstrate that EvoCAD outperforms previous approaches on multiple metrics, particularly in generating topologically correct objects, which can be efficiently evaluated using our two novel metrics that complement existing spatial metrics.</li>
<li><strong>摘要：</strong>将大型语言模型与进化计算算法相结合代表了一个有前途的研究方向，利用了法学硕士卓越的生成和上下文学习能力以及进化算法的优势。在这项工作中，我们提出了 EvoCAD，一种使用视觉语言模型和进化优化通过符号表示生成计算机辅助设计 (CAD) 对象的方法。我们的方法对多个 CAD 对象进行采样，然后使用视觉语言和推理语言模型的进化方法对其进行优化。我们使用 GPT-4V 和 GPT-4o 评估我们的方法，在 CADPrompt 基准数据集上对其进行评估，并将其与之前的方法进行比较。此外，我们引入了两个基于欧拉特征定义的拓扑属性的新度量，它们捕获 3D 对象之间语义相似性的一种形式。我们的结果表明，EvoCAD 在多个指标上优于以前的方法，特别是在生成拓扑正确的对象方面，可以使用我们补充现有空间指标的两个新颖指标来有效评估这些对象。</li>
</ul>

<h3>Title: InfiniHuman: Infinite 3D Human Creation with Precise Control</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, Gerard Pons-Moll</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11650">https://arxiv.org/abs/2510.11650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11650">https://arxiv.org/pdf/2510.11650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11650]] InfiniHuman: Infinite 3D Human Creation with Precise Control(https://arxiv.org/abs/2510.11650)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generating realistic and controllable 3D human avatars is a long-standing challenge, particularly when covering broad attribute ranges such as ethnicity, age, clothing styles, and detailed body shapes. Capturing and annotating large-scale human datasets for training generative models is prohibitively expensive and limited in scale and diversity. The central question we address in this paper is: Can existing foundation models be distilled to generate theoretically unbounded, richly annotated 3D human data? We introduce InfiniHuman, a framework that synergistically distills these models to produce richly annotated human data at minimal cost and with theoretically unlimited scalability. We propose InfiniHumanData, a fully automatic pipeline that leverages vision-language and image generation models to create a large-scale multi-modal dataset. User study shows our automatically generated identities are undistinguishable from scan renderings. InfiniHumanData contains 111K identities spanning unprecedented diversity. Each identity is annotated with multi-granularity text descriptions, multi-view RGB images, detailed clothing images, and SMPL body-shape parameters. Building on this dataset, we propose InfiniHumanGen, a diffusion-based generative pipeline conditioned on text, body shape, and clothing assets. InfiniHumanGen enables fast, realistic, and precisely controllable avatar generation. Extensive experiments demonstrate significant improvements over state-of-the-art methods in visual quality, generation speed, and controllability. Our approach enables high-quality avatar generation with fine-grained control at effectively unbounded scale through a practical and affordable solution. We will publicly release the automatic data generation pipeline, the comprehensive InfiniHumanData dataset, and the InfiniHumanGen models at this https URL.</li>
<li><strong>摘要：</strong>生成逼真且可控的 3D 人体头像是一项长期存在的挑战，特别是在涵盖种族、年龄、服装风格和详细体型等广泛属性范围时。捕获和注释用于训练生成模型的大规模人类数据集非常昂贵，并且规模和多样性有限。我们在本文中解决的核心问题是：能否提炼现有的基础模型来生成理论上无限的、注释丰富的 3D 人体数据？我们引入了 InfiniHuman，这是一个框架，可以协同提炼这些模型，以最低的成本生成丰富注释的人类数据，并且理论上具有无限的可扩展性。我们提出了 InfiniHumanData，这是一个全自动管道，利用视觉语言和图像生成模型来创建大规模多模式数据集。用户研究表明，我们自动生成的身份与扫描渲染无法区分。 InfiniHumanData 包含 111K 个身份，涵盖前所未有的多样性。每个身份都用多粒度文本描述、多视图 RGB 图像、详细的服装图像和 SMPL 体形参数进行注释。在此数据集的基础上，我们提出了 InfiniHumanGen，一种基于扩散的生成管道，以文本、体形和服装资产为条件。 InfiniHumanGen 可实现快速、逼真且精确控制的化身生成。大量实验表明，在视觉质量、生成速度和可控性方面比最先进的方法有显着改进。我们的方法通过实用且经济实惠的解决方案，能够以有效无限的规模生成高质量的化身，并进行细粒度控制。我们将在此 https URL 公开发布自动数据生成管道、综合 InfiniHumanData 数据集和 InfiniHumanGen 模型。</li>
</ul>

<h3>Title: An Eulerian Perspective on Straight-Line Sampling</h3>
<ul>
<li><strong>Authors: </strong>Panos Tsimpos, Youssef Marzouk</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11657">https://arxiv.org/abs/2510.11657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11657">https://arxiv.org/pdf/2510.11657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11657]] An Eulerian Perspective on Straight-Line Sampling(https://arxiv.org/abs/2510.11657)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We study dynamic measure transport for generative modeling: specifically, flows induced by stochastic processes that bridge a specified source and target distribution. The conditional expectation of the process' velocity defines an ODE whose flow map achieves the desired transport. We ask \emph{which processes produce straight-line flows} -- i.e., flows whose pointwise acceleration vanishes and thus are exactly integrable with a first-order method? We provide a concise PDE characterization of straightness as a balance between conditional acceleration and the divergence of a weighted covariance (Reynolds) tensor. Using this lens, we fully characterize affine-in-time interpolants and show that straightness occurs exactly under deterministic endpoint couplings. We also derive necessary conditions that constrain flow geometry for general processes, offering broad guidance for designing transports that are easier to integrate.</li>
<li><strong>摘要：</strong>我们研究生成建模的动态测量传输：具体来说，是由桥接指定源和目标分布的随机过程引起的流。过程速度的条件期望定义了一个 ODE，其流量图实现了所需的传输。我们问\emph{哪些过程产生直线流}——即点向加速度消失并因此可以用一阶方法精确积分的流？我们提供了直线度的简明偏微分方程表征，作为条件加速度和加权协方差（雷诺）张量的散度之间的平衡。使用这个透镜，我们充分描述了时间仿射插值器的特征，并表明直线度恰好发生在确定性端点耦合下。我们还得出了限制一般流程的流动几何形状的必要条件，为设计更易于集成的传输提供了广泛的指导。</li>
</ul>

<h3>Title: Chronologically Consistent Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Songrun He, Linying Lv, Asaf Manela, Jimmy Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, q-fin.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11677">https://arxiv.org/abs/2510.11677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11677">https://arxiv.org/pdf/2510.11677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11677]] Chronologically Consistent Generative AI(https://arxiv.org/abs/2510.11677)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce a family of chronologically consistent, instruction-following large language models to eliminate lookahead bias. Each model is trained only on data available before a clearly defined knowledge-cutoff date, ensuring strict temporal separation from any post-cutoff data. The resulting framework offers (i) a simple, conversational chat interface, (ii) fully open, fixed model weights that guarantee replicability, and (iii) a conservative lower bound on forecast accuracy, isolating the share of predictability that survives once training leakage is removed. Together, these features provide researchers with an easy-to-use generative AI tool useful for a wide range of prediction tasks that is free of lookahead bias.</li>
<li><strong>摘要：</strong>我们引入了一系列按时间顺序一致、遵循指令的大型语言模型来消除先行偏差。每个模型仅根据明确定义的知识截止日期之前可用的数据进行训练，确保与任何截止后数据严格时间分离。由此产生的框架提供了（i）一个简单的对话式聊天界面，（ii）完全开放的、保证可复制性的固定模型权重，以及（iii）预测准确性的保守下限，隔离了消除训练泄漏后仍然存在的可预测性份额。这些功能共同为研究人员提供了一种易于使用的生成人工智能工具，可用于广泛的预测任务，且不存在前瞻偏差。</li>
</ul>

<h3>Title: Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Nianyi Lin, Jiajie Zhang, Lei Hou, Juanzi Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11683">https://arxiv.org/abs/2510.11683</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11683">https://arxiv.org/pdf/2510.11683</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11683]] Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models(https://arxiv.org/abs/2510.11683)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>A key challenge in applying reinforcement learning (RL) to diffusion large language models (dLLMs) lies in the intractability of their likelihood functions, which are essential for the RL objective, necessitating corresponding approximation in each training step. While existing methods approximate the log-likelihoods by their evidence lower bounds (ELBOs) via customized Monte Carlo (MC) sampling, the forward computational graphs of all MC samples need to be retained for the gradient computation of non-linear terms in the RL objective, resulting in significant memory overhead. This constraint restricts feasible sample sizes, leading to imprecise likelihood approximations and ultimately distorting the RL objective. To overcome this limitation, we propose \emph{Boundary-Guided Policy Optimization} (BGPO), a memory-efficient RL algorithm that maximizes a specially constructed lower bound of the ELBO-based objective. This lower bound is carefully designed to satisfy two key properties: (1) Linearity: it is formulated in a linear sum where each term depends only on a single MC sample, thereby enabling gradient accumulation across samples and ensuring constant memory usage; (2) Equivalence: Both the value and gradient of this lower bound are equal to those of the ELBO-based objective in on-policy training, making it also an effective approximation for the original RL objective. These properties allow BGPO to adopt a large MC sample size, resulting in more accurate likelihood approximations and improved RL objective estimation, which in turn leads to enhanced performance. Experiments show that BGPO significantly outperforms previous RL algorithms for dLLMs in math problem solving, code generation, and planning tasks.</li>
<li><strong>摘要：</strong>将强化学习 (RL) 应用于扩散大语言模型 (dLLM) 的一个关键挑战在于其似然函数的棘手性，这对于 RL 目标至关重要，需要在每个训练步骤中进行相应的近似。虽然现有方法通过定制的蒙特卡罗 (MC) 采样通过证据下界 (ELBO) 来近似对数似然，但需要保留所有 MC 样本的前向计算图以用于 RL 目标中非线性项的梯度计算，从而导致显着的内存开销。这种约束限制了可行的样本量，导致似然近似不精确，并最终扭曲 RL 目标。为了克服这一限制，我们提出了 \emph{边界引导策略优化} (BGPO)，这是一种内存高效的 RL 算法，可最大化基于 ELBO 的目标的专门构造的下界。这个下界经过精心设计，以满足两个关键属性：（1）线性：它以线性和的形式表示，其中每一项仅依赖于单个MC样本，从而实现跨样本的梯度累积并确保恒定的内存使用； (2) 等价性：该下界的值和梯度都等于在策略训练中基于 ELBO 的目标的值和梯度，这使得它也是原始 RL 目标的有效近似。这些特性使 BGPO 能够采用较大的 MC 样本量，从而获得更准确的似然近似并改进 RL 目标估计，从而提高性能。实验表明，BGPO 在数学问题解决、代码生成和规划任务方面明显优于以前的 dLLM 强化学习算法。</li>
</ul>

<h3>Title: Diffusion Transformers with Representation Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Boyang Zheng, Nanye Ma, Shengbang Tong, Saining Xie</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11690">https://arxiv.org/abs/2510.11690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11690">https://arxiv.org/pdf/2510.11690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11690]] Diffusion Transformers with Representation Autoencoders(https://arxiv.org/abs/2510.11690)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Latent generative modeling, where a pretrained autoencoder maps pixels into a latent space for the diffusion process, has become the standard strategy for Diffusion Transformers (DiT); however, the autoencoder component has barely evolved. Most DiTs continue to rely on the original VAE encoder, which introduces several limitations: outdated backbones that compromise architectural simplicity, low-dimensional latent spaces that restrict information capacity, and weak representations that result from purely reconstruction-based training and ultimately limit generative quality. In this work, we explore replacing the VAE with pretrained representation encoders (e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term Representation Autoencoders (RAEs). These models provide both high-quality reconstructions and semantically rich latent spaces, while allowing for a scalable transformer-based architecture. Since these latent spaces are typically high-dimensional, a key challenge is enabling diffusion transformers to operate effectively within them. We analyze the sources of this difficulty, propose theoretically motivated solutions, and validate them empirically. Our approach achieves faster convergence without auxiliary representation alignment losses. Using a DiT variant equipped with a lightweight, wide DDT head, we achieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no guidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers clear advantages and should be the new default for diffusion transformer training.</li>
<li><strong>摘要：</strong>潜在生成建模，其中预训练的自动编码器将像素映射到扩散过程的潜在空间中，已成为扩散变压器（DiT）的标准策略；然而，自动编码器组件几乎没有发展。大多数 DiT 继续依赖原始的 VAE 编码器，这引入了一些限制：过时的主干会损害架构的简单性、限制信息容量的低维潜在空间以及纯粹基于重建的训练产生的弱表示并最终限制生成质量。在这项工作中，我们探索用预训练的表示编码器（例如 DINO、SigLIP、MAE）与经过训练的解码器配对来取代 VAE，形成我们所说的表示自动编码器（RAE）。这些模型提供高质量的重建和语义丰富的潜在空间，同时允许可扩展的基于变压器的架构。由于这些潜在空间通常是高维的，因此一个关键的挑战是使扩散变压器能够在其中有效运行。我们分析了这一困难的根源，提出了理论上的解决方案，并通过经验进行了验证。我们的方法实现了更快的收敛，而没有辅助表示对齐损失。使用配备轻量、宽 DDT 头的 DiT 变体，我们在 ImageNet 上实现了强大的图像生成结果：256x256（无引导）下的 FID 为 1.51，256x256 和 512x512（有引导）下的 FID 为 1.13。 RAE 具有明显的优势，应该成为扩散变压器训练的新默认设置。</li>
</ul>

<h3>Title: DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training</h3>
<ul>
<li><strong>Authors: </strong>Haoran Feng, Dizhe Zhang, Xiangtai Li, Bo Du, Lu Qi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11712">https://arxiv.org/abs/2510.11712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11712">https://arxiv.org/pdf/2510.11712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11712]] DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training(https://arxiv.org/abs/2510.11712)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In this work, we propose DiT360, a DiT-based framework that performs hybrid training on perspective and panoramic data for panoramic image generation. For the issues of maintaining geometric fidelity and photorealism in generation quality, we attribute the main reason to the lack of large-scale, high-quality, real-world panoramic data, where such a data-centric view differs from prior methods that focus on model design. Basically, DiT360 has several key modules for inter-domain transformation and intra-domain augmentation, applied at both the pre-VAE image level and the post-VAE token level. At the image level, we incorporate cross-domain knowledge through perspective image guidance and panoramic refinement, which enhance perceptual quality while regularizing diversity and photorealism. At the token level, hybrid supervision is applied across multiple modules, which include circular padding for boundary continuity, yaw loss for rotational robustness, and cube loss for distortion awareness. Extensive experiments on text-to-panorama, inpainting, and outpainting tasks demonstrate that our method achieves better boundary consistency and image fidelity across eleven quantitative metrics. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>在这项工作中，我们提出了 DiT360，这是一种基于 DiT 的框架，可对透视数据和全景数据进行混合训练以生成全景图像。对于生成质量中保持几何保真度和真实感的问题，我们将主要原因归结为缺乏大规模、高质量、真实世界的全景数据，这种以数据为中心的视图与之前专注于模型设计的方法不同。基本上，DiT360 有几个用于域间转换和域内增强的关键模块，应用于 VAE 前图像级别和 VAE 后令牌级别。在图像层面，我们通过透视图像引导和全景细化融合跨领域知识，在规范多样性和真实感的同时增强感知质量。在令牌级别，混合监督应用于多个模块，其中包括用于边界连续性的圆形填充、用于旋转鲁棒性的偏航损失以及用于失真感知的立方体损失。对文本到全景、修复和修复任务的大量实验表明，我们的方法在十一个定量指标上实现了更好的边界一致性和图像保真度。我们的代码可以在这个 https URL 上找到。</li>
</ul>

<h3>Title: Point Prompting: Counterfactual Tracking with Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Ayush Shrivastava, Sanyam Mehta, Daniel Geng, Andrew Owens</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11715">https://arxiv.org/abs/2510.11715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11715">https://arxiv.org/pdf/2510.11715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11715]] Point Prompting: Counterfactual Tracking with Video Diffusion Models(https://arxiv.org/abs/2510.11715)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Trackers and video generators solve closely related problems: the former analyze motion, while the latter synthesize it. We show that this connection enables pretrained video diffusion models to perform zero-shot point tracking by simply prompting them to visually mark points as they move over time. We place a distinctively colored marker at the query point, then regenerate the rest of the video from an intermediate noise level. This propagates the marker across frames, tracing the point's trajectory. To ensure that the marker remains visible in this counterfactual generation, despite such markers being unlikely in natural videos, we use the unedited initial frame as a negative prompt. Through experiments with multiple image-conditioned video diffusion models, we find that these "emergent" tracks outperform those of prior zero-shot methods and persist through occlusions, often obtaining performance that is competitive with specialized self-supervised models.</li>
<li><strong>摘要：</strong>跟踪器和视频生成器解决密切相关的问题：前者分析运动，而后者合成运动。我们表明，这种连接使预训练的视频扩散模型能够通过简单地提示它们在随时间移动时对点进行视觉标记来执行零射击点跟踪。我们在查询点放置一个独特的彩色标记，然后从中间噪声级别重新生成视频的其余部分。这会跨帧传播标记，跟踪点的轨迹。为了确保标记在这个反事实生成中保持可见，尽管此类标记在自然视频中不太可能出现，我们使用未经编辑的初始帧作为负面提示。通过对多个图像条件视频扩散模型的实验，我们发现这些“新兴”轨迹优于先前的零样本方法，并且在遮挡中持续存在，通常获得与专门的自监督模型相媲美的性能。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
