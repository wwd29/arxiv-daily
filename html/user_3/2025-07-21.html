<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-07-21</h1>
<h3>Title: Low-Light Enhancement via Encoder-Decoder Network with Illumination Guidance</h3>
<ul>
<li><strong>Authors: </strong>Le-Anh Tran, Chung Nguyen Tran, Ngoc-Luu Nguyen, Nhan Cach Dang, Jordi Carrabina, David Castells-Rufas, Minh Son Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13360">https://arxiv.org/abs/2507.13360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13360">https://arxiv.org/pdf/2507.13360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13360]] Low-Light Enhancement via Encoder-Decoder Network with Illumination Guidance(https://arxiv.org/abs/2507.13360)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel deep learning framework for low-light image enhancement, named the Encoder-Decoder Network with Illumination Guidance (EDNIG). Building upon the U-Net architecture, EDNIG integrates an illumination map, derived from Bright Channel Prior (BCP), as a guidance input. This illumination guidance helps the network focus on underexposed regions, effectively steering the enhancement process. To further improve the model's representational power, a Spatial Pyramid Pooling (SPP) module is incorporated to extract multi-scale contextual features, enabling better handling of diverse lighting conditions. Additionally, the Swish activation function is employed to ensure smoother gradient propagation during training. EDNIG is optimized within a Generative Adversarial Network (GAN) framework using a composite loss function that combines adversarial loss, pixel-wise mean squared error (MSE), and perceptual loss. Experimental results show that EDNIG achieves competitive performance compared to state-of-the-art methods in quantitative metrics and visual quality, while maintaining lower model complexity, demonstrating its suitability for real-world applications. The source code for this work is available at this https URL.</li>
<li><strong>摘要：</strong>本文介绍了一个新颖的深度学习框架，用于低光图像增强功能，该框架将带有照明指导（EDNIG）的编码器折磨网络（EDNIG）。 Ednig在U-NET体系结构的基础上集成了一个来自Bright Channel Prir（BCP）的照明图作为指导输入。该照明指南有助于网络专注于未充满刺激的区域，从而有效地转向增强过程。为了进一步提高模型的代表力，合并了空间金字塔池（SPP）模块，以提取多尺度的上下文特征，从而更好地处理各种照明条件。此外，使用Swish激活功能来确保训练过程中的梯度传播。 Ednig使用复合损失函数在生成对抗网络（GAN）框架中进行了优化，该损失函数结合了对抗损耗，像素均方根误差（MSE）和感知损失。实验结果表明，与定量指标和视觉质量的最新方法相比，Ednig在保持较低的模型复杂性的同时，取得了竞争性能，证明了其适用于现实世界应用。此工作的源代码可在此HTTPS URL上获得。</li>
</ul>

<h3>Title: Minimalist Concept Erasure in Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Yang Zhang, Er Jin, Yanfei Dong, Yixuan Wu, Philip Torr, Ashkan Khakzar, Johannes Stegmaier, Kenji Kawaguchi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13386">https://arxiv.org/abs/2507.13386</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13386">https://arxiv.org/pdf/2507.13386</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13386]] Minimalist Concept Erasure in Generative Models(https://arxiv.org/abs/2507.13386)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative models have demonstrated remarkable capabilities in producing high-quality images, but their reliance on large-scale unlabeled data has raised significant safety and copyright concerns. Efforts to address these issues by erasing unwanted concepts have shown promise. However, many existing erasure methods involve excessive modifications that compromise the overall utility of the model. In this work, we address these issues by formulating a novel minimalist concept erasure objective based \emph{only} on the distributional distance of final generation outputs. Building on our formulation, we derive a tractable loss for differentiable optimization that leverages backpropagation through all generation steps in an end-to-end manner. We also conduct extensive analysis to show theoretical connections with other models and methods. To improve the robustness of the erasure, we incorporate neuron masking as an alternative to model fine-tuning. Empirical evaluations on state-of-the-art flow-matching models demonstrate that our method robustly erases concepts without degrading overall model performance, paving the way for safer and more responsible generative models.</li>
<li><strong>摘要：</strong>生成模型的最新进展表明，在产生高质量的图像方面具有显着的功能，但是它们对大规模无标记数据的依赖引起了重大安全性和版权的关注。通过删除不需要的概念来解决这些问题的努力已显示出希望。但是，许多现有的擦除方法涉及过多的修改，以损害该模型的整体效用。在这项工作中，我们通过制定基于最终一代输出的分布距离的新型简约概念擦除目标来解决这些问题。在我们的公式的基础上，我们获得了可拖动的损失，以通过端到端的方式通过所有生成步骤来利用反向传播。我们还进行了广泛的分析，以显示与其他模型和方法的理论联系。为了提高擦除的鲁棒性，我们将神经元掩盖融合为模型微调的替代方案。对最先进的流量匹配模型的经验评估表明，我们的方法可以稳健地擦除概念，而不会降低整体模型性能，为更安全，更负责任的生成模型铺平了道路。</li>
</ul>

<h3>Title: MADI: Masking-Augmented Diffusion with Inference-Time Scaling for Visual Editing</h3>
<ul>
<li><strong>Authors: </strong>Shreya Kadambi, Risheek Garrepalli, Shubhankar Borse, Munawar Hyatt, Fatih Porikli</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13401">https://arxiv.org/abs/2507.13401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13401">https://arxiv.org/pdf/2507.13401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13401]] MADI: Masking-Augmented Diffusion with Inference-Time Scaling for Visual Editing(https://arxiv.org/abs/2507.13401)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Despite the remarkable success of diffusion models in text-to-image generation, their effectiveness in grounded visual editing and compositional control remains challenging. Motivated by advances in self-supervised learning and in-context generative modeling, we propose a series of simple yet powerful design choices that significantly enhance diffusion model capacity for structured, controllable generation and editing. We introduce Masking-Augmented Diffusion with Inference-Time Scaling (MADI), a framework that improves the editability, compositionality and controllability of diffusion models through two core innovations. First, we introduce Masking-Augmented gaussian Diffusion (MAgD), a novel training strategy with dual corruption process which combines standard denoising score matching and masked reconstruction by masking noisy input from forward process. MAgD encourages the model to learn discriminative and compositional visual representations, thus enabling localized and structure-aware editing. Second, we introduce an inference-time capacity scaling mechanism based on Pause Tokens, which act as special placeholders inserted into the prompt for increasing computational capacity at inference time. Our findings show that adopting expressive and dense prompts during training further enhances performance, particularly for MAgD. Together, these contributions in MADI substantially enhance the editability of diffusion models, paving the way toward their integration into more general-purpose, in-context generative diffusion architectures.</li>
<li><strong>摘要：</strong>尽管扩散模型在文本到图像生成中取得了显着的成功，但它们在接地的视觉编辑和组成控制中的有效性仍然具有挑战性。我们提出了一系列简单而强大的设计选择，这是由自我监管学习和内在生成建模的进步的动机，可以显着增强结构化，可控制的生成和编辑的扩散模型的能力。我们通过推理时间缩放（MADI）介绍了蒙版的扩散，该框架通过两种核心创新来提高扩散模型的编辑性，组成性和可控性。首先，我们介绍了带有双重损坏过程的新型训练策略，介绍了蒙版增强的高斯扩散（MAGD），结合了标准的DeNosing分数匹配和通过掩盖正向过程的噪声输入来结合掩盖的重建。 MAGD鼓励该模型学习歧视性和组成的视觉表示，从而实现局部和结构感知的编辑。其次，我们基于暂停代币引入了推理时间能力缩放机制，该机制充当特殊占位符，插入推理时间在推理时提高计算能力的提示。我们的发现表明，在训练中采用表现力和密集的提示进一步提高了性能，尤其是对于MAGD而言。共同，MADI中的这些贡献大大提高了扩散模型的编辑性，为它们集成到更通用的，可感状的生成扩散体系结构铺平了道路。</li>
</ul>

<h3>Title: AortaDiff: Volume-Guided Conditional Diffusion Models for Multi-Branch Aortic Surface Generation</h3>
<ul>
<li><strong>Authors: </strong>Delin An, Pan Du, Jian-Xun Wang, Chaoli Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13404">https://arxiv.org/abs/2507.13404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13404">https://arxiv.org/pdf/2507.13404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13404]] AortaDiff: Volume-Guided Conditional Diffusion Models for Multi-Branch Aortic Surface Generation(https://arxiv.org/abs/2507.13404)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Accurate 3D aortic construction is crucial for clinical diagnosis, preoperative planning, and computational fluid dynamics (CFD) simulations, as it enables the estimation of critical hemodynamic parameters such as blood flow velocity, pressure distribution, and wall shear stress. Existing construction methods often rely on large annotated training datasets and extensive manual intervention. While the resulting meshes can serve for visualization purposes, they struggle to produce geometrically consistent, well-constructed surfaces suitable for downstream CFD analysis. To address these challenges, we introduce AortaDiff, a diffusion-based framework that generates smooth aortic surfaces directly from CT/MRI volumes. AortaDiff first employs a volume-guided conditional diffusion model (CDM) to iteratively generate aortic centerlines conditioned on volumetric medical images. Each centerline point is then automatically used as a prompt to extract the corresponding vessel contour, ensuring accurate boundary delineation. Finally, the extracted contours are fitted into a smooth 3D surface, yielding a continuous, CFD-compatible mesh representation. AortaDiff offers distinct advantages over existing methods, including an end-to-end workflow, minimal dependency on large labeled datasets, and the ability to generate CFD-compatible aorta meshes with high geometric fidelity. Experimental results demonstrate that AortaDiff performs effectively even with limited training data, successfully constructing both normal and pathologically altered aorta meshes, including cases with aneurysms or coarctation. This capability enables the generation of high-quality visualizations and positions AortaDiff as a practical solution for cardiovascular research.</li>
<li><strong>摘要：</strong>准确的3D主动脉构建对于临床诊断，术前计划和计算流体动力学（CFD）模拟至关重要，因为它可以估算关键血液动力学参数，例如血流速度，压力分布和壁切应力。现有的施工方法通常依赖大型注释的培训数据集和大量的手动干预。尽管所得的网格可以用于可视化目的，但它们难以生成几何一致，结构良好的表面，适合下游CFD分析。为了应对这些挑战，我们引入了Aortadiff，这是一种基于扩散的框架，可直接从CT/MRI量产生光滑的主动脉表面。 Aortadiff首先采用体积引导的条件扩散模型（CDM）来迭代产生以体积医学图像为条件的主动脉中心。然后，每个中心线点被自动用作提示提取相应的容器轮廓的提示，从而确保准确的边界描绘。最后，将提取的轮廓安装到光滑的3D表面中，从而产生连续的CFD兼容网格表示。 Aortadiff与现有方法具有不同的优势，包括端到端的工作流程，对大型标记数据集的最小依赖性以及生成具有较高几何标志性的CFD兼容主动脉网格的能力。实验结果表明，即使在有限的训练数据中，主动脉的性能也有效地执行，成功地构建了正常和病理改变的主动脉网络，包括动脉瘤或骨质的病例。该能力使高质量可视化的产生和位置主动脉症作为心血管研究的实用解决方案。</li>
</ul>

<h3>Title: IConMark: Robust Interpretable Concept-Based Watermark For AI Images</h3>
<ul>
<li><strong>Authors: </strong>Vinu Sankar Sadasivan, Mehrdad Saberi, Soheil Feizi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13407">https://arxiv.org/abs/2507.13407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13407">https://arxiv.org/pdf/2507.13407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13407]] IConMark: Robust Interpretable Concept-Based Watermark For AI Images(https://arxiv.org/abs/2507.13407)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>With the rapid rise of generative AI and synthetic media, distinguishing AI-generated images from real ones has become crucial in safeguarding against misinformation and ensuring digital authenticity. Traditional watermarking techniques have shown vulnerabilities to adversarial attacks, undermining their effectiveness in the presence of attackers. We propose IConMark, a novel in-generation robust semantic watermarking method that embeds interpretable concepts into AI-generated images, as a first step toward interpretable watermarking. Unlike traditional methods, which rely on adding noise or perturbations to AI-generated images, IConMark incorporates meaningful semantic attributes, making it interpretable to humans and hence, resilient to adversarial manipulation. This method is not only robust against various image augmentations but also human-readable, enabling manual verification of watermarks. We demonstrate a detailed evaluation of IConMark's effectiveness, demonstrating its superiority in terms of detection accuracy and maintaining image quality. Moreover, IConMark can be combined with existing watermarking techniques to further enhance and complement its robustness. We introduce IConMark+SS and IConMark+TM, hybrid approaches combining IConMark with StegaStamp and TrustMark, respectively, to further bolster robustness against multiple types of image manipulations. Our base watermarking technique (IConMark) and its variants (+TM and +SS) achieve 10.8%, 14.5%, and 15.9% higher mean area under the receiver operating characteristic curve (AUROC) scores for watermark detection, respectively, compared to the best baseline on various datasets.</li>
<li><strong>摘要：</strong>随着生成性AI和合成介质的快速增长，将AI生成的图像与真实图像区分开来对保护错误信息和确保数字真实性至关重要。传统的水印技术显示出对对抗攻击的脆弱性，在攻击者面前破坏了它们的有效性。我们提出了Iconmark，这是一种新颖的内部鲁棒语义水印方法，将可解释的概念嵌入AI生成的图像中，这是迈向可解释水印的第一步。与依靠在AI生成的图像中添加噪声或扰动的传统方法不同，Iconmark包含有意义的语义属性，使其可以解释为人类，因此可以弹性。这种方法不仅可以抵抗各种图像增强功能，而且对人类可读，从而可以手动验证水印。我们证明了对Iconmark的有效性的详细评估，证明了其在检测准确性和保持图像质量方面的优势。此外，Iconmark可以与现有的水印技术结合使用，以进一步增强和补充其稳健性。我们介绍了ICONMARK+SS和ICONMARK+TM，分别将Iconmark与Stegastamp和Trustmark结合使用的混合方法，以进一步支持多种类型的图像操作。我们的基础水印技术（ICONMARK）及其变体（ +TM和 +SS）在接收器操作特征曲线（AUROC）下的平均面积（AUROC）得分分别高出10.8％，14.5％和15.9％，分别与各种数据集的最佳基线相比，分别用于水印。</li>
</ul>

<h3>Title: LightAutoDS-Tab: Multi-AutoML Agentic System for Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Aleksey Lapin, Igor Hromov, Stanislav Chumakov, Mile Mitrovic, Dmitry Simakov, Nikolay O. Nikitin, Andrey V. Savchenko</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13413">https://arxiv.org/abs/2507.13413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13413">https://arxiv.org/pdf/2507.13413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13413]] LightAutoDS-Tab: Multi-AutoML Agentic System for Tabular Data(https://arxiv.org/abs/2507.13413)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>AutoML has advanced in handling complex tasks using the integration of LLMs, yet its efficiency remains limited by dependence on specific underlying tools. In this paper, we introduce LightAutoDS-Tab, a multi-AutoML agentic system for tasks with tabular data, which combines an LLM-based code generation with several AutoML tools. Our approach improves the flexibility and robustness of pipeline design, outperforming state-of-the-art open-source solutions on several data science tasks from Kaggle. The code of LightAutoDS-Tab is available in the open repository this https URL</li>
<li><strong>摘要：</strong>Automl使用LLM的集成来处理复杂的任务，但其效率仍然受到对特定基础工具的依赖的限制。在本文中，我们介绍了Lightautods-TAB，这是一种用于具有表格数据的任务的多AUTOML代理系统，该系统将基于LLM的代码生成与多个Automl工具相结合。我们的方法提高了管道设计的灵活性和鲁棒性，在Kaggle的几项数据科学任务上表现优于最先进的开源解决方案。 Lightautods-tab的代码可在此HTTPS URL的开放存储库中获得</li>
</ul>

<h3>Title: Gauge Flow Models</h3>
<ul>
<li><strong>Authors: </strong>Alexander Strunk, Roland Assam</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.DG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13414">https://arxiv.org/abs/2507.13414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13414">https://arxiv.org/pdf/2507.13414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13414]] Gauge Flow Models(https://arxiv.org/abs/2507.13414)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper introduces Gauge Flow Models, a novel class of Generative Flow Models. These models incorporate a learnable Gauge Field within the Flow Ordinary Differential Equation (ODE). A comprehensive mathematical framework for these models, detailing their construction and properties, is provided. Experiments using Flow Matching on Gaussian Mixture Models demonstrate that Gauge Flow Models yields significantly better performance than traditional Flow Models of comparable or even larger size. Additionally, unpublished research indicates a potential for enhanced performance across a broader range of generative tasks.</li>
<li><strong>摘要：</strong>本文介绍了轨距流模型，这是一种新型的生成流模型。这些模型在流量普通微分方程（ODE）中包含一个可学习的量规场。这些模型的全面数学框架提供了详细说明其结构和属性。在高斯混合物模型上使用流量匹配的实验表明，仪表流模型比传统的相当尺寸甚至更大的流程模型的性能明显好得多。此外，未发表的研究表明，在更广泛的生成任务中具有提高性能的潜力。</li>
</ul>

<h3>Title: "PhyWorldBench": A Comprehensive Evaluation of Physical Realism in Text-to-Video Models</h3>
<ul>
<li><strong>Authors: </strong>Jing Gu, Xian Liu, Yu Zeng, Ashwin Nagarajan, Fangrui Zhu, Daniel Hong, Yue Fan, Qianqi Yan, Kaiwen Zhou, Ming-Yu Liu, Xin Eric Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13428">https://arxiv.org/abs/2507.13428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13428">https://arxiv.org/pdf/2507.13428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13428]] "PhyWorldBench": A Comprehensive Evaluation of Physical Realism in Text-to-Video Models(https://arxiv.org/abs/2507.13428)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Video generation models have achieved remarkable progress in creating high-quality, photorealistic content. However, their ability to accurately simulate physical phenomena remains a critical and unresolved challenge. This paper presents PhyWorldBench, a comprehensive benchmark designed to evaluate video generation models based on their adherence to the laws of physics. The benchmark covers multiple levels of physical phenomena, ranging from fundamental principles like object motion and energy conservation to more complex scenarios involving rigid body interactions and human or animal motion. Additionally, we introduce a novel ""Anti-Physics"" category, where prompts intentionally violate real-world physics, enabling the assessment of whether models can follow such instructions while maintaining logical consistency. Besides large-scale human evaluation, we also design a simple yet effective method that could utilize current MLLM to evaluate the physics realism in a zero-shot fashion. We evaluate 12 state-of-the-art text-to-video generation models, including five open-source and five proprietary models, with a detailed comparison and analysis. we identify pivotal challenges models face in adhering to real-world physics. Through systematic testing of their outputs across 1,050 curated prompts-spanning fundamental, composite, and anti-physics scenarios-we identify pivotal challenges these models face in adhering to real-world physics. We then rigorously examine their performance on diverse physical phenomena with varying prompt types, deriving targeted recommendations for crafting prompts that enhance fidelity to physical principles.</li>
<li><strong>摘要：</strong>视频生成模型在创建高质量的影迷内容方面取得了显着的进步。但是，它们准确模拟身体现象的能力仍然是一个关键且尚未解决的挑战。本文介绍了Phyworldbench，这是一种综合基准，旨在根据其遵守物理定律评估视频生成模型。基准涵盖了多种级别的物理现象，从物体运动和能量保存等基本原理到涉及僵化的身体相互作用和人类或动物运动的更复杂的场景。此外，我们介绍了一种小说“抗物理学”类别，其中提示有意违反现实世界的物理学，从而可以评估模型是否可以遵循此类指令，同时保持逻辑一致性。除了大规模的人类评估外，我们还设计了一种简单而有效的方法，可以利用当前的MLLM以零拍的方式评估物理现实主义。我们通过详细的比较和分析评估了12种最先进的文本对视频生成模型，包括五个开源和五个专有模型。我们确定模型在遵守现实世界物理学方面面临的关键挑战。通过对1,050个策划的提示跨度基础，复合和反物理的系统进行系统测试，我们确定了这些模型在遵守现实世界物理学方面所面临的关键挑战。然后，我们严格地以不同的及时类型来检查它们在各种物理现象上的表现，从而提出了针对性的建议，以提示提示，从而提高了对身体原则的忠诚。</li>
</ul>

<h3>Title: $\nabla$NABLA: Neighborhood Adaptive Block-Level Attention</h3>
<ul>
<li><strong>Authors: </strong>Dmitrii Mikhailov, Aleksey Letunovskiy, Maria Kovaleva, Vladimir Arkhipkin, Vladimir Korviakov, Vladimir Polovnikov, Viacheslav Vasilev, Evelina Sidorova, Denis Dimitrov</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13546">https://arxiv.org/abs/2507.13546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13546">https://arxiv.org/pdf/2507.13546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13546]] $\nabla$NABLA: Neighborhood Adaptive Block-Level Attention(https://arxiv.org/abs/2507.13546)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent progress in transformer-based architectures has demonstrated remarkable success in video generation tasks. However, the quadratic complexity of full attention mechanisms remains a critical bottleneck, particularly for high-resolution and long-duration video sequences. In this paper, we propose NABLA, a novel Neighborhood Adaptive Block-Level Attention mechanism that dynamically adapts to sparsity patterns in video diffusion transformers (DiTs). By leveraging block-wise attention with adaptive sparsity-driven threshold, NABLA reduces computational overhead while preserving generative quality. Our method does not require custom low-level operator design and can be seamlessly integrated with PyTorch's Flex Attention operator. Experiments demonstrate that NABLA achieves up to 2.7x faster training and inference compared to baseline almost without compromising quantitative metrics (CLIP score, VBench score, human evaluation score) and visual quality drop. The code and model weights are available here: this https URL</li>
<li><strong>摘要：</strong>基于变压器的架构的最新进展表明，在视频生成任务中取得了巨大的成功。但是，全部注意机制的二次复杂性仍然是关键的瓶颈，尤其是对于高分辨率和长期视频序列。在本文中，我们提出了Nabla，Nabla是一种新型的邻域自适应块层的注意机制，该机制动态适应了视频扩散变压器（DITS）中的稀疏模式。通过使用自适应稀疏性驱动的阈值利用障碍物的关注，Nabla可以减少计算开销，同时保持生成质量。我们的方法不需要自定义的低级操作员设计，并且可以与Pytorch的Flex注意操作员无缝集成。实验表明，与基线相比，Nabla的训练和推理几乎没有损害定量指标（剪辑得分，VBENCH评分，人类评估得分）和视觉质量下降，而Nabla的训练和推理比基线的训练和推理的速度更快高达2.7倍。代码和型号的权重这里可用：此HTTPS URL</li>
</ul>

<h3>Title: Apple Intelligence Foundation Language Models: Tech Report 2025</h3>
<ul>
<li><strong>Authors: </strong>Hanzhi Zhou, Erik Hornberger, Pengsheng Guo, Xiyou Zhou, Saiwen Wang, Xin Wang, Yifei He, Xuankai Chang, Rene Rauch, Louis D'hauwe, John Peebles, Alec Doane, Kohen Chia, Jenna Thibodeau, Zi-Yi Dou, Yuanyang Zhang, Ruoming Pang, Reed Li, Zhifeng Chen, Jeremy Warner, Zhaoyang Xu, Sophy Lee, David Mizrahi, Ramsey Tantawi, Chris Chaney, Kelsey Peterson, Jun Qin, Alex Dombrowski, Mira Chiang, Aiswarya Raghavan, Gerard Casamayor, Qibin Chen, Aonan Zhang, Nathalie Tran, Jianyu Wang, Hang Su, Thomas Voice, Alessandro Pappalardo, Brycen Wershing, Prasanth Yadla, Rui Li, Priyal Chhatrapati, Ismael Fernandez, Yusuf Goren, Xin Zheng, Forrest Huang, Tao Lei, Eray Yildiz, Alper Kokmen, Gokul Santhanam, Areeba Kamal, Kaan Elgin, Dian Ang Yap, Jeremy Liu, Peter Gray, Howard Xing, Kieran Liu, Matteo Ronchi, Moritz Schwarzer-Becker, Yun Zhu, Mandana Saebi, Jeremy Snow, David Griffiths, Guillaume Tartavel, Erin Feldman, Simon Lehnerer, Fernando Bermúdez-Medina, Hans Han, Joe Zhou, Xiaoyi Ren, Sujeeth Reddy, Zirui Wang, Tom Gunter, Albert Antony, Yuanzhi Li, John Dennison, Tony Sun, Yena Han, Yi Qin, Sam Davarnia, Jeffrey Bigham, Wayne Shan, Hannah Gillis Coleman, Guillaume Klein, Peng Liu, Muyang Yu, Jack Cackler, Yuan Gao, Crystal Xiao, Binazir Karimzadeh, Zhengdong Zhang, Felix Bai, Albin Madappally Jose, Feng Nan, Nazir Kamaldin, Dong Yin, Hans Hao, Yanchao Sun, Yi Hua, Charles Maalouf</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13575">https://arxiv.org/abs/2507.13575</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13575">https://arxiv.org/pdf/2507.13575</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13575]] Apple Intelligence Foundation Language Models: Tech Report 2025(https://arxiv.org/abs/2507.13575)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce two multilingual, multimodal foundation language models that power Apple Intelligence features across Apple devices and services: i a 3B-parameter on-device model optimized for Apple silicon through architectural innovations such as KV-cache sharing and 2-bit quantization-aware training; and ii a scalable server model built on a novel Parallel-Track Mixture-of-Experts PT-MoE transformer that combines track parallelism, mixture-of-experts sparse computation, and interleaved global-local attention to deliver high quality with competitive cost on Apple's Private Cloud Compute platform. Both models are trained on large-scale multilingual and multimodal datasets sourced via responsible web crawling, licensed corpora, and high-quality synthetic data, then further refined with supervised fine-tuning and reinforcement learning on a new asynchronous platform. The resulting models support several additional languages while understanding images and executing tool calls. In public benchmarks and human evaluations, both the server model and the on-device model match or surpass comparably sized open baselines. A new Swift-centric Foundation Models framework exposes guided generation, constrained tool calling, and LoRA adapter fine-tuning, allowing developers to integrate these capabilities with a few lines of code. The latest advancements in Apple Intelligence models are grounded in our Responsible AI approach with safeguards like content filtering and locale-specific evaluation, as well as our commitment to protecting our users' privacy with innovations like Private Cloud Compute.</li>
<li><strong>摘要：</strong>我们介绍了两种多语言的多式联运基础语言模型，这些模型为Apple Intelligence提供了跨苹果设备和服务的功能：我是通过建筑创新（例如KV-CACHE共享和2位量化量化的培训）为Apple Silicon优化的3B参数的启动式车型模型； II以新型的并行轨道混合物为PT-MOE变压器建立的可扩展服务器模型，结合了轨道并行性，Experters稀疏计算的混合物以及相互交织的全球位置关注，以在Apple的私有云计算平台上提供高质量的质量，并具有竞争力的成本。这两种模型均经过大规模多语言和多模式数据集的培训，这些数据集是通过负责任的网络爬行，许可的语料库和高质量合成数据来源的，然后在新的异步平台上通过有监督的微调和增强学习进一步完善。最终的模型支持几种其他语言，同时了解图像和执行工具调用。在公共基准和人类评估中，服务器模型和设备模型匹配或超过尺寸的开放基线。一个新的以Swift为中心的基础模型框架揭示了有指导的生成，受限的工具调用和Lora适配器微调，从而使开发人员可以将这些功能与几行代码相结合。 Apple Intelligence模型中的最新进步基于我们负责的AI方法，其保障措施（例如内容过滤和特定于语言环境的评估）以及我们致力于通过私人云计算等创新来保护用户隐私的承诺。</li>
</ul>

<h3>Title: Efficient Burst Super-Resolution with One-step Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Kento Kawai, Takeru Oba, Kyotaro Tokoro, Kazutoshi Akita, Norimichi Ukita</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13607">https://arxiv.org/abs/2507.13607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13607">https://arxiv.org/pdf/2507.13607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13607]] Efficient Burst Super-Resolution with One-step Diffusion(https://arxiv.org/abs/2507.13607)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>While burst Low-Resolution (LR) images are useful for improving their Super Resolution (SR) image compared to a single LR image, prior burst SR methods are trained in a deterministic manner, which produces a blurry SR image. Since such blurry images are perceptually degraded, we aim to reconstruct sharp and high-fidelity SR images by a diffusion model. Our method improves the efficiency of the diffusion model with a stochastic sampler with a high-order ODE as well as one-step diffusion using knowledge distillation. Our experimental results demonstrate that our method can reduce the runtime to 1.6 % of its baseline while maintaining the SR quality measured based on image distortion and perceptual quality.</li>
<li><strong>摘要：</strong>与单个LR图像相比，爆发低分辨率（LR）图像对于改善其超级分辨率（SR）图像很有用，但以确定性的方式训练了先前的爆发SR方法，从而产生模糊的SR图像。由于这种模糊的图像在感知上被降解，因此我们旨在通过扩散模型重建尖锐和高保真的SR图像。我们的方法使用知识蒸馏使用具有高阶颂歌以及一步扩散的随机采样器来提高扩散模型的效率。我们的实验结果表明，我们的方法可以将运行时间降低到其基线的1.6％，同时根据图像失真和感知质量来维持SR质量。</li>
</ul>

<h3>Title: A Comprehensive Review of Transformer-based language models for Protein Sequence Analysis and Design</h3>
<ul>
<li><strong>Authors: </strong>Nimisha Ghosh, Daniele Santoni, Debaleena Nawn, Eleonora Ottaviani, Giovanni Felici</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13646">https://arxiv.org/abs/2507.13646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13646">https://arxiv.org/pdf/2507.13646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13646]] A Comprehensive Review of Transformer-based language models for Protein Sequence Analysis and Design(https://arxiv.org/abs/2507.13646)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The impact of Transformer-based language models has been unprecedented in Natural Language Processing (NLP). The success of such models has also led to their adoption in other fields including bioinformatics. Taking this into account, this paper discusses recent advances in Transformer-based models for protein sequence analysis and design. In this review, we have discussed and analysed a significant number of works pertaining to such applications. These applications encompass gene ontology, functional and structural protein identification, generation of de novo proteins and binding of proteins. We attempt to shed light on the strength and weaknesses of the discussed works to provide a comprehensive insight to readers. Finally, we highlight shortcomings in existing research and explore potential avenues for future developments. We believe that this review will help researchers working in this field to have an overall idea of the state of the art in this field, and to orient their future studies.</li>
<li><strong>摘要：</strong>基于变压器的语言模型的影响在自然语言处理（NLP）中是前所未有的。这种模型的成功也导致了它们在包括生物信息学在内的其他领域的采用。考虑到这一点，本文讨论了基于变压器的蛋白质序列分析和设计模型的最新进展。在这篇综述中，我们讨论并分析了与此类应用有关的大量作品。这些应用包括基因本体论，功能和结构蛋白鉴定，从头蛋白的产生以及蛋白质的结合。我们试图阐明讨论的作品的优势和劣势，以为读者提供全面的见解。最后，我们重点介绍了现有研究的缺点，并探索了未来发展的潜在途径。我们认为，这篇评论将帮助在这一领域工作的研究人员对该领域的最新技术有一个整体思想，并确定他们的未来研究。</li>
</ul>

<h3>Title: EPSilon: Efficient Point Sampling for Lightening of Hybrid-based 3D Avatar Generation</h3>
<ul>
<li><strong>Authors: </strong>Seungjun Moon, Sangjoon Yu, Gyeong-Moon Park</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13648">https://arxiv.org/abs/2507.13648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13648">https://arxiv.org/pdf/2507.13648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13648]] EPSilon: Efficient Point Sampling for Lightening of Hybrid-based 3D Avatar Generation(https://arxiv.org/abs/2507.13648)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The rapid advancement of neural radiance fields (NeRF) has paved the way to generate animatable human avatars from a monocular video. However, the sole usage of NeRF suffers from a lack of details, which results in the emergence of hybrid representation that utilizes SMPL-based mesh together with NeRF representation. While hybrid-based models show photo-realistic human avatar generation qualities, they suffer from extremely slow inference due to their deformation scheme: to be aligned with the mesh, hybrid-based models use the deformation based on SMPL skinning weights, which needs high computational costs on each sampled point. We observe that since most of the sampled points are located in empty space, they do not affect the generation quality but result in inference latency with deformation. In light of this observation, we propose EPSilon, a hybrid-based 3D avatar generation scheme with novel efficient point sampling strategies that boost both training and inference. In EPSilon, we propose two methods to omit empty points at rendering; empty ray omission (ERO) and empty interval omission (EIO). In ERO, we wipe out rays that progress through the empty space. Then, EIO narrows down the sampling interval on the ray, which wipes out the region not occupied by either clothes or mesh. The delicate sampling scheme of EPSilon enables not only great computational cost reduction during deformation but also the designation of the important regions to be sampled, which enables a single-stage NeRF structure without hierarchical sampling. Compared to existing methods, EPSilon maintains the generation quality while using only 3.9% of sampled points and achieves around 20 times faster inference, together with 4 times faster training convergence. We provide video results on this https URL.</li>
<li><strong>摘要：</strong>神经辐射场（NERF）的快速发展为从单眼视频中产生可动画的人化身铺平了道路。但是，NERF的唯一用法遭受了缺乏细节的影响，这导致混合表示的出现，该杂种表示利用基于SMPL的网格以及NERF代表。虽然基于混合动力的模型显示出光真逼真的人类头像的产生质量，但由于其变形方案，它们的推断非常缓慢：要与网格保持一致，基于混合的模型使用基于SMPL Skinning权重的变形，这需要每个采样点上的计算成本高。我们观察到，由于大多数采样点位于空白空间中，因此它们不会影响发电质量，但会导致变形的推理潜伏期。鉴于这一观察，我们提出了Epsilon，这是一种基于混合的3D化身生成方案，具有新型的有效点采样策略，可以促进训练和推理。在Epsilon中，我们提出了两种方法来省略渲染时的空点。空射线遗漏（ERO）和空间隔遗漏（EIO）。在ERO中，我们消除了在空白空间中进展的光线。然后，EIO缩小了射线上的采样间隔，该间隔擦除了不被衣服或网眼所占据的区域。 Epsilon的精致采样方案不仅可以在变形过程中降低大幅度的计算成本，而且还可以采样重要区域的指定，从而实现了无层次采样的单阶段NERF结构。与现有方法相比，Epsilon保持了生成质量，同时仅使用3.9％的采样点并获得更快的推断速度约20倍，并加快了4倍的训练收敛。我们在此HTTPS URL上提供视频结果。</li>
</ul>

<h3>Title: Global Modeling Matters: A Fast, Lightweight and Effective Baseline for Efficient Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Xingyu Jiang, Ning Gao, Hongkun Dou, Xiuhui Zhang, Xiaoqing Zhong, Yue Deng, Hongjue Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13663">https://arxiv.org/abs/2507.13663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13663">https://arxiv.org/pdf/2507.13663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13663]] Global Modeling Matters: A Fast, Lightweight and Effective Baseline for Efficient Image Restoration(https://arxiv.org/abs/2507.13663)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, super-resolution</a></li>
<li><strong>Abstract: </strong>Natural image quality is often degraded by adverse weather conditions, significantly impairing the performance of downstream tasks. Image restoration has emerged as a core solution to this challenge and has been widely discussed in the literature. Although recent transformer-based approaches have made remarkable progress in image restoration, their increasing system complexity poses significant challenges for real-time processing, particularly in real-world deployment scenarios. To this end, most existing methods attempt to simplify the self-attention mechanism, such as by channel self-attention or state space model. However, these methods primarily focus on network architecture while neglecting the inherent characteristics of image restoration itself. In this context, we explore a pyramid Wavelet-Fourier iterative pipeline to demonstrate the potential of Wavelet-Fourier processing for image restoration. Inspired by the above findings, we propose a novel and efficient restoration baseline, named Pyramid Wavelet-Fourier Network (PW-FNet). Specifically, PW-FNet features two key design principles: 1) at the inter-block level, integrates a pyramid wavelet-based multi-input multi-output structure to achieve multi-scale and multi-frequency bands decomposition; and 2) at the intra-block level, incorporates Fourier transforms as an efficient alternative to self-attention mechanisms, effectively reducing computational complexity while preserving global modeling capability. Extensive experiments on tasks such as image deraining, raindrop removal, image super-resolution, motion deblurring, image dehazing, image desnowing and underwater/low-light enhancement demonstrate that PW-FNet not only surpasses state-of-the-art methods in restoration quality but also achieves superior efficiency, with significantly reduced parameter size, computational cost and inference time.</li>
<li><strong>摘要：</strong>自然图像质量通常会因不利天气条件而降低，从而大大损害了下游任务的执行。图像恢复已成为解决这一挑战的核心解决方案，并在文献中广泛讨论。尽管最近的基于变压器的方法在图像恢复方面取得了显着进展，但它们增加的系统复杂性对实时处理构成了重大挑战，尤其是在现实世界部署方案中。为此，大多数现有的方法试图简化自我发挥的机制，例如通过渠道自我注意或状态空间模型。但是，这些方法主要集中于网络体系结构，同时忽略了图像恢复本身的固有特征。在这种情况下，我们探索了金字塔小波迭代的迭代管道，以证明小波折价处理图像恢复的潜力。受上述发现的启发，我们提出了一种新颖有效的恢复基线，称为金字塔小波爆弱网络（PW-FNET）。具体而言，PW-FNET具有两个关键的设计原理：1）在块间级别，集成了基于金字塔小波的多输入多输出结构，以实现多尺度和多频带的分解； 2）在块内水平上，将傅立叶变换作为自我发项机制的有效替代方案，有效地降低了计算复杂性，同时保留了全局建模能力。对诸如图像降低，雨滴清除，超级分辨率，运动去脱毛，去掩饰，丧失图像，避免图像和水下/低光增强等任务进行的大规模实验表明，PW-FNET不仅超过了恢复质量的最先进方法，还可以达到优质效率，还可以实现优质的效率，并显着降低了尺寸的成本和计算成本，计算成本和计算成本，计算成本和分解时间。</li>
</ul>

<h3>Title: Gaussian kernel-based motion measurement</h3>
<ul>
<li><strong>Authors: </strong>Hongyi Liu, Haifeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13693">https://arxiv.org/abs/2507.13693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13693">https://arxiv.org/pdf/2507.13693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13693]] Gaussian kernel-based motion measurement(https://arxiv.org/abs/2507.13693)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>The growing demand for structural health monitoring has driven increasing interest in high-precision motion measurement, as structural information derived from extracted motions can effectively reflect the current condition of the structure. Among various motion measurement techniques, vision-based methods stand out due to their low cost, easy installation, and large-scale measurement. However, when it comes to sub-pixel-level motion measurement, current vision-based methods either lack sufficient accuracy or require extensive manual parameter tuning (e.g., pyramid layers, target pixels, and filter parameters) to reach good precision. To address this issue, we developed a novel Gaussian kernel-based motion measurement method, which can extract the motion between different frames via tracking the location of Gaussian kernels. The motion consistency, which fits practical structural conditions, and a super-resolution constraint, are introduced to increase accuracy and robustness of our method. Numerical and experimental validations show that it can consistently reach high accuracy without customized parameter setup for different test samples.</li>
<li><strong>摘要：</strong>对结构健康监测的需求不断增长，导致人们对高精度运动测量的兴趣增加，因为从提取的运动中得出的结构信息可以有效地反映结构的当前状况。在各种运动测量技术中，基于视觉的方法由于其低成本，易于安装和大规模测量而脱颖而出。但是，当涉及子像素级运动测量时，基于当前的视觉方法要么缺乏足够的精度，要么需要大量的手动参数调整（例如金字塔层，目标像素和滤波器参数）才能达到良好的精度。为了解决这个问题，我们开发了一种基于高斯内核的新型运动测量方法，该方法可以通过跟踪高斯内核的位置来提取不同帧之间的运动。引入了适合实际结构条件和超分辨率约束的运动一致性，以提高我们方法的准确性和鲁棒性。数值和实验验证表明，如果没有针对不同的测试样本的自定义参数设置，它可以始终达到高精度。</li>
</ul>

<h3>Title: PoemTale Diffusion: Minimising Information Loss in Poem to Image Generation with Multi-Stage Prompt Refinement</h3>
<ul>
<li><strong>Authors: </strong>Sofia Jamil, Bollampalli Areen Reddy, Raghvendra Kumar, Sriparna Saha, Koustava Goswami, K.J. Joseph</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13708">https://arxiv.org/abs/2507.13708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13708">https://arxiv.org/pdf/2507.13708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13708]] PoemTale Diffusion: Minimising Information Loss in Poem to Image Generation with Multi-Stage Prompt Refinement(https://arxiv.org/abs/2507.13708)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advancements in text-to-image diffusion models have achieved remarkable success in generating realistic and diverse visual content. A critical factor in this process is the model's ability to accurately interpret textual prompts. However, these models often struggle with creative expressions, particularly those involving complex, abstract, or highly descriptive language. In this work, we introduce a novel training-free approach tailored to improve image generation for a unique form of creative language: poetic verse, which frequently features layered, abstract, and dual meanings. Our proposed PoemTale Diffusion approach aims to minimise the information that is lost during poetic text-to-image conversion by integrating a multi stage prompt refinement loop into Language Models to enhance the interpretability of poetic texts. To support this, we adapt existing state-of-the-art diffusion models by modifying their self-attention mechanisms with a consistent self-attention technique to generate multiple consistent images, which are then collectively used to convey the poem's meaning. Moreover, to encourage research in the field of poetry, we introduce the P4I (PoemForImage) dataset, consisting of 1111 poems sourced from multiple online and offline resources. We engaged a panel of poetry experts for qualitative assessments. The results from both human and quantitative evaluations validate the efficacy of our method and contribute a novel perspective to poem-to-image generation with enhanced information capture in the generated images.</li>
<li><strong>摘要：</strong>文本到图像扩散模型的最新进步在产生现实和多样化的视觉内容方面取得了巨大的成功。此过程中的关键因素是模型准确解释文本提示的能力。但是，这些模型通常会在创造性的表达中挣扎，尤其是涉及复杂，抽象或高度描述性语言的模型。在这项工作中，我们介绍了一种量身定制的新颖的无训练方法，以改善一种独特的创意语言形式的形式：诗歌经常具有分层，抽象和双重含义。我们提出的诗歌扩散方法旨在通过将多阶段及时的简化循环整合到语言模型中，以增强诗意文本的解释性，从而最大程度地减少诗歌到图像转换过程中丢失的信息。为了支持这一点，我们通过使用一致的自我发言技术来修改其自我注意力的机制来调整现有的最新扩散模型，以产生多个一致的图像，然后将其集体用于传达这首诗的含义。此外，为了鼓励在诗歌领域的研究，我们介绍了P4I（Poemforimage）数据集，该数据集由来自多个在线和离线资源的1111首诗组成。我们聘请了诗歌专家小组进行定性评估。人类和定量评估的结果都证明了我们方法的功效，并为诗歌形象产生贡献了新的观点，并在生成的图像中以增强的信息捕获。</li>
</ul>

<h3>Title: Benchmarking of EEG Analysis Techniques for Parkinson's Disease Diagnosis: A Comparison between Traditional ML Methods and Foundation DL Methods</h3>
<ul>
<li><strong>Authors: </strong>Danilo Avola, Andrea Bernardini, Giancarlo Crocetti, Andrea Ladogana, Mario Lezoche, Maurizio Mancini, Daniele Pannone, Amedeo Ranaldi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13716">https://arxiv.org/abs/2507.13716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13716">https://arxiv.org/pdf/2507.13716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13716]] Benchmarking of EEG Analysis Techniques for Parkinson's Disease Diagnosis: A Comparison between Traditional ML Methods and Foundation DL Methods(https://arxiv.org/abs/2507.13716)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Parkinson's Disease PD is a progressive neurodegenerative disorder that affects motor and cognitive functions with early diagnosis being critical for effective clinical intervention Electroencephalography EEG offers a noninvasive and costeffective means of detecting PDrelated neural alterations yet the development of reliable automated diagnostic models remains a challenge In this study we conduct a systematic benchmark of traditional machine learning ML and deep learning DL models for classifying PD using a publicly available oddball task dataset Our aim is to lay the groundwork for developing an effective learning system and to determine which approach produces the best results We implement a unified sevenstep preprocessing pipeline and apply consistent subjectwise crossvalidation and evaluation criteria to ensure comparability across models Our results demonstrate that while baseline deep learning architectures particularly CNNLSTM models achieve the best performance compared to other deep learning architectures underlining the importance of capturing longrange temporal dependencies several traditional classifiers such as XGBoost also offer strong predictive accuracy and calibrated decision boundaries By rigorously comparing these baselines our work provides a solid reference framework for future studies aiming to develop and evaluate more complex or specialized architectures Establishing a reliable set of baseline results is essential to contextualize improvements introduced by novel methods ensuring scientific rigor and reproducibility in the evolving field of EEGbased neurodiagnostics</li>
<li><strong>摘要：</strong>帕金森氏病PD是一种进行性神经退行性疾病，会影响运动和认知功能，早期诊断对于有效的临床临床介绍至关重要我们的目的是为开发有效的学习系统奠定基础，并确定哪种方法产生了最佳结果，我们实现了统一的七个步骤预处理管道，并应用一致的主题上的跨验交易和评估标准，以确保在模型中确保在基线学习中尤其是CNNLSTM MATERTINE的模型，以确保跨模型的相比，尤其是CNNLSTM的最佳型号。 temporal dependencies several traditional classifiers such as XGBoost also offer strong predictive accuracy and calibrated decision boundaries By rigorously comparing these baselines our work provides a solid reference framework for future studies aiming to develop and evaluate more complex or specialized architectures Establishing a reliable set of baseline results is essential to contextualize improvements introduced by novel methods ensuring scientific rigor and reproducibility in the evolving field of EEGbased神经诊断</li>
</ul>

<h3>Title: Tackling fake images in cybersecurity -- Interpretation of a StyleGAN and lifting its black-box</h3>
<ul>
<li><strong>Authors: </strong>Julia Laubmann, Johannes Reschke</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13722">https://arxiv.org/abs/2507.13722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13722">https://arxiv.org/pdf/2507.13722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13722]] Tackling fake images in cybersecurity -- Interpretation of a StyleGAN and lifting its black-box(https://arxiv.org/abs/2507.13722)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In today's digital age, concerns about the dangers of AI-generated images are increasingly common. One powerful tool in this domain is StyleGAN (style-based generative adversarial networks), a generative adversarial network capable of producing highly realistic synthetic faces. To gain a deeper understanding of how such a model operates, this work focuses on analyzing the inner workings of StyleGAN's generator component. Key architectural elements and techniques, such as the Equalized Learning Rate, are explored in detail to shed light on the model's behavior. A StyleGAN model is trained using the PyTorch framework, enabling direct inspection of its learned weights. Through pruning, it is revealed that a significant number of these weights can be removed without drastically affecting the output, leading to reduced computational requirements. Moreover, the role of the latent vector -- which heavily influences the appearance of the generated faces -- is closely examined. Global alterations to this vector primarily affect aspects like color tones, while targeted changes to individual dimensions allow for precise manipulation of specific facial features. This ability to finetune visual traits is not only of academic interest but also highlights a serious ethical concern: the potential misuse of such technology. Malicious actors could exploit this capability to fabricate convincing fake identities, posing significant risks in the context of digital deception and cybercrime.</li>
<li><strong>摘要：</strong>在当今的数字时代，人们对AI生成图像的危险的担忧越来越普遍。该域中的一个强大工具是StyleGan（基于样式的生成对抗网络），这是一个能够生成高度逼真的合成面的生成对抗网络。为了更深入地了解这种模型的运作方式，这项工作着重于分析Stylegan生成器组件的内部工作。详细探讨了关键的架构元素和技术，例如均衡的学习率，以阐明模型的行为。使用Pytorch框架对StyleGan模型进行了训练，从而可以直接检查其学习的权重。通过修剪，可以揭示出可以消除大量这些权重的情况，而不会极大地影响输出，从而减少了计算要求。此外，仔细研究了潜在矢量的作用 - 严重影响生成的面孔的外观。该向量的全球变化主要影响诸如色调之类的方面，而对各个维度的有针对性更改则可以精确地操纵特定的面部特征。这种命运视觉特征的能力不仅具有学术利益，而且突出了一个严重的道德问题：这种技术的潜在滥用。恶意演员可以利用这种能力来构建令人信服的虚假身份，在数字欺骗和网络犯罪的背景下构成重大风险。</li>
</ul>

<h3>Title: Can Synthetic Images Conquer Forgetting? Beyond Unexplored Doubts in Few-Shot Class-Incremental Learning</h3>
<ul>
<li><strong>Authors: </strong>Junsu Kim, Yunhoe Ku, Seungryul Baek</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13739">https://arxiv.org/abs/2507.13739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13739">https://arxiv.org/pdf/2507.13739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13739]] Can Synthetic Images Conquer Forgetting? Beyond Unexplored Doubts in Few-Shot Class-Incremental Learning(https://arxiv.org/abs/2507.13739)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Few-shot class-incremental learning (FSCIL) is challenging due to extremely limited training data; while aiming to reduce catastrophic forgetting and learn new information. We propose Diffusion-FSCIL, a novel approach that employs a text-to-image diffusion model as a frozen backbone. Our conjecture is that FSCIL can be tackled using a large generative model's capabilities benefiting from 1) generation ability via large-scale pre-training; 2) multi-scale representation; 3) representational flexibility through the text encoder. To maximize the representation capability, we propose to extract multiple complementary diffusion features to play roles as latent replay with slight support from feature distillation for preventing generative biases. Our framework realizes efficiency through 1) using a frozen backbone; 2) minimal trainable components; 3) batch processing of multiple feature extractions. Extensive experiments on CUB-200, \emph{mini}ImageNet, and CIFAR-100 show that Diffusion-FSCIL surpasses state-of-the-art methods, preserving performance on previously learned classes and adapting effectively to new ones.</li>
<li><strong>摘要：</strong>由于培训数据极为有限，很少有类班级学习（FSCIL）具有挑战性。旨在减少灾难性遗忘和学习新信息。我们提出了扩散-FSCIL，这是一种新型方法，该方法采用文本对图扩散模型作为冷冻骨干。我们的猜想是，可以使用大型生成模型的能力来解决FSCIL，从而受益于1）通过大规模预训练的发电能力； 2）多尺度表示； 3）通过文本编码器表示灵活性。为了最大程度地提高表示能力，我们建议提取多个互补扩散特征，以发挥作用，作为潜在重放的作用，并在特征蒸馏中略有支持以防止产生偏见。我们的框架通过1）使用冷冻的主链实现效率； 2）最小的可训练组件； 3）多种特征提取的批处理处理。对Cub-200，\ Emph {mini}和CIFAR-100进行的广泛实验表明，扩散FSCIL超过了最新方法，可以保留先前学到的类别的性能，并有效地适应新的类别。</li>
</ul>

<h3>Title: Encapsulated Composition of Text-to-Image and Text-to-Video Models for High-Quality Video Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Tongtong Su, Chengyu Wang, Bingyan Liu, Jun Huang, Dongming Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13753">https://arxiv.org/abs/2507.13753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13753">https://arxiv.org/pdf/2507.13753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13753]] Encapsulated Composition of Text-to-Image and Text-to-Video Models for High-Quality Video Synthesis(https://arxiv.org/abs/2507.13753)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In recent years, large text-to-video (T2V) synthesis models have garnered considerable attention for their abilities to generate videos from textual descriptions. However, achieving both high imaging quality and effective motion representation remains a significant challenge for these T2V models. Existing approaches often adapt pre-trained text-to-image (T2I) models to refine video frames, leading to issues such as flickering and artifacts due to inconsistencies across frames. In this paper, we introduce EVS, a training-free Encapsulated Video Synthesizer that composes T2I and T2V models to enhance both visual fidelity and motion smoothness of generated videos. Our approach utilizes a well-trained diffusion-based T2I model to refine low-quality video frames by treating them as out-of-distribution samples, effectively optimizing them with noising and denoising steps. Meanwhile, we employ T2V backbones to ensure consistent motion dynamics. By encapsulating the T2V temporal-only prior into the T2I generation process, EVS successfully leverages the strengths of both types of models, resulting in videos of improved imaging and motion quality. Experimental results validate the effectiveness of our approach compared to previous approaches. Our composition process also leads to a significant improvement of 1.6x-4.5x speedup in inference time. Source codes: this https URL.</li>
<li><strong>摘要：</strong>近年来，大型文本访问（T2V）合成模型已引起了他们从文本描述中生成视频的能力的极大关注。但是，对于这些T2V模型，达到高成像质量和有效运动表示仍然是一个重大挑战。现有的方法通常会适应预先训练的文本对图像（T2I）模型来完善视频帧，从而导致诸如跨帧的不一致引起的闪烁和人工制品等问题。在本文中，我们介绍了EVS，这是一种无训练的封装视频合成器，该视频合成器构成了T2I和T2V模型，以增强生成的视频的视觉保真度和运动平滑度。我们的方法利用训练有素的基于基于扩散的T2I模型通过将其视为分布式样本，通过陈述和脱氧步骤有效地优化它们，从而优化低质量的视频帧。同时，我们采用T2V主干来确保一致的运动动力学。通过将T2V时间限制为T2I生成过程中，EV成功地利用了两种模型的优势，从而产生了改进的成像和运动质量的视频。实验结果证明了与以前的方法相比，我们的方法的有效性。我们的组成过程还导致推理时间的显着提高1.6倍-4.5倍。源代码：此HTTPS URL。</li>
</ul>

<h3>Title: MolPIF: A Parameter Interpolation Flow Model for Molecule Generation</h3>
<ul>
<li><strong>Authors: </strong>Yaowei Jin, Junjie Wang, Wenkai Xiang, Duanhua Cao, Dan Teng, Zhehuan Fan, Jiacheng Xiong, Xia Sheng, Chuanlong Zeng, Mingyue Zheng, Qian Shi</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13762">https://arxiv.org/abs/2507.13762</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13762">https://arxiv.org/pdf/2507.13762</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13762]] MolPIF: A Parameter Interpolation Flow Model for Molecule Generation(https://arxiv.org/abs/2507.13762)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Advances in deep learning for molecular generation show promise in accelerating drug discovery. Bayesian Flow Networks (BFNs) have recently shown impressive performance across diverse chemical tasks, with their success often ascribed to the paradigm of modeling in a low-variance parameter space. However, the Bayesian inference-based strategy imposes limitations on designing more flexible distribution transformation pathways, making it challenging to adapt to diverse data distributions and varied task requirements. Furthermore, the potential for simpler, more efficient parameter-space-based models is unexplored. To address this, we propose a novel Parameter Interpolation Flow model (named PIF) with detailed theoretical foundation, training, and inference procedures. We then develop MolPIF for structure-based drug design, demonstrating its superior performance across diverse metrics compared to baselines. This work validates the effectiveness of parameter-space-based generative modeling paradigm for molecules and offers new perspectives for model design.</li>
<li><strong>摘要：</strong>分子生成深度学习的进步在加速药物发现方面表现出了希望。贝叶斯流动网络（BFN）最近在各种化学任务中表现出令人印象深刻的性能，其成功通常归因于在低相位参数空间中建模的范式。但是，基于贝叶斯推理的策略对设计更灵活的分配转换途径施加了限制，因此适应各种数据分布和各种任务要求的挑战。此外，无法探索更简单，更有效的基于参数空间的模型的潜力。为了解决这个问题，我们提出了一个具有详细理论基础，训练和推理程序的新型参数插值流模型（名为PIF）。然后，我们开发用于基于结构的药物设计的Molpif，与基线相比，它在不同的指标上表现出了卓越的性能。这项工作验证了基于参数空间的生成建模范式对分子的有效性，并为模型设计提供了新的观点。</li>
</ul>

<h3>Title: Localized FNO for Spatiotemporal Hemodynamic Upsampling in Aneurysm MRI</h3>
<ul>
<li><strong>Authors: </strong>Kyriakos Flouris, Moritz Halter, Yolanne Y. R. Lee, Samuel Castonguay, Luuk Jacobs, Pietro Dirix, Jonathan Nestmann, Sebastian Kozerke, Ender Konukoglu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13789">https://arxiv.org/abs/2507.13789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13789">https://arxiv.org/pdf/2507.13789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13789]] Localized FNO for Spatiotemporal Hemodynamic Upsampling in Aneurysm MRI(https://arxiv.org/abs/2507.13789)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Hemodynamic analysis is essential for predicting aneurysm rupture and guiding treatment. While magnetic resonance flow imaging enables time-resolved volumetric blood velocity measurements, its low spatiotemporal resolution and signal-to-noise ratio limit its diagnostic utility. To address this, we propose the Localized Fourier Neural Operator (LoFNO), a novel 3D architecture that enhances both spatial and temporal resolution with the ability to predict wall shear stress (WSS) directly from clinical imaging data. LoFNO integrates Laplacian eigenvectors as geometric priors for improved structural awareness on irregular, unseen geometries and employs an Enhanced Deep Super-Resolution Network (EDSR) layer for robust upsampling. By combining geometric priors with neural operator frameworks, LoFNO de-noises and spatiotemporally upsamples flow data, achieving superior velocity and WSS predictions compared to interpolation and alternative deep learning methods, enabling more precise cerebrovascular diagnostics.</li>
<li><strong>摘要：</strong>血液动力学分析对于预测动脉瘤破裂和指导治疗至关重要。尽管磁共振流成像可实现时间分辨的体积血速速度测量值，但其时空分辨率低和信噪比限制了其诊断效用。为了解决这个问题，我们提出了局部的傅立叶神经操作员（LOFNO），这是一种新型的3D体系结构，可通过直接从临床成像数据中预测壁剪应力（WSS）的能力来增强空间和时间分辨率。 LOFNO将拉普拉斯特征向量整合为几何先验，以提高对不规则，看不见的几何形状的结构意识，并采用增强的深层超分辨率网络（EDSR）层来进行稳健的上采样。通过将几何先验与神经操作员框架相结合，LOFNO DE-NOASES和时空临时示例流数据，与插值和替代性深度学习方法相比，实现了较高的速度和WSS预测，从而实现了更精确的大脑诊断。</li>
</ul>

<h3>Title: DynFaceRestore: Balancing Fidelity and Quality in Diffusion-Guided Blind Face Restoration with Dynamic Blur-Level Mapping and Guidance</h3>
<ul>
<li><strong>Authors: </strong>Huu-Phu Do, Yu-Wei Chen, Yi-Cheng Liao, Chi-Wei Hsiao, Han-Yang Wang, Wei-Chen Chiu, Ching-Chun Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13797">https://arxiv.org/abs/2507.13797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13797">https://arxiv.org/pdf/2507.13797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13797]] DynFaceRestore: Balancing Fidelity and Quality in Diffusion-Guided Blind Face Restoration with Dynamic Blur-Level Mapping and Guidance(https://arxiv.org/abs/2507.13797)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generation</a></li>
<li><strong>Abstract: </strong>Blind Face Restoration aims to recover high-fidelity, detail-rich facial images from unknown degraded inputs, presenting significant challenges in preserving both identity and detail. Pre-trained diffusion models have been increasingly used as image priors to generate fine details. Still, existing methods often use fixed diffusion sampling timesteps and a global guidance scale, assuming uniform degradation. This limitation and potentially imperfect degradation kernel estimation frequently lead to under- or over-diffusion, resulting in an imbalance between fidelity and quality. We propose DynFaceRestore, a novel blind face restoration approach that learns to map any blindly degraded input to Gaussian blurry images. By leveraging these blurry images and their respective Gaussian kernels, we dynamically select the starting timesteps for each blurry image and apply closed-form guidance during the diffusion sampling process to maintain fidelity. Additionally, we introduce a dynamic guidance scaling adjuster that modulates the guidance strength across local regions, enhancing detail generation in complex areas while preserving structural fidelity in contours. This strategy effectively balances the trade-off between fidelity and quality. DynFaceRestore achieves state-of-the-art performance in both quantitative and qualitative evaluations, demonstrating robustness and effectiveness in blind face restoration.</li>
<li><strong>摘要：</strong>盲人面部修复旨在从未知的退化输入中恢复高保真，细节丰富的面部图像，在保留身份和细节方面面临着重大挑战。预训练的扩散模型已越来越多地用作图像先验，以生成细节。尽管如此，现有方法通常使用固定的扩散抽样时间段和全球指导量表，假设均匀降解。这种局限性以及潜在的不完美降解核估计经常导致不足或过度扩张，从而导致忠诚度和质量之间的不平衡。我们提出了Dynfacerestore，这是一种新颖的盲人恢复方法，该方法学会映射任何盲目退化的输入到高斯模糊图像中。通过利用这些模糊的图像及其各自的高斯内核，我们动态选择了每个模糊图像的起始时间段，并在扩散采样过程中应用封闭形式的指导以维持忠诚度。此外，我们引入了动态的指导缩放调节器，该调节器调节了各个地方区域的指导强度，从而增强了复杂区域的细节生成，同时保留了轮廓中的结构忠诚度。该策略有效地平衡了忠诚与质量之间的权衡。 Dynfacerestore在定量和定性评估中都取得了最新的表现，证明了盲人面部恢复方面的鲁棒性和有效性。</li>
</ul>

<h3>Title: On-the-Fly Fine-Tuning of Foundational Neural Network Potentials: A Bayesian Neural Network Approach</h3>
<ul>
<li><strong>Authors: </strong>Tim Rensmeyer, Denis Kramer, Oliver Niggemann</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13805">https://arxiv.org/abs/2507.13805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13805">https://arxiv.org/pdf/2507.13805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13805]] On-the-Fly Fine-Tuning of Foundational Neural Network Potentials: A Bayesian Neural Network Approach(https://arxiv.org/abs/2507.13805)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Due to the computational complexity of evaluating interatomic forces from first principles, the creation of interatomic machine learning force fields has become a highly active field of research. However, the generation of training datasets of sufficient size and sample diversity itself comes with a computational burden that can make this approach impractical for modeling rare events or systems with a large configuration space. Fine-tuning foundation models that have been pre-trained on large-scale material or molecular databases offers a promising opportunity to reduce the amount of training data necessary to reach a desired level of accuracy. However, even if this approach requires less training data overall, creating a suitable training dataset can still be a very challenging problem, especially for systems with rare events and for end-users who don't have an extensive background in machine learning. In on-the-fly learning, the creation of a training dataset can be largely automated by using model uncertainty during the simulation to decide if the model is accurate enough or if a structure should be recalculated with classical methods and used to update the model. A key challenge for applying this form of active learning to the fine-tuning of foundation models is how to assess the uncertainty of those models during the fine-tuning process, even though most foundation models lack any form of uncertainty quantification. In this paper, we overcome this challenge by introducing a fine-tuning approach based on Bayesian neural network methods and a subsequent on-the-fly workflow that automatically fine-tunes the model while maintaining a pre-specified accuracy and can detect rare events such as transition states and sample them at an increased rate relative to their occurrence.</li>
<li><strong>摘要：</strong>由于从第一原则评估原子间力的计算复杂性，原子间机器学习力场的创建已成为一个高度活跃的研究领域。但是，具有足够大小和样本多样性本身的培训数据集的生成带有计算负担，该计算负担可能使这种方法不切实际地建模具有较大配置空间的稀有事件或系统。在大规模材料或分子数据库中进行了预先培训的微调基础模型为减少达到所需准确性所需的训练数据量提供了有希望的机会。但是，即使这种方法总体上需要更少的培训数据，也可以创建合适的培训数据集仍然是一个非常具有挑战性的问题，尤其是对于有罕见事件的系统，对于没有广泛的机器学习背景的最终用户而言。在直接学习中，可以通过模型在模拟过程中使用模型不确定性来确定模型是否足够准确，或者是否应使用经典方法重新计算结构并用来更新模型来确定结构是否足够准确，从而在很大程度上可以自动化训练数据集的创建。将这种主动学习形式应用于基础模型进行微调的主要挑战是如何在微调过程中评估这些模型的不确定性，即使大多数基础模型都缺乏任何形式的不确定性量化。在本文中，我们通过基于贝叶斯神经网络方法引入微调方法来克服这一挑战，以及随后的直立工作流程，在维持预先指定的准确性的同时自动微调该模型，并可以检测到罕见的事件，例如过渡状态，并以相对于其出现的速率提高了它们。</li>
</ul>

<h3>Title: PositionIC: Unified Position and Identity Consistency for Image Customization</h3>
<ul>
<li><strong>Authors: </strong>Junjie Hu, Tianyang Han, Kai Ma, Jialin Gao, Hao Dou, Song Yang, Xianhua He, Jianhui Zhang, Junfeng Luo, Xiaoming Wei, Wenqiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13861">https://arxiv.org/abs/2507.13861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13861">https://arxiv.org/pdf/2507.13861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13861]] PositionIC: Unified Position and Identity Consistency for Image Customization(https://arxiv.org/abs/2507.13861)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent subject-driven image customization has achieved significant advancements in fidelity, yet fine-grained entity-level spatial control remains elusive, hindering the broader real-world application. This limitation is mainly attributed to scalable datasets that bind identity with precise positional cues are absent. To this end, we introduce PositionIC, a unified framework that enforces position and identity consistency for multi-subject customization. We construct a scalable synthesis pipeline that employs a bidirectional generation paradigm to eliminate subject drift and maintain semantic coherence. On top of these data, we design a lightweight positional modulation layer that decouples spatial embeddings among subjects, enabling independent, accurate placement while preserving visual fidelity. Extensive experiments demonstrate that our approach can achieve precise spatial control while maintaining high consistency in image customization task. PositionIC paves the way for controllable, high-fidelity image customization in open-world, multi-entity scenarios and will be released to foster further research.</li>
<li><strong>摘要：</strong>最近以受试者为驱动的图像定制已取得了良好的忠诚，但良好的实体级空间控制仍然难以捉摸，这阻碍了更广泛的现实世界应用。该限制主要归因于没有精确位置线索结合身份的可扩展数据集。为此，我们介绍了统一框架，该框架为多主体定制实施了位置和身份一致性。我们构建了一种可扩展的合成管道，该管道采用双向产生范式消除受试者漂移并保持语义连贯性。在这些数据之外，我们设计了一个轻巧的位置调制层，该层将主题之间的空间嵌入方式解开，从而实现独立，准确的放置，同时保留视觉保真度。广泛的实验表明，我们的方法可以实现精确的空间控制，同时保持图像自定义任务的高度一致性。位置式为开放世界中的可控，高保真图像定制铺平了道路，并将发布以促进进一步的研究。</li>
</ul>

<h3>Title: Generalist Forecasting with Frozen Video Models via Latent Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Jacob C Walker, Pedro Vélez, Luisa Polania Cabrera, Guangyao Zhou, Rishabh Kabra, Carl Doersch, Maks Ovsjanikov, João Carreira, Shiry Ginosar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13942">https://arxiv.org/abs/2507.13942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13942">https://arxiv.org/pdf/2507.13942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13942]] Generalist Forecasting with Frozen Video Models via Latent Diffusion(https://arxiv.org/abs/2507.13942)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Forecasting what will happen next is a critical skill for general-purpose systems that plan or act in the world at different levels of abstraction. In this paper, we identify a strong correlation between a vision model's perceptual ability and its generalist forecasting performance over short time horizons. This trend holds across a diverse set of pretrained models-including those trained generatively-and across multiple levels of abstraction, from raw pixels to depth, point tracks, and object motion. The result is made possible by a novel generalist forecasting framework that operates on any frozen vision backbone: we train latent diffusion models to forecast future features in the frozen representation space, which are then decoded via lightweight, task-specific readouts. To enable consistent evaluation across tasks, we introduce distributional metrics that compare distributional properties directly in the space of downstream tasks and apply this framework to nine models and four tasks. Our results highlight the value of bridging representation learning and generative modeling for temporally grounded video understanding.</li>
<li><strong>摘要：</strong>预测接下来会发生的事情是通用系统的关键技能，这些系统在世界上以不同级别的抽象计划计划或行动。在本文中，我们确定了视觉模型的感知能力与其通才预测性能之间的密切相关性。这种趋势构成了各种审慎的模型，包括训练有素的训练以及从原始像素到深度，点轨道和物体运动的多个级别的抽象。一个新颖的通才预测框架使结果成为可能，该预测框架在任何冷冻视觉主链上运行：我们训练潜在扩散模型，以预测冷冻表示空间中的未来特征，然后通过轻巧，特定于任务的读数进行解码。为了启用跨任务的一致评估，我们引入了分布指标，这些指标直接比较下游任务空间中的分布属性，并将此框架应用于九个模型和四个任务。我们的结果突出了桥接表示学习和生成建模的价值，以实现时间接地的视频理解。</li>
</ul>

<h3>Title: MoDyGAN: Combining Molecular Dynamics With GANs to Investigate Protein Conformational Space</h3>
<ul>
<li><strong>Authors: </strong>Jingbo Liang, Bruna Jacobson</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.bio-ph, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13950">https://arxiv.org/abs/2507.13950</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13950">https://arxiv.org/pdf/2507.13950</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13950]] MoDyGAN: Combining Molecular Dynamics With GANs to Investigate Protein Conformational Space(https://arxiv.org/abs/2507.13950)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Extensively exploring protein conformational landscapes remains a major challenge in computational biology due to the high computational cost involved in dynamic physics-based simulations. In this work, we propose a novel pipeline, MoDyGAN, that leverages molecular dynamics (MD) simulations and generative adversarial networks (GANs) to explore protein conformational spaces. MoDyGAN contains a generator that maps Gaussian distributions into MD-derived protein trajectories, and a refinement module that combines ensemble learning with a dual-discriminator to further improve the plausibility of generated conformations. Central to our approach is an innovative representation technique that reversibly transforms 3D protein structures into 2D matrices, enabling the use of advanced image-based GAN architectures. We use three rigid proteins to demonstrate that MoDyGAN can generate plausible new conformations. We also use deca-alanine as a case study to show that interpolations within the latent space closely align with trajectories obtained from steered molecular dynamics (SMD) simulations. Our results suggest that representing proteins as image-like data unlocks new possibilities for applying advanced deep learning techniques to biomolecular simulation, leading to an efficient sampling of conformational states. Additionally, the proposed framework holds strong potential for extension to other complex 3D structures.</li>
<li><strong>摘要：</strong>由于基于动态物理学的模拟涉及的高计算成本，广泛探索蛋白质构象景观仍然是计算生物学的主要挑战。在这项工作中，我们提出了一种新型管道Modygan，该管道利用分子动力学（MD）模拟和生成的对抗网络（GAN）来探索蛋白质构象空间。 Modygan包含一个将高斯分布映射到MD衍生的蛋白质轨迹中的发电机，以及将合奏学习与双歧歧歧胶器相结合的改进模块，以进一步提高生成的构象的合理性。我们方法的核心是一种创新的表示技术，它可逆地将3D蛋白结构转换为2D矩阵，从而实现了高级基于图像的GAN体系结构。我们使用三种刚性蛋白来证明Modygan可以产生合理的新构象。我们还使用deca-丙氨酸作为案例研究，以表明潜在空间内的插值与从转向分子动力学（SMD）模拟获得的轨迹紧密一致。我们的结果表明，代表蛋白质作为图像样数据可以解锁将先进的深度学习技术应用于生物分子模拟的新可能性，从而有效地对构象状态进行了采样。此外，所提出的框架具有向其他复合物3D结构扩展的强大潜力。</li>
</ul>

<h3>Title: CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models</h3>
<ul>
<li><strong>Authors: </strong>Quang-Binh Nguyen, Minh Luu, Quang Nguyen, Anh Tran, Khoi Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13984">https://arxiv.org/abs/2507.13984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13984">https://arxiv.org/pdf/2507.13984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13984]] CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models(https://arxiv.org/abs/2507.13984)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Disentangling content and style from a single image, known as content-style decomposition (CSD), enables recontextualization of extracted content and stylization of extracted styles, offering greater creative flexibility in visual synthesis. While recent personalization methods have explored the decomposition of explicit content style, they remain tailored for diffusion models. Meanwhile, Visual Autoregressive Modeling (VAR) has emerged as a promising alternative with a next-scale prediction paradigm, achieving performance comparable to that of diffusion models. In this paper, we explore VAR as a generative framework for CSD, leveraging its scale-wise generation process for improved disentanglement. To this end, we propose CSD-VAR, a novel method that introduces three key innovations: (1) a scale-aware alternating optimization strategy that aligns content and style representation with their respective scales to enhance separation, (2) an SVD-based rectification method to mitigate content leakage into style representations, and (3) an Augmented Key-Value (K-V) memory enhancing content identity preservation. To benchmark this task, we introduce CSD-100, a dataset specifically designed for content-style decomposition, featuring diverse subjects rendered in various artistic styles. Experiments demonstrate that CSD-VAR outperforms prior approaches, achieving superior content preservation and stylization fidelity.</li>
<li><strong>摘要：</strong>将内容和样式从单个图像（称为内容式分解（CSD））中解开，可以重新定下提取的内容和提取样式的风格化，从而在视觉合成方面具有更大的创造性灵活性。尽管最近的个性化方法探索了显式内容样式的分解，但它们仍然针对扩散模型进行量身定制。同时，视觉自回旋建模（VAR）已成为一个有前途的替代方案，具有下一尺度的预测范式，实现了与扩散模型相当的性能。在本文中，我们探索VAR作为CSD的生成框架，利用其规模生成过程来改善分离。 To this end, we propose CSD-VAR, a novel method that introduces three key innovations: (1) a scale-aware alternating optimization strategy that aligns content and style representation with their respective scales to enhance separation, (2) an SVD-based rectification method to mitigate content leakage into style representations, and (3) an Augmented Key-Value (K-V) memory enhancing content identity preservation.为了基准这项任务，我们介绍了CSD-100，这是一种专门为内容式分解而设计的数据集，其中包含以各种艺术风格呈现的各种主题。实验表明，CSD-VAR的表现优于先前的方法，从而实现了卓越的内容保存和风格化保真度。</li>
</ul>

<h3>Title: DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation</h3>
<ul>
<li><strong>Authors: </strong>Haoran Li, Yuli Tian, Kun Lan, Yong Liao, Lin Wang, Pan Hui, Peng Yuan Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13985">https://arxiv.org/abs/2507.13985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13985">https://arxiv.org/pdf/2507.13985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13985]] DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation(https://arxiv.org/abs/2507.13985)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating 3D scenes from natural language holds great promise for applications in gaming, film, and design. However, existing methods struggle with automation, 3D consistency, and fine-grained control. We present DreamScene, an end-to-end framework for high-quality and editable 3D scene generation from text or dialogue. DreamScene begins with a scene planning module, where a GPT-4 agent infers object semantics and spatial constraints to construct a hybrid graph. A graph-based placement algorithm then produces a structured, collision-free layout. Based on this layout, Formation Pattern Sampling (FPS) generates object geometry using multi-timestep sampling and reconstructive optimization, enabling fast and realistic synthesis. To ensure global consistent, DreamScene employs a progressive camera sampling strategy tailored to both indoor and outdoor settings. Finally, the system supports fine-grained scene editing, including object movement, appearance changes, and 4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior methods in quality, consistency, and flexibility, offering a practical solution for open-domain 3D content creation. Code and demos are available at this https URL.</li>
<li><strong>摘要：</strong>从自然语言中产生3D场景对游戏，电影和设计中的应用有很大的希望。但是，现有的方法与自动化，3D一致性和细粒度的控制困难。我们介绍了DreamsCene，这是一个从文本或对话中获得高质量和可编辑3D场景的端到端框架。 DreamsCene始于场景计划模块，其中GPT-4代理会侵入对象语义和空间约束以构建混合图。然后，基于图的放置算法会产生一个结构化的，无碰撞的布局。基于此布局，形成模式采样（FPS）使用多动蛋白采样和重建优化生成对象几何形状，从而实现快速和逼真的综合。为了确保全球一致，DreamsCene采用了针对室内和室外设置量身定制的渐进式摄像机抽样策略。最后，系统支持细粒度的场景编辑，包括对象运动，外观变化和4D动态运动。实验表明，梦境超过了质量，一致性和灵活性的先前方法，为开放域3D内容创建提供了实用的解决方案。代码和演示可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: VLA-Mark: A cross modal watermark for large vision-language alignment model</h3>
<ul>
<li><strong>Authors: </strong>Shuliang Liu, Qi Zheng, Jesse Jiaxi Xu, Yibo Yan, He Geng, Aiwei Liu, Peijie Jiang, Jia Liu, Yik-Cheung Tam, Xuming Hu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14067">https://arxiv.org/abs/2507.14067</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14067">https://arxiv.org/pdf/2507.14067</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14067]] VLA-Mark: A cross modal watermark for large vision-language alignment model(https://arxiv.org/abs/2507.14067)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision-language models demand watermarking solutions that protect intellectual property without compromising multimodal coherence. Existing text watermarking methods disrupt visual-textual alignment through biased token selection and static strategies, leaving semantic-critical concepts vulnerable. We propose VLA-Mark, a vision-aligned framework that embeds detectable watermarks while preserving semantic fidelity through cross-modal coordination. Our approach integrates multiscale visual-textual alignment metrics, combining localized patch affinity, global semantic coherence, and contextual attention patterns, to guide watermark injection without model retraining. An entropy-sensitive mechanism dynamically balances watermark strength and semantic preservation, prioritizing visual grounding during low-uncertainty generation phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than conventional methods, with near-perfect detection (98.8% AUC). The framework demonstrates 96.1\% attack resilience against attacks such as paraphrasing and synonym substitution, while maintaining text-visual consistency, establishing new standards for quality-preserving multimodal watermarking</li>
<li><strong>摘要：</strong>视觉模型需要在不损害多模式相干性的情况下保护知识产权的水印解决方案。现有的文本水印方法通过有偏见的令牌选择和静态策略破坏了视觉文本对齐，使语义关键的概念易受伤害。我们提出了VLA-MARK，这是一个与视觉一致的框架，该框架嵌入可检测的水印，同时通过交叉模式协调来保留语义忠诚。我们的方法集成了多尺度视觉文本对齐指标，结合了局部贴片亲和力，全球语义连贯性和上下文注意力模式，以指导水印注入而不模型再培训。熵敏感的机制动态平衡了水印强度和语义保存，在低确定性生成阶段期间优先考虑视觉接地。实验显示，与常规方法相比，PPL的PPL低7.4％，BLEU高26.6％，近完美检测（98.8％AUC）。该框架展示了96.1 \％的攻击弹性对诸如释义和同义词替代等攻击的弹性弹性，同时保持文本视觉一致性，建立了新的标准，以提供优质的多态水印</li>
</ul>

<h3>Title: NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining</h3>
<ul>
<li><strong>Authors: </strong>Maksim Kuprashevich, Grigorii Alekseenko, Irina Tolstykh, Georgii Fedorov, Bulat Suleimanov, Vladimir Dokholyan, Aleksandr Gordeev</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14119">https://arxiv.org/abs/2507.14119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14119">https://arxiv.org/pdf/2507.14119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14119]] NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining(https://arxiv.org/abs/2507.14119)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative modeling enable image editing assistants that follow natural language instructions without additional user input. Their supervised training requires millions of triplets: original image, instruction, edited image. Yet mining pixel-accurate examples is hard. Each edit must affect only prompt-specified regions, preserve stylistic coherence, respect physical plausibility, and retain visual appeal. The lack of robust automated edit-quality metrics hinders reliable automation at scale. We present an automated, modular pipeline that mines high-fidelity triplets across domains, resolutions, instruction complexities, and styles. Built on public generative models and running without human intervention, our system uses a task-tuned Gemini validator to score instruction adherence and aesthetics directly, removing any need for segmentation or grounding models. Inversion and compositional bootstrapping enlarge the mined set by approximately 2.2x, enabling large-scale high-fidelity training data. By automating the most repetitive annotation steps, the approach allows a new scale of training without human labeling effort. To democratize research in this resource-intensive area, we release NHR-Edit: an open dataset of 358k high-quality triplets. In the largest cross-dataset evaluation, it surpasses all public alternatives. We also release Bagel-NHR-Edit, an open-source fine-tuned Bagel model, which achieves state-of-the-art metrics in our experiments.</li>
<li><strong>摘要：</strong>生成建模的最新进展使图像编辑助理遵循自然语言指令，而无需其他用户输入。他们的监督培训需要数百万的三胞胎：原始图像，指令，编辑图像。然而，挖掘像素精确的例子很难。每个编辑必须仅影响及时指定的区域，保留风格连贯性，尊重身体上的合理性并保持视觉吸引力。缺乏强大的自动化编辑质量指标会在大规模上阻碍可靠的自动化。我们提出了一个自动化的模块化管道，该管道将跨域，分辨率，指导复杂性和样式的高保真三胞胎矿化。我们的系统基于公共生成模型并在没有人工干预的情况下运行，直接使用任务调整的双子座验证器来评分指令依从性和美学，从而消除了对细分或接地模型的任何需求。反转和组成的自举将大约2.2倍设置的开采设置，从而实现大规模的高保真训练数据。通过自动化最重复的注释步骤，该方法允许在无需人工标记工作的情况下进行新的培训规模。为了使该资源密集型领域的研究民主化，我们发布了NHR-Edit：358K高质量三胞胎的开放数据集。在最大的跨数据库评估中，它超过了所有公共替代方案。我们还发布了一种开源微调的百吉饼模型Bagel-NHR-Edit，在我们的实验中实现了最先进的指标。</li>
</ul>

<h3>Title: Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical Perspective</h3>
<ul>
<li><strong>Authors: </strong>Pankaj Yadav, Vivek Vijay</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14121">https://arxiv.org/abs/2507.14121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14121">https://arxiv.org/pdf/2507.14121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14121]] Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical Perspective(https://arxiv.org/abs/2507.14121)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Kolmogorov Arnold Networks (KANs) are recent architectural advancement in neural computation that offer a mathematically grounded alternative to standard neural networks. This study presents an empirical evaluation of KANs in context of class imbalanced classification, using ten benchmark datasets. We observe that KANs can inherently perform well on raw imbalanced data more effectively than Multi-Layer Perceptrons (MLPs) without any resampling strategy. However, conventional imbalance strategies fundamentally conflict with KANs mathematical structure as resampling and focal loss implementations significantly degrade KANs performance, while marginally benefiting MLPs. Crucially, KANs suffer from prohibitive computational costs without proportional performance gains. Statistical validation confirms that MLPs with imbalance techniques achieve equivalence with KANs (|d| < 0.08 across metrics) at minimal resource costs. These findings reveal that KANs represent a specialized solution for raw imbalanced data where resources permit. But their severe performance-resource tradeoffs and incompatibility with standard resampling techniques currently limits practical deployment. We identify critical research priorities as developing KAN specific architectural modifications for imbalance learning, optimizing computational efficiency, and theoretical reconciling their conflict with data augmentation. This work establishes foundational insights for next generation KAN architectures in imbalanced classification scenarios.</li>
<li><strong>摘要：</strong>Kolmogorov Arnold Networks（KANS）是神经计算中最近的建筑进步，它为标准神经网络提供了数学上扎根的替代方案。这项研究使用十个基准数据集对在类不平衡分类的背景下进行了对KAN的经验评估。我们观察到，与多层感知器（MLP）相比，KAN可以在原始不平衡数据上固有地表现良好，而无需任何重新采样策略。但是，传统的不平衡策略从根本上与堪萨斯州的数学结构进行冲突，因为重新采样和焦点损失实施极大地降低了坎斯的绩效，同时略微使MLP受益。至关重要的是，堪萨斯州的计算成本不佳而没有成比例的绩效提高。统计验证证实，具有不平衡技术的MLP以最低的资源成本与堪萨斯州（范围内的指标| d | <0.08）达到等效性。这些发现表明，堪萨斯州代表了用于资源允许的原始数据的专门解决方案。但是，他们严重的性能资源权衡以及与标准重采样技术不兼容的目前限制了实际部署。我们确定了关键的研究优先级，因为为不平衡学习，优化计算效率以及理论上调和了与数据扩展的冲突。这项工作为下一代KAN体系结构建立了基础洞察力，在分类场景中。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
