<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-01-19</h1>
<h3>Title: Future Optical Flow Prediction Improves Robot Control & Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Kanchana Ranasinghe, Honglu Zhou, Yu Fang, Luyu Yang, Le Xue, Ran Xu, Caiming Xiong, Silvio Savarese, Michael S Ryoo, Juan Carlos Niebles</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.10781">https://arxiv.org/abs/2601.10781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.10781">https://arxiv.org/pdf/2601.10781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.10781]] Future Optical Flow Prediction Improves Robot Control & Video Generation(https://arxiv.org/abs/2601.10781)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Future motion representations, such as optical flow, offer immense value for control and generative tasks. However, forecasting generalizable spatially dense motion representations remains a key challenge, and learning such forecasting from noisy, real-world data remains relatively unexplored. We introduce FOFPred, a novel language-conditioned optical flow forecasting model featuring a unified Vision-Language Model (VLM) and Diffusion architecture. This unique combination enables strong multimodal reasoning with pixel-level generative fidelity for future motion prediction. Our model is trained on web-scale human activity data-a highly scalable but unstructured source. To extract meaningful signals from this noisy video-caption data, we employ crucial data preprocessing techniques and our unified architecture with strong image pretraining. The resulting trained model is then extended to tackle two distinct downstream tasks in control and generation. Evaluations across robotic manipulation and video generation under language-driven settings establish the cross-domain versatility of FOFPred, confirming the value of a unified VLM-Diffusion architecture and scalable learning from diverse web data for future optical flow prediction.</li>
<li><strong>摘要：</strong>未来的运动表示，例如光流，为控制和生成任务提供了巨大的价值。然而，预测可推广的空间密集运动表示仍然是一个关键挑战，并且从嘈杂的现实世界数据中学习这种预测仍然相对未经探索。我们介绍 FOFPred，一种新颖的语言条件光流预测模型，具有统一的视觉语言模型 (VLM) 和扩散架构。这种独特的组合可以为未来的运动预测提供具有像素级生成保真度的强大多模态推理。我们的模型是根据网络规模的人类活动数据进行训练的，这是一种高度可扩展但非结构化的数据源。为了从这些嘈杂的视频字幕数据中提取有意义的信号，我们采用了关键的数据预处理技术和具有强大图像预训练的统一架构。然后将所得的训练模型扩展到处理控制和生成中两个不同的下游任务。在语言驱动设置下对机器人操作和视频生成的评估建立了 FOFPred 的跨域多功能性，确认了统一的 VLM-Diffusion 架构和从不同网络数据中进行可扩展学习以用于未来光流预测的价值。</li>
</ul>

<h3>Title: Digital Metabolism: Decoupling Logic from Facts via Regenerative Unlearning -- Towards a Pure Neural Logic Core</h3>
<ul>
<li><strong>Authors: </strong>Mengmeng Peng, Zhenyu Fang, He Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.10810">https://arxiv.org/abs/2601.10810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.10810">https://arxiv.org/pdf/2601.10810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.10810]] Digital Metabolism: Decoupling Logic from Facts via Regenerative Unlearning -- Towards a Pure Neural Logic Core(https://arxiv.org/abs/2601.10810)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) currently suffer from parameter entanglement, where general reasoning capabilities (logic) and specific factual knowledge (facts) exist in a superposition state within shared weights. This coupling leads to the "memory wall," where computational capacity is squandered on simulating retrieval, often resulting in hallucinations. In this paper, we propose "digital metabolism," a thermodynamic hypothesis suggesting that targeted forgetting is necessary for distilling a pure neural logic core. To validate this hypothesis, we introduce the Regenerative Logic-Core Protocol (RLCP), a dual-stream training framework that renders specific factual dependencies linearly undecodable via deep-layer gradient reversal. Applying RLCP to Qwen2.5-0.5B, we observe a distinct phase transition: the model achieves near-zero retention of targeted factual associations (Accuracy < 7%) while exhibiting changes consistent with an emergent "structural crystallization" effect. Empirical analysis on GSM8K reveals that the "metabolized" model spontaneously adopts chain-of-thought (CoT) scaffolding, which we interpret as compensating for the loss of direct associative recall (shifting from $O(1)$ recall to $O(N)$ reasoning). While the causal mechanism underlying this behavioral shift requires further investigation, our findings provide a dynamic weight-level counterpart to architectural innovations like DeepSeek's Engram, paving the way for modular "Neural CPU + Symbolic RAM" architectures.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）目前受到参数纠缠的困扰，其中一般推理能力（逻辑）和特定事实知识（事实）在共享权重内以叠加状态存在。这种耦合导致了“记忆墙”，计算能力被浪费在模拟检索上，通常会导致幻觉。在本文中，我们提出了“数字新陈代谢”，这是一种热力学假设，表明有针对性的遗忘对于提炼纯粹的神经逻辑核心是必要的。为了验证这一假设，我们引入了再生逻辑核心协议（RLCP），这是一种双流训练框架，可通过深层梯度反转使特定的事实依赖关系线性不可解码。将 RLCP 应用于 Qwen2.5-0.5B，我们观察到明显的相变：该模型实现了目标事实关联的近乎零保留（准确度 < 7%），同时表现出与出现的“结构结晶”效应一致的变化。对 GSM8K 的实证分析表明，“代谢”模型自发地采用了思想链 (CoT) 支架，我们将其解释为补偿直接联想回忆的损失（从 $O(1)$ 回忆转变为 $O(N)$ 推理）。虽然这种行为转变背后的因果机制需要进一步研究，但我们的研究结果为 DeepSeek 的 Engram 等架构创新提供了动态权重级别的对应物，为模块化“神经 CPU + 符号 RAM”架构铺平了道路。</li>
</ul>

<h3>Title: A Unified 3D Object Perception Framework for Real-Time Outside-In Multi-Camera Systems</h3>
<ul>
<li><strong>Authors: </strong>Yizhou Wang, Sameer Pusegaonkar, Yuxing Wang, Anqi Li, Vishal Kumar, Chetan Sethi, Ganapathy Aiyer, Yun He, Kartikay Thakkar, Swapnil Rathi, Bhushan Rupde, Zheng Tang, Sujit Biswas</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.10819">https://arxiv.org/abs/2601.10819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.10819">https://arxiv.org/pdf/2601.10819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.10819]] A Unified 3D Object Perception Framework for Real-Time Outside-In Multi-Camera Systems(https://arxiv.org/abs/2601.10819)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Accurate 3D object perception and multi-target multi-camera (MTMC) tracking are fundamental for the digital transformation of industrial infrastructure. However, transitioning "inside-out" autonomous driving models to "outside-in" static camera networks presents significant challenges due to heterogeneous camera placements and extreme occlusion. In this paper, we present an adapted Sparse4D framework specifically optimized for large-scale infrastructure environments. Our system leverages absolute world-coordinate geometric priors and introduces an occlusion-aware ReID embedding module to maintain identity stability across distributed sensor networks. To bridge the Sim2Real domain gap without manual labeling, we employ a generative data augmentation strategy using the NVIDIA COSMOS framework, creating diverse environmental styles that enhance the model's appearance-invariance. Evaluated on the AI City Challenge 2025 benchmark, our camera-only framework achieves a state-of-the-art HOTA of $45.22$. Furthermore, we address real-time deployment constraints by developing an optimized TensorRT plugin for Multi-Scale Deformable Aggregation (MSDA). Our hardware-accelerated implementation achieves a $2.15\times$ speedup on modern GPU architectures, enabling a single Blackwell-class GPU to support over 64 concurrent camera streams.</li>
<li><strong>摘要：</strong>准确的 3D 物体感知和多目标多摄像头 (MTMC) 跟踪是工业基础设施数字化转型的基础。然而，由于异构摄像头放置和极端遮挡，将“由内而外”的自动驾驶模型过渡到“由外而内”的静态摄像头网络面临着巨大的挑战。在本文中，我们提出了一种专门针对大规模基础设施环境进行优化的 Sparse4D 框架。我们的系统利用绝对世界坐标几何先验，并引入遮挡感知 ReID 嵌入模块来维持分布式传感器网络中的身份稳定性。为了在无需手动标记的情况下弥合 Sim2Real 域差距，我们采用了使用 NVIDIA COSMOS 框架的生成数据增强策略，创建了多种环境样式，从而增强了模型的外观不变性。根据 AI City Challenge 2025 基准进行评估，我们的仅摄像头框架实现了 45.22 美元的最先进 HOTA。此外，我们通过开发用于多尺度可变形聚合（MSDA）的优化 TensorRT 插件来解决实时部署限制。我们的硬件加速实现在现代 GPU 架构上实现了 2.15 美元的加速，使单个 Blackwell 级 GPU 能够支持超过 64 个并发摄像机流。</li>
</ul>

<h3>Title: Towards Reliable ML Feature Engineering via Planning in Constrained-Topology of LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Himanshu Thakur, Anusha Kamath, Anurag Muthyala, Dhwani Sanmukhani, Smruthi Mukund, Jay Katukuri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.10820">https://arxiv.org/abs/2601.10820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.10820">https://arxiv.org/pdf/2601.10820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.10820]] Towards Reliable ML Feature Engineering via Planning in Constrained-Topology of LLM Agents(https://arxiv.org/abs/2601.10820)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in code generation models have unlocked unprecedented opportunities for automating feature engineering, yet their adoption in real-world ML teams remains constrained by critical challenges: (i) the scarcity of datasets capturing the iterative and complex coding processes of production-level feature engineering, (ii) limited integration and personalization of widely used coding agents, such as CoPilot and Devin, with a team's unique tools, codebases, workflows, and practices, and (iii) suboptimal human-AI collaboration due to poorly timed or insufficient feedback. We address these challenges with a planner-guided, constrained-topology multi-agent framework that generates code for repositories in a multi-step fashion. The LLM-powered planner leverages a team's environment, represented as a graph, to orchestrate calls to available agents, generate context-aware prompts, and use downstream failures to retroactively correct upstream artifacts. It can request human intervention at critical steps, ensuring generated code is reliable, maintainable, and aligned with team expectations. On a novel in-house dataset, our approach achieves 38% and 150% improvement in the evaluation metric over manually crafted and unplanned workflows respectively. In practice, when building features for recommendation models serving over 120 million users, our approach has delivered real-world impact by reducing feature engineering cycles from three weeks to a single day.</li>
<li><strong>摘要：</strong>代码生成模型的最新进展为自动化特征工程带来了前所未有的机会，但它们在现实世界的机器学习团队中的采用仍然受到关键挑战的限制：(i) 捕获生产级特征工程的迭代和复杂编码过程的数据集的稀缺，(ii) 广泛使用的编码代理（例如 CoPilot 和 Devin）与团队独特的工具、代码库、工作流程和实践的集成和个性化有限，以及 (iii) 由于人类与人工智能协作不理想，反馈不及时或不足。我们通过规划器引导的约束拓扑多代理框架来应对这些挑战，该框架以多步骤方式为存储库生成代码。由 LLM 支持的规划器利用以图形表示的团队环境来协调对可用代理的调用，生成上下文感知提示，并使用下游故障来追溯纠正上游工件。它可以在关键步骤请求人工干预，确保生成的代码可靠、可维护并符合团队期望。在新颖的内部数据集上，我们的方法在评估指标上分别比手动制作和计划外的工作流程提高了 38% 和 150%。在实践中，当为超过 1.2 亿用户提供服务的推荐模型构建功能时，我们的方法通过将功能工程周期从三周缩短到一天，产生了现实世界的影响。</li>
</ul>

<h3>Title: AI-Guided Human-In-the-Loop Inverse Design of High Performance Engineering Structures</h3>
<ul>
<li><strong>Authors: </strong>Dat Quoc Ha, Md Ferdous Alam, Markus J. Buehler, Faez Ahmed, Josephine V. Carstensen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.10859">https://arxiv.org/abs/2601.10859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.10859">https://arxiv.org/pdf/2601.10859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.10859]] AI-Guided Human-In-the-Loop Inverse Design of High Performance Engineering Structures(https://arxiv.org/abs/2601.10859)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Inverse design tools such as Topology Optimization (TO) can achieve new levels of improvement for high-performance engineered structures. However, widespread use is hindered by high computational times and a black-box nature that inhibits user interaction. Human-in-the-loop TO approaches are emerging that integrate human intuition into the design generation process. However, these rely on the time-consuming bottleneck of iterative region selection for design modifications. To reduce the number of iterative trials, this contribution presents an AI co-pilot that uses machine learning to predict the user's preferred regions. The prediction model is configured as an image segmentation task with a U-Net architecture. It is trained on synthetic datasets where human preferences either identify the longest topological member or the most complex structural connection. The model successfully predicts plausible regions for modification and presents them to the user as AI recommendations. The human preference model demonstrates generalization across diverse and non-standard TO problems and exhibits emergent behavior outside the single-region selection training data. Demonstration examples show that the new human-in-the-loop TO approach that integrates the AI co-pilot can improve manufacturability or improve the linear buckling load by 39% while only increasing the total design time by 15 sec compared to conventional simplistic TO.</li>
<li><strong>摘要：</strong>拓扑优化 (TO) 等逆向设计工具可以将高性能工程结构的改进提高到新的水平。然而，高计算时间和抑制用户交互的黑盒性质阻碍了广泛使用。人机交互 TO 方法正在兴起，它将人类直觉整合到设计生成过程中。然而，这些依赖于设计修改的迭代区域选择的耗时瓶颈。为了减少迭代试验的次数，本贡献提出了一个人工智能副驾驶，它使用机器学习来预测用户的首选区域。预测模型被配置为具有 U-Net 架构的图像分割任务。它是在合成数据集上进行训练的，其中人类偏好识别最长的拓扑成员或最复杂的结构连接。该模型成功预测了可能需要修改的区域，并将其作为人工智能建议呈现给用户。人类偏好模型展示了对各种非标准 TO 问题的泛化，并展示了单区域选择训练数据之外的紧急行为。演示示例表明，与传统的简单 TO 相比，集成了 AI 副驾驶的新型人机交互 TO 方法可以提高可制造性或将线性屈曲载荷提高 39%，同时仅将总设计时间增加 15 秒。</li>
</ul>

<h3>Title: FrankenMotion: Part-level Human Motion Generation and Composition</h3>
<ul>
<li><strong>Authors: </strong>Chuqiao Li, Xianghui Xie, Yong Cao, Andreas Geiger, Gerard Pons-Moll</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.10909">https://arxiv.org/abs/2601.10909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.10909">https://arxiv.org/pdf/2601.10909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.10909]] FrankenMotion: Part-level Human Motion Generation and Composition(https://arxiv.org/abs/2601.10909)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Human motion generation from text prompts has made remarkable progress in recent years. However, existing methods primarily rely on either sequence-level or action-level descriptions due to the absence of fine-grained, part-level motion annotations. This limits their controllability over individual body parts. In this work, we construct a high-quality motion dataset with atomic, temporally-aware part-level text annotations, leveraging the reasoning capabilities of large language models (LLMs). Unlike prior datasets that either provide synchronized part captions with fixed time segments or rely solely on global sequence labels, our dataset captures asynchronous and semantically distinct part movements at fine temporal resolution. Based on this dataset, we introduce a diffusion-based part-aware motion generation framework, namely FrankenMotion, where each body part is guided by its own temporally-structured textual prompt. This is, to our knowledge, the first work to provide atomic, temporally-aware part-level motion annotations and have a model that allows motion generation with both spatial (body part) and temporal (atomic action) control. Experiments demonstrate that FrankenMotion outperforms all previous baseline models adapted and retrained for our setting, and our model can compose motions unseen during training. Our code and dataset will be publicly available upon publication.</li>
<li><strong>摘要：</strong>近年来，通过文本提示生成人体动作取得了显着的进展。然而，由于缺乏细粒度的部分级运动注释，现有方法主要依赖于序列级或动作级描述。这限制了他们对各个身体部位的控制。在这项工作中，我们利用大型语言模型（LLM）的推理能力，构建了一个具有原子的、时间感知的部分级文本注释的高质量运动数据集。与之前的数据集提供固定时间段的同步部分标题或仅依赖全局序列标签不同，我们的数据集以精细的时间分辨率捕获异步且语义上不同的部分运动。基于该数据集，我们引入了一种基于扩散的部位感知运动生成框架，即 FrankenMotion，其中每个身体部位都由其自己的时间结构文本提示引导。据我们所知，这是第一个提供原子的、时间感知的部分级运动注释的工作，并拥有一个允许通过空间（身体部位）和时间（原子动作）控制进行运动生成的模型。实验表明，FrankenMotion 优于之前针对我们的设置进行调整和重新训练的所有基线模型，并且我们的模型可以组成训练期间看不见的动作。我们的代码和数据集将在发布后公开。</li>
</ul>

<h3>Title: Multivariate LSTM-Based Forecasting for Renewable Energy: Enhancing Climate Change Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Farshid Kamrani, Kristen Schell</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.10961">https://arxiv.org/abs/2601.10961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.10961">https://arxiv.org/pdf/2601.10961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.10961]] Multivariate LSTM-Based Forecasting for Renewable Energy: Enhancing Climate Change Mitigation(https://arxiv.org/abs/2601.10961)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The increasing integration of renewable energy sources (RESs) into modern power systems presents significant opportunities but also notable challenges, primarily due to the inherent variability of RES generation. Accurate forecasting of RES generation is crucial for maintaining the reliability, stability, and economic efficiency of power system operations. Traditional approaches, such as deterministic methods and stochastic programming, frequently depend on representative scenarios generated through clustering techniques like K-means. However, these methods may fail to fully capture the complex temporal dependencies and non-linear patterns within RES data. This paper introduces a multivariate Long Short-Term Memory (LSTM)-based network designed to forecast RESs generation using their real-world historical data. The proposed model effectively captures long-term dependencies and interactions between different RESs, utilizing historical data from both local and neighboring areas to enhance predictive accuracy. In the case study, we showed that the proposed forecasting approach results in lower CO2 emissions, and a more reliable supply of electric loads.</li>
<li><strong>摘要：</strong>可再生能源 (RES) 越来越多地融入现代电力系统，带来了巨大的机遇，但也带来了显着的挑战，这主要是由于可再生能源发电固有的可变性。准确预测可再生能源发电量对于维持电力系统运行的可靠性、稳定性和经济效率至关重要。传统方法（例如确定性方法和随机编程）经常依赖于通过 K 均值等聚类技术生成的代表性场景。然而，这些方法可能无法完全捕获 RES 数据中复杂的时间依赖性和非线性模式。本文介绍了一种基于多元长短期记忆 (LSTM) 的网络，旨在使用真实世界的历史数据来预测 RES 的生成。所提出的模型有效地捕获了不同RES之间的长期依赖性和相互作用，利用来自当地和邻近地区的历史数据来提高预测准确性。在案例研究中，我们表明所提出的预测方法可以降低二氧化碳排放量，并提供更可靠的电力负载供应。</li>
</ul>

<h3>Title: Toward Adaptive Grid Resilience: A Gradient-Free Meta-RL Framework for Critical Load Restoration</h3>
<ul>
<li><strong>Authors: </strong>Zain ul Abdeen, Waris Gill, Ming Jin</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.10973">https://arxiv.org/abs/2601.10973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.10973">https://arxiv.org/pdf/2601.10973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.10973]] Toward Adaptive Grid Resilience: A Gradient-Free Meta-RL Framework for Critical Load Restoration(https://arxiv.org/abs/2601.10973)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generation</a></li>
<li><strong>Abstract: </strong>Restoring critical loads after extreme events demands adaptive control to maintain distribution-grid resilience, yet uncertainty in renewable generation, limited dispatchable resources, and nonlinear dynamics make effective restoration difficult. Reinforcement learning (RL) can optimize sequential decisions under uncertainty, but standard RL often generalizes poorly and requires extensive retraining for new outage configurations or generation patterns. We propose a meta-guided gradient-free RL (MGF-RL) framework that learns a transferable initialization from historical outage experiences and rapidly adapts to unseen scenarios with minimal task-specific tuning. MGF-RL couples first-order meta-learning with evolutionary strategies, enabling scalable policy search without gradient computation while accommodating nonlinear, constrained distribution-system dynamics. Experiments on IEEE 13-bus and IEEE 123-bus test systems show that MGF-RL outperforms standard RL, MAML-based meta-RL, and model predictive control across reliability, restoration speed, and adaptation efficiency under renewable forecast errors. MGF-RL generalizes to unseen outages and renewable patterns while requiring substantially fewer fine-tuning episodes than conventional RL. We also provide sublinear regret bounds that relate adaptation efficiency to task similarity and environmental variation, supporting the empirical gains and motivating MGF-RL for real-time load restoration in renewable-rich distribution grids.</li>
<li><strong>摘要：</strong>极端事件后恢复关键负荷需要自适应控制来维持配电网的弹性，但可再生能源发电的不确定性、有限的可调度资源和非线性动态使得有效恢复变得困难。强化学习 (RL) 可以在不确定性下优化顺序决策，但标准 RL 的泛化能力通常很差，并且需要针对新的停电配置或发电模式进行大量的再训练。我们提出了一种元引导的无梯度强化学习 (MGF-RL) 框架，该框架从历史中断经验中学习可转移的初始化，并以最少的特定于任务的调整快速适应未见过的场景。 MGF-RL 将一阶元学习与进化策略结合起来，无需梯度计算即可实现可扩展的策略搜索，同时适应非线性、受限的分布系统动态。在 IEEE 13 总线和 IEEE 123 总线测试系统上的实验表明，MGF-RL 在可再生预测误差下的可靠性、恢复速度和适应效率方面优于标准 RL、基于 MAML 的元 RL 和模型预测控制。 MGF-RL 可以推广到看不见的断电和可再生模式，同时比传统 RL 需要的微调次数少得多。我们还提供了次线性后悔界限，将适应效率与任务相似性和环境变化联系起来，支持经验收益并激励 MGF-RL 在可再生能源丰富的配电网中进行实时负载恢复。</li>
</ul>

<h3>Title: Your One-Stop Solution for AI-Generated Video Detection</h3>
<ul>
<li><strong>Authors: </strong>Long Ma, Zihao Xue, Yan Wang, Zhiyuan Yan, Jin Xu, Xiaorui Jiang, Haiyang Yu, Yong Liao, Zhen Bi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.11035">https://arxiv.org/abs/2601.11035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.11035">https://arxiv.org/pdf/2601.11035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.11035]] Your One-Stop Solution for AI-Generated Video Detection(https://arxiv.org/abs/2601.11035)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative modeling can create remarkably realistic synthetic videos, making it increasingly difficult for humans to distinguish them from real ones and necessitating reliable detection methods. However, two key limitations hinder the development of this field. \textbf{From the dataset perspective}, existing datasets are often limited in scale and constructed using outdated or narrowly scoped generative models, making it difficult to capture the diversity and rapid evolution of modern generative techniques. Moreover, the dataset construction process frequently prioritizes quantity over quality, neglecting essential aspects such as semantic diversity, scenario coverage, and technological representativeness. \textbf{From the benchmark perspective}, current benchmarks largely remain at the stage of dataset creation, leaving many fundamental issues and in-depth analysis yet to be systematically explored. Addressing this gap, we propose AIGVDBench, a benchmark designed to be comprehensive and representative, covering \textbf{31} state-of-the-art generation models and over \textbf{440,000} videos. By executing more than \textbf{1,500} evaluations on \textbf{33} existing detectors belonging to four distinct categories. This work presents \textbf{8 in-depth analyses} from multiple perspectives and identifies \textbf{4 novel findings} that offer valuable insights for future research. We hope this work provides a solid foundation for advancing the field of AI-generated video detection. Our benchmark is open-sourced at this https URL.</li>
<li><strong>摘要：</strong>生成模型的最新进展可以创建非常逼真的合成视频，使人类越来越难以将它们与真实视频区分开来，因此需要可靠的检测方法。然而，两个关键限制阻碍了该领域的发展。 \textbf{从数据集的角度来看}，现有的数据集通常规模有限，并且使用过时的或范围狭窄的生成模型构建，使得难以捕捉现代生成技术的多样性和快速发展。此外，数据集构建过程经常优先考虑数量而非质量，忽略了语义多样性、场景覆盖和技术代表性等基本方面。 \textbf{从基准测试的角度来看}，当前的基准测试很大程度上停留在数据集创建阶段，许多基本问题和深入分析有待系统探索。为了解决这一差距，我们提出了 AIGVDBench，这是一个旨在全面且具有代表性的基准测试，涵盖 \textbf{31} 最先进的生成模型和超过 \textbf{440,000} 视频。通过对属于四个不同类别的 \textbf{33} 现有检测器执行超过 \textbf{1,500} 次评估。这项工作从多个角度提出了 \textbf{8 深入分析}，并确定了 \textbf{4 新颖的发现}，为未来的研究提供了宝贵的见解。我们希望这项工作为推进人工智能生成视频检测领域奠定坚实的基础。我们的基准测试是在此 https URL 上开源的。</li>
</ul>

<h3>Title: PhysRVG: Physics-Aware Unified Reinforcement Learning for Video Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Qiyuan Zhang, Biao Gong, Shuai Tan, Zheng Zhang, Yujun Shen, Xing Zhu, Yuyuan Li, Kelu Yao, Chunhua Shen, Changqing Zou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.11087">https://arxiv.org/abs/2601.11087</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.11087">https://arxiv.org/pdf/2601.11087</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.11087]] PhysRVG: Physics-Aware Unified Reinforcement Learning for Video Generative Models(https://arxiv.org/abs/2601.11087)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Physical principles are fundamental to realistic visual simulation, but remain a significant oversight in transformer-based video generation. This gap highlights a critical limitation in rendering rigid body motion, a core tenet of classical mechanics. While computer graphics and physics-based simulators can easily model such collisions using Newton formulas, modern pretrain-finetune paradigms discard the concept of object rigidity during pixel-level global denoising. Even perfectly correct mathematical constraints are treated as suboptimal solutions (i.e., conditions) during model optimization in post-training, fundamentally limiting the physical realism of generated videos. Motivated by these considerations, we introduce, for the first time, a physics-aware reinforcement learning paradigm for video generation models that enforces physical collision rules directly in high-dimensional spaces, ensuring the physics knowledge is strictly applied rather than treated as conditions. Subsequently, we extend this paradigm to a unified framework, termed Mimicry-Discovery Cycle (MDcycle), which allows substantial fine-tuning while fully preserving the model's ability to leverage physics-grounded feedback. To validate our approach, we construct new benchmark PhysRVGBench and perform extensive qualitative and quantitative experiments to thoroughly assess its effectiveness.</li>
<li><strong>摘要：</strong>物理原理是现实视觉模拟的基础，但在基于变压器的视频生成中仍然是一个重要的监督。这一差距凸显了渲染刚体运动的关键限制，而刚体运动是经典力学的核心原则。虽然计算机图形和基于物理的模拟器可以使用牛顿公式轻松地对此类碰撞进行建模，但现代预训练微调范式在像素级全局去噪期间放弃了对象刚性的概念。在训练后的模型优化过程中，即使是完全正确的数学约束也会被视为次优解（即条件），从根本上限制了生成视频的物理真实感。出于这些考虑，我们首次引入了用于视频生成模型的物理感知强化学习范例，该范例直接在高维空间中强制执行物理碰撞规则，确保物理知识被严格应用而不是被视为条件。随后，我们将此范式扩展到一个统一的框架，称为拟态发现循环（MDcycle），它允许进行大量微调，同时完全保留模型利用基于物理的反馈的能力。为了验证我们的方法，我们构建了新的基准 PhysRVGBench 并进行了广泛的定性和定量实验以彻底评估其有效性。</li>
</ul>

<h3>Title: Optimized Algorithms for Text Clustering with LLM-Generated Constraints</h3>
<ul>
<li><strong>Authors: </strong>Chaoqi Jia, Weihong Wu, Longkun Guo, Zhigang Lu, Chao Chen, Kok-Leong Ong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.11118">https://arxiv.org/abs/2601.11118</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.11118">https://arxiv.org/pdf/2601.11118</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.11118]] Optimized Algorithms for Text Clustering with LLM-Generated Constraints(https://arxiv.org/abs/2601.11118)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Clustering is a fundamental tool that has garnered significant interest across a wide range of applications including text analysis. To improve clustering accuracy, many researchers have incorporated background knowledge, typically in the form of must-link and cannot-link constraints, to guide the clustering process. With the recent advent of large language models (LLMs), there is growing interest in improving clustering quality through LLM-based automatic constraint generation. In this paper, we propose a novel constraint-generation approach that reduces resource consumption by generating constraint sets rather than using traditional pairwise constraints. This approach improves both query efficiency and constraint accuracy compared to state-of-the-art methods. We further introduce a constrained clustering algorithm tailored to the characteristics of LLM-generated constraints. Our method incorporates a confidence threshold and a penalty mechanism to address potentially inaccurate constraints. We evaluate our approach on five text datasets, considering both the cost of constraint generation and the overall clustering performance. The results show that our method achieves clustering accuracy comparable to the state-of-the-art algorithms while reducing the number of LLM queries by more than 20 times.</li>
<li><strong>摘要：</strong>聚类是一种基本工具，在包括文本分析在内的广泛应用中引起了人们的极大兴趣。为了提高聚类准确性，许多研究人员结合了背景知识（通常以必须链接和不能链接约束的形式）来指导聚类过程。随着最近大型语言模型 (LLM) 的出现，人们越来越关注通过基于 LLM 的自动约束生成来提高聚类质量。在本文中，我们提出了一种新颖的约束生成方法，该方法通过生成约束集而不是使用传统的成对约束来减少资源消耗。与最先进的方法相比，这种方法提高了查询效率和约束准确性。我们进一步引入了一种针对 LLM 生成的约束特征的约束聚类算法。我们的方法结合了置信度阈值和惩罚机制来解决潜在的不准确约束。我们在五个文本数据集上评估我们的方法，同时考虑约束生成的成本和整体聚类性能。结果表明，我们的方法实现了与最先进算法相当的聚类精度，同时将 LLM 查询数量减少了 20 倍以上。</li>
</ul>

<h3>Title: Shape-morphing programming of soft materials on complex geometries via neural operator</h3>
<ul>
<li><strong>Authors: </strong>Lu Chen, Gengxiang Chen, Xu Liu, Jingyan Su, Xuhao Lyu, Lihui Wang, Yingguang Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.11126">https://arxiv.org/abs/2601.11126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.11126">https://arxiv.org/pdf/2601.11126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.11126]] Shape-morphing programming of soft materials on complex geometries via neural operator(https://arxiv.org/abs/2601.11126)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Shape-morphing soft materials can enable diverse target morphologies through voxel-level material distribution design, offering significant potential for various applications. Despite progress in basic shape-morphing design with simple geometries, achieving advanced applications such as conformal implant deployment or aerodynamic morphing requires accurate and diverse morphing designs on complex geometries, which remains challenging. Here, we present a Spectral and Spatial Neural Operator (S2NO), which enables high-fidelity morphing prediction on complex geometries. S2NO effectively captures global and local morphing behaviours on irregular computational domains by integrating Laplacian eigenfunction encoding and spatial convolutions. Combining S2NO with evolutionary algorithms enables voxel-level optimisation of material distributions for shape morphing programming on various complex geometries, including irregular-boundary shapes, porous structures, and thin-walled structures. Furthermore, the neural operator's discretisation-invariant property enables super-resolution material distribution design, further expanding the diversity and complexity of morphing design. These advancements significantly improve the efficiency and capability of programming complex shape morphing.</li>
<li><strong>摘要：</strong>变形软材料可以通过体素级材料分布设计实现不同的目标形态，为各种应用提供巨大的潜力。尽管在简单几何形状的基本形状变形设计方面取得了进展，但实现保形植入物部署或空气动力学变形等高级应用需要在复杂几何形状上进行准确和多样化的变形设计，这仍然具有挑战性。在这里，我们提出了光谱和空间神经算子（S2NO），它可以对复杂几何形状进行高保真变形预测。 S2NO 通过集成拉普拉斯特征函数编码和空间卷积，有效地捕获不规则计算域上的全局和局部变形行为。将 S2NO 与进化算法相结合，可以对材料分布进行体素级优化，以便对各种复杂几何形状（包括不规则边界形状、多孔结构和薄壁结构）进行形状变形编程。此外，神经算子的离散不变特性实现了超分辨率材料分布设计，进一步扩展了变形设计的多样性和复杂性。这些进步显着提高了复杂形状变形编程的效率和能力。</li>
</ul>

<h3>Title: Democratizing planetary-scale analysis: An ultra-lightweight Earth embedding database for accurate and flexible global land monitoring</h3>
<ul>
<li><strong>Authors: </strong>Shuang Chen, Jie Wang, Shuai Yuan, Jiayang Li, Yu Xia, Yuanhong Liao, Junbo Wei, Jincheng Yuan, Xiaoqing Xu, Xiaolin Zhu, Peng Zhu, Hongsheng Zhang, Yuyu Zhou, Haohuan Fu, Huabing Huang, Bin Chen, Fan Dai, Peng Gong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.11183">https://arxiv.org/abs/2601.11183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.11183">https://arxiv.org/pdf/2601.11183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.11183]] Democratizing planetary-scale analysis: An ultra-lightweight Earth embedding database for accurate and flexible global land monitoring(https://arxiv.org/abs/2601.11183)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The rapid evolution of satellite-borne Earth Observation (EO) systems has revolutionized terrestrial monitoring, yielding petabyte-scale archives. However, the immense computational and storage requirements for global-scale analysis often preclude widespread use, hindering planetary-scale studies. To address these barriers, we present Embedded Seamless Data (ESD), an ultra-lightweight, 30-m global Earth embedding database spanning the 25-year period from 2000 to 2024. By transforming high-dimensional, multi-sensor observations from the Landsat series (5, 7, 8, and 9) and MODIS Terra into information-dense, quantized latent vectors, ESD distills essential geophysical and semantic features into a unified latent space. Utilizing the ESDNet architecture and Finite Scalar Quantization (FSQ), the dataset achieves a transformative ~340-fold reduction in data volume compared to raw archives. This compression allows the entire global land surface for a single year to be encapsulated within approximately 2.4 TB, enabling decadal-scale global analysis on standard local workstations. Rigorous validation demonstrates high reconstructive fidelity (MAE: 0.0130; RMSE: 0.0179; CC: 0.8543). By condensing the annual phenological cycle into 12 temporal steps, the embeddings provide inherent denoising and a semantically organized space that outperforms raw reflectance in land-cover classification, achieving 79.74% accuracy (vs. 76.92% for raw fusion). With robust few-shot learning capabilities and longitudinal consistency, ESD provides a versatile foundation for democratizing planetary-scale research and advancing next-generation geospatial artificial intelligence.</li>
<li><strong>摘要：</strong>星载地球观测 (EO) 系统的快速发展彻底改变了地面监测，产生了 PB 级档案。然而，全球规模分析的巨大计算和存储需求往往阻碍了广泛使用，从而阻碍了行星规模的研究。为了解决这些障碍，我们提出了嵌入式无缝数据 (ESD)，这是一个超轻量级、30 米的全球地球嵌入数据库，跨越 2000 年至 2024 年的 25 年时间。通过将 Landsat 系列（5、7、8 和 9）和 MODIS Terra 的高维多传感器观测数据转换为信息密集的量化潜在向量，ESD 将基本的地球物理和语义特征提炼为统一的潜在向量。空间。该数据集利用 ESDNet 架构和有限标量量化 (FSQ)，与原始档案相比，数据量减少了约 340 倍。这种压缩使得一年内的整个全球陆地表面被封装在大约 2.4 TB 内，从而能够在标准本地工作站上进行十年尺度的全球分析。严格的验证证明了高重建保真度（MAE：0.0130；RMSE：0.0179；CC：0.8543）。通过将年度物候周期压缩为 12 个时间步骤，嵌入提供了固有的去噪和语义组织的空间，其性能优于土地覆盖分类中的原始反射率，实现了 79.74% 的准确度（原始融合的准确度为 76.92%）。凭借强大的少样本学习能力和纵向一致性，ESD 为全球范围的研究民主化和推进下一代地理空间人工智能提供了多功能基础。</li>
</ul>

<h3>Title: TimeMar: Multi-Scale Autoregressive Modeling for Unconditional Time Series Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiangyu Xu, Qingsong Zhong, Jilin Hu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.11184">https://arxiv.org/abs/2601.11184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.11184">https://arxiv.org/pdf/2601.11184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.11184]] TimeMar: Multi-Scale Autoregressive Modeling for Unconditional Time Series Generation(https://arxiv.org/abs/2601.11184)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative modeling offers a promising solution to data scarcity and privacy challenges in time series analysis. However, the structural complexity of time series, characterized by multi-scale temporal patterns and heterogeneous components, remains insufficiently addressed. In this work, we propose a structure-disentangled multiscale generation framework for time series. Our approach encodes sequences into discrete tokens at multiple temporal resolutions and performs autoregressive generation in a coarse-to-fine manner, thereby preserving hierarchical dependencies. To tackle structural heterogeneity, we introduce a dual-path VQ-VAE that disentangles trend and seasonal components, enabling the learning of semantically consistent latent representations. Additionally, we present a guidance-based reconstruction strategy, where coarse seasonal signals are utilized as priors to guide the reconstruction of fine-grained seasonal patterns. Experiments on six datasets show that our approach produces higher-quality time series than existing methods. Notably, our model achieves strong performance with a significantly reduced parameter count and exhibits superior capability in generating high-quality long-term sequences. Our implementation is available at this https URL.</li>
<li><strong>摘要：</strong>生成建模为时间序列分析中的数据稀缺和隐私挑战提供了一种有前景的解决方案。然而，以多尺度时间模式和异构成分为特征的时间序列的结构复杂性仍然没有得到充分解决。在这项工作中，我们提出了一种结构解缠的时间序列多尺度生成框架。我们的方法以多个时间分辨率将序列编码为离散标记，并以从粗到细的方式执行自回归生成，从而保留层次依赖性。为了解决结构异质性，我们引入了双路径 VQ-VAE，它可以解开趋势和季节性成分，从而能够学习语义一致的潜在表示。此外，我们提出了一种基于指导的重建策略，其中粗略的季节性信号被用作先验，以指导细粒度季节性模式的重建。对六个数据集的实验表明，我们的方法比现有方法产生更高质量的时间序列。值得注意的是，我们的模型通过显着减少参数数量实现了强大的性能，并在生成高质量长期序列方面表现出卓越的能力。我们的实现可以通过此 https URL 获得。</li>
</ul>

<h3>Title: ATATA: One Algorithm to Align Them All</h3>
<ul>
<li><strong>Authors: </strong>Boyi Pang, Savva Ignatyev, Vladimir Ippolitov, Ramil Khafizov, Yurii Melnik, Oleg Voynov, Maksim Nakhodnov, Aibek Alanov, Xiaopeng Fan, Peter Wonka, Evgeny Burnaev</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.11194">https://arxiv.org/abs/2601.11194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.11194">https://arxiv.org/pdf/2601.11194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.11194]] ATATA: One Algorithm to Align Them All(https://arxiv.org/abs/2601.11194)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We suggest a new multi-modal algorithm for joint inference of paired structurally aligned samples with Rectified Flow models. While some existing methods propose a codependent generation process, they do not view the problem of joint generation from a structural alignment perspective. Recent work uses Score Distillation Sampling to generate aligned 3D models, but SDS is known to be time-consuming, prone to mode collapse, and often provides cartoonish results. By contrast, our suggested approach relies on the joint transport of a segment in the sample space, yielding faster computation at inference time. Our approach can be built on top of an arbitrary Rectified Flow model operating on the structured latent space. We show the applicability of our method to the domains of image, video, and 3D shape generation using state-of-the-art baselines and evaluate it against both editing-based and joint inference-based competing approaches. We demonstrate a high degree of structural alignment for the sample pairs obtained with our method and a high visual quality of the samples. Our method improves the state-of-the-art for image and video generation pipelines. For 3D generation, it is able to show comparable quality while working orders of magnitude faster.</li>
<li><strong>摘要：</strong>我们提出了一种新的多模态算法，用于使用整流流模型对结构对齐的配对样本进行联合推理。虽然一些现有方法提出了相互依赖的生成过程，但它们没有从结构对齐的角度来看待联合生成问题。最近的工作使用分数蒸馏采样来生成对齐的 3D 模型，但众所周知 SDS 非常耗时，容易出现模式崩溃，并且通常会提供卡通化的结果。相比之下，我们建议的方法依赖于样本空间中片段的联合传输，从而在推理时产生更快的计算。我们的方法可以建立在在结构化潜在空间上运行的任意整流流模型之上。我们使用最先进的基线展示了我们的方法在图像、视频和 3D 形状生成领域的适用性，并针对基于编辑和基于联合推理的竞争方法对其进行了评估。我们展示了通过我们的方法获得的样本对的高度结构对齐以及样本的高视觉质量。我们的方法提高了图像和视频生成管道的最先进水平。对于 3D 生成，它能够显示可比的质量，同时工作速度要快几个数量级。</li>
</ul>

<h3>Title: FAQ: Mitigating Quantization Error via Regenerating Calibration Data with Family-Aware Quantization</h3>
<ul>
<li><strong>Authors: </strong>Haiyang Xiao, Weiqing Li, Jinyue Guo, Guochao Jiang, Guohua Liu, Yuewei Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.11200">https://arxiv.org/abs/2601.11200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.11200">https://arxiv.org/pdf/2601.11200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.11200]] FAQ: Mitigating Quantization Error via Regenerating Calibration Data with Family-Aware Quantization(https://arxiv.org/abs/2601.11200)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Although post-training quantization (PTQ) provides an efficient numerical compression scheme for deploying large language models (LLMs) on resource-constrained devices, the representativeness and universality of calibration data remain a core bottleneck in determining the accuracy of quantization parameters. Traditional PTQ methods typically rely on limited samples, making it difficult to capture the activation distribution during the inference phase, leading to biases in quantization parameters. To address this, we propose \textbf{FAQ} (Family-Aware Quantization), a calibration data regeneration framework that leverages prior knowledge from LLMs of the same family to generate high-fidelity calibration samples. Specifically, FAQ first inputs the original calibration samples into a larger LLM from the same family as the target model, regenerating a series of high-fidelity calibration data using a highly consistent knowledge system. Subsequently, this data, carrying Chain-of-Thought reasoning and conforming to the expected activation distribution, undergoes group competition under expert guidance to select the best samples, which are then re-normalized to enhance the effectiveness of standard PTQ. Experiments on multiple model series, including Qwen3-8B, show that FAQ reduces accuracy loss by up to 28.5\% compared to the baseline with original calibration data, demonstrating its powerful potential and contribution.</li>
<li><strong>摘要：</strong>尽管训练后量化（PTQ）为在资源受限的设备上部署大型语言模型（LLM）提供了一种有效的数值压缩方案，但校准数据的代表性和普遍性仍然是确定量化参数准确性的核心瓶颈。传统的PTQ方法通常依赖于有限的样本，因此很难捕获推理阶段的激活分布，从而导致量化参数出现偏差。为了解决这个问题，我们提出了 \textbf{FAQ}（Family-Aware Quantization），这是一种校准数据重新生成框架，它利用同一家族的 LLM 的先验知识来生成高保真校准样本。具体来说，FAQ首先将原始校准样本输入到与目标模型同族的更大的LLM中，利用高度一致的知识体系重新生成一系列高保真校准数据。随后，这些带有思想链推理、符合预期激活分布的数据，在专家指导下进行群体竞争，选出最佳样本，然后重新归一化，以增强标准PTQ的有效性。在包括 Qwen3-8B 在内的多个模型系列上的实验表明，与原始校准数据的基线相比，FAQ 降低了高达 28.5% 的精度损失，展示了其强大的潜力和贡献。</li>
</ul>

<h3>Title: SAMannot: A Memory-Efficient, Local, Open-source Framework for Interactive Video Instance Segmentation based on SAM2</h3>
<ul>
<li><strong>Authors: </strong>Gergely Dinya, András Gelencsér, Krisztina Kupán, Clemens Küpper, Kristóf Karacs, Anna Gelencsér-Horváth</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.11301">https://arxiv.org/abs/2601.11301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.11301">https://arxiv.org/pdf/2601.11301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.11301]] SAMannot: A Memory-Efficient, Local, Open-source Framework for Interactive Video Instance Segmentation based on SAM2(https://arxiv.org/abs/2601.11301)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Current research workflows for precise video segmentation are often forced into a compromise between labor-intensive manual curation, costly commercial platforms, and/or privacy-compromising cloud-based services. The demand for high-fidelity video instance segmentation in research is often hindered by the bottleneck of manual annotation and the privacy concerns of cloud-based tools. We present SAMannot, an open-source, local framework that integrates the Segment Anything Model 2 (SAM2) into a human-in-the-loop workflow. To address the high resource requirements of foundation models, we modified the SAM2 dependency and implemented a processing layer that minimizes computational overhead and maximizes throughput, ensuring a highly responsive user interface. Key features include persistent instance identity management, an automated ``lock-and-refine'' workflow with barrier frames, and a mask-skeletonization-based auto-prompting mechanism. SAMannot facilitates the generation of research-ready datasets in YOLO and PNG formats alongside structured interaction logs. Verified through animal behavior tracking use-cases and subsets of the LVOS and DAVIS benchmark datasets, the tool provides a scalable, private, and cost-effective alternative to commercial platforms for complex video annotation tasks.</li>
<li><strong>摘要：</strong>当前精确视频分割的研究工作流程常常被迫在劳动密集型的手动管理、昂贵的商业平台和/或损害隐私的基于云的服务之间做出妥协。研究中对高保真视频实例分割的需求常常受到手动注释的瓶颈和基于云的工具的隐私问题的阻碍。我们推出 SAMannot，这是一个开源本地框架，它将 Segment Anything Model 2 (SAM2) 集成到人机交互工作流程中。为了解决基础模型的高资源需求，我们修改了 SAM2 依赖项并实现了一个处理层，该处理层可最大限度地减少计算开销并最大限度地提高吞吐量，从而确保高度响应的用户界面。主要功能包括持久实例身份管理、带有屏障框架的自动化“锁定和优化”工作流程以及基于掩码骨架化的自动提示机制。 SAMannot 有助于生成 YOLO 和 PNG 格式的研究就绪数据集以及结构化交互日志。该工具通过动物行为跟踪用例以及 LVOS 和 DAVIS 基准数据集的子集进行验证，为复杂视频注释任务的商业平台提供了可扩展、私密且经济高效的替代方案。</li>
</ul>

<h3>Title: Unlocking the Potentials of Retrieval-Augmented Generation for Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chuanyue Yu, Jiahui Wang, Yuhan Li, Heng Chang, Ge Lan, Qingyun Sun, Jia Li, Jianxin Li, Ziwei Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.11342">https://arxiv.org/abs/2601.11342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.11342">https://arxiv.org/pdf/2601.11342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.11342]] Unlocking the Potentials of Retrieval-Augmented Generation for Diffusion Language Models(https://arxiv.org/abs/2601.11342)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion Language Models (DLMs) have recently demonstrated remarkable capabilities in natural language processing tasks. However, the potential of Retrieval-Augmented Generation (RAG), which shows great successes for enhancing large language models (LLMs), has not been well explored, due to the fundamental difference between LLM and DLM decoding. To fill this critical gap, we systematically test the performance of DLMs within the RAG framework. Our findings reveal that DLMs coupled with RAG show promising potentials with stronger dependency on contextual information, but suffer from limited generation precision. We identify a key underlying issue: Response Semantic Drift (RSD), where the generated answer progressively deviates from the query's original semantics, leading to low precision content. We trace this problem to the denoising strategies in DLMs, which fail to maintain semantic alignment with the query throughout the iterative denoising process. To address this, we propose Semantic-Preserving REtrieval-Augmented Diffusion (SPREAD), a novel framework that introduces a query-relevance-guided denoising strategy. By actively guiding the denoising trajectory, SPREAD ensures the generation remains anchored to the query's semantics and effectively suppresses drift. Experimental results demonstrate that SPREAD significantly enhances the precision and effectively mitigates RSD of generated answers within the RAG framework.</li>
<li><strong>摘要：</strong>扩散语言模型（DLM）最近在自然语言处理任务中表现出了非凡的能力。然而，由于 LLM 和 DLM 解码之间的根本区别，检索增强生成 (RAG) 在增强大型语言模型 (LLM) 方面取得了巨大成功，其潜力尚未得到充分开发。为了填补这一关键空白，我们系统地测试了 RAG 框架内 DLM 的性能。我们的研究结果表明，DLM 与 RAG 结合显示出对上下文信息有更强依赖性的良好潜力，但其生成精度有限。我们确定了一个关键的潜在问题：响应语义漂移（RSD），其中生成的答案逐渐偏离查询的原始语义，导致内容精度低。我们将此问题追溯到 DLM 中的去噪策略，该策略无法在整个迭代去噪过程中保持与查询的语义对齐。为了解决这个问题，我们提出了语义保留检索增强扩散（SPREAD），这是一种引入查询相关性引导的去噪策略的新颖框架。通过主动引导去噪轨迹，SPREAD 确保生成保持锚定于查询的语义并有效抑制漂移。实验结果表明，SPREAD 显着提高了 RAG 框架内生成答案的精度并有效降低了 RSD。</li>
</ul>

<h3>Title: SUG-Occ: An Explicit Semantics and Uncertainty Guided Sparse Learning Framework for Real-Time 3D Occupancy Prediction</h3>
<ul>
<li><strong>Authors: </strong>Hanlin Wu, Pengfei Lin, Ehsan Javanmardi, Nanren Bao, Bo Qian, Hao Si, Manabu Tsukada</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.11396">https://arxiv.org/abs/2601.11396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.11396">https://arxiv.org/pdf/2601.11396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.11396]] SUG-Occ: An Explicit Semantics and Uncertainty Guided Sparse Learning Framework for Real-Time 3D Occupancy Prediction(https://arxiv.org/abs/2601.11396)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>As autonomous driving moves toward full scene understanding, 3D semantic occupancy prediction has emerged as a crucial perception task, offering voxel-level semantics beyond traditional detection and segmentation paradigms. However, such a refined representation for scene understanding incurs prohibitive computation and memory overhead, posing a major barrier to practical real-time deployment. To address this, we propose SUG-Occ, an explicit Semantics and Uncertainty Guided Sparse Learning Enabled 3D Occupancy Prediction Framework, which exploits the inherent sparsity of 3D scenes to reduce redundant computation while maintaining geometric and semantic completeness. Specifically, we first utilize semantic and uncertainty priors to suppress projections from free space during view transformation while employing an explicit unsigned distance encoding to enhance geometric consistency, producing a structurally consistent sparse 3D representation. Secondly, we design an cascade sparse completion module via hyper cross sparse convolution and generative upsampling to enable efficiently coarse-to-fine reasoning. Finally, we devise an object contextual representation (OCR) based mask decoder that aggregates global semantic context from sparse features and refines voxel-wise predictions via lightweight query-context interactions, avoiding expensive attention operations over volumetric features. Extensive experiments on SemanticKITTI benchmark demonstrate that the proposed approach outperforms the baselines, achieving a 7.34/% improvement in accuracy and a 57.8\% gain in efficiency.</li>
<li><strong>摘要：</strong>随着自动驾驶向全场景理解迈进，3D 语义占用预测已成为一项关键的感知任务，提供超越传统检测和分割范式的体素级语义。然而，这种用于场景理解的精细表示会带来过高的计算和内存开销，对实际的实时部署构成主要障碍。为了解决这个问题，我们提出了 SUG-Occ，一种显式语义和不确定性引导稀疏学习的 3D 占用预测框架，它利用 3D 场景固有的稀疏性来减少冗余计算，同时保持几何和语义完整性。具体来说，我们首先利用语义和不确定性先验来抑制视图变换期间自由空间的投影，同时采用显式无符号距离编码来增强几何一致性，从而产生结构一致的稀疏 3D 表示。其次，我们通过超交叉稀疏卷积和生成上采样设计了一个级联稀疏完成模块，以实现高效的从粗到精的推理。最后，我们设计了一种基于对象上下文表示（OCR）的掩码解码器，它从稀疏特征聚合全局语义上下文，并通过轻量级查询上下文交互细化体素预测，避免对体积特征进行昂贵的注意操作。 SemanticKITTI 基准测试的大量实验表明，所提出的方法优于基线，准确率提高了 7.34%，效率提高了 57.8%。</li>
</ul>

<h3>Title: GenDA: Generative Data Assimilation on Complex Urban Areas via Classifier-Free Diffusion Guidance</h3>
<ul>
<li><strong>Authors: </strong>Francisco Giral, Álvaro Manzano, Ignacio Gómez, Ricardo Vinuesa, Soledad Le Clainche</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.11440">https://arxiv.org/abs/2601.11440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.11440">https://arxiv.org/pdf/2601.11440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.11440]] GenDA: Generative Data Assimilation on Complex Urban Areas via Classifier-Free Diffusion Guidance(https://arxiv.org/abs/2601.11440)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Urban wind flow reconstruction is essential for assessing air quality, heat dispersion, and pedestrian comfort, yet remains challenging when only sparse sensor data are available. We propose GenDA, a generative data assimilation framework that reconstructs high-resolution wind fields on unstructured meshes from limited observations. The model employs a multiscale graph-based diffusion architecture trained on computational fluid dynamics (CFD) simulations and interprets classifier-free guidance as a learned posterior reconstruction mechanism: the unconditional branch learns a geometry-aware flow prior, while the sensor-conditioned branch injects observational constraints during sampling. This formulation enables obstacle-aware reconstruction and generalization across unseen geometries, wind directions, and mesh resolutions without retraining. We consider both sparse fixed sensors and trajectory-based observations using the same reconstruction procedure. When evaluated against supervised graph neural network (GNN) baselines and classical reduced-order data assimilation methods, GenDA reduces the relative root-mean-square error (RRMSE) by 25-57% and increases the structural similarity index (SSIM) by 23-33% across the tested meshes. Experiments are conducted on Reynolds-averaged Navier-Stokes (RANS) simulations of a real urban neighbourhood in Bristol, United Kingdom, at a characteristic Reynolds number of $\mathrm{Re}\approx2\times10^{7}$, featuring complex building geometry and irregular terrain. The proposed framework provides a scalable path toward generative, geometry-aware data assimilation for environmental monitoring in complex domains.</li>
<li><strong>摘要：</strong>城市风流重建对于评估空气质量、散热和行人舒适度至关重要，但在只有稀疏的传感器数据可用时仍然具有挑战性。我们提出了 GenDA，一种生成数据同化框架，可以根据有限的观测在非结构化网格上重建高分辨率风场。该模型采用基于计算流体动力学 (CFD) 模拟的多尺度图扩散架构，并将无分类器引导解释为学习后重建机制：无条件分支先学习几何感知流，而传感器条件分支在采样期间注入观察约束。这种公式能够在不可见的几何形状、风向和网格分辨率上进行障碍物感知重建和泛化，而无需重新训练。我们使用相同的重建程序来考虑稀疏固定传感器和基于轨迹的观测。当根据监督图神经网络 (GNN) 基线和经典降阶数据同化方法进行评估时，GenDA 将测试网格的相对均方根误差 (RRMSE) 降低了 25-57%，并将结构相似性指数 (SSIM) 提高了 23-33%。实验对英国布里斯托尔的真实城市社区进行雷诺平均纳维斯托克斯 (RANS) 模拟，特征雷诺数为 $\mathrm{Re}\approx2\times10^{7}$，具有复杂的建筑几何形状和不规则地形。所提出的框架为复杂领域环境监测的生成、几何感知数据同化提供了一条可扩展的路径。</li>
</ul>

<h3>Title: When Are Two Scores Better Than One? Investigating Ensembles of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Raphaël Razafindralambo, Rémy Sun, Frédéric Precioso, Damien Garreau, Pierre-Alexandre Mattei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, math.ST, stat.ME, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.11444">https://arxiv.org/abs/2601.11444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.11444">https://arxiv.org/pdf/2601.11444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.11444]] When Are Two Scores Better Than One? Investigating Ensembles of Diffusion Models(https://arxiv.org/abs/2601.11444)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion models now generate high-quality, diverse samples, with an increasing focus on more powerful models. Although ensembling is a well-known way to improve supervised models, its application to unconditional score-based diffusion models remains largely unexplored. In this work we investigate whether it provides tangible benefits for generative modelling. We find that while ensembling the scores generally improves the score-matching loss and model likelihood, it fails to consistently enhance perceptual quality metrics such as FID on image datasets. We confirm this observation across a breadth of aggregation rules using Deep Ensembles, Monte Carlo Dropout, on CIFAR-10 and FFHQ. We attempt to explain this discrepancy by investigating possible explanations, such as the link between score estimation and image quality. We also look into tabular data through random forests, and find that one aggregation strategy outperforms the others. Finally, we provide theoretical insights into the summing of score models, which shed light not only on ensembling but also on several model composition techniques (e.g. guidance).</li>
<li><strong>摘要：</strong>扩散模型现在可以生成高质量、多样化的样本，并且越来越关注更强大的模型。尽管集成是改进监督模型的一种众所周知的方法，但其在无条件基于评分的扩散模型中的应用在很大程度上仍未得到探索。在这项工作中，我们研究它是否为生成建模提供了切实的好处。我们发现，虽然集成分数通常可以改善分数匹配损失和模型可能性，但它无法持续增强感知质量指标，例如图像数据集上的 FID。我们在 CIFAR-10 和 FFHQ 上使用 Deep Ensembles、Monte Carlo Dropout 在广泛的聚合规则中证实了这一观察结果。我们试图通过调查可能的解释来解释这种差异，例如分数估计和图像质量之间的联系。我们还通过随机森林研究表格数据，发现一种聚合策略优于其他策略。最后，我们提供了对分数模型求和的理论见解，这不仅揭示了集成，而且还揭示了几种模型组合技术（例如指导）。</li>
</ul>

<h3>Title: Generative Scenario Rollouts for End-to-End Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Rajeev Yasarla, Deepti Hegde, Shizhong Han, Hsin-Pai Cheng, Yunxiao Shi, Meysam Sadeghigooghari, Shweta Mahajan, Apratim Bhattacharyya, Litian Liu, Risheek Garrepalli, Thomas Svantesson, Fatih Porikli, Hong Cai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.11475">https://arxiv.org/abs/2601.11475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.11475">https://arxiv.org/pdf/2601.11475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.11475]] Generative Scenario Rollouts for End-to-End Autonomous Driving(https://arxiv.org/abs/2601.11475)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Vision-Language-Action (VLA) models are emerging as highly effective planning models for end-to-end autonomous driving systems. However, current works mostly rely on imitation learning from sparse trajectory annotations and under-utilize their potential as generative models. We propose Generative Scenario Rollouts (GeRo), a plug-and-play framework for VLA models that jointly performs planning and generation of language-grounded future traffic scenes through an autoregressive rollout strategy. First, a VLA model is trained to encode ego vehicle and agent dynamics into latent tokens under supervision from planning, motion, and language tasks, facilitating text-aligned generation. Next, GeRo performs language-conditioned autoregressive generation. Given multi-view images, a scenario description, and ego-action questions, it generates future latent tokens and textual responses to guide long-horizon rollouts. A rollout-consistency loss stabilizes predictions using ground truth or pseudo-labels, mitigating drift and preserving text-action alignment. This design enables GeRo to perform temporally consistent, language-grounded rollouts that support long-horizon reasoning and multi-agent planning. On Bench2Drive, GeRo improves driving score and success rate by +15.7 and +26.2, respectively. By integrating reinforcement learning with generative rollouts, GeRo achieves state-of-the-art closed-loop and open-loop performance, demonstrating strong zero-shot robustness. These results highlight the promise of generative, language-conditioned reasoning as a foundation for safer and more interpretable end-to-end autonomous driving.</li>
<li><strong>摘要：</strong>视觉-语言-动作（VLA）模型正在成为端到端自动驾驶系统的高效规划模型。然而，当前的工作主要依赖于稀疏轨迹注释的模仿学习，并没有充分利用它们作为生成模型的潜力。我们提出了生成场景推出（GeRo），这是一种 VLA 模型的即插即用框架，通过自回归推出策略联合执行基于语言的未来交通场景的规划和生成。首先，训练 VLA 模型，在规划、运动和语言任务的监督下将自我车辆和代理动态编码为潜在标记，从而促进文本对齐生成。接下来，GeRo 执行语言条件自回归生成。给定多视图图像、场景描述和自我行动问题，它会生成未来的潜在标记和文本响应来指导长期部署。推出一致性损失使用地面事实或伪标签稳定预测，减轻漂移并保持文本动作对齐。这种设计使 GeRo 能够执行时间一致、基于语言的部署，支持长期推理和多代理规划。在 Bench2Drive 上，GeRo 将驾驶分数和成功率分别提高了 +15.7 和 +26.2。通过将强化学习与生成式部署相结合，GeRo 实现了最先进的闭环和开环性能，展示了强大的零样本鲁棒性。这些结果凸显了生成式、语言条件推理作为更安全、更可解释的端到端自动驾驶基础的前景。</li>
</ul>

<h3>Title: ShapeR: Robust Conditional 3D Shape Generation from Casual Captures</h3>
<ul>
<li><strong>Authors: </strong>Yawar Siddiqui, Duncan Frost, Samir Aroudj, Armen Avetisyan, Henry Howard-Jenkins, Daniel DeTone, Pierre Moulon, Qirui Wu, Zhengqin Li, Julian Straub, Richard Newcombe, Jakob Engel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.11514">https://arxiv.org/abs/2601.11514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.11514">https://arxiv.org/pdf/2601.11514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.11514]] ShapeR: Robust Conditional 3D Shape Generation from Casual Captures(https://arxiv.org/abs/2601.11514)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in 3D shape generation have achieved impressive results, but most existing methods rely on clean, unoccluded, and well-segmented inputs. Such conditions are rarely met in real-world scenarios. We present ShapeR, a novel approach for conditional 3D object shape generation from casually captured sequences. Given an image sequence, we leverage off-the-shelf visual-inertial SLAM, 3D detection algorithms, and vision-language models to extract, for each object, a set of sparse SLAM points, posed multi-view images, and machine-generated captions. A rectified flow transformer trained to effectively condition on these modalities then generates high-fidelity metric 3D shapes. To ensure robustness to the challenges of casually captured data, we employ a range of techniques including on-the-fly compositional augmentations, a curriculum training scheme spanning object- and scene-level datasets, and strategies to handle background clutter. Additionally, we introduce a new evaluation benchmark comprising 178 in-the-wild objects across 7 real-world scenes with geometry annotations. Experiments show that ShapeR significantly outperforms existing approaches in this challenging setting, achieving an improvement of 2.7x in Chamfer distance compared to state of the art.</li>
<li><strong>摘要：</strong>3D 形状生成领域的最新进展取得了令人印象深刻的成果，但大多数现有方法都依赖于干净、无遮挡且分段良好的输入。在现实场景中很少满足这样的条件。我们提出了 ShapeR，这是一种根据随意捕获的序列生成条件 3D 对象形状的新颖方法。给定图像序列，我们利用现成的视觉惯性 SLAM、3D 检测算法和视觉语言模型，为每个对象提取一组稀疏 SLAM 点、摆出的多视图图像和机器生成的说明。经过训练可以有效调节这些模态的整流流量变压器随后会生成高保真度的公制 3D 形状。为了确保应对随意捕获的数据挑战的鲁棒性，我们采用了一系列技术，包括动态合成增强、涵盖对象和场景级数据集的课程培训方案以及处理背景杂波的策略。此外，我们还引入了一个新的评估基准，其中包含 7 个真实场景中的 178 个野外物体，并带有几何注释。实验表明，ShapeR 在这种具有挑战性的环境中显着优于现有方法，与现有技术相比，倒角距离提高了 2.7 倍。</li>
</ul>

<h3>Title: UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation</h3>
<ul>
<li><strong>Authors: </strong>Ruiheng Zhang, Jingfeng Yao, Huangxuan Zhao, Hao Yan, Xiao He, Lei Chen, Zhou Wei, Yong Luo, Zengmao Wang, Lefei Zhang, Dacheng Tao, Bo Du</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.11522">https://arxiv.org/abs/2601.11522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.11522">https://arxiv.org/pdf/2601.11522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.11522]] UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation(https://arxiv.org/abs/2601.11522)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite recent progress, medical foundation models still struggle to unify visual understanding and generation, as these tasks have inherently conflicting goals: semantic abstraction versus pixel-level reconstruction. Existing approaches, typically based on parameter-shared autoregressive architectures, frequently lead to compromised performance in one or both tasks. To address this, we present UniX, a next-generation unified medical foundation model for chest X-ray understanding and generation. UniX decouples the two tasks into an autoregressive branch for understanding and a diffusion branch for high-fidelity generation. Crucially, a cross-modal self-attention mechanism is introduced to dynamically guide the generation process with understanding features. Coupled with a rigorous data cleaning pipeline and a multi-stage training strategy, this architecture enables synergistic collaboration between tasks while leveraging the strengths of diffusion models for superior generation. On two representative benchmarks, UniX achieves a 46.1% improvement in understanding performance (Micro-F1) and a 24.2% gain in generation quality (FD-RadDino), using only a quarter of the parameters of LLM-CXR. By achieving performance on par with task-specific models, our work establishes a scalable paradigm for synergistic medical image understanding and generation. Codes and models are available at this https URL.</li>
<li><strong>摘要：</strong>尽管最近取得了进展，医学基础模型仍然难以统一视觉理解和生成，因为这些任务具有本质上相互冲突的目标：语义抽象与像素级重建。现有的方法通常基于参数共享的自回归架构，经常导致一项或两项任务的性能受到影响。为了解决这个问题，我们推出了 UniX，这是一种用于胸部 X 射线理解和生成的下一代统一医学基础模型。 Unix 将这两个任务解耦为一个用于理解的自回归分支和一个用于高保真生成的扩散分支。至关重要的是，引入了跨模式自注意力机制，通过理解特征来动态指导生成过程。加上严格的数据清理管道和多阶段训练策略，该架构可以实现任务之间的协同协作，同时利用扩散模型的优势来实现卓越的生成。在两个代表性基准测试中，UniX 仅使用 LLM-CXR 四分之一的参数，在理解性能 (Micro-F1) 方面实现了 46.1% 的提升，在生成质量 (FD-RadDino) 方面实现了 24.2% 的提升。通过实现与特定任务模型相当的性能，我们的工作为协同医学图像理解和生成建立了可扩展的范例。代码和模型可从此 https URL 获取。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
