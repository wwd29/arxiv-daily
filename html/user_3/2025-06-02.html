<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-06-02</h1>
<h3>Title: Mamba Integrated with Physics Principles Masters Long-term Chaotic System Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Chang Liu, Bohao Zhao, Jingtao Ding, Huandong Wang, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23863">https://arxiv.org/abs/2505.23863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23863">https://arxiv.org/pdf/2505.23863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23863]] Mamba Integrated with Physics Principles Masters Long-term Chaotic System Forecasting(https://arxiv.org/abs/2505.23863)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Long-term forecasting of chaotic systems from short-term observations remains a fundamental and underexplored challenge due to the intrinsic sensitivity to initial conditions and the complex geometry of strange attractors. Existing approaches often rely on long-term training data or focus on short-term sequence correlations, struggling to maintain predictive stability and dynamical coherence over extended horizons. We propose PhyxMamba, a novel framework that integrates a Mamba-based state-space model with physics-informed principles to capture the underlying dynamics of chaotic systems. By reconstructing the attractor manifold from brief observations using time-delay embeddings, PhyxMamba extracts global dynamical features essential for accurate forecasting. Our generative training scheme enables Mamba to replicate the physical process, augmented by multi-token prediction and attractor geometry regularization for physical constraints, enhancing prediction accuracy and preserving key statistical invariants. Extensive evaluations on diverse simulated and real-world chaotic systems demonstrate that PhyxMamba delivers superior long-term forecasting and faithfully captures essential dynamical invariants from short-term data. This framework opens new avenues for reliably predicting chaotic systems under observation-scarce conditions, with broad implications across climate science, neuroscience, epidemiology, and beyond. Our code is open-source at this https URL.</li>
<li><strong>摘要：</strong>由于对初始条件的固有敏感性以及奇怪的吸引子的复杂几何形状，因此对短期观察的长期预测仍然是一个基本且不受欢迎的挑战。现有的方法通常依赖于长期培训数据或专注于短期序列相关性，努力维持预测稳定性和在扩展视野上的动态连贯性。我们提出了phyxmamba，这是一个新颖的框架，该框架将基于曼巴的状态空间模型与物理知识的原理集成在一起，以捕获混沌系统的基本动力学。通过使用时间延迟嵌入的简短观察重建吸引者歧管，phyxMamba提取了全局动力学特征，对于准确的预测至关重要。我们的生成培训方案使Mamba能够复制物理过程，并通过多键预测和吸引子几何正规化的物理约束增强，增强了预测准确性并保留了关键的统计不变性。对各种模拟和现实世界混乱系统的广泛评估表明，PhyxMamba提供了卓越的长期预测，并忠实地捕获了短期数据中的基本动力不变剂。该框架为在观察筛查条件下可靠地预测混乱系统开辟了新的途径，在气候科学，神经科学，流行病学及其他方面具有广泛的影响。我们的代码在此HTTPS URL上是开源的。</li>
</ul>

<h3>Title: MaCP: Minimal yet Mighty Adaptation via Hierarchical Cosine Projection</h3>
<ul>
<li><strong>Authors: </strong>Yixian Shen, Qi Bi, Jia-Hong Huang, Hongyi Zhu, Andy D. Pimentel, Anuj Pathania</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23870">https://arxiv.org/abs/2505.23870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23870">https://arxiv.org/pdf/2505.23870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23870]] MaCP: Minimal yet Mighty Adaptation via Hierarchical Cosine Projection(https://arxiv.org/abs/2505.23870)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present a new adaptation method MaCP, Minimal yet Mighty adaptive Cosine Projection, that achieves exceptional performance while requiring minimal parameters and memory for fine-tuning large foundation models. Its general idea is to exploit the superior energy compaction and decorrelation properties of cosine projection to improve both model efficiency and accuracy. Specifically, it projects the weight change from the low-rank adaptation into the discrete cosine space. Then, the weight change is partitioned over different levels of the discrete cosine spectrum, and each partition's most critical frequency components are selected. Extensive experiments demonstrate the effectiveness of MaCP across a wide range of single-modality tasks, including natural language understanding, natural language generation, text summarization, as well as multi-modality tasks such as image classification and video understanding. MaCP consistently delivers superior accuracy, significantly reduced computational complexity, and lower memory requirements compared to existing alternatives.</li>
<li><strong>摘要：</strong>我们提出了一种新的适应方法MACP，即最小而又强大的自适应余弦投影，在需要最小的参数和记忆中，以实现出色的性能，以进行微调大型基础模型。它的一般思想是利用余弦投影的优质能量压实和去相关的特性，以提高模型效率和准确性。具体而言，它将重量从低级别适应的变化变化为离散的余弦空间。然后，重量变化在离散余弦谱的不同级别上进行分区，并选择了每个分区最关键的频率组件。广泛的实验证明了MACP在各种单模式任务中的有效性，包括自然语言理解，自然语言的产生，文本摘要以及多模式的任务，例如图像分类和视频理解。与现有替代方案相比，MACP始终提供卓越的准确性，显着降低计算复杂性和降低的内存需求。</li>
</ul>

<h3>Title: A comparative analysis of a neural network with calculated weights and a neural network with random generation of weights based on the training dataset size</h3>
<ul>
<li><strong>Authors: </strong>Polad Geidarov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23876">https://arxiv.org/abs/2505.23876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23876">https://arxiv.org/pdf/2505.23876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23876]] A comparative analysis of a neural network with calculated weights and a neural network with random generation of weights based on the training dataset size(https://arxiv.org/abs/2505.23876)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The paper discusses the capabilities of multilayer perceptron neural networks implementing metric recognition methods, for which the values of the weights are calculated analytically by formulas. Comparative experiments in training a neural network with pre-calculated weights and with random initialization of weights on different sizes of the MNIST training dataset are carried out. The results of the experiments show that a multilayer perceptron with pre-calculated weights can be trained much faster and is much more robust to the reduction of the training dataset.</li>
<li><strong>摘要：</strong>本文讨论了实施度量识别方法的多层感知神经网络的功能，为此，通过公式分析了权重的值。在训练具有预计量权重的神经网络和不同尺寸的MNIST训练数据集上的权重的随机初始化中，进行了比较实验。实验的结果表明，具有预算权重的多层感知器可以更快地训练，并且对于减少训练数据集的功能更加可靠。</li>
</ul>

<h3>Title: Generating Fit Check Videos with a Handheld Camera</h3>
<ul>
<li><strong>Authors: </strong>Bowei Chen, Brian Curless, Ira Kemelmacher-Shlizerman, Steven M. Seitz</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23886">https://arxiv.org/abs/2505.23886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23886">https://arxiv.org/pdf/2505.23886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23886]] Generating Fit Check Videos with a Handheld Camera(https://arxiv.org/abs/2505.23886)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Self-captured full-body videos are popular, but most deployments require mounted cameras, carefully-framed shots, and repeated practice. We propose a more convenient solution that enables full-body video capture using handheld mobile devices. Our approach takes as input two static photos (front and back) of you in a mirror, along with an IMU motion reference that you perform while holding your mobile phone, and synthesizes a realistic video of you performing a similar target motion. We enable rendering into a new scene, with consistent illumination and shadows. We propose a novel video diffusion-based model to achieve this. Specifically, we propose a parameter-free frame generation strategy, as well as a multi-reference attention mechanism, that effectively integrate appearance information from both the front and back selfies into the video diffusion model. Additionally, we introduce an image-based fine-tuning strategy to enhance frame sharpness and improve the generation of shadows and reflections, achieving a more realistic human-scene composition.</li>
<li><strong>摘要：</strong>自捕获的全身视频很受欢迎，但是大多数部署都需要安装的摄像头，精心打架的镜头和重复的练习。我们提出了一种更方便的解决方案，该解决方案可以使用手持移动设备允许全身视频捕获。我们的方法将您的两张静态照片（正面和背面）作为镜子的输入，以及您在握住手机时执行的IMU运动参考，并合成了您执行类似目标运动的现实视频。我们以一致的照明和阴影启用渲染效果。我们提出了一个基于视频扩散的新型模型来实现这一目标。具体而言，我们提出了一种无参数的框架生成策略以及一种多参考注意机制，该策略有效地将前后自拍照的外观信息整合到视频扩散模型中。此外，我们还引入了一种基于图像的微调策略，以增强框架清晰度并改善阴影和反射的产生，从而实现更现实的人类场景组成。</li>
</ul>

<h3>Title: Cora: Correspondence-aware image editing using few step diffusion</h3>
<ul>
<li><strong>Authors: </strong>Amirhossein Almohammadi, Aryan Mikaeili, Sauradip Nag, Negar Hassanpour, Andrea Tagliasacchi, Ali Mahdavi-Amiri</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23907">https://arxiv.org/abs/2505.23907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23907">https://arxiv.org/pdf/2505.23907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23907]] Cora: Correspondence-aware image editing using few step diffusion(https://arxiv.org/abs/2505.23907)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Image editing is an important task in computer graphics, vision, and VFX, with recent diffusion-based methods achieving fast and high-quality results. However, edits requiring significant structural changes, such as non-rigid deformations, object modifications, or content generation, remain challenging. Existing few step editing approaches produce artifacts such as irrelevant texture or struggle to preserve key attributes of the source image (e.g., pose). We introduce Cora, a novel editing framework that addresses these limitations by introducing correspondence-aware noise correction and interpolated attention maps. Our method aligns textures and structures between the source and target images through semantic correspondence, enabling accurate texture transfer while generating new content when necessary. Cora offers control over the balance between content generation and preservation. Extensive experiments demonstrate that, quantitatively and qualitatively, Cora excels in maintaining structure, textures, and identity across diverse edits, including pose changes, object addition, and texture refinements. User studies confirm that Cora delivers superior results, outperforming alternatives.</li>
<li><strong>摘要：</strong>图像编辑是计算机图形，视觉和VFX的重要任务，最近基于扩散的方法可实现快速和高质量的结果。但是，需要重大结构变化的编辑，例如非刚性变形，对象修改或内容产生，仍然具有挑战性。现有的几个步骤编辑方法会产生诸如无关的纹理或努力保留源图像的关键属性（例如姿势）之类的文物。我们介绍了Cora，这是一个新颖的编辑框架，通过引入对应感知噪声校正和插值注意图来解决这些局限性。我们的方法通过语义对应关系使源和目标图像之间的纹理和结构保持一致，在必要时在生成新内容的同时，可以准确地纹理传输。 Cora可以控制内容产生和保存之间的平衡。广泛的实验表明，在定量和定性上，Cora在维持各种编辑的结构，纹理和身份方面表现出色，包括姿势变化，对象添加和纹理细化。用户研究证实，Cora提供了卓越的结果，表现优于替代方案。</li>
</ul>

<h3>Title: Simplifying Bayesian Optimization Via In-Context Direct Optimum Sampling</h3>
<ul>
<li><strong>Authors: </strong>Gustavo Sutter Pessurno de Carvalho, Mohammed Abdulrahman, Hao Wang, Sriram Ganapathi Subramanian, Marc St-Aubin, Sharon O'Sullivan, Lawrence Wan, Luis Ricardez-Sandoval, Pascal Poupart, Agustinus Kristiadi</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23913">https://arxiv.org/abs/2505.23913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23913">https://arxiv.org/pdf/2505.23913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23913]] Simplifying Bayesian Optimization Via In-Context Direct Optimum Sampling(https://arxiv.org/abs/2505.23913)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The optimization of expensive black-box functions is ubiquitous in science and engineering. A common solution to this problem is Bayesian optimization (BO), which is generally comprised of two components: (i) a surrogate model and (ii) an acquisition function, which generally require expensive re-training and optimization steps at each iteration, respectively. Although recent work enabled in-context surrogate models that do not require re-training, virtually all existing BO methods still require acquisition function maximization to select the next observation, which introduces many knobs to tune, such as Monte Carlo samplers and multi-start optimizers. In this work, we propose a completely in-context, zero-shot solution for BO that does not require surrogate fitting or acquisition function optimization. This is done by using a pre-trained deep generative model to directly sample from the posterior over the optimum point. We show that this process is equivalent to Thompson sampling and demonstrate the capabilities and cost-effectiveness of our foundation model on a suite of real-world benchmarks. We achieve an efficiency gain of more than 35x in terms of wall-clock time when compared with Gaussian process-based BO, enabling efficient parallel and distributed BO, e.g., for high-throughput optimization.</li>
<li><strong>摘要：</strong>昂贵的黑盒功能的优化在科学和工程中无处不在。解决此问题的一个常见解决方案是贝叶斯优化（BO），通常由两个组成部分组成：（i）替代模型和（ii）采集函数，通常需要在每次迭代处分别需要昂贵的重新训练和优化步骤。尽管最近的工作启用了不需要重新训练的文本替代模型，但实际上所有现有的BO方法仍然需要最大化的获取功能才能选择下一个观察值，该观察值引入了许多旋钮以调整调音，例如Monte Carlo Samplers和Multi-Start Optimizer。在这项工作中，我们为BO提出了一个完全的零击解决方案，该解决方案不需要替代拟合或采集功能优化。这是通过使用预先训练的深生成模型直接从最佳点的后端进行采样来完成的。我们表明，这个过程等同于汤普森采样，并在一系列现实世界的基准中展示了我们基础模型的能力和成本效益。与基于高斯过程的BO相比，我们在壁锁时的效率增益超过35倍，从而实现有效的并行和分布式BO，例如，以进行高通量优化。</li>
</ul>

<h3>Title: Position: The Future of Bayesian Prediction Is Prior-Fitted</h3>
<ul>
<li><strong>Authors: </strong>Samuel Müller, Arik Reuter, Noah Hollmann, David Rügamer, Frank Hutter</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23947">https://arxiv.org/abs/2505.23947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23947">https://arxiv.org/pdf/2505.23947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23947]] Position: The Future of Bayesian Prediction Is Prior-Fitted(https://arxiv.org/abs/2505.23947)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Training neural networks on randomly generated artificial datasets yields Bayesian models that capture the prior defined by the dataset-generating distribution. Prior-data Fitted Networks (PFNs) are a class of methods designed to leverage this insight. In an era of rapidly increasing computational resources for pre-training and a near stagnation in the generation of new real-world data in many applications, PFNs are poised to play a more important role across a wide range of applications. They enable the efficient allocation of pre-training compute to low-data scenarios. Originally applied to small Bayesian modeling tasks, the field of PFNs has significantly expanded to address more complex domains and larger datasets. This position paper argues that PFNs and other amortized inference approaches represent the future of Bayesian inference, leveraging amortized learning to tackle data-scarce problems. We thus believe they are a fruitful area of research. In this position paper, we explore their potential and directions to address their current limitations.</li>
<li><strong>摘要：</strong>在随机生成的人工数据集上训练神经网络会产生贝叶斯模型，以捕获数据集生成分布所定义的先前模型。 Prif-DATA拟合网络（PFN）是一类旨在利用这种见解的方法。在一个迅速增加的计算资源以进行预训练的时代，并且在许多应用程序中生成新的现实世界数据中几乎停滞不前，PFN有望在广泛的应用中扮演更重要的角色。它们可以有效地分配预训练对低数据表情况。 PFN的领域最初应用于小型贝叶斯建模任务，已大大扩展，以解决更复杂的域和较大的数据集。该立场论文认为，PFN和其他摊销推理方法代表了贝叶斯推论的未来，利用摊销学习来解决数据筛选问题。因此，我们认为它们是一个富有成果的研究领域。在该职位论文中，我们探讨了他们解决当前局限性的潜力和方向。</li>
</ul>

<h3>Title: TSENOR: Highly-Efficient Algorithm for Finding Transposable N:M Sparse Masks</h3>
<ul>
<li><strong>Authors: </strong>Xiang Meng, Mehdi Makni, Rahul Mazumder</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.23949">https://arxiv.org/abs/2505.23949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.23949">https://arxiv.org/pdf/2505.23949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.23949]] TSENOR: Highly-Efficient Algorithm for Finding Transposable N:M Sparse Masks(https://arxiv.org/abs/2505.23949)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Network pruning reduces the computational requirements of large neural networks, with N:M sparsity -- retaining only N out of every M consecutive weights -- offering a compelling balance between compressed model quality and hardware acceleration. However, N:M sparsity only accelerates forward-pass computations, as N:M patterns are not preserved during matrix transposition, limiting efficiency during training where both passes are computationally intensive. While transposable N:M sparsity has been proposed to address this limitation, existing methods for finding transposable N:M sparse masks either fail to scale to large models or are restricted to M=4 which results in suboptimal compression-accuracy trade-off. We introduce an efficient solver for transposable N:M masks that scales to billion-parameter models. We formulate mask generation as optimal transport problems and solve through entropy regularization and Dykstra's algorithm, followed by a rounding procedure. Our tensor-based implementation exploits GPU parallelism, achieving up to 100x speedup with only 1-10% error compared to existing methods. Our approach can be integrated with layer-wise N:M pruning frameworks including Wanda, SparseGPT and ALPS to produce transposable N:M sparse models with arbitrary N:M values. Experiments show that LLaMA3.2-8B with transposable 16:32 sparsity maintains performance close to its standard N:M counterpart and outperforms standard 2:4 sparse model, showing the practical value of our approach.</li>
<li><strong>摘要：</strong>网络修剪会减少大型神经网络的计算要求，其中N：m稀疏性 - 仅保留每个连续的重量中的n个 - 在压缩模型质量和硬件加速度之间提供了令人信服的平衡。但是，n：m的稀疏性仅加速前向计算，因为在矩阵转置过程中未保留n：m模式，这限制了两次通过的训练期间的效率。尽管已提出了可替座的n：m稀疏性来解决此限制，但现有的可转座n：m稀疏掩码的方法要么无法扩展到大型模型，要么仅限于M = 4，从而导致了次优压缩 - 准确性的权衡。我们引入了一个有效的求解器，用于可转座的n：m掩模，该掩膜扩展到数十亿个参数模型。我们将面膜生成作为最佳传输问题，并通过熵正则化和Dykstra的算法解决，然后进行圆形程序。我们基于张量的实现利用了GPU并行性，与现有方法相比，达到100倍的速度，仅1-10％的误差。我们的方法可以与层的N：M修剪框架（包括Wanda，SparseGpt和Alps）进行集成，以产生具有任意N：M值的可转座N：M稀疏模型。实验表明，具有转座的Llama3.2-8b 16:32稀疏性保持绩效接近其标准n：m对应物，并且胜过标准2：4稀疏模型，显示了我们方法的实际价值。</li>
</ul>

<h3>Title: DGIQA: Depth-guided Feature Attention and Refinement for Generalizable Image Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Vaishnav Ramesh, Junliang Liu, Haining Wang, Md Jahidul Islam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24002">https://arxiv.org/abs/2505.24002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24002">https://arxiv.org/pdf/2505.24002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24002]] DGIQA: Depth-guided Feature Attention and Refinement for Generalizable Image Quality Assessment(https://arxiv.org/abs/2505.24002)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>A long-held challenge in no-reference image quality assessment (NR-IQA) learning from human subjective perception is the lack of objective generalization to unseen natural distortions. To address this, we integrate a novel Depth-Guided cross-attention and refinement (Depth-CAR) mechanism, which distills scene depth and spatial features into a structure-aware representation for improved NR-IQA. This brings in the knowledge of object saliency and relative contrast of the scene for more discriminative feature learning. Additionally, we introduce the idea of TCB (Transformer-CNN Bridge) to fuse high-level global contextual dependencies from a transformer backbone with local spatial features captured by a set of hierarchical CNN (convolutional neural network) layers. We implement TCB and Depth-CAR as multimodal attention-based projection functions to select the most informative features, which also improve training time and inference efficiency. Experimental results demonstrate that our proposed DGIQA model achieves state-of-the-art (SOTA) performance on both synthetic and authentic benchmark datasets. More importantly, DGIQA outperforms SOTA models on cross-dataset evaluations as well as in assessing natural image distortions such as low-light effects, hazy conditions, and lens flares.</li>
<li><strong>摘要：</strong>从人类主观感知中学习的无参考图像质量评估（NR-IQA）中的长期挑战是缺乏客观的概括来看不见自然扭曲。为了解决这个问题，我们将新颖的深度引导的交叉注意和改进（深度车）机制整合在一起，该机制将场景深度和空间特征提炼成一个结构感知的表示，以改善NR-IQA。这带来了对物体显着性和场景相对对比的了解，以进行更歧视的特征学习。此外，我们介绍了TCB（变压器-CNN桥）的想法，以将来自变压器主链的高级全局上下文依赖性融合，并具有由一组层次CNN（卷积神经网络）层捕获的局部空间特征。我们将TCB和深度卡车作为基于多模式的投影功能来选择最有用的功能，从而提高训练时间和推理效率。实验结果表明，我们提出的DGIQA模型在合成和正宗基准数据集上都达到了最新的（SOTA）性能。更重要的是，DGIQA在跨数据库评估以及评估自然图像扭曲（例如低光效应，朦胧的条件和镜片）上的表现优于SOTA模型。</li>
</ul>

<h3>Title: Multi-Group Proportional Representation for Text-to-Image Models</h3>
<ul>
<li><strong>Authors: </strong>Sangwon Jung, Alex Oesterling, Claudio Mayrink Verdun, Sajani Vithana, Taesup Moon, Flavio P. Calmon</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24023">https://arxiv.org/abs/2505.24023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24023">https://arxiv.org/pdf/2505.24023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24023]] Multi-Group Proportional Representation for Text-to-Image Models(https://arxiv.org/abs/2505.24023)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) generative models can create vivid, realistic images from textual descriptions. As these models proliferate, they expose new concerns about their ability to represent diverse demographic groups, propagate stereotypes, and efface minority populations. Despite growing attention to the "safe" and "responsible" design of artificial intelligence (AI), there is no established methodology to systematically measure and control representational harms in image generation. This paper introduces a novel framework to measure the representation of intersectional groups in images generated by T2I models by applying the Multi-Group Proportional Representation (MPR) metric. MPR evaluates the worst-case deviation of representation statistics across given population groups in images produced by a generative model, allowing for flexible and context-specific measurements based on user requirements. We also develop an algorithm to optimize T2I models for this metric. Through experiments, we demonstrate that MPR can effectively measure representation statistics across multiple intersectional groups and, when used as a training objective, can guide models toward a more balanced generation across demographic groups while maintaining generation quality.</li>
<li><strong>摘要：</strong>文本对图像（T2I）生成模型可以从文本描述中创建生动，逼真的图像。随着这些模型的繁殖，他们揭示了对代表不同人口群体，传播刻板印象和埃德少数群体的能力的新关注。尽管人们对人工智能（AI）的“安全”和“负责任的”设计的关注度越来越多，但尚无既定方法来系统地衡量和控制图像产生中的代表性危害。本文介绍了一个新颖的框架，以通过应用多组比例表示（MPR）度量来衡量T2I模型产生的图像中相交组的表示。 MPR评估了由生成模型产生的图像中给定的人群群体中的表示统计数据的最严重偏差，从而允许根据用户需求进行灵活和特定于上下文的测量。我们还开发了一种算法来优化该指标的T2I模型。通过实验，我们证明了MPR可以有效地测量多个相互组的表示统计数据，并且在用作训练目标时，可以指导模型，同时跨人群群体平衡，同时保持发电质量。</li>
</ul>

<h3>Title: ComposeAnything: Composite Object Priors for Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Zeeshan Khan, Shizhe Chen, Cordelia Schmid</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24086">https://arxiv.org/abs/2505.24086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24086">https://arxiv.org/pdf/2505.24086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24086]] ComposeAnything: Composite Object Priors for Text-to-Image Generation(https://arxiv.org/abs/2505.24086)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating images from text involving complex and novel object arrangements remains a significant challenge for current text-to-image (T2I) models. Although prior layout-based methods improve object arrangements using spatial constraints with 2D layouts, they often struggle to capture 3D positioning and sacrifice quality and coherence. In this work, we introduce ComposeAnything, a novel framework for improving compositional image generation without retraining existing T2I models. Our approach first leverages the chain-of-thought reasoning abilities of LLMs to produce 2.5D semantic layouts from text, consisting of 2D object bounding boxes enriched with depth information and detailed captions. Based on this layout, we generate a spatial and depth aware coarse composite of objects that captures the intended composition, serving as a strong and interpretable prior that replaces stochastic noise initialization in diffusion-based T2I models. This prior guides the denoising process through object prior reinforcement and spatial-controlled denoising, enabling seamless generation of compositional objects and coherent backgrounds, while allowing refinement of inaccurate priors. ComposeAnything outperforms state-of-the-art methods on the T2I-CompBench and NSR-1K benchmarks for prompts with 2D/3D spatial arrangements, high object counts, and surreal compositions. Human evaluations further demonstrate that our model generates high-quality images with compositions that faithfully reflect the text.</li>
<li><strong>摘要：</strong>从涉及复杂和新颖对象布置的文本中生成图像仍然是当前文本对图像（T2I）模型的重大挑战。尽管先前基于布局的方法使用具有2D布局的空间约束来改善对象布置，但他们经常难以捕获3D定位，牺牲质量和连贯性。在这项工作中，我们介绍了Composeanything，这是一个新的框架，用于改善构图形象的生成而无需重新培训现有T2I模型。我们的方法首先利用LLM的经过思考的推理能力从文本中产生2.5d语义布局，由2D对象边界框组成，并具有深度信息和详细的字幕。基于这种布局，我们生成了一个空间和深度意识到的对象的粗略复合物，该对象捕获了预期的组合物，作为强大而可解释的先验，可以在基于扩散的T2I模型中代替随机噪声初始化。此前，通过对象事先加强和空间对照的denoinging来指导deno的过程，从而实现了无缝生成的组成对象和相干背景，同时允许不准确的先验进行完善。对于具有2D/3D空间排列，高对象计数和超现实组成的提示，CompoSeanything的表现优于T2i-CompBench和NSR-1K基准上的最先进方法。人类评估进一步表明，我们的模型以忠实地反映文本的构图产生高质量的图像。</li>
</ul>

<h3>Title: Autoregressive regularized score-based diffusion models for multi-scenarios fluid flow prediction</h3>
<ul>
<li><strong>Authors: </strong>Wilfried Genuist, Éric Savin, Filippo Gatti, Didier Clouteau</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.flu-dyn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24145">https://arxiv.org/abs/2505.24145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24145">https://arxiv.org/pdf/2505.24145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24145]] Autoregressive regularized score-based diffusion models for multi-scenarios fluid flow prediction(https://arxiv.org/abs/2505.24145)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Building on recent advances in scientific machine learning and generative modeling for computational fluid dynamics, we propose a conditional score-based diffusion model designed for multi-scenarios fluid flow prediction. Our model integrates an energy constraint rooted in the statistical properties of turbulent flows, improving prediction quality with minimal training, while enabling efficient sampling at low cost. The method features a simple and general architecture that requires no problem-specific design, supports plug-and-play enhancements, and enables fast and flexible solution generation. It also demonstrates an efficient conditioning mechanism that simplifies training across different scenarios without demanding a redesign of existing models. We further explore various stochastic differential equation formulations to demonstrate how thoughtful design choices enhance performance. We validate the proposed methodology through extensive experiments on complex fluid dynamics datasets encompassing a variety of flow regimes and configurations. Results demonstrate that our model consistently achieves stable, robust, and physically faithful predictions, even under challenging turbulent conditions. With properly tuned parameters, it achieves accurate results across multiple scenarios while preserving key physical and statistical properties. We present a comprehensive analysis of stochastic differential equation impact and discuss our approach across diverse fluid mechanics tasks.</li>
<li><strong>摘要：</strong>在计算流体动力学的科学机器学习和生成建模的最新进展的基础上，我们提出了一种基于有条件的分数扩散模型，该模型设计用于多阶段流体流动流量预测。我们的模型集成了植根于湍流的统计特性的能量约束，通过最小的训练提高了预测质量，同时以低成本实现有效的采样。该方法具有简单而通用的体系结构，不需要特定问题的设计，支持插入式增强功能，并可以快速，灵活地生成。它还展示了一种有效的条件机制，该机制简化了不同情况下的训练，而无需重新设计现有模型。我们进一步探索各种随机微分方程公式，以证明周到的设计选择如何增强性能。我们通过对涵盖各种流动机制和配置的复杂流体动力学数据集进行了广泛的实验来验证提出的方法。结果表明，即使在充满挑战的动荡条件下，我们的模型也始终达到稳定，健壮和身体忠实的预测。通过正确调整的参数，它可以在多种情况下实现准确的结果，同时保留关键的物理和统计属性。我们对随机微分方程影响进行了全面分析，并讨论了我们跨不同流体力学任务的方法。</li>
</ul>

<h3>Title: Towards a Generalizable Bimanual Foundation Policy via Flow-based Video Prediction</h3>
<ul>
<li><strong>Authors: </strong>Chenyou Fan, Fangzheng Yan, Chenjia Bai, Jiepeng Wang, Chi Zhang, Zhen Wang, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24156">https://arxiv.org/abs/2505.24156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24156">https://arxiv.org/pdf/2505.24156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24156]] Towards a Generalizable Bimanual Foundation Policy via Flow-based Video Prediction(https://arxiv.org/abs/2505.24156)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Learning a generalizable bimanual manipulation policy is extremely challenging for embodied agents due to the large action space and the need for coordinated arm movements. Existing approaches rely on Vision-Language-Action (VLA) models to acquire bimanual policies. However, transferring knowledge from single-arm datasets or pre-trained VLA models often fails to generalize effectively, primarily due to the scarcity of bimanual data and the fundamental differences between single-arm and bimanual manipulation. In this paper, we propose a novel bimanual foundation policy by fine-tuning the leading text-to-video models to predict robot trajectories and training a lightweight diffusion policy for action generation. Given the lack of embodied knowledge in text-to-video models, we introduce a two-stage paradigm that fine-tunes independent text-to-flow and flow-to-video models derived from a pre-trained text-to-video model. Specifically, optical flow serves as an intermediate variable, providing a concise representation of subtle movements between images. The text-to-flow model predicts optical flow to concretize the intent of language instructions, and the flow-to-video model leverages this flow for fine-grained video prediction. Our method mitigates the ambiguity of language in single-stage text-to-video prediction and significantly reduces the robot-data requirement by avoiding direct use of low-level actions. In experiments, we collect high-quality manipulation data for real dual-arm robot, and the results of simulation and real-world experiments demonstrate the effectiveness of our method.</li>
<li><strong>摘要：</strong>由于较大的动作空间和对协调的手臂运动的需求，学习可推广的双人操纵政策对于具体的代理人来说是极具挑战性的。现有的方法依靠视觉语言行动（VLA）模型来获得双人政策。但是，从单臂数据集或预训练的VLA模型转移知识通常无法有效地概括，这主要是由于双臂数据的稀缺性以及单臂和双臂操纵之间的基本差异。在本文中，我们通过微调领先的文本对视频模型来预测机器人轨迹并培训轻量级扩散策略，从而提出了一种新颖的双人基础政策。鉴于文本到视频模型缺乏具体知识，我们引入了一个两阶段的范式，该范式通过预先训练的文本对视频模型进行微调独立的文本对流程和流程到视频模型。具体而言，光流充当中间变量，提供了图像之间微妙运动的简洁表示。文本到流量模型可以预测光流，以构成语言指令的意图，而流向视频模型则利用此流程来获得细粒度的视频预测。我们的方法减轻了单阶段文本对视频预测中语言的歧义，并通过避免直接使用低级动作来大大减少机器人数据的要求。在实验中，我们收集了实际双臂机器人的高质量操纵数据，模拟和现实世界实验的结果证明了我们方法的有效性。</li>
</ul>

<h3>Title: DrVD-Bench: Do Vision-Language Models Reason Like Human Doctors in Medical Image Diagnosis?</h3>
<ul>
<li><strong>Authors: </strong>Tianhong Zhou, Yin Xu, Yingtao Zhu, Chuxi Xiao, Haiyang Bian, Lei Wei, Xuegong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24173">https://arxiv.org/abs/2505.24173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24173">https://arxiv.org/pdf/2505.24173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24173]] DrVD-Bench: Do Vision-Language Models Reason Like Human Doctors in Medical Image Diagnosis?(https://arxiv.org/abs/2505.24173)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision-language models (VLMs) exhibit strong zero-shot generalization on natural images and show early promise in interpretable medical image analysis. However, existing benchmarks do not systematically evaluate whether these models truly reason like human clinicians or merely imitate superficial patterns. To address this gap, we propose DrVD-Bench, the first multimodal benchmark for clinical visual reasoning. DrVD-Bench consists of three modules: Visual Evidence Comprehension, Reasoning Trajectory Assessment, and Report Generation Evaluation, comprising a total of 7,789 image-question pairs. Our benchmark covers 20 task types, 17 diagnostic categories, and five imaging modalities-CT, MRI, ultrasound, radiography, and pathology. DrVD-Bench is explicitly structured to reflect the clinical reasoning workflow from modality recognition to lesion identification and diagnosis. We benchmark 19 VLMs, including general-purpose and medical-specific, open-source and proprietary models, and observe that performance drops sharply as reasoning complexity increases. While some models begin to exhibit traces of human-like reasoning, they often still rely on shortcut correlations rather than grounded visual understanding. DrVD-Bench offers a rigorous and structured evaluation framework to guide the development of clinically trustworthy VLMs.</li>
<li><strong>摘要：</strong>视觉模型（VLMS）在自然图像上表现出强烈的零拍概括，并在可解释的医学图像分析中表现出了早期的希望。但是，现有的基准并未系统地评估这些模型是真正的理由，例如人类临床医生还是仅模仿浅表模式。为了解决这一差距，我们提出了DRVD板凳，这是第一个用于临床视觉推理的多模式基准。 DRVD板凳由三个模块组成：视觉证据理解，推理轨迹评估和报告生成评估，包括7,789个图像问题对。我们的基准涵盖了20种任务类型，17种诊断类别以及五个成像模式-CT，MRI，超声，放射线照相和病理学。 DRVD基座的结构明确，以反映从形态识别到病变识别和诊断的临床推理工作流程。我们基准为19 VLM，包括通用和特定医学，开源和专有模型，并观察到随着推理复杂性的增加，性能会急剧下降。尽管某些模型开始表现出类似人类的推理的痕迹，但它们通常仍然依赖捷径相关性，而不是扎根的视觉理解。 DRVD板凳提供了一个严格且结构化的评估框架，以指导临床值得信赖的VLM的开发。</li>
</ul>

<h3>Title: CodeV-R1: Reasoning-Enhanced Verilog Generation</h3>
<ul>
<li><strong>Authors: </strong>Yaoyu Zhu, Di Huang, Hanqi Lyu, Xiaoyun Zhang, Chongxiao Li, Wenxuan Shi, Yutong Wu, Jianan Mu, Jinghua Wang, Yang Zhao, Pengwei Jin, Shuyao Cheng, Shengwen Liang, Xishan Zhang, Rui Zhang, Zidong Du, Qi Guo, Xing Hu, Yunji Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24183">https://arxiv.org/abs/2505.24183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24183">https://arxiv.org/pdf/2505.24183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24183]] CodeV-R1: Reasoning-Enhanced Verilog Generation(https://arxiv.org/abs/2505.24183)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) trained via reinforcement learning with verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit, automatable verification, such as software programming and mathematical problems. Extending RLVR to electronic design automation (EDA), especially automatically generating hardware description languages (HDLs) like Verilog from natural-language (NL) specifications, however, poses three key challenges: the lack of automated and accurate verification environments, the scarcity of high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To this end, we introduce CodeV-R1, an RLVR framework for training Verilog generation LLMs. First, we develop a rule-based testbench generator that performs robust equivalence checking against golden references. Second, we propose a round-trip data synthesis method that pairs open-source Verilog snippets with LLM-generated NL descriptions, verifies code-NL-code consistency via the generated testbench, and filters out inequivalent examples to yield a high-quality dataset. Third, we employ a two-stage "distill-then-RL" training pipeline: distillation for the cold start of reasoning abilities, followed by adaptive DAPO, our novel RLVR algorithm that can reduce training cost by adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves 68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively, surpassing prior state-of-the-art by 12~20%, while matching or even exceeding the performance of 671B DeepSeek-R1. We will release our model, training pipeline, and dataset to facilitate research in EDA and LLM communities.</li>
<li><strong>摘要：</strong>通过加强学习培训的大型语言模型（LLM）具有可验证的奖励（RLVR），已在具有明确，可自动验证的任务上取得了突破，例如软件编程和数学问题。 Extending RLVR to electronic design automation (EDA), especially automatically generating hardware description languages (HDLs) like Verilog from natural-language (NL) specifications, however, poses three key challenges: the lack of automated and accurate verification environments, the scarcity of high-quality NL-code pairs, and the prohibitive computation cost of RLVR.为此，我们介绍了用于培训Verilog Generation LLM的RLVR框架CodeV-R1。首先，我们开发了一个基于规则的测试台生成器，该生成器可以针对黄金引用执行强大的等价检查。其次，我们提出了一种往返数据综合方法，该方法将开源Verilog片段与LLM生成的NL描述配对，通过生成的测试台验证Code-NL代码一致性，并过滤不相等的示例以产生高质量数据集。第三，我们采用了两个阶段的“蒸馏”训练管道：推理能力的冷启动，然后是自适应DAPO，我们的新型RLVR算法可以通过调整采样率来降低培训成本。所得的模型CodeV-R1-7B分别在Verilogeval V2和RTLLM V1.1上获得68.6％和72.9％的PASS@1，在匹配甚至超过671B DeepSeek-R1的性能的同时，超过了先前的最新时间。我们将发布我们的模型，培训管道和数据集，以促进EDA和LLM社区的研究。</li>
</ul>

<h3>Title: Boosting All-in-One Image Restoration via Self-Improved Privilege Learning</h3>
<ul>
<li><strong>Authors: </strong>Gang Wu, Junjun Jiang, Kui Jiang, Xianming Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24207">https://arxiv.org/abs/2505.24207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24207">https://arxiv.org/pdf/2505.24207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24207]] Boosting All-in-One Image Restoration via Self-Improved Privilege Learning(https://arxiv.org/abs/2505.24207)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Unified image restoration models for diverse and mixed degradations often suffer from unstable optimization dynamics and inter-task conflicts. This paper introduces Self-Improved Privilege Learning (SIPL), a novel paradigm that overcomes these limitations by innovatively extending the utility of privileged information (PI) beyond training into the inference stage. Unlike conventional Privilege Learning, where ground-truth-derived guidance is typically discarded after training, SIPL empowers the model to leverage its own preliminary outputs as pseudo-privileged signals for iterative self-refinement at test time. Central to SIPL is Proxy Fusion, a lightweight module incorporating a learnable Privileged Dictionary. During training, this dictionary distills essential high-frequency and structural priors from privileged feature representations. Critically, at inference, the same learned dictionary then interacts with features derived from the model's initial restoration, facilitating a self-correction loop. SIPL can be seamlessly integrated into various backbone architectures, offering substantial performance improvements with minimal computational overhead. Extensive experiments demonstrate that SIPL significantly advances the state-of-the-art on diverse all-in-one image restoration benchmarks. For instance, when integrated with the PromptIR model, SIPL achieves remarkable PSNR improvements of +4.58 dB on composite degradation tasks and +1.28 dB on diverse five-task benchmarks, underscoring its effectiveness and broad applicability. Codes are available at our project page this https URL.</li>
<li><strong>摘要：</strong>用于多种和混合降解的统一图像恢复模型通常遭受不稳定的优化动态和任务间冲突。本文介绍了自我改进的特权学习（SIPL），这是一种新颖的范式，通过将特权信息（PI）的实用性扩展到推理阶段，从而克服了这些局限性。与传统的特权学习不同，在训练后通常会丢弃地面衍生的指导，SIPL使该模型能够利用其自己的初步输出作为伪游离的信号，以便在测试时间进行迭代自我进行。 SIPL的核心是代理融合，这是一个结合了可学习特权字典的轻量级模块。在培训期间，该词典将基本的高频和结构性先验提炼为特权特征表示。至关重要的是，在推断时，同一学习的词典与从模型的初始恢复中得出的特征相互作用，从而促进了自校正环路。可以将SIPL无缝集成到各种骨干架构中，从而通过最小的计算开销提供了实质性的改进。广泛的实验表明，SIPL在各种多合一图像恢复基准上显着提高了最新的实验。例如，当与Profectir模型集成时，SIPL在复合降解任务上实现了+4.58 dB的显着改善，而在不同的五任务基准测试方面，+1.28 dB实现了+1.28 dB，强调了其有效性和广泛的适用性。代码可在我们的项目页面此https URL上找到。</li>
</ul>

<h3>Title: STORK: Improving the Fidelity of Mid-NFE Sampling for Diffusion and Flow Matching Models</h3>
<ul>
<li><strong>Authors: </strong>Zheng Tan, Weizhen Wang, Andrea L. Bertozzi, Ernest K. Ryu</a></li>
<li><strong>Subjects: </strong>cs.CV, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24210">https://arxiv.org/abs/2505.24210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24210">https://arxiv.org/pdf/2505.24210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24210]] STORK: Improving the Fidelity of Mid-NFE Sampling for Diffusion and Flow Matching Models(https://arxiv.org/abs/2505.24210)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models (DMs) have demonstrated remarkable performance in high-fidelity image and video generation. Because high-quality generations with DMs typically require a large number of function evaluations (NFEs), resulting in slow sampling, there has been extensive research successfully reducing the NFE to a small range (<10) while maintaining acceptable image quality. However, many practical applications, such as those involving Stable Diffusion 3.5, FLUX, and SANA, commonly operate in the mid-NFE regime (20-50 NFE) to achieve superior results, and, despite the practical relevance, research on the effective sampling within this mid-NFE regime remains underexplored. In this work, we propose a novel, training-free, and structure-independent DM ODE solver called the Stabilized Taylor Orthogonal Runge--Kutta (STORK) method, based on a class of stiff ODE solvers with a Taylor expansion adaptation. Unlike prior work such as DPM-Solver, which is dependent on the semi-linear structure of the DM ODE, STORK is applicable to any DM sampling, including noise-based and flow matching-based models. Within the 20-50 NFE range, STORK achieves improved generation quality, as measured by FID scores, across unconditional pixel-level generation and conditional latent-space generation tasks using models like Stable Diffusion 3.5 and SANA. Code is available at this https URL.</li>
<li><strong>摘要：</strong>扩散模型（DMS）在高保真图像和视频生成方面表现出色。由于具有DMS的高质量世代通常需要大量的功能评估（NFE），从而导致采样缓慢，因此在维持可接受的图像质量的同时，已有广泛的研究成功地将NFE降低到较小的范围（<10）。但是，许多实用应用，例如涉及稳定扩散3.5，通量和SANA的实用应用通常在Mid-NFE制度（20-50 NFE）中运行，以取得较高的结果，尽管实际相关性，但对此中NFE中的有效采样的研究仍然保持不变。在这项工作中，我们提出了一种新型，无训练和与结构无关的DM ODE求解器，称为稳定的泰勒正交runge-kutta（Stork）方法，基于具有泰勒膨胀适应的一类刚性ode求解器。与先前的工作（例如DPM-Solver）取决于DM ODE的半线性结构，Stork适用于任何DM采样，包括基于噪声的基于噪声和基于流动的模型。在20-50 NFE范围内，通过使用稳定的扩散3.5和SANA等模型，在无条件像素级的生成和条件潜在空间生成任务中，通过FID得分衡量，达到了提高的发电质量。代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Unleashing High-Quality Image Generation in Diffusion Sampling Using Second-Order Levenberg-Marquardt-Langevin</h3>
<ul>
<li><strong>Authors: </strong>Fangyikang Wang, Hubery Yin, Lei Qian, Yinan Li, Shaobin Zhuang, Huminhao Zhu, Yilin Zhang, Yanlong Tang, Chao Zhang, Hanbin Zhao, Hui Qian, Chen Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24222">https://arxiv.org/abs/2505.24222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24222">https://arxiv.org/pdf/2505.24222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24222]] Unleashing High-Quality Image Generation in Diffusion Sampling Using Second-Order Levenberg-Marquardt-Langevin(https://arxiv.org/abs/2505.24222)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The diffusion models (DMs) have demonstrated the remarkable capability of generating images via learning the noised score function of data distribution. Current DM sampling techniques typically rely on first-order Langevin dynamics at each noise level, with efforts concentrated on refining inter-level denoising strategies. While leveraging additional second-order Hessian geometry to enhance the sampling quality of Langevin is a common practice in Markov chain Monte Carlo (MCMC), the naive attempts to utilize Hessian geometry in high-dimensional DMs lead to quadratic-complexity computational costs, rendering them non-scalable. In this work, we introduce a novel Levenberg-Marquardt-Langevin (LML) method that approximates the diffusion Hessian geometry in a training-free manner, drawing inspiration from the celebrated Levenberg-Marquardt optimization algorithm. Our approach introduces two key innovations: (1) A low-rank approximation of the diffusion Hessian, leveraging the DMs' inherent structure and circumventing explicit quadratic-complexity computations; (2) A damping mechanism to stabilize the approximated Hessian. This LML approximated Hessian geometry enables the diffusion sampling to execute more accurate steps and improve the image generation quality. We further conduct a theoretical analysis to substantiate the approximation error bound of low-rank approximation and the convergence property of the damping mechanism. Extensive experiments across multiple pretrained DMs validate that the LML method significantly improves image generation quality, with negligible computational overhead.</li>
<li><strong>摘要：</strong>扩散模型（DMS）证明了通过学习数据分布的噪声得分函数生成图像的显着能力。当前的DM采样技术通常依赖于每个噪声水平的一阶Langevin动力学，努力集中在精炼层间denoising策略上。尽管利用其他二阶Hessian几何形状来提高Langevin的采样质量是Markov Chain Monte Carlo（MCMC）的一种常见实践，但天真的尝试在高维DMS中利用Hessian几何形状导致了四边形的计算成本，从而使它们不可降低。在这项工作中，我们介绍了一种小说的Levenberg-Marquardt-Langevin（LML）方法，该方法以无训练的方式近似于扩散的Hessian几何形状，从著名的Levenberg-Marquardt优化算法中汲取灵感。我们的方法介绍了两个关键的创新：（1）扩散黑森的低级别近似，利用DMS的固有结构并规避显式的二次复合计算； （2）稳定近似黑森的阻尼机制。该LML近似的Hessian几何形状使扩散采样能够执行更准确的步骤并提高图像生成质量。我们进一步进行了理论分析，以证实低级近似值的近似误差和阻尼机理的收敛性。跨多个经过预测的DMS的广泛实验验证了LML方法可以显着提高图像的产生质量，并具有可忽略的计算开销。</li>
</ul>

<h3>Title: Reasoning Can Hurt the Inductive Abilities of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haibo Jin, Peiyan Zhang, Man Luo, Haohan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24225">https://arxiv.org/abs/2505.24225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24225">https://arxiv.org/pdf/2505.24225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24225]] Reasoning Can Hurt the Inductive Abilities of Large Language Models(https://arxiv.org/abs/2505.24225)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown remarkable progress across domains, yet their ability to perform inductive reasoning - inferring latent rules from sparse examples - remains limited. It is often assumed that chain-of-thought (CoT) prompting, as used in Large Reasoning Models (LRMs), enhances such reasoning. We investigate this assumption with creating four controlled, diagnostic game-based tasks - chess, Texas Hold'em, dice games, and blackjack - with hidden human-defined rules. We find that CoT reasoning can degrade inductive performance, with LRMs often underperforming their non-reasoning counterparts. To explain this, we present a theoretical framework that reveals how reasoning steps can amplify error through three failure modes: incorrect sub-task decomposition, incorrect sub-task solving, and incorrect final answer summarization. Based on our theoretical and empirical analysis, we introduce structured interventions that adapt CoT generation according to our identified failure types. These interventions improve inductive accuracy without retraining. Our findings suggest that effective (CoT) reasoning depends not only on taking more steps but also on ensuring those steps are well-structured.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）在范围内显示出了显着的进步，但是它们执行归纳推理的能力 - 从稀疏示例中推断潜在规则 - 仍然有限。通常假定在大型推理模型（LRMS）中使用的思想链（COT）提示会增强这种推理。我们使用隐藏的人类定义规则来创建四个受控的，基于游戏的任务 - 国际象棋，德克萨斯州霍尔德，骰子游戏和二十一点。我们发现，COT推理可以降低归纳性能，而LRM的表现通常不足以降低其非选择对应。为了解释这一点，我们提出了一个理论框架，该框架揭示了推理步骤如何通过三种故障模式放大误差：不正确的子任务分解，错误的子任务求解和错误的最终答案摘要。基于我们的理论和经验分析，我们引入了根据我们确定的故障类型来适应COT产生的结构化干预措施。这些干预措施提高了电感精度而无需再培训。我们的发现表明，有效的（COT）推理不仅取决于采取更多步骤，还取决于确保这些步骤结构良好。</li>
</ul>

<h3>Title: LTM3D: Bridging Token Spaces for Conditional 3D Generation with Auto-Regressive Diffusion Framework</h3>
<ul>
<li><strong>Authors: </strong>Xin Kang, Zihan Zheng, Lei Chu, Yue Gao, Jiahao Li, Hao Pan, Xuejin Chen, Yan Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24245">https://arxiv.org/abs/2505.24245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24245">https://arxiv.org/pdf/2505.24245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24245]] LTM3D: Bridging Token Spaces for Conditional 3D Generation with Auto-Regressive Diffusion Framework(https://arxiv.org/abs/2505.24245)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present LTM3D, a Latent Token space Modeling framework for conditional 3D shape generation that integrates the strengths of diffusion and auto-regressive (AR) models. While diffusion-based methods effectively model continuous latent spaces and AR models excel at capturing inter-token dependencies, combining these paradigms for 3D shape generation remains a challenge. To address this, LTM3D features a Conditional Distribution Modeling backbone, leveraging a masked autoencoder and a diffusion model to enhance token dependency learning. Additionally, we introduce Prefix Learning, which aligns condition tokens with shape latent tokens during generation, improving flexibility across modalities. We further propose a Latent Token Reconstruction module with Reconstruction-Guided Sampling to reduce uncertainty and enhance structural fidelity in generated shapes. Our approach operates in token space, enabling support for multiple 3D representations, including signed distance fields, point clouds, meshes, and 3D Gaussian Splatting. Extensive experiments on image- and text-conditioned shape generation tasks demonstrate that LTM3D outperforms existing methods in prompt fidelity and structural accuracy while offering a generalizable framework for multi-modal, multi-representation 3D generation.</li>
<li><strong>摘要：</strong>我们提出了LTM3D，这是条件3D形成生成的潜在令牌空间建模框架，该框架集成了扩散和自动回归（AR）模型的优势。尽管基于扩散的方法有效地模拟了连续的潜在空间和AR模型在捕获相互依赖方面表现出色，但将这些范式组合为3D形状生成的范式仍然是一个挑战。为了解决这个问题，LTM3D具有条件分布建模主链，利用掩盖的自动编码器和扩散模型来增强令牌依赖性学习。此外，我们介绍了前缀学习，该学习将条件令牌与形状潜在令牌保持一致，从而提高了各种方式的灵活性。我们进一步提出了一个潜在的代币重建模块，该模块具有重建引导的采样，以降低不确定性并增强生成形状的结构保真度。我们的方法在令牌空间中运行，为多个3D表示形式提供了支持，包括签名的距离字段，点云，网格和3D高斯脱落。关于图像和文本条件形状生成任务的广泛实验表明，LTM3D以迅速的保真度和结构准确性优于现有方法，同时为多模式的多模式，多代价3D生成提供了可推广的框架。</li>
</ul>

<h3>Title: 50 Years of Automated Face Recognition</h3>
<ul>
<li><strong>Authors: </strong>Minchul Kim, Anil Jain, Xiaoming Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24247">https://arxiv.org/abs/2505.24247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24247">https://arxiv.org/pdf/2505.24247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24247]] 50 Years of Automated Face Recognition(https://arxiv.org/abs/2505.24247)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Over the past 50 years, automated face recognition has evolved from rudimentary, handcrafted systems into sophisticated deep learning models that rival and often surpass human performance. This paper chronicles the history and technological progression of FR, from early geometric and statistical methods to modern deep neural architectures leveraging massive real and AI-generated datasets. We examine key innovations that have shaped the field, including developments in dataset, loss function, neural network design and feature fusion. We also analyze how the scale and diversity of training data influence model generalization, drawing connections between dataset growth and benchmark improvements. Recent advances have achieved remarkable milestones: state-of-the-art face verification systems now report False Negative Identification Rates of 0.13% against a 12.4 million gallery in NIST FRVT evaluations for 1:N visa-to-border matching. While recent advances have enabled remarkable accuracy in high- and low-quality face scenarios, numerous challenges persist. While remarkable progress has been achieved, several open research problems remain. We outline critical challenges and promising directions for future face recognition research, including scalability, multi-modal fusion, synthetic identity generation, and explainable systems.</li>
<li><strong>摘要：</strong>在过去的50年中，自动化的面部识别已从基本的手工制作的系统演变为复杂的深度学习模型，这些模型与人类的表现相抗衡。本文记录了FR的历史和技术进步，从早期的几何和统计方法到现代深度神经体系结构利用大量的真实和AI生成的数据集。我们研究了塑造该领域的关键创新，包括数据集的发展，损耗功能，神经网络设计和功能融合。我们还分析了培训数据的规模和多样性如何影响模型的概括，培养数据集增长与基准改进之间的联系。最近的进步已经取得了显着的里程碑：最先进的面部验证系统现在报告说，在NIST FRVT评估中，对1：N Visa-Border匹配的NIST FRVT评估中的虚假负面识别率为0.13％。尽管最近的进步在高质量和低质量的面景方面启用了出色的准确性，但许多挑战仍然存在。尽管已经取得了显着的进展，但仍然存在一些开放的研究问题。我们概述了未来面部识别研究的关键挑战和有希望的方向，包括可伸缩性，多模式融合，合成身份生成和可解释的系统。</li>
</ul>

<h3>Title: Interactive Video Generation via Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Ishaan Rawal, Suryansh Kumar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24253">https://arxiv.org/abs/2505.24253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24253">https://arxiv.org/pdf/2505.24253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24253]] Interactive Video Generation via Domain Adaptation(https://arxiv.org/abs/2505.24253)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-conditioned diffusion models have emerged as powerful tools for high-quality video generation. However, enabling Interactive Video Generation (IVG), where users control motion elements such as object trajectory, remains challenging. Recent training-free approaches introduce attention masking to guide trajectory, but this often degrades perceptual quality. We identify two key failure modes in these methods, both of which we interpret as domain shift problems, and propose solutions inspired by domain adaptation. First, we attribute the perceptual degradation to internal covariate shift induced by attention masking, as pretrained models are not trained to handle masked attention. To address this, we propose mask normalization, a pre-normalization layer designed to mitigate this shift via distribution matching. Second, we address initialization gap, where the randomly sampled initial noise does not align with IVG conditioning, by introducing a temporal intrinsic diffusion prior that enforces spatio-temporal consistency at each denoising step. Extensive qualitative and quantitative evaluations demonstrate that mask normalization and temporal intrinsic denoising improve both perceptual quality and trajectory control over the existing state-of-the-art IVG techniques.</li>
<li><strong>摘要：</strong>文本条件扩散模型已成为高质量视频生成的强大工具。但是，启用交互式视频生成（IVG），用户控制运动元素（例如对象轨迹）仍然具有挑战性。最近的无培训方法引入了注意力掩盖以指导轨迹，但这常常会降低感知质量。我们在这些方法中确定了两种关键的故障模式，这两者都将其解释为域移位问题，并提出了受域适应性启发的解决方案。首先，我们将感知降解归因于注意力掩盖引起的内部协变量转移，因为预审计的模型未经训练以处理掩盖的注意力。为了解决这个问题，我们提出了掩模归一化，这是一个旨在通过分布匹配来减轻这种转移的预归一化层。其次，我们解决了初始化差距，其中随机采样的初始噪声与IVG条件不符，通过引入时间固有扩散之前，该噪声在每个DENOISIS步骤中都会实现时空的一致性。广泛的定性和定量评估表明，掩盖标准化和时间内在的deNOCO，改善了对现有最新IVG技术的感知质量和轨迹控制。</li>
</ul>

<h3>Title: Out of Sight, Not Out of Context? Egocentric Spatial Reasoning in VLMs Across Disjoint Frames</h3>
<ul>
<li><strong>Authors: </strong>Sahithya Ravi, Gabriel Sarch, Vibhav Vineet, Andrew D. Wilson, Balasaravanan Thoravi Kumaravel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24257">https://arxiv.org/abs/2505.24257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24257">https://arxiv.org/pdf/2505.24257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24257]] Out of Sight, Not Out of Context? Egocentric Spatial Reasoning in VLMs Across Disjoint Frames(https://arxiv.org/abs/2505.24257)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>An embodied AI assistant operating on egocentric video must integrate spatial cues across time - for instance, determining where an object A, glimpsed a few moments ago lies relative to an object B encountered later. We introduce Disjoint-3DQA , a generative QA benchmark that evaluates this ability of VLMs by posing questions about object pairs that are not co-visible in the same frame. We evaluated seven state-of-the-art VLMs and found that models lag behind human performance by 28%, with steeper declines in accuracy (60% to 30 %) as the temporal gap widens. Our analysis further reveals that providing trajectories or bird's-eye-view projections to VLMs results in only marginal improvements, whereas providing oracle 3D coordinates leads to a substantial 20% performance increase. This highlights a core bottleneck of multi-frame VLMs in constructing and maintaining 3D scene representations over time from visual signals. Disjoint-3DQA therefore sets a clear, measurable challenge for long-horizon spatial reasoning and aims to catalyze future research at the intersection of vision, language, and embodied AI.</li>
<li><strong>摘要：</strong>在以Egentric视频上操作的体现的AI助手必须在时间上整合空间线索 - 例如，确定对象A在片刻前瞥见的位置，相对于稍后遇到的对象B。我们介绍了Disboins-3DQA，这是一种生成的质量质量质量标准，该基准通过提出有关对象对的问题来评估VLM的这种能力，这些问题在同一帧中不可辨认。我们评估了七个最先进的VLM，发现模型落后于人类绩效的28％，随着时间间隙的扩大，准确性的急剧下降（60％至30％）。我们的分析进一步表明，向VLMS提供轨迹或鸟眼观看预测仅会导致边际改进，而提供Oracle 3D坐标的性能会导致绩效的20％大幅提高。这突出了多帧VLM的核心瓶颈在随着时间的流逝中构建和维护3D场景表示形式时，视觉信号从视觉信号中构建和维护了3D场景表示形式。因此，DISHOINT-3DQA为长途空间推理设定了一个明确，可衡量的挑战，并旨在在视觉，语言和体现的AI交集中催化未来的研究。</li>
</ul>

<h3>Title: EgoExOR: An Ego-Exo-Centric Operating Room Dataset for Surgical Activity Understanding</h3>
<ul>
<li><strong>Authors: </strong>Ege Özsoy, Arda Mamur, Felix Tristram, Chantal Pellegrini, Magdalena Wysocki, Benjamin Busam, Nassir Navab</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24287">https://arxiv.org/abs/2505.24287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24287">https://arxiv.org/pdf/2505.24287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24287]] EgoExOR: An Ego-Exo-Centric Operating Room Dataset for Surgical Activity Understanding(https://arxiv.org/abs/2505.24287)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Operating rooms (ORs) demand precise coordination among surgeons, nurses, and equipment in a fast-paced, occlusion-heavy environment, necessitating advanced perception models to enhance safety and efficiency. Existing datasets either provide partial egocentric views or sparse exocentric multi-view context, but do not explore the comprehensive combination of both. We introduce EgoExOR, the first OR dataset and accompanying benchmark to fuse first-person and third-person perspectives. Spanning 94 minutes (84,553 frames at 15 FPS) of two emulated spine procedures, Ultrasound-Guided Needle Insertion and Minimally Invasive Spine Surgery, EgoExOR integrates egocentric data (RGB, gaze, hand tracking, audio) from wearable glasses, exocentric RGB and depth from RGB-D cameras, and ultrasound imagery. Its detailed scene graph annotations, covering 36 entities and 22 relations (568,235 triplets), enable robust modeling of clinical interactions, supporting tasks like action recognition and human-centric perception. We evaluate the surgical scene graph generation performance of two adapted state-of-the-art models and offer a new baseline that explicitly leverages EgoExOR's multimodal and multi-perspective signals. This new dataset and benchmark set a new foundation for OR perception, offering a rich, multimodal resource for next-generation clinical perception.</li>
<li><strong>摘要：</strong>手术室（ORS）要求在快节奏，闭塞繁重的环境中进行外科医生，护士和设备之间的精确协调，需要提高先进的感知模型，以提高安全性和效率。现有的数据集可以提供部分自我中心的视图或稀疏的以外的多视图上下文，但不要探索两者的全面组合。我们介绍了EgoExor，第一个或数据集以及随附的基准，以融合第一人称和第三人称视角。 Spanning 94 minutes (84,553 frames at 15 FPS) of two emulated spine procedures, Ultrasound-Guided Needle Insertion and Minimally Invasive Spine Surgery, EgoExOR integrates egocentric data (RGB, gaze, hand tracking, audio) from wearable glasses, exocentric RGB and depth from RGB-D cameras, and ultrasound imagery.其详细的场景图表注释涵盖了36个实体和22个关系（568,235个三胞胎），实现了临床相互作用的强大建模，支持了诸如动作识别和以人为中心的知觉等任务。我们评估了两种改编的最先进模型的外科手术场景生成性能，并提供了一个新的基线，该基线明确利用了Egoexor的多模式和多观点信号。这个新的数据集和基准为下一代临床感知提供了丰富的多模式资源，为新的或感知树立了新的基础。</li>
</ul>

<h3>Title: AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, Tongkai Yang, Binhang Yuan, Yi Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24298">https://arxiv.org/abs/2505.24298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24298">https://arxiv.org/pdf/2505.24298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24298]] AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning(https://arxiv.org/abs/2505.24298)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has become a trending paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous by alternating generation and training in a batch setting, where the rollouts in each training batch are generated by the same (or latest) model. This stabilizes RL training but suffers from severe system-level inefficiency. Generation must wait until the longest output in the batch is completed before model update, resulting in GPU underutilization. We present AReaL, a \emph{fully asynchronous} RL system that completely decouples generation from training. Rollout workers in AReaL continuously generate new outputs without waiting, while training workers update the model whenever a batch of data is collected. AReaL also incorporates a collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AReaL balances the workload of rollout and training workers to control data staleness, and adopts a staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AReaL achieves \textbf{up to 2.57$\times$ training speedup} compared to the best synchronous systems with the same number of GPUs and matched or even improved final performance. The code of AReaL is available at this https URL.</li>
<li><strong>摘要：</strong>强化学习（RL）已成为培训大语言模型（LLM）的趋势范式，尤其是用于推理任务。 LLM的有效RL需要大量的并行化，并迫切需要有效的训练系统。通过在批处理设置中交替产生和培训，大多数现有的LLM的大型RL系统都是同步的，在该设置中，每个培训批次中的推出都是由相同（或最新）模型生成的。这可以稳定RL训练，但患有严重的系统级效率。生成必须等到批处理中最长的输出在模型更新之前完成，从而导致GPU充分利用。我们提出了一个\ emph {完全异步} rl系统，该系统完全使生成无法训练。在培训工作人员收集一批数据时，培训工人在不等待的同时不断地生成新的输出，而无需等待，就会不断生成新的输出。 Areal还结合了系统级优化的集合，从而导致GPU利用率较高。为了稳定RL培训，Areal可以平衡推出和培训工人的工作量以控制数据的稳定性，并采用稳固性增强的PPO变体来更好地处理过时的培训样本。与具有相同数量的GPU并匹配甚至改进的最终性能的最佳同步系统相比，关于数学和代码推理基准的大量实验表明，Areal实现了\ TextBf {最高2.57 $ \ times $ triending speedup}。该https URL可用地带守则。</li>
</ul>

<h3>Title: Category-aware EEG image generation based on wavelet transform and contrast semantic loss</h3>
<ul>
<li><strong>Authors: </strong>Enshang Zhang, Zhicheng Zhang, Takashi Hanakawa</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24301">https://arxiv.org/abs/2505.24301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24301">https://arxiv.org/pdf/2505.24301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24301]] Category-aware EEG image generation based on wavelet transform and contrast semantic loss(https://arxiv.org/abs/2505.24301)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reconstructing visual stimuli from EEG signals is a crucial step in realizing brain-computer interfaces. In this paper, we propose a transformer-based EEG signal encoder integrating the Discrete Wavelet Transform (DWT) and the gating mechanism. Guided by the feature alignment and category-aware fusion losses, this encoder is used to extract features related to visual stimuli from EEG signals. Subsequently, with the aid of a pre-trained diffusion model, these features are reconstructed into visual stimuli. To verify the effectiveness of the model, we conducted EEG-to-image generation and classification tasks using the THINGS-EEG dataset. To address the limitations of quantitative analysis at the semantic level, we combined WordNet-based classification and semantic similarity metrics to propose a novel semantic-based score, emphasizing the ability of our model to transfer neural activities into visual representations. Experimental results show that our model significantly improves semantic alignment and classification accuracy, which achieves a maximum single-subject accuracy of 43\%, outperforming other state-of-the-art methods. The source code and supplementary material is available at this https URL.</li>
<li><strong>摘要：</strong>从EEG信号重建视觉刺激是实现脑部计算机界面的关键步骤。在本文中，我们提出了一个基于变压器的EEG信号编码器，该信号编码器整合了离散小波变换（DWT）和门控机制。在特征比对和类别感知的融合损失的指导下，该编码器用于从EEG信号中提取与视觉刺激相关的特征。随后，借助预训练的扩散模型，将这些特征重建为视觉刺激。为了验证模型的有效性，我们使用Things-EEG数据集进行了eeg-to-to-to Image生成和分类任务。为了解决语义级别的定量分析的局限性，我们将基于WordNet的分类和语义相似性指标结合在一起，提出了一种新型的基于语义的分数，强调了我们的模型将神经活动转移到视觉表示中的能力。实验结果表明，我们的模型显着提高了语义一致性和分类精度，该准确性达到了43 \％的最大单个主体精度，表现优于其他最先进的方法。源代码和补充材料可在此HTTPS URL上获得。</li>
</ul>

<h3>Title: InteractAnything: Zero-shot Human Object Interaction Synthesis via LLM Feedback and Object Affordance Parsing</h3>
<ul>
<li><strong>Authors: </strong>Jinlu Zhang, Yixin Chen, Zan Wang, Jie Yang, Yizhou Wang, Siyuan Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24315">https://arxiv.org/abs/2505.24315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24315">https://arxiv.org/pdf/2505.24315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24315]] InteractAnything: Zero-shot Human Object Interaction Synthesis via LLM Feedback and Object Affordance Parsing(https://arxiv.org/abs/2505.24315)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in 3D human-aware generation have made significant progress. However, existing methods still struggle with generating novel Human Object Interaction (HOI) from text, particularly for open-set objects. We identify three main challenges of this task: precise human-object relation reasoning, affordance parsing for any object, and detailed human interaction pose synthesis aligning description and object geometry. In this work, we propose a novel zero-shot 3D HOI generation framework without training on specific datasets, leveraging the knowledge from large-scale pre-trained models. Specifically, the human-object relations are inferred from large language models (LLMs) to initialize object properties and guide the optimization process. Then we utilize a pre-trained 2D image diffusion model to parse unseen objects and extract contact points, avoiding the limitations imposed by existing 3D asset knowledge. The initial human pose is generated by sampling multiple hypotheses through multi-view SDS based on the input text and object geometry. Finally, we introduce a detailed optimization to generate fine-grained, precise, and natural interaction, enforcing realistic 3D contact between the 3D object and the involved body parts, including hands in grasping. This is achieved by distilling human-level feedback from LLMs to capture detailed human-object relations from the text instruction. Extensive experiments validate the effectiveness of our approach compared to prior works, particularly in terms of the fine-grained nature of interactions and the ability to handle open-set 3D objects.</li>
<li><strong>摘要：</strong>3D人才一代的最新进展取得了重大进展。但是，现有的方法仍然很难从文本中产生新的人类对象互动（HOI），尤其是对于开放式对象。我们确定了这项任务的三个主要挑战：精确的人类对象关系推理，对任何对象的负担解析以及详细的人类互动构成构成的综合描述和对象几何形状。在这项工作中，我们提出了一个新型的零射击3D HOI生成框架，而无需在特定数据集上进行培训，从而利用了大规模的预训练模型的知识。具体而言，人类对象关系是从大语言模型（LLM）推断出来的，以初始化对象属性并指导优化过程。然后，我们利用预先训练的2D图像扩散模型来解析看不见的对象并提取接触点，避免现有3D资产知识施加的局限性。最初的人姿势是通过基于输入文本和对象几何形状通过多视图SD进行多个假设来生成的。最后，我们引入了详细的优化，以产生细粒度，精确和自然的相互作用，从而在3D对象和所涉及的身体部位（包括手抓住手）之间实现了现实的3D接触。这是通过从LLM​​中提取人类水平的反馈来从文本指令中捕获详细的人类对象关系来实现的。与先前的工作相比，广泛的实验验证了我们方法的有效性，尤其是在相互作用的细粒度和处理开放式3D对象的能力方面。</li>
</ul>

<h3>Title: ROAD: Responsibility-Oriented Reward Design for Reinforcement Learning in Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Yongming Chen, Miner Chen, Liewen Liao, Mingyang Jiang, Xiang Zuo, Hengrui Zhang, Yuchen Xi, Songan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24317">https://arxiv.org/abs/2505.24317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24317">https://arxiv.org/pdf/2505.24317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24317]] ROAD: Responsibility-Oriented Reward Design for Reinforcement Learning in Autonomous Driving(https://arxiv.org/abs/2505.24317)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) in autonomous driving employs a trial-and-error mechanism, enhancing robustness in unpredictable environments. However, crafting effective reward functions remains challenging, as conventional approaches rely heavily on manual design and demonstrate limited efficacy in complex scenarios. To address this issue, this study introduces a responsibility-oriented reward function that explicitly incorporates traffic regulations into the RL framework. Specifically, we introduced a Traffic Regulation Knowledge Graph and leveraged Vision-Language Models alongside Retrieval-Augmented Generation techniques to automate reward assignment. This integration guides agents to adhere strictly to traffic laws, thus minimizing rule violations and optimizing decision-making performance in diverse driving conditions. Experimental validations demonstrate that the proposed methodology significantly improves the accuracy of assigning accident responsibilities and effectively reduces the agent's liability in traffic incidents.</li>
<li><strong>摘要：</strong>自主驾驶中的强化学习（RL）采用了试验机制，在无法预测的环境中增强了鲁棒性。但是，制定有效的奖励功能仍然具有挑战性，因为传统方法在很大程度上依赖手动设计并在复杂场景中表现出有限的功效。为了解决这个问题，本研究介绍了一个面向责任的奖励功能，该功能将交通法规明确纳入RL框架。具体而言，我们介绍了交通调节知识图和杠杆视觉语言模型以及检索提示的生成技术，以使奖励分配自动化。这种集成指导代理人严格遵守交通法，从而最大程度地限制规则违规并在各种驾驶条件下优化决策绩效。实验验证表明，所提出的方法可显着提高分配事故职责的准确性，并有效地降低了代理商在交通事故中的责任。</li>
</ul>

<h3>Title: SwiftEval: Developing a Language-Specific Benchmark for LLM-generated Code Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Ivan Petrukha, Yana Kurliak, Nataliia Stulova</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.PL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24324">https://arxiv.org/abs/2505.24324</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24324">https://arxiv.org/pdf/2505.24324</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24324]] SwiftEval: Developing a Language-Specific Benchmark for LLM-generated Code Evaluation(https://arxiv.org/abs/2505.24324)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In recent years, large language models (LLMs) have showcased significant advancements in code generation. However, most evaluation benchmarks are primarily oriented towards Python, making it difficult to evaluate other programming languages, such as Swift, with high quality. By examining widely established multilingual benchmarks like HumanEval-XL and MultiPL-E, we identified critical issues specific to their Swift components, making them insufficient or even irrelevant for assessing LLM coding capabilities on Swift. Unlike these existing approaches, which prioritize rapid scaling and generalization by automatically translating Python-centric benchmarks with LLMs, we adopt a quality-over-quantity methodology. We present SwiftEval, the first Swift-oriented benchmark consisting of 28 carefully hand-crafted problems, and evaluate 44 popular Code LLMs on it. Our results show significant LLM scores drop for problems requiring language-specific features, most noticeable in the models of smaller sizes.</li>
<li><strong>摘要：</strong>近年来，大型语言模型（LLMS）展示了代码生成的重大进步。但是，大多数评估基准主要针对Python，因此很难用高质量评估其他编程语言，例如Swift。通过检查广泛建立的多语言基准（如HumaneVal-XL和Multipl-E），我们确定了针对其Swift组件的关键问题，使其不足甚至不足以评估Swift上的LLM编码功能。与这些现有方法不同，这些方法通过自动将以LLMS以Python为中心的基准转换为快速缩放和概括，我们采用了一种优质的定量方法。我们提出了Swifteval，这是第一个以迅速的基准测试，由28个精心制作的问题组成，并在其上评估了44个流行的代码LLM。我们的结果表明，对于需要语言特定功能的问题，LLM分数下降了，这在较小尺寸的模型中最引人注目。</li>
</ul>

<h3>Title: Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning</h3>
<ul>
<li><strong>Authors: </strong>Stepan Shabalin, Ayush Panda, Dmitrii Kharlapenko, Abdur Raheem Ali, Yixiong Hao, Arthur Conmy</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24360">https://arxiv.org/abs/2505.24360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24360">https://arxiv.org/pdf/2505.24360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24360]] Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning(https://arxiv.org/abs/2505.24360)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Sparse autoencoders are a promising new approach for decomposing language model activations for interpretation and control. They have been applied successfully to vision transformer image encoders and to small-scale diffusion models. Inference-Time Decomposition of Activations (ITDA) is a recently proposed variant of dictionary learning that takes the dictionary to be a set of data points from the activation distribution and reconstructs them with gradient pursuit. We apply Sparse Autoencoders (SAEs) and ITDA to a large text-to-image diffusion model, Flux 1, and consider the interpretability of embeddings of both by introducing a visual automated interpretation pipeline. We find that SAEs accurately reconstruct residual stream embeddings and beat MLP neurons on interpretability. We are able to use SAE features to steer image generation through activation addition. We find that ITDA has comparable interpretability to SAEs.</li>
<li><strong>摘要：</strong>稀疏的自动编码器是分解解释和控制语言模型激活语言模型的新方法。它们已成功应用于视觉变压器图像编码器和小规模扩散模型。推理时间的激活分解（ITDA）是最近提出的字典学习变体，它使字典是激活分布中的一组数据点，并以梯度追求重建它们。我们将稀疏的自动编码器（SAE）和ITDA应用于大型文本扩散模型Flux 1，并通过引入视觉自动解释管道来考虑两者的嵌入性。我们发现SAE可以准确地重建残留的流嵌入，并在解释性上击败MLP神经元。我们能够使用SAE功能通过添加激活来引导图像生成。我们发现ITDA与SAE具有可比性的解释性。</li>
</ul>

<h3>Title: Adversarial Preference Learning for Robust LLM Alignment</h3>
<ul>
<li><strong>Authors: </strong>Yuanfu Wang, Pengyu Wang, Chenyang Xi, Bo Tang, Junyi Zhu, Wenqiang Wei, Chen Chen, Chao Yang, Jingfeng Zhang, Chaochao Lu, Yijun Niu, Keming Mao, Zhiyu Li, Feiyu Xiong, Jie Hu, Mingchuan Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24369">https://arxiv.org/abs/2505.24369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24369">https://arxiv.org/pdf/2505.24369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24369]] Adversarial Preference Learning for Robust LLM Alignment(https://arxiv.org/abs/2505.24369)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Modern language models often rely on Reinforcement Learning from Human Feedback (RLHF) to encourage safe behaviors. However, they remain vulnerable to adversarial attacks due to three key limitations: (1) the inefficiency and high cost of human annotation, (2) the vast diversity of potential adversarial attacks, and (3) the risk of feedback bias and reward hacking. To address these challenges, we introduce Adversarial Preference Learning (APL), an iterative adversarial training method incorporating three key innovations. First, a direct harmfulness metric based on the model's intrinsic preference probabilities, eliminating reliance on external assessment. Second, a conditional generative attacker that synthesizes input-specific adversarial variations. Third, an iterative framework with automated closed-loop feedback, enabling continuous adaptation through vulnerability discovery and mitigation. Experiments on Mistral-7B-Instruct-v0.3 demonstrate that APL significantly enhances robustness, achieving 83.33% harmlessness win rate over the base model (evaluated by GPT-4o), reducing harmful outputs from 5.88% to 0.43% (measured by LLaMA-Guard), and lowering attack success rate by up to 65% according to HarmBench. Notably, APL maintains competitive utility, with an MT-Bench score of 6.59 (comparable to the baseline 6.78) and an LC-WinRate of 46.52% against the base model.</li>
<li><strong>摘要：</strong>现代语言模型通常依赖于从人类反馈中学习（RLHF）来鼓励安全行为。但是，由于三个关键局限性，它们仍然容易受到对抗性攻击的影响：（1）人类注释的效率低下和高成本，（2）潜在的对抗性攻击的大量多样性，以及（3）反馈偏见和奖励黑客黑客的风险。为了应对这些挑战，我们介绍了对抗性偏好学习（APL），这是一种结合了三个关键创新的迭代对抗训练方法。首先，基于模型的内在偏好概率的直接有害度度量，消除了对外部评估的依赖。其次，有条件的生成攻击者综合了输入特异性对手变化。第三，一个具有自动闭环反馈的迭代框架，通过脆弱性发现和缓解措施可以连续适应。对Mistral-7b-7b-Instruct-V0.3进行的实验表明，APL显着提高了鲁棒性，在基本模型中实现了83.33％的无害赢率（通过GPT-4O评估）（通过GPT-4O评估），将有害产量从5.88％降低到0.43％（通过Llama-Guard衡量），并根据Harment的攻击率降低到655％。值得注意的是，APL保持竞争性效用，MT基础台得分为6.59（可与基线6.78​​相当），而LC-Winrate则在基本模型上为46.52％。</li>
</ul>

<h3>Title: D2AF: A Dual-Driven Annotation and Filtering Framework for Visual Grounding</h3>
<ul>
<li><strong>Authors: </strong>Yichi Zhang, Gongwei Chen, Jun Zhu, Jia Wan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24372">https://arxiv.org/abs/2505.24372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24372">https://arxiv.org/pdf/2505.24372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24372]] D2AF: A Dual-Driven Annotation and Filtering Framework for Visual Grounding(https://arxiv.org/abs/2505.24372)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Visual Grounding is a task that aims to localize a target region in an image based on a free-form natural language description. With the rise of Transformer architectures, there is an increasing need for larger datasets to boost performance. However, the high cost of manual annotation poses a challenge, hindering the scale of data and the ability of large models to enhance their effectiveness. Previous pseudo label generation methods heavily rely on human-labeled captions of the original dataset, limiting scalability and diversity. To address this, we propose D2AF, a robust annotation framework for visual grounding using only input images. This approach overcomes dataset size limitations and enriches both the quantity and diversity of referring expressions. Our approach leverages multimodal large models and object detection models. By implementing dual-driven annotation strategies, we effectively generate detailed region-text pairs using both closed-set and open-set approaches. We further conduct an in-depth analysis of data quantity and data distribution. Our findings demonstrate that increasing data volume enhances model performance. However, the degree of improvement depends on how well the pseudo labels broaden the original data distribution. Based on these insights, we propose a consistency and distribution aware filtering method to further improve data quality by effectively removing erroneous and redundant data. This approach effectively eliminates noisy data, leading to improved performance. Experiments on three visual grounding tasks demonstrate that our method significantly improves the performance of existing models and achieves state-of-the-art results.</li>
<li><strong>摘要：</strong>视觉接地是一项旨在根据自由形式的自然语言描述将目标区域定位在图像中的任务。随着变压器体系结构的兴起，越来越需要更大的数据集来提高性能。但是，高成本的手动注释成本构成了挑战，阻碍了数据的规模以及大型模型提高其有效性的能力。以前的伪标签生成方法在很大程度上依赖于原始数据集的人体标签标题，从而限制了可扩展性和多样性。为了解决这个问题，我们提出了D2AF，这是仅使用输入图像的可视接地的强大注释框架。这种方法克服了数据集大小的限制，并丰富了参考表达式的数量和多样性。我们的方法利用多模式的大型模型和对象检测模型。通过实施双驱动的注释策略，我们使用封闭设置和开放设定的方法有效地生成了详细的区域文本对。我们进一步对数据数量和数据分布进行了深入的分析。我们的发现表明，增加数据量可以增强模型性能。但是，改进程度取决于伪标签拓宽原始数据分布的程度。基于这些见解，我们提出了一种一致性和分布意识的过滤方法，以通过有效删除错误和冗余数据来进一步提高数据质量。这种方法有效地消除了嘈杂的数据，从而提高了性能。在三个视觉接地任务上进行的实验表明，我们的方法显着提高了现有模型的性能并实现了最先进的结果。</li>
</ul>

<h3>Title: IRBridge: Solving Image Restoration Bridge with Pre-trained Generative Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Hanting Wang, Tao Jin, Wang Lin, Shulei Wang, Hai Huang, Shengpeng Ji, Zhou Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24406">https://arxiv.org/abs/2505.24406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24406">https://arxiv.org/pdf/2505.24406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24406]] IRBridge: Solving Image Restoration Bridge with Pre-trained Generative Diffusion Models(https://arxiv.org/abs/2505.24406)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generative</a></li>
<li><strong>Abstract: </strong>Bridge models in image restoration construct a diffusion process from degraded to clear images. However, existing methods typically require training a bridge model from scratch for each specific type of degradation, resulting in high computational costs and limited performance. This work aims to efficiently leverage pretrained generative priors within existing image restoration bridges to eliminate this requirement. The main challenge is that standard generative models are typically designed for a diffusion process that starts from pure noise, while restoration tasks begin with a low-quality image, resulting in a mismatch in the state distributions between the two processes. To address this challenge, we propose a transition equation that bridges two diffusion processes with the same endpoint distribution. Based on this, we introduce the IRBridge framework, which enables the direct utilization of generative models within image restoration bridges, offering a more flexible and adaptable approach to image restoration. Extensive experiments on six image restoration tasks demonstrate that IRBridge efficiently integrates generative priors, resulting in improved robustness and generalization performance. Code will be available at GitHub.</li>
<li><strong>摘要：</strong>图像恢复中的桥模型构建了从降解到清除图像的扩散过程。但是，现有方法通常需要从划痕的每种特定类型的退化训练桥梁模型，从而导致高计算成本和有限的性能。这项工作旨在有效利用现有图像恢复桥梁中预定的生成先验，以消除这一要求。主要的挑战是标准生成模型通常是为从纯噪声开始的扩散过程设计的，而恢复任务则以低质量图像开始，导致两个过程之间的状态分布不匹配。为了应对这一挑战，我们提出了一个过渡方程，该方程将两个扩散过程桥接具有相同的端点分布。基于此，我们介绍了IRBRIDGE框架，该框架可以直接利用图像修复桥中的生成模型，从而提供了更灵活，更适应性的图像修复方法。对六个图像恢复任务进行的广泛实验表明，Irbridge有效地整合了生成先验，从而改善了鲁棒性和泛化性能。代码将在Github上找到。</li>
</ul>

<h3>Title: Graph Flow Matching: Enhancing Image Generation with Neighbor-Aware Flow Fields</h3>
<ul>
<li><strong>Authors: </strong>Md Shahriar Rahim Siddiqui, Moshe Eliasof, Eldad Haber</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24434">https://arxiv.org/abs/2505.24434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24434">https://arxiv.org/pdf/2505.24434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24434]] Graph Flow Matching: Enhancing Image Generation with Neighbor-Aware Flow Fields(https://arxiv.org/abs/2505.24434)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Flow matching casts sample generation as learning a continuous-time velocity field that transports noise to data. Existing flow matching networks typically predict each point's velocity independently, considering only its location and time along its flow trajectory, and ignoring neighboring points. However, this pointwise approach may overlook correlations between points along the generation trajectory that could enhance velocity predictions, thereby improving downstream generation quality. To address this, we propose Graph Flow Matching (GFM), a lightweight enhancement that decomposes the learned velocity into a reaction term -- any standard flow matching network -- and a diffusion term that aggregates neighbor information via a graph neural module. This reaction-diffusion formulation retains the scalability of deep flow models while enriching velocity predictions with local context, all at minimal additional computational cost. Operating in the latent space of a pretrained variational autoencoder, GFM consistently improves Fréchet Inception Distance (FID) and recall across five image generation benchmarks (LSUN Church, LSUN Bedroom, FFHQ, AFHQ-Cat, and CelebA-HQ at $256\times256$), demonstrating its effectiveness as a modular enhancement to existing flow matching architectures.</li>
<li><strong>摘要：</strong>流量匹配的样本生成是学习连续的时速度场，该速度场将噪声传输到数据。现有的流量匹配网络通常独立地预测每个点的速度，仅考虑其沿其流轨迹的位置和时间，而忽略相邻点。但是，这种点的方法可能会忽略生成轨迹之间的点之间的相关性，从而可以增强速度预测，从而提高下游生成质量。为了解决这个问题，我们提出了图流量匹配（GFM），这是一种将学习速度分解为反应术语（任何标准流匹配网络）的轻巧增强，以及通过图神经模块聚集邻居信息的扩散项。这种反应扩散配方保留了深流模型的可扩展性，同时以局部环境丰富了速度预测，这都是以最小的额外计算成本。 GFM在预测的变异自动编码器的潜在空间中运行，始终在五个图像生成基准（LSUN教堂，LSUN卧室，FFHQ，AFHQ-CAT，AFHQ-CAT和Celeba-HQ和256美元的$ 256 \ TIME256 $）上，以有效的架构为型号，以表现为有效的增强型。</li>
</ul>

<h3>Title: SPPSFormer: High-quality Superpoint-based Transformer for Roof Plane Instance Segmentation from Point Clouds</h3>
<ul>
<li><strong>Authors: </strong>Cheng Zeng, Xiatian Qi, Chi Chen, Kai Sun, Wangle Zhang, Yuxuan Liu, Yan Meng, Bisheng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24475">https://arxiv.org/abs/2505.24475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24475">https://arxiv.org/pdf/2505.24475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24475]] SPPSFormer: High-quality Superpoint-based Transformer for Roof Plane Instance Segmentation from Point Clouds(https://arxiv.org/abs/2505.24475)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Transformers have been seldom employed in point cloud roof plane instance segmentation, which is the focus of this study, and existing superpoint Transformers suffer from limited performance due to the use of low-quality superpoints. To address this challenge, we establish two criteria that high-quality superpoints for Transformers should satisfy and introduce a corresponding two-stage superpoint generation process. The superpoints generated by our method not only have accurate boundaries, but also exhibit consistent geometric sizes and shapes, both of which greatly benefit the feature learning of superpoint Transformers. To compensate for the limitations of deep learning features when the training set size is limited, we incorporate multidimensional handcrafted features into the model. Additionally, we design a decoder that combines a Kolmogorov-Arnold Network with a Transformer module to improve instance prediction and mask extraction. Finally, our network's predictions are refined using traditional algorithm-based postprocessing. For evaluation, we annotated a real-world dataset and corrected annotation errors in the existing RoofN3D dataset. Experimental results show that our method achieves state-of-the-art performance on our dataset, as well as both the original and reannotated RoofN3D datasets. Moreover, our model is not sensitive to plane boundary annotations during training, significantly reducing the annotation burden. Through comprehensive experiments, we also identified key factors influencing roof plane segmentation performance: in addition to roof types, variations in point cloud density, density uniformity, and 3D point precision have a considerable impact. These findings underscore the importance of incorporating data augmentation strategies that account for point cloud quality to enhance model robustness under diverse and challenging conditions.</li>
<li><strong>摘要：</strong>变压器很少用于该研究的焦点云屋顶平面实例分割，这是本研究的重点，由于使用低质量的超级点，现有的Superpoint变形金刚的性能有限。为了应对这一挑战，我们建立了两个标准，即变压器的高质量超级点应满足并引入相应的两阶段超级点生成过程。我们方法生成的超级点不仅具有准确的边界，而且表现出一致的几何大小和形状，这两者都极大地使SuperPoint Transformers的特征学习受益。为了弥补训练集大小有限时深度学习功能的局限性，我们将多维手工制作的功能纳入模型中。此外，我们设计了一个解码器，将Kolmogorov-Arnold网络与变压器模块相结合，以改善实例预测和掩盖提取。最后，使用基于传统算法的后处理来完善我们的网络的预测。为了进行评估，我们注释了现实世界中的数据集并纠正了现有屋顶数据集中的注释错误。实验结果表明，我们的方法在我们的数据集以及原始屋顶数据集以及原始屋顶数据集上实现了最先进的性能。此外，我们的模型对训练过程中的平面边界注释不敏感，从而大大减轻了注释负担。通过全面的实验，我们还确定了影响屋顶平面分割性能的关键因素：除了屋顶类型，点云密度，密度均匀性和3D点精度的变化还具有相当大的影响。这些发现强调了结合数据增强策略的重要性，该策略解释了点云质量，以增强在多样化和具有挑战性的条件下的模型鲁棒性。</li>
</ul>

<h3>Title: Reason-SVG: Hybrid Reward RL for Aha-Moments in Vector Graphics Generation</h3>
<ul>
<li><strong>Authors: </strong>Ximing Xing, Yandong Guan, Jing Zhang, Dong Xu, Qian Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24499">https://arxiv.org/abs/2505.24499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24499">https://arxiv.org/pdf/2505.24499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24499]] Reason-SVG: Hybrid Reward RL for Aha-Moments in Vector Graphics Generation(https://arxiv.org/abs/2505.24499)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating high-quality Scalable Vector Graphics (SVGs) is challenging for Large Language Models (LLMs), as it requires advanced reasoning for structural validity, semantic faithfulness, and visual coherence -- capabilities in which current LLMs often fall short. In this work, we introduce Reason-SVG, a novel framework designed to enhance LLM reasoning for SVG generation. Reason-SVG pioneers the "Drawing-with-Thought" (DwT) paradigm, in which models generate both SVG code and explicit design rationales, mimicking the human creative process. Reason-SVG adopts a two-stage training strategy: First, Supervised Fine-Tuning (SFT) trains the LLM on the DwT paradigm to activate foundational reasoning abilities. Second, Reinforcement Learning (RL), utilizing Group Relative Policy Optimization (GRPO), empowers the model to generate both DwT and SVGs rationales through refined, reward-driven reasoning. To facilitate reasoning-driven SVG generation, we design a Hybrid Reward function that evaluates the presence and utility of DwT reasoning, along with structural validity, semantic alignment, and visual quality. We also introduce the SVGX-DwT-10k dataset, a high-quality corpus of 10,000 SVG-DwT pairs, where each SVG code is generated based on explicit DwT reasoning. By integrating DwT, SFT, and Hybrid Reward-guided RL, Reason-SVG significantly improves LLM performance in generating accurate and visually compelling SVGs, potentially fostering "Aha moments" in design.</li>
<li><strong>摘要：</strong>生成高质量的可伸缩矢量图形（SVG）对于大型语言模型（LLMS）来说是具有挑战性的，因为它需要用于结构有效性，语义忠诚和视觉连贯性的高级推理 - 当前LLMS通常不足的功能。在这项工作中，我们介绍了Reason-SVG，这是一个新颖的框架，旨在增强SVG生成的LLM推理。理性 -  SVG开创了“与绘画”（DWT）范式的“绘画”范式，其中模型同时生成了SVG代码和显式设计理由，模仿了人类的创造过程。 Reason-SVG采用了两阶段的培训策略：首先，有监督的微调（SFT）训练DWT范式上的LLM，以激活基础推理能力。其次，使用小组相对策略优化（GRPO）的强化学习（RL）赋予模型通过精致的，奖励驱动的推理来生成DWT和SVG的理由。为了促进推理驱动的SVG生成，我们设计了一种混合奖励功能，可以评估DWT推理的存在和实用性，以及结构有效性，语义对准和视觉质量。我们还介绍了SVGX-DWT-10K数据集，这是10,000个SVG-DWT对的高质量语料库，其中每个SVG代码都是基于显式DWT推理生成的。通过集成DWT，SFT和混合奖励指导的RL，推理SVG显着提高了LLM的性能，从而在产生准确和视觉上引人注目的SVG上，有可能促进设计中的“ aha arments”。</li>
</ul>

<h3>Title: Learning to Optimally Dispatch Power: Performance on a Nation-Wide Real-World Dataset</h3>
<ul>
<li><strong>Authors: </strong>Ignacio Boero, Santiago Diaz, Tomás Vázquez, Enzo Coppes, Pablo Belzarena, Federico Larroca</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24505">https://arxiv.org/abs/2505.24505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24505">https://arxiv.org/pdf/2505.24505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24505]] Learning to Optimally Dispatch Power: Performance on a Nation-Wide Real-World Dataset(https://arxiv.org/abs/2505.24505)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The Optimal Reactive Power Dispatch (ORPD) problem plays a crucial role in power system operations, ensuring voltage stability and minimizing power losses. Recent advances in machine learning, particularly within the ``learning to optimize'' framework, have enabled fast and efficient approximations of ORPD solutions, typically by training models on precomputed optimization results. While these approaches have demonstrated promising performance on synthetic datasets, their effectiveness under real-world grid conditions remains largely unexplored. This paper makes two key contributions. First, we introduce a publicly available power system dataset that includes both the structural characteristics of Uruguay's electrical grid and nearly two years of real-world operational data, encompassing actual demand and generation profiles. Given Uruguay's high penetration of renewable energy, the ORPD problem has become the primary optimization challenge in its power network. Second, we assess the impact of real-world data on learning-based ORPD solutions, revealing a significant increase in prediction errors when transitioning from synthetic to actual demand and generation inputs. Our results highlight the limitations of existing models in learning under the complex statistical properties of real grid conditions and emphasize the need for more expressive architectures. By providing this dataset, we aim to facilitate further research into robust learning-based optimization techniques for power system management.</li>
<li><strong>摘要：</strong>最佳的反应电源调度（ORPD）问题在功率系统操作中起着至关重要的作用，从而确保电压稳定性并最大程度地减少功率损耗。机器学习的最新进展，尤其是在``学习优化''框架中的最新进展，已实现了ORPD解决方案的快速有效近似，通常是通过对预先计算的优化结果进行培训模型。尽管这些方法在合成数据集上表现出了有希望的性能，但在现实世界中的网格条件下，它们的有效性在很大程度上尚未得到探索。本文做出了两个关键的贡献。首先，我们引入了一个公开可用的电力系统数据集，其中包括乌拉圭电网的结构特征，以及将近两年的现实操作数据，包括实际需求和发电概况。鉴于乌拉圭对可再生能源的高渗透，ORPD问题已成为其电力网络中的主要优化挑战。其次，我们评估了现实数据对基于学习的ORPD解决方案的影响，从而揭示了从合成到实际需求和发电输入过渡时预测错误的显着增加。我们的结果强调了现有模型在实际网格条件的复杂统计特性下学习的局限性，并强调了对更具表现力的建筑的需求。通过提供此数据集，我们旨在促进对电力系统管理基于强大的学习优化技术的进一步研究。</li>
</ul>

<h3>Title: Airborne Neural Network</h3>
<ul>
<li><strong>Authors: </strong>Paritosh Ranjan, Surajit Majumder, Prodip Roy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24513">https://arxiv.org/abs/2505.24513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24513">https://arxiv.org/pdf/2505.24513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24513]] Airborne Neural Network(https://arxiv.org/abs/2505.24513)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Deep Learning, driven by neural networks, has led to groundbreaking advancements in Artificial Intelligence by enabling systems to learn and adapt like the human brain. These models have achieved remarkable results, particularly in data-intensive domains, supported by massive computational infrastructure. However, deploying such systems in Aerospace, where real time data processing and ultra low latency are critical, remains a challenge due to infrastructure limitations. This paper proposes a novel concept: the Airborne Neural Network a distributed architecture where multiple airborne devices each host a subset of neural network neurons. These devices compute collaboratively, guided by an airborne network controller and layer specific controllers, enabling real-time learning and inference during flight. This approach has the potential to revolutionize Aerospace applications, including airborne air traffic control, real-time weather and geographical predictions, and dynamic geospatial data processing. By enabling large-scale neural network operations in airborne environments, this work lays the foundation for the next generation of AI powered Aerospace systems.</li>
<li><strong>摘要：</strong>由神经网络驱动的深度学习，通过使系统能够像人脑一样学习和适应人工智能的突破性进步。这些模型取得了显着的结果，尤其是在数据密集型域中，并得到了大规模的计算基础架构的支持。但是，由于基础设施限制，在实时数据处理和超低潜伏期至关重要的航空航天中部署此类系统仍然是一个挑战。本文提出了一个新颖的概念：空中神经网络一个分布式建筑，其中多个空降设备每个设备都有神经网络神经元的子集。这些设备在机载网络控制器和特定控制器的指导下进行协作进行协作，从而实现了飞行过程中的实时学习和推理。这种方法有可能彻底改变航空航天应用，包括空中空中交通管制，实时天气和地理预测以及动态的地理空间数据处理。通过在空降环境中启用大规模的神经网络操作，这项工作为下一代AI动力航空系统奠定了基础。</li>
</ul>

<h3>Title: un$^2$CLIP: Improving CLIP's Visual Detail Capturing Ability via Inverting unCLIP</h3>
<ul>
<li><strong>Authors: </strong>Yinqi Li, Jiahe Zhao, Hong Chang, Ruibing Hou, Shiguang Shan, Xilin Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24517">https://arxiv.org/abs/2505.24517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24517">https://arxiv.org/pdf/2505.24517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24517]] un$^2$CLIP: Improving CLIP's Visual Detail Capturing Ability via Inverting unCLIP(https://arxiv.org/abs/2505.24517)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Contrastive Language-Image Pre-training (CLIP) has become a foundation model and has been applied to various vision and multimodal tasks. However, recent works indicate that CLIP falls short in distinguishing detailed differences in images and shows suboptimal performance on dense-prediction and vision-centric multimodal tasks. Therefore, this work focuses on improving existing CLIP models, aiming to capture as many visual details in images as possible. We find that a specific type of generative models, unCLIP, provides a suitable framework for achieving our goal. Specifically, unCLIP trains an image generator conditioned on the CLIP image embedding. In other words, it inverts the CLIP image encoder. Compared to discriminative models like CLIP, generative models are better at capturing image details because they are trained to learn the data distribution of images. Additionally, the conditional input space of unCLIP aligns with CLIP's original image-text embedding space. Therefore, we propose to invert unCLIP (dubbed un$^2$CLIP) to improve the CLIP model. In this way, the improved image encoder can gain unCLIP's visual detail capturing ability while preserving its alignment with the original text encoder simultaneously. We evaluate our improved CLIP across various tasks to which CLIP has been applied, including the challenging MMVP-VLM benchmark, the dense-prediction open-vocabulary segmentation task, and multimodal large language model tasks. Experiments show that un$^2$CLIP significantly improves the original CLIP and previous CLIP improvement methods. Code and models will be available at this https URL.</li>
<li><strong>摘要：</strong>对比语言图像预训练（剪辑）已成为基础模型，并已应用于各种愿景和多模式任务。然而，最近的作品表明，剪辑在区分图像的详细差异方面缺乏，并在密集预测和以视觉为中心的多模式任务上表现出次优性能。因此，这项工作着重于改进现有的剪辑模型，旨在捕获图像中尽可能多的视觉细节。我们发现，特定类型的生成模型Unclip为实现我们的目标提供了合适的框架。具体而言，Unclip训练图像发生器在剪辑图像嵌入的条件下。换句话说，它会反转剪辑图像编码器。与剪辑（例如剪辑）相比，生成模型更好地捕获图像细节，因为它们经过培训以了解图像的数据分布。此外，Unclip的条件输入空间与夹子的原始图像文本嵌入空间对齐。因此，我们建议将Unverve unvell（称为Un $^2 $夹）改进剪辑模型。这样，改进的图像编码器可以同时保留其与原始文本编码器的对齐方式，从而获得Unclip的视觉细节捕获能力。我们评估了已应用剪辑的各种任务的改进剪辑，包括具有挑战性的MMVP-VLM基准，密集预测的开放式录音率细分任务以及多模式的大语言模型任务。实验表明，UN $^2 $夹显着改善了原始剪辑和以前的剪辑改进方法。代码和型号将在此HTTPS URL上可用。</li>
</ul>

<h3>Title: AMIA: Automatic Masking and Joint Intention Analysis Makes LVLMs Robust Jailbreak Defenders</h3>
<ul>
<li><strong>Authors: </strong>Yuqi Zhang, Yuchun Miao, Zuchao Li, Liang Ding</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24519">https://arxiv.org/abs/2505.24519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24519">https://arxiv.org/pdf/2505.24519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24519]] AMIA: Automatic Masking and Joint Intention Analysis Makes LVLMs Robust Jailbreak Defenders(https://arxiv.org/abs/2505.24519)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce AMIA, a lightweight, inference-only defense for Large Vision-Language Models (LVLMs) that (1) Automatically Masks a small set of text-irrelevant image patches to disrupt adversarial perturbations, and (2) conducts joint Intention Analysis to uncover and mitigate hidden harmful intents before response generation. Without any retraining, AMIA improves defense success rates across diverse LVLMs and jailbreak benchmarks from an average of 52.4% to 81.7%, preserves general utility with only a 2% average accuracy drop, and incurs only modest inference overhead. Ablation confirms both masking and intention analysis are essential for a robust safety-utility trade-off.</li>
<li><strong>摘要：</strong>我们介绍了对大型视觉模型（LVLMS）的轻巧，仅推理的防御，该防御（1）自动掩盖了一小部分文本 - 近视图像贴片以破坏对抗性扰动，（2）进行关节意图分析以发现并减轻响应生成之前的隐藏有害意图。没有任何重新培训，AMIA将各种LVLM和越狱基准的国防成功率从平均52.4％到81.7％，仅平均准确度下降了2％，仅带有适度推理的开销。消融证实掩盖和意图分析对于强大的安全性权衡至关重要。</li>
</ul>

<h3>Title: UniGeo: Taming Video Diffusion for Unified Consistent Geometry Estimation</h3>
<ul>
<li><strong>Authors: </strong>Yang-Tian Sun, Xin Yu, Zehuan Huang, Yi-Hua Huang, Yuan-Chen Guo, Ziyi Yang, Yan-Pei Cao, Xiaojuan Qi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24521">https://arxiv.org/abs/2505.24521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24521">https://arxiv.org/pdf/2505.24521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24521]] UniGeo: Taming Video Diffusion for Unified Consistent Geometry Estimation(https://arxiv.org/abs/2505.24521)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recently, methods leveraging diffusion model priors to assist monocular geometric estimation (e.g., depth and normal) have gained significant attention due to their strong generalization ability. However, most existing works focus on estimating geometric properties within the camera coordinate system of individual video frames, neglecting the inherent ability of diffusion models to determine inter-frame correspondence. In this work, we demonstrate that, through appropriate design and fine-tuning, the intrinsic consistency of video generation models can be effectively harnessed for consistent geometric estimation. Specifically, we 1) select geometric attributes in the global coordinate system that share the same correspondence with video frames as the prediction targets, 2) introduce a novel and efficient conditioning method by reusing positional encodings, and 3) enhance performance through joint training on multiple geometric attributes that share the same correspondence. Our results achieve superior performance in predicting global geometric attributes in videos and can be directly applied to reconstruction tasks. Even when trained solely on static video data, our approach exhibits the potential to generalize to dynamic video scenes.</li>
<li><strong>摘要：</strong>最近，利用扩散模型先验的方法来帮助单眼几何估计（例如，深度和正常），由于其强大的概括能力，已引起了很大的关注。但是，大多数现有的作品都集中在估计单个视频帧相机坐标系中的几何属性上，从而忽略了扩散模型确定框架间对应关系的固有能力。在这项工作中，我们证明，通过适当的设计和微调，可以有效利用视频生成模型的固有一致性，以进行一致的几何估计。具体而言，我们1）在全球坐标系中选择与预测目标共享相同对应关系的几何属性，2）通过重复使用位置编码来引入一种新颖而有效的条件方法，3）3）通过对共享相同对应关系的多个几何属性上的联合培训来增强性能。我们的结果在预测视频中的全局几何属性方面取得了卓越的性能，并且可以直接应用于重建任务。即使仅根据静态视频数据进行培训，我们的方法也表现出有可能推广到动态视频场景的潜力。</li>
</ul>

<h3>Title: AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for Auto-Generating Chemical Process and Instrumentation Diagrams</h3>
<ul>
<li><strong>Authors: </strong>Sakhinana Sagar Srinivas, Shivam Gupta, Venkataramana Runkana</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24584">https://arxiv.org/abs/2505.24584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24584">https://arxiv.org/pdf/2505.24584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24584]] AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for Auto-Generating Chemical Process and Instrumentation Diagrams(https://arxiv.org/abs/2505.24584)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in generative AI have accelerated the discovery of novel chemicals and materials; however, transitioning these discoveries to industrial-scale production remains a critical bottleneck, as it requires the development of entirely new chemical manufacturing processes. Current AI methods cannot auto-generate PFDs or PIDs, despite their critical role in scaling chemical processes, while adhering to engineering constraints. We present a closed loop, physics aware framework for the automated generation of industrially viable PFDs and PIDs. The framework integrates domain specialized small scale language models (SLMs) (trained for chemical process QA tasks) with first principles simulation, leveraging three key components: (1) a hierarchical knowledge graph of process flow and instrumentation descriptions for 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes domain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Retrieval-Augmented Instruction Tuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure feasibility. To improve both runtime efficiency and model compactness, the framework incorporates advanced inference time optimizations including FlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization, and Test Time Inference Scaling and independently applies structural pruning techniques (width and depth) guided by importance heuristics to reduce model size with minimal accuracy loss. Experiments demonstrate that the framework generates simulator-validated process descriptions with high fidelity, outperforms baseline methods in correctness, and generalizes to unseen chemicals. By bridging AI-driven design with industrial-scale feasibility, this work significantly reduces R&D timelines from lab discovery to plant deployment.</li>
<li><strong>摘要：</strong>生成AI的最新进展加速了新型化学物质和材料的发现。但是，将这些发现过渡到工业规模的生产仍然是一个关键的瓶颈，因为它需要开发全新的化学制造工艺。当前的AI方法在遵守工程限制的同时，尽管它们在缩放化学过程中的作用至关重要，但仍无法自动产生PFD或PID。我们为自动生成工业可行的PFD和PID提供了一个封闭的循环，物理意识框架。 The framework integrates domain specialized small scale language models (SLMs) (trained for chemical process QA tasks) with first principles simulation, leveraging three key components: (1) a hierarchical knowledge graph of process flow and instrumentation descriptions for 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes domain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT), Direct Preference Optimization （DPO）和检索提示的指令调整（RAIT）和（3）基于DWSIM的模拟器在循环验证中以确保可行性。为了提高运行时效率和模型的紧凑性，该框架结合了高级推理时间优化，包括闪光发音，lookahead解码，使用KV-CACHE量化的打法以及测试时间推理量表，并独立应用结构修剪技术（宽度和深度），以减少模型尺寸降低最小值损失的重要性持久性。实验表明，该框架以高保真度生成模拟器验证的过程描述，优于正确性的基线方法，并推广到看不见的化学物质。通过将AI驱动的设计与工业规模的可行性桥接，这项工作大大减少了从实验室发现到植物部署的研发时间表。</li>
</ul>

<h3>Title: SARD: A Large-Scale Synthetic Arabic OCR Dataset for Book-Style Text Recognition</h3>
<ul>
<li><strong>Authors: </strong>Omer Nacar, Yasser Al-Habashi, Serry Sibaee, Adel Ammar, Wadii Boulila</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24600">https://arxiv.org/abs/2505.24600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24600">https://arxiv.org/pdf/2505.24600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24600]] SARD: A Large-Scale Synthetic Arabic OCR Dataset for Book-Style Text Recognition(https://arxiv.org/abs/2505.24600)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Arabic Optical Character Recognition (OCR) is essential for converting vast amounts of Arabic print media into digital formats. However, training modern OCR models, especially powerful vision-language models, is hampered by the lack of large, diverse, and well-structured datasets that mimic real-world book layouts. Existing Arabic OCR datasets often focus on isolated words or lines or are limited in scale, typographic variety, or structural complexity found in books. To address this significant gap, we introduce SARD (Large-Scale Synthetic Arabic OCR Dataset). SARD is a massive, synthetically generated dataset specifically designed to simulate book-style documents. It comprises 843,622 document images containing 690 million words, rendered across ten distinct Arabic fonts to ensure broad typographic coverage. Unlike datasets derived from scanned documents, SARD is free from real-world noise and distortions, offering a clean and controlled environment for model training. Its synthetic nature provides unparalleled scalability and allows for precise control over layout and content variation. We detail the dataset's composition and generation process and provide benchmark results for several OCR models, including traditional and deep learning approaches, highlighting the challenges and opportunities presented by this dataset. SARD serves as a valuable resource for developing and evaluating robust OCR and vision-language models capable of processing diverse Arabic book-style texts.</li>
<li><strong>摘要：</strong>阿拉伯光学特征识别（OCR）对于将大量阿拉伯印刷媒体转换为数字格式至关重要。但是，培训现代的OCR模型，尤其是强大的视觉语言模型，由于缺乏模仿现实世界中的图书布局而缺乏大型，多样化且结构良好的数据集所阻碍。现有的阿拉伯OCR数据集通常专注于孤立的单词或线条，或者在书籍中发现的规模，印刷变化或结构复杂性的限制。为了解决这一重大差距，我们介绍了SAD（大规模合成阿拉伯OCR数据集）。 Sard是一个庞大的合成生成的数据集，专门为模拟书籍式文档而设计。它包含843,622个文档图像，其中包含6.9亿个单词，这些单词在十个不同的阿拉伯字体上呈现，以确保广泛的印刷覆盖范围。与从扫描文档中得出的数据集不同，Sard没有现实世界的噪音和扭曲，提供了用于模型培训的干净和控制的环境。它的合成性质提供了无与伦比的可伸缩性，并允许精确控制布局和内容变化。我们详细介绍了数据集的组成和生成过程，并为多种OCR模型（包括传统和深度学习方法）提供基准结果，突出了该数据集提出的挑战和机遇。 Sard是开发和评估能够处理多种阿拉伯书籍式文本的强大的OCR和视觉模型的宝贵资源。</li>
</ul>

<h3>Title: HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts</h3>
<ul>
<li><strong>Authors: </strong>Neil He, Rishabh Anand, Hiren Madhu, Ali Maatouk, Smita Krishnaswamy, Leandros Tassiulas, Menglin Yang, Rex Ying</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24722">https://arxiv.org/abs/2505.24722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24722">https://arxiv.org/pdf/2505.24722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24722]] HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts(https://arxiv.org/abs/2505.24722)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown great success in text modeling tasks across domains. However, natural language exhibits inherent semantic hierarchies and nuanced geometric structure, which current LLMs do not capture completely owing to their reliance on Euclidean operations. Recent studies have also shown that not respecting the geometry of token embeddings leads to training instabilities and degradation of generative capabilities. These findings suggest that shifting to non-Euclidean geometries can better align language models with the underlying geometry of text. We thus propose to operate fully in Hyperbolic space, known for its expansive, scale-free, and low-distortion properties. We thus introduce HELM, a family of HypErbolic Large Language Models, offering a geometric rethinking of the Transformer-based LLM that addresses the representational inflexibility, missing set of necessary operations, and poor scalability of existing hyperbolic LMs. We additionally introduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert operates in a distinct curvature space to encode more fine-grained geometric structure from text, as well as a dense model, HELM-D. For HELM-MICE, we further develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient, reduced-KV-cache training and inference. For both models, we develop essential hyperbolic equivalents of rotary positional encodings and RMS normalization. We are the first to train fully hyperbolic LLMs at billion-parameter scale, and evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM problem-solving, general knowledge, and commonsense reasoning. Our results show consistent gains from our HELM architectures -- up to 4% -- over popular Euclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy and enhanced reasoning afforded by hyperbolic geometry in large-scale LM pretraining.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）在跨域的文本建模任务方面取得了巨大成功。但是，自然语言表现出固有的语义层次结构和细微的几何结构，由于它们对欧几里得操作的依赖，目前的LLM并未完全捕获。最近的研究还表明，不尊重令牌嵌入的几何形状会导致训练不稳定性和生成能力的降解。这些发现表明，转移到非欧几里得几何形状可以使语言模型与文本的基本几何形状更好地结合。因此，我们建议在双曲线空间中充分运行，以其膨胀，无尺度和低缩度特性而闻名。因此，我们介绍了Helm，这是一个双曲线大型语言模型的家族，提供了基于变压器的LLM的几何思考，以解决代表性的不灵活性，缺少必要操作的缺失以及现有双曲线LMS的可扩展性差。我们还引入了壮天的专家模型Helm-Mice，每个专家都在不同的曲率空间中运行，从文本和密集的模型Helm-D中编码更细颗粒的几何结构。对于舵手，我们进一步发展了双曲线多头的潜在注意力（HMLA），以进行有效的，降低的KV-CACHE训练和推理。对于这两种模型，我们都会开发出旋转位置编码和RMS归一化的基本双曲线当量。我们是第一个以十亿参数量表训练完全双曲线LLM的人，并在诸如MMLU和ARC等众所周知的基准上对其进行评估，跨越STEM解决问题，常识和常识性推理。我们的结果表明，与在大规模LM预处理中双曲线几何形状相比，我们的掌舵体系结构从掌舵体系结构（高达4％）持续增长，高达4％ - 高达乳腺和DeepSeek的流行结构。</li>
</ul>

<h3>Title: DreamDance: Animating Character Art via Inpainting Stable Gaussian Worlds</h3>
<ul>
<li><strong>Authors: </strong>Jiaxu Zhang, Xianfang Zeng, Xin Chen, Wei Zuo, Gang Yu, Guosheng Lin, Zhigang Tu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24733">https://arxiv.org/abs/2505.24733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24733">https://arxiv.org/pdf/2505.24733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24733]] DreamDance: Animating Character Art via Inpainting Stable Gaussian Worlds(https://arxiv.org/abs/2505.24733)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This paper presents DreamDance, a novel character art animation framework capable of producing stable, consistent character and scene motion conditioned on precise camera trajectories. To achieve this, we re-formulate the animation task as two inpainting-based steps: Camera-aware Scene Inpainting and Pose-aware Video Inpainting. The first step leverages a pre-trained image inpainting model to generate multi-view scene images from the reference art and optimizes a stable large-scale Gaussian field, which enables coarse background video rendering with camera trajectories. However, the rendered video is rough and only conveys scene motion. To resolve this, the second step trains a pose-aware video inpainting model that injects the dynamic character into the scene video while enhancing background quality. Specifically, this model is a DiT-based video generation model with a gating strategy that adaptively integrates the character's appearance and pose information into the base background video. Through extensive experiments, we demonstrate the effectiveness and generalizability of DreamDance, producing high-quality and consistent character animations with remarkable camera dynamics.</li>
<li><strong>摘要：</strong>本文介绍了DreamDance，这是一个新颖的角色艺术动画框架，能够在精确的相机轨迹上产生稳定的，一致的角色和场景运动。为了实现这一目标，我们将动画任务重新构建为两个基于介绍的步骤：摄像头感知场景介绍和姿势感知的视频介绍。第一步利用预先训练的图像介绍模型从参考艺术中生成多视图场景图像，并优化了稳定的大型高斯字段，从而可以使用摄像头轨迹进行粗糙的背景视频渲染。但是，渲染的视频很粗糙，只传达了场景动作。为了解决这个问题，第二步训练了一个姿势感知的视频介绍模型，该模型将动态角色注入场景视频，同时增强背景质量。具体而言，该模型是一种基于DIT的视频生成模型，具有门控策略，可将角色的外观自适应地整合到基本背景视频中。通过广泛的实验，我们证明了梦幻般的有效性和概括性，以出色的相机动力制作了高质量且一致的角色动画。</li>
</ul>

<h3>Title: REASONING GYM: Reasoning Environments for Reinforcement Learning with Verifiable Rewards</h3>
<ul>
<li><strong>Authors: </strong>Zafir Stojanovski, Oliver Stanley, Joe Sharratt, Richard Jones, Abdulhakeem Adefioye, Jean Kaddour, Andreas Köpf</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24760">https://arxiv.org/abs/2505.24760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24760">https://arxiv.org/pdf/2505.24760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24760]] REASONING GYM: Reasoning Environments for Reinforcement Learning with Verifiable Rewards(https://arxiv.org/abs/2505.24760)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce Reasoning Gym (RG), a library of reasoning environments for reinforcement learning with verifiable rewards. It provides over 100 data generators and verifiers spanning multiple domains including algebra, arithmetic, computation, cognition, geometry, graph theory, logic, and various common games. Its key innovation is the ability to generate virtually infinite training data with adjustable complexity, unlike most previous reasoning datasets, which are typically fixed. This procedural generation approach allows for continuous evaluation across varying difficulty levels. Our experimental results demonstrate the efficacy of RG in both evaluating and reinforcement learning of reasoning models.</li>
<li><strong>摘要：</strong>我们介绍了推理体育馆（RG），这是一个以可验证的奖励的推理环境库。它提供了100多个数据生成器和验证仪，这些数据生成器和验证者涵盖了多个域，包括代数，算术，计算，认知，几何学，图理论，逻辑和各种常见游戏。它的关键创新是能够以可调节的复杂性生成几乎无限的培训数据，这与大多数以前的推理数据集不同，这些数据集通常是固定的。这种程序生成方法允许在不同的难度水平上进行持续评估。我们的实验结果表明，RG在评估和加强推理模型中的功效。</li>
</ul>

<h3>Title: Diffusion-Based Symbolic Regression</h3>
<ul>
<li><strong>Authors: </strong>Zachary Bastiani, Robert M. Kirby, Jacob Hochhalter, Shandian Zhe</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24776">https://arxiv.org/abs/2505.24776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24776">https://arxiv.org/pdf/2505.24776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24776]] Diffusion-Based Symbolic Regression(https://arxiv.org/abs/2505.24776)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion has emerged as a powerful framework for generative modeling, achieving remarkable success in applications such as image and audio synthesis. Enlightened by this progress, we propose a novel diffusion-based approach for symbolic regression. We construct a random mask-based diffusion and denoising process to generate diverse and high-quality equations. We integrate this generative processes with a token-wise Group Relative Policy Optimization (GRPO) method to conduct efficient reinforcement learning on the given measurement dataset. In addition, we introduce a long short-term risk-seeking policy to expand the pool of top-performing candidates, further enhancing performance. Extensive experiments and ablation studies have demonstrated the effectiveness of our approach.</li>
<li><strong>摘要：</strong>扩散已成为生成建模的强大框架，在图像和音频综合等应用中取得了巨大的成功。在这个进步的启发下，我们提出了一种基于扩散的新方法来进行符号回归。我们构建一个基于掩模的扩散和降解过程，以产生多样化和高质量的方程。我们将这种生成过程与代币的相对策略优化（GRPO）方法集成在一起，以在给定的测量数据集上进行有效的增强学习。此外，我们引入了一项漫长的短期寻求风险政策，以扩大表现最佳的候选人的库，从而进一步提高绩效。广泛的实验和消融研究已经证明了我们方法的有效性。</li>
</ul>

<h3>Title: EVA-MILP: Towards Standardized Evaluation of MILP Instance Generation</h3>
<ul>
<li><strong>Authors: </strong>Yidong Luo, Chenguang Wang, Jiahao Yang, Fanzeng Xia, Tianshu Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24779">https://arxiv.org/abs/2505.24779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24779">https://arxiv.org/pdf/2505.24779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24779]] EVA-MILP: Towards Standardized Evaluation of MILP Instance Generation(https://arxiv.org/abs/2505.24779)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Mixed-Integer Linear Programming (MILP) is fundamental to solving complex decision-making problems. The proliferation of MILP instance generation methods, driven by machine learning's demand for diverse optimization datasets and the limitations of static benchmarks, has significantly outpaced standardized evaluation techniques. Consequently, assessing the fidelity and utility of synthetic MILP instances remains a critical, multifaceted challenge. This paper introduces a comprehensive benchmark framework designed for the systematic and objective evaluation of MILP instance generation methods. Our framework provides a unified and extensible methodology, assessing instance quality across crucial dimensions: mathematical validity, structural similarity, computational hardness, and utility in downstream machine learning tasks. A key innovation is its in-depth analysis of solver-internal features -- particularly by comparing distributions of key solver outputs including root node gap, heuristic success rates, and cut plane usage -- leveraging the solver's dynamic solution behavior as an `expert assessment' to reveal nuanced computational resemblances. By offering a structured approach with clearly defined solver-independent and solver-dependent metrics, our benchmark aims to facilitate robust comparisons among diverse generation techniques, spur the development of higher-quality instance generators, and ultimately enhance the reliability of research reliant on synthetic MILP data. The framework's effectiveness in systematically comparing the fidelity of instance sets is demonstrated using contemporary generative models.</li>
<li><strong>摘要：</strong>混合工作者线性编程（MILP）是解决复杂决策问题的基础。 MILP实例生成方法的扩散是由机器学习对各种优化数据集的需求以及静态基准的局限性驱动的，其标准化评估技术大大超过了标准化的评估技术。因此，评估合成MILP实例的保真度和效用仍然是一个至关重要的，多方面的挑战。本文介绍了一个全面的基准框架，旨在对MILP实例生成方法进行系统和客观评估。我们的框架提供了一种统一且可扩展​​的方法，评估了跨关键维度的实例质量：在下游机器学习任务中的数学有效性，结构相似性，计算硬度和效用。一个关键的创新是其对求解器内部特征的深入分析 - 尤其是通过比较关键求解器输出的分布，包括根节点间隙，启发式成功率和剪切平面的使用 - 利用求解器的动态解决方案行为作为“专家评估”来揭示细微的计算量。通过提供具有明确定义的求解器独立和求解器依赖性指标的结构化方法，我们的基准旨在促进多样化的生成技术之间的稳健比较，刺激更高质量实例生成器的发展，并最终增强了依赖于合成MILP数据的研究的可靠性。该框架在系统地比较实例集的忠诚度中的有效性是使用当代生成模型证明的。</li>
</ul>

<h3>Title: QGAN-based data augmentation for hybrid quantum-classical neural networks</h3>
<ul>
<li><strong>Authors: </strong>Run-Ze He, Jun-Jian Su, Su-Juan Qin, Zheng-Ping Jin, Fei Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24780">https://arxiv.org/abs/2505.24780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24780">https://arxiv.org/pdf/2505.24780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24780]] QGAN-based data augmentation for hybrid quantum-classical neural networks(https://arxiv.org/abs/2505.24780)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Quantum neural networks converge faster and achieve higher accuracy than classical models. However, data augmentation in quantum machine learning remains underexplored. To tackle data scarcity, we integrate quantum generative adversarial networks (QGANs) with hybrid quantum-classical neural networks (HQCNNs) to develop an augmentation framework. We propose two strategies: a general approach to enhance data processing and classification across HQCNNs, and a customized strategy that dynamically generates samples tailored to the HQCNN's performance on specific data categories, improving its ability to learn from complex datasets. Simulation experiments on the MNIST dataset demonstrate that QGAN outperforms traditional data augmentation methods and classical GANs. Compared to baseline DCGAN, QGAN achieves comparable performance with half the parameters, balancing efficiency and effectiveness. This suggests that QGANs can simplify models and generate high-quality data, enhancing HQCNN accuracy and performance. These findings pave the way for applying quantum data augmentation techniques in machine learning.</li>
<li><strong>摘要：</strong>量子神经网络比经典模型更快地收敛并获得更高的精度。但是，量子机学习中的数据增强尚未被逐渐解散。为了解决数据稀缺性，我们将量子生成对抗网络（QGANS）与混合量子古典神经网络（HQCNN）集成在一起，以开发增强框架。我们提出了两种策略：一种加强HQCNN数据处理和分类的一般方法，以及一种自定义策略，该策略动态生成针对HQCNN在特定数据类别上的性能量身定制的样品，从而提高了其从复杂数据集中学习的能力。 MNIST数据集上的仿真实验表明，QGAN的表现优于传统数据增强方法和经典gan。与基线DCGAN相比，Qgan与一半参数，平衡效率和有效性达到了可比的性能。这表明QGAN可以简化模型并生成高质量的数据，从而提高HQCNN的准确性和性能。这些发现铺平了在机器学习中应用量子数据增强技术的方式。</li>
</ul>

<h3>Title: Draw ALL Your Imagine: A Holistic Benchmark and Agent Framework for Complex Instruction-based Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Yucheng Zhou, Jiahao Yuan, Qianning Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24787">https://arxiv.org/abs/2505.24787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24787">https://arxiv.org/pdf/2505.24787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24787]] Draw ALL Your Imagine: A Holistic Benchmark and Agent Framework for Complex Instruction-based Image Generation(https://arxiv.org/abs/2505.24787)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, quality assessment</a></li>
<li><strong>Abstract: </strong>Recent advancements in text-to-image (T2I) generation have enabled models to produce high-quality images from textual descriptions. However, these models often struggle with complex instructions involving multiple objects, attributes, and spatial relationships. Existing benchmarks for evaluating T2I models primarily focus on general text-image alignment and fail to capture the nuanced requirements of complex, multi-faceted prompts. Given this gap, we introduce LongBench-T2I, a comprehensive benchmark specifically designed to evaluate T2I models under complex instructions. LongBench-T2I consists of 500 intricately designed prompts spanning nine diverse visual evaluation dimensions, enabling a thorough assessment of a model's ability to follow complex instructions. Beyond benchmarking, we propose an agent framework (Plan2Gen) that facilitates complex instruction-driven image generation without requiring additional model training. This framework integrates seamlessly with existing T2I models, using large language models to interpret and decompose complex prompts, thereby guiding the generation process more effectively. As existing evaluation metrics, such as CLIPScore, fail to adequately capture the nuances of complex instructions, we introduce an evaluation toolkit that automates the quality assessment of generated images using a set of multi-dimensional metrics. The data and code are released at this https URL.</li>
<li><strong>摘要：</strong>文本到图像（T2i）生成的最新进展使模型能够从文本描述中产生高质量的图像。但是，这些模型通常在涉及多个对象，属性和空间关系的复杂指令中挣扎。评估T2I模型的现有基准主要集中在一般文本图像对齐方式上，并且无法捕获复杂的，多方面的提示的细微要求。鉴于此差距，我们介绍了Longbench-T2I，这是一种全面的基准测试，该基准专门旨在在复杂的指令下评估T2I模型。 Longbench-T2I由500个复杂设计的提示组成，涵盖了九种不同的视觉评估维度，从而可以彻底评估模型遵循复杂指令的能力。除了基准测试之外，我们还提出了一个代理框架（PLAN2GEN），该框架促进了复杂的指令驱动的图像生成，而无需进行其他模型培训。该框架与现有T2I模型无缝集成，使用大型语言模型来解释和分解复杂的提示，从而更有效地指导生成过程。由于现有的评估指标（例如夹克）无法充分捕获复杂说明的细微差别，因此我们引入了一个评估工具包，该工具包可以使用一组多维指标对生成图像的质量评估自动化。数据和代码在此HTTPS URL上发布。</li>
</ul>

<h3>Title: Inference Acceleration of Autoregressive Normalizing Flows by Selective Jacobi Decoding</h3>
<ul>
<li><strong>Authors: </strong>Jiaru Zhang, Juanwu Lu, Ziran Wang, Ruqi Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24791">https://arxiv.org/abs/2505.24791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24791">https://arxiv.org/pdf/2505.24791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24791]] Inference Acceleration of Autoregressive Normalizing Flows by Selective Jacobi Decoding(https://arxiv.org/abs/2505.24791)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Normalizing flows are promising generative models with advantages such as theoretical rigor, analytical log-likelihood computation, and end-to-end training. However, the architectural constraints to ensure invertibility and tractable Jacobian computation limit their expressive power and practical usability. Recent advancements utilize autoregressive modeling, significantly enhancing expressive power and generation quality. However, such sequential modeling inherently restricts parallel computation during inference, leading to slow generation that impedes practical deployment. In this paper, we first identify that strict sequential dependency in inference is unnecessary to generate high-quality samples. We observe that patches in sequential modeling can also be approximated without strictly conditioning on all preceding patches. Moreover, the models tend to exhibit low dependency redundancy in the initial layer and higher redundancy in subsequent layers. Leveraging these observations, we propose a selective Jacobi decoding (SeJD) strategy that accelerates autoregressive inference through parallel iterative optimization. Theoretical analyses demonstrate the method's superlinear convergence rate and guarantee that the number of iterations required is no greater than the original sequential approach. Empirical evaluations across multiple datasets validate the generality and effectiveness of our acceleration technique. Experiments demonstrate substantial speed improvements up to 4.7 times faster inference while keeping the generation quality and fidelity.</li>
<li><strong>摘要：</strong>归一化的流是具有有希望的生成模型，具有理论严格，分析对数类似计算和端到端训练等优点。但是，确保可逆性和可拖动的雅各布计算限制了其表达能力和实际可用性。最近的进步利用自回旋建模，可显着提高表达能力和发电质量。但是，这种顺序建模固有地限制了推断期间的并行计算，从而导致速度的生成阻碍了实际部署。在本文中，我们首先确定不需要进行高质量样本的严格的顺序依赖性。我们观察到，顺序建模中的斑块也可以近似，而无需严格在所有上述贴片上调节。此外，这些模型倾向于在初始层中表现出较低的依赖性冗余，并且随后层中的冗余性较高。利用这些观察结果，我们提出了一种选择性的Jacobi解码（SEJD）策略，该策略通过并行的迭代优化加速了自回应推断。理论分析证明了该方法的超线收敛率，并确保所需的迭代数不比原始顺序方法大。多个数据集的经验评估验证了加速技术的一般性和有效性。实验表明，在保持发电质量和忠诚度的同时，最大提高了高达4.7倍的速度。</li>
</ul>

<h3>Title: Cascading Adversarial Bias from Injection to Distillation in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Harsh Chaudhari, Jamie Hayes, Matthew Jagielski, Ilia Shumailov, Milad Nasr, Alina Oprea</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24842">https://arxiv.org/abs/2505.24842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24842">https://arxiv.org/pdf/2505.24842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24842]] Cascading Adversarial Bias from Injection to Distillation in Language Models(https://arxiv.org/abs/2505.24842)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Model distillation has become essential for creating smaller, deployable language models that retain larger system capabilities. However, widespread deployment raises concerns about resilience to adversarial manipulation. This paper investigates vulnerability of distilled models to adversarial injection of biased content during training. We demonstrate that adversaries can inject subtle biases into teacher models through minimal data poisoning, which propagates to student models and becomes significantly amplified. We propose two propagation modes: Untargeted Propagation, where bias affects multiple tasks, and Targeted Propagation, focusing on specific tasks while maintaining normal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning rate), student models generate biased responses 76.9% of the time in targeted scenarios - higher than 69.4% in teacher models. For untargeted propagation, adversarial bias appears 6x-29x more frequently in student models on unseen tasks. We validate findings across six bias types (targeted advertisements, phishing links, narrative manipulations, insecure coding practices), various distillation methods, and different modalities spanning text and code generation. Our evaluation reveals shortcomings in current defenses - perplexity filtering, bias detection systems, and LLM-based autorater frameworks - against these attacks. Results expose significant security vulnerabilities in distilled models, highlighting need for specialized safeguards. We propose practical design principles for building effective adversarial bias mitigation strategies.</li>
<li><strong>摘要：</strong>模型蒸馏对于创建保留较大系统功能的较小，可部署的语言模型至关重要。但是，广泛的部署引起了人们对对抗操纵的弹性的担忧。本文调查了蒸馏模型对培训期间偏见含量注入的脆弱性。我们证明，对手可以通过最小的数据中毒将微妙的偏见注入教师模型，从而传播学生模型并大大放大。我们提出了两种传播模式：非目标传播，偏见会影响多个任务和靶向传播，重点关注特定任务，同时在其他地方保持正常行为。学生模型只有25个中毒样本（0.25％的中毒率），在目标场景中产生了76.9％的偏见反应 - 在教师模型中高于69.4％。对于不靶向的繁殖，在看不见的任务的学生模型中，对抗性偏见更频繁地出现6x-29x。我们验证了六种偏见类型（目标广告，网络钓鱼链接，叙事操纵，不安全的编码实践），各种蒸馏方法以及跨越文本和代码生成的不同模式的发现。我们的评估揭示了当前防御措施的缺点 - 对这些攻击的困惑过滤，偏置检测系统和基于LLM的自动化框架。结果暴露了蒸馏模型中的重大安全性漏洞，强调了对专门保障措施的需求。我们提出了实用的设计原则，以建立有效的对抗性偏见缓解策略。</li>
</ul>

<h3>Title: Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization</h3>
<ul>
<li><strong>Authors: </strong>Joschka Braun, Carsten Eickhoff, Seyed Ali Bahrainian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24859">https://arxiv.org/abs/2505.24859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24859">https://arxiv.org/pdf/2505.24859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24859]] Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization(https://arxiv.org/abs/2505.24859)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Steering vectors are a lightweight method for controlling text properties by adding a learned bias to language model activations at inference time. So far, steering vectors have predominantly been evaluated in multiple-choice settings, while their effectiveness in free-form generation tasks remains understudied. Moving "Beyond Multiple Choice," we thoroughly evaluate the effectiveness of steering vectors in adaptively controlling topical focus, sentiment, toxicity, and readability in abstractive summaries of the NEWTS dataset. We find that steering effectively controls the targeted summary properties, but high steering strengths consistently degrade both intrinsic and extrinsic text quality. Compared to steering, prompting offers weaker control, while preserving text quality. Combining steering and prompting yields the strongest control over text properties and offers the most favorable efficacy-quality trade-off at moderate steering strengths. Our results underscore the practical trade-off between control strength and text quality preservation when applying steering vectors to free-form generation tasks.</li>
<li><strong>摘要：</strong>转向向量是一种轻巧的方法，可以通过在推理时为语言模型激活添加学习的偏差来控制文本属性。到目前为止，转向向量主要在多项选择设置中进行了评估，而它们在自由形式的任务中的有效性仍在研究中。我们彻底评估了转向向量在自适应控制局部焦点，情感，毒性和可读性中的效果中，我们可以彻底评估转向向量的有效性。我们发现转向有效地控制了目标的摘要属性，但是高转向强度始终降低了内在和外在文本质量。与转向相比，提示提供了较弱的控制，同时保持文本质量。将转向和提示结合起来可以最大程度地控制文本属性，并以适度的转向强度提供最有利的效能优质折衷。我们的结果强调了在将转向向量应用于自由形式生成任务时，控制强度和文本质量保存之间的实际权衡。</li>
</ul>

<h3>Title: ViStoryBench: Comprehensive Benchmark Suite for Story Visualization</h3>
<ul>
<li><strong>Authors: </strong>Cailin Zhuang, Ailin Huang, Wei Cheng, Jingwei Wu, Yaoqi Hu, Jiaqi Liao, Zhewei Huang, Hongyuan Wang, Xinyao Liao, Weiwei Cai, Hengyuan Xu, Xuanyang Zhang, Xianfang Zeng, Gang Yu, Chi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24862">https://arxiv.org/abs/2505.24862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24862">https://arxiv.org/pdf/2505.24862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24862]] ViStoryBench: Comprehensive Benchmark Suite for Story Visualization(https://arxiv.org/abs/2505.24862)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Story visualization, which aims to generate a sequence of visually coherent images aligning with a given narrative and reference images, has seen significant progress with recent advancements in generative models. To further enhance the performance of story visualization frameworks in real-world scenarios, we introduce a comprehensive evaluation benchmark, ViStoryBench. We collect a diverse dataset encompassing various story types and artistic styles, ensuring models are evaluated across multiple dimensions such as different plots (e.g., comedy, horror) and visual aesthetics (e.g., anime, 3D renderings). ViStoryBench is carefully curated to balance narrative structures and visual elements, featuring stories with single and multiple protagonists to test models' ability to maintain character consistency. Additionally, it includes complex plots and intricate world-building to challenge models in generating accurate visuals. To ensure comprehensive comparisons, our benchmark incorporates a wide range of evaluation metrics assessing critical aspects. This structured and multifaceted framework enables researchers to thoroughly identify both the strengths and weaknesses of different models, fostering targeted improvements.</li>
<li><strong>摘要：</strong>故事可视化旨在生成一系列视觉上一致的图像与给定的叙述和参考图像对齐的序列，它在生成模型的最新进步中取得了重大进展。为了进一步提高现实情况下故事可视化框架的性能，我们引入了全面的评估基准Vistorybench。我们收集了一个包括各种故事类型和艺术风格的各种数据集，确保在多个维度（例如喜剧，恐怖）和视觉美学（例如动漫，3D渲染图）等多个维度上评估模型。 Vistorybench经过精心策划，以平衡叙事结构和视觉元素，其中包含具有单个和多个主角的故事，以测试模型保持角色一致性的能力。此外，它包括复杂的图和复杂的世界建设，以挑战模型，以产生准确的视觉效果。为了确保全面的比较，我们的基准包括评估关键方面的广泛评估指标。这个结构化和多方面的框架使研究人员能够彻底识别不同模型的优势和劣势，从而促进目标改进。</li>
</ul>

<h3>Title: TalkingHeadBench: A Multi-Modal Benchmark & Analysis of Talking-Head DeepFake Detection</h3>
<ul>
<li><strong>Authors: </strong>Xinqi Xiong, Prakrut Patel, Qingyuan Fan, Amisha Wadhwa, Sarathy Selvam, Xiao Guo, Luchao Qi, Xiaoming Liu, Roni Sengupta</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24866">https://arxiv.org/abs/2505.24866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24866">https://arxiv.org/pdf/2505.24866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24866]] TalkingHeadBench: A Multi-Modal Benchmark & Analysis of Talking-Head DeepFake Detection(https://arxiv.org/abs/2505.24866)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of talking-head deepfake generation fueled by advanced generative models has elevated the realism of synthetic videos to a level that poses substantial risks in domains such as media, politics, and finance. However, current benchmarks for deepfake talking-head detection fail to reflect this progress, relying on outdated generators and offering limited insight into model robustness and generalization. We introduce TalkingHeadBench, a comprehensive multi-model multi-generator benchmark and curated dataset designed to evaluate the performance of state-of-the-art detectors on the most advanced generators. Our dataset includes deepfakes synthesized by leading academic and commercial models and features carefully constructed protocols to assess generalization under distribution shifts in identity and generator characteristics. We benchmark a diverse set of existing detection methods, including CNNs, vision transformers, and temporal models, and analyze their robustness and generalization capabilities. In addition, we provide error analysis using Grad-CAM visualizations to expose common failure modes and detector biases. TalkingHeadBench is hosted on this https URL with open access to all data splits and protocols. Our benchmark aims to accelerate research towards more robust and generalizable detection models in the face of rapidly evolving generative techniques.</li>
<li><strong>摘要：</strong>高级生成模型所推动的说话头摄影产生的快速发展将合成视频的现实主义提升到了在诸如媒体，政治和金融等领域中带来重大风险的水平。但是，当前的深击说话头检测基准无法反映这一进步，依靠过时的发电机，并提供了对模型鲁棒性和概括的有限见解。我们介绍了TakeHheadBench，这是一种综合的多模型多生物基准测试和策划的数据集，旨在评估最先进的发电机上最先进的探测器的性能。我们的数据集包括由领先的学术和商业模型合成的深击，以及精心构造的协议，以评估身份和发电机特征分布变化的概括。我们基准了各种现有检测方法，包括CNN，视觉变压器和时间模型，并分析其鲁棒性和泛化能力。此外，我们使用Grad-CAM可视化提供了错误分析，以暴露常见的故障模式和检测器偏见。 TalkingheadBench托管在此HTTPS URL上，开放访问所有数据拆分和协议。我们的基准旨在在面对迅速发展的生成技术的情况下加速研究朝着更健壮和可推广的检测模型。</li>
</ul>

<h3>Title: GenSpace: Benchmarking Spatially-Aware Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Zehan Wang, Jiayang Xu, Ziang Zhang, Tianyu Pan, Chao Du, Hengshuang Zhao, Zhou Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24870">https://arxiv.org/abs/2505.24870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24870">https://arxiv.org/pdf/2505.24870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24870]] GenSpace: Benchmarking Spatially-Aware Image Generation(https://arxiv.org/abs/2505.24870)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Humans can intuitively compose and arrange scenes in the 3D space for photography. However, can advanced AI image generators plan scenes with similar 3D spatial awareness when creating images from text or image prompts? We present GenSpace, a novel benchmark and evaluation pipeline to comprehensively assess the spatial awareness of current image generation models. Furthermore, standard evaluations using general Vision-Language Models (VLMs) frequently fail to capture the detailed spatial errors. To handle this challenge, we propose a specialized evaluation pipeline and metric, which reconstructs 3D scene geometry using multiple visual foundation models and provides a more accurate and human-aligned metric of spatial faithfulness. Our findings show that while AI models create visually appealing images and can follow general instructions, they struggle with specific 3D details like object placement, relationships, and measurements. We summarize three core limitations in the spatial perception of current state-of-the-art image generation models: 1) Object Perspective Understanding, 2) Egocentric-Allocentric Transformation and 3) Metric Measurement Adherence, highlighting possible directions for improving spatial intelligence in image generation.</li>
<li><strong>摘要：</strong>人类可以直观地在3D空间中构成和安排场景进行摄影。但是，从文本或图像提示创建图像时，高级AI图像生成器可以计划具有类似3D空间意识的场景吗？我们提出了Genspace，这是一种新颖的基准和评估管道，可全面评估当前图像生成模型的空间意识。此外，使用通用视觉模型（VLM）的标准评估通常无法捕获详细的空间错误。为了应对这一挑战，我们提出了一个专门的评估管道和指标，该指标使用多个视觉基础模型重建了3D场景几何形状，并提供了更准确，更加人性化的空间忠诚度量的指标。我们的发现表明，尽管AI模型创建了视觉上吸引人的图像并可以遵循一般说明，但它们在特定的3D细节（例如对象放置，人际关系和测量）中挣扎。我们总结了当前最新图像生成模型的空间感知中的三个核心局限性：1）对象透视理解，2）以自我为中心的上心转换和3）度量测量依从性，突出了可能提高图像生成中空间智能的可能方向。</li>
</ul>

<h3>Title: MiniMax-Remover: Taming Bad Noise Helps Video Object Removal</h3>
<ul>
<li><strong>Authors: </strong>Bojia Zi, Weixuan Peng, Xianbiao Qi, Jianan Wang, Shihao Zhao, Rong Xiao, Kam-Fai Wong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24873">https://arxiv.org/abs/2505.24873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24873">https://arxiv.org/pdf/2505.24873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24873]] MiniMax-Remover: Taming Bad Noise Helps Video Object Removal(https://arxiv.org/abs/2505.24873)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in video diffusion models have driven rapid progress in video editing techniques. However, video object removal, a critical subtask of video editing, remains challenging due to issues such as hallucinated objects and visual artifacts. Furthermore, existing methods often rely on computationally expensive sampling procedures and classifier-free guidance (CFG), resulting in slow inference. To address these limitations, we propose MiniMax-Remover, a novel two-stage video object removal approach. Motivated by the observation that text condition is not best suited for this task, we simplify the pretrained video generation model by removing textual input and cross-attention layers, resulting in a more lightweight and efficient model architecture in the first stage. In the second stage, we distilled our remover on successful videos produced by the stage-1 model and curated by human annotators, using a minimax optimization strategy to further improve editing quality and inference speed. Specifically, the inner maximization identifies adversarial input noise ("bad noise") that makes failure removals, while the outer minimization step trains the model to generate high-quality removal results even under such challenging conditions. As a result, our method achieves a state-of-the-art video object removal results with as few as 6 sampling steps and doesn't rely on CFG, significantly improving inference efficiency. Extensive experiments demonstrate the effectiveness and superiority of MiniMax-Remover compared to existing methods. Codes and Videos are available at: this https URL.</li>
<li><strong>摘要：</strong>视频扩散模型的最新进展推动了视频编辑技术的快速进步。但是，由于幻觉对象和视觉伪像等问题，视频对象的删除是视频编辑的关键子任务，仍然具有挑战性。此外，现有方法通常依赖于计算昂贵的采样程序和无分类器指导（CFG），从而导致推理缓慢。为了解决这些局限性，我们提出了一种新型的两阶段视频对象删除方法Minimax-Remover。通过观察文本条件最不适合此任务的观察，我们通过删除文本输入和跨注意层来简化了预验证的视频生成模型，从而在第一阶段提供了更轻巧，更有效的模型体系结构。在第二阶段，我们使用了最小值优化策略来进一步提高编辑质量和推理速度，从而将拆卸剂蒸馏到了阶段1模型并由人类注释者策划的成功视频上。具体而言，内部最大化确定了导致故障去除的对抗输入噪声（“不良噪声”），而外部最小化步骤也会训练该模型，即使在这种挑战性的条件下，也可以生成高质量的去除结果。结果，我们的方法实现了最新的视频对象删除结果，其结果只有6个采样步骤，并且不依赖CFG，从而显着提高了推论效率。与现有方法相比，广泛的实验证明了微型避难所的有效性和优势。代码和视频可在以下网址提供：此HTTPS URL。</li>
</ul>

<h3>Title: ReasonGen-R1: CoT for Autoregressive Image generation models through SFT and RL</h3>
<ul>
<li><strong>Authors: </strong>Yu Zhang, Yunqi Li, Yifan Yang, Rui Wang, Yuqing Yang, Dai Qi, Jianmin Bao, Dongdong Chen, Chong Luo, Lili Qiu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24875">https://arxiv.org/abs/2505.24875</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24875">https://arxiv.org/pdf/2505.24875</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24875]] ReasonGen-R1: CoT for Autoregressive Image generation models through SFT and RL(https://arxiv.org/abs/2505.24875)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Although chain-of-thought reasoning and reinforcement learning (RL) have driven breakthroughs in NLP, their integration into generative vision models remains underexplored. We introduce ReasonGen-R1, a two-stage framework that first imbues an autoregressive image generator with explicit text-based "thinking" skills via supervised fine-tuning on a newly generated reasoning dataset of written rationales, and then refines its outputs using Group Relative Policy Optimization. To enable the model to reason through text before generating images, We automatically generate and release a corpus of model crafted rationales paired with visual prompts, enabling controlled planning of object layouts, styles, and scene compositions. Our GRPO algorithm uses reward signals from a pretrained vision language model to assess overall visual quality, optimizing the policy in each update. Evaluations on GenEval, DPG, and the T2I benchmark demonstrate that ReasonGen-R1 consistently outperforms strong baselines and prior state-of-the-art models. More: this http URL.</li>
<li><strong>摘要：</strong>尽管经过思考的推理和加强学习（RL）在NLP中取得了突破，但它们整合到生成视觉模型中仍然没有被逐渐发展。我们介绍了Reasongen-R1，这是一个两阶段的框架，首先通过在新生成的书面理由的新生成的推理数据集中进行微调，然后使用小组相对策略优化来完善其输出。为了使模型能够在生成图像之前通过文本进行推理，我们将自动生成和释放模型制作的理由与视觉提示配对，从而可以对对象布局，样式和场景组成的控制计划。我们的GRPO算法使用验证视觉语言模型的奖励信号来评估整体视觉质量，从而在每个更新中优化策略。对Geneval，DPG和T2I基准的评估表明，Reasongen-R1始终优于强大的基准和先前的最新模型。更多：此HTTP URL。</li>
</ul>

<h3>Title: AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Yangyi Huang, Ye Yuan, Xueting Li, Jan Kautz, Umar Iqbal</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.24877">https://arxiv.org/abs/2505.24877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.24877">https://arxiv.org/pdf/2505.24877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.24877]] AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion(https://arxiv.org/abs/2505.24877)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Existing methods for image-to-3D avatar generation struggle to produce highly detailed, animation-ready avatars suitable for real-world applications. We introduce AdaHuman, a novel framework that generates high-fidelity animatable 3D avatars from a single in-the-wild image. AdaHuman incorporates two key innovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes consistent multi-view images in arbitrary poses alongside corresponding 3D Gaussian Splats (3DGS) reconstruction at each diffusion step; (2) A compositional 3DGS refinement module that enhances the details of local body parts through image-to-image refinement and seamlessly integrates them using a novel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These components allow AdaHuman to generate highly realistic standardized A-pose avatars with minimal self-occlusion, enabling rigging and animation with any input motion. Extensive evaluation on public benchmarks and in-the-wild images demonstrates that AdaHuman significantly outperforms state-of-the-art methods in both avatar reconstruction and reposing. Code and models will be publicly available for research purposes.</li>
<li><strong>摘要：</strong>现有的图像到3D头像生成的方法难以生成适合现实世界应用的高度详细的，可动画的化身。我们介绍了Adahuman，这是一个新颖的框架，该框架从单个野外图像中生成了高保真动画的3D化身。 Adahuman结合了两个关键的创新：（1）姿势条件的3D关节扩散模型，该模型以任意姿势合成一致的多视图图像，并在每个扩散步骤上与相应的3D Gaussian splat（3DGS）重建相同； （2）一个组成3DGS改进模块，通过图像到图像的细化来增强本地身体部位的细节，并使用新颖的农作物相机射线射线图无缝整合它们，从而产生一个具有凝聚力的详细的3D AVATAR。这些组件使Adahuman能够以最小的自我概括生成高度逼真的标准化A-Pose化身，从而可以使用任何输入运动来索具和动画。对公共基准和野外图像的广泛评估表明，Adahuman在头像重建和安息中的最先进方法都显着胜过。代码和模型将用于研究目的。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
