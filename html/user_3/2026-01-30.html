<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-01-30</h1>
<h3>Title: Rethinking LLM-Driven Heuristic Design: Generating Efficient and Specialized Solvers via Dynamics-Aware Optimization</h3>
<ul>
<li><strong>Authors: </strong>Rongzheng Wang, Yihong Huang, Muquan Li, Jiakai Li, Di Liang, Bob Simons, Pei Ke, Shuang Liang, Ke Qin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20868">https://arxiv.org/abs/2601.20868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20868">https://arxiv.org/pdf/2601.20868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20868]] Rethinking LLM-Driven Heuristic Design: Generating Efficient and Specialized Solvers via Dynamics-Aware Optimization(https://arxiv.org/abs/2601.20868)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have advanced the field of Combinatorial Optimization through automated heuristic generation. Instead of relying on manual design, this LLM-Driven Heuristic Design (LHD) process leverages LLMs to iteratively generate and refine solvers to achieve high performance. However, existing LHD frameworks face two critical limitations: (1) Endpoint-only evaluation, which ranks solvers solely by final quality, ignoring the convergence process and runtime efficiency; (2) High adaptation costs, where distribution shifts necessitate re-adaptation to generate specialized solvers for new instance groups. To address these issues, we propose Dynamics-Aware Solver Heuristics (DASH), a framework that co-optimizes solver search mechanisms and runtime schedules guided by a convergence-aware metric, thereby identifying efficient and high-performance solvers. Furthermore, to mitigate expensive re-adaptation, DASH incorporates Profiled Library Retrieval (PLR). PLR efficiently archives specialized solvers concurrently with the evolutionary process, enabling cost-effective warm-starts for heterogeneous distributions. Experiments on four combinatorial optimization problems demonstrate that DASH improves runtime efficiency by over 3 times, while surpassing the solution quality of state-of-the-art baselines across diverse problem scales. Furthermore, by enabling profile-based warm starts, DASH maintains superior accuracy under different distributions while cutting LLM adaptation costs by over 90%.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 通过自动启发式生成推动了组合优化领域的发展。这种 LLM 驱动的启发式设计 (LHD) 流程不依赖手动设计，而是利用 LLM 迭代生成和完善求解器以实现高性能。然而，现有的LHD框架面临两个关键限制：（1）仅端点评估，仅根据最终质量对求解器进行排名，忽略收敛过程和运行时效率； (2) 适应成本高，分布变化需要重新适应才能为新实例组生成专门的求解器。为了解决这些问题，我们提出了动态感知求解器启发式（DASH），这是一个框架，可在收敛感知指标的指导下共同优化求解器搜索机制和运行时计划，从而识别高效且高性能的求解器。此外，为了减轻昂贵的重新适应成本，DASH 结合了分析库检索 (PLR)。 PLR 在进化过程的同时有效地存档专用求解器，从而为异构分布实现经济高效的热启动。对四个组合优化问题的实验表明，DASH 将运行效率提高了 3 倍以上，同时超越了不同问题规模的最先进基线的解决方案质量。此外，通过启用基于配置文件的热启动，DASH 在不同分布下保持卓越的准确性，同时将 LLM 适应成本降低 90% 以上。</li>
</ul>

<h3>Title: A generative machine learning model for designing metal hydrides applied to hydrogen storage</h3>
<ul>
<li><strong>Authors: </strong>Xiyuan Liu, Christian Hacker, Shengnian Wang, Yuhua Duan</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20892">https://arxiv.org/abs/2601.20892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20892">https://arxiv.org/pdf/2601.20892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20892]] A generative machine learning model for designing metal hydrides applied to hydrogen storage(https://arxiv.org/abs/2601.20892)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Developing new metal hydrides is a critical step toward efficient hydrogen storage in carbon-neutral energy systems. However, existing materials databases, such as the Materials Project, contain a limited number of well-characterized hydrides, which constrains the discovery of optimal candidates. This work presents a framework that integrates causal discovery with a lightweight generative machine learning model to generate novel metal hydride candidates that may not exist in current databases. Using a dataset of 450 samples (270 training, 90 validation, and 90 testing), the model generates 1,000 candidates. After ranking and filtering, six previously unreported chemical formulas and crystal structures are identified, four of which are validated by density functional theory simulations and show strong potential for future experimental investigation. Overall, the proposed framework provides a scalable and time-efficient approach for expanding hydrogen storage datasets and accelerating materials discovery.</li>
<li><strong>摘要：</strong>开发新的金属氢化物是碳中性能源系统中高效储氢的关键一步。然而，现有的材料数据库（例如材料项目）包含数量有限的特征良好的氢化物，这限制了最佳候选材料的发现。这项工作提出了一个框架，将因果发现与轻量级生成机器学习模型相结合，以生成当前数据库中可能不存在的新型金属氢化物候选物。该模型使用 450 个样本的数据集（270 个训练样本、90 个验证样本和 90 个测试样本）生成 1,000 个候选样本。经过排序和过滤后，确定了六个以前未报道的化学式和晶体结构，其中四个通过密度泛函理论模拟进行了验证，并显示出未来实验研究的巨大潜力。总体而言，所提出的框架为扩展氢存储数据集和加速材料发现提供了一种可扩展且省时的方法。</li>
</ul>

<h3>Title: Non-Markov Multi-Round Conversational Image Generation with History-Conditioned MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Haochen Zhang, Animesh Sinha, Felix Juefei-Xu, Haoyu Ma, Kunpeng Li, Zhipeng Fan, Meng Dong, Xiaoliang Dai, Tingbo Hou, Peizhao Zhang, Zecheng He</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20911">https://arxiv.org/abs/2601.20911</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20911">https://arxiv.org/pdf/2601.20911</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20911]] Non-Markov Multi-Round Conversational Image Generation with History-Conditioned MLLMs(https://arxiv.org/abs/2601.20911)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Conversational image generation requires a model to follow user instructions across multiple rounds of interaction, grounded in interleaved text and images that accumulate as chat history. While recent multimodal large language models (MLLMs) can generate and edit images, most existing multi-turn benchmarks and training recipes are effectively Markov: the next output depends primarily on the most recent image, enabling shortcut solutions that ignore long-range history. In this work we formalize and target the more challenging non-Markov setting, where a user may refer back to earlier states, undo changes, or reference entities introduced several rounds ago. We present (i) non-Markov multi-round data construction strategies, including rollback-style editing that forces retrieval of earlier visual states and name-based multi-round personalization that binds names to appearances across rounds; (ii) a history-conditioned training and inference framework with token-level caching to prevent multi-round identity drift; and (iii) enabling improvements for high-fidelity image reconstruction and editable personalization, including a reconstruction-based DiT detokenizer and a multi-stage fine-tuning curriculum. We demonstrate that explicitly training for non-Markov interactions yields substantial improvements in multi-round consistency and instruction compliance, while maintaining strong single-round editing and personalization.</li>
<li><strong>摘要：</strong>对话图像生成需要一个模型在多轮交互中遵循用户指令，以作为聊天历史记录累积的交错文本和图像为基础。虽然最近的多模态大语言模型（MLLM）可以生成和编辑图像，但大多数现有的多轮基准和训练方法实际上都是马尔可夫：下一个输出主要取决于最新的图像，从而实现忽略长期历史的快捷解决方案。在这项工作中，我们形式化并针对更具挑战性的非马尔可夫设置，用户可以在其中引用较早的状态、撤消更改或引用几轮前引入的实体。我们提出（i）非马尔可夫多轮数据构建策略，包括强制检索早期视觉状态的回滚式编辑和将名称与各轮外观绑定的基于名称的多轮个性化； (ii) 具有令牌级缓存的历史条件训练和推理框架，以防止多轮身份漂移； (iii) 改进高保真图像重建和可编辑个性化，包括基于重建的 DiT 去标记器和多阶段微调课程。我们证明，对非马尔可夫交互的显式训练可以显着提高多轮一致性和指令合规性，同时保持强大的单轮编辑和个性化。</li>
</ul>

<h3>Title: MADE: Benchmark Environments for Closed-Loop Materials Discovery</h3>
<ul>
<li><strong>Authors: </strong>Shreshth A Malik, Tiarnan Doherty, Panagiotis Tigas, Muhammed Razzak, Stephen J. Roberts, Aron Walsh, Yarin Gal</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20996">https://arxiv.org/abs/2601.20996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20996">https://arxiv.org/pdf/2601.20996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20996]] MADE: Benchmark Environments for Closed-Loop Materials Discovery(https://arxiv.org/abs/2601.20996)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Existing benchmarks for computational materials discovery primarily evaluate static predictive tasks or isolated computational sub-tasks. While valuable, these evaluations neglect the inherently iterative and adaptive nature of scientific discovery. We introduce MAterials Discovery Environments (MADE), a novel framework for benchmarking end-to-end autonomous materials discovery pipelines. MADE simulates closed-loop discovery campaigns in which an agent or algorithm proposes, evaluates, and refines candidate materials under a constrained oracle budget, capturing the sequential and resource-limited nature of real discovery workflows. We formalize discovery as a search for thermodynamically stable compounds relative to a given convex hull, and evaluate efficacy and efficiency via comparison to baseline algorithms. The framework is flexible; users can compose discovery agents from interchangeable components such as generative models, filters, and planners, enabling the study of arbitrary workflows ranging from fixed pipelines to fully agentic systems with tool use and adaptive decision making. We demonstrate this by conducting systematic experiments across a family of systems, enabling ablation of components in discovery pipelines, and comparison of how methods scale with system complexity.</li>
<li><strong>摘要：</strong>计算材料发现的现有基准主要评估静态预测任务或孤立的计算子任务。这些评估虽然有价值，但忽视了科学发现固有的迭代和适应性本质。我们介绍材料发现环境（MADE），这是一种用于对端到端自主材料发现管道进行基准测试的新颖框架。 MADE 模拟闭环发现活动，其中代理或算法在有限的预言机预算下提出、评估和完善候选材料，捕获真实发现工作流程的顺序和资源有限的性质。我们将发现形式化为寻找相对于给定凸包的热力学稳定化合物，并通过与基线算法比较来评估功效和效率。框架灵活；用户可以使用可互换的组件（例如生成模型、过滤器和规划器）组成发现代理，从而能够研究从固定管道到具有工具使用和自适应决策的完全代理系统的任意工作流程。我们通过在一系列系统中进行系统实验、消除发现管道中的组件以及比较方法如何随系统复杂性进行扩展来证明这一点。</li>
</ul>

<h3>Title: Conditional Denoising Model as a Physical Surrogate Model</h3>
<ul>
<li><strong>Authors: </strong>José Afonso, Pedro Viegas, Rodrigo Ventura, Vasco Guerra</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.plasm-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21021">https://arxiv.org/abs/2601.21021</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21021">https://arxiv.org/pdf/2601.21021</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21021]] Conditional Denoising Model as a Physical Surrogate Model(https://arxiv.org/abs/2601.21021)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Surrogate modeling for complex physical systems typically faces a trade-off between data-fitting accuracy and physical consistency. Physics-consistent approaches typically treat physical laws as soft constraints within the loss function, a strategy that frequently fails to guarantee strict adherence to the governing equations, or rely on post-processing corrections that do not intrinsically learn the underlying solution geometry. To address these limitations, we introduce the {Conditional Denoising Model (CDM)}, a generative model designed to learn the geometry of the physical manifold itself. By training the network to restore clean states from noisy ones, the model learns a vector field that points continuously towards the valid solution subspace. We introduce a time-independent formulation that transforms inference into a deterministic fixed-point iteration, effectively projecting noisy approximations onto the equilibrium manifold. Validated on a low-temperature plasma physics and chemistry benchmark, the CDM achieves higher parameter and data efficiency than physics-consistent baselines. Crucially, we demonstrate that the denoising objective acts as a powerful implicit regularizer: despite never seeing the governing equations during training, the model adheres to physical constraints more strictly than baselines trained with explicit physics losses.</li>
<li><strong>摘要：</strong>复杂物理系统的代理建模通常面临数据拟合准确性和物理一致性之间的权衡。物理一致的方法通常将物理定律视为损失函数内的软约束，这种策略经常无法保证严格遵守控制方程，或者依赖于本质上不学习底层解几何的后处理校正。为了解决这些限制，我们引入了{条件去噪模型（CDM）}，这是一种旨在学习物理流形本身的几何形状的生成模型。通过训练网络从噪声状态恢复干净状态，模型学习一个连续指向有效解子空间的向量场。我们引入了一种与时间无关的公式，将推理转换为确定性的定点迭代，有效地将噪声近似投影到平衡流形上。经过低温等离子体物理和化学基准验证，CDM 实现了比物理一致基准更高的参数和数据效率。至关重要的是，我们证明了去噪目标充当了强大的隐式正则化器：尽管在训练期间从未看到控制方程，但该模型比使用显式物理损失训练的基线更严格地遵守物理约束。</li>
</ul>

<h3>Title: SIGMA-PPG: Statistical-prior Informed Generative Masking Architecture for PPG Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Zongheng Guo, Tao Chen, Yang Jiao, Yi Pan, Xiao Hu, Manuela Ferrario</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21031">https://arxiv.org/abs/2601.21031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21031">https://arxiv.org/pdf/2601.21031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21031]] SIGMA-PPG: Statistical-prior Informed Generative Masking Architecture for PPG Foundation Model(https://arxiv.org/abs/2601.21031)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Current foundation model for photoplethysmography (PPG) signals is challenged by the intrinsic redundancy and noise of the signal. Standard masked modeling often yields trivial solutions while contrastive methods lack morphological precision. To address these limitations, we propose a Statistical-prior Informed Generative Masking Architecture (SIGMA-PPG), a generative foundation model featuring a Prior-Guided Adversarial Masking mechanism, where a reinforcement learning-driven teacher leverages statistical priors to create challenging learning paths that prevent overfitting to noise. We also incorporate a semantic consistency constraint via vector quantization to ensure that physiologically identical waveforms (even those altered by recording artifacts or minor perturbations) map to shared indices. This enhances codebook semantic density and eliminates redundant feature structures. Pre-trained on over 120,000 hours of data, SIGMA-PPG achieves superior average performance compared to five state-of-the-art baselines across 12 diverse downstream tasks. The code is available at this https URL.</li>
<li><strong>摘要：</strong>当前光电体积描记法 (PPG) 信号的基础模型受到信号固有冗余和噪声的挑战。标准掩模建模通常会产生微不足道的解决方案，而对比方法缺乏形态学精度。为了解决这些限制，我们提出了一种统计先验知情生成屏蔽架构（SIGMA-PPG），这是一种具有先验引导对抗性屏蔽机制的生成基础模型，其中强化学习驱动的教师利用统计先验来创建具有挑战性的学习路径，以防止过度拟合噪声。我们还通过矢量量化纳入语义一致性约束，以确保生理上相同的波形（即使是通过记录伪影或微小扰动而改变的波形）映射到共享索引。这增强了码本语义密度并消除了冗余特征结构。 SIGMA-PPG 经过超过 120,000 小时的数据预训练，与 12 个不同下游任务的 5 个最先进的基线相比，实现了卓越的平均性能。该代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Predict-Project-Renoise: Sampling Diffusion Models under Hard Constraints</h3>
<ul>
<li><strong>Authors: </strong>Omer Rochman-Sharabi, Gilles Louppe</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21033">https://arxiv.org/abs/2601.21033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21033">https://arxiv.org/pdf/2601.21033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21033]] Predict-Project-Renoise: Sampling Diffusion Models under Hard Constraints(https://arxiv.org/abs/2601.21033)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Neural emulators based on diffusion models show promise for scientific applications, but vanilla models cannot guarantee physical accuracy or constraint satisfaction. We address this by introducing a constrained sampling framework that enforces hard constraints, such as physical laws or observational consistency, at generation time. Our approach defines a constrained forward process that diffuses only over the feasible set of constraint-satisfying samples, inducing constrained marginal distributions. To reverse this, we propose Predict-Project-Renoise (PPR), an iterative algorithm that samples from the constrained marginals by alternating between denoising predictions, projecting onto the feasible set, and renoising. Experiments on 2D distributions, PDEs, and global weather forecasting demonstrate that PPR reduces constraint violations by over an order of magnitude while improving sample consistency and better matching the true constrained distribution compared to baselines.</li>
<li><strong>摘要：</strong>基于扩散模型的神经模拟器显示出科学应用的前景，但普通模型无法保证物理准确性或约束满足。我们通过引入一个约束采样框架来解决这个问题，该框架在生成时强制执行硬约束，例如物理定律或观察一致性。我们的方法定义了一个受约束的前向过程，该过程仅在一组可行的满足约束的样本上扩散，从而产生受约束的边际分布。为了扭转这种情况，我们提出了 Predict-Project-Renoise (PPR)，这是一种迭代算法，通过在去噪预测、投影到可行集和再噪声之间交替来从受约束的边缘进行采样。对 2D 分布、偏微分方程和全球天气预报的实验表明，与基线相比，PPR 将约束违规减少了一个数量级以上，同时提高了样本一致性并更好地匹配真实的约束分布。</li>
</ul>

<h3>Title: Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Chengzu Li, Zanyi Wang, Jiaang Li, Yi Xu, Han Zhou, Huanyu Zhang, Ruichuan An, Dengyang Jiang, Zhaochong An, Ivan Vulić, Serge Belongie, Anna Korhonen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21037">https://arxiv.org/abs/2601.21037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21037">https://arxiv.org/pdf/2601.21037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21037]] Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning(https://arxiv.org/abs/2601.21037)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision-Language Models have excelled at textual reasoning, but they often struggle with fine-grained spatial understanding and continuous action planning, failing to simulate the dynamics required for complex visual reasoning. In this work, we formulate visual reasoning by means of video generation models, positing that generated frames can act as intermediate reasoning steps between initial states and solutions. We evaluate their capacity in two distinct regimes: Maze Navigation for sequential discrete planning with low visual change and Tangram Puzzle for continuous manipulation with high visual change. Our experiments reveal three critical insights: (1) Robust Zero-Shot Generalization: In both tasks, the model demonstrates strong performance on unseen data distributions without specific finetuning. (2) Visual Context: The model effectively uses visual context as explicit control, such as agent icons and tangram shapes, enabling it to maintain high visual consistency and adapt its planning capability robustly to unseen patterns. (3) Visual Test-Time Scaling: We observe a test-time scaling law in sequential planning; increasing the generated video length (visual inference budget) empowers better zero-shot generalization to spatially and temporally complex paths. These findings suggest that video generation is not merely a media tool, but a scalable, generalizable paradigm for visual reasoning.</li>
<li><strong>摘要：</strong>视觉语言模型在文本推理方面表现出色，但它们经常在细粒度的空间理解和连续的动作规划方面遇到困难，无法模拟复杂视觉推理所需的动态。在这项工作中，我们通过视频生成模型来制定视觉推理，假设生成的帧可以充当初始状态和解决方案之间的中间推理步骤。我们在两种不同的机制中评估它们的能力：用于低视觉变化的顺序离散规划的迷宫导航和用于高视觉变化的连续操作的七巧板拼图。我们的实验揭示了三个关键见解：（1）鲁棒的零样本泛化：在这两项任务中，该模型在未见的数据分布上表现出强大的性能，而无需进行特定的微调。 （2）视觉上下文：该模型有效地使用视觉上下文作为显式控制，例如代理图标和七巧板形状，使其能够保持高度的视觉一致性并使其规划能力稳健地适应看不见的模式。 (3) 视觉测试时间缩放：我们在顺序规划中观察到测试时间缩放定律；增加生成的视频长度（视觉推理预算）可以更好地对空间和时间复杂路径进行零样本泛化。这些发现表明，视频生成不仅仅是一种媒体工具，而且是一种可扩展、可概括的视觉推理范例。</li>
</ul>

<h3>Title: Signal from Structure: Exploiting Submodular Upper Bounds in Generative Flow Networks</h3>
<ul>
<li><strong>Authors: </strong>Alexandre Larouche, Audrey Durand</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21061">https://arxiv.org/abs/2601.21061</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21061">https://arxiv.org/pdf/2601.21061</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21061]] Signal from Structure: Exploiting Submodular Upper Bounds in Generative Flow Networks(https://arxiv.org/abs/2601.21061)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative Flow Networks (GFlowNets; GFNs) are a class of generative models that learn to sample compositional objects proportionally to their a priori unknown value, their reward. We focus on the case where the reward has a specified, actionable structure, namely that it is submodular. We show submodularity can be harnessed to retrieve upper bounds on the reward of compositional objects that have not yet been observed. We provide in-depth analyses of the probability of such bounds occurring, as well as how many unobserved compositional objects can be covered by a bound. Following the Optimism in the Face of Uncertainty principle, we then introduce SUBo-GFN, which uses the submodular upper bounds to train a GFN. We show that SUBo-GFN generates orders of magnitude more training data than classical GFNs for the same number of queries to the reward function. We demonstrate the effectiveness of SUBo-GFN in terms of distribution matching and high-quality candidate generation on synthetic and real-world submodular tasks.</li>
<li><strong>摘要：</strong>生成流网络（GFlowNets；GFN）是一类生成模型，它学习按组合对象的先验未知值（即奖励）进行采样。我们关注奖励具有指定的、可操作的结构的情况，即它是子模块的。我们证明可以利用子模块性来检索尚未观察到的组合对象的奖励上限。我们深入分析了此类边界发生的概率，以及边界可以覆盖多少未观察到的组合对象。遵循面对不确定性的乐观原则，我们随后引入 SUBo-GFN，它使用子模上限来训练 GFN。我们表明，对于相同数量的奖励函数查询，SUBo-GFN 生成的训练数据比经典 GFN 多几个数量级。我们证明了 SUBo-GFN 在合成和现实世界子模块任务的分布匹配和高质量候选生成方面的有效性。</li>
</ul>

<h3>Title: Shape of Thought: Progressive Object Assembly via Visual Chain-of-Thought</h3>
<ul>
<li><strong>Authors: </strong>Yu Huo, Siyu Zhang, Kun Zeng, Haoyue Liu, Owen Lee, Junlin Chen, Yuquan Lu, Yifu Guo, Yaodong Liang, Xiaoying Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21081">https://arxiv.org/abs/2601.21081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21081">https://arxiv.org/pdf/2601.21081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21081]] Shape of Thought: Progressive Object Assembly via Visual Chain-of-Thought(https://arxiv.org/abs/2601.21081)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Multimodal models for text-to-image generation have achieved strong visual fidelity, yet they remain brittle under compositional structural constraints-notably generative numeracy, attribute binding, and part-level relations. To address these challenges, we propose Shape-of-Thought (SoT), a visual CoT framework that enables progressive shape assembly via coherent 2D projections without external engines at inference time. SoT trains a unified multimodal autoregressive model to generate interleaved textual plans and rendered intermediate states, helping the model capture shape-assembly logic without producing explicit geometric representations. To support this paradigm, we introduce SoT-26K, a large-scale dataset of grounded assembly traces derived from part-based CAD hierarchies, and T2S-CompBench, a benchmark for evaluating structural integrity and trace faithfulness. Fine-tuning on SoT-26K achieves 88.4% on component numeracy and 84.8% on structural topology, outperforming text-only baselines by around 20%. SoT establishes a new paradigm for transparent, process-supervised compositional generation. The code is available at this https URL. The SoT-26K dataset will be released upon acceptance.</li>
<li><strong>摘要：</strong>用于文本到图像生成的多模态模型已经实现了很强的视觉保真度，但它们在组合结构约束下仍然很脆弱，特别是生成计算、属性绑定和部分级别关系。为了应对这些挑战，我们提出了 Shape-of-Thought (SoT)，这是一种视觉 CoT 框架，可以通过连贯的 2D 投影进行渐进形状组装，而无需在推理时使用外部引擎。 SoT 训练统一的多模态自回归模型来生成交错的文本计划并渲染中间状态，帮助模型捕获形状组装逻辑，而无需生成显式的几何表示。为了支持这一范式，我们引入了 SoT-26K（一个源自基于零件的 CAD 层次结构的基础装配轨迹的大型数据集）和 T2S-CompBench（用于评估结构完整性和轨迹忠实度的基准）。 SoT-26K 上的微调在组件计算上实现了 88.4%，在结构拓扑上实现了 84.8%，比纯文本基线高出约 20%。 SoT 为透明的、过程监督的合成生成建立了新的范例。该代码可从此 https URL 获取。 SoT-26K 数据集将在接受后发布。</li>
</ul>

<h3>Title: AC2L-GAD: Active Counterfactual Contrastive Learning for Graph Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Kamal Berahmand, Saman Forouzandeh, Mehrnoush Mohammadi, Parham Moradi, Mahdi Jalili</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21171">https://arxiv.org/abs/2601.21171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21171">https://arxiv.org/pdf/2601.21171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21171]] AC2L-GAD: Active Counterfactual Contrastive Learning for Graph Anomaly Detection(https://arxiv.org/abs/2601.21171)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Graph anomaly detection aims to identify abnormal patterns in networks, but faces significant challenges from label scarcity and extreme class imbalance. While graph contrastive learning offers a promising unsupervised solution, existing methods suffer from two critical limitations: random augmentations break semantic consistency in positive pairs, while naive negative sampling produces trivial, uninformative contrasts. We propose AC2L-GAD, an Active Counterfactual Contrastive Learning framework that addresses both limitations through principled counterfactual reasoning. By combining information-theoretic active selection with counterfactual generation, our approach identifies structurally complex nodes and generates anomaly-preserving positive augmentations alongside normal negative counterparts that provide hard contrasts, while restricting expensive counterfactual generation to a strategically selected subset. This design reduces computational overhead by approximately 65% compared to full-graph counterfactual generation while maintaining detection quality. Experiments on nine benchmark datasets, including real-world financial transaction graphs from GADBench, show that AC2L-GAD achieves competitive or superior performance compared to state-of-the-art baselines, with notable gains in datasets where anomalies exhibit complex attribute-structure interactions.</li>
<li><strong>摘要：</strong>图异常检测旨在识别网络中的异常模式，但面临着标签稀缺和极端类别不平衡的重大挑战。虽然图对比学习提供了一种有前途的无监督解决方案，但现有方法存在两个关键限制：随机增强破坏了正对中的语义一致性，而朴素的负采样会产生微不足道的、无信息的对比。我们提出了 AC2L-GAD，一种主动反事实对比学习框架，通过有原则的反事实推理解决这两个限制。通过将信息论主动选择与反事实生成相结合，我们的方法识别结构复杂的节点，并生成保留异常的正增强以及提供硬对比的正常负对应项，同时将昂贵的反事实生成限制为战略选择的子集。与全图反事实生成相比，这种设计可减少约 65% 的计算开销，同时保持检测质量。对九个基准数据集（包括来自 GDBench 的真实金融交易图）的实验表明，与最先进的基准相比，AC2L-GAD 实现了具有竞争力或优越的性能，并且在异常表现出复杂的属性-结构交互的数据集中取得了显着的进步。</li>
</ul>

<h3>Title: Rethinking Refinement: Correcting Generative Bias without Noise Injection</h3>
<ul>
<li><strong>Authors: </strong>Xin Peng, Ang Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21182">https://arxiv.org/abs/2601.21182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21182">https://arxiv.org/pdf/2601.21182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21182]] Rethinking Refinement: Correcting Generative Bias without Noise Injection(https://arxiv.org/abs/2601.21182)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models, including diffusion and flow-based models, often exhibit systematic biases that degrade sample quality, particularly in high-dimensional settings. We revisit refinement methods and show that effective bias correction can be achieved as a post-hoc procedure, without noise injection or multi-step resampling of the sampling process. We propose a flow-matching-based \textbf{Bi-stage Flow Refinement (BFR)} framework with two refinement strategies operating at different stages: latent space alignment for approximately invertible generators and data space refinement trained with lightweight augmentations. Unlike previous refiners that perturb sampling dynamics, BFR preserves the original ODE trajectory and applies deterministic corrections to generated samples. Experiments on MNIST, CIFAR-10, and FFHQ at 256x256 resolution demonstrate consistent improvements in fidelity and coverage; notably, starting from base samples with FID 3.95, latent space refinement achieves a \textbf{state-of-the-art} FID of \textbf{1.46} on MNIST using only a single additional function evaluation (1-NFE), while maintaining sample diversity.</li>
<li><strong>摘要：</strong>生成模型，包括基于扩散和流动的模型，通常会表现出系统偏差，从而降低样本质量，特别是在高维设置中。我们重新审视了细化方法，并表明可以作为事后程序实现有效的偏差校正，而无需噪声注入或多步重采样采样过程。我们提出了一种基于流匹配的 \textbf{Bi-stage Flow Refinement (BFR)} 框架，该框架具有在不同阶段运行的两种细化策略：近似可逆生成器的潜在空间对齐和使用轻量级增强训练的数据空间细化。与之前扰乱采样动力学的精炼器不同，BFR 保留了原始 ODE 轨迹并对生成的样本应用确定性校正。在 MNIST、CIFAR-10 和 FFHQ 上以 256x256 分辨率进行的实验表明，保真度和覆盖范围得到了持续改进；值得注意的是，从 FID 3.95 的基本样本开始，潜在空间细化仅使用单个附加函数评估 (1-NFE) 在 MNIST 上实现了 \textbf{state-of-the-art} FID \textbf{1.46}，同时保持样本多样性。</li>
</ul>

<h3>Title: Generative Recall, Dense Reranking: Learning Multi-View Semantic IDs for Efficient Text-to-Video Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Zecheng Zhao, Zhi Chen, Zi Huang, Shazia Sadiq, Tong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21193">https://arxiv.org/abs/2601.21193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21193">https://arxiv.org/pdf/2601.21193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21193]] Generative Recall, Dense Reranking: Learning Multi-View Semantic IDs for Efficient Text-to-Video Retrieval(https://arxiv.org/abs/2601.21193)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Text-to-Video Retrieval (TVR) is essential in video platforms. Dense retrieval with dual-modality encoders leads in accuracy, but its computation and storage scale poorly with corpus size. Thus, real-time large-scale applications adopt two-stage retrieval, where a fast recall model gathers a small candidate pool, which is reranked by an advanced dense retriever. Due to hugely reduced candidates, the reranking model can use any off-the-shelf dense retriever without hurting efficiency, meaning the recall model bounds two-stage TVR performance. Recently, generative retrieval (GR) replaces dense video embeddings with discrete semantic IDs and retrieves by decoding text queries into ID tokens. GR offers near-constant inference and storage complexity, and its semantic IDs capture high-level video features via quantization, making it ideal for quickly eliminating irrelevant candidates during recall. However, as a recall model in two-stage TVR, GR suffers from (i) semantic ambiguity, where each video satisfies diverse queries but is forced into one semantic ID; and (ii) cross-modal misalignment, as semantic IDs are solely derived from visual features without text supervision. We propose Generative Recall and Dense Reranking (GRDR), designing a novel GR method to uplift recalled candidate quality. GRDR assigns multiple semantic IDs to each video using a query-guided multi-view tokenizer exposing diverse semantic access paths, and jointly trains the tokenizer and generative retriever via a shared codebook to cast semantic IDs as the semantic bridge between texts and videos. At inference, trie-constrained decoding generates a compact candidate set reranked by a dense model for fine-grained matching. Experiments on TVR benchmarks show GRDR matches strong dense retrievers in accuracy while reducing index storage by an order of magnitude and accelerating up to 300$\times$ in full-corpus retrieval.</li>
<li><strong>摘要：</strong>文本到视频检索（TVR）在视频平台中至关重要。使用双模态编码器的密集检索在准确性方面领先，但其计算和存储规模随语料库大小的变化很差。因此，实时大规模应用采用两阶段检索，其中快速召回模型收集小型候选池，并由高级密集检索器重新排序。由于候选者数量大幅减少，重新排序模型可以使用任何现成的密集检索器，而不会影响效率，这意味着召回模型限制了两阶段 TVR 性能。最近，生成检索 (GR) 用离散语义 ID 取代了密集视频嵌入，并通过将文本查询解码为 ID 令牌来进行检索。 GR 提供近乎恒定的推理和存储复杂性，其语义 ID 通过量化捕获高级视频特征，使其成为在召回过程中快速消除不相关候选者的理想选择。然而，作为两阶段 TVR 中的召回模型，GR 面临以下问题：（i）语义模糊，每个视频满足不同的查询，但被迫使用一个语义 ID； (ii) 跨模式错位，因为语义 ID 仅源自视觉特征，没有文本监督。我们提出了生成召回和密集重排序（GRDR），设计了一种新颖的 GR 方法来提高召回候选质量。 GRDR 使用查询引导的多视图标记器为每个视频分配多个语义 ID，公开不同的语义访问路径，并通过共享码本联合训练标记器和生成检索器，以将语义 ID 作为文本和视频之间的语义桥梁。在推理时，特里约束解码生成一个紧凑的候选集，并通过密集模型重新排序以进行细粒度匹配。 TVR 基准测试的实验表明，GRDR 在准确度上与强密集检索器相匹配，同时将索引存储减少一个数量级，并在全语料库检索中加速高达 300$\times$。</li>
</ul>

<h3>Title: PHDME: Physics-Informed Diffusion Models without Explicit Governing Equations</h3>
<ul>
<li><strong>Authors: </strong>Kaiyuan Tan, Kendra Givens, Peilun Li, Thomas Beckers</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21234">https://arxiv.org/abs/2601.21234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21234">https://arxiv.org/pdf/2601.21234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21234]] PHDME: Physics-Informed Diffusion Models without Explicit Governing Equations(https://arxiv.org/abs/2601.21234)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models provide expressive priors for forecasting trajectories of dynamical systems, but are typically unreliable in the sparse data regime. Physics-informed machine learning (PIML) improves reliability in such settings; however, most methods require \emph{explicit governing equations} during training, which are often only partially known due to complex and nonlinear dynamics. We introduce \textbf{PHDME}, a port-Hamiltonian diffusion framework designed for \emph{sparse observations} and \emph{incomplete physics}. PHDME leverages port-Hamiltonian structural prior but does not require full knowledge of the closed-form governing equations. Our approach first trains a Gaussian process distributed Port-Hamiltonian system (GP-dPHS) on limited observations to capture an energy-based representation of the dynamics. The GP-dPHS is then used to generate a physically consistent artificial dataset for diffusion training, and to inform the diffusion model with a structured physics residual loss. After training, the diffusion model acts as an amortized sampler and forecaster for fast trajectory generation. Finally, we apply split conformal calibration to provide uncertainty statements for the generated predictions. Experiments on PDE benchmarks and a real-world spring system show improved accuracy and physical consistency under data scarcity.</li>
<li><strong>摘要：</strong>扩散模型为预测动力系统的轨迹提供了富有表现力的先验，但在稀疏数据状态下通常不可靠。物理信息机器学习 (PIML) 提高了此类环境中的可靠性；然而，大多数方法在训练期间需要\emph{显式控制方程}，由于复杂和非线性动力学，这些方程通常只能部分已知。我们引入 \textbf{PHDME}，一个为 \emph{稀疏观测} 和 \emph{不完整物理} 设计的端口哈密尔顿扩散框架。 PHDME 利用端口哈密尔顿结构先验，但不需要完全了解封闭式控制方程。我们的方法首先在有限的观测上训练高斯过程分布式波特哈密尔顿系统（GP-dPHS），以捕获基于能量的动力学表示。然后，GP-dPHS 用于生成用于扩散训练的物理一致的人工数据集，并为扩散模型提供结构化的物理残差损失。训练后，扩散模型充当摊销采样器和预测器，用于快速生成轨迹。最后，我们应用分割共形校准来为生成的预测提供不确定性陈述。 PDE 基准和真实弹簧系统的实验表明，在数据稀缺的情况下，准确性和物理一致性得到了提高。</li>
</ul>

<h3>Title: PTQ4ARVG: Post-Training Quantization for AutoRegressive Visual Generation Models</h3>
<ul>
<li><strong>Authors: </strong>Xuewen Liu, Zhikai Li, Jing Zhang, Mengjuan Chen, Qingyi Gu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21238">https://arxiv.org/abs/2601.21238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21238">https://arxiv.org/pdf/2601.21238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21238]] PTQ4ARVG: Post-Training Quantization for AutoRegressive Visual Generation Models(https://arxiv.org/abs/2601.21238)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>AutoRegressive Visual Generation (ARVG) models retain an architecture compatible with language models, while achieving performance comparable to diffusion-based models. Quantization is commonly employed in neural networks to reduce model size and computational latency. However, applying quantization to ARVG remains largely underexplored, and existing quantization methods fail to generalize effectively to ARVG models. In this paper, we explore this issue and identify three key challenges: (1) severe outliers at channel-wise level, (2) highly dynamic activations at token-wise level, and (3) mismatched distribution information at sample-wise level. To these ends, we propose PTQ4ARVG, a training-free post-training quantization (PTQ) framework consisting of: (1) Gain-Projected Scaling (GPS) mitigates the channel-wise outliers, which expands the quantization loss via a Taylor series to quantify the gain of scaling for activation-weight quantization, and derives the optimal scaling factor through differentiation.(2) Static Token-Wise Quantization (STWQ) leverages the inherent properties of ARVG, fixed token length and position-invariant distribution across samples, to address token-wise variance without incurring dynamic calibration overhead.(3) Distribution-Guided Calibration (DGC) selects samples that contribute most to distributional entropy, eliminating the sample-wise distribution mismatch. Extensive experiments show that PTQ4ARVG can effectively quantize the ARVG family models to 8-bit and 6-bit while maintaining competitive performance. Code is available at this http URL .</li>
<li><strong>摘要：</strong>自回归视觉生成 (ARVG) 模型保留了与语言模型兼容的架构，同时实现了与基于扩散的模型相当的性能。量化通常在神经网络中采用，以减少模型大小和计算延迟。然而，将量化应用于 ARVG 仍然很大程度上尚未得到充分探索，并且现有的量化方法无法有效地推广到 ARVG 模型。在本文中，我们探讨了这个问题并确定了三个关键挑战：（1）通道级别的严重异常值，（2）令牌级别的高度动态激活，以及（3）样本级别的分布信息不匹配。为此，我们提出了 PTQ4ARVG，一种免训练的训练后量化 (PTQ) 框架，包括：(1) 增益投影缩放 (GPS) 减轻通道异常值，通过泰勒级数扩展量化损失以量化激活权重量化的缩放增益，并通过微分导出最佳缩放因子。(2) 静态分词量化 (STWQ) 利用ARVG，跨样本的固定令牌长度和位置不变分布，以解决令牌方面的方差，而不会产生动态校准开销。（3）分布引导校准（DGC）选择对分布熵贡献最大的样本，消除样本方面的分布不匹配。大量实验表明，PTQ4ARVG 可以有效地将 ARVG 系列模型量化为 8 位和 6 位，同时保持具有竞争力的性能。代码可从此 http URL 获取。</li>
</ul>

<h3>Title: Understanding Diffusion Models via Ratio-Based Function Approximation with SignReLU Networks</h3>
<ul>
<li><strong>Authors: </strong>Luwei Sun, Dongrui Shen, Jianfe Li, Yulong Zhao, Han Feng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21242">https://arxiv.org/abs/2601.21242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21242">https://arxiv.org/pdf/2601.21242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21242]] Understanding Diffusion Models via Ratio-Based Function Approximation with SignReLU Networks(https://arxiv.org/abs/2601.21242)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Motivated by challenges in conditional generative modeling, where the target conditional density takes the form of a ratio f1 over f2, this paper develops a theoretical framework for approximating such ratio-type functionals. Here, f1 and f2 are kernel-based marginal densities that capture structured interactions, a setting central to diffusion-based generative models. We provide a concise proof for approximating these ratio-type functionals using deep neural networks with the SignReLU activation function, leveraging the activation's piecewise structure. Under standard regularity assumptions, we establish L^p(Omega) approximation bounds and convergence rates. Specializing to Denoising Diffusion Probabilistic Models (DDPMs), we construct a SignReLU-based neural estimator for the reverse process and derive bounds on the excess Kullback-Leibler (KL) risk between the generated and true data distributions. Our analysis decomposes this excess risk into approximation and estimation error components. These results provide generalization guarantees for finite-sample training of diffusion-based generative models.</li>
<li><strong>摘要：</strong>受条件生成模型挑战（目标条件密度采用 f1 与 f2 之比的形式）的启发，本文开发了一个用于逼近此类比率型泛函的理论框架。这里，f1 和 f2 是基于核的边际密度，捕获结构化交互，这是基于扩散的生成模型的核心设置。我们提供了一个简明的证明，用于使用具有 SignReLU 激活函数的深度神经网络来逼近这些比率型泛函，并利用激活的分段结构。在标准正则性假设下，我们建立 L^p(Omega) 近似界限和收敛速度。我们专注于去噪扩散概率模型 (DDPM)，为逆向过程构建了一个基于 SignReLU 的神经估计器，并导出了生成的数据分布和真实数据分布之间的过量 Kullback-Leibler (KL) 风险的界限。我们的分析将这种超额风险分解为近似误差和估计误差分量。这些结果为基于扩散的生成模型的有限样本训练提供了泛化保证。</li>
</ul>

<h3>Title: Conditional Generative Framework with Peak-Aware Attention for Robust Chemical Detection under Interferences</h3>
<ul>
<li><strong>Authors: </strong>Namkyung Yoon, Sanghong Kim, Hwangnam Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21246">https://arxiv.org/abs/2601.21246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21246">https://arxiv.org/pdf/2601.21246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21246]] Conditional Generative Framework with Peak-Aware Attention for Robust Chemical Detection under Interferences(https://arxiv.org/abs/2601.21246)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Gas chromatography-mass spectrometry (GC-MS) is a widely used analytical method for chemical substance detection, but measurement reliability tends to deteriorate in the presence of interfering substances. In particular, interfering substances cause nonspecific peaks, residence time shifts, and increased background noise, resulting in reduced sensitivity and false alarms. To overcome these challenges, in this paper, we propose an artificial intelligence discrimination framework based on a peak-aware conditional generative model to improve the reliability of GC-MS measurements under interference conditions. The framework is learned with a novel peak-aware mechanism that highlights the characteristic peaks of GC-MS data, allowing it to generate important spectral features more faithfully. In addition, chemical and solvent information is encoded in a latent vector embedded with it, allowing a conditional generative adversarial neural network (CGAN) to generate a synthetic GC-MS signal consistent with the experimental conditions. This generates an experimental dataset that assumes indirect substance situations in chemical substance data, where acquisition is limited without conducting real experiments. These data are used for the learning of AI-based GC-MS discrimination models to help in accurate chemical substance discrimination. We conduct various quantitative and qualitative evaluations of the generated simulated data to verify the validity of the proposed framework. We also verify how the generative model improves the performance of the AI discrimination framework. Representatively, the proposed method is shown to consistently achieve cosine similarity and Pearson correlation coefficient values above 0.9 while preserving peak number diversity and reducing false alarms in the discrimination model.</li>
<li><strong>摘要：</strong>气相色谱-质谱法（GC-MS）是一种广泛使用的化学物质检测分析方法，但在存在干扰物质的情况下，测量可靠性往往会下降。特别是，干扰物质会导致非特异性峰、停留时间变化和背景噪音增加，从而导致灵敏度降低和误报。为了克服这些挑战，在本文中，我们提出了一种基于峰值感知条件生成模型的人工智能判别框架，以提高干扰条件下 GC-MS 测量的可靠性。该框架通过一种新颖的峰感知机制进行学习，该机制突出显示 GC-MS 数据的特征峰，使其能够更忠实地生成重要的光谱特征。此外，化学和溶剂信息被编码在嵌入的潜在向量中，允许条件生成对抗神经网络 (CGAN) 生成与实验条件一致的合成 GC-MS 信号。这会生成一个实验数据集，该数据集假设化学物质数据中的间接物质情况，在不进行真实实验的情况下，获取数据受到限制。这些数据用于学习基于人工智能的GC-MS判别模型，以帮助准确判别化学物质。我们对生成的模拟数据进行各种定量和定性评估，以验证所提出框架的有效性。我们还验证了生成模型如何提高人工智能判别框架的性能。代表性地，所提出的方法被证明能够一致地实现余弦相似度和皮尔逊相关系数值高于 0.9，同时保留峰值数量多样性并减少判别模型中的误报。</li>
</ul>

<h3>Title: NFCDS: A Plug-and-Play Noise Frequency-Controlled Diffusion Sampling Strategy for Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Zhen Wang, Hongyi Liu, Jianing Li, Zhihui Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21248">https://arxiv.org/abs/2601.21248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21248">https://arxiv.org/pdf/2601.21248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21248]] NFCDS: A Plug-and-Play Noise Frequency-Controlled Diffusion Sampling Strategy for Image Restoration(https://arxiv.org/abs/2601.21248)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generation</a></li>
<li><strong>Abstract: </strong>Diffusion sampling-based Plug-and-Play (PnP) methods produce images with high perceptual quality but often suffer from reduced data fidelity, primarily due to the noise introduced during reverse diffusion. To address this trade-off, we propose Noise Frequency-Controlled Diffusion Sampling (NFCDS), a spectral modulation mechanism for reverse diffusion noise. We show that the fidelity-perception conflict can be fundamentally understood through noise frequency: low-frequency components induce blur and degrade fidelity, while high-frequency components drive detail generation. Based on this insight, we design a Fourier-domain filter that progressively suppresses low-frequency noise and preserves high-frequency content. This controlled refinement injects a data-consistency prior directly into sampling, enabling fast convergence to results that are both high-fidelity and perceptually convincing--without additional training. As a PnP module, NFCDS seamlessly integrates into existing diffusion-based restoration frameworks and improves the fidelity-perception balance across diverse zero-shot tasks.</li>
<li><strong>摘要：</strong>基于扩散采样的即插即用 (PnP) 方法可生成具有高感知质量的图像，但通常会受到数据保真度降低的影响，这主要是由于反向扩散期间引入的噪声。为了解决这种权衡问题，我们提出了噪声频率控制扩散采样（NFCDS），这是一种用于反向扩散噪声的频谱调制机制。我们表明，可以通过噪声频率从根本上理解保真度与感知的冲突：低频分量会引起模糊并降低保真度，而高频分量则驱动细节生成。基于这一见解，我们设计了一种傅里叶域滤波器，可逐步抑制低频噪声并保留高频内容。这种受控细化将数据一致性先验直接注入采样中，从而能够快速收敛到高保真度和感知上令人信服的结果，而无需额外的训练。作为 PnP 模块，NFCDS 无缝集成到现有的基于扩散的恢复框架中，并提高了各种零样本任务之间的保真度-感知平衡。</li>
</ul>

<h3>Title: WorldBench: Disambiguating Physics for Diagnostic Evaluation of World Models</h3>
<ul>
<li><strong>Authors: </strong>Rishi Upadhyay, Howard Zhang, Jim Solomon, Ayush Agrawal, Pranay Boreddy, Shruti Satya Narayana, Yunhao Ba, Alex Wong, Celso M de Melo, Achuta Kadambi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21282">https://arxiv.org/abs/2601.21282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21282">https://arxiv.org/pdf/2601.21282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21282]] WorldBench: Disambiguating Physics for Diagnostic Evaluation of World Models(https://arxiv.org/abs/2601.21282)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative foundational models, often termed "world models," have propelled interest in applying them to critical tasks like robotic planning and autonomous system training. For reliable deployment, these models must exhibit high physical fidelity, accurately simulating real-world dynamics. Existing physics-based video benchmarks, however, suffer from entanglement, where a single test simultaneously evaluates multiple physical laws and concepts, fundamentally limiting their diagnostic capability. We introduce WorldBench, a novel video-based benchmark specifically designed for concept-specific, disentangled evaluation, allowing us to rigorously isolate and assess understanding of a single physical concept or law at a time. To make WorldBench comprehensive, we design benchmarks at two different levels: 1) an evaluation of intuitive physical understanding with concepts such as object permanence or scale/perspective, and 2) an evaluation of low-level physical constants and material properties such as friction coefficients or fluid viscosity. When SOTA video-based world models are evaluated on WorldBench, we find specific patterns of failure in particular physics concepts, with all tested models lacking the physical consistency required to generate reliable real-world interactions. Through its concept-specific evaluation, WorldBench offers a more nuanced and scalable framework for rigorously evaluating the physical reasoning capabilities of video generation and world models, paving the way for more robust and generalizable world-model-driven learning.</li>
<li><strong>摘要：</strong>生成基础模型（通常被称为“世界模型”）的最新进展激发了人们将其应用于机器人规划和自主系统训练等关键任务的兴趣。为了实现可靠的部署，这些模型必须表现出高物理保真度，准确模拟现实世界的动态。然而，现有的基于物理的视频基准测试存在纠缠问题，其中单个测试同时评估多个物理定律和概念，从根本上限制了它们的诊断能力。我们推出了 WorldBench，这是一种新颖的基于视频的基准测试，专为特定概念、解开的评估而设计，使我们能够一次严格隔离和评估对单个物理概念或定律的理解。为了使 WorldBench 更全面，我们设计了两个不同级别的基准：1) 使用物体持久性或尺度/视角等概念来评估直观的物理理解，2) 对低级物理常数和材料属性（例如摩擦系数或流体粘度）进行评估。当在 WorldBench 上评估基于 SOTA 视频的世界模型时，我们发现特定物理概念中的特定失败模式，所有测试的模型都缺乏生成可靠的现实世界交互所需的物理一致性。通过针对具体概念的评估，WorldBench 提供了一个更加细致和可扩展的框架，用于严格评估视频生成和世界模型的物理推理能力，为更强大和更通用的世界模型驱动学习铺平道路。</li>
</ul>

<h3>Title: DUET: Distilled LLM Unlearning from an Efficiently Contextualized Teacher</h3>
<ul>
<li><strong>Authors: </strong>Yisheng Zhong, Zhengbang Yang, Zhuangdi Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21283">https://arxiv.org/abs/2601.21283</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21283">https://arxiv.org/pdf/2601.21283</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21283]] DUET: Distilled LLM Unlearning from an Efficiently Contextualized Teacher(https://arxiv.org/abs/2601.21283)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>LLM unlearning is a technique to remove the impacts of undesirable knowledge from the model without retraining from scratch, which is indispensable towards trustworthy AI. Existing unlearning methods face significant limitations: conventional tuning-based unlearning is computationally heavy and prone to catastrophic forgetting. In contrast, in-contextualized unlearning is lightweight for precise unlearning but vulnerable to prompt removal or reverse engineering attacks. In response, we propose Distilled Unlearning from an Efficient Teacher (DUET), a novel distillation-based unlearning method that combines the merits of these two lines of work. It learns a student model to imitate the behavior of a prompt-steered teacher that effectively refuses undesirable knowledge generation while preserving general domain knowledge. Extensive evaluations on existing benchmarks with our enriched evaluation protocols demonstrate that DUET achieves higher performance in both forgetting and utility preservation, while being orders of magnitude more data-efficient than state-of-the-art unlearning methods.</li>
<li><strong>摘要：</strong>LLM 去学习是一种从模型中消除不需要的知识的影响而无需从头开始重新训练的技术，这对于值得信赖的人工智能来说是必不可少的。现有的遗忘方法面临很大的局限性：传统的基于调整的遗忘方法计算量很大，并且容易发生灾难性遗忘。相比之下，上下文中的忘却对于精确的忘却来说是轻量级的，但容易受到快速删除或逆向工程攻击。作为回应，我们提出了高效教师的蒸馏忘却（DUET），这是一种基于蒸馏的新颖忘却方法，结合了这两条工作的优点。它学习学生模型来模仿及时指导的教师的行为，从而有效地拒绝不需要的知识生成，同时保留一般领域知识。通过我们丰富的评估协议对现有基准进行的广泛评估表明，DUET 在遗忘和效用保留方面都取得了更高的性能，同时比最先进的遗忘方法的数据效率高出几个数量级。</li>
</ul>

<h3>Title: PILD: Physics-Informed Learning via Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Tianyi Zeng, Tianyi Wang, Jiaru Zhang, Zimo Zeng, Feiyang Zhang, Yiming Xu, Sikai Chen, Yajie Zou, Yangyang Wang, Junfeng Jiao, Christian Claudel, Xinbo Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.ET, math.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21284">https://arxiv.org/abs/2601.21284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21284">https://arxiv.org/pdf/2601.21284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21284]] PILD: Physics-Informed Learning via Diffusion(https://arxiv.org/abs/2601.21284)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as powerful generative tools for modeling complex data distributions, yet their purely data-driven nature limits applicability in practical engineering and scientific problems where physical laws need to be followed. This paper proposes Physics-Informed Learning via Diffusion (PILD), a framework that unifies diffusion modeling and first-principles physical constraints by introducing a virtual residual observation sampled from a Laplace distribution to supervise generation during training. To further integrate physical laws, a conditional embedding module is incorporated to inject physical information into the denoising network at multiple layers, ensuring consistent guidance throughout the diffusion process. The proposed PILD framework is concise, modular, and broadly applicable to problems governed by ordinary differential equations, partial differential equations, as well as algebraic equations or inequality constraints. Extensive experiments across engineering and scientific tasks including estimating vehicle trajectories, tire forces, Darcy flow and plasma dynamics, demonstrate that our PILD substantially improves accuracy, stability, and generalization over existing physics-informed and diffusion-based baselines.</li>
<li><strong>摘要：</strong>扩散模型已成为对复杂数据分布进行建模的强大生成工具，但其纯粹数据驱动的性质限制了在需要遵循物理定律的实际工程和科学问题中的适用性。本文提出了通过扩散进行物理知情学习（PILD），这是一个框架，通过引入从拉普拉斯分布采样的虚拟残差观察来监督训练期间的生成，从而统一扩散建模和第一原理物理约束。为了进一步整合物理定律，引入了条件嵌入模块，将物理信息注入多层去噪网络中，确保整个扩散过程的指导一致。所提出的 PILD 框架简洁、模块化，广泛适用于常微分方程、偏微分方程以及代数方程或不等式约束所控制的问题。跨工程和科学任务（包括估计车辆轨迹、轮胎力、达西流和等离子体动力学）的广泛实验表明，我们的 PILD 比现有的基于物理和基于扩散的基线显着提高了准确性、稳定性和泛化性。</li>
</ul>

<h3>Title: HiFi-Mesh: High-Fidelity Efficient 3D Mesh Generation via Compact Autoregressive Dependence</h3>
<ul>
<li><strong>Authors: </strong>Yanfeng Li, Tao Tan, Qingquan Gao, Zhiwen Cao, Xiaohong liu, Yue Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21314">https://arxiv.org/abs/2601.21314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21314">https://arxiv.org/pdf/2601.21314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21314]] HiFi-Mesh: High-Fidelity Efficient 3D Mesh Generation via Compact Autoregressive Dependence(https://arxiv.org/abs/2601.21314)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>High-fidelity 3D meshes can be tokenized into one-dimension (1D) sequences and directly modeled using autoregressive approaches for faces and vertices. However, existing methods suffer from insufficient resource utilization, resulting in slow inference and the ability to handle only small-scale sequences, which severely constrains the expressible structural details. We introduce the Latent Autoregressive Network (LANE), which incorporates compact autoregressive dependencies in the generation process, achieving a $6\times$ improvement in maximum generatable sequence length compared to existing methods. To further accelerate inference, we propose the Adaptive Computation Graph Reconfiguration (AdaGraph) strategy, which effectively overcomes the efficiency bottleneck of traditional serial inference through spatiotemporal decoupling in the generation process. Experimental validation demonstrates that LANE achieves superior performance across generation speed, structural detail, and geometric consistency, providing an effective solution for high-quality 3D mesh generation.</li>
<li><strong>摘要：</strong>高保真 3D 网格可以标记为一维 (1D) 序列，并使用面和顶点的自回归方法直接建模。然而，现有方法存在资源利用率不足、推理速度慢且只能处理小规模序列的问题，严重限制了可表达的结构细节。我们引入了潜在自回归网络（LANE），它在生成过程中结合了紧凑的自回归依赖关系，与现有方法相比，最大可生成序列长度提高了 6 倍。为了进一步加速推理，我们提出了自适应计算图重构（AdaGraph）策略，通过生成过程中的时空解耦，有效克服了传统串行推理的效率瓶颈。实验验证表明，LANE 在生成速度、结构细节和几何一致性方面实现了卓越的性能，为高质量 3D 网格生成提供了有效的解决方案。</li>
</ul>

<h3>Title: SR$^{2}$-Net: A General Plug-and-Play Model for Spectral Refinement in Hyperspectral Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Ji-Xuan He, Guohang Zhuang, Junge Bo, Tingyi Li, Chen Ling, Yanan Qiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21338">https://arxiv.org/abs/2601.21338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21338">https://arxiv.org/pdf/2601.21338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21338]] SR$^{2}$-Net: A General Plug-and-Play Model for Spectral Refinement in Hyperspectral Image Super-Resolution(https://arxiv.org/abs/2601.21338)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>HSI-SR aims to enhance spatial resolution while preserving spectrally faithful and physically plausible characteristics. Recent methods have achieved great progress by leveraging spatial correlations to enhance spatial resolution. However, these methods often neglect spectral consistency across bands, leading to spurious oscillations and physically implausible artifacts. While spectral consistency can be addressed by designing the network architecture, it results in a loss of generality and flexibility. To address this issue, we propose a lightweight plug-and-play rectifier, physically priors Spectral Rectification Super-Resolution Network (SR$^{2}$-Net), which can be attached to a wide range of HSI-SR models without modifying their architectures. SR$^{2}$-Net follows an enhance-then-rectify pipeline consisting of (i) Hierarchical Spectral-Spatial Synergy Attention (H-S$^{3}$A) to reinforce cross-band interactions and (ii) Manifold Consistency Rectification (MCR) to constrain the reconstructed spectra to a compact, physically plausible spectral manifold. In addition, we introduce a degradation-consistency loss to enforce data fidelity by encouraging the degraded SR output to match the observed low resolution input. Extensive experiments on multiple benchmarks and diverse backbones demonstrate consistent improvements in spectral fidelity and overall reconstruction quality with negligible computational overhead. Our code will be released upon publication.</li>
<li><strong>摘要：</strong>HSI-SR 旨在提高空间分辨率，同时保留光谱忠实和物理上合理的特性。最近的方法通过利用空间相关性来提高空间分辨率，取得了巨大进展。然而，这些方法常常忽略跨频段的光谱一致性，导致寄生振荡和物理上不可信的伪影。虽然可以通过设计网络架构来解决频谱一致性问题，但这会导致通用性和灵活性的损失。为了解决这个问题，我们提出了一种轻量级的即插即用整流器，物理先验频谱整流超分辨率网络（SR$^{2}$-Net），它可以连接到各种 HSI-SR 模型而无需修改其架构。 SR$^{2}$-Net 遵循先增强后校正的流程，其中包括 (i) 分层谱空间协同注意力 (H-S$^{3}$A)，以增强跨频带相互作用；(ii) 流形一致性校正 (MCR)，以将重建的谱限制为紧凑的、物理上合理的谱流形。此外，我们引入了降级一致性损失，通过鼓励降级的 SR 输出与观察到的低分辨率输入相匹配来增强数据保真度。对多个基准和不同主干网的广泛实验表明，光谱保真度和整体重建质量得到了持续改进，而计算开销可以忽略不计。我们的代码将在发布后发布。</li>
</ul>

<h3>Title: Memorization Control in Diffusion Models from Denoising-centric Perspective</h3>
<ul>
<li><strong>Authors: </strong>Thuy Phuong Vu, Mai Viet Hoang Do, Minhhuy Le, Dinh-Cuong Hoang, Phan Xuan Tan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21348">https://arxiv.org/abs/2601.21348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21348">https://arxiv.org/pdf/2601.21348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21348]] Memorization Control in Diffusion Models from Denoising-centric Perspective(https://arxiv.org/abs/2601.21348)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Controlling memorization in diffusion models is critical for applications that require generated data to closely match the training distribution. Existing approaches mainly focus on data centric or model centric modifications, treating the diffusion model as an isolated predictor. In this paper, we study memorization in diffusion models from a denoising centric perspective. We show that uniform timestep sampling leads to unequal learning contributions across denoising steps due to differences in signal to noise ratio, which biases training toward memorization. To address this, we propose a timestep sampling strategy that explicitly controls where learning occurs along the denoising trajectory. By adjusting the width of the confidence interval, our method provides direct control over the memorization generalization trade off. Experiments on image and 1D signal generation tasks demonstrate that shifting learning emphasis toward later denoising steps consistently reduces memorization and improves distributional alignment with training data, validating the generality and effectiveness of our approach.</li>
<li><strong>摘要：</strong>对于需要生成的数据与训练分布紧密匹配的应用程序来说，控制扩散模型中的记忆至关重要。现有方法主要关注以数据为中心或以模型为中心的修改，将扩散模型视为孤立的预测器。在本文中，我们从以去噪为中心的角度研究扩散模型中的记忆。我们表明，由于信噪比的差异，统一的时间步采样会导致去噪步骤中的学习贡献不均，从而使训练偏向于记忆。为了解决这个问题，我们提出了一种时间步采样策略，该策略明确控制学习沿去噪轨迹发生的位置。通过调整置信区间的宽度，我们的方法提供了对记忆泛化权衡的直接控制。图像和一维信号生成任务的实验表明，将学习重点转向后面的去噪步骤可以持续减少记忆并改善与训练数据的分布对齐，从而验证了我们方法的通用性和有效性。</li>
</ul>

<h3>Title: Rectifying Geometry-Induced Similarity Distortions for Real-World Aerial-Ground Person Re-Identification</h3>
<ul>
<li><strong>Authors: </strong>Kailash A. Hambarde, Hugo Proença</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21405">https://arxiv.org/abs/2601.21405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21405">https://arxiv.org/pdf/2601.21405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21405]] Rectifying Geometry-Induced Similarity Distortions for Real-World Aerial-Ground Person Re-Identification(https://arxiv.org/abs/2601.21405)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Aerial-ground person re-identification (AG-ReID) is fundamentally challenged by extreme viewpoint and distance discrepancies between aerial and ground cameras, which induce severe geometric distortions and invalidate the assumption of a shared similarity space across views. Existing methods primarily rely on geometry-aware feature learning or appearance-conditioned prompting, while implicitly assuming that the geometry-invariant dot-product similarity used in attention mechanisms remains reliable under large viewpoint and scale variations. We argue that this assumption does not hold. Extreme camera geometry systematically distorts the query-key similarity space and degrades attention-based matching, even when feature representations are partially aligned. To address this issue, we introduce Geometry-Induced Query-Key Transformation (GIQT), a lightweight low-rank module that explicitly rectifies the similarity space by conditioning query-key interactions on camera geometry. Rather than modifying feature representations or the attention formulation itself, GIQT adapts the similarity computation to compensate for dominant geometry-induced anisotropic distortions. Building on this local similarity rectification, we further incorporate a geometry-conditioned prompt generation mechanism that provides global, view-adaptive representation priors derived directly from camera geometry. Experiments on four aerial-ground person re-identification benchmarks demonstrate that the proposed framework consistently improves robustness under extreme and previously unseen geometric conditions, while introducing minimal computational overhead compared to state-of-the-art methods.</li>
<li><strong>摘要：</strong>空中-地面人员重新识别（AG-ReID）从根本上受到空中和地面摄像机之间极端视点和距离差异的挑战，这会导致严重的几何扭曲并使跨视图共享相似空间的假设失效。现有方法主要依赖于几何感知特征学习或外观条件提示，同时隐含地假设注意机制中使用的几何不变点积相似性在大视点和尺度变化下仍然可靠。我们认为这个假设不成立。极端的相机几何结构会系统地扭曲查询键相似性空间并降低基于注意力的匹配，即使特征表示部分对齐也是如此。为了解决这个问题，我们引入了几何诱导的查询键转换（GIQT），这是一个轻量级的低秩模块，它通过调节相机几何上的查询键交互来显式地纠正相似性空间。 GIQT 不是修改特征表示或注意力公式本身，而是采用相似性计算来补偿主要几何引起的各向异性失真。在这种局部相似性校正的基础上，我们进一步结合了几何条件提示生成机制，该机制提供直接从相机几何导出的全局、视图自适应表示先验。对四个空中-地面人员重新识别基准的实验表明，所提出的框架在极端和以前未见过的几何条件下持续提高了鲁棒性，同时与最先进的方法相比引入了最小的计算开销。</li>
</ul>

<h3>Title: Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation</h3>
<ul>
<li><strong>Authors: </strong>Zihan Su, Hongyang Wei, Kangrui Cen, Yong Wang, Guanhua Chen, Chun Yuan, Xiangxiang Chu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21406">https://arxiv.org/abs/2601.21406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21406">https://arxiv.org/pdf/2601.21406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21406]] Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation(https://arxiv.org/abs/2601.21406)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Unified Multimodal Models (UMMs) integrate both visual understanding and generation within a single framework. Their ultimate aspiration is to create a cycle where understanding and generation mutually reinforce each other. While recent post-training methods have successfully leveraged understanding to enhance generation, the reverse direction of utilizing generation to improve understanding remains largely unexplored. In this work, we propose UniMRG (Unified Multi-Representation Generation), a simple yet effective architecture-agnostic post-training method. UniMRG enhances the understanding capabilities of UMMs by incorporating auxiliary generation tasks. Specifically, we train UMMs to generate multiple intrinsic representations of input images, namely pixel (reconstruction), depth (geometry), and segmentation (structure), alongside standard visual understanding objectives. By synthesizing these diverse representations, UMMs capture complementary information regarding appearance, spatial relations, and structural layout. Consequently, UMMs develop a deeper and more comprehensive understanding of visual inputs. Extensive experiments across diverse UMM architectures demonstrate that our method notably enhances fine-grained perception, reduces hallucinations, and improves spatial understanding, while simultaneously boosting generation capabilities.</li>
<li><strong>摘要：</strong>统一多模态模型 (UMM) 将视觉理解和生成集成在一个框架内。他们的最终愿望是创​​造一个理解与生成相互促进的循环。虽然最近的训练后方法已经成功地利用理解来增强生成，但利用生成来提高理解的反向方向在很大程度上仍未被探索。在这项工作中，我们提出了 UniMRG（统一多重表示生成），这是一种简单而有效的与架构无关的后训练方法。 UniMRG 通过合并辅助生成任务来增强 UMM 的理解能力。具体来说，我们训练 UMM 生成输入图像的多种内在表示，即像素（重建）、深度（几何）和分割（结构），以及标准视觉理解目标。通过综合这些不同的表示，UMM 捕获有关外观、空间关系和结构布局的补充信息。因此，UMM 对视觉输入有了更深入、更全面的理解。跨不同 UMM 架构的大量实验表明，我们的方法显着增强了细粒度感知、减少幻觉并提高空间理解，同时提高生成能力。</li>
</ul>

<h3>Title: MPF-Net: Exposing High-Fidelity AI-Generated Video Forgeries via Hierarchical Manifold Deviation and Micro-Temporal Fluctuations</h3>
<ul>
<li><strong>Authors: </strong>Xinan He, Kaiqing Lin, Yue Zhou, Jiaming Zhong, Wei Ye, Wenhui Yi, Bing Fan, Feng Ding, Haodong Li, Bo Cao, Bin Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21408">https://arxiv.org/abs/2601.21408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21408">https://arxiv.org/pdf/2601.21408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21408]] MPF-Net: Exposing High-Fidelity AI-Generated Video Forgeries via Hierarchical Manifold Deviation and Micro-Temporal Fluctuations(https://arxiv.org/abs/2601.21408)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of video generation models such as Veo and Wan, the visual quality of synthetic content has reached a level where macro-level semantic errors and temporal inconsistencies are no longer prominent. However, this does not imply that the distinction between real and cutting-edge high-fidelity fake is untraceable. We argue that AI-generated videos are essentially products of a manifold-fitting process rather than a physical recording. Consequently, the pixel composition logic of consecutive adjacent frames residual in AI videos exhibits a structured and homogenous characteristic. We term this phenomenon `Manifold Projection Fluctuations' (MPF). Driven by this insight, we propose a hierarchical dual-path framework that operates as a sequential filtering process. The first, the Static Manifold Deviation Branch, leverages the refined perceptual boundaries of Large-Scale Vision Foundation Models (VFMs) to capture residual spatial anomalies or physical violations that deviate from the natural real-world manifold (off-manifold). For the remaining high-fidelity videos that successfully reside on-manifold and evade spatial detection, we introduce the Micro-Temporal Fluctuation Branch as a secondary, fine-grained filter. By analyzing the structured MPF that persists even in visually perfect sequences, our framework ensures that forgeries are exposed regardless of whether they manifest as global real-world manifold deviations or subtle computational fingerprints.</li>
<li><strong>摘要：</strong>随着Veo和Wan等视频生成模型的快速进步，合成内容的视觉质量已经达到了宏观语义错误和时间不一致不再突出的水平。然而，这并不意味着真品和尖端高保真假货之间的区别是无迹可寻的。我们认为人工智能生成的视频本质上是流形拟合过程的产物，而不是物理记录。因此，AI视频中连续相邻帧残差的像素组成逻辑表现出结构化和同质的特征。我们将这种现象称为“流形投影波动”(MPF)。在这种见解的驱动下，我们提出了一个分层双路径框架，该框架作为顺序过滤过程运行。第一个是静态流形偏差分支，利用大规模视觉基础模型 (VFM) 的精细感知边界来捕获偏离自然现实世界流形（流形外）的残余空间异常或物理违规。对于成功驻留在流形上并逃避空间检测的剩余高保真视频，我们引入了微时间波动分支作为辅助细粒度过滤器。通过分析即使在视觉上完美的序列中仍然存在的结构化 MPF，我们的框架可以确保伪造品被暴露，无论它们是否表现为全局现实世界的流形偏差或微妙的计算指纹。</li>
</ul>

<h3>Title: Mitigating Overthinking in Large Reasoning Models via Difficulty-aware Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Qian Wan, Ziao Xu, Luona Wei, Xiaoxuan Shen, Jianwen Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21418">https://arxiv.org/abs/2601.21418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21418">https://arxiv.org/pdf/2601.21418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21418]] Mitigating Overthinking in Large Reasoning Models via Difficulty-aware Reinforcement Learning(https://arxiv.org/abs/2601.21418)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Reasoning Models (LRMs) achieve explicit chain-of-thought expansion by imitating deep thinking behaviors of humans, demonstrating excellent performance in complex task scenarios. However, the deep-thinking mode often leads to unnecessarily lengthy reasoning and resource inefficiency when handling simple tasks. This overthinking phenomenon may arise from the generation preference triggered by the reward function during post-training. Existing research attempts to mitigate overthinking from the perspective of prompt design or model training, but generally underestimates the importance of task difficulty awareness, which makes it difficult for LRMs to effectively allocate reasoning resources. In this paper, we propose Difficulty-aware Policy Optimization (DiPO), a reinforcement learning-based LRM training framework. DiPO encourages LRM to spontaneously model task complexity, and integrates them into reinforcement learning framework to adjust the generation preferences introduced by post-training. A difficulty modeling method based on model self-reasoning is proposed, which significantly reduces the dependence on manual annotation and formalize task complexity. We further develop a difficulty-signal-enhanced reward function that incorporates a penalty for lengthy reasoning while considering reasoning performance and output format. Experimental results indicate that DiPO enables the model to spontaneously adjust inference overhead, significantly reducing redundant tokens without losing performance due to thought compression.</li>
<li><strong>摘要：</strong>大型推理模型（LRM）通过模仿人类的深度思维行为，实现显式的思维链扩展，在复杂的任务场景中表现出优异的性能。然而，深度思考模式在处理简单任务时往往会导致不必要的冗长推理和资源效率低下。这种过度思考的现象可能是由训练后奖励函数触发的生成偏好引起的。现有研究试图从提示设计或模型训练的角度来缓解过度思考，但普遍低估了任务难度意识的重要性，这使得LRM难以有效分配推理资源。在本文中，我们提出了难度感知策略优化（DiPO），这是一种基于强化学习的 LRM 训练框架。 DiPO鼓励LRM自发地对任务复杂性进行建模，并将其集成到强化学习框架中，以调整训练后引入的生成偏好。提出了一种基于模型自推理的难度建模方法，显着减少了对人工标注的依赖，形式化了任务复杂度。我们进一步开发了一种难度信号增强奖励函数，在考虑推理性能和输出格式的同时，结合了对冗长推理的惩罚。实验结果表明，DiPO 使模型能够自发调整推理开销，显着减少冗余标记，而不会因思想压缩而损失性能。</li>
</ul>

<h3>Title: Revisiting Diffusion Model Predictions Through Dimensionality</h3>
<ul>
<li><strong>Authors: </strong>Qing Jin, Chaoyang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21419">https://arxiv.org/abs/2601.21419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21419">https://arxiv.org/pdf/2601.21419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21419]] Revisiting Diffusion Model Predictions Through Dimensionality(https://arxiv.org/abs/2601.21419)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion and flow matching models have highlighted a shift in the preferred prediction target -- moving from noise ($\varepsilon$) and velocity (v) to direct data (x) prediction -- particularly in high-dimensional settings. However, a formal explanation of why the optimal target depends on the specific properties of the data remains elusive. In this work, we provide a theoretical framework based on a generalized prediction formulation that accommodates arbitrary output targets, of which $\varepsilon$-, v-, and x-prediction are special cases. We derive the analytical relationship between data's geometry and the optimal prediction target, offering a rigorous justification for why x-prediction becomes superior when the ambient dimension significantly exceeds the data's intrinsic dimension. Furthermore, while our theory identifies dimensionality as the governing factor for the optimal prediction target, the intrinsic dimension of manifold-bound data is typically intractable to estimate in practice. To bridge this gap, we propose k-Diff, a framework that employs a data-driven approach to learn the optimal prediction parameter k directly from data, bypassing the need for explicit dimension estimation. Extensive experiments in both latent-space and pixel-space image generation demonstrate that k-Diff consistently outperforms fixed-target baselines across varying architectures and data scales, providing a principled and automated approach to enhancing generative performance.</li>
<li><strong>摘要：</strong>扩散和流动匹配模型的最新进展凸显了首选预测目标的转变——从噪声 ($\varepsilon$) 和速度 (v) 转向直接数据 (x) 预测——特别是在高维设置中。然而，对于为什么最佳目标取决于数据的特定属性的正式解释仍然难以捉摸。在这项工作中，我们提供了一个基于广义预测公式的理论框架，该公式可容纳任意输出目标，其中$\varepsilon$-、v-和x-预测是特殊情况。我们推导了数据的几何形状和最佳预测目标之间的分析关系，为为什么当环境维度显着超过数据的固有维度时 x 预测变得优越提供了严格的理由。此外，虽然我们的理论将维度确定为最佳预测目标的控制因素，但流形绑定数据的内在维度在实践中通常难以估计。为了弥补这一差距，我们提出了 k-Diff，这是一个采用数据驱动方法直接从数据中学习最佳预测参数 k 的框架，绕过了显式维度估计的需要。潜在空间和像素空间图像生成方面的大量实验表明，k-Diff 在不同的架构和数据规模上始终优于固定目标基线，从而提供了一种有原则的自动化方法来增强生成性能。</li>
</ul>

<h3>Title: Synthetic Pattern Generation and Detection of Financial Activities using Graph Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Francesco Zola, Lucia Muñoz, Andrea Venturi, Amaia Gil</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21446">https://arxiv.org/abs/2601.21446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21446">https://arxiv.org/pdf/2601.21446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21446]] Synthetic Pattern Generation and Detection of Financial Activities using Graph Autoencoders(https://arxiv.org/abs/2601.21446)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Illicit financial activities such as money laundering often manifest through recurrent topological patterns in transaction networks. Detecting these patterns automatically remains challenging due to the scarcity of labeled real-world data and strict privacy constraints. To address this, we investigate whether Graph Autoencoders (GAEs) can effectively learn and distinguish topological patterns that mimic money laundering operations when trained on synthetic data. The analysis consists of two phases: (i) data generation, where synthetic samples are created for seven well-known illicit activity patterns using parametrized generators that preserve structural consistency while introducing realistic variability; and (ii) model training and validation, where separate GAEs are trained on each pattern without explicit labels, relying solely on reconstruction error as an indicator of learned structure. We compare three GAE implementations based on three distinct convolutional layers: Graph Convolutional (GAE-GCN), GraphSAGE (GAE-SAGE), and Graph Attention Network (GAE-GAT). Experimental results show that GAE-GCN achieves the most consistent reconstruction performance across patterns, while GAE-SAGE and GAE-GAT exhibit competitive results only in few specific patterns. These findings suggest that graph-based representation learning on synthetic data provides a viable path toward developing AI-driven tools for detecting illicit behaviors, overcoming the limitations of financial datasets.</li>
<li><strong>摘要：</strong>洗钱等非法金融活动通常通过交易网络中反复出现的拓扑模式表现出来。由于标记的现实世界数据的稀缺和严格的隐私限制，自动检测这些模式仍然具有挑战性。为了解决这个问题，我们研究了图自动编码器（GAE）在合成数据上进行训练时是否可以有效地学习和区分模仿洗钱操作的拓扑模式。该分析包括两个阶段：(i) 数据生成，使用参数化生成器为七种众所周知的非法活动模式创建合成样本，该生成器在引入实际可变性的同时保持结构一致性； (ii) 模型训练和验证，其中单独的 GAE 在没有明确标签的情况下针对每个模式进行训练，仅依靠重建误差作为学习结构的指标。我们比较基于三个不同卷积层的三种 GAE 实现：图卷积 (GAE-GCN)、GraphSAGE (GAE-SAGE) 和图注意网络 (GAE-GAT)。实验结果表明，GAE-GCN 在不同模式下实现了最一致的重建性能，而 GAE-SAGE 和 GAE-GAT 仅在少数特定模式中表现出有竞争力的结果。这些发现表明，基于图形的合成数据表示学习为开发人工智能驱动的工具来检测非法行为、克服金融数据集的局限性提供了一条可行的途径。</li>
</ul>

<h3>Title: SAGE: Sequence-level Adaptive Gradient Evolution for Generative Recommendation</h3>
<ul>
<li><strong>Authors: </strong>Yu Xie, Xing Kai Ren, Ying Qi, Hu Yao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21452">https://arxiv.org/abs/2601.21452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21452">https://arxiv.org/pdf/2601.21452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21452]] SAGE: Sequence-level Adaptive Gradient Evolution for Generative Recommendation(https://arxiv.org/abs/2601.21452)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While works such as OneRec have validated the scaling laws of Large Language Models (LLMs) in recommender systems, they rely on a cumbersome separate vocabulary. This dependency prevents the model architecture from reusing native LLM vocabularies, resulting in high maintenance costs and poor scalability. In response, we aim to efficiently reuse open-source LLM architectures without constructing a separate tokenization vocabulary. Furthermore, we identify that the optimization strategy of OneRec Gradient Bounded Policy Optimization (GBPO),suffers from a "Symmetric Conservatism" problem: its static gradient boundaries structurally suppress the update momentum required for cold-start items and fail to prevent diversity collapse in high-noise this http URL address this issue, we propose SAGE (Sequence-level Adaptive Gradient Evolution), a unified optimization framework tailored for list-wise generative recommendation. SAGE introduces two key innovations:(1) Sequence-level Signal Decoupling: By combining a geometric mean importance ratio with decoupled multi-objective advantages, we eliminate token-level variance and resolve the "Reward Collapse" problem. (2) Asymmetric Adaptive Dynamics: We construct a dynamic gradient manifold that applies a "Boost Factor" to high-potential cold start items to achieve super-linear updates and employs an "Entropy Aware Penalty" to break information cocoons. Theoretical analysis and empirical results demonstrate that SAGE effectively unblocks cold-start traffic and sustains recommendation diversity, all while retaining the numerical stability of GBPO.</li>
<li><strong>摘要：</strong>虽然 OneRec 等作品已经验证了推荐系统中大型语言模型 (LLM) 的缩放法则，但它们依赖于繁琐的单独词汇表。这种依赖性导致模型架构无法重用本地LLM词汇，导致维护成本高且可扩展性差。作为回应，我们的目标是有效地重用开源 LLM 架构，而无需构建单独的标记化词汇表。此外，我们发现 OneRec 梯度有界策略优化（GBPO）的优化策略存在“对称保守主义”问题：其静态梯度边界在结构上抑制了冷启动项所需的更新动量，并且无法防止高噪声中的多样性崩溃。针对此问题，我们提出了 SAGE（序列级自适应梯度进化），这是一种专为列表式生成推荐而定制的统一优化框架。 SAGE 引入了两个关键创新：（1）序列级信号解耦：通过将几何平均重要性比与解耦的多目标优势相结合，我们消除了令牌级方差并解决了“奖励崩溃”问题。 （2）非对称自适应动力学：我们构建了一个动态梯度流形，将“Boost Factor”应用于高潜力的冷启动项以实现超线性更新，并采用“Entropy Aware Penalty”来打破信息茧。理论分析和实证结果表明，SAGE 有效地畅通了冷启动流量并维持了推荐多样性，同时保留了 GBPO 的数值稳定性。</li>
</ul>

<h3>Title: Partial Feedback Online Learning</h3>
<ul>
<li><strong>Authors: </strong>Shihao Shao, Cong Fang, Zhouchen Lin, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21462">https://arxiv.org/abs/2601.21462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21462">https://arxiv.org/pdf/2601.21462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21462]] Partial Feedback Online Learning(https://arxiv.org/abs/2601.21462)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We study partial-feedback online learning, where each instance admits a set of correct labels, but the learner only observes one correct label per round; any prediction within the correct set is counted as correct. This model captures settings such as language generation, where multiple responses may be valid but data provide only a single reference. We give a near-complete characterization of minimax regret for both deterministic and randomized learners in the set-realizable regime, i.e., in the regime where sublinear regret is generally attainable. For deterministic learners, we introduce the Partial-Feedback Littlestone dimension (PFLdim) and show it precisely governs learnability and minimax regret; technically, PFLdim cannot be defined via the standard version space, requiring a new collection version space viewpoint and an auxiliary dimension used only in the proof. We further develop the Partial-Feedback Measure Shattering dimension (PMSdim) to obtain tight bounds for randomized learners. We identify broad conditions ensuring inseparability between deterministic and randomized learnability (e.g., finite Helly number or nested-inclusion label structure), and extend the argument to set-valued online learning, resolving an open question of Raman et al. [2024b]. Finally, we show a sharp separation from weaker realistic and agnostic variants: outside set realizability, the problem can become information-theoretically intractable, with linear regret possible even for $|H|=2$. This highlights the need for fundamentally new, noise-sensitive complexity measures to meaningfully characterize learnability beyond set realizability.</li>
<li><strong>摘要：</strong>我们研究部分反馈在线学习，其中每个实例都承认一组正确的标签，但学习者每轮只观察到一个正确的标签；正确组内的任何预测都被视为正确。该模型捕获语言生成等设置，其中多个响应可能有效，但数据仅提供单个参考。我们对确定性学习者和随机学习者在集合可实现机制（即通常可以实现亚线性后悔的机制）中给出了极小最大遗憾的近乎完整的表征。对于确定性学习者，我们引入了部分反馈 Littlestone 维度（PFLdim），并表明它精确控制可学习性和极小极大遗憾；从技术上讲，PFLdim 无法通过标准版本空间来定义，需要一个新的集合版本空间视点和仅在证明中使用的辅助维度。我们进一步开发了部分反馈测量粉碎维度（PMSdim）以获得随机学习者的严格界限。我们确定了确保确定性和随机可学习性之间不可分离的广泛条件（例如，有限 Helly 数或嵌套包含标签结构），并将论证扩展到集值在线学习，解决了 Raman 等人的一个悬而未决的问题。 [2024b]。最后，我们展示了与较弱的现实和不可知变体的明显区别：在设定的可实现性之外，问题可能会变得信息理论上难以解决，即使对于 $|H|=2$ 也可能存在线性遗憾。这凸显了对全新的、对噪声敏感的复杂性测量的需求，以有意义地表征超越设定可实现性的可学习性。</li>
</ul>

<h3>Title: ETS: Energy-Guided Test-Time Scaling for Training-Free RL Alignment</h3>
<ul>
<li><strong>Authors: </strong>Xiuyu Li, Jinkai Zhang, Mingyang Yi, Yu Li, Longqiang Wang, Yue Wang, Ju Fan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21484">https://arxiv.org/abs/2601.21484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21484">https://arxiv.org/pdf/2601.21484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21484]] ETS: Energy-Guided Test-Time Scaling for Training-Free RL Alignment(https://arxiv.org/abs/2601.21484)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) post-training alignment for language models is effective, but also costly and unstable in practice, owing to its complicated training process. To address this, we propose a training-free inference method to sample directly from the optimal RL policy. The transition probability applied to Masked Language Modeling (MLM) consists of a reference policy model and an energy term. Based on this, our algorithm, Energy-Guided Test-Time Scaling (ETS), estimates the key energy term via online Monte Carlo, with a provable convergence rate. Moreover, to ensure practical efficiency, ETS leverages modern acceleration frameworks alongside tailored importance sampling estimators, substantially reducing inference latency while provably preserving sampling quality. Experiments on MLM (including autoregressive models and diffusion language models) across reasoning, coding, and science benchmarks show that our ETS consistently improves generation quality, validating its effectiveness and design.</li>
<li><strong>摘要：</strong>强化学习（RL）语言模型的训练后对齐是有效的，但由于其复杂的训练过程，在实践中成本高昂且不稳定。为了解决这个问题，我们提出了一种无需训练的推理方法，可以直接从最优强化学习策略中进行采样。应用于掩码语言模型（MLM）的转移概率由参考策略模型和能量项组成。基于此，我们的算法能量引导测试时间缩放（ETS）通过在线蒙特卡罗估计关键能量项，并具有可证明的收敛速度。此外，为了确保实际效率，ETS 利用现代加速框架以及定制的重要性采样估计器，显着减少推理延迟，同时可证明保持采样质量。跨推理、编码和科学基准的 MLM（包括自回归模型和扩散语言模型）实验表明，我们的 ETS 持续提高生成质量，验证其有效性和设计。</li>
</ul>

<h3>Title: SimGraph: A Unified Framework for Scene Graph-Based Image Generation and Editing</h3>
<ul>
<li><strong>Authors: </strong>Thanh-Nhan Vo, Trong-Thuan Nguyen, Tam V. Nguyen, Minh-Triet Tran</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21498">https://arxiv.org/abs/2601.21498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21498">https://arxiv.org/pdf/2601.21498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21498]] SimGraph: A Unified Framework for Scene Graph-Based Image Generation and Editing(https://arxiv.org/abs/2601.21498)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in Generative Artificial Intelligence (GenAI) have significantly enhanced the capabilities of both image generation and editing. However, current approaches often treat these tasks separately, leading to inefficiencies and challenges in maintaining spatial consistency and semantic coherence between generated content and edits. Moreover, a major obstacle is the lack of structured control over object relationships and spatial arrangements. Scene graph-based methods, which represent objects and their interrelationships in a structured format, offer a solution by providing greater control over composition and interactions in both image generation and editing. To address this, we introduce SimGraph, a unified framework that integrates scene graph-based image generation and editing, enabling precise control over object interactions, layouts, and spatial coherence. In particular, our framework integrates token-based generation and diffusion-based editing within a single scene graph-driven model, ensuring high-quality and consistent results. Through extensive experiments, we empirically demonstrate that our approach outperforms existing state-of-the-art methods.</li>
<li><strong>摘要：</strong>生成人工智能（GenAI）的最新进展显着增强了图像生成和编辑的能力。然而，当前的方法通常单独处理这些任务，导致在维护生成的内容和编辑之间的空间一致性和语义一致性方面效率低下并面临挑战。此外，一个主要障碍是缺乏对对象关系和空间安排的结构化控制。基于场景图的方法以结构化格式表示对象及其相互关系，通过提供对图像生成和编辑中的合成和交互的更好控制来提供解决方案。为了解决这个问题，我们引入了 SimGraph，这是一个集成了基于场景图的图像生成和编辑的统一框架，能够精确控制对象交互、布局和空间连贯性。特别是，我们的框架将基于令牌的生成和基于扩散的编辑集成在单个场景图驱动模型中，确保高质量和一致的结果。通过大量的实验，我们凭经验证明我们的方法优于现有的最先进的方法。</li>
</ul>

<h3>Title: Task-Awareness Improves LLM Generations and Uncertainty</h3>
<ul>
<li><strong>Authors: </strong>Tim Tomov, Dominik Fuchsgruber, Stephan Günnemann</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21500">https://arxiv.org/abs/2601.21500</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21500">https://arxiv.org/pdf/2601.21500</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21500]] Task-Awareness Improves LLM Generations and Uncertainty(https://arxiv.org/abs/2601.21500)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In many applications of LLMs, natural language responses often have an underlying structure such as representing discrete labels, numerical values, or graphs. Yet, existing decoding and uncertainty estimation methods operate only in language space and largely disregard structural information. We address this by modeling LLM outputs directly in a task-dependent latent structure. By equipping this structure with a dissimilarity measure, we can compute Bayes-optimal responses. These are not selected from sampled generations but are newly synthesized by combining individual responses in the latent space. Across different tasks, Bayes-optimal responses consistently outperform standard decoding methods like beam search. Moreover, quantifying uncertainty via the induced Bayesian risk captures variations in terms of the latent structure and improves alignment with output quality and correctness. Our decision-theoretic framework is applicable to any problem that admits a latent response structure and enables reliable task-aware LLM predictions.</li>
<li><strong>摘要：</strong>在法学硕士的许多应用中，自然语言响应通常具有底层结构，例如表示离散标签、数值或图形。然而，现有的解码和不确定性估计方法仅在语言空间中运行并且很大程度上忽略了结构信息。我们通过直接在任务相关的潜在结构中对 LLM 输出进行建模来解决这个问题。通过为该结构配备相异性度量，我们可以计算贝叶斯最优响应。这些不是从采样的世代中选择的，而是通过组合潜在空间中的个体响应来新合成的。在不同的任务中，贝叶斯最优响应始终优于波束搜索等标准解码方法。此外，通过引入贝叶斯风险来量化不确定性可以捕获潜在结构的变化，并提高与输出质量和正确性的一致性。我们的决策理论框架适用于任何承认潜在响应结构并能够实现可靠的任务感知 LLM 预测的问题。</li>
</ul>

<h3>Title: HERS: Hidden-Pattern Expert Learning for Risk-Specific Vehicle Damage Adaptation in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Teerapong Panboonyuen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21517">https://arxiv.org/abs/2601.21517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21517">https://arxiv.org/pdf/2601.21517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21517]] HERS: Hidden-Pattern Expert Learning for Risk-Specific Vehicle Damage Adaptation in Diffusion Models(https://arxiv.org/abs/2601.21517)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in text-to-image (T2I) diffusion models have enabled increasingly realistic synthesis of vehicle damage, raising concerns about their reliability in automated insurance workflows. The ability to generate crash-like imagery challenges the boundary between authentic and synthetic data, introducing new risks of misuse in fraud or claim manipulation. To address these issues, we propose HERS (Hidden-Pattern Expert Learning for Risk-Specific Damage Adaptation), a framework designed to improve fidelity, controllability, and domain alignment of diffusion-generated damage images. HERS fine-tunes a base diffusion model via domain-specific expert adaptation without requiring manual annotation. Using self-supervised image-text pairs automatically generated by a large language model and T2I pipeline, HERS models each damage category, such as dents, scratches, broken lights, or cracked paint, as a separate expert. These experts are later integrated into a unified multi-damage model that balances specialization with generalization. We evaluate HERS across four diffusion backbones and observe consistent improvements: plus 5.5 percent in text faithfulness and plus 2.3 percent in human preference ratings compared to baselines. Beyond image fidelity, we discuss implications for fraud detection, auditability, and safe deployment of generative models in high-stakes domains. Our findings highlight both the opportunities and risks of domain-specific diffusion, underscoring the importance of trustworthy generation in safety-critical applications such as auto insurance.</li>
<li><strong>摘要：</strong>文本到图像 (T2I) 扩散模型的最新进展使得车辆损坏的合成变得越来越真实，引发了人们对其在自动化保险工作流程中的可靠性的担忧。生成类似崩溃图像的能力挑战了真实数据和合成数据之间的界限，带来了欺诈或索赔操纵中滥用的新风险。为了解决这些问题，我们提出了 HERS（针对特定风险的损伤适应的隐藏模式专家学习），这是一个旨在提高扩散生成的损伤图像的保真度、可控性和域对齐的框架。 HERS 通过特定领域的专家适应来微调基础扩散模型，而无需手动注释。 HERS 使用由大型语言模型和 T2I 管道自动生成的自监督图像文本对，将每个损坏类别（例如凹痕、划痕、破碎的灯或破裂的油漆）作为单独的专家进行建模。这些专家随后被集成到一个统一的多损伤模型中，以平衡专业化与泛化。我们评估了四个扩散主干的 HERS，并观察到一致的改进：与基线相比，文本忠实度提高了 5.5%，人类偏好评分提高了 2.3%。除了图像保真度之外，我们还讨论了对欺诈检测、可审计性以及高风险领域中生成模型的安全部署的影响。我们的研究结果强调了特定领域扩散的机遇和风险，强调了可信发电在汽车保险等安全关键应用中的重要性。</li>
</ul>

<h3>Title: Bi-Anchor Interpolation Solver for Accelerating Generative Modeling</h3>
<ul>
<li><strong>Authors: </strong>Hongxu Chen, Hongxiang Li, Zhen Wang, Long Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21542">https://arxiv.org/abs/2601.21542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21542">https://arxiv.org/pdf/2601.21542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21542]] Bi-Anchor Interpolation Solver for Accelerating Generative Modeling(https://arxiv.org/abs/2601.21542)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Flow Matching (FM) models have emerged as a leading paradigm for high-fidelity synthesis. However, their reliance on iterative Ordinary Differential Equation (ODE) solving creates a significant latency bottleneck. Existing solutions face a dichotomy: training-free solvers suffer from significant performance degradation at low Neural Function Evaluations (NFEs), while training-based one- or few-steps generation methods incur prohibitive training costs and lack plug-and-play versatility. To bridge this gap, we propose the Bi-Anchor Interpolation Solver (BA-solver). BA-solver retains the versatility of standard training-free solvers while achieving significant acceleration by introducing a lightweight SideNet (1-2% backbone size) alongside the frozen backbone. Specifically, our method is founded on two synergistic components: \textbf{1) Bidirectional Temporal Perception}, where the SideNet learns to approximate both future and historical velocities without retraining the heavy backbone; and 2) Bi-Anchor Velocity Integration, which utilizes the SideNet with two anchor velocities to efficiently approximate intermediate velocities for batched high-order integration. By utilizing the backbone to establish high-precision ``anchors'' and the SideNet to densify the trajectory, BA-solver enables large interval sizes with minimized error. Empirical results on ImageNet-256^2 demonstrate that BA-solver achieves generation quality comparable to 100+ NFEs Euler solver in just 10 NFEs and maintains high fidelity in as few as 5 NFEs, incurring negligible training costs. Furthermore, BA-solver ensures seamless integration with existing generative pipelines, facilitating downstream tasks such as image editing.</li>
<li><strong>摘要：</strong>流匹配 (FM) 模型已成为高保真合成的领先范例。然而，它们对迭代常微分方程 (ODE) 求解的依赖造成了严重的延迟瓶颈。现有的解决方案面临着二分法：免训练求解器在神经功能评估 (NFE) 较低时会遭受显着的性能下降，而基于训练的一步或几步生成方法会产生高昂的培训成本，并且缺乏即插即用的多功能性。为了弥补这一差距，我们提出了双锚插值求解器（BA 求解器）。 BA 求解器保留了标准免训练求解器的多功能性，同时通过在冻结主干网旁边引入轻量级 SideNet（主干网大小 1-2%）来实现显着加速。具体来说，我们的方法建立在两个协同组件的基础上： \textbf{1) 双向时间感知}，其中 SideNet 学习近似未来和历史速度，而无需重新训练重型骨干网； 2) 双锚速度集成，利用具有两个锚速度的 SideNet 来有效地近似中间速度，以进行批量高阶集成。通过利用主干网络建立高精度“锚点”并使用 SideNet 来致密化轨迹，BA 求解器能够以最小的误差实现较大的间隔大小。 ImageNet-256^2 上的经验结果表明，BA 求解器仅用 10 个 NFE 即可实现与 100 多个 NFE 欧拉求解器相当的生成质量，并在少至 5 个 NFE 中保持高保真度，从而产生的训练成本可以忽略不计。此外，BA-solver 确保与现有生成管道无缝集成，促进图像编辑等下游任务。</li>
</ul>

<h3>Title: FlexCausal: Flexible Causal Disentanglement via Structural Flow Priors and Manifold-Aware Interventions</h3>
<ul>
<li><strong>Authors: </strong>Yutao Jin, Yuang Tao, Junyong Zhai</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21567">https://arxiv.org/abs/2601.21567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21567">https://arxiv.org/pdf/2601.21567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21567]] FlexCausal: Flexible Causal Disentanglement via Structural Flow Priors and Manifold-Aware Interventions(https://arxiv.org/abs/2601.21567)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Causal Disentangled Representation Learning(CDRL) aims to learn and disentangle low dimensional representations and their underlying causal structure from observations. However, existing disentanglement methods rely on a standard mean-field approximation with a diagonal posterior covariance, which decorrelates all latent dimensions. Additionally, these methods often assume isotropic Gaussian priors for exogenous noise, failing to capture the complex, non-Gaussian statistical properties prevalent in real-world causal factors. Therefore, we propose FlexCausal, a novel CDRL framework based on a block-diagonal covariance VAE. FlexCausal utilizes a Factorized Flow-based Prior to realistically model the complex densities of exogenous noise, effectively decoupling the learning of causal mechanisms from distributional statistics. By integrating supervised alignment objectives with counterfactual consistency constraints, our framework ensures a precise structural correspondence between the learned latent subspaces and the ground-truth causal relations. Finally, we introduce a manifold-aware relative intervention strategy to ensure high-fidelity generation. Experimental results on both synthetic and real-world datasets demonstrate that FlexCausal significantly outperforms other methods.</li>
<li><strong>摘要：</strong>因果解缠表示学习（CDRL）旨在从观察中学习并解开低维表示及其潜在的因果结构。然而，现有的解缠结方法依赖于具有对角后验协方差的标准平均场近似，该近似可以解相关所有潜在维度。此外，这些方法通常假设外源噪声具有各向同性高斯先验，无法捕获现实世界因果因素中普遍存在的复杂的非高斯统计特性。因此，我们提出了 FlexCausal，一种基于块对角协方差 VAE 的新型 CDRL 框架。 FlexCausal 利用基于因子流的先验对外生噪声的复杂密度进行实际建模，有效地将因果机制的学习与分布统计数据解耦。通过将监督对齐目标与反事实一致性约束相结合，我们的框架确保了学习的潜在子空间和真实因果关系之间的精确结构对应。最后，我们引入了一种流形感知的相对干预策略以确保高保真度生成。合成数据集和真实数据集的实验结果表明，FlexCausal 的性能显着优于其他方法。</li>
</ul>

<h3>Title: CORDS: Continuous Representations of Discrete Structures</h3>
<ul>
<li><strong>Authors: </strong>Tin Hadži Veljković, Erik Bekkers, Michael Tiemann, Jan-Willem van de Meent</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21583">https://arxiv.org/abs/2601.21583</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21583">https://arxiv.org/pdf/2601.21583</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21583]] CORDS: Continuous Representations of Discrete Structures(https://arxiv.org/abs/2601.21583)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Many learning problems require predicting sets of objects when the number of objects is not known beforehand. Examples include object detection, molecular modeling, and scientific inference tasks such as astrophysical source detection. Existing methods often rely on padded representations or must explicitly infer the set size, which often poses challenges. We present a novel strategy for addressing this challenge by casting prediction of variable-sized sets as a continuous inference problem. Our approach, CORDS (Continuous Representations of Discrete Structures), provides an invertible mapping that transforms a set of spatial objects into continuous fields: a density field that encodes object locations and count, and a feature field that carries their attributes over the same support. Because the mapping is invertible, models operate entirely in field space while remaining exactly decodable to discrete sets. We evaluate CORDS across molecular generation and regression, object detection, simulation-based inference, and a mathematical task involving recovery of local maxima, demonstrating robust handling of unknown set sizes with competitive accuracy.</li>
<li><strong>摘要：</strong>许多学习问题需要在事先未知对象数量的情况下预测对象集。示例包括物体检测、分子建模和科学推理任务（例如天体物理源检测）。现有方法通常依赖于填充表示或必须显式推断集合大小，这通常会带来挑战。我们提出了一种新颖的策略，通过将可变大小集的预测作为连续推理问题来应对这一挑战。我们的方法 CORDS（离散结构的连续表示）提供了一种可逆映射，将一组空间对象转换为连续字段：对对象位置和计数进行编码的密度字段，以及通过相同支持承载其属性的特征字段。由于映射是可逆的，因此模型完全在场空间中运行，同时保持精确地可解码为离散集。我们评估了分子生成和回归、对象检测、基于模拟的推理以及涉及局部最大值恢复的数学任务的 CORDS，展示了对未知集合大小的鲁棒处理以及具有竞争力的准确性。</li>
</ul>

<h3>Title: Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening</h3>
<ul>
<li><strong>Authors: </strong>Xiaotong Ji, Rasul Tutunov, Matthieu Zimmer, Haitham Bou Ammar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21590">https://arxiv.org/abs/2601.21590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21590">https://arxiv.org/pdf/2601.21590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21590]] Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening(https://arxiv.org/abs/2601.21590)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) post-training is a dominant approach for improving the reasoning performance of large language models (LLMs), yet growing evidence suggests that its gains arise primarily from distribution sharpening rather than the acquisition of new capabilities. Recent work has shown that sampling from the power distribution of LLMs using Markov chain Monte Carlo (MCMC) can recover performance comparable to RL post-training without relying on external rewards; however, the high computational cost of MCMC makes such approaches impractical for widespread adoption. In this work, we propose a theoretically grounded alternative that eliminates the need for iterative MCMC. We derive a novel formulation showing that the global power distribution can be approximated by a token-level scaled low-temperature one, where the scaling factor captures future trajectory quality. Leveraging this insight, we introduce a training-free and verifier-free algorithm that sharpens the base model's generative distribution autoregressively. Empirically, we evaluate our method on math, QA, and code tasks across four LLMs, and show that our method matches or surpasses one-shot GRPO without relying on any external rewards, while reducing inference latency by over 10x compared to MCMC-based sampling.</li>
<li><strong>摘要：</strong>强化学习 (RL) 后训练是提高大型语言模型 (LLM) 推理性能的主要方法，但越来越多的证据表明，其收益主要来自分布锐化，而不是获得新能力。最近的工作表明，使用马尔可夫链蒙特卡罗（MCMC）从 LLM 的功率分布中采样可以在不依赖外部奖励的情况下恢复与 RL 训练后相当的性能；然而，MCMC 的高计算成本使得这种方法无法广泛采用。在这项工作中，我们提出了一种基于理论的替代方案，消除了迭代 MCMC 的需要。我们推导出一种新颖的公式，表明全局功率分布可以通过令牌级缩放的低温功率分布来近似，其中缩放因子捕获未来的轨迹质量。利用这种洞察力，我们引入了一种免训练和免验证的算法，该算法可以自回归锐化基本模型的生成分布。根据经验，我们在四个法学硕士的数学、QA 和代码任务上评估了我们的方法，并表明我们的方法在不依赖任何外部奖励的情况下匹配或超过了一次性 GRPO，同时与基于 MCMC 的采样相比，推理延迟减少了 10 倍以上。</li>
</ul>

<h3>Title: Unifying Heterogeneous Degradations: Uncertainty-Aware Diffusion Bridge Model for All-in-One Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Luwei Tu, Jiawei Wu, Xing Luo, Zhi Jin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21592">https://arxiv.org/abs/2601.21592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21592">https://arxiv.org/pdf/2601.21592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21592]] Unifying Heterogeneous Degradations: Uncertainty-Aware Diffusion Bridge Model for All-in-One Image Restoration(https://arxiv.org/abs/2601.21592)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>All-in-One Image Restoration (AiOIR) faces the fundamental challenge in reconciling conflicting optimization objectives across heterogeneous degradations. Existing methods are often constrained by coarse-grained control mechanisms or fixed mapping schedules, yielding suboptimal adaptation. To address this, we propose an Uncertainty-Aware Diffusion Bridge Model (UDBM), which innovatively reformulates AiOIR as a stochastic transport problem steered by pixel-wise uncertainty. By introducing a relaxed diffusion bridge formulation which replaces the strict terminal constraint with a relaxed constraint, we model the uncertainty of degradations while theoretically resolving the drift singularity inherent in standard diffusion bridges. Furthermore, we devise a dual modulation strategy: the noise schedule aligns diverse degradations into a shared high-entropy latent space, while the path schedule adaptively regulates the transport trajectory motivated by the viscous dynamics of entropy regularization. By effectively rectifying the transport geometry and dynamics, UDBM achieves state-of-the-art performance across diverse restoration tasks within a single inference step.</li>
<li><strong>摘要：</strong>多合一图像恢复 (AiOIR) 面临着协调异构退化中相互冲突的优化目标的根本挑战。现有方法通常受到粗粒度控制机制或固定映射时间表的限制，从而产生次优的适应。为了解决这个问题，我们提出了一种不确定性感知扩散桥模型（UDBM），该模型创新地将 AiOIR 重新表述为由像素级不确定性引导的随机传输问题。通过引入宽松的扩散桥公式，用宽松的约束代替严格的终端约束，我们对退化的不确定性进行建模，同时从理论上解决标准扩散桥固有的漂移奇点。此外，我们设计了一种双重调制策略：噪声调度将不同的退化排列到共享的高熵潜在空间中，而路径调度自适应地调节由熵正则化的粘性动力学驱动的传输轨迹。通过有效地纠正传输几何和动力学，UDBM 在单个推理步骤中实现了跨不同恢复任务的最先进的性能。</li>
</ul>

<h3>Title: HydroSense: A Dual-Microcontroller IoT Framework for Real-Time Multi-Parameter Water Quality Monitoring with Edge Processing and Cloud Analytics</h3>
<ul>
<li><strong>Authors: </strong>Abdul Hasib, A. S. M. Ahsanul Sarkar Akib, Anish Giri</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21595">https://arxiv.org/abs/2601.21595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21595">https://arxiv.org/pdf/2601.21595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21595]] HydroSense: A Dual-Microcontroller IoT Framework for Real-Time Multi-Parameter Water Quality Monitoring with Edge Processing and Cloud Analytics(https://arxiv.org/abs/2601.21595)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>The global water crisis necessitates affordable, accurate, and real-time water quality monitoring solutions. Traditional approaches relying on manual sampling or expensive commercial systems fail to address accessibility challenges in resource-constrained environments. This paper presents HydroSense, an innovative Internet of Things framework that integrates six critical water quality parameters including pH, dissolved oxygen (DO), temperature, total dissolved solids (TDS), estimated nitrogen, and water level into a unified monitoring system. HydroSense employs a novel dual-microcontroller architecture, utilizing Arduino Uno for precision analog measurements with five-point calibration algorithms and ESP32 for wireless connectivity, edge processing, and cloud integration. The system implements advanced signal processing techniques including median filtering for TDS measurement, temperature compensation algorithms, and robust error handling. Experimental validation over 90 days demonstrates exceptional performance metrics: pH accuracy of plus or minus 0.08 units across the 0 to 14 range, DO measurement stability within plus or minus 0.2 mg/L, TDS accuracy of plus or minus 1.9 percent across 0 to 1000 ppm, and 99.8 percent cloud data transmission reliability. With a total implementation cost of 32,983 BDT (approximately 300 USD), HydroSense achieves an 85 percent cost reduction compared to commercial systems while providing enhanced connectivity through the Firebase real-time database. This research establishes a new paradigm for accessible environmental monitoring, demonstrating that professional-grade water quality assessment can be achieved through intelligent system architecture and cost-effective component selection.</li>
<li><strong>摘要：</strong>全球水危机需要经济实惠、准确且实时的水质监测解决方案。依赖手动采样或昂贵的商业系统的传统方法无法解决资源有限环境中的可访问性挑战。本文介绍了 HydroSense，这是一种创新的物联网框架，它将包括 pH、溶解氧 (DO)、温度、总溶解固体 (TDS)、估计氮和水位在内的六个关键水质参数集成到一个统一的监测系统中。 HydroSense 采用新颖的双微控制器架构，利用 Arduino Uno 通过五点校准算法进行精密模拟测量，并利用 ESP32 进行无线连接、边缘处理和云集成。该系统采用先进的信号处理技术，包括用于 TDS 测量的中值滤波、温度补偿算法和稳健的错误处理。超过 90 天的实验验证展示了卓越的性能指标：在 0 至 14 范围内的 pH 准确度为正负 0.08 个单位，DO 测量稳定性在正负 0.2 mg/L 范围内，TDS 准确度在 0 至 1000 ppm 范围内正负 1.9%，以及 99.8% 的云数据传输可靠性。 HydroSense 的总实施成本为 32,983 BDT（约 300 美元），与商业系统相比，成本降低了 85%，同时通过 Firebase 实时数据库提供增强的连接性。这项研究建立了可访问环境监测的新范例，证明可以通过智能系统架构和经济高效的组件选择来实现专业级水质评估。</li>
</ul>

<h3>Title: WMVLM: Evaluating Diffusion Model Image Watermarking via Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zijin Yang, Yu Sun, Kejiang Chen, Jiawei Zhao, Jun Jiang, Weiming Zhang, Nenghai Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21610">https://arxiv.org/abs/2601.21610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21610">https://arxiv.org/pdf/2601.21610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21610]] WMVLM: Evaluating Diffusion Model Image Watermarking via Vision-Language Models(https://arxiv.org/abs/2601.21610)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Digital watermarking is essential for securing generated images from diffusion models. Accurate watermark evaluation is critical for algorithm development, yet existing methods have significant limitations: they lack a unified framework for both residual and semantic watermarks, provide results without interpretability, neglect comprehensive security considerations, and often use inappropriate metrics for semantic watermarks. To address these gaps, we propose WMVLM, the first unified and interpretable evaluation framework for diffusion model image watermarking via vision-language models (VLMs). We redefine quality and security metrics for each watermark type: residual watermarks are evaluated by artifact strength and erasure resistance, while semantic watermarks are assessed through latent distribution shifts. Moreover, we introduce a three-stage training strategy to progressively enable the model to achieve classification, scoring, and interpretable text generation. Experiments show WMVLM outperforms state-of-the-art VLMs with strong generalization across datasets, diffusion models, and watermarking methods.</li>
<li><strong>摘要：</strong>数字水印对于保护扩散模型生成的图像至关重要。准确的水印评估对于算法开发至关重要，但现有方法具有显着的局限性：它们缺乏残差水印和语义水印的统一框架，提供的结果缺乏可解释性，忽视全面的安全考虑，并且经常对语义水印使用不适当的度量。为了解决这些差距，我们提出了 WMVLM，这是第一个通过视觉语言模型（VLM）进行扩散模型图像水印的统一且可解释的评估框架。我们重新定义了每种水印类型的质量和安全指标：残留水印通过伪影强度和抗擦除性进行评估，而语义水印则通过潜在分布变化进行评估。此外，我们引入了三阶段训练策略，逐步使模型能够实现分类、评分和可解释文本生成。实验表明，WMVLM 的性能优于最先进的 VLM，具有跨数据集、扩散模型和水印方法的强大泛化能力。</li>
</ul>

<h3>Title: PathReasoner-R1: Instilling Structured Reasoning into Pathology Vision-Language Model via Knowledge-Guided Policy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Songhan Jiang, Fengchun Liu, Ziyue Wang, Linghan Cai, Yongbing Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21617">https://arxiv.org/abs/2601.21617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21617">https://arxiv.org/pdf/2601.21617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21617]] PathReasoner-R1: Instilling Structured Reasoning into Pathology Vision-Language Model via Knowledge-Guided Policy Optimization(https://arxiv.org/abs/2601.21617)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) are advancing computational pathology with superior visual understanding capabilities. However, current systems often reduce diagnosis to directly output conclusions without verifiable evidence-linked reasoning, which severely limits clinical trust and hinders expert error rectification. To address these barriers, we construct PathReasoner, the first large-scale dataset of whole-slide image (WSI) reasoning. Unlike previous work reliant on unverified distillation, we develop a rigorous knowledge-guided generation pipeline. By leveraging medical knowledge graphs, we explicitly align structured pathological findings and clinical reasoning with diagnoses, generating over 20K high-quality instructional samples. Based on the database, we propose PathReasoner-R1, which synergizes trajectory-masked supervised fine-tuning with reasoning-oriented reinforcement learning to instill structured chain-of-thought capabilities. To ensure medical rigor, we engineer a knowledge-aware multi-granular reward function incorporating an Entity Reward mechanism strictly aligned with knowledge graphs. This effectively guides the model to optimize for logical consistency rather than mere outcome matching, thereby enhancing robustness. Extensive experiments demonstrate that PathReasoner-R1 achieves state-of-the-art performance on both PathReasoner and public benchmarks across various image scales, equipping pathology models with transparent, clinically grounded reasoning capabilities. Dataset and code are available at this https URL.</li>
<li><strong>摘要：</strong>视觉语言模型 (VLM) 凭借卓越的视觉理解能力正在推进计算病理学。然而，当前的系统常常将诊断简化为直接输出结论，而没有可验证的证据相关推理，这严重限制了临床信任并阻碍了专家错误纠正。为了解决这些障碍，我们构建了 PathReasoner，这是第一个全幻灯片图像 (WSI) 推理的大型数据集。与之前依赖未经验证的蒸馏的工作不同，我们开发了严格的知识引导生成管道。通过利用医学知识图，我们将结构化病理发现和临床推理与诊断明确结合起来，生成超过 20K 的高质量教学样本。基于该数据库，我们提出了 PathReasoner-R1，它将轨迹屏蔽的监督微调与面向推理的强化学习相结合，以灌输结构化的思维链能力。为了确保医疗的严谨性，我们设计了一个知识感知的多粒度奖励函数，其中包含与知识图谱严格一致的实体奖励机制。这有效地引导模型针对逻辑一致性进行优化，而不仅仅是结果匹配，从而增强鲁棒性。大量实验表明，PathReasoner-R1 在各种图像尺度的 PathReasoner 和公共基准上均实现了最先进的性能，为病理模型配备了透明的、基于临床的推理能力。数据集和代码可从此 https URL 获取。</li>
</ul>

<h3>Title: A Tilted Seesaw: Revisiting Autoencoder Trade-off for Controllable Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Pu Cao, Yiyang Ma, Feng Zhou, Xuedan Yin, Qing Song, Lu Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21633">https://arxiv.org/abs/2601.21633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21633">https://arxiv.org/pdf/2601.21633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21633]] A Tilted Seesaw: Revisiting Autoencoder Trade-off for Controllable Diffusion(https://arxiv.org/abs/2601.21633)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In latent diffusion models, the autoencoder (AE) is typically expected to balance two capabilities: faithful reconstruction and a generation-friendly latent space (e.g., low gFID). In recent ImageNet-scale AE studies, we observe a systematic bias toward generative metrics in handling this trade-off: reconstruction metrics are increasingly under-reported, and ablation-based AE selection often favors the best-gFID configuration even when reconstruction fidelity degrades. We theoretically analyze why this gFID-dominant preference can appear unproblematic for ImageNet generation, yet becomes risky when scaling to controllable diffusion: AEs can induce condition drift, which limits achievable condition alignment. Meanwhile, we find that reconstruction fidelity, especially instance-level measures, better indicates controllability. We empirically validate the impact of tilted autoencoder evaluation on controllability by studying several recent ImageNet AEs. Using a multi-dimensional condition-drift evaluation protocol reflecting controllable generation tasks, we find that gFID is only weakly predictive of condition preservation, whereas reconstruction-oriented metrics are substantially more aligned. ControlNet experiments further confirm that controllability tracks condition preservation rather than gFID. Overall, our results expose a gap between ImageNet-centric AE evaluation and the requirements of scalable controllable diffusion, offering practical guidance for more reliable benchmarking and model selection.</li>
<li><strong>摘要：</strong>在潜在扩散模型中，自动编码器（AE）通常期望平衡两种功能：忠实重建和生成友好的潜在空间（例如，低 gFID）。在最近的 ImageNet 规模 AE 研究中，我们观察到在处理这种权衡时对生成指标存在系统性偏差：重建指标的报告越来越少，并且基于消融的 AE 选择通常倾向于最佳 gFID 配置，即使重建保真度下降也是如此。我们从理论上分析了为什么这种 gFID 主导的偏好对于 ImageNet 生成来说似乎没有问题，但在扩展到可控扩散时却变得有风险：AE 会引起条件漂移，从而限制了可实现的条件对齐。同时，我们发现重建保真度，尤其是实例级度量，更好地表明了可控性。我们通过研究最近的几个 ImageNet AE，实证验证了倾斜自动编码器评估对可控性的影响。使用反映可控生成任务的多维条件漂移评估协议，我们发现 gFID 对条件保存的预测能力很弱，而面向重建的指标则更加一致。 ControlNet 实验进一步证实可控性跟踪条件保存而不是 gFID。总体而言，我们的结果揭示了以 ImageNet 为中心的 AE 评估与可扩展可控扩散的要求之间的差距，为更可靠的基准测试和模型选择提供了实用指导。</li>
</ul>

<h3>Title: Generative Design of Ship Propellers using Conditional Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Patrick Kruger, Rafael Diaz, Simon Hauschulz, Stefan Harries, Hanno Gottschalk</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21637">https://arxiv.org/abs/2601.21637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21637">https://arxiv.org/pdf/2601.21637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21637]] Generative Design of Ship Propellers using Conditional Flow Matching(https://arxiv.org/abs/2601.21637)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In this paper, we explore the use of generative artificial intelligence (GenAI) for ship propeller design. While traditional forward machine learning models predict the performance of mechanical components based on given design parameters, GenAI models aim to generate designs that achieve specified performance targets. In particular, we employ conditional flow matching to establish a bidirectional mapping between design parameters and simulated noise that is conditioned on performance labels. This approach enables the generation of multiple valid designs corresponding to the same performance targets by sampling over the noise vector. To support model training, we generate data using a vortex lattice method for numerical simulation and analyze the trade-off between model accuracy and the amount of available data. We further propose data augmentation using pseudo-labels derived from less data-intensive forward surrogate models, which can often improve overall model performance. Finally, we present examples of distinct propeller geometries that exhibit nearly identical performance characteristics, illustrating the versatility and potential of GenAI in engineering design.</li>
<li><strong>摘要：</strong>在本文中，我们探讨了生成式人工智能（GenAI）在船舶螺旋桨设计中的应用。传统的前向机器学习模型根据给定的设计参数预测机械部件的性能，而 GenAI 模型旨在生成实现指定性能目标的设计。特别是，我们采用条件流匹配来建立设计参数和以性能标签为条件的模拟噪声之间的双向映射。这种方法可以通过对噪声向量进行采样来生成与相同性能目标相对应的多个有效设计。为了支持模型训练，我们使用涡格法生成数据进行数值模拟，并分析模型精度和可用数据量之间的权衡。我们进一步建议使用从数据密集程度较低的前向代理模型派生的伪标签来增强数据，这通常可以提高整体模型性能。最后，我们展示了具有几乎相同性能特征的不同螺旋桨几何形状的示例，说明了 GenAI 在工程设计中的多功能性和潜力。</li>
</ul>

<h3>Title: Expected Return Causes Outcome-Level Mode Collapse in Reinforcement Learning and How to Fix It with Inverse Probability Scaling</h3>
<ul>
<li><strong>Authors: </strong>Abhijeet Sinha, Sundari Elango, Dianbo Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21669">https://arxiv.org/abs/2601.21669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21669">https://arxiv.org/pdf/2601.21669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21669]] Expected Return Causes Outcome-Level Mode Collapse in Reinforcement Learning and How to Fix It with Inverse Probability Scaling(https://arxiv.org/abs/2601.21669)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Many reinforcement learning (RL) problems admit multiple terminal solutions of comparable quality, where the goal is not to identify a single optimum but to represent a diverse set of high-quality outcomes. Nevertheless, policies trained by standard expected return maximization routinely collapse onto a small subset of outcomes, a phenomenon commonly attributed to insufficient exploration or weak regularization. We show that this explanation is incomplete: outcome level mode collapse is a structural consequence of the expected-return objective itself. Under idealized learning dynamics, the log-probability ratio between any two outcomes evolves linearly in their reward difference, implying exponential ratio divergence and inevitable collapse independent of the exploration strategy, entropy regularization, or optimization algorithm. We identify the source of this pathology as the probability multiplier inside the expectation and propose a minimal correction: inverse probability scaling, which removes outcome-frequency amplification from the learning signal, fundamentally changes the learning dynamics, and provably yields reward-proportional terminal distributions, preventing collapse in multimodal settings. We instantiate this principle in Group Relative Policy Optimization (GRPO) as a drop-in modification, IPS-GRPO, requiring no auxiliary models or architectural changes. Across different reasoning and molecular generation tasks, IPS-GRPO consistently reduces outcome-level mode collapse while matching or exceeding baseline performance, suggesting that correcting the objective rather than adding exploration heuristics is key to reliable multimodal policy optimization.</li>
<li><strong>摘要：</strong>许多强化学习 (RL) 问题都承认具有可比较质量的多个终端解决方案，其目标不是确定单个最优值，而是代表一组不同的高质量结果。然而，经过标准预期回报最大化训练的政策通常会崩溃到结果的一小部分，这种现象通常归因于探索不足或正规化薄弱。 We show that this explanation is incomplete: outcome level mode collapse is a structural consequence of the expected-return objective itself.在理想化的学习动态下，任何两个结果之间的对数概率比在其奖励差异中线性演变，这意味着指数比发散和不可避免的崩溃，与探索策略、熵正则化或优化算法无关。我们将这种病态的根源确定为期望内的概率乘数，并提出了最小修正：逆概率缩放，它消除了学习信号中的结果频率放大，从根本上改变了学习动态，并可证明产生与奖励成比例的终端分布，防止多模式设置中的崩溃。我们在组相对策略优化 (GRPO) 中将这一原则实例化为直接修改 IPS-GRPO，不需要辅助模型或架构更改。在不同的推理和分子生成任务中，IPS-GRPO 始终如一地减少结果级模式崩溃，同时匹配或超过基线性能，这表明纠正目标而不是添加探索启发法是可靠的多模式策略优化的关键。</li>
</ul>

<h3>Title: SmartMeterFM: Unifying Smart Meter Data Generative Tasks Using Flow Matching Models</h3>
<ul>
<li><strong>Authors: </strong>Nan Lin, Yanbo Wang, Jacco Heres, Peter Palensky, Pedro P. Vergara</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21706">https://arxiv.org/abs/2601.21706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21706">https://arxiv.org/pdf/2601.21706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21706]] SmartMeterFM: Unifying Smart Meter Data Generative Tasks Using Flow Matching Models(https://arxiv.org/abs/2601.21706)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generation, generative</a></li>
<li><strong>Abstract: </strong>Smart meter data is the foundation for planning and operating the distribution network. Unfortunately, such data are not always available due to privacy regulations. Meanwhile, the collected data may be corrupted due to sensor or transmission failure, or it may not have sufficient resolution for downstream tasks. A wide range of generative tasks is formulated to address these issues, including synthetic data generation, missing data imputation, and super-resolution. Despite the success of machine learning models on these tasks, dedicated models need to be designed and trained for each task, leading to redundancy and inefficiency. In this paper, by recognizing the powerful modeling capability of flow matching models, we propose a new approach to unify diverse smart meter data generative tasks with a single model trained for conditional generation. The proposed flow matching models are trained to generate challenging, high-dimensional time series data, specifically monthly smart meter data at a 15 min resolution. By viewing different generative tasks as distinct forms of partial data observations and injecting them into the generation process, we unify tasks such as imputation and super-resolution with a single model, eliminating the need for re-training. The data generated by our model not only are consistent with the given observations but also remain realistic, showing better performance against interpolation and other machine learning based baselines dedicated to the tasks.</li>
<li><strong>摘要：</strong>智能电表数据是配电网规划和运营的基础。不幸的是，由于隐私法规的原因，此类数据并不总是可用。同时，收集的数据可能由于传感器或传输故障而损坏，或者可能没有足够的分辨率来完成下游任务。为了解决这些问题，制定了广泛的生成任务，包括合成数据生成、缺失数据插补和超分辨率。尽管机器学习模型在这些任务上取得了成功，但需要为每项任务设计和训练专用模型，从而导致冗余和低效率。在本文中，通过认识到流量匹配模型强大的建模能力，我们提出了一种新方法，将不同的智能电表数据生成任务与经过条件生成训练的单个模型统一起来。所提出的流量匹配模型经过训练，可以生成具有挑战性的高维时间序列数据，特别是分辨率为 15 分钟的每月智能电表数据。通过将不同的生成任务视为不同形式的部分数据观察并将它们注入生成过程中，我们将插补和超分辨率等任务与单个模型统一起来，从而消除了重新训练的需要。我们的模型生成的数据不仅与给定的观察结果一致，而且保持现实，与专用于任务的插值和其他基于机器学习的基线相比，显示出更好的性能。</li>
</ul>

<h3>Title: DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Mingshuang Luo, Shuang Liang, Zhengkun Rong, Yuxuan Luo, Tianshu Hu, Ruibing Hou, Hong Chang, Yong Li, Yuan Zhang, Mingyuan Gao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21716">https://arxiv.org/abs/2601.21716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21716">https://arxiv.org/pdf/2601.21716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21716]] DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning(https://arxiv.org/abs/2601.21716)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Character image animation aims to synthesize high-fidelity videos by transferring motion from a driving sequence to a static reference image. Despite recent advancements, existing methods suffer from two fundamental challenges: (1) suboptimal motion injection strategies that lead to a trade-off between identity preservation and motion consistency, manifesting as a "see-saw", and (2) an over-reliance on explicit pose priors (e.g., skeletons), which inadequately capture intricate dynamics and hinder generalization to arbitrary, non-humanoid characters. To address these challenges, we present DreamActor-M2, a universal animation framework that reimagines motion conditioning as an in-context learning problem. Our approach follows a two-stage paradigm. First, we bridge the input modality gap by fusing reference appearance and motion cues into a unified latent space, enabling the model to jointly reason about spatial identity and temporal dynamics by leveraging the generative prior of foundational models. Second, we introduce a self-bootstrapped data synthesis pipeline that curates pseudo cross-identity training pairs, facilitating a seamless transition from pose-dependent control to direct, end-to-end RGB-driven animation. This strategy significantly enhances generalization across diverse characters and motion scenarios. To facilitate comprehensive evaluation, we further introduce AW Bench, a versatile benchmark encompassing a wide spectrum of characters types and motion scenarios. Extensive experiments demonstrate that DreamActor-M2 achieves state-of-the-art performance, delivering superior visual fidelity and robust cross-domain generalization. Project Page: this https URL</li>
<li><strong>摘要：</strong>角色图像动画旨在通过将运动从驾驶序列转移到静态参考图像来合成高保真视频。尽管最近取得了进展，现有方法仍面临两个基本挑战：（1）次优的运动注入策略导致身份保留和运动一致性之间的权衡，表现为“拉锯”，以及（2）过度依赖明确的姿势先验（例如骨骼），这不能充分捕获复杂的动态并阻碍对任意非人形角色的泛化。为了应对这些挑战，我们推出了 DreamActor-M2，这是一个通用动画框架，它将运动调节重新想象为上下文学习问题。我们的方法遵循两阶段范式。首先，我们通过将参考外观和运动线索融合到统一的潜在空间中来弥合输入模态差距，使模型能够利用基础模型的生成先验来联合推理空间身份和时间动态。其次，我们引入了一个自引导数据合成管道，该管道可以管理伪交叉身份训练对，从而促进从依赖姿势的控制到直接的端到端 RGB 驱动动画的无缝过渡。该策略显着增强了不同角色和运动场景的泛化能力。为了便于全面评估，我们进一步引入了 AW Bench，这是一个涵盖广泛角色类型和运动场景的多功能基准。大量实验表明，DreamActor-M2 实现了最先进的性能，提供卓越的视觉保真度和强大的跨域泛化能力。项目页面：此 https URL</li>
</ul>

<h3>Title: From Global to Granular: Revealing IQA Model Performance via Correlation Surface</h3>
<ul>
<li><strong>Authors: </strong>Baoliang Chen, Danni Huang, Hanwei Zhu, Lingyu Zhu, Wei Zhou, Shiqi Wang, Yuming Fang, Weisi Lin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21738">https://arxiv.org/abs/2601.21738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21738">https://arxiv.org/pdf/2601.21738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21738]] From Global to Granular: Revealing IQA Model Performance via Correlation Surface(https://arxiv.org/abs/2601.21738)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Evaluation of Image Quality Assessment (IQA) models has long been dominated by global correlation metrics, such as Pearson Linear Correlation Coefficient (PLCC) and Spearman Rank-Order Correlation Coefficient (SRCC). While widely adopted, these metrics reduce performance to a single scalar, failing to capture how ranking consistency varies across the local quality spectrum. For example, two IQA models may achieve identical SRCC values, yet one ranks high-quality images (related to high Mean Opinion Score, MOS) more reliably, while the other better discriminates image pairs with small quality/MOS differences (related to $|\Delta$MOS$|$). Such complementary behaviors are invisible under global metrics. Moreover, SRCC and PLCC are sensitive to test-sample quality distributions, yielding unstable comparisons across test sets. To address these limitations, we propose \textbf{Granularity-Modulated Correlation (GMC)}, which provides a structured, fine-grained analysis of IQA performance. GMC includes: (1) a \textbf{Granularity Modulator} that applies Gaussian-weighted correlations conditioned on absolute MOS values and pairwise MOS differences ($|\Delta$MOS$|$) to examine local performance variations, and (2) a \textbf{Distribution Regulator} that regularizes correlations to mitigate biases from non-uniform quality distributions. The resulting \textbf{correlation surface} maps correlation values as a joint function of MOS and $|\Delta$MOS$|$, providing a 3D representation of IQA performance. Experiments on standard benchmarks show that GMC reveals performance characteristics invisible to scalar metrics, offering a more informative and reliable paradigm for analyzing, comparing, and deploying IQA models. Codes are available at this https URL.</li>
<li><strong>摘要：</strong>图像质量评估（IQA）模型的评估长期以来一直以全局相关性指标为主，例如皮尔逊线性相关系数（PLCC）和斯皮尔曼等级相关系数（SRCC）。虽然被广泛采用，但这些指标将性能降低到单个标量，无法捕获排名一致性在本地质量范围内的变化。例如，两个 IQA 模型可能实现相同的 SRCC 值，但其中一个模型可以更可靠地对高质量图像（与高平均意见得分，MOS 相关）进行排名，而另一个模型可以更好地区分质量/MOS 差异较小的图像对（与 $|\Delta$MOS$|$ 相关）。这种互补行为在全球指标下是不可见的。此外，SRCC 和 PLCC 对测试样本质量分布很敏感，导致测试集之间的比较不稳定。为了解决这些限制，我们提出\textbf{粒度调制相关性（GMC）}，它提供了 IQA 性能的结构化、细粒度分析。 GMC 包括：(1) 一个 \textbf{Granularity Modulator}，它应用以绝对 MOS 值和成对 MOS 差异 ($|\Delta$MOS$|$) 为条件的高斯加权相关性来检查局部性能变化，以及 (2) 一个 \textbf{Distribution Regulator}，它对相关性进行正则化，以减轻非均匀质量分布带来的偏差。生成的 \textbf{相关面} 将相关值映射为 MOS 和 $|\Delta$MOS$|$ 的联合函数，提供 IQA 性能的 3D 表示。标准基准测试的实验表明，GMC 揭示了标量指标不可见的性能特征，为分析、比较和部署 IQA 模型提供了信息更丰富、更可靠的范例。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Knowledge Vector Weakening: Efficient Training-free Unlearning for Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yejin Kim, Dongjun Hwang, Sungmin Cha, Junsuk Choe</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21794">https://arxiv.org/abs/2601.21794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21794">https://arxiv.org/pdf/2601.21794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21794]] Knowledge Vector Weakening: Efficient Training-free Unlearning for Large Vision-Language Models(https://arxiv.org/abs/2601.21794)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Vision-Language Models (LVLMs) are widely adopted for their strong multimodal capabilities, yet they raise serious concerns such as privacy leakage and harmful content generation. Machine unlearning has emerged as a promising solution for removing the influence of specific data from trained models. However, existing approaches largely rely on gradient-based optimization, incurring substantial computational costs for large-scale LVLMs. To address this limitation, we propose Knowledge Vector Weakening (KVW), a training-free unlearning method that directly intervenes in the full model without gradient computation. KVW identifies knowledge vectors that are activated during the model's output generation on the forget set and progressively weakens their contributions, thereby preventing the model from exploiting undesirable knowledge. Experiments on the MLLMU and CLEAR benchmarks demonstrate that KVW achieves a stable forget-retain trade-off while significantly improving computational efficiency over gradient-based and LoRA-based unlearning methods.</li>
<li><strong>摘要：</strong>大视觉语言模型（LVLM）因其强大的多模态能力而被广泛采用，但它们引起了隐私泄露和有害内容生成等严重问题。机器去学习已经成为一种有前景的解决方案，可以消除训练模型中特定数据的影响。然而，现有方法很大程度上依赖于基于梯度的优化，这会给大规模 LVLM 带来大量的计算成本。为了解决这个限制，我们提出了知识向量弱化（KVW），这是一种免训练的去学习方法，无需梯度计算即可直接干预整个模型。 KVW 识别模型在遗忘集上生成输出期间激活的知识向量，并逐渐削弱它们的贡献，从而防止模型利用不需要的知识。 MLLMU 和 CLEAR 基准测试表明，KVW 实现了稳定的遗忘-保留权衡，同时显着提高了基于梯度和基于 LoRA 的去学习方法的计算效率。</li>
</ul>

<h3>Title: CG-MLLM: Captioning and Generating 3D content via Multi-modal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Junming Huang, Weiwei Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21798">https://arxiv.org/abs/2601.21798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21798">https://arxiv.org/pdf/2601.21798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21798]] CG-MLLM: Captioning and Generating 3D content via Multi-modal Large Language Models(https://arxiv.org/abs/2601.21798)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models(LLMs) have revolutionized text generation and multimodal perception, but their capabilities in 3D content generation remain underexplored. Existing methods compromise by producing either low-resolution meshes or coarse structural proxies, failing to capture fine-grained geometry natively. In this paper, we propose CG-MLLM, a novel Multi-modal Large Language Model (MLLM) capable of 3D captioning and high-resolution 3D generation in a single framework. Leveraging the Mixture-of-Transformer architecture, CG-MLLM decouples disparate modeling needs, where the Token-level Autoregressive (TokenAR) Transformer handles token-level content, and the Block-level Autoregressive (BlockAR) Transformer handles block-level content. By integrating a pre-trained vision-language backbone with a specialized 3D VAE latent space, CG-MLLM facilitates long-context interactions between standard tokens and spatial blocks within a single integrated architecture. Experimental results show that CG-MLLM significantly outperforms existing MLLMs in generating high-fidelity 3D objects, effectively bringing high-resolution 3D content creation into the mainstream LLM paradigm.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 彻底改变了文本生成和多模式感知，但它们在 3D 内容生成方面的功能仍未得到充分开发。现有方法通过生成低分辨率网格或粗结构代理来妥协，无法原生捕获细粒度几何体。在本文中，我们提出了 CG-MLLM，一种新颖的多模态大语言模型 (MLLM)，能够在单一框架中进行 3D 字幕和高分辨率 3D 生成。利用 Mixture-of-Transformer 架构，CG-MLLM 解耦了不同的建模需求，其中令牌级自回归 (TokenAR) 转换器处理令牌级内容，块级自回归 (BlockAR) 转换器处理块级内容。通过将预先训练的视觉语言主干与专门的 3D VAE 潜在空间集成，CG-MLLM 促进了单个集成架构中标准令牌和空间块之间的长上下文交互。实验结果表明，CG-MLLM 在生成高保真 3D 对象方面显着优于现有的 MLLM，有效地将高分辨率 3D 内容创建带入主流 LLM 范式。</li>
</ul>

<h3>Title: MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods</h3>
<ul>
<li><strong>Authors: </strong>Honglin Lin, Zheng Liu, Yun Zhu, Chonghan Qin, Juekai Lin, Xiaoran Shang, Conghui He, Wentao Zhang, Lijun Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21821">https://arxiv.org/abs/2601.21821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21821">https://arxiv.org/pdf/2601.21821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21821]] MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods(https://arxiv.org/abs/2601.21821)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in Vision Language Models (VLMs) have driven significant progress in visual reasoning. However, open-source VLMs still lag behind proprietary systems, largely due to the lack of high-quality reasoning data. Existing datasets offer limited coverage of challenging domains such as STEM diagrams and visual puzzles, and lack consistent, long-form Chain-of-Thought (CoT) annotations essential for eliciting strong reasoning capabilities. To bridge this gap, we introduce MMFineReason, a large-scale multimodal reasoning dataset comprising 1.8M samples and 5.1B solution tokens, featuring high-quality reasoning annotations distilled from Qwen3-VL-235B-A22B-Thinking. The dataset is established via a systematic three-stage pipeline: (1) large-scale data collection and standardization, (2) CoT rationale generation, and (3) comprehensive selection based on reasoning quality and difficulty awareness. The resulting dataset spans STEM problems, visual puzzles, games, and complex diagrams, with each sample annotated with visually grounded reasoning traces. We fine-tune Qwen3-VL-Instruct on MMFineReason to develop MMFineReason-2B/4B/8B versions. Our models establish new state-of-the-art results for their size class. Notably, MMFineReason-4B succesfully surpasses Qwen3-VL-8B-Thinking, and MMFineReason-8B even outperforms Qwen3-VL-30B-A3B-Thinking while approaching Qwen3-VL-32B-Thinking, demonstrating remarkable parameter efficiency. Crucially, we uncover a "less is more" phenomenon via our difficulty-aware filtering strategy: a subset of just 7\% (123K samples) achieves performance comparable to the full dataset. Notably, we reveal a synergistic effect where reasoning-oriented data composition simultaneously boosts general capabilities.</li>
<li><strong>摘要：</strong>视觉语言模型 (VLM) 的最新进展推动了视觉推理领域的重大进展。然而，开源 VLM 仍然落后于专有系统，这很大程度上是由于缺乏高质量的推理数据。现有数据集对 STEM 图和视觉谜题等具有挑战性的领域的覆盖范围有限，并且缺乏对于引发强大推理能力至关重要的一致的长形式思想链 (CoT) 注释。为了弥补这一差距，我们引入了 MMFineReason，这是一个大规模多模态推理数据集，包含 180 万个样本和 5.1B 个解决方案标记，具有从 Qwen3-VL-235B-A22B-Thinking 中提取的高质量推理注释。该数据集通过系统的三阶段流程建立：（1）大规模数据收集和标准化，（2）CoT基本原理生成，（3）基于推理质量和难度意识的综合选择。生成的数据集涵盖 STEM 问题、视觉谜题、游戏和复杂图表，每个样本都用基于视觉的推理轨迹进行注释。我们在 MMFineReason 上微调 Qwen3-VL-Instruct 以开发 MMFineReason-2B/4B/8B 版本。我们的模型在其尺寸级别中建立了新的最先进的结果。值得注意的是，MMFineReason-4B 成功超越了 Qwen3-VL-8B-Thinking，MMFineReason-8B 甚至超越了 Qwen3-VL-30B-A3B-Thinking，同时接近 Qwen3-VL-32B-Thinking，展示了显着的参数效率。至关重要的是，我们通过难度感知过滤策略发现了“少即是多”的现象：仅 7%（123K 样本）的子集就实现了与完整数据集相当的性能。值得注意的是，我们揭示了一种协同效应，即面向推理的数据组合同时提高了一般能力。</li>
</ul>

<h3>Title: Visual Disentangled Diffusion Autoencoders: Scalable Counterfactual Generation for Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Sidney Bender, Marco Morik</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21851">https://arxiv.org/abs/2601.21851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21851">https://arxiv.org/pdf/2601.21851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21851]] Visual Disentangled Diffusion Autoencoders: Scalable Counterfactual Generation for Foundation Models(https://arxiv.org/abs/2601.21851)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Foundation models, despite their robust zero-shot capabilities, remain vulnerable to spurious correlations and 'Clever Hans' strategies. Existing mitigation methods often rely on unavailable group labels or computationally expensive gradient-based adversarial optimization. To address these limitations, we propose Visual Disentangled Diffusion Autoencoders (DiDAE), a novel framework integrating frozen foundation models with disentangled dictionary learning for efficient, gradient-free counterfactual generation directly for the foundation model. DiDAE first edits foundation model embeddings in interpretable disentangled directions of the disentangled dictionary and then decodes them via a diffusion autoencoder. This allows the generation of multiple diverse, disentangled counterfactuals for each factual, much faster than existing baselines, which generate single entangled counterfactuals. When paired with Counterfactual Knowledge Distillation, DiDAE-CFKD achieves state-of-the-art performance in mitigating shortcut learning, improving downstream performance on unbalanced datasets.</li>
<li><strong>摘要：</strong>尽管基础模型具有强大的零样本能力，但仍然容易受到虚假相关性和“聪明的汉斯”策略的影响。现有的缓解方法通常依赖于不可用的组标签或计算成本昂贵的基于梯度的对抗性优化。为了解决这些限制，我们提出了视觉解缠结扩散自动编码器（DiDAE），这是一种将冻结基础模型与解缠结字典学习相结合的新颖框架，可直接为基础模型高效、无梯度的反事实生成。 DiDAE 首先在解缠结字典的可解释解缠结方向上编辑基础模型嵌入，然后通过扩散自动编码器对其进行解码。这允许为每个事实生成多个不同的、解开的反事实，比生成单个纠缠的反事实的现有基线快得多。当与反事实知识蒸馏结合使用时，DiDAE-CFKD 在缓解捷径学习、提高不平衡数据集的下游性能方面实现了最先进的性能。</li>
</ul>

<h3>Title: Trajectory-Guided Diffusion for Foreground-Preserving Background Generation in Multi-Layer Documents</h3>
<ul>
<li><strong>Authors: </strong>Taewon Kang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21857">https://arxiv.org/abs/2601.21857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21857">https://arxiv.org/pdf/2601.21857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21857]] Trajectory-Guided Diffusion for Foreground-Preserving Background Generation in Multi-Layer Documents(https://arxiv.org/abs/2601.21857)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We present a diffusion-based framework for document-centric background generation that achieves foreground preservation and multi-page stylistic consistency through latent-space design rather than explicit constraints. Instead of suppressing diffusion updates or applying masking heuristics, our approach reinterprets diffusion as the evolution of stochastic trajectories through a structured latent space. By shaping the initial noise and its geometric alignment, background generation naturally avoids designated foreground regions, allowing readable content to remain intact without auxiliary mechanisms. To address the long-standing issue of stylistic drift across pages, we decouple style control from text conditioning and introduce cached style directions as persistent vectors in latent space. Once selected, these directions constrain diffusion trajectories to a shared stylistic subspace, ensuring consistent appearance across pages and editing iterations. This formulation eliminates the need for repeated prompt-based style specification and provides a more stable foundation for multi-page generation. Our framework admits a geometric and physical interpretation, where diffusion paths evolve on a latent manifold shaped by preferred directions, and foreground regions are rarely traversed as a consequence of trajectory initialization rather than explicit exclusion. The proposed method is training-free, compatible with existing diffusion backbones, and produces visually coherent, foreground-preserving results across complex documents. By reframing diffusion as trajectory design in latent space, we offer a principled approach to consistent and structured generative modeling.</li>
<li><strong>摘要：</strong>我们提出了一种基于扩散的框架，用于以文档为中心的背景生成，通过潜在空间设计而不是显式约束来实现前景保留和多页面风格一致性。我们的方法不是抑制扩散更新或应用掩蔽启发法，而是将扩散重新解释为随机轨迹通过结构化潜在空间的演化。通过塑造初始噪声及其几何对齐，背景生成自然地避免指定的前景区域，从而允许可读内容在没有辅助机制的情况下保持完整。为了解决跨页面风格漂移的长期问题，我们将风格控制与文本调节分离，并将缓存的风格方向引入为潜在空间中的持久向量。一旦选择，这些方向将扩散轨迹限制到共享的风格子空间，确保跨页面和编辑迭代的外观一致。这种表述消除了重复的基于提示的样式规范的需要，并为多页面生成提供了更稳定的基础。我们的框架承认几何和物理解释，其中扩散路径在由首选方向形成的潜在流形上演化，并且由于轨迹初始化而不是显式排除，前景区域很少被遍历。所提出的方法无需训练，与现有的扩散主干兼容，并在复杂的文档中产生视觉连贯、前景保留的结果。通过将扩散重新定义为潜在空间中的轨迹设计，我们提供了一种一致且结构化的生成建模的原则方法。</li>
</ul>

<h3>Title: Improving Classifier-Free Guidance of Flow Matching via Manifold Projection</h3>
<ul>
<li><strong>Authors: </strong>Jian-Feng Cai, Haixia Liu, Zhengyi Su, Chao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21892">https://arxiv.org/abs/2601.21892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21892">https://arxiv.org/pdf/2601.21892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21892]] Improving Classifier-Free Guidance of Flow Matching via Manifold Projection(https://arxiv.org/abs/2601.21892)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Classifier-free guidance (CFG) is a widely used technique for controllable generation in diffusion and flow-based models. Despite its empirical success, CFG relies on a heuristic linear extrapolation that is often sensitive to the guidance scale. In this work, we provide a principled interpretation of CFG through the lens of optimization. We demonstrate that the velocity field in flow matching corresponds to the gradient of a sequence of smoothed distance functions, which guides latent variables toward the scaled target image set. This perspective reveals that the standard CFG formulation is an approximation of this gradient, where the prediction gap, the discrepancy between conditional and unconditional outputs, governs guidance sensitivity. Leveraging this insight, we reformulate the CFG sampling as a homotopy optimization with a manifold constraint. This formulation necessitates a manifold projection step, which we implement via an incremental gradient descent scheme during sampling. To improve computational efficiency and stability, we further enhance this iterative process with Anderson Acceleration without requiring additional model evaluations. Our proposed methods are training-free and consistently refine generation fidelity, prompt alignment, and robustness to the guidance scale. We validate their effectiveness across diverse benchmarks, demonstrating significant improvements on large-scale models such as DiT-XL-2-256, Flux, and Stable Diffusion 3.5.</li>
<li><strong>摘要：</strong>无分类器引导（CFG）是一种广泛使用的技术，用于在扩散和基于流动的模型中进行可控生成。尽管 CFG 在实证上取得了成功，但它依赖于启发式线性外推法，该外推法通常对指导尺度敏感。在这项工作中，我们通过优化的视角对 CFG 进行了原则性解释。我们证明了流匹配中的速度场对应于一系列平滑距离函数的梯度，它将潜在变量引导至缩放的目标图像集。这一观点揭示了标准 CFG 公式是该梯度的近似值，其中预测间隙（条件输出和无条件输出之间的差异）控制着指导灵敏度。利用这一见解，我们将 CFG 采样重新表述为具有流形约束的同伦优化。这个公式需要一个流形投影步骤，我们在采样期间通过增量梯度下降方案来实现。为了提高计算效率和稳定性，我们通过安德森加速进一步增强了这一迭代过程，而无需额外的模型评估。我们提出的方法无需训练，并且持续改进生成保真度、及时对齐和指导尺度的鲁棒性。我们在不同的基准测试中验证了它们的有效性，展示了 DiT-XL-2-256、Flux 和 Stable Diffusion 3.5 等大型模型的显着改进。</li>
</ul>

<h3>Title: Past- and Future-Informed KV Cache Policy with Salience Estimation in Autoregressive Video Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Hanmo Chen, Chenghao Xu, Xu Yang, Xuan Chen, Cheng Deng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21896">https://arxiv.org/abs/2601.21896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21896">https://arxiv.org/pdf/2601.21896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21896]] Past- and Future-Informed KV Cache Policy with Salience Estimation in Autoregressive Video Diffusion(https://arxiv.org/abs/2601.21896)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Video generation is pivotal to digital media creation, and recent advances in autoregressive video generation have markedly enhanced the efficiency of real-time video synthesis. However, existing approaches generally rely on heuristic KV Cache policies, which ignore differences in token importance in long-term video generation. This leads to the loss of critical spatiotemporal information and the accumulation of redundant, invalid cache, thereby degrading video generation quality and efficiency. To address this limitation, we first observe that token contributions to video generation are highly time-heterogeneous and accordingly propose a novel Past- and Future-Informed KV Cache Policy (PaFu-KV). Specifically, PaFu-KV introduces a lightweight Salience Estimation Head distilled from a bidirectional teacher to estimate salience scores, allowing the KV cache to retain informative tokens while discarding less relevant ones. This policy yields a better quality-efficiency trade-off by shrinking KV cache capacity and reducing memory footprint at inference time. Extensive experiments on benchmarks demonstrate that our method preserves high-fidelity video generation quality while enables accelerated inference, thereby enabling more efficient long-horizon video generation. Our code will be released upon paper acceptance.</li>
<li><strong>摘要：</strong>视频生成对于数字媒体创作至关重要，自回归视频生成的最新进展显着提高了实时视频合成的效率。然而，现有方法通常依赖于启发式 KV 缓存策略，该策略忽略了长期视频生成中令牌重要性的差异。这会导致关键时空信息的丢失以及冗余、无效缓存的积累，从而降低视频生成质量和效率。为了解决这个限制，我们首先观察到视频生成的代币贡献具有高度的时间异质性，因此提出了一种新颖的过去和未来通知的 KV 缓存策略（PaFu-KV）。具体来说，PaFu-KV 引入了一个从双向教师中提取的轻量级显着性估计头来估计显着性分数，允许 KV 缓存保留信息丰富的标记，同时丢弃不太相关的标记。该策略通过缩小 KV 缓存容量并减少推理时的内存占用，实现更好的质量效率权衡。对基准的大量实验表明，我们的方法保留了高保真视频生成质量，同时能够加速推理，从而实现更高效的长视野视频生成。我们的代码将在论文接受后发布。</li>
</ul>

<h3>Title: VideoAesBench: Benchmarking the Video Aesthetics Perception Capabilities of Large Multimodal Models</h3>
<ul>
<li><strong>Authors: </strong>Yunhao Li, Sijing Wu, Zhilin Gao, Zicheng Zhang, Qi Jia, Huiyu Duan, Xiongkuo Min, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21915">https://arxiv.org/abs/2601.21915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21915">https://arxiv.org/pdf/2601.21915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21915]] VideoAesBench: Benchmarking the Video Aesthetics Perception Capabilities of Large Multimodal Models(https://arxiv.org/abs/2601.21915)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Large multimodal models (LMMs) have demonstrated outstanding capabilities in various visual perception tasks, which has in turn made the evaluation of LMMs significant. However, the capability of video aesthetic quality assessment, which is a fundamental ability for human, remains underexplored for LMMs. To address this, we introduce VideoAesBench, a comprehensive benchmark for evaluating LMMs' understanding of video aesthetic quality. VideoAesBench has several significant characteristics: (1) Diverse content including 1,804 videos from multiple video sources including user-generated (UGC), AI-generated (AIGC), compressed, robotic-generated (RGC), and game videos. (2) Multiple question formats containing traditional single-choice questions, multi-choice questions, True or False questions, and a novel open-ended questions for video aesthetics description. (3) Holistic video aesthetics dimensions including visual form related questions from 5 aspects, visual style related questions from 4 aspects, and visual affectiveness questions from 3 aspects. Based on VideoAesBench, we benchmark 23 open-source and commercial large multimodal models. Our findings show that current LMMs only contain basic video aesthetics perception ability, their performance remains incomplete and imprecise. We hope our VideoAesBench can be served as a strong testbed and offer insights for explainable video aesthetics assessment.</li>
<li><strong>摘要：</strong>大型多模态模型（LMM）在各种视觉感知任务中表现出了出色的能力，这反过来又使得对 LMM 的评估具有重要意义。然而，作为人类基本能力的视频美学质量评估能力对于 LMM 来说仍然没有得到充分探索。为了解决这个问题，我们引入了 VideoAesBench，这是一个用于评估 LMM 对视频美学质量理解的综合基准。 VideoAesBench有几个显着的特点：（1）内容丰富，包括来自多个视频源的1,804个视频，包括用户生成（UGC）、人工智能生成（AIGC）、压缩、机器人生成（RGC）和游戏视频。 （2）多种题型，包括传统的单选题、多选题、判断题、以及新颖的视频美学描述开放式题。 (3)整体视频美学维度，包括5个方面的视觉形式相关问题、4个方面的视觉风格相关问题、3个方面的视觉情感问题。基于VideoAesBench，我们对23个开源和商业大型多模态模型进行了基准测试。我们的研究结果表明，当前的 LMM 仅包含基本的视频美学感知能力，其性能仍然不完整且不精确。我们希望我们的 VideoAesBench 能够作为一个强大的测试平台，并为可解释的视频美学评估提供见解。</li>
</ul>

<h3>Title: Zero-Shot Video Restoration and Enhancement with Assistance of Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Cong Cao, Huanjing Yue, Shangbin Xie, Xin Liu, Jingyu Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21922">https://arxiv.org/abs/2601.21922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21922">https://arxiv.org/pdf/2601.21922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21922]] Zero-Shot Video Restoration and Enhancement with Assistance of Video Diffusion Models(https://arxiv.org/abs/2601.21922)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Although diffusion-based zero-shot image restoration and enhancement methods have achieved great success, applying them to video restoration or enhancement will lead to severe temporal flickering. In this paper, we propose the first framework that utilizes the rapidly-developed video diffusion model to assist the image-based method in maintaining more temporal consistency for zero-shot video restoration and enhancement. We propose homologous latents fusion, heterogenous latents fusion, and a COT-based fusion ratio strategy to utilize both homologous and heterogenous text-to-video diffusion models to complement the image method. Moreover, we propose temporal-strengthening post-processing to utilize the image-to-video diffusion model to further improve temporal consistency. Our method is training-free and can be applied to any diffusion-based image restoration and enhancement methods. Experimental results demonstrate the superiority of the proposed method.</li>
<li><strong>摘要：</strong>尽管基于扩散的零样本图像恢复和增强方法取得了巨大成功，但将其应用于视频恢复或增强会导致严重的时间闪烁。在本文中，我们提出了第一个框架，利用快速发展的视频扩散模型来协助基于图像的方法保持更多的时间一致性，以实现零镜头视频恢复和增强。我们提出同源潜在融合、异质潜在融合和基于 COT 的融合比率策略，以利用同源和异质文本到视频扩散模型来补充图像方法。此外，我们提出时间强化后处理，以利用图像到视频扩散模型进一步提高时间一致性。我们的方法无需训练，可应用于任何基于扩散的图像恢复和增强方法。实验结果证明了该方法的优越性。</li>
</ul>

<h3>Title: Entropy-Based Dimension-Free Convergence and Loss-Adaptive Schedules for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Ahmad Aghapour, Erhan Bayraktar, Ziqing Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21943">https://arxiv.org/abs/2601.21943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21943">https://arxiv.org/pdf/2601.21943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21943]] Entropy-Based Dimension-Free Convergence and Loss-Adaptive Schedules for Diffusion Models(https://arxiv.org/abs/2601.21943)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion generative models synthesize samples by discretizing reverse-time dynamics driven by a learned score (or denoiser). Existing convergence analyses of diffusion models typically scale at least linearly with the ambient dimension, and sharper rates often depend on intrinsic-dimension assumptions or other geometric restrictions on the target distribution. We develop an alternative, information-theoretic approach to dimension-free convergence that avoids any geometric assumptions. Under mild assumptions on the target distribution, we bound KL divergence between the target and generated distributions by $O(H^2/K)$ (up to endpoint factors), where $H$ is the Shannon entropy and $K$ is the number of sampling steps. Moreover, using a reformulation of the KL divergence, we propose a Loss-Adaptive Schedule (LAS) for efficient discretization of reverse SDE which is lightweight and relies only on the training loss, requiring no post-training heavy computation. Empirically, LAS improves sampling quality over common heuristic schedules.</li>
<li><strong>摘要：</strong>扩散生成模型通过离散化由学习分数（或降噪器）驱动的逆时动态来合成样本。扩散模型的现有收敛分析通常至少与环境维度成线性比例，并且更陡的速率通常取决于内在维度假设或目标分布的其他几何限制。我们开发了一种替代的信息论方法来实现无量纲收敛，避免任何几何假设。在对目标分布的温和假设下，我们将目标分布和生成分布之间的 KL 散度限制为 $O(H^2/K)$（最多端点因子），其中 $H$ 是香农熵，$K$ 是采样步骤数。此外，利用 KL 散度的重新表述，我们提出了一种用于反向 SDE 高效离散化的损失自适应计划（LAS），它是轻量级的，仅依赖于训练损失，不需要训练后的大量计算。根据经验，LAS 比常见的启发式计划提高了采样质量。</li>
</ul>

<h3>Title: Embracing Aleatoric Uncertainty in Medical Multimodal Learning with Missing Modalities</h3>
<ul>
<li><strong>Authors: </strong>Linxiao Gong, Yang Liu, Lianlong Sun, Yulai Bi, Jing Liu, Xiaoguang Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21950">https://arxiv.org/abs/2601.21950</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21950">https://arxiv.org/pdf/2601.21950</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21950]] Embracing Aleatoric Uncertainty in Medical Multimodal Learning with Missing Modalities(https://arxiv.org/abs/2601.21950)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Medical multimodal learning faces significant challenges with missing modalities prevalent in clinical practice. Existing approaches assume equal contribution of modality and random missing patterns, neglecting inherent uncertainty in medical data acquisition. In this regard, we propose the Aleatoric Uncertainty Modeling (AUM) that explicitly quantifies unimodal aleatoric uncertainty to address missing modalities. Specifically, AUM models each unimodal representation as a multivariate Gaussian distribution to capture aleatoric uncertainty and enable principled modality reliability quantification. To adaptively aggregate captured information, we develop a dynamic message-passing mechanism within a bipartite patient-modality graph using uncertainty-aware aggregation mechanism. Through this process, missing modalities are naturally accommodated, while more reliable information from available modalities is dynamically emphasized to guide representation generation. Our AUM framework achieves an improvement of 2.26% AUC-ROC on MIMIC-IV mortality prediction and 2.17% gain on eICU, outperforming existing state-of-the-art approaches.</li>
<li><strong>摘要：</strong>医学多模式学习面临着临床实践中普遍存在的模式缺失的重大挑战。现有方法假设模态和随机缺失模式的贡献相等，忽略了医疗数据采集中固有的不确定性。在这方面，我们提出了任意不确定性建模（AUM），它明确量化单峰任意不确定性以解决缺失的模态。具体来说，AUM 将每个单峰表示建模为多元高斯分布，以捕获任意不确定性并实现有原则的模态可靠性量化。为了自适应地聚合捕获的信息，我们使用不确定性感知聚合机制在二分患者模态图中开发了动态消息传递机制。通过这个过程，自然地适应了缺失的模态，同时动态地强调了来自可用模态的更可靠的信息以指导表示生成。我们的 AUM 框架在 MIMIC-IV 死亡率预测方面实现了 2.26% 的 AUC-ROC 提高，在 eICU 方面实现了 2.17% 的提高，优于现有的最先进方法。</li>
</ul>

<h3>Title: From Tokens to Blocks: A Block-Diffusion Perspective on Molecular Generation</h3>
<ul>
<li><strong>Authors: </strong>Qianwei Yang, Dong Xu, Zhangfan Yang, Sisi Yuan, Zexuan Zhu, Jianqiang Li, Junkai Ji</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21964">https://arxiv.org/abs/2601.21964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21964">https://arxiv.org/pdf/2601.21964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21964]] From Tokens to Blocks: A Block-Diffusion Perspective on Molecular Generation(https://arxiv.org/abs/2601.21964)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Drug discovery can be viewed as a combinatorial search over an immense chemical space, motivating the development of deep generative models for de novo molecular design. Among these, GPT-based molecular language models (MLM) have shown strong molecular design performance by learning chemical syntax and semantics from large-scale data. However, existing MLMs face two fundamental limitations: they inadequately capture the graph-structured nature of molecules when formulated as next-token prediction problems, and they typically lack explicit mechanisms for target-aware generation. Here, we propose SoftMol, a unified framework that co-designs molecular representation, model architecture, and search strategy for target-aware molecular generation. SoftMol introduces soft fragments, a rule-free block representation of SMILES that enables diffusion-native modeling, and develops SoftBD, the first block-diffusion molecular language model that combines local bidirectional diffusion with autoregressive generation under molecular structural constraints. To favor generated molecules with high drug-likeness and synthetic accessibility, SoftBD is trained on a carefully curated dataset named ZINC-Curated. SoftMol further integrates a gated Monte Carlo tree search to assemble fragments in a target-aware manner. Experimental results show that, compared with current state-of-the-art models, SoftMol achieves 100% chemical validity, improves binding affinity by 9.7%, yields a 2-3x increase in molecular diversity, and delivers a 6.6x speedup in inference efficiency. Code is available at this https URL</li>
<li><strong>摘要：</strong>药物发现可以被视为对巨大化学空间的组合搜索，推动了从头分子设计的深层生成模型的开发。其中，基于GPT的分子语言模型（MLM）通过从大规模数据中学习化学语法和语义，表现出了强大的分子设计性能。然而，现有的 MLM 面临两个基本限制：当将其表述为下一个标记预测问题时，它们无法充分捕获分子的图结构性质，并且它们通常缺乏用于目标感知生成的明确机制。在这里，我们提出了 SoftMol，一个统一的框架，可以共同设计分子表示、模型架构和目标感知分子生成的搜索策略。 SoftMol 引入了软片段，这是一种无规则的 SMILES 块表示，可实现扩散本机建模，并开发了 SoftBD，这是第一个在分子结构约束下将局部双向扩散与自回归生成相结合的块扩散分子语言模型。为了有利于生成具有高度药物相似性和合成可及性的分子，SoftBD 在精心策划的名为 ZINC-Culated 的数据集上进行了训练。 SoftMol 进一步集成了门控蒙特卡罗树搜索，以目标感知的方式组装片段。实验结果表明，与当前最先进的模型相比，SoftMol 实现了 100% 的化学有效性，提高了 9.7% 的结合亲和力，分子多样性增加了 2-3 倍，推理效率提高了 6.6 倍。代码可在此 https URL 获取</li>
</ul>

<h3>Title: PowerGenie: Analytically-Guided Evolutionary Discovery of Superior Reconfigurable Power Converters</h3>
<ul>
<li><strong>Authors: </strong>Jian Gao, Yiwei Zou, Abhishek Pradhan, Wenhao Huang, Yumin Su, Kaiyuan Yang, Xuan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21984">https://arxiv.org/abs/2601.21984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21984">https://arxiv.org/pdf/2601.21984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21984]] PowerGenie: Analytically-Guided Evolutionary Discovery of Superior Reconfigurable Power Converters(https://arxiv.org/abs/2601.21984)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Discovering superior circuit topologies requires navigating an exponentially large design space-a challenge traditionally reserved for human experts. Existing AI methods either select from predefined templates or generate novel topologies at a limited scale without rigorous verification, leaving large-scale performance-driven discovery underexplored. We present PowerGenie, a framework for automated discovery of higher-performance reconfigurable power converters at scale. PowerGenie introduces: (1) an automated analytical framework that determines converter functionality and theoretical performance limits without component sizing or SPICE simulation, and (2) an evolutionary finetuning method that co-evolves a generative model with its training distribution through fitness selection and uniqueness verification. Unlike existing methods that suffer from mode collapse and overfitting, our approach achieves higher syntax validity, function validity, novelty rate, and figure-of-merit (FoM). PowerGenie discovers a novel 8-mode reconfigurable converter with 23% higher FoM than the best training topology. SPICE simulations confirm average absolute efficiency gains of 10% across 8 modes and up to 17% at a single mode. Code is available at this https URL.</li>
<li><strong>摘要：</strong>发现卓越的电路拓扑需要探索指数级大的设计空间——这是传统上留给人类专家的挑战。现有的人工智能方法要么从预定义的模板中进行选择，要么在未经严格验证的情况下在有限的范围内生成新颖的拓扑，从而导致大规模性能驱动的发现尚未得到充分探索。我们推出了 PowerGenie，这是一个用于大规模自动发现更高性能可重构电源转换器的框架。 PowerGenie 引入了：(1) 一种自动分析框架，无需组件尺寸调整或 SPICE 模拟即可确定转换器功能和理论性能限制；(2) 一种进化微调方法，可通过适应性选择和唯一性验证来共同进化生成模型及其训练分布。与遭受模式崩溃和过度拟合的现有方法不同，我们的方法实现了更高的语法有效性、函数有效性、新颖性和品质因数（FoM）。 PowerGenie 发现了一种新颖的 8 模式可重配置转换器，其 FoM 比最佳训练拓扑高 23%。 SPICE 仿真证实，8 种模式下的平均绝对效率增益为 10%，单一模式下的平均绝对效率增益高达 17%。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Elign: Equivariant Diffusion Model Alignment from Foundational Machine Learning Force Fields</h3>
<ul>
<li><strong>Authors: </strong>Yunyang Li, Lin Huang, Luojia Xia, Wenhe Zhang, Mark Gerstein</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21985">https://arxiv.org/abs/2601.21985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21985">https://arxiv.org/pdf/2601.21985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21985]] Elign: Equivariant Diffusion Model Alignment from Foundational Machine Learning Force Fields(https://arxiv.org/abs/2601.21985)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative models for 3D molecular conformations must respect Euclidean symmetries and concentrate probability mass on thermodynamically favorable, mechanically stable structures. However, E(3)-equivariant diffusion models often reproduce biases from semi-empirical training data rather than capturing the equilibrium distribution of a high-fidelity Hamiltonian. While physics-based guidance can correct this, it faces two computational bottlenecks: expensive quantum-chemical evaluations (e.g., DFT) and the need to repeat such queries at every sampling step. We present Elign, a post-training framework that amortizes both costs. First, we replace expensive DFT evaluations with a faster, pretrained foundational machine-learning force field (MLFF) to provide physical signals. Second, we eliminate repeated run-time queries by shifting physical steering to the training phase. To achieve the second amortization, we formulate reverse diffusion as a reinforcement learning problem and introduce Force--Energy Disentangled Group Relative Policy Optimization (FED-GRPO) to fine-tune the denoising policy. FED-GRPO includes a potential-based energy reward and a force-based stability reward, which are optimized and group-normalized independently. Experiments show that Elign generates conformations with lower gold-standard DFT energies and forces, while improving stability. Crucially, inference remains as fast as unguided sampling, since no energy evaluations are required during generation.</li>
<li><strong>摘要：</strong>3D 分子构象的生成模型必须尊重欧几里德对称性，并将概率质量集中在热力学有利、机械稳定的结构上。然而，E(3) 等变扩散模型通常会从半经验训练数据中再现偏差，而不是捕获高保真哈密顿量的平衡分布。虽然基于物理的指导可以纠正这个问题，但它面临两个计算瓶颈：昂贵的量子化学评估（例如 DFT）以及需要在每个采样步骤重复此类查询。我们提出了 Elign，一个可以分摊这两种成本的培训后框架。首先，我们用更快的、预先训练的基础机器学习力场 (MLFF) 取代昂贵的 DFT 评估，以提供物理信号。其次，我们通过将物理转向转移到训练阶段来消除重复的运行时查询。为了实现第二次摊销，我们将反向扩散制定为强化学习问题，并引入力-能量解缠组相对策略优化（FED-GRPO）来微调去噪策略。 FED-GRPO 包括基于势的能量奖励和基于力的稳定性奖励，它们是独立优化和群体标准化的。实验表明，Elig 生成的构象具有较低的金标准 DFT 能量和力，同时提高了稳定性。至关重要的是，推理仍然与无引导采样一样快，因为在生成过程中不需要进行能量评估。</li>
</ul>

<h3>Title: Exploring Diverse Generation Paths via Inference-time Stiefel Activation Steering</h3>
<ul>
<li><strong>Authors: </strong>Dongxuan Zhu, Ly Tran Ho Khanh, Andy Yat-Ming Cheung, Man-Chung Yue, Viet Anh Nguyen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22010">https://arxiv.org/abs/2601.22010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22010">https://arxiv.org/pdf/2601.22010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22010]] Exploring Diverse Generation Paths via Inference-time Stiefel Activation Steering(https://arxiv.org/abs/2601.22010)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Language models often default to a narrow set of high-probability outputs, leaving their generation paths homogeneous and prone to mode collapse. Sampling-based strategies inject randomness but still struggle to guarantee diversity across multiple concurrent generation runs. We address this limitation by introducing STARS ($\textbf{St}$iefel-based $\textbf{A}$ctivation Steering for Diverse $\textbf{R}$ea$\textbf{S}$oning), a training-free, inference-time intervention method that transforms activation steering into an exploration engine. At each token, STARS collects the hidden activations of concurrent generation runs and optimizes multiple additive steering directions jointly on the Stiefel manifold. STARS maximizes the geometric volume of the steered activations, while the Stiefel manifold induces orthogonality of the steering interventions. This formulation explicitly promotes divergent activation vectors of concurrent generation runs, and implicitly promotes divergent generation trajectories. This manifold optimization formulation can be solved using a Riemannian gradient descent algorithm with convergence guarantees, but this algorithm is too time-consuming for real-time inference. To guarantee low latency, we further design a lightweight one-step update with an aggressive, closed-form stepsize. For test case generation and scientific discovery benchmarks, STARS consistently outperforms standard sampling methods, achieving greater diversity without sacrificing qualitative performance.</li>
<li><strong>摘要：</strong>语言模型通常默认为一组狭窄的高概率输出，使其生成路径同质并容易发生模式崩溃。基于采样的策略注入了随机性，但仍然难以保证多个并发生成运行的多样性。我们通过引入 STARS（$\textbf{St}$iefel-based $\textbf{A}$activation Steering for Diverse $\textbf{R}$ea$\textbf{S}$oning）来解决这一限制，这是一种免训练的推理时间干预方法，可将激活转向转变为探索引擎。在每个标记处，STARS 收集并发生成运行的隐藏激活，并在 Stiefel 流形上联合优化多个附加转向方向。 STARS 最大化了转向激活的几何体积，而 Stiefel 流形则导致了转向干预的正交性。该公式显式促进并发生成运行的发散激活向量，并隐式促进发散生成轨迹。这种流形优化公式可以使用具有收敛保证的黎曼梯度下降算法来求解，但该算法对于实时推理来说太耗时。为了保证低延迟，我们进一步设计了一种轻量级的一步更新，具有激进的封闭式步长。对于测试用例生成和科学发现基准，STARS 始终优于标准抽样方法，在不牺牲定性性能的情况下实现更大的多样性。</li>
</ul>

<h3>Title: From Logits to Latents: Contrastive Representation Shaping for LLM Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Haoran Tang, Rajiv Khanna</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22028">https://arxiv.org/abs/2601.22028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22028">https://arxiv.org/pdf/2601.22028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22028]] From Logits to Latents: Contrastive Representation Shaping for LLM Unlearning(https://arxiv.org/abs/2601.22028)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Most LLM unlearning methods aim to approximate retrain-from-scratch behaviors with minimal distribution shift, often via alignment-style objectives defined in the prediction space. While effective at reducing forgotten content generation, such approaches may act as suppression: forgotten concepts can persist in representations and remain entangled with retained knowledge. We introduce CLReg, a contrastive representation regularizer that identifies forget features while pushing them away from retain features, explicitly reducing forget-retain interference with minimal shifts on retain features. We provide first theoretical insights that relate representation shaping to entanglement reduction. Across unlearning benchmarks and LLMs of different sizes, CLReg decreases forget-retain representation entanglement that facilitates mainstream unlearning methods without positing extra privacy risks, inspiring future work that reshapes the representation space to remove forget concepts.</li>
<li><strong>摘要：</strong>大多数法学硕士取消学习方法旨在以最小的分布偏移来近似从头开始的重新训练行为，通常是通过预测空间中定义的对齐式目标。虽然可以有效减少遗忘内容的生成，但这种方法可能会起到抑制作用：遗忘的概念可能会持续存在于表征中，并与保留的知识纠缠在一起。我们引入了 CLReg，一种对比表示正则化器，它可以识别遗忘特征，同时将它们推离保留特征，从而以保留特征上的最小变化显着减少遗忘保留干扰。我们提供了将表示整形与纠缠减少联系起来的第一个理论见解。在不同规模的遗忘基准和法学硕士中，CLReg 减少了遗忘-保留表征纠缠，促进了主流遗忘方法的发展，同时又不会带来额外的隐私风险，从而激发了未来重塑表征空间以消除遗忘概念的工作。</li>
</ul>

<h3>Title: The Ensemble Inverse Problem: Applications and Methods</h3>
<ul>
<li><strong>Authors: </strong>Zhengyan Huan, Camila Pazos, Martin Klassen, Vincent Croft, Pierre-Hugues Beauchemin, Shuchin Aeron</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22029">https://arxiv.org/abs/2601.22029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22029">https://arxiv.org/pdf/2601.22029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22029]] The Ensemble Inverse Problem: Applications and Methods(https://arxiv.org/abs/2601.22029)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce a new multivariate statistical problem that we refer to as the Ensemble Inverse Problem (EIP). The aim of EIP is to invert for an ensemble that is distributed according to the pushforward of a prior under a forward process. In high energy physics (HEP), this is related to a widely known problem called unfolding, which aims to reconstruct the true physics distribution of quantities, such as momentum and angle, from measurements that are distorted by detector effects. In recent applications, the EIP also arises in full waveform inversion (FWI) and inverse imaging with unknown priors. We propose non-iterative inference-time methods that construct posterior samplers based on a new class of conditional generative models, which we call ensemble inverse generative models. For the posterior modeling, these models additionally use the ensemble information contained in the observation set on top of single measurements. Unlike existing methods, our proposed methods avoid explicit and iterative use of the forward model at inference time via training across several sets of truth-observation pairs that are consistent with the same forward model, but originate from a wide range of priors. We demonstrate that this training procedure implicitly encodes the likelihood model. The use of ensemble information helps posterior inference and enables generalization to unseen priors. We benchmark the proposed method on several synthetic and real datasets in inverse imaging, HEP, and FWI. The codes are available at this https URL.</li>
<li><strong>摘要：</strong>我们引入了一个新的多元统计问题，称为集成反问题（EIP）。 EIP 的目的是对根据前向过程下先验的前推分布的集成进行反演。在高能物理（HEP）中，这与一个众所周知的称为展开的问题有关，该问题旨在根据因探测器效应而扭曲的测量结果重建动量和角度等量的真实物理分布。在最近的应用中，EIP 还出现在全波形反演（FWI）和先验未知的逆成像中。我们提出了非迭代推理时间方法，该方法基于一类新的条件生成模型（我们称之为集成逆生成模型）构造后采样器。对于后验建模，这些模型除了单个测量之外还使用观测集中包含的集合信息。与现有方法不同，我们提出的方法通过在与同一前向模型一致但源自广泛先验的几组真相观察对上进行训练，避免在推理时显式和迭代地使用前向模型。我们证明这个训练过程隐式编码了似然模型。集成信息的使用有助于后验推理，并能够泛化到未见过的先验。我们在逆向成像、HEP 和 FWI 的几个合成和真实数据集上对所提出的方法进行了基准测试。这些代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Holographic generative flows with AdS/CFT</h3>
<ul>
<li><strong>Authors: </strong>Ehsan Mirafzali, Sanjit Shashi, Sanya Murdeshwar, Edgar Shaghoulian, Daniele Venturi, Razvan Marinescu</a></li>
<li><strong>Subjects: </strong>cs.LG, gr-qc, hep-th</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22033">https://arxiv.org/abs/2601.22033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22033">https://arxiv.org/pdf/2601.22033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22033]] Holographic generative flows with AdS/CFT(https://arxiv.org/abs/2601.22033)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present a framework for generative machine learning that leverages the holographic principle of quantum gravity, or to be more precise its manifestation as the anti-de Sitter/conformal field theory (AdS/CFT) correspondence, with techniques for deep learning and transport theory. Our proposal is to represent the flow of data from a base distribution to some learned distribution using the bulk-to-boundary mapping of scalar fields in AdS. In the language of machine learning, we are representing and augmenting the flow-matching algorithm with AdS physics. Using a checkerboard toy dataset and MNIST, we find that our model achieves faster and higher quality convergence than comparable physics-free flow-matching models. Our method provides a physically interpretable version of flow matching. More broadly, it establishes the utility of AdS physics and geometry in the development of novel paradigms in generative modeling.</li>
<li><strong>摘要：</strong>我们提出了一个生成机器学习框架，该框架利用量子引力的全息原理，或者更准确地说，其表现为反德西特/共形场论（AdS/CFT）对应，并具有深度学习和传输理论技术。我们的建议是使用 AdS 中标量场的批量到边界映射来表示从基本分布到某些学习分布的数据流。在机器学习的语言中，我们用 AdS 物理来表示和增强流匹配算法。使用棋盘玩具数据集和 MNIST，我们发现我们的模型比类似的无物理场匹配模型实现了更快、更高质量的收敛。我们的方法提供了流匹配的物理可解释版本。更广泛地说，它确立了 AdS 物理和几何在生成建模新颖范式开发中的实用性。</li>
</ul>

<h3>Title: MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources</h3>
<ul>
<li><strong>Authors: </strong>Baorui Ma, Jiahui Yang, Donglin Di, Xuancheng Zhang, Jianxun Cui, Hao Li, Yan Xie, Wei Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22054">https://arxiv.org/abs/2601.22054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22054">https://arxiv.org/pdf/2601.22054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22054]] MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources(https://arxiv.org/abs/2601.22054)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at this http URL to support community research.</li>
<li><strong>摘要：</strong>缩放推动了视觉基础模型的最新进展，但由于异构传感器噪声、相机相关偏差以及嘈杂的跨源 3D 数据中的度量模糊性，将这种范式扩展到度量深度估计仍然具有挑战性。我们推出了 Metric Anything，这是一个简单且可扩展的预训练框架，可以从嘈杂、多样化的 3D 源中学习度量深度，而无需手动设计提示、特定于相机的建模或特定于任务的架构。我们方法的核心是稀疏度量提示，它是通过随机屏蔽深度图创建的，它作为一个通用接口，将空间推理与传感器和相机偏差分离。使用跨越 10000 个相机模型重建、捕获和渲染 3D 数据的约 2000 万个图像深度对，我们首次展示了公制深度轨道中清晰的缩放趋势。预训练模型擅长提示驱动任务，例如深度完成、超分辨率和雷达相机融合，而其精炼的无提示学生在单目深度估计、相机内在恢复、单/多视图度量 3D 重建和 VLA 规划方面取得了最先进的结果。我们还表明，使用 Metric Anything 的预训练 ViT 作为视觉编码器可以显着增强空间智能中的多模态大语言模型能力。这些结果表明，度量深度估计可以受益于驱动现代基础模型的相同缩放法则，从而建立一条通向可扩展且高效的现实世界度量感知的新路径。我们在此 http URL 开源 MetricAnything 以支持社区研究。</li>
</ul>

<h3>Title: Unsupervised Decomposition and Recombination with Discriminator-Driven Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Archer Wang, Emile Anand, Yilun Du, Marin Soljačić</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22057">https://arxiv.org/abs/2601.22057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22057">https://arxiv.org/pdf/2601.22057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22057]] Unsupervised Decomposition and Recombination with Discriminator-Driven Diffusion Models(https://arxiv.org/abs/2601.22057)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Decomposing complex data into factorized representations can reveal reusable components and enable synthesizing new samples via component recombination. We investigate this in the context of diffusion-based models that learn factorized latent spaces without factor-level supervision. In images, factors can capture background, illumination, and object attributes; in robotic videos, they can capture reusable motion components. To improve both latent factor discovery and quality of compositional generation, we introduce an adversarial training signal via a discriminator trained to distinguish between single-source samples and those generated by recombining factors across sources. By optimizing the generator to fool this discriminator, we encourage physical and semantic consistency in the resulting recombinations. Our method outperforms implementations of prior baselines on CelebA-HQ, Virtual KITTI, CLEVR, and Falcor3D, achieving lower FID scores and better disentanglement as measured by MIG and MCC. Furthermore, we demonstrate a novel application to robotic video trajectories: by recombining learned action components, we generate diverse sequences that significantly increase state-space coverage for exploration on the LIBERO benchmark.</li>
<li><strong>摘要：</strong>将复杂数据分解为分解表示可以揭示可重用组件，并能够通过组件重组合成新样本。我们在基于扩散的模型的背景下对此进行了研究，该模型在没有因子级监督的情况下学习分解的潜在空间。在图像中，因素可以捕获背景、照明和物体属性；在机器人视频中，他们可以捕捉可重复使用的运动组件。为了提高潜在因素发现和组合生成的质量，我们通过训练鉴别器引入对抗性训练信号，该鉴别器经过训练以区分单源样本和通过跨源重组因素生成的样本。通过优化生成器来欺骗这个鉴别器，我们鼓励结果重组中的物理和语义一致性。我们的方法优于 CelebA-HQ、Virtual KITTI、CLEVR 和 Falcor3D 上先前基线的实现，通过 MIG 和 MCC 测量，实现了更低的 FID 分数和更好的解缠结。此外，我们展示了机器人视频轨迹的一种新颖应用：通过重新组合学习的动作组件，我们生成不同的序列，显着增加 LIBERO 基准上探索的状态空间覆盖范围。</li>
</ul>

<h3>Title: Where Do the Joules Go? Diagnosing Inference Energy Consumption</h3>
<ul>
<li><strong>Authors: </strong>Jae-Won Chung, Ruofan Wu, Jeff J. Ma, Mosharaf Chowdhury</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22076">https://arxiv.org/abs/2601.22076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22076">https://arxiv.org/pdf/2601.22076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22076]] Where Do the Joules Go? Diagnosing Inference Energy Consumption(https://arxiv.org/abs/2601.22076)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Energy is now a critical ML computing resource. While measuring energy consumption and observing trends is a valuable first step, accurately understanding and diagnosing why those differences occur is crucial for optimization. To that end, we begin by presenting a large-scale measurement study of inference time and energy across the generative AI landscape with 46 models, 7 tasks, and 1,858 different configurations on NVIDIA H100 and B200 GPUs. Our empirical findings span order-of-magnitude variations: LLM task type can lead to 25$\times$ energy differences, video generation sometimes consumes more than 100$\times$ the energy of images, and GPU utilization differences can result in 3--5$\times$ energy differences. Based on our observations, we present a framework for reasoning about the underlying mechanisms that govern time and energy consumption. The essence is that time and energy are determined by latent metrics like memory and utilization, which are in turn affected by various factors across the algorithm, software, and hardware layers. Our framework also extends directly to throughput per watt, a critical metric for power-constrained datacenters.</li>
<li><strong>摘要：</strong>能源现在是一种重要的 ML 计算资源。虽然测量能耗和观察趋势是有价值的第一步，但准确理解和诊断出现这些差异的原因对于优化至关重要。为此，我们首先在 NVIDIA H100 和 B200 GPU 上使用 46 个模型、7 个任务和 1,858 种不同配置，对生成式 AI 领域的推理时间和能量进行大规模测量研究。我们的实证研究结果存在数量级的差异：LLM 任务类型可能会导致 25$\times$ 的能量差异，视频生成有时会消耗超过 100$\times$ 图像的能量，而 GPU 利用率差异可能会导致 3--5$\times$ 的能量差异。根据我们的观察，我们提出了一个框架来推理控制时间和能源消耗的基本机制。本质上，时间和能量是由内存和利用率等潜在指标决定的，而这些指标又受到算法、软件和硬件层的各种因素的影响。我们的框架还直接扩展到每瓦吞吐量，这是功率受限的数据中心的一个关键指标。</li>
</ul>

<h3>Title: RefAny3D: 3D Asset-Referenced Diffusion Models for Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Hanzhuo Huang, Qingyang Bao, Zekai Gu, Zhongshuo Du, Cheng Lin, Yuan Liu, Sibei Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22094">https://arxiv.org/abs/2601.22094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22094">https://arxiv.org/pdf/2601.22094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22094]] RefAny3D: 3D Asset-Referenced Diffusion Models for Image Generation(https://arxiv.org/abs/2601.22094)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a 3D asset-referenced diffusion model for image generation, exploring how to integrate 3D assets into image diffusion models. Existing reference-based image generation methods leverage large-scale pretrained diffusion models and demonstrate strong capability in generating diverse images conditioned on a single reference image. However, these methods are limited to single-image references and cannot leverage 3D assets, constraining their practical versatility. To address this gap, we present a cross-domain diffusion model with dual-branch perception that leverages multi-view RGB images and point maps of 3D assets to jointly model their colors and canonical-space coordinates, achieving precise consistency between generated images and the 3D references. Our spatially aligned dual-branch generation architecture and domain-decoupled generation mechanism ensure the simultaneous generation of two spatially aligned but content-disentangled outputs, RGB images and point maps, linking 2D image attributes with 3D asset attributes. Experiments show that our approach effectively uses 3D assets as references to produce images consistent with the given assets, opening new possibilities for combining diffusion models with 3D content creation.</li>
<li><strong>摘要：</strong>在本文中，我们提出了一种用于图像生成的3D资产参考扩散模型，探索如何将3D资产集成到图像扩散模型中。现有的基于参考的图像生成方法利用大规模预训练扩散模型，并表现出在单个参考图像上生成不同图像的强大能力。然而，这些方法仅限于单图像参考，无法利用 3D 资产，限制了它们的实际多功能性。为了解决这一差距，我们提出了一种具有双分支感知的跨域扩散模型，该模型利用多视图 RGB 图像和 3D 资产的点图来联合建模其颜色和规范空间坐标，从而实现生成图像和 3D 参考之间的精确一致性。我们的空间对齐双分支生成架构和域解耦生成机制确保同时生成两个空间对齐但内容分离的输出：RGB 图像和点图，将 2D 图像属性与 3D 资产属性链接起来。实验表明，我们的方法有效地使用 3D 资产作为参考来生成与给定资产一致的图像，为将扩散模型与 3D 内容创建相结合开辟了新的可能性。</li>
</ul>

<h3>Title: Prior-Informed Flow Matching for Graph Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Harvey Chen, Nicolas Zilberstein, Santiago Segarra</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22107">https://arxiv.org/abs/2601.22107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22107">https://arxiv.org/pdf/2601.22107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22107]] Prior-Informed Flow Matching for Graph Reconstruction(https://arxiv.org/abs/2601.22107)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce Prior-Informed Flow Matching (PIFM), a conditional flow model for graph reconstruction. Reconstructing graphs from partial observations remains a key challenge; classical embedding methods often lack global consistency, while modern generative models struggle to incorporate structural priors. PIFM bridges this gap by integrating embedding-based priors with continuous-time flow matching. Grounded in a permutation equivariant version of the distortion-perception theory, our method first uses a prior, such as graphons or GraphSAGE/node2vec, to form an informed initial estimate of the adjacency matrix based on local information. It then applies rectified flow matching to refine this estimate, transporting it toward the true distribution of clean graphs and learning a global coupling. Experiments on different datasets demonstrate that PIFM consistently enhances classical embeddings, outperforming them and state-of-the-art generative baselines in reconstruction accuracy.</li>
<li><strong>摘要：</strong>我们引入先验流匹配（PIFM），这是一种用于图重建的条件流模型。从部分观察结果重建图表仍然是一个关键挑战；经典的嵌入方法通常缺乏全局一致性，而现代生成模型则难以融入结构先验。 PIFM 通过将基于嵌入的先验与连续时间流匹配相结合来弥补这一差距。基于失真感知理论的排列等变版本，我们的方法首先使用先验，例如 graphon 或 GraphSAGE/node2vec，基于局部信息形成邻接矩阵的知情初始估计。然后，它应用修正流匹配来完善该估计，将其传输到干净图的真实分布并学习全局耦合。对不同数据集的实验表明，PIFM 始终如一地增强了经典嵌入，在重建精度方面优于经典嵌入和最先进的生成基线。</li>
</ul>

<h3>Title: SINA: A Circuit Schematic Image-to-Netlist Generator Using Artificial Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Saoud Aldowaish, Yashwanth Karumanchi, Kai-Chen Chiang, Soroosh Noorzad, Morteza Fayazi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22114">https://arxiv.org/abs/2601.22114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22114">https://arxiv.org/pdf/2601.22114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22114]] SINA: A Circuit Schematic Image-to-Netlist Generator Using Artificial Intelligence(https://arxiv.org/abs/2601.22114)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Current methods for converting circuit schematic images into machine-readable netlists struggle with component recognition and connectivity inference. In this paper, we present SINA, an open-source, fully automated circuit schematic image-to-netlist generator. SINA integrates deep learning for accurate component detection, Connected-Component Labeling (CCL) for precise connectivity extraction, and Optical Character Recognition (OCR) for component reference designator retrieval, while employing a Vision-Language Model (VLM) for reliable reference designator assignments. In our experiments, SINA achieves 96.47% overall netlist-generation accuracy, which is 2.72x higher than state-of-the-art approaches.</li>
<li><strong>摘要：</strong>当前将电路原理图图像转换为机器可读网表的方法在组件识别和连接推断方面存在困难。在本文中，我们介绍了 SINA，这是一款开源、全自动的电路原理图图像到网表生成器。 SINA 集成了用于精确元件检测的深度学习、用于精确连接提取的连接元件标签 (CCL) 和用于元件参考指示符检索的光学字符识别 (OCR)，同时采用视觉语言模型 (VLM) 进行可靠的参考指示符分配。在我们的实验中，SINA 实现了 96.47% 的总体网表生成精度，比最先进的方法高 2.72 倍。</li>
</ul>

<h3>Title: Learning Hamiltonian Flow Maps: Mean Flow Consistency for Large-Timestep Molecular Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Winfried Ripken, Michael Plainer, Gregor Lied, Thorben Frank, Oliver T. Unke, Stefan Chmiela, Frank Noé, Klaus Robert Müller</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22123">https://arxiv.org/abs/2601.22123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22123">https://arxiv.org/pdf/2601.22123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22123]] Learning Hamiltonian Flow Maps: Mean Flow Consistency for Large-Timestep Molecular Dynamics(https://arxiv.org/abs/2601.22123)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Simulating the long-time evolution of Hamiltonian systems is limited by the small timesteps required for stable numerical integration. To overcome this constraint, we introduce a framework to learn Hamiltonian Flow Maps by predicting the mean phase-space evolution over a chosen time span $\Delta t$, enabling stable large-timestep updates far beyond the stability limits of classical integrators. To this end, we impose a Mean Flow consistency condition for time-averaged Hamiltonian dynamics. Unlike prior approaches, this allows training on independent phase-space samples without access to future states, avoiding expensive trajectory generation. Validated across diverse Hamiltonian systems, our method in particular improves upon molecular dynamics simulations using machine-learned force fields (MLFF). Our models maintain comparable training and inference cost, but support significantly larger integration timesteps while trained directly on widely-available trajectory-free MLFF datasets.</li>
<li><strong>摘要：</strong>模拟哈密顿系统的长期演化受到稳定数值积分所需的小时间步长的限制。为了克服这一限制，我们引入了一个框架，通过预测选定时间跨度 $\Delta t$ 内的平均相空间演化来学习哈密顿流图，从而实现稳定的大时间步长更新，远远超出经典积分器的稳定性限制。为此，我们对时间平均哈密顿动力学施加了平均流一致性条件。与之前的方法不同，这允许在独立相空间样本上进行训练，而无需访问未来状态，从而避免了昂贵的轨迹生成。我们的方法经过不同哈密顿系统的验证，特别改进了使用机器学习力场（MLFF）的分子动力学模拟。我们的模型保持了相当的训练和推理成本，但在直接在广泛可用的无轨迹 MLFF 数据集上进行训练时，支持显着更大的集成时间步长。</li>
</ul>

<h3>Title: Creative Image Generation with Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Kunpeng Song, Ahmed Elgammal</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22125">https://arxiv.org/abs/2601.22125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22125">https://arxiv.org/pdf/2601.22125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22125]] Creative Image Generation with Diffusion Model(https://arxiv.org/abs/2601.22125)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Creative image generation has emerged as a compelling area of research, driven by the need to produce novel and high-quality images that expand the boundaries of imagination. In this work, we propose a novel framework for creative generation using diffusion models, where creativity is associated with the inverse probability of an image's existence in the CLIP embedding space. Unlike prior approaches that rely on a manual blending of concepts or exclusion of subcategories, our method calculates the probability distribution of generated images and drives it towards low-probability regions to produce rare, imaginative, and visually captivating outputs. We also introduce pullback mechanisms, achieving high creativity without sacrificing visual fidelity. Extensive experiments on text-to-image diffusion models demonstrate the effectiveness and efficiency of our creative generation framework, showcasing its ability to produce unique, novel, and thought-provoking images. This work provides a new perspective on creativity in generative models, offering a principled method to foster innovation in visual content synthesis.</li>
<li><strong>摘要：</strong>创意图像生成已成为一个引人注目的研究领域，这是由于需要生成新颖且高质量的图像来扩展想象力的边界。在这项工作中，我们提出了一种使用扩散模型进行创意生成的新颖框架，其中创意与 CLIP 嵌入空间中图像存在的逆概率相关。与依赖于手动混合概念或排除子类别的先前方法不同，我们的方法计算生成图像的概率分布并将其驱动到低概率区域以产生罕见的、富有想象力的和视觉上迷人的输出。我们还引入了回拉机制，在不牺牲视觉保真度的情况下实现高创造力。对文本到图像扩散模型的大量实验证明了我们的创意生成框架的有效性和效率，展示了其生成独特、新颖且发人深省的图像的能力。这项工作为生成模型中的创造力提供了新的视角，提供了促进视觉内容合成创新的原则方法。</li>
</ul>

<h3>Title: EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>John Flynn, Wolfgang Paier, Dimitar Dinev, Sam Nhut Nguyen, Hayk Poghosyan, Manuel Toribio, Sandipan Banerjee, Guy Gafni</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22127">https://arxiv.org/abs/2601.22127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22127">https://arxiv.org/pdf/2601.22127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22127]] EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers(https://arxiv.org/abs/2601.22127)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Current generative video models excel at producing novel content from text and image prompts, but leave a critical gap in editing existing pre-recorded videos, where minor alterations to the spoken script require preserving motion, temporal coherence, speaker identity, and accurate lip synchronization. We introduce EditYourself, a DiT-based framework for audio-driven video-to-video (V2V) editing that enables transcript-based modification of talking head videos, including the seamless addition, removal, and retiming of visually spoken content. Building on a general-purpose video diffusion model, EditYourself augments its V2V capabilities with audio conditioning and region-aware, edit-focused training extensions. This enables precise lip synchronization and temporally coherent restructuring of existing performances via spatiotemporal inpainting, including the synthesis of realistic human motion in newly added segments, while maintaining visual fidelity and identity consistency over long durations. This work represents a foundational step toward generative video models as practical tools for professional video post-production.</li>
<li><strong>摘要：</strong>当前的生成视频模型擅长根据文本和图像提示生成新颖的内容，但在编辑现有预先录制的视频时留下了关键的空白，其中对口语脚本的微小更改需要保留运动、时间连贯性、说话者身份和准确的唇形同步。我们推出了 EditYourself，这是一个基于 DiT 的音频驱动视频到视频 (V2V) 编辑框架，可以对头部说话视频进行基于转录的修改，包括无缝添加、删除和重新定时视觉语音内容。 EditYourself 以通用视频扩散模型为基础，通过音频调节和区域感知、以编辑为重点的培训扩展增强了其 V2V 功能。这可以通过时空修复实现精确的口型同步和对现有表演的时间连贯重组，包括在新添加的片段中合成真实的人体运动，同时在长时间内保持视觉保真度和身份一致性。这项工作代表了生成视频模型作为专业视频后期制作实用工具的基础性一步。</li>
</ul>

<h3>Title: Pay for Hints, Not Answers: LLM Shepherding for Cost-Efficient Inference</h3>
<ul>
<li><strong>Authors: </strong>Ziming Dong, Hardik Sharma, Evan O'Toole, Jaya Prakash Champati, Kui Wu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22132">https://arxiv.org/abs/2601.22132</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22132">https://arxiv.org/pdf/2601.22132</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22132]] Pay for Hints, Not Answers: LLM Shepherding for Cost-Efficient Inference(https://arxiv.org/abs/2601.22132)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) deliver state-of-the-art performance on complex reasoning tasks, but their inference costs limit deployment at scale. Small Language Models (SLMs) offer dramatic cost savings yet lag substantially in accuracy. Existing approaches - routing and cascading - treat the LLM as an all-or-nothing resource: either the query bypasses the LLM entirely, or the LLM generates a complete response at full cost. We introduce LLM Shepherding, a framework that requests only a short prefix (a hint) from the LLM and provides it to SLM. This simple mechanism is surprisingly effective for math and coding tasks: even hints comprising 10-30% of the full LLM response improve SLM accuracy significantly. Shepherding generalizes both routing and cascading, and it achieves lower cost under oracle decision-making. We develop a two-stage predictor that jointly determines whether a hint is needed and how many tokens to request. On the widely-used mathematical reasoning (GSM8K, CNK12) and code generation (HumanEval, MBPP) benchmarks, Shepherding reduces costs by 42-94% relative to LLM-only inference. Compared to state-of-the-art routing and cascading baselines, shepherding delivers up to 2.8x cost reduction while matching accuracy. To our knowledge, this is the first work to exploit token-level budget control for SLM-LLM collaboration.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在复杂的推理任务上提供最先进的性能，但其推理成本限制了大规模部署。小语言模型 (SLM) 可以显着节省成本，但准确性却严重滞后。现有方法（路由和级联）将 LLM 视为全有或全无的资源：要么查询完全绕过 LLM，要么 LLM 以全部成本生成完整响应。我们引入了 LLM Shepherding，这是一个仅向 LLM 请求短前缀（提示）并将其提供给 SLM 的框架。这种简单的机制对于数学和编码任务非常有效：即使提示占完整 LLM 响应的 10-30%，也能显着提高 SLM 准确性。 Shepherding概括了路由和级联，在预言机决策下实现了更低的成本。我们开发了一个两阶段预测器，共同确定是否需要提示以及请求多少令牌。在广泛使用的数学推理（GSM8K、CNK12）和代码生成（HumanEval、MBPP）基准上，相对于仅 LLM 推理，Shepherding 降低了 42-94% 的成本。与最先进的路由和级联基线相比，牧养可在匹配准确性的同时降低高达 2.8 倍的成本。据我们所知，这是第一个利用代币级预算控制进行 SLM-LLM 协作的工作。</li>
</ul>

<h3>Title: UEval: A Benchmark for Unified Multimodal Generation</h3>
<ul>
<li><strong>Authors: </strong>Bo Li, Yida Yin, Wenhao Chai, Xingyu Fu, Zhuang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22155">https://arxiv.org/abs/2601.22155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22155">https://arxiv.org/pdf/2601.22155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22155]] UEval: A Benchmark for Unified Multimodal Generation(https://arxiv.org/abs/2601.22155)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce UEval, a benchmark to evaluate unified models, i.e., models capable of generating both images and text. UEval comprises 1,000 expert-curated questions that require both images and text in the model output, sourced from 8 real-world tasks. Our curated questions cover a wide range of reasoning types, from step-by-step guides to textbook explanations. Evaluating open-ended multimodal generation is non-trivial, as simple LLM-as-a-judge methods can miss the subtleties. Different from previous works that rely on multimodal Large Language Models (MLLMs) to rate image quality or text accuracy, we design a rubric-based scoring system in UEval. For each question, reference images and text answers are provided to a MLLM to generate an initial rubric, consisting of multiple evaluation criteria, and human experts then refine and validate these rubrics. In total, UEval contains 10,417 validated rubric criteria, enabling scalable and fine-grained automatic scoring. UEval is challenging for current unified models: GPT-5-Thinking scores only 66.4 out of 100, while the best open-source model reaches merely 49.1. We observe that reasoning models often outperform non-reasoning ones, and transferring reasoning traces from a reasoning model to a non-reasoning model significantly narrows the gap. This suggests that reasoning may be important for tasks requiring complex multimodal understanding and generation.</li>
<li><strong>摘要：</strong>我们引入 UEval，这是评估统一模型（即能够生成图像和文本的模型）的基准。 UEval 包含 1,000 个专家策划的问题，这些问题需要模型输出中的图像和文本，这些问题源自 8 个现实世界的任务。我们精心策划的问题涵盖了广泛的推理类型，从分步指南到教科书解释。评估开放式多模式生成并非易事，因为简单的法学硕士作为法官的方法可能会错过其中的微妙之处。与之前依赖多模态大语言模型（MLLM）来评估图像质量或文本准确性的作品不同，我们在 UEval 中设计了一个基于评分标准的评分系统。对于每个问题，参考图像和文本答案都会提供给 MLLM，以生成包含多个评估标准的初始评估标准，然后由人类专家完善和验证这些评估标准。 UEval 总共包含 10,417 个经过验证的评分标准，可实现可扩展且细粒度的自动评分。 UEval 对于当前的统一模型来说具有挑战性：GPT-5-Thinking 得分仅为 66.4（满分 100），而最好的开源模型仅达到 49.1。我们观察到推理模型通常优于非推理模型，并且将推理轨迹从推理模型转移到非推理模型可以显着缩小差距。这表明推理对于需要复杂的多模态理解和生成的任务可能很重要。</li>
</ul>

<h3>Title: One-step Latent-free Image Generation with Pixel Mean Flows</h3>
<ul>
<li><strong>Authors: </strong>Yiyang Lu, Susie Lu, Qiao Sun, Hanhong Zhao, Zhicheng Jiang, Xianbang Wang, Tianhong Li, Zhengyang Geng, Kaiming He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22158">https://arxiv.org/abs/2601.22158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22158">https://arxiv.org/pdf/2601.22158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22158]] One-step Latent-free Image Generation with Pixel Mean Flows(https://arxiv.org/abs/2601.22158)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose "pixel MeanFlow" (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.</li>
<li><strong>摘要：</strong>用于图像生成的现代基于扩散/流的模型通常表现出两个核心特征：（i）使用多步采样，以及（ii）在潜在空间中操作。最近的进展在每个方面都取得了令人鼓舞的进展，为无潜伏的一步扩散/流动铺平了道路。在这项工作中，我们朝着这个目标又迈出了一步，提出了“像素平均流”（pMF）。我们的核心准则是分别制定网络输出空间和损失空间。网络目标被设计为位于假定的低维图像流形（即 x 预测）上，而损失是通过速度空间中的 MeanFlow 定义的。我们引入图像流形和平均速度场之间的简单变换。在实验中，pMF 在 ImageNet 上以 256x256 分辨率（2.22 FID）和 512x512 分辨率（2.48 FID）实现单步潜在无生成，取得了很好的结果，填补了这一领域的关键缺失。我们希望我们的研究将进一步推进基于扩散/流的生成模型的边界。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
