<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-08</h1>
<h3>Title: Generative Inverse Design: From Single Point Optimization to a Diverse Design Portfolio via Conditional Variational Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Arif Hakimi Zamrai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05160">https://arxiv.org/abs/2510.05160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05160">https://arxiv.org/pdf/2510.05160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05160]] Generative Inverse Design: From Single Point Optimization to a Diverse Design Portfolio via Conditional Variational Autoencoders(https://arxiv.org/abs/2510.05160)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Inverse design, which seeks to find optimal parameters for a target output, is a central challenge in engineering. Surrogate-based optimization (SBO) has become a standard approach, yet it is fundamentally structured to converge to a single-point solution, thereby limiting design space exploration and ignoring potentially valuable alternative topologies. This paper presents a paradigm shift from single-point optimization to generative inverse design. We introduce a framework based on a Conditional Variational Autoencoder (CVAE) that learns a probabilistic mapping between a system's design parameters and its performance, enabling the generation of a diverse portfolio of high-performing candidates conditioned on a specific performance objective. We apply this methodology to the complex, non-linear problem of minimizing airfoil self-noise, using a high-performing SBO method from a prior benchmark study as a rigorous baseline. The CVAE framework successfully generated 256 novel designs with a 94.1\% validity rate. A subsequent surrogate-based evaluation revealed that 77.2\% of these valid designs achieved superior performance compared to the single optimal design found by the SBO baseline. This work demonstrates that the generative approach not only discovers higher-quality solutions but also provides a rich portfolio of diverse candidates, fundamentally enhancing the engineering design process by enabling multi-criteria decision-making.</li>
<li><strong>摘要：</strong>逆向设计旨在寻找目标输出的最佳参数，是工程领域的一个核心挑战。基于代理的优化 (SBO) 已成为一种标准方法，但其从根本上构建为收敛于单点解决方案，从而限制了设计空间探索并忽略了潜在有价值的替代拓扑。本文提出了从单点优化到生成逆向设计的范式转变。我们引入了一个基于条件变分自动编码器（CVAE）的框架，该框架学习系统设计参数与其性能之间的概率映射，从而能够生成基于特定性能目标的多样化高性能候选组合。我们将这种方法应用于最小化翼型自噪声的复杂非线性问题，并使用先前基准研究中的高性能 SBO 方法作为严格的基线。 CVAE 框架成功生成了 256 个新颖的设计，有效率为 94.1%。随后的基于替代的评估显示，与 SBO 基线发现的单个最佳设计相比，这些有效设计中有 77.2% 实现了卓越的性能。这项工作表明，生成方法不仅可以发现更高质量的解决方案，而且还提供了丰富的多样化候选方案组合，通过实现多标准决策从根本上增强了工程设计过程。</li>
</ul>

<h3>Title: Attention-Enhanced Prototypical Learning for Few-Shot Infrastructure Defect Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Christina Thrainer, Md Meftahul Ferdaus, Mahdi Abdelguerfi, Christian Guetl, Steven Sloan, Kendall N. Niles, Ken Pathak</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05266">https://arxiv.org/abs/2510.05266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05266">https://arxiv.org/pdf/2510.05266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05266]] Attention-Enhanced Prototypical Learning for Few-Shot Infrastructure Defect Segmentation(https://arxiv.org/abs/2510.05266)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Few-shot semantic segmentation is vital for deep learning-based infrastructure inspection applications, where labeled training examples are scarce and expensive. Although existing deep learning frameworks perform well, the need for extensive labeled datasets and the inability to learn new defect categories with little data are problematic. We present our Enhanced Feature Pyramid Network (E-FPN) framework for few-shot semantic segmentation of culvert and sewer defect categories using a prototypical learning framework. Our approach has three main contributions: (1) adaptive E-FPN encoder using InceptionSepConv blocks and depth-wise separable convolutions for efficient multi-scale feature extraction; (2) prototypical learning with masked average pooling for powerful prototype generation from small support examples; and (3) attention-based feature representation through global self-attention, local self-attention and cross-attention. Comprehensive experimentation on challenging infrastructure inspection datasets illustrates that the method achieves excellent few-shot performance, with the best configuration being 8-way 5-shot training configuration at 82.55% F1-score and 72.26% mIoU in 2-way classification testing. The self-attention method had the most significant performance improvements, providing 2.57% F1-score and 2.9% mIoU gain over baselines. Our framework addresses the critical need to rapidly respond to new defect types in infrastructure inspection systems with limited new training data that lead to more efficient and economical maintenance plans for critical infrastructure systems.</li>
<li><strong>摘要：</strong>很少有语义细分对于基于深度学习的基础架构检查应用程序至关重要，在这些基础架构检查应用程序中，标有培训示例稀缺且昂贵。尽管现有的深度学习框架表现良好，但需要广泛的标记数据集，并且无法学习新的缺陷类别，而数据很少有问题。我们介绍了使用典型的学习框架对涵洞和下水道缺陷类别进行几次范围的语义分割的增强功能金字塔网络（E-FPN）框架。我们的方法具有三个主要贡献：（1）使用InceptionSepConv块和深度可分离卷积的自适应E-FPN编码器，以进行有效的多尺度特征提取； （2）由遮罩的平均汇总，用于从小型支持示例中生成强大原型的原型学习； （3）通过全球自我注意，局部自我注意和交叉注意的基于注意力的特征表示。对挑战基础设施检查数据集进行的全面实验表明，该方法可实现出色的少量性能，最佳配置是82.55％F1得分为82.55％的8速5-Shot培训配置，在2-Way分类测试中进行了72.26％MIOU。自我注意力的方法具有最显着的性能提高，可提供2.57％的F1得分和2.9％的MIOU比基线增长。我们的框架解决了对基础设施检查系统中新的缺陷类型迅速响应的关键需求，其新培训数据有限，从而为关键基础设施系统提供了更有效，更经济的维护计划。</li>
</ul>

<h3>Title: Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Hyung Gyu Rho</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05342">https://arxiv.org/abs/2510.05342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05342">https://arxiv.org/pdf/2510.05342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05342]] Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization(https://arxiv.org/abs/2510.05342)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Direct Preference Optimization (DPO) has emerged as a simple and effective method for aligning large language models. However, its reliance on a fixed temperature parameter leads to suboptimal training on diverse preference data, causing overfitting on easy examples and under-learning from informative ones. Recent methods have emerged to counter this. While IPO addresses general overfitting, its uniform regularization can be overly conservative. The more targeted approach of $\beta$-DPO suffers from its own limitations: its batch-level adaptation applies a single, compromised temperature to mixed-margin pairs, its linear update rule can produce unstable negative $\beta$ values, and its filtering mechanism discards potentially useful training signals. In this work, we introduce Margin-Adaptive Direct Preference Optimization (MADPO), a method that provides a stable, data-preserving, and instance-level solution. MADPO employs a practical two-step approach: it first trains a reward model to estimate preference margins and then uses these margins to apply a continuous, adaptive weight to the DPO loss for each individual training sample. This re-weighting scheme creates an effective target margin that is amplified for hard pairs and dampened for easy pairs, allowing for granular control over the learning signal. We provide a comprehensive theoretical analysis, proving that MADPO has a well-behaved optimization landscape and is robust to reward model estimation errors. We validate our theory with experiments on a sentiment generation task, where MADPO consistently and significantly outperforms strong baselines across datasets of varying quality. It achieves performance gains of up to +33.3\% on High Quality data and +10.5\% on Low Quality data over the next-best method. Our results establish MADPO as a more robust and principled approach to preference alignment.</li>
<li><strong>摘要：</strong>直接偏好优化 (DPO) 已成为对齐大型语言模型的简单而有效的方法。然而，它对固定温度参数的依赖导致对不同偏好数据的训练不理想，导致对简单示例的过度拟合和对信息丰富示例的学习不足。最近出现了解决这个问题的方法。虽然 IPO 解决了一般的过度拟合问题，但其统一正则化可能过于保守。 $\beta$-DPO 更具针对性的方法也存在其自身的局限性：它的批量级自适应将单一的、折衷的温度应用于混合裕度对，其线性更新规则可能会产生不稳定的负 $\beta$ 值，并且其过滤机制会丢弃潜在有用的训练信号。在这项工作中，我们介绍了保证金自适应直接偏好优化（MADPO），这是一种提供稳定、数据保护和实例级解决方案的方法。 MADPO 采用实用的两步方法：首先训练奖励模型来估计偏好边际，然后使用这些边际对每个单独训练样本的 DPO 损失应用连续的自适应权重。这种重新加权方案创建了一个有效的目标裕度，对于困难对，该目标裕度被放大，对于简单对，该目标裕度被抑制，从而允许对学习信号进行精细控制。我们提供了全面的理论分析，证明 MADPO 具有良好的优化景观，并且对于奖励模型估计错误具有鲁棒性。我们通过情感生成任务的实验验证了我们的理论，其中 MADPO 在不同质量的数据集中始终显着优于强大的基线。与次优方法相比，它在高质量数据上的性能提升高达 +33.3\%，在低质量数据上的性能提升高达 +10.5\%。我们的结果表明，MADPO 是一种更稳健、更有原则的偏好调整方法。</li>
</ul>

<h3>Title: Mitigating Diffusion Model Hallucinations with Dynamic Guidance</h3>
<ul>
<li><strong>Authors: </strong>Kostas Triaridis, Alexandros Graikos, Aggelina Chatziagapi, Grigorios G. Chrysos, Dimitris Samaras</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05356">https://arxiv.org/abs/2510.05356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05356">https://arxiv.org/pdf/2510.05356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05356]] Mitigating Diffusion Model Hallucinations with Dynamic Guidance(https://arxiv.org/abs/2510.05356)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models, despite their impressive demos, often produce hallucinatory samples with structural inconsistencies that lie outside of the support of the true data distribution. Such hallucinations can be attributed to excessive smoothing between modes of the data distribution. However, semantic interpolations are often desirable and can lead to generation diversity, thus we believe a more nuanced solution is required. In this work, we introduce Dynamic Guidance, which tackles this issue. Dynamic Guidance mitigates hallucinations by selectively sharpening the score function only along the pre-determined directions known to cause artifacts, while preserving valid semantic variations. To our knowledge, this is the first approach that addresses hallucinations at generation time rather than through post-hoc filtering. Dynamic Guidance substantially reduces hallucinations on both controlled and natural image datasets, significantly outperforming baselines.</li>
<li><strong>摘要：</strong>扩散模型尽管有令人印象深刻的演示，但通常会产生结构不一致的幻觉样本，这些样本超出了真实数据分布的支持。这种幻觉可归因于数据分布模式之间的过度平滑。然而，语义插值通常是可取的，并且可以导致生成多样性，因此我们认为需要更细致的解决方案。在这项工作中，我们引入了动态引导来解决这个问题。动态引导通过仅沿着已知会导致伪影的预定方向选择性地锐化评分函数来减轻幻觉，同时保留有效的语义变化。据我们所知，这是第一种在生成时而不是通过事后过滤来解决幻觉的方法。动态引导大大减少了受控图像数据集和自然图像数据集上的幻觉，显着优于基线。</li>
</ul>

<h3>Title: LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Yang Xiao, Gen Li, Kaiyuan Deng, Yushu Wu, Zheng Zhan, Yanzhi Wang, Xiaolong Ma, Bo Hui</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05367">https://arxiv.org/abs/2510.05367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05367">https://arxiv.org/pdf/2510.05367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05367]] LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation(https://arxiv.org/abs/2510.05367)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Training-free acceleration has emerged as an advanced research area in video generation based on diffusion models. The redundancy of latents in diffusion model inference provides a natural entry point for acceleration. In this paper, we decompose the inference process into the encoding, denoising, and decoding stages, and observe that cache-based acceleration methods often lead to substantial memory surges in the latter two stages. To address this problem, we analyze the characteristics of inference across different stages and propose stage-specific strategies for reducing memory consumption: 1) Asynchronous Cache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same time, we ensure that the time overhead introduced by these three strategies remains lower than the acceleration gains themselves. Compared with the baseline, our approach achieves faster inference speed and lower memory usage, while maintaining quality degradation within an acceptable range. The Code is available at this https URL .</li>
<li><strong>摘要：</strong>根据扩散模型，无培训的加速已成为视频生成的高级研究领域。扩散模型推断中潜伏期的冗余为加速提供了自然的入口点。在本文中，我们将推理过程分解为编码，降解和解码阶段，并观察到基于缓存的加速方法通常会导致后两个阶段的大量记忆飙升。为了解决这个问题，我们分析了跨不同阶段推论的特征，并提出了阶段特定的策略来减少记忆消耗：1）异步缓存交换。 2）特征块。 3）切片潜在的解码。同时，我们确保这三种策略所引入的高间接时间仍然低于加速度的收益。与基线相比，我们的方法达到了更快的推理速度和较低的内存使用量，同时将质量降解在可接受的范围内。该代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: KVLinC : KV Cache Quantization with Hadamard Rotation and Linear Correction</h3>
<ul>
<li><strong>Authors: </strong>Utkarsh Saxena, Kaushik Roy</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05373">https://arxiv.org/abs/2510.05373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05373">https://arxiv.org/pdf/2510.05373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05373]] KVLinC : KV Cache Quantization with Hadamard Rotation and Linear Correction(https://arxiv.org/abs/2510.05373)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Quantizing the key-value (KV) cache is a promising strategy for improving the inference efficiency of large language models (LLMs). However, aggressive quantization to very low precision (e.g., 2 bits) introduces significant errors in the stored key and value tensors, which propagate through the dot-product attention mechanism and ultimately degrade generation quality. To address this, we propose KVLinC, a framework to mitigate attention errors introduced by KV cache quantization in the extreme low-precision regime. KVLinC combines a Hadamard rotation, which reduces quantization error in values, with lightweight linear correction adapters that explicitly compensate for errors introduced by quantized keys. Across extensive evaluations on the LLaMA, Qwen2.5, and Qwen3 model families, KVLinC consistently matches or surpasses strong baselines while achieving higher KV-cache compression. Furthermore, we implement a custom attention kernel that results in upto 2.55x faster inference compared to Flash Attention baseline, enabling efficient long-context LLM inference.</li>
<li><strong>摘要：</strong>量化键值（KV）缓存是提高大语言模型（LLMS）推理效率的有前途的策略。但是，积极的量化非常低的精度（例如，2位）引入了存储的密钥和价值张量的重大错误，这些错误通过点 - 产品的注意机制传播，并最终降低生成质量。为了解决这个问题，我们提出了KVLINC，这是一个框架，以减轻极端低精度制度中KV缓存量化引入的注意力错误。 KVLINC结合了Hadamard旋转，该旋转降低了值的量化误差，轻质线性校正适配器明确补偿了量化键引入的错误。在对骆驼，QWEN2.5和QWEN3模型家族的广泛评估中，KVLINC始终匹配或超过强大的基线，同时实现了更高的KV-CACHE压缩。此外，我们实现了一种自定义注意力内核，与闪光灯基线相比，推断的速度最高2.55倍，从而实现了有效的长篇文化LLM推理。</li>
</ul>

<h3>Title: Draft, Verify, and Improve: Toward Training-Aware Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Shrenik Bhansali, Larry Heck</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05421">https://arxiv.org/abs/2510.05421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05421">https://arxiv.org/pdf/2510.05421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05421]] Draft, Verify, and Improve: Toward Training-Aware Speculative Decoding(https://arxiv.org/abs/2510.05421)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Autoregressive (AR) decoding is a major latency bottleneck for large language models. Speculative decoding (SD) accelerates AR by letting a drafter propose multi-token blocks that a verifier accepts or rejects. However, many SD systems require heavy offline training or extra components. These choices raise data/compute cost and can yield brittle drafters under distribution drift. We introduce \emph{Draft, Verify, \& Improve (DVI)}, a training-aware self-speculative framework that combines inference with continual online learning. We partition an LLM into a drafter and a verifier, and during generation, verifier accept/reject decisions are converted into supervision signals and used to update the drafter head. A simple \emph{KL$\rightarrow$RL} schedule bootstraps calibration via online distillation and then adds reward-masked cross-entropy with a on-policy policy-gradient term, preserving lossless, single model deployment. On Spec-Bench, DVI achieves a $2.16\times$ wall-time speedup, on par with SoTA approaches like EAGLE-2, while orders of magnitude less data for training, and ablations show that DVI outperforms KL-only online distillation. DVI demonstrates that \emph{training-aware} self-speculation can deliver state-of-the-art, lossless speedups with minimal training overhead.</li>
<li><strong>摘要：</strong>自回归（AR）解码是大型语言模型的主要潜伏期瓶颈。投机解码（SD）通过让起草者提出验证者接受或拒绝的多型块来加速AR。但是，许多SD系统都需要大量的离线培训或额外的组件。这些选择提高了数据/计算成本，并可以在分配漂移下产生脆弱的起草者。我们介绍\ emph {草稿，验证，\＆Revor（DVI）}，这是一个训练意识到的自我指定框架，将推论与持续的在线学习相结合。我们将LLM分配为起草者和验证者，在生成期间，验证者接受/拒绝决策将转换为监督信号，并用于更新起草者的头部。一个简单的\ emph {kl $ \ rightarrow $ rl}通过在线蒸馏计划进行引导校准，然后添加奖励屏蔽的跨透明镜，并具有在上方的策略颁奖典礼术语中，保留无损的单个模型部署。在Spec-Bench上，DVI获得了$ 2.16 \ times $ $ wall Time的加速，与SOTA方法相当，例如Eagle-2，而培训的数据较少订单较少，并且消融表明DVI优于KL仅在线蒸馏。 DVI证明了\ emph {训练 - 意识}自我规范可以通过最小的训练开销来提供最先进的无损速度。</li>
</ul>

<h3>Title: Adversarial Reinforcement Learning for Large Language Model Agent Safety</h3>
<ul>
<li><strong>Authors: </strong>Zizhao Wang, Dingcheng Li, Vaishakh Keshava, Phillip Wallis, Ananth Balashankar, Peter Stone, Lukas Rutishauser</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05442">https://arxiv.org/abs/2510.05442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05442">https://arxiv.org/pdf/2510.05442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05442]] Adversarial Reinforcement Learning for Large Language Model Agent Safety(https://arxiv.org/abs/2510.05442)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) agents can leverage tools such as Google Search to complete complex tasks. However, this tool usage introduces the risk of indirect prompt injections, where malicious instructions hidden in tool outputs can manipulate the agent, posing security risks like data leakage. Current defense strategies typically rely on fine-tuning LLM agents on datasets of known attacks. However, the generation of these datasets relies on manually crafted attack patterns, which limits their diversity and leaves agents vulnerable to novel prompt injections. To address this limitation, we propose Adversarial Reinforcement Learning for Agent Safety (ARLAS), a novel framework that leverages adversarial reinforcement learning (RL) by formulating the problem as a two-player zero-sum game. ARLAS co-trains two LLMs: an attacker that learns to autonomously generate diverse prompt injections and an agent that learns to defend against them while completing its assigned tasks. To ensure robustness against a wide range of attacks and to prevent cyclic learning, we employ a population-based learning framework that trains the agent to defend against all previous attacker checkpoints. Evaluated on BrowserGym and AgentDojo, agents fine-tuned with ARLAS achieve a significantly lower attack success rate than the original model while also improving their task success rate. Our analysis further confirms that the adversarial process generates a diverse and challenging set of attacks, leading to a more robust agent compared to the base model.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 代理可以利用 Google 搜索等工具来完成复杂的任务。然而，这种工具的使用引入了间接提示注入的风险，其中隐藏在工具输出中的恶意指令可以操纵代理，从而带来数据泄露等安全风险。当前的防御策略通常依赖于在已知攻击的数据集上微调 LLM 代理。然而，这些数据集的生成依赖于手动设计的攻击模式，这限制了它们的多样性，并使代理容易受到新的提示注入的影响。为了解决这一限制，我们提出了代理安全对抗强化学习（ARLAS），这是一种新颖的框架，通过将问题表述为两人零和游戏来利用对抗强化学习（RL）。 ARLAS 共同训练两名 LLM：攻击者学习自动生成不同的提示注入，代理学习在完成指定任务的同时防御它们。为了确保针对各种攻击的鲁棒性并防止循环学习，我们采用基于群体的学习框架来训练代理防御所有先前的攻击者检查点。在 BrowserGym 和 AgentDojo 上进行评估，使用 ARLAS 进行微调的代理的攻击成功率明显低于原始模型，同时也提高了任务成功率。我们的分析进一步证实，对抗过程会产生一系列多样化且具有挑战性的攻击，从而产生比基本模型更强大的代理。</li>
</ul>

<h3>Title: AMAQ: Adaptive Mixed-bit Activation Quantization for Collaborative Parameter Efficient Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Yurun Song, Zhuoyi Yang, Ian G. Harris, Sangeetha Abdu Jyothi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05468">https://arxiv.org/abs/2510.05468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05468">https://arxiv.org/pdf/2510.05468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05468]] AMAQ: Adaptive Mixed-bit Activation Quantization for Collaborative Parameter Efficient Fine-tuning(https://arxiv.org/abs/2510.05468)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are scaling rapidly, creating significant challenges for collaborative server client distributed training, particularly in terms of communication efficiency and computational overheads. To address these challenges, we implement Parameter-efficient Split Learning, which effectively balances efficiency and performance for collaborative training on low-resource devices. To reduce communication overhead in collaborative training, we introduce Adaptive Mixed bit Activation Quantization (AMAQ), a strategy that progressively compresses activations and gradients from high precision (6 to 8 bits) to low precision (3 to 4 bits). AMAQ achieves this by effectively allocating bit budgets across channels based on feature wise and layer wise importance using bit regularization. Under the same bit budgets, AMAQ outperforms fixed-precision approaches, delivering about 2.5% higher generation accuracy and about 1.3% better classification accuracy for models like LLaMA3 8B and Qwen2.5 7B. In addition, it significantly enhances training stability and reducing ultra-low bit representation collapse during the training. Experiments demonstrate that AMAQ integrates effectively into practical multi-machine collaborative training setups, offering superior inference accuracy with only a modest communication overhead for bits adaptation during training. This trade off makes AMAQ a practical and effective solution for collaborative training with minimal communication cost.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）正在迅速扩展，为协作服务器客户端分布式培训带来了重大挑战，尤其是在沟通效率和计算开销方面。为了应对这些挑战，我们实施了参数效率的分裂学习，这可以有效地平衡低资源设备的协作培训的效率和绩效。为了减少协作培训中的交流开销，我们引入了自适应混合钻头激活量化（AMAQ），该策略逐渐压缩了从高精度（6到8位）到低精度（3至4位）的激活和梯度。 AMAQ通过基于特征明智的跨渠道分配位预算，并使用位正规化的重要性来实现这一目标。在相同的位预算下，AMAQ的表现优于固定精确的方法，对于Llama3 8B和QWEN2.5 7B等模型，生成准确性提高了约2.5％，分类精度提高了约1.3％。此外，它可以显着提高训练稳定性并减少训练期间的超低位表示崩溃。实验表明，AMAQ有效地集成在实用的多机械协作培训设置中，从而提供了卓越的推理准确性，并且在培训期间仅提供适度的沟通开销。这种权衡使AMAQ成为实用有效的解决方案，用于以最低的沟通成本进行协作培训。</li>
</ul>

<h3>Title: High-Fidelity Synthetic ECG Generation via Mel-Spectrogram Informed Diffusion Training</h3>
<ul>
<li><strong>Authors: </strong>Zhuoyi Huang, Nutan Sahoo, Anamika Kumari, Girish Kumar, Kexuan Cai, Shixing Cao, Yue Kang, Tian Xia, Somya Chatterjee, Nicholas Hausman, Aidan Jay, Eric S. Rosenthal, Soundar Srinivasan, Sadid Hasan, Alex Fedorov, Sulaiman Vesal, Soundar Srinivasan, Sadid Hasan, Alex Fedorov, Sulaiman Vesal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05492">https://arxiv.org/abs/2510.05492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05492">https://arxiv.org/pdf/2510.05492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05492]] High-Fidelity Synthetic ECG Generation via Mel-Spectrogram Informed Diffusion Training(https://arxiv.org/abs/2510.05492)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The development of machine learning for cardiac care is severely hampered by privacy restrictions on sharing real patient electrocardiogram (ECG) data. Although generative AI offers a promising solution, the real-world use of existing model-synthesized ECGs is limited by persistent gaps in trustworthiness and clinical utility. In this work, we address two major shortcomings of current generative ECG methods: insufficient morphological fidelity and the inability to generate personalized, patient-specific physiological signals. To address these gaps, we build on a conditional diffusion-based Structured State Space Model (SSSD-ECG) with two principled innovations: (1) MIDT-ECG (Mel-Spectrogram Informed Diffusion Training), a novel training paradigm with time-frequency domain supervision to enforce physiological structural realism, and (2) multi-modal demographic conditioning to enable patient-specific synthesis. We comprehensively evaluate our approach on the PTB-XL dataset, assessing the synthesized ECG signals on fidelity, clinical coherence, privacy preservation, and downstream task utility. MIDT-ECG achieves substantial gains: it improves morphological coherence, preserves strong privacy guarantees with all metrics evaluated exceeding the baseline by 4-8%, and notably reduces the interlead correlation error by an average of 74%, while demographic conditioning enhances signal-to-noise ratio and personalization. In critical low-data regimes, a classifier trained on datasets supplemented with our synthetic ECGs achieves performance comparable to a classifier trained solely on real data. Together, we demonstrate that ECG synthesizers, trained with the proposed time-frequency structural regularization scheme, can serve as personalized, high-fidelity, privacy-preserving surrogates when real data are scarce, advancing the responsible use of generative AI in healthcare.</li>
<li><strong>摘要：</strong>对于共享实际患者心电图（ECG）数据的隐私限制，心脏护理的机器学习开发受到严重阻碍。尽管生成的AI提供了有希望的解决方案，但现实世界中现有模型合成的ECG的使用受到可信度和临床实用性的持续差距的限制。在这项工作中，我们解决了当前生成性心电图方法的两个主要缺点：形态保真度不足和无法产生个性化的，特定于患者的生理信号。为了解决这些差距，我们建立在有条件的基于有条件的结构化状态空间模型（SSSD-ECG）的基础上，并具有两个原则上的创新：（1）MIDT-ECG（MEL-SPECTROGRAM知情扩散培训），这是一种新颖的培训范式，具有时间频率污染的训练范式，以实现了实时的结构性现实主义，并强制性地构成了（2）多数模型，以实现物理结构现实的状态。我们在PTB-XL数据集上全面评估了我们的方法，评估了有关忠诚度，临床连贯性，隐私保护和下游任务实用程序的综合ECG信号。 MIDT-ECG取得了可观的增长：它提高了形态的连贯性，保留了强大的隐私保证，所有评估的指标都超过基线的指标，并尤其将Interlead相关性误差降低74％，而人口统计学调节可以增强信号差异的比例和个性化。在关键的低数据制度中，在补充我们的合成ECGS的数据集上培训的分类器可实现与仅在实际数据上训练的分类器相当的性能。我们共同证明，经过拟议的时频结构正规化方案培训的ECG合成器可以作为个性化，高保真，具有隐私性的代理人，当真实数据稀缺，推动负责任地使用生成AI在医疗保健中。</li>
</ul>

<h3>Title: Be Tangential to Manifold: Discovering Riemannian Metric for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Shinnosuke Saito, Takashi Matsubara</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05509">https://arxiv.org/abs/2510.05509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05509">https://arxiv.org/pdf/2510.05509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05509]] Be Tangential to Manifold: Discovering Riemannian Metric for Diffusion Models(https://arxiv.org/abs/2510.05509)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion models are powerful deep generative models (DGMs) that generate high-fidelity, diverse content. However, unlike classical DGMs, they lack an explicit, tractable low-dimensional latent space that parameterizes the data manifold. This absence limits manifold-aware analysis and operations, such as interpolation and editing. Existing interpolation methods for diffusion models typically follow paths through high-density regions, which are not necessarily aligned with the data manifold and can yield perceptually unnatural transitions. To exploit the data manifold learned by diffusion models, we propose a novel Riemannian metric on the noise space, inspired by recent findings that the Jacobian of the score function captures the tangent spaces to the local data manifold. This metric encourages geodesics in the noise space to stay within or run parallel to the learned data manifold. Experiments on image interpolation show that our metric produces perceptually more natural and faithful transitions than existing density-based and naive baselines.</li>
<li><strong>摘要：</strong>扩散模型是强大的深层生成模型（DGM），产生高保真性，多样化的内容。但是，与经典的DGM不同，它们缺乏明确的，可处理的低维潜在空间，该空间参数化了数据歧管。这种缺乏限制了流动性分析和操作，例如插值和编辑。扩散模型的现有插值方法通常遵循高密度区域的路径，这些路径不一定与数据歧管保持一致，并且可以产生感知上不自然的过渡。为了利用扩散模型学到的数据歧管，我们在噪声空间上提出了一个新型的Riemannian指标，这是受分数函数的雅各布式捕获局部数据歧管的切线空间的最新发现的启发。该指标鼓励在噪声空间中的测量学保持与学习的数据歧管平行或平行运行。图像插值的实验表明，与现有的基于密度和天真的基层相比，我们的度量可以在感知上产生更自然和忠实的过渡。</li>
</ul>

<h3>Title: Teamwork: Collaborative Diffusion with Low-rank Coordination and Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Sam Sartor, Pieter Peers</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05532">https://arxiv.org/abs/2510.05532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05532">https://arxiv.org/pdf/2510.05532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05532]] Teamwork: Collaborative Diffusion with Low-rank Coordination and Adaptation(https://arxiv.org/abs/2510.05532)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large pretrained diffusion models can provide strong priors beneficial for many graphics applications. However, generative applications such as neural rendering and inverse methods such as SVBRDF estimation and intrinsic image decomposition require additional input or output channels. Current solutions for channel expansion are often application specific and these solutions can be difficult to adapt to different diffusion models or new tasks. This paper introduces Teamwork: a flexible and efficient unified solution for jointly increasing the number of input and output channels as well as adapting a pretrained diffusion model to new tasks. Teamwork achieves channel expansion without altering the pretrained diffusion model architecture by coordinating and adapting multiple instances of the base diffusion model (\ie, teammates). We employ a novel variation of Low Rank-Adaptation (LoRA) to jointly address both adaptation and coordination between the different teammates. Furthermore Teamwork supports dynamic (de)activation of teammates. We demonstrate the flexibility and efficiency of Teamwork on a variety of generative and inverse graphics tasks such as inpainting, single image SVBRDF estimation, intrinsic decomposition, neural shading, and intrinsic image synthesis.</li>
<li><strong>摘要：</strong>大型预处理的扩散模型可以为许多图形应用提供强大的先验。但是，诸如神经渲染和诸如SVBRDF估计和固有图像分解之类的生成应用需要其他输入或输出通道。当前的通道扩展解决方案通常是针对应用程序的，这些解决方案可能很难适应不同的扩散模型或新任务。本文介绍了团队合作：一种灵活，有效的统一解决方案，用于共同增加输入和输出渠道的数量，并将预算扩散模型调整为新任务。团队合作可以通过协调和调整基本扩散模型的多个实例（\ ie，队友）来实现渠道扩展。我们采用低级适应（LORA）的新型变化，以共同解决不同队友之间的适应和协调。此外，团队合作支持队友的动态激活。我们演示了团队合作在各种生成和反图形任务上的灵活性和效率，例如介入，单图SVBRDF估计，内在分解，神经阴影和固有图像合成。</li>
</ul>

<h3>Title: Permutation-Invariant Representation Learning for Robust and Privacy-Preserving Feature Selection</h3>
<ul>
<li><strong>Authors: </strong>Rui Liu, Tao Zhe, Yanjie Fu, Feng Xia, Ted Senator, Dongjie Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05535">https://arxiv.org/abs/2510.05535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05535">https://arxiv.org/pdf/2510.05535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05535]] Permutation-Invariant Representation Learning for Robust and Privacy-Preserving Feature Selection(https://arxiv.org/abs/2510.05535)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Feature selection eliminates redundancy among features to improve downstream task performance while reducing computational overhead. Existing methods often struggle to capture intricate feature interactions and adapt across diverse application scenarios. Recent advances employ generative intelligence to alleviate these drawbacks. However, these methods remain constrained by permutation sensitivity in embedding and reliance on convexity assumptions in gradient-based search. To address these limitations, our initial work introduces a novel framework that integrates permutation-invariant embedding with policy-guided search. Although effective, it still left opportunities to adapt to realistic distributed scenarios. In practice, data across local clients is highly imbalanced, heterogeneous and constrained by strict privacy regulations, limiting direct sharing. These challenges highlight the need for a framework that can integrate feature selection knowledge across clients without exposing sensitive information. In this extended journal version, we advance the framework from two perspectives: 1) developing a privacy-preserving knowledge fusion strategy to derive a unified representation space without sharing sensitive raw data. 2) incorporating a sample-aware weighting strategy to address distributional imbalance among heterogeneous local clients. Extensive experiments validate the effectiveness, robustness, and efficiency of our framework. The results further demonstrate its strong generalization ability in federated learning scenarios. The code and data are publicly available: this https URL.</li>
<li><strong>摘要：</strong>功能选择消除了功能之间的冗余，以改善下游任务性能，同时减少计算开销。现有的方法通常难以捕获复杂的特征互动并在各种应用程序场景中适应。最近的进步采用生成情报来减轻这些缺点。但是，这些方法在基于梯度的搜索中嵌入和依赖凸度假设中的置换敏感性仍受到限制。为了解决这些局限性，我们的初始工作介绍了一个新颖的框架，该框架将置换不变的嵌入与政策指导的搜索集成在一起。尽管有效，但它仍然留下了适应现实的分布式场景的机会。实际上，本地客户的数据高度不平衡，异构性和受严格隐私法规的约束，从而限制了直接共享。这些挑战强调了一个框架的需求，该框架可以在不暴露敏感信息的情况下整合跨客户的特征选择知识。在此扩展期刊版本中，我们从两个角度提高了框架：1）制定保护隐私的知识融合策略，以在不共享敏感原始数据的情况下得出统一的表示空间。 2）结合了一种样本感知的加权策略，以解决异质本地客户之间的分配失衡。广泛的实验验证了我们框架的有效性，鲁棒性和效率。结果进一步证明了其在联合学习方案中的强大概括能力。代码和数据公开可用：此HTTPS URL。</li>
</ul>

<h3>Title: HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video</h3>
<ul>
<li><strong>Authors: </strong>Hongchi Xia, Chih-Hao Lin, Hao-Yu Hsu, Quentin Leboutet, Katelyn Gao, Michael Paulitsch, Benjamin Ummenhofer, Shenlong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05560">https://arxiv.org/abs/2510.05560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05560">https://arxiv.org/pdf/2510.05560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05560]] HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video(https://arxiv.org/abs/2510.05560)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Digitizing the physical world into accurate simulation-ready virtual environments offers significant opportunities in a variety of fields such as augmented and virtual reality, gaming, and robotics. However, current 3D reconstruction and scene-understanding methods commonly fall short in one or more critical aspects, such as geometry completeness, object interactivity, physical plausibility, photorealistic rendering, or realistic physical properties for reliable dynamic simulation. To address these limitations, we introduce HoloScene, a novel interactive 3D reconstruction framework that simultaneously achieves these requirements. HoloScene leverages a comprehensive interactive scene-graph representation, encoding object geometry, appearance, and physical properties alongside hierarchical and inter-object relationships. Reconstruction is formulated as an energy-based optimization problem, integrating observational data, physical constraints, and generative priors into a unified, coherent objective. Optimization is efficiently performed via a hybrid approach combining sampling-based exploration with gradient-based refinement. The resulting digital twins exhibit complete and precise geometry, physical stability, and realistic rendering from novel viewpoints. Evaluations conducted on multiple benchmark datasets demonstrate superior performance, while practical use-cases in interactive gaming and real-time digital-twin manipulation illustrate HoloScene's broad applicability and effectiveness. Project page: this https URL.</li>
<li><strong>摘要：</strong>将物理世界数字化为精确的模拟就绪虚拟环境，为增强现实和虚拟现实、游戏和机器人等各个领域提供了巨大的机遇。然而，当前的 3D 重建和场景理解方法通常在一个或多个关键方面存在不足，例如几何完整性、对象交互性、物理合理性、照片级真实感渲染或用于可靠动态模拟的真实物理属性。为了解决这些限制，我们引入了 HoloScene，一种新颖的交互式 3D 重建框架，可以同时满足这些要求。 HoloScene 利用全面的交互式场景图表示，对对象几何形状、外观和物理属性以及层次结构和对象间关系进行编码。重建被表述为基于能量的优化问题，将观测数据、物理约束和生成先验集成到一个统一的、连贯的目标中。通过将基于采样的探索与基于梯度的细化相结合的混合方法可以有效地执行优化。由此产生的数字孪生从新颖的角度展示了完整而精确的几何形状、物理稳定性和真实渲染。对多个基准数据集进行的评估表明了其卓越的性能，而交互式游戏和实时数字孪生操作中的实际用例则说明了 HoloScene 的广泛适用性和有效性。项目页面：此 https URL。</li>
</ul>

<h3>Title: Generative Dynamic Graph Representation Learning for Conspiracy Spoofing Detection</h3>
<ul>
<li><strong>Authors: </strong>Sheng Xiang, Yidong Jiang, Yunting Chen, Dawei Cheng, Guoping Zhao, Changjun Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05562">https://arxiv.org/abs/2510.05562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05562">https://arxiv.org/pdf/2510.05562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05562]] Generative Dynamic Graph Representation Learning for Conspiracy Spoofing Detection(https://arxiv.org/abs/2510.05562)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Spoofing detection in financial trading is crucial, especially for identifying complex behaviors such as conspiracy spoofing. Traditional machine-learning approaches primarily focus on isolated node features, often overlooking the broader context of interconnected nodes. Graph-based techniques, particularly Graph Neural Networks (GNNs), have advanced the field by leveraging relational information effectively. However, in real-world spoofing detection datasets, trading behaviors exhibit dynamic, irregular patterns. Existing spoofing detection methods, though effective in some scenarios, struggle to capture the complexity of dynamic and diverse, evolving inter-node relationships. To address these challenges, we propose a novel framework called the Generative Dynamic Graph Model (GDGM), which models dynamic trading behaviors and the relationships among nodes to learn representations for conspiracy spoofing detection. Specifically, our approach incorporates the generative dynamic latent space to capture the temporal patterns and evolving market conditions. Raw trading data is first converted into time-stamped sequences. Then we model trading behaviors using the neural ordinary differential equations and gated recurrent units, to generate the representation incorporating temporal dynamics of spoofing patterns. Furthermore, pseudo-label generation and heterogeneous aggregation techniques are employed to gather relevant information and enhance the detection performance for conspiratorial spoofing behaviors. Experiments conducted on spoofing detection datasets demonstrate that our approach outperforms state-of-the-art models in detection accuracy. Additionally, our spoofing detection system has been successfully deployed in one of the largest global trading markets, further validating the practical applicability and performance of the proposed method.</li>
<li><strong>摘要：</strong>金融交易中的欺骗检测至关重要，尤其是确定诸如阴谋欺骗之类的复杂行为。传统的机器学习方法主要集中在孤立的节点特征上，通常忽略了更广泛的互连节点的环境。基于图形的技术，尤其是图形神经网络（GNN），通过有效利用关系信息来推进该领域。但是，在现实世界中的欺骗检测数据集中，交易行为表现出动态的，不规则的模式。现有的欺骗检测方法虽然在某些情况下有效，但仍在努力捕获动态和多样化的复杂性，从而不断发展的节点之间的关系。为了应对这些挑战，我们提出了一个名为“生成动态图”模型（GDGM）的新颖框架，该框架对动态交易行为进行建模以及节点之间的关系，以学习对阴谋欺骗检测的表示表示。具体而言，我们的方法结合了生成动态的潜在空间，以捕获时间模式和不断发展的市场条件。原始交易数据首先将其转换为时标的序列。然后，我们使用神经普通微分方程和门控复发单元对交易行为进行建模，以生成结合欺骗模式的时间动态的表示形式。此外，采用伪标签生成和异质聚合技术来收集相关信息并提高阴谋欺骗行为的检测性能。对欺骗检测数据集进行的实验表明，我们的方法在检测准确性方面的表现优于最先进的模型。此外，我们的欺骗检测系统已成功地部署在最大的全球贸易市场之一中，进一步验证了所提出方法的实际适用性和性能。</li>
</ul>

<h3>Title: Efficient Learning-based Graph Simulation for Temporal Graphs</h3>
<ul>
<li><strong>Authors: </strong>Sheng Xiang, Chenhao Xu, Dawei Cheng, Xiaoyang Wang, Ying Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05569">https://arxiv.org/abs/2510.05569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05569">https://arxiv.org/pdf/2510.05569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05569]] Efficient Learning-based Graph Simulation for Temporal Graphs(https://arxiv.org/abs/2510.05569)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Graph simulation has recently received a surge of attention in graph processing and analytics. In real-life applications, e.g. social science, biology, and chemistry, many graphs are composed of a series of evolving graphs (i.e., temporal graphs). While most of the existing graph generators focus on static graphs, the temporal information of the graphs is ignored. In this paper, we focus on simulating temporal graphs, which aim to reproduce the structural and temporal properties of the observed real-life temporal graphs. In this paper, we first give an overview of the existing temporal graph generators, including recently emerged learning-based approaches. Most of these learning-based methods suffer from one of the limitations: low efficiency in training or slow generating, especially for temporal random walk-based methods. Therefore, we propose an efficient learning-based approach to generate graph snapshots, namely temporal graph autoencoder (TGAE). Specifically, we propose an attention-based graph encoder to encode temporal and structural characteristics on sampled ego-graphs. And we proposed an ego-graph decoder that can achieve a good trade-off between simulation quality and efficiency in temporal graph generation. Finally, the experimental evaluation is conducted among our proposed TGAE and representative temporal graph generators on real-life temporal graphs and synthesized graphs. It is reported that our proposed approach outperforms the state-of-the-art temporal graph generators by means of simulation quality and efficiency.</li>
<li><strong>摘要：</strong>Graph Simulation最近在图形处理和分析方面引起了人们的关注。在现实生活中，例如社会科学，生物学和化学，许多图由一系列不断发展的图（即时间​​图）组成。尽管大多数现有的图形生成器都集中在静态图上，但图形的时间信息被忽略了。在本文中，我们着重于模拟时间表，该图旨在重现观察到的现实生活图表的结构和时间特性。在本文中，我们首先概述了现有的时间图生成器，包括最近基于学习的方法。这些基于学习的方法中的大多数都受到局限性之一：训练或缓慢产生的效率低，尤其是对于时间随机步行的方法。因此，我们提出了一种有效的基于学习的方法来生成图形快照，即时间图自动编码器（TGAE）。具体而言，我们提出了一个基于注意力的图形编码器，以编码采样的自我图形上的时间和结构特征。我们提出了一个自我绘图解码器，可以在时间图生成中实现良好的模拟质量和效率之间的良好权衡。最后，实验评估是在我们提出的TGAE和代表性的时间图生成器中进行的，对现实生活图和合成图。据报道，我们提出的方法通过模拟质量和效率优于最先进的时间图生成器。</li>
</ul>

<h3>Title: Power Mechanism: Private Tabular Representation Release for Model Agnostic Consumption</h3>
<ul>
<li><strong>Authors: </strong>Praneeth Vepakomma, Kaustubh Ponkshe</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05581">https://arxiv.org/abs/2510.05581</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05581">https://arxiv.org/pdf/2510.05581</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05581]] Power Mechanism: Private Tabular Representation Release for Model Agnostic Consumption(https://arxiv.org/abs/2510.05581)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Traditional collaborative learning approaches are based on sharing of model weights between clients and a server. However, there are advantages to resource efficiency through schemes based on sharing of embeddings (activations) created from the data. Several differentially private methods were developed for sharing of weights while such mechanisms do not exist so far for sharing of embeddings. We propose Ours to learn a privacy encoding network in conjunction with a small utility generation network such that the final embeddings generated from it are equipped with formal differential privacy guarantees. These privatized embeddings are then shared with a more powerful server, that learns a post-processing that results in a higher accuracy for machine learning tasks. We show that our co-design of collaborative and private learning results in requiring only one round of privatized communication and lesser compute on the client than traditional methods. The privatized embeddings that we share from the client are agnostic to the type of model (deep learning, random forests or XGBoost) used on the server in order to process these activations to complete a task.</li>
<li><strong>摘要：</strong>传统的协作学习方法基于客户端和服务器之间共享模型权重。然而，通过基于共享数据创建的嵌入（激活）的方案，可以提高资源效率。人们开发了几种不同的私有方法来共享权重，但迄今为止还不存在用于共享嵌入的机制。我们建议我们学习一个与小型效用生成网络结合的隐私编码网络，以便由它生成的最终嵌入配备正式的差分隐私保证。然后，这些私有化嵌入会与更强大的服务器共享，该服务器会学习后处理，从而提高机器学习任务的准确性。我们表明，与传统方法相比，我们对协作和私人学习的共同设计只需要一轮私有化通信，并且客户端的计算量更少。我们从客户端共享的私有化嵌入与服务器上用于处理这些激活以完成任务的模型类型（深度学习、随机森林或 XGBoost）无关。</li>
</ul>

<h3>Title: Improving Chain-of-Thought Efficiency for Autoregressive Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Zeqi Gu, Markos Georgopoulos, Xiaoliang Dai, Marjan Ghazvininejad, Chu Wang, Felix Juefei-Xu, Kunpeng Li, Yujun Shi, Zecheng He, Zijian He, Jiawei Zhou, Abe Davis, Jialiang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05593">https://arxiv.org/abs/2510.05593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05593">https://arxiv.org/pdf/2510.05593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05593]] Improving Chain-of-Thought Efficiency for Autoregressive Image Generation(https://arxiv.org/abs/2510.05593)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Autoregressive multimodal large language models have recently gained popularity for image generation, driven by advances in foundation models. To enhance alignment and detail, newer approaches employ chain-of-thought (CoT) reasoning, expanding user inputs into elaborated prompts prior to image synthesis. However, this strategy can introduce unnecessary redundancy -- a phenomenon we call visual overthinking -- which increases computational costs and can introduce details that contradict the original prompt. In this work, we explore how to generate more concise CoT sequences for more efficient image generation. We introduce ShortCoTI, a lightweight optimization framework that encourages more concise CoT while preserving output image quality. ShortCoTI rewards more concise prompts with an adaptive function that scales according to an estimated difficulty for each task. Incorporating this reward into a reinforcement learning paradigm reduces prompt reasoning length by 54% while maintaining or slightly improving quality metrics across multiple benchmarks (T2I-CompBench, GenEval). Qualitative analysis shows that our method eliminates verbose explanations and repetitive refinements, producing reasoning prompts that are both concise and semantically rich. As a result, ShortCoTI improves computational efficiency without compromising the fidelity or visual appeal of generated images.</li>
<li><strong>摘要：</strong>在基础模型进步的推动下，自回归多模态大语言模型最近在图像生成领域越来越受欢迎。为了增强对齐和细节，较新的方法采用了思维链 (CoT) 推理，在图像合成之前将用户输入扩展为详细的提示。然而，这种策略可能会引入不必要的冗余——我们称之为视觉过度思考的现象——这会增加计算成本，并可能引入与原始提示相矛盾的细节。在这项工作中，我们探索如何生成更简洁的 CoT 序列以更有效地生成图像。我们引入了 ShortCoTI，这是一个轻量级优化框架，它鼓励更简洁的 CoT，同时保持输出图像质量。 ShortCoTI 通过自适应功能奖励更简洁的提示，该功能根据每个任务的估计难度进行缩放。将此奖励纳入强化学习范式可将即时推理长度缩短 54%，同时在多个基准（T2I-CompBench、GenEval）上保持或略微提高质量指标。定性分析表明，我们的方法消除了冗长的解释和重复的细化，产生了简洁且语义丰富的推理提示。因此，ShortCoTI 提高了计算效率，同时又不影响生成图像的保真度或视觉吸引力。</li>
</ul>

<h3>Title: Efficient Conditional Generation on Scale-based Visual Autoregressive Models</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Liu, Tao Huang, Chang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05610">https://arxiv.org/abs/2510.05610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05610">https://arxiv.org/pdf/2510.05610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05610]] Efficient Conditional Generation on Scale-based Visual Autoregressive Models(https://arxiv.org/abs/2510.05610)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in autoregressive (AR) models have demonstrated their potential to rival diffusion models in image synthesis. However, for complex spatially-conditioned generation, current AR approaches rely on fine-tuning the pre-trained model, leading to significant training costs. In this paper, we propose the Efficient Control Model (ECM), a plug-and-play framework featuring a lightweight control module that introduces control signals via a distributed architecture. This architecture consists of context-aware attention layers that refine conditional features using real-time generated tokens, and a shared gated feed-forward network (FFN) designed to maximize the utilization of its limited capacity and ensure coherent control feature learning. Furthermore, recognizing the critical role of early-stage generation in determining semantic structure, we introduce an early-centric sampling strategy that prioritizes learning early control sequences. This approach reduces computational cost by lowering the number of training tokens per iteration, while a complementary temperature scheduling during inference compensates for the resulting insufficient training of late-stage tokens. Extensive experiments on scale-based AR models validate that our method achieves high-fidelity and diverse control over image generation, surpassing existing baselines while significantly improving both training and inference efficiency.</li>
<li><strong>摘要：</strong>自回归 (AR) 模型的最新进展已证明其在图像合成方面具有与扩散模型相媲美的潜力。然而，对于复杂的空间条件生成，当前的 AR 方法依赖于微调预训练模型，导致训练成本高昂。在本文中，我们提出了高效控制模型（ECM），这是一种即插即用框架，具有轻量级控制模块，通过分布式架构引入控制信号。该架构由上下文感知关注层和共享门控前馈网络（FFN）组成，前者使用实时生成的令牌来细化条件特征，后者旨在最大限度地利用其有限容量并确保连贯的控制特征学习。此外，认识到早期生成在确定语义结构中的关键作用，我们引入了一种以早期为中心的采样策略，该策略优先考虑学习早期控制序列。这种方法通过降低每次迭代的训练令牌数量来降低计算成本，而推理过程中的补充温度调度可以补偿由此产生的后期令牌训练不足。基于尺度的 AR 模型的大量实验验证了我们的方法实现了对图像生成的高保真和多样化控制，超越了现有基线，同时显着提高了训练和推理效率。</li>
</ul>

<h3>Title: PointNSP: Autoregressive 3D Point Cloud Generation with Next-Scale Level-of-Detail Prediction</h3>
<ul>
<li><strong>Authors: </strong>Ziqiao Meng, Qichao Wang, Zhiyang Dou, Zixing Song, Zhipeng Zhou, Irwin King, Peilin Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05613">https://arxiv.org/abs/2510.05613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05613">https://arxiv.org/pdf/2510.05613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05613]] PointNSP: Autoregressive 3D Point Cloud Generation with Next-Scale Level-of-Detail Prediction(https://arxiv.org/abs/2510.05613)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Autoregressive point cloud generation has long lagged behind diffusion-based approaches in quality. The performance gap stems from the fact that autoregressive models impose an artificial ordering on inherently unordered point sets, forcing shape generation to proceed as a sequence of local predictions. This sequential bias emphasizes short-range continuity but undermines the model's capacity to capture long-range dependencies, hindering its ability to enforce global structural properties such as symmetry, consistent topology, and large-scale geometric regularities. Inspired by the level-of-detail (LOD) principle in shape modeling, we propose PointNSP, a coarse-to-fine generative framework that preserves global shape structure at low resolutions and progressively refines fine-grained geometry at higher scales through a next-scale prediction paradigm. This multi-scale factorization aligns the autoregressive objective with the permutation-invariant nature of point sets, enabling rich intra-scale interactions while avoiding brittle fixed orderings. Experiments on ShapeNet show that PointNSP establishes state-of-the-art (SOTA) generation quality for the first time within the autoregressive paradigm. In addition, it surpasses strong diffusion-based baselines in parameter, training, and inference efficiency. Finally, in dense generation with 8,192 points, PointNSP's advantages become even more pronounced, underscoring its scalability potential.</li>
<li><strong>摘要：</strong>自回归点云生成在质量上长期以来落后于基于扩散的方法。性能差距源于这样一个事实：自回归模型对本质上无序的点集强加了人为排序，迫使形状生成作为一系列局部预测进行。这种顺序偏差强调短程连续性，但削弱了模型捕获长程依赖性的能力，阻碍了其强制执行全局结构属性（例如对称性、一致拓扑和大规模几何规律）的能力。受形状建模中细节层次（LOD）原理的启发，我们提出了PointNSP，这是一种从粗到细的生成框架，它在低分辨率下保留全局形状结构，并通过下一尺度的预测范式逐步细化更高尺度的细粒度几何结构。这种多尺度分解使自回归目标与点集的排列不变性质保持一致，从而实现丰富的尺度内交互，同时避免脆弱的固定排序。 ShapeNet 上的实验表明，PointNSP 首次在自回归范式中建立了最先进的 (SOTA) 生成质量。此外，它在参数、训练和推理效率方面超越了基于扩散的强大基线。最后，在具有 8,192 个点的密集生成中，PointNSP 的优势变得更加明显，凸显了其可扩展性潜力。</li>
</ul>

<h3>Title: Beyond Spectral Peaks: Interpreting the Cues Behind Synthetic Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Sara Mandelli, Diego Vila-Portela, David Vázquez-Padín, Paolo Bestagini, Fernando Pérez-González</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05633">https://arxiv.org/abs/2510.05633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05633">https://arxiv.org/pdf/2510.05633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05633]] Beyond Spectral Peaks: Interpreting the Cues Behind Synthetic Image Detection(https://arxiv.org/abs/2510.05633)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Over the years, the forensics community has proposed several deep learning-based detectors to mitigate the risks of generative AI. Recently, frequency-domain artifacts (particularly periodic peaks in the magnitude spectrum), have received significant attention, as they have been often considered a strong indicator of synthetic image generation. However, state-of-the-art detectors are typically used as black-boxes, and it still remains unclear whether they truly rely on these peaks. This limits their interpretability and trust. In this work, we conduct a systematic study to address this question. We propose a strategy to remove spectral peaks from images and analyze the impact of this operation on several detectors. In addition, we introduce a simple linear detector that relies exclusively on frequency peaks, providing a fully interpretable baseline free from the confounding influence of deep learning. Our findings reveal that most detectors are not fundamentally dependent on spectral peaks, challenging a widespread assumption in the field and paving the way for more transparent and reliable forensic tools.</li>
<li><strong>摘要：</strong>多年来，法医社区提出了几个基于深度学习的探测器，以减轻生成AI的风险。最近，频域伪像（尤其是大小频谱中的周期性峰）受到了极大的关注，因为它们通常被认为是合成图像产生的有力指标。但是，最先进的探测器通常用作黑盒，仍然不清楚它们是否真正依赖这些峰。这限制了他们的解释性和信任。在这项工作中，我们进行了一项系统的研究来解决这个问题。我们提出了一种从图像中删除光谱峰并分析该操作对多个检测器的影响的策略。此外，我们引入了一个简单的线性检测器，该检测器仅依赖于频率峰，从而使基线完全可解释，而没有深度学习的混杂影响。我们的发现表明，大多数探测器从根本上取决于光谱峰，这挑战了该领域的广泛假设，并为更透明和可靠的法医工具铺平了道路。</li>
</ul>

<h3>Title: When and How to Cut Classical Concerts? A Multimodal Automated Video Editing Approach</h3>
<ul>
<li><strong>Authors: </strong>Daniel Gonzálbez-Biosca, Josep Cabacas-Maso, Carles Ventura, Ismael Benito-Altamirano</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05661">https://arxiv.org/abs/2510.05661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05661">https://arxiv.org/pdf/2510.05661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05661]] When and How to Cut Classical Concerts? A Multimodal Automated Video Editing Approach(https://arxiv.org/abs/2510.05661)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Automated video editing remains an underexplored task in the computer vision and multimedia domains, especially when contrasted with the growing interest in video generation and scene understanding. In this work, we address the specific challenge of editing multicamera recordings of classical music concerts by decomposing the problem into two key sub-tasks: when to cut and how to cut. Building on recent literature, we propose a novel multimodal architecture for the temporal segmentation task (when to cut), which integrates log-mel spectrograms from the audio signals, plus an optional image embedding, and scalar temporal features through a lightweight convolutional-transformer pipeline. For the spatial selection task (how to cut), we improve the literature by updating from old backbones, e.g. ResNet, with a CLIP-based encoder and constraining distractor selection to segments from the same concert. Our dataset was constructed following a pseudo-labeling approach, in which raw video data was automatically clustered into coherent shot segments. We show that our models outperformed previous baselines in detecting cut points and provide competitive visual shot selection, advancing the state of the art in multimodal automated video editing.</li>
<li><strong>摘要：</strong>自动视频编辑仍然是计算机视觉和多媒体领域中一项尚未充分探索的任务，特别是与人们对视频生成和场景理解日益增长的兴趣相比。在这项工作中，我们通过将问题分解为两个关键子任务来解决编辑古典音乐会多机位录音的具体挑战：何时剪切和如何剪切。基于最近的文献，我们提出了一种用于时间分割任务（何时切割）的新颖的多模态架构，它集成了音频信号的 log-mel 频谱图，加上可选的图像嵌入，以及通过轻量级卷积变换器管道的标量时间特征。对于空间选择任务（如何切割），我们通过更新旧的主干来改进文献，例如ResNet，具有基于 CLIP 的编码器，并将干扰项选择限制为来自同一音乐会的片段。我们的数据集是按照伪标签方法构建的，其中原始视频数据自动聚类成连贯的镜头片段。我们表明，我们的模型在检测切点和提供有竞争力的视觉镜头选择方面优于以前的基线，从而推进了多模式自动视频编辑的最先进技术。</li>
</ul>

<h3>Title: vAttention: Verified Sparse Attention</h3>
<ul>
<li><strong>Authors: </strong>Aditya Desai, Kumar Krishna Agrawal, Shuo Yang, Alejandro Cuadron, Luis Gaspar Schroeder, Matei Zaharia, Joseph E. Gonzalez, Ion Stoica</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05688">https://arxiv.org/abs/2510.05688</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05688">https://arxiv.org/pdf/2510.05688</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05688]] vAttention: Verified Sparse Attention(https://arxiv.org/abs/2510.05688)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>State-of-the-art sparse attention methods for reducing decoding latency fall into two main categories: approximate top-$k$ (and its extension, top-$p$) and recently introduced sampling-based estimation. However, these approaches are fundamentally limited in their ability to approximate full attention: they fail to provide consistent approximations across heads and query vectors and, most critically, lack guarantees on approximation quality, limiting their practical deployment. We observe that top-$k$ and random sampling are complementary: top-$k$ performs well when attention scores are dominated by a few tokens, whereas random sampling provides better estimates when attention scores are relatively uniform. Building on this insight and leveraging the statistical guarantees of sampling, we introduce vAttention, the first practical sparse attention mechanism with user-specified $(\epsilon, \delta)$ guarantees on approximation accuracy (thus, verified). These guarantees make vAttention a compelling step toward practical, reliable deployment of sparse attention at scale. By unifying top-k and sampling, vAttention outperforms both individually, delivering a superior quality-efficiency trade-off. Our experiments show that vAttention significantly improves the quality of sparse attention (e.g., $\sim$4.5 percentage points for Llama-3.1-8B-Inst and Deepseek-R1-Distill-Llama-8B on RULER-HARD), and effectively bridges the gap between full and sparse attention (e.g., across datasets, it matches full model quality with upto 20x sparsity). We also demonstrate that it can be deployed in reasoning scenarios to achieve fast decoding without compromising model quality (e.g., vAttention achieves full model quality on AIME2024 at 10x sparsity with up to 32K token generations). Code is open-sourced at this https URL.</li>
<li><strong>摘要：</strong>最先进的稀疏注意方法可以减少解码潜伏期分为两个主要类别：近似$ $ k $（及其扩展名，上$ p $），最近引入了基于抽样的估计。但是，这些方法从根本上受到了近似关注的能力的限制：它们无法提供跨头和查询向量的一致近似值，并且最重要的是，对近似质量的保证缺乏保证，从而限制了其实际部署。我们观察到，当注意力评分以少数几个令牌为主时，顶部$ K $和随机抽样是互补的：Top-$ k $的表现良好，而当注意力分数相对均匀时，随机抽样提供了更好的估计。在这种见识并利用抽样的统计保证的基础上，我们引入了VATTENTION，这是第一个实用的稀疏注意机制，其用户指定的$（\ epsilon，\ delta）$保证了近似准确性（因此，已验证）。这些保证使VATTENTION成为迈出的实用，可靠地部署稀疏注意力的令人信服的一步。通过统一TOUP-K和采样，VATTENTION既优于单独的表现，又超过了质量效率较高的权衡。我们的实验表明，VATTENTION显着提高了稀疏注意力的质量（例如，$ \ sim $ 4.5个百分点的llama-3.1-8b-inst和deepSeek-r1-distill-llalama-8b在标尺上），并且有效地弥补了全部和稀疏注意力之间的差距（例如，与数据范围的差异相比，它都有20x的质量）。我们还证明，它可以在推理方案中部署，以实现快速解码而不会损害模型质量（例如，Vattention在10倍稀疏时达到AIME2024的完整模型质量，最多32K代币世代）。代码在此HTTPS URL上开源。</li>
</ul>

<h3>Title: AgeBooth: Controllable Facial Aging and Rejuvenation via Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Shihao Zhu, Bohan Cao, Ziheng Ouyang, Zhen Li, Peng-Tao Jiang, Qibin Hou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05715">https://arxiv.org/abs/2510.05715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05715">https://arxiv.org/pdf/2510.05715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05715]] AgeBooth: Controllable Facial Aging and Rejuvenation via Diffusion Models(https://arxiv.org/abs/2510.05715)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent diffusion model research focuses on generating identity-consistent images from a reference photo, but they struggle to accurately control age while preserving identity, and fine-tuning such models often requires costly paired images across ages. In this paper, we propose AgeBooth, a novel age-specific finetuning approach that can effectively enhance the age control capability of adapterbased identity personalization models without the need for expensive age-varied datasets. To reduce dependence on a large amount of age-labeled data, we exploit the linear nature of aging by introducing age-conditioned prompt blending and an age-specific LoRA fusion strategy that leverages SVDMix, a matrix fusion technique. These techniques enable high-quality generation of intermediate-age portraits. Our AgeBooth produces realistic and identity-consistent face images across different ages from a single reference image. Experiments show that AgeBooth achieves superior age control and visual quality compared to previous state-of-the-art editing-based methods.</li>
<li><strong>摘要：</strong>最近的扩散模型研究侧重于从参考照片生成身份一致的图像，但它们很难在保留身份的同时准确控制年龄，并且对此类模型进行微调通常需要昂贵的跨年龄配对图像。在本文中，我们提出了 AgeBooth，一种新颖的特定年龄微调方法，可以有效增强基于适配器的身份个性化模型的年龄控制能力，而不需要昂贵的年龄变化数据集。为了减少对大量年龄标记数据的依赖，我们通过引入年龄条件提示混合和利用 SVDMix（一种矩阵融合技术）的年龄特定 LoRA 融合策略来利用衰老的线性本质。这些技术可以生成高质量的中年肖像。我们的 AgeBooth 从单个参考图像生成不同年龄段的真实且身份一致的面部图像。实验表明，与之前最先进的基于编辑的方法相比，AgeBooth 实现了卓越的年龄控制和视觉质量。</li>
</ul>

<h3>Title: DiffSDA: Unsupervised Diffusion Sequential Disentanglement Across Modalities</h3>
<ul>
<li><strong>Authors: </strong>Hedi Zisling, Ilan Naiman, Nimrod Berman, Supasorn Suwajanakorn, Omri Azencot</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05717">https://arxiv.org/abs/2510.05717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05717">https://arxiv.org/pdf/2510.05717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05717]] DiffSDA: Unsupervised Diffusion Sequential Disentanglement Across Modalities(https://arxiv.org/abs/2510.05717)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Unsupervised representation learning, particularly sequential disentanglement, aims to separate static and dynamic factors of variation in data without relying on labels. This remains a challenging problem, as existing approaches based on variational autoencoders and generative adversarial networks often rely on multiple loss terms, complicating the optimization process. Furthermore, sequential disentanglement methods face challenges when applied to real-world data, and there is currently no established evaluation protocol for assessing their performance in such settings. Recently, diffusion models have emerged as state-of-the-art generative models, but no theoretical formalization exists for their application to sequential disentanglement. In this work, we introduce the Diffusion Sequential Disentanglement Autoencoder (DiffSDA), a novel, modal-agnostic framework effective across diverse real-world data modalities, including time series, video, and audio. DiffSDA leverages a new probabilistic modeling, latent diffusion, and efficient samplers, while incorporating a challenging evaluation protocol for rigorous testing. Our experiments on diverse real-world benchmarks demonstrate that DiffSDA outperforms recent state-of-the-art methods in sequential disentanglement.</li>
<li><strong>摘要：</strong>无监督表示学习，特别是顺序解缠，旨在不依赖标签地分离数据变化的静态和动态因素。这仍然是一个具有挑战性的问题，因为基于变分自动编码器和生成对抗网络的现有方法通常依赖于多个损失项，使优化过程变得复杂。此外，顺序解缠方法在应用于现实世界数据时面临挑战，并且目前还没有建立的评估协议来评估其在这种情况下的性能。最近，扩散模型已成为最先进的生成模型，但尚不存在将其应用于顺序解缠的理论形式化。在这项工作中，我们介绍了扩散顺序解缠自动编码器（DiffSDA），这是一种新颖的、与模态无关的框架，可有效应对各种现实世界的数据模态，包括时间序列、视频和音频。 DiffSDA 利用新的概率建模、潜在扩散和高效采样器，同时结合具有挑战性的评估协议进行严格测试。我们对各种现实世界基准的实验表明，DiffSDA 在顺序解缠方面优于最新的最先进方法。</li>
</ul>

<h3>Title: Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect</h3>
<ul>
<li><strong>Authors: </strong>Amirtaha Amanzadi, Zahra Dehghanian, Hamid Beigy, Hamid R. Rabiee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05740">https://arxiv.org/abs/2510.05740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05740">https://arxiv.org/pdf/2510.05740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05740]] Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect(https://arxiv.org/abs/2510.05740)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid development of generative models has made it increasingly crucial to develop detectors that can reliably detect synthetic images. Although most of the work has now focused on cross-generator generalization, we argue that this viewpoint is too limited. Detecting synthetic images involves another equally important challenge: generalization across visual domains. To bridge this gap,we present the OmniGen Benchmark. This comprehensive evaluation dataset incorporates 12 state-of-the-art generators, providing a more realistic way of evaluating detector performance under realistic conditions. In addition, we introduce a new method, FusionDetect, aimed at addressing both vectors of generalization. FusionDetect draws on the benefits of two frozen foundation models: CLIP & Dinov2. By deriving features from both complementary models,we develop a cohesive feature space that naturally adapts to changes in both thecontent and design of the generator. Our extensive experiments demonstrate that FusionDetect delivers not only a new state-of-the-art, which is 3.87% more accurate than its closest competitor and 6.13% more precise on average on established benchmarks, but also achieves a 4.48% increase in accuracy on OmniGen,along with exceptional robustness to common image perturbations. We introduce not only a top-performing detector, but also a new benchmark and framework for furthering universal AI image detection. The code and dataset are available at this http URL</li>
<li><strong>摘要：</strong>生成模型的快速发展使开发可以可靠地检测合成图像的检测器变得越来越重要。尽管大多数工作现在都集中在交叉产物概括上，但我们认为这种观点太有限了。检测合成图像涉及另一个同样重要的挑战：跨视觉域的概括。为了弥合这一差距，我们提出了综合基准。这个全面的评估数据集包含12个最先进的发电机，提供了一种更现实的方法来评估检测器在现实条件下的性能。此外，我们引入了一种新方法FusionDetect，旨在解决概括的两个向量。 FusionDetect借鉴了两个冷冻基础模型的好处：Clip＆Dinov2。通过从这两个互补模型中得出功能，我们开发了一个具有内聚物的特征空间，该空间自然适应了发电机的探测器和设计的变化。我们的广泛实验表明，FusionDetect不仅提供了新的最先进的实验，它比其最接近的竞争对手更准确3.87％，在既定的基准测试中平均要精确6.13％，而且还可以提高4.48％的准确性，同时增强了Omnigen的准确性，以及与常见图像敏感性的鲁棒性。我们不仅介绍了表现最佳的检测器，还引入了一个新的基准和框架，以促进通用AI图像检测。该代码和数据集可在此HTTP URL上找到</li>
</ul>

<h3>Title: OneVision: An End-to-End Generative Framework for Multi-view E-commerce Vision Search</h3>
<ul>
<li><strong>Authors: </strong>Zexin Zheng, Huangyu Dai, Lingtao Mao, Xinyu Sun, Zihan Liang, Ben Chen, Yuqing Ding, Chenyi Lei, Wenwu Ou, Han Li, Kun Gai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05759">https://arxiv.org/abs/2510.05759</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05759">https://arxiv.org/pdf/2510.05759</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05759]] OneVision: An End-to-End Generative Framework for Multi-view E-commerce Vision Search(https://arxiv.org/abs/2510.05759)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Traditional vision search, similar to search and recommendation systems, follows the multi-stage cascading architecture (MCA) paradigm to balance efficiency and conversion. Specifically, the query image undergoes feature extraction, recall, pre-ranking, and ranking stages, ultimately presenting the user with semantically similar products that meet their preferences. This multi-view representation discrepancy of the same object in the query and the optimization objective collide across these stages, making it difficult to achieve Pareto optimality in both user experience and conversion. In this paper, an end-to-end generative framework, OneVision, is proposed to address these problems. OneVision builds on VRQ, a vision-aligned residual quantization encoding, which can align the vastly different representations of an object across multiple viewpoints while preserving the distinctive features of each product as much as possible. Then a multi-stage semantic alignment scheme is adopted to maintain strong visual similarity priors while effectively incorporating user-specific information for personalized preference generation. In offline evaluations, OneVision performs on par with online MCA, while improving inference efficiency by 21% through dynamic pruning. In A/B tests, it achieves significant online improvements: +2.15% item CTR, +2.27% CVR, and +3.12% order volume. These results demonstrate that a semantic ID centric, generative architecture can unify retrieval and personalization while simplifying the serving pathway.</li>
<li><strong>摘要：</strong>传统的视觉搜索与搜索和推荐系统类似，遵循多级级联架构（MCA）范式来平衡效率和转换。具体来说，查询图像经历特征提取、召回、预排序和排序阶段，最终向用户呈现符合其偏好的语义相似的产品。查询中同一对象的多视图表示差异与优化目标在这些阶段之间发生冲突，使得用户体验和转化都难以实现帕累托最优。在本文中，提出了一个端到端生成框架 OneVision 来解决这些问题。 OneVision 以 VRQ 为基础，VRQ 是一种视觉对齐残差量化编码，它可以跨多个视点对齐对象的截然不同的表示，同时尽可能保留每个产品的独特特征。然后采用多阶段语义对齐方案来保持强视觉相似性先验，同时有效地结合用户特定信息以生成个性化偏好。在离线评估中，OneVision 的表现与在线 MCA 相当，同时通过动态剪枝将推理效率提高了 21%。在 A/B 测试中，它实现了显着的在线改进：+2.15% 的商品点击率、+2.27% 的 CVR 和+3.12% 的订单量。这些结果表明，以语义 ID 为中心的生成架构可以统一检索和个性化，同时简化服务路径。</li>
</ul>

<h3>Title: Rasterized Steered Mixture of Experts for Efficient 2D Image Regression</h3>
<ul>
<li><strong>Authors: </strong>Yi-Hsin Li, Thomas Sikora, Sebastian Knorr, Mårten Sjöström</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05814">https://arxiv.org/abs/2510.05814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05814">https://arxiv.org/pdf/2510.05814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05814]] Rasterized Steered Mixture of Experts for Efficient 2D Image Regression(https://arxiv.org/abs/2510.05814)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>The Steered Mixture of Experts regression framework has demonstrated strong performance in image reconstruction, compression, denoising, and super-resolution. However, its high computational cost limits practical applications. This work introduces a rasterization-based optimization strategy that combines the efficiency of rasterized Gaussian kernel rendering with the edge-aware gating mechanism of the Steered Mixture of Experts. The proposed method is designed to accelerate two-dimensional image regression while maintaining the model's inherent sparsity and reconstruction quality. By replacing global iterative optimization with a rasterized formulation, the method achieves significantly faster parameter updates and more memory-efficient model representations. In addition, the proposed framework supports applications such as native super-resolution and image denoising, which are not directly achievable with standard rasterized Gaussian kernel approaches. The combination of fast rasterized optimization with the edge-aware structure of the Steered Mixture of Experts provides a new balance between computational efficiency and reconstruction fidelity for two-dimensional image processing tasks.</li>
<li><strong>摘要：</strong>专家混合引导回归框架在图像重建、压缩、去噪和超分辨率方面表现出了强大的性能。然而，其较高的计算成本限制了实际应用。这项工作引入了一种基于光栅化的优化策略，该策略将光栅化高斯内核渲染的效率与转向混合专家的边缘感知门控机制相结合。该方法旨在加速二维图像回归，同时保持模型固有的稀疏性和重建质量。通过用栅格化公式替换全局迭代优化，该方法实现了明显更快的参数更新和更节省内存的模型表示。此外，所提出的框架支持原生超分辨率和图像去噪等应用，而这些应用是标准光栅化高斯核方法无法直接实现的。快速光栅化优化与专家混合引导的边缘感知结构相结合，为二维图像处理任务的计算效率和重建保真度之间提供了新的平衡。</li>
</ul>

<h3>Title: Mitigating Premature Exploitation in Particle-based Monte Carlo for Inference-Time Scaling</h3>
<ul>
<li><strong>Authors: </strong>Giorgio Giannone, Guangxuan Xu, Nikhil Shivakumar Nayak, Rohan Mahesh Awhad, Shivchander Sudalairaj, Kai Xu, Akash Srivastava</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05825">https://arxiv.org/abs/2510.05825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05825">https://arxiv.org/pdf/2510.05825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05825]] Mitigating Premature Exploitation in Particle-based Monte Carlo for Inference-Time Scaling(https://arxiv.org/abs/2510.05825)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Inference-Time Scaling (ITS) improves language models by allocating more computation at generation time. Particle Filtering (PF) has emerged as a strong ITS method for complex mathematical reasoning tasks, but it is vulnerable when guided by process reward models, which often assign overconfident scores early in the reasoning process. This causes PF to suffer from premature exploitation: it myopically commits to locally promising trajectories, prunes potentially correct hypotheses, and converges to suboptimal solutions. This failure mode, known as particle impoverishment, is especially severe under constrained computational budgets. To address this, we analyze the problem and identify two root causes: a lack of diversity in the particle set due to overconfident resampling and consequent inability to assess the potential of a reasoning path. We introduce Entropic Particle Filtering (ePF), an algorithm that integrates two new techniques to solve these issues. The first technique, Entropic Annealing (EA), directly mitigates particle impoverishment by monitoring search diversity via entropy; when diversity drops, it intervenes by dynamically annealing the resampling distribution to preserve exploration. The second, an enhancement called Look-ahead Modulation (LaM), adds a predictive guide to evaluate a state's potential based on its successors. On several challenging math benchmarks, ePF significantly outperforms strong baselines and achieves up to a 50 % relative improvement in task reward. Together, these methods improve PF's resilience by balancing the exploration of diverse solution spaces with the exploitation of high-reward regions, ultimately leading to higher-quality solutions.</li>
<li><strong>摘要：</strong>推理时间缩放 (ITS) 通过在生成时分配更多计算来改进语言模型。粒子过滤 (PF) 已成为复杂数学推理任务的强大 ITS 方法，但在过程奖励模型的指导下它很容易受到攻击，过程奖励模型通常会在推理过程的早期分配过度自信的分数。这导致 PF 遭受过早的利用：它目光短浅地致力于局部有希望的轨迹，修剪潜在的正确假设，并收敛到次优解决方案。这种失效模式被称为粒子贫化，在计算预算有限的情况下尤其严重。为了解决这个问题，我们分析了问题并确定了两个根本原因：由于过度自信的重采样而导致粒子集缺乏多样性，以及因此无法评估推理路径的潜力。我们引入熵粒子过滤（ePF），这是一种集成了两种新技术来解决这些问题的算法。第一种技术是熵退火（EA），通过熵监控搜索多样性来直接减轻粒子贫化；当多样性下降时，它会通过动态退火重采样分布来进行干预，以保留探索。第二个是称为“前瞻调制”(LaM) 的增强功能，它添加了一个预测指南，用于根据一个国家的继任者评估其潜力。在几个具有挑战性的数学基准上，ePF 显着优于强大的基准，并在任务奖励方面实现了高达 50% 的相对改进。这些方法共同通过平衡对不同解决方案空间的探索与对高回报区域的开发来提高 PF 的弹性，最终产生更高质量的解决方案。</li>
</ul>

<h3>Title: ESS-Flow: Training-free guidance of flow-based models as inference in source space</h3>
<ul>
<li><strong>Authors: </strong>Adhithyan Kalaivanan, Zheng Zhao, Jens Sjölund, Fredrik Lindsten</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05849">https://arxiv.org/abs/2510.05849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05849">https://arxiv.org/pdf/2510.05849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05849]] ESS-Flow: Training-free guidance of flow-based models as inference in source space(https://arxiv.org/abs/2510.05849)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Guiding pretrained flow-based generative models for conditional generation or to produce samples with desired target properties enables solving diverse tasks without retraining on paired data. We present ESS-Flow, a gradient-free method that leverages the typically Gaussian prior of the source distribution in flow-based models to perform Bayesian inference directly in the source space using Elliptical Slice Sampling. ESS-Flow only requires forward passes through the generative model and observation process, no gradient or Jacobian computations, and is applicable even when gradients are unreliable or unavailable, such as with simulation-based observations or quantization in the generation or observation process. We demonstrate its effectiveness on designing materials with desired target properties and predicting protein structures from sparse inter-residue distance measurements.</li>
<li><strong>摘要：</strong>指导预训练的基于流的生成模型进行条件生成或生成具有所需目标属性的样本，可以解决各种任务，而无需对配对数据进行重新训练。我们提出了 ESS-Flow，这是一种无梯度方法，它利用基于流的模型中源分布的典型高斯先验，使用椭圆切片采样直接在源空间中执行贝叶斯推理。 ESS-Flow 仅需要前向传递生成模型和观测过程，无需梯度或雅可比计算，并且即使在梯度不可靠或不可用时也适用，例如在生成或观测过程中基于模拟的观测或量化。我们证明了其在设计具有所需目标特性的材料以及通过稀疏残基间距离测量预测蛋白质结构方面的有效性。</li>
</ul>

<h3>Title: $\bf{D^3}$QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Yanran Zhang, Bingyao Yu, Yu Zheng, Wenzhao Zheng, Yueqi Duan, Lei Chen, Jie Zhou, Jiwen Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05891">https://arxiv.org/abs/2510.05891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05891">https://arxiv.org/pdf/2510.05891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05891]] $\bf{D^3}$QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection(https://arxiv.org/abs/2510.05891)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The emergence of visual autoregressive (AR) models has revolutionized image generation while presenting new challenges for synthetic image detection. Unlike previous GAN or diffusion-based methods, AR models generate images through discrete token prediction, exhibiting both marked improvements in image synthesis quality and unique characteristics in their vector-quantized representations. In this paper, we propose to leverage Discrete Distribution Discrepancy-aware Quantization Error (D$^3$QE) for autoregressive-generated image detection that exploits the distinctive patterns and the frequency distribution bias of the codebook existing in real and fake images. We introduce a discrete distribution discrepancy-aware transformer that integrates dynamic codebook frequency statistics into its attention mechanism, fusing semantic features and quantization error latent. To evaluate our method, we construct a comprehensive dataset termed ARForensics covering 7 mainstream visual AR models. Experiments demonstrate superior detection accuracy and strong generalization of D$^3$QE across different AR models, with robustness to real-world perturbations. Code is available at \href{this https URL}{this https URL}.</li>
<li><strong>摘要：</strong>视觉自回归（AR）模型的出现彻底改变了图像生成，同时给合成图像检测带来了新的挑战。与之前的 GAN 或基于扩散的方法不同，AR 模型通过离散标记预测生成图像，在图像合成质量和矢量量化表示方面表现出显着的改进和独特的特征。在本文中，我们建议利用离散分布差异感知量化误差（D$^3$QE）进行自回归生成的图像检测，利用真实和虚假图像中存在的码本的独特模式和频率分布偏差。我们引入了一种离散分布差异感知变压器，它将动态码本频率统计集成到其注意机制中，融合语义特征和潜在量化误差。为了评估我们的方法，我们构建了一个名为 ARForensics 的综合数据集，涵盖 7 个主流视觉 AR 模型。实验证明了 D$^3$QE 在不同 AR 模型中具有卓越的检测精度和强大的泛化能力，并且对现实世界的扰动具有鲁棒性。代码可在 \href{此 https URL}{此 https URL} 中找到。</li>
</ul>

<h3>Title: Carré du champ flow matching: better quality-generalisation tradeoff in generative models</h3>
<ul>
<li><strong>Authors: </strong>Jacob Bamberger, Iolo Jones, Dennis Duncan, Michael M. Bronstein, Pierre Vandergheynst, Adam Gosztolai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.DG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05930">https://arxiv.org/abs/2510.05930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05930">https://arxiv.org/pdf/2510.05930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05930]] Carré du champ flow matching: better quality-generalisation tradeoff in generative models(https://arxiv.org/abs/2510.05930)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep generative models often face a fundamental tradeoff: high sample quality can come at the cost of memorisation, where the model reproduces training data rather than generalising across the underlying data geometry. We introduce Carré du champ flow matching (CDC-FM), a generalisation of flow matching (FM), that improves the quality-generalisation tradeoff by regularising the probability path with a geometry-aware noise. Our method replaces the homogeneous, isotropic noise in FM with a spatially varying, anisotropic Gaussian noise whose covariance captures the local geometry of the latent data manifold. We prove that this geometric noise can be optimally estimated from the data and is scalable to large data. Further, we provide an extensive experimental evaluation on diverse datasets (synthetic manifolds, point clouds, single-cell genomics, animal motion capture, and images) as well as various neural network architectures (MLPs, CNNs, and transformers). We demonstrate that CDC-FM consistently offers a better quality-generalisation tradeoff. We observe significant improvements over standard FM in data-scarce regimes and in highly non-uniformly sampled datasets, which are often encountered in AI for science applications. Our work provides a mathematical framework for studying the interplay between data geometry, generalisation and memorisation in generative models, as well as a robust and scalable algorithm that can be readily integrated into existing flow matching pipelines.</li>
<li><strong>摘要：</strong>深层生成模型通常会面临基本的权衡：高样本质量可以以记忆为基础，在这种情况下，该模型再现了培训数据，而不是在基础数据几何学上概括。我们介绍了CarréDuChamp Flow匹配（CDC-FM），即流量匹配（FM）的概括，通过用几何学感知的噪声正规化概率路径来改善质量成本的权衡。我们的方法用空间变化的各向异性高斯噪声代替了FM中的均匀的各向同性噪声，其协方差捕获了潜在数据歧管的局部几何形状。我们证明，可以从数据中最佳估计这种几何噪声，并且可扩展到大数据。此外，我们对各种数据集（合成歧管，点云，单细胞基因组学，动物运动捕获和图像）以及各种神经网络体系结构（MLP，CNN和变形金刚）提供了广泛的实验评估。我们证明，CDC-FM始终提供质量更高的将军的权衡。我们观察到数据筛选制度和高度不均匀采样的数据集的标准FM的显着改善，这些数据集经常在AI中遇到科学应用程序。我们的工作提供了一个数学框架，用于研究生成模型中数据几何，概括和记忆之间的相互作用，以及可以很容易地将其集成到现有流动匹配管道中的可靠且可扩展的算法。</li>
</ul>

<h3>Title: LLM-FS-Agent: A Deliberative Role-based Large Language Model Architecture for Transparent Feature Selection</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Bal-Ghaoui, Fayssal Sabri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05935">https://arxiv.org/abs/2510.05935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05935">https://arxiv.org/pdf/2510.05935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05935]] LLM-FS-Agent: A Deliberative Role-based Large Language Model Architecture for Transparent Feature Selection(https://arxiv.org/abs/2510.05935)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>High-dimensional data remains a pervasive challenge in machine learning, often undermining model interpretability and computational efficiency. While Large Language Models (LLMs) have shown promise for dimensionality reduction through feature selection, existing LLM-based approaches frequently lack structured reasoning and transparent justification for their decisions. This paper introduces LLM-FS-Agent, a novel multi-agent architecture designed for interpretable and robust feature selection. The system orchestrates a deliberative "debate" among multiple LLM agents, each assigned a specific role, enabling collective evaluation of feature relevance and generation of detailed justifications. We evaluate LLM-FS-Agent in the cybersecurity domain using the CIC-DIAD 2024 IoT intrusion detection dataset and compare its performance against strong baselines, including LLM-Select and traditional methods such as PCA. Experimental results demonstrate that LLM-FS-Agent consistently achieves superior or comparable classification performance while reducing downstream training time by an average of 46% (statistically significant improvement, p = 0.028 for XGBoost). These findings highlight that the proposed deliberative architecture enhances both decision transparency and computational efficiency, establishing LLM-FS-Agent as a practical and reliable solution for real-world applications.</li>
<li><strong>摘要：</strong>高维数据仍然是机器学习中的普遍挑战，通常会破坏模型的解释性和计算效率。尽管大型语言模型（LLM）通过特征选择显示了降低维度的希望，但现有的基于LLM的方法经常缺乏结构化的推理和决策透明的理由。本文介绍了LLM-FS-Agent，这是一种新型的多代理体系结构，旨在可解释和健壮的功能选择。该系统在多个LLM代理之间进行了审议的“辩论”，每个LLM代理都分配了特定的角色，从而可以集体评估特征相关性和详细说明的生成。我们使用CIC-DIAD 2024 IOT Intrusion检测数据集评估了网络安全域中的LLM-FS代理，并将其性能与强质基线进行比较，包括LLM-Select和PCA等传统方法。实验结果表明，LLM-FS代理始终取得优越或可比的分类性能，同时将下游训练时间平均减少46％（统计学上显着改善，XGBOOST的P = 0.028）。这些发现凸显了拟议的审议架构提高了决策透明度和计算效率，从而确立了LLM-FS代理作为实用应用程序的实用和可靠解决方案。</li>
</ul>

<h3>Title: Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis</h3>
<ul>
<li><strong>Authors: </strong>Eashan Adhikarla, Yixin Liu, Brian D. Davison</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05976">https://arxiv.org/abs/2510.05976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05976">https://arxiv.org/pdf/2510.05976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05976]] Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis(https://arxiv.org/abs/2510.05976)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Low-light image enhancement (LLIE) is vital for safety-critical applications such as surveillance, autonomous navigation, and medical imaging, where visibility degradation can impair downstream task performance. Recently, diffusion models have emerged as a promising generative paradigm for LLIE due to their capacity to model complex image distributions via iterative denoising. This survey provides an up-to-date critical analysis of diffusion models for LLIE, distinctively featuring an in-depth comparative performance evaluation against Generative Adversarial Network and Transformer-based state-of-the-art methods, a thorough examination of practical deployment challenges, and a forward-looking perspective on the role of emerging paradigms like foundation models. We propose a multi-perspective taxonomy encompassing six categories: Intrinsic Decomposition, Spectral & Latent, Accelerated, Guided, Multimodal, and Autonomous; that map enhancement methods across physical priors, conditioning schemes, and computational efficiency. Our taxonomy is grounded in a hybrid view of both the model mechanism and the conditioning signals. We evaluate qualitative failure modes, benchmark inconsistencies, and trade-offs between interpretability, generalization, and inference efficiency. We also discuss real-world deployment constraints (e.g., memory, energy use) and ethical considerations. This survey aims to guide the next generation of diffusion-based LLIE research by highlighting trends and surfacing open research questions, including novel conditioning, real-time adaptation, and the potential of foundation models.</li>
<li><strong>摘要：</strong>低光图像增强（LLIE）对于诸如监视，自动导航和医学成像之类的安全至关重要的应用至关重要，在该应用中，可见性降解会损害下游任务绩效。最近，由于LLIE通过迭代denoing进行对复杂图像分布进行建模的能力，扩散模型已成为有希望的生成范式。这项调查为LLIE的扩散模型提供了最新的批判性分析，其鲜明的比较性能评估对生成的对抗性网络和基于变压器的最先进方法，对实践部署挑战进行了彻底的研究，以及对基础模型（例如基础模型）的作用的前瞻性观点。我们提出了一个跨越六类的多人分类法：内在分解，光谱和潜在，加速，指导，多模式和自治；该绘制跨物理先验，调节方案和计算效率的增强方法。我们的分类学是基于模型机制和条件信号的混合视图。我们评估定性故障模式，基准不一致以及可解释性，泛化和推理效率之间的权衡。我们还讨论了现实的部署约束（例如，内存，能源使用）和道德考虑。这项调查旨在指导下一代基于扩散的LLIE研究，以突出趋势并浮出水面的开放研究问题，包括新颖的调理，实时适应和基础模型的潜力。</li>
</ul>

<h3>Title: Diffusion-Based Image Editing for Breaking Robust Watermarks</h3>
<ul>
<li><strong>Authors: </strong>Yunyi Ni, Finn Carter, Ze Niu, Emily Davis, Bo Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05978">https://arxiv.org/abs/2510.05978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05978">https://arxiv.org/pdf/2510.05978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05978]] Diffusion-Based Image Editing for Breaking Robust Watermarks(https://arxiv.org/abs/2510.05978)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Robust invisible watermarking aims to embed hidden information into images such that the watermark can survive various image manipulations. However, the rise of powerful diffusion-based image generation and editing techniques poses a new threat to these watermarking schemes. In this paper, we present a theoretical study and method demonstrating that diffusion models can effectively break robust image watermarks that were designed to resist conventional perturbations. We show that a diffusion-driven ``image regeneration'' process can erase embedded watermarks while preserving perceptual image content. We further introduce a novel guided diffusion attack that explicitly targets the watermark signal during generation, significantly degrading watermark detectability. Theoretically, we prove that as an image undergoes sufficient diffusion-based transformation, the mutual information between the watermarked image and the embedded watermark payload vanishes, resulting in decoding failure. Experimentally, we evaluate our approach on multiple state-of-the-art watermarking schemes (including the deep learning-based methods StegaStamp, TrustMark, and VINE) and demonstrate near-zero watermark recovery rates after attack, while maintaining high visual fidelity of the regenerated images. Our findings highlight a fundamental vulnerability in current robust watermarking techniques against generative model-based attacks, underscoring the need for new watermarking strategies in the era of generative AI.</li>
<li><strong>摘要：</strong>强大的无形水印旨在将隐藏的信息嵌入图像中，以便水印可以在各种图像操作中存活。但是，强大的基于扩散的图像生成和编辑技术的兴起对这些水印方案构成了新的威胁。在本文中，我们提出了一项理论研究和方法，表明扩散模型可以有效地打破旨在抵抗常规扰动的强大图像水印。我们表明，扩散驱动的``图像再生''过程可以在保留感知图像含量的同时擦除嵌入式水印。我们进一步引入了一种新型的引导扩散攻击，该攻击在发电过程中明确靶向水印信号，从而显着降低了水印的可检测性。从理论上讲，我们证明，作为图像经历了足够的基于扩散的转换，水印图像与嵌入式水印有效载荷之间的相互信息消失，从而导致解码故障。在实验上，我们评估了对多种最先进的水印方案（包括基于深度学习的方法Stegastamp，Trustmark和Vine）的方法，并在攻击后显示了接近零的水印恢复速率，同时保持了重新生成图像的高视觉保真度。我们的发现突出了当前强大的水印技术针对基于生成模型的攻击的根本脆弱性，强调了在生成AI时代对新水印策略的需求。</li>
</ul>

<h3>Title: Sample Smart, Not Hard: Correctness-First Decoding for Better Reasoning in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xueyan Li, Guinan Su, Mrinmaya Sachan, Jonas Geiping</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05987">https://arxiv.org/abs/2510.05987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05987">https://arxiv.org/pdf/2510.05987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05987]] Sample Smart, Not Hard: Correctness-First Decoding for Better Reasoning in LLMs(https://arxiv.org/abs/2510.05987)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly applied to complex tasks that require extended reasoning. In such settings, models often benefit from diverse chains-of-thought to arrive at multiple candidate solutions. This requires two competing objectives: to inject enough stochasticity to explore multiple reasoning chains, and to ensure sufficient accuracy and quality in each path. Existing works pursue the first objective by increasing exploration at highly uncertain steps with higher temperature or larger candidate token sets, while others improve reliability by rejecting samples with low confidence post-generation, implying that low confidence correlates with low answer quality. These two lines of thought are in conflict, as they conflate different sources of uncertainty. To resolve this, we argue that the decoding rule should be calibrated by correctness, not confidence alone. We should sample from tokens with higher estimated correctness, and reduce sampling where expected correctness is low. We propose simple strategies that achieve this goal: Greedy-Threshold makes sampling greedy at very low confidence steps. Calibrated-TopK and Calibrated-epsilon set truncation threshold based on estimated rank-wise correctness. Together, our findings challenge prevailing heuristics about decoding under uncertainty and show gains across math and general reasoning benchmarks.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）越来越多地应用于需要扩展推理的复杂任务。在这种情况下，模型通常会受益于各种各样的链条，从而获得了多种候选解决方案。这需要两个相互竞争的目标：注入足够的随机性来探索多个推理链，并确保每条路径中足够的准确性和质量。现有作品通过在高度不确定的步骤中增加探索，以较高的温度或较高的候选令牌集来追求第一个目标，而其他工程则通过拒绝置信后发电后的样本来提高可靠性，这意味着低信心与低答案质量相关。这两条思想陷入了冲突，因为它们将不同的不确定性来源混为一谈。为了解决这个问题，我们认为应该通过正确性而不是单独信心来校准解码规则。我们应该从具有较高估计正确性的代币中采样，并在预期正确性较低的情况下减少采样。我们提出了实现这一目标的简单策略：贪婪 - 阈值使采样贪婪以非常低的信心步骤。基于估计的排名正确性，校准的topk和校准 -  ePsilon设置了截断阈值。我们的发现共同挑战了关于在不确定性下解码并显示数学和一般推理基准的增长的启发式方法。</li>
</ul>

<h3>Title: Continual Learning for Image Captioning through Improved Image-Text Alignment</h3>
<ul>
<li><strong>Authors: </strong>Bertram Taetz, Gal Bordelius</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06009">https://arxiv.org/abs/2510.06009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06009">https://arxiv.org/pdf/2510.06009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06009]] Continual Learning for Image Captioning through Improved Image-Text Alignment(https://arxiv.org/abs/2510.06009)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating accurate and coherent image captions in a continual learning setting remains a major challenge due to catastrophic forgetting and the difficulty of aligning evolving visual concepts with language over time. In this work, we propose a novel multi-loss framework for continual image captioning that integrates semantic guidance through prompt-based continual learning and contrastive alignment. Built upon a pretrained ViT-GPT-2 backbone, our approach combines standard cross-entropy loss with three additional components: (1) a prompt-based cosine similarity loss that aligns image embeddings with synthetically constructed prompts encoding objects, attributes, and actions; (2) a CLIP-style loss that promotes alignment between image embeddings and target caption embedding; and (3) a language-guided contrastive loss that employs a triplet loss to enhance class-level discriminability between tasks. Notably, our approach introduces no additional overhead at inference time and requires no prompts during caption generation. We find that this approach mitigates catastrophic forgetting, while achieving better semantic caption alignment compared to state-of-the-art methods. The code can be found via the following link this https URL Gepardius/Taetz_Bordelius_Continual_ImageCaptioning.</li>
<li><strong>摘要：</strong>由于灾难性遗忘以及随着时间的推移将不断发展的视觉概念与语言保持一致的困难，在持续学习环境中生成准确且连贯的图像说明仍然是一个重大挑战。在这项工作中，我们提出了一种用于连续图像字幕的新颖的多重损失框架，该框架通过基于提示的持续学习和对比对齐来集成语义指导。我们的方法建立在预训练的 ViT-GPT-2 主干基础上，将标准交叉熵损失与三个附加组件相结合：（1）基于提示的余弦相似度损失，将图像嵌入与编码对象、属性和动作的综合构建的提示对齐； (2) CLIP 式损失，促进图像嵌入和目标标题嵌入之间的对齐； （3）语言引导的对比损失，它采用三元组损失来增强任务之间的类级别辨别力。值得注意的是，我们的方法在推理时不会引入额外的开销，并且在标题生成期间不需要提示。我们发现，与最先进的方法相比，这种方法可以减轻灾难性遗忘，同时实现更好的语义标题对齐。可以通过以下链接找到该代码：https URL Gepardius/Taetz_Bordelius_Continual_ImageCaptioning。</li>
</ul>

<h3>Title: Edit-Based Flow Matching for Temporal Point Processes</h3>
<ul>
<li><strong>Authors: </strong>David Lüdke, Marten Lienen, Marcel Kollovieh, Stephan Günnemann</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06050">https://arxiv.org/abs/2510.06050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06050">https://arxiv.org/pdf/2510.06050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06050]] Edit-Based Flow Matching for Temporal Point Processes(https://arxiv.org/abs/2510.06050)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Temporal point processes (TPPs) are a fundamental tool for modeling event sequences in continuous time, but most existing approaches rely on autoregressive parameterizations that are limited by their sequential sampling. Recent non-autoregressive, diffusion-style models mitigate these issues by jointly interpolating between noise and data through event insertions and deletions in a discrete Markov chain. In this work, we generalize this perspective and introduce an Edit Flow process for TPPs that transports noise to data via insert, delete, and substitute edit operations. By learning the instantaneous edit rates within a continuous-time Markov chain framework, we attain a flexible and efficient model that effectively reduces the total number of necessary edit operations during generation. Empirical results demonstrate the generative flexibility of our unconditionally trained model in a wide range of unconditional and conditional generation tasks on benchmark TPPs.</li>
<li><strong>摘要：</strong>时间点过程（TPP）是在连续时间建模事件序列的基本工具，但是大多数现有方法依赖于受其顺序采样限制的自回归参数化。最近的非自动进取，扩散式模型通过在离散马尔可夫链中通过事件插入和删除在噪声和数据之间共同插值来减轻这些问题。在这项工作中，我们概括了此观点，并为TPP引入了一个编辑流程，该流程通过插入，删除和替代编辑操作将噪声传输到数据。通过学习连续时间马尔可夫链框架内的瞬时编辑率，我们获得了一个灵活而有效的模型，可有效减少生成过程中必要的编辑操作的总数。经验结果证明了我们在基准TPP上无条件和条件生成任务中无条件训练的模型的生成灵活性。</li>
</ul>

<h3>Title: When Thinking Drifts: Evidential Grounding for Robust Video Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Mi Luo, Zihui Xue, Alex Dimakis, Kristen Grauman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06077">https://arxiv.org/abs/2510.06077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06077">https://arxiv.org/pdf/2510.06077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06077]] When Thinking Drifts: Evidential Grounding for Robust Video Reasoning(https://arxiv.org/abs/2510.06077)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Video reasoning, the task of enabling machines to infer from dynamic visual content through multi-step logic, is crucial for advanced AI. While the Chain-of-Thought (CoT) mechanism has enhanced reasoning in text-based tasks, its application to video understanding remains underexplored. This paper presents a systematic analysis revealing that CoT often degrades performance in video reasoning, generating verbose but misleading internal monologues, and leading to hallucinated visual details and overridden correct intuitions - a phenomenon we term "visual thinking drift". We explain this drift through a Bayesian lens, positing that CoT traces often diverge from actual visual evidence, instead amplifying internal biases or language priors, causing models to storytell rather than engage in grounded reasoning. To counteract this, we introduce Visual Evidence Reward (VER), a novel reinforcement learning framework that explicitly rewards the generation of reasoning traces that are verifiably grounded in visual evidence. Comprehensive evaluation across 10 diverse video understanding benchmarks demonstrates that our Video-VER consistently achieves top performance. Our work sheds light on the distinct challenges of video-centric reasoning and encourages the development of AI that robustly grounds its inferences in visual evidence - for large multimodal models that not only "think before answering", but also "see while thinking".</li>
<li><strong>摘要：</strong>视频推理，使机器能够通过多步逻辑从动态视觉内容推断的任务对于高级AI至关重要。虽然对基于文本的任务中的经过思考链（COT）机制增强了推理，但其在视频理解中的应用仍未得到充满激光。本文提出了系统的分析，表明COT经常在视频推理中降低性能，产生冗长但具有误导性的内部独白，并导致幻觉的视觉细节和覆盖正确的直觉 - 我们称为“视觉思维漂移”的现象。我们通过贝叶斯镜头解释了这种漂移，认为Cot的痕迹经常与实际的视觉证据不同，而是放大了内部偏见或语言先验，导致模型导致讲故事而不是进行扎根的推理。为了抵消这一点，我们引入了视觉证据奖励（VER），这是一个新颖的增强学习框架，明确地奖励了在视觉证据中有明确基础的推理痕迹的产生。对10种不同视频理解基准的全面评估表明，我们的视频始终取得了最佳性能。我们的作品阐明了以视频为中心的推理的独特挑战，并鼓励AI的发展，这些AI在视觉证据中牢固地理解其推论 - 对于大型多模型，这些模型不仅是“在回答之前思考”，而且“在思考时也看到”。</li>
</ul>

<h3>Title: Compact Multi-level-prior Tensor Representation for Hyperspectral Image Super-resolution</h3>
<ul>
<li><strong>Authors: </strong>Yinjian Wang, Wei Li, Yuanyuan Gui, Gemine Vivone</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06098">https://arxiv.org/abs/2510.06098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06098">https://arxiv.org/pdf/2510.06098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06098]] Compact Multi-level-prior Tensor Representation for Hyperspectral Image Super-resolution(https://arxiv.org/abs/2510.06098)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Fusing a hyperspectral image with a multispectral image acquired over the same scene, \textit{i.e.}, hyperspectral image super-resolution, has become a popular computational way to access the latent high-spatial-spectral-resolution image. To date, a variety of fusion methods have been proposed, among which the tensor-based ones have testified that multiple priors, such as multidimensional low-rankness and spatial total variation at multiple levels, effectively drive the fusion process. However, existing tensor-based models can only effectively leverage one or two priors at one or two levels, since simultaneously incorporating multi-level priors inevitably increases model complexity. This introduces challenges in both balancing the weights of different priors and optimizing multi-block structures. Concerning this, we present a novel hyperspectral super-resolution model compactly characterizing these multi-level priors of hyperspectral images within the tensor framework. Firstly, the proposed model decouples the spectral low-rankness and spatial priors by casting the latent high-spatial-spectral-resolution image into spectral subspace and spatial maps via block term decomposition. Secondly, these spatial maps are stacked as the spatial tensor encoding the high-order spatial low-rankness and smoothness priors, which are co-modeled via the proposed non-convex mode-shuffled tensor correlated total variation. Finally, we draw inspiration from the linearized alternating direction method of multipliers to design an efficient algorithm to optimize the resulting model, theoretically proving its Karush-Kuhn-Tucker convergence under mild conditions. Experiments on multiple datasets demonstrate the effectiveness of the proposed algorithm. The code implementation will be available from this https URL.</li>
<li><strong>摘要：</strong>将高光谱图像与在同一场景中获取的多光谱图像融合，即高光谱图像超分辨率，已成为访问潜在高空间光谱分辨率图像的流行计算方法。迄今为止，已经提出了多种融合方法，其中基于张量的融合方法已证明多维低秩和多个级别的空间总变差等多个先验可以有效地驱动融合过程。然而，现有的基于张量的模型只能有效地利用一两个级别的一两个先验，因为同时合并多级先验不可避免地会增加模型的复杂性。这给平衡不同先验的权重和优化多块结构带来了挑战。关于这一点，我们提出了一种新颖的高光谱超分辨率模型，在张量框架内紧凑地表征了高光谱图像的这些多级先验。首先，所提出的模型通过块项分解将潜在的高空间光谱分辨率图像转换为光谱子空间和空间图，从而解耦光谱低秩和空间先验。其次，这些空间图被堆叠为编码高阶空间低秩和平滑先验的空间张量，这些先验通过所提出的非凸模式洗牌张量相关总变分进行共同建模。最后，我们从乘子的线性化交替方向方法中汲取灵感，设计了一种有效的算法来优化所得模型，从理论上证明了其在温和条件下的Karush-Kuhn-Tucker收敛性。多个数据集上的实验证明了所提出算法的有效性。可以从此 https URL 获取代码实现。</li>
</ul>

<h3>Title: PolyGraph Discrepancy: a classifier-based metric for graph generation</h3>
<ul>
<li><strong>Authors: </strong>Markus Krimmel, Philip Hartout, Karsten Borgwardt, Dexiong Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06122">https://arxiv.org/abs/2510.06122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06122">https://arxiv.org/pdf/2510.06122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06122]] PolyGraph Discrepancy: a classifier-based metric for graph generation(https://arxiv.org/abs/2510.06122)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Existing methods for evaluating graph generative models primarily rely on Maximum Mean Discrepancy (MMD) metrics based on graph descriptors. While these metrics can rank generative models, they do not provide an absolute measure of performance. Their values are also highly sensitive to extrinsic parameters, namely kernel and descriptor parametrization, making them incomparable across different graph descriptors. We introduce PolyGraph Discrepancy (PGD), a new evaluation framework that addresses these limitations. It approximates the Jensen-Shannon distance of graph distributions by fitting binary classifiers to distinguish between real and generated graphs, featurized by these descriptors. The data log-likelihood of these classifiers approximates a variational lower bound on the JS distance between the two distributions. Resulting metrics are constrained to the unit interval [0,1] and are comparable across different graph descriptors. We further derive a theoretically grounded summary metric that combines these individual metrics to provide a maximally tight lower bound on the distance for the given descriptors. Thorough experiments demonstrate that PGD provides a more robust and insightful evaluation compared to MMD metrics. The PolyGraph framework for benchmarking graph generative models is made publicly available at this https URL.</li>
<li><strong>摘要：</strong>评估图生成模型的现有方法主要依赖于基于图描述符的最大平均差异（MMD）指标。虽然这些指标可以对生成模型进行排名，但它们并不能提供绝对的性能衡量标准。它们的值对外部参数（即内核和描述符参数化）也高度敏感，使得它们在不同的图描述符之间无法比较。我们引入了 PolyGraph Discrepancy (PGD)，这是一种解决这些限制的新评估框架。它通过拟合二元分类器来近似图分布的 Jensen-Shannon 距离，以区分由这些描述符表征的真实图和生成图。这些分类器的数据对数似然近似于两个分布之间 JS 距离的变分下界。生成的度量被限制在单位区间 [0,1] 内，并且在不同的图描述符之间具有可比性。我们进一步推导了一个理论上有根据的汇总指标，该指标结合了这些单独的指标，为给定的描述符提供了最严格的距离下限。彻底的实验表明，与 MMD 指标相比，PGD 提供了更稳健、更有洞察力的评估。用于对图形生成模型进行基准测试的 PolyGraph 框架已在此 https URL 上公开提供。</li>
</ul>

<h3>Title: Towards Data-Efficient Medical Imaging: A Generative and Semi-Supervised Framework</h3>
<ul>
<li><strong>Authors: </strong>Mosong Ma, Tania Stathaki, Michalis Lazarou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06123">https://arxiv.org/abs/2510.06123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06123">https://arxiv.org/pdf/2510.06123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06123]] Towards Data-Efficient Medical Imaging: A Generative and Semi-Supervised Framework(https://arxiv.org/abs/2510.06123)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep learning in medical imaging is often limited by scarce and imbalanced annotated data. We present SSGNet, a unified framework that combines class specific generative modeling with iterative semisupervised pseudo labeling to enhance both classification and segmentation. Rather than functioning as a standalone model, SSGNet augments existing baselines by expanding training data with StyleGAN3 generated images and refining labels through iterative pseudo labeling. Experiments across multiple medical imaging benchmarks demonstrate consistent gains in classification and segmentation performance, while Frechet Inception Distance analysis confirms the high quality of generated samples. These results highlight SSGNet as a practical strategy to mitigate annotation bottlenecks and improve robustness in medical image analysis.</li>
<li><strong>摘要：</strong>医学成像中的深度学习通常受到稀缺和不平衡的注释数据的限制。我们提出了SSGNET，这是一个统一的框架，将特定于类的生成性建模与迭代性半培训的伪标记结合在一起，以增强分类和分割。 SSGNET并没有充当独立模型，而是通过使用迭代伪标签来扩展训练数据并精炼标签，而不是通过独立模型来增强现有基线。跨多个医学成像基准的实验表现出分类和分割性能的一致增长，而Frechet Inception距离分析证实了生成的样品的高质量。这些结果强调了SSGNET是减轻注释瓶颈并改善医学图像分析中的鲁棒性的实用策略。</li>
</ul>

<h3>Title: Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Mao, Yuhan Wang, Lifeng Chen, Can Zhao, Yucheng Tang, Dong Yang, Liangqiong Qu, Daguang Xu, Yuyin Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06131">https://arxiv.org/abs/2510.06131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06131">https://arxiv.org/pdf/2510.06131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06131]] Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation(https://arxiv.org/abs/2510.06131)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative medical models are constrained by modality-specific scenarios that hinder the integration of complementary evidence from imaging, pathology, and clinical notes. This fragmentation limits their evolution into foundation models that can learn and reason across the full spectrum of biomedical data. We propose MeDiM, the first medical discrete diffusion model that learns shared distributions across modalities without modality-specific components. MeDiM unifies multiple generative tasks: translating between images and text, and jointly producing image-report pairs across domains in response to prompts. Built on a discrete diffusion framework, MeDiM bridges vision and language representations through a shared probabilistic space. To enable unified and flexible medical generation, we employ a multimodal large language model (MLLM) as the diffusion backbone, leveraging its prior knowledge and cross-modal reasoning. Two key designs are introduced: (1) removing the causal attention mask for bidirectional context, and (2) injecting continuous timestep embeddings for diffusion awareness. Experiments demonstrate high-fidelity medical generation (FID 16.60 on MIMIC-CXR and FID 24.19 on PathGen) and accurate report generation (METEOR 0.2650 and 0.2580). Jointly generated image-report pairs further enhance downstream performance (plus6.43 percent BLEU-1, plus18.57 percent BLEU-2, plus31.58 percent BLEU-3, plus4.80 percent METEOR), showing that MeDiM supports coherent and clinically grounded multimodal outputs.</li>
<li><strong>摘要：</strong>生成医学模型的最新进展受到特定于模态的情况的限制，这阻碍了成像，病理学和临床注释中互补证据的整合。这种破碎化将它们的演变限制在基础模型中，这些模型可以在整个生物医学数据中学习和推理。我们提出了MEDIM，这是第一个医学离散扩散模型，该模型在没有模式特异性组件的情况下学习共享分布。 Medim统一了多个生成任务：在图像和文本之间转换，并响应提示，共同产生跨域的图像报告对。 Medim建立在离散的扩散框架的基础上，通过共享的概率空间桥梁视觉和语言表示。为了实现统一和灵活的医学一代，我们采用多模式的大语言模型（MLLM）作为扩散骨干，利用其先验知识和跨模式推理。引入了两个关键设计：（1）删除双向背景的因果注意面罩，以及（2）为扩散意识注入连续的时间段嵌入。实验证明了高保真医学的生成（MIMIC-CXR上的FID 16.60，而PENGEN上的FID 24.19）和准确的报告生成（Meteor 0.2650和0.2580）。联合生成的图像报告对进一步提高了下游性能（加上6.43％的BLEU-1，加上18.57％的BLEU-2，加上31.58％的BLEU-3，加4.80％的流星），表明MEDIM支持相干和临床接地的多媒体输出。</li>
</ul>

<h3>Title: Deforming Videos to Masks: Flow Matching for Referring Video Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Zanyi Wang, Dengyang Jiang, Liuzhuozheng Li, Sizhe Dang, Chengzu Li, Harry Yang, Guang Dai, Mengmeng Wang, Jingdong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06139">https://arxiv.org/abs/2510.06139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06139">https://arxiv.org/pdf/2510.06139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06139]] Deforming Videos to Masks: Flow Matching for Referring Video Segmentation(https://arxiv.org/abs/2510.06139)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Referring Video Object Segmentation (RVOS) requires segmenting specific objects in a video guided by a natural language description. The core challenge of RVOS is to anchor abstract linguistic concepts onto a specific set of pixels and continuously segment them through the complex dynamics of a video. Faced with this difficulty, prior work has often decomposed the task into a pragmatic `locate-then-segment' pipeline. However, this cascaded design creates an information bottleneck by simplifying semantics into coarse geometric prompts (e.g, point), and struggles to maintain temporal consistency as the segmenting process is often decoupled from the initial language grounding. To overcome these fundamental limitations, we propose FlowRVS, a novel framework that reconceptualizes RVOS as a conditional continuous flow problem. This allows us to harness the inherent strengths of pretrained T2V models, fine-grained pixel control, text-video semantic alignment, and temporal coherence. Instead of conventional generating from noise to mask or directly predicting mask, we reformulate the task by learning a direct, language-guided deformation from a video's holistic representation to its target mask. Our one-stage, generative approach achieves new state-of-the-art results across all major RVOS benchmarks. Specifically, achieving a $\mathcal{J}\&\mathcal{F}$ of 51.1 in MeViS (+1.6 over prior SOTA) and 73.3 in the zero shot Ref-DAVIS17 (+2.7), demonstrating the significant potential of modeling video understanding tasks as continuous deformation processes.</li>
<li><strong>摘要：</strong>引用视频对象细分（RVO）需要在自然语言描述引导的视频中分割特定对象。 RVO的核心挑战是将抽象的语言概念固定在一组特定的像素上，并通过视频的复杂动力不断分割它们。面对这一困难，先前的工作经常将任务分解为务实的“定位段”管道。但是，这种级联设计通过将语义简化为粗糙的几何提示（例如，点）来创建信息瓶颈，并努力保持时间一致性，因为分段过程通常与初始语言接地脱钩。为了克服这些基本局限性，我们提出了FlowRV，这是一个新型框架，将RVO重新概念化为有条件的连续流问题。这使我们能够利用验证的T2V模型的固有优势，细粒的像素控制，文本视频语义对准和时间连贯性。我们通过学习直接，语言引导的变形从视频的整体表示到其目标掩码来重新制定任务，而不是从噪声到掩盖或直接预测面具的传统产生。我们的一个阶段，生成的方法在所有主要的RVOS基准中都取得了新的最新结果。具体而言，在MeVis中获得51.1的$ \ MATHCAL {J} \＆\ MATHCAL {f} $（在先前的SOTA上+1.6）和73.3中的零射击Ref-Davis17（+2.7）中的73.3，这表明将视频理解任务作为持续变形过程建模的重要潜力。</li>
</ul>

<h3>Title: Thermodynamic Performance Limits for Score-Based Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Nathan X. Kodama, Michael Hinczewski</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.stat-mech</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06174">https://arxiv.org/abs/2510.06174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06174">https://arxiv.org/pdf/2510.06174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06174]] Thermodynamic Performance Limits for Score-Based Diffusion Models(https://arxiv.org/abs/2510.06174)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We establish a fundamental connection between score-based diffusion models and non-equilibrium thermodynamics by deriving performance limits based on entropy rates. Our main theoretical contribution is a lower bound on the negative log-likelihood of the data that relates model performance to entropy rates of diffusion processes. We numerically validate this bound on a synthetic dataset and investigate its tightness. By building a bridge to entropy rates - system, intrinsic, and exchange entropy - we provide new insights into the thermodynamic operation of these models, drawing parallels to Maxwell's demon and implications for thermodynamic computing hardware. Our framework connects generative modeling performance to fundamental physical principles through stochastic thermodynamics.</li>
<li><strong>摘要：</strong>我们通过基于熵率来得出性能限制，在基于得分的扩散模型和非平衡热力学之间建立了基本联系。我们的主要理论贡献是将模型性能与扩散过程的熵率相关联的数据的负模样的下限。我们从数值上验证了该绑定在合成数据集上并研究其紧密度。通过建造通往熵率的桥梁 - 系统，内在和交换熵 - 我们为这些模型的热力学操作提供了新的见解，从而使麦克斯韦的恶魔的相似之处及其对热力学计算硬件的影响。我们的框架通过随机热力学将生成建模性能与基本物理原理联系起来。</li>
</ul>

<h3>Title: On Powerful Ways to Generate: Autoregression, Diffusion, and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Chenxiao Yang, Cai Zhou, David Wipf, Zhiyuan Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06190">https://arxiv.org/abs/2510.06190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06190">https://arxiv.org/pdf/2510.06190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06190]] On Powerful Ways to Generate: Autoregression, Diffusion, and Beyond(https://arxiv.org/abs/2510.06190)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This paper formally studies generation processes, including auto-regressive next-token prediction and masked diffusion, that abstract beyond architectural specifics. At this level of abstraction, we quantify their benefits and limitations through measurable criteria such as computational hardness and learnability. In particular, we demonstrate that allowing generation to proceed beyond autoregression and current masked diffusion, with capabilities to rewrite and length-variable edit, can bring significant theoretical and empirical advantages, with important implications for frontier LLMs that aspire to tackle increasingly hard problems and work universally across domains beyond natural language, such as coding and science.</li>
<li><strong>摘要：</strong>本文正式研究了生成过程，包括自动回火的下一步预测和掩盖的扩散，这些预测超出了建筑细节。在这个抽象的水平上，我们通过可测量的标准（例如计算硬度和可学习性）来量化它们的收益和限制。特别是，我们证明，允许产生能够超出自动估计和当前的掩盖扩散，并具有重写和长度可变性的能力，可以带来显着的理论和经验优势，对Frontier LLMS的重要含义，对解决越来越多的硬性问题的边境LLM，以及跨越自然语言的跨越域，例如编码和科学等跨越域的越来越多的问题。</li>
</ul>

<h3>Title: ShapeGen4D: Towards High Quality 4D Shape Generation from Videos</h3>
<ul>
<li><strong>Authors: </strong>Jiraphon Yenphraphai, Ashkan Mirzaei, Jianqi Chen, Jiaxu Zou, Sergey Tulyakov, Raymond A. Yeh, Peter Wonka, Chaoyang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06208">https://arxiv.org/abs/2510.06208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06208">https://arxiv.org/pdf/2510.06208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06208]] ShapeGen4D: Towards High Quality 4D Shape Generation from Videos(https://arxiv.org/abs/2510.06208)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Video-conditioned 4D shape generation aims to recover time-varying 3D geometry and view-consistent appearance directly from an input video. In this work, we introduce a native video-to-4D shape generation framework that synthesizes a single dynamic 3D representation end-to-end from the video. Our framework introduces three key components based on large-scale pre-trained 3D models: (i) a temporal attention that conditions generation on all frames while producing a time-indexed dynamic representation; (ii) a time-aware point sampling and 4D latent anchoring that promote temporally consistent geometry and texture; and (iii) noise sharing across frames to enhance temporal stability. Our method accurately captures non-rigid motion, volume changes, and even topological transitions without per-frame optimization. Across diverse in-the-wild videos, our method improves robustness and perceptual fidelity and reduces failure modes compared with the baselines.</li>
<li><strong>摘要：</strong>视频调节 4D 形状生成旨在直接从输入视频恢复随时间变化的 3D 几何形状和视图一致的外观。在这项工作中，我们引入了一个原生视频到 4D 形状生成框架，该框架从视频中端到端合成单个动态 3D 表示。我们的框架引入了基于大规模预训练 3D 模型的三个关键组件：（i）时间注意力，它在生成时间索引动态表示的同时调节所有帧的生成； (ii) 时间感知点采样和 4D 潜在锚定，可促进几何形状和纹理的时间一致性； (iii) 跨帧共享噪声以增强时间稳定性。我们的方法可以准确捕获非刚性运动、体积变化，甚至拓扑转变，而无需每帧优化。在各种野外视频中，与基线相比，我们的方法提高了鲁棒性和感知保真度，并减少了故障模式。</li>
</ul>

<h3>Title: Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Wang, Zhenpei Yang, Yijing Bai, Yingwei Li, Yuliang Zou, Bo Sun, Abhijit Kundu, Jose Lezama, Luna Yue Huang, Zehao Zhu, Jyh-Jing Hwang, Dragomir Anguelov, Mingxing Tan, Chiyu Max Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06209">https://arxiv.org/abs/2510.06209</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06209">https://arxiv.org/pdf/2510.06209</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06209]] Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models(https://arxiv.org/abs/2510.06209)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative models have sparked exciting new possibilities in the field of autonomous vehicles. Specifically, video generation models are now being explored as controllable virtual testing environments. Simultaneously, end-to-end (E2E) driving models have emerged as a streamlined alternative to conventional modular autonomous driving systems, gaining popularity for their simplicity and scalability. However, the application of these techniques to simulation and planning raises important questions. First, while video generation models can generate increasingly realistic videos, can these videos faithfully adhere to the specified conditions and be realistic enough for E2E autonomous planner evaluation? Second, given that data is crucial for understanding and controlling E2E planners, how can we gain deeper insights into their biases and improve their ability to generalize to out-of-distribution scenarios? In this work, we bridge the gap between the driving models and generative world models (Drive&Gen) to address these questions. We propose novel statistical measures leveraging E2E drivers to evaluate the realism of generated videos. By exploiting the controllability of the video generation model, we conduct targeted experiments to investigate distribution gaps affecting E2E planner performance. Finally, we show that synthetic data produced by the video generation model offers a cost-effective alternative to real-world data collection. This synthetic data effectively improves E2E model generalization beyond existing Operational Design Domains, facilitating the expansion of autonomous vehicle services into new operational contexts.</li>
<li><strong>摘要：</strong>生成模型的最新进展激发了自动驾驶汽车领域令人兴奋的新可能性。具体来说，视频生成模型现在正在作为可控虚拟测试环境进行探索。与此同时，端到端（E2E）驾驶模型已成为传统模块化自动驾驶系统的简化替代方案，因其简单性和可扩展性而受到欢迎。然而，这些技术在模拟和规划中的应用提出了重要的问题。首先，虽然视频生成模型可以生成越来越真实的视频，但这些视频能否忠实地遵守指定条件并且足够真实以供端到端自主规划器评估？其次，鉴于数据对于理解和控制端到端规划者至关重要，我们如何才能更深入地了解他们的偏见并提高他们推广到分布外场景的能力？在这项工作中，我们弥合了驾驶模型和生成世界模型（Drive&Gen）之间的差距来解决这些问题。我们提出了利用端到端驱动程序来评估生成视频的真实性的新颖统计方法。通过利用视频生成模型的可控性，我们进行了有针对性的实验来研究影响端到端规划器性能的分布差距。最后，我们表明视频生成模型生成的合成数据为现实世界的数据收集提供了一种经济高效的替代方案。这些合成数据有效地提高了 E2E 模型的通用性，超越了现有的操作设计领域，从而促进自动驾驶汽车服务扩展到新的操作环境中。</li>
</ul>

<h3>Title: Fine-grained Defocus Blur Control for Generative Image Models</h3>
<ul>
<li><strong>Authors: </strong>Ayush Shrivastava, Connelly Barnes, Xuaner Zhang, Lingzhi Zhang, Andrew Owens, Sohrab Amirghodsi, Eli Shechtman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06215">https://arxiv.org/abs/2510.06215</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06215">https://arxiv.org/pdf/2510.06215</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06215]] Fine-grained Defocus Blur Control for Generative Image Models(https://arxiv.org/abs/2510.06215)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Current text-to-image diffusion models excel at generating diverse, high-quality images, yet they struggle to incorporate fine-grained camera metadata such as precise aperture settings. In this work, we introduce a novel text-to-image diffusion framework that leverages camera metadata, or EXIF data, which is often embedded in image files, with an emphasis on generating controllable lens blur. Our method mimics the physical image formation process by first generating an all-in-focus image, estimating its monocular depth, predicting a plausible focus distance with a novel focus distance transformer, and then forming a defocused image with an existing differentiable lens blur model. Gradients flow backwards through this whole process, allowing us to learn without explicit supervision to generate defocus effects based on content elements and the provided EXIF data. At inference time, this enables precise interactive user control over defocus effects while preserving scene contents, which is not achievable with existing diffusion models. Experimental results demonstrate that our model enables superior fine-grained control without altering the depicted scene.</li>
<li><strong>摘要：</strong>当前的文本到图像扩散模型在产生多样化的高质量图像方面表现出色，但它们很难结合精细的相机元数据，例如精确的光圈设置。在这项工作中，我们引入了一个新颖的文本到图像扩散框架，该框架利用相机元数据或Exif数据，通常嵌入图像文件中，重点是生成可控制的镜头模糊。我们的方法通过首先生成全焦点图像，估算其单眼深度，通过新颖的焦点距离变压器预测合理的聚焦距离，然后使用现有的不同镜头模型形成散焦图像，从而模仿物理图像形成过程。渐变在整个过程中向后流动，使我们能够在没有明确监督的情况下学习，从而根据内容元素和所提供的EXIF数据产生散热效果。在推理时，这可以使精确的交互式用户控制散热器效果，同时保留场景内容，而现有扩散模型无法实现。实验结果表明，我们的模型可以在不改变所描绘场景的情况下进行优质细粒度控制。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
