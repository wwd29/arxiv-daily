<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-03</h1>
<h3>Title: A-FloPS: Accelerating Diffusion Sampling with Adaptive Flow Path Sampler</h3>
<ul>
<li><strong>Authors: </strong>Cheng Jin, Zhenyu Xiao, Yuantao Gu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00036">https://arxiv.org/abs/2509.00036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00036">https://arxiv.org/pdf/2509.00036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00036]] A-FloPS: Accelerating Diffusion Sampling with Adaptive Flow Path Sampler(https://arxiv.org/abs/2509.00036)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models deliver state-of-the-art generative performance across diverse modalities but remain computationally expensive due to their inherently iterative sampling process. Existing training-free acceleration methods typically improve numerical solvers for the reverse-time ODE, yet their effectiveness is fundamentally constrained by the inefficiency of the underlying sampling trajectories. We propose A-FloPS (Adaptive Flow Path Sampler), a principled, training-free framework that reparameterizes the sampling trajectory of any pre-trained diffusion model into a flow-matching form and augments it with an adaptive velocity decomposition. The reparameterization analytically maps diffusion scores to flow-compatible velocities, yielding integration-friendly trajectories without retraining. The adaptive mechanism further factorizes the velocity field into a linear drift term and a residual component whose temporal variation is actively suppressed, restoring the accuracy benefits of high-order integration even in extremely low-NFE regimes. Extensive experiments on conditional image generation and text-to-image synthesis show that A-FloPS consistently outperforms state-of-the-art training-free samplers in both sample quality and efficiency. Notably, with as few as $5$ function evaluations, A-FloPS achieves substantially lower FID and generates sharper, more coherent images. The adaptive mechanism also improves native flow-based generative models, underscoring its generality. These results position A-FloPS as a versatile and effective solution for high-quality, low-latency generative modeling.</li>
<li><strong>摘要：</strong>扩散模型在各种方式上提供了最先进的生成性能，但由于其固有的迭代采样过程，因此在计算方面保持昂贵。现有的无训练加速度方法通常改善了相反时间ode的数值求解器，但是它们的有效性在根本上受到基础采样轨迹的效率低下的限制。我们提出了A-Flops（自适应流动路径采样器），这是一种原则中的，无训练的框架，将任何预训练的扩散模型的采样轨迹重新分配为流动匹配形式，并以自适应速度分解增强其。重新聚集分析地图扩散得分为流动兼容的速度，从而产生了对积分友好的轨迹而无需重新培训。自适应机制将速度场进一步分解为线性漂移项和一个残留分量，其时间变化被积极抑制，即使在极低的NFE机制中，也可以恢复高阶整合的准确性。关于条件图像产生和文本对图像合成的广泛实验表明，A-Flops在样本质量和效率方面始终优于最先进的无培训采样器。值得注意的是，A-Flops的功能评估少于$ 5 $，因此可以大大降低FID，并产生更清晰，更连贯的图像。自适应机制还改善了基于天然流动的生成模型，强调了其通用性。这些结果将A-Flops定位为高质量，低延迟生成建模的多功能和有效解决方案。</li>
</ul>

<h3>Title: Exploring and Reshaping the Weight Distribution in LLM</h3>
<ul>
<li><strong>Authors: </strong>Chunming Ye, Songzhou Li, Xu Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00046">https://arxiv.org/abs/2509.00046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00046">https://arxiv.org/pdf/2509.00046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00046]] Exploring and Reshaping the Weight Distribution in LLM(https://arxiv.org/abs/2509.00046)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The performance of Large Language Models is influenced by their characteristics such as architecture, model sizes, decoding methods and so on. Due to differences in structure or function, the weights in different layers of large models have varying distributions. This paper explores the correlations between different types of layers in terms of weights distribution and studies the potential impact of these correlations on LoRA training effectiveness. Firstly, the study reveals that in the model the cosine distances between weights of different layers manifest power-law distribution. We extract Query-projection, down-projection and other weight matrices from the self-attention layers and MLP layers, calculate the singular values of the matrices using singular value decomposition, and organize a certain number of singular values into matrices according to projection's type. By analyzing the probability distribution of the cosine distances between these matrices, it is found that the cosine distances values between them have distinct power-law distribution characteristics. Secondly, based on the results of distance calculations and analysis across different layers of model, a qualitative method is proposed to describe the distribution characteristics of different models. Next, to construct weights that align with the distribution characteristics, a data generator is designed using a combination of Gaussian process and Pareto distribution functions. The generator is used to simulate the generation of data that aligns with specific distribution characteristics. Finally, based on the aforementioned distribution characteristics and data generation method, the weights in LoRA initialization are reshaped for training. Experimental results indicate that, without altering the model structure or training process, this method achieves a certain improvement in the performance of LoRA training.</li>
<li><strong>摘要：</strong>大语言模型的性能受其特征的影响，例如建筑，模型大小，解码方法等。由于结构或功能的差异，大型模型的不同层中的权重具有不同的分布。本文探讨了不同类型的层在权重分布方面的相关性，并研究了这些相关性对LORA培训效果的潜在影响。首先，研究表明，在模型中，不同层的权重之间的余弦距离表现出幂律分布。我们从自发层和MLP层中提取查询预测，下调和其他重量矩阵，使用奇异值分解来计算矩阵的奇异值，并根据投影类型将一定数量的奇异值组织成矩阵。通过分析这些矩阵之间余弦距离的概率分布，发现它们之间的余弦距离值具有不同的幂律分布特征。其次，基于模型不同层的距离计算和分析的结果，提出了一种定性方法来描述不同模型的分布特征。接下来，为了构建与分布特性保持一致的权重，使用高斯过程和帕累托分布功能的组合设计了数据生成器。发电机用于模拟与特定分布特征对齐的数据的产生。最后，基于上述分布特征和数据生成方法，洛拉初始化的权重被重塑用于训练。实验结果表明，在不改变模型结构或训练过程的情况下，该方法可以在洛拉训练的性能方面有所改善。</li>
</ul>

<h3>Title: From Data to Decision: A Multi-Stage Framework for Class Imbalance Mitigation in Optical Network Failure Analysis</h3>
<ul>
<li><strong>Authors: </strong>Yousuf Moiz Ali, Jaroslaw E. Prilepsky, Nicola Sambo, Joao Pedro, Mohammad M. Hosseini, Antonio Napoli, Sergei K. Turitsyn, Pedro Freire</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00057">https://arxiv.org/abs/2509.00057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00057">https://arxiv.org/pdf/2509.00057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00057]] From Data to Decision: A Multi-Stage Framework for Class Imbalance Mitigation in Optical Network Failure Analysis(https://arxiv.org/abs/2509.00057)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Machine learning-based failure management in optical networks has gained significant attention in recent years. However, severe class imbalance, where normal instances vastly outnumber failure cases, remains a considerable challenge. While pre- and in-processing techniques have been widely studied, post-processing methods are largely unexplored. In this work, we present a direct comparison of pre-, in-, and post-processing approaches for class imbalance mitigation in failure detection and identification using an experimental dataset. For failure detection, post-processing methods-particularly Threshold Adjustment-achieve the highest F1 score improvement (up to 15.3%), while Random Under-Sampling provides the fastest inference. In failure identification, GenAI methods deliver the most substantial performance gains (up to 24.2%), whereas post-processing shows limited impact in multi-class settings. When class overlap is present and latency is critical, over-sampling methods such as the SMOTE are most effective; without latency constraints, Meta-Learning yields the best results. In low-overlap scenarios, Generative AI approaches provide the highest performance with minimal inference time.</li>
<li><strong>摘要：</strong>近年来，光网络中基于机器学习的失败管理引起了人们的关注。但是，正常情况大大超过失败案件，严重的阶级失衡仍然是一个巨大的挑战。虽然已经广泛研究了预处理和进行内部处理的技术，但后处理方法在很大程度上没有探索。在这项工作中，我们介绍了使用实验数据集进行类别失败检测和识别的类失衡的预处理和后处理方法的直接比较。为了进行故障检测，后处理方法 - 最高阈值调整 - 最高的F1分数提高（高达15.3％），而随机的下采样可提供最快的推断。在故障识别中，Genai方法可带来最大的性能增长（高达24.2％），而后处理显示在多级设置中的影响有限。当存在类重叠并且延迟至关重要时，诸如SMOTE之类的过度采样方法最有效。没有延迟限制，元学习可以产生最佳结果。在低度曲线场景中，生成的AI方法可提供最高的性能，而推理时间很少。</li>
</ul>

<h3>Title: Scaffold Diffusion: Sparse Multi-Category Voxel Structure Generation with Discrete Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Justin Jung</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00062">https://arxiv.org/abs/2509.00062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00062">https://arxiv.org/pdf/2509.00062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00062]] Scaffold Diffusion: Sparse Multi-Category Voxel Structure Generation with Discrete Diffusion(https://arxiv.org/abs/2509.00062)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generating realistic sparse multi-category 3D voxel structures is difficult due to the cubic memory scaling of voxel structures and moreover the significant class imbalance caused by sparsity. We introduce Scaffold Diffusion, a generative model designed for sparse multi-category 3D voxel structures. By treating voxels as tokens, Scaffold Diffusion uses a discrete diffusion language model to generate 3D voxel structures. We show that discrete diffusion language models can be extended beyond inherently sequential domains such as text to generate spatially coherent 3D structures. We evaluate on Minecraft house structures from the 3D-Craft dataset and demonstrate that, unlike prior baselines and an auto-regressive formulation, Scaffold Diffusion produces realistic and coherent structures even when trained on data with over 98% sparsity. We provide an interactive viewer where readers can visualize generated samples and the generation process. Our results highlight discrete diffusion as a promising framework for 3D sparse voxel generative modeling.</li>
<li><strong>摘要：</strong>由于体素结构的立方内存缩放以及稀疏性引起的明显类不平衡，因此很难生成逼真的稀疏多类别3D体素结构。我们引入了脚手架扩散，这是一种用于稀疏多类别3D体素结构的生成模型。通过将体素视为令牌，支架扩散使用离散扩散语言模型来生成3D体素结构。我们表明，离散扩散语言模型可以扩展到固有的顺序域（例如文本），以生成空间相干的3D结构。我们从3D-Craft数据集对Minecraft House结构进行了评估，并证明，与先前的基准和自动回归配方不同，支架扩散即使在对超过98％稀疏性的数据进行培训时也会产生现实且相干的结构。我们提供了一个交互式查看器，读者可以在其中可视化生成的样本和生成过程。我们的结果强调了离散扩散是3D稀疏体素生成建模的有希望的框架。</li>
</ul>

<h3>Title: SynCircuit: Automated Generation of New Synthetic RTL Circuits Can Enable Big Data in Circuits</h3>
<ul>
<li><strong>Authors: </strong>Shang Liu, Jing Wang, Wenji Fang, Zhiyao Xie</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00071">https://arxiv.org/abs/2509.00071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00071">https://arxiv.org/pdf/2509.00071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00071]] SynCircuit: Automated Generation of New Synthetic RTL Circuits Can Enable Big Data in Circuits(https://arxiv.org/abs/2509.00071)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In recent years, AI-assisted IC design methods have demonstrated great potential, but the availability of circuit design data is extremely limited, especially in the public domain. The lack of circuit data has become the primary bottleneck in developing AI-assisted IC design methods. In this work, we make the first attempt, SynCircuit, to generate new synthetic circuits with valid functionalities in the HDL format. SynCircuit automatically generates synthetic data using a framework with three innovative steps: 1) We propose a customized diffusion-based generative model to resolve the Directed Cyclic Graph (DCG) generation task, which has not been well explored in the AI community. 2) To ensure our circuit is valid, we enforce the circuit constraints by refining the initial graph generation outputs. 3) The Monte Carlo tree search (MCTS) method further optimizes the logic redundancy in the generated graph. Experimental results demonstrate that our proposed SynCircuit can generate more realistic synthetic circuits and enhance ML model performance in downstream circuit design tasks.</li>
<li><strong>摘要：</strong>近年来，AI辅助IC设计方法具有很大的潜力，但是电路设计数据的可用性极为有限，尤其是在公共领域。缺乏电路数据已成为开发AI辅助IC设计方法的主要瓶颈。在这项工作中，我们首次尝试Syncircuit，以生成具有HDL格式有效功能的新合成电路。 Syncircuit使用具有三个创新步骤的框架自动生成综合数据：1）我们提出了一个基于定制扩散的生成模型，以解决有向的循环图（DCG）生成任务，该任务在AI社区中尚未得到很好的探索。 2）为了确保我们的电路有效，我们通过完善初始图形生成输出来强制执行电路约束。 3）蒙特卡洛树搜索（MCT）方法进一步优化了生成图中的逻辑冗余。实验结果表明，我们提出的Syncircuit可以生成更逼真的合成回路，并在下游电路设计任务中增强ML模型性能。</li>
</ul>

<h3>Title: Mitigating Clinician Information Overload: Generative AI for Integrated EHR and RPM Data Analysis</h3>
<ul>
<li><strong>Authors: </strong>Ankit Shetgaonkar, Dipen Pradhan, Lakshit Arora, Sanjay Surendranath Girija, Shashank Kapoor, Aman Raj</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00073">https://arxiv.org/abs/2509.00073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00073">https://arxiv.org/pdf/2509.00073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00073]] Mitigating Clinician Information Overload: Generative AI for Integrated EHR and RPM Data Analysis(https://arxiv.org/abs/2509.00073)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative Artificial Intelligence (GenAI), particularly Large Language Models (LLMs), offer powerful capabilities for interpreting the complex data landscape in healthcare. In this paper, we present a comprehensive overview of the capabilities, requirements and applications of GenAI for deriving clinical insights and improving clinical efficiency. We first provide some background on the forms and sources of patient data, namely real-time Remote Patient Monitoring (RPM) streams and traditional Electronic Health Records (EHRs). The sheer volume and heterogeneity of this combined data present significant challenges to clinicians and contribute to information overload. In addition, we explore the potential of LLM-powered applications for improving clinical efficiency. These applications can enhance navigation of longitudinal patient data and provide actionable clinical decision support through natural language dialogue. We discuss the opportunities this presents for streamlining clinician workflows and personalizing care, alongside critical challenges such as data integration complexity, ensuring data quality and RPM data reliability, maintaining patient privacy, validating AI outputs for clinical safety, mitigating bias, and ensuring clinical acceptance. We believe this work represents the first summarization of GenAI techniques for managing clinician data overload due to combined RPM / EHR data complexities.</li>
<li><strong>摘要：</strong>生成人工智能（Genai），尤其是大型语言模型（LLM），为解释医疗保健中复杂的数据格局提供了有力的功能。在本文中，我们介绍了Genai在获得临床见解和提高临床效率方面的能力，要求和应用的全面概述。我们首先提供有关患者数据的形式和来源的背景，即实时远程患者监测（RPM）流和传统的电子健康记录（EHR）。该组合数据的纯粹数量和异质性给临床医生带来了重大挑战，并有助于信息超负荷。此外，我们还探讨了LLM驱动应用提高临床效率的潜力。这些应用可以增强纵向患者数据的导航，并通过自然语言对话提供可行的临床决策支持。我们讨论了这一给临床医生工作流程和个性化护理的机会，以及诸如数据集成复杂性，确保数据质量和RPM数据可靠性，保持患者隐私，验证AI的临床安全性，减轻偏见以及确保临床接受度的关键挑战。我们认为，这项工作代表了由于RPM / EHR数据复杂性而导致的Genai技术的首次摘要。</li>
</ul>

<h3>Title: Data Cartography for Detecting Memorization Hotspots and Guiding Data Interventions in Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Laksh Patel, Neel Shanbhag</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00083">https://arxiv.org/abs/2509.00083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00083">https://arxiv.org/pdf/2509.00083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00083]] Data Cartography for Detecting Memorization Hotspots and Guiding Data Interventions in Generative Models(https://arxiv.org/abs/2509.00083)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Modern generative models risk overfitting and unintentionally memorizing rare training examples, which can be extracted by adversaries or inflate benchmark performance. We propose Generative Data Cartography (GenDataCarto), a data-centric framework that assigns each pretraining sample a difficulty score (early-epoch loss) and a memorization score (frequency of ``forget events''), then partitions examples into four quadrants to guide targeted pruning and up-/down-weighting. We prove that our memorization score lower-bounds classical influence under smoothness assumptions and that down-weighting high-memorization hotspots provably decreases the generalization gap via uniform stability bounds. Empirically, GenDataCarto reduces synthetic canary extraction success by over 40\% at just 10\% data pruning, while increasing validation perplexity by less than 0.5\%. These results demonstrate that principled data interventions can dramatically mitigate leakage with minimal cost to generative performance.</li>
<li><strong>摘要：</strong>现代生成模型有过度拟合和无意识地记住罕见训练例子的风险，可以通过对手提取或膨胀基准表现。我们提出了一个以数据为中心的框架（Gendatacarto）的生成数据制图（Gendatacarto），该框架为每个预处理样本分配了难度分数（早期 - 上面的损失）和记忆分数（``忘记事件''的频率），然后将分区示例分为四个象限，以指导目标的降临和上/下/下/下/下降。我们证明，在平滑度假设下，我们的记忆得分较低的经典影响力，并且降低加权的高度迁移热点可通过统一的稳定性界限降低概括差距。从经验上讲，Gendatacarto在仅10 \％的数据修剪时将合成的金丝雀萃取成功降低了40 \％，同时将验证的困惑增加了小于0.5 \％。这些结果表明，有原则的数据干预措施可以大大减轻泄漏，而生成性能的成本最低。</li>
</ul>

<h3>Title: Learning to Refine: Self-Refinement of Parallel Reasoning in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Qibin Wang, Pu Zhao, Shaohan Huang, Fangkai Yang, Lu Wang, Furu Wei, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00084">https://arxiv.org/abs/2509.00084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00084">https://arxiv.org/pdf/2509.00084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00084]] Learning to Refine: Self-Refinement of Parallel Reasoning in LLMs(https://arxiv.org/abs/2509.00084)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>To further enhance the ability of Large Language Models (LLMs) to solve complex, multi-step reasoning problems, test-time scaling (TTS) methods have gained widespread attention. Existing approaches such as Best-of-N and majority voting are limited as their performance depends on the quality of candidate responses, making them unable to produce a correct solution when all candidates are incorrect. Introducing an additional model to select the best response also incurs significant deployment costs. To this end, we introduce Generative Self-Refinement (GSR), a novel parallel test-time scaling framework where a unified model first generates a set of candidate responses in parallel and then performs self-refinement to synthesize a new superior solution based on a prompt consisting of the problem and these candidates. However, LLMs struggle to perform refinement effectively when prompted directly. Therefore, we design a hybrid training pipeline by jointly optimizing for two complementary objectives, solving problems directly and refining candidate responses. Experimental results demonstrate that our method achieves state-of-the-art performance across five mathematical benchmarks. We further show that this learned self-refinement skill is a model-agnostic enhancement, robust across different model scales and generalizing to out-of-distribution reasoning tasks.</li>
<li><strong>摘要：</strong>为了进一步提高大语模型（LLM）解决复杂，多步推理问题的能力，测试时间缩放（TTS）方法已引起广泛关注。现有的方法（例如最佳N和多数投票）受到限制，因为它们的绩效取决于候选人的响应质量，因此当所有候选人不正确时，它们都无法提供正确的解决方案。引入一个额外的模型以选择最佳响应还会产生大量的部署成本。为此，我们介绍了一种新型的平行测试时间缩放框架（统一模型首先生成一组候选响应，然后执行自我进行，然后根据问题和这些候选人组成的提示来综合一个新的优质解决方案，因此我们引入了一种新型的平行测试时间缩放框架（GSR），该统一模型首先生成一组候选响应。但是，在直接提示时，LLMS难以有效地进行精炼。因此，我们通过共同优化两个互补目标，直接解决问题并提炼候选反应来设计混合培训管道。实验结果表明，我们的方法在五个数学基准中实现了最先进的性能。我们进一步表明，这种学到的自我启动技能是一种模型不合时宜的增强，在不同的模型量表上进行了强大的稳定性，并推广到分布范围的推理任务。</li>
</ul>

<h3>Title: Robust Detection of Synthetic Tabular Data under Schema Variability</h3>
<ul>
<li><strong>Authors: </strong>G. Charbel N. Kindji (MALT), Elisa Fromont (MALT), Lina Maria Rojas-Barahona, Tanguy Urvoy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00092">https://arxiv.org/abs/2509.00092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00092">https://arxiv.org/pdf/2509.00092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00092]] Robust Detection of Synthetic Tabular Data under Schema Variability(https://arxiv.org/abs/2509.00092)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rise of powerful generative models has sparked concerns over data authenticity. While detection methods have been extensively developed for images and text, the case of tabular data, despite its ubiquity, has been largely overlooked. Yet, detecting synthetic tabular data is especially challenging due to its heterogeneous structure and unseen formats at test time. We address the underexplored task of detecting synthetic tabular data in the wild, where tables have variable and previously unseen schemas. We introduce a novel datum-wise transformer architecture that significantly outperforms the only previously published baseline, improving both AUC and accuracy by 7 points. By incorporating a table-adaptation component, our model gains an additional 7 accuracy points, demonstrating enhanced robustness. This work provides the first strong evidence that detecting synthetic tabular data in real-world conditions is not only feasible, but can be done with high reliability.</li>
<li><strong>摘要：</strong>强大的生成模型的兴起引发了人们对数据真实性的关注。尽管检测方法已经为图像和文本广泛开发，但表格数据的情况在很大程度上被忽略了。但是，由于其异质结构和测试时格式看不见，检测合成表格数据尤其具有挑战性。我们解决了在野外检测合成表格数据的未置信的任务，在该数据中，表具有可变且以前看不见的模式。我们介绍了一种新颖的基准变压器体系结构，该架构的表现可以大大优于唯一发表的基线，将AUC和精度提高了7分。通过合并桌子适应成分，我们的模型获得了7个精度点，证明了鲁棒性增强。这项工作提供了第一个有力的证据，表明在现实情况下检测合成表格数据不仅是可行的，而且可以使用高可靠性来完成。</li>
</ul>

<h3>Title: Financial Decision Making using Reinforcement Learning with Dirichlet Priors and Quantum-Inspired Genetic Optimization</h3>
<ul>
<li><strong>Authors: </strong>Prasun Nandy, Debjit Dhar, Rik Das</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00095">https://arxiv.org/abs/2509.00095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00095">https://arxiv.org/pdf/2509.00095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00095]] Financial Decision Making using Reinforcement Learning with Dirichlet Priors and Quantum-Inspired Genetic Optimization(https://arxiv.org/abs/2509.00095)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Traditional budget allocation models struggle with the stochastic and nonlinear nature of real-world financial data. This study proposes a hybrid reinforcement learning (RL) framework for dynamic budget allocation, enhanced with Dirichlet-inspired stochasticity and quantum mutation-based genetic optimization. Using Apple Inc. quarterly financial data (2009 to 2025), the RL agent learns to allocate budgets between Research and Development and Selling, General and Administrative to maximize profitability while adhering to historical spending patterns, with L2 penalties discouraging unrealistic deviations. A Dirichlet distribution governs state evolution to simulate shifting financial contexts. To escape local minima and improve generalization, the trained policy is refined using genetic algorithms with quantum mutation via parameterized qubit rotation circuits. Generation-wise rewards and penalties are logged to visualize convergence and policy behavior. On unseen fiscal data, the model achieves high alignment with actual allocations (cosine similarity 0.9990, KL divergence 0.0023), demonstrating the promise of combining deep RL, stochastic modeling, and quantum-inspired heuristics for adaptive enterprise budgeting.</li>
<li><strong>摘要：</strong>传统的预算分配模型在现实世界财务数据的随机性和非线性性质上挣扎。这项研究提出了一个动态预算分配的混合增强学习框架（RL）框架，并通过迪利奇（Dirichlet）启发的随机性和基于量子突变的遗传优化增强。使用Apple Inc.季刊（2009年至2025年），RL代理商学会了在研发和销售之间分配预算，一般和行政，以最大程度地提高盈利能力，同时遵守历史支出模式，而L2罚款阻止了不切实际的偏差。 Dirichlet分布管理国家进化，以模拟转移的财务环境。为了逃避局部最小值并改善概括，使用具有参数化Qubit旋转电路的量子突变的遗传算法来完善训练的策略。通过生成奖励和惩罚来可视化融合和政策行为。在看不见的财政数据上，该模型与实际分配（余弦相似性0.9990，KL Divergence 0.0023）实现了高度对齐，这表明有望结合深度RL，随机建模和量子启发的启发式启发式启发式方法，以适应适应性企业预算。</li>
</ul>

<h3>Title: Principled Approximation Methods for Efficient and Scalable Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Pedro Savarese</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00174">https://arxiv.org/abs/2509.00174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00174">https://arxiv.org/pdf/2509.00174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00174]] Principled Approximation Methods for Efficient and Scalable Deep Learning(https://arxiv.org/abs/2509.00174)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent progress in deep learning has been driven by increasingly larger models. However, their computational and energy demands have grown proportionally, creating significant barriers to their deployment and to a wider adoption of deep learning technologies. This thesis investigates principled approximation methods for improving the efficiency of deep learning systems, with a particular focus on settings that involve discrete constraints and non-differentiability. We study three main approaches toward improved efficiency: architecture design, model compression, and optimization. For model compression, we propose novel approximations for pruning and quantization that frame the underlying discrete problem as continuous and differentiable, enabling gradient-based training of compression schemes alongside the model's parameters. These approximations allow for fine-grained sparsity and precision configurations, leading to highly compact models without significant fine-tuning. In the context of architecture design, we design an algorithm for neural architecture search that leverages parameter sharing across layers to efficiently explore implicitly recurrent architectures. Finally, we study adaptive optimization, revisiting theoretical properties of widely used methods and proposing an adaptive optimizer that allows for quick hyperparameter tuning. Our contributions center on tackling computationally hard problems via scalable and principled approximations. Experimental results on image classification, language modeling, and generative modeling tasks show that the proposed methods provide significant improvements in terms of training and inference efficiency while maintaining, or even improving, the model's performance.</li>
<li><strong>摘要：</strong>深度学习的最新进展是由越来越大的模型驱动的。但是，它们的计算和能源需求已经按比例增长，为其部署和更广泛的深度学习技术造成了重大障碍。本文研究了原则上的近似方法来提高深度学习系统的效率，特别关注涉及离散约束和非差异性的设置。我们研究提高效率的三种主要方法：建筑设计，模型压缩和优化。对于模型压缩，我们提出了用于修剪和量化的新型近似值，将基本离散问题构建为连续且可区分的，从而可以基于梯度的压缩方案训练以及模型的参数。这些近似值允许细粒度的稀疏性和精确构型，从而导致高度紧凑的模型而没有明显的微调。在体系结构设计的背景下，我们设计了一种用于神经体系结构搜索的算法，该算法利用跨层的参数共享，以有效地探索隐式的重复架构。最后，我们研究自适应优化，重新审视广泛使用方法的理论特性，并提出一种自适应优化器，以快速进行超级参数调整。我们的贡献中心通过可扩展和原则性近似来解决计算困难问题。图像分类，语言建模和生成建模任务的实验结果表明，所提出的方法在培训和推理效率方面提供了显着改善，同时维护甚至改进了模型的性能。</li>
</ul>

<h3>Title: Category-level Text-to-Image Retrieval Improved: Bridging the Domain Gap with Diffusion Models and Vision Encoders</h3>
<ul>
<li><strong>Authors: </strong>Faizan Farooq Khan, Vladan Stojnić, Zakaria Laskar, Mohamed Elhoseiny, Giorgos Tolias</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00177">https://arxiv.org/abs/2509.00177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00177">https://arxiv.org/pdf/2509.00177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00177]] Category-level Text-to-Image Retrieval Improved: Bridging the Domain Gap with Diffusion Models and Vision Encoders(https://arxiv.org/abs/2509.00177)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>This work explores text-to-image retrieval for queries that specify or describe a semantic category. While vision-and-language models (VLMs) like CLIP offer a straightforward open-vocabulary solution, they map text and images to distant regions in the representation space, limiting retrieval performance. To bridge this modality gap, we propose a two-step approach. First, we transform the text query into a visual query using a generative diffusion model. Then, we estimate image-to-image similarity with a vision model. Additionally, we introduce an aggregation network that combines multiple generated images into a single vector representation and fuses similarity scores across both query modalities. Our approach leverages advancements in vision encoders, VLMs, and text-to-image generation models. Extensive evaluations show that it consistently outperforms retrieval methods relying solely on text queries. Source code is available at: this https URL</li>
<li><strong>摘要：</strong>这项工作探讨了指定或描述语义类别的查询的文本对图像检索。虽然视觉和语言模型（VLM）（例如剪辑）提供了一个直接的开放式唱片解决方案，但它们将文本和图像映射到表示空间中的遥远区域，从而限制了检索性能。为了弥合这种方式差距，我们提出了一种两步方法。首先，我们使用生成扩散模型将文本查询转换为视觉查询。然后，我们与视觉模型估计图像到图像相似性。此外，我们引入了一个聚合网络，该网络将多个生成的图像结合到单个向量表示中，并融合了两个查询模式的相似性分数。我们的方法利用了视觉编码，VLM和文本对图像生成模型的进步。广泛的评估表明，它始终优于仅依靠文本查询的检索方法。源代码可用：此HTTPS URL</li>
</ul>

<h3>Title: Democratizing Agentic AI with Fast Test-Time Scaling on the Edge</h3>
<ul>
<li><strong>Authors: </strong>Hao Mark Chen, Zhiwen Mo, Guanxi Lu, Shuang Liang, Lingxiao Ma, Wayne Luk, Hongxiang Fan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00195">https://arxiv.org/abs/2509.00195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00195">https://arxiv.org/pdf/2509.00195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00195]] Democratizing Agentic AI with Fast Test-Time Scaling on the Edge(https://arxiv.org/abs/2509.00195)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Deploying agentic AI on edge devices is crucial for privacy and responsiveness, but memory constraints typically relegate these systems to smaller Large Language Models (LLMs) with inferior reasoning capabilities. Test-Time Scaling (TTS) can bridge this reasoning gap by dedicating more compute during inference, but existing methods incur prohibitive overhead on edge hardware. To overcome this, we introduce FlashTTS, a serving system that makes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces three synergistic optimizations: (i) Speculative Beam Extension to mitigate system stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model Memory Allocation to dynamically balance memory between generation and verification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache reuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on a single consumer GPU (24 GB) to match the accuracy and latency of large cloud models. Our evaluation demonstrates that FlashTTS achieves an average 2.2x higher goodput and reduces latency by 38%-68% compared to a vLLM baseline, paving the way for democratized, high-performance agentic AI on edge devices.</li>
<li><strong>摘要：</strong>在边缘设备上部署代理AI对于隐私和响应性至关重要，但是内存约束通常将这些系统降级到具有劣质推理能力的较小的大型语言模型（LLMS）。测试时间缩放（TTS）可以通过在推理期间专用更多计算来弥合这一推理差距，但是现有的方法在边缘硬件上产生了刺激性的开销。为了克服这一点，我们介绍了FlashTTS，这是一种使TTS实用的服务系统，可用于内存约束的LLM推理。 FlashTTS引入了三个协同优化：（i）从不规则推理路径中减轻系统散落者的投机梁扩展； （ii）非对称多模型内存分配，以动态平衡生成和验证之间的记忆； （iii）动态前缀感知计划，以最大化KV-CACHE重复使用。 FlashTTS作为VLLM的插件库构建，可在单个消费者GPU（24 GB）上启用Edge LLM，以匹配大型云模型的准确性和延迟。我们的评估表明，与VLLM基线相比，FlashTTS平均达到2.2倍，并将延迟降低38％-68％，为在边缘设备上民主化，高性能的代理AI铺平了道路。</li>
</ul>

<h3>Title: Generative AI for Industrial Contour Detection: A Language-Guided Vision System</h3>
<ul>
<li><strong>Authors: </strong>Liang Gong, Tommy (Zelin)Wang, Sara Chaker, Yanchen Dong, Fouad Bousetouane, Brenden Morton, Mark Mendez</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00284">https://arxiv.org/abs/2509.00284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00284">https://arxiv.org/pdf/2509.00284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00284]] Generative AI for Industrial Contour Detection: A Language-Guided Vision System(https://arxiv.org/abs/2509.00284)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Industrial computer vision systems often struggle with noise, material variability, and uncontrolled imaging conditions, limiting the effectiveness of classical edge detectors and handcrafted pipelines. In this work, we present a language-guided generative vision system for remnant contour detection in manufacturing, designed to achieve CAD-level precision. The system is organized into three stages: data acquisition and preprocessing, contour generation using a conditional GAN, and multimodal contour refinement through vision-language modeling, where standardized prompts are crafted in a human-in-the-loop process and applied through image-text guided synthesis. On proprietary FabTrack datasets, the proposed system improved contour fidelity, enhancing edge continuity and geometric alignment while reducing manual tracing. For the refinement stage, we benchmarked several vision-language models, including Google's Gemini 2.0 Flash, OpenAI's GPT-image-1 integrated within a VLM-guided workflow, and open-source baselines. Under standardized conditions, GPT-image-1 consistently outperformed Gemini 2.0 Flash in both structural accuracy and perceptual quality. These findings demonstrate the promise of VLM-guided generative workflows for advancing industrial computer vision beyond the limitations of classical pipelines.</li>
<li><strong>摘要：</strong>工业计算机视觉系统通常会在噪声，物质可变性和不受控制的成像条件上挣扎，从而限制了经典边缘检测器和手工管道的有效性。在这项工作中，我们提出了一种语言引导的生成视觉系统，用于制造中的残留轮廓检测，旨在实现CAD级的精度。该系统分为三个阶段：数据采集和预处理，使用有条件的GAN进行轮廓生成以及通过视觉语言建模进行多模式轮廓改进，在该模型中，标准化的提示是在人类在循环过程中制成的，并通过图像文本引导的综合进行应用。在专有的Fabtrack数据集上，提议的系统改善了轮廓保真度，增强了边缘连续性和几何形状对准，同时减少了手动跟踪。在改进阶段，我们对几种视觉语言模型进行了基准测试，包括Google的Gemini 2.0 Flash，OpenAI的GPT-Image-1集成在VLM指导的工作流程中，以及开源的基线。在标准化条件下，GPT-Image-1在结构精确度和感知质量方面始终优于Gemini 2.0闪光灯。这些发现证明了VLM引导的生成工作流程的希望超出了经典管道的局限性。</li>
</ul>

<h3>Title: Are We Really Learning the Score Function? Reinterpreting Diffusion Models Through Wasserstein Gradient Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>An B. Vuong, Michael T. McCann, Javier E. Santos, Yen Ting Lin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00336">https://arxiv.org/abs/2509.00336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00336">https://arxiv.org/pdf/2509.00336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00336]] Are We Really Learning the Score Function? Reinterpreting Diffusion Models Through Wasserstein Gradient Flow Matching(https://arxiv.org/abs/2509.00336)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion models are commonly interpreted as learning the score function, i.e., the gradient of the log-density of noisy data. However, this assumption implies that the target of learning is a conservative vector field, which is not enforced by the neural network architectures used in practice. We present numerical evidence that trained diffusion networks violate both integral and differential constraints required of true score functions, demonstrating that the learned vector fields are not conservative. Despite this, the models perform remarkably well as generative mechanisms. To explain this apparent paradox, we advocate a new theoretical perspective: diffusion training is better understood as flow matching to the velocity field of a Wasserstein Gradient Flow (WGF), rather than as score learning for a reverse-time stochastic differential equation. Under this view, the "probability flow" arises naturally from the WGF framework, eliminating the need to invoke reverse-time SDE theory and clarifying why generative sampling remains successful even when the neural vector field is not a true score. We further show that non-conservative errors from neural approximation do not necessarily harm density transport. Our results advocate for adopting the WGF perspective as a principled, elegant, and theoretically grounded framework for understanding diffusion generative models.</li>
<li><strong>摘要：</strong>扩散模型通常被解释为学习得分函数，即嘈杂数据的对数密度的梯度。但是，该假设意味着学习的目标是保守的向量领域，这不是由实践中使用的神经网络体系结构强制执行的。我们提供了数值证据，即受过训练的扩散网络违反了真实分数函数所需的积分和差异约束，这表明学到的向量场不保守。尽管如此，这些模型作为生成机制的表现非常好。为了解释这一明显的悖论，我们提倡一种新的理论观点：更好地理解了扩散训练是与瓦斯坦斯坦梯度流（WGF）的速度场相匹配的流，而不是为反向时间随机微分方程的得分学习。在此观点下，“概率流”自然而然地来自WGF框架，消除了需要调用反时SDE理论的需求，并阐明了为什么即使神经向量场不是真正的分数，生成抽样即使生成抽样仍然成功。我们进一步表明，神经近似产生的非保守误差不一定会损害密度转运。我们的结果主张将WGF视角作为一种原则，优雅和理论上的框架来理解扩散生成模型。</li>
</ul>

<h3>Title: Activation Steering Meets Preference Optimization: Defense Against Jailbreaks in Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sihao Wu, Gaojie Jin, Wei Huang, Jianhong Wang, Xiaowei Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00373">https://arxiv.org/abs/2509.00373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00373">https://arxiv.org/pdf/2509.00373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00373]] Activation Steering Meets Preference Optimization: Defense Against Jailbreaks in Vision Language Models(https://arxiv.org/abs/2509.00373)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision Language Models (VLMs) have demonstrated impressive capabilities in integrating visual and textual information for understanding and reasoning, but remain highly vulnerable to adversarial attacks. While activation steering has emerged as a promising defence, existing approaches often rely on task-specific contrastive prompts to extract harmful directions, which exhibit suboptimal performance and can degrade visual grounding performance. To address these limitations, we propose \textit{Sequence-Level Preference Optimization} for VLM (\textit{SPO-VLM}), a novel two-stage defense framework that combines activation-level intervention with policy-level optimization to enhance model robustness. In \textit{Stage I}, we compute adaptive layer-specific steering vectors from diverse data sources, enabling generalized suppression of harmful behaviors during inference. In \textit{Stage II}, we refine these steering vectors through a sequence-level preference optimization process. This stage integrates automated toxicity assessment, as well as visual-consistency rewards based on caption-image alignment, to achieve safe and semantically grounded text generation. The two-stage structure of SPO-VLM balances efficiency and effectiveness by combining a lightweight mitigation foundation in Stage I with deeper policy refinement in Stage II. Extensive experiments shown SPO-VLM enhances safety against attacks via activation steering and preference optimization, while maintaining strong performance on benign tasks without compromising visual understanding capabilities. We will release our code, model weights, and evaluation toolkit to support reproducibility and future research. \textcolor{red}{Warning: This paper may contain examples of offensive or harmful text and images.}</li>
<li><strong>摘要：</strong>视觉语言模型（VLM）在整合视觉和文本信息以进行理解和推理方面表现出了令人印象深刻的能力，但仍然很容易受到对抗性攻击的影响。尽管激活转向已经成为一种有前途的防御，但现有方法通常依赖于特定于任务的对比提示来提取有害方向，这些方向表现出次优性能并可以降低视觉接地性能。为了解决这些限制，我们建议VLM（\ textIt {spo-vlm}）\ textIt {sequence-Level首选项优化}，这是一个新颖的两阶段防御框架，将激活级干预与策略级别的优化结合起来，以增强模型鲁棒性。在\ textit {age i}中，我们从不同的数据源中计算自适应层特异性转向向量，从而在推断过程中可以广泛地抑制有害行为。在\ textIt {agepti ii}中，我们通过序列级别的偏好优化过程来完善这些转向向量。该阶段将自动毒性评估以及基于字幕图像对齐方式的视觉持续奖励整​​合在一起，以实现安全和语义上的文本生成。 SPO-VLM的两阶段结构通过将轻量缓解基础结合在第一阶段的效率和有效性之间，并在II阶段进行更深入的政策改进。广泛的实验表明，SPO-VLM通过激活转向和偏好优化提高了对攻击的安全性，同时在不损害视觉理解能力的情况下保持了良性任务的强劲表现。我们将发布代码，模型权重和评估工具包，以支持可重复性和未来研究。 \ TextColor {Red} {警告：本文可能包含令人反感或有害文本和图像的示例。}</li>
</ul>

<h3>Title: NoiseCutMix: A Novel Data Augmentation Approach by Mixing Estimated Noise in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Shumpei Takezaki, Ryoma Bise, Shinnosuke Matsuo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00378">https://arxiv.org/abs/2509.00378</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00378">https://arxiv.org/pdf/2509.00378</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00378]] NoiseCutMix: A Novel Data Augmentation Approach by Mixing Estimated Noise in Diffusion Models(https://arxiv.org/abs/2509.00378)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In this study, we propose a novel data augmentation method that introduces the concept of CutMix into the generation process of diffusion models, thereby exploiting both the ability of diffusion models to generate natural and high-resolution images and the characteristic of CutMix, which combines features from two classes to create diverse augmented data. Representative data augmentation methods for combining images from multiple classes include CutMix and MixUp. However, techniques like CutMix often result in unnatural boundaries between the two images due to contextual differences. Therefore, in this study, we propose a method, called NoiseCutMix, to achieve natural, high-resolution image generation featuring the fused characteristics of two classes by partially combining the estimated noise corresponding to two different classes in a diffusion model. In the classification experiments, we verified the effectiveness of the proposed method by comparing it with conventional data augmentation techniques that combine multiple classes, random image generation using Stable Diffusion, and combinations of these methods. Our codes are available at: this https URL</li>
<li><strong>摘要：</strong>在这项研究中，我们提出了一种新型的数据增强方法，该方法将cutmix的概念引入了扩散模型的生成过程中，从而利用了扩散模型生成自然和高分辨率图像的能力以及CutMix的特征，该图像结合了两个类别创建多样化的增强数据的特征。用于组合多个类图像的代表性数据增强方法包括cutmix和Mixup。但是，由于上下文差异，诸如cutmix之类的技术通常会导致两个图像之间的不自然边界。因此，在这项研究中，我们提出了一种称为noisecutmix的方法，以通过部分组合相对于扩散模型中的两个不同类别的估计噪声来实现自然的高分辨率图像生成。在分类实验中，我们通过将其与常规数据增强技术进行比较，验证了提出的方法的有效性，这些数据结合了多个类别，使用稳定扩散的随机图像生成以及这些方法的组合。我们的代码可用：此HTTPS URL</li>
</ul>

<h3>Title: Visually Grounded Narratives: Reducing Cognitive Burden in Researcher-Participant Interaction</h3>
<ul>
<li><strong>Authors: </strong>Runtong Wu, Jiayao Song, Fei Teng, Xianhao Ren, Yuyan Gao, Kailun Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00381">https://arxiv.org/abs/2509.00381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00381">https://arxiv.org/pdf/2509.00381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00381]] Visually Grounded Narratives: Reducing Cognitive Burden in Researcher-Participant Interaction(https://arxiv.org/abs/2509.00381)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Narrative inquiry has been one of the prominent application domains for the analysis of human experience, aiming to know more about the complexity of human society. However, researchers are often required to transform various forms of data into coherent hand-drafted narratives in storied form throughout narrative analysis, which brings an immense burden of data analysis. Participants, too, are expected to engage in member checking and presentation of these narrative products, which involves reviewing and responding to large volumes of documents. Given the dual burden and the need for more efficient and participant-friendly approaches to narrative making and representation, we made a first attempt: (i) a new paradigm is proposed, NAME, as the initial attempt to push the field of narrative inquiry. Name is able to transfer research documents into coherent story images, alleviating the cognitive burden of interpreting extensive text-based materials during member checking for both researchers and participants. (ii) We develop an actor location and shape module to facilitate plausible image generation. (iii) We have designed a set of robust evaluation metrics comprising three key dimensions to objectively measure the perceptual quality and narrative consistency of generated characters. Our approach consistently demonstrates state-of-the-art performance across different data partitioning schemes. Remarkably, while the baseline relies on the full 100% of the available data, our method requires only 0.96% yet still reduces the FID score from 195 to 152. Under identical data volumes, our method delivers substantial improvements: for the 70:30 split, the FID score decreases from 175 to 152, and for the 95:5 split, it is nearly halved from 96 to 49. Furthermore, the proposed model achieves a score of 3.62 on the newly introduced metric, surpassing the baseline score of 2.66.</li>
<li><strong>摘要：</strong>叙事探究一直是分析人类经验的著名应用领域之一，旨在更多地了解人类社会的复杂性。但是，通常需要研究人员将各种形式的数据转换为在整个叙事分析中以传奇形式的一致手工脱落的叙述，这带来了巨大的数据分析负担。还希望参与者参与成员检查和介绍这些叙事产品，其中涉及审查和响应大量文档。鉴于双重负担以及对叙事制作和代表制的更有效和参与者的方法的需求，我们首次尝试：（i）提出了新的范式，名称为最初尝试推动叙事询问领域的尝试。名称能够将研究文件转移到连贯的故事图像中，从而减轻了在研究人员和参与者检查会员检查过程中解释广泛的基于文本材料的认知负担。 （ii）我们开发了一个演员位置和形状模块，以促进合理的图像产生。 （iii）我们设计了一组强大的评估指标，其中包括三个关键维度，以客观地衡量产生的字符的感知质量和叙事一致性。我们的方法始终证明了不同数据分配方案的最新性能。值得注意的是，虽然基线依赖于整个可用数据的100％，但我们的方法仅需要0.96％，但仍将FID得分从195降低到152。在相同的数据量下，我们的方法可实现实质性改进：对于70:30，FID分数从175：175降低至152，并且速度差异为96：5分数，均为96级成绩。 3.62在新引入的度量标准中，超过了2.66的基线得分。</li>
</ul>

<h3>Title: DevilSight: Augmenting Monocular Human Avatar Reconstruction through a Virtual Perspective</h3>
<ul>
<li><strong>Authors: </strong>Yushuo Chen, Ruizhi Shao, Youxin Pang, Hongwen Zhang, Xinyi Wu, Rihui Wu, Yebin Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00403">https://arxiv.org/abs/2509.00403</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00403">https://arxiv.org/pdf/2509.00403</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00403]] DevilSight: Augmenting Monocular Human Avatar Reconstruction through a Virtual Perspective(https://arxiv.org/abs/2509.00403)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We present a novel framework to reconstruct human avatars from monocular videos. Recent approaches have struggled either to capture the fine-grained dynamic details from the input or to generate plausible details at novel viewpoints, which mainly stem from the limited representational capacity of the avatar model and insufficient observational data. To overcome these challenges, we propose to leverage the advanced video generative model, Human4DiT, to generate the human motions from alternative perspective as an additional supervision signal. This approach not only enriches the details in previously unseen regions but also effectively regularizes the avatar representation to mitigate artifacts. Furthermore, we introduce two complementary strategies to enhance video generation: To ensure consistent reproduction of human motion, we inject the physical identity into the model through video fine-tuning. For higher-resolution outputs with finer details, a patch-based denoising algorithm is employed. Experimental results demonstrate that our method outperforms recent state-of-the-art approaches and validate the effectiveness of our proposed strategies.</li>
<li><strong>摘要：</strong>我们提出了一个新颖的框架，可以从单眼视频中重建人类化身。最近的方法努力捕获从输入中捕获细粒的动态细节，或者在新的观点上生成了合理的细节，这主要源于阿凡达模型的有限代表能力和不足的观察数据。为了克服这些挑战，我们建议利用先进的视频生成模型Human4Dit，从另类角度作为额外的监督信号产生人类动作。这种方法不仅丰富了以前看不见的区域中的细节，而且还可以有效地使头像表示以减轻伪影。此外，我们介绍了两种互补的策略来增强视频的生成：为了确保人类运动的持续再现，我们通过视频微调将物理身份注入模型。对于具有更细节的高分辨率输出，采用了基于补丁的DeNoising算法。实验结果表明，我们的方法表现优于最新的方法，并验证了我们提出的策略的有效性。</li>
</ul>

<h3>Title: Mixture of Global and Local Experts with Diffusion Transformer for Controllable Face Generation</h3>
<ul>
<li><strong>Authors: </strong>Xuechao Zou, Shun Zhang, Xing Fu, Yue Li, Kai Li, Yushe Cao, Congyan Lang, Pin Tao, Junliang Xing</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00428">https://arxiv.org/abs/2509.00428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00428">https://arxiv.org/pdf/2509.00428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00428]] Mixture of Global and Local Experts with Diffusion Transformer for Controllable Face Generation(https://arxiv.org/abs/2509.00428)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Controllable face generation poses critical challenges in generative modeling due to the intricate balance required between semantic controllability and photorealism. While existing approaches struggle with disentangling semantic controls from generation pipelines, we revisit the architectural potential of Diffusion Transformers (DiTs) through the lens of expert specialization. This paper introduces Face-MoGLE, a novel framework featuring: (1) Semantic-decoupled latent modeling through mask-conditioned space factorization, enabling precise attribute manipulation; (2) A mixture of global and local experts that captures holistic structure and region-level semantics for fine-grained controllability; (3) A dynamic gating network producing time-dependent coefficients that evolve with diffusion steps and spatial locations. Face-MoGLE provides a powerful and flexible solution for high-quality, controllable face generation, with strong potential in generative modeling and security applications. Extensive experiments demonstrate its effectiveness in multimodal and monomodal face generation settings and its robust zero-shot generalization capability. Project page is available at this https URL.</li>
<li><strong>摘要：</strong>由于语义可控性和光真相之间所需的复杂平衡，可控的面部生成在生成建模中构成了关键挑战。尽管现有的方法与发电管道的分离语义控制斗争时，我们通过专家专业镜头来重新审视扩散变压器（DIT）的建筑潜力。本文介绍了Face-Mogle，这是一个新颖的框架，其中包括：（1）通过面膜条件的空间分解的语义脱落潜在建模，从而实现精确的属性操纵； （2）全球和本地专家的混合物，可捕获整体结构和区域级语义，以实现细粒度的可控性； （3）动态门控网络产生时间相关系数，以扩散步骤和空间位置进化。 Face-Mogle为高质量，可控制的面部生成提供了强大而灵活的解决方案，具有强大的生成建模和安全应用程序的潜力。广泛的实验证明了其在多模式和单形成面部生成环境中的有效性及其稳健的零光概括能力。项目页面可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: VideoRewardBench: Comprehensive Evaluation of Multimodal Reward Models for Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Zhihong Zhang, Xiaojian Huang, Jin Xu, Zhuodong Luo, Xinzhi Wang, Jiansheng Wei, Xuejin Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00484">https://arxiv.org/abs/2509.00484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00484">https://arxiv.org/pdf/2509.00484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00484]] VideoRewardBench: Comprehensive Evaluation of Multimodal Reward Models for Video Understanding(https://arxiv.org/abs/2509.00484)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Multimodal reward models (MRMs) play a crucial role in the training, inference, and evaluation of Large Vision Language Models (LVLMs) by assessing response quality. However, existing benchmarks for evaluating MRMs in the video domain suffer from a limited number and diversity of questions, a lack of comprehensive evaluation dimensions, and inadequate evaluation of diverse types of MRMs. To address these gaps, we introduce VideoRewardBench, the first comprehensive benchmark covering four core aspects of video understanding: perception, knowledge, reasoning, and safety. Through our AI-assisted data pipeline, we curate a high-quality preference dataset of 1,563 annotated samples, including 1,482 unique videos and 1,559 distinct questions--15 times the number found in the most question-rich prior benchmark. Each sample is a triplet consisting of a video-text prompt, a chosen response, and a rejected response. We also conduct a comprehensive evaluation across 28 multimodal reward models spanning three categories: generative, discriminative, and semi-scalar. Results show that even the top-performing model GPT-4o achieves only 57.0% overall accuracy, and the state-of-the-art open-source model Qwen2.5-VL-72B reaches merely 53.3%. Our analysis further reveals three key insights: (i) MRMs trained with reinforcement learning (RL) do not necessarily exhibit stronger cross-modal generalization than those trained without RL; (ii) except for discriminative MRMs, other types of MRMs across varying model capacities can benefit from inference-time scaling; and (iii) variations in input video frame count have different effects on different types of MRMs. We believe VideoRewardBench offers a challenging and valuable benchmark for advancing the evaluation and development of MRMs in the video domain.</li>
<li><strong>摘要：</strong>多模式奖励模型（MRMS）通过评估响应质量在训练，推理和评估大型视力语言模型（LVLM）中起着至关重要的作用。但是，现有用于评估视频域中MRM的基准遇到了数量有限和多样性，缺乏全面的评估维度以及对不同类型MRM的评估不足。为了解决这些差距，我们介绍了VideoReWardBench，这是第一个综合基准，涵盖了视频理解的四个核心方面：感知，知识，推理和安全。通过我们的AI辅助数据管道，我们策划了1,563个带注释的样本的高质量偏好数据集，包括1,482个独特的视频和1,559个不同的问题 -  15倍的数量是最有问题的先前基准的数量。每个样本都是由视频文本提示，选择的响应和拒绝的响应组成的三胞胎。我们还对28个多模式奖励模型进行了全面的评估：生成，歧视性和半标度。结果表明，即使表现最佳的GPT-4O也只能达到总体准确性57.0％，而最先进的开源模型QWEN2.5-VL-72B仅达到53.3％。我们的分析进一步揭示了三个关键见解：（i）接受强化学习训练的MRM并不一定表现出比没有RL训练的跨模式概括更强的； （ii）除歧视性MRM外，不同模型能力的其他类型的MRM可以受益于推理时间缩放； （iii）输入视频框架计数的变化对不同类型的MRM具有不同的影响。我们认为，VideOrdardWardBench为推进视频域中MRM的评估和开发提供了具有挑战性且有价值的基准。</li>
</ul>

<h3>Title: Localizing and Mitigating Memorization in Image Autoregressive Models</h3>
<ul>
<li><strong>Authors: </strong>Aditya Kasliwal, Franziska Boenisch, Adam Dziedzic</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00488">https://arxiv.org/abs/2509.00488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00488">https://arxiv.org/pdf/2509.00488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00488]] Localizing and Mitigating Memorization in Image Autoregressive Models(https://arxiv.org/abs/2509.00488)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Image AutoRegressive (IAR) models have achieved state-of-the-art performance in speed and quality of generated images. However, they also raise concerns about memorization of their training data and its implications for privacy. This work explores where and how such memorization occurs within different image autoregressive architectures by measuring a fine-grained memorization. The analysis reveals that memorization patterns differ across various architectures of IARs. In hierarchical per-resolution architectures, it tends to emerge early and deepen with resolutions, while in IARs with standard autoregressive per token prediction, it concentrates in later processing stages. These localization of memorization patterns are further connected to IARs' ability to memorize and leak training data. By intervening on their most memorizing components, we significantly reduce the capacity for data extraction from IARs with minimal impact on the quality of generated images. These findings offer new insights into the internal behavior of image generative models and point toward practical strategies for mitigating privacy risks.</li>
<li><strong>摘要：</strong>图像自回旋（IAR）模型已在生成图像的速度和质量方面实现了最先进的性能。但是，他们还引起了人们对培训数据的记忆及其对隐私的影响的担忧。这项工作探讨了通过测量细粒度的记忆，这种记忆在不同图像自回归体系结构中的何处发生。分析表明，在IAR的各种体系结构中，记忆模式有所不同。在层次的每分辨率体系结构中，它倾向于尽早出现并加深决议，而在具有标准的标记预测的IAR中，它集中于以后的处理阶段。记忆模式的这些定位进一步连接到IARS记忆和泄漏训练数据的能力。通过介入其最记忆的组件，我们大大降低了从IAR中提取数据的能力，对生成图像的质量影响很小。这些发现为图像生成模型的内部行为提供了新的见解，并指出了减轻隐私风险的实用策略。</li>
</ul>

<h3>Title: A Modality-agnostic Multi-task Foundation Model for Human Brain Imaging</h3>
<ul>
<li><strong>Authors: </strong>Peirong Liu, Oula Puonti, Xiaoling Hu, Karthik Gopinath, Annabel Sorby-Adams, Daniel C. Alexander, W. Taylor Kimberly, Juan E. Iglesias</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00549">https://arxiv.org/abs/2509.00549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00549">https://arxiv.org/pdf/2509.00549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00549]] A Modality-agnostic Multi-task Foundation Model for Human Brain Imaging(https://arxiv.org/abs/2509.00549)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent learning-based approaches have made astonishing advances in calibrated medical imaging like computerized tomography (CT), yet they struggle to generalize in uncalibrated modalities -- notably magnetic resonance (MR) imaging, where performance is highly sensitive to the differences in MR contrast, resolution, and orientation. This prevents broad applicability to diverse real-world clinical protocols. Here we introduce BrainFM, a modality-agnostic, multi-task vision foundation model for human brain imaging. With the proposed "mild-to-severe" intra-subject generation and "real-synth" mix-up training strategy, BrainFM is resilient to the appearance of acquired images (e.g., modality, contrast, deformation, resolution, artifacts), and can be directly applied to five fundamental brain imaging tasks, including image synthesis for CT and T1w/T2w/FLAIR MRI, anatomy segmentation, scalp-to-cortical distance, bias field estimation, and registration. We evaluate the efficacy of BrainFM on eleven public datasets, and demonstrate its robustness and effectiveness across all tasks and input modalities. Code is available at this https URL.</li>
<li><strong>摘要：</strong>最近的基于学习的方法使校准的医学成像（例如计算机断层扫描（CT））的进步令人惊讶，但它们努力以未校准的方式概括 - 尤其是磁共振成像（MR）成像，在这种情况下，性能对MR对比度，分辨率，分辨率和方向的差异高度敏感。这样可以防止对各种现实世界临床方案的广泛适用性。在这里，我们介绍了BrainFM，这是人类脑成像的一种模态，多任务视觉基础模型。借助提出的“轻度至重点”受试者内部和“实现”混合训练策略，BrainFM具有弹性，可抵御获得的图像的外观（例如，模态，模态，对比度，变形，分辨率，伪像），可以直接应用于五个基本的脑成像任务，包括用于CT/T1W的图像范围，包括五个基本的脑成像任务。头皮到皮层距离，偏置场估计和注册。我们评估了BrainFM对11个公共数据集的功效，并证明了其在所有任务和输入方式中的鲁棒性和有效性。代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: C-DiffDet+: Fusing Global Scene Context with Generative Denoising for High-Fidelity Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Abdellah Zakaria Sellam, Ilyes Benaissa, Salah Eddine Bekhouche, Abdenour Hadid, Vito Renó, Cosimo Distante</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00578">https://arxiv.org/abs/2509.00578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00578">https://arxiv.org/pdf/2509.00578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00578]] C-DiffDet+: Fusing Global Scene Context with Generative Denoising for High-Fidelity Object Detection(https://arxiv.org/abs/2509.00578)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Fine-grained object detection in challenging visual domains, such as vehicle damage assessment, presents a formidable challenge even for human experts to resolve reliably. While DiffusionDet has advanced the state-of-the-art through conditional denoising diffusion, its performance remains limited by local feature conditioning in context-dependent scenarios. We address this fundamental limitation by introducing Context-Aware Fusion (CAF), which leverages cross-attention mechanisms to integrate global scene context with local proposal features directly. The global context is generated using a separate dedicated encoder that captures comprehensive environmental information, enabling each object proposal to attend to scene-level understanding. Our framework significantly enhances the generative detection paradigm by enabling each object proposal to attend to comprehensive environmental information. Experimental results demonstrate an improvement over state-of-the-art models on the CarDD benchmark, establishing new performance benchmarks for context-aware object detection in fine-grained domains</li>
<li><strong>摘要：</strong>在具有挑战性的视觉领域（例如车辆损坏评估）中的细粒对象检测，即使人类专家可以可靠地解决，也提出了巨大的挑战。尽管diffusiondet通过有条件的denoing扩散推进了最新的，但其性能仍受到与上下文相关场景中局部特征条件的限制。我们通过引入上下文感知融合（CAF）来解决这一基本限制，该融合（CAF）利用跨注意机制将全球场景上下文与本地提案特征整合在一起。全局上下文是使用单独的专用编码器生成的，该编码器可捕获全面的环境信息，从而使每个对象提案都能参与场景级别的理解。我们的框架可以通过使每个对象提案能够参与全面的环境信息，从而大大增强了生成检测范例。实验结果表明，对CADD基准测试的最先进模型有所改善，为在细粒域中的上下文感知对象检测建立了新的性能基准测试</li>
</ul>

<h3>Title: TimeCopilot</h3>
<ul>
<li><strong>Authors: </strong>Azul Garza, Reneé Rosillo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00616">https://arxiv.org/abs/2509.00616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00616">https://arxiv.org/pdf/2509.00616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00616]] TimeCopilot(https://arxiv.org/abs/2509.00616)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce TimeCopilot, the first open-source agentic framework for forecasting that combines multiple Time Series Foundation Models (TSFMs) with Large Language Models (LLMs) through a single unified API. TimeCopilot automates the forecasting pipeline: feature analysis, model selection, cross-validation, and forecast generation, while providing natural language explanations and supporting direct queries about the future. The framework is LLM-agnostic, compatible with both commercial and open-source models, and supports ensembles across diverse forecasting families. Results on the large-scale GIFT-Eval benchmark show that TimeCopilot achieves state-of-the-art probabilistic forecasting performance at low cost. Our framework provides a practical foundation for reproducible, explainable, and accessible agentic forecasting systems.</li>
<li><strong>摘要：</strong>我们介绍了TimeCopilot，这是第一个用于预测的开源代理框架，该框架结合了多个时间序列基础模型（TSFM）与大语言模型（LLMS）通过单个统一API结合使用。 TimeCopilot自动化预测管道：功能分析，模型选择，交叉验证和预测生成，同时提供自然语言解释并支持有关未来的直接查询。该框架是LLM-Agnostic，与商业和开源模型兼容，并支持各种预测家庭的合奏。大规模的礼品基准的结果表明，时间上限局以低成本实现最先进的概率预测性能。我们的框架为可再现，可解释和可访问的代理预测系统提供了实用的基础。</li>
</ul>

<h3>Title: AMCR: A Framework for Assessing and Mitigating Copyright Risks in Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Zhipeng Yin, Zichong Wang, Avash Palikhe, Zhen Liu, Jun Liu, Wenbin Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00641">https://arxiv.org/abs/2509.00641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00641">https://arxiv.org/pdf/2509.00641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00641]] AMCR: A Framework for Assessing and Mitigating Copyright Risks in Generative Models(https://arxiv.org/abs/2509.00641)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative models have achieved impressive results in text to image tasks, significantly advancing visual content creation. However, this progress comes at a cost, as such models rely heavily on large-scale training data and may unintentionally replicate copyrighted elements, creating serious legal and ethical challenges for real-world deployment. To address these concerns, researchers have proposed various strategies to mitigate copyright risks, most of which are prompt based methods that filter or rewrite user inputs to prevent explicit infringement. While effective in handling obvious cases, these approaches often fall short in more subtle situations, where seemingly benign prompts can still lead to infringing outputs. To address these limitations, this paper introduces Assessing and Mitigating Copyright Risks (AMCR), a comprehensive framework which i) builds upon prompt-based strategies by systematically restructuring risky prompts into safe and non-sensitive forms, ii) detects partial infringements through attention-based similarity analysis, and iii) adaptively mitigates risks during generation to reduce copyright violations without compromising image quality. Extensive experiments validate the effectiveness of AMCR in revealing and mitigating latent copyright risks, offering practical insights and benchmarks for the safer deployment of generative models.</li>
<li><strong>摘要：</strong>生成模型在图像任务的文本中取得了令人印象深刻的结果，从而大大推动了视觉内容创建。但是，这一进步是有代价的，因为这样的模型在很大程度上依赖大规模的培训数据，并且可能无意中复制了受版权保护的要素，从而为现实世界部署构成了严重的法律和道德挑战。为了解决这些问题，研究人员提出了各种策略来减轻版权风险，其中大多数是基于及时的方法，可过滤或重写用户输入以防止明确侵权。尽管有效地处理明显的情况，但这些方法通常在更微妙的情况下缺乏，在这种情况下，看似良性的提示仍然会导致侵权产出。为了解决这些局限性，本文介绍了评估和减轻版权风险（AMCR），这是一个全面的框架，i）通过系统地重组风险提示为安全和非敏感形式，基于迅速的策略，ii）ii）ii）通过基于注意力的相似性分析来检测部分侵犯，而不必在较生成的情况下降低风险的质量，以降低易变的质量。广泛的实验验证了AMCR在揭示和减轻潜在版权风险，为生成模型更安全部署的实用见解和基准方面的有效性。</li>
</ul>

<h3>Title: Missing Data Imputation using Neural Cellular Automata</h3>
<ul>
<li><strong>Authors: </strong>Tin Luu, Binh Nguyen, Man Ngo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00651">https://arxiv.org/abs/2509.00651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00651">https://arxiv.org/pdf/2509.00651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00651]] Missing Data Imputation using Neural Cellular Automata(https://arxiv.org/abs/2509.00651)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>When working with tabular data, missingness is always one of the most painful problems. Throughout many years, researchers have continuously explored better and better ways to impute missing data. Recently, with the rapid development evolution in machine learning and deep learning, there is a new trend of leveraging generative models to solve the imputation task. While the imputing version of famous models such as Variational Autoencoders or Generative Adversarial Networks were investigated, prior work has overlooked Neural Cellular Automata (NCA), a powerful computational model. In this paper, we propose a novel imputation method that is inspired by NCA. We show that, with some appropriate adaptations, an NCA-based model is able to address the missing data imputation problem. We also provide several experiments to evidence that our model outperforms state-of-the-art methods in terms of imputation error and post-imputation performance.</li>
<li><strong>摘要：</strong>当使用表格数据时，丢失始终是最痛苦的问题之一。多年来，研究人员一直在不断探索越来越多的方法来估算丢失的数据。最近，随着机器学习和深度学习的快速发展，利用生成模型来解决插补任务的新趋势。虽然研究了著名模型（例如变异自动编码器或生成对抗网络）的插图版本，但先前的工作忽略了一种强大的计算模型的神经蜂窝自动机（NCA）。在本文中，我们提出了一种受NCA启发的新型插补方法。我们表明，通过一些适当的改编，基于NCA的模型能够解决丢失的数据插补问题。我们还提供了一些实验，以证明我们的模型在插入误差和输入后绩效方面优于最先进的方法。</li>
</ul>

<h3>Title: Fusion to Enhance: Fusion Visual Encoder to Enhance Multimodal Language Model</h3>
<ul>
<li><strong>Authors: </strong>Yifei She, Huangxuan Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00664">https://arxiv.org/abs/2509.00664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00664">https://arxiv.org/pdf/2509.00664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00664]] Fusion to Enhance: Fusion Visual Encoder to Enhance Multimodal Language Model(https://arxiv.org/abs/2509.00664)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have made significant progress in bridging visual perception with high-level textual reasoning. However, they face a fundamental contradiction: while excelling at complex semantic understanding, these models often fail at basic visual tasks that require precise detail perception. This deficiency primarily stems from the prevalent architectural reliance on a single vision encoder optimized for high-level semantic alignment, which inherently sacrifices the ability to capture fine-grained visual information. To address this issue, we introduce Fusion to Enhance (FtZ), a novel vision tower framework. FtZ moves beyond the single-encoder design by innovatively composing a semantically powerful anchor encoder with a perception-rich augmenting encoder via a lightweight Multi-Head Cross-Attention mechanism. Experimental results demonstrate that on several challenging benchmarks demanding fine-grained visual understanding, such as TextVQA, POPE, MMMU, MME and MM-Vet, our FtZ model significantly outperforms baselines that use only a single encoder or existing feature fusion methods. This work proves that composing heterogeneous expert encoders is an efficient and effective path to overcoming the visual perception bottleneck in current MLLMs, offering a new design paradigm for building next-generation AI systems with stronger perceptual capabilities.</li>
<li><strong>摘要：</strong>多模式的大型语言模型（MLLM）在桥接视觉感知和高级文本推理方面取得了重大进展。但是，他们面临着一个根本的矛盾：尽管在复杂的语义理解方面表现出色，但这些模型通常在需要精确细节感知的基本视觉任务上失败。这种缺陷主要源于对高级语义对齐的优化单一视觉编码器的普遍依赖，该构造固有地牺牲了捕获细粒度的视觉信息的能力。为了解决这个问题，我们将Fusion介绍为增强（FTZ），这是一个新颖的视觉塔框架。 FTZ通过通过轻巧的多头跨注意机制来创新具有富含感知的增强编码器的语义功能强大的锚编码器，超越了单个编码器设计。实验结果表明，在几个具有挑战性的基准测试中，需要细粒度的视觉理解，例如TextVQA，POPE，MMMU，MME，MME和MM-VET，我们的FTZ模型显着优于仅使用单个编码器或现有特征融合方法的基准。这项工作证明，组成异构专家编码器是克服当前MLLM中视觉感知瓶颈的有效途径，为建立具有更强感知能力的下一代AI系统提供了新的设计范式。</li>
</ul>

<h3>Title: LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model</h3>
<ul>
<li><strong>Authors: </strong>Xiyao Wang, Chunyuan Li, Jianwei Yang, Kai Zhang, Bo Liu, Tianyi Xiong, Furong Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00676">https://arxiv.org/abs/2509.00676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00676">https://arxiv.org/pdf/2509.00676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00676]] LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model(https://arxiv.org/abs/2509.00676)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In vision-language modeling, critic models are typically trained to evaluate outputs -- assigning scalar scores or pairwise preferences -- rather than to generate responses. This separation from policy models, which produce the responses, is so entrenched that critics are rarely considered for direct policy use. In this work, we challenge this convention. We propose to reorganize preference-labeled critic datasets into verifiable training signals and perform reinforcement learning directly on a base generative model, producing LLaVA-Critic-R1, a multimodal critic trained to optimize preference judgments while retaining full generation ability. Surprisingly, LLaVA-Critic-R1 emerges not only as a top-performing critic but also as a competitive policy model -- matching or surpassing specialized reasoning VLMs trained with in-domain data across 26 visual reasoning and understanding benchmarks, with an average gain of +5.7% over its base model (Qwen-2.5-VL-7B). Extending this approach to existing strong reasoning VLMs yields LLaVA-Critic-R1+, which further advances policy performance without sacrificing critic quality, achieving a SoTA performance of 71.9 on MMMU at the 7B scale. Finally, we show that the enhanced critic ability benefits inference: applying self-critique at test time yields an average +13.8% improvement on five representative reasoning tasks without additional training. Our results reveal that RL training on critic data can produce a unified model excelling at both evaluation and generation, offering a simple path toward scalable, self-improving multimodal systems.</li>
<li><strong>摘要：</strong>在视觉建模中，通常对评论家模型进行培训，以评估输出 - 分配标量分数或成对的偏好 - 而不是生成响应。与产生回应的政策模型的这种分离是如此根深蒂固，以至于很少考虑批评者以供直接政策使用。在这项工作中，我们挑战了这一公约。我们建议将偏好标记的评论家数据集重新组织为可验证的培训信号，并直接在基本生成模型上进行强化学习，从而产生Llava-Critic-R1，这是一个多模式的评论家，该批评家训练有素，可以优化优先判断，同时保持全代能力。令人惊讶的是，LLAVA-CRITIC-R1不仅是表现最好的批评家，而且作为竞争政策模型而出现 - 匹配或超越了专业推理VLM，在26个视觉推理和理解基准中，在26个视觉推理和理解基准的内域数据中训练，平均增长了 +5.7％的基础模型（QWEN-2.5-VL-VL-7B）。将这种方法扩展到现有强大的推理VLM会产生Llava-Critic-R1+，这进一步提高了政策绩效而不会牺牲批评的质量，在7B量表上实现了MMMU的SOTA性能为71.9。最后，我们表明，增强的评论家能力益处推断：在考试时间应用自我评价可以平均在没有额外培训的情况下，平均在五项代表性推理任务上提高 +13.8％。我们的结果表明，对评论家数据的RL培训可以在评估和生成方面产生统一的模型，从而为可扩展，自我提高的多模式系统提供简单的途径。</li>
</ul>

<h3>Title: Valid Property-Enhanced Contrastive Learning for Targeted Optimization & Resampling for Novel Drug Design</h3>
<ul>
<li><strong>Authors: </strong>Amartya Banerjee, Somnath Kar, Anirban Pal, Debabrata Maiti</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00684">https://arxiv.org/abs/2509.00684</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00684">https://arxiv.org/pdf/2509.00684</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00684]] Valid Property-Enhanced Contrastive Learning for Targeted Optimization & Resampling for Novel Drug Design(https://arxiv.org/abs/2509.00684)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Efficiently steering generative models toward pharmacologically relevant regions of chemical space remains a major obstacle in molecular drug discovery under low-data regimes. We present VECTOR+: Valid-property-Enhanced Contrastive Learning for Targeted Optimization and Resampling, a framework that couples property-guided representation learning with controllable molecule generation. VECTOR+ applies to both regression and classification tasks and enables interpretable, data-efficient exploration of functional chemical space. We evaluate on two datasets: a curated PD-L1 inhibitor set (296 compounds with experimental $IC_{50}$ values) and a receptor kinase inhibitor set (2,056 molecules by binding mode). Despite limited training data, VECTOR+ generates novel, synthetically tractable candidates. Against PD-L1 (PDB 5J89), 100 of 8,374 generated molecules surpass a docking threshold of $-15.0$ kcal/mol, with the best scoring $-17.6$ kcal/mol compared to the top reference inhibitor ($-15.4$ kcal/mol). The best-performing molecules retain the conserved biphenyl pharmacophore while introducing novel motifs. Molecular dynamics (250 ns) confirm binding stability (ligand RMSD < $2.5$ angstroms). VECTOR+ generalizes to kinase inhibitors, producing compounds with stronger docking scores than established drugs such as brigatinib and sorafenib. Benchmarking against JT-VAE and MolGPT across docking, novelty, uniqueness, and Tanimoto similarity highlights the superior performance of our method. These results position our work as a robust, extensible approach for property-conditioned molecular design in low-data settings, bridging contrastive learning and generative modeling for reproducible, AI-accelerated discovery.</li>
<li><strong>摘要：</strong>有效地将生成模型转向了化学空间与药理学相关的区域，在低数据表格下发现分子药物发现仍然是一个主要障碍。我们提出矢量+：针对目标优化和重采样的有效质体增强的对比度学习，该框架将属性引导的表示与可控的分子产生结合。向量+适用于回归和分类任务，并可以对功能化学空间进行可解释的，可解释的数据有效探索。我们在两个数据集上评估：一个策划的PD-L1抑制剂集（296个具有实验性$ IC_ {50} $值的化合物）和一个受体激酶抑制剂集（通过结合模式为2,056个分子）。尽管训练数据有限，但Vector+会生成新颖的合成典型候选者。与PD-L1（PDB 5J89）相比，8,374个产生的分子中有100个超过了$ -15.0 $ kcal/mol的对接阈值，与最佳的参考抑制剂相比，得分最高的$ -17.6 $ kcal/mol（$ -15.4 $ kcal/mol）。表现最佳的分子在引入新的基序的同时保留了保守的双苯基药片。分子动力学（250 ns）确认结合稳定性（配体RMSD <$ 2.5 $埃斯特罗姆）。向量+概括为激酶抑制剂，产生比Brigatinib和Sorafenib等确定药物具有更强对接得分的化合物。在对接，新颖性，唯一性和Tanimoto相似性的基准测试中，对JT-VAE和MOLGPT进行了基准测试，这突出了我们方法的出色性能。这些结果将我们的工作定位为在低数据环境中对财产条件的分子设计的强大，可扩展的方法，桥接对比度学习和生成性建模，用于可重复的，AI-Accelerate的发现。</li>
</ul>

<h3>Title: DELTA: Variational Disentangled Learning for Privacy-Preserving Data Reprogramming</h3>
<ul>
<li><strong>Authors: </strong>Arun Vignesh Malarkkan, Haoyue Bai, Anjali Kaushik, Yanjie Fu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00693">https://arxiv.org/abs/2509.00693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00693">https://arxiv.org/pdf/2509.00693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00693]] DELTA: Variational Disentangled Learning for Privacy-Preserving Data Reprogramming(https://arxiv.org/abs/2509.00693)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In real-world applications, domain data often contains identifiable or sensitive attributes, is subject to strict regulations (e.g., HIPAA, GDPR), and requires explicit data feature engineering for interpretability and transparency. Existing feature engineering primarily focuses on advancing downstream task performance, often risking privacy leakage. We generalize this learning task under such new requirements as Privacy-Preserving Data Reprogramming (PPDR): given a dataset, transforming features to maximize target attribute prediction accuracy while minimizing sensitive attribute prediction accuracy. PPDR poses challenges for existing systems: 1) generating high-utility feature transformations without being overwhelmed by a large search space, and 2) disentangling and eliminating sensitive information from utility-oriented features to reduce privacy inferability. To tackle these challenges, we propose DELTA, a two-phase variational disentangled generative learning framework. Phase I uses policy-guided reinforcement learning to discover feature transformations with downstream task utility, without any regard to privacy inferability. Phase II employs a variational LSTM seq2seq encoder-decoder with a utility-privacy disentangled latent space design and adversarial-causal disentanglement regularization to suppress privacy signals during feature generation. Experiments on eight datasets show DELTA improves predictive performance by ~9.3% and reduces privacy leakage by ~35%, demonstrating robust, privacy-aware data transformation.</li>
<li><strong>摘要：</strong>在实际应用程序中，域数据通常包含可识别或敏感的属性，受严格的法规（例如HIPAA，GDPR）的约束，并且需要明确的数据功能工程以解释性和透明度。现有的功能工程主要集中于推进下游任务性能，通常会冒着隐私泄漏的风险。我们在新要求下概括了该学习任务，例如保留隐私数据重编程（PPDR）：给定数据集，转换功能以最大化目标属性预测准确性，同时最大程度地降低敏感属性预测准确性。 PPDR对现有系统构成挑战：1）产生高纯粹的特征转换，而不会被大型搜索空间所淹没，而2）解散并消除了从实用方向的功能中删除敏感信息，以减少隐私性可推广性。为了应对这些挑战，我们提出了Delta，这是一个两阶段分离的生成学习框架。第一阶段使用政策引导的强化学习来发现具有下游任务实用程序的特征转换，而无需考虑隐私性可推广性。第二阶段采用了差异LSTM SEQ2SEQ编码器，并具有效用私人空间的潜在空间设计和对抗性 - 疾病的分解正规化，以抑制特征生成期间的隐私信号。八个数据集的实验显示，增量可将预测性能提高〜9.3％，并将隐私泄漏降低〜35％，证明了稳健的隐私感知数据转换。</li>
</ul>

<h3>Title: Why Pool When You Can Flow? Active Learning with GFlowNets</h3>
<ul>
<li><strong>Authors: </strong>Renfei Zhang, Mohit Pandey, Artem Cherkasov, Martin Ester</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00704">https://arxiv.org/abs/2509.00704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00704">https://arxiv.org/pdf/2509.00704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00704]] Why Pool When You Can Flow? Active Learning with GFlowNets(https://arxiv.org/abs/2509.00704)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The scalability of pool-based active learning is limited by the computational cost of evaluating large unlabeled datasets, a challenge that is particularly acute in virtual screening for drug discovery. While active learning strategies such as Bayesian Active Learning by Disagreement (BALD) prioritize informative samples, it remains computationally intensive when scaled to libraries containing billions samples. In this work, we introduce BALD-GFlowNet, a generative active learning framework that circumvents this issue. Our method leverages Generative Flow Networks (GFlowNets) to directly sample objects in proportion to the BALD reward. By replacing traditional pool-based acquisition with generative sampling, BALD-GFlowNet achieves scalability that is independent of the size of the unlabeled pool. In our virtual screening experiment, we show that BALD-GFlowNet achieves a performance comparable to that of standard BALD baseline while generating more structurally diverse molecules, offering a promising direction for efficient and scalable molecular discovery.</li>
<li><strong>摘要：</strong>基于池的主动学习的可伸缩性受到评估大型未标记数据集的计算成本的限制，这是在虚拟筛查药物发现时尤其严重的挑战。尽管主动学习策略，例如通过分歧（BALD）优先考虑的贝叶斯积极学习，但在缩放到包含数十亿个样本的库时，它仍然在计算中保持密集。在这项工作中，我们介绍了Bald-Gflownet，这是一个生成活跃的学习框架，绕过了这个问题。我们的方法利用生成流网络（GFLOWNETS）直接按秃头奖励进行采样对象。通过用生成抽样替换传统的基于池的采集，Bald-Gflownet实现了与未标记池的大小无关的可扩展性。在我们的虚拟筛选实验中，我们表明，秃头gflownet的性能与标准秃头基线的性能相当，同时产生了更具结构上不同的分子，为有希望的有希望的方向提供了有效且可扩展的分子发现。</li>
</ul>

<h3>Title: Attribute Fusion-based Classifier on Framework of Belief Structure</h3>
<ul>
<li><strong>Authors: </strong>Qiying Hu, Yingying Liang, Qianli Zhou, Witold Pedrycz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00754">https://arxiv.org/abs/2509.00754</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00754">https://arxiv.org/pdf/2509.00754</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00754]] Attribute Fusion-based Classifier on Framework of Belief Structure(https://arxiv.org/abs/2509.00754)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Dempster-Shafer Theory (DST) provides a powerful framework for modeling uncertainty and has been widely applied to multi-attribute classification tasks. However, traditional DST-based attribute fusion-based classifiers suffer from oversimplified membership function modeling and limited exploitation of the belief structure brought by basic probability assignment (BPA), reducing their effectiveness in complex real-world scenarios. This paper presents an enhanced attribute fusion-based classifier that addresses these limitations through two key innovations. First, we adopt a selective modeling strategy that utilizes both single Gaussian and Gaussian Mixture Models (GMMs) for membership function construction, with model selection guided by cross-validation and a tailored evaluation metric. Second, we introduce a novel method to transform the possibility distribution into a BPA by combining simple BPAs derived from normalized possibility distributions, enabling a much richer and more flexible representation of uncertain information. Furthermore, we apply the belief structure-based BPA generation method to the evidential K-Nearest Neighbors classifier, enhancing its ability to incorporate uncertainty information into decision-making. Comprehensive experiments on benchmark datasets are conducted to evaluate the performance of the proposed attribute fusion-based classifier and the enhanced evidential K-Nearest Neighbors classifier in comparison with both evidential classifiers and conventional machine learning classifiers. The results demonstrate that our proposed classifier outperforms the best existing evidential classifier, achieving an average accuracy improvement of 4.84%, while maintaining low variance, thus confirming its superior effectiveness and robustness.</li>
<li><strong>摘要：</strong>Dempster-Shafer理论（DST）为建模不确定性提供了一个强大的框架，并已广泛应用于多属性分类任务。但是，基于DST的传统属性基于融合的分类器遭受过度简化的成员功能建模和对基本概率分配（BPA）带来的信念结构的有限剥削，从而降低了它们在复杂的现实世界中的有效性。本文提出了一个增强的属性基于融合的分类器，该分类器通过两个关键创新来解决这些限制。首先，我们采用选择性建模策略，该策略利用单个高斯和高斯混合模型（GMM）进行会员功能构建，并以交叉验证和量身定制的评估指标为指导的模型选择。其次，我们引入了一种新颖的方法，将可能性分布转换为BPA，通过结合源自归一化可能性分布的简单BPA，从而使不确定信息的更丰富，更灵活地表示。此外，我们将基于信念结构的BPA生成方法应用于证据k-near的邻居分类器，从而增强了其将不确定性信息纳入决策的能力。与证据分类器和常规机器学习分类器相比，对基于基准数据集进行了全面的实验，以评估提出的属性基于融合的分类器的性能和增强的证据k-nearest邻居分类器。结果表明，我们提出的分类器的表现优于现有的最佳证据分类器，在保持较低方差的同时，平均准确性提高了4.84％，从而确保了其出色的有效性和鲁棒性。</li>
</ul>

<h3>Title: InterPose: Learning to Generate Human-Object Interactions from Large-Scale Web Videos</h3>
<ul>
<li><strong>Authors: </strong>Yangsong Zhang, Abdul Ahad Butt, Gül Varol, Ivan Laptev</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00767">https://arxiv.org/abs/2509.00767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00767">https://arxiv.org/pdf/2509.00767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00767]] InterPose: Learning to Generate Human-Object Interactions from Large-Scale Web Videos(https://arxiv.org/abs/2509.00767)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Human motion generation has shown great advances thanks to the recent diffusion models trained on large-scale motion capture data. Most of existing works, however, currently target animation of isolated people in empty scenes. Meanwhile, synthesizing realistic human-object interactions in complex 3D scenes remains a critical challenge in computer graphics and robotics. One obstacle towards generating versatile high-fidelity human-object interactions is the lack of large-scale datasets with diverse object manipulations. Indeed, existing motion capture data is typically restricted to single people and manipulations of limited sets of objects. To address this issue, we propose an automatic motion extraction pipeline and use it to collect interaction-rich human motions. Our new dataset InterPose contains 73.8K sequences of 3D human motions and corresponding text captions automatically obtained from 45.8K videos with human-object interactions. We perform extensive experiments and demonstrate InterPose to bring significant improvements to state-of-the-art methods for human motion generation. Moreover, using InterPose we develop an LLM-based agent enabling zero-shot animation of people interacting with diverse objects and scenes.</li>
<li><strong>摘要：</strong>由于最近在大规模运动捕获数据中训练的扩散模型，人类运动产生的进步已取得了长足进步。但是，目前，大多数现有作品都针对空场中孤立的人的动画。同时，在复杂的3D场景中综合现实的人对象相互作用仍然是计算机图形和机器人技术的关键挑战。产生多功能人类对象相互作用的一种障碍是缺乏具有多种物体操作的大规模数据集。实际上，现有的运动捕获数据通常仅限于单身人士，并且对有限的对象集进行操作。为了解决这个问题，我们提出了一个自动运动提取管道，并使用它来收集互动丰富的人类动作。我们的新数据集插入包含3D人体动作的73.8K序列和相应的文本字幕，从具有人类对象相互作用的45.8K视频中自动获得。我们进行了广泛的实验，并展示了插入，以对人类运动的最新方法带来重大改进。此外，使用Interpose，我们开发了一个基于LLM的代理，从而使人们能够与各种对象和场景互动的人进行零拍动画。</li>
</ul>

<h3>Title: Diffusion-Based Image-to-Brain Signal Generation with Cross-Attention Mechanisms for Visual Prostheses</h3>
<ul>
<li><strong>Authors: </strong>Ganxi Xu, Jinyi Long, Jia Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00787">https://arxiv.org/abs/2509.00787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00787">https://arxiv.org/pdf/2509.00787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00787]] Diffusion-Based Image-to-Brain Signal Generation with Cross-Attention Mechanisms for Visual Prostheses(https://arxiv.org/abs/2509.00787)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Visual prostheses have shown great potential in restoring vision for blind individuals. On the one hand, researchers have been continuously improving the brain decoding framework of visual prostheses by leveraging the powerful image generation capabilities of diffusion models. On the other hand, the brain encoding stage of visual prostheses struggles to generate brain signals with sufficient biological similarity. Although existing works have recognized this problem, the quality of predicted stimuli still remains a critical issue, as existing approaches typically lack supervised signals from real brain responses to validate the biological plausibility of predicted stimuli. To address this issue, we propose a novel image-to-brain framework based on denoising diffusion probabilistic models (DDPMs) enhanced with cross-attention mechanisms. Our framework consists of two key architectural components: a pre-trained CLIP visual encoder that extracts rich semantic representations from input images, and a cross-attention enhanced U-Net diffusion model that learns to reconstruct biologically plausible brain signals through iterative denoising. Unlike conventional generative models that rely on simple concatenation for conditioning, our cross-attention modules enable dynamic interaction between visual features and brain signal representations, facilitating fine-grained alignment during the generation process. We evaluate our framework on two multimodal datasets (THINGS-EEG2 and THINGS-MEG) to demonstrate its effectiveness in generating biologically plausible brain signals. Moreover, we visualize the training and test M/EEG topographies for all subjects on both datasets to intuitively demonstrate the intra-subject variations and inter-subject variations in M/EEG signals.</li>
<li><strong>摘要：</strong>视觉假体在恢复盲人的视野方面表现出了巨大的潜力。一方面，研究人员通过利用扩散模型的强大图像产生能力来不断地改善视觉假体的大脑解码框架。另一方面，大脑编码视觉假体的阶段难以生成具有足够生物学相似性的大脑信号。尽管现有作品已经意识到了这个问题，但预测的刺激的质量仍然是一个关键问题，因为现有方法通常缺乏真正的大脑反应中的监督信号，无法验证预测刺激的生物学合理性。为了解决这个问题，我们提出了一个新的图像到脑框架，基于通过跨注意机制增强的脱氧扩散概率模型（DDPM）。我们的框架由两个关键的架构组件组成：一种预先训练的剪辑视觉编码器，该编码器从输入图像中提取丰富的语义表示，以及通过迭代的迭代型迭代来重建生物学上具有生物学上的脑信号的交叉练习增强的U-NET扩散模型。与依靠简单串联进行调理的常规生成模型不同，我们的交叉意见模块可以在视觉特征和大脑信号表示之间进行动态相互作用，从而在生成过程中促进细粒度的对齐。我们在两个多模式数据集（Things-eeg2和Things-Meg）上评估了我们的框架，以证明其在生成生物学上合理的大脑信号方面的有效性。此外，我们可视化两个数据集中所有受试者的训练和测试M/EEG地形图，以直观地证明M/EEG信号中对象内的变化和受试者间变化。</li>
</ul>

<h3>Title: ProCause: Generating Counterfactual Outcomes to Evaluate Prescriptive Process Monitoring Methods</h3>
<ul>
<li><strong>Authors: </strong>Jakob De Moor, Hans Weytjens, Johannes De Smedt</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00797">https://arxiv.org/abs/2509.00797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00797">https://arxiv.org/pdf/2509.00797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00797]] ProCause: Generating Counterfactual Outcomes to Evaluate Prescriptive Process Monitoring Methods(https://arxiv.org/abs/2509.00797)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Prescriptive Process Monitoring (PresPM) is the subfield of Process Mining that focuses on optimizing processes through real-time interventions based on event log data. Evaluating PresPM methods is challenging due to the lack of ground-truth outcomes for all intervention actions in datasets. A generative deep learning approach from the field of Causal Inference (CI), RealCause, has been commonly used to estimate the outcomes for proposed intervention actions to evaluate a new policy. However, RealCause overlooks the temporal dependencies in process data, and relies on a single CI model architecture, TARNet, limiting its effectiveness. To address both shortcomings, we introduce ProCause, a generative approach that supports both sequential (e.g., LSTMs) and non-sequential models while integrating multiple CI architectures (S-Learner, T-Learner, TARNet, and an ensemble). Our research using a simulator with known ground truths reveals that TARNet is not always the best choice; instead, an ensemble of models offers more consistent reliability, and leveraging LSTMs shows potential for improved evaluations when temporal dependencies are present. We further validate ProCause's practical effectiveness through a real-world data analysis, ensuring a more reliable evaluation of PresPM methods.</li>
<li><strong>摘要：</strong>规定过程监视（PRESPM）是过程挖掘的子场，它的重点是基于事件日志数据通过实时干预措施来优化过程。由于缺乏数据集中所有干预措施的基础真相结果，评估PREPM方法是具有挑战性的。从因果推理（CI）（CI）领域的一种生成深度学习方法，通常用于估计提出的干预行动的结果以评估新政策。但是，实数忽略了过程数据中的时间依赖性，并且依赖于单个CI模型体系结构，限制了其有效性。为了解决这两个缺点，我们介绍了Propause，这是一种支持顺序（例如LSTMS）和非序列模型的生成方法，同时集成了多个CI体系结构（S-Learner，T-Learner，Tarnet和一个集合）。我们使用具有已知基础真理的模拟器的研究表明，Tarnet并不总是最好的选择。取而代之的是，模型的合奏提供了更一致的可靠性，当存在时间依赖性时，利用LSTMS显示出改进评估的潜力。我们通过现实世界数据分析进一步验证了procause的实际有效性，从而确保对PRESPM方法的更可靠评估。</li>
</ul>

<h3>Title: Multimodal Iterative RAG for Knowledge Visual Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Changin Choi, Wonseok Lee, Jungmin Ko, Wonjong Rhee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00798">https://arxiv.org/abs/2509.00798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00798">https://arxiv.org/pdf/2509.00798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00798]] Multimodal Iterative RAG for Knowledge Visual Question Answering(https://arxiv.org/abs/2509.00798)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While Multimodal Large Language Models (MLLMs) have significantly advanced multimodal understanding, their performance remains limited on knowledge-intensive visual questions that require external knowledge beyond the image. Retrieval-Augmented Generation (RAG) has become a promising solution for providing models with external knowledge, its conventional single-pass framework often fails to gather sufficient knowledge. To overcome this limitation, we propose MI-RAG, a Multimodal Iterative RAG framework that leverages reasoning to enhance retrieval and update reasoning over newly retrieved knowledge across modalities. At each iteration, MI-RAG leverages an accumulated reasoning record to dynamically formulate a multi-query. These queries then drive a joint search across heterogeneous knowledge bases containing both visually-grounded and textual knowledge. The newly acquired knowledge is synthesized into the reasoning record, progressively refining understanding across iterations. Experiments on challenging benchmarks, including Encyclopedic VQA, InfoSeek, and OK-VQA, show that MI-RAG significantly improves both retrieval recall and answer accuracy, establishing a scalable approach for compositional reasoning in knowledge-intensive VQA.</li>
<li><strong>摘要：</strong>尽管多模式大型语言模型（MLLM）具有显着高级的多模式理解，但它们的表现仍限于知识密集的视觉问题，这些问题需要外部知识以外的外部知识。检索演示的一代（RAG）已成为为模型提供外部知识的有前途的解决方案，其常规的单通框架通常无法收集足够的知识。为了克服这一限制，我们提出了Mi-rag，这是一种多模式迭代的抹布框架，利用推理来增强检索和更新推理，而不是跨模态的新检索知识。在每次迭代中，Mi-rag利用累积的推理记录动态制定多电量。然后，这些查询推动了包含视觉和文本知识的异质知识库的联合搜索。新获得的知识被合成为推理记录，逐渐完善了跨越迭代的理解。有关挑战性基准的实验，包括百科全书VQA，Infoseek和OK-VQA，表明Mi-rag显着提高了检索召回和答案的准确性，并在知识密集型VQA中建立了可扩展的构图推理方法。</li>
</ul>

<h3>Title: Satellite Image Utilization for Dehazing with Swin Transformer-Hybrid U-Net and Watershed loss</h3>
<ul>
<li><strong>Authors: </strong>Jongwook Si, Sungyoung Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00835">https://arxiv.org/abs/2509.00835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00835">https://arxiv.org/pdf/2509.00835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00835]] Satellite Image Utilization for Dehazing with Swin Transformer-Hybrid U-Net and Watershed loss(https://arxiv.org/abs/2509.00835)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Satellite imagery plays a crucial role in various fields; however, atmospheric interference and haze significantly degrade image clarity and reduce the accuracy of information extraction. To address these challenges, this paper proposes a hybrid dehazing framework that integrates Swin Transformer and U-Net to balance global context learning and local detail restoration, called SUFERNOBWA. The proposed network employs SwinRRDB, a Swin Transformer-based Residual-in-Residual Dense Block, in both the encoder and decoder to effectively extract features. This module enables the joint learning of global contextual information and fine spatial structures, which is crucial for structural preservation in satellite image. Furthermore, we introduce a composite loss function that combines L2 loss, guided loss, and a novel watershed loss, which enhances structural boundary preservation and ensures pixel-level accuracy. This architecture enables robust dehazing under diverse atmospheric conditions while maintaining structural consistency across restored images. Experimental results demonstrate that the proposed method outperforms state-of-the-art models on both the RICE and SateHaze1K datasets. Specifically, on the RICE dataset, the proposed approach achieved a PSNR of 33.24 dB and an SSIM of 0.967, which is a significant improvement over existing method. This study provides an effective solution for mitigating atmospheric interference in satellite imagery and highlights its potential applicability across diverse remote sensing applications.</li>
<li><strong>摘要：</strong>卫星图像在各个领域都起着至关重要的作用。但是，大气干扰和雾霾显着降低了图像的清晰度，并降低了信息提取的准确性。为了应对这些挑战，本文提出了一个混合脱掩的框架，该框架将Swin Transformer和U-NET整合在一起，以平衡全球环境学习和本地细节恢复，称为Sufernobwa。所提出的网络采用SwinrrdB，这是一种基于SWIN变压器的残基密集块，在编码器和解码器中都可以有效提取特征。该模块可以联合学习全球上下文信息和精细的空间结构，这对于卫星图像中的结构保存至关重要。此外，我们引入了一个复合损失函数，该函数结合了L2损失，引导损失和新的分水岭损失，从而增强了结构边界的保存并确保像素级的准确性。该体系结构在多种大气条件下实现了强大的飞行，同时保持跨恢复图像的结构一致性。实验结果表明，所提出的方法在大米和satehaze1k数据集上都优于最先进的模型。具体而言，在稻米数据集上，提出的方法达到了33.24 dB的PSNR，SSIM为0.967，这比现有方法是一个显着改善。这项研究提供了一种有效的解决方案，可减轻大气干扰卫星图像，并突出显示其在各种遥感应用中的潜在适用性。</li>
</ul>

<h3>Title: Look Beyond: Two-Stage Scene View Generation via Panorama and Video Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Xueyang Kang, Zhengkang Xiang, Zezheng Zhang, Kourosh Khoshelham</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00843">https://arxiv.org/abs/2509.00843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00843">https://arxiv.org/pdf/2509.00843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00843]] Look Beyond: Two-Stage Scene View Generation via Panorama and Video Diffusion(https://arxiv.org/abs/2509.00843)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Novel view synthesis (NVS) from a single image is highly ill-posed due to large unobserved regions, especially for views that deviate significantly from the input. While existing methods focus on consistency between the source and generated views, they often fail to maintain coherence and correct view alignment across long-range or looped trajectories. We propose a model that addresses this by decomposing single-view NVS into a 360-degree scene extrapolation followed by novel view interpolation. This design ensures long-term view and scene consistency by conditioning on keyframes extracted and warped from a generated panoramic representation. In the first stage, a panorama diffusion model learns the scene prior from the input perspective image. Perspective keyframes are then sampled and warped from the panorama and used as anchor frames in a pre-trained video diffusion model, which generates novel views through a proposed spatial noise diffusion process. Compared to prior work, our method produces globally consistent novel views -- even in loop closure scenarios -- while enabling flexible camera control. Experiments on diverse scene datasets demonstrate that our approach outperforms existing methods in generating coherent views along user-defined trajectories. Our implementation is available at this https URL.</li>
<li><strong>摘要：</strong>来自单个图像的新型视图合成（NVS）由于未观察到的区域而高度不足，特别是对于显着偏离输入的视图。尽管现有方法集中在源和生成视图之间的一致性上，但它们通常无法保持连贯性并在远距离或循环轨迹上正确的视图对齐。我们提出了一个模型，通过将单视NV分解为360度场景外推，然后进行新的视图插值来解决此问题。该设计通过根据从生成的全景表示中提取并扭曲的密钥帧来确保长期视图和场景一致性。在第一阶段，全景扩散模型从输入的角度图像中了解了先前的场景。然后将透视键帧从全景图中进行采样并扭曲，并在预训练的视频扩散模型中用作锚框，该模型通过提出的空间噪声扩散过程生成新视图。与先前的工作相比，我们的方法会产生全球一致的新型视图 - 即使在循环封闭场景中，也可以实现灵活的相机控制。各种场景数据集的实验表明，我们的方法在沿用户定义的轨迹生成相干视图方面的现有方法。我们的实现可在此HTTPS URL上获得。</li>
</ul>

<h3>Title: An Explainable Gaussian Process Auto-encoder for Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Wei Zhang, Brian Barr, John Paisley</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00884">https://arxiv.org/abs/2509.00884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00884">https://arxiv.org/pdf/2509.00884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00884]] An Explainable Gaussian Process Auto-encoder for Tabular Data(https://arxiv.org/abs/2509.00884)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Explainable machine learning has attracted much interest in the community where the stakes are high. Counterfactual explanations methods have become an important tool in explaining a black-box model. The recent advances have leveraged the power of generative models such as an autoencoder. In this paper, we propose a novel method using a Gaussian process to construct the auto-encoder architecture for generating counterfactual samples. The resulting model requires fewer learnable parameters and thus is less prone to overfitting. We also introduce a novel density estimator that allows for searching for in-distribution samples. Furthermore, we introduce an algorithm for selecting the optimal regularization rate on density estimator while searching for counterfactuals. We experiment with our method in several large-scale tabular datasets and compare with other auto-encoder-based methods. The results show that our method is capable of generating diversified and in-distribution counterfactual samples.</li>
<li><strong>摘要：</strong>可解释的机器学习引起了人们对赌注很高的社区的极大兴趣。反事实解释方法已成为解释黑盒模型的重要工具。最近的进步利用了生成模型（例如自动编码器）的力量。在本文中，我们提出了一种使用高斯工艺来构建自动编码器结构来生成反事实样本的新方法。所得模型需要更少的可学习参数，因此不容易过度拟合。我们还引入了一个新型的密度估计器，该估计量可以搜索分布样品。此外，我们引入了一种算法，用于在搜索反事实时在密度估计器上选择最佳正则速率。我们在几个大尺度表格数据集中尝试了我们的方法，并与其他基于自动编码器的方法进行比较。结果表明，我们的方法能够生成多元化和分布的反事实样本。</li>
</ul>

<h3>Title: AI-driven Dispensing of Coral Reseeding Devices for Broad-scale Restoration of the Great Barrier Reef</h3>
<ul>
<li><strong>Authors: </strong>Scarlett Raine, Benjamin Moshirian, Tobias Fischer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01019">https://arxiv.org/abs/2509.01019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01019">https://arxiv.org/pdf/2509.01019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01019]] AI-driven Dispensing of Coral Reseeding Devices for Broad-scale Restoration of the Great Barrier Reef(https://arxiv.org/abs/2509.01019)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Coral reefs are on the brink of collapse, with climate change, ocean acidification, and pollution leading to a projected 70-90% loss of coral species within the next decade. Restoration efforts are crucial, but their success hinges on introducing automation to upscale efforts. We present automated deployment of coral re-seeding devices powered by artificial intelligence, computer vision, and robotics. Specifically, we perform automated substrate classification, enabling detection of areas of the seafloor suitable for coral growth, thus significantly reducing reliance on human experts and increasing the range and efficiency of restoration. Real-world testing of the algorithms on the Great Barrier Reef leads to deployment accuracy of 77.8%, sub-image patch classification of 89.1%, and real-time model inference at 5.5 frames per second. Further, we present and publicly contribute a large collection of annotated substrate image data to foster future research in this area.</li>
<li><strong>摘要：</strong>珊瑚礁处于崩溃的边缘，气候变化，海洋酸化和污染，导致未来十年内预计珊瑚物种损失70-90％。恢复工作至关重要，但是他们的成功取决于将自动化引入高档努力。我们介绍了由人工智能，计算机视觉和机器人技术提供动力的珊瑚重种植设备的自动部署。具体而言，我们执行自动化的底物分类，从而可以检测适合珊瑚生长的海底区域，从而显着降低了对人类专家的依赖，并提高了恢复的范围和效率。对大屏障礁上算法的现实测试可导致77.8％的部署准确性，89.1％的子图像贴片分类和每秒5.5帧的实时模型推断。此外，我们介绍并公开贡献了大量注释的底物图像数据，以促进该领域的未来研究。</li>
</ul>

<h3>Title: Any-Order Flexible Length Masked Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Jaeyeon Kim, Lee Cheuk-Kit, Carles Domingo-Enrich, Yilun Du, Sham Kakade, Timothy Ngotiaoco, Sitan Chen, Michael Albergo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01025">https://arxiv.org/abs/2509.01025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01025">https://arxiv.org/pdf/2509.01025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01025]] Any-Order Flexible Length Masked Diffusion(https://arxiv.org/abs/2509.01025)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Masked diffusion models (MDMs) have recently emerged as a promising alternative to autoregressive models over discrete domains. MDMs generate sequences in an any-order, parallel fashion, enabling fast inference and strong performance on non-causal tasks. However, a crucial limitation is that they do not support token insertions and are thus limited to fixed-length generations. To this end, we introduce Flexible Masked Diffusion Models (FlexMDMs), a discrete diffusion paradigm that simultaneously can model sequences of flexible length while provably retaining MDMs' flexibility of any-order inference. Grounded in an extension of the stochastic interpolant framework, FlexMDMs generate sequences by inserting mask tokens and unmasking them. Empirically, we show that FlexMDMs match MDMs in perplexity while modeling length statistics with much higher fidelity. On a synthetic maze planning task, they achieve $\approx 60 \%$ higher success rate than MDM baselines. Finally, we show pretrained MDMs can easily be retrofitted into FlexMDMs: on 16 H100s, it takes only three days to fine-tune LLaDA-8B into a FlexMDM, achieving superior performance on math (GSM8K, $58\% \to 67\%$) and code infilling performance ($52\% \to 65\%$).</li>
<li><strong>摘要：</strong>蒙版扩散模型（MDMS）最近已成为离散域上自回归模型的有前途的替代方法。 MDMS以任何顺序，并行的方式生成序列，从而在非伴奏任务上实现快速推断和强大的性能。但是，关键的限制是它们不支持令牌插入，因此仅限于固定长度。为此，我们引入了灵活的掩蔽扩散模型（FlexMDMS），这是一种离散的扩散范式，同时可以模拟柔性长度的序列，同时可证明可以保留MDMS的任何级别推断灵活性。 flexMDM接地在随机插值框架的扩展中，通过插入掩码令牌和揭开掩码来生成序列。从经验上讲，我们表明FlexMDMS在困惑中与MDM匹配，同时建模长度统计数据以更高的保真度进行建模。在一项合成迷宫计划任务中，他们的成功率比MDM基线高约60 \％$。最后，我们显示预估计的MDM可以轻松地重新改造为FlexMDM：在16 H100时，将Llada-8B微调为FlexMDM只需三天，在数学上取得了出色的性能（GSM8K，$ 58 \％\ $ \％\至67 \％$）和代码散发性能（$ 52 \％\％\％\％\％\％\％\％\％$ \％\％\％\％\％）。</li>
</ul>

<h3>Title: CompSlider: Compositional Slider for Disentangled Multiple-Attribute Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Zixin Zhu, Kevin Duarte, Mamshad Nayeem Rizve, Chengyuan Xu, Ratheesh Kalarot, Junsong Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01028">https://arxiv.org/abs/2509.01028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01028">https://arxiv.org/pdf/2509.01028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01028]] CompSlider: Compositional Slider for Disentangled Multiple-Attribute Image Generation(https://arxiv.org/abs/2509.01028)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In text-to-image (T2I) generation, achieving fine-grained control over attributes - such as age or smile - remains challenging, even with detailed text prompts. Slider-based methods offer a solution for precise control of image attributes. Existing approaches typically train individual adapter for each attribute separately, overlooking the entanglement among multiple attributes. As a result, interference occurs among different attributes, preventing precise control of multiple attributes together. To address this challenge, we aim to disentangle multiple attributes in slider-based generation to enbale more reliable and independent attribute manipulation. Our approach, CompSlider, can generate a conditional prior for the T2I foundation model to control multiple attributes simultaneously. Furthermore, we introduce novel disentanglement and structure losses to compose multiple attribute changes while maintaining structural consistency within the image. Since CompSlider operates in the latent space of the conditional prior and does not require retraining the foundation model, it reduces the computational burden for both training and inference. We evaluate our approach on a variety of image attributes and highlight its generality by extending to video generation.</li>
<li><strong>摘要：</strong>在文本到图像（T2i）的一代中，即使有详细的文本提示，对属性（例如年龄或微笑）的精细元素控制（例如年龄或微笑）仍然具有挑战性。基于滑块的方法为精确控制图像属性提供了解决方案。现有方法通常会分别为每个属性训练单个适配器，从而忽略多个属性之间的纠缠。结果，干扰发生在不同属性之间，从而防止对多个属性的精确控制。为了应对这一挑战，我们的目标是将基于滑块的生成中的多个属性解散，以更可靠和独立的属性操纵。我们的方法Compslider可以生成T2I基础模型的条件先验，以同时控制多个属性。此外，我们引入了新型的分离和结构损失，以构成多个属性变化，同时保持图像中的结构一致性。由于Compslider在条件之前的潜在空间中运行，并且不需要重新基础模型，因此它减轻了训练和推理的计算负担。我们在各种图像属性上评估了我们的方法，并通过扩展到视频生成来突出显示其通用性。</li>
</ul>

<h3>Title: Seeing through Unclear Glass: Occlusion Removal with One Shot</h3>
<ul>
<li><strong>Authors: </strong>Qiang Li, Yuanming Cao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01033">https://arxiv.org/abs/2509.01033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01033">https://arxiv.org/pdf/2509.01033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01033]] Seeing through Unclear Glass: Occlusion Removal with One Shot(https://arxiv.org/abs/2509.01033)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Images taken through window glass are often degraded by contaminants adhered to the glass surfaces. Such contaminants cause occlusions that attenuate the incoming light and scatter stray light towards the camera. Most of existing deep learning methods for neutralizing the effects of contaminated glasses relied on synthetic training data. Few researchers used real degraded and clean image pairs, but they only considered removing or alleviating the effects of rain drops on glasses. This paper is concerned with the more challenging task of learning the restoration of images taken through glasses contaminated by a wide range of occluders, including muddy water, dirt and other small foreign particles found in reality. To facilitate the learning task we have gone to a great length to acquire real paired images with and without glass contaminants. More importantly, we propose an all-in-one model to neutralize contaminants of different types by utilizing the one-shot test-time adaptation mechanism. It involves a self-supervised auxiliary learning task to update the trained model for the unique occlusion type of each test image. Experimental results show that the proposed method outperforms the state-of-the-art methods quantitatively and qualitatively in cleaning realistic contaminated images, especially the unseen ones.</li>
<li><strong>摘要：</strong>通过窗玻璃拍摄的图像通常被粘附在玻璃表面上的污染物降解。这样的污染物会引起闭塞，从而使传入的光线和散落的光线向相机散开。大多数现有的深度学习方法中和污染眼镜的影响依赖于合成训练数据。很少有研究人员使用真正的降级和干净的图像对，但他们仅考虑去除或减轻降雨滴对眼镜的影响。本文关注的是，学习通过各种封闭器污染的玻璃恢复图像的更具挑战性的任务，包括现实中发现的泥泞的水，污垢和其他小型外国颗粒。为了促进学习任务，我们竭尽全力获得有或没有玻璃污染物的真实成对图像。更重要的是，我们提出了一个多合一模型，以利用单发测试时间适应机制来中和不同类型的污染物。它涉及一项自我监督的辅助学习任务，以更新训练的每个测试图像唯一遮挡类型的训练模型。实验结果表明，该方法在清洁现实污染的图像（尤其是看不见的图像）方面，在定量上和质量上优于最先进的方法。</li>
</ul>

<h3>Title: A Unified Low-level Foundation Model for Enhancing Pathology Image Quality</h3>
<ul>
<li><strong>Authors: </strong>Ziyi Liu, Zhe Xu, Jiabo Ma, Wenqaing Li, Junlin Hou, Fuxiang Huang, Xi Wang, Ronald Cheong Kin Chan, Terence Tsz Wai Wong, Hao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01071">https://arxiv.org/abs/2509.01071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01071">https://arxiv.org/pdf/2509.01071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01071]] A Unified Low-level Foundation Model for Enhancing Pathology Image Quality(https://arxiv.org/abs/2509.01071)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, super-resolution</a></li>
<li><strong>Abstract: </strong>Foundation models have revolutionized computational pathology by achieving remarkable success in high-level diagnostic tasks, yet the critical challenge of low-level image enhancement remains largely unaddressed. Real-world pathology images frequently suffer from degradations such as noise, blur, and low resolution due to slide preparation artifacts, staining variability, and imaging constraints, while the reliance on physical staining introduces significant costs, delays, and inconsistency. Although existing methods target individual problems like denoising or super-resolution, their task-specific designs lack the versatility to handle the diverse low-level vision challenges encountered in practice. To bridge this gap, we propose the first unified Low-level Pathology Foundation Model (LPFM), capable of enhancing image quality in restoration tasks, including super-resolution, deblurring, and denoising, as well as facilitating image translation tasks like virtual staining (H&E and special stains), all through a single adaptable architecture. Our approach introduces a contrastive pre-trained encoder that learns transferable, stain-invariant feature representations from 190 million unlabeled pathology images, enabling robust identification of degradation patterns. A unified conditional diffusion process dynamically adapts to specific tasks via textual prompts, ensuring precise control over output quality. Trained on a curated dataset of 87,810 whole slied images (WSIs) across 34 tissue types and 5 staining protocols, LPFM demonstrates statistically significant improvements (p<0.01) over state-of-the-art methods in most tasks (56/66), achieving Peak Signal-to-Noise Ratio (PSNR) gains of 10-15% for image restoration and Structural Similarity Index Measure (SSIM) improvements of 12-18% for virtual staining.</li>
<li><strong>摘要：</strong>基金会模型通过在高级诊断任务中取得了显着的成功来彻底改变计算病理，但是低级图像增强的关键挑战仍然很大程度上没有解决。现实世界的病理图像经常因滑动制备工件，染色可变性和成像限制而引起的噪声，模糊和低分辨率等降解，而对物理染色的依赖会引入大量成本，延误和不一致。尽管现有的方法针对个人问题，例如denoising或超分辨率，但他们的特定于任务的设计缺乏处理实践中遇到的各种低级视力挑战的多功能性。为了弥合这一差距，我们提出了第一个统一的低级病理基础模型（LPFM），能够在恢复任务中增强图像质量，包括超分辨率，脱张和脱氧，以及通过单个可靠的架构，包括虚拟染色（H＆E和特殊STAINS），例如虚拟染色（H＆E和特殊STAINS）。我们的方法引入了一种对比的预训练的编码器，该编码器从1.9亿个未标记的病理图像中学习可转移的，不变的特征表示形式，从而实现了对降解模式的强大识别。统一的条件扩散过程通过文本提示动态适应特定任务，从而确保对输出质量的精确控制。 LPFM在34种组织类型和5种染色方案的87,810个整体图像（WSI）的策划数据集上进行了培训，LPFM在大多数任务（56/66）（56/66）中表现出统计学上显着的改进（P <0.01），以实现信号到峰值的测量比率（56/66），以实现10-15％的图像统计级别（PSNR）的升高。虚拟染色为12-18％。</li>
</ul>

<h3>Title: REFINESTAT: Efficient Exploration for Probabilistic Program Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Madhav Kanda, Shubham Ugare, Sasa Misailovic</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01082">https://arxiv.org/abs/2509.01082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01082">https://arxiv.org/pdf/2509.01082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01082]] REFINESTAT: Efficient Exploration for Probabilistic Program Synthesis(https://arxiv.org/abs/2509.01082)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Probabilistic programming offers a powerful framework for modeling uncertainty, yet statistical model discovery in this domain entails navigating an immense search space under strict domain-specific constraints. When small language models are tasked with generating probabilistic programs, they frequently produce outputs that suffer from both syntactic and semantic errors, such as flawed inference constructs. Motivated by probabilistic programmers' domain expertise and debugging strategies, we introduce RefineStat, a language model--driven framework that enforces semantic constraints ensuring synthesized programs contain valid distributions and well-formed parameters, and then applies diagnostic-aware refinement by resampling prior or likelihood components whenever reliability checks fail. We evaluate RefineStat on multiple probabilistic-programming code-generation tasks using smaller language models (SLMs) and find that it produces programs that are both syntactically sound and statistically reliable, often matching or surpassing those from closed-source large language models (e.g., OpenAI o3).</li>
<li><strong>摘要：</strong>概率编程为建模不确定性提供了一个强大的框架，但是该领域中的统计模型发现需要在严格的特定领域约束下导航巨大的搜索空间。当小型语言模型的任务是生成概率程序时，它们经常产生构成句法和语义错误（例如有缺陷的推理构建体）的输出。由概率程序员的领域专业知识和调试策略的激励，我们介绍了一种语言模型Refinestat，这是一种驱动的框架，该框架可以执行语义约束，以确保合成程序包含有效的分布和良好的分布参数，然后在可靠性的可靠性检查时应用诊断性的分布，然后应用诊断性诊断。我们使用较小的语言模型（SLMS）评估了多个概率编程代码生成任务的Refinestat，并发现它产生的程序既具有句法声音又是统计上可靠的，通常与封闭源的大型语言模型（例如OpenAI O3）相匹配或超越这些程序。</li>
</ul>

<h3>Title: Bidirectional Sparse Attention for Faster Video Diffusion Training</h3>
<ul>
<li><strong>Authors: </strong>Chenlu Zhan, Wen Li, Chuyu Shen, Jun Zhang, Suhui Wu, Hao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01085">https://arxiv.org/abs/2509.01085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01085">https://arxiv.org/pdf/2509.01085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01085]] Bidirectional Sparse Attention for Faster Video Diffusion Training(https://arxiv.org/abs/2509.01085)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Video diffusion Transformer (DiT) models excel in generative quality but hit major computational bottlenecks when producing high-resolution, long-duration videos. The quadratic complexity of full attention leads to prohibitively high training and inference costs. Full attention inefficiency stems from two key challenges: excessive computation due to the inherent sparsity of Queries and Key-Value pairs, and redundant computation as fixed sparse patterns fail to leverage DiT's dynamic attention. To overcome this limitation, we propose a Bidirectional Sparse Attention (BSA) framework for faster video DiT training, the first to dynamically sparsify both Queries and Key-Value pairs within 3D full attention, thereby substantially improving training and inference efficiency. BSA addresses these issues through two key components. Query sparsity is optimized by selecting the most informative query tokens via semantic similarity and with a dynamic spatial-time training strategy, while KV sparsity is achieved by computing a statistical dynamic threshold to retain only the most salient KV blocks for computation. Extensive experiments demonstrate that BSA significantly accelerates DiT training across long sequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention training, while preserving or even surpassing the generative quality of full attention.</li>
<li><strong>摘要：</strong>视频扩散变压器（DIT）模型在生成质量方面表现出色，但在制作高分辨率，长期视频时会达到主要的计算瓶颈。全部注意力的二次复杂性导致高度培训和推理成本。效率低下的关注源于两个关键挑战：由于查询和键值对的固有稀疏性而导致过度的计算，并且随着固定稀疏模式无法利用DIT的动态关注，冗余计算。为了克服这一局限性，我们提出了一个双向稀疏注意（BSA）框架，以进行更快的视频训练，这是第一个在3D全部关注中动态稀疏查询和键值对的框架，从而实质上提高了训练和推理效率。 BSA通过两个关键组件解决了这些问题。通过语义相似性和动态空间时间训练策略选择最有用的查询令牌，可以优化查询稀疏性，而通过计算统计动态阈值来实现KV稀疏性，仅保留最明显的KV块来计算。广泛的实验表明，BSA显着加速了跨长序列的DIT训练，使FLOP降低到20倍，并在保持17.79倍的关注训练中，同时保留甚至超过了全部注意力的产生质量。</li>
</ul>

<h3>Title: FICGen: Frequency-Inspired Contextual Disentanglement for Layout-driven Degraded Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Wenzhuang Wang, Yifan Zhao, Mingcan Ma, Ming Liu, Zhonglin Jiang, Yong Chen, Jia Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01107">https://arxiv.org/abs/2509.01107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01107">https://arxiv.org/pdf/2509.01107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01107]] FICGen: Frequency-Inspired Contextual Disentanglement for Layout-driven Degraded Image Generation(https://arxiv.org/abs/2509.01107)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Layout-to-image (L2I) generation has exhibited promising results in natural domains, but suffers from limited generative fidelity and weak alignment with user-provided layouts when applied to degraded scenes (i.e., low-light, underwater). We primarily attribute these limitations to the "contextual illusion dilemma" in degraded conditions, where foreground instances are overwhelmed by context-dominant frequency distributions. Motivated by this, our paper proposes a new Frequency-Inspired Contextual Disentanglement Generative (FICGen) paradigm, which seeks to transfer frequency knowledge of degraded images into the latent diffusion space, thereby facilitating the rendering of degraded instances and their surroundings via contextual frequency-aware guidance. To be specific, FICGen consists of two major steps. Firstly, we introduce a learnable dual-query mechanism, each paired with a dedicated frequency resampler, to extract contextual frequency prototypes from pre-collected degraded exemplars in the training set. Secondly, a visual-frequency enhanced attention is employed to inject frequency prototypes into the degraded generation process. To alleviate the contextual illusion and attribute leakage, an instance coherence map is developed to regulate latent-space disentanglement between individual instances and their surroundings, coupled with an adaptive spatial-frequency aggregation module to reconstruct spatial-frequency mixed degraded representations. Extensive experiments on 5 benchmarks involving a variety of degraded scenarios-from severe low-light to mild blur-demonstrate that FICGen consistently surpasses existing L2I methods in terms of generative fidelity, alignment and downstream auxiliary trainability.</li>
<li><strong>摘要：</strong>布局到图像（L2i）的一代在天然域中表现出了令人鼓舞的结果，但是当应用于退化的场景（即低光，水下）时，生成的保真度和与用户提供的布局的一致性有限。我们主要将这些局限性归因于在退化条件下的“上下文幻觉困境”，在这种情况下，前景实例被上下文主持的频率分布所淹没。在此激励的基础上，我们的论文提出了一种新的频率启发的上下文解散生成（FICGEN）范式，该范式旨在将退化图像的频率知识传递到潜在的扩散空间中，从而通过上下文频率宣传指导促进了降级实例及其周围环境的渲染。具体来说，FICGEN由两个主要步骤组成。首先，我们引入了一个可学习的双疑化机制，每种机制都与专用的频率重采样器配对，以从训练集中的预收集的退化示例中提取上下文频率原型。其次，采用视觉频率增强的注意力将频率原型注入降解的生成过程中。为了减轻上下文的幻觉和属性泄漏，开发了一个实例相干图，以调节各个实例及其周围环境之间的潜在空间分离，并与自适应的空间频率聚合模块相结合以重建空间频率频率混合脱落的表示。在5个基准上进行的大量实验，涉及各种降解的场景，从严重的低光中进行轻度模糊表明，ficgen始终超过了现有的L2I方法，从生成的忠诚度，对准性和下游辅助训练性来看。</li>
</ul>

<h3>Title: GPSToken: Gaussian Parameterized Spatially-adaptive Tokenization for Image Representation and Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhengqiang Zhang, Rongyuan Wu, Lingchen Sun, Lei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01109">https://arxiv.org/abs/2509.01109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01109">https://arxiv.org/pdf/2509.01109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01109]] GPSToken: Gaussian Parameterized Spatially-adaptive Tokenization for Image Representation and Generation(https://arxiv.org/abs/2509.01109)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Effective and efficient tokenization plays an important role in image representation and generation. Conventional methods, constrained by uniform 2D/1D grid tokenization, are inflexible to represent regions with varying shapes and textures and at different locations, limiting their efficacy of feature representation. In this work, we propose $\textbf{GPSToken}$, a novel $\textbf{G}$aussian $\textbf{P}$arameterized $\textbf{S}$patially-adaptive $\textbf{Token}$ization framework, to achieve non-uniform image tokenization by leveraging parametric 2D Gaussians to dynamically model the shape, position, and textures of different image regions. We first employ an entropy-driven algorithm to partition the image into texture-homogeneous regions of variable sizes. Then, we parameterize each region as a 2D Gaussian (mean for position, covariance for shape) coupled with texture features. A specialized transformer is trained to optimize the Gaussian parameters, enabling continuous adaptation of position/shape and content-aware feature extraction. During decoding, Gaussian parameterized tokens are reconstructed into 2D feature maps through a differentiable splatting-based renderer, bridging our adaptive tokenization with standard decoders for end-to-end training. GPSToken disentangles spatial layout (Gaussian parameters) from texture features to enable efficient two-stage generation: structural layout synthesis using lightweight networks, followed by structure-conditioned texture generation. Experiments demonstrate the state-of-the-art performance of GPSToken, which achieves rFID and FID scores of 0.65 and 1.50 on image reconstruction and generation tasks using 128 tokens, respectively. Codes and models of GPSToken can be found at $\href{this https URL}{this https URL}$.</li>
<li><strong>摘要：</strong>有效有效的令牌化在图像表示和生成中起着重要作用。传统的方法受统一的2D/1D网格令牌的约束，不灵活地表示具有不同形状和纹理的区域以及在不同的位置，从而限制了其特征表示的功效。 In this work, we propose $\textbf{GPSToken}$, a novel $\textbf{G}$aussian $\textbf{P}$arameterized $\textbf{S}$patially-adaptive $\textbf{Token}$ization framework, to achieve non-uniform image tokenization by leveraging parametric 2D Gaussians to动态建模不同图像区域的形状，位置和纹理。我们首先采用熵驱动的算法将图像划分为可变大小的纹理均匀区域。然后，我们将每个区域作为2D高斯（平均位置，形状的协方差）和纹理特征的参数化。对专门的变压器进行了训练，可以优化高斯参数，从而可以连续适应位置/形状和内容感知功能提取。在解码过程中，高斯参数为代币通过基于可区分的剥离渲染器重建为2D特征图，将我们的适应性令牌与标准解码器桥接起来，以进行端到端训练。 GPSTOKEN从纹理特征中删除空间布局（高斯参数），以实现有效的两阶段生成：使用轻质网络的结构布局合成，然后是结构条件条件的纹理生成。实验证明了GPSTOKEN的最新性能，该表现分别在图像重建和使用128个代币的图像重建和生成任务上达到0.65和1.50。 GPStoken的代码和模型可以在$ \ href {this HTTPS url} {此https url} $上找到。</li>
</ul>

<h3>Title: FocusDPO: Dynamic Preference Optimization for Multi-Subject Personalized Image Generation via Adaptive Focus</h3>
<ul>
<li><strong>Authors: </strong>Qiaoqiao Jin, Siming Fu, Dong She, Weinan Jia, Hualiang Wang, Mu Liu, Jidong Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01181">https://arxiv.org/abs/2509.01181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01181">https://arxiv.org/pdf/2509.01181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01181]] FocusDPO: Dynamic Preference Optimization for Multi-Subject Personalized Image Generation via Adaptive Focus(https://arxiv.org/abs/2509.01181)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multi-subject personalized image generation aims to synthesize customized images containing multiple specified subjects without requiring test-time optimization. However, achieving fine-grained independent control over multiple subjects remains challenging due to difficulties in preserving subject fidelity and preventing cross-subject attribute leakage. We present FocusDPO, a framework that adaptively identifies focus regions based on dynamic semantic correspondence and supervision image complexity. During training, our method progressively adjusts these focal areas across noise timesteps, implementing a weighted strategy that rewards information-rich patches while penalizing regions with low prediction confidence. The framework dynamically adjusts focus allocation during the DPO process according to the semantic complexity of reference images and establishes robust correspondence mappings between generated and reference subjects. Extensive experiments demonstrate that our method substantially enhances the performance of existing pre-trained personalized generation models, achieving state-of-the-art results on both single-subject and multi-subject personalized image synthesis benchmarks. Our method effectively mitigates attribute leakage while preserving superior subject fidelity across diverse generation scenarios, advancing the frontier of controllable multi-subject image synthesis.</li>
<li><strong>摘要：</strong>多主体个性化的图像生成旨在合成包含多个指定主题的自定义图像，而无需测试时间优化。但是，由于难以保留受试者保真度和防止跨主题属性泄漏，因此对多个受试者进行细粒度独立控制仍然具有挑战性。我们提出了焦点DPO，该框架是根据动态语义对应关系和监督图像复杂性自适应地识别焦点区域的。在培训期间，我们的方法逐渐调整了跨噪声时间步长的这些焦点区域，实施了加权策略，以奖励信息丰富的补丁，同时以低预测信心对区域进行惩罚。该框架根据参考图像的语义复杂性在DPO过程中动态调整焦点分配，并在生成的主体和参考主体之间建立强大的对应映射。广泛的实验表明，我们的方法基本上提高了现有的预训练的个性化生成模型的性能，从而在单个受试者和多主体个性化的图像合成基准上都取得了最新的结果。我们的方法有效地减轻了属性泄漏，同时保留了各种生成场景的优越的忠诚度，从而推进了可控的多主体图像合成的前沿。</li>
</ul>

<h3>Title: SegAssess: Panoramic quality mapping for robust and transferable unsupervised segmentation assessment</h3>
<ul>
<li><strong>Authors: </strong>Bingnan Yang, Mi Zhang, Zhili Zhang, Zhan Zhang, Yuanxin Zhao, Xiangyun Hu, Jianya Gong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01183">https://arxiv.org/abs/2509.01183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01183">https://arxiv.org/pdf/2509.01183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01183]] SegAssess: Panoramic quality mapping for robust and transferable unsupervised segmentation assessment(https://arxiv.org/abs/2509.01183)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>High-quality image segmentation is fundamental to pixel-level geospatial analysis in remote sensing, necessitating robust segmentation quality assessment (SQA), particularly in unsupervised settings lacking ground truth. Although recent deep learning (DL) based unsupervised SQA methods show potential, they often suffer from coarse evaluation granularity, incomplete assessments, and poor transferability. To overcome these limitations, this paper introduces Panoramic Quality Mapping (PQM) as a new paradigm for comprehensive, pixel-wise SQA, and presents SegAssess, a novel deep learning framework realizing this approach. SegAssess distinctively formulates SQA as a fine-grained, four-class panoramic segmentation task, classifying pixels within a segmentation mask under evaluation into true positive (TP), false positive (FP), true negative (TN), and false negative (FN) categories, thereby generating a complete quality map. Leveraging an enhanced Segment Anything Model (SAM) architecture, SegAssess uniquely employs the input mask as a prompt for effective feature integration via cross-attention. Key innovations include an Edge Guided Compaction (EGC) branch with an Aggregated Semantic Filter (ASF) module to refine predictions near challenging object edges, and an Augmented Mixup Sampling (AMS) training strategy integrating multi-source masks to significantly boost cross-domain robustness and zero-shot transferability. Comprehensive experiments across 32 datasets derived from 6 sources demonstrate that SegAssess achieves state-of-the-art (SOTA) performance and exhibits remarkable zero-shot transferability to unseen masks, establishing PQM via SegAssess as a robust and transferable solution for unsupervised SQA. The code is available at this https URL.</li>
<li><strong>摘要：</strong>高质量的图像细分是遥感中的像素级地理空间分析的基础，需要强大的分割质量评估（SQA），尤其是在缺乏地面真相的无监督环境中。尽管最近基于深度学习（DL）的无监督SQA方法显示出潜力，但它们通常会遭受粗略的评估粒度，不完整的评估和不良的可传递性。为了克服这些局限性，本文介绍了全景质量映射（PQM），作为综合，像素SQA的新范式，并展示了Segassess，这是一种实现这种方法的新颖深入学习框架。 Segassess将SQA独特地提出为精细的四级全景分割任务，将评估的分割掩码中的像素分类为真实的正（TP），false（FP），真为负（TN）和假阴性（FN）类别，从而产生完整的质量图。 Segassess利用增强的细分段（SAM）体系结构，独特地采用了输入掩码，以提示通过交叉注意力进行有效的特征集成。关键创新包括带有汇总语义滤波器（ASF）模块的边缘引导压实分支（EGC）分支，以优化在具有挑战性的对象边缘附近的预测，以及增强的混合采样采样（AMS）训练策略，整合了多源遮罩，以显着提高跨域鲁棒性鲁棒性鲁棒性和零变位可传递性。从6个来源得出的32个数据集进行的全面实验表明，Segassess实现了最先进的（SOTA）性能，并表现出明显的零拍可转移性，从而通过Segassess建立PQM，作为一种可靠的解决方案，可用于无人居住的SQA。该代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Measuring Image-Relation Alignment: Reference-Free Evaluation of VLMs and Synthetic Pre-training for Open-Vocabulary Scene Graph Generation</h3>
<ul>
<li><strong>Authors: </strong>Maëlic Neau, Zoe Falomir, Cédric Buche, Akihiro Sugimoto</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01209">https://arxiv.org/abs/2509.01209</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01209">https://arxiv.org/pdf/2509.01209</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01209]] Measuring Image-Relation Alignment: Reference-Free Evaluation of VLMs and Synthetic Pre-training for Open-Vocabulary Scene Graph Generation(https://arxiv.org/abs/2509.01209)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Scene Graph Generation (SGG) encodes visual relationships between objects in images as graph structures. Thanks to the advances of Vision-Language Models (VLMs), the task of Open-Vocabulary SGG has been recently proposed where models are evaluated on their functionality to learn a wide and diverse range of relations. Current benchmarks in SGG, however, possess a very limited vocabulary, making the evaluation of open-source models inefficient. In this paper, we propose a new reference-free metric to fairly evaluate the open-vocabulary capabilities of VLMs for relation prediction. Another limitation of Open-Vocabulary SGG is the reliance on weakly supervised data of poor quality for pre-training. We also propose a new solution for quickly generating high-quality synthetic data through region-specific prompt tuning of VLMs. Experimental results show that pre-training with this new data split can benefit the generalization capabilities of Open-Voc SGG models.</li>
<li><strong>摘要：</strong>场景图生成（SGG）编码图像中对象之间的视觉关系作为图形结构。多亏了视觉模型（VLM）的进步，最近提出了开放式Vocabulary SGG的任务，其中评估了其功能，以学习广泛而多样化的关系。但是，SGG中的当前基准测试具有非常有限的词汇量，从而评估开源模型效率低下。在本文中，我们提出了一个新的无参考度量，以公平地评估VLMS的开放式视频量表以进行关系预测。开放式SGG的另一个局限性是依赖较弱的预训练质量较差数据。我们还提出了一种新解决方案，用于通过VLM的区域特定及时调整快速生成高质量的合成数据。实验结果表明，使用此新数据拆分的预训练可以使开放式SGG模型的概括能力受益。</li>
</ul>

<h3>Title: FantasyHSI: Video-Generation-Centric 4D Human Synthesis In Any Scene through A Graph-based Multi-Agent Framework</h3>
<ul>
<li><strong>Authors: </strong>Lingzhou Mu, Qiang Wang, Fan Jiang, Mengchao Wang, Yaqi Fan, Mu Xu, Kai Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01232">https://arxiv.org/abs/2509.01232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01232">https://arxiv.org/pdf/2509.01232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01232]] FantasyHSI: Video-Generation-Centric 4D Human Synthesis In Any Scene through A Graph-based Multi-Agent Framework(https://arxiv.org/abs/2509.01232)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Human-Scene Interaction (HSI) seeks to generate realistic human behaviors within complex environments, yet it faces significant challenges in handling long-horizon, high-level tasks and generalizing to unseen scenes. To address these limitations, we introduce FantasyHSI, a novel HSI framework centered on video generation and multi-agent systems that operates without paired data. We model the complex interaction process as a dynamic directed graph, upon which we build a collaborative multi-agent system. This system comprises a scene navigator agent for environmental perception and high-level path planning, and a planning agent that decomposes long-horizon goals into atomic actions. Critically, we introduce a critic agent that establishes a closed-loop feedback mechanism by evaluating the deviation between generated actions and the planned path. This allows for the dynamic correction of trajectory drifts caused by the stochasticity of the generative model, thereby ensuring long-term logical consistency. To enhance the physical realism of the generated motions, we leverage Direct Preference Optimization (DPO) to train the action generator, significantly reducing artifacts such as limb distortion and foot-sliding. Extensive experiments on our custom SceneBench benchmark demonstrate that FantasyHSI significantly outperforms existing methods in terms of generalization, long-horizon task completion, and physical realism. Ours project page: this https URL</li>
<li><strong>摘要：</strong>人类习惯互动（HSI）试图在复杂的环境中产生现实的人类行为，但它在处理长途，高级任务并推广到看不见的场景方面面临着巨大的挑战。为了解决这些限制，我们介绍了Fantasyhsi，这是一个以视频生成和多代理系统为中心的新型HSI框架，无需配对数据。我们将复杂的交互过程建模为动态的有向图，并在其上构建协作多代理系统。该系统包括用于环境感知和高级路径计划的场景导航代理，以及将长马目标分解为原子行动的计划代理。至关重要的是，我们介绍了一个评论家，该批评者通过评估生成的动作与计划中的路径之间的偏差来建立闭环反馈机制。这允许通过生成模型的随机性引起的轨迹漂移的动态校正，从而确保了长期的逻辑一致性。为了增强生成动作的物理现实主义，我们利用直接偏好优化（DPO）来训练动作发生器，从而大大减少肢体失真和脚部滑动等工件。在我们的自定义场景基准基准上进行的广泛实验表明，Fantasyhsi在概括，长途任务完成和物理现实主义方面大大优于现有方法。我们的项目页面：此HTTPS URL</li>
</ul>

<h3>Title: Towards More Diverse and Challenging Pre-training for Point Cloud Learning: Self-Supervised Cross Reconstruction with Decoupled Views</h3>
<ul>
<li><strong>Authors: </strong>Xiangdong Zhang, Shaofeng Zhang, Junchi Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01250">https://arxiv.org/abs/2509.01250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01250">https://arxiv.org/pdf/2509.01250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01250]] Towards More Diverse and Challenging Pre-training for Point Cloud Learning: Self-Supervised Cross Reconstruction with Decoupled Views(https://arxiv.org/abs/2509.01250)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Point cloud learning, especially in a self-supervised way without manual labels, has gained growing attention in both vision and learning communities due to its potential utility in a wide range of applications. Most existing generative approaches for point cloud self-supervised learning focus on recovering masked points from visible ones within a single view. Recognizing that a two-view pre-training paradigm inherently introduces greater diversity and variance, it may thus enable more challenging and informative pre-training. Inspired by this, we explore the potential of two-view learning in this domain. In this paper, we propose Point-PQAE, a cross-reconstruction generative paradigm that first generates two decoupled point clouds/views and then reconstructs one from the other. To achieve this goal, we develop a crop mechanism for point cloud view generation for the first time and further propose a novel positional encoding to represent the 3D relative position between the two decoupled views. The cross-reconstruction significantly increases the difficulty of pre-training compared to self-reconstruction, which enables our method to surpass previous single-modal self-reconstruction methods in 3D self-supervised learning. Specifically, it outperforms the self-reconstruction baseline (Point-MAE) by 6.5%, 7.0%, and 6.7% in three variants of ScanObjectNN with the Mlp-Linear evaluation protocol. The code is available at this https URL.</li>
<li><strong>摘要：</strong>Point Cloud Learning，尤其是在没有手动标签的情况下以一种自我监督的方式，由于其在广泛的应用中的潜在效用，在视觉和学习社区中都引起了人们的关注。点云自我监督学习的大多数生成方法集中在单个视图中从可见点中恢复掩盖点。认识到两视图预训练范式固有地引入了更大的多样性和差异，因此它可能会使更具挑战性和信息丰富的预培训。受此启发，我们探讨了该领域的两视图学习的潜力。在本文中，我们提出了Point-Pqae，这是一种跨重建生成范式，首先生成两个解耦的点云/视图，然后从另一个重建一个。为了实现这一目标，我们首次开发了一种用于点云视图生成的作物机制，并进一步提出了一种新颖的位置编码，以代表两个解耦视图之间的3D相对位置。与自我重建相比，跨重建大大增加了预训练的难度，这使我们的方法能够超过3D自我监督学习中以前的单模式自我重建方法。具体而言，使用MLP线性评估协议，它在三种scanobjectnn中的自我重建基线（Point-MAE）的表现优于6.5％，7.0％和6.7％。该代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: ReCap: Event-Aware Image Captioning with Article Retrieval and Semantic Gaussian Normalization</h3>
<ul>
<li><strong>Authors: </strong>Thinh-Phuc Nguyen, Thanh-Hai Nguyen, Gia-Huy Dinh, Lam-Huy Nguyen, Minh-Triet Tran, Trung-Nghia Le</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01259">https://arxiv.org/abs/2509.01259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01259">https://arxiv.org/pdf/2509.01259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01259]] ReCap: Event-Aware Image Captioning with Article Retrieval and Semantic Gaussian Normalization(https://arxiv.org/abs/2509.01259)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Image captioning systems often produce generic descriptions that fail to capture event-level semantics which are crucial for applications like news reporting and digital archiving. We present ReCap, a novel pipeline for event-enriched image retrieval and captioning that incorporates broader contextual information from relevant articles to generate narrative-rich, factually grounded captions. Our approach addresses the limitations of standard vision-language models that typically focus on visible content while missing temporal, social, and historical contexts. ReCap comprises three integrated components: (1) a robust two-stage article retrieval system using DINOv2 embeddings with global feature similarity for initial candidate selection followed by patch-level mutual nearest neighbor similarity re-ranking; (2) a context extraction framework that synthesizes information from article summaries, generic captions, and original source metadata; and (3) a large language model-based caption generation system with Semantic Gaussian Normalization to enhance fluency and relevance. Evaluated on the OpenEvents V1 dataset as part of Track 1 in the EVENTA 2025 Grand Challenge, ReCap achieved a strong overall score of 0.54666, ranking 2nd on the private test set. These results highlight ReCap's effectiveness in bridging visual perception with real-world knowledge, offering a practical solution for context-aware image understanding in high-stakes domains. The code is available at this https URL.</li>
<li><strong>摘要：</strong>图像字幕系统通常会产生通用描述，这些描述无法捕获事件级的语义，这些语义对于新闻报告和数字归档等应用至关重要。我们提出了回顾，这是一种用于事件增强图像检索和字幕的新型管道，其中包含了来自相关文章的更广泛的上下文信息，以生成叙事丰富的，实际上扎根的标题。我们的方法解决了标准视觉语言模型的局限性，这些模型通常专注于可见内容，同时缺少时间，社会和历史背景。 RECAP包括三个集成的组件：（1）使用具有全局特征相似性的Dinov2嵌入的稳健的两阶段文章检索系统，用于初始候选选择，然后是贴片级相互的近距离邻居相似性重新排列； （2）从文章摘要，通用字幕和原始源元数据中综合信息的上下文提取框架； （3）具有语义高斯标准化的基于语言模型的大型字幕生成系统，以提高流利性和相关性。 RECAP在Eventa 2025 Grand Challenge中作为轨道1的一部分进行评估，作为轨道1的一部分，Recap的总体得分为0.54666，在私人测试集中排名第二。这些结果突出了回顾在用现实世界知识桥接视觉感知的有效性，为高风险域中的上下文感知图像理解提供了实用的解决方案。该代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Guided Model-based LiDAR Super-Resolution for Resource-Efficient Automotive scene Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Alexandros Gkillas, Nikos Piperigkos, Aris S. Lalos</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01317">https://arxiv.org/abs/2509.01317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01317">https://arxiv.org/pdf/2509.01317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01317]] Guided Model-based LiDAR Super-Resolution for Resource-Efficient Automotive scene Segmentation(https://arxiv.org/abs/2509.01317)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>High-resolution LiDAR data plays a critical role in 3D semantic segmentation for autonomous driving, but the high cost of advanced sensors limits large-scale deployment. In contrast, low-cost sensors such as 16-channel LiDAR produce sparse point clouds that degrade segmentation accuracy. To overcome this, we introduce the first end-to-end framework that jointly addresses LiDAR super-resolution (SR) and semantic segmentation. The framework employs joint optimization during training, allowing the SR module to incorporate semantic cues and preserve fine details, particularly for smaller object classes. A new SR loss function further directs the network to focus on regions of interest. The proposed lightweight, model-based SR architecture uses significantly fewer parameters than existing LiDAR SR approaches, while remaining easily compatible with segmentation networks. Experiments show that our method achieves segmentation performance comparable to models operating on high-resolution and costly 64-channel LiDAR data.</li>
<li><strong>摘要：</strong>高分辨率LiDAR数据在3D语义分段中起着至关重要的作用，用于自动驾驶，但高级传感器的高成本限制了大规模部署。相反，低成本传感器（例如16通道激光雷达）产生稀疏点云，从而降低分割精度。为了克服这一点，我们介绍了第一个端到端的框架，该框架共同解决了LiDar超级分辨率（SR）和语义分割。该框架在培训过程中采用了联合优化，从而使SR模块可以合并语义提示并保留细节，特别是对于较小的对象类。新的SR损失功能进一步指导网络专注于感兴趣的区域。所提出的基于模型的SR体系结构所采用的参数明显少于现有的LIDAR SR方法，同时保持与分割网络兼容。实验表明，我们的方法可以达到分割性能，与在高分辨率和昂贵的64通道激光雷达数据上运行的模型相当。</li>
</ul>

<h3>Title: Street-Level Geolocalization Using Multimodal Large Language Models and Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Yunus Serhat Bicakci, Joseph Shingleton, Anahid Basiri</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01341">https://arxiv.org/abs/2509.01341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01341">https://arxiv.org/pdf/2509.01341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01341]] Street-Level Geolocalization Using Multimodal Large Language Models and Retrieval-Augmented Generation(https://arxiv.org/abs/2509.01341)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Street-level geolocalization from images is crucial for a wide range of essential applications and services, such as navigation, location-based recommendations, and urban planning. With the growing popularity of social media data and cameras embedded in smartphones, applying traditional computer vision techniques to localize images has become increasingly challenging, yet highly valuable. This paper introduces a novel approach that integrates open-weight and publicly accessible multimodal large language models with retrieval-augmented generation. The method constructs a vector database using the SigLIP encoder on two large-scale datasets (EMP-16 and OSV-5M). Query images are augmented with prompts containing both similar and dissimilar geolocation information retrieved from this database before being processed by the multimodal large language models. Our approach has demonstrated state-of-the-art performance, achieving higher accuracy compared against three widely used benchmark datasets (IM2GPS, IM2GPS3k, and YFCC4k). Importantly, our solution eliminates the need for expensive fine-tuning or retraining and scales seamlessly to incorporate new data sources. The effectiveness of retrieval-augmented generation-based multimodal large language models in geolocation estimation demonstrated by this paper suggests an alternative path to the traditional methods which rely on the training models from scratch, opening new possibilities for more accessible and scalable solutions in GeoAI.</li>
<li><strong>摘要：</strong>来自图像的街道级地理定位对于广泛的基本应用和服务至关重要，例如导航，基于位置的建议和城市规划。随着社交媒体数据和嵌入在智能手机中的相机的日益普及，应用传统的计算机视觉技术来本地化图像变得越来越具有挑战性，但非常有价值。本文介绍了一种新颖的方法，该方法将开放式且公共可访问的多模式模型与检索效果相结合。该方法使用两个大规模数据集（EMP-16和OSV-5M）上的siglip编码器构建矢量数据库。在通过多模式大语言模型处理之前，通过包含从该数据库中检索到的相似和不同的地理位置信息的提示来增强查询图像。与三个广泛使用的基准数据集（IM2GPS，IM2GPS3K和YFCC4K）相比，我们的方法表明了最先进的性能，具有更高的精度。重要的是，我们的解决方案消除了对昂贵的微调或再培训的需求，并无缝地缩放以结合新的数据源。本文所证明的地理位置估算中基于检索的基于生成的多模式大型语言模型的有效性提出了依靠从头开始依靠训练模型的传统方法的替代途径，为在Geoai中提供了更易于访问且可扩展的解决方案的新可能性。</li>
</ul>

<h3>Title: Causal Sensitivity Identification using Generative Learning</h3>
<ul>
<li><strong>Authors: </strong>Soma Bandyopadhyay, Sudeshna Sarkar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01352">https://arxiv.org/abs/2509.01352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01352">https://arxiv.org/pdf/2509.01352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01352]] Causal Sensitivity Identification using Generative Learning(https://arxiv.org/abs/2509.01352)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this work, we propose a novel generative method to identify the causal impact and apply it to prediction tasks. We conduct causal impact analysis using interventional and counterfactual perspectives. First, applying interventions, we identify features that have a causal influence on the predicted outcome, which we refer to as causally sensitive features, and second, applying counterfactuals, we evaluate how changes in the cause affect the effect. Our method exploits the Conditional Variational Autoencoder (CVAE) to identify the causal impact and serve as a generative predictor. We are able to reduce confounding bias by identifying causally sensitive features. We demonstrate the effectiveness of our method by recommending the most likely locations a user will visit next in their spatiotemporal trajectory influenced by the causal relationships among various features. Experiments on the large-scale GeoLife [Zheng et al., 2010] dataset and the benchmark Asia Bayesian network validate the ability of our method to identify causal impact and improve predictive performance.</li>
<li><strong>摘要：</strong>在这项工作中，我们提出了一种新颖的生成方法来识别因果影响并将其应用于预测任务。我们使用介入和反事实观点进行因果影响分析。首先，采用干预措施，我们确定对预测结果有因果影响的特征，我们称之为因果敏感的特征，其次，应用反事实，我们评估原因的变化如何影响效果。我们的方法利用条件变异自动编码器（CVAE）来识别因果影响并用作生成性预测因子。我们能够通过识别因果敏感的特征来减少混杂的偏见。我们通过推荐用户将在时空轨迹下访问的最有可能的位置来证明我们方法的有效性，并受到各种特征之间因果关系的影响。大规模Geolife [Zheng等，2010]数据集和基准亚洲贝叶斯网络的实验验证了我们方法识别因果影响并改善预测性能的能力。</li>
</ul>

<h3>Title: M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via Self-Supervision</h3>
<ul>
<li><strong>Authors: </strong>Che Liu, Zheng Jiang, Chengyu Fang, Heng Guo, Yan-Jie Zhou, Jiaqi Qu, Le Lu, Minfeng Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01360">https://arxiv.org/abs/2509.01360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01360">https://arxiv.org/pdf/2509.01360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01360]] M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via Self-Supervision(https://arxiv.org/abs/2509.01360)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Medical image retrieval is essential for clinical decision-making and translational research, relying on discriminative visual representations. Yet, current methods remain fragmented, relying on separate architectures and training strategies for 2D, 3D, and video-based medical data. This modality-specific design hampers scalability and inhibits the development of unified representations. To enable unified learning, we curate a large-scale hybrid-modality dataset comprising 867,653 medical imaging samples, including 2D X-rays and ultrasounds, RGB endoscopy videos, and 3D CT scans. Leveraging this dataset, we train M3Ret, a unified visual encoder without any modality-specific customization. It successfully learns transferable representations using both generative (MAE) and contrastive (SimDINO) self-supervised learning (SSL) paradigms. Our approach sets a new state-of-the-art in zero-shot image-to-image retrieval across all individual modalities, surpassing strong baselines such as DINOv3 and the text-supervised BMC-CLIP. More remarkably, strong cross-modal alignment emerges without paired data, and the model generalizes to unseen MRI tasks, despite never observing MRI during pretraining, demonstrating the generalizability of purely visual self-supervision to unseen modalities. Comprehensive analyses further validate the scalability of our framework across model and data sizes. These findings deliver a promising signal to the medical imaging community, positioning M3Ret as a step toward foundation models for visual SSL in multimodal medical image understanding.</li>
<li><strong>摘要：</strong>医疗图像检索对于依靠判别性视觉表示，对于临床决策和翻译研究至关重要。然而，目前的方法仍然分散，依靠2D，3D和基于视频的医疗数据的单独架构和培训策略。这种特定于模式的设计会阻碍可扩展性并抑制统一表示形式的发展。为了实现统一的学习，我们策划了一个包括867,653个医学成像样本的大规模混合模式数据集，包括2D X射线和超声检查，RGB内窥镜视频和3D CT扫描。利用此数据集，我们训练M3RET，这是一种统一的视觉编码器，没有任何特定于模态的自定义。它成功学习了使用生成（MAE）和对比度（Simdino）自学学习（SSL）范式的可转移表示。我们的方法在所有单个模式中设定了一个新的最先进的图像到图像检索，超过了诸如Dinov3和文本监管的BMC-CLIP之类的强大基线。更明显的是，尽管没有配对数据，但强烈的跨模式对齐是出现的，尽管在审议过程中从未观察到MRI，但该模型概括了看不见的MRI任务，这表明纯粹的视觉自我遵守对看不见的方式的普遍性。全面的分析进一步验证了跨模型和数据大小的框架的可扩展性。这些发现向医学成像社区发出了有希望的信号，将M3RET定位为迈向多模式医学图像理解中视觉SSL的基础模型的一步。</li>
</ul>

<h3>Title: Identity-Preserving Text-to-Video Generation via Training-Free Prompt, Image, and Guidance Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Gao, Changcheng Hua, Qingchao Chen, Yuxin Peng, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01362">https://arxiv.org/abs/2509.01362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01362">https://arxiv.org/pdf/2509.01362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01362]] Identity-Preserving Text-to-Video Generation via Training-Free Prompt, Image, and Guidance Enhancement(https://arxiv.org/abs/2509.01362)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Identity-preserving text-to-video (IPT2V) generation creates videos faithful to both a reference subject image and a text prompt. While fine-tuning large pretrained video diffusion models on ID-matched data achieves state-of-the-art results on IPT2V, data scarcity and high tuning costs hinder broader improvement. We thus introduce a Training-Free Prompt, Image, and Guidance Enhancement (TPIGE) framework that bridges the semantic gap between the video description and the reference image and design sampling guidance that enhances identity preservation and video quality, achieving performance gains at minimal this http URL, we first propose Face Aware Prompt Enhancement, using GPT-4o to enhance the text prompt with facial details derived from the reference image. We then propose Prompt Aware Reference Image Enhancement, leveraging an identity-preserving image generator to refine the reference image, rectifying conflicts with the text prompt. The above mutual refinement significantly improves input quality before video generation. Finally, we propose ID-Aware Spatiotemporal Guidance Enhancement, utilizing unified gradients to optimize identity preservation and video quality jointly during this http URL method outperforms prior work and is validated by automatic and human evaluations on a 1000 video test set, winning first place in the ACM Multimedia 2025 Identity-Preserving Video Generation Challenge, demonstrating state-of-the-art performance and strong generality. The code is available at this https URL.</li>
<li><strong>摘要：</strong>具有身份的文本对视频（IPT2V）一代为参考主题图像和文本提示而创建视频。在微调ID匹配数据上的大型视频扩散模型的同时，可以在IPT2V上获得最新的结果，但数据稀缺和高调成本却在更广泛的改善中。 We thus introduce a Training-Free Prompt, Image, and Guidance Enhancement (TPIGE) framework that bridges the semantic gap between the video description and the reference image and design sampling guidance that enhances identity preservation and video quality, achieving performance gains at minimal this http URL, we first propose Face Aware Prompt Enhancement, using GPT-4o to enhance the text prompt with facial details derived from the reference image.然后，我们提出了提示意识参考图像增强功能，利用具有身份的图像生成器来完善参考图像，并用文本提示进行纠正。上面的相互改进可以显着提高视频生成之前的输入质量。最后，我们提出ID意识时空指导增强，利用统一的梯度在此HTTP URL方法期间共同优化身份保​​存和视频质量，优于先前的工作，并通过1000个视频测试集对自动和人类的评估进行了验证，在ACM MultimeDia 2025 Identia 2025 Identity Videmity Identity Identity Sententity Generation中赢得了一定的一部分，并证明了这一生成一代的范围。该代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: CbLDM: A Diffusion Model for recovering nanostructure from pair distribution function</h3>
<ul>
<li><strong>Authors: </strong>Jiarui Cao, Zhiyang Zhang, Heming Wang, Jun Xu, Ling Lan, Ran Gu</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01370">https://arxiv.org/abs/2509.01370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01370">https://arxiv.org/pdf/2509.01370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01370]] CbLDM: A Diffusion Model for recovering nanostructure from pair distribution function(https://arxiv.org/abs/2509.01370)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Nowadays, the nanostructure inverse problem is an attractive problem that helps researchers to understand the relationship between the properties and the structure of nanomaterials. This article focuses on the problem of using PDF to recover the nanostructure, which this article views as a conditional generation problem. This article propose a deep learning model CbLDM, Condition-based Latent Diffusion Model. Based on the original latent diffusion model, the sampling steps of the diffusion model are reduced and the sample generation efficiency is improved by using the conditional prior to estimate conditional posterior distribution, which is the approximated distribution of p(z|x). In addition, this article uses the Laplacian matrix instead of the distance matrix to recover the nanostructure, which can reduce the reconstruction error. Finally, this article compares CbLDM with existing models which were used to solve the nanostructure inverse problem, and find that CbLDM demonstrates significantly higher prediction accuracy than these models, which reflects the ability of CbLDM to solve the nanostructure inverse problem and the potential to cope with other continuous conditional generation tasks.</li>
<li><strong>摘要：</strong>如今，纳米结构反问题是一个有吸引力的问题，可以帮助研究人员了解纳米材料的性质与结构之间的关系。本文重点介绍了使用PDF恢复纳米结构的问题，本文将其视为有条件的生成问题。本文提出了一个深度学习模型CBLDM，基于条件的潜扩散模型。基于原始的潜扩散模型，通过在估算条件后验分布之前使用条件性的条件，即扩散模型的采样步骤可提高样本的产生效率，即P（z | x）的近似分布。此外，本文使用拉普拉斯矩阵而不是距离矩阵来恢复纳米结构，这可以减少重建误差。最后，本文将CBLDM与用于解决纳米结构逆问题的现有模型进行了比较，并发现CBLDM比这些模型证明了预测准确性明显更高，这反映了CBLDM解决纳米结构反问题的能力，并且可以与其他连续的有条件生成任务来应对其他潜力。</li>
</ul>

<h3>Title: Distillation of a tractable model from the VQ-VAE</h3>
<ul>
<li><strong>Authors: </strong>Armin Hadžić, Milan Papez, Tomáš Pevný</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01400">https://arxiv.org/abs/2509.01400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01400">https://arxiv.org/pdf/2509.01400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01400]] Distillation of a tractable model from the VQ-VAE(https://arxiv.org/abs/2509.01400)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Deep generative models with discrete latent space, such as the Vector-Quantized Variational Autoencoder (VQ-VAE), offer excellent data generation capabilities, but, due to the large size of their latent space, their probabilistic inference is deemed intractable. We demonstrate that the VQ-VAE can be distilled into a tractable model by selecting a subset of latent variables with high probabilities. This simple strategy is particularly efficient, especially if the VQ-VAE underutilizes its latent space, which is, indeed, very often the case. We frame the distilled model as a probabilistic circuit, and show that it preserves expressiveness of the VQ-VAE while providing tractable probabilistic inference. Experiments illustrate competitive performance in density estimation and conditional generation tasks, challenging the view of the VQ-VAE as an inherently intractable model.</li>
<li><strong>摘要：</strong>具有离散潜在空间的深层生成模型，例如矢量定量的变分自动编码器（VQ-VAE），具有出色的数据生成能力，但是由于其潜在空间的较大尺寸，其概率推断被认为是可悲的。我们证明，通过选择具有较高概率的潜在变量的子集，可以将VQ-VAE蒸馏成可拖动的模型。这种简单的策略特别有效，尤其是如果VQ-VAE不利于其潜在空间，这确实是这种情况。我们将蒸馏模型构架为概率电路，并表明它在提供可拖动的概率推断的同时保留了VQ-VAE的表现力。实验说明了密度估计和有条件生成任务的竞争性能，从而挑战了VQ-VAE作为固有棘手的模型的观点。</li>
</ul>

<h3>Title: MILO: A Lightweight Perceptual Quality Metric for Image and Latent-Space Optimization</h3>
<ul>
<li><strong>Authors: </strong>Uğur Çoğalan, Mojtaba Bemana, Karol Myszkowski, Hans-Peter Seidel, Colin Groth</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01411">https://arxiv.org/abs/2509.01411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01411">https://arxiv.org/pdf/2509.01411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01411]] MILO: A Lightweight Perceptual Quality Metric for Image and Latent-Space Optimization(https://arxiv.org/abs/2509.01411)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, super-resolution, generative, quality assessment</a></li>
<li><strong>Abstract: </strong>We present MILO (Metric for Image- and Latent-space Optimization), a lightweight, multiscale, perceptual metric for full-reference image quality assessment (FR-IQA). MILO is trained using pseudo-MOS (Mean Opinion Score) supervision, in which reproducible distortions are applied to diverse images and scored via an ensemble of recent quality metrics that account for visual masking effects. This approach enables accurate learning without requiring large-scale human-labeled datasets. Despite its compact architecture, MILO outperforms existing metrics across standard FR-IQA benchmarks and offers fast inference suitable for real-time applications. Beyond quality prediction, we demonstrate the utility of MILO as a perceptual loss in both image and latent domains. In particular, we show that spatial masking modeled by MILO, when applied to latent representations from a VAE encoder within Stable Diffusion, enables efficient and perceptually aligned optimization. By combining spatial masking with a curriculum learning strategy, we first process perceptually less relevant regions before progressively shifting the optimization to more visually distorted areas. This strategy leads to significantly improved performance in tasks like denoising, super-resolution, and face restoration, while also reducing computational overhead. MILO thus functions as both a state-of-the-art image quality metric and as a practical tool for perceptual optimization in generative pipelines.</li>
<li><strong>摘要：</strong>我们提出Milo（图像和潜在空间优化的度量），这是一种轻巧，多尺度，感知度量，用于全参考图像质量评估（FR-IQA）。使用伪MOS（平均意见分数）监督对Milo进行了训练，其中可重复的扭曲被应用于不同的图像，并通过最近造成视觉掩盖效果的近期质量指标进行了评分。这种方法可实现准确的学习，而无需大规模的人体标记的数据集。尽管具有紧凑的体系结构，但Milo的表现优于标准FR-IQA基准的现有指标，并提供适合实时应用程序的快速推理。除了质量预测之外，我们还证明了Milo作为图像和潜在领域的感知损失的实用性。特别是，我们表明，当将Milo建模的空间掩蔽应用于稳定扩散中VAE编码器的潜在表示时，可以实现有效且具有感知的优化优化。通过将空间掩盖与课程学习策略相结合，我们首先在感知上较小的相关区域进行处理，然后再将优化转移到更具视觉扭曲的领域。这种策略可显着提高诸如Denoising，Super-Losolution和Face Restoration之类的任务，同时还会减少计算开销。因此，米洛（Milo）既是最先进的图像质量度量标准，也是生成管道中知觉优化的实用工具。</li>
</ul>

<h3>Title: InfoScale: Unleashing Training-free Variable-scaled Image Generation via Effective Utilization of Information</h3>
<ul>
<li><strong>Authors: </strong>Guohui Zhang, Jiangtong Tan, Linjiang Huang, Zhonghang Yuan, Naishan Zheng, Jie Huang, Feng Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01421">https://arxiv.org/abs/2509.01421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01421">https://arxiv.org/pdf/2509.01421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01421]] InfoScale: Unleashing Training-free Variable-scaled Image Generation via Effective Utilization of Information(https://arxiv.org/abs/2509.01421)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models (DMs) have become dominant in visual generation but suffer performance drop when tested on resolutions that differ from the training scale, whether lower or higher. In fact, the key challenge in generating variable-scale images lies in the differing amounts of information across resolutions, which requires information conversion procedures to be varied for generating variable-scaled images. In this paper, we investigate the issues of three critical aspects in DMs for a unified analysis in variable-scaled generation: dilated convolution, attention mechanisms, and initial noise. Specifically, 1) dilated convolution in DMs for the higher-resolution generation loses high-frequency information. 2) Attention for variable-scaled image generation struggles to adjust the information aggregation adaptively. 3) The spatial distribution of information in the initial noise is misaligned with variable-scaled image. To solve the above problems, we propose \textbf{InfoScale}, an information-centric framework for variable-scaled image generation by effectively utilizing information from three aspects correspondingly. For information loss in 1), we introduce Progressive Frequency Compensation module to compensate for high-frequency information lost by dilated convolution in higher-resolution generation. For information aggregation inflexibility in 2), we introduce Adaptive Information Aggregation module to adaptively aggregate information in lower-resolution generation and achieve an effective balance between local and global information in higher-resolution generation. For information distribution misalignment in 3), we design Noise Adaptation module to re-distribute information in initial noise for variable-scaled generation. Our method is plug-and-play for DMs and extensive experiments demonstrate the effectiveness in variable-scaled image generation.</li>
<li><strong>摘要：</strong>扩散模型（DMS）在视觉产生中已占主导地位，但是在与训练量表不同的决议上进行测试时，无论是较低还是更高。实际上，生成可变尺度图像的关键挑战在于跨分辨率的信息量不同，这需要信息转换过程以生成可变尺度的图像而变化。在本文中，我们研究了DMS中三个关键方面的问题，以进行可变产生的统一分析：扩张的卷积，注意机制和初始噪声。具体而言，1）DMS中的卷积扩张，用于高分辨率生成失去高频信息。 2）注意变量尺度的图像产生努力以适应性地调整信息聚集。 3）初始噪声中信息的空间分布与可变尺度的图像未对准。为了解决上述问题，我们提出\ textbf {Infscale}，这是一个以信息为中心的框架，用于可变尺度的图像生成，通过相应地利用来自三个方面的信息。对于1）中的信息丢失，我们介绍了进行性频率补偿模块，以补偿高分辨率生成中扩张卷积丢失的高频信息。对于2）中的信息汇总不灵活性，我们引入了自适应信息聚合模块，以在低分辨率生成中适应汇总信息，并在高分辨率生成中实现本地和全球信息之间的有效平衡。对于3）中的信息分布未对准，我们设计了噪声适应模块，以将信息重新分布到初始噪声中，以使变量缩放的生成。我们的方法是DMS的插件，广泛的实验证明了可变图像生成的有效性。</li>
</ul>

<h3>Title: SoccerHigh: A Benchmark Dataset for Automatic Soccer Video Summarization</h3>
<ul>
<li><strong>Authors: </strong>Artur Díaz-Juan, Coloma Ballester, Gloria Haro</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01439">https://arxiv.org/abs/2509.01439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01439">https://arxiv.org/pdf/2509.01439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01439]] SoccerHigh: A Benchmark Dataset for Automatic Soccer Video Summarization(https://arxiv.org/abs/2509.01439)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Video summarization aims to extract key shots from longer videos to produce concise and informative summaries. One of its most common applications is in sports, where highlight reels capture the most important moments of a game, along with notable reactions and specific contextual events. Automatic summary generation can support video editors in the sports media industry by reducing the time and effort required to identify key segments. However, the lack of publicly available datasets poses a challenge in developing robust models for sports highlight generation. In this paper, we address this gap by introducing a curated dataset for soccer video summarization, designed to serve as a benchmark for the task. The dataset includes shot boundaries for 237 matches from the Spanish, French, and Italian leagues, using broadcast footage sourced from the SoccerNet dataset. Alongside the dataset, we propose a baseline model specifically designed for this task, which achieves an F1 score of 0.3956 in the test set. Furthermore, we propose a new metric constrained by the length of each target summary, enabling a more objective evaluation of the generated content. The dataset and code are available at this https URL.</li>
<li><strong>摘要：</strong>视频摘要旨在从更长的视频中提取关键镜头，以产生简洁而有益的摘要。它最常见的应用之一是在体育中，突出显示卷轴捕获了游戏中最重要的时刻，以及显着的反应和特定的上下文事件。自动摘要生成可以通过减少识别关键细分所需的时间和精力来支持体育媒体行业的视频编辑。但是，缺乏公开可用的数据集在为体育亮点一代开发健壮的模型方面构成了挑战。在本文中，我们通过引入一个用于足球视频摘要的策划数据集来解决这一差距，该数据集旨在作为任务的基准。该数据集包括使用Soccernet数据集采购的广播镜头，包括西班牙，法国和意大利联赛的237场比赛的射击边界。在数据集之外，我们提出了一个专门为此任务设计的基线模型，该模型在测试集中达到了F1分数为0.3956。此外，我们提出了一个由每个目标摘要的长度约束的新度量，从而可以对生成的内容进行更客观的评估。该数据集和代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Prior-Guided Flow Matching for Target-Aware Molecule Design with Learnable Atom Number</h3>
<ul>
<li><strong>Authors: </strong>Jingyuan Zhou, Hao Qian, Shikui Tu, Lei Xu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01486">https://arxiv.org/abs/2509.01486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01486">https://arxiv.org/pdf/2509.01486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01486]] Prior-Guided Flow Matching for Target-Aware Molecule Design with Learnable Atom Number(https://arxiv.org/abs/2509.01486)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Structure-based drug design (SBDD), aiming to generate 3D molecules with high binding affinity toward target proteins, is a vital approach in novel drug discovery. Although recent generative models have shown great potential, they suffer from unstable probability dynamics and mismatch between generated molecule size and the protein pockets geometry, resulting in inconsistent quality and off-target effects. We propose PAFlow, a novel target-aware molecular generation model featuring prior interaction guidance and a learnable atom number predictor. PAFlow adopts the efficient flow matching framework to model the generation process and constructs a new form of conditional flow matching for discrete atom types. A protein-ligand interaction predictor is incorporated to guide the vector field toward higher-affinity regions during generation, while an atom number predictor based on protein pocket information is designed to better align generated molecule size with target geometry. Extensive experiments on the CrossDocked2020 benchmark show that PAFlow achieves a new state-of-the-art in binding affinity (up to -8.31 Avg. Vina Score), simultaneously maintains favorable molecular properties.</li>
<li><strong>摘要：</strong>基于结构的药物设计（SBDD）旨在产生对靶蛋白具有高结合亲和力的3D分子，是新型药物发现中的一种至关重要的方法。尽管最近的生成模型表现出巨大的潜力，但它们的可能性动态不稳定和产生的分子大小与蛋白质口袋几何形状之间的不匹配，导致质量不一致和脱靶效应。我们提出了PAFLOW，这是一种新型的目标感知分子生成模型，具有先前的相互作用引导和可学习的原子数预测指标。 PAFlow采用有效的流匹配框架来建模生成过程，并为离散原子类型构建一种新形式的条件流匹配形式。纳入了蛋白质 - 配体相互作用预测因子，以指导载体场在生成过程中朝着高亲和力区域朝向高亲和力区域，而基于蛋白质口袋信息的原子数预测变量旨在更好地对齐产生的分子大小与目标几何形状。在CrossDocked2020基准上进行的广泛实验表明，PAFlow在结合亲和力方面达到了新的最新最新技术（高达-8.31Avg。Vina得分），同时保持有利的分子特性。</li>
</ul>

<h3>Title: A Continuous-Time Consistency Model for 3D Point Cloud Generation</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Eilermann, René Heesch, Oliver Niggemann</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01492">https://arxiv.org/abs/2509.01492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01492">https://arxiv.org/pdf/2509.01492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01492]] A Continuous-Time Consistency Model for 3D Point Cloud Generation(https://arxiv.org/abs/2509.01492)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Fast and accurate 3D shape generation from point clouds is essential for applications in robotics, AR/VR, and digital content creation. We introduce ConTiCoM-3D, a continuous-time consistency model that synthesizes 3D shapes directly in point space, without discretized diffusion steps, pre-trained teacher models, or latent-space encodings. The method integrates a TrigFlow-inspired continuous noise schedule with a Chamfer Distance-based geometric loss, enabling stable training on high-dimensional point sets while avoiding expensive Jacobian-vector products. This design supports efficient one- to two-step inference with high geometric fidelity. In contrast to previous approaches that rely on iterative denoising or latent decoders, ConTiCoM-3D employs a time-conditioned neural network operating entirely in continuous time, thereby achieving fast generation. Experiments on the ShapeNet benchmark show that ConTiCoM-3D matches or outperforms state-of-the-art diffusion and latent consistency models in both quality and efficiency, establishing it as a practical framework for scalable 3D shape generation.</li>
<li><strong>摘要：</strong>从点云中快速准确的3D形成生成对于机器人技术，AR/VR和数字内容创建中的应用至关重要。我们介绍了Conticom-3D，这是一个连续的时间一致性模型，该模型直接在点空间中综合3D形状，而无需离散的扩散步骤，预训练的教师模型或潜在空间编码。该方法将基于倒角距离的几何损失集成了三通启发的连续噪声时间表，从而在高维点集上稳定训练，同时避免了昂贵的雅各布矢量产品。该设计以高几何保真度支持有效的一到两步推断。与以前依靠迭代脱氧或潜在解码器的方法相反，Corticom-3D采用了一个时间条件的神经网络，完全在连续的时间内运行，从而实现了快速的生成。 Shapenet基准测试的实验表明，Porticom-3D在质量和效率方面都匹配或优于最先进的扩散和潜在的一致性模型，从而将其确立为可扩展3D形状生成的实用框架。</li>
</ul>

<h3>Title: Unsupervised Identification and Replay-based Detection (UIRD) for New Category Anomaly Detection in ECG Signal</h3>
<ul>
<li><strong>Authors: </strong>Zhangyue Shi, Zekai Wang, Yuxuan Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01512">https://arxiv.org/abs/2509.01512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01512">https://arxiv.org/pdf/2509.01512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01512]] Unsupervised Identification and Replay-based Detection (UIRD) for New Category Anomaly Detection in ECG Signal(https://arxiv.org/abs/2509.01512)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In clinical practice, automatic analysis of electrocardiogram (ECG) is widely applied to identify irregular heart rhythms and other electrical anomalies of the heart, enabling timely intervention and potentially improving clinical outcomes. However, due to the limited samples in certain types of ECG signals, the class imbalance issues pose a challenge for ECG-based detection. In addition, as the volume of patient data grows, long-term storage of all historical data becomes increasingly burdensome as training samples to recognize new patterns and classify existing ECG signals accurately. Therefore, to enhance the performance of anomaly detection while addressing storage limitations, we propose a pseudo-replay based semi-supervised continual learning framework, which consists of two components: unsupervised identification and replay-based detection. For unsupervised identification, an unsupervised generative adversarial network (GAN)-based framework is integrated to detect novel patterns. Besides, instead of directly storing all historical data, a pseudo replay-based learning strategy is proposed which utilizes a generator to learn the data distribution for each individual task. When a new task arises, the generator synthesizes pseudo data representative of previous learnt classes, enabling the model to detect both the existed patterns and the newly presented anomalies. The effectiveness of the proposed framework is validated in four public ECG datasets, which leverages supervised classification problems for anomaly detection. The experimental results show that the developed approach is very promising in identifying novel anomalies while maintaining good performance on detecting existing ECG signals.</li>
<li><strong>摘要：</strong>在临床实践中，心电图（ECG）的自动分析被广泛应用以确定心脏的不规则心律和其他电气异常，从而及时干预并有可能改善临床结果。但是，由于某些类型的心电图信号中的样本有限，因此类别不平衡对基于ECG的检测构成了挑战。此外，随着患者数据的数量的增长，随着培训样本以识别新模式并准确地对现有的ECG信号进行分类，所有历史数据的长期存储变得越来越繁重。因此，为了在解决存储限制的同时提高异常检测的性能，我们提出了一个基于伪复制的半监督连续学习框架，该框架由两个组成部分组成：无监督的识别和基于重播的检测。对于无监督的识别，将无监督的生成对抗网络（GAN）的框架集成在一起以检测新模式。此外，提出了基于伪重播的学习策略，而不是直接存储所有历史数据，它利用发电机来学习每个任务的数据分布。当出现新任务时，生成器综合了先前学习类的伪数据，使模型能够检测存在的模式和新呈现的异常。在四个公共ECG数据集中验证了拟议框架的有效性，该数据集利用监督分类问题进行异常检测。实验结果表明，开发的方法在识别新型异常方面非常有前途，同时在检测现有的ECG信号方面保持良好的性能。</li>
</ul>

<h3>Title: Prediction, Generation of WWTPs microbiome community structures and Clustering of WWTPs various feature attributes using DE-BP model, SiTime-GAN model and DPNG-EPMC ensemble clustering algorithm with modulation of microbial ecosystem health</h3>
<ul>
<li><strong>Authors: </strong>Mingzhi Dai, Weiwei Cai, Xiang Feng, Huiqun Yu, Weibin Guo, Miao Guo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01526">https://arxiv.org/abs/2509.01526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01526">https://arxiv.org/pdf/2509.01526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01526]] Prediction, Generation of WWTPs microbiome community structures and Clustering of WWTPs various feature attributes using DE-BP model, SiTime-GAN model and DPNG-EPMC ensemble clustering algorithm with modulation of microbial ecosystem health(https://arxiv.org/abs/2509.01526)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Microbiomes not only underpin Earth's biogeochemical cycles but also play crucial roles in both engineered and natural ecosystems, such as the soil, wastewater treatment, and the human gut. However, microbiome engineering faces significant obstacles to surmount to deliver the desired improvements in microbiome control. Here, we use the backpropagation neural network (BPNN), optimized through differential evolution (DE-BP), to predict the microbial composition of activated sludge (AS) systems collected from wastewater treatment plants (WWTPs) located worldwide. Furthermore, we introduce a novel clustering algorithm termed Directional Position Nonlinear Emotional Preference Migration Behavior Clustering (DPNG-EPMC). This method is applied to conduct a clustering analysis of WWTPs across various feature attributes. Finally, we employ the Similar Time Generative Adversarial Networks (SiTime-GAN), to synthesize novel microbial compositions and feature attributes data. As a result, we demonstrate that the DE-BP model can provide superior predictions of the microbial composition. Additionally, we show that the DPNG-EPMC can be applied to the analysis of WWTPs under various feature attributes. Finally, we demonstrate that the SiTime-GAN model can generate valuable incremental synthetic data. Our results, obtained through predicting the microbial community and conducting analysis of WWTPs under various feature attributes, develop an understanding of the factors influencing AS communities.</li>
<li><strong>摘要：</strong>微生物组不仅是地球的生物地球化学周期的基础，而且在工程和自然生态系统（例如土壤，废水处理和人类肠道）中都起着至关重要的作用。但是，微生物组工程面临着重大障碍，以克服微生物组控制所需的改进。在这里，我们使用通过差异进化（DE-BP）优化的反向传播神经网络（BPNN）来预测从全球废水处理厂（WWTPS）收集的活性污泥（AS）系统的微生物组成。此外，我们引入了一种新型的聚类算法，称为方向位置非线性情绪偏好迁移行为聚类（DPNG-EPMC）。该方法应用于在各种特征属性上对WWTPS进行聚类分析。最后，我们采用类似的时间生成对抗网络（Sitime-GAN）来合成新型的微生物组成和特征属性数据。结果，我们证明了DE-BP模型可以为微生物组成提供优质的预测。此外，我们表明DPNG-EPMC可以应用于各种特征属性下的WWTPS分析。最后，我们证明Sitime-GAN模型可以生成有价值的增量合成数据。我们的结果是通过预测微生物群落并在各种特征属性下对WWTP进行分析获得的结果，他们对影响为社区的因素有了了解。</li>
</ul>

<h3>Title: Feynman-Kac-Flow: Inference Steering of Conditional Flow Matching to an Energy-Tilted Posterior</h3>
<ul>
<li><strong>Authors: </strong>Konstantin Mark, Leonard Galustian, Maximilian P.-P. Kovar, Esther Heid</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01543">https://arxiv.org/abs/2509.01543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01543">https://arxiv.org/pdf/2509.01543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01543]] Feynman-Kac-Flow: Inference Steering of Conditional Flow Matching to an Energy-Tilted Posterior(https://arxiv.org/abs/2509.01543)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Conditional Flow Matching(CFM) represents a fast and high-quality approach to generative modelling, but in many applications it is of interest to steer the generated samples towards precise requirements. While steering approaches like gradient-based guidance, sequential Monte Carlo steering or Feynman-Kac steering are well established for diffusion models, they have not been extended to flow matching approaches yet. In this work, we formulate this requirement as tilting the output with an energy potential. We derive, for the first time, Feynman-Kac steering for CFM. We evaluate our approach on a set of synthetic tasks, including the generation of tilted distributions in a high-dimensional space, which is a particularly challenging case for steering approaches. We then demonstrate the impact of Feynman-Kac steered CFM on the previously unsolved challenge of generated transition states of chemical reactions with the correct chirality, where the reactants or products can have a different handedness, leading to geometric constraints of the viable reaction pathways connecting reactants and products. Code to reproduce this study is avaiable open-source at this https URL.</li>
<li><strong>摘要：</strong>有条件的流量匹配（CFM）代表了一种快速且高质量的生成建模方法，但是在许多应用中，将生成的样品引导到精确的需求很感兴趣。尽管基于梯度的指导，顺序蒙特卡洛转向或Feynman-kac转向的转向方法已经很好地建立了扩散模型，但尚未将其扩展到流匹配方法。在这项工作中，我们将这一要求提出为以能量电位倾斜输出。我们首次获得了CFM的Feynman-Kac转向。我们在一组合成任务上评估了我们的方法，包括在高维空间中产生倾斜的分布，这对于转向方法来说是一个特别具有挑战性的案例。然后，我们证明了Feynman-Kac的影响CFM对具有正确手性的化学反应的过渡状态的先前未解决的挑战，在这种情况下，反应物或产品可以具有不同的手段，从而导致连接反应物和产物的可行反应途径的几何约束。在此HTTPS URL上，可以避免使用该研究的代码。</li>
</ul>

<h3>Title: Model Unmerging: Making Your Models Unmergeable for Secure Model Sharing</h3>
<ul>
<li><strong>Authors: </strong>Zihao Wang, Enneng Yang, Lu Yin, Shiwei Liu, Li Shen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01548">https://arxiv.org/abs/2509.01548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01548">https://arxiv.org/pdf/2509.01548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01548]] Model Unmerging: Making Your Models Unmergeable for Secure Model Sharing(https://arxiv.org/abs/2509.01548)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Model merging leverages multiple finetuned expert models to construct a multi-task model with low cost, and is gaining increasing attention. However, as a growing number of finetuned models become publicly available, concerns about the safety of model merging have emerged. Unauthorized merging may infringe on developers' rights and risk leaking sensitive personal information. Most existing methods focus on detecting whether a merged model originates from a specific source model, but fail to effectively prevent illegal merging. In this paper, we propose MergeLock, an active protection mechanism that disrupts model parameters to render them unmergeable, thereby directly preventing unauthorized model merging. Specifically, leveraging the inherent symmetry of the attention mechanism in Transformer-based models, we randomly sample two pairs of invertible matrices and apply them to the Query-Key (QK) and Value-Output (VO) branches. This transformation keeps the model's output unchanged while pushing it away from the shared parameter space of other finetuned models. Extensive experiments across both vision and language tasks demonstrate that MergeLock can degrade the performance of merged models by over 95% when a protected model is involved in most cases, demonstrating its effectiveness. Moreover, we further demonstrate that merged models protected by MergeLock cannot be effectively recovered using low-cost restoration methods, further enhancing robustness against unauthorized merging. The code is available at this https URL.</li>
<li><strong>摘要：</strong>模型合并的模型利用了多个填充专家模型来构建具有低成本的多任务模型，并且正在越来越关注。但是，随着越来越多的填充模型公开可用，人们对模型合并安全性的担忧已经出现。未经授权的合并可能会侵犯开发商的权利和泄漏敏感的个人信息的风险。大多数现有的方法着重于检测合并模型是否来自特定的源模型，但无法有效防止非法合并。在本文中，我们提出了Mergelock，这是一种有效的保护机制，它破坏了模型参数以使其变得不可加操作，从而直接防止未经授权的模型合并。具体而言，我们利用基于变压器的模型中注意机制的固有对称性，我们随机采样了两对可逆矩阵，并将其应用于查询键（QK）和值输出（VO）分支。这种转换使模型的输出保持不变，同时将其远离其他列出模型的共享参数空间。视觉和语言任务的广泛实验表明，当在大多数情况下涉及受保护的模型时，Mergelock可以将合并模型的性能降低95％，从而证明其有效性。此外，我们进一步证明，不能使用低成本恢复方法有效地回收了受默格洛克保护的合并模型，从而进一步增强了针对未经授权合并的鲁棒性。该代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Variation-aware Vision Token Dropping for Faster Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Junjie Chen, Xuyang Liu, Zichen Wen, Yiyu Wang, Siteng Huang, Honggang Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01552">https://arxiv.org/abs/2509.01552</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01552">https://arxiv.org/pdf/2509.01552</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01552]] Variation-aware Vision Token Dropping for Faster Large Vision-Language Models(https://arxiv.org/abs/2509.01552)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large vision-language models (LVLMs) have demonstrated remarkable capabilities in multimodal understanding tasks. However, the increasing demand for high-resolution image and long-video understanding results in substantial token counts, leading to reduced inference efficiency. Token compression offers a direct solution by reducing the number of tokens to be processed, thereby improving computational efficiency. Through extensive analysis, we identify two critical limitations in existing inner-LLM token compression methods: positional bias and incompatibility with efficient operators, which hinder their practical deployment for LVLM acceleration. This paper presents the first approach from a token variation perspective, revealing that visual token variations within LLMs exhibit task-agnostic properties. We propose Variation-aware Vision Token Dropping (\textit{i.e.}, \textbf{V$^2$Drop}), which progressively removes visual tokens with minimal variation during LVLM inference, thereby enhancing computational efficiency. Extensive experiments across multiple models and benchmarks demonstrate that our V$^2$Drop is able to maintain \textbf{94.0\%} and \textbf{98.6\%} of the original model performance for image and video understanding tasks respectively, while reducing LLM generation latency by \textbf{31.5\%} and \textbf{74.2\%}. When combined with efficient operators, V$^2$Drop further reduces GPU peak memory usage.</li>
<li><strong>摘要：</strong>大型视觉模型（LVLM）在多模式理解任务中表现出了显着的功能。但是，对高​​分辨率图像和长期理解的需求不断增长，导致了大量令牌计数，从而降低了推理效率。令牌压缩通过减少要处理的令牌数量来提供直接解决方案，从而提高了计算效率。通过广泛的分析，我们确定了现有的内部LLM令牌压缩方法的两个关键局限性：位置偏见和与有效操作员的不兼容，这阻碍了其实际部署LVLM加速。本文从令牌变化的角度提出了第一种方法，表明LLMS内的视觉令牌变化具有任务不合命中率。我们提出了变异感知的视觉令牌下降（\ textit {i.e。}，\ textbf {v $^2 $ drop}），该降低在LVLM推理期间逐渐消除具有最小变化的视觉令牌，从而提高了计算效率。多种模型和基准的广泛实验表明，我们的V $^2 $降低能够分别维护\ textbf {94.0 \％}和\ textbf {98.6 \％}图像和视频理解任务的原始模型性能的，而\ textbf则降低了\ textbf {31.5 \％}，并降低了LLM生成llm llm的生成。当与有效的运算符结合使用时，V $^2 $降低进一步降低了GPU峰值内存的使用情况。</li>
</ul>

<h3>Title: Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling</h3>
<ul>
<li><strong>Authors: </strong>Natalia Frumkin, Diana Marculescu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01624">https://arxiv.org/abs/2509.01624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01624">https://arxiv.org/pdf/2509.01624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01624]] Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling(https://arxiv.org/abs/2509.01624)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models are computationally intensive, often requiring dozens of forward passes through large transformer backbones. For instance, Stable Diffusion XL generates high-quality images with 50 evaluations of a 2.6B-parameter model, an expensive process even for a single batch. Few-step diffusion models reduce this cost to 2-8 denoising steps but still depend on large, uncompressed U-Net or diffusion transformer backbones, which are often too costly for full-precision inference without datacenter GPUs. These requirements also limit existing post-training quantization methods that rely on full-precision calibration. We introduce Q-Sched, a new paradigm for post-training quantization that modifies the diffusion model scheduler rather than model weights. By adjusting the few-step sampling trajectory, Q-Sched achieves full-precision accuracy with a 4x reduction in model size. To learn quantization-aware pre-conditioning coefficients, we propose the JAQ loss, which combines text-image compatibility with an image quality metric for fine-grained optimization. JAQ is reference-free and requires only a handful of calibration prompts, avoiding full-precision inference during calibration. Q-Sched delivers substantial gains: a 15.5% FID improvement over the FP16 4-step Latent Consistency Model and a 16.6% improvement over the FP16 8-step Phased Consistency Model, showing that quantization and few-step distillation are complementary for high-fidelity generation. A large-scale user study with more than 80,000 annotations further confirms Q-Sched's effectiveness on both FLUX.1[schnell] and SDXL-Turbo.</li>
<li><strong>摘要：</strong>文本到图像扩散模型在计算上是密集型的，通常需要数十个前向通过大型变压器骨架。例如，稳定的扩散XL生成高质量的图像，使用50个2.6B参数模型的评估，即使是单个批次，也是一个昂贵的过程。几步之遥的扩散模型将这一成本降低到2-8个脱索步骤，但仍然取决于大型，未压缩的U-NET或扩散变压器骨架，对于没有数据中心GPU的完整推理来说，这通常太成本太高了。这些要求还限制了依赖完全精确校准的现有培训后量化方法。我们介绍了Q-SCHED，这是一种用于修改扩散模型调度程序而不是模型权重的新范围训练量化范式。通过调整少量样本轨迹，Q-SCHED可实现完整精度，模型尺寸降低了4倍。为了学习量化意识的预处理系数，我们提出了JAQ损失，该损失将文本图像兼容性与图像质量指标结合在一起，以进行细粒度优化。 JAQ无参考，仅需要少数校准提示，避免在校准过程中进行全精确推理。 Q-SCHED可实现可观的增长：比FP16 4步潜在一致性模型提高了15.5％的FID，比FP16 8步分阶段的一致性模型提高了16.6％，这表明量化和很少的步骤蒸馏是互补的。一项具有80,000多个注释的大规模用户研究进一步证实了Q-Sched对Flux.1 [Schnell]和SDXL-Turbo的有效性。</li>
</ul>

<h3>Title: Relative Trajectory Balance is equivalent to Trust-PCL</h3>
<ul>
<li><strong>Authors: </strong>Tristan Deleu, Padideh Nouri, Yoshua Bengio, Doina Precup</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01632">https://arxiv.org/abs/2509.01632</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01632">https://arxiv.org/pdf/2509.01632</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01632]] Relative Trajectory Balance is equivalent to Trust-PCL(https://arxiv.org/abs/2509.01632)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent progress in generative modeling has highlighted the importance of Reinforcement Learning (RL) for fine-tuning, with KL-regularized methods in particular proving to be highly effective for both autoregressive and diffusion models. Complementing this line of work, the Relative Trajectory Balance (RTB) objective was recently introduced in the context of Generative Flow Networks (GFlowNets) to serve the same role of improving fine-tuning in sequential generative models. Building on prior work linking GFlowNets and maximum-entropy RL, we establish in this paper an equivalence between RTB and Trust-PCL, an off-policy RL method with KL regularization. This equivalence situates RTB within the broader theoretical landscape of KL-regularized RL, and clarifies its relationship to earlier methods. Leveraging this insight, we revisit an illustrative example from the RTB paper and show that KL-regularized RL methods achieve comparable performance, offering an alternative perspective to what was previously reported.</li>
<li><strong>摘要：</strong>生成建模的最新进展强调了增强学习（RL）对于微调的重要性，特别是通过KL调查方法证明对自动回归和扩散模型都非常有效。在补充这一工作线上，最近在生成流网络（Gflownets）的背景下引入了相对轨迹平衡（RTB）目标，以发挥相同的作用，以改善顺序生成模型中的微调。在链接Gflownets和Maximum-Entropy RL的先前工作的基础上，我们在本文中建立了RTB和Trust-PCL之间的等效性，RTB和Trust-PCL是KL正则化的一种非政策RL方法。这种等效性将RTB置于KL登记的RL的更广泛的理论景观中，并阐明了其与早期方法的关系。利用这种见解，我们重新审视了RTB论文中的一个说明性示例，并表明KL调查的RL方法具有可比的性能，为先前报道的内容提供了另一种观点。</li>
</ul>

<h3>Title: OpenVision 2: A Family of Generative Pretrained Visual Encoders for Multimodal Learning</h3>
<ul>
<li><strong>Authors: </strong>Yanqing Liu, Xianhang Li, Letian Zhang, Zirui Wang, Zeyu Zheng, Yuyin Zhou, Cihang Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01644">https://arxiv.org/abs/2509.01644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01644">https://arxiv.org/pdf/2509.01644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01644]] OpenVision 2: A Family of Generative Pretrained Visual Encoders for Multimodal Learning(https://arxiv.org/abs/2509.01644)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper provides a simplification on OpenVision's architecture and loss design for enhancing its training efficiency. Following the prior vision-language pretraining works CapPa and AIMv2, as well as modern multimodal designs like LLaVA, our changes are straightforward: we remove the text encoder (and therefore the contrastive loss), retaining only the captioning loss as a purely generative training signal. We name this new version OpenVision 2. The initial results are promising: despite this simplification, OpenVision 2 competitively matches the original model's performance on a broad set of multimodal benchmarks while substantially cutting both training time and memory consumption. For example, with ViT-L/14, it reduces training time by about 1.5x (from 83h to 57h), and memory usage by about 1.8x (from 24.5GB to 13.8GB, equivalently allowing the maximum batch size to grow from 2k to 8k). This superior training efficiency also allows us to scale far beyond the largest vision encoder used in OpenVision, reaching more than 1 billion parameters. We hold a strong belief that this lightweight, generative-only paradigm is compelling for future vision encoder development in multimodal foundation models.</li>
<li><strong>摘要：</strong>本文简化了OpenVision的体系结构和损失设计，以提高其培训效率。遵循先前的视觉训练预处理作品CAPPA和AIMV2，以及Llava等现代的多模式设计，我们的变化很简单：我们删除了文本编码器（因此以及对比度损失），仅保留字幕损失作为纯粹生成的训练信号。我们命名了这个新版本的OpenVision 2。最初的结果是有希望的：尽管简化了，OpenVision 2在广泛的多模式基准测试中竞争竞争性匹配原始模型的性能，同时大大减少了训练时间和内存消耗。例如，使用VIT-L/14，它将训练时间减少了约1.5倍（从83h到57h），并且记忆使用率减少了约1.8倍（从24.5GB到13.8GB，等效地允许最大批次大小从2K增长到8K）。这种卓越的训练效率还使我们能够扩展到开放式中使用的最大视力编码器，达到了超过10亿个参数。我们坚信，这种轻巧的，仅生成的范式对多模式基础模型中的未来视觉编码器的发展具有吸引力。</li>
</ul>

<h3>Title: Reinforced Visual Perception with Tools</h3>
<ul>
<li><strong>Authors: </strong>Zetong Zhou, Dongping Chen, Zixian Ma, Zhihan Hu, Mingyang Fu, Sinan Wang, Yao Wan, Zhou Zhao, Ranjay Krishna</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01656">https://arxiv.org/abs/2509.01656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01656">https://arxiv.org/pdf/2509.01656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01656]] Reinforced Visual Perception with Tools(https://arxiv.org/abs/2509.01656)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Visual reasoning, a cornerstone of human intelligence, encompasses complex perceptual and logical processes essential for solving diverse visual problems. While advances in computer vision have produced powerful models for various perceptual tasks, leveraging these for general visual reasoning remains challenging. Prior work demonstrates that augmenting LLMs with vision models via supervised finetuning improves performance, but faces key limitations such as expensive data generation, reliance on careful data filtering, and poor generalization. To address these issues, we propose ReVPT to enhance multi-modal LLMs' abilities to reason about and use visual tools through reinforcement learning. We introduce a novel RL algorithm based on GRPO, designed to train models to reason with a suite of four visual tools. Through extensive experiments, we show that our method achieves state-of-the-art performance on several perception-heavy benchmarks, including SAT, CV-Bench, BLINK and MMStar, significantly outperforming the supervised and text-based RL finetuning baselines. Notably, Our ReVPT-3B and ReVPT-7B outperform the instruct models by 9.03% and 9.44% on CV-Bench. Finally, we bring to the community new insights on RL-based visual tool-usage through extensive ablations. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>视觉推理是人类智力的基石，它涵盖了解决各种视觉问题所必需的复杂感知和逻辑过程。尽管计算机视觉的进步为各种感知任务产生了强大的模型，但利用这些任务来进行一般视觉推理仍然具有挑战性。先前的工作表明，通过监督填充来增强视觉模型的LLM可以提高性能，但面临着昂贵的数据生成，依赖仔细数据过滤的关键局限性以及不良的概括。为了解决这些问题，我们提出了Revpt，以增强多模式LLM的能力来通过增强学习来推理和使用视觉工具。我们介绍了一种基于GRPO的新型RL算法，该算法旨在使用四个视觉工具来训练模型。通过广泛的实验，我们表明我们的方法在几个较重的基准测试中实现了最先进的性能，包括SAT，CV-Bench，眨眼，眨眼和MMSTAR，显着优于基于监督和基于文本的RL Finetuntuning Baselines。值得注意的是，我们的REVPT-3B和REVPT-7B在CV板台上优于9.03％和9.44％的指示模型。最后，我们通过大量消融为社区带来了有关基于RL的视觉工具使用的新见解。我们的代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: PractiLight: Practical Light Control Using Foundational Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yotam Erel, Rishabh Dabral, Vladislav Golyanik, Amit H. Bermano, Christian Theobalt</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01837">https://arxiv.org/abs/2509.01837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01837">https://arxiv.org/pdf/2509.01837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01837]] PractiLight: Practical Light Control Using Foundational Diffusion Models(https://arxiv.org/abs/2509.01837)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Light control in generated images is a difficult task, posing specific challenges, spanning over the entire image and frequency spectrum. Most approaches tackle this problem by training on extensive yet domain-specific datasets, limiting the inherent generalization and applicability of the foundational backbones used. Instead, PractiLight is a practical approach, effectively leveraging foundational understanding of recent generative models for the task. Our key insight is that lighting relationships in an image are similar in nature to token interaction in self-attention layers, and hence are best represented there. Based on this and other analyses regarding the importance of early diffusion iterations, PractiLight trains a lightweight LoRA regressor to produce the direct irradiance map for a given image, using a small set of training images. We then employ this regressor to incorporate the desired lighting into the generation process of another image using Classifier Guidance. This careful design generalizes well to diverse conditions and image domains. We demonstrate state-of-the-art performance in terms of quality and control with proven parameter and data efficiency compared to leading works over a wide variety of scenes types. We hope this work affirms that image lighting can feasibly be controlled by tapping into foundational knowledge, enabling practical and general relighting.</li>
<li><strong>摘要：</strong>生成的图像中的光控制是一项艰巨的任务，构成了特定的挑战，跨越了整个图像和频率频谱。大多数方法通过对广泛但特定领域的数据集进行培训来解决此问题，从而限制了所使用的基础骨干的固有概括和适用性。取而代之的是，实习是一种实践方法，有效地利用了对最近生成模型的基础理解。我们的关键见解是，图像中的照明关系本质上与自我发项层中的令牌相互作用相似，因此最好在那里代表。基于有关早期扩散迭代的重要性的该分析和其他分析，实用训练轻巧的Lora回归器，使用一小部分训练图像生成给定图像的直接辐照图。然后，我们使用此回归器将所需的照明纳入使用分类器指南的另一个图像的生成过程中。这种仔细的设计很好地推广到了各种条件和图像域。与各种场景类型的领先作品相比，我们在质量和控制方面表现出了最先进的性能。我们希望这项工作确认，可以通过利用基础知识，实现实用和一般的重新照明来控制图像照明。</li>
</ul>

<h3>Title: Deep Reinforcement Learning for Real-Time Drone Routing in Post-Disaster Road Assessment Without Domain Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Huatian Gong, Jiuh-Biing Sheu, Zheng Wang, Xiaoguang Yang, Ran Yan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01886">https://arxiv.org/abs/2509.01886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01886">https://arxiv.org/pdf/2509.01886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01886]] Deep Reinforcement Learning for Real-Time Drone Routing in Post-Disaster Road Assessment Without Domain Knowledge(https://arxiv.org/abs/2509.01886)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Rapid post-disaster road damage assessment is critical for effective emergency response, yet traditional optimization methods suffer from excessive computational time and require domain knowledge for algorithm design, making them unsuitable for time-sensitive disaster scenarios. This study proposes an attention-based encoder-decoder model (AEDM) for real-time drone routing decision in post-disaster road damage assessment. The method employs deep reinforcement learning to determine high-quality drone assessment routes without requiring algorithmic design knowledge. A network transformation method is developed to convert link-based routing problems into equivalent node-based formulations, while a synthetic road network generation technique addresses the scarcity of large-scale training datasets. The model is trained using policy optimization with multiple optima (POMO) with multi-task learning capabilities to handle diverse parameter combinations. Experimental results demonstrate two key strengths of AEDM: it outperforms commercial solvers by 16--69\% in solution quality and achieves real-time inference (1--2 seconds) versus 100--2,000 seconds for traditional methods. The model exhibits strong generalization across varying problem scales, drone numbers, and time constraints, consistently outperforming baseline methods on unseen parameter distributions and real-world road networks. The proposed method effectively balances computational efficiency with solution quality, making it particularly suitable for time-critical disaster response applications where rapid decision-making is essential for saving lives.</li>
<li><strong>摘要：</strong>快速的灾后道路损害评估对于有效的应急响应至关重要，但是传统优化方法的计算时间过多，并且需要域知识以进行算法设计，这使得它们不适合时间敏感的灾难场景。这项研究提出了一种基于注意力的编码器模型（AEDM），用于实时无人机路由决策，并在污点后道路损害评估中进行实时路由决策。该方法采用深度强化学习来确定高质量的无人机评估途径，而无需算法设计知识。开发了一种网络转换方法将基于链路的路由问题转换为基于等效节点的配方，而合成的道路网络生成技术解决了大规模培训数据集的稀缺性。该模型是使用具有多任务学习功能的多个Optima（POMO）的策略优化培训的，以处理各种参数组合。实验结果证明了AEDM的两个关键优势：它的溶液质量优于16--69 \％，并实现实时推理（1--2秒）与传统方法的100--2,000秒。该模型在不同的问题量表，无人机数量和时间限制之间表现出强烈的概括，对看不见的参数分布和现实世界道路网络的表现始终优于基线方法。提出的方法有效地将计算效率与解决方案质量均衡，使其特别适合于迅速决策对于挽救生命至关重要的时期灾难响应应用程序。</li>
</ul>

<h3>Title: DroneSR: Rethinking Few-shot Thermal Image Super-Resolution from Drone-based Perspective</h3>
<ul>
<li><strong>Authors: </strong>Zhipeng Weng, Xiaopeng Liu, Ce Liu, Xingyuan Guo, Yukai Shi, Liang Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01898">https://arxiv.org/abs/2509.01898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01898">https://arxiv.org/pdf/2509.01898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01898]] DroneSR: Rethinking Few-shot Thermal Image Super-Resolution from Drone-based Perspective(https://arxiv.org/abs/2509.01898)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generative</a></li>
<li><strong>Abstract: </strong>Although large scale models achieve significant improvements in performance, the overfitting challenge still frequently undermines their generalization ability. In super resolution tasks on images, diffusion models as representatives of generative models typically adopt large scale architectures. However, few-shot drone-captured infrared training data frequently induces severe overfitting in large-scale architectures. To address this key challenge, our method proposes a new Gaussian quantization representation learning method oriented to diffusion models that alleviates overfitting and enhances robustness. At the same time, an effective monitoring mechanism tracks large scale architectures during training to detect signs of overfitting. By introducing Gaussian quantization representation learning, our method effectively reduces overfitting while maintaining architecture complexity. On this basis, we construct a multi source drone-based infrared image benchmark dataset for detection and use it to emphasize overfitting issues of large scale architectures in few sample, drone-based diverse drone-based image reconstruction scenarios. To verify the efficacy of the method in mitigating overfitting, experiments are conducted on the constructed benchmark. Experimental results demonstrate that our method outperforms existing super resolution approaches and significantly mitigates overfitting of large scale architectures under complex conditions. The code and DroneSR dataset will be available at: this https URL.</li>
<li><strong>摘要：</strong>尽管大型模型在性能方面取得了重大改善，但过度拟合的挑战仍然经常破坏其概括能力。在图像上的超级分辨率任务中，扩散模型作为生成模型的代表通常采用大型体系结构。但是，很少有无人机捕获的红外训练数据经常引起大规模架构中的严重过度拟合。为了应对这一关键挑战，我们的方法提出了一种新的高斯量化表示方法学习方法，该学习方法是针对减轻过度拟合并增强鲁棒性的扩散模型的。同时，有效的监视机制在训练过程中跟踪大规模架构，以检测过度拟合的迹象。通过引入高斯量化表示学习，我们的方法可以有效地减少过度拟合，同时保持体系结构的复杂性。在此基础上，我们构建了一个基于多源无人机的红外图像基准数据集以进行检测，并使用它来强调在几个样本，基于无人机的基于无人机的图像重建方案中大规模架构的过度拟合问题。为了验证该方法在缓解过度拟合的功效，在构造的基准上进行了实验。实验结果表明，我们的方法优于现有的超级分辨率方法，并在复杂条件下显着减轻了大规模架构的过度拟合。代码和无人机数据集将在以下网址提供：此HTTPS URL。</li>
</ul>

<h3>Title: A Diffusion-Based Framework for Configurable and Realistic Multi-Storage Trace Generation</h3>
<ul>
<li><strong>Authors: </strong>Seohyun Kim, Junyoung Lee, Jongho Park, Jinhyung Koo, Sungjin Lee, Yeseong Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01919">https://arxiv.org/abs/2509.01919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01919">https://arxiv.org/pdf/2509.01919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01919]] A Diffusion-Based Framework for Configurable and Realistic Multi-Storage Trace Generation(https://arxiv.org/abs/2509.01919)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We propose DiTTO, a novel diffusion-based framework for generating realistic, precisely configurable, and diverse multi-device storage traces. Leveraging advanced diffusion tech- niques, DiTTO enables the synthesis of high-fidelity continuous traces that capture temporal dynamics and inter-device dependencies with user-defined configurations. Our experimental results demonstrate that DiTTO can generate traces with high fidelity and diversity while aligning closely with guided configurations with only 8% errors.</li>
<li><strong>摘要：</strong>我们提出了Ditto，这是一种基于扩散的新型框架，用于生成逼真的，可配置和多样化的多设备存储轨迹。利用高级扩散技术，同上可以合成高保真性连续跟踪，以捕获具有用户定义的配置的时间动力学和设备间依赖关系。我们的实验结果表明，同上可以生成具有高忠诚度和多样性的痕迹，同时与只有8％误差的指导配置紧密对齐。</li>
</ul>

<h3>Title: 2D Gaussian Splatting with Semantic Alignment for Image Inpainting</h3>
<ul>
<li><strong>Authors: </strong>Hongyu Li, Chaofeng Chen, Xiaoming Li, Guangming Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01964">https://arxiv.org/abs/2509.01964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01964">https://arxiv.org/pdf/2509.01964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01964]] 2D Gaussian Splatting with Semantic Alignment for Image Inpainting(https://arxiv.org/abs/2509.01964)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, super-resolution</a></li>
<li><strong>Abstract: </strong>Gaussian Splatting (GS), a recent technique for converting discrete points into continuous spatial representations, has shown promising results in 3D scene modeling and 2D image super-resolution. In this paper, we explore its untapped potential for image inpainting, which demands both locally coherent pixel synthesis and globally consistent semantic restoration. We propose the first image inpainting framework based on 2D Gaussian Splatting, which encodes incomplete images into a continuous field of 2D Gaussian splat coefficients and reconstructs the final image via a differentiable rasterization process. The continuous rendering paradigm of GS inherently promotes pixel-level coherence in the inpainted results. To improve efficiency and scalability, we introduce a patch-wise rasterization strategy that reduces memory overhead and accelerates inference. For global semantic consistency, we incorporate features from a pretrained DINO model. We observe that DINO's global features are naturally robust to small missing regions and can be effectively adapted to guide semantic alignment in large-mask scenarios, ensuring that the inpainted content remains contextually consistent with the surrounding scene. Extensive experiments on standard benchmarks demonstrate that our method achieves competitive performance in both quantitative metrics and perceptual quality, establishing a new direction for applying Gaussian Splatting to 2D image processing.</li>
<li><strong>摘要：</strong>高斯脱落（GS）是一种将离散点转换为连续空间表示的技术，它在3D场景建模和2D图像超分辨率中显示出令人鼓舞的结果。在本文中，我们探讨了其未开发的图像覆盖潜力，这既需要局部连贯的像素合成和全球一致的语义恢复。我们提出了基于2D高斯脱落的第一个图像介绍框架，该框架将不完整的图像编码为2D高斯Splat系数的连续字段，并通过可区分的栅格化过程重建了最终图像。 GS的持续渲染范式固有地促进了成分结果中的像素级连贯性。为了提高效率和可扩展性，我们引入了贴片的栅格化策略，该策略可降低内存开销并加速推理。对于全球语义一致性，我们结合了验证的Dino模型的功能。我们观察到，Dino的全球特征自然对小缺少区域具有鲁棒性，并且可以有效地适应大面罩场景中的语义对齐，从而确保了成熟的内容与周围的场景保持上下文。对标准基准测试的广泛实验表明，我们的方法在定量指标和感知质量方面都达到了竞争性能，从而建立了将高斯脱落应用于2D图像处理的新方向。</li>
</ul>

<h3>Title: Knowledge distillation as a pathway toward next-generation intelligent ecohydrological modeling systems</h3>
<ul>
<li><strong>Authors: </strong>Long Jiang, Yang Yang, Ting Fong May Chui, Morgan Thornwell, Hoshin Vijai Gupta</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.geo-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01972">https://arxiv.org/abs/2509.01972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01972">https://arxiv.org/pdf/2509.01972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01972]] Knowledge distillation as a pathway toward next-generation intelligent ecohydrological modeling systems(https://arxiv.org/abs/2509.01972)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Simulating ecohydrological processes is essential for understanding complex environmental systems and guiding sustainable management amid accelerating climate change and human pressures. Process-based models provide physical realism but can suffer from structural rigidity, high computational costs, and complex calibration, while machine learning (ML) methods are efficient and flexible yet often lack interpretability and transferability. We propose a unified three-phase framework that integrates process-based models with ML and progressively embeds them into artificial intelligence (AI) through knowledge distillation. Phase I, behavioral distillation, enhances process models via surrogate learning and model simplification to capture key dynamics at lower computational cost. Phase II, structural distillation, reformulates process equations as modular components within a graph neural network (GNN), enabling multiscale representation and seamless integration with ML models. Phase III, cognitive distillation, embeds expert reasoning and adaptive decision-making into intelligent modeling agents using the Eyes-Brain-Hands-Mouth architecture. Demonstrations for the Samish watershed highlight the framework's applicability to ecohydrological modeling, showing that it can reproduce process-based model outputs, improve predictive accuracy, and support scenario-based decision-making. The framework offers a scalable and transferable pathway toward next-generation intelligent ecohydrological modeling systems, with the potential extension to other process-based domains.</li>
<li><strong>摘要：</strong>模拟生态水文过程对于理解复杂的环境系统和指导可持续管理至关重要，并加速了气候变化和人类压力。基于过程的模型提供了物理现实主义，但可能会遭受结构性僵化，高计算成本和复杂的校准，而机器学习（ML）方法是有效且灵活的，但通常缺乏可解释性和可传递性。我们提出了一个统一的三相框架，该框架将基于过程的模型与ML集成，并通过知识蒸馏逐渐将它们嵌入人工智能（AI）。第一阶段的行为蒸馏，通过替代学习和模型简化增强了过程模型，以较低的计算成本捕获关键动态。第二阶段的结构蒸馏，将过程方程式重新定义为图神经网络（GNN）中的模块化组件，从而使多尺度表示并与ML模型无缝集成。第三阶段的认知蒸馏，将专家推理和适应性决策嵌入到使用眼睛脑嘴巴体系结构的智能建模剂中。萨米什（Samish）流域的演示突出了该框架对生态水文建模的适用性，表明它可以重现基于过程的模型输出，提高预测精度并支持基于场景的决策。该框架为下一代智能生态水文建模系统提供了可扩展且可转移的途径，并具有潜在扩展到其他基于过程的域。</li>
</ul>

<h3>Title: MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware Alignment and Disentanglement</h3>
<ul>
<li><strong>Authors: </strong>Dong She, Siming Fu, Mushui Liu, Qiaoqiao Jin, Hualiang Wang, Mu Liu, Jidong Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01977">https://arxiv.org/abs/2509.01977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01977">https://arxiv.org/pdf/2509.01977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01977]] MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware Alignment and Disentanglement(https://arxiv.org/abs/2509.01977)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multi-subject personalized generation presents unique challenges in maintaining identity fidelity and semantic coherence when synthesizing images conditioned on multiple reference subjects. Existing methods often suffer from identity blending and attribute leakage due to inadequate modeling of how different subjects should interact within shared representation spaces. We present MOSAIC, a representation-centric framework that rethinks multi-subject generation through explicit semantic correspondence and orthogonal feature disentanglement. Our key insight is that multi-subject generation requires precise semantic alignment at the representation level - knowing exactly which regions in the generated image should attend to which parts of each reference. To enable this, we introduce SemAlign-MS, a meticulously annotated dataset providing fine-grained semantic correspondences between multiple reference subjects and target images, previously unavailable in this domain. Building on this foundation, we propose the semantic correspondence attention loss to enforce precise point-to-point semantic alignment, ensuring high consistency from each reference to its designated regions. Furthermore, we develop the multi-reference disentanglement loss to push different subjects into orthogonal attention subspaces, preventing feature interference while preserving individual identity characteristics. Extensive experiments demonstrate that MOSAIC achieves state-of-the-art performance on multiple benchmarks. Notably, while existing methods typically degrade beyond 3 subjects, MOSAIC maintains high fidelity with 4+ reference subjects, opening new possibilities for complex multi-subject synthesis applications.</li>
<li><strong>摘要：</strong>当合成以多个参考主题为条件的图像时，多主体个性化生成在保持身份保真度和语义连贯性方面提出了独特的挑战。现有方法通常由于对不同受试者如何在共享表示空间中相互作用的建模而经常遭受身份混合和属性泄漏的影响。我们提出Mosaic，这是一个以表示为中心的框架，可通过明确的语义对应关系和正交特征分离来重新考虑多主体的生成。我们的关键见解是，多主题生成需要在表示级别上精确的语义对齐方式 - 确切知道生成图像中的哪些区域应参与每个参考的哪些部分。为了实现这一目标，我们介绍了Semalign-MS，这是一种精心注释的数据集，可在多个参考主体和目标图像之间提供精细的语义对应关系，以前在该域中不可用。在这个基础的基础上，我们提出了语义对应的注意力损失，以实施精确的点对点语义对准，从而确保每个参考都对其指定区域的高度一致性。此外，我们将多参考的分解损失发展为将不同的受试者推向正交注意子空间，从而防止特征干扰，同时保留个人身份特征。广泛的实验表明，马赛克在多个基准测试中实现了最先进的性能。值得注意的是，尽管现有方法通常会降低3个受试者，但Mosaic通过4个以上的参考主题保持高忠诚度，为复杂的多主体合成应用开辟了新的可能性。</li>
</ul>

<h3>Title: Discrete Noise Inversion for Next-scale Autoregressive Text-based Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Quan Dao, Xiaoxiao He, Ligong Han, Ngan Hoai Nguyen, Amin Heyrani Nobar, Faez Ahmed, Han Zhang, Viet Anh Nguyen, Dimitris Metaxas</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01984">https://arxiv.org/abs/2509.01984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01984">https://arxiv.org/pdf/2509.01984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01984]] Discrete Noise Inversion for Next-scale Autoregressive Text-based Image Editing(https://arxiv.org/abs/2509.01984)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Visual autoregressive models (VAR) have recently emerged as a promising class of generative models, achieving performance comparable to diffusion models in text-to-image generation tasks. While conditional generation has been widely explored, the ability to perform prompt-guided image editing without additional training is equally critical, as it supports numerous practical real-world applications. This paper investigates the text-to-image editing capabilities of VAR by introducing Visual AutoRegressive Inverse Noise (VARIN), the first noise inversion-based editing technique designed explicitly for VAR models. VARIN leverages a novel pseudo-inverse function for argmax sampling, named Location-aware Argmax Inversion (LAI), to generate inverse Gumbel noises. These inverse noises enable precise reconstruction of the source image and facilitate targeted, controllable edits aligned with textual prompts. Extensive experiments demonstrate that VARIN effectively modifies source images according to specified prompts while significantly preserving the original background and structural details, thus validating its efficacy as a practical editing approach.</li>
<li><strong>摘要：</strong>视觉自回旋模型（VAR）最近已成为一类有希望的生成模型类别，实现了与文本到图像生成任务中扩散模型相当的性能。尽管有条件的生成已被广泛探索，但在没有其他培训的情况下执行及时引导图像编辑的能力同样至关重要，因为它支持众多实用的现实世界应用。本文通过引入视觉自动回归反向噪声（VARIN）来研究VAR的文本对图像编辑功能，这是第一个基于噪声反转的编辑技术，该技术是针对VAR模型明确设计的。瓦林利用新颖的伪内函数来进行argmax采样，称为位置吸引的argmax倒置（LAI），以产生倒数gumbel的噪声。这些逆噪声可实现源图像的精确重建，并促进有针对性的可控编辑与文本提示对齐。广泛的实验表明，varin根据指定的提示有效地修改源图像，同时显着保留原始背景和结构细节，从而验证其作为实际编辑方法的功效。</li>
</ul>

<h3>Title: Draw-In-Mind: Learning Precise Image Editing via Chain-of-Thought Imagination</h3>
<ul>
<li><strong>Authors: </strong>Ziyun Zeng, Junhao Zhang, Wei Li, Mike Zheng Shou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01986">https://arxiv.org/abs/2509.01986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01986">https://arxiv.org/pdf/2509.01986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01986]] Draw-In-Mind: Learning Precise Image Editing via Chain-of-Thought Imagination(https://arxiv.org/abs/2509.01986)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In recent years, integrating multimodal understanding and generation into a single unified model has emerged as a promising paradigm. While this approach achieves strong results in text-to-image (T2I) generation, it still struggles with precise image editing. We attribute this limitation to an imbalanced division of responsibilities. The understanding module primarily functions as a translator that encodes user instructions into semantic conditions, while the generation module must simultaneously act as designer and painter, inferring the original layout, identifying the target editing region, and rendering the new content. This imbalance is counterintuitive because the understanding module is typically trained with several times more data on complex reasoning tasks than the generation module. To address this issue, we introduce Draw-In-Mind (DIM), a dataset comprising two complementary subsets: (i) DIM-T2I, containing 14M long-context image-text pairs to enhance complex instruction comprehension; and (ii) DIM-Edit, consisting of 233K chain-of-thought imaginations generated by GPT-4o, serving as explicit design blueprints for image edits. We connect a frozen Qwen2.5-VL-3B with a trainable SANA1.5-1.6B via a lightweight two-layer MLP, and train it on the proposed DIM dataset, resulting in DIM-4.6B-T2I/Edit. Despite its modest parameter scale, DIM-4.6B-Edit achieves SOTA or competitive performance on the ImgEdit and GEdit-Bench benchmarks, outperforming much larger models such as UniWorld-V1 and Step1X-Edit. These findings demonstrate that explicitly assigning the design responsibility to the understanding module provides significant benefits for image editing. Our dataset and models will be available at this https URL.</li>
<li><strong>摘要：</strong>近年来，将多模式的理解和生成整合到单个统一模型中已成为有希望的范式。尽管这种方法在文本到图像（T2i）的一代中取得了强大的结果，但它仍然在精确的图像编辑中挣扎。我们将这种限制归因于职责的不平衡划分。理解模块主要是将用户指令编码为语义条件的翻译器，而生成模块必须同时充当设计师和画家，推断原始布局，识别目标编辑区域并渲染新内容。这种不平衡是违反直觉的，因为理解模块通常经过比生成模块的复杂推理任务数据多得多的训练。为了解决此问题，我们介绍了一个包含两个互补子集的数据集：（i）dim-t2i，包含14m长的图像 - 文本对，以增强复杂的指令理解； （ii）Dim-Edit，由GPT-4O产生的233k链链的想象力组成，是图像编辑的显式设计蓝图。我们通过轻巧的两层MLP将冷冻的QWEN2.5-VL-3B与可训练的SANA1.5-1.6B连接起来，并在提议的DIM数据集中训练它，从而导致DIM-4.6B-T2I/EDIT。尽管具有适度的参数量表，DIM-4.6B-EDIT在IMGEDIT和GEDIT基础基准测试基准上仍能达到SOTA或竞争性能，表现优于诸如Uniworld-v1和step1x-edit之类的更大模型。这些发现表明，将设计责任分配给理解模块为图像编辑提供了重大好处。我们的数据集和模型将在此HTTPS URL上可用。</li>
</ul>

<h3>Title: Palette Aligned Image Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Elad Aharoni, Noy Porat, Dani Lischinski, Ariel Shamir</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02000">https://arxiv.org/abs/2509.02000</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02000">https://arxiv.org/pdf/2509.02000</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02000]] Palette Aligned Image Diffusion(https://arxiv.org/abs/2509.02000)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce the Palette-Adapter, a novel method for conditioning text-to-image diffusion models on a user-specified color palette. While palettes are a compact and intuitive tool widely used in creative workflows, they introduce significant ambiguity and instability when used for conditioning image generation. Our approach addresses this challenge by interpreting palettes as sparse histograms and introducing two scalar control parameters: histogram entropy and palette-to-histogram distance, which allow flexible control over the degree of palette adherence and color variation. We further introduce a negative histogram mechanism that allows users to suppress specific undesired hues, improving adherence to the intended palette under the standard classifier-free guidance mechanism. To ensure broad generalization across the color space, we train on a carefully curated dataset with balanced coverage of rare and common colors. Our method enables stable, semantically coherent generation across a wide range of palettes and prompts. We evaluate our method qualitatively, quantitatively, and through a user study, and show that it consistently outperforms existing approaches in achieving both strong palette adherence and high image quality.</li>
<li><strong>摘要：</strong>我们介绍了调色器，这是一种在用户指定调色板上调节文本对图像扩散模型的新颖方法。尽管调色板是一种紧凑而直观的工具，在创意工作流程中广泛使用，但它们在用于调理图像的生成时会引入明显的歧义和不稳定。我们的方法通过将调色板解释为稀疏直方图并引入两个标量控制参数来解决这一挑战：直方图熵和调色板到历史的距离，从而可以灵活地控制调色板粘附和颜色变化的程度。我们进一步引入了一种负面的直方图机制，该机制使用户可以抑制特定的不希望的色调，从而提高了无标准分类器指导机制下对预期的调色板的依从性。为了确保在整个颜色空间中进行广泛的概括，我们在经过精心策划的数据集上进行训练，并具有平衡的稀有和常见颜色的覆盖范围。我们的方法可以在广泛的调色板和提示中稳定，语义上的连贯产生。我们通过定性，定量和用户研究评估我们的方法，并表明它始终优于现有的方法，在实现强大的调色板依从性和高图像质量方面。</li>
</ul>

<h3>Title: Fake & Square: Training Self-Supervised Vision Transformers with Synthetic Data and Synthetic Hard Negatives</h3>
<ul>
<li><strong>Authors: </strong>Nikolaos Giakoumoglou, Andreas Floros, Kleanthis Marios Papadopoulos, Tania Stathaki</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02029">https://arxiv.org/abs/2509.02029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02029">https://arxiv.org/pdf/2509.02029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02029]] Fake & Square: Training Self-Supervised Vision Transformers with Synthetic Data and Synthetic Hard Negatives(https://arxiv.org/abs/2509.02029)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper does not introduce a new method per se. Instead, we build on existing self-supervised learning approaches for vision, drawing inspiration from the adage "fake it till you make it". While contrastive self-supervised learning has achieved remarkable success, it typically relies on vast amounts of real-world data and carefully curated hard negatives. To explore alternatives to these requirements, we investigate two forms of "faking it" in vision transformers. First, we study the potential of generative models for unsupervised representation learning, leveraging synthetic data to augment sample diversity. Second, we examine the feasibility of generating synthetic hard negatives in the representation space, creating diverse and challenging contrasts. Our framework - dubbed Syn2Co - combines both approaches and evaluates whether synthetically enhanced training can lead to more robust and transferable visual representations on DeiT-S and Swin-T architectures. Our findings highlight the promise and limitations of synthetic data in self-supervised learning, offering insights for future work in this direction.</li>
<li><strong>摘要：</strong>本文本身不会引入新方法。取而代之的是，我们以现有的自我监督学习方法为基础，从格言中汲取灵感，从格言“伪造直到你实现”。尽管对比自我监督的学习取得了巨大的成功，但它通常依赖大量的现实数据和精心策划的艰难负面影响。为了探索这些要求的替代方案，我们在视觉变压器中研究了两种“伪造”形式。首先，我们研究了生成模型的潜在无监督的表示学习，利用合成数据来增强样本多样性。其次，我们研究了在表示空间中产生合成硬质量负面因素的可行性，从而产生了多样化且具有挑战性的对比度。我们的框架（称为Syn2CO）结合了两种方法，并评估合成增强的训练是否会导致在DEIT -S和SWIN -T体系结构上具有更健壮和可转移的视觉表示。我们的发现突出了自我监督学习中综合数据的希望和局限性，为朝这个方向朝着这一方向提供了见解。</li>
</ul>

<h3>Title: Privacy-Utility Trade-off in Data Publication: A Bilevel Optimization Framework with Curvature-Guided Perturbation</h3>
<ul>
<li><strong>Authors: </strong>Yi Yin, Guangquan Zhang, Hua Zuo, Jie Lu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02048">https://arxiv.org/abs/2509.02048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02048">https://arxiv.org/pdf/2509.02048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02048]] Privacy-Utility Trade-off in Data Publication: A Bilevel Optimization Framework with Curvature-Guided Perturbation(https://arxiv.org/abs/2509.02048)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Machine learning models require datasets for effective training, but directly sharing raw data poses significant privacy risk such as membership inference attacks (MIA). To mitigate the risk, privacy-preserving techniques such as data perturbation, generalization, and synthetic data generation are commonly utilized. However, these methods often degrade data accuracy, specificity, and diversity, limiting the performance of downstream tasks and thus reducing data utility. Therefore, striking an optimal balance between privacy preservation and data utility remains a critical challenge. To address this issue, we introduce a novel bilevel optimization framework for the publication of private datasets, where the upper-level task focuses on data utility and the lower-level task focuses on data privacy. In the upper-level task, a discriminator guides the generation process to ensure that perturbed latent variables are mapped to high-quality samples, maintaining fidelity for downstream tasks. In the lower-level task, our framework employs local extrinsic curvature on the data manifold as a quantitative measure of individual vulnerability to MIA, providing a geometric foundation for targeted privacy protection. By perturbing samples toward low-curvature regions, our method effectively suppresses distinctive feature combinations that are vulnerable to MIA. Through alternating optimization of both objectives, we achieve a synergistic balance between privacy and utility. Extensive experimental evaluations demonstrate that our method not only enhances resistance to MIA in downstream tasks but also surpasses existing methods in terms of sample quality and diversity.</li>
<li><strong>摘要：</strong>机器学习模型需要数据集进行有效的培训，但直接共享原始数据构成了重要的隐私风险，例如会员推理攻击（MIA）。为了减轻风险，通常使用具有数据扰动，概括和合成数据等隐私技术。但是，这些方法通常会降低数据准确性，特异性和多样性，从而限制了下游任务的性能，从而降低了数据实用性。因此，在隐私保护和数据实用程序之间达到最佳平衡仍然是一个关键挑战。为了解决此问题，我们为发布私人数据集的出版物介绍了一个新颖的双层优化框架，其中高级任务侧重于数据实用程序，而下层任务则侧重于数据隐私。在上层任务中，歧视器指导生成过程，以确保将扰动的潜在变量映射到高质量的样本中，从而保持下游任务的保真度。在较低级别的任务中，我们的框架在数据歧管上采用本地外部曲率，作为对MIA脆弱性的定量度量，为有针对性的隐私保护提供了几何基础。通过将样品驱动到低外生区域，我们的方法有效地抑制了容易受到MIA的独特特征组合。通过交替优化这两个目标，我们在隐私和公用事业之间实现了协同平衡。广泛的实验评估表明，我们的方法不仅增强了下游任务中对MIA的抵抗力，而且还超过了样本质量和多样性方面的现有方法。</li>
</ul>

<h3>Title: Data-Dependent Smoothing for Protein Discovery with Walk-Jump Sampling</h3>
<ul>
<li><strong>Authors: </strong>Srinivas Anumasa, Barath Chandran.C, Tingting Chen, Dianbo Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02069">https://arxiv.org/abs/2509.02069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02069">https://arxiv.org/pdf/2509.02069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02069]] Data-Dependent Smoothing for Protein Discovery with Walk-Jump Sampling(https://arxiv.org/abs/2509.02069)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as a powerful class of generative models by learning to iteratively reverse the noising process. Their ability to generate high-quality samples has extended beyond high-dimensional image data to other complex domains such as proteins, where data distributions are typically sparse and unevenly spread. Importantly, the sparsity itself is uneven. Empirically, we observed that while a small fraction of samples lie in dense clusters, the majority occupy regions of varying sparsity across the data space. Existing approaches largely ignore this data-dependent variability. In this work, we introduce a Data-Dependent Smoothing Walk-Jump framework that employs kernel density estimation (KDE) as a preprocessing step to estimate the noise scale $\sigma$ for each data point, followed by training a score model with these data-dependent $\sigma$ values. By incorporating local data geometry into the denoising process, our method accounts for the heterogeneous distribution of protein data. Empirical evaluations demonstrate that our approach yields consistent improvements across multiple metrics, highlighting the importance of data-aware sigma prediction for generative modeling in sparse, high-dimensional settings.</li>
<li><strong>摘要：</strong>扩散模型已通过学习迭代扭转尖锐的过程而成为强大的生成模型类别。它们生成高质量样品的能力已将高维图像数据扩展到其他复杂域，例如蛋白质，其中数据分布通常是稀疏且不均匀的。重要的是，稀疏本身是不平衡的。从经验上讲，我们观察到，尽管一小部分样品位于密集的簇中，但大多数占领整个数据空间中不同稀疏性的区域。现有方法在很大程度上忽略了这种依赖数据的变异性。在这项工作中，我们引入了一个与数据相关的平滑步行框架，该框架采用内核密度估计（KDE）作为一个预处理步骤，以估算每个数据点的噪声量表$ \ sigma $，然后对这些与数据相关的$ \ sigma $值培训得分模型。通过将局部数据的几何形状纳入去胶过程，我们的方法解释了蛋白质数据的异质分布。经验评估表明，我们的方法在多个指标之间产生一致的改进，强调了数据感知的Sigma预测在稀疏，高维设置中生成建模的重要性。</li>
</ul>

<h3>Title: Abex-rat: Synergizing Abstractive Augmentation and Adversarial Training for Classification of Occupational Accident Reports</h3>
<ul>
<li><strong>Authors: </strong>Jian Chen, Jinbao Tian, Yunqi Xu, Zhou Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02072">https://arxiv.org/abs/2509.02072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02072">https://arxiv.org/pdf/2509.02072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02072]] Abex-rat: Synergizing Abstractive Augmentation and Adversarial Training for Classification of Occupational Accident Reports(https://arxiv.org/abs/2509.02072)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The automatic classification of occupational accident reports is a critical research area for enhancing workplace safety and enabling large-scale risk analysis. However, the severe class imbalance inherent in these real-world datasets often compromises the performance of analytical models, particularly for rare but severe incident types, hindering the development of reliable automated systems. To address this challenge, we propose ABEX-RAT, a novel and efficient framework that synergizes generative data augmentation with robust adversarial training. Our approach first employs a twostep abstractive-expansive (ABEX) pipeline, which leverages a large language model to distill core incident semantics and then uses a generative model to create diverse, highquality synthetic samples for underrepresented classes. Subsequently, a lightweight classifier is trained on the augmented data using a computationally efficient random adversarial training (RAT) protocol, which stochastically applies perturbations to enhance model generalization and robustness without significant overhead. Experimental results on the public OSHA dataset demonstrate that our method achieves new state-of-the-art performance, reaching a macro-F1 score of 90.32% and significantly outperforming previous SOTA and fine-tuned large model baselines. Our work validates that this synergistic strategy is a highly effective and efficient alternative to brute-force fine-tuning for specialized, imbalanced classification tasks. The code is publicly available at:this https URL.</li>
<li><strong>摘要：</strong>职业事故报告的自动分类是提高工作场所安全并实现大规模风险分析的关键研究领域。但是，这些实际数据集固有的严重类别不平衡通常会损害分析模型的性能，特别是对于罕见但严重的事件类型，阻碍了可靠的自动化系统的发展。为了应对这一挑战，我们提出了Abex-Rat，这是一个新颖有效的框架，通过强大的对抗训练协同生成数据增强。我们的方法首先采用了TwoStep抽象性的（ABEX）管道，该管道利用大型语言模型来提炼核心事件语义语义，然后使用生成模型为代表性不足的类创建多样化的高质量合成样本。随后，使用计算上有效的随机对抗训练（RAT）方案对轻量级分类器进行了对增强数据的培训，该协议随机应用扰动以增强模型的概括和稳健性而没有明显的开销。公共OSHA数据集的实验结果表明，我们的方法达到了新的最先进的性能，达到90.32％的宏观F1分数，并且明显超过了先前的SOTA和微调的大型模型基线。我们的工作证明了这种协同策略是针对专业，不平衡分类任务的蛮力微调的高效和有效替代方案。该代码可公开可用，网址为：此HTTPS URL。</li>
</ul>

<h3>Title: Differentiable Expectation-Maximisation and Applications to Gaussian Mixture Model Optimal Transport</h3>
<ul>
<li><strong>Authors: </strong>Samuel Boïté, Eloi Tanguy, Julie Delon, Agnès Desolneux, Rémi Flamary</a></li>
<li><strong>Subjects: </strong>cs.LG, math.PR, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02109">https://arxiv.org/abs/2509.02109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02109">https://arxiv.org/pdf/2509.02109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02109]] Differentiable Expectation-Maximisation and Applications to Gaussian Mixture Model Optimal Transport(https://arxiv.org/abs/2509.02109)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The Expectation-Maximisation (EM) algorithm is a central tool in statistics and machine learning, widely used for latent-variable models such as Gaussian Mixture Models (GMMs). Despite its ubiquity, EM is typically treated as a non-differentiable black box, preventing its integration into modern learning pipelines where end-to-end gradient propagation is essential. In this work, we present and compare several differentiation strategies for EM, from full automatic differentiation to approximate methods, assessing their accuracy and computational efficiency. As a key application, we leverage this differentiable EM in the computation of the Mixture Wasserstein distance $\mathrm{MW}_2$ between GMMs, allowing $\mathrm{MW}_2$ to be used as a differentiable loss in imaging and machine learning tasks. To complement our practical use of $\mathrm{MW}_2$, we contribute a novel stability result which provides theoretical justification for the use of $\mathrm{MW}_2$ with EM, and also introduce a novel unbalanced variant of $\mathrm{MW}_2$. Numerical experiments on barycentre computation, colour and style transfer, image generation, and texture synthesis illustrate the versatility and effectiveness of the proposed approach in different settings.</li>
<li><strong>摘要：</strong>期望最大化（EM）算法是统计和机器学习中的中心工具，广泛用于潜在可 - 可添加模型，例如高斯混合模型（GMMS）。尽管它无处不在，但EM通常被视为一种非不同的黑匣子，以防止其整合到端到端梯度繁殖至关重要的现代学习管道中。在这项工作中，我们介绍并比较了EM的几种分化策略，从完全自动差异到近似方法，评估其准确性和计算效率。作为关键应用程序，我们利用GMM之间的混合物距离$ \ MATHRM {MW} _2 $在计算混合物的计算中利用这种可区分的EM，允许$ \ mathrm {mw} _2 $用作成像和机器学习任务中的可区分损失。为了补充我们对$ \ mathrm {mw} _2 $的实际使用，我们为新颖的稳定性结果提供了一种新颖的稳定性结果，该结果为使用$ \ mathrm {mw} _2 $提供了理论上的理由，并引入了$ \ m m i \ m m i {mw} _2 $ _2 $的新颖不平衡变体。关于Barycentre计算，颜色和样式转移，图像生成和纹理合成的数值实验说明了在不同设置中提出的方法的多功能性和有效性。</li>
</ul>

<h3>Title: Conditional-$t^3$VAE: Equitable Latent Space Allocation for Fair Generation</h3>
<ul>
<li><strong>Authors: </strong>Aymene Mohammed Bouayed, Samuel Deslauriers-Gauthier, Adrian Iaccovelli, David Naccache</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02154">https://arxiv.org/abs/2509.02154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02154">https://arxiv.org/pdf/2509.02154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02154]] Conditional-$t^3$VAE: Equitable Latent Space Allocation for Fair Generation(https://arxiv.org/abs/2509.02154)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Variational Autoencoders (VAEs) with global priors mirror the training set's class frequency in latent space, underrepresenting tail classes and reducing generative fairness on imbalanced datasets. While $t^3$VAE improves robustness via heavy-tailed Student's t-distribution priors, it still allocates latent volume proportionally to the class this http URL this work, we address this issue by explicitly enforcing equitable latent space allocation across classes. To this end, we propose Conditional-$t^3$VAE, which defines a per-class \mbox{Student's t} joint prior over latent and output variables, preventing dominance by majority classes. Our model is optimized using a closed-form objective derived from the $\gamma$-power divergence. Moreover, for class-balanced generation, we derive an equal-weight latent mixture of Student's t-distributions. On SVHN-LT, CIFAR100-LT, and CelebA, Conditional-$t^3$VAE consistently achieves lower FID scores than both $t^3$VAE and Gaussian-based VAE baselines, particularly under severe class imbalance. In per-class F1 evaluations, Conditional-$t^3$VAE also outperforms the conditional Gaussian VAE across all highly imbalanced settings. While Gaussian-based models remain competitive under mild imbalance ratio ($\rho \lesssim 3$), our approach substantially improves generative fairness and diversity in more extreme regimes.</li>
<li><strong>摘要：</strong>带有全球先验的变异自动编码器（VAE）反映了训练集在潜在空间中的类频率，代表性的尾巴类别不足，并且降低了不平衡数据集的生成公平性。虽然$ t^3 $ vae通过重尾学生的t分布先验提高了鲁棒性，但它仍然可以按比例分配此工作的http url类成比例地分配潜在的量，我们通过在各个类中明确执行公平的潜在空间分配来解决此问题。为此，我们提出了条件 -  $ t^3 $ vae，它在潜在和输出变量上定义了一个每级\ mbox {student t}联合，从而阻止了多数类的优势。我们的模型使用$ \ gamma $ -power Divergence得出的封闭形式的目标进行了优化。此外，对于班级平衡的一代，我们得出了学生T分布的同等权重混合物。在SVHN-LT，CIFAR100-LT和CELEBA上，有条件的-T^3 $ VAE始终取得的FID得分低于$ T^3 $ vae和基于高斯的VAE基线，尤其是在严重的班级失衡下。在每类F1评估中，有条件的$ t^3 $ vae在所有高度不平衡的设置上也优于条件高斯VAE。尽管基于高斯的模型在轻度失衡比（$ \ rho \ lyssim 3 $）下保持竞争力，但我们的方法显着提高了更极端政权的生成公平和多样性。</li>
</ul>

<h3>Title: Enhancing Zero-Shot Pedestrian Attribute Recognition with Synthetic Data Generation: A Comparative Study with Image-To-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Pablo Ayuso-Albizu, Juan C. SanMiguel, Pablo Carballeira</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02161">https://arxiv.org/abs/2509.02161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02161">https://arxiv.org/pdf/2509.02161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02161]] Enhancing Zero-Shot Pedestrian Attribute Recognition with Synthetic Data Generation: A Comparative Study with Image-To-Image Diffusion Models(https://arxiv.org/abs/2509.02161)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Pedestrian Attribute Recognition (PAR) involves identifying various human attributes from images with applications in intelligent monitoring systems. The scarcity of large-scale annotated datasets hinders the generalization of PAR models, specially in complex scenarios involving occlusions, varying poses, and diverse environments. Recent advances in diffusion models have shown promise for generating diverse and realistic synthetic images, allowing to expand the size and variability of training data. However, the potential of diffusion-based data expansion for generating PAR-like images remains underexplored. Such expansion may enhance the robustness and adaptability of PAR models in real-world scenarios. This paper investigates the effectiveness of diffusion models in generating synthetic pedestrian images tailored to PAR tasks. We identify key parameters of img2img diffusion-based data expansion; including text prompts, image properties, and the latest enhancements in diffusion-based data augmentation, and examine their impact on the quality of generated images for PAR. Furthermore, we employ the best-performing expansion approach to generate synthetic images for training PAR models, by enriching the zero-shot datasets. Experimental results show that prompt alignment and image properties are critical factors in image generation, with optimal selection leading to a 4.5% improvement in PAR recognition performance.</li>
<li><strong>摘要：</strong>行人属性识别（PAR）涉及从图像中识别出具有智能监控系统应用的图像中的各种人类属性。大规模注释数据集的稀缺性阻碍了PAR模型的概括，特别是在涉及阻塞，不同姿势和不同环境的复杂场景中。扩散模型的最新进展已显示出产生多样化和现实的合成图像的希望，从而扩大了训练数据的大小和变异性。但是，基于扩散的数据扩展对产生类似PAR样图像的潜力仍然没有被忽略。这种扩展可以增强在现实情况下PAR模型的鲁棒性和适应性。本文研究了扩散模型在生成针对PAR任务的合成行人图像中的有效性。我们确定基于IMG2IMG扩散数据扩展的关键参数；包括文本提示，图像属性以及基于扩散数据增强的最新增强功能，并检查其对PAR生成的图像质量的影响。此外，我们采用表现最佳的扩展方法来通过丰富零摄像数据集来为训练PAR模型生成合成图像。实验结果表明，迅速的比对和图像属性是图像产生的关键因素，最佳选择会导致识别性能提高4.5％。</li>
</ul>

<h3>Title: Understanding Space Is Rocket Science - Only Top Reasoning Models Can Solve Spatial Understanding Tasks</h3>
<ul>
<li><strong>Authors: </strong>Nils Hoehing, Mayug Maniparambil, Ellen Rushe, Noel E. O'Connor, Anthony Ventresque</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02175">https://arxiv.org/abs/2509.02175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02175">https://arxiv.org/pdf/2509.02175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02175]] Understanding Space Is Rocket Science - Only Top Reasoning Models Can Solve Spatial Understanding Tasks(https://arxiv.org/abs/2509.02175)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We propose RocketScience, an open-source contrastive VLM benchmark that tests for spatial relation understanding. It is comprised of entirely new real-world image-text pairs covering mostly relative spatial understanding and the order of objects. The benchmark is designed to be very easy for humans and hard for the current generation of VLMs, and this is empirically verified. Our results show a striking lack of spatial relation understanding in open source and frontier commercial VLMs and a surprisingly high performance of reasoning models. Additionally, we perform a disentanglement analysis to separate the contributions of object localization and spatial reasoning in chain-of-thought-based models and find that the performance on the benchmark is bottlenecked by spatial reasoning and not object localization capabilities. We release the dataset with a CC-BY-4.0 license and make the evaluation code available at: this https URL</li>
<li><strong>摘要：</strong>我们提出了RocketScience，这是一种开源对比VLM基准，该基准测试了空间关系的理解。它由全新的现实世界图像文本对组成，其中主要涵盖了相对空间理解和对象的顺序。该基准的设计对于人类来说非常容易，对于当前的VLM来说很难，这是经验验证的。我们的结果表明，在开源和边界商业VLM中缺乏空间关系的理解，以及推理模型的高度高性能。此外，我们执行分离分析，以将基于思想链的模型中对象定位和空间推理的贡献分开，并发现基准上的性能是通过空间推理瓶颈而不是对象定位功能的。我们使用CC-BY-4.0许可证发布数据集，并使评估代码可用：this HTTPS URL</li>
</ul>

<h3>Title: Data-Driven Loss Functions for Inference-Time Optimization in Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Sapir Esther Yiflach, Yuval Atzmon, Gal Chechik</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02295">https://arxiv.org/abs/2509.02295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02295">https://arxiv.org/pdf/2509.02295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02295]] Data-Driven Loss Functions for Inference-Time Optimization in Text-to-Image Generation(https://arxiv.org/abs/2509.02295)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models can generate stunning visuals, yet they often fail at tasks children find trivial--like placing a dog to the right of a teddy bear rather than to the left. When combinations get more unusual--a giraffe above an airplane--these failures become even more pronounced. Existing methods attempt to fix these spatial reasoning failures through model fine-tuning or test-time optimization with handcrafted losses that are suboptimal. Rather than imposing our assumptions about spatial encoding, we propose learning these objectives directly from the model's internal representations. We introduce Learn-to-Steer, a novel framework that learns data-driven objectives for test-time optimization rather than handcrafting them. Our key insight is to train a lightweight classifier that decodes spatial relationships from the diffusion model's cross-attention maps, then deploy this classifier as a learned loss function during inference. Training such classifiers poses a surprising challenge: they can take shortcuts by detecting linguistic traces rather than learning true spatial patterns. We solve this with a dual-inversion strategy that enforces geometric understanding. Our method dramatically improves spatial accuracy: from 0.20 to 0.61 on FLUX.1-dev and from 0.07 to 0.54 on SD2.1 across standard benchmarks. Moreover, our approach generalizes to multiple relations and significantly improves accuracy.</li>
<li><strong>摘要：</strong>文本对图像扩散模型可以产生令人惊叹的视觉效果，但是他们经常失败的任务孩子发现琐碎的任务 - 像将狗放在泰迪熊的右边，而不是向左放在泰迪熊的右边。当组合变得更加不寻常时 - 飞机上方的长颈鹿 - 这些故障变得更加明显。现有的方法试图通过模型进行微调或测试时间优化来解决这些空间推理故障，并使用次优的手工损失进行测试。我们没有直接从模型的内部表示形式中学习这些目标，而不是强加我们对空间编码的假设。我们介绍了学习 - 转向者，这是一个新颖的框架，该框架学习了数据驱动的目标，以进行测试时间优化，而不是对其进行手工制作。我们的关键见解是训练一个轻量级分类器，该分类器从扩散模型的跨注意图中解码空间关系，然后将此分类器部署为推断期间学习的损失函数。培训此类分类器提出了一个令人惊讶的挑战：它们可以通过检测语言痕迹而不是学习真正的空间模式来缩短捷径。我们通过双重转换策略来解决这一目标，从而实施几何理解。我们的方法急剧提高了空间精度：跨标准基准的Flux.1-DEV上的0.20升至0.61，从SD2.1上的0.07到0.54。此外，我们的方法概括了多个关系，并显着提高了准确性。</li>
</ul>

<h3>Title: OmniActor: A Generalist GUI and Embodied Agent for 2D&3D Worlds</h3>
<ul>
<li><strong>Authors: </strong>Longrong Yang, Zhixiong Zeng, Yufeng Zhong, Jing Huang, Liming Zheng, Lei Chen, Haibo Qiu, Zequn Qin, Lin Ma, Xi Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02322">https://arxiv.org/abs/2509.02322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02322">https://arxiv.org/pdf/2509.02322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02322]] OmniActor: A Generalist GUI and Embodied Agent for 2D&3D Worlds(https://arxiv.org/abs/2509.02322)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal large language models are evolving toward multimodal agents capable of proactively executing tasks. Most agent research focuses on GUI or embodied scenarios, which correspond to agents interacting with 2D virtual worlds or 3D real worlds, respectively. However, many complex tasks typically require agents to interleavely interact with these two types of environment. We initially mix GUI and embodied data to train, but find the performance degeneration brought by the data conflict. Further analysis reveals that GUI and embodied data exhibit synergy and conflict at the shallow and deep layers, respectively, which resembles the cerebrum-cerebellum mechanism in the human brain. To this end, we propose a high-performance generalist agent OmniActor, designed from both structural and data perspectives. First, we propose Layer-heterogeneity MoE to eliminate the conflict between GUI and embodied data by separating deep-layer parameters, while leverage their synergy by sharing shallow-layer parameters. By successfully leveraging the synergy and eliminating the conflict, OmniActor outperforms agents only trained by GUI or embodied data in GUI or embodied tasks. Furthermore, we unify the action spaces of GUI and embodied tasks, and collect large-scale GUI and embodied data from various sources for training. This significantly improves OmniActor under different scenarios, especially in GUI tasks. The code will be publicly available.</li>
<li><strong>摘要：</strong>多模式的大语言模型正在发展为能够主动执行任务的多模式代理。大多数代理研究的重点是GUI或体现的场景，这些场景分别与与2D虚拟世界或3D现实世界相互作用的代理相对应。但是，许多复杂的任务通常要求代理与这两种类型的环境交流。我们最初将GUI和体现的数据混合在一起训练，但发现数据冲突带来的性能退化。进一步的分析表明，GUI和体现的数据分别在浅层和深层上表现出协同作用和冲突，与人脑中的大脑机制相似。为此，我们提出了一种从结构和数据角度设计的高性能通才代理综合剂。首先，我们建议通过分离深层参数来消除GUI和体现数据之间的冲突，同时通过共享浅层参数来利用其协同作用。通过成功利用协同作用并消除冲突，OmniActor优于仅在GUI或GUI或体现任务中训练的GUI或体现数据的代理。此外，我们统一了GUI和具体任务的动作空间，并从各种来源收集大规模的GUI和体现的数据进行培训。在不同的情况下，尤其是在GUI任务中，这大大改善了综合器。该代码将公开可用。</li>
</ul>

<h3>Title: Faster and Better: Reinforced Collaborative Distillation and Self-Learning for Infrared-Visible Image Fusion</h3>
<ul>
<li><strong>Authors: </strong>Yuhao Wang, Lingjuan Miao, Zhiqiang Zhou, Yajun Qiao, Lei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02424">https://arxiv.org/abs/2509.02424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02424">https://arxiv.org/pdf/2509.02424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02424]] Faster and Better: Reinforced Collaborative Distillation and Self-Learning for Infrared-Visible Image Fusion(https://arxiv.org/abs/2509.02424)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Infrared and visible image fusion plays a critical role in enhancing scene perception by combining complementary information from different modalities. Despite recent advances, achieving high-quality image fusion with lightweight models remains a significant challenge. To bridge this gap, we propose a novel collaborative distillation and self-learning framework for image fusion driven by reinforcement learning. Unlike conventional distillation, this approach not only enables the student model to absorb image fusion knowledge from the teacher model, but more importantly, allows the student to perform self-learning on more challenging samples to enhance its capabilities. Particularly, in our framework, a reinforcement learning agent explores and identifies a more suitable training strategy for the this http URL agent takes both the student's performance and the teacher-student gap as inputs, which leads to the generation of challenging samples to facilitate the student's self-learning. Simultaneously, it dynamically adjusts the teacher's guidance strength based on the student's state to optimize the knowledge transfer. Experimental results demonstrate that our method can significantly improve student performance and achieve better fusion results compared to existing techniques.</li>
<li><strong>摘要：</strong>红外和可见图像融合通过结合来自不同方式的互补信息来增强场景感知的关键作用。尽管最近进步，但与轻量级模型实现高质量的图像融合仍然是一个重大挑战。为了弥合这一差距，我们提出了一个新颖的协作蒸馏和由增强学习驱动的图像融合的自学习框架。与传统的蒸馏不同，这种方法不仅使学生模型能够从教师模型中吸收图像融合知识，而且更重要的是，学生可以对更具挑战性的样本进行自学习以增强其功能。尤其是在我们的框架中，强化学习者探索并确定了该HTTP URL代理的更合适的培训策略，将学生的表现和教师差距作为输入作为输入，从而导致产生具有挑战性的样本，以促进学生的自学学习。同时，它根据学生的状态动态调整教师的指导力量，以优化知识转移。实验结果表明，与现有技术相比，我们的方法可以显着提高学生的绩效并获得更好的融合结果。</li>
</ul>

<h3>Title: Towards High-Fidelity, Identity-Preserving Real-Time Makeup Transfer: Decoupling Style Generation</h3>
<ul>
<li><strong>Authors: </strong>Lydia Kin Ching Chau, Zhi Yu, Ruo Wei Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02445">https://arxiv.org/abs/2509.02445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02445">https://arxiv.org/pdf/2509.02445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02445]] Towards High-Fidelity, Identity-Preserving Real-Time Makeup Transfer: Decoupling Style Generation(https://arxiv.org/abs/2509.02445)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present a novel framework for real-time virtual makeup try-on that achieves high-fidelity, identity-preserving cosmetic transfer with robust temporal consistency. In live makeup transfer applications, it is critical to synthesize temporally coherent results that accurately replicate fine-grained makeup and preserve user's identity. However, existing methods often struggle to disentangle semitransparent cosmetics from skin tones and other identify features, causing identity shifts and raising fairness concerns. Furthermore, current methods lack real-time capabilities and fail to maintain temporal consistency, limiting practical adoption. To address these challenges, we decouple makeup transfer into two steps: transparent makeup mask extraction and graphics-based mask rendering. After the makeup extraction step, the makeup rendering can be performed in real time, enabling live makeup try-on. Our makeup extraction model trained on pseudo-ground-truth data generated via two complementary methods: a graphics-based rendering pipeline and an unsupervised k-means clustering approach. To further enhance transparency estimation and color fidelity, we propose specialized training objectives, including alpha-weighted reconstruction and lip color losses. Our method achieves robust makeup transfer across diverse poses, expressions, and skin tones while preserving temporal smoothness. Extensive experiments demonstrate that our approach outperforms existing baselines in capturing fine details, maintaining temporal stability, and preserving identity integrity.</li>
<li><strong>摘要：</strong>我们为实时虚拟化妆尝试提供了一个新颖的框架，该框架以稳健的时间一致性实现了高保真，具有身份的化妆品转移。在实时化妆传输应用程序中，至关重要的是，综合时间连贯的结果可以准确复制细粒化的化妆并保留用户的身份。但是，现有的方法通常很难将半透明化妆品与肤色和其他识别特征相关，从而引起身份转移并提高公平关注。此外，当前方法缺乏实时功能，并且无法保持时间一致性，从而限制了实际采用。为了应对这些挑战，我们将化妆转移分为两个步骤：透明的化妆遮罩提取和基于图形的掩码渲染。在化妆提取步骤之后，可以实时执行化妆渲染，从而实现实时化妆。我们通过两种互补方法生成的伪地面数据训练的化妆模型：一种基于图形的渲染管道和无监督的K-均值聚类方法。为了进一步提高透明度估计和颜色保真度，我们提出了专门的培训目标，包括α加权重建和唇彩损失。我们的方法可以在保持时间光滑的同时，在各种姿势，表情和肤色上实现了强大的化妆转移。广泛的实验表明，我们的方法在捕获细节，保持时间稳定和保持身份完整性方面优于现有基准。</li>
</ul>

<h3>Title: Generative Sequential Notification Optimization via Multi-Objective Decision Transformers</h3>
<ul>
<li><strong>Authors: </strong>Borja Ocejo, Ruofan Wang, Ke Liu, Rohit K. Patra, Haotian Shen, David Liu, Yiwen Yuan, Gokulraj Mohanasundaram, Fedor Borisyuk, Prakruthi Prabhakar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02458">https://arxiv.org/abs/2509.02458</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02458">https://arxiv.org/pdf/2509.02458</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02458]] Generative Sequential Notification Optimization via Multi-Objective Decision Transformers(https://arxiv.org/abs/2509.02458)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Notifications are an important communication channel for delivering timely and relevant information. Optimizing their delivery involves addressing complex sequential decision-making challenges under constraints such as message utility and user fatigue. Offline reinforcement learning (RL) methods, such as Conservative Q-Learning (CQL), have been applied to this problem but face practical challenges at scale, including instability, sensitivity to distribution shifts, limited reproducibility, and difficulties with explainability in high-dimensional recommendation settings. We present a Decision Transformer (DT) based framework that reframes policy learning as return-conditioned supervised learning, improving robustness, scalability, and modeling flexibility. Our contributions include a real-world comparison with CQL, a multi-reward design suitable for non-episodic tasks, a quantile regression approach to return-to-go conditioning, and a production-ready system with circular buffer-based sequence processing for near-real-time inference. Extensive offline and online experiments in a deployed notification system show that our approach improves notification utility and overall session activity while minimizing user fatigue. Compared to a multi-objective CQL-based agent, the DT-based approach achieved a +0.72% increase in sessions for notification decision-making at LinkedIn by making notification recommendation more relevant.</li>
<li><strong>摘要：</strong>通知是提供及时和相关信息的重要通信渠道。优化其交付涉及解决在限制（例如消息实用程序和用户疲劳之类的限制）下解决复杂的顺序决策挑战。离线增强学习（RL）方法，例如保守的Q学习（CQL），已应用于此问题，但面临大规模的实际挑战，包括不稳定性，对分配变化有限的敏感性，有限的可重复性以及在高维建议环境中具有解释性的困难。我们提出了一个基于决策者（DT）的框架，该框架将策略学习重新构架为返回的监督学习，提高鲁棒性，可扩展性和建模灵活性。我们的贡献包括与CQL进行的现实比较，CQL，一种适用于非剧本任务的多回报设计，一种返回调节的分位数回归方法，以及具有基于循环缓冲序列处理的生产准备系统，用于近实时推断。部署通知系统中的大量离线和在线实验表明，我们的方法改善了通知实用程序和整体会话活动，同时最大程度地减少了用户疲劳。与基于多目标的基于CQL的代理相比，基于DT的方法通过使通知建议更相关，可以使LinkedIn通知决策的会话增加 +0.72％。</li>
</ul>

<h3>Title: GenCompositor: Generative Video Compositing with Diffusion Transformer</h3>
<ul>
<li><strong>Authors: </strong>Shuzhou Yang, Xiaoyu Li, Xiaodong Cun, Guangzhi Wang, Lingen Li, Ying Shan, Jian Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02460">https://arxiv.org/abs/2509.02460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02460">https://arxiv.org/pdf/2509.02460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02460]] GenCompositor: Generative Video Compositing with Diffusion Transformer(https://arxiv.org/abs/2509.02460)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Video compositing combines live-action footage to create video production, serving as a crucial technique in video creation and film production. Traditional pipelines require intensive labor efforts and expert collaboration, resulting in lengthy production cycles and high manpower costs. To address this issue, we automate this process with generative models, called generative video compositing. This new task strives to adaptively inject identity and motion information of foreground video to the target video in an interactive manner, allowing users to customize the size, motion trajectory, and other attributes of the dynamic elements added in final video. Specifically, we designed a novel Diffusion Transformer (DiT) pipeline based on its intrinsic properties. To maintain consistency of the target video before and after editing, we revised a light-weight DiT-based background preservation branch with masked token injection. As to inherit dynamic elements from other sources, a DiT fusion block is proposed using full self-attention, along with a simple yet effective foreground augmentation for training. Besides, for fusing background and foreground videos with different layouts based on user control, we developed a novel position embedding, named Extended Rotary Position Embedding (ERoPE). Finally, we curated a dataset comprising 61K sets of videos for our new task, called VideoComp. This data includes complete dynamic elements and high-quality target videos. Experiments demonstrate that our method effectively realizes generative video compositing, outperforming existing possible solutions in fidelity and consistency.</li>
<li><strong>摘要：</strong>视频合成结合了真人录像来创建视频制作，是视频创作和电影制作中的至关重要技术。传统管道需要大量的劳动力和专家协作，从而导致长期生产周期和高人力成本。为了解决此问题，我们使用称为“生成视频合成”的生成模型自动化此过程。这项新任务旨在以交互式方式适应前景视频的注入身份和运动信息，从而使用户可以自定义最终视频中添加的动态元素的大小，运动轨迹和其他属性。具体而言，我们根据其内在特性设计了一种新型扩散变压器（DIT）管道。为了在编辑之前和之后保持目标视频的一致性，我们修改了一个基于蒙面的令牌注入的基于轻巧的背景保护分支。至于从其他来源继承动态元素，使用完全自我注意力提出了一个DIT融合块，以及简单而有效的前景增强训练。此外，对于基于用户控制的融合背景和前景视频，我们开发了一种新颖的位置嵌入，称为扩展旋转位置嵌入（EROPE）。最后，我们策划了一个数据集，其中包含61k的视频，用于我们的新任务，称为VideoComp。该数据包括完整的动态元素和高质量的目标视频。实验表明，我们的方法有效地实现了生成视频合成，超过了忠诚度和一致性的现有解决方案。</li>
</ul>

<h3>Title: TeRA: Rethinking Text-driven Realistic 3D Avatar Generation</h3>
<ul>
<li><strong>Authors: </strong>Yanwen Wang, Yiyu Zhuang, Jiawei Zhang, Li Wang, Yifei Zeng, Xun Cao, Xinxin Zuo, Hao Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02466">https://arxiv.org/abs/2509.02466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02466">https://arxiv.org/pdf/2509.02466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02466]] TeRA: Rethinking Text-driven Realistic 3D Avatar Generation(https://arxiv.org/abs/2509.02466)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In this paper, we rethink text-to-avatar generative models by proposing TeRA, a more efficient and effective framework than the previous SDS-based models and general large 3D generative this http URL approach employs a two-stage training strategy for learning a native 3D avatar generative model. Initially, we distill a decoder to derive a structured latent space from a large human reconstruction model. Subsequently, a text-controlled latent diffusion model is trained to generate photorealistic 3D human avatars within this latent space. TeRA enhances the model performance by eliminating slow iterative optimization and enables text-based partial customization through a structured 3D human this http URL have proven our approach's superiority over previous text-to-avatar generative models in subjective and objective evaluation.</li>
<li><strong>摘要：</strong>在本文中，我们通过提出TERA来重新考虑文本到阿瓦塔尔生成模型，这是比以前的基于SDS的模型更高效，更有效的框架，而通用大型3D生成型这种HTTP URL方法采用了两阶段的培训策略来学习本地3D AVATAR生成模型。最初，我们将解码器提炼出从大型人类重建模型中得出结构化的潜在空间。随后，对文本控制的潜扩散模型进行了训练，以在此潜在空间内生成逼真的3D人类化身。 TERA通过消除缓慢的迭代优化来增强模型性能，并通过结构化的3D人类来实现基于文本的部分自定义这个HTTP URL，这证明了我们在主观和客观评估中与以前的文本到阿瓦塔尔生成模型的优越性。</li>
</ul>

<h3>Title: Exploring Variational Graph Autoencoders for Distribution Grid Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Syed Zain Abbas, Ehimare Okoyomon</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02469">https://arxiv.org/abs/2509.02469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02469">https://arxiv.org/pdf/2509.02469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02469]] Exploring Variational Graph Autoencoders for Distribution Grid Data Generation(https://arxiv.org/abs/2509.02469)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>To address the lack of public power system data for machine learning research in energy networks, we investigate the use of variational graph autoencoders (VGAEs) for synthetic distribution grid generation. Using two open-source datasets, ENGAGE and DINGO, we evaluate four decoder variants and compare generated networks against the original grids using structural and spectral metrics. Results indicate that simple decoders fail to capture realistic topologies, while GCN-based approaches achieve strong fidelity on ENGAGE but struggle on the more complex DINGO dataset, producing artifacts such as disconnected components and repeated motifs. These findings highlight both the promise and limitations of VGAEs for grid synthesis, underscoring the need for more expressive generative models and robust evaluation. We release our models and analysis as open source to support benchmarking and accelerate progress in ML-driven power system research.</li>
<li><strong>摘要：</strong>为了解决能源网络中机器学习研究缺乏公共电力系统数据，我们研究了变异图自动编码器（VGAES）用于合成分布网格产生的使用。我们使用两个开源数据集，即参与和丁戈，我们评估了四个解码器变体，并使用结构和光谱指标将生成的网络与原始网格进行比较。结果表明，简单的解码器无法捕获现实的拓扑，而基于GCN的方法在参与方面实现了强烈的保真度，但在更复杂的Dingo数据集上挣扎，产生了诸如断开的组件和重复图案之类的人工制品。这些发现突出了VGAE对网格合成的希望和局限性，强调了对更具表现力的生成模型的需求和可靠的评估。我们将模型和分析作为开源，以支持ML驱动的电力系统研究中的基准测试和加速进度。</li>
</ul>

<h3>Title: SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Zhenghai Xue, Longtao Zheng, Qian Liu, Yingru Li, Xiaosen Zheng, Zejun Ma, Bo An</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02479">https://arxiv.org/abs/2509.02479</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02479">https://arxiv.org/pdf/2509.02479</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02479]] SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning(https://arxiv.org/abs/2509.02479)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) can significantly improve their reasoning capabilities by interacting with external tools, a paradigm known as Tool-Integrated Reasoning (TIR). However, extending TIR to multi-turn scenarios using Reinforcement Learning (RL) is often hindered by training instability and performance collapse. We identify that such instability is primarily caused by a distributional drift from external tool feedback, leading to the generation of low-probability tokens. This issue compounds over successive turns, causing catastrophic gradient norm explosions that derail the training process. To address this challenge, we introduce SimpleTIR , a plug-and-play algorithm that stabilizes multi-turn TIR training. Its core strategy is to identify and filter out trajectories containing void turns, i.e., turns that yield neither a code block nor a final answer. By removing these problematic trajectories from the policy update, SimpleTIR effectively blocks the harmful, high-magnitude gradients, thus stabilizing the learning dynamics. Extensive experiments show that SimpleTIR achieves state-of-the-art performance on challenging math reasoning benchmarks, notably elevating the AIME24 score from a text-only baseline of 22.1 to 50.5 when starting from the Qwen2.5-7B base model. Furthermore, by avoiding the constraints of supervised fine-tuning, SimpleTIR encourages the model to discover diverse and sophisticated reasoning patterns, such as self-correction and cross-validation.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）可以通过与外部工具的互动，一种称为工具集成推理（TIR）的范式来显着提高其推理能力。但是，使用增强学习（RL）将TIR扩展到多转变场景通常受到训练不稳定和性能崩溃的阻碍。我们确定这种不稳定性主要是由于外部工具反馈的分布漂移引起的，导致产生低概率令牌。这个问题在连续的转弯中加入了混合，导致灾难性的梯度规范爆炸使训练过程脱轨。为了应对这一挑战，我们介绍了SimpleTir，这是一种稳定多转移TIR训练的插件算法。它的核心策略是识别和滤除包含空隙转弯的轨迹，即既不会产生代码块也不产生最终答案的转弯。通过从策略更新中删除这些有问题的轨迹，SimpleTir有效地阻止了有害的高磁性梯度，从而稳定了学习动态。广泛的实验表明，SimpleTir在具有挑战性的数学推理基准上实现了最新的性能，从QWEN2.5-7B基本模型开始时，AIME24分数尤其将AIME24分数从仅文本基线的22.1提高到50.5。此外，通过避免受监督的微调的限制，SimpleTir鼓励模型发现各种而复杂的推理模式，例如自我纠正和交叉验证。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
