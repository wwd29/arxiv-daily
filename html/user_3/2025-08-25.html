<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-08-25</h1>
<h3>Title: Text-Driven 3D Hand Motion Generation from Sign Language Data</h3>
<ul>
<li><strong>Authors: </strong>Léore Bensabath, Mathis Petrovich, Gül Varol</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.15902">https://arxiv.org/abs/2508.15902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.15902">https://arxiv.org/pdf/2508.15902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.15902]] Text-Driven 3D Hand Motion Generation from Sign Language Data(https://arxiv.org/abs/2508.15902)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Our goal is to train a generative model of 3D hand motions, conditioned on natural language descriptions specifying motion characteristics such as handshapes, locations, finger/hand/arm movements. To this end, we automatically build pairs of 3D hand motions and their associated textual labels with unprecedented scale. Specifically, we leverage a large-scale sign language video dataset, along with noisy pseudo-annotated sign categories, which we translate into hand motion descriptions via an LLM that utilizes a dictionary of sign attributes, as well as our complementary motion-script cues. This data enables training a text-conditioned hand motion diffusion model HandMDM, that is robust across domains such as unseen sign categories from the same sign language, but also signs from another sign language and non-sign hand movements. We contribute extensive experimental investigation of these scenarios and will make our trained models and data publicly available to support future research in this relatively new field.</li>
<li><strong>摘要：</strong>我们的目标是训练3D手动作的生成模型，以自然语言描述为条件，指定运动特征，例如握手，位置，手指/手/手臂运动。为此，我们自动以前所未有的量表构建了3D手动作及其相关的文本标签。具体来说，我们利用大型手语视频数据集以及嘈杂的伪注销的标志类别，我们通过利用符号属性字典的LLM转化为手运动描述，以及我们的互补运动 - 订阅线索。该数据使培训文本条件的手运动扩散模型手动模型，该模型在跨域中非常强大，例如来自同一手语的看不见的标志类别，但也来自另一种手语和非符号手动运动。我们对这些方案进行了广泛的实验研究，并将公开训练有素的模型和数据，以支持这个相对较新的领域的未来研究。</li>
</ul>

<h3>Title: Two-flow Feedback Multi-scale Progressive Generative Adversarial Network</h3>
<ul>
<li><strong>Authors: </strong>Sun Weikai, Song Shijie, Chi Wenjie</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.16089">https://arxiv.org/abs/2508.16089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.16089">https://arxiv.org/pdf/2508.16089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.16089]] Two-flow Feedback Multi-scale Progressive Generative Adversarial Network(https://arxiv.org/abs/2508.16089)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Although diffusion model has made good progress in the field of image generation, GAN\cite{huang2023adaptive} still has a large development space due to its unique advantages, such as WGAN\cite{liu2021comparing}, SSGAN\cite{guibas2021adaptive} \cite{zhang2022vsa} \cite{zhou2024adapt} and so on. In this paper, we propose a novel two-flow feedback multi-scale progressive generative adversarial network (MSPG-SEN) for GAN models. This paper has four contributions: 1) : We propose a two-flow feedback multi-scale progressive Generative Adversarial network (MSPG-SEN), which not only improves image quality and human visual perception on the basis of retaining the advantages of the existing GAN model, but also simplifies the training process and reduces the training cost of GAN networks. Our experimental results show that, MSPG-SEN has achieved state-of-the-art generation results on the following five datasets,INKK The dataset is 89.7\%,AWUN The dataset is 78.3\%,IONJ The dataset is 85.5\%,POKL The dataset is 88.7\%,OPIN The dataset is 96.4\%. 2) : We propose an adaptive perception-behavioral feedback loop (APFL), which effectively improves the robustness and training stability of the model and reduces the training cost. 3) : We propose a globally connected two-flow dynamic residual network(). After ablation experiments, it can effectively improve the training efficiency and greatly improve the generalization ability, with stronger flexibility. 4) : We propose a new dynamic embedded attention mechanism (DEMA). After experiments, the attention can be extended to a variety of image processing tasks, which can effectively capture global-local information, improve feature separation capability and feature expression capabilities, and requires minimal computing resources only 88.7\% with INJK With strong cross-task capability.</li>
<li><strong>摘要：</strong>尽管扩散模型在图像生成领域取得了良好的进步，但Gan \ cite {huang2023apaptive}由于其独特的优势，例如wgan \ cite {liu2021 -comparing}，ssgan，ssgan \ cite {guibas2021aptive} \ cite {zhang2022， \ cite {zhou2024Adapt}等。在本文中，我们提出了一种新型的两流反馈多尺度的渐进生成对抗网络（MSPG-SEN），用于GAN模型。本文有四个贡献：1）：我们提出了一个两流反馈的多尺度进行性生成对抗网络（MSPG-SEN），不仅可以根据保留现有GAN模型的优势来改善图像质量和人类视觉感知，而且还可以简化训练过程，并减少培训过程并降低gan网络的培训成本。我们的实验结果表明，MSPG-SEN在以下五个数据集中取得了最先进的生成结果，Inkk数据集为89.7 \％，AWUN数据集为78.3 \％，IONJ数据集为85.5 \％，POKL，POKL数据集为88.7 \％\％，请注意数据集为88.7 \％，即96.4 \％\％\％\％\％\ \ f。 2）：我们提出了一种自适应感知 - 行为反馈回路（APFL），该反馈有效地提高了模型的稳健性和训练稳定性并降低了训练成本。 3）：我们提出了一个全球连接的两流动态残留网络（）。经过消融实验，它可以有效提高训练效率，并以更强的灵活性大大提高概括能力。 4）：我们提出了一种新的动态嵌入式注意机制（DEMA）。实验之后，可以将注意力扩展到各种图像处理任务，这些任务可以有效地捕获全局本地信息，提高特征分离能力和特征表达能力，并且需要最少的计算资源，仅具有强大的交叉任务能力，仅88.7％\％。</li>
</ul>

<h3>Title: Machine Learning for Medicine Must Be Interpretable, Shareable, Reproducible and Accountable by Design</h3>
<ul>
<li><strong>Authors: </strong>Ayyüce Begüm Bektaş, Mithat Gönen</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.16097">https://arxiv.org/abs/2508.16097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.16097">https://arxiv.org/pdf/2508.16097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.16097]] Machine Learning for Medicine Must Be Interpretable, Shareable, Reproducible and Accountable by Design(https://arxiv.org/abs/2508.16097)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper claims that machine learning models deployed in high stakes domains such as medicine must be interpretable, shareable, reproducible and accountable. We argue that these principles should form the foundational design criteria for machine learning algorithms dealing with critical medical data, including survival analysis and risk prediction tasks. Black box models, while often highly accurate, struggle to gain trust and regulatory approval in health care due to a lack of transparency. We discuss how intrinsically interpretable modeling approaches (such as kernel methods with sparsity, prototype-based learning, and deep kernel models) can serve as powerful alternatives to opaque deep networks, providing insight into biomedical predictions. We then examine accountability in model development, calling for rigorous evaluation, fairness, and uncertainty quantification to ensure models reliably support clinical decisions. Finally, we explore how generative AI and collaborative learning paradigms (such as federated learning and diffusion-based data synthesis) enable reproducible research and cross-institutional integration of heterogeneous biomedical data without compromising privacy, hence shareability. By rethinking machine learning foundations along these axes, we can develop medical AI that is not only accurate but also transparent, trustworthy, and translatable to real-world clinical settings.</li>
<li><strong>摘要：</strong>本文声称，部署在诸如医学之类的高赌注域中的机器学习模型必须可解释，可共享，可重现和负责任。我们认为，这些原则应构成用于处理关键医学数据的机器学习算法的基础设计标准，包括生存分析和风险预测任务。黑匣子模型虽然通常非常准确，但由于缺乏透明度，努力获得在医疗保健方面的信任和监管批准。我们讨论本质上可解释的建模方法（例如具有稀疏性的内核方法，基于原型的学习和深内核模型）可以作为不透明深网络的强大替代方法，从而提供了对生物医学预测的见解。然后，我们检查模型开发中的问责制，要求进行严格的评估，公平性和不确定性量化，以确保模型可靠地支持临床决策。最后，我们探讨了生成的AI和协作学习范式（例如联合学习和基于扩散的数据综合）如何实现可重复的研究和异构生物医学数据的跨机构整合，而不会损害隐私，因此是共享性的。通过重新思考沿这些轴的机器学习基础，我们可以开发医学AI，它不仅准确，而且可以透明，值得信赖，并且可以转换为现实世界中的临床环境。</li>
</ul>

<h3>Title: RAGSR: Regional Attention Guided Diffusion for Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Haodong He, Yancheng Bai, Rui Lan, Xu Duan, Lei Sun, Xiangxiang Chu, Gui-Song Xia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.16158">https://arxiv.org/abs/2508.16158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.16158">https://arxiv.org/pdf/2508.16158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.16158]] RAGSR: Regional Attention Guided Diffusion for Image Super-Resolution(https://arxiv.org/abs/2508.16158)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generative</a></li>
<li><strong>Abstract: </strong>The rich textual information of large vision-language models (VLMs) combined with the powerful generative prior of pre-trained text-to-image (T2I) diffusion models has achieved impressive performance in single-image super-resolution (SISR). However, existing methods still face significant challenges in generating clear and accurate regional details, particularly in scenarios involving multiple objects. This challenge primarily stems from a lack of fine-grained regional descriptions and the models' insufficient ability to capture complex prompts. To address these limitations, we propose a Regional Attention Guided Super-Resolution (RAGSR) method that explicitly extracts localized fine-grained information and effectively encodes it through a novel regional attention mechanism, enabling both enhanced detail and overall visually coherent SR results. Specifically, RAGSR localizes object regions in an image and assigns fine-grained caption to each region, which are formatted as region-text pairs as textual priors for T2I models. A regional guided attention is then leveraged to ensure that each region-text pair is properly considered in the attention process while preventing unwanted interactions between unrelated region-text pairs. By leveraging this attention mechanism, our approach offers finer control over the integration of text and image information, thereby effectively overcoming limitations faced by traditional SISR techniques. Experimental results on benchmark datasets demonstrate that our approach exhibits superior performance in generating perceptually authentic visual details while maintaining contextual consistency compared to existing approaches.</li>
<li><strong>摘要：</strong>大型视觉模型（VLM）的丰富文本信息与预先训练的文本对图像（T2I）扩散模型相结合的强大生成性先验已在单像超级分辨率（SISR）中实现了令人印象深刻的性能。但是，现有方法在产生清晰准确的区域细节方面仍然面临重大挑战，尤其是在涉及多个对象的情况下。这项挑战主要是由于缺乏细粒度的区域描述以及模型捕获复杂提示的能力不足。为了解决这些局限性，我们提出了一种区域注意力指导的超分辨率（RAGSR）方法，该方法明确提取了局部的细粒度信息，并通过一种新型的区域注意机制有效地对其进行编码，从而既可以增强的细节和整体视觉相干SR结果。具体而言，RAGSR将对象区域定位在图像中，并为每个区域分配细颗粒的字幕，该区域以区域文本对格式化为T2i模型的文本先验。然后，将杠杆置一些区域引导的注意力，以确保在注意过程中正确考虑每个区域文本对，同时防止无关的区域文本对之间不必要的相互作用。通过利用这种注意机制，我们的方法可以更好地控制文本和图像信息的整合，从而有效地克服了传统SISR技术所面临的局限性。基准数据集的实验结果表明，我们的方法在产生感知真实的视觉细节时表现出卓越的性能，同时与现有方法相比保持上下文一致性。</li>
</ul>

<h3>Title: Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Shikang Zheng, Liang Feng, Xinyu Wang, Qinming Zhou, Peiliang Cai, Chang Zou, Jiacheng Liu, Yuqi Lin, Junjie Chen, Yue Ma, Linfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.16211">https://arxiv.org/abs/2508.16211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.16211">https://arxiv.org/pdf/2508.16211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.16211]] Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion Transformers(https://arxiv.org/abs/2508.16211)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generation</a></li>
<li><strong>Abstract: </strong>Diffusion Transformers (DiTs) have demonstrated exceptional performance in high-fidelity image and video generation. To reduce their substantial computational costs, feature caching techniques have been proposed to accelerate inference by reusing hidden representations from previous timesteps. However, current methods often struggle to maintain generation quality at high acceleration ratios, where prediction errors increase sharply due to the inherent instability of long-step forecasting. In this work, we adopt an ordinary differential equation (ODE) perspective on the hidden-feature sequence, modeling layer representations along the trajectory as a feature-ODE. We attribute the degradation of existing caching strategies to their inability to robustly integrate historical features under large skipping intervals. To address this, we propose FoCa (Forecast-then-Calibrate), which treats feature caching as a feature-ODE solving problem. Extensive experiments on image synthesis, video generation, and super-resolution tasks demonstrate the effectiveness of FoCa, especially under aggressive acceleration. Without additional training, FoCa achieves near-lossless speedups of 5.50 times on FLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high quality with a 4.53 times speedup on DiT.</li>
<li><strong>摘要：</strong>扩散变压器（DIT）在高保真图像和视频生成中表现出了出色的性能。为了降低其大量计算成本，已经提出了功能缓存技术来加速推理，通过重复以前的时间段中隐藏的表示。但是，当前的方法通常很难以高加速度的比率保持发电质量，因为由于长时间预测的固有不稳定，预测错误急剧增加。在这项工作中，我们对隐藏特征序列采用了普通的微分方程（ODE）透视图，将沿轨迹的图层表示作为特征对模型。我们将现有的缓存策略的退化归因于它们在大型跳过间隔下坚固地整合历史特征的原因。为了解决这个问题，我们提出了foca（预测 - 校准），该焦点将特征缓存视为特征码解决问题。关于图像合成，视频产生和超分辨率任务的广泛实验证明了FOCA的有效性，尤其是在积极的加速下。如果没有额外的训练，FOCA就可以在通量上获得近乎无情的加速度，在Hunyuanvideo上达到6.45次，在INF-DIT上达到3.17次，并保持高质量，DIT上的加速度为4.53倍。</li>
</ul>

<h3>Title: OmniCache: A Trajectory-Oriented Global Perspective on Training-Free Cache Reuse for Diffusion Transformer Models</h3>
<ul>
<li><strong>Authors: </strong>Huanpeng Chu, Wei Wu, Guanyu Fen, Yutao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.16212">https://arxiv.org/abs/2508.16212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.16212">https://arxiv.org/pdf/2508.16212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.16212]] OmniCache: A Trajectory-Oriented Global Perspective on Training-Free Cache Reuse for Diffusion Transformer Models(https://arxiv.org/abs/2508.16212)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as a powerful paradigm for generative tasks such as image synthesis and video generation, with Transformer architectures further enhancing performance. However, the high computational cost of diffusion Transformers-stemming from a large number of sampling steps and complex per-step computations-presents significant challenges for real-time deployment. In this paper, we introduce OmniCache, a training-free acceleration method that exploits the global redundancy inherent in the denoising process. Unlike existing methods that determine caching strategies based on inter-step similarities and tend to prioritize reusing later sampling steps, our approach originates from the sampling perspective of DIT models. We systematically analyze the model's sampling trajectories and strategically distribute cache reuse across the entire sampling process. This global perspective enables more effective utilization of cached computations throughout the diffusion trajectory, rather than concentrating reuse within limited segments of the sampling this http URL addition, during cache reuse, we dynamically estimate the corresponding noise and filter it out to reduce its impact on the sampling this http URL experiments demonstrate that our approach accelerates the sampling process while maintaining competitive generative quality, offering a promising and practical solution for efficient deployment of diffusion-based generative models.</li>
<li><strong>摘要：</strong>扩散模型已成为用于生成任务（例如图像合成和视频生成）的强大范式，变压器体系结构进一步增强了性能。但是，从大量的采样步骤和每个步骤计算的复杂计算中，扩散变压器的高计算成本造成了实时部署的重大挑战。在本文中，我们介绍了Omnicache，这是一种无训练的加速方法，利用了DeNoisis过程中固有的全球冗余。与现有的方法不同的方法基于跨步骤的相似性来确定缓存策略并倾向于重复使用以后的采样步骤，我们的方法源自DIT模型的采样角度。我们系统地分析了模型的采样轨迹，并在整个采样过程中策略性地分配了缓存重复使用。这种全球视角能够在整个扩散轨迹中更有效地利用缓存的计算，而不是将重复使用集中在有限的采样段中，在缓存重复使用过程中，我们动态估计相应的噪声并过滤掉它，以减少其对http url的影响，以降低其在htttp url的范围内，以表明其进一步的实验效果，使得进一步的实验能够启动良好的启动，以使其逐步启动，以使其逐渐启动，以使其逐步启动，以使其逐步启动，以使其逐步启动，以表明该速度的良好型号的效果，使得逐渐启动了进一步的实验效果。以及有效部署基于扩散的生成模型的实用解决方案。</li>
</ul>

<h3>Title: FlexMUSE: Multimodal Unification and Semantics Enhancement Framework with Flexible interaction for Creative Writing</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Chen, Zhiyong Ma, Wenbiao Du, Qingyuan Chuai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.16230">https://arxiv.org/abs/2508.16230</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.16230">https://arxiv.org/pdf/2508.16230</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.16230]] FlexMUSE: Multimodal Unification and Semantics Enhancement Framework with Flexible interaction for Creative Writing(https://arxiv.org/abs/2508.16230)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Multi-modal creative writing (MMCW) aims to produce illustrated articles. Unlike common multi-modal generative (MMG) tasks such as storytelling or caption generation, MMCW is an entirely new and more abstract challenge where textual and visual contexts are not strictly related to each other. Existing methods for related tasks can be forcibly migrated to this track, but they require specific modality inputs or costly training, and often suffer from semantic inconsistencies between modalities. Therefore, the main challenge lies in economically performing MMCW with flexible interactive patterns, where the semantics between the modalities of the output are more aligned. In this work, we propose FlexMUSE with a T2I module to enable optional visual input. FlexMUSE promotes creativity and emphasizes the unification between modalities by proposing the modality semantic alignment gating (msaGate) to restrict the textual input. Besides, an attention-based cross-modality fusion is proposed to augment the input features for semantic enhancement. The modality semantic creative direct preference optimization (mscDPO) within FlexMUSE is designed by extending the rejected samples to facilitate the writing creativity. Moreover, to advance the MMCW, we expose a dataset called ArtMUSE which contains with around 3k calibrated text-image pairs. FlexMUSE achieves promising results, demonstrating its consistency, creativity and coherence.</li>
<li><strong>摘要：</strong>多模式创意写作（MMCW）旨在制作插图文章。与常见的多模式生成（MMG）任务（例如讲故事或字幕生成）不同，MMCW是一个全新的，更抽象的挑战，文本和视觉上下文并不严格相互关系。可以强行将相关任务的现有方法迁移到此轨道，但是它们需要特定的方式输入或昂贵的培训，并且通常会遇到模态之间的语义不一致。因此，主要的挑战在于经济上执行MMCW具有灵活的交互式模式，在该模式下，输出方式之间的语义更加一致。在这项工作中，我们提出了使用T2I模块的FlexMuse，以启用可选的视觉输入。 Flexmuse通过提出模态语义对准门（MSAGATE）来限制文本输入，从而促进了创造力，并强调了模式之间的统一。此外，提出了基于注意力的跨模式融合，以增强语义增强的输入特征。 FlexMuse中的模态语义创意直接优化优化（MSCDPO）的设计是通过扩展被拒绝的样本以促进写作创造力的设计。此外，为了推进MMCW，我们将公开一个名为ArtMuse的数据集，该数据集包含大约3K校准的文本图像对。 Flexmuse取得了令人鼓舞的结果，证明了其一致性，创造力和连贯性。</li>
</ul>

<h3>Title: UniEM-3M: A Universal Electron Micrograph Dataset for Microstructural Segmentation and Generation</h3>
<ul>
<li><strong>Authors: </strong>Nan wang, Zhiyi Xia, Yiming Li, Shi Tang, Zuxin Fan, Xi Fang, Haoyi Tao, Xiaochen Cai, Guolin Ke, Linfeng Zhang, Yanhui Hong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.16239">https://arxiv.org/abs/2508.16239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.16239">https://arxiv.org/pdf/2508.16239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.16239]] UniEM-3M: A Universal Electron Micrograph Dataset for Microstructural Segmentation and Generation(https://arxiv.org/abs/2508.16239)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Quantitative microstructural characterization is fundamental to materials science, where electron micrograph (EM) provides indispensable high-resolution insights. However, progress in deep learning-based EM characterization has been hampered by the scarcity of large-scale, diverse, and expert-annotated datasets, due to acquisition costs, privacy concerns, and annotation complexity. To address this issue, we introduce UniEM-3M, the first large-scale and multimodal EM dataset for instance-level understanding. It comprises 5,091 high-resolution EMs, about 3 million instance segmentation labels, and image-level attribute-disentangled textual descriptions, a subset of which will be made publicly available. Furthermore, we are also releasing a text-to-image diffusion model trained on the entire collection to serve as both a powerful data augmentation tool and a proxy for the complete data distribution. To establish a rigorous benchmark, we evaluate various representative instance segmentation methods on the complete UniEM-3M and present UniEM-Net as a strong baseline model. Quantitative experiments demonstrate that this flow-based model outperforms other advanced methods on this challenging benchmark. Our multifaceted release of a partial dataset, a generative model, and a comprehensive benchmark -- available at huggingface -- will significantly accelerate progress in automated materials analysis.</li>
<li><strong>摘要：</strong>定量微结构表征是材料科学的基础，其中电子显微照片（EM）提供了必不可少的高分辨率见解。但是，由于获得成本，隐私问题和注释复杂性，大规模，多样化和专家注册的数据集的稀缺性的稀缺使基于深度学习的EM表征的进展受到了阻碍。为了解决此问题，我们介绍了Uniem-3M，这是第一个大规模和多模式数据集，以供实例级别的理解。它包括5,091个高分辨率EMS，大约300万个实例细分标签和图像级属性 - 访问式文本描述，其中将公开可用。此外，我们还发布了在整个集合中训练的文本对图扩散模型，既可以用作强大的数据增强工具，又是完整数据分布的代理。为了建立严格的基准测试，我们评估了完整的Uniem-3M上的各种代表性实例分割方法，并将Uniem-NET作为强大的基线模型。定量实验表明，这种基于流的模型在该挑战性基准上优于其他高级方法。我们多方面的部分数据集，生成模型和全面的基准（可在HuggingFace中获得）将大大加速自动化材料分析中的进展。</li>
</ul>

<h3>Title: FEST: A Unified Framework for Evaluating Synthetic Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Weijie Niu, Alberto Huertas Celdran, Karoline Siarsky, Burkhard Stiller</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.16254">https://arxiv.org/abs/2508.16254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.16254">https://arxiv.org/pdf/2508.16254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.16254]] FEST: A Unified Framework for Evaluating Synthetic Tabular Data(https://arxiv.org/abs/2508.16254)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Synthetic data generation, leveraging generative machine learning techniques, offers a promising approach to mitigating privacy concerns associated with real-world data usage. Synthetic data closely resembles real-world data while maintaining strong privacy guarantees. However, a comprehensive assessment framework is still missing in the evaluation of synthetic data generation, especially when considering the balance between privacy preservation and data utility in synthetic data. This research bridges this gap by proposing FEST, a systematic framework for evaluating synthetic tabular data. FEST integrates diverse privacy metrics (attack-based and distance-based), along with similarity and machine learning utility metrics, to provide a holistic assessment. We develop FEST as an open-source Python-based library and validate it on multiple datasets, demonstrating its effectiveness in analyzing the privacy-utility trade-off of different synthetic data generation models. The source code of FEST is available on Github.</li>
<li><strong>摘要：</strong>合成数据生成，利用生成机器学习技术，提供了一种有希望的方法来减轻与现实世界数据使用相关的隐私问题。综合数据在维持强大的隐私保证的同时，与现实世界中的数据非常相似。但是，在评估合成数据生成时，仍然缺少全面的评估框架，尤其是在考虑合成数据中隐私保存与数据实用程序之间的平衡时。这项研究通过提出节日来弥合这一差距，这是评估合成表格数据的系统框架。 Fest集成了各种隐私指标（基于攻击和基于距离），以及相似性和机器学习实用程序指标，以提供整体评估。我们开发为开源Python图书馆的节日，并在多个数据集上进行验证，并证明了其在分析不同合成数据生成模型的隐私性权衡权衡方面的有效性。 Fest的源代码可在GitHub上获得。</li>
</ul>

<h3>Title: Chunked Data Shapley: A Scalable Dataset Quality Assessment for Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Andreas Loizou, Dimitrios Tsoumakos</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.16255">https://arxiv.org/abs/2508.16255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.16255">https://arxiv.org/pdf/2508.16255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.16255]] Chunked Data Shapley: A Scalable Dataset Quality Assessment for Machine Learning(https://arxiv.org/abs/2508.16255)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>As the volume and diversity of available datasets continue to increase, assessing data quality has become crucial for reliable and efficient Machine Learning analytics. A modern, game-theoretic approach for evaluating data quality is the notion of Data Shapley which quantifies the value of individual data points within a dataset. State-of-the-art methods to scale the NP-hard Shapley computation also face severe challenges when applied to large-scale datasets, limiting their practical use. In this work, we present a Data Shapley approach to identify a dataset's high-quality data tuples, Chunked Data Shapley (C-DaSh). C-DaSh scalably divides the dataset into manageable chunks and estimates the contribution of each chunk using optimized subset selection and single-iteration stochastic gradient descent. This approach drastically reduces computation time while preserving high quality results. We empirically benchmark our method on diverse real-world classification and regression tasks, demonstrating that C-DaSh outperforms existing Shapley approximations in both computational efficiency (achieving speedups between 80x - 2300x) and accuracy in detecting low-quality data regions. Our method enables practical measurement of dataset quality on large tabular datasets, supporting both classification and regression pipelines.</li>
<li><strong>摘要：</strong>随着可用数据集的数量和多样性继续增加，评估数据质量对于可靠，有效的机器学习分析至关重要。用于评估数据质量的现代游戏理论方法是数据莎普利的概念，该概念量化了数据集中各个数据点的价值。当应用于大规模数据集时，缩放NP-Hard Shapley计算的最先进方法也面临着严重的挑战，从而限制了它们的实际使用。在这项工作中，我们提供了一种数据Shapley方法，以识别数据集的高质量数据元组，即块状数据Shapley（C-DASH）。 C-DASH可靠地将数据集划分为可管理的块，并使用优化的子集选择和单材料随机梯度下降来估算每个块的贡献。这种方法大大减少了计算时间，同时保留了高质量的结果。我们从经验上基于各种现实世界分类和回归任务的方法基准，这表明C-Dash在计算效率（达到80x-2300x之间达到速度）和检测低质量数据区域的准确性都优于现有的Shapley近似值。我们的方法可以在大型表格数据集上对数据集质量进行实际测量，从而支持分类和回归管道。</li>
</ul>

<h3>Title: Structuring GUI Elements through Vision Language Models: Towards Action Space Generation</h3>
<ul>
<li><strong>Authors: </strong>Yi Xu, Yesheng Zhang, jiajia Liu, Jingdong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.16271">https://arxiv.org/abs/2508.16271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.16271">https://arxiv.org/pdf/2508.16271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.16271]] Structuring GUI Elements through Vision Language Models: Towards Action Space Generation(https://arxiv.org/abs/2508.16271)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have emerged as pivotal tools in enhancing human-computer interaction. In this paper we focus on the application of MLLMs in the field of graphical user interface (GUI) elements structuring, where they assist in processing user instructions based on screen contents. Despite the promise of MLLMs, their performance in precisely generating UI element coordinates, a critical aspect of GUI understanding, is hindered by the nature of next-token prediction training. This challenge arises from the semantic void surrounding numerical UI coordinates in language representation spaces, necessitating a substantial and diverse dataset to bolster visual module capabilities. To address these limitations, we introduce an IoU-Augmented Maximum Likelihood (IAML) training paradigm. Specifically, our approach involves a novel pipeline for IoU-based coordinate sampling to augment the training data, which considers the proximity to ground truth coordinates. This data augmentation strategy is then employed to fine-tune MLLMs under the IAML paradigm, which is designed to mitigate the exposure bias problem inherent in traditional maximum likelihood estimation. Through extensive experiments, we demonstrate the superior performance of our IAML training approach over traditional training paradigms.</li>
<li><strong>摘要：</strong>多模式的大语言模型（MLLM）已成为增强人类计算机相互作用的关键工具。在本文中，我们着重于MLLM在图形用户界面（GUI）元素构造领域的应用，它们可以根据屏幕内容协助处理用户指令。尽管有MLLM的承诺，但它们在精确产生UI元素坐标方面的表现，这是GUI理解的关键方面，这受到了下一步预测培训的性质的阻碍。这一挑战来自语言表示空间中数值UI坐标的语义空隙，因此需要一个大量而多样的数据集来增强视觉模块功能。为了解决这些限制，我们引入了一个iou-augment的最大可能性（IAML）训练范式。具体而言，我们的方法涉及基于IOU的坐标采样的新型管道，以增强培训数据，从而考虑了与地面真相坐标的接近性。然后将这种数据增强策略用于IAML范式下的微调MLLM，旨在减轻传统最大似然估计中固有的暴露偏差问题。通过广泛的实验，我们证明了IAML培训方法的出色表现，而不是传统训练范式。</li>
</ul>

<h3>Title: Vision encoders should be image size agnostic and task driven</h3>
<ul>
<li><strong>Authors: </strong>Nedyalko Prisadnikov, Danda Pani Paudel, Yuqian Fu, Luc Van Gool</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.16317">https://arxiv.org/abs/2508.16317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.16317">https://arxiv.org/pdf/2508.16317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.16317]] Vision encoders should be image size agnostic and task driven(https://arxiv.org/abs/2508.16317)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This position paper argues that the next generation of vision encoders should be image size agnostic and task driven. The source of our inspiration is biological. Not a structural aspect of biological vision, but a behavioral trait -- efficiency. We focus on a couple of ways in which vision in nature is efficient, but modern vision encoders not. We -- humans and animals -- deal with vast quantities of visual data, and need to be smart where we focus our limited energy -- it depends on the task. It is our belief that vision encoders should be dynamic and the computational complexity should depend on the task at hand rather than the size of the image. We, also, provide concrete first steps towards our vision -- a proof-of-concept solution for image classification. Despite classification being not very representative for what we are trying to achieve, it shows that our approach is feasible and promising.</li>
<li><strong>摘要：</strong>该立场论文认为，下一代视觉编码器应该是图像大小不可知和任务驱动的。我们灵感的来源是生物学。不是生物视觉的结构性方面，而是行为特征 - 效率。我们专注于几种自然景象有效的方式，但现代视觉却没有编码。我们 - 人类和动物 - 处理大量的视觉数据，需要在我们集中精力有限的地方变得聪明 - 这取决于任务。我们相信，视觉编码器应该是动态的，并且计算复杂性应取决于手头的任务，而不是图像的大小。我们还提供了朝着愿景的具体第一步 - 图像分类的概念验证解决方案。尽管分类不是我们试图实现的代表性的，但它表明我们的方法是可行的和有前途的。</li>
</ul>

<h3>Title: Boardwalk: Towards a Framework for Creating Board Games with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Álvaro Guglielmin Becker, Gabriel Bauer de Oliveira, Lana Bertoldo Rossato, Anderson Rocha Tavares</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.16447">https://arxiv.org/abs/2508.16447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.16447">https://arxiv.org/pdf/2508.16447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.16447]] Boardwalk: Towards a Framework for Creating Board Games with LLMs(https://arxiv.org/abs/2508.16447)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Implementing board games in code can be a time-consuming task. However, Large Language Models (LLMs) have been proven effective at generating code for domain-specific tasks with simple contextual information. We aim to investigate whether LLMs can implement digital versions of board games from rules described in natural language. This would be a step towards an LLM-assisted framework for quick board game code generation. We expect to determine the main challenges for LLMs to implement the board games, and how different approaches and models compare to one another. We task three state-of-the-art LLMs (Claude, DeepSeek and ChatGPT) with coding a selection of 12 popular and obscure games in free-form and within Boardwalk, our proposed General Game Playing API. We anonymize the games and components to avoid evoking pre-trained LLM knowledge. The implementations are tested for playability and rule compliance. We evaluate success rate and common errors across LLMs and game popularity. Our approach proves viable, with the best performing model, Claude 3.7 Sonnet, yielding 55.6\% of games without any errors. While compliance with the API increases error frequency, the severity of errors is more significantly dependent on the LLM. We outline future steps for creating a framework to integrate this process, making the elaboration of board games more accessible.</li>
<li><strong>摘要：</strong>在代码中实施棋盘游戏可能是一项耗时的任务。但是，大型语言模型（LLMS）已被证明有效地通过简单的上下文信息为特定领域的任务生成代码。我们旨在调查LLM是否可以从自然语言中描述的规则中实施数字版本的棋盘游戏。这将是迈向LLM辅助框架的一步，以快速棋盘游戏代码生成。我们希望确定LLMS实施棋盘游戏的主要挑战，以及不同的方法和模型如何相比。我们将三个最先进的LLMS（Claude，DeepSeek和Chatgpt）任命为编码在自由形式和木板路内的12种流行和晦涩的游戏，我们拟议的一般游戏玩API。我们将游戏和组件匿名化，以避免唤起预先训练的LLM知识。测试了实现的可玩性和规则合规性。我们评估LLM和游戏流行度的成功率和常见错误。我们的方法证明了可行的，具有表现最好的模型Claude 3.7十四行诗，在没有任何错误的情况下产生了55.6％的游戏。虽然符合API会增加误差频率，但错误的严重性更明显地取决于LLM。我们概述了创建一个集成此过程的框架的未来步骤，从而使棋盘游戏的详细说明更加易于访问。</li>
</ul>

<h3>Title: Arbitrary-Scale 3D Gaussian Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Huimin Zeng, Yue Bai, Yun Fu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.16467">https://arxiv.org/abs/2508.16467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.16467">https://arxiv.org/pdf/2508.16467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.16467]] Arbitrary-Scale 3D Gaussian Super-Resolution(https://arxiv.org/abs/2508.16467)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generative</a></li>
<li><strong>Abstract: </strong>Existing 3D Gaussian Splatting (3DGS) super-resolution methods typically perform high-resolution (HR) rendering of fixed scale factors, making them impractical for resource-limited scenarios. Directly rendering arbitrary-scale HR views with vanilla 3DGS introduces aliasing artifacts due to the lack of scale-aware rendering ability, while adding a post-processing upsampler for 3DGS complicates the framework and reduces rendering efficiency. To tackle these issues, we build an integrated framework that incorporates scale-aware rendering, generative prior-guided optimization, and progressive super-resolving to enable 3D Gaussian super-resolution of arbitrary scale factors with a single 3D model. Notably, our approach supports both integer and non-integer scale rendering to provide more flexibility. Extensive experiments demonstrate the effectiveness of our model in rendering high-quality arbitrary-scale HR views (6.59 dB PSNR gain over 3DGS) with a single model. It preserves structural consistency with LR views and across different scales, while maintaining real-time rendering speed (85 FPS at 1080p).</li>
<li><strong>摘要：</strong>现有的3D高斯脱落（3DGS）超分辨率方法通常对固定比例因子进行高分辨率（HR）渲染，这使得它们对于资源有限的方案不切实际。通过缺乏比例意识的渲染能力，使用香草3DG的任意规模的人力资源视图引入了混乱的工件，同时添加了3DGS的后处理UPSMPLER会使框架复杂化并降低渲染效率。为了解决这些问题，我们建立了一个集成的框架，该框架结合了规模感知的渲染，生成的先前指导优化和渐进的超级解决方案，以通过单个3D模型启用3D高斯任意尺度因子的超级分辨率。值得注意的是，我们的方法支持整数和非整数规模渲染，以提供更大的灵活性。广泛的实验证明了我们模型在使用单个模型的高质量任意尺度的HR视图（6.59 dB PSNR增益）方面的有效性。它可以保持LR视图和不同尺度的结构一致性，同时保持实时渲染速度（1080p时为85 fps）。</li>
</ul>

<h3>Title: Seeing Clearly, Forgetting Deeply: Revisiting Fine-Tuned Video Generators for Driving Simulation</h3>
<ul>
<li><strong>Authors: </strong>Chun-Peng Chang, Chen-Yu Wang, Julian Schmidt, Holger Caesar, Alain Pagani</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.16512">https://arxiv.org/abs/2508.16512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.16512">https://arxiv.org/pdf/2508.16512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.16512]] Seeing Clearly, Forgetting Deeply: Revisiting Fine-Tuned Video Generators for Driving Simulation(https://arxiv.org/abs/2508.16512)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advancements in video generation have substantially improved visual quality and temporal coherence, making these models increasingly appealing for applications such as autonomous driving, particularly in the context of driving simulation and so-called "world models". In this work, we investigate the effects of existing fine-tuning video generation approaches on structured driving datasets and uncover a potential trade-off: although visual fidelity improves, spatial accuracy in modeling dynamic elements may degrade. We attribute this degradation to a shift in the alignment between visual quality and dynamic understanding objectives. In datasets with diverse scene structures within temporal space, where objects or perspective shift in varied ways, these objectives tend to highly correlated. However, the very regular and repetitive nature of driving scenes allows visual quality to improve by modeling dominant scene motion patterns, without necessarily preserving fine-grained dynamic behavior. As a result, fine-tuning encourages the model to prioritize surface-level realism over dynamic accuracy. To further examine this phenomenon, we show that simple continual learning strategies, such as replay from diverse domains, can offer a balanced alternative by preserving spatial accuracy while maintaining strong visual quality.</li>
<li><strong>摘要：</strong>视频生成的最新进展已大大提高了视觉质量和时间连贯性，使这些模型越来越吸引诸如自主驾驶的应用，尤其是在驾驶模拟和所谓的“世界模型”的背景下。在这项工作中，我们研究了现有的微调视频生成方法对结构化驾驶数据集的影响，并发现了潜在的权衡：尽管视觉保真度有所提高，但在建模动态元素中的空间准确性可能会降低。我们将这种退化归因于视觉质量与动态理解目标之间的一致性的转变。在时间空间内具有不同场景结构的数据集中，对象或视角以各种方式转移，这些目标往往高度相关。但是，驾驶场景的非常规定和重复的性质使视觉质量通过对主场运动模式进行建模，而不必保留细粒度的动态行为。结果，微调鼓励模型优先考虑表面级别的现实主义而不是动态精度。为了进一步研究这种现象，我们表明，简单的持续学习策略（例如来自不同领域的重放）可以通过保持空间准确性，同时保持强大的视觉质量来提供平衡的替代方案。</li>
</ul>

<h3>Title: FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the Data Synthesis Pipeline</h3>
<ul>
<li><strong>Authors: </strong>Parker Seegmiller, Kartik Mehta, Soumya Saha, Chenyang Tao, Shereen Oraby, Arpit Gupta, Tagyoung Chung, Mohit Bansal, Nanyun Peng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.16514">https://arxiv.org/abs/2508.16514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.16514">https://arxiv.org/pdf/2508.16514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.16514]] FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the Data Synthesis Pipeline(https://arxiv.org/abs/2508.16514)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent works improving LLM math reasoning with synthetic data have used unique setups, making comparison of data synthesis strategies impractical. This leaves many unanswered questions about the roles of different factors in the synthetic data pipeline, such as the impact of filtering low-quality problems. To address this gap, we introduce FLAMES, a Framework for LLM Assessment of Math rEasoning Data Synthesis, and perform a systematic study of 10 existing data synthesis strategies and multiple other factors impacting the performance of synthetic math reasoning data. Our FLAMES experiments provide several valuable insights about the optimal balance of difficulty and diversity of synthetic data. First, data agents designed to increase problem complexity lead to best improvements on most math metrics. Second, with a fixed data generation budget, keeping higher problem coverage is more important than keeping only problems with reliable solutions. Third, GSM8K- and MATH-based synthetic data can lead to improvements on competition-level benchmarks, showcasing easy-to-hard generalization. Leveraging insights from our FLAMES experiments, we design two novel data synthesis strategies for improving out-of-domain generalization and robustness. Further, we develop the FLAMES dataset, an effective blend of our novel and existing data synthesis strategies, outperforming public datasets on OlympiadBench (+15.7), CollegeMath (+4.5), GSMPlus (+6.5), and MATH (+3.1). Fine-tuning Qwen2.5-Math-7B on the FLAMES dataset achieves 81.4% on MATH, surpassing larger Llama3 405B, GPT-4o and Claude 3.5 Sonnet.</li>
<li><strong>摘要：</strong>使用合成数据改善LLM数学推理的最新工作使用了独特的设置，从而比较了数据合成策略不切实际。这留下了许多关于不同因素在合成数据管道中的作用的未解决问题，例如过滤低质量问题的影响。为了解决这一差距，我们介绍了火焰，这是LLM评估数学推理数据综合的框架，并对10种现有数据综合策略以及影响合成数学推理数据的性能的10个其他因素进行系统研究。我们的火焰实验提供了几种有价值的见解，内容涉及综合数据的难度和多样性的最佳平衡。首先，旨在提高问题复杂性的数据代理会导致大多数数学指标的最佳改进。其次，凭借固定的数据生成预算，保持更高的问题覆盖范围比仅保留可靠解决方案的问题更重要。第三，基于GSM8K和基于数学的合成数据可以改善竞争级别的基准测试，从而展示易于硬化的概括。利用我们的火焰实验的见解，我们设计了两种新型的数据综合策略，以改善跨域的概括和鲁棒性。此外，我们开发了火焰数据集，这是我们新颖和现有的数据综合策略的有效融合，在奥林匹克邦德（Olympiadbench）（+15.7），collegemath（+4.5），GSMPLU（+6.5）和Math（+3.1）上的公共数据集优于公共数据集（+15.7）。 Flames数据集上的微调QWEN2.5-MATH-7B在数学上达到81.4％，超过了较大的Llama3 405B，GPT-4O和Claude 3.5十四行诗。</li>
</ul>

<h3>Title: Guiding Diffusion Models with Reinforcement Learning for Stable Molecule Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhijian Zhou, Junyi An, Zongkai Liu, Yunfei Shi, Xuan Zhang, Fenglei Cao, Chao Qu, Yuan Qi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.16521">https://arxiv.org/abs/2508.16521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.16521">https://arxiv.org/pdf/2508.16521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.16521]] Guiding Diffusion Models with Reinforcement Learning for Stable Molecule Generation(https://arxiv.org/abs/2508.16521)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generating physically realistic 3D molecular structures remains a core challenge in molecular generative modeling. While diffusion models equipped with equivariant neural networks have made progress in capturing molecular geometries, they often struggle to produce equilibrium structures that adhere to physical principles such as force field consistency. To bridge this gap, we propose Reinforcement Learning with Physical Feedback (RLPF), a novel framework that extends Denoising Diffusion Policy Optimization to 3D molecular generation. RLPF formulates the task as a Markov decision process and applies proximal policy optimization to fine-tune equivariant diffusion models. Crucially, RLPF introduces reward functions derived from force-field evaluations, providing direct physical feedback to guide the generation toward energetically stable and physically meaningful structures. Experiments on the QM9 and GEOM-drug datasets demonstrate that RLPF significantly improves molecular stability compared to existing methods. These results highlight the value of incorporating physics-based feedback into generative modeling. The code is available at: this https URL.</li>
<li><strong>摘要：</strong>产生物理逼真的3D分子结构仍然是分子生成建模的核心挑战。虽然配备了模仿神经网络的扩散模型在捕获分子几何形状方面取得了进展，但它们通常很难生成遵循物理原理（例如力场一致性）的平衡结构。为了弥合这一差距，我们建议使用物理反馈（RLPF）进行增强学习，这是一个新型框架，将扩散策略优化扩展到3D分子生成。 RLPF将任务制定为马尔可夫决策过程，并将近端策略优化应用于微调型号扩散模型。至关重要的是，RLPF引入了从力场评估中得出的奖励功能，提供了直接的物理反馈，以指导生成能量稳定且物理上有意义的结构。与现有方法相比，QM9和GEOM-PRUG数据集的实验表明，RLPF显着提高了分子稳定性。这些结果突出了将基于物理的反馈纳入生成建模的价值。该代码可用：此HTTPS URL。</li>
</ul>

<h3>Title: RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs. Reinforcement Learning Fine-Tuning for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Hangzhan Jin, Sicheng Lv, Sifan Wu, Mohammad Hamdaqa</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.16546">https://arxiv.org/abs/2508.16546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.16546">https://arxiv.org/pdf/2508.16546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.16546]] RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs. Reinforcement Learning Fine-Tuning for LLMs(https://arxiv.org/abs/2508.16546)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Training large language models (LLMs) from scratch is increasingly impractical, making post-training methods such as supervised fine-tuning (SFT) and reinforcement-learning fine-tuning (RL-FT, e.g., PPO) central to modern practice. Using an out-of-distribution (OOD) variant of the 24-point card game and new spectrum-based diagnostics, we revisit how these two stages reshape model representation and OOD performance. Our key findings are- (1) RL-FT can restore much of the OOD performance loss from SFT (e.g., Llama-11B 8.97% to 15.38%, Qwen-7B 17.09% to 19.66%). But when SFT induces severe overfitting and a clear distribution shift, RL-FT cannot fully recover OOD performance. (2) Direction shifts of singular vectors matter more than singular value magnitudes. These shifts concentrate on directions linked to the largest and smallest singular values, leaving the bulk spectrum intact. (3) Low-rank and shallow recovery is effective: restoring singular vector directions for the top 20% of values or first 25% of layers recovers 70-80% of OOD performance. (4) Stronger SFT checkpoints enable better recovery by RL, while overfitted ones resist restoration. These results reconcile prior reports of RL superior OOD performance: RL primarily counteracts SFT-induced directional drift rather than finding new solutions. Our spectrum-aware analysis highlights inexpensive recovery knobs low-rank UV merging and shallow-layer resets that practitioners can use before costly RL fine-tuning.</li>
<li><strong>摘要：</strong>从头开始培训大型语言模型（LLMS）越来越不切实际，从而使训练后的方法（例如监督微调（SFT）和加强学习微调（RL-FT）（例如PPO）是现代实践中心的。使用24点卡游戏和基于新的基于频谱的诊断的分布式（OOD）变体，我们重新审视这两个阶段如何重新设计模型表示和OOD性能。我们的主要发现是 - （1）RL-FT可以恢复SFT的大部分OOD性能损失（例如Llama-111b 8.97％至15.38％，QWEN-7B 17.09％至19.66％）。但是，当SFT引起严重的过度拟合和明显的分配变化时，RL-FT将无法完全恢复OOD的性能。 （2）奇异矢量的方向移动比单数值的幅度更重要。这些变化集中在与最大和最小的单数值相关的方向上，使整体频谱完好无损。 （3）低级别和浅恢复是有效的：恢复值前20％或前25％层恢复了OOD性能的70-80％的奇异矢量方向。 （4）更强的SFT检查点可以通过RL更好地恢复，而过于拟合的SFT检查点可以抵抗恢复。这些结果调和了RL上级性能的先前报告：RL主要抵消SFT诱导的方向漂移，而不是找到新的解决方案。我们的频谱感知分析强调了廉价的恢复旋钮低级别的紫外线合并和浅层重置，从业人员可以在昂贵的RL微调之前使用这些旋钮。</li>
</ul>

<h3>Title: TinyML Towards Industry 4.0: Resource-Efficient Process Monitoring of a Milling Machine</h3>
<ul>
<li><strong>Authors: </strong>Tim Langer, Matthias Widra, Volkhard Beyer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.ET, eess.SP, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.16553">https://arxiv.org/abs/2508.16553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.16553">https://arxiv.org/pdf/2508.16553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.16553]] TinyML Towards Industry 4.0: Resource-Efficient Process Monitoring of a Milling Machine(https://arxiv.org/abs/2508.16553)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In the context of industry 4.0, long-serving industrial machines can be retrofitted with process monitoring capabilities for future use in a smart factory. One possible approach is the deployment of wireless monitoring systems, which can benefit substantially from the TinyML paradigm. This work presents a complete TinyML flow from dataset generation, to machine learning model development, up to implementation and evaluation of a full preprocessing and classification pipeline on a microcontroller. After a short review on TinyML in industrial process monitoring, the creation of the novel MillingVibes dataset is described. The feasibility of a TinyML system for structure-integrated process quality monitoring could be shown by the development of an 8-bit-quantized convolutional neural network (CNN) model with 12.59kiB parameter storage. A test accuracy of 100.0% could be reached at 15.4ms inference time and 1.462mJ per quantized CNN inference on an ARM Cortex M4F microcontroller, serving as a reference for future TinyML process monitoring solutions.</li>
<li><strong>摘要：</strong>在行业4.0的背景下，可以使用较长的工业机器进行改装，并具有过程监视功能，以供将来在智能工厂中使用。一种可能的方法是部署无线监控系统，该系统可以从Tinyml范式中受益匪浅。这项工作介绍了从数据集生成到机器学习模型开发，到对微控制器上完整的预处理和分类管道的实施和评估的完整底层流。在对工业过程监测中的Tinyml进行了简短的审查之后，描述了新型Millingvibes数据集的创建。通过开发具有12.59KIB参数存储的8位定量卷积神经网络（CNN）模型，可以显示用于结构集成过程质量监测的Tinyml系统用于结构集成过程质量监测的可行性。可以在15.4ms的推理时间达到100.0％的测试准确性，而每量化的CNN推断ARM Cortex M4F Mycontroller的CNN推断为1.462MJ，可作为未来Tinyml过程监测溶液的参考。</li>
</ul>

<h3>Title: MV-RAG: Retrieval Augmented Multiview Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Yosef Dayani, Omer Benishu, Sagie Benaim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.16577">https://arxiv.org/abs/2508.16577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.16577">https://arxiv.org/pdf/2508.16577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.16577]] MV-RAG: Retrieval Augmented Multiview Diffusion(https://arxiv.org/abs/2508.16577)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-3D generation approaches have advanced significantly by leveraging pretrained 2D diffusion priors, producing high-quality and 3D-consistent outputs. However, they often fail to produce out-of-domain (OOD) or rare concepts, yielding inconsistent or inaccurate results. To this end, we propose MV-RAG, a novel text-to-3D pipeline that first retrieves relevant 2D images from a large in-the-wild 2D database and then conditions a multiview diffusion model on these images to synthesize consistent and accurate multiview outputs. Training such a retrieval-conditioned model is achieved via a novel hybrid strategy bridging structured multiview data and diverse 2D image collections. This involves training on multiview data using augmented conditioning views that simulate retrieval variance for view-specific reconstruction, alongside training on sets of retrieved real-world 2D images using a distinctive held-out view prediction objective: the model predicts the held-out view from the other views to infer 3D consistency from 2D data. To facilitate a rigorous OOD evaluation, we introduce a new collection of challenging OOD prompts. Experiments against state-of-the-art text-to-3D, image-to-3D, and personalization baselines show that our approach significantly improves 3D consistency, photorealism, and text adherence for OOD/rare concepts, while maintaining competitive performance on standard benchmarks.</li>
<li><strong>摘要：</strong>文本到3D生成方法通过利用预估计的2D扩散先验，产生高质量和3D一致的输出来显着提高。但是，它们通常无法产生室外（OOD）或稀有概念，从而产生不一致或不准确的结果。为此，我们提出了MV-rag，这是一种新型的文本到3D管道，首先从大型野外2D数据库中检索相关的2D图像，然后在这些图像上调节多视频扩散模型，以合成一致且准确的多视输出。培训这种检索条件模型是通过新型混合策略桥接结构化的多视图数据和不同的2D图像收集来实现的。这涉及使用增强条件视图对多视图数据进行培训，这些视图模拟了特定于视图的重建的检索差异，以及使用独特的固定视图预测目标对检索到的现实世界2D图像集进行培训：从2D数据中推断出其他视图从其他视图中预示了持有的视图。为了促进严格的OOD评估，我们介绍了新的具有挑战性的OOD提示。针对最先进的文本到3D，图像到3D和个性化基线的实验表明，我们的方法可显着提高3D一致性，光真实主义和OOD/稀有概念的文本依从性，同时保持在标准基准上的竞争性能。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
