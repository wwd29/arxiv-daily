<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-25</h1>
<h3>Title: Frame-based Equivariant Diffusion Models for 3D Molecular Generation</h3>
<ul>
<li><strong>Authors: </strong>Mohan Guo (Faculty of Science, University of Amsterdam), Cong Liu (AMLab, University of Amsterdam), Patrick Forré (AMLab, University of Amsterdam)</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19506">https://arxiv.org/abs/2509.19506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19506">https://arxiv.org/pdf/2509.19506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19506]] Frame-based Equivariant Diffusion Models for 3D Molecular Generation(https://arxiv.org/abs/2509.19506)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent methods for molecular generation face a trade-off: they either enforce strict equivariance with costly architectures or relax it to gain scalability and flexibility. We propose a frame-based diffusion paradigm that achieves deterministic E(3)-equivariance while decoupling symmetry handling from the backbone. Building on this paradigm, we investigate three variants: Global Frame Diffusion (GFD), which assigns a shared molecular frame; Local Frame Diffusion (LFD), which constructs node-specific frames and benefits from additional alignment constraints; and Invariant Frame Diffusion (IFD), which relies on pre-canonicalized invariant representations. To enhance expressivity, we further utilize EdgeDiT, a Diffusion Transformer with edge-aware attention. On the QM9 dataset, GFD with EdgeDiT achieves state-of-the-art performance, with a test NLL of -137.97 at standard scale and -141.85 at double scale, alongside atom stability of 98.98%, and molecular stability of 90.51%. These results surpass all equivariant baselines while maintaining high validity and uniqueness and nearly 2x faster sampling compared to EDM. Altogether, our study establishes frame-based diffusion as a scalable, flexible, and physically grounded paradigm for molecular generation, highlighting the critical role of global structure preservation.</li>
<li><strong>摘要：</strong>分子发电的最新方法面临权衡：它们要么强制与昂贵的体系结构实施严格的均衡性，要么放松其以获得可伸缩性和灵活性。我们提出了一个基于框架的扩散范式，该范式可以实现确定性E（3） - 等值时，同时将对称性处理与主链分解对称性处理。在此范式的基础上，我们研究了三种变体：全局框架扩散（GFD），该变体分配了共享的分子框架；局部框架扩散（LFD），该扩散构建节点特异性帧和受益的额外对齐约束；和不变的帧扩散（IFD），依赖于预先的不变表示。为了提高表达性，我们进一步利用了EdgedIt，这是一种具有边缘感知注意力的扩散变压器。在QM9数据集中，具有边缘的GFD达到了最先进的性能，测试NLL的标准尺度为-137.97，双尺度为-141.85，原子稳定性为98.98％，分子稳定性为90.51％。与EDM相比，这些结果超过了所有均衡基线，同时保持高有效性和唯一性和近2倍的采样速度。总的来说，我们的研究将基于框架的扩散确立为分子产生的可扩展，灵活和物理扎根的范式，突出了全球结构保存的关键作用。</li>
</ul>

<h3>Title: Synthesizing Artifact Dataset for Pixel-level Detection</h3>
<ul>
<li><strong>Authors: </strong>Dennis Menn, Feng Liang, Diana Marculescu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19589">https://arxiv.org/abs/2509.19589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19589">https://arxiv.org/pdf/2509.19589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19589]] Synthesizing Artifact Dataset for Pixel-level Detection(https://arxiv.org/abs/2509.19589)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Artifact detectors have been shown to enhance the performance of image-generative models by serving as reward models during fine-tuning. These detectors enable the generative model to improve overall output fidelity and aesthetics. However, training the artifact detector requires expensive pixel-level human annotations that specify the artifact regions. The lack of annotated data limits the performance of the artifact detector. A naive pseudo-labeling approach-training a weak detector and using it to annotate unlabeled images-suffers from noisy labels, resulting in poor performance. To address this, we propose an artifact corruption pipeline that automatically injects artifacts into clean, high-quality synthetic images on a predetermined region, thereby producing pixel-level annotations without manual labeling. The proposed method enables training of an artifact detector that achieves performance improvements of 13.2% for ConvNeXt and 3.7% for Swin-T, as verified on human-labeled data, compared to baseline approaches. This work represents an initial step toward scalable pixel-level artifact annotation datasets that integrate world knowledge into artifact detection.</li>
<li><strong>摘要：</strong>已显示伪影检测器通过在微调过程中充当奖励模型来增强图像产生模型的性能。这些探测器使生成模型能够改善整体产出保真度和美学。但是，培训伪影检测器需要昂贵的像素级人类注释来指定工件区域。缺乏注释数据限制了伪影检测器的性能。一种幼稚的伪标记的方法训练弱检测器，并使用它注释噪音标签的未标记的图像挡板，导致性能差。为了解决这个问题，我们提出了一个工件腐败管道，该管道会自动在预定的区域上自动将伪像的人工制品注入干净，高质量的合成图像中，从而在没有手动标记的情况下产生像素级注释。提出的方法可以培训伪造检测器，该探测器与基线方法相比，在人体标记的数据上验证了Convnext的绩效提高13.2％，Swin-T的性能提高了3.7％。这项工作代表了朝着可扩展的像素级伪像注释数据集迈出的第一步，该数据集将世界知识整合到人工制品检测中。</li>
</ul>

<h3>Title: TIMED: Adversarial and Autoregressive Refinement of Diffusion-Based Time Series Generation</h3>
<ul>
<li><strong>Authors: </strong>MohammadReza EskandariNasab, Shah Muhammad Hamdi, Soukaina Filali Boubrahimi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19638">https://arxiv.org/abs/2509.19638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19638">https://arxiv.org/pdf/2509.19638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19638]] TIMED: Adversarial and Autoregressive Refinement of Diffusion-Based Time Series Generation(https://arxiv.org/abs/2509.19638)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generating high-quality synthetic time series is a fundamental yet challenging task across domains such as forecasting and anomaly detection, where real data can be scarce, noisy, or costly to collect. Unlike static data generation, synthesizing time series requires modeling both the marginal distribution of observations and the conditional temporal dependencies that govern sequential dynamics. We propose TIMED, a unified generative framework that integrates a denoising diffusion probabilistic model (DDPM) to capture global structure via a forward-reverse diffusion process, a supervisor network trained with teacher forcing to learn autoregressive dependencies through next-step prediction, and a Wasserstein critic that provides adversarial feedback to ensure temporal smoothness and fidelity. To further align the real and synthetic distributions in feature space, TIMED incorporates a Maximum Mean Discrepancy (MMD) loss, promoting both diversity and sample quality. All components are built using masked attention architectures optimized for sequence modeling and are trained jointly to effectively capture both unconditional and conditional aspects of time series data. Experimental results across diverse multivariate time series benchmarks demonstrate that TIMED generates more realistic and temporally coherent sequences than state-of-the-art generative models.</li>
<li><strong>摘要：</strong>产生高质量的合成时间序列是跨预测和异常检测的根本但具有挑战性的任务，在这些领域中，实际数据可能稀缺，嘈杂或收集成本高昂。与静态数据生成不同，合成时间序列需要对观测值的边际分布和控制顺序动力学的条件时间依赖性进行建模。我们提出了定时式，这是一个统一的生成框架，该框架整合了通过前向反向扩散过程捕获全球结构的denoing扩散概率模型（DDPM），这是一个受过教师强迫通过下一步的预测来学习自动依赖依赖性的主管网络，并提供了一种批评，以确保较平稳性。为了进一步将特征空间中的真实和合成分布对齐，定时符合最大平均差异（MMD）损失，从而促进了多样性和样本质量。所有组件均使用针对序列建模优化的蒙版注意体系结构构建，并经过培训，以有效地捕获时间序列数据的无条件和条件方面。各种多元时间序列基准之间的实验结果表明，与最先进的生成模型相比，时机会产生更现实和时间相干的序列。</li>
</ul>

<h3>Title: Deep Learning for Clouds and Cloud Shadow Segmentation in Methane Satellite and Airborne Imaging Spectroscopy</h3>
<ul>
<li><strong>Authors: </strong>Manuel Perez-Carrasco, Maya Nasr, Sebastien Roche, Chris Chan Miller, Zhan Zhang, Core Francisco Park, Eleanor Walker, Cecilia Garraffo, Douglas Finkbeiner, Ritesh Gautam, Steven Wofsy</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19665">https://arxiv.org/abs/2509.19665</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19665">https://arxiv.org/pdf/2509.19665</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19665]] Deep Learning for Clouds and Cloud Shadow Segmentation in Methane Satellite and Airborne Imaging Spectroscopy(https://arxiv.org/abs/2509.19665)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Effective cloud and cloud shadow detection is a critical prerequisite for accurate retrieval of concentrations of atmospheric methane or other trace gases in hyperspectral remote sensing. This challenge is especially pertinent for MethaneSAT and for its airborne companion mission, MethaneAIR. In this study, we use machine learning methods to address the cloud and cloud shadow detection problem for sensors with these high spatial resolutions instruments. Cloud and cloud shadows in remote sensing data need to be effectively screened out as they bias methane retrievals in remote sensing imagery and impact the quantification of emissions. We deploy and evaluate conventional techniques including Iterative Logistic Regression (ILR) and Multilayer Perceptron (MLP), with advanced deep learning architectures, namely UNet and a Spectral Channel Attention Network (SCAN) method. Our results show that conventional methods struggle with spatial coherence and boundary definition, affecting the detection of clouds and cloud shadows. Deep learning models substantially improve detection quality: UNet performs best in preserving spatial structure, while SCAN excels at capturing fine boundary details. Notably, SCAN surpasses UNet on MethaneSAT data, underscoring the benefits of incorporating spectral attention for satellite specific features. This in depth assessment of various disparate machine learning techniques demonstrates the strengths and effectiveness of advanced deep learning architectures in providing robust, scalable solutions for clouds and cloud shadow screening towards enhancing methane emission quantification capacity of existing and next generation hyperspectral missions. Our data and code is publicly available at this https URL</li>
<li><strong>摘要：</strong>有效的云和云阴影检测是准确检索高度光谱遥感中大气甲烷或其他痕量气体浓度的关键先决条件。这一挑战尤其与Methanesat及其机载同伴Madeair有关。在这项研究中，我们使用机器学习方法来解决使用这些高空间分辨率仪器的传感器的云阴影检测问题。遥感数据中的云和云阴影需要有效筛选，因为它们在遥感图像中偏向甲烷检索并影响排放的量化。我们部署和评估传统技术，包括迭代逻辑回归（ILR）和多层感知器（MLP），具有先进的深度学习体系结构，即UNET和光谱通道注意网络（SCAN）方法。我们的结果表明，常规方法与空间连贯性和边界定义抗争，影响云和云阴影的检测。深度学习模型基本上提高了检测质量：UNET在保持空间结构方面表现最好，而扫描却擅长捕获细节细节。值得注意的是，扫描超过了甲壳剂数据的UNET，强调了将光谱关注纳入卫星特定特征的好处。对各种不同机器学习技术的深入评估表明，先进深度学习体系结构在为云和云阴影筛选提供强大的可扩展解决方案方面具有优势和有效性，以增强现有和下一代高光谱任务的甲烷排放量化能力。我们的数据和代码可在此HTTPS URL上公开获取</li>
</ul>

<h3>Title: Robust RGB-T Tracking via Learnable Visual Fourier Prompt Fine-tuning and Modality Fusion Prompt Generation</h3>
<ul>
<li><strong>Authors: </strong>Hongtao Yang, Bineng Zhong, Qihua Liang, Zhiruo Zhu, Yaozong Zheng, Ning Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19733">https://arxiv.org/abs/2509.19733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19733">https://arxiv.org/pdf/2509.19733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19733]] Robust RGB-T Tracking via Learnable Visual Fourier Prompt Fine-tuning and Modality Fusion Prompt Generation(https://arxiv.org/abs/2509.19733)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recently, visual prompt tuning is introduced to RGB-Thermal (RGB-T) tracking as a parameter-efficient finetuning (PEFT) method. However, these PEFT-based RGB-T tracking methods typically rely solely on spatial domain information as prompts for feature extraction. As a result, they often fail to achieve optimal performance by overlooking the crucial role of frequency-domain information in prompt learning. To address this issue, we propose an efficient Visual Fourier Prompt Tracking (named VFPTrack) method to learn modality-related prompts via Fast Fourier Transform (FFT). Our method consists of symmetric feature extraction encoder with shared parameters, visual fourier prompts, and Modality Fusion Prompt Generator that generates bidirectional interaction prompts through multi-modal feature fusion. Specifically, we first use a frozen feature extraction encoder to extract RGB and thermal infrared (TIR) modality features. Then, we combine the visual prompts in the spatial domain with the frequency domain prompts obtained from the FFT, which allows for the full extraction and understanding of modality features from different domain information. Finally, unlike previous fusion methods, the modality fusion prompt generation module we use combines features from different modalities to generate a fused modality prompt. This modality prompt is interacted with each individual modality to fully enable feature interaction across different modalities. Extensive experiments conducted on three popular RGB-T tracking benchmarks show that our method demonstrates outstanding performance.</li>
<li><strong>摘要：</strong>最近，将视觉提示调谐引入RGB-thermal（RGB-T）跟踪，作为参数效率的芬特（PEFT）方法。但是，这些基于PEFT的RGB-T跟踪方法通常仅依赖于空间域信息作为特征提取的提示。结果，他们通常无法通过忽略频率域信息在及时学习中的关键作用来实现最佳性能。为了解决此问题，我们提出了一种有效的视觉傅立叶提示跟踪（命名为VFPTRACK）方法，以通过快速傅立叶变换（FFT）学习与模态相关的提示。我们的方法由具有共享参数，可视傅立叶提示和模态融合提示生成器的对称特征提取编码器组成，该发电机通过多模式特征融合生成双向交互提示。具体而言，我们首先使用冷冻特征提取编码器来提取RGB和热红外（TIR）模态特征。然后，我们将空间域中的视觉提示与从FFT获得的频域提示相结合，从而使来自不同域信息的模态特征的完整提取和理解。最后，与以前的融合方法不同，模态融合提示生成模块我们使用不同模态的功能来生成融合的模态提示。该模态提示与每种单独的模态进行交互，以完全启用不同模态的特征交互。在三个流行的RGB-T跟踪基准上进行的广泛实验表明，我们的方法表现出出色的性能。</li>
</ul>

<h3>Title: Rectified Decoupled Dataset Distillation: A Closer Look for Fair and Comprehensive Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Xinhao Zhong, Shuoyang Sun, Xulin Gu, Chenyang Zhu, Bin Chen, Yaowei Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19743">https://arxiv.org/abs/2509.19743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19743">https://arxiv.org/pdf/2509.19743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19743]] Rectified Decoupled Dataset Distillation: A Closer Look for Fair and Comprehensive Evaluation(https://arxiv.org/abs/2509.19743)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Dataset distillation aims to generate compact synthetic datasets that enable models trained on them to achieve performance comparable to those trained on full real datasets, while substantially reducing storage and computational costs. Early bi-level optimization methods (e.g., MTT) have shown promising results on small-scale datasets, but their scalability is limited by high computational overhead. To address this limitation, recent decoupled dataset distillation methods (e.g., SRe$^2$L) separate the teacher model pre-training from the synthetic data generation process. These methods also introduce random data augmentation and epoch-wise soft labels during the post-evaluation phase to improve performance and generalization. However, existing decoupled distillation methods suffer from inconsistent post-evaluation protocols, which hinders progress in the field. In this work, we propose Rectified Decoupled Dataset Distillation (RD$^3$), and systematically investigate how different post-evaluation settings affect test accuracy. We further examine whether the reported performance differences across existing methods reflect true methodological advances or stem from discrepancies in evaluation procedures. Our analysis reveals that much of the performance variation can be attributed to inconsistent evaluation rather than differences in the intrinsic quality of the synthetic data. In addition, we identify general strategies that improve the effectiveness of distilled datasets across settings. By establishing a standardized benchmark and rigorous evaluation protocol, RD$^3$ provides a foundation for fair and reproducible comparisons in future dataset distillation research.</li>
<li><strong>摘要：</strong>数据集蒸馏旨在生成紧凑的合成数据集，使经过训练的模型能够实现与在完整真实数据集中训练的人相当的性能，同时大大降低了存储和计算成本。早期的双层优化方法（例如MTT）在小规模数据集上显示出令人鼓舞的结果，但是它们的可扩展性受到高计算开销的限制。为了解决这一限制，最新的解耦数据集蒸馏方法（例如SRE $^2 $ L）将教师模型预培训与合成数据生成过程分开。这些方法还在评估阶段引入随机数据增强和时期的软标签，以提高性能和概括。但是，现有的解耦蒸馏方法遭受了不一致的评估方案，这阻碍了该领域的进展。在这项工作中，我们提出了校正的解耦数据集蒸馏（RD $^3 $），并系统地研究了不同的评估后设置如何影响测试准确性。我们进一步研究了现有方法的报告绩效差异是否反映了真正的方法论进步还是源于评估程序中的差异。我们的分析表明，许多性能变化都可以归因于不一致的评估，而不是合成数据的内在质量的差异。此外，我们确定了一般策略，以提高跨环境的蒸馏数据集的有效性。通过建立标准化的基准和严格的评估协议，RD $^3 $为将来的数据集蒸馏研究提供了公平且可重复的比较的基础。</li>
</ul>

<h3>Title: Talking Head Generation via AU-Guided Landmark Prediction</h3>
<ul>
<li><strong>Authors: </strong>Shao-Yu Chang, Jingyi Xu, Hieu Le, Dimitris Samaras</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19749">https://arxiv.org/abs/2509.19749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19749">https://arxiv.org/pdf/2509.19749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19749]] Talking Head Generation via AU-Guided Landmark Prediction(https://arxiv.org/abs/2509.19749)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We propose a two-stage framework for audio-driven talking head generation with fine-grained expression control via facial Action Units (AUs). Unlike prior methods relying on emotion labels or implicit AU conditioning, our model explicitly maps AUs to 2D facial landmarks, enabling physically grounded, per-frame expression control. In the first stage, a variational motion generator predicts temporally coherent landmark sequences from audio and AU intensities. In the second stage, a diffusion-based synthesizer generates realistic, lip-synced videos conditioned on these landmarks and a reference image. This separation of motion and appearance improves expression accuracy, temporal stability, and visual realism. Experiments on the MEAD dataset show that our method outperforms state-of-the-art baselines across multiple metrics, demonstrating the effectiveness of explicit AU-to-landmark modeling for expressive talking head generation.</li>
<li><strong>摘要：</strong>我们为通过面部动作单元（AUS）提出了一个两阶段的框架，用于通过面部动作单元（AUS）进行细粒度的表达控制。与依靠情绪标签或隐式AU调节的先前方法不同，我们的模型将AUS明确映射到2D面部标志，从而实现了物理地接地的人均表达控制。在第一阶段，变异运动发生器可以预测音频和AU强度的时间连贯的地标序列。在第二阶段，基于扩散的合成器会在这些地标和参考图像上生成逼真的，唇部同步的视频。运动和外观的这种分离提高了表达精度，时间稳定性和视觉现实主义。 Mead数据集的实验表明，我们的方法在多个指标上优于最先进的基线，这证明了显式AU至地表建模对于表达性交谈的头部生成的有效性。</li>
</ul>

<h3>Title: PPGFlowECG: Latent Rectified Flow with Cross-Modal Encoding for PPG-Guided ECG Generation and Cardiovascular Disease Detection</h3>
<ul>
<li><strong>Authors: </strong>Xiaocheng Fang, Jiarui Jin, Haoyu Wang, Che Liu, Jieyi Cai, Guangkun Nie, Jun Li, Hongyan Li, Shenda Hong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19774">https://arxiv.org/abs/2509.19774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19774">https://arxiv.org/pdf/2509.19774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19774]] PPGFlowECG: Latent Rectified Flow with Cross-Modal Encoding for PPG-Guided ECG Generation and Cardiovascular Disease Detection(https://arxiv.org/abs/2509.19774)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In clinical practice, electrocardiography (ECG) remains the gold standard for cardiac monitoring, providing crucial insights for diagnosing a wide range of cardiovascular diseases (CVDs). However, its reliance on specialized equipment and trained personnel limits feasibility for continuous routine monitoring. Photoplethysmography (PPG) offers accessible, continuous monitoring but lacks definitive electrophysiological information, preventing conclusive diagnosis. Generative models present a promising approach to translate PPG into clinically valuable ECG signals, yet current methods face substantial challenges, including the misalignment of physiological semantics in generative models and the complexity of modeling in high-dimensional signals. To this end, we propose PPGFlowECG, a two-stage framework that aligns PPG and ECG in a shared latent space via the CardioAlign Encoder and employs latent rectified flow to generate ECGs with high fidelity and interpretability. To the best of our knowledge, this is the first study to experiment on MCMED, a newly released clinical-grade dataset comprising over 10 million paired PPG-ECG samples from more than 118,000 emergency department visits with expert-labeled cardiovascular disease annotations. Results demonstrate the effectiveness of our method for PPG-to-ECG translation and cardiovascular disease detection. Moreover, cardiologist-led evaluations confirm that the synthesized ECGs achieve high fidelity and improve diagnostic reliability, underscoring our method's potential for real-world cardiovascular screening.</li>
<li><strong>摘要：</strong>在临床实践中，心电图（ECG）仍然是心脏监测的黄金标准，为诊断多种心血管疾病（CVD）提供了关键见解。但是，它依赖专业设备和受过训练的人员限制了连续常规监控的可行性。 Photoplethysmmography（PPG）提供了可访问的连续监测，但缺乏明确的电生理信息，从而阻止了最终的诊断。生成模型提出了一种有前途的方法，可以将PPG转化为临床上有价值的ECG信号，但是当前的方法面临着重大挑战，包括生成模型中生理语义的错位以及高维信号中建模的复杂性。为此，我们提出了PPGFlowEcg，这是一个两阶段的框架，通过心脏分解编码器将PPG和ECG在共享潜在空间中对齐，并采用潜在的矫正流程来生成具有高忠诚度和可解释性的ECG。据我们所知，这是对MCMED进行实验的首次研究，这是一个新发布的临床级数据集，其中包括超过118,000个急诊室访问超过1000万个配对PPG-ECG样品，并具有专家标记的心血管疾病注释。结果证明了我们方法对PPG至ECG翻译和心血管疾病检测的有效性。此外，心脏病专家主导的评估证实，合成的ECG可实现高保真度并提高诊断可靠性，从而强调了我们方法的现实世界心血管筛查的潜力。</li>
</ul>

<h3>Title: StrCGAN: A Generative Framework for Stellar Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Shantanusinh Parmar</a></li>
<li><strong>Subjects: </strong>cs.CV, astro-ph.IM, astro-ph.SR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19805">https://arxiv.org/abs/2509.19805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19805">https://arxiv.org/pdf/2509.19805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19805]] StrCGAN: A Generative Framework for Stellar Image Restoration(https://arxiv.org/abs/2509.19805)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generative</a></li>
<li><strong>Abstract: </strong>We introduce StrCGAN (Stellar Cyclic GAN), a generative model designed to enhance low-resolution astrophotography images. Our goal is to reconstruct high-fidelity ground truth-like representations of celestial objects, a task that is challenging due to the limited resolution and quality of small-telescope observations such as the MobilTelesco dataset. Traditional models such as CycleGAN provide a foundation for image-to-image translation but are restricted to 2D mappings and often distort the morphology of stars and galaxies. To overcome these limitations, we extend the CycleGAN framework with three key innovations: 3D convolutional layers to capture volumetric spatial correlations, multi-spectral fusion to align optical and near-infrared (NIR) domains, and astrophysical regularization modules to preserve stellar morphology. Ground-truth references from multi-mission all-sky surveys spanning optical to NIR guide the training process, ensuring that reconstructions remain consistent across spectral bands. Together, these components allow StrCGAN to generate reconstructions that are not only visually sharper but also physically consistent, outperforming standard GAN models in the task of astrophysical image enhancement.</li>
<li><strong>摘要：</strong>我们介绍了Strcgan（Stellar Cyclic Gan），这是一种生成模型，旨在增强低分辨率的天体摄影图像。我们的目标是重建天体对象的高保真地面真相般的表示，这一任务由于小型观测值（例如Mobiltelesco数据集）的分辨率和质量有限而具有挑战性。诸如Cyclegan之类的传统模型为图像到图像翻译提供了基础，但仅限于2D映射，并且经常扭曲恒星和星系的形态。为了克服这些局限性，我们通过三个关键创新扩展了自行车框架：3D卷积层以捕获体积的空间相关性，多光谱融合到对齐光学和近边缘（NIR）域以及天体物理正则化模块，以保留恒星形态。从光学到NIR指导训练过程的多损性的全天空调查中的地面真实参考，以确保重建在光谱频段之间保持一致。这些组件一起允许Strcgan生成重建，这些重建不仅在视觉上更加锐利，而且在天体物理图像增强的任务下在物理上一致，超过标准的GAN模型。</li>
</ul>

<h3>Title: Oversampling and Downsampling with Core-Boundary Awareness: A Data Quality-Driven Approach</h3>
<ul>
<li><strong>Authors: </strong>Samir Brahim Belhaouari, Yunis Carreon Kahalan, Humaira Shaffique, Ismael Belhaouari, Ashhadul Islam</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19856">https://arxiv.org/abs/2509.19856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19856">https://arxiv.org/pdf/2509.19856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19856]] Oversampling and Downsampling with Core-Boundary Awareness: A Data Quality-Driven Approach(https://arxiv.org/abs/2509.19856)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The effectiveness of machine learning models, particularly in unbalanced classification tasks, is often hindered by the failure to differentiate between critical instances near the decision boundary and redundant samples concentrated in the core of the data distribution. In this paper, we propose a method to systematically identify and differentiate between these two types of data. Through extensive experiments on multiple benchmark datasets, we show that the boundary data oversampling method improves the F1 score by up to 10\% on 96\% of the datasets, whereas our core-aware reduction method compresses datasets up to 90\% while preserving their accuracy, making it 10 times more powerful than the original dataset. Beyond imbalanced classification, our method has broader implications for efficient model training, particularly in computationally expensive domains such as Large Language Model (LLM) training. By prioritizing high-quality, decision-relevant data, our approach can be extended to text, multimodal, and self-supervised learning scenarios, offering a pathway to faster convergence, improved generalization, and significant computational savings. This work paves the way for future research in data-efficient learning, where intelligent sampling replaces brute-force expansion, driving the next generation of AI advancements. Our code is available as a Python package at this https URL .</li>
<li><strong>摘要：</strong>机器学习模型的有效性，尤其是在不平衡的分类任务中，通常会因未能区分决策界限附近的关键实例和集中在数据分布核心的冗余样本之间而受到阻碍。在本文中，我们提出了一种系统识别和区分这两种类型数据的方法。通过在多个基准数据集上进行的大量实验，我们表明，边界数据过采样方法在96 \％的数据集上提高了最高10 \％的F1分数，而我们的核心意识减少方法压缩了90 \％的核心降低方法，最多可保留其准确性，同时使其比原始数据集中更强大。除了分类不平衡，我们的方法还对有效的模型培训具有更广泛的影响，尤其是在计算昂贵的域（例如大语言模型（LLM）培训）中。通过优先考虑高质量，与决策相关的数据，我们的方法可以扩展到文本，多模式和自我监督的学习场景，提供更快的融合，改善概括和大量计算节省的途径。这项工作为未来的数据学习学习铺平了道路，在该学习中，智能抽样取代了蛮力的扩展，推动了下一代AI的进步。我们的代码可在此HTTPS URL上作为Python软件包。</li>
</ul>

<h3>Title: Latent Iterative Refinement Flow: A Geometric-Constrained Approach for Few-Shot Generation</h3>
<ul>
<li><strong>Authors: </strong>Songtao Li, Zhenyu Liao, Tianqi Hou, Ting Gao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19903">https://arxiv.org/abs/2509.19903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19903">https://arxiv.org/pdf/2509.19903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19903]] Latent Iterative Refinement Flow: A Geometric-Constrained Approach for Few-Shot Generation(https://arxiv.org/abs/2509.19903)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Few-shot generation, the synthesis of high-quality and diverse samples from limited training data, remains a significant challenge in generative modeling. Existing methods trained from scratch often fail to overcome overfitting and mode collapse, and fine-tuning large models can inherit biases while neglecting the crucial geometric structure of the latent space. To address these limitations, we introduce Latent Iterative Refinement Flow (LIRF), a novel approach that reframes few-shot generation as the progressive densification of geometrically structured manifold. LIRF establishes a stable latent space using an autoencoder trained with our novel \textbf{manifold-preservation loss} $L_{\text{manifold}}$. This loss ensures that the latent space maintains the geometric and semantic correspondence of the input data. Building on this, we propose an iterative generate-correct-augment cycle. Within this cycle, candidate samples are refined by a geometric \textbf{correction operator}, a provably contractive mapping that pulls samples toward the data manifold while preserving diversity. We also provide the \textbf{Convergence Theorem} demonstrating a predictable decrease in Hausdorff distance between generated and true data manifold. We also demonstrate the framework's scalability by generating coherent, high-resolution images on AFHQ-Cat. Ablation studies confirm that both the manifold-preserving latent space and the contractive correction mechanism are critical components of this success. Ultimately, LIRF provides a solution for data-scarce generative modeling that is not only theoretically grounded but also highly effective in practice.</li>
<li><strong>摘要：</strong>在有限的训练数据中，几乎没有发电的生成，高质量的样本和多种样本，仍然是生成建模的重大挑战。从头开始训练的现有方法通常无法克服过度拟合和模式崩溃，并且微调大型模型可以继承偏见，同时忽略潜在空间的关键几何结构。为了解决这些局限性，我们引入了潜在的迭代精炼流（LIRF），这是一种新颖的方法，它将很少的发射产生作为几何结构歧管的逐步致密化。 LIRF使用我们的小说\ textbf {comperold-Prestervert损失} $ l _ {\ text {comperold}} $训练的自动编码器建立了稳定的潜在空间。这种损失确保潜在空间维持输入数据的几何和语义对应关系。在此基础上，我们提出了一个迭代生成校正仪的周期。在此周期内，候选样本通过几何\ textbf {校正操作员}进行了完善，这是一个可证明的关同映射，可将样品朝着数据歧管划分，同时保留多样性。我们还提供了\ textbf {Convergence定理}，该\ textbf {Convergence Thesorem}证明了生成的数据歧管和真实数据歧管之间的Hausdorff距离可预测的下降。我们还通过在AFHQ-CAT上生成相干的高分辨率图像来证明该框架的可扩展性。消融研究证实，具有歧管的潜在空间和合同校正机制都是这种成功的关键组成部分。最终，LIRF为数据制品生成建模提供了一种解决方案，该模型不仅是理论上扎根的，而且在实践中也非常有效。</li>
</ul>

<h3>Title: TABFAIRGDT: A Fast Fair Tabular Data Generator using Autoregressive Decision Trees</h3>
<ul>
<li><strong>Authors: </strong>Emmanouil Panagiotou, Benoît Ronval, Arjun Roy, Ludwig Bothmann, Bernd Bischl, Siegfried Nijssen, Eirini Ntoutsi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19927">https://arxiv.org/abs/2509.19927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19927">https://arxiv.org/pdf/2509.19927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19927]] TABFAIRGDT: A Fast Fair Tabular Data Generator using Autoregressive Decision Trees(https://arxiv.org/abs/2509.19927)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Ensuring fairness in machine learning remains a significant challenge, as models often inherit biases from their training data. Generative models have recently emerged as a promising approach to mitigate bias at the data level while preserving utility. However, many rely on deep architectures, despite evidence that simpler models can be highly effective for tabular data. In this work, we introduce TABFAIRGDT, a novel method for generating fair synthetic tabular data using autoregressive decision trees. To enforce fairness, we propose a soft leaf resampling technique that adjusts decision tree outputs to reduce bias while preserving predictive performance. Our approach is non-parametric, effectively capturing complex relationships between mixed feature types, without relying on assumptions about the underlying data distributions. We evaluate TABFAIRGDT on benchmark fairness datasets and demonstrate that it outperforms state-of-the-art (SOTA) deep generative models, achieving better fairness-utility trade-off for downstream tasks, as well as higher synthetic data quality. Moreover, our method is lightweight, highly efficient, and CPU-compatible, requiring no data pre-processing. Remarkably, TABFAIRGDT achieves a 72% average speedup over the fastest SOTA baseline across various dataset sizes, and can generate fair synthetic data for medium-sized datasets (10 features, 10K samples) in just one second on a standard CPU, making it an ideal solution for real-world fairness-sensitive applications.</li>
<li><strong>摘要：</strong>确保机器学习中的公平性仍然是一个重大挑战，因为模型通常会从其培训数据中继承偏见。最近，生成模型已成为一种有前途的方法，可以在保存实用程序的同时减轻数据级别的偏差。但是，尽管有证据表明更简单的模型对于表格数据可能非常有效，但许多人依靠深层体系结构。在这项工作中，我们介绍了Tabfairgdt，这是一种使用自回归决策树生成公平合成表格数据的新方法。为了实施公平性，我们提出了一种软叶重采样技术，该技术调整决策树的输出以减少偏见，同时保持预测性能。我们的方法是非参数，有效地捕获混合特征类型之间的复杂关系，而无需依赖于基础数据分布的假设。我们在基准公平数据集上评估了Tabfairgdt，并证明它的表现要优于最先进的（SOTA）深层生成模型，从而实现了下游任务的更好的公平耐用权衡以及更高的合成数据质量。此外，我们的方法是轻巧，高效且兼容CPU，不需要数据预处理。值得注意的是，Tabfairgdt在各种数据集尺寸的最快SOTA基线上达到了72％的平均速度，并且可以在仅在标准CPU上的一秒钟内生成中型数据集（10个功能，10K样品）的公平合成数据，从而使其成为现实世界中公平的公平应用程序的理想解决方案。</li>
</ul>

<h3>Title: GS-RoadPatching: Inpainting Gaussians via 3D Searching and Placing for Driving Scenes</h3>
<ul>
<li><strong>Authors: </strong>Guo Chen, Jiarun Liu, Sicong Du, Chenming Wu, Deqi Li, Shi-Sheng Huang, Guofeng Zhang, Sheng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19937">https://arxiv.org/abs/2509.19937</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19937">https://arxiv.org/pdf/2509.19937</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19937]] GS-RoadPatching: Inpainting Gaussians via 3D Searching and Placing for Driving Scenes(https://arxiv.org/abs/2509.19937)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper presents GS-RoadPatching, an inpainting method for driving scene completion by referring to completely reconstructed regions, which are represented by 3D Gaussian Splatting (3DGS). Unlike existing 3DGS inpainting methods that perform generative completion relying on 2D perspective-view-based diffusion or GAN models to predict limited appearance or depth cues for missing regions, our approach enables substitutional scene inpainting and editing directly through the 3DGS modality, extricating it from requiring spatial-temporal consistency of 2D cross-modals and eliminating the need for time-intensive retraining of Gaussians. Our key insight is that the highly repetitive patterns in driving scenes often share multi-modal similarities within the implicit 3DGS feature space and are particularly suitable for structural matching to enable effective 3DGS-based substitutional inpainting. Practically, we construct feature-embedded 3DGS scenes to incorporate a patch measurement method for abstracting local context at different scales and, subsequently, propose a structural search method to find candidate patches in 3D space effectively. Finally, we propose a simple yet effective substitution-and-fusion optimization for better visual harmony. We conduct extensive experiments on multiple publicly available datasets to demonstrate the effectiveness and efficiency of our proposed method in driving scenes, and the results validate that our method achieves state-of-the-art performance compared to the baseline methods in terms of both quality and interoperability. Additional experiments in general scenes also demonstrate the applicability of the proposed 3D inpainting strategy. The project page and code are available at: this https URL</li>
<li><strong>摘要：</strong>本文介绍了GS-RoadPatching，这是一种通过参考完全重建的区域来完成驾驶场景完成的介绍方法，该区域由3D高斯分裂（3DGS）表示。 Unlike existing 3DGS inpainting methods that perform generative completion relying on 2D perspective-view-based diffusion or GAN models to predict limited appearance or depth cues for missing regions, our approach enables substitutional scene inpainting and editing directly through the 3DGS modality, extricating it from requiring spatial-temporal consistency of 2D cross-modals and eliminating the need for time-intensive retraining of Gaussians.我们的主要见解是，在驱动场景中，高度重复的模式通常在隐式3DGS中具有多模式相似性，并且特别适合结构匹配以实现有效的基于3DGS的替代涂层。实际上，我们构建了特征包裹的3DGS场景，以合并一种补丁测量方法，用于在不同尺度上抽象本地上下文，然后提出了一种结构搜索方法，以有效地找到3D空间中的候选贴片。最后，我们提出了一种简单而有效的替代和融合优化，以更好地视觉和谐。我们对多个公开可用数据集进行了广泛的实验，以证明我们提出的方法在驱动场景中的有效性和效率，结果证明了我们的方法在质量和互操作性方面与基线方法相比，我们的方法可以实现最先进的性能。一般场景中的其他实验还证明了所提出的3D授课策略的适用性。项目页面和代码可用：此HTTPS URL</li>
</ul>

<h3>Title: When Words Can't Capture It All: Towards Video-Based User Complaint Text Generation with Multimodal Video Complaint Dataset</h3>
<ul>
<li><strong>Authors: </strong>Sarmistha Das, R E Zera Marveen Lyngkhoi, Kirtan Jain, Vinayak Goyal, Sriparna Saha, Manish Gupta</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19952">https://arxiv.org/abs/2509.19952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19952">https://arxiv.org/pdf/2509.19952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19952]] When Words Can't Capture It All: Towards Video-Based User Complaint Text Generation with Multimodal Video Complaint Dataset(https://arxiv.org/abs/2509.19952)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While there exists a lot of work on explainable complaint mining, articulating user concerns through text or video remains a significant challenge, often leaving issues unresolved. Users frequently struggle to express their complaints clearly in text but can easily upload videos depicting product defects (e.g., vague text such as `worst product' paired with a 5-second video depicting a broken headphone with the right earcup). This paper formulates a new task in the field of complaint mining to aid the common users' need to write an expressive complaint, which is Complaint Description from Videos (CoD-V) (e.g., to help the above user articulate her complaint about the defective right earcup). To this end, we introduce ComVID, a video complaint dataset containing 1,175 complaint videos and the corresponding descriptions, also annotated with the emotional state of the complainer. Additionally, we present a new complaint retention (CR) evaluation metric that discriminates the proposed (CoD-V) task against standard video summary generation and description tasks. To strengthen this initiative, we introduce a multimodal Retrieval-Augmented Generation (RAG) embedded VideoLLaMA2-7b model, designed to generate complaints while accounting for the user's emotional state. We conduct a comprehensive evaluation of several Video Language Models on several tasks (pre-trained and fine-tuned versions) with a range of established evaluation metrics, including METEOR, perplexity, and the Coleman-Liau readability score, among others. Our study lays the foundation for a new research direction to provide a platform for users to express complaints through video. Dataset and resources are available at: this https URL.</li>
<li><strong>摘要：</strong>尽管在可解释的投诉挖掘方面存在大量工作，但通过文本或视频表达用户担忧仍然是一个重大挑战，通常尚未解决问题。用户经常在文本中清楚地表达自己的投诉，但可以轻松地上传描绘产品缺陷的视频（例如，含糊的文字，例如“最差的产品”，并配上5秒的视频，描绘了带有右耳塞的耳机破裂的视频）。本文在挖掘领域制定了一项新任务，以帮助普通用户撰写表达性投诉，这是视频（COD-V）的投诉说明（例如，帮助上述用户表达她对右责任有缺陷的投诉）。为此，我们介绍了COMVID，这是一个视频投诉数据集，其中包含1,175个投诉视频和相应的描述，也以投诉者的情绪状态注释。此外，我们提出了一个新的投诉保留（CR）评估度量标准，该指标将提出的（COD-V）任务与标准视频摘要生成和说明任务区分开来。为了加强这项计划，我们引入了多式联运检索型发电（RAG）嵌入式Videolama2-7B模型，旨在在考虑用户的情绪状态时产生投诉。我们对几种视频语言模型进行了全面评估，这些模型对几个任务（预训练和微调版本）进行了一系列既定的评估指标，包括流星，困惑和Coleman-Liau可读性得分等。我们的研究奠定了一个新的研究方向的基础，为用户提供了通过视频表达投诉的平台。数据集和资源可用：此HTTPS URL。</li>
</ul>

<h3>Title: Learnable Sampler Distillation for Discrete Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Feiyang Fu, Tongxian Guo, Zhaoqiang Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19962">https://arxiv.org/abs/2509.19962</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19962">https://arxiv.org/pdf/2509.19962</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19962]] Learnable Sampler Distillation for Discrete Diffusion Models(https://arxiv.org/abs/2509.19962)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Discrete diffusion models (DDMs) have shown powerful generation ability for discrete data modalities like text and molecules. However, their practical application is hindered by inefficient sampling, requiring a large number of sampling steps. Accelerating DDMs by using larger step sizes typically introduces significant problems in generation quality, as it amplifies the impact of both the compounding decoding error due to factorized predictions and discretization error from numerical approximations, leading to a significant decrease in sampling quality. To address these challenges, we propose learnable sampler distillation (LSD), a novel approach to train fast and high-fidelity samplers for DDMs. LSD employs a distillation approach where a student sampler with a few steps learns to align its intermediate score trajectory with that of a high-quality teacher sampler with numerous steps. This alignment is achieved by optimizing learnable sampler coefficients that adaptively adjust sampling dynamics. Additionally, we further propose LSD+, which also learns time schedules that allocate steps non-uniformly. Experiments across text generation, image generation, and synthetic tasks demonstrate that our proposed approaches outperform existing samplers for DDMs, achieving substantially higher sampling quality with significantly fewer sampling steps. Our code is available at \href{this https URL}{this https URL}.</li>
<li><strong>摘要：</strong>离散扩散模型（DDMS）表现出具有强大的生成能力，可用于文本和分子等离散数据模式。但是，他们的实际应用受到效率低下的采样的阻碍，需要大量的采样步骤。通过使用较大步骤大小加速DDM，通常会引入发电质量的重大问题，因为它扩大了由于分解的预测和数值近似的离散化误差而导致的复合解码误差的影响，从而导致采样质量的显着下降。为了应对这些挑战，我们提出了可学习的采样器蒸馏（LSD），这是一种新颖的方法，用于训练DDMS快速和高保真采样器。 LSD采用了一种蒸馏方法，其中有几个步骤的学生采样器学会了将其中级得分轨迹与具有许多步骤的高质量老师采样器的轨迹保持一致。通过优化可自适应调整采样动力学的可学习采样器系数来实现此比对。此外，我们进一步提出了LSD+，该+还学习了时间表，这些时间表是不均匀分配步骤的时间表。跨文本生成，图像生成和合成任务的实验表明，我们提出的方法的表现优于DDM的现有采样器，实现了更高的采样质量，采样步骤较少。我们的代码可在\ href {此https url} {this HTTPS url}上获得。</li>
</ul>

<h3>Title: SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding</h3>
<ul>
<li><strong>Authors: </strong>Phyo Thet Yee, Dimitrios Kollias, Sudeepta Mishra, Abhinav Dhall</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19965">https://arxiv.org/abs/2509.19965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19965">https://arxiv.org/pdf/2509.19965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19965]] SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding(https://arxiv.org/abs/2509.19965)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Audio-driven talking face generation has received growing interest, particularly for applications requiring expressive and natural human-avatar interaction. However, most existing emotion-aware methods rely on a single modality (either audio or image) for emotion embedding, limiting their ability to capture nuanced affective cues. Additionally, most methods condition on a single reference image, restricting the model's ability to represent dynamic changes in actions or attributes across time. To address these issues, we introduce SynchroRaMa, a novel framework that integrates a multi-modal emotion embedding by combining emotional signals from text (via sentiment analysis) and audio (via speech-based emotion recognition and audio-derived valence-arousal features), enabling the generation of talking face videos with richer and more authentic emotional expressiveness and fidelity. To ensure natural head motion and accurate lip synchronization, SynchroRaMa includes an audio-to-motion (A2M) module that generates motion frames aligned with the input audio. Finally, SynchroRaMa incorporates scene descriptions generated by Large Language Model (LLM) as additional textual input, enabling it to capture dynamic actions and high-level semantic attributes. Conditioning the model on both visual and textual cues enhances temporal consistency and visual realism. Quantitative and qualitative experiments on benchmark datasets demonstrate that SynchroRaMa outperforms the state-of-the-art, achieving improvements in image quality, expression preservation, and motion realism. A user study further confirms that SynchroRaMa achieves higher subjective ratings than competing methods in overall naturalness, motion diversity, and video smoothness. Our project page is available at <this https URL.</li>
<li><strong>摘要：</strong>音频驱动的说话的面部发电引起了人们的兴趣越来越大，特别是对于需要表现力和自然人为互动的应用。但是，大多数现有的情绪感知方法都依赖于一种情感嵌入的单一模态（音频或图像），从而限制了它们捕获细微的情感线索的能力。此外，大多数方法在单个参考图像上条件条件，从而限制了模型代表跨时间的动态变化或属性的动态变化的能力。为了解决这些问题，我们介绍了Synchrorama，这是一个新颖的框架，通过结合文本（通过情感分析）和音频（通过基于语音的情感识别和音频衍生的价值 - 声音 - 声音 - 声音 - 声音 - 声音 - 源自）的情感信号来整合多模式的情感，从而使会说话的脸部与丰富的情感和更真实的表达和表现力和富裕的表达和富有表现力。为了确保自然的头部运动和准确的唇部同步，Synchrorama包括一个音频到运动（A2M）模块，该模块生成与输入音频对齐的运动框架。最后，Synchrorama将大语言模型（LLM）生成的场景描述作为其他文本输入，使其能够捕获动态动作和高级语义属性。在视觉和文本提示上调节模型可以增强时间一致性和视觉现实主义。基准数据集上的定量和定性实验表明，Synchrorama优于最先进的实验，从而在图像质量，表达保存和运动现实主义方面取得了改善。一项用户研究进一步证实，在整体自然性，运动多样性和视频平稳性方面，Synchrorama获得的主观评分高于竞争方法。我们的项目页面可在<this HTTPS URL上找到。</li>
</ul>

<h3>Title: CamPVG: Camera-Controlled Panoramic Video Generation with Epipolar-Aware Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Chenhao Ji, Chaohui Yu, Junyao Gao, Fan Wang, Cairong Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19979">https://arxiv.org/abs/2509.19979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19979">https://arxiv.org/pdf/2509.19979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19979]] CamPVG: Camera-Controlled Panoramic Video Generation with Epipolar-Aware Diffusion(https://arxiv.org/abs/2509.19979)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recently, camera-controlled video generation has seen rapid development, offering more precise control over video generation. However, existing methods predominantly focus on camera control in perspective projection video generation, while geometrically consistent panoramic video generation remains challenging. This limitation is primarily due to the inherent complexities in panoramic pose representation and spherical projection. To address this issue, we propose CamPVG, the first diffusion-based framework for panoramic video generation guided by precise camera poses. We achieve camera position encoding for panoramic images and cross-view feature aggregation based on spherical projection. Specifically, we propose a panoramic Plücker embedding that encodes camera extrinsic parameters through spherical coordinate transformation. This pose encoder effectively captures panoramic geometry, overcoming the limitations of traditional methods when applied to equirectangular projections. Additionally, we introduce a spherical epipolar module that enforces geometric constraints through adaptive attention masking along epipolar lines. This module enables fine-grained cross-view feature aggregation, substantially enhancing the quality and consistency of generated panoramic videos. Extensive experiments demonstrate that our method generates high-quality panoramic videos consistent with camera trajectories, far surpassing existing methods in panoramic video generation.</li>
<li><strong>摘要：</strong>最近，摄像机控制的视频生成已经快速开发，提供了对视频生成的更精确的控制。但是，现有方法主要集中在透视投影视频中，而几何一致的全景视频生成仍然具有挑战性。该限制主要是由于全景姿势表示和球形投影的固有复杂性。为了解决这个问题，我们提出了Campvg，这是由精确的相机姿势指导的第一个基于扩散的视频生成框架。我们实现了基于球形投影的全景图像和跨视图汇总的相机位置。具体而言，我们提出了一个全景plücker嵌入，该嵌入通过球形坐标转换来编码相机外部参数。这种姿势编码器有效地捕获了全景几何形状，克服了应用于等应角投影时的传统方法的局限性。此外，我们引入了一个球形的表现模块，该模块通过沿着外两极线的自适应注意力掩盖了几何约束。该模块可实现细粒度的跨视图特征聚合，从而大大提高了生成的全景视频的质量和一致性。广泛的实验表明，我们的方法生成了与摄像头轨迹一致的高质量全景视频，超过了全景视频的现有方法。</li>
</ul>

<h3>Title: Generative Adversarial Networks Applied for Privacy Preservation in Biometric-Based Authentication and Identification</h3>
<ul>
<li><strong>Authors: </strong>Lubos Mjachky, Ivan Homoliak</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20024">https://arxiv.org/abs/2509.20024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20024">https://arxiv.org/pdf/2509.20024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20024]] Generative Adversarial Networks Applied for Privacy Preservation in Biometric-Based Authentication and Identification(https://arxiv.org/abs/2509.20024)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Biometric-based authentication systems are getting broadly adopted in many areas. However, these systems do not allow participating users to influence the way their data is used. Furthermore, the data may leak and can be misused without the users' knowledge. In this paper, we propose a new authentication method that preserves the privacy of individuals and is based on a generative adversarial network (GAN). Concretely, we suggest using the GAN for translating images of faces to a visually private domain (e.g., flowers or shoes). Classifiers, which are used for authentication purposes, are then trained on the images from the visually private domain. Based on our experiments, the method is robust against attacks and still provides meaningful utility.</li>
<li><strong>摘要：</strong>基于生物识别的身份验证系统在许多领域都广泛采用。但是，这些系统不允许参与的用户影响其数据的使用方式。此外，数据可能会泄漏并且可以在没有用户知识的情况下被滥用。在本文中，我们提出了一种新的身份验证方法，该方法保留个人的隐私，并基于生成的对抗网络（GAN）。具体而言，我们建议使用gan将面部图像转换为视觉上的私人域（例如花或鞋子）。然后，用于身份验证目的的分类器将对来自视觉上私有域的图像进行培训。根据我们的实验，该方法可抵抗攻击，并且仍然提供有意义的效用。</li>
</ul>

<h3>Title: Predictive Quality Assessment for Mobile Secure Graphics</h3>
<ul>
<li><strong>Authors: </strong>Cas Steigstra, Sergey Milyaev, Shaodi You</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20028">https://arxiv.org/abs/2509.20028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20028">https://arxiv.org/pdf/2509.20028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20028]] Predictive Quality Assessment for Mobile Secure Graphics(https://arxiv.org/abs/2509.20028)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>The reliability of secure graphic verification, a key anti-counterfeiting tool, is undermined by poor image acquisition on smartphones. Uncontrolled user captures of these high-entropy patterns cause high false rejection rates, creating a significant 'reliability gap'. To bridge this gap, we depart from traditional perceptual IQA and introduce a framework that predictively estimates a frame's utility for the downstream verification task. We propose a lightweight model to predict a quality score for a video frame, determining its suitability for a resource-intensive oracle model. Our framework is validated using re-contextualized FNMR and ISRR metrics on a large-scale dataset of 32,000+ images from 105 smartphones. Furthermore, a novel cross-domain analysis on graphics from different industrial printing presses reveals a key finding: a lightweight probe on a frozen, ImageNet-pretrained network generalizes better to an unseen printing technology than a fully fine-tuned model. This provides a key insight for real-world generalization: for domain shifts from physical manufacturing, a frozen general-purpose backbone can be more robust than full fine-tuning, which can overfit to source-domain artifacts.</li>
<li><strong>摘要：</strong>智能手机上的不良图像采集破坏了安全的图形验证的可靠性，这是一种关键的反遭遇工具。这些高渗透模式的不受控制的用户捕获会导致高拒绝率，从而产生了显着的“可靠性差距”。为了弥合这一差距，我们偏离了传统的感知IQA，并引入了一个框架，该框架可预测地估算下游验证任务的框架实用程序。我们提出了一个轻巧的模型，以预测视频框架的质量分数，从而确定其对资源密集型甲骨文模型的适用性。我们的框架是使用重新定下的FNMR和ISRR指标验证的，这是105张智能手机的32,000多个图像的大规模数据集上的。此外，对来自不同工业印刷机的图形的新型跨域分析揭示了一个关键发现：对冷冻，Imagenet预测网络的轻量级探针比完全微调的模型更好地推广到了看不见的印刷技术。这为实际概括提供了一个关键的见解：对于从物理制造的域转移，冷冻通用骨架比完整的微调更强大，这可以过度fortit到源域伪像。</li>
</ul>

<h3>Title: Unleashing the Potential of the Semantic Latent Space in Diffusion Models for Image Dehazing</h3>
<ul>
<li><strong>Authors: </strong>Zizheng Yang, Hu Yu, Bing Li, Jinghao Zhang, Jie Huang, Feng Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20091">https://arxiv.org/abs/2509.20091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20091">https://arxiv.org/pdf/2509.20091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20091]] Unleashing the Potential of the Semantic Latent Space in Diffusion Models for Image Dehazing(https://arxiv.org/abs/2509.20091)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have recently been investigated as powerful generative solvers for image dehazing, owing to their remarkable capability to model the data distribution. However, the massive computational burden imposed by the retraining of diffusion models, coupled with the extensive sampling steps during the inference, limit the broader application of diffusion models in image dehazing. To address these issues, we explore the properties of hazy images in the semantic latent space of frozen pre-trained diffusion models, and propose a Diffusion Latent Inspired network for Image Dehazing, dubbed DiffLI$^2$D. Specifically, we first reveal that the semantic latent space of pre-trained diffusion models can represent the content and haze characteristics of hazy images, as the diffusion time-step changes. Building upon this insight, we integrate the diffusion latent representations at different time-steps into a delicately designed dehazing network to provide instructions for image dehazing. Our DiffLI$^2$D avoids re-training diffusion models and iterative sampling process by effectively utilizing the informative representations derived from the pre-trained diffusion models, which also offers a novel perspective for introducing diffusion models to image dehazing. Extensive experiments on multiple datasets demonstrate that the proposed method achieves superior performance to existing image dehazing methods. Code is available at this https URL.</li>
<li><strong>摘要：</strong>由于它们的显着能力对数据分布进行建模，因此已将扩散模型作为图像飞机的强大生成求解器进行了研究。然而，扩散模型的重新培训施加的巨大计算负担，再加上推断期间的广泛采样步骤，限制扩散模型在图像脱壳中的更广泛应用。为了解决这些问题，我们在冷冻预训练的扩散模型的语义潜在空间中探索了朦胧图像的属性，并提出了一个扩散的潜在潜在灵感网络，用于图像去悬式，称为diffli $^2 $ d。具体而言，我们首先揭示了预训练扩散模型的语义潜在空间可以代表朦胧图像的内容和雾霾特征，因为扩散时间步长会发生变化。在此洞察力的基础上，我们将不同时间步骤的扩散潜在表示纳入了精心设计的飞行网络，以提供图像去悬式的说明。我们的Diffli $^2 $ d通过有效利用从预训练的扩散模型中得出的信息表示，避免了重新训练扩散模型和迭代采样过程，该模型还提供了一种新的观点，用于将扩散模型引入图像模糊。在多个数据集上进行的广泛实验表明，所提出的方法可在现有图像脱掩的方法上实现出色的性能。代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Incomplete Data, Complete Dynamics: A Diffusion Approach</h3>
<ul>
<li><strong>Authors: </strong>Zihan Zhou, Chenguang Wang, Hongyi Ye, Yongtao Guan, Tianshu Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20098">https://arxiv.org/abs/2509.20098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20098">https://arxiv.org/pdf/2509.20098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20098]] Incomplete Data, Complete Dynamics: A Diffusion Approach(https://arxiv.org/abs/2509.20098)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Learning physical dynamics from data is a fundamental challenge in machine learning and scientific modeling. Real-world observational data are inherently incomplete and irregularly sampled, posing significant challenges for existing data-driven approaches. In this work, we propose a principled diffusion-based framework for learning physical systems from incomplete training samples. To this end, our method strategically partitions each such sample into observed context and unobserved query components through a carefully designed splitting strategy, then trains a conditional diffusion model to reconstruct the missing query portions given available contexts. This formulation enables accurate imputation across arbitrary observation patterns without requiring complete data supervision. Specifically, we provide theoretical analysis demonstrating that our diffusion training paradigm on incomplete data achieves asymptotic convergence to the true complete generative process under mild regularity conditions. Empirically, we show that our method significantly outperforms existing baselines on synthetic and real-world physical dynamics benchmarks, including fluid flows and weather systems, with particularly strong performance in limited and irregular observation regimes. These results demonstrate the effectiveness of our theoretically principled approach for learning and imputing partially observed dynamics.</li>
<li><strong>摘要：</strong>从数据中学习物理动态是机器学习和科学建模的基本挑战。现实世界的观察数据本质上是不完整且不规则采样的，对现有数据驱动的方法构成了重大挑战。在这项工作中，我们提出了一个基于不完整的培训样本学习物理系统的原则性基于扩散的框架。为此，我们的方法通过精心设计的分裂策略将每个样本的每个样本策略性地将每个样本划分为观察到的上下文和未观察到的查询组件，然后训练有条件的扩散模型，以重建给定可用上下文的缺失查询部分。该公式可以在不需要完整的数据监督的情况下进行跨任意观察模式的准确插补。具体而言，我们提供了理论分析，表明我们对不完整数据的扩散训练范式在轻度规律条件下实现了渐近完整生成过程的渐近收敛。从经验上讲，我们表明我们的方法在合成和现实世界的物理动力学基准（包括流体流和天气系统）上的现有基准极大地胜过现有的基准，在有限和不规则观察方面的性能特别强。这些结果证明了我们理论上原则性学习和推出部分观察到的动态的有效性。</li>
</ul>

<h3>Title: Optical Ocean Recipes: Creating Realistic Datasets to Facilitate Underwater Vision Research</h3>
<ul>
<li><strong>Authors: </strong>Patricia Schöntag, David Nakath, Judith Fischer, Rüdiger Röttgers, Kevin Köser</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20171">https://arxiv.org/abs/2509.20171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20171">https://arxiv.org/pdf/2509.20171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20171]] Optical Ocean Recipes: Creating Realistic Datasets to Facilitate Underwater Vision Research(https://arxiv.org/abs/2509.20171)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>The development and evaluation of machine vision in underwater environments remains challenging, often relying on trial-and-error-based testing tailored to specific applications. This is partly due to the lack of controlled, ground-truthed testing environments that account for the optical challenges, such as color distortion from spectrally variant light attenuation, reduced contrast and blur from backscatter and volume scattering, and dynamic light patterns from natural or artificial illumination. Additionally, the appearance of ocean water in images varies significantly across regions, depths, and seasons. However, most machine vision evaluations are conducted under specific optical water types and imaging conditions, therefore often lack generalizability. Exhaustive testing across diverse open-water scenarios is technically impractical. To address this, we introduce the \textit{Optical Ocean Recipes}, a framework for creating realistic datasets under controlled underwater conditions. Unlike synthetic or open-water data, these recipes, using calibrated color and scattering additives, enable repeatable and controlled testing of the impact of water composition on image appearance. Hence, this provides a unique framework for analyzing machine vision in realistic, yet controlled underwater scenarios. The controlled environment enables the creation of ground-truth data for a range of vision tasks, including water parameter estimation, image restoration, segmentation, visual SLAM, and underwater image synthesis. We provide a demonstration dataset generated using the Optical Ocean Recipes and briefly demonstrate the use of our system for two underwater vision tasks. The dataset and evaluation code will be made available.</li>
<li><strong>摘要：</strong>水下环境中机器视觉的开发和评估通常依赖于针对特定应用程序量身定制的基于反复的测试。这部分是由于缺乏控制光学挑战的受控，经过经过经过经过经过经验的测试环境，例如频谱变化的光衰减的颜色扭曲，对比度减少和对反向散射和体积散射的模糊以及自然或人工照明产生的动态光模式。此外，图像中海水的出现在各个地区，深度和季节之间差异很大。但是，大多数机器视觉评估是在特定的光学水类型和成像条件下进行的，因此通常缺乏普遍性。在技​​术上，详尽的测试在技术上是不切实际的。为了解决这个问题，我们介绍了\ textit {光学海洋食谱}，这是一个在受控的水下条件下创建现实数据集的框架。与合成或开放水的数据不同，这些食谱使用校准的颜色和散射添加剂，可以对水成分对图像外观的影响进行可重复和控制的测试。因此，这为分析现实但受控制的水下场景分析机器视觉提供了独特的框架。受控环境可以为一系列视觉任务创建基地真实数据，包括水参数估计，图像恢复，分割，视觉猛击和水下图像综合。我们提供了使用光学海洋食谱生成的演示数据集，并简要证明了我们的系统用于两个水下视觉任务。将提供数据集和评估代码。</li>
</ul>

<h3>Title: Generative Model Inversion Through the Lens of the Manifold Hypothesis</h3>
<ul>
<li><strong>Authors: </strong>Xiong Peng, Bo Han, Fengfei Yu, Tongliang Liu, Feng Liu, Mingyuan Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20177">https://arxiv.org/abs/2509.20177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20177">https://arxiv.org/pdf/2509.20177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20177]] Generative Model Inversion Through the Lens of the Manifold Hypothesis(https://arxiv.org/abs/2509.20177)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Model inversion attacks (MIAs) aim to reconstruct class-representative samples from trained models. Recent generative MIAs utilize generative adversarial networks to learn image priors that guide the inversion process, yielding reconstructions with high visual quality and strong fidelity to the private training data. To explore the reason behind their effectiveness, we begin by examining the gradients of inversion loss with respect to synthetic inputs, and find that these gradients are surprisingly noisy. Further analysis reveals that generative inversion implicitly denoises these gradients by projecting them onto the tangent space of the generator manifold, filtering out off-manifold components while preserving informative directions aligned with the manifold. Our empirical measurements show that, in models trained with standard supervision, loss gradients often exhibit large angular deviations from the data manifold, indicating poor alignment with class-relevant directions. This observation motivates our central hypothesis: models become more vulnerable to MIAs when their loss gradients align more closely with the generator manifold. We validate this hypothesis by designing a novel training objective that explicitly promotes such alignment. Building on this insight, we further introduce a training-free approach to enhance gradient-manifold alignment during inversion, leading to consistent improvements over state-of-the-art generative MIAs.</li>
<li><strong>摘要：</strong>模型反转攻击（MIAS）旨在重建训练有素的模型的类代表性样本。最近的生成MIA利用生成的对抗网络来学习指导反转过程的图像先验，从而产生具有高视觉质量的重建，并为私人训练数据提供了强烈的忠诚。为了探索其有效性的原因，我们首先检查了综合输入方面的反演损失梯度，并发现这些梯度令人惊讶地嘈杂。进一步的分析表明，生成反演通过将这些梯度投射到发电机歧管的切线空间，从而滤除了模型组件，同时保留了与歧管一致的信息方向，从而暗中将这些梯度投射到了发电机歧管的切线上。我们的经验测量表明，在接受标准监督训练的模型中，损耗梯度通常表现出与数据歧管的较大角度偏差，表明与相关的方向的对齐不良。该观察结果激发了我们的中心假设：当模型与发电机歧管更紧密地保持一致时，模型变得更容易受到MIA的影响。我们通过设计一个新的训练目标来明确促进这种比对来验证这一假设。在这种见解的基础上，我们进一步引入了一种无训练的方法，以增强反转过程中梯度 - 模型对齐，从而对最先进的生成MIAS进行了持续的改进。</li>
</ul>

<h3>Title: An Anisotropic Cross-View Texture Transfer with Multi-Reference Non-Local Attention for CT Slice Interpolation</h3>
<ul>
<li><strong>Authors: </strong>Kwang-Hyun Uhm, Hyunjun Cho, Sung-Hoo Hong, Seung-Won Jung</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20242">https://arxiv.org/abs/2509.20242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20242">https://arxiv.org/pdf/2509.20242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20242]] An Anisotropic Cross-View Texture Transfer with Multi-Reference Non-Local Attention for CT Slice Interpolation(https://arxiv.org/abs/2509.20242)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Computed tomography (CT) is one of the most widely used non-invasive imaging modalities for medical diagnosis. In clinical practice, CT images are usually acquired with large slice thicknesses due to the high cost of memory storage and operation time, resulting in an anisotropic CT volume with much lower inter-slice resolution than in-plane resolution. Since such inconsistent resolution may lead to difficulties in disease diagnosis, deep learning-based volumetric super-resolution methods have been developed to improve inter-slice resolution. Most existing methods conduct single-image super-resolution on the through-plane or synthesize intermediate slices from adjacent slices; however, the anisotropic characteristic of 3D CT volume has not been well explored. In this paper, we propose a novel cross-view texture transfer approach for CT slice interpolation by fully utilizing the anisotropic nature of 3D CT volume. Specifically, we design a unique framework that takes high-resolution in-plane texture details as a reference and transfers them to low-resolution through-plane images. To this end, we introduce a multi-reference non-local attention module that extracts meaningful features for reconstructing through-plane high-frequency details from multiple in-plane images. Through extensive experiments, we demonstrate that our method performs significantly better in CT slice interpolation than existing competing methods on public CT datasets including a real-paired benchmark, verifying the effectiveness of the proposed framework. The source code of this work is available at this https URL.</li>
<li><strong>摘要：</strong>计算机断层扫描（CT）是用于医学诊断的最广泛使用的非侵入性成像方式之一。在临床实践中，由于存储器存储和操作时间的高成本，CT图像通常具有较大的切片厚度，从而导致各向异性CT体积，其分辨率分辨率比平面内分辨率低得多。由于这种不一致的分辨率可能会导致疾病诊断的困难，因此已经开发了基于学习的大量超级分辨率方法来改善切片间分辨率。大多数现有方法在平面或合成相邻切片的中间切片上进行单像超分辨率；但是，尚未很好地探索3D CT体积的各向异性特征。在本文中，我们通过充分利用3D CT体积的各向异性性质提出了一种新型的跨视图转移方法，用于CT切片插值。具体而言，我们设计了一个独特的框架，该框架将高分辨率内纹理细节作为参考，并将其转移到低分辨率的跨平面图像中。为此，我们引入了一个多参考的非本地注意模块，该模块提取有意义的特征，以从多个面板图像中重建平面高频细节。通过广泛的实验，我们证明了我们的方法在CT切片插值中的性能要比公共CT数据集中的现有竞争方法好得多，包括真正的基准测试，从而验证了拟议框架的有效性。此工作的源代码可在此HTTPS URL上获得。</li>
</ul>

<h3>Title: 4D Driving Scene Generation With Stereo Forcing</h3>
<ul>
<li><strong>Authors: </strong>Hao Lu, Zhuang Ma, Guangfeng Jiang, Wenhang Ge, Bohan Li, Yuzhan Cai, Wenzhao Zheng, Yunpeng Zhang, Yingcong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20251">https://arxiv.org/abs/2509.20251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20251">https://arxiv.org/pdf/2509.20251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20251]] 4D Driving Scene Generation With Stereo Forcing(https://arxiv.org/abs/2509.20251)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Current generative models struggle to synthesize dynamic 4D driving scenes that simultaneously support temporal extrapolation and spatial novel view synthesis (NVS) without per-scene optimization. Bridging generation and novel view synthesis remains a major challenge. We present PhiGenesis, a unified framework for 4D scene generation that extends video generation techniques with geometric and temporal consistency. Given multi-view image sequences and camera parameters, PhiGenesis produces temporally continuous 4D Gaussian splatting representations along target 3D trajectories. In its first stage, PhiGenesis leverages a pre-trained video VAE with a novel range-view adapter to enable feed-forward 4D reconstruction from multi-view images. This architecture supports single-frame or video inputs and outputs complete 4D scenes including geometry, semantics, and motion. In the second stage, PhiGenesis introduces a geometric-guided video diffusion model, using rendered historical 4D scenes as priors to generate future views conditioned on trajectories. To address geometric exposure bias in novel views, we propose Stereo Forcing, a novel conditioning strategy that integrates geometric uncertainty during denoising. This method enhances temporal coherence by dynamically adjusting generative influence based on uncertainty-aware perturbations. Our experimental results demonstrate that our method achieves state-of-the-art performance in both appearance and geometric reconstruction, temporal generation and novel view synthesis (NVS) tasks, while simultaneously delivering competitive performance in downstream evaluations. Homepage is at \href{this https URL}{PhiGensis}.</li>
<li><strong>摘要：</strong>当前的生成模型难以合成动态4D驾驶场景，同时支持时间外推和空间新型视图合成（NVS），而无需每场比赛优化。桥接产生和新型观点综合仍然是一个主要挑战。我们提出了Phigenesis，这是4D场景生成的统一框架，它以几何和时间的一致性扩展了视频生成技术。给定的多视图图像序列和摄像机参数，化根发生沿目标3D轨迹产生时间连续的4D高斯脱落表示形式。在其第一阶段，化根利用具有新颖的范围视图适配器的预训练的视频VAE来启用来自多视图图像的前馈4D重建。该体系结构支持单帧或视频输入和输出完整的4D场景，包括几何，语义和运动。在第二阶段，Phigenesis介绍了几何引导的视频扩散模型，使用渲染的历史4D场景作为先验，以生成以轨迹为条件的未来视图。为了解决新型观点中的几何暴露偏见，我们提出了立体声强迫，这是一种新颖的调理策略，可以整合denoing期间的几何不确定性。该方法通过基于不确定性意识的扰动来动态调整生成影响来增强时间连贯性。我们的实验结果表明，我们的方法在外观和几何重建，时间产生和新型视图合成（NVS）任务中都达到了最先进的性能，同时在下游评估中同时提供了竞争性能。主页位于\ href {此https url} {phigensis}。</li>
</ul>

<h3>Title: A co-evolving agentic AI system for medical imaging analysis</h3>
<ul>
<li><strong>Authors: </strong>Songhao Li, Jonathan Xu, Tiancheng Bao, Yuxuan Liu, Yuchen Liu, Yihang Liu, Lilin Wang, Wenhui Lei, Sheng Wang, Yinuo Xu, Yan Cui, Jialu Yao, Shunsuke Koga, Zhi Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20279">https://arxiv.org/abs/2509.20279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20279">https://arxiv.org/pdf/2509.20279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20279]] A co-evolving agentic AI system for medical imaging analysis(https://arxiv.org/abs/2509.20279)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Agentic AI is rapidly advancing in healthcare and biomedical research. However, in medical image analysis, their performance and adoption remain limited due to the lack of a robust ecosystem, insufficient toolsets, and the absence of real-time interactive expert feedback. Here we present "TissueLab", a co-evolving agentic AI system that allows researchers to ask direct questions, automatically plan and generate explainable workflows, and conduct real-time analyses where experts can visualize intermediate results and refine them. TissueLab integrates tool factories across pathology, radiology, and spatial omics domains. By standardizing inputs, outputs, and capabilities of diverse tools, the system determines when and how to invoke them to address research and clinical questions. Across diverse tasks with clinically meaningful quantifications that inform staging, prognosis, and treatment planning, TissueLab achieves state-of-the-art performance compared with end-to-end vision-language models (VLMs) and other agentic AI systems such as GPT-5. Moreover, TissueLab continuously learns from clinicians, evolving toward improved classifiers and more effective decision strategies. With active learning, it delivers accurate results in unseen disease contexts within minutes, without requiring massive datasets or prolonged retraining. Released as a sustainable open-source ecosystem, TissueLab aims to accelerate computational research and translational adoption in medical imaging while establishing a foundation for the next generation of medical AI.</li>
<li><strong>摘要：</strong>代理AI在医疗保健和生物医学研究方面正在迅速发展。但是，在医学图像分析中，由于缺乏强大的生态系统，工具集不足以及缺乏实时交互式专家反馈，它们的性能和采用率仍然有限。在这里，我们提出了“ Tissuelab”，这是一种共同发展的代理AI系统，允许研究人员提出直接问题，自动计划和生成可解释的工作流程，并进行实时分析，专家可以在其中可视化中间结果并完善它们。 Tissuelab跨病理学，放射学和空间上的OMICS领域整合了工具工厂。通过标准化不同工具的输入，产出和功能，系统确定何时以及如何调用它们来解决研究和临床问题。在具有临床意义的量化方面的各种任务中，与端到端视力语言模型（VLM）和其他代理AI系统（例如GPT-5）相比，Tissuelab实现了最先进的性能。此外，Tissuelab不断向临床医生学习，向改进的分类器和更有效的决策策略发展。通过积极的学习，它可以在几分钟之内提供准确的疾病环境，而无需大量数据集或延长的重新培训。 Tissuelab作为可持续的开源生态系统发行，旨在在医学成像中加速计算研究和翻译采用，同时为下一代医学AI建立基础。</li>
</ul>

<h3>Title: FAST: Foreground-aware Diffusion with Accelerated Sampling Trajectory for Segmentation-oriented Anomaly Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Xichen Xu, Yanshu Wang, Jinbao Wang, Xiaoning Lei, Guoyang Xie, Guannan Jiang, Zhichao Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20295">https://arxiv.org/abs/2509.20295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20295">https://arxiv.org/pdf/2509.20295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20295]] FAST: Foreground-aware Diffusion with Accelerated Sampling Trajectory for Segmentation-oriented Anomaly Synthesis(https://arxiv.org/abs/2509.20295)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Industrial anomaly segmentation relies heavily on pixel-level annotations, yet real-world anomalies are often scarce, diverse, and costly to label. Segmentation-oriented industrial anomaly synthesis (SIAS) has emerged as a promising alternative; however, existing methods struggle to balance sampling efficiency and generation quality. Moreover, most approaches treat all spatial regions uniformly, overlooking the distinct statistical differences between anomaly and background areas. This uniform treatment hinders the synthesis of controllable, structure-specific anomalies tailored for segmentation tasks. In this paper, we propose FAST, a foreground-aware diffusion framework featuring two novel modules: the Anomaly-Informed Accelerated Sampling (AIAS) and the Foreground-Aware Reconstruction Module (FARM). AIAS is a training-free sampling algorithm specifically designed for segmentation-oriented industrial anomaly synthesis, which accelerates the reverse process through coarse-to-fine aggregation and enables the synthesis of state-of-the-art segmentation-oriented anomalies in as few as 10 steps. Meanwhile, FARM adaptively adjusts the anomaly-aware noise within the masked foreground regions at each sampling step, preserving localized anomaly signals throughout the denoising trajectory. Extensive experiments on multiple industrial benchmarks demonstrate that FAST consistently outperforms existing anomaly synthesis methods in downstream segmentation tasks. We release the code at: this https URL.</li>
<li><strong>摘要：</strong>工业异常细分在很大程度上依赖于像素级注释，但是现实世界中的异常通常稀少，多样化和昂贵的标签。面向分割的工业异常合成（SIA）已成为有前途的替代方案。但是，现有的方法难以平衡抽样效率和发电质量。此外，大多数方法都统一地对待所有空间区域，忽略了异常和背景区域之间的明显统计差异。这种均匀的处理阻碍了针对分割任务量身定制的可控，结构特异性异常的综合。在本文中，我们提出了快速的，这是一个具有两个新型模块的前景吸引的扩散框架：异常的加速采样（AIA）和前景感知的重建模块（farm）。 AIAS是一种专门针对分割为分割的工业异常合成而设计的无训练采样算法，通过粗到细节的聚合加速了反向过程，并允许在几乎10个步骤中综合以最先进的面向分段的异常。同时，农场在每个采样步骤中自适应地调节蒙面前景区域内的异常噪声，从而保留整个denoising轨迹的局部异常信号。在多个工业基准上进行的广泛实验表明，在下游分割任务中，快速始终优于现有的异常合成方法。我们在以下位置发布代码：此HTTPS URL。</li>
</ul>

<h3>Title: Video models are zero-shot learners and reasoners</h3>
<ul>
<li><strong>Authors: </strong>Thaddäus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, Robert Geirhos</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20328">https://arxiv.org/abs/2509.20328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20328">https://arxiv.org/pdf/2509.20328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20328]] Video models are zero-shot learners and reasoners(https://arxiv.org/abs/2509.20328)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The remarkable zero-shot capabilities of Large Language Models (LLMs) have propelled natural language processing from task-specific models to unified, generalist foundation models. This transformation emerged from simple primitives: large, generative models trained on web-scale data. Curiously, the same primitives apply to today's generative video models. Could video models be on a trajectory towards general-purpose vision understanding, much like LLMs developed general-purpose language understanding? We demonstrate that Veo 3 can solve a broad variety of tasks it wasn't explicitly trained for: segmenting objects, detecting edges, editing images, understanding physical properties, recognizing object affordances, simulating tool use, and more. These abilities to perceive, model, and manipulate the visual world enable early forms of visual reasoning like maze and symmetry solving. Veo's emergent zero-shot capabilities indicate that video models are on a path to becoming unified, generalist vision foundation models.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的显着零拍功能从特定于任务的模型到统一的通才基础模型推动了自然语言处理。这种转换来自简单的原始图：在网络规模数据上训练的大型生成模型。奇怪的是，同样的原始词也适用于当今的生成视频模型。视频模型能否朝着通用视觉理解的轨迹上，就像LLM一样，开发了通用语言的理解吗？我们证明，VEO 3可以解决多种任务，它没有明确训练：分割对象，检测边缘，编辑图像，了解物理属性，识别对象提供，模拟工具使用等。这些能力感知，模型和操纵视觉世界可以实现迷宫和对称性解决的视觉推理的早期形式。 VEO的新兴零拍功能表明，视频模型正在成为统一的通才愿景基础模型的道路。</li>
</ul>

<h3>Title: PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Chen Wang, Chuhao Chen, Yiming Huang, Zhiyang Dou, Yuan Liu, Jiatao Gu, Lingjie Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20358">https://arxiv.org/abs/2509.20358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20358">https://arxiv.org/pdf/2509.20358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20358]] PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation(https://arxiv.org/abs/2509.20358)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Existing video generation models excel at producing photo-realistic videos from text or images, but often lack physical plausibility and 3D controllability. To overcome these limitations, we introduce PhysCtrl, a novel framework for physics-grounded image-to-video generation with physical parameters and force control. At its core is a generative physics network that learns the distribution of physical dynamics across four materials (elastic, sand, plasticine, and rigid) via a diffusion model conditioned on physics parameters and applied forces. We represent physical dynamics as 3D point trajectories and train on a large-scale synthetic dataset of 550K animations generated by physics simulators. We enhance the diffusion model with a novel spatiotemporal attention block that emulates particle interactions and incorporates physics-based constraints during training to enforce physical plausibility. Experiments show that PhysCtrl generates realistic, physics-grounded motion trajectories which, when used to drive image-to-video models, yield high-fidelity, controllable videos that outperform existing methods in both visual quality and physical plausibility. Project Page: this https URL</li>
<li><strong>摘要：</strong>现有的视频生成模型在产生文本或图像的照片真实视频方面表现出色，但通常缺乏身体上的合理性和3D可控性。为了克服这些局限性，我们介绍了Physctrl，这是一个具有物理参数和力控制的物理界面形成图像之间的新型框架。其核心是一个生成物理网络，它通过以物理参数和施加力为条件的扩散模型了解了四种材料（弹性，沙子，塑料和刚性）之间物理动力学的分布。我们将物理动力学表示为3D点轨迹，并在物理模拟器生成的550K动画的大规模合成数据集上进行训练。我们使用新型的时空注意力块增强了扩散模型，该模型模拟了粒子相互作用，并在训练过程中纳入了基于物理的约束以实现物理合理性。实验表明，PhysCtrl会生成逼真的，物理接收的运动轨迹，当用于驱动图像到视频模型时，会产生高保真，可控的视频，以优于视觉质量和物理合理性的现有方法。项目页面：此HTTPS URL</li>
</ul>

<h3>Title: EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Xuan Ju, Tianyu Wang, Yuqian Zhou, He Zhang, Qing Liu, Nanxuan Zhao, Zhifei Zhang, Yijun Li, Yuanhao Cai, Shaoteng Liu, Daniil Pakhomov, Zhe Lin, Soo Ye Kim, Qiang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20360">https://arxiv.org/abs/2509.20360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20360">https://arxiv.org/pdf/2509.20360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20360]] EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning(https://arxiv.org/abs/2509.20360)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in foundation models highlight a clear trend toward unification and scaling, showing emergent capabilities across diverse domains. While image generation and editing have rapidly transitioned from task-specific to unified frameworks, video generation and editing remain fragmented due to architectural limitations and data scarcity. In this work, we introduce EditVerse, a unified framework for image and video generation and editing within a single model. By representing all modalities, i.e., text, image, and video, as a unified token sequence, EditVerse leverages self-attention to achieve robust in-context learning, natural cross-modal knowledge transfer, and flexible handling of inputs and outputs with arbitrary resolutions and durations. To address the lack of video editing training data, we design a scalable data pipeline that curates 232K video editing samples and combines them with large-scale image and video datasets for joint training. Furthermore, we present EditVerseBench, the first benchmark for instruction-based video editing covering diverse tasks and resolutions. Extensive experiments and user studies demonstrate that EditVerse achieves state-of-the-art performance, surpassing existing open-source and commercial models, while exhibiting emergent editing and generation abilities across modalities.</li>
<li><strong>摘要：</strong>基础模型的最新进展突出了统一和扩展的明确趋势，显示了各种领域的新兴能力。尽管图像生成和编辑已迅速从特定于任务的框架过渡到统一的框架，但由于建筑局限性和数据稀缺性，视频生成和编辑仍然存在分散。在这项工作中，我们介绍了Editverse，这是一个统一的图像和视频生成框架，并在单个模型中进行编辑。通过表示所有模式，即文本，图像和视频，作为统一的令牌序列，Editverse Leververs Leververs of自我注意力以实现强大的内在学习，自然的跨模式知识传递以及具有任意决议和持续时间的输入和输出的灵活处理。为了解决缺乏视频编辑培训数据，我们设计了一条可扩展的数据管道，该管道策划了232K视频编辑样本，并将它们与大型图像和视频数据集结合在一起，以进行联合培训。此外，我们介绍了EditverseBench，这是基于教学的视频编辑的第一个基准，涵盖了各种任务和决议。广泛的实验和用户研究表明，Editverse实现了最先进的性能，超过了现有的开源和商业模型，同时表现出跨模式的紧急编辑和发电能力。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
