<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-11-04</h1>
<h3>Title: Generative human motion mimicking through feature extraction in denoising diffusion settings</h3>
<ul>
<li><strong>Authors: </strong>Alexander Okupnik, Johannes Schneider, Kyriakos Flouris</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00011">https://arxiv.org/abs/2511.00011</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00011">https://arxiv.org/pdf/2511.00011</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00011]] Generative human motion mimicking through feature extraction in denoising diffusion settings(https://arxiv.org/abs/2511.00011)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent success with large language models has sparked a new wave of verbal human-AI interaction. While such models support users in a variety of creative tasks, they lack the embodied nature of human interaction. Dance, as a primal form of human expression, is predestined to complement this experience. To explore creative human-AI interaction exemplified by dance, we build an interactive model based on motion capture (MoCap) data. It generates an artificial other by partially mimicking and also "creatively" enhancing an incoming sequence of movement data. It is the first model, which leverages single-person motion data and high level features in order to do so and, thus, it does not rely on low level human-human interaction data. It combines ideas of two diffusion models, motion inpainting, and motion style transfer to generate movement representations that are both temporally coherent and responsive to a chosen movement reference. The success of the model is demonstrated by quantitatively assessing the convergence of the feature distribution of the generated samples and the test set which serves as simulating the human performer. We show that our generations are first steps to creative dancing with AI as they are both diverse showing various deviations from the human partner while appearing realistic.</li>
<li><strong>摘要：</strong>最近大型语言模型的成功引发了人类与人工智能言语交互的新浪潮。虽然此类模型支持用户执行各种创造性任务，但它们缺乏人类交互的具体本质。舞蹈作为人类表达的一种原始形式，注定会补充这种体验。为了探索以舞蹈为代表的创造性人机交互，我们构建了一个基于动作捕捉 (MoCap) 数据的交互模型。它通过部分模仿并“创造性地”增强传入的运动数据序列来生成人工他人。它是第一个模型，它利用单人运动数据和高级特征来实现这一点，因此，它不依赖于低级人与人交互数据。它结合了两种扩散模型、运动修复和运动风格转移的思想，以生成在时间上连贯且对所选运动参考做出响应的运动表示。该模型的成功通过定量评估生成样本的特征分布与模拟人类表演者的测试集的收敛性来证明。我们表明，我们这一代人是利用人工智能进行创意舞蹈的第一步，因为他们都是多样化的，表现出与人类舞伴的各种偏差，同时又显得现实。</li>
</ul>

<h3>Title: Probing Knowledge Holes in Unlearned LLMs</h3>
<ul>
<li><strong>Authors: </strong>Myeongseob Ko, Hoang Anh Just, Charles Fleming, Ming Jin, Ruoxi Jia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00030">https://arxiv.org/abs/2511.00030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00030">https://arxiv.org/pdf/2511.00030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00030]] Probing Knowledge Holes in Unlearned LLMs(https://arxiv.org/abs/2511.00030)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Machine unlearning has emerged as a prevalent technical solution for selectively removing unwanted knowledge absorbed during pre-training, without requiring full retraining. While recent unlearning techniques can effectively remove undesirable content without severely compromising performance on standard benchmarks, we find that they may inadvertently create ``knowledge holes'' -- unintended losses of benign knowledge that standard benchmarks fail to capture. To probe where unlearned models reveal knowledge holes, we propose a test case generation framework that explores both immediate neighbors of unlearned content and broader areas of potential failures. Our evaluation demonstrates significant hidden costs of unlearning: up to 98.7\% of the test cases yield irrelevant or nonsensical responses from unlearned models, despite being answerable by the pretrained model. These findings necessitate rethinking the conventional approach to evaluating knowledge preservation in unlearning, moving beyond standard, static benchmarks.</li>
<li><strong>摘要：</strong>机器取消学习已成为一种流行的技术解决方案，用于选择性地删除预训练期间吸收的不需要的知识，而不需要完全重新训练。虽然最近的忘却技术可以有效地删除不需要的内容，而不会严重影响标准基准测试的性能，但我们发现它们可能会无意中造成“知识漏洞”——标准基准测试无法捕获的良性知识的意外丢失。为了探究未学习的模型在哪里揭示了知识漏洞，我们提出了一个测试用例生成框架，该框架可以探索未学习内容的直接邻居和更广泛的潜在故障领域。我们的评估表明了遗忘的显着隐性成本：尽管预训练模型可以回答，但高达 98.7% 的测试用例会从未学习的模型中产生不相关或无意义的响应。这些发现需要重新思考评估遗忘过程中知识保存的传统方法，超越标准的静态基准。</li>
</ul>

<h3>Title: Neural Architecture Search for global multi-step Forecasting of Energy Production Time Series</h3>
<ul>
<li><strong>Authors: </strong>Georg Velev, Stefan Lessmann</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00035">https://arxiv.org/abs/2511.00035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00035">https://arxiv.org/pdf/2511.00035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00035]] Neural Architecture Search for global multi-step Forecasting of Energy Production Time Series(https://arxiv.org/abs/2511.00035)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The dynamic energy sector requires both predictive accuracy and runtime efficiency for short-term forecasting of energy generation under operational constraints, where timely and precise predictions are crucial. The manual configuration of complex methods, which can generate accurate global multi-step predictions without suffering from a computational bottleneck, represents a procedure with significant time requirements and high risk for human-made errors. A further intricacy arises from the temporal dynamics present in energy-related data. Additionally, the generalization to unseen data is imperative for continuously deploying forecasting techniques over time. To overcome these challenges, in this research, we design a neural architecture search (NAS)-based framework for the automated discovery of time series models that strike a balance between computational efficiency, predictive performance, and generalization power for the global, multi-step short-term forecasting of energy production time series. In particular, we introduce a search space consisting only of efficient components, which can capture distinctive patterns of energy time series. Furthermore, we formulate a novel objective function that accounts for performance generalization in temporal context and the maximal exploration of different regions of our high-dimensional search space. The results obtained on energy production time series show that an ensemble of lightweight architectures discovered with NAS outperforms state-of-the-art techniques, such as Transformers, as well as pre-trained forecasting models, in terms of both efficiency and accuracy.</li>
<li><strong>摘要：</strong>动态能源领域需要预测准确性和运行效率，以便在运营限制下对能源发电进行短期预测，其中及时、准确的预测至关重要。复杂方法的手动配置可以生成准确的全局多步骤预测，而不会遇到计算瓶颈，这代表了一个具有大量时间要求和人为错误风险高的过程。能源相关数据中存在的时间动态导致了进一步的复杂性。此外，对于随着时间的推移持续部署预测技术而言，对未见数据的概括是必要的。为了克服这些挑战，在本研究中，我们设计了一个基于神经架构搜索（NAS）的框架，用于自动发现时间序列模型，该框架在计算效率、预测性能和泛化能力之间取得平衡，以实现能源生产时间序列的全局、多步短期预测。特别是，我们引入了仅由有效组件组成的搜索空间，它可以捕获能量时间序列的独特模式。此外，我们制定了一个新颖的目标函数，该函数考虑了时间上下文中的性能泛化以及对高维搜索空间不同区域的最大探索。在能源生产时间序列上获得的结果表明，使用 NAS 发现的轻量级架构集合在效率和准确性方面均优于最先进的技术，例如 Transformer 以及预训练的预测模型。</li>
</ul>

<h3>Title: ReLaX-Net: Reusing Layers for Parameter-Efficient Physical Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Kohei Tsuchiyama, Andre Roehm, Takatomo Mihana, Ryoichi Horisaki</a></li>
<li><strong>Subjects: </strong>cs.LG, nlin.AO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00044">https://arxiv.org/abs/2511.00044</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00044">https://arxiv.org/pdf/2511.00044</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00044]] ReLaX-Net: Reusing Layers for Parameter-Efficient Physical Neural Networks(https://arxiv.org/abs/2511.00044)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Physical Neural Networks (PNN) are promising platforms for next-generation computing systems. However, recent advances in digital neural network performance are largely driven by the rapid growth in the number of trainable parameters and, so far, demonstrated PNNs are lagging behind by several orders of magnitude in terms of scale. This mirrors size and performance constraints found in early digital neural networks. In that period, efficient reuse of parameters contributed to the development of parameter-efficient architectures such as convolutional neural networks. In this work, we numerically investigate hardware-friendly weight-tying for PNNs. Crucially, with many PNN systems, there is a time-scale separation between the fast dynamic active elements of the forward pass and the only slowly trainable elements implementing weights and biases. With this in mind,we propose the Reuse of Layers for eXpanding a Neural Network (ReLaX-Net) architecture, which employs a simple layer-by-layer time-multiplexing scheme to increase the effective network depth and efficiently use the number of parameters. We only require the addition of fast switches for existing PNNs. We validate ReLaX-Nets via numerical experiments on image classification and natural language processing tasks. Our results show that ReLaX-Net improves computational performance with only minor modifications to a conventional PNN. We observe a favorable scaling, where ReLaX-Nets exceed the performance of equivalent traditional RNNs or DNNs with the same number of parameters.</li>
<li><strong>摘要：</strong>物理神经网络（PNN）是下一代计算系统的有前途的平台。然而，数字神经网络性能的最新进展很大程度上是由可训练参数数量的快速增长推动的，到目前为止，事实证明 PNN 在规模上落后了几个数量级。这反映了早期数字神经网络中发现的尺寸和性能限制。在那段时期，参数的有效重用有助于参数高效架构（例如卷积神经网络）的发展。在这项工作中，我们对 PNN 的硬件友好权重绑定进行了数值研究。至关重要的是，对于许多 PNN 系统，前向传播的快速动态活动元素与实现权重和偏差的唯一缓慢可训练元素之间存在时间尺度分离。考虑到这一点，我们提出了扩展神经网络的层重用（ReLaX-Net）架构，该架构采用简单的逐层时间复用方案来增加有效网络深度并有效使用参数数量。我们只需要为现有 PNN 添加快速开关。我们通过图像分类和自然语言处理任务的数值实验来验证 ReLaX-Nets。我们的结果表明，ReLaX-Net 只需对传统 PNN 进行少量修改即可提高计算性能。我们观察到良好的扩展性，其中 ReLaX-Net 的性能超过了具有相同参数数量的等效传统 RNN 或 DNN。</li>
</ul>

<h3>Title: Calibrating and Rotating: A Unified Framework for Weight Conditioning in PEFT</h3>
<ul>
<li><strong>Authors: </strong>Da Chang, Peng Xue, Yu Li, Yongxiang Liu, Pengxiang Xu, Shixun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00051">https://arxiv.org/abs/2511.00051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00051">https://arxiv.org/pdf/2511.00051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00051]] Calibrating and Rotating: A Unified Framework for Weight Conditioning in PEFT(https://arxiv.org/abs/2511.00051)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Parameter-Efficient Fine-Tuning (PEFT) methods are crucial for adapting large pre-trained models. Among these, LoRA is considered a foundational approach. Building on this, the influential DoRA method enhances performance by decomposing weight updates into magnitude and direction. However, its underlying mechanism remains unclear, and it introduces significant computational overhead. In this work, we first identify that DoRA's success stems from its capacity to increase the singular value entropy of the weight update matrix, which promotes a more uniform update distribution akin to full fine-tuning. We then reformulate DoRA into a mathematically equivalent and more efficient matrix form, revealing it as a learnable weight conditioning method. Based on this insight, we propose a unified framework for designing advanced PEFT methods by exploring two orthogonal dimensions: the architectural placement and the transformation type of the conditioning matrix. Within this framework, we introduce two novel methods: (1) \textbf{Pre-Diag}, which applies a diagonal conditioning matrix before the LoRA update to efficiently calibrate the pre-trained weights, thereby enhancing performance while reducing training time; and (2) \textbf{S}kewed \textbf{O}rthogonal \textbf{R}otation \textbf{A}daptation (\textbf{SORA}), which employs a parameter-efficient orthogonal rotation to perform a more powerful, norm-preserving transformation of the feature space. Extensive experiments on natural language understanding and generation tasks demonstrate that our proposed methods achieve superior performance and efficiency compared to both LoRA and DoRA. The code is available at this https URL.</li>
<li><strong>摘要：</strong>参数高效微调（PEFT）方法对于适应大型预训练模型至关重要。其中，LoRA 被认为是一种基础方法。在此基础上，颇具影响力的 DoRA 方法通过将权重更新分解为幅度和方向来提高性能。然而，其底层机制仍不清楚，并且引入了巨大的计算开销。在这项工作中，我们首先确定 DoRA 的成功源于其增加权重更新矩阵的奇异值熵的能力，这促进了类似于完全微调的更均匀的更新分布。然后，我们将 DoRA 重新表述为数学上等效且更有效的矩阵形式，将其揭示为一种可学习的权重调节方法。基于这一见解，我们提出了一个统一的框架，通过探索两个正交维度：条件矩阵的架构布局和转换类型，来设计先进的 PEFT 方法。在此框架内，我们引入了两种新颖的方法：（1）\textbf{Pre-Diag}，它在LoRA更新之前应用对角条件矩阵来有效地校准预训练的权重，从而在提高性能的同时减少训练时间； (2) \textbf{S}kewed \textbf{O}rthogonal \textbf{R}otation \textbf{A}daptation (\textbf{SORA})，它采用参数有效的正交旋转来对特征空间执行更强大的、规范保持的变换。关于自然语言理解和生成任务的大量实验表明，与 LoRA 和 DoRA 相比，我们提出的方法实现了卓越的性能和效率。该代码可从此 https URL 获取。</li>
</ul>

<h3>Title: World Simulation with Video Foundation Models for Physical AI</h3>
<ul>
<li><strong>Authors: </strong>NVIDIA: Arslan Ali, Junjie Bai, Maciej Bala, Yogesh Balaji, Aaron Blakeman, Tiffany Cai, Jiaxin Cao, Tianshi Cao, Elizabeth Cha, Yu-Wei Chao, Prithvijit Chattopadhyay, Mike Chen, Yongxin Chen, Yu Chen, Shuai Cheng, Yin Cui, Jenna Diamond, Yifan Ding, Jiaojiao Fan, Linxi Fan, Liang Feng, Francesco Ferroni, Sanja Fidler, Xiao Fu, Ruiyuan Gao, Yunhao Ge, Jinwei Gu, Aryaman Gupta, Siddharth Gururani, Imad El Hanafi, Ali Hassani, Zekun Hao, Jacob Huffman, Joel Jang, Pooya Jannaty, Jan Kautz, Grace Lam, Xuan Li, Zhaoshuo Li, Maosheng Liao, Chen-Hsuan Lin, Tsung-Yi Lin, Yen-Chen Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Yifan Lu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Seungjun Nah, Yashraj Narang, Abhijeet Panaskar, Lindsey Pavao, Trung Pham, Morteza Ramezanali, Fitsum Reda, Scott Reed, Xuanchi Ren, Haonan Shao, Yue Shen, Stella Shi, Shuran Song, Bartosz Stefaniak, Shangkun Sun, Shitao Tang, Sameena Tasmeen, Lyne Tchapmi, Wei-Cheng Tseng, Jibin Varghese, Andrew Z. Wang, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Jiashu Xu, Dinghao Yang, Xiaodong Yang, Haotian Ye, Seonghyeon Ye, Xiaohui Zeng, Jing Zhang, Qinsheng Zhang, Kaiwen Zheng, Andrew Zhu, Yuke Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00062">https://arxiv.org/abs/2511.00062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00062">https://arxiv.org/pdf/2511.00062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00062]] World Simulation with Video Foundation Models for Physical AI(https://arxiv.org/abs/2511.00062)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World Foundation Models for Physical AI. Built on a flow-based architecture, [Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language model, to provide richer text grounding and finer control of world simulation. Trained on 200M curated video clips and refined with reinforcement learning-based post-training, [Cosmos-Predict2.5] achieves substantial improvements over [Cosmos-Predict1] in video quality and instruction alignment, with models released at 2B and 14B scales. These capabilities enable more reliable synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems. We further extend the family with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and Real2Real world translation. Despite being 3.5$\times$ smaller than [Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video generation. Together, these advances establish [Cosmos-Predict2.5] and [Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To accelerate research and deployment in Physical AI, we release source code, pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model License at this https URL and this https URL. We hope these open resources lower the barrier to adoption and foster innovation in building the next generation of embodied intelligence.</li>
<li><strong>摘要：</strong>我们介绍 [Cosmos-Predict2.5]，最新一代物理 AI 的 Cosmos 世界基础模型。 [Cosmos-Predict2.5] 基于流式架构构建，将 Text2World、Image2World 和 Video2World 生成统一在一个模型中，并利用物理 AI 视觉语言模型 [Cosmos-Reason1] 来提供更丰富的文本基础和对世界模拟的更精细控制。 [Cosmos-Predict2.5] 经过 2 亿个精选视频剪辑的训练，并通过基于强化学习的后期训练进行了改进，在视频质量和指令对齐方面比 [Cosmos-Predict1] 取得了实质性改进，并以 2B 和 14B 尺度发布了模型。这些功能可以为机器人和自主系统提供更可靠的合成数据生成、策略评估和闭环仿真。我们通过 [Cosmos-Transfer2.5] 进一步扩展了该系列，这是一个用于 Sim2Real 和 Real2Real 世界翻译的控制网风格框架。尽管比 [Cosmos-Transfer1] 小 3.5$\times$，但它提供了更高的保真度和强大的长视野视频生成。这些进步共同确立了 [Cosmos-Predict2.5] 和 [Cosmos-Transfer2.5] 作为扩展体现智能的多功能工具。为了加速物理 AI 的研究和部署，我们根据 NVIDIA 开放模型许可证在此 https URL 和此 https URL 发布源代码、预训练检查点和策划基准。我们希望这些开放资源能够降低采用的障碍，并促进构建下一代具体智能的创新。</li>
</ul>

<h3>Title: Benchmarking Generative AI Against Bayesian Optimization for Constrained Multi-Objective Inverse Design</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Bilal Awan, Abdul Razzaq, Abdul Shahid</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00070">https://arxiv.org/abs/2511.00070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00070">https://arxiv.org/pdf/2511.00070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00070]] Benchmarking Generative AI Against Bayesian Optimization for Constrained Multi-Objective Inverse Design(https://arxiv.org/abs/2511.00070)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>This paper investigates the performance of Large Language Models (LLMs) as generative optimizers for solving constrained multi-objective regression tasks, specifically within the challenging domain of inverse design (property-to-structure mapping). This problem, critical to materials informatics, demands finding complex, feasible input vectors that lie on the Pareto optimal front. While LLMs have demonstrated universal effectiveness across generative and reasoning tasks, their utility in constrained, continuous, high-dimensional numerical spaces tasks they weren't explicitly architected for remains an open research question. We conducted a rigorous comparative study between established Bayesian Optimization (BO) frameworks and a suite of fine-tuned LLMs and BERT models. For BO, we benchmarked the foundational BoTorch Ax implementation against the state-of-the-art q-Expected Hypervolume Improvement (qEHVI, BoTorchM). The generative approach involved fine-tuning models via Parameter-Efficient Fine-Tuning (PEFT), framing the challenge as a regression problem with a custom output head. Our results show that BoTorch qEHVI achieved perfect convergence (GD=0.0), setting the performance ceiling. Crucially, the best-performing LLM (WizardMath-7B) achieved a Generational Distance (GD) of 1.21, significantly outperforming the traditional BoTorch Ax baseline (GD=15.03). We conclude that specialized BO frameworks remain the performance leader for guaranteed convergence, but fine-tuned LLMs are validated as a promising, computationally fast alternative, contributing essential comparative metrics to the field of AI-driven optimization. The findings have direct industrial applications in optimizing formulation design for resins, polymers, and paints, where multi-objective trade-offs between mechanical, rheological, and chemical properties are critical to innovation and production efficiency.</li>
<li><strong>摘要：</strong>本文研究了大型语言模型（LLM）作为生成优化器解决约束多目标回归任务的性能，特别是在逆向设计（属性到结构映射）的挑战性领域。这个问题对于材料信息学至关重要，需要找到位于帕累托最优前沿的复杂、可行的输入向量。虽然法学硕士已经证明了在生成和推理任务中的普遍有效性，但它们在未明确设计的受限、连续、高维数字空间任务中的效用仍然是一个悬而未决的研究问题。我们对已建立的贝叶斯优化 (BO) 框架和一套经过微调的 LLM 和 BERT 模型进行了严格的比较研究。对于 BO，我们将基础 BoTorch Ax 实现与最先进的 q-Expected Hypervolume Improvement（qEHVI、BoTorchM）进行了基准测试。生成方法涉及通过参数高效微调（PEFT）对模型进行微调，将挑战视为具有自定义输出头的回归问题。我们的结果表明 BoTorch qEHVI 实现了完美收敛（GD=0.0），设定了性能上限。至关重要的是，表现最好的 LLM (WizardMath-7B) 的代际距离 (GD) 为 1.21，显着优于传统的 BoTorch Ax 基线 (GD=15.03)。我们的结论是，专门的 BO 框架仍然是保证收敛的性能领先者，但经过微调的 LLM 被验证为一种有前景、计算速度快的替代方案，为人工智能驱动的优化领域提供了重要的比较指标。这些发现在优化树脂、聚合物和涂料的配方设计方面具有直接的工业应用，其中机械、流变和化学性能之间的多目标权衡对于创新和生产效率至关重要。</li>
</ul>

<h3>Title: LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Huanlin Gao, Ping Chen, Fuyuan Shi, Chao Tan, Zhaoxiang Liu, Fang Zhao, Kai Wang, Shiguo Lian</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00090">https://arxiv.org/abs/2511.00090</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00090">https://arxiv.org/pdf/2511.00090</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00090]] LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation(https://arxiv.org/abs/2511.00090)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9x speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis. Our code is available at :this https URL</li>
<li><strong>摘要：</strong>我们推出了 LeMiCa，一种无需训练且高效的加速框架，用于基于扩散的视频生成。虽然现有的缓存策略主要侧重于减少局部启发式错误，但它们经常忽略全局错误的累积，导致加速视频和原始视频之间的内容明显下降。为了解决这个问题，我们将缓存调度制定为具有误差加权边的有向图，并引入词典最小最大路径优化策略，该策略明确限制最坏情况的路径误差。这种方法大大提高了生成框架之间全局内容和样式的一致性。对多个文本到视频基准的大量实验表明，LeMiCa 在推理速度和生成质量方面提供了双重改进。值得注意的是，我们的方法在 Latte 模型上实现了 2.9 倍的加速，并在 Open-Sora 上达到了 0.05 的 LPIPS 分数，优于之前的缓存技术。重要的是，这些收益伴随着最小的感知质量下降，使 LeMiCa 成为加速基于扩散的视频生成的强大且可推广的范例。我们相信这种方法可以为未来高效可靠的视频合成研究奠定坚实的基础。我们的代码可在以下位置找到：此 https URL</li>
</ul>

<h3>Title: Self-Improving Vision-Language-Action Models with Data Generation via Residual RL</h3>
<ul>
<li><strong>Authors: </strong>Wenli Xiao, Haotian Lin, Andy Peng, Haoru Xue, Tairan He, Yuqi Xie, Fengyuan Hu, Jimmy Wu, Zhengyi Luo, Linxi "Jim" Fan, Guanya Shi, Yuke Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00091">https://arxiv.org/abs/2511.00091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00091">https://arxiv.org/pdf/2511.00091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00091]] Self-Improving Vision-Language-Action Models with Data Generation via Residual RL(https://arxiv.org/abs/2511.00091)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Supervised fine-tuning (SFT) has become the de facto post-training strategy for large vision-language-action (VLA) models, but its reliance on costly human demonstrations limits scalability and generalization. We propose Probe, Learn, Distill (PLD), a three-stage plug-and-play framework that improves VLAs through residual reinforcement learning (RL) and distribution-aware data collection. In Stage 1, we train lightweight residual actors to probe failure regions of the VLA generalist. In Stage 2, we use a hybrid rollout scheme that aligns collected trajectories with the generalist's deployment distribution while capturing recovery behaviors. In Stage 3, we distill the curated trajectories back into the generalist with standard SFT. PLD achieves near-saturated 99% task success on LIBERO, over 50% gains in SimplerEnv, and 100% success on real-world Franka and YAM arm manipulation tasks. Ablations show that residual probing and distribution-aware replay are key to collecting deployment-aligned data that improves both seen and unseen tasks, offering a scalable path toward self-improving VLA models.</li>
<li><strong>摘要：</strong>有监督微调（SFT）已成为大型视觉语言动作（VLA）模型事实上的后训练策略，但其对昂贵的人类演示的依赖限制了可扩展性和泛化性。我们提出了 Probe、Learn、Distill (PLD)，这是一个三阶段即插即用框架，可通过残差强化学习 (RL) 和分布感知数据收集来改进 VLA。在第一阶段，我们训练轻量级剩余参与者来探测 VLA 通才的失败区域。在第二阶段，我们使用混合推出方案，将收集的轨迹与通才的部署分布保持一致，同时捕获恢复行为。在第 3 阶段，我们使用标准 SFT 将策划的轨迹提炼回通才。 PLD 在 LIBERO 上实现了近乎饱和的 99% 任务成功率，在 SimplerEnv 上实现了超过 50% 的增益，在现实世界的 Franka 和 YAM 手臂操作任务上实现了 100% 的成功。消融表明，残余探测和分布感知重放是收集与部署一致的数据的关键，这些数据可以改进可见和不可见的任务，从而为自我改进 VLA 模型提供可扩展的路径。</li>
</ul>

<h3>Title: A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation</h3>
<ul>
<li><strong>Authors: </strong>Marios Impraimakis, Evangelia Nektaria Palkanoglou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, eess.SP, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00099">https://arxiv.org/abs/2511.00099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00099">https://arxiv.org/pdf/2511.00099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00099]] A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation(https://arxiv.org/abs/2511.00099)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The optimization-based damage detection and damage state digital twinning capabilities are examined here of a novel conditional-labeled generative adversarial network methodology. The framework outperforms current approaches for fault anomaly detection as no prior information is required for the health state of the system: a topic of high significance for real-world applications. Specifically, current artificial intelligence-based digital twinning approaches suffer from the uncertainty related to obtaining poor predictions when a low number of measurements is available, physics knowledge is missing, or when the damage state is unknown. To this end, an unsupervised framework is examined and validated rigorously on the benchmark structural health monitoring measurements of Z24 Bridge: a post-tensioned concrete highway bridge in Switzerland. In implementing the approach, firstly, different same damage-level measurements are used as inputs, while the model is forced to converge conditionally to two different damage states. Secondly, the process is repeated for a different group of measurements. Finally, the convergence scores are compared to identify which one belongs to a different damage state. The process for both healthy-to-healthy and damage-to-healthy input data creates, simultaneously, measurements for digital twinning purposes at different damage states, capable of pattern recognition and machine learning data generation. Further to this process, a support vector machine classifier and a principal component analysis procedure is developed to assess the generated and real measurements of each damage category, serving as a secondary new dynamics learning indicator in damage scenarios. Importantly, the approach is shown to capture accurately damage over healthy measurements, providing a powerful tool for vibration-based system-level monitoring and scalable infrastructure resilience.</li>
<li><strong>摘要：</strong>这里检查了一种新颖的条件标记生成对抗网络方法的基于优化的损伤检测和损伤状态数字孪生功能。该框架优于当前的故障异常检测方法，因为系统的健康状态不需要先验信息：这对于现实世界的应用程序具有重要意义。具体来说，当前基于人工智能的数字孪生方法存在不确定性，当可用测量数量较少、物理知识缺失或损坏状态未知时，无法获得较差的预测。为此，我们在瑞士后张法混凝土公路桥 Z24 桥的基准结构健康监测测量中严格检查和验证了无监督框架。在实现该方法时，首先，使用不同的相同损坏级别测量作为输入，而模型被迫有条件地收敛到两种不同的损坏状态。其次，针对不同的测量组重复该过程。最后，比较收敛分数以确定哪一个属于不同的损坏状态。健康到健康和损伤到健康输入数据的过程同时创建不同损伤状态下数字孪生的测量结果，能够进行模式识别和机器学习数据生成。在此过程的进一步发展中，开发了支持向量机分类器和主成分分析程序来评估每个损坏类别的生成和实际测量，作为损坏场景中的辅助新动态学习指标。重要的是，该方法被证明可以准确捕获健康测量的损坏，为基于振动的系统级监控和可扩展的基础设施弹性提供强大的工具。</li>
</ul>

<h3>Title: FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video</h3>
<ul>
<li><strong>Authors: </strong>Rotem Ezra, Hedi Zisling, Nimrod Berman, Ilan Naiman, Alexey Gorkor, Liran Nochumsohn, Eliya Nachmani, Omri Azencot</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00103">https://arxiv.org/abs/2511.00103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00103">https://arxiv.org/pdf/2511.00103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00103]] FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video(https://arxiv.org/abs/2511.00103)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have become state-of-the-art generative models for images, audio, and video, yet enabling fine-grained controllable generation, i.e., continuously steering specific concepts without disturbing unrelated content, remains challenging. Concept Sliders (CS) offer a promising direction by discovering semantic directions through textual contrasts, but they require per-concept training and architecture-specific fine-tuning (e.g., LoRA), limiting scalability to new modalities. In this work we introduce FreeSliders, a simple yet effective approach that is fully training-free and modality-agnostic, achieved by partially estimating the CS formula during inference. To support modality-agnostic evaluation, we extend the CS benchmark to include both video and audio, establishing the first suite for fine-grained concept generation control with multiple modalities. We further propose three evaluation properties along with new metrics to improve evaluation quality. Finally, we identify an open problem of scale selection and non-linear traversals and introduce a two-stage procedure that automatically detects saturation points and reparameterizes traversal for perceptually uniform, semantically meaningful edits. Extensive experiments demonstrate that our method enables plug-and-play, training-free concept control across modalities, improves over existing baselines, and establishes new tools for principled controllable generation. An interactive presentation of our benchmark and method is available at: this https URL</li>
<li><strong>摘要：</strong>扩散模型已成为图像、音频和视频的最先进的生成模型，但实现细粒度的可控生成，即在不干扰不相关内容的情况下持续引导特定概念，仍然具有挑战性。概念滑块（CS）通过文本对比发现语义方向，提供了一个有前途的方向，但它们需要每个概念的训练和特定于架构的微调（例如 LoRA），限制了新模式的可扩展性。在这项工作中，我们介绍了 FreeSliders，这是一种简单而有效的方法，完全无需训练且与模态无关，通过在推理过程中部分估计 CS 公式来实现。为了支持与模态无关的评估，我们将 CS 基准扩展为包括视频和音频，建立了第一个用于具有多种模态的细粒度概念生成控制的套件。我们进一步提出了三个评估属性以及新的指标来提高评估质量。最后，我们确定了尺度选择和非线性遍历的开放问题，并引入了一个两阶段过程，该过程自动检测饱和点并重新参数化遍历，以实现感知均匀、语义上有意义的编辑。大量的实验表明，我们的方法能够实现跨模式的即插即用、免训练概念控制，改进现有基线，并为有原则的可控生成建立新工具。我们的基准和方法的交互式演示可在以下网址找到：此 https URL</li>
</ul>

<h3>Title: AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency</h3>
<ul>
<li><strong>Authors: </strong>Piyushkumar Patel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00107">https://arxiv.org/abs/2511.00107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00107">https://arxiv.org/pdf/2511.00107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00107]] AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency(https://arxiv.org/abs/2511.00107)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Text to video generation has emerged as a critical frontier in generative artificial intelligence, yet existing approaches struggle with maintaining temporal consistency, compositional understanding, and fine grained control over visual narratives. We present MOVAI (Multimodal Original Video AI), a novel hierarchical framework that integrates compositional scene understanding with temporal aware diffusion models for high fidelity text to video synthesis. Our approach introduces three key innovations: (1) a Compositional Scene Parser (CSP) that decomposes textual descriptions into hierarchical scene graphs with temporal annotations, (2) a Temporal-Spatial Attention Mechanism (TSAM) that ensures coherent motion dynamics across frames while preserving spatial details, and (3) a Progressive Video Refinement (PVR) module that iteratively enhances video quality through multi-scale temporal reasoning. Extensive experiments on standard benchmarks demonstrate that MOVAI achieves state-of-the-art performance, improving video quality metrics by 15.3% in LPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existing methods. Our framework shows particular strength in generating complex multi-object scenes with realistic temporal dynamics and fine-grained semantic control.</li>
<li><strong>摘要：</strong>文本到视频生成已成为生成人工智能的关键前沿，但现有方法在维持时间一致性、构图理解和对视觉叙事的细粒度控制方面存在困难。我们提出了 MOVAI（多模态原始视频 AI），这是一种新颖的分层框架，它将构图场景理解与时间感知扩散模型相结合，以实现高保真文本到视频的合成。我们的方法引入了三个关键创新：(1) 组合场景解析器 (CSP)，将文本描述分解为带有时间注释的分层场景图；(2) 时空注意力机制 (TSAM)，确保跨帧的连贯运动动态，同时保留空间细节；(3) 渐进视频细化 (PVR) 模块，通过多尺度时间推理迭代增强视频质量。对标准基准的大量实验表明，MOVAI 实现了最先进的性能，与现有方法相比，LPIPS 中的视频质量指标提高了 15.3%，FVD 中提高了 12.7%，用户偏好研究中提高了 18.9%。我们的框架在生成具有现实时间动态和细粒度语义控制的复杂多对象场景方面表现出特殊的优势。</li>
</ul>

<h3>Title: Chain of Time: In-Context Physical Simulation with Image Generation Models</h3>
<ul>
<li><strong>Authors: </strong>YingQiao Wang, Eric Bigelow, Boyi Li, Tomer Ullman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00110">https://arxiv.org/abs/2511.00110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00110">https://arxiv.org/pdf/2511.00110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00110]] Chain of Time: In-Context Physical Simulation with Image Generation Models(https://arxiv.org/abs/2511.00110)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We propose a novel cognitively-inspired method to improve and interpret physical simulation in vision-language models. Our ``Chain of Time" method involves generating a series of intermediate images during a simulation, and it is motivated by in-context reasoning in machine learning, as well as mental simulation in humans. Chain of Time is used at inference time, and requires no additional fine-tuning. We apply the Chain-of-Time method to synthetic and real-world domains, including 2-D graphics simulations and natural 3-D videos. These domains test a variety of particular physical properties, including velocity, acceleration, fluid dynamics, and conservation of momentum. We found that using Chain-of-Time simulation substantially improves the performance of a state-of-the-art image generation model. Beyond examining performance, we also analyzed the specific states of the world simulated by an image model at each time step, which sheds light on the dynamics underlying these simulations. This analysis reveals insights that are hidden from traditional evaluations of physical reasoning, including cases where an image generation model is able to simulate physical properties that unfold over time, such as velocity, gravity, and collisions. Our analysis also highlights particular cases where the image generation model struggles to infer particular physical parameters from input images, despite being capable of simulating relevant physical processes.</li>
<li><strong>摘要：</strong>我们提出了一种新颖的认知启发方法来改进和解释视觉语言模型中的物理模拟。我们的“时间链”方法涉及在模拟过程中生成一系列中间图像，其动机是机器学习中的上下文推理以及人类的心理模拟。时间链用于推理时间，不需要额外的微调。我们将时间链方法应用于合成和现实世界领域，包括 2D 图形模拟和自然 3D 视频。这些领域测试各种特定的物理属性，包括速度、加速度、流体动力学和守恒我们发现，使用时间链模拟可以显着提高最先进的图像生成模型的性能。除了检查性能之外，我们还分析了图像模型在每个时间步模拟的世界的特定状态，这揭示了这些模拟背后的动态，包括图像生成模型能够模拟随时间推移而展开的物理属性的情况，例如速度、重力和碰撞。尽管能够模拟相关的物理过程，但生成模型仍难以从输入图像中推断出特定的物理参数。</li>
</ul>

<h3>Title: End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning</h3>
<ul>
<li><strong>Authors: </strong>Hanae Elmekki, Amanda Spilkin, Ehsan Zakeri, Antonela Mariel Zanuttini, Ahmed Alagha, Hani Sami, Jamal Bentahar, Lyes Kadem, Wen-Fang Xie, Philippe Pibarot, Rabeb Mizouni, Hadi Otrok, Azzam Mourad, Sami Muhaidat</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00114">https://arxiv.org/abs/2511.00114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00114">https://arxiv.org/pdf/2511.00114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00114]] End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning(https://arxiv.org/abs/2511.00114)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Cardiac ultrasound (US) is among the most widely used diagnostic tools in cardiology for assessing heart health, but its effectiveness is limited by operator dependence, time constraints, and human error. The shortage of trained professionals, especially in remote areas, further restricts access. These issues underscore the need for automated solutions that can ensure consistent, and accessible cardiac imaging regardless of operator skill or location. Recent progress in artificial intelligence (AI), especially in deep reinforcement learning (DRL), has gained attention for enabling autonomous decision-making. However, existing DRL-based approaches to cardiac US scanning lack reproducibility, rely on proprietary data, and use simplified models. Motivated by these gaps, we present the first end-to-end framework that integrates generative AI and DRL to enable autonomous and reproducible cardiac US scanning. The framework comprises two components: (i) a conditional generative simulator combining Generative Adversarial Networks (GANs) with Variational Autoencoders (VAEs), that models the cardiac US environment producing realistic action-conditioned images; and (ii) a DRL module that leverages this simulator to learn autonomous, accurate scanning policies. The proposed framework delivers AI-driven guidance through expert-validated models that classify image type and assess quality, supports conditional generation of realistic US images, and establishes a reproducible foundation extendable to other organs. To ensure reproducibility, a publicly available dataset of real cardiac US scans is released. The solution is validated through several experiments. The VAE-GAN is benchmarked against existing GAN variants, with performance assessed using qualitative and quantitative approaches, while the DRL-based scanning system is evaluated under varying configurations to demonstrate effectiveness.</li>
<li><strong>摘要：</strong>心脏超声（US）是心脏病学中用于评估心脏健康状况最广泛使用的诊断工具之一，但其有效性受到操作员依赖、时间限制和人为错误的限制。缺乏训练有素的专业人员，特别是在偏远地区，进一步限制了准入。这些问题强调了对自动化解决方案的需求，无论操作员技能如何或位置如何，都可以确保一致且易于访问的心脏成像。人工智能（AI）的最新进展，特别是深度强化学习（DRL），因实现自主决策而受到关注。然而，现有的基于 DRL 的心脏超声扫描方法缺乏可重复性，依赖专有数据并使用简化的模型。在这些差距的推动下，我们提出了第一个端到端框架，该框架集成了生成式人工智能和 DRL，以实现自主且可重复的心脏超声扫描。该框架由两个部分组成：（i）条件生成模拟器，将生成对抗网络（GAN）与变分自动编码器（VAE）相结合，对心脏超声环境进行建模，生成逼真的动作条件图像； (ii) DRL 模块，利用该模拟器来学习自主、准确的扫描策略。拟议的框架通过专家验证的模型提供人工智能驱动的指导，对图像类型进行分类并评估质量，支持有条件生成逼真的美国图像，并建立可扩展到其他器官的可重复基础。为了确保可重复性，发布了真实心脏超声扫描的公开数据集。该解决方案通过多次实验得到验证。 VAE-GAN 以现有的 GAN 变体为基准，使用定性和定量方法评估性能，同时在不同的配置下评估基于 DRL 的扫描系统以证明有效性。</li>
</ul>

<h3>Title: DCcluster-Opt: Benchmarking Dynamic Multi-Objective Optimization for Geo-Distributed Data Center Workloads</h3>
<ul>
<li><strong>Authors: </strong>Antonio Guillen-Perez, Avisek Naug, Vineet Gundecha, Sahand Ghorbanpour, Ricardo Luna Gutierrez, Ashwin Ramesh Babu, Munther Salim, Shubhanker Banerjee, Eoin H. Oude Essink, Damien Fay, Soumyendu Sarkar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MA, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00117">https://arxiv.org/abs/2511.00117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00117">https://arxiv.org/pdf/2511.00117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00117]] DCcluster-Opt: Benchmarking Dynamic Multi-Objective Optimization for Geo-Distributed Data Center Workloads(https://arxiv.org/abs/2511.00117)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The increasing energy demands and carbon footprint of large-scale AI require intelligent workload management in globally distributed data centers. Yet progress is limited by the absence of benchmarks that realistically capture the interplay of time-varying environmental factors (grid carbon intensity, electricity prices, weather), detailed data center physics (CPUs, GPUs, memory, HVAC energy), and geo-distributed network dynamics (latency and transmission costs). To bridge this gap, we present DCcluster-Opt: an open-source, high-fidelity simulation benchmark for sustainable, geo-temporal task scheduling. DCcluster-Opt combines curated real-world datasets, including AI workload traces, grid carbon intensity, electricity markets, weather across 20 global regions, cloud transmission costs, and empirical network delay parameters with physics-informed models of data center operations, enabling rigorous and reproducible research in sustainable computing. It presents a challenging scheduling problem where a top-level coordinating agent must dynamically reassign or defer tasks that arrive with resource and service-level agreement requirements across a configurable cluster of data centers to optimize multiple objectives. The environment also models advanced components such as heat recovery. A modular reward system enables an explicit study of trade-offs among carbon emissions, energy costs, service level agreements, and water use. It provides a Gymnasium API with baseline controllers, including reinforcement learning and rule-based strategies, to support reproducible ML research and a fair comparison of diverse algorithms. By offering a realistic, configurable, and accessible testbed, DCcluster-Opt accelerates the development and validation of next-generation sustainable computing solutions for geo-distributed data centers.</li>
<li><strong>摘要：</strong>大规模人工智能不断增长的能源需求和碳足迹需要在全球分布的数据中心进行智能工作负载管理。然而，由于缺乏能够真实捕捉随时间变化的环境因素（电网碳强度、电价、天气）、详细数据中心物理（CPU、GPU、内存、HVAC 能源）和地理分布式网络动态（延迟和传输成本）之间相互作用的基准，进展受到限制。为了弥补这一差距，我们推出了 DCcluster-Opt：一个用于可持续地理时间任务调度的开源高保真模拟基准。 DCcluster-Opt 将精心策划的现实世界数据集（包括人工智能工作负载轨迹、电网碳强度、电力市场、全球 20 个地区的天气、云传输成本和经验网络延迟参数）与数据中心运营的物理模型相结合，从而实现可持续计算方面的严格且可重复的研究。它提出了一个具有挑战性的调度问题，其中顶级协调代理必须动态地重新分配或推迟跨可配置的数据中心集群的资源和服务级别协议要求到达的任务，以优化多个目标。该环境还对热回收等先进组件进行建模。模块化奖励系统可以对碳排放、能源成本、服务水平协议和用水之间的权衡进行明确的研究。它提供了带有基线控制器的 Gymnasium API，包括强化学习和基于规则的策略，以支持可重复的 ML 研究和不同算法的公平比较。通过提供现实、可配置且可访问的测试台，DCcluster-Opt 加速了地理分布式数据中心的下一代可持续计算解决方案的开发和验证。</li>
</ul>

<h3>Title: Cross-fluctuation phase transitions reveal sampling dynamics in diffusion models</h3>
<ul>
<li><strong>Authors: </strong>Sai Niranjan Ramachandran, Manish Krishan Lal, Suvrit Sra</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00124">https://arxiv.org/abs/2511.00124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00124">https://arxiv.org/pdf/2511.00124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00124]] Cross-fluctuation phase transitions reveal sampling dynamics in diffusion models(https://arxiv.org/abs/2511.00124)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We analyse how the sampling dynamics of distributions evolve in score-based diffusion models using cross-fluctuations, a centered-moment statistic from statistical physics. Specifically, we show that starting from an unbiased isotropic normal distribution, samples undergo sharp, discrete transitions, eventually forming distinct events of a desired distribution while progressively revealing finer structure. As this process is reversible, these transitions also occur in reverse, where intermediate states progressively merge, tracing a path back to the initial distribution. We demonstrate that these transitions can be detected as discontinuities in $n^{\text{th}}$-order cross-fluctuations. For variance-preserving SDEs, we derive a closed-form for these cross-fluctuations that is efficiently computable for the reverse trajectory. We find that detecting these transitions directly boosts sampling efficiency, accelerates class-conditional and rare-class generation, and improves two zero-shot tasks--image classification and style transfer--without expensive grid search or retraining. We also show that this viewpoint unifies classical coupling and mixing from finite Markov chains with continuous dynamics while extending to stochastic SDEs and non Markovian samplers. Our framework therefore bridges discrete Markov chain theory, phase analysis, and modern generative modeling.</li>
<li><strong>摘要：</strong>我们使用交叉波动（统计物理学的中心矩统计量）来分析分布的采样动态如何在基于分数的扩散模型中演变。具体来说，我们表明，从无偏各向同性正态分布开始，样本经历急剧、离散的转变，最终形成所需分布的不同事件，同时逐渐揭示更精细的结构。由于这个过程是可逆的，这些转变也会反向发生，中间状态逐渐合并，追踪回到初始分布的路径。我们证明这些转变可以被检测为 $n^{\text{th}}$ 阶交叉波动中的不连续性。对于保留方差的 SDE，我们推导了这些交叉波动的封闭形式，可以有效地计算反向轨迹。我们发现，检测这些转换可以直接提高采样效率，加速类条件和稀有类的生成，并改进两个零样本任务——图像分类和风格迁移——而无需昂贵的网格搜索或再训练。我们还表明，这种观点将有限马尔可夫链的经典耦合和混合与连续动力学相结合，同时扩展到随机 SDE 和非马尔可夫采样器。因此，我们的框架连接了离散马尔可夫链理论、阶段分析和现代生成模型。</li>
</ul>

<h3>Title: From Evidence to Verdict: An Agent-Based Forensic Framework for AI-Generated Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Mengfei Liang, Yiting Qu, Yukun Jiang, Michael Backes, Yang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00181">https://arxiv.org/abs/2511.00181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00181">https://arxiv.org/pdf/2511.00181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00181]] From Evidence to Verdict: An Agent-Based Forensic Framework for AI-Generated Image Detection(https://arxiv.org/abs/2511.00181)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid evolution of AI-generated images poses unprecedented challenges to information integrity and media authenticity. Existing detection approaches suffer from fundamental limitations: traditional classifiers lack interpretability and fail to generalize across evolving generative models, while vision-language models (VLMs), despite their promise, remain constrained to single-shot analysis and pixel-level reasoning. To address these challenges, we introduce AIFo (Agent-based Image Forensics), a novel training-free framework that emulates human forensic investigation through multi-agent collaboration. Unlike conventional methods, our framework employs a set of forensic tools, including reverse image search, metadata extraction, pre-trained classifiers, and VLM analysis, coordinated by specialized LLM-based agents that collect, synthesize, and reason over cross-source evidence. When evidence is conflicting or insufficient, a structured multi-agent debate mechanism allows agents to exchange arguments and reach a reliable conclusion. Furthermore, we enhance the framework with a memory-augmented reasoning module that learns from historical cases to improve future detection accuracy. Our comprehensive evaluation spans 6,000 images across both controlled laboratory settings and challenging real-world scenarios, including images from modern generative platforms and diverse online sources. AIFo achieves 97.05% accuracy, substantially outperforming traditional classifiers and state-of-the-art VLMs. These results demonstrate that agent-based procedural reasoning offers a new paradigm for more robust, interpretable, and adaptable AI-generated image detection.</li>
<li><strong>摘要：</strong>人工智能生成图像的快速发展对信息完整性和媒体真实性提出了前所未有的挑战。现有的检测方法存在根本性的局限性：传统的分类器缺乏可解释性，无法在不断发展的生成模型中进行泛化，而视觉语言模型（VLM）尽管前景广阔，但仍然局限于单次分析和像素级推理。为了应对这些挑战，我们引入了 AIFo（基于代理的图像取证），这是一种新颖的免训练框架，通过多代理协作模拟人类取证调查。与传统方法不同，我们的框架采用了一组取证工具，包括反向图像搜索、元数据提取、预训练分类器和 VLM 分析，并由专门的基于 LLM 的代理协调，这些代理收集、合成和推理跨源证据。当证据相互矛盾或不充分时，结构化的多主体辩论机制允许主体交换论据并得出可靠的结论。此外，我们通过记忆增强推理模块增强了框架，该模块可以从历史案例中学习，以提高未来的检测准确性。我们的综合评估涵盖了 6,000 张图像，涵盖受控实验室环境和具有挑战性的现实世界场景，包括来自现代生成平台和各种在线来源的图像。 AIFo 的准确率达到 97.05%，大大优于传统分类器和最先进的 VLM。这些结果表明，基于代理的程序推理为更强大、可解释和适应性更强的人工智能生成图像检测提供了新的范例。</li>
</ul>

<h3>Title: PDE-SHARP: PDE Solver Hybrids Through Analysis & Refinement Passes</h3>
<ul>
<li><strong>Authors: </strong>Shaghayegh Fazliani, Madeleine Udell</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00183">https://arxiv.org/abs/2511.00183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00183">https://arxiv.org/pdf/2511.00183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00183]] PDE-SHARP: PDE Solver Hybrids Through Analysis & Refinement Passes(https://arxiv.org/abs/2511.00183)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Current LLM-driven approaches using test-time computing to generate PDE solvers execute a large number of solver samples to identify high-accuracy solvers. These paradigms are especially costly for complex PDEs requiring substantial computational resources for numerical evaluation. We introduce PDE-SHARP, a framework to reduce computational costs by replacing expensive scientific computation by cheaper LLM inference that achieves superior solver accuracy with 60-75% fewer computational evaluations. PDE-SHARP employs three stages: (1) Analysis: mathematical chain-of-thought analysis including PDE classification, solution type detection, and stability analysis; (2) Genesis: solver generation based on mathematical insights from the previous stage; and (3) Synthesis: collaborative selection-hybridization tournaments in which LLM judges iteratively refine implementations through flexible performance feedback. To generate high-quality solvers, PDE-SHARP requires fewer than 13 solver evaluations on average compared to 30+ for baseline methods, improving accuracy uniformly across tested PDEs by $4\times$ on average, and demonstrates robust performance across LLM architectures, from general-purpose to specialized reasoning models.</li>
<li><strong>摘要：</strong>当前的 LLM 驱动方法使用测试时计算来生成 PDE 求解器，执行大量求解器样本来识别高精度求解器。对于需要大量计算资源进行数值评估的复杂偏微分方程来说，这些范例的成本特别高。我们引入了 PDE-SHARP，这是一种通过用更便宜的 LLM 推理取代昂贵的科学计算来降低计算成本的框架，该推理可实现卓越的求解器精度，同时减少 60-75% 的计算评估。 PDE-SHARP采用三个阶段：（1）分析：数学思路分析，包括PDE分类、解类型检测和稳定性分析； (2) Genesis：基于前一阶段的数学见解生成求解器； (3)综合：协作选择混合锦标赛，其中法学硕士评委通过灵活的表现反馈迭代地完善实施。为了生成高质量的求解器，PDE-SHARP 平均需要少于 13 次求解器评估，而基线方法需要 30 多个求解器评估，将测试的 PDE 的准确性平均提高 4 倍，并在从通用到专用推理模型的 LLM 架构中展示了强大的性能。</li>
</ul>

<h3>Title: Diffusion LLMs are Natural Adversaries for any LLM</h3>
<ul>
<li><strong>Authors: </strong>David Lüdke, Tom Wollschläger, Paul Ungermann, Stephan Günnemann, Leo Schwinn</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00203">https://arxiv.org/abs/2511.00203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00203">https://arxiv.org/pdf/2511.00203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00203]] Diffusion LLMs are Natural Adversaries for any LLM(https://arxiv.org/abs/2511.00203)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We introduce a novel framework that transforms the resource-intensive (adversarial) prompt optimization problem into an \emph{efficient, amortized inference task}. Our core insight is that pretrained, non-autoregressive generative LLMs, such as Diffusion LLMs, which model the joint distribution over prompt-response pairs, can serve as powerful surrogates for prompt search. This approach enables the direct conditional generation of prompts, effectively replacing costly, per-instance discrete optimization with a small number of parallelizable samples. We provide a probabilistic analysis demonstrating that under mild fidelity assumptions, only a few conditional samples are required to recover high-reward (harmful) prompts. Empirically, we find that the generated prompts are low-perplexity, diverse jailbreaks that exhibit strong transferability to a wide range of black-box target models, including robustly trained and proprietary LLMs. Beyond adversarial prompting, our framework opens new directions for red teaming, automated prompt optimization, and leveraging emerging Flow- and Diffusion-based LLMs.</li>
<li><strong>摘要：</strong>我们引入了一种新颖的框架，它将资源密集型（对抗性）提示优化问题转化为\emph{高效、摊销推理任务}。我们的核心见解是，预训练的非自回归生成 LLM（例如扩散 LLM）可以对提示响应对的联合分布进行建模，可以作为提示搜索的强大替代品。这种方法可以直接有条件地生成提示，从而有效地用少量的可并行样本取代成本高昂的按实例离散优化。我们提供了概率分析，证明在温和保真度假设下，只需要少量条件样本即可恢复高奖励（有害）提示。根据经验，我们发现生成的提示是低复杂度、多样化的越狱，表现出对各种黑盒目标模型的强大可转移性，包括经过严格训练的专有法学硕士。除了对抗性提示之外，我们的框架还为红队、自动提示优化以及利用新兴的基于流动和扩散的法学硕士开辟了新的方向。</li>
</ul>

<h3>Title: Diffusion Models at the Drug Discovery Frontier: A Review on Generating Small Molecules versus Therapeutic Peptides</h3>
<ul>
<li><strong>Authors: </strong>Yiquan Wang, Yahui Ma, Yuhan Chang, Jiayao Yan, Jialin Zhang, Minnuo Cai, Kai Wei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.BM, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00209">https://arxiv.org/abs/2511.00209</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00209">https://arxiv.org/pdf/2511.00209</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00209]] Diffusion Models at the Drug Discovery Frontier: A Review on Generating Small Molecules versus Therapeutic Peptides(https://arxiv.org/abs/2511.00209)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as a leading framework in generative modeling, showing significant potential to accelerate and transform the traditionally slow and costly process of drug discovery. This review provides a systematic comparison of their application in designing two principal therapeutic modalities: small molecules and therapeutic peptides. We analyze how a unified framework of iterative denoising is adapted to the distinct molecular representations, chemical spaces, and design objectives of each modality. For small molecules, these models excel at structure-based design, generating novel, pocket-fitting ligands with desired physicochemical properties, yet face the critical hurdle of ensuring chemical synthesizability. Conversely, for therapeutic peptides, the focus shifts to generating functional sequences and designing de novo structures, where the primary challenges are achieving biological stability against proteolysis, ensuring proper folding, and minimizing immunogenicity. Despite these distinct challenges, both domains face shared hurdles: the need for more accurate scoring functions, the scarcity of high-quality experimental data, and the crucial requirement for experimental validation. We conclude that the full potential of diffusion models will be unlocked by bridging these modality-specific gaps and integrating them into automated, closed-loop Design-Build-Test-Learn (DBTL) platforms, thereby shifting the paradigm from chemical exploration to the targeted creation of novel therapeutics.</li>
<li><strong>摘要：</strong>扩散模型已成为生成建模的领先框架，显示出加速和改变传统缓慢且昂贵的药物发现过程的巨大潜力。本综述对它们在设计两种主要治疗方式（小分子和治疗肽）中的应用进行了系统比较。我们分析了迭代去噪的统一框架如何适应每种模态的不同分子表示、化学空间和设计目标。对于小分子，这些模型擅长基于结构的设计，生成具有所需理化性质的新型、袖珍型配体，但面临着确保化学合成性的关键障碍。相反，对于治疗性肽，重点转移到生成功能序列和设计从头结构，其中主要挑战是实现抗蛋白水解的生物稳定性、确保正确折叠和最小化免疫原性。尽管存在这些不同的挑战，但这两个领域都面临着共同的障碍：需要更准确的评分函数、高质量实验数据的稀缺以及实验验证的关键要求。我们的结论是，通过弥合这些特定于模态的差距并将其集成到自动化、闭环设计-构建-测试-学习（DBTL）平台中，将释放扩散模型的全部潜力，从而将范式从化学探索转变为有针对性地创造新疗法。</li>
</ul>

<h3>Title: Iterative Foundation Model Fine-Tuning on Multiple Rewards</h3>
<ul>
<li><strong>Authors: </strong>Pouya M. Ghari, Simone Sciabola, Ye Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00220">https://arxiv.org/abs/2511.00220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00220">https://arxiv.org/pdf/2511.00220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00220]] Iterative Foundation Model Fine-Tuning on Multiple Rewards(https://arxiv.org/abs/2511.00220)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Fine-tuning foundation models has emerged as a powerful approach for generating objects with specific desired properties. Reinforcement learning (RL) provides an effective framework for this purpose, enabling models to generate outputs that maximize a given reward function. However, in many applications such as text generation and drug discovery, it can be suboptimal to optimize using a single reward signal, as multiple evaluation criteria are often necessary. This paper proposes a novel reinforcement learning-based method for fine-tuning foundation models using multiple reward signals. By employing an iterative fine-tuning strategy across these rewards, our approach generalizes state-of-the-art RL-based methods. We further provide a theoretical analysis that offers insights into the performance of multi-reward RL fine-tuning. Experimental results across diverse domains including text, biological sequence, and small molecule generation, demonstrate the effectiveness of the proposed algorithm compared to state-of-the-art baselines.</li>
<li><strong>摘要：</strong>微调基础模型已成为生成具有特定所需属性的对象的强大方法。强化学习 (RL) 为此目的提供了一个有效的框架，使模型能够生成最大化给定奖励函数的输出。然而，在文本生成和药物发现等许多应用中，使用单个奖励信号进行优化可能不是最理想的，因为通常需要多个评估标准。本文提出了一种基于强化学习的新颖方法，用于使用多个奖励信号微调基础模型。通过对这些奖励采用迭代微调策略，我们的方法概括了最先进的基于强化学习的方法。我们进一步提供了理论分析，为多奖励 RL 微调的性能提供了见解。跨不同领域（包括文本、生物序列和小分子生成）的实验结果证明了所提出的算法与最先进的基线相比的有效性。</li>
</ul>

<h3>Title: Object-Aware 4D Human Motion Generation</h3>
<ul>
<li><strong>Authors: </strong>Shurui Gui, Deep Anil Patel, Xiner Li, Martin Renqiang Min</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00248">https://arxiv.org/abs/2511.00248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00248">https://arxiv.org/pdf/2511.00248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00248]] Object-Aware 4D Human Motion Generation(https://arxiv.org/abs/2511.00248)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in video diffusion models have enabled the generation of high-quality videos. However, these videos still suffer from unrealistic deformations, semantic violations, and physical inconsistencies that are largely rooted in the absence of 3D physical priors. To address these challenges, we propose an object-aware 4D human motion generation framework grounded in 3D Gaussian representations and motion diffusion priors. With pre-generated 3D humans and objects, our method, Motion Score Distilled Interaction (MSDI), employs the spatial and prompt semantic information in large language models (LLMs) and motion priors through the proposed Motion Diffusion Score Distillation Sampling (MSDS). The combination of MSDS and LLMs enables our spatial-aware motion optimization, which distills score gradients from pre-trained motion diffusion models, to refine human motion while respecting object and semantic constraints. Unlike prior methods requiring joint training on limited interaction datasets, our zero-shot approach avoids retraining and generalizes to out-of-distribution object aware human motions. Experiments demonstrate that our framework produces natural and physically plausible human motions that respect 3D spatial context, offering a scalable solution for realistic 4D generation.</li>
<li><strong>摘要：</strong>视频扩散模型的最新进展使得高质量视频的生成成为可能。然而，这些视频仍然存在不切实际的变形、语义违规和物理不一致，这在很大程度上源于缺乏 3D 物理先验。为了应对这些挑战，我们提出了一种基于 3D 高斯表示和运动扩散先验的对象感知 4D 人体运动生成框架。通过预先生成的 3D 人类和物体，我们的方法“运动得分蒸馏交互 (MSDI)”通过提出的运动扩散得分蒸馏采样 (MSDS)，利用大语言模型 (LLM) 和运动先验中的空间和提示语义信息。 MSDS 和 LLM 的结合使我们能够进行空间感知运动优化，从预先训练的运动扩散模型中提取分数梯度，以在尊重对象和语义约束的同时细化人体运动。与之前需要在有限交互数据集上进行联合训练的方法不同，我们的零样本方法避免了重新训练并泛化到分布外的对象感知人体运动。实验表明，我们的框架可产生自然且物理上合理的人体运动，尊重 3D 空间背景，为逼真的 4D 生成提供可扩展的解决方案。</li>
</ul>

<h3>Title: Multi-View Consistent Human Image Customization via In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Hengjia Li, Jianjin Xu, Keli Cheng, Lei Wang, Ning Bi, Boxi Wu, Fernando De la Torre, Deng Cai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00293">https://arxiv.org/abs/2511.00293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00293">https://arxiv.org/pdf/2511.00293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00293]] Multi-View Consistent Human Image Customization via In-Context Learning(https://arxiv.org/abs/2511.00293)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in personalized generative models demonstrate impressive results in creating identity-consistent images of the same person under diverse settings. Yet, we note that most methods cannot control the viewpoint of the generated image, nor generate consistent multiple views of the person. To address this problem, we propose a lightweight adaptation method, PersonalView, capable of enabling an existing model to acquire multi-view generation capability with as few as 100 training samples. PersonalView consists of two key components: First, we design a conditioning architecture to take advantage of the in-context learning ability of the pre-trained diffusion transformer. Second, we preserve the original generative ability of the pretrained model with a new Semantic Correspondence Alignment Loss. We evaluate the multi-view consistency, text alignment, identity similarity, and visual quality of PersonalView and compare it to recent baselines with potential capability of multi-view customization. PersonalView significantly outperforms baselines trained on a large corpus of multi-view data with only 100 training samples.</li>
<li><strong>摘要：</strong>个性化生成模型的最新进展表明，在不同环境下创建同一个人的身份一致图像方面取得了令人印象深刻的结果。然而，我们注意到大多数方法无法控制生成图像的视点，也无法生成人物的一致的多个视图。为了解决这个问题，我们提出了一种轻量级的自适应方法PersonalView，能够使现有模型以少至100个训练样本获得多视图生成能力。 PersonalView 由两个关键组件组成：首先，我们设计了一个调节架构，以利用预训练扩散变压器的上下文学习能力。其次，我们通过新的语义对应对齐损失保留了预训练模型的原始生成能力。我们评估了 PersonalView 的多视图一致性、文本对齐、身份相似性和视觉质量，并将其与具有多视图定制潜在能力的最新基线进行比较。 PersonalView 显着优于在仅 100 个训练样本的大型多视图数据语料库上训练的基线。</li>
</ul>

<h3>Title: A Technical Exploration of Causal Inference with Hybrid LLM Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Dana Kim, Yichen Xu, Tiffany Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00318">https://arxiv.org/abs/2511.00318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00318">https://arxiv.org/pdf/2511.00318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00318]] A Technical Exploration of Causal Inference with Hybrid LLM Synthetic Data(https://arxiv.org/abs/2511.00318)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) offer a flexible means to generate synthetic tabular data, yet existing approaches often fail to preserve key causal parameters such as the average treatment effect (ATE). In this technical exploration, we first demonstrate that state-of-the-art synthetic data generators, both GAN- and LLM-based, can achieve high predictive fidelity while substantially misestimating causal effects. To address this gap, we propose a hybrid generation framework that combines model-based covariate synthesis (monitored via distance-to-closest-record filtering) with separately learned propensity and outcome models, thereby ensuring that (W, A, Y) triplets retain their underlying causal structure. We further introduce a synthetic pairing strategy to mitigate positivity violations and a realistic evaluation protocol that leverages unlimited synthetic samples to benchmark traditional estimators (IPTW, AIPW, substitution) under complex covariate distributions. This work lays the groundwork for LLM-powered data pipelines that support robust causal analysis. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 提供了一种灵活的方法来生成合成表格数据，但现有方法通常无法保留关键因果参数，例如平均治疗效果 (ATE)。在这项技术探索中，我们首先证明了基于 GAN 和 LLM 的最先进的合成数据生成器可以实现高预测保真度，同时大大错误估计因果效应。为了解决这一差距，我们提出了一种混合生成框架，它将基于模型的协变量合成（通过距离最近记录过滤进行监控）与单独学习的倾向和结果模型相结合，从而确保（W，A，Y）三元组保留其潜在的因果结构。我们进一步引入了一种合成配对策略来减轻积极性违规，以及一种现实的评估协议，该协议利用无限的合成样本在复杂的协变量分布下对传统估计器（IPTW、AIPW、替换）进行基准测试。这项工作为支持稳健因果分析的法学硕士支持的数据管道奠定了基础。我们的代码可以在这个 https URL 上找到。</li>
</ul>

<h3>Title: OSMGen: Highly Controllable Satellite Image Synthesis using OpenStreetMap Data</h3>
<ul>
<li><strong>Authors: </strong>Amir Ziashahabi, Narges Ghasemi, Sajjad Shahabi, John Krumm, Salman Avestimehr, Cyrus Shahabi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00345">https://arxiv.org/abs/2511.00345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00345">https://arxiv.org/pdf/2511.00345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00345]] OSMGen: Highly Controllable Satellite Image Synthesis using OpenStreetMap Data(https://arxiv.org/abs/2511.00345)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Accurate and up-to-date geospatial data are essential for urban planning, infrastructure monitoring, and environmental management. Yet, automating urban monitoring remains difficult because curated datasets of specific urban features and their changes are scarce. We introduce OSMGen, a generative framework that creates realistic satellite imagery directly from raw OpenStreetMap (OSM) data. Unlike prior work that relies on raster tiles, OSMGen uses the full richness of OSM JSON, including vector geometries, semantic tags, location, and time, giving fine-grained control over how scenes are generated. A central feature of the framework is the ability to produce consistent before-after image pairs: user edits to OSM inputs translate into targeted visual changes, while the rest of the scene is preserved. This makes it possible to generate training data that addresses scarcity and class imbalance, and to give planners a simple way to preview proposed interventions by editing map data. More broadly, OSMGen produces paired (JSON, image) data for both static and changed states, paving the way toward a closed-loop system where satellite imagery can automatically drive structured OSM updates. Source code is available at this https URL.</li>
<li><strong>摘要：</strong>准确和最新的地理空间数据对于城市规划、基础设施监测和环境管理至关重要。然而，自动化城市监测仍然很困难，因为特定城市特征及其变化​​的精选数据集很少。我们介绍 OSMGen，这是一个生成框架，可以直接从原始 OpenStreetMap (OSM) 数据创建逼真的卫星图像。与之前依赖栅格图块的工作不同，OSMGen 使用 OSM JSON 的全部丰富性，包括矢量几何、语义标签、位置和时间，从而对场景的生成方式进行细粒度控制。该框架的一个核心功能是能够生成一致的前后图像对：用户对 OSM 输入的编辑会转化为有针对性的视觉变化，而场景的其余部分则被保留。这使得生成解决稀缺性和类别不平衡问题的训练数据成为可能，并为规划者提供一种简单的方法来通过编辑地图数据来预览建议的干预措施。更广泛地说，OSMGen 为静态和变化状态生成配对（JSON、图像）数据，为卫星图像可以自动驱动结构化 OSM 更新的闭环系统铺平了道路。源代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Detecting AI-Generated Images via Diffusion Snap-Back Reconstruction: A Forensic Approach</h3>
<ul>
<li><strong>Authors: </strong>Mohd Ruhul Ameen, Akif Islam</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00352">https://arxiv.org/abs/2511.00352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00352">https://arxiv.org/pdf/2511.00352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00352]] Detecting AI-Generated Images via Diffusion Snap-Back Reconstruction: A Forensic Approach(https://arxiv.org/abs/2511.00352)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid rise of generative diffusion models has made distinguishing authentic visual content from synthetic imagery increasingly challenging. Traditional deepfake detection methods, which rely on frequency or pixel-level artifacts, fail against modern text-to-image systems such as Stable Diffusion and DALL-E that produce photorealistic and artifact-free results. This paper introduces a diffusion-based forensic framework that leverages multi-strength image reconstruction dynamics, termed diffusion snap-back, to identify AI-generated images. By analysing how reconstruction metrics (LPIPS, SSIM, and PSNR) evolve across varying noise strengths, we extract interpretable manifold-based features that differentiate real and synthetic images. Evaluated on a balanced dataset of 4,000 images, our approach achieves 0.993 AUROC under cross-validation and remains robust to common distortions such as compression and noise. Despite using limited data and a single diffusion backbone (Stable Diffusion v1.5), the proposed method demonstrates strong generalization and interpretability, offering a foundation for scalable, model-agnostic synthetic media forensics.</li>
<li><strong>摘要：</strong>生成扩散模型的迅速兴起使得区分真实的视觉内容和合成图像变得越来越具有挑战性。传统的深度伪造检测方法依赖于频率或像素级伪影，无法对抗现代文本到图像系统，例如稳定扩散和 DALL-E，这些系统可以产生逼真且无伪影的结果。本文介绍了一种基于扩散的取证框架，该框架利用多强度图像重建动力学（称为扩散回弹）来识别人工智能生成的图像。通过分析重建指标（LPIPS、SSIM 和 PSNR）如何在不同的噪声强度下演变，我们提取了可解释的基于流形的特征，以区分真实图像和合成图像。在 4,000 张图像的平衡数据集上进行评估，我们的方法在交叉验证下实现了 0.993 AUROC，并且对压缩和噪声等常见失真保持鲁棒性。尽管使用有限的数据和单一扩散主干（Stable Diffusion v1.5），所提出的方法表现出强大的泛化性和可解释性，为可扩展的、与模型无关的合成媒体取证提供了基础。</li>
</ul>

<h3>Title: Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery</h3>
<ul>
<li><strong>Authors: </strong>Momen Khandoker Ope, Akif Islam, Mohd Ruhul Ameen, Abu Saleh Musa Miah, Md Rashedul Islam, Jungpil Shin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00362">https://arxiv.org/abs/2511.00362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00362">https://arxiv.org/pdf/2511.00362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00362]] Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery(https://arxiv.org/abs/2511.00362)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generation, generative</a></li>
<li><strong>Abstract: </strong>Cultural heritage restoration in Bangladesh faces a dual challenge of limited resources and scarce technical expertise. Traditional 3D digitization methods, such as photogrammetry or LiDAR scanning, require expensive hardware, expert operators, and extensive on-site access, which are often infeasible in developing contexts. As a result, many of Bangladesh's architectural treasures, from the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to decay and inaccessible in digital form. This paper introduces Oitijjo-3D, a cost-free generative AI framework that democratizes 3D cultural preservation. By using publicly available Google Street View imagery, Oitijjo-3D reconstructs faithful 3D models of heritage structures through a two-stage pipeline - multimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture synthesis, and neural image-to-3D generation through Hexagen for geometry recovery. The system produces photorealistic, metrically coherent reconstructions in seconds, achieving significant speedups compared to conventional Structure-from-Motion pipelines, without requiring any specialized hardware or expert supervision. Experiments on landmarks such as Ahsan Manzil, Choto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both visual and structural fidelity while drastically lowering economic and technical barriers. By turning open imagery into digital heritage, this work reframes preservation as a community-driven, AI-assisted act of cultural continuity for resource-limited nations.</li>
<li><strong>摘要：</strong>孟加拉国的文化遗产修复面临着资源有限和技术专长匮乏的双重挑战。传统的 3D 数字化方法（例如摄影测量或 LiDAR 扫描）需要昂贵的硬件、专业操作员和广泛的现场访问，而这在开发环境中通常是不可行的。因此，从帕哈普尔佛教寺院到阿赫桑曼齐勒，孟加拉国的许多建筑瑰宝仍然容易腐烂，并且无法以数字形式访问。本文介绍了 Oitijjo-3D，这是一种免费的生成式 AI 框架，可实现 3D 文化保护的民主化。通过使用公开的 Google 街景图像，Oitijjo-3D 通过两阶段管道重建遗产结构的忠实 3D 模型 - 使用 Gemini 2.5 Flash Image 进行多模态视觉推理以进行结构纹理合成，并通过 Hexagen 进行神经图像到 3D 生成以进行几何恢复。该系统可在几秒钟内生成逼真的、度量相干的重建，与传统的运动结构管道相比，实现了显着的加速，而不需要任何专门的硬件或专家的监督。在 Ahsan Manzil、Choto Sona 清真寺和 Paharpur 等地标建筑上进行的实验表明，Oitijjo-3D 保留了视觉和结构保真度，同时大大降低了经济和技术障碍。通过将开放图像转化为数字遗产，这项工作将保护重新定义为社区驱动、人工智能辅助的资源有限国家文化连续性行为。</li>
</ul>

<h3>Title: PolyRecommender: A Multimodal Recommendation System for Polymer Discovery</h3>
<ul>
<li><strong>Authors: </strong>Xin Wang, Yunhao Xiao, Rui Qiao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00375">https://arxiv.org/abs/2511.00375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00375">https://arxiv.org/pdf/2511.00375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00375]] PolyRecommender: A Multimodal Recommendation System for Polymer Discovery(https://arxiv.org/abs/2511.00375)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce PolyRecommender, a multimodal discovery framework that integrates chemical language representations from PolyBERT with molecular graph-based representations from a graph encoder. The system first retrieves candidate polymers using language-based similarity and then ranks them using fused multimodal embeddings according to multiple target properties. By leveraging the complementary knowledge encoded in both modalities, PolyRecommender enables efficient retrieval and robust ranking across related polymer properties. Our work establishes a generalizable multimodal paradigm, advancing AI-guided design for the discovery of next-generation polymers.</li>
<li><strong>摘要：</strong>我们引入了 PolyRecommender，这是一个多模式发现框架，它将 PolyBERT 的化学语言表示与图形编码器的基于分子图的表示集成在一起。该系统首先使用基于语言的相似性检索候选聚合物，然后根据多个目标属性使用融合多模态嵌入对它们进行排名。通过利用两种模式编码的互补知识，PolyRecommender 可以实现相关聚合物属性的高效检索和稳健排名。我们的工作建立了一个可推广的多模式范式，推进人工智能引导设计以发现下一代聚合物。</li>
</ul>

<h3>Title: VisionCAD: An Integration-Free Radiology Copilot Framework</h3>
<ul>
<li><strong>Authors: </strong>Jiaming Li, Junlei Wu, Sheng Wang, Honglin Xiong, Jiangdong Cai, Zihao Zhao, Yitao Zhu, Yuan Yin, Dinggang Shen, Qian Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00381">https://arxiv.org/abs/2511.00381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00381">https://arxiv.org/pdf/2511.00381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00381]] VisionCAD: An Integration-Free Radiology Copilot Framework(https://arxiv.org/abs/2511.00381)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Widespread clinical deployment of computer-aided diagnosis (CAD) systems is hindered by the challenge of integrating with existing hospital IT infrastructure. Here, we introduce VisionCAD, a vision-based radiological assistance framework that circumvents this barrier by capturing medical images directly from displays using a camera system. The framework operates through an automated pipeline that detects, restores, and analyzes on-screen medical images, transforming camera-captured visual data into diagnostic-quality images suitable for automated analysis and report generation. We validated VisionCAD across diverse medical imaging datasets, demonstrating that our modular architecture can flexibly utilize state-of-the-art diagnostic models for specific tasks. The system achieves diagnostic performance comparable to conventional CAD systems operating on original digital images, with an F1-score degradation typically less than 2\% across classification tasks, while natural language generation metrics for automated reports remain within 1\% of those derived from original images. By requiring only a camera device and standard computing resources, VisionCAD offers an accessible approach for AI-assisted diagnosis, enabling the deployment of diagnostic capabilities in diverse clinical settings without modifications to existing infrastructure.</li>
<li><strong>摘要：</strong>计算机辅助诊断 (CAD) 系统的广泛临床部署因与现有医院 IT 基础设施集成的挑战而受到阻碍。在这里，我们介绍 VisionCAD，这是一种基于视觉的放射辅助框架，它通过使用摄像头系统直接从显示器捕获医学图像来绕过这一障碍。该框架通过自动管道运行，检测、恢复和分析屏幕上的医学图像，将相机捕获的视觉数据转换为适合自动分析和报告生成的诊断质量图像。我们在不同的医学成像数据集上验证了 VisionCAD，证明我们的模块化架构可以灵活地利用最先进的诊断模型来执行特定任务。该系统的诊断性能可与在原始数字图像上运行的传统 CAD 系统相媲美，在分类任务中 F1 分数下降通常小于 2%，而自动报告的自然语言生成指标与原始图像的自然语言生成指标保持在 1% 以内。 VisionCAD 仅需要摄像头设备和标准计算资源，为人工智能辅助诊断提供了一种易于访问的方法，无需修改现有基础设施即可在不同的临床环境中部署诊断功能。</li>
</ul>

<h3>Title: VinciCoder: Unifying Multimodal Code Generation via Coarse-to-fine Visual Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Xuanle Zhao, Deyang Jiang, Zhixiong Zeng, Lei Chen, Haibo Qiu, Jing Huang, Yufeng Zhong, Liming Zheng, Yilin Cao, Lin Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00391">https://arxiv.org/abs/2511.00391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00391">https://arxiv.org/pdf/2511.00391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00391]] VinciCoder: Unifying Multimodal Code Generation via Coarse-to-fine Visual Reinforcement Learning(https://arxiv.org/abs/2511.00391)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal code generation has garnered significant interest within the research community. Despite the notable success of recent vision-language models (VLMs) on specialized tasks like Chart-to-code generation, their reliance on single-task training regimens fosters a narrow paradigm that hinders the development of generalized \textbf{VI}sio\textbf{N} \textbf{C}ode \textbf{I}ntelligence. In this work, we introduce \textbf{VinciCoder}, a unified multimodal code generation model that addresses this limitation via a two-stage training framework. We begin by constructing a large-scale Supervised Finetuning (SFT) corpus comprising 1.6M image-code pairs for tasks involving direct code generation and visual-based code refinement. Subsequently, we introduce a Visual Reinforcement Learning (ViRL) strategy, which employs a coarse-to-fine reward mechanism to improve visual fidelity by calculating visual similarity across local and global image patches. Extensive experiments on various multimodal code generation benchmarks demonstrate that VinciCoder achieves state-of-the-art performance, underscoring the effectiveness of our coarse-to-fine ViRL strategy. The code and model will be available at this https URL.</li>
<li><strong>摘要：</strong>多模式代码生成引起了研究界的极大兴趣。尽管最近的视觉语言模型（VLM）在图表到代码生成等专门任务上取得了显着的成功，但它们对单任务训练方案的依赖形成了一种狭窄的范式，阻碍了广义 \textbf{VI}sio\textbf{N} \textbf{C}ode \textbf{I} 智能的发展。在这项工作中，我们引入了 \textbf{VinciCoder}，这是一种统一的多模式代码生成模型，它通过两阶段训练框架解决了这一限制。我们首先构建一个大规模监督微调 (SFT) 语料库，其中包含 160 万个图像代码对，用于涉及直接代码生成和基于视觉的代码细化的任务。随后，我们引入了视觉强化学习（ViRL）策略，该策略采用从粗到细的奖励机制，通过计算局部和全局图像块的视觉相似性来提高视觉保真度。对各种多模式代码生成基准的大量实验表明，VinciCoder 实现了最先进的性能，强调了我们从粗到细的 ViRL 策略的有效性。代码和模型将在此 https URL 中提供。</li>
</ul>

<h3>Title: UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Zhibin Lan, Liqiang Niu, Fandong Meng, Jie Zhou, Jinsong Su</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00405">https://arxiv.org/abs/2511.00405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00405">https://arxiv.org/pdf/2511.00405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00405]] UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings(https://arxiv.org/abs/2511.00405)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The remarkable success of multimodal large language models (MLLMs) has driven advances in multimodal embeddings, yet existing models remain inherently discriminative, limiting their ability to benefit from reasoning-driven generation paradigm. In this work, we pioneer the exploration of generative embeddings, unifying embedding tasks within a generative paradigm. We propose UME-R1, a universal multimodal embedding framework consisting of a two-stage training strategy: a cold-start supervised fine-tuning equips the model with reasoning capabilities and enables it to generate both discriminative and generative embeddings; a subsequent reinforcement learning enhances reasoning and further optimizes generative embedding quality. This pioneering work reveals four key insights: 1) generative embeddings unlock substantial performance gains over conventional discriminative embeddings by leveraging the powerful generative reasoning capabilities of MLLMs; 2) discriminative and generative embeddings are complementary, whose combined oracle performance far exceeding that of either alone; 3) RL can effectively enhance generative embeddings, establishing a scalable optimization paradigm.; 4) repeated sampling at inference boosts downstream task coverage (pass@k), highlighting the inference-time scalability potential of generative embeddings. Evaluated on the MMEB-V2 benchmark across 78 tasks spanning video, image, and visual documents, UME-R1 significantly outperforms conventional discriminative embedding models and offers a foundation for more interpretable, reasoning-driven generative multimodal embeddings. Our code, models, and datasets will be publicly available at this https URL.</li>
<li><strong>摘要：</strong>多模态大语言模型（MLLM）的巨大成功推动了多模态嵌入的进步，但现有模型仍然具有固有的歧视性，限制了它们从推理驱动的生成范式中受益的能力。在这项工作中，我们开创了对生成嵌入的探索，将嵌入任务统一在生成范式中。我们提出了 UME-R1，一种通用的多模态嵌入框架，由两阶段训练策略组成：冷启动监督微调为模型配备推理能力，使其能够生成判别性嵌入和生成嵌入；随后的强化学习增强了推理并进一步优化了生成嵌入质量。这项开创性的工作揭示了四个关键见解：1）通过利用 MLLM 强大的生成推理能力，生成嵌入比传统判别式嵌入获得了显着的性能提升； 2）判别嵌入和生成嵌入是互补的，其组合的预言机性能远远超过其中任何一个单独的性能； 3）强化学习可以有效增强生成嵌入，建立可扩展的优化范式； 4）推理时的重复采样提高了下游任务覆盖率（pass@k），突出了生成嵌入的推理时间可扩展性潜力。 UME-R1 在 MMEB-V2 基准上针对涵盖视频、图像和视觉文档的 78 个任务进行了评估，显着优于传统的判别式嵌入模型，并为更具可解释性、推理驱动的生成多模态嵌入奠定了基础。我们的代码、模型和数据集将在此 https URL 上公开提供。</li>
</ul>

<h3>Title: Tree Training: Accelerating Agentic LLMs Training via Shared Prefix Reuse</h3>
<ul>
<li><strong>Authors: </strong>Shaojie Wang, Jinghui Wang, Yinghan Cui, Xuxing Chen, Chao Wang, Liang Huang, Xiaojiang Zhang, Junyi Peng, Li Wan, Haotian Zhang, Bin Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00413">https://arxiv.org/abs/2511.00413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00413">https://arxiv.org/pdf/2511.00413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00413]] Tree Training: Accelerating Agentic LLMs Training via Shared Prefix Reuse(https://arxiv.org/abs/2511.00413)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>In agentic LLM scenarios, an agent's interaction process during a single rollout often exhibits branching behaviors. Due to memory retrieval and concurrent tool executions at certain decision points, the token trajectory of one task evolves into a tree-like structure rather than a linear sequence. However, current training pipelines decompose such tree-structured trajectories into separate linear segments, treating each branch as an independent sequence. As a result, shared prefixes across these branches are repeatedly recomputed during both forward and backward passes. To address this inefficiency, we propose Tree Training, a paradigm that computes each shared prefix only once and reuses its intermediate results across related branches during both forward and backward passes, substantially improving computation efficiency in large-scale agentic training. This is achieved via (i) Tree Packing, which efficiently reuses shared computations across trajectories, and (ii) Gradient Restoration, which ensures correct gradient propagation across reused prefixes. Experiments on multiple open-source models demonstrate up to 3.9x reduction in total training time, enabling more efficient agentic LLM SFT and RL training.</li>
<li><strong>摘要：</strong>在代理 LLM 场景中，代理在单次部署期间的交互过程通常会表现出分支行为。由于某些决策点的内存检索和并发工具执行，一项任务的令牌轨迹演变成树状结构，而不是线性序列。然而，当前的训练管道将此类树形结构轨迹分解为单独的线性段，将每个分支视为独立的序列。因此，在前向和后向传递过程中，这些分支之间的共享前缀都会被重复重新计算。为了解决这种低效率问题，我们提出了树训练，这种范例仅计算每个共享前缀一次，并在前向和后向传递过程中在相关分支上重用其中间结果，从而大大提高了大规模代理训练的计算效率。这是通过（i）树打包（它有效地重用跨轨迹的共享计算）和（ii）梯度恢复（它确保跨重用前缀的正确梯度传播）来实现的。对多个开源模型的实验表明，总训练时间最多可减少 3.9 倍，从而实现更高效的代理 LLM SFT 和 RL 训练。</li>
</ul>

<h3>Title: Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Daichi Zhang, Tong Zhang, Jianmin Bao, Shiming Ge, Sabine Süsstrunk</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00427">https://arxiv.org/abs/2511.00427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00427">https://arxiv.org/pdf/2511.00427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00427]] Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection(https://arxiv.org/abs/2511.00427)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the rapid development of generative models, detecting generated fake images to prevent their malicious use has become a critical issue recently. Existing methods frame this challenge as a naive binary image classification task. However, such methods focus only on visual clues, yielding trained detectors susceptible to overfitting specific image patterns and incapable of generalizing to unseen models. In this paper, we address this issue from a multi-modal perspective and find that fake images cannot be properly aligned with corresponding captions compared to real images. Upon this observation, we propose a simple yet effective detector termed ITEM by leveraging the image-text misalignment in a joint visual-language space as discriminative clues. Specifically, we first measure the misalignment of the images and captions in pre-trained CLIP's space, and then tune a MLP head to perform the usual detection task. Furthermore, we propose a hierarchical misalignment scheme that first focuses on the whole image and then each semantic object described in the caption, which can explore both global and fine-grained local semantic misalignment as clues. Extensive experiments demonstrate the superiority of our method against other state-of-the-art competitors with impressive generalization and robustness on various recent generative models.</li>
<li><strong>摘要：</strong>随着生成模型的快速发展，检测生成的假图像以防止其被恶意使用已成为最近的一个关键问题。现有方法将这一挑战视为简单的二值图像分类任务。然而，此类方法仅关注视觉线索，产生的训练有素的检测器容易过度拟合特定图像模式，并且无法推广到看不见的模型。在本文中，我们从多模态的角度解决了这个问题，发现与真实图像相比，假图像无法与相应的标题正确对齐。根据这一观察，我们提出了一种简单而有效的检测器，称为 ITEM，利用联合视觉语言空间中的图像文本未对齐作为判别线索。具体来说，我们首先测量预先训练的 CLIP 空间中图像和标题的错位，然后调整 MLP 头来执行通常的检测任务。此外，我们提出了一种分层错位方案，首先关注整个图像，然后关注标题中描述的每个语义对象，它可以探索全局和细粒度局部语义错位作为线索。大量的实验证明了我们的方法相对于其他最先进的竞争对手的优越性，在各种最新的生成模型上具有令人印象深刻的泛化性和鲁棒性。</li>
</ul>

<h3>Title: HumanCrafter: Synergizing Generalizable Human Reconstruction and Semantic 3D Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Panwang Pan, Tingting Shen, Chenxin Li, Yunlong Lin, Kairun Wen, Jingjing Zhao, Yixuan Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00468">https://arxiv.org/abs/2511.00468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00468">https://arxiv.org/pdf/2511.00468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00468]] HumanCrafter: Synergizing Generalizable Human Reconstruction and Semantic 3D Segmentation(https://arxiv.org/abs/2511.00468)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative models have achieved high-fidelity in 3D human reconstruction, yet their utility for specific tasks (e.g., human 3D segmentation) remains constrained. We propose HumanCrafter, a unified framework that enables the joint modeling of appearance and human-part semantics from a single image in a feed-forward manner. Specifically, we integrate human geometric priors in the reconstruction stage and self-supervised semantic priors in the segmentation stage. To address labeled 3D human datasets scarcity, we further develop an interactive annotation procedure for generating high-quality data-label pairs. Our pixel-aligned aggregation enables cross-task synergy, while the multi-task objective simultaneously optimizes texture modeling fidelity and semantic consistency. Extensive experiments demonstrate that HumanCrafter surpasses existing state-of-the-art methods in both 3D human-part segmentation and 3D human reconstruction from a single image.</li>
<li><strong>摘要：</strong>生成模型的最新进展已经在 3D 人体重建中实现了高保真度，但它们在特定任务（例如人体 3D 分割）中的实用性仍然受到限制。我们提出了 HumanCrafter，这是一个统一的框架，能够以前馈方式对单个图像的外观和人体部分语义进行联合建模。具体来说，我们在重建阶段整合了人类几何先验，在分割阶段整合了自监督语义先验。为了解决带标签的 3D 人类数据集的稀缺问题，我们进一步开发了一种交互式注释程序，用于生成高质量的数据标签对。我们的像素对齐聚合可实现跨任务协同，而多任务目标同时优化纹理建模保真度和语义一致性。大量实验表明，HumanCrafter 在 3D 人体部位分割和从单个图像进行 3D 人体重建方面都超越了现有的最先进方法。</li>
</ul>

<h3>Title: Diff4Splat: Controllable 4D Scene Generation with Latent Dynamic Reconstruction Models</h3>
<ul>
<li><strong>Authors: </strong>Panwang Pan, Chenguo Lin, Jingjing Zhao, Chenxin Li, Yuchen Lin, Haopeng Li, Honglei Yan, Kairun Wen, Yunlong Lin, Yixuan Yuan, Yadong Mu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00503">https://arxiv.org/abs/2511.00503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00503">https://arxiv.org/pdf/2511.00503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00503]] Diff4Splat: Controllable 4D Scene Generation with Latent Dynamic Reconstruction Models(https://arxiv.org/abs/2511.00503)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We introduce Diff4Splat, a feed-forward method that synthesizes controllable and explicit 4D scenes from a single image. Our approach unifies the generative priors of video diffusion models with geometry and motion constraints learned from large-scale 4D datasets. Given a single input image, a camera trajectory, and an optional text prompt, Diff4Splat directly predicts a deformable 3D Gaussian field that encodes appearance, geometry, and motion, all in a single forward pass, without test-time optimization or post-hoc refinement. At the core of our framework lies a video latent transformer, which augments video diffusion models to jointly capture spatio-temporal dependencies and predict time-varying 3D Gaussian primitives. Training is guided by objectives on appearance fidelity, geometric accuracy, and motion consistency, enabling Diff4Splat to synthesize high-quality 4D scenes in 30 seconds. We demonstrate the effectiveness of Diff4Splatacross video generation, novel view synthesis, and geometry extraction, where it matches or surpasses optimization-based methods for dynamic scene synthesis while being significantly more efficient.</li>
<li><strong>摘要：</strong>我们引入了 Diff4Splat，这是一种前馈方法，可以从单个图像合成可控且显式的 4D 场景。我们的方法将视频扩散模型的生成先验与从大规模 4D 数据集中学习到的几何和运动约束相结合。给定单个输入图像、相机轨迹和可选的文本提示，Diff4Splat 直接预测可变形 3D 高斯场，该场对外观、几何形状和运动进行编码，所有这些都在一次前向传递中完成，无需测试时优化或事后细化。我们框架的核心是视频潜在转换器，它增强了视频扩散模型，以共同捕获时空依赖性并预测时变 3D 高斯基元。训练以外观保真度、几何精度和运动一致性为目标，使 Diff4Splat 能够在 30 秒内合成高质量的 4D 场景。我们展示了 Diff4Splata 在视频生成、新颖视图合成和几何提取方面的有效性，它匹配或超越基于优化的动态场景合成方法，同时效率显着提高。</li>
</ul>

<h3>Title: ID-Composer: Multi-Subject Video Synthesis with Hierarchical Identity Preservation</h3>
<ul>
<li><strong>Authors: </strong>Panwang Pan, Jingjing Zhao, Yuchen Lin, Chenguo Lin, Chenxin Li, Haopeng Li, Honglei Yan, Tingting Shen, Yadong Mu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00511">https://arxiv.org/abs/2511.00511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00511">https://arxiv.org/pdf/2511.00511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00511]] ID-Composer: Multi-Subject Video Synthesis with Hierarchical Identity Preservation(https://arxiv.org/abs/2511.00511)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Video generative models pretrained on large-scale datasets can produce high-quality videos, but are often conditioned on text or a single image, limiting controllability and applicability. We introduce ID-Composer, a novel framework that addresses this gap by tackling multi-subject video generation from a text prompt and reference images. This task is challenging as it requires preserving subject identities, integrating semantics across subjects and modalities, and maintaining temporal consistency. To faithfully preserve the subject consistency and textual information in synthesized videos, ID-Composer designs a \textbf{hierarchical identity-preserving attention mechanism}, which effectively aggregates features within and across subjects and modalities. To effectively allow for the semantic following of user intention, we introduce \textbf{semantic understanding via pretrained vision-language model (VLM)}, leveraging VLM's superior semantic understanding to provide fine-grained guidance and capture complex interactions between multiple subjects. Considering that standard diffusion loss often fails in aligning the critical concepts like subject ID, we employ an \textbf{online reinforcement learning phase} to drive the overall training objective of ID-Composer into RLVR. Extensive experiments demonstrate that our model surpasses existing methods in identity preservation, temporal consistency, and video quality.</li>
<li><strong>摘要：</strong>在大规模数据集上预训练的视频生成模型可以生成高质量的视频，但通常以文本或单个图像为条件，限制了可控性和适用性。我们推出了 ID-Composer，这是一种新颖的框架，它通过从文本提示和参考图像生成多主题视频来解决这一问题。这项任务具有挑战性，因为它需要保留主题身份、跨主题和模式集成语义以及保持时间一致性。为了忠实地保留合成视频中的主题一致性和文本信息，ID-Composer 设计了一种 \textbf{分层身份保留注意机制}，它有效地聚合了主题和模式内部和之间的特征。为了有效地实现用户意图的语义跟踪，我们引入了 \textbf{通过预训练的视觉语言模型 (VLM) 进行语义理解}，利用 VLM 卓越的语义理解来提供细粒度的指导并捕获多个主体之间的复杂交互。考虑到标准扩散损失通常无法对齐主题 ID 等关键概念，我们采用 \textbf{在线强化学习阶段} 将 ID-Composer 的整体训练目标驱动到 RLVR 中。大量的实验表明，我们的模型在身份保存、时间一致性和视频质量方面超越了现有的方法。</li>
</ul>

<h3>Title: Reasoning Planning for Language Models</h3>
<ul>
<li><strong>Authors: </strong>Bao Nguyen, Hieu Trung Nguyen, Ruifeng She, Xiaojin Fu, Viet Anh Nguyen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00521">https://arxiv.org/abs/2511.00521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00521">https://arxiv.org/pdf/2511.00521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00521]] Reasoning Planning for Language Models(https://arxiv.org/abs/2511.00521)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Selecting an appropriate reasoning method for a given query remains a key challenge in language model generation. Existing approaches typically generate multiple candidate responses and use an aggregation strategy to select the output answer, often assuming that more candidate answers yield higher accuracy. We revisit this assumption through a rigorous theoretical analysis, deriving accuracy bounds for standard aggregation methods under fixed generation distributions and candidate sizes. Building on these insights, we introduce EPIC, an Ensemble Planning with Contrastive learning framework to learn a shared representation space that captures both model reasoning abilities and query-method compatibility. EPIC incorporates our probability bounds as a regularizer in a utility-driven optimization that balances accuracy and computational cost. Experiments on diverse mathematical reasoning tasks show that EPIC consistently selects optimal reasoning methods, improving accuracy while reducing computational overhead. Our code can be found at this https URL.</li>
<li><strong>摘要：</strong>为给定查询选择合适的推理方法仍然是语言模型生成中的关键挑战。现有方法通常生成多个候选响应，并使用聚合策略来选择输出答案，通常假设更多候选答案会产生更高的准确性。我们通过严格的理论分析重新审视这一假设，得出固定生成分布和候选大小下标准聚合方法的准确度范围。基于这些见解，我们引入了 EPIC，这是一种具有对比学习框架的集成规划，用于学习可捕获模型推理能力和查询方法兼容性的共享表示空间。 EPIC 将我们的概率范围作为正则化器纳入效用驱动的优化中，以平衡准确性和计算成本。对各种数学推理任务的实验表明，EPIC 始终如一地选择最佳推理方法，提高准确性，同时减少计算开销。我们的代码可以在此 https URL 中找到。</li>
</ul>

<h3>Title: Real-IAD Variety: Pushing Industrial Anomaly Detection Dataset to a Modern Era</h3>
<ul>
<li><strong>Authors: </strong>Wenbing Zhu, Chengjie Wang, Bin-Bin Gao, Jiangning Zhang, Guannan Jiang, Jie Hu, Zhenye Gan, Lidong Wang, Ziqing Zhou, Linjie Cheng, Yurui Pan, Bo Peng, Mingmin Chi, Lizhuang Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00540">https://arxiv.org/abs/2511.00540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00540">https://arxiv.org/pdf/2511.00540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00540]] Real-IAD Variety: Pushing Industrial Anomaly Detection Dataset to a Modern Era(https://arxiv.org/abs/2511.00540)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Industrial Anomaly Detection (IAD) is critical for enhancing operational safety, ensuring product quality, and optimizing manufacturing efficiency across global industries. However, the IAD algorithms are severely constrained by the limitations of existing public benchmarks. Current datasets exhibit restricted category diversity and insufficient scale, frequently resulting in metric saturation and limited model transferability to real-world scenarios. To address this gap, we introduce Real-IAD Variety, the largest and most diverse IAD benchmark, comprising 198,960 high-resolution images across 160 distinct object categories. Its diversity is ensured through comprehensive coverage of 28 industries, 24 material types, and 22 color variations. Our comprehensive experimental analysis validates the benchmark's substantial challenge: state-of-the-art multi-class unsupervised anomaly detection methods experience significant performance degradation when scaled from 30 to 160 categories. Crucially, we demonstrate that vision-language models exhibit remarkable robustness to category scale-up, with minimal performance variation across different category counts, significantly enhancing generalization capabilities in diverse industrial contexts. The unprecedented scale and complexity of Real-IAD Variety position it as an essential resource for training and evaluating next-generation foundation models for anomaly detection. By providing this comprehensive benchmark with rigorous evaluation protocols across multi-class unsupervised, multi-view, and zero-/few-shot settings, we aim to accelerate research beyond domain-specific constraints, enabling the development of scalable, general-purpose anomaly detection systems. Real-IAD Variety will be made publicly available to facilitate innovation in this critical field.</li>
<li><strong>摘要：</strong>工业异常检测 (IAD) 对于提高全球各行业的操作安全性、确保产品质量和优化制造效率至关重要。然而，IAD 算法受到现有公共基准的限制的严重限制。当前的数据集表现出有限的类别多样性和规模不足，经常导致指标饱和和有限的模型可转移到现实世界场景。为了解决这一差距，我们引入了 Real-IAD Variety，这是最大、最多样化的 IAD 基准，包含 160 个不同对象类别的 198,960 张高分辨率图像。通过全面覆盖 28 个行业、24 种材料类型和 22 种颜色变化，确保了其多样性。我们全面的实验分析验证了基准测试面临的重大挑战：最先进的多类无监督异常检测方法在从 30 个类别扩展到 160 个类别时会出现显着的性能下降。至关重要的是，我们证明视觉语言模型对类别扩展表现出卓越的鲁棒性，不同类别数量之间的性能差异最小，从而显着增强了不同工业环境中的泛化能力。 Real-IAD Variety 前所未有的规模和复杂性使其成为训练和评估下一代异常检测基础模型的重要资源。通过提供这种全面的基准测试以及跨多类无监督、多视图和零/少样本设置的严格评估协议，我们的目标是加速超越特定领域限制的研究，从而实现可扩展的通用异常检测系统的开发。 Real-IAD Variety 将公开发布，以促进这一关键领域的创新。</li>
</ul>

<h3>Title: Learning an Efficient Optimizer via Hybrid-Policy Sub-Trajectory Balance</h3>
<ul>
<li><strong>Authors: </strong>Yunchuan Guan, Yu Liu, Ke Zhou, Hui Li, Sen Jia, Zhiqi Shen, Ziyang Wang, Xinglin Zhang, Tao Chen, Jenq-Neng Hwang, Lei Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00543">https://arxiv.org/abs/2511.00543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00543">https://arxiv.org/pdf/2511.00543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00543]] Learning an Efficient Optimizer via Hybrid-Policy Sub-Trajectory Balance(https://arxiv.org/abs/2511.00543)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative modeling enable neural networks to generate weights without relying on gradient-based optimization. However, current methods are limited by issues of over-coupling and long-horizon. The former tightly binds weight generation with task-specific objectives, thereby limiting the flexibility of the learned optimizer. The latter leads to inefficiency and low accuracy during inference, caused by the lack of local constraints. In this paper, we propose Lo-Hp, a decoupled two-stage weight generation framework that enhances flexibility through learning various optimization policies. It adopts a hybrid-policy sub-trajectory balance objective, which integrates on-policy and off-policy learning to capture local optimization policies. Theoretically, we demonstrate that learning solely local optimization policies can address the long-horizon issue while enhancing the generation of global optimal weights. In addition, we validate Lo-Hp's superior accuracy and inference efficiency in tasks that require frequent weight updates, such as transfer learning, few-shot learning, domain generalization, and large language model adaptation.</li>
<li><strong>摘要：</strong>生成建模的最新进展使神经网络能够在不依赖基于梯度的优化的情况下生成权重。然而，当前的方法受到过度耦合和长视野问题的限制。前者将权重生成与特定任务的目标紧密结合，从而限制了学习优化器的灵活性。后者由于缺乏局部约束而导致推理过程中效率低下且准确性低。在本文中，我们提出了 Lo-Hp，一种解耦的两阶段权重生成框架，通过学习各种优化策略来增强灵活性。它采用混合策略子轨迹平衡目标，集成了在策略和离策略学习来捕获局部优化策略。从理论上讲，我们证明仅学习局部优化策略可以解决长期问题，同时增强全局最优权重的生成。此外，我们还验证了 Lo-Hp 在需要频繁权重更新的任务中的卓越准确性和推理效率，例如迁移学习、小样本学习、领域泛化和大型语言模型自适应。</li>
</ul>

<h3>Title: CueBench: Advancing Unified Understanding of Context-Aware Video Anomalies in Real-World</h3>
<ul>
<li><strong>Authors: </strong>Yating Yu, Congqi Cao, Zhaoying Wang, Weihua Meng, Jie Li, Yuxin Li, Zihao Wei, Zhongpei Shen, Jiajun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00613">https://arxiv.org/abs/2511.00613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00613">https://arxiv.org/pdf/2511.00613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00613]] CueBench: Advancing Unified Understanding of Context-Aware Video Anomalies in Real-World(https://arxiv.org/abs/2511.00613)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>How far are deep models from real-world video anomaly understanding (VAU)? Current works typically emphasize on detecting unexpected occurrences deviated from normal patterns or comprehending anomalous events with interpretable descriptions. However, they exhibit only a superficial comprehension of real-world anomalies, with limited breadth in complex principles and subtle context that distinguish the anomalies from normalities, e.g., climbing cliffs with safety gear vs. without it. To this end, we introduce CueBench, the first of its kind Benchmark, devoted to Context-aware video anomalies within a Unified Evaluation framework. We comprehensively establish an event-centric hierarchical taxonomy that anchors two core event types: 14 conditional and 18 absolute anomaly events, defined by their refined semantics from diverse contexts across 174 scenes and 198 attributes. Based on this, we propose to unify and benchmark context-aware VAU with various challenging tasks across recognition, temporal grounding, detection, and anticipation. This also serves as a rigorous and fair probing evaluation suite for generative-discriminative as well as generalized-specialized vision-language models (VLMs). To address the challenges underlying CueBench, we further develop Cue-R1 based on R1-style reinforcement fine-tuning with verifiable, task-aligned, and hierarchy-refined rewards in a unified generative manner. Extensive results on CueBench reveal that, existing VLMs are still far from satisfactory real-world anomaly understanding, while our Cue-R1 surpasses these state-of-the-art approaches by over 24% on average.</li>
<li><strong>摘要：</strong>深度模型距离现实世界的视频异常理解（VAU）还有多远？当前的工作通常强调检测偏离正常模式的意外事件或通过可解释的描述来理解异常事件。然而，他们仅表现出对现实世界异常现象的肤浅理解，在区分异常与正常情况的复杂原理和微妙背景方面的广度有限，例如，携带安全装备与不携带安全装备攀爬悬崖。为此，我们推出了 CueBench，这是同类基准中的第一个，致力于统一评估框架内的上下文感知视频异常。我们全面建立了一个以事件为中心的分层分类法，锚定了两种核心事件类型：14 个条件异常事件和 18 个绝对异常事件，由来自 174 个场景和 198 个属性的不同上下文的精炼语义定义。基于此，我们建议将上下文感知 VAU 与识别、时间基础、检测和预测等各种具有挑战性的任务进行统一和基准测试。这也可以作为生成判别模型以及广义专业视觉语言模型（VLM）的严格且公平的探测评估套件。为了解决 CueBench 背后的挑战，我们进一步开发了基于 R1 式强化微调的 Cue-R1，以统一的生成方式提供可验证、任务一致和层次细化的奖励。 CueBench 上的大量结果表明，现有的 VLM 仍远未达到令人满意的现实世界异常理解，而我们的 Cue-R1 平均超过这些最先进的方法 24% 以上。</li>
</ul>

<h3>Title: Diluting Restricted Boltzmann Machines</h3>
<ul>
<li><strong>Authors: </strong>C. Díaz-Faloh, R. Mulet</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00648">https://arxiv.org/abs/2511.00648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00648">https://arxiv.org/pdf/2511.00648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00648]] Diluting Restricted Boltzmann Machines(https://arxiv.org/abs/2511.00648)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in artificial intelligence have relied heavily on increasingly large neural networks, raising concerns about their computational and environmental costs. This paper investigates whether simpler, sparser networks can maintain strong performance by studying Restricted Boltzmann Machines (RBMs) under extreme pruning conditions. Inspired by the Lottery Ticket Hypothesis, we demonstrate that RBMs can achieve high-quality generative performance even when up to 80% of the connections are pruned before training, confirming that they contain viable sub-networks. However, our experiments reveal crucial limitations: trained networks cannot fully recover lost performance through retraining once additional pruning is applied. We identify a sharp transition above which the generative quality degrades abruptly when pruning disrupts a minimal core of essential connections. Moreover, re-trained networks remain constrained by the parameters originally learned performing worse than networks trained from scratch at equivalent sparsity levels. These results suggest that for sparse networks to work effectively, pruning should be implemented early in training rather than attempted afterwards. Our findings provide practical insights for the development of efficient neural architectures and highlight the persistent influence of initial conditions on network capabilities.</li>
<li><strong>摘要：</strong>人工智能的最新进展严重依赖于日益庞大的神经网络，引发了人们对其计算和环境成本的担忧。本文通过研究极端剪枝条件下的受限玻尔兹曼机（RBM）来研究更简单、更稀疏的网络是否能够保持强大的性能。受彩票假说的启发，我们证明，即使在训练前修剪高达 80% 的连接，RBM ​​也可以实现高质量的生成性能，从而确认它们包含可行的子网络。然而，我们的实验揭示了关键的局限性：一旦应用额外的修剪，经过训练的网络就无法通过重新训练完全恢复损失的性能。我们发现了一个急剧的转变，当修剪破坏基本连接的最小核心时，生成质量会突然下降。此外，重新训练的网络仍然受到最初学习的参数的限制，其性能比在同等稀疏水平下从头开始训练的网络要差。这些结果表明，为了使稀疏网络有效工作，应在训练早期实施剪枝，而不是事后尝试。我们的研究结果为高效神经架构的开发提供了实用的见解，并强调了初始条件对网络能力的持续影响。</li>
</ul>

<h3>Title: Sensitivity Analysis for Climate Science with Generative Flow Models</h3>
<ul>
<li><strong>Authors: </strong>Alex Dobra, Jakiw Pidstrigach, Tim Reichelt, Paolo Fraccaro, Johannes Jakubik, Anne Jones, Christian Schroeder de Witt, Philip Stier, Philip Torr</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00663">https://arxiv.org/abs/2511.00663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00663">https://arxiv.org/pdf/2511.00663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00663]] Sensitivity Analysis for Climate Science with Generative Flow Models(https://arxiv.org/abs/2511.00663)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Sensitivity analysis is a cornerstone of climate science, essential for understanding phenomena ranging from storm intensity to long-term climate feedbacks. However, computing these sensitivities using traditional physical models is often prohibitively expensive in terms of both computation and development time. While modern AI-based generative models are orders of magnitude faster to evaluate, computing sensitivities with them remains a significant bottleneck. This work addresses this challenge by applying the adjoint state method for calculating gradients in generative flow models, with diffusion models as a special case. We apply this method to the cBottle generative model, an emulator of ERA5 data, to perform sensitivity analysis with respect to sea surface temperatures. Furthermore, we propose a novel gradient self-consistency check to quantitatively validate the computed sensitivities against the model's own outputs. Our results provide initial evidence that this approach can produce reliable gradients, reducing the computational cost of sensitivity analysis from weeks on a supercomputer with a physical model to hours on a GPU, thereby simplifying a critical workflow in climate science.</li>
<li><strong>摘要：</strong>敏感性分析是气候科学的基石，对于理解从风暴强度到长期气候反馈等现象至关重要。然而，使用传统物理模型计算这些敏感性在计算和开发时间方面通常都非常昂贵。虽然现代基于人工智能的生成模型的评估速度快了几个数量级，但它们的计算敏感性仍然是一个重大瓶颈。这项工作通过应用伴随状态方法来计算生成流模型中的梯度来解决这一挑战，其中扩散模型是一个特例。我们将此方法应用于 cBottle 生成模型（ERA5 数据的模拟器），以执行关于海面温度的敏感性分析。此外，我们提出了一种新颖的梯度自一致性检查，以根据模型自身的输出定量验证计算的灵敏度。我们的结果提供了初步证据，表明这种方法可以产生可靠的梯度，将敏感性分析的计算成本从使用物理模型的超级计算机上的数周减少到在 GPU 上的数小时，从而简化了气候科学的关键工作流程。</li>
</ul>

<h3>Title: Outlier-Aware Post-Training Quantization for Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Hailing Wang, jianglin Lu, Yitian Zhang, Yun Fu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00682">https://arxiv.org/abs/2511.00682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00682">https://arxiv.org/pdf/2511.00682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00682]] Outlier-Aware Post-Training Quantization for Image Super-Resolution(https://arxiv.org/abs/2511.00682)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Quantization techniques, including quantization-aware training (QAT) and post-training quantization (PTQ), have become essential for inference acceleration of image super-resolution (SR) networks. Compared to QAT, PTQ has garnered significant attention as it eliminates the need for ground truth and model retraining. However, existing PTQ methods for SR often fail to achieve satisfactory performance as they overlook the impact of outliers in activation. Our empirical analysis reveals that these prevalent activation outliers are strongly correlated with image color information, and directly removing them leads to significant performance degradation. Motivated by this, we propose a dual-region quantization strategy that partitions activations into an outlier region and a dense region, applying uniform quantization to each region independently to better balance bit-width allocation. Furthermore, we observe that different network layers exhibit varying sensitivities to quantization, leading to different levels of performance degradation. To address this, we introduce sensitivity-aware finetuning that encourages the model to focus more on highly sensitive layers, further enhancing quantization performance. Extensive experiments demonstrate that our method outperforms existing PTQ approaches across various SR networks and datasets, while achieving performance comparable to QAT methods in most scenarios with at least a 75 speedup.</li>
<li><strong>摘要：</strong>量化技术，包括量化感知训练（QAT）和训练后量化（PTQ），已成为图像超分辨率（SR）网络推理加速的关键。与 QAT 相比，PTQ 因其消除了地面实况和模型重新训练的需要而受到了极大的关注。然而，现有的 SR PTQ 方法往往无法达到令人满意的性能，因为它们忽略了激活中异常值的影响。我们的实证分析表明，这些普遍的激活异常值与图像颜色信息密切相关，直接删除它们会导致性能显着下降。受此启发，我们提出了一种双区域量化策略，将激活划分为离群区域和密集区域，对每个区域独立应用均匀量化，以更好地平衡位宽分配。此外，我们观察到不同的网络层对量化表现出不同的敏感性，导致不同程度的性能下降。为了解决这个问题，我们引入了敏感度感知微调，鼓励模型更多地关注高度敏感的层，进一步增强量化性能。大量实验表明，我们的方法在各种 SR 网络和数据集上都优于现有的 PTQ 方法，同时在大多数情况下实现了与 QAT 方法相当的性能，加速至少为 75。</li>
</ul>

<h3>Title: Evolve to Inspire: Novelty Search for Diverse Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Alex Inch, Passawis Chaiyapattanaporn, Yuchen Zhu, Yuan Lu, Ting-Wen Ko, Davide Paglieri</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00686">https://arxiv.org/abs/2511.00686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00686">https://arxiv.org/pdf/2511.00686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00686]] Evolve to Inspire: Novelty Search for Diverse Image Generation(https://arxiv.org/abs/2511.00686)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models, while proficient at generating high-fidelity im- ages, often suffer from limited output diversity, hindering their application in exploratory and ideation tasks. Existing prompt optimization techniques typically target aesthetic fitness or are ill-suited to the creative visual domain. To address this shortcoming, we introduce WANDER, a novelty search-based approach to generating diverse sets of images from a single input prompt. WANDER operates directly on natural language prompts, employing a Large Language Model (LLM) for semantic evolution of diverse sets of images, and using CLIP embeddings to quantify novelty. We additionally apply emitters to guide the search into distinct regions of the prompt space, and demonstrate that they boost the diversity of the generated images. Empirical evaluations using FLUX-DEV for generation and GPT-4o-mini for mutation demonstrate that WANDER significantly outperforms existing evolutionary prompt optimization baselines in diversity metrics. Ablation studies confirm the efficacy of emitters.</li>
<li><strong>摘要：</strong>文本到图像的扩散模型虽然擅长生成高保真图像，但常常受到输出多样性有限的影响，阻碍了它们在探索和构思任务中的应用。现有的提示优化技术通常以审美适应性为目标，或者不适合创意视觉领域。为了解决这个缺点，我们引入了 WANDER，这是一种新颖的基于搜索的方法，可以从单个输入提示生成不同的图像集。 WANDER 直接根据自然语言提示运行，采用大型语言模型 (LLM) 进行不同图像集的语义演化，并使用 CLIP 嵌入来量化新颖性。我们还应用发射器来引导搜索到提示空间的不同区域，并证明它们可以提高生成图像的多样性。使用 FLUX-DEV 进行生成和 GPT-4o-mini 进行突变的实证评估表明，WANDER 在多样性指标方面显着优于现有的进化提示优化基线。消融研究证实了发射器的功效。</li>
</ul>

<h3>Title: Toward Better Optimization of Low-Dose CT Enhancement: A Critical Analysis of Loss Functions and Image Quality Assessment Metrics</h3>
<ul>
<li><strong>Authors: </strong>Taifour Yousra, Beghdadi Azeddine, Marie Luong, Zuheng Ming</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00698">https://arxiv.org/abs/2511.00698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00698">https://arxiv.org/pdf/2511.00698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00698]] Toward Better Optimization of Low-Dose CT Enhancement: A Critical Analysis of Loss Functions and Image Quality Assessment Metrics(https://arxiv.org/abs/2511.00698)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Low-dose CT (LDCT) imaging is widely used to reduce radiation exposure to mitigate high exposure side effects, but often suffers from noise and artifacts that affect diagnostic accuracy. To tackle this issue, deep learning models have been developed to enhance LDCT images. Various loss functions have been employed, including classical approaches such as Mean Square Error and adversarial losses, as well as customized loss functions(LFs) designed for specific architectures. Although these models achieve remarkable performance in terms of PSNR and SSIM, these metrics are limited in their ability to reflect perceptual quality, especially for medical images. In this paper, we focus on one of the most critical elements of DL-based architectures, namely the loss function. We conduct an objective analysis of the relevance of different loss functions for LDCT image quality enhancement and their consistency with image quality metrics. Our findings reveal inconsistencies between LFs and quality metrics, and highlight the need of consideration of image quality metrics when developing a new loss function for image quality enhancement.</li>
<li><strong>摘要：</strong>低剂量 CT (LDCT) 成像广泛用于减少辐射暴露，以减轻高暴露副作用，但经常受到噪声和伪影的影响，影响诊断准确性。为了解决这个问题，开发了深度学习模型来增强 LDCT 图像。人们已经采用了各种损失函数，包括均方误差和对抗性损失等经典方法，以及为特定架构设计的定制损失函数（LF）。尽管这些模型在 PSNR 和 SSIM 方面取得了显着的性能，但这些指标反映感知质量的能力有限，尤其是对于医学图像。在本文中，我们重点关注基于深度学习的架构中最关键的元素之一，即损失函数。我们对不同损失函数与 LDCT 图像质量增强的相关性及其与图像质量指标的一致性进行了客观分析。我们的研究结果揭示了 LF 和质量指标之间的不一致，并强调在开发用于图像质量增强的新损失函数时需要考虑图像质量指标。</li>
</ul>

<h3>Title: Inference-Time Chain-of-Thought Pruning with Latent Informativeness Signals</h3>
<ul>
<li><strong>Authors: </strong>Sophie Li (1), Nicholas Huang (2), Nayan Saxena (3), Nina Luo (4), Vincent Lin (5), Kevin Zhu (3), Sunishchal Dev (3) ((1) Columbia University, (2) University of British Columbia, (3) Algoverse AI Research, (4) Harvey Mudd College, (5) University of Florida)</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00699">https://arxiv.org/abs/2511.00699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00699">https://arxiv.org/pdf/2511.00699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00699]] Inference-Time Chain-of-Thought Pruning with Latent Informativeness Signals(https://arxiv.org/abs/2511.00699)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) improve reasoning accuracy when generating multiple candidate solutions at test time, but standard methods like Best-of-N (BoN) incur high computational cost by fully generating all branches. Self-Truncation Best-of-N (ST-BoN) mitigates this by truncating unpromising paths early, but its reliance on consistency-based heuristics is a limitation as it does not directly evaluate branch quality. We present KL-Adjusted Pruned Path Algorithm (KAPPA), an inference-time method that combines Kullback-Leibler divergence, confidence, and entropy into a principled scoring function to guide progressive pruning. By promoting diversity during exploration and selectively eliminating low-scoring branches, KAPPA maintains accuracy while substantially reducing memory and token usage. Experiments on GSM8K and MATH500 with DeepSeek-R1-Distill-Qwen-1.5B and Qwen2.5-7B-Instruct demonstrate that KAPPA stabilizes performance in smaller models and achieves up to ~60% reduction in peak memory and ~90% reduction in total token generation relative to BoN, with minimal impact on accuracy.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在测试时生成多个候选解决方案时可提高推理准确性，但 Best-of-N (BoN) 等标准方法会因完全生成所有分支而产生较高的计算成本。自截断 Best-of-N (ST-BoN) 通过尽早截断无希望的路径来缓解这种情况，但它对基于一致性的启发式方法的依赖是一个限制，因为它不直接评估分支质量。我们提出了 KL 调整剪枝路径算法（KAPPA），这是一种推理时间方法，它将 Kullback-Leibler 散度、置信度和熵结合到一个有原则的评分函数中，以指导渐进剪枝。通过在探索期间促进多样性并有选择地消除低分分支，KAPPA 保持准确性，同时大幅减少内存和令牌使用。使用 DeepSeek-R1-Distill-Qwen-1.5B 和 Qwen2.5-7B-Instruct 在 GSM8K 和 MATH500 上进行的实验表明，KAPPA 稳定了较小模型中的性能，相对于 BoN，峰值内存减少了约 60%，令牌生成总量减少了约 90%，对准确性的影响最小。</li>
</ul>

<h3>Title: Privacy-Aware Time Series Synthesis via Public Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Penghang Liu, Haibei Zhu, Eleonora Kreacic, Svitlana Vyetrenko</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00700">https://arxiv.org/abs/2511.00700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00700">https://arxiv.org/pdf/2511.00700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00700]] Privacy-Aware Time Series Synthesis via Public Knowledge Distillation(https://arxiv.org/abs/2511.00700)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Sharing sensitive time series data in domains such as finance, healthcare, and energy consumption, such as patient records or investment accounts, is often restricted due to privacy concerns. Privacy-aware synthetic time series generation addresses this challenge by enforcing noise during training, inherently introducing a trade-off between privacy and utility. In many cases, sensitive sequences is correlated with publicly available, non-sensitive contextual metadata (e.g., household electricity consumption may be influenced by weather conditions and electricity prices). However, existing privacy-aware data generation methods often overlook this opportunity, resulting in suboptimal privacy-utility trade-offs. In this paper, we present Pub2Priv, a novel framework for generating private time series data by leveraging heterogeneous public knowledge. Our model employs a self-attention mechanism to encode public data into temporal and feature embeddings, which serve as conditional inputs for a diffusion model to generate synthetic private sequences. Additionally, we introduce a practical metric to assess privacy by evaluating the identifiability of the synthetic data. Experimental results show that Pub2Priv consistently outperforms state-of-the-art benchmarks in improving the privacy-utility trade-off across finance, energy, and commodity trading domains.</li>
<li><strong>摘要：</strong>由于隐私问题，在金融、医疗保健和能源消耗等领域共享敏感时间序列数据（例如患者记录或投资账户）通常受到限制。具有隐私意识的合成时间序列生成通过在训练期间强制噪声来解决这一挑战，本质上引入了隐私和实用性之间的权衡。在许多情况下，敏感序列与公开可用的非敏感上下文元数据相关（例如，家庭用电量可能受到天气条件和电价的影响）。然而，现有的隐私感知数据生成方法常常忽视这个机会，导致隐私与效用的权衡不理想。在本文中，我们提出了 Pub2Priv，一种利用异构公共知识生成私有时间序列数据的新颖框架。我们的模型采用自注意力机制将公共数据编码为时间和特征嵌入，这些嵌入作为扩散模型的条件输入来生成合成的私有序列。此外，我们引入了一种实用的指标，通过评估合成数据的可识别性来评估隐私。实验结果表明，Pub2Priv 在改善金融、能源和商品交易领域的隐私与效用权衡方面始终优于最先进的基准。</li>
</ul>

<h3>Title: TRISKELION-1: Unified Descriptive-Predictive-Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Nardeep Kumar, Arun Kanwar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00711">https://arxiv.org/abs/2511.00711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00711">https://arxiv.org/pdf/2511.00711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00711]] TRISKELION-1: Unified Descriptive-Predictive-Generative AI(https://arxiv.org/abs/2511.00711)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>TRISKELION-1 is a unified descriptive-predictive-generative architecture that integrates statistical, mechanistic, and generative reasoning within a single encoder-decoder framework. The model demonstrates how descriptive representation learning, predictive inference, and generative synthesis can be jointly optimized using variational objectives. Experiments on MNIST validate that descriptive reconstruction, predictive classification, and generative sampling can coexist stably within one model. The framework provides a blueprint toward universal intelligence architectures that connect interpretability, accuracy, and creativity.</li>
<li><strong>摘要：</strong>TRISKELION-1 是一种统一的描述性预测生成架构，它将统计、机械和生成推理集成在单个编码器解码器框架内。该模型演示了如何使用变分目标联合优化描述性表示学习、预测推理和生成合成。 MNIST 上的实验验证了描述性重建、预测分类和生成采样可以在一个模型中稳定共存。该框架为连接可解释性、准确性和创造力的通用智能架构提供了蓝图。</li>
</ul>

<h3>Title: Effective Series Decomposition and Components Learning for Time Series Generation</h3>
<ul>
<li><strong>Authors: </strong>Zixuan Ma, Chenfeng Huang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00747">https://arxiv.org/abs/2511.00747</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00747">https://arxiv.org/pdf/2511.00747</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00747]] Effective Series Decomposition and Components Learning for Time Series Generation(https://arxiv.org/abs/2511.00747)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Time series generation focuses on modeling the underlying data distribution and resampling to produce authentic time series data. Key components, such as trend and seasonality, drive temporal fluctuations, yet many existing approaches fail to employ interpretative decomposition methods, limiting their ability to synthesize meaningful trend and seasonal patterns. To address this gap, we introduce Seasonal-Trend Diffusion (STDiffusion), a novel framework for multivariate time series generation that integrates diffusion probabilistic models with advanced learnable series decomposition techniques, enhancing the interpretability of the generation process. Our approach separates the trend and seasonal learning into distinct blocks: a Multi-Layer Perceptron (MLP) structure captures the trend, while adaptive wavelet distillation facilitates effective multi-resolution learning of seasonal components. This decomposition improves the interpretability of the model on multiple scales. In addition, we designed a comprehensive correction mechanism aimed at ensuring that the generated components exhibit a high degree of internal consistency and preserve meaningful interrelationships with one another. Our empirical studies on eight real-world datasets demonstrate that STDiffusion achieves state-of-the-art performance in time series generation tasks. Furthermore, we extend the model's application to multi-window long-sequence time series generation, which delivered reliable results and highlighted its robustness and versatility.</li>
<li><strong>摘要：</strong>时间序列生成侧重于对底层数据分布进行建模并重新采样以生成真实的时间序列数据。趋势和季节性等关键组成部分会驱动时间波动，但许多现有方法未能采用解释性分解方法，限制了它们综合有意义的趋势和季节性模式的能力。为了解决这一差距，我们引入了季节性趋势扩散（STDiffusion），这是一种用于多元时间序列生成的新颖框架，它将扩散概率模型与先进的可学习序列分解技术相结合，增强了生成过程的可解释性。我们的方法将趋势和季节性学习分成不同的块：多层感知器（MLP）结构捕获趋势，而自适应小波蒸馏促进季节性成分的有效多分辨率学习。这种分解提高了模型在多个尺度上的可解释性。此外，我们设计了一种全面的校正机制，旨在确保生成的组件表现出高度的内部一致性，并保持彼此之间有意义的相互关系。我们对八个真实世界数据集的实证研究表明，STDiffusion 在时间序列生成任务中实现了最先进的性能。此外，我们将该模型的应用扩展到多窗口长序列时间序列生成，提供了可靠的结果并突出了其稳健性和多功能性。</li>
</ul>

<h3>Title: Erasing 'Ugly' from the Internet: Propagation of the Beauty Myth in Text-Image Models</h3>
<ul>
<li><strong>Authors: </strong>Tanvi Dinkar, Aiqi Jiang, Gavin Abercrombie, Ioannis Konstas</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00749">https://arxiv.org/abs/2511.00749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00749">https://arxiv.org/pdf/2511.00749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00749]] Erasing 'Ugly' from the Internet: Propagation of the Beauty Myth in Text-Image Models(https://arxiv.org/abs/2511.00749)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Social media has exacerbated the promotion of Western beauty norms, leading to negative self-image, particularly in women and girls, and causing harm such as body dysmorphia. Increasingly content on the internet has been artificially generated, leading to concerns that these norms are being exaggerated. The aim of this work is to study how generative AI models may encode 'beauty' and erase 'ugliness', and discuss the implications of this for society. To investigate these aims, we create two image generation pipelines: a text-to-image model and a text-to-language model-to image model. We develop a structured beauty taxonomy which we use to prompt three language models (LMs) and two text-to-image models to cumulatively generate 5984 images using our two pipelines. We then recruit women and non-binary social media users to evaluate 1200 of the images through a Likert-scale within-subjects study. Participants show high agreement in their ratings. Our results show that 86.5% of generated images depicted people with lighter skin tones, 22% contained explicit content despite Safe for Work (SFW) training, and 74% were rated as being in a younger age demographic. In particular, the images of non-binary individuals were rated as both younger and more hypersexualised, indicating troubling intersectional effects. Notably, prompts encoded with 'negative' or 'ugly' beauty traits (such as "a wide nose") consistently produced higher Not SFW (NSFW) ratings regardless of gender. This work sheds light on the pervasive demographic biases related to beauty standards present in generative AI models -- biases that are actively perpetuated by model developers, such as via negative prompting. We conclude by discussing the implications of this on society, which include pollution of the data streams and active erasure of features that do not fall inside the stereotype of what is considered beautiful by developers.</li>
<li><strong>摘要：</strong>社交媒体加剧了西方美丽规范的推广，导致负面的自我形象，尤其是女性和女孩，并造成身体畸形等伤害。互联网上越来越多的内容是人为生成的，导致人们担心这些规范被夸大了。这项工作的目的是研究生成人工智能模型如何编码“美”并消除“丑”，并讨论这对社会的影响。为了研究这些目标，我们创建了两个图像生成管道：文本到图像模型和文本到语言模型到图像模型。我们开发了一种结构化美容分类法，用于提示三种语言模型 (LM) 和两种文本到图像模型，使用我们的两条管道累计生成 5984 张图像。然后，我们招募女性和非二元社交媒体用户，通过李克特量表的受试者内研究来评估 1200 张图像。参与者的评分表现出高度一致。我们的结果显示，86.5% 的生成图像描绘的是肤色较浅的人，22% 的图像尽管经过安全工作 (SFW) 培训，仍包含露骨内容，74% 的图像被评为年轻群体。特别是，非二元个体的形象被评为更年轻、更性感，这表明令人不安的交叉效应。值得注意的是，无论性别如何，带有“负面”或“丑陋”美容特征（例如“宽鼻子”）的提示始终会产生较高的非 SFW (NSFW) 评分。这项工作揭示了生成人工智能模型中与美容标准相关的普遍人口偏见——模型开发人员积极延续的偏见，例如通过负面提示。最后，我们讨论了这对社会的影响，其中包括数据流的污染和主动删除不属于开发人员认为美丽的刻板印象的功能。</li>
</ul>

<h3>Title: Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided Medical Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Zhihui Chen, Mengling Feng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00801">https://arxiv.org/abs/2511.00801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00801">https://arxiv.org/pdf/2511.00801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00801]] Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided Medical Image Editing(https://arxiv.org/abs/2511.00801)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in multimodal large language models have enabled remarkable medical image editing capabilities. However, the research community's progress remains constrained by the absence of large-scale, high-quality, and openly accessible datasets built specifically for medical image editing with strict anatomical and clinical constraints. We introduce Med-Banana-50K, a comprehensive 50K-image dataset for instruction-based medical image editing spanning three modalities (chest X-ray, brain MRI, fundus photography) and 23 disease types. Our dataset is constructed by leveraging Gemini-2.5-Flash-Image to generate bidirectional edits (lesion addition and removal) from real medical images. What distinguishes Med-Banana-50K from general-domain editing datasets is our systematic approach to medical quality control: we employ LLM-as-Judge with a medically grounded rubric (instruction compliance, structural plausibility, realism, and fidelity preservation) and history-aware iterative refinement up to five rounds. Beyond single-turn editing, Med-Banana-50K includes 37K failed attempts with full conversation logs for preference learning and alignment research. By providing this large-scale, medically validated, and fully documented resource, Med-Banana-50K establishes a foundation for training and evaluating the next generation of medical image editing this http URL dataset and code are publicly available at [this https URL].</li>
<li><strong>摘要：</strong>多模态大语言模型的最新进展实现了卓越的医学图像编辑能力。然而，由于缺乏专门为具有严格解剖和临床限制的医学图像编辑而构建的大规模、高质量和可公开访问的数据集，研究界的进展仍然受到限制。我们推出 Med-Banana-50K，这是一个全面的 50K 图像数据集，用于基于指令的医学图像编辑，涵盖三种模式（胸部 X 射线、脑部 MRI、眼底摄影）和 23 种疾病类型。我们的数据集是通过利用 Gemini-2.5-Flash-Image 从真实医学图像生成双向编辑（病变添加和删除）来构建的。 Med-Banana-50K 与通用领域编辑数据集的区别在于我们的系统性医疗质量控制方法：我们采用 LLM-as-Judge 以及基于医学的评分标准（指令合规性、结构合理性、真实性和保真度）和最多五轮的历史感知迭代细化。除了单轮编辑之外，Med-Banana-50K 还包括 37K 次失败的尝试，以及用于偏好学习和一致性研究的完整对话日志。通过提供这种大规模、经过医学验证且记录完整的资源，Med-Banana-50K 为训练和评估下一代医学图像编辑奠定了基础。此 http URL 数据集和代码可在 [此 https URL] 上公开获取。</li>
</ul>

<h3>Title: EraseFlow: Learning Concept Erasure Policies via GFlowNet-Driven Alignment</h3>
<ul>
<li><strong>Authors: </strong>Abhiram Kusumba, Maitreya Patel, Kyle Min, Changhoon Kim, Chitta Baral, Yezhou Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00804">https://arxiv.org/abs/2511.00804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00804">https://arxiv.org/pdf/2511.00804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00804]] EraseFlow: Learning Concept Erasure Policies via GFlowNet-Driven Alignment(https://arxiv.org/abs/2511.00804)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Erasing harmful or proprietary concepts from powerful text to image generators is an emerging safety requirement, yet current "concept erasure" techniques either collapse image quality, rely on brittle adversarial losses, or demand prohibitive retraining cycles. We trace these limitations to a myopic view of the denoising trajectories that govern diffusion based generation. We introduce EraseFlow, the first framework that casts concept unlearning as exploration in the space of denoising paths and optimizes it with GFlowNets equipped with the trajectory balance objective. By sampling entire trajectories rather than single end states, EraseFlow learns a stochastic policy that steers generation away from target concepts while preserving the model's prior. EraseFlow eliminates the need for carefully crafted reward models and by doing this, it generalizes effectively to unseen concepts and avoids hackable rewards while improving the performance. Extensive empirical results demonstrate that EraseFlow outperforms existing baselines and achieves an optimal trade off between performance and prior preservation.</li>
<li><strong>摘要：</strong>从强大的文本到图像生成器中删除有害或专有概念是一项新兴的安全要求，但当前的“概念删除”技术要么会破坏图像质量，要么依赖于脆弱的对抗性损失，要么需要令人望而却步的再训练周期。我们将这些限制归因于对控制基于扩散的生成的去噪轨迹的短视看法。我们介绍了 EraseFlow，这是第一个框架，它将概念取消学习作为去噪路径空间中的探索，并使用配备轨迹平衡目标的 GFlowNet 对其进行优化。通过对整个轨迹而不是单个最终状态进行采样，EraseFlow 学习了一种随机策略，该策略可以引导生成远离目标概念，同时保留模型的先验。 EraseFlow 消除了对精心设计的奖励模型的需要，通过这样做，它可以有效地推广到看不见的概念，并在提高性能的同时避免可破解的奖励。大量的实证结果表明，EraseFlow 的性能优于现有基线，并在性能和先前保存之间实现了最佳权衡。</li>
</ul>

<h3>Title: GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding</h3>
<ul>
<li><strong>Authors: </strong>Shijie Zhou, Viet Dac Lai, Hao Tan, Jihyung Kil, Wanrong Zhu, Changyou Chen, Ruiyi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00810">https://arxiv.org/abs/2511.00810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00810">https://arxiv.org/pdf/2511.00810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00810]] GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding(https://arxiv.org/abs/2511.00810)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Graphical user interface (GUI) grounding is a key function of computer-use agents, which maps natural-language instructions to actionable screen regions. Existing approaches based on Multimodal Large Language Models (MLLMs) typically formulate it as a text-based coordinate generation task, yet directly generating precise coordinates from visual inputs remains challenging and computationally intensive. An intuitive way to implement GUI grounding is to first select visual patches relevant to the instructions and then determine the precise click location within those patches. Based on the observations that general MLLMs have some native grounding capability, nested within their attentions, we propose GUI-AIMA, an attention-based and coordinate-free supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns the intrinsic multimodal attention of MLLMs with patch-wise grounding signals. These signals are calculated adaptively for diverse user instructions by multi-head aggregation on simplified query-visual attention matrices. Besides, its coordinate-free manner can easily integrate a plug-and-play zoom-in stage. GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional data efficiency and verifying that light training can trigger the native grounding capability of MLLMs. It achieves state-of-the-art performance among 3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2% on OSWorld-G. Project page: this https URL</li>
<li><strong>摘要：</strong>图形用户界面（GUI）基础是计算机使用代理的一项关键功能，它将自然语言指令映射到可操作的屏幕区域。基于多模态大语言模型（MLLM）的现有方法通常将其表述为基于文本的坐标生成任务，但直接从视觉输入生成精确坐标仍然具有挑战性且计算量大。实现 GUI 接地的直观方法是首先选择与指令相关的视觉补丁，然后确定这些补丁内的精确点击位置。基于对一般 MLLM 具有一些嵌套在其注意力内的本机基础能力的观察，我们提出了 GUI-AIMA，这是一种基于注意力、无坐标的监督微调框架，用于高效的 GUI 基础。 GUI-AIMA 将 MLLM 的内在多模态注意力与补丁接地信号结合起来。通过在简化的查询视觉注意矩阵上进行多头聚合，针对不同的用户指令自适应地计算这些信号。此外，其无坐标方式可以轻松集成即插即用的放大舞台。 GUI-AIMA-3B 仅使用 85k 屏幕截图进行训练，展示了卓越的数据效率并验证了轻训练可以触发 MLLM 的本机接地功能。它在 3B 模型中实现了最先进的性能，在 ScreenSpot-Pro 上达到 58.6% 的平均准确度，在 OSWorld-G 上达到 62.2% 的平均准确度。项目页面：此 https URL</li>
</ul>

<h3>Title: Linear Differential Vision Transformer: Learning Visual Contrasts via Pairwise Differentials</h3>
<ul>
<li><strong>Authors: </strong>Yifan Pu, Jixuan Ying, Qixiu Li, Tianzhu Ye, Dongchen Han, Xiaochen Wang, Ziyi Wang, Xinyu Shao, Gao Huang, Xiu Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00833">https://arxiv.org/abs/2511.00833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00833">https://arxiv.org/pdf/2511.00833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00833]] Linear Differential Vision Transformer: Learning Visual Contrasts via Pairwise Differentials(https://arxiv.org/abs/2511.00833)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision Transformers (ViTs) have become a universal backbone for both image recognition and image generation. Yet their Multi-Head Self-Attention (MHSA) layer still performs a quadratic query-key interaction for every token pair, spending the bulk of computation on visually weak or redundant correlations. We introduce Visual-Contrast Attention (VCA), a drop-in replacement for MHSA that injects an explicit notion of discrimination while reducing the theoretical complexity from O(N N C) to O(N n C) with n << N. VCA first distils each head's dense query field into a handful of spatially pooled visual-contrast tokens, then splits them into a learnable positive and negative stream whose differential interaction highlights what truly separates one region from another. The module adds fewer than 0.3M parameters to a DeiT-Tiny backbone, requires no extra FLOPs, and is wholly architecture-agnostic. Empirically, VCA lifts DeiT-Tiny top-1 accuracy on ImageNet-1K from 72.2% to 75.6% (+3.4) and improves three strong hierarchical ViTs by up to 3.1%, while in class-conditional ImageNet generation it lowers FID-50K by 2.1 to 5.2 points across both diffusion (DiT) and flow (SiT) models. Extensive ablations confirm that (i) spatial pooling supplies low-variance global cues, (ii) dual positional embeddings are indispensable for contrastive reasoning, and (iii) combining the two in both stages yields the strongest synergy. VCA therefore offers a simple path towards faster and sharper Vision Transformers. The source code is available at this https URL.</li>
<li><strong>摘要：</strong>视觉转换器 (ViT) 已成为图像识别和图像生成的通用支柱。然而，他们的多头自注意力（MHSA）层仍然对每个令牌对执行二次查询键交互，将大量计算花费在视觉上较弱或冗余的相关性上。我们引入了视觉对比注意力（VCA），它是 MHSA 的直接替代品，它注入了明确的辨别概念，同时将理论复杂性从 O(N N C) 降低到 O(N n C)（其中 n << N）。VCA 首先将每个头的密集查询字段提炼为少量空间池视觉对比标记，然后将它们分成可学习的正负流，其差异交互突出了一个区域与另一个区域的真正区别。该模块向 DeiT-Tiny 主干添加的参数少于 0.3M，不需要额外的 FLOP，并且完全与架构无关。根据经验，VCA 将 ImageNet-1K 上的 DeiT-Tiny top-1 准确率从 72.2% 提升到 75.6% (+3.4)，并将三个强大的分层 ViT 提高了 3.1%，而在类条件 ImageNet 生成中，它在扩散 (DiT) 和流 (SiT) 模型中将 FID-50K 降低了 2.1 到 5.2 个点。广泛的消融证实（i）空间池提供低方差全局线索，（ii）双重位置嵌入对于对比推理是必不可少的，（iii）在两个阶段将两者结合起来产生最强的协同作用。因此，VCA 提供了一条通往更快、更清晰的 Vision Transformer 的简单途径。源代码可从此 https URL 获取。</li>
</ul>

<h3>Title: FlexiCache: Leveraging Temporal Stability of Attention Heads for Efficient KV Cache Management</h3>
<ul>
<li><strong>Authors: </strong>Nazmul Takbir, Hamidreza Alikhani, Nikil Dutt, Sangeetha Abdu Jyothi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00868">https://arxiv.org/abs/2511.00868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00868">https://arxiv.org/pdf/2511.00868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00868]] FlexiCache: Leveraging Temporal Stability of Attention Heads for Efficient KV Cache Management(https://arxiv.org/abs/2511.00868)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) serving is increasingly constrained by the growing size of the key-value (KV) cache, which scales with both context length and generation length. Prior work shows that attention is dominated by a small subset of critical tokens, yet existing systems struggle to exploit this efficiently without degrading accuracy, especially in long generation. We make a key observation: the temporal stability of these critical tokens varies significantly across KV heads: some heads consistently focus on the same tokens, while others shift frequently. Building on this insight, we introduce FlexiCache, a hierarchical KV-cache management system that leverages the temporal stability of KV heads to reduce GPU memory usage and computation overhead, while preserving model accuracy. FlexiCache classifies KV heads as stable or unstable: it retains all KV-cache pages from unstable heads in GPU memory, whereas for stable heads, it keeps only the top-K pages on the GPU and offloads the rest to host memory. By exploiting temporal stability, FlexiCache performs periodic reranking for stable heads to fetch newly promoted top pages. Implemented atop vLLM, FlexiCache reduces GPU memory footprint for long-context requests by up to 70%, improves offline serving throughput by 1.38-1.55x, and lowers online token latency by 1.6-2.1x, all while maintaining accuracy in long-context, long-generation scenarios.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 服务越来越受到键值 (KV) 缓存大小不断增长的限制，该缓存会随着上下文长度和生成长度而扩展。先前的工作表明，注意力主要由一小部分关键令牌主导，但现有系统很难在不降低准确性的情况下有效地利用这一点，尤其是在长生成中。我们做了一个关键的观察：这些关键令牌的时间稳定性在 KV 头中存在显着差异：一些头始终关注相同的令牌，而另一些头则频繁转移。基于这一见解，我们引入了 FlexiCache，这是一种分层 KV 缓存管理系统，它利用 KV 头的时间稳定性来减少 GPU 内存使用和计算开销，同时保持模型准确性。 FlexiCache 将 KV 头分类为稳定或不稳定：它将来自不稳定头的所有 KV 缓存页面保留在 GPU 内存中，而对于稳定头，它仅在 GPU 上保留前 K 个页面，并将其余页面卸载到主机内存。通过利用时间稳定性，FlexiCache 对稳定的头执行定期重新排序，以获取新提升的首页。 FlexiCache 在 vLLM 之上实施，可将长上下文请求的 GPU 内存占用量减少高达 70%，将离线服务吞吐量提高 1.38-1.55 倍，并将在线令牌延迟降低 1.6-2.1 倍，同时保持长上下文、长生成场景中的准确性。</li>
</ul>

<h3>Title: Using Synthetic Data to estimate the True Error is theoretically and practically doable</h3>
<ul>
<li><strong>Authors: </strong>Hai Hoang Thanh, Duy-Tung Nguyen, Hung The Tran, Khoat Than</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00964">https://arxiv.org/abs/2511.00964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00964">https://arxiv.org/pdf/2511.00964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00964]] Using Synthetic Data to estimate the True Error is theoretically and practically doable(https://arxiv.org/abs/2511.00964)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Accurately evaluating model performance is crucial for deploying machine learning systems in real-world applications. Traditional methods often require a sufficiently large labeled test set to ensure a reliable evaluation. However, in many contexts, a large labeled dataset is costly and labor-intensive. Therefore, we sometimes have to do evaluation by a few labeled samples, which is theoretically challenging. Recent advances in generative models offer a promising alternative by enabling the synthesis of high-quality data. In this work, we make a systematic investigation about the use of synthetic data to estimate the test error of a trained model under limited labeled data conditions. To this end, we develop novel generalization bounds that take synthetic data into account. Those bounds suggest novel ways to optimize synthetic samples for evaluation and theoretically reveal the significant role of the generator's quality. Inspired by those bounds, we propose a theoretically grounded method to generate optimized synthetic data for model evaluation. Experimental results on simulation and tabular datasets demonstrate that, compared to existing baselines, our method achieves accurate and more reliable estimates of the test error.</li>
<li><strong>摘要：</strong>准确评估模型性能对于在实际应用中部署机器学习系统至关重要。传统方法通常需要足够大的标记测试集以确保可靠的评估。然而，在许多情况下，大型标记数据集成本高昂且劳动密集型。因此，我们有时必须通过一些标记样本来进行评估，这在理论上是有挑战性的。生成模型的最新进展通过合成高质量数据提供了一种有前途的替代方案。在这项工作中，我们对使用合成数据来估计有限标记数据条件下训练模型的测试误差进行了系统的研究。为此，我们开发了考虑合成数据的新颖的泛化界限。这些界限提出了优化合成样本进行评估的新方法，并从理论上揭示了发电机质量的重要作用。受这些界限的启发，我们提出了一种基于理论的方法来生成用于模型评估的优化合成数据。模拟和表格数据集的实验结果表明，与现有基线相比，我们的方法可以实现准确且更可靠的测试误差估计。</li>
</ul>

<h3>Title: Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow</h3>
<ul>
<li><strong>Authors: </strong>Kristiyan Sakalyan, Alessandro Palma, Filippo Guerranti, Fabian J. Theis, Stephan Günnemann</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.00977">https://arxiv.org/abs/2511.00977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.00977">https://arxiv.org/pdf/2511.00977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.00977]] Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow(https://arxiv.org/abs/2511.00977)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Understanding the evolution of cellular microenvironments in spatiotemporal data is essential for deciphering tissue development and disease progression. While experimental techniques like spatial transcriptomics now enable high-resolution mapping of tissue organization across space and time, current methods that model cellular evolution operate at the single-cell level, overlooking the coordinated development of cellular states in a tissue. We introduce NicheFlow, a flow-based generative model that infers the temporal trajectory of cellular microenvironments across sequential spatial slides. By representing local cell neighborhoods as point clouds, NicheFlow jointly models the evolution of cell states and spatial coordinates using optimal transport and Variational Flow Matching. Our approach successfully recovers both global spatial architecture and local microenvironment composition across diverse spatiotemporal datasets, from embryonic to brain development.</li>
<li><strong>摘要：</strong>了解时空数据中细胞微环境的演变对于破译组织发育和疾病进展至关重要。虽然空间转录组学等实验技术现在可以在空间和时间上对组织组织进行高分辨率绘制，但目前模拟细胞进化的方法在单细胞水平上运行，忽略了组织中细胞状态的协调发展。我们引入了 NicheFlow，这是一种基于流的生成模型，可以推断连续空间幻灯片中细胞微环境的时间轨迹。通过将局部细胞邻域表示为点云，NicheFlow 使用最优传输和变分流匹配对细胞状态和空间坐标的演化进行联合建模。我们的方法成功地恢复了从胚胎到大脑发育的不同时空数据集的全局空间结构和局部微环境组成。</li>
</ul>

<h3>Title: MedEqualizer: A Framework Investigating Bias in Synthetic Medical Data and Mitigation via Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Sama Salarian, Yue Zhang, Swati Padhee, Srinivasan Parthasarathy</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01054">https://arxiv.org/abs/2511.01054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01054">https://arxiv.org/pdf/2511.01054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01054]] MedEqualizer: A Framework Investigating Bias in Synthetic Medical Data and Mitigation via Augmentation(https://arxiv.org/abs/2511.01054)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Synthetic healthcare data generation presents a viable approach to enhance data accessibility and support research by overcoming limitations associated with real-world medical datasets. However, ensuring fairness across protected attributes in synthetic data is critical to avoid biased or misleading results in clinical research and decision-making. In this study, we assess the fairness of synthetic data generated by multiple generative adversarial network (GAN)-based models using the MIMIC-III dataset, with a focus on representativeness across protected demographic attributes. We measure subgroup representation using the logarithmic disparity metric and observe significant imbalances, with many subgroups either underrepresented or overrepresented in the synthetic data, compared to the real data. To mitigate these disparities, we introduce MedEqualizer, a model-agnostic augmentation framework that enriches the underrepresented subgroups prior to synthetic data generation. Our results show that MedEqualizer significantly improves demographic balance in the resulting synthetic datasets, offering a viable path towards more equitable and representative healthcare data synthesis.</li>
<li><strong>摘要：</strong>合成医疗保健数据生成提供了一种可行的方法，通过克服与现实世界医疗数据集相关的限制来增强数据可访问性并支持研究。然而，确保合成数据中受保护属性的公平性对于避免临床研究和决策中出现偏见或误导性结果至关重要。在本研究中，我们使用 MIMIC-III 数据集评估多个基于生成对抗网络 (GAN) 的模型生成的合成数据的公平性，重点关注受保护的人口统计属性的代表性。我们使用对数视差度量来测量子组表示，并观察到显着的不平衡，与真实数据相比，许多子组在合成数据中要么代表性不足，要么代表性过高。为了减轻这些差异，我们引入了 MedEqualizer，这是一种与模型无关的增强框架，可在生成合成数据之前丰富代表性不足的亚组。我们的结果表明，MedEqualizer 显着改善了生成的合成数据集中的人口平衡，为实现更公平和更具代表性的医疗保健数据合成提供了一条可行的途径。</li>
</ul>

<h3>Title: Window-Based Feature Engineering for Cognitive Workload Detection</h3>
<ul>
<li><strong>Authors: </strong>Andrew Hallam, R G Gayathri, Glory Lee, Atul Sajjanhar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01060">https://arxiv.org/abs/2511.01060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01060">https://arxiv.org/pdf/2511.01060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01060]] Window-Based Feature Engineering for Cognitive Workload Detection(https://arxiv.org/abs/2511.01060)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Cognitive workload is a topic of increasing interest across various fields such as health, psychology, and defense applications. In this research, we focus on classifying cognitive workload using the COLET dataset, employing a window-based approach for feature generation and machine/deep learning techniques for classification. We apply window-based temporal partitioning to enhance features used in existing research, followed by machine learning and deep learning models to classify different levels of cognitive workload. The results demonstrate that deep learning models, particularly tabular architectures, outperformed traditional machine learning methods in precision, F1-score, accuracy, and classification precision. This study highlights the effectiveness of window-based temporal feature extraction and the potential of deep learning techniques for real-time cognitive workload assessment in complex and dynamic tasks.</li>
<li><strong>摘要：</strong>认知工作负载是健康、心理学和国防应用等各个领域日益关注的话题。在这项研究中，我们专注于使用 COLET 数据集对认知工作负载进行分类，采用基于窗口的方法进行特征生成，并采用机器/深度学习技术进行分类。我们应用基于窗口的时间分区来增强现有研究中使用的功能，然后使用机器学习和深度学习模型来对不同级别的认知工作负载进行分类。结果表明，深度学习模型，特别是表格架构，在精度、F1 分数、准确度和分类精度方面优于传统机器学习方法。这项研究强调了基于窗口的时间特征提取的有效性以及深度学习技术在复杂和动态任务中实时认知工作量评估的潜力。</li>
</ul>

<h3>Title: T-MLA: A Targeted Multiscale Log--Exponential Attack Framework for Neural Image Compression</h3>
<ul>
<li><strong>Authors: </strong>Nikolay I. Kalmykov, Razan Dibo, Kaiyu Shen, Xu Zhonghan, Anh-Huy Phan, Yipeng Liu, Ivan Oseledets</a></li>
<li><strong>Subjects: </strong>cs.CV, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01079">https://arxiv.org/abs/2511.01079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01079">https://arxiv.org/pdf/2511.01079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01079]] T-MLA: A Targeted Multiscale Log--Exponential Attack Framework for Neural Image Compression(https://arxiv.org/abs/2511.01079)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Neural image compression (NIC) has become the state-of-the-art for rate-distortion performance, yet its security vulnerabilities remain significantly less understood than those of classifiers. Existing adversarial attacks on NICs are often naive adaptations of pixel-space methods, overlooking the unique, structured nature of the compression pipeline. In this work, we propose a more advanced class of vulnerabilities by introducing T-MLA, the first targeted multiscale log--exponential attack framework. Our approach crafts adversarial perturbations in the wavelet domain by directly targeting the quality of the attacked and reconstructed images. This allows for a principled, offline attack where perturbations are strategically confined to specific wavelet subbands, maximizing distortion while ensuring perceptual stealth. Extensive evaluation across multiple state-of-the-art NIC architectures on standard image compression benchmarks reveals a large drop in reconstruction quality while the perturbations remain visually imperceptible. Our findings reveal a critical security flaw at the core of generative and content delivery pipelines.</li>
<li><strong>摘要：</strong>神经图像压缩（NIC）已成为率失真性能的最先进技术，但其安全漏洞仍然比分类器的安全漏洞更不为人所知。现有的针对 NIC 的对抗性攻击通常是像素空间方法的幼稚改编，忽视了压缩管道独特的结构化性质。在这项工作中，我们通过引入 T-MLA（第一个有针对性的多尺度日志指数攻击框架）提出了更高级的漏洞类别。我们的方法通过直接针对攻击和重建图像的质量来在小波域中制造对抗性扰动。这允许进行有原则的离线攻击，其中扰动策略性地限制在特定的小波子带内，在确保感知隐形的同时最大化失真。在标准图像压缩基准上对多个最先进的 NIC 架构进行的广泛评估表明，重建质量大幅下降，而扰动在视觉上仍然难以察觉。我们的研究结果揭示了生成和内容交付管道核心的关键安全缺陷。</li>
</ul>

<h3>Title: GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction</h3>
<ul>
<li><strong>Authors: </strong>Narges Ghasemi, Amir Ziashahabi, Salman Avestimehr, Cyrus Shahabi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01082">https://arxiv.org/abs/2511.01082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01082">https://arxiv.org/pdf/2511.01082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01082]] GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction(https://arxiv.org/abs/2511.01082)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Image geolocalization, the task of determining an image's geographic origin, poses significant challenges, largely due to visual similarities across disparate locations and the large search space. To address these issues, we propose a hierarchical sequence prediction approach inspired by how humans narrow down locations from broad regions to specific addresses. Analogously, our model predicts geographic tokens hierarchically, first identifying a general region and then sequentially refining predictions to increasingly precise locations. Rather than relying on explicit semantic partitions, our method uses S2 cells, a nested, multiresolution global grid, and sequentially predicts finer-level cells conditioned on visual inputs and previous predictions. This procedure mirrors autoregressive text generation in large language models. Much like in language modeling, final performance depends not only on training but also on inference-time strategy. We investigate multiple top-down traversal methods for autoregressive sampling, incorporating techniques from test-time compute scaling used in language models. Specifically, we integrate beam search and multi-sample inference while exploring various selection strategies to determine the final output. This enables the model to manage uncertainty by exploring multiple plausible paths through the hierarchy. We evaluate our method on the Im2GPS3k and YFCC4k datasets against two distinct sets of baselines: those that operate without a Multimodal Large Language Model (MLLM) and those that leverage one. In the MLLM-free setting, our model surpasses other comparable baselines on nearly all metrics, achieving state-of-the-art performance with accuracy gains of up to 13.9%. When augmented with an MLLM, our model outperforms all baselines, setting a new state-of-the-art across all metrics. The source code is available at this https URL.</li>
<li><strong>摘要：</strong>图像地理定位是确定图像地理起源的任务，它提出了重大挑战，这主要是由于不同位置和巨大搜索空间之间的视觉相似性。为了解决这些问题，我们提出了一种分层序列预测方法，其灵感来自于人类如何将位置从广泛区域缩小到特定地址。类似地，我们的模型按层次结构预测地理标记，首先识别一般区域，然后依次将预测细化到越来越精确的位置。我们的方法不依赖显式语义分区，而是使用 S2 单元（嵌套的多分辨率全局网格），并根据视觉输入和先前的预测顺序预测更精细级别的单元。此过程反映了大型语言模型中的自回归文本生成。就像语言建模一样，最终性能不仅取决于训练，还取决于推理时间策略。我们研究了用于自回归采样的多种自上而下的遍历方法，结合了语言模型中使用的测试时计算缩放技术。具体来说，我们集成了波束搜索和多样本推理，同时探索各种选择策略来确定最终输出。这使得模型能够通过探索层次结构中的多个合理路径来管理不确定性。我们根据两组不同的基线在 Im2GPS3k 和 YFCC4k 数据集上评估我们的方法：一组没有使用多模态大语言模型 (MLLM)，另一组则利用多模态大语言模型 (MLLM)。在无 MLLM 的设置中，我们的模型在几乎所有指标上都超越了其他可比较的基线，实现了最先进的性能，准确度提升高达 13.9%。当使用 MLLM 进行增强时，我们的模型优于所有基线，在所有指标上都创下了新的最先进水平。源代码可从此 https URL 获取。</li>
</ul>

<h3>Title: SliceVision-F2I: A Synthetic Feature-to-Image Dataset for Visual Pattern Representation on Network Slices</h3>
<ul>
<li><strong>Authors: </strong>Md. Abid Hasan Rafi, Mst. Fatematuj Johora, Pankaj Bhowmik</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01087">https://arxiv.org/abs/2511.01087</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01087">https://arxiv.org/pdf/2511.01087</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01087]] SliceVision-F2I: A Synthetic Feature-to-Image Dataset for Visual Pattern Representation on Network Slices(https://arxiv.org/abs/2511.01087)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The emergence of 5G and 6G networks has established network slicing as a significant part of future service-oriented architectures, demanding refined identification methods supported by robust datasets. The article presents SliceVision-F2I, a dataset of synthetic samples for studying feature visualization in network slicing for next-generation networking systems. The dataset transforms multivariate Key Performance Indicator (KPI) vectors into visual representations through four distinct encoding methods: physically inspired mappings, Perlin noise, neural wallpapering, and fractal branching. For each encoding method, 30,000 samples are generated, each comprising a raw KPI vector and a corresponding RGB image at low-resolution pixels. The dataset simulates realistic and noisy network conditions to reflect operational uncertainties and measurement imperfections. SliceVision-F2I is suitable for tasks involving visual learning, network state classification, anomaly detection, and benchmarking of image-based machine learning techniques applied to network data. The dataset is publicly available and can be reused in various research contexts, including multivariate time series analysis, synthetic data generation, and feature-to-image transformations.</li>
<li><strong>摘要：</strong>5G和6G网络的出现使网络切片成为未来面向服务的架构的重要组成部分，需要强大的数据集支持的精细化识别方法。本文介绍了 SliceVision-F2I，这是一个合成样本数据集，用于研究下一代网络系统网络切片中的特征可视化。该数据集通过四种不同的编码方法将多元关键绩效指标 (KPI) 向量转换为视觉表示：物理启发映射、柏林噪声、神经壁纸和分形分支。对于每种编码方法，都会生成 30,000 个样本，每个样本包含一个原始 KPI 向量和一个相应的低分辨率像素 RGB 图像。该数据集模拟现实和嘈杂的网络条件，以反映操作的不确定性和测量缺陷。 SliceVision-F2I 适用于涉及视觉学习、网络状态分类、异常检测以及应用于网络数据的基于图像的机器学习技术基准测试的任务。该数据集是公开的，可以在各种研究环境中重复使用，包括多元时间序列分析、合成数据生成和特征到图像转换。</li>
</ul>

<h3>Title: One model to solve them all: 2BSDE families via neural operators</h3>
<ul>
<li><strong>Authors: </strong>Takashi Furuya, Anastasis Kratsios, Dylan Possamaï, Bogdan Raonić</a></li>
<li><strong>Subjects: </strong>cs.LG, math.AP, math.NA, math.PR, q-fin.CP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01125">https://arxiv.org/abs/2511.01125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01125">https://arxiv.org/pdf/2511.01125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01125]] One model to solve them all: 2BSDE families via neural operators(https://arxiv.org/abs/2511.01125)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce a mild generative variant of the classical neural operator model, which leverages Kolmogorov--Arnold networks to solve infinite families of second-order backward stochastic differential equations ($2$BSDEs) on regular bounded Euclidean domains with random terminal time. Our first main result shows that the solution operator associated with a broad range of $2$BSDE families is approximable by appropriate neural operator models. We then identify a structured subclass of (infinite) families of $2$BSDEs whose neural operator approximation requires only a polynomial number of parameters in the reciprocal approximation rate, as opposed to the exponential requirement in general worst-case neural operator guarantees.</li>
<li><strong>摘要：</strong>我们引入了经典神经算子模型的温和生成变体，它利用 Kolmogorov-Arnold 网络来求解具有随机终端时间的正则有界欧几里德域上的无限族二阶向后随机微分方程 ($2$BSDE)。我们的第一个主要结果表明，与广泛的 $2$BSDE 系列相关的解算子可以通过适当的神经算子模型来近似。然后，我们确定 $2$BSDE 的（无限）族的结构化子类，其神经算子逼近仅需要倒数逼近率中的多项式参数，而不是一般最坏情况神经算子保证中的指数要求。</li>
</ul>

<h3>Title: Weakly Supervised Concept Learning with Class-Level Priors for Interpretable Medical Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Md Nahiduzzaman, Steven Korevaar, Alireza Bab-Hadiashar, Ruwan Tennakoon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01131">https://arxiv.org/abs/2511.01131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01131">https://arxiv.org/pdf/2511.01131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01131]] Weakly Supervised Concept Learning with Class-Level Priors for Interpretable Medical Diagnosis(https://arxiv.org/abs/2511.01131)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Human-interpretable predictions are essential for deploying AI in medical imaging, yet most interpretable-by-design (IBD) frameworks require concept annotations for training data, which are costly and impractical to obtain in clinical contexts. Recent attempts to bypass annotation, such as zero-shot vision-language models or concept-generation frameworks, struggle to capture domain-specific medical features, leading to poor reliability. In this paper, we propose a novel Prior-guided Concept Predictor (PCP), a weakly supervised framework that enables concept answer prediction without explicit supervision or reliance on language models. PCP leverages class-level concept priors as weak supervision and incorporates a refinement mechanism with KL divergence and entropy regularization to align predictions with clinical reasoning. Experiments on PH2 (dermoscopy) and WBCatt (hematology) show that PCP improves concept-level F1-score by over 33% compared to zero-shot baselines, while delivering competitive classification performance on four medical datasets (PH2, WBCatt, HAM10000, and CXR4) relative to fully supervised concept bottleneck models (CBMs) and V-IP.</li>
<li><strong>摘要：</strong>人类可解释的预测对于在医学成像中部署人工智能至关重要，但大多数可解释设计 (IBD) 框架都需要训练数据的概念注释，而在临床环境中获得这些概念注释成本高昂且不切实际。最近尝试绕过注释，例如零样本视觉语言模型或概念生成框架，难以捕获特定领域的医学特征，导致可靠性较差。在本文中，我们提出了一种新颖的先验引导概念预测器（PCP），这是一种弱监督框架，无需显式监督或依赖语言模型即可实现概念答案预测。 PCP 利用类级概念先验作为弱监督，并结合 KL 散度和熵正则化的细化机制，使预测与临床推理保持一致。 PH2（皮肤镜检查）和 WBCatt（血液学）实验表明，与零样本基线相比，PCP 将概念级 F1 分数提高了 33% 以上，同时相对于完全监督的概念瓶颈模型 (CBM) 和 V-IP，在四个医学数据集（PH2、WBCatt、HAM10000 和 CXR4）上提供有竞争力的分类性能。</li>
</ul>

<h3>Title: ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation</h3>
<ul>
<li><strong>Authors: </strong>Yongyuan Liang, Wei Chow, Feng Li, Ziqiao Ma, Xiyao Wang, Jiageng Mao, Jiuhai Chen, Jiatao Gu, Yue Wang, Furong Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01163">https://arxiv.org/abs/2511.01163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01163">https://arxiv.org/pdf/2511.01163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01163]] ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation(https://arxiv.org/abs/2511.01163)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Unified multimodal models (UMMs) have emerged as a powerful paradigm for seamlessly unifying text and image understanding and generation. However, prevailing evaluations treat these abilities in isolation, such that tasks with multimodal inputs and outputs are scored primarily through unimodal reasoning, i.e., textual benchmarks emphasize language-based reasoning, while visual benchmarks emphasize reasoning outcomes manifested in the pixels. We introduce ROVER to address this pressing need to test reciprocal cross-modal reasoning, the use of one modality to guide, verify, or refine outputs in the other, an ability central to the vision of unified multimodal intelligence. ROVER is a human-annotated benchmark that explicitly targets reciprocal cross-modal reasoning, which contains 1312 tasks grounded in 1876 images, spanning two complementary settings. Verbally-augmented reasoning for visual generation evaluates whether models can use verbal prompts and reasoning chains to guide faithful image synthesis. Visually-augmented reasoning for verbal generation evaluates whether models can generate intermediate visualizations that strengthen their own reasoning processes for question answering. Experiments on 17 unified models reveal two key findings: (i) Cross-modal reasoning determines visual generation quality, with interleaved models significantly outperforming non-interleaved ones; notably, combining strong unimodal models fails to achieve comparable reasoning. (ii) Models show dissociation between physical and symbolic reasoning: they succeed at interpreting perceptual concepts literally but fail to construct visual abstractions for symbolic tasks, where faulty reasoning harms performance. These results highlight reciprocal cross-modal reasoning as a critical frontier for enabling true omnimodal generation.</li>
<li><strong>摘要：</strong>统一多模态模型 (UMM) 已成为无缝统一文本和图像理解和生成的强大范例。然而，主流的评估是孤立地对待这些能力的，例如具有多模态输入和输出的任务主要通过单模态推理进行评分，即文本基准强调基于语言的推理，而视觉基准则强调像素中体现的推理结果。我们引入 ROVER 是为了解决测试互惠跨模态推理的迫切需求，即使用一种模态来指导、验证或完善另一种模态的输出，这是统一多模态智能愿景的核心能力。 ROVER 是一个人工注释的基准测试，明确针对交互跨模态推理，其中包含基于 1876 个图像的 1312 个任务，跨越两个互补的设置。用于视觉生成的言语增强推理评估模型是否可以使用言语提示和推理链来指导忠实的图像合成。用于言语生成的视觉增强推理评估模型是否可以生成中间可视化，以加强其自身的问答推理过程。对 17 个统一模型的实验揭示了两个关键发现：（i）跨模态推理决定视觉生成质量，交错模型的性能显着优于非交错模型；值得注意的是，结合强大的单峰模型无法实现可比较的推理。 (ii) 模型显示物理推理和符号推理之间的分离：它们成功地从字面上解释感知概念，但无法为符号任务构建视觉抽象，而错误的推理会损害性能。这些结果强调了交互跨模态推理是实现真正全模态生成的关键前沿。</li>
</ul>

<h3>Title: Diffusion Transformer meets Multi-level Wavelet Spectrum for Single Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Peng Du, Hui Li, Han Xu, Paul Barom Jeon, Dongwook Lee, Daehyun Ji, Ran Yang, Feng Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01175">https://arxiv.org/abs/2511.01175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01175">https://arxiv.org/pdf/2511.01175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01175]] Diffusion Transformer meets Multi-level Wavelet Spectrum for Single Image Super-Resolution(https://arxiv.org/abs/2511.01175)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generation</a></li>
<li><strong>Abstract: </strong>Discrete Wavelet Transform (DWT) has been widely explored to enhance the performance of image superresolution (SR). Despite some DWT-based methods improving SR by capturing fine-grained frequency signals, most existing approaches neglect the interrelations among multiscale frequency sub-bands, resulting in inconsistencies and unnatural artifacts in the reconstructed images. To address this challenge, we propose a Diffusion Transformer model based on image Wavelet spectra for SR (DTWSR).DTWSR incorporates the superiority of diffusion models and transformers to capture the interrelations among multiscale frequency sub-bands, leading to a more consistence and realistic SR image. Specifically, we use a Multi-level Discrete Wavelet Transform (MDWT) to decompose images into wavelet spectra. A pyramid tokenization method is proposed which embeds the spectra into a sequence of tokens for transformer model, facilitating to capture features from both spatial and frequency domain. A dual-decoder is designed elaborately to handle the distinct variances in lowfrequency (LF) and high-frequency (HF) sub-bands, without omitting their alignment in image generation. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our method, with high performance on both perception quality and fidelity.</li>
<li><strong>摘要：</strong>离散小波变换（DWT）已被广泛探索以增强图像超分辨率（SR）的性能。尽管一些基于 DWT 的方法通过捕获细粒度频率信号来提高 SR，但大多数现有方法忽略了多尺度频率子带之间的相互关系，导致重建图像中的不一致和不自然的伪影。为了应对这一挑战，我们提出了一种基于图像小波谱的 SR 扩散变换器模型 (DTWSR)。DTWSR 结合了扩散模型和变换器的优势来捕获多尺度频率子带之间的相互关系，从而获得更加一致和真实的 SR 图像。具体来说，我们使用多级离散小波变换（MDWT）将图像分解为小波谱。提出了一种金字塔标记化方法，将频谱嵌入到变压器模型的标记序列中，有助于从空间和频域捕获特征。双解码器经过精心设计，可以处理低频 (LF) 和高频 (HF) 子带的明显差异，同时不会忽略它们在图像生成中的对齐。对多个基准数据集的大量实验证明了我们方法的有效性，在感知质量和保真度方面都具有高性能。</li>
</ul>

<h3>Title: A Topology-Aware Graph Convolutional Network for Human Pose Similarity and Action Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Minmin Zeng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01194">https://arxiv.org/abs/2511.01194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01194">https://arxiv.org/pdf/2511.01194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01194]] A Topology-Aware Graph Convolutional Network for Human Pose Similarity and Action Quality Assessment(https://arxiv.org/abs/2511.01194)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Action Quality Assessment (AQA) requires fine-grained understanding of human motion and precise evaluation of pose similarity. This paper proposes a topology-aware Graph Convolutional Network (GCN) framework, termed GCN-PSN, which models the human skeleton as a graph to learn discriminative, topology-sensitive pose embeddings. Using a Siamese architecture trained with a contrastive regression objective, our method outperforms coordinate-based baselines and achieves competitive performance on AQA-7 and FineDiving benchmarks. Experimental results and ablation studies validate the effectiveness of leveraging skeletal topology for pose similarity and action quality assessment.</li>
<li><strong>摘要：</strong>动作质量评估（AQA）需要对人体运动的细粒度理解和对姿势相似性的精确评估。本文提出了一种拓扑感知图卷积网络（GCN）框架，称为 GCN-PSN，它将人体骨骼建模为图来学习有区别的、拓扑敏感的姿势嵌入。使用经过对比回归目标训练的 Siamese 架构，我们的方法优于基于坐标的基线，并在 AQA-7 和 FineDiving 基准上实现了具有竞争力的性能。实验结果和消融研究验证了利用骨骼拓扑进行姿势相似性和动作质量评估的有效性。</li>
</ul>

<h3>Title: MoSa: Motion Generation with Scalable Autoregressive Modeling</h3>
<ul>
<li><strong>Authors: </strong>Mengyuan Liu, Sheng Yan, Yong Wang, Yingjie Li, Gui-Bin Bian, Hong Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01200">https://arxiv.org/abs/2511.01200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01200">https://arxiv.org/pdf/2511.01200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01200]] MoSa: Motion Generation with Scalable Autoregressive Modeling(https://arxiv.org/abs/2511.01200)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We introduce MoSa, a novel hierarchical motion generation framework for text-driven 3D human motion generation that enhances the Vector Quantization-guided Generative Transformers (VQ-GT) paradigm through a coarse-to-fine scalable generation process. In MoSa, we propose a Multi-scale Token Preservation Strategy (MTPS) integrated into a hierarchical residual vector quantization variational autoencoder (RQ-VAE). MTPS employs interpolation at each hierarchical quantization to effectively retain coarse-to-fine multi-scale tokens. With this, the generative transformer supports Scalable Autoregressive (SAR) modeling, which predicts scale tokens, unlike traditional methods that predict only one token at each step. Consequently, MoSa requires only 10 inference steps, matching the number of RQ-VAE quantization layers. To address potential reconstruction degradation from frequent interpolation, we propose CAQ-VAE, a lightweight yet expressive convolution-attention hybrid VQ-VAE. CAQ-VAE enhances residual block design and incorporates attention mechanisms to better capture global dependencies. Extensive experiments show that MoSa achieves state-of-the-art generation quality and efficiency, outperforming prior methods in both fidelity and speed. On the Motion-X dataset, MoSa achieves an FID of 0.06 (versus MoMask's 0.20) while reducing inference time by 27 percent. Moreover, MoSa generalizes well to downstream tasks such as motion editing, requiring no additional fine-tuning. The code is available at this https URL</li>
<li><strong>摘要：</strong>我们介绍了 MoSa，这是一种用于文本驱动的 3D 人体运动生成的新型分层运动生成框架，它通过从粗到细的可扩展生成过程增强了矢量量化引导的生成变压器 (VQ-GT) 范例。在 MoSa 中，我们提出了一种集成到分层残差矢量量化变分自动编码器（RQ-VAE）中的多尺度令牌保存策略（MTPS）。 MTPS 在每个分层量化时采用插值来有效保留从粗到细的多尺度标记。这样，生成转换器支持可扩展自回归 (SAR) 建模，该模型可以预测比例标记，这与每一步仅预测一个标记的传统方法不同。因此，MoSa 仅需要 10 个推理步骤，与 RQ-VAE 量化层的数量相匹配。为了解决频繁插值带来的潜在重建退化问题，我们提出了 CAQ-VAE，一种轻量级但富有表现力的卷积-注意力混合 VQ-VAE。 CAQ-VAE 增强了残差块设计并结合了注意力机制以更好地捕获全局依赖性。大量实验表明，MoSa 实现了最先进的发电质量和效率，在保真度和速度方面均优于现有方法。在 Motion-X 数据集上，MoSa 实现了 0.06 的 FID（相对于 MoMask 的 0.20），同时将推理时间缩短了 27%。此外，MoSa 可以很好地推广到动作编辑等下游任务，不需要额外的微调。该代码可在此 https URL 获取</li>
</ul>

<h3>Title: WindMiL: Equivariant Graph Learning for Wind Loading Prediction</h3>
<ul>
<li><strong>Authors: </strong>Themistoklis Vargiemezis, Charilaos Kanatsoulis, Catherine Gorlé</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01226">https://arxiv.org/abs/2511.01226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01226">https://arxiv.org/pdf/2511.01226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01226]] WindMiL: Equivariant Graph Learning for Wind Loading Prediction(https://arxiv.org/abs/2511.01226)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Accurate prediction of wind loading on buildings is crucial for structural safety and sustainable design, yet conventional approaches such as wind tunnel testing and large-eddy simulation (LES) are prohibitively expensive for large-scale exploration. Each LES case typically requires at least 24 hours of computation, making comprehensive parametric studies infeasible. We introduce WindMiL, a new machine learning framework that combines systematic dataset generation with symmetry-aware graph neural networks (GNNs). First, we introduce a large-scale dataset of wind loads on low-rise buildings by applying signed distance function interpolation to roof geometries and simulating 462 cases with LES across varying shapes and wind directions. Second, we develop a reflection-equivariant GNN that guarantees physically consistent predictions under mirrored geometries. Across interpolation and extrapolation evaluations, WindMiL achieves high accuracy for both the mean and the standard deviation of surface pressure coefficients (e.g., RMSE $\leq 0.02$ for mean $C_p$) and remains accurate under reflected-test evaluation, maintaining hit rates above $96\%$ where the non-equivariant baseline model drops by more than $10\%$. By pairing a systematic dataset with an equivariant surrogate, WindMiL enables efficient, scalable, and accurate predictions of wind loads on buildings.</li>
<li><strong>摘要：</strong>准确预测建筑物上的风荷载对于结构安全和可持续设计至关重要，但风洞测试和大涡模拟 (LES) 等传统方法对于大规模勘探来说成本高昂。每个 LES 案例通常需要至少 24 小时的计算，使得全面的参数研究不可行。我们引入了 WindMiL，这是一种新的机器学习框架，它将系统数据集生成与对称感知图神经网络 (GNN) 相结合。首先，我们通过将符号距离函数插值应用于屋顶几何形状，并使用 LES 模拟不同形状和风向的 462 个案例，引入了低层建筑风荷载的大规模数据集。其次，我们开发了一个反射等变 GNN，保证镜像几何下物理一致的预测。在插值和外推评估中，WindMiL 的表面压力系数平均值和标准偏差均实现了高精度（例如，平均值 $C_p$ 的 RMSE $\leq 0.02$），并且在反射测试评估下保持准确，将命中率保持在 $96\%$ 以上，其中非等变基线模型下降超过 $10\%$。通过将系统数据集与等变代理配对，WindMiL 能够高效、可扩展且准确地预测建筑物上的风荷载。</li>
</ul>

<h3>Title: Gesture Generation (Still) Needs Improved Human Evaluation Practices: Insights from a Community-Driven State-of-the-Art Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Rajmund Nagy (1), Hendric Voss (2), Thanh Hoang-Minh (3), Mihail Tsakov (4), Teodor Nikolov (5), Zeyi Zhang (6), Tenglong Ao (6), Sicheng Yang (7), Shaoli Huang (8), Yongkang Cheng (8), M. Hamza Mughal (9), Rishabh Dabral (9), Kiran Chhatre (1), Christian Theobalt (9), Libin Liu (6), Stefan Kopp (2), Rachel McDonnell (10), Michael Neff (11), Taras Kucherenko (12), Youngwoo Yoon (13), Gustav Eje Henter (1 and 5) ((1) KTH Royal Institute of Technology, (2) Bielefeld University, (3) University of Science -- VNUHCM, (4) Independent Researcher, (5) Motorica AB, (6) Peking University, (7) Huawei Technologies Ltd., (8) Astribot, (9) Max-Planck Institute for Informatics, SIC, (10) Trinity College Dublin, (11) University of California, Davis, (12) SEED -- Electronic Arts, (13) Electronics and Telecommunications Research Institute (ETRI))</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01233">https://arxiv.org/abs/2511.01233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01233">https://arxiv.org/pdf/2511.01233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01233]] Gesture Generation (Still) Needs Improved Human Evaluation Practices: Insights from a Community-Driven State-of-the-Art Benchmark(https://arxiv.org/abs/2511.01233)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We review human evaluation practices in automated, speech-driven 3D gesture generation and find a lack of standardisation and frequent use of flawed experimental setups. This leads to a situation where it is impossible to know how different methods compare, or what the state of the art is. In order to address common shortcomings of evaluation design, and to standardise future user studies in gesture-generation works, we introduce a detailed human evaluation protocol for the widely-used BEAT2 motion-capture dataset. Using this protocol, we conduct large-scale crowdsourced evaluation to rank six recent gesture-generation models -- each trained by its original authors -- across two key evaluation dimensions: motion realism and speech-gesture alignment. Our results provide strong evidence that 1) newer models do not consistently outperform earlier approaches; 2) published claims of high motion realism or speech-gesture alignment may not hold up under rigorous evaluation; and 3) the field must adopt disentangled assessments of motion quality and multimodal alignment for accurate benchmarking in order to make progress. Finally, in order to drive standardisation and enable new evaluation research, we will release five hours of synthetic motion from the benchmarked models; over 750 rendered video stimuli from the user studies -- enabling new evaluations without model reimplementation required -- alongside our open-source rendering script, and the 16,000 pairwise human preference votes collected for our benchmark.</li>
<li><strong>摘要：</strong>我们回顾了自动化、语音驱动的 3D 手势生成中的人类评估实践，发现缺乏标准化并且经常使用有缺陷的实验设置。这导致了一种情况，即不可能知道不同方法的比较情况，或者最新技术是什么。为了解决评估设计的常见缺点，并标准化未来手势生成工作中的用户研究，我们为广泛使用的 BEAT2 动作捕捉数据集引入了详细的人体评估协议。使用该协议，我们进行大规模众包评估，对六个最近的手势生成模型进行排名——每个模型均由其原始作者训练——跨越两个关键评估维度：运动真实性和语音手势对齐。我们的结果提供了强有力的证据表明：1）新模型的性能并不总是优于早期方法； 2) 已发表的高运动真实感或语音手势对齐的主张可能无法经过严格的评估； 3）该领域必须采用运动质量和多模态对准的解开评估来进行准确的基准测试，才能取得进展。最后，为了推动标准化并启用新的评估研究，我们将发布基准模型的 5 小时合成运动；来自用户研究的超过 750 个渲染视频刺激（无需重新实现模型即可进行新评估）以及我们的开源渲染脚本，以及为我们的基准收集的 16,000 个成对人类偏好投票。</li>
</ul>

<h3>Title: A Saddle Point Remedy: Power of Variable Elimination in Non-convex Optimization</h3>
<ul>
<li><strong>Authors: </strong>Min Gan, Guang-Yong Chen, Yang Yi, Lin Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01234">https://arxiv.org/abs/2511.01234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01234">https://arxiv.org/pdf/2511.01234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01234]] A Saddle Point Remedy: Power of Variable Elimination in Non-convex Optimization(https://arxiv.org/abs/2511.01234)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The proliferation of saddle points, rather than poor local minima, is increasingly understood to be a primary obstacle in large-scale non-convex optimization for machine learning. Variable elimination algorithms, like Variable Projection (VarPro), have long been observed to exhibit superior convergence and robustness in practice, yet a principled understanding of why they so effectively navigate these complex energy landscapes has remained elusive. In this work, we provide a rigorous geometric explanation by comparing the optimization landscapes of the original and reduced formulations. Through a rigorous analysis based on Hessian inertia and the Schur complement, we prove that variable elimination fundamentally reshapes the critical point structure of the objective function, revealing that local maxima in the reduced landscape are created from, and correspond directly to, saddle points in the original formulation. Our findings are illustrated on the canonical problem of non-convex matrix factorization, visualized directly on two-parameter neural networks, and finally validated in training deep Residual Networks, where our approach yields dramatic improvements in stability and convergence to superior minima. This work goes beyond explaining an existing method; it establishes landscape simplification via saddle point transformation as a powerful principle that can guide the design of a new generation of more robust and efficient optimization algorithms.</li>
<li><strong>摘要：</strong>鞍点的扩散，而不是糟糕的局部极小值，越来越多地被认为是机器学习大规模非凸优化的主要障碍。变量消除算法，如变量投影（VarPro），长期以来一直被观察到在实践中表现出卓越的收敛性和鲁棒性，但对于它们为何如此有效地驾驭这些复杂的能源景观的原则性理解仍然难以捉摸。在这项工作中，我们通过比较原始配方和简化配方的优化景观，提供了严格的几何解释。通过基于 Hessian 惯性和 Schur 补的严格分析，我们证明变量消除从根本上重塑了目标函数的临界点结构，揭示了简化景观中的局部最大值是从原始公式中的鞍点创建的，并直接对应于鞍点。我们的研究结果说明了非凸矩阵分解的典型问题，直接在二参数神经网络上可视化，并最终在深度残差网络的训练中得到验证，我们的方法在稳定性和收敛到卓越最小值方面产生了显着的改进。这项工作不仅仅是解释现有的方法；它将通过鞍点变换建立景观简化作为一个强大的原则，可以指导新一代更强大和更高效的优化算法的设计。</li>
</ul>

<h3>Title: MotionStream: Real-Time Video Generation with Interactive Motion Controls</h3>
<ul>
<li><strong>Authors: </strong>Joonghyuk Shin, Zhengqi Li, Richard Zhang, Jun-Yan Zhu, Jaesik Park, Eli Schechtman, Xun Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01266">https://arxiv.org/abs/2511.01266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01266">https://arxiv.org/pdf/2511.01266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01266]] MotionStream: Real-Time Video Generation with Interactive Motion Controls(https://arxiv.org/abs/2511.01266)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Current motion-conditioned video generation methods suffer from prohibitive latency (minutes per video) and non-causal processing that prevents real-time interaction. We present MotionStream, enabling sub-second latency with up to 29 FPS streaming generation on a single GPU. Our approach begins by augmenting a text-to-video model with motion control, which generates high-quality videos that adhere to the global text prompt and local motion guidance, but does not perform inference on the fly. As such, we distill this bidirectional teacher into a causal student through Self Forcing with Distribution Matching Distillation, enabling real-time streaming inference. Several key challenges arise when generating videos of long, potentially infinite time-horizons: (1) bridging the domain gap from training on finite length and extrapolating to infinite horizons, (2) sustaining high quality by preventing error accumulation, and (3) maintaining fast inference, without incurring growth in computational cost due to increasing context windows. A key to our approach is introducing carefully designed sliding-window causal attention, combined with attention sinks. By incorporating self-rollout with attention sinks and KV cache rolling during training, we properly simulate inference-time extrapolations with a fixed context window, enabling constant-speed generation of arbitrarily long videos. Our models achieve state-of-the-art results in motion following and video quality while being two orders of magnitude faster, uniquely enabling infinite-length streaming. With MotionStream, users can paint trajectories, control cameras, or transfer motion, and see results unfold in real-time, delivering a truly interactive experience.</li>
<li><strong>摘要：</strong>当前的运动调节视频生成方法存在令人望而却步的延迟（每个视频几分钟）和阻碍实时交互的非因果处理。我们推出了 MotionStream，可在单个 GPU 上实现亚秒级延迟和高达 29 FPS 的流生成。我们的方法首先通过运动控制增强文本到视频模型，该模型生成遵循全局文本提示和本地运动指导的高质量视频，但不会即时执行推理。因此，我们通过自我强迫和分布匹配蒸馏将双向教师提炼为因果学生，从而实现实时流推理。生成长的、可能无限的时间范围的视频时会出现几个关键挑战：（1）弥合有限长度训练和外推到无限范围之间的领域差距，（2）通过防止错误积累来维持高质量，以及（3）保持快速推理，而不会因上下文窗口的增加而导致计算成本增加。我们方法的关键是引入精心设计的滑动窗口因果注意力，并与注意力池相结合。通过在训练期间将自推出与注意池和 KV 缓存滚动相结合，我们可以使用固定的上下文窗口正确地模拟推理时间外推，从而能够恒速生成任意长的视频。我们的模型在运动跟踪和视频质量方面实现了最先进的结果，同时速度提高了两个数量级，独特地实现了无限长度的流媒体。借助 MotionStream，用户可以绘制轨迹、控制摄像机或传输运动，并实时查看结果展开，从而提供真正的交互式体验。</li>
</ul>

<h3>Title: PRevivor: Reviving Ancient Chinese Paintings using Prior-Guided Color Transformers</h3>
<ul>
<li><strong>Authors: </strong>Tan Tang, Yanhong Wu, Junming Gao, Yingcai Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01274">https://arxiv.org/abs/2511.01274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01274">https://arxiv.org/pdf/2511.01274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01274]] PRevivor: Reviving Ancient Chinese Paintings using Prior-Guided Color Transformers(https://arxiv.org/abs/2511.01274)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Ancient Chinese paintings are a valuable cultural heritage that is damaged by irreversible color degradation. Reviving color-degraded paintings is extraordinarily difficult due to the complex chemistry mechanism. Progress is further slowed by the lack of comprehensive, high-quality datasets, which hampers the creation of end-to-end digital restoration tools. To revive colors, we propose PRevivor, a prior-guided color transformer that learns from recent paintings (e.g., Ming and Qing Dynasty) to restore ancient ones (e.g., Tang and Song Dynasty). To develop PRevivor, we decompose color restoration into two sequential sub-tasks: luminance enhancement and hue correction. For luminance enhancement, we employ two variational U-Nets and a multi-scale mapping module to translate faded luminance into restored counterparts. For hue correction, we design a dual-branch color query module guided by localized hue priors extracted from faded paintings. Specifically, one branch focuses attention on regions guided by masked priors, enforcing localized hue correction, whereas the other branch remains unconstrained to maintain a global reasoning capability. To evaluate PRevivor, we conduct extensive experiments against state-of-the-art colorization methods. The results demonstrate superior performance both quantitatively and qualitatively.</li>
<li><strong>摘要：</strong>中国古代绘画是一种宝贵的文化遗产，但由于不可逆转的色彩退化而受到损害。由于复杂的化学机制，恢复颜色退化的绘画是非常困难的。由于缺乏全面、高质量的数据集，进展进一步放缓，这阻碍了端到端数字修复工具的创建。为了恢复色彩，我们提出了 PREvivor，一种预先引导的色彩转换器，可以从最近的绘画（例如明清时期）中学习来恢复古代绘画（例如唐宋时期）。为了开发 PRevivor，我们将颜色恢复分解为两个连续的子任务：亮度增强和色调校正。对于亮度增强，我们采用两个变分 U-Net 和一个多尺度映射模块，将褪色的亮度转换为恢复的亮度。对于色调校正，我们设计了一个双分支颜色查询模块，该模块由从褪色绘画中提取的局部色调先验引导。具体来说，一个分支将注意力集中在由屏蔽先验引导的区域上，强制进行局部色调校正，而另一个分支则不受约束以保持全局推理能力。为了评估 PRevivor，我们针对最先进的着色方法进行了广泛的实验。结果在数量和质量上都证明了卓越的性能。</li>
</ul>

<h3>Title: Adaptation of Foundation Models for Medical Image Analysis: Strategies, Challenges, and Future Directions</h3>
<ul>
<li><strong>Authors: </strong>Karma Phuntsho, Abdullah, Kyungmi Lee, Ickjai Lee, Euijoon Ahn</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01284">https://arxiv.org/abs/2511.01284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01284">https://arxiv.org/pdf/2511.01284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01284]] Adaptation of Foundation Models for Medical Image Analysis: Strategies, Challenges, and Future Directions(https://arxiv.org/abs/2511.01284)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Foundation models (FMs) have emerged as a transformative paradigm in medical image analysis, offering the potential to provide generalizable, task-agnostic solutions across a wide range of clinical tasks and imaging modalities. Their capacity to learn transferable representations from large-scale data has the potential to address the limitations of conventional task-specific models. However, adaptation of FMs to real-world clinical practice remains constrained by key challenges, including domain shifts, limited availability of high-quality annotated data, substantial computational demands, and strict privacy requirements. This review presents a comprehensive assessment of strategies for adapting FMs to the specific demands of medical imaging. We examine approaches such as supervised fine-tuning, domain-specific pretraining, parameter-efficient fine-tuning, self-supervised learning, hybrid methods, and multimodal or cross-modal frameworks. For each, we evaluate reported performance gains, clinical applicability, and limitations, while identifying trade-offs and unresolved challenges that prior reviews have often overlooked. Beyond these established techniques, we also highlight emerging directions aimed at addressing current gaps. These include continual learning to enable dynamic deployment, federated and privacy-preserving approaches to safeguard sensitive data, hybrid self-supervised learning to enhance data efficiency, data-centric pipelines that combine synthetic generation with human-in-the-loop validation, and systematic benchmarking to assess robust generalization under real-world clinical variability. By outlining these strategies and associated research gaps, this review provides a roadmap for developing adaptive, trustworthy, and clinically integrated FMs capable of meeting the demands of real-world medical imaging.</li>
<li><strong>摘要：</strong>基础模型 (FM) 已成为医学图像分析领域的变革范式，有望在广泛的临床任务和成像模式中提供通用的、与任务无关的解决方案。它们从大规模数据中学习可转移表示的能力有可能解决传统任务特定模型的局限性。然而，FM 适应现实世界的临床实践仍然受到关键挑战的限制，包括领域转移、高质量注释数据的可用性有限、大量的计算需求和严格的隐私要求。本综述对 FM 适应医学成像特定需求的策略进行了全面评估。我们研究了监督微调、特定领域预训练、参数高效微调、自我监督学习、混合方法以及多模态或跨模态框架等方法。对于每一个，我们都会评估报告的性能提升、临床适用性和局限性，同时确定之前的评论经常忽视的权衡和未解决的挑战。除了这些已建立的技术之外，我们还强调了旨在解决当前差距的新兴方向。其中包括实现动态部署的持续学习、保护敏感数据的联合和隐私保护方法、提高数据效率的混合自监督学习、将合成生成与人在环验证相结合的以数据为中心的管道，以及评估现实世界临床变异性下稳健泛化的系统基准测试。通过概述这些策略和相关的研究差距，本综述为开发能够满足现实世界医学成像需求的自适应、值得信赖和临床集成的 FM 提供了路线图。</li>
</ul>

<h3>Title: Koopman-based Prediction of Connectivity for Flying Ad Hoc Networks</h3>
<ul>
<li><strong>Authors: </strong>Sivaram Krishnan, Jinho Choi, Jihong Park, Gregory Sherman, Benjamin Campbell</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01286">https://arxiv.org/abs/2511.01286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01286">https://arxiv.org/pdf/2511.01286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01286]] Koopman-based Prediction of Connectivity for Flying Ad Hoc Networks(https://arxiv.org/abs/2511.01286)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The application of machine learning (ML) to communication systems is expected to play a pivotal role in future artificial intelligence (AI)-based next-generation wireless networks. While most existing works focus on ML techniques for static wireless environments, they often face limitations when applied to highly dynamic environments, such as flying ad hoc networks (FANETs). This paper explores the use of data-driven Koopman approaches to address these challenges. Specifically, we investigate how these approaches can model UAV trajectory dynamics within FANETs, enabling more accurate predictions and improved network performance. By leveraging Koopman operator theory, we propose two possible approaches -- centralized and distributed -- to efficiently address the challenges posed by the constantly changing topology of FANETs. To demonstrate this, we consider a FANET performing surveillance with UAVs following pre-determined trajectories and predict signal-to-interference-plus-noise ratios (SINRs) to ensure reliable communication between UAVs. Our results show that these approaches can accurately predict connectivity and isolation events that lead to modelled communication outages. This capability could help UAVs schedule their transmissions based on these predictions.</li>
<li><strong>摘要：</strong>机器学习（ML）在通信系统中的应用预计将在未来基于人工智能（AI）的下一代无线网络中发挥关键作用。虽然大多数现有工作都专注于静态无线环境的机器学习技术，但它们在应用于高度动态的环境（例如飞行自组织网络（FANET））时通常面临限制。本文探讨了使用数据驱动的库普曼方法来应对这些挑战。具体来说，我们研究了这些方法如何对 FANET 内的无人机轨迹动态进行建模，从而实现更准确的预测并提高网络性能。通过利用 Koopman 算子理论，我们提出了两种可能的方法——集中式和分布式——来有效解决 FANET 不断变化的拓扑带来的挑战。为了证明这一点，我们考虑使用 FANET 按照预定轨迹对无人机进行监视，并预测信号干扰加噪声比 (SINR)，以确保无人机之间的可靠通信。我们的结果表明，这些方法可以准确预测导致建模通信中断的连接和隔离事件。此功能可以帮助无人机根据这些预测安排传输。</li>
</ul>

<h3>Title: Detecting Generated Images by Fitting Natural Image Distributions</h3>
<ul>
<li><strong>Authors: </strong>Yonggang Zhang, Jun Nie, Xinmei Tian, Mingming Gong, Kun Zhang, Bo Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01293">https://arxiv.org/abs/2511.01293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01293">https://arxiv.org/pdf/2511.01293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01293]] Detecting Generated Images by Fitting Natural Image Distributions(https://arxiv.org/abs/2511.01293)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The increasing realism of generated images has raised significant concerns about their potential misuse, necessitating robust detection methods. Current approaches mainly rely on training binary classifiers, which depend heavily on the quantity and quality of available generated images. In this work, we propose a novel framework that exploits geometric differences between the data manifolds of natural and generated images. To exploit this difference, we employ a pair of functions engineered to yield consistent outputs for natural images but divergent outputs for generated ones, leveraging the property that their gradients reside in mutually orthogonal subspaces. This design enables a simple yet effective detection method: an image is identified as generated if a transformation along its data manifold induces a significant change in the loss value of a self-supervised model pre-trained on natural images. Further more, to address diminishing manifold disparities in advanced generative models, we leverage normalizing flows to amplify detectable differences by extruding generated images away from the natural image manifold. Extensive experiments demonstrate the efficacy of this method. Code is available at this https URL.</li>
<li><strong>摘要：</strong>生成的图像日益真实，引起了人们对其潜在滥用的严重担忧，因此需要强大的检测方法。当前的方法主要依赖于训练二元分类器，这在很大程度上取决于可用生成图像的数量和质量。在这项工作中，我们提出了一种新颖的框架，该框架利用自然图像和生成图像的数据流形之间的几何差异。为了利用这种差异，我们采用了一对函数，这些函数被设计为自然图像产生一致的输出，但生成的图像产生发散的输出，利用它们的梯度驻留在相互正交的子空间中的属性。这种设计实现了一种简单而有效的检测方法：如果沿数据流形的变换导致在自然图像上预训练的自监督模型的损失值发生显着变化，则图像被识别为生成。此外，为了解决先进生成模型中流形差异不断减小的问题，我们利用归一化流通过将生成的图像从自然图像流形中挤出来放大可检测的差异。大量的实验证明了该方法的有效性。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: UniREditBench: A Unified Reasoning-based Image Editing Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Feng Han, Yibin Wang, Chenglin Li, Zheming Liang, Dianyi Wang, Yang Jiao, Zhipeng Wei, Chao Gong, Cheng Jin, Jingjing Chen, Jiaqi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01295">https://arxiv.org/abs/2511.01295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01295">https://arxiv.org/pdf/2511.01295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01295]] UniREditBench: A Unified Reasoning-based Image Editing Benchmark(https://arxiv.org/abs/2511.01295)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in multi-modal generative models have driven substantial improvements in image editing. However, current generative models still struggle with handling diverse and complex image editing tasks that require implicit reasoning, underscoring the need for a comprehensive benchmark to systematically assess their performance across various reasoning scenarios. Existing benchmarks primarily focus on single-object attribute transformation in realistic scenarios, which, while effective, encounter two key challenges: (1) they largely overlook multi-object interactions as well as game-world scenarios that involve human-defined rules, which are common in real-life applications; (2) they only rely on textual references to evaluate the generated images, potentially leading to systematic misjudgments, especially in complex reasoning scenarios. To this end, this work proposes UniREditBench, a unified benchmark for reasoning-based image editing evaluation. It comprises 2,700 meticulously curated samples, covering both real- and game-world scenarios across 8 primary dimensions and 18 sub-dimensions. To improve evaluation reliability, we introduce multimodal dual-reference evaluation, providing both textual and ground-truth image references for each sample assessment. Furthermore, we design an automated multi-scenario data synthesis pipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with high-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel on this dataset and develop UniREdit-Bagel, demonstrating substantial improvements in both in-domain and out-of-distribution settings. Through thorough benchmarking of both open-source and closed-source image editing models, we reveal their strengths and weaknesses across various aspects.</li>
<li><strong>摘要：</strong>多模态生成模型的最新进展推动了图像编辑的显着改进。然而，当前的生成模型仍然难以处理需要隐式推理的多样化且复杂的图像编辑任务，这凸显了需要一个全面的基准来系统地评估其在各种推理场景中的性能。现有的基准主要关注现实场景中的单对象属性转换，虽然有效，但遇到两个关键挑战：（1）它们在很大程度上忽视了多对象交互以及涉及人类定义规则的游戏世界场景，这在现实生活应用中很常见； （2）它们仅依赖文本参考来评估生成的图像，可能导致系统性误判，尤其是在复杂的推理场景中。为此，本工作提出了UniREditBench，一个基于推理的图像编辑评估的统一基准。它包含 2,700 个精心策划的样本，涵盖 8 个主要维度和 18 个子维度的现实和游戏世界场景。为了提高评估的可靠性，我们引入了多模态双参考评估，为每个样本评估提供文本和真实图像参考。此外，我们设计了一个自动化的多场景数据合成管道，并构建了 UniREdit-Data-100K，这是一个具有高质量思想链（CoT）推理注释的大规模合成数据集。我们在此数据集上微调 Bagel 并开发 UniREdit-Bagel，展示了域内和分发外设置的重大改进。通过对开源和闭源图像编辑模型进行彻底的基准测试，我们揭示了它们在各个方面的优点和缺点。</li>
</ul>

<h3>Title: Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Tae-Young Lee, Juwon Seo, Jong Hwan Ko, Gyeong-Moon Park</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01307">https://arxiv.org/abs/2511.01307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01307">https://arxiv.org/pdf/2511.01307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01307]] Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models(https://arxiv.org/abs/2511.01307)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion models have enabled high-quality synthesis of specific subjects, such as identities or objects. This capability, while unlocking new possibilities in content creation, also introduces significant privacy risks, as personalization techniques can be misused by malicious users to generate unauthorized content. Although several studies have attempted to counter this by generating adversarially perturbed samples designed to disrupt personalization, they rely on unrealistic assumptions and become ineffective in the presence of even a few clean images or under simple image transformations. To address these challenges, we shift the protection target from the images to the diffusion model itself to hinder the personalization of specific subjects, through our novel framework called Anti-Personalized Diffusion Models (APDM). We first provide a theoretical analysis demonstrating that a naive approach of existing loss functions to diffusion models is inherently incapable of ensuring convergence for robust anti-personalization. Motivated by this finding, we introduce Direct Protective Optimization (DPO), a novel loss function that effectively disrupts subject personalization in the target model without compromising generative quality. Moreover, we propose a new dual-path optimization strategy, coined Learning to Protect (L2P). By alternating between personalization and protection paths, L2P simulates future personalization trajectories and adaptively reinforces protection at each step. Experimental results demonstrate that our framework outperforms existing methods, achieving state-of-the-art performance in preventing unauthorized personalization. The code is available at this https URL.</li>
<li><strong>摘要：</strong>扩散模型的最新进展使得特定主题（例如身份或物体）的高质量合成成为可能。此功能在释放内容创建新可能性的同时，也带来了重大的隐私风险，因为恶意用户可能会滥用个性化技术来生成未经授权的内容。尽管一些研究试图通过生成旨在破坏个性化的对抗性扰动样本来应对这一问题，但它们依赖于不切实际的假设，并且在存在一些干净图像或简单图像转换的情况下变得无效。为了应对这些挑战，我们通过称为反个性化扩散模型（APDM）的新颖框架，将保护目标从图像转移到扩散模型本身，以阻碍特定主题的个性化。我们首先提供理论分析，证明现有损失函数对扩散模型的简单方法本质上无法确保鲁棒反个性化的收敛。受这一发现的启发，我们引入了直接保护优化（DPO），这是一种新颖的损失函数，可以有效地破坏目标模型中的受试者个性化，而不会影响生成质量。此外，我们提出了一种新的双路径优化策略，称为学习保护（L2P）。通过在个性化和保护路径之间交替，L2P 模拟未来的个性化轨迹，并在每一步自适应地加强保护。实验结果表明，我们的框架优于现有方法，在防止未经授权的个性化方面实现了最先进的性能。该代码可从此 https URL 获取。</li>
</ul>

<h3>Title: A Generative Adversarial Approach to Adversarial Attacks Guided by Contrastive Language-Image Pre-trained Model</h3>
<ul>
<li><strong>Authors: </strong>Sampriti Soor, Alik Pramanick, Jothiprakash K, Arijit Sur</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01317">https://arxiv.org/abs/2511.01317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01317">https://arxiv.org/pdf/2511.01317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01317]] A Generative Adversarial Approach to Adversarial Attacks Guided by Contrastive Language-Image Pre-trained Model(https://arxiv.org/abs/2511.01317)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid growth of deep learning has brought about powerful models that can handle various tasks, like identifying images and understanding language. However, adversarial attacks, an unnoticed alteration, can deceive models, leading to inaccurate predictions. In this paper, a generative adversarial attack method is proposed that uses the CLIP model to create highly effective and visually imperceptible adversarial perturbations. The CLIP model's ability to align text and image representation helps incorporate natural language semantics with a guided loss to generate effective adversarial examples that look identical to the original inputs. This integration allows extensive scene manipulation, creating perturbations in multi-object environments specifically designed to deceive multilabel classifiers. Our approach integrates the concentrated perturbation strategy from Saliency-based Auto-Encoder (SSAE) with the dissimilar text embeddings similar to Generative Adversarial Multi-Object Scene Attacks (GAMA), resulting in perturbations that both deceive classification models and maintain high structural similarity to the original images. The model was tested on various tasks across diverse black-box victim models. The experimental results show that our method performs competitively, achieving comparable or superior results to existing techniques, while preserving greater visual fidelity.</li>
<li><strong>摘要：</strong>深度学习的快速发展带来了强大的模型，可以处理各种任务，例如识别图像和理解语言。然而，对抗性攻击（一种未被注意到的改变）可能会欺骗模型，导致预测不准确。本文提出了一种生成对抗性攻击方法，该方法使用 CLIP 模型创建高效且视觉上难以察觉的对抗性扰动。 CLIP 模型对齐文本和图像表示的能力有助于将自然语言语义与引导损失相结合，以生成看起来与原始输入相同的有效对抗性示例。这种集成允许广泛的场景操作，在专门设计用于欺骗多标签分类器的多对象环境中产生扰动。我们的方法将基于显着性的自动编码器（SSAE）的集中扰动策略与类似于生成对抗性多对象场景攻击（GAMA）的不同文本嵌入相结合，产生既欺骗分类模型又保持与原始图像的高度结构相似性的扰动。该模型在不同的黑盒受害者模型中的各种任务上进行了测试。实验结果表明，我们的方法具有竞争力，实现了与现有技术相当或更好的结果，同时保持了更高的视觉保真度。</li>
</ul>

<h3>Title: Diffusion-Based Solver for CNF Placement on the Cloud-Continuum</h3>
<ul>
<li><strong>Authors: </strong>Álvaro Vázquez Rodríguez, Manuel Fernández-Veiga, Carlos Giraldo-Rodríguez</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01343">https://arxiv.org/abs/2511.01343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01343">https://arxiv.org/pdf/2511.01343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01343]] Diffusion-Based Solver for CNF Placement on the Cloud-Continuum(https://arxiv.org/abs/2511.01343)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The placement of Cloud-Native Network Functions (CNFs) across the Cloud-Continuum represents a core challenge in the orchestration of current 5G and future 6G networks. The process involves the placement of interdependent computing tasks, structured as Service Function Chains, over distributed cloud infrastructures. This is achieved while satisfying strict resource, bandwidth and latency constraints. It is acknowledged that classical approaches, including mixed-integer nonlinear programming, heuristics and reinforcement learning are limited in terms of scalability, constraint handling and generalisation capacity. In the present study, a novel theoretical framework is proposed, which is based on Denoising Diffusion Probabilistic Models (DDPM) for CNF placement. The present approach proposes a reconceptualisation of placement as a generative graph to assignment task, where the placement problem is encoded as a heterogeneous graph, and a Graph Neural Network denoiser is trained to iteratively refine noisy CNF-to-cloud assignment matrices. The model incorporates constraint-specific losses directly into the loss function, thereby allowing it to learn feasible solution spaces. The integration of the DDPM formulation with structured combinatorial constraints is achieved through a rigorous and systematic approach. Extensive evaluations across diverse topologies have been conducted, which have confirmed that the model consistently produces feasible solutions with orders of magnitude faster inference than MINLP solvers. The results obtained demonstrate the potential of diffusion-based generative modelling for constrained network embedding problems, making an impact towards the practical, scalable orchestration of distributed Cloud-Native Network Functions.</li>
<li><strong>摘要：</strong>在 Cloud-Continuum 中放置云原生网络功能 (CNF) 是当前 5G 和未来 6G 网络编排的核心挑战。该过程涉及在分布式云基础设施上放置相互依赖的计算任务（结构为服务功能链）。这是在满足严格的资源、带宽和延迟限制的情况下实现的。众所周知，包括混合整数非线性规划、启发式和强化学习在内的经典方法在可扩展性、约束处理和泛化能力方面受到限制。在本研究中，提出了一种新的理论框架，该框架基于用于 CNF 放置的去噪扩散概率模型（DDPM）。本方法提出将放置重新概念化为分配任务的生成图，其中放置问题被编码为异构图，并且训练图神经网络降噪器以迭代地细化噪声 CNF 到云分配矩阵。该模型将特定于约束的损失直接合并到损失函数中，从而使其能够学习可行的解决方案空间。 DDPM 公式与结构化组合约束的集成是通过严格且系统的方法实现的。我们对不同的拓扑进行了广泛的评估，证实该模型始终能够生成可行的解决方案，其推理速度比 MINLP 求解器快几个数量级。获得的结果证明了基于扩散的生成模型对于约束网络嵌入问题的潜力，对分布式云原生网络功能的实用、可扩展编排产生了影响。</li>
</ul>

<h3>Title: Expanding the Content-Style Frontier: a Balanced Subspace Blending Approach for Content-Style LoRA Fusion</h3>
<ul>
<li><strong>Authors: </strong>Linhao Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01355">https://arxiv.org/abs/2511.01355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01355">https://arxiv.org/pdf/2511.01355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01355]] Expanding the Content-Style Frontier: a Balanced Subspace Blending Approach for Content-Style LoRA Fusion(https://arxiv.org/abs/2511.01355)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advancements in text-to-image diffusion models have significantly improved the personalization and stylization of generated images. However, previous studies have only assessed content similarity under a single style intensity. In our experiments, we observe that increasing style intensity leads to a significant loss of content features, resulting in a suboptimal content-style frontier. To address this, we propose a novel approach to expand the content-style frontier by leveraging Content-Style Subspace Blending and a Content-Style Balance loss. Our method improves content similarity across varying style intensities, significantly broadening the content-style frontier. Extensive experiments demonstrate that our approach outperforms existing techniques in both qualitative and quantitative evaluations, achieving superior content-style trade-off with significantly lower Inverted Generational Distance (IGD) and Generational Distance (GD) scores compared to current methods.</li>
<li><strong>摘要：</strong>文本到图像扩散模型的最新进展显着改善了生成图像的个性化和风格化。然而，先前的研究仅评估单一风格强度下的内容相似性。在我们的实验中，我们观察到增加风格强度会导致内容特征的显着损失，从而导致内容风格边界不理想。为了解决这个问题，我们提出了一种新颖的方法，通过利用内容风格子空间混合和内容风格平衡损失来扩展内容风格前沿。我们的方法提高了不同风格强度的内容相似性，显着拓宽了内容风格边界。大量的实验表明，我们的方法在定性和定量评估方面均优于现有技术，与当前方法相比，实现了卓越的内容风格权衡，并且倒代距离（IGD）和代距离（GD）分数显着降低。</li>
</ul>

<h3>Title: Learning Intractable Multimodal Policies with Reparameterization and Diversity Regularization</h3>
<ul>
<li><strong>Authors: </strong>Ziqi Wang, Jiashun Liu, Ling Pan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01374">https://arxiv.org/abs/2511.01374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01374">https://arxiv.org/pdf/2511.01374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01374]] Learning Intractable Multimodal Policies with Reparameterization and Diversity Regularization(https://arxiv.org/abs/2511.01374)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Traditional continuous deep reinforcement learning (RL) algorithms employ deterministic or unimodal Gaussian actors, which cannot express complex multimodal decision distributions. This limitation can hinder their performance in diversity-critical scenarios. There have been some attempts to design online multimodal RL algorithms based on diffusion or amortized actors. However, these actors are intractable, making existing methods struggle with balancing performance, decision diversity, and efficiency simultaneously. To overcome this challenge, we first reformulate existing intractable multimodal actors within a unified framework, and prove that they can be directly optimized by policy gradient via reparameterization. Then, we propose a distance-based diversity regularization that does not explicitly require decision probabilities. We identify two diversity-critical domains, namely multi-goal achieving and generative RL, to demonstrate the advantages of multimodal policies and our method, particularly in terms of few-shot robustness. In conventional MuJoCo benchmarks, our algorithm also shows competitive performance. Moreover, our experiments highlight that the amortized actor is a promising policy model class with strong multimodal expressivity and high performance. Our code is available at this https URL</li>
<li><strong>摘要：</strong>传统的连续深度强化学习 (RL) 算法采用确定性或单峰高斯参与者，无法表达复杂的多峰决策分布。这种限制可能会影响它们在多样性关键场景中的性能。人们已经尝试过设计基于扩散或摊销参与者的在线多模态强化学习算法。然而，这些参与者很棘手，使得现有方法难以同时平衡性能、决策多样性和效率。为了克服这一挑战，我们首先在统一的框架内重新制定现有的棘手的多模式参与者，并证明它们可以通过重新参数化通过政策梯度直接优化。然后，我们提出了一种基于距离的多样性正则化，它不明确需要决策概率。我们确定了两个多样性关键领域，即多目标实现和生成强化学习，以证明多模式策略和我们的方法的优势，特别是在小样本鲁棒性方面。在传统的 MuJoCo 基准测试中，我们的算法也显示出具有竞争力的性能。此外，我们的实验强调，摊销参与者是一种有前途的政策模型类，具有很强的多模态表达能力和高性能。我们的代码可在此 https URL 获取</li>
</ul>

<h3>Title: SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Mao, Junsi Li, Haoji Zhang, Yu Liang, Ming Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01390">https://arxiv.org/abs/2511.01390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01390">https://arxiv.org/pdf/2511.01390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01390]] SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment(https://arxiv.org/abs/2511.01390)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Fine-grained cross-modal alignment aims to establish precise local correspondences between vision and language, forming a cornerstone for visual question answering and related multimodal applications. Current approaches face challenges in addressing patch redundancy and ambiguity, which arise from the inherent information density disparities across modalities. Recently, Multimodal Large Language Models (MLLMs) have emerged as promising solutions to bridge this gap through their robust semantic generation capabilities. However, the dense textual outputs from MLLMs may introduce conflicts with the original sparse captions. Furthermore, accurately quantifying semantic relevance between rich visual patches and concise textual descriptions remains a core challenge. To overcome these limitations, we introduce the Semantic-Enhanced Patch Slimming (SEPS) framework, which systematically addresses patch redundancy and ambiguity. Our approach employs a two-stage mechanism to integrate unified semantics from both dense and sparse texts, enabling the identification of salient visual patches. Additionally, it leverages relevance-aware selection with mean value computation to highlight crucial patch-word correspondences, thereby improving cross-modal similarity assessment. Comprehensive experiments on Flickr30K and MS-COCO datasets validate that SEPS achieves superior performance, surpassing existing approaches by 23\%-86\% in rSum across diverse model architectures, with notable enhancements in text-to-image retrieval scenarios. Our implementation is available at this https URL.</li>
<li><strong>摘要：</strong>细粒度的跨模态对齐旨在建立视觉和语言之间精确的局部对应关系，为视觉问答和相关多模态应用奠定基础。当前的方法在解决补丁冗余和模糊性方面面临挑战，这些挑战是由不同模式固有的信息密度差异引起的。最近，多模态大型语言模型（MLLM）作为有前景的解决方案出现，通过其强大的语义生成功能来弥补这一差距。然而，MLLM 的密集文本输出可能会与原始稀疏字幕产生冲突。此外，准确量化丰富的视觉块和简洁的文本描述之间的语义相关性仍然是一个核心挑战。为了克服这些限制，我们引入了语义增强补丁精简（SEPS）框架，该框架系统地解决了补丁冗余和歧义问题。我们的方法采用两阶段机制来集成密集和稀疏文本的统一语义，从而能够识别显着的视觉斑块。此外，它利用相关性感知选择和平均值计算来突出关键的补丁词对应关系，从而改进跨模式相似性评估。在 Flickr30K 和 MS-COCO 数据集上的综合实验验证了 SEPS 实现了卓越的性能，在跨不同模型架构的 rSum 方面超越了现有方法 23%-86%，并且在文本到图像检索场景中具有显着增强。我们的实现可以通过此 https URL 获得。</li>
</ul>

<h3>Title: Towards One-step Causal Video Generation via Adversarial Self-Distillation</h3>
<ul>
<li><strong>Authors: </strong>Yongqi Yang, Huayang Huang, Xu Peng, Xiaobin Hu, Donghao Luo, Jiangning Zhang, Chengjie Wang, Yu Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01419">https://arxiv.org/abs/2511.01419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01419">https://arxiv.org/pdf/2511.01419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01419]] Towards One-step Causal Video Generation via Adversarial Self-Distillation(https://arxiv.org/abs/2511.01419)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent hybrid video generation models combine autoregressive temporal dynamics with diffusion-based spatial denoising, but their sequential, iterative nature leads to error accumulation and long inference times. In this work, we propose a distillation-based framework for efficient causal video generation that enables high-quality synthesis with extremely limited denoising steps. Our approach builds upon the Distribution Matching Distillation (DMD) framework and proposes a novel Adversarial Self-Distillation (ASD) strategy, which aligns the outputs of the student model's n-step denoising process with its (n+1)-step version at the distribution level. This design provides smoother supervision by bridging small intra-student gaps and more informative guidance by combining teacher knowledge with locally consistent student behavior, substantially improving training stability and generation quality in extremely few-step scenarios (e.g., 1-2 steps). In addition, we present a First-Frame Enhancement (FFE) strategy, which allocates more denoising steps to the initial frames to mitigate error propagation while applying larger skipping steps to later frames. Extensive experiments on VBench demonstrate that our method surpasses state-of-the-art approaches in both one-step and two-step video generation. Notably, our framework produces a single distilled model that flexibly supports multiple inference-step settings, eliminating the need for repeated re-distillation and enabling efficient, high-quality video synthesis.</li>
<li><strong>摘要：</strong>最近的混合视频生成模型将自回归时间动态与基于扩散的空间去噪相结合，但它们的顺序迭代性质导致错误累积和较长的推理时间。在这项工作中，我们提出了一种基于蒸馏的框架，用于高效的因果视频生成，能够以极其有限的去噪步骤实现高质量的合成。我们的方法建立在分布匹配蒸馏 (DMD) 框架的基础上，并提出了一种新颖的对抗性自蒸馏 (ASD) 策略，该策略将学生模型的 n 步去噪过程的输出与其在分布级别的 (n+1) 步版本保持一致。这种设计通过缩小学生内部的差距来提供更顺畅的监督，并通过将教师知识与本地一致的学生行为相结合来提供更信息丰富的指导，从而在极少步骤的场景（例如 1-2 步）中显着提高训练稳定性和生成质量。此外，我们提出了第一帧增强（FFE）策略，该策略为初始帧分配更多的去噪步骤以减轻错误传播，同时对后面的帧应用更大的跳跃步骤。 VBench 上的大量实验表明，我们的方法在一步和两步视频生成方面都超越了最先进的方法。值得注意的是，我们的框架生成了一个单一的蒸馏模型，可以灵活地支持多个推理步骤设置，从而消除了重复重新蒸馏的需要，并实现了高效、高质量的视频合成。</li>
</ul>

<h3>Title: Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Jie Du, Xinyu Gong, Qingshan Tan, Wen Li, Yangming Cheng, Weitao Wang, Chenlu Zhan, Suhui Wu, Hao Zhang, Jun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01450">https://arxiv.org/abs/2511.01450</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01450">https://arxiv.org/pdf/2511.01450</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01450]] Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation(https://arxiv.org/abs/2511.01450)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent studies have identified Direct Preference Optimization (DPO) as an efficient and reward-free approach to improving video generation quality. However, existing methods largely follow image-domain paradigms and are mainly developed on small-scale models (approximately 2B parameters), limiting their ability to address the unique challenges of video tasks, such as costly data construction, unstable training, and heavy memory consumption. To overcome these limitations, we introduce a GT-Pair that automatically builds high-quality preference pairs by using real videos as positives and model-generated videos as negatives, eliminating the need for any external annotation. We further present Reg-DPO, which incorporates the SFT loss as a regularization term into the DPO objective to enhance training stability and generation fidelity. Additionally, by combining the FSDP framework with multiple memory optimization techniques, our approach achieves nearly three times higher training capacity than using FSDP alone. Extensive experiments on both I2V and T2V tasks across multiple datasets demonstrate that our method consistently outperforms existing approaches, delivering superior video generation quality.</li>
<li><strong>摘要：</strong>最近的研究发现直接偏好优化（DPO）是一种有效且无奖励的提高视频生成质量的方法。然而，现有的方法很大程度上遵循图像域范式，并且主要是在小规模模型（大约2B参数）上开发的，限制了它们解决视频任务独特挑战的能力，例如昂贵的数据构建、不稳定的训练和大量的内存消耗。为了克服这些限制，我们引入了 GT-Pair，它通过使用真实视频作为正例，使用模型生成的视频作为负例，自动构建高质量的偏好对，从而无需任何外部注释。我们进一步提出了 Reg-DPO，它将 SFT 损失作为正则化项纳入 DPO 目标中，以增强训练稳定性和生成保真度。此外，通过将 FSDP 框架与多种内存优化技术相结合，我们的方法实现的训练容量比单独使用 FSDP 高出近三倍。跨多个数据集的 I2V 和 T2V 任务的广泛实验表明，我们的方法始终优于现有方法，提供卓越的视频生成质量。</li>
</ul>

<h3>Title: HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA</h3>
<ul>
<li><strong>Authors: </strong>Lei Hu, Yongjing Ye, Shihong Xia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01463">https://arxiv.org/abs/2511.01463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01463">https://arxiv.org/pdf/2511.01463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01463]] HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA(https://arxiv.org/abs/2511.01463)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The expansion of instruction-tuning data has enabled foundation language models to exhibit improved instruction adherence and superior performance across diverse downstream tasks. Semantically-rich 3D human motion is being progressively integrated with these foundation models to enhance multimodal understanding and cross-modal generation capabilities. However, the modality gap between human motion and text raises unresolved concerns about catastrophic forgetting during this integration. In addition, developing autoregressive-compatible pose representations that preserve generalizability across heterogeneous downstream tasks remains a critical technical barrier. To address these issues, we propose the Human Motion-Vision-Language Model (HMVLM), a unified framework based on the Mixture of Expert Low-Rank Adaption(MoE LoRA) strategy. The framework leverages the gating network to dynamically allocate LoRA expert weights based on the input prompt, enabling synchronized fine-tuning of multiple tasks. To mitigate catastrophic forgetting during instruction-tuning, we introduce a novel zero expert that preserves the pre-trained parameters for general linguistic tasks. For pose representation, we implement body-part-specific tokenization by partitioning the human body into different joint groups, enhancing the spatial resolution of the representation. Experiments show that our method effectively alleviates knowledge forgetting during instruction-tuning and achieves remarkable performance across diverse human motion downstream tasks.</li>
<li><strong>摘要：</strong>指令调优数据的扩展使得基础语言模型能够在不同的下游任务中展现出更好的指令依从性和卓越的性能。语义丰富的 3D 人体运动正在逐步与这些基础模型集成，以增强多模态理解和跨模态生成能力。然而，人类运动和文本之间的模态差距引起了人们对这种整合过程中灾难性遗忘的未解决的担忧。此外，开发自回归兼容的姿势表示以保持异构下游任务的通用性仍然是一个关键的技术障碍。为了解决这些问题，我们提出了人类运动视觉语言模型（HMVLM），这是一个基于专家低阶适应混合（MoE LoRA）策略的统一框架。该框架利用门控网络根据输入提示动态分配 LoRA 专家权重，从而实现多个任务的同步微调。为了减轻指令调整过程中的灾难性遗忘，我们引入了一种新颖的零专家，它保留了一般语言任务的预训练参数。对于姿势表示，我们通过将人体划分为不同的关节组来实现特定于身体部位的标记化，从而增强表示的空间分辨率。实验表明，我们的方法有效地减轻了指令调整过程中的知识遗忘，并在不同的人体运动下游任务中取得了显着的性能。</li>
</ul>

<h3>Title: SecDiff: Diffusion-Aided Secure Deep Joint Source-Channel Coding Against Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Changyuan Zhao, Jiacheng Wang, Ruichen Zhang, Dusit Niyato, Hongyang Du, Zehui Xiong, Dong In Kim, Ping Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01466">https://arxiv.org/abs/2511.01466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01466">https://arxiv.org/pdf/2511.01466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01466]] SecDiff: Diffusion-Aided Secure Deep Joint Source-Channel Coding Against Adversarial Attacks(https://arxiv.org/abs/2511.01466)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep joint source-channel coding (JSCC) has emerged as a promising paradigm for semantic communication, delivering significant performance gains over conventional separate coding schemes. However, existing JSCC frameworks remain vulnerable to physical-layer adversarial threats, such as pilot spoofing and subcarrier jamming, compromising semantic fidelity. In this paper, we propose SecDiff, a plug-and-play, diffusion-aided decoding framework that significantly enhances the security and robustness of deep JSCC under adversarial wireless environments. Different from prior diffusion-guided JSCC methods that suffer from high inference latency, SecDiff employs pseudoinverse-guided sampling and adaptive guidance weighting, enabling flexible step-size control and efficient semantic reconstruction. To counter jamming attacks, we introduce a power-based subcarrier masking strategy and recast recovery as a masked inpainting problem, solved via diffusion guidance. For pilot spoofing, we formulate channel estimation as a blind inverse problem and develop an expectation-minimization (EM)-driven reconstruction algorithm, guided jointly by reconstruction loss and a channel operator. Notably, our method alternates between pilot recovery and channel estimation, enabling joint refinement of both variables throughout the diffusion process. Extensive experiments over orthogonal frequency-division multiplexing (OFDM) channels under adversarial conditions show that SecDiff outperforms existing secure and generative JSCC baselines by achieving a favorable trade-off between reconstruction quality and computational cost. This balance makes SecDiff a promising step toward practical, low-latency, and attack-resilient semantic communications.</li>
<li><strong>摘要：</strong>深度联合源通道编码（JSCC）已成为语义通信的一种有前景的范例，与传统的单独编码方案相比，可显着提高性能。然而，现有的 JSCC 框架仍然容易受到物理层对抗性威胁的影响，例如飞行员欺骗和子载波干扰，从而损害语义保真度。在本文中，我们提出了 SecDiff，一种即插即用的扩散辅助解码框架，可显着增强对抗性无线环境下深度 JSCC 的安全性和鲁棒性。与之前的扩散引导 JSCC 方法存在高推理延迟的问题不同，SecDiff 采用伪逆引导采样和自适应引导加权，能够实现灵活的步长控制和高效的语义重建。为了对抗干扰攻击，我们引入了基于功率的子载波掩蔽策略，并将恢复重铸为掩蔽修复问题，通过扩散引导解决。对于导频欺骗，我们将信道估计表述为盲逆问题，并开发了一种期望最小化（EM）驱动的重建算法，由重建损失和信道算子联合指导。值得注意的是，我们的方法在导频恢复和信道估计之间交替，从而能够在整个扩散过程中联合细化这两个变量。在对抗性条件下对正交频分复用 (OFDM) 通道进行的大量实验表明，SecDiff 通过在重建质量和计算成本之间实现有利的权衡，优于现有的安全和生成 JSCC 基线。这种平衡使 SecDiff 朝着实用、低延迟和抗攻击的语义通信迈出了有希望的一步。</li>
</ul>

<h3>Title: DAMBench: A Multi-Modal Benchmark for Deep Learning-based Atmospheric Data Assimilation</h3>
<ul>
<li><strong>Authors: </strong>Hao Wang, Zixuan Weng, Jindong Han, Wei Fan, Hao Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01468">https://arxiv.org/abs/2511.01468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01468">https://arxiv.org/pdf/2511.01468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01468]] DAMBench: A Multi-Modal Benchmark for Deep Learning-based Atmospheric Data Assimilation(https://arxiv.org/abs/2511.01468)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Data Assimilation is a cornerstone of atmospheric system modeling, tasked with reconstructing system states by integrating sparse, noisy observations with prior estimation. While traditional approaches like variational and ensemble Kalman filtering have proven effective, recent advances in deep learning offer more scalable, efficient, and flexible alternatives better suited for complex, real-world data assimilation involving large-scale and multi-modal observations. However, existing deep learning-based DA research suffers from two critical limitations: (1) reliance on oversimplified scenarios with synthetically perturbed observations, and (2) the absence of standardized benchmarks for fair model comparison. To address these gaps, in this work, we introduce DAMBench, the first large-scale multi-modal benchmark designed to evaluate data-driven DA models under realistic atmospheric conditions. DAMBench integrates high-quality background states from state-of-the-art forecasting systems and real-world multi-modal observations (i.e., real-world weather stations and satellite imagery). All data are resampled to a common grid and temporally aligned to support systematic training, validation, and testing. We provide unified evaluation protocols and benchmark representative data assimilation approaches, including latent generative models and neural process frameworks. Additionally, we propose a lightweight multi-modal plugin to demonstrate how integrating realistic observations can enhance even simple baselines. Through comprehensive experiments, DAMBench establishes a rigorous foundation for future research, promoting reproducibility, fair comparison, and extensibility to real-world multi-modal scenarios. Our dataset and code are publicly available at this https URL.</li>
<li><strong>摘要：</strong>数据同化是大气系统建模的基石，其任务是通过将稀疏、噪声观测与先前估计相结合来重建系统状态。虽然变分和集成卡尔曼滤波等传统方法已被证明是有效的，但深度学习的最新进展提供了更具可扩展性、高效和灵活的替代方案，更适合涉及大规模和多模态观测的复杂的现实世界数据同化。然而，现有的基于深度学习的 DA 研究存在两个关键局限性：(1) 依赖于具有综合扰动观察的过于简化的场景，以及 (2) 缺乏用于公平模型比较的标准化基准。为了解决这些差距，在这项工作中，我们引入了 DMBench，这是第一个大规模多模态基准测试，旨在评估真实大气条件下数据驱动的 DA 模型。 DMBench 集成了来自最先进的预报系统和现实世界多模式观测（即现实世界气象站和卫星图像）的高质量背景状态。所有数据都被重新采样到一个公共网格并在时间上对齐以支持系统培训、验证和测试。我们提供统一的评估协议和基准代表性数据同化方法，包括潜在生成模型和神经过程框架。此外，我们提出了一个轻量级的多模式插件来演示如何集成现实观察可以增强甚至简单的基线。通过全面的实验，DAMBench 为未来的研究奠定了严格的基础，促进了现实世界多模态场景的可重复性、公平比较和可扩展性。我们的数据集和代码可通过此 https URL 公开获取。</li>
</ul>

<h3>Title: Luminance-Aware Statistical Quantization: Unsupervised Hierarchical Learning for Illumination Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Derong Kong, Zhixiong Yang, Shengxi Li, Shuaifeng Zhi, Li Liu, Zhen Liu, Jingyuan Xia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01510">https://arxiv.org/abs/2511.01510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01510">https://arxiv.org/pdf/2511.01510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01510]] Luminance-Aware Statistical Quantization: Unsupervised Hierarchical Learning for Illumination Enhancement(https://arxiv.org/abs/2511.01510)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Low-light image enhancement (LLIE) faces persistent challenges in balancing reconstruction fidelity with cross-scenario generalization. While existing methods predominantly focus on deterministic pixel-level mappings between paired low/normal-light images, they often neglect the continuous physical process of luminance transitions in real-world environments, leading to performance drop when normal-light references are unavailable. Inspired by empirical analysis of natural luminance dynamics revealing power-law distributed intensity transitions, this paper introduces Luminance-Aware Statistical Quantification (LASQ), a novel framework that reformulates LLIE as a statistical sampling process over hierarchical luminance distributions. Our LASQ re-conceptualizes luminance transition as a power-law distribution in intensity coordinate space that can be approximated by stratified power functions, therefore, replacing deterministic mappings with probabilistic sampling over continuous luminance layers. A diffusion forward process is designed to autonomously discover optimal transition paths between luminance layers, achieving unsupervised distribution emulation without normal-light references. In this way, it considerably improves the performance in practical situations, enabling more adaptable and versatile light restoration. This framework is also readily applicable to cases with normal-light references, where it achieves superior performance on domain-specific datasets alongside better generalization-ability across non-reference datasets.</li>
<li><strong>摘要：</strong>低光图像增强（LLIE）在平衡重建保真度与跨场景泛化方面面临着持续的挑战。虽然现有方法主要关注成对的低/正常光图像之间的确定性像素级映射，但它们经常忽略现实环境中亮度过渡的连续物理过程，导致当正常光参考不可用时性能下降。受揭示幂律分布强度转变的自然亮度动态实证分析的启发，本文介绍了亮度感知统计量化 (LASQ)，这是一种新颖的框架，它将 LLIE 重新表述为分层亮度分布上的统计采样过程。我们的 LASQ 将亮度转变重新概念化为强度坐标空间中的幂律分布，可以通过分层幂函数来近似，因此，用连续亮度层上的概率采样替换确定性映射。扩散前向过程旨在自动发现亮度层之间的最佳过渡路径，从而在没有法向光参考的情况下实现无监督分布仿真。通过这种方式，它大大提高了实际情况下的性能，实现了更具适应性和通用性的光恢复。该框架也很容易适用于具有普通光参考的情况，它在特定领域的数据集上实现了卓越的性能，同时在非参考数据集上具有更好的泛化能力。</li>
</ul>

<h3>Title: Example-Based Feature Painting on Textures</h3>
<ul>
<li><strong>Authors: </strong>Andrei-Timotei Ardelean, Tim Weyrich</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01513">https://arxiv.org/abs/2511.01513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01513">https://arxiv.org/pdf/2511.01513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01513]] Example-Based Feature Painting on Textures(https://arxiv.org/abs/2511.01513)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In this work, we propose a system that covers the complete workflow for achieving controlled authoring and editing of textures that present distinctive local characteristics. These include various effects that change the surface appearance of materials, such as stains, tears, holes, abrasions, discoloration, and more. Such alterations are ubiquitous in nature, and including them in the synthesis process is crucial for generating realistic textures. We introduce a novel approach for creating textures with such blemishes, adopting a learning-based approach that leverages unlabeled examples. Our approach does not require manual annotations by the user; instead, it detects the appearance-altering features through unsupervised anomaly detection. The various textural features are then automatically clustered into semantically coherent groups, which are used to guide the conditional generation of images. Our pipeline as a whole goes from a small image collection to a versatile generative model that enables the user to interactively create and paint features on textures of arbitrary size. Notably, the algorithms we introduce for diffusion-based editing and infinite stationary texture generation are generic and should prove useful in other contexts as well. Project page: this https URL</li>
<li><strong>摘要：</strong>在这项工作中，我们提出了一个涵盖完整工作流程的系统，用于实现呈现独特本地特征的纹理的受控创作和编辑。其中包括改变材料表面外观的各种效果，例如污渍、撕裂、孔洞、擦伤、变色等。这种改变在自然界中普遍存在，将它们包含在合成过程中对于生成逼真的纹理至关重要。我们引入了一种新颖的方法来创建具有此类瑕疵的纹理，采用基于学习的方法，利用未标记的示例。我们的方法不需要用户手动注释；相反，它通过无监督的异常检测来检测外观改变的特征。然后，各种纹理特征自动聚集成语义连贯的组，用于指导图像的条件生成。我们的管道作为一个整体，从一个小型图像集合到一个多功能的生成模型，使用户能够在任意大小的纹理上交互式地创建和绘制特征。值得注意的是，我们为基于扩散的编辑和无限固定纹理生成引入的算法是通用的，并且在其他情况下也应该证明是有用的。项目页面：此 https URL</li>
</ul>

<h3>Title: NSYNC: Negative Synthetic Image Generation for Contrastive Training to Improve Stylized Text-To-Image Translation</h3>
<ul>
<li><strong>Authors: </strong>Serkan Ozturk, Samet Hicsonmez, Pinar Duygulu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01517">https://arxiv.org/abs/2511.01517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01517">https://arxiv.org/pdf/2511.01517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01517]] NSYNC: Negative Synthetic Image Generation for Contrastive Training to Improve Stylized Text-To-Image Translation(https://arxiv.org/abs/2511.01517)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Current text conditioned image generation methods output realistic looking images, but they fail to capture specific styles. Simply finetuning them on the target style datasets still struggles to grasp the style features. In this work, we present a novel contrastive learning framework to improve the stylization capability of large text-to-image diffusion models. Motivated by the astonishing advance in image generation models that makes synthetic data an intrinsic part of model training in various computer vision tasks, we exploit synthetic image generation in our approach. Usually, the generated synthetic data is dependent on the task, and most of the time it is used to enlarge the available real training dataset. With NSYNC, alternatively, we focus on generating negative synthetic sets to be used in a novel contrastive training scheme along with real positive images. In our proposed training setup, we forward negative data along with positive data and obtain negative and positive gradients, respectively. We then refine the positive gradient by subtracting its projection onto the negative gradient to get the orthogonal component, based on which the parameters are updated. This orthogonal component eliminates the trivial attributes that are present in both positive and negative data and directs the model towards capturing a more unique style. Experiments on various styles of painters and illustrators show that our approach improves the performance over the baseline methods both quantitatively and qualitatively. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>当前的文本条件图像生成方法输出逼真的图像，但它们无法捕获特定的样式。简单地在目标风格数据集上对它们进行微调仍然很难掌握风格特征。在这项工作中，我们提出了一种新颖的对比学习框架，以提高大型文本到图像扩散模型的风格化能力。图像生成模型取得了惊人的进步，使得合成数据成为各种计算机视觉任务中模型训练的固有部分，我们在我们的方法中利用了合成图像生成。通常，生成的合成数据取决于任务，并且大多数时候它用于扩大可用的真实训练数据集。另一方面，通过 NSYNC，我们专注于生成负合成集，以便与真实的正图像一起用于新颖的对比训练方案。在我们提出的训练设置中，我们将负数据与正数据一起转发，并分别获得负梯度和正梯度。然后，我们通过减去其在负梯度上的投影来细化正梯度，以获得正交分量，并在此基础上更新参数。这种正交分量消除了正数据和负数据中存在的琐碎属性，并引导模型捕捉更独特的风格。对各种风格的画家和插画家的实验表明，我们的方法在数量和质量上都比基线方法提高了性能。我们的代码可以在这个 https URL 上找到。</li>
</ul>

<h3>Title: Driving scenario generation and evaluation using a structured layer representation and foundational models</h3>
<ul>
<li><strong>Authors: </strong>Arthur Hubert, Gamal Elghazaly, Raphaël Frank</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01541">https://arxiv.org/abs/2511.01541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01541">https://arxiv.org/pdf/2511.01541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01541]] Driving scenario generation and evaluation using a structured layer representation and foundational models(https://arxiv.org/abs/2511.01541)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Rare and challenging driving scenarios are critical for autonomous vehicle development. Since they are difficult to encounter, simulating or generating them using generative models is a popular approach. Following previous efforts to structure driving scenario representations in a layer model, we propose a structured five-layer model to improve the evaluation and generation of rare scenarios. We use this model alongside large foundational models to generate new driving scenarios using a data augmentation strategy. Unlike previous representations, our structure introduces subclasses and characteristics for every agent of the scenario, allowing us to compare them using an embedding specific to our layer-model. We study and adapt two metrics to evaluate the relevance of a synthetic dataset in the context of a structured representation: the diversity score estimates how different the scenarios of a dataset are from one another, while the originality score calculates how similar a synthetic dataset is from a real reference set. This paper showcases both metrics in different generation setup, as well as a qualitative evaluation of synthetic videos generated from structured scenario descriptions. The code and extended results can be found at this https URL.</li>
<li><strong>摘要：</strong>罕见且具有挑战性的驾驶场景对于自动驾驶汽车的开发至关重要。由于它们很难遇到，因此使用生成模型模拟或生成它们是一种流行的方法。继之前在层模型中构造驾驶场景表示的努力之后，我们提出了一种结构化的五层模型来改进罕见场景的评估和生成。我们将该模型与大型基础模型一起使用，通过数据增强策略生成新的驾驶场景。与以前的表示不同，我们的结构引入了场景中每个代理的子类和特征，使我们能够使用特定于我们的层模型的嵌入来比较它们。我们研究并调整两个指标来评估合成数据集在结构化表示的背景下的相关性：多样性得分估计数据集场景之间的差异程度，而原创性得分则计算合成数据集与真实参考集的相似程度。本文展示了不同生成设置中的两个指标，以及对从结构化场景描述生成的合成视频的定性评估。代码和扩展结果可以在此 https URL 中找到。</li>
</ul>

<h3>Title: Generative Adversarial Synthesis and Deep Feature Discrimination of Brain Tumor MRI Images</h3>
<ul>
<li><strong>Authors: </strong>Md Sumon Ali, Muzammil Behzad</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01574">https://arxiv.org/abs/2511.01574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01574">https://arxiv.org/pdf/2511.01574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01574]] Generative Adversarial Synthesis and Deep Feature Discrimination of Brain Tumor MRI Images(https://arxiv.org/abs/2511.01574)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Compared to traditional methods, Deep Learning (DL) becomes a key technology for computer vision tasks. Synthetic data generation is an interesting use case for DL, especially in the field of medical imaging such as Magnetic Resonance Imaging (MRI). The need for this task since the original MRI data is limited. The generation of realistic medical images is completely difficult and challenging. Generative Adversarial Networks (GANs) are useful for creating synthetic medical images. In this paper, we propose a DL based methodology for creating synthetic MRI data using the Deep Convolutional Generative Adversarial Network (DC-GAN) to address the problem of limited data. We also employ a Convolutional Neural Network (CNN) classifier to classify the brain tumor using synthetic data and real MRI data. CNN is used to evaluate the quality and utility of the synthetic images. The classification result demonstrates comparable performance on real and synthetic images, which validates the effectiveness of GAN-generated images for downstream tasks.</li>
<li><strong>摘要：</strong>与传统方法相比，深度学习（DL）成为计算机视觉任务的关键技术。合成数据生成是深度学习的一个有趣用例，尤其是在磁共振成像 (MRI) 等医学成像领域。由于原始 MRI 数据有限，因此需要执行此任务。生成逼真的医学图像是完全困难且具有挑战性的。生成对抗网络 (GAN) 对于创建合成医学图像非常有用。在本文中，我们提出了一种基于深度学习的方法，使用深度卷积生成对抗网络（DC-GAN）创建合成 MRI 数据，以解决数据有限的问题。我们还采用卷积神经网络 (CNN) 分类器，使用合成数据和真实 MRI 数据对脑肿瘤进行分类。 CNN 用于评估合成图像的质量和实用性。分类结果在真实图像和合成图像上表现出相当的性能，这验证了 GAN 生成的图像对于下游任务的有效性。</li>
</ul>

<h3>Title: Wave-Particle (Continuous-Discrete) Dualistic Visual Tokenization for Unified Understanding and Generation</h3>
<ul>
<li><strong>Authors: </strong>Yizhu Chen, Chen Ju, Zhicheng Wang, Shuai Xiao, Xu Chen, Jinsong Lan, Xiaoyong Zhu, Ying Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01593">https://arxiv.org/abs/2511.01593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01593">https://arxiv.org/pdf/2511.01593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01593]] Wave-Particle (Continuous-Discrete) Dualistic Visual Tokenization for Unified Understanding and Generation(https://arxiv.org/abs/2511.01593)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The unification of understanding and generation within a single multi-modal large model (MLLM) remains one significant challenge, largely due to the dichotomy between continuous and discrete visual tokenizations. Continuous tokenizer (CT) achieves strong performance by bridging multiple independently-trained understanding modules and generation modules, but suffers from complex multi-stage pipelines and substantial engineering overhead. Conversely, discrete tokenizers (DT) offer a conceptually elegant idea by quantizing each image into a primitive, but inevitably leading to information loss and performance degradation. To resolve this tension, we question the binary choice between CT and DT, inspired by the wave-particle duality of light, and propose the Continuous-Discrete Dualistic Visual Tokenizer (CDD-VT). We treat visual data as a flexible composition of image primitives derived from quantized codebooks, with the crucial insight that the primitive number assigned to each visual sample is adaptively determined according to its complexity: simple instances use a few primitives, emulating discrete tokenization, while complex instances use many, approximating continuous tokenization. Two core components are designed: Diverse Quantitative Primitives, which encourage primitives orthogonality to better populate information space, and Dynamic Primitive Allocator, which assesses sample complexity to determine the optimal set of primitives. Extensive experiments on reconstruction, retrieval and classification show that CDD-VT achieves superior performance over to specialized CT and DT, effectively getting strong result within a concise and scalable MLLM.</li>
<li><strong>摘要：</strong>在单个多模态大模型（MLLM）中统一理解和生成仍然是一项重大挑战，这主要是由于连续和离散视觉标记化之间的二分法。连续分词器（CT）通过桥接多个独立训练的理解模块和生成模块来实现强大的性能，但面临复杂的多级管道和大量的工程开销。相反，离散分词器（DT）通过将每个图像量化为基元提供了概念上优雅的想法，但不可避免地导致信息丢失和性能下降。为了解决这种紧张关系，受光的波粒二象性启发，我们对 CT 和 DT 之间的二元选择提出质疑，并提出了连续离散二元视觉分词器 (CDD-VT)。我们将视觉数据视为源自量化码本的图像基元的灵活组合，并具有重要的洞察力，即分配给每个视觉样本的基元数量是根据其复杂性自适应确定的：简单实例使用一些基元，模拟离散标记化，而复杂实例使用许多基元，近似连续标记化。设计了两个核心组件：多样化的定量基元，它鼓励基元正交性以更好地填充信息空间，以及动态基元分配器，它评估样本复杂性以确定最佳基元集。大量的重建、检索和分类实验表明，CDD-VT 的性能优于专门的 CT 和 DT，在简洁且可扩展的 MLLM 中有效地获得了强大的结果。</li>
</ul>

<h3>Title: Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with Efficient LLM Serving</h3>
<ul>
<li><strong>Authors: </strong>Chengying Huan, Ziheng Meng, Yongchao Liu, Zhengyi Yang, Yun Zhu, Yue Yun, Shipeng Li, Rong Gu, Xiabao Wu, Haitao Zhang, Chuntao Hong, Shaonan Ma, Guihai Chen, Chen Tian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01633">https://arxiv.org/abs/2511.01633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01633">https://arxiv.org/pdf/2511.01633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01633]] Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with Efficient LLM Serving(https://arxiv.org/abs/2511.01633)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Graph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to perform step-by-step reasoning over graph-structured knowledge, but existing pipelines suffer from low accuracy, excessive token usage, high latency, and low throughput due to single-agent monolithic prompts, repeated context re-encoding, and inefficient serving execution. We present GLM, the first multi-agent Graph-CoT system co-designed with an optimized LLM serving architecture. GLM decomposes reasoning into specialized agents for classification, reasoning, action generation, and graph retrieval, enabling branching and selective context sharing to reduce prompt length and reasoning iterations while preserving reasoning quality, thereby improving accuracy and reducing overall token consumption. To scale inference, we introduce a Graph-CoT-aware LLM inference mechanism with graph-specific KV-cache management, priority-based eviction, and pipelined execution to improve serving efficiency. Experiments demonstrate that GLM improves answer accuracy by up to 38%, reduces token cost by up to 95.7%, lowers inference latency by 90.3%, and achieves up to 15.1x higher throughput compared to state-of-the-art Graph-CoT baselines, enabling efficient adoption for complex real-world reasoning at scale.</li>
<li><strong>摘要：</strong>图思维链（Graph-CoT）使大型语言模型（LLM）能够对图结构知识进行逐步推理，但由于单代理整体提示、重复的上下文重新编码和低效的服务执行，现有的管道存在准确性低、令牌使用过多、高延迟和低吞吐量的问题。我们推出了 GLM，这是第一个与优化的 LLM 服务架构共同设计的多代理 Graph-CoT 系统。 GLM 将推理分解为用于分类、推理、动作生成和图检索的专用代理，支持分支和选择性上下文共享，以减少提示长度和推理迭代，同时保持推理质量，从而提高准确性并减少总体令牌消耗。为了扩展推理，我们引入了一种 Graph-CoT 感知的 LLM 推理机制，具有特定于图的 KV 缓存管理、基于优先级的驱逐和流水线执行，以提高服务效率。实验表明，与最先进的 Graph-CoT 基线相比，GLM 将答案准确性提高了 38%，将令牌成本降低了 95.7%，将推理延迟降低了 90.3%，吞吐量提高了 15.1 倍，从而能够大规模高效地采用复杂的现实世界推理。</li>
</ul>

<h3>Title: Enhancing Diffusion-based Restoration Models via Difficulty-Adaptive Reinforcement Learning with IQA Reward</h3>
<ul>
<li><strong>Authors: </strong>Xiaogang Xu, Ruihang Chu, Jian Wang, Kun Zhou, Wenjie Shu, Harry Yang, Ser-Nam Lim, Hao Chen, Liang Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01645">https://arxiv.org/abs/2511.01645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01645">https://arxiv.org/pdf/2511.01645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01645]] Enhancing Diffusion-based Restoration Models via Difficulty-Adaptive Reinforcement Learning with IQA Reward(https://arxiv.org/abs/2511.01645)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generation, quality assessment</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) has recently been incorporated into diffusion models, e.g., tasks such as text-to-image. However, directly applying existing RL methods to diffusion-based image restoration models is suboptimal, as the objective of restoration fundamentally differs from that of pure generation: it places greater emphasis on fidelity. In this paper, we investigate how to effectively integrate RL into diffusion-based restoration models. First, through extensive experiments with various reward functions, we find that an effective reward can be derived from an Image Quality Assessment (IQA) model, instead of intuitive ground-truth-based supervision, which has already been optimized during the Supervised Fine-Tuning (SFT) stage prior to RL. Moreover, our strategy focuses on using RL for challenging samples that are significantly distant from the ground truth, and our RL approach is innovatively implemented using MLLM-based IQA models to align distributions with high-quality images initially. As the samples approach the ground truth's distribution, RL is adaptively combined with SFT for more fine-grained alignment. This dynamic process is facilitated through an automatic weighting strategy that adjusts based on the relative difficulty of the training samples. Our strategy is plug-and-play that can be seamlessly applied to diffusion-based restoration models, boosting its performance across various restoration tasks. Extensive experiments across multiple benchmarks demonstrate the effectiveness of our proposed RL framework.</li>
<li><strong>摘要：</strong>强化学习（RL）最近已被纳入扩散模型中，例如文本到图像等任务。然而，直接将现有的强化学习方法应用于基于扩散的图像恢复模型并不是最优的，因为恢复的目标与纯生成的目标根本不同：它更强调保真度。在本文中，我们研究了如何有效地将强化学习集成到基于扩散的恢复模型中。首先，通过对各种奖励函数的广泛实验，我们发现有效的奖励可以来自图像质量评估（IQA）模型，而不是基于直观的基于事实的监督，后者已经在强化学习之前的监督微调（SFT）阶段进行了优化。此外，我们的策略侧重于使用 RL 来处理与真实情况相差很大的挑战性样本，并且我们的 RL 方法是使用基于 MLLM 的 IQA 模型创新地实现的，以便最初将分布与高质量图像对齐。当样本接近真实分布时，RL 会自适应地与 SFT 相结合，以实现更细粒度的对齐。这个动态过程是通过根据训练样本的相对难度进行调整的自动加权策略来促进的。我们的策略是即插即用，可以无缝应用于基于扩散的恢复模型，从而提高其在各种恢复任务中的性能。跨多个基准的广泛实验证明了我们提出的 RL 框架的有效性。</li>
</ul>

<h3>Title: Collaborative Large Language Model Inference via Resource-Aware Parallel Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Jungyeon Koh, Hyun Jong Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01695">https://arxiv.org/abs/2511.01695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01695">https://arxiv.org/pdf/2511.01695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01695]] Collaborative Large Language Model Inference via Resource-Aware Parallel Speculative Decoding(https://arxiv.org/abs/2511.01695)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The growing demand for on-device large language model (LLM) inference highlights the need for efficient mobile edge computing (MEC) solutions, especially in resource-constrained settings. Speculative decoding offers a promising solution by partitioning token generation between a lightweight draft model on mobile devices and a powerful target model on edge servers, but suffers from communication overhead and asynchronous delays. This paper is the first to propose a unified framework that jointly optimizes user association and resource allocation (UARA) to support efficient parallel speculative decoding. We solve the UARA problem using a multi-agent deep reinforcement learning algorithm. To evaluate our approach under realistic conditions, we conduct experiments using the Sionna simulator. Results show that our method achieves up to 28.0% and an average of 23.7% reduction in end-to-end latency without compromising inference accuracy, enabling scalable and low-latency LLM services in MEC systems.</li>
<li><strong>摘要：</strong>对设备上大语言模型 (LLM) 推理的需求不断增长，凸显了对高效移动边缘计算 (MEC) 解决方案的需求，特别是在资源有限的环境中。推测性解码通过在移动设备上的轻量级草稿模型和边缘服务器上的强大目标模型之间划分令牌生成，提供了一种有前景的解决方案，但会受到通信开销和异步延迟的影响。本文首次提出了联合优化用户关联和资源分配（UARA）的统一框架，以支持高效的并行推测解码。我们使用多智能体深度强化学习算法解决 UARA 问题。为了在现实条件下评估我们的方法，我们使用 Sionna 模拟器进行实验。结果表明，我们的方法在不影响推理准确性的情况下，在端到端延迟方面实现了高达 28.0% 和平均 23.7% 的降低，从而在 MEC 系统中实现可扩展和低延迟的 LLM 服务。</li>
</ul>

<h3>Title: Progressive Translation of H&E to IHC with Enhanced Structural Fidelity</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Kang, Ziyu Su, Tianyang Wang, Zaibo Li, Wei Chen, Muhammad Khalid Khan Niazi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01698">https://arxiv.org/abs/2511.01698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01698">https://arxiv.org/pdf/2511.01698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01698]] Progressive Translation of H&E to IHC with Enhanced Structural Fidelity(https://arxiv.org/abs/2511.01698)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Compared to hematoxylin-eosin (H&E) staining, immunohistochemistry (IHC) not only maintains the structural features of tissue samples, but also provides high-resolution protein localization, which is essential for aiding in pathology diagnosis. Despite its diagnostic value, IHC remains a costly and labor-intensive technique. Its limited scalability and constraints in multiplexing further hinder widespread adoption, especially in resource-limited settings. Consequently, researchers are increasingly exploring computational stain translation techniques to synthesize IHC-equivalent images from H&E-stained slides, aiming to extract protein-level information more efficiently and cost-effectively. However, most existing stain translation techniques rely on a linearly weighted summation of multiple loss terms within a single objective function, strategy that often overlooks the interdepedence among these components-resulting in suboptimal image quality and an inability to simultaneously preserve structural authenticity and color fidelity. To address this limitation, we propose a novel network architecture that follows a progressive structure, incorporating color and cell border generation logic, which enables each visual aspect to be optimized in a stage-wise and decoupled manner. To validate the effectiveness of our proposed network architecture, we build upon the Adaptive Supervised PatchNCE (ASP) framework as our baseline. We introduce additional loss functions based on 3,3'-diaminobenzidine (DAB) chromogen concentration and image gradient, enhancing color fidelity and cell boundary clarity in the generated IHC images. By reconstructing the generation pipeline using our structure-color-cell boundary progressive mechanism, experiments on HER2 and ER datasets demonstrated that the model significantly improved visual quality and achieved finer structural details.</li>
<li><strong>摘要：</strong>与苏木精-伊红 (H&E) 染色相比，免疫组织化学 (IHC) 不仅保留了组织样本的结构特征，而且还提供了高分辨率的蛋白质定位，这对于辅助病理诊断至关重要。尽管 IHC 具有诊断价值，但它仍然是一种昂贵且劳动密集型的技术。其有限的可扩展性和多路复用方面的限制进一步阻碍了广泛采用，特别是在资源有限的环境中。因此，研究人员越来越多地探索计算染色翻译技术，从 H&E 染色玻片合成 IHC 等效图像，旨在更有效、更经济地提取蛋白质水平信息。然而，大多数现有的染色翻译技术依赖于单个目标函数内多个损失项的线性加权求和，该策略经常忽略这些组件之间的相互依赖关系，导致图像质量不佳，并且无法同时保留结构真实性和颜色保真度。为了解决这个限制，我们提出了一种新颖的网络架构，它遵循渐进式结构，结合颜色和单元格边界生成逻辑，使每个视觉方面能够以分阶段和解耦的方式进行优化。为了验证我们提出的网络架构的有效性，我们以自适应监督 PatchNCE (ASP) 框架为基准。我们引入了基于 3,3'-二氨基联苯胺 (DAB) 色原浓度和图像梯度的额外损失函数，增强了生成的 IHC 图像中的颜色保真度和细胞边界清晰度。通过使用我们的结构-颜色-细胞边界渐进机制重建生成管道，在 HER2 和 ER 数据集上的实验表明，该模型显着提高了视觉质量并实现了更精细的结构细节。</li>
</ul>

<h3>Title: Learnable Fractional Reaction-Diffusion Dynamics for Under-Display ToF Imaging and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Xin Qiao, Matteo Poggi, Xing Wei, Pengchao Deng, Yanhui Zhou, Stefano Mattoccia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01704">https://arxiv.org/abs/2511.01704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01704">https://arxiv.org/pdf/2511.01704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01704]] Learnable Fractional Reaction-Diffusion Dynamics for Under-Display ToF Imaging and Beyond(https://arxiv.org/abs/2511.01704)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Under-display ToF imaging aims to achieve accurate depth sensing through a ToF camera placed beneath a screen panel. However, transparent OLED (TOLED) layers introduce severe degradations-such as signal attenuation, multi-path interference (MPI), and temporal noise-that significantly compromise depth quality. To alleviate this drawback, we propose Learnable Fractional Reaction-Diffusion Dynamics (LFRD2), a hybrid framework that combines the expressive power of neural networks with the interpretability of physical modeling. Specifically, we implement a time-fractional reaction-diffusion module that enables iterative depth refinement with dynamically generated differential orders, capturing long-term dependencies. In addition, we introduce an efficient continuous convolution operator via coefficient prediction and repeated differentiation to further improve restoration quality. Experiments on four benchmark datasets demonstrate the effectiveness of our approach. The code is publicly available at this https URL.</li>
<li><strong>摘要：</strong>屏下 ToF 成像旨在通过放置在屏幕面板下方的 ToF 相机实现精确的深度感测。然而，透明 OLED (TOLED) 层会带来严重的性能下降，例如信号衰减、多径干扰 (MPI) 和时间噪声，从而严重影响深度质量。为了缓解这一缺点，我们提出了可学习分数反应扩散动力学（LFRD2），这是一种混合框架，它将神经网络的表达能力与物理建模的可解释性结合起来。具体来说，我们实现了一个时间分数反应扩散模块，该模块能够通过动态生成的微分阶数进行迭代深度细化，捕获长期依赖性。此外，我们通过系数预测和重复微分引入了高效的连续卷积算子，以进一步提高恢复质量。对四个基准数据集的实验证明了我们方法的有效性。该代码可通过此 https URL 公开获取。</li>
</ul>

<h3>Title: Game-theoretic distributed learning of generative models for heterogeneous data collections</h3>
<ul>
<li><strong>Authors: </strong>Dmitrij Schlesinger, Boris Flach</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01740">https://arxiv.org/abs/2511.01740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01740">https://arxiv.org/pdf/2511.01740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01740]] Game-theoretic distributed learning of generative models for heterogeneous data collections(https://arxiv.org/abs/2511.01740)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>One of the main challenges in distributed learning arises from the difficulty of handling heterogeneous local models and data. In light of the recent success of generative models, we propose to meet this challenge by building on the idea of exchanging synthetic data instead of sharing model parameters. Local models can then be treated as ``black boxes'' with the ability to learn their parameters from data and to generate data according to these parameters. Moreover, if the local models admit semi-supervised learning, we can extend the approach by enabling local models on different probability spaces. This allows to handle heterogeneous data with different modalities. We formulate the learning of the local models as a cooperative game starting from the principles of game theory. We prove the existence of a unique Nash equilibrium for exponential family local models and show that the proposed learning approach converges to this equilibrium. We demonstrate the advantages of our approach on standard benchmark vision datasets for image classification and conditional generation.</li>
<li><strong>摘要：</strong>分布式学习的主要挑战之一来自于处理异构本地模型和数据的困难。鉴于生成模型最近取得的成功，我们建议通过交换合成数据而不是共享模型参数的想法来应对这一挑战。然后，局部模型可以被视为“黑匣子”，能够从数据中学习其参数并根据这些参数生成数据。此外，如果局部模型允许半监督学习，我们可以通过在不同的概率空间上启用局部模型来扩展该方法。这允许以不同的方式处理异构数据。我们从博弈论原理出发，将局部模型的学习制定为合作博弈。我们证明了指数族局部模型存在唯一的纳什均衡，并表明所提出的学习方法收敛于该均衡。我们在用于图像分类和条件生成的标准基准视觉数据集上展示了我们的方法的优势。</li>
</ul>

<h3>Title: Towards Efficient Federated Learning of Networked Mixture-of-Experts for Mobile Edge Computing</h3>
<ul>
<li><strong>Authors: </strong>Song Gao, Shusen Jing, Shuai Zhang, Yue Wang, Xiangwei Zhou, Songyang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01743">https://arxiv.org/abs/2511.01743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01743">https://arxiv.org/pdf/2511.01743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01743]] Towards Efficient Federated Learning of Networked Mixture-of-Experts for Mobile Edge Computing(https://arxiv.org/abs/2511.01743)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advancements in large artificial intelligence models (LAMs) are driving significant innovations in mobile edge computing within next-generation wireless networks. However, the substantial demands for computational resources and large-scale training data required to train LAMs conflict with the limited storage and computational capacity of edge devices, posing significant challenges to training and deploying LAMs at the edge. In this work, we introduce the Networked Mixture-of-Experts (NMoE) system, in which clients infer collaboratively by distributing tasks to suitable neighbors based on their expertise and aggregate the returned results. For training the NMoE, we propose a federated learning framework that integrates both supervised and self-supervised learning to balance personalization and generalization, while preserving communication efficiency and data privacy. We conduct extensive experiments to demonstrate the efficacy of the proposed NMoE system, providing insights and benchmarks for the NMoE training algorithms.</li>
<li><strong>摘要：</strong>大型人工智能模型 (LAM) 的最新进展正在推动下一代无线网络中移动边缘计算的重大创新。然而，训练 LAM 所需的大量计算资源和大规模训练数据与边缘设备有限的存储和计算能力相冲突，给边缘训练和部署 LAM 带来了重大挑战。在这项工作中，我们引入了网络专家混合（NMoE）系统，在该系统中，客户端根据他们的专业知识将任务分配给合适的邻居来进行协作推理，并汇总返回的结果。为了训练 NMoE，我们提出了一个联邦学习框架，该框架集成了监督学习和自监督学习，以平衡个性化和泛化，同时保持通信效率和数据隐私。我们进行了大量的实验来证明所提出的 NMoE 系统的有效性，为 NMoE 训练算法提供见解和基准。</li>
</ul>

<h3>Title: RLAC: Reinforcement Learning with Adversarial Critic for Free-Form Generation Tasks</h3>
<ul>
<li><strong>Authors: </strong>Mian Wu, Gavin Zhang, Sewon Min, Sergey Levine, Aviral Kumar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01758">https://arxiv.org/abs/2511.01758</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01758">https://arxiv.org/pdf/2511.01758</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01758]] RLAC: Reinforcement Learning with Adversarial Critic for Free-Form Generation Tasks(https://arxiv.org/abs/2511.01758)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Open-ended generation tasks require outputs to satisfy diverse and often implicit task-specific evaluation rubrics. The sheer number of relevant rubrics leads to prohibitively high verification costs and incomplete assessments of a response, making reinforcement learning (RL) post-training with rubric-based rewards difficult to scale. This problem is exacerbated by the fact that often the best way to combine these rubrics into one single reward is also highly prompt-specific. We propose Reinforcement Learning with Adversarial Critic (RLAC), a post-training approach that addresses these challenges via dynamic rubric verification. Our approach employs a large language model (LLM) as a critic that dynamically identifies only the most likely failure modes (e.g., a factual error or unhandled edge case), which are then verified by an external validator to optimize both generator and critic jointly. By training both the generator and the critic, this game enhances the critic's error detection and the generator's output quality while reducing required verifications. Our experiments demonstrate that RLAC improves factual accuracy in text generation and correctness in code generation, while also outperforming exhaustive verification and reward model methods. We show that dynamic critics are more effective than fixed critics, showcasing the potential of RLAC for scaling RL post-training to free-form generation tasks.</li>
<li><strong>摘要：</strong>开放式生成任务要求输出满足多样化且通常隐含的特定于任务的评估标准。相关评分标准的数量庞大，导致验证成本过高且对响应的评估不完整，使得基于评分标准的奖励的强化学习 (RL) 训练后难以规模化。由于将这些规则组合成单一奖励的最佳方式往往也是高度特定于提示的，这一事实加剧了这个问题。我们提出了对抗性批评的强化学习（RLAC），这是一种通过动态标题验证来解决这些挑战的训练后方法。我们的方法采用大型语言模型（LLM）作为批评者，仅动态识别最可能的故障模式（例如，事实错误或未处理的边缘情况），然后由外部验证器进行验证，以联合优化生成器和批评者。通过训练生成器和批评者，该游戏增强了批评者的错误检测和生成器的输出质量，同时减少了所需的验证。我们的实验表明，RLAC 提高了文本生成的事实准确性和代码生成的正确性，同时也优于详尽的验证和奖励模型方法。我们证明动态批评家比固定批评家更有效，展示了 RLAC 将 RL 训练后扩展到自由形式生成任务的潜力。</li>
</ul>

<h3>Title: Wonder3D++: Cross-domain Diffusion for High-fidelity 3D Generation from a Single Image</h3>
<ul>
<li><strong>Authors: </strong>Yuxiao Yang, Xiao-Xiao Long, Zhiyang Dou, Cheng Lin, Yuan Liu, Qingsong Yan, Yuexin Ma, Haoqian Wang, Zhiqiang Wu, Wei Yin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01767">https://arxiv.org/abs/2511.01767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01767">https://arxiv.org/pdf/2511.01767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01767]] Wonder3D++: Cross-domain Diffusion for High-fidelity 3D Generation from a Single Image(https://arxiv.org/abs/2511.01767)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In this work, we introduce \textbf{Wonder3D++}, a novel method for efficiently generating high-fidelity textured meshes from single-view images. Recent methods based on Score Distillation Sampling (SDS) have shown the potential to recover 3D geometry from 2D diffusion priors, but they typically suffer from time-consuming per-shape optimization and inconsistent geometry. In contrast, certain works directly produce 3D information via fast network inferences, but their results are often of low quality and lack geometric details. To holistically improve the quality, consistency, and efficiency of single-view reconstruction tasks, we propose a cross-domain diffusion model that generates multi-view normal maps and the corresponding color images. To ensure the consistency of generation, we employ a multi-view cross-domain attention mechanism that facilitates information exchange across views and modalities. Lastly, we introduce a cascaded 3D mesh extraction algorithm that drives high-quality surfaces from the multi-view 2D representations in only about $3$ minute in a coarse-to-fine manner. Our extensive evaluations demonstrate that our method achieves high-quality reconstruction results, robust generalization, and good efficiency compared to prior works. Code available at this https URL.</li>
<li><strong>摘要：</strong>在这项工作中，我们引入了 \textbf{Wonder3D++}，这是一种从单视图图像有效生成高保真纹理网格的新颖方法。最近基于分数蒸馏采样 (SDS) 的方法已显示出从 2D 扩散先验恢复 3D 几何形状的潜力，但它们通常会受到耗时的每个形状优化和不一致的几何形状的影响。相比之下，某些作品通过快速网络推理直接产生 3D 信息，但其结果往往质量较低且缺乏几何细节。为了全面提高单视图重建任务的质量、一致性和效率，我们提出了一种跨域扩散模型，可以生成多视图法线图和相应的彩色图像。为了确保生成的一致性，我们采用多视图跨域注意机制，促进跨视图和模式的信息交换。最后，我们引入了一种级联 3D 网格提取算法，该算法只需大约 3 美元分钟即可以从粗到细的方式从多视图 2D 表示中驱动高质量的表面。我们的广泛评估表明，与之前的工作相比，我们的方法实现了高质量的重建结果、鲁棒的泛化性和良好的效率。代码可在此 https URL 获取。</li>
</ul>

<h3>Title: How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment</h3>
<ul>
<li><strong>Authors: </strong>Zhen Chen, Qing Xu, Jinlin Wu, Biao Yang, Yuhao Zhai, Geng Guo, Jing Zhang, Yinlu Ding, Nassir Navab, Jiebo Luo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01775">https://arxiv.org/abs/2511.01775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01775">https://arxiv.org/pdf/2511.01775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01775]] How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment(https://arxiv.org/abs/2511.01775)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Foundation models in video generation are demonstrating remarkable capabilities as potential world models for simulating the physical world. However, their application in high-stakes domains like surgery, which demand deep, specialized causal knowledge rather than general physical rules, remains a critical unexplored gap. To systematically address this challenge, we present SurgVeo, the first expert-curated benchmark for video generation model evaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel, four-tiered framework tailored to assess model outputs from basic appearance to complex surgical strategy. On the basis of the SurgVeo benchmark, we task the advanced Veo-3 model with a zero-shot prediction task on surgical clips from laparoscopic and neurosurgical procedures. A panel of four board-certified surgeons evaluates the generated videos according to the SPP. Our results reveal a distinct "plausibility gap": while Veo-3 achieves exceptional Visual Perceptual Plausibility, it fails critically at higher levels of the SPP, including Instrument Operation Plausibility, Environment Feedback Plausibility, and Surgical Intent Plausibility. This work provides the first quantitative evidence of the chasm between visually convincing mimicry and causal understanding in surgical AI. Our findings from SurgVeo and the SPP establish a crucial foundation and roadmap for developing future models capable of navigating the complexities of specialized, real-world healthcare domains.</li>
<li><strong>摘要：</strong>视频生成中的基础模型正在展示出作为模拟物理世界的潜在世界模型的非凡能力。然而，它们在外科手术等高风险领域的应用仍然是一个尚未探索的关键领域，因为这些领域需要深入、专业的因果知识而不是一般的物理规则。为了系统地应对这一挑战，我们推出了 SurgVeo（第一个专家策划的手术视频生成模型评估基准）和手术合理性金字塔（SPP），这是一种新颖的四层框架，专门用于评估从基本外观到复杂手术策略的模型输出。在 SurgVeo 基准的基础上，我们对先进的 Veo-3 模型进行了腹腔镜和神经外科手术中手术夹子的零样本预测任务。根据 SPP 的说法，由四名经过委员会认证的外科医生组成的小组对生成的视频进行评估。我们的结果揭示了明显的“合理性差距”：虽然 Veo-3 实现了出色的视觉感知合理性，但它在更高水平的 SPP 上严重失败，包括仪器操作合理性、环境反馈合理性和手术意图合理性。这项工作首次提供了定量证据，证明了外科人工智能中视觉上令人信服的模仿与因果理解之间的鸿沟。我们从 SurgVeo 和 SPP 获得的研究结果为开发能够应对专业的、现实世界医疗保健领域的复杂性的未来模型奠定了重要的基础和路线图。</li>
</ul>

<h3>Title: Fractional Diffusion Bridge Models</h3>
<ul>
<li><strong>Authors: </strong>Gabriel Nobis, Maximilian Springenberg, Arina Belova, Rembert Daems, Christoph Knochenhauer, Manfred Opper, Tolga Birdal, Wojciech Samek</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.RO, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01795">https://arxiv.org/abs/2511.01795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01795">https://arxiv.org/pdf/2511.01795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01795]] Fractional Diffusion Bridge Models(https://arxiv.org/abs/2511.01795)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present Fractional Diffusion Bridge Models (FDBM), a novel generative diffusion bridge framework driven by an approximation of the rich and non-Markovian fractional Brownian motion (fBM). Real stochastic processes exhibit a degree of memory effects (correlations in time), long-range dependencies, roughness and anomalous diffusion phenomena that are not captured in standard diffusion or bridge modeling due to the use of Brownian motion (BM). As a remedy, leveraging a recent Markovian approximation of fBM (MA-fBM), we construct FDBM that enable tractable inference while preserving the non-Markovian nature of fBM. We prove the existence of a coupling-preserving generative diffusion bridge and leverage it for future state prediction from paired training data. We then extend our formulation to the Schrödinger bridge problem and derive a principled loss function to learn the unpaired data translation. We evaluate FDBM on both tasks: predicting future protein conformations from aligned data, and unpaired image translation. In both settings, FDBM achieves superior performance compared to the Brownian baselines, yielding lower root mean squared deviation (RMSD) of C$_\alpha$ atomic positions in protein structure prediction and lower Fréchet Inception Distance (FID) in unpaired image translation.</li>
<li><strong>摘要：</strong>我们提出了分数扩散桥模型（FDBM），这是一种新颖的生成扩散桥框架，由丰富的非马尔可夫分数布朗运动（fBM）的近似驱动。真实的随机过程表现出一定程度的记忆效应（时间相关性）、长程依赖性、粗糙度和异常扩散现象，由于使用布朗运动 (BM)，这些现象在标准扩散或桥模型中未捕获。作为一种补救措施，我们利用最近的 fBM 马尔可夫近似（MA-fBM），构建了 FDBM，它可以在保留 fBM 的非马尔可夫性质的同时实现易于处理的推理。我们证明了保持耦合的生成扩散桥的存在，并利用它从配对训练数据进行未来状态预测。然后，我们将公式扩展到薛定谔桥问题，并推导出原则性损失函数来学习不成对的数据翻译。我们在两项任务上评估 FDBM：根据对齐数据预测未来的蛋白质构象，以及不成对的图像翻译。在这两种设置中，FDBM 与布朗基线相比都实现了卓越的性能，在蛋白质结构预测中产生了较低的 C$_\​​alpha$ 原子位置均方根偏差 (RMSD)，在不成对图像平移中产生了较低的 Fréchet 起始距离 (FID)。</li>
</ul>

<h3>Title: PROPEX-RAG: Enhanced GraphRAG using Prompt-Driven Prompt Execution</h3>
<ul>
<li><strong>Authors: </strong>Tejas Sarnaik, Manan Shah, Ravi Hegde</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01802">https://arxiv.org/abs/2511.01802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01802">https://arxiv.org/pdf/2511.01802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01802]] PROPEX-RAG: Enhanced GraphRAG using Prompt-Driven Prompt Execution(https://arxiv.org/abs/2511.01802)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) has become a robust framework for enhancing Large Language Models (LLMs) with external knowledge. Recent advances in RAG have investigated graph based retrieval for intricate reasoning; however, the influence of prompt design on enhancing the retrieval and reasoning process is still considerably under-examined. In this paper, we present a prompt-driven GraphRAG framework that underscores the significance of prompt formulation in facilitating entity extraction, fact selection, and passage reranking for multi-hop question answering. Our approach creates a symbolic knowledge graph from text data by encoding entities and factual relationships as structured facts triples. We use LLMs selectively during online retrieval to perform semantic filtering and answer generation. We also use entity-guided graph traversal through Personalized PageRank (PPR) to support efficient, scalable retrieval based on the knowledge graph we built. Our system gets state-of-the-art performance on HotpotQA and 2WikiMultiHopQA, with F1 scores of 80.7% and 78.9%, and Recall@5 scores of 97.1% and 98.1%, respectively. These results show that prompt design is an important part of improving retrieval accuracy and response quality. This research lays the groundwork for more efficient and comprehensible multi-hop question-answering systems, highlighting the importance of prompt-aware graph reasoning.</li>
<li><strong>摘要：</strong>检索增强生成（RAG）已成为利用外部知识增强大型语言模型（LLM）的强大框架。 RAG 的最新进展研究了基于图形的复杂推理检索；然而，提示设计对增强检索和推理过程的影响仍然没有得到充分的研究。在本文中，我们提出了一个提示驱动的 GraphRAG 框架，该框架强调了提示表述在促进实体提取、事实选择和多跳问答的段落重新排序方面的重要性。我们的方法通过将实体和事实关系编码为结构化事实三元组，从文本数据创建符号知识图。我们在在线检索期间选择性地使用法学硕士来执行语义过滤和答案生成。我们还通过个性化 PageRank (PPR) 使用实体引导的图遍历来支持基于我们构建的知识图的高效、可扩展的检索。我们的系统在 HotpotQA 和 2WikiMultiHopQA 上获得了最先进的性能，F1 分数分别为 80.7% 和 78.9%，Recall@5 分数分别为 97.1% 和 98.1%。这些结果表明提示设计是提高检索准确性和响应质量的重要组成部分。这项研究为更高效、更易于理解的多跳问答系统奠定了基础，强调了提示感知图形推理的重要性。</li>
</ul>

<h3>Title: Towards Multi-Fidelity Scaling Laws of Neural Surrogates in CFD</h3>
<ul>
<li><strong>Authors: </strong>Paul Setinek, Gianluca Galletti, Johannes Brandstetter</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.flu-dyn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.01830">https://arxiv.org/abs/2511.01830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.01830">https://arxiv.org/pdf/2511.01830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.01830]] Towards Multi-Fidelity Scaling Laws of Neural Surrogates in CFD(https://arxiv.org/abs/2511.01830)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Scaling laws describe how model performance grows with data, parameters and compute. While large datasets can usually be collected at relatively low cost in domains such as language or vision, scientific machine learning is often limited by the high expense of generating training data through numerical simulations. However, by adjusting modeling assumptions and approximations, simulation fidelity can be traded for computational cost, an aspect absent in other domains. We investigate this trade-off between data fidelity and cost in neural surrogates using low- and high-fidelity Reynolds-Averaged Navier-Stokes (RANS) simulations. Reformulating classical scaling laws, we decompose the dataset axis into compute budget and dataset composition. Our experiments reveal compute-performance scaling behavior and exhibit budget-dependent optimal fidelity mixes for the given dataset configuration. These findings provide the first study of empirical scaling laws for multi-fidelity neural surrogate datasets and offer practical considerations for compute-efficient dataset generation in scientific machine learning.</li>
<li><strong>摘要：</strong>缩放定律描述了模型性能如何随着数据、参数和计算而增长。虽然在语言或视觉等领域通常可以以相对较低的成本收集大型数据集，但科学机器学习通常受到通过数值模拟生成训练数据的高昂费用的限制。然而，通过调整建模假设和近似值，可以用模拟保真度来换取计算成本，这是其他领域所不具备的。我们使用低保真度和高保真度雷诺平均纳维斯托克斯 (RANS) 模拟来研究神经代理中数据保真度和成本之间的权衡。重新制定经典的缩放法则，我们将数据集轴分解为计算预算和数据集组成。我们的实验揭示了计算性能扩展行为，并展示了给定数据集配置的依赖于预算的最佳保真度组合。这些发现首次研究了多保真神经代理数据集的经验缩放定律，并为科学机器学习中计算高效的数据集生成提供了实际考虑。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
