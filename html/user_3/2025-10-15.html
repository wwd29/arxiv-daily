<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-15</h1>
<h3>Title: GAR: Generative Adversarial Reinforcement Learning for Formal Theorem Proving</h3>
<ul>
<li><strong>Authors: </strong>Ruida Wang, Jiarui Yao, Rui Pan, Shizhe Diao, Tong Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11769">https://arxiv.org/abs/2510.11769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11769">https://arxiv.org/pdf/2510.11769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11769]] GAR: Generative Adversarial Reinforcement Learning for Formal Theorem Proving(https://arxiv.org/abs/2510.11769)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Solving math problems through verifiable languages such as Lean has significantly impacted both the mathematics and computer science communities. Current state-of-the-art models are often trained with expensive online Reinforcement Learning (RL) or expert iteration. However, these approaches rely on fixed problem sets, which causes inefficient training and limits the model to tackle complex problems. To overcome these limitations, we propose GAR: Generative Adversarial Reinforcement learning, a comprehensive RL training framework that jointly trains the problem composer and solver in an adversarial loop. GAR introduces an implicit curriculum learning mechanism, which aligns task difficulty with the prover's evolving capability. It thereby improves the training efficiency and enables stronger performance of proving advanced theorems. Experiments show that with GAR training, Goedel-Prover-V2-8B and DeepSeek-Prover-V2-7B achieve an average relative improvement in pass@32 of 4.20% on MiniF2F-Test benchmark, while DeepSeek-Prover-V2's pass@32 on ProofNet-Test increases from 22.58% to 25.81%. Beyond formal proving, GAR establishes a general RL paradigm for co-evolution of problem generation and solving under verifiable environments.</li>
<li><strong>摘要：</strong>通过可验证的语言（例如精益）解决数学问题对数学和计算机科学界都产生了重大影响。当前最先进的模型通常采用昂贵的在线强化学习（RL）或专家迭代进行训练。然而，这些方法依赖于固定的问题集，导致训练效率低下，并限制了模型解决复杂问题的能力。为了克服这些限制，我们提出了 GAR：生成对抗强化学习，这是一种综合的 RL 训练框架，可以在对抗循环中联合训练问题编写者和解决者。 GAR 引入了隐式课程学习机制，将任务难度与证明者不断发展的能力结合起来。从而提高了训练效率，并使得证明高级定理的性能更强。实验表明，通过 GAR 训练，Goedel-Prover-V2-8B 和 DeepSeek-Prover-V2-7B 在 MiniF2F-Test 基准上的 pass@32 平均相对改进为 4.20%，而 DeepSeek-Prover-V2 在 ProofNet-Test 上的 pass@32 从 22.58% 增加到 25.81%。除了形式证明之外，GAR 还建立了一个通用的 RL 范式，用于在可验证的环境下共同进化问题的生成和解决。</li>
</ul>

<h3>Title: Schrödinger bridge for generative AI: Soft-constrained formulation and convergence analysis</h3>
<ul>
<li><strong>Authors: </strong>Jin Ma, Ying Tan, Renyuan Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DS, math.OC, q-fin.MF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11829">https://arxiv.org/abs/2510.11829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11829">https://arxiv.org/pdf/2510.11829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11829]] Schrödinger bridge for generative AI: Soft-constrained formulation and convergence analysis(https://arxiv.org/abs/2510.11829)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative AI can be framed as the problem of learning a model that maps simple reference measures into complex data distributions, and it has recently found a strong connection to the classical theory of the Schrödinger bridge problems (SBPs) due partly to their common nature of interpolating between prescribed marginals via entropy-regularized stochastic dynamics. However, the classical SBP enforces hard terminal constraints, which often leads to instability in practical implementations, especially in high-dimensional or data-scarce regimes. To address this challenge, we follow the idea of the so-called soft-constrained Schrödinger bridge problem (SCSBP), in which the terminal constraint is replaced by a general penalty function. This relaxation leads to a more flexible stochastic control formulation of McKean-Vlasov type. We establish the existence of optimal solutions for all penalty levels and prove that, as the penalty grows, both the controls and value functions converge to those of the classical SBP at a linear rate. Our analysis builds on Doob's h-transform representations, the stability results of Schrödinger potentials, Gamma-convergence, and a novel fixed-point argument that couples an optimization problem over the space of measures with an auxiliary entropic optimal transport problem. These results not only provide the first quantitative convergence guarantees for soft-constrained bridges but also shed light on how penalty regularization enables robust generative modeling, fine-tuning, and transfer learning.</li>
<li><strong>摘要：</strong>生成式人工智能可以被定义为学习一个模型的问题，该模型将简单的参考测量映射到复杂的数据分布中，并且最近发现它与薛定谔桥问题（SBP）的经典理论有很强的联系，部分原因在于它们通过熵正则化随机动力学在规定边际之间进行插值的共同性质。然而，经典的 SBP 强制执行硬终端约束，这通常会导致实际实现中的不稳定，特别是在高维或数据稀缺的情况下。为了应对这一挑战，我们遵循所谓的软约束薛定谔桥问题（SCSBP）的思想，其中终端约束被通用惩罚函数取代。这种松弛导致了更灵活的 McKean-Vlasov 类型的随机控制公式。我们建立了所有惩罚级别的最优解的存在性，并证明，随着惩罚的增加，控制函数和价值函数都以线性速率收敛到经典 SBP 的函数。我们的分析建立在 Doob 的 h 变换表示、薛定谔势的稳定性结果、伽玛收敛性以及将测度空间上的优化问题与辅助熵最优传输问题耦合起来的新颖定点论证的基础上。这些结果不仅为软约束桥提供了第一个定量收敛保证，而且还揭示了惩罚正则化如何实现鲁棒的生成建模、微调和迁移学习。</li>
</ul>

<h3>Title: Z0-Inf: Zeroth Order Approximation for Data Influence</h3>
<ul>
<li><strong>Authors: </strong>Narine Kokhlikyan, Kamalika Chaudhuri, Saeed Mahloujifar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11832">https://arxiv.org/abs/2510.11832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11832">https://arxiv.org/pdf/2510.11832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11832]] Z0-Inf: Zeroth Order Approximation for Data Influence(https://arxiv.org/abs/2510.11832)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>A critical aspect of analyzing and improving modern machine learning systems lies in understanding how individual training examples influence a model's predictive behavior. Estimating this influence enables critical applications, including data selection and model debugging; in particular, self-influence, which quantifies the influence of a training point on itself, has found many uses in data quality assessment and outlier detection. Existing methods for measuring data influence, however, are often impractical for large models due to low accuracy or prohibitive computational costs: most approaches either provide poor approximations or rely on gradients and inverse-Hessian computations that remain challenging to scale. In this work, we introduce a highly efficient zeroth-order approximation for estimating the influence of training data that requires only a fraction of the time and memory footprint of prior methods. Notably, our method relies solely on loss values of intermediate checkpoints on the training and test data, along with the checkpoints themselves, making it broadly applicable even when the loss function of interest is non-differentiable. Beyond its computational efficiency, our approach achieves superior accuracy in estimating self-influence and comparable or improved accuracy in estimating train-test influence for fine-tuned large language models, enabling scalable and practical analysis of how training data shapes model behavior.</li>
<li><strong>摘要：</strong>分析和改进现代机器学习系统的一个关键方面在于理解单个训练示例如何影响模型的预测行为。估计这种影响可以实现关键应用，包括数据选择和模型调试；特别是，自我影响，它量化训练点对其自身的影响，在数据质量评估和异常值检测中具有许多用途。然而，由于精度低或计算成本过高，测量数据影响的现有方法对于大型模型通常不切实际：大多数方法要么提供较差的近似值，要么依赖梯度和逆 Hessian 计算，而这些计算仍然难以扩展。在这项工作中，我们引入了一种高效的零阶近似来估计训练数据的影响，该近似只需要现有方法的一小部分时间和内存占用。值得注意的是，我们的方法仅依赖于训练和测试数据上的中间检查点的损失值以及检查点本身，这使其即使在感兴趣的损失函数不可微分时也能广泛适用。除了计算效率之外，我们的方法在估计自我影响方面实现了卓越的准确性，并且在估计微调大型语言模型的训练测试影响方面实现了可比或提高的准确性，从而能够对训练数据如何塑造模型行为进行可扩展和实用的分析。</li>
</ul>

<h3>Title: Don't Walk the Line: Boundary Guidance for Filtered Generation</h3>
<ul>
<li><strong>Authors: </strong>Sarah Ball, Andreas Haupt</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11834">https://arxiv.org/abs/2510.11834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11834">https://arxiv.org/pdf/2510.11834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11834]] Don't Walk the Line: Boundary Guidance for Filtered Generation(https://arxiv.org/abs/2510.11834)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative models are increasingly paired with safety classifiers that filter harmful or undesirable outputs. A common strategy is to fine-tune the generator to reduce the probability of being filtered, but this can be suboptimal: it often pushes the model toward producing samples near the classifier's decision boundary, increasing both false positives and false negatives. We propose Boundary Guidance, a reinforcement learning fine-tuning method that explicitly steers generation away from the classifier's margin. On a benchmark of jailbreak and ambiguous prompts, Boundary Guidance improves both the safety and the utility of outputs, as judged by LLM-as-a-Judge evaluations. Comprehensive ablations across model scales and reward designs demonstrate the robustness of our approach.</li>
<li><strong>摘要：</strong>生成模型越来越多地与安全分类器配合使用，以过滤有害或不需要的输出。一种常见的策略是微调生成器以降低被过滤的概率，但这可能不是最理想的：它通常会推动模型在分类器的决策边界附近生成样本，从而增加误报和漏报。我们提出了边界指导，这是一种强化学习微调方法，可以明确地引导生成远离分类器的边缘。根据法学硕士法官评估的判断，在越狱和模糊提示的基准上，边界指导提高了输出的安全性和实用性。跨模型规模和奖励设计的全面消融证明了我们方法的稳健性。</li>
</ul>

<h3>Title: WaveletDiff: Multilevel Wavelet Diffusion For Time Series Generation</h3>
<ul>
<li><strong>Authors: </strong>Yu-Hsiang Wang, Olgica Milenkovic</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11839">https://arxiv.org/abs/2510.11839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11839">https://arxiv.org/pdf/2510.11839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11839]] WaveletDiff: Multilevel Wavelet Diffusion For Time Series Generation(https://arxiv.org/abs/2510.11839)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Time series are ubiquitous in many applications that involve forecasting, classification and causal inference tasks, such as healthcare, finance, audio signal processing and climate sciences. Still, large, high-quality time series datasets remain scarce. Synthetic generation can address this limitation; however, current models confined either to the time or frequency domains struggle to reproduce the inherently multi-scaled structure of real-world time series. We introduce WaveletDiff, a novel framework that trains diffusion models directly on wavelet coefficients to exploit the inherent multi-resolution structure of time series data. The model combines dedicated transformers for each decomposition level with cross-level attention mechanisms that enable selective information exchange between temporal and frequency scales through adaptive gating. It also incorporates energy preservation constraints for individual levels based on Parseval's theorem to preserve spectral fidelity throughout the diffusion process. Comprehensive tests across six real-world datasets from energy, finance, and neuroscience domains demonstrate that WaveletDiff consistently outperforms state-of-the-art time-domain and frequency-domain generative methods on both short and long time series across five diverse performance metrics. For example, WaveletDiff achieves discriminative scores and Context-FID scores that are $3\times$ smaller on average than the second-best baseline across all datasets.</li>
<li><strong>摘要：</strong>时间序列在涉及预测、分类和因果推理任务的许多应用中无处不在，例如医疗保健、金融、音频信号处理和气候科学。尽管如此，大型、高质量的时间序列数据集仍然稀缺。合成发电可以解决这个限制；然而，当前的模型仅限于时域或频域，难以再现现实世界时间序列固有的多尺度结构。我们引入了 WaveletDiff，这是一种直接在小波系数上训练扩散模型的新颖框架，以利用时间序列数据固有的多分辨率结构。该模型将每个分解级别的专用变压器与跨级别注意机制相结合，通过自适应门控实现时间和频率尺度之间的选择性信息交换。它还结合了基于帕塞瓦尔定理的各个能级的能量保存约束，以在整个扩散过程中保持光谱保真度。对来自能源、金融和神经科学领域的六个真实世界数据集的综合测试表明，WaveletDiff 在五个不同性能指标的短时间序列和长时间序列上始终优于最先进的时域和频域生成方法。例如，WaveletDiff 实现的判别分数和 Context-FID 分数平均比所有数据集中的第二最佳基线小 $3\time$。</li>
</ul>

<h3>Title: Balancing Synthetic Data and Replay for Enhancing Task-Specific Capabilities</h3>
<ul>
<li><strong>Authors: </strong>Urs Spiegelhalter, Jörg K.H. Franke, Frank Hutter</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11842">https://arxiv.org/abs/2510.11842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11842">https://arxiv.org/pdf/2510.11842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11842]] Balancing Synthetic Data and Replay for Enhancing Task-Specific Capabilities(https://arxiv.org/abs/2510.11842)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Adapting language models to new tasks through continued pretraining faces a fundamental trade-off: models must learn new capabilities while avoiding catastrophic forgetting of existing knowledge. While prior work has studied synthetic data generation techniques, the optimal replay ratios for balancing task performance and knowledge retention under computational constraints remain poorly understood. We present a comprehensive empirical study investigating the interplay between replay ratio configuration and computational budget when adapting language models to new tasks. Using the bAbI reasoning tasks as our target objective, we apply synthetic data generation and systematically evaluate different total token budgets and replay ratio configurations. We analyze their effects on both task mastery and general knowledge retention. Our experiments reveal an optimal configuration that balances task-specific performance with general knowledge retention. Based on our findings, we provide empirically-grounded guidelines for selecting replay ratios based on computational budget, enabling practitioners to achieve strong task adaptation with significantly reduced training costs.</li>
<li><strong>摘要：</strong>通过持续的预训练使语言模型适应新任务面临着一个基本的权衡：模型必须学习新的能力，同时避免对现有知识的灾难性遗忘。虽然之前的工作研究了合成数据生成技术，但在计算限制下平衡任务性能和知识保留的最佳重播率仍然知之甚少。我们提出了一项全面的实证研究，调查在使语言模型适应新任务时重播率配置和计算预算之间的相互作用。使用 bAbI 推理任务作为我们的目标，我们应用合成数据生成并系统地评估不同的总代币预算和重放率配置。我们分析了它们对任务掌握和一般知识保留的影响。我们的实验揭示了一种最佳配置，可以平衡特定任务的性能与一般知识的保留。根据我们的发现，我们提供了基于经验的指南，用于根据计算预算选择重放率，使从业者能够在显着降低训练成本的情况下实现强大的任务适应。</li>
</ul>

<h3>Title: Y-shaped Generative Flows</h3>
<ul>
<li><strong>Authors: </strong>Arip Asadulaev, Semyon Semenov, Abduragim Shtanchaev, Eric Moulines, Fakhri Karray, Martin Takac</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11955">https://arxiv.org/abs/2510.11955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11955">https://arxiv.org/pdf/2510.11955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11955]] Y-shaped Generative Flows(https://arxiv.org/abs/2510.11955)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Modern continuous-time generative models often induce V-shaped transport: each sample travels independently along nearly straight trajectories from prior to data, overlooking shared structure. We introduce Y-shaped generative flows, which move probability mass together along shared pathways before branching to target-specific endpoints. Our formulation is based on novel velocity-powered transport cost with a sublinear exponent (between zero and one). this concave dependence rewards joint and fast mass movement. Practically, we instantiate the idea in a scalable neural ODE training objective. On synthetic, image, and biology datasets, Y-flows recover hierarchy-aware structure, improve distributional metrics over strong flow-based baselines, and reach targets with fewer integration steps.</li>
<li><strong>摘要：</strong>现代连续时间生成模型通常会引发 V 形传输：每个样本沿着之前数据的近乎笔直的轨迹独立移动，忽略了共享结构。我们引入了 Y 形生成流，它在分支到特定目标端点之前沿着共享路径将概率质量一起移动。我们的公式基于具有次线性指数（0 到 1 之间）的新型速度驱动运输成本。这种凹形依赖性有利于联合且快速的群众运动。实际上，我们在可扩展的神经 ODE 训练目标中实例化了这个想法。在合成、图像和生物数据集上，Y 流恢复层次结构感知结构，在强大的基于流的基线上改进分布指标，并以更少的集成步骤达到目标。</li>
</ul>

<h3>Title: MosaicDiff: Training-free Structural Pruning for Diffusion Model Acceleration Reflecting Pretraining Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Bowei Guo, Shengkun Tang, Cong Zeng, Zhiqiang Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11962">https://arxiv.org/abs/2510.11962</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11962">https://arxiv.org/pdf/2510.11962</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11962]] MosaicDiff: Training-free Structural Pruning for Diffusion Model Acceleration Reflecting Pretraining Dynamics(https://arxiv.org/abs/2510.11962)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion models are renowned for their generative capabilities, yet their pretraining processes exhibit distinct phases of learning speed that have been entirely overlooked in prior post-training acceleration efforts in the community. In this study, we introduce a novel framework called MosaicDiff that aligns diffusion pretraining dynamics with post-training sampling acceleration via trajectory-aware structural pruning. Our approach leverages the observation that the middle, fast-learning stage of diffusion pretraining requires more conservative pruning to preserve critical model features, while the early and later, slow-learning stages benefit from a more aggressive pruning strategy. This adaptive pruning mechanism is the first to explicitly mirror the inherent learning speed variations of diffusion pretraining, thereby harmonizing the model's inner training dynamics with its accelerated sampling process. Extensive experiments on DiT and SDXL demonstrate that our method achieves significant speed-ups in sampling without compromising output quality, outperforming previous state-of-the-art methods by large margins, also providing a new viewpoint for more efficient and robust training-free diffusion acceleration.</li>
<li><strong>摘要：</strong>扩散模型以其生成能力而闻名，但它们的预训练过程表现出不同的学习速度阶段，而这些阶段在社区之前的训练后加速工作中完全被忽视了。在这项研究中，我们引入了一种名为 MosaicDiff 的新颖框架，该框架通过轨迹感知结构修剪将扩散预训练动态与训练后采样加速相结合。我们的方法利用了这样的观察结果：扩散预训练的中间快速学习阶段需要更保守的剪枝以保留关键模型特征，而早期和后期的慢速学习阶段则受益于更积极的剪枝策略。这种自适应剪枝机制是第一个明确反映扩散预训练的固有学习速度变化的机制，从而使模型的内部训练动态与其加速采样过程相协调。 DiT 和 SDXL 上的大量实验表明，我们的方法在不影响输出质量的情况下实现了采样的显着加速，大幅优于以前最先进的方法，也为更高效、更强大的免训练扩散加速提供了新的视角。</li>
</ul>

<h3>Title: QLENS: Towards A Quantum Perspective of Language Transformers</h3>
<ul>
<li><strong>Authors: </strong>Aditya Gupta, Kirandeep Kaur, Vinayak Gupta</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11963">https://arxiv.org/abs/2510.11963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11963">https://arxiv.org/pdf/2510.11963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11963]] QLENS: Towards A Quantum Perspective of Language Transformers(https://arxiv.org/abs/2510.11963)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In natural language processing, current methods for understanding Transformers are successful at identifying intermediate predictions during a model's inference. However, these approaches function as limited diagnostic checkpoints, lacking a mathematical framework for mechanistically modeling how each layer facilitates transitions between these evolving states. This interpretability gap and past successes of interdisciplinary outlooks inspire us to turn to physics in search of a descriptive mathematical framework for Transformers. We observe that language models are intrinsically probabilistic, an attribute that is echoed in the core postulates of quantum mechanics. This parallel inspires us to translate insights from this discipline to that of natural language processing. Towards this objective, we propose QLENS a novel attempt to develop a physics-based perspective on the Transformer generation process. Under QLENS, a Transformer is studied by converting its latent activations into a state vector in a Hilbert space derived from the model's output units. This state subsequently evolves through hidden layers - reformulated as unitary operators and analogously defined Hamiltonians - during inference. The model's final probability distribution is obtained by applying the Born rule to the end state using a specific measurement operator. To demonstrate QLENS's potential, we conduct a proof-of-concept by probing a toy Transformer to investigate the influence of individual layers in a model's prediction trajectory. We present our work as a foundation for cross-domain insights to be leveraged towards a broader understanding of Transformers.</li>
<li><strong>摘要：</strong>在自然语言处理中，当前理解 Transformer 的方法成功地识别了模型推理过程中的中间预测。然而，这些方法的功能有限，缺乏一个数学框架来机械地建模每一层如何促进这些演变状态之间的转换。这种可解释性的差距和过去跨学科观点的成功激励我们转向物理学，为变形金刚寻找描述性的数学框架。我们观察到语言模型本质上是概率性的，这一属性在量子力学的核心假设中得到了呼应。这种相似之处激励我们将这一学科的见解转化为自然语言处理的见解。为了实现这一目标，我们提出 QLENS 是一种新颖的尝试，旨在开发基于物理的 Transformer 生成过程视角。在 QLENS 下，通过将 Transformer 的潜在激活转换为从模型输出单元导出的希尔伯特空间中的状态向量来研究 Transformer。随后，在推理过程中，这种状态通过隐藏层演变——重新表述为酉算子和类似定义的哈密顿量。模型的最终概率分布是通过使用特定测量算子将玻恩规则应用于最终状态来获得的。为了展示 QLENS 的潜力，我们通过探测玩具 Transformer 来研究模型预测轨迹中各个层的影响，从而进行概念验证。我们将我们的工作作为跨领域见解的基础，以帮助人们更广泛地了解 Transformers。</li>
</ul>

<h3>Title: APGNet: Adaptive Prior-Guided for Underwater Camouflaged Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Xinxin Huang, Han Sun, Junmin Cai, Ningzhong Liu, Huiyu Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12056">https://arxiv.org/abs/2510.12056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12056">https://arxiv.org/pdf/2510.12056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12056]] APGNet: Adaptive Prior-Guided for Underwater Camouflaged Object Detection(https://arxiv.org/abs/2510.12056)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Detecting camouflaged objects in underwater environments is crucial for marine ecological research and resource exploration. However, existing methods face two key challenges: underwater image degradation, including low contrast and color distortion, and the natural camouflage of marine organisms. Traditional image enhancement techniques struggle to restore critical features in degraded images, while camouflaged object detection (COD) methods developed for terrestrial scenes often fail to adapt to underwater environments due to the lack of consideration for underwater optical characteristics. To address these issues, we propose APGNet, an Adaptive Prior-Guided Network, which integrates a Siamese architecture with a novel prior-guided mechanism to enhance robustness and detection accuracy. First, we employ the Multi-Scale Retinex with Color Restoration (MSRCR) algorithm for data augmentation, generating illumination-invariant images to mitigate degradation effects. Second, we design an Extended Receptive Field (ERF) module combined with a Multi-Scale Progressive Decoder (MPD) to capture multi-scale contextual information and refine feature representations. Furthermore, we propose an adaptive prior-guided mechanism that hierarchically fuses position and boundary priors by embedding spatial attention in high-level features for coarse localization and using deformable convolution to refine contours in low-level features. Extensive experimental results on two public MAS datasets demonstrate that our proposed method APGNet outperforms 15 state-of-art methods under widely used evaluation metrics.</li>
<li><strong>摘要：</strong>检测水下环境中的伪装物体对于海洋生态研究和资源勘探至关重要。然而，现有方法面临两个关键挑战：水下图像退化，包括低对比度和颜色失真，以及海洋生物的自然伪装。传统的图像增强技术难以恢复退化图像中的关键特征，而针对陆地场景开发的伪装目标检测（COD）方法由于缺乏对水下光学特性的考虑，往往无法适应水下环境。为了解决这些问题，我们提出了 APGNet，一种自适应先验引导网络，它将 Siamese 架构与新颖的先验引导机制相结合，以增强鲁棒性和检测准确性。首先，我们采用带有颜色恢复的多尺度 Retinex (MSRCR) 算法进行数据增强，生成光照不变图像以减轻退化影响。其次，我们设计了一个扩展感受野（ERF）模块与多尺度渐进解码器（MPD）相结合，以捕获多尺度上下文信息并细化特征表示。此外，我们提出了一种自适应先验引导机制，通过在高级特征中嵌入空间注意力以进行粗定位，并使用可变形卷积来细化低级特征中的轮廓，从而分层融合位置和边界先验。在两个公共 MAS 数据集上的广泛实验结果表明，我们提出的方法 APGNet 在广泛使用的评估指标下优于 15 种最先进的方法。</li>
</ul>

<h3>Title: Your VAR Model is Secretly an Efficient and Explainable Generative Classifier</h3>
<ul>
<li><strong>Authors: </strong>Yi-Chung Chen, David I. Inouye, Jing Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12060">https://arxiv.org/abs/2510.12060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12060">https://arxiv.org/pdf/2510.12060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12060]] Your VAR Model is Secretly an Efficient and Explainable Generative Classifier(https://arxiv.org/abs/2510.12060)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative classifiers, which leverage conditional generative models for classification, have recently demonstrated desirable properties such as robustness to distribution shifts. However, recent progress in this area has been largely driven by diffusion-based models, whose substantial computational cost severely limits scalability. This exclusive focus on diffusion-based methods has also constrained our understanding of generative classifiers. In this work, we propose a novel generative classifier built on recent advances in visual autoregressive (VAR) modeling, which offers a new perspective for studying generative classifiers. To further enhance its performance, we introduce the Adaptive VAR Classifier$^+$ (A-VARC$^+$), which achieves a superior trade-off between accuracy and inference speed, thereby significantly improving practical applicability. Moreover, we show that the VAR-based method exhibits fundamentally different properties from diffusion-based methods. In particular, due to its tractable likelihood, the VAR-based classifier enables visual explainability via token-wise mutual information and demonstrates inherent resistance to catastrophic forgetting in class-incremental learning tasks.</li>
<li><strong>摘要：</strong>利用条件生成模型进行分类的生成分类器最近表现出了理想的特性，例如对分布变化的鲁棒性。然而，这一领域的最新进展很大程度上是由基于扩散的模型推动的，其大量的计算成本严重限制了可扩展性。这种对基于扩散的方法的专门关注也限制了我们对生成分类器的理解。在这项工作中，我们提出了一种基于视觉自回归（VAR）建模最新进展的新型生成分类器，它为研究生成分类器提供了新的视角。为了进一步增强其性能，我们引入了自适应VAR分类器$^+$（A-VARC$^+$），它实现了准确性和推理速度之间的优越权衡，从而显着提高了实际适用性。此外，我们表明基于 VAR 的方法表现出与基于扩散的方法根本不同的属性。特别是，由于其易于处理的可能性，基于 VAR 的分类器可以通过 token-wise 互信息实现视觉可解释性，并在类增量学习任务中表现出对灾难性遗忘的固有抵抗力。</li>
</ul>

<h3>Title: VIDMP3: Video Editing by Representing Motion with Pose and Position Priors</h3>
<ul>
<li><strong>Authors: </strong>Sandeep Mishra, Oindrila Saha, Alan C. Bovik</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12069">https://arxiv.org/abs/2510.12069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12069">https://arxiv.org/pdf/2510.12069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12069]] VIDMP3: Video Editing by Representing Motion with Pose and Position Priors(https://arxiv.org/abs/2510.12069)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Motion-preserved video editing is crucial for creators, particularly in scenarios that demand flexibility in both the structure and semantics of swapped objects. Despite its potential, this area remains underexplored. Existing diffusion-based editing methods excel in structure-preserving tasks, using dense guidance signals to ensure content integrity. While some recent methods attempt to address structure-variable editing, they often suffer from issues such as temporal inconsistency, subject identity drift, and the need for human intervention. To address these challenges, we introduce VidMP3, a novel approach that leverages pose and position priors to learn a generalized motion representation from source videos. Our method enables the generation of new videos that maintain the original motion while allowing for structural and semantic flexibility. Both qualitative and quantitative evaluations demonstrate the superiority of our approach over existing methods. The code will be made publicly available at this https URL.</li>
<li><strong>摘要：</strong>运动保留视频编辑对于创作者来说至关重要，特别是在交换对象的结构和语义都需要灵活性的场景中。尽管潜力巨大，但该领域仍未得到充分开发。现有的基于扩散的编辑方法在结构保留任务中表现出色，使用密集的引导信号来确保内容完整性。虽然最近的一些方法试图解决结构变量编辑问题，但它们经常遇到时间不一致、受试者身份漂移和需要人工干预等问题。为了解决这些挑战，我们引入了 VidMP3，这是一种利用姿势和位置先验从源视频中学习广义运动表示的新颖方法。我们的方法能够生成保持原始运动的新视频，同时允许结构和语义的灵活性。定性和定量评估都证明了我们的方法优于现有方法。该代码将在此 https URL 上公开提供。</li>
</ul>

<h3>Title: A Review on Domain Adaption and Generative Adversarial Networks(GANs)</h3>
<ul>
<li><strong>Authors: </strong>Aashish Dhawan, Divyanshu Mudgal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12075">https://arxiv.org/abs/2510.12075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12075">https://arxiv.org/pdf/2510.12075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12075]] A Review on Domain Adaption and Generative Adversarial Networks(GANs)(https://arxiv.org/abs/2510.12075)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The major challenge in today's computer vision scenario is the availability of good quality labeled data. In a field of study like image classification, where data is of utmost importance, we need to find more reliable methods which can overcome the scarcity of data to produce results comparable to previous benchmark results. In most cases, obtaining labeled data is very difficult because of the high cost of human labor and in some cases impossible. The purpose of this paper is to discuss Domain Adaptation and various methods to implement it. The main idea is to use a model trained on a particular dataset to predict on data from a different domain of the same kind, for example - a model trained on paintings of airplanes predicting on real images of airplanes</li>
<li><strong>摘要：</strong>当今计算机视觉场景的主要挑战是高质量标记数据的可用性。在图像分类等数据至关重要的研究领域，我们需要找到更可靠的方法来克服数据稀缺的问题，以产生与之前的基准结果相当的结果。在大多数情况下，由于人力成本高昂，获取标记数据非常困难，在某些情况下甚至是不可能的。本文的目的是讨论域适应以及实现它的各种方法。主要思想是使用在特定数据集上训练的模型来预测来自同类不同域的数据，例如 - 在飞机绘画上训练的模型预测飞机的真实图像</li>
</ul>

<h3>Title: Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback</h3>
<ul>
<li><strong>Authors: </strong>Xingpei Ma, Shenneng Huang, Jiaran Cai, Yuansheng Guan, Shen Zheng, Hanfeng Zhao, Qiang Zhang, Shunsi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12089">https://arxiv.org/abs/2510.12089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12089">https://arxiv.org/pdf/2510.12089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12089]] Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback(https://arxiv.org/abs/2510.12089)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion models have significantly improved audio-driven human video generation, surpassing traditional methods in both quality and controllability. However, existing approaches still face challenges in lip-sync accuracy, temporal coherence for long video generation, and multi-character animation. In this work, we propose a diffusion transformer (DiT)-based framework for generating lifelike talking videos of arbitrary length, and introduce a training-free method for multi-character audio-driven animation. First, we employ a LoRA-based training strategy combined with a position shift inference approach, which enables efficient long video generation while preserving the capabilities of the foundation model. Moreover, we combine partial parameter updates with reward feedback to enhance both lip synchronization and natural body motion. Finally, we propose a training-free approach, Mask Classifier-Free Guidance (Mask-CFG), for multi-character animation, which requires no specialized datasets or model modifications and supports audio-driven animation for three or more characters. Experimental results demonstrate that our method outperforms existing state-of-the-art approaches, achieving high-quality, temporally coherent, and multi-character audio-driven video generation in a simple, efficient, and cost-effective manner.</li>
<li><strong>摘要：</strong>扩散模型的最新进展显着改进了音频驱动的人类视频生成，在质量和可控性方面超越了传统方法。然而，现有的方法仍然面临口型同步准确性、长视频生成的时间连贯性和多角色动画方面的挑战。在这项工作中，我们提出了一种基于扩散变压器（DiT）的框架，用于生成任意长度的逼真谈话视频，并引入了一种用于多字符音频驱动动画的免训练方法。首先，我们采用基于 LoRA 的训练策略与位置偏移推理方法相结合，可以实现高效的长视频生成，同时保留基础模型的功能。此外，我们将部分参数更新与奖励反馈相结合，以增强嘴唇同步和自然的身体运动。最后，我们提出了一种用于多角色动画的免训练方法，即无掩模分类器指导（Mask-CFG），它不需要专门的数据集或模型修改，并支持三个或更多角色的音频驱动动画。实验结果表明，我们的方法优于现有的最先进方法，以简单、高效且经济高效的方式实现高质量、时间连贯和多字符音频驱动的视频生成。</li>
</ul>

<h3>Title: IL3D: A Large-Scale Indoor Layout Dataset for LLM-Driven 3D Scene Generation</h3>
<ul>
<li><strong>Authors: </strong>Wenxu Zhou, Kaixuan Nie, Hang Du, Dong Yin, Wei Huang, Siqiang Guo, Xiaobo Zhang, Pengbo Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12095">https://arxiv.org/abs/2510.12095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12095">https://arxiv.org/pdf/2510.12095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12095]] IL3D: A Large-Scale Indoor Layout Dataset for LLM-Driven 3D Scene Generation(https://arxiv.org/abs/2510.12095)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In this study, we present IL3D, a large-scale dataset meticulously designed for large language model (LLM)-driven 3D scene generation, addressing the pressing demand for diverse, high-quality training data in indoor layout design. Comprising 27,816 indoor layouts across 18 prevalent room types and a library of 29,215 high-fidelity 3D object assets, IL3D is enriched with instance-level natural language annotations to support robust multimodal learning for vision-language tasks. We establish rigorous benchmarks to evaluate LLM-driven scene generation. Experimental results show that supervised fine-tuning (SFT) of LLMs on IL3D significantly improves generalization and surpasses the performance of SFT on other datasets. IL3D offers flexible multimodal data export capabilities, including point clouds, 3D bounding boxes, multiview images, depth maps, normal maps, and semantic masks, enabling seamless adaptation to various visual tasks. As a versatile and robust resource, IL3D significantly advances research in 3D scene generation and embodied intelligence, by providing high-fidelity scene data to support environment perception tasks of embodied agents.</li>
<li><strong>摘要：</strong>在这项研究中，我们提出了 IL3D，这是一个为大型语言模型 (LLM) 驱动的 3D 场景生成而精心设计的大型数据集，解决了室内布局设计中对多样化、高质量训练数据的迫切需求。 IL3D 包含 18 种流行房间类型的 27,816 个室内布局以及包含 29,215 个高保真 3D 对象资产的库，并富含实例级自然语言注释，以支持视觉语言任务的强大多模态学习。我们建立了严格的基准来评估法学硕士驱动的场景生成。实验结果表明，IL3D 上的 LLM 的监督微调（SFT）显着提高了泛化能力，并超越了 SFT 在其他数据集上的性能。 IL3D 提供灵活的多模式数据导出功能，包括点云、3D 边界框、多视图图像、深度图、法线图和语义掩模，从而能够无缝适应各种视觉任务。作为一种多功能且强大的资源，IL3D 通过提供高保真场景数据来支持实体代理的环境感知任务，显着推进了 3D 场景生成和实体智能的研究。</li>
</ul>

<h3>Title: An Adaptive Edge-Guided Dual-Network Framework for Fast QR Code Motion Deblurring</h3>
<ul>
<li><strong>Authors: </strong>Jianping Li, Dongyang Guo, Wenjie Li, Wei Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12098">https://arxiv.org/abs/2510.12098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12098">https://arxiv.org/pdf/2510.12098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12098]] An Adaptive Edge-Guided Dual-Network Framework for Fast QR Code Motion Deblurring(https://arxiv.org/abs/2510.12098)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Unlike general image deblurring that prioritizes perceptual quality, QR code deblurring focuses on ensuring successful decoding. QR codes are characterized by highly structured patterns with sharp edges, a robust prior for restoration. Yet existing deep learning methods rarely exploit these priors explicitly. To address this gap, we propose the Edge-Guided Attention Block (EGAB), which embeds explicit edge priors into a Transformer architecture. Based on EGAB, we develop Edge-Guided Restormer (EG-Restormer), an effective network that significantly boosts the decoding rate of severely blurred QR codes. For mildly blurred inputs, we design the Lightweight and Efficient Network (LENet) for fast deblurring. We further integrate these two networks into an Adaptive Dual-network (ADNet), which dynamically selects the suitable network based on input blur severity, making it ideal for resource-constrained mobile devices. Extensive experiments show that our EG-Restormer and ADNet achieve state-of-the-art performance with a competitive speed. Project page: this https URL</li>
<li><strong>摘要：</strong>与优先考虑感知质量的一般图像去模糊不同，QR 码去模糊侧重于确保成功解码。 QR 码的特点是具有尖锐边缘的高度结构化图案，是恢复的强大先验。然而现有的深度学习方法很少明确地利用这些先验。为了解决这一差距，我们提出了边缘引导注意力块（EGAB），它将显式边缘先验嵌入到 Transformer 架构中。基于 EGAB，我们开发了 Edge-Guided Restormer（EG-Restormer），这是一种有效的网络，可以显着提高严重模糊的 QR 码的解码率。对于轻度模糊的输入，我们设计了轻量级高效网络（LENet）以实现快速去模糊。我们进一步将这两个网络集成到自适应双网络（ADNet）中，该网络根据输入模糊严重程度动态选择合适的网络，使其成为资源受限的移动设备的理想选择。大量实验表明，我们的 EG-Restormer 和 ADNet 以具有竞争力的速度实现了最先进的性能。项目页面：此 https URL</li>
</ul>

<h3>Title: G4Splat: Geometry-Guided Gaussian Splatting with Generative Prior</h3>
<ul>
<li><strong>Authors: </strong>Junfeng Ni, Yixin Chen, Zhifei Yang, Yu Liu, Ruijie Lu, Song-Chun Zhu, Siyuan Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12099">https://arxiv.org/abs/2510.12099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12099">https://arxiv.org/pdf/2510.12099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12099]] G4Splat: Geometry-Guided Gaussian Splatting with Generative Prior(https://arxiv.org/abs/2510.12099)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Despite recent advances in leveraging generative prior from pre-trained diffusion models for 3D scene reconstruction, existing methods still face two critical limitations. First, due to the lack of reliable geometric supervision, they struggle to produce high-quality reconstructions even in observed regions, let alone in unobserved areas. Second, they lack effective mechanisms to mitigate multi-view inconsistencies in the generated images, leading to severe shape-appearance ambiguities and degraded scene geometry. In this paper, we identify accurate geometry as the fundamental prerequisite for effectively exploiting generative models to enhance 3D scene reconstruction. We first propose to leverage the prevalence of planar structures to derive accurate metric-scale depth maps, providing reliable supervision in both observed and unobserved regions. Furthermore, we incorporate this geometry guidance throughout the generative pipeline to improve visibility mask estimation, guide novel view selection, and enhance multi-view consistency when inpainting with video diffusion models, resulting in accurate and consistent scene completion. Extensive experiments on Replica, ScanNet++, and DeepBlending show that our method consistently outperforms existing baselines in both geometry and appearance reconstruction, particularly for unobserved regions. Moreover, our method naturally supports single-view inputs and unposed videos, with strong generalizability in both indoor and outdoor scenarios with practical real-world applicability. The project page is available at this https URL.</li>
<li><strong>摘要：</strong>尽管最近在利用预训练扩散模型的生成先验进行 3D 场景重建方面取得了进展，但现有方法仍然面临两个关键限制。首先，由于缺乏可靠的几何监督，即使在观察区域，他们也很难产生高质量的重建，更不用说在未观察区域了。其次，它们缺乏有效的机制来减轻生成图像中的多视图不一致，从而导致严重的形状外观模糊和场景几何形状退化。在本文中，我们将精确的几何形状确定为有效利用生成模型来增强 3D 场景重建的基本前提。我们首先建议利用平面结构的普遍性来导出准确的公制尺度深度图，从而在观察到和未观察到的区域提供可靠的监督。此外，我们在整个生成流程中融入了这种几何指导，以改进可见性掩模估计，指导新颖的视图选择，并在使用视频扩散模型修复时增强多视图一致性，从而实现准确且一致的场景完成。在 Replica、ScanNet++ 和 DeepBlending 上进行的大量实验表明，我们的方法在几何和外观重建方面始终优于现有基线，特别是对于未观察到的区域。此外，我们的方法自然支持单视图输入和未摆出的视频，在室内和室外场景中具有很强的通用性，具有实际的现实应用性。项目页面可通过此 https URL 获取。</li>
</ul>

<h3>Title: Self-Supervised Selective-Guided Diffusion Model for Old-Photo Face Restoration</h3>
<ul>
<li><strong>Authors: </strong>Wenjie Li, Xiangyi Wang, Heng Guo, Guangwei Gao, Zhanyu Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12114">https://arxiv.org/abs/2510.12114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12114">https://arxiv.org/pdf/2510.12114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12114]] Self-Supervised Selective-Guided Diffusion Model for Old-Photo Face Restoration(https://arxiv.org/abs/2510.12114)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Old-photo face restoration poses significant challenges due to compounded degradations such as breakage, fading, and severe blur. Existing pre-trained diffusion-guided methods either rely on explicit degradation priors or global statistical guidance, which struggle with localized artifacts or face color. We propose Self-Supervised Selective-Guided Diffusion (SSDiff), which leverages pseudo-reference faces generated by a pre-trained diffusion model under weak guidance. These pseudo-labels exhibit structurally aligned contours and natural colors, enabling region-specific restoration via staged supervision: structural guidance applied throughout the denoising process and color refinement in later steps, aligned with the coarse-to-fine nature of diffusion. By incorporating face parsing maps and scratch masks, our method selectively restores breakage regions while avoiding identity mismatch. We further construct VintageFace, a 300-image benchmark of real old face photos with varying degradation levels. SSDiff outperforms existing GAN-based and diffusion-based methods in perceptual quality, fidelity, and regional controllability. Code link: this https URL.</li>
<li><strong>摘要：</strong>由于破损、褪色和严重模糊等复合退化，老照片面部修复面临重大挑战。现有的预先训练的扩散引导方法要么依赖于显式的退化先验，要么依赖于全局统计指导，这与局部伪影或面部颜色作斗争。我们提出自监督选择性引导扩散（SSDiff），它利用弱引导下预训练扩散模型生成的伪参考面。这些伪标签表现出结构一致的轮廓和自然颜色，通过分阶段监督实现特定区域的恢复：在整个去噪过程中应用结构指导，并在后续步骤中进行颜色细化，与扩散的从粗到细的性质保持一致。通过结合人脸解析图和划痕掩模，我们的方法选择性地恢复破损区域，同时避免身份不匹配。我们进一步构建了 VintageFace，一个包含不同退化级别的真实老脸照片的 300 张图像基准。 SSDiff 在感知质量、保真度和区域可控性方面优于现有的基于 GAN 和基于扩散的方法。代码链接：这个https URL。</li>
</ul>

<h3>Title: ImageSentinel: Protecting Visual Datasets from Unauthorized Retrieval-Augmented Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Ziyuan Luo, Yangyi Zhao, Ka Chun Cheung, Simon See, Renjie Wan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12119">https://arxiv.org/abs/2510.12119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12119">https://arxiv.org/pdf/2510.12119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12119]] ImageSentinel: Protecting Visual Datasets from Unauthorized Retrieval-Augmented Image Generation(https://arxiv.org/abs/2510.12119)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The widespread adoption of Retrieval-Augmented Image Generation (RAIG) has raised significant concerns about the unauthorized use of private image datasets. While these systems have shown remarkable capabilities in enhancing generation quality through reference images, protecting visual datasets from unauthorized use in such systems remains a challenging problem. Traditional digital watermarking approaches face limitations in RAIG systems, as the complex feature extraction and recombination processes fail to preserve watermark signals during generation. To address these challenges, we propose ImageSentinel, a novel framework for protecting visual datasets in RAIG. Our framework synthesizes sentinel images that maintain visual consistency with the original dataset. These sentinels enable protection verification through randomly generated character sequences that serve as retrieval keys. To ensure seamless integration, we leverage vision-language models to generate the sentinel images. Experimental results demonstrate that ImageSentinel effectively detects unauthorized dataset usage while preserving generation quality for authorized applications. Code is available at this https URL.</li>
<li><strong>摘要：</strong>检索增强图像生成（RAIG）的广泛采用引起了人们对未经授权使用私有图像数据集的严重担忧。虽然这些系统在通过参考图像提高生成质量方面表现出了卓越的能力，但保护视觉数据集免遭此类系统中未经授权的使用仍然是一个具有挑战性的问题。传统的数字水印方法在 RAIG 系统中面临局限性，因为复杂的特征提取和重组过程无法在生成过程中保留水印信号。为了应对这些挑战，我们提出了 ImageSentinel，这是一种用于保护 RAIG 中视觉数据集的新颖框架。我们的框架合成哨兵图像，保持与原始数据集的视觉一致性。这些哨兵通过随机生成的字符序列（用作检索密钥）实现保护验证。为了确保无缝集成，我们利用视觉语言模型来生成哨兵图像。实验结果表明，ImageSentinel 可以有效检测未经授权的数据集使用，同时保持授权应用程序的生成质量。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Self-Verifying Reflection Helps Transformers with CoT Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Zhongwei Yu, Wannian Xia, Xue Yan, Bo Xu, Haifeng Zhang, Yali Du, Jun Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12157">https://arxiv.org/abs/2510.12157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12157">https://arxiv.org/pdf/2510.12157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12157]] Self-Verifying Reflection Helps Transformers with CoT Reasoning(https://arxiv.org/abs/2510.12157)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Advanced large language models (LLMs) frequently reflect in reasoning chain-of-thoughts (CoTs), where they self-verify the correctness of current solutions and explore alternatives. However, given recent findings that LLMs detect limited errors in CoTs, how reflection contributes to empirical improvements remains unclear. To analyze this issue, in this paper, we present a minimalistic reasoning framework to support basic self-verifying reflection for small transformers without natural language, which ensures analytic clarity and reduces the cost of comprehensive experiments. Theoretically, we prove that self-verifying reflection guarantees improvements if verification errors are properly bounded. Experimentally, we show that tiny transformers, with only a few million parameters, benefit from self-verification in both training and reflective execution, reaching remarkable LLM-level performance in integer multiplication and Sudoku. Similar to LLM results, we find that reinforcement learning (RL) improves in-distribution performance and incentivizes frequent reflection for tiny transformers, yet RL mainly optimizes shallow statistical patterns without faithfully reducing verification errors. In conclusion, integrating generative transformers with discriminative verification inherently facilitates CoT reasoning, regardless of scaling and natural language.</li>
<li><strong>摘要：</strong>高级大语言模型 (LLM) 经常反映在推理思路 (CoT) 中，它们自我验证当前解决方案的正确性并探索替代方案。然而，鉴于最近的研究结果表明法学硕士在 CoT 中发现的错误有限，反思如何有助于实证改进仍不清楚。为了分析这个问题，在本文中，我们提出了一个简约的推理框架，支持没有自然语言的小型变压器的基本自验证反射，这保证了分析的清晰度并降低了综合实验的成本。从理论上讲，我们证明，如果验证错误得到适当限制，自我验证反射可以保证改进。通过实验，我们证明只有几百万个参数的微型 Transformer 受益于训练和反射执行中的自我验证，在整数乘法和数独方面达到了卓越的 LLM 级别性能。与 LLM 结果类似，我们发现强化学习 (RL) 提高了分布内性能并激励小型变压器的频繁反射，但 RL 主要优化浅层统计模式，而没有忠实地减少验证错误。总之，无论缩放比例和自然语言如何，将生成变压器与判别性验证相结合本质上促进了 CoT 推理。</li>
</ul>

<h3>Title: DPL: Spatial-Conditioned Diffusion Prototype Enhancement for One-Shot Medical Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ziyuan Gao, Philippe Morel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12159">https://arxiv.org/abs/2510.12159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12159">https://arxiv.org/pdf/2510.12159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12159]] DPL: Spatial-Conditioned Diffusion Prototype Enhancement for One-Shot Medical Segmentation(https://arxiv.org/abs/2510.12159)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>One-shot medical image segmentation faces fundamental challenges in prototype representation due to limited annotated data and significant anatomical variability across patients. Traditional prototype-based methods rely on deterministic averaging of support features, creating brittle representations that fail to capture intra-class diversity essential for robust generalization. This work introduces Diffusion Prototype Learning (DPL), a novel framework that reformulates prototype construction through diffusion-based feature space exploration. DPL models one-shot prototypes as learnable probability distributions, enabling controlled generation of diverse yet semantically coherent prototype variants from minimal labeled data. The framework operates through three core innovations: (1) a diffusion-based prototype enhancement module that transforms single support prototypes into diverse variant sets via forward-reverse diffusion processes, (2) a spatial-aware conditioning mechanism that leverages geometric properties derived from prototype feature statistics, and (3) a conservative fusion strategy that preserves prototype fidelity while maximizing representational diversity. DPL ensures training-inference consistency by using the same diffusion enhancement and fusion pipeline in both phases. This process generates enhanced prototypes that serve as the final representations for similarity calculations, while the diffusion process itself acts as a regularizer. Extensive experiments on abdominal MRI and CT datasets demonstrate significant improvements respectively, establishing new state-of-the-art performance in one-shot medical image segmentation.</li>
<li><strong>摘要：</strong>由于注释数据有限和患者之间显着的解剖变异性，一次性医学图像分割在原型表示方面面临着根本挑战。传统的基于原型的方法依赖于支持特征的确定性平均，创建了脆弱的表示，无法捕获稳健泛化所必需的类内多样性。这项工作介绍了扩散原型学习（DPL），这是一种通过基于扩散的特征空间探索重新构建原型的新颖框架。 DPL 将一次性原型建模为可学习的概率分布，从而能够从最少的标记数据中受控地生成多样化但语义一致的原型变体。该框架通过三个核心创新进行运作：（1）基于扩散的原型增强模块，通过前向-反向扩散过程将单个支持原型转换为不同的变体集；（2）利用原型特征统计数据导出的几何属性的空间感知调节机制；（3）保守的融合策略，在最大限度地提高表征多样性的同时保留原型保真度。 DPL 通过在两个阶段使用相同的扩散增强和融合管道来确保训练-推理的一致性。该过程生成增强的原型，用作相似性计算的最终表示，而扩散过程本身则充当正则化器。对腹部 MRI 和 CT 数据集的大量实验分别证明了显着的改进，在一次性医学图像分割中建立了新的最先进的性能。</li>
</ul>

<h3>Title: BEEP3D: Box-Supervised End-to-End Pseudo-Mask Generation for 3D Instance Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Youngju Yoo, Seho Kim, Changick Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12182">https://arxiv.org/abs/2510.12182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12182">https://arxiv.org/pdf/2510.12182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12182]] BEEP3D: Box-Supervised End-to-End Pseudo-Mask Generation for 3D Instance Segmentation(https://arxiv.org/abs/2510.12182)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>3D instance segmentation is crucial for understanding complex 3D environments, yet fully supervised methods require dense point-level annotations, resulting in substantial annotation costs and labor overhead. To mitigate this, box-level annotations have been explored as a weaker but more scalable form of supervision. However, box annotations inherently introduce ambiguity in overlapping regions, making accurate point-to-instance assignment challenging. Recent methods address this ambiguity by generating pseudo-masks through training a dedicated pseudo-labeler in an additional training stage. However, such two-stage pipelines often increase overall training time and complexity, hinder end-to-end optimization. To overcome these challenges, we propose BEEP3D-Box-supervised End-to-End Pseudo-mask generation for 3D instance segmentation. BEEP3D adopts a student-teacher framework, where the teacher model serves as a pseudo-labeler and is updated by the student model via an Exponential Moving Average. To better guide the teacher model to generate precise pseudo-masks, we introduce an instance center-based query refinement that enhances position query localization and leverages features near instance centers. Additionally, we design two novel losses-query consistency loss and masked feature consistency loss-to align semantic and geometric signals between predictions and pseudo-masks. Extensive experiments on ScanNetV2 and S3DIS datasets demonstrate that BEEP3D achieves competitive or superior performance compared to state-of-the-art weakly supervised methods while remaining computationally efficient.</li>
<li><strong>摘要：</strong>3D 实例分割对于理解复杂的 3D 环境至关重要，但完全监督的方法需要密集的点级注释，从而导致大量的注释成本和人工开销。为了缓解这种情况，框级注释已被探索为一种较弱但更具可扩展性的监督形式。然而，框注释本质上会在重叠区域中引入歧义，使得准确的点到实例分配具有挑战性。最近的方法通过在额外的训练阶段训练专用伪标记器来生成伪掩模来解决这种模糊性。然而，这种两阶段管道通常会增加整体训练时间和复杂性，阻碍端到端优化。为了克服这些挑战，我们提出了用于 3D 实例分割的 BEEP3D-Box 监督的端到端伪掩模生成。 BEEP3D 采用学生-教师框架，其中教师模型充当伪标签器，并由学生模型通过指数移动平均线进行更新。为了更好地指导教师模型生成精确的伪掩模，我们引入了基于实例中心的查询细化，它增强了位置查询本地化并利用实例中心附近的特征。此外，我们设计了两种新颖的损失——查询一致性损失和屏蔽特征一致性损失——以在预测和伪掩模之间对齐语义和几何信号。在 ScanNetV2 和 S3DIS 数据集上进行的大量实验表明，与最先进的弱监督方法相比，BEEP3D 实现了具有竞争力或优越的性能，同时保持了计算效率。</li>
</ul>

<h3>Title: Hierarchical Reasoning with Vision-Language Models for Incident Reports from Dashcam Videos</h3>
<ul>
<li><strong>Authors: </strong>Shingo Yokoi, Kento Sasaki, Yu Yamaguchi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12190">https://arxiv.org/abs/2510.12190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12190">https://arxiv.org/pdf/2510.12190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12190]] Hierarchical Reasoning with Vision-Language Models for Incident Reports from Dashcam Videos(https://arxiv.org/abs/2510.12190)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in end-to-end (E2E) autonomous driving have been enabled by training on diverse large-scale driving datasets, yet autonomous driving models still struggle in out-of-distribution (OOD) scenarios. The COOOL benchmark targets this gap by encouraging hazard understanding beyond closed taxonomies, and the 2COOOL challenge extends it to generating human-interpretable incident reports. We present a hierarchical reasoning framework for incident report generation from dashcam videos that integrates frame-level captioning, incident frame detection, and fine-grained reasoning within vision-language models (VLMs). We further improve factual accuracy and readability through model ensembling and a Blind A/B Scoring selection protocol. On the official 2COOOL open leaderboard, our method ranks 2nd among 29 teams and achieves the best CIDEr-D score, producing accurate and coherent incident narratives. These results indicate that hierarchical reasoning with VLMs is a promising direction for accident analysis and for broader understanding of safety-critical traffic events. The implementation and code are available at this https URL.</li>
<li><strong>摘要：</strong>端到端（E2E）自动驾驶的最新进展是通过对各种大规模驾驶数据集进行训练而实现的，但自动驾驶模型在分布式（OOD）场景中仍然举步维艰。 COOOL 基准通过鼓励超越封闭分类法的危险理解来解决这一差距，而 2COOOL 挑战则将其扩展到生成人类可解释的事件报告。我们提出了一个用于从行车记录仪视频生成事件报告的分层推理框架，该框架集成了帧级字幕、事件帧检测和视觉语言模型 (VLM) 内的细粒度推理。我们通过模型集成和盲 A/B 评分选择协议进一步提高事实准确性和可读性。在官方 2COOOL 开放排行榜上，我们的方法在 29 个团队中排名第二，并获得了最好的 CIDEr-D 分数，生成了准确且连贯的事件叙述。这些结果表明，VLM 的分层推理是事故分析和更广泛地理解安全关键交通事件的一个有前途的方向。可以在此 https URL 获取实现和代码。</li>
</ul>

<h3>Title: The Impact of Synthetic Data on Object Detection Model Performance: A Comparative Analysis with Real-World Data</h3>
<ul>
<li><strong>Authors: </strong>Muammer Bay, Timo von Marcard, Dren Fazlija</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12208">https://arxiv.org/abs/2510.12208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12208">https://arxiv.org/pdf/2510.12208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12208]] The Impact of Synthetic Data on Object Detection Model Performance: A Comparative Analysis with Real-World Data(https://arxiv.org/abs/2510.12208)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative AI, particularly in computer vision (CV), offer new opportunities to optimize workflows across industries, including logistics and manufacturing. However, many AI applications are limited by a lack of expertise and resources, which forces a reliance on general-purpose models. Success with these models often requires domain-specific data for fine-tuning, which can be costly and inefficient. Thus, using synthetic data for fine-tuning is a popular, cost-effective alternative to gathering real-world data. This work investigates the impact of synthetic data on the performance of object detection models, compared to models trained on real-world data only, specifically within the domain of warehouse logistics. To this end, we examined the impact of synthetic data generated using the NVIDIA Omniverse Replicator tool on the effectiveness of object detection models in real-world scenarios. It comprises experiments focused on pallet detection in a warehouse setting, utilizing both real and various synthetic dataset generation strategies. Our findings provide valuable insights into the practical applications of synthetic image data in computer vision, suggesting that a balanced integration of synthetic and real data can lead to robust and efficient object detection models.</li>
<li><strong>摘要：</strong>生成式人工智能（尤其是计算机视觉 (CV)）领域的最新进展为优化跨行业（包括物流和制造）的工作流程提供了新的机会。然而，许多人工智能应用因缺乏专业知识和资源而受到限制，这迫使人们依赖通用模型。这些模型的成功通常需要特定领域的数据进行微调，这可能成本高昂且效率低下。因此，使用合成数据进行微调是收集真实世界数据的一种流行且经济高效的替代方案。这项工作研究了合成数据对对象检测模型性能的影响，与仅根据真实世界数据训练的模型进行比较，特别是在仓库物流领域。为此，我们研究了使用 NVIDIA Omniverse Replicator 工具生成的合成数据对现实场景中对象检测模型有效性的影响。它包括专注于仓库环境中托盘检测的实验，利用真实和各种合成数据集生成策略。我们的研究结果为合成图像数据在计算机视觉中的实际应用提供了宝贵的见解，表明合成数据和真实数据的平衡集成可以产生稳健且高效的目标检测模型。</li>
</ul>

<h3>Title: Hierarchical Koopman Diffusion: Fast Generation with Interpretable Diffusion Trajectory</h3>
<ul>
<li><strong>Authors: </strong>Hanru Bai, Weiyang Ding, Difan Zou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12220">https://arxiv.org/abs/2510.12220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12220">https://arxiv.org/pdf/2510.12220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12220]] Hierarchical Koopman Diffusion: Fast Generation with Interpretable Diffusion Trajectory(https://arxiv.org/abs/2510.12220)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved impressive success in high-fidelity image generation but suffer from slow sampling due to their inherently iterative denoising process. While recent one-step methods accelerate inference by learning direct noise-to-image mappings, they sacrifice the interpretability and fine-grained control intrinsic to diffusion dynamics, key advantages that enable applications like editable generation. To resolve this dichotomy, we introduce \textbf{Hierarchical Koopman Diffusion}, a novel framework that achieves both one-step sampling and interpretable generative trajectories. Grounded in Koopman operator theory, our method lifts the nonlinear diffusion dynamics into a latent space where evolution is governed by globally linear operators, enabling closed-form trajectory solutions. This formulation not only eliminates iterative sampling but also provides full access to intermediate states, allowing manual intervention during generation. To model the multi-scale nature of images, we design a hierarchical architecture that disentangles generative dynamics across spatial resolutions via scale-specific Koopman subspaces, capturing coarse-to-fine details systematically. We empirically show that the Hierarchical Koopman Diffusion not only achieves competitive one-step generation performance but also provides a principled mechanism for interpreting and manipulating the generative process through spectral analysis. Our framework bridges the gap between fast sampling and interpretability in diffusion models, paving the way for explainable image synthesis in generative modeling.</li>
<li><strong>摘要：</strong>扩散模型在高保真图像生成方面取得了令人瞩目的成功，但由于其固有的迭代去噪过程，采样速度较慢。虽然最近的一步法通过学习直接噪声到图像的映射来加速推理，但它们牺牲了扩散动力学固有的可解释性和细粒度控制，而这些是实现可编辑生成等应用的关键优势。为了解决这种二分法，我们引入了 \textbf{Hierarchical Koopman Diffusion}，这是一种新颖的框架，可以实现一步采样和可解释的生成轨迹。基于库普曼算子理论，我们的方法将非线性扩散动力学提升到一个潜在空间，其中演化由全局线性算子控制，从而实现封闭形式的轨迹解决方案。这种公式不仅消除了迭代采样，而且还提供了对中间状态的完全访问，允许在生成过程中进行手动干预。为了模拟图像的多尺度性质，我们设计了一种分层架构，通过特定尺度的库普曼子空间来解开跨空间分辨率的生成动力学，系统地捕获从粗到细的细节。我们凭经验证明，分层库普曼扩散不仅实现了有竞争力的一步生成性能，而且还提供了一种通过谱分析解释和操纵生成过程的原理机制。我们的框架弥合了扩散模型中快速采样和可解释性之间的差距，为生成建模中可解释的图像合成铺平了道路。</li>
</ul>

<h3>Title: BIGFix: Bidirectional Image Generation with Token Fixing</h3>
<ul>
<li><strong>Authors: </strong>Victor Besnier, David Hurych, Andrei Bursuc, Eduardo Valle</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12231">https://arxiv.org/abs/2510.12231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12231">https://arxiv.org/pdf/2510.12231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12231]] BIGFix: Bidirectional Image Generation with Token Fixing(https://arxiv.org/abs/2510.12231)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in image and video generation have raised significant interest from both academia and industry. A key challenge in this field is improving inference efficiency, as model size and the number of inference steps directly impact the commercial viability of generative models while also posing fundamental scientific challenges. A promising direction involves combining auto-regressive sequential token modeling with multi-token prediction per step, reducing inference time by up to an order of magnitude. However, predicting multiple tokens in parallel can introduce structural inconsistencies due to token incompatibilities, as capturing complex joint dependencies during training remains challenging. Traditionally, once tokens are sampled, there is no mechanism to backtrack and refine erroneous predictions. We propose a method for self-correcting image generation by iteratively refining sampled tokens. We achieve this with a novel training scheme that injects random tokens in the context, improving robustness and enabling token fixing during sampling. Our method preserves the efficiency benefits of parallel token prediction while significantly enhancing generation quality. We evaluate our approach on image generation using the ImageNet-256 and CIFAR-10 datasets, as well as on video generation with UCF-101 and NuScenes, demonstrating substantial improvements across both modalities.</li>
<li><strong>摘要：</strong>图像和视频生成的最新进展引起了学术界和工业界的极大兴趣。该领域的一个关键挑战是提高推理效率，因为模型大小和推理步骤数量直接影响生成模型的商业可行性，同时也带来了基本的科学挑战。一个有前途的方向是将自回归顺序令牌建模与每步多令牌预测相结合，将推理时间缩短一个数量级。然而，并行预测多个令牌可能会由于令牌不兼容而引入结构不一致，因为在训练期间捕获复杂的联合依赖关系仍然具有挑战性。传统上，一旦对令牌进行采样，就没有机制可以回溯和完善错误的预测。我们提出了一种通过迭代细化采样标记来自动校正图像生成的方法。我们通过一种新颖的训练方案来实现这一目标，该方案在上下文中注入随机令牌，提高鲁棒性并在采样期间实现令牌修复。我们的方法保留了并行令牌预测的效率优势，同时显着提高了生成质量。我们使用 ImageNet-256 和 CIFAR-10 数据集评估我们的图像生成方法，以及使用 UCF-101 和 NuScenes 的视频生成方法，展示了两种模式的重大改进。</li>
</ul>

<h3>Title: Diffusion Models for Reinforcement Learning: Foundations, Taxonomy, and Development</h3>
<ul>
<li><strong>Authors: </strong>Changfu Xu, Jianxiong Guo, Yuzhu Liang, Haiyang Huang, Haodong Zou, Xi Zheng, Shui Yu, Xiaowen Chu, Jiannong Cao, Tian Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12253">https://arxiv.org/abs/2510.12253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12253">https://arxiv.org/pdf/2510.12253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12253]] Diffusion Models for Reinforcement Learning: Foundations, Taxonomy, and Development(https://arxiv.org/abs/2510.12253)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion Models (DMs), as a leading class of generative models, offer key advantages for reinforcement learning (RL), including multi-modal expressiveness, stable training, and trajectory-level planning. This survey delivers a comprehensive and up-to-date synthesis of diffusion-based RL. We first provide an overview of RL, highlighting its challenges, and then introduce the fundamental concepts of DMs, investigating how they are integrated into RL frameworks to address key challenges in this research field. We establish a dual-axis taxonomy that organizes the field along two orthogonal dimensions: a function-oriented taxonomy that clarifies the roles DMs play within the RL pipeline, and a technique-oriented taxonomy that situates implementations across online versus offline learning regimes. We also provide a comprehensive examination of this progression from single-agent to multi-agent domains, thereby forming several frameworks for DM-RL integration and highlighting their practical utility. Furthermore, we outline several categories of successful applications of diffusion-based RL across diverse domains, discuss open research issues of current methodologies, and highlight key directions for future research to advance the field. Finally, we summarize the survey to identify promising future development directions. We are actively maintaining a GitHub repository (this https URL) for papers and other related resources to apply DMs for RL.</li>
<li><strong>摘要：</strong>扩散模型 (DM) 作为领先的生成模型，为强化学习 (RL) 提供了关键优势，包括多模态表达、稳定训练和轨迹级规划。这项调查提供了基于扩散的强化学习的全面且最新的综合。我们首先概述 RL，强调其挑战，然后介绍 DM 的基本概念，研究如何将它们集成到 RL 框架中以解决该研究领域的关键挑战。我们建立了一个双轴分类法，沿着两个正交维度组织该领域：一个以功能为导向的分类法，阐明了 DM 在 RL 管道中所扮演的角色，以及一个以技术为导向的分类法，该分类法定位了在线与离线学习体系的实现。我们还对从单智能体到多智能体领域的进展进行了全面的检查，从而形成了 DM-RL 集成的几个框架，并强调了它们的实用性。此外，我们概述了基于扩散的强化学习在不同领域的成功应用的几类，讨论了当前方法论的开放研究问题，并强调了未来研究推进该领域的关键方向。最后，我们总结了调查，以确定有希望的未来发展方向。我们正在积极维护一个 GitHub 存储库（此 https URL），用于存储论文和其他相关资源，以将 DM 应用于 RL。</li>
</ul>

<h3>Title: Multi-Action Self-Improvement for Neural Combinatorial Optimization</h3>
<ul>
<li><strong>Authors: </strong>Laurin Luttmann, Lin Xie</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12273">https://arxiv.org/abs/2510.12273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12273">https://arxiv.org/pdf/2510.12273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12273]] Multi-Action Self-Improvement for Neural Combinatorial Optimization(https://arxiv.org/abs/2510.12273)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Self-improvement has emerged as a state-of-the-art paradigm in Neural Combinatorial Optimization (NCO), where models iteratively refine their policies by generating and imitating high-quality solutions. Despite strong empirical performance, existing methods face key limitations. Training is computationally expensive, as policy updates require sampling numerous candidate solutions per instance to extract a single expert trajectory. More fundamentally, these approaches fail to exploit the structure of combinatorial problems involving the coordination of multiple agents, such as vehicles in min-max routing or machines in scheduling. By supervising on single-action trajectories, they fail to exploit agent-permutation symmetries, where distinct sequences of actions yield identical solutions, hindering generalization and the ability to learn coordinated behavior. We address these challenges by extending self-improvement to operate over joint multi-agent actions. Our model architecture predicts complete agent-task assignments jointly at each decision step. To explicitly leverage symmetries, we employ a set-prediction loss, which supervises the policy on multiple expert assignments for any given state. This approach enhances sample efficiency and the model's ability to learn coordinated behavior. Furthermore, by generating multi-agent actions in parallel, it drastically accelerates the solution generation phase of the self-improvement loop. Empirically, we validate our method on several combinatorial problems, demonstrating consistent improvements in the quality of the final solution and a reduced generation latency compared to standard self-improvement.</li>
<li><strong>摘要：</strong>自我完善已成为神经组合优化 (NCO) 中最先进的范例，其中模型通过生成和模仿高质量的解决方案来迭代地完善其策略。尽管实证表现强劲，但现有方法仍面临关键局限性。训练的计算成本很高，因为策略更新需要对每个实例的大量候选解决方案进行采样以提取单个专家轨迹。更根本的是，这些方法未能利用涉及多个代理协调的组合问题的结构，例如最小-最大路由中的车辆或调度中的机器。通过监督单动作轨迹，他们无法利用主体排列对称性，即不同的动作序列产生相同的解决方案，从而阻碍了泛化和学习协调行为的能力。我们通过将自我改进扩展到联合多主体行动来应对这些挑战。我们的模型架构在每个决策步骤共同预测完整的代理任务分配。为了明确利用对称性，我们采用了集合预测损失，它监督任何给定状态的多个专家分配的策略。这种方法提高了样本效率和模型学习协调行为的能力。此外，通过并行生成多智能体操作，它大大加速了自我改进循环的解决方案生成阶段。根据经验，我们在几个组合问题上验证了我们的方法，证明了最终解决方案质量的持续改进以及与标准自我改进相比生成延迟的减少。</li>
</ul>

<h3>Title: Hybrid Gaussian Splatting for Novel Urban View Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Omran, Farhad Zanjani, Davide Abati, Jens Petersen, Amirhossein Habibian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12308">https://arxiv.org/abs/2510.12308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12308">https://arxiv.org/pdf/2510.12308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12308]] Hybrid Gaussian Splatting for Novel Urban View Synthesis(https://arxiv.org/abs/2510.12308)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>This paper describes the Qualcomm AI Research solution to the RealADSim-NVS challenge, hosted at the RealADSim Workshop at ICCV 2025. The challenge concerns novel view synthesis in street scenes, and participants are required to generate, starting from car-centric frames captured during some training traversals, renders of the same urban environment as viewed from a different traversal (e.g. different street lane or car direction). Our solution is inspired by hybrid methods in scene generation and generative simulators merging gaussian splatting and diffusion models, and it is composed of two stages: First, we fit a 3D reconstruction of the scene and render novel views as seen from the target cameras. Then, we enhance the resulting frames with a dedicated single-step diffusion model. We discuss specific choices made in the initialization of gaussian primitives as well as the finetuning of the enhancer model and its training data curation. We report the performance of our model design and we ablate its components in terms of novel view quality as measured by PSNR, SSIM and LPIPS. On the public leaderboard reporting test results, our proposal reaches an aggregated score of 0.432, achieving the second place overall.</li>
<li><strong>摘要：</strong>本文描述了 Qualcomm AI Research 针对 RealADSim-NVS 挑战赛的解决方案，该挑战赛在 ICCV 2025 的 RealADSim 研讨会上主办。该挑战赛涉及街道场景中的新颖视图合成，参与者需要从一些训练遍历期间捕获的以汽车为中心的帧开始，生成从不同遍历（例如不同街道车道或汽车方向）查看的相同城市环境的渲染。我们的解决方案受到场景生成和生成模拟器中融合高斯泼溅和扩散模型的混合方法的启发，它由两个阶段组成：首先，我们拟合场景的 3D 重建并渲染从目标摄像机看到的新颖视图。然后，我们使用专用的单步扩散模型增强生成的帧。我们讨论在高斯原语初始化以及增强器模型及其训练数据管理的微调中做出的具体选择。我们报告模型设计的性能，并根据 PSNR、SSIM 和 LPIPS 测量的新颖视图质量来消除其组件。在公开排行榜报告测试结果中，我们的提案的总分达到了 0.432，总体排名第二。</li>
</ul>

<h3>Title: Traveling Salesman-Based Token Ordering Improves Stability in Homomorphically Encrypted Language Models</h3>
<ul>
<li><strong>Authors: </strong>Donghwan Rho, Sieun Seo, Hyewon Sung, Chohong Min, Ernest K. Ryu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12343">https://arxiv.org/abs/2510.12343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12343">https://arxiv.org/pdf/2510.12343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12343]] Traveling Salesman-Based Token Ordering Improves Stability in Homomorphically Encrypted Language Models(https://arxiv.org/abs/2510.12343)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>As users increasingly interact with large language models (LLMs) using private information, secure and encrypted communication becomes essential. Homomorphic encryption (HE) provides a principled solution by enabling computation directly on encrypted data. Although prior work has explored aspects of running LLMs under HE, the challenge of text generation, particularly next-token prediction, has received limited attention and remains a key obstacle to practical encrypted interaction. In this work, we propose a TSP-based token reordering strategy to address the difficulties of encrypted text generation, together with a post-processing step that further reduces approximation error. Theoretical analysis and experimental results demonstrate that our method prevents collapse, improves coherence in generated text, and preserves data privacy throughout. Overall, our contributions advance the feasibility of practical and privacy-preserving LLM inference.</li>
<li><strong>摘要：</strong>随着用户越来越多地使用私人信息与大型语言模型 (LLM) 进行交互，安全和加密的通信变得至关重要。同态加密 (HE) 通过直接对加密数据进行计算提供了原则性的解决方案。尽管之前的工作已经探索了在 HE 下运行法学硕士的各个方面，但文本生成的挑战，特别是下一个令牌预测，受到的关注有限，并且仍然是实际加密交互的主要障碍。在这项工作中，我们提出了一种基于 TSP 的令牌重新排序策略来解决加密文本生成的困难，以及进一步减少近似误差的后处理步骤。理论分析和实验结果表明，我们的方法可以防止崩溃，提高生成文本的连贯性，并始终保护数据隐私。总的来说，我们的贡献提高了实用且保护隐私的法学硕士推理的可行性。</li>
</ul>

<h3>Title: Continuous Uniqueness and Novelty Metrics for Generative Modeling of Inorganic Crystals</h3>
<ul>
<li><strong>Authors: </strong>Masahiro Negishi, Hyunsoo Park, Kinga O. Mastej, Aron Walsh</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12405">https://arxiv.org/abs/2510.12405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12405">https://arxiv.org/pdf/2510.12405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12405]] Continuous Uniqueness and Novelty Metrics for Generative Modeling of Inorganic Crystals(https://arxiv.org/abs/2510.12405)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>To address pressing scientific challenges such as climate change, increasingly sophisticated generative artificial intelligence models are being developed that can efficiently sample the large chemical space of possible functional materials. These models can quickly sample new chemical compositions paired with crystal structures. They are typically evaluated using uniqueness and novelty metrics, which depend on a chosen crystal distance function. However, the most prevalent distance function has four limitations: it fails to quantify the degree of similarity between compounds, cannot distinguish compositional difference and structural difference, lacks Lipschitz continuity against shifts in atomic coordinates, and results in a uniqueness metric that is not invariant against the permutation of generated samples. In this work, we propose using two continuous distance functions to evaluate uniqueness and novelty, which theoretically overcome these limitations. Our experiments show that these distances reveal insights missed by traditional distance functions, providing a more reliable basis for evaluating and comparing generative models for inorganic crystals.</li>
<li><strong>摘要：</strong>为了解决气候变化等紧迫的科学挑战，人们正在开发日益复杂的生成人工智能模型，该模型可以有效地对可能的功能材料的巨大化学空间进行采样。这些模型可以快速采样与晶体结构配对的新化学成分。通常使用独特性和新颖性指标来评估它们，这取决于所选的晶体距离函数。然而，最普遍的距离函数有四个局限性：它无法量化化合物之间的相似程度，无法区分成分差异和结构差异，缺乏针对原子坐标移动的利普希茨连续性，以及导致唯一性度量对于生成样本的排列不是不变的。在这项工作中，我们建议使用两个连续距离函数来评估独特性和新颖性，这在理论上克服了这些限制。我们的实验表明，这些距离揭示了传统距离函数所遗漏的见解，为评估和比较无机晶体的生成模型提供了更可靠的基础。</li>
</ul>

<h3>Title: Low-Field Magnetic Resonance Image Quality Enhancement using a Conditional Flow Matching Model</h3>
<ul>
<li><strong>Authors: </strong>Huu Tien Nguyen, Ahmed Karam Eldaly</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12408">https://arxiv.org/abs/2510.12408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12408">https://arxiv.org/pdf/2510.12408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12408]] Low-Field Magnetic Resonance Image Quality Enhancement using a Conditional Flow Matching Model(https://arxiv.org/abs/2510.12408)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel framework for image quality transfer based on conditional flow matching (CFM). Unlike conventional generative models that rely on iterative sampling or adversarial objectives, CFM learns a continuous flow between a noise distribution and target data distributions through the direct regression of an optimal velocity field. We evaluate this approach in the context of low-field magnetic resonance imaging (LF-MRI), a rapidly emerging modality that offers affordable and portable scanning but suffers from inherently low signal-to-noise ratio and reduced diagnostic quality. Our framework is designed to reconstruct high-field-like MR images from their corresponding low-field inputs, thereby bridging the quality gap without requiring expensive infrastructure. Experiments demonstrate that CFM not only achieves state-of-the-art performance, but also generalizes robustly to both in-distribution and out-of-distribution data. Importantly, it does so while utilizing significantly fewer parameters than competing deep learning methods. These results underline the potential of CFM as a powerful and scalable tool for MRI reconstruction, particularly in resource-limited clinical environments.</li>
<li><strong>摘要：</strong>本文介绍了一种基于条件流匹配（CFM）的图像质量传输新颖框架。与依赖迭代采样或对抗性目标的传统生成模型不同，CFM 通过最佳速度场的直接回归来学习噪声分布和目标数据分布之间的连续流。我们在低场磁共振成像（LF-MRI）的背景下评估这种方法，这是一种快速新兴的模式，可提供价格实惠且便携式的扫描，但存在固有的低信噪比和诊断质量下降的问题。我们的框架旨在从相应的低场输入重建类似高场的 MR 图像，从而弥补质量差距，而不需要昂贵的基础设施。实验表明，CFM 不仅实现了最先进的性能，而且还可以稳健地推广到分布内和分布外数据。重要的是，它使用的参数比竞争深度学习方法少得多。这些结果强调了 CFM 作为 MRI 重建的强大且可扩展工具的潜力，特别是在资源有限的临床环境中。</li>
</ul>

<h3>Title: A Review of Longitudinal Radiology Report Generation: Dataset Composition, Methods, and Performance Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Shaoyang Zhou, Yingshu Li, Yunyi Liu, Lingqiao Liu, Lei Wang, Luping Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12444">https://arxiv.org/abs/2510.12444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12444">https://arxiv.org/pdf/2510.12444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12444]] A Review of Longitudinal Radiology Report Generation: Dataset Composition, Methods, and Performance Evaluation(https://arxiv.org/abs/2510.12444)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Chest Xray imaging is a widely used diagnostic tool in modern medicine, and its high utilization creates substantial workloads for radiologists. To alleviate this burden, vision language models are increasingly applied to automate Chest Xray radiology report generation (CXRRRG), aiming for clinically accurate descriptions while reducing manual effort. Conventional approaches, however, typically rely on single images, failing to capture the longitudinal context necessary for producing clinically faithful comparison statements. Recently, growing attention has been directed toward incorporating longitudinal data into CXR RRG, enabling models to leverage historical studies in ways that mirror radiologists diagnostic workflows. Nevertheless, existing surveys primarily address single image CXRRRG and offer limited guidance for longitudinal settings, leaving researchers without a systematic framework for model design. To address this gap, this survey provides the first comprehensive review of longitudinal radiology report generation (LRRG). Specifically, we examine dataset construction strategies, report generation architectures alongside longitudinally tailored designs, and evaluation protocols encompassing both longitudinal specific measures and widely used benchmarks. We further summarize LRRG methods performance, alongside analyses of different ablation studies, which collectively highlight the critical role of longitudinal information and architectural design choices in improving model performance. Finally, we summarize five major limitations of current research and outline promising directions for future development, aiming to lay a foundation for advancing this emerging field.</li>
<li><strong>摘要：</strong>胸部X射线成像是现代医学中广泛使用的诊断工具，其高利用率给放射科医生带来了大量的工作量。为了减轻这种负担，视觉语言模型越来越多地应用于自动化胸部 X 射线放射学报告生成 (CXRRRG)，旨在实现临床准确的描述，同时减少手动工作。然而，传统方法通常依赖于单个图像，无法捕获产生临床忠实比较陈述所需的纵向背景。最近，越来越多的注意力集中在将纵向数据纳入 CXR RRG 中，使模型能够以反映放射科医生诊断工作流程的方式利用历史研究。然而，现有的调查主要针对单图像 CXRRRG，并为纵向设置提供有限的指导，使研究人员没有系统的模型设计框架。为了解决这一差距，本次调查首次对纵向放射学报告生成 (LRRG) 进行了全面审查。具体来说，我们检查数据集构建策略、报告生成架构以及纵向定制设计，以及包含纵向具体措施和广泛使用的基准的评估协议。我们进一步总结了 LRRG 方法的性能，同时分析了不同的消融研究，共同强调了纵向信息和架构设计选择在提高模型性能方面的关键作用。最后，我们总结了当前研究的五个主要局限性，并概述了未来发展的有前景的方向，旨在为推进这一新兴领域奠定基础。</li>
</ul>

<h3>Title: Time-Correlated Video Bridge Matching</h3>
<ul>
<li><strong>Authors: </strong>Viacheslav Vasilev, Arseny Ivanov, Nikita Gushchin, Maria Kovaleva, Alexander Korotin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12453">https://arxiv.org/abs/2510.12453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12453">https://arxiv.org/pdf/2510.12453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12453]] Time-Correlated Video Bridge Matching(https://arxiv.org/abs/2510.12453)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generation</a></li>
<li><strong>Abstract: </strong>Diffusion models excel in noise-to-data generation tasks, providing a mapping from a Gaussian distribution to a more complex data distribution. However they struggle to model translations between complex distributions, limiting their effectiveness in data-to-data tasks. While Bridge Matching (BM) models address this by finding the translation between data distributions, their application to time-correlated data sequences remains unexplored. This is a critical limitation for video generation and manipulation tasks, where maintaining temporal coherence is particularly important. To address this gap, we propose Time-Correlated Video Bridge Matching (TCVBM), a framework that extends BM to time-correlated data sequences in the video domain. TCVBM explicitly models inter-sequence dependencies within the diffusion bridge, directly incorporating temporal correlations into the sampling process. We compare our approach to classical methods based on bridge matching and diffusion models for three video-related tasks: frame interpolation, image-to-video generation, and video super-resolution. TCVBM achieves superior performance across multiple quantitative metrics, demonstrating enhanced generation quality and reconstruction fidelity.</li>
<li><strong>摘要：</strong>扩散模型在噪声到数据生成任务中表现出色，提供从高斯分布到更复杂的数据分布的映射。然而，他们很难对复杂分布之间的转换进行建模，从而限制了它们在数据到数据任务中的有效性。虽然桥匹配（BM）模型通过查找数据分布之间的转换来解决这个问题，但它们在时间相关数据序列中的应用仍未得到探索。这是视频生成和操作任务的一个关键限制，其中保持时间一致性尤为重要。为了解决这个问题，我们提出了时间相关视频桥接匹配（TCVBM），这是一个将 BM 扩展到视频域中时间相关数据序列的框架。 TCVBM 对扩散桥内的序列间依赖性进行显式建模，直接将时间相关性纳入采样过程。我们将我们的方法与基于桥匹配和扩散模型的经典方法进行比较，以完成三个视频相关任务：帧插值、图像到视频生成和视频超分辨率。 TCVBM 在多个定量指标上实现了卓越的性能，展示了增强的生成质量和重建保真度。</li>
</ul>

<h3>Title: MS-GAGA: Metric-Selective Guided Adversarial Generation Attack</h3>
<ul>
<li><strong>Authors: </strong>Dion J. X. Ho, Gabriel Lee Jun Rong, Niharika Shrivastava, Harshavardhan Abichandani, Pai Chet Ng, Xiaoxiao Miao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12468">https://arxiv.org/abs/2510.12468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12468">https://arxiv.org/pdf/2510.12468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12468]] MS-GAGA: Metric-Selective Guided Adversarial Generation Attack(https://arxiv.org/abs/2510.12468)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present MS-GAGA (Metric-Selective Guided Adversarial Generation Attack), a two-stage framework for crafting transferable and visually imperceptible adversarial examples against deepfake detectors in black-box settings. In Stage 1, a dual-stream attack module generates adversarial candidates: MNTD-PGD applies enhanced gradient calculations optimized for small perturbation budgets, while SG-PGD focuses perturbations on visually salient regions. This complementary design expands the adversarial search space and improves transferability across unseen models. In Stage 2, a metric-aware selection module evaluates candidates based on both their success against black-box models and their structural similarity (SSIM) to the original image. By jointly optimizing transferability and imperceptibility, MS-GAGA achieves up to 27% higher misclassification rates on unseen detectors compared to state-of-the-art attacks.</li>
<li><strong>摘要：</strong>我们提出了 MS-GAGA（度量选择性引导对抗生成攻击），这是一个两阶段框架，用于在黑盒设置中针对 Deepfake 探测器制作可转移且视觉上难以察觉的对抗示例。在第一阶段，双流攻击模块生成对抗性候选者：MNTD-PGD 应用针对小扰动预算优化的增强梯度计算，而 SG-PGD 将扰动集中在视觉显着区域。这种互补的设计扩大了对抗性搜索空间，并提高了跨未知模型的可转移性。在第 2 阶段，度量感知选择模块根据候选者在黑盒模型中的成功情况以及与原始图像的结构相似性 (SSIM) 来评估候选者。通过联合优化可转移性和不可感知性，与最先进的攻击相比，MS-GAGA 在看不见的检测器上实现了高达 27% 的错误分类率。</li>
</ul>

<h3>Title: BSGS: Bi-stage 3D Gaussian Splatting for Camera Motion Deblurring</h3>
<ul>
<li><strong>Authors: </strong>An Zhao, Piaopiao Yu, Zhe Zhu, Mingqiang Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12493">https://arxiv.org/abs/2510.12493</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12493">https://arxiv.org/pdf/2510.12493</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12493]] BSGS: Bi-stage 3D Gaussian Splatting for Camera Motion Deblurring(https://arxiv.org/abs/2510.12493)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>3D Gaussian Splatting has exhibited remarkable capabilities in 3D scene this http URL, reconstructing high-quality 3D scenes from motion-blurred images caused by camera motion poses a significant this http URL performance of existing 3DGS-based deblurring methods are limited due to their inherent mechanisms, such as extreme dependence on the accuracy of camera poses and inability to effectively control erroneous Gaussian primitives densification caused by motion this http URL solve these problems, we introduce a novel framework, Bi-Stage 3D Gaussian Splatting, to accurately reconstruct 3D scenes from motion-blurred this http URL contains two stages. First, Camera Pose Refinement roughly optimizes camera poses to reduce motion-induced distortions. Second, with fixed rough camera poses, Global RigidTransformation further corrects motion-induced blur this http URL alleviate multi-subframe gradient conflicts, we propose a subframe gradient aggregation strategy to optimize both this http URL, a space-time bi-stage optimization strategy is introduced to dynamically adjust primitive densification thresholds and prevent premature noisy Gaussian generation in blurred regions. Comprehensive experiments verify the effectiveness of our proposed deblurring method and show its superiority over the state of the arts.</li>
<li><strong>摘要：</strong>3D Gaussian Splatting 在 3D 场景中展现了非凡的能力，从相机运动引起的运动模糊图像中重建高质量的 3D 场景具有显着的意义。现有的基于 3DGS 的去模糊方法的性能由于其固有机制而受到限制，例如极度依赖相机姿态的准确性以及无法有效控制错误的高斯基元致密化 为了解决这些问题，我们引入了一种新颖的框架，Bi-Stage 3D Gaussian Splatting，可以从包含两个阶段的运动模糊的 http URL 中准确地重建 3D 场景。首先，相机姿势细化粗略地优化相机姿势，以减少运动引起的扭曲。其次，在固定的粗糙相机姿态下，全局刚性变换进一步纠正运动引起的模糊，该http URL减轻了多子帧梯度冲突，我们提出了一种子帧梯度聚合策略来优化该http URL，引入时空双阶段优化策略来动态调整原始致密化阈值并防止模糊区域中过早产生噪声高斯。综合实验验证了我们提出的去模糊方法的有效性，并显示了其相对于现有技术的优越性。</li>
</ul>

<h3>Title: Mitigating the Noise Shift for Denoising Generative Models via Noise Awareness Guidance</h3>
<ul>
<li><strong>Authors: </strong>Jincheng Zhong, Boyuan Jiang, Xin Tao, Pengfei Wan, Kun Gai, Mingsheng Long</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12497">https://arxiv.org/abs/2510.12497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12497">https://arxiv.org/pdf/2510.12497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12497]] Mitigating the Noise Shift for Denoising Generative Models via Noise Awareness Guidance(https://arxiv.org/abs/2510.12497)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Existing denoising generative models rely on solving discretized reverse-time SDEs or ODEs. In this paper, we identify a long-overlooked yet pervasive issue in this family of models: a misalignment between the pre-defined noise level and the actual noise level encoded in intermediate states during sampling. We refer to this misalignment as noise shift. Through empirical analysis, we demonstrate that noise shift is widespread in modern diffusion models and exhibits a systematic bias, leading to sub-optimal generation due to both out-of-distribution generalization and inaccurate denoising updates. To address this problem, we propose Noise Awareness Guidance (NAG), a simple yet effective correction method that explicitly steers sampling trajectories to remain consistent with the pre-defined noise schedule. We further introduce a classifier-free variant of NAG, which jointly trains a noise-conditional and a noise-unconditional model via noise-condition dropout, thereby eliminating the need for external classifiers. Extensive experiments, including ImageNet generation and various supervised fine-tuning tasks, show that NAG consistently mitigates noise shift and substantially improves the generation quality of mainstream diffusion models.</li>
<li><strong>摘要：</strong>现有的去噪生成模型依赖于求解离散逆时 SDE 或 ODE。在本文中，我们发现了该模型系列中一个长期被忽视但普遍存在的问题：预定义的噪声水平与采样期间以中间状态编码的实际噪声水平之间的不一致。我们将这种错位称为噪声偏移。通过实证分析，我们证明噪声偏移在现代扩散模型中普遍存在，并表现出系统偏差，导致由于分布外泛化和不准确的去噪更新而导致次优生成。为了解决这个问题，我们提出了噪声意识指导（NAG），这是一种简单而有效的校正方法，可以明确引导采样轨迹以与预定义的噪声时间表保持一致。我们进一步引入了 NAG 的无分类器变体，它通过噪声条件 dropout 联合训练噪声条件模型和噪声无条件模型，从而消除了对外部分类器的需要。包括 ImageNet 生成和各种监督微调任务在内的大量实验表明，NAG 始终如一地减轻了噪声偏移，并显着提高了主流扩散模型的生成质量。</li>
</ul>

<h3>Title: Unconditional Human Motion and Shape Generation via Balanced Score-Based Diffusion</h3>
<ul>
<li><strong>Authors: </strong>David Björkstrand, Tiesheng Wang, Lars Bretzner, Josephine Sullivan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12537">https://arxiv.org/abs/2510.12537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12537">https://arxiv.org/pdf/2510.12537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12537]] Unconditional Human Motion and Shape Generation via Balanced Score-Based Diffusion(https://arxiv.org/abs/2510.12537)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent work has explored a range of model families for human motion generation, including Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and diffusion-based models. Despite their differences, many methods rely on over-parameterized input features and auxiliary losses to improve empirical results. These strategies should not be strictly necessary for diffusion models to match the human motion distribution. We show that on par with state-of-the-art results in unconditional human motion generation are achievable with a score-based diffusion model using only careful feature-space normalization and analytically derived weightings for the standard L2 score-matching loss, while generating both motion and shape directly, thereby avoiding slow post hoc shape recovery from joints. We build the method step by step, with a clear theoretical motivation for each component, and provide targeted ablations demonstrating the effectiveness of each proposed addition in isolation.</li>
<li><strong>摘要：</strong>最近的工作探索了一系列用于人体运动生成的模型系列，包括变分自动编码器（VAE）、生成对抗网络（GAN）和基于扩散的模型。尽管存在差异，但许多方法依赖于过度参数化的输入特征和辅助损失来改善经验结果。这些策略对于扩散模型匹配人体运动分布来说并不是严格必要的。我们表明，通过基于分数的扩散模型，仅使用仔细的特征空间归一化和标准 L2 分数匹配损失的分析得出的权重，可以实现无条件人体运动生成的最先进结果，同时直接生成运动和形状，从而避免从关节缓慢的事后形状恢复。我们逐步构建该方法，为每个组件提供明确的理论动机，并提供有针对性的消融，证明每个提议的单独添加的有效性。</li>
</ul>

<h3>Title: Learning Human Motion with Temporally Conditional Mamba</h3>
<ul>
<li><strong>Authors: </strong>Quang Nguyen, Tri Le, Baoru Huang, Minh Nhat Vu, Ngan Le, Thieu Vo, Anh Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12573">https://arxiv.org/abs/2510.12573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12573">https://arxiv.org/pdf/2510.12573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12573]] Learning Human Motion with Temporally Conditional Mamba(https://arxiv.org/abs/2510.12573)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Learning human motion based on a time-dependent input signal presents a challenging yet impactful task with various applications. The goal of this task is to generate or estimate human movement that consistently reflects the temporal patterns of conditioning inputs. Existing methods typically rely on cross-attention mechanisms to fuse the condition with motion. However, this approach primarily captures global interactions and struggles to maintain step-by-step temporal alignment. To address this limitation, we introduce Temporally Conditional Mamba, a new mamba-based model for human motion generation. Our approach integrates conditional information into the recurrent dynamics of the Mamba block, enabling better temporally aligned motion. To validate the effectiveness of our method, we evaluate it on a variety of human motion tasks. Extensive experiments demonstrate that our model significantly improves temporal alignment, motion realism, and condition consistency over state-of-the-art approaches. Our project page is available at this https URL.</li>
<li><strong>摘要：</strong>基于时间相关的输入信号学习人体运动对于各种应用来说是一项具有挑战性但有影响力的任务。此任务的目标是生成或估计始终反映调节输入的时间模式的人体运动。现有方法通常依赖交叉注意机制将条件与运动融合。然而，这种方法主要捕获全局交互，并且难以保持逐步的时间对齐。为了解决这个限制，我们引入了临时条件 Mamba，这是一种基于曼巴的新人体运动生成模型。我们的方法将条件信息集成到曼巴块的循环动态中，从而实现更好的时间对齐运动。为了验证我们方法的有效性，我们在各种人体运动任务上对其进行了评估。大量的实验表明，与最先进的方法相比，我们的模型显着改善了时间对齐、运动真实性和条件一致性。我们的项目页面可通过此 https URL 访问。</li>
</ul>

<h3>Title: LayerSync: Self-aligning Intermediate Layers</h3>
<ul>
<li><strong>Authors: </strong>Yasaman Haghighi, Bastien van Delft, Mariam Hassan, Alexandre Alahi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12581">https://arxiv.org/abs/2510.12581</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12581">https://arxiv.org/pdf/2510.12581</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12581]] LayerSync: Self-aligning Intermediate Layers(https://arxiv.org/abs/2510.12581)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We propose LayerSync, a domain-agnostic approach for improving the generation quality and the training efficiency of diffusion models. Prior studies have highlighted the connection between the quality of generation and the representations learned by diffusion models, showing that external guidance on model intermediate representations accelerates training. We reconceptualize this paradigm by regularizing diffusion models with their own intermediate representations. Building on the observation that representation quality varies across diffusion model layers, we show that the most semantically rich representations can act as an intrinsic guidance for weaker ones, reducing the need for external supervision. Our approach, LayerSync, is a self-sufficient, plug-and-play regularizer term with no overhead on diffusion model training and generalizes beyond the visual domain to other modalities. LayerSync requires no pretrained models nor additional data. We extensively evaluate the method on image generation and demonstrate its applicability to other domains such as audio, video, and motion generation. We show that it consistently improves the generation quality and the training efficiency. For example, we speed up the training of flow-based transformer by over 8.75x on ImageNet dataset and improved the generation quality by 23.6%. The code is available at this https URL.</li>
<li><strong>摘要：</strong>我们提出了 LayerSync，一种与领域无关的方法，用于提高扩散模型的生成质量和训练效率。先前的研究强调了生成质量与扩散模型学习的表示之间的联系，表明对模型中间表示的外部指导可以加速训练。我们通过用扩散模型自己的中间表示对扩散模型进行正则化来重新概念化这种范式。基于对不同扩散模型层的表示质量不同的观察，我们表明语义最丰富的表示可以作为较弱表示的内在指导，从而减少对外部监督的需求。我们的方法 LayerSync 是一种自给自足、即插即用的正则项，在扩散模型训练上没有开销，并且可以推广到视觉领域之外的其他模式。 LayerSync 不需要预先训练的模型或额外的数据。我们广泛评估了该方法在图像生成方面的性能，并证明了其在音频、视频和运动生成等其他领域的适用性。我们证明它持续提高了生成质量和训练效率。例如，我们在 ImageNet 数据集上将基于流的 Transformer 的训练速度提高了 8.75 倍以上，并将生成质量提高了 23.6%。该代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training</h3>
<ul>
<li><strong>Authors: </strong>Jiachen Lei, Keli Liu, Julius Berner, Haiming Yu, Hongkai Zheng, Jiahong Wu, Xiangxiang Chu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12586">https://arxiv.org/abs/2510.12586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12586">https://arxiv.org/pdf/2510.12586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12586]] Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training(https://arxiv.org/abs/2510.12586)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Pixel-space generative models are often more difficult to train and generally underperform compared to their latent-space counterparts, leaving a persistent performance and efficiency gap. In this paper, we introduce a novel two-stage training framework that closes this gap for pixel-space diffusion and consistency models. In the first stage, we pre-train encoders to capture meaningful semantics from clean images while aligning them with points along the same deterministic sampling trajectory, which evolves points from the prior to the data distribution. In the second stage, we integrate the encoder with a randomly initialized decoder and fine-tune the complete model end-to-end for both diffusion and consistency models. Our training framework demonstrates strong empirical performance on ImageNet dataset. Specifically, our diffusion model reaches an FID of 2.04 on ImageNet-256 and 2.35 on ImageNet-512 with 75 number of function evaluations (NFE), surpassing prior pixel-space methods by a large margin in both generation quality and efficiency while rivaling leading VAE-based models at comparable training cost. Furthermore, on ImageNet-256, our consistency model achieves an impressive FID of 8.82 in a single sampling step, significantly surpassing its latent-space counterpart. To the best of our knowledge, this marks the first successful training of a consistency model directly on high-resolution images without relying on pre-trained VAEs or diffusion models.</li>
<li><strong>摘要：</strong>与潜在空间模型相比，像素空间生成模型通常更难训练，并且通常表现不佳，从而造成持续的性能和效率差距。在本文中，我们介绍了一种新颖的两阶段训练框架，该框架弥补了像素空间扩散和一致性模型的差距。在第一阶段，我们预训练编码器以从干净图像中捕获有意义的语义，同时将它们与沿相同确定性采样轨迹的点对齐，这从数据分布之前演化出点。在第二阶段，我们将编码器与随机初始化的解码器集成，并对扩散模型和一致性模型进行端到端的微调。我们的训练框架在 ImageNet 数据集上展示了强大的实证性能。具体来说，我们的扩散模型在经过 75 次函数评估 (NFE) 后，在 ImageNet-256 上达到了 2.04 的 FID，在 ImageNet-512 上达到了 2.35 的 FID，在生成质量和效率方面都大幅超越了之前的像素空间方法，同时以相当的训练成本与基于 VAE 的领先模型相媲美。此外，在 ImageNet-256 上，我们的一致性模型在单个采样步骤中实现了令人印象深刻的 FID 8.82，显着超过了其潜在空间对应模型。据我们所知，这标志着首次在高分辨率图像上成功训练一致性模型，而无需依赖预先训练的 VAE 或扩散模型。</li>
</ul>

<h3>Title: WaterFlow: Explicit Physics-Prior Rectified Flow for Underwater Saliency Mask Generation</h3>
<ul>
<li><strong>Authors: </strong>Runting Li, Shijie Lian, Hua Li, Yutong Li, Wenhui Wu, Sam Kwong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12605">https://arxiv.org/abs/2510.12605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12605">https://arxiv.org/pdf/2510.12605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12605]] WaterFlow: Explicit Physics-Prior Rectified Flow for Underwater Saliency Mask Generation(https://arxiv.org/abs/2510.12605)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Underwater Salient Object Detection (USOD) faces significant challenges, including underwater image quality degradation and domain gaps. Existing methods tend to ignore the physical principles of underwater imaging or simply treat degradation phenomena in underwater images as interference factors that must be eliminated, failing to fully exploit the valuable information they contain. We propose WaterFlow, a rectified flow-based framework for underwater salient object detection that innovatively incorporates underwater physical imaging information as explicit priors directly into the network training process and introduces temporal dimension modeling, significantly enhancing the model's capability for salient object identification. On the USOD10K dataset, WaterFlow achieves a 0.072 gain in S_m, demonstrating the effectiveness and superiority of our method. The code will be published after the acceptance.</li>
<li><strong>摘要：</strong>水下显着物体检测 (USOD) 面临重大挑战，包括水下图像质量下降和域间隙。现有方法往往忽视水下成像的物理原理，或者简单地将水下图像中的退化现象视为必须消除的干扰因素，未能充分利用它们所包含的有价值的信息。我们提出了 WaterFlow，一种基于修正流的水下显着目标检测框架，创新地将水下物理成像信息作为显式先验直接纳入网络训练过程，并引入时间维度建模，显着增强了模型识别显着目标的能力。在 USOD10K 数据集上，WaterFlow 在 S_m 上实现了 0.072 的增益，证明了我们方法的有效性和优越性。代码将在接受后发布。</li>
</ul>

<h3>Title: Laminar: A Scalable Asynchronous RL Post-Training Framework</h3>
<ul>
<li><strong>Authors: </strong>Guangming Sheng, Yuxuan Tong, Borui Wan, Wang Zhang, Chaobo Jia, Xibin Wu, Yuqi Wu, Xiang Li, Chi Zhang, Yanghua Peng, Haibin Lin, Xin Liu, Chuan Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12633">https://arxiv.org/abs/2510.12633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12633">https://arxiv.org/pdf/2510.12633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12633]] Laminar: A Scalable Asynchronous RL Post-Training Framework(https://arxiv.org/abs/2510.12633)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) post-training for Large Language Models (LLMs) is now scaling to large clusters and running for extended durations to enhance model reasoning performance. However, the scalability of existing RL frameworks is limited, as extreme long-tail skewness in RL trajectory generation causes severe GPU underutilization. Current asynchronous RL systems attempt to mitigate this, but they rely on global weight synchronization between the actor and all rollouts, which creates a rigid model update schedule. This global synchronization is ill-suited for the highly skewed and evolving distribution of trajectory generation latency in RL training, crippling training efficiency. Our key insight is that efficient scaling requires breaking this lockstep through trajectory-level asynchrony, which generates and consumes each trajectory independently. We propose Laminar, a scalable and robust RL post-training system built on a fully decoupled architecture. First, we replace global updates with a tier of relay workers acting as a distributed parameter service. This enables asynchronous and fine-grained weight synchronization, allowing rollouts to pull the latest weight anytime without stalling the actor's training loop. Second, a dynamic repack mechanism consolidates long-tail trajectories onto a few dedicated rollouts, maximizing generation throughput. The fully decoupled design also isolates failures, ensuring robustness for long-running jobs. Our evaluation on a 1024-GPU cluster shows that Laminar achieves up to 5.48$\times$ training throughput speedup over state-of-the-art systems, while reducing model convergence time.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的强化学习 (RL) 后训练现已扩展到大型集群并延长运行时间，以增强模型推理性能。然而，现有强化学习框架的可扩展性有限，因为强化学习轨迹生成中的极端长尾偏斜会导致 GPU 严重利用率不足。当前的异步强化学习系统试图缓解这一问题，但它们依赖于 actor 和所有 rollout 之间的全局权重同步，这会创建严格的模型更新时间表。这种全局同步不适合强化学习训练中轨迹生成延迟的高度倾斜和不断变化的分布，从而降低训练效率。我们的主要见解是，有效的扩展需要通过轨迹级异步来打破这种锁步，轨迹级异步独立地生成和消耗每个轨迹。我们提出了 Laminar，这是一种基于完全解耦架构构建的可扩展且强大的 RL 后训练系统。首先，我们用一层充当分布式参数服务的中继工作人员替换全局更新。这实现了异步和细粒度的权重同步，允许随时拉动最新的权重，而不会阻碍演员的训练循环。其次，动态重新打包机制将长尾轨迹整合到一些专用的部署上，从而最大限度地提高发电吞吐量。完全解耦的设计还可以隔离故障，确保长时间运行作业的稳健性。我们对 1024-GPU 集群的评估表明，Laminar 比最先进的系统实现了高达 5.48$\times$ 的训练吞吐量加速，同时缩短了模型收敛时间。</li>
</ul>

<h3>Title: DiffEM: Learning from Corrupted Data with Diffusion Models via Expectation Maximization</h3>
<ul>
<li><strong>Authors: </strong>Danial Hosseintabar, Fan Chen, Giannis Daras, Antonio Torralba, Constantinos Daskalakis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12691">https://arxiv.org/abs/2510.12691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12691">https://arxiv.org/pdf/2510.12691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12691]] DiffEM: Learning from Corrupted Data with Diffusion Models via Expectation Maximization(https://arxiv.org/abs/2510.12691)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as powerful generative priors for high-dimensional inverse problems, yet learning them when only corrupted or noisy observations are available remains challenging. In this work, we propose a new method for training diffusion models with Expectation-Maximization (EM) from corrupted data. Our proposed method, DiffEM, utilizes conditional diffusion models to reconstruct clean data from observations in the E-step, and then uses the reconstructed data to refine the conditional diffusion model in the M-step. Theoretically, we provide monotonic convergence guarantees for the DiffEM iteration, assuming appropriate statistical conditions. We demonstrate the effectiveness of our approach through experiments on various image reconstruction tasks.</li>
<li><strong>摘要：</strong>扩散模型已经成为高维逆问题的强大生成先验，但是当只有损坏或噪声的观察可用时学习它们仍然具有挑战性。在这项工作中，我们提出了一种利用损坏数据的期望最大化（EM）来训练扩散模型的新方法。我们提出的方法 DiffEM 利用条件扩散模型从 ​​E 步骤中的观测中重建干净的数据，然后使用重建的数据来细化 M 步骤中的条件扩散模型。理论上，假设适当的统计条件，我们为 DiffEM 迭代提供单调收敛保证。我们通过各种图像重建任务的实验证明了我们方法的有效性。</li>
</ul>

<h3>Title: CARVQ: Corrective Adaptor with Group Residual Vector Quantization for LLM Embedding Compression</h3>
<ul>
<li><strong>Authors: </strong>Dayin Gou, Sanghyun Byun, Nilesh Malpeddi, Gabrielle De Micheli, Prathamesh Vaste, Jacob Song, Woo Seong Chung</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12721">https://arxiv.org/abs/2510.12721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12721">https://arxiv.org/pdf/2510.12721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12721]] CARVQ: Corrective Adaptor with Group Residual Vector Quantization for LLM Embedding Compression(https://arxiv.org/abs/2510.12721)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) typically rely on a large number of parameters for token embedding, leading to substantial storage requirements and memory footprints. In particular, LLMs deployed on edge devices are memory-bound, and reducing the memory footprint by compressing the embedding layer not only frees up the memory bandwidth but also speeds up inference. To address this, we introduce CARVQ, a post-training novel Corrective Adaptor combined with group Residual Vector Quantization. CARVQ relies on the composition of both linear and non-linear maps and mimics the original model embedding to compress to approximately 1.6 bits without requiring specialized hardware to support lower-bit storage. We test our method on pre-trained LLMs such as LLaMA-3.2-1B, LLaMA-3.2-3B, LLaMA-3.2-3B-Instruct, LLaMA-3.1-8B, Qwen2.5-7B, Qwen2.5-Math-7B and Phi-4, evaluating on common generative, discriminative, math and reasoning tasks. We show that in most cases, CARVQ can achieve lower average bitwidth-per-parameter while maintaining reasonable perplexity and accuracy compared to scalar quantization. Our contributions include a novel compression technique that is compatible with state-of-the-art transformer quantization methods and can be seamlessly integrated into any hardware supporting 4-bit memory to reduce the model's memory footprint in memory-constrained devices. This work demonstrates a crucial step toward the efficient deployment of LLMs on edge devices.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 通常依赖大量参数进行令牌嵌入，从而导致大量的存储需求和内存占用。特别是，部署在边缘设备上的 LLM 受到内存限制，通过压缩嵌入层来减少内存占用不仅可以释放内存带宽，还可以加快推理速度。为了解决这个问题，我们引入了 CARVQ，一种与群残差向量量化相结合的训练后新颖的校正适配器。 CARVQ 依赖于线性和非线性映射的组合，并模仿原始模型嵌入来压缩到大约 1.6 位，而不需要专门的硬件来支持低位存储。我们在预训练的 LLM（例如 LLaMA-3.2-1B、LLaMA-3.2-3B、LLaMA-3.2-3B-Instruct、LLaMA-3.1-8B、Qwen2.5-7B、Qwen2.5-Math-7B 和 Phi-4）上测试我们的方法，评估常见的生成、判别、数学和推理任务。我们表明，在大多数情况下，与标量量化相比，CARVQ 可以实现较低的每个参数平均位宽，同时保持合理的困惑度和准确性。我们的贡献包括一种新颖的压缩技术，该技术与最先进的变压器量化方法兼容，并且可以无缝集成到任何支持 4 位内存的硬件中，以减少模型在内存受限设备中的内存占用。这项工作展示了在边缘设备上高效部署 LLM 的关键一步。</li>
</ul>

<h3>Title: Doctor Rashomon and the UNIVERSE of Madness: Variable Importance with Unobserved Confounding and the Rashomon Effect</h3>
<ul>
<li><strong>Authors: </strong>Jon Donnelly, Srikar Katta, Emanuele Borgonovo, Cynthia Rudin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12734">https://arxiv.org/abs/2510.12734</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12734">https://arxiv.org/pdf/2510.12734</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12734]] Doctor Rashomon and the UNIVERSE of Madness: Variable Importance with Unobserved Confounding and the Rashomon Effect(https://arxiv.org/abs/2510.12734)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Variable importance (VI) methods are often used for hypothesis generation, feature selection, and scientific validation. In the standard VI pipeline, an analyst estimates VI for a single predictive model with only the observed features. However, the importance of a feature depends heavily on which other variables are included in the model, and essential variables are often omitted from observational datasets. Moreover, the VI estimated for one model is often not the same as the VI estimated for another equally-good model - a phenomenon known as the Rashomon Effect. We address these gaps by introducing UNobservables and Inference for Variable importancE using Rashomon SEts (UNIVERSE). Our approach adapts Rashomon sets - the sets of near-optimal models in a dataset - to produce bounds on the true VI even with missing features. We theoretically guarantee the robustness of our approach, show strong performance on semi-synthetic simulations, and demonstrate its utility in a credit risk task.</li>
<li><strong>摘要：</strong>变量重要性 (VI) 方法通常用于假设生成、特征选择和科学验证。在标准 VI 管道中，分析师仅使用观察到的特征来估计单个预测模型的 VI。然而，特征的重要性在很大程度上取决于模型中包含哪些其他变量，并且观察数据集中经常省略基本变量。此外，一个模型的 VI 估计值通常与另一个同样好的模型的 VI 估计值不同，这种现象称为罗生门效应。我们通过使用 Rashomon SEts (UNIVERSE) 引入不可观察量和变量重要性推理来解决这些差距。我们的方法采用罗生门集（数据集中接近最优的模型集），即使缺少特征，也能在真实 VI 上产生边界。我们从理论上保证了我们方法的稳健性，在半合成模拟中表现出强大的性能，并证明了其在信用风险任务中的实用性。</li>
</ul>

<h3>Title: FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Junhao Zhuang, Shi Guo, Xin Cai, Xiaohui Li, Yihao Liu, Chun Yuan, Tianfan Xue</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12747">https://arxiv.org/abs/2510.12747</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12747">https://arxiv.org/pdf/2510.12747</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12747]] FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution(https://arxiv.org/abs/2510.12747)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, super-resolution</a></li>
<li><strong>Abstract: </strong>Diffusion models have recently advanced video restoration, but applying them to real-world video super-resolution (VSR) remains challenging due to high latency, prohibitive computation, and poor generalization to ultra-high resolutions. Our goal in this work is to make diffusion-based VSR practical by achieving efficiency, scalability, and real-time performance. To this end, we propose FlashVSR, the first diffusion-based one-step streaming framework towards real-time VSR. FlashVSR runs at approximately 17 FPS for 768x1408 videos on a single A100 GPU by combining three complementary innovations: (i) a train-friendly three-stage distillation pipeline that enables streaming super-resolution, (ii) locality-constrained sparse attention that cuts redundant computation while bridging the train-test resolution gap, and (iii) a tiny conditional decoder that accelerates reconstruction without sacrificing quality. To support large-scale training, we also construct VSR-120K, a new dataset with 120k videos and 180k images. Extensive experiments show that FlashVSR scales reliably to ultra-high resolutions and achieves state-of-the-art performance with up to 12x speedup over prior one-step diffusion VSR models. We will release the code, pretrained models, and dataset to foster future research in efficient diffusion-based VSR.</li>
<li><strong>摘要：</strong>扩散模型最近在视频恢复方面取得了进展，但由于高延迟、令人望而却步的计算以及对超高分辨率的泛化能力较差，将它们应用于现实世界的视频超分辨率 (VSR) 仍然具有挑战性。我们这项工作的目标是通过实现效率、可扩展性和实时性能，使基于扩散的 VSR 变得实用。为此，我们提出了 FlashVSR，这是第一个基于扩散的实时 VSR 一步式流媒体框架。 FlashVSR 通过结合三个互补的创新，在单个 A100 GPU 上以约 17 FPS 的速度运行 768x1408 视频：(i) 训练友好的三级蒸馏管道，可实现流式超分辨率；(ii) 局部约束的稀疏注意力，可减少冗余计算，同时缩小训练测试分辨率差距；(iii) 一个微型条件解码器 在不牺牲质量的情况下加速重建。为了支持大规模训练，我们还构建了 VSR-120K，一个包含 120k 视频和 180k 图像的新数据集。大量实验表明，FlashVSR 能够可靠地扩展到超高分辨率，并实现最先进的性能，与之前的一步扩散 VSR 模型相比，加速速度高达 12 倍。我们将发布代码、预训练模型和数据集，以促进基于高效扩散的 VSR 的未来研究。</li>
</ul>

<h3>Title: SPORTS: Simultaneous Panoptic Odometry, Rendering, Tracking and Segmentation for Urban Scenes Understanding</h3>
<ul>
<li><strong>Authors: </strong>Zhiliu Yang, Jinyu Dai, Jianyuan Zhang, Zhu Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12749">https://arxiv.org/abs/2510.12749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12749">https://arxiv.org/pdf/2510.12749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12749]] SPORTS: Simultaneous Panoptic Odometry, Rendering, Tracking and Segmentation for Urban Scenes Understanding(https://arxiv.org/abs/2510.12749)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The scene perception, understanding, and simulation are fundamental techniques for embodied-AI agents, while existing solutions are still prone to segmentation deficiency, dynamic objects' interference, sensor data sparsity, and view-limitation problems. This paper proposes a novel framework, named SPORTS, for holistic scene understanding via tightly integrating Video Panoptic Segmentation (VPS), Visual Odometry (VO), and Scene Rendering (SR) tasks into an iterative and unified perspective. Firstly, VPS designs an adaptive attention-based geometric fusion mechanism to align cross-frame features via enrolling the pose, depth, and optical flow modality, which automatically adjust feature maps for different decoding stages. And a post-matching strategy is integrated to improve identities tracking. In VO, panoptic segmentation results from VPS are combined with the optical flow map to improve the confidence estimation of dynamic objects, which enhances the accuracy of the camera pose estimation and completeness of the depth map generation via the learning-based paradigm. Furthermore, the point-based rendering of SR is beneficial from VO, transforming sparse point clouds into neural fields to synthesize high-fidelity RGB views and twin panoptic views. Extensive experiments on three public datasets demonstrate that our attention-based feature fusion outperforms most existing state-of-the-art methods on the odometry, tracking, segmentation, and novel view synthesis tasks.</li>
<li><strong>摘要：</strong>场景感知、理解和模拟是嵌入式人工智能代理的基本技术，而现有的解决方案仍然容易出现分割缺陷、动态对象干扰、传感器数据稀疏和视图限制等问题。本文提出了一种名为 SPORTS 的新颖框架，通过将视频全景分割（VPS）、视觉里程计（VO）和场景渲染（SR）任务紧密集成到迭代和统一的视角中来实现整体场景理解。首先，VPS 设计了一种基于注意力的自适应几何融合机制，通过注册姿态、深度和光流模态来对齐跨帧特征，从而自动调整不同解码阶段​​的特征图。并集成了匹配后策略以改进身份跟踪。在VO中，VPS的全景分割结果与光流图相结合，以提高动态对象的置信度估计，从而通过基于学习的范例提高相机位姿估计的准确性和深度图生成的完整性。此外，SR的基于点的渲染有利于VO，将稀疏点云转换为神经场以合成高保真RGB视图和双全景视图。对三个公共数据集的广泛实验表明，我们基于注意力的特征融合在里程计、跟踪、分割和新颖的视图合成任务方面优于大多数现有的最先进的方法。</li>
</ul>

<h3>Title: Efficient Perceptual Image Super Resolution: AIM 2025 Study and Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Bruno Longarela, Marcos V. Conde, Alvaro Garcia, Radu Timofte</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12765">https://arxiv.org/abs/2510.12765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12765">https://arxiv.org/pdf/2510.12765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12765]] Efficient Perceptual Image Super Resolution: AIM 2025 Study and Benchmark(https://arxiv.org/abs/2510.12765)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>This paper presents a comprehensive study and benchmark on Efficient Perceptual Super-Resolution (EPSR). While significant progress has been made in efficient PSNR-oriented super resolution, approaches focusing on perceptual quality metrics remain relatively inefficient. Motivated by this gap, we aim to replicate or improve the perceptual results of Real-ESRGAN while meeting strict efficiency constraints: a maximum of 5M parameters and 2000 GFLOPs, calculated for an input size of 960x540 pixels. The proposed solutions were evaluated on a novel dataset consisting of 500 test images of 4K resolution, each degraded using multiple degradation types, without providing the original high-quality counterparts. This design aims to reflect realistic deployment conditions and serves as a diverse and challenging benchmark. The top-performing approach manages to outperform Real-ESRGAN across all benchmark datasets, demonstrating the potential of efficient methods in the perceptual domain. This paper establishes the modern baselines for efficient perceptual super resolution.</li>
<li><strong>摘要：</strong>本文对高效感知超分辨率（EPSR）进行了全面的研究和基准测试。虽然在高效的 PSNR 导向的超分辨率方面取得了重大进展，但专注于感知质量指标的方法仍然相对低效。受这一差距的激励，我们的目标是复制或改进 Real-ESRGAN 的感知结果，同时满足严格的效率限制：最多 5M 个参数和 2000 GFLOP，针对 960x540 像素的输入大小计算。所提出的解决方案在由 500 张 4K 分辨率测试图像组成的新颖数据集上进行了评估，每个图像均使用多种降级类型进行降级，但没有提供原始的高质量对应图像。该设计旨在反映现实的部署条件，并作为多样化且具有挑战性的基准。表现最好的方法在所有基准数据集上都超越了 Real-ESRGAN，展示了感知领域有效方法的潜力。本文建立了高效感知超分辨率的现代基线。</li>
</ul>

<h3>Title: What If : Understanding Motion Through Sparse Interactions</h3>
<ul>
<li><strong>Authors: </strong>Stefan Andreas Baumann, Nick Stracke, Timy Phan, Björn Ommer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12777">https://arxiv.org/abs/2510.12777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12777">https://arxiv.org/pdf/2510.12777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12777]] What If : Understanding Motion Through Sparse Interactions(https://arxiv.org/abs/2510.12777)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Understanding the dynamics of a physical scene involves reasoning about the diverse ways it can potentially change, especially as a result of local interactions. We present the Flow Poke Transformer (FPT), a novel framework for directly predicting the distribution of local motion, conditioned on sparse interactions termed "pokes". Unlike traditional methods that typically only enable dense sampling of a single realization of scene dynamics, FPT provides an interpretable directly accessible representation of multi-modal scene motion, its dependency on physical interactions and the inherent uncertainties of scene dynamics. We also evaluate our model on several downstream tasks to enable comparisons with prior methods and highlight the flexibility of our approach. On dense face motion generation, our generic pre-trained model surpasses specialized baselines. FPT can be fine-tuned in strongly out-of-distribution tasks such as synthetic datasets to enable significant improvements over in-domain methods in articulated object motion estimation. Additionally, predicting explicit motion distributions directly enables our method to achieve competitive performance on tasks like moving part segmentation from pokes which further demonstrates the versatility of our FPT. Code and models are publicly available at this https URL.</li>
<li><strong>摘要：</strong>理解物理场景的动态涉及推理它可能发生变化的多种方式，尤其是由于局部交互而发生的变化。我们提出了 Flow Poke Transformer (FPT)，这是一种直接预测局部运动分布的新颖框架，以称为“poke”的稀疏交互为条件。与通常只能对场景动态的单一实现进行密集采样的传统方法不同，FPT 提供了多模态场景运动的可解释的直接访问表示、其对物理交互的依赖以及场景动态的固有不确定性。我们还在几个下游任务上评估我们的模型，以便与以前的方法进行比较并强调我们方法的灵活性。在密集面部运动生成方面，我们的通用预训练模型超越了专门的基线。 FPT 可以在合成数据集等强分布外任务中进行微调，从而在铰接物体运动估计方面比域内方法得到显着改进。此外，预测显式运动分布直接使我们的方法能够在诸如从戳中分割运动部件等任务中实现有竞争力的性能，这进一步证明了我们的 FPT 的多功能性。代码和模型可通过此 https URL 公开获取。</li>
</ul>

<h3>Title: SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models</h3>
<ul>
<li><strong>Authors: </strong>Weiyang Jin, Yuwei Niu, Jiaqi Liao, Chengqi Duan, Aoxue Li, Shenghua Gao, Xihui Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12784">https://arxiv.org/abs/2510.12784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12784">https://arxiv.org/pdf/2510.12784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12784]] SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models(https://arxiv.org/abs/2510.12784)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recently, remarkable progress has been made in Unified Multimodal Models (UMMs), which integrate vision-language generation and understanding capabilities within a single framework. However, a significant gap exists where a model's strong visual understanding often fails to transfer to its visual generation. A model might correctly understand an image based on user instructions, yet be unable to generate a faithful image from text prompts. This phenomenon directly raises a compelling question: Can a model achieve self-improvement by using its understanding module to reward its generation module? To bridge this gap and achieve self-improvement, we introduce SRUM, a self-rewarding post-training framework that can be directly applied to existing UMMs of various designs. SRUM creates a feedback loop where the model's own understanding module acts as an internal ``evaluator'', providing corrective signals to improve its generation module, without requiring additional human-labeled data. To ensure this feedback is comprehensive, we designed a global-local dual reward system. To tackle the inherent structural complexity of images, this system offers multi-scale guidance: a \textbf{global reward} ensures the correctness of the overall visual semantics and layout, while a \textbf{local reward} refines fine-grained, object-level fidelity. SRUM leads to powerful capabilities and shows strong generalization, boosting performance on T2I-CompBench from 82.18 to \textbf{88.37} and on T2I-ReasonBench from 43.82 to \textbf{46.75}. Overall, our work establishes a powerful new paradigm for enabling a UMMs' understanding module to guide and enhance its own generation via self-rewarding.</li>
<li><strong>摘要：</strong>最近，统一多模态模型（UMM）取得了显着进展，它将视觉语言生成和理解能力集成在一个框架内。然而，存在一个显着的差距，模型的强大视觉理解通常无法转移到其视觉生成。模型可能会根据用户指令正确理解图像，但无法根据文本提示生成忠实的图像。这种现象直接提出了一个引人注目的问题：模型能否通过使用其理解模块来奖励其生成模块来实现自我改进？为了弥补这一差距并实现自我改进，我们引入了 SRUM，这是一种自我奖励的培训后框架，可以直接应用于各种设计的现有 UMM。 SRUM 创建了一个反馈循环，其中模型自己的理解模块充当内部“评估器”，提供纠正信号来改进其生成模块，而不需要额外的人工标记数据。为了确保反馈的全面性，我们设计了全球-本地双重奖励系统。为了解决图像固有的结构复杂性，该系统提供了多尺度指导：全局奖励确保整体视觉语义和布局的正确性，而局部奖励则细化细粒度的对象级保真度。 SRUM 具有强大的功能并表现出很强的泛化能力，将 T2I-CompBench 上的性能从 82.18 提升到 \textbf{88.37}，将 T2I-ReasonBench 上的性能从 43.82 提升到 \textbf{46.75}。总体而言，我们的工作建立了一个强大的新范例，使 UMM 的理解模块能够通过自我奖励来指导和增强其自身的生成。</li>
</ul>

<h3>Title: MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars</h3>
<ul>
<li><strong>Authors: </strong>Felix Taubner, Ruihang Zhang, Mathieu Tuli, Sherwin Bahmani, David B. Lindell</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12785">https://arxiv.org/abs/2510.12785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12785">https://arxiv.org/pdf/2510.12785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12785]] MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars(https://arxiv.org/abs/2510.12785)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Digital human avatars aim to simulate the dynamic appearance of humans in virtual environments, enabling immersive experiences across gaming, film, virtual reality, and more. However, the conventional process for creating and animating photorealistic human avatars is expensive and time-consuming, requiring large camera capture rigs and significant manual effort from professional 3D artists. With the advent of capable image and video generation models, recent methods enable automatic rendering of realistic animated avatars from a single casually captured reference image of a target subject. While these techniques significantly lower barriers to avatar creation and offer compelling realism, they lack constraints provided by multi-view information or an explicit 3D representation. So, image quality and realism degrade when rendered from viewpoints that deviate strongly from the reference image. Here, we build a video model that generates animatable multi-view videos of digital humans based on a single reference image and target expressions. Our model, MVP4D, is based on a state-of-the-art pre-trained video diffusion model and generates hundreds of frames simultaneously from viewpoints varying by up to 360 degrees around a target subject. We show how to distill the outputs of this model into a 4D avatar that can be rendered in real-time. Our approach significantly improves the realism, temporal consistency, and 3D consistency of generated avatars compared to previous methods.</li>
<li><strong>摘要：</strong>数字化身旨在模拟虚拟环境中人类的动态外观，从而在游戏、电影、虚拟现实等领域实现身临其境的体验。然而，创建逼真的人物头像并对其进行动画处理的传统过程既昂贵又耗时，需要大型相机捕捉设备以及专业 3D 艺术家的大量手动工作。随着功能强大的图像和视频生成模型的出现，最近的方法能够从单个随意捕获的目标对象的参考图像自动渲染逼真的动画化身。虽然这些技术显着降低了头像创建的障碍并提供了令人信服的真实感，但它们缺乏多视图信息或显式 3D 表示所提供的限制。因此，当从与参考图像严重偏离的视点进行渲染时，图像质量和真实感会下降。在这里，我们构建了一个视频模型，该模型基于单个参考图像和目标表情生成数字人类的可动画多视图视频。我们的模型 MVP4D 基于最先进的预训练视频扩散模型，可从围绕目标主体最多 360 度变化的视点同时生成数百个帧。我们展示了如何将该模型的输出提炼成可以实时渲染的 4D 头像。与以前的方法相比，我们的方法显着提高了生成的头像的真实性、时间一致性和 3D 一致性。</li>
</ul>

<h3>Title: Efficient Real-World Deblurring using Single Images: AIM 2025 Challenge Report</h3>
<ul>
<li><strong>Authors: </strong>Daniel Feijoo, Paula Garrido-Mellado, Marcos V. Conde, Jaesung Rim, Alvaro Garcia, Sunghyun Cho, Radu Timofte</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12788">https://arxiv.org/abs/2510.12788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12788">https://arxiv.org/pdf/2510.12788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12788]] Efficient Real-World Deblurring using Single Images: AIM 2025 Challenge Report(https://arxiv.org/abs/2510.12788)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>This paper reviews the AIM 2025 Efficient Real-World Deblurring using Single Images Challenge, which aims to advance in efficient real-blur restoration. The challenge is based on a new test set based on the well known RSBlur dataset. Pairs of blur and degraded images in this dataset are captured using a double-camera system. Participant were tasked with developing solutions to effectively deblur these type of images while fulfilling strict efficiency constraints: fewer than 5 million model parameters and a computational budget under 200 GMACs. A total of 71 participants registered, with 4 teams finally submitting valid solutions. The top-performing approach achieved a PSNR of 31.1298 dB, showcasing the potential of efficient methods in this domain. This paper provides a comprehensive overview of the challenge, compares the proposed solutions, and serves as a valuable reference for researchers in efficient real-world image deblurring.</li>
<li><strong>摘要：</strong>本文回顾了 AIM 2025 使用单图像高效现实世界去模糊挑战赛，该挑战赛旨在推进高效的真实模糊恢复。该挑战基于基于众所周知的 RSBlur 数据集的新测试集。该数据集中的成对模糊和退化图像是使用双摄像头系统捕获的。参与者的任务是开发解决方案，以有效地去模糊这些类型的图像，同时满足严格的效率限制：模型参数少于 500 万个，计算预算低于 200 GMAC。共有71名参赛者报名，最终有4支队伍提交了有效方案。性能最佳的方法实现了 31.1298 dB 的 PSNR，展示了该领域有效方法的潜力。本文全面概述了这一挑战，比较了所提出的解决方案，为研究人员有效地进行现实世界图像去模糊提供了宝贵的参考。</li>
</ul>

<h3>Title: UniFusion: Vision-Language Model as Unified Encoder in Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Kevin Li, Manuel Brack, Sudeep Katakol, Hareesh Ravi, Ajinkya Kale</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12789">https://arxiv.org/abs/2510.12789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12789">https://arxiv.org/pdf/2510.12789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12789]] UniFusion: Vision-Language Model as Unified Encoder in Image Generation(https://arxiv.org/abs/2510.12789)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Although recent advances in visual generation have been remarkable, most existing architectures still depend on distinct encoders for images and text. This separation constrains diffusion models' ability to perform cross-modal reasoning and knowledge transfer. Prior attempts to bridge this gap often use the last layer information from VLM, employ multiple visual encoders, or train large unified models jointly for text and image generation, which demands substantial computational resources and large-scale data, limiting its this http URL present UniFusion, a diffusion-based generative model conditioned on a frozen large vision-language model (VLM) that serves as a unified multimodal encoder. At the core of UniFusion is the Layerwise Attention Pooling (LAP) mechanism that extracts both high level semantics and low level details from text and visual tokens of a frozen VLM to condition a diffusion generative model. We demonstrate that LAP outperforms other shallow fusion architectures on text-image alignment for generation and faithful transfer of visual information from VLM to the diffusion model which is key for editing. We propose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI), which conditions a diffusion transformer (DiT) only on the text tokens generated by the VLM during in-model prompt rewriting. VERIFI combines the alignment of the conditioning distribution with the VLM's reasoning capabilities for increased capabilities and flexibility at inference. In addition, finetuning on editing task not only improves text-image alignment for generation, indicative of cross-modality knowledge transfer, but also exhibits tremendous generalization capabilities. Our model when trained on single image editing, zero-shot generalizes to multiple image references further motivating the unified encoder design of UniFusion.</li>
<li><strong>摘要：</strong>尽管视觉生成领域最近取得了显着的进步，但大多数现有架构仍然依赖于不同的图像和文本编码器。这种分离限制了扩散模型执行跨模式推理和知识转移的能力。之前弥补这一差距的尝试通常使用 VLM 的最后一层信息，采用多个视觉编码器，或者联合训练大型统一模型来生成文本和图像，这需要大量的计算资源和大规模数据，从而限制了该 http URL 目前的 UniFusion，这是一种基于扩散的生成模型，以冻结的大型视觉语言模型 (VLM) 为条件，充当统一的多模态编码器。 UniFusion 的核心是分层注意力池 (LAP) 机制，该机制从冻结的 VLM 的文本和视觉标记中提取高级语义和低级细节，以调节扩散生成模型。我们证明，LAP 在文本图像对齐方面优于其他浅层融合架构，可生成视觉信息并将其忠实地从 VLM 传输到扩散模型，这对于编辑至关重要。我们提出了具有灵活推理功能的 VLM 重写注入 (VERIFI)，它仅在模型内提示重写期间 VLM 生成的文本标记上设置扩散变换器 (DiT)。 VERIFI 将调节分布的一致性与 VLM 的推理功能相结合，以提高推理能力和灵活性。此外，对编辑任务的微调不仅提高了生成时的文本图像对齐，表明跨模态知识转移，而且还表现出了巨大的泛化能力。我们的模型在对单图像编辑进行训练时，零样本可推广到多个图像参考，进一步推动了 UniFusion 的统一编码器设计。</li>
</ul>

<h3>Title: DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search</h3>
<ul>
<li><strong>Authors: </strong>Kartik Narayan, Yang Xu, Tian Cao, Kavya Nerella, Vishal M. Patel, Navid Shiee, Peter Grasch, Chao Jia, Yinfei Yang, Zhe Gan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12801">https://arxiv.org/abs/2510.12801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12801">https://arxiv.org/pdf/2510.12801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12801]] DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search(https://arxiv.org/abs/2510.12801)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) in real-world applications require access to external knowledge sources and must remain responsive to the dynamic and ever-changing real-world information in order to address information-seeking and knowledge-intensive user queries. Existing approaches, such as retrieval augmented generation (RAG) methods, search agents, and search equipped MLLMs, often suffer from rigid pipelines, excessive search calls, and poorly constructed search queries, which result in inefficiencies and suboptimal outcomes. To address these limitations, we present DeepMMSearch-R1, the first multimodal LLM capable of performing on-demand, multi-turn web searches and dynamically crafting queries for both image and text search tools. Specifically, DeepMMSearch-R1 can initiate web searches based on relevant crops of the input image making the image search more effective, and can iteratively adapt text search queries based on retrieved information, thereby enabling self-reflection and self-correction. Our approach relies on a two-stage training pipeline: a cold start supervised finetuning phase followed by an online reinforcement learning optimization. For training, we introduce DeepMMSearchVQA, a novel multimodal VQA dataset created through an automated pipeline intermixed with real-world information from web search tools. This dataset contains diverse, multi-hop queries that integrate textual and visual information, teaching the model when to search, what to search for, which search tool to use and how to reason over the retrieved information. We conduct extensive experiments across a range of knowledge-intensive benchmarks to demonstrate the superiority of our approach. Finally, we analyze the results and provide insights that are valuable for advancing multimodal web-search.</li>
<li><strong>摘要：</strong>现实应用中的多模态大型语言模型 (MLLM) 需要访问外部知识源，并且必须保持对动态和不断变化的现实世界信息的响应，以便解决信息查找和知识密集型用户查询。现有的方法，例如检索增强生成 (RAG) 方法、搜索代理和配备搜索的 MLLM，通常会遇到僵化的管道、过多的搜索调用和构造不良的搜索查询，从而导致效率低下和结果不佳。为了解决这些限制，我们推出了 DeepMMSearch-R1，这是第一个能够执行按需、多轮网络搜索并为图像和文本搜索工具动态创建查询的多模式法学硕士。具体来说，DeepMMSearch-R1可以根据输入图像的相关裁剪启动网络搜索，使图像搜索更加有效，并且可以根据检索到的信息迭代地调整文本搜索查询，从而实现自我反思和自我纠正。我们的方法依赖于两阶段的训练流程：冷启动监督微调阶段，然后是在线强化学习优化。对于训练，我们引入了 DeepMMSearchVQA，这是一种新颖的多模式 VQA 数据集，通过自动化管道与来自网络搜索工具的真实信息混合创建。该数据集包含集成文本和视觉信息的多样化、多跳查询，教导模型何时搜索、搜索什么、使用哪种搜索工具以及如何对检索到的信息进行推理。我们在一系列知识密集型基准上进行了广泛的实验，以证明我们方法的优越性。最后，我们分析结果并提供对于推进多模式网络搜索有价值的见解。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
