<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-02-13</h1>
<h3>Title: GAC-KAN: An Ultra-Lightweight GNSS Interference Classifier for GenAI-Powered Consumer Edge Devices</h3>
<ul>
<li><strong>Authors: </strong>Zhihan Zeng, Kaihe Wang, Zhongpei Zhang, Yue Xiu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11186">https://arxiv.org/abs/2602.11186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11186">https://arxiv.org/pdf/2602.11186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11186]] GAC-KAN: An Ultra-Lightweight GNSS Interference Classifier for GenAI-Powered Consumer Edge Devices(https://arxiv.org/abs/2602.11186)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The integration of Generative AI (GenAI) into Consumer Electronics (CE)--from AI-powered assistants in wearables to generative planning in autonomous Uncrewed Aerial Vehicles (UAVs)--has revolutionized user experiences. However, these GenAI applications impose immense computational burdens on edge hardware, leaving strictly limited resources for fundamental security tasks like Global Navigation Satellite System (GNSS) signal protection. Furthermore, training robust classifiers for such devices is hindered by the scarcity of real-world interference data. To address the dual challenges of data scarcity and the extreme efficiency required by the GenAI era, this paper proposes a novel framework named GAC-KAN. First, we adopt a physics-guided simulation approach to synthesize a large-scale, high-fidelity jamming dataset, mitigating the data bottleneck. Second, to reconcile high accuracy with the stringent resource constraints of GenAI-native chips, we design a Multi-Scale Ghost-ACB-Coordinate (MS-GAC) backbone. This backbone combines Asymmetric Convolution Blocks (ACB) and Ghost modules to extract rich spectral-temporal features with minimal redundancy. Replacing the traditional Multi-Layer Perceptron (MLP) decision head, we introduce a Kolmogorov-Arnold Network (KAN), which employs learnable spline activation functions to achieve superior non-linear mapping capabilities with significantly fewer parameters. Experimental results demonstrate that GAC-KAN achieves an overall accuracy of 98.0\%, outperforming state-of-the-art baselines. Significantly, the model contains only 0.13 million parameter--approximately 660 times fewer than Vision Transformer (ViT) baselines. This extreme lightweight characteristic makes GAC-KAN an ideal "always-on" security companion, ensuring GNSS reliability without contending for the computational resources required by primary GenAI tasks.</li>
<li><strong>摘要：</strong>生成式人工智能 (GenAI) 与消费电子产品 (CE) 的集成——从可穿戴设备中的人工智能助手到自主无人飞行器 (UAV) 中的生成式规划——彻底改变了用户体验。然而，这些 GenAI 应用程序给边缘硬件带来了巨大的计算负担，为全球导航卫星系统 (GNSS) 信号保护等基本安全任务留下了严格有限的资源。此外，现实世界干扰数据的缺乏阻碍了为此类设备训练鲁棒分类器。为了解决GenAI时代数据稀缺和极高效率的双重挑战，本文提出了一种名为GAC-KAN的新型框架。首先，我们采用物理引导的模拟方法来合成大规模、高保真的干扰数据集，缓解数据瓶颈。其次，为了协调高精度与 GenAI 原生芯片严格的资源限制，我们设计了多尺度 Ghost-ACB 坐标（MS-GAC）主干。该骨干网结合了非对称卷积块（ACB）和 Ghost 模块，以最小的冗余提取丰富的频谱时间特征。我们引入了柯尔莫哥洛夫-阿诺德网络 (KAN) 来取代传统的多层感知器 (MLP) 决策头，该网络采用可学习的样条激活函数，以显着减少的参数实现卓越的非线性映射能力。实验结果表明，GAC-KAN 的总体准确率达到 98.0%，优于最先进的基线。值得注意的是，该模型仅包含 13 万个参数，比 Vision Transformer (ViT) 基线少约 660 倍。这种极其轻量的特性使 GAC-KAN 成为理想的“永远在线”安全伴侣，确保 GNSS 可靠性，而无需争夺主要 GenAI 任务所需的计算资源。</li>
</ul>

<h3>Title: UltraLIF: Fully Differentiable Spiking Neural Networks via Ultradiscretization and Max-Plus Algebra</h3>
<ul>
<li><strong>Authors: </strong>Jose Marie Antonio Miñoza</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, math.RA, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11206">https://arxiv.org/abs/2602.11206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11206">https://arxiv.org/pdf/2602.11206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11206]] UltraLIF: Fully Differentiable Spiking Neural Networks via Ultradiscretization and Max-Plus Algebra(https://arxiv.org/abs/2602.11206)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Spiking Neural Networks (SNNs) offer energy-efficient, biologically plausible computation but suffer from non-differentiable spike generation, necessitating reliance on heuristic surrogate gradients. This paper introduces UltraLIF, a principled framework that replaces surrogate gradients with ultradiscretization, a mathematical formalism from tropical geometry providing continuous relaxations of discrete dynamics. The central insight is that the max-plus semiring underlying ultradiscretization naturally models neural threshold dynamics: the log-sum-exp function serves as a differentiable soft-maximum that converges to hard thresholding as a learnable temperature parameter $\eps \to 0$. Two neuron models are derived from distinct dynamical systems: UltraLIF from the LIF ordinary differential equation (temporal dynamics) and UltraDLIF from the diffusion equation modeling gap junction coupling across neuronal populations (spatial dynamics). Both yield fully differentiable SNNs trainable via standard backpropagation with no forward-backward mismatch. Theoretical analysis establishes pointwise convergence to classical LIF dynamics with quantitative error bounds and bounded non-vanishing gradients. Experiments on six benchmarks spanning static images, neuromorphic vision, and audio demonstrate improvements over surrogate gradient baselines, with gains most pronounced in single-timestep ($T{=}1$) settings on neuromorphic and temporal datasets. An optional sparsity penalty enables significant energy reduction while maintaining competitive accuracy.</li>
<li><strong>摘要：</strong>尖峰神经网络 (SNN) 提供节能、生物学上合理的计算，但受到不可微分尖峰生成的影响，需要依赖启发式替代梯度。本文介绍了 UltraLIF，这是一个用超离散化取代代理梯度的原则框架，超离散化是一种来自热带几何的数学形式，提供离散动力学的连续松弛。中心思想是，基于超离散化的 max-plus 半环自然地模拟了神经阈值动态：log-sum-exp 函数充当可微分的软最大值，作为可学习的温度参数 $\eps \to 0$ 收敛到硬阈值。两个神经元模型源自不同的动力系统：UltraLIF 来自 LIF 常微分方程（时间动力学），UltraDLIF 来自模拟神经元群体间隙连接耦合的扩散方程（空间动力学）。两者都产生完全可微的 SNN，可通过标准反向传播进行训练，没有前向-后向失配。理论分析建立了与经典 LIF 动力学的逐点收敛，具有定量误差界限和有界非消失梯度。对涵盖静态图像、神经形态视觉和音频的六个基准的实验证明了相对于替代梯度基线的改进，其中神经形态和时间数据集的单时间步长 ($T{=}1$) 设置中的增益最为明显。可选的稀疏性惩罚可以显着降低能耗，同时保持有竞争力的精度。</li>
</ul>

<h3>Title: Adaptive Physics Transformer with Fused Global-Local Attention for Subsurface Energy Systems</h3>
<ul>
<li><strong>Authors: </strong>Xin Ju, Nok Hei (Hadrian)Fung, Yuyan Zhang, Carl Jacquemyn, Matthew Jackson, Randolph Settgast, Sally M. Benson, Gege Wen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11208">https://arxiv.org/abs/2602.11208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11208">https://arxiv.org/pdf/2602.11208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11208]] Adaptive Physics Transformer with Fused Global-Local Attention for Subsurface Energy Systems(https://arxiv.org/abs/2602.11208)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>The Earth's subsurface is a cornerstone of modern society, providing essential energy resources like hydrocarbons, geothermal, and minerals while serving as the primary reservoir for $CO_2$ sequestration. However, full physics numerical simulations of these systems are notoriously computationally expensive due to geological heterogeneity, high resolution requirements, and the tight coupling of physical processes with distinct propagation time scales. Here we propose the \textbf{Adaptive Physics Transformer} (APT), a geometry-, mesh-, and physics-agnostic neural operator that explicitly addresses these challenges. APT fuses a graph-based encoder to extract high-resolution local heterogeneous features with a global attention mechanism to resolve long-range physical impacts. Our results demonstrate that APT outperforms state-of-the-art architectures in subsurface tasks across both regular and irregular grids with robust super-resolution capabilities. Notably, APT is the first architecture that directly learns from adaptive mesh refinement simulations. We also demonstrate APT's capability for cross-dataset learning, positioning it as a robust and scalable backbone for large-scale subsurface foundation model development.</li>
<li><strong>摘要：</strong>地球地下是现代社会的基石，提供碳氢化合物、地热和矿物等重要能源，同时也是 $CO_2$ 封存的主要储存库。然而，由于地质异质性、高分辨率要求以及物理过程与不同传播时间尺度的紧密耦合，这些系统的完整物理数值模拟的计算成本非常昂贵。在这里，我们提出了 \textbf{自适应物理变换器} (APT)，这是一种与几何、网格和物理无关的神经算子，可以明确解决这些挑战。 APT 融合基于图的编码器来提取高分辨率局部异构特征，并通过全局注意机制来解决远程物理影响。我们的结果表明，APT 在规则和不规则网格的地下任务中均优于最先进的架构，具有强大的超分辨率能力。值得注意的是，APT 是第一个直接从自适应网格细化模拟中学习的架构。我们还展示了 APT 的跨数据集学习能力，将其定位为大规模地下基础模型开发的强大且可扩展的骨干。</li>
</ul>

<h3>Title: Protein Language Model Embeddings Improve Generalization of Implicit Transfer Operators</h3>
<ul>
<li><strong>Authors: </strong>Panagiotis Antoniadis, Beatrice Pavesi, Simon Olsson, Ole Winther</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.bio-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11216">https://arxiv.org/abs/2602.11216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11216">https://arxiv.org/pdf/2602.11216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11216]] Protein Language Model Embeddings Improve Generalization of Implicit Transfer Operators(https://arxiv.org/abs/2602.11216)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Molecular dynamics (MD) is a central computational tool in physics, chemistry, and biology, enabling quantitative prediction of experimental observables as expectations over high-dimensional molecular distributions such as Boltzmann distributions and transition densities. However, conventional MD is fundamentally limited by the high computational cost required to generate independent samples. Generative molecular dynamics (GenMD) has recently emerged as an alternative, learning surrogates of molecular distributions either from data or through interaction with energy models. While these methods enable efficient sampling, their transferability across molecular systems is often limited. In this work, we show that incorporating auxiliary sources of information can improve the data efficiency and generalization of transferable implicit transfer operators (TITO) for molecular dynamics. We find that coarse-grained TITO models are substantially more data-efficient than Boltzmann Emulators, and that incorporating protein language model (pLM) embeddings further improves out-of-distribution generalization. Our approach, PLaTITO, achieves state-of-the-art performance on equilibrium sampling benchmarks for out-of-distribution protein systems, including fast-folding proteins. We further study the impact of additional conditioning signals -- such as structural embeddings, temperature, and large-language-model-derived embeddings -- on model performance.</li>
<li><strong>摘要：</strong>分子动力学 (MD) 是物理、化学和生物学领域的核心计算工具，可以根据玻尔兹曼分布和跃迁密度等高维分子分布的预期对实验观测值进行定量预测。然而，传统MD从根本上受到生成独立样本所需的高计算成本的限制。生成分子动力学（GenMD）最近作为一种替代方案出现，它可以从数据或通过与能量模型的相互作用来学习分子分布的替代物。虽然这些方法能够实现高效采样，但它们跨分子系统的可转移性通常受到限制。在这项工作中，我们表明，合并辅助信息源可以提高分子动力学可转移隐式转移算子（TITO）的数据效率和泛化性。我们发现粗粒度 TITO 模型比玻尔兹曼模拟器的数据效率要高得多，并且结合蛋白质语言模型 (pLM) 嵌入进一步提高了分布外泛化能力。我们的方法 PLATITO 在非分布蛋白质系统（包括快速折叠蛋白质）的平衡采样基准上实现了最先进的性能。我们进一步研究了额外的条件信号（例如结构嵌入、温度和大型语言模型衍生的嵌入）对模型性能的影响。</li>
</ul>

<h3>Title: Patch the Distribution Mismatch: RL Rewriting Agent for Stable Off-Policy SFT</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Wang, Ping Jian, Zhen Yang, Zirong Chen, Keren Liao, Zhongbin Guo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11220">https://arxiv.org/abs/2602.11220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11220">https://arxiv.org/pdf/2602.11220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11220]] Patch the Distribution Mismatch: RL Rewriting Agent for Stable Off-Policy SFT(https://arxiv.org/abs/2602.11220)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have made rapid progress, yet adapting them to downstream scenarios still commonly relies on supervised fine-tuning (SFT). When downstream data exhibit a substantial distribution shift from the model's prior training distribution, SFT can induce catastrophic forgetting. To narrow this gap, data rewriting has been proposed as a data-centric approach that rewrites downstream training data prior to SFT. However, existing methods typically sample rewrites from a prompt-induced conditional distribution, so the resulting targets are not necessarily aligned with the model's natural QA-style generation distribution. Moreover, reliance on fixed templates can lead to diversity collapse. To address these issues, we cast data rewriting as a policy learning problem and learn a rewriting policy that better matches the backbone's QA-style generation distribution while preserving diversity. Since distributional alignment, diversity and task consistency are automatically evaluable but difficult to optimize end-to-end with differentiable objectives, we leverage reinforcement learning to optimize the rewrite distribution under reward feedback and propose an RL-based data-rewriting agent. The agent jointly optimizes QA-style distributional alignment and diversity under a hard task-consistency gate, thereby constructing a higher-quality rewritten dataset for downstream SFT. Extensive experiments show that our method achieves downstream gains comparable to standard SFT while reducing forgetting on non-downstream benchmarks by 12.34% on average. Our code is available at this https URL .</li>
<li><strong>摘要：</strong>大型语言模型（LLM）已经取得了快速进展，但使其适应下游场景仍然通常依赖于监督微调（SFT）。当下游数据与模型先前的训练分布相比出现显着的分布变化时，SFT 可能会导致灾难性遗忘。为了缩小这一差距，数据重写被提出作为一种以数据为中心的方法，在 SFT 之前重写下游训练数据。然而，现有方法通常从提示引发的条件分布中对重写进行采样，因此生成的目标不一定与模型的自然 QA 式生成分布一致。此外，对固定模板的依赖可能导致多样性崩溃。为了解决这些问题，我们将数据重写作为一个策略学习问题，并学习一种重写策略，该策略可以更好地匹配主干的 QA 式生成分布，同时保留多样性。由于分布对齐、多样性和任务一致性是自动评估的，但很难以可微分的目标进行端到端优化，因此我们利用强化学习来优化奖励反馈下的重写分布，并提出一种基于 RL 的数据重写代理。该代理在硬任务一致性门下联合优化 QA 式的分布对齐和多样性，从而为下游 SFT 构建更高质量的重写数据集。大量实验表明，我们的方法实现了与标准 SFT 相当的下游增益，同时将非下游基准的遗忘平均减少了 12.34%。我们的代码可在此 https URL 获取。</li>
</ul>

<h3>Title: ReTracing: An Archaeological Approach Through Body, Machine, and Generative Systems</h3>
<ul>
<li><strong>Authors: </strong>Yitong Wang, Yue Yao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11242">https://arxiv.org/abs/2602.11242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11242">https://arxiv.org/pdf/2602.11242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11242]] ReTracing: An Archaeological Approach Through Body, Machine, and Generative Systems(https://arxiv.org/abs/2602.11242)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present ReTracing, a multi-agent embodied performance art that adopts an archaeological approach to examine how artificial intelligence shapes, constrains, and produces bodily movement. Drawing from science-fiction novels, the project extracts sentences that describe human-machine interaction. We use large language models (LLMs) to generate paired prompts "what to do" and "what not to do" for each excerpt. A diffusion-based text-to-video model transforms these prompts into choreographic guides for a human performer and motor commands for a quadruped robot. Both agents enact the actions on a mirrored floor, captured by multi-camera motion tracking and reconstructed into 3D point clouds and motion trails, forming a digital archive of motion traces. Through this process, ReTracing serves as a novel approach to reveal how generative systems encode socio-cultural biases through choreographed movements. Through an immersive interplay of AI, human, and robot, ReTracing confronts a critical question of our time: What does it mean to be human among AIs that also move, think, and leave traces behind?</li>
<li><strong>摘要：</strong>我们展示了 ReTracing，这是一种多智能体体现的表演艺术，它采用考古学的方法来研究人工智能如何塑造、约束和产生身体运动。该项目借鉴科幻小说，提取描述人机交互的句子。我们使用大型语言模型 (LLM) 为每个摘录生成配对提示“该做什么”和“不该做什么”。基于扩散的文本到视频模型将这些提示转换为人类表演者的编舞指南和四足机器人的运动命令。两个代理都在镜像地板上执行动作，通过多摄像头运动跟踪捕获并重建为 3D 点云和运动轨迹，形成运动轨迹的数字档案。通过这个过程，ReTracing 作为一种新颖的方法来揭示生成系统如何通过精心设计的动作来编码社会文化偏见。通过人工智能、人类和机器人的沉浸式互动，《ReTracing》面临着我们这个时代的一个关键问题：在同样会移动、思考和留下痕迹的人工智能中，作为人类意味着什么？</li>
</ul>

<h3>Title: Advancing Digital Twin Generation Through a Novel Simulation Framework and Quantitative Benchmarking</h3>
<ul>
<li><strong>Authors: </strong>Jacob Rubinstein, Avi Donaty, Don Engel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11314">https://arxiv.org/abs/2602.11314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11314">https://arxiv.org/pdf/2602.11314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11314]] Advancing Digital Twin Generation Through a Novel Simulation Framework and Quantitative Benchmarking(https://arxiv.org/abs/2602.11314)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The generation of 3D models from real-world objects has often been accomplished through photogrammetry, i.e., by taking 2D photos from a variety of perspectives and then triangulating matched point-based features to create a textured mesh. Many design choices exist within this framework for the generation of digital twins, and differences between such approaches are largely judged qualitatively. Here, we present and test a novel pipeline for generating synthetic images from high-quality 3D models and programmatically generated camera poses. This enables a wide variety of repeatable, quantifiable experiments which can compare ground-truth knowledge of virtual camera parameters and of virtual objects against the reconstructed estimations of those perspectives and subjects.</li>
<li><strong>摘要：</strong>从现实世界的物体生成 3D 模型通常是通过摄影测量来完成的，即从各种角度拍摄 2D 照片，然后对匹配的基于点的特征进行三角测量以创建纹理网格。在这个生成数字孪生的框架内存在许多设计选择，并且这些方法之间的差异在很大程度上是定性判断的。在这里，我们提出并测试了一种新颖的管道，用于从高质量 3D 模型和以编程方式生成的相机姿势生成合成图像。这使得各种可重复、可量化的实验成为可能，这些实验可以将虚拟相机参数和虚拟对象的真实知识与这些视角和主题的重建估计进行比较。</li>
</ul>

<h3>Title: Exploring Real-Time Super-Resolution: Benchmarking and Fine-Tuning for Streaming Content</h3>
<ul>
<li><strong>Authors: </strong>Evgeney Bogatyrev, Khaled Abud, Ivan Molodetskikh, Nikita Alutis, Dmitry Vatolin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11339">https://arxiv.org/abs/2602.11339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11339">https://arxiv.org/pdf/2602.11339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11339]] Exploring Real-Time Super-Resolution: Benchmarking and Fine-Tuning for Streaming Content(https://arxiv.org/abs/2602.11339)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Recent advancements in real-time super-resolution have enabled higher-quality video streaming, yet existing methods struggle with the unique challenges of compressed video content. Commonly used datasets do not accurately reflect the characteristics of streaming media, limiting the relevance of current benchmarks. To address this gap, we introduce a comprehensive dataset - StreamSR - sourced from YouTube, covering a wide range of video genres and resolutions representative of real-world streaming scenarios. We benchmark 11 state-of-the-art real-time super-resolution models to evaluate their performance for the streaming use-case. Furthermore, we propose EfRLFN, an efficient real-time model that integrates Efficient Channel Attention and a hyperbolic tangent activation function - a novel design choice in the context of real-time super-resolution. We extensively optimized the architecture to maximize efficiency and designed a composite loss function that improves training convergence. EfRLFN combines the strengths of existing architectures while improving both visual quality and runtime performance. Finally, we show that fine-tuning other models on our dataset results in significant performance gains that generalize well across various standard benchmarks. We made the dataset, the code, and the benchmark available at this https URL.</li>
<li><strong>摘要：</strong>实时超分辨率的最新进展使得更高质量的视频流成为可能，但现有方法却难以应对压缩视频内容的独特挑战。常用的数据集不能准确反映流媒体的特征，限制了当前基准的相关性。为了解决这一差距，我们引入了一个来自 YouTube 的综合数据集 - StreamSR，涵盖了代表现实世界流媒体场景的各种视频类型和分辨率。我们对 11 个最先进的实时超分辨率模型进行了基准测试，以评估它们在流媒体用例中的性能。此外，我们提出了 EfRLFN，这是一种集成了高效通道注意力和双曲正切激活函数的高效实时模型，这是实时超分辨率背景下的一种新颖的设计选择。我们广泛优化了架构以最大限度地提高效率，并设计了一个复合损失函数来提高训练收敛性。 EfRLFN 结合了现有架构的优势，同时提高了视觉质量和运行时性能。最后，我们表明，对数据集上的其他模型进行微调可以带来显着的性能提升，并且可以在各种标准基准测试中得到很好的推广。我们在此 https URL 上提供了数据集、代码和基准测试。</li>
</ul>

<h3>Title: General and Efficient Steering of Unconditional Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Qingsong Wang, Mikhail Belkin, Yusu Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11395">https://arxiv.org/abs/2602.11395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11395">https://arxiv.org/pdf/2602.11395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11395]] General and Efficient Steering of Unconditional Diffusion(https://arxiv.org/abs/2602.11395)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Guiding unconditional diffusion models typically requires either retraining with conditional inputs or per-step gradient computations (e.g., classifier-based guidance), both of which incur substantial computational overhead. We present a general recipe for efficiently steering unconditional diffusion {without gradient guidance during inference}, enabling fast controllable generation. Our approach is built on two observations about diffusion model structure: Noise Alignment: even in early, highly corrupted stages, coarse semantic steering is possible using a lightweight, offline-computed guidance signal, avoiding any per-step or per-sample gradients. Transferable concept vectors: a concept direction in activation space once learned transfers across both {timesteps} and {samples}; the same fixed steering vector learned near low noise level remains effective when injected at intermediate noise levels for every generation trajectory, providing refined conditional control with efficiency. Such concept directions can be efficiently and reliably identified via Recursive Feature Machine (RFM), a light-weight backpropagation-free feature learning method. Experiments on CIFAR-10, ImageNet, and CelebA demonstrate improved accuracy/quality over gradient-based guidance, while achieving significant inference speedups.</li>
<li><strong>摘要：</strong>指导无条件扩散模型通常需要使用条件输入进行重新训练或每步梯度计算（例如基于分类器的指导），这两者都会产生大量的计算开销。我们提出了一种有效引导无条件扩散的通用方法（推理期间无需梯度引导），从而实现快速可控生成。我们的方法建立在关于扩散模型结构的两个观察之上： 噪声对齐：即使在早期、高度损坏的阶段，也可以使用轻量级、离线计算的引导信号进行粗略语义引导，从而避免任何每步或每样本梯度。可转移概念向量：激活空间中的概念方向一旦学习到跨{timesteps}和{samples}转移；当为每一代轨迹以中间噪声水平注入时，在低噪声水平附近学习到的相同固定转向矢量仍然有效，从而提供高效的精细条件控制。这些概念方向可以通过递归特征机（RFM）（一种轻量级的无反向传播特征学习方法）高效可靠地识别。 CIFAR-10、ImageNet 和 CelebA 上的实验表明，与基于梯度的指导相比，准确性/质量得到了提高，同时实现了显着的推理加速。</li>
</ul>

<h3>Title: Latent Forcing: Reordering the Diffusion Trajectory for Pixel-Space Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Alan Baade, Eric Ryan Chan, Kyle Sargent, Changan Chen, Justin Johnson, Ehsan Adeli, Li Fei-Fei</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11401">https://arxiv.org/abs/2602.11401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11401">https://arxiv.org/pdf/2602.11401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11401]] Latent Forcing: Reordering the Diffusion Trajectory for Pixel-Space Image Generation(https://arxiv.org/abs/2602.11401)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Latent diffusion models excel at generating high-quality images but lose the benefits of end-to-end modeling. They discard information during image encoding, require a separately trained decoder, and model an auxiliary distribution to the raw data. In this paper, we propose Latent Forcing, a simple modification to existing architectures that achieves the efficiency of latent diffusion while operating on raw natural images. Our approach orders the denoising trajectory by jointly processing latents and pixels with separately tuned noise schedules. This allows the latents to act as a scratchpad for intermediate computation before high-frequency pixel features are generated. We find that the order of conditioning signals is critical, and we analyze this to explain differences between REPA distillation in the tokenizer and the diffusion model, conditional versus unconditional generation, and how tokenizer reconstruction quality relates to diffusability. Applied to ImageNet, Latent Forcing achieves a new state-of-the-art for diffusion transformer-based pixel generation at our compute scale.</li>
<li><strong>摘要：</strong>潜在扩散模型擅长生成高质量图像，但失去了端到端建模的优势。它们在图像编码期间丢弃信息，需要单独训练的解码器，并对原始数据的辅助分布进行建模。在本文中，我们提出了潜在强迫，这是对现有架构的简单修改，可在处理原始自然图像时实现潜在扩散的效率。我们的方法通过使用单独调整的噪声计划联合处理潜伏和像素来排序去噪轨迹。这使得潜在变量可以在生成高频像素特征之前充当中间计算的暂存器。我们发现条件信号的顺序至关重要，我们对此进行分析，以解释分词器中的 REPA 蒸馏和扩散模型之间的差异、条件生成与无条件生成，以及分词器重建质量与扩散性的关系。应用于 ImageNet 时，潜在强迫在我们的计算规模上实现了基于扩散变换器的像素生成的最新技术。</li>
</ul>

<h3>Title: CADET: Context-Conditioned Ads CTR Prediction With a Decoder-Only Transformer</h3>
<ul>
<li><strong>Authors: </strong>David Pardoe, Neil Daftary, Miro Furtado, Aditya Aiyer, Yu Wang, Liuqing Li, Tao Song, Lars Hertel, Young Jin Yun, Senthil Radhakrishnan, Zhiwei Wang, Tommy Li, Khai Tran, Ananth Nagarajan, Ali Naqvi, Yue Zhang, Renpeng Fang, Avi Romascanu, Arjun Kulothungun, Deepak Kumar, Praneeth Boda, Fedor Borisyuk, Ruoyan Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11410">https://arxiv.org/abs/2602.11410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11410">https://arxiv.org/pdf/2602.11410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11410]] CADET: Context-Conditioned Ads CTR Prediction With a Decoder-Only Transformer(https://arxiv.org/abs/2602.11410)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Click-through rate (CTR) prediction is fundamental to online advertising systems. While Deep Learning Recommendation Models (DLRMs) with explicit feature interactions have long dominated this domain, recent advances in generative recommenders have shown promising results in content recommendation. However, adapting these transformer-based architectures to ads CTR prediction still presents unique challenges, including handling post-scoring contextual signals, maintaining offline-online consistency, and scaling to industrial workloads. We present CADET (Context-Conditioned Ads Decoder-Only Transformer), an end-to-end decoder-only transformer for ads CTR prediction deployed at LinkedIn. Our approach introduces several key innovations: (1) a context-conditioned decoding architecture with multi-tower prediction heads that explicitly model post-scoring signals such as ad position, resolving the chicken-and-egg problem between predicted CTR and ranking; (2) a self-gated attention mechanism that stabilizes training by adaptively regulating information flow at both representation and interaction levels; (3) a timestamp-based variant of Rotary Position Embedding (RoPE) that captures temporal relationships across timescales from seconds to months; (4) session masking strategies that prevent the model from learning dependencies on unavailable in-session events, addressing train-serve skew; and (5) production engineering techniques including tensor packing, sequence chunking, and custom Flash Attention kernels that enable efficient training and serving at scale. In online A/B testing, CADET achieves a 11.04\% CTR lift compared to the production LiRank baseline model, a hybrid ensemble of DCNv2 and sequential encoders. The system has been successfully deployed on LinkedIn's advertising platform, serving the main traffic for homefeed sponsored updates.</li>
<li><strong>摘要：</strong>点击率 (CTR) 预测是在线广告系统的基础。虽然具有显式特征交互的深度学习推荐模型（DLRM）长期以来一直主导着该领域，但生成推荐器的最新进展在内容推荐方面显示出了有希望的结果。然而，使这些基于 Transformer 的架构适应广告点击率预测仍然面临着独特的挑战，包括处理评分后上下文信号、保持离线在线一致性以及扩展到工业工作负载。我们提出了 CADET（上下文条件广告解码器专用变压器），这是一种用于 LinkedIn 部署的广告点击率预测的端到端解码器专用变压器。我们的方法引入了几个关键创新：（1）具有多塔预测头的上下文条件解码架构，可以显式地对广告位置等评分后信号进行建模，解决预测点击率和排名之间的先有鸡还是先有蛋的问题； （2）自门注意力机制，通过在表示和交互层面自适应调节信息流来稳定训练； (3) 基于时间戳的旋转位置嵌入 (RoPE) 变体，可捕获从几秒到几个月的跨时间尺度的时间关系； (4) 会话屏蔽策略，防止模型学习对不可用会话事件的依赖性，解决列车服务偏差问题； (5) 生产工程技术，包括张量打包、序列分块和自定义 Flash Attention 内核，可实现大规模高效训练和服务。在在线 A/B 测试中，与生产 LiRank 基线模型（DCNv2 和顺序编码器的混合集成）相比，CADET 的 CTR 提升了 11.04%。该系统已成功部署在LinkedIn的广告平台上，为Homefeed赞助更新的主要流量提供服务。</li>
</ul>

<h3>Title: Ctrl&Shift: High-Quality Geometry-Aware Object Manipulation in Visual Generation</h3>
<ul>
<li><strong>Authors: </strong>Penghui Ruan, Bojia Zi, Xianbiao Qi, Youze Huang, Rong Xiao, Pichao Wang, Jiannong Cao, Yuhui Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11440">https://arxiv.org/abs/2602.11440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11440">https://arxiv.org/pdf/2602.11440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11440]] Ctrl&Shift: High-Quality Geometry-Aware Object Manipulation in Visual Generation(https://arxiv.org/abs/2602.11440)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Object-level manipulation, relocating or reorienting objects in images or videos while preserving scene realism, is central to film post-production, AR, and creative editing. Yet existing methods struggle to jointly achieve three core goals: background preservation, geometric consistency under viewpoint shifts, and user-controllable transformations. Geometry-based approaches offer precise control but require explicit 3D reconstruction and generalize poorly; diffusion-based methods generalize better but lack fine-grained geometric control. We present Ctrl&Shift, an end-to-end diffusion framework to achieve geometry-consistent object manipulation without explicit 3D representations. Our key insight is to decompose manipulation into two stages, object removal and reference-guided inpainting under explicit camera pose control, and encode both within a unified diffusion process. To enable precise, disentangled control, we design a multi-task, multi-stage training strategy that separates background, identity, and pose signals across tasks. To improve generalization, we introduce a scalable real-world dataset construction pipeline that generates paired image and video samples with estimated relative camera poses. Extensive experiments demonstrate that Ctrl&Shift achieves state-of-the-art results in fidelity, viewpoint consistency, and controllability. To our knowledge, this is the first framework to unify fine-grained geometric control and real-world generalization for object manipulation, without relying on any explicit 3D modeling.</li>
<li><strong>摘要：</strong>对象级操作，即重新定位或重新定向图像或视频中的对象，同时保留场景真实感，是电影后期制作、AR 和创意编辑的核心。然而现有的方法很难共同实现三个核心目标：背景保留、视点变换下的几何一致性以及用户可控的变换。基于几何的方法提供精确控制，但需要显式 3D 重建，且泛化能力较差；基于扩散的方法概括性更好，但缺乏细粒度的几何控制。我们提出了 Ctrl&Shift，一个端到端扩散框架，无需显式 3D 表示即可实现几何一致的对象操作。我们的主要见解是将操作分解为两个阶段，即在显式相机姿势控制下的对象移除和参考引导修复，并在统一的扩散过程中对这两个阶段进行编码。为了实现精确、分离的控制，我们设计了一种多任务、多阶段的训练策略，可以跨任务分离背景、身份和姿势信号。为了提高泛化能力，我们引入了一个可扩展的现实世界数据集构建管道，该管道可生成具有估计的相对相机姿势的配对图像和视频样本。大量实验表明，Ctrl&Shift 在保真度、视点一致性和可控性方面实现了最先进的结果。据我们所知，这是第一个将细粒度几何控制和对象操作的现实世界泛化统一起来的框架，而不依赖于任何显式的 3D 建模。</li>
</ul>

<h3>Title: Enhanced Portable Ultra Low-Field Diffusion Tensor Imaging with Bayesian Artifact Correction and Deep Learning-Based Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Mark D. Olchanyi, Annabel Sorby-Adams, John Kirsch, Brian L. Edlow, Ava Farnan, Renfei Liu, Matthew S. Rosen, Emery N. Brown, W. Taylor Kimberly, Juan Eugenio Iglesias</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11446">https://arxiv.org/abs/2602.11446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11446">https://arxiv.org/pdf/2602.11446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11446]] Enhanced Portable Ultra Low-Field Diffusion Tensor Imaging with Bayesian Artifact Correction and Deep Learning-Based Super-Resolution(https://arxiv.org/abs/2602.11446)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Portable, ultra-low-field (ULF) magnetic resonance imaging has the potential to expand access to neuroimaging but currently suffers from coarse spatial and angular resolutions and low signal-to-noise ratios. Diffusion tensor imaging (DTI), a sequence tailored to detect and reconstruct white matter tracts within the brain, is particularly prone to such imaging degradation due to inherent sequence design coupled with prolonged scan times. In addition, ULF DTI scans exhibit artifacting that spans both the space and angular domains, requiring a custom modelling algorithm for subsequent correction. We introduce a nine-direction, single-shell ULF DTI sequence, as well as a companion Bayesian bias field correction algorithm that possesses angular dependence and convolutional neural network-based superresolution algorithm that is generalizable across DTI datasets and does not require re-training (''DiffSR''). We show through a synthetic downsampling experiment and white matter assessment in real, matched ULF and high-field DTI scans that these algorithms can recover microstructural and volumetric white matter information at ULF. We also show that DiffSR can be directly applied to white matter-based Alzheimers disease classification in synthetically degraded scans, with notable improvements in agreement between DTI metrics, as compared to un-degraded scans. We freely disseminate the Bayesian bias correction algorithm and DiffSR with the goal of furthering progress on both ULF reconstruction methods and general DTI sequence harmonization. We release all code related to DiffSR for $\href{this https URL}{public \space use}$.</li>
<li><strong>摘要：</strong>便携式超低场 (ULF) 磁共振成像有潜力扩大神经成像的应用范围，但目前存在空间和角度分辨率较差以及信噪比较低的问题。弥散张量成像（DTI）是一种专门用于检测和重建大脑内白质束的序列，由于固有的序列设计加上较长的扫描时间，特别容易出现这种成像质量下降的情况。此外，ULF DTI 扫描表现出跨越空间和角度域的伪影，需要自定义建模算法进行后续校正。我们引入了一个九方向、单壳 ULF DTI 序列，以及一个配套的贝叶斯偏差场校正算法，该算法具有角度依赖性和基于卷积神经网络的超分辨率算法，该算法可在 DTI 数据集上推广，并且不需要重新训练（“DiffSR”）。我们通过真实、匹配的 ULF 和高场 DTI 扫描中的合成下采样实验和白质评估表明，这些算法可以恢复 ULF 下的微观结构和体积白质信息。我们还表明，DiffSR 可以直接应用于合成降解扫描中基于白质的阿尔茨海默病分类，与未降解扫描相比，DTI 指标之间的一致性显着改善。我们免费传播贝叶斯偏差校正算法和 DiffSR，目标是进一步推动 ULF 重建方法和通用 DTI 序列协调的进展。我们为 $\href{此 https URL}{public \space use}$ 发布与 DiffSR 相关的所有代码。</li>
</ul>

<h3>Title: Assessing Low Back Movement with Motion Tape Sensor Data Through Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Jared Levy, Aarti Lalwani, Elijah Wyckoff, Kenneth J. Loh, Sara P. Gombatto, Rose Yu, Emilia Farcas</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11465">https://arxiv.org/abs/2602.11465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11465">https://arxiv.org/pdf/2602.11465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11465]] Assessing Low Back Movement with Motion Tape Sensor Data Through Deep Learning(https://arxiv.org/abs/2602.11465)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Back pain is a pervasive issue affecting a significant portion of the population, often worsened by certain movements of the lower back. Assessing these movements is important for helping clinicians prescribe appropriate physical therapy. However, it can be difficult to monitor patients' movements remotely outside the clinic. High-fidelity data from motion capture sensors can be used to classify different movements, but these sensors are costly and impractical for use in free-living environments. Motion Tape (MT), a new fabric-based wearable sensor, addresses these issues by being low cost and portable. Despite these advantages, novelty and variability in sensor stability make the MT dataset small scale and inherent to noise. In this work, we propose the Motion-Tape Augmentation Inference Model (MT-AIM), a deep learning classification pipeline trained on MT data. In order to address the challenges of limited sample size and noise present within the MT dataset, MT-AIM leverages conditional generative models to generate synthetic MT data of a desired movement, as well as predicting joint kinematics as additional features. This combination of synthetic data generation and feature augmentation enables MT-AIM to achieve state-of-the-art accuracy in classifying lower back movements, bridging the gap between physiological sensing and movement analysis.</li>
<li><strong>摘要：</strong>背痛是影响很大一部分人口的普遍问题，通常因下背部的某些运动而加剧。评估这些运动对于帮助临床医生制定适当的物理治疗非常重要。然而，在诊所外远程监控患者的活动可能很困难。来自运动捕捉传感器的高保真数据可用于对不同的运动进行分类，但这些传感器成本高昂且不适合在自由生活环境中使用。 Motion Tape (MT) 是一种新型的基于织物的可穿戴传感器，通过低成本和便携性解决了这些问题。尽管有这些优点，但传感器稳定性的新颖性和可变性使得 MT 数据集规模较小并且存在固有的噪声。在这项工作中，我们提出了 Motion-Tape 增强推理模型 (MT-AIM)，这是一种在 MT 数据上训练的深度学习分类管道。为了解决 MT 数据集中存在的样本量有限和噪声的挑战，MT-AIM 利用条件生成模型来生成所需运动的合成 MT 数据，并预测关节运动学作为附加特征。合成数据生成和特征增强的结合使 MT-AIM 能够在分类下背部运动方面实现最先进的准确性，从而弥合生理传感和运动分析之间的差距。</li>
</ul>

<h3>Title: External Division of Two Bregman Proximity Operators for Poisson Inverse Problems</h3>
<ul>
<li><strong>Authors: </strong>Kazuki Haishima, Kyohei Suzuki, Konstantinos Slavakis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11482">https://arxiv.org/abs/2602.11482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11482">https://arxiv.org/pdf/2602.11482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11482]] External Division of Two Bregman Proximity Operators for Poisson Inverse Problems(https://arxiv.org/abs/2602.11482)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>This paper presents a novel method for recovering sparse vectors from linear models corrupted by Poisson noise. The contribution is twofold. First, an operator defined via the external division of two Bregman proximity operators is introduced to promote sparse solutions while mitigating the estimation bias induced by classical $\ell_1$-norm regularization. This operator is then embedded into the already established NoLips algorithm, replacing the standard Bregman proximity operator in a plug-and-play manner. Second, the geometric structure of the proposed external-division operator is elucidated through two complementary reformulations, which provide clear interpretations in terms of the primal and dual spaces of the Poisson inverse problem. Numerical tests show that the proposed method exhibits more stable convergence behavior than conventional Kullback-Leibler (KL)-based approaches and achieves significantly superior performance on synthetic data and an image restoration problem.</li>
<li><strong>摘要：</strong>本文提出了一种从泊松噪声损坏的线性模型中恢复稀疏向量的新方法。贡献是双重的。首先，引入通过两个 Bregman 邻近算子的外除定义的算子来促进稀疏解，同时减轻经典 $\ell_1$-norm 正则化引起的估计偏差。然后将该算子嵌入到已经建立的 NoLips 算法中，以即插即用的方式取代标准 Bregman 邻近算子。其次，通过两个互补的重新表述阐明了所提出的外除算子的几何结构，这为泊松反演问题的原始空间和对偶空间提供了清晰的解释。数值测试表明，所提出的方法比传统的基于 Kullback-Leibler (KL) 的方法表现出更稳定的收敛行为，并且在合成数据和图像恢复问题上实现了显着优越的性能。</li>
</ul>

<h3>Title: Exploring Multiple High-Scoring Subspaces in Generative Flow Networks</h3>
<ul>
<li><strong>Authors: </strong>Xuan Yu, Xu Wang, Rui Zhu, Yudong Zhang, Yang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11491">https://arxiv.org/abs/2602.11491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11491">https://arxiv.org/pdf/2602.11491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11491]] Exploring Multiple High-Scoring Subspaces in Generative Flow Networks(https://arxiv.org/abs/2602.11491)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>As a probabilistic sampling framework, Generative Flow Networks (GFlowNets) show strong potential for constructing complex combinatorial objects through the sequential composition of elementary components. However, existing GFlowNets often suffer from excessive exploration over vast state spaces, leading to over-sampling of low-reward regions and convergence to suboptimal distributions. Effectively biasing GFlowNets toward high-reward solutions remains a non-trivial challenge. In this paper, we propose CMAB-GFN, which integrates a combinatorial multi-armed bandit (CMAB) framework with GFlowNet policies. The CMAB component prunes low-quality actions, yielding compact high-scoring subspaces for exploration. Restricting GFNs to these compact high-scoring subspaces accelerates the discovery of high-value candidates, while the exploration of different subspaces ensures that diversity is not sacrificed. Experimental results on multiple tasks demonstrate that CMAB-GFN generates higher-reward candidates than existing approaches.</li>
<li><strong>摘要：</strong>作为一种概率采样框架，生成流网络（GFlowNets）在通过基本组件的顺序组合构建复杂的组合对象方面表现出强大的潜力。然而，现有的 GFlowNet 经常遭受对巨大状态空间的过度探索，导致低奖励区域的过度采样并收敛到次优分布。有效地将 GFlowNet 偏向高回报解决方案仍然是一个不小的挑战。在本文中，我们提出了 CMAB-GFN，它将组合多臂老虎机（CMAB）框架与 GFlowNet 策略集成在一起。 CMAB 组件修剪低质量的动作，产生紧凑的高分子空间用于探索。将 GFN 限制在这些紧凑的高分子空间可以加速高价值候选者的发现，而对不同子空间的探索可以确保不牺牲多样性。多项任务的实验结果表明，CMAB-GFN 比现有方法产生更高奖励的候选者。</li>
</ul>

<h3>Title: Partial GFlowNet: Accelerating Convergence in Large State Spaces via Strategic Partitioning</h3>
<ul>
<li><strong>Authors: </strong>Xuan Yu, Xu Wang, Rui Zhu, Yudong Zhang, Yang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11498">https://arxiv.org/abs/2602.11498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11498">https://arxiv.org/pdf/2602.11498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11498]] Partial GFlowNet: Accelerating Convergence in Large State Spaces via Strategic Partitioning(https://arxiv.org/abs/2602.11498)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative Flow Networks (GFlowNets) have shown promising potential to generate high-scoring candidates with probability proportional to their rewards. As existing GFlowNets freely explore in state space, they encounter significant convergence challenges when scaling to large state spaces. Addressing this issue, this paper proposes to restrict the exploration of actor. A planner is introduced to partition the entire state space into overlapping partial state spaces. Given their limited size, these partial state spaces allow the actor to efficiently identify subregions with higher rewards. A heuristic strategy is introduced to switch partial regions thus preventing the actor from wasting time exploring fully explored or low-reward partial regions. By iteratively exploring these partial state spaces, the actor learns to converge towards the high-reward subregions within the entire state space. Experiments on several widely used datasets demonstrate that \modelname converges faster than existing works on large state spaces. Furthermore, \modelname not only generates candidates with higher rewards but also significantly improves their diversity.</li>
<li><strong>摘要：</strong>生成流网络（GFlowNets）已显示出产生高分候选者的巨大潜力，其概率与其奖励成正比。由于现有的 GFlowNet 在状态空间中自由探索，因此在扩展到大型状态空间时遇到了重大的收敛挑战。针对这个问题，本文提出限制Actor的探索。引入规划器将整个状态空间划分为重叠的部分状态空间。鉴于其有限的大小，这些部分状态空间允许参与者有效地识别具有更高奖励的子区域。引入启发式策略来切换部分区域，从而防止参与者浪费时间探索完全探索或低回报的部分区域。通过迭代探索这些部分状态空间，参与者学会向整个状态空间内的高奖励子区域收敛。对几个广泛使用的数据集的实验表明，\modelname 比大型状态空间上的现有工作收敛得更快。此外，\modelname 不仅生成具有更高奖励的候选者，而且还显着提高了他们的多样性。</li>
</ul>

<h3>Title: What if Agents Could Imagine? Reinforcing Open-Vocabulary HOI Comprehension through Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhenlong Yuan, Xiangyan Qu, Jing Tang, Rui Chen, Lei Sun, Ruidong Chen, Hongwei Yu, Chengxuan Qian, Xiangxiang Chu, Shuo Li, Yuyin Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11499">https://arxiv.org/abs/2602.11499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11499">https://arxiv.org/pdf/2602.11499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11499]] What if Agents Could Imagine? Reinforcing Open-Vocabulary HOI Comprehension through Generation(https://arxiv.org/abs/2602.11499)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models have shown promising capabilities in bridging visual and textual reasoning, yet their reasoning capabilities in Open-Vocabulary Human-Object Interaction (OV-HOI) are limited by cross-modal hallucinations and occlusion-induced ambiguity. To address this, we propose \textbf{ImagineAgent}, an agentic framework that harmonizes cognitive reasoning with generative imagination for robust visual understanding. Specifically, our method innovatively constructs cognitive maps that explicitly model plausible relationships between detected entities and candidate actions. Subsequently, it dynamically invokes tools including retrieval augmentation, image cropping, and diffusion models to gather domain-specific knowledge and enriched visual evidence, thereby achieving cross-modal alignment in ambiguous scenarios. Moreover, we propose a composite reward that balances prediction accuracy and tool efficiency. Evaluations on SWIG-HOI and HICO-DET datasets demonstrate our SOTA performance, requiring approximately 20\% of training data compared to existing methods, validating our robustness and efficiency.</li>
<li><strong>摘要：</strong>多模态大语言模型在桥接视觉和文本推理方面表现出了良好的能力，但它们在开放词汇人机交互（OV-HOI）中的推理能力受到跨模态幻觉和遮挡引起的歧义的限制。为了解决这个问题，我们提出了 \textbf{ImagineAgent}，一个代理框架，它将认知推理与生成想象力相协调，以实现强大的视觉理解。具体来说，我们的方法创新地构建了认知图，该认知图明确地模拟了检测到的实体和候选动作之间的合理关系。随后，它动态调用包括检索增强、图像裁剪和扩散模型在内的工具来收集特定领域的知识和丰富的视觉证据，从而实现模糊场景中的跨模态对齐。此外，我们提出了一种平衡预测准确性和工具效率的复合奖励。对 SWIG-HOI 和 HICO-DET 数据集的评估展示了我们的 SOTA 性能，与现有方法相比，需要大约 20% 的训练数据，验证了我们的稳健性和效率。</li>
</ul>

<h3>Title: Krause Synchronization Transformers</h3>
<ul>
<li><strong>Authors: </strong>Jingkun Liu, Yisong Yue, Max Welling, Yue Song</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11534">https://arxiv.org/abs/2602.11534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11534">https://arxiv.org/pdf/2602.11534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11534]] Krause Synchronization Transformers(https://arxiv.org/abs/2602.11534)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Self-attention in Transformers relies on globally normalized softmax weights, causing all tokens to compete for influence at every layer. When composed across depth, this interaction pattern induces strong synchronization dynamics that favor convergence toward a dominant mode, a behavior associated with representation collapse and attention sink phenomena. We introduce Krause Attention, a principled attention mechanism inspired by bounded-confidence consensus dynamics. Krause Attention replaces similarity-based global aggregation with distance-based, localized, and selectively sparse interactions, promoting structured local synchronization instead of global mixing. We relate this behavior to recent theory modeling Transformer dynamics as interacting particle systems, and show how bounded-confidence interactions naturally moderate attention concentration and alleviate attention sinks. Restricting interactions to local neighborhoods also reduces runtime complexity from quadratic to linear in sequence length. Experiments across vision (ViT on CIFAR/ImageNet), autoregressive generation (MNIST/CIFAR-10), and large language models (Llama/Qwen) demonstrate consistent gains with substantially reduced computation, highlighting bounded-confidence dynamics as a scalable and effective inductive bias for attention.</li>
<li><strong>摘要：</strong>Transformers 中的自注意力依赖于全局标准化的 softmax 权重，导致所有代币在每一层争夺影响力。当跨深度组合时，这种交互模式会引发强烈的同步动态，有利于向主导模式收敛，这是一种与表征崩溃和注意力下沉现象相关的行为。我们引入了 Krause Attention，这是一种受有限置信共识动态启发的原则性注意力机制。 Krause Attention 用基于距离的、局部的、选择性稀疏的交互取代了基于相似性的全局聚合，促进了结构化的局部同步而不是全局混合。我们将这种行为与最近将 Transformer 动力学建模为相互作用的粒子系统的理论联系起来，并展示了有限置信相互作用如何自然地调节注意力集中并减轻注意力下沉。限制与局部邻域的交互还可以将运行时复杂性从序列长度的二次降低到线性。跨视觉（CIFAR/ImageNet 上的 ViT）、自回归生成（MNIST/CIFAR-10）和大型语言模型（Llama/Qwen）的实验证明了在大幅减少计算的情况下获得的一致收益，强调有限置信动态作为一种可扩展且有效的注意力归纳偏差。</li>
</ul>

<h3>Title: Supervise-assisted Multi-modality Fusion Diffusion Model for PET Restoration</h3>
<ul>
<li><strong>Authors: </strong>Yingkai Zhang, Shuang Chen, Ye Tian, Yunyi Gao, Jianyong Jiang, Ying Fu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11545">https://arxiv.org/abs/2602.11545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11545">https://arxiv.org/pdf/2602.11545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11545]] Supervise-assisted Multi-modality Fusion Diffusion Model for PET Restoration(https://arxiv.org/abs/2602.11545)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Positron emission tomography (PET) offers powerful functional imaging but involves radiation exposure. Efforts to reduce this exposure by lowering the radiotracer dose or scan time can degrade image quality. While using magnetic resonance (MR) images with clearer anatomical information to restore standard-dose PET (SPET) from low-dose PET (LPET) is a promising approach, it faces challenges with the inconsistencies in the structure and texture of multi-modality fusion, as well as the mismatch in out-of-distribution (OOD) data. In this paper, we propose a supervise-assisted multi-modality fusion diffusion model (MFdiff) for addressing these challenges for high-quality PET restoration. Firstly, to fully utilize auxiliary MR images without introducing extraneous details in the restored image, a multi-modality feature fusion module is designed to learn an optimized fusion feature. Secondly, using the fusion feature as an additional condition, high-quality SPET images are iteratively generated based on the diffusion model. Furthermore, we introduce a two-stage supervise-assisted learning strategy that harnesses both generalized priors from simulated in-distribution datasets and specific priors tailored to in-vivo OOD data. Experiments demonstrate that the proposed MFdiff effectively restores high-quality SPET images from multi-modality inputs and outperforms state-of-the-art methods both qualitatively and quantitatively.</li>
<li><strong>摘要：</strong>正电子发射断层扫描 (PET) 提供强大的功能成像，但涉及辐射暴露。通过降低放射性示踪剂剂量或扫描时间来减少这种暴露的努力可能会降低图像质量。虽然使用具有更清晰解剖信息的磁共振（MR）图像从低剂量PET（LPET）恢复标准剂量PET（SPET）是一种很有前途的方法，但它面临着多模态融合的结构和纹理不一致以及分布外（OOD）数据不匹配的挑战。在本文中，我们提出了一种监督辅助的多模态融合扩散模型（MFdiff）来解决高质量 PET 修复的这些挑战。首先，为了充分利用辅助MR图像而不在恢复图像中引入无关细节，设计了多模态特征融合模块来学习优化的融合特征。其次，以融合特征作为附加条件，基于扩散模型迭代生成高质量的SPET图像。此外，我们引入了一种两阶段监督辅助学习策略，该策略利用来自模拟分布数据集的广义先验和针对体内 OOD 数据定制的特定先验。实验表明，所提出的 MFdiff 可以有效地从多模态输入中恢复高质量的 SPET 图像，并且在定性和定量上都优于最先进的方法。</li>
</ul>

<h3>Title: Perception-based Image Denoising via Generative Compression</h3>
<ul>
<li><strong>Authors: </strong>Nam Nguyen, Thinh Nguyen, Bella Bose</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11553">https://arxiv.org/abs/2602.11553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11553">https://arxiv.org/pdf/2602.11553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11553]] Perception-based Image Denoising via Generative Compression(https://arxiv.org/abs/2602.11553)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generative</a></li>
<li><strong>Abstract: </strong>Image denoising aims to remove noise while preserving structural details and perceptual realism, yet distortion-driven methods often produce over-smoothed reconstructions, especially under strong noise and distribution shift. This paper proposes a generative compression framework for perception-based denoising, where restoration is achieved by reconstructing from entropy-coded latent representations that enforce low-complexity structure, while generative decoders recover realistic textures via perceptual measures such as learned perceptual image patch similarity (LPIPS) loss and Wasserstein distance. Two complementary instantiations are introduced: (i) a conditional Wasserstein GAN (WGAN)-based compression denoiser that explicitly controls the rate-distortion-perception (RDP) trade-off, and (ii) a conditional diffusion-based reconstruction strategy that performs iterative denoising guided by compressed latents. We further establish non-asymptotic guarantees for the compression-based maximum-likelihood denoiser under additive Gaussian noise, including bounds on reconstruction error and decoding error probability. Experiments on synthetic and real-noise benchmarks demonstrate consistent perceptual improvements while maintaining competitive distortion performance.</li>
<li><strong>摘要：</strong>图像去噪旨在去除噪声，同时保留结构细节和感知真实感，但失真驱动的方法通常会产生过度平滑的重建，特别是在强噪声和分布偏移的情况下。本文提出了一种基于感知的去噪的生成压缩框架，其中通过从强制低复杂度结构的熵编码潜在表示重建来实现恢复，而生成解码器通过感知测量（例如学习感知图像块相似性（LPIPS）损失和 Wasserstein 距离）恢复真实纹理。引入了两个互补的实例：(i) 基于条件 Wasserstein GAN (WGAN) 的压缩降噪器，显式控制速率-失真-感知 (RDP) 权衡；(ii) 基于条件扩散的重建策略，在压缩潜值的指导下执行迭代降噪。我们进一步为加性高斯噪声下基于压缩的最大似然降噪器建立非渐近保证，包括重建误差和解码误差概率的界限。合成和真实噪声基准的实验表明，在保持有竞争力的失真性能的同时，感知得到了一致的改进。</li>
</ul>

<h3>Title: LUVE : Latent-Cascaded Ultra-High-Resolution Video Generation with Dual Frequency Experts</h3>
<ul>
<li><strong>Authors: </strong>Chen Zhao, Jiawei Chen, Hongyu Li, Zhuoliang Kang, Shilin Lu, Xiaoming Wei, Kai Zhang, Jian Yang, Ying Tai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11564">https://arxiv.org/abs/2602.11564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11564">https://arxiv.org/pdf/2602.11564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11564]] LUVE : Latent-Cascaded Ultra-High-Resolution Video Generation with Dual Frequency Experts(https://arxiv.org/abs/2602.11564)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in video diffusion models have significantly improved visual quality, yet ultra-high-resolution (UHR) video generation remains a formidable challenge due to the compounded difficulties of motion modeling, semantic planning, and detail synthesis. To address these limitations, we propose \textbf{LUVE}, a \textbf{L}atent-cascaded \textbf{U}HR \textbf{V}ideo generation framework built upon dual frequency \textbf{E}xperts. LUVE employs a three-stage architecture comprising low-resolution motion generation for motion-consistent latent synthesis, video latent upsampling that performs resolution upsampling directly in the latent space to mitigate memory and computational overhead, and high-resolution content refinement that integrates low-frequency and high-frequency experts to jointly enhance semantic coherence and fine-grained detail generation. Extensive experiments demonstrate that our LUVE achieves superior photorealism and content fidelity in UHR video generation, and comprehensive ablation studies further validate the effectiveness of each component. The project is available at \href{this https URL}{this https URL}.</li>
<li><strong>摘要：</strong>视频扩散模型的最新进展显着提高了视觉质量，但由于运动建模、语义规划和细节合成的复杂困难，超高分辨率 (UHR) 视频生成仍然是一个艰巨的挑战。为了解决这些限制，我们提出了 \textbf{LUVE}，一个基于双频 \textbf{E}xperts 构建的 \textbf{L}atent 级联 \textbf{U}HR \textbf{V}ideo 生成框架。 LUVE 采用三阶段架构，包括用于运动一致潜在合成的低分辨率运动生成、直接在潜在空间中执行分辨率上采样以减轻内存和计算开销的视频潜在上采样，以及集成低频和高频专家以共同增强语义一致性和细粒度细节生成的高分辨率内容细化。大量实验表明，我们的 LUVE 在 UHR 视频生成中实现了卓越的照片真实感和内容保真度，全面的消融研究进一步验证了每个组件的有效性。该项目位于 \href{此 https URL}{此 https URL}。</li>
</ul>

<h3>Title: Learn from Your Mistakes: Self-Correcting Masked Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yair Schiff, Omer Belhasin, Roy Uziel, Guanghan Wang, Marianne Arriola, Gilad Turok, Michael Elad, Volodymyr Kuleshov</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11590">https://arxiv.org/abs/2602.11590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11590">https://arxiv.org/pdf/2602.11590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11590]] Learn from Your Mistakes: Self-Correcting Masked Diffusion Models(https://arxiv.org/abs/2602.11590)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Masked diffusion models (MDMs) have emerged as a promising alternative to autoregressive models, enabling parallel token generation while achieving competitive performance. Despite these advantages, MDMs face a fundamental limitation: once tokens are unmasked, they remain fixed, leading to error accumulation and ultimately degrading sample quality. We address this by proposing a framework that trains a model to perform both unmasking and correction. By reusing outputs from the MDM denoising network as inputs for corrector training, we train a model to recover from potential mistakes. During generation we apply additional corrective refinement steps between unmasking ones in order to change decoded tokens and improve outputs. We name our training and sampling method Progressive Self-Correction (ProSeCo) for its unique ability to iteratively refine an entire sequence, including already generated tokens. We conduct extensive experimental validation across multiple conditional and unconditional tasks, demonstrating that ProSeCo yields better quality-efficiency trade-offs (up to ~2-3x faster sampling) and enables inference-time compute scaling to further increase sample quality beyond standard MDMs (up to ~1.3x improvement on benchmarks).</li>
<li><strong>摘要：</strong>掩蔽扩散模型 (MDM) 已成为自回归模型的一种有前景的替代方案，可实现并行代币生成，同时实现有竞争力的性能。尽管有这些优势，MDM 仍面临一个根本性的限制：一旦标记被揭露，它们就保持固定，导致错误累积并最终降低样本质量。我们通过提出一个框架来训练模型来执行揭露和校正来解决这个问题。通过重复使用 MDM 去噪网络的输出作为校正器训练的输入，我们训练模型以从潜在错误中恢复。在生成过程中，我们在解密步骤之间应用额外的纠正细化步骤，以便更改解码的令牌并改进输出。我们将我们的训练和采样方法命名为渐进式自校正（ProSeCo），因为它具有迭代细化整个序列（包括已生成的标记）的独特能力。我们在多个条件和无条件任务中进行了广泛的实验验证，证明 ProSeCo 可以实现更好的质量效率权衡（采样速度提高约 2-3 倍），并支持推理时间计算扩展，以进一步提高样本质量，超越标准 MDM（基准提高约 1.3 倍）。</li>
</ul>

<h3>Title: PLOT-CT: Pre-log Voronoi Decomposition Assisted Generation for Low-dose CT Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Bin Huang, Xun Yu, Yikun Zhang, Yi Zhang, Yang Chen, Qiegen Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11625">https://arxiv.org/abs/2602.11625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11625">https://arxiv.org/pdf/2602.11625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11625]] PLOT-CT: Pre-log Voronoi Decomposition Assisted Generation for Low-dose CT Reconstruction(https://arxiv.org/abs/2602.11625)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Low-dose computed tomography (LDCT) reconstruction is fundamentally challenged by severe noise and compromised data fidelity under reduced radiation exposure. Most existing methods operate either in the image or post-log projection domain, which fails to fully exploit the rich structural information in pre-log measurements while being highly susceptible to noise. The requisite logarithmic transformation critically amplifies noise within these data, imposing exceptional demands on reconstruction precision. To overcome these challenges, we propose PLOT-CT, a novel framework for Pre-Log vOronoi decomposiTion-assisted CT generation. Our method begins by applying Voronoi decomposition to pre-log sinograms, disentangling the data into distinct underlying components, which are embedded in separate latent spaces. This explicit decomposition significantly enhances the model's capacity to learn discriminative features, directly improving reconstruction accuracy by mitigating noise and preserving information inherent in the pre-log domain. Extensive experiments demonstrate that PLOT-CT achieves state-of-the-art performance, attaining a 2.36dB PSNR improvement over traditional methods at the 1e4 incident photon level in the pre-log domain.</li>
<li><strong>摘要：</strong>低剂量计算机断层扫描 (LDCT) 重建从根本上受到严重噪声和辐射照射减少下数据保真度受损的挑战。大多数现有方法要么在图像要么在对数后投影域中运行，无法充分利用对数前测量中丰富的结构信息，同时极易受到噪声的影响。必要的对数变换极大地放大了这些数据中的噪声，对重建精度提出了特殊的要求。为了克服这些挑战，我们提出了 PLOT-CT，这是一种用于 Pre-Log voronoi 分解辅助 CT 生成的新颖框架。我们的方法首先将 Voronoi 分解应用于预对数正弦图，将数据分解为不同的底层组件，这些组件嵌入到单独的潜在空间中。这种显式分解显着增强了模型学习判别特征的能力，通过减轻噪声和保留前日志域中固有的信息来直接提高重建精度。大量实验表明，PLOT-CT 实现了最先进的性能，在前对数域中的 1e4 入射光子水平上，PSNR 比传统方法提高了 2.36dB。</li>
</ul>

<h3>Title: GR-Diffusion: 3D Gaussian Representation Meets Diffusion in Whole-Body PET Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Mengxiao Geng, Zijie Chen, Ran Hong, Bingxuan Li, Qiegen Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11653">https://arxiv.org/abs/2602.11653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11653">https://arxiv.org/pdf/2602.11653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11653]] GR-Diffusion: 3D Gaussian Representation Meets Diffusion in Whole-Body PET Reconstruction(https://arxiv.org/abs/2602.11653)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Positron emission tomography (PET) reconstruction is a critical challenge in molecular imaging, often hampered by noise amplification, structural blurring, and detail loss due to sparse sampling and the ill-posed nature of inverse problems. The three-dimensional discrete Gaussian representation (GR), which efficiently encodes 3D scenes using parameterized discrete Gaussian distributions, has shown promise in computer vision. In this work, we pro-pose a novel GR-Diffusion framework that synergistically integrates the geometric priors of GR with the generative power of diffusion models for 3D low-dose whole-body PET reconstruction. GR-Diffusion employs GR to generate a reference 3D PET image from projection data, establishing a physically grounded and structurally explicit benchmark that overcomes the low-pass limitations of conventional point-based or voxel-based methods. This reference image serves as a dual guide during the diffusion process, ensuring both global consistency and local accuracy. Specifically, we employ a hierarchical guidance mechanism based on the GR reference. Fine-grained guidance leverages differences to refine local details, while coarse-grained guidance uses multi-scale difference maps to correct deviations. This strategy allows the diffusion model to sequentially integrate the strong geometric prior from GR and recover sub-voxel information. Experimental results on the UDPET and Clinical datasets with varying dose levels show that GR-Diffusion outperforms state-of-the-art methods in enhancing 3D whole-body PET image quality and preserving physiological details.</li>
<li><strong>摘要：</strong>正电子发射断层扫描 (PET) 重建是分子成像中的一项关键挑战，通常会受到噪声放大、结构模糊以及由于稀疏采样和逆问题的不适定性质导致的细节损失的阻碍。三维离散高斯表示 (GR) 使用参数化离散高斯分布有效地对 3D 场景进行编码，在计算机视觉领域展现出了良好的前景。在这项工作中，我们提出了一种新颖的 GR-Diffusion 框架，该框架将 GR 的几何先验与扩散模型的生成能力协同集成，用于 3D 低剂量全身 PET 重建。 GR-Diffusion 采用 GR 从投影数据生成参考 3D PET 图像，建立物理基础和结构明确的基准，克服传统基于点或基于体素的方法的低通限制。该参考图像在扩散过程中充当双重指导，确保全局一致性和局部准确性。具体来说，我们采用了基于GR参考的分层指导机制。细粒度指导利用差异来细化局部细节，而粗粒度指导则使用多尺度差异图来纠正偏差。该策略允许扩散模型顺序整合来自 GR 的强几何先验并恢复子体素信息。不同剂量水平的 UDPET 和临床数据集的实验结果表明，GR-Diffusion 在增强 3D 全身 PET 图像质量和保留生理细节方面优于最先进的方法。</li>
</ul>

<h3>Title: EmoSpace: Fine-Grained Emotion Prototype Learning for Immersive Affective Content Generation</h3>
<ul>
<li><strong>Authors: </strong>Bingyuan Wang, Xingbei Chen, Zongyang Qiu, Linping Yuan, Zeyu Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11658">https://arxiv.org/abs/2602.11658</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11658">https://arxiv.org/pdf/2602.11658</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11658]] EmoSpace: Fine-Grained Emotion Prototype Learning for Immersive Affective Content Generation(https://arxiv.org/abs/2602.11658)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Emotion is important for creating compelling virtual reality (VR) content. Although some generative methods have been applied to lower the barrier to creating emotionally rich content, they fail to capture the nuanced emotional semantics and the fine-grained control essential for immersive experiences. To address these limitations, we introduce EmoSpace, a novel framework for emotion-aware content generation that learns dynamic, interpretable emotion prototypes through vision-language alignment. We employ a hierarchical emotion representation with rich learnable prototypes that evolve during training, enabling fine-grained emotional control without requiring explicit emotion labels. We develop a controllable generation pipeline featuring multi-prototype guidance, temporal blending, and attention reweighting that supports diverse applications, including emotional image outpainting, stylized generation, and emotional panorama generation for VR environments. Our experiments demonstrate the superior performance of EmoSpace over existing methods in both qualitative and quantitative evaluations. Additionally, we present a comprehensive user study investigating how VR environments affect emotional perception compared to desktop settings. Our work facilitates immersive visual content generation with fine-grained emotion control and supports applications like therapy, education, storytelling, artistic creation, and cultural preservation. Code and models will be made publicly available.</li>
<li><strong>摘要：</strong>情感对于创建引人入胜的虚拟现实 (VR) 内容非常重要。尽管一些生成方法已被用来降低创建情感丰富的内容的障碍，但它们无法捕捉细致入微的情感语义和沉浸式体验所必需的细粒度控制。为了解决这些限制，我们引入了 EmoSpace，这是一种用于情感感知内容生成的新颖框架，它通过视觉语言对齐来学习动态、可解释的情感原型。我们采用分层情感表示，具有丰富的可学习原型，这些原型在训练过程中不断演变，无需明确的情感标签即可实现细粒度的情感控制。我们开发了一个可控的生成管道，具有多原型引导、时间混合和注意力重新加权等功能，支持多种应用，包括情感图像外画、风格化生成和 VR 环境的情感全景生成。我们的实验证明了 EmoSpace 在定性和定量评估方面均优于现有方法。此外，我们还开展了一项全面的用户研究，调查与桌面设置相比，VR 环境如何影响情绪感知。我们的工作通过精细的情感控制促进沉浸式视觉内容的生成，并支持治疗、教育、讲故事、艺术创作和文化保护等应用。代码和模型将公开。</li>
</ul>

<h3>Title: RI-Mamba: Rotation-Invariant Mamba for Robust Text-to-Shape Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Khanh Nguyen, Dasith de Silva Edirimuni, Ghulam Mubashar Hassan, Ajmal Mian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11673">https://arxiv.org/abs/2602.11673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11673">https://arxiv.org/pdf/2602.11673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11673]] RI-Mamba: Rotation-Invariant Mamba for Robust Text-to-Shape Retrieval(https://arxiv.org/abs/2602.11673)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>3D assets have rapidly expanded in quantity and diversity due to the growing popularity of virtual reality and gaming. As a result, text-to-shape retrieval has become essential in facilitating intuitive search within large repositories. However, existing methods require canonical poses and support few object categories, limiting their real-world applicability where objects can belong to diverse classes and appear in random orientations. To address this challenge, we propose RI-Mamba, the first rotation-invariant state-space model for point clouds. RI-Mamba defines global and local reference frames to disentangle pose from geometry and uses Hilbert sorting to construct token sequences with meaningful geometric structure while maintaining rotation invariance. We further introduce a novel strategy to compute orientational embeddings and reintegrate them via feature-wise linear modulation, effectively recovering spatial context and enhancing model expressiveness. Our strategy is inherently compatible with state-space models and operates in linear time. To scale up retrieval, we adopt cross-modal contrastive learning with automated triplet generation, allowing training on diverse datasets without manual annotation. Extensive experiments demonstrate RI-Mamba's superior representational capacity and robustness, achieving state-of-the-art performance on the OmniObject3D benchmark across more than 200 object categories under arbitrary orientations. Our code will be made available at this https URL.</li>
<li><strong>摘要：</strong>由于虚拟现实和游戏的日益普及，3D 资产的数量和多样性迅速扩大。因此，文本到形状检索对于促进大型存储库中的直观搜索变得至关重要。然而，现有的方法需要规范的姿势并支持很少的对象类别，限制了它们在现实世界中的适用性，其中对象可以属于不同的类别并以随机方向出现。为了应对这一挑战，我们提出了 RI-Mamba，这是第一个点云旋转不变状态空间模型。 RI-Mamba 定义了全局和局部参考系，以将姿态与几何体分离，并使用希尔伯特排序来构建具有有意义的几何结构的标记序列，同时保持旋转不变性。我们进一步引入了一种新的策略来计算方向嵌入，并通过特征线性调制重新整合它们，有效地恢复空间上下文并增强模型的表达能力。我们的策略本质上与状态空间模型兼容，并以线性时间运行。为了扩大检索规模，我们采用跨模式对比学习和自动三元组生成，允许在不同的数据集上进行训练，而无需手动注释。大量实验证明了 RI-Mamba 卓越的表征能力和鲁棒性，在任意方向下的 200 多个对象类别的 OmniObject3D 基准上实现了最先进的性能。我们的代码将在此 https URL 中提供。</li>
</ul>

<h3>Title: LLM-Driven 3D Scene Generation of Agricultural Simulation Environments</h3>
<ul>
<li><strong>Authors: </strong>Arafa Yoncalik, Wouter Jansen, Nico Huebel, Mohammad Hasan Rahmani, Jan Steckel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11706">https://arxiv.org/abs/2602.11706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11706">https://arxiv.org/pdf/2602.11706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11706]] LLM-Driven 3D Scene Generation of Agricultural Simulation Environments(https://arxiv.org/abs/2602.11706)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Procedural generation techniques in 3D rendering engines have revolutionized the creation of complex environments, reducing reliance on manual design. Recent approaches using Large Language Models (LLMs) for 3D scene generation show promise but often lack domain-specific reasoning, verification mechanisms, and modular design. These limitations lead to reduced control and poor scalability. This paper investigates the use of LLMs to generate agricultural synthetic simulation environments from natural language prompts, specifically to address the limitations of lacking domain-specific reasoning, verification mechanisms, and modular design. A modular multi-LLM pipeline was developed, integrating 3D asset retrieval, domain knowledge injection, and code generation for the Unreal rendering engine using its API. This results in a 3D environment with realistic planting layouts and environmental context, all based on the input prompt and the domain knowledge. To enhance accuracy and scalability, the system employs a hybrid strategy combining LLM optimization techniques such as few-shot prompting, Retrieval-Augmented Generation (RAG), finetuning, and validation. Unlike monolithic models, the modular architecture enables structured data handling, intermediate verification, and flexible expansion. The system was evaluated using structured prompts and semantic accuracy metrics. A user study assessed realism and familiarity against real-world images, while an expert comparison demonstrated significant time savings over manual scene design. The results confirm the effectiveness of multi-LLM pipelines in automating domain-specific 3D scene generation with improved reliability and precision. Future work will explore expanding the asset hierarchy, incorporating real-time generation, and adapting the pipeline to other simulation domains beyond agriculture.</li>
<li><strong>摘要：</strong>3D 渲染引擎中的程序生成技术彻底改变了复杂环境的创建，减少了对手动设计的依赖。最近使用大型语言模型 (LLM) 进行 3D 场景生成的方法显示出前景，但通常缺乏特定领域的推理、验证机制和模块化设计。这些限制导致控制力减弱和可扩展性差。本文研究了使用法学硕士根据自然语言提示生成农业综合模拟环境，特别是为了解决缺乏特定领域推理、验证机制和模块化设计的局限性。开发了一个模块化的多 LLM 管道，使用其 API 集成了 3D 资产检索、领域知识注入和虚幻渲染引擎的代码生成。这会产生具有真实种植布局和环境背景的 3D 环境，所有这些都基于输入提示和领域知识。为了提高准确性和可扩展性，系统采用了结合 LLM 优化技术的混合策略，例如少样本提示、检索增强生成 (RAG)、微调和验证。与整体模型不同，模块化架构支持结构化数据处理、中间验证和灵活扩展。使用结构化提示和语义准确性指标对该系统进行评估。用户研究评估了真实世界图像的真实性和熟悉度，而专家比较则证明比手动场景设计节省了大量时间。结果证实了多 LLM 管道在自动化特定领域 3D 场景生成方面的有效性，并提高了可靠性和精度。未来的工作将探索扩展资产层次结构，纳入实时生成，并使管道适应农业以外的其他模拟领域。</li>
</ul>

<h3>Title: DICE: Diffusion Large Language Models Excel at Generating CUDA Kernels</h3>
<ul>
<li><strong>Authors: </strong>Haolei Bai, Lingcheng Kong, Xueyi Chen, Jianmian Wang, Zhiqiang Tao, Huan Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11715">https://arxiv.org/abs/2602.11715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11715">https://arxiv.org/pdf/2602.11715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11715]] DICE: Diffusion Large Language Models Excel at Generating CUDA Kernels(https://arxiv.org/abs/2602.11715)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion large language models (dLLMs) have emerged as a compelling alternative to autoregressive (AR) LLMs, owing to their capacity for parallel token generation. This paradigm is particularly well-suited for code generation, where holistic structural planning and non-sequential refinement are critical. Despite this potential, tailoring dLLMs for CUDA kernel generation remains challenging, obstructed not only by the high specialization but also by the severe lack of high-quality training data. To address these challenges, we construct CuKe, an augmented supervised fine-tuning dataset optimized for high-performance CUDA kernels. On top of it, we propose a bi-phase curated reinforcement learning (BiC-RL) framework consisting of a CUDA kernel infilling stage and an end-to-end CUDA kernel generation stage. Leveraging this training framework, we introduce DICE, a series of diffusion large language models designed for CUDA kernel generation, spanning three parameter scales, 1.7B, 4B, and 8B. Extensive experiments on KernelBench demonstrate that DICE significantly outperforms both autoregressive and diffusion LLMs of comparable scale, establishing a new state-of-the-art for CUDA kernel generation.</li>
<li><strong>摘要：</strong>扩散大语言模型 (dLLM) 因其并行令牌生成的能力而成为自回归 (AR) LLM 的引人注目的替代方案。这种范例特别适合代码生成，其中整体结构规划和非顺序细化至关重要。尽管有这种潜力，为 CUDA 内核生成定制 dLLM 仍然具有挑战性，这不仅受到高度专业化的阻碍，而且还受到严重缺乏高质量训练数据的阻碍。为了应对这些挑战，我们构建了 CuKe，一个针对高性能 CUDA 内核进行优化的增强监督微调数据集。在此之上，我们提出了一个双阶段策划强化学习（BiC-RL）框架，由 CUDA 内核填充阶段和端到端 CUDA 内核生成阶段组成。利用这个训练框架，我们引入了 DICE，这是一系列专为 CUDA 内核生成而设计的扩散大型语言模型，涵盖三个参数尺度：1.7B、4B 和 8B。 KernelBench 上的大量实验表明，DICE 的性能显着优于同等规模的自回归和扩散 LLM，为 CUDA 内核生成建立了新的最先进技术。</li>
</ul>

<h3>Title: Code2Worlds: Empowering Coding LLMs for 4D World Generation</h3>
<ul>
<li><strong>Authors: </strong>Yi Zhang, Yunshuang Wang, Zeyu Zhang, Hao Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11757">https://arxiv.org/abs/2602.11757</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11757">https://arxiv.org/pdf/2602.11757</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11757]] Code2Worlds: Empowering Coding LLMs for 4D World Generation(https://arxiv.org/abs/2602.11757)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Achieving spatial intelligence requires moving beyond visual plausibility to build world simulators grounded in physical laws. While coding LLMs have advanced static 3D scene generation, extending this paradigm to 4D dynamics remains a critical frontier. This task presents two fundamental challenges: multi-scale context entanglement, where monolithic generation fails to balance local object structures with global environmental layouts; and a semantic-physical execution gap, where open-loop code generation leads to physical hallucinations lacking dynamic fidelity. We introduce Code2Worlds, a framework that formulates 4D generation as language-to-simulation code generation. First, we propose a dual-stream architecture that disentangles retrieval-augmented object generation from hierarchical environmental orchestration. Second, to ensure dynamic fidelity, we establish a physics-aware closed-loop mechanism in which a PostProcess Agent scripts dynamics, coupled with a VLM-Motion Critic that performs self-reflection to iteratively refine simulation code. Evaluations on the Code4D benchmark show Code2Worlds outperforms baselines with a 41% SGS gain and 49% higher Richness, while uniquely generating physics-aware dynamics absent in prior static methods. Code: this https URL. Website: this https URL.</li>
<li><strong>摘要：</strong>实现空间智能需要超越视觉合理性来构建基于物理定律的世界模拟器。虽然编码 LLM 具有先进的静态 3D 场景生成功能，但将此范式扩展到 4D 动态仍然是一个关键前沿。这项任务提出了两个基本挑战：多尺度上下文纠缠，其中整体生成无法平衡局部对象结构与全局环境布局；语义-物理执行差距，开环代码生成导致缺乏动态保真度的物理幻觉。我们引入了 Code2Worlds，这是一个将 4D 生成制定为语言到模拟代码生成的框架。首先，我们提出了一种双流架构，将检索增强对象生成与分层环境编排分开。其次，为了确保动态保真度，我们建立了一种物理感知闭环机制，其中 PostProcess Agent 编写动态脚本，并结合 VLM-Motion Critic 进行自我反思以迭代地完善模拟代码。对 Code4D 基准的评估显示，Code2Worlds 的性能优于基准，SGS 增益提高了 41%，丰富度提高了 49%，同时独特地生成了先前静态方法中不存在的物理感知动态。代码：此 https URL。网站：此 https URL。</li>
</ul>

<h3>Title: Light4D: Training-Free Extreme Viewpoint 4D Video Relighting</h3>
<ul>
<li><strong>Authors: </strong>Zhenghuang Wu, Kang Chen, Zeyu Zhang, Hao Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11769">https://arxiv.org/abs/2602.11769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11769">https://arxiv.org/pdf/2602.11769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11769]] Light4D: Training-Free Extreme Viewpoint 4D Video Relighting(https://arxiv.org/abs/2602.11769)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion-based generative models have established a new paradigm for image and video relighting. However, extending these capabilities to 4D relighting remains challenging, due primarily to the scarcity of paired 4D relighting training data and the difficulty of maintaining temporal consistency across extreme viewpoints. In this work, we propose Light4D, a novel training-free framework designed to synthesize consistent 4D videos under target illumination, even under extreme viewpoint changes. First, we introduce Disentangled Flow Guidance, a time-aware strategy that effectively injects lighting control into the latent space while preserving geometric integrity. Second, to reinforce temporal consistency, we develop Temporal Consistent Attention within the IC-Light architecture and further incorporate deterministic regularization to eliminate appearance flickering. Extensive experiments demonstrate that our method achieves competitive performance in temporal consistency and lighting fidelity, robustly handling camera rotations from -90 to 90. Code: this https URL. Website: this https URL.</li>
<li><strong>摘要：</strong>基于扩散的生成模型的最新进展为图像和视频重新照明建立了新的范式。然而，将这些功能扩展到 4D 重新照明仍然具有挑战性，这主要是由于配对 4D 重新照明训练数据的稀缺以及在极端视点之间保持时间一致性的困难。在这项工作中，我们提出了 Light4D，这是一种新颖的免训练框架，旨在在目标照明下，甚至在极端的视点变化下合成一致的 4D 视频。首先，我们引入解缠结流引导，这是一种时间感知策略，可以有效地将照明控制注入到潜在空间，同时保持几何完整性。其次，为了增强时间一致性，我们在 IC-Light 架构中开发了时间一致性注意力，并进一步结合确定性正则化来消除外观闪烁。大量实验表明，我们的方法在时间一致性和光照保真度方面实现了具有竞争力的性能，稳健地处理从 -90 到 90 的相机旋转。代码：此 https URL。网站：此 https URL。</li>
</ul>

<h3>Title: Temperature as a Meta-Policy: Adaptive Temperature in LLM Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Haoran Dang, Cuiling Lan, Hai Wan, Xibin Zhao, Yan Lu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11779">https://arxiv.org/abs/2602.11779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11779">https://arxiv.org/pdf/2602.11779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11779]] Temperature as a Meta-Policy: Adaptive Temperature in LLM Reinforcement Learning(https://arxiv.org/abs/2602.11779)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Temperature is a crucial hyperparameter in large language models (LLMs), controlling the trade-off between exploration and exploitation during text generation. High temperatures encourage diverse but noisy outputs, while low temperatures produce focused outputs but may cause premature convergence. Yet static or heuristic temperature schedules fail to adapt to the dynamic demands of reinforcement learning (RL) throughout training, often limiting policy improvement. We propose Temperature Adaptive Meta Policy Optimization (TAMPO), a new framework that recasts temperature control as a learnable meta-policy. TAMPO operates through a hierarchical two-loop process. In the inner loop, the LLM policy is updated (e.g., using GRPO) with trajectories sampled at the temperature selected by the meta-policy. In the outer loop, meta-policy updates the distribution over candidate temperatures by rewarding those that maximize the likelihood of high-advantage trajectories. This trajectory-guided, reward-driven mechanism enables online adaptation without additional rollouts, directly aligning exploration with policy improvement. On five mathematical reasoning benchmarks, TAMPO outperforms baselines using fixed or heuristic temperatures, establishing temperature as an effective learnable meta-policy for adaptive exploration in LLM reinforcement learning. Accepted at ICLR 2026.</li>
<li><strong>摘要：</strong>温度是大型语言模型 (LLM) 中的一个关键超参数，控制文本生成过程中探索和利用之间的权衡。高温鼓励多样化但嘈杂的输出，而低温产生集中的输出，但可能导致过早收敛。然而，静态或启发式温度计划无法适应整个训练过程中强化学习 (RL) 的动态需求，通常会限制策略的改进。我们提出了温度自适应元策略优化（TAMPO），这是一种新框架，它将温度控制重塑为可学习的元策略。 TAMPO 通过分层的双循环流程运行。在内循环中，LLM 策略使用在元策略选择的温度下采样的轨迹进行更新（例如，使用 GRPO）。在外循环中，元策略通过奖励那些最大化高优势轨迹的可能性来更新候选温度的分布。这种轨迹引导、奖励驱动的机制无需额外部署即可实现在线适应，从而直接使探索与政策改进保持一致。在五个数学推理基准上，TAMPO 优于使用固定或启发式温度的基线，将温度确立为 LLM 强化学习中自适应探索的有效可学习元策略。在 ICLR 2026 上被接受。</li>
</ul>

<h3>Title: TopoFair: Linking Topological Bias to Fairness in Link Prediction Benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Lilian Marey, Mathilde Perez, Tiphaine Viard, Charlotte Laclau</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11802">https://arxiv.org/abs/2602.11802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11802">https://arxiv.org/pdf/2602.11802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11802]] TopoFair: Linking Topological Bias to Fairness in Link Prediction Benchmarks(https://arxiv.org/abs/2602.11802)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Graph link prediction (LP) plays a critical role in socially impactful applications, such as job recommendation and friendship formation. Ensuring fairness in this task is thus essential. While many fairness-aware methods manipulate graph structures to mitigate prediction disparities, the topological biases inherent to social graph structures remain poorly understood and are often reduced to homophily alone. This undermines the generalization potential of fairness interventions and limits their applicability across diverse network topologies. In this work, we propose a novel benchmarking framework for fair LP, centered on the structural biases of the underlying graphs. We begin by reviewing and formalizing a broad taxonomy of topological bias measures relevant to fairness in graphs. In parallel, we introduce a flexible graph generation method that simultaneously ensures fidelity to real-world graph patterns and enables controlled variation across a wide spectrum of structural biases. We apply this framework to evaluate both classical and fairness-aware LP models across multiple use cases. Our results provide a fine-grained empirical analysis of the interactions between predictive fairness and structural biases. This new perspective reveals the sensitivity of fairness interventions to beyond-homophily biases and underscores the need for structurally grounded fairness evaluations in graph learning.</li>
<li><strong>摘要：</strong>图链接预测（LP）在具有社会影响力的应用中发挥着至关重要的作用，例如工作推荐和友谊形成。因此，确保这项任务的公平性至关重要。虽然许多公平意识方法操纵图结构来减轻预测差异，但社交图结构固有的拓扑偏差仍然知之甚少，并且常常被简化为仅同质性。这破坏了公平干预的泛化潜力，并限制了它们在不同网络拓扑中的适用性。在这项工作中，我们提出了一种新颖的公平 LP 基准测试框架，以底层图的结构偏差为中心。我们首先回顾并规范与图中公平性相关的拓扑偏差度量的广泛分类。与此同时，我们引入了一种灵活的图形生成方法，该方法同时确保对现实世界图形模式的保真度，并能够在广泛的结构偏差中实现受控变化。我们应用这个框架来评估跨多个用例的经典和公平感知的 LP 模型。我们的结果提供了对预测公平性和结构性偏差之间相互作用的细粒度实证分析。这一新视角揭示了公平干预对超同质偏差的敏感性，并强调了图学习中基于结构的公平评估的必要性。</li>
</ul>

<h3>Title: Deep Kernel Fusion for Transformers</h3>
<ul>
<li><strong>Authors: </strong>Zixi Zhang, Zhiwen Mo, Yiren Zhao, Robert Mullins</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11808">https://arxiv.org/abs/2602.11808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11808">https://arxiv.org/pdf/2602.11808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11808]] Deep Kernel Fusion for Transformers(https://arxiv.org/abs/2602.11808)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Agentic LLM inference with long contexts is increasingly limited by memory bandwidth rather than compute. In this setting, SwiGLU MLP blocks, whose large weights exceed cache capacity, become a major yet under-optimized bottleneck. We propose DeepFusionKernel, a deeply fused kernel that cuts HBM traffic and boosts cache reuse, delivering up to 13.2% speedup on H100 and 9.7% on A100 over SGLang. Integrated with SGLang and paired with a kernel scheduler, DeepFusionKernel ensures consistent accelerations over generation lengths, while remaining adaptable to diverse models, inference configurations, and hardware platforms.</li>
<li><strong>摘要：</strong>具有长上下文的代理 LLM 推理越来越受到内存带宽而不是计算的限制。在这种情况下，SwiGLU MLP 块的权重超过了缓存容量，成为主要但未优化的瓶颈。我们提出了 DeepFusionKernel，这是一种深度融合的内核，可减少 HBM 流量并提高缓存重用，与 SGLang 相比，在 H100 上可实现高达 13.2% 的加速，在 A100 上可实现 9.7% 的加速。 DeepFusionKernel 与 SGLang 集成并与内核调度程序配合使用，可确保在一代长度内实现一致的加速，同时保持对不同模型、推理配置和硬件平台的适应性。</li>
</ul>

<h3>Title: How to Sample High Quality 3D Fractals for Action Recognition Pre-Training?</h3>
<ul>
<li><strong>Authors: </strong>Marko Putak, Thomas B. Moeslund, Joakim Bruslund Haurum</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11810">https://arxiv.org/abs/2602.11810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11810">https://arxiv.org/pdf/2602.11810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11810]] How to Sample High Quality 3D Fractals for Action Recognition Pre-Training?(https://arxiv.org/abs/2602.11810)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Synthetic datasets are being recognized in the deep learning realm as a valuable alternative to exhaustively labeled real data. One such synthetic data generation method is Formula Driven Supervised Learning (FDSL), which can provide an infinite number of perfectly labeled data through a formula driven approach, such as fractals or contours. FDSL does not have common drawbacks like manual labor, privacy and other ethical concerns. In this work we generate 3D fractals using 3D Iterated Function Systems (IFS) for pre-training an action recognition model. The fractals are temporally transformed to form a video that is used as a pre-training dataset for downstream task of action recognition. We find that standard methods of generating fractals are slow and produce degenerate 3D fractals. Therefore, we systematically explore alternative ways of generating fractals and finds that overly-restrictive approaches, while generating aesthetically pleasing fractals, are detrimental for downstream task performance. We propose a novel method, Targeted Smart Filtering, to address both the generation speed and fractal diversity issue. The method reports roughly 100 times faster sampling speed and achieves superior downstream performance against other 3D fractal filtering methods.</li>
<li><strong>摘要：</strong>合成数据集在深度学习领域被认为是详尽标记的真实数据的有价值的替代方案。其中一种合成数据生成方法是公式驱动监督学习（FDSL），它可以通过公式驱动方法（例如分形或轮廓）提供无限数量的完美标记数据。 FDSL 没有体力劳动、隐私和其他道德问题等常见缺点。在这项工作中，我们使用 3D 迭代函数系统 (IFS) 生成 3D 分形，以预训练动作识别模型。分形在时间上进行变换以形成视频，该视频用作动作识别下游任务的预训练数据集。我们发现生成分形的标准方法很慢并且会产生简并的 3D 分形。因此，我们系统地探索了生成分形的替代方法，并发现过度限制的方法虽然生成美观的分形，但不利于下游任务的性能。我们提出了一种新颖的方法，即目标智能过滤，来解决生成速度和分形多样性问题。与其他 3D 分形过滤方法相比，该方法的采样速度快了大约 100 倍，并实现了卓越的下游性能。</li>
</ul>

<h3>Title: Free Lunch for Stabilizing Rectified Flow Inversion</h3>
<ul>
<li><strong>Authors: </strong>Chenru Wang, Beier Zhu, Chi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11850">https://arxiv.org/abs/2602.11850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11850">https://arxiv.org/pdf/2602.11850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11850]] Free Lunch for Stabilizing Rectified Flow Inversion(https://arxiv.org/abs/2602.11850)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Rectified-Flow (RF)-based generative models have recently emerged as strong alternatives to traditional diffusion models, demonstrating state-of-the-art performance across various tasks. By learning a continuous velocity field that transforms simple noise into complex data, RF-based models not only enable high-quality generation, but also support training-free inversion, which facilitates downstream tasks such as reconstruction and editing. However, existing inversion methods, such as vanilla RF-based inversion, suffer from approximation errors that accumulate across timesteps, leading to unstable velocity fields and degraded reconstruction and editing quality. To address this challenge, we propose Proximal-Mean Inversion (PMI), a training-free gradient correction method that stabilizes the velocity field by guiding it toward a running average of past velocities, constrained within a theoretically derived spherical Gaussian. Furthermore, we introduce mimic-CFG, a lightweight velocity correction scheme for editing tasks, which interpolates between the current velocity and its projection onto the historical average, balancing editing effectiveness and structural consistency. Extensive experiments on PIE-Bench demonstrate that our methods significantly improve inversion stability, image reconstruction quality, and editing fidelity, while reducing the required number of neural function evaluations. Our approach achieves state-of-the-art performance on the PIE-Bench with enhanced efficiency and theoretical soundness.</li>
<li><strong>摘要：</strong>基于整流流 (RF) 的生成模型最近已成为传统扩散模型的强大替代品，在各种任务中展示了最先进的性能。通过学习将简单噪声转换为复杂数据的连续速度场，基于 RF 的模型不仅能够实现高质量生成，而且还支持免训练反演，从而有利于重建和编辑等下游任务。然而，现有的反演方法，例如基于普通 RF 的反演，存在跨时间步长累积的近似误差，导致速度场不稳定以及重建和编辑质量下降。为了应对这一挑战，我们提出了近均值反演（PMI），这是一种无需训练的梯度校正方法，通过引导速度场趋向过去速度的运行平均值（限制在理论上推导的球面高斯）内来稳定速度场。此外，我们引入了imit-CFG，一种用于编辑任务的轻量级速度校正方案，它在当前速度及其对历史平均值的投影之间进行插值，平衡编辑效果和结构一致性。 PIE-Bench 上的大量实验表明，我们的方法显着提高了反演稳定性、图像重建质量和编辑保真度，同时减少了所需的神经功能评估数量。我们的方法在 PIE-Bench 上实现了最先进的性能，并提高了效率和理论可靠性。</li>
</ul>

<h3>Title: Robust Optimization Approach and Learning Based Hide-and-Seek Game for Resilient Network Design</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Khosravi, Setareh Maghsudi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11854">https://arxiv.org/abs/2602.11854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11854">https://arxiv.org/pdf/2602.11854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11854]] Robust Optimization Approach and Learning Based Hide-and-Seek Game for Resilient Network Design(https://arxiv.org/abs/2602.11854)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We study the design of resilient and reliable communication networks in which a signal can be transferred only up to a limited distance before its quality falls below an acceptable threshold. When excessive signal degradation occurs, regeneration is required through regenerators installed at selected network nodes. In this work, both network links and nodes are subject to uncertainty. The installation costs of regenerators are modeled using a budgeted uncertainty set. In addition, link lengths follow a dynamic budgeted uncertainty set introduced in this paper, where deviations may vary over time. Robust optimization seeks solutions whose performance is guaranteed under all scenarios represented by the underlying uncertainty set. Accordingly, the objective is to identify a minimum-cost subset of nodes for regenerator deployment that ensures full network connectivity, even under the worst possible realizations of uncertainty. To solve the problem, we first formulate it within a robust optimization framework, and then develop scalable solution methods based on column-and-constraint generation, Benders decomposition, and iterative robust optimization. In addition, we formulate a learning-based hide-and-seek game to further analyze the problem structure. The proposed approaches are evaluated against classical static budgeted robust models and deterministic worst-case formulations. Both theoretical analysis and computational results demonstrate the effectiveness and advantages of our methodology.</li>
<li><strong>摘要：</strong>我们研究弹性且可靠的通信网络的设计，在该网络中信号只能传输到有限的距离，然后其质量就会低于可接受的阈值。当发生过度的信号衰减时，需要通过安装在选定的网络节点处的再生器进行再生。在这项工作中，网络链路和节点都受到不确定性的影响。再生器的安装成本使用预算不确定性集进行建模。此外，链路长度遵循本文引入的动态预算不确定性集，其中偏差可能随时间变化。鲁棒优化寻求在由潜在不确定性集表示的所有场景下其性能得到保证的解决方案。因此，目标是确定再生器部署的最低成本节点子集，即使在最坏的不确定性实现下，也能确保完整的网络连接。为了解决这个问题，我们首先在鲁棒优化框架内制定它，然后开发基于列和约束生成、Benders 分解和迭代鲁棒优化的可扩展解决方法。此外，我们制定了一个基于学习的捉迷藏游戏来进一步分析问题结构。所提出的方法根据经典静态预算鲁棒模型和确定性最坏情况公式进行评估。理论分析和计算结果都证明了我们方法的有效性和优势。</li>
</ul>

<h3>Title: DiffPlace: Street View Generation via Place-Controllable Diffusion Model Enhancing Place Recognition</h3>
<ul>
<li><strong>Authors: </strong>Ji Li, Zhiwei Li, Shihao Li, Zhenjiang Yu, Boyang Wang, Haiou Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11875">https://arxiv.org/abs/2602.11875</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11875">https://arxiv.org/pdf/2602.11875</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11875]] DiffPlace: Street View Generation via Place-Controllable Diffusion Model Enhancing Place Recognition(https://arxiv.org/abs/2602.11875)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative models have advanced significantly in realistic image synthesis, with diffusion models excelling in quality and stability. Recent multi-view diffusion models improve 3D-aware street view generation, but they struggle to produce place-aware and background-consistent urban scenes from text, BEV maps, and object bounding boxes. This limits their effectiveness in generating realistic samples for place recognition tasks. To address these challenges, we propose DiffPlace, a novel framework that introduces a place-ID controller to enable place-controllable multi-view image generation. The place-ID controller employs linear projection, perceiver transformer, and contrastive learning to map place-ID embeddings into a fixed CLIP space, allowing the model to synthesize images with consistent background buildings while flexibly modifying foreground objects and weather conditions. Extensive experiments, including quantitative comparisons and augmented training evaluations, demonstrate that DiffPlace outperforms existing methods in both generation quality and training support for visual place recognition. Our results highlight the potential of generative models in enhancing scene-level and place-aware synthesis, providing a valuable approach for improving place recognition in autonomous driving</li>
<li><strong>摘要：</strong>生成模型在真实图像合成方面取得了显着进步，扩散模型在质量和稳定性方面表现出色。最近的多视图扩散模型改进了 3D 感知街景生成，但它们很难从文本、BEV 地图和对象边界框生成位置感知和背景一致的城市场景。这限制了它们为地点识别任务生成真实样本的有效性。为了应对这些挑战，我们提出了 DiffPlace，这是一种新颖的框架，它引入了位置 ID 控制器来实现位置可控的多视图图像生成。地点 ID 控制器采用线性投影、感知器变换器和对比学习将地点 ID 嵌入映射到固定的 CLIP 空间中，使模型能够合成具有一致背景建筑物的图像，同时灵活修改前景物体和天气条件。包括定量比较和增强训练评估在内的大量实验表明，DiffPlace 在生成质量和视觉位置识别的训练支持方面均优于现有方法。我们的结果凸显了生成模型在增强场景级和位置感知合成方面的潜力，为提高自动驾驶中的位置识别提供了一种有价值的方法</li>
</ul>

<h3>Title: DynaHOI: Benchmarking Hand-Object Interaction for Dynamic Target</h3>
<ul>
<li><strong>Authors: </strong>BoCheng Hu, Zhonghan Zhao, Kaiyue Zhou, Hongwei Wang, Gaoang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11919">https://arxiv.org/abs/2602.11919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11919">https://arxiv.org/pdf/2602.11919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11919]] DynaHOI: Benchmarking Hand-Object Interaction for Dynamic Target(https://arxiv.org/abs/2602.11919)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Most existing hand motion generation benchmarks for hand-object interaction (HOI) focus on static objects, leaving dynamic scenarios with moving targets and time-critical coordination largely untested. To address this gap, we introduce the DynaHOI-Gym, a unified online closed-loop platform with parameterized motion generators and rollout-based metrics for dynamic capture evaluation. Built on DynaHOI-Gym, we release DynaHOI-10M, a large-scale benchmark with 10M frames and 180K hand capture trajectories, whose target motions are organized into 8 major categories and 22 fine-grained subcategories. We also provide a simple observe-before-act baseline (ObAct) that integrates short-term observations with the current frame via spatiotemporal attention to predict actions, achieving an 8.1% improvement in location success rate.</li>
<li><strong>摘要：</strong>大多数现有的手与物体交互（HOI）的手部动作生成基准都侧重于静态物体，而具有移动目标的动态场景和时间关键的协调在很大程度上未经测试。为了解决这一差距，我们引入了 DynaHOI-Gym，这是一个统一的在线闭环平台，具有参数化运动生成器和用于动态捕捉评估的基于推出的指标。基于 DynaHOI-Gym，我们发布了 DynaHOI-10M，这是一个具有 10M 帧和 180K 手部捕捉轨迹的大规模基准测试，其目标运动分为 8 个大类和 22 个细粒度子类。我们还提供了一个简单的行动前观察基线（ObAct），通过时空注意力将短期观察与当前帧相结合来预测行动，实现定位成功率提高 8.1%。</li>
</ul>

<h3>Title: Extending Puzzle for Mixture-of-Experts Reasoning Models with Application to GPT-OSS Acceleration</h3>
<ul>
<li><strong>Authors: </strong>Akhiad Bercovich, Nir Ailon, Vladimir Anisimov, Tomer Asida, Nave Assaf, Mohammad Dabbah, Ido Galil, Amnon Geifman, Yonatan Geifman, Izhak Golan, Roi Koren, Itay Levy, Zach Moshe, Pavlo Molchanov, Najeeb Nabwani, Mostofa Patwari, Omri Puny, Tomer Ronen, Itamar Schen, Elad Segal, Ido Shahaf, Oren Tropp, Ran Zilberstein, Ran El-Yaniv</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11937">https://arxiv.org/abs/2602.11937</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11937">https://arxiv.org/pdf/2602.11937</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11937]] Extending Puzzle for Mixture-of-Experts Reasoning Models with Application to GPT-OSS Acceleration(https://arxiv.org/abs/2602.11937)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reasoning-focused LLMs improve answer quality by generating longer reasoning traces, but the additional tokens dramatically increase serving cost, motivating inference optimization. We extend and apply Puzzle, a post-training neural architecture search (NAS) framework, to gpt-oss-120B to produce gpt-oss-puzzle-88B, a deployment-optimized derivative. Our approach combines heterogeneous MoE expert pruning, selective replacement of full-context attention with window attention, FP8 KV-cache quantization with calibrated scales, and post-training reinforcement learning to recover accuracy, while maintaining low generation length. In terms of per-token speeds, on an 8XH100 node we achieve 1.63X and 1.22X throughput speedups in long-context and short-context settings, respectively. gpt-oss-puzzle-88B also delivers throughput speedups of 2.82X on a single NVIDIA H100 GPU. However, because token counts can change with reasoning effort and model variants, per-token throughput (tok/s) and latency (ms/token) do not necessarily lead to end-to-end speedups: a 2X throughput gain is erased if traces grow 2X. Conversely, throughput gains can be spent on more reasoning tokens to improve accuracy; we therefore advocate request-level efficiency metrics that normalize throughput by tokens generated and trace an accuracy--speed frontier across reasoning efforts. We show that gpt-oss-puzzle-88B improves over gpt-oss-120B along the entire frontier, delivering up to 1.29X higher request-level efficiency. Across various benchmarks, gpt-oss-puzzle-88B matches or slightly exceeds the parent on suite-average accuracy across reasoning efforts, with retention ranging from 100.8% (high) to 108.2% (low), showing that post-training architecture search can substantially reduce inference costs without sacrificing quality.</li>
<li><strong>摘要：</strong>以推理为重点的法学硕士通过生成更长的推理轨迹来提高答案质量，但额外的令牌会显着增加服务成本，从而促进推理优化。我们将训练后神经架构搜索 (NAS) 框架 Puzzle 扩展并应用到 gpt-oss-120B，以生成 gpt-oss-puzzle-88B，这是一种部署优化的衍生产品。我们的方法结合了异构 MoE 专家剪枝、用窗口注意力选择性替换全上下文注意力、具有校准尺度的 FP8 KV 缓存量化以及训练后强化学习来恢复准确性，同时保持较低的生成长度。就每个令牌的速度而言，在 8XH100 节点上，我们在长上下文和短上下文设置中分别实现了 1.63 倍和 1.22 倍的吞吐量加速。 gpt-oss-puzzle-88B 还在单个 NVIDIA H100 GPU 上提供 2.82 倍的吞吐量加速。然而，由于令牌计数可能会随着推理工作和模型变体而变化，因此每个令牌的吞吐量（tok/s）和延迟（毫秒/令牌）不一定会导致端到端加速：如果跟踪增长 2 倍，则 2 倍的吞吐量增益会被消除。相反，吞吐量的增益可以花在更多的推理令牌上，以提高准确性；因此，我们提倡请求级效率指标，通过生成的令牌标准化吞吐量，并跟踪推理工作的准确性-速度边界。我们表明，gpt-oss-puzzle-88B 在整个边界上比 gpt-oss-120B 有所改进，请求级别的效率提高了 1.29 倍。在各种基准测试中，gpt-oss-puzzle-88B 在推理工作中的套件平均准确度上与父级相匹配或略胜一筹，保留率范围从 100.8%（高）到 108.2%（低），这表明训练后架构搜索可以在不牺牲质量的情况下大幅降低推理成本。</li>
</ul>

<h3>Title: Benchmarking Vision-Language Models for French PDF-to-Markdown Conversion</h3>
<ul>
<li><strong>Authors: </strong>Bruno Rigal, Victor Dupriez, Alexis Mignon, Ronan Le Hy, Nicolas Mery</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11960">https://arxiv.org/abs/2602.11960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11960">https://arxiv.org/pdf/2602.11960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11960]] Benchmarking Vision-Language Models for French PDF-to-Markdown Conversion(https://arxiv.org/abs/2602.11960)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This report evaluates PDF-to-Markdown conversion using recent Vision-Language Models (VLMs) on challenging French documents. Document parsing is a critical step for Retrieval-Augmented Generation (RAG) pipelines, where transcription and layout errors propagate to downstream retrieval and grounding. Existing benchmarks often emphasize English or Chinese and can over-penalize benign formatting and linearization choices (e.g., line breaks, list segmentation, alternative table renderings) that are largely irrelevant for downstream use. We introduce a French-focused benchmark of difficult pages selected via model-disagreement sampling from a corpus of 60{,}000 documents, covering handwritten forms, complex layouts, dense tables, and graphics-rich pages. Evaluation is performed with unit-test-style checks that target concrete failure modes (text presence, reading order, and local table constraints) combined with category-specific normalization designed to discount presentation-only variance. Across 15 models, we observe substantially higher robustness for the strongest proprietary models on handwriting and forms, while several open-weights systems remain competitive on standard printed layouts.</li>
<li><strong>摘要：</strong>本报告使用最新的视觉语言模型 (VLM) 在具有挑战性的法语文档上评估 PDF 到 Markdown 的转换。文档解析是检索增强生成 (RAG) 管道的关键步骤，其中转录和布局错误会传播到下游检索和基础。现有的基准通常强调英语或中文，并且可能会过度惩罚与下游使用很大程度上无关的良性格式化和线性化选择（例如换行符、列表分段、替代表格渲染）。我们引入了一个以法语为中心的困难页面基准，该基准是通过模型不一致抽样从 60{,}000 个文档的语料库中选择的，涵盖手写表格、复杂布局、密集表格和图形丰富的页面。评估是通过单元测试式检查来执行的，该检查针对具体的故障模式（文本存在、阅读顺序和本地表约束），并结合旨在减少仅表示差异的特定于类别的标准化。在 15 个模型中，我们观察到最强的专有模型在手写和表格方面具有更高的鲁棒性，而一些开放权重系统在标准印刷布局上仍然具有竞争力。</li>
</ul>

<h3>Title: Spatial Chain-of-Thought: Bridging Understanding and Generation Models for Spatial Reasoning Generation</h3>
<ul>
<li><strong>Authors: </strong>Wei Chen, Yancheng Long, Mingqiao Liu, Haojie Ding, Yankai Yang, Hongyang Wei, Yi-Fan Zhang, Bin Wen, Fan Yang, Tingting Gao, Han Li, Long Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11980">https://arxiv.org/abs/2602.11980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11980">https://arxiv.org/pdf/2602.11980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11980]] Spatial Chain-of-Thought: Bridging Understanding and Generation Models for Spatial Reasoning Generation(https://arxiv.org/abs/2602.11980)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>While diffusion models have shown exceptional capabilities in aesthetic image synthesis, they often struggle with complex spatial understanding and reasoning. Existing approaches resort to Multimodal Large Language Models (MLLMs) to enhance this capability. However, they either incur high computational costs through joint training or suffer from spatial information loss when relying solely on textual prompts. To alleviate these limitations, we propose a Spatial Chain-of-Thought (SCoT) framework, a plug-and-play approach that effectively bridges the reasoning capabilities of MLLMs with the generative power of diffusion models. Specifically, we first enhance the diffusion model's layout awareness by training it on an interleaved text-coordinate instruction format. We then leverage state-of-the-art MLLMs as planners to generate comprehensive layout plans, transferring their spatial planning capabilities directly to the generation process. Extensive experiments demonstrate that our method achieves state-of-the-art performance on image generation benchmarks and significantly outperforms baselines on complex reasoning tasks, while also showing strong efficacy in image editing scenarios.</li>
<li><strong>摘要：</strong>虽然扩散模型在美学图像合成方面表现出了卓越的能力，但它们常常难以应对复杂的空间理解和推理。现有方法依靠多模式大语言模型（MLLM）来增强这种能力。然而，它们要么通过联合训练产生高昂的计算成本，要么在仅依赖文本提示时遭受空间信息丢失。为了缓解这些限制，我们提出了空间思想链（SCoT）框架，这是一种即插即用的方法，可以有效地将 MLLM 的推理能力与扩散模型的生成能力联系起来。具体来说，我们首先通过在交错文本坐标指令格式上进行训练来增强扩散模型的布局意识。然后，我们利用最先进的 MLLM 作为规划器来生成全面的布局规划，将其空间规划能力直接转移到生成过程中。大量的实验表明，我们的方法在图像生成基准上实现了最先进的性能，并且在复杂推理任务上显着优于基准，同时在图像编辑场景中也显示出强大的功效。</li>
</ul>

<h3>Title: Can Local Vision-Language Models improve Activity Recognition over Vision Transformers? -- Case Study on Newborn Resuscitation</h3>
<ul>
<li><strong>Authors: </strong>Enrico Guerriero, Kjersti Engan, Øyvind Meinich-Bache</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12002">https://arxiv.org/abs/2602.12002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12002">https://arxiv.org/pdf/2602.12002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12002]] Can Local Vision-Language Models improve Activity Recognition over Vision Transformers? -- Case Study on Newborn Resuscitation(https://arxiv.org/abs/2602.12002)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Accurate documentation of newborn resuscitation is essential for quality improvement and adherence to clinical guidelines, yet remains underutilized in practice. Previous work using 3D-CNNs and Vision Transformers (ViT) has shown promising results in detecting key activities from newborn resuscitation videos, but also highlighted the challenges in recognizing such fine-grained activities. This work investigates the potential of generative AI (GenAI) methods to improve activity recognition from such videos. Specifically, we explore the use of local vision-language models (VLMs), combined with large language models (LLMs), and compare them to a supervised TimeSFormer baseline. Using a simulated dataset comprising 13.26 hours of newborn resuscitation videos, we evaluate several zero-shot VLM-based strategies and fine-tuned VLMs with classification heads, including Low-Rank Adaptation (LoRA). Our results suggest that small (local) VLMs struggle with hallucinations, but when fine-tuned with LoRA, the results reach F1 score at 0.91, surpassing the TimeSformer results of 0.70.</li>
<li><strong>摘要：</strong>准确记录新生儿复苏对于提高质量和遵守临床指南至关重要，但在实践中仍未得到充分利用。之前使用 3D-CNN 和 Vision Transformers (ViT) 的工作在检测新生儿复苏视频中的关键活动方面显示出了良好的结果，但也凸显了识别此类细粒度活动的挑战。这项工作研究了生成人工智能 (GenAI) 方法改善此类视频活动识别的潜力。具体来说，我们探索了本地视觉语言模型（VLM）与大型语言模型（LLM）的结合，并将它们与有监督的 TimeSFormer 基线进行比较。使用包含 13.26 小时新生儿复苏视频的模拟数据集，我们评估了几种基于零样本 VLM 的策略和带有分类头的微调 VLM，包括低秩适应 (LoRA)。我们的结果表明，小型（本地）VLM 与幻觉作斗争，但当使用 LoRA 进行微调时，结果达到 F1 分数 0.91，超过了 TimeSformer 的结果 0.70。</li>
</ul>

<h3>Title: Fourier Transformers for Latent Crystallographic Diffusion and Generative Modeling</h3>
<ul>
<li><strong>Authors: </strong>Jed A. Duersch, Elohan Veillon, Astrid Klipfel, Adlane Sayede, Zied Bouraoui</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12045">https://arxiv.org/abs/2602.12045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12045">https://arxiv.org/pdf/2602.12045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12045]] Fourier Transformers for Latent Crystallographic Diffusion and Generative Modeling(https://arxiv.org/abs/2602.12045)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The discovery of new crystalline materials calls for generative models that handle periodic boundary conditions, crystallographic symmetries, and physical constraints, while scaling to large and structurally diverse unit cells. We propose a reciprocal-space generative pipeline that represents crystals through a truncated Fourier transform of the species-resolved unit-cell density, rather than modeling atomic coordinates directly. This representation is periodicity-native, admits simple algebraic actions of space-group symmetries, and naturally supports variable atomic multiplicities during generation, addressing a common limitation of particle-based approaches. Using only nine Fourier basis functions per spatial dimension, our approach reconstructs unit cells containing up to 108 atoms per chemical species. We instantiate this pipeline with a transformer variational autoencoder over complex-valued Fourier coefficients, and a latent diffusion model that generates in the compressed latent space. We evaluate reconstruction and latent diffusion on the LeMaterial benchmark and compare unconditional generation against coordinate-based baselines in the small-cell regime ($\leq 16$ atoms per unit cell).</li>
<li><strong>摘要：</strong>新晶体材料的发现需要生成模型来处理周期性边界条件、晶体对称性和物理约束，同时扩展到大型且结构多样的晶胞。我们提出了一种倒易空间生成管道，通过对物种解析的晶胞密度进行截断傅里叶变换来表示晶体，而不是直接对原子坐标进行建模。这种表示是周期性的，允许空间群对称的简单代数作用，并且自然地支持生成过程中可变的原子多重性，解决了基于粒子的方法的常见限制。我们的方法在每个空间维度仅使用 9 个傅里叶基函数，重建每个化学物质包含多达 108 个原子的晶胞。我们使用复值傅立叶系数上的变换器变分自动编码器以及在压缩潜在空间中生成的潜在扩散模型来实例化该管道。我们在 LeMaterial 基准上评估重建和潜在扩散，并将无条件生成与小细胞体系中基于坐标的基线进行比较（每个晶胞 $\leq 16$ 个原子）。</li>
</ul>

<h3>Title: Improving HPC Code Generation Capability of LLMs via Online Reinforcement Learning with Real-Machine Benchmark Rewards</h3>
<ul>
<li><strong>Authors: </strong>Ryo Mikasa, Shun-ichiro Hayashi, Daichi Mukunoki, Tetsuya Hoshino, Takahiro Katagiri</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12049">https://arxiv.org/abs/2602.12049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12049">https://arxiv.org/pdf/2602.12049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12049]] Improving HPC Code Generation Capability of LLMs via Online Reinforcement Learning with Real-Machine Benchmark Rewards(https://arxiv.org/abs/2602.12049)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated strong code generation capabilities, yet the runtime performance of generated code is not guaranteed, and there have been few attempts to train LLMs using runtime performance as a reward in the HPC domain. We propose an online reinforcement learning approach that executes LLM-generated code on a supercomputer and directly feeds back the measured runtime performance (GFLOPS) as a reward. We further introduce a Staged Quality-Diversity (SQD) algorithm that progressively varies the permitted optimization techniques on a per-problem basis, enabling the model to learn code optimization from diverse perspectives. We build a distributed system connecting a GPU training cluster with a CPU benchmarking cluster, and train Qwen2.5 Coder 14B on a double-precision matrix multiplication task using Group Relative Policy Optimization (GRPO). Through two experiments, we show that reinforcement learning combining runtime performance feedback with staged optimization can improve the HPC code generation capability of LLMs.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）已经展示了强大的代码生成能力，但生成代码的运行时性能无法得到保证，并且在 HPC 领域很少尝试使用运行时性能作为奖励来训练 LLM。我们提出了一种在线强化学习方法，在超级计算机上执行 LLM 生成的代码，并直接反馈测量的运行时性能（GFLOPS）作为奖励。我们进一步引入了分阶段质量多样性（SQD）算法，该算法根据每个问题逐步改变允许的优化技术，使模型能够从不同的角度学习代码优化。我们构建了一个连接 GPU 训练集群与 CPU 基准测试集群的分布式系统，并使用组相对策略优化 (GRPO) 在双精度矩阵乘法任务上训练 Qwen2.5 Coder 14B。通过两个实验，我们表明，将运行时性能反馈与阶段性优化相结合的强化学习可以提高法学硕士的 HPC 代码生成能力。</li>
</ul>

<h3>Title: AssetFormer: Modular 3D Assets Generation with Autoregressive Transformer</h3>
<ul>
<li><strong>Authors: </strong>Lingting Zhu, Shengju Qian, Haidi Fan, Jiayu Dong, Zhenchao Jin, Siwei Zhou, Gen Dong, Xin Wang, Lequan Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12100">https://arxiv.org/abs/2602.12100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12100">https://arxiv.org/pdf/2602.12100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12100]] AssetFormer: Modular 3D Assets Generation with Autoregressive Transformer(https://arxiv.org/abs/2602.12100)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The digital industry demands high-quality, diverse modular 3D assets, especially for user-generated content~(UGC). In this work, we introduce AssetFormer, an autoregressive Transformer-based model designed to generate modular 3D assets from textual descriptions. Our pilot study leverages real-world modular assets collected from online platforms. AssetFormer tackles the challenge of creating assets composed of primitives that adhere to constrained design parameters for various applications. By innovatively adapting module sequencing and decoding techniques inspired by language models, our approach enhances asset generation quality through autoregressive modeling. Initial results indicate the effectiveness of AssetFormer in streamlining asset creation for professional development and UGC scenarios. This work presents a flexible framework extendable to various types of modular 3D assets, contributing to the broader field of 3D content generation. The code is available at this https URL.</li>
<li><strong>摘要：</strong>数字行业需要高质量、多样化的模块化 3D 资产，尤其是用户生成内容~（UGC）。在这项工作中，我们介绍了 AssetFormer，这是一种基于 Transformer 的自回归模型，旨在根据文本描述生成模块化 3D 资产。我们的试点研究利用从在线平台收集的现实世界模块化资产。 AssetFormer 解决了创建由基元组成的资产的挑战，这些基元遵守各种应用程序的约束设计参数。通过创新地采用受语言模型启发的模块排序和解码技术，我们的方法通过自回归建模提高了资产生成质量。初步结果表明 AssetFormer 在简化专业发展和 UGC 场景资产创建方面的有效性。这项工作提出了一个可扩展到各种类型的模块化 3D 资产的灵活框架，为更广泛的 3D 内容生成领域做出了贡献。该代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation</h3>
<ul>
<li><strong>Authors: </strong>Wenkai Yang, Weijie Liu, Ruobing Xie, Kai Yang, Saiyong Yang, Yankai Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12125">https://arxiv.org/abs/2602.12125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12125">https://arxiv.org/pdf/2602.12125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12125]] Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation(https://arxiv.org/abs/2602.12125)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>On-policy distillation (OPD), which aligns the student with the teacher's logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this work, we first theoretically show that OPD is a special case of dense KL-constrained RL where the reward function and the KL regularization are always weighted equally and the reference model can by any model. Then, we propose the Generalized On-Policy Distillation (G-OPD) framework, which extends the standard OPD objective by introducing a flexible reference model and a reward scaling factor that controls the relative weight of the reward term against the KL regularization. Through comprehensive experiments on math reasoning and code generation tasks, we derive two novel insights: (1) Setting the reward scaling factor to be greater than 1 (i.e., reward extrapolation), which we term ExOPD, consistently improves over standard OPD across a range of teacher-student size pairings. In particular, in the setting where we merge the knowledge from different domain experts, obtained by applying domain-specific RL to the same student model, back into the original student, ExOPD enables the student to even surpass the teacher's performance boundary and outperform the domain teachers. (2) Building on ExOPD, we further find that in the strong-to-weak distillation setting (i.e., distilling a smaller student from a larger teacher), performing reward correction by choosing the reference model as the teacher's base model before RL yields a more accurate reward signal and further improves distillation performance. However, this choice assumes access to the teacher's pre-RL variant and incurs more computational overhead. We hope our work offers new insights for future research on OPD.</li>
<li><strong>摘要：</strong>在策略蒸馏 (OPD) 使学生与教师在学生生成的轨迹上的逻辑分布保持一致，在提高学生成绩方面表现出强大的经验收益，并且通常优于离策略蒸馏和强化学习 (RL) 范式。在这项工作中，我们首先从理论上证明 OPD 是密集 KL 约束 RL 的特例，其中奖励函数和 KL 正则化始终加权相等，并且参考模型可以是任何模型。然后，我们提出了广义在策略蒸馏（G-OPD）框架，该框架通过引入灵活的参考模型和控制奖励项相对于 KL 正则化的相对权重的奖励比例因子来扩展标准 OPD 目标。通过对数学推理和代码生成任务的综合实验，我们得出了两个新颖的见解：（1）将奖励比例因子设置为大于 1（即奖励外推法），我们称之为 ExOPD，在一系列师生规模配对中始终优于标准 OPD。特别是，在我们将不同领域专家的知识（通过将特定领域的强化学习应用到同一学生模型而获得的知识）合并回原始学生的情况下，ExOPD 使学生甚至能够超越教师的表现边界并超越领域教师。 (2) 在 ExOPD 的基础上，我们进一步发现，在强到弱的蒸馏设置中（即从体型较大的教师中蒸馏出较小的学生），在 RL 产生更准确的奖励信号并进一步提高蒸馏性能之前，通过选择参考模型作为教师的基础模型来进行奖励校正。然而，这种选择假设可以访问教师的预强化学习变体，并会产生更多的计算开销。我们希望我们的工作为 OPD 的未来研究提供新的见解。</li>
</ul>

<h3>Title: PosterOmni: Generalized Artistic Poster Creation via Task Distillation and Unified Reward Feedback</h3>
<ul>
<li><strong>Authors: </strong>Sixiang Chen, Jianyu Lai, Jialin Gao, Hengyu Shi, Zhongying Liu, Tian Ye, Junfeng Luo, Xiaoming Wei, Lei Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12127">https://arxiv.org/abs/2602.12127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12127">https://arxiv.org/pdf/2602.12127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12127]] PosterOmni: Generalized Artistic Poster Creation via Task Distillation and Unified Reward Feedback(https://arxiv.org/abs/2602.12127)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Image-to-poster generation is a high-demand task requiring not only local adjustments but also high-level design understanding. Models must generate text, layout, style, and visual elements while preserving semantic fidelity and aesthetic coherence. The process spans two regimes: local editing, where ID-driven generation, rescaling, filling, and extending must preserve concrete visual entities; and global creation, where layout- and style-driven tasks rely on understanding abstract design concepts. These intertwined demands make image-to-poster a multi-dimensional process coupling entity-preserving editing with concept-driven creation under image-prompt control. To address these challenges, we propose PosterOmni, a generalized artistic poster creation framework that unlocks the potential of a base edit model for multi-task image-to-poster generation. PosterOmni integrates the two regimes, namely local editing and global creation, within a single system through an efficient data-distillation-reward pipeline: (i) constructing multi-scenario image-to-poster datasets covering six task types across entity-based and concept-based creation; (ii) distilling knowledge between local and global experts for supervised fine-tuning; and (iii) applying unified PosterOmni Reward Feedback to jointly align visual entity-preserving and aesthetic preference across all tasks. Additionally, we establish PosterOmni-Bench, a unified benchmark for evaluating both local editing and global creation. Extensive experiments show that PosterOmni significantly enhances reference adherence, global composition quality, and aesthetic harmony, outperforming all open-source baselines and even surpassing several proprietary systems.</li>
<li><strong>摘要：</strong>图像到海报的生成是一项高要求的任务，不仅需要局部调整，还需要高水平的设计理解。模型必须生成文本、布局、风格和视觉元素，同时保持语义保真度和美学连贯性。该过程跨越两种机制：本地编辑，其中 ID 驱动的生成、重新缩放、填充和扩展必须保留具体的视觉实体；以及全局创作，其中布局和风格驱动的任务依赖于对抽象设计概念的理解。这些相互交织的需求使得图像到海报成为一个多维过程，将实体保留编辑与图像提示控制下的概念驱动创作相结合。为了应对这些挑战，我们提出了 PosterOmni，这是一个通用的艺术海报创作框架，它释放了多任务图像到海报生成的基本编辑模型的潜力。 PosterOmni 通过高效的数据蒸馏奖励管道将本地编辑和全局创建这两种机制集成在一个系统中：（i）构建多场景图像到海报数据集，涵盖基于实体和基于概念的创建的六种任务类型； (ii) 提炼本地和全球专家之间的知识以进行监督微调； (iii) 应用统一的 PosterOmni 奖励反馈来共同协调所有任务中的视觉实体保留和审美偏好。此外，我们还建立了 PosterOmni-Bench，这是一个用于评估本地编辑和全局创作的统一基准。大量实验表明，PosterOmni 显着提高了参考依从性、全局构图质量和审美和谐性，超越了所有开源基线，甚至超越了多个专有系统。</li>
</ul>

<h3>Title: It's TIME: Towards the Next Generation of Time Series Forecasting Benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Zhongzheng Qiao, Sheng Pan, Anni Wang, Viktoriya Zhukova, Yong Liu, Xudong Jiang, Qingsong Wen, Mingsheng Long, Ming Jin, Chenghao Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12147">https://arxiv.org/abs/2602.12147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12147">https://arxiv.org/pdf/2602.12147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12147]] It's TIME: Towards the Next Generation of Time Series Forecasting Benchmarks(https://arxiv.org/abs/2602.12147)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Time series foundation models (TSFMs) are revolutionizing the forecasting landscape from specific dataset modeling to generalizable task evaluation. However, we contend that existing benchmarks exhibit common limitations in four dimensions: constrained data composition dominated by reused legacy sources, compromised data integrity lacking rigorous quality assurance, misaligned task formulations detached from real-world contexts, and rigid analysis perspectives that obscure generalizable insights. To bridge these gaps, we introduce TIME, a next-generation task-centric benchmark comprising 50 fresh datasets and 98 forecasting tasks, tailored for strict zero-shot TSFM evaluation free from data leakage. Integrating large language models and human expertise, we establish a rigorous human-in-the-loop benchmark construction pipeline to ensure high data integrity and redefine task formulation by aligning forecasting configurations with real-world operational requirements and variate predictability. Furthermore, we propose a novel pattern-level evaluation perspective that moves beyond traditional dataset-level evaluations based on static meta labels. By leveraging structural time series features to characterize intrinsic temporal properties, this approach offers generalizable insights into model capabilities across diverse patterns. We evaluate 12 representative TSFMs and establish a multi-granular leaderboard to facilitate in-depth analysis and visualized inspection. The leaderboard is available at this https URL.</li>
<li><strong>摘要：</strong>时间序列基础模型 (TSFM) 正在彻底改变预测领域，从特定数据集建模到通用任务评估。然而，我们认为现有的基准在四个方面表现出共同的局限性：由重复使用的遗留来源主导的数据构成受限、缺乏严格的质量保证的数据完整性受损、脱离现实世界背景的不协调的任务制定以及模糊普遍见解的僵化分析视角。为了弥补这些差距，我们引入了 TIME，这是一种以任务为中心的下一代基准测试，包含 50 个新数据集和 98 个预测任务，专为严格的零样本 TSFM 评估而定制，不会造成数据泄漏。集成大型语言模型和人类专业知识，我们建立了严格的人机循环基准构建管道，以确保高度的数据完整性，并通过使预测配置与现实世界的操作要求和变量可预测性保持一致来重新定义任务制定。此外，我们提出了一种新颖的模式级评估视角，超越了基于静态元标签的传统数据集级评估。通过利用结构时间序列特征来表征内在时间属性，该方法提供了对跨不同模式的模型功能的普遍洞察。我们评估了12个具有代表性的TSFM，并建立了多粒度的排行榜，以方便深入分析和可视化检查。排行榜可通过此 https URL 获取。</li>
</ul>

<h3>Title: FAIL: Flow Matching Adversarial Imitation Learning for Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Yeyao Ma, Chen Li, Xiaosong Zhang, Han Hu, Weidi Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12155">https://arxiv.org/abs/2602.12155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12155">https://arxiv.org/pdf/2602.12155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12155]] FAIL: Flow Matching Adversarial Imitation Learning for Image Generation(https://arxiv.org/abs/2602.12155)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Post-training of flow matching models-aligning the output distribution with a high-quality target-is mathematically equivalent to imitation learning. While Supervised Fine-Tuning mimics expert demonstrations effectively, it cannot correct policy drift in unseen states. Preference optimization methods address this but require costly preference pairs or reward modeling. We propose Flow Matching Adversarial Imitation Learning (FAIL), which minimizes policy-expert divergence through adversarial training without explicit rewards or pairwise comparisons. We derive two algorithms: FAIL-PD exploits differentiable ODE solvers for low-variance pathwise gradients, while FAIL-PG provides a black-box alternative for discrete or computationally constrained settings. Fine-tuning FLUX with only 13,000 demonstrations from Nano Banana pro, FAIL achieves competitive performance on prompt following and aesthetic benchmarks. Furthermore, the framework generalizes effectively to discrete image and video generation, and functions as a robust regularizer to mitigate reward hacking in reward-based optimization. Code and data are available at this https URL.</li>
<li><strong>摘要：</strong>流匹配模型的后训练——将输出分布与高质量目标对齐——在数学上等同于模仿学习。虽然监督微调有效地模仿了专家的演示，但它无法纠正看不见的状态中的政策漂移。偏好优化方法可以解决这个问题，但需要昂贵的偏好对或奖励建模。我们提出了流匹配对抗性模仿学习（FAIL），它通过对抗性训练来最小化政策专家分歧，而无需明确的奖励或成对比较。我们推导出两种算法：FAIL-PD 利用可微分 ODE 求解器来实现低方差路径梯度，而 FAIL-PG 则为离散或计算约束设置提供黑盒替代方案。仅通过 Nano Banana pro 的 13,000 次演示对 FLUX 进行微调，FAIL 在快速跟随和美学基准方面实现了具有竞争力的性能。此外，该框架有效地推广到离散图像和视频生成，并作为强大的正则化器来减轻基于奖励的优化中的奖励黑客行为。代码和数据可从此 https URL 获取。</li>
</ul>

<h3>Title: TexSpot: 3D Texture Enhancement with Spatially-uniform Point Latent Representation</h3>
<ul>
<li><strong>Authors: </strong>Ziteng Lu, Yushuang Wu, Chongjie Ye, Yuda Qiu, Jing Shao, Xiaoyang Guo, Jiaqing Zhou, Tianlei Hu, Kun Zhou, Xiaoguang Han</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12157">https://arxiv.org/abs/2602.12157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12157">https://arxiv.org/pdf/2602.12157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12157]] TexSpot: 3D Texture Enhancement with Spatially-uniform Point Latent Representation(https://arxiv.org/abs/2602.12157)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>High-quality 3D texture generation remains a fundamental challenge due to the view-inconsistency inherent in current mainstream multi-view diffusion pipelines. Existing representations either rely on UV maps, which suffer from distortion during unwrapping, or point-based methods, which tightly couple texture fidelity to geometric density that limits high-resolution texture generation. To address these limitations, we introduce TexSpot, a diffusion-based texture enhancement framework. At its core is Texlet, a novel 3D texture representation that merges the geometric expressiveness of point-based 3D textures with the compactness of UV-based representation. Each Texlet latent vector encodes a local texture patch via a 2D encoder and is further aggregated using a 3D encoder to incorporate global shape context. A cascaded 3D-to-2D decoder reconstructs high-quality texture patches, enabling the Texlet space learning. Leveraging this representation, we train a diffusion transformer conditioned on Texlets to refine and enhance textures produced by multi-view diffusion methods. Extensive experiments demonstrate that TexSpot significantly improves visual fidelity, geometric consistency, and robustness over existing state-of-the-art 3D texture generation and enhancement approaches. Project page: this https URL.</li>
<li><strong>摘要：</strong>由于当前主流多视图扩散管道固有的视图不一致，高质量 3D 纹理生成仍然是一个基本挑战。现有的表示要么依赖于 UV 贴图（在展开过程中会出现失真），要么依赖于基于点的方法（将纹理保真度与几何密度紧密结合在一起，从而限制了高分辨率纹理的生成）。为了解决这些限制，我们引入了 TexSpot，一种基于扩散的纹理增强框架。其核心是 Texlet，这是一种新颖的 3D 纹理表示，它将基于点的 3D 纹理的几何表现力与基于 UV 的表示的紧凑性融为一体。每个 Texlet 潜在向量通过 2D 编码器对局部纹理块进行编码，并使用 3D 编码器进一步聚合以合并全局形状上下文。级联 3D 到 2D 解码器可重建高质量纹理块，从而实现 Texlet 空间学习。利用这种表示，我们训练了一个以 Texlet 为条件的扩散变换器，以细化和增强多视图扩散方法产生的纹理。大量实验表明，与现有最先进的 3D 纹理生成和增强方法相比，TexSpot 显着提高了视觉保真度、几何一致性和鲁棒性。项目页面：此 https URL。</li>
</ul>

<h3>Title: SafeNeuron: Neuron-Level Safety Alignment for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhaoxin Wang, Jiaming Liang, Fengbin Zhu, Weixiang Zhao, Junfeng Fang, Jiayi Ji, Handing Wang, Tat-Seng Chua</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12158">https://arxiv.org/abs/2602.12158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12158">https://arxiv.org/pdf/2602.12158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12158]] SafeNeuron: Neuron-Level Safety Alignment for Large Language Models(https://arxiv.org/abs/2602.12158)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) and multimodal LLMs are typically safety-aligned before release to prevent harmful content generation. However, recent studies show that safety behaviors are concentrated in a small subset of parameters, making alignment brittle and easily bypassed through neuron-level attacks. Moreover, most existing alignment methods operate at the behavioral level, offering limited control over the model's internal safety mechanisms. In this work, we propose SafeNeuron, a neuron-level safety alignment framework that improves robustness by redistributing safety representations across the network. SafeNeuron first identifies safety-related neurons, then freezes these neurons during preference optimization to prevent reliance on sparse safety pathways and force the model to construct redundant safety representations. Extensive experiments across models and modalities demonstrate that SafeNeuron significantly improves robustness against neuron pruning attacks, reduces the risk of open-source models being repurposed as red-team generators, and preserves general capabilities. Furthermore, our layer-wise analysis reveals that safety behaviors are governed by stable and shared internal representations. Overall, SafeNeuron provides an interpretable and robust perspective for model alignment.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 和多模式 LLM 通常在发布前进行安全调整，以防止生成有害内容。然而，最近的研究表明，安全行为集中在一小部分参数中，使得对齐变得脆弱，并且很容易通过神经元级攻击绕过。此外，大多数现有的对齐方法在行为层面上运行，对模型的内部安全机制提供有限的控制。在这项工作中，我们提出了 SafeNeuron，这是一种神经元级安全对齐框架，它通过在网络上重新分配安全表示来提高鲁棒性。 SafeNeuron 首先识别与安全相关的神经元，然后在偏好优化期间冻结这些神经元，以防止对稀疏安全路径的依赖，并迫使模型构建冗余的安全表示。跨模型和模式的广泛实验表明，SafeNeuron 显着提高了针对神经元修剪攻击的鲁棒性，降低了开源模型被重新用作红队生成器的风险，并保留了一般功能。此外，我们的分层分析表明，安全行为是由稳定且共享的内部表示控制的。总体而言，SafeNeuron 为模型对齐提供了可解释且稳健的视角。</li>
</ul>

<h3>Title: DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Xu Guo, Fulong Ye, Qichao Sun, Liyang Chen, Bingchuan Li, Pengze Zhang, Jiawei Liu, Songtao Zhao, Qian He, Xiangwang Hou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12160">https://arxiv.org/abs/2602.12160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12160">https://arxiv.org/pdf/2602.12160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12160]] DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation(https://arxiv.org/abs/2602.12160)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in foundation models have revolutionized joint audio-video generation. However, existing approaches typically treat human-centric tasks including reference-based audio-video generation (R2AV), video editing (RV2AV) and audio-driven video animation (RA2V) as isolated objectives. Furthermore, achieving precise, disentangled control over multiple character identities and voice timbres within a single framework remains an open challenge. In this paper, we propose DreamID-Omni, a unified framework for controllable human-centric audio-video generation. Specifically, we design a Symmetric Conditional Diffusion Transformer that integrates heterogeneous conditioning signals via a symmetric conditional injection scheme. To resolve the pervasive identity-timbre binding failures and speaker confusion in multi-person scenarios, we introduce a Dual-Level Disentanglement strategy: Synchronized RoPE at the signal level to ensure rigid attention-space binding, and Structured Captions at the semantic level to establish explicit attribute-subject mappings. Furthermore, we devise a Multi-Task Progressive Training scheme that leverages weakly-constrained generative priors to regularize strongly-constrained tasks, preventing overfitting and harmonizing disparate objectives. Extensive experiments demonstrate that DreamID-Omni achieves comprehensive state-of-the-art performance across video, audio, and audio-visual consistency, even outperforming leading proprietary commercial models. We will release our code to bridge the gap between academic research and commercial-grade applications.</li>
<li><strong>摘要：</strong>基础模型的最新进展彻底改变了联合音频视频生成。然而，现有方法通常将以人为中心的任务视为孤立的目标，包括基于参考的音频视频生成（R2AV）、视频编辑（RV2AV）和音频驱动视频动画（RA2V）。此外，在单一框架内实现对多个角色身份和音色的精确、分离的控制仍然是一个开放的挑战。在本文中，我们提出了 DreamID-Omni，这是一个用于可控的以人为中心的音频视频生成的统一框架。具体来说，我们设计了一个对称条件扩散变压器，它通过对称条件注入方案集成异构调节信号。为了解决多人场景中普遍存在的身份-音色绑定失败和说话者混乱的问题，我们引入了一种双层解缠策略：信号级别的同步 RoPE 以确保严格的注意力空间绑定，语义级别的结构化字幕以建立明确的属性-主题映射。此外，我们设计了一种多任务渐进训练方案，利用弱约束的生成先验来规范强约束的任务，防止过度拟合并协调不同的目标。大量实验表明，DreamID-Omni 在视频、音频和视听一致性方面实现了全面的最先进性能，甚至超越了领先的专有商业模型。我们将发布我们的代码，以弥合学术研究和商业级应用程序之间的差距。</li>
</ul>

<h3>Title: EO-VAE: Towards A Multi-sensor Tokenizer for Earth Observation Data</h3>
<ul>
<li><strong>Authors: </strong>Nils Lehmann, Yi Wang, Zhitong Xiong, Xiaoxiang Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12177">https://arxiv.org/abs/2602.12177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12177">https://arxiv.org/pdf/2602.12177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12177]] EO-VAE: Towards A Multi-sensor Tokenizer for Earth Observation Data(https://arxiv.org/abs/2602.12177)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>State-of-the-art generative image and video models rely heavily on tokenizers that compress high-dimensional inputs into more efficient latent representations. While this paradigm has revolutionized RGB generation, Earth observation (EO) data presents unique challenges due to diverse sensor specifications and variable spectral channels. We propose EO-VAE, a multi-sensor variational autoencoder designed to serve as a foundational tokenizer for the EO domain. Unlike prior approaches that train separate tokenizers for each modality, EO-VAE utilizes a single model to encode and reconstruct flexible channel combinations via dynamic hypernetworks. Our experiments on the TerraMesh dataset demonstrate that EO-VAE achieves superior reconstruction fidelity compared to the TerraMind tokenizers, establishing a robust baseline for latent generative modeling in remote sensing.</li>
<li><strong>摘要：</strong>最先进的生成图像和视频模型在很大程度上依赖于将高维输入压缩为更有效的潜在表示的分词器。虽然这种范例彻底改变了 RGB 生成，但由于不同的传感器规格和可变的光谱通道，地球观测 (EO) 数据提出了独特的挑战。我们提出了 EO-VAE，一种多传感器变分自动编码器，旨在作为 EO 领域的基础标记器。与之前为每种模态训练单独的标记器的方法不同，EO-VAE 利用单个模型通过动态超网络来编码和重建灵活的通道组合。我们在 TerraMesh 数据集上的实验表明，与 TerraMind 分词器相比，EO-VAE 实现了卓越的重建保真度，为遥感中的潜在生成建模建立了稳健的基线。</li>
</ul>

<h3>Title: DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing</h3>
<ul>
<li><strong>Authors: </strong>Dianyi Wang, Ruihang Li, Feng Han, Chaofan Ma, Wei Song, Siyuan Wang, Yibin Wang, Yi Xin, Hongjian Liu, Zhixiong Zhang, Shengyuan Ding, Tianhang Wang, Zhenglin Cheng, Tao Lin, Cheng Jin, Kaicheng Yu, Jingjing Chen, Wenjie Wang, Zhongyu Wei, Jiaqi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12205">https://arxiv.org/abs/2602.12205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12205">https://arxiv.org/pdf/2602.12205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12205]] DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing(https://arxiv.org/abs/2602.12205)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Current unified multimodal models for image generation and editing typically rely on massive parameter scales (e.g., >10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, a lightweight 5B unified model that achieves comprehensive capabilities competitive with or surpassing much larger counterparts. To overcome the limitations of compact models in semantic understanding and fine-grained control, we introduce Stacked Channel Bridging (SCB), a deep alignment framework that extracts hierarchical features from multiple VLM layers and fuses them with learnable 'think tokens' to provide the generative backbone with structured, reasoning-rich guidance. We further design a data-centric training strategy spanning three progressive stages: (1) Alignment Pre-training on large-scale image-text pairs and editing triplets to synchronize VLM and DiT representations, (2) Joint Supervised Fine-tuning on a high-quality mixture of generation, editing, and reasoning tasks to foster omni-capabilities, and (3) Reinforcement Learning with MR-GRPO, which leverages a mixture of reward functions and supervision signals, resulting in substantial gains in generation quality and alignment with human preferences, while maintaining stable training progress and avoiding visual artifacts. Despite being trained on only ~50M samples, DeepGen 1.0 achieves leading performance across diverse benchmarks, surpassing the 80B HunyuanImage by 28% on WISE and the 27B Qwen-Image-Edit by 37% on UniREditBench. By open-sourcing our training code, weights, and datasets, we provide an efficient, high-performance alternative to democratize unified multimodal research.</li>
<li><strong>摘要：</strong>当前用于图像生成和编辑的统一多模态模型通常依赖于大量参数尺度（例如，>10B），从而导致训练成本和部署占用空间过高。在这项工作中，我们提出了 DeepGen 1.0，这是一种轻量级的 5B 统一模型，其综合能力可与更大的同行竞争或超越。为了克服紧凑模型在语义理解和细粒度控制方面的局限性，我们引入了堆叠通道桥接（SCB），这是一种深度对齐框架，它从多个 VLM 层中提取层次特征，并将它们与可学习的“思考标记”融合，为生成主干提供结构化、推理丰富的指导。我们进一步设计了一个以数据为中心的训练策略，跨越三个渐进阶段：(1) 对大规模图像文本对和编辑三元组进行对齐预训练，以同步 VLM 和 DiT 表示，(2) 对生成、编辑和推理任务的高质量混合进行联合监督微调，以培养全方位能力，以及 (3) 使用 MR-GRPO 进行强化学习，它利用奖励函数和监督信号的混合，从而在生成质量和对齐方面取得实质性进展符合人类偏好，同时保持稳定的训练进度并避免视觉伪影。尽管仅接受了约 5000 万样本的训练，但 DeepGen 1.0 在各种基准测试中均实现了领先的性能，在 WISE 上超过 80B HunyuanImage 28%，在 UniREditBench 上超过 27B Qwen-Image-Edit 37%。通过开源我们的训练代码、权重和数据集，我们提供了一种高效、高性能的替代方案，使统一的多模式研究民主化。</li>
</ul>

<h3>Title: Best of Both Worlds: Multimodal Reasoning and Generation via Unified Discrete Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Onkar Susladkar, Tushar Prakash, Gayatri Deshmukh, Kiet A. Nguyen, Jiaxun Zhang, Adheesh Juvekar, Tianshu Bao, Lin Chai, Sparsh Mittal, Inderjit S Dhillon, Ismini Lourentzou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12221">https://arxiv.org/abs/2602.12221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12221">https://arxiv.org/pdf/2602.12221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12221]] Best of Both Worlds: Multimodal Reasoning and Generation via Unified Discrete Flow Matching(https://arxiv.org/abs/2602.12221)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We propose UniDFlow, a unified discrete flow-matching framework for multimodal understanding, generation, and editing. It decouples understanding and generation via task-specific low-rank adapters, avoiding objective interference and representation entanglement, while a novel reference-based multimodal preference alignment optimizes relative outcomes under identical conditioning, improving faithfulness and controllability without large-scale retraining. UniDFlpw achieves SOTA performance across eight benchmarks and exhibits strong zero-shot generalization to tasks including inpainting, in-context image generation, reference-based editing, and compositional generation, despite no explicit task-specific training.</li>
<li><strong>摘要：</strong>我们提出了 UniDFlow，一个用于多模态理解、生成和编辑的统一离散流匹配框架。它通过特定于任务的低阶适配器将理解和生成解耦，避免客观干扰和表示纠缠，同时一种新颖的基于参考的多模态偏好对齐在相同条件下优化相对结果，无需大规模再训练即可提高忠实度和可控性。尽管没有明确的特定任务训练，UniDFlpw 在八个基准测试中实现了 SOTA 性能，并对修复、上下文图像生成、基于参考的编辑和合成生成等任务表现出强大的零样本泛化能力。</li>
</ul>

<h3>Title: Categorical Flow Maps</h3>
<ul>
<li><strong>Authors: </strong>Daan Roos, Oscar Davis, Floor Eijkelboom, Michael Bronstein, Max Welling, İsmail İlkan Ceylan, Luca Ambrogioni, Jan-Willem van de Meent</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12233">https://arxiv.org/abs/2602.12233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12233">https://arxiv.org/pdf/2602.12233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12233]] Categorical Flow Maps(https://arxiv.org/abs/2602.12233)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce Categorical Flow Maps, a flow-matching method for accelerated few-step generation of categorical data via self-distillation. Building on recent variational formulations of flow matching and the broader trend towards accelerated inference in diffusion and flow-based models, we define a flow map towards the simplex that transports probability mass toward a predicted endpoint, yielding a parametrisation that naturally constrains model predictions. Since our trajectories are continuous rather than discrete, Categorical Flow Maps can be trained with existing distillation techniques, as well as a new objective based on endpoint consistency. This continuous formulation also automatically unlocks test-time inference: we can directly reuse existing guidance and reweighting techniques in the categorical setting to steer sampling toward downstream objectives. Empirically, we achieve state-of-the-art few-step results on images, molecular graphs, and text, with strong performance even in single-step generation.</li>
<li><strong>摘要：</strong>我们引入了分类流图，这是一种流匹配方法，用于通过自蒸馏加速分类数据的几步生成。基于最近的流匹配变分公式以及扩散和基于流的模型中加速推理的更广泛趋势，我们定义了一个朝向单纯形的流图，它将概率质量传输到预测的端点，产生自然约束模型预测的参数化。由于我们的轨迹是连续的而不是离散的，因此可以使用现有的蒸馏技术以及基于端点一致性的新目标来训练分类流图。这种连续的公式还会自动解锁测试时推理：我们可以在分类设置中直接重用现有的指导和重新加权技术，以引导采样实现下游目标。根据经验，我们在图像、分子图和文本上实现了最先进的几步结果，即使在单步生成中也具有很强的性能。</li>
</ul>

<h3>Title: Self-Supervised Learning via Flow-Guided Neural Operator on Time-Series Data</h3>
<ul>
<li><strong>Authors: </strong>Duy Nguyen, Jiachen Yao, Jiayun Wang, Julius Berner, Animashree Anandkumar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12267">https://arxiv.org/abs/2602.12267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12267">https://arxiv.org/pdf/2602.12267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12267]] Self-Supervised Learning via Flow-Guided Neural Operator on Time-Series Data(https://arxiv.org/abs/2602.12267)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) is a powerful paradigm for learning from unlabeled time-series data. However, popular methods such as masked autoencoders (MAEs) rely on reconstructing inputs from a fixed, predetermined masking ratio. Instead of this static design, we propose treating the corruption level as a new degree of freedom for representation learning, enhancing flexibility and performance. To achieve this, we introduce the Flow-Guided Neural Operator (FGNO), a novel framework combining operator learning with flow matching for SSL training. FGNO learns mappings in functional spaces by using Short-Time Fourier Transform to unify different time resolutions. We extract a rich hierarchy of features by tapping into different network layers and flow times that apply varying strengths of noise to the input data. This enables the extraction of versatile representations, from low-level patterns to high-level global features, using a single model adaptable to specific tasks. Unlike prior generative SSL methods that use noisy inputs during inference, we propose using clean inputs for representation extraction while learning representations with noise; this eliminates randomness and boosts accuracy. We evaluate FGNO across three biomedical domains, where it consistently outperforms established baselines. Our method yields up to 35% AUROC gains in neural signal decoding (BrainTreeBank), 16% RMSE reductions in skin temperature prediction (DREAMT), and over 20% improvement in accuracy and macro-F1 on SleepEDF under low-data regimes. These results highlight FGNO's robustness to data scarcity and its superior capacity to learn expressive representations for diverse time series.</li>
<li><strong>摘要：</strong>自监督学习 (SSL) 是从未标记的时间序列数据中学习的强大范例。然而，诸如屏蔽自动编码器（MAE）之类的流行方法依赖于根据固定的、预定的屏蔽比率重建输入。我们建议将腐败水平视为表示学习的新自由度，而不是这种静态设计，从而增强灵活性和性能。为了实现这一目标，我们引入了流引导神经算子（FGNO），这是一种将算子学习与流匹配相结合的新颖框架，用于 SSL 训练。 FGNO 通过使用短时傅里叶变换来学习功能空间中的映射来统一不同的时间分辨率。我们通过利用不同的网络层和流动时间来提取丰富的特征层次，这些网络层和流动时间对输入数据应用不同强度的噪声。这使得能够使用适用于特定任务的单个模型来提取从低级模式到高级全局特征的通用表示。与先前在推理过程中使用噪声输入的生成 SSL 方法不同，我们建议在学习带有噪声的表示时使用干净的输入进行表示提取；这消除了随机性并提高了准确性。我们在三个生物医学领域评估 FGNO，其表现始终优于既定基线。我们的方法在神经信号解码 (BrainTreeBank) 中产生高达 35% 的 AUROC 增益，在皮肤温度预测 (DREAMT) 中使 RMSE 降低 16%，并且在低数据情况下 SleepEDF 的准确性和宏 F1 提高超过 20%。这些结果凸显了 FGNO 对数据稀缺的鲁棒性及其学习不同时间序列表达表示的卓越能力。</li>
</ul>

<h3>Title: MonarchRT: Efficient Attention for Real-Time Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Krish Agarwal, Zhuoming Chen, Cheng Luo, Yongqi Chen, Haizhong Zheng, Xun Huang, Atri Rudra, Beidi Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12271">https://arxiv.org/abs/2602.12271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12271">https://arxiv.org/pdf/2602.12271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12271]] MonarchRT: Efficient Attention for Real-Time Video Generation(https://arxiv.org/abs/2602.12271)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Real-time video generation with Diffusion Transformers is bottlenecked by the quadratic cost of 3D self-attention, especially in real-time regimes that are both few-step and autoregressive, where errors compound across time and each denoising step must carry substantially more information. In this setting, we find that prior sparse-attention approximations break down, despite showing strong results for bidirectional, many-step diffusion. Specifically, we observe that video attention is not reliably sparse, but instead combines pronounced periodic structure driven by spatiotemporal position with dynamic, sparse semantic correspondences and dense mixing, exceeding the representational capacity of even oracle top-k attention. Building on this insight, we propose Monarch-RT, a structured attention parameterization for video diffusion models that factorizes attention using Monarch matrices. Through appropriately aligned block structure and our extended tiled Monarch parameterization, we achieve high expressivity while preserving computational efficiency. We further overcome the overhead of parameterization through finetuning, with custom Triton kernels. We first validate the high efficacy of Monarch-RT over existing sparse baselines designed only for bidirectional models. We further observe that Monarch-RT attains up to 95% attention sparsity with no loss in quality when applied to the state-of-the-art model Self-Forcing, making Monarch-RT a pioneering work on highly-capable sparse attention parameterization for real-time video generation. Our optimized implementation outperforms FlashAttention-2, FlashAttention-3, and FlashAttention-4 kernels on Nvidia RTX 5090, H100, and B200 GPUs respectively, providing kernel speedups in the range of 1.4-11.8X. This enables us, for the first time, to achieve true real-time video generation with Self-Forcing at 16 FPS on a single RTX 5090.</li>
<li><strong>摘要：</strong>使用扩散变压器的实时视频生成受到 3D 自注意力二次成本的瓶颈，特别是在少步和自回归的实时机制中，其中误差随时间复合，每个去噪步骤必须携带更多信息。在这种情况下，我们发现尽管双向、多步扩散显示出很强的结果，但先前的稀疏注意力近似方法还是失效了。具体来说，我们观察到视频注意力并不是可靠的稀疏，而是将由时空位置驱动的显着周期结构与动态、稀疏语义对应和密集混合相结合，甚至超过了 oracle top-k 注意力的表示能力。基于这一见解，我们提出了 Monarch-RT，这是一种用于视频扩散模型的结构化注意力参数化，它使用 Monarch 矩阵分解注意力。通过适当对齐的块结构和扩展的平铺 Monarch 参数化，我们在保持计算效率的同时实现了高表达力。我们通过使用自定义 Triton 内核进行微调，进一步克服了参数化的开销。我们首先验证 Monarch-RT 相对于仅为双向模型设计的现有稀疏基线的高效性。我们进一步观察到，当应用于最先进的模型 Self-Forcing 时，Monarch-RT 获得了高达 95% 的注意力稀疏度，且质量没有损失，这使得 Monarch-RT 成为实时视频生成的高性能稀疏注意力参数化的开创性工作。我们的优化实现在 Nvidia RTX 5090、H100 和 B200 GPU 上的性能分别优于 FlashAttention-2、FlashAttention-3 和 FlashAttention-4 内核，提供 1.4-11.8 倍的内核加速。这使我们第一次能够在单个 RTX 5090 上以 16 FPS 的速度实现真正的实时视频生成。</li>
</ul>

<h3>Title: Function-Space Decoupled Diffusion for Forward and Inverse Modeling in Carbon Capture and Storage</h3>
<ul>
<li><strong>Authors: </strong>Xin Ju, Jiachen Yao, Anima Anandkumar, Sally M. Benson, Gege Wen</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.geo-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12274">https://arxiv.org/abs/2602.12274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12274">https://arxiv.org/pdf/2602.12274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12274]] Function-Space Decoupled Diffusion for Forward and Inverse Modeling in Carbon Capture and Storage(https://arxiv.org/abs/2602.12274)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Accurate characterization of subsurface flow is critical for Carbon Capture and Storage (CCS) but remains challenged by the ill-posed nature of inverse problems with sparse observations. We present Fun-DDPS, a generative framework that combines function-space diffusion models with differentiable neural operator surrogates for both forward and inverse modeling. Our approach learns a prior distribution over geological parameters (geomodel) using a single-channel diffusion model, then leverages a Local Neural Operator (LNO) surrogate to provide physics-consistent guidance for cross-field conditioning on the dynamics field. This decoupling allows the diffusion prior to robustly recover missing information in parameter space, while the surrogate provides efficient gradient-based guidance for data assimilation. We demonstrate Fun-DDPS on synthetic CCS modeling datasets, achieving two key results: (1) For forward modeling with only 25% observations, Fun-DDPS achieves 7.7% relative error compared to 86.9% for standard surrogates (an 11x improvement), proving its capability to handle extreme data sparsity where deterministic methods fail. (2) We provide the first rigorous validation of diffusion-based inverse solvers against asymptotically exact Rejection Sampling (RS) posteriors. Both Fun-DDPS and the joint-state baseline (Fun-DPS) achieve Jensen-Shannon divergence less than 0.06 against the ground truth. Crucially, Fun-DDPS produces physically consistent realizations free from the high-frequency artifacts observed in joint-state baselines, achieving this with 4x improved sample efficiency compared to rejection sampling.</li>
<li><strong>摘要：</strong>地下流的准确表征对于碳捕获和封存（CCS）至关重要，但仍然受到稀疏观测反演问题的不适定性质的挑战。我们提出了 Fun-DDPS，这是一个将函数空间扩散模型与可微神经算子代理相结合的生成框架，用于正向和逆向建模。我们的方法使用单通道扩散模型学习地质参数（地质模型）的先验分布，然后利用局部神经算子（LNO）代理为动力学场的跨场调节提供物理一致的指导。这种解耦允许在参数空间中稳健地恢复丢失的信息之前进行扩散，而替代项则为数据同化提供有效的基于梯度的指导。我们在合成 CCS 建模数据集上演示了 Fun-DDPS，取得了两个关键结果：(1) 对于仅包含 25% 观测值的正向建模，Fun-DDPS 实现了 7.7% 的相对误差，而标准替代项的相对误差为 86.9%（提高了 11 倍），证明了其处理确定性方法失败的极端数据稀疏性的能力。 (2) 我们首次对基于扩散的逆求解器针对渐进精确拒绝采样 (RS) 后验进行了严格验证。 Fun-DDPS 和联合状态基线 (Fun-DPS) 的 Jensen-Shannon 散度相对于真实情况均小于 0.06。至关重要的是，Fun-DDPS 产生了物理上一致的实现，没有在联合状态基线中观察到的高频伪影，与拒绝采样相比，样本效率提高了 4 倍，从而实现了这一目标。</li>
</ul>

<h3>Title: UniT: Unified Multimodal Chain-of-Thought Test-time Scaling</h3>
<ul>
<li><strong>Authors: </strong>Leon Liangyu Chen, Haoyu Ma, Zhipeng Fan, Ziqi Huang, Animesh Sinha, Xiaoliang Dai, Jialiang Wang, Zecheng He, Jianwei Yang, Chunyuan Li, Junzhe Sun, Chu Wang, Serena Yeung-Levy, Felix Juefei-Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12279">https://arxiv.org/abs/2602.12279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12279">https://arxiv.org/pdf/2602.12279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12279]] UniT: Unified Multimodal Chain-of-Thought Test-time Scaling(https://arxiv.org/abs/2602.12279)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis, unified model training, and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning. These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models.</li>
<li><strong>摘要：</strong>统一模型可以在单个架构中处理多模态理解和生成，但它们通常在单次传递中运行，而无需迭代地细化其输出。许多多模态任务，特别是那些涉及复杂空间组成、多个交互对象或不断发展的指令的任务，需要分解指令、验证中间结果并进行迭代修正。虽然测试时间扩展 (TTS) 已经证明，为迭代推理分配额外的推理计算可以显着提高语言模型的性能，但将此范式扩展到统一的多模态模型仍然是一个开放的挑战。我们引入了 UnitT，这是一个用于多模式思想链测试时间扩展的框架，它使单个统一模型能够在多轮中进行推理、验证和细化。 UnitT 结合了代理数据合成、统一模型训练和灵活的测试时推理，以引发认知行为，包括验证、子目标分解和内容记忆。我们的主要发现是：（1）在短推理轨迹上训练的统一模型在测试时可以推广到更长的推理链； (2) 顺序思想链推理提供了比并行采样更具可扩展性和计算效率的 TTS 策略； （3）生成和编辑轨迹的训练改善了分布外的视觉推理。这些结果将多模式测试时间缩放确立为促进统一模型的生成和理解的有效范例。</li>
</ul>

<h3>Title: Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching</h3>
<ul>
<li><strong>Authors: </strong>Huai-Hsun Cheng, Siang-Ling Zhang, Yu-Lun Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.12280">https://arxiv.org/abs/2602.12280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.12280">https://arxiv.org/pdf/2602.12280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.12280]] Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching(https://arxiv.org/abs/2602.12280)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Visual illusions traditionally rely on spatial manipulations such as multi-view consistency. In this work, we introduce Progressive Semantic Illusions, a novel vector sketching task where a single sketch undergoes a dramatic semantic transformation through the sequential addition of strokes. We present Stroke of Surprise, a generative framework that optimizes vector strokes to satisfy distinct semantic interpretations at different drawing stages. The core challenge lies in the "dual-constraint": initial prefix strokes must form a coherent object (e.g., a duck) while simultaneously serving as the structural foundation for a second concept (e.g., a sheep) upon adding delta strokes. To address this, we propose a sequence-aware joint optimization framework driven by a dual-branch Score Distillation Sampling (SDS) mechanism. Unlike sequential approaches that freeze the initial state, our method dynamically adjusts prefix strokes to discover a "common structural subspace" valid for both targets. Furthermore, we introduce a novel Overlay Loss that enforces spatial complementarity, ensuring structural integration rather than occlusion. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art baselines in recognizability and illusion strength, successfully expanding visual anagrams from the spatial to the temporal dimension. Project page: this https URL</li>
<li><strong>摘要：</strong>视觉错觉传统上依赖于空间操纵，例如多视图一致性。在这项工作中，我们介绍了渐进式语义幻觉，这是一种新颖的矢量草图任务，其中单个草图通过顺序添加笔画经历戏剧性的语义转换。我们提出了 Stroke of Surprise，这是一个生成框架，可以优化矢量笔画以满足不同绘图阶段的不同语义解释。核心挑战在于“双重约束”：初始前缀笔划必须形成一个连贯的物体（例如鸭子），同时在添加增量笔划后充当第二个概念（例如羊）的结构基础。为了解决这个问题，我们提出了一种由双分支分数蒸馏采样（SDS）机制驱动的序列感知联合优化框架。与冻结初始状态的顺序方法不同，我们的方法动态调整前缀笔画以发现对两个目标都有效的“公共结构子空间”。此外，我们引入了一种新颖的叠加损失，可以增强空间互补性，确保结构整合而不是遮挡。大量的实验表明，我们的方法在可识别性和错觉强度方面显着优于最先进的基线，成功地将视觉字谜从空间维度扩展到时间维度。项目页面：此 https URL</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
