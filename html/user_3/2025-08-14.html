<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-08-14</h1>
<h3>Title: Physics-Constrained Fine-Tuning of Flow-Matching Models for Generation and Inverse Problems</h3>
<ul>
<li><strong>Authors: </strong>Jan Tauberschmidt, Sophie Fellenz, Sebastian J. Vollmer, Andrew B. Duncan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09156">https://arxiv.org/abs/2508.09156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09156">https://arxiv.org/pdf/2508.09156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09156]] Physics-Constrained Fine-Tuning of Flow-Matching Models for Generation and Inverse Problems(https://arxiv.org/abs/2508.09156)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We present a framework for fine-tuning flow-matching generative models to enforce physical constraints and solve inverse problems in scientific systems. Starting from a model trained on low-fidelity or observational data, we apply a differentiable post-training procedure that minimizes weak-form residuals of governing partial differential equations (PDEs), promoting physical consistency and adherence to boundary conditions without distorting the underlying learned distribution. To infer unknown physical inputs, such as source terms, material parameters, or boundary data, we augment the generative process with a learnable latent parameter predictor and propose a joint optimization strategy. The resulting model produces physically valid field solutions alongside plausible estimates of hidden parameters, effectively addressing ill-posed inverse problems in a data-driven yet physicsaware manner. We validate our method on canonical PDE benchmarks, demonstrating improved satisfaction of PDE constraints and accurate recovery of latent coefficients. Our approach bridges generative modelling and scientific inference, opening new avenues for simulation-augmented discovery and data-efficient modelling of physical systems.</li>
<li><strong>摘要：</strong>我们提出了一个微调流量匹配生成模型的框架，以实施物理约束并解决科学系统中的反问题。从对低保真性或观察数据进行训练的模型开始，我们采用可区分的训练后程序，将管理部分微分方程（PDES）的弱形式残留降至最低，从而促进了物理一致性和对边界条件的依从性，而不会扭曲潜在的学识分布。为了推断未知的物理输入，例如源术语，材料参数或边界数据，我们使用可学习的潜在参数预测器增强生成过程，并提出关节优化策略。最终的模型与隐藏参数的合理估计产生了物理有效的现场解决方案，从而有效地解决了以数据驱动但物理学的方式解决不良的反问题。我们验证了规范PDE基准测试的方法，证明了PDE限制的满意度提高并准确地恢复潜在系数。我们的方法桥接了生成的建模和科学推断，为实体系统的模拟发现和数据有效的建模开辟了新的途径。</li>
</ul>

<h3>Title: EvaDrive: Evolutionary Adversarial Policy Optimization for End-to-End Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Siwen Jiao, Kangan Qian, Hao Ye, Yang Zhong, Ziang Luo, Sicong Jiang, Zilin Huang, Yangyi Fang, Jinyu Miao, Zheng Fu, Yunlong Wang, Kun Jiang, Diange Yang, Rui Fan, Baoyun Peng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09158">https://arxiv.org/abs/2508.09158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09158">https://arxiv.org/pdf/2508.09158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09158]] EvaDrive: Evolutionary Adversarial Policy Optimization for End-to-End Autonomous Driving(https://arxiv.org/abs/2508.09158)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, quality assessment</a></li>
<li><strong>Abstract: </strong>Autonomous driving faces significant challenges in achieving human-like iterative decision-making, which continuously generates, evaluates, and refines trajectory proposals. Current generation-evaluation frameworks isolate trajectory generation from quality assessment, preventing iterative refinement essential for planning, while reinforcement learning methods collapse multi-dimensional preferences into scalar rewards, obscuring critical trade-offs and yielding scalarization this http URL overcome these issues, we present EvaDrive, a novel multi-objective reinforcement learning framework that establishes genuine closed-loop co-evolution between trajectory generation and evaluation via adversarial optimization. EvaDrive frames trajectory planning as a multi-round adversarial game. In this game, a hierarchical generator continuously proposes candidate paths by combining autoregressive intent modeling for temporal causality with diffusion-based refinement for spatial flexibility. These proposals are then rigorously assessed by a trainable multi-objective critic that explicitly preserves diverse preference structures without collapsing them into a single scalarization this http URL adversarial interplay, guided by a Pareto frontier selection mechanism, enables iterative multi-round refinement, effectively escaping local optima while preserving trajectory this http URL experiments on NAVSIM and Bench2Drive benchmarks demonstrate SOTA performance, achieving 94.9 PDMS on NAVSIM v1 (surpassing DiffusionDrive by 6.8, DriveSuprim by 5.0, and TrajHF by 0.9) and 64.96 Driving Score on Bench2Drive. EvaDrive generates diverse driving styles via dynamic weighting without external preference data, introducing a closed-loop adversarial framework for human-like iterative decision-making, offering a novel scalarization-free trajectory optimization approach.</li>
<li><strong>摘要：</strong>自主驾驶在实现人类迭代决策方面面临重大挑战，该决策不断产生，评估和完善轨迹建议。当前的一代评估框架将轨迹产生从质量评估中分离出来，防止迭代改进对计划必不可少的，同时增强学习方法将多维偏好折叠成标量奖励，从而掩盖了关键的折衷，并掩盖了标量的标准化此HTTP URL克服了这些问题，我们将建立了疏远的跨度学习框架，并建立了一种疏远的跨度学习，并建立了新型的跨度学习框架，以实现疏远，并建立了新的跨度学习框架 - 通过对抗优化评估。 Evadrive框架轨迹计划是一种多轮对抗游戏。在此游戏中，分层发电机通过将自回归的意图建模与时间因果关系与基于扩散的空间灵活性相结合，不断提出候选路径。 These proposals are then rigorously assessed by a trainable multi-objective critic that explicitly preserves diverse preference structures without collapsing them into a single scalarization this http URL adversarial interplay, guided by a Pareto frontier selection mechanism, enables iterative multi-round refinement, effectively escaping local optima while preserving trajectory this http URL experiments on NAVSIM and Bench2Drive基准测试表现出SOTA性能，在NAVSIM V1上达到94.9 PDM（超过6.8的扩散驱动器，驱动器驱动器乘以5.0，而TRAJHF则达到0.9），在Banch2Drive上驾驶得分为64.96。 Evadrive通过动态加权产生多种驾驶风格，而无需外部偏好数据，引入了一个闭环对抗框架，用于人类样的迭代决策，提供了一种新颖的无标量轨迹优化方法。</li>
</ul>

<h3>Title: An Unsupervised Deep XAI Framework for Localization of Concurrent Replay Attacks in Nuclear Reactor Signals</h3>
<ul>
<li><strong>Authors: </strong>Konstantinos Vasili, Zachery T. Dahm, William Richards, Stylianos Chatzidakis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09162">https://arxiv.org/abs/2508.09162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09162">https://arxiv.org/pdf/2508.09162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09162]] An Unsupervised Deep XAI Framework for Localization of Concurrent Replay Attacks in Nuclear Reactor Signals(https://arxiv.org/abs/2508.09162)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Next generation advanced nuclear reactors are expected to be smaller both in size and power output, relying extensively on fully digital instrumentation and control systems. These reactors will generate a large flow of information in the form of multivariate time series data, conveying simultaneously various non linear cyber physical, process, control, sensor, and operational states. Ensuring data integrity against deception attacks is becoming increasingly important for networked communication and a requirement for safe and reliable operation. Current efforts to address replay attacks, almost universally focus on watermarking or supervised anomaly detection approaches without further identifying and characterizing the root cause of the anomaly. In addition, these approaches rely mostly on synthetic data with uncorrelated Gaussian process and measurement noise and full state feedback or are limited to univariate signals, signal stationarity, linear quadratic regulators, or other linear-time invariant state-space which may fail to capture any unmodeled system dynamics. In the realm of regulated nuclear cyber-physical systems, additional work is needed on characterization of replay attacks and explainability of predictions using real data. Here, we propose an unsupervised explainable AI framework based on a combination of autoencoder and customized windowSHAP algorithm to fully characterize real-time replay attacks, i.e., detection, source identification, timing and type, of increasing complexity during a dynamic time evolving reactor process. The proposed XAI framework was benchmarked on several real world datasets from Purdue's nuclear reactor PUR-1 with up to six signals concurrently being replayed. In all cases, the XAI framework was able to detect and identify the source and number of signals being replayed and the duration of the falsification with 95 percent or better accuracy.</li>
<li><strong>摘要：</strong>预计下一代高级核反应堆的尺寸和功率输出都将较小，并广泛依赖于完全数字仪器和控制系统。这些反应堆将以多元时间序列数据的形式产生大量信息，同时传达各种非线性网络物理，过程，控制，传感器和操作状态。确保针对欺骗攻击的数据完整性对于网络通信和安全可靠操作的要求变得越来越重要。当前的努力解决重播攻击，几乎普遍地关注水印或监督异常检测方法，而无需进一步识别和表征异常的根本原因。此外，这些方法主要依赖于具有不相关的高斯过程和测量噪声和全州反馈的合成数据，或仅限于单变量信号，信号平稳性，线性二次调节器或其他线性不变性状态空间，可能无法捕获任何未模拟的系统动力学。在受调节的核网络物理系统的领域中，需要进行额外的工作，以表征重播攻击和使用真实数据的预测性。在这里，我们提出了一个无监督的可解释的AI框架，基于自动编码器和自定义的窗框算法的组合，以充分表征实时重播攻击，即在动态时间演变反应器过程中，在动态时间进化过程中的检测，源识别，时机和类型，增加了复杂性的增加。拟议的XAI框架是在普渡大学核反应堆PUR-1的几个现实世界数据集上进行了基准测试的，最多六个信号同时被重播。在所有情况下，XAI框架都能够检测和确定被重播的信号和信号数量以及95％或更高准确性的伪造持续时间。</li>
</ul>

<h3>Title: Generating Feasible and Diverse Synthetic Populations Using Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Min Tang, Peng Lu, Qing Feng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09164">https://arxiv.org/abs/2508.09164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09164">https://arxiv.org/pdf/2508.09164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09164]] Generating Feasible and Diverse Synthetic Populations Using Diffusion Models(https://arxiv.org/abs/2508.09164)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Population synthesis is a critical task that involves generating synthetic yet realistic representations of populations. It is a fundamental problem in agent-based modeling (ABM), which has become the standard to analyze intelligent transportation systems. The synthetic population serves as the primary input for ABM transportation simulation, with traveling agents represented by population members. However, when the number of attributes describing agents becomes large, survey data often cannot densely support the joint distribution of the attributes in the population due to the curse of dimensionality. This sparsity makes it difficult to accurately model and produce the population. Interestingly, deep generative models trained from available sample data can potentially synthesize possible attribute combinations that present in the actual population but do not exist in the sample data(called sampling zeros). Nevertheless, this comes at the cost of falsely generating the infeasible attribute combinations that do not exist in the population (called structural zeros). In this study, a novel diffusion model-based population synthesis method is proposed to estimate the underlying joint distribution of a population. This approach enables the recovery of numerous missing sampling zeros while keeping the generated structural zeros minimal. Our method is compared with other recently proposed approaches such as Variational Autoencoders (VAE) and Generative Adversarial Network (GAN) approaches, which have shown success in high dimensional tabular population synthesis. We assess the performance of the synthesized outputs using a range of metrics, including marginal distribution similarity, feasibility, and diversity. The results demonstrate that our proposed method outperforms previous approaches in achieving a better balance between the feasibility and diversity of the synthesized population.</li>
<li><strong>摘要：</strong>人口综合是一项关键任务，涉及产生人口的综合而现实的表示。这是基于代理的建模（ABM）的基本问题，它已成为分析智能运输系统的标准。合成人群是ABM运输模拟的主要输入，由人口成员代表的旅行者。但是，当描述药物的属性数量变大时，由于维度的诅咒，调查数据通常不能密切支持人口中属性的联合分布。这种稀疏性使得难以准确建模并产生人口。有趣的是，经过可用样本数据训练的深层生成模型可能会合成实际种群中存在但不存在样本数据中不存在的可能属性组合（称为采样零）。然而，这是以虚假产生人群中不存在的不可行的属性组合（称为结构零）的代价。在这项研究中，提出了一种基于扩散模型的新型种群合成方法来估计人口的潜在联合分布。这种方法可以使许多缺失的采样零在保持生成的结构零最小的同时恢复。将我们的方法与其他最近提出的方法进行了比较，例如变异自动编码器（VAE）和生成对抗网络（GAN）方法，这些方法已在高维表格种群合成中表现出成功。我们使用一系列指标评估合成输出的性能，包括边际分布相似性，可行性和多样性。结果表明，我们提出的方法在合成人群的可行性和多样性之间取得更好的平衡，优于以前的方法。</li>
</ul>

<h3>Title: SVGen: Interpretable Vector Graphics Generation with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Feiyu Wang, Zhiyuan Zhao, Yuandong Liu, Da Zhang, Junyu Gao, Hao Sun, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09168">https://arxiv.org/abs/2508.09168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09168">https://arxiv.org/pdf/2508.09168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09168]] SVGen: Interpretable Vector Graphics Generation with Large Language Models(https://arxiv.org/abs/2508.09168)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Scalable Vector Graphics (SVG) is widely used in front-end development and UI/UX design due to its scalability, editability, and rendering efficiency. However, turning creative ideas into precise vector graphics remains a time-consuming challenge. To address this, we introduce SVG-1M, a large-scale dataset of high-quality SVGs paired with natural language descriptions. Through advanced data augmentation and annotation, we create well-aligned Text to SVG training pairs, including a subset with Chain of Thought annotations for enhanced semantic guidance. Based on this dataset, we propose SVGen, an end-to-end model that generates SVG code from natural language inputs. Our approach ensures semantic accuracy and structural completeness, supported by curriculum learning and reinforcement learning optimization. Experiments show that SVGen outperforms general large models and traditional rendering methods in both effectiveness and efficiency. Code, model, and dataset are available on GitHub.</li>
<li><strong>摘要：</strong>可扩展的向量图形（SVG）由于其可伸缩性，可编辑性和渲染效率而广泛用于前端开发和UI/UX设计。但是，将创意变成精确的矢量图形仍然是一个耗时的挑战。为了解决这个问题，我们介绍了SVG-1M，这是一个与自然语言描述配对的高质量SVG的大规模数据集。通过高级数据扩展和注释，我们为SVG培训对创建了良好的文本，其中包括带有一系列思想注释的子集，以增强语义指导。基于此数据集，我们提出了SVGEN，SVGEN是一种端到端模型，该模型从自然语言输入中生成SVG代码。我们的方法确保了课程学习和强化学习优化支持的语义准确性和结构完整性。实验表明，SVGEN在有效性和效率方面都优于一般大型模型和传统渲染方法。代码，模型和数据集可在GitHub上找到。</li>
</ul>

<h3>Title: Multimodal RAG Enhanced Visual Description</h3>
<ul>
<li><strong>Authors: </strong>Amit Kumar Jaiswal, Haiming Liu, Ingo Frommholz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09170">https://arxiv.org/abs/2508.09170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09170">https://arxiv.org/pdf/2508.09170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09170]] Multimodal RAG Enhanced Visual Description(https://arxiv.org/abs/2508.09170)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Textual descriptions for multimodal inputs entail recurrent refinement of queries to produce relevant output images. Despite efforts to address challenges such as scaling model size and data volume, the cost associated with pre-training and fine-tuning remains substantial. However, pre-trained large multimodal models (LMMs) encounter a modality gap, characterised by a misalignment between textual and visual representations within a common embedding space. Although fine-tuning can potentially mitigate this gap, it is typically expensive and impractical due to the requirement for extensive domain-driven data. To overcome this challenge, we propose a lightweight training-free approach utilising Retrieval-Augmented Generation (RAG) to extend across the modality using a linear mapping, which can be computed efficiently. During inference, this mapping is applied to images embedded by an LMM enabling retrieval of closest textual descriptions from the training set. These textual descriptions, in conjunction with an instruction, cater as an input prompt for the language model to generate new textual descriptions. In addition, we introduce an iterative technique for distilling the mapping by generating synthetic descriptions via the language model facilitating optimisation for standard utilised image description measures. Experimental results on two benchmark multimodal datasets demonstrate significant improvements.</li>
<li><strong>摘要：</strong>多模式输入的文本描述需要对查询的复发进行复发，以产生相关的输出图像。尽管努力应对诸如缩放模型规模和数据量之类的挑战，但与预训练和微调相关的成本仍然很大。但是，预训练的大型多模型模型（LMMS）遇到了一种模态差距，其特征在于通用嵌入空间内文本和视觉表示之间的错位。尽管微调可能会减轻这一差距，但由于需要广泛的域驱动数据，因此通常昂贵且不切实际。为了克服这一挑战，我们提出了一种使用检索功能的生成（RAG）的轻量级训练方法，使用线性映射可以在模态上扩展，可以有效地计算出。在推断期间，该映射应用于通过LMM嵌入的图像，从而可以从训练集中检索最接近的文本描述。这些文本描述与指令结合使用，作为输入提示，以提示语言模型生成新的文本描述。此外，我们引入了一种迭代技术，用于通过通过语言模型生成合成描述来提炼映射，从而促进标准使用的图像描述措施的优化。两个基准多模式数据集的实验结果显示出显着改善。</li>
</ul>

<h3>Title: Synthetic Data Generation for Emotional Depth Faces: Optimizing Conditional DCGANs via Genetic Algorithms in the Latent Space and Stabilizing Training with Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Seyed Muhammad Hossein Mousavi, S. Younes Mirinezhad</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09188">https://arxiv.org/abs/2508.09188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09188">https://arxiv.org/pdf/2508.09188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09188]] Synthetic Data Generation for Emotional Depth Faces: Optimizing Conditional DCGANs via Genetic Algorithms in the Latent Space and Stabilizing Training with Knowledge Distillation(https://arxiv.org/abs/2508.09188)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Affective computing faces a major challenge: the lack of high-quality, diverse depth facial datasets for recognizing subtle emotional expressions. We propose a framework for synthetic depth face generation using an optimized GAN with Knowledge Distillation (EMA teacher models) to stabilize training, improve quality, and prevent mode collapse. We also apply Genetic Algorithms to evolve GAN latent vectors based on image statistics, boosting diversity and visual quality for target emotions. The approach outperforms GAN, VAE, GMM, and KDE in both diversity and quality. For classification, we extract and concatenate LBP, HOG, Sobel edge, and intensity histogram features, achieving 94% and 96% accuracy with XGBoost. Evaluation using FID, IS, SSIM, and PSNR shows consistent improvement over state-of-the-art methods.</li>
<li><strong>摘要：</strong>情感计算面临的主要挑战：缺乏高质量，深度的面部数据集来识别微妙的情感表达。我们建议使用具有知识蒸馏（EMA教师模型）的优化GAN来稳定训练，提高质量并防止模式崩溃的框架来生成合成深度的面部。我们还将遗传算法基于图像统计数据，增强目标情绪的多样性和视觉质量来发展gan潜在向量。该方法在多样性和质量方面的表现都超过了GAN，VAE，GMM和KDE。为了进行分类，我们提取和加入LBP，猪，Sobel边缘和强度直方图特征，使用XGBoost达到94％和96％的精度。使用FID，IS，SSIM和PSNR的评估表现出对最新方法的一致改进。</li>
</ul>

<h3>Title: From Values to Tokens: An LLM-Driven Framework for Context-aware Time Series Forecasting via Symbolic Discretization</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Tao, Shilong Zhang, Mingyue Cheng, Daoyu Wang, Tingyue Pan, Bokai Pan, Changqing Zhang, Shijin Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09191">https://arxiv.org/abs/2508.09191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09191">https://arxiv.org/pdf/2508.09191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09191]] From Values to Tokens: An LLM-Driven Framework for Context-aware Time Series Forecasting via Symbolic Discretization(https://arxiv.org/abs/2508.09191)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Time series forecasting plays a vital role in supporting decision-making across a wide range of critical applications, including energy, healthcare, and finance. Despite recent advances, forecasting accuracy remains limited due to the challenge of integrating historical numerical sequences with contextual features, which often comprise unstructured textual data. To address this challenge, we propose TokenCast, an LLM-driven framework that leverages language-based symbolic representations as a unified intermediary for context-aware time series forecasting. Specifically, TokenCast employs a discrete tokenizer to transform continuous numerical sequences into temporal tokens, enabling structural alignment with language-based inputs. To bridge the semantic gap between modalities, both temporal and contextual tokens are embedded into a shared representation space via a pre-trained large language model (LLM), further optimized with autoregressive generative objectives. Building upon this unified semantic space, the aligned LLM is subsequently fine-tuned in a supervised manner to predict future temporal tokens, which are then decoded back into the original numerical space. Extensive experiments on diverse real-world datasets enriched with contextual features demonstrate the effectiveness and generalizability of TokenCast.</li>
<li><strong>摘要：</strong>时间序列预测在支持各种关键应用程序（包括能源，医疗保健和金融）的决策方面起着至关重要的作用。尽管有最近的进步，但由于将历史数字序列与上下文特征相结合的挑战，预测准确性仍然有限，这通常包含非结构化的文本数据。为了应对这一挑战，我们提出了Tokencast，这是一个由LLM驱动的框架，它利用基于语言的符号表示作为上下文感知时间序列预测的统一中介。具体而言，Tokencast采用离散令牌来将连续的数值序列转换为时间令牌，从而可以与基于语言的输入进行结构对齐。为了弥合模式之间的语义差距，时间和上下文令牌都通过预先训练的大语言模型（LLM）嵌入共享表示空间中，并通过自动回归的生成目标进一步优化。在这个统一的语义空间的基础上，对齐的LLM随后以监督的方式进行了微调，以预测未来的暂时令牌，然后将其解码回原始的数值空间。对具有上下文特征的多种现实世界数据集进行的广泛实验证明了Tokencast的有效性和普遍性。</li>
</ul>

<h3>Title: Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing</h3>
<ul>
<li><strong>Authors: </strong>Xu Wang, Chenkai Xu, Yijie Jin, Jiachun Jin, Hao Zhang, Zhijie Deng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09192">https://arxiv.org/abs/2508.09192</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09192">https://arxiv.org/pdf/2508.09192</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09192]] Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing(https://arxiv.org/abs/2508.09192)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to autoregressive (AR) LLMs for text generation, with the potential to decode multiple tokens in a single iteration. However, none of the existing open-source dLLMs have achieved superior inference speed over AR LLMs of similar size. This paper breaks this barrier based on a simple and effective strategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key capabilities: (1) block-wise autoregressive generation to enable KV cache utilization; (2) prediction of following tokens without requiring completion of prior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs are refurbished into an AR-diffusion hybrid paradigm for efficient inference. D2F can be implemented with an asymmetric distillation process based on pre-trained dLLMs. We further propose a pipelined parallel decoding algorithm, which enables a trade-off between efficiency and efficacy. Empirically, D2F dLLMs achieve more than $\mathbf{2.5\times}$ inference speed than LLaMA3 and Qwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the acceleration can be more than $\mathbf{50\times}$ while maintaining comparable output quality. The code is available at this https URL.</li>
<li><strong>摘要：</strong>扩散大语言模型（DLLM）已成为文本生成自回归（AR）LLM的有希望的替代方法，并有可能在单个迭代中解码多个令牌。但是，现有的开源DLLM都没有达到高于相似大小的AR LLM的较高推理速度。本文基于一个名为离散扩散强迫（D2F）的简单有效策略打破了这一障碍。 D2F将DLLMS具有两个关键功能：（1）构架自动回归生成以启用KV缓存利用率； （2）预测以下令牌，而无需完成块间平行解码的先验块。这样，将香草dllms翻新成AR-扩散杂种范式以有效推断。 D2F可以基于预先训练的DLLM的不对称蒸馏过程来实现。我们进一步提出了一种管道的平行解码算法，这可以在效率和功效之间进行权衡。从经验上讲，D2F DLLM在GSM8K上的$ \ Mathbf {2.5 \ times} $推理速度超过$ \ Mathbf {2.5 \ times} $推理速度。与Llada和Dream等香草DLLM相比，加速度可以超过$ \ mathbf {50 \ times} $，同时保持可比的输出质量。该代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Multi-Objective Instruction-Aware Representation Learning in Procedural Content Generation RL</h3>
<ul>
<li><strong>Authors: </strong>Sung-Hyun Kim, In-Chang Baek, Seo-Young Lee, Geum-Hwan Hwang, Kyung-Joong Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09193">https://arxiv.org/abs/2508.09193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09193">https://arxiv.org/pdf/2508.09193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09193]] Multi-Objective Instruction-Aware Representation Learning in Procedural Content Generation RL(https://arxiv.org/abs/2508.09193)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in generative modeling emphasize the importance of natural language as a highly expressive and accessible modality for controlling content generation. However, existing instructed reinforcement learning for procedural content generation (IPCGRL) method often struggle to leverage the expressive richness of textual input, especially under complex, multi-objective instructions, leading to limited controllability. To address this problem, we propose \textit{MIPCGRL}, a multi-objective representation learning method for instructed content generators, which incorporates sentence embeddings as conditions. MIPCGRL effectively trains a multi-objective embedding space by incorporating multi-label classification and multi-head regression networks. Experimental results show that the proposed method achieves up to a 13.8\% improvement in controllability with multi-objective instructions. The ability to process complex instructions enables more expressive and flexible content generation.</li>
<li><strong>摘要：</strong>生成建模的最新进展强调了自然语言作为控制内容生成的一种高度表达和易于访问的方式。但是，现有的指导性增强程序生成（IPCGRL）方法通常难以利用文本输入的表现力丰富，尤其是在复杂的多目标指令下，从而导致有限的可控性。为了解决此问题，我们建议\ textit {mipcgrl}，这是一种用于指导内容生成器的多目标表示方法，该方法将句子嵌入为条件。 MIPCGRL通过合并多标签分类和多头回归网络有效地训练多目标嵌入空间。实验结果表明，通过多目标指令，该提出的方法可在可控性提高13.8 \％。处理复杂指令的能力可以使更具表现力和灵活的内容生成。</li>
</ul>

<h3>Title: Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method</h3>
<ul>
<li><strong>Authors: </strong>Masoumeh Sharafi, Soufiane Belharbi, Houssem Ben Salem, Ali Etemad, Alessandro Lameiras Koerich, Marco Pedersoli, Simon Bacon, Eric Granger</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09202">https://arxiv.org/abs/2508.09202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09202">https://arxiv.org/pdf/2508.09202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09202]] Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method(https://arxiv.org/abs/2508.09202)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Facial expression recognition (FER) models are employed in many video-based affective computing applications, such as human-computer interaction and healthcare monitoring. However, deep FER models often struggle with subtle expressions and high inter-subject variability, limiting their performance in real-world applications. To improve their performance, source-free domain adaptation (SFDA) methods have been proposed to personalize a pretrained source model using only unlabeled target domain data, thereby avoiding data privacy, storage, and transmission constraints. This paper addresses a challenging scenario where source data is unavailable for adaptation, and only unlabeled target data consisting solely of neutral expressions is available. SFDA methods are not typically designed to adapt using target data from only a single class. Further, using models to generate facial images with non-neutral expressions can be unstable and computationally intensive. In this paper, personalized feature translation (PFT) is proposed for SFDA. Unlike current image translation methods for SFDA, our lightweight method operates in the latent space. We first pre-train the translator on the source domain data to transform the subject-specific style features from one source subject into another. Expression information is preserved by optimizing a combination of expression consistency and style-aware objectives. Then, the translator is adapted on neutral target data, without using source data or image synthesis. By translating in the latent space, PFT avoids the complexity and noise of face expression generation, producing discriminative embeddings optimized for classification. Using PFT eliminates the need for image synthesis, reduces computational overhead (using a lightweight translator), and only adapts part of the model, making the method efficient compared to image-based translation.</li>
<li><strong>摘要：</strong>面部表达识别（FER）模型用于许多基于视频的情感计算应用中，例如人类计算机的互动和医疗保健监测。但是，深FER模型通常会在微妙的表达和高主体间的变异性上挣扎，从而限制了它们在现实应用程序中的性能。为了提高其性能，已经提出了仅使用未标记的目标域数据来个性化验证源模型的无源域适应（SFDA）方法，从而避免了数据隐私，存储和传输约束。本文解决了一个充满挑战的方案，其中无法适应源数据，并且仅提供仅由中性表达式组成的未标记的目标数据。 SFDA方法通常不是为了使用仅一类的目标数据而设计的。此外，使用模型来生成具有非中性表达式的面部图像可能是不稳定的，并且在计算上很密集。在本文中，提出了针对SFDA的个性化功能翻译（PFT）。与SFDA的当前图像翻译方法不同，我们的轻量级方法在潜在空间中运行。我们首先在源域数据上预先培训翻译器，以将特定于主题的样式从一个源主题转换为另一个源。表达信息是通过优化表达一致性和样式感知目标的组合来保留的。然后，在不使用源数据或图像合成的情况下，将翻译器适用于中性目标数据。通过在潜在空间中翻译，PFT避免了面部表达产生的复杂性和噪声，从而产生了针对分类的歧视性嵌入。使用PFT消除了对图像合成的需求，减少了计算开销（使用轻型翻译器），并且仅适应了模型的一部分，与基于图像的翻译相比，该方法有效。</li>
</ul>

<h3>Title: The First Differentiable Transfer-Based Algorithm for Discrete MicroLED Repair</h3>
<ul>
<li><strong>Authors: </strong>Ning-Yuan Lue</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09206">https://arxiv.org/abs/2508.09206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09206">https://arxiv.org/pdf/2508.09206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09206]] The First Differentiable Transfer-Based Algorithm for Discrete MicroLED Repair(https://arxiv.org/abs/2508.09206)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Laser-enabled selective transfer, a key process in high-throughput microLED fabrication, requires computational models that can plan shift sequences to minimize motion of XY stages and adapt to varying optimization objectives across the substrate. We propose the first repair algorithm based on a differentiable transfer module designed to model discrete shifts of transfer platforms, while remaining trainable via gradient-based optimization. Compared to local proximity searching algorithms, our approach achieves superior repair performance and enables more flexible objective designs, such as minimizing the number of steps. Unlike reinforcement learning (RL)-based approaches, our method eliminates the need for handcrafted feature extractors and trains significantly faster, allowing scalability to large arrays. Experiments show a 50% reduction in transfer steps and sub-2-minute planning time on 2000x2000 arrays. This method provides a practical and adaptable solution for accelerating microLED repair in AR/VR and next-generation display fabrication.</li>
<li><strong>摘要：</strong>启用激光选择性转移是高通量微滤线制造中的一个关键过程，需要计算模型，该计算模型可以计划移动序列以最大程度地减少XY阶段的运动并适应整个基板上不同的优化目标。我们根据旨在建模传输平台的离散变化，同时可以通过基于梯度的优化来训练的第一个修复算法，以模拟转移平台的离散变化。与局部接近搜索算法相比，我们的方法可实现出色的维修性能，并实现更灵活的目标设计，例如最大程度地减少步骤数量。与加固学习（RL）的方法不同，我们的方法消除了对手工提取器和火车的需求，可以更快地使用较大的阵列。实验表明，在2000x2000阵列上的传输步骤和低2分钟的计划时间减少了50％。该方法提供了一种实用且适应性的解决方案，用于加速AR/VR和下一代展示制造中的微胶片修复。</li>
</ul>

<h3>Title: Towards Effective MLLM Jailbreaking Through Balanced On-Topicness and OOD-Intensity</h3>
<ul>
<li><strong>Authors: </strong>Zuoou Li, Weitong Zhang, Jingyuan Wang, Shuyuan Zhang, Wenjia Bai, Bernhard Kainz, Mengyun Qiao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09218">https://arxiv.org/abs/2508.09218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09218">https://arxiv.org/pdf/2508.09218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09218]] Towards Effective MLLM Jailbreaking Through Balanced On-Topicness and OOD-Intensity(https://arxiv.org/abs/2508.09218)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) are widely used in vision-language reasoning tasks. However, their vulnerability to adversarial prompts remains a serious concern, as safety mechanisms often fail to prevent the generation of harmful outputs. Although recent jailbreak strategies report high success rates, many responses classified as "successful" are actually benign, vague, or unrelated to the intended malicious goal. This mismatch suggests that current evaluation standards may overestimate the effectiveness of such attacks. To address this issue, we introduce a four-axis evaluation framework that considers input on-topicness, input out-of-distribution (OOD) intensity, output harmfulness, and output refusal rate. This framework identifies truly effective jailbreaks. In a substantial empirical study, we reveal a structural trade-off: highly on-topic prompts are frequently blocked by safety filters, whereas those that are too OOD often evade detection but fail to produce harmful content. However, prompts that balance relevance and novelty are more likely to evade filters and trigger dangerous output. Building on this insight, we develop a recursive rewriting strategy called Balanced Structural Decomposition (BSD). The approach restructures malicious prompts into semantically aligned sub-tasks, while introducing subtle OOD signals and visual cues that make the inputs harder to detect. BSD was tested across 13 commercial and open-source MLLMs, where it consistently led to higher attack success rates, more harmful outputs, and fewer refusals. Compared to previous methods, it improves success rates by $67\%$ and harmfulness by $21\%$, revealing a previously underappreciated weakness in current multimodal safety systems.</li>
<li><strong>摘要：</strong>多模式大语言模型（MLLM）广泛用于视觉推理任务中。但是，由于安全机制通常无法阻止产生有害产出，因此它们对对抗提示的脆弱性仍然是一个严重的关注。尽管最近的越狱策略报告了很高的成功率，但许多被归类为“成功”的回应实际上与预期的恶意目标是良性，模糊或无关的。这种不匹配表明当前的评估标准可能高估了此类攻击的有效性。为了解决这个问题，我们引入了一个四轴评估框架，该框架考虑了输入主题，输入分布（OOD）强度，输出有害和输出拒绝率。该框架确定了真正有效的越狱。在一项大量的实证研究中，我们揭示了结构上的权衡：高度的主题提示经常被安全过滤器阻止，而那些过于ood的提示经常逃避检测，但未能产生有害内容。但是，提示平衡相关性和新颖性更有可能逃避过滤器并触发危险的输出。在这种见识的基础上，我们制定了一种称为平衡结构分解（BSD）的递归重写策略。该方法将恶意提示重组成语义对齐子任务，同时引入了微妙的OOD信号和视觉提示，从而使输入更难检测到。在13个商业和开源MLLM中对BSD进行了测试，在该商业和开源MLLM中，它始终导致攻击成功率，更有害的产出和更少的拒绝。与以前的方法相比，它将成功率提高了67美元\％$，有害性提高了$ 21 \％$，这表明在当前的多模式安全系统中，以前的弱点不足。</li>
</ul>

<h3>Title: FineState-Bench: A Comprehensive Benchmark for Fine-Grained State Control in GUI Agents</h3>
<ul>
<li><strong>Authors: </strong>Fengxian Ji, Jingpu Yang, Zirui Song, Yuanxi Wang, Zhexuan Cui, Yuke Li, Qian Jiang, Miao Fang, Xiuying Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09241">https://arxiv.org/abs/2508.09241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09241">https://arxiv.org/pdf/2508.09241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09241]] FineState-Bench: A Comprehensive Benchmark for Fine-Grained State Control in GUI Agents(https://arxiv.org/abs/2508.09241)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of generative artificial intelligence technology, Graphical User Interface (GUI) agents have demonstrated tremendous potential for autonomously managing daily tasks through natural language instructions. However, current evaluation frameworks for GUI agents suffer from fundamental flaws: existing benchmarks overly focus on coarse-grained task completion while neglecting fine-grained control capabilities crucial for real-world applications. To address this, we introduce FineState-Bench, the first evaluation and diagnostic standard for fine-grained GUI proxy operations, designed to quantify fine-grained control. This multi-platform (desktop, Web, mobile) framework includes 2257 task benchmarks in four components and uses a four-phase indicator for comprehensive perception-to-control assessment. To analyze perception and positioning for refined operations, we developed the plug-and-play Visual Diagnostic Assistant (VDA), enabling the first quantitative decoupling analysis of these capabilities. Experimental results on our benchmark show that the most advanced models achieve only 32.8% fine-grained interaction accuracy. Using our VDA in controlled experiments, quantifying the impact of visual capabilities, we showed that ideal visual localization boosts Gemini-2.5-Flash's success rate by 14.9\%. Our diagnostic framework confirms for the first time that the primary bottleneck for current GUI proxies is basic visual positioning this http URL resources are fully open-source. github: this https URL huggingface: this https URL</li>
<li><strong>摘要：</strong>随着生成人工智能技术的快速发展，图形用户界面（GUI）代理具有通过自然语言指令自主管理日常任务的巨大潜力。但是，当前针对GUI代理的评估框架遭受了基本缺陷的困扰：现有的基准测试过度关注粗粒度的任务完成，同时忽略了对现实应用程序至关重要的细粒度控制能力。为了解决这个问题，我们介绍了Finestate Bench，这是细度GUI代理操作的第一个评估和诊断标准，旨在量化细粒度的控制。这个多平台（桌面，Web，移动）框架包括四个组件中的2257个任务基准，并使用四相指标进行全面的感知到控制评估。为了分析精致操作的感知和定位，我们开发了插件的视觉诊断助手（VDA），从而实现了这些功能的首次定量解耦分析。基准上的实验结果表明，最先进的模型仅达到32.8％的细粒相互作用精度。使用我们的VDA在受控的实验中，量化了视觉能力的影响，我们表明理想的视觉定位将Gemini-2.5-Flash的成功率提高了14.9 \％。我们的诊断框架首次证实了当前GUI代理的主要瓶颈是基本的视觉定位，该HTTP URL资源是完全开源的。 github：此https url huggingface：此https url</li>
</ul>

<h3>Title: Lung-DDPM+: Efficient Thoracic CT Image Synthesis using Diffusion Probabilistic Model</h3>
<ul>
<li><strong>Authors: </strong>Yifan Jiang, Ahmad Shariftabrizi, Venkata SK. Manem</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09327">https://arxiv.org/abs/2508.09327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09327">https://arxiv.org/pdf/2508.09327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09327]] Lung-DDPM+: Efficient Thoracic CT Image Synthesis using Diffusion Probabilistic Model(https://arxiv.org/abs/2508.09327)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative artificial intelligence (AI) has been playing an important role in various domains. Leveraging its high capability to generate high-fidelity and diverse synthetic data, generative AI is widely applied in diagnostic tasks, such as lung cancer diagnosis using computed tomography (CT). However, existing generative models for lung cancer diagnosis suffer from low efficiency and anatomical imprecision, which limit their clinical applicability. To address these drawbacks, we propose Lung-DDPM+, an improved version of our previous model, Lung-DDPM. This novel approach is a denoising diffusion probabilistic model (DDPM) guided by nodule semantic layouts and accelerated by a pulmonary DPM-solver, enabling the method to focus on lesion areas while achieving a better trade-off between sampling efficiency and quality. Evaluation results on the public LIDC-IDRI dataset suggest that the proposed method achieves 8$\times$ fewer FLOPs (floating point operations per second), 6.8$\times$ lower GPU memory consumption, and 14$\times$ faster sampling compared to Lung-DDPM. Moreover, it maintains comparable sample quality to both Lung-DDPM and other state-of-the-art (SOTA) generative models in two downstream segmentation tasks. We also conducted a Visual Turing Test by an experienced radiologist, showing the advanced quality and fidelity of synthetic samples generated by the proposed method. These experimental results demonstrate that Lung-DDPM+ can effectively generate high-quality thoracic CT images with lung nodules, highlighting its potential for broader applications, such as general tumor synthesis and lesion generation in medical imaging. The code and pretrained models are available at this https URL.</li>
<li><strong>摘要：</strong>生成人工智能（AI）一直在各个领域发挥重要作用。利用其高能力产生高保真性和多样化的合成数据，生成的AI被广泛应用于诊断任务中，例如使用计算机断层扫描（CT）的肺癌诊断。但是，现有的用于肺癌诊断的生成模型患有低效率和解剖不精确，这限制了它们的临床适用性。为了解决这些缺点，我们提出了肺ddpm+，这是我们以前型号的改进版本肺ddpm。这种新颖的方法是一种以结节语义布局为指导并通过肺部dpm溶剂加速的转化扩散概率模型（DDPM），使该方法能够专注于病变区域，同时在抽样效率和质量之间取得更好的折衷。对公共LIDC-IDRI数据集的评估结果表明，所提出的方法可实现8 $ \ times $ $ $较少的拖鞋（每秒浮点操作），6.8 $ \ times $降低GPU内存消耗，而14 $ \ tims $ \ times $ $ \ times $ $更快地采样了。此外，在两个下游分割任务中，它保持与肺-DDPM和其他最先进（SOTA）生成模型的可比样品质量。我们还通过经验丰富的放射科医生进行了视觉图灵测试，显示了该方法产生的合成样品的先进质量和保真度。这些实验结果表明，肺DDPM+可以有效地产生具有肺结节的高质量胸CT图像，从而突出了其对更广泛应用的潜力，例如一般肿瘤合成和医疗成像中的病变产生。该https URL可用代码和验证的模型。</li>
</ul>

<h3>Title: X-UniMotion: Animating Human Images with Expressive, Unified and Identity-Agnostic Motion Latents</h3>
<ul>
<li><strong>Authors: </strong>Guoxian Song, Hongyi Xu, Xiaochen Zhao, You Xie, Tianpei Gu, Zenan Li, Chenxu Zhang, Linjie Luo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09383">https://arxiv.org/abs/2508.09383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09383">https://arxiv.org/pdf/2508.09383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09383]] X-UniMotion: Animating Human Images with Expressive, Unified and Identity-Agnostic Motion Latents(https://arxiv.org/abs/2508.09383)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present X-UniMotion, a unified and expressive implicit latent representation for whole-body human motion, encompassing facial expressions, body poses, and hand gestures. Unlike prior motion transfer methods that rely on explicit skeletal poses and heuristic cross-identity adjustments, our approach encodes multi-granular motion directly from a single image into a compact set of four disentangled latent tokens -- one for facial expression, one for body pose, and one for each hand. These motion latents are both highly expressive and identity-agnostic, enabling high-fidelity, detailed cross-identity motion transfer across subjects with diverse identities, poses, and spatial configurations. To achieve this, we introduce a self-supervised, end-to-end framework that jointly learns the motion encoder and latent representation alongside a DiT-based video generative model, trained on large-scale, diverse human motion datasets. Motion--identity disentanglement is enforced via 2D spatial and color augmentations, as well as synthetic 3D renderings of cross-identity subject pairs under shared poses. Furthermore, we guide motion token learning with auxiliary decoders that promote fine-grained, semantically aligned, and depth-aware motion embeddings. Extensive experiments show that X-UniMotion outperforms state-of-the-art methods, producing highly expressive animations with superior motion fidelity and identity preservation.</li>
<li><strong>摘要：</strong>我们提出了X-Unimotion，这是全身人类运动的统一和表达的隐式潜在表示，包括面部表情，身体姿势和手势。与依赖于显式骨骼姿势和启发式跨认同调整的先前运动转移方法不同，我们的方法将多个晶体运动直接从单个图像中编码为一个紧凑的四个分离的潜在标记，一个用于面部表达，一种用于身体姿势，一个用于每只手。这些运动潜在的潜在的情绪高度表达和身份敏捷，可以跨具有不同身份，姿势和空间配置的受试者的高保真，详细的跨认同运动转移。为了实现这一目标，我们介绍了一个自制的，端到端的框架，该框架共同学习了运动编码器和潜在表示，以及基于DIT的视频生成模型，该模型接受了大规模的，多样化的人类运动数据集。运动 - 通过2D空间和颜色增强以及共享姿势下的跨认同受试者对的合成3D渲染来强制实现。此外，我们使用辅助解码器指导运动令牌学习，从而促进细粒度，语义对齐和深度感知的运动嵌入。广泛的实验表明，X- unimotion的表现优于最先进的方法，从而产生具有出色的运动保真度和身份保存的高度表达动画。</li>
</ul>

<h3>Title: Understanding Dementia Speech Alignment with Diffusion-Based Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Mansi, Anastasios Lepipas, Dominika Woszczyk, Yiying Guan, Soteris Demetriou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09385">https://arxiv.org/abs/2508.09385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09385">https://arxiv.org/pdf/2508.09385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09385]] Understanding Dementia Speech Alignment with Diffusion-Based Image Generation(https://arxiv.org/abs/2508.09385)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-image models generate highly realistic images based on natural language descriptions and millions of users use them to create and share images online. While it is expected that such models can align input text and generated image in the same latent space little has been done to understand whether this alignment is possible between pathological speech and generated images. In this work, we examine the ability of such models to align dementia-related speech information with the generated images and develop methods to explain this alignment. Surprisingly, we found that dementia detection is possible from generated images alone achieving 75% accuracy on the ADReSS dataset. We then leverage explainability methods to show which parts of the language contribute to the detection.</li>
<li><strong>摘要：</strong>文本到图像模型基于自然语言描述生成高度逼真的图像，数以百万计的用户使用它们来在线创建和共享图像。虽然可以预期，此类模型可以在同一潜在空间中对齐输入文本和生成图像，但很少有了解这种比对在病理语音和生成的图像之间是否可能进行对齐。在这项工作中，我们研究了此类模型将与痴呆症相关的语音信息与生成的图像保持一致的能力，并开发了解释这种比对的方法。令人惊讶的是，我们发现仅产生的图像就可以在Adress数据集上实现75％的精度，就可以进行痴呆检测。然后，我们利用解释性方法来显示该语言的哪些部分有助于检测。</li>
</ul>

<h3>Title: Distilling LLM Prior to Flow Model for Generalizable Agent's Imagination in Object Goal Navigation</h3>
<ul>
<li><strong>Authors: </strong>Badi Li, Ren-jie Lu, Yu Zhou, Jingke Meng, Wei-shi Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09423">https://arxiv.org/abs/2508.09423</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09423">https://arxiv.org/pdf/2508.09423</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09423]] Distilling LLM Prior to Flow Model for Generalizable Agent's Imagination in Object Goal Navigation(https://arxiv.org/abs/2508.09423)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The Object Goal Navigation (ObjectNav) task challenges agents to locate a specified object in an unseen environment by imagining unobserved regions of the scene. Prior approaches rely on deterministic and discriminative models to complete semantic maps, overlooking the inherent uncertainty in indoor layouts and limiting their ability to generalize to unseen environments. In this work, we propose GOAL, a generative flow-based framework that models the semantic distribution of indoor environments by bridging observed regions with LLM-enriched full-scene semantic maps. During training, spatial priors inferred from large language models (LLMs) are encoded as two-dimensional Gaussian fields and injected into target maps, distilling rich contextual knowledge into the flow model and enabling more generalizable completions. Extensive experiments demonstrate that GOAL achieves state-of-the-art performance on MP3D and Gibson, and shows strong generalization in transfer settings to HM3D. Codes and pretrained models are available at this https URL.</li>
<li><strong>摘要：</strong>对象目标导航（ObjectNAV）任务通过想象场景的未观察区域来挑战代理在看不见的环境中找到指定的对象。先前的方法依靠确定性和判别模型来完成语义图，忽略室内布局的固有不确定性，并限制其概括到看不见的环境的能力。在这项工作中，我们提出了一个基于生成流的框架，该目标是通过用LLM富含富含LLM的全景观语义图桥接的区域来对室内环境的语义分布进行建模。在训练过程中，从大语言模型（LLM）推断出的空间先验被编码为二维高斯字段，并将其注入目标图中，将丰富的上下文知识提炼成流程模型，并实现了更具概括性的完成。广泛的实验表明，目标在MP3D和Gibson上实现了最先进的表现，并在转移到HM3D的转移设置中表现出强烈的概括。该HTTPS URL可用代码和预估计的模型。</li>
</ul>

<h3>Title: MPT: Motion Prompt Tuning for Micro-Expression Recognition</h3>
<ul>
<li><strong>Authors: </strong>Jiateng Liu, Hengcan Shi, Feng Chen, Zhiwen Shao, Yaonan Wang, Jianfei Cai, Wenming Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09446">https://arxiv.org/abs/2508.09446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09446">https://arxiv.org/pdf/2508.09446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09446]] MPT: Motion Prompt Tuning for Micro-Expression Recognition(https://arxiv.org/abs/2508.09446)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Micro-expression recognition (MER) is crucial in the affective computing field due to its wide application in medical diagnosis, lie detection, and criminal investigation. Despite its significance, obtaining micro-expression (ME) annotations is challenging due to the expertise required from psychological professionals. Consequently, ME datasets often suffer from a scarcity of training samples, severely constraining the learning of MER models. While current large pre-training models (LMs) offer general and discriminative representations, their direct application to MER is hindered by an inability to capture transitory and subtle facial movements-essential elements for effective MER. This paper introduces Motion Prompt Tuning (MPT) as a novel approach to adapting LMs for MER, representing a pioneering method for subtle motion prompt tuning. Particularly, we introduce motion prompt generation, including motion magnification and Gaussian tokenization, to extract subtle motions as prompts for LMs. Additionally, a group adapter is carefully designed and inserted into the LM to enhance it in the target MER domain, facilitating a more nuanced distinction of ME representation. Furthermore, extensive experiments conducted on three widely used MER datasets demonstrate that our proposed MPT consistently surpasses state-of-the-art approaches and verifies its effectiveness.</li>
<li><strong>摘要：</strong>微表达识别（MER）在情感计算领域至关重要，因为它在医学诊断，谎言检测和刑事调查中的广泛应用。尽管它具有重要意义，但由于心理专业人员需要的专业知识，获得微表达（ME）注释是具有挑战性的。因此，我的数据集经常遭受少数训练样本的稀缺性，从而严重限制了MER模型的学习。尽管当前的大型训练模型（LMS）提供了一般和歧视性表示，但它们在MER上的直接应用是由于无法捕获暂时性和微妙的面部运动 - 必需元素以实现有效MER的限制。本文引入了运动及时调整（MPT），作为将LMS适应MER的新方法，代表了一种用于微妙运动及时调整的开拓方法。特别是，我们引入运动及时产生，包括运动放大倍率和高斯令牌化，以提取微妙的运动作为LMS的提示。此外，小组适配器经过精心设计并插入LM，以增强其在目标MER域中，从而促进了ME代表的更加细微的区别。此外，在三个广泛使用的MER数据集上进行的广泛实验表明，我们提议的MPT始终超过最新方法并验证其有效性。</li>
</ul>

<h3>Title: RASR: Retrieval-Augmented Super Resolution for Practical Reference-based Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Yan, Shuning Xu, Xiangyu Chen, Dell Zhang, Jie Tang, Gangshan Wu, Jie Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09449">https://arxiv.org/abs/2508.09449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09449">https://arxiv.org/pdf/2508.09449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09449]] RASR: Retrieval-Augmented Super Resolution for Practical Reference-based Image Restoration(https://arxiv.org/abs/2508.09449)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Reference-based Super Resolution (RefSR) improves upon Single Image Super Resolution (SISR) by leveraging high-quality reference images to enhance texture fidelity and visual realism. However, a critical limitation of existing RefSR approaches is their reliance on manually curated target-reference image pairs, which severely constrains their practicality in real-world scenarios. To overcome this, we introduce Retrieval-Augmented Super Resolution (RASR), a new and practical RefSR paradigm that automatically retrieves semantically relevant high-resolution images from a reference database given only a low-quality input. This enables scalable and flexible RefSR in realistic use cases, such as enhancing mobile photos taken in environments like zoos or museums, where category-specific reference data (e.g., animals, artworks) can be readily collected or pre-curated. To facilitate research in this direction, we construct RASR-Flickr30, the first benchmark dataset designed for RASR. Unlike prior datasets with fixed target-reference pairs, RASR-Flickr30 provides per-category reference databases to support open-world retrieval. We further propose RASRNet, a strong baseline that combines a semantic reference retriever with a diffusion-based RefSR generator. It retrieves relevant references based on semantic similarity and employs a diffusion-based generator enhanced with semantic conditioning. Experiments on RASR-Flickr30 demonstrate that RASRNet consistently improves over SISR baselines, achieving +0.38 dB PSNR and -0.0131 LPIPS, while generating more realistic textures. These findings highlight retrieval augmentation as a promising direction to bridge the gap between academic RefSR research and real-world applicability.</li>
<li><strong>摘要：</strong>基于参考的超级分辨率（REFSR）通过利用高质量的参考图像来增强纹理保真度和视觉现实主义来改善单图像超级分辨率（SISR）。但是，现有REFSR方法的关键局限性是它们依赖手动策划的目标参考图像对，这严重限制了它们在现实情况下的实用性。为了克服这一点，我们介绍了一种新的且实用的RefSR范式，我们引入了检索式超级分辨率（RASR），该范式自动从参考数据库中自动检索与仅给出低质量输入的参考数据库。这可以在现实的用例中实现可扩展且灵活的Refsr，例如在动物园或博物馆等环境中拍摄的移动照片，在这些环境中，特定类别的参考数据（例如，动物，艺术品）可以很容易地收集或预先策划。为了促进这个方向的研究，我们构建了RASR-Flickr30，这是第一个为RASR设计的基准数据集。与先前具有固定目标参考对的数据集不同，RASR-FLICKR30提供了每个类别参考数据库，以支持开放世界检索。我们进一步提出了RASRNET，这是一种强大的基线，将语义参考检索器与基于扩散的REFSR发生器相结合。它根据语义相似性检索相关的参考，并采用基于扩散的发电机增强了语义调节。 RASR -FLICKR30上的实验表明，Rasrnet始终改善SISR基准，实现+0.38 dB PSNR和-0.0131 LPIPS，同时产生更现实的纹理。这些发现突出了检索的增强，这是弥合学术参考研究与现实世界中适用性之间差距的有希望的方向。</li>
</ul>

<h3>Title: A Unified Contrastive-Generative Framework for Time Series Classification</h3>
<ul>
<li><strong>Authors: </strong>Ziyu Liu, Azadeh Alavi, Minyi Li, Xiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09451">https://arxiv.org/abs/2508.09451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09451">https://arxiv.org/pdf/2508.09451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09451]] A Unified Contrastive-Generative Framework for Time Series Classification(https://arxiv.org/abs/2508.09451)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) for multivariate time series mainly includes two paradigms: contrastive methods that excel at instance discrimination and generative approaches that model data distributions. While effective individually, their complementary potential remains unexplored. We propose a Contrastive Generative Time series framework (CoGenT), the first framework to unify these paradigms through joint contrastive-generative optimization. CoGenT addresses fundamental limitations of both approaches: it overcomes contrastive learning's sensitivity to high intra-class similarity in temporal data while reducing generative methods' dependence on large datasets. We evaluate CoGenT on six diverse time series datasets. The results show consistent improvements, with up to 59.2% and 14.27% F1 gains over standalone SimCLR and MAE, respectively. Our analysis reveals that the hybrid objective preserves discriminative power while acquiring generative robustness. These findings establish a foundation for hybrid SSL in temporal domains. We will release the code shortly.</li>
<li><strong>摘要：</strong>多元时间序列的自学学习（SSL）主要包括两个范式：在实例歧视和生成方法上表现出对数据分布的生成方法的符合性方法。虽然单独有效，但它们的互补潜力仍未开发。我们提出了一个对比的生成时间序列框架（Cogent），这是通过联合对比基因相培养优化统一这些范式的第一个框架。有说服力解决两种方法的基本局限性：它克服了对比度学习对时间数据中高层相似性的敏感性，同时降低了生成方法对大型数据集的依赖性。我们评估了六个不同的时间序列数据集的cogent。结果显示出一致的改进，比独立SIMCLR和MAE分别高达59.2％和14.27％的F1增益。我们的分析表明，混合目标在获得生成鲁棒性的同时保持了歧视能力。这些发现在时间域中为混合SSL建立了基础。我们将尽快发布代码。</li>
</ul>

<h3>Title: Gen-AFFECT: Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy</h3>
<ul>
<li><strong>Authors: </strong>Hao Yu, Rupayan Mallick, Margrit Betke, Sarah Adel Bargal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09461">https://arxiv.org/abs/2508.09461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09461">https://arxiv.org/pdf/2508.09461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09461]] Gen-AFFECT: Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy(https://arxiv.org/abs/2508.09461)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Different forms of customized 2D avatars are widely used in gaming applications, virtual communication, education, and content creation. However, existing approaches often fail to capture fine-grained facial expressions and struggle to preserve identity across different expressions. We propose GEN-AFFECT, a novel framework for personalized avatar generation that generates expressive and identity-consistent avatars with a diverse set of facial expressions. Our framework proposes conditioning a multimodal diffusion transformer on an extracted identity-expression representation. This enables identity preservation and representation of a wide range of facial expressions. GEN-AFFECT additionally employs consistent attention at inference for information sharing across the set of generated expressions, enabling the generation process to maintain identity consistency over the array of generated fine-grained expressions. GEN-AFFECT demonstrates superior performance compared to previous state-of-the-art methods on the basis of the accuracy of the generated expressions, the preservation of the identity and the consistency of the target identity across an array of fine-grained facial expressions.</li>
<li><strong>摘要：</strong>不同形式的自定义2D化身广泛用于游戏应用程序，虚拟通信，教育和内容创建。但是，现有的方法通常无法捕获细粒度的面部表情，并难以在不同表达式上保留身份。我们提出了Gen-Actect，这是一个针对个性化的化身生成的新型框架，它产生具有各种面部表情的表现力和符合认同的化身。我们的框架提出了在提取的身份表达表示上调节多模式扩散变压器。这可以保存身份，并表示各种面部表情。 Gen-Actect另外，在推断一组生成的表达式中对信息共享的推断还具有一致的关注，从而使生成过程能够维持对一系列生成的细粒表达式的身份一致性。与先前的最新方法相比，Gen-Actect表现出了卓越的性能，基于生成的表达式的准确性，在一系列细粒的面部表达式中的身份的保留和目标身份的一致性。</li>
</ul>

<h3>Title: Learn to Explore: Meta NAS via Bayesian Optimization Guided Graph Generation</h3>
<ul>
<li><strong>Authors: </strong>Zijun Sun, Yanning Shen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09467">https://arxiv.org/abs/2508.09467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09467">https://arxiv.org/pdf/2508.09467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09467]] Learn to Explore: Meta NAS via Bayesian Optimization Guided Graph Generation(https://arxiv.org/abs/2508.09467)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Neural Architecture Search (NAS) automates the design of high-performing neural networks but typically targets a single predefined task, thereby restricting its real-world applicability. To address this, Meta Neural Architecture Search (Meta-NAS) has emerged as a promising paradigm that leverages prior knowledge across tasks to enable rapid adaptation to new ones. Nevertheless, existing Meta-NAS methods often struggle with poor generalization, limited search spaces, or high computational costs. In this paper, we propose a novel Meta-NAS framework, GraB-NAS. Specifically, GraB-NAS first models neural architectures as graphs, and then a hybrid search strategy is developed to find and generate new graphs that lead to promising neural architectures. The search strategy combines global architecture search via Bayesian Optimization in the search space with local exploration for novel neural networks via gradient ascent in the latent space. Such a hybrid search strategy allows GraB-NAS to discover task-aware architectures with strong performance, even beyond the predefined search space. Extensive experiments demonstrate that GraB-NAS outperforms state-of-the-art Meta-NAS baselines, achieving better generalization and search effectiveness.</li>
<li><strong>摘要：</strong>神经体系结构搜索（NAS）自动化高性能神经网络的设计，但通常针对单个预定义的任务，从而限制其现实世界的适用性。为了解决这个问题，元神经体系结构搜索（Meta-NAS）已成为一种有希望的范式，它利用跨任务的先验知识，以便快速适应新的知识。然而，现有的元NAS方法通常会在概括，搜索空间有限或高计算成本方面遇到困难。在本文中，我们提出了一个新颖的Meta-NAS框架，Grab-Nas。具体而言，Grab-NAS首先建模神经体系结构作为图形，然后开发了混合搜索策略，以查找和生成带来有希望的神经体系结构的新图形。搜索策略通过搜索空间中的贝叶斯优化结合了全球体系结构搜索，以及通过潜在空间中的梯度上升对新型神经网络的本地探索。这种混合搜索策略使Grab-Nas可以发现具有强大性能的任务感知体系结构，甚至超出了预定义的搜索空间。广泛的实验表明，Grab-NAS优于最先进的Meta-NAS基线，实现了更好的概括和搜索效果。</li>
</ul>

<h3>Title: DeepFeatIoT: Unifying Deep Learned, Randomized, and LLM Features for Enhanced IoT Time Series Sensor Data Classification in Smart Industries</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Sakib Khan Inan, Kewen Liao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09468">https://arxiv.org/abs/2508.09468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09468">https://arxiv.org/pdf/2508.09468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09468]] DeepFeatIoT: Unifying Deep Learned, Randomized, and LLM Features for Enhanced IoT Time Series Sensor Data Classification in Smart Industries(https://arxiv.org/abs/2508.09468)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Internet of Things (IoT) sensors are ubiquitous technologies deployed across smart cities, industrial sites, and healthcare systems. They continuously generate time series data that enable advanced analytics and automation in industries. However, challenges such as the loss or ambiguity of sensor metadata, heterogeneity in data sources, varying sampling frequencies, inconsistent units of measurement, and irregular timestamps make raw IoT time series data difficult to interpret, undermining the effectiveness of smart systems. To address these challenges, we propose a novel deep learning model, DeepFeatIoT, which integrates learned local and global features with non-learned randomized convolutional kernel-based features and features from large language models (LLMs). This straightforward yet unique fusion of diverse learned and non-learned features significantly enhances IoT time series sensor data classification, even in scenarios with limited labeled data. Our model's effectiveness is demonstrated through its consistent and generalized performance across multiple real-world IoT sensor datasets from diverse critical application domains, outperforming state-of-the-art benchmark models. These results highlight DeepFeatIoT's potential to drive significant advancements in IoT analytics and support the development of next-generation smart systems.</li>
<li><strong>摘要：</strong>物联网（IoT）传感器是在智能城市，工业站点和医疗保健系统中部署的无处不在的技术。他们不断生成时间序列数据，以实现行业的高级分析和自动化。然而，诸如传感器元数据的丢失或歧义，数据源的异质性，不同的采样频率，测量单位不一致以及不规则时间戳使原始IoT时间序列数据难以解释，破坏智能系统的有效性，诸如传感器元数据的丢失或模棱两可的挑战。为了应对这些挑战，我们提出了一种新颖的深度学习模型DeepFeatiot，该模型将学习的本地和全球特征与非学习的随机卷积内核的特征和基于大型语言模型（LLMS）的特征相结合。即使在具有有限的标记数据的情况下，这种直接而独特的融合也可以显着增强IoT时间序列传感器数据分类。我们的模型的有效性通过来自不同关键应用程序域的多个现实IOT传感器数据集的一致和广义性能来证明，表现优于最先进的基准模型。这些结果凸显了DeepFeatiot在物联网分析方面取得重大进步并支持下一代智能系统的开发的潜力。</li>
</ul>

<h3>Title: Leveraging Failed Samples: A Few-Shot and Training-Free Framework for Generalized Deepfake Detection</h3>
<ul>
<li><strong>Authors: </strong>Shibo Yao, Renshuai Tao, Xiaolong Zheng, Chao Liang, Chunjie Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09475">https://arxiv.org/abs/2508.09475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09475">https://arxiv.org/pdf/2508.09475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09475]] Leveraging Failed Samples: A Few-Shot and Training-Free Framework for Generalized Deepfake Detection(https://arxiv.org/abs/2508.09475)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent deepfake detection studies often treat unseen sample detection as a ``zero-shot" task, training on images generated by known models but generalizing to unknown ones. A key real-world challenge arises when a model performs poorly on unknown samples, yet these samples remain available for analysis. This highlights that it should be approached as a ``few-shot" task, where effectively utilizing a small number of samples can lead to significant improvement. Unlike typical few-shot tasks focused on semantic understanding, deepfake detection prioritizes image realism, which closely mirrors real-world distributions. In this work, we propose the Few-shot Training-free Network (FTNet) for real-world few-shot deepfake detection. Simple yet effective, FTNet differs from traditional methods that rely on large-scale known data for training. Instead, FTNet uses only one fake samplefrom an evaluation set, mimicking the scenario where new samples emerge in the real world and can be gathered for use, without any training or parameter updates. During evaluation, each test sample is compared to the known fake and real samples, and it is classified based on the category of the nearest sample. We conduct a comprehensive analysis of AI-generated images from 29 different generative models and achieve a new SoTA performance, with an average improvement of 8.7\% compared to existing methods. This work introduces a fresh perspective on real-world deepfake detection: when the model struggles to generalize on a few-shot sample, leveraging the failed samples leads to better performance.</li>
<li><strong>摘要：</strong>最近的深层检测研究通常将未见的样本检测视为``零射击''的任务，对已知模型产生的图像的培训，但要概括为未知的图像。当模型在未知的未知数上表现较差时，就会出现一个关键的现实世界挑战，但是这些样本仍然可用于分析。该示例应以``大量''的范围来进行分析，从而有效地将其用于较小的示例，以效率为少数的范围，以实现较小的任务，该示例有效地构成了一定的数量，即用于一定效果。与典型的几个射击任务不同，侧重于语义理解，DeepFake检测优先考虑图像现实主义，这密切反映了现实世界的分布。在这项工作中，我们提出了用于现实世界中的少量训练网络（FTNET），几乎没有训练。简单但有效的FTNET与依赖大规模的已知数据进行培训的传统方法不同。取而代之的是，FTNET仅使用一个从评估集中使用的假示例，模仿了现实世界中新样本的情况，并且可以收集使用，而无需任何培训或参数更新。在评估过程中，将每个测试样本与已知的假和真实样品进行比较，并根据最近样本的类别进行分类。我们对来自29种不同生成模型的AI生成的图像进行了全面分析，并实现了新的SOTA性能，与现有方法相比，平均提高了8.7 \％。这项工作介绍了现实世界中的深层检测的新观点：当模型努力概括一些弹药样本时，利用失败的样本会导致更好的性能。</li>
</ul>

<h3>Title: From Large Angles to Consistent Faces: Identity-Preserving Video Generation via Mixture of Facial Experts</h3>
<ul>
<li><strong>Authors: </strong>Yuji Wang, Moran Li, Xiaobin Hu, Ran Yi, Jiangning Zhang, Chengming Xu, Weijian Cao, Yabiao Wang, Chengjie Wang, Lizhuang Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09476">https://arxiv.org/abs/2508.09476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09476">https://arxiv.org/pdf/2508.09476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09476]] From Large Angles to Consistent Faces: Identity-Preserving Video Generation via Mixture of Facial Experts(https://arxiv.org/abs/2508.09476)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Current video generation models struggle with identity preservation under large facial angles, primarily facing two challenges: the difficulty in exploring an effective mechanism to integrate identity features into DiT structure, and the lack of targeted coverage of large facial angles in existing open-source video datasets. To address these, we present two key innovations. First, we introduce a Mixture of Facial Experts (MoFE) that dynamically combines complementary cues from three specialized experts, each designed to capture distinct but mutually reinforcing aspects of facial attributes. The identity expert captures cross-pose identity-sensitive features, the semantic expert extracts high-level visual semantxics, and the detail expert preserves pixel-level features (e.g., skin texture, color gradients). Furthermore, to mitigate dataset limitations, we have tailored a data processing pipeline centered on two key aspects: Face Constraints and Identity Consistency. Face Constraints ensure facial angle diversity and a high proportion of facial regions, while Identity Consistency preserves coherent person-specific features across temporal sequences, collectively addressing the scarcity of large facial angles and identity-stable training data in existing datasets. Leveraging this pipeline, we have curated and refined a Large Face Angles (LFA) Dataset from existing open-source human video datasets, comprising 460K video clips with annotated facial angles. Experimental results on the LFA benchmark demonstrate that our method, empowered by the LFA dataset, significantly outperforms prior SOTA methods in face similarity, face FID, and CLIP semantic alignment. The code and dataset will be made publicly available at this https URL.</li>
<li><strong>摘要：</strong>当前的视频生成模型在大面角度下与身份保存斗争，主要面临两个挑战：难以探索将身份特征整合到DIT结构中的有效机制，并且缺乏在现有的开放源视频数据集中对大面部角度的有针对性的覆盖范围。为了解决这些问题，我们提出了两个关键的创新。首先，我们介绍了面部专家（MOFE）的混合物，该面部专家（MOFE）动态结合了三个专家的互补线索，每个专家旨在捕获面部属性的不同但相互加强的方面。身份专家捕获了对身份敏感的特征，语义专家提取物高水平的视觉半敏感，细节专家保留了像素级特征（例如，皮肤纹理，颜色梯度）。此外，为了减轻数据集限制，我们为以两个关键方面为中心的数据处理管道量身定制了一个数据处理管道：面部限制和身份一致性。面部约束确保面部角度多样性和大量面部区域，而身份一致性则保留了跨时间序列的特定于人的特定特征，从而共同解决了现有数据集中大的面部角度和身份稳定的培训数据的稀缺性。利用这条管道，我们从现有的开源人类视频数据集中策划并完善了一个大面角（LFA）数据集，其中包括带有带注释的面部角度的460K视频剪辑。 LFA基准测试的实验结果表明，我们的方法在LFA数据集中授权，在面部相似性，FACE，FID和剪辑语义对齐中的先前SOTA方法显着优于先前的SOTA方法。该代码和数据集将在此HTTPS URL上公开可用。</li>
</ul>

<h3>Title: CLIP-Flow: A Universal Discriminator for AI-Generated Images Inspired by Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Zhipeng Yuan, Kai Wang, Weize Quan, Dong-Ming Yan, Tieru Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09477">https://arxiv.org/abs/2508.09477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09477">https://arxiv.org/pdf/2508.09477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09477]] CLIP-Flow: A Universal Discriminator for AI-Generated Images Inspired by Anomaly Detection(https://arxiv.org/abs/2508.09477)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of AI generative models, the visual quality of AI-generated images (AIIs) has become increasingly close to natural images, which inevitably raises security concerns. Most AII detectors often employ the conventional image classification pipeline with natural images and AIIs (generated by a generative model), which can result in limited detection performance for AIIs from unseen generative models. To solve this, we proposed a universal AI-generated image detector from the perspective of anomaly detection. Our discriminator does not need to access any AIIs and learn a generalizable representation with unsupervised learning. Specifically, we use the pre-trained CLIP encoder as the feature extractor and design a normalizing flow-like unsupervised model. Instead of AIIs, proxy images, e.g., obtained by applying a spectral modification operation on natural images, are used for training. Our models are trained by minimizing the likelihood of proxy images, optionally combined with maximizing the likelihood of natural images. Extensive experiments demonstrate the effectiveness of our method on AIIs produced by various image generators.</li>
<li><strong>摘要：</strong>随着AI生成模型的快速发展，AI生成的图像（AIIS）的视觉质量变得越来越接近自然图像，这不可避免地引起了安全问题。大多数AII检测器通常使用自然图像和AII（由生成模型生成）采用常规图像分类管道，这可能会导致来自看不见的生成模型的AII有限的检测性能。为了解决这个问题，我们从异常检测的角度提出了一个通用AI生成的图像检测器。我们的歧视者不需要访问任何AII，也不需要通过无监督的学习来学习可推广的代表。具体而言，我们将预训练的夹子编码器用作特征提取器，并设计一种标准化流动的无监督模型。代替AII，代理图像，例如，通过在自然图像上应用光谱修改操作获得的代理图像用于训练。我们的模型是通过最大程度地减少代理图像的可能性来训练的，可选地结合了最大化自然图像的可能性。广泛的实验证明了我们方法对各种图像发生器产生的AII的有效性。</li>
</ul>

<h3>Title: SARE: Semantic-Aware Reconstruction Error for Generalizable Diffusion-Generated Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Ju Yeon Kang, Jaehong Park, Semin Kim, Ji Won Yoon, Nam Soo Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09487">https://arxiv.org/abs/2508.09487</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09487">https://arxiv.org/pdf/2508.09487</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09487]] SARE: Semantic-Aware Reconstruction Error for Generalizable Diffusion-Generated Image Detection(https://arxiv.org/abs/2508.09487)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recently, diffusion-generated image detection has gained increasing attention, as the rapid advancement of diffusion models has raised serious concerns about their potential misuse. While existing detection methods have achieved promising results, their performance often degrades significantly when facing fake images from unseen, out-of-distribution (OOD) generative models, since they primarily rely on model-specific artifacts. To address this limitation, we explore a fundamental property commonly observed in fake images. Motivated by the observation that fake images tend to exhibit higher similarity to their captions than real images, we propose a novel representation, namely Semantic-Aware Reconstruction Error (SARE), that measures the semantic difference between an image and its caption-guided reconstruction. The hypothesis behind SARE is that real images, whose captions often fail to fully capture their complex visual content, may undergo noticeable semantic shifts during the caption-guided reconstruction process. In contrast, fake images, which closely align with their captions, show minimal semantic changes. By quantifying these semantic shifts, SARE can be utilized as a discriminative feature for robust detection across diverse generative models. We empirically demonstrate that the proposed method exhibits strong generalization, outperforming existing baselines on benchmarks including GenImage and CommunityForensics.</li>
<li><strong>摘要：</strong>最近，随着扩散模型的快速发展引起了人们对其潜在滥用的严重关注，扩散产生的图像检测引起了人们的关注。尽管现有的检测方法已经取得了令人鼓舞的结果，但在面对从未看到的伪造图像（OOD）生成模型中时，它们的性能通常会大大降低，因为它们主要依赖于模型特定的伪影。为了解决这一限制，我们探索了在假图像中通常观察到的基本属性。通过观察到，假图像倾向于与其标题显示出比真实图像更高的相似性，我们提出了一种新颖的表示，即语义意识的重建误差（SARE），该误差（SARE）衡量了图像及其字幕引导的重建之间的语义差异。萨尔背后的假设是，实际图像的字幕通常无法完全捕获其复杂的视觉内容，在字幕引导的重建过程中可能会发生明显的语义变化。相比之下，伪造的图像与标题紧密相符，显示出最小的语义变化。通过量化这些语义转移，可以将刺激用作跨不同生成模型的稳健检测的歧视特征。我们从经验上证明，所提出的方法表现出强烈的概括，在包括Genimage和Community Formensics在内的基准上的现有基准优于现有基准。</li>
</ul>

<h3>Title: Enhancing Memory Recall in LLMs with Gauss-Tin: A Hybrid Instructional and Gaussian Replay Approach</h3>
<ul>
<li><strong>Authors: </strong>Iing Muttakhiroh, Thomas Fevens</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09510">https://arxiv.org/abs/2508.09510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09510">https://arxiv.org/pdf/2508.09510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09510]] Enhancing Memory Recall in LLMs with Gauss-Tin: A Hybrid Instructional and Gaussian Replay Approach(https://arxiv.org/abs/2508.09510)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite the significant advancements in Large Language Models (LLMs), catastrophic forgetting remains a substantial challenge, where models lose previously acquired knowledge upon learning new information. Continual learning (CL) strategies have emerged as a potential solution to this problem, with replay-based techniques demonstrating superior performance in preserving learned knowledge. In this context, we introduce Gauss-Tin, a novel approach that integrates the replay strategy with a Gaussian mixture model to enhance the quality of sample selection during training, supplemented by instructional guidance to facilitate the generation of past learning. This method aims to improve LLMs' retention capabilities by strategically reinforcing important past learnings while accommodating new information. Our experimental results indicate a promising 6\% improvement in retention metrics over traditional methods, suggesting that Gauss-Tin is an effective strategy for mitigating catastrophic forgetting in LLMs. This study underscores the potential of hybrid models in enhancing the robustness and adaptability of LLMs in dynamic learning environments.</li>
<li><strong>摘要：</strong>尽管大语言模型（LLM）取得了重大进步，但灾难性遗忘仍然是一个重大挑战，在学习新信息时，模型失去了以前获得的知识。持续学习（CL）策略已经成为解决此问题的潜在解决方案，基于重播的技术证明了在保留学习知识方面的卓越表现。在这种情况下，我们介绍了一种新颖的方法，该方法将重播策略与高斯混合模型集成在一起，以增强培训期间样本选择的质量，并补充了教学指导，以促进过去学习的产生。该方法旨在通过策略性地加强过去的知识，同时安装新信息，从而提高LLMS的保留能力。我们的实验结果表明，与传统方法相比，保留指标的有希望的6 \％提高，这表明高斯 - 锡是减轻LLMS灾难性遗忘的有效策略。这项研究强调了混合模型在增强动态学习环境中LLM的鲁棒性和适应性方面的潜力。</li>
</ul>

<h3>Title: Generation of Indian Sign Language Letters, Numbers, and Words</h3>
<ul>
<li><strong>Authors: </strong>Ajeet Kumar Yadav, Nishant Kumar, Rathna G N</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09522">https://arxiv.org/abs/2508.09522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09522">https://arxiv.org/pdf/2508.09522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09522]] Generation of Indian Sign Language Letters, Numbers, and Words(https://arxiv.org/abs/2508.09522)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Sign language, which contains hand movements, facial expressions and bodily gestures, is a significant medium for communicating with hard-of-hearing people. A well-trained sign language community communicates easily, but those who don't know sign language face significant challenges. Recognition and generation are basic communication methods between hearing and hard-of-hearing individuals. Despite progress in recognition, sign language generation still needs to be explored. The Progressive Growing of Generative Adversarial Network (ProGAN) excels at producing high-quality images, while the Self-Attention Generative Adversarial Network (SAGAN) generates feature-rich images at medium resolutions. Balancing resolution and detail is crucial for sign language image generation. We are developing a Generative Adversarial Network (GAN) variant that combines both models to generate feature-rich, high-resolution, and class-conditional sign language images. Our modified Attention-based model generates high-quality images of Indian Sign Language letters, numbers, and words, outperforming the traditional ProGAN in Inception Score (IS) and Fréchet Inception Distance (FID), with improvements of 3.2 and 30.12, respectively. Additionally, we are publishing a large dataset incorporating high-quality images of Indian Sign Language alphabets, numbers, and 129 words.</li>
<li><strong>摘要：</strong>手语包含手动运动，面部表情和身体手势，是与听力障碍人士交流的重要媒介。训练有素的手语社区很容易沟通，但是那些不知道手语的人面临着重大挑战。识别和产生是听力和听力障碍个人之间的基本沟通方法。尽管取得了认可的进展，但仍需要探索手语的产生。生成对抗网络（Progan）的逐步生长在产生高质量的图像方面擅长，而自我发挥的生成对抗网络（Sagan）在中等分辨率下生成功能丰富的图像。平衡分辨率和细节对于手语图像产生至关重要。我们正在开发一个生成的对抗网络（GAN）变体，该变体结合了两个模型，以生成功能丰富，高分辨率和班级条件的手语图像。我们经过修改的基于注意力的模型产生了印度手语字母，数字和单词的高质量图像，在Inception评分（IS）和FréchetInception Intevion距离（FID）中的表现分别优于传统的Progan，分别提高了3.2和30.12。此外，我们正在发布一个大型数据集，其中包含印度手语字母，数字和129个单词的高质量图像。</li>
</ul>

<h3>Title: Learning Spatial Decay for Vision Transformers</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Mao, Zhen Qin, Jinxing Zhou, Bin Fan, Jing Zhang, Yiran Zhong, Yuchao Dai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09525">https://arxiv.org/abs/2508.09525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09525">https://arxiv.org/pdf/2508.09525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09525]] Learning Spatial Decay for Vision Transformers(https://arxiv.org/abs/2508.09525)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision Transformers (ViTs) have revolutionized computer vision, yet their self-attention mechanism lacks explicit spatial inductive biases, leading to suboptimal performance on spatially-structured tasks. Existing approaches introduce data-independent spatial decay based on fixed distance metrics, applying uniform attention weighting regardless of image content and limiting adaptability to diverse visual scenarios. Inspired by recent advances in large language models where content-aware gating mechanisms (e.g., GLA, HGRN2, FOX) significantly outperform static alternatives, we present the first successful adaptation of data-dependent spatial decay to 2D vision transformers. We introduce \textbf{Spatial Decay Transformer (SDT)}, featuring a novel Context-Aware Gating (CAG) mechanism that generates dynamic, data-dependent decay for patch interactions. Our approach learns to modulate spatial attention based on both content relevance and spatial proximity. We address the fundamental challenge of 1D-to-2D adaptation through a unified spatial-content fusion framework that integrates manhattan distance-based spatial priors with learned content representations. Extensive experiments on ImageNet-1K classification and generation tasks demonstrate consistent improvements over strong baselines. Our work establishes data-dependent spatial decay as a new paradigm for enhancing spatial attention in vision transformers.</li>
<li><strong>摘要：</strong>视觉变压器（VIT）彻底改变了计算机视觉，但是他们的自我发挥作用机制缺乏明确的空间归纳偏见，从而导致在空间结构的任务上表现出色。现有方法介绍了基于固定距离指标的数据独立的空间衰减，无论图像内容如何，都应用统一的注意力加权，并将适应性限制为各种视觉场景。受到大语模型的最新进展的启发，内容感知的门控机制（例如GLA，HGRN2，FOX）的表现明显超过了静态替代方案，我们提出了第一个成功的数据依赖性空间衰变对2D Vision Transformers的成功适应。我们介绍了\ textbf {空间衰减变压器（SDT）}，其中具有一种新颖的上下文感知门（CAG）机制，该机制生成动态，数据依赖于贴片交互的衰减。我们的方法学会根据内容相关性和空间接近性来调节空间注意力。我们通过统一的空间融合框架来应对1D到2D适应的基本挑战，该框架将基于曼哈顿的距离空间先验与学习的内容表示。关于Imagenet-1K分类和生成任务的广泛实验表明，对强基础的进步一致。我们的工作将数据依赖的空间衰减确立为一种新的范式，以增强视觉变压器中的空间注意力。</li>
</ul>

<h3>Title: GoViG: Goal-Conditioned Visual Navigation Instruction Generation</h3>
<ul>
<li><strong>Authors: </strong>Fengyi Wu, Yifei Dong, Zhi-Qi Cheng, Yilong Dai, Guangyu Chen, Hang Wang, Qi Dai, Alexander G. Hauptmann</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09547">https://arxiv.org/abs/2508.09547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09547">https://arxiv.org/pdf/2508.09547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09547]] GoViG: Goal-Conditioned Visual Navigation Instruction Generation(https://arxiv.org/abs/2508.09547)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce Goal-Conditioned Visual Navigation Instruction Generation (GoViG), a new task that aims to autonomously generate precise and contextually coherent navigation instructions solely from egocentric visual observations of initial and goal states. Unlike conventional approaches that rely on structured inputs such as semantic annotations or environmental maps, GoViG exclusively leverages raw egocentric visual data, substantially improving its adaptability to unseen and unstructured environments. Our method addresses this task by decomposing it into two interconnected subtasks: (1) visual forecasting, which predicts intermediate visual states bridging the initial and goal views; and (2) instruction generation, which synthesizes linguistically coherent instructions grounded in both observed and anticipated visuals. These subtasks are integrated within an autoregressive multimodal large language model trained with tailored objectives to ensure spatial accuracy and linguistic clarity. Furthermore, we introduce two complementary multimodal reasoning strategies, one-pass and interleaved reasoning, to mimic incremental human cognitive processes during navigation. To evaluate our method, we propose the R2R-Goal dataset, combining diverse synthetic and real-world trajectories. Empirical results demonstrate significant improvements over state-of-the-art methods, achieving superior BLEU-4 and CIDEr scores along with robust cross-domain generalization.</li>
<li><strong>摘要：</strong>我们介绍了目标条件的视觉导航指令生成（GOVIG），该任务旨在自主生成精确且上下文相干的导航指令，仅来自对初始和目标状态的自我中心的视觉观察。与依靠结构化输入（例如语义注释或环境图）等结构化输入的传统方法不同，Govig专门利用原始的自我为中心的视觉数据，从而实质上改善了其对看不见和非结构化环境的适应性。我们的方法通过将其分解为两个互连子任务来解决此任务：（1）视觉预测，该预测可以预测桥接初始和目标视图的中间视觉状态； （2）指导生成，它综合了基于观察和预期视觉效果的语言相干指令。这些子任务集成在自回归的多模式大型语言模型中，该模型训练有量身定制的目标，以确保空间的准确性和语言清晰度。此外，我们介绍了两种互补的多模式推理策略，即一通和交织的推理，以模仿导航期间的人类认知过程。为了评估我们的方法，我们提出了R2R-GAAL数据集，结合了多样的合成和现实世界轨迹。经验结果表明，对最先进的方法有了显着改善，从而达到了上级BLEU-4和苹果酒评分以及强大的跨域概括。</li>
</ul>

<h3>Title: Exploring the Equivalence of Closed-Set Generative and Real Data Augmentation in Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Haowen Wang, Guowei Zhang, Xiang Zhang, Zeyuan Chen, Haiyang Xu, Dou Hoon Kwark, Zhuowen Tu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09550">https://arxiv.org/abs/2508.09550</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09550">https://arxiv.org/pdf/2508.09550</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09550]] Exploring the Equivalence of Closed-Set Generative and Real Data Augmentation in Image Classification(https://arxiv.org/abs/2508.09550)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this paper, we address a key scientific problem in machine learning: Given a training set for an image classification task, can we train a generative model on this dataset to enhance the classification performance? (i.e., closed-set generative data augmentation). We start by exploring the distinctions and similarities between real images and closed-set synthetic images generated by advanced generative models. Through extensive experiments, we offer systematic insights into the effective use of closed-set synthetic data for augmentation. Notably, we empirically determine the equivalent scale of synthetic images needed for augmentation. In addition, we also show quantitative equivalence between the real data augmentation and open-set generative augmentation (generative models trained using data beyond the given training set). While it aligns with the common intuition that real images are generally preferred, our empirical formulation also offers a guideline to quantify the increased scale of synthetic data augmentation required to achieve comparable image classification performance. Our results on natural and medical image datasets further illustrate how this effect varies with the baseline training set size and the amount of synthetic data incorporated.</li>
<li><strong>摘要：</strong>在本文中，我们解决了机器学习中的关键科学问题：给定针对图像分类任务的培训集，我们可以在该数据集上培训生成模型以提高分类性能吗？ （即，封闭式生成数据增强）。首先，我们探索真实图像和由高级生成模型生成的封闭式合成图像之间的区别和相似性。通过广泛的实验，我们可以系统地见解封闭式合成数据进行增强的有效使用。值得注意的是，我们从经验上确定了增强所需的合成图像的等效规模。此外，我们还显示了实际数据增强和开放式生成增强（使用给定训练集以外的数据训练的生成模型）之间的定量等效性。尽管它与普遍的真实图像通常是首选的共同直觉一致，但我们的经验配方还提供了一个指南，可以量化增加合成数据扩展规模，以实现可比的图像分类性能。我们对自然和医疗图像数据集的结果进一步说明了这种效果如何随基线训练集的大小和合成数据的量而变化。</li>
</ul>

<h3>Title: Edge General Intelligence Through World Models and Agentic AI: Fundamentals, Solutions, and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Changyuan Zhao, Guangyuan Liu, Ruichen Zhang, Yinqiu Liu, Jiacheng Wang, Jiawen Kang, Dusit Niyato, Zan Li, Xuemin (Sherman)Shen, Zhu Han, Sumei Sun, Chau Yuen, Dong In Kim</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09561">https://arxiv.org/abs/2508.09561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09561">https://arxiv.org/pdf/2508.09561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09561]] Edge General Intelligence Through World Models and Agentic AI: Fundamentals, Solutions, and Challenges(https://arxiv.org/abs/2508.09561)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Edge General Intelligence (EGI) represents a transformative evolution of edge computing, where distributed agents possess the capability to perceive, reason, and act autonomously across diverse, dynamic environments. Central to this vision are world models, which act as proactive internal simulators that not only predict but also actively imagine future trajectories, reason under uncertainty, and plan multi-step actions with foresight. This proactive nature allows agents to anticipate potential outcomes and optimize decisions ahead of real-world interactions. While prior works in robotics and gaming have showcased the potential of world models, their integration into the wireless edge for EGI remains underexplored. This survey bridges this gap by offering a comprehensive analysis of how world models can empower agentic artificial intelligence (AI) systems at the edge. We first examine the architectural foundations of world models, including latent representation learning, dynamics modeling, and imagination-based planning. Building on these core capabilities, we illustrate their proactive applications across EGI scenarios such as vehicular networks, unmanned aerial vehicle (UAV) networks, the Internet of Things (IoT) systems, and network functions virtualization, thereby highlighting how they can enhance optimization under latency, energy, and privacy constraints. We then explore their synergy with foundation models and digital twins, positioning world models as the cognitive backbone of EGI. Finally, we highlight open challenges, such as safety guarantees, efficient training, and constrained deployment, and outline future research directions. This survey provides both a conceptual foundation and a practical roadmap for realizing the next generation of intelligent, autonomous edge systems.</li>
<li><strong>摘要：</strong>边缘通用智能（EGI）代表边缘计算的变革性演变，其中分布式药物具有在不同的，动态的环境中自主性自主行动的能力。这个愿景的核心是世界模型，它们充当主动的内部模拟器，不仅可以预测，而且还可以积极地想象未来的轨迹，不确定性下的理性以及具有远见卓识的多步骤行动。这种积极主动的本质使代理商可以预测潜在的结果并在现实世界相互作用之前优化决策。尽管机器人技术和游戏领域的先前作品展示了世界模型的潜力，但它们融入了EGI的无线边缘，但仍未得到充分展望。这项调查通过对世界模型如何授权代理人人工智能（AI）系统的全面分析来弥合这一差距。我们首先研究了世界模型的建筑基础，包括潜在表示学习，动态建模和基于想象力的计划。在这些核心功能的基础上，我们说明了它们在EGI场景中的积极应用程序，例如车辆网络，无人驾驶飞机（UAV）网络，物联网（IoT）系统和网络功能虚拟化，从而突出了它们如何在延迟，能源和隐私约束下增强优化。然后，我们与基础模型和数字双胞胎一起探索他们的协同作用，将世界模型定位为EGI的认知骨干。最后，我们重点介绍了公开挑战，例如安全保证，有效的培训和部署限制，并概述了未来的研究指示。这项调查既是实现下一代智能自主边缘系统的概念基础，也提供了实用的路线图。</li>
</ul>

<h3>Title: WEC-DG: Multi-Exposure Wavelet Correction Method Guided by Degradation Description</h3>
<ul>
<li><strong>Authors: </strong>Ming Zhao, Pingping Liu, Tongshun Zhang, Zhe Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09565">https://arxiv.org/abs/2508.09565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09565">https://arxiv.org/pdf/2508.09565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09565]] WEC-DG: Multi-Exposure Wavelet Correction Method Guided by Degradation Description(https://arxiv.org/abs/2508.09565)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Multi-exposure correction technology is essential for restoring images affected by insufficient or excessive lighting, enhancing the visual experience by improving brightness, contrast, and detail richness. However, current multi-exposure correction methods often encounter challenges in addressing intra-class variability caused by diverse lighting conditions, shooting environments, and weather factors, particularly when processing images captured at a single exposure level. To enhance the adaptability of these models under complex imaging conditions, this paper proposes a Wavelet-based Exposure Correction method with Degradation Guidance (WEC-DG). Specifically, we introduce a degradation descriptor within the Exposure Consistency Alignment Module (ECAM) at both ends of the processing pipeline to ensure exposure consistency and achieve final alignment. This mechanism effectively addresses miscorrected exposure anomalies caused by existing methods' failure to recognize 'blurred' exposure degradation. Additionally, we investigate the light-detail decoupling properties of the wavelet transform to design the Exposure Restoration and Detail Reconstruction Module (EDRM), which processes low-frequency information related to exposure enhancement before utilizing high-frequency information as a prior guide for reconstructing spatial domain details. This serial processing strategy guarantees precise light correction and enhances detail recovery. Extensive experiments conducted on multiple public datasets demonstrate that the proposed method outperforms existing algorithms, achieving significant performance improvements and validating its effectiveness and practical applicability.</li>
<li><strong>摘要：</strong>多曝光校正技术对于恢复受到不足或过度照明影响的图像至关重要，从而通过改善亮度，对比度和细节丰富度来增强视觉体验。但是，当前的多曝光校正方法在解决由各种照明条件，射击环境和天气因素引起的类内变异性方面经常遇到挑战，尤其是在处理单个暴露水平捕获的图像时。为了在复杂的成像条件下增强这些模型的适应性，本文提出了一种基于小波的曝光校正方法（WEC-DG）。具体而言，我们在处理管道两端的暴露一致性对齐模块（ECAM）中引入了降级描述符，以确保暴露一致性并实现最终对齐。这种机制有效地解决了由于现有方法“无法识别“模糊”暴露降解的情况，导致的暴露异常不好。此外，我们研究了小波变换的轻尾解耦属性，以设计曝光恢复和细节重建模块（EDRM），该模块（EDRM）处理与曝光增强有关的低频信息，然后再利用高频信息作为重建空间域细节的先前指南。这种串行处理策略可以确保精确的光校正并增强细节恢复。在多个公共数据集上进行的广泛实验表明，所提出的方法的表现优于现有算法，实现了重大的性能改进并验证其有效性和实际适用性。</li>
</ul>

<h3>Title: A Chain of Diagnosis Framework for Accurate and Explainable Radiology Report Generation</h3>
<ul>
<li><strong>Authors: </strong>Haibo Jin, Haoxuan Che, Sunan He, Hao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09566">https://arxiv.org/abs/2508.09566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09566">https://arxiv.org/pdf/2508.09566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09566]] A Chain of Diagnosis Framework for Accurate and Explainable Radiology Report Generation(https://arxiv.org/abs/2508.09566)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite the progress of radiology report generation (RRG), existing works face two challenges: 1) The performances in clinical efficacy are unsatisfactory, especially for lesion attributes description; 2) the generated text lacks explainability, making it difficult for radiologists to trust the results. To address the challenges, we focus on a trustworthy RRG model, which not only generates accurate descriptions of abnormalities, but also provides basis of its predictions. To this end, we propose a framework named chain of diagnosis (CoD), which maintains a chain of diagnostic process for clinically accurate and explainable RRG. It first generates question-answer (QA) pairs via diagnostic conversation to extract key findings, then prompts a large language model with QA diagnoses for accurate generation. To enhance explainability, a diagnosis grounding module is designed to match QA diagnoses and generated sentences, where the diagnoses act as a reference. Moreover, a lesion grounding module is designed to locate abnormalities in the image, further improving the working efficiency of radiologists. To facilitate label-efficient training, we propose an omni-supervised learning strategy with clinical consistency to leverage various types of annotations from different datasets. Our efforts lead to 1) an omni-labeled RRG dataset with QA pairs and lesion boxes; 2) a evaluation tool for assessing the accuracy of reports in describing lesion location and severity; 3) extensive experiments to demonstrate the effectiveness of CoD, where it outperforms both specialist and generalist models consistently on two RRG benchmarks and shows promising explainability by accurately grounding generated sentences to QA diagnoses and images.</li>
<li><strong>摘要：</strong>尽管放射学报告产生（RRG）的进展，但现有作品面临两个挑战：1）临床功效的表现不令人满意，尤其是对于病变属性描述； 2）生成的文本缺乏解释性，因此放射科医生很难相信结果。为了应对挑战，我们专注于值得信赖的RRG模型，该模型不仅产生了对异常的准确描述，而且还提供了其预测的基础。为此，我们提出了一个名为诊断链（COD）的框架，该框架维护了临床准确且可解释的RRG的一系列诊断过程。它首先通过诊断对话生成问答（QA）对以提取关键发现，然后提示具有QA诊断的大型语言模型以获得准确的生成。为了增强解释性，诊断接地模块旨在匹配QA诊断和生成的句子，其中诊断作为参考。此外，病变接地模块旨在定位图像中的异常，从而进一步提高了放射科医生的工作效率。为了促进标签有效的培训，我们提出了一种具有临床一致性的Omni监督学习策略，以利用来自不同数据集的各种注释。我们的努力导致1）带有QA对和病变盒的Omni标记的RRG数据集； 2）评估工具，用于评估报告在描述病变位置和严重程度的报告的准确性； 3）广泛的实验证明了COD的有效性，在两个RRG基准上，它始终超过专家和通才模型，并通过将生成的句子准确地接地到QA诊断和图像，并显示出令人鼓舞的解释性。</li>
</ul>

<h3>Title: Dual Recursive Feedback on Generation and Appearance Latents for Pose-Robust Text-to-Image Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Jiwon Kim, Pureum Kim, SeonHwa Kim, Soobin Park, Eunju Cha, Kyong Hwan Jin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09575">https://arxiv.org/abs/2508.09575</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09575">https://arxiv.org/pdf/2508.09575</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09575]] Dual Recursive Feedback on Generation and Appearance Latents for Pose-Robust Text-to-Image Diffusion(https://arxiv.org/abs/2508.09575)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advancements in controllable text-to-image (T2I) diffusion models, such as Ctrl-X and FreeControl, have demonstrated robust spatial and appearance control without requiring auxiliary module training. However, these models often struggle to accurately preserve spatial structures and fail to capture fine-grained conditions related to object poses and scene layouts. To address these challenges, we propose a training-free Dual Recursive Feedback (DRF) system that properly reflects control conditions in controllable T2I models. The proposed DRF consists of appearance feedback and generation feedback that recursively refines the intermediate latents to better reflect the given appearance information and the user's intent. This dual-update mechanism guides latent representations toward reliable manifolds, effectively integrating structural and appearance attributes. Our approach enables fine-grained generation even between class-invariant structure-appearance fusion, such as transferring human motion onto a tiger's form. Extensive experiments demonstrate the efficacy of our method in producing high-quality, semantically coherent, and structurally consistent image generations. Our source code is available at this https URL.</li>
<li><strong>摘要：</strong>CTRL-X和FreeControl等可控文本图像（T2I）扩散模型的最新进展已证明了强大的空间和外观控制，而无需辅助模块训练。但是，这些模型通常难以准确保留空间结构，并且无法捕获与物体姿势和场景布局相关的细颗粒条件。为了应对这些挑战，我们提出了一个无训练的双递归反馈（DRF）系统，该系统正确反映了可控T2I模型中的控制条件。所提出的DRF由外观反馈和一代反馈组成，这些反馈和发电反馈递归地完善了中级潜伏期，以更好地反映给定的外观信息和用户的意图。这种双层机制指导潜在表示可靠的流形，有效地整合了结构和外观属性。我们的方法甚至可以在类不变的结构 - 表现融合之间进行细粒的生成，例如将人类运动转移到老虎的形式上。广泛的实验证明了我们方法在产生高质量，语义相干和结构一致的图像世代的功效。我们的源代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Images Speak Louder Than Scores: Failure Mode Escape for Enhancing Generative Quality</h3>
<ul>
<li><strong>Authors: </strong>Jie Shao, Ke Zhu, Minghao Fu, Guo-hua Wang, Jianxin Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09598">https://arxiv.org/abs/2508.09598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09598">https://arxiv.org/pdf/2508.09598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09598]] Images Speak Louder Than Scores: Failure Mode Escape for Enhancing Generative Quality(https://arxiv.org/abs/2508.09598)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative, quality assessment</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved remarkable progress in class-to-image generation. However, we observe that despite impressive FID scores, state-of-the-art models often generate distorted or low-quality images, especially in certain classes. This gap arises because FID evaluates global distribution alignment, while ignoring the perceptual quality of individual samples. We further examine the role of CFG, a common technique used to enhance generation quality. While effective in improving metrics and suppressing outliers, CFG can introduce distribution shift and visual artifacts due to its misalignment with both training objectives and user expectations. In this work, we propose FaME, a training-free and inference-efficient method for improving perceptual quality. FaME uses an image quality assessment model to identify low-quality generations and stores their sampling trajectories. These failure modes are then used as negative guidance to steer future sampling away from poor-quality regions. Experiments on ImageNet demonstrate that FaME brings consistent improvements in visual quality without compromising FID. FaME also shows the potential to be extended to improve text-to-image generation.</li>
<li><strong>摘要：</strong>扩散模型在阶级到图像生成方面取得了显着进步。但是，我们观察到，尽管令人印象深刻的FID得分，但最先进的模型通常会产生扭曲或低质量的图像，尤其是在某些类别中。之所以出现此差距，是因为FID评估了全球分布对齐，同时忽略了单个样本的感知质量。我们进一步研究了CFG的作用，CFG是一种用于提高发电质量的常见技术。 CFG虽然有效地改善指标和抑制异常值，但由于其与培训目标和用户期望的不对准，CFG可以引入分销转移和视觉伪像。在这项工作中，我们提出了名望，这是一种提高感知质量的无培训和推理方法。名望使用图像质量评估模型来识别低品质的世代并存储其采样轨迹。然后，这些故障模式被用作消极的指导，以转向未来的采样，从质量较差的地区进行抽样。 ImageNet上的实验表明，名望在不损害FID的情况下会带来一致的视觉质量提高。名望还显示了扩展的潜力，以改善文本对图像的生成。</li>
</ul>

<h3>Title: MInDI-3D: Iterative Deep Learning in 3D for Sparse-view Cone Beam Computed Tomography</h3>
<ul>
<li><strong>Authors: </strong>Daniel Barco (1), Marc Stadelmann (1), Martin Oswald (1), Ivo Herzig (2), Lukas Lichtensteiger (2), Pascal Paysan (3), Igor Peterlik (3), Michal Walczak (3), Bjoern Menze (4), Frank-Peter Schilling (1) ((1) Centre for Artificial Intelligence (CAI), Zurich University of Applied Sciences (ZHAW), Winterthur, Switzerland, (2) Institute of Applied Mathematics and Physics (IAMP), Zurich University of Applied Sciences (ZHAW), Winterthur, Switzerland, (3) Varian Medical Systems Imaging Lab, Baden, Switzerland, (4) Biomedical Image Analysis and Machine Learning, University of Zurich, Zurich, Switzerland)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09616">https://arxiv.org/abs/2508.09616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09616">https://arxiv.org/pdf/2508.09616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09616]] MInDI-3D: Iterative Deep Learning in 3D for Sparse-view Cone Beam Computed Tomography(https://arxiv.org/abs/2508.09616)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present MInDI-3D (Medical Inversion by Direct Iteration in 3D), the first 3D conditional diffusion-based model for real-world sparse-view Cone Beam Computed Tomography (CBCT) artefact removal, aiming to reduce imaging radiation exposure. A key contribution is extending the "InDI" concept from 2D to a full 3D volumetric approach for medical images, implementing an iterative denoising process that refines the CBCT volume directly from sparse-view input. A further contribution is the generation of a large pseudo-CBCT dataset (16,182) from chest CT volumes of the CT-RATE public dataset to robustly train MInDI-3D. We performed a comprehensive evaluation, including quantitative metrics, scalability analysis, generalisation tests, and a clinical assessment by 11 clinicians. Our results show MInDI-3D's effectiveness, achieving a 12.96 (6.10) dB PSNR gain over uncorrected scans with only 50 projections on the CT-RATE pseudo-CBCT (independent real-world) test set and enabling an 8x reduction in imaging radiation exposure. We demonstrate its scalability by showing that performance improves with more training data. Importantly, MInDI-3D matches the performance of a 3D U-Net on real-world scans from 16 cancer patients across distortion and task-based metrics. It also generalises to new CBCT scanner geometries. Clinicians rated our model as sufficient for patient positioning across all anatomical sites and found it preserved lung tumour boundaries well.</li>
<li><strong>摘要：</strong>我们提出了Mindi-3D（在3D中进行直接迭代的医学反转），这是第一个基于3D有条件扩散的模型，用于实际稀疏视图锥束计算机断层扫描（CBCT）人工伪影，旨在减少成像辐射暴露。一个关键的贡献是将“ INDI”概念从2D扩展到用于医学图像的完整3D体积方法，从而实施了一个迭代的剥离过程，该过程直接从稀疏视图输入中完善了CBCT量。另一个贡献是从CT率公共数据集的胸部CT卷中生成了一个大型伪CBCT数据集（16,182），从而强有力地训练Mindi-3D。我们进行了全面的评估，包括定量指标，可伸缩性分析，概括测试和11位临床医生的临床评估。我们的结果表明，Mindi-3D的有效性，在未经校正的扫描中获得了12.96（6.10）DB PSNR的增益，仅在CT速率Pseudo-CBCT（独立的现实世界）测试集上进行了50个投影，并使成像辐射暴露的减少8倍。我们通过显示更多培训数据来证明绩效提高来证明其可扩展性。重要的是，Mindi-3D与在失真和基于任务的指标中对16名癌症患者的实际扫描中的3D U-NET的性能匹配。它也概括了新的CBCT扫描仪几何形状。临床医生将我们的模型评为足以在所有解剖部位的患者定位，并发现它很好地保留了肺部肿瘤边界。</li>
</ul>

<h3>Title: Semantic-aware DropSplat: Adaptive Pruning of Redundant Gaussians for 3D Aerial-View Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Xu Tang, Junan Jia, Yijing Wang, Jingjing Ma, Xiangrong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09626">https://arxiv.org/abs/2508.09626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09626">https://arxiv.org/pdf/2508.09626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09626]] Semantic-aware DropSplat: Adaptive Pruning of Redundant Gaussians for 3D Aerial-View Segmentation(https://arxiv.org/abs/2508.09626)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In the task of 3D Aerial-view Scene Semantic Segmentation (3D-AVS-SS), traditional methods struggle to address semantic ambiguity caused by scale variations and structural occlusions in aerial images. This limits their segmentation accuracy and consistency. To tackle these challenges, we propose a novel 3D-AVS-SS approach named SAD-Splat. Our method introduces a Gaussian point drop module, which integrates semantic confidence estimation with a learnable sparsity mechanism based on the Hard Concrete distribution. This module effectively eliminates redundant and semantically ambiguous Gaussian points, enhancing both segmentation performance and representation compactness. Furthermore, SAD-Splat incorporates a high-confidence pseudo-label generation pipeline. It leverages 2D foundation models to enhance supervision when ground-truth labels are limited, thereby further improving segmentation accuracy. To advance research in this domain, we introduce a challenging benchmark dataset: 3D Aerial Semantic (3D-AS), which encompasses diverse real-world aerial scenes with sparse annotations. Experimental results demonstrate that SAD-Splat achieves an excellent balance between segmentation accuracy and representation compactness. It offers an efficient and scalable solution for 3D aerial scene understanding.</li>
<li><strong>摘要：</strong>在3D航空观场景语义分割（3D-AVS-SS）的任务中，传统方法难以解决由尺度变化和空中图像中的结构遮挡引起的语义歧义。这限制了他们的细分准确性和一致性。为了应对这些挑战，我们提出了一种新颖的3D-AVS-SS方法，名为Sad-Splat。我们的方法引入了高斯点滴模块，该模块将语义置信度估计与基于硬具体分布的可学习的稀疏机制集成在一起。该模块有效地消除了冗余和语义上模棱两可的高斯点，从而增强了分割性能和表示紧凑。此外，SADSPLAT结合了高信心伪标签的生成管道。当地面真相标签受到限制时，它利用2D基础模型来增强监督，从而进一步提高细分精度。为了推进该领域的研究，我们介绍了一个具有挑战性的基准数据集：3D航空语义（3D-AS），该数据集涵盖了具有稀疏注释的各种现实世界空中场景。实验结果表明，SADSPLAT在分割准确性和表示紧凑度之间达到了良好的平衡。它提供了一个有效且可扩展的解决方案，可用于3D航空场景。</li>
</ul>

<h3>Title: Preacher: Paper-to-Video Agentic System</h3>
<ul>
<li><strong>Authors: </strong>Jingwei Liu, Ling Yang, Hao Luo, Fan Wang Hongyan Li, Mengdi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09632">https://arxiv.org/abs/2508.09632</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09632">https://arxiv.org/pdf/2508.09632</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09632]] Preacher: Paper-to-Video Agentic System(https://arxiv.org/abs/2508.09632)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The paper-to-video task converts a research paper into a structured video abstract, distilling key concepts, methods, and conclusions into an accessible, well-organized format. While state-of-the-art video generation models demonstrate potential, they are constrained by limited context windows, rigid video duration constraints, limited stylistic diversity, and an inability to represent domain-specific knowledge. To address these limitations, we introduce Preacher, the first paper-to-video agentic system. Preacher employs a top-down approach to decompose, summarize, and reformulate the paper, followed by bottom-up video generation, synthesizing diverse video segments into a coherent abstract. To align cross-modal representations, we define key scenes and introduce a Progressive Chain of Thought (P-CoT) for granular, iterative planning. Preacher successfully generates high-quality video abstracts across five research fields, demonstrating expertise beyond current video generation models. Code will be released at: this https URL</li>
<li><strong>摘要：</strong>纸与视频任务将研究论文转换为结构化视频摘要，提炼关键概念，方法和结论，以易于访问，组织良好的格式。尽管最先进的视频生成模型表现出了潜力，但它们受到有限的上下文窗口，严格的视频持续时间约束，有限的风格多样性以及无法代表特定领域的知识的限制。为了解决这些限制，我们介绍了传教士，这是第一个纸与视频代理系统。传教士采用自上而下的方法来分解，总结和重新制定论文，然后是自下而上的视频生成，将各种视频片段综合为连贯的摘要。为了使跨模式表示，我们定义了关键场景，并引入了渐进的思想链（P-COT），用于颗粒状，迭代计划。传教士成功地在五个研究领域生成了高质量的视频摘要，展示了当前视频生成模型以外的专业知识。代码将在：此HTTPS URL上发布：</li>
</ul>

<h3>Title: Multi-Sequence Parotid Gland Lesion Segmentation via Expert Text-Guided Segment Anything Model</h3>
<ul>
<li><strong>Authors: </strong>Zhongyuan Wu, Chuan-Xian Ren, Yu Wang, Xiaohua Ban, Jianning Xiao, Xiaohui Duan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09645">https://arxiv.org/abs/2508.09645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09645">https://arxiv.org/pdf/2508.09645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09645]] Multi-Sequence Parotid Gland Lesion Segmentation via Expert Text-Guided Segment Anything Model(https://arxiv.org/abs/2508.09645)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Parotid gland lesion segmentation is essential for the treatment of parotid gland diseases. However, due to the variable size and complex lesion boundaries, accurate parotid gland lesion segmentation remains challenging. Recently, the Segment Anything Model (SAM) fine-tuning has shown remarkable performance in the field of medical image segmentation. Nevertheless, SAM's interaction segmentation model relies heavily on precise lesion prompts (points, boxes, masks, etc.), which are very difficult to obtain in real-world applications. Besides, current medical image segmentation methods are automatically generated, ignoring the domain knowledge of medical experts when performing segmentation. To address these limitations, we propose the parotid gland segment anything model (PG-SAM), an expert diagnosis text-guided SAM incorporating expert domain knowledge for cross-sequence parotid gland lesion segmentation. Specifically, we first propose an expert diagnosis report guided prompt generation module that can automatically generate prompt information containing the prior domain knowledge to guide the subsequent lesion segmentation process. Then, we introduce a cross-sequence attention module, which integrates the complementary information of different modalities to enhance the segmentation effect. Finally, the multi-sequence image features and generated prompts are feed into the decoder to get segmentation result. Experimental results demonstrate that PG-SAM achieves state-of-the-art performance in parotid gland lesion segmentation across three independent clinical centers, validating its clinical applicability and the effectiveness of diagnostic text for enhancing image segmentation in real-world clinical settings.</li>
<li><strong>摘要：</strong>腮腺病变分割对于治疗腮腺疾病至关重要。但是，由于尺寸的变化和复杂的病变边界，准确的腮腺病变分段仍然具有挑战性。最近，该细分模型（SAM）微调在医学图像分割领域表现出色。然而，SAM的交互分割模型在很大程度上依赖于精确的病变提示（点，框，口罩等），这在现实世界中很难获得。此外，当前的医学图像分割方法是自动生成的，在执行分割时忽略了医学专家的领域知识。为了解决这些局限性，我们提出了腮腺段的任何模型（PG-SAM），这是专家诊断的文本引导的SAM，将专家领域知识纳入了跨序列腮腺病变细分。具体来说，我们首先提出了一份专家诊断报告指导了及时生成模块，该报告可以自动生成包含先前域知识的及时信息，以指导后续的病变细分过程。然后，我们引入了一个跨序列注意模块，该模块集成了不同模态的互补信息以增强分割效果。最后，多序列图像特征和生成的提示是将其输入解码器以获得分割结果。实验结果表明，PG-SAM在三个独立的临床中心跨三个独立的临床中心的腮腺病变分割中实现了最先进的性能，从而验证了其临床适用性和诊断文本的有效性，以增强现实世界中临床环境中的图像分割。</li>
</ul>

<h3>Title: NegFaceDiff: The Power of Negative Context in Identity-Conditioned Diffusion for Synthetic Face Generation</h3>
<ul>
<li><strong>Authors: </strong>Eduarda Caldeira, Naser Damer, Fadi Boutros</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09661">https://arxiv.org/abs/2508.09661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09661">https://arxiv.org/pdf/2508.09661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09661]] NegFaceDiff: The Power of Negative Context in Identity-Conditioned Diffusion for Synthetic Face Generation(https://arxiv.org/abs/2508.09661)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The use of synthetic data as an alternative to authentic datasets in face recognition (FR) development has gained significant attention, addressing privacy, ethical, and practical concerns associated with collecting and using authentic data. Recent state-of-the-art approaches have proposed identity-conditioned diffusion models to generate identity-consistent face images, facilitating their use in training FR models. However, these methods often lack explicit sampling mechanisms to enforce inter-class separability, leading to identity overlap in the generated data and, consequently, suboptimal FR performance. In this work, we introduce NegFaceDiff, a novel sampling method that incorporates negative conditions into the identity-conditioned diffusion process. NegFaceDiff enhances identity separation by leveraging negative conditions that explicitly guide the model away from unwanted features while preserving intra-class consistency. Extensive experiments demonstrate that NegFaceDiff significantly improves the identity consistency and separability of data generated by identity-conditioned diffusion models. Specifically, identity separability, measured by the Fisher Discriminant Ratio (FDR), increases from 2.427 to 5.687. These improvements are reflected in FR systems trained on the NegFaceDiff dataset, which outperform models trained on data generated without negative conditions across multiple benchmarks.</li>
<li><strong>摘要：</strong>在面部识别（FR）开发中使用合成数据作为真实数据集的替代方案已引起了人们的重大关注，解决了与收集和使用真实数据有关的隐私，道德和实际问题。最新的最新方法提出了身份条件条件的扩散模型，以生成符合身份的面部图像，从而促进了它们在训练FR模型中的使用。但是，这些方法通常缺乏明确的采样机制来强制阶层间的可分离性，从而导致身份重叠，因此，次优FR性能。在这项工作中，我们引入了Negfacediff，这是一种新颖的抽样方法，将负面条件纳入身份条件条件的扩散过程中。 Negfacediff通过利用负面条件来增强身份分离，这些条件明确指导该模型远离不需要的特征，同时保留了类内的一致性。广泛的实验表明，Negfacediff显着提高了由身份条件条件扩散模型产生的数据的身份一致性和可分离性。具体而言，通过Fisher判别比率（FDR）测量的身份可分离性从2.427增加到5.687。这些改进反映在对Negfacediff数据集进行培训的FR系统中，该系统的表现优于在多个基准的情况下生成没有负面条件的数据的模型。</li>
</ul>

<h3>Title: GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors</h3>
<ul>
<li><strong>Authors: </strong>Xingyilang Yin, Qi Zhang, Jiahao Chang, Ying Feng, Qingnan Fan, Xi Yang, Chi-Man Pun, Huaqi Zhang, Xiaodong Cun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09667">https://arxiv.org/abs/2508.09667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09667">https://arxiv.org/pdf/2508.09667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09667]] GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors(https://arxiv.org/abs/2508.09667)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generative</a></li>
<li><strong>Abstract: </strong>Reconstructing 3D scenes using 3D Gaussian Splatting (3DGS) from sparse views is an ill-posed problem due to insufficient information, often resulting in noticeable artifacts. While recent approaches have sought to leverage generative priors to complete information for under-constrained regions, they struggle to generate content that remains consistent with input observations. To address this challenge, we propose GSFixer, a novel framework designed to improve the quality of 3DGS representations reconstructed from sparse inputs. The core of our approach is the reference-guided video restoration model, built upon a DiT-based video diffusion model trained on paired artifact 3DGS renders and clean frames with additional reference-based conditions. Considering the input sparse views as references, our model integrates both 2D semantic features and 3D geometric features of reference views extracted from the visual geometry foundation model, enhancing the semantic coherence and 3D consistency when fixing artifact novel views. Furthermore, considering the lack of suitable benchmarks for 3DGS artifact restoration evaluation, we present DL3DV-Res which contains artifact frames rendered using low-quality 3DGS. Extensive experiments demonstrate our GSFixer outperforms current state-of-the-art methods in 3DGS artifact restoration and sparse-view 3D reconstruction. Project page: this https URL.</li>
<li><strong>摘要：</strong>从稀疏视图中使用3D高斯碎片（3DG）重建3D场景是由于信息不足而导致的一个问题，通常导致明显的伪影。尽管最近的方法试图利用生成先验来完成不受约束区域的信息，但他们努力生成与输入观测值保持一致的内容。为了应对这一挑战，我们提出了GSFIXER，这是一个新颖的框架，旨在提高从稀疏输入中重建的3DGS表示的质量。我们方法的核心是参考引导的视频恢复模型，该模型基于基于DIT的视频扩散模型，该模型在配对的Artifact 3DGS渲染和干净的框架上训练，并具有其他基于参考的条件。考虑到输入稀疏视图作为参考，我们的模型集成了从视觉几何基础模型中提取的参考视图的2D语义特征和3D几何特征，在固定伪影小说视图时增强了语义连贯性和3D一致性。此外，考虑到缺乏用于3DGS人工恢复评估的合适基准，我们提出了DL3DV-RES，其中包含使用低质量3DGS呈现的人工制品框架。广泛的实验证明了我们的GSFIXER在3DGS伪影恢复和稀疏视图3D重建中的当前最新方法。项目页面：此HTTPS URL。</li>
</ul>

<h3>Title: GraphTreeGen: Subtree-Centric Approach to Efficient and Supervised Graph Generation</h3>
<ul>
<li><strong>Authors: </strong>Yitong Luo, Islem Rekik</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09710">https://arxiv.org/abs/2508.09710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09710">https://arxiv.org/pdf/2508.09710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09710]] GraphTreeGen: Subtree-Centric Approach to Efficient and Supervised Graph Generation(https://arxiv.org/abs/2508.09710)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generation, generative</a></li>
<li><strong>Abstract: </strong>Brain connectomes, representing neural connectivity as graphs, are crucial for understanding brain organization but costly and time-consuming to acquire, motivating generative approaches. Recent advances in graph generative modeling offer a data-driven alternative, enabling synthetic connectome generation and reducing dependence on large neuroimaging datasets. However, current models face key limitations: (i) compressing the whole graph into a single latent code (e.g., VGAEs) blurs fine-grained local motifs; (ii) relying on rich node attributes rarely available in connectomes reduces reconstruction quality; (iii) edge-centric models emphasize topology but overlook accurate edge-weight prediction, harming quantitative fidelity; and (iv) computationally expensive designs (e.g., edge-conditioned convolutions) impose high memory demands, limiting scalability. We propose GraphTreeGen (GTG), a subtree-centric generative framework for efficient, accurate connectome synthesis. GTG decomposes each connectome into entropy-guided k-hop trees capturing informative local structure, encoded by a shared GCN. A bipartite message-passing layer fuses subtree embeddings with global node features, while a dual-branch decoder jointly predicts edge existence and weights to reconstruct the adjacency matrix. GTG outperforms state-of-the-art baselines in self-supervised tasks and remains competitive in supervised settings, delivering higher structural fidelity and more precise weights with far less memory. Its modular design enables extensions to connectome super-resolution and cross-modality synthesis. Code: this https URL</li>
<li><strong>摘要：</strong>代表神经连通性作为图形的脑连接组对于理解大脑组织至关重要，但昂贵且耗时，可以激发生成方法。图形生成建模的最新进展提供了数据驱动的替代方案，从而使合成连接组生成并减少对大型神经成像数据集的依赖。但是，当前模型面临关键局限性：（i）将整个图形压缩为单个潜在代码（例如，VGAES）模糊的局部局部图案； （ii）依靠连接组中很少可用的丰富节点属性降低了重建质量； （iii）以边缘为中心的模型强调拓扑，但忽略了准确的边缘重量预测，损害了定量保真度； （iv）计算昂贵的设计（例如边缘条件的卷积）施加了高内存需求，从而限制了可扩展性。我们提出了GraphTreegen（GTG），这是一种以子树为中心的生成框架，用于有效，准确的Connectome合成。 GTG将每个连接组分解为熵引导的K-HOP树，捕获了由共享GCN编码的信息的本地结构。二分之一的消息层融合了带有全局节点特征的子树嵌入，而双支分支解码器共同预测边缘的存在和权重，以重建邻接矩阵。在自我监督任务中，GTG优于最先进的基线，并且在监督环境中保持竞争力，提供更高的结构保真度和更精确的权重，而记忆力却少得多。它的模块化设计可扩展到Connectome超分辨率和跨模式合成。代码：此HTTPS URL</li>
</ul>

<h3>Title: NEURAL: Attention-Guided Pruning for Unified Multimodal Resource-Constrained Clinical Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Devvrat Joshi, Islem Rekik</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09715">https://arxiv.org/abs/2508.09715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09715">https://arxiv.org/pdf/2508.09715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09715]] NEURAL: Attention-Guided Pruning for Unified Multimodal Resource-Constrained Clinical Evaluation(https://arxiv.org/abs/2508.09715)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid growth of multimodal medical imaging data presents significant storage and transmission challenges, particularly in resource-constrained clinical settings. We propose NEURAL, a novel framework that addresses this by using semantics-guided data compression. Our approach repurposes cross-attention scores between the image and its radiological report from a fine-tuned generative vision-language model to structurally prune chest X-rays, preserving only diagnostically critical regions. This process transforms the image into a highly compressed, graph representation. This unified graph-based representation fuses the pruned visual graph with a knowledge graph derived from the clinical report, creating a universal data structure that simplifies downstream modeling. Validated on the MIMIC-CXR and CheXpert Plus dataset for pneumonia detection, NEURAL achieves a 93.4-97.7\% reduction in image data size while maintaining a high diagnostic performance of 0.88-0.95 AUC, outperforming other baseline models that use uncompressed data. By creating a persistent, task-agnostic data asset, NEURAL resolves the trade-off between data size and clinical utility, enabling efficient workflows and teleradiology without sacrificing performance. Our NEURAL code is available at this https URL.</li>
<li><strong>摘要：</strong>多模式医学成像数据的快速增长提出了巨大的存储和传输挑战，尤其是在资源约束的临床环境中。我们提出了神经，这是一个新颖的框架，通过使用语义引导的数据压缩来解决此问题。我们的方法将图像与放射学报告之间的交叉注意分数从微调的生成视觉模型模型到结构修剪胸部X射线，仅保留诊断至关重要的区域。此过程将图像转换为高度压缩的图形表示。这种基于图的统一表示形式将修剪的视觉图与临床报告得出的知识图融合在一起，从而创建了一个通用数据结构，从而简化了下游建模。在Mimic-CXR和CHEXPERT加上用于肺炎检测的数据集上，神经可实现93.4-97.7 \％的图像数据尺寸降低，同时保持0.88-0.95 AUC的较高诊断性能为0.88-0.95 AUC，超过了其他基线模型使用未压缩数据的其他基线模型。通过创建持久的，任务无关的数据资产，神经可以解决数据大小和临床实用程序之间的权衡，从而在不牺牲性能的情况下实现了有效的工作流程和远射学。我们的神经代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Generative Modeling with Multi-Instance Reward Learning for E-commerce Creative Optimization</h3>
<ul>
<li><strong>Authors: </strong>Qiaolei Gu, Yu Li, DingYi Zeng, Lu Wang, Ming Pang, Changping Peng, Zhangang Lin, Ching Law, Jingping Shao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09730">https://arxiv.org/abs/2508.09730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09730">https://arxiv.org/pdf/2508.09730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09730]] Generative Modeling with Multi-Instance Reward Learning for E-commerce Creative Optimization(https://arxiv.org/abs/2508.09730)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In e-commerce advertising, selecting the most compelling combination of creative elements -- such as titles, images, and highlights -- is critical for capturing user attention and driving conversions. However, existing methods often evaluate creative components individually, failing to navigate the exponentially large search space of possible combinations. To address this challenge, we propose a novel framework named GenCO that integrates generative modeling with multi-instance reward learning. Our unified two-stage architecture first employs a generative model to efficiently produce a diverse set of creative combinations. This generative process is optimized with reinforcement learning, enabling the model to effectively explore and refine its selections. Next, to overcome the challenge of sparse user feedback, a multi-instance learning model attributes combination-level rewards, such as clicks, to the individual creative elements. This allows the reward model to provide a more accurate feedback signal, which in turn guides the generative model toward creating more effective combinations. Deployed on a leading e-commerce platform, our approach has significantly increased advertising revenue, demonstrating its practical value. Additionally, we are releasing a large-scale industrial dataset to facilitate further research in this important domain.</li>
<li><strong>摘要：</strong>在电子商务广告中，选择创意元素（例如标题，图像和亮点）最引人注目的组合对于吸引用户的关注和推动转换至关重要。但是，现有方法通常会单独评估创意组件，但未能导航可能组合的较大搜索空间。为了应对这一挑战，我们提出了一个名为GENCO的新颖框架，该框架将生成型建模与多构成奖励学习集成在一起。我们统一的两阶段体系结构首先采用生成模型来有效地生成各种各样的创意组合。通过增强学习来优化此生成过程，使模型能够有效地探索和完善其选择。接下来，为了克服稀疏用户反馈的挑战，一个多现实学习模型组合级别的奖励（例如点击）属于单个创意元素。这允许奖励模型提供更准确的反馈信号，进而指导生成模型来创建更有效的组合。我们的方法部署在领先的电子商务平台上，大大增加了广告收入，证明了其实际价值。此外，我们正在发布一个大规模的工业数据集，以促进该重要领域的进一步研究。</li>
</ul>

<h3>Title: Region-to-Region: Enhancing Generative Image Harmonization with Adaptive Regional Injection</h3>
<ul>
<li><strong>Authors: </strong>Zhiqiu Zhang, Dongqi Fan, Mingjie Wang, Qiang Tang, Jian Yang, Zili Yi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09746">https://arxiv.org/abs/2508.09746</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09746">https://arxiv.org/pdf/2508.09746</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09746]] Region-to-Region: Enhancing Generative Image Harmonization with Adaptive Regional Injection(https://arxiv.org/abs/2508.09746)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The goal of image harmonization is to adjust the foreground in a composite image to achieve visual consistency with the background. Recently, latent diffusion model (LDM) are applied for harmonization, achieving remarkable results. However, LDM-based harmonization faces challenges in detail preservation and limited harmonization ability. Additionally, current synthetic datasets rely on color transfer, which lacks local variations and fails to capture complex real-world lighting conditions. To enhance harmonization capabilities, we propose the Region-to-Region transformation. By injecting information from appropriate regions into the foreground, this approach preserves original details while achieving image harmonization or, conversely, generating new composite data. From this perspective, We propose a novel model R2R. Specifically, we design Clear-VAE to preserve high-frequency details in the foreground using Adaptive Filter while eliminating disharmonious elements. To further enhance harmonization, we introduce the Harmony Controller with Mask-aware Adaptive Channel Attention (MACA), which dynamically adjusts the foreground based on the channel importance of both foreground and background regions. To address the limitation of existing datasets, we propose Random Poisson Blending, which transfers color and lighting information from a suitable region to the foreground, thereby generating more diverse and challenging synthetic images. Using this method, we construct a new synthetic dataset, RPHarmony. Experiments demonstrate the superiority of our method over other methods in both quantitative metrics and visual harmony. Moreover, our dataset helps the model generate more realistic images in real examples. Our code, dataset, and model weights have all been released for open access.</li>
<li><strong>摘要：</strong>图像协调的目的是调整复合图像中的前景，以实现与背景的视觉一致性。最近，潜在扩散模型（LDM）用于协调，取得了显着的结果。但是，基于LDM的协调面临详细保存和有限协调能力的挑战。此外，当前的合成数据集依赖于颜色传递，这缺乏局部变化，并且无法捕获复杂的现实照明条件。为了增强协调能力，我们提出了区域到区域的变换。通过将适当区域的信息注入前景，此方法可以在实现图像协调或相反生成新的复合数据的同时保留原始细节。从这个角度来看，我们提出了一种新颖的R2R模型。具体而言，我们设计清晰的VAE，以使用自适应过滤器在前景中保留高频细节，同时消除不和谐的元素。为了进一步增强协调，我们介绍了用面具感知的自适应通道注意（MACA）引入和谐控制器，该控制器根据前景和背景区域的通道重要性动态调整前景。为了解决现有数据集的局限性，我们提出了随机的泊松混合物，该泊松泊松混合物将颜色和照明信息从合适的区域转移到前景，从而产生更多多样化和具有挑战性的合成图像。使用此方法，我们构建了一个新的合成数据集Rpharmony。实验证明了我们方法比定量指标和视觉和谐中其他方法的优越性。此外，我们的数据集可帮助模型在真实示例中生成更逼真的图像。我们的代码，数据集和模型权重已发布，以进行开放访问。</li>
</ul>

<h3>Title: MUJICA: Reforming SISR Models for PBR Material Super-Resolution via Cross-Map Attention</h3>
<ul>
<li><strong>Authors: </strong>Xin Du, Maoyuan Xu, Zhi Ying</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09802">https://arxiv.org/abs/2508.09802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09802">https://arxiv.org/pdf/2508.09802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09802]] MUJICA: Reforming SISR Models for PBR Material Super-Resolution via Cross-Map Attention(https://arxiv.org/abs/2508.09802)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Physically Based Rendering (PBR) materials are typically characterized by multiple 2D texture maps such as basecolor, normal, metallic, and roughness which encode spatially-varying bi-directional reflectance distribution function (SVBRDF) parameters to model surface reflectance properties and microfacet interactions. Upscaling SVBRDF material is valuable for modern 3D graphics applications. However, existing Single Image Super-Resolution (SISR) methods struggle with cross-map inconsistency, inadequate modeling of modality-specific features, and limited generalization due to data distribution shifts. In this work, we propose Multi-modal Upscaling Joint Inference via Cross-map Attention (MUJICA), a flexible adapter that reforms pre-trained Swin-transformer-based SISR models for PBR material super-resolution. MUJICA is seamlessly attached after the pre-trained and frozen SISR backbone. It leverages cross-map attention to fuse features while preserving remarkable reconstruction ability of the pre-trained SISR model. Applied to SISR models such as SwinIR, DRCT, and HMANet, MUJICA improves PSNR, SSIM, and LPIPS scores while preserving cross-map consistency. Experiments demonstrate that MUJICA enables efficient training even with limited resources and delivers state-of-the-art performance on PBR material datasets.</li>
<li><strong>摘要：</strong>基于物理的渲染（PBR）材料通常以多个2D纹理图（例如碱基，正常，金属和粗糙度）进行特征，这些图编码在空间变化的双向反射分布分布函数（SVBRDF）参数以模拟表面反射率和微胶质相互作用。升级SVBRDF材料对于现代3D图形应用很有价值。但是，现有的单像超分辨率（SISR）方法与交叉映射不一致，模式特异性特征的建模不足以及由于数据分布变化而导致的概括有限。在这项工作中，我们提出了通过交叉映射注意（Mujica）的多模式升级关节推断，这是一种灵活的适配器，改革了预先训练的基于SWIN-TransFormer的SISR SISR模型，以用于PBR材料超级分辨率。预先训练和冷冻的SISR主链后，将Mujica无缝连接。它利用交叉图对保险丝特征的关注，同时保留了预训练的SISR模型的显着重建能力。 Mujica应用于Swinir，Drct和Hmanet等SISR模型，改善了PSNR，SSIM和LPIPS得分，同时保留了交叉映射一致性。实验表明，即使资源有限，也可以在PBR材料数据集上提供最先进的性能，也可以进行有效的培训。</li>
</ul>

<h3>Title: Evolution of Low-Level and Texture Human-CLIP Alignment</h3>
<ul>
<li><strong>Authors: </strong>Pablo Hernández-Cámara, Jose Manuel Jaén-Lorites, Jorge Vila-Tomás, Jesus Malo, Valero Laparra</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09814">https://arxiv.org/abs/2508.09814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09814">https://arxiv.org/pdf/2508.09814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09814]] Evolution of Low-Level and Texture Human-CLIP Alignment(https://arxiv.org/abs/2508.09814)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>During the training of multi-modal models like CLIP, we observed an intriguing phenomenon: the correlation with low-level human image quality assessments peaks in the early epochs before gradually declining. This study investigates this observation and seeks to understand its causes through two key factors: shape-texture bias alignment and classification accuracy drop under noise. Our findings suggest that CLIP initially learn low-level visual features, enhancing its alignment with low-level human perception but also increasing its sensitivity to noise and its texture bias. As training progresses, the model shifts toward more abstract shape-based representations, improving noise robustness but reducing alignment with low-level human perception. These results suggest that these factors shared an underlying learning mechanism and provide new insights into optimizing the trade-off between perceptual alignment and robustness in vision-language models.</li>
<li><strong>摘要：</strong>在训练诸如剪辑之类的多模式模型的训练中，我们观察到了一种有趣的现象：与低级人类图像质量评估的相关性在早期逐渐下降之前达到了峰值。这项研究调查了这一观察结果，并试图通过两个关键因素理解其原因：形状质量偏置的比对对准和分类精度在噪声下下降。我们的发现表明，剪辑最初学习低级视觉特征，增强其与低级人类感知的对齐，同时也增加了对噪声及其质地偏见的敏感性。随着训练的进展，该模型转向了基于抽象的形状表示，改善了噪声稳健性，但通过低级人类感知减少了对齐。这些结果表明，这些因素具有潜在的学习机制，并提供了新见解，以优化视觉模型中感知对齐和鲁棒性之间的权衡。</li>
</ul>

<h3>Title: ViMoNet: A Multimodal Vision-Language Framework for Human Behavior Understanding from Motion and Video</h3>
<ul>
<li><strong>Authors: </strong>Rajan Das Gupta, Md Yeasin Rahat, Nafiz Fahad, Abir Ahmed, Liew Tze Hui</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09818">https://arxiv.org/abs/2508.09818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09818">https://arxiv.org/pdf/2508.09818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09818]] ViMoNet: A Multimodal Vision-Language Framework for Human Behavior Understanding from Motion and Video(https://arxiv.org/abs/2508.09818)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This study investigates how large language models (LLMs) can be used to understand human behavior using motion and video data. We think that mixing both types is essential to completely capture the nuanced movements and meanings of human actions, in contrast to recent models that simply concentrate on motion data or films. To address this, we provide ViMoNet, a straightforward yet effective framework for comprehending, characterizing, and deducing human action. ViMoNet employs a joint training strategy that leverages the advantages of two data types: detailed motion-text data, which is more exact, and generic video-text data, which is more comprehensive but less detailed. This aids in the model's acquisition of rich data regarding time and space in human behavior. Additionally, we provide a brand new dataset named VIMOS that contains a variety of films, motion sequences, instructions, and subtitles. We developed ViMoNet-Bench, a standardized benchmark with carefully labeled samples, to evaluate how well models understand human behavior. Our tests show that ViMoNet outperforms existing methods in caption generation, motion understanding, and behavior interpretation.</li>
<li><strong>摘要：</strong>这项研究调查了使用运动和视频数据可以使用大型语言模型（LLM）来理解人类行为。我们认为，与仅关注运动数据或电影的最新模型相比，混合两种类型对于完全捕获人类行为的细微运动和含义至关重要。为了解决这个问题，我们提供了Vimonet，这是理解，表征和推论人类行动的直接而有效的框架。 Vimonet采用了一种联合培训策略，利用两种数据类型的优点：详细的运动文本数据，更精确和通用的视频文本数据，这更全面，但较少详细。这有助于该模型获得有关人类行为时间和空间的丰富数据。此外，我们提供了一个名为VIMO的全新数据集，其中包含各种电影，运动序列，说明和字幕。我们开发了Vimonet Bench，这是一种标准化的基准，带有精心标记的样品，以评估模型对人类行为的了解程度。我们的测试表明，Vimonet在标题生成，运动理解和行为解释中的表现优于现有方法。</li>
</ul>

<h3>Title: Physical Autoregressive Model for Robotic Manipulation without Action Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Zijian Song, Sihan Qin, Tianshui Chen, Liang Lin, Guangrun Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09822">https://arxiv.org/abs/2508.09822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09822">https://arxiv.org/pdf/2508.09822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09822]] Physical Autoregressive Model for Robotic Manipulation without Action Pretraining(https://arxiv.org/abs/2508.09822)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The scarcity of manipulation data has motivated the use of pretrained large models from other modalities in robotics. In this work, we build upon autoregressive video generation models to propose a Physical Autoregressive Model (PAR), where physical tokens combine frames and actions to represent the joint evolution of the robot and its environment. PAR leverages the world knowledge embedded in video pretraining to understand physical dynamics without requiring action pretraining, enabling accurate video prediction and consistent action trajectories. It also adopts a DiT-based de-tokenizer to model frames and actions as continuous tokens, mitigating quantization errors and facilitating mutual enhancement. Furthermore, we incorporate a causal mask with inverse kinematics, parallel training, and the KV-cache mechanism to further improve performance and efficiency. Experiments on the ManiSkill benchmark show that PAR achieves a 100\% success rate on the PushCube task, matches the performance of action-pretrained baselines on other tasks, and accurately predicts future videos with tightly aligned action trajectories. These findings underscore a promising direction for robotic manipulation by transferring world knowledge from autoregressive video pretraining.</li>
<li><strong>摘要：</strong>操纵数据的稀缺性促使使用机器人技术中其他模式的大型模型使用。在这项工作中，我们基于自回归视频生成模型，以提出一种物理自回旋模型（PAR），其中物理令牌结合了框架和动作以代表机器人及其环境的关节演变。 PAR利用嵌入在视频预处理中的世界知识来理解物理动力学，而无需进行操作进行预处理，实现了准确的视频预测和一致的动作轨迹。它还采用基于DIT的De-tokenizer将框架和动作建模为连续令牌，减轻量化错误并促进相互增强。此外，我们结合了一个因子面具，具有逆运动学，平行训练和KV-CACHE机制，以进一步提高性能和效率。 Maniskill基准测试的实验表明，PAR在电机轴任务上达到了100 \％的成功率，与其他任务上的动作预测基线的性能相匹配，并准确地预测了具有紧密调整的动作轨迹的未来视频。这些发现强调了机器人操纵的有希望的方向，通过从自回归视频预处理中转移世界知识。</li>
</ul>

<h3>Title: KonfAI: A Modular and Fully Configurable Framework for Deep Learning in Medical Imaging</h3>
<ul>
<li><strong>Authors: </strong>Valentin Boussot, Jean-Louis Dillenseger</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09823">https://arxiv.org/abs/2508.09823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09823">https://arxiv.org/pdf/2508.09823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09823]] KonfAI: A Modular and Fully Configurable Framework for Deep Learning in Medical Imaging(https://arxiv.org/abs/2508.09823)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>KonfAI is a modular, extensible, and fully configurable deep learning framework specifically designed for medical imaging tasks. It enables users to define complete training, inference, and evaluation workflows through structured YAML configuration files, without modifying the underlying code. This declarative approach enhances reproducibility, transparency, and experimental traceability while reducing development time. Beyond the capabilities of standard pipelines, KonfAI provides native abstractions for advanced strategies including patch-based learning, test-time augmentation, model ensembling, and direct access to intermediate feature representations for deep supervision. It also supports complex multi-model training setups such as generative adversarial architectures. Thanks to its modular and extensible architecture, KonfAI can easily accommodate custom models, loss functions, and data processing components. The framework has been successfully applied to segmentation, registration, and image synthesis tasks, and has contributed to top-ranking results in several international medical imaging challenges. KonfAI is open source and available at \href{this https URL}{this https URL}.</li>
<li><strong>摘要：</strong>Konfai是一个模块化，可扩展且完全可配置的深度学习框架，专门为医学成像任务设计。它使用户可以通过结构化的YAML配置文件定义完整的培训，推理和评估工作流，而无需修改基础代码。这种声明的方法可以增强可重复性，透明度和实验性可追溯性，同时减少开发时间。除了标准管道的功能之外，Konfai还为高级策略提供了本地抽象，包括基于补丁的学习，测试时间扩展，模型结合和直接访问中间功能表示以进行深入监督。它还支持复杂的多模型训练设置，例如生成对手体系结构。由于其模块化且可扩展的体系结构，Konfai可以轻松容纳自定义模型，损失功能和数据处理组件。该框架已成功地应用于细分，注册和图像综合任务，并在几个国际医学成像挑战中促成了最高级别的结果。 konfai是开源的，可在\ href {this https url} {this https url}上获得。</li>
</ul>

<h3>Title: Reverse Convolution and Its Applications to Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Xuhong Huang, Shiqi Liu, Kai Zhang, Ying Tai, Jian Yang, Hui Zeng, Lei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09824">https://arxiv.org/abs/2508.09824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09824">https://arxiv.org/pdf/2508.09824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09824]] Reverse Convolution and Its Applications to Image Restoration(https://arxiv.org/abs/2508.09824)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, super-resolution</a></li>
<li><strong>Abstract: </strong>Convolution and transposed convolution are fundamental operators widely used in neural networks. However, transposed convolution (a.k.a. deconvolution) does not serve as a true inverse of convolution due to inherent differences in their mathematical formulations. To date, no reverse convolution operator has been established as a standard component in neural architectures. In this paper, we propose a novel depthwise reverse convolution operator as an initial attempt to effectively reverse depthwise convolution by formulating and solving a regularized least-squares optimization problem. We thoroughly investigate its kernel initialization, padding strategies, and other critical aspects to ensure its effective implementation. Building upon this operator, we further construct a reverse convolution block by combining it with layer normalization, 1$\times$1 convolution, and GELU activation, forming a Transformer-like structure. The proposed operator and block can directly replace conventional convolution and transposed convolution layers in existing architectures, leading to the development of ConverseNet. Corresponding to typical image restoration models such as DnCNN, SRResNet and USRNet, we train three variants of ConverseNet for Gaussian denoising, super-resolution and deblurring, respectively. Extensive experiments demonstrate the effectiveness of the proposed reverse convolution operator as a basic building module. We hope this work could pave the way for developing new operators in deep model design and applications.</li>
<li><strong>摘要：</strong>卷积和转置卷积是广泛用于神经网络的基本操作员。但是，由于其数学表述的固有差异，转置卷积（又称反卷积）并不是卷积的真正倒数。迄今为止，尚未建立反向卷积操作员作为神经体系结构的标准组件。在本文中，我们提出了一个新型的深度反向卷积操作员，作为初步尝试通过制定和解决正则最小二乘优化问题来有效逆转深度卷积的初步尝试。我们彻底研究其内核初始化，填充策略和其他关键方面，以确保其有效的实施。在该操作员的基础上，我们通过将其与图层归一化，1 $ \ times $ 1卷积和GELU激活相结合，进一步构建了反向卷积块，形成了类似变压器的结构。拟议的操作员和块可以直接替换现有架构中的常规卷积和转置卷积层，从而导致对Conversenet的发展。对应于典型的图像恢复模型，例如DNCNN，SRRESNET和USRNET，我们分别训练三种converseenet变体，分别用于高斯denoising，超分辨率和脱张。广泛的实验证明了拟议的反卷积操作员作为基本建筑模块的有效性。我们希望这项工作可以为开发深层模型设计和应用的新运营商铺平道路。</li>
</ul>

<h3>Title: Hierarchical Graph Attention Network for No-Reference Omnidirectional Image Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Hao Yang, Xu Zhang, Jiaqi Ma, Linwei Zhu, Yun Zhang, Huan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09843">https://arxiv.org/abs/2508.09843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09843">https://arxiv.org/pdf/2508.09843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09843]] Hierarchical Graph Attention Network for No-Reference Omnidirectional Image Quality Assessment(https://arxiv.org/abs/2508.09843)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Current Omnidirectional Image Quality Assessment (OIQA) methods struggle to evaluate locally non-uniform distortions due to inadequate modeling of spatial variations in quality and ineffective feature representation capturing both local details and global context. To address this, we propose a graph neural network-based OIQA framework that explicitly models structural relationships between viewports to enhance perception of spatial distortion non-uniformity. Our approach employs Fibonacci sphere sampling to generate viewports with well-structured topology, representing each as a graph node. Multi-stage feature extraction networks then derive high-dimensional node representation. To holistically capture spatial dependencies, we integrate a Graph Attention Network (GAT) modeling fine-grained local distortion variations among adjacent viewports, and a graph transformer capturing long-range quality interactions across distant regions. Extensive experiments on two large-scale OIQA databases with complex spatial distortions demonstrate that our method significantly outperforms existing approaches, confirming its effectiveness and strong generalization capability.</li>
<li><strong>摘要：</strong>当前的全向图像质量评估（OIQA）方法由于在质量和无效特征表示空间变化的建模不足而难以评估本地不均匀的扭曲，从而捕获了本地细节和全球环境。为了解决这个问题，我们提出了一个基于图神经网络的OIQA框架，该框架明确地模拟了视口之间的结构关系，以增强对空间失真不均匀性的感知。我们的方法采用斐波那契球体采样来生成具有结构良好的拓扑结构的视口，将每个拓扑表示为图节点。然后，多级特征提取网络得出高维节点表示。为了整体捕获空间依赖性，我们整合了一个图形注意网络（GAT），对相邻视口之间的细粒度局部失真变化进行建模，以及一个图形变压器捕获跨遥远区域的长距离质量相互作用的图形变压器。对两个具有复杂空间扭曲的大规模OIQA数据库的广泛实验表明，我们的方法显着胜过现有方法，证实了其有效性和强大的概括能力。</li>
</ul>

<h3>Title: Enhancing Diffusion Face Generation with Contrastive Embeddings and SegFormer Guidance</h3>
<ul>
<li><strong>Authors: </strong>Dhruvraj Singh Rawat, Enggen Sherpa, Rishikesan Kirupanantha, Tin Hoang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09847">https://arxiv.org/abs/2508.09847</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09847">https://arxiv.org/pdf/2508.09847</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09847]] Enhancing Diffusion Face Generation with Contrastive Embeddings and SegFormer Guidance(https://arxiv.org/abs/2508.09847)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present a benchmark of diffusion models for human face generation on a small-scale CelebAMask-HQ dataset, evaluating both unconditional and conditional pipelines. Our study compares UNet and DiT architectures for unconditional generation and explores LoRA-based fine-tuning of pretrained Stable Diffusion models as a separate experiment. Building on the multi-conditioning approach of Giambi and Lisanti, which uses both attribute vectors and segmentation masks, our main contribution is the integration of an InfoNCE loss for attribute embedding and the adoption of a SegFormer-based segmentation encoder. These enhancements improve the semantic alignment and controllability of attribute-guided synthesis. Our results highlight the effectiveness of contrastive embedding learning and advanced segmentation encoding for controlled face generation in limited data settings.</li>
<li><strong>摘要：</strong>我们为在小型Celebamask-HQ数据集上的人脸生成的扩散模型提供了基准，评估了无条件和条件管道。我们的研究比较了无条件生成的UNET和DIT体系结构，并探讨了基于洛拉的稳定扩散模型的微调，作为一个单独的实验。在使用属性向量和分割面罩的Giambi和Lisanti的多条件方法的基础上，我们的主要贡献是属性嵌入的Infonce损失的整合以及采用基于Segformer的片段编码器。这些增强功能改善了属性引导合成的语义一致性和可控性。我们的结果突出了对比度嵌入学习和在有限数据设置中受控面部生成的对比度嵌入学习和高级分割编码的有效性。</li>
</ul>

<h3>Title: HumanGenesis: Agent-Based Geometric and Generative Modeling for Synthetic Human Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Weiqi Li, Zehao Zhang, Liang Lin, Guangrun Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09858">https://arxiv.org/abs/2508.09858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09858">https://arxiv.org/pdf/2508.09858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09858]] HumanGenesis: Agent-Based Geometric and Generative Modeling for Synthetic Human Dynamics(https://arxiv.org/abs/2508.09858)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>\textbf{Synthetic human dynamics} aims to generate photorealistic videos of human subjects performing expressive, intention-driven motions. However, current approaches face two core challenges: (1) \emph{geometric inconsistency} and \emph{coarse reconstruction}, due to limited 3D modeling and detail preservation; and (2) \emph{motion generalization limitations} and \emph{scene inharmonization}, stemming from weak generative capabilities. To address these, we present \textbf{HumanGenesis}, a framework that integrates geometric and generative modeling through four collaborative agents: (1) \textbf{Reconstructor} builds 3D-consistent human-scene representations from monocular video using 3D Gaussian Splatting and deformation decomposition. (2) \textbf{Critique Agent} enhances reconstruction fidelity by identifying and refining poor regions via multi-round MLLM-based reflection. (3) \textbf{Pose Guider} enables motion generalization by generating expressive pose sequences using time-aware parametric encoders. (4) \textbf{Video Harmonizer} synthesizes photorealistic, coherent video via a hybrid rendering pipeline with diffusion, refining the Reconstructor through a Back-to-4D feedback loop. HumanGenesis achieves state-of-the-art performance on tasks including text-guided synthesis, video reenactment, and novel-pose generalization, significantly improving expressiveness, geometric fidelity, and scene integration.</li>
<li><strong>摘要：</strong>\ textbf {综合人动力学}旨在生成对人类受试者进行表现力，意图驱动动作的视频。但是，当前方法面临两个核心挑战：（1）\ emph {几何不一致}和\ emph {粗{粗{粗构建}，这是由于有限的3D建模和细节保存而导致的； （2）\ emph {运动概括限制}和\ emph {场景inharmonization}，源于弱生成能力。为了解决这些问题，我们提出了\ textbf {Homangenesis}，该框架通过四种协作代理集成了几何和生成性建模：（1）\ textbf {reconstructor}构建3D偶然的人类娱乐表述，使用3D高斯分裂和缺陷和缺陷分解和缺陷型从单眼视频中构建。 （2）\ textbf {Critique Agent}通过通过基于MLLM的多轮反射来识别和完善贫困区域，从而增强了重建保真度。 （3）\ textbf {姿势指南}通过使用时间吸引参数编码生成表达姿势序列来实现运动概括。 （4）\ textbf {视频谐波}通过混合渲染管道与扩散的混合渲染管道合成了逼真的，连贯的视频，从而通过背对面的4D反馈循环来完善重建器。 Humangenesis在任务上实现了最新的绩效，包括文本引导的综合，视频重演和新颖的置换概括，可显着提高表现力，几何忠诚度和场景集成。</li>
</ul>

<h3>Title: Beyond Scaling Law: A Data-Efficient Distillation Framework for Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Xiaojun Wu, Xiaoguang Jiang, Huiyang Li, Jucai Zhai, Dengfeng Liu, Qiaobo Hao, Huang Liu, Zhiguo Yang, Ji Xie, Ninglun Gu, Jin Yang, Kailai Zhang, Yelun Bao, Jun Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09883">https://arxiv.org/abs/2508.09883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09883">https://arxiv.org/pdf/2508.09883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09883]] Beyond Scaling Law: A Data-Efficient Distillation Framework for Reasoning(https://arxiv.org/abs/2508.09883)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demonstrate remarkable reasoning capabilities in tasks such as algorithmic coding and mathematical problem-solving. Recent methods have improved reasoning through expanded corpus and multistage training combining reinforcement learning and supervised fine-tuning. Although some methods suggest that small but targeted dataset can incentivize reasoning via only distillation, a reasoning scaling laws is still taking shape, increasing computational costs. To address this, we propose a data-efficient distillation framework (DED) that optimizes the Pareto frontier of reasoning distillation. Inspired by the on-policy learning and diverse roll-out strategies of reinforcement learning, the key idea of our approach is threefold: (1) We identify that benchmark scores alone do not determine an effective teacher model. Through comprehensive comparisons of leading reasoning LLMs, we develop a method to select an optimal teacher model. (2) While scaling distillation can enhance reasoning, it often degrades out-of-domain performance. A carefully curated, smaller corpus achieves a balanced trade-off between in-domain and out-of-domain capabilities. (3) Diverse reasoning trajectories encourage the student model to develop robust reasoning skills. We validate our method through evaluations on mathematical reasoning (AIME 2024/2025, MATH-500) and code generation (LiveCodeBench), achieving state-of-the-art results with only 0.8k carefully curated examples, bypassing the need for extensive scaling. Our systematic analysis demonstrates that DED outperforms existing methods by considering factors beyond superficial hardness, token length, or teacher model capability. This work offers a practical and efficient pathway to advanced reasoning while preserving general capabilities.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）在算法编码和数学问题解决等任务中表现出了显着的推理功能。最近的方法通过扩展的语料库和多阶段培训来改善推理，从而结合了强化学习和监督微调。尽管某些方法表明，只有小而有针对性的数据集只能通过蒸馏激励推理，但推理缩放定律仍在形成，从而增加了计算成本。为了解决这个问题，我们提出了一个数据有效的蒸馏框架（DED），以优化推理蒸馏的帕累托前沿。受到实行学习和强化学习的多样性推出策略的启发，我们方法的关键思想是三重的：（1）我们确定仅基准分数并不能决定有效的教师模型。通过对领先推理LLM的全面比较，我们开发了一种选择最佳教师模型的方法。 （2）虽然缩放蒸馏可以增强推理，但它通常会降低外域的性能。经过精心策划的较小语料库实现了内域和域外能力之间的平衡权衡。 （3）各种推理轨迹鼓励学生模型发展强大的推理技能。我们通过评估数学推理（AIME 2024/2025，MATH-500）和代码生成（LiveCodeBench）来验证我们的方法，仅使用0.8K精心策划的示例来实现最新结果，从而绕开了对广泛扩展的需求。我们的系统分析表明，通过考虑浅表硬度，象征性长度或教师模型能力的因素，提出的表现优于现有方法。这项工作为高级推理提供了实用，有效的途径，同时保留一般能力。</li>
</ul>

<h3>Title: Prototype-Guided Diffusion: Visual Conditioning without External Memory</h3>
<ul>
<li><strong>Authors: </strong>Bilal Faye, Hanane Azzag, Mustapha Lebbah</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09922">https://arxiv.org/abs/2508.09922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09922">https://arxiv.org/pdf/2508.09922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09922]] Prototype-Guided Diffusion: Visual Conditioning without External Memory(https://arxiv.org/abs/2508.09922)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as a leading framework for high-quality image generation, offering stable training and strong performance across diverse domains. However, they remain computationally intensive, particularly during the iterative denoising process. Latent-space models like Stable Diffusion alleviate some of this cost by operating in compressed representations, though at the expense of fine-grained detail. More recent approaches such as Retrieval-Augmented Diffusion Models (RDM) address efficiency by conditioning denoising on similar examples retrieved from large external memory banks. While effective, these methods introduce drawbacks: they require costly storage and retrieval infrastructure, depend on static vision-language models like CLIP for similarity, and lack adaptability during training. We propose the Prototype Diffusion Model (PDM), a method that integrates prototype learning directly into the diffusion process for efficient and adaptive visual conditioning - without external memory. Instead of retrieving reference samples, PDM constructs a dynamic set of compact visual prototypes from clean image features using contrastive learning. These prototypes guide the denoising steps by aligning noisy representations with semantically relevant visual patterns, enabling efficient generation with strong semantic grounding. Experiments show that PDM maintains high generation quality while reducing computational and storage overhead, offering a scalable alternative to retrieval-based conditioning in diffusion models.</li>
<li><strong>摘要：</strong>扩散模型已成为高质量图像生成的领先框架，在不同领域提供稳定的训练和强大的性能。但是，它们仍然在计算密集程度上，尤其是在迭代denoising过程中。诸如稳定扩散之类的潜在模型通过以压缩表示形式运行来减轻其中的一些成本，尽管以细粒的细节为代价。诸如检索式扩散模型（RDM）等最新方法通过根据从大型外部记忆库检索的类似示例进行调节来解决效率。尽管有效，但这些方法引入了缺点：它们需要昂贵的存储和检索基础架构，但取决于静态视觉语言模型（例如剪辑）的相似性，并且在训练过程中缺乏适应性。我们提出了原型扩散模型（PDM），该方法将原型学习直接整合到无外部内存的有效和适应性视觉条件的扩散过程中。 PDM没有检索参考样本，而是使用对比度学习从干净的图像特征中构造了一组紧凑的视觉原型。这些原型通过将嘈杂的表示与语义相关的视觉模式对齐，从而指导denoising步骤，从而使具有强大语义接地的有效产生。实验表明，PDM保持高生成质量，同时降低计算和存储开销，从而在扩散模型中提供可扩展的基于基于检索的条件的替代方案。</li>
</ul>

<h3>Title: Quo Vadis Handwritten Text Generation for Handwritten Text Recognition?</h3>
<ul>
<li><strong>Authors: </strong>Vittorio Pippi, Konstantina Nikolaidou, Silvia Cascianelli, George Retsinas, Giorgos Sfikas, Rita Cucchiara, Marcus Liwicki</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.DL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09936">https://arxiv.org/abs/2508.09936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09936">https://arxiv.org/pdf/2508.09936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09936]] Quo Vadis Handwritten Text Generation for Handwritten Text Recognition?(https://arxiv.org/abs/2508.09936)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The digitization of historical manuscripts presents significant challenges for Handwritten Text Recognition (HTR) systems, particularly when dealing with small, author-specific collections that diverge from the training data distributions. Handwritten Text Generation (HTG) techniques, which generate synthetic data tailored to specific handwriting styles, offer a promising solution to address these challenges. However, the effectiveness of various HTG models in enhancing HTR performance, especially in low-resource transcription settings, has not been thoroughly evaluated. In this work, we systematically compare three state-of-the-art styled HTG models (representing the generative adversarial, diffusion, and autoregressive paradigms for HTG) to assess their impact on HTR fine-tuning. We analyze how visual and linguistic characteristics of synthetic data influence fine-tuning outcomes and provide quantitative guidelines for selecting the most effective HTG model. The results of our analysis provide insights into the current capabilities of HTG methods and highlight key areas for further improvement in their application to low-resource HTR.</li>
<li><strong>摘要：</strong>历史手稿的数字化对手写文本识别（HTR）系统提出了重大挑战，尤其是在处理与培训数据分布不同的小型，特定于作者的集合时。手写文本生成（HTG）技术生成针对特定手写样式的合成数据，提供了一种有希望的解决方案来应对这些挑战。但是，尚未对各种HTG模型在增强HTR性能（尤其是在低资源转录设置中）的有效性。在这项工作中，我们系统地比较了三种最先进的HTG模型（代表HTG的生成对抗，扩散和自动回归范式），以评估其对HTR微调的影响。我们分析合成数据的视觉和语言特征如何影响微调结果，并为选择最有效的HTG模型提供定量指南。我们的分析结果提供了有关HTG方法当前功能的见解，并突出了关键领域，以进一步改善其在低资源HTR中的应用。</li>
</ul>

<h3>Title: AST-n: A Fast Sampling Approach for Low-Dose CT Reconstruction using Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Tomás de la Sotta, José M. Saavedra, Héctor Henríquez, Violeta Chang, Aline Xavier</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09943">https://arxiv.org/abs/2508.09943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09943">https://arxiv.org/pdf/2508.09943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09943]] AST-n: A Fast Sampling Approach for Low-Dose CT Reconstruction using Diffusion Models(https://arxiv.org/abs/2508.09943)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Low-dose CT (LDCT) protocols reduce radiation exposure but increase image noise, compromising diagnostic confidence. Diffusion-based generative models have shown promise for LDCT denoising by learning image priors and performing iterative refinement. In this work, we introduce AST-n, an accelerated inference framework that initiates reverse diffusion from intermediate noise levels, and integrate high-order ODE solvers within conditioned models to further reduce sampling steps. We evaluate two acceleration paradigms--AST-n sampling and standard scheduling with high-order solvers -- on the Low Dose CT Grand Challenge dataset, covering head, abdominal, and chest scans at 10-25 % of standard dose. Conditioned models using only 25 steps (AST-25) achieve peak signal-to-noise ratio (PSNR) above 38 dB and structural similarity index (SSIM) above 0.95, closely matching standard baselines while cutting inference time from ~16 seg to under 1 seg per slice. Unconditional sampling suffers substantial quality loss, underscoring the necessity of conditioning. We also assess DDIM inversion, which yields marginal PSNR gains at the cost of doubling inference time, limiting its clinical practicality. Our results demonstrate that AST-n with high-order samplers enables rapid LDCT reconstruction without significant loss of image fidelity, advancing the feasibility of diffusion-based methods in clinical workflows.</li>
<li><strong>摘要：</strong>低剂量CT（LDCT）方案减少了辐射暴露，但增加了图像噪声，损害了诊断信心。基于扩散的生成模型通过学习图像先验和进行迭代精致表现出了LDCT Denoising的希望。在这项工作中，我们引入了AST-N，这是一种加速的推理框架，该框架启动了从中间噪声水平的反向扩散，并将高阶Ode求解器集成到条件模型中，以进一步减少采样步骤。我们在低剂量CT Grand挑战数据集上评估了两个加速度范式（用高阶求解器），以高剂量求解器进行评估，覆盖头，腹部，腹部和胸部扫描，分别为标准剂量的10-25％。条件模型仅使用25个步骤（AST-25）实现高于38 dB的峰值信噪比（PSNR），而结构相似性指数（SSIM）高于0.95，同时将推理时间从〜16 se的切割至16 seg降至1 seg。无条件采样会遭受重大质量损失，强调了调节的必要性。我们还评估了DDIM倒置，该倒置会以倍增时间的成本增长，从而限制其临床实用性。我们的结果表明，使用高阶采样器的AST-N可以快速的LDCT重建，而不会显着损失图像保真度，从而提高了基于扩散方法在临床工作流程中的可行性。</li>
</ul>

<h3>Title: Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Luca Eyring, Shyamgopal Karthik, Alexey Dosovitskiy, Nataniel Ruiz, Zeynep Akata</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09968">https://arxiv.org/abs/2508.09968</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09968">https://arxiv.org/pdf/2508.09968</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09968]] Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models(https://arxiv.org/abs/2508.09968)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The new paradigm of test-time scaling has yielded remarkable breakthroughs in Large Language Models (LLMs) (e.g. reasoning models) and in generative vision models, allowing models to allocate additional computation during inference to effectively tackle increasingly complex problems. Despite the improvements of this approach, an important limitation emerges: the substantial increase in computation time makes the process slow and impractical for many applications. Given the success of this paradigm and its growing usage, we seek to preserve its benefits while eschewing the inference overhead. In this work we propose one solution to the critical problem of integrating test-time scaling knowledge into a model during post-training. Specifically, we replace reward guided test-time noise optimization in diffusion models with a Noise Hypernetwork that modulates initial input noise. We propose a theoretically grounded framework for learning this reward-tilted distribution for distilled generators, through a tractable noise-space objective that maintains fidelity to the base model while optimizing for desired characteristics. We show that our approach recovers a substantial portion of the quality gains from explicit test-time optimization at a fraction of the computational cost. Code is available at this https URL</li>
<li><strong>摘要：</strong>测试时间缩放的新范式在大型语言模型（LLMS）（例如推理模型）和生成视觉模型中产生了显着的突破，允许模型在推断过程中分配额外的计算以有效解决日益复杂的问题。尽管这种方法有所改善，但仍出现了一个重要的局限性：计算时间的大幅度增加使得该过程在许多应用中缓慢而不切实际。鉴于这种范式的成功及其不断增长的用法，我们试图在避免推理开销时保持其利益。在这项工作中，我们提出了一种解决方案，即在培训后将测试时间缩放知识整合到模型中的关键问题。具体而言，我们用调节初始输入噪声的噪声超网络替换了扩散模型中的奖励指导测试时间噪声优化。我们提出了一个理论上扎根的框架，用于通过一个可拖动的噪声空间目标来学习这种奖励发电机的奖励倾斜分布，该目标保持对基本模型的保真度，同时优化所需的特性。我们表明，我们的方法从计算成本的一小部分中从明确的测试时间优化中恢复了质量的很大一部分。代码可在此HTTPS URL上找到</li>
</ul>

<h3>Title: A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation</h3>
<ul>
<li><strong>Authors: </strong>Shuting He, Peilin Ji, Yitong Yang, Changshuo Wang, Jiayi Ji, Yinglin Wang, Henghui Ding</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09977">https://arxiv.org/abs/2508.09977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09977">https://arxiv.org/pdf/2508.09977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09977]] A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation(https://arxiv.org/abs/2508.09977)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>3D Gaussian Splatting (3DGS) has recently emerged as a powerful alternative to Neural Radiance Fields (NeRF) for 3D scene representation, offering high-fidelity photorealistic rendering with real-time performance. Beyond novel view synthesis, the explicit and compact nature of 3DGS enables a wide range of downstream applications that require geometric and semantic understanding. This survey provides a comprehensive overview of recent progress in 3DGS applications. It first introduces 2D foundation models that support semantic understanding and control in 3DGS applications, followed by a review of NeRF-based methods that inform their 3DGS counterparts. We then categorize 3DGS applications into segmentation, editing, generation, and other functional tasks. For each, we summarize representative methods, supervision strategies, and learning paradigms, highlighting shared design principles and emerging trends. Commonly used datasets and evaluation protocols are also summarized, along with comparative analyses of recent methods across public benchmarks. To support ongoing research and development, a continually updated repository of papers, code, and resources is maintained at this https URL.</li>
<li><strong>摘要：</strong>3D高斯脱落（3DGS）最近已成为3D场景表示的神经辐射场（NERF）的有力替代品，提供了具有实时性能的高保真感性逼真的渲染。除了新颖的视图综合外，3DG的明确和紧凑性能还可以使需要几何学和语义理解的广泛下游应用。这项调查提供了有关3DGS应用程序最近进展的全面概述。它首先引入了2D基础模型，该模型支持3DGS应用程序中的语义理解和控制，然后对基于NERF的方法进行综述，该方法为其3DGS提供了信息。然后，我们将3DGS应用程序分为细分，编辑，生成和其他功能任务。对于每个人，我们总结了代表性方法，监督策略和学习范式，突出了共同的设计原理和新兴趋势。还总结了常用的数据集和评估协议，以及对公共基准的最新方法的比较分析。为了支持正在进行的研究和开发，在此HTTPS URL上维护了不断更新的论文，代码和资源的存储库。</li>
</ul>

<h3>Title: Story2Board: A Training-Free Approach for Expressive Storyboard Generation</h3>
<ul>
<li><strong>Authors: </strong>David Dinkevich, Matan Levy, Omri Avrahami, Dvir Samuel, Dani Lischinski</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09983">https://arxiv.org/abs/2508.09983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09983">https://arxiv.org/pdf/2508.09983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09983]] Story2Board: A Training-Free Approach for Expressive Storyboard Generation(https://arxiv.org/abs/2508.09983)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present Story2Board, a training-free framework for expressive storyboard generation from natural language. Existing methods narrowly focus on subject identity, overlooking key aspects of visual storytelling such as spatial composition, background evolution, and narrative pacing. To address this, we introduce a lightweight consistency framework composed of two components: Latent Panel Anchoring, which preserves a shared character reference across panels, and Reciprocal Attention Value Mixing, which softly blends visual features between token pairs with strong reciprocal attention. Together, these mechanisms enhance coherence without architectural changes or fine-tuning, enabling state-of-the-art diffusion models to generate visually diverse yet consistent storyboards. To structure generation, we use an off-the-shelf language model to convert free-form stories into grounded panel-level prompts. To evaluate, we propose the Rich Storyboard Benchmark, a suite of open-domain narratives designed to assess layout diversity and background-grounded storytelling, in addition to consistency. We also introduce a new Scene Diversity metric that quantifies spatial and pose variation across storyboards. Our qualitative and quantitative results, as well as a user study, show that Story2Board produces more dynamic, coherent, and narratively engaging storyboards than existing baselines.</li>
<li><strong>摘要：</strong>我们介绍了Story2board，这是一个无训练的框架，用于从自然语言中发挥表达性故事板的生成。现有的方法狭义地关注主题身份，忽略了视觉讲故事的关键方面，例如空间组成，背景演变和叙事节奏。为了解决这个问题，我们介绍了一个由两个组件组成的轻质一致性框架：潜在面板锚定，该框架保留了整个面板之间的共享字符参考，以及相互注意值混合，该参考值将视觉特征在令牌对之间柔和地融合在一起，并具有强烈的相互关注。这些机制共同提高了连贯性，而无需建筑变化或微调，从而使最先进的扩散模型能够产生视觉上多样化但一致的故事板。为了结构生成，我们使用现成的语言模型将自由形式的故事转换为接地面板级别的提示。为了评估，我们提出了丰富的故事板基准，这是一套开阔的域叙事，旨在评估布局多样性和背景故事的讲故事，此外还具有一致性。我们还引入了一个新的场景多样性指标，该度量量化了故事板之间的空间和姿势变化。我们的定性和定量结果以及用户研究表明，Story2板比现有基准产生的动态，连贯和叙事的故事板更具动态，连贯和引人入胜的故事板。</li>
</ul>

<h3>Title: Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Junyan Ye, Dongzhi Jiang, Zihao Wang, Leqi Zhu, Zhenghao Hu, Zilong Huang, Jun He, Zhiyuan Yan, Jinghua Yu, Hongsheng Li, Conghui He, Weijia Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09987">https://arxiv.org/abs/2508.09987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09987">https://arxiv.org/pdf/2508.09987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09987]] Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation(https://arxiv.org/abs/2508.09987)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recently, GPT-4o has garnered significant attention for its strong performance in image generation, yet open-source models still lag behind. Several studies have explored distilling image data from GPT-4o to enhance open-source models, achieving notable progress. However, a key question remains: given that real-world image datasets already constitute a natural source of high-quality data, why should we use GPT-4o-generated synthetic data? In this work, we identify two key advantages of synthetic images. First, they can complement rare scenarios in real-world datasets, such as surreal fantasy or multi-reference image generation, which frequently occur in user queries. Second, they provide clean and controllable supervision. Real-world data often contains complex background noise and inherent misalignment between text descriptions and image content, whereas synthetic images offer pure backgrounds and long-tailed supervision signals, facilitating more accurate text-to-image alignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale synthetic dataset generated by GPT-4o, harnessing the power of synthetic image data to address blind spots in real-world coverage. Using this dataset, we fine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o. In addition, we propose two new evaluation benchmarks for a more accurate and challenging assessment of image generation capabilities: GenEval++, which increases instruction complexity to mitigate score saturation, and Imagine-Bench, which focuses on evaluating both the understanding and generation of imaginative content. Echo-4o demonstrates strong performance across standard benchmarks. Moreover, applying Echo-4o-Image to other foundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains across multiple metrics, highlighting the datasets strong transferability.</li>
<li><strong>摘要：</strong>最近，GPT-4O因其在图像生成方面的出色表现而引起了极大的关注，但开源模型仍然落后。几项研究探索了从GPT-4O的提炼图像数据，以增强开源模型，从而取得了显着的进步。但是，一个关键问题仍然是：鉴于现实世界图像数据集已经构成了高质量数据的自然来源，为什么我们应该使用GPT-4O生成的合成数据？在这项工作中，我们确定了合成图像的两个关键优势。首先，它们可以补充现实世界数据集中的稀有场景，例如超现实幻想或多参考图像生成，这些图像经常在用户查询中发生。其次，它们提供干净可控制的监督。现实世界中的数据通常包含复杂的背景噪声和文本描述和图像内容之间的固有未对准，而合成图像则提供了纯粹的背景和长尾的监督信号，从而促进了更准确的文本对象对齐。在这些见解的基础上，我们介绍了GPT-4O生成的180k级合成数据集Echo-4O图像，利用合成图像数据的力量来解决现实世界中的盲点。使用此数据集，我们将统一的多模式生成基线百吉饼微调以获得Echo-4O。此外，我们提出了两个新的评估基准，以更加准确，更具挑战性的图像产生能力评估：Geneval ++，这增加了教学复杂性以减轻得分饱和度，并想象基础，该基础侧重于评估对想象力内容的理解和产生。 Echo-4O在标准基准测试中表现出强劲的性能。此外，将Echo-4O图像应用于其他基础模型（例如Omnigen2，blip3-O）可在多个指标上产生一致的性能提高，从而突出了数据集的强转换性。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
