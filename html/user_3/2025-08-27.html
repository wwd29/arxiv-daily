<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-08-27</h1>
<h3>Title: FastAvatar: Instant 3D Gaussian Splatting for Faces from Single Unconstrained Poses</h3>
<ul>
<li><strong>Authors: </strong>Hao Liang, Zhixuan Ge, Ashish Tiwari, Soumendu Majee, G.M. Dilshan Godaliyadda, Ashok Veeraraghavan, Guha Balakrishnan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18389">https://arxiv.org/abs/2508.18389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18389">https://arxiv.org/pdf/2508.18389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18389]] FastAvatar: Instant 3D Gaussian Splatting for Faces from Single Unconstrained Poses(https://arxiv.org/abs/2508.18389)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present FastAvatar, a pose-invariant, feed-forward framework that can generate a 3D Gaussian Splatting (3DGS) model from a single face image from an arbitrary pose in near-instant time (<10ms). FastAvatar uses a novel encoder-decoder neural network design to achieve both fast fitting and identity preservation regardless of input pose. First, FastAvatar constructs a 3DGS face ``template'' model from a training dataset of faces with multi-view captures. Second, FastAvatar encodes the input face image into an identity-specific and pose-invariant latent embedding, and decodes this embedding to predict residuals to the structural and appearance parameters of each Gaussian in the template 3DGS model. By only inferring residuals in a feed-forward fashion, model inference is fast and robust. FastAvatar significantly outperforms existing feed-forward face 3DGS methods (e.g., GAGAvatar) in reconstruction quality, and runs 1000x faster than per-face optimization methods (e.g., FlashAvatar, GaussianAvatars and GASP). In addition, FastAvatar's novel latent space design supports real-time identity interpolation and attribute editing which is not possible with any existing feed-forward 3DGS face generation framework. FastAvatar's combination of excellent reconstruction quality and speed expands the scope of 3DGS for photorealistic avatar applications in consumer and interactive systems.</li>
<li><strong>摘要：</strong>我们提出了FastAvatar，这是一种姿势不变的馈送前框架，可以从单个脸部图像中从任意姿势（<10ms）中从单个脸部图像中产生3D高斯分裂（3DGS）模型。 FastAvatar使用新颖的编码器神经网络设计，无论输入姿势如何，都可以实现快速拟合和身份保存。首先，FastAvatar从具有多视图捕获的面孔的训练数据集中构造了一个3DGS face``模板''模型。其次，FastAvatar将输入面图像编码为特定于身份的和姿势不变的潜在嵌入，并解码此嵌入以预测每个高斯在模板3DGS模型中每个高斯的结构和外观参数的残差。通过仅以馈送方式推断残差，模型推断是快速而稳健的。 FastAvatar在重建质量方面的现有前馈3DGS方法（例如Gagavatar）的表现明显胜过，并且比每面face优化方法快1000倍（例如Flashavatar，Gaussianavatars和GASP）。此外，Fastavatar的新型潜在空间设计还支持实时身份插值和属性编辑，这对于任何现有的Feed-Forward 3DGS面对生成框架都是不可能的。 Fastavatar出色的重建质量和速度的结合扩大了在消费者和交互式系统中的感光化化身应用的3DG范围。</li>
</ul>

<h3>Title: Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?</h3>
<ul>
<li><strong>Authors: </strong>Fatemeh Ziaeetabar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18421">https://arxiv.org/abs/2508.18421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18421">https://arxiv.org/pdf/2508.18421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18421]] Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?(https://arxiv.org/abs/2508.18421)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision foundation models (FMs) have become the predominant architecture in computer vision, providing highly transferable representations learned from large-scale, multimodal corpora. Nonetheless, they exhibit persistent limitations on tasks that require explicit reasoning over entities, roles, and spatio-temporal relations. Such relational competence is indispensable for fine-grained human activity recognition, egocentric video understanding, and multimodal medical image analysis, where spatial, temporal, and semantic dependencies are decisive for performance. We advance the position that next-generation FMs should incorporate explicit relational interfaces, instantiated as dynamic relational graphs (graphs whose topology and edge semantics are inferred from the input and task context). We illustrate this position with cross-domain evidence from recent systems in human manipulation action recognition and brain tumor segmentation, showing that augmenting FMs with lightweight, context-adaptive graph-reasoning modules improves fine-grained semantic fidelity, out of distribution robustness, interpretability, and computational efficiency relative to FM only baselines. Importantly, by reasoning sparsely over semantic nodes, such hybrids also achieve favorable memory and hardware efficiency, enabling deployment under practical resource constraints. We conclude with a targeted research agenda for FM graph hybrids, prioritizing learned dynamic graph construction, multi-level relational reasoning (e.g., part object scene in activity understanding, or region organ in medical imaging), cross-modal fusion, and evaluation protocols that directly probe relational competence in structured vision tasks.</li>
<li><strong>摘要：</strong>视觉基础模型（FMS）已成为计算机视觉中的主要体系结构，提供了从大规模的多模式语料库中学到的高度可转移的表示形式。但是，它们对需要对实体，角色和时空关系的任务表现出持续的局限性。这种关系能力对于精细的人类活动识别，以自我为中心的视频理解以及多模式医学图像分析是必不可少的，其中空间，时间和语义依赖性对性能是决定性的。我们推进了下一代FMS应该合并明确的关系接口的立场，即作为动态关系图实例化（从输入和任务上下文中推断出其拓扑和边缘语义的图形）。我们用最近系统的跨域证据来说明人类操纵作用识别和脑肿瘤分割的跨域证据，表明使用轻量轻巧的，上下文自适应的图形 - 复原模块可以增强FMS，可提高相对于FM唯一的基准，提高了精细的语义效果，不超出分布稳健性，可解释性和计算效率。重要的是，通过对语义节点进行稀疏推理，此类混合动力车也可以实现有利的内存和硬件效率，从而在实际的资源限制下实现了部署。我们以针对FM图杂种的有针对性的研究议程结束，优先考虑学习的动态图构建，多级关系推理（例如，活动理解中的部分对象场景，或医学成像中的区域器官），跨模式融合中的区域器官以及在结构视觉任务中直接探测相关能力的评估协议。</li>
</ul>

<h3>Title: LPLC: A Dataset for License Plate Legibility Classification</h3>
<ul>
<li><strong>Authors: </strong>Lucas Wojcik, Gabriel E. Lima, Valfride Nascimento, Eduil Nascimento Jr., Rayson Laroca, David Menotti</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18425">https://arxiv.org/abs/2508.18425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18425">https://arxiv.org/pdf/2508.18425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18425]] LPLC: A Dataset for License Plate Legibility Classification(https://arxiv.org/abs/2508.18425)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Automatic License Plate Recognition (ALPR) faces a major challenge when dealing with illegible license plates (LPs). While reconstruction methods such as super-resolution (SR) have emerged, the core issue of recognizing these low-quality LPs remains unresolved. To optimize model performance and computational efficiency, image pre-processing should be applied selectively to cases that require enhanced legibility. To support research in this area, we introduce a novel dataset comprising 10,210 images of vehicles with 12,687 annotated LPs for legibility classification (the LPLC dataset). The images span a wide range of vehicle types, lighting conditions, and camera/image quality levels. We adopt a fine-grained annotation strategy that includes vehicle- and LP-level occlusions, four legibility categories (perfect, good, poor, and illegible), and character labels for three categories (excluding illegible LPs). As a benchmark, we propose a classification task using three image recognition networks to determine whether an LP image is good enough, requires super-resolution, or is completely unrecoverable. The overall F1 score, which remained below 80% for all three baseline models (ViT, ResNet, and YOLO), together with the analyses of SR and LP recognition methods, highlights the difficulty of the task and reinforces the need for further research. The proposed dataset is publicly available at this https URL.</li>
<li><strong>摘要：</strong>自动车牌识别（ALPR）在处理难以辨认的车牌（LPS）时面临重大挑战。尽管已经出现了诸如超分辨率（SR）之类的重建方法，但识别这些低质量LP的核心问题仍未解决。为了优化模型性能和计算效率，应将图像预处理选择性地应用于需要增强可透性的情况。为了支持该领域的研究，我们介绍了一个新颖的数据集，其中包括10,210张带有12,687个注释LPS的车辆图像，用于清晰分类（LPLC数据集）。这些图像涵盖了广泛的车辆类型，照明条件和相机/图像质量水平。我们采用了一种细粒度的注释策略，其中包括车辆和LP级的遮挡，四个可读性类别（完美，良好，贫穷和难以辨认）以及三个类别（不可辨认的LPS）的角色标签。作为基准，我们建议使用三个图像识别网络的分类任务来确定LP图像是否足够好，需要超分辨率或完全无法恢复。对于所有三种基线模型（VIT，RESNET和YOLO）的总体F1分数以及对SR和LP识别方法的分析，突出了任务的困难，并增强了进一步研究的需求。所提出的数据集可在此HTTPS URL上公开获得。</li>
</ul>

<h3>Title: VQualA 2025 Challenge on Face Image Quality Assessment: Methods and Results</h3>
<ul>
<li><strong>Authors: </strong>Sizhuo Ma, Wei-Ting Chen, Qiang Gao, Jian Wang, Chris Wei Zhou, Wei Sun, Weixia Zhang, Linhan Cao, Jun Jia, Xiangyang Zhu, Dandan Zhu, Xiongkuo Min, Guangtao Zhai, Baoying Chen, Xiongwei Xiao, Jishen Zeng, Wei Wu, Tiexuan Lou, Yuchen Tan, Chunyi Song, Zhiwei Xu, MohammadAli Hamidi, Hadi Amirpour, Mingyin Bai, Jiawang Du, Zhenyu Jiang, Zilong Lu, Ziguan Cui, Zongliang Gan, Xinpeng Li, Shiqi Jiang, Chenhui Li, Changbo Wang, Weijun Yuan, Zhan Li, Yihang Chen, Yifan Deng, Ruting Deng, Zhanglu Chen, Boyang Yao, Shuling Zheng, Feng Zhang, Zhiheng Fu, Abhishek Joshi, Aman Agarwal, Rakhil Immidisetti, Ajay Narasimha Mopidevi, Vishwajeet Shukla, Hao Yang, Ruikun Zhang, Liyuan Pan, Kaixin Deng, Hang Ouyang, Fan yang, Zhizun Luo, Zhuohang Shi, Songning Lai, Weilin Ruan, Yutao Yue</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18445">https://arxiv.org/abs/2508.18445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18445">https://arxiv.org/pdf/2508.18445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18445]] VQualA 2025 Challenge on Face Image Quality Assessment: Methods and Results(https://arxiv.org/abs/2508.18445)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Face images play a crucial role in numerous applications; however, real-world conditions frequently introduce degradations such as noise, blur, and compression artifacts, affecting overall image quality and hindering subsequent tasks. To address this challenge, we organized the VQualA 2025 Challenge on Face Image Quality Assessment (FIQA) as part of the ICCV 2025 Workshops. Participants created lightweight and efficient models (limited to 0.5 GFLOPs and 5 million parameters) for the prediction of Mean Opinion Scores (MOS) on face images with arbitrary resolutions and realistic degradations. Submissions underwent comprehensive evaluations through correlation metrics on a dataset of in-the-wild face images. This challenge attracted 127 participants, with 1519 final submissions. This report summarizes the methodologies and findings for advancing the development of practical FIQA approaches.</li>
<li><strong>摘要：</strong>面部图像在众多应用中起着至关重要的作用。但是，现实世界中的条件经常引入退化，例如噪声，模糊和压缩工件，影响整体图像质量和阻碍后续任务。为了应对这一挑战，我们组织了VQUALA 2025面对图像质量评估（FIQA）的挑战，作为ICCV 2025讲习班的一部分。参与者创建了轻巧有效的模型（限制为0.5 GFLOPS和500万参数），以预测具有任意分辨率和现实降解的面部图像上的平均意见分数（MOS）。提交通过在野外面部图像的数据集中通过相关指标进行了全面评估。这项挑战吸引了127名参与者，并提交了1519个最终提交。本报告总结了推进实际FIQA方法发展的方法和发现。</li>
</ul>

<h3>Title: VERIRL: Boosting the LLM-based Verilog Code Generation via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Fu Teng, Miao Pan, Xuhong Zhang, Zhezhi He, Yiyao Yang, Xinyi Chai, Mengnan Qi, Liqiang Lu, Jianwei Yin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18462">https://arxiv.org/abs/2508.18462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18462">https://arxiv.org/pdf/2508.18462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18462]] VERIRL: Boosting the LLM-based Verilog Code Generation via Reinforcement Learning(https://arxiv.org/abs/2508.18462)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advancements in code generation have shown remarkable success across software domains, yet hardware description languages (HDLs) such as Verilog remain underexplored due to their concurrency semantics, syntactic rigidity, and simulation complexity. In this work, we address these challenges by introducing a reinforcement learning (RL) framework tailored for Verilog code generation. We first construct Veribench-53K, a high-quality dataset curated from over 700K Verilog problems, enriched with structured prompts, complexity labels, and diverse testbenches. To tackle the problem of sparse and noisy reward signals, we propose a Trace-back based Rescore mechanism that leverages reasoning paths and iterative refinement to enhance feedback reliability and support reward model training. Furthermore, to mitigate catastrophic forgetting and overfitting during RL fine-tuning, we introduce a sample-balanced weighting strategy that adaptively balances learning dynamics based on reward-probability distributions. These innovations are integrated into an iterative RL pipeline that co-evolves the policy and reward models. In contrast to recent work such as CraftRTL, which relies on large-scale closed-source model distillation, and DeepSeek-style approaches that struggle with sparse feedback, our method demonstrates superior performance using a smaller but high-quality dataset combined with RL optimization. Experiments on Verilog generation tasks demonstrate state-of-the-art performance, with substantial gains in test pass rate, functional correctness, and compilation robustness. Our findings highlight the potential of RL-driven approaches for structured code generation in hardware-centric domains. VERIRL is publicly available at this https URL.</li>
<li><strong>摘要：</strong>代码生成的最新进展显示了整个软件域的取得了巨大的成功，但是由于其并发语义，句法刚度和仿真复杂性，硬件说明语言（HDL）（HDL）仍然没有充满反感。在这项工作中，我们通过引入为Verilog代码生成量身定制的强化学习（RL）框架来解决这些挑战。我们首先构建了Veribench-53K，这是一种由超过700k Verilog问题策划的高质量数据集，并具有结构化提示，复杂性标签和不同的测试台。为了解决稀疏和嘈杂的奖励信号的问题，我们提出了一种基于痕迹的救援机制，该机制利用推理路径和迭代性改进来增强反馈可靠性和支持奖励模型培训。此外，为了减轻RL微调期间的灾难性遗忘和过度拟合，我们引入了一种样本平衡的加权策略，该策略可以根据奖励概论分布适应地平衡学习动态。这些创新被整合到迭代的RL管道中，该管道共同发展了政策和奖励模型。与最近的工作（例如CrafTrTL）相反，CrafTRTL依赖于大规模的封闭源模型蒸馏，而DeepSeek风格的方法则与稀疏反馈抗争，我们的方法证明了使用较小但高质量的数据集结合了RL优化的较小但高质量的数据集。对Verilog生成任务的实验表明了最先进的性能，并具有大量的测试率，功能正确性和汇编鲁棒性。我们的发现突出了RL驱动方法在以硬件为中心的域中生成结构化代码的潜力。 Verirl在此HTTPS URL上公开可用。</li>
</ul>

<h3>Title: SAT-SKYLINES: 3D Building Generation from Satellite Imagery and Coarse Geometric Priors</h3>
<ul>
<li><strong>Authors: </strong>Zhangyu Jin, Andrew Feng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18531">https://arxiv.org/abs/2508.18531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18531">https://arxiv.org/pdf/2508.18531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18531]] SAT-SKYLINES: 3D Building Generation from Satellite Imagery and Coarse Geometric Priors(https://arxiv.org/abs/2508.18531)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present SatSkylines, a 3D building generation approach that takes satellite imagery and coarse geometric priors. Without proper geometric guidance, existing image-based 3D generation methods struggle to recover accurate building structures from the top-down views of satellite images alone. On the other hand, 3D detailization methods tend to rely heavily on highly detailed voxel inputs and fail to produce satisfying results from simple priors such as cuboids. To address these issues, our key idea is to model the transformation from interpolated noisy coarse priors to detailed geometries, enabling flexible geometric control without additional computational cost. We have further developed Skylines-50K, a large-scale dataset of over 50,000 unique and stylized 3D building assets in order to support the generations of detailed building models. Extensive evaluations indicate the effectiveness of our model and strong generalization ability.</li>
<li><strong>摘要：</strong>我们提出了Satskylines，这是一种3D建筑生成的方法，它采用卫星图像和粗糙的几何先验。如果没有适当的几何指导，现有的基于图像的3D生成方法就难以仅凭卫星图像的自上而下的视图来恢复准确的建筑结构。另一方面，3D详细方法倾向于严重依赖高度详细的体素输入，并且无法从诸如Cuboid之类的简单先验中产生令人满意的结果。为了解决这些问题，我们的关键思想是建模从插值嘈杂的粗略先验到详细的几何形状的转换，从而实现灵活的几何控制，而无需额外的计算成本。我们进一步开发了Skylines-50k，这是一个大规模的数据集，拥有超过50,000个独特和风格的3D建筑资产，以支持几代详细的建筑模型。广泛的评估表明我们的模型和强大的概括能力的有效性。</li>
</ul>

<h3>Title: Enhancing Chemical Explainability Through Counterfactual Masking</h3>
<ul>
<li><strong>Authors: </strong>Łukasz Janisiów, Marek Kochańczyk, Bartosz Zieliński, Tomasz Danel</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18561">https://arxiv.org/abs/2508.18561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18561">https://arxiv.org/pdf/2508.18561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18561]] Enhancing Chemical Explainability Through Counterfactual Masking(https://arxiv.org/abs/2508.18561)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Molecular property prediction is a crucial task that guides the design of new compounds, including drugs and materials. While explainable artificial intelligence methods aim to scrutinize model predictions by identifying influential molecular substructures, many existing approaches rely on masking strategies that remove either atoms or atom-level features to assess importance via fidelity metrics. These methods, however, often fail to adhere to the underlying molecular distribution and thus yield unintuitive explanations. In this work, we propose counterfactual masking, a novel framework that replaces masked substructures with chemically reasonable fragments sampled from generative models trained to complete molecular graphs. Rather than evaluating masked predictions against implausible zeroed-out baselines, we assess them relative to counterfactual molecules drawn from the data distribution. Our method offers two key benefits: (1) molecular realism underpinning robust and distribution-consistent explanations, and (2) meaningful counterfactuals that directly indicate how structural modifications may affect predicted properties. We demonstrate that counterfactual masking is well-suited for benchmarking model explainers and yields more actionable insights across multiple datasets and property prediction tasks. Our approach bridges the gap between explainability and molecular design, offering a principled and generative path toward explainable machine learning in chemistry.</li>
<li><strong>摘要：</strong>分子性质预测是指导新化合物（包括药物和材料）设计的至关重要的任务。尽管可解释的人工智能方法旨在通过识别有影响力的分子子结构来审查模型预测，但许多现有方法依赖于掩盖策略，这些策略可以消除原子或原子级特征来通过保真度指标评估重要性。但是，这些方法通常无法遵守潜在的分子分布，因此产生了不直觉的解释。在这项工作中，我们提出了反事实掩蔽，这是一个新型框架，将掩盖的子结构用化学合理的片段取代，这些片段是从经过训练以完成分子图的生成模型中采样的。我们没有评估针对难以置信的零基线的掩盖预测，而是相对于从数据分布中得出的反事实分子进行评估。我们的方法提供了两个关键的好处：（1）分子现实主义是强大和分配一致的解释的基础，以及（2）有意义的反事实，直接表明结构修饰如何影响预测的性质。我们证明，反事实掩蔽非常适合基准测试模型解释器，并在多个数据集和属性预测任务上产生更可行的见解。我们的方法弥合了解释性和分子设计之间的差距，为化学中的可解释机器学习提供了有原则的生成途径。</li>
</ul>

<h3>Title: History Rhymes: Accelerating LLM Reinforcement Learning with RhymeRL</h3>
<ul>
<li><strong>Authors: </strong>Jingkai He, Tianjian Li, Erhu Feng, Dong Du, Qian Liu, Tao Liu, Yubin Xia, Haibo Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18588">https://arxiv.org/abs/2508.18588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18588">https://arxiv.org/pdf/2508.18588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18588]] History Rhymes: Accelerating LLM Reinforcement Learning with RhymeRL(https://arxiv.org/abs/2508.18588)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of large language models (LLMs), reinforcement learning (RL) has emerged as a pivotal methodology for enhancing the reasoning capabilities of LLMs. Unlike traditional pre-training approaches, RL encompasses multiple stages: rollout, reward, and training, which necessitates collaboration among various worker types. However, current RL systems continue to grapple with substantial GPU underutilization, due to two primary factors: (1) The rollout stage dominates the overall RL process due to test-time scaling; (2) Imbalances in rollout lengths (within the same batch) result in GPU bubbles. While prior solutions like asynchronous execution and truncation offer partial relief, they may compromise training accuracy for efficiency. Our key insight stems from a previously overlooked observation: rollout responses exhibit remarkable similarity across adjacent training epochs. Based on the insight, we introduce RhymeRL, an LLM RL system designed to accelerate RL training with two key innovations. First, to enhance rollout generation, we present HistoSpec, a speculative decoding inference engine that utilizes the similarity of historical rollout token sequences to obtain accurate drafts. Second, to tackle rollout bubbles, we introduce HistoPipe, a two-tier scheduling strategy that leverages the similarity of historical rollout distributions to balance workload among rollout workers. We have evaluated RhymeRL within a real production environment, demonstrating scalability from dozens to thousands of GPUs. Experimental results demonstrate that RhymeRL achieves a 2.6x performance improvement over existing methods, without compromising accuracy or modifying the RL paradigm.</li>
<li><strong>摘要：</strong>随着大语言模型（LLM）的快速发展，增强学习（RL）已成为增强LLMS推理能力的关键方法。与传统的培训方法不同，RL包括多个阶段：推出，奖励和培训，这需要在各种工人类型之间进行协作。但是，由于两个主要因素，当前的RL系统继续努力应对大量的GPU未充分利用：（1）由于测试时间缩放，推出阶段主导了整体RL过程； （2）推出长度的不平衡（在同一批次内）导致GPU气泡。尽管异步执行和截断等先前的解决方案可以部分缓解，但它们可能会损害培训准确性以提高效率。我们的主要见解源于以前被忽视的观察：推出响应在相邻训练时期表现出显着的相似性。根据洞察力，我们引入了Rhymerl，这是一种LLM RL系统，旨在通过两项关键创新加速RL培训。首先，为了增强推出生成，我们提出了HistoSpec，这是一种投机解码引擎，利用历史推广令牌序列的相似性来获得准确的草稿。其次，为了解决推出泡沫，我们介绍了HistoPipe，这是一种两层调度策略，利用历史推出分布的相似性来平衡推出工人之间的工作量。我们已经在实际生产环境中评估了押韵，证明了从数十个GPU的可伸缩性。实验结果表明，押韵对现有方法的性能提高了2.6倍，而不会损害准确性或修改RL范式。</li>
</ul>

<h3>Title: Wan-S2V: Audio-Driven Cinematic Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, Dechao Meng, Jinwei Qi, Penchong Qiao, Zhen Shen, Yafei Song, Ke Sun, Linrui Tian, Guangyuan Wang, Qi Wang, Zhongjian Wang, Jiayu Xiao, Sheng Xu, Bang Zhang, Peng Zhang, Xindi Zhang, Zhe Zhang, Jingren Zhou, Lian Zhuo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18621">https://arxiv.org/abs/2508.18621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18621">https://arxiv.org/pdf/2508.18621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18621]] Wan-S2V: Audio-Driven Cinematic Video Generation(https://arxiv.org/abs/2508.18621)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Current state-of-the-art (SOTA) methods for audio-driven character animation demonstrate promising performance for scenarios primarily involving speech and singing. However, they often fall short in more complex film and television productions, which demand sophisticated elements such as nuanced character interactions, realistic body movements, and dynamic camera work. To address this long-standing challenge of achieving film-level character animation, we propose an audio-driven model, which we refere to as Wan-S2V, built upon Wan. Our model achieves significantly enhanced expressiveness and fidelity in cinematic contexts compared to existing approaches. We conducted extensive experiments, benchmarking our method against cutting-edge models such as Hunyuan-Avatar and Omnihuman. The experimental results consistently demonstrate that our approach significantly outperforms these existing solutions. Additionally, we explore the versatility of our method through its applications in long-form video generation and precise video lip-sync editing.</li>
<li><strong>摘要：</strong>音频驱动的角色动画的当前最新方法（SOTA）方法表明了主要涉及语音和唱歌的场景表现。但是，它们通常在更复杂的电影和电视作品中跌落，这些电影需要复杂的元素，例如细微的角色相互作用，现实的身体运动和动态的相机作品。为了应对实现电影级角色动画的长期挑战，我们提出了一个音频驱动的模型，我们将其作为WAN建立的WAN-S2V进行调查。与现有方法相比，我们的模型在电影背景下的表现力和忠诚度显着提高。我们进行了广泛的实验，对我们的方法进行了针对诸如Hunyuan-Avatar和Omnihuman之类的尖端模型的方法。实验结果始终表明，我们的方法显着优于这些现有解决方案。此外，我们通过在长篇视频生成和精确的视频唇部同步编辑中的应用中探索了我们的方法的多功能性。</li>
</ul>

<h3>Title: ROSE: Remove Objects with Side Effects in Videos</h3>
<ul>
<li><strong>Authors: </strong>Chenxuan Miao, Yutong Feng, Jianshu Zeng, Zixiang Gao, Hantang Liu, Yunfeng Yan, Donglian Qi, Xi Chen, Bin Wang, Hengshuang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18633">https://arxiv.org/abs/2508.18633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18633">https://arxiv.org/pdf/2508.18633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18633]] ROSE: Remove Objects with Side Effects in Videos(https://arxiv.org/abs/2508.18633)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Video object removal has achieved advanced performance due to the recent success of video generative models. However, when addressing the side effects of objects, e.g., their shadows and reflections, existing works struggle to eliminate these effects for the scarcity of paired video data as supervision. This paper presents ROSE, termed Remove Objects with Side Effects, a framework that systematically studies the object's effects on environment, which can be categorized into five common cases: shadows, reflections, light, translucency and mirror. Given the challenges of curating paired videos exhibiting the aforementioned effects, we leverage a 3D rendering engine for synthetic data generation. We carefully construct a fully-automatic pipeline for data preparation, which simulates a large-scale paired dataset with diverse scenes, objects, shooting angles, and camera trajectories. ROSE is implemented as an video inpainting model built on diffusion transformer. To localize all object-correlated areas, the entire video is fed into the model for reference-based erasing. Moreover, additional supervision is introduced to explicitly predict the areas affected by side effects, which can be revealed through the differential mask between the paired videos. To fully investigate the model performance on various side effect removal, we presents a new benchmark, dubbed ROSE-Bench, incorporating both common scenarios and the five special side effects for comprehensive evaluation. Experimental results demonstrate that ROSE achieves superior performance compared to existing video object erasing models and generalizes well to real-world video scenarios. The project page is this https URL.</li>
<li><strong>摘要：</strong>由于视频生成模型的最新成功，视频对象的删除已取得了高级性能。但是，当解决对象的副作用（例如它们的阴影和反射）时，现有作品难以消除这些效果，以至于缺乏配对的视频数据作为监督。本文介绍了玫瑰，称为删除副作用的物体，该框架系统地研究对象对环境的影响，可以将其分为五种常见情况：阴影，反射，光，光透明和镜像。鉴于策划配对视频表现出上述效果的挑战，我们利用3D渲染引擎来生成合成数据。我们仔细构建了一条完全自动的管道来进行数据准备，该管道模拟了一个大型配对数据集，这些数据集具有不同的场景，对象，射击角度和摄像头轨迹。 Rose被用作建立在扩散变压器上的视频介绍模型。为了本地化所有与对象相关的区域，整个视频都被馈入模型以进行基于参考的擦除。此外，引入了其他监督以明确预测受副作用影响的区域，可以通过配对视频之间的差分掩模来揭示这些区域。为了充分研究各种副作用去除的模型性能，我们提出了一个新的基准，称为玫瑰板，既包含了常见的场景和五个特殊副作用，又可以进行全面评估。实验结果表明，与现有的视频对象擦除模型相比，Rose的性能表现出色，并将其推广到现实世界的视频方案。项目页面是此HTTPS URL。</li>
</ul>

<h3>Title: OwlCap: Harmonizing Motion-Detail for Video Captioning via HMD-270K and Caption Set Equivalence Reward</h3>
<ul>
<li><strong>Authors: </strong>Chunlin Zhong, Qiuxia Hou, Zhangjun Zhou, Shuang Hao, Haonan Lu, Yanhao Zhang, He Tang, Xiang Bai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18634">https://arxiv.org/abs/2508.18634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18634">https://arxiv.org/pdf/2508.18634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18634]] OwlCap: Harmonizing Motion-Detail for Video Captioning via HMD-270K and Caption Set Equivalence Reward(https://arxiv.org/abs/2508.18634)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Video captioning aims to generate comprehensive and coherent descriptions of the video content, contributing to the advancement of both video understanding and generation. However, existing methods often suffer from motion-detail imbalance, as models tend to overemphasize one aspect while neglecting the other. This imbalance results in incomplete captions, which in turn leads to a lack of consistency in video understanding and generation. To address this issue, we propose solutions from two aspects: 1) Data aspect: We constructed the Harmonizing Motion-Detail 270K (HMD-270K) dataset through a two-stage pipeline: Motion-Detail Fusion (MDF) and Fine-Grained Examination (FGE). 2) Optimization aspect: We introduce the Caption Set Equivalence Reward (CSER) based on Group Relative Policy Optimization (GRPO). CSER enhances completeness and accuracy in capturing both motion and details through unit-to-set matching and bidirectional validation. Based on the HMD-270K supervised fine-tuning and GRPO post-training with CSER, we developed OwlCap, a powerful video captioning multi-modal large language model (MLLM) with motion-detail balance. Experimental results demonstrate that OwlCap achieves significant improvements compared to baseline models on two benchmarks: the detail-focused VDC (+4.2 Acc) and the motion-focused DREAM-1K (+4.6 F1). The HMD-270K dataset and OwlCap model will be publicly released to facilitate video captioning research community advancements.</li>
<li><strong>摘要：</strong>视频字幕旨在生成视频内容的全面和连贯的描述，从而促进视频理解和发电的发展。但是，现有方法通常会遭受运动范围的失衡，因为模型倾向于过分强调一个方面，而忽略了另一个方面。这种不平衡导致标题不完整，这又导致视频理解和发电缺乏一致性。为了解决这个问题，我们从两个方面提出了解决方案：1）数据方面：我们通过两阶段的管道构建了协调的运动范围270K（HMD-270K）数据集：运动范围范围：动作范围频道融合（MDF）和细粒度检查（FGE）。 2）优化方面：我们基于组相对策略优化（GRPO）介绍了标题集的等价奖励（CSER）。 CSER通过单位对匹配和双向验证来捕获运动和细节的完整性和准确性。基于CSER的HMD-270K监督微调和GRPO训练后，我们开发了OWLCAP，这是一个有力的视频字幕的多模式大型语言模型（MLLM），并具有动作范围的平衡。实验结果表明，与两个基准上的基线模型相比，OWLCAP取得了重大改进：以细节为中心的VDC（+4.2 ACC）和以运动为中心的Dream-1K（+4.6 F1）。 HMD-270K数据集和OWLCAP模型将公开发布，以促进视频字幕研究社区的进步。</li>
</ul>

<h3>Title: Biologically Disentangled Multi-Omic Modeling Reveals Mechanistic Insights into Pan-Cancer Immunotherapy Resistance</h3>
<ul>
<li><strong>Authors: </strong>Ifrah Tariq, Ernest Fraenkel</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18638">https://arxiv.org/abs/2508.18638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18638">https://arxiv.org/pdf/2508.18638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18638]] Biologically Disentangled Multi-Omic Modeling Reveals Mechanistic Insights into Pan-Cancer Immunotherapy Resistance(https://arxiv.org/abs/2508.18638)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Immune checkpoint inhibitors (ICIs) have transformed cancer treatment, yet patient responses remain highly variable, and the biological mechanisms underlying resistance are poorly understood. While machine learning models hold promise for predicting responses to ICIs, most existing methods lack interpretability and do not effectively leverage the biological structure inherent to multi-omics data. Here, we introduce the Biologically Disentangled Variational Autoencoder (BDVAE), a deep generative model that integrates transcriptomic and genomic data through modality- and pathway-specific encoders. Unlike existing rigid, pathway-informed models, BDVAE employs a modular encoder architecture combined with variational inference to learn biologically meaningful latent features associated with immune, genomic, and metabolic processes. Applied to a pan-cancer cohort of 366 patients across four cancer types treated with ICIs, BDVAE accurately predicts treatment response (AUC-ROC = 0.94 on unseen test data) and uncovers critical resistance mechanisms, including immune suppression, metabolic shifts, and neuronal signaling. Importantly, BDVAE reveals that resistance spans a continuous biological spectrum rather than strictly binary states, reflecting gradations of tumor dysfunction. Several latent features correlate with survival outcomes and known clinical subtypes, demonstrating BDVAE's capability to generate interpretable, clinically relevant insights. These findings underscore the value of biologically structured machine learning in elucidating complex resistance patterns and guiding precision immunotherapy strategies.</li>
<li><strong>摘要：</strong>免疫检查点抑制剂（ICIS）已转化为癌症治疗，但患者反应仍然很大，并且对耐药性的生物学机制知之甚少。尽管机器学习模型有望预测对ICI的响应的希望，但大多数现有方法缺乏可解释性，并且不能有效利用多摩变数据固有的生物结构。在这里，我们介绍了生物学分解的变分自动编码器（BDVAE），这是一个深层生成模型，该模型通过模态和途径特异性的编码器整合了转录组和基因组数据。与现有的刚性模型不同，BDVAE采用模块化编码器结构，并结合了各种推理，以学习与免疫，基因组和代谢过程相关的生物学意义的潜在特征。 BDVAE应用于四种用ICI治疗的四种癌症类型的366例患者组成的泛伴侣队列，可以准确预测治疗反应（在看不见的测试数据上AUC-ROC = 0.94），并发现关键的抗药性机制，包括免疫抑制，代谢移位和神经元信号传导。重要的是，BDVAE揭示了抗性跨越连续的生物学光谱，而不是严格的二元状态，反映了肿瘤功能障碍的等级。几种潜在特征与生存结果和已知的临床亚型相关，这表明BDVAE能够产生可解释的临床相关见解。这些发现强调了生物结构化机器学习在阐明复杂抗性模式和指导精确免疫疗法策略中的价值。</li>
</ul>

<h3>Title: Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vectorized Drawings</h3>
<ul>
<li><strong>Authors: </strong>Feiwei Qin, Shichao Lu, Junhao Hou, Changmiao Wang, Meie Fang, Ligang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18733">https://arxiv.org/abs/2508.18733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18733">https://arxiv.org/pdf/2508.18733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18733]] Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vectorized Drawings(https://arxiv.org/abs/2508.18733)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Computer-Aided Design (CAD) generative modeling is driving significant innovations across industrial applications. Recent works have shown remarkable progress in creating solid models from various inputs such as point clouds, meshes, and text descriptions. However, these methods fundamentally diverge from traditional industrial workflows that begin with 2D engineering drawings. The automatic generation of parametric CAD models from these 2D vector drawings remains underexplored despite being a critical step in engineering design. To address this gap, our key insight is to reframe CAD generation as a sequence-to-sequence learning problem where vector drawing primitives directly inform the generation of parametric CAD operations, preserving geometric precision and design intent throughout the transformation process. We propose Drawing2CAD, a framework with three key technical components: a network-friendly vector primitive representation that preserves precise geometric information, a dual-decoder transformer architecture that decouples command type and parameter generation while maintaining precise correspondence, and a soft target distribution loss function accommodating inherent flexibility in CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing, a dataset of paired engineering drawings and parametric CAD models, and conduct thorough experiments to demonstrate the effectiveness of our method. Code and dataset are available at this https URL.</li>
<li><strong>摘要：</strong>计算机辅助设计（CAD）生成建模正在推动工业应用的重大创新。最近的作品在从各种输入（例如点云，网格和文本描述）中创建固体模型方面显示出了显着的进步。但是，这些方法从根本上与传统的工业工作流程不同，这些工业工作流程从2D工程图纸开始。尽管这是工程设计的关键一步，但这些2D矢量图中的参数CAD模型的自动生成仍未得到充实。为了解决这一差距，我们的关键见解是将CAD生成重新构成序列学习问题，其中矢量绘制原始素直接为参数CAD操作的生成提供了信息，从而保留了整个转换过程中的几何精度和设计意图。我们提出了Draws2CAD，这是一个具有三个关键技术组件的框架：一个网络友好的向量原始表示形式，可保留精确的几何信息，一种双键编码器变压器架构，该架构可以解除命令类型和参数生成，同时保持精确的对应关系，以及可容纳CAD参数中固有的灵活性的软目标分配损失功能。为了训练和评估Drawing2CAD，我们创建了CAD-VGDRAWING，这是配对工程图和参数CAD模型的数据集，并进行彻底的实验以证明我们方法的有效性。代码和数据集可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Rethinking Human-Object Interaction Evaluation for both Vision-Language Models and HOI-Specific Methods</h3>
<ul>
<li><strong>Authors: </strong>Qinqian Lei, Bo Wang, Robby T. Tan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18753">https://arxiv.org/abs/2508.18753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18753">https://arxiv.org/pdf/2508.18753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18753]] Rethinking Human-Object Interaction Evaluation for both Vision-Language Models and HOI-Specific Methods(https://arxiv.org/abs/2508.18753)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Prior human-object interaction (HOI) detection methods have integrated early vision-language models (VLMs) such as CLIP, but only as supporting components within their frameworks. In contrast, recent advances in large, generative VLMs suggest that these models may already possess strong ability to understand images involving HOI. This naturally raises an important question: can general-purpose standalone VLMs effectively solve HOI detection, and how do they compare with specialized HOI methods? Answering this requires a benchmark that can accommodate both paradigms. However, existing HOI benchmarks such as HICO-DET were developed before the emergence of modern VLMs, and their evaluation protocols require exact matches to annotated HOI classes. This is poorly aligned with the generative nature of VLMs, which often yield multiple valid interpretations in ambiguous cases. For example, a static image may capture a person mid-motion with a frisbee, which can plausibly be interpreted as either "throwing" or "catching". When only "catching" is annotated, the other, though equally plausible for the image, is marked incorrect when exact matching is used. As a result, correct predictions might be penalized, affecting both VLMs and HOI-specific methods. To avoid penalizing valid predictions, we introduce a new benchmark that reformulates HOI detection as a multiple-answer multiple-choice task, where each question includes only ground-truth positive options and a curated set of negatives that are constructed to reduce ambiguity (e.g., when "catching" is annotated, "throwing" is not selected as a negative to avoid penalizing valid predictions). The proposed evaluation protocol is the first of its kind for both VLMs and HOI methods, enabling direct comparison and offering new insight into the current state of progress in HOI understanding.</li>
<li><strong>摘要：</strong>先前的人类对象相互作用（HOI）检测方法已集成了早期视觉模型（VLM），例如剪辑，但仅作为其框架中的支持组件。相比之下，最新生成的VLM的最新进展表明，这些模型可能已经具有强大的能力来理解涉及HOI的图像。这自然提出了一个重要的问题：通用独立的VLM可以有效地求解HOI检测，以及它们如何与专门的HOI方法进行比较？回答这需要一个可以容纳这两个范式的基准。但是，现有的HOI基准（例如HICO-DET）是在现代VLM出现之前开发的，其评估协议需要与带注释的HOI类别相匹配。这与VLM的生成性质一致，这通常在模棱两可的情况下产生多种有效的解释。例如，静态图像可以用飞盘捕获一个中的人，可以合理地将其解释为“投掷”或“捕捉”。当仅对“捕获”注释时，当使用精确匹配时，另一个虽然对图像同样合理，但标记为不正确。结果，正确的预测可能会受到惩罚，从而影响VLM和HOI特异性方法。为了避免惩罚有效的预测，我们引入了一个新的基准测试，将HOI检测重新定义为多种选择的多项选择任务，其中每个问题仅包括基本真相的积极选择和一组策划的负面因素，这些构建以减少歧义（例如，当“捕捉”时，throw thort throw tht throw tht throw s to not far nak no nak noal for nail有效预测）。提出的评估协议是VLM和HOI方法的第一个此类评估协议，从而可以直接比较并为HOI理解中的当前进度状态提供新的见解。</li>
</ul>

<h3>Title: Governance-as-a-Service: A Multi-Agent Framework for AI System Compliance and Policy Enforcement</h3>
<ul>
<li><strong>Authors: </strong>Helen Pervez, Suyash Gaurav, Jukka Heikkonen, Jatin Chaudhary</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18765">https://arxiv.org/abs/2508.18765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18765">https://arxiv.org/pdf/2508.18765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18765]] Governance-as-a-Service: A Multi-Agent Framework for AI System Compliance and Policy Enforcement(https://arxiv.org/abs/2508.18765)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>As AI systems evolve into distributed ecosystems with autonomous execution, asynchronous reasoning, and multi-agent coordination, the absence of scalable, decoupled governance poses a structural risk. Existing oversight mechanisms are reactive, brittle, and embedded within agent architectures, making them non-auditable and hard to generalize across heterogeneous deployments. We introduce Governance-as-a-Service (GaaS): a modular, policy-driven enforcement layer that regulates agent outputs at runtime without altering model internals or requiring agent cooperation. GaaS employs declarative rules and a Trust Factor mechanism that scores agents based on compliance and severity-weighted violations. It enables coercive, normative, and adaptive interventions, supporting graduated enforcement and dynamic trust modulation. To evaluate GaaS, we conduct three simulation regimes with open-source models (LLaMA3, Qwen3, DeepSeek-R1) across content generation and financial decision-making. In the baseline, agents act without governance; in the second, GaaS enforces policies; in the third, adversarial agents probe robustness. All actions are intercepted, evaluated, and logged for analysis. Results show that GaaS reliably blocks or redirects high-risk behaviors while preserving throughput. Trust scores track rule adherence, isolating and penalizing untrustworthy components in multi-agent systems. By positioning governance as a runtime service akin to compute or storage, GaaS establishes infrastructure-level alignment for interoperable agent ecosystems. It does not teach agents ethics; it enforces them.</li>
<li><strong>摘要：</strong>随着AI系统的发展，具有自主性执行，异步推理和多机构协调的分布式生态系统，没有可扩展的，脱钩的治理会带来结构性的风险。现有的监督机制是反应性的，脆的，并且嵌入了代理体系结构中，使其无审计，难以在异质部署中概括。我们介绍了治理 -  AS-A-Service（GAAS）：一个模块化的，政策驱动的执法层，可在不改变模型内部或需要代理合作的情况下在运行时调节代理输出。 GAAS采用声明性规则和信任因素机制，该机制根据遵守和严重性加权违规行为进行评分。它可以实现强制性，规范和自适应干预措施，支持毕业的执法和动态信任调制。为了评估GAAS，我们在内容生成和财务决策中进行了三个模拟模型（Llama3，Qwen3，DeepSeek-R1）。在基准中，代理人无治疗行为；第二，GAAS执行政策；在第三个，对抗剂探测鲁棒性。所有动作均被截获，评估和记录以进行分析。结果表明，GAA可靠地阻止或重定向高风险行为，同时保存吞吐量。信任分数轨道规则依从性，隔离和惩罚多代理系统中的不信任组件。通过将治理定位为类似于计算或存储的运行时服务，GAAS建立了用于可互操作的代理生态系统的基础架构级别对齐。它不教代理人的道德；它执行他们。</li>
</ul>

<h3>Title: Beyond the Textual: Generating Coherent Visual Options for MCQs</h3>
<ul>
<li><strong>Authors: </strong>Wanqiang Wang, Longzhu He, Wei Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18772">https://arxiv.org/abs/2508.18772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18772">https://arxiv.org/pdf/2508.18772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18772]] Beyond the Textual: Generating Coherent Visual Options for MCQs(https://arxiv.org/abs/2508.18772)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multiple-choice questions (MCQs) play a crucial role in fostering deep thinking and knowledge integration in education. However, previous research has primarily focused on generating MCQs with textual options, but it largely overlooks the visual options. Moreover, generating high-quality distractors remains a major challenge due to the high cost and limited scalability of manual authoring. To tackle these problems, we propose a Cross-modal Options Synthesis (CmOS), a novel framework for generating educational MCQs with visual options. Our framework integrates Multimodal Chain-of-Thought (MCoT) reasoning process and Retrieval-Augmented Generation (RAG) to produce semantically plausible and visually similar answer and distractors. It also includes a discrimination module to identify content suitable for visual options. Experimental results on test tasks demonstrate the superiority of CmOS in content discrimination, question generation and visual option generation over existing methods across various subjects and educational levels.</li>
<li><strong>摘要：</strong>多项选择问题（MCQ）在促进教育中的深入思考和知识融合方面起着至关重要的作用。但是，以前的研究主要集中于通过文本选项生成MCQ，但在很大程度上忽略了视觉选项。此外，由于手动创作的高成本和有限的可扩展性，产生高质量的干扰物仍然是一个重大挑战。为了解决这些问题，我们提出了跨模式选项合成（CMOS），这是一个新颖的框架，用于通过视觉选项生成教育MCQ。我们的框架集成了多模式的链链（MCOT）推理过程和检索 - 演奏生成（RAG），以产生语义上合理且视觉上相似的答案和分散分散器。它还包括一个歧视模块，以识别适合视觉选项的内容。测试任务的实验结果证明了CMO在内容歧视，问题产生和视觉期权产生中的优势，而不是各种受试者和教育水平的现有方法。</li>
</ul>

<h3>Title: Embedding Font Impression Word Tags Based on Co-occurrence</h3>
<ul>
<li><strong>Authors: </strong>Yugo Kubota, Seiichi Uchida</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18825">https://arxiv.org/abs/2508.18825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18825">https://arxiv.org/pdf/2508.18825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18825]] Embedding Font Impression Word Tags Based on Co-occurrence(https://arxiv.org/abs/2508.18825)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Different font styles (i.e., font shapes) convey distinct impressions, indicating a close relationship between font shapes and word tags describing those impressions. This paper proposes a novel embedding method for impression tags that leverages these shape-impression relationships. For instance, our method assigns similar vectors to impression tags that frequently co-occur in order to represent impressions of fonts, whereas standard word embedding methods (e.g., BERT and CLIP) yield very different vectors. This property is particularly useful for impression-based font generation and font retrieval. Technically, we construct a graph whose nodes represent impression tags and whose edges encode co-occurrence relationships. Then, we apply spectral embedding to obtain the impression vectors for each tag. We compare our method with BERT and CLIP in qualitative and quantitative evaluations, demonstrating that our approach performs better in impression-guided font generation.</li>
<li><strong>摘要：</strong>不同的字体样式（即字体形状）传达出不同的印象，表明字体形状与描述这些印象的字体标签之间有着密切的关系。本文提出了一种新型的嵌入方法，用于使用这些形状印象关系的印象标签。例如，我们的方法将相似的向量分配给了经常同时发生以表示字体印象的印象标签，而标准单词嵌入方法（例如，BERT和CLIP）产生了非常不同的向量。该属性对于基于印象的字体生成和字体检索特别有用。从技术上讲，我们构造了一个图形，其节点代表印象标签，其边缘编码共发生的关系。然后，我们应用光谱嵌入以获得每个标签的印象向量。我们将方法与BERT和剪辑进行了比较，并在定性和定量评估中进行了比较，表明我们的方法在印象引导的字体生成中的表现更好。</li>
</ul>

<h3>Title: Energy-Based Flow Matching for Generating 3D Molecular Structure</h3>
<ul>
<li><strong>Authors: </strong>Wenyin Zhou, Christopher Iliffe Sprague, Vsevolod Viliuga, Matteo Tadiello, Arne Elofsson, Hossein Azizpour</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18949">https://arxiv.org/abs/2508.18949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18949">https://arxiv.org/pdf/2508.18949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18949]] Energy-Based Flow Matching for Generating 3D Molecular Structure(https://arxiv.org/abs/2508.18949)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Molecular structure generation is a fundamental problem that involves determining the 3D positions of molecules' constituents. It has crucial biological applications, such as molecular docking, protein folding, and molecular design. Recent advances in generative modeling, such as diffusion models and flow matching, have made great progress on these tasks by modeling molecular conformations as a distribution. In this work, we focus on flow matching and adopt an energy-based perspective to improve training and inference of structure generation models. Our view results in a mapping function, represented by a deep network, that is directly learned to \textit{iteratively} map random configurations, i.e. samples from the source distribution, to target structures, i.e. points in the data manifold. This yields a conceptually simple and empirically effective flow matching setup that is theoretically justified and has interesting connections to fundamental properties such as idempotency and stability, as well as the empirically useful techniques such as structure refinement in AlphaFold. Experiments on protein docking as well as protein backbone generation consistently demonstrate the method's effectiveness, where it outperforms recent baselines of task-associated flow matching and diffusion models, using a similar computational budget.</li>
<li><strong>摘要：</strong>分子结构的产生是一个基本问题，涉及确定分子成分的3D位置。它具有关键的生物学应用，例如分子对接，蛋白质折叠和分子设计。生成建模的最新进展（例如扩散模型和流量匹配）通过将分子构象作为分布进行建模，在这些任务上取得了巨大进步。在这项工作中，我们专注于流程匹配，并采用基于能量的观点来改善结构生成模型的训练和推断。我们的视图产生了由深网表示的映射函数，该函数直接学到了\ textIt {迭代}映射随机配置，即从源分布中的样本到目标结构，即数据歧管中的点。这产生了一种概念上简单且经验上有效的流匹配设置，理论上是合理的，并且与基本属性（如能力和稳定性）以及经验上有用的技术（例如Alphafold中的结构改进）具有有趣的联系。对蛋白质对接以及蛋白质主链产生的实验始终证明了该方法的有效性，在这种有效性上，它超过了与任务相关的流动匹配和扩散模型的最新基准，并使用类似的计算预算。</li>
</ul>

<h3>Title: Generative AI in Map-Making: A Technical Exploration and Its Implications for Cartographers</h3>
<ul>
<li><strong>Authors: </strong>Claudio Affolter, Sidi Wu, Yizi Chen, Lorenz Hurni</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18959">https://arxiv.org/abs/2508.18959</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18959">https://arxiv.org/pdf/2508.18959</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18959]] Generative AI in Map-Making: A Technical Exploration and Its Implications for Cartographers(https://arxiv.org/abs/2508.18959)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Traditional map-making relies heavily on Geographic Information Systems (GIS), requiring domain expertise and being time-consuming, especially for repetitive tasks. Recent advances in generative AI (GenAI), particularly image diffusion models, offer new opportunities for automating and democratizing the map-making process. However, these models struggle with accurate map creation due to limited control over spatial composition and semantic layout. To address this, we integrate vector data to guide map generation in different styles, specified by the textual prompts. Our model is the first to generate accurate maps in controlled styles, and we have integrated it into a web application to improve its usability and accessibility. We conducted a user study with professional cartographers to assess the fidelity of generated maps, the usability of the web application, and the implications of ever-emerging GenAI in map-making. The findings have suggested the potential of our developed application and, more generally, the GenAI models in helping both non-expert users and professionals in creating maps more efficiently. We have also outlined further technical improvements and emphasized the new role of cartographers to advance the paradigm of AI-assisted map-making.</li>
<li><strong>摘要：</strong>传统的地图制作在很大程度上依赖地理信息系统（GIS），需要域专业知识和耗时，尤其是对于重复的任务。生成AI（Genai），尤其是图像扩散模型的最新进展为使制图过程的自动化和民主化提供了新的机会。但是，由于对空间组成和语义布局的控制有限，这些模型与准确的地图创建相比。为了解决这个问题，我们集成了矢量数据，以指导图表生成不同样式，该样式由文本提示指定。我们的模型是第一个在受控样式中生成准确地图的模型，我们将其集成到Web应用程序中以提高其可用性和可访问性。我们与专业制图师进行了一项用户研究，以评估生成的地图的保真度，Web应用程序的可用性以及随之而来的Genai在地图制作中的含义。这些发现提出了我们开发的应用程序的潜力，更一般而言，Genai模型在帮助非专业用户和专业人员更有效地创建地图方面都提出了潜力。我们还概述了进一步的技术改进，并强调了制图师的新作用，以推动AI辅助地图制作的范式。</li>
</ul>

<h3>Title: USO: Unified Style and Subject-Driven Generation via Disentangled and Reward Learning</h3>
<ul>
<li><strong>Authors: </strong>Shaojin Wu, Mengqi Huang, Yufeng Cheng, Wenxu Wu, Jiahe Tian, Yiming Luo, Fei Ding, Qian He</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18966">https://arxiv.org/abs/2508.18966</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18966">https://arxiv.org/pdf/2508.18966</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18966]] USO: Unified Style and Subject-Driven Generation via Disentangled and Reward Learning(https://arxiv.org/abs/2508.18966)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Existing literature typically treats style-driven and subject-driven generation as two disjoint tasks: the former prioritizes stylistic similarity, whereas the latter insists on subject consistency, resulting in an apparent antagonism. We argue that both objectives can be unified under a single framework because they ultimately concern the disentanglement and re-composition of content and style, a long-standing theme in style-driven research. To this end, we present USO, a Unified Style-Subject Optimized customization model. First, we construct a large-scale triplet dataset consisting of content images, style images, and their corresponding stylized content images. Second, we introduce a disentangled learning scheme that simultaneously aligns style features and disentangles content from style through two complementary objectives, style-alignment training and content-style disentanglement training. Third, we incorporate a style reward-learning paradigm denoted as SRL to further enhance the model's performance. Finally, we release USO-Bench, the first benchmark that jointly evaluates style similarity and subject fidelity across multiple metrics. Extensive experiments demonstrate that USO achieves state-of-the-art performance among open-source models along both dimensions of subject consistency and style similarity. Code and model: this https URL</li>
<li><strong>摘要：</strong>现有文献通常将风格驱动和主题驱动的一代视为两个不相交的任务：前者优先考虑风格相似性，而后者则坚持主题一致性，从而产生了明显的对抗。我们认为，这两个目标都可以在一个单一框架下统一，因为它们最终涉及内容和风格的分离和重组，这是风格驱动的研究中的长期主题。为此，我们提出了USO，这是一种统一样式的优化自定义模型。首先，我们构建了一个大规模的三重态数据集，该数据集由内容图像，样式图像及其相应的程式化内容图像组成。其次，我们介绍了一个分离的学习计划，该方案同时通过两个互补的目标，样式培训和内容式的删除培训来使风格功能与样式的内容相结合，并将内容从样式中分离出来。第三，我们结合了一种样式的奖励学习范式，该范式表示为SRL，以进一步提高模型的性能。最后，我们发布了USO Bench，这是第一个共同评估多个指标的样式相似性和主题保真度的基准。广泛的实验表明，USO在主题一致性和样式相似性的方面都可以在开源模型之间达到最先进的性能。代码和模型：此HTTPS URL</li>
</ul>

<h3>Title: Enhancing Document VQA Models via Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Eric López, Artemis Llabrés, Ernest Valveny</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18984">https://arxiv.org/abs/2508.18984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18984">https://arxiv.org/pdf/2508.18984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18984]] Enhancing Document VQA Models via Retrieval-Augmented Generation(https://arxiv.org/abs/2508.18984)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Document Visual Question Answering (Document VQA) must cope with documents that span dozens of pages, yet leading systems still concatenate every page or rely on very large vision-language models, both of which are memory-hungry. Retrieval-Augmented Generation (RAG) offers an attractive alternative, first retrieving a concise set of relevant segments before generating answers from this selected evidence. In this paper, we systematically evaluate the impact of incorporating RAG into Document VQA through different retrieval variants - text-based retrieval using OCR tokens and purely visual retrieval without OCR - across multiple models and benchmarks. Evaluated on the multi-page datasets MP-DocVQA, DUDE, and InfographicVQA, the text-centric variant improves the "concatenate-all-pages" baseline by up to +22.5 ANLS, while the visual variant achieves +5.0 ANLS improvement without requiring any text extraction. An ablation confirms that retrieval and reranking components drive most of the gain, whereas the layout-guided chunking strategy - proposed in several recent works to leverage page structure - fails to help on these datasets. Our experiments demonstrate that careful evidence selection consistently boosts accuracy across multiple model sizes and multi-page benchmarks, underscoring its practical value for real-world Document VQA.</li>
<li><strong>摘要：</strong>文档视觉问题回答（文档VQA）必须应对遍布数十页的文档，但是领先的系统仍会使每个页面串联或依靠非常大的视觉语言模型，这两种模型都是渴望记忆的。检索演示的一代（RAG）提供了一种有吸引力的替代方案，首先要检索一套简洁的相关细分市场，然后再从此选定的证据中产生答案。在本文中，我们系统地评估了通过不同的检索变体将抹布纳入文档VQA的影响 - 使用OCR令牌基于文本的检索，而纯粹的视觉检索则在没有OCR的情况下进行了多个模型和基准测试。在多页数据集MP-DOCVQA，Dude和InfographicVQA上进行了评估，以文本为中心的变体将“ Concatenate-All-Pages”基线提高了+22.5 ANLS，而Visual Variant无需任何文本提取即可提高任何文本。消融证实，检索和重新我们的收益大部分增长，而布局引导的块策略（在最近的几项旨在利用页面结构的工作中提出） - 未能在这些数据集上提供帮助。我们的实验表明，仔细的证据选择始终提高多个模型尺寸和多页基准的精度，从而强调了其对现实世界文档VQA的实际价值。</li>
</ul>

<h3>Title: No Label Left Behind: A Unified Surface Defect Detection Model for all Supervision Regimes</h3>
<ul>
<li><strong>Authors: </strong>Blaž Rolih, Matic Fučka, Danijel Skočaj</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19060">https://arxiv.org/abs/2508.19060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19060">https://arxiv.org/pdf/2508.19060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19060]] No Label Left Behind: A Unified Surface Defect Detection Model for all Supervision Regimes(https://arxiv.org/abs/2508.19060)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Surface defect detection is a critical task across numerous industries, aimed at efficiently identifying and localising imperfections or irregularities on manufactured components. While numerous methods have been proposed, many fail to meet industrial demands for high performance, efficiency, and adaptability. Existing approaches are often constrained to specific supervision scenarios and struggle to adapt to the diverse data annotations encountered in real-world manufacturing processes, such as unsupervised, weakly supervised, mixed supervision, and fully supervised settings. To address these challenges, we propose SuperSimpleNet, a highly efficient and adaptable discriminative model built on the foundation of SimpleNet. SuperSimpleNet incorporates a novel synthetic anomaly generation process, an enhanced classification head, and an improved learning procedure, enabling efficient training in all four supervision scenarios, making it the first model capable of fully leveraging all available data annotations. SuperSimpleNet sets a new standard for performance across all scenarios, as demonstrated by its results on four challenging benchmark datasets. Beyond accuracy, it is very fast, achieving an inference time below 10 ms. With its ability to unify diverse supervision paradigms while maintaining outstanding speed and reliability, SuperSimpleNet represents a promising step forward in addressing real-world manufacturing challenges and bridging the gap between academic research and industrial applications. Code: this https URL</li>
<li><strong>摘要：</strong>表面缺陷检测是众多行业的关键任务，旨在有效地识别和本地化的缺陷或制造成分的不规则性。尽管已经提出了许多方法，但许多方法无法满足对高性能，效率和适应性的工业需求。现有的方法通常受到特定的监督方案的限制，并难以适应现实世界制造过程中遇到的各种数据注释，例如无监督，弱监督，混合监督和完全监督的设置。为了应对这些挑战，我们提出了SuperSimplenet，这是一种基于SimpleNet基础的高效和适应性的判别模型。 SuperSimplenet结合了一种新型的合成异常生成过程，增强的分类头和改进的学习过程，在所有四种监督方案中都可以有效培训，使其成为第一个能够完全利用所有可用数据注释的模型。 SuperSimplenet在所有方案中为性能设定了新的标准，如它在四个具有挑战性的基准数据集中的结果所证明。除了准确性之外，它非常快，达到了10毫秒以下的推理时间。 SuperSimplenet具有在保持出色的速度和可靠性的同时统一多样化的监督范式的能力，这代表了解决现实世界中的制造挑战并弥合学术研究与工业应用之间的差距的前进一步。代码：此HTTPS URL</li>
</ul>

<h3>Title: VibES: Induced Vibration for Persistent Event-Based Sensing</h3>
<ul>
<li><strong>Authors: </strong>Vincenzo Polizzi, Stephen Yang, Quentin Clark, Jonathan Kelly, Igor Gilitschenski, David B. Lindell</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19094">https://arxiv.org/abs/2508.19094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19094">https://arxiv.org/pdf/2508.19094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19094]] VibES: Induced Vibration for Persistent Event-Based Sensing(https://arxiv.org/abs/2508.19094)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Event cameras are a bio-inspired class of sensors that asynchronously measure per-pixel intensity changes. Under fixed illumination conditions in static or low-motion scenes, rigidly mounted event cameras are unable to generate any events, becoming unsuitable for most computer vision tasks. To address this limitation, recent work has investigated motion-induced event stimulation that often requires complex hardware or additional optical components. In contrast, we introduce a lightweight approach to sustain persistent event generation by employing a simple rotating unbalanced mass to induce periodic vibrational motion. This is combined with a motion-compensation pipeline that removes the injected motion and yields clean, motion-corrected events for downstream perception tasks. We demonstrate our approach with a hardware prototype and evaluate it on real-world captured datasets. Our method reliably recovers motion parameters and improves both image reconstruction and edge detection over event-based sensing without motion induction.</li>
<li><strong>摘要：</strong>事件摄像机是一种以生物为灵感的传感器类别，它们异步测量每像素强度的变化。在固定的照明条件下，在静态或低移动场景中，严格安装的事件摄像头无法生成任何事件，因此不适合大多数计算机视觉任务。为了解决这一限制，最近的工作调查了运动引起的事件刺激，通常需要复杂的硬件或其他光学组件。相比之下，我们通过采用简单的旋转不平衡的质量来诱导周期性振动运动，引入了一种轻巧的方法来维持持久的事件产生。这与运动补偿管道结合使用，该管道可去除注入的运动并产生干净的，运动校正的事件，以实现下游感知任务。我们使用硬件原型演示了我们的方法，并在现实世界中捕获的数据集上对其进行了评估。我们的方法可靠地恢复运动参数，并改善图像重建和边缘检测，而不是基于事件的传感，而无需运动感应。</li>
</ul>

<h3>Title: Composition and Alignment of Diffusion Models using Constrained Learning</h3>
<ul>
<li><strong>Authors: </strong>Shervin Khalafi, Ignacio Hounie, Dongsheng Ding, Alejandro Ribeiro</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.IV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19104">https://arxiv.org/abs/2508.19104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19104">https://arxiv.org/pdf/2508.19104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19104]] Composition and Alignment of Diffusion Models using Constrained Learning(https://arxiv.org/abs/2508.19104)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have become prevalent in generative modeling due to their ability to sample from complex distributions. To improve the quality of generated samples and their compliance with user requirements, two commonly used methods are: (i) Alignment, which involves fine-tuning a diffusion model to align it with a reward; and (ii) Composition, which combines several pre-trained diffusion models, each emphasizing a desirable attribute in the generated outputs. However, trade-offs often arise when optimizing for multiple rewards or combining multiple models, as they can often represent competing properties. Existing methods cannot guarantee that the resulting model faithfully generates samples with all the desired properties. To address this gap, we propose a constrained optimization framework that unifies alignment and composition of diffusion models by enforcing that the aligned model satisfies reward constraints and/or remains close to (potentially multiple) pre-trained models. We provide a theoretical characterization of the solutions to the constrained alignment and composition problems and develop a Lagrangian-based primal-dual training algorithm to approximate these solutions. Empirically, we demonstrate the effectiveness and merits of our proposed approach in image generation, applying it to alignment and composition, and show that our aligned or composed model satisfies constraints effectively, and improves on the equally-weighted approach. Our implementation can be found at this https URL.</li>
<li><strong>摘要：</strong>由于其从复杂分布中采样的能力，扩散模型在生成建模中变得普遍。为了提高生成样品的质量及其遵守用户要求，两种常用的方法是：（i）对齐，其中涉及对扩散模型进行微调以与奖励对齐； （ii）组合物结合了几个预训练的扩散模型，每个模型都强调了生成的输出中的理想属性。但是，在优化多个奖励或组合多个模型时，通常会出现权衡取舍，因为它们通常可以代表竞争性属性。现有方法无法保证所产生的模型忠实地生成具有所有所需属性的样品。为了解决这一差距，我们提出了一个受约束的优化框架，该框架通过强制执行对齐模型满足奖励约束和/或保持接近（潜在的多个）预训练的模型来统一扩散模型的对齐和组成。我们为约束对齐和组成问题的解决方案提供了理论表征，并开发了一种基于拉格朗日的原始偶型训练算法来近似这些解决方案。从经验上讲，我们证明了我们提出的方法在图像生成中的有效性和优点，将其应用于对齐和组成，并表明我们的对齐或组成的模型有效地满足了约束，并改善了同样加权的方法。我们的实现可以在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Saddle Hierarchy in Dense Associative Memory</h3>
<ul>
<li><strong>Authors: </strong>Robin Thériault, Daniele Tantari</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.dis-nn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19151">https://arxiv.org/abs/2508.19151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19151">https://arxiv.org/pdf/2508.19151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19151]] Saddle Hierarchy in Dense Associative Memory(https://arxiv.org/abs/2508.19151)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Dense associative memory (DAM) models have been attracting renewed attention since they were shown to be robust to adversarial examples and closely related to state-of-the-art machine learning paradigms, such as the attention mechanisms in transformers and generative diffusion models. We study a DAM built upon a three-layer Boltzmann machine with Potts hidden units, which represent data clusters and classes. Through a statistical mechanics analysis, we derive saddle-point equations that characterize both the stationary points of DAMs trained on real data and the fixed points of DAMs trained on synthetic data within a teacher-student framework. Based on these results, we propose a novel regularization scheme that makes training significantly more stable. Moreover, we show empirically that our DAM learns interpretable solutions to both supervised and unsupervised classification problems. Pushing our theoretical analysis further, we find that the weights learned by relatively small DAMs correspond to unstable saddle points in larger DAMs. We implement a network-growing algorithm that leverages this saddle-point hierarchy to drastically reduce the computational cost of training dense associative memory.</li>
<li><strong>摘要：</strong>密集的关联记忆（DAM）模型一直吸引着新的注意力，因为它们被证明是对对抗性示例的鲁棒性，并且与最新的机器学习范式密切相关，例如变形金刚中的注意机制和生成扩散模型。我们研究建立在带有Potts隐藏单元的三层玻尔兹曼机器上的大坝，该机器代表数据簇和类。通过统计力学分析，我们得出了鞍点方程，这些方程式表征了在实际数据上训练的大坝的固定点和在教师学生框架内对合成数据训练的大坝的固定点。基于这些结果，我们提出了一种新型的正则化计划，使训练更加稳定。此外，我们从经验上表明，我们的大坝学习可解释的解决方案，以应对受监督和无监督的分类问题。进一步推动理论分析，我们发现，相对较小的水坝学到的权重对应于大坝中不稳定的马鞍点。我们实施了一种网络增长算法，该算法利用该鞍点层次结构大大降低训练密集的关联内存的计算成本。</li>
</ul>

<h3>Title: RDDM: Practicing RAW Domain Diffusion Model for Real-world Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Yan Chen, Yi Wen, Wei Li, Junchao Liu, Yong Guo, Jie Hu, Xinghao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19154">https://arxiv.org/abs/2508.19154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19154">https://arxiv.org/pdf/2508.19154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19154]] RDDM: Practicing RAW Domain Diffusion Model for Real-world Image Restoration(https://arxiv.org/abs/2508.19154)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generation</a></li>
<li><strong>Abstract: </strong>We present the RAW domain diffusion model (RDDM), an end-to-end diffusion model that restores photo-realistic images directly from the sensor RAW data. While recent sRGB-domain diffusion methods achieve impressive results, they are caught in a dilemma between high fidelity and realistic generation. As these models process lossy sRGB inputs and neglect the accessibility of the sensor RAW images in many scenarios, e.g., in image and video capturing in edge devices, resulting in sub-optimal performance. RDDM bypasses this limitation by directly restoring images in the RAW domain, replacing the conventional two-stage image signal processing (ISP) + IR pipeline. However, a simple adaptation of pre-trained diffusion models to the RAW domain confronts the out-of-distribution (OOD) issues. To this end, we propose: (1) a RAW-domain VAE (RVAE) learning optimal latent representations, (2) a differentiable Post Tone Processing (PTP) module enabling joint RAW and sRGB space optimization. To compensate for the deficiency in the dataset, we develop a scalable degradation pipeline synthesizing RAW LQ-HQ pairs from existing sRGB datasets for large-scale training. Furthermore, we devise a configurable multi-bayer (CMB) LoRA module handling diverse RAW patterns such as RGGB, BGGR, etc. Extensive experiments demonstrate RDDM's superiority over state-of-the-art sRGB diffusion methods, yielding higher fidelity results with fewer artifacts.</li>
<li><strong>摘要：</strong>我们提出了原始域扩散模型（RDDM），这是一种端到端扩散模型，直接从传感器原始数据中恢复了光真实的图像。尽管最近的SRGB域扩散方法取得了令人印象深刻的结果，但它们却陷入了高保真度和现实产生之间的困境。随着这些模型处理损失的SRGB输入并忽略了在许多情况下，在Edge设备中的图像和视频捕获中，传感器原始图像的可访问性，从而导致了次优性能。 RDDM通过直接恢复原始域中的图像，替换常规的两阶段图像信号处理（ISP） + IR管道来绕过此限制。但是，对原始领域的预先训练扩散模型的简单改编遇到了分布（OOD）问题。为此，我们提出：（1）原始域vae（RVAE）学习最佳潜在表示，（2）可区分的后音调处理（PTP）模块，启用关节RAW和SRGB空间优化。为了弥补数据集中的缺陷，我们开发了可扩展的降解管道合成现有SRGB数据集的原始LQ-HQ对进行大规模培训。此外，我们设计了一个可配置的多托式（CMB）LORA模块处理多种原始模式，例如RGGB，BGGR等。广泛的实验表明，RDDM优于最先进的SRGB SRGB扩散方法，从而产生了更高的富裕度，从而产生了较少的伪像。</li>
</ul>

<h3>Title: FastMesh:Efficient Artistic Mesh Generation via Component Decoupling</h3>
<ul>
<li><strong>Authors: </strong>Jeonghwan Kim, Yushi Lan, Armando Fortes, Yongwei Chen, Xingang Pan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19188">https://arxiv.org/abs/2508.19188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19188">https://arxiv.org/pdf/2508.19188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19188]] FastMesh:Efficient Artistic Mesh Generation via Component Decoupling(https://arxiv.org/abs/2508.19188)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent mesh generation approaches typically tokenize triangle meshes into sequences of tokens and train autoregressive models to generate these tokens sequentially. Despite substantial progress, such token sequences inevitably reuse vertices multiple times to fully represent manifold meshes, as each vertex is shared by multiple faces. This redundancy leads to excessively long token sequences and inefficient generation processes. In this paper, we propose an efficient framework that generates artistic meshes by treating vertices and faces separately, significantly reducing redundancy. We employ an autoregressive model solely for vertex generation, decreasing the token count to approximately 23\% of that required by the most compact existing tokenizer. Next, we leverage a bidirectional transformer to complete the mesh in a single step by capturing inter-vertex relationships and constructing the adjacency matrix that defines the mesh faces. To further improve the generation quality, we introduce a fidelity enhancer to refine vertex positioning into more natural arrangements and propose a post-processing framework to remove undesirable edge connections. Experimental results show that our method achieves more than 8$\times$ faster speed on mesh generation compared to state-of-the-art approaches, while producing higher mesh quality.</li>
<li><strong>摘要：</strong>最近的网格生成方法通常将三角形网状网格定为令牌序列和训练自回归模型的序列，以依次生成这些令牌。尽管取得了很大的进步，但这种令牌序列不可避免地多次重复使用顶点以完全表示多种多样的网格，因为每个顶点都由多个面共享。这种冗余导致令状序列过长和效率低下的生成过程。在本文中，我们提出了一个有效的框架，该框架通过分别处理顶点和面部来产生艺术网格，从而大大降低冗余。我们使用仅用于顶点生成的自回旋模型，将令牌计数降低到最紧凑的现有令牌仪所需的模型的大约23％。接下来，我们利用双向变压器通过捕获vertex关系并构建定义网格面的邻接矩阵来完成网格。为了进一步提高发电质量，我们引入了一个保真度增强器，以将顶点定位改进到更自然的安排中，并提出一个后处理框架，以消除不良的边缘连接。实验结果表明，与最先进的方法相比，我们的方法在网格生成中的速度更快，同时产生更高的网格质量。</li>
</ul>

<h3>Title: LSD-3D: Large-Scale 3D Driving Scene Generation with Geometry Grounding</h3>
<ul>
<li><strong>Authors: </strong>Julian Ost, Andrea Ramazzina, Amogh Joshi, Maximilian Bömer, Mario Bijelic, Felix Heide</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19204">https://arxiv.org/abs/2508.19204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19204">https://arxiv.org/pdf/2508.19204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19204]] LSD-3D: Large-Scale 3D Driving Scene Generation with Geometry Grounding(https://arxiv.org/abs/2508.19204)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large-scale scene data is essential for training and testing in robot learning. Neural reconstruction methods have promised the capability of reconstructing large physically-grounded outdoor scenes from captured sensor data. However, these methods have baked-in static environments and only allow for limited scene control -- they are functionally constrained in scene and trajectory diversity by the captures from which they are reconstructed. In contrast, generating driving data with recent image or video diffusion models offers control, however, at the cost of geometry grounding and causality. In this work, we aim to bridge this gap and present a method that directly generates large-scale 3D driving scenes with accurate geometry, allowing for causal novel view synthesis with object permanence and explicit 3D geometry estimation. The proposed method combines the generation of a proxy geometry and environment representation with score distillation from learned 2D image priors. We find that this approach allows for high controllability, enabling the prompt-guided geometry and high-fidelity texture and structure that can be conditioned on map layouts -- producing realistic and geometrically consistent 3D generations of complex driving scenes.</li>
<li><strong>摘要：</strong>大型场景数据对于机器人学习中的培训和测试至关重要。神经重建方法已承诺能够从捕获的传感器数据中重建大型物理室外场景。但是，这些方法具有烘焙的静态环境，仅允许有限的场景控制 - 它们在场景和轨迹多样性中受到重建的捕获的限制。相比之下，使用最新图像或视频扩散模型生成驾驶数据可提供控制，而以几何接地和因果关系为代价。在这项工作中，我们旨在弥合这一差距，并提出一种直接生成具有准确几何形状的大规模3D驾驶场景的方法，从而使因果新视图合成具有对象持久性和显式3D几何估计。所提出的方法将代理几何形状和环境表示的生成与学习的2D图像先验的得分蒸馏。我们发现，这种方法允许高可控性，可以在地图布局上进行及时引导的几何形状以及高保真的纹理和结构 - 产生了现实且几何的3D代人，几代人的复杂驾驶场景。</li>
</ul>

<h3>Title: Articulate3D: Zero-Shot Text-Driven 3D Object Posing</h3>
<ul>
<li><strong>Authors: </strong>Oishi Deb, Anjun Hu, Ashkan Khakzar, Philip Torr, Christian Rupprecht</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19244">https://arxiv.org/abs/2508.19244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19244">https://arxiv.org/pdf/2508.19244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19244]] Articulate3D: Zero-Shot Text-Driven 3D Object Posing(https://arxiv.org/abs/2508.19244)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We propose a training-free method, Articulate3D, to pose a 3D asset through language control. Despite advances in vision and language models, this task remains surprisingly challenging. To achieve this goal, we decompose the problem into two steps. We modify a powerful image-generator to create target images conditioned on the input image and a text instruction. We then align the mesh to the target images through a multi-view pose optimisation step. In detail, we introduce a self-attention rewiring mechanism (RSActrl) that decouples the source structure from pose within an image generative model, allowing it to maintain a consistent structure across varying poses. We observed that differentiable rendering is an unreliable signal for articulation optimisation; instead, we use keypoints to establish correspondences between input and target images. The effectiveness of Articulate3D is demonstrated across a diverse range of 3D objects and free-form text prompts, successfully manipulating poses while maintaining the original identity of the mesh. Quantitative evaluations and a comparative user study, in which our method was preferred over 85\% of the time, confirm its superiority over existing approaches. Project page:this https URL</li>
<li><strong>摘要：</strong>我们建议一种无培训方法Articulate3D，通过语言控制构成3D资产。尽管视觉和语言模型取得了进步，但此任务仍然令人惊讶地具有挑战性。为了实现这一目标，我们将问题分为两个步骤。我们修改功能强大的图像生成器，以创建在输入图像和文本指令下的目标图像。然后，我们通过多视图姿势优化步骤将网格与目标图像对齐。详细介绍，我们引入了一种自我发场的重新布线机制（RSACTRL），该机制将源结构从图像生成模型中的姿势中解除，从而使其能够在不同姿势之间保持一致的结构。我们观察到可区分的渲染是表达优化的不可靠信号。相反，我们使用关键点在输入和目标图像之间建立对应关系。在各种3D对象和自由形式的文本提示中证明了Articulate3D的有效性，在保持网格的原始身份的同时，成功地操纵了姿势。定量评估和比较用户研究，其中我们的方法在85 \％以上的时间超过85 \％，确认其优于现有方法的优势。项目页面：此HTTPS URL</li>
</ul>

<h3>Title: VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space</h3>
<ul>
<li><strong>Authors: </strong>Lin Li, Zehuan Huang, Haoran Feng, Gengxiong Zhuang, Rui Chen, Chunchao Guo, Lu Sheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19247">https://arxiv.org/abs/2508.19247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19247">https://arxiv.org/pdf/2508.19247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19247]] VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space(https://arxiv.org/abs/2508.19247)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>3D local editing of specified regions is crucial for game industry and robot interaction. Recent methods typically edit rendered multi-view images and then reconstruct 3D models, but they face challenges in precisely preserving unedited regions and overall coherence. Inspired by structured 3D generative models, we propose VoxHammer, a novel training-free approach that performs precise and coherent editing in 3D latent space. Given a 3D model, VoxHammer first predicts its inversion trajectory and obtains its inverted latents and key-value tokens at each timestep. Subsequently, in the denoising and editing phase, we replace the denoising features of preserved regions with the corresponding inverted latents and cached key-value tokens. By retaining these contextual features, this approach ensures consistent reconstruction of preserved areas and coherent integration of edited parts. To evaluate the consistency of preserved regions, we constructed Edit3D-Bench, a human-annotated dataset comprising hundreds of samples, each with carefully labeled 3D editing regions. Experiments demonstrate that VoxHammer significantly outperforms existing methods in terms of both 3D consistency of preserved regions and overall quality. Our method holds promise for synthesizing high-quality edited paired data, thereby laying the data foundation for in-context 3D generation. See our project page at this https URL.</li>
<li><strong>摘要：</strong>特定区域的3D本地编辑对于游戏行业和机器人互动至关重要。最近的方法通常编辑渲染的多视图图像，然后重建3D模型，但它们在精确保存未编辑的区域和整体连贯性方面面临挑战。受结构化3D生成模型的启发，我们提出了Voxhammer，这是一种新型的无训练方法，在3D潜在空间中执行精确且连贯的编辑。给定3D模型，Voxhammer首先预测其反转轨迹，并在每个时间步中获得其倒立的潜在和键值令牌。随后，在非授予和编辑阶段，我们用相应的倒潜在和缓存的键值令牌代替了保留区域的去核特征。通过保留这些上下文特征，这种方法可确保对保留区域的一致重建和编辑零件的连贯整合。为了评估保留区域的一致性，我们构建了Edit3D-Bench，这是一个由人类通知的数据集组成的数据集，该数据集包含数百个样本，每个样本都经过精心标记为3D编辑区域。实验表明，从保存区域的3D一致性和整体质量方面，Voxhammer显着优于现有方法。我们的方法有望合成高质量编辑的配对数据，从而为内部文化3D生成奠定了数据基础。在此HTTPS URL上查看我们的项目页面。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
