<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-06-16</h1>
<h3>Title: When Algorithms Play Favorites: Lookism in the Generation and Perception of Faces</h3>
<ul>
<li><strong>Authors: </strong>Miriam Doh, Aditya Gulati, Matei Mancas, Nuria Oliver</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11025">https://arxiv.org/abs/2506.11025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11025">https://arxiv.org/pdf/2506.11025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11025]] When Algorithms Play Favorites: Lookism in the Generation and Perception of Faces(https://arxiv.org/abs/2506.11025)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This paper examines how synthetically generated faces and machine learning-based gender classification algorithms are affected by algorithmic lookism, the preferential treatment based on appearance. In experiments with 13,200 synthetically generated faces, we find that: (1) text-to-image (T2I) systems tend to associate facial attractiveness to unrelated positive traits like intelligence and trustworthiness; and (2) gender classification models exhibit higher error rates on "less-attractive" faces, especially among non-White women. These result raise fairness concerns regarding digital identity systems.</li>
<li><strong>摘要：</strong>本文探讨了合成生成的面孔以及基于机器学习的性别分类算法受算法外观主义的影响，这是基于外观的优先处理。在具有13,200个合成面孔的实验中，我们发现：（1）文本对图像（T2i）系统倾向于将面部吸引力与智力和可信度等无关的积极特征相关联； （2）性别分类模型在“不那么吸引人”面孔上显示出更高的错误率，尤其是在非白人妇女中。这些结果引起了关于数字身份系统的公平关注。</li>
</ul>

<h3>Title: Evaluating Privacy-Utility Tradeoffs in Synthetic Smart Grid Data</h3>
<ul>
<li><strong>Authors: </strong>Andre Catarino, Rui Melo, Rui Abreu, Luis Cruz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11026">https://arxiv.org/abs/2506.11026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11026">https://arxiv.org/pdf/2506.11026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11026]] Evaluating Privacy-Utility Tradeoffs in Synthetic Smart Grid Data(https://arxiv.org/abs/2506.11026)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The widespread adoption of dynamic Time-of-Use (dToU) electricity tariffs requires accurately identifying households that would benefit from such pricing structures. However, the use of real consumption data poses serious privacy concerns, motivating the adoption of synthetic alternatives. In this study, we conduct a comparative evaluation of four synthetic data generation methods, Wasserstein-GP Generative Adversarial Networks (WGAN), Conditional Tabular GAN (CTGAN), Diffusion Models, and Gaussian noise augmentation, under different synthetic regimes. We assess classification utility, distribution fidelity, and privacy leakage. Our results show that architectural design plays a key role: diffusion models achieve the highest utility (macro-F1 up to 88.2%), while CTGAN provide the strongest resistance to reconstruction attacks. These findings highlight the potential of structured generative models for developing privacy-preserving, data-driven energy systems.</li>
<li><strong>摘要：</strong>广泛采用的动态使用时间（DTOU）电价需要准确地识别将受益于此类定价结构的家庭。但是，实际消费数据的使用构成了严重的隐私问题，激发了合成替代方案的采用。在这项研究中，我们对四种合成数据生成方法，WASSERSTEIN-GP生成对抗网络（WGAN），条件表格GAN（CTGAN），扩散模型和高斯噪声增强进行了比较评估。我们评估分类实用程序，分配保真度和隐私泄漏。我们的结果表明，建筑设计起着关键作用：扩散模型达到了最高的实用程序（宏F1高达88.2％），而CTGAN对重建攻击的阻力最强。这些发现突出了结构化生成模型开发隐私，数据驱动的能源系统的潜力。</li>
</ul>

<h3>Title: From Reasoning to Code: GRPO Optimization for Underrepresented Languages</h3>
<ul>
<li><strong>Authors: </strong>Federico Pennino, Bianca Raimondi, Massimo Rondelli, Andrea Gurioli, Maurizio Gabbrielli</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11027">https://arxiv.org/abs/2506.11027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11027">https://arxiv.org/pdf/2506.11027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11027]] From Reasoning to Code: GRPO Optimization for Underrepresented Languages(https://arxiv.org/abs/2506.11027)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating accurate and executable code using large language models (LLMs) is challenging for languages with limited public training data compared to popular languages such as Python. This paper introduces a generalizable approach that uses small-scale code versions of the Qwen 2.5 model combined with Group Relative Policy Optimization (GRPO) to enable effective code generation through explicit reasoning steps, which is particularly beneficial for languages with smaller source code databases. Using Prolog as a representative use case -- given its limited online presence -- the initial model faced challenges in generating executable code. After some training steps, the model successfully produces logically consistent and syntactically accurate code by directly integrating reasoning-driven feedback into the reinforcement learning loop. Experimental evaluations using mathematical logic problem benchmarks illustrate significant improvements in reasoning quality, code accuracy, and logical correctness, underscoring the potential of this approach to benefit a wide range of programming languages lacking extensive training resources.</li>
<li><strong>摘要：</strong>与Python等流行语言相比，使用大语言模型（LLM）生成准确和可执行的代码对于具有有限的公共培训数据的语言具有挑战性。本文介绍了一种可推广的方法，该方法使用QWEN 2.5模型的小规模代码版本与小组相对策略优化（GRPO）相结合，以通过显式推理步骤来实现有效的代码生成，这对具有较小源代码数据库的语言特别有益。使用Prolog作为代表性用例（鉴于其在线限制的存在），最初的模型在生成可执行代码方面面临着挑战。经过一些培训步骤，该模型通过将推理驱动的反馈直接集成到加强学习循环中，成功地产生了逻辑一致和句法准确的代码。使用数学逻辑问题基准进行的实验评估说明了推理质量，代码准确性和逻辑正确性的显着改善，强调了这种方法的潜力，从而使缺乏广泛培训资源的广泛编程语言受益。</li>
</ul>

<h3>Title: Output Scaling: YingLong-Delayed Chain of Thought in a Large Pretrained Time Series Forecasting Model</h3>
<ul>
<li><strong>Authors: </strong>Xue Wang, Tian Zhou, Jinyang Gao, Bolin Ding, Jingren Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11029">https://arxiv.org/abs/2506.11029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11029">https://arxiv.org/pdf/2506.11029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11029]] Output Scaling: YingLong-Delayed Chain of Thought in a Large Pretrained Time Series Forecasting Model(https://arxiv.org/abs/2506.11029)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present a joint forecasting framework for time series prediction that contrasts with traditional direct or recursive methods. This framework achieves state-of-the-art performance for our designed foundation model, YingLong, and reveals a novel scaling effect: longer outputs significantly enhance model accuracy due to delayed chain-of-thought reasoning in our non-causal approach. YingLong is a non-causal, bidirectional attention encoder-only transformer trained through masked token recovery, aligning more effectively with language understanding tasks than with generation tasks. Additionally, we boost performance by tackling output variance with a multi-input ensemble. We release four foundation models ranging from 6M to 300M parameters, demonstrating superior results in zero-shot tasks on the ETT and Weather datasets. YingLong achieves more than 60% best performance. To ensure generalizability, we assessed the models using the GIFT-Eval benchmark, which comprises 23 time series datasets across 7 domains. Yinglong significantly outperformed the best time-series foundation models, end-to-end trained models by 14% and 44% in rank this http URL pretrained 300M model is available at this https URL</li>
<li><strong>摘要：</strong>我们提出了时间序列预测的联合预测框架，该预测与传统的直接或递归方法形成对比。该框架为我们设计的基础模型Yinglong实现了最先进的性能，并揭示了一种新颖的缩放效果：由于我们的非疗法方法，由于延迟的想法链推理，更长的输出显着提高了模型的精度。 Yinglong是一种非导致双向关注的仅编码的仅通过掩盖的令牌恢复训练的变压器，比与生成任务更有效地与语言理解任务保持一致。此外，我们通过使用多输入合奏来解决输出差异来提高性能。我们发布了四个基础模型，范围从6m到300m，参数显示了ETT和天气数据集中零弹药任务的卓越结果。 Yinglong取得了超过60％的最佳性能。为了确保通用性，我们使用礼品词基准评估了模型，该基准包括跨7个域的23个时间序列数据集。在此HTTPS URL上，Yinglong的表现明显优于最佳时间序列粉底型模型，端到端训练的型号在排名中占14％和44％。</li>
</ul>

<h3>Title: GenFT: A Generative Parameter-Efficient Fine-Tuning Method for Pretrained Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Baoquan Zhang, Guangning Xu, Michael. K. Ng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11042">https://arxiv.org/abs/2506.11042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11042">https://arxiv.org/pdf/2506.11042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11042]] GenFT: A Generative Parameter-Efficient Fine-Tuning Method for Pretrained Foundation Models(https://arxiv.org/abs/2506.11042)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Pretrained Foundation Models (PFMs) have transformed numerous applications by enabling efficient adaptation to customized tasks. Parameter-Efficient Fine-Tuning (PEFT) has emerged as a resource-efficient alternative to full fine-tuning, especially leveraging reparameterized weights $\Delta W$ to adapt models for downstream tasks. However, a critical yet underexplored question remains: can we utilize well-pretrained weights $W_0$ to guide the update of task-specific $\Delta W$, avoiding inefficient training it from scratch? To end this, we propose Generative Parameter-Efficient Fine-Tuning (GenFT), a novel method that extracts structured, transferable information from $W_0$ for efficient $\Delta W$ training. To extract row and column structure information, GenFT applies row and column transformations to distill essential patterns from $W_0$. A tailored policy further decomposes $\Delta W$ into layer-shared and layer-specific components, balancing information reuse and individualized flexibility. GenFT is simple yet effective, achieving superior performance across CV and NLP tasks. Extensive experiments on VTAB-1K, FGVC, and GLUE benchmarks demonstrate that GenFT outperforms state-of-the-art PEFT methods, offering a new perspective for efficient model adaptation.</li>
<li><strong>摘要：</strong>预验证的基础模型（PFM）通过有效适应自定义任务，从而改变了许多应用程序。参数有效的微调（PEFT）已成为完整微调的资源效率替代品，尤其是利用重新处理重量$ \ delta w $以适应下游任务的模型。但是，一个关键但毫无疑问的问题仍然存在：我们是否可以利用精心预言的权重$ W_0 $来指导特定于任务的$ \ delta w $的更新，从而避免效率低下的培训？为了结束这一点，我们提出了生成参数有效的微调（GenFT），这是一种新颖的方法，从$ W_0 $中提取结构化的，可转移的信息，以进行有效的$ \ delta w $训练。要提取行和列结构信息，GenFT应用行和列转换以从$ W_0 $中提取基本模式。量身定制的策略进一步将$ \ delta w $分解为层共享和特定于图层的组件，平衡信息重用和个性化的灵活性。 GenFT简单而有效，在简历和NLP任务中实现了卓越的性能。对VTAB-1K，FGVC和GLUE基准的广泛实验表明，GenFT的表现优于最先进的PEFT方法，为有效的模型适应提供了新的视角。</li>
</ul>

<h3>Title: Procedural Environment Generation for Tool-Use Agents</h3>
<ul>
<li><strong>Authors: </strong>Michael Sullivan, Mareike Hartmann, Alexander Koller</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11045">https://arxiv.org/abs/2506.11045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11045">https://arxiv.org/pdf/2506.11045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11045]] Procedural Environment Generation for Tool-Use Agents(https://arxiv.org/abs/2506.11045)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Although the power of LLM tool-use agents has ignited a flurry of recent research in this area, the curation of tool-use training data remains an open problem$-$especially for online RL training. Existing approaches to synthetic tool-use data generation tend to be non-interactive, and/or non-compositional. We introduce RandomWorld, a pipeline for the procedural generation of interactive tools and compositional tool-use data. We show that models tuned via SFT and RL on synthetic RandomWorld data improve on a range of tool-use benchmarks, and set the new SoTA for two metrics on the NESTFUL dataset. Further experiments show that downstream performance scales with the amount of RandomWorld-generated training data, opening up the possibility of further improvement through the use of entirely synthetic data.</li>
<li><strong>摘要：</strong>尽管LLM工具使用代理的功能激发了该领域最近的一系列研究，但工具使用培训数据的策划仍然是一个开放的问题$  -  $  - 尤其是在线RL培训。现有的合成工具使用数据生成的方法往往是非相互作用和/或非组成的。我们介绍了RandomWorld，这是一种用于过程生成交互式工具和组成工具使用数据的管道。我们表明，通过SFT调整的模型和RL对合成随机世界数据的数据改进了一系列工具使用基准，并为嵌套数据集上的两个指标设置了新的SOTA。进一步的实验表明，下游的性能尺度缩放了随机世界生成的训练数据的量，从而通过使用完全合成的数据来开辟进一步改善的可能性。</li>
</ul>

<h3>Title: ACCORD: Autoregressive Constraint-satisfying Generation for COmbinatorial Optimization with Routing and Dynamic attention</h3>
<ul>
<li><strong>Authors: </strong>Henrik Abgaryan, Tristan Cazenave, Ararat Harutyunyan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11052">https://arxiv.org/abs/2506.11052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11052">https://arxiv.org/pdf/2506.11052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11052]] ACCORD: Autoregressive Constraint-satisfying Generation for COmbinatorial Optimization with Routing and Dynamic attention(https://arxiv.org/abs/2506.11052)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, yet their direct application to NP-hard combinatorial problems (CPs) remains underexplored. In this work, we systematically investigate the reasoning abilities of LLMs on a variety of NP-hard combinatorial optimization tasks and introduce ACCORD: Autoregressive Constraint-satisfying generation for COmbinatorial optimization with Routing and Dynamic attention. ACCORD features a novel dataset representation and model architecture that leverage the autoregressive nature of LLMs to dynamically enforce feasibility constraints, coupled with attention-based routing to activate problem-specific LoRA modules. We also present the ACCORD-90k supervised dataset, covering six NP-hard combinatorial problems: TSP, VRP, Knapsack, FlowShop, JSSP, and BinPacking. Extensive experiments demonstrate that our ACCORD model, built on an 8B-parameter Llama backbone, consistently outperforms standard prompting and input-output methods, even when compared to much larger LLMs, such as gpt-4. Ablation studies further show that our output structure enhances solution feasibility. To the best of our knowledge, this is the first large-scale, end-to-end framework for exploring the applications of LLMs to a broad spectrum of combinatorial optimization problems. The codes are publicly available at this https URL</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）表现出了令人印象深刻的推理能力，但它们直接应用于NP-HARD组合问题（CPS）仍然没有得到充实的态度。在这项工作中，我们系统地研究了LLMS在各种NP-HARD组合优化任务上的推理能力，并引入协议：与路由和动态关注的组合优化和动态注意力的组合优化的自动回归约束 - 完整生成。 Accord具有一种新颖的数据集表示和模型体系结构，该架构利用LLMS的自回归性质来动态执行可行性约束，再加上基于注意力的路由以激活特定问题的Lora模块。我们还提出了Accord-90K监督数据集，涵盖了六个NP-HARD组合问题：TSP，VRP，Knapsack，Flowshop，JSSP和Binpacking。广泛的实验表明，我们的协议模型建立在8B参数的Llama骨架上，即使与更大的LLMS（例如GPT-4）相比，即使是在标准提示和输入输出方法的表现。消融研究进一步表明，我们的输出结构增强了解决方案的可行性。据我们所知，这是第一个大规模的端到端框架，用于探索LLM在广泛的组合优化问题上的应用。这些代码在此HTTPS URL上公开可用</li>
</ul>

<h3>Title: PolyMicros: Bootstrapping a Foundation Model for Polycrystalline Material Structure</h3>
<ul>
<li><strong>Authors: </strong>Michael Buzzy, Andreas Robertson, Peng Chen, Surya Kalidindi</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11055">https://arxiv.org/abs/2506.11055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11055">https://arxiv.org/pdf/2506.11055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11055]] PolyMicros: Bootstrapping a Foundation Model for Polycrystalline Material Structure(https://arxiv.org/abs/2506.11055)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in Foundation Models for Materials Science are poised to revolutionize the discovery, manufacture, and design of novel materials with tailored properties and responses. Although great strides have been made, successes have been restricted to materials classes where multi-million sample data repositories can be readily curated (e.g., atomistic structures). Unfortunately, for many structural and functional materials (e.g., mesoscale structured metal alloys), such datasets are too costly or prohibitive to construct; instead, datasets are limited to very few examples. To address this challenge, we introduce a novel machine learning approach for learning from hyper-sparse, complex spatial data in scientific domains. Our core contribution is a physics-driven data augmentation scheme that leverages an ensemble of local generative models, trained on as few as five experimental observations, and coordinates them through a novel diversity curation strategy to generate a large-scale, physically diverse dataset. We utilize this framework to construct PolyMicros, the first Foundation Model for polycrystalline materials (a structural material class important across a broad range of industrial and scientific applications). We demonstrate the utility of PolyMicros by zero-shot solving several long standing challenges related to accelerating 3D experimental microscopy. Finally, we make both our models and datasets openly available to the community.</li>
<li><strong>摘要：</strong>材料科学基础模型的最新进展有望彻底改变具有量身定制的特性和回应的新型材料的发现，制造和设计。尽管取得了长足的进步，但成功限于材料类别，这些材料类别可以容易地策划数百万样品数据存储库（例如，原子结构）。不幸的是，对于许多结构和功能材料（例如，中尺度结构金属合金），此类数据集太昂贵或过于构建；相反，数据集仅限于很少的示例。为了应对这一挑战，我们介绍了一种新型的机器学习方法，用于从科学领域中的超级公平，复杂的空间数据学习。我们的核心贡献是一种由物理驱动的数据增强方案，该方案利用了当地生成模型的集合，经过五个实验性观察的培训，并通过一种新颖的多样性策略来协调它们，以产生一个大规模的，物理上多样化的数据集。我们利用该框架来构建多晶材料的第一个基础模型（在广泛的工业和科学应用中重要的结构材料类别）。我们通过零拍解决与加速3D实验显微镜相关的几个长期挑战来证明了多因素的实用性。最后，我们将我们的模型和数据集公开提供给社区。</li>
</ul>

<h3>Title: Adaptive Object Detection with ESRGAN-Enhanced Resolution & Faster R-CNN</h3>
<ul>
<li><strong>Authors: </strong>Divya Swetha K, Ziaul Haque Choudhury, Hemanta Kumar Bhuyan, Biswajit Brahma, Nilayam Kumar Kamila</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11122">https://arxiv.org/abs/2506.11122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11122">https://arxiv.org/pdf/2506.11122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11122]] Adaptive Object Detection with ESRGAN-Enhanced Resolution & Faster R-CNN(https://arxiv.org/abs/2506.11122)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generative</a></li>
<li><strong>Abstract: </strong>In this study, proposes a method for improved object detection from the low-resolution images by integrating Enhanced Super-Resolution Generative Adversarial Networks (ESRGAN) and Faster Region-Convolutional Neural Network (Faster R-CNN). ESRGAN enhances low-quality images, restoring details and improving clarity, while Faster R-CNN performs accurate object detection on the enhanced images. The combination of these techniques ensures better detection performance, even with poor-quality inputs, offering an effective solution for applications where image resolution is in consistent. ESRGAN is employed as a pre-processing step to enhance the low-resolution input image, effectively restoring lost details and improving overall image quality. Subsequently, the enhanced image is fed into the Faster R-CNN model for accurate object detection and localization. Experimental results demonstrate that this integrated approach yields superior performance compared to traditional methods applied directly to low-resolution images. The proposed framework provides a promising solution for applications where image quality is variable or limited, enabling more robust and reliable object detection in challenging scenarios. It achieves a balance between improved image quality and efficient object detection</li>
<li><strong>摘要：</strong>在这项研究中，提出了一种通过整合增强的超分辨率生成对抗网络（ESRGAN）和更快的区域 - 跨趋化神经网络（更快的R-CNN）来改善对象检测方法的方法。 Esrgan增强了低质量的图像，恢复细节并提高清晰度，而更快的R-CNN在增强的图像上执行准确的对象检测。这些技术的组合也可以确保更好的检测性能，即使输入质量不佳，为图像分辨率保持一致的应用提供了有效的解决方案。 Esrgan被用作增强低分辨率输入图像的预处理步骤，有效地恢复丢失的细节并提高整体图像质量。随后，增强的图像被馈入更快的R-CNN模型，以进行准确的对象检测和定位。实验结果表明，与直接应用于低分辨率图像的传统方法相比，这种综合方法的性能卓越。所提出的框架为图像质量是可变或有限的应用提供了一个有希望的解决方案，从而在具有挑战性的情况下可以更健壮和可靠的对象检测。它在改进的图像质量和有效的对象检测之间取得了平衡</li>
</ul>

<h3>Title: Technical Report for Argoverse2 Scenario Mining Challenges on Iterative Error Correction and Spatially-Aware Prompting</h3>
<ul>
<li><strong>Authors: </strong>Yifei Chen, Ross Greer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11124">https://arxiv.org/abs/2506.11124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11124">https://arxiv.org/pdf/2506.11124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11124]] Technical Report for Argoverse2 Scenario Mining Challenges on Iterative Error Correction and Spatially-Aware Prompting(https://arxiv.org/abs/2506.11124)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Scenario mining from extensive autonomous driving datasets, such as Argoverse 2, is crucial for the development and validation of self-driving systems. The RefAV framework represents a promising approach by employing Large Language Models (LLMs) to translate natural-language queries into executable code for identifying relevant scenarios. However, this method faces challenges, including runtime errors stemming from LLM-generated code and inaccuracies in interpreting parameters for functions that describe complex multi-object spatial relationships. This technical report introduces two key enhancements to address these limitations: (1) a fault-tolerant iterative code-generation mechanism that refines code by re-prompting the LLM with error feedback, and (2) specialized prompt engineering that improves the LLM's comprehension and correct application of spatial-relationship functions. Experiments on the Argoverse 2 validation set with diverse LLMs-Qwen2.5-VL-7B, Gemini 2.5 Flash, and Gemini 2.5 Pro-show consistent gains across multiple metrics; most notably, the proposed system achieves a HOTA-Temporal score of 52.37 on the official test set using Gemini 2.5 Pro. These results underline the efficacy of the proposed techniques for reliable, high-precision scenario mining.</li>
<li><strong>摘要：</strong>来自广泛的自动驾驶数据集（例如Argoverse 2）的场景挖掘对于自动驾驶系统的开发和验证至关重要。垃圾框架框架通过采用大型语言模型（LLM）将自然语言查询转化为可执行的代码，以识别相关场景，这代表了一种有希望的方法。但是，该方法面临挑战，包括源自LLM生成的代码引起的运行时错误和在解释参数的函数中描述复杂多物体空间关系的函数时的不准确性。该技术报告介绍了两个关键的增强，以解决以下局限性：（1）具有错误反馈通过重新提出LLM来完善代码的耐故障的迭代代码生成机制，以及（2）专门的及时工程，可改善LLM的理解和正确应用空间关联功能。具有不同的LLMS-QWEN2.5-VL-7B，GEMINI 2.5 Flash和Gemini 2.5 Pro-Show一致的增长，对Argoverse 2验证设置的实验设置。最值得注意的是，拟议的系统在使用Gemini 2.5 Pro的官方测试集上达到了52.37的HOTA-Spormal得分。这些结果强调了所提出的技术对可靠的高精度场景挖掘的功效。</li>
</ul>

<h3>Title: AlignHuman: Improving Motion and Fidelity via Timestep-Segment Preference Optimization for Audio-Driven Human Animation</h3>
<ul>
<li><strong>Authors: </strong>Chao Liang, Jianwen Jiang, Wang Liao, Jiaqi Yang, Zerong zheng, Weihong Zeng, Han Liang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11144">https://arxiv.org/abs/2506.11144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11144">https://arxiv.org/pdf/2506.11144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11144]] AlignHuman: Improving Motion and Fidelity via Timestep-Segment Preference Optimization for Audio-Driven Human Animation(https://arxiv.org/abs/2506.11144)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advancements in human video generation and animation tasks, driven by diffusion models, have achieved significant progress. However, expressive and realistic human animation remains challenging due to the trade-off between motion naturalness and visual fidelity. To address this, we propose \textbf{AlignHuman}, a framework that combines Preference Optimization as a post-training technique with a divide-and-conquer training strategy to jointly optimize these competing objectives. Our key insight stems from an analysis of the denoising process across timesteps: (1) early denoising timesteps primarily control motion dynamics, while (2) fidelity and human structure can be effectively managed by later timesteps, even if early steps are skipped. Building on this observation, we propose timestep-segment preference optimization (TPO) and introduce two specialized LoRAs as expert alignment modules, each targeting a specific dimension in its corresponding timestep interval. The LoRAs are trained using their respective preference data and activated in the corresponding intervals during inference to enhance motion naturalness and fidelity. Extensive experiments demonstrate that AlignHuman improves strong baselines and reduces NFEs during inference, achieving a 3.3$\times$ speedup (from 100 NFEs to 30 NFEs) with minimal impact on generation quality. Homepage: \href{this https URL}{this https URL}</li>
<li><strong>摘要：</strong>由扩散模型驱动的人类视频生成和动画任务的最新进展取得了重大进展。但是，由于运动自然性和视觉保真度之间的权衡，表现力和现实的人类动画仍然具有挑战性。为了解决这个问题，我们提出\ textbf {alignhuman}，该框架将优先优化作为训练后技术与分界线和争议培训策略相结合，以共同优化这些竞争性目标。我们的关键见解源于对时间步中的脱索过程的分析：（1）早期的时间段主要控制运动动力学，而（2）即使较早的步骤被跳过，也可以通过以后的时间段来有效地管理（2）忠诚度和人类结构。在此观察结果的基础上，我们提出了TimeStep-segrent段偏好优化（TPO），并引入了两个专门的Loras作为专家对齐模块，每个Loras在其相应的时间段间隔中都针对特定的维度。使用其各自的偏好数据训练Loras，并在推断过程中以相应的间隔激活，以增强运动的自然性和忠诚度。广泛的实验表明，Alignhuman在推理过程中改善了强质基线并减少了NFE，从而达到3.3 $ \ times $ speedup（从100 NFES到30 NFE），对发电质量的影响最小。主页：\ href {this https url} {此https url}</li>
</ul>

<h3>Title: LLM-to-Phy3D: Physically Conform Online 3D Object Generation with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Melvin Wong, Yueming Lyu, Thiago Rios, Stefan Menzel, Yew-Soon Ong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11148">https://arxiv.org/abs/2506.11148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11148">https://arxiv.org/pdf/2506.11148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11148]] LLM-to-Phy3D: Physically Conform Online 3D Object Generation with LLMs(https://arxiv.org/abs/2506.11148)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The emergence of generative artificial intelligence (GenAI) and large language models (LLMs) has revolutionized the landscape of digital content creation in different modalities. However, its potential use in Physical AI for engineering design, where the production of physically viable artifacts is paramount, remains vastly underexplored. The absence of physical knowledge in existing LLM-to-3D models often results in outputs detached from real-world physical constraints. To address this gap, we introduce LLM-to-Phy3D, a physically conform online 3D object generation that enables existing LLM-to-3D models to produce physically conforming 3D objects on the fly. LLM-to-Phy3D introduces a novel online black-box refinement loop that empowers large language models (LLMs) through synergistic visual and physics-based evaluations. By delivering directional feedback in an iterative refinement process, LLM-to-Phy3D actively drives the discovery of prompts that yield 3D artifacts with enhanced physical performance and greater geometric novelty relative to reference objects, marking a substantial contribution to AI-driven generative design. Systematic evaluations of LLM-to-Phy3D, supported by ablation studies in vehicle design optimization, reveal various LLM improvements gained by 4.5% to 106.7% in producing physically conform target domain 3D designs over conventional LLM-to-3D models. The encouraging results suggest the potential general use of LLM-to-Phy3D in Physical AI for scientific and engineering applications.</li>
<li><strong>摘要：</strong>生成人工智能（Genai）和大语言模型（LLM）的出现彻底改变了数字内容创建的景观。但是，它在物理AI中的潜在用途在工程设计中的生产至关重要，在该设计中的生产至关重要。现有的LLM到3D模型中缺乏物理知识通常会导致与现实世界中物理约束分离的输出。为了解决此差距，我们介绍了LLM-TO-PHY3D，这是一种物理上符合在线3D对象生成的，它使现有的LLM-TO-3D模型可以随时生成物理符合3D对象。 LLM-TO-PHY3D引入了一种新颖的在线黑盒细化环，该循环通过基于协同的视觉和物理学的评估来赋予大型语言模型（LLMS）。通过在迭代改进过程中提供定向反馈，LLM-to-Phy3D积极地发现了提示，这些提示相对于参考物体而产生的3D工件具有增强的物理性能和更大的几何新颖性，这标志着对AI-Drived生成设计的实质性贡献。在车辆设计优化方面的消融研究支持的LLM-TO-PHY3D的系统评估显示，与传统的LLM至3D模型相比，在产生物理上符合目标域3D设计方面，各种LLM改进获得了4.5％至106.7％。令人鼓舞的结果表明，LLM-to-PHY3D在物理AI中的一般使用中可能使用科学和工程应用。</li>
</ul>

<h3>Title: Synthetic Geology -- Structural Geology Meets Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Simon Ghyselincks, Valeriia Okhmak, Stefano Zampini, George Turkiyyah, David Keyes, Eldad Haber</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11164">https://arxiv.org/abs/2506.11164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11164">https://arxiv.org/pdf/2506.11164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11164]] Synthetic Geology -- Structural Geology Meets Deep Learning(https://arxiv.org/abs/2506.11164)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Visualizing the first few kilometers of the Earth's subsurface, a long-standing challenge gating a virtually inexhaustible list of important applications, is coming within reach through deep learning. Building on techniques of generative artificial intelligence applied to voxelated images, we demonstrate a method that extends surface geological data supplemented by boreholes to a three-dimensional subsurface region by training a neural network. The Earth's land area having been extensively mapped for geological features, the bottleneck of this or any related technique is the availability of data below the surface. We close this data gap in the development of subsurface deep learning by designing a synthetic data-generator process that mimics eons of geological activity such as sediment compaction, volcanic intrusion, and tectonic dynamics to produce a virtually limitless number of samples of the near lithosphere. A foundation model trained on such synthetic data is able to generate a 3D image of the subsurface from a previously unseen map of surface topography and geology, showing increasing fidelity with increasing access to borehole data, depicting such structures as layers, faults, folds, dikes, and sills. We illustrate the early promise of the combination of a synthetic lithospheric generator with a trained neural network model using generative flow matching. Ultimately, such models will be fine-tuned on data from applicable campaigns, such as mineral prospecting in a given region. Though useful in itself, a regionally fine-tuned models may be employed not as an end but as a means: as an AI-based regularizer in a more traditional inverse problem application, in which the objective function represents the mismatch of additional data with physical models with applications in resource exploration, hazard assessment, and geotechnical engineering.</li>
<li><strong>摘要：</strong>可视化地球地下的前几公里，这是一个长期以来的挑战，几乎无尽的重要应用程序列表正在通过深度学习来实现。我们基于应用于体素图像的生成人工智能技术的基础，我们演示了一种通过训练神经网络扩展到三维地下区域的表面地质数据。地球的土地面积已被广泛绘制为地质特征，该技术或任何相关技术的瓶颈是表面下方的数据的可用性。我们通过设计一个合成数据生成器过程来缩小地下深度学习的发展，该过程模仿地质活动的Eons，例如沉积物压实，火山入侵和构造动力学，以生成几乎无限的近岩石层样品。经过此类合成数据训练的基础模型能够从以前看不见的地形和地质图中生成地下的3D图像，显示出增加对钻孔数据的访问量增加的忠诚度，描绘了层，故障，折叠，折叠，dikes和sills等结构。我们说明了使用生成流匹配的合成岩石圈发生器与受过训练的神经网络模型组合的早期希望。最终，此类模型将在适用广告系列的数据中进行微调，例如在给定地区的矿产勘探。尽管本身很有用，但在更传统的反问题应用中，可以使用区域微调的模型，而是作为一种方法：作为基于AI的正常化程序，其目标函数代表了与物理模型的其他数据不匹配，并在资源探索，危害评估和地理技术工程中使用了应用程序的物理模型。</li>
</ul>

<h3>Title: BrainMAP: Multimodal Graph Learning For Efficient Brain Disease Localization</h3>
<ul>
<li><strong>Authors: </strong>Nguyen Linh Dan Le, Jing Ren, Ciyuan Peng, Chengyao Xie, Bowen Li, Feng Xia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11178">https://arxiv.org/abs/2506.11178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11178">https://arxiv.org/pdf/2506.11178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11178]] BrainMAP: Multimodal Graph Learning For Efficient Brain Disease Localization(https://arxiv.org/abs/2506.11178)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent years have seen a surge in research focused on leveraging graph learning techniques to detect neurodegenerative diseases. However, existing graph-based approaches typically lack the ability to localize and extract the specific brain regions driving neurodegenerative pathology within the full connectome. Additionally, recent works on multimodal brain graph models often suffer from high computational complexity, limiting their practical use in resource-constrained devices. In this study, we present BrainMAP, a novel multimodal graph learning framework designed for precise and computationally efficient identification of brain regions affected by neurodegenerative diseases. First, BrainMAP utilizes an atlas-driven filtering approach guided by the AAL atlas to pinpoint and extract critical brain subgraphs. Unlike recent state-of-the-art methods, which model the entire brain network, BrainMAP achieves more than 50% reduction in computational overhead by concentrating on disease-relevant subgraphs. Second, we employ an advanced multimodal fusion process comprising cross-node attention to align functional magnetic resonance imaging (fMRI) and diffusion tensor imaging (DTI) data, coupled with an adaptive gating mechanism to blend and integrate these modalities dynamically. Experimental results demonstrate that BrainMAP outperforms state-of-the-art methods in computational efficiency, without compromising predictive accuracy.</li>
<li><strong>摘要：</strong>近年来，研究的激增着重于利用图形学习技术检测神经退行性疾病。但是，现有的基于图的方法通常缺乏在整个连接组中局部驱动神经退行性病理学的特定大脑区域的能力。此外，关于多模式脑图模型的最新作品通常遭受高计算复杂性的影响，从而限制了它们在资源受限设备中的实际使用。在这项研究中，我们提出了Brainmap，这是一种新型的多模式图学习框架，旨在精确和计算有效地识别受神经退行性疾病影响的大脑区域。首先，Brainmap利用ALAS驱动的过滤方法以AAL ATLAS引导来查明和提取关键的大脑子图。与最近对整个大脑网络进行建模的最新方法不同，脑图通过集中于疾病相关的亚图可实现50％以上的计算开销降低。其次，我们采用了一个高级多模式融合过程，该过程包括跨节点的注意力集对对齐功能磁共振成像（fMRI）和扩散张量成像（DTI）数据，并与适应性门控机制结合，以动态混合和整合这些模态。实验结果表明，脑图在计算效率方面的表现优于最先进的方法，而不会损害预测精度。</li>
</ul>

<h3>Title: Domain-Constrained Diffusion Models to Synthesize Tabular Data: A Case Study in Power Systems</h3>
<ul>
<li><strong>Authors: </strong>Milad Hoseinpour, Vladimir Dvorkin</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11281">https://arxiv.org/abs/2506.11281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11281">https://arxiv.org/pdf/2506.11281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11281]] Domain-Constrained Diffusion Models to Synthesize Tabular Data: A Case Study in Power Systems(https://arxiv.org/abs/2506.11281)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Growing concerns over privacy, security, and legal barriers are driving the rising demand for synthetic data across domains such as healthcare, finance, and energy. While generative models offer a promising solution to overcome these barriers, their utility depends on the incorporation of domain-specific knowledge. We propose to synthesize data using a guided diffusion model that integrates domain constraints directly into the generative process. We develop the model in the context of power systems, with potential applicability to other domains that involve tabular data. Specifically, we synthesize statistically representative and high-fidelity power flow datasets. To satisfy domain constraints, e.g., Kirchhoff laws, we introduce a gradient-based guidance to steer the sampling trajectory in a feasible direction. Numerical results demonstrate the effectiveness of our approach.</li>
<li><strong>摘要：</strong>人们对隐私，安全性和法律障碍的越来越关注正在推动对医疗保健，金融和能源等领域的合成数据的不断增长。尽管生成模型提供了一种有希望的解决方案来克服这些障碍，但它们的效用取决于特定于领域的知识的融合。我们建议使用将域约束直接集成到生成过程中的指导扩散模型来合成数据。我们在电力系统的背景下开发模型，并具有潜在的适用性，可适用于涉及表格数据的其他域。具体而言，我们合成统计代表性和高保真功率流数据集。为了满足域约束，例如基尔乔夫法律，我们引入了基于梯度的指导，以将采样轨迹引导到可行的方向。数值结果证明了我们方法的有效性。</li>
</ul>

<h3>Title: TARDIS STRIDE: A Spatio-Temporal Road Image Dataset for Exploration and Autonomy</h3>
<ul>
<li><strong>Authors: </strong>Héctor Carrión, Yutong Bai, Víctor A. Hernández Castro, Kishan Panaganti, Ayush Zenith, Matthew Trang, Tony Zhang, Pietro Perona, Jitendra Malik</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11302">https://arxiv.org/abs/2506.11302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11302">https://arxiv.org/pdf/2506.11302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11302]] TARDIS STRIDE: A Spatio-Temporal Road Image Dataset for Exploration and Autonomy(https://arxiv.org/abs/2506.11302)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>World models aim to simulate environments and enable effective agent behavior. However, modeling real-world environments presents unique challenges as they dynamically change across both space and, crucially, time. To capture these composed dynamics, we introduce a Spatio-Temporal Road Image Dataset for Exploration (STRIDE) permuting 360-degree panoramic imagery into rich interconnected observation, state and action nodes. Leveraging this structure, we can simultaneously model the relationship between egocentric views, positional coordinates, and movement commands across both space and time. We benchmark this dataset via TARDIS, a transformer-based generative world model that integrates spatial and temporal dynamics through a unified autoregressive framework trained on STRIDE. We demonstrate robust performance across a range of agentic tasks such as controllable photorealistic image synthesis, instruction following, autonomous self-control, and state-of-the-art georeferencing. These results suggest a promising direction towards sophisticated generalist agents--capable of understanding and manipulating the spatial and temporal aspects of their material environments--with enhanced embodied reasoning capabilities. Training code, datasets, and model checkpoints are made available at this https URL.</li>
<li><strong>摘要：</strong>世界模型旨在模拟环境并实现有效的代理行为。但是，建模现实世界环境会在空间和至关重要的时间上动态变化时提出了独特的挑战。为了捕获这些组成的动力学，我们引入了一个时空的道路图像数据集，以进行探索（步幅），将360度全景图像置于丰富的互连观察，状态和动作节点。利用这种结构，我们可以同时建模以自我为中心的视图，位置坐标和运动命令之间的关系。我们通过TARDIS进行基准测试该数据集，TARDIS是一种基于变压器的生成世界模型，该模型通过在大步前进的统一自动回归框架来集成空间和时间动态。我们在一系列代理任务中表现出了稳健的性能，例如可控的影像图像综合，指导下，自主自治和最先进的地理因素。这些结果表明了朝着复杂的通才代理人的有希望的方向 - 能够理解和操纵其物质环境的空间和时间方面 - 具有增强的体现的推理能力。培训代码，数据集和模型检查点可在此HTTPS URL上提供。</li>
</ul>

<h3>Title: A Watermark for Auto-Regressive Image Generation Models</h3>
<ul>
<li><strong>Authors: </strong>Yihan Wu, Xuehao Cui, Ruibo Chen, Georgios Milis, Heng Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11371">https://arxiv.org/abs/2506.11371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11371">https://arxiv.org/pdf/2506.11371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11371]] A Watermark for Auto-Regressive Image Generation Models(https://arxiv.org/abs/2506.11371)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The rapid evolution of image generation models has revolutionized visual content creation, enabling the synthesis of highly realistic and contextually accurate images for diverse applications. However, the potential for misuse, such as deepfake generation, image based phishing attacks, and fabrication of misleading visual evidence, underscores the need for robust authenticity verification mechanisms. While traditional statistical watermarking techniques have proven effective for autoregressive language models, their direct adaptation to image generation models encounters significant challenges due to a phenomenon we term retokenization mismatch, a disparity between original and retokenized sequences during the image generation process. To overcome this limitation, we propose C-reweight, a novel, distortion-free watermarking method explicitly designed for image generation models. By leveraging a clustering-based strategy that treats tokens within the same cluster equivalently, C-reweight mitigates retokenization mismatch while preserving image fidelity. Extensive evaluations on leading image generation platforms reveal that C-reweight not only maintains the visual quality of generated images but also improves detectability over existing distortion-free watermarking techniques, setting a new standard for secure and trustworthy image synthesis.</li>
<li><strong>摘要：</strong>图像生成模型的快速演变彻底改变了视觉内容的创建，从而使高度现实和上下文准确的图像为各种应用程序综合。然而，滥用的潜力，例如产生深层，基于图像的网络钓鱼攻击以及误导性视觉证据的制造，强调了对强大的真实性验证机制的需求。尽管传统的统计水印技术已被证明对自回归语言模型有效，但由于我们称其为重述不匹配的现象，它们对图像生成模型的直接适应遇到了重大挑战，这是图像生成过程中原始序列和重述序列之间的差异。为了克服这一限制，我们提出了C-Reweight，这是一种针对图像生成模型的新颖，无失真的水印方法。通过利用基于聚类的策略，该策略等效地将令牌在同一群集内处理，C-Reweight会减轻边过反式的不匹配，同时保留图像保真度。对领先图像生成平台的广泛评估表明，C-Reweight不仅保持生成图像的视觉质量，而且还可以改善现有无失真水印技术的可检测性，从而为安全且可信赖的图像合成树立了新的标准。</li>
</ul>

<h3>Title: The Effect of Stochasticity in Score-Based Diffusion Sampling: a KL Divergence Analysis</h3>
<ul>
<li><strong>Authors: </strong>Bernardo P. Schaeffer, Ricardo M. S. Rosa, Glauco Valle</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11378">https://arxiv.org/abs/2506.11378</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11378">https://arxiv.org/pdf/2506.11378</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11378]] The Effect of Stochasticity in Score-Based Diffusion Sampling: a KL Divergence Analysis(https://arxiv.org/abs/2506.11378)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Sampling in score-based diffusion models can be performed by solving either a probability flow ODE or a reverse-time stochastic differential equation (SDE) parameterized by an arbitrary stochasticity parameter. In this work, we study the effect of stochasticity on the generation process through bounds on the Kullback-Leibler (KL) divergence and complement the analysis with numerical and analytical examples. Our results apply to general forward SDEs with additive noise and Lipschitz-continuous score functions, and quantify how errors from the prior distribution and score approximation propagate under different choices of the stochasticity parameter. The theoretical bounds are derived using log-Sobolev inequalities for the marginals of the forward process, which enable a more effective control of the KL divergence decay along sampling. For exact score functions, we find that stochasticity acts as an error-correcting mechanism, decreasing KL divergence along the sampling trajectory. For an approximate score function, there is a trade-off between error correction and score error amplification, so that stochasticity can either improve or worsen the performance, depending on the structure of the score error. Numerical experiments on simple datasets and a fully analytical example are included to illustrate and enlighten the theoretical results.</li>
<li><strong>摘要：</strong>可以通过求解概率流ode或通过任意随机性参数参数化的概率流ode或反向时间随机微分方程（SDE）来执行基于得分的扩散模型的采样。在这项工作中，我们通过对kullback-leibler（KL）差异的界限来研究随机性对发电过程的影响，并通过数值和分析示例补充分析。我们的结果适用于具有附加噪声和与Lipschitz连续得分函数的一般正向SDE，并量化了在随机性参数的不同选择下，先前分布和得分近似值的错误如何传播。理论界限是使用对向前过程边缘的log-sobolev不等式得出的，这可以更有效地控制沿采样的KL差异衰变。对于精确的得分函数，我们发现随机性充当误差校正机制，沿采样轨迹降低了KL差异。对于近似分数函数，误差校正和分数误差放大之间存在权衡，因此随机性可以改善或恶化性能，具体取决于得分误差的结构。包括在简单数据集和一个完全分析的示例上进行的数值实验，以说明和启发理论结果。</li>
</ul>

<h3>Title: Enhance Multimodal Consistency and Coherence for Text-Image Plan Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiaoxin Lu, Ranran Haoran Zhang, Yusen Zhang, Rui Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11380">https://arxiv.org/abs/2506.11380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11380">https://arxiv.org/pdf/2506.11380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11380]] Enhance Multimodal Consistency and Coherence for Text-Image Plan Generation(https://arxiv.org/abs/2506.11380)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>People get informed of a daily task plan through diverse media involving both texts and images. However, most prior research only focuses on LLM's capability of textual plan generation. The potential of large-scale models in providing text-image plans remains understudied. Generating high-quality text-image plans faces two main challenges: ensuring consistent alignment between two modalities and keeping coherence among visual steps. To address these challenges, we propose a novel framework that generates and refines text-image plans step-by-step. At each iteration, our framework (1) drafts the next textual step based on the prediction history; (2) edits the last visual step to obtain the next one; (3) extracts PDDL-like visual information; and (4) refines the draft with the extracted visual information. The textual and visual step produced in stage (4) and (2) will then serve as inputs for the next iteration. Our approach offers a plug-and-play improvement to various backbone models, such as Mistral-7B, Gemini-1.5, and GPT-4o. To evaluate the effectiveness of our approach, we collect a new benchmark consisting of 1,100 tasks and their text-image pair solutions covering 11 daily topics. We also design and validate a new set of metrics to evaluate the multimodal consistency and coherence in text-image plans. Extensive experiment results show the effectiveness of our approach on a range of backbone models against competitive baselines. Our code and data are available at this https URL.</li>
<li><strong>摘要：</strong>人们通过涉及文本和图像的各种媒体来了解日常任务计划。但是，大多数先前的研究仅着眼于LLM的文本计划生成能力。大型模型在提供文本图像计划方面的潜力仍在研究。产生高质量的文本图像计划面临两个主要挑战：确保两种方式之间的一致对齐并保持视觉步骤之间的连贯性。为了应对这些挑战，我们提出了一个新颖的框架，该框架逐步生成和完善文本图像计划。在每次迭代中，我们的框架（1）根据预测历史起草下一个文本步骤； （2）编辑最后一个视觉步骤以获得下一个步骤； （3）提取类似PDDL的视觉信息； （4）用提取的视觉信息完善草稿。然后，在阶段（4）和（2）中产生的文本和视觉步骤将作为下一次迭代的输入。我们的方法为各种主链模型提供了插件的改进，例如Mistral-7B，Gemini-1.5和GPT-4O。为了评估我们方法的有效性，我们收集了一个新的基准测试，该基准包括1,100个任务及其文本图像对解决方案，涵盖11个每日主题。我们还设计和验证了一组新的指标，以评估文本图像计划中的多模式一致性和连贯性。广泛的实验结果表明，我们的方法对针对竞争基线的一系列主链模型的有效性。我们的代码和数据可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Bias Amplification in RAG: Poisoning Knowledge Retrieval to Steer LLMs</h3>
<ul>
<li><strong>Authors: </strong>Linlin Wang, Tianqing Zhu, Laiqiao Qin, Longxiang Gao, Wanlei Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11415">https://arxiv.org/abs/2506.11415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11415">https://arxiv.org/pdf/2506.11415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11415]] Bias Amplification in RAG: Poisoning Knowledge Retrieval to Steer LLMs(https://arxiv.org/abs/2506.11415)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In Large Language Models, Retrieval-Augmented Generation (RAG) systems can significantly enhance the performance of large language models by integrating external knowledge. However, RAG also introduces new security risks. Existing research focuses mainly on how poisoning attacks in RAG systems affect model output quality, overlooking their potential to amplify model biases. For example, when querying about domestic violence victims, a compromised RAG system might preferentially retrieve documents depicting women as victims, causing the model to generate outputs that perpetuate gender stereotypes even when the original query is gender neutral. To show the impact of the bias, this paper proposes a Bias Retrieval and Reward Attack (BRRA) framework, which systematically investigates attack pathways that amplify language model biases through a RAG system manipulation. We design an adversarial document generation method based on multi-objective reward functions, employ subspace projection techniques to manipulate retrieval results, and construct a cyclic feedback mechanism for continuous bias amplification. Experiments on multiple mainstream large language models demonstrate that BRRA attacks can significantly enhance model biases in dimensions. In addition, we explore a dual stage defense mechanism to effectively mitigate the impacts of the attack. This study reveals that poisoning attacks in RAG systems directly amplify model output biases and clarifies the relationship between RAG system security and model fairness. This novel potential attack indicates that we need to keep an eye on the fairness issues of the RAG system.</li>
<li><strong>摘要：</strong>在大型语言模型中，检索功能的生成（RAG）系统可以通过集成外部知识来显着增强大语言模型的性能。但是，RAG还引入了新的安全风险。现有的研究主要关注抹布系统中的中毒攻击如何影响模型输出质量，从而忽略了它们扩大模型偏见的潜力。例如，在查询家庭暴力受害者时，折磨的抹布系统可能会优先检索将妇女描绘成受害者的文件，从而导致该模型产生的产量，即使原始查询是性别中性的，也可以使性别刻板印象永存。为了显示偏见的影响，本文提出了一个偏见检索和奖励攻击（BRRA）框架，该框架系统地研究了攻击途径，从而通过破布系统操纵来扩大语言模型的偏见。我们设计了一种基于多目标奖励功能的对抗文档生成方法，采用子空间投影技术来操纵检索结果，并构建一种环状反馈机制，以进行连续偏置放大。在多个主流大语言模型上进行的实验表明，BRRA攻击可以显着增强维度的模型偏差。此外，我们还探索了双重阶段防御机制，以有效减轻攻击的影响。这项研究表明，抹布系统中的中毒攻击直接扩大模型输出偏见，并阐明了抹布系统安全与模型公平之间的关系。这种新颖的潜在攻击表明，我们需要密切关注抹布系统的公平问题。</li>
</ul>

<h3>Title: Auditing Data Provenance in Real-world Text-to-Image Diffusion Models for Privacy and Copyright Protection</h3>
<ul>
<li><strong>Authors: </strong>Jie Zhu, Leye Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11434">https://arxiv.org/abs/2506.11434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11434">https://arxiv.org/pdf/2506.11434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11434]] Auditing Data Provenance in Real-world Text-to-Image Diffusion Models for Privacy and Copyright Protection(https://arxiv.org/abs/2506.11434)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion model since its propose has significantly influenced the content creation due to its impressive generation capability. However, this capability depends on large-scale text-image datasets gathered from web platforms like social media, posing substantial challenges in copyright compliance and personal privacy leakage. Though there are some efforts devoted to explore approaches for auditing data provenance in text-to-image diffusion models, existing work has unrealistic assumptions that can obtain model internal knowledge, e.g., intermediate results, or the evaluation is not reliable. To fill this gap, we propose a completely black-box auditing framework called Feature Semantic Consistency-based Auditing (FSCA). It utilizes two types of semantic connections within the text-to-image diffusion model for auditing, eliminating the need for access to internal knowledge. To demonstrate the effectiveness of our FSCA framework, we perform extensive experiments on LAION-mi dataset and COCO dataset, and compare with eight state-of-the-art baseline approaches. The results show that FSCA surpasses previous baseline approaches across various metrics and different data distributions, showcasing the superiority of our FSCA. Moreover, we introduce a recall balance strategy and a threshold adjustment strategy, which collectively allows FSCA to reach up a user-level accuracy of 90% in a real-world auditing scenario with only 10 samples/user, highlighting its strong auditing potential in real-world applications. Our code is made available at this https URL.</li>
<li><strong>摘要：</strong>由于其建议的生成能力令人印象深刻，因此文本到图像扩散模型由于其提议的提议而极大地影响了内容的创建。但是，该功能取决于从社交媒体等网络平台收集的大规模文本图像数据集，在版权合规性和个人隐私泄漏方面构成了重大挑战。尽管有一些努力探索在文本到图像扩散模型中审核数据出处的方法，但现有工作具有不切实际的假设，可以获得模型的内部知识，例如中级结果，或评估不可靠。为了填补这一空白，我们提出了一个完全黑色的审核框架，称为“特征语义一致性审核”（FSCA）。它利用文本到图像扩散模型中的两种类型的语义连接进行审核，从而消除了访问内部知识的需求。为了证明我们的FSCA框架的有效性，我们在LAION-MI数据集和可可数据集上进行了广泛的实验，并与八种最先进的基线方法进行了比较。结果表明，FSCA超过了各种指标和不同数据分布的先前基线方法，从而展示了我们的FSCA的优势。此外，我们引入了召回余额策略和阈值调整策略，该策略允许FSCA在现实世界中只有10个样本/用户的现实审计场景中达到90％的用户级准确性，突出了其在现实世界中的强大审计潜力。我们的代码可在此HTTPS URL上提供。</li>
</ul>

<h3>Title: RollingQ: Reviving the Cooperation Dynamics in Multimodal Transformer</h3>
<ul>
<li><strong>Authors: </strong>Haotian Ni, Yake Wei, Hang Liu, Gong Chen, Chong Peng, Hao Lin, Di Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11465">https://arxiv.org/abs/2506.11465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11465">https://arxiv.org/pdf/2506.11465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11465]] RollingQ: Reviving the Cooperation Dynamics in Multimodal Transformer(https://arxiv.org/abs/2506.11465)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Multimodal learning faces challenges in effectively fusing information from diverse modalities, especially when modality quality varies across samples. Dynamic fusion strategies, such as attention mechanism in Transformers, aim to address such challenge by adaptively emphasizing modalities based on the characteristics of input data. However, through amounts of carefully designed experiments, we surprisingly observed that the dynamic adaptability of widely-used self-attention models diminishes. Model tends to prefer one modality regardless of data characteristics. This bias triggers a self-reinforcing cycle that progressively overemphasizes the favored modality, widening the distribution gap in attention keys across modalities and deactivating attention mechanism's dynamic properties. To revive adaptability, we propose a simple yet effective method Rolling Query (RollingQ), which balances attention allocation by rotating the query to break the self-reinforcing cycle and mitigate the key distribution gap. Extensive experiments on various multimodal scenarios validate the effectiveness of RollingQ and the restoration of cooperation dynamics is pivotal for enhancing the broader capabilities of widely deployed multimodal Transformers. The source code is available at this https URL.</li>
<li><strong>摘要：</strong>多模式学习在有效地融合不同方式的信息方面面临挑战，尤其是当样本各种样本各不相同时。动态融合策略，例如变形金刚的注意机制，旨在通过根据输入数据的特征自适应强调模式来应对这种挑战。但是，通过精心设计的实验数量，我们出人意料地观察到了广泛使用的自我注意力学模型的动态适应性会减少。无论数据特征如何，模型都倾向于偏爱一种方式。这种偏见触发了一个自我增强的周期，该周期逐渐过度强调了偏爱的方式，从而扩大了跨模态的注意力键的分布差距，并停用了注意机制的动态特性。为了恢复适应性，我们提出了一个简单而有效的方法滚动查询（RollingQ），该查询通过旋转查询以打破自我增强周期并减轻键分布差距来平衡注意力分配。在各种多模式场景上进行的广泛实验验证了RollingQ的有效性和合作动力学的恢复，这对于增强了广泛部署的多模式变压器的更广泛功能是关键的。源代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: FAME: A Lightweight Spatio-Temporal Network for Model Attribution of Face-Swap Deepfakes</h3>
<ul>
<li><strong>Authors: </strong>Wasim Ahmad, Yan-Tsung Peng, Yuan-Hao Chang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11477">https://arxiv.org/abs/2506.11477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11477">https://arxiv.org/pdf/2506.11477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11477]] FAME: A Lightweight Spatio-Temporal Network for Model Attribution of Face-Swap Deepfakes(https://arxiv.org/abs/2506.11477)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The widespread emergence of face-swap Deepfake videos poses growing risks to digital security, privacy, and media integrity, necessitating effective forensic tools for identifying the source of such manipulations. Although most prior research has focused primarily on binary Deepfake detection, the task of model attribution -- determining which generative model produced a given Deepfake -- remains underexplored. In this paper, we introduce FAME (Fake Attribution via Multilevel Embeddings), a lightweight and efficient spatio-temporal framework designed to capture subtle generative artifacts specific to different face-swap models. FAME integrates spatial and temporal attention mechanisms to improve attribution accuracy while remaining computationally efficient. We evaluate our model on three challenging and diverse datasets: Deepfake Detection and Manipulation (DFDM), FaceForensics++, and FakeAVCeleb. Results show that FAME consistently outperforms existing methods in both accuracy and runtime, highlighting its potential for deployment in real-world forensic and information security applications.</li>
<li><strong>摘要：</strong>面部交通深层视频的广泛出现给数字安全，隐私和媒体完整性带来了越来越多的风险，因此需要有效的法医工具来识别此类操作的来源。尽管大多数先前的研究主要集中在二进制深层检测上，但是模型归因的任务（确定哪种生成模型产生了给定的深泡沫）仍然没有被忽视。在本文中，我们介绍了名声（通过多层嵌入的假归因），这是一种轻巧，有效的时空框架，旨在捕获针对不同面部塑料模型特有的微妙生成伪像。名望集成了空间和时间注意机制，以提高归因精度，同时保持计算有效。我们评估了三个挑战和不同数据集的模型：DeepFake检测和操纵（DFDM），FaceForensics ++和FakeAvceleb。结果表明，名望在精度和运行时都始终优于现有方法，从而突出了其在现实法律和信息安全应用程序中部署的潜力。</li>
</ul>

<h3>Title: Composite Data Augmentations for Synthetic Image Detection Against Real-World Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Efthymia Amarantidou, Christos Koutlis, Symeon Papadopoulos, Panagiotis C. Petrantonakis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11490">https://arxiv.org/abs/2506.11490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11490">https://arxiv.org/pdf/2506.11490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11490]] Composite Data Augmentations for Synthetic Image Detection Against Real-World Perturbations(https://arxiv.org/abs/2506.11490)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The advent of accessible Generative AI tools enables anyone to create and spread synthetic images on social media, often with the intention to mislead, thus posing a significant threat to online information integrity. Most existing Synthetic Image Detection (SID) solutions struggle on generated images sourced from the Internet, as these are often altered by compression and other operations. To address this, our research enhances SID by exploring data augmentation combinations, leveraging a genetic algorithm for optimal augmentation selection, and introducing a dual-criteria optimization approach. These methods significantly improve model performance under real-world perturbations. Our findings provide valuable insights for developing detection models capable of identifying synthetic images across varying qualities and transformations, with the best-performing model achieving a mean average precision increase of +22.53% compared to models without augmentations. The implementation is available at this http URL.</li>
<li><strong>摘要：</strong>可访问的生成AI工具的出现使任何人都可以在社交媒体上创建和传播综合图像，这通常是为了误导，从而对在线信息完整性构成了重大威胁。大多数现有的合成图像检测（SID）解决方案在从Internet产生的产生图像上挣扎，因为这些图像通常会因压缩和其他操作而改变。为了解决这一问题，我们的研究通过探索数据增强组合，利用遗传算法进行最佳增强选择以及引入双标准优化方法来增强SID。这些方法在现实世界扰动下显着改善了模型性能。我们的发现为开发能够识别跨不同品质和转换的合成图像的检测模型提供了宝贵的见解，与没有增强模型相比，表现最佳的模型达到平均平均精度增长 +22.53％。该实现可在此HTTP URL上获得。</li>
</ul>

<h3>Title: Task-Driven Discrete Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Tung-Long Vuong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11511">https://arxiv.org/abs/2506.11511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11511">https://arxiv.org/pdf/2506.11511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11511]] Task-Driven Discrete Representation Learning(https://arxiv.org/abs/2506.11511)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In recent years, deep discrete representation learning (DRL) has achieved significant success across various domains. Most DRL frameworks (e.g., the widely used VQ-VAE and its variants) have primarily focused on generative settings, where the quality of a representation is implicitly gauged by the fidelity of its generation. In fact, the goodness of a discrete representation remain ambiguously defined across the literature. In this work, we adopt a practical approach that examines DRL from a task-driven perspective. We propose a unified framework that explores the usefulness of discrete features in relation to downstream tasks, with generation naturally viewed as one possible application. In this context, the properties of discrete representations as well as the way they benefit certain tasks are also relatively understudied. We therefore provide an additional theoretical analysis of the trade-off between representational capacity and sample complexity, shedding light on how discrete representation utilization impacts task performance. Finally, we demonstrate the flexibility and effectiveness of our framework across diverse applications.</li>
<li><strong>摘要：</strong>近年来，深度离散表示学习（DRL）在各个领域取得了巨大的成功。大多数DRL框架（例如，使用广泛使用的VQ-VAE及其变体）主要集中在生成设置上，在这种情况下，代表的质量被其一代的忠诚度暗示。实际上，在文献中，离散表示的好处仍然模棱两可。在这项工作中，我们采用了一种实用方法，该方法从任务驱动的角度检查了DRL。我们提出了一个统一的框架，该框架探讨了离散功能对下游任务的有用性，而生成自然视为一种可能的应用程序。在这种情况下，离散表示的属性以及它们受益某些任务的方式也相对研究。因此，我们提供了代表能力和样本复杂性之间权衡的额外理论分析，阐明了离散表示的利用如何影响任务绩效。最后，我们证明了跨不同应用程序框架的灵活性和有效性。</li>
</ul>

<h3>Title: Robust Filtering -- Novel Statistical Learning and Inference Algorithms with Applications</h3>
<ul>
<li><strong>Authors: </strong>Aamir Hussain Chughtai</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11530">https://arxiv.org/abs/2506.11530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11530">https://arxiv.org/pdf/2506.11530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11530]] Robust Filtering -- Novel Statistical Learning and Inference Algorithms with Applications(https://arxiv.org/abs/2506.11530)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>State estimation or filtering serves as a fundamental task to enable intelligent decision-making in applications such as autonomous vehicles, robotics, healthcare monitoring, smart grids, intelligent transportation, and predictive maintenance. Standard filtering assumes prior knowledge of noise statistics to extract latent system states from noisy sensor data. However, real-world scenarios involve abnormalities like outliers, biases, drifts, and missing observations with unknown or partially known statistics, limiting conventional approaches. This thesis presents novel robust nonlinear filtering methods to mitigate these challenges. Based on insights from our filtering proposals, we extend the formulations to offline estimation/learning setups and propose smoothing extensions. Our methods leverage Bayesian inference frameworks, employing both deterministic and stochastic approximation techniques including Variational Inference (VI) and Particle Filters/Sequential Monte Carlo (SMC). We also study theoretical estimation limits using Bayesian Cramér-Rao bounds (BCRBs) in the context of measurement abnormalities. To validate the performance gains of the proposed methods, we perform simulations and experiments in scenarios including target tracking, indoor localization, 3D point cloud registration, mesh registration, and pose graph optimization. The fundamental nature of the work makes it useful in diverse applications, with possible future extensions toward developing outlier-robust machine learning pipelines, learning system dynamics from anomalous data, and addressing challenges in generative AI where standard diffusion models struggle with outliers, imbalanced datasets, and mode collapse.</li>
<li><strong>摘要：</strong>州估计或过滤是一项基本任务，可以在诸如自动驾驶汽车，机器人技术，医疗保健监控，智能电网，智能运输和预测性维护等应用程序中实现智能决策。标准过滤假设噪声统计信息的先验知识从嘈杂的传感器数据中提取潜在系统状态。但是，现实世界中的场景涉及异常情况，例如具有未知或部分已知统计的异常值，偏见，漂移和缺失的观察结果，从而限制了常规方法。本文提出了新颖的鲁棒非线性过滤方法来减轻这些挑战。根据我们的过滤建议的见解，我们将配方扩展到离线估计/学习设置并提出平滑扩展。我们的方法利用贝叶斯推理框架采用确定性和随机近似技术，包括变异推理（VI）和粒子过滤器/顺序蒙特卡洛（SMC）。我们还使用贝叶斯cramér-rao边界（BCRB）研究了理论估计限，在测量异常的情况下。为了验证所提出方法的性能提高，我们在包括目标跟踪，室内定位，3D点云注册，网格注册和姿势图形优化在内的方案中执行模拟和实验。这项工作的基本性质使其在不同的应用程序中有用，并可能将来可能扩展，以开发异常机器的学习管道，从异常数据中学习系统动态，并应对生成AI中的挑战，在这种情况下，标准扩散模型与异常值，不平衡数据集和模式崩溃的标准扩散模型斗争。</li>
</ul>

<h3>Title: EyeSim-VQA: A Free-Energy-Guided Eye Simulation Framework for Video Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Zhaoyang Wang, Wen Lu, Jie Li, Lihuo He, Maoguo Gong, Xinbo Gao</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11549">https://arxiv.org/abs/2506.11549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11549">https://arxiv.org/pdf/2506.11549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11549]] EyeSim-VQA: A Free-Energy-Guided Eye Simulation Framework for Video Quality Assessment(https://arxiv.org/abs/2506.11549)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, quality assessment</a></li>
<li><strong>Abstract: </strong>Free-energy-guided self-repair mechanisms have shown promising results in image quality assessment (IQA), but remain under-explored in video quality assessment (VQA), where temporal dynamics and model constraints pose unique challenges. Unlike static images, video content exhibits richer spatiotemporal complexity, making perceptual restoration more difficult. Moreover, VQA systems often rely on pre-trained backbones, which limits the direct integration of enhancement modules without affecting model stability. To address these issues, we propose EyeSimVQA, a novel VQA framework that incorporates free-energy-based self-repair. It adopts a dual-branch architecture, with an aesthetic branch for global perceptual evaluation and a technical branch for fine-grained structural and semantic analysis. Each branch integrates specialized enhancement modules tailored to distinct visual inputs-resized full-frame images and patch-based fragments-to simulate adaptive repair behaviors. We also explore a principled strategy for incorporating high-level visual features without disrupting the original backbone. In addition, we design a biologically inspired prediction head that models sweeping gaze dynamics to better fuse global and local representations for quality prediction. Experiments on five public VQA benchmarks demonstrate that EyeSimVQA achieves competitive or superior performance compared to state-of-the-art methods, while offering improved interpretability through its biologically grounded design.</li>
<li><strong>摘要：</strong>自由能引导的自我修复机制在图像质量评估（IQA）方面显示出令人鼓舞的结果，但在视频质量评估（VQA）中仍然不足，其中时间动态和模型约束会带来独特的挑战。与静态图像不同，视频内容具有更丰富的时空复杂性，使感知恢复更加困难。此外，VQA系统通常依赖于预训练的骨干，这限制了增强模块的直接集成而不会影响模型稳定性。为了解决这些问题，我们提出了Eyesimvqa，这是一个新颖的VQA框架，结合了基于自由能的自我修复。它采用双分支结构，并具有审美分支，用于全球感知评估，并提供了用于细粒结构和语义分析的技术分支。每个分支都集成了专门的增强模块，该模块量身定制为不同的视觉输入大量的全帧图像和基于贴片的片段，以模拟自适应修复行为。我们还探索了一种合并高级视觉特征的原则性策略，而不会破坏原始的骨干。此外，我们设计了一个以生物学启发的预测头，该预测头会模拟彻底的凝视动力学，以更好地融合全球和本地表示以进行质量预测。与最先进的方法相比，五个公共VQA基准的实验表明，EyesIMVQA具有竞争性或卓越的性能，同时通过其生物扎根的设计提供了改善的可解释性。</li>
</ul>

<h3>Title: SignAligner: Harmonizing Complementary Pose Modalities for Coherent Sign Language Generation</h3>
<ul>
<li><strong>Authors: </strong>Xu Wang, Shengeng Tang, Lechao Cheng, Feng Li, Shuo Wang, Richang Hong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11621">https://arxiv.org/abs/2506.11621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11621">https://arxiv.org/pdf/2506.11621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11621]] SignAligner: Harmonizing Complementary Pose Modalities for Coherent Sign Language Generation(https://arxiv.org/abs/2506.11621)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Sign language generation aims to produce diverse sign representations based on spoken language. However, achieving realistic and naturalistic generation remains a significant challenge due to the complexity of sign language, which encompasses intricate hand gestures, facial expressions, and body movements. In this work, we introduce PHOENIX14T+, an extended version of the widely-used RWTH-PHOENIX-Weather 2014T dataset, featuring three new sign representations: Pose, Hamer and Smplerx. We also propose a novel method, SignAligner, for realistic sign language generation, consisting of three stages: text-driven pose modalities co-generation, online collaborative correction of multimodality, and realistic sign video synthesis. First, by incorporating text semantics, we design a joint sign language generator to simultaneously produce posture coordinates, gesture actions, and body movements. The text encoder, based on a Transformer architecture, extracts semantic features, while a cross-modal attention mechanism integrates these features to generate diverse sign language representations, ensuring accurate mapping and controlling the diversity of modal features. Next, online collaborative correction is introduced to refine the generated pose modalities using a dynamic loss weighting strategy and cross-modal attention, facilitating the complementarity of information across modalities, eliminating spatiotemporal conflicts, and ensuring semantic coherence and action consistency. Finally, the corrected pose modalities are fed into a pre-trained video generation network to produce high-fidelity sign language videos. Extensive experiments demonstrate that SignAligner significantly improves both the accuracy and expressiveness of the generated sign videos.</li>
<li><strong>摘要：</strong>手语的产生旨在基于口语产生各种符号表示形式。然而，由于手语的复杂性，涵盖了复杂的手势，面部表情和身体运动，因此实现现实和自然的一代仍然是一个重大挑战。在这项工作中，我们介绍了Phoenix14t+，这是广泛使用的RWTH-PHOENIX-WEATER 2014T数据集的扩展版本，其中包含三个新的符号表示：Pose，Hamer和Smplerx。我们还提出了一种新颖的方法，即Signaligner，用于现实的手语产生，由三个阶段组成：文本驱动姿势模式共同生成，在线协作多模式的校正和现实的标志视频综合。首先，通过合并文本语义，我们设计了一个联合手语生成器，以同时产生姿势坐标，手势动作和身体运动。基于变压器体系结构的文本编码器提取语义特征，而跨模式的注意机制则集成了这些功能，以生成各种手语表示，从而确保准确的映射并控制模态特征的多样性。接下来，引入了在线协作校正，以使用动态的减肥策略和交叉模式的关注来完善产生的姿势模式，从而促进跨模态信息的互补性，消除时空冲突，并确保语义连贯性和动作一致性。最后，校正后的姿势模式被送入预先训练的视频生成网络中，以制作高保真的手语视频。广泛的实验表明，信号签名者显着提高了生成的符号视频的准确性和表现力。</li>
</ul>

<h3>Title: DiffFuSR: Super-Resolution of all Sentinel-2 Multispectral Bands using Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Sarmad, Arnt-Børre Salberg, Michael Kampffmeyer</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11764">https://arxiv.org/abs/2506.11764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11764">https://arxiv.org/pdf/2506.11764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11764]] DiffFuSR: Super-Resolution of all Sentinel-2 Multispectral Bands using Diffusion Models(https://arxiv.org/abs/2506.11764)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generative</a></li>
<li><strong>Abstract: </strong>This paper presents DiffFuSR, a modular pipeline for super-resolving all 12 spectral bands of Sentinel-2 Level-2A imagery to a unified ground sampling distance (GSD) of 2.5 meters. The pipeline comprises two stages: (i) a diffusion-based super-resolution (SR) model trained on high-resolution RGB imagery from the NAIP and WorldStrat datasets, harmonized to simulate Sentinel-2 characteristics; and (ii) a learned fusion network that upscales the remaining multispectral bands using the super-resolved RGB image as a spatial prior. We introduce a robust degradation model and contrastive degradation encoder to support blind SR. Extensive evaluations of the proposed SR pipeline on the OpenSR benchmark demonstrate that the proposed method outperforms current SOTA baselines in terms of reflectance fidelity, spectral consistency, spatial alignment, and hallucination suppression. Furthermore, the fusion network significantly outperforms classical pansharpening approaches, enabling accurate enhancement of Sentinel-2's 20 m and 60 m bands. This study underscores the power of harmonized learning with generative priors and fusion strategies to create a modular framework for Sentinel-2 SR. Our code and models can be found at this https URL.</li>
<li><strong>摘要：</strong>本文介绍了Difffusr，这是一种模块化管道，用于将Sentinel-2 Level-2a成像的所有12个光谱带的超级分辨率带到2.5米的统一地面采样距离（GSD）。管道包括两个阶段：（i）基于扩散的超分辨率（SR）模型，该模型在NAIP和WorldStrat数据集中对高分辨率RGB图像进行训练，并协调以模拟Sentinel-2特征； （ii）一个博学的融合网络，该网络使用超级分辨的RGB图像作为空间先验来介绍其余的多光谱频段。我们引入了强大的降解模型和对比性降解编码器，以支持盲目SR。对OpenSR基准的建议SR管道进行了广泛的评估表明，该方法在反射率，光谱一致性，空间对准和幻觉抑制方面优于当前的SOTA基准。此外，融合网络的表现明显优于经典的pansharpening方法，从而可以准确增强Sentinel-2的20 m和60 m频段。这项研究强调了与生成先验和融合策略的协调学习力量，以创建Sentinel-2 SR的模块化框架。我们的代码和模型可以在此HTTPS URL上找到。</li>
</ul>

<h3>Title: MambaVSR: Content-Aware Scanning State Space Model for Video Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Linfeng He, Meiqin Liu, Qi Tang, Chao Yao, Yao Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11768">https://arxiv.org/abs/2506.11768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11768">https://arxiv.org/pdf/2506.11768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11768]] MambaVSR: Content-Aware Scanning State Space Model for Video Super-Resolution(https://arxiv.org/abs/2506.11768)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Video super-resolution (VSR) faces critical challenges in effectively modeling non-local dependencies across misaligned frames while preserving computational efficiency. Existing VSR methods typically rely on optical flow strategies or transformer architectures, which struggle with large motion displacements and long video sequences. To address this, we propose MambaVSR, the first state-space model framework for VSR that incorporates an innovative content-aware scanning mechanism. Unlike rigid 1D sequential processing in conventional vision Mamba methods, our MambaVSR enables dynamic spatiotemporal interactions through the Shared Compass Construction (SCC) and the Content-Aware Sequentialization (CAS). Specifically, the SCC module constructs intra-frame semantic connectivity graphs via efficient sparse attention and generates adaptive spatial scanning sequences through spectral clustering. Building upon SCC, the CAS module effectively aligns and aggregates non-local similar content across multiple frames by interleaving temporal features along the learned spatial order. To bridge global dependencies with local details, the Global-Local State Space Block (GLSSB) synergistically integrates window self-attention operations with SSM-based feature propagation, enabling high-frequency detail recovery under global dependency guidance. Extensive experiments validate MambaVSR's superiority, outperforming the Transformer-based method by 0.58 dB PSNR on the REDS dataset with 55% fewer parameters.</li>
<li><strong>摘要：</strong>视频超分辨率（VSR）在有效地对未对准框架的非本地依赖性建模的同时，在保持计算效率的同时，面临着关键的挑战。现有的VSR方法通常依赖于光流策略或变压器体系结构，这些策略在大型运动位移和长时间的视频序列中挣扎。为了解决这个问题，我们提出了Mambavsr，这是VSR的第一个状态空间模型框架，其中包含了创新的内容感知的扫描机制。与常规视觉mamba方法中的刚性1D顺序处理不同，我们的Mambavsr通过共享指南针构造（SCC）和内容感知的顺序化（CAS）实现动态时空相互作用。具体而言，SCC模块通过有效的稀疏注意力构建框​​内语义连接图，并通过光谱群集生成自适应的空间扫描序列。在SCC的基础上，CAS模块通过沿着学习的空间顺序交织的时间特征有效地对齐和聚集了多个框架的非本地类似内容。为了启动全球依赖性，全球本地状态空间块（GLSSB）协同将窗口自我发项操作与基于SSM的特征传播相结合，从而在全球依赖性指导下实现高频细节恢复。广泛的实验验证了Mambavsr的优势，在REDS数据集上以0.58 dB PSNR优于基于变压器的方法，参数减少了55％。</li>
</ul>

<h3>Title: CLIP Meets Diffusion: A Synergistic Approach to Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Byeongchan Lee, John Won, Seunghyun Lee, Jinwoo Shin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11772">https://arxiv.org/abs/2506.11772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11772">https://arxiv.org/pdf/2506.11772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11772]] CLIP Meets Diffusion: A Synergistic Approach to Anomaly Detection(https://arxiv.org/abs/2506.11772)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Anomaly detection is a complex problem due to the ambiguity in defining anomalies, the diversity of anomaly types (e.g., local and global defect), and the scarcity of training data. As such, it necessitates a comprehensive model capable of capturing both low-level and high-level features, even with limited data. To address this, we propose CLIPFUSION, a method that leverages both discriminative and generative foundation models. Specifically, the CLIP-based discriminative model excels at capturing global features, while the diffusion-based generative model effectively captures local details, creating a synergistic and complementary approach. Notably, we introduce a methodology for utilizing cross-attention maps and feature maps extracted from diffusion models specifically for anomaly detection. Experimental results on benchmark datasets (MVTec-AD, VisA) demonstrate that CLIPFUSION consistently outperforms baseline methods, achieving outstanding performance in both anomaly segmentation and classification. We believe that our method underscores the effectiveness of multi-modal and multi-model fusion in tackling the multifaceted challenges of anomaly detection, providing a scalable solution for real-world applications.</li>
<li><strong>摘要：</strong>由于定义异常，异常类型的多样性（例如本地和全球缺陷）以及训练数据的稀缺性，异常检测是一个复杂的问题。因此，即使数据有限，它也需要一个能够捕获低级和高级功能的综合模型。为了解决这个问题，我们提出了夹层，这是一种利用歧视性和生成基础模型的方法。具体而言，基于夹的判别模型在捕获全局特征方面表现出色，而基于扩散的生成模型有效地捕获了本地细节，从而创建了一种协同和互补的方法。值得注意的是，我们引入了一种利用跨注意图和特征图的方法，这些图从专门用于异常检测的扩散模型中提取。基准数据集（MVTEC-AD，VISA）的实验结果表明，夹层始终优于基线方法，在异常分割和分类中都取得了出色的性能。我们认为，我们的方法强调了多模式和多模型融合在应对异常检测的多方面挑战方面的有效性，从而为实际应用提供了可扩展的解决方案。</li>
</ul>

<h3>Title: AgentSense: Virtual Sensor Data Generation Using LLM Agent in Simulated Home Environments</h3>
<ul>
<li><strong>Authors: </strong>Zikang Leng, Megha Thukral, Yaqi Liu, Hrudhai Rajasekhar, Shruthi K. Hiremath, Thomas Plötz</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11773">https://arxiv.org/abs/2506.11773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11773">https://arxiv.org/pdf/2506.11773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11773]] AgentSense: Virtual Sensor Data Generation Using LLM Agent in Simulated Home Environments(https://arxiv.org/abs/2506.11773)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>A major obstacle in developing robust and generalizable smart home-based Human Activity Recognition (HAR) systems is the lack of large-scale, diverse labeled datasets. Variability in home layouts, sensor configurations, and user behavior adds further complexity, as individuals follow varied routines and perform activities in distinct ways. Building HAR systems that generalize well requires training data that captures the diversity across users and environments. To address these challenges, we introduce AgentSense, a virtual data generation pipeline where diverse personas are generated by leveraging Large Language Models. These personas are used to create daily routines, which are then decomposed into low-level action sequences. Subsequently, the actions are executed in a simulated home environment called VirtualHome that we extended with virtual ambient sensors capable of recording the agents activities as they unfold. Overall, AgentSense enables the generation of rich, virtual sensor datasets that represent a wide range of users and home settings. Across five benchmark HAR datasets, we show that leveraging our virtual sensor data substantially improves performance, particularly when real data are limited. Notably, models trained on a combination of virtual data and just a few days of real data achieve performance comparable to those trained on the entire real datasets. These results demonstrate and prove the potential of virtual data to address one of the most pressing challenges in ambient sensing, which is the distinct lack of large-scale, annotated datasets without requiring any manual data collection efforts.</li>
<li><strong>摘要：</strong>开发坚固且可推广的基于智能家庭活动的人类活动识别（HAR）系统的主要障碍是缺乏大规模，标记的数据集。随着个人遵循不同的例程并以不同的方式执行活动，家庭布局，传感器配置和用户行为的可变性增加了进一步的复杂性。构建良好概括的HAR系统需要培训数据，以捕获用户和环境之间的多样性。为了应对这些挑战，我们介绍了Admentsense，这是一种虚拟数据生成管道，通过利用大型语言模型来产生不同的角色。这些角色用于创建日常工作，然后将其分解为低级动作序列。随后，这些动作是在称为VirtualHome的模拟家庭环境中执行的，我们将通过虚拟环境传感器进行扩展，能够在展开时记录代理活动。总体而言，Adgentens可以使代表广泛的用户和家庭设置的丰富，虚拟传感器数据集产生。在五个基准HAR数据集中，我们表明利用虚拟传感器数据可大大提高性能，尤其是在实际数据受到限制的情况下。值得注意的是，经过培训的虚拟数据训练的模型，仅几天的真实数据实现了与在整个真实数据集中训练的模型相当的性能。这些结果证明并证明了虚拟数据的潜力，可以解决环境传感中最紧迫的挑战之一，这显然缺乏大规模的注释数据集，而无需任何手动数据收集工作。</li>
</ul>

<h3>Title: SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks</h3>
<ul>
<li><strong>Authors: </strong>Hwiwon Lee, Ziqi Zhang, Hanxiao Lu, Lingming Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11791">https://arxiv.org/abs/2506.11791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11791">https://arxiv.org/pdf/2506.11791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11791]] SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks(https://arxiv.org/abs/2506.11791)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Rigorous security-focused evaluation of large language model (LLM) agents is imperative for establishing trust in their safe deployment throughout the software development lifecycle. However, existing benchmarks largely rely on synthetic challenges or simplified vulnerability datasets that fail to capture the complexity and ambiguity encountered by security engineers in practice. We introduce SEC-bench, the first fully automated benchmarking framework for evaluating LLM agents on authentic security engineering tasks. SEC-bench employs a novel multi-agent scaffold that automatically constructs code repositories with harnesses, reproduces vulnerabilities in isolated environments, and generates gold patches for reliable evaluation. Our framework automatically creates high-quality software vulnerability datasets with reproducible artifacts at a cost of only $0.87 per instance. Using SEC-bench, we implement two critical software security tasks to rigorously evaluate LLM agents' capabilities: proof-of-concept (PoC) generation and vulnerability patching. A comprehensive evaluation of state-of-the-art LLM code agents reveals significant performance gaps, achieving at most 18.0% success in PoC generation and 34.0% in vulnerability patching on our complete dataset. These results highlight the crucial steps needed toward developing LLM agents that are more practical, intelligent, and autonomous for security engineering.</li>
<li><strong>摘要：</strong>对大型语言模型（LLM）代理的严格注重评估对于在整个软件开发生命周期中建立对其安全部署的信任至关重要。但是，现有的基准在很大程度上依赖于综合挑战或简化的漏洞数据集，这些数据集未能捕获安全工程师在实践中遇到的复杂性和歧义性。我们介绍了SEC Bench，这是第一个全自动基准测试框架，用于评估LLM代理在真实的安全工程任务上。 SEC Bench采用了一种新型的多代理脚手架，该脚手架会自动构建具有线束的代码存储库，在隔离环境中重现脆弱性，并生成金贴片以进行可靠的评估。我们的框架会自动创建具有可重现工件的高质量软件漏洞数据集，每个实例仅需$ 0.87。使用SEC板凳，我们实施了两个关键的软件安全任务，以严格评估LLM代理的功能：概念证明（POC）生成和漏洞修补程序。对最先进的LLM代码代理的全面评估揭示了巨大的性能差距，最多在POC生成中取得了18.0％的成功，而在我们完整的数据集中进行了34.0％的漏洞修补。这些结果突出了开发更实用，更聪明且自治的LLM代理所需的关键步骤。</li>
</ul>

<h3>Title: Rethinking Multilingual Vision-Language Translation: Dataset, Evaluation, and Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Xintong Wang, Jingheng Pan, Yixiao Liu, Xiaohu Zhao, Chenyang Lyu, Minghao Wu, Chris Biemann, Longyue Wang, Linlong Xu, Weihua Luo, Kaifu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11820">https://arxiv.org/abs/2506.11820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11820">https://arxiv.org/pdf/2506.11820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11820]] Rethinking Multilingual Vision-Language Translation: Dataset, Evaluation, and Adaptation(https://arxiv.org/abs/2506.11820)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision-Language Translation (VLT) is a challenging task that requires accurately recognizing multilingual text embedded in images and translating it into the target language with the support of visual context. While recent Large Vision-Language Models (LVLMs) have demonstrated strong multilingual and visual understanding capabilities, there is a lack of systematic evaluation and understanding of their performance on VLT. In this work, we present a comprehensive study of VLT from three key perspectives: data quality, model architecture, and evaluation metrics. (1) We identify critical limitations in existing datasets, particularly in semantic and cultural fidelity, and introduce AibTrans -- a multilingual, parallel, human-verified dataset with OCR-corrected annotations. (2) We benchmark 11 commercial LVLMs/LLMs and 6 state-of-the-art open-source models across end-to-end and cascaded architectures, revealing their OCR dependency and contrasting generation versus reasoning behaviors. (3) We propose Density-Aware Evaluation to address metric reliability issues under varying contextual complexity, introducing the DA Score as a more robust measure of translation quality. Building upon these findings, we establish a new evaluation benchmark for VLT. Notably, we observe that fine-tuning LVLMs on high-resource language pairs degrades cross-lingual performance, and we propose a balanced multilingual fine-tuning strategy that effectively adapts LVLMs to VLT without sacrificing their generalization ability.</li>
<li><strong>摘要：</strong>视觉语言翻译（VLT）是一项具有挑战性的任务，需要准确地识别图像中嵌入的多语言文本，并在视觉上下文的支持下将其转化为目标语言。尽管最近的大型视力模型（LVLM）表现出强大的多语言和视觉理解能力，但缺乏系统的评估和对其在VLT上的表现的理解。在这项工作中，我们从三个关键角度介绍了VLT的全面研究：数据质量，模型架构和评估指标。 （1）我们确定现有数据集中的临界局限性，尤其是在语义和文化保真度中，并引入AIBTRANS  - 一种具有OCR校正注释的多语言，平行，人为验证的数据集。 （2）我们在端到端和级联体系结构上进行基准11个商业LVLM/LLM和6个最先进的开源模型，揭示了其OCR依赖性以及对比的生成与推理行为。 （3）我们提出密度感知评估，以解决不同上下文复杂性下的度量可靠性问题，将DA得分引入了更强大的翻译质量衡量标准。在这些发现的基础上，我们为VLT建立了新的评估基准。值得注意的是，我们观察到，在高资源语言对上进行微调LVLM会降低跨语性表现，并且我们提出了一种平衡的多语言微调策略，可以有效地适应LVLMS而不牺牲其概括能力。</li>
</ul>

<h3>Title: TreeRL: LLM Reinforcement Learning with On-Policy Tree Search</h3>
<ul>
<li><strong>Authors: </strong>Zhenyu Hou, Ziniu Hu, Yujiang Li, Rui Lu, Jie Tang, Yuxiao Dong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11902">https://arxiv.org/abs/2506.11902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11902">https://arxiv.org/pdf/2506.11902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11902]] TreeRL: LLM Reinforcement Learning with On-Policy Tree Search(https://arxiv.org/abs/2506.11902)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) with tree search has demonstrated superior performance in traditional reasoning tasks. Compared to conventional independent chain sampling strategies with outcome supervision, tree search enables better exploration of the reasoning space and provides dense, on-policy process rewards during RL training but remains under-explored in On-Policy LLM RL. We propose TreeRL, a reinforcement learning framework that directly incorporates on-policy tree search for RL training. Our approach includes intermediate supervision and eliminates the need for a separate reward model training. Existing approaches typically train a separate process reward model, which can suffer from distribution mismatch and reward hacking. We also introduce a cost-effective tree search approach that achieves higher search efficiency under the same generation token budget by strategically branching from high-uncertainty intermediate steps rather than using random branching. Experiments on challenging math and code reasoning benchmarks demonstrate that TreeRL achieves superior performance compared to traditional ChainRL, highlighting the potential of tree search for LLM. TreeRL is open-sourced at this https URL.</li>
<li><strong>摘要：</strong>通过树木搜索进行的强化学习（RL）在传统推理任务中表现出了出色的表现。与传统的独立链采样策略相比，树木搜索可以更好地探索推理空间，并在RL培训期间提供密集的，式的过程奖励，但在policy llm rl中仍未探索。我们提出了Treerl，这是一种加固学习框架，直接将其在policy树上搜索RL培训。我们的方法包括中间监督，并消除了对单独的奖励模型培训的需求。现有方法通常会训练单独的过程奖励模型，该模型可能会遭受分销不匹配和奖励黑客的困扰。我们还引入了一种具有成本效益的树搜索方法，该方法通过策略性地从高级不确定性中间步骤进行分支，而不是使用随机分支，从而在同一一代代币预算下实现了更高的搜索效率。关于挑战数学和代码推理基准的实验表明，与传统的ChainRL相比，Treerl的性能优于性能，突出了树木搜索LLM的潜力。 Treerl在此HTTPS URL上开源。</li>
</ul>

<h3>Title: Aligned Novel View Image and Geometry Synthesis via Cross-modal Attention Instillation</h3>
<ul>
<li><strong>Authors: </strong>Min-Seop Kwak, Junho Kim, Sangdoo Yun, Dongyoon Han, Taekyoung Kim, Seungryong Kim, Jin-Hwa Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11924">https://arxiv.org/abs/2506.11924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11924">https://arxiv.org/pdf/2506.11924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11924]] Aligned Novel View Image and Geometry Synthesis via Cross-modal Attention Instillation(https://arxiv.org/abs/2506.11924)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We introduce a diffusion-based framework that performs aligned novel view image and geometry generation via a warping-and-inpainting methodology. Unlike prior methods that require dense posed images or pose-embedded generative models limited to in-domain views, our method leverages off-the-shelf geometry predictors to predict partial geometries viewed from reference images, and formulates novel-view synthesis as an inpainting task for both image and geometry. To ensure accurate alignment between generated images and geometry, we propose cross-modal attention distillation, where attention maps from the image diffusion branch are injected into a parallel geometry diffusion branch during both training and inference. This multi-task approach achieves synergistic effects, facilitating geometrically robust image synthesis as well as well-defined geometry prediction. We further introduce proximity-based mesh conditioning to integrate depth and normal cues, interpolating between point cloud and filtering erroneously predicted geometry from influencing the generation process. Empirically, our method achieves high-fidelity extrapolative view synthesis on both image and geometry across a range of unseen scenes, delivers competitive reconstruction quality under interpolation settings, and produces geometrically aligned colored point clouds for comprehensive 3D completion. Project page is available at this https URL.</li>
<li><strong>摘要：</strong>我们介绍了一个基于扩散的框架，该框架通过翘曲和侵蚀方法执行了一致的新型视图图像和几何形状生成。与需要密集的姿势图像或姿势变成生成模型的先前方法不同，我们的方法利用了现成的几何形状预测器来预测从参考图像中观察到的部分几何形状，并将新颖的视图合成作为图像和几何形状的构成任务。为了确保生成的图像和几何形状之间的准确比对，我们提出了跨模式的注意蒸馏，其中将图像扩散分支的注意图注入训练和推理期间，将图像扩散分支注入了平行的几何扩散分支。这种多任务方法实现了协同效应，促进了几何鲁棒的图像合成以及定义明确的几何预测。我们进一步介绍了基于接近度的网格条件，以整合深度和正常线索，在点云之间插值并通过影响生成过程的错误预测几何形状。从经验上讲，我们的方法在一系列看不见的场景中实现了图像和几何形状上的高保真外推视综合，在插值设置下提供了竞争性的重建质量，并产生几何校准的有色点云，以综合3D完成。项目页面可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Simple Radiology VLLM Test-time Scaling with Thought Graph Traversal</h3>
<ul>
<li><strong>Authors: </strong>Yue Yao, Zelin Wen, Yan Tong, Xinyu Tian, Xuqing Li, Xiao Ma, Dongliang Xu, Tom Gedeon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11989">https://arxiv.org/abs/2506.11989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11989">https://arxiv.org/pdf/2506.11989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11989]] Simple Radiology VLLM Test-time Scaling with Thought Graph Traversal(https://arxiv.org/abs/2506.11989)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Test-time scaling offers a promising way to improve the reasoning performance of vision-language large models (VLLMs) without additional training. In this paper, we explore a simple but effective approach for applying test-time scaling to radiology report generation. Specifically, we introduce a lightweight Thought Graph Traversal (TGT) framework that guides the model to reason through organ-specific findings in a medically coherent order. This framework integrates structured medical priors into the prompt, enabling deeper and more logical analysis with no changes to the underlying model. To further enhance reasoning depth, we apply a reasoning budget forcing strategy that adjusts the model's inference depth at test time by dynamically extending its generation process. This simple yet powerful combination allows a frozen radiology VLLM to self-correct and generate more accurate, consistent chest X-ray reports. Our method outperforms baseline prompting approaches on standard benchmarks, and also reveals dataset biases through traceable reasoning paths. Code and prompts are open-sourced for reproducibility at this https URL.</li>
<li><strong>摘要：</strong>测试时间扩展提供了一种有希望的方法，可以提高视觉语言大型模型（VLLM）的推理性能，而无需额外的培训。在本文中，我们探讨了一种简单但有效的方法，将测试时间缩放应用于放射学报告的生成。具体而言，我们引入了一个轻巧的思想图遍历（TGT）框架，该框架可以指导模型以医学上一致的顺序通过器官特定的发现进行推理。该框架将结构化的医疗先验集成到及时，从而实现了更深入，更逻辑的分析，而不会改变基础模型。为了进一步增强推理深度，我们采用推理预算强迫策略，该策略通过动态扩展其生成过程来调整模型在测试时间的推理深度。这种简单而功能强大的组合允许冷冻放射学VLLM自我校正并产生更准确，一致的胸部X射线报告。我们的方法的表现优于基线提示在标准基准测试上的方法，并且还通过可追溯的推理路径揭示了数据集偏见。代码和提示是在此HTTPS URL上进行可重复可重复性的。</li>
</ul>

<h3>Title: Affogato: Learning Open-Vocabulary Affordance Grounding with Automated Data Generation at Scale</h3>
<ul>
<li><strong>Authors: </strong>Junha Lee, Eunha Park, Chunghyun Park, Dahyun Kang, Minsu Cho</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.12009">https://arxiv.org/abs/2506.12009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.12009">https://arxiv.org/pdf/2506.12009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.12009]] Affogato: Learning Open-Vocabulary Affordance Grounding with Automated Data Generation at Scale(https://arxiv.org/abs/2506.12009)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Affordance grounding-localizing object regions based on natural language descriptions of interactions-is a critical challenge for enabling intelligent agents to understand and interact with their environments. However, this task remains challenging due to the need for fine-grained part-level localization, the ambiguity arising from multiple valid interaction regions, and the scarcity of large-scale datasets. In this work, we introduce Affogato, a large-scale benchmark comprising 150K instances, annotated with open-vocabulary text descriptions and corresponding 3D affordance heatmaps across a diverse set of objects and interactions. Building on this benchmark, we develop simple yet effective vision-language models that leverage pretrained part-aware vision backbones and a text-conditional heatmap decoder. Our models trained with the Affogato dataset achieve promising performance on the existing 2D and 3D benchmarks, and notably, exhibit effectiveness in open-vocabulary cross-domain generalization. The Affogato dataset is shared in public: this https URL</li>
<li><strong>摘要：</strong>基于自然语言相互作用的自然语言描述，负担得起的对象区域是使智能代理能够理解和与环境互动的关键挑战。但是，由于需要细粒度的零件级定位，多个有效互动区域引起的歧义以及大规模数据集的稀缺性，因此此任务仍然具有挑战性。在这项工作中，我们介绍了affogato，这是一个大规模的基准，包括150k实例，并用开放式摄影文本描述和相应的3D负担热图进行了注释，这些热图跨各种物体和相互作用。在此基准测试的基础上，我们开发了简单而有效的视觉模型，以利用预验证的部分视觉骨架和文本条件热图解码器。我们经过Affogato数据集训练的模型在现有的2D和3D基准上实现了有希望的性能，尤其是在开放式跨域跨域概括中表现出有效性。 affogato数据集在公共场所共享：此HTTPS URL</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
